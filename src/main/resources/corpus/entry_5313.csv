2019,Structured Variational Inference in Continuous Cox Process Models,We propose a scalable framework for inference in a continuous sigmoidal Cox process that assumes the corresponding intensity function is given by a Gaussian process (GP) prior  transformed with a scaled logistic sigmoid function.  We present a tractable representation of the likelihood through augmentation with a  superposition of Poisson processes. This view enables a structured variational approximation capturing dependencies across variables in the model. Our framework avoids discretization of the domain  does not require accurate numerical integration over the input space and is not limited to GPs with squared exponential kernels. We evaluate our approach on synthetic and real-world data showing that its benefits are particularly pronounced on multivariate input settings where it overcomes the limitations of  mean-field methods and sampling schemes. We provide the state of-the-art in terms of speed  accuracy and uncertainty quantification trade-offs.,Structured Variational Inference in

Continuous Cox Process Models

Virginia Aglietti

University of Warwick
The Alan Turing Institute

V.Aglietti@warwick.ac.uk

Theodoros Damoulas
University of Warwick
The Alan Turing Institute

T.Damoulas@warwick.ac.uk

Edwin V. Bonilla
CSIRO’s Data61

Edwin.Bonilla@data61.csiro.au

Sally Cripps

Centre for Translational Data Science

The University of Sydney

Sally.Cripps@sydney.edu.au

Abstract

We propose a scalable framework for inference in a continuous sigmoidal Cox
process that assumes the corresponding intensity function is given by a Gaussian
process (GP) prior transformed with a scaled logistic sigmoid function. We present a
tractable representation of the likelihood through augmentation with a superposition
of Poisson processes. This view enables a structured variational approximation
capturing dependencies across variables in the model. Our framework avoids
discretization of the domain  does not require accurate numerical integration over
the input space and is not limited to GPs with squared exponential kernels. We
evaluate our approach on synthetic and real-world data showing that its beneﬁts
are particularly pronounced on multivariate input settings where it overcomes the
limitations of mean-ﬁeld methods and sampling schemes. We provide the state
of-the-art in terms of speed  accuracy and uncertainty quantiﬁcation trade-offs.

1

Introduction

Point processes have been used effectively to model a variety of event data such as occurrences of
diseases [9  19]  location of earthquakes [21] or crime events [2  11] . The most commonly adopted
class of models for such discrete data are non-homogenous Poisson processes and in particular Cox
processes [6]. In these  the observed events are assumed to be generated from a Poisson point process
(PPP) whose intensity is stochastic  enabling non-parametric inference and uncertainty quantiﬁcation.
Gaussian processes [GPs; 25] form a ﬂexible prior over functions and  therefore  have been used
to model the intensity of a Cox process via a non-linear positive link function. Typical mappings
are the exponential [9  22]  the square [17  19] and the sigmoidal [1  10  12] transformations. In
general  inferring the intensity function over a continuous input space X is highly problematic as it
requires integrating an inﬁnite-dimensional random function. This integral is generally intractable
and  depending on the transformation used  different algorithms have been proposed to deal with this
issue. For example  under the exponential transformation  a regular computational grid is commonly
introduced [9]. While this signiﬁcantly simpliﬁes inference  it leads to poor approximations  especially
in high dimensional settings. Increasing the resolution of the grid to improve the approximation
yields computationally prohibitive algorithms that do not scale  highlighting the well-known trade-off
between statistical performance and computational cost.
Other algorithms have been proposed to deal with a continuous X but they are computationally
expensive [1  12]  are limited to simple covariance functions [19]  require accurate numerical integra-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Summary of related work. X represents the inputs space with(cid:82) and(cid:80) denoting continuous

and discrete models respectively. O gives the time complexity of the algorithm. M represents the
number of thinned points derived from the thinning [16] of a PPP. K indicates the number of inducing
inputs. STVB denotes our approach.

Inference
O
λ(x)λ(x)λ(x)
XXX

STVB
SVI
K 3

(cid:82)

λ(cid:63)σ(f (x))

exp(f (x))

LGCP [22]

MCMC

N 3

(cid:80)

SGCP [1]
MCMC

(N + M )3
λ(cid:63)σ(f (x))

(cid:82)

Gunter et al. [12]

VBPP [19] Lian et al. [17] MFVB [10]

MCMC

(N + M )3
λ(cid:63)σ(f (x))

(cid:82)

VI-MF
N K 2
(f (x))2

(cid:82)

VI-MF
N K 2
(f (x))2

(cid:80)

VI-MF
N K 2

(cid:82)

λ(cid:63)σ(f (x))

tion over the domain [10] or do not account for the model dependencies in the posterior distribution
[10]. In this paper we propose an inference framework that addresses all of these modeling and
inference limitations by having a tractable representation of the likelihood via augmentation with a
superposition of PPPs. This enables a scalable structured variational inference algorithm (SVI) in the
continuous space directly  where the approximate posterior distribution incorporates dependencies
between the variables of interest. Our speciﬁc contributions are as follows.
Scalable inference in continuous input spaces: The augmentation of the input space via a process
superposition view allows us to develop a scalable variational inference algorithm that does not
require discretization or accurate numerical integration. With this view  we obtain a joint distribution
that is readily normalized  providing a natural regularization over the latent variables in our model.
Efﬁcient structured posterior estimation: We estimate a joint posterior that captures the complex
variable dependencies in the model while being signiﬁcantly faster than sampling approaches.
State-of-the-art performance: Our experimental evaluation shows the beneﬁts of our approach
when compared to state-of-the-art inference schemes  link functions  augmentation schemes and
representations of the input space X .

1.1 Related work

GP-modulated Poisson point processes are the gold standard for modeling event data. Performing
inference in these models is challenging due to the need to integrate an inﬁnite-dimensional random
function over X . Under the exponential transformation  inference has typically required discretization
where the domain is gridded and the intensity function is assumed to be constant over each grid cell [4 
7  9  22]. Alternatively  Lasko [15] also considers an exponential link function and performs inference
over a renewal process resorting to numerical integration within a computationally expensive sampling
scheme. These methods suffer from poor scaling with the dimensionality of X and sensitivity to the
choice of discretization or numerical integration technique. Several approaches have been proposed
to deal with inference in the continuous domain directly by using alternative transformations along
with additional modeling assumptions and computational tricks or by constraining the GP [20].
One of those alternative transformation is the squared mapping as developed in the Permanental
process [13  17–19  28]. Although the square transformation enables analytical computation of the
required integrals over X   this only holds for certain standard types of kernels such as the squared
exponential. In addition  Permanental processes suffer from important identiﬁability issues such as
reﬂection invariance1 and lead to models with “nodal lines” [13].
Another transformation is the scaled logistic sigmoid function proposed by [1] that achieves tractabil-
ity by augmenting the input space via thinning [16]  which can be seen as a point-process variant
of rejection sampling. This model is known as the sigmoidal Gaussian Cox process (SGCP). Their
proposed inference algorithm is based on Markov chain Monte Carlo (MCMC)  which enables draw-
ing ‘exact’ samples from the posterior intensity. However  as acknowledged by the authors  it has
signiﬁcant computational demands making it unfeasible to large datasets. As an extension to this
work  [12] introduce the concept of “adaptive thinning” and propose an expensive MCMC scheme
which scales as O(N 3). More recently  [10] introduced a neat double augmentation scheme for
SGCP which enables closed form updates using a mean-ﬁeld approximation (VI-MF). However  it

1With reﬂection invariance we refer to the invariance of the intensity function with respect to the sign change

of the GP used to model it.

2

requires accurate numerical integration over X   which makes the performance of the algorithm highly
dependent on the number of integration points.
In this work  we overcome the limitations of the mentioned VI-MF and MCMC schemes by proposing
an SVI framework  henceforth STVB  which takes into account the complex posterior dependencies
while being scalable and thus applicable to high-dimensional real-world settings. To the best of
our knowledge we are the ﬁrst to propose a fast structured variational inference framework for GP
modulated point process models. See Tab. 1 for a summary of the most relevant related works.

2 Model formulation
We consider learning problems where we are given a dataset of N events D = {xn}N
n=1  where xn
is a D-dimensional vector in the compact space X ⊂ RD. We aim at modeling these data via a PPP 
inferring its underlying intensity function λ(x) : X → R+ and making probabilistic predictions.

2.1 Sigmoidal Gaussian Cox process
Consider a realization ξ = (N {x1  ...  xn}) of a PPP on X where the points {x1  ...  xn} are treated
as indistinguishable apart from their locations [8]. Conditioned on λ(x)  the Cox process likelihood
function evaluated at ξ can be written as:

L(ξ|λ(x)) = exp

−

λ(x)dx

λ(xn) 

(1)

(cid:18)

(cid:90)

X

(cid:19) N(cid:89)

n=1

N !

L(ξ|λ(x))

where the intensity is given by λ(x) = λ(cid:63)σ(f (x)) with λ(cid:63) > 0 being an upperbound on λ(x)
with prior distribution p(λ(cid:63))  σ(·) denoting the the logistic sigmoid function and f is drawn from
a zero-mean GP prior with covariance function κ(x  x(cid:48); θ) and hyperparameters θ  i.e. f|θ ∼
GP(0  κ(x  x(cid:48); θ)). We will refer to this joint model as the sigmoidal Gaussian Cox process (SGCP).
Notice that  when considering the tuple (x1  ...  xn) instead of the set {x1  ...  xn}  and thus the event
ξ0 = (N  (x1  ...  xn))  the likelihood function is given by L(ξ0|λ(x)) =
. There are indeed
N ! permutations of the events {x1  ...  xn} giving the same point process realization. When the set
{x1  ...  xn} is known  considering L(ξ|λ(x)) or L(ξ0|λ(x)) does not affect the inference procedure.
The same holds for MCMC algorithms inferring the event locations. In this case  the factorial term
disappears in the computation of the acceptance ratio. However  as we shall see later  when the event
locations are latent variables in a model and inference proceeds via a variational approximation the
difference between the two likelihoods is essential. Indeed  while L(ξ0|λ(x)) is normalized with
respect to N  one must be cautious when integrating the likelihood in Eq. (1) over sets and bring
back the missing N ! factor so as to obtain a proper discrete probability mass function for N.
As it turns out  inference in SGCP is doubly intractable  as it requires solving the integral in Eq. (1)
and computing the intractable posterior distribution for the latent function at the N event locations
and the bounding intensity  i.e. p(fN   λ(cid:63)|{xn}N
n=1)  which in turns requires computing the marginal
likelihood. One way to avoid the ﬁrst source on intractability (integral in Eq.
(1)) is through
augmentation of the input space [1  10]  a procedure that introduces precisely those latent (event)
variables that require explicit normalization during variational inference. We will describe below a
process superposition view of this augmented scheme that allows us to deﬁne a proper distribution
over the joint space of observed and latent variables and carry out posterior estimation via variational
inference. By superimposing two PPP with opposite intensities we obtain an homogenous PPP and
thus avoid the integration of the GP over X while reducing the integral in Eq. (1) to the computation

of the measure of the input space(cid:82)

X dx.

2.2 Augmentation via superposition

A very useful property of independent PPPs is that their superposition  which is deﬁned as the
combination of events from two processes in a single one  is a PPP. Consider two PPP with intensities
λ(x) and ν(x) and realisations (N {x1  ...  xn}) and (M {y1  ...  yM}) respectively. The combined
event ξR = (R = M + N {v1  ...  vR}) is a realization of a PPP with intensity given by λ(x) + ν(x)
where knowledge of which points originated from which process is assumed lost. The likelihood for

3

α

β

θ

λ(cid:63)

M

ym

M

u

f
N+M

Zd

K

Figure 1: Plate diagram representing the posterior distribution accounting for all model dependencies.
In our variational posterior (Eq. (6)) we drop the dependency represented by the dashed line.

L(ξR|λ(x)  ν(x)) can be thus written as:

R(cid:88)

(cid:18)N + M

(cid:19) (cid:88)

 exp(−(cid:82)

N =0

N

PN∈PN

X λ(x)dx)
N !

λ(r) × exp(−(cid:82)

X ν(x)dx)
M !

(cid:89)

r∈PN

  

ν(r)

(cid:89)

r∈P c

N

N is its complement.

probability for the event to be latent is σ(−f (x)). In addition  let λ(cid:63)(cid:82)

(2)
where PN denotes the collection of all possible partitions of size N  PN represents an element of PN
and P c
Consider now R = N + M to be the total number of events resulting from thinning [16] where N
is the number of observed events while M is the number of latent events with stochastic locations
y1  ...  yM . We assume that the probability of observing an event is given by σ(f (x)) while the
X dx be the expected total
number of events. We can see the realization (M + N  (x1  ...  xN   y1  ...  yM )) as the result of the
superposition of two PPPs with intensities λ(x) = λ(cid:63)σ(f (x)) and ν(x) = λ(cid:63)σ(−f (x)). Differently
from the standard superposition  we do know which events are observed and which are latent. In
writing the likelihood for (M + N {x1  ...  xN   y1  ...  yM}) we thus do not need to consider all the
possible partitions of N. We can write LN +M

def= L(N + M  (x1  ...  xN   y1  ...  yM )):

(cid:89)

X ν(x)dx)
M !

M(cid:89)

ν(r)

N

r∈P c
σ(−f (ym)).

(3)

(4)

exp(−(cid:82)

LN +M =

X λ(x)dx)
N !

(cid:89)

r∈PN

λ(r) × exp(−(cid:82)
N(cid:89)

(cid:90)

X

=

1

N !M !

exp(−λ(cid:63)

dx)(λ(cid:63))M +N

σ(f (xn))

n=1

m=1

The augmentation via superposition offers a different view on the thinning procedure proposed in
Adams et al. [1]. However  there is a crucial difference between Eq. 4 and the usual likelihood
considered in SGCP [1]. Eq. (4) represents a distribution over tuples and thus  as mentioned above  is
properly normalized. In addition  it makes a distinction between the observed and latent events and it
is thus different from Eq. (1) written for the the tuple (M + N {x1  ...  xN   y1  ...  yM}). We can
write the full joint distribution as L+

def= L({xn}N

(λ(cid:63))N +M exp(−λ(cid:63)(cid:82)

N +M

X dx)

N(cid:89)

m=1  M  f   λ(cid:63)|X   θ):
n=1 {ym}M
M(cid:89)

σ(−f (ym)) × p(f ) × p(λ(cid:63)) 

(5)

L+
N +M =

N !M !

σ(f (xn))

n=1

m=1

where p(f ) def= p(fN +M ) denotes the joint prior at both {xn}N
m=1 and p(λ(cid:63)) denotes
the prior over the upper bound of λ(x). We consider p(λ(cid:63)) = Gamma(a  b) and set a and b so that
λ(cid:63) has mean and standard deviation equal to 2× and 1× the intensity we would expect from an
homogenous PPP on X . Eq. (5) represents the joint distribution for the data and the variables in
the model. Estimating their posterior distributions requires computing the marginal likelihood by
integrating out all variables in Eq. (5). This is generally intractable and in section §3 we perform
inference via a variational approximation which minimises a bound  the so-called evidence lower
bound (ELBO)  to the marginal likelihood.

n=1 and {ym}M

4

Figure 2: Qualitative results on synthetic data. Solid colored lines denote posterior mean intensities
while shaded areas are ± standard deviation.

2.3 Scalability via inducing variables

As in standard GP modulated models  the introduction of a GP prior poses signiﬁcant computational
challenges during posterior estimation as inference is dominated by algebraic operations that are cubic
on the number of observations. In order to make inference scalable  we follow the inducing-variable
approach proposed by [27] and further developed by [3]. To this end  we consider an augmented
prior p(f   u) with K underlying inducing variables denoted by u. The corresponding inducing inputs
are given by the K × D matrix Z. Major computational gains are realized when K (cid:28) N + M.
The augmented prior distributions for the inducing variables and the latent functions are p(u|θ) =
N (0  KZZ) and p(f|u  θ) = N (KXZ(KZZ)−1u  KXX − AKZX) where A = KXZ(KZZ)−1
and X denotes the (N + M ) × D matrix of all events locations {xn  ym}N M
n=1 m=1. KUV is the
covariance matrix obtained by evaluating the covariance function at all pairwise columns of matrices
U and V.

3 Structured Variational Inference in the augmented space

Given the joint distribution in Eq. 5  our goal is to estimate the posterior distribution over all latent
variables given the data. i.e. p(f   u  M {ym}M
m=1  λ(cid:63)|D). This posterior is analytically intractable
and we resort to variational inference [14]. Variational inference entails deﬁning an approximate
posterior q(f   u  M {ym}M
m=1  λ(cid:63)) and optimizing the ELBO with respect to this distribution. In
SGCP  the GP and the latent variables are highly coupled and breaking their dependencies would
lead to poor approximations  especially in high dimensional settings. Fig. 1 shows the structure
of a general posterior distribution for SGCP without any factorisation assumption. We consider an
approximate posterior distribution that takes dependencies into account:

Q(f   u  M {ym}M

m=1  λ(cid:63)) = p(f|u)q({ym}M

m=1|M )q(M|f   λ(cid:63))q(u)q(λ(cid:63))

(6)

M(cid:89)

m=1

s=1

S(cid:88)

With respect to the general posterior distribution  the only factorisation we impose in Eq. (6) is in the
factor q({ym}M

m=1|M ) where we drop the dependency on f  see dashed line in Fig. 1. We set:
πsNT (µs  σ2

q(λ(cid:63)) = Gamma(α  β)

q({ym}M

m=1|M ) =

q(u) = N (m  S)

s ;X )
where NT (·;X ) denotes a truncated Gaussian distribution on X . The factorisation assumption
m=1 can be relaxed by considering a PPP with intensity λ(cid:63)σ(−f (x)) as the
between f and {ym}M
joint variational distribution q(M {ym}M
m=1)  which is indeed the true posterior distributions for the
number of latent events and their locations [8]. Considering a fully structured posterior distribution
signiﬁcantly increases the computational cost of the algorithm as it would require sampling from
the full posterior in the computation of the ELBO. The mixture of truncated Gaussians provides a
ﬂexible and computationally advantageous alternative while satisfying the constraint of being within
the domain of interest.
X σ(−f (x))dx. This is
indeed the true conditional posterior distribution for the number of latent points  see Proposition (3.7)
in [23]. Considering q(M|f   λ(cid:63)) we thus fully account for the dependency structure existing among

More importantly  we assume q(M|f   λ(cid:63)) = Poisson(η) with η = λ(cid:63)(cid:82)

5

+

N(cid:88)
where V =(cid:82)

n=1

(cid:124)
(cid:123)(cid:122)

T3

α
β

M  f and λ(cid:63). Crucially  while in this work we estimate(cid:82)

X σ(−f (x))dx via Monte Carlo  STVB
does not require accurate estimation of this term. Indeed  differently from the competing techniques
[10]  where the algorithm convergence and the posterior q(f ) is directly dependent on numerical
integration  STVB only requires evaluation of the integral during optimisation but q(f ) and thus λ(x)
do not directly depend on its value. In other words  the quality of the posterior intensity does not
depend directly on the accurate estimation of this integral.

3.1 Evidence Lower Bound

Following standard variational inference arguments  it is straightforward to show that the ELBO
decomposes as:
Lelbo = N (ψ(α) − log(β)) − V

− log(N !) + EQ[M log(λ(cid:63))]

− EQ[log(M !)]

α
β

(cid:34) M(cid:88)

m=1

(cid:124)

(cid:123)(cid:122)

T1

(cid:125)
(cid:35)
(cid:125)

(cid:124)

+

(cid:123)(cid:122)
kl − Lλ(cid:63)

(cid:125)
(cid:124)(cid:123)(cid:122)(cid:125)
kl − LM

ent

T2

T4

−L{ym}M

ent

m=1

(cid:124)

(cid:123)(cid:122)

T5

(cid:125)

Eq(f )[log(σ(f (xn)))] + EQ

log(σ(−f (ym)))

−Lu

(7)
X dx  ψ(·) is the digamma function and q(f ) = N (Am  KXX − AKZX + ASAT ) .
The terms denoted by Ti  i = 1  ...  5 cannot be computed analytically. Naïvely  black-box variational
inference algorithms could be used to estimate these terms via Monte Carlo  thus sampling from
the full variational posterior in Eq. (6). This would require sampling f  λ(cid:63)  M and {ym}M
m=1
thus slowing down the algorithm while leading to slow convergence. On the contrary  we exploit
the structure of the model and the approximate posterior to simplify these terms and increase the

algorithm efﬁciency. Denote µ(f ) =(cid:82)

X σ(−f (x))dx  we can write:

T1 = Eq(λ(cid:63))[λ(cid:63) log(λ(cid:63))]Eq(f )[µ(f )]  T5 =

Eq(ym)[log q(ym)]Eq(f )[µ(f )]

T3 =

α
β

Eq(f )[µ(f )]Eq(f )q(ym)[log(σ(−f (ym)))] 

T4 =

α
β

Eq(f )[µ(f ) [log(µ(f )) − 1]] + Eq(λ(cid:63))[λ(cid:63) log(λ(cid:63))]Eq(f )[µ(f )] − EQ[log(M !)].

(8)

(9)

(10)

Notice how the term −EQ[log(M !)] in T4  which would require further approximations  appears
with opposite sign in T2 (Eq. (7)) and thus cancels out in the computation of the ELBO. See the
supplement (§1) for the full derivations.
(8)–(10) give an expression for Lelbo which avoids sampling from q(M|f   λ(cid:63)) and
Eqs.
m=1|M ) and does not require computing the GP on the stochastic locations of the latent
q({ym}M
events. The remaining expectations are with respect to reparameterizable distributions. We thus
avoid the use of other estimators (such as the score function estimators) which would lead to high-
variance gradient estimates. Stochastic optimisation techniques can be used to evaluate T3 and
Eq(f )[log(σ(f (xn)))] in Eq. (7) thus reducing the computational cost by making it indepen-
dent of M and N. This would reduce the computational complexity of the algorithm to O(K 3).
However  when the number of inputs used per mini-batch equals N  the time complexity becomes
O(N K 2). In the following experiments  we show how the proposed structured approach together
with these efﬁcient ELBO computations lead to higher predictive performances and better uncertainty
quantiﬁcation. The presented results do not exploit the computational gains attainable via stochastic
optimisation thus the CPU times and performances are directly comparable across all methods.

(cid:80)N

n=1

4 Experiments

We test our algorithm on three 1D synthetic data settings and on two 2D real-world applications2.

2Code and data for all the experiments is provided at https://github.com/VirgiAgl/STVB.

6

Baselines We compare against alternative inference schemes  different link functions and a different
augmentation scheme. In terms of continuous models  we consider a sampling approach [SGCP 
1]  a Permanental Point process model [VBPP  19] and a mean-ﬁeld approximation based on a
Pólya-Gamma augmentation [MFVB  10]. In addition  we compare against a discrete variational
log-Gaussian Cox process model [LGCP  24]. Details are given in the supplement (§3).
Performance measures We test the algorithms evaluating the l2 norm to the true intensity function
(for the synthetic datasets)  the test log likelihood ((cid:96)test) on the test set and the negative log predicted
likelihood (NLPL) on the training set. In order to assess the model capabilities in terms of uncertainty
quantiﬁcation  we compute the empirical coverage (EC)  i.e. the coverage of the empirical count
distributions obtained by sampling from the posterior intensity. We do that for different credible
intervals (CI) on both the training (in-sample  p(N|D)) and test set (out-of-sample  p(N∗|D)). Details
on the metrics are in the supplement (§2). For the synthetic data experiments  we run the algorithms
with 10 training datasets each including a different PPP realization sampled from the ground truth.
For each different training set  we then evaluate the performance on other 10 unseen realizations
sampled again from the ground truth. We compute the mean and the standard deviation for the
presented metrics averaging across the training and test sets. For the real data settings  we compute
the NLPL and in-sample EC on the observed events. We then test the algorithm computing both (cid:96)test
and out-of-sample EC on the held-out events. In order to compute the out-of-sample EC we rescale
X dx. We then

the intensity function as λtest(x) = λtrain(x) − Ntrain/V + Ntest/V with V =(cid:82)

sample from λtest(x) and generate the predicted count distributions for different seeds.
Synthetic experiments We test our approach using the three toy example proposed by [1]:

x ∈ [0  5] and

x ∈ [0  50] 

• λ1(x) = 2 exp(−1/15) + exp(−[(x − 15)/10]2)
• λ2(x) = 5sin(x2) + 6
• λ3(x) piecewise linear through (0  20)  (25  3) (50  1)  (75  2.5)  (100  3)

x ∈ [0  100].
For LGCP  we discretize the input space considering a grid cell width of one for λ1(x) and λ3(x) and
m=1|M )  we
of 0.5 for λ2(x). For MFVB we consider 1000 integration points. In terms of q({ym}M
set S = 5 but consistent results where found across different values of this parameter. The results
are given in Fig. 2 and Tab. 2  where we see that all algorithms recover similar predicted mean
intensities and give roughly comparable performances across all metrics. Out of all 9 settings and
metrics (top section of Tab. 2) our method (STVB) outperforms competing methods on 3 cases and it
is only second to SGCP on 6 cases. However  the CPU time of SGCP is almost an order of magnitude
larger than ours even in these simple low-dimensional problems. This conﬁrms the beneﬁts of having
structured approximate posteriors within a computationally efﬁcient inference algorithm such as
VI. In terms of uncertainty quantiﬁcation (bottom section of Tab. 2)  our algorithm outperforms all
competing approaches for λ1(x) and λ2(x).

2D real data experiments
In this section we show the performance of the algorithm on two 2D
real-world datasets. In both cases  we assume independent two-dimensional truncated Gaussian
distributions for q({ym}M
m=1|M ) so that they factorize across input dimensions. Qualitative and
quantitative results are given in Fig. 3  Fig. 4 and Tab. 3.
Our ﬁrst dataset is concerned with neuronal data  where event locations correspond to the position of
a mouse moving in an arena when a recorded cell ﬁred [5  26]. We randomly assign the events to
either training (N = 583) or test (N = 29710) and we run the model using a regular grid of 10 × 10
inducing inputs. We see that the intensity function recovered by the three methods vary in terms of
smoothness with MFVB estimating the smoothest λ(x) and VBPP recovering an irregular surface (Fig.
3). MFVB gives slightly better performance in terms of (cid:96)test but our method (STVB) outperforms
competing approaches in terms of NLPL and EC ﬁgures. Remarkably  STVB contains the true number
of test events in the 30% credible intervals for 56% of the simulations from the posterior intensity
(Tab. 3 and Fig. 4).
As a second dataset  we consider the Porto taxi dataset3 which contains the trajectories of 7000
taxi travels in the years 2013/2014 in the city of Porto. As in [10]  we consider the pick-up loca-
tions as observations of a PPP and restrict the analysis to events happening within the coordinates
(41.147 −8.58) and (41.18 −8.65). We select N = 1000 events at random as training set and train

3http://www.geolink.pt/ecmlpkdd2015-challenge/dataset.html.

7

Table 2: Average performances on synthetic data across 10 training and 10 test datasets with standard
errors in brackets. Top: Lower values of l2  NLPL and higher values of (cid:96)test are better. Bottom:
Out-of-sample EC for different CI  higher values are better. Our method denoted by STVB.

l2
3.44
(1.43)
4.56
(1.43)
9.19
(2.32)
4.22
(1.88)
67.76
(24.38)

30% CI
0.81
(0.27)
0.76
(0.25)
0.75
(0.21)
0.39
(0.28)
0.08
(0.12)

STVB

MFVB

VBPP

SGCP

LGCP

STVB

MFVB

VBPP

SGCP

LGCP

λ1(x)
(cid:96)test
-1.39
(1.05)
-2.84
(1.0)
-7.71
(3.31)
-1.39
(1.28)
-5.26
(8.84)
EC–λ1(x)
40% CI
0.72
(0.27)
0.61
(0.28)
0.41
(0.25)
0.27
(0.22)
0.03
(0.09)

λ2(x)
(cid:96)test
56.04
(4.47)
55.35
(4.72)
56.82
(4.42)
55.05
(1.35)
28.56
(6.88)

l2

46.28
(9.95)
44.44
(10.7)
48.15
(13.16)
43.50
(8.69)
106.74
(13.89)

NLPL
5.62
(0.72)
5.52
(1.29)
5.20
(1.33)
3.77
(0.54)
15.75
(3.36)

l2
7.39
(2.76)
8.17
(3.43)
20.54
(6.53)
14.44
(2.97)
19.24
(6.44)

λ3(x)
(cid:96)test
153.98
(11.91)
155.08
(10.20)
152.82
(11.43)
165.66
(2.12)
147.67
(11.76)

NLPL
6.41
(0.64)
5.82
(0.61)
8.35
(2.28)
4.78
(0.33)
10.84
(1.36)

NLPL
4.71
(0.51)
4.74
(0.1)
8.91
(1.19)
4.21
(1.04)
26.26
(8.09)

EC–λ2(x)
40% CI
0.88
(0.23)
0.84
(0.29)
0.45
(0.26)
0.14
(0.05)
0.00
(0.00)

50% CI
0.86
(0.22)
0.82
(0.29)
0.05
(0.05)
0.00
(0.00)
0.00
(0.00)

30% CI
0.91
(0.24)
0.89
(0.23)
0.76
(0.26)
0.64
(0.09)
0.04
(0.08)

EC–λ3(x)
40% CI
0.97
(0.09)
0.91
(0.14)
0.43
(0.14)
0.34
(0.07)
0.99
(0.12)

30% CI
0.99
(0.03)
0.97
(0.09)
0.83
(0.19)
0.49
(0.03)
0.99
(0.00)

50% CI

0.6
(0.34)
0.52
(0.29)
0.04
(0.09)
0.08
(0.12)
0.01
(0.03)

CPU
time (s)

315.59

0.01

0.44

2764.88

4.74

50% CI
0.92
(0.15)
0.78
(0.15)
0.03
(0.05)
0.02
(0.04)
0.95
(0.10)

Table 3: Average performances on real-data experiments with standard errors in brackets. EC is
computed across 100 replications using different seeds. Higher (cid:96)test  EC and lower NLPL are better.
EC ﬁgures are given as In-sample - Out-of-sample.

(cid:96)test[×103]

-84.55
(16.05)
-83.54
(4.60)
-83.89
(12.49)

NLPL
10.10
(7.02)
10.71
(3.39)
11.39
(8.18)

STVB

MFVB

VBPP

Neuronal data

EC-30% CI
1.00-1.00

(0.00)-(0.00)

1.00-0.03

(0.00)-(0.17)

1.00-0.00

(0.00) - (0.00)

EC-40% CI
0.99-0.56

(0.10)-(0.50)

0.78-0.00

(0.41)-(0.00)

0.83-0.00

(0.38)-(0.00)

CPU time (s)

(cid:96)test[×106]

NLPL [×104]

193.07

0.35

26.23

-27.96
(9.16)
-40.8
(6.41)
-31.32
(8.18)

27.96
(9.16)
40.65
(6.41)
31.32
(8.18)

Taxi data
EC-30% CI
0.81-0.37

(0.39)-(0.48)

0.00-0.00

(0.00)-(0.00)

0.98-0.00

(0.14)-(0.00)

EC-40% CI
0.09-0.01

(0.29)-(0.10)

0.00-0.00

(0.00)-(0.00)

0.48-0.00

(0.50)-(0.00)

CPU time (s)

290.34

0.24

3.62

the model with 400 inducing points placed on a regular grid. The test log likelihood is then computed
on the remaining 3401 events. We see that our method (STVB) outperforms competing methods on all
performance metrics (Tab. 3)  recovering an intensity that is smoother than VBPP and captures more
structure compared to MFVB (Fig. 3). In terms of uncertainty quantiﬁcation  the coverage of p(N∗|D)
are the highest for STVB across all CI. Notice how the irregularity of the VBPP intensity leads to good
performance on the training set but results in a p(N∗|D) which is centered on a signiﬁcantly higher
number of test events (Fig. 4). As expected  the SVI approach implies wider counts distributions
compared to the mean ﬁeld approximation. This generally yields better predictive performances in a
variety of settings and especially in higher dimensions.

Figure 3: Real data. Posterior mean intensities and events on the two-dimensional input space.

8

Figure 4: Predicted counts distributions for the training set (p(N|D)) and the test set (p(N∗|D)) on
real data. The gray line denotes the number of observed events. The red bars on the x-axis denote
breaks in the axis due to the different shifts of the distributions.

Table 4: Average performances on the spatio-temporal
Taxi dataset. Standard errors in brackets. EC is com-
puted across 100 replications using different seeds.
Higher (cid:96)test  EC and lower NLPL are better. EC ﬁgures
are given as In-sample - Out-of-sample.

(cid:96)test[×107]

-31.26
(10.88)
-42.97
(9.56)

STVB

VBPP

NLPL[×105]

31.26
(10.88)
42.97
(9.56)

Spatio-temporal Taxi Data

EC-30% CI
1.00-0.00

(0.00)-(0.00)

0.00-0.00

(0.00)-(0.00)

EC-40% CI
0.98-0.00

(0.14)-(0.00)

0.00-0.00

(0.00)-(0.00)

CPU time (s)

1208.00

1.00

Figure 5: Predicted counts distributions for
the training set (p(N|D)) and the test set
(p(N∗|D)).

3D real data experiment Finally  we show the performance of the algorithm on the spatio-temporal
Taxi dataset used above where  for each taxi travel  we consider both the trajectory and the pickup
time in seconds. While VBPP does not currently support D > 2  we found STVB to outperform MFVB
both in terms of performance metrics and uncertainty quantiﬁcation (Tab. 4  Fig. 5).

5 Conclusions and discussion

We have proposed a new variational inference framework for estimating the intensity of a continuous
sigmoidal Cox process. By seeing an augmented input space from a superposition of two PPPs 
we have derived a scalable and computationally efﬁcient structured variational approximation. Our
framework does not require discretization or accurate numerical computation of integrals on the input
space  it is not limited to speciﬁc kernel functions and properly accounts for the strong dependencies
existing across the latent variables. Through extensive empirical evaluation we have shown that
our methods compares favorably against ‘exact’ but computationally costly MCMC schemes  while
being almost an order of magnitude faster. More importantly  our inference scheme outperforms all
competing approaches in terms of uncertainty quantiﬁcation. The beneﬁt of the proposed scheme
and resulting SVI are particularity pronounced on multivariate input settings where accounting for
the highly coupled variables become crucial for interpolation and prediction. Future work will
focus on relaxing the factorization assumption between the GP and the latent points in the posterior.
Introducing a fully structured variational inference would further improve the accuracy performance
of the method but would require further approximations in the variational objective.

Acknowledgments

This work was supported by the EPSRC grant EP/L016710/1  The Alan Turing Institute under EPSRC
grant EP/N510129/1  the Lloyds Register Foundation programme on Data Centric Engineering  the
University of Sydney’s Centre for Translational Data Science and the Australian Research Council
ARC FT140101266.

9

References
[1] Adams  R. P.  Murray  I.  and MacKay  D. J. (2009). Tractable nonparametric Bayesian inference
in Poisson processes with Gaussian process intensities. In Annual International Conference on
Machine Learning  pages 9–16.

[2] Aglietti  V.  Damoulas  T.  and Bonilla  E. V. (2019). Efﬁcient Inference in Multi-task Cox

Process Models. In Artiﬁcial Intelligence and Statistics  pages 537–546.

[3] Bonilla  E. V.  Krauth  K.  and Dezfouli  A. (2019). Generic Inference in Latent Gaussian Process

Models. Journal of Machine Learning Research  20(117):1–63.

[4] Brix  A. and Diggle  P. J. (2001). Spatiotemporal prediction for log-Gaussian Cox processes.

Journal of the Royal Statistical Society: Series B (Statistical Methodology)  63(4):823–841.

[5] Centre For The Biology Of Memory and Sargolini  F. (2014). Grid cell data Sargolini et al. 2006.

[6] Cox  D. R. (1955). Some statistical methods connected with series of events. Journal of the

Royal Statistical Society. Series B (Methodological)  pages 129–164.

[7] Cunningham  J. P.  Shenoy  K. V.  and Sahani  M. (2008). Fast Gaussian Process Methods for
Point Process Intensity Estimation. In International Conference on Machine Learning  pages
192–199.

[8] Daley  D. J. and Vere-Jones  D. (2003). An Introduction to the Theory of Point Processes. Volume

I: Elementary Theory and Methods. Springer Science & Business Media.

[9] Diggle  P. J.  Moraga  P.  Rowlingson  B.  and Taylor  B. M. (2013). Spatial and spatio-temporal
log-Gaussian Cox processes: Extending the geostatistical paradigm. Statistical Science  pages
542–563.

[10] Donner  C. and Opper  M. (2018). Efﬁcient Bayesian Inference of Sigmoidal Gaussian Cox

Processes. In Journal of Machine Learning Research  volume 19  pages 2710–2743.

[11] Grubesic  T. H. and Mack  E. A. (2008). Spatio-temporal interaction of urban crime. Journal of

Quantitative Criminology  24(3):285–306.

[12] Gunter  T.  Lloyd  C.  Osborne  M. A.  and Roberts  S. J. (2014). Efﬁcient Bayesian Nonpara-
metric Modelling of Structured Point Processes. In Uncertainty in Artiﬁcial Intelligence  pages
310–319.

[13] John  S. and Hensman  J. (2018). Large-Scale Cox Process Inference using Variational Fourier

Features. In International Conference on Machine Learning  pages 2362–2370.

[14] Jordan  M. I.  Ghahramani  Z.  Jaakkola  T. S.  and Saul  L. K. (1999). An introduction to

variational methods for graphical models. Machine learning  37(2):183–233.

[15] Lasko  T. A. (2014). Efﬁcient Inference of Gaussian-Process-Modulated Renewal Processes
with Application to Medical Event Data. In Uncertainty in Artiﬁcial Intelligence  pages 469–476.

[16] Lewis  P. W. and Shedler  G. S. (1979). Simulation of nonhomogeneous Poisson processes by

thinning. Naval research logistics quarterly  26(3):403–413.

[17] Lian  W.  Henao  R.  Rao  V.  Lucas  J.  and Carin  L. (2015). A Multitask Point Process

Predictive Model. In International Conference on Machine Learning  pages 2030–2038.

[18] Lloyd  C.  Gunter  T.  Nickson  T.  Osborne  M.  and Roberts  S. J. (2016). Latent Point Process

Allocation. In Artiﬁcial Intelligence and Statistics  pages 389–397.

[19] Lloyd  C.  Gunter  T.  Osborne  M. A.  and Roberts  S. J. (2015). Variational Inference for
Gaussian Process Modulated Poisson Processes. In International Conference on Machine Learning 
pages 1814–1822.

[20] López-Lopera  A. F.  John  S.  and Durrande  N. (2019). Gaussian Process Modulated Cox

Processes under Linear Inequality Constraints. In Artiﬁcial Intelligence and Statistics.

10

[21] Marsan  D. and Lengline  O. (2008). Extending earthquakes’ reach through cascading. Science 

319(5866):1076–1079.

[22] Møller  J.  Syversveen  A. R.  and Waagepetersen  R. P. (1998). Log Gaussian Cox Processes.

Scandinavian journal of statistics  25(3):451–482.

[23] Moller  J. and Waagepetersen  R. P. (2003). Statistical inference and simulation for spatial point

processes. Chapman and Hall/CRC.

[24] Nguyen  T. V. and Bonilla  E. V. (2014). Automated Variational Inference for Gaussian Process

Models. In Neural Information Processing Systems  pages 1404–1412.

[25] Rasmussen  C. E. and Williams  C. K. I. (2005). Gaussian Processes for Machine Learning.

The MIT Press.

[26] Sargolini  F.  Fyhn  M.  Hafting  T.  McNaughton  B. L.  Witter  M. P.  Moser  M.-B.  and Moser 
E. I. (2006). Conjunctive representation of position  direction  and velocity in entorhinal cortex.
Science  312(5774):758–762.

[27] Titsias  M. K. (2009). Variational Learning of Inducing Variables in Sparse Gaussian Processes.

In Artiﬁcial Intelligence and Statistics  pages 567–574.

[28] Walder  C. J. and Bishop  A. N. (2017). Fast Bayesian Intensity Estimation for the Permanental

Process. In International Conference on Machine Learning  pages 3579–3588.

11

,Virginia Aglietti
Edwin Bonilla
Theodoros Damoulas
Sally Cripps