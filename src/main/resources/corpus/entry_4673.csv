2018,Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach,Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically  such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However  this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper  we apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses. This approach is both computationally efficient and general  and is shown numerically to improve convergence across discrete and continuous domains and different reinforcement learning algorithms.,Reinforcement Learning with Multiple Experts:

A Bayesian Model Combination Approach

Michael Gimelfarb

Scott Sanner

Mechanical and Industrial Engineering

Mechanical and Industrial Engineering

University of Toronto

mike.gimelfarb@mail.utoronto.ca

University of Toronto

ssanner@mie.utoronto.ca

Chi-Guhn Lee

Mechanical and Industrial Engineering

University of Toronto

cglee@mie.utoronto.ca

Abstract

Potential based reward shaping is a powerful technique for accelerating convergence
of reinforcement learning algorithms. Typically  such information includes an
estimate of the optimal value function and is often provided by a human expert or
other sources of domain knowledge. However  this information is often biased or
inaccurate and can mislead many reinforcement learning algorithms. In this paper 
we apply Bayesian Model Combination with multiple experts in a way that learns
to trust a good combination of experts as training progresses. This approach is
both computationally efﬁcient and general  and is shown numerically to improve
convergence across discrete and continuous domains and different reinforcement
learning algorithms.

1

Introduction

Potential-based reward shaping incorporates prior domain knowledge in the form of additional
rewards provided during training to speed up convergence of reinforcement learning algorithms 
without changing the optimal policies (Ng et al. [1999]). While much of the existing theory and
applications assume that advice comes from a single source throughout training (Grze´s [2017] 
Harutyunyan et al. [2015]  Tenorio-Gonzalez et al. [2010])  there is much less work done on learning
from multiple sources of advice as training progresses. One reason for doing so is that expert
demonstrations or advice can often be biased or incomplete  so being able to identify good advice
from bad is critical to guarantee robustness of convergence.
In this paper  the decision maker is presented with multiple sources of expert advice in the form of
potential-based reward functions  some of which can be misleading and should not be trusted. The
decision maker does not know a priori which expert(s) to trust  but rather learns this from experience
in a Bayesian framework. More speciﬁcally  the decision maker starts with a prior distribution over
the probability simplex  and updates the belief to a posterior distribution as new training rewards are
observed. Because our proposed algorithm follows the potential-based reward shaping framework  it
preserves the theoretical guarantees for policy invariance established in Ng et al. [1999].
This paper proceeds as follows. Section 2 introduces the key deﬁnitions used throughout the paper.
In Section 3  we apply Bayesian model combination  that allows the decision maker to asymptotically
learn the best combination of experts  all with reduced variance as compared to similar approaches. In
Section 3.1  we show that the total return can be written as a linear combination of individual return

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

contributions from each expert  weighted by the expected posterior belief that the expert is correct. In
Section 3.2  we show that the exact posterior updates are analytically intractable. Instead  we apply
moment matching to project the true posterior distribution onto the multivariate Dirichlet distribution 
and show how accurate approximation and inference can be done in linear time in the number of
experts. In Section 3.3 we then show how our approach can be incorporated into any reinforcement
learning algorithm  preserving the asymptotically optimal policy without incurring additional runtime
complexity. Finally  in Section 4  we demonstrate the effectiveness of this approach across various
reinforcement learning methods and problem domains.

Related Work

Learning from expert knowledge is not new. In transfer learning  for example  the decision maker
uses prior knowledge obtained from training on task(s) to improve performance on future tasks
(Konidaris and Barto [2006]). In inverse reinforcement learning  the agent recovers an unknown
reward function that can then be used for shaping (Suay et al. [2016]). In many cases  a human
expert can directly provide the learning agent with training examples or preferences before or during
training to guide exploration (Brys et al. [2015]  Christiano et al. [2017]). All of these approaches
try to perturb the intermediate value functions to encourage more guided exploration of the state
space. A somewhat different approach  called policy shaping  instead reshapes the learned policies
(Grifﬁth et al. [2013]). Grzes and Kudenko [2009] and Grze´s and Kudenko [2010] recently introduced
on-line methods to learn a reward shaping function  but only for model-based learning using R-Max
or model-free learning with multi-grid discretization. Our approach can work in on-line settings  with
general algorithms under minimal assumptions  and with value function approximation.
The idea of combining multiple models/experts or learning algorithms to improve performance is
central to ensemble learning (Dietterich [2000])  and has been applied in a variety of ways in the RL
literature. For example  Maclin et al. [2005] used kernel regression  Philipp and Rettinger [2017]
used contextual bandits  and Downey and Sanner [2010] applied Bayesian model averaging. Asmuth
et al. [2009] applied a Bayesian method to sample multiple models for action selection. The only
work we are aware of that incorporated reward shaping advice in a Bayesian learning framework
is the recent paper by Marom and Rosman [2018]. However  that paper exploited the structure
of the transition model (belief clusters) in order to do efﬁcient Bayesian inference  whereas our
paper focuses on posterior approximation using variational ideas  and their analysis and results are
considerably different from ours. More generally  Bayesian approaches have many advantages over
frequentist approaches  including prior speciﬁcation  clear and intuitive interpretation  ability to test
hypotheses (O’Hagan [2004])  and theoretically optimal exploration (Thompson [1933]).

2 Deﬁnitions

2.1 Markov Decision Process

The decision-making framework used throughout this paper focuses on the Markov decision process
(MDP) (Bertsekas and Shreve [2004]). Formally  an MDP is deﬁned as a tuple (S A  T  R  γ)  where:
S is a general set of states  A is a ﬁnite set of actions  T : S ×A×S → R+ is a stationary Markovian
transition function  where T (s  a  s(cid:48)) = P (s(cid:48)|s  a)  the probability of transitioning to state s(cid:48) after
taking action a in state s  R : S × A × S → R is a bounded reward function  where R(s  a  s(cid:48)) is the
immediate reward received upon transitioning to state s(cid:48) after taking action a in state s  and γ ∈ [0  1]
is a discount factor.
We deﬁne a random policy µ as a probability distribution µ(s  a) = P (a|s) over actions A given
current state s. Given an MDP (S A  T  R  γ)  a policy µ  and initial state-action pair (s  a)  we
deﬁne the inﬁnite-horizon expected discounted rewards as

(cid:34) ∞(cid:88)

t=0

(cid:35)
(cid:12)(cid:12)(cid:12)s0 = s  a0 = a

 

Qµ(s  a) = E

γtR(st  at  st+1)

(1)

where at ∼ P (·|st) = µ(st ·) and st+1 ∼ T (st  at ·). The objective of the agent is to ﬁnd an
optimal policy µ∗ that maximizes (1). When the transition and reward functions are known  the
existence of an optimal deterministic stationary policy is guaranteed  in which case value iteration or
policy iteration can be used to ﬁnd an optimal policy (Bertsekas and Shreve [2004]).

2

2.2 Reinforcement Learning

In the reinforcement learning (RL) setting  the transition probabilities and reward function are not
explicitly known to the agent but rather learned from experience. In order to learn the optimal
policies in this framework and to facilitate the development of the paper  we follow generalized policy
iteration (GPI) (Sutton and Barto [2018]). However  the Bayesian framework developed in this paper
is dependent on neither the exploration policy used nor the value function representation  and can be
applied with on-policy and off-policy learning  value function approximation  traces  deep RL (Li
[2017])  and other approaches.
More speciﬁcally  GPI performs two steps in alternation: a policy evaluation step that estimates
the value Qµi of the current policy µi  and a policy improvement step that uses Qµi to construct
a new policy µi+1. In practice  these two steps are often interleaved. A simple yet effective way
to implement GPI is to follow the ε-greedy policy  that encourages exploration by randomly and
uniformly selecting an action in A at time t with probability εt  and otherwise selects the best action
based on Qµi(s  a); the parameter εt ∈ (0  1)  t ≥ 0 controls the trade-off between exploration and
exploitation.
In order to estimate the value of policy µ = µi  we follow the temporal difference learning (TD)
approach. Speciﬁcally  given a new estimate of the expected future returns Rt at time t after taking
action at in state st according to some policy µ  Q(st  at) (dropping the dependence on µ) is updated
as follows

Qt+1(st  at) = Qt(st  at) + α [Rt − Qt(st  at)]  

(2)

where α > 0 is a problem-dependent learning rate parameter.
Two popular approaches for estimating Rt are Q-learning and SARSA  given respectively as

Rt = rt + γ max

a(cid:48)∈A Qt(st+1  a(cid:48))

Rt = rt + γQt(st+1  at+1) 

(3)

where rt = R(st  at  st+1) is the immediate reward  st+1 ∼ T (st  at ·) and at+1 ∼ µ(st+1 ·).
While both approaches compute Rt by bootstrapping from current Q-values  the key distinction
between them is that SARSA is an on-policy algorithm whereas Q-learning is off-policy. n-step TD
and TD(λ) are more sophisticated examples of TD-learning algorithms (Sutton and Barto [2018]).

2.3 Potential-Based Reward Shaping

In many domains  particularly when rewards are sparse and the agent cannot learn quickly  it is
necessary to incorporate prior knowledge in order for TD-learning to converge faster. The idea
of reward shaping is to incorporate prior knowledge about the domain in the form of additional
rewards during training to speed up convergence towards the optimal policy. Formally  given an
MDP (S A  T  R  γ) and a reward shaping function F : S × A × S → R  we solve the MDP
(S A  T  R(cid:48)  γ) with reward function R(cid:48) given by

R(cid:48)(s  a  s(cid:48)) = R(s  a  s(cid:48)) + F (s  a  s(cid:48)).

(4)

While this approach has been applied successfully to many problems  an improper choice of shaping
function can change the optimal policy (Randløv and Alstrøm [1998]).
In order to address this problem  potential-based reward shaping was proposed  in which F is
restricted to functions of the form

F (s  a  s(cid:48)) = γΦ(s(cid:48)) − Φ(s) 

(5)
where Φ : S → R is called the potential function. It has been shown that this is the only class of
reward shaping functions that preserves policy optimality (Ng et al. [1999]). Reward shaping has
also been shown to be equivalent to Q-value initialization (Wiewiora [2003]). More recently  policy
invariance has been extended for non-stationary time-dependent potential functions of the form

F (s  a  t  s(cid:48)  t(cid:48)) = γΦ(s(cid:48)  t(cid:48)) − Φ(s  t)

(6)

(Devlin and Kudenko [2012])  for action-dependence (Wiewiora et al. [2003])  as well as for partially-
observed (Eck et al. [2013]) and multi-agent systems (Devlin and Kudenko [2011]).

3

3 Bayesian Reward Shaping
The decision maker is presented with advice from N ≥ 1 experts in the form of potential functions
Φ1  Φ2  . . . ΦN . The advice could come from heuristics or guesses (Harutyunyan et al. [2015])  from
similar solved tasks (Taylor and Stone [2009])  from demonstrations (Brys et al. [2015])  and in
general can be analytic or computational. One concrete example that our proposed setup can be
applied to is transfer learning (Taylor and Stone [2009]). Here  models are ﬁrst trained on a number
of tasks to obtain corresponding value functions. By deﬁning suitable inter-task mappings (Taylor
et al. [2007])  these value functions can be incorporated into a target task as reward shaping advice.
Unfortunately in practice  the advice available to the learning agent is often contradictory or contains
numerical errors  in which case it could hurt convergence. In order to make optimal use of the expert
advice during the learning process  the agent should ideally learn which expert(s) to trust as more
information becomes available  and act on this knowledge by applying the techniques in Section 2.2.
To do this  the agent assigns weights w to the experts and updates them on-line during training.
The two main approaches to incorporating multiple models in a Bayesian framework are Bayesian
model averaging (BMA) and Bayesian model combination (BMC). Roughly speaking  taking experts
as hypotheses  BMA converges asymptotically toward the optimal hypothesis  while BMC converges
toward the optimal ensemble. The model combination approach has two clear advantages over
model averaging: (1) when two or more potential functions are optimal  it will converge to a linear
combination of them  and (2) it provides an estimator with reduced variance (Minka [2000]). In this
section  we show how BMC can be used to incorporate imperfect advice from multiple experts into
reinforcement learning problems  all with the same space and time complexity as TD-learning.

3.1 Bayesian Model Combination

(cid:110)
w ∈ RN :(cid:80)N

(cid:111)
i=1 wi = 1  wi ≥ 0

In the general setting of Bayesian model combination  we interpret Q-values for each state-action
pair qs a as random variables  and maintain a set of past return observations D and a multi-
variate posterior probability distribution P (q|D) over Q-values. We also maintain a posterior
probability distribution π : S N−1 → R+ over the (N − 1)-dimensional probability simplex
S N−1 =
. Here  weight vectors w are interpreted as cate-
gorical distributions over experts; such a mechanism will allow us to learn the optimal distribution
over experts  rather than a single expert. In the following subsections  we show how to maintain each
of these distributions over time  but here we show how to use them for the general RL problem.
Given a state s = st and action a = at at time t  the return under model combination ρt(s  a) is

ρt(s  a) = E [qs a|D] =

q P (q|D) dq

P (q|D  w) P (w|D) dw dq =

P (q|i)wiπt(w) dw dq

(cid:90)

R

(cid:90)

R

(cid:90)

SN−1

q P (q|i)

(cid:90)
(cid:90)

q

R

R

(cid:90)
N(cid:88)
N(cid:88)

i=1

i=1

=

=

=

(cid:90)
(cid:90)

q

(cid:90)
N(cid:88)

R

i=1

R

N(cid:88)

i=1

SN−1
q P (q|i) Eπt [wi] dq

(7)

(8)

(9)

wiπt(w) dw dq =

SN−1

N(cid:88)

i=1

Eπt [wi]

q P (q|i) dq =

Eπt [wi] E [qs a|i] 

where: the ﬁrst equality in (7) follows from the law of total probability applied to P (q|D)  whereas
the second equality follows from conditioning on the expert i ∈ {1  2 . . . N}  using the facts that qs a
is independent of w given i and P (i|w) = wi; the ﬁrst equality in (8) follows from interchange of
summation and integration  while the second from the deﬁnition of expectation over wi; ﬁnally  (9)
follows from the deﬁnition of expectation of qs a given i.
This result is intuitively and computationally pleasing  and shows that the total return can be written
as a linear combination of individual return “contributions" from each expert model  weighted by the
expected posterior belief that the expert is correct. We now show how each of these two expectations
can be computed.

4

3.2 Posterior Approximation using Moment Matching
Starting with prior distribution πt at time t over the simplex S N−1  and given new data point d  we
would like to perform a posterior update using Bayes’ theorem

πt+1(w) = P (w|D  d) ∝ P (d|w)πt(w) ∝ N(cid:88)

P (d|i) P (i|w)πt(w)

i=1

eiwiπt(w) 

(10)

where we denote evidence ei = P (d|i)  and Ct+1 is the normalizing constant for πt+1 determined as

Ct+1 =

eiwiπt(w) dw =

ei

wiπt(w) dw =

ei Eπt [wi].

(11)

(cid:90)

N(cid:88)

i=1

SN−1

N(cid:88)

i=1

Unfortunately the exact posterior update is computationally intractable for general evidence ei  and
so an approximate posterior update is required.
Assumed density ﬁltering  or moment matching  projects the true posterior distribution πt+1 onto an
exponential subfamily of proposal distributions by minimizing the KL-divergence between πt+1 and
the proposal distribution. We note that an excellent exponential family proposal distribution for our
posterior in (10) is the multivariate Dirichlet distribution with parameters α ∈ RN
+   density function

N(cid:88)

i=1

=

1

Ct+1

(cid:90)

N(cid:88)

SN−1

i=1

(cid:17)

Γ

i=1 αi

i=1 Γ(αi)

(cid:16)(cid:80)N
(cid:81)N
(cid:16)(cid:80)N
(cid:16)(cid:80)N

Γ

i=1 αi

N(cid:89)
(cid:17)

i=1

i=1(αi + ni)

(cid:17) N(cid:89)

i=1

f (w; α) =

(cid:34) N(cid:89)

i=1

(cid:35)

wni
i

=

Γ

Ef

wαi−1

i

  w ∈ S N−1 

Γ(αi + ni)

Γ(αi)

  ni ≥ 0.

and generalized moments

(12)

(13)

For the exponential family of proposal distributions  exact moment matching requires the moments
over the sufﬁcient statistics. Since this is not available for the Dirichlet family in closed form  it
necessitates an iterative approach that is not computationally feasible in on-line RL. Instead  we
follow Hsu and Poupart [2016] and Omar [2016] by matching the moments (13)  leading to an
efﬁcient closed-form O(N ) time update.
In particular  given means m1  m2 . . . mN−1 of marginals w1  w2 . . . wN−1 of πt+1 and second
moment s1 of w1  we apply approximate moment matching with proposal Dir(α) by solving the
system of equations

mi =

s1 =

  i = 1  2 . . . N − 1

αi
α0
α1(α1 + 1)
α0(α0 + 1)

(14)

(15)

where α0 =(cid:80)N

i=1 αi > 0. Please note that the second moment condition (15) is necessary here 
since without it the system is under-determined. Also  we could use any of s2  s3  . . . sN in place of
s1; in our experiments  we use the value of si which results in the largest value of si − m2
i to avoid
underﬂow in the solution. The unique positive solution of (14) and (15) is

α0 =

m1 − s1
s1 − m2

1

αi = miα0 = mi

(cid:18) m1 − s1

s1 − m2

1

(cid:19)

  i = 1  2 . . . N − 1.

(16)

In order to apply the moment matching solution (16) to approximate the posterior update (10)  it
remains to compute the moments m1  m2  . . . mN−1 and s1 of πt+1.

5

we obtain Ct+1 =(cid:80)N

We proceed by induction on t. More speciﬁcally  we assume that the prior π0 = Dir(α0) was chosen
arbitrarily and that the projection Dir(αt) of πt was already obtained. Given new evidence e ∈ RN
+  
i=1 αt i > 0. Using

i=1 ei Eπt [wi] =(cid:80)N

where αt 0 =(cid:80)N

= e·αt

i=1 ei

αt 0

(10) and (13) 

ej

(cid:90)
(cid:90)
N(cid:88)
ei Eπt
ei

j=1

αt i
αt 0

N(cid:88)

j=1

(cid:88)

j(cid:54)=i

mi = Eπt+1 [wi] =

αt 0
e · αt

SN−1

ejwjwiπt(w) dw

N(cid:88)

j=1

αt 0
e · αt



ej Eπt [wiwj]



ej

αt iαt j

αt 0(αt 0 + 1)

=

αt 0
e · αt

=

αt 0
e · αt

wjwiπt(w) dw =

SN−1

(cid:2)w2

(cid:3) +

i

(cid:88)

j(cid:54)=i

ej Eπt [wiwj]

αt i(αt i + 1)
αt 0(αt 0 + 1)

+

=

=

αt 0
e · αt
αt i(ei + e · αt)
(e · αt)(αt 0 + 1)

.

(17)

(18)

Using the same technique  we can readily obtain the corresponding formula for s1 

s1 =

αt 1(αt 1 + 1)(2e1 + e · αt)
(e · αt)(αt 0 + 1)(αt 0 + 2)

.

Combining (17) and (18) with the general solution to the moment matching problem (16) yields
the new projected posterior Dir(αt+1). This leads to a very efﬁcient O(N ) algorithm for posterior
updates given in Algorithm 1.

Algorithm 1 PosteriorUpdate(αt  e)
1: for i = 1  2 . . . N − 1 do
mi ← αt i(ei+e·αt)
2:
(e·αt)(αt 0+1)
3: s1 ← αt 1(αt 1+1)(2e1+e·αt)
(e·αt)(αt 0+1)(αt 0+2)
4: αt+1 0 ← m1−s1
s1−m2
5: for i = 1  2 . . . N − 1 do
αt+1 i ← miαt+1 0
6:

7: αt+1 N ← αt+1 0 −(cid:80)N−1

1

i=1 αt+1 i

8: return αt+1

(cid:46) Compute posterior moments

(cid:46) Compute αt+1

Finally  once we have obtained αt  we can compute Eπt [wi] = αt i
show how to compute E [qs a|i] and evidence e.

αt 0

= αt i(cid:80)N

j=1 αt j

. It remains only to

3.3 Algorithm

Following the Bayesian Q-learning framework (Dearden et al. [1998])  we model Q-values for each
state-action pair as independent Gaussian distributed random variables. Since the best choice of Φ
should be the optimal value function V ∗  we model Q-values qs a given the best expert Φi as

qs a|i ∼ N(cid:0)Φi(s)  (σi

s a)2(cid:1)  

(19)

where i ∈ {1  2 . . . N}. Since (σi
s a)2.
However  maintaining an estimate for each expert and state-action pair would not be practical for
large spaces  so we follow Downey and Sanner [2010] and replace (σi
s a)2 by the sample variance ˆσ2
of D. This permits constant-time updates per sample without any additional memory overhead  and
this worked very well in our experiments.

s a)2 is not known  we need to maintain an estimator of (σi

6

Using these observations and the approximation πt = Dir(αt) from Section 3.2  (9) reduces to

N(cid:88)

i=1

(cid:80)N
(cid:80)N

i=1 Φi(s)αt i

i=1 αt i

ρt(s  a) =

E [qs a|i] Eπt [wi] =

 

(20)

and deﬁnes the reward shaping potential function ˆΦ used during training. Finally  given a return
observation d ∈ D in state s  the evidence ei for each i ∈ {1  2 . . . N} is computed simply from the

Gaussian probability distribution N(cid:0)Φi(s)  ˆσ2(cid:1) in (19).

We note that all steps can be performed efﬁciently on-line and so this approach does not require
storing D explicitly. Furthermore  it can be easily incorporated into general reinforcement learning
algorithms without increasing the runtime complexity. Perhaps most importantly  since ρt in (20) is a
potential-based reward shaping function  it would not change the asymptotically optimal policy. The
complete algorithm is summarized in Algorithm 2. Here  TrainRL(F ) is a general procedure for
training on one state-action-reward sequence using the immediate reward function R + F .

+

Algorithm 2 RL with Bayesian Reward Shaping
1: initialize α ∈ RN
2: for episode = 0  1 . . . M do
3:

ˆΦ ← (cid:80)N
(cid:80)N
F (s  a  s(cid:48)) ← γ ˆΦ(s(cid:48)) − ˆΦ(s)
(Rt  st)t=1...T ← TrainRL(F )
for all (Rt  st) do

i=1 Φiαi

i=1 αi

update ˆσ2 and compute e
α ← PosteriorUpdate(α  e)

4:
5:
6:
7:
8:

(cid:46) Main loop
(cid:46) Pool experts and compute shaped reward

(cid:46) Perform one episode of training
(cid:46) Posterior update

Remarks: Steps 3 and 4 in Algorithm 2 update the advice off-line on a sequence of cached observations.
It is possible to make this algorithm on-line by performing steps 3 and 4 after each observation  but
care must be taken to ensure consistency of the optimal policies (Devlin and Kudenko [2012]).

4 Experimental Results

In order to validate the effectiveness of our proposed algorithm  we apply it to a Gridworld problem
with subgoals and the classical CartPole problem. We implement the exact tabular Q-learning and
SARSA algorithms (2) and the off-policy Deep Q-Learning algorithm with experience replay (Mnih
et al. [2013]). In all cases  we followed ε-greedy policies introduced in Section 2.2  and manually
selected parameters that worked well for all experts. Policies are learned from scratch  with table
entries initialized to zero and neural networks initialized randomly.

4.1 Gridworld

This is the 5-by-5 navigation problem with subgoals introduced in Ng et al. [1999]. We charge +1
points for every move  and one additional point whenever it is invalid (e.g. choosing “UP" when
adjacent to the top edge  or an attempt is made to collect a ﬂag in an incorrect order) to encourage the
agent to choose valid moves. For all algorithms  we set the length of each episode to T = 200 steps 
γ = 1.0  and εt = 0.98t  where t ≥ 0 is the episode.
In the tabular case  we set α = 0.4 for Q-learning and α = 0.36 for SARSA. The DQN is a dense
network with encoded state s as inputs and action-values {Q(s  a) : a ∈ A} as outputs  and two
fully-connected hidden layers with 25 neurons per layer. We use one-hot encoding for states (see 
e.g. Lantz [2013]). Hidden neurons use leaky ReLU activations and outputs use linear to allow
unbounded values. The learning rate is ﬁxed at 0.001 throughout training that is done on-line using
the Adam optimizer in batches of size 16 sampled randomly from memory of size 10000 (we found
that doing 5 epochs of training per batch led to more stable convergence).
We consider the following ﬁve experts in our analysis: Φopt(s) = V ∗(s) the optimal value function 
Φgood(x  y  c) = −22(5 − c − 0.5)/5 is reasoned in Ng et al. [1999] assuming equidistant subgoals 
Φzero(x  y  c) = 0  Φrand(x  y  c) = U where U ∼ U [−20  20]  and Φneg(s) = −V ∗(s).

7

Figure 1: Test performance (number of steps required to reach the ﬁnal goal) of the learned policy on
Gridworld for each potential versus the number of training episodes  averaged over 100 independent
runs of tabular Q/SARSA and 20 runs of DQN. BMC corresponds to Algorithm 2 applied to all
potential functions.

Figure 2: Test performance (number of steps that the pole was balanced) of the learned policy on
CartPole for each potential versus the number of training episodes  averaged over 100 runs of tabular
Q/SARSA and 20 of DQN.

4.2 CartPole

This is a classical control problem described in Geva and Sitte [1993] and implemented in OpenAI
Gym (Brockman et al. [2016]). In order to encourage the agent to balance the pole  we provide a
reward of +1 at every step as long as the pole is upright. We set T = 500 frames  γ = 0.95  and
εt = 0.98t. Finally  to prevent over-ﬁtting  we stop training whenever the score attained on each of
the last 5 episodes is 500.
In both tabular cases  we set αt = max{0.01  1
2 0.99t} and εt = max{0.01  0.98t}  where t ≥ 0 is
the episode. States (x  θ  ˙x  ˙θ) are discretized into 3  3  6 and 3 bins  respectively  for a total of 162
states. The neural network takes continuous inputs in R4 and has two fully-connected hidden layers
with 12 neurons in each. Once again we use ReLU activations for hidden neurons and linear for
output neurons. We set the learning rate to 0.0005 and train using the Adam optimizer. To further
prevent over-ﬁtting  we train off-line at the end of each episode on 100 batches of size 32 and use L2
regularization with penalty 1E-6.
We consider the following ﬁve experts: Φguess(s) = 20(1 − |θ|
0.2618 ) assigns a reward based on
the proximity of the pole angle to the vertical  Φnet is a pre-trained neural network with two
hidden layers with 6 neurons per layer  Φzero(s) = 0  Φrand(s) = U where U ∼ U [−20  20]  and
Φneg(s) = −Φnet(s).

4.3 Summary

The performance obtained from each expert and the model combination approach are illustrated in
Figure 1 for Gridworld and 2 for CartPole  and the learned expert weights are illustrated in Figure 3.

8

No.Trials050100150200250300350StepstoGoal050100150200250Steps to Goal for Gridworld using Tabular QBMCOpt.GoodZeroRand.Neg.No.Trials0100200300400StepstoGoal050100150200250Steps to Goal for Gridworld using SarsaBMCOpt.GoodZeroRand.Neg.No.Trials0255075100125StepstoGoal050100150200250Steps to Goal for Gridworld using DQNBMCOpt.GoodZeroRand.Neg.No.Trials050100150200250300StepsBalanced0100200300400500600Steps Balanced for CartPole using QBMCNetGuessZeroRand.Neg.No.Trials050100150200250300StepsBalanced0100200300400500600Steps Balanced for CartPole using SarsaBMCNetGuessZeroRand.Neg.No.Trials050100150200StepsBalanced0100200300400500600Steps Balanced for CartPole using DQNBMCNetGuessZeroRand.Neg.Figure 3: Posterior weights assigned to each potential as a function of the number of episodes of
training  averaged over 100 independent runs using tabular Q and SARSA and 20 runs using DQN.

In Gridworld  it is interesting to see that Φgood and Φopt are quantitatively very similar  yet both have
similar effects on the rate of convergence in the tabular case  whereas Φopt considerably outperforms
Φgood in the deep learning case. As shown in Figure 3  our algorithm assigns most of its weight to
Φopt and results in near-optimal performance in all three cases.
In CartPole  it is not immediately clear that Φguess is better than Φnet  since both should be very
close to V ∗. However  Φnet is both a biased estimate of V ∗ and noisy (due the inexactness of gradient
descent)  whereas the simple expert Φguess is highly related to the goal (keeping the pole centered).
Furthermore  Φnet is even less accurate in the tabular case due to state discretization. Once again 
Figure 3 clearly shows that our approach can handle both analytic and computational advice and is
sensitive to approximation error and noise.

5 Conclusion and Future Work

In this paper  the decision maker is presented with multiple sources of expert advice in the form of
potential-based reward functions  some of which can be misleading and should not be trusted. We
assumed that the decision makes does not know a priori which expert(s) to trust  but rather learns
this from experience in a Bayesian framework. More speciﬁcally  we followed the Bayesian model
combination approach and assigned posterior probabilities to distributions over experts. We showed
that the total expected return is a linear combination of individual expert predictions  weighted by the
posterior beliefs assigned to them. We solved the issue of tractability by projecting the true posterior
distribution onto the Dirichlet family using moment matching  and then specialized our analysis to
Bayesian Q-learning. Our approach followed the potential-based reward shaping framework and
does not change the optimal policies. Finally we showed that our proposed method accelerated the
learning phase when solving discrete and continuous domains using different learning algorithms.
Further extensions and generalizations of this work could include rigorous theoretical analysis of
posterior convergence under certain conditions on the reward shaping functions. It is also possible
to extend our analysis to state/action-dependent weightings of experts  at the cost of higher space
complexity; this could be useful in situations where the most suitable potential function changes in
different regions of the state space. It also remains to scale our work to large-scale and real-world
problems where imperfect advice and issues in convergence could be more prevalent.

Acknowledgments

We would like to thank the NeurIPS reviewers for their feedback  that signiﬁcantly improved this
paper.

References
J. Asmuth  L. Li  M. L. Littman  A. Nouri  and D. Wingate. A bayesian sampling approach to exploration in

reinforcement learning. In UAI  pages 19–26. AUAI Press  2009.

D. P. Bertsekas and S. Shreve. Stochastic optimal control: the discrete-time case. Athena Scientiﬁc  2004.

G. Brockman  V. Cheung  L. Pettersson  J. Schneider  J. Schulman  J. Tang  and W. Zaremba. Openai gym.

arXiv preprint arXiv:1606.01540  2016.

T. Brys  A. Harutyunyan  H. B. Suay  S. Chernova  M. E. Taylor  and A. Nowé. Reinforcement learning from

demonstration through shaping. In IJCAI  pages 3352–3358  2015.

9

No.Trials0255075100Probability00.51Posterior Weightsfor Gridworld using QOpt.GoodZeroRand.Neg.No.Trials0255075100Probability00.51Posterior Weights forGridworld using SarsaOpt.GoodZeroRand.Neg.No.Trials0255075100Probability00.51Posterior Weightsfor Gridworld using DQNOpt.GoodZeroRand.Neg.No.Trials0255075100Probability00.51Posterior Weightsfor CartPole using QNetGuessZeroRand.Neg.No.Trials0255075100Probability00.51Posterior Weights forCartPole using SarsaNetGuessZeroRand.Neg.No.Trials0255075100Probability00.51Posterior Weightsfor CartPole using DQNNetGuessZeroRand.Neg.P. F. Christiano  J. Leike  T. Brown  M. Martic  S. Legg  and D. Amodei. Deep reinforcement learning from

human preferences. In NeurIPS  pages 4302–4310  2017.

R. Dearden  N. Friedman  and S. Russell. Bayesian q-learning. In AAAI/IAAI  pages 761–768  1998.

S. Devlin and D. Kudenko. Theoretical considerations of potential-based reward shaping for multi-agent systems.
In AAMAS  pages 225–232. International Foundation for Autonomous Agents and Multiagent Systems  2011.

S. Devlin and D. Kudenko. Dynamic potential-based reward shaping. In AAMAS  pages 433–440. International

Foundation for Autonomous Agents and Multiagent Systems  2012.

T. G. Dietterich. Ensemble methods in machine learning. In International workshop on multiple classiﬁer

systems  pages 1–15. Springer  2000.

C. Downey and S. Sanner. Temporal difference bayesian model averaging: A bayesian perspective on adapting

lambda. In ICML  pages 311–318  2010.

A. Eck  L.-K. Soh  S. Devlin  and D. Kudenko. Potential-based reward shaping for pomdps. In AAMAS  pages

1123–1124. International Foundation for Autonomous Agents and Multiagent Systems  2013.

S. Geva and J. Sitte. A cartpole experiment benchmark for trainable controllers. IEEE Control Systems  13(5):

40–51  1993.

S. Grifﬁth  K. Subramanian  J. Scholz  C. L. Isbell  and A. L. Thomaz. Policy shaping: Integrating human

feedback with reinforcement learning. In NeurIPS  pages 2625–2633  2013.

M. Grze´s. Reward shaping in episodic reinforcement learning. In AAMAS  pages 565–573. International

Foundation for Autonomous Agents and Multiagent Systems  2017.

M. Grzes and D. Kudenko. Learning shaping rewards in model-based reinforcement learning. In Proc. AAMAS

2009 Workshop on Adaptive Learning Agents  volume 115  2009.

M. Grze´s and D. Kudenko. Online learning of shaping rewards in reinforcement learning. Neural Networks 
23(4):541 – 550  2010. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2010.01.001. The 18th
International Conference on Artiﬁcial Neural Networks  ICANN 2008.

A. Harutyunyan  T. Brys  P. Vrancx  and A. Nowé. Shaping mario with human advice. In AAMAS  pages

1913–1914. International Foundation for Autonomous Agents and Multiagent Systems  2015.

W.-S. Hsu and P. Poupart. Online bayesian moment matching for topic modeling with unknown number of

topics. In NeurIPS  pages 4536–4544  2016.

G. Konidaris and A. Barto. Autonomous shaping: Knowledge transfer in reinforcement learning. In ICML 

pages 489–496. ACM  2006.

B. Lantz. Machine learning with R. Packt Publishing Ltd  2013.

Y. Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274  2017.

R. Maclin  J. Shavlik  L. Torrey  T. Walker  and E. Wild. Giving advice about preferred actions to reinforcement

learners via knowledge-based kernel regression. In AAAI  pages 819–824  2005.

O. Marom and B. S. Rosman. Belief reward shaping in reinforcement learning. In AAAI  2018.

T. P. Minka. Bayesian model averaging is not model combination. Available electronically at http://www. stat.

cmu. edu/minka/papers/bma. html  pages 1–2  2000.

V. Mnih  K. Kavukcuoglu  D. Silver  A. Graves  I. Antonoglou  D. Wierstra  and M. Riedmiller. Playing atari

with deep reinforcement learning. arXiv preprint arXiv:1312.5602  2013.

A. Y. Ng  D. Harada  and S. Russell. Policy invariance under reward transformations: Theory and application to

reward shaping. In ICML  volume 99  pages 278–287  1999.

F. Omar. Online bayesian learning in probabilistic graphical models using moment matching with applications.

2016.

A. O’Hagan. Bayesian statistics: principles and beneﬁts. Frontis  pages 31–45  2004.

P. Philipp and A. Rettinger. Reinforcement learning for multi-step expert advice. In AAMAS  pages 962–971.

International Foundation for Autonomous Agents and Multiagent Systems  2017.

10

J. Randløv and P. Alstrøm. Learning to drive a bicycle using reinforcement learning and shaping. In ICML 

volume 98  pages 463–471  1998.

H. B. Suay  T. Brys  M. E. Taylor  and S. Chernova. Learning from demonstration for shaping through inverse
reinforcement learning. In AAMAS  pages 429–437. International Foundation for Autonomous Agents and
Multiagent Systems  2016.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction  volume 1. MIT press Cambridge  2018.

M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. JMLR  10(Jul):

1633–1685  2009.

M. E. Taylor  P. Stone  and Y. Liu. Transfer learning via inter-task mappings for temporal difference learning.

JMLR  8(Sep):2125–2167  2007.

A. C. Tenorio-Gonzalez  E. F. Morales  and L. Villaseñor-Pineda. Dynamic reward shaping: training a robot by

voice. In Ibero-American Conference on Artiﬁcial Intelligence  pages 483–492. Springer  2010.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of

two samples. Biometrika  25(3/4):285–294  1933.

E. Wiewiora. Potential-based shaping and q-value initialization are equivalent. JAIR  19:205–208  2003.

E. Wiewiora  G. W. Cottrell  and C. Elkan. Principled methods for advising reinforcement learning agents. In

ICML  pages 792–799  2003.

11

,Michael Gimelfarb
Scott Sanner
Chi-Guhn Lee