2016,Combinatorial Multi-Armed Bandit with General Reward Functions,In this paper  we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function  whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the $\max()$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables  such as the upper confidence bound (UCB) technique  do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB)  which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve $O(\log T)$ distribution-dependent regret and $\tilde{O}(\sqrt{T})$ distribution-independent regret  where $T$ is the time horizon. We apply our results to the $K$-MAX problem and expected utility maximization problems. In particular  for $K$-MAX  we provide the first polynomial-time approximation scheme (PTAS) for its offline problem  and give the first $\tilde{O}(\sqrt T)$ bound on the $(1-\epsilon)$-approximation regret of its online problem  for any $\epsilon>0$.,Combinatorial Multi-Armed Bandit with General

Reward Functions

Wei Chen∗

Wei Hu†

Fu Li‡

Jian Li§

Yu Liu¶

Pinyan Lu(cid:107)

Abstract

In this paper  we study the stochastic combinatorial multi-armed bandit (CMAB)
framework that allows a general nonlinear reward function  whose expected value
may not depend only on the means of the input random variables but possibly
on the entire distributions of these variables. Our framework enables a much
larger class of reward functions such as the max() function and nonlinear utility
functions. Existing techniques relying on accurate estimations of the means of
random variables  such as the upper conﬁdence bound (UCB) technique  do not
work directly on these functions. We propose a new algorithm called stochastically
dominant conﬁdence bound (SDCB)  which estimates the distributions of under-
lying random variables and their stochastically dominant conﬁdence bounds. We
prove that SDCB can achieve O(log T ) distribution-dependent regret and ˜O(
T )
distribution-independent regret  where T is the time horizon. We apply our results
to the K-MAX problem and expected utility maximization problems. In particular 
√
for K-MAX  we provide the ﬁrst polynomial-time approximation scheme (PTAS)
T ) bound on the (1−)-approximation
for its ofﬂine problem  and give the ﬁrst ˜O(
regret of its online problem  for any  > 0.

√

1

Introduction

Stochastic multi-armed bandit (MAB) is a classical online learning problem typically speciﬁed as a
player against m machines or arms. Each arm  when pulled  generates a random reward following an
unknown distribution. The task of the player is to select one arm to pull in each round based on the
historical rewards she collected  and the goal is to collect cumulative reward over multiple rounds as
much as possible. In this paper  unless otherwise speciﬁed  we use MAB to refer to stochastic MAB.
MAB problem demonstrates the key tradeoff between exploration and exploitation: whether the
player should stick to the choice that performs the best so far  or should try some less explored
alternatives that may provide better rewards. The performance measure of an MAB strategy is its
cumulative regret  which is deﬁned as the difference between the cumulative reward obtained by
always playing the arm with the largest expected reward and the cumulative reward achieved by the
√
learning strategy. MAB and its variants have been extensively studied in the literature  with classical
results such as tight Θ(log T ) distribution-dependent and Θ(
T ) distribution-independent upper and
lower bounds on the regret in T rounds [19  2  1].
An important extension to the classical MAB problem is combinatorial multi-armed bandit (CMAB).
In CMAB  the player selects not just one arm in each round  but a subset of arms or a combinatorial

∗Microsoft Research  email: weic@microsoft.com. The authors are listed in alphabetical order.
†Princeton University  email: huwei@cs.princeton.edu.
‡The University of Texas at Austin  email: fuli.theory.research@gmail.com.
§Tsinghua University  email: lapordge@gmail.com.
¶Tsinghua University  email: liuyujyyz@gmail.com.
(cid:107)Shanghai University of Finance and Economics  email: lu.pinyan@mail.shufe.edu.cn.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

E[u((cid:80)

object in general  referred to as a super arm  which collectively provides a random reward to the
player. The reward depends on the outcomes from the selected arms. The player may observe partial
feedbacks from the selected arms to help her in decision making. CMAB has wide applications
in online advertising  online recommendation  wireless routing  dynamic channel allocations  etc. 
because in all these settings the action unit is a combinatorial object (e.g. a set of advertisements  a
set of recommended items  a route in a wireless network  and an allocation between channels and
users)  and the reward depends on unknown stochastic behaviors (e.g. users’ click through behaviors 
wireless transmission quality  etc.). Therefore CMAB has attracted a lot of attention in online learning
research in recent years [12  8  22  15  7  16  18  17  23  9].
Most of these studies focus on linear reward functions  for which the expected reward for playing a
super arm is a linear combination of the expected outcomes from the constituent base arms. Even for
studies that do generalize to non-linear reward functions  they typically still assume that the expected
reward for choosing a super arm is a function of the expected outcomes from the constituent base
arms in this super arm [8  17]. However  many natural reward functions do not satisfy this property.
For example  for the function max()  which takes a group of variables and outputs the maximum one
among them  its expectation depends on the full distributions of the input random variables  not just
their means. Function max() and its variants underly many applications. As an illustrative example 
we consider the following scenario in auctions: the auctioneer is repeatedly selling an item to m
bidders; in each round the auctioneer selects K bidders to bid; each of the K bidders independently
draws her bid from her private valuation distribution and submits the bid; the auctioneer uses the
ﬁrst-price auction to determine the winner and collects the largest bid as the payment.1 The goal of
the auctioneer is to gain as high cumulative payments as possible. We refer to this problem as the
K-MAX bandit problem  which cannot be effectively solved in the existing CMAB framework.
Beyond the K-MAX problem  many expected utility maximization (EUM) problems are studied
in stochastic optimization literature [27  20  21  4]. The problem can be formulated as maximizing
i∈S Xi)] among all feasible sets S  where Xi’s are independent random variables and u(·) is
a utility function. For example  Xi could be the random delay of edge ei in a routing graph  S is a
routing path in the graph  and the objective is maximizing the utility obtained from any routing path 
and typically the shorter the delay  the larger the utility. The utility function u(·) is typically nonlinear
to model risk-averse or risk-prone behaviors of users (e.g. a concave utility function is often used to
model risk-averse behaviors). The non-linear utility function makes the objective function much more
complicated: in particular  it is no longer a function of the means of the underlying random variables
Xi’s. When the distributions of Xi’s are unknown  we can turn EUM into an online learning problem
where the distributions of Xi’s need to be learned over time from online feedbacks  and we want to
maximize the cumulative reward in the learning process. Again  this is not covered by the existing
CMAB framework since only learning the means of Xi’s is not enough.
In this paper  we generalize the existing CMAB framework with semi-bandit feedbacks to handle
general reward functions  where the expected reward for playing a super arm may depend more
than just the means of the base arms  and the outcome distribution of a base arm can be arbitrary.
This generalization is non-trivial  because almost all previous works on CMAB rely on estimating
the expected outcomes from base arms  while in our case  we need an estimation method and an
analytical tool to deal with the whole distribution  not just its mean. To this end  we turn the problem
into estimating the cumulative distribution function (CDF) of each arm’s outcome distribution. We
use stochastically dominant conﬁdence bound (SDCB) to obtain a distribution that stochastically
√
dominates the true distribution with high probability  and hence we also name our algorithm SDCB.
We are able to show O(log T ) distribution-dependent and ˜O(
T ) distribution-independent regret
bounds in T rounds. Furthermore  we propose a more efﬁcient algorithm called Lazy-SDCB  which
√
ﬁrst executes a discretization step and then applies SDCB on the discretized problem. We show that
Lazy-SDCB also achieves ˜O(
T ) distribution-independent regret bound. Our regret bounds are
tight with respect to their dependencies on T (up to a logarithmic factor for distribution-independent
bounds). To make our scheme work  we make a few reasonable assumptions  including boundedness 
monotonicity and Lipschitz-continuity2 of the reward function  and independence among base arms.
We apply our algorithms to the K-MAX and EUM problems  and provide efﬁcient solutions with
concrete regret bounds. Along the way  we also provide the ﬁrst polynomial time approximation

1We understand that the ﬁrst-price auction is not truthful  but this example is only for illustrative purpose for

the max() function.

2The Lipschitz-continuity assumption is only made for Lazy-SDCB. See Section 4.

2

scheme (PTAS) for the ofﬂine K-MAX problem  which is formulated as maximizing E[maxi∈S Xi]
subject to a cardinality constraint |S| ≤ K  where Xi’s are independent nonnegative random
variables.
To summarize  our contributions include: (a) generalizing the CMAB framework to allow a general
reward function whose expectation may depend on the entire distributions of the input random
variables; (b) proposing the SDCB algorithm to achieve efﬁcient learning in this framework with
near-optimal regret bounds  even for arbitrary outcome distributions; (c) giving the ﬁrst PTAS for the
ofﬂine K-MAX problem. Our general framework treats any ofﬂine stochastic optimization algorithm
as an oracle  and effectively integrates it into the online learning framework.

Related Work. As already mentioned  most relevant to our work are studies on CMAB frameworks 
among which [12  16  18  9] focus on linear reward functions while [8  17] look into non-linear
reward functions. In particular  Chen et al. [8] look at general non-linear reward functions and Kveton
et al. [17] consider speciﬁc non-linear reward functions in a conjunctive or disjunctive form  but
both papers require that the expected reward of playing a super arm is determined by the expected
outcomes from base arms.
The only work in combinatorial bandits we are aware of that does not require the above assumption on
the expected reward is [15]  which is based on a general Thompson sampling framework. However 
they assume that the joint distribution of base arm outcomes is from a known parametric family within
known likelihood function and only the parameters are unknown. They also assume the parameter
space to be ﬁnite. In contrast  our general case is non-parametric  where we allow arbitrary bounded
distributions. Although in our known ﬁnite support case the distribution can be parametrized by
probabilities on all supported points  our parameter space is continuous. Moreover  it is unclear how
to efﬁciently compute posteriors in their algorithm  and their regret bounds depend on complicated
problem-dependent coefﬁcients which may be very large for many combinatorial problems. They
also provide a result on the K-MAX problem  but they only consider Bernoulli outcomes from base
arms  much simpler than our case where general distributions are allowed.
There are extensive studies on the classical MAB problem  for which we refer to a survey by Bubeck
and Cesa-Bianchi [5]. There are also some studies on adversarial combinatorial bandits  e.g. [26  6].
Although it bears conceptual similarities with stochastic CMAB  the techniques used are different.
Expected utility maximization (EUM) encompasses a large class of stochastic optimization problems
and has been well studied (e.g. [27  20  21  4]). To the best of our knowledge  we are the ﬁrst to study
the online learning version of these problems  and we provide a general solution to systematically
address all these problems as long as there is an available ofﬂine (approximation) algorithm. The
K-MAX problem may be traced back to [13]  where Goel et al. provide a constant approximation
algorithm to a generalized version in which the objective is to choose a subset S of cost at most K
and maximize the expectation of a certain knapsack proﬁt.

2 Setup and Notation

Problem Formulation. We model a combinatorial multi-armed bandit (CMAB) problem as a tuple
(E F  D  R)  where E = [m] = {1  2  . . .   m} is a set of m (base) arms  F ⊆ 2E is a set of subsets
of E  D is a probability distribution over [0  1]m  and R is a reward function deﬁned on [0  1]m × F.
The arms produce stochastic outcomes X = (X1  X2  . . .   Xm) drawn from distribution D  where
the i-th entry Xi is the outcome from the i-th arm. Each feasible subset of arms S ∈ F is called a
super arm. Under a realization of outcomes x = (x1  . . .   xm)  the player receives a reward R(x  S)
when she chooses the super arm S to play. Without loss of generality  we assume the reward value to
be nonnegative. Let K = maxS∈F |S| be the maximum size of any super arm.
Let X (1)  X (2)  . . . be an i.i.d.
sequence of random vectors drawn from D  where X (t) =
(X (t)
m ) is the outcome vector generated in the t-th round. In the t-th round  the player
chooses a super arm St ∈ F to play  and then the outcomes from all arms in St  i.e.  {X (t)
| i ∈ St} 
are revealed to the player. According to the deﬁnition of the reward function  the reward value in the
t-th round is R(X (t)  St). The expected reward for choosing a super arm S in any round is denoted
by rD(S) = EX∼D[R(X  S)].

1   . . .   X (t)

i

3

We also assume that for a ﬁxed super arm S ∈ F  the reward R(x  S) only depends on the revealed
outcomes xS = (xi)i∈S. Therefore  we can alternatively express R(x  S) as RS(xS)  where RS is a
function deﬁned on [0  1]S.3
A learning algorithm A for the CMAB problem selects which super arm to play in each round
t be the super arm selected by A
based on the revealed outcomes in all previous rounds. Let SA
in the t-th round.4 The goal is to maximize the expected cumulative reward in T rounds  which

t )(cid:3). Note that when the underlying distribution D is

is E(cid:104)(cid:80)T

known  the optimal algorithm A∗ chooses the optimal super arm S∗ = argmaxS∈F{rD(S)} in every
round. The quality of an algorithm A is measured by its regret in T rounds  which is the difference
between the expected cumulative reward of the optimal algorithm A∗ and that of A:

t=1 R(X (t)  SA
t )

(cid:105)

t=1

E(cid:2)rD(SA
=(cid:80)T
D(T ) = T · rD(S∗) − T(cid:88)

Reg

A

E(cid:2)rD(SA
t )(cid:3) .

t=1

For some CMAB problem instances  the optimal super arm S∗ may be computationally hard to ﬁnd
even when the distribution D is known  but efﬁcient approximation algorithms may exist  i.e.  an
α-approximate (0 < α ≤ 1) solution S(cid:48) ∈ F which satisﬁes rD(S(cid:48)) ≥ α · maxS∈F{rD(S)} can be
efﬁciently found given D as input. We will provide the exact formulation of our requirement on such
an α-approximation computation oracle shortly. In such cases  it is not fair to compare a CMAB
algorithm A with the optimal algorithm A∗ which always chooses the optimal super arm S∗. Instead 
we deﬁne the α-approximation regret of an algorithm A as

D α(T ) = T · α · rD(S∗) − T(cid:88)

A

E(cid:2)rD(SA
t )(cid:3) .

Reg

t=1

As mentioned  almost all previous work on CMAB requires that the expected reward rD(S) of
a super arm S depends only on the expectation vector µ = (µ1  . . .   µm) of outcomes  where
µi = EX∼D[Xi]. This is a strong restriction that cannot be satisﬁed by a general nonlinear function
RS and a general distribution D. The main motivation of this work is to remove this restriction.

Assumptions. Throughout this paper  we make several assumptions on the outcome distribution D
and the reward function R.
Assumption 1 (Independent outcomes from arms). The outcomes from all m arms are mutually
independent  i.e.  for X ∼ D  X1  X2  . . .   Xm are mutually independent. We write D as D =
D1 × D2 × ··· × Dm  where Di is the distribution of Xi.
We remark that the above independence assumption is also made for past studies on the ofﬂine EUM
and K-MAX problems [27  20  21  4  13]  so it is not an extra assumption for the online learning case.
Assumption 2 (Bounded reward value). There exists M > 0 such that for any x ∈ [0  1]m and any
S ∈ F  we have 0 ≤ R(x  S) ≤ M.
Assumption 3 (Monotone reward function). If two vectors x  x(cid:48) ∈ [0  1]m satisfy xi ≤ x(cid:48)
i (∀i ∈ [m]) 
then for any S ∈ F  we have R(x  S) ≤ R(x(cid:48)  S).

Computation Oracle for Discrete Distributions with Finite Supports. We require that there
exists an α-approximation computation oracle (0 < α ≤ 1) for maximizing rD(S)  when each Di
(i ∈ [m]) has a ﬁnite support. In this case  Di can be fully described by a ﬁnite set of numbers
(i.e.  its support {vi 1  vi 2  . . .   vi si} and the values of its cumulative distribution function (CDF)
Fi on the supported points: Fi(vi j) = PrXi∼Di [Xi ≤ vi j] (j ∈ [si])). The oracle takes such a
representation of D as input  and can output a super arm S(cid:48) = Oracle(D) ∈ F such that rD(S(cid:48)) ≥
α · maxS∈F{rD(S)}.

3 SDCB Algorithm

3[0  1]S is isomorphic to [0  1]|S|; the coordinates in [0  1]S are indexed by elements in S.
4Note that SA

t may be random due to the random outcomes in previous rounds and the possible randomness

used by A.

4

Algorithm 1 SDCB (Stochastically dominant conﬁdence bound)
1: Throughout the algorithm  for each arm i ∈ [m]  maintain: (i) a counter Ti which stores the
number of times arm i has been played so far  and (ii) the empirical distribution ˆDi of the
observed outcomes from arm i so far  which is represented by its CDF ˆFi

// Action in the i-th round
Play a super arm Si that contains arm i
Update Tj and ˆFj for each j ∈ Si

2: // Initialization
3: for i = 1 to m do
4:
5:
6:
7: end for
8: for t = m + 1  m + 2  . . . do
// Action in the t-th round
9:
For each i ∈ [m]  let Di be a distribution whose CDF Fi is
10:

(cid:40)

max{ ˆFi(x) −(cid:113) 3 ln t

Fi(x) =

1 

2Ti

  0}  0 ≤ x < 1

x = 1

Play the super arm St ← Oracle(D)  where D = D1 × D2 × ··· × Dm
Update Tj and ˆFj for each j ∈ St

11:
12:
13: end for

We present our algorithm stochastically dominant conﬁdence bound (SDCB) in Algorithm 1. Through-
out the algorithm  we store  in a variable Ti  the number of times the outcomes from arm i are observed
so far. We also maintain the empirical distribution ˆDi of the observed outcomes from arm i so far 
which can be represented by its CDF ˆFi: for x ∈ [0  1]  the value of ˆFi(x) is just the fraction of
the observed outcomes from arm i that are no larger than x. Note that ˆFi is always a step function
which has “jumps” at the points that are observed outcomes from arm i. Therefore it sufﬁces to store
these discrete points as well as the values of ˆFi at these points in order to store the whole function
ˆFi. Similarly  the later computation of stochastically dominant CDF Fi (line 10) only requires
computation at these points  and the input to the ofﬂine oracle only needs to provide these points and
corresponding CDF values (line 11).
The algorithm starts with m initialization rounds in which each arm is played at least once5 (lines 2-7).
In the t-th round (t > m)  the algorithm consists of three steps. First  it calculates for each i ∈ [m] a
distribution Di whose CDF Fi is obtained by lowering the CDF ˆFi (line 10). The second step is to
call the α-approximation oracle with the newly constructed distribution D = D1 ×···× Dm as input
(line 11)  and thus the super arm St output by the oracle satisﬁes rD(St) ≥ α · maxS∈F{rD(S)}.
Finally  the algorithm chooses the super arm St to play  observes the outcomes from all arms in St 
and updates Tj’s and ˆFj’s accordingly for each j ∈ St.
The idea behind our algorithm is the optimism in the face of uncertainty principle  which is the key
principle behind UCB-type algorithms. Our algorithm ensures that with high probability we have
Fi(x) ≤ Fi(x) simultaneously for all i ∈ [m] and all x ∈ [0  1]  where Fi is the CDF of the outcome
distribution Di. This means that each Di has ﬁrst-order stochastic dominance over Di.6 Then from
the monotonicity property of R(x  S) (Assumption 3) we know that rD(S) ≥ rD(S) holds for all
S ∈ F with high probability. Therefore D provides an “optimistic” estimation on the expected
reward from each super arm.

Regret Bounds. We prove O(log T ) distribution-dependent and O(
independent upper bounds on the regret of SDCB (Algorithm 1).

√

T log T ) distribution-

5Without loss of generality  we assume that each arm i ∈ [m] is contained in at least one super arm.
6We remark that while Fi(x) is a numerical lower conﬁdence bound on Fi(x) for all x ∈ [0  1]  at the

distribution level  Di serves as a “stochastically dominant (upper) conﬁdence bound” on Di.

5

We call a super arm S bad if rD(S) < α · rD(S∗). For each super arm S  we deﬁne

∆S = max{α · rD(S∗) − rD(S)  0}.

Let FB = {S ∈ F | ∆S > 0}  which is the set of all bad super arms. Let EB ⊆ [m] be the set of
arms that are contained in at least one bad super arm. For each i ∈ EB  we deﬁne

∆i min = min{∆S | S ∈ FB  i ∈ S}.

Recall that M is an upper bound on the reward value (Assumption 2) and K = maxS∈F |S|.
Theorem 1. A distribution-dependent upper bound on the α-approximation regret of SDCB (Algo-
rithm 1) in T rounds is

M 2K

ln T +

+ 1

αM m 

2136
∆i min

(cid:88)

i∈EB

√

(cid:18) π2
(cid:18) π2

3

+ 1

3

(cid:19)

(cid:19)

αM m.

and a distribution-independent upper bound is

93M

mKT ln T +

the summation reward function R(x  S) =(cid:80)

The proof of Theorem 1 is given in the supplementary material. The main idea is to reduce our
analysis on general reward functions satisfying Assumptions 1-3 to the one in [18] that deals with
i∈S xi. Our analysis relies on the Dvoretzky-Kiefer-
Wolfowitz inequality [10  24]  which gives a uniform concentration bound on the empirical CDF of a
distribution.

Applying Our Algorithm to the Previous CMAB Framework. Although our focus is on general
reward functions  we note that when SDCB is applied to the previous CMAB framework where the
expected reward depends only on the means of the random variables  it can achieve the same regret
bounds as the previous combinatorial upper conﬁdence bound (CUCB) algorithm in [8  18].
Let µi = EX∼D[Xi] be arm i’s mean outcome. In each round CUCB calculates (for each arm i) an
upper conﬁdence bound ¯µi on µi  with the essential property that µi ≤ ¯µi ≤ µi + Λi holds with
high probability  for some Λi > 0. In SDCB  we use Di as a stochastically dominant conﬁdence
bound of Di. We can show that µi ≤ EYi∼Di[Yi] ≤ µi + Λi holds with high probability  with the
same interval length Λi as in CUCB. (The proof is given in the supplementary material.) Hence  the
analysis in [8  18] can be applied to SDCB  resulting in the same regret bounds.We further remark that
in this case we do not need the three assumptions stated in Section 2 (in particular the independence
assumption on Xi’s): the summation reward case just works as in [18] and the nonlinear reward case
relies on the properties of monotonicity and bounded smoothness used in [8].

4

Improved SDCB Algorithm by Discretization

In Section 3  we have shown that our algorithm SDCB achieves near-optimal regret bounds. However 
that algorithm might suffer from large running time and memory usage. Note that  in the t-th round 
an arm i might have been observed t − 1 times already  and it is possible that all the observed values
from arm i are different (e.g.  when arm i’s outcome distribution Di is continuous). In such case 
it takes Θ(t) space to store the empirical CDF ˆFi of the observed outcomes from arm i  and both
calculating the stochastically dominant CDF Fi and updating ˆFi take Θ(t) time. Therefore  the
worst-case space usage of SDCB in T rounds is Θ(T )  and the worst-case running time is Θ(T 2)
(ignoring the dependence on m and K); here we do not count the time and space used by the ofﬂine
computation oracle.
In this section  we propose an improved algorithm Lazy-SDCB which reduces the worst-case memory
usage and running time to O(
T log T )
distribution-independent regret bound. To this end  we need an additional assumption on the reward
function:
Assumption 4 (Lipschitz-continuous reward function). There exists C > 0 such that for any S ∈ F
and any x  x(cid:48) ∈ [0  1]m  we have |R(x  S) − R(x(cid:48)  S)| ≤ C(cid:107)xS − x(cid:48)
S(cid:107)1 =

T ) and O(T 3/2)  respectively  while preserving the O(

S(cid:107)1  where (cid:107)xS − x(cid:48)

√

√

(cid:80)
i∈S |xi − x(cid:48)
i|.

6

Algorithm 2 Lazy-SDCB with known time horizon
(cid:26)[0  1
1: s ← (cid:100)√
Input: time horizon T
2: Ij ←
3: Invoke SDCB (Algorithm 1) for T rounds  with the following change: whenever observing an

T(cid:101)
s ] 
( j−1
s   j
s ] 

j = 1
j = 2  . . .   s

outcome x (from any arm)  ﬁnd j ∈ [s] such that x ∈ Ij  and regard this outcome as j

s

Algorithm 3 Lazy-SDCB without knowing the time horizon
1: q ← (cid:100)log2 m(cid:101)
2: In rounds 1  2  . . .   2q  invoke Algorithm 2 with input T = 2q
3: for k = q  q + 1  q + 2  . . . do
4:
5: end for

In rounds 2k + 1  2k + 2  . . .   2k+1  invoke Algorithm 2 with input T = 2k

We ﬁrst describe the algorithm when the time horizon T is known in advance. The algorithm is
summarized in Algorithm 2. We perform a discretization on the distribution D = D1 × ··· × Dm to
obtain a discrete distribution ˜D = ˜D1 × ··· × ˜Dm such that (i) for ˜X ∼ ˜D  ˜X1  . . .   ˜Xm are also
s   . . .   1} 
mutually independent  and (ii) every ˜Di is supported on a set of equally-spaced values { 1
where s is set to be (cid:100)√
s ]  I2 =
s   s−1
s ]  . . .   Is−1 = ( s−2
( 1
s   2
Pr

s   2
T(cid:101). Speciﬁcally  we partition [0  1] into s intervals: I1 = [0  1
s ]  Is = ( s−1
[ ˜Xi = j/s] = Pr

s   1]  and deﬁne ˜Di as

[Xi ∈ Ij]  

j = 1  . . .   s.

˜Xi∼ ˜Di

Xi∼Di

For the CMAB problem ([m] F  D  R)  our algorithm “pretends” that the outcomes are drawn from
˜D instead of D  by replacing any outcome x ∈ Ij by j
s (∀j ∈ [s])  and then applies SDCB to the
problem ([m] F  ˜D  R). Since each ˜Di has a known support { 1
s   . . .   1}  the algorithm only needs
to maintain the number of occurrences of each support value in order to obtain the empirical CDF of
√
all the observed outcomes from arm i. Therefore  all the operations in a round can be done using
√
T ) time and space  and the total time and space used by Lazy-SDCB are O(T 3/2) and
O(s) = O(
O(
The discretization parameter s in Algorithm 2 depends on the time horizon T   which is why Algo-
rithm 2 has to know T in advance. We can use the doubling trick to avoid the dependency on T . We
present such an algorithm (without knowing T ) in Algorithm 3. It is easy to see that Algorithm 3 has
the same asymptotic time and space usages as Algorithm 2.

T )  respectively.

s   2

Regret Bounds. We show that both Algorithm 2 and Algorithm 3 achieve O(
T log T )
distribution-independent regret bounds. The full proofs are given in the supplementary material.
Recall that C is the coefﬁcient in the Lipschitz condition in Assumption 4.
Theorem 2. Suppose the time horizon T is known in advance. Then the α-approximation regret of
Algorithm 2 in T rounds is at most

√

√

√

93M

mKT ln T + 2CK

T +

+ 1

αM m.

(cid:19)

(cid:18) π2

3

Proof Sketch. The regret consists of two parts: (i) the regret for the discretized CMAB problem
([m] F  ˜D  R)  and (ii) the error due to discretization. We directly apply Theorem 1 for the ﬁrst
part. For the second part  a key step is to show |rD(S) − r ˜D(S)| ≤ CK/s for all S ∈ F (see the
supplementary material).
Theorem 3. For any time horizon T ≥ 2  the α-approximation regret of Algorithm 3 in T rounds is
at most

√

T + 10αM m ln T.

√

318M

mKT ln T + 7CK

7

5 Applications

We describe the K-MAX problem and the class of expected utility maximization problems as
applications of our general CMAB framework.

The K-MAX Problem.
In this problem  the player is allowed to select at most K arms from the
set of m arms in each round  and the reward is the maximum one among the outcomes from the

selected arms. In other words  the set of feasible super arms is F = (cid:8)S ⊆ [m](cid:12)(cid:12)|S| ≤ K(cid:9)  and

the reward function is R(x  S) = maxi∈S xi. It is easy to verify that this reward function satisﬁes
Assumptions 2  3 and 4 with M = C = 1.
Now we consider the corresponding ofﬂine K-MAX problem of selecting at most K arms from
m independent arms  with the largest expected reward. It can be implied by a result in [14] that
ﬁnding the exact optimal solution is NP-hard  so we resort to approximation algorithms. We can
show  using submodularity  that a simple greedy algorithm can achieve a (1 − 1/e)-approximation.
Furthermore  we give the ﬁrst PTAS for this problem. Our PTAS can be generalized to constraints
other than the cardinality constraint |S| ≤ K  including s-t simple paths  matchings  knapsacks  etc.
The algorithms and corresponding proofs are given in the supplementary material.
Theorem 4. There exists a PTAS for the ofﬂine K-MAX problem. In other words  for any constant
 > 0  there is a polynomial-time (1 − )-approximation algorithm for the ofﬂine K-MAX problem.

√
We thus can apply our SDCB algorithm to the K-MAX bandit problem and obtain O(log T )
√
distribution-dependent and ˜O(
T ) distribution-independent regret bounds according to Theorem 1 
or can apply Lazy-SDCB to get ˜O(
T ) distribution-independent bound according to Theorem 2 or 3.
Streeter and Golovin [26] study an online submodular maximization problem in the oblivious
√
adversary model. In particular  their result can cover the stochastic K-MAX bandit problem as a
mT log m) upper bound on the (1 − 1/e)-regret can be shown. While
special case  and an O(K
the techniques in [26] can only give a bound on the (1 − 1/e)-approximation regret for K-MAX 
√
T ) bound on the (1 − )-approximation regret for any constant  > 0 
we can obtain the ﬁrst ˜O(
using our PTAS as the ofﬂine oracle. Even when we use the simple greedy algorithm as the oracle 
our experiments show that SDCB performs signiﬁcantly better than the algorithm in [26] (see the
supplementary material).

form R(x  S) = u((cid:80)
problem is to maximize the expected utility E[u((cid:80)

Expected Utility Maximization. Our framework can also be applied to reward functions of the
i∈S xi)  where u(·) is an increasing utility function. The corresponding ofﬂine
i∈S xi)] subject to a feasibility constraint S ∈ F.
Note that if u is nonlinear  the expected utility may not be a function of the means of the arms in
S. Following the celebrated von Neumann-Morgenstern expected utility theorem  nonlinear utility
functions have been extensively used to capture risk-averse or risk-prone behaviors in economics (see
e.g.  [11])  while linear utility functions correspond to risk-neutrality.
Li and Deshpande [20] obtain a PTAS for the expected utility maximization (EUM) problem for
several classes of utility functions (including for example increasing concave functions which
typically indicate risk-averseness)  and a large class of feasibility constraints (including cardinality
constraint  s-t simple paths  matchings  and knapsacks). Similar results for other utility functions and
feasibility constraints can be found in [27  21  4]. In the online problem  we can apply our algorithms 
using their PTASs as the ofﬂine oracle. Again  we can obtain the ﬁrst tight regret bounds on the
(1 − )-approximation regret for any  > 0  for the class of online EUM problems.

Acknowledgments

Wei Chen was supported in part by the National Natural Science Foundation of China (Grant No.
61433014). Jian Li and Yu Liu were supported in part by the National Basic Research Program
of China grants 2015CB358700  2011CBA00300  2011CBA00301  and the National NSFC grants
61033001  61361136003. The authors would like to thank Tor Lattimore for referring to us the DKW
inequality.

8

References
[1] Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits. In

COLT  pages 217–226  2009.

[2] Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2-3):235–256  2002.

[3] Peter Auer  Nicolo Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing  32(1):48–77  2002.

[4] Anand Bhalgat and Sanjeev Khanna. A utility equivalence theorem for concave functions. In IPCO  pages

126–137. Springer  2014.

[5] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[6] Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences 

78(5):1404–1422  2012.

[7] Shouyuan Chen  Tian Lin  Irwin King  Michael R. Lyu  and Wei Chen. Combinatorial pure exploration of

multi-armed bandits. In NIPS  2014.

[8] Wei Chen  Yajun Wang  Yang Yuan  and Qinshi Wang. Combinatorial multi-armed bandit and its extension

to probabilistically triggered arms. Journal of Machine Learning Research  17(50):1–33  2016.

[9] Richard Combes  M. Sadegh Talebi  Alexandre Proutiere  and Marc Lelarge. Combinatorial bandits

revisited. In NIPS  2015.

[10] Aryeh Dvoretzky  Jack Kiefer  and Jacob Wolfowitz. Asymptotic minimax character of the sample
distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics 
pages 642–669  1956.

[11] P. C. Fishburn. The foundations of expected utility. Dordrecht: Reidel  1982.
[12] Yi Gai  Bhaskar Krishnamachari  and Rahul Jain. Combinatorial network optimization with unknown
variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions
on Networking  20(5):1466–1478  2012.

[13] Ashish Goel  Sudipto Guha  and Kamesh Munagala. Asking the right questions: Model-driven optimization

using probes. In PODS  pages 203–212. ACM  2006.

[14] Ashish Goel  Sudipto Guha  and Kamesh Munagala. How to probe for an extreme value. ACM Transactions

on Algorithms (TALG)  7(1):12:1–12:20  2010.

[15] Aditya Gopalan  Shie Mannor  and Yishay mansour. Thompson sampling for complex online problems. In

ICML  pages 100–108  2014.

[16] Branislav Kveton  Zheng Wen  Azin Ashkan  Hoda Eydgahi  and Brian Eriksson. Matroid bandits: Fast

combinatorial optimization with learning. In UAI  pages 420–429  2014.

[17] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvári. Combinatorial cascading bandits. In

NIPS  2015.

[18] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvári. Tight regret bounds for stochastic

combinatorial semi-bandits. In AISTATS  pages 535–543  2015.

[19] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

applied mathematics  6(1):4–22  1985.

[20] Jian Li and Amol Deshpande. Maximizing expected utility for stochastic combinatorial optimization

problems. In FOCS  pages 797–806  2011.

[21] Jian Li and Wen Yuan. Stochastic combinatorial optimization via poisson approximation. In STOC  pages

971–980  2013.

[22] Tian Lin  Bruno Abrahao  Robert Kleinberg  John Lui  and Wei Chen. Combinatorial partial monitoring

game with linear feedback and its applications. In ICML  pages 901–909  2014.

[23] Tian Lin  Jian Li  and Wei Chen. Stochastic online greedy learning with semi-bandit feedbacks. In NIPS 

2015.

[24] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The Annals of Probability 

pages 1269–1283  1990.

[25] George L. Nemhauser  Laurence A. Wolsey  and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions – I. Mathematical Programming  14(1):265–294  1978.

[26] Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions. In

NIPS  2008.

[27] Jiajin Yu and Shabbir Ahmed. Maximizing expected utility over a knapsack constraint. Operations

Research Letters  44(2):180–185  2016.

9

,Emily Denton
Soumith Chintala
arthur szlam
Rob Fergus
Wei Chen
Wei Hu
Fu Li
Jian Li
Yu Liu
Pinyan Lu