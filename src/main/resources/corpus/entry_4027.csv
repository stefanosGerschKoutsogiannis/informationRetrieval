2019,Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback,In this paper  we propose three online algorithms for submodular maximization. The first one  Mono-Frank-Wolfe  reduces the number of per-function gradient evaluations from $T^{1/2}$  [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1  and achieves a $(1-1/e)$-regret bound of $O(T^{4/5})$. The second one  Bandit-Frank-Wolfe  is the first bandit algorithm for continuous DR-submodular maximization  which achieves a $(1-1/e)$-regret bound of $O(T^{8/9})$. Finally  we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization  Responsive-Frank-Wolfe  which attains a $(1-1/e)$-regret bound of $O(T^{8/9})$ in the responsive bandit setting.,Online Continuous Submodular Maximization: From

Full-Information to Bandit Feedback

Mingrui Zhang† Lin Chen‡ Hamed Hassani(cid:93) Amin Karbasi‡ (cid:92)

† Department of Statistics and Data Science  Yale University

‡ Department of Electrical Engineering  Yale University

(cid:93) Department of Electrical and Systems Engineering  University of Pennsylvania

(cid:92) Department of Computer Science  Yale University

{mingrui.zhang  lin.chen  amin.karbasi}@yale.edu

hassani@seas.upenn.edu

Abstract

In this paper  we propose three online algorithms for submodular maximization.
The ﬁrst one  Mono-Frank-Wolfe  reduces the number of per-function gradient
evaluations from T 1/2 [18] and T 3/2 [17] to 1  and achieves a (1 − 1/e)-regret
bound of O(T 4/5). The second one  Bandit-Frank-Wolfe  is the ﬁrst bandit al-
gorithm for continuous DR-submodular maximization  which achieves a (1− 1/e)-
regret bound of O(T 8/9). Finally  we extend Bandit-Frank-Wolfe to a bandit
algorithm for discrete submodular maximization  Responsive-Frank-Wolfe 
which attains a (1 − 1/e)-regret bound of O(T 8/9) in the responsive bandit set-
ting.

1

Introduction

Let F : X → R≥0 be a differentiable function deﬁned on a box X (cid:44)(cid:81)d

Submodularity naturally arises in a variety of disciplines  and has numerous applications in machine
learning  including data summarization [45]  active and semi-supervised learning [26  47]  com-
pressed sensing and structured sparsity [7]  fairness in machine learning [8]  mean-ﬁeld inference in
probabilistic models [10]  and MAP inference in determinantal point processes (DPPs) [36].
We say that a set function f : 2Ω → R≥0 deﬁned on a ﬁnite ground set Ω is submodular if for every
A ⊆ B ⊆ Ω and x ∈ Ω \ B  we have f (x|A) ≥ f (x|B)  where f (x|A) (cid:44) f (A ∪ {x}) − f (A)
is a discrete derivative [39]. Continuous DR-submodular functions are the continuous analogue.
i=1 Xi  where each Xi is
a closed interval of R≥0. We say that F is continuous DR-submodular if for every x  y ∈ X that
(y)  where x ≤ y means
satisfy x ≤ y and every i ∈ [d] (cid:44) {1  . . .   d}  we have ∂F
xi ≤ yi ∀i ∈ [d] [9].
In this paper  we focus on online and bandit maximization of submodular set functions and contin-
uous DR-submodular functions. In contrast to ofﬂine optimization where the objective function is
completely known beforehand  online optimization can be viewed as a two-player game between
the player and the adversary in a sequential manner [50  42  28]. Let F be a family of real-valued
functions. The player wants to maximize a sequence of functions F1  . . .   FT ∈ F subject to a
constraint set K. The player has no a priori knowledge of the functions  while the constraint set
is known and we assume that it is a closed convex set in Rd. The natural number T is termed the
horizon of the online optimization problem. At the t-th iteration  without the knowledge of Ft  the
player has to select a point xt ∈ K. After the player commits to this choice  the adversary selects
a function Ft ∈ F. The player receives a reward Ft(xt)  observes the function Ft determined by
the adversary  and proceeds to the next iteration. In the more challenging bandit setting  even the

(x) ≥ ∂F

∂xi

∂xi

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

single choice in hindsight. To be precise  the regret is deﬁned by maxx∈K(cid:80)T

function Ft is unavailable to the player and the only observable information is the reward that the
player receives [23  3  11].
The performance of the algorithm that the player uses to determine her choices x1  . . .   xT is
quantiﬁed by the regret  which is the gap between her accumulated reward and the reward of the best
t=1 Ft(xt).
However  even in the ofﬂine scenario  it is shown that the maximization problem of a continuous
DR-submodular function cannot be approximated within a factor of (1 − 1/e + ) for any  > 0 in
polynomial time  unless RP = N P [9]. Therefore  we consider the (1 − 1/e)-regret [44  34  18]

t=1 Ft(x)−(cid:80)T

R1−1/e T (cid:44) (1 − 1/e) max
x∈K

Ft(xt).

T(cid:88)

t=1

Ft(x) − T(cid:88)

t=1

For ease of notation  we write RT for R1−1/e T throughout this paper.
In this paper  we study the following three problems:

• OCSM: the Online Continuous DR-Submodular Maximization problem 
• BCSM: the Bandit Continuous DR-Submodular Maximization problem  and
• RBSM: the Responsive Bandit Submodular Maximization problem.

We note that although special cases of bandit submodular maximization problem (BSM) were studied
in [44  27]  the vanilla BSM problem is still open for general monotone submodular functions under a
matroid constraint. In BSM  the objective functions f1  . . .   fT are submodular set functions deﬁned
on a common ﬁnite ground set Ω and subject to a common constraint I. For each function fi  the
player has to select a subset Xi ∈ I. Only after playing the subset Xi  the reward fi(Xi) is received
and thereby observed.
If the value of the corresponding multilinear extension1 F can be estimated by the submodular
set function f  we may expect to solve the vanilla BSM by invoking algorithms for continuous DR-
submodular maximization. In this paper  however  we will show a hardness result that subject to some
constraint I  it is impossible to construct a one-point unbiased estimator of the multilinear extension
F based on the value of f  without knowing the information of f in advance. This result motivates the
study of a slightly relaxed setting termed the Responsive Bandit Submodular Maximization problem
(RBSM). In RBSM  at round i  if Xi /∈ I  the player is still allowed to play Xi and observe the function
value fi(Xi)  but gets zero reward out of it.
OCSM was studied in [18  17]  where T 1/2 exact gradient evaluations or T 3/2 stochastic gradient
evaluations are required per iteration (T is the horizon). Therefore  they cannot be extended to the
bandit setting (BCSM and RBSM) where one single function evaluation per iteration is permitted. As a
result  no known bandit algorithm attains a sublinear (1 − 1/e)-regret.
In this paper  we ﬁrst propose Mono-Frank-Wolfe for OCSM  which requires one stochastic gra-
dient per function and still attains a (1 − 1/e)-regret bound of O(T 4/5). This is signiﬁcant as
it reduces the number of per-function gradient evaluations from T 3/2 to 1. Furthermore  it pro-
vides a feasible avenue to solving BCSM and RBSM. We then propose Bandit-Frank-Wolfe and
Responsive-Frank-Wolfe that attain a (1 − 1/e)-regret bound of O(T 8/9) for BCSM and RBSM 
respectively. To the best of our knowledge  Bandit-Frank-Wolfe and Responsive-Frank-Wolfe
are the ﬁrst algorithms that attain a sublinear (1 − 1/e)-regret bound for BCSM and RBSM  respectively.
The performance of prior approaches and our proposed algorithms is summarized in Table 1. We also
list further related works in Appendix A.

2 Preliminaries

Monotonicity  Smoothness  and Directional Concavity Property A submodular set function
f : 2Ω → R is called monotone if for any two sets A ⊆ B ⊆ Ω we have f (A) ≤ f (B).
For two vectors x and y  we write x ≤ y if xi ≤ yi holds for every i. Let F be a continuous
DR-submodular function deﬁned on X . We say that F is monotone if F (x) ≤ F (y) for every
x  y ∈ X obeying x ≤ y. Additionally  F is called L-smooth if for every x  y ∈ X it holds that

1We formally deﬁne the multilinear extension of a submodular set function in Section 2.

2

Table 1: Comparison of previous and our proposed algorithms.

Setting Algorithm

OCSM

BCSM
RBSM

Meta-FW [18]
VR-FW [17]
Mono-FW (this work)
Bandit-FW (this work)
Responsive-FW (this work)

Stochastic
gradient
No
Yes
Yes
-
-

# of grad.
evaluations

T 1/2
T 3/2
1
-
-

(1 − 1/e)-regret

√
√
T )
O(
O(
T )
O(T 4/5)

O(T 8/9)

O(T 8/9)

by F (x) =(cid:80)

(cid:107)∇F (x) − ∇F (y)(cid:107)≤ L(cid:107)x − y(cid:107). Throughout the paper  we use the notation (cid:107)·(cid:107) for the Euclidean
norm. An important implication of continuous DR-submodularity is concavity along the non-negative
directions [16  9]  i.e.  for all x ≤ y  we have F (y) ≤ F (x) + (cid:104)∇F (x)  y − x(cid:105).
Multilinear Extension Given a submodular set function f : 2Ω → R≥0 deﬁned on a ﬁnite ground
set Ω  its multilinear extension is a continuous DR-submodular function F : [0  1]|Ω| → R≥0 deﬁned
S⊆Ω f (S)Πi∈SxiΠj /∈S(1 − xj)  where xi is the i-th coordinate of x. Equivalently  for
any vector x ∈ [0  1]|Ω| we have F (x) = ES∼x[f (S)] where S ∼ x means that S is a random subset
of Ω such that every element i ∈ Ω is contained in S independently with probability xi.
Geometric Notations The d-dimensional unit ball is denoted by Bd  and the (d − 1)-dimensional
unit sphere is denoted by Sd−1. Let K be a bounded set. We deﬁne its diameter D = supx y∈K(cid:107)x−y(cid:107)
and radius R = supx∈K(cid:107)x(cid:107). We say a set K has lower bound u if u ∈ K  and ∀x ∈ K  x ≥ u.

3 One-shot Online Continuous DR-Submodular Maximization

In this section  we propose Mono-Frank-Wolfe  an online continuous DR-submodular maximization
algorithm which only needs one gradient evaluation per function. This algorithm is the basis of the
methods presented in the next section for the bandit setting. We also note that throughout this paper 
∇F denotes the exact gradient for F   while ˜∇F denotes the stochastic gradient.
We begin by reviewing the Frank-Wolfe (FW) [24  33] method for maximizing monotone continuous
DR-submodular functions in the ofﬂine setting [9]  where we have one single objective function F .
Assuming that we have access to the exact gradient ∇F   the FW method is an iterative procedure that
starts from the initial point x(1) = 0  and at the k-th iteration  solves a linear optimization problem
(1)

v(k) ← arg max

(cid:104)v ∇F (x(k))(cid:105)

v∈K

t

3

t

t

t = 0  update x(k+1)

which is used to update x(k+1) ← x(k) + ηkv(k)  where ηk is the step size.
We aim to extend the FW method to the online setting. Inspired by the FW update above  to get high
rewards for each objective function Ft  we start from x(1)
for
multiple iterations (let K denote the number of iterations)  then play the last iterate x(K+1)
for Ft. To
obtain the point x(K+1)
which we play  we need to solve the linear program Eq. (1) and thus get v(k)
 
where we have to know the gradient in advance. However  in the online setting  we can only observe
the stochastic gradient ˜∇Ft after we play some point for Ft. So the key issue is to obtain the vector
t which at least approximately maximizes (cid:104)· ∇Ft(x(k)
v(k)
To do so  we use K no-regret online linear maximization oracles {E (k)}  k ∈ [K]  and let v(k)
output vector of E (k) at round t. Once we update x(k+1)
Ft  we can observe ˜∇Ft(x(k)
estimation of ∇Ft(x(k)
(cid:104)·  d(k)

be the
for
)  an
) [37  38] for all k ∈ [K]. Then we set
(cid:105) as the objective function for oracle E (k) at round t. Thanks to the no-regret property of E (k) 

t
) and iteratively construct d(k)
) with a lower variance than ˜∇Ft(x(k)

by v(k)
t = (1 − ρk)d(k−1)

for all k ∈ [K]  and play x(K+1)
+ ρk ˜∇Ft(x(k)

)(cid:105)  before we play some point for Ft.

t

= x(k)

t + ηkv(k)

t

t

t

t

t

t

t

t

t

t

t

t

t

q

)(cid:105).

(cid:80)K

q = 0  update x(k+1)

(cid:105)  thus also approximately maximizes (cid:104)· ∇Ft(x(k)

  which is obtained before we play some point for Ft and observe the gradient  approximately

v(k)
t
maximizes (cid:104)·  d(k)
This approach was ﬁrst proposed in [18  17]  where stochastic gradients at K = T 3/2 points (i.e. 
{x(k)
t }  k ∈ [K]) are required for each function Ft. To carry this general idea into the one-shot setting
where we can only access one gradient per function  we need the following blocking procedure.
We divide the upcoming objective functions F1  . . .   FT into Q equisized blocks of size K (so
T = QK). For the q-th block  we ﬁrst set x(1)
  and play
the same point xq = x(K+1)
for all the functions F(q−1)K+1  . . .   FqK. The reason why we play
the same point xq will be explained later. We also deﬁne the average function in the q-th block as
¯Fq (cid:44) 1
k=1 F(q−1)K+k. In order to reduce the required number of gradients per function  the key
idea is to view the average functions ¯F1  . . .   ¯FQ as virtual objective functions.
Precisely  in the q-th block  let (tq 1  . . .   tq K) be a random permutation of the indices {(q − 1)K +
1  . . .   qK}. After we update all the x(k)
  for each Ft  we play xq and ﬁnd the corresponding k(cid:48) such
that t = tq k(cid:48)  then observe ˜∇Ft (i.e.  ˜∇Ftq k(cid:48) ) at x(k(cid:48))
q ) for all
k ∈ [K]. Since tq k is a random variable such that E[Ftq k ] = ¯Fq  ˜∇Ftq k (x(k)
q ) is also an estimation
of ∇ ¯Fq(x(k)
q )  which holds for all k ∈ [K]. As a result  with only one gradient evaluation per function
Ftq k  we can obtain stochastic gradients of the virtual objective function ¯Fq at K points. In this way 
the required number of per-function gradient evaluations is reduced from K to 1 successfully.
Note that since we play yt = xq for each Ft in the q-th block  the regret w.r.t. the original objective
functions and that w.r.t. the average functions satisfy that

. Thus we can obtain ˜∇Ftq k (x(k)

q + ηkv(k)

= x(k)

K

q

q

q

q

(1 − 1/e) max
x∈K

Ft(yt) = K

(1 − 1/e) max
x∈K

T(cid:88)

Ft(x) − T(cid:88)

t=1

t=1

(cid:34)

Q(cid:88)

¯Fq(x) − Q(cid:88)

q=1

t=1

(cid:35)

¯Fq(xq)

 

q

q

q

1

(cid:80)K

q ). So v(k)

+ ρk ˜∇Ftq k (x(k)

q = (1 − ρk)d(k−1)

q )  thus also an estimation of ∇ ¯Fq(x(k)

q )(cid:105). Inspired by the ofﬂine FW method  playing xq = x(K+1)

which makes it possible to view the functions ¯Fq as virtual objective functions in the regret analysis.
Moreover  we iteratively construct d(k)
q ) as an estimation of
  the output of E (k)  approximately
∇Ftq k (x(k)
maximizes (cid:104)· ∇ ¯Fq(x(k)
  the last iterate
in the FW procedure  may obtain high rewards for ¯Fq. As a result  we play the same point xq in the
q-th block.
We also note that once tq 1  . . .   tq k are revealed  conditioned on the knowledge  the expecta-
tion of Ftq k+1 is no longer the average function ¯Fq but the residual average function ¯Fq k(x) =
i=k+1 Ftq i(x). As more indices tq k are revealed  ¯Fq k becomes increasingly different from
K−k
¯Fq  which makes the observed gradient ˜∇Ftq k+1 (x(k+1)
) any
more. As a result  although we use the averaging technique (the update of d(k)
) as in [37  38] for vari-
ance reduction  a completely different gradient error analysis is required. In Lemma 6 (Appendix B) 
we establish that the squared error of d(k)
exhibits an inverted bell-shaped tendency; i.e.  the squared
error is large at the initial and ﬁnal stages and is small at the intermediate stage.
We present our proposed Mono-Frank-Wolfe algorithm in Algorithm 1.
We will show that Mono-Frank-Wolfe achieves a (1 − 1/e)-regret bound of O(T 4/5). In order
to prove this result  we ﬁrst make the following assumptions on the constraint set K  the objective
functions Ft  the stochastic gradient ˜∇Ft  and the online linear maximization oracles.
Assumption 1. The constraint set K is a convex and compact set that contains 0.
Assumption 2. Every objective function Ft is monotone  continuous DR-Submodular  L1-Lipschitz 
and L2-smooth.
Assumption 3. The stochastic gradient ˜∇Ft(x) is unbiased  i.e.  E[ ˜∇Ft(x)] = ∇Ft(x). Addi-
tionally  it has a uniformly bounded norm (cid:107) ˜∇Ft(x)(cid:107)≤ M0 and a uniformly bounded variance
E[(cid:107)∇Ft(x) − ˜∇Ft(x)(cid:107)2] ≤ σ2

0 for every x ∈ K and objective function Ft.

) not a good estimation of ∇ ¯Fq(x(k+1)

q

q

q

q

4

Algorithm 1 Mono-Frank-Wolfe
Input: constraint set K  horizon T   block size K  online linear maximization oracles on K:

E (1) ···  E (K)  step sizes ρk ∈ (0  1)  ηk ∈ (0  1)  number of blocks Q = T /K

Output: y1  y2  . . .
1: for q = 1  2  . . .   Q do
q ← 0
2:
3:

q ← 0  x(1)
d(0)
q ∈ K be the output of E (k) in round q  x(k+1)
For k = 1  2  . . .   K  let v(k)
Let (tq 1  . . .   tq K) be a random permutation of {(q − 1)K + 1  . . .   qK}
For t = (q − 1)K + 1  . . .   qK  play yt = xq and obtain the reward Ft(yt); ﬁnd the

q + ηkv(k)

← x(k)

Set xq ← x(K+1)

.

q

q

q

corresponding k(cid:48) ∈ [K] such that t = tq k(cid:48)  observe ˜∇Ft(x(k(cid:48))

q

)  i.e.  ˜∇Ftq k(cid:48) (x(k(cid:48))

)
q )  compute (cid:104)v(k)

q

q

q (cid:105) as

  d(k)

For k = 1  2  . . .   K  d(k)

reward for E (k)  and feed back d(k)

q

to E (k)

q ← (1 − ρk)d(k−1)

q

+ ρk ˜∇Ftq k (x(k)

4:
5:

6:

7: end for

√

t ≤ C

t ∀i ∈ [K]  where C > 0 is a constant.

Assumption 4. For the online linear maximization oracles  the regret at horizon t (denoted by RE (i)
satisﬁes RE (i)
Note that there exist online linear maximization oracles E (i) with regret RE (i)
t ∀i ∈ [K] for
any horizon t (for example  the online gradient descent [50]). Therefore  Assumption 4 is fulﬁlled.
Theorem 1 (Proof in Appendix B). Under Assumptions 1 to 4  if we set K = T 3/5  ηk = 1
K   ρk =
(K−k+2)2/3 when K/2 + 2 ≤ k ≤ K  where we assume
(k+3)2/3 when 1 ≤ k ≤ K/2 + 1  and ρk =
that K is even for simplicity  then yt ∈ K ∀t  and the expected (1 − 1/e)-regret of Algorithm 1 is at
most

t ≤ C

√

1.5

2

t

)

E[RT ] ≤ (N + C + D2)T 4/5 +

L2D2

where N = max{52/3(L1 +M0)2  4(L2

1 +σ2

0)+32G  2.25(L2

T 2/5 
0)+7G/3}  G = (L2R +2L1)2.

2
1 +σ2

4 Bandit Continuous DR-Submodular Maximization

In this section  we present the ﬁrst bandit algorithm for continuous DR-submodular maximization 
Bandit-Frank-Wolfe  which attains a (1− 1/e)-regret bound of O(T 8/9). We begin by explaining
the one-point gradient estimator [23]  which is crucial to the proposed bandit algorithm. The proposed
algorithm and main results are illustrated in Section 4.2.

4.1 One-Point Gradient Estimator
Given a function F   we deﬁne its δ-smoothed version ˆFδ(x) (cid:44) Ev∼Bd [F (x + δv)]  where v ∼ Bd
denotes that v is drawn uniformly at random from the unit ball Bd. Thus the function F is averaged
over a ball of radius δ. It can be easily veriﬁed that if F is monotone  continuous DR-submodular 
L1-Lipschitz  and L2-smooth  then so is ˆFδ  and for all x we have | ˆFδ(x) − F (x)|≤ L1δ (Lemma 7
in Appendix C). So the δ-smoothed version ˆFδ is indeed an approximation of F . A maximizer of ˆFδ
also maximizes F approximately.
More importantly  the gradient of the smoothed function ˆFδ admits a one-point unbiased estimator
[23  32]: ∇ ˆFδ(x) = Eu∼Sd−1
at random from the unit sphere Sd−1. Thus the player can estimate the gradient of the smoothed
function at point x by playing the random point x + δu for the original function F . So usually  we
can extend a one-shot online algorithm to the bandit setting by replacing the observed stochastic
gradients with the one-point gradient estimations.
In our setting  however  we cannot use the one-point gradient estimator directly. When the point x is
close to the boundary of the constraint set K  the point x + δu may fall outside of K. To address this

(cid:2) d
δ F (x + δu)u(cid:3)  where u ∼ Sd−1 denotes that u is drawn uniformly

5

issue  we introduce the notion of δ-interior. A set is said to be a δ-interior of K if it is a subset of

intδ(K) = {x ∈ K| inf

s∈∂K d(x  s) ≥ δ}  

where d(· ·) denotes the Euclidean distance.
In other words  K(cid:48) is a δ-interior of K if it holds for every x ∈ K(cid:48) that B(x  δ) ⊆ K (Fig. 1a in
Appendix D). We note that there can be inﬁnitely many δ-interiors of K. In the sequel  K(cid:48) will denote
the δ-interior that we consider. We also deﬁne the discrepancy between K and K(cid:48) by

d(K K(cid:48)) = sup
x∈K

d(x K(cid:48)) 

optimal total reward on K (maxx∈K(cid:80)T

t=1 Ft(x)) by that on K(cid:48) (maxx∈K(cid:48)(cid:80)T

which is the supremum of the distances between points in K and the set K(cid:48). The distance d(x K(cid:48)) is
given by inf y∈K(cid:48) d(x  y).
By deﬁnition  every point x ∈ K(cid:48) satisﬁes x + δu ∈ K  which enables us to use the one-point gradient
estimator on K(cid:48). Moreover  if every Ft is Lipschitz and d(K K(cid:48)) is small  we can approximate the
t=1 Ft(x))  and thereby
obtain the regret bound subject to the original constraint set K  by running bandit algorithms on K(cid:48).
We also note that if the constraint set K satisﬁes Assumption 1 and is down-closed (e.g.  a matroid
polytope)  for sufﬁciently small δ  we can construct K(cid:48)  a down-closed δ-interior of K  with d(K K(cid:48))
sufﬁciently small (actually it is a linear function of δ). Recall that a set P is down-closed if it has a
lower bound u such that (1) ∀y ∈ P  u ≤ y; and (2) ∀y ∈ P  x ∈ Rd  u ≤ x ≤ y =⇒ x ∈ P [9].
We ﬁrst deﬁne Bd≥0 = Bd ∩ Rd≥0 and make the following assumption2:
Assumption 5. There exists a positive number r such that rBd≥0 ⊆ K.
To construct K(cid:48)  for sufﬁciently small δ such that δ < r√
< 1  and
shrink K by a factor of (1 − α) to obtain Kα = (1 − α)K. Then we translate the shrunk set Kα by
δ1 (Fig. 1b in Appendix D). In other words  the set that we ﬁnally obtain is

  we ﬁrst set α = (

d+1)δ

d+1

√

r

K(cid:48) = Kα + δ1 = (1 − α)K + δ1.

r

√

d+1)δ

d( R

r ]δ.

r + 1) + R

In Lemma 1  we establish that K(cid:48) is indeed a δ-interior of K and deduce a linear bound for d(K K(cid:48)).
Lemma 1 (Proof in Appendix D). We assume Assumptions 1 and 5 and also assume that K is
< 1. The set K(cid:48) = (1− α)K + δ1
down-closed and that δ is sufﬁciently small such that α = (
is convex and compact. Moreover  K(cid:48) is a down-closed δ-interior of K and satisﬁes d(K K(cid:48)) ≤
√
[
4.2 No-(1 − 1/e)-Regret Biphasic Bandit Algorithm
Our proposed bandit algorithm is based on the online algorithm Mono-Frank-Wolfe in Section 3.
Precisely  we want to replace the stochastic gradients in Algorithm 1 with the one-point gradient
estimators  and run the modiﬁed algorithm on K(cid:48)  a proper δ-interior of the constraint set K. Note
that the one-point estimator requires that the point at which we estimate the gradient (i.e.  x) must
be identical to the point that we play (i.e.  x + δu)  if we ignore the random δu. In Algorithm 1 
however  we play point xq but obtain estimated gradient at other points x(k(cid:48))
(Line 5). This suggests
that Algorithm 1 cannot be extended to the bandit setting via the one-point gradient estimator directly.
To circumvent this limitation  we propose a biphasic approach that categorizes the plays into the
exploration and exploitation phases. To motivate this biphasic method  recall that in Algorithm 1  we
need to play xq to gain high rewards (exploitation)  whilst we observe ˜∇Ft(x(k(cid:48))
) to obtain gradient
information (exploration). So in our biphasic approach  we expend a large portion of plays on
exploitation (play xq  so we can still get high rewards) and a small portion of plays on exploring the
gradient (play x(k(cid:48))
to get one-point gradient estimators  so we can still obtain sufﬁcient information).
To be precise  we divide the T objective functions into Q equisized blocks of size L  where L = T /Q.
Each block is subdivided into two phases. As shown in Algorithm 2  we randomly choose K (cid:28) L
functions for exploration (Line 6) and use the remaining (L − K) functions for exploitation (Line 7).

q

q

q

2This assumption is an analogue of the assumption rBd ⊆ K ⊆ RBd in [23].

6

We describe our algorithm formally in Algorithm 2. We also note that for a general constraint
set K with a proper δ-interior K(cid:48) such that d(K K(cid:48)) ≤ c1δγ  Theorem 4 (Appendix E.1) shows a
3+5 min{1 γ}
(1 − 1/e)-regret bound of O(T
3+6 min{1 γ} ). Moreover  with Lemma 1  this result can be extended to
down-closed constraint sets K  as shown in Theorem 2.

Algorithm 2 Bandit-Frank-Wolfe
Input: smoothing radius δ  δ-interior K(cid:48) with lower bound u  horizon T   block size L  the number
of exploration steps per block K  online linear maximization oracles on K(cid:48): E (1) ···  E (K)  step
sizes ρk ∈ (0  1)  ηk ∈ (0  1)  the number of blocks Q = T /L

Output: y1  y2  . . .
1: for q = 1  2  . . .   Q do
q ← u
2:
3:

← x(k)

q +

4:
5:
6:

7:
8:
9:

q ← 0  x(1)
d(0)
For k = 1  2  . . .   K  let v(k)
q − u). Set xq ← x(K+1)
Let (tq 1  . . .   tq L) be a random permutation of {(q − 1)L + 1 ···   qL}
for t = (q − 1)L + 1 ···   qL do

q ∈ K(cid:48) be the output of E (k) in round q  x(k+1)

q

q

ηk(v(k)

yt = ytq k(cid:48) = x(k(cid:48))

If t ∈ {tq 1 ···   tq K}  ﬁnd the corresponding k(cid:48) ∈ [K] such that t = tq k(cid:48)  play
(cid:46) Exploration
(cid:46) Exploitation

q + δuq k(cid:48) for Ft (i.e.  Ftq k(cid:48) )  where uq k(cid:48) ∼ Sd−1

If t ∈ {(q − 1)L + 1 ···   qL} \ {tq 1 ···   tq K}  play yt = xq for Ft
q ← (1 − ρk)d(k−1)

+ ρkgq k  compute

end for
For k = 1  2  . . .   K  gq k ← d
  d(k)

q (cid:105) as reward for E (k)  and feed back d(k)

δ Ftq k (ytq k )uq k  d(k)
to E (k)

(cid:104)v(k)

q

q

q

10: end for
Assumption 6. Every objective function Ft satisﬁes that supx∈K|Ft(x)|≤ M1.
Theorem 2 (Proof in Appendix E.2). We assume Assumptions 1  2 and 4 to 6  and also assume
that K is down-closed. If we generate K(cid:48) as in Lemma 1  and set δ = r√
9   K =
T 2
3   ηk = 1
most

(k+2)2/3   then yt ∈ K ∀t  and the expected (1 − 1/e)-regret of Algorithm 2 is at

9   L = T 7

K   ρk =

T − 1

d+2

2

E[RT ] ≤N T

8
9 +

3r[2L2

1 + (3L2R + 2L1)2]
√
41/3(

T

2
3 +

L2D2

2

T

1
3  

where N = (1−1/e)r

√

d+2

√

[

d( R

r +1)+ R

r ]L1 + (2−1/e)r

√

d+2

d + 2)
√
L1 +2M1 + 3·41/6(

d+2)d2M 2
1
r

+ 3(

√

d+2)D2
4r

+C.

5 Bandit Submodular Set Maximization

In this section we aim to solve the problem of bandit submodular set maximization by lifting it to
the continuous domain. Let objective functions f1 ···   fT : 2Ω → R≥0 be a sequence of monotone
submodular set functions deﬁned on a common ground set Ω = {1  . . .   d}. We also let I denote the
matroid constraint  and K be the matroid polytope of I  i.e.  K = conv{1I : I ∈ I} ⊆ [0  1]d [16] 
where conv denotes the convex hull.

5.1 An Impossibility Result

A natural idea is that at each round t  we apply Bandit-Frank-Wolfe  the continuous algorithm
in Section 4.2  on Ft subject to K  where Ft is the multilinear extension of the discrete objective
function ft. Then we get a fractional solution yt ∈ K  round it to a set Yt ∈ I  and play Yt for ft.
For the exploitation phase  we will use a lossless rounding scheme such that ft(Yt) ≥ Ft(yt)  so we
will not get lower rewards after the rounding. Instances of such a lossless rounding scheme include
pipage rounding [4  16] and the contention resolution scheme [46].
In the exploration phase  we need to use the reward ft(Yt) to obtain an unbiased gradient estimator of
the smoothed version of Ft. As the one-point estimator d
δ F (x + δu)u in Algorithm 2 is unbiased  we

7

require the (random) rounding scheme roundI : [0  1]d → I to satisfy the following unbiasedness
condition

E[f (roundI(x))] = F (x) 

∀x ∈ [0  1]d

(2)

(3)

for any submodular set function f on the ground set Ω and its multilinear extension F .
Since we have no a priori knowledge of the objective function ft before playing a subset for it  such
a rounding scheme roundI should not depend on the function choice f. In other words  we need to
ﬁnd an independent roundI such that Eq. (2) holds for any submodular function f deﬁned on Ω.
We ﬁrst review the random rounding scheme RandRound : [0  1]d → I

(cid:26)i ∈ RandRound(x) with probability xi ;

i /∈ RandRound(x) with probability 1 − xi .

In other words  each element i ∈ Ω is included with an independent probability xi  where xi is the
i-th coordinate of x. RandRound satisﬁes the unbiasedness requirement Eq. (2). However  its range
is 2Ω in general  so the rounded set may fall outside of I. In fact  as shown in Lemma 2  there exists
a matroid I for which we cannot ﬁnd a proper unbiased rounding scheme whose range is contained
in I.
Lemma 2 (Proof in Appendix F). There exists a matroid I for which there is no rounding scheme
round : [0  1]d → I whose construction does not depend on the function f and which satisﬁes Eq. (2)
for any submodular set function f.

5.2 Responsive Bandit Algorithm

The impossibility result Lemma 2 shows that the one-point estimator may be incapable of solving the
general BSM problem. As a result  we study a slightly relaxed setting termed the responsive bandit
submodular maximization problem (RBSM). Let Xt be the subset that we play at the t-th round. The
only difference between the responsive bandit setting and the vanilla bandit setting is that in the
responsive setting  if Xt /∈ I  we can still observe the function value ft(Xt) as feedback  while the
received reward at round t is 0 (since the subset that we play violates the constraint I). In other words 
the environment is always responsive to the player’s decisions  no matter whether Xt is in I or not.
We note that the RBSM problem has broad applications in both theory and practice. In theory  RBSM
can be regarded as a relaxation of BSM  which helps us to better understand the nature of BSM. In
practice  the responsive model (not only for submodular maximization or bandit) has potentially many
applications when a decision cannot be committed  while we can still get the potential outcome of the
decision as feedback. For example  suppose that we have a replenishable inventory of items where
customers arrive (in an online fashion) with a utility function unknown to us. We need to allocate a
collection of items to each customer  and the goal is to maximize the total utility (reward) of all the
customers. We may use a partition matroid to model diversity (in terms of category  time  etc). In the
RBSM model  we cannot allocate the collection of items which violates the constraint to the customer 
but we can use it as a questionnaire  and the customer will tell us the potential utility if she received
those items. The feedback will help us to make better decisions in the future. Similar examples
include portfolio selection when the investment choice is too risky  i.e.  violates the recommended
constraint set  we may stop trading and thus get no reward on that trading period  but at the same
time observe the potential reward if we invested in that way.
Now  we turn to propose our algorithm. As discussed in Section 5.1  we want to solve the problem of
bandit submodular set maximization by applying Algorithm 2 on the multilinear extensions Ft with
different rounding schemes. Precisely  in the responsive setting  we use the RandRound Eq. (3) in
the exploration phase to guarantee that we can always obtain unbiased gradient estimators  and use
a lossless rounding scheme LosslessRound in the exploitation phase to receive high rewards. We
present Responsive-Frank-Wolfe in Algorithm 3  and show that it achieves a (1 − 1/e)-regret
bound of O(T 8/9).
Assumption 7. Every objective function ft is monotone submodular with supX⊆Ω|ft(X)|≤ M1.
Theorem 3 (Proof in Appendix G). Under Assumptions 4  5 and 7  if we generate K(cid:48) as in Lemma 1 
and set δ = r√
(k+2)2/3   then in the responsive setting 

9   K = T 2

9   L = T 7

T − 1

d+2

3   ηk = 1

K   ρk =

2

8

Algorithm 3 Responsive-Frank-Wolfe
Input: matroid constraint I  matroid polytope K  smoothing radius δ  δ-interior K(cid:48) with lower
bound u  horizon T   block size L  the number of exploration steps per block K  online linear
maximization oracles on K(cid:48): E (1) ···  E (K)  steps sizes ρk ∈ (0  1)  ηk ∈ (0  1)  the number of
blocks Q = T /L
Output: Y1  Y2  . . .
1: for q = 1  2  . . .   Q do
q ← 0  x(1)
q ← u
d(0)
2:
For k = 1  2  . . .   K  let v(k)
3:
q − u). Set xq ← x(K+1)
Let (tq 1  . . .   tq L) be a random permutation of {(q − 1)L + 1 ···   qL}
for t = (q − 1)L + 1 ···   qL do

q ∈ K(cid:48) be the output of E (k) in round q  x(k+1)

Ytq k(cid:48) = RandRound(ytq k(cid:48) ) for ft (i.e.  ftq k(cid:48) )  where ytq k(cid:48) = x(k(cid:48))
Yt ∈ I  get reward ft(Yt); otherwise  get reward 0.

If t ∈ {tq 1 ···   tq K}  ﬁnd the corresponding k(cid:48) ∈ [K] such that t = tq k(cid:48)  play Yt =
q + δuq k(cid:48)  uq k(cid:48) ∼ Sd−1. If
(cid:46) Exploration
If t ∈ {(q − 1)L + 1 ···   qL} \ {tq 1 ···   tq K}  play Yt = LosslessRound(yt) for ft 
(cid:46) Exploitation

← x(k)

ηk(v(k)

where yt = xq

4:
5:
6:

q +

q

q

end for
For k = 1  2  . . .   K  gq k ← d
  d(k)

q (cid:105) as reward for E (k)  and feed back d(k)

δ ftq k (Ytq k )uq k  d(k)
to E (k)

q

(cid:104)v(k)

q

q ← (1 − ρk)d(k−1)

q

+ ρkgq k  compute

7:

8:
9:

10: end for

the expected (1 − 1/e)-regret of Algorithm 3 is at most

E[RT ] ≤N T

8
9 +

where N = (1−1/e)r
L1 = 2M1

[ d
r +
d  L2 = 4M1

√

d+2

√

√

(cid:112)d(d − 1).

d(1 + 1

3r[2L2

1 + (3
41/3(
r )]L1 + (2−1/e)r

√

d+2

√
dL2 + 2L1)2]
√
d + 2)
√
L1 + 3M1 + 3·42/3(

2
3 +

T

L2d

2

T

1
3  

√

d+2)d2M 2
1
r

+ 3(

d+2)d
4r

+ C 

6 Conclusion

In this paper  by proposing a series of novel methods including the blocking procedure and the
permutation methods  we developed Mono-Frank-Wolfe for the OCSM problem  which requires only
one stochastic gradient evaluation per function and still achieves a (1− 1/e)-regret bound of O(T 4/5).
We then introduced the biphasic method and the notion of δ-interior  to extend Mono-Frank-Wolfe
to Bandit-Frank-Wolfe for the BCSM problem. Finally  we introduced the responsive model and
the corresponding Responsive-Frank-Wolfe Algorithm for the RBSM problem. We proved that
both Bandit-Frank-Wolfe and Responsive-Frank-Wolfe attain a (1 − 1/e)-regret bound of
O(T 8/9).

Acknowledgments

This work is partially supported by the Google PhD Fellowship  NSF (IIS-1845032)  ONR (N00014-
19-1-2406) and AFOSR (FA9550-18-1-0160). We would like to thank Marko Mitrovic for his
valuable comments and Zheng Wei for help preparing some of the illustrations.

References
[1] Jacob D Abernethy  Elad Hazan  and Alexander Rakhlin. Competing in the dark: An efﬁcient

algorithm for bandit linear optimization. In COLT  pages 263–274  2008.

[2] Alekh Agarwal  Ofer Dekel  and Lin Xiao. Optimal algorithms for online convex optimization

with multi-point bandit feedback. In COLT  pages 28–40. Citeseer  2010.

9

[3] Alekh Agarwal  Dean P Foster  Daniel J Hsu  Sham M Kakade  and Alexander Rakhlin.

Stochastic convex optimization with bandit feedback. In NIPS  pages 1035–1043  2011.

[4] Alexander A Ageev and Maxim I Sviridenko. Pipage rounding: A new method of constructing
algorithms with proven performance guarantee. Journal of Combinatorial Optimization  8(3):
307–328  2004.

[5] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing.

Journal of Computer and System Sciences  74(1):97–114  2008.

[6] Francis Bach. Submodular functions: from discrete to continous domains. arXiv preprint

arXiv:1511.00394  2015.

[7] Francis Bach  Rodolphe Jenatton  Julien Mairal  Guillaume Obozinski  et al. Optimization with
sparsity-inducing penalties. Foundations and Trends R(cid:13) in Machine Learning  4(1):1–106  2012.
[8] Eric Balkanski and Yaron Singer. Mechanisms for fair attribution. In Proceedings of the

Sixteenth ACM Conference on Economics and Computation  pages 529–546. ACM  2015.

[9] An Bian  Baharan Mirzasoleiman  Joachim M. Buhmann  and Andreas Krause. Guaranteed
non-convex optimization: Submodular maximization over continuous domains. In AISTATS 
February 2017.

[10] An Bian  Joachim M Buhmann  and Andreas Krause. Optimal dr-submodular maximization

and applications to provable mean ﬁeld inference. arXiv preprint arXiv:1805.07482  2018.

[11] Sébastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit

convex optimization. In COLT  pages 583–589  2016.

[12] Sébastien Bubeck  Nicolo Cesa-Bianchi  and Sham Kakade. Towards minimax policies for
online linear optimization with bandit feedback. In COLT  volume 23  pages 41.1–41.14  2012.

[13] Sébastien Bubeck  Nicolo Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122 
2012.
√

[14] Sébastien Bubeck  Ofer Dekel  Tomer Koren  and Yuval Peres. Bandit convex optimization:

T

regret in one dimension. In COLT  pages 266–278  2015.

[15] Sébastien Bubeck  Yin Tat Lee  and Ronen Eldan. Kernel-based methods for bandit convex

optimization. In STOC  pages 72–85. ACM  2017.

[16] Gruia Calinescu  Chandra Chekuri  Martin Pál  and Jan Vondrák. Maximizing a monotone
submodular function subject to a matroid constraint. SIAM Journal on Computing  40(6):
1740–1766  2011.

[17] Lin Chen  Christopher Harshaw  Hamed Hassani  and Amin Karbasi. Projection-free online
optimization with stochastic gradient: From convexity to submodularity. In ICML  page to
appear  2018.

[18] Lin Chen  Hamed Hassani  and Amin Karbasi. Online continuous submodular maximization.

In AISTATS  pages 1896–1905  2018.

[19] Lin Chen  Mingrui Zhang  Hamed Hassani  and Amin Karbasi. Black box submodular maxi-

mization: Discrete and continuous settings. arXiv preprint arXiv:1901.09515  2019.

[20] Lin Chen  Mingrui Zhang  and Amin Karbasi. Projection-free bandit convex optimization. In

AISTATS  pages 2047–2056  2019.

[21] Varsha Dani  Sham M Kakade  and Thomas P Hayes. The price of bandit information for online
optimization. In Advances in Neural Information Processing Systems  pages 345–352  2008.

[22] Ofer Dekel  Ronen Eldan  and Tomer Koren. Bandit smooth convex optimization: Improving

the bias-variance tradeoff. In NIPS  pages 2926–2934  2015.

10

[23] Abraham D Flaxman  Adam Tauman Kalai  and H Brendan McMahan. Online convex opti-
mization in the bandit setting: gradient descent without a gradient. In SODA  pages 385–394 
2005.

[24] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research

Logistics (NRL)  3(1-2):95–110  1956.

[25] Victor Gabillon  Branislav Kveton  Zheng Wen  Brian Eriksson  and S Muthukrishnan. Adaptive
submodular maximization in bandit setting. In Advances in Neural Information Processing
Systems  pages 2697–2705  2013.

[26] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in

active learning and stochastic optimization. JAIR  42:427–486  2011.

[27] Daniel Golovin  Andreas Krause  and Matthew Streeter. Online submodular maximization
under a matroid constraint with application to learning assignments. Technical report  arXiv 
2014.

[28] Elad Hazan and Satyen Kale. Projection-free online learning. In ICML  pages 1843–1850 

2012.

[29] Elad Hazan and Kﬁr Levy. Bandit convex optimization: Towards tight bounds. In NIPS  pages

784–792  2014.

[30] Elad Hazan and Yuanzhi Li. An optimal algorithm for bandit convex optimization. arXiv

preprint arXiv:1603.04350  2016.

[31] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning  69(2):169–192  2007.

[32] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R(cid:13) in

Optimization  2(3-4):157–325  2016.

[33] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML 

pages 427–435  2013.

[34] Sham M Kakade  Adam Tauman Kalai  and Katrina Ligett. Playing games with approximation

algorithms. SIAM Journal on Computing  39(3):1088–1106  2009.

[35] Robert D Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In NIPS 

pages 697–704  2005.

[36] Alex Kulesza  Ben Taskar  et al. Determinantal point processes for machine learning. Founda-

tions and Trends R(cid:13) in Machine Learning  5(2–3):123–286  2012.

[37] Aryan Mokhtari  Hamed Hassani  and Amin Karbasi. Conditional gradient method for stochastic

submodular maximization: Closing the gap. In AISTATS  pages 1886–1895  2018.

[38] Aryan Mokhtari  Hamed Hassani  and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554 
2018.

[39] George L Nemhauser  Laurence A Wolsey  and Marshall L Fisher. An analysis of approximations
for maximizing submodular set functions i. Mathematical Programming  14(1):265–294  1978.

[40] Ankan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimiza-

tion with bandit feedback. In AISTATS  pages 636–642  2011.

[41] Shai Shalev-Shwartz. Online learning: Theory  algorithms  and applications. PhD thesis  The

Hebrew University of Jerusalem  2007.

[42] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of online learning algorithms.

Machine Learning  69(2-3):115–142  2007.

11

[43] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.

In COLT  pages 3–24  2013.

[44] Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular

functions. In NIPS  pages 1577–1584  2009.

[45] Sebastian Tschiatschek  Rishabh K Iyer  Haochen Wei  and Jeff A Bilmes. Learning mixtures of
submodular functions for image collection summarization. In Advances in neural information
processing systems  pages 1413–1421  2014.

[46] Jan Vondrák  Chandra Chekuri  and Rico Zenklusen. Submodular function maximization via
the multilinear relaxation and contention resolution schemes. In STOC  pages 783–792. ACM 
2011.

[47] Kai Wei  Rishabh Iyer  and Jeff Bilmes. Submodularity in data subset selection and active

learning. In International Conference on Machine Learning  pages 1954–1963  2015.

[48] Baosheng Yu  Meng Fang  and Dacheng Tao. Linear submodular bandits with a knapsack

constraint. In Thirtieth AAAI Conference on Artiﬁcial Intelligence  2016.

[49] Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversiﬁed

retrieval. In NIPS  pages 2483–2491  2011.

[50] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In ICML  pages 928–936  2003.

12

,Wenruo Bai
William Stafford Noble
Jeff Bilmes
Mingrui Zhang
Lin Chen
Hamed Hassani
Amin Karbasi