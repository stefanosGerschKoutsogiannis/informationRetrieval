2017,SGD Learns the Conjugate Kernel Class of the Network,We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn  in polynomial time  a function that is competitive with the best function in the conjugate kernel space of the network  as defined in Daniely  Frostig and Singer. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge  it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two.  As corollaries  it follows that for neural networks of any depth between 2 and log(n)  SGD is guaranteed to learn  in polynomial time  constant degree polynomials with polynomially bounded coefficients. Likewise  it follows  that SGD on large enough networks can learn any continuous function (not in polynomial time)  complementing classical expressivity results.,SGD Learns the Conjugate Kernel Class of the

Network

Amit Daniely

Hebrew University and Google Research

amit.daniely@mail.huji.ac.il

Abstract

We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn 
in polynomial time  a function that is competitive with the best function in the conjugate
kernel space of the network  as deﬁned in Daniely et al. [2016]. The result holds for log-
depth networks from a rich family of architectures. To the best of our knowledge  it is
the ﬁrst polynomial-time guarantee for the standard neural network learning algorithm for
networks of depth more that two.
As corollaries  it follows that for neural networks of any depth between 2 and log(n)  SGD
is guaranteed to learn  in polynomial time  constant degree polynomials with polynomially
bounded coefﬁcients. Likewise  it follows that SGD on large enough networks can learn any
continuous function (not in polynomial time)  complementing classical expressivity results.

1

Introduction

While stochastic gradient decent (SGD) from a random initialization is probably the most popular
supervised learning algorithm today  we have very few results that depicts conditions that guarantee
its success. Indeed  to the best of our knowledge  Andoni et al. [2014] provides the only known result
of this form  and it is valid in a rather restricted setting. Namely  for depth-2 networks  where the
underlying distribution is Gaussian  the algorithm is full gradient decent (rather than SGD)  and the
task is regression when the learnt function is a constant degree polynomial.
We build on the framework of Daniely et al. [2016] to establish guarantees on SGD in a rather
general setting. Daniely et al. [2016] deﬁned a framework that associates a reproducing kernel to a
network architecture. They also connected the kernel to the network via the random initialization.
Namely  they showed that right after the random initialization  any function in the kernel space can
be approximated by changing the weights of the last layer. The quality of the approximation depends
on the size of the network and the norm of the function in the kernel space.
As optimizing the last layer is a convex procedure  the result of Daniely et al. [2016] intuitively
shows that the optimization process starts from a favourable point for learning a function in the
conjugate kernel space. In this paper we verify this intuition. Namely  for a fairly general family of
architectures (that contains fully connected networks and convolutional networks) and supervised
learning tasks  we show that if the network is large enough  the learning rate is small enough  and
the number of SGD steps is large enough as well  SGD is guaranteed to learn any function in the
corresponding kernel space. We emphasize that the number of steps and the size of the network are
only required to be polynomial (which is best possible) in the relevant parameters – the norm of the
function  the required accuracy parameter (�)  and the dimension of the input and the output of the
network. Likewise  the result holds for any input distribution.
To evaluate our result  one should understand which functions it guarantee that SGD will learn.
Namely  what functions reside in the conjugate kernel space  how rich it is  and how good those
functions are as predictors. From an empirical perspective  in [Daniely et al.  2017]  it is shown that
for standard convolutional networks the conjugate class contains functions whose performance is

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

close to the performance of the function that is actually learned by the network. This is based on
experiments on the standard CIFAR-10 dataset. From a theoretical perspective  we list below a few
implications that demonstrate the richness of the conjugate kernel space. These implications are valid
for fully connected networks of any depth between 2 and log(n)  where n is the input dimension.
Likewise  they are also valid for convolutional networks of any depth between 2 and log(n)  and with
constantly many convolutional layers.

• SGD is guaranteed to learn in polynomial time constant degree polynomials with polynomi-
ally bounded coefﬁcients. As a corollary  SGD is guaranteed to learn in polynomial time
conjunctions  DNF and CNF formulas with constantly many terms  and DNF and CNF
formulas with constantly many literals in each term. These function classes comprise a con-
siderable fraction of the function classes that are known to be poly-time (PAC) learnable by
any method. Exceptions include constant degree polynomial thresholds with no restriction
on the coefﬁcients  decision lists and parities.

• SGD is guaranteed to learn  not necessarily in polynomial time  any continuous function.
This complements classical universal approximation results that show that neural networks
can (approximately) express any continuous function (see Scarselli and Tsoi [1998] for a
survey). Our results strengthen those results and show that networks are not only able to
express those functions  but actually guaranteed to learn them.

1.1 Related work

Guarantees on SGD. As noted above  there are very few results that provide polynomial time
guarantees for SGD on NN. One notable exception is the work of Andoni et al. [2014]  that proves
a result that is similar to ours  but in a substantially more restricted setting. Concretely  their result
holds for depth-2 fully connected networks  as opposed to rather general architecture and constant or
logarithmic depth in our case. Likewise  the marginal distribution on the instance space is assumed to
be Gaussian or uniform  as opposed to arbitrary in our case. In addition  the algorithm they consider
is full gradient decent  which corresponds to SGD with inﬁnitely large mini-batch  as opposed to
SGD with arbitrary mini-batch size in our case. Finally  the underlying task is regression in which
the target function is a constant degree polynomial  whereas we consider rather general supervised
learning setting.

Other polynomial time guarantees on learning deep architectures. Various recent papers show
that poly-time learning is possible in the case that the the learnt function can be realized by a neural
network with certain (usually fairly strong) restrictions on the weights [Livni et al.  2014  Zhang
et al.  2016a  2015  2016b]  or under the assumption that the data is generated by a generative model
that is derived from the network architecture [Arora et al.  2014  2016]. We emphasize that the
main difference of those results from our results and the results of Andoni et al. [2014] is that they
do not provide guarantees on the standard SGD learning algorithm. Rather  they show that under
those aforementioned conditions  there are some algorithms  usually very different from SGD on the
network  that are able to learn in polynomial time.

Connection to kernels. As mentioned earlier  our paper builds on Daniely et al. [2016]  who
developed the association of kernels to NN which we rely on. Several previous papers [Mairal et al. 
2014  Cho and Saul  2009  Rahimi and Recht  2009  2007  Neal  2012  Williams  1997  Kar and
Karnick  2012  Pennington et al.  2015  Bach  2015  2014  Hazan and Jaakkola  2015  Anselmi et al. 
2015] investigated such associations  but in a more restricted settings (i.e.  for less architectures).
Some of those papers [Rahimi and Recht  2009  2007  Daniely et al.  2016  Kar and Karnick 
2012  Bach  2015  2014] also provide measure of concentration results  that show that w.h.p. the
random initialization of the network’s weights is reach enough to approximate the functions in the
corresponding kernel space. As a result  these papers provide polynomial time guarantees on the
variant of SGD  where only the last layer is trained. We remark that with the exception of Daniely
et al. [2016]  those results apply just to depth-2 networks.

1.2 Discussion and future directions

We next want to place this work in the appropriate learning theoretic context  and to elaborate further
on this paper’s approach for investigating neural networks. For the sake of concreteness  let us

2

restrict the discussion to binary classiﬁcation over the Boolean cube. Namely  given examples from
a distribution D on {±1}n × {0  1}  the goal is to learn a function h : {±1}n → {0  1} whose 0-1
error  L0−1
D (h) = Pr(x y)∼D (h(x) �= y)  is as small as possible. We will use a bit of terminology.
A model is a distribution D on {±1}n × {0  1} and a model class is a collection M of models. We
note that any function class H ⊂ {0  1}{±1}n deﬁnes a model class  M(H)  consisting of all models
D such that L0−1
D (h) = 0 for some h ∈ H. We deﬁne the capacity of a model class as the minimal
number m for which there is an algorithm such that for every D ∈ M the following holds. Given m
samples from D  the algorithm is guaranteed to return  w.p. ≥ 9
10 over the samples and its internal
randomness  a function h : {±1}n → {0  1} with 0-1 error ≤ 1
10 . We note that for function classes
the capacity is the VC dimension  up to a constant factor.
Learning theory analyses learning algorithms via model classes. Concretely  one ﬁxes some model
class M and show that the algorithm is guaranteed to succeed whenever the underlying model is from
M. Often  the connection between the algorithm and the class at hand is very clear. For example  in
the case that the model is derived from a function class H  the algorithm might simply be one that
ﬁnds a function in H that makes no mistake on the given sample. The natural choice for a model class
for analyzing SGD on NN would be the class of all functions that can be realized by the network 
possibly with some reasonable restrictions on the weights. Unfortunately  this approach it is probably
doomed to fail  as implied by various computational hardness results [Blum and Rivest  1989  Kearns
and Valiant  1994  Blum et al.  1994  Kharitonov  1993  Klivans and Sherstov  2006  2007  Daniely
et al.  2014  Daniely and Shalev-Shwartz  2016].
So  what model classes should we consider? With a few isolated exceptions (e.g. Bshouty et al.
[1998]) all known efﬁciently learnable model classes are either a linear model class  or contained in
an efﬁciently learnable linear model class. Namely  functions classes composed of compositions of
some predeﬁned embedding with linear threshold functions  or linear functions over some ﬁnite ﬁeld.
Coming up we new tractable models would be a fascinating progress. Still  as linear function classes
are the main tool that learning theory currently has for providing guarantees on learning  it seems
natural to try to analyze SGD via linear model classes. Our work follows this line of thought  and
we believe that there is much more to achieve via this approach. Concretely  while our bounds are
polynomial  the degree of the polynomials is rather large  and possibly much better quantitative
bounds can be achieved. To be more concrete  suppose that we consider simple fully connected
architecture  with 2-layers  ReLU activation  and n hidden neurons. In this case  the capacity of the

3�. For comparison  the capacity
model class that our results guarantee that SGD will learn is Θ�n
of the class of all functions that are realized by this network is Θ�n2�. As a challenge  we encourage

the reader to prove that with this architecture (possibly with an activation that is different from the
ReLU)  SGD is guaranteed to learn some model class of capacity that is super-linear in n.

1

2 Preliminaries

Notation. We denote vectors by bold-face letters (e.g. x)  matrices by upper case letters (e.g. W ) 
and collection of matrices by bold-face upper case letters (e.g. W). The p-norm of x ∈ Rd is denoted
p . We will also use the convention that �x� = �x�2. For functions

by �x�p = ��d

σ : R → R we let

i=1 |xi|p� 1
�σ� :=�EX∼N (0 1) σ2(X) =� 1√2π� ∞

−∞

σ2(x)e− x2

2 dx .

Let G = (V  E) be a directed acyclic graph. The set of neighbors incoming to a vertex v is
denoted in(v) := {u ∈ V | uv ∈ E}. We also denote deg(v) = |in(v)|. Given weight function
δ : V → [0 ∞) and U ⊂ V we let δ(U ) =�u∈U δ(u). The d − 1 dimensional sphere is denoted
Sd−1 = {x ∈ Rd | �x� = 1}. We use [x]+ to denote max(x  0).
Input space. Throughout the paper we assume that each example is a sequence of n elements 
each of which is represented as a unit vector. Namely  we ﬁx n and take the input space to be

X = Xn d =�Sd−1�n. Each input example is denoted 

x = (x1  . . .   xn)  where xi ∈ Sd−1 .

(1)

3

While this notation is slightly non-standard  it uniﬁes input types seen in various domains (see Daniely
et al. [2016]).

m�m

Supervised learning. The goal in supervised learning is to devise a mapping from the input
space X to an output space Y based on a sample S = {(x1  y1)  . . .   (xm  ym)}  where (xi  yi) ∈
X × Y drawn i.i.d. from a distribution D over X × Y. A supervised learning problem is further
speciﬁed by an output length k and a loss function � : Rk × Y → [0 ∞)  and the goal is to ﬁnd
a predictor h : X → Rk whose loss  LD(h) := E(x y)∼D �(h(x)  y)  is small. The empirical loss
i=1 �(h(xi)  yi) is commonly used as a proxy for the loss LD. When h is deﬁned
LS(h) := 1
by a vector w of parameters  we will use the notations LD(w) = LD(h)  LS(w) = LS(h) and
�(x y)(w) = �(h(x)  y).
Regression problems correspond to k = 1  Y = R and  for instance  the squared loss �square(ˆy  y) =
(ˆy − y)2. Binary classiﬁcation is captured by k = 1  Y = {±1} and  say  the zero-one loss
�0−1(ˆy  y) = 1[ˆyy ≤ 0] or the hinge loss �hinge(ˆy  y) = [1 − ˆyy]+. Multiclass classiﬁcation is
captured by k being the number of classes  Y = [k]  and  say  the zero-one loss �0−1(ˆy  y) =
1[ˆyy ≤ argmaxy� ˆyy� ] or the logistic loss �log(ˆy  y) = − log (py(ˆy)) where p : Rk → Δk−1 is
given by pi(ˆy) = e ˆyi
j=1 e ˆyj . A loss � is L-Lipschitz if for all y ∈ Y  the function �y(ˆy) := �(ˆy  y) is
�k
L-Lipschitz. Likewise  it is convex if �y is convex for every y ∈ Y.
Neural network learning. We deﬁne a neural network N to be a vertices weighted directed acyclic
graph (DAG) whose nodes are denoted V (N ) and edges E(N ). The weight function will be denoted
by δ : V (N ) → [0 ∞)  and its sole role would be to dictate the distribution of the initial weights.
We will refer N ’s nodes by neurons. Each of non-input neuron  i.e. neuron with incoming edges  is
associated with an activation function σv : R → R. In this paper  an activation can be any function
σ : R → R that is right and left differentiable  square integrable with respect to the Gaussian measure
on R  and is normalized in the sense that �σ� = 1. The set of neurons having only incoming edges
are called the output neurons. To match the setup of supervised learning deﬁned above  a network
N has nd input neurons and k output neurons  denoted o1  . . .   ok. A network N together with
a weight vector w = {wuv | uv ∈ E} ∪ {bv | v ∈ V is an internal neuron} deﬁnes a predictor
hN  w : X → Rk whose prediction is given by “propagating” x forward through the network.
Concretely  we deﬁne hv w(·) to be the output of the subgraph of the neuron v as follows: for an
input neuron v  hv w outputs the corresponding coordinate in x  and internal neurons  we deﬁne hv w
recursively as

Finally  we let hN  w(x) = (ho1 w(x)  . . .   hok w(x)).
We next describe the learning algorithm that we analyze in this paper. While there is no standard
training algorithm for neural networks  the algorithms used in practice are usually quite similar
to the one we describe  both in the way the weights are initialized and the way they are updated.
We will use the popular Xavier initialization [Glorot and Bengio  2010] for the network weights.
Fix 0 ≤ β ≤ 1. We say that w0 = {w0
uv}uv∈E ∪ {bv}v∈V is an internal neuron are β-biased random
weights (or  β-biased random initialization) if each weight wuv is sampled independently from a
normal distribution with mean 0 and variance (1 − β)dδ(u)/δ(in(v)) if u is an input neuron and
(1 − β)δ(u)/δ(in(v)) otherwise. Finally  each bias term bv is sampled independently from a normal
distribution with mean 0 and variance β. We note that the rational behind this initialization scheme is
that for every example x and every neuron v we have Ew0 (hv w0 (x))2 = 1 (see Glorot and Bengio
[2010])

Kernel classes. A function κ : X × X → R is a reproducing kernel  or simply a kernel  if
for every x1  . . .   xr ∈ X   the r × r matrix Γi j = {κ(xi  xj)} is positive semi-deﬁnite. Each
kernel induces a Hilbert space Hκ of functions from X to R with a corresponding norm � · �κ. For
κ. A kernel and its corresponding space are normalized if
h ∈ Hk
∀x ∈ X   κ(x  x) = 1.

κ we denote �h�κ =��k

i=1 �hi�2

4

For output neurons  we deﬁne hv w as

hv w(x) = σv��u∈in(v) wuv hu w(x) + bv� .

hv w(x) =�u∈in(v) wuv hu w(x) .

Algorithm 1 Generic Neural Network Training

Input: Network N   learning rate η > 0  batch size m  number of steps T > 0  bias parameter
0 ≤ β ≤ 1  ﬂag zero prediction layer ∈ {True  False}.
Let w0 be β-biased random weights
if zero prediction layer then

Set w0

uv = 0 whenever v is an output neuron

end if
for t = 1  . . .   T do

Obtain a mini-batch St = {(xt
Using back-propagation  calculate a stochastic gradient vt = ∇LSt (wt)
Update wt+1 = wt − ηvt

i=1 ∼ Dm

i )}m

i  yt

end for

S1

S2

S3
Figure 1: Examples of computation skeletons.

S4

Kernels give rise to popular benchmarks for learning algorithms. Fix a normalized kernel κ and
M > 0. It is well known that that for L-Lipschitz loss �  the SGD algorithm is guaranteed to return
� �2 examples. In the
a function h such that ELD(h) ≤ minh�∈Hk
context of multiclass classiﬁcation  for γ > 0 we deﬁne �γ : Rk × [k] → R by �γ(ˆy  y) = 1[ˆyy ≤
γ + maxy��=y ˆyy� ]. We say that a distribution D on X × [k] is M-separable w.r.t. κ if there is h∗ ∈ Hk
such that �h∗�κ ≤ M and L1
D(h∗) = 0. In this case  the perceptron algorithm is guaranteed to return
D (h) ≤ � using 2M 2
a function h such that EL0−1
examples. We note that both for perceptron and
SGD  the above mentioned results are best possible  in the sense that any algorithm with the same
guarantees  will have to use at least the same number of examples  up to a constant factor.

κ  �h��κ≤M LD(h�) + � using� LM

κ

�

Computation skeletons [Daniely et al.  2016]
In this section we deﬁne a simple structure which
we term a computation skeleton. The purpose of a computational skeleton is to compactly describe
a feed-forward computation from an input to an output. A single skeleton encompasses a family
of neural networks that share the same skeletal structure. Likewise  it deﬁnes a corresponding
normalized kernel.
Deﬁnition 1. A computation skeleton S is a DAG with n inputs  whose non-input nodes are labeled
by activations  and has a single output node out(S).
Figure 1 shows four example skeletons  omitting the designation of the activation functions. We
denote by |S| the number of non-input nodes of S. The following deﬁnition shows how a skeleton 
accompanied with a replication parameter r ≥ 1 and a number of output nodes k  induces a neural
network architecture.

5

S

N (S  5  4)

Figure 2: A (5  4)-realization of the computation skeleton S with d = 2.

Deﬁnition 2 (Realization of a skeleton). Let S be a computation skeleton and consider input
coordinates in Sd−1 as in (1). For r  k ≥ 1 we deﬁne the following neural network N = N (S  r  k).
For each input node in S  N has d corresponding input neurons with weight 1/d. For each internal
node v ∈ S labelled by an activation σ  N has r neurons v1  . . .   vr  each with an activation σ and
weight 1/r. In addition  N has k output neurons o1  . . .   ok with the identity activation σ(x) = x
and weight 1. There is an edge viuj ∈ E(N ) whenever uv ∈ E(S). For every output node v in S 
each neuron vj is connected to all output neurons o1  . . .   ok. We term N the (r  k)-fold realization
of S.
Note that the notion of the replication parameter r corresponds  in the terminology of convolutional
networks  to the number of channels taken in a convolutional layer and to the number of hidden
neurons taken in a fully-connected layer.
In addition to networks’ architectures  a computation skeleton S also deﬁnes a normalized kernel
κS : X × X → [−1  1]. To deﬁne the kernel  we use the notion of a conjugate activation. For
ρ ∈ [−1  1]  we denote by Nρ the multivariate Gaussian distribution on R2 with mean 0 and covariance
matrix� 1 ρ
ρ 1�.
Deﬁnition 3 (Conjugate activation). The conjugate activation of an activation σ is the function
ˆσ : [−1  1] → R deﬁned as ˆσ(ρ) = E(X Y )∼Nρ σ(X)σ(Y ) .
The following deﬁnition gives the kernel corresponding to a skeleton
Deﬁnition 4 (Compositional kernels). Let S be a computation skeleton and let 0 ≤ β ≤ 1. For every
node v  inductively deﬁne a kernel κβ
v : X × X → R as follows. For an input node v corresponding
to the ith coordinate  deﬁne κβ

v (x  y) = �xi  yi�. For a non-input node v  deﬁne
+ β� .

v (x  y) = ˆσv�(1 − β)�u∈in(v) κβ
out(S). The resulting Hilbert space and norm are denoted HS β and � · �S β

|in(v)|

κβ

u(x  y)

The ﬁnal kernel κβ
S
respectively.

is κβ

3 Main results

An activation σ : R → R is called C-bounded if �σ�∞ �σ��∞ �σ���∞ ≤ C. Fix a skeleton S
and 1-Lipschitz1 convex loss �. Deﬁne comp(S) =�depth(S)
maxv∈S depth(v)=i(deg(v) + 1) and
C(S) = (8C)depth(S)�comp(S)  where C is the minimal number for which all the activations in S
are C-bounded  and depth(v) is the maximal length of a path from an input node to v. We also deﬁne
C�(S) = (4C)depth(S)�comp(S)  where C is the minimal number for which all the activations in
S are C-Lipschitz and satisfy |σ(0)| ≤ C. Through this and remaining sections we use � to hide
universal constants. Likewise  we ﬁx the bias parameter β and therefore omit it from the relevant
notation.

i=1

1If � is L-Lipschitz  we can replace � by 1

L � and the learning rate η by Lη. The operation of algorithm 1 will
be identical to its operation before the modiﬁcation. Given this observation  it is very easy to derive results for
general L given our results. Hence  to save one paramater  we will assume that L = 1.

6

We note that for constant depth skeletons with maximal degree that is polynomial in n  C(S) and
C�(S) are polynomial in n. These quantities are polynomial in n also for various log-depth skeletons.
For example  this is true for fully connected skeletons  or more generally  layered skeletons with
constantly many layers that are not fully connected.
Theorem 1. Suppose that all activations are C-bounded. Let M  � > 0. Suppose that we run
algorithm 1 on the network N (S  r  k) with the following parameters:

�

η��

(C�(S))2

r for η� �

• η = η�
• T � M 2
• r � C4(T η�)2M 2(C�(S))4
• Zero initialized prediction layer
• Arbitrary m

�2

log( C|S|�δ )

+ d

Then  w.p. ≥ 1 − δ over the choice of the initial weights  there is t ∈ [T ] such that ELD(wt) ≤
  �h�S≤M LD(h) + �. Here  the expectation is over the training examples.
minh∈Hk
S
We next consider ReLU activations. Here  C�(S) = (√32)depth(S)�comp(S).
Theorem 2. Suppose that all activations are the ReLU. Let M  � > 0. Suppose that we run algorithm
1 on the network N (S  r  k) with the following parameters:

�

η��

(C�(S))2

r for η� �

• η = η�
• T � M 2
• r � (T η�)2M 2(C�(S))4
+ d
• Zero initialized prediction layer
• Arbitrary m

log( |S|�δ )

�2

  �h�S≤M LD(h) + �. Here  the expectation is over the training examples.

Then  w.p. ≥ 1 − δ over the choice of the initial weights  there is t ∈ [T ] such that ELD(wt) ≤
minh∈Hk
S
Finally  we consider the case in which the last layer is also initialized randomly. Here  we provide
guarantees in a more restricted setting of supervised learning. Concretely  we consider multiclass
classiﬁcation  when D is separable with margin  and � is the logistic loss.
Theorem 3. Suppose that all activations are C-bounded  that D is M-separable with w.r.t. κS and
let � > 0. Suppose we run algorithm 1 on N (S  r  k) with the following parameters:

�2

η��2

M 2(C(S))4

• η = η�
r for η� �
• T � log(k)M 2
• r � C 4 (C(S))4 M 2 (T η�)2 log� C|S|� � + k + d
• Randomly initialized prediction layer
• Arbitrary m

Then  w.p. ≥ 1
that L0−1

D (wt) ≤ �

4 over the choice of the initial weights and the training examples  there is t ∈ [T ] such

7

3.1 Implications

To demonstrate our results  let us elaborate on a few implications for speciﬁc network architectures.
To this end  let us ﬁx the instance space X to be either {±1}n or Sn−1. Also  ﬁx a bias parameter
1 ≥ β > 0  a batch size m  and a skeleton S that is a skeleton of a fully connected network of
depth between 2 and log(n). Finally  we also ﬁx the activation function to be either the ReLU or a
C-bounded activation  assume that the prediction layer is initialized to 0  and ﬁx the loss function to
be some convex and Lipschitz loss function. Very similar results are valid for convolutional networks
with constantly many convolutional layers. We however omit the details for brevity.
Our ﬁrst implication shows that SGD is guaranteed to efﬁciently learn constant degree polynomials
with polynomially bounded weights. To this end  let us denote by Pt the collection of degree t
polynomials. Furthermore  for any polynomial p we denote by �p� the �2 norm of its coefﬁcients.
Corollary 4. Fix any positive integers t0  t1. Suppose that we run algorithm 1 on the network
N (S  r  1) with the following parameters:

• η � poly� �
n�
• T  r � poly� n

�   log (1/δ)�

Then  w.p. ≥ 1 − δ over the choice of the initial weights  there is t ∈ [T ] such that ELD(wt) ≤
minp∈Pt0   �p�≤nt1 LD(p) + �. Here  the expectation is over the training examples.
We note that several hypothesis classes that were studied in PAC learning can be realized by polyno-
mial threshold functions with polynomially bounded coefﬁcients. This includes conjunctions  DNF
and CNF formulas with constantly many terms  and DNF and CNF formulas with constantly many
literals in each term. If we take the loss function to be the logistic loss or the hinge loss  Corollary 4
implies that SGD efﬁciently learns these hypothesis classes as well.
Our second implication shows that any continuous function is learnable (not necessarily in polynomial
time) by SGD.
Corollary 5. Fix a continuous function h∗ : Sn−1 → R and �  δ > 0. Assume that D is realized2 by
h∗. Assume that we run algorithm 1 on the network N (S  r  1). If η > 0 is sufﬁciently small and T
and r are sufﬁciently large  then  w.p. ≥ 1 − δ over the choice of the initial weights  there is t ∈ [T ]
such that ELD(wt) ≤ �.
3.2 Extensions

We next remark on two extensions of our main results. The extended results can be proved in a similar
fashion to our results. To avoid cumbersome notation  we restrict the proofs to the main theorems as
stated  and will elaborate on the extended results in an extended version of this manuscript. First  we
assume that the replication parameter is the same for all nodes. In practice  replication parameters for
different nodes are different. This can be captured by a vector {rv}v∈Int(S). Our main results can
be extended to this case if for all v  rv ≤�u∈in(v) ru (a requirement that usually holds in practice).
Second  we assume that there is no weight sharing that is standard in convolutional networks. Our
results can be extended to convolutional networks with weight sharing.
We also note that we assume that in each step of algorithm 1  a fresh batch of examples is given. In
practice this is often not the case. Rather  the algorithm is given a training set of examples  and at
each step it samples from that set. In this case  our results provide guarantees on the training loss.
If the training set is large enough  this also implies guarantees on the population loss via standard
sample complexity results.

Acknowledgments

The author thanks Roy Frostig  Yoram Singer and Kunal Talwar for valuable discussions and
comments.

2That is  if (x  y) ∼ D then y = h∗(x) with probability 1.

8

References
A. Andoni  R. Panigrahy  G. Valiant  and L. Zhang. Learning polynomials with neural networks. In
Proceedings of the 31st International Conference on Machine Learning  pages 1908–1916  2014.

F. Anselmi  L. Rosasco  C. Tan  and T. Poggio. Deep convolutional networks are hierarchical kernel

machines. arXiv:1508.01084  2015.

Sanjeev Arora  Aditya Bhaskara  Rong Ge  and Tengyu Ma. Provable bounds for learning some deep

representations. In ICML  pages 584–592  2014.

Sanjeev Arora  Rong Ge  Tengyu Ma  and Andrej Risteski. Provable learning of noisy-or networks.

arXiv preprint arXiv:1612.08795  2016.

F. Bach. Breaking the curse of dimensionality with convex neural networks. arXiv:1412.8690  2014.

F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. 2015.

A. Blum  M. Furst  J. Jackson  M. Kearns  Y. Mansour  and Steven Rudich. Weakly learning DNF and
characterizing statistical query learning using fourier analysis. In Proceedings of the twenty-sixth
annual ACM symposium on Theory of computing  pages 253–262. ACM  1994.

Avrim Blum and Ronald L. Rivest. Training a 3-node neural net is NP-Complete. In David S.
Touretzky  editor  Advances in Neural Information Processing Systems I  pages 494–501. Morgan
Kaufmann  1989.

Nader H Bshouty  Christino Tamon  and David K Wilson. On learning width two branching programs.

Information Processing Letters  65(4):217–222  1998.

Y. Cho and L.K. Saul. Kernel methods for deep learning.

processing systems  pages 342–350  2009.

In Advances in neural information

A. Daniely and S. Shalev-Shwartz. Complexity theoretic limitations on learning DNFs. In COLT 

2016.

A. Daniely  N. Linial  and S. Shalev-Shwartz. From average case complexity to improper learning

complexity. In STOC  2014.

Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural networks: The

power of initialization and a dual view on expressivity. In NIPS  2016.

Amit Daniely  Roy Frostig  Vineet Gupta  and Yoram Singer. Random features for compositional

kernels. arXiv preprint arXiv:1703.07872  2017.

X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks.

In International conference on artiﬁcial intelligence and statistics  pages 249–256  2010.

T. Hazan and T. Jaakkola. Steps toward deep kernel methods from inﬁnite neural networks.

arXiv:1508.05133  2015.

Gautam C Kamath. Bounds on the expectation of the maximum of samples from a gaussian  2015.

URL http://www. gautamkamath. com/writings/gaussian max. pdf.

P. Kar and H. Karnick. Random feature maps for dot product kernels. arXiv:1201.6530  2012.

M. Kearns and L.G. Valiant. Cryptographic limitations on learning Boolean formulae and ﬁnite

automata. Journal of the Association for Computing Machinery  41(1):67–95  January 1994.

Michael Kharitonov. Cryptographic hardness of distribution-speciﬁc learning. In Proceedings of the

twenty-ﬁfth annual ACM symposium on Theory of computing  pages 372–381. ACM  1993.

A.R. Klivans and A.A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. In

FOCS  2006.

A.R. Klivans and A.A. Sherstov. Unconditional lower bounds for learning intersections of halfspaces.

Machine Learning  69(2-3):97–114  2007.

9

,Amit Daniely