2013,Phase Retrieval using Alternating Minimization,Phase retrieval problems involve solving linear equations  but with missing sign (or phase  for complex numbers) information. Over the last two decades  a popular generic empirical approach to the many variants of this problem has been one of alternating minimization; i.e. alternating between estimating the missing phase information  and the candidate solution. In this paper  we show that a simple alternating minimization algorithm geometrically converges to the solution of one such problem -- finding a vector $x$ from $y A$  where $y = |A'x|$ and $|z|$ denotes a vector of element-wise magnitudes of $z$ -- under the assumption that $A$ is Gaussian.  Empirically  our algorithm performs similar to recently proposed convex techniques for this variant (which are based on lifting" to a convex matrix problem) in sample complexity and robustness to noise. However  our algorithm is much more efficient and can scale to large problems. Analytically  we show geometric convergence to the solution  and sample complexity that is off by log factors from obvious lower bounds. We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the only known proof of alternating minimization for any variant of phase retrieval problems in the non-convex setting.",Phase Retrieval using Alternating Minimization

Praneeth Netrapalli
Department of ECE

The University of Texas at Austin

Austin  TX 78712

praneethn@utexas.edu

Prateek Jain

Microsoft Research India

Bangalore  India

prajain@microsoft.com

Sujay Sanghavi

Department of ECE

The University of Texas at Austin

Austin  TX 78712

sanghavi@mail.utexas.edu

Abstract

Phase retrieval problems involve solving linear equations  but with missing sign
(or phase  for complex numbers). Over the last two decades  a popular generic em-
pirical approach to the many variants of this problem has been one of alternating
minimization; i.e. alternating between estimating the missing phase information 
and the candidate solution. In this paper  we show that a simple alternating min-
imization algorithm geometrically converges to the solution of one such problem

– ﬁnding a vector x from y  A  where y = |AT x| and |z| denotes a vector of

element-wise magnitudes of z – under the assumption that A is Gaussian.
Empirically  our algorithm performs similar to recently proposed convex tech-
niques for this variant (which are based on “lifting” to a convex matrix problem)
in sample complexity and robustness to noise. However  our algorithm is much
more efﬁcient and can scale to large problems. Analytically  we show geometric
convergence to the solution  and sample complexity that is off by log factors from
obvious lower bounds. We also establish close to optimal scaling for the case
when the unknown vector is sparse. Our work represents the only known the-
oretical guarantee for alternating minimization for any variant of phase retrieval
problems in the non-convex setting.

1

Introduction

In this paper we are interested in recovering a complex1 vector x∗ ∈ Cn from magnitudes of its
linear measurements. That is  for ai ∈ Cn  if

(1)

then the task is to recover x∗ using y and the measurement matrix A = [a1 a2 . . . am].

yi = |hai  x∗i| 

for i = 1  . . .   m

The above problem arises in many settings where it is harder / infeasible to record the phase of mea-
surements  while recording the magnitudes is signiﬁcantly easier. This problem  known as phase
retrieval  is encountered in several applications in crystallography  optics  spectroscopy and tomog-
raphy [14]. Moreover  the problem is broadly studied in the following two settings:

(i) The measurements in (1) correspond to the Fourier transform (the number of measurements

here is equal to n) and there is some apriori information about the signal.

1Our results also cover the real case  i.e. where all quantities are real.

1

(ii) The set of measurements y are overcomplete (i.e.  m > n)  while some apriori information

about the signal may or may not be available.

In the ﬁrst case  various types of apriori information about the underlying signal such as positivity 
magnitude information on the signal [11]  sparsity [25] and so on have been studied. In the second
case  algorithms for various measurement schemes such as Fourier oversampling [21]  multiple
random illuminations [4  28] and wavelet transform [28] have been suggested.

By and large  the most well known methods for solving this problem are the error reduction algo-
rithms due to Gerchberg and Saxton [13] and Fienup [11]  and variants thereof. These algorithms
are alternating projection algorithms that iterate between the unknown phases of the measurements
and the unknown underlying vector. Though the empirical performance of these algorithms has been
well studied [11  19]  and they are used in many applications [20]  there are not many theoretical
guarantees regarding their performance.

More recently  a line of work [7  6  28] has approached this problem from a different angle  based
on the realization that recovering x∗ is equivalent to recovering the rank-one matrix x∗x∗T   i.e.  its
outer product. Inspired by the recent literature on trace norm relaxation of the rank constraint  they
design SDPs to solve this problem. Refer Section 1.1 for more details.

In this paper we go back to the empirically more popular ideology of alternating minimization;
we develop a new alternating minimization algorithm  for which we show that (a) empirically  it
noticeably outperforms convex methods  and (b) analytically  a natural resampled version of this
algorithm requires O(n log3 n) i.i.d. random Gaussian measurements to geometrically converge to
the true vector.
Our contribution:

• The iterative part of our algorithm is implicit in previous work [13  11  28  4]; the novelty
in our algorithmic contribution is the initialization step which makes it more likely for the
iterative procedure to succeed - see Figures 1 and 2.

alternating minimization for the phase retrieval problem in a non-convex setting.

• Our analytical contribution is the ﬁrst theoretical guarantee regarding the convergence of
• When the underlying vector is sparse  we design another algorithm that achieves a sample
complexity of O(cid:16)(x∗min)−4 log n + log3 k(cid:1)(cid:17) where k is the sparsity and x∗min is the mini-

mum non-zero entry of x∗. This algorithm also runs over Cn and scales much better than
SDP based methods.

Besides being an empirically better algorithm for this problem  our work is also interesting in a
broader sense: there are many problems in machine learning where the natural formulation of a
problem is non-convex; examples include rank constrained problems  applications of EM algorithms
etc.  and alternating minimization has good empirical performance. However  the methods with the
best (or only) analytical guarantees involve convex relaxations (e.g.  by relaxing the rank constraint
and penalizing the trace norm). In most of these settings  correctness of alternating minimization is
an open question. We believe that our results in this paper are of interest  and may have implications 
in this larger context.

The rest of the paper is organized as follows: In section 1.1  we brieﬂy review related work. We
clarify our notation in Section 2. We present our algorithm in Section 3 and the main results in
Section 4. We present our results for the sparse case in Section 5. Finally  we present experimental
results in Section 6.

1.1 Related Work

Phase Retrieval via Non-Convex Procedures: Inspite of the huge amount of work it has attracted 
phase retrieval has been a long standing open problem. Early work in this area focused on using
holography to capture the phase information along with magnitude measurements [12]. However 
computational methods for reconstruction of the signal using only magnitude measurements re-
ceived a lot of attention due to their applicability in resolving spurious noise  fringes  optical system
aberrations and so on and difﬁculties in the implementation of interferometer setups [9]. Though
such methods have been developed to solve this problem in various practical settings [8  20]  our

2

theoretical understanding of this problem is still far from complete. Many papers have focused on
determining conditions under which (1) has a unique solution - see [24] and references therein.
However  the uniqueness results of these papers do not resolve the algorithmic question of how to
ﬁnd the solution to (1).

Since the seminal work of Gerchberg and Saxton [13] and Fienup [11]  many iterated projection
algorithms have been developed targeted towards various applications [1  10  2]. [21] ﬁrst suggested
the use of multiple magnitude measurements to resolve the phase problem. This approach has been
successfully used in many practical applications - see [9] and references there in. Following the
empirical success of these algorithms  researchers were able to explain its success in some of the
instances [29] using Bregman’s theory of iterated projections onto convex sets [3]. However  many
instances  such as the one we consider in this paper  are out of reach of this theory since they involve
magnitude constraints which are non-convex. To the best of our knowledge  there are no theoretical
results on the convergence of these approaches in a non-convex setting.

Phase Retrieval via Convex Relaxation: An interesting recent approach for solving this problem
formulates it as one of ﬁnding the rank-one solution to a system of linear matrix equations. The
papers [7  6] then take the approach of relaxing the rank constraint by a trace norm penalty  making
the overall algorithm a convex program (called PhaseLift) over n × n matrices. Another recent line
of work [28] takes a similar but different approach : it uses an SDP relaxation (called PhaseCut) that
is inspired by the classical SDP relaxation for the max-cut problem. To date  these convex methods
are the only ones with analytical guarantees on statistical performance [5  28] (i.e. the number m of
measurements required to recover x∗) – under an i.i.d. random Gaussian model on the measurement
vectors ai. However  by “lifting” a vector problem to a matrix one  these methods lead to a much
larger representation of the state space  and higher computational cost as a result.

Sparse Phase Retrieval: A special case of the phase retrieval problem which has received a lot
of attention recently is when the underlying signal x∗ is known to be sparse. Though this problem
is closely related to the compressed sensing problem  lack of phase information makes this harder.
However  the ℓ1 regularization approach of compressed sensing has been successfully used in this
setting as well. In particular  if x∗ is sparse  then the corresponding lifted matrix x∗x∗T is also
sparse. [22  18] use this observation to design ℓ1 regularized SDP algorithms for phase retrieval
of sparse vectors. For random Gaussian measurements  [18] shows that ℓ1 regularized PhaseLift
recovers x∗ correctly if the number of measurements is Ω(k2 log n). By the results of [23]  this
result is tight up to logarithmic factors for ℓ1 and trace norm regularized SDP relaxations.

Alternating Minimization (a.k.a. ALS): Alternating minimization has been successfully applied
to many applications in the low-rank matrix setting. For example  clustering  sparse PCA  non-
negative matrix factorization  signed network prediction etc.
- see [15] and references there in.
However  despite empirical success  for most of the problems  there are no theoretical guarantees
regarding its convergence except to a local minimum. The only exceptions are the results in [16  15]
which give provable guarantees for alternating minimization for the problems of matrix sensing and
matrix completion.

2 Notation

We use bold capital letters (A  B etc.) for matrices  bold small case letters (x  y etc.) for vectors
and non-bold letters (α  U etc.) for scalars. For every complex vector w ∈ Cn  |w| ∈ Rn denotes
its element-wise magnitude vector. wT and AT denote the Hermitian transpose of the vector w
and the matrix A respectively. e1  e2  etc. denote the canonical basis vectors in Cn. z denotes the
complex conjugate of the complex number z. In this paper we use the standard Gaussian (or normal)
distribution over Cn. a is said to be distributed according to this distribution if a = a1 + ia2  where
a1 and a2 are independent and are distributed according to N (0  I). We also deﬁne Ph (z) def= z
|z|
for every w1  w2 ∈ Cn. Finally  we

for every z ∈ C  and dist (w1  w2) def= r1 −(cid:12)(cid:12)(cid:12)

use the shorthand wlog for without loss of generality and whp for with high probability.

2

hw1 w2i

kw1k2kw2k2(cid:12)(cid:12)(cid:12)

3 Algorithm

In this section  we present our alternating minimization based algorithm for solving the phase re-

trieval problem. Let A ∈ Cn×m be the measurement matrix  with ai as its ith column; similarly let

3

Algorithm 1 AltMinPhase
input A  y  t0

1: Initialize x0 ← top singular vector of Pi y2
2: for t = 0 ···   t0 − 1 do
3: Ct+1 ← Diag Ph AT xt(cid:1)(cid:1)
xt+1 ← argminx∈Rn(cid:13)(cid:13)AT x − Ct+1y(cid:13)(cid:13)2

4:
5: end for
output xt0

T

i aiai

y be the vector of recorded magnitudes. Then 

Recall that  given y and A  the goal is to recover x∗. If we had access to the true phase c∗ of AT x∗
(i.e.  c∗i = Ph (hai  x∗i)) and m ≥ n  then our problem reduces to one of solving a system of linear

equations:

y = | AT x∗ |.

C∗y = AT x∗ 

where C∗ def= Diag(c∗) is the diagonal matrix of phases. Of course we do not know C∗  hence one
approach to recovering x∗ is to solve:

argmin

C x

kAT x − Cyk2 

(2)

where x ∈ Cn and C ∈ Cm×m is a diagonal matrix with each diagonal entry of magnitude 1. Note

that the above problem is not convex since C is restricted to be a diagonal phase matrix and hence 
one cannot use standard convex optimization methods to solve it.

Instead  our algorithm uses the well-known alternating minimization: alternatingly update x and C
so as to minimize (2). Note that given C  the vector x can be obtained by solving the following least

squares problem: minx kAT x − Cyk2. Since the number of measurements m is larger than the
dimensionality n and since each entry of A is sampled from independent Gaussians  A is invertible
with probability 1. Hence  the above least squares problem has a unique solution. On the other hand 
given x  the optimal C is given by C = Diag Ph AT x(cid:1)(cid:1).

While the above algorithm is simple and intuitive  it is known that with bad initial points  the solu-
tion might not converge to x∗. In fact  this algorithm with a uniformly random initial point has been
empirically evaluated for example in [28]  where it performs worse than SDP based methods. More-
over  since the underlying problem is non-convex  standard analysis techniques fail to guarantee
convergence to the global optimum  x∗. Hence  the key challenges here are: a) a good initialization
step for this method  b) establishing this method’s convergence to x∗.

We address the ﬁrst key challenge in our AltMinPhase algorithm (Algorithm 1) by initializing x as
the largest singular vector of the matrix S = 1
T . Theorem 4.1 shows that when A is
sampled from standard complex normal distribution  this initialization is accurate. In particular  if

i aiai

mPi y2

m ≥ C1n log3 n for large enough C1 > 0  then whp we have kx0 − x∗k2 ≤ 1/100 (or any other

constant).

Theorem 4.2 addresses the second key challenge and shows that a variant of AltMinPhase (see
Algorithm 2) actually converges to the global optimum x∗ at linear rate. See section 4 for a detailed
analysis of our algorithm.

We would like to stress that not only does a natural variant of our proposed algorithm have rigorous
theoretical guarantees  it also is effective practically as each of its iterations is fast  has a closed form
solution and does not require SVD computation. AltMinPhase has similar statistical complexity to
that of PhaseLift and PhaseCut while being much more efﬁcient computationally. In particular  for
accuracy ǫ  we only need to solve each least squares problem only up to accuracy O (ǫ). Now  since
the measurement matrix A is sampled from Gaussian with m > Cn  it is well conditioned. Hence 

ǫ(cid:1) time. When m = O (n) and
using conjugate gradient method  each such step takes O mn log 1
we have geometric convergence  the total time taken by the algorithm is O n2 log2 1
ǫ(cid:1). SDP based
methods on the other hand require Ω(n3/√ǫ) time. Moreover  our initialization step increases the
likelihood of successful recovery as opposed to a random initialization (which has been considered
so far in prior work). Refer Figure 1 for an empirical validation of these claims.

4

(a)

(b)

Figure 1: Sample and Time complexity of various methods for Gaussian measurement matrices A.
Figure 1(a) compares the number of measurements required for successful recovery by various meth-
ods. We note that our initialization improves sample complexity over that of random initialization
(AltMin (random init)) by a factor of 2. AltMinPhase requires similar number of measurements as
PhaseLift and PhaseCut. Figure 1(b) compares the running time of various algorithms on log-scale.
Note that AltMinPhase is almost two orders of magnitude faster than PhaseLift and PhaseCut.

4 Main Results: Analysis

In this section we describe the main contribution of this paper: provable statistical guarantees for the
success of alternating minimization in solving the phase recovery problem. To this end  we consider
the setting where each measurement vector ai is iid and is sampled from the standard complex
normal distribution. We would like to stress that all the existing guarantees for phase recovery also
use exactly the same setting [6  5  28]. Table 1 presents a comparison of the theoretical guarantees
of Algorithm 2 as compared to PhaseLift and PhaseCut.

Algorithm 2
PhaseLift [5]
PhaseCut [28]

Sample complexity

O n log3 n + log 1

ǫ log log 1

O (n)
O (n)

Comp. complexity

ǫ(cid:1)(cid:1) O n2 log3 n + log2 1
O n3/ǫ2(cid:1)
O n3/√ǫ(cid:1)

ǫ log log 1

ǫ(cid:1)(cid:1)

Table 1: Comparison of Algorithm 2 with PhaseLift and PhaseCut: Though the sample complexity
of Algorithm 2 is off by log factors from that of PhaseLift and PhaseCut  it is O (n) better than them
in computational complexity. Note that  we can solve the least squares problem in each iteration
approximately by using conjugate gradient method which requires only O (mn) time.

Our proof for convergence of alternating minimization can be broken into two key results. We ﬁrst

show that if m ≥ Cn log3 n  then whp the initialization step used by AltMinPhase returns x0 which
is at most a constant distance away from x∗. Furthermore  that constant can be controlled by using
more samples (see Theorem 4.1).
We then show that if xt is a ﬁxed vector such that dist xt  x∗(cid:1) < c (small enough) and A is sampled
independently of xt with m > Cn (C large enough) then whp xt+1 satisﬁes: dist xt+1  x∗(cid:1) <
4 dist xt  x∗(cid:1) (see Theorem 4.2). Note that our analysis critically requires xt to be “ﬁxed” and

be independent of the sample matrix A. Hence  we cannot re-use the same A in each iteration;
instead  we need to resample A in every iteration. Using these results  we prove the correctness of
Algorithm 2  which is a natural resampled version of AltMinPhase.

3

We now present the two results mentioned above. For our proofs  wlog  we assume that kx∗k2 = 1.

Our ﬁrst result guarantees a good initial vector.
Theorem 4.1. There exists a constant C1 such that if m > C1
probability greater than 1 − 4/m2 we have:

c2 n log3 n  then in Algorithm 2  with

kx0 − x∗k2 < c.

5

ǫ

Algorithm 2 AltMinPhase with Resampling
input A  y  ǫ
1: t0 ← c log 1
2: Partition y and (the corresponding columns of) A into t0 + 1 equal disjoint sets:
(y0  A0)  (y1  A1) ···   (yt0   At0 )
3: x0 ← top singular vector of Pl y0
l(cid:1)2
ℓ  a0
ℓ(cid:1)T
4: for t = 0 ···   t0 − 1 do
5: Ct+1 ← Diag(cid:16)Ph(cid:16) At+1(cid:1)T
xt(cid:17)(cid:17)
xt+1 ← argminx∈Rn(cid:13)(cid:13)(cid:13) At+1(cid:1)T
x − Ct+1yt+1(cid:13)(cid:13)(cid:13)2

a0

6:

7: end for
output xt0

The second result proves geometric decay of error assuming a good initialization.

Theorem 4.2. There exist constants c  bc and ec such that in iteration t of Algorithm 2 
dist xt  x∗(cid:1) < c and the number of columns of At is greater thanbcn log 1
more than 1 − η  we have:

if
η then  with probability

dist xt+1  x∗(cid:1) <

3
4

dist xt  x∗(cid:1)   and kxt+1 − x∗k2 <ec dist xt  x∗(cid:1) .

Proof. For simplicity of notation in the proof of the theorem  we will use A for At+1  C for Ct+1 
x for xt  x+ for xt+1  and y for yt+1. Now consider the update in the (t + 1)th iteration:

where D is a diagonal matrix with Dll

x+ = argmin

ex∈Rn (cid:13)(cid:13)ATex − Cy(cid:13)(cid:13)2 = AAT(cid:1)−1
x+ = AAT(cid:1)−1

def= Ph(cid:16)aℓ

ADAT x∗ = x∗ + AAT(cid:1)−1

T x · aℓ

ADAT x∗ 

ACy = AAT(cid:1)−1
T x∗(cid:17). Now (3) can be rewritten as:

A (D − I) AT x∗ 

(3)

(4)

that is  x+ can be viewed as a perturbation of x∗ and the goal is to bound the error term (the second
term above). We break the proof into two main steps:

1. ∃ a constant c1 such that |hx∗  x+i| ≥ 1 − c1dist (x  x∗) (see Lemma A.2)  and
2. |hz  x+i| ≤ 5

9 dist (x  x∗)  for all z s.t. zT x∗ = 0. (see Lemma A.4)

Assuming the above two bounds and choosing c < 1

100c1

  we can prove the theorem:

dist x+  x∗(cid:1)2

<

(25/81) · dist (x  x∗)2
(1 − c1dist (x  x∗))2 ≤

9
16

dist (x  x∗)2  

proving the ﬁrst part of the theorem. The second part follows easily from (3) and Lemma A.2.

Intuition and key challenge: If we look at step 6 of Algorithm 2  we see that  for the measurements 
we use magnitudes calculated from x∗ and phases calculated from x. Intuitively  this means that we
are trying to push x+ towards x∗ (since we use its magnitudes) and x (since we use its phases) at
the same time. The key intuition behind the success of this procedure is that the push towards x∗ is
stronger than the push towards x  when x is close to x∗. The key lemma that captures this effect is
stated below:
Lemma 4.3. Let w1 and w2 be two independent standard complex Gaussian random variables2.

Let U = |w1| w2(cid:16)Ph(cid:16)1 +
that if √1 − α2 < γ  then: E [U ] ≤ (1 + δ)√1 − α2.

α|w1| (cid:17) − 1(cid:17) . Fix δ > 0. Then  there exists a constant γ > 0 such
√1−α2w2

2z is standard complex Gaussian if z = z1 + iz2 where z1 and z2 are independent standard normal random

variables.

6

Algorithm 3 SparseAltMinPhase
input A  y  k

1: S ← top-k argmaxj∈[n]Pm

i=1 |aijyi| {Pick indices of k largest absolute value inner product}
2: Apply Algorithm 2 on AS  yS and output the resulting vector with elements in Sc set to zero.

Algorithm 3

ǫ log log 1

Sample complexity

Comp. complexity

ℓ1-PhaseLift [18]

ǫ(cid:1)(cid:1)
Table 2: Comparison of Algorithm 3 with ℓ1-PhaseLift when x∗min = Ω(cid:16)1/√k(cid:17). Note that the

ǫ(cid:1)(cid:1) O k2 kn log n + log2 1
O n3/ǫ2(cid:1)

O k k log n + log 1
O k2 log n(cid:1)

complexity of Algorithm 3 is dominated by the support ﬁnding step. If k = O (1)  Algorithm 3 runs
in quasi-linear time.

ǫ log log 1

See Appendix A for a proof of the above lemma and how we use it to prove Theorem 4.2.

Combining Theorems 4.1 and 4.2  and a simple observation that kxT − x∗k2 <ec dist xT  x∗(cid:1) for
a constantec  we can establish the correctness of Algorithm 2.
vectors. For every η > 0  there exists a constant c such that if m > cn log3 n + log 1
ǫ(cid:1)
then  with probability greater than 1 − η  Algorithm 2 outputs xt0 such that kxt0 − x∗k2 < ǫ.

Theorem 4.4. Suppose the measurement vectors in (1) are independent standard complex normal
ǫ log log 1

5 Sparse Phase Retrieval

In this section  we consider the case where x∗ is known to be sparse  with sparsity k. A natural
and practical question to ask here is: can the sample and computational complexity of the recovery
algorithm be improved when k ≪ n.
Recently  [18] studied this problem for Gaussian A and showed that for ℓ1 regularized PhaseLift 
m = O(k2 log n) samples sufﬁce for exact recovery of x∗. However  the computational complexity
of this algorithm is still O(n3/ǫ2).

In this section  we provide a simple extension of our AltMinPhase algorithm that we call SparseAlt-
MinPhase  for the case of sparse x∗. The main idea behind our algorithm is to ﬁrst recover the
support of x∗. Then  the problem reduces to phase retrieval of a k-dimensional signal. We then
solve the reduced problem using Algorithm 2. The pseudocode for SparseAltMinPhase is presented
in Algorithm 3. Table 2 provides a comparison of Algorithm 3 with ℓ1-regularized PhaseLift in
terms of sample complexity as well as computational complexity.

The following lemma shows that if the number of measurements is large enough  step 1 of SparseAlt-
MinPhase recovers the support of x∗ correctly.
Lemma 5.1. Suppose x∗ is k-sparse with support S and kx∗k2 = 1. If ai are standard complex
Gaussian random vectors and m >
δ   then Algorithm 3 recovers S with probability
greater than 1 − δ  where x∗min is the minimum non-zero entry of x∗.
The key step of our proof is to show that if j ∈ supp(x∗)  then random variable Zij = Pi |aijyi|
has signiﬁcantly higher mean than for the case when j /∈ supp(x∗). Now  by applying appropriate
concentration bounds  we can ensure that minj∈supp(x∗) |Zij| > maxj /∈supp(x∗) |Zij| and hence our
algorithm never picks up an element outside the true support set supp(x∗). See Appendix B for a
detailed proof of the above lemma.

c
min)4 log n
(x∗

The correctness of Algorithm 3 now is a direct consequence of Lemma 5.1 and Theorem 4.4. For the

special case where each non-zero value in x∗ is from {− 1√k
Corollary 5.2. Suppose x∗ is k-sparse with non-zero elements ± 1√k
m > c k2 log n
probability greater than 1 − δ.

  1√k}  we have the following corollary:
ǫ(cid:1)  then Algorithm 3 will recover x∗ up to accuracy ǫ with

δ + k log2 k + k log 1

. If the number of measurements

7

(a)

(b)

(c)

Figure 2: (a) & (b): Sample and time complexity for successful recovery using random Gaussian
illumination ﬁlters. Similar to Figure 1  we observe that AltMinPhase has similar number of ﬁlters
(J ) as PhaseLift and PhaseCut  but is computationally much more efﬁcient. We also see that Alt-
MinPhase performs better than AltMin (randominit). (c): Recovery error kx − x∗k2 incurred by
various methods with increasing amount of noise (σ). AltMinPhase and PhaseCut perform compa-
rably while PhaseLift incurs signiﬁcantly larger error.

6 Experiments

In this section  we present experimental evaluation of AltMinPhase (Algorithm 1) and compare its
performance with the SDP based methods PhaseLift [6] and PhaseCut [28]. We also empirically
demonstrate the advantage of our initialization procedure over random initialization (denoted by
AltMin (random init))  which has thus far been considered in the literature [13  11  28  4]. AltMin
(random init) is the same as AltMinPhase except that step 1 of Algorithm 1 is replaced with:x0 ←
Uniformly random vector from the unit sphere.
We ﬁrst choose x∗ uniformly at random from the unit sphere. In the noiseless setting  a trial is said
to succeed if the output x satisﬁes kx − x∗k2 < 10−2. For a given dimension  we do a linear search
for smallest m (number of samples) such that empirical success ratio over 20 runs is at least 0.8. We
implemented our methods in Matlab  while we obtained the code for PhaseLift and PhaseCut from
the authors of [22] and [28] respectively.

We now present results from our experiments in three different settings.

Independent Random Gaussian Measurements: Each measurement vector ai is generated from
the standard complex Gaussian distribution. This measurement scheme was ﬁrst suggested by [6]
and till date  this is the only scheme with theoretical guarantees.

Multiple Random Illumination Filters: We now present our results for the setting where the mea-
surements are obtained using multiple illumination ﬁlters; this setting was suggested by [4].
In

particular  choose J vectors z(1) ···   z(J) and compute the following discrete Fourier transforms:

bx(u) = DFT(cid:16)x∗ · ∗ z(u)(cid:17)  

where ·∗ denotes component-wise multiplication. Our measurements will then be the magnitudes of
components of the vectorsbx(1) ···  bx(J). The above measurement scheme can be implemented by
modulating the light beam or by the use of masks; see [4] for more details.

We again perform the same experiments as in the previous setting. Figures 2 (a) and (b) present the
results. We again see that the measurement complexity of AltMinPhase is similar to that of PhaseCut
and PhaseLift  but AltMinPhase is orders of magnitude faster than PhaseLift and PhaseCut.

Noisy Phase Retrieval: Finally  we study our method in the following noisy measurement scheme:

yi = |hai  x∗ + wii|

for i = 1  . . .   m 

(5)

where wi is the noise in the i-th measurement and is sampled from N (0  σ2). We ﬁx n = 64
and m = 6n. We then vary the amount of noise added σ and measure the ℓ2 error in recovery 
i.e.  kx − x∗k2  where x is the recovered vector. Figure 2(c) compares the performance of various

methods with varying amount of noise. We observe that our method outperforms PhaseLift and has
similar recovery error as PhaseCut.

Acknowledgments

S. Sanghavi would like to acknowledge support from NSF grants 0954059  1302435  ARO grant
W911NF-11-1-0265 and a DTRA YIP award.

8

References

[1] J. Abrahams and A. Leslie. Methods used in the structure determination of bovine mitochondrial f1

atpase. Acta Crystallographica Section D: Biological Crystallography  52(1):30–42  1996.

[2] H. H. Bauschke  P. L. Combettes  and D. R. Luke. Hybrid projection–reﬂection method for phase retrieval.

JOSA A  20(6):1025–1034  2003.

[3] L. Bregman. Finding the common point of convex sets by the method of successive projection.(russian).

In Dokl. Akad. Nauk SSSR  volume 162  pages 487–490  1965.

[4] E. J. Candes  Y. C. Eldar  T. Strohmer  and V. Voroninski. Phase retrieval via matrix completion. SIAM

Journal on Imaging Sciences  6(1):199–225  2013.

[5] E. J. Candes and X. Li. Solving quadratic equations via phaselift when there are about as many equations

as unknowns. arXiv preprint arXiv:1208.6247  2012.

[6] E. J. Candes  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude

measurements via convex programming. Communications on Pure and Applied Mathematics  2012.

[7] A. Chai  M. Moscoso  and G. Papanicolaou. Array imaging using intensity-only measurements. Inverse

Problems  27(1):015005  2011.

[8] J. C. Dainty and J. R. Fienup. Phase retrieval and image reconstruction for astronomy. Image Recovery:

Theory and Application  ed. byH. Stark  Academic Press  San Diego  pages 231–275  1987.

[9] H. Duadi  O. Margalit  V. Mico  J. A. Rodrigo  T. Alieva  J. Garcia  and Z. Zalevsky. Digital holography

and phase retrieval. Source: Holography  Research and Technologies. InTech  2011.

[10] V. Elser. Phase retrieval by iterated projections. JOSA A  20(1):40–55  2003.

[11] J. R. Fienup et al. Phase retrieval algorithms: a comparison. Applied optics  21(15):2758–2769  1982.

[12] D. Gabor. A new microscopic principle. Nature  161(4098):777–778  1948.

[13] R. W. Gerchberg and W. O. Saxton. A practical algorithm for the determination of phase from image and

diffraction plane pictures. Optik  35:237  1972.

[14] N. E. Hurt. Phase Retrieval and Zero Crossings: Mathematical Methods in Image Reconstruction  vol-

ume 52. Kluwer Academic Print on Demand  2001.

[15] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimization. arXiv

preprint arXiv:1212.0467  2012.

[16] R. H. Keshavan. Efﬁcient algorithms for collaborative ﬁltering. Phd Thesis  Stanford University  2012.

[17] W. V. Li and A. Wei. Gaussian integrals involving absolute value functions.

In Proceedings of the

Conference in Luminy  2009.

[18] X. Li and V. Voroninski. Sparse signal recovery from quadratic measurements via convex programming.

arXiv preprint arXiv:1209.4785  2012.

[19] S. Marchesini. Invited article: A uniﬁed evaluation of iterative projection algorithms for phase retrieval.

Review of Scientiﬁc Instruments  78(1):011301–011301  2007.

[20] J. Miao  P. Charalambous  J. Kirz  and D. Sayre. Extending the methodology of x-ray crystallography to

allow imaging of micrometre-sized non-crystalline specimens. Nature  400(6742):342–344  1999.

[21] D. Misell. A method for the solution of the phase problem in electron microscopy. Journal of Physics D:

Applied Physics  6(1):L6  1973.

[22] H. Ohlsson  A. Y. Yang  R. Dong  and S. S. Sastry. Compressive phase retrieval from squared output

measurements via semideﬁnite programming. arXiv preprint arXiv:1111.6323  2011.

[23] S. Oymak  A. Jalali  M. Fazel  Y. C. Eldar  and B. Hassibi. Simultaneously structured models with

application to sparse and low-rank matrices. arXiv preprint arXiv:1212.3753  2012.

[24] J. L. Sanz. Mathematical considerations for the problem of fourier transform phase retrieval from magni-

tude. SIAM Journal on Applied Mathematics  45(4):651–664  1985.

[25] Y. Shechtman  Y. C. Eldar  A. Szameit  and M. Segev. Sparsity based sub-wavelength imaging with

partially incoherent light via quadratic compressed sensing. arXiv preprint arXiv:1104.4406  2011.

[26] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

Mathematics  12(4):389–434  2012.

[27] R. Vershynin.

Introduction to the non-asymptotic analysis of random matrices.

arXiv preprint

arXiv:1011.3027  2010.

[28] I. Waldspurger  A. d’Aspremont  and S. Mallat. Phase recovery  maxcut and complex semideﬁnite pro-

gramming. arXiv preprint arXiv:1206.0102  2012.

[29] D. C. Youla and H. Webb. Image restoration by the method of convex projections: Part 1theory. Medical

Imaging  IEEE Transactions on  1(2):81–94  1982.

9

,Praneeth Netrapalli
Prateek Jain
Sujay Sanghavi