2017,Conic Scan-and-Cover algorithms for nonparametric topic modeling,We propose new algorithms for topic modeling when the number of topics is unknown. Our approach relies on an analysis of the concentration of mass and angular geometry of the topic simplex  a convex polytope constructed by taking the convex hull of vertices representing the latent topics. Our algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation  which requires the number of topics be given. Moreover  they are one of the fastest among several state of the art parametric techniques. Statistical consistency of our estimator is established under some conditions.,Conic Scan-and-Cover algorithms for

nonparametric topic modeling

Mikhail Yurochkin
Department of Statistics
University of Michigan

moonfolk@umich.edu

Aritra Guha

Department of Statistics
University of Michigan
aritra@umich.edu

XuanLong Nguyen

Department of Statistics
University of Michigan

xuanlong@umich.edu

Abstract

We propose new algorithms for topic modeling when the number of topics is
unknown. Our approach relies on an analysis of the concentration of mass and
angular geometry of the topic simplex  a convex polytope constructed by taking
the convex hull of vertices representing the latent topics. Our algorithms are shown
in practice to have accuracy comparable to a Gibbs sampler in terms of topic
estimation  which requires the number of topics be given. Moreover  they are one
of the fastest among several state of the art parametric techniques.1 Statistical
consistency of our estimator is established under some conditions.

1

Introduction

A well-known challenge associated with topic modeling inference can be succinctly summed up
by the statement that sampling based approaches may be accurate but computationally very slow 
e.g.  Pritchard et al. (2000); Grifﬁths & Steyvers (2004)  while the variational inference approaches
are faster but their estimates may be inaccurate  e.g.  Blei et al. (2003); Hoffman et al. (2013). For
nonparametric topic inference  i.e.  when the number of topics is a priori unknown  the problem
becomes more acute. The Hierarchical Dirichlet Process model (Teh et al.  2006) is an elegant
Bayesian nonparametric approach which allows for the number of topics to grow with data size  but
its sampling based inference is much more inefﬁcient compared to the parametric counterpart. As
pointed out by Yurochkin & Nguyen (2016)  the root of the inefﬁciency can be traced to the need for
approximating the posterior distributions of the latent variables representing the topic labels — these
are not geometrically intrinsic as any permutation of the labels yields the same likelihood.
A promising approach in addressing the aforementioned challenges is to take a convex geometric
perspective  where topic learning and inference may be formulated as a convex geometric problem: the
observed documents correspond to points randomly drawn from a topic polytope  a convex set whose
vertices represent the topics to be inferred. This perspective has been adopted to establish posterior
contraction behavior of the topic polytope in both theory and practice (Nguyen  2015; Tang et al. 
2014). A method for topic estimation that exploits convex geometry  the Geometric Dirichlet Means
(GDM) algorithm  was proposed by Yurochkin & Nguyen (2016)  which demonstrates attractive
behaviors both in terms of running time and estimation accuracy. In this paper we shall continue to
amplify this viewpoint to address nonparametric topic modeling  a setting in which the number of
topics is unknown  as is the distribution inside the topic polytope (in some situations).
We will propose algorithms for topic estimation by explicitly accounting for the concentration of
mass and angular geometry of the topic polytope  typically a simplex in topic modeling applications.
The geometric intuition is fairly clear: each vertex of the topic simplex can be identiﬁed by a ray
emanating from its center (to be deﬁned formally)  while the concentration of mass can be quantiﬁed

1Code is available at https://github.com/moonfolk/Geometric-Topic-Modeling.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

for the cones hinging on the apex positioned at the center. Such cones can be rotated around the
center to scan for high density regions inside the topic simplex — under mild conditions such cones
can be constructed efﬁciently to recover both the number of vertices and their estimates.
We also mention another fruitful approach  which casts topic estimation as a matrix factorization
problem (Deerwester et al.  1990; Xu et al.  2003; Anandkumar et al.  2012; Arora et al.  2012). A
notable recent algorithm coming from the matrix factorization perspective is RecoverKL (Arora et al. 
2012)  which solves non-negative matrix factorization (NMF) efﬁciently under assumptions on the
existence of so-called anchor words. RecoverKL remains to be a parametric technique — we will
extend it to a nonparametric setting and show that the anchor word assumption appears to limit the
number of topics one can efﬁciently learn.
Our paper is organized as follows. In Section 2 we discuss recent developments in geometric topic
modeling and introduce our approach; Sections 3 and 4 deliver the contributions outlined above;
Section 5 demonstrates experimental performance; we conclude with a discussion in Section 6.

2 Geometric topic modeling

Background and related work In this section we present the convex geometry of the Latent
Dirichlet Allocation (LDA) model of Blei et al. (2003)  along with related theoretical and algorithmic
results that motivate our work. Let V be vocabulary size and ∆V −1 be the corresponding vocabulary
probability simplex. Sample K topics (i.e.  distributions on words) βk ∼ DirV (η)  k = 1  . . .   K 
(i.e.  topic proportions) θm ∼ DirK(α) and then setting pm :=(cid:80)
where η ∈ RV
+. Next  sample M document-word probabilities pm residing in the topic simplex
B := Conv(β1  . . .   βK) (cf. Nguyen (2015))  by ﬁrst generating their barycentric coordinates
k βkθmk for m = 1  . . .   M and
α ∈ RK
+ . Finally  word counts of the m-th document can be sampled wm ∼ Mult(pm  Nm)  where
Nm ∈ N is the number of words in document m. The above model is equivalent to the LDA when
individual words to topic label assignments are marginalized out.
Nguyen (2015) established posterior contraction rates of the topic simplex  provided that αk ≤ 1∀k
and either number of topics K is known or topics are sufﬁciently separated in terms of the Euclidean
distance. Yurochkin & Nguyen (2016) devised an estimate for B  taken to be a ﬁxed unknown
quantity  by formulating a geometric objective function  which is minimized when topic simplex B
is close to the normalized documents ¯wm := wm/Nm. They showed that the estimation of topic
proportions θm given B simply reduces to taking barycentric coordinates of the projection of ¯wm
onto B. To estimate B given K  they proposed a Geometric Dirichlet Means (GDM) algorithm 
which operated by performing a k-means clustering on the normalized documents  followed by a
geometric correction for the cluster centroids. The resulting algorithm is remarkably fast and accurate 
supporting the potential of the geometric approach. The GDM is not applicable when K is unknown 
but it provides a motivation which our approach is built on.

The Conic Scan-and-Cover approach To enable the inference of B when K is not known  we
need to investigate the concentration of mass inside the topic simplex. It sufﬁces to focus on two
types of geometric objects: cones and spheres  which provide the basis for a complete coverage of the
simplex. To gain intuition of our procedure  which we call Conic Scan-and-Cover (CoSAC) approach 
imagine someone standing at a center point of a triangular dark room trying to ﬁgure out all corners
with a portable ﬂashlight  which can produce a cone of light. A room corner can be identiﬁed with
the direction of the farthest visible data objects. Once a corner is found  one can turn the ﬂashlight to
another direction to scan for the next ones. See Fig. 1a  where red denotes the scanned area. To make
sure that all corners are detected  the cones of light have to be open to an appropriate range of angles
so that enough data objects can be captured and removed from the room. To make sure no false
corners are declared  we also need a suitable stopping criterion  by relying only on data points that lie
beyond a certain spherical radius  see Fig. 1b. Hence  we need to be able to gauge the concentration
of mass for suitable cones and spherical balls in ∆V −1. This is the subject of the next section.

3 Geometric estimation of the topic simplex

We start by representing B in terms of its convex and angular geometry. First  B is centered at a point
denoted by Cp. The centered probability simplex is denoted by ∆V −1
:= {x ∈ RV |x+Cp ∈ ∆V −1}.

0

2

(b) Complete coverage using

3 cones (red) and a ball (yellow).

(a) An incomplete coverage using
3 cones (containing red points).
Figure 1: Complete coverage of topic simplex by cones and a spherical ball for K = 3  V = 3.
Then  write bk := βk − Cp ∈ ∆V −1
for k = 1  . . .   K and ˜pm := pm − Cp ∈ ∆V −1
for
m = 1  . . .   M. Note that re-centering leaves corresponding barycentric coordinates θm ∈ ∆K−1
unchanged. Moreover  the extreme points of centered topic simplex ˜B := Conv{b1  . . .   bK} can
now be represented by their directions vk ∈ RV and corresponding radii Rk ∈ R+ such that
bk = Rkvk for any k = 1  . . .   K.

(c) Cap Λc(v1) and cone Sω(v1).

0

0

3.1 Coverage of the topic simplex

0

min)/(2R2

max)  max

The ﬁrst step toward formulating a CoSAC approach is to show how ˜B can be covered with
exactly K cones and one spherical ball positioned at Cp. A cone is deﬁned as set Sω(v) :=
{p ∈ ∆V −1
|dcos(v  p) < ω}  where we employ the angular distance (a.k.a. cosine distance)
dcos(v  p) := 1 − cos(v  p) and cos(v  p) is the cosine of angle ∠(v  p) formed by vectors v and p.
K(cid:83)
The Conical coverage
It is possible to choose ω so that the topic simplex can be covered with
k=1Sω(vk) ⊇ ˜B. Moreover  each cone contains exactly one vertex. Suppose
exactly K cones  that is 
that Cp is the incenter of the topic simplex ˜B  with r being the inradius. The incenter and inradius
correspond to the maximum volume sphere contained in ˜B. Let ai k denote the distance between
the i-th and k-th vertex of ˜B  with amin ≤ ai k ≤ amax for all i  k  and Rmax  Rmin such that
Rmin ≤ Rk := (cid:107)bk(cid:107)2 ≤ Rmax ∀ k = 1  . . .   K. Then we can establish the following.
Proposition 1. For simplex ˜B and ω ∈ (ω1  ω2)  where ω1 = 1 − r/Rmax and ω2 =
(1 − cos(bi  bk)}  the cone Sω(v) around any vertex direction
max{(a2
v of ˜B contains exactly one vertex. Moreover  complete coverage holds:
(cid:16)

We say there is an angular separation if cos(bi  bk) ≤ 0 for any i  k = 1  . . .   K (i.e.  the angles for
all pairs are at least π/2)  then ω ∈
(cid:54)= ∅. Thus  under angular separation  the range ω
that allows for full coverage is nonempty independently of K. Our result is in agreement with that of
Nguyen (2015)  whose result suggested that topic simplex B can be consistently estimated without
knowing K  provided there is a minimum edge length amin > 0. The notion of angular separation
leads naturally to the Conic Scan-and-Cover algorithm. Before getting there  we show a series of
results allowing us to further extend the range of admissible ω.
The inclusion of a spherical ball centered at Cp allows us to expand substantially the range of ω
for which conical coverage continues to hold. In particular  we can reduce the lower bound on ω in
Proposition 1  since we only need to cover the regions near the vertices of ˜B with cones using the
following proposition. Fig. 1b provides an illustration.
Proposition 2. Let B(Cp R) = {˜p ∈ RV |(cid:107)˜p − Cp(cid:107)2 ≤ R}  R > 0; ω1  ω2 given in Prop. 1  and

K(cid:83)
k=1Sω(vk) ⊇ ˜B.

1 − r

i k=1 ... K

(cid:17)

  1

Rmax

(cid:115)

(cid:26)

 Rk sin2(bi  bk)

R

ω3 := 1 − min

min
i k

   1

(cid:27)

 

(1)

R2

k sin2(bi  bj)

R2

+ cos(bi  bk)

1 −

3

0.40.20.00.20.40.20.10.00.10.20.30.4(v1)(v2)(v3)0.40.20.00.20.40.20.10.00.10.20.30.4(v1)(v2)(v3)0.40.20.00.20.40.30.20.10.00.10.20.30.4c(v1)(v1)c1cc1cK(cid:83)
k=1Sω(vk) ∪ B(Cp R) ⊇ ˜B whenever ω ∈ (min{ω1  ω3}  ω2).

then we have

Notice that as R → Rmax   the value of ω3 → 0. Hence if R ≤ Rmin ≈ Rmax  the admissible
range for ω in Prop. 2 results in a substantial strengthening from Prop. 1. It is worth noting that the
above two geometric propositions do not require any distributional properties inside the simplex.

Coverage leftovers
In practice complete coverage may fail if ω and R are chosen outside of
corresponding ranges suggested by the previous two propositions. In that case  it is useful to note that
leftover regions will have a very low mass. Next we quantify the mass inside a cone that does contain
a vertex  which allows us to reject a cone that has low mass  therefore not containing a vertex in it.
Proposition 3. The cone Sω(v1) whose axis is a topic direction v1 has mass
P(Sω(v1)) > P(Λc(b1)) =
i(cid:54)=1 αi(1 − c)α1 Γ((cid:80)K
(cid:80)
((cid:80)
i(cid:54)=1 αi)Γ(α1)Γ((cid:80)

(cid:80)
i(cid:54)=1 αi−1dθ1
(cid:80)
(1 − θ1)
i(cid:54)=1 αi−1dθ1
(1 − θ1)
i=1 αi

(cid:82) 1
(cid:82) 1
1−c θα1−1
c(cid:80)K
(cid:20)
0 θα1−1
(cid:80)

c2((cid:80)K
i=1 αi)((cid:80)K
i(cid:54)=1 αi + 1)((cid:80)
((cid:80)

i=1 αi + 1)
i(cid:54)=1 αi + 2)

i=1 αi)
i(cid:54)=1 αi)

i(cid:54)=1 αi + 1

+ ···

1

1

(cid:21)

 

1 +

(2)

+

=

c

where Λc(b1) is the simplicial cap of Sω(v1) which is composed of vertex b1 and a base parallel to
the corresponding base of ˜B and cutting adjacent edges of ˜B in the ratio c : (1 − c).
See Fig. 1c for an illustration for the simplicial cap described in the proposition. Given the lower
bound for the mass around a cone containing a vertex  we have arrived at the following guarantee.
Proposition 4. For λ ∈ (0  1)  let cλ be such that λ = min

P(Λcλ(bk)) and let ωλ be such that

k

(cid:32)(cid:32)

(cid:115)

(cid:33)

cλ =

2

1 −

r2
R2

max

(cid:33)−1

(cid:19)(cid:19)

(3)

(4)

(sin(d) cot(arccos(1 − ωλ)) + cos(d))

 

where angle d ≤ min

i k

(cid:18)
∠(bk  bk − bi). Then  as long as

(cid:18) a2

min

the bound P(Sω(vk)) ≥ λ holds for all k = 1  . . .   K.
3.2 CoSAC: Conic Scan-and-Cover algorithm

ω ∈

ωλ  max

2R2

max

  max
i k=1 ... K

(1 − cos(bi  bk)

 

Having laid out the geometric foundations  we are ready to present the Conic Scan-and-Cover
(cid:80)
(CoSAC) algorithm  which is a scanning procedure for detecting the presence of simplicial vertices
based on data drawn randomly from the simplex. The idea is simple: iteratively pick the farthest point
from the center estimate ˆCp := 1
m pm  say v  then construct a cone Sω(v) for some suitably
M
chosen ω  and remove all the data residing in this cone. Repeat until there is no data point left.
Speciﬁcally  let A = {1  . . .   M} be the index set of the initially unseen data  then set v :=
˜pm:m∈A (cid:107)˜pm(cid:107)2 and update A := A \ Sω(v). The parameter ω needs to be sufﬁciently large to ensure
argmax
that the farthest point is a good estimate of a true vertex  and that the scan will be completed in exactly
K iterations; ω needs to be not too large  so that Sω(v) does not contain more than one vertex. The
existence of such ω is guaranteed by Prop. 1. In particular  for an equilateral ˜B  the condition of the
Prop. 1 is satisﬁed as long as ω ∈ (1 − 1/√K − 1  1 + 1/(K − 1)).

In our setting  K is unknown. A smaller ω would be a more robust choice  and accordingly the set A
will likely remain non-empty after K iterations. See the illustration of Fig. 1a  where the blue regions
correspond to A after K = 3 iterations of the scan. As a result  we proceed by adopting a stopping
criteria based on Prop. 2: the procedure is stopped as soon as ∀ m ∈ A (cid:107)˜pm(cid:107)2 < R  which allows us
to complete the scan in K iterations (as in Fig. 1b for K = 3).
The CoSAC algorithm is formally presented by Algorithm 1. Its running is illustrated in Fig. 2 
where we show iterations 1  26  29  30 of the algorithm by plotting norms of the centered documents

4

in the active set A and cone Sω(v) against cosine distance to the chosen direction of a topic. Iteration
30 (right) satisﬁes stopping criteria and therefore CoSAC recovered correct K = 30. Note that this
type of visual representation can be useful in practice to verify choices of ω and R. The following
pm := (cid:80)
theorem establishes the consistency of the CoSAC procedure.
Theorem 1. Suppose {β1  . . .   βK} are the true topics  incenter Cp is given  θm ∼ DirK(α) and
+ . Let ˆK be the estimated number of topics 
(cid:32)(cid:40)
(cid:41)
{ ˆβ1  . . .   ˆβ ˆK} be the output of Algorithm 1 trained with ω and R as in Prop. 2. Then ∀  > 0 
P
j∈{1 ...  ˆK}(cid:107)βi − ˆβj(cid:107) >    for any i ∈ {1  . . .   ˆK}
→ 0 as M → ∞.

k βkθmk for m = 1  . . .   M and α ∈ RK

∪ {K (cid:54)= ˆK}

(cid:33)

min

Remark We found the choices ω = 0.6 and R to be median of {(cid:107)˜p1(cid:107)2  . . .  (cid:107)˜pM(cid:107)2} to be robust in
practice and agreeing with our theoretical results. From Prop. 3 it follows that choosing R as median
1−c )1−1/K ≥
length is equivalent to choosing ω resulting in an edge cut ratio c such that 1 − K
1/2  then c ≤ ( K−1
2K )K/(K−1)  which  for any equilateral topic simplex B  is satisﬁed by setting
ω ∈ (0.3  1)  provided that K ≤ 2000 based on the Eq. (3).
4 Document Conic Scan-and-Cover algorithm

K−1 ( c

M

˜wm:m∈A(cid:107) ˜wm(cid:107)2  where ˜wm := ¯wm − ˆCp ∈ ∆V −1

(cid:80) ¯wm. In practice  M and Nm are

In the topic modeling problem  pm for m = 1  . . .   M are not given. Instead  under the bag-of-words
assumption  we are given the frequencies of words in documents w1  . . .   wM which provide a point
estimate ¯wm := wm/Nm for the pm. Clearly  if number of documents M → ∞ and length of
documents Nm → ∞ ∀m  we can use Algorithm 1 with the plug-in estimates ¯wm in place of pm 
since ¯wm → pm. Moreover  Cp will be estimated by ˆCp := 1
ﬁnite  some of which may take relatively small values. Taking the topic direction to be the farthest
point in the topic simplex  i.e.  v = argmax
  may no
longer yield a robust estimate  because the variance of this topic direction estimator can be quite high
(in the Supplement we show that it is upper bounded with (1 − 1/V )/Nm).
To obtain improved estimates  we propose a technique that we call “mean-shifting”. Instead of taking
the farthest point in the simplex  this technique is designed to shift the estimate of a topic to a high
density region  where true topics are likely to be found. Precisely  given a (current) cone Sω(v)  we
(cid:80)
re-position the cone by updating v := argmin
m∈Sω(v) (cid:107) ˜wm(cid:107)2(1 − cos( ˜wm  v)). In other words 
we re-position the cone by centering it around the mean direction of the cone weighted by the norms
m∈Sω(v) ˜wm/ card(Sω(v)). This results in
of the data points inside  which is simply given by v ∝
reduced variance of the topic direction estimate  due to the averaging over data residing in the cone.
The mean-shifting technique may be slightly modiﬁed and taken as a local update for a subsequent
optimization which cycles through the entire set of documents and iteratively updates the cones. The
optimization is with respect to the following weighted spherical k-means objective:

(cid:80)

v

0

K(cid:88)

(cid:88)

k=1

m∈Sk(vk)

min

(cid:107)vk(cid:107)2=1 k=1 ...K

(cid:107) ˜wm(cid:107)2(1 − cos(vk  ˜wm)) 

(5)

K(cid:70)
where cones Sk(vk) = {m|dcos(vk  ˜pm) < dcos(vl  ˜pi) ∀l (cid:54)= k} yield a disjoint data partition
Sk(vk) = {1  . . .   M} (this is different from Sω(vk)). The rationale of spherical k-means
k=1
optimization is to use full data for estimation of topic directions  hence further reducing the variance
due to short documents. The connection between objective function (5) and topic simplex estimation
is given in the Supplement. Finally  obtain topic norms Rk along the directions vk using maximum
projection: Rk := max

m:m∈Sk(vk)(cid:104)vk  ˜wm(cid:105). Our entire procedure is summarized in Algorithm 2.

Remark In Step 9 of the algorithm  cone Sω(v) with a very low cardinality  i.e.  card(Sω(v)) <
λM  for some small constant λ  is discarded because this is likely an outlier region that does not actu-
ally contain a true vertex. The choice of λ is governed by results of Prop. 4. For small αk = 1/K  ∀k 

5

(K−1)(1−c) and for an equilateral ˜B we can choose d such that cos(d) =
1−ω√1−(1−ω)2 ) +

(cid:19)(cid:19)−1
λ ≤ P(Λc) ≈ c(K−1)/K
ging these values into Eq. (3) leads to c =
Now  plugging in ω = 0.6 we obtain λ ≤ K−1 for large K. Our approximations were based on large
K to get a sense of λ  we now make a conservative choice λ = 0.001  so that (K)−1 > λ ∀K < 1000.
As a result  a topic is rejected if the corresponding cone contains less than 0.1% of the data.

(cid:17) (cid:18)(cid:113) K−1

2K . Plug-

(cid:18)(cid:16)

1 − 1

K2

2K (

(cid:113)

2

.

(cid:113) K+1
(cid:113) K+1

2K

Finding anchor words using Conic Scan-and-Cover Another approach to reduce the noise is
to consider the problem from a different viewpoint  where Algorithm 1 will prove itself useful.
RecoverKL by Arora et al. (2012) can identify topics with diminishing errors (in number of documents
M)  provided that topics contain anchor words. The problem of ﬁnding anchor words geometrically
reduces to identifying rows of the word-to-word co-occurrence matrix that form a simplex containing
other rows of the same matrix (cf. Arora et al. (2012) for details). An advantage of this approach
is that noise in the word-to-word co-occurrence matrix goes to zero as M → ∞ no matter the
document lengths  hence we can use Algorithm 1 with "documents" being rows of the word-to-word
co-occurrence matrix to learn anchor words nonparametrically and then run RecoverKL to obtain
topic estimates. We will call this procedure cscRecoverKL.

Algorithm 1 Conic Scan-and-Cover (CoSAC)
Input: document generating distributions p1  . . .   pM  

(cid:80)

angle threshold ω  norm threshold R

Output: topics β1  . . .   βk
1: ˆCp = 1
m pm {ﬁnd center};
M
2: A1 = {1  . . .   M} {initialize active set};
3: while ∃m ∈ Ak : (cid:107)˜pm(cid:107)2 > R do
4:

˜pm:m∈Ak (cid:107)˜pm(cid:107)2 {ﬁnd topic}

vk = argmax
Sω(vk) = {m : dcos(˜pm  vk) < ω} {ﬁnd cone of near documents}
βk = vk + ˆCp  k = k + 1 {compute topic}

5:
6: Ak = Ak \ Sω(vk) {update active set}
7:
8: end while

˜pm := pm − ˆCp for m = 1  . . .   M {center the data}

k = 1 {initialize topic count}

Figure 2: Iterations 1  26  29  30 of the Algorithm 1. Red are the documents in the cone Sω(vk); blue
are the documents in the active set Ak+1 for next iteration. Yellow are documents (cid:107)˜pm(cid:107)2 < R.

5 Experimental results

5.1 Simulation experiments

In the simulation studies we shall compare CoSAC (Algorithm 2) and cscRecoverKL based on
Algorithm 1 both of which don’t have access to the true K  versus popular parametric topic modeling
approaches (trained with true K): Stochastic Variational Inference (SVI)  Collapsed Gibbs sampler 
RecoverKL and GDM (more details in the Supplement). The comparisons are done on the basis of
minimum-matching Euclidean distance  which quantiﬁes distance between topic simplices (Tang
et al.  2014)  and running times (perplexity scores comparison is given in the Supplement). Lastly we
will demonstrate the ability of CoSAC to recover correct number of topics for a varying K.

6

0.00.20.40.60.81.01.2cosinedistancedcos(v1 ˜pi)0.020.040.060.080.10normk˜pik2topicv1ω=0.60Sω(v1)A20.00.20.40.60.81.01.2cosinedistancedcos(v26 ˜pi)topicv26ω=0.60Sω(v26)A270.00.20.40.60.81.01.2cosinedistancedcos(v29 ˜pi)topicv29ω=0.60Sω(v29)A300.00.20.40.60.81.01.2cosinedistancedcos(v30 ˜pi)topicv30ω=0.60R=0.047Sω(v30)A31Algorithm 2 CoSAC for documents
Input: normalized documents ¯w1  . . .   ¯wM  

(cid:80)

angle threshold ω  norm threshold R  outlier threshold λ

Output: topics β1  . . .   βk
1: ˆCp = 1
m ¯wm {ﬁnd center};
M
2: A1 = {1  . . .   M} {initialize active set};
3: while ∃ m ∈ Ak : (cid:107) ˜wm(cid:107)2 > R do
4:

˜wm:m∈Ak (cid:107) ˜wm(cid:107)2 {initialize direction}
vk = argmax
while vk not converged do {mean-shifting}

˜wm := ¯wm − ˆCp for m = 1  . . .   M {center the data}

k = 1 {initialize topic count}

vk =(cid:80)

Sω(vk) = {m : dcos( ˜wm  vk) < ω} {ﬁnd cone of near documents}

m∈Sω(vk) ˜wm/ card(Sω(vk)) {update direction}

end while
if card(Sω(vk)) > λM

5:
6:
7:
8:
9: Ak = Ak \ Sω(vk) {update active set}
10: end while
11: v1  . . .   vk = weighted spherical k-means (v1  . . .   vk  ˜w1  . . .   ˜wM )
12: for l in {1  . . .   k} do
m:m∈Sl(vl)(cid:104)vl  ˜wm(cid:105) {ﬁnd topic length along direction vl}
13: Rl := max

then k = k + 1 {record topic direction}

βl = Rlvl + ˆCp {compute topic}

14:
15: end for

Figure 3: Minimum matching Euclidean distance for (a) varying corpora size  (b) varying length of
documents; (c) Running times for varying corpora size; (d) Estimation of number of topics.

Figure 4: Gibbs sampler convergence analysis for (a) Minimum matching Euclidean distance for
corpora sizes 1000 and 5000; (b) Perplexity for corpora sizes 1000 and 5000; (c) Perplexity for
NYTimes data.

Estimation of the LDA topics First we evaluate the ability of CoSAC and cscRecoverKL to
estimate topics β1  . . .   βK  ﬁxing K = 15. Fig. 3(a) shows performance for the case of fewer
M ∈ [100  10000] but longer Nm = 500 documents (e.g. scientiﬁc articles  novels  legal documents).
CoSAC demonstrates performance comparable in accuracy to Gibbs sampler and GDM.
Next we consider larger corpora M = 30000 of shorter Nm ∈ [25  300] documents (e.g. news
articles  social media posts). Fig. 3(b) shows that this scenario is harder and CoSAC matches the
performance of Gibbs sampler for Nm ≥ 75. Indeed across both experiments CoSAC only made
mistakes in terms of K for the case of Nm = 25  when it was underestimating on average by 4 topics

7

llllllllllllllllllllll0.0000.0250.0500.0750200040006000800010000Number of documents MMinimum Matching distancellcscRecoverKLRecoverKLCoSACGDMGibbsSVIllllllllllllll0.00.10.20.350100150200250300Length of documents NmMinimum Matching distancellcscRecoverKLRecoverKLCoSACGDMGibbsSVIllllllllllllllllllllll01002003000200040006000800010000Number of documents MRunning time  secllcscRecoverKLRecoverKLCoSACGDMGibbsSVI0102030401020304050True number of topics KAbsolute topic number errorcscRecoverKLCoSACBayes factorlllllllllll0.020.040.06050100150Training time  secMinimum Matching distancelGibbs  M=1000Gibbs  M=5000CoSAC  M=1000CoSAC  M=5000lllllllllll675700725750775050100150Training time  secPerplexitylGibbs  M=1000Gibbs  M=5000CoSAC  M=1000CoSAC  M=50001500155016000500100015002000Training time  minPerplexityLDA GibbsHDP GibbsCoSACand for Nm = 50 when it was off by around 1  which explains the earlier observation. Experiments
with varying V and α are given in the Supplement.
It is worth noting that cscRecoverKL appears to be strictly better than its predecessor. This suggests
that our procedure for selection of anchor words is more accurate in addition to being nonparametric.

Running time A notable advantage of the CoSAC algorithm is its speed. In Fig. 3(c) we see
that Gibbs  SVI  GDM and CoSAC all have linear complexity growth in M  but the slopes are very
different and approximately are INm for SVI and Gibbs (where I is the number of iterations which
has to be large enough for convergence)  number of k-means iterations to converge for GDM and is
of order K for the CoSAC procedure making it the fastest algorithm of all under consideration.
Next we compare CoSAC to per iteration quality of the Gibbs sampler trained with 500 iterations for
M = 1000 and M = 5000. Fig. 4(b) shows that Gibbs sampler  when true K is given  can achieve
good perplexity score as fast as CoSAC and outperforms it as training continues  although Fig. 4(a)
suggests that much longer training time is needed for Gibbs sampler to achieve good topic estimates
and small estimation variance.

Estimating number of topics Model selection in the LDA context is a quite challenging task and 
to the best of our knowledge  there is no "go to" procedure. One of the possible approaches is based
on reﬁtting LDA with multiple choices of K and using Bayes Factor for model selection (Grifﬁths &
Steyvers  2004). Another option is to adopt the Hierarchical Dirichlet Process (HDP) model  but we
should understand that it is not a procedure to estimate K of the LDA model  but rather a particular
prior on the number of topics  that assumes K to grow with the data. A more recent suggestion is to
slightly modify LDA and use Bayes moment matching (Hsu & Poupart  2016)  but  as can be seen
from Figure 2 of their paper  estimation variance is high and the method is not very accurate (we
tried it with true K = 15 and it took above 1 hour to ﬁt and found 35 topics). Next we compare
Bayes factor model selection versus CoSAC and cscRecoverKL for K ∈ [5  50]. Fig. 3(d) shows that
CoSAC consistently recovers exact number of topics in a wide range.
We also observe that cscRecoverKL does not estimate K well (underestimates) in the higher range.
This is expected because cscRecoverKL ﬁnds the number of anchor words  not topics. The former
is decreasing when later is increasing. Attempting to ﬁt RecoverKL with more topics than there
are anchor words might lead to deteriorating performance and our modiﬁcation can address this
limitation of the RecoverKL method.

5.2 Real data analysis

In this section we demonstrate CoSAC algorithm for topic modeling on one of the standard bag
of words datasets — NYTimes news articles. After preprocessing we obtained M ≈ 130  000
documents over V = 5320 words. Bayes factor for the LDA selected the smallest model among
K ∈ [80  195]  while CoSAC selected 159 topics. We think that disagreement between the two
procedures is attributed to the misspeciﬁcation of the LDA model when real data is in play  which
affects Bayes factor  while CoSAC is largely based on the geometry of the topic simplex.
The results are summarized in Table 1 — CoSAC found 159 topics in less than 20min; cscRecoverKL
estimated the number of anchor words in the data to be 27 leading to fewer topics. Fig. 4(c) compares
CoSAC perplexity score to per iteration test perplexity of the LDA (1000 iterations) and HDP (100
iterations) Gibbs samplers. Text ﬁles with top 20 words of all topics are included in the Supplementary
material. We note that CoSAC procedure recovered meaningful topics  contextually similar to LDA
and HDP (e.g. elections  terrorist attacks  Enron scandal  etc.) and also recovered more speciﬁc topics
about Mike Tyson  boxing and case of Timothy McVeigh which were present among HDP topics  but
not LDA ones. We conclude that CoSAC is a practical procedure for topic modeling on large scale
corpora able to ﬁnd meaningful topics in a short amount of time.

6 Discussion

We have analyzed the problem of estimating topic simplex without assuming number of vertices
(i.e.  topics) to be known. We showed that it is possible to cover topic simplex using two types of
geometric shapes  cones and a sphere  leading to a class of Conic Scan-and-Cover algorithms. We

8

Table 1: Modeling topics of NYTimes articles

cscRecoverKL
HDP Gibbs
LDA Gibbs
CoSAC

K Perplexity
27
2603
221 ± 5
80
159

Coherence
-238
1477 ± 1.6 −442 ± 1.7
1520 ± 1.5 −300 ± 0.7
-322

1568

Time
37 min
35 hours
5.3 hours
19 min

then proposed several geometric correction techniques to account for the noisy data. Our procedure is
accurate in recovering the true number of topics  while remaining practical due to its computational
speed. We think that angular geometric approach might allow for fast and elegant solutions to other
clustering problems  although as of now it does not immediately offer a unifying problem solving
framework like MCMC or variational inference. An interesting direction in a geometric framework is
related to building models based on geometric quantities such as distances and angles.

Acknowledgments

This research is supported in part by grants NSF CAREER DMS-1351362  NSF CNS-1409303  a
research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award.

9

References
Anandkumar  A.  Foster  D. P.  Hsu  D.  Kakade  S. M.  and Liu  Y. A spectral algorithm for Latent Dirichlet

Allocation. NIPS  2012.

Arora  S.  Ge  R.  Halpern  Y.  Mimno  D.  Moitra  A.  Sontag  D.  Wu  Y.  and Zhu  M. A practical algorithm for

topic modeling with provable guarantees. arXiv preprint arXiv:1212.4777  2012.

Blei  D. M.  Ng  A. Y.  and Jordan  M. I. Latent Dirichlet Allocation. J. Mach. Learn. Res.  3:993–1022  March

2003.

Deerwester  S.  Dumais  S. T.  Furnas  G. W.  Landauer  T. K.  and Harshman  R. Indexing by latent semantic

analysis. Journal of the American Society for Information Science  41(6):391  Sep 01 1990.

Grifﬁths  Thomas L and Steyvers  Mark. Finding scientiﬁc topics. PNAS  101(suppl. 1):5228–5235  2004.

Hoffman  Ma. D.  Blei  D. M.  Wang  C.  and Paisley  J. Stochastic variational inference. J. Mach. Learn. Res. 

14(1):1303–1347  May 2013.

Hsu  Wei-Shou and Poupart  Pascal. Online bayesian moment matching for topic modeling with unknown

number of topics. In Advances In Neural Information Processing Systems  pp. 4529–4537  2016.

Nguyen  XuanLong. Posterior contraction of the population polytope in ﬁnite admixture models. Bernoulli  21

(1):618–646  02 2015.

Pritchard  Jonathan K  Stephens  Matthew  and Donnelly  Peter. Inference of population structure using multilocus

genotype data. Genetics  155(2):945–959  2000.

Tang  Jian  Meng  Zhaoshi  Nguyen  Xuanlong  Mei  Qiaozhu  and Zhang  Ming. Understanding the limiting
In Proceedings of The 31st International

factors of topic modeling via posterior contraction analysis.
Conference on Machine Learning  pp. 190–198. ACM  2014.

Teh  Y. W.  Jordan  M. I.  Beal  M. J.  and Blei  D. M. Hierarchical dirichlet processes. Journal of the american

statistical association  101(476)  2006.

Xu  Wei  Liu  Xin  and Gong  Yihong. Document clustering based on non-negative matrix factorization. In
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in
Informaion Retrieval  SIGIR ’03  pp. 267–273. ACM  2003.

Yurochkin  Mikhail and Nguyen  XuanLong. Geometric dirichlet means algorithm for topic inference. In

Advances in Neural Information Processing Systems  pp. 2505–2513  2016.

10

,Mikhail Yurochkin
Aritra Guha
XuanLong Nguyen