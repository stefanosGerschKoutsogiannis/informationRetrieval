2019,Global Convergence of Gradient Descent  for Deep Linear Residual Networks,We analyze the global convergence of gradient descent for deep linear residual
  networks by proposing a new initialization: zero-asymmetric (ZAS)
  initialization. It is motivated by avoiding stable manifolds of saddle points.
  We prove that under the ZAS initialization  for an arbitrary target matrix 
  gradient descent converges to an $\varepsilon$-optimal point in $O\left( L^3
  \log(1/\varepsilon) \right)$ iterations  which scales polynomially with the
  network depth $L$. Our result and the $\exp(\Omega(L))$ convergence time for the 
  standard initialization (Xavier or near-identity)
  \cite{shamir2018exponential} together demonstrate the importance of the
  residual structure and the initialization in the optimization for deep linear
  neural networks  especially when $L$ is large.,Global Convergence of Gradient Descent

for Deep Linear Residual Networks

Lei Wu∗ Qingcan Wang∗ Chao Ma

Program in Applied and Computational Mathematics

Princeton University

Princeton  NJ 08544  USA

{leiwu qingcanw chaom}@princeton.edu

Abstract

We analyze the global convergence of gradient descent for deep linear residual
networks by proposing a new initialization: zero-asymmetric (ZAS) initialization.
It is motivated by avoiding stable manifolds of saddle points. We prove that under
the ZAS initialization  for an arbitrary target matrix  gradient descent converges

to an ε-optimal point in O(cid:0)L3 log(1/ε)(cid:1) iterations  which scales polynomially

with the network depth L. Our result and the exp(Ω(L)) convergence time for
the standard initialization (Xavier or near-identity) [18] together demonstrate the
importance of the residual structure and the initialization in the optimization for
deep linear neural networks  especially when L is large.

1

Introduction

It is widely observed that simple gradient-based optimization algorithms are efﬁcient for training deep
neural networks [21]  whose landscape is highly non-convex. To explain the efﬁciency  traditional
optimization theories cannot be directly applied and the special structures of neural networks must be
taken into consideration. Recently many researches are devoted to this topic [13  21  4  7  6  1  23 
15  16]  but the theoretical understanding is still far from sufﬁcient.
In this paper  we focus on a simpliﬁed case: the deep linear neural network

f (x; W1  . . .   WL) = WLWL−1 ··· W1x 

(1.1)
where W1  . . .   WL are the weight matrices and L is the depth. Linear networks are simple since
they can only represent linear transformation  but they preserve one of the most important aspects of
deep neural networks  the layered structure. Therefore  analysis of linear networks will be helpful for
understanding nonlinear cases. For example  the random orthogonal initialization proposed in [17]
that analyzes the gradient descent dynamics of deep linear networks was later shown to be useful for
training recurrent networks with long term dependences [19].
Despite the simplicity  the optimization of deep linear neural networks is still far from being well
understood  especially the global convergence. [18] proves that the number of iterations required for
convergence could scales exponentially with the depth L. The result requires two conditions: (1) the
width of each layer is 1; (2) the gradient descent starts from the standard Xavier [9] or near-identity
[11] initialization. If these conditions break  the negative results does not imply that gradient descent
cannot efﬁciently learn deep linear networks in general. [5] shows that if the width of every layer
increases with the network depth  gradient descent with the Gaussian random initialization does ﬁnd
the global minima while the convergence time only scales polynomially with the depth. Here we
attempt to circumvent the negative result in [18] by using better initialization strategies instead of
increasing the width.
∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Our Contributions We propose the zero-asymmetric (ZAS) initialization  which initializes the
output layer WL to be zero and all the other layers Wl  l = 1  . . .   L − 1 to be identity. So it is a
linear residual network with all the residual blocks and the output layer being zero. We then analyze
how the initialization affects the gradient descent dynamics.

• We prove that starting from the ZAS initialization  the number of iterations required for

gradient descent to ﬁnd an ε-optimal point is O(cid:0)L3 log(1/ε)(cid:1). The only requirement for

the network is that the width of each layer is not less than the input dimension and the result
applies to arbitrary target matrices.

• We numerically compare the gradient descent dynamics between the ZAS and the near-
identity initialization for multi-dimensional deep linear networks. The comparison clearly
shows that the convergence of gradient descent with the near-identity initialization involves
a saddle point escape process  while the ZAS initialization never encounters any saddle
point during the whole optimization process.

• We provide an extension of the ZAS initialization to the nonlinear case. Moreover  the

numerical experiments justify its superiority compared to the standard initializations.

1.1 Related work

Linear networks The ﬁrst line of works analyze the whole landscape. The early work [3] proves
that for two-layer linear networks  all the local minima are also global minima  and this result is
extended to deep linear networks in [13  14]. [10] provides a simpler proof of this result for deep
residual networks  and shows that the Polyak-Łojasiewicz condition is satisﬁed in a neighborhood of
a global minimum. However  these results do not imply that gradient descent can ﬁnd global minima 
and also cannot tell us the number of iterations required for convergence.
The second line of works directly deal with the trajectory of gradient descent dynamics  and our
work lies in this venue. [17] provides an analytic analysis to the gradient descent dynamics of linear
networks  which nevertheless does not show that gradient descent can ﬁnd global minima. [12]
studies the properties of solutions that the gradient descent converges to  without providing any
convergence rate. [4  2] consider the following simpliﬁed objective function for whitened data 

R(W1  . . .   WL) =

(cid:107)WL ··· W1 − Φ(cid:107)2
F .

1
2

l+1Wl+1 − WlW T

Speciﬁcally  [4] analyzes the convergence of gradient descent with the identity initialization: WL =
··· = W1 = I  and proves that if the target matrix Φ is positive semi-deﬁnite or the initial loss is small
enough  a polynomial-time convergence can be guaranteed. [2] extends the analysis to more general
target matrices by imposing more conditions on the initialization: (1) approximately balance condition 
(cid:107)W T
l (cid:107)F ≤ δ; (2) rank-deﬁcient condition  (cid:107)WL ··· W1 − Φ(cid:107)F ≤ σmin(Φ) − c for
a constant c > 0. The condition (2) still requires small initial loss  thus the convergence is local in
nature. As a comparison  we do not impose any assumption on the target matrix or the initial loss.
As mentioned above  our work is closely related to [18]  which proves that for one-dimensional deep
linear networks  gradient descent with the standard Xavier or near-identity initialization requires
at least exp(Ω(L)) iterations for ﬁtting the target matrix Φ = −I. However  our result shows that
this difﬁculty can be overcome by adopting a better initialization. [5] shows that if the width of
each layer is larger than Ω(L log(L))  then gradient descent converges to global minima at a rate
O(log(1/ε)). As a comparison  our result only requires that the width of each layer is not less than
the input dimension.

Nonlinear networks
[6  1  23] establish the global convergence for deep networks with the width
m ≥ poly(n  L)  where n denotes the number of training examples. [8] proves a similar result
but for speciﬁc neural networks with long-distance skip connections  which only requires the depth
L ≥ poly(n) and the width m ≥ d + 1  where d is the input dimension.
The ZAS initialization we propose also closely resembles the “ﬁxup initialization” recently proposed
in [22]. Therefore  our result partially provides a theoretical explanation to the efﬁciency of ﬁxup
initialization for training deep residual networks.

2

2 Preliminaries
Given training data {(xi  yi)}n
layers is deﬁned as

i=1 where xi ∈ Rdx and yi ∈ Rdy  a linear neural network with L
f (x; W1  . . .   WL) = WLWL−1 ··· W1x 
(2.1)
where Wl ∈ Rdl×dl−1  l = 1  . . .   L are parameter matrices  and d0 = dx  dL = dy. Then the
least-squares loss

˜R(W1  . . .   WL) def=

(cid:107)WLWL−1 ··· W2W1X − Y (cid:107)2
F  
where X = (x1  x2  . . .   xn) ∈ Rdx×n and Y = (y1  y2  . . .   yn) ∈ Rdy×n.
Following [4  2]  in this paper we focus on the following simpliﬁed objective function

1
2

(2.2)

R(W1  . . .   WL) def=

(2.3)
where Wl ∈ Rd×d  l = 1  . . .   L and Φ ∈ Rd×d is the target matrix. Here we assume dl = d 
l = 1  . . .   L for simplicity.
The gradient descent is given by

(cid:107)WLWL−1 ··· W2W1 − Φ(cid:107)2
F  

1
2

Wl(t + 1) = Wl(t) − η∇lR(t) 

(2.4)
In the following  we will always use the index t to denote the value of a variable after the t-th iteration.
∇lR is the gradient of R with respect to the weight matrix Wl:

l = 1  . . .   L  t = 0  1  2  . . .

∇lR def=

= W T

L:l+1(WL:1 − Φ)W T

l−1:1 

∂R
∂Wl

where Wl2:l1
iterations.

def= Wl2Wl2−1 ··· Wl1+1Wl1. Moreover  we keep the learning rate η > 0 ﬁxed for all

Notations
In matrix equations  let I and 0 be the d-dimensional identity matrix and zero matrix
respectively. Let λmin(S) be the minimal eigenvalue of a symmetric matrix S and σmin(A) be the
minimal singular value of a square matrix A. Let (cid:107)A(cid:107)F and (cid:107)A(cid:107)2 be the Frobenius norm and (cid:96)2 norm
of matrix A respectively. Recall that A(t) denotes the value of any variable A after the t-th iteration 
and ∇lR is the gradient of R with respect to the weight matrix Wl. We use standard notation O(·)
and Ω(·) to hide constants independent of network depth L.

3 Zero-asymmetric initialization

In this section  we ﬁrst describe the zero-asymmetric initialization  and then illustrate by a simple
example why this special initialization is helpful for optimization.
Deﬁnition. For deep linear neural network (2.3)  deﬁne the zero-asymmetric (ZAS) initialization as
(3.1)

Wl(0) = I  l = 1  . . .   L − 1 

and WL(0) = 0.

Under the ZAS initialization  the function represented by the network is a zero matrix. While
commonly used initialization such as the Xavier and the near-identity initialization treats all the layers
equally  our initialization takes the output layer differently. In this sense  we call the initialization
asymmetric.
Let Wl = I + Ul  l = 1  . . .   L − 1  then the linear network has the residual form

R =

1
2

(cid:107)WL(I + UL−1)··· (I + U1) − Φ(cid:107)2
F .

Since ∂R/∂Ul = ∂R/∂Wl  the dynamics will be the same as ZAS if we initialize Ul(0) = WL(0) =
0. Therefore  ZAS is equivalent to initializing all the residual blocks and the output layer with zero in
a linear residual network. From this perspective  the ZAS initialization closely resembles the “ﬁxup
initialization” [22] for nonlinear ResNets.

3

Figure 1: (Left) The landscape of the toy model R(w1  w2) and the two gradient descent trajectories.
(Right) The dynamics of loss for two gradient descent trajectories. The blue curve is the gradient
descent trajectory initialized from (1−0.001  1+0.001) (near-identity)  and the red curve corresponds
to the ZAS initialization (1  0). We observe that the blue curve takes a long time in the neighborhood
of saddle point (0  0)  however the red curve does not.

Understanding the role of initialization Following [18]  consider the following optimization
problem for one-dimensional linear network with target Φ = −1:

(3.2)
The origin O(0  . . .   0) is a saddle point of R  so gradient descent with small initialization  e.g. 
Xavier initialization  will spend long time escaping the neighborhood of O. In addition 

R(w1  w2  . . .   wL) = (wLwL−1 ··· w1 + 1)2/2.

near-identity initialization introduces perturbation to leave M: wl(0) ∼ N(cid:0)1  σ2(cid:1)  l = 1  . . .   L

is a stable manifold of O  i.e.  gradient ﬂow starting from any point in M will converge to O. The

M = {(w1  . . .   wL) : w1 = w2 = ··· = wL ≥ 0}

for some small σ. However  [18] proves that it will still be attracted to the neighborhood of O  thus
cannot guarantee the polynomial-time converge. As a comparison  the ZAS initialization breaks the
symmetry by initialize the output layer to be 0.
Figure 1 provides a numerical result for depth L = 2. The near-identity initialization (blue curve)
spends long time escaping the saddle region  while the ZAS initialization (red curve) converges to the
global minima without attraction by the saddle point.

4 Main results

We ﬁrst provide and prove the continuous version of our main convergence result  i.e.  the limit
dynamics when η → 0. Then we give the result for discrete gradient descent  whose detailed proof is
left to the appendix.

4.1 Continuous-time gradient descent

The continuous-time gradient descent dynamics is given by

˙Wl(t) = −∇lR(t) 

l = 1  . . .   L  t ≥ 0.

(4.1)
In this section  we always denote ˙A(t) = dA(t)/dt for any variable A depending on t. For the
continuous dynamics  we have the following convergence result.
Theorem 4.1 (Continuous-time gradient descent). For the deep linear network (2.3)  the continuous-
time gradient descent (4.1) with the zero-asymmetric initialization (3.1) satisﬁes

R(t) ≤ e−2tR(0) 

t ≥ 0 

(4.2)

for any Φ ∈ Rd×d and L ≥ 1.
The theorem above holds for arbitrary Φ  and does not require depth or width to be large. To prove
the theorem  we ﬁrst deﬁne a group of invariant matrices as following. Note that they also play a key
role in the analysis of [2].

4

1.00.50.00.51.01.00.50.00.51.0(0 0): saddle point020406080100120140Number of iterations107105103101101LossGD(0.999  1.001)GD(1 0)Deﬁnition. For a deep linear network (2.3)  deﬁne the invariant matrix

Dl = W T

l+1Wl+1 − WlW T
l  

l = 1  2  . . .   L − 1.

(4.3)

Lemma 4.2. The invariant matrices (4.3) are indeed invariances under continuous-time gradient
descent (4.1)  i.e.  Dl(t) = Dl(0) for l = 1  . . .   L − 1 and t ≥ 0.

Proof. Recall that

˙Wl = −∇lR = −W T

L:l+1(WL:1 − Φ)W T

l−1:1 

we have

then

˙Dl =

d
dt

(cid:2)W T

˙WlW T

l = −W T
L:l+1(WL:1 − Φ)W T
(cid:104)

W T

l+1

˙Wl+1 − ˙WlW T

l

(cid:3) =

(cid:105)

(cid:104)

l+1Wl+1 − WlW T

l

l:1 = W T
l+1

˙Wl+1 

+

W T

l+1

˙Wl+1 − ˙WlW T

l

(cid:105)T

= 0.

Therefore  Dl(t) = Dl(0).

Proof of Theorem 4.1. From the ZAS initialization  Dl(t) = Dl(0) = 0  l = 1  . . .   L − 2 and
DL−1(t) = DL−1(0) = −I  i.e. 

WlW T

l = W T

WL−1W T

L−1 = I + W T

l+1Wl+1 
L WL.

l = 1  . . .   L − 2 

WL−1:1W T

L−1:2 = WL−1:2W T
L−1:1 = WL−1:2W1W T
1 W T
2
= WL−1:3(W2W T
W T
2 )
= ···

L−1:3

2 W2W T

L−1:2

So we have

and

Then

 

L−1

L WL

=(cid:0)WL−1W T
=(cid:0)I + W T
F =(cid:13)(cid:13)(WL:1 − Φ)W T
(cid:16)(cid:0)I + W T
(cid:16)∇T

l R(t) ˙Wl(t)

(cid:1)L−1
(cid:1)L−1
(cid:13)(cid:13)2
(cid:1)L−1(cid:17) · 2R ≥ 2R.
= − L(cid:88)

(cid:107)∇lR(cid:107)2

min(WL−1:1)(cid:107)WL:1 − Φ(cid:107)2

F = λmin

≥ σ2

= λmin

L−1:1

L(cid:88)

L WL

(cid:17)

tr

F

(cid:107)∇LR(cid:107)2

˙R(t) =

(cid:0)WL−1:1W T

L−1:1

(cid:1) · 2R

(4.4)

F ≤ −(cid:107)∇LR(cid:107)2

F ≤ −2R.

l=1

l=1

Therefore  R(t) ≤ e−2tR(0).

(cid:20) Id0
(cid:0)X TX(cid:1) > 0  following the similar proof  we will have (cid:107)∇L ˜R(cid:107)2

Remark. (1). For rectangular weight matrices Wl ∈ Rdl×dl−1  if dl ≥ d0 = dx  l = 1  . . .   L − 1 
  then the
we can always ignore the redundant nodes by initializing WL = 0 and Wl =
proof of Theorem 4.1 still holds. (2). For the general square loss ˜R in (2.2) with un-whitened data
F ≥ 2λX ˜R  and
X  if λX
˜R(t) ≤ e−2λX t ˜R(0).

def= λmin

(cid:21)

0
0

0

5

4.2 Discrete-time gradient descent

η ≤ min

Now we consider the discrete-time gradient descent (2.4). The main theorem is stated below.
Theorem 4.3 (Discrete gradient descent). For deep linear network (2.3) with the zero-asymmetric
initialization (3.1) and discrete-time gradient descent (2.4)  if the learning rate satisﬁes

 (cid:0)144L2φ4(cid:1)−1(cid:111)
where φ = max(cid:8)2(cid:107)Φ(cid:107)F   3L−1/2  1(cid:9)  then we have linear convergence
R(t) ≤(cid:16)
Since the learning rate η = O(cid:0)L−3(cid:1)  the theorem indicates that gradient descent can achieve
R(t) ≤ ε in O(cid:0)L3 log(1/ε)(cid:1) iterations.

(cid:110)(cid:0)4L3φ6(cid:1)−1
(cid:17)tR(0) 

1 − η
2

t = 0  1  2  . . .

(4.5)

4.2.1 Overview of the proof

The following is the proof sketch  and the detailed proof is deferred to the appendix.
The approach to the discrete-time result is similar to the continuous-time case. However  the matrices
deﬁned in (4.3) are not exactly invariant  but change slowly during the training process  which need
to be controlled carefully.
First  we propose the following three conditions  and prove that the ﬁrst condition implies the other
two.

Approximate invariances For invariant matrices deﬁned in (4.3) 

(cid:107)Dl(cid:107)2 = O(cid:0)L−3(cid:1)   l = 1  . . .   L − 2 

Weight bounds For weight matrices Wl 

and (cid:107)I + DL−1(cid:107)2 = O(cid:0)L−2(cid:1) .
L−1/2(cid:17)

(cid:107)WL−1(cid:107) = O

(cid:16)

and

(4.6)

.

(4.7)

(cid:18) log L

(cid:19)

L

(cid:107)Wl(cid:107)2 = 1 + O

  l = 1  . . .   L − 1 

Gradient bound The gradient of the last layer

(4.8)
Lemma 4.4. The approximate invariances condition (4.6) implies the weight bounds (4.7) and the
gradient bound (4.8).

(cid:107)∇LR(cid:107)2

F ≥ R.

Second  to show that (4.6)–(4.8) always holds during the training process  we need to estimate the
change of invariant matrix Dl(t + 1) − Dl(t) and the decrease of loss R(t + 1) − R(t) in one step.
Lemma 4.5. If the weight bounds (4.7) hold at iteration t  then the change of invariant matrices
after one-step update with learning rate η satisﬁes

(cid:107)Dl(t + 1) − Dl(t)(cid:107)2 = O(cid:0)η2(cid:1)R(t)  l = 1  . . .   L − 2 
(cid:107)DL−1(t + 1) − DL−1(t)(cid:107)2 = O(cid:0)η2L(cid:1)R(t).

(4.9)
Lemma 4.6. If the weight bounds (4.7) and the gradient bound (4.8) hold  and the learning rate

η = O(cid:0)L−2(cid:1)  then the loss function

R(t + 1) ≤(cid:16)

(cid:17)R(t).

1 − η
2

(4.10)

With the three lemmas above  we are now ready to prove Theorem 4.3.

Proof of Theorem 4.3 (informal). We do induction on (4.5) and (4.6). Assume that they hold for
0  1  . . .   t. From the three lemmas above  (4.7)–(4.10) also hold for 0  1  . . .   t. So the loss function

R(t + 1) ≤(cid:16)

(cid:17)R(t) ≤(cid:16)

1 − η
2

(cid:17)t+1R(0) 

1 − η
2

6

1 − η
2

s=0

η

t(cid:88)

s=0

(cid:17)s ≤ 2

R(0) = O(cid:0)η−1(cid:1) .

i.e.  (4.5) holds for t + 1. Now we have
R(s) ≤ R(0)

Recall that the invariant matrices Dl(0) = 0  l = 1  . . .   L − 2 and I + DL−1(0) = 0 at the

(cid:16)
t(cid:88)
initialization  and η = O(cid:0)L−3(cid:1). From (4.9) 
for l = 1  . . .   L − 2. Similarly  (cid:107)I + DL−1(t + 1)(cid:107)2 ≤ O(ηL) = O(cid:0)L−2(cid:1)  i.e.  (4.6) holds for
ization with perturbation: Wl(0) ∼ N(cid:0)I  σ2(cid:1)  l = 1  . . .   L − 1 and WL(0) ∼ N(cid:0)0  σ2(cid:1)  where σ

t + 1. Then we complete the induction.
Remark. Following the proof sketch  we can actually prove Theorem 4.3 under “near-ZAS” initial-

R(s) = O(η) = O(cid:0)L−3(cid:1) .

(cid:107)Dl(t + 1)(cid:107)2 ≤ t(cid:88)

(cid:107)Dl(s + 1) − Dl(s)(cid:107)2 = O(η2)

t(cid:88)

s=0

s=0

is sufﬁciently small such that the approximate invariances condition (4.6) holds at the initialization.
Note that the constants hidden in O(·) may depend on the target matrix Φ.

5 Numerical experiments

5.1 Dependence on the depth

O(cid:0)L3(cid:1)  which holds for any target matrix in Rd×d. The ﬁrst experiment examines how this depth

Theorem 4.3 theoretically shows that the number of iterations required for convergence is at most

dependence behaves in practice.
In experiments  we generate target matrices in two ways:

Both d = 2 and d = 100 are considered.

• Gaussian random matrix: Φ = (φij) ∈ Rd×d with φij independently drawn from N (0  1).
• Negative identity matrix: Φ = −I ∈ Rd×d. This target is adopted from [18]  which proves
that in the case d = 1  the number of iteration required for convergence under the Xavier
and the near-identity initialization scales exponentially with the depth L. Both d = 1 and
d = 100 are considered.

The ZAS initialization (3.1) is applied for linear neural networks with different depth L  and we
manually tune the optimal learning rate for each L. As suggested by Theorem 4.3  we numerically
ﬁnd that the optimal learning rate decrease with L.
Figure 2 shows number of iterations required to make the objective R ≤ ε = 10−10. It is clear to
see that the number of iterations required roughly scale as O (Lγ)  where γ ≈ 1/2 for the negative
identity matrix and γ ≈ 1 for the Gaussian random matrices. These scalings are better than the
theoretical γ = 3 in Theorem 4.3  which is a worst case result.

5.2 Comparison with near-identity initialization in multi-dimensional cases

The near-identity initialization initializes each layer by

(Ul)ij ∼ N (0  1/(dL)) i.i.d. 

l = 1  . . .   L

Wl = I + Ul 

(5.1)
where I is the identity matrix. Numerically  it was observed in [18] that for multi-dimensional
networks (d = 25 in the experiments)  gradient descent with the initialization (5.1) requires number
of iterations to scale only polynomially with the depth  instead of exponentially. Here we compare
it with the ZAS initialization by ﬁtting negative identity matrix with 6-layer linear networks. The
learning rate η = 0.01 for both initialization.
Figure 3 shows the dynamics trajectories for both initializations. It strongly suggests that the ZAS
initialization is more efﬁcient than the near-identity initialization (5.1). Gradient descent with the
near-identity initialization is attracted to a saddle region  spends a long time escaping that region  and
then converges fast to a global minimum. As a comparison  gradient descent with ZAS initialization
does not encounter any saddle region during the whole optimization process.

7

Figure 2: Number of iterations required for the ZAS initialization to reach an ε-optimal solution
where ε = 10−10. Two type of target matrices  negative identity and Gaussian random matrices are
considered. It is shown that the number of iterations required scales polynomially with the network
depth.

Figure 3: Comparison between the ZAS and the near-identity initialization. The 5 dashed lines
correspond to the multiple runs of gradient descent with the near-identity initialization. It is shown
that GD with the near-identity successfully escape the saddle region only 2 of 5 times in the given
number of iterations  while the ZAS does not suffer from the attraction of saddle point at all.

6 An extension to nonlinear Residual networks
Consider the following residual network f : Rd → Rd(cid:48)

:

z0 = V0x 
zl = zl−1 + Ulσ(Vlzl−1) 

l = 1  . . .   L 

f (x) = UL+1zL 

(6.1)
where V0 ∈ RD×d  Ul ∈ RD×m  Vl ∈ Rm×D and UL+1 ∈ Rd(cid:48)×D; d is the input dimension  d(cid:48) is
the output dimension  m is the width of the residual blocks and D is the width of skip connections.
For the nonlinear residual network (6.1)  we propose the following modiﬁed ZAS (mZAS) initialization:

Ul = 0 

l = 1  2  . . .   L + 1 

(Vl)ij ∼ N (0  1/D) i.i.d. 

l = 0  1  . . .   L.

(6.2)

We test two types of initialization: (1) standard Xavier initialization; (2) mZAS initialization (6.2).
The experiments are conducted on Fashion-MNIST [20]  where we select 1000 training samples
forming the new training set to speed up the computation. Depth L = 100  200  2000  10000 are
tested  and the learning rate for each depth is tuned to the achieve the fastest convergence. The results
are displayed in Figure 4.
It is shown that mZAS initialization always outperforms the Xavier initialization. Moreover  gradient
descent with mZAS initialization is even able to successfully optimize a 10000-layer residual network.
It clearly demonstrates that the ZAS-type initialization can be helpful for optimizing deep nonlinear
residual networks.

8

101102103104Depth101102NumberofiterationsNegativeidentityd=1d=100101102103104Depth101102103NumberofiterationsGaussianrandommatricesd=2d=100025005000750010000Numberofiterations10−810−610−410−2100102LossZAS025005000750010000Numberofiterations10−610−3100103NormofthegradientsZASFigure 4: For the nonlinear residual network and Fashion-MNIST dataset  the mZAS initialization
outperforms the Xavier initialization. The latter blow up for depth L = 2000  10000. The learning
rates are tuned to achieve the fastest convergence.

7 Conclusion

In this paper we propose the ZAS initialization for deep linear residual network  under which gradient
descent converges to global minima for arbitrary target matrices with linear rate. Moreover  the
rate only scales polynomially with the network depth. Numerical experiments show that the ZAS
initialization indeed avoids the attraction of saddle points  comparing to the near-identity initialization.
This type of initialization may be extended to the analysis of deep nonlinear residual networks  which
we leave as future work.

Acknowledgments

We are grateful to Prof. Weinan E for helpful discussions  and the anonymous reviewers for valuable
comments and suggestions. This work is supported in part by a gift to Princeton University from
iFlytek and the ONR grant N00014-13-1-0338.

References
[1] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning  pages 242–252 
2019.

[2] Sanjeev Arora  Nadav Cohen  Noah Golowich  and Wei Hu. A convergence analysis of
gradient descent for deep linear neural networks. In International Conference on Learning
Representations  2019.

[3] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning

from examples without local minima. Neural networks  2(1):53–58  1989.

[4] Peter Bartlett  Dave Helmbold  and Phil Long. Gradient descent with identity initialization
In International Conference on

efﬁciently learns positive deﬁnite linear transformations.
Machine Learning  pages 520–529  2018.

[5] Simon S. Du and Wei Hu. Width provably matters in optimization for deep linear neural

networks. arXiv preprint arXiv:1901.08572  2019.

[6] Simon S. Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds

global minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018.

[7] Simon S. Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably
In International Conference on Learning

optimizes over-parameterized neural networks.
Representations  2019.

[8] Weinan E  Chao Ma  Qingcan Wang  and Lei Wu. Analysis of the gradient descent algorithm for
a deep neural network model with skip-connections. arXiv preprint arXiv:1904.05263  2019.

9

020406080100Numberofiterations(×100)20406080100TrainingAccuracy(%)L=100 lr=1e-1 mZASL=200 lr=1e-1 mZASL=2000 lr=2e-2 mZASL=10000 lr=2e-3 mZASL=100 lr=1e-3 XavierL=200 lr=1e-6 Xavier[9] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward

neural networks. In Aistats  volume 9  pages 249–256  2010.

[10] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on

Learning Representations  2017.

[11] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 770–778  2016.

[12] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv

preprint arXiv:1810.02032  2018.

[13] Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information

Processing Systems  pages 586–594  2016.

[14] Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima

are global. In International Conference on Machine Learning  pages 2908–2913  2018.

[15] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layers neural networks. In Proceedings of the National Academy of Sciences  volume 115 
pages E7665–E7671  2018.

[16] Grant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915  2018.

[17] Andrew M. Saxe  James L. McClelland  and Surya Ganguli. Exact solutions to the nonlinear

dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120  2013.

[18] Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep

linear neural networks. arXiv preprint arXiv:1809.08587  2018.

[19] Eugene Vorontsov  Chiheb Trabelsi  Samuel Kadoury  and Chris Pal. On orthogonality and
learning recurrent networks with long term dependencies. In International Conference on
Machine Learning  pages 3570–3578  2017.

[20] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: A novel image dataset for

benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747  2017.

[21] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning
Representations  2017.

[22] Hongyi Zhang  Yann N. Dauphin  and Tengyu Ma. Fixup initialization: Residual learning

without normalization. In International Conference on Learning Representations  2019.

[23] Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent optimizes

over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888  2018.

10

,Lei Wu
Qingcan Wang
Chao Ma