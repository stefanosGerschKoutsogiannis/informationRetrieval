2012,Putting Bayes to sleep,We consider sequential prediction algorithms that are given the predictions from a set of models as inputs. If the nature of the data is changing over time in that different models predict well on different segments of the data  then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior (kind of like a weak restart). However  what if the favored models in each segment are from a small subset  i.e. the data is likely to be predicted well by models that predicted well before? Curiously  fitting such ''sparse composite models'' is achieved by mixing in a bit of all the past posteriors. This self-referential updating method is rather peculiar  but it is efficient and gives superior performance on many natural data sets. Also it is important because it introduces a long-term memory: any model that has done well in the past can be recovered quickly. While Bayesian interpretations can be found for mixing in a bit of the initial prior  no Bayesian interpretation is known for mixing in past posteriors.  We build atop the ''specialist'' framework from the online learning literature to give the Mixing Past Posteriors update a proper Bayesian foundation. We apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound.,Putting Bayes to sleep

Wouter M. Koolen∗

Dmitry Adamskiy†

Manfred K. Warmuth‡

Abstract

We consider sequential prediction algorithms that are given the predictions from
a set of models as inputs. If the nature of the data is changing over time in that
different models predict well on different segments of the data  then adaptivity is
typically achieved by mixing into the weights in each round a bit of the initial prior
(kind of like a weak restart). However  what if the favored models in each segment
are from a small subset  i.e. the data is likely to be predicted well by models
that predicted well before? Curiously  ﬁtting such “sparse composite models” is
achieved by mixing in a bit of all the past posteriors. This self-referential updating
method is rather peculiar  but it is efﬁcient and gives superior performance on
many natural data sets. Also it is important because it introduces a long-term
memory: any model that has done well in the past can be recovered quickly. While
Bayesian interpretations can be found for mixing in a bit of the initial prior  no
Bayesian interpretation is known for mixing in past posteriors.
We build atop the “specialist” framework from the online learning literature to give
the Mixing Past Posteriors update a proper Bayesian foundation. We apply our
method to a well-studied multitask learning problem and obtain a new intriguing
efﬁcient update that achieves a signiﬁcantly better bound.

1

Introduction

We consider sequential prediction of outcomes y1  y2  . . . using a set of models m = 1  . . .   M for
this task. In practice m could range over a mix of human experts  parametric models  or even com-
plex machine learning algorithms. In any case we denote the prediction of model m for outcome yt
given past observations y<t = (y1  . . .   yt−1) by P (yt|y<t  m). The goal is to design a computa-
tionally efﬁcient predictor P (yt|y<t) that maximally leverages the predictive power of these models
as measured in log loss. The yardstick in this paper is a notion of regret deﬁned w.r.t. a given com-
parator class of models or composite models: it is the additional loss of the predictor over the best
comparator. For example if the comparator class is the set of base models m = 1  . . .   M  then the
regret for a sequence of T outcomes y≤T = (y1  . . .   yT ) is

T(cid:88)

t=1

− ln P (yt|y<t  m).

T(cid:88)

t=1

R :=

− ln P (yt|y<t) − M
min
m=1

The Bayesian predictor (detailed below) with uniform model prior has regret at most ln M for all T .
Typically the nature of the data is changing with time: in an initial segment one model predicts
well  followed by a second segment in which another model has small loss and so forth. For this
scenario the natural comparator class is the set of partition models which divide the sequence of T
outcomes into B segments and specify the model that predicts in each segment. By running Bayes
on all exponentially many partition models comprising the comparator class  we can guarantee regret

(cid:1)+B ln M  which is optimal. The goal then is to ﬁnd efﬁcient algorithms with approximately

B−1
∗Supported by NWO Rubicon grant 680-50-1010.
†Supported by Veterinary Laboratories Agency of Department for Environment  Food and Rural Affairs.
‡Supported by NSF grant IIS-0917397.

ln(cid:0)T−1

1

the same guarantee as full Bayes. In this case this is achieved by the Fixed Share [HW98] predictor.
It assigns a certain prior to all partition models for which the exponentially many posterior weights
collapse to M posterior weights that can be maintained efﬁciently. Modiﬁcations of this algorithm
achieve essentially the same bound for all T   B and M simultaneously [VW98  KdR08].
In an open problem Yoav Freund [BW02] asked whether there are algorithms that have small regret
against sparse partition models where the base models allocated to the segments are from a small
subset of N of the M models. The Bayes algorithm when run on all such partition models achieves

(cid:1) + B ln N  but contrary to the non-sparse case  emulating this algorithm

regret ln(cid:0)M

(cid:1) + ln(cid:0)T−1

N

B−1

9

3

7 3

T outcomes

7 9 7

is NP-hard. However in a breakthrough paper  Bousquet and Warmuth in 2001 [BW02] gave the
efﬁcient MPP algorithm with only a slightly weaker regret bound. Like Fixed Share  MPP maintains
M “posterior” weights  but it instead mixes in a bit of all past posteriors in each update. This causes
weights of previously good models to “glow” a little bit  even if they perform bad locally. When
the data later favors one of those good models  its weight is pulled up quickly. However the term
“posterior” is a misnomer because no Bayesian interpretation for this curious self-referential update
was known. Understanding the MPP update is a very important problem because in many practical
applications [HLSS00  GWBA02]1 it signiﬁcantly outperforms Fixed Share.
Our main philosophical contribution is ﬁnding a Bayesian interpretation for MPP. We employ the
specialist framework from online learning [FSSW97  CV09  CKZV10]. So-called specialist models
are either awake or asleep. When they are awake  they predict as usual. However when they are
asleep  they “go with the rest”  i.e. they predict with the combined prediction of all awake models.
Instead of fully coordinated partition models  we construct partition
specialists consisting of a base model and a set of segments where
this base model is awake. The ﬁgure to the right shows how a com-
parator partition model is assembled from partition specialists. We
can emulate Bayes on all partition specialists; NP-completeness is
avoided by forgoing a-priori segment synchronization. By care-
fully choosing the prior  the exponentially many posterior weights
collapse to the small number of weights used by the efﬁcient MPP
algorithm. Our analysis technique magically aggregates the contri-
bution of the N partition specialists that constitute the comparator
partition  showing that we achieve regret close to the regret of Bayes
when run on all full partition models. Actually our new insights into
the nature of MPP result in slightly improved regret bounds.
We then apply our methods to an online multitask learning problem where a small subset of models
from a big set solve a large number of tasks. Again simulating Bayes on all sparse assignments
of models to tasks is NP-hard. We split an assignment into subset specialists that assign a single
base model to a subset of tasks. With the right prior  Bayes on these subset specialists again gently
collapses to an efﬁcient algorithm with a regret bound not much larger than Bayes on all assignments.
This considerably improves the previous regret bound of [ABR07]. Our algorithm simply maintains
one weight per model/task pair and does not rely on sampling (often used for multitask learning).
Why is this line of research important? We found a new intuitive Bayesian method to quickly recover
information that was learned before  allowing us to exploit sparse composite models. Moreover 
it expressly avoids computational hardness by splitting composite models into smaller constituent
“specialists” that are asleep in time steps outside their jurisdiction. This method clearly beats Fixed
Share when few base models constitute a partition  i.e. the composite models are sparse.
We expect this methodology to become a main tool for making Bayesian prediction adapt to sparse
models. The goal is to develop general tools for adding this type of adaptivity to existing Bayesian
models without losing efﬁciency. It also lets us look again at the updates used in Nature in a new
light  where species/genes cannot dare adapt too quickly to the current environment and must guard
themselves against an environment that changes or ﬂuctuates at a large scale. Surprisingly these
type of updates might now be amenable to a Bayesian analysis. For example  it might be possible
to interpret sex and the double stranded recessive/dominant gene device employed by Nature as a
Bayesian update of genes that are either awake or asleep.

3

7

7

7

3

9

9

1The experiments reported in [HLSS00] are based on precursors of MPP. However MPP outperforms these

algorithms in later experiments we have done on natural data for the same problem (not shown).

2

(a) A comparator partition model:
segmentation and model assignment

3

. .

zzzz Z Z ZZZ. .
3

(b) Decomposition into 3 partition
specialists  asleep at shaded times

2 Bayes and Specialists

M(cid:88)

m=1

We consider sequential prediction of outcomes y1  y2  . . . from a ﬁnite alphabet. Assume that
we have access to a collection of models m = 1  . . .   M with data likelihoods P (y1  y2  . . .|m).
We then design a prior P (m) with roughly two goals in mind: the Bayes algorithm should “col-
lapse” (become efﬁcient) and have a good regret bound. After observing past outcomes y<t :=
(y1  . . .   yt−1)  the next outcome yt is predicted by the predictive distribution P (yt|y<t)  which
averages the model predictions P (yt|y<t  m) according to the posterior distribution P (m|y<t):

P (yt|y<t) =

P (yt|y<t  m)P (m|y<t)  where P (m|y<t) =

P (y<t|m)P (m)

P (y<t)

.

(cid:125)

(cid:123)(cid:122)

(cid:16) M(cid:88)

m=1

The latter is conveniently updated step-wise: P (m|yt  y<t) = P (yt|y<t  m)P (m|y<t)/P (yt|y<t).
The log loss of the Bayesian predictor on data y≤T := (y1  . . .   yT ) is the cumulative loss of the
predictive distributions and this readily relates to the cumulative loss of any model ˆm:

(cid:17) −(cid:0)−lnP (y≤T| ˆm)(cid:1) ≤ −lnP ( ˆm).

−(cid:0)−lnP (y≤T| ˆm)
(cid:125)
(cid:124)

(cid:123)(cid:122)

(cid:80)T
t=1 −lnP (yt|y<t  ˆm)

(cid:1) = −ln

P (y≤T|m)P (m)

(cid:124)
−lnP (y≤T )
(cid:80)T
t=1 −lnP (yt|y<t)
That is  the additional loss (or regret) of Bayes w.r.t. model ˆm is at most − ln P ( ˆm). The uniform
prior P (m) = 1/M ensures regret at most ln M w.r.t. any model ˆm. This is a so-called individual
sequence result  because no probabilistic assumptions were made on the data.
Our main results will make essential use of the following fancier weighted notion of regret.

Here U (m) is any distribution on the models and (cid:52)(cid:0)U (m)(cid:13)(cid:13)P (m)(cid:1) denotes the relative entropy
(cid:80)M
M(cid:88)
U (m)(cid:0)− ln P (y≤T )−(− ln P (y≤T|m))(cid:1) = (cid:52)(cid:0)U (m)(cid:13)(cid:13)P (m)(cid:1)−(cid:52)(cid:0)U (m)(cid:13)(cid:13)P (m|y≤T )(cid:1). (1)

P (m) between the distributions U (m) and P (m):

m=1 U (m) ln U (m)

m=1

By dropping the subtracted positive term we get an upper bound. The previous regret bound is now
the special case when U is concentrated on model ˆm. However when multiple models are good we
achieve tighter regret bounds by letting U be the uniform distribution on all of them.

Specialists We now consider a complication of the prediction task  which was introduced in the
online learning literature under the name specialists [FSSW97]. The Bayesian algorithm  adapted
to this task  will serve as the foundation of our main results. The idea is that in practice the pre-
dictions P (yt|y<t  m) of some models may be unavailable. Human forecasters may be specialized 
unreachable or too expensive  algorithms may run out of memory or simply take too long. We call
models that may possibly abstain from prediction specialists. The question is how to produce quality
predictions from the predictions that are available.
We will denote by Wt the set of specialists whose predictions are available at time t  and call them
awake and the others asleep. The crucial idea  introduced in [CV09]  is to assign to the sleeping
specialists the prediction P (yt|y<t). But wait! That prediction P (yt|y<t) is deﬁned to average all
model predictions  including those of the sleeping specialists  which we just deﬁned to be P (yt|y<t):

P (yt|y<t) =

P (yt|y<t  m) P (m|y<t) +

P (yt|y<t) P (m|y<t).

(cid:88)

m∈Wt

(cid:88)

m /∈Wt

Although this equation is self-referential  it does have a unique solution  namely

P (yt|y<t) :=

m∈Wt

P (yt|y<t  m)P (m|y<t)
P (Wt|y<t)

.

(cid:80)

Thus the sleeping specialists are assigned the average prediction of the awake ones. This completes
them to full models to which we can apply the unaltered Bayesian method as before. At ﬁrst this
may seem like a kludge  but actually this phenomenon arises naturally wherever concentrations are

3

manipulated. For example  in a democracy abstaining essentially endorses the vote of the participat-
ing voters or in Nature unexpressed genes reproduce at rates determined by the active genes of the
organism. The effect of abstaining on the update of the posterior weights is also intuitive: weights
of asleep specialists are unaffected  whereas weights of awake models are updated with Bayes rule
and then renormalised to the original weight of the awake set:

P (m|y≤t) =

(cid:80)

=

m∈Wt
= P (m|y<t)


P (yt|y<t)P (m|y<t)

P (yt|y<t)

P (yt|y<t)

 P (yt|y<t m)P (m|y<t)
(cid:88)

t≤T : m∈Wt

− ln P (yt|y<t) −(cid:88)

M(cid:88)

m=1

U (m)

To obtain regret bounds in the specialist setting  we use the fact that sleeping specialists m /∈ Wt
are deﬁned to predict P (yt|y<t  m) := P (yt|y<t) like the Bayesian aggregate. Now (1) becomes:
Theorem 1 ([FSSW97  Theorem 1]). Let U (m) be any distribution on a set of specialists with wake
sets W1  W2  . . . Then for any T   Bayes guarantees

 ≤ (cid:52)(cid:0)U (m)(cid:13)(cid:13)P (m)(cid:1).

− ln P (yt|y<t  m)

t≤T : m∈Wt

P (yt|y<t m)P (m|y<t)

P (yt|y<t m)P (m|y<t) P (Wt|y<t)

if m ∈ Wt 
if m /∈ Wt.

(2)

3 Sparse partition learning

We design efﬁcient predictors with small regret compared to the best sparse partition model. We
do this by constructing partition specialists from the input models and obtain a proper Bayesian
predictor by averaging their predictions. We consider two priors. With the ﬁrst prior we obtain the
Mixing Past Posteriors (MPP) algorithm  giving it a Bayesian interpretation and slightly improving
its regret bound. We then develop a new Markov chain prior. Bayes with this prior collapses to an
efﬁcient algorithm for which we prove the best known regret bound compared to sparse partitions.

Construction Each partition specialist (χ  m) is parameterized by a model index m and a circa-
dian (wake/sleep pattern) χ = (χ1  χ2  . . .) with χt ∈ {w  s}. We use inﬁnite circadians in order
to obtain algorithms that do not depend on a time horizon. The wake set Wt includes all partition
specialists that are awake at time t  i.e. Wt := {(χ  m) | χt = w}. An awake specialist (χ  m)
in Wt predicts as the base model m  i.e. P(yt|y<t  (χ  m)) := P (yt|y<t  m). The Bayesian joint
distribution P is completed2 by choosing a prior on partition specialists. In this paper we enforce
the independence P(χ  m) := P(χ)P(m) and deﬁne P(m) := 1/M uniform on the base models.
We now can apply Theorem 1 to bound the regret w.r.t. any partition model with time horizon T by
decomposing it into N partition specialists (χ1≤T   ˆm1)  . . .   (χN≤T   ˆmN ) and choosing U (·) = 1/N
uniform on these specialists:

R ≤ N ln

− lnP(χn≤T ).

(3)

The overhead of selecting N reference models from the pool of size M closely approximates
the information-theoretic ideal N ln M
This improves previous regret bounds
[BW02  ABR07  CBGLS12] by an additive N ln N. Next we consider two choices for P(χ): one
for which we retrieve MPP  and a natural one which leads to efﬁcient algorithms and sharper bounds.

N

3.1 A circadian prior equivalent to Mixing Past Posteriors

yt with Predt(yt) := (cid:80)M

The Mixing Past Posteriors algorithm is parameterized a so-called mixing scheme  which is a se-
quence γ1  γ2  . . . of distributions  each γt with support {0  . . .   t − 1}. MPP predicts outcome
m=1 P (yt|y<t  m) vt(m)  i.e. by averaging the model predictions with

weights vt(m) deﬁned recursively by

vt(m) :=

˜vs+1(m) γt(s) where

˜v1(m) :=

1
M

and

˜vt+1(m) :=

P (yt|y<t  m)vt(m)

Predt(yt)

.

t−1(cid:88)

s=0

2From here on we use the symbol P for the Bayesian joint to avoid a fundamental ambiguity: P(yt|y<t  m)
does not equal the prediction P (yt|y<t  m) of the input model m  since it averages over both asleep and awake
specialists (χ  m). The predictions of base models are now recovered as P(yt|y<t  Wt  m) = P (yt|y<t  m).

4

M
N

N(cid:88)
N ≈ ln(cid:0)M

n=1

+

(cid:1).

The auxiliary distribution ˜vt+1(m) is formally the (incremental) posterior from prior vt(m). The
predictive weights vt(m) are then the pre-speciﬁed γt mixture of all such past posteriors.
To make the Bayesian predictor equal to MPP  we deﬁne from the MPP mixing scheme a circadian
prior measure P(χ) that puts mass only on sequences with a ﬁnite nonzero number of w’s  by

γsj (sj−1) where s≤J are the indices of the w’s in χ and s0 = 0.

(4)

J(cid:89)

P(χ) :=

1

sJ (sJ + 1)

j=1

We built the independence m ⊥ χ into the prior P(χ  m) and (4) ensures χ<t ⊥ χ>t | χt = w for
all t. Since the outcomes y≤t are a stochastic function of m and χ≤t  the Bayesian joint satisﬁes

(5)
Theorem 2. Let Predt(yt) be the prediction of MPP for some mixing scheme γ1  γ2  . . . Let
P(yt|y<t) be the prediction of Bayes with prior (4). Then for all outcomes y≤t

for all t.

y≤t  m ⊥ χ>t | χt = w

Predt(yt) = P(yt|y<t).

q := {χt = χq = w and χr = s for all q < r < t}
Proof. Partition the event Wt = {χt = w} into Z t
for all 0 ≤ q < t  with the convention that χ0 = w. We ﬁrst establish that the Bayesian joint with
prior (4) satisﬁes y≤t ⊥ Wt for all t. Namely  by induction on t  for all q < t

P(y<t|Z t

and therefore P(y≤t|Wt) = P(yt|y<t)(cid:80)t−1

q) = P(y<t|y≤q)P(y≤q|Z t
q)
q=0 P(y<t|Z t

q|Wt) = P(y≤t)  i.e. y≤t ⊥ Wt. The
theorem will be implied by the stronger claim vt(m) = P(m|y<t  Wt)  which we again prove by
induction on t. The case t = 1 is trivial. For t > 1  we expand the right-hand side  apply (5)  use
the independence we just proved  and the fact that asleep specialist predict with the rest:

q)P(Z t

= P(y<t|y≤q)P(y≤q|Wq) Induction= P(y<t) 

(5)

P(m|y<t  Wt) =

=

P(m|y≤q  Wq)

P(Z t

q|m  y≤q  Wq)P(Wq|HHy≤q)

P(Wt|HHy<t)

P(y<t|Z t


P(y<t|y≤q)

q  m  y≤q)

P (yq|y<q  m)P(m|y<q  Wq)

P(yq|y<q)

P(Z t

q|Wt)

t−1(cid:88)
t−1(cid:88)

q=0

q=0

By (4) P(Z t

q|Wt) = γt(q)  and the proof is completed by applying the induction hypothesis.

The proof of the theorem provides a Bayesian interpretation of all the MPP weights: vt(m) =
P(m|y<t  Wt) is the predictive distribution  ˜vt+1(m) = P(m|y≤t  Wt) is the posterior  and γt(q) =
P(Z t

q|Wt) is the conditional probability of the previous awake time.

3.2 A simple Markov chain circadian prior

In the previous section we recovered circadian priors corresponding to the MPP mixing schemes.
Here we design priors afresh from ﬁrst principles. Our goal is efﬁciency and good regret bounds. A
simple and intuitive choice for prior P(χ) is a Markov chain on states {w  s} with initial distribution
θ(·) and transition probabilities θ(·|w) and θ(·|s)  that is

P(χ≤t) := θ(χ1)

θ(χs|χs−1).

(6)

s=2

By choosing low transition probabilities we obtain a prior that favors temporal locality in that it
allocates high probability to circadians that are awake and asleep in contiguous segments. Thus if a
good sparse partition model exists for the data  our algorithm will pick up on this and predict well.
The resulting Bayesian strategy (aggregating inﬁnitely many specialists) can be executed efﬁciently.
Theorem 3. The prediction P(yt|y<t) of Bayes with Markov prior (6) equals the prediction
Predt(yt) of Algorithm 1  which can be computed in O(M ) time per outcome using O(M ) space.

5

t(cid:89)

Proof. We prove by induction on t that vt(b  m) = P(χt = b  m|y<t) for each model m and
b ∈ {w  s}. The base case t = 1 is automatic. For the induction step we expand
P(χt+1 = b  m|y≤t)
= θ(b|w)P(χt = w  m|y≤t) + θ(b|s)P(χt = s  m|y≤t)
= θ(b|w)

(cid:80)M
P(χt = w  m|y<t)P (yt|y<t  m)
i=1 P(i|χt = w  y<t)P (yt|y<t  i)

+ θ(b|s)P(χt = s  m|y<t).

(6)

(2)

By applying the induction hypothesis we obtain the update rule for vt+1(b  m).

Algorithm 1 Bayes with Markov circadian prior (6) (for Freund’s problem)

Input: Distributions θ(·)  θ(·|w) and θ(·|s) on {w  s}.
Initialize v1(b  m) := θ(b)/M for each model m and b ∈ {w  s}
for t = 1  2  . . . do

Receive prediction P (yt|y<t  m) of each model m

Predict with Predt(yt) := (cid:80)M

Observe outcome yt and suffer loss − ln Predt(yt).
Update vt+1(b  m) := θ(b|w) P (yt|y<t m)

m=1 P (yt|y<t  m)vt(m|w) where vt(m|w) =
Predt(yt) vt(w  m) + θ(b|s)vt(s  m).

end for

(cid:80)M
m(cid:48)=1 vt(w m(cid:48))

vt(w m)

B−1

(cid:19)

(cid:18) 1

The previous theorem establishes that we can predict fast. Next we show that we predict well.
Theorem 4. Let ˆm1  . . .   ˆmT be an N-sparse assignment of M models to T times with B seg-
ments. The regret of Bayes (Algorithm 1) with tuning θ(w) = 1/N  θ(s|w) = B−1
T−1 and
θ(w|s) =
+ (N − 1)(T − 1)H
R ≤ N ln
where H(p) := −p ln(p) − (1 − p) ln(1 − p) is the binary entropy function.
Proof. Without generality assume ˆmt ∈ {1  . . .   N}. For each reference model n pick circadian
χn≤T with χn

t = w iff ˆmt = n. Expanding the deﬁnition of the prior (6) we ﬁnd

(N−1)(T−1) is at most
M
N

(cid:18) B − 1

(N − 1)(T − 1)

+ (T − 1)H

+ N H

B − 1

T − 1

(cid:19)

(cid:18)

(cid:19)

N

 

P(χn≤T ) = θ(w)θ(s)N−1θ(s|s)(N−1)(T−1)−(B−1)θ(w|w)T−Bθ(w|s)B−1θ(s|w)B−1 

N(cid:89)
The information-theoretic ideal regret is ln(cid:0)M

n=1

which is in fact maximized by the proposed tuning. The theorem follows from (3).

(cid:1) + ln(cid:0)T−1

(cid:1) + B ln N. Theorem 4 is very close to

this except for a factor of 2 in front of the middle term; since nH(k/n) ≤ k ln(n/k) + k we have

N

R ≤ N ln

M
N

+ 2

(B − 1) ln

+ B ln N + 2B.

B−1
T − 1
B − 1

The origin of this factor remained a mystery in [BW02]  but becomes clear in our analysis: it is the
price of coordination between the specialists that constitute the best partition model. To see this  let
us regard a circadian as a sequence of wake/sleep transition times. With this viewpoint (3) bounds
the regret by summing the prior costs of all the reference wake/sleep transition times. This means
that we incur overhead at each segment boundary of the comparator twice: once as the sleep time of
the preceding model  and once more as the wake time of the subsequent model.
In practice the comparator parameters T   N and B are unknown. This can be addressed by standard
orthogonal techniques. Of particular interest is the method inspired by [SM99  KdR08  Koo11] of
changing the Markov transition probabilities as a function of time. It can be shown that by setting
θ(w) = 1/2 and increasing θ(w|w) and θ(s|s) as exp(−
t ln2(t+1) ) we keep the update time and
space of the algorithm at O(M ) and guarantee regret bounded for all T   N and B as
+ 2N + 2(B − 1) ln T + 4(B − 1) ln ln(T + 1).

R ≤ N ln

1

At no computational overhead  this bound is remarkably close to the fully tuned bound of the theo-
rem above  especially when the number of segments B is modest as a function of T .

M
N

6

4 Sparse multitask learning

We transition to an extension of the sequential prediction setup called online multitask learning
[ABR07  RAB07  ARB08  LPS09  CCBG10  SRDV11]. The new ingredient is that before predict-
ing outcome yt we are given its task number κt ∈ {1  . . .   K}. The goal is to exploit similarities
between tasks. As before  we have access to M models that each issue a prediction each round. If a
single model predicts well on several tasks we want to ﬁgure this out quickly and exploit it. Simply
ignoring the task number would not result in an adaptive algorithm. Applying a separate Bayesian
predictor to each task independently would not result in any inter-task synergy. Nevertheless  it
would guarantee regret at most K ln M overall. Now suppose each task is predicted well by some
model from a small subset of models of size N (cid:28) M. Running Bayes on all N-sparse allocations

(cid:1) + K ln N. However  emulating Bayes in this case is NP-hard [RAB07].

would achieve regret ln(cid:0)M

The goal is to design efﬁcient algorithms with approximately the same regret bound.
In [ABR07] this multiclass problem is reduced to MPP  giving regret bound N ln M
N + B ln N. Here
B is the number of same-task segments in the task sequence κ≤T . When all outcomes with the same
task number are consecutive  i.e. B = K  then the desired bound is achieved. However the tasks
may be interleaved  making the number of segments B much larger than K. We now eliminate the
dependence on B  i.e. we solve a key open problem of [ABR07].
We apply the method of specialists to multitask learning  and obtain regret bounds close to the
information-theoretic ideal  which in particular do not depend on the task segment count B at all.

N

P(S) := σ(w)|S|σ(s)K−|S|.

Construction We create a subset specialist (S  m) for each basic model index m and subset of
tasks S ⊆ {1  . . .   K}. At time t  specialists with the current task κt in their set S are awake 
i.e. Wt := {(S  m) | κt ∈ S}  and issue the prediction P(yt|y<t  S  m) := P (yt|y<t  m) of
model m. We assign to subset specialist (S  m) prior probability P(S  m) := P(S)P(m) where
P(m) := 1/M is uniform  and P(S) includes each task independently with some ﬁxed bias σ(w)
(7)
This construction has the property that the product of prior weights of two loners ({κ1}  ˆm) and
({κ2}  ˆm) is dramatically lower than the single pair specialist ({κ1  κ2}  ˆm)  especially so when the
number of models M is large or when we consider larger task clusters. By strongly favoring it in
the prior  any inter-task similarity present will be picked up fast.
The resulting Bayesian strategy involving M 2K subset specialists can be implemented efﬁciently.
Theorem 5. The predictions P(yt|y<t) of Bayes with the set prior (7) equal the predictions
Predt(yt) of Algorithm 2. They can be computed in O(M ) time per outcome using O(KM ) storage.
t+1(m). This would be a regular Bayesian
Of particular interest is Algorithm 2’s update rule for f κ
posterior calculation if vt(m) in Predt(yt) were replaced by f κ
t (m). In fact  vt(m) is the commu-
nication channel by which knowledge about the performance of model m in other tasks is received.

Proof. The resource analysis follows from inspection  noting that the update is fast because only
the weights f κ
t (m) associated to the current task κ are changed. We prove by induction on t that
P(m|y<t  Wt) = vt(m). In the base case t = 1 both equal 1/M. For the induction step we expand
P(m|y≤t  Wt+1)  which is by deﬁnition proportional to

σ(w)|S|σ(s)K−|S|

1
M

P (yq|y<q  m)

P(yq|y<q)

(8)

The product form of both set prior and likelihood allows us to factor this exponential sum of products
into a product of binary sums. It follows from the induction hypothesis that

(cid:88)

S(cid:51)κt+1

 (cid:89)

q≤t : κq∈S

 (cid:89)

q≤t : κq /∈S

 .

f k
t (m) =

σ(w)
σ(s)

P (yq|y<q  m)
P(yq|y<q)

Then we can divide (8) by P(y≤t)σ(s)K and reorganize to
P(m|y≤t  Wt+1) ∝ 1
M

(cid:89)

f κt+1
t

(m)

k(cid:54)=κt+1

1
M

f κt+1
t
f κt+1
t

(m)

(m) + 1

(cid:0)f k
t (m) + 1(cid:1)

K(cid:89)

k=1

(cid:89)
(cid:0)f k
t (m) + 1(cid:1) =

q≤t : κq=k

7

Since the algorithm maintains πt(m) =(cid:81)K

k=1(f k

t (m) + 1) this is proportional to vt+1(m).

Algorithm 2 Bayes with set prior (7) (for online multitask learning)

Input: Number of tasks K ≥ 2  distribution σ(·) on {w  s}.
Initialize f k
for t = 1  2  . . . do

σ(s) for each task k and π1(m) :=(cid:81)K

1 (m) := σ(w)

k=1(f k

1 (m) + 1).

t (i) πt(i)/(f κ

t (m) πt(m)/(f κ
i=1 f κ

(cid:80)M
Observe task index κ = κt.
Compute auxiliary vt(m) := f κ
Issue prediction Predt(yt) :=(cid:80)M
Receive prediction P (yt|y<t  m) of each model m
Observe outcome yt and suffer loss − ln Predt(yt).
Update f κ
t (m) and keep f k
Update πt+1(m) :=

t+1(m) := P (yt|y<t m)

Predt(yt) f κ
f κ
t+1(m)+1
t (m)+1 πt(m).
f κ

t (m)+1)

t (i)+1) .

m=1 P (yt|y<t  m)vt(m).

end for

t+1(m) := f k

t (m) for all k (cid:54)= κ.

The Bayesian strategy is hence emulated fast by Algorithm 2. We now show it predicts well.
Theorem 6. Let ˆm1  . . .   ˆmK be an N-sparse allocation of M models to K tasks. With tuned
inclusion rate σ(w) = 1/N  the regret of Bayes (Algorithm 2) is bounded by

R ≤ N ln ( M /N ) + KN H(1/N ).

ˆmk = n}. The sets Sn for n = 1  . . .   N form a partition of the K tasks. By (7)(cid:81)N

Proof. Without loss of generality assume that ˆmk ∈ {1  . . .   N}. Let Sn := {1 ≤ k ≤ K |
n=1 P(Sn) =
σ(w)Kσ(s)(N−1)K  which is maximized by the proposed tuning. The theorem now follows from
(3).
We achieve the desired goal since KN H(1/N ) ≈ K ln N. In practice N is of course unavailable
for tuning  and we may tune σ(w) = 1/K pessimistically to get K ln K + N instead for all N
simultaneously. Or alternatively  we may sacriﬁce some time efﬁciency to externally mix over all
M possible values with decreasing prior  increasing the tuned regret by just ln N + O(ln ln N ).
If in addition the number of tasks is unknown or unbounded  we may (as done in Section 3.2)
decrease the membership rate σ(w) with each new task encountered and guarantee regret R ≤
N ln(M/N ) + K ln K + 4N + 2K ln ln K where now K is the number of tasks actually received.

5 Discussion

We showed that Mixing Past Posteriors is not just a heuristic with an unusual regret bound: we
gave it a full Bayesian interpretation using specialist models. We then applied our method to a
multitask problem. Again an unusual algorithm resulted that exploits sparsity by pulling up the
weights of models that have done well before in other tasks. In other words  if all tasks are well
predicted by a small subset of base models  then this algorithm improves its prior over models as it
learns from previous tasks. Both algorithms closely circumvent NP-hardness. The deep question is
whether some of the common updates used in Nature can be brought into the Bayesian fold using
the specialist mechanism.
There are a large number of more immediate technical open problems (we just discuss a few). We
presented our results using probabilities and log loss. However the bounds should easily carry over to
the typical pseudo-likelihoods employed in online learning in connection with other loss functions.
Next  it would be worthwhile to investigate for which inﬁnite sets of models we can still employ our
updates implicitly. It was already shown in [KvE10  Koo11] that MPP can be efﬁciently emulated
on all Bernoulli models. However  what about Gaussians  exponential families in general  or even
linear regression? Finally  is there a Bayesian method for modeling concurrent multitasking  i.e. can
the Bayesian analysis be generalized to the case where a small subset of models solve many tasks in
parallel?

8

References
[ABR07]

[ARB08]

[BW02]

Jacob Ducan Abernethy  Peter Bartlett  and Alexander Rakhlin. Multitask learning
with expert advice. Technical report  University of California at Berkeley  January
2007.
Alekh Agarwal  Alexander Rakhlin  and Peter Bartlett. Matrix regularization tech-
niques for online multitask learning  October 2008.
Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing
past posteriors. Journal of Machine Learning Research  3:363–396  2002.

[CBGLS12] Nicol`o Cesa-Bianchi  Pierre Gaillard  G´abor Lugosi  and Gilles Stoltz. A new look at

shifting regret. CoRR  abs/1202.3323  2012.

[CCBG10] Giovanni Cavallanti  Nicol`o Cesa-Bianchi  and Claudio Gentile. Linear algorithms for
online multitask classiﬁcation. J. Mach. Learn. Res.  11:2901–2934  December 2010.
[CKZV10] Alexey Chernov  Yuri Kalnishkan  Fedor Zhdanov  and Vladimir Vovk. Supermartin-
gales in prediction with expert advice. Theor. Comput. Sci.  411(29-30):2647–2669 
June 2010.
Alexey Chernov and Vladimir Vovk. Prediction with expert evaluators’ advice. In Pro-
ceedings of the 20th international conference on Algorithmic learning theory  ALT’09 
pages 8–22  Berlin  Heidelberg  2009. Springer-Verlag.

[CV09]

[FSSW97] Y. Freund  R. E. Schapire  Y. Singer  and M. K. Warmuth. Using and combining predic-
tors that specialize. In Proc. 29th Annual ACM Symposium on Theory of Computing 
pages 334–343. ACM  1997.

[GWBA02] Robert B. Gramacy  Manfred K. Warmuth  Scott A. Brandt  and Ismail Ari. Adaptive
caching by refetching. In Suzanna Becker  Sebastian Thrun  and Klaus Obermayer 
editors  NIPS  pages 1465–1472. MIT Press  2002.

[HLSS00] David P. Helmbold  Darrell D. E. Long  Tracey L. Sconyers  and Bruce Sherrod. Adap-
tive disk spin-down for mobile computers. ACM/Baltzer Mobile Networks and Appli-
cations (MONET)  pages 285–297  2000.
Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning 
32:151–178  1998.

[HW98]

[Koo11]

[KvE10]

[LPS09]

[SM99]

[KdR08] Wouter M. Koolen and Steven de Rooij. Combining expert advice efﬁciently.

In
Rocco Servedio and Tong Zang  editors  Proceedings of the 21st Annual Conference
on Learning Theory (COLT 2008)  pages 275–286  June 2008.
Wouter M. Koolen. Combining Strategies Efﬁciently: High-quality Decisions from
Conﬂicting Advice. PhD thesis  Institute of Logic  Language and Computation (ILLC) 
University of Amsterdam  January 2011.
Wouter M. Koolen and Tim van Erven. Freezing and sleeping: Tracking experts that
learn by evolving past posteriors. CoRR  abs/1008.4654  2010.
G´abor Lugosi  Omiros Papaspiliopoulos  and Gilles Stoltz. Online multi-task learning
with hard constraints. In COLT  2009.
Alexander Rakhlin  Jacob Abernethy  and Peter L. Bartlett. Online discovery of sim-
In Proceedings of the 24th international conference on Machine
ilarity mappings.
learning  ICML ’07  pages 767–774  New York  NY  USA  2007. ACM.
Gil I. Shamir and Neri Merhav. Low complexity sequential lossless coding for piece-
wise stationary memoryless sources. IEEE Trans. Info. Theory  45:1498–1519  1999.
[SRDV11] Avishek Saha  Piyush Rai  Hal Daum´e III  and Suresh Venkatasubramanian. Online
learning of multiple tasks and their relationships. In AISTATS  Ft. Lauderdale  Florida 
2011.
Paul A.J. Volf and Frans M.J. Willems. Switching between two universal source coding
In Proceedings of the Data Compression Conference  Snowbird  Utah 
algorithms.
pages 491–500  1998.

[RAB07]

[VW98]

9

,Xiaosong Zhang
Fang Wan
Chang Liu
Rongrong Ji
Qixiang Ye