2019,Optimal Sampling and Clustering in the Stochastic Block Model,This paper investigates the design of joint adaptive sampling and clustering algorithms in networks whose structure follows the celebrated Stochastic Block Model (SBM). To extract hidden clusters  the interaction between edges (pairs of nodes) may be sampled sequentially  in an adaptive manner. After gathering samples  the learner returns cluster estimates. We derive information-theoretical upper bounds on the cluster recovery rate. These bounds actually reveal the optimal sequential edge sampling strategy  and interestingly  the latter does not depend on the sampling budget  but on the parameters of the SBM only. We devise a joint sampling and clustering algorithm matching the recovery rate upper bounds. The algorithm initially uses a fraction of the sampling budget to estimate the SBM parameters  and to learn the optimal sampling strategy. This strategy then guides the remaining sampling process  which confers the optimality of the algorithm. We show both analytically and numerically that adaptive edge sampling yields important improvements over random sampling (traditionally used in the SBM analysis). For example  we prove that adaptive sampling significantly enlarges the region of the SBM parameters where asymptotically exact cluster recovery is feasible.,Optimal Sampling and Clustering

in the Stochastic Block Model

Se-Young Yun

KAIST

Daejeon  South Korea

yunseyoung@kaist.ac.kr

Alexandre Proutière

KTH

Stockholm  Sweden
alepro@kth.se

Abstract

This paper investigates the design of joint adaptive sampling and clustering al-
gorithms in networks whose structure follows the celebrated Stochastic Block
Model (SBM). To extract hidden clusters  the interaction between edges (pairs
of nodes) may be sampled sequentially  in an adaptive manner. After gathering
samples  the learner returns cluster estimates. We derive information-theoretical
upper bounds on the cluster recovery rate. These bounds actually reveal the optimal
sequential edge sampling strategy  and interestingly  the latter does not depend on
the sampling budget  but on the parameters of the SBM only. We devise a joint
sampling and clustering algorithm matching the recovery rate upper bounds. The
algorithm initially uses a fraction of the sampling budget to estimate the SBM
parameters  and to learn the optimal sampling strategy. This strategy then guides
the remaining sampling process  which confers the optimality of the algorithm.
We show both analytically and numerically that adaptive edge sampling yields
important improvements over random sampling (traditionally used in the SBM
analysis). For example  we prove that adaptive sampling signiﬁcantly enlarges
the region of the SBM parameters where asymptotically exact cluster recovery is
feasible.

1

Introduction

Extracting clusters in networks is a central task in many ﬁelds including biology  computer science 
and social science. The Stochastic Block Model (SBM) [9] and its extensions provide a natural
statistical benchmark to assess the performance of network clustering algorithms. The SBM deﬁnes a
random graph with n nodes and consisting of K non-overlapping clusters  V1  . . .  VK  of respective
sizes α1n  . . .   αKn with αk > 0 for all k. An edge between two nodes from respective clusters
Vi and Vj indicates whether these nodes interact and appears in the graph with probability pij 
independently of other edges. The SBM is hence parametrized by p = [pij]1≤i j≤K and α =
(α1  . . .   αK). We assume that the relative cluster sizes α do not depend on the network size n 
whereas on the contrary  p may vary with n. Most existing work on the SBM and its extensions
investigate the problem of recovering the clusters from an observed realization of the random graph.
In contrast  in this paper  we are interested in active learning scenarios where the interaction between
pairs of nodes may be sampled sequentially  which allows a given node pair to be sampled several
times. In these scenarios  the algorithm samples edges in an adaptive manner: In a given round  the
edge selected to be sampled may depend on the information gathered previously  and should the
algorithm select the edge (v  w) ∈ Vi × Vj  it observes a Bernoulli r.v. with mean pij  independent
of the previous observations. The algorithm has an observation budget of T samples (typically
depending on the network size)  and after collecting these samples  it should return estimates of the
clusters. The objective is to devise a joint sampling and clustering algorithm such that the estimated
clusters are as accurate as possible. Speciﬁcally  we aim at characterizing the minimal cluster recovery

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

error rate for a given observation budget T . Adaptive sampling can be critical in clustering tasks
where collecting edge samples is expensive (e.g.  in biology  one has to run tedious experiments to
assess whether two proteins share similarities). For such tasks  it is important to discover clusters
with a minimum number of samples (these in turn need to be selected in an adaptive manner).
Even for the simple binary symmetric SBM (i.e.  two clusters of equal sizes) with non-adaptive
sampling  obtaining an explicit expression for the minimal number of mis-classiﬁed nodes remains
illusory  especially when the graph is sparse  i.e.  when the pij’s scale as 1/n (see e.g. [6  14]). Hence 
in this paper  we restrict our attention to models where only a vanishing fraction of nodes is allowed to
be mis-classiﬁed. More precisely  for any s = o(n)  we aim at identifying a necessary and sufﬁcient
condition on n  T   p and α such that there exists a joint adaptive sampling and clustering algorithm
mis-classifying less than s nodes with high probability. This objective is more ambitious than just
deriving conditions for weak consistency (also referred to as asymptotically accurate detection)
[15  12  1]  that is to say  conditions under which the proportion of mis-classiﬁed nodes vanishes as
n grows large. Indeed  we are interested in the minimal recovery error rate. Further observe that
deriving conditions for asymptotically exact recovery is part of our objective (these conditions are
obtained selecting s = 1).

Main results. We establish that under mild assumptions  for any s = o(n)  a necessary and sufﬁcient
condition for the existence of a joint adaptive sampling and clustering algorithm mis-classifying less
than s nodes w.h.p.1 is

lim inf
n→∞

2T D(p  α)
n log(n/s)

≥ 1 

(1)

where the divergence D(p  α) is deﬁned as: D(p  α) = maxx∈X (α) ∆(x  p) 

K(cid:88)

with ∆(x  p) = min
i j:i(cid:54)=j

X (α) =

xikKL(pik  pjk) and

k=1

x = [xij] : αixij = αjxji 

i=1

j=1

K(cid:88)

K(cid:88)

xij = 1  and xij ≥ 0  ∀i  j

αi

  

and where KL(a  b) denotes the KL divergence between two Bernoulli distributions with respective
means a and b. A consequence of this result is that when T = ω(n)  the best possible joint sampling
and clustering mis-classiﬁes n exp(− 2T

n D(p  α)(1 + o(1)) nodes.

Gains through adaptive sampling: exact recovery conditions and numerical experiments. To
illustrate the gain obtained by adaptive sampling  we can compare the conditions for asymptotically
exact recovery with or without adaptive sampling. Consider the following binary symmetric SBM:
K = 2  α = (1/2  1/2)  p11 = p22 = af (n)
n . For the classical cluster
recovery problem in SBMs without adaptive sampling  one observes a realization of the random
graph and hence one has T = n(n − 1)/2 observations (one per pair of nodes) to estimate clusters.
For the above binary symmetric SBM  asymptotically exact recovery [3] is possible if and only if
either f (n) = ω(log(n)) or f (n) = log(n) and

n   and p12 = p21 = bf (n)

max{√

a −

√

√

b − √

b 

a} >

√

2

(Non-adaptive sampling)

Now with adaptive sampling and the same observation budget T = n(n − 1)/2  our results show that
asymptotically exact recovery is feasible if and only if either f (n) = ω(log(n)) or f (n) = log(n)
and

max{a log(

a
b

) + b − a  b log(

) + a − b} >

b
a

1
2

(Adaptive sampling)

Figure 1 (left) presents the regions (described through a and b) where asymptotically exact recovery
is feasible with or without adaptive sampling. Observe that adaptive sampling signiﬁcantly enlarges
the region where exact recovery is possible.

1w.h.p. means that the probability tends to 1 as n grows large.

2

Figure 1: (Left) Regions where asymptotically exact recovery is possible for the binary symmetric
SBM. The intra- and inter-cluster edge probabilities are a log(n)/n and b log(n)/n. Dark green:
region with non-adaptive sampling  dark green+light green: region with adaptive sampling. (Right)
Recovery error rate of ASP (with adaptive sampling) in blue and of the optimal clustering algorithm
with random sampling [16] in red for the binary symmetric SBM with n = 20000 nodes and
p11 = 0.5 and p12 = 0.1 – Figure done using matlab boxplot function with outliers (the red crosses).

To further illustrate the gain achieved with adaptive sampling  we compare Figure 1 (right) the
cluster recovery rate using ASP (Adaptive Sampling Partition)  the proposed optimal joint sampling
and clustering algorithm and the optimal clustering algorithm with random sampling presented in
[16]. This experiment concerns the above binary symmetric SBM with 20000 nodes and parameters
p11 = p22 = 0.5  and p12 = p21 = 0.1. As soon as the sampling budget exceeds 450000  ASP
exactly recovers the clusters  whereas a much larger budget is needed to achieve exact recovery if
edges are sampled randomly.

Deriving fundamental limits. Existing information-theoretical limits derived in the classical SBM
without adaptive sampling concern the expected number of mis-classiﬁed nodes [16  7]. Here  we are
interested in establishing lower bounds on the number of mis-classiﬁed nodes that hold with high
probability  which is more challenging than establishing similar bounds in expectation. To derive the
necessary condition (1)  we leverage and combine change-of-measure arguments as those used in
online stochastic optimization [10]  as well as tools from hypothesis testing. In particular  we need to
consider and enumerate a large number of hypotheses pertaining to the way nodes are allocated to the
various clusters (these hypotheses concern allocations that differ from more than s nodes). Such an
enumeration is not required to derive lower bounds on the expected number of mis-classiﬁed nodes
[16]. There  simple symmetry arguments can be exploited instead.

An explicit optimal sampling strategy. As in some other sequential decision making problems (e.g.
bandit problems)  the fundamental limits do not only provide the performance of the best possible
algorithm  but also provide insights into the design of such an algorithm. To devise a joint sampling
and clustering strategy whose performance matches our fundamental limits  we ﬁrst exploit the
following interpretation of the divergence D(p  α) = maxx∈X (α) ∆(x  p) involved in our necessary
condition. There  the vector x encodes the average number of samples of edges between the various
clusters. More precisely  2xijT /n is the average number of samples of edges between a given node
in cluster Vi and nodes in cluster Vj. With this interpretation  an optimal sampling strategy consists
in allocating the observation budget T according to x∗(p  α) = arg maxx∈X (α) ∆(x  p). Note that
interestingly  x∗(p  α) does not depend on the total observation budget. However  the optimal budget
allocation depends on the initially unknown SBM parameters (p  α). To devise an optimal joint
sampling and clustering algorithm  we start  using a fraction of the observation budget  by estimating
x∗(p  α). More precisely  the proposed algorithm consists in three main steps: (i) ﬁrst  we use a
small fraction of the observation budget and spectral methods to obtain initial cluster estimates; (ii)
the latter are then used to derive precise estimators of the SBM parameters  which in turn yield an
estimate ˆx∗ of x∗(p  α); (iii) ﬁnally  ˆx∗ and our initial cluster estimates dictate the way to sample
edges with the remaining budget  and based on these observations  the cluster estimates are improved.

3

400000450000500000550000600000650000T10-410-310-2Error rate2 Related Work

Clustering in the SBM and its extensions have received a lot of attention recently. Almost all studies
concern the problem of recovering the clusters from a realization of the random graph generated
under the SBM (one sample for each edge is observed). Nevertheless  it is interesting to summarize
the results obtained in this simple non-adaptive setting. Results may be categorized depending the
targeted level of performance.
The lowest level of performance is often referred to as detectability and requires that the extracted
clusters should be only positively correlated with the true clusters. In fact  the question of detectability
is mostly relevant in the case of the sparse SBM  where the intra- and inter-cluster edge probabilities
n. In the case of the binary symmetric SBM  it has been
p and q scale as 1/n  say p = a

established that a necessary and sufﬁcient condition for detectability was a − b > (cid:112)2(a + b)

n  q = b

[5  13  11]. Refer to [1] for more recent results.
Asymptotically accurate recovery or weak consistency refers to scenarios where the proportion of
mis-classiﬁed nodes vanishes as n grows large. Necessary and sufﬁcient conditions for such recovery
have been derived in [15  12  4]. Recently  the authors of [17  7  16] manage to quantify the optimal
recovery rate when asymptotically accurate recovery is possible. Unfortunately  in these papers  the
authors establish a lower bound for the expected number of mis-classiﬁed nodes and provide an
algorithm with guarantees valid with high probability. In this paper  we ﬁx this gap  and develop new
techniques to derive lower bounds valid with high probability.
The highest level of performance  asymptotically exact recovery  means that there is no mis-classiﬁed
node asymptotically with high probability. Necessary and sufﬁcient for exact recovery are provided
in [2  12  8  16].
In this paper  we cover both asymptotically accurate detection  and exact recovery. But unlike the
aforementioned papers  we investigate the design of joint adaptive sampling and clustering algorithms.
As far as we are aware  the only relevant reference for adaptive sampling in the SBM is [15]  and
only provides a condition for asymptotically accurate detection in homogeneous SBMs where the
intra- and inter-cluster edge probabilities do not depend on the clusters (i.e.  pii = p and pij = q for
all i (cid:54)= j). We manage to derive matching lower and upper bounds valid with high probability on the
recovery rate for general SBMs. Our algorithm is very different than that developed in [15] since it is
based on the explicit optimal sampling strategy revealed by our lower bounds on the cluster recovery
rate (lower bounds that are lacking in [15]).

3 Fundamental Limits

This section is devoted to state and prove a necessary condition for the existence of a joint sampling
and clustering algorithm mis-classifying less than s = o(n) nodes with high probability. The
derivation of the necessary condition combines hypothesis testing techniques and change-of-measure
arguments where we pretend that the observations are generated by models obtained by slightly
modifying the true SBM model. More precisely  modiﬁed models are built by moving nodes from one
cluster to another. Since the clusters have different sizes  the number of nodes moved from cluster
Vi to cluster Vj should depend on i and j. As a consequence  the different resulting models should
have different distribution vectors α. To deal with this asymmetry  we hence introduce the class of
(s  β)-locally stable algorithms deﬁned as follows.
Deﬁnition 1 ((s  β)-locally stable algorithms). A joint sampling and clustering algorithm π is (s  β)-
locally stable at (p  α)  if there exists a sequence ηn ≥ 0 with limn→∞ ηn = 0 such that for all
partition vectors ˜α such that (cid:107) ˜α− α(cid:107)2 ≤ β  π mis-classiﬁes at most s nodes with probability greater
than 1 − ηn for any n. (Note that the deﬁnition makes sense even if p depends on n.)
We derive our necessary condition for (s  β)-locally stable algorithms. Considering (s  β)-locally
stable algorithms is not restrictive  as good algorithms should adapt to the SBM and in particular  to
various possible proportions of nodes in the different clusters. Furthermore  the theorem below is
valid for all β ≥ s
Theorem 1. Let s = o(n). Assume that there exists a (s  β)-locally stable clustering algorithm at
(p  α) for β ≥ s

(cid:1)  and hence β can be made as small as we want when n grows large.
(cid:1). Then we have: lim inf n→∞ 2T D(p α)

n log(cid:0) n
n log(cid:0) n

n log(n/s) ≥ 1.

s

s

4

To establish Theorem 1  we consider a (s  β)-locally stable algorithm  and assume that the corre-
sponding budget allocation is deﬁned by x (representing the expected number of samples gathered
from a node to all clusters). We then exhibit a large number M of hypotheses  each deﬁned by
an allocation of nodes to clusters. These hypotheses correspond to allocations differing from each
other by more than s nodes. We enumerate the hypotheses and quantify M as a function of n and
the SBM parameters. Using the fact that the algorithm is (s  β)-locally stable  we can ﬁnd a worst
hypothesis (not corresponding to the true allocation of nodes) occurring with probability less than
ηn/M. Next  we apply a change-of-measure argument. Speciﬁcally  we pretend that the observa-
tions are generated by a (perturbed) allocation built from that corresponding the worst hypothesis.
We study the log-likelihood ratio of the observations under the true and the perturbed allocations.
Combining the analysis with the fact that the worst hypothesis occurs with probability less than
ηn/M  we conclude that the number of nodes from Vj actually classiﬁed in Vi must roughly exceed
αjn exp(− 2T
n ∆(x  p)) nodes are
mis-classiﬁed. Now optimizing this lower bound over x  we deduce that at least n exp(− 2T
n D(p  α))
nodes are mis-classiﬁed. The complete proof is presented in Appendix. There  we also provide the
proof for the binary symmetric SBM (this proof helps the understanding of that for general SBMs).

(cid:80)K
k=1 xikKL(pik  pjk)). This implies that at least n exp(− 2T

n

4 The Adaptive Spectral Partition Algorithm

In this section  we present the Adaptive Spectral Partition (ASP) algorithm  whose pseudo-code is
given in Algorithm 1  and prove that it mis-classiﬁes less than s = o(n) w.h.p. whenever this is at all
possible  i.e.  when (1) holds.

4.1 Algorithm and its optimality

The design of the ASP algorithm leverages the results derived to establish fundamental perfor-
mance limits. In particular  we know that an optimal sampling strategy corresponds to x∗(p  α) =
arg maxx∈X (α) ∆(x  p). ASP consists in three main steps: (i) ﬁrst  we use a small fraction of the
observation budget and apply spectral methods to obtain initial cluster estimates (Line 1 in Algorithm
1); (ii) the latter are then used to derive precise estimators of the SBM parameters  which in turn
yield an estimate ˆx∗ of x∗(p  α) (Lines 2 and 3); (iii) ﬁnally  ˆx∗ dictates the way to sample edges
with the remaining budget  and based on these additional observations  the cluster estimates are
improved (Lines 4 and 5). The complexity of the ASP is polynomial to both n and T . Indeed  Step 1 
including the Spectral Clustering Algorithm  requires O(T log(n)) operations. Step 2 requires O(T )
operations to estimate parameters and Step 3 solves a linear program where the number of variables
is k2 which does not scale with n and T . The remaining steps simply check the log-likelihood values
of each node  which requires O(T ) computations. Overall  the computational complexity of ASP is
O(T log n).
We analyze the performance of ASP under the following mild assumptions  essentially stating some
kind of homogeneity of the SBM parameters associated to the various clusters. There exist positive
constants κL and κU such that

(cid:12)(cid:12)(cid:12)(cid:12)log

(A1)

(A2) κL ≤

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) ≤ κU

(cid:18) pik(1 − pjk)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:18) pik

pjk(1 − pik)

pjk

for all

i  j  k

for all

i  j  k.

We emphasize that no other assumptions are made on p.
Theorem 2. Assume that (A1) and (A2) hold. Let s = o(n). The ASP algorithm mis-classiﬁes less
than s nodes with high probability  if lim inf n→∞ 2T D(p α)

n log(n/s) ≥ 1.

The above theorem is proved in the next subsection. In addition  the following lemma  proved in
Appendix  directly implies that the ASP algorithm is (s  β)-locally stable at (p  α) for β = s
n log( n
s )
when (1) holds.
Lemma 1. Assume that (A1) and (A2) hold. For all α and ˜α  |D(p α)−D(p  ˜α)|
D(p α)(cid:107) ˜α−α(cid:107)2

= O (1) .

5

Algorithm 1 Adaptive Spectral Partition(T  δ  K)

T

T

1. Initial random observations.
Sample

1

log e(V V)T
d(V V)n

4 log(T /n) edges uniformly at random without replacement and compute δ ←
4 −
4 log(T /n) additional edges uniformly at random without replacement

|Si|(|Si|−1) for all 1 ≤ i ≤ K and ˆpij = 4e(Si Sj )
n(n−1)

Sample δT
Extract S1  . . .  SK using the spectral clustering algorithm of [16]
2. Estimating the SBM parameters. Estimate ˆα and ˆp from the observations made in 1. and the
extracted clusters S1  . . .  SK:
n and ˆpii = 4e(Si Si)
|Si|
ˆαi =
i (cid:54)= j.
3. Computing the optimal sampling strategy. Solve ˆx∗ ∈ arg maxx∈X ( ˆα) D(x  ˆp)
4. First round of cluster improvement.
ˆp ← maxi j ˆpij; e(A  B) ← 0 for all A  B (i.e.  reset all pairs)
Randomly observe 2(1 − δ
for all 1 ≤ i  j ≤ K
for all 1 ≤ i ≤ K do
ˆVi = ∅
for all v ∈ Si do

n edges between node v ∈ Si and nodes in Sj for all v ∈ Si and

n(n−1)
2|Si||Sj| for all

2 )ˆx∗

δT

δT

ij

T

(cid:12)(cid:12)e(v Sk) − 2(1 − δ

2 )ˆx∗

ik ˆpik

T
n

(cid:12)(cid:12) ≤ δ

4 ˆp T

n

Add v to ˆVi when max1≤k≤K

end for

end for
5. Second round of cluster improvement.
e(A  B) ← 0 for all A  B (i.e.  reset all pairs)
for all v ∈ ∪K

δ
4K

k=1(Sk \ ˆVk) do
n−(cid:80)K
k=1 | ˆVk| nodes from ˆVi for all i; and observe the edges between v and
Randomly select
(cid:17)
(cid:16)
(cid:80)K
the selected nodes
v is assigned to ˆVk∗ where
k∗ = arg max1≤i≤K

e(v Sk) log (ˆpik) + ( δ

k=1 | ˆVk| − e(v Sk)) log (1 − ˆpik)

n−(cid:80)K

k=1

4K

T

T

.

end for

4.2 The steps of ASP and their analysis

We describe below the various steps of the ASP algorithm  and analyze their performance. The proofs
of all lemmas are postponed to the Appendix. In Algorithm 1  the pseudo-code of ASP  e(A B)
denotes the number of positive observations between nodes in A and nodes in B  and d(A B) denotes
the number of sampled edges between A and B.

4.2.1 Initial random observations

4 random edge samples so as to build initial cluster
The ﬁrst task of the algorithm is to collect δT
estimates and approximate the SBM parameters (p  α) which in turn will lead to an approximate
optimal sampling strategy x∗(p  α). To output initial cluster estimates  we plan to use the spectral
decomposition algorithm from [16]  and to exploit the results therein about its performance. With this
goal in mind  the parameter δ must be set so that the initial cluster estimates have an appropriate level
4 log(T /n) samples
by e(V V)
log pT
. The
n
spectral decomposition algorithm outputs S1  . . .  SK  which in view of Theorem 2 in [16]  satisfy
with high probability  and for some constant C > 0 

of accuracy. To set δ  we ﬁrst estimate p =(cid:80)K
(cid:17)−1
(cid:12)(cid:12) ≤ n exp

j=1 αiαjpij from the ﬁrst
i=1
which is approximately equal to

(cid:16)
(cid:12)(cid:12)∪K

d(V V). We then let δ =

.

(2)

log e(V V)T
d(V V)n

i=1Vi \ Si

(cid:80)K

(cid:17)−1

(cid:19)

(cid:18)

−C

(cid:16)

T

pT /n

log(pT /n)

4.2.2 Estimating the SBM parameters

Next  using the initial cluster estimates  ASP approximates the SBM parameters (p  α) by ( ˆp  ˆα)
(refer to Line 2 in Algorithm 1). The previous step extracted the hidden clusters with at most

6

n exp(− T

n Cp) mis-classiﬁed nodes. This observation directly implies that: for any k 

|ˆαk − αk| ≤ exp

−C

(cid:18)

.

pT /n

log(pT /n)

(cid:19)
(cid:12)(cid:12) ≤ n exp
(cid:17)
i=1Vi \ Si
T p/n + 1√

.

n

In addition  we can show that the initial cluster estimates also lead to a very accurate estimate of p  as
stated in the following lemma.

Lemma 2. Assume that (A1) and (A2) hold. When(cid:12)(cid:12)∪K
(cid:16) log(T p/n)

n = ω(1)  with high probability  |pij− ˆpij|

= O

pT

pij

(cid:16)−C pT /n

log(pT /n)

(cid:17)

and

4.2.3 Computing the optimal sampling strategy
ASP now computes ˆx∗ ∈ arg maxx∈X (α) ∆( ˆx  ˆα). The main idea behind ASP is to use ˆx∗ to deﬁne
which edges should be sampled. However  ˆx∗ deﬁnes how many times edges from cluster Vi to
cluster Vj should be sampled  and these clusters are for now just approximated by Si and Sj. Hence
using ˆx∗ induces two sources of errors: ﬁrst ˆx∗ is inexact and then when randomly sampling an edge
from v ∈ Vi to Sj  the binary outcome is a Bernoulli r.v. with mean ¯pij rather than pij  where

K(cid:88)

¯pij =

1
|Sj|

pik|Sj ∩ Vk|.

Let ¯p = [¯pij]i j. The following lemma is instrumental to bound the impact of these errors:

Lemma 3. Assume that (A1) and (A2) hold. When (cid:12)(cid:12)∪K

k=1

(cid:12)(cid:12) ≤ n exp

(cid:16)−C pT /n

log(pT /n)

(cid:17)

 

i=1Vi \ Si

(cid:16) log(T p/n)

(cid:17)

|pij− ˆpij|

pij

= O

T p/n + 1√
|∆(x∗( ˆp  ˆα)  ¯p) − ∆(x∗(p  α)  p)|

n = ω(1)  with high probability 
log(T p/n)

  and pT

n

∆(x∗(p  α)  p)

= O(

T p/n

+

1√
n

) 

4.2.4 First round of cluster improvement
In this step  the ASP algorithm ﬁrst re-sets the values of e(A B) to 0 for all sets of nodes A and B.
It then randomly samples edges according to the sampling strategy ˆx∗. More precisely  for every
v ∈ Si  for each k = 1  . . .   K  it randomly selects 2(1 − δ
n edges from v to Sk. Due to the
re-set  after these new samples  e(v Sk) is a sum of independent Bernoulli r.v. with mean ¯pjk when
v ∈ Vj.
In this ﬁrst round of cluster improvement  ASP classiﬁes a node v ∈ Si only if the values e(v Sk)’s
clearly indicate its cluster. More precisely  v ∈ Si is classiﬁed in ˆVi only if:

2 )ˆx∗

ik

T

max
1≤k≤K
Note that E[e(v Sk)] = 2(1 − δ
ik ¯pik
probability at least 1 − exp(− pT /n
and the Markov inequality  with high probability 

(3)
n when v ∈ Si ∩ Vi. When v ∈ Si ∩ Vi  v satisﬁes (3) with
2 )ˆx∗
(log(pT /n))3 ) from Chernoff-Hoeffding inequality. Therefore  from (2)

ik ˆpik

ˆp

4

.

)ˆx∗

(cid:12)(cid:12)(cid:12)(cid:12)e(v Sk) − 2(1 − δ
(cid:12)(cid:12)(cid:12) ≤ |Si \ Vi| +
(cid:12)(cid:12)(cid:12)(Vi ∩ Si) \ ˆVi

2

T

T
n

(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ
(cid:12)(cid:12)(cid:12) ≤ αine

−

T
n

(cid:12)(cid:12)(cid:12)Si \ ˆVi

pT /n

(log(pT /n))4 .

δ

T

ik ˆpik

k=1(Si \ ˆVi) will be classiﬁed in the second round of cluster improvement.

n which could be far from 2(1 −
n . Thus  to satisfy (3)  e(v Sk) has to deviate from its mean a lot. From Chernoff-
n (∆(x∗( ˆp  ˆα)  ¯p) +

All the nodes in ∪K
When v ∈ Si ∩ Vj for j (cid:54)= i  E[e(v Sk)] = 2(1 − δ
2 )ˆx∗
Hoeffding inequality  we show that (3) holds with probability at most exp(− 2T
O(δp))). Thus  using Lemma 3 and the Markov inequality  with high probability 
αine−2 T
In summary  we have:

(cid:12)(cid:12)(cid:12) ˆVi \ Vi

n ·D(p α)+O( δpT

(cid:12)(cid:12)(cid:12) ≤

2 )ˆx∗

ik ¯pjk

n ).

T

7

Lemma 4. Assume that (A1) and (A2) hold. After the ﬁrst round of cluster improvement of the ASP
algorithm  we have  with high probability  for all 1 ≤ i ≤ K 

−

pT /n

(log(pT /n))4

and

n ·D(p α)+O( δpT

n ).

(cid:12)(cid:12)(cid:12) ˆVi \ Vi

(cid:12)(cid:12)(cid:12) ≤ αine−2 T

(cid:12)(cid:12)(cid:12)Si \ ˆVi

(cid:12)(cid:12)(cid:12) ≤ αine

4.2.5 Second round of cluster improvement

Finally  we use the δ
assigned to a cluster in the previous round. From Lemma 4  there are at most n exp

4 T remaining samples to classify the nodes in ∪K

(cid:16)

(cid:17)

such nodes. Hence  we can use at least δT
node.
In the second round of cluster improvement  the ASP algorithm classiﬁes nodes using the maximum
likelihood with ˆp  which is very similar to the greedy improvement step of the spectral clustering
algorithm of [16]. Deﬁne:

samples to classify each remaining

4n exp

(log(pT /n))4

pT /n

(cid:17)
k=1(Si \ ˆVi) that were not

(cid:16)− pT /n

(log(pT /n))4

DR(p  α) = min
i j:i(cid:54)=j

D+(pi  pj  α) with

(cid:40) K(cid:88)

k=1

(cid:41)

K(cid:88)

k=1

D+(pi  pj  α) =

min

y∈[0 1]K

max

αkKL(yk  pik) 

αkKL(yk  pjk)

.

From the deﬁnition of DR( ˆp  ˆα)  we can show as in [16] that v is mis-classiﬁed only when

K(cid:88)

k=1

d(v Sk)KL(

e(v Sk)
d(v Sk)

  ˆpik) ≥ d(v V)DR( ˆp  ˆα).

(cid:16)− pT

From Chernoff-Hoeffding inequality  we analyze the probability of the above event and conclude:
Lemma
n exp
(A2).

round
at most
mis-classiﬁed nodes with high probability under (A1) and

5.
n exp

second
pT /n

improvement

generates

(log(pT /n))5

cluster

The

of

(cid:17)(cid:17)

(cid:16)

n ·D(p α)+O( δpT

So far we have analyzed the number of mis-classiﬁed nodes generated in the ﬁrst round and
the second round of cluster improvement.
In the ﬁrst round  the ASP algorithm generates at
(cid:16)
most ne−2 T
n ) mis-classiﬁed nodes and in the second round  generates at most
mis-classiﬁed nodes. Note that the number of mis-classiﬁed nodes in the
ne
second round is negligible compared to that in the ﬁrst round. We thus conclude that the ASP
algorithm outputs cluster estimates with at most

(log(pT /n))5

n exp

− pT

(cid:17)

pT /n

(cid:12)(cid:12)(cid:12)∪K

ˆVi \ Vi

i=1

(cid:12)(cid:12)(cid:12) ≤ ne−2 T

n ·D(p α)+O( δpT
n )

mis-classiﬁed nodes  which concludes the proof of Theorem 2  in view of our choice of δ.

5 Conclusion

In this paper  we derived a necessary condition for the existence of a joint sampling and clustering
algorithm mis-classifying less than s = o(n) nodes w.h.p. in the SBM. This derivation revealed the
optimal sampling strategy  and allowed us to devise ASP  an algorithm that mis-classiﬁed s = o(n)
when the necessary condition holds. To our knowledge  this is the ﬁrst time the optimal cluster
recovery rate is characterized in the SBM with adaptive sampling. Being able to characterize the
optimal sampling strategy is promising  and opens up new research directions. In particular  we could
now investigate various online optimization problems involving random graphs generated by the
SBM and using adaptive edge sampling.

Acknowledgments

S. Yun was supported by Korea Electric Power Corporation. (Grant number:R18XA05). A. Proutiere
was partially supported by the Wallenberg Autonomous Systems and Software Program (WASP)
funded by the Knut and Alice Wallenberg Foundation.

8

References
[1] Emmanuel Abbe. Community detection and stochastic block models. Foundations and Trends

in Communications and Information Theory  14(1-2):1–162  2018.

[2] Emmanuel Abbe  Alfonso Bandeira  and Georgina Hall. Exact recovery in the stochastic block

model. CoRR  abs/1405.3267  2014.

[3] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
Fundamental limits and efﬁcient algorithms for recovery. In Proceedings of the 2015 IEEE 56th
Annual Symposium on Foundations of Computer Science (FOCS)  pages 670–688  2015.

[4] Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic block
model without knowing the parameters. In Advances in Neural Information Processing Systems
28  pages 676–684. Curran Associates  Inc.  2015.

[5] Aurelien Decelle  Florent Krzakala  Cristopher Moore  and Lenka Zdeborová. Inference and
phase transitions in the detection of modules in sparse networks. Phys. Rev. Lett.  107  Aug
2011.

[6] Yash Deshpande  Emmanuel Abbe  and Andrea Montanari. Asymptotic mutual information for
the binary stochastic block model. In IEEE International Symposium on Information Theory 
ISIT 2016  Barcelona  Spain  July 10-15  2016  pages 185–189  2016.

[7] Chao Gao  Zongming Ma  Anderson Y. Zhang  and Harrison H. Zhou. Achieving optimal
misclassiﬁcation proportion in stochastic block models. J. Mach. Learn. Res.  18(1):1980–2024 
January 2017.

[8] Bruce Hajek  Yihong Wu  and Jiaming Xu. Achieving exact cluster recovery threshold via
semideﬁnite programming: Extensions. IEEE Trans. Inf. Theor.  62(10):5918–5937  October
2016.

[9] Paul Holland  Kathryn Laskey  and Samuel Leinhardt. Stochastic blockmodels: First steps.

Social Networks  5(2):109 – 137  1983.

[10] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Adv.

Appl. Math.  6(1):4–22  March 1985.

[11] Laurent Massoulié. Community detection thresholds and the weak ramanujan property. In
Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing  pages 694–
703  2014.

[12] Elchanan Mossel  Joe Neeman  and Allan Sly. Consistency thresholds for the planted bisection
model. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing 
pages 69–75  2015.

[13] Elchanan Mossel  Joe Neeman  and Allan Sly. Reconstruction and estimation in the planted

partition model. Probability Theory and Related Fields  162(3-4):431–461  2015.

[14] Elchanan Mossel and Jiaming Xu. Density evolution in the degree-correlated stochastic block
model. In Proceedings of the 29th Conference on Learning Theory  COLT 2016  New York 
USA  June 23-26  2016  pages 1319–1356  2016.

[15] Se-Young Yun and Alexandre Proutiere. Community detection via random and adaptive
sampling. In Proceedings of The 27th Conference on Learning Theory  COLT 2014  Barcelona 
Spain  June 13-15  2014  pages 138–175  2014.

[16] Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic

block model. In Advances in Neural Information Processing Systems  pages 965–973  2016.

[17] Anderson Y. Zhang and Harrison H. Zhou. Minimax rates of community detection in stochastic

block models. Ann. Statist.  44(5):2252–2280  10 2016.

9

,Se-Young Yun
Alexandre Proutiere