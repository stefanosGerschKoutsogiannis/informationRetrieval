2011,Active Learning with a Drifting Distribution,We study the problem of active learning in a stream-based setting  allowing the distribution of the examples to change over time.  We prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms  both in the realizable case and under Tsybakov noise.  We further prove minimax lower bounds for this problem.,Active Learning with a Drifting Distribution

Liu Yang

Machine Learning Department

Carnegie Mellon University

liuy@cs.cmu.edu

Abstract

We study the problem of active learning in a stream-based setting  allowing the
distribution of the examples to change over time. We prove upper bounds on
the number of prediction mistakes and number of label requests for established
disagreement-based active learning algorithms  both in the realizable case and
under Tsybakov noise. We further prove minimax lower bounds for this problem.

Introduction

1
Most existing analyses of active learning are based on an i.i.d. assumption on the data. In this work 
we assume the data are independent  but we allow the distribution from which the data are drawn to
shift over time  while the target concept remains ﬁxed. We consider this problem in a stream-based
selective sampling model  and are interested in two quantities: the number of mistakes the algorithm
makes on the ﬁrst T examples in the stream  and the number of label requests among the ﬁrst T
examples in the stream.

In particular  we study scenarios in which the distribution may drift within a ﬁxed totally bounded
family of distributions. Unlike previous models of distribution drift [Bar92  CMEDV10]  the mini-
max number of mistakes (or excess number of mistakes  in the noisy case) can be sublinear in the
number of samples.

We speciﬁcally study the classic CAL active learning strategy [CAL94] in this context  and bound
the number of mistakes and label requests the algorithm makes in the realizable case  under condi-
tions on the concept space and the family of possible distributions. We also exhibit lower bounds
on these quantities that match our upper bounds in certain cases. We further study a noise-robust
variant of CAL  and analyze its number of mistakes and number of label requests in noisy scenarios
where the noise distribution remains ﬁxed over time but the marginal distribution on X may shift.
In particular  we upper bound these quantities under Tsybakov’s noise conditions [MT99]. We also
prove minimax lower bounds under these same conditions  though there is a gap between our upper
and lower bounds.

2 Deﬁnition and Notations
As in the usual statistical learning problem  there is a standard Borel space X   called the instance
space  and a set C of measurable classiﬁers h : X → {−1  +1}  called the concept space. We
additionally have a space D of distributions on X   called the distribution space. Throughout  we
suppose that the VC dimension of C  denoted d below  is ﬁnite.
For any µ1  µ2 ∈ D  let kµ1−µ2k = supA µ1(A)−µ2(A) denote the total variation pseudo-distance
between µ1 and µ2  where the set A in the sup ranges over all measurable subsets of X . For any
ǫ > 0  let Dǫ denote a minimal ǫ-cover of D  meaning that Dǫ ⊆ D and ∀µ1 ∈ D ∃µ2 ∈ Dǫ s.t.
kµ1− µ2k < ǫ  and that Dǫ has minimal possible size |Dǫ| among all subsets of D with this property.
In the learning problem  there is an unobservable sequence of distributions D1 D2  . . .  with each
Dt ∈ D  and an unobservable time-independent regular conditional distribution  which we represent

1

t=1 denote an inﬁnite
by a function η : X → [0  1]. Based on these quantities  we let Z = {(Xt  Yt)}∞
sequence of independent random variables  such that ∀t  Xt ∼ Dt  and the conditional distribution
of Yt given Xt satisﬁes ∀x ∈ X   P(Yt = +1|Xt = x) = η(x). Thus  the joint distribution of
(Xt  Yt) is speciﬁed by the pair (Dt  η)  and the distribution of Z is speciﬁed by the collection
t=1 along with η. We also denote by Zt = {(X1  Y1)  (X2  Y2)  . . .   (Xt  Yt)} the ﬁrst t
{Dt}∞
such labeled examples. Note that the η conditional distribution is time-independent  since we are
restricting ourselves to discussing drifting marginal distributions on X   rather than drifting concepts.
Concept drift is an important and interesting topic  but is beyond the scope of our present discussion.
In the active learning protocol  at each time t  the algorithm is presented with the value Xt  and
is required to predict a label ˆYt ∈ {−1  +1}; then after making this prediction  it may optionally
request to observe the true label value Yt; as a means of book-keeping  if the algorithm requests a
label Yt on round t  we deﬁne Qt = 1  and otherwise Qt = 0.

number of labels requested up to time T .

t=1

T on T   where ¯M ∗

t=1

t=1 Qt  is the total
In particular  we will study the expectations of these

Ih ˆYt 6= Yti  is the cumulative
We are primarily interested in two quantities. The ﬁrst  ˆMT =PT
number of mistakes up to time T . The second quantity of interest  ˆQT = PT
quantities: ¯MT = Eh ˆMTi and ¯QT = Eh ˆQTi. We are particularly interested in the asymptotic
I [h(Xt) 6= Yt]i. We refer
T = inf h∈C EhPT
dependence of ¯QT and ¯MT − ¯M ∗
to ¯QT as the expected number of label requests  and to ¯MT − ¯M ∗
T as the expected excess number
of mistakes. For any distribution P on X   we deﬁne erP (h) = EX∼P [η(X)I[h(X) = −1] + (1 −
η(X))I[h(X) = +1]]  the probability of h making a mistake for X ∼ P and Y with conditional
probability of being +1 equal η(X). Note that  abbreviating ert(h) = erDt (h) = P(h(Xt) 6= Yt) 
we have ¯M ∗
t=1 ert(h).
Scenarios in which both ¯MT − ¯M ∗
T and ¯QT are o(T ) (i.e.  sublinear) are considered desirable  as
these represent cases in which we do “learn” the proper way to predict labels  while asymptoti-
cally using far fewer labels than passive learning. Once establishing conditions under which this is
possible  we may then further explore the trade-off between these two quantities.
For V ⊆ C 

let diamt(V ) =
We will additionally make use of the following notions.
1
suph g∈V Dt({x : h(x) 6= g(x)}). For h : X → {−1  +1}  ¯ers:t(h) =
u=s eru(h) 
and for ﬁnite S ⊆ X × {−1  +1}  ˆer(h; S) = 1
I[h(x) 6= y]. Also let C[S] = {h ∈ C :
ˆer(h; S) = 0}. Finally  for a distribution P on X and r > 0  deﬁne BP (h  r) = {g ∈ C : P (x :
h(x) 6= g(x)) ≤ r}.

|S|P(x y)∈S

t−s+1Pt

T = inf h∈CPT

2.1 Assumptions
In addition to the assumption of independence of the Xt variables and that d < ∞  each result
below is stated under various additional assumptions. The weakest such assumption is that D is
totally bounded  in the following sense. For each ǫ > 0  let Dǫ denote a minimal subset of D such
that ∀D ∈ D ∃D′ ∈ Dǫ s.t. kD − D′k < ǫ: that is  a minimal ǫ-cover of D. We say that D is totally
bounded if it satisﬁes the following assumption.
Assumption 1. ∀ǫ > 0 |Dǫ| < ∞.
In some of the results below  we will be interested in deriving speciﬁc rates of convergence. Doing so
requires us to make stronger assumptions about D than mere total boundedness. We will speciﬁcally
consider the following condition  in which c  m ∈ [0 ∞) are constants.
Assumption 2. ∀ǫ > 0 |Dǫ| < c · ǫ−m.
For an example of a class D satisfying the total boundedness assumption  consider X = [0  1]n  and
let D be the collection of distributions that have uniformly continuous density function with respect
to the Lebesgue measure on X   with modulus of continuity at most some value ω(ǫ) for each value
of ǫ > 0  where ω(ǫ) is a ﬁxed real-valued function with limǫ→0 ω(ǫ) = 0.
As a more concrete example  when ω(ǫ) = Lǫ for some L ∈ (0 ∞)  this corresponds to the family
of Lipschitz continuous density functions with Lipschitz constant at most L. In this case  we have
|Dǫ| ≤ O (ǫ−n)  satisfying Assumption 2.

2

3 Related Work
We discuss active learning under distribution drift  with ﬁxed target concept. There are several
branches of the literature that are highly relevant to this  including domain adaptation [MMR09 
MMR08]  online learning [Lit88]  learning with concept drift  and empirical processes for indepen-
dent but not identically distributed data [vdG00].

Streamed-based Active Learning with a Fixed Distribution [DKM09] show that a certain mod-
iﬁed perceptron-like active learning algorithm can achieve a mistake bound O(d log(T )) and query
bound ˜O(d log(T ))  when learning a linear separator under a uniform distribution on the unit sphere 
in the realizable case. [DGS10] also analyze the problem of learning linear separators under a uni-

form distribution  but allowing Tsybakov noise. They ﬁnd that with ¯QT = ˜O(cid:16)d
it is possible to achieve an expected excess number of mistakes ¯MT − M ∗
At this time  we know of no work studying the number of mistakes and queries achievable by active
learning in a stream-based setting where the distribution may change over time.

α+2(cid:17) queries 
α+2(cid:17).
α+2 · T

T = ˜O(cid:16)d

α+2 T

α+1

2α

2

1

Stream-based Passive Learning with a Drifting Distribution There has been work on learning
with a drifting distribution and ﬁxed target  in the context of passive learning. [Bar92  BL97] study
the problem of learning a subset of a domain from randomly chosen examples when the probability
distribution of the examples changes slowly but continually throughout the learning process; they
give upper and lower bounds on the best achievable probability of misclassiﬁcation after a given
number of examples. They consider learning problems in which a changing environment is modeled
by a slowly changing distribution on the product space. The allowable drift is restricted by ensuring
that consecutive probability distributions are close in total variation distance. However  this assump-
tion allows for certain malicious choices of distribution sequences  which shift the probability mass
into smaller and smaller regions where the algorithm is uncertain of the target’s behavior  so that
the number of mistakes grows linearly in the number of samples in the worst case. More recently 
[FM97] have investigated learning when the distribution changes as a linear function of time. They
present algorithms that estimate the error of functions  using knowledge of this linear drift.

4 Active Learning in the Realizable Case
Throughout this section  suppose C is a ﬁxed concept space and h∗ ∈ C is a ﬁxed target function:
that is  ert(h∗) = 0. The family of scenarios in which this is true are often collectively referred
to as the realizable case. We begin our analysis by studying this realizable case because it greatly
simpliﬁes the analysis  laying bare the core ideas in plain form. We will discuss more general
scenarios  in which ert(h∗) ≥ 0  in later sections  where we ﬁnd that essentially the same principles
apply there as in this initial realizable-case analysis.

We will be particularly interested in the performance of the following simple algorithm  due to
[CAL94]  typically referred to as CAL after its discoverers. The version presented here is speciﬁed in
terms of a passive learning subroutine A (mapping any sequence of labeled examples to a classiﬁer).
In it  we use the notation DIS(V ) = {x ∈ X : ∃h  g ∈ V s.t. h(x) 6= g(x)}  also used below.
CAL
1. t ← 0  Q0 ← ∅  and let ˆh0 = A(∅)
2. Do
3.
4. Predict ˆYt = ˆht−1(Xt)
5.

t ← t + 1
If max

y∈{−1 +1}

min
h∈C

ˆer(h;Qt−1 ∪ {(Xt  y)}) = 0

min
h∈C

t = argmin
y∈{−1 +1}

Request Yt  let Qt = Qt−1 ∪ {(Xt  Yt)}

ˆer(h;Qt−1 ∪ {(Xt  y)})  and let Qt ← Qt−1 ∪ {(Xt  Y ′

6.
7. Else let Y ′
8. Let ˆht = A(Qt)
Below  we let A1IG denote the one-inclusion graph prediction strategy of [HLW94]. Speciﬁcally 
the passive learning algorithm A1IG is speciﬁed as follows. For a sequence of data points U ∈ X t+1 

t )}

3

the one-inclusion graph is a graph  where each vertex represents a distinct labeling of U that can be
realized by some classiﬁer in C  and two vertices are adjacent if and only if their corresponding
labelings for U differ by exactly one label. We use the one-inclusion graph to deﬁne a classiﬁer
based on t training points as follows. Given t labeled data points L = {(x1  y1)  . . .   (xt  yt)}  and
one test point xt+1 we are asked to predict a label for  we ﬁrst construct the one-inclusion graph
on U = {x1  . . .   xt+1}; we then orient the graph (give each edge a unique direction) in a way that
minimizes the maximum out-degree  and breaks ties in a way that is invariant to permutations of the
order of points in U; after orienting the graph in this way  we examine the subset of vertices whose
corresponding labeling of U is consistent with L; if there is only one such vertex  then we predict for
xt+1 the corresponding label from that vertex; otherwise  if there are two such vertices  then they are
adjacent in the one-inclusion graph  and we choose the one toward which the edge is directed and
use the label for xt+1 in the corresponding labeling of U as our prediction for the label of xt+1. See
[HLW94] and subsequent work for detailed studies of the one-inclusion graph prediction strategy.

4.1 Learning with a Fixed Distribution
We begin the discussion with the simplest case: namely  when |D| = 1.
Deﬁnition 1. [Han07  Han11] Deﬁne the disagreement coefﬁcient of h∗ under a distribution P as

θP (ǫ) = sup
r>ǫ

P (DIS(BP (h∗  r))) /r.

Theorem 1. For any distribution P on X  
then running CAL with A =
A1IG achieves expected mistake bound ¯MT = O (d log(T )) and expected query bound ¯QT =
O(cid:0)θP (ǫT )d log2(T )(cid:1)  for ǫT = d log(T )/T .

For completeness  the proof is included in the supplemental materials.

if D = {P} 

4.2 Learning with a Drifting Distribution
We now generalize the above results to any sequence of distributions from a totally bounded space
D. Throughout this section  let θD(ǫ) = supP ∈D θP (ǫ).
First  we prove a basic result stating that CAL can achieve a sublinear number of mistakes  and
under conditions on the disagreement coefﬁcient  also a sublinear number of queries.
Theorem 2. If D is totally bounded (Assumption 1)  then CAL (with A any empirical risk minimiza-
tion algorithm) achieves an expected mistake bound ¯MT = o(T )  and if θD(ǫ) = o(1/ǫ)  then CAL
makes an expected number of queries ¯QT = o(T ).

Proof. As mentioned  given that erQt−1 (h∗) = 0  we have that Y ′
t in Step 7 must equal h∗(Xt) 
so that the invariant erQt (h∗) = 0 is maintained for all t by induction. In particular  this implies
Qt = Zt for all t.
Fix any ǫ > 0  and enumerate the elements of Dǫ so that Dǫ = {P1  P2  . . .   P|Dǫ|}. For each t ∈ N 
let k(t) = argmink≤|Dǫ| kPk − Dtk  breaking ties arbitrarily. Let
√ǫ(cid:19) + ln(cid:18) 4

√ǫ(cid:18)d ln(cid:18) 24

L(ǫ) =(cid:24) 8

√ǫ(cid:19)(cid:19)(cid:25) .

For each i ≤ |Dǫ|  if k(t) = i for inﬁnitely many t ∈ N  then let Ti denote the smallest value of T
such that |{t ≤ T : k(t) = i}| = L(ǫ). If k(t) = i only ﬁnitely many times  then let Ti denote the
largest index t for which k(t) = i  or Ti = 1 if no such index t exists.
Let Tǫ = maxi≤|Dǫ| Ti and Vǫ = C[ZTǫ ]. We have that ∀t > Tǫ  diamt(Vǫ) ≤ diamk(t)(Vǫ) + ǫ.
For each i  let Li be a sequence of L(ǫ) i.i.d. pairs (X  Y ) with X ∼ Pi and Y = h∗(X)  and let
Vi = C[Li]. Then ∀t > Tǫ 
E(cid:2)diamk(t)(Vǫ)(cid:3) ≤ E(cid:2)diamk(t)(Vk(t))(cid:3)+ Xs≤Ti:k(s)=k(t)
kDs−Pk(s)k ≤ E(cid:2)diamk(t)(Vk(t))(cid:3)+L(ǫ)ǫ.
By classic results in the theory of PAC learning [AB99  Vap82] and our choice of L(ǫ)  ∀t >
Tǫ  E(cid:2)diamk(t)(Vk(t))(cid:3) ≤ √ǫ.

4

Combining the above arguments 

E" T
Xt=1

diamt(C[Zt−1])# ≤ Tǫ +

T

Xt=Tǫ+1

E [diamt(Vǫ)] ≤ Tǫ + ǫT +

E(cid:2)diamk(t)(Vǫ)(cid:3)

T

Xt=Tǫ+1
E(cid:2)diamk(t)(Vk(t))(cid:3)

T

Xt=Tǫ+1
≤ Tǫ + ǫT + L(ǫ)ǫT +
≤ Tǫ + ǫT + L(ǫ)ǫT + √ǫT.

Let ǫT be any nonincreasing sequence in (0  1) such that 1 ≪ TǫT ≪ T . Since |Dǫ| < ∞ for all
ǫ > 0  we must have ǫT → 0. Thus  noting that limǫ→0 L(ǫ)ǫ = 0  we have

E" T
Xt=1

diamt(C[Zt−1])# ≤ TǫT + ǫT T + L(ǫT )ǫT T + √ǫT T ≪ T.

(1)

The result on ¯MT now follows by noting that for any ˆht−1 ∈ C[Zt−1] has ert(ˆht−1) ≤
diamt(C[Zt−1])  so

¯MT = E" T
Xt=1

ert(cid:16)ˆht−1(cid:17)# ≤ E" T
Xt=1

diamt(C[Zt−1])# ≪ T.

Similarly  for r > 0  we have
P(Request Yt) = E [P(Xt ∈ DIS(C[Zt−1])|Zt−1)] ≤ E [P(Xt ∈ DIS(C[Zt−1] ∪ BDt (h∗  r)))]
≤ E [θD(r) · max{diamt(C[Zt−1])  r}] ≤ θD(r) · r + θD(r) · E [diamt(C[Zt−1])] .
Letting rT = T −1EhPT
t=1 diamt(C[Zt−1])i  we see that rT → 0 by (1)  and since θD(ǫ) =
o(1/ǫ)  we also have θD(rT )rT → 0  so that θD(rT )rT T ≪ T . Therefore  ¯QT equals
diamt(C[Zt−1])# = 2θD(rT )·rT·T ≪ T.
Xt=1

P(Request Yt) ≤ θD(rT )·rT·T +θD(rT )·E" T
Xt=1

T

1

m

m+1 d

1

m

m+1 d

m+1 log2 T(cid:17) and ¯QT = O(cid:16)θD (ǫT ) T

We can also state a more speciﬁc result in the case when we have some more detailed information
on the sizes of the ﬁnite covers of D.
Theorem 3. If Assumption 2 is satisﬁed  then CAL (with A any empirical risk minimization algo-
rithm) achieves an expected mistake bound ¯MT and expected number of queries ¯QT such that ¯MT =
O(cid:16)T
Proof. Fix ǫ > 0  enumerate Dǫ = {P1  P2  . . .   P|Dǫ|}  and for each t ∈ N  let k(t) =
argmin1≤k≤|Dǫ| kDt − Pkk. Let {X ′
t ∼ Pk(t) 
and Z ′
t = {(X ′
E" T
diamt(C[Zt−1])# ≤ E" T
Xt=1
Xt=1
≤ E" T
Xt=1

t=1 be a sequence of independent samples  with X ′
t}∞
t  h∗(X ′

m+1 log2 T(cid:17)  where ǫT = (d/T )

t−1])# +
Xt=1
t−1])# + ǫT ≤

diamt(C[Z ′

E(cid:2)diamPk(t)(C[Z ′

t−1])(cid:3) + 2ǫT.

1  h∗(X ′

1))  . . .   (X ′

The classic convergence rates results from PAC learning [AB99  Vap82] imply

diamt(C[Z ′

kDt − Pk(t)k

t)}. Then

Xt=1

1

m+1 .

T

T

T

Xt=1
E(cid:2)diamPk(t)(C[Z ′
Xt=1
≤ O(d log T ) ·

T

T

d log t

Xt=1

O(cid:16)

|{i≤t:k(i)=k(t)}|(cid:17)
t−1])(cid:3) =
|{i≤t:k(i)=k(t)}| ≤ O(d log T ) · |Dǫ| ·

1

⌈T /|Dǫ|⌉

Xu=1

1

u ≤ O(cid:0)d|Dǫ| log2(T )(cid:1) .

5

Taking ǫ = (T /d)− 1

t=1

Thus PT
¯MT ≤ E" T
Xt=1

E [diamt(C[Zt−1])] ≤ O(cid:0)d|Dǫ| log2(T ) + ǫT(cid:1) ≤ O(cid:0)d · ǫ−m log2(T ) + ǫT(cid:1).

m+1   this is O(cid:16)d

m+1 log2(T )(cid:17). We therefore have
m+1 · T
ert(h)# ≤ E" T
diamt(C[Zt−1])# ≤ O(cid:16)d
Xt=1
m+1 · T

sup

h∈C[Zt−1]

1

m

m+1 log2(T )(cid:17) .

1

m

Similarly  letting ǫT = (d/T )

1

m+1   ¯QT is at most

Dt (DIS (BDt (h∗  max{diamt(C[Zt−1])  ǫT})))#

E" T
Dt(DIS(C[Zt−1]))# ≤ E" T
Xt=1
Xt=1
≤ E" T
θD (ǫT ) · max{diamt(C[Zt−1])  ǫT}#
Xt=1
≤ E" T
θD (ǫT ) · diamt(C[Zt−1])#+ θD (ǫT ) T ǫT ≤ O(cid:16)θD (ǫT ) · d
Xt=1

1

m+1 · T

m

m+1 log2(T )(cid:17) .

We can additionally construct a lower bound for this scenario  as follows. Suppose C contains a full
inﬁnite binary tree for which all classiﬁers in the tree agree on some point. That is  there is a set of
points {xb : b ∈ {0  1}k  k ∈ N} such that  for b1 = 0 and ∀b2  b3  . . . ∈ {0  1}  ∃h ∈ C such that
h(x(b1 ... bj−1)) = bj for j ≥ 2. For instance  this is the case for linear separators (and most other
natural “geometric” concept spaces).
Theorem 4. For any C as above  for any active learning algorithm  ∃ a set D satsifying Assump-
tion 2  a target function h∗ ∈ C  and a sequence of distributions {Dt}T
t=1 in D such that the achieved
¯MT and ¯QT satisfy ¯MT = Ω(cid:0)T

The proof is analogous to that of Theorem 9 below  and is therefore omitted for brevity.

m+1(cid:1) =⇒ ¯QT = Ω(cid:0)T

m+1(cid:1)  and ¯MT = O(cid:0)T

m+1(cid:1).

m

m

m

5 Learning with Noise
In this section  we extend the above analysis to allow for various types of noise conditions commonly
studied in the literature. For this  we will need to study a noise-robust variant of CAL  below
referred to as Agnostic CAL (or ACAL). We prove upper bounds achieved by ACAL  as well as
(non-matching) minimax lower bounds.

5.1 Noise Conditions
The following assumption may be referred to as a strictly benign noise condition  which essentially
says the model is speciﬁed correctly in that h∗ ∈ C  and though the labels may be stochastic  they
are not completely random  but rather each is slightly biased toward the h∗ label.
Assumption 3. h∗ = sign(η − 1/2) ∈ C and ∀x  η(x) 6= 1/2.
A particularly interesting special case of Assumption 3 is given by Tsybakov’s noise conditions 
which essentially control how common it is to have η values close to 1/2. Formally:
Assumption 4. η satisﬁes Assumption 3 and for some c > 0 and α ≥ 0 
∀t > 0  P (|η(x) − 1/2| < t) < c · tα.
In the setting of shifting distributions  we will be interested in conditions for which the above as-
sumptions are satisifed simultaneously for all distributions in D. We formalize this in the following.
Assumption 5. Assumption 4 is satisﬁed for all D ∈ D  with the same c and α values.
5.2 Agnostic CAL
The following algorithm is essentially taken from [DHM07  Han11]  adapted here for this stream-
based setting. It is based on a subroutine: LEARN(L Q) = argmin
ˆer(h;L) =
0  and otherwise LEARN(L Q) = ∅.

ˆer(h;Q) if min

h∈C: ˆer(h;L)=0

h∈C

6

ACAL
1.
2. Do
3.
4.
5.
6.

t ← 0  Lt ← ∅  Qt ← ∅  let ˆht be any element of C
t ← t + 1
Predict ˆYt = ˆht−1(Xt)
For each y ∈ {−1  +1}  let h(y) = LEARN(Lt−1 Qt−1)
If either y has h(−y) = ∅ or

ˆer(h(−y);Lt−1 ∪ Qt−1) − ˆer(h(y);Lt−1 ∪ Qt−1) > ˆEt−1(Lt−1 Qt−1)

Lt ← Lt−1 ∪ {(Xt  y)}  Qt ← Qt−1

Else Request Yt  and let Lt ← Lt−1  Qt ← Qt−1 ∪ {(Xt  Yt)}
Let ˆht = LEARN(Lt Qt)
If t is a power of 2
Lt ← ∅  Qt ← ∅

7.
8.
9.
10.
11.
The algorithm is expressed in terms of a function ˆEt(L Q)  deﬁned as follows. Let δi be
a nonincreasing sequence of values in (0  1). Let ξ1  ξ2  . . . denote a sequence of indepen-
dent Uniform({−1  +1}) random variables  also independent from the data. For V ⊆ C 
t−2⌊log2 (t−1)⌋ Pt
let ˆRt(V ) = suph1 h2∈V
m=2⌊log2 (t−1)⌋+1 ξm · (h1(Xm) − h2(Xm))  ˆDt(V ) =
m=2⌊log2 (t−1)⌋+1 |h1(Xm) − h2(Xm)|  ˆUt(V  δ) = 12 ˆRt(V ) +
suph1 h2∈V
34q ˆDt(V ) ln(32t2/δ)
. Also  for any ﬁnite sets L Q ⊆ X × Y  let C[L] = {h ∈
C : ˆer(h;L) = 0}  ˆC(ǫ;L Q) = {h ∈ C[L] : ˆer(h;L ∪ Q) − ming∈C[L] ˆer(g;L ∪ Q) ≤ ǫ}. Then
deﬁne ˆUt(ǫ  δ;L Q) = ˆUt( ˆCt(ǫ;L Q)  δ)  and (letting Zǫ = {j ∈ Z : 2j ≥ ǫ})

t−2⌊log2 (t−1)⌋ Pt

+ 752 ln(32t2/δ)

1

1

t

t

ˆEt(L Q) = inf(cid:26)ǫ > 0 : ∀j ∈ Zǫ  min

m∈N

ˆUt(ǫ  δ⌊log(t)⌋;L Q) ≤ 2j−4(cid:27) .

1

≪ δi ≪ 2−i/i  ACAL achieves an expected
T = o(T )  and if θP (ǫ) = o(1/ǫ)  then ACAL makes an

5.3 Learning with a Fixed Distribution
The following results essentially follow from [Han11]  adapted to this stream-based setting.
Theorem 5. For any strictly benign (P  η)  if 2−2i
excess number of mistakes ¯MT − M ∗
expected number of queries ¯QT = o(T ).
Theorem 6. For any (P  η) satisfying Assumption 4  if D = {P}  ACAL achieves an expected
δi2i(cid:17). and
excess number of mistakes ¯MT − M ∗
α+2 · T
δi2i(cid:17).
an expected number of queries ¯QT = ˜O(cid:16)θP (ǫT ) · d
where ǫT = T − α
Corollary 1. For any (P  η) satisfying Assumption 4  if D = {P} and δi = 2−i in ACAL  the
algorithm achieves an expected number of mistakes ¯MT and expected number of queries ¯QT such
α+2(cid:17).
that  for ǫT = T − α

α+2(cid:17)  and ¯QT = ˜O(cid:16)θP (ǫT ) · d

δ⌊log(T )⌋(cid:17) +P⌊log(T )⌋

δ⌊log(T )⌋(cid:17) +P⌊log(T )⌋

α+2 log(cid:16)
α+2 · T

T = ˜O(cid:16)d

α+2   ¯MT − M ∗

T = ˜O(cid:16)d

α+2 log(cid:16)

α+2 · T

α+2 · T

α+2 .

i=0

i=0

α+1

α+1

5.4 Learning with a Drifting Distribution
We can now state our results concerning ACAL  which are analogous to Theorems 2 and 3 proved
earlier for CAL in the realizable case.
Theorem 7. If D is totally bounded (Assumption 1) and η satisﬁes Assumption 3  then ACAL with
δi = 2−i achieves an excess expected mistake bound ¯MT − M ∗
T = o(T )  and if additionally
θD(ǫ) = o(1/ǫ)  then ACAL makes an expected number of queries ¯QT = o(T ).

1

1

α

α

2

1

2

The proof of Theorem 7 essentially follows from a combination of the reasoning for Theorem 2 and
Theorem 8 below. Its proof is omitted.
Theorem 8. If Assumptions 2 and 5 are satisﬁed  then ACAL achieves an expected excess num-
T = ˜O(cid:16)T
ber of mistakes ¯MT − M ∗
number of queries ¯QT = ˜O(cid:16)θD(ǫT )T

δ⌊log(T )⌋(cid:17) +P⌊log(T )⌋
log(cid:16)

δi2i(cid:17)  and an expected
δi2i(cid:17)  where ǫT =

(α+2)(m+1) log(cid:16)

δ⌊log(T )⌋(cid:17) +P⌊log(T )⌋

(α+2)(m+1) .

(α+2)(m+1)−α

(α+2)(m+1)

T −

(α+2)m+1

i=0

i=0

1

1

α

7

The proof of this result is in many ways similar to that given above for the realizable case  and is
included among the supplemental materials.
We immediately have the following corollary for a speciﬁc δi sequence.
Corollary 2. With δi = 2−i in ACAL  the algorithm achieves expected number of mistakes ¯M and
expected number of queries ¯QT such that  for ǫT = T −

(α+2)(m+1)  

α

¯MT − M ∗

T = ˜O(cid:16)T

(α+2)m+1

(α+2)(m+1)(cid:17) and ¯QT = ˜O(cid:16)θD(ǫT ) · T

(α+2)(m+1)−α

(α+2)(m+1) (cid:17).

Just as in the realizable case  we can also state a minimax lower bound for this noisy setting.
Theorem 9. For any C as in Theorem 4  for any active learning algorithm  ∃ a set D satisfying
Assumption 2  a conditional distribution η  such that Assumption 5 is satisﬁed  and a sequence of
t=1 in D such that the ¯MT and ¯QT achieved by the learning algorithm satisfy
distributions {Dt}T
T = Ω(cid:16)T
¯MT − M ∗
The proof is included in the supplemental material.

α+2+mα(cid:17) =⇒ ¯QT = Ω(cid:16)T

α+2+mα(cid:17) and ¯MT − M ∗

T = O(cid:16)T

α+2+mα(cid:17).

2+mα

1+mα

1+mα

6 Discussion
Querying before Predicting: One interesting alternative to the above framework is to allow the
learner to make a label request before making its label predictions. From a practical perspective  this
may be more desirable and in many cases quite realistic. From a theoretical perspective  analysis
of this alternative framework essentially separates out the mistakes due to over-conﬁdence from the
mistakes due to recognized uncertainty. In some sense  this is related to the KWIK model of learning
of [LLW08].

Analyzing the above procedures in this alternative model yields several interesting details. Specif-
ically  the natural modiﬁcation of CAL produces a method that (in the realizable case) makes the
same number of label requests as before  except that now it makes zero mistakes  since CAL will
request a label if there is any uncertainty about its label.

On the other hand  the analysis of the natural modiﬁcation to ACAL can be far more subtle  when
there is noise. In particular  because the version space is only guaranteed to contain the best clas-
siﬁer with high conﬁdence  there is still a small probability of making a prediction that disagrees
with the best classiﬁer h∗ on each round that we do not request a label. So controlling the num-
ber of mistakes in this setting comes down to controlling the probability of removing h∗ from
the version space. However  this conﬁdence parameter appears in the analysis of the number of
queries  so that we have a natural trade-off between the number of mistakes and the number of
In particular  under Assumptions 2 and 5  this procedure achieves an expected
label requests.
T ≤ P⌊log(T )⌋
excess number of mistakes ¯MT − M ∗
δi2i  and an expected number of queries
¯QT = ˜O(cid:16)θD(ǫT ) · T
log(cid:16)
δ⌊log(T )⌋(cid:17) +P⌊log(T )⌋
(α+2)(m+1) .
In particular  given any nondecreasing sequence MT   we can set this δi sequence to maintain
¯MT − M ∗
Open Problems: What is not implied by the results above is any sort of trade-off between the
number of mistakes and the number of queries. Intuitively  such a trade-off should exist; however 
as CAL lacks any parameter to adjust the behavior with respect to this trade-off  it seems we need a
different approach to address that question. In the batch setting  the analogous question is the trade-
off between the number of label requests and the number of unlabeled examples needed. In the
realizable case  that trade-off is tightly characterized by Dasgupta’s splitting index analysis [Das05].
It would be interesting to determine whether the splitting index tightly characterizes the mistakes-
vs-queries trade-off in this stream-based setting as well.

δi2i(cid:17)  where ǫT = T −

T ≤ MT for all T .

(α+2)(m+1)−α

(α+2)(m+1)

i=0

i=1

1

α

In the batch setting  in which unlabeled examples are considered free  and performance is only mea-
sured as a function of the number of label requests  [BHV10] have found that there is an important
distinction between the veriﬁable label complexity and the unveriﬁable label complexity. In partic-
ular  while the former is sometimes no better than passive learning  the latter can always provide
improvements for VC classes. Is there such a thing as unveriﬁable performance measures in the
stream-based setting? To be concrete  we have the following open problem. Is there a method for
every VC class that achieves O(log(T )) mistakes and o(T ) queries in the realizable case?

8

[Das05]

[DGS10]

[DHM07]

[DKM09]

[FM97]

[Han07]

[Han11]

[HLW94]

[Lit88]

[LLW08]

[MMR08]

[MMR09]

[MT99]

[Vap82]

[vdG00]

In Advances in Neural

concept drift. In COLT  pages 168–180  2010.
S. Dasgupta. Coarse sample complexity bounds for active learning.
Information Processing Systems 18  2005.
O. Dekel  C. Gentile  and K. Sridharam. Robust selective sampling from single and multiple
teachers. In Conference on Learning Theory  2010.
S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. Tech-
nical Report CS2007-0898  Department of Computer Science and Engineering  University of
California  San Diego  2007.
S. Dasgupta  A. Kalai  and C. Monteleoni. Analysis of perceptron-based active learning. Journal
of Machine Learning Research  10:281–299  2009.
Y. Freund and Y. Mansour. Learning under persistent drift. In Proceedings of the Third European
Conference on Computational Learning Theory  EuroCOLT ’97  pages 109–118  1997.
S. Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the
24th International Conference on Machine Learning  2007.
S. Hanneke. Rates of convergence in active learning. The Annals of Statistics  39(1):333–361 
2011.
D. Haussler  N. Littlestone  and M. Warmuth. Predicting {0
points. Information and Computation  115:248–292  1994.
N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold al-
gorithm. Machine Learning  2:285–318  1988.
L. Li  M. L. Littman  and T. J. Walsh. Knows what it knows: A framework for self-aware
learning. In International Conference on Machine Learning  2008.
Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation with multiple sources. In In
Advances in Neural Information Processing Systems (NIPS)  pages 1041–1048  2008.
Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation: Learning bounds and algo-
rithms. In COLT  2009.
E. Mammen and A.B. Tsybakov. Smooth discrimination analysis. The Annals of Statistics 
27:1808–1829  1999.
V. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag  New York 
1982.
S. van de Geer. Empirical Processes in M-Estimation (Cambridge Series in Statistical and Prob-
abilistic Mathematics). Cambridge University Press  2000.

1}-functions on randomly drawn

References

[AB99]

[Bar92]

[BHV10]

[BL97]

[CAL94]

M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge
University Press  1999.
P. L. Bartlett. Learning with a slowly changing distribution. In Proceedings of the ﬁfth annual
workshop on Computational learning theory  COLT ’92  pages 243–252  1992.
M.-F. Balcan  S. Hanneke  and J. Wortman Vaughan. The true sample complexity of active
learning. Machine Learning  80(2–3):111–139  September 2010.
R. D. Barve and P. M. Long. On the complexity of learning from drifting distributions.
Comput.  138(2):170–193  1997.
D. Cohn  L. Atlas  and R. Ladner.
Learning  15(2):201–221  1994.

Improving generalization with active learning. Machine

Inf.

[CMEDV10] K. Crammer  Y. Mansour  E. Even-Dar  and J. Wortman Vaughan. Regret minimization with

 

9

,Tuan Nguyen
Subbarao Kambhampati
Minh Do
Yuqian Zhang
Han-wen Kuo
John Wright