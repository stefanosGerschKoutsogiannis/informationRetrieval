2017,Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks,Convolutional neural networks (CNNs) have recently been applied to the optical flow estimation problem. As training the CNNs requires sufficiently large ground truth training data  existing approaches resort to synthetic  unrealistic datasets. On the other hand  unsupervised methods are capable of leveraging real-world videos for training where the ground truth flow fields are not available. These methods  however  rely on the fundamental assumptions of brightness constancy and spatial smoothness priors which do not hold near motion boundaries. In this paper  we propose to exploit unlabeled videos for semi-supervised learning of optical flow with a Generative Adversarial Network. Our key insight is that the adversarial loss can capture the structural patterns of flow warp errors without making explicit assumptions. Extensive experiments on benchmark datasets demonstrate that the proposed semi-supervised algorithm performs favorably against purely supervised and semi-supervised learning schemes.,Semi-Supervised Learning for Optical Flow

with Generative Adversarial Networks

Wei-Sheng Lai1
1University of California  Merced

Jia-Bin Huang2

2Virginia Tech

1{wlai24|mhyang}@ucmerced.edu

2jbhuang@vt.edu

Ming-Hsuan Yang1 3
3Nvidia Research

Abstract

Convolutional neural networks (CNNs) have recently been applied to the optical
ﬂow estimation problem. As training the CNNs requires sufﬁciently large amounts
of labeled data  existing approaches resort to synthetic  unrealistic datasets. On
the other hand  unsupervised methods are capable of leveraging real-world videos
for training where the ground truth ﬂow ﬁelds are not available. These methods 
however  rely on the fundamental assumptions of brightness constancy and spatial
smoothness priors that do not hold near motion boundaries. In this paper  we
propose to exploit unlabeled videos for semi-supervised learning of optical ﬂow
with a Generative Adversarial Network. Our key insight is that the adversarial
loss can capture the structural patterns of ﬂow warp errors without making explicit
assumptions. Extensive experiments on benchmark datasets demonstrate that the
proposed semi-supervised algorithm performs favorably against purely supervised
and baseline semi-supervised learning schemes.

Introduction

1
Optical ﬂow estimation is one of the fundamental problems in computer vision. The classical formu-
lation builds upon the assumptions of brightness constancy and spatial smoothness [15  25]. Recent
advancements in this ﬁeld include using sparse descriptor matching as guidance [4]  leveraging dense
correspondences from hierarchical features [2  39]  or adopting edge-preserving interpolation tech-
niques [32]. Existing classical approaches  however  involve optimizing computationally expensive
non-convex objective functions.
With the rapid growth of deep convolutional neural networks (CNNs)  several approaches have
been proposed to solve optical ﬂow estimation in an end-to-end manner. Due to the lack of the
large-scale ground truth ﬂow datasets of real-world scenes  existing approaches [8  16  30] rely on
training on synthetic datasets. These synthetic datasets  however  do not reﬂect the complexity of
realistic photometric effects  motion blur  illumination  occlusion  and natural image noise. Several
recent methods [1  40] propose to leverage real-world videos for training CNNs in an unsupervised
setting (i.e.  without using ground truth ﬂow). The main idea is to use loss functions measuring
brightness constancy and spatial smoothness of ﬂow ﬁelds as a proxy for losses using ground truth
ﬂow. However  the assumptions of brightness constancy and spatial smoothness often do not hold
near motion boundaries. Despite the acceleration in computational speed  the performance of these
approaches still does not match up to the classical ﬂow estimation algorithms.
With the limited quantity and unrealistic of ground truth ﬂow and the large amounts of real-world
unlabeled data  it is thus of great interest to explore the semi-supervised learning framework. A
straightforward approach is to minimize the End Point Error (EPE) loss for data with ground truth
ﬂow and the loss functions that measure classical brightness constancy and smoothness assumptions
for unlabeled training images (Figure 1 (a)). However  we show that such an approach is sensitive
to the choice of parameters and may sometimes decrease the accuracy of ﬂow estimation. Prior
work [1  40] minimizes a robust loss function (e.g.  Charbonnier function) on the ﬂow warp error

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

(a) Baseline semi-supervised learning

(b) The proposed semi-supervised learning

Figure 1: Semi-supervised learning for optical ﬂow estimation. (a) A baseline semi-supervised
algorithm utilizes the assumptions of brightness constancy and spatial smoothness to train CNN from
unlabeled data (e.g.  [1  40]). (b) We train a generative adversarial network to capture the structure
patterns in ﬂow warp error images without making any prior assumptions.

(i.e.  the difference between the ﬁrst input image and the warped second image) by modeling the
brightness constancy with a Laplacian distribution. As shown in Figure 2  although robust loss
functions can ﬁt the likelihood of the per-pixel ﬂow warp error well  the spatial structure in the warp
error images cannot be modeled by simple distributions. Such structural patterns often arise from
occlusion and dis-occlusion caused by large object motion  where the brightness constancy assumption
does not hold. A few approaches have been developed to cope with such brightness inconsistency
problem using the Fields-of-Experts (FoE) [37] or a Gaussian Mixture Model (GMM) [33]. However 
the inference of optical ﬂow entails solving time-consuming optimization problems.
In this work  our goal is to leverage both the labeled and the unlabeled data without making explicit
assumptions on the brightness constancy and ﬂow smoothness. Speciﬁcally  we propose to impose an
adversarial loss [12] on the ﬂow warp error image to replace the commonly used brightness constancy
loss. We formulate the optical ﬂow estimation as a conditional Generative Adversarial Network
(GAN) [12]. Our generator takes the input image pair and predicts the ﬂow. We then compute the ﬂow
warp error image using a bilinear sampling layer. We learn a discriminator to distinguish between the
ﬂow warp error from predicted ﬂow and ground truth optical ﬂow ﬁelds. The adversarial training
scheme encourages the generator to produce the ﬂow warp error images that are indistinguishable
from the ground truth. The adversarial loss serves as a regularizer for both labeled and unlabeled data
(Figure 1 (b)). With the adversarial training  our network learns to model the structural patterns of
ﬂow warp error to reﬁne the motion boundary. During the test phase  the generator can efﬁciently
predict optical ﬂow in one feed-forward pass.
We make the following three contributions:
• We propose a generative adversarial training framework to learn to predict optical ﬂow by leverag-
• We develop a network to capture the spatial structure of the ﬂow warp error without making
• We demonstrate that the proposed semi-supervised ﬂow estimation method outperforms the purely
supervised and baseline semi-supervised learning when using the same amount of ground truth
ﬂow and network parameters.

ing both labeled and unlabeled data in a semi-supervised learning framework.

primitive assumptions on brightness constancy or spatial smoothness.

2 Related Work
In the following  we discuss the learning-based optical ﬂow algorithms  CNN-based semi-supervised
learning approaches  and generative adversarial networks within the context of this work.
Optical ﬂow. Classical optical ﬂow estimation approaches typically rely on the assumptions of
brightness constancy and spatial smoothness [15  25]. Sun et al. [36] provide a uniﬁed review of
classical algorithms. Here we focus our discussion on recent learning-based methods in this ﬁeld.
Learning-based methods aim to learn priors from natural image sequences without using hand-crafted
assumptions. Sun et al. [37] assume that the ﬂow warp error at each pixel is independent and use a
set of linear ﬁlters to learn the brightness inconsistency. Rosenbaum and Weiss [33] use a GMM to
learn the ﬂow warp error at the patch level. The work of Rosenbaum et al. [34] learns patch priors

2

Flow warp errorLabeled dataUnlabeled dataGround truth flowPredicted flowPredicted flowCNNCNNEPE lossBrightness constancy lossSmoothness lossFlow warp errorFlow warp errorLabeled dataUnlabeled dataEPE lossPredicted flowPredicted flowAdversarial lossCNNCNNGround truth flowInput image 1

Input image 2

Ground truth optical ﬂow

Ground truth ﬂow warp error

Negative log likelihood

Figure 2: Modeling the distribution of ﬂow warp error. The robust loss functions  e.g.  Lorentzian
or Charbonnier functions  can model the distribution of per-pixel ﬂow warp error well. However  the
spatial pattern resulting from large motion and occlusion cannot be captured by simple distributions.

to model the local ﬂow statistics. These approaches incorporate the learned priors into the classical
formulation and thus require solving time-consuming alternative optimization to infer the optical ﬂow.
Furthermore  the limited amount of training data (e.g.  Middlebury [3] or Sintel [5]) may not fully
demonstrate the capability of learning-based optical ﬂow algorithms. In contrast  we train a deep
CNN with large datasets (FlyingChairs [8] and KITTI [10]) in an end-to-end manner. Our model can
predict ﬂow efﬁciently in a single feed-forward pass.
The FlowNet [8] presents a deep CNN approach for learning optical ﬂow. Even though the network
is trained on a large dataset with ground truth ﬂow  strong data augmentation and the variational
reﬁnement are required. Ilg et al. [16] extend the FlowNet by stacking multiple networks and using
more training data with different motion types including complex 3D motion and small displacements.
To handle large motion  the SPyNet approach [30] estimates ﬂow in a classical spatial pyramid
framework by warping one of the input images and predicting the residual ﬂow at each pyramid level.
A few attempts have recently been made to learn optical ﬂow from unlabeled videos in an unsupervised
manner. The USCNN method [1] approximates the brightness constancy with a Taylor series
expansion and trains a deep network using the UCF101 dataset [35]. Yu et al. [40] enables the back-
propagation of the warping function using the bilinear sampling layer from the spatial transformer
network [18] and explicitly optimizes the brightness constancy and spatial smoothness assumptions.
While Yu et al. [40] demonstrate comparable performance with the FlowNet on the KITTI dataset 
the method requires signiﬁcantly more sophisticated data augmentation techniques and different
parameter settings for each dataset. Our approach differs from these methods in that we use both
labeled and unlabeled data to learn optical ﬂow in a semi-supervised framework.
Semi-supervised learning. Several methods combine the classiﬁcation objective with unsupervised
reconstruction losses for image recognition [31  41]. In low-level vision tasks  Kuznietsov et al. [21]
train a deep CNN using sparse ground truth data for single-image depth estimation. This method
optimizes a supervised loss for pixels with ground truth depth value as well as an unsupervised
image alignment cost and a regularization cost. The image alignment cost resembles the brightness
constancy  and the regularization cost enforces the spatial smoothness on the predicted depth maps.
We show that adopting a similar idea to combine the EPE loss with image reconstruction and
smoothness losses may not improve ﬂow accuracy. Instead  we use the adversarial training scheme
for learning to model the structural ﬂow warp error without making assumptions on images or ﬂow.
Generative adversarial networks. The GAN framework [12] has been successfully applied to
numerous problems  including image generation [7  38]  image inpainting [28]  face completion [23] 
image super-resolution [22]  semantic segmentation [24]  and image-to-image translation [17  42].
Within the scope of domain adaptation [9  14]  the discriminator learns to differentiate the features
from the two different domains  e.g.  synthetic  and real images. Kozi´nski et al. [20] adopt the
adversarial training framework for semi-supervised learning on the image segmentation task where the
discriminator is trained to distinguish between the predictions produced from labeled and unlabeled
data. Different from Kozi´nski et al. [20]  our discriminator learns to distinguish the ﬂow warp errors
between using the ground truth ﬂow and using the estimated ﬂow. The generator thus learns to model
the spatial structure of ﬂow warp error images and can improve ﬂow estimation accuracy around
motion boundaries.

3

Flow warp error-0.15-0.1-0.0500.050.10.15Negative log likelihoodFlow warp errorGaussianLaplacianLorentzian3 Semi-Supervised Optical Flow Estimation

In this section  we describe the semi-supervised learning approach for optical ﬂow estimation  the
design methodology of the proposed generative adversarial network for learning the ﬂow warp error 
and the use of the adversarial loss to leverage labeled and unlabeled data.

3.1 Semi-supervised learning

(cid:113)

We address the problem of learning optical ﬂow by using both labeled data (i.e.  with the ground truth
dense optical ﬂow) and unlabeled data (i.e.  raw videos). Given a pair of input images {I1  I2}  we
train a deep network to generate the dense optical ﬂow ﬁeld f = [u  v]. For labeled data with the
ground truth optical ﬂow (denoted by ˆf = [ˆu  ˆv])  we optimize the EPE loss between the predicted
and ground truth ﬂow:

LEPE(f  ˆf ) =

(u − ˆu)2 + (v − ˆv)2.

(1)
For unlabeled data  existing work [40] makes use of the classical brightness constancy and spatial
smoothness to deﬁne the image warping loss and ﬂow smoothness loss:

Lwarp(I1  I2  f ) = ρ (I1 − W (I2  f ))  
Lsmooth(f ) = ρ(∂xu) + ρ(∂yu) + ρ(∂xv) + ρ(∂yv) 

(2)
(3)
where ∂x and ∂y are horizontal and vertical gradient operators and ρ(·) is the robust penalty function.
The warping function W (I2  f ) uses the bilinear sampling [18] to warp I2 according to the ﬂow
ﬁeld f. The difference I1 − W (I2  f ) is the ﬂow warp error as shown in Figure 2. Minimizing
Lwarp(I1  I2  f ) enforces the ﬂow warp error to be close to zero at every pixel.
A baseline semi-supervised learning approach is to minimize LEPE for labeled data and minimize
Lwarp and Lsmooth for unlabeled data:

(cid:88)

i∈Dl

(cid:16)

f (i)  ˆf (i)(cid:17)

LEPE

(cid:88)

(cid:16)

+

j∈Du

(cid:16)

2   f (j)(cid:17)

(cid:16)

f (j)(cid:17)(cid:17)

λwLwarp

I (j)
1   I (j)

+ λsLsmooth

 

(4)

where Dl and Du represent labeled and unlabeled datasets  respectively. However  the commonly
used robust loss functions (e.g.  Lorentzian and Charbonnier) assume that the error is independent
at each pixel and thus cannot model the structural patterns of ﬂow warp error caused by occlusion.
Minimizing the combination of the supervised loss in (1) and unsupervised losses in (2) and (3)
may degrade the ﬂow accuracy  especially when large motion present in the input image pair. As a
result  instead of using the unsupervised losses based on classical assumptions  we propose to impose
an adversarial loss on the ﬂow warp images within a generative adversarial network. We use the
adversarial loss to regularize the ﬂow estimation for both labeled and unlabeled data.

3.2 Adversarial training

Training a GAN involves optimizing the two networks: a generator G and a discriminator D. The
generator G takes a pair of input images to generate optical ﬂow. The discriminator D performs
binary classiﬁcation to distinguish whether a ﬂow warp error image is produced by the estimated
ﬂow from the generator G or by the ground truth ﬂow. We denote the ﬂow warp error image from the
ground truth ﬂow and generated ﬂow by ˆy = I1 − W(I2  ˆf ) and y = I1 − W(I2  f )  respectively.
The objective function to train the GAN can be expressed as:

(5)
We incorporate the adversarial loss with the supervised EPE loss and solve the following minmax
problem for optimizing G and D:

Ladv(y  ˆy) = Eˆy[log D(ˆy)] + Ey[log (1 − D(y))].

min

G

max

D

LEPE(G) + λadvLadv(G  D) 

(6)

where λadv controls the relative importance of the adversarial loss for optical ﬂow estimation.
Following the standard procedure for GAN training  we alternate between the following two steps
to solve (6): (1) update the discriminator D while holding the generator G ﬁxed and (2) update
generator G while holding the discriminator D ﬁxed.

4

(a) Update discriminator D using labeled data

(b) Update generator G using both labeled and unlabeled data

Figure 3: Adversarial training procedure. Training a generative adversarial network involves the
alternative optimization of the discriminator D and generator G.

Updating discriminator D. We train the discriminator D to classify between the ground truth
ﬂow warp error (real samples  labeled as 1) and the ﬂow warp error from the predicted ﬂow (fake
samples  labeled as 0). The maximization of (5) is equivalent to minimizing the binary cross-entropy
loss LBCE(p  t) = −t log(p) − (1 − t) log(1 − p) where p is the output from the discriminator and t
is the target label. The adversarial loss for updating D is deﬁned as:

LD
adv(y  ˆy) = LBCE(D(ˆy)  1) + LBCE(D(y)  0)

(7)
As the ground truth ﬂow is required to train the discriminator  only the labeled data Dl is involved in
this step. By ﬁxing G in (6)  we minimize the following loss function for updating D:

= − log D(ˆy) − log(1 − D(y)).

(8)

(cid:88)

i∈Dl

LD
adv(y(i)  ˆy(i)).

Updating generator G. The goal of the generator is to “fool” the discriminator by producing ﬂow
to generate realistic ﬂow warp error images. Optimizing (6) with respect to G becomes minimizing
log(1 − D(y)). As suggested by Goodfellow et al. [12]  one can instead minimize − log(D(y)) to
speed up the convergence. The adversarial loss for updating G is then equivalent to the binary cross
entropy loss that assigns label 1 to the generated ﬂow warp error y:

(9)
By combining the adversarial loss with the supervised EPE loss  we minimize the following function

LG
adv(y) = LBCE(D(y)  1) = − log(D(y)).

for updating G: (cid:88)

(cid:16)LEPE

(cid:16)

f (i)  ˆf (i)(cid:17)

i∈Dl

(cid:17)

(cid:88)

j∈Du

+ λadvLG

adv(y(i))

+

λadvLG

adv(y(j)).

(10)

We note that the adversarial loss is computed for both labeled and unlabeled data  and thus guides
the ﬂow estimation for image pairs without the ground truth ﬂow. Figure 3 illustrates the two main
steps to update the generator D and the discriminator G in the proposed semi-supervised learning
framework.

5

Predicted flowGround truthflowGenerator GDiscriminator DFlow warp errorGround truthflow warp errorLabeled dataℒ"#$%(Eq. 7)UpdatedFrozenUnlabeled dataPredicted flowFlow warp errorPredicted flowGround truth flowGenerator GLabeled dataFlow warp errorℒ"#"(Eq. 1)ℒ$%&’(Eq. 9)Discriminator DUpdatedFrozen3.3 Network architecture and implementation details

Generator. We construct a 5-level SPyNet [30] as our generator. Instead of using simple stacks
of convolutional layers as sub-networks [30]  we choose the encoder-decoder architecture with skip
connections to effectively increase the receptive ﬁelds. Each convolutional layer has a 3 × 3 spatial
support and is followed by a ReLU activation. We present the details of our SPyNet architecture in
the supplementary material.
Discriminator. As we aim to learn the local structure of ﬂow warp error at motion boundaries  it
is more effective to penalize the structure at the scale of local patches instead of the whole image.
Therefore  we use the PatchGAN [17] architecture as our discriminator. The PatchGAN is a fully
convolutional classiﬁer that classiﬁes whether each N × N overlapping patch is real or fake. The
PatchGAN has a receptive ﬁeld of 47 × 47 pixels.
Implementation details. We implement the proposed method using the Torch framework [6]. We
use the Adam solver [19] to optimize both the generator and discriminator with β1 = 0.9  β2 = 0.999
and the weight decay of 1e − 4. We set the initial learning rate as 1e − 4 and then multiply by 0.5
every 100k iterations after the ﬁrst 200k iterations. We train the network for a total of 600k iterations.
We use the FlyingChairs dataset [8] as the labeled dataset and the KITTI raw videos [10] as the
unlabeled dataset. In each mini-batch  we randomly sample 4 image pairs from each dataset. We
randomly augment the training data in the following ways: (1) Scaling between [1  2]  (2) Rotating
within [−17◦  17◦]  (3) Adding Gaussian noise with a sigma of 0.1  (4) Using color jitter with respect
to brightness  contrast and saturation uniformly sampled from [0  0.04]. We then crop images to
384 × 384 patches and normalize by the mean and standard deviation computed from the ImageNet
dataset [13]. The source code is publicly available on http://vllab.ucmerced.edu/wlai24/
semiFlowGAN.

4 Experimental Results

We evaluate the performance of optical ﬂow estimation on ﬁve benchmark datasets. We conduct
ablation studies to analyze the contributions of individual components and present comparisons with
the state-of-the-art algorithms including classical variational algorithms and CNN-based approaches.

4.1 Evaluated datasets and metrics

We evaluate the proposed optical ﬂow estimation method on the benchmark datasets: MPI-Sintel [5] 
KITTI 2012 [11]  KITTI 2015 [27]  Middlebury [3] and the test set of FlyingChairs [8]. The MPI-
Sintel and FlyingChairs are synthetic datasets with dense ground truth ﬂow. The Sintel dataset
provides two rendered sets  Clean and Final  that contain both small displacements and large motion.
The training and test sets contain 1041 and 552 image pairs  respectively. The FlyingChairs test set
is composed of 640 image pairs with similar motion statistics to the training set. The Middlebury
dataset has only eight image pairs with small motion. The images from the KITTI 2012 and KITTI
2015 datasets are collected from driving real-world scenes with large forward motion. The ground
truth optical ﬂow is obtained from a 3D laser scanner and thus only covers about 50% of image pixels.
There are 194 image pairs in the KITTI 2012 dataset  and 200 image pairs in the KITTI 2015 dataset.
We compute the average EPE (1) on pixels with the ground truth ﬂow available for each dataset. On
the KITTI-2015 dataset  we also compute the Fl score [27]  which is the ratio of pixels that have EPE
greater than 3 pixels and 5% of the ground truth value.

4.2 Ablation study

We conduct ablation studies to analyze the contributions of the adversarial loss and the proposed
semi-supervised learning with different training schemes.

Adversarial loss. We adjust the weight of the adversarial loss λadv in (10) to validate the effect
of the adversarial training. When λadv = 0  our method falls back to the fully supervised learning
setting. We show the quantitative evaluation in Table 1. Using larger values of λadv may decrease the
performance and cause visual artifacts as shown in Figure 4. We therefore choose λadv = 0.01.

6

Table 1: Analysis on adversarial loss. We train the proposed model using different weights for the
adversarial loss in (10).

λadv

0

0.01
0.1
1

Sintel-Clean

EPE

Sintel-Final

EPE

KITTI 2012

EPE

KITTI 2015

EPE

Fl-all

FlyingChairs

EPE

3.51
3.30
3.57
3.93

4.70
4.68
4.73
5.18

7.69
7.16
8.25
13.89

17.19
16.02
16.82
21.07

40.82%
38.77%
42.78%
63.43%

2.15
1.95
2.11
2.21

Table 2: Analysis on receptive ﬁeld of discriminator. We vary the number of strided convolutional
layers in the discriminator to achieve different size of receptive ﬁelds.

# Strided
convolutions

Receptive ﬁeld

Sintel-Clean

EPE

Sintel-Final

EPE

KITTI 2012

EPE

KITTI 2015

EPE

Fl-all

FlyingChairs

EPE

d = 2
d = 3
d = 4

23 × 23
47 × 47
95 × 95

3.66
3.30
3.70

4.90
4.68
5.00

7.38
7.16
7.54

16.28
16.02
16.38

40.19%
38.77%
41.52%

2.15
1.95
2.16

Receptive ﬁelds of discriminator. The receptive ﬁeld of the discriminator is equivalent to the size
of patches used for classiﬁcation. The size of the receptive ﬁeld is determined by the number of
strided convolutional layers  denoted by d. We test three different values  d = 2  3  4  which are
corresponding to the receptive ﬁeld of 23×23  47×47  and 95×95  respectively. As shown in Table 2 
the network with d = 3 performs favorably against other choices on all benchmark datasets. Using
too large or too small patch sizes might not be able to capture the structure of ﬂow warp error well.
Therefore  we design our discriminator to have a receptive ﬁeld of 47 × 47 pixels.
Training schemes. We train the same network (i.e.  our generator G) with the following training
schemes: (a) Supervised: minimizing the EPE loss (1) on the FlyingChairs dataset. (b) Unsupervised:
minimizing the classical brightness constancy (2) and spatial smoothness (3) using the Charbonnier
loss function on the KITTI raw dataset. (c) Baseline semi-supervised: minimizing the combination
of supervised and unsupervised losses (4) on the FlyingChairs and KITTI raw datasets. For the
semi-supervised setting  we evaluate different combinations of λw and λs in Table 3. We note that it
is not easy to run grid search to ﬁnd the best parameter combination for all evaluated datasets. We
choose λw = 1 and λs = 0.01 for the baseline semi-supervised and unsupervised settings.
We provide the quantitative evaluation of the above training schemes in Table 4 and visual comparisons
in Figure 5 and 6. As images in KITTI 2015 have large forward motion  there are large occluded/dis-
occluded regions  particularly on the image and moving object boundaries. The brightness constancy
does not hold in these regions. Consequently  minimizing the image warping loss (2) results in
inaccurate ﬂow estimation. Compared to the fully supervised learning  our method further reﬁnes the
motion boundaries by modeling the ﬂow warp error. By incorporating both labeled and unlabeled
data in training  our method effectively reduces EPEs on the KITTI 2012 and 2015 datasets.
Training on partially labeled data. We further analyze the effect of the proposed semi-supervised
method by reducing the amount of labeled training data. Speciﬁcally  we use 75%  50% and 25%

Input images

λadv = 0

λadv = 0.01

Ground truth ﬂow

λadv = 0.1

λadv = 1

Figure 4: Comparisons of adversarial loss λadv. Using larger value of λadv does not necessarily
improve the performance and may cause unwanted visual artifacts.

7

Table 3: Evaluation for baseline semi-supervised setting. We test different combinations of λw
and λs in (4). We note that it is difﬁcult to ﬁnd the best parameters for all evaluated datasets.

λw

1
1
1
0.1
0.01

λs

0
0.1
0.01
0.01
0.01

Sintel-Clean

EPE

Sintel-Final

EPE

KITTI 2012

EPE

KITTI 2015

EPE

Fl-all

FlyingChairs

EPE

3.77
3.75
3.69
3.64
3.57

5.02
5.05
4.86
4.81
4.82

10.90
11.82
10.38
10.15
8.63

18.52
19.98
18.07
18.94
18.87

39.94%
43.18%
39.33%
40.85 %
42.63 %

2.25
2.19
2.11
2.17
2.22

Table 4: Analysis on different training schemes. “Chairs” represents the FlyingChairs dataset and
“KITTI” denotes the KITTI raw dataset. The baseline semi-supervised settings cannot improve the
ﬂow accuracy as the brightness constancy assumption does not hold on occluded regions. In contrast 
our approach effectively utilizes the unlabeled data to improve the performance.

Method

Training Datasets

Sintel-Clean

EPE

Sintel-Final

EPE

KITTI 2012

EPE

KITTI 2015
Fl

EPE

FlyingChairs

EPE

Supervised
Unsupervised

Baseline semi-supervised
Proposed semi-supervised

Chairs
KITTI

Chairs + KITTI
Chairs + KITTI

3.51
8.01
3.69
3.30

4.70
8.97
4.86
4.68

7.69
16.54
10.38
7.16

17.19
25.53
18.07
16.02

40.82%
54.40%
39.33%
38.77%

2.15
6.66
2.11
1.95

of labeled data with ground truth ﬂow from the FlyingChairs dataset and treat the remaining part as
unlabeled data to train the proposed semi-supervised method. We also train the purely supervised
method with the same amount of labeled data for comparisons. Table 5 shows that the proposed semi-
supervised method consistently outperforms the purely supervised method on the Sintel  KITTI2012
and KITTI2015 datasets. The performance gap becomes larger when using less labeled data  which
demonstrates the capability of the proposed method on utilizing the unlabeled data.
4.3 Comparisons with the state-of-the-arts
In Table 6  we compare the proposed algorithm with four variational methods: EpicFlow [32] 
DeepFlow [39]  LDOF [4] and FlowField [2]  and four CNN-based algorithms: FlowNetS [8] 
FlowNetC [8]  SPyNet [30] and FlowNet 2.0 [16]. We further ﬁne-tune our model on the Sintel
training set (denoted by “+ft“) and compare with the ﬁne-tuned results of FlowNetS  FlowNetC 
SPyNet  and FlowNet2. We note that the SPyNet+ft is also ﬁne-tuned on the Driving dataset [26]
for evaluating on the KITTI2012 and KITTI2015 datasets  while other methods are ﬁne-tuned on
the Sintel training data. The FlowNet 2.0 has signiﬁcantly more network parameters and uses more
training datasets (e.g.  FlyingThings3D [26]) to achieve the state-of-the-art performance. We show
that our model achieves competitive performance with the FlowNet and SPyNet when using the same
amount of ground truth ﬂow (i.e.  FlyingChairs and Sintel datasets). We present more qualitative
comparisons with the state-of-the-art methods in the supplementary material.

4.4 Limitations
As the images in the KITTI raw dataset are captured in driving scenes and have a strong prior of
forward camera motion  the gain of our semi-supervised learning over the supervised setting is mainly
on the KITTI 2012 and 2015 datasets. In contrast  the Sintel dataset typically has moving objects
with various types of motion. Exploring different types of video datasets  e.g.  UCF101 [35] or
DAVIS [29]  as the source of unlabeled data in our semi-supervised learning framework is a promising
future direction to improve the accuracy on general scenes.

Table 5: Training on partial labeled data. We use 75%  50% and 25% of data with ground truth
ﬂow from the FlyingChair dataset as labeled data and treat the remaining part as unlabeled data. The
proposed semi-supervised method consistently outperforms the purely supervised method.

Method

Amount of
labeled data

Sintel-Clean

EPE

Sintel-Final

EPE

KITTI 2012

EPE

KITTI 2015

EPE

Fl-all

FlyingChairs

EPE

Supervised

Proposed semi-supervised

Supervised

Proposed semi-supervised

Supervised

Proposed semi-supervised

75%

50%

25%

4.35
3.58

4.48
3.67

4.91
3.95

8.22
7.30

9.34
7.39

10.60
7.40

17.43
16.46

18.71
16.64

19.90
16.61

41.62%
41.00%

42.14%
40.48%

43.79%
40.68%

1.96
2.20

2.04
2.28

2.09
2.33

5.40
4.81

5.46
4.92

5.78
5.00

8

Input images

Unsupervised

Supervised

Ground truth ﬂow

Baseline semi-supervised

Proposed semi-supervised

Figure 5: Comparisons of training schemes. The proposed method learns the ﬂow warp error
using the adversarial training and improve the ﬂow accuracy on motion boundary.

Ground truth

Baseline semi-supervised

Proposed semi-supervised

Figure 6: Comparisons of ﬂow warp error. The baseline semi-supervised approach penalizes the
ﬂow warp error on occluded regions and thus produce inaccurate ﬂow.

Table 6: Comparisons with state-of-the-arts. We report the average EPE on six benchmark
datasets and the Fl score on the KITTI 2015 dataset.
Sintel-Final
Test
Train
EPE
EPE

Sintel-Clean
Test
Train
EPE
EPE

KITTI 2012
Test
Train
EPE
EPE

Train
EPE

Train
Fl-all

Middlebury

Train
EPE

Method

KITTI 2015

Test
Fl-all

Chairs
Test
EPE

EpicFlow
DeepFlow
LDOF
FlowField

FlowNetS
FlowNetC
SpyNet
FlowNet2

[32]
[39]
[4]
[2]

[8]
[8]
[30]
[16]

FlowNetS + ft
[8]
FlowNetC + ft [8]
SpyNet + ft
[30]
FlowNet2 + ft [16]

Ours
Ours + ft

0.31
0.25
0.44
0.27

1.09
1.15
0.33
0.35

0.98
0.93
0.33
0.35

0.37
0.32

2.27
2.66
4.64
1.86

4.50
4.31
4.12
2.02

(3.66)
(3.78)
(3.17)
(1.45)

3.30
(2.41)

4.12
5.38
7.56
3.75

7.42
7.28
6.69
3.96

6.96
6.85
6.64
4.16

6.28
6.27

3.57
4.40
5.96
3.06

5.45
5.87
5.57
3.14

(4.44)
(5.28)
(4.32)
(2.01)

4.68
(3.16)

6.29
7.21
9.12
5.81

8.43
8.81
8.43
6.02

7.76
8.51
8.36
5.74

7.61
7.31

3.47
4.58
10.94
3.33

8.26
9.35
9.12
4.09

7.52
8.79
4.13
3.61

7.16
5.23

3.8
5.8
12.4
3.5

-
-
-
-

9.10

-
4.7
-

7.5
6.8

9.27
10.63
18.19
8.33

15.44
12.52
20.56
10.06

-
-
-

9.84

27.18% 27.10%
26.52% 29.18%
38.11%
24.43%

-
-

52.86%
47.93%
44.78%
30.37%

-
-
-

28.20%

-
-
-
-

-
-
-
-

16.02
14.69

38.77% 39.71%
30.30% 31.01 %

2.94
3.53
3.47

-

2.71
2.19
2.63
1.68

3.04
2.27
3.07

-

1.95
2.41

5 Conclusions
In this work  we propose a generative adversarial network for learning optical ﬂow in a semi-
supervised manner. We use a discriminative network and an adversarial loss to learn the structural
patterns of the ﬂow warp error without making assumptions on brightness constancy and spatial
smoothness. The adversarial loss serves as guidance for estimating optical ﬂow from both labeled and
unlabeled datasets. Extensive evaluations on benchmark datasets validate the effect of the adversarial
loss and demonstrate that the proposed method performs favorably against the purely supervised and
the straightforward semi-supervised learning approaches for learning optical ﬂow.

Acknowledgement
This work is supported in part by the NSF CAREER Grant #1149783  gifts from Adobe and NVIDIA.

9

References

[1] A. Ahmadi and I. Patras. Unsupervised convolutional neural networks for motion estimation.

In ICIP  2016.

[2] C. Bailer  B. Taetz  and D. Stricker. Flow ﬁelds: Dense correspondence ﬁelds for highly accurate

large displacement optical ﬂow estimation. In ICCV  2015.

[3] S. Baker  D. Scharstein  J. Lewis  S. Roth  M. J. Black  and R. Szeliski. A database and

evaluation methodology for optical ﬂow. IJCV  92(1):1–31  2011.

[4] T. Brox and J. Malik. Large displacement optical ﬂow: descriptor matching in variational

motion estimation. TPAMI  33(3):500–513  2011.

[5] D. J. Butler  J. Wulff  G. B. Stanley  and M. J. Black. A naturalistic open source movie for

optical ﬂow evaluation. In ECCV  2012.

[6] R. Collobert  K. Kavukcuoglu  and C. Farabet. Torch7: A Matlab-like environment for machine

learning. In BigLearn  NIPS Workshop  2011.

[7] E. L. Denton  S. Chintala  and R. Fergus. Deep generative image models using a laplacian

pyramid of adversarial networks. In NIPS  2015.

[8] P. Fischer  A. Dosovitskiy  E. Ilg  P. Häusser  C. Hazırba¸s  V. Golkov  P. van der Smagt 
D. Cremers  and T. Brox. FlowNet: Learning optical ﬂow with convolutional networks. In
ICCV  2015.

[9] Y. Ganin  E. Ustinova  H. Ajakan  P. Germain  H. Larochelle  F. Laviolette  M. Marchand  and
V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning
Research  17(59):1–35  2016.

[10] A. Geiger  P. Lenz  C. Stiller  and R. Urtasun. Vision meets robotics: The KITTI dataset. The

International Journal of Robotics Research  32(11):1231–1237  2013.

[11] A. Geiger  P. Lenz  and R. Urtasun. Are we ready for autonomous driving? The KITTI vision

benchmark suite. In CVPR  2012.

[12] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and

Y. Bengio. Generative adversarial nets. In NIPS  2014.

[13] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

2016.

[14] J. Hoffman  D. Wang  F. Yu  and T. Darrell. FCNs in the wild: Pixel-level adversarial and

constraint-based adaptation. arXiv  2016.

[15] B. K. Horn and B. G. Schunck. Determining optical ﬂow. Artiﬁcial intelligence  17(1-3):185–

203  1981.

[16] E. Ilg  N. Mayer  T. Saikia  M. Keuper  A. Dosovitskiy  and T. Brox. FlowNet 2.0: Evolution of

optical ﬂow estimation with deep networks. In CVPR  2017.

[17] P. Isola  J.-Y. Zhu  T. Zhou  and A. A. Efros. Image-to-image translation with conditional

adversarial networks. In CVPR  2017.

[18] M. Jaderberg  K. Simonyan  and A. Zisserman. Spatial transformer networks. In NIPS  2015.
[19] D. Kingma and J. Ba. ADAM: A method for stochastic optimization. In ICLR  2015.
[20] M. Kozi´nski  L. Simon  and F. Jurie. An adversarial regularisation for semi-supervised training

of structured output neural networks. arXiv  2017.

[21] Y. Kuznietsov  J. Stückler  and B. Leibe. Semi-supervised deep learning for monocular depth

map prediction. In CVPR  2017.

[22] C. Ledig  L. Theis  F. Huszár  J. Caballero  A. Cunningham  A. Acosta  A. Aitken  A. Tejani 
J. Totz  Z. Wang  and W. Shi. Photo-realistic single image super-resolution using a generative
adversarial network. In CVPR  2017.

[23] Y. Li  S. Liu  J. Yang  and M.-H. Yang. Generative face completion. In CVPR  2017.
[24] P. Luc  C. Couprie  S. Chintala  and J. Verbeek. Semantic segmentation using adversarial

networks. In NIPS Workshop on Adversarial Training  2016.

[25] B. D. Lucas and T. Kanade. An iterative image registration technique with an application to

stereo vision. In International Joint Conference on Artiﬁcial Intelligence  1981.

[26] N. Mayer  E. Ilg  P. Häusser  P. Fischer  D. Cremers  A. Dosovitskiy  and T. Brox. A large
dataset to train convolutional networks for disparity  optical ﬂow  and scene ﬂow estimation. In
CVPR  2016.

[27] M. Menze and A. Geiger. Object scene ﬂow for autonomous vehicles. In CVPR  2015.

10

[28] D. Pathak  P. Krahenbuhl  J. Donahue  T. Darrell  and A. A. Efros. Context encoders: Feature

learning by inpainting. In CVPR  2016.

[29] F. Perazzi  J. Pont-Tuset  B. McWilliams  L. Van Gool  M. Gross  and A. Sorkine-Hornung. A
benchmark dataset and evaluation methodology for video object segmentation. In CVPR  2016.
[30] A. Ranjan and M. J. Black. Optical ﬂow estimation using a spatial pyramid network. In CVPR 

[31] A. Rasmus  M. Berglund  M. Honkala  H. Valpola  and T. Raiko. Semi-supervised learning with

2017.

ladder networks. In NIPS  2015.

[32] J. Revaud  P. Weinzaepfel  Z. Harchaoui  and C. Schmid. EpicFlow: Edge-preserving interpola-

tion of correspondences for optical ﬂow. In CVPR  2015.

[33] D. Rosenbaum and Y. Weiss. Beyond brightness constancy: Learning noise models for optical

[34] D. Rosenbaum  D. Zoran  and Y. Weiss. Learning the local statistics of optical ﬂow. In NIPS 

ﬂow. arXiv  2016.

2013.

[35] K. Soomro  A. R. Zamir  and M. Shah. UCF101: A dataset of 101 human actions classes from

videos in the wild. CRCV-TR-12-01  2012.

[36] D. Sun  S. Roth  and M. J. Black. A quantitative analysis of current practices in optical ﬂow

estimation and the principles behind them. IJCV  106(2):115–137  2014.

[37] D. Sun  S. Roth  J. Lewis  and M. Black. Learning optical ﬂow. In ECCV  2008.
[38] X. Wang and A. Gupta. Generative image modeling using style and structure adversarial

networks. In ECCV  2016.

[39] P. Weinzaepfel  J. Revaud  Z. Harchaoui  and C. Schmid. DeepFlow: Large displacement optical

ﬂow with deep matching. In ICCV  2013.

[40] J. J. Yu  A. W. Harley  and K. G. Derpanis. Back to basics: Unsupervised learning of optical

ﬂow via brightness constancy and motion smoothness. In ECCV Workshops  2016.

[41] Y. Zhang  K. Lee  and H. Lee. Augmenting supervised neural networks with unsupervised

objectives for large-scale image classiﬁcation. In ICML  2016.

[42] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using

cycle-consistent adversarial networks. In ICCV  2017.

11

,Wei-Sheng Lai
Jia-Bin Huang
Ming-Hsuan Yang