2019,Average Case Column Subset Selection for Entrywise $\ell_1$-Norm Loss,We study the column subset selection problem with respect to the entrywise $\ell_1$-norm loss. It is known that in the worst case  to obtain a good rank-$k$ approximation to a matrix  one needs an arbitrarily large $n^{\Omega(1)}$ number of columns to obtain a $(1+\epsilon)$-approximation to an $n \times n$ matrix. Nevertheless  we show that under certain minimal and realistic distributional settings  it is possible to obtain a $(1+\epsilon)$-approximation with a nearly linear running time and poly$(k/\epsilon)+O(k\log n)$ columns. Namely  we show that if the input matrix $A$ has the form $A = B + E$  where $B$ is an arbitrary rank-$k$ matrix  and $E$ is a matrix with i.i.d. entries drawn from any distribution $\mu$ for which the $(1+\gamma)$-th moment exists  for an arbitrarily small constant $\gamma > 0$  then it is possible to obtain a $(1+\epsilon)$-approximate column subset selection to the entrywise $\ell_1$-norm in nearly linear time. Conversely we show that if the first moment does not exist  then it is not possible to obtain a $(1+\epsilon)$-approximate subset selection algorithm even if one chooses any $n^{o(1)}$ columns. This is the first algorithm of any kind for achieving a $(1+\epsilon)$-approximation for entrywise $\ell_1$-norm loss low rank approximation.,Average Case Column Subset Selection for Entrywise

(cid:96)1-Norm Loss

Zhao Song∗

University of Washington

magic.linuxkde@gmail.com

David P. Woodruff∗

Carnegie Mellon University
dwoodruf@cs.cmu.edu

Peilin Zhong∗

Columbia University

pz2225@columbia.edu

Abstract

We study the column subset selection problem with respect to the entrywise (cid:96)1-
norm loss. It is known that in the worst case  to obtain a good rank-k approxima-
tion to a matrix  one needs an arbitrarily large nΩ(1) number of columns to obtain
a (1 + )-approximation to the best entrywise (cid:96)1-norm low rank approximation of
an n × n matrix. Nevertheless  we show that under certain minimal and realistic
distributional settings  it is possible to obtain a (1+)-approximation with a nearly
linear running time and poly(k/) + O(k log n) columns. Namely  we show that
if the input matrix A has the form A = B + E  where B is an arbitrary rank-k ma-
trix  and E is a matrix with i.i.d. entries drawn from any distribution µ for which
the (1 + γ)-th moment exists  for an arbitrarily small constant γ > 0  then it is
possible to obtain a (1 + )-approximate column subset selection to the entrywise
(cid:96)1-norm in nearly linear time. Conversely we show that if the ﬁrst moment does
not exist  then it is not possible to obtain a (1 + )-approximate subset selection
algorithm even if one chooses any no(1) columns. This is the ﬁrst algorithm of any
kind for achieving a (1 + )-approximation for entrywise (cid:96)1-norm loss low rank
approximation.

1

Introduction

Numerical linear algebra algorithms are fundamental building blocks in many machine learning and
data mining tasks. A well-studied problem is low rank matrix approximation. The most common
version of the problem is also known as Principal Component Analysis (PCA)  in which the goal is
to ﬁnd a low rank matrix to approximate a given matrix such that the Frobenius norm of the error
is minimized. The optimal solution of this objective can be obtained via the singular value decom-
position (SVD). Hence  the problem can be solved in polynomial time. If approximate solutions are
allowed  then the running time can be made almost linear in the number of non-zero entries of the
given matrix [1  2  3  4  5  6].
An important variant of the PCA problem is the entrywise (cid:96)1-norm low rank matrix approximation
problem. In this problem  instead of minimizing the Frobenius norm of the error  we seek to mini-
mize the (cid:96)1-norm of the error. In particular  given an n × n input matrix A  and a rank parameter
k  we want to ﬁnd a matrix B with rank at most k such that (cid:107)A − B(cid:107)1 is minimized  where for
i j |Ci j|. There are several reasons for using the (cid:96)1-norm as
the error measure. For example  solutions with respect to the (cid:96)1-norm loss are usually more robust
than solutions with Frobenius norm loss [7  8]. Further  the (cid:96)1-norm loss is often used as a relax-
ation of the (cid:96)0-loss  which has wide applications including sparse recovery  matrix completion  and
robust PCA; see e.g.  [9  8]. Although a number of algorithms have been proposed for the (cid:96)1-norm
loss [10  11  12  13  14  15  16  17  18  19  20  21  22]  the problem is known to be NP-hard [23]. The

a matrix C  (cid:107)C(cid:107)1 is deﬁned to be(cid:80)

∗equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

ﬁrst (cid:96)1-low rank approximation with provable guarantees was proposed by [24]. To cope with NP-
hardness  the authors gave a solution with a poly(k log n)-approximation ratio  i.e.  their algorithm
outputs a rank-k matrix B(cid:48) ∈ Rn×n for which

(cid:107)A − B(cid:48)(cid:107)1 ≤ α · min

(cid:107)A − B(cid:107)1

rank −k B

(1)

for α = poly(k log n). The approximation ratio α was further improved to O(k log k) by allowing
B(cid:48) to have a slightly larger k(cid:48) = O(k log n) rank [25]. Such B(cid:48) with larger rank is referred to as
a bicriteria solution. However  in high precision applications  such approximation factors are too
large. A natural question is if one can compute a (1 + )-approximate solution efﬁciently for (cid:96)1-
norm low rank approximation. In fact  a (1 + )-approximation algorithm was given in [26]  but
the running time of their algorithm is a prohibitive npoly(k/). Unfortunately  [26] shows in the
worst case that a 2kΩ(1) running time is necessary for any constant approximation given a standard
conjecture in complexity theory.
Notation. To describe our results  let us ﬁrst introduce some notation. We will use [n] to denote the
set {1  2 ···   n}. We use Ai to denote the ith column of A. We use Aj to denote the jth row of A.
Let Q ⊆ [n]. We use AQ to denote the matrix which is comprised of the columns of A with column
indices in Q. Similarly  we use AQ to denote the matrix which is comprised of the rows of A with
Frobenius norm of a matrix A  i.e.  (cid:107)A(cid:107)F is the square root of the sum of squares of all the entries
in A. For 1 ≤ p < 2  we use (cid:107)A(cid:107)p to denote the entry-wise (cid:96)p-norm of a matrix A  i.e.  (cid:107)A(cid:107)p is the
p-th root of the sum of p-th powers of the absolute values of the entries of A. (cid:107)A(cid:107)1 is an important
special case of (cid:107)A(cid:107)p  which corresponds to the sum of absolute values of the entries in A. A random
variable X has the Cauchy distribution if its probability density function is f (z) =

(cid:1) to denote the set of all the size-t subsets of [n]. Let (cid:107)A(cid:107)F denote the

row indices in Q. We use(cid:0)[n]

t

1

π(1+z2).

1.1 Our Results

We propose an efﬁcient bicriteria (1 + )-approximate column subset selection algorithm for the
(cid:96)1-norm. We bypass the running time lower bound mentioned above by making a mild assumption
on the input data  and also show that our assumption is necessary in a certain sense.
Our main algorithmic result is described as follows.
Theorem 1.1 (Informal version of Theorem 2.13). Suppose we are given a matrix A = A∗ + ∆ ∈
Rn×n  where rank(A∗) = k for k = no(1)  and ∆ is a random matrix for which the ∆i j are
i.i.d. symmetric random variables with E[|∆i j|p] = O(E[|∆i j|]p) for some constant p > 1. Let
which can output a subset S ⊆ [n] with |S| ≤ poly(k/) + O(k log n) for which

 ∈ (0  1/2) satisfy 1/ = no(1). There is an (cid:101)O(n2 + n poly(k/))2 time algorithm (Algorithm 1)

X∈R|S|×n
holds with probability at least 99/100.

min

(cid:107)ASX − A(cid:107)1 ≤ (1 + )(cid:107)∆(cid:107)1 

Note the running time in Theorem 1.1 is nearly linear in the number of non-zero entries of A  since
for an n× n matrix with i.i.d. noise drawn from any continuous distribution  the number of non-zero
entries of A will be n2 with probability 1. We also show the moment assumption of Theorem 1.1 is
necessary in the following precise sense.
Theorem 1.2 (Hardness  informal version of Theorem B.20). Let n > 0 be sufﬁciently large. Let
A = η · 1 · 1(cid:62) + ∆ ∈ Rn×n be a random matrix where η = nc0 for some sufﬁciently large constant
c0  1 ∈ Rn is the all-ones vector  and ∀i  j ∈ [n]  ∆i j ∼ C(0  1) are i.i.d. standard Cauchy random
variables. Let r = no(1). Then with probability at least 1 − O(1/ log log n)  ∀S ⊆ [n] with |S| = r 

(cid:107)ASX − A(cid:107)1 ≥ 1.002(cid:107)∆(cid:107)1.

min

X∈Rr×n

1.2 Our Techniques

For an overview of our hardness result  we refer readers to the supplementary material  namely 
Appendix B. In the following  we will outline the main techniques used in our algorithm.

2We use the notation (cid:101)O(f ) := O(f · logO(1) f ).

2

(1 + )-Approximate (cid:96)1-Low Rank Approximation. We make the following distributional as-
sumption on the input matrix A ∈ Rn×n: namely  A = A∗ + ∆ where A∗ is an arbitrary rank-k
from any symmetric distribution with E[|∆i j|] = 1 and
matrix and the entries of ∆ are i.i.d.
E[|∆i j|p] = O(1) for any real number p strictly greater than 1  e.g.  p = 1.000001 would sufﬁce.
Note that such an assumption is mild compared to typical noise models which require the noise be
Gaussian or have bounded variance; in our case the random variables may even be heavy-tailed with
inﬁnite variance. In this setting we show it is possible to obtain a subset of poly(k(−1 + log n))
columns spanning a (1 + )-approximation. This provably overcomes the column subset selection
lower bound of [24] which shows for entrywise (cid:96)1-low rank approximation that there are matrices
for which any subset of poly(k) columns spans at best a kΩ(1)-approximation.
Consider the following algorithm: sample poly(k/) columns of A  and try to cover as many of the
remaining columns as possible. Here  by covering a column i  we mean that if AI is the subset of
columns sampled  then miny (cid:107)AI y − Ai(cid:107)1 ≤ (1 + O())n. The reason for this notion of covering
is that we are able to show in Lemma 2.1 that in this noise model  (cid:107)∆(cid:107)1 ≥ (1 − )n2 w.h.p.  and
so if we could cover every column i  our overall cost would be (1 + O())n2  which would give a
(1 + O())-approximation to the overall cost.
We will not be able to cover all columns  unfortunately  with our initial sample of poly(k/) columns
subsets S of columns of size at most n/r  for r ≥ (1/γ)1+1/(p−1) satisfy(cid:80)
of A. Instead  though  we will show that we will be able to cover all but a set T of n/(k log k) of
the columns. Fortunately  we show in Lemma 2.4 another property of the noise matrix ∆ is that all
j∈S (cid:107)∆j(cid:107)1 = O(γn2).
and then we know that(cid:80)
Thus  for the above set T that we do not cover  we can apply this lemma to it with γ = /(k log k) 
(cid:101)O(k)-approximate (cid:96)1 low rank approximation algorithm [25] on the set T   which will only incur
j∈T (cid:107)∆j(cid:107)1 = O(n2/(k log k))  which then enables us to run a previous
total cost O(n2)  and since by Lemma 2.1 above the overall cost is at least (1 − )n2  we can still
obtain a (1 + O())-approximation overall.
The main missing piece of the algorithm to describe is why we are able to cover all but a small
fraction of the columns. One thing to note is that our noise distribution may not have a ﬁnite variance 
and consequently  there can be very large entries ∆i j in some columns. In Lemma 2.3  we show
the number of columns in ∆ for which there exists an entry larger than n1/2+1/(2p) in magnitude is
O(n(2−p)/2)  which since p > 1 is a constant bounded away from 1  is sublinear. Let us call this set
with entries larger than n1/2+1/(2p) in magnitude the set H of “heavy" columns; we will not make
any guarantees about H  rather  we will stuff it into the small set T of columns above on which we
will run our earlier O(k log k)-approximation.
For the remaining  non-heavy columns  which constitute almost all of our columns  we show in
Lemma 2.5 that (cid:107)∆i(cid:107)1 ≤ (1 + )n w.h.p. The reason this is important is that recall to cover some
column i by a sample set I of columns  we need miny (cid:107)AI y − Ai(cid:107)1 ≤ (1 + O())n. It turns out  as
we now explain  that we will get miny (cid:107)AI y − Ai(cid:107)1 ≤ (cid:107)∆i(cid:107)1 + ei  where ei is a quantity which we
can control and make O(n) by increasing our sample size I. Consequently  since (cid:107)∆i(cid:107)1 ≤ (1+)n 
overall we will have miny (cid:107)AI y − Ai(cid:107)1 ≤ (1 + O())n  which means that i will be covered. We
now explain what ei is  and why miny (cid:107)AI y − Ai(cid:107)1 ≤ (cid:107)∆i(cid:107)1 + ei.
Towards this end  we ﬁrst explain a key insight in this model. Since the p-th moment exists for
some real number p > 1 (e.g.  p = 1.000001 sufﬁces)  averaging helps reduce the noise of ﬁtting a
column Ai by subsets of other columns. Namely  we show in Lemma 2.2 that for any t non-heavy
j=1 αj∆ij(cid:107)1 =
O(t1/pn)  that is  since the individual coordinates of the ∆ij are zero-mean random variables  their
sum concentrates as we add up more columns. We do not need bounded variance for this property.
How can we use this averaging property for subset selection? The idea is  instead of sampling
a single subset I of O(k) columns and trying to cover each remaining column with this subset
as shown in [25]  we will sample multiple independent subsets I1  I2  . . .   It. Each set has size
poly(k/) and we will sample at most poly(k/) subsets. By a similar argument of [25]  for any
given column index i ∈ [n]  for most of these subset Ij  we have that A∗
i /(cid:107)∆i(cid:107)1 can be expressed
(cid:96) /(cid:107)∆(cid:96)(cid:107)1  (cid:96) ∈ Ij  via coefﬁcients of absolute value at most 1.
as a linear combination of columns A∗
Note that this is only true for most i and most j; we develop terminology for this in Deﬁnitions 2.6 

column ∆i1  . . .   ∆it of ∆  and any coefﬁcients α1  α2  . . .   αt ∈ [−1  1]  (cid:107)(cid:80)t

3

2.7  2.8  and 2.9  referring to what we call a good core. We quantify what we mean by most i and
most j having this property in Lemma 2.11 and Lemma 2.12.
The key though  that drives the analysis  is Lemma 2.10  which shows that miny (cid:107)Aiy − Ai(cid:107)1 ≤
(cid:107)∆i(cid:107)1 + ei  where ei = O(q1/p/t1−1/pn)  where q is the size of each Ij  and t is the number of
different Ij. We need q to be at least k  just as before  so that we can be guaranteed that when we
i /(cid:107)∆i(cid:107)1 can be expressed
adjoin a column index i to Ij  there is some positive probability that A∗
(cid:96) /(cid:107)∆(cid:96)(cid:107)1  (cid:96) ∈ Ij  with coefﬁcients of absolute value at most
as a linear combination of columns A∗
1. What is different in our noise model though is the division by t1−1/p. Since p > 1  if we set t
to be a large enough poly(k/)  then ei = O(n)  and then we will have covered Ai  as desired.
i /(cid:107)∆i(cid:107)1
This captures the main property that averaging the linear combinations for expression A∗
i /(cid:107)∆i(cid:107)1. Of course we
using different subsets Ij gives us better and better approximations to A∗
need to ensure several properties such as not sampling a heavy column (the averaging in Lemma 2.2
does not apply when this happens)  we need to ensure most of the Ij have small-coefﬁcient linear
combinations expressing A∗

i /(cid:107)∆i(cid:107)1  etc. This is handled in our main theorem  Theorem 2.13.

2

(cid:96)1-Norm Column Subset Selection

We ﬁrst present two subroutines.
Linear regression with (cid:96)1 loss. The ﬁrst subroutine needed is an approximate (cid:96)1 linear regression
In particular  given a matrix M ∈ Rn×d  n vectors b1  b2 ···   bn ∈ Rn  and an error
solver.
parameter  ∈ (0  1)  we want to compute x1  x2 ···   xn ∈ Rd for which ∀i ∈ [n]  we have

(cid:107)M xi − bi(cid:107)1 ≤ (1 + ) · min
x∈Rd

(cid:107)M x − bi(cid:107)1.

Furthermore  we also need an estimate vi of the regression cost (cid:107)M xi − bi(cid:107)1 for each i ∈ [n] such
that (cid:107)M xi − bi(cid:107)1 ≤ vi ≤ (1 + )(cid:107)M xi − bi(cid:107)1. Such an (cid:96)1-regression problem can be solved
efﬁciently (see [28] for a survey). The total running time to solve these n regression problems

simultaneously is at most (cid:101)O(n2) + n · poly(d log n)  and the success probability is at least 0.999.

(cid:96)1 Column subset selection for general matrices. The second subroutine needed is an (cid:96)1-low rank
approximation solver for general input matrices  though we allow a large approximation ratio. We
use the algorithm proposed by [25] for this purpose. In particular  given an n× d (d ≤ n) matrix M
and a rank parameter k  the algorithm can output a small set S ⊂ [n] with size at most O(k log n) 
such that

Furthermore  the running time is at most (cid:101)O(n2) + n · poly(k log n)  and the success probability is

rank −k B

X∈R|S|×d

(cid:107)MSX − M(cid:107)1 ≤ O(k log k) · min

(cid:107)M − B(cid:107)1.

min

at least 0.999. Now we can present our algorithm  Algorithm 1.

Sample a set I from(cid:0)[n]

Algorithm 1 (cid:96)1-Low Rank Approximation with Input Assumption
1: procedure L1NOISYLOWRANKAPPROX(A ∈ Rn×n  k  )
2:
3:

(cid:46) Theorem 2.13
Solve the approximate (cid:96)1-regression problem minx∈R|I| (cid:107)AI x − Ai(cid:107)1 for each i ∈ [n]  and
Compute the set T = {i ∈ [n] | vi is one of the top l largest values among v1  v2 ···   vn} 

(cid:1) uniformly at random  where s = poly(k/).

let vi be the estimated regression cost.

s

where l = n/ poly(k/).

Solve (cid:96)1-column subset selection for AT . Let the solution be AQ.
(cid:98)X be the solution. Return A(I∪Q) and (cid:98)X. (cid:46) A(I∪Q)(cid:98)X is a good low rank approximation to A
Solve the approximate (cid:96)1-regression problem minX∈R(|I|+|Q|)×n (cid:107)A(I∪Q)X − A(cid:107)1  and let

4:

5:
6:

7: end procedure

Running time. Uniformly sampling a set I can be done in poly(k/) time. According to our (cid:96)1-

regression subroutine  solving minx (cid:107)AI x − Ai(cid:107)1 for all i ∈ [n] can be ﬁnished in (cid:101)O(n2) + n ·

poly(k log(n)/) time. We only need sorting to compute the set T which takes O(n log n) time. By

4

our second subroutine  the (cid:96)1-column subset selection for AT will take (cid:101)O(n2) + n · poly(k log n).
The last step only needs an (cid:96)1-regression solver  which takes (cid:101)O(n2) + n · poly(k log(n)/) time.
Thus  the overall running time is (cid:101)O(n2) + n · poly(k log(n)/).

The remaining parts in this section will focus on analyzing the correctness of the algorithm.

2.1 Properties of the Noise Matrix
Recall that the input matrix A ∈ Rn×n can be decomposed as A∗ + ∆  where A∗ is the ground truth 
and ∆ is a random noise matrix. In particular  A∗ is an arbitrary rank-k matrix  and ∆ is a random
matrix where each entry is an i.i.d. sample drawn from an unknown symmetric distribution. The
only assumption on ∆ is that each entry ∆i j satisﬁes E[|∆i j|p] = O(E[|∆i j|p]) for some constant
p > 1  i.e.  the p-th moment of the noise distribution is bounded. Without loss of generality  we will
suppose E[|∆i j|] = 1  E[|∆i j|p] = O(1)  and p ∈ (1  2) throughout the paper. In this section  we
will present some key properties of the noise matrix.
The following lemma provides a lower bound on (cid:107)∆(cid:107)1. Once we have the such lower bound  we
can focus on ﬁnding a solution for which the approximation cost is at most that lower bound.
Lemma 2.1 (Lower bound on the noise matrix). Let ∆ ∈ Rn×n be a random matrix where ∆i j are
i.i.d. samples drawn from a symmetric distribution. Suppose E[|∆i j|] = 1 and E[|∆i j|p] = O(1)
for some constant p ∈ (1  2). Then  ∀ ∈ (0  1) which satisﬁes 1/ = no(1)  we have

Pr(cid:2)(cid:107)∆(cid:107)1 ≥ (1 − )n2(cid:3) ≥ 1 − e−Θ(n).

The next lemma shows the main reason why we are able to get a small ﬁtting cost when running
regression. Consider a toy example. Suppose we have a target number a ∈ R  and another t numbers
a + g1  a + g2 ···   a + gt ∈ R  where gi are i.i.d. samples drawn from the standard Gaussian
distribution N (0  1). If we use a + gi to ﬁt a  then the expected cost is E[|a + gi − a|] = E[|gi|] =

(cid:112)2/π. However  if we use the average of a + g1  a + g2 ···   a + gt to ﬁt a  then the expected
cost is E[|(cid:80)t
variance t  which means that the above expected cost is(cid:112)2/π/

i=1 gi|/t]. Since the gi are independent (cid:80)t

i=1 gi is a random Gaussian variable with
t. Thus the ﬁtting cost is reduced

√

t. By generalizing the above argument  we obtain the following lemma.

by a factor
Lemma 2.2 (Averaging reduces the noise). Let ∆1  ∆2 ···   ∆t ∈ Rn be t random vectors. The
∆i j are i.i.d. symmetric random variables with E[|∆i j|] = 1 and E[|∆i j|p] = O(1) for some
constant p ∈ (1  2). Let α1  α2 ···   αt ∈ [−1  1] be t real numbers. Conditioned on ∀i ∈ [n]  j ∈
[t] |∆i j| ≤ n1/2+1/(2p)  with probability at least 1 − 2−nΘ(1)

 

√

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

αi∆i

≤ O(t1/pn).

The above lemma needs a condition that each entry in the noise column should not be too large.
Fortunately  we can show that most of the (noise) columns do not have any large entry.
Lemma 2.3 (Only a small number of columns have large entries). Let ∆ ∈ Rn×n be a random
matrix where the ∆i j are i.i.d. symmetric random variables with E[|∆i j|] = 1 and E[|∆i j|p] =
O(1) for some constant p ∈ (1  2). Let

H = {j ∈ [n](cid:12)(cid:12) ∃i ∈ [n] |∆i j| > n1/2+1/(2p)}.

Then with probability at least 0.999 |H| ≤ O(n1−(p−1)/2).
The following lemma shows that any small subset of the columns of the noise matrix ∆ cannot
contribute too much to the overall error. By combining with the previous lemma  the entrywise (cid:96)1
cost of all columns containing large entries can be bounded.
Lemma 2.4. Let ∆ ∈ Rn×n be a random matrix where ∆i j are i.i.d. symmetric random variables
with E[|∆i j|] = 1 and E[|∆i j|p] = O(1) for some constant p ∈ (1  2). Let  ∈ (0  1) satisfy
1/ = no(1). Let r ≥ (1/)1+1/(p−1). Then  with probability at least .999  ∀S ⊂ [n] with |S| ≤ n/r 

(cid:80)
j∈S (cid:107)∆j(cid:107)1 = O(n2).

5

We say a (noise) column is good if it does not have a large entry. We can show that  with high
probability  the entry-wise (cid:96)1 cost of a good (noise) column is small.
Lemma 2.5 (Cost of good noise columns). Let ∆ ∈ Rn be a random vector where ∆i are i.i.d.
symmetric random variables with E[|∆i|] = 1 and E[|∆i|p] = O(1) for some constant p ∈ (1  2).
Let  ∈ (0  1) satisfy 1/ = no(1). If ∀i ∈ [n] |∆i| ≤ n1/2+1/(2p)  then with probability at least
1 − 2−nΘ(1)

  (cid:107)∆(cid:107)1 ≤ (1 + )n.

2.2 Deﬁnition of Tuples and Cores

In this section  we provide some basic deﬁnitions  e.g.  of a tuple  a good tuple  the core of a tuple 
and a coefﬁcients tuple. These deﬁnitions will be heavily used later when we analyze the correctness
of our algorithm.
Before we present the deﬁnitions  we introduce a notion RA∗ (S). Given a matrix A∗ ∈ Rn1×n2  for
a set S ⊆ [n2]  we deﬁne

(cid:26)(cid:12)(cid:12)(cid:12)det

(cid:16)

(A∗)Q

P

(cid:17)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12) |P| = |Q| = rank(A∗

(cid:27)

S)  Q ⊆ [n1]

 

RA∗ (S) := arg max
P :P⊆S

P )| ≥ | det(M Q

[k+1]\{j})|  we have (cid:107)x(cid:107)∞ ≤ 1.

P )| is maximized. Since M has rank k  we know det(M Q

where for a squared matrix C  det(C) denotes the determinant of C. The above maximum is over
both P and Q while RA∗ (S) only takes the value of the corresponding P . By Cramer’s rule  if we
use the columns of A∗ with index in the set RA∗ (S) to ﬁt any column of A∗ with index in the set S 
the absolute value of any ﬁtting coefﬁcient will be at most 1. The use of Cramer’s rule is as follows.
Consider a rank k matrix M ∈ Rn×(k+1). Let P ⊆ [k + 1]  Q ⊆ [n] |P| = |Q| = k be such that
| det(M Q
P ) (cid:54)= 0 and thus the columns of
MP are independent. Let i ∈ [k + 1]\ P . Then the linear equation MP x = Mi is feasible and there
is a unique solution x. Furthermore  by Cramer’s rule xj = det(M Q
P ). Since
| det(M Q
Small ﬁtting coefﬁcients are good since they will not increase the noise by too much. For example 
Sx and (cid:107)x(cid:107)∞ ≤ 1  i.e.  the i-th column can be ﬁt by the columns with indices in
suppose A∗
the set S and the ﬁtting coefﬁcients x ∈ R|S| are small. If we use the noisy columns of A∗
S + ∆S
i + ∆i)(cid:107)1 ≤
to ﬁt the noisy column A∗
(cid:107)∆i(cid:107)1 + (cid:107)∆Sx(cid:107)1. Since (cid:107)x(cid:107)∞ ≤ 1  it is possible to give a good upper bound for (cid:107)∆Sx(cid:107)1.
Deﬁnition 2.6 (Tuple). A (q  t  n)−tuple is deﬁned to be (S1  S2 ···   St  i)  where ∀j ∈ [t]  Sj ⊂
j=1 Sj. Then |S| = qt  i.e.  S1  S2 ···   St are disjoint. Furthermore 
i ∈ [n] and i (cid:54)∈ S. For simplicity  we use (S[t]  i) to denote (S1  S2 ···   St  i).
We next provide the deﬁnition of a good tuple.
Deﬁnition 2.7 (Good tuple). Given a rank-k matrix A∗ ∈ Rn×n  an (A∗  q  t  α)-good tuple is a
(q  t  n)-tuple (S[t]  i) which satisﬁes

[n] with |Sj| = q. Let S =(cid:83)t

i + ∆i  then the ﬁtting cost is at most (cid:107)(A∗

S + ∆S)x − (A∗

[k+1]\{j})/det(M Q

i = A∗

|{j ∈ [t] | i (cid:54)∈ RA∗ (Sj ∪ {i})}| ≥ α · t.

We need the deﬁnition of the core of a tuple.
Deﬁnition 2.8 (Core of a tuple). The core of (S[t]  i) is deﬁned to be the set

{j ∈ [t] | i (cid:54)∈ RA∗ (Sj ∪ {i})}.

We deﬁne a coefﬁcients tuple as follows.
Deﬁnition 2.9 (Coefﬁcients tuple). Given a rank-k matrix A∗ ∈ Rn×n  let (S[t]  i) be an
(A∗  q  t  α)-good tuple. Let C be the core of (S[t]  i). A coefﬁcients tuple corresponding to (S[t]  i)
is deﬁned to be (x1  x2 ···   xt) where ∀j ∈ [t]  xj ∈ Rq. The vector xj ∈ Rq satisﬁes: xj = 0
if j ∈ [t]\C  while A∗
i and (cid:107)xj(cid:107)∞ ≤ 1  if j ∈ C. To guarantee the coefﬁcients tuple is
unique  we restrict each vector xj ∈ Rq to be one that has the minimum lexicographic order.

xj = A∗

Sj

6

min
y∈Rqt

(cid:17)

j=1 Sj

(cid:13)(cid:13)(cid:13)1

≤

H =

j ∈ [n]

(cid:27)

.

2.3 Properties of a Good Tuple and a Coefﬁcients Tuple
Consider a good tuple (S1  S2 ···   St  i). By the deﬁnition of a good tuple  the size of the core C
of the tuple is large. For each j ∈ C  the coefﬁcients xj of using A∗
i should have absolute
value at most 1. Now consider the noisy setting. As discussed in the previous section  using ASj to ﬁt
Ai has cost at most (cid:107)∆i(cid:107)1 +(cid:107)∆Sj xj(cid:107)1. Although (cid:107)∆Sj xj(cid:107)1 has a good upper bound  it is not small
enough. To further reduce the (cid:96)1 ﬁtting cost  we can now apply the averaging argument (Lemma 2.2)
over all the ﬁtting choices corresponding to C. Formally  we have the following lemma.
Lemma 2.10 (Good tuples imply low ﬁtting cost). Suppose we are given a matrix A ∈ Rn×n which
satisﬁes A = A∗ + ∆  where A∗ ∈ Rn×n has rank k. Here ∆ ∈ Rn×n is a random matrix where
∆i j are i.i.d. symmetric random variables with E[|∆i j|] = 1 and E[|∆i j|p] = O(1) for some
constant p ∈ (1  2). Let H ⊂ [n] be deﬁned as follows:

to ﬁt A∗

Sj

  for all (A∗  q  t  1/2)-good tuples

(cid:12)(cid:12)(cid:12)(cid:12) ∃i ∈ [n] |∆i j| > n1/2+1/(2p)
(S1  S2 ···   St  i) which satisfy H ∩(cid:16)(cid:83)t
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
t(cid:88)

Let q  t ≤ no(1). Then  with probability at least 1 − 2−nΘ(1)
= ∅  we have

(cid:13)(cid:13)(cid:13)A{(cid:83)t

j=1 Sj}y − Ai

ASj xj − Ai

≤ (cid:107)∆i(cid:107)1 + O(q1/p/t1−1/pn) 

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:1). Let π : I → I be a random permutation of

where C is the core of (S1  S2 ···   St  i)  and (x1  x2 ···   xt) is the coefﬁcients tuple correspond-
ing to (S1  S2 ···   St  i).
We next show that if we choose columns randomly  it is easy to ﬁnd a good tuple.
Lemma 2.11. Given a rank-k matrix A∗ ∈ Rn×n  let q > 10k  t > 0. Let I = {i1  i2 ···   iqt+1}
qt + 1 elements. ∀j ∈ [t]  let

be a subset drawn uniformly at random from(cid:0) [n]

Sj =(cid:8)iπ((j−1)q+1)  iπ((j−1)q+2) ···   iπ((j−1)q+q)

We use i to denote iπ(qt+1). With probability ≥ 1 − 2k/q  (S1  S2 ···   St  i) is an
(A∗  q  t  1/2)−good tuple.
Lemma 2.11 implies that if we randomly choose S1  S2 ···   St  then with high probability  there
are many choices of i ∈ [n]  such that (S1  S2 ···   St  i) is a good tuple. Precisely  we can show
the following.
Lemma 2.12. Given a rank-k matrix A∗ ∈ Rn×n  let q > 10k  t > 0. Let I = {i1  i2 ···   iqt} be a

random subset uniformly drawn from(cid:0)[n]
(cid:1). Let π be a random permutation of qt elements. ∀j ∈ [t] 
(cid:12)(cid:12)(cid:8)i ∈ [n] \ I(cid:12)(cid:12) (S1  S2 ···   St  i) is an (A∗  q  t  1/2)−good tuple(cid:9)(cid:12)(cid:12) ≥ (1 − 4k/q)(n − qt).

Sj =(cid:8)iπ((j−1)q+1)  iπ((j−1)q+2) ···   iπ((j−1)q+q)

Then with probability at least 2k/q 

we deﬁne Sj as follows:

|C|

j=1

(cid:9) .

(cid:9) .

(cid:26)

qt+1

qt

2.4 Main Result

Now we are able to put all ingredients together to prove our main theorem  Theorem 2.13.
Theorem 2.13 (Formal version of Theorem 1.1). Suppose we are given a matrix A = A∗ + ∆ ∈
Rn×n  where rank(A∗) = k for k = no(1)  and ∆ is a random matrix for which the ∆i j are
i.i.d. symmetric random variables with E[|∆i j|] = 1 and E[|∆i j|p] = O(1) for some constant
(Algorithm 1) which can output a subset S ∈ [n] with |S| ≤ poly(k/) + O(k log n) for which

p ∈ (1  2). Let  ∈ (0  1/2) satisfy 1/ = no(1). There is an (cid:101)O(n2 + n poly(k/)) time algorithm

X∈R|S|×n
holds with probability at least 99/100.

min

(cid:107)ASX − A(cid:107)1 ≤ (1 + )(cid:107)∆(cid:107)1 

7

(cid:19)

(cid:18)
(cid:110)

Proof. We discussed the running time at the beginning of Section 2. Next  we turn to correctness.
Let q = Ω

. Let r = Θ(q/k). Let

1+ 1

p−1

1

k(k log k)
1+ 1


p−1

  t = q


p−1
1+ 1
p−1

(cid:111)

(cid:110)

(cid:111)

(cid:110)

qt

qt

I1 =

  I2 =

i(2)
1   i(2)

i(1)
1   i(1)

 ···   Ir =

2  ···   i(1)

2  ···   i(2)

be r independent subsets drawn uniformly at random from (cid:0)[n]
(cid:12)(cid:12)(cid:8)i ∈ [n] \ Is

s∈[r] Is  which is
the same as that in Algorithm 1. Let π1  π2 ···   πr be r independent random permutations of qt
elements. Due to Lemma 2.12 and a Chernoff bound  with probability at least .999  ∃s ∈ [r] 

(cid:12)(cid:12) (S1  S2 ···   St  i) is an (A∗  q  t  1/2)−good tuple(cid:9)(cid:12)(cid:12) ≥ (1 − 4k/q)(n − qt)
(cid:110)

i(r)
1   i(r)

(cid:1). Let I = (cid:83)

2  ···   i(r)

where

(cid:111)

 

qt

qt

Sj =

i(s)
πs((j−1)q+1)  i(s)

πs((j−1)q+2) ···   i(s)

πs((j−1)q+q)

 ∀j ∈ [t].

(cid:111)

Let set H ⊂ [n] be deﬁned as follows:

H = {j ∈ [n] | ∃i ∈ [n] |∆i j| > n1/2+1/(2p)}.

Then due to Lemma 2.3  with probability at least 0.999  |H| ≤ O(n1−(p−1)/2). Thus  for j ∈ [r] 
the probability that H ∩ Ij (cid:54)= ∅ is at most O(qt · n1−(p−1)/2/(n − qt)) = 1/nΩ(1). By taking a
union bound over all j ∈ [r]  with probability at least 1 − 1/nΩ(1)  ∀j ∈ [r]  Ij ∩ H = ∅. Thus  we
can condition on Is ∩ H = ∅. Due to Lemma 2.10 and q1/p/t1−1/p =  

i ∈ [n] \ Is

(cid:107)AIs y − Ai(cid:107)1 ≤ (cid:107)∆i(cid:107)1 + O(n)

Due to Lemma 2.5 and a union bound over all i ∈ [n] \ H  with probability at least .999  ∀i (cid:54)∈
H (cid:107)∆i(cid:107) ≤ (1 + )n. Thus 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:26)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:26)

y∈Rqt

(cid:12)(cid:12)(cid:12)(cid:12) min
(cid:12)(cid:12)(cid:12)(cid:12) min

y∈Rqt

(cid:27)(cid:12)(cid:12)(cid:12)(cid:12) ≥ (1 − 4k/q)(n − qt).
(cid:27)(cid:12)(cid:12)(cid:12)(cid:12) ≥ (1 − 4k/q)(n − qt) − |H|.

(cid:27)

i ∈ [n] \ Is

Let

(cid:107)AIsy − Ai(cid:107)1 ≤ (1 + O())n
(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12) min

y∈Rqt

T (cid:48) = [n] \

i ∈ [n]

(cid:107)AIsy − Ai(cid:107)1 ≤ (1 + O())n

.

Then |T (cid:48)| ≤ O(kn/q + n1−(p−1)/2) = O(kn/q) = O((/(k log k))1+1/(p−1)n). By our selec-
tion of T in algorithm 1  T (cid:48) should be a subset of T . Due to Lemma 2.4  with probability at
least .999  (cid:107)∆T(cid:107)1 ≤ O(n2/(k log k)). By our second subroutine mentioned at the beginning
of Section 2 it can ﬁnd a set Q ⊂ [n] with |Q| = O(k log n) such that minX∈R|Q|×|T|(cid:107)AQX −
AT(cid:107)1 ≤ O(k log k)(cid:107)∆T(cid:107)1 ≤ O(n2). Thus  we have minX∈R(|Q|+q·t·r)×n (cid:107)A(Q∪I)X −
A(cid:107)1 ≤ minX1∈R(q·t·r)×n (cid:107)AI X1 − A[n]\T(cid:107)1 + minX2∈R|Q|×n (cid:107)AQX2 − AT(cid:107)1 ≤ (1 +
O())n2. Due to Lemma 2.1  with probability at least .999  (cid:107)∆(cid:107)1 ≥ (1 − )n2  and thus
minX∈R(|Q|+q·t·r)×n (cid:107)A(Q∪I)X − A(cid:107)1 ≤ (1 + O())(cid:107)∆(cid:107)1.

3 Experiments

The take-home message from our theoretical analysis is that although the noise distribution may be
heavy-tailed  if the p-th (p > 1) moment of the distribution exists  averaging the noise may reduce
the noise. In the spirit of averaging  we found that taking a median works a bit better in practice.
Inspired by our theoretical analysis  we propose a simple heuristic algorithm (Algorithm 2) which
can output a rank-k solution. We tested Algorithm 2 on both synthetic and real datasets.

Datasets. For each rank-k experiment  we chose a high rank matrix (cid:98)A ∈ Rn×d  applied top-k SVD
to (cid:98)A and obtained a rank-k matrix A∗ as our ground truth matrix. For our synthetic data experiments 
the matrix (cid:98)A ∈ R500×500 was generated at random  where each entry was drawn uniformly from

8

Sample a set I = {i1  i2 ···   isk} from(cid:0)[n]

Algorithm 2 Median Heuristic
1: procedure L1NOISYLOWRANKAPPROXHEU(A ∈ Rn×d  k ≥ 1)
2:
3:
4:
5: end procedure

Compute B ∈ Rn×k s.t.  for t ∈ [n]  q ∈ [k]  Bt q = median(At is(q−1)+1  ···   At isq ).
Solve minX∈Rk×d (cid:107)BX − A(cid:107)1 and let the solution be X∗. Output BX∗.

(cid:1) uniformly at random.

sk

SYNTHETIC

ISOLET

MFEAT

Figure 1: Empirical results. The noise distributions of the experiments in the ﬁrst row are from a 1.1-
stable distribution. The noise distributions corresponding to the second row are the 1.1-th root of a Cauchy
distribution. The blue  red  orange and yellow bar denote SVD  the entrywise (cid:96)1-norm low rank algorithm in
[24]  the uniform k-column subset sampling algorithm in [25]  and Algorithm 2 respectively.

{0  1 ···   9}. For real datasets  we chose isolet3 (617 × 1559) or mfeat4 (651 × 2000) as (cid:98)A [29].

We tested two different noise distributions. One distribution is the standard Lévy 1.1-stable distribu-
tion [30]. Another distribution is constructed from the standard Cauchy distribution  i.e.  to draw a
sample from the constructed distribution  we draw a sample from the Cauchy distribution  keep the
sign unchanged  and take the 1
1.1-th power of the absolute value. Notice that both distributions have
bounded 1.1-th moment  but do not have a p-th moment for any p > 1.1. To construct the noise

matrix ∆ ∈ Rn×d  we drew a matrix (cid:98)∆ where each entry is an i.i.d. sample from one of the two
noise distributions  and then scaled the noise: ∆ = (cid:98)∆ · (cid:107)A∗(cid:107)1
20·n·d . We set A = A∗ + ∆ as the input.

Methodologies. We compare Algorithm 2 with SVD  poly(k  log n)-approximate entrywise (cid:96)1 low
rank approximation [24]  and uniform k-column subset sampling [25]5. For Algorithm 2  we set
s = min(50 (cid:98)n/k(cid:99)). For all of algorithms we repeated the experiment the same number of times
and compared the best solution obtained by each algorithm. We report the approximation ratio
(cid:107)B − A(cid:107)1/(cid:107)∆(cid:107)1 for each algorithm  where B ∈ Rn×d is the output rank-k matrix. The results are
shown in Figure 1. As shown in the ﬁgure  Algorithm 2 outperformed all of the other algorithms.

Acknowledgments. David P. Woodruff was supported in part by Ofﬁce of Naval Research (ONR)
grant N00014- 18-1-2562. Part of this work was done while he was visiting the Simons Institute
for the Theory of Computing. Peilin Zhong is supported in part by NSF grants (CCF-1703925 
CCF-1421161  CCF-1714818  CCF-1617955 and CCF-1740833)  Simons Foundation (#491119 to
Alexandr Andoni)  Google Research Award and a Google Ph.D. fellowship. Part of this work was
done while Zhao Song and Peilin Zhong were interns at IBM Research - Almaden and while Zhao
Song was visiting the Simons Institute for the Theory of Computing.

3https://archive.ics.uci.edu/ml/datasets/isolet
4https://archive.ics.uci.edu/ml/datasets/Multiple+Features
5We chose to compare with [24  25] due to their theoretical guarantees. Though the uniform k-column
subset sampling described in the experiments of [25] is a heuristic algorithm  it is inspired by their theoretical
algorithm.

9

12345678910k1.01.52.05.0Approximation RatioSVDL1LowRankUniformMedian12345678910k1.01.52.05.0Approximation RatioSVDL1LowRankUniformMedian12345678910k1.01.52.05.0Approximation RatioSVDL1LowRankUniformMedian12345678910k1.01.52.05.0Approximation RatioSVDL1LowRankUniformMedian12345678910k1.01.52.05.0Approximation RatioSVDL1LowRankUniformMedian12345678910k1.01.52.05.0Approximation RatioSVDL1LowRankUniformMedianReferences
[1] Tamás Sarlós. Improved approximation algorithms for large matrices via random projections.
In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)   21-24 October
2006  Berkeley  California  USA  Proceedings  pages 143–152  2006.

[2] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference  STOC’13  Palo Alto  CA 
USA  June 1-4  2013  pages 81–90. https://arxiv.org/pdf/1207.6365  2013.

[3] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-
In Proceedings of the forty-ﬁfth
sparsity time and applications to robust linear regression.
annual ACM symposium on Theory of computing  pages 91–100. ACM  https://arxiv.
org/pdf/1210.3135  2013.

[4] Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via
sparser subspace embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Com-
puter Science (FOCS)  pages 117–126. IEEE  https://arxiv.org/pdf/1211.1002  2013.

[5] Jean Bourgain  Sjoerd Dirksen  and Jelani Nelson. Toward a uniﬁed theory of sparse dimen-
sionality reduction in euclidean space. In Proceedings of the Forty-Seventh Annual ACM on
Symposium on Theory of Computing  STOC 2015  Portland  OR  USA  June 14-17  2015  pages
499–508  2015.

[6] Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities.

In
Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA)  Arlington  VA  USA  January 10-12  2016  pages 278–287  2016.

[7] Peter J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statis-

tics  35(1):73–101  1964.

[8] Emmanuel J Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component

analysis? Journal of the ACM (JACM)  58(3):11  2011.

[9] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust pca via outlier pursuit.

Advances in Neural Information Processing Systems  pages 2496–2504  2010.

In

[10] Qifa Ke and Takeo Kanade. Robust subspace computation using (cid:96)1 norm. Technical Report

CMU-CS-03-172  Carnegie Mellon University  Pittsburgh  PA.  2003.

[11] Qifa Ke and Takeo Kanade. Robust (cid:96)1 norm factorization in the presence of outliers and
missing data by alternative convex programming. In 2005 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition (CVPR)  volume 1  pages 739–746. IEEE  2005.

[12] Eunwoo Kim  Minsik Lee  Chong-Ho Choi  Nojun Kwak  and Songhwai Oh. Efﬁcient-norm-
based low-rank matrix approximations for large-scale problems using alternating rectiﬁed gra-
dient method. IEEE transactions on neural networks and learning systems  26(2):237–251 
2015.

[13] Nojun Kwak. Principal component analysis based on (cid:96)1-norm maximization. IEEE transac-

tions on pattern analysis and machine intelligence  30(9):1672–1680  2008.

[14] Yinqiang Zheng  Guangcan Liu  Shigeki Sugimoto  Shuicheng Yan  and Masatoshi Okutomi.
In 2012 IEEE Conference
Practical low-rank matrix approximation under robust (cid:96)1-norm.
on Computer Vision and Pattern Recognition  Providence  RI  USA  June 16-21  2012  pages
1410–1417  2012.

[15] J. Paul Brooks and Sapan Jot. Pcal1: An implementation in r of three methods for (cid:96)1-norm

principal component analysis. Optimization Online preprint  2012.

[16] J. Paul Brooks and José H. Dulá. The (cid:96)1-norm best-ﬁt hyperplane problem. Appl. Math. Lett. 

26(1):51–55  2013.

[17] J. Paul Brooks  José H. Dulá  and Edward L Boone. A pure (cid:96)1-norm principal component

analysis. Computational statistics & data analysis  61:83–98  2013.

10

[18] Deyu Meng  Zongben Xu  Lei Zhang  and Ji Zhao. A cyclic weighted median method for (cid:96)1

low-rank matrix factorization with missing entries. In AAAI  volume 4  page 6  2013.

[19] Panos P. Markopoulos  George N. Karystinos  and Dimitrios A. Pados. Some options for (cid:96)1-
subspace signal processing. In ISWCS 2013  The Tenth International Symposium on Wireless
Communication Systems  Ilmenau  TU Ilmenau  Germany  August 27-30  2013  pages 1–5 
2013.

[20] Panos P. Markopoulos  George N. Karystinos  and Dimitrios A. Pados. Optimal algorithms for

(cid:96)1-subspace signal processing. IEEE Trans. Signal Processing  62(19):5046–5058  2014.

[21] P. P. Markopoulos  S. Kundu  S. Chamadia  and D. A. Pados. Efﬁcient (cid:96)1-Norm Principal-

Component Analysis via Bit Flipping. ArXiv e-prints  2016.

[22] Young Woong Park and Diego Klabjan.

Iteratively reweighted least squares algorithms for

(cid:96)1-norm principal component analysis. arXiv preprint arXiv:1609.02997  2016.

[23] Nicolas Gillis and Stephen A Vavasis. On the complexity of robust pca and (cid:96)1-norm low-rank

matrix approximation. arXiv preprint arXiv:1509.09236  2015.

[24] Zhao Song  David P Woodruff  and Peilin Zhong. Low rank approximation with entrywise
In Proceedings of the 49th Annual Symposium on the Theory of Computing

(cid:96)1-norm error.
(STOC). ACM  https://arxiv.org/pdf/1611.00898  2017.

[25] Flavio Chierichetti  Sreenivas Gollapudi  Ravi Kumar  Silvio Lattanzi  Rina Panigrahy  and
In ICML. arXiv preprint

David P Woodruff. Algorithms for (cid:96)p low rank approximation.
arXiv:1705.06730  2017.

[26] Frank Ban  Vijay Bhattiprolu  Karl Bringmann  Pavel Kolev  Euiwoong Lee  and David P.

Woodruff. A PTAS for (cid:96)p-low rank approximation. In SODA  2019.

[27] Zhao Song  Ruosong Wang  Lin F Yang  Hongyang Zhang  and Peilin Zhong. Efﬁcient sym-

metric norm regression via linear sketching. arXiv preprint arXiv:1910.01788  2019.

[28] David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends

in Theoretical Computer Science  10(1-2):1–157  2014.

[29] Arthur Asuncion and David Newman. Uci machine learning repository  2007.

[30] Benoit Mandelbrot. The pareto-levy law and the distribution of income. International Eco-

nomic Review  1(2):79–106  1960.

[31] Andreas Maurer. A bound on the deviation probability for sums of non-negative random vari-

ables. J. Inequalities in Pure and Applied Mathematics  4(1):15  2003.

[32] Rafal Latala. Estimation of moments of sums of independent real random variables. The

Annals of Probability  pages 1502–1513  1997.

[33] Anirban Dasgupta  Petros Drineas  Boulos Harb  Ravi Kumar  and Michael W Mahoney. Sam-
pling algorithms and coresets for (cid:96)p regression. SIAM Journal on Computing  38(5):2060–
2078  2009.

11

,Zhao Song
David Woodruff
Peilin Zhong