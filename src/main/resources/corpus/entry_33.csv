2014,A Boosting Framework on Grounds of Online Learning,By exploiting the duality between boosting and online learning  we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework  we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting  smooth-distribution boosting  agnostic learning and  as a by-product  some generalization to double-projection online learning algorithms.,A Boosting Framework on Grounds of Online

Learning

Toﬁgh Naghibi  Beat Pﬁster

Computer Engineering and Networks Laboratory

ETH Zurich  Switzerland

naghibi@tik.ee.ethz.ch  pfister@tik.ee.ethz.ch

Abstract

By exploiting the duality between boosting and online learning  we present a
boosting framework which proves to be extremely powerful thanks to employing
the vast knowledge available in the online learning area. Using this framework 
we develop various algorithms to address multiple practically and theoretically
interesting questions including sparse boosting  smooth-distribution boosting  ag-
nostic learning and  as a by-product  some generalization to double-projection
online learning algorithms.

1 Introduction

A boosting algorithm can be seen as a meta-algorithm that maintains a distribution over the sample
space. At each iteration a weak hypothesis is learned and the distribution is updated  accordingly.
The output (strong hypothesis) is a convex combination of the weak hypotheses. Two dominant
views to describe and design boosting algorithms are “weak to strong learner” (WTSL)  which is
the original viewpoint presented in [1  2]  and boosting by “coordinate-wise gradient descent in the
functional space” (CWGD) appearing in later works [3  4  5]. A boosting algorithm adhering to the
ﬁrst view guarantees that it only requires a ﬁnite number of iterations (equivalently  ﬁnite number of
weak hypotheses) to learn a (1− ǫ)-accurate hypothesis. In contrast  an algorithm resulting from the
CWGD viewpoint (usually called potential booster) may not necessarily be a boosting algorithm in
the probability approximately correct (PAC) learning sense. However  while it is rather difﬁcult to
construct a boosting algorithm based on the ﬁrst view  the algorithmic frameworks  e.g.  AnyBoost
[4]  resulting from the second viewpoint have proven to be particularly proliﬁc when it comes to
developing new boosting algorithms. Under the CWGD view  the choice of the convex loss function
to be minimized is (arguably) the cornerstone of designing a boosting algorithm. This  however  is
a severe disadvantage in some applications.

In CWGD  the weights are not directly controllable (designable) and are only viewed as the values
of the gradient of the loss function. In many applications  some characteristics of the desired dis-
tribution are known or given as problem requirements while  ﬁnding a loss function that generates
such a distribution is likely to be difﬁcult. For instance  what loss functions can generate sparse
distributions?1 What family of loss functions results in a smooth distribution?2 We even can go
further and imagine the scenarios in which a loss function needs to put more weights on a given
subset of examples than others  either because that subset has more reliable labels or it is a prob-
lem requirement to have a more accurate hypothesis for that part of the sample space. Then  what

1In the boosting terminology  sparsity usually refers to the greedy hypothesis-selection strategy of boost-
ing methods in the functional space. However  sparsity in this paper refers to the sparsity of the distribution
(weights) over the sample space.

2A smooth distribution is a distribution that does not put too much weight on any single sample or in other
words  a distribution emulated by the booster does not dramatically diverge from the target distribution [6  7].

1

loss function can generate such a customized distribution? Moreover  does it result in a provable
boosting algorithm? In general  how can we characterize the accuracy of the ﬁnal hypothesis?

Although  to be fair  the so-called loss function hunting approach has given rise to useful boosting
algorithms such as LogitBoost  FilterBoost  GiniBoost and MadaBoost [5  8  9  10] which (to some
extent) answer some of the above questions  it is an inﬂexible and relatively unsuccessful approach
to addressing the boosting problems with distribution constraints.

Another approach to designing a boosting algorithm is to directly follow the WTSL viewpoint
[11  6  12]. The immediate advantages of such an approach are  ﬁrst  the resultant algorithms
are provable boosting algorithms  i.e.  they output a hypothesis of arbitrary accuracy. Second  the
booster has direct control over the weights  making it more suitable for boosting problems subject to
some distribution constraints. However  since the WTSL view does not offer any algorithmic frame-
work (as opposed to the CWGD view)  it is rather difﬁcult to come up with a distribution update
mechanism resulting in a provable boosting algorithm. There are  however  a few useful  and al-
beit fairly limited  algorithmic frameworks such as TotalBoost [13] that can be used to derive other
provable boosting algorithms. The TotalBoost algorithm can maximize the margin by iteratively
solving a convex problem with the totally corrective constraint. A more general family of boost-
ing algorithms was later proposed by Shalev-Shwartz et. al. [14]  where it was shown that weak
learnability and linear separability are equivalent  a result following from von Neumann’s minmax
theorem. Using this theorem  they constructed a family of algorithms that maintain smooth distribu-
tions over the sample space  and consequently are noise tolerant. Their proposed algorithms ﬁnd an
(1− ǫ)-accurate solution after performing at most O(log(N )/ǫ2) iterations  where N is the number
of training examples.

1.1 Our Results

We present a family of boosting algorithms that can be derived from well-known online learning
algorithms  including projected gradient descent [15] and its generalization  mirror descent (both
active and lazy updates  see [16]) and composite objective mirror descent (COMID) [17]. We prove
the PAC learnability of the algorithms derived from this framework and we show that this framework
in fact generates maximum margin algorithms. That is  given a desired accuracy level ν  it outputs a
hypothesis of margin γmin − ν with γmin being the minimum edge that the weak classiﬁer guarantees
to return.

The duality between (linear) online learning and boosting is by no means new. This duality was ﬁrst
pointed out in [2] and was later elaborated and formalized by using the von Neumann’s minmax
theorem [18]. Following this line  we provide several proof techniques required to show the PAC
learnability of the derived boosting algorithms. These techniques are fairly versatile and can be used
to translate many other online learning methods into our boosting framework. To motivate our boost-
ing framework  we derive two practically and theoretically interesting algorithms: (I) SparseBoost
algorithm which by maintaining a sparse distribution over the sample space tries to reduce the space
and the computation complexity. In fact this problem  i.e.  applying batch boosting on the successive
subsets of data when there is not sufﬁcient memory to store an entire dataset  was ﬁrst discussed by
Breiman in [19]  though no algorithm with theoretical guarantee was suggested. SparseBoost is the
ﬁrst provable batch booster that can (partially) address this problem. By analyzing this algorithm 
we show that the tuning parameter of the regularization term ℓ1 at each round t should not exceed
γt
2 ηt to still have a boosting algorithm  where ηt is the coefﬁcient of the tth weak hypothesis and γt
is its edge. Exploiting sparsity in example domain has also been investigated in [20] by Hatano and
Takimoto. (II) A smooth boosting algorithm that requires only O(log 1/ǫ) number of rounds to learn
a (1 − ǫ)-accurate hypothesis. This algorithm can also be seen as an agnostic boosting algorithm3
due to the fact that smooth distributions provide a theoretical guarantee for noise tolerance in various
noisy learning settings  such as agnostic boosting [22  23].

Furthermore  we provide an interesting theoretical result about MadaBoost [10]. We give a proof
(to the best of our knowledge the only available unconditional proof) for the boosting property of
(a variant of) MadaBoost and show that  unlike the common presumption  its convergence rate is of
O(1/ǫ2) rather than O(1/ǫ).

3Unlike the PAC model  the agnostic learning model allows an arbitrary target function (labeling function)

that may not belong to the class studied  and hence  can be viewed as a noise tolerant learning model [21].

2

Finally  we show our proof technique can be employed to generalize some of the known online
learning algorithms. Speciﬁcally  consider the Lazy update variant of the online Mirror Descent
(LMD) algorithm (see for instance [16]). The standard proof to show that the LMD update scheme
achieves vanishing regret bound is through showing its equivalence to the FTRL algorithm [16] in
the case that they are both linearized  i.e.  the cost function is linear. However  this indirect proof is
fairly restrictive when it comes to generalizing the LMD-type algorithms. Here  we present a direct
proof for it  which can be easily adopted to generalize the LMD-type algorithms.

2 Preliminaries

Let {(xi  ai)}  1 ≤ i ≤ N   be N training samples  where xi ∈ X and ai ∈ {−1  +1}. Assume
h ∈ H is a real-valued function mapping X into [−1  1]. Denote a distribution over the training data
by w = [w1  . . .   wN ]⊤ and deﬁne a loss vector d = [−a1h(x1)  . . .   −aN h(xN )]⊤. We deﬁne
γ = −w⊤d as the edge of the hypothesis h under the distribution w and it is assumed to be positive
when h is returned by a weak learner. In this paper we do not consider the branching program based
boosters and adhere to the typical boosting protocol (described in Section 1).

Since a central notion throughout this paper is that of Bregman divergences  we brieﬂy revisit some
of their properties. A Bregman divergence is deﬁned with respect to a convex function R as

BR(x  y) = R(x) − R(y) − ∇R(y)(x − y)⊤

(1)

and can be interpreted as a distance measure between x and y. Due to the convexity of R  a
Bregman divergence is always non-negative  i.e.  BR(x  y) ≥ 0. In this work we consider R to
be a β-strongly convex function4 with respect to a norm ||.||. With this choice of R  the Bregman
divergence BR(x  y) ≥ β
2 x⊤x (which is 1-strongly convex
with respect to ||.||2)  then BR(x  y) = 1
2 is the Euclidean distance. Another example
is the negative entropy function R(x) = PN
i=1 xi log xi (resulting in the KL-divergence) which is

known to be 1-strongly convex over the probability simplex with respect to ℓ1 norm.

2 ||x − y||2. As an example  if R(x) = 1

2 ||x − y||2

The Bregman projection is another fundamental concept of our framework.

Deﬁnition 1 (Bregman Projection). The Bregman projection of a vector y onto a convex set S with
respect to a Bregman divergence BR is

ΠS(y) = arg min

x∈S

BR(x  y)

(2)

Moreover  the following generalized Pythagorean theorem holds for Bregman projections.

Lemma 1 (Generalized Pythagorean) [24  Lemma 11.3]. Given a point y ∈ RN   a convex set S
and ˆy = ΠS(y) as the Bregman projection of y onto S  for all x ∈ S we have

(3)

(4)

(5)

Exact:

Relaxed:

BR(x  y) ≥ BR(x  ˆy) + BR(ˆy  y)
BR(x  y) ≥ BR(x  ˆy)

The relaxed version follows from the fact that BR(ˆy  y) ≥ 0 and thus can be ignored.
Lemma 2. For any vectors x  y  z  we have

(x − y)⊤(∇R(z) − ∇R(y)) = BR(x  y) − BR(x  z) + BR(y  z)

The above lemma follows directly from the Bregman divergence deﬁnition in (1). Additionally  the
following deﬁnitions from convex analysis are useful throughout the paper.

Deﬁnition 2 (Norm & dual norm). Let ||.||A be a norm. Then its dual norm is deﬁned as

(6)
For instance  the dual norm of ||.||2 = ℓ2 is ||.||2∗ = ℓ2 norm and the dual norm of ℓ1 is ℓ∞ norm.
Further 

||y||A∗ = sup{y⊤x  ||x||A ≤ 1}

Lemma 3. For any vectors x  y and any norm ||.||A  the following inequality holds:

x⊤y ≤ ||x||A||y||A∗ ≤

1
2

||x||2

A +

1
2

||y||2

A∗

(7)

4That is  its second derivative (Hessian in higher dimensions) is bounded away from zero by at least β.

3

Throughout this paper  we use the shorthands ||.||A = ||.|| and ||.||A∗ = ||.||∗ for the norm and its
dual  respectively.

Finally  before continuing  we establish our notations. Vectors are lower case bold letters and their
entries are non-bold letters with subscripts  such as xi of x  or non-bold letter with superscripts if the
vector already has a subscript  such as xi
t of xt. Moreover  an N-dimensional probability simplex is
i=1 wi = 1  wi ≥ 0}. The proofs of the theorems and the lemmas can be

denoted by S = {w|PN

found in the Supplement.

3 Boosting Framework

Let R(x) be a 1-strongly convex function with respect to a norm ||.|| and denote its as-
sociated Bregman divergence BR. Moreover 
the dual norm of a loss vector dt
for dt as deﬁned
It
be upper bounded 
in MABoost  L = 1 when ||.||∗ = ℓ∞ and L = N when ||.||∗ = ℓ2.
The
following Mirror Ascent Boosting (MABoost) algorithm is our boosting framework.

||dt||∗ ≤ L.

is easy to verify that

i.e. 

let

Algorithm 1: Mirror Ascent Boosting (MABoost)

Input: R(x) 1-strongly convex function  w1 = [ 1

N   . . .   1

N ]⊤ and z1 = [ 1

N   . . .   1

N ]⊤

For t = 1  . . .   T do

(a) Train classiﬁer with wt and get ht  let dt = [−a1ht(x1)  . . .   −aN ht(xN )]

and γt = −w⊤t dt.

(b) Set ηt = γt
L

(c) Update weights:

(d) Project onto S:

End

∇R(zt+1) = ∇R(zt) + ηtdt
∇R(zt+1) = ∇R(wt) + ηtdt
BR(w  zt+1)

wt+1 = argmin

w∈S

(lazy update)

(active update)

Output: The ﬁnal hypothesis f (x) = sign(cid:18)PT

t=1 ηtht(x)(cid:19).

This algorithm is a variant of the mirror descent algorithm [16]  modiﬁed to work as a boosting
algorithm. The basic principle in this algorithm is quite clear. As in ADABoost  the weight of
a wrongly (correctly) classiﬁed sample increases (decreases). The weight vector is then projected
onto the probability simplex in order to keep the weight sum equal to 1. The distinction between
the active and lazy update versions and the fact that the algorithm may behave quite differently
under different update strategies should be emphasized. In the lazy update version  the norm of the
auxiliary variable zt is unbounded which makes the lazy update inappropriate in some situations.
In the active update version  on the other hand  the algorithm always needs to access (compute) the
previous projected weight wt to update the weight at round t and this may not be possible in some
applications (such as boosting-by-ﬁltering).

Due to the duality between online learning and boosting  it is not surprising that MABoost (both
the active and lazy versions) is a boosting algorithm. The proof of its boosting property  however 
reveals some interesting properties which enables us to generalize the MABoost framework. In the
following  only the proof of the active update is given and the lazy update is left to Section 3.4.

Theorem 1. Suppose that MABoost generates weak hypotheses h1  . . .   hT whose edges are
γ1  . . .   γT . Then the error ǫ of the combined hypothesis f on the training set is bounded as:

R(w) =

1
2

||w||2

2 :

R(w) =

N

Xi=1

wi log wi :

ǫ ≤

1 +PT
ǫ ≤ e− PT

t=1

1
t=1 γ2
t

1

2 γ 2

t

(8)

(9)

In fact  the ﬁrst bound (8) holds for any 1-strongly convex R  though for some R (e.g.  negative
entropy) a much tighter bound as in (9) can be achieved.

4

Proof : Assume w∗ = [w∗1  . . .   w∗N ]⊤ is a distribution vector where w∗i = 1
N ǫ if f (xi) 6= ai 
and 0 otherwise. w∗ can be seen as a uniform distribution over the wrongly classiﬁed samples by
the ensemble hypothesis f . Using this vector and following the approach in [16]  we derive the
t ] is a loss vector as deﬁned in

t=1 ηt(w∗⊤dt−w⊤t dt) where dt = [d1

t   . . .  dN

upper bound of PT

Algorithm 1.

(w∗ − wt)⊤ηtdt = (w∗ − wt)⊤(cid:0)∇R(zt+1) − ∇R(wt)(cid:1)

= BR(w∗  wt) − BR(w∗  zt+1) + BR(wt  zt+1)
≤ BR(w∗  wt) − BR(w∗  wt+1) + BR(wt  zt+1)

(10c)
where the ﬁrst equation follows Lemma 2 and inequality (10c) results from the relaxed version of
Lemma 1. Note that Lemma 1 can be applied here because w∗ ∈ S.
Further  the BR(wt  zt+1) term is bounded. By applying Lemma 3

(10a)

(10b)

BR(wt  zt+1) + BR(zt+1  wt) = (zt+1 − wt)⊤ηtdt ≤

η2
t ||dt||2
∗
2 ||zt+1 − wt||2 due to the 1-strongly convexity of R  we have

||zt+1 − wt||2 +

and since BR(zt+1  wt) ≥ 1

1
2

1
2

(11)

(12)

(13)

BR(wt  zt+1) ≤

1
2

t ||dt||2
η2
∗

Now  replacing (12) into (10c) and summing it up from t = 1 to T   yields

T

Xt=1

w∗⊤ηtdt− w⊤t ηtdt ≤

T

Xt=1

1
2

η2
t ||dt||2

∗ + BR(w∗  w1) − BR(w∗  wT +1)

Moreover  it is evident from the algorithm description that for mistakenly classiﬁed samples

ηtdi

−aif (xi) = −aisign(cid:18) T
Xt=1

ηtht(xi)(cid:19) = sign(cid:18) T
Xt=1
Following (14)  the ﬁrst term in (13) will be w∗⊤PT
t=1 ηtdt ≥ 0 and thus  can be ignored. More-
t=1 −w⊤t ηtdt = PT
over  by the deﬁnition of γ  the second term is PT
t=1 ηtγt. Putting all these
Xt=1

∗ with its upper bound L  yields
η2
t −

t(cid:19) ≥ 0 ∀xi ∈ {x|f (xi) 6= ai}

together  ignoring the last term in (13) and replacing ||dt||2

−BR(w∗  w1) ≤ L

Xt=1

ηtγt

(15)

(14)

1
2

T

T

Replacing the left side with −BR = − 1
2N ǫ for the case of quadratic R  and with
−BR = log(ǫ) when R is a negative entropy function  taking the derivative w.r.t ηt and equating
it to zero (which yields ηt = γt
L ) we achieve the error bounds in (8) and (9). Note that in the case
of R being the negative entropy function  Algorithm 1 degenerates into ADABoost with a different
choice of ηt.

2 ||w∗ − w1||2 = ǫ−1

Before continuing our discussion  it is important to mention that the cornerstone concept of the
proof is the choice of w∗. For instance  a different choice of w∗ results in the following max-margin
theorem.
Theorem 2. Setting ηt = γt
L√t
is a desired accuracy level and tends to zero in O( log T√T
Observations: Two observations follow immediately from the proof of Theorem 1. First  the re-
quirement of using Lemma 1 is w∗ ∈ S  so in the case of projecting onto a smaller convex set
Sk ⊆ S  as long as w∗ ∈ Sk holds  the proof is intact. Second  only the relaxed version of Lemma 1
is required in the proof (to obtain inequality (10c)). Hence  if there is an approximate projection

  MABoost outputs a hypothesis of margin at least γmin − ν  where ν

) rounds of boosting.

operator ˆΠS that satisﬁes the inequality BR(w∗  zt+1) ≥ BR(cid:0)w∗  ˆΠS(zt+1)(cid:1)  it can be substituted
for the exact projection operator ΠS and the active update version of the algorithm still works. A
practical approximate operator of this type can be obtained through a double-projection strategy.

Lemma 4. Consider the convex sets K and S  where S ⊆ K. Then for any x ∈ S and y ∈ RN  

ˆΠS(y) = ΠS(cid:16)ΠK(y)(cid:17) is an approximate projection that satisﬁes BR(x  y) ≥ BR(cid:0)x  ˆΠS (y)(cid:1).

These observations are employed to generalize Algorithm 1. However  we want to emphasis that the
approximate Bregman projection is only valid for the active update version of MABoost.

5

3.1 Smooth Boosting

Let k > 0 be a smoothness parameter. A distribution w is smooth w.r.t a given distribution D if
wi ≤ kDi for all 1 ≤ i ≤ N . Here  we consider the smoothness w.r.t to the uniform distribution 
i.e.  Di = 1
N . Then  given a desired smoothness parameter k  we require a boosting algorithm
that only constructs distributions w such that wi ≤ k
k )-
accurate hypothesis. To this end  we only need to replace the probability simplex S with Sk =
N } in MABoost to obtain a smooth distribution boosting algorithm 

N   while guaranteeing to output a (1 − 1

i=1 wi = 1  0 ≤ wi ≤ k

called smooth-MABoost. That is  the update rule is: wt+1 = argmin
w∈Sk

BR(w  zt+1).

{w|PN

Note that the proof of Theorem 1 holds for smooth-MABoost  as well. As long as ǫ ≥ 1
k   the error
N ǫ ≤ k
distribution w∗ (w∗i = 1
N ǫ if f (xi) 6= ai  and 0 otherwise) is in Sk because 1
N . Thus  based
on the ﬁrst observation  the error bounds achieved in Theorem 1 hold for ǫ ≥ 1
k . In particular  ǫ = 1
k
is reached after a ﬁnite number of iterations. This projection problem has already appeared in the
literature. An entropic projection algorithm (R is negative entropy)  for instance  was proposed
in [14]. Using negative entropy and their suggested projection algorithm results in a fast smooth
boosting algorithm with the following convergence rate.

Theorem 3. Given R(w) = PN

accurate hypothesis in O(log( 1

ǫ )/γ2) of iterations.

i=1 wi log wi and a desired ǫ  smooth-MABoost ﬁnds a (1 − ǫ)-

3.2 Combining Datasets

Let’s assume we have two sets of data. A primary dataset A and a secondary dataset B. The goal
is to train a classiﬁer that achieves (1 − ǫ) accuracy on A while limiting the error on dataset B to
ǫB ≤ 1
k . This scenario has many potential applications including transfer learning [25]  weighted
combination of datasets based on their noise level and emphasizing on a particular region of a sam-
ple space as a problem requirement (e.g.  a medical diagnostic test that should not make a wrong
diagnosis when the sample is a pregnant woman). To address this problem  we only need to replace
N ∀i ∈ B} where i ∈ A
shorthands the indices of samples in A. By generating smooth distributions on B  this algorithm
limits the weight of the secondary dataset  which intuitively results in limiting its effect on the ﬁnal
hypothesis. The proof of its boosting property is quite similar to Theorem 1 (see supplement).

S in MABoost with Sc = {w|PN

i=1 wi = 1  0 ≤ wi ∀i ∈ A ∧ 0 ≤ wi ≤ k

3.3 Sparse Boosting

2 ||w||2

Let R(w) = 1
2. Since in this case the projection onto the simplex is in fact an ℓ1-constrained
optimization problem  it is plausible that some of the weights are zero (sparse distribution)  which
is already a useful observation. To promote the sparsity of the weight vector  we want to directly
regularize the projection with the ℓ1 norm  i.e.  adding ||w||1 to the objective function in the pro-
jection step. It is  however  not possible in MABoost  since ||w||1 is trivially constant on the sim-
plex. Therefore  we split the projection step into two consecutive steps. The ﬁrst projection is onto
RN

+ = {y | 0 ≤ yi}.

Surprisingly  projection onto RN
+ implicitly regularizes the weights of the correctly classiﬁed sam-
ples with a weighted ℓ1 norm term (see supplement). To further enhance sparsity  we may introduce
an explicit ℓ1 norm regularization term into the projection step with a regularization factor denoted
by αtηt. The solution of the projection step is then normalized to get a feasible point on the prob-
ability simplex. This algorithm is listed in Algorithm 2. αtηt is the regularization factor of the
explicit ℓ1 norm at round t. Note that the dominant regularization factor is ηtdi
t which only pushes
the weights of the correctly classiﬁed samples to zero .i.e.  when di
t < 0. This can become evident
by substituting the update step in the projection step for zt+1.
For simplicity we consider two cases: when αt = min(1  1
theorem bounds the training error.

2 γt||yt||1)and when αt = 0. The following

Theorem 4. Suppose that SparseBoost generates weak hypotheses h1  . . .   hT whose edges are
γ1  . . .   γT . Then the error ǫ of the combined hypothesis f on the training set is bounded as follows:

ǫ ≤

1
t=1 γ2

t ||yt||2
1

1 + cPT

6

(16)

Note that this bound holds for any choice of α ∈ (cid:2)0  min(1  γt||yt||1)(cid:1). Particularly  in our two

cases constant c is 1 for αt = 0  and 1

4 when αt = min(1  1

2 γt||yt||1).

For αt = 0  the ℓ1 norm of the weights ||yt||1 can be bounded away from zero by 1
N (see sup-
plement). Thus  the error ǫ tends to zero by O( N 2
γ 2T ). That is  in this case Sparseboost is a
provable boosting algorithm. However  for αt 6= 0  the ℓ1 norm ||yt||1 may rapidly go to zero
which consequently results in a non-vanishing upper bound (as T increases) for the training error in
(16). In this case  it may not be possible to conclude that the algorithm is in fact a boosting algo-
rithm5. It is noteworthy that SparseBoost can be seen as a variant of the COMID algorithm in [17].

Algorithm 2: SparseBoost

+ = {y | 0 ≤ yi}; Set y1 = [ 1

Let RN
At t = 1  . . .   T   train ht  set (ηt = γt||yt||1
update

N   . . .   1

N ]⊤;

N   αt = 0) or (ηt = γt||yt||1

2N   αt = 1

2 γt||yt||1)  and

||y − zt+1||2 + αtηt||y||1 → yi

t+1 = max(0  yi

t + ηtdi

t − αtηt)

zt+1 = yt + ηtdt
1
2

yt+1 = arg min
y∈RN
yt+1
i=1 yi
t

wt+1 =

+

PN

Output the ﬁnal hypothesis f (x) = sign(cid:18)PT

t=1 ηtht(x)(cid:19).

3.4 Lazy Update Boosting

In this section  we present the proof for the lazy update version of MABoost (LAMABoost) in
Theorem 1. The proof technique is novel and can be used to generalize several known online learning
algorithms such as OMDA in [26] and Meta algorithm in [27]. Moreover  we show that MadaBoost
[10] can be presented in the LAMABoost setting. This gives a simple proof for MadaBoost without
making the assumption that the edge sequence is monotonically decreasing (as in [10]).
Proof : Assume w∗ = [w∗1  . . .   w∗N ]⊤ is a distribution vector where w∗i = 1
otherwise. Then 
(w∗ − wt)⊤ηtdt = (wt+1 − wt)⊤(cid:0)∇R(zt+1) − ∇R(zt)(cid:1)

+ (zt+1 − wt+1)⊤(cid:0)∇R(zt+1) − ∇R(zt)(cid:1) + (w∗ − zt+1)⊤(cid:0)∇R(zt+1) − ∇R(zt)(cid:1)
∗ + BR(wt+1  zt+1) − BR(wt+1  zt) + BR(zt+1  zt)

N ǫ if f (xi) 6= ai  and 0

||wt+1 − wt||2 +

η2
t ||dt||2

1
2

1
2

≤
− BR(w∗  zt+1) + BR(w∗  zt) − BR(zt+1  zt)
≤
+ BR(wt+1  zt+1) − BR(wt  zt) − BR(w∗  zt+1) + BR(w∗  zt)

∗ − BR(wt+1  wt)

||wt+1 − wt||2 +

η2
t ||dt||2

1
2

1
2

where the ﬁrst inequality follows applying Lemma 3 to the ﬁrst term and Lemma 2 to the rest
of the terms and the second inequality is the result of applying the exact version of Lemma 1 to
BR(wt+1  zt). Moreover  since BR(wt+1  wt) − 1
2 ||wt+1 − wt||2 ≥ 0  they can be ignored in (17).
Summing up the inequality (17) from t = 1 to T   yields

−BR(w∗  z1) ≤ L

T

Xt=1

1
2

T

η2
t −

ηtγt

Xt=1
t=1 −w⊤t ηtdt = PT

where we used the facts that w∗⊤PT
inequality is exactly the same as (15)  and replacing −BR with ǫ−1

t=1 ηtdt ≥ 0 and PT

t=1 ηtγt. The above
N ǫ or log(ǫ) yields the same

5Nevertheless  for some choices of αt 6= 0 such as αt ∝ 1

t2   the boosting property of the algorithm is still

provable.

7

(17)

(18)

error bounds in Theorem 1. Note that  since the exact version of Lemma 1 is required to obtain
(17)  this proof does not reveal whether LAMABoost can be generalized to employ the double-
projection strategy. In some particular cases  however  we may show that a double-projection variant
of LAMABoost is still a provable boosting algorithm.

In the following  we brieﬂy show that MadaBoost can be seen as a double-projection LAMABoost.

Algorithm 3: Variant of MadaBoost

Let R(w) be the negative entropy and K a unit hypercube; Set z1 = [1  . . .   1]⊤;

At t = 1  . . .   T   train ht with wt  set ft(x) = sign(cid:18)Pt
ǫt = PN

  set ηt = ǫtγt and update

1
2 |ft(xi) − ai|

i=1

N

t′=1 ηt′ ht′(x)(cid:19) and calculate

∇R(zt+1) = ∇R(zt) + ηtdt
BR(y  zt+1)

yt+1 = arg min

y∈K

→ zi
→ yi

teηtdi

t+1 = zi
t+1 = min(1  zi

t

t+1)

→ wi

t+1 =

yi
t+1

||yt+1||1

wt+1 = arg min

BR(w  yt+1)
Output the ﬁnal hypothesis f (x) = sign(cid:18)PT

w∈S

t=1 ηtht(x)(cid:19).

Algorithm 3 is essentially MadaBoost  only with a different choice of ηt. It is well-known that the
entropy projection onto the probability simplex results in the normalization and thus  the second
projection of Algorithm 3. The entropy projection onto the unit hypercube  however  maybe less
known and thus  its proof is given in the Supplement.

Theorem 5. Algorithm 3 yields a (1− ǫ)-accurate hypothesis after at most T = O(

1

ǫ2γ2 ).

This is an important result since it shows that MadaBoost seems  at least in theory  to be slower than
what we hoped  namely O( 1

ǫγ2 ).

4 Conclusion and Discussion

In this work  we provided a boosting framework that can produce provable boosting algorithms.
This framework is mainly suitable for designing boosting algorithms with distribution constraints.
A sparse boosting algorithm that samples only a fraction of examples at each round was derived
from this framework. However  since our proposed algorithm cannot control the exact number of
zeros in the weight vector  a natural extension to this algorithm is to develop a boosting algorithm
that receives the sparsity level as an input. However  this immediately raises the question: what is
the maximum number of examples that can be removed at each round from the dataset  while still
achieving a (1− ǫ)-accurate hypothesis?

The boosting framework derived in this work is essentially the dual of the online mirror descent
algorithm. This framework can be generalized in different ways. Here  we showed that replacing the
Bregman projection step with the double-projection strategy  or as we call it approximate Bregman
projection  still results in a boosting algorithm in the active version of MABoost  though this may
not hold for the lazy version. In some special cases (MadaBoost for instance)  however  it can be
shown that this double-projection strategy works for the lazy version as well. Our conjecture is that
under some conditions on the ﬁrst convex set  the lazy version can also be generalized to work with
the approximate projection operator. Finally  we provided a new error bound for the MadaBoost
algorithm that does not depend on any assumption. Unlike the common conjecture  the convergence
rate of MadaBoost (at least with our choice of η) is of O(1/ǫ2).

Acknowledgments

This work was partially supported by SNSF. We would like to thank Professor Rocco Servedio for
an inspiring email conversation and our colleague Hui Liang for his helpful comments.

8

References

[1] R. E. Schapire. The strength of weak learnability. Journal of Machine Learning Research  1990.

[2] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Computer and System Sciences  1997.

[3] L. Breiman. Prediction games and arcing algorithms. Neural Computation  1999.

[4] L. Mason  J. Baxter  P. Bartlett  and M. Frean. Boosting algorithms as gradient descent. In NIPS  1999.

[5] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: a statistical view of boosting.

Annals of Statistics  1998.

[6] R. A. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning

Research  2003.

[7] D. Gavinsky. Optimally-smooth adaptive boosting and application to agnostic learning. Journal of Ma-

chine Learning Research  2003.

[8] J. K. Bradley and R. E. Schapire. Filterboost: Regression and classiﬁcation on large datasets. In NIPS.

2008.

[9] K. Hatano. Smooth boosting using an information-based criterion. In Algorithmic Learning Theory. 2006.

[10] C. Domingo and O. Watanabe. Madaboost: A modiﬁcation of AdaBoost. In COLT  2000.

[11] Y. Freund. Boosting a weak learning algorithm by majority. Journal of Information and Computation 

1995.

[12] N. H. Bshouty  D. Gavinsky  and M. Long. On boosting with polynomially bounded distributions. Journal

of Machine Learning Research  2002.

[13] M. K. Warmuth  J. Liao  and G. R¨atsch. Totally corrective boosting algorithms that maximize the margin.

In ICML  2006.

[14] S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: new

relaxations and efﬁcient boosting algorithms. In COLT  2008.

[15] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML  2003.

[16] E. Hazan. A survey: The convex optimization approach to regret minimization. Working draft  2009.

[17] J. C. Duchi  S. Shalev-shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent. In COLT 

2010.

[18] Y. Freund and R. E. Schapire. Game theory  on-line prediction and boosting. In COLT  1996.

[19] L. Breiman. Pasting bites together for prediction in large data sets and on-line. Technical report  Dept.

Statistics  Univ. California  Berkeley  1997.

[20] K. Hatano and E. Takimoto. Linear programming boosting by column and row generation. In Proceedings

of Discovery Science  2009.

[21] M. J. Kearns  R. E. Schapire  and L. M. Sellie. Toward efﬁcient agnostic learning. In COLT  1992.

[22] A. Kalai and V. Kanade. Potential-based agnostic boosting. In NIPS. 2009.

[23] S. Ben-David  P. Long  and Y. Mansour. Agnostic boosting. In COLT. 2001.

[24] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University Press  2006.

[25] W. Dai  Q. Yang  G. Xue  and Y. Yong. Boosting for transfer learning. In ICML  2007.

[26] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In COLT  2013.

[27] C. Chiang  T. Yang  C. Lee  M. Mahdavi  C. Lu  R. Jin  and S. Zhu. Online optimization with gradual

variations. In COLT  2012.

9

,Tofigh Naghibi Mohamadpoor
Beat Pfister
Kyle Ulrich
Kafui Dzirasa
Lawrence Carin