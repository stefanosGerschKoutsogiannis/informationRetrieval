2009,Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes,We provide some insights into how task correlations in multi-task Gaussian process (GP) regression affect the generalization error and the learning curve.  We analyze the asymmetric two-task case  where a secondary task is to help the learning of a primary task. Within this setting  we give bounds on the generalization error and the learning curve of the primary task. Our approach admits intuitive understandings of the multi-task GP by relating it to single-task GPs. For the case of one-dimensional input-space under optimal sampling with data only for the secondary task  the limitations of multi-task GP can be quantified explicitly.,Generalization Errors and Learning Curves for
Regression with Multi-task Gaussian Processes

Kian Ming A. Chai

School of Informatics  University of Edinburgh 
10 Crichton Street  Edinburgh EH8 9AB  UK

k.m.a.chai@ed.ac.uk

Abstract

We provide some insights into how task correlations in multi-task Gaussian pro-
cess (GP) regression affect the generalization error and the learning curve. We
analyze the asymmetric two-tasks case  where a secondary task is to help the learn-
ing of a primary task. Within this setting  we give bounds on the generalization
error and the learning curve of the primary task. Our approach admits intuitive
understandings of the multi-task GP by relating it to single-task GPs. For the
case of one-dimensional input-space under optimal sampling with data only for
the secondary task  the limitations of multi-task GP can be quantiﬁed explicitly.

1 Introduction

Gaussian processes (GPs) (see e.g.  [1]) have been applied to many practical problems. In recent
years  a number of models for multi-task learning with GPs have been proposed to allow different
tasks to leverage on one another [2–5]. While it is generally assumed that learning multiple tasks
together is beneﬁcial  we are not aware of any work that quantiﬁes such beneﬁts  other than PAC-
based theoretical analysis for multi-task learning [6–8]. Following the tradition of the theoretical
works on GPs in machine learning  our goal is to quantify the beneﬁts using average-case analysis.
We concentrate on the asymmetric two-tasks case  where the secondary task is to help the learning
of the primary task. Within this setting  the main parameters are (1) the degree of “relatedness” ρ
between the two tasks  and (2) the ratio πS of total training data for the secondary task. While higher
|ρ| and lower πS is clearly more beneﬁcial to the primary task  the extent and manner that this is
so has not been clear. To address this  we measure the beneﬁts using generalization error  learning
curve and optimal error  and investigate the inﬂuence of ρ and πS on these quantities.
We will give non-trivial lower and upper bounds on the generalization error and the learning curve.
Both types of bounds are important in providing assurance on the quality of predictions: an upper
bound provides an estimate of the amount of training data needed to attain a minimum performance
level  while a lower bound provides an understanding of the limitations of the model [9]. Our
approach relates multi-task GPs to single-task GPs and admits intuitive understandings of multi-task
GPs. For one-dimensional input-space under optimal sampling with data only for the secondary task 
we show the limit to which error for the primary task can be reduced. This dispels any misconception
that abundant data for the secondary task can remedy no data for the primary task.

2 Preliminaries and problem statement

2.1 Multi-task GP regression model and setup
The multi-task Gaussian process regression model in [5] learns M related functions {fm}M
m=1 by
placing a zero mean GP prior which directly induces correlations between tasks. Let ym be an

1

observation of the mth function at x. Then the model is given by

ym ∼ N (fm(x)  σ2

(cid:104)fm(x)fm(cid:48)(x(cid:48))(cid:105) def= K f

mm(cid:48) kx(x  x(cid:48))

m) 

m is the noise variance for the mth task.

(1)
where kx is a covariance function over inputs  and K f is a positive semi-deﬁnite matrix of inter-task
similarities  and σ2
The current focus is on the two tasks case  where the secondary task S is to help improve the
performance of the primary task T ; this is the asymmetric multi-task learning as coined in [10]. We
ﬁx K f to be a correlation matrix  and let the variance be explained fully by kx (the converse has been
done in [5]). Thus K f is fully speciﬁed by the correlation ρ ∈ [−1  1] between the two tasks. We
further ﬁx the noise variances of the two tasks to be the same  say σ2
n. For the training data  there are
nT (resp. nS) observations at locations XT (resp. XS) for task T (resp. S). We use n def= nT + nS
for the total number of observations  πS
def= nS/n for the proportion of observations for task S  and
also X def= XT ∪ XS. The aim is to infer the noise-free response fT∗ for task T at x∗. See Figure 1.
The covariance matrix of the noisy training data is K(ρ) + σ2

nI  where

(cid:18) K x

(cid:19)

K(ρ) def=

ρK x
T T
ρK x
ST K x
SS

T S

;

(2)

T T (resp. K x

n  XT   XS) = k∗∗ − kT∗ (K(ρ) + σ2

T S is the matrix of cross-covariances from locations in XT to locations in XS; and K x

nI)−1k∗  where kT∗ def= (cid:0)(kx

SS) is the matrix of covariances (due to kx) between locations in XT (resp. XS);
ST is K x

and K x
K x
transposed. The posterior variance at x∗ for task T is
T (x∗  ρ  σ2
σ2
and k∗∗ is the prior variance at x∗  and kx
S∗) is the vector of covariances (due to kx)
between locations in XT (resp. XS) and x∗. Where appropriate and clear from context  we will
n  XT   XS)  or use X for (XT   XS). Note that
suppress some of the parameters in σ2
T (−1); for brevity  we only write the former.
T (ρ) = σ2
σ2
If the GP prior is correctly speciﬁed  then the posterior variance (3) is also the generalization error at
x∗ [1  §7.3]. The latter is deﬁned as (cid:104)(f (cid:63)
  where ¯fT (x∗) is the posterior mean
at x∗ for task T   and the expectation is taken over the distribution from which the true function f (cid:63)
T
is drawn. In this paper  in order to distinguish succinctly from the generalization error introduced
in the next section  we use posterior variance to mean the generalization error at x∗. Note that the
actual y-values observed at X do not effect the posterior variance at any test location.

T (x∗  ρ  σ2
T (1) is the same as σ2

S∗)T(cid:1) ; (3)

T (x∗) − ¯fT (x∗))2(cid:105)f (cid:63)

T (−ρ)  so that σ2

T∗ (resp. kx

T∗)T ρ(kx

T S

T

Problem statement Given the above setting  the aim is to investigate how training observations
for task S can beneﬁt the predictions for task T . We measure the beneﬁts using generalization error 
learning curve and optimal error  and investigate how these quantities vary with ρ and πS.

2.2 Generalization errors  learning curves and optimal errors
We outline the general approach to obtain the generalization error and the learning curve [1  §7.3]
under our setting  where we have two tasks and are concerned with the primary task T . Let p(x)
be the probability density  common to both tasks  from which test and training locations are drawn 
and assume that the GP prior is correctly speciﬁed. The generalization error for task T is obtained
by averaging the posterior variance for task T over x∗  and the learning curve for task T is obtained
by averaging the generalization error over training sets X:
generalization error:
learning curve:

(4)
(5)
where the training locations in X are drawn i.i.d  that is  p(X) factorizes completely into a product of
p(x)s. Besides averaging T to obtain the learning curve  one may also use the optimal experimental
design methodology and minimize T over X to ﬁnd the optimal generalization error [11  chap. II]:
(6)
Both T (0  σ2
n  XT   XS) reduce to single-task GP cases; the former discards
training observations at XS  while the latter includes them. Similar analogues to single-task GP
cases for avg
n  πS  n) can be
obtained. Note that avg

n  XT   XS) def= (cid:82) σ2
n  πS  n) def= (cid:82) T (ρ  σ2

T are well-deﬁned since πSn = nS ∈ N0 by the deﬁnition of πS.

n  πS  n) def= minX T (ρ  σ2

n  XT   XS) and T (1  σ2

n  XT   XS)p(x∗)dx∗

n  XT   XS)p(X)dX 

n  πS  n)  and opt

n  πS  n) and avg

n  πS  n) and opt

T and opt

n  XT   XS).

optimal error:

T (x∗  ρ  σ2

avg
T (ρ  σ2

opt
T (ρ  σ2

T (0  σ2

T (1  σ2

T (0  σ2

T (1  σ2

T (ρ  σ2

2

Task-space

ρ

S

T

kx(x  x(cid:48))

|XS| = nS

Input
space
|XT| = nT

(cid:126)

Figure 1: The two tasks S and T have
task correlation ρ. The data set XT (resp.
XS) for task T (resp. S) consists of the
•s (resp.
s). The test location x∗ for
task T is denoted by (cid:126).

2.3 Eigen-analysis

T (x∗)
σ2

ρ = 0

ρ = 1

x∗

Figure 2: The posterior variances of each test location
within [0  1] given data •s at 1/3 and 2/3 for task T   and
s at 1/5  1/2 and 4/5 for task S.

they satisfy the integral equation (cid:82) kx(x  x(cid:48))φi(x)p(x)dx = κiφi(x(cid:48)). Let

We now state known results of eigen-analysis used in this paper. Let ¯κ def= κ1 > κ2 > . . . and
φ1(·)  φ2(·)  . . . be the eigenvalues and eigenfunctions of the covariance function kx under the
measure p(x)dx:
¯λ def= λ1 > λ2 > . . . > λnS
SS. If the locations in XS are sampled
from p(x)  then κi = limnS→∞ λi/nS  i = 1 . . . nS; see e.g.  [1  §4.3.2] and [12  Theorem 3.4].
However  for ﬁnite nS used in practice  the estimate λi/nS for κi is better for the larger eigenvalues
than for the smaller ones. Additionally  in one-dimension with uniform p(x) on the unit interval 
if kx satisﬁes the Sacks-Ylvisaker conditions of order r  then κi ∝ (πi)−2r−2 in the limit i → ∞
[11  Proposition IV.10  Remark IV.2]. Broadly speaking  an order r process is exactly r times mean
square differentiable. For example  the stationary Ornstein-Uhlenbeck process is of order r = 0.

def= ¯λ be the eigenvalues of K x

3 Generalization error

In this section  we derive expressions for the generalization error (and the bounds thereon) for the
two-tasks case in terms of the single-task one. To illustrate and further motivate the problem  Fig-
T (x∗  ρ) as a function of x∗ given two observations for task T
ure 2 plots the posterior variance σ2
and three observations for task S. We roughly follow [13  Fig. 2]  and use squared exponential co-
n = 0.05. Six solid curves are plotted 
variance function with length-scale 0.11 and noise variance σ2
corresponding  from top to bottom  to ρ2 = 0  1/8  1/4  1/2  3/4 and 1. The two dashed curves en-
veloping each solid curve are the lower and upper bounds derived in this section; the dashed curves
are hardly visible because the bounds are rather tight. The dotted line is the prior noise variance.
T (x∗  ρ)
Similar to the case of single-task learning  each training point creates a depression on the σ2
surface [9  13]. However  while each training point for task T creates a “full” depression that reaches
the prior noise variance (horizontal dotted line at 0.05)  the depression created by each training
point for task S depends on ρ  “deeper” depressions for larger ρ2. From the ﬁgure  and also from
deﬁnition  it is clear that the following trivial bounds on σ2
T (x∗  ρ) (cid:54) σ2
Proposition 1. For all x∗  σ2
Integrating wrt to x∗ then gives the following corollary:
Corollary 2. T (1  σ2

n  XT   XS) (cid:54) T (ρ  σ2

T (x∗  1) (cid:54) σ2

n  XT   XS) (cid:54) T (0  σ2

n  XT   XS).

T (x∗  ρ) hold:

T (x∗  0).

Sections 3.2 and 3.3 derive lower and upper bounds that are tighter than the above trivial bounds.
Prior to the bounds  we consider a degenerate case to illustrate the limitations of multi-task learning.

3.1 The degenerate case of no training data for primary task
It is clear that if there is no training data for the secondary task  that is  if XS = ∅  then σ2
T (x∗  ρ) = σ2
σ2
primary task  that is  XT = ∅  we instead have the following proposition:

T (x∗1) =
T (x∗0) for all x∗ and ρ. In the converse case where there is no training data for the

3

00.20.40.60.8100.20.40.60.81Proposition 3. For all x∗  σ2
Proof.

T (x∗  ρ ∅  XS) = k∗∗ − ρ2(kx
σ2

T (x∗  ρ ∅  XS) = ρ2σ2
S∗)T(K x

= (1 − ρ2)k∗∗ + ρ2(cid:2)k∗∗ − (kx

T (x∗  1 ∅  XS) + (1 − ρ2)k∗∗.
SS + σ2

nI)−1kx
S∗
S∗)T(K x
SS + σ2
T (x∗  1 ∅  XS).

= (1 − ρ2)k∗∗ + ρ2σ2

nI)−1kx

S∗(cid:3)

limnS→∞ σ2

T (x∗  1 ∅  XS) = 0

=⇒ limnS→∞ σ2

T (x∗  ρ ∅  XS) = (1 − ρ2)k∗∗.

Hence the posterior variance is a weighted average of the prior variance k∗∗ and the posterior vari-
ance at perfect correlation. When the cardinality of XS increases under inﬁll asymptotics [14  §3.3] 
(7)
This is the limit for the posterior variance at any test location for task T   if one has training data only
for the secondary task S. This is because a correlation of ρ between the tasks prevents any training
location for task S from having correlation higher than ρ with a test location for task T . Suppose
correlations in the input-space are given by an isotropic covariance function kx(|x − x(cid:48)|). If we
translate correlations into distances between data locations  then any training location from task S
is beyond a certain radius from any test location for task T . In contrast  a training location from task
T may lay arbitrarily close to a test location for task T   subject to the constraints of noise.
We obtain the generalization error in this degenerate case  by integrating Proposition 3 wrt p(x∗)dx∗
and using the fact that the mean prior variance is given by the sum of the process eigenvalues.
Corollary 4. T (ρ  σ2

n ∅  XS) + (1 − ρ2)(cid:80)∞

n ∅  XS) = ρ2T (1  σ2

i=1 κi.

3.2 A lower bound
When XT (cid:54)= ∅  the correlations between locations in XT and locations in XS complicate the situa-
T (ρ) is a continuous and monotonically decreasing function of ρ  there exists
tion. However  since σ2
T (1) + (1 − α)σ2
an α ∈ [0  1]  which depends on ρ  x∗ and X  such that σ2
T (0). That
T (ρ) of the
α depends on x∗ obstructs further analysis. The next proposition gives a lower bound
¯σ2
T (1) (cid:54)
T (ρ)  where the mixing proportion is independent of x∗.
same form satisfying σ2
Proposition 5. Let
T (x∗  ρ) def= ρ2σ2
¯σ2

T (x∗  1) + (1 − ρ2)σ2

T (x∗  0). Then for all x∗:

T (ρ) (cid:54) σ2
¯σ2

T (ρ) = ασ2

T (x∗  ρ) (cid:54) σ2
T (x∗  ρ)
(a)
¯σ2
T (x∗  ρ) −
T (x∗  ρ) (cid:54) ρ2(σ2
(b) σ2
¯σ2
T (x∗  ρ) −

(c) arg maxρ2(cid:2)σ2

T (x∗  0) − σ2

T (x∗  ρ)(cid:3) (cid:62) 1/2.

¯σ2

T (x∗  1))

T (0).

T (1) and σ2

The proofs are in supplementary material §S.2. The lower bound
T (ρ) depends explicitly on ρ2.
¯σ2
It depends implicitly on πS  which is the proportion of observations for task S  through the gap
If there is no training data for the primary task  i.e.  if πS = 1  the
between σ2
bound reduces to Proposition 3  and becomes exact for all values of ρ. If πS = 0  the bound is also
exact. For πS (cid:54)∈ {0  1}  the bound is exact when ρ ∈ {−1  0  1}. As from Figure 2 and later from
our simulation results in section 5.3  this bound is rather tight. Part (b) of the proposition states the
T (0) and
tightness of the bound: it is no more than factor ρ2 of the gap between the trivial bounds σ2
T (1). Part (c) of the proposition says that the bound is least tight for a value of ρ2 greater than 1/2.
σ2
We provide an intuition on Proposition 5a. Let ¯f1 (resp. ¯f0) be the posterior mean of the single-task
GP when ρ = 1 (resp. ρ = 0). Contrasted with the multi-task predictor ¯fT   ¯f1 directly involves the
noisy observations for task T at XS  so it has more information on task T . Hence  predicting ¯f1(x∗)
T (ρ) is obtained by “throwing
gives the trivial lower bound σ2
¯σ2
away” information and predicting ¯f1(x∗) with probability ρ2 and ¯f0(x∗) with probability (1 − ρ2).
Finally  the next corollary is readily obtained from Proposition 5a by integrating wrt p(x∗)dx∗. This
is possible because ρ is independent of x∗.
Corollary 6. Let
¯T (ρ  σ2
3.3 An upper bound via equivalent isotropic noise at XS

n  XT   XS) + (1 − ρ2)T (0  σ2

n  XT   XS) (cid:54) T (ρ  σ2

n  XT   XS) def= ρ2T (1  σ2

T (ρ). The tighter bound

n  XT   XS). Then

n  XT   XS).

¯T (ρ  σ2

T (1) on σ2

The following question motivates our upper bound: if the training locations in XS had been observed
for task T rather than for task S  what is the variance ˜σ2
n of the equivalent isotropic noise at XS so

4

For any x∗ there is always a ˜σ2

T (x∗  1  σ2
σ2
n that satisﬁes the equation because the difference

n) = σ2

n  ˜σ2

n  σ2

n).

T (x∗ρ  σ2
n) − σ2

(9)

that the posterior variance remains the same? To answer this question  we ﬁrst reﬁne the deﬁnition
of σ2

(cid:17)(cid:105)−1
T (·) to include a different noise variance parameter s2 for the XS observations:
k∗;
T (·). The variance ˜σ2

nI 0
0 s2I
cf. (3). We may suppress the parameters x∗  XT and XS when writing σ2
the equivalent isotropic noise is a function of x∗ deﬁned by the equation

n  s2  XT   XS) def= k∗∗ − kT∗

T (x∗  ρ  σ2
σ2

(cid:16) σ2

(8)
n of

K(ρ) +

(cid:104)

n  σ2

n  σ2

∆(ρ  σ2

T (1  σ2

T (ρ  σ2

n for ˜σ2

T (x∗  1  σ2

T (x∗  ρ  σ2

T (x∗  ρ  σ2

n  s2) def= σ2

n  σ2
n) (cid:54) σ2

n that is independent of the choice of x∗: ∆(ρ  σ2

n  ¯σ2
n  which is the minimum possible ¯σ2
n) (cid:54) σ2
T (x∗  1  σ2

n  s2)
(10)
is a continuous and monotonically decreasing function of s2. To make progress  we seek an upper
n) (cid:54) 0 for all test locations. Of
bound ¯σ2
interest is the tight upper bound ¯¯σ2
n  given in the next proposition.
SS  β def= ρ−2 − 1 and ¯¯σ2
Proposition 7. Let ¯λ be the maximum eigenvalue of K x
n)+ σ2
n.
n  ¯¯σ2
n). The bound is tight in this sense: for any
Then for all x∗  σ2
n  if ∀x∗ σ2
n  ¯σ2
¯σ2
n).
Proof sketch. Matrix K(ρ) may be factorized as

(cid:19)
(cid:1) factors  we obtain
By using this factorization in the posterior variance (8) and taking out the(cid:0) I 0
where (kx∗)T def= (cid:0)(kx
(cid:19)
(cid:18)0
(cid:18)K x

(cid:18)I
S∗)T(cid:1) and
n  s2) = k∗∗ − (kx∗)T[Σ(ρ  σ2
(cid:19)

(cid:19)(cid:18)K x

n)  then ∀x∗ σ2

n  s2)]−1kx∗ 

(cid:19)(cid:18)I

def= β(¯λ+ σ2

K x
ρ−2K x

T (x∗  ρ  σ2

T (x∗  1  σ2

(cid:18)σ2

n) (cid:54) σ2

0
0 ρI

0
0 ρI

T (ρ  σ2
σ2

K(ρ) =

T T
K x
ST

n  ¯¯σ2

(cid:19)

n  ¯σ2

(11)

(12)

0 ρI

T S

SS

0

0

n

.

+

nI
0

ρ−2s2I

= Σ(1  σ2

n  s2) + β

0 K x

SS + s2I

.

T∗)T  (kx
K x
ρ−2K x

T S

T T
K x
ST

SS

Σ(ρ  σ2

n  s2) def=

n)  having data XS for task
The second expression for Σ makes clear that  in the terms of σ2
S is equivalent to an additional correlated noise at these observations for task T . This expression
motivates the question that began this section. Note that ρ−2 (cid:62) 1  and hence β (cid:62) 0.
The increase in posterior variance due to having XS at task S with noise variance σ2
having them at task T with noise variance s2 is given by ∆(ρ  σ2

T (ρ  σ2

n  σ2

n  s2) = (kx∗)T(cid:2)(Σ(1  σ2

∆(ρ  σ2

n rather than
n  s2)  which we may now write as
(13)
n  σ2
n) (cid:54) 0 for all test locations. In
n; details can be found in supplementary material

n  s2))−1 − (Σ(ρ  σ2
n  ¯σ2

n such that ∆(ρ  σ2
(cid:54) ¯σ2

n))−1(cid:3) kx∗.

n

n

n

(cid:54)

=σ2
n

def= β(

(cid:54) ¯σ2

(cid:54) ¯¯σ2

(cid:54) ˜σ2

n for ˜σ2
n) + σ2

nI. Analogously  the tight lower bound on ˜σ2

Recall that we seek an upper bound ¯σ2
def= β(¯λ + σ2
general  this requires ¯¯σ2
§S.3. The tightness ¯¯σ2
n
n is evident from the construction.
n  ¯¯σ2
n) is the tight upper bound because it inﬂates the noise (co)variance at XS
T (x∗  1  σ2
Intuitively  σ2
nI/ρ2) to ¯¯σ2
SS + σ2
just sufﬁciently  from (βK x
n is given
n. In summary  ρ−2σ2
n) + σ2
¯λ + σ2
n  where the ﬁrst inequality
by =σ2
n
n
is obtained by substituting in zero for
n. Hence observing XS at S is at most as “noisy” as
¯λ in =σ2
an additional β(¯λ + σ2
n) noise
variance. Since β decreases with |ρ|  the additional noise variances are smaller when |ρ| is larger 
i.e.  when the task S is more correlated with task T .
We give a description of how the above bounds scale with nS  using the results stated
in section 2.3.
Further-
if the covariance
more  for uniformly distributed inputs in the one-dimension unit interval 

function satisﬁes Sacks-Ylvisaker conditions of order r  then κnS = Θ(cid:0)(πnS)−2r−2(cid:1)  so that
¯λ = Θ(cid:0)(πnS)−2r−1(cid:1). Since ¯¯σ2
n + β Θ(cid:0)n−2r−1

(cid:1). For the upper bound ¯¯σ2

For large enough nS  we may write ¯λ ≈ nS ¯κ and

n) noise variance  and at least as “noisy” as an additional β(

¯λ ≈ nSκnS .
n = ρ−2σ2

and =σ2
with nS  the eigenvalues of K(1) scales with n  thus σ2
contrast the lower bound =σ2
even for moderate sizes nS. Therefore  the lower bound is not as useful as the upper bound.
Finally  if we reﬁne T as we have done for σ2
Corollary 8. Let ¯T (ρ  σ2

T in (8)  we obtain the following corollary:

¯λ  we have ¯¯σ2
n + β Θ(nS)
n  note that although it scales linearly
n  ¯¯σ2
def= nS/n. In
T (1  σ2
n) does not depend on πS

n is dominated by ρ−2σ2

n are linear in ¯λ and

T (1  σ2
n  so that σ2

n) depends on πS

n  XT   XS) def= T (1  σ2

n = ρ−2σ2

¯λ + σ2

n and =σ2

n  ¯¯σ2

n  =σ2

S

n  σ2
¯T (ρ  σ2

n  XT   XS) (cid:62) T (ρ  σ2

n  σ2

n  XT   XS). Then
n  XT   XS).

n  σ2

5

3.4 Exact computation of generalization error

T (ρ  σ2

The factorization of σ2
T expressed by (12) allows the generalization error to be computed exactly in
certain cases. We replace the quadratic form in (12) by matrix trace and then integrate out x∗ to give

n  XT   XS) = (cid:104)k∗∗(cid:105) − tr(cid:0)Σ−1(cid:104)kx∗(kx∗)T(cid:105)(cid:1) =(cid:80)∞

where Σ denotes Σ(ρ  σ2

def= (cid:82) kx(xp  x∗) kx(xq  x∗) p(x∗)dx∗ =(cid:80)∞

n)  the expectations are taken over x∗  and M is an n-by-n matrix with
When the eigenfunctions φi(·)s are not bounded  the inﬁnite-summation expression for Mpq is often
difﬁcult to use. Nevertheless  analytical results for Mpq are still possible in some cases using the
integral expression. An example is the case of the squared exponential covariance function with
normally distributed x  when the integrand is a product of three Gaussians.

i φi(xp)φi(xq)  where xp  xq ∈ X.

i=1 κi − tr(cid:0)Σ−1M(cid:1)  

i=1 κ2

n  σ2

Mpq

4 Optimal error for the degenerate case of no training data for primary task

If training examples are provided only for task S  then task T has the following optimal performance.
Proposition 9. Under optimal sampling on a 1-d space  if the covariance function satisﬁes Sacks-
Ylvisaker conditions of order r  then opt

) + (1 − ρ2)(cid:80)∞

T (ρ  σ2  1  n) = Θ(n

i=1 κi.

T (ρ  σ2  1  n) = ρ2opt

Proof. We obtain opt
i=1 κi by minimizing Corol-
lary 4 wrt XS. Under the same conditions as the proposition  the optimal generalization error using
the single-task GP decays with training set size n as Θ(n−(2r+1)/(2r+2)) [11  Proposition V.3]. Thus
ρ2opt

A directly corollary of the above result is that one cannot expect to do better than (1 − ρ2)(cid:80) κi on

−(2r+1)/(2r+2)
S

−(2r+1)/(2r+2)
S

n  1  n) = ρ2Θ(n

T (1  σ2

T (1  σ2

the average. As this is a lower bound  the same can be said for incorrectly speciﬁed GP priors.

) = Θ(n

).

−(2r+1)/(2r+2)
S

n  1  n) + (1 − ρ2)(cid:80)∞

5 Theoretical bounds on learning curve

T (1  σ2

T (ρ  σ2

n  πS  n) (cid:54) avg

n  πS  n) (cid:54) avg

Using the results from section 3  lower and upper bounds on the learning curve may be computed by
averaging over the choice of X using Monte Carlo approximation.1 For example  using Corollary 2
and integrating wrt p(X)dX gives the following trivial bounds on the learning curve:
Corollary 10. avg
The gap between the trivial bounds can be analyzed as follows. Recall that πSn ∈ N0 by deﬁnition 
n  πS  (1 − πS)n) = avg
so that avg
T (1  σ2
n  πS  n) is equivalent to
n  πS  n) scaled along the n-axis by the factor (1 − πS) ∈ [0  1]  and hence the gap between
avg
T (0  σ2
the trivial bounds becomes wider with πS.
In the rest of this section  we derive non-trivial theoretical bounds on the learning curve before
providing simulation results. Theoretical bounds are particularly attractive for high-dimensional
input-spaces  on which Monte Carlo approximation is harder.

n  πS  n). Therefore avg

n  πS  n).

T (0  σ2

T (0  σ2

T (1  σ2

5.1 Lower bound

For the single-task GP  a lower bound on its learning curve is σ2
n
shall call this the single-task OV bound. This lower bound can be combined with Corollary 6.

i=1 κi/(σ2

n + nκi) [15]. We

Proposition 11. avg

T (ρ  σ2

n  πS  n) (cid:62) ρ2σ2

κi

+ (1 − ρ2)σ2

n

(cid:80)∞

∞(cid:88)

i=1

with b1
i

def=

  with b0
i

def=

n

∞(cid:88)
∞(cid:88)
∞(cid:88)

i=1

i=1

i=1

n + nκi
σ2
b1
i κi
n + nκi
σ2
b0
i κi

 

n + (1 − πS)nκi
σ2

κi

n + (1 − πS)nκi
 
σ2
n + (1 − ρ2πS)nκi
σ2
n + (1 − πS)nκi
σ2
n + (1 − ρ2πS)nκi
σ2

n + nκi
σ2

 

.

or equivalently  avg

T (ρ  σ2

n  πS  n) (cid:62) σ2

n

or equivalently  avg

T (ρ  σ2

n  πS  n) (cid:62) σ2

n

1Approximate lower bounds are also possible  by combining Corollary 6 and approximations in  e.g.  [13].

6

n + nκi) for every i.

i with that of κi/(σ2

Proof sketch. To obtain the ﬁrst inequality  we integrate Corollary 6 wrt to p(X)dX  and apply the
single-task OV bound twice. For the second inequality  its ith summand is obtained by combining
the corresponding pair of ith summands in the ﬁrst inequality. The third inequality is obtained from
the second by swapping the denominator of b1
n  πS and n  denote the above bound by OVρ. Then OV0 and OV1 are both single task
For ﬁxed σ2
bounds. In particular  from Corollary 10  we have that the OV1 is a lower bound on avg
n  πS  n).
From the ﬁrst expression of the above proposition  it is clear from the “mixture” nature of the bound
that the two-tasks bound OVρ is always better than OV1. As ρ2 decreases  the two-tasks bound moves
towards the OV0; and as πS increases  the gap between OV0 and OV1 increases. In addition  the gap
is also larger for rougher processes  which are harder to learn. Therefore  the relative tightness of
OVρ over OV1 is more noticeable for lower ρ2  higher πS and rougher processes.
The second expression in the Proposition 11 is useful for comparing with the OV1. Each summand
for the two-tasks case is a factor b1
i of the corresponding summand for the single-task case. Since
i ∈ [1  (1 − ρ2πS)/(1 − πS)[   OVρ is more than OV1 by at most (1 − ρ2)πS/(1 − πS) times.
b1
Similarly  the third expression of the proposition is useful for comparing with OV0: each summand
i ∈ ](1 − ρ2πS)  1] of the corresponding single-task one.
for the the two-tasks case is a factor b0
Hence  OVρ is less than OV0 by up to ρ2πS times. In terms of the lower bound  this is the limit to
which multi-task learning can outperform the single-task learning that ignores the secondary task.

T (ρ  σ2

5.2 Upper bound using equivalent noise

An upper bound on the learning curve of a single-task GP is given in [16]. We shall refer to this
as the single-task FWO bound and combine it with the approach in section 3.3 to obtain an upper
on the learning curve of task T . Although the single-task FWO bound was derived for observations
with isotropic noise  with some modiﬁcations (see supplementary material §S.4)  the derivations are
still valid for observations with heteroscedastic and correlated noise. Below is a version of the FWO
bound that has yet to assume isotropic noise:
Theorem 12. ([16]  modiﬁed second part of Theorem 6) Consider a zero-mean GP with covari-
ance function kx(· ·)  and eigenvalues κi and eigenfunctions φi(·) under the measure p(x)dx;
and suppose that the noise (co)variances of the observations are given by γ2(· ·). For n ob-
servations {xi}n
def= kx(xi  xj) + γ2(xi  xj) and
Φij
i /ci  where
ci
independently from p(x).

def= φj(xi). Then the learning curve at n is upper-bounded by(cid:80)∞
def= (cid:10)(ΦTHΦ)ii

(cid:11) /n  and the expectation in ci is taken over the set of n input locations drawn

i=1 κi − n(cid:80)∞

let H and Φ be matrices such that Hij

i=1 κ2

i=1 

T (1  σ2

n  ¯¯σ2

(cid:110)(cid:2)(1 + βπ2

δ(xi ∈ XT )δ(xj ∈ XT ) δijσ2

so that  through the deﬁnition of ci in Theorem 12  we obtain

Unlike [16]  we do not assume that the noise variance γ2(xi  xj) is of the form σ2
of proceeding from the upper bound σ2
variance given by (12). Thus we set the observation noise (co)variance γ2(xi  xj) to

nδij. Instead
n)  we proceed directly from the exact posterior

(cid:3)  
n + δ(xi ∈ XS)δ(xj ∈ XS)(cid:2)βkx(xi  xj) + ρ−2δijσ2
(cid:111)
S)n/(1 + βπS) − 1(cid:3) κi +(cid:82) kx(x  x) [φi(x)]2 p(x)dx + σ2
n  πS  n) (cid:54)(cid:80)∞

i=1 κi − n(cid:80)∞

ci = (1 + βπS)
details are in the supplementary material §S.5. This leads to the following proposition:
Proposition 13. Let β def= ρ−2 − 1. Then  using the cis deﬁned in (15)  we have

bound is recovered. However  FWOρ with ρ = 0 gives the prior variance(cid:80) κi instead. A trivial

Denote the above upper bound by FWOρ. When ρ = ±1 or πS = 0  the single-task FWO upper
upper bound can be obtained using Corollary 10  by replacing n with (1 − πS)n in the single-task
FWO bound. The FWOρ bound is better than this trivial single-task bound for small n and high |ρ|.

(14)

;

(15)

n

n

avg
T (ρ  σ2

i=1 κ2

i /ci.

5.3 Comparing bounds by simulations of learning curve

We compare our bounds with simulated learning curves. We follow the third scenario in [13]: the in-
put space is one dimensional with Gaussian distribution N (0  1/12)  the covariance function is the

7

avg
T

avg
T

OVρ / (cid:104)(cid:104)T (ρ)(cid:105)(cid:105) / FWOρ
(cid:104)(cid:104)T (1)(cid:105)(cid:105) / (cid:104)(cid:104)T (0)(cid:105)(cid:105)
× (cid:104)(cid:104)
¯T (ρ)(cid:105)(cid:105)
(cid:52) (cid:104)(cid:104)¯T (ρ)(cid:105)(cid:105)

OVρ / (cid:104)(cid:104)T (ρ)(cid:105)(cid:105) / FWOρ
(cid:104)(cid:104)T (1)(cid:105)(cid:105) / (cid:104)(cid:104)T (0)(cid:105)(cid:105)
× (cid:104)(cid:104)
¯T (ρ)(cid:105)(cid:105)
(cid:52) (cid:104)(cid:104)¯T (ρ)(cid:105)(cid:105)

(a) ρ2 = 1/2  πS = 1/2

(b) ρ2 = 3/4  πS = 3/4

n

n

T against n
)  the theoretical lower/upper bounds
)  and the empirical lower/upper bounds using Corollaries 6/8 (×/ (cid:52)). The
)  the empirical trivial lower/upper bounds using Corollary

Figure 3: Comparison of various bounds for two settings of (ρ  πS). Each graph plots avg
and consists of the “true” multi-task learning curve (middle
of Propositions 11/13 (lower/upper
10 (lower/upper
thickness of the “true” multi-task learning curve reﬂects 95% conﬁdence interval.
unit variance squared exponential kx(x  x(cid:48)) = exp[−(x − x(cid:48))2/(2l2)] with length-scale l = 0.01 
n = 0.05  and the learning curves are computed for up to n = 300
the observation noise variance is σ2
training data points. When required  the average over x∗ is computed analytically (see section 3.4).
The empirical average over X def= XT ∪ XS  denoted by (cid:104)(cid:104)·(cid:105)(cid:105)  is computed over 100 randomly sam-
pled training sets. The process eigenvalues κis needed to compute the theoretical bounds are given
in [17]. Supplementary material §S.6 gives further details.
Learning curves for pairwise combinations of ρ2 ∈ {1/8  1/4  1/2  3/4} and πS ∈ {1/4  1/2  3/4}
are computed. We compare the following: (a) the “true” multi-task learning curve (cid:104)(cid:104)T (ρ)(cid:105)(cid:105) obtained
T (ρ) over x∗ and X; (b) the theoretical bounds OVρ and FWOρ of Propositions 11
by averaging σ2
and 13; (c) the trivial upper and lower bounds that are single-task learning curves (cid:104)(cid:104)T (0)(cid:105)(cid:105) and
(cid:104)(cid:104)T (1)(cid:105)(cid:105) obtained by averaging σ2
¯T (ρ)(cid:105)(cid:105) and
upper bound (cid:104)(cid:104)¯T (ρ)(cid:105)(cid:105) using Corollaries 6 and 8. Figure 3 gives some indicative plots of the curves.
We summarize with the following observations: (a) The gap between the trivial bounds (cid:104)(cid:104)T (0)(cid:105)(cid:105)
and (cid:104)(cid:104)T (1)(cid:105)(cid:105) increases with πS  as described at the start of section 5. (b) We ﬁnd the lower bound
(cid:104)(cid:104)
¯T (ρ)(cid:105)(cid:105) a rather close approximation to the multi-task learning curve (cid:104)(cid:104)T (ρ)(cid:105)(cid:105)  as evidenced by
the much overlap between the × lines and the middle
lines in Figure 3. (c) The curve for the
empirical upper bound (cid:104)(cid:104)¯T (ρ)(cid:105)(cid:105) using the equivalent noise method has jumps  e.g.  the (cid:52) lines in
Figure 3  because the equivalent noise variance ¯¯σ2
n increases whenever a datum for XS is sampled.
(d) For small n  (cid:104)(cid:104)T (ρ)(cid:105)(cid:105) is closer to FWOρ  but becomes closer to OVρ as n increases  as shown by
the unmarked solid lines in Figure 3. This is because the theoretical lower bound OVρ is based on the
¯T (ρ) bound  which is observed to approximate
asymptotically exact single-task OV bound and the
the multi-task learning curve rather closely (point (b)).

T (0) and σ2

T (1); and (d) the empirical lower bound (cid:104)(cid:104)

Conclusions We have measured the inﬂuence of the secondary task on the primary task using the
generalization error and the learning curve  parameterizing these with the correlation ρ between the
two tasks  and the proportion πS of observations for the secondary task. We have provided bounds
on the generalization error and learning curves  and these bounds highlight the effects of ρ and πS.
This is a step towards understanding the role of the matrix K f of inter-task similarities in multi-task
GPs with more than two tasks. Analysis on the degenerate case of no training data for the primary
task has uncovered an intrinsic limitation of multi-task GP. Our work contributes to an understanding
of multi-task learning that is orthogonal to the existing PAC-based results in the literature.

Acknowledgments

I thank E Bonilla for motivating this problem  CKI Williams for helpful discussions and for propos-
ing the equivalent isotropic noise approach  and DSO National Laboratories  Singapore  for ﬁnancial
support. This work is supported in part by the EU through the PASCAL2 Network of Excellence.

8

0501001502002503000.20.40.60.810501001502002503000.20.40.60.81References
[1] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.

MIT Press  Cambridge  Massachusetts  2006.

[2] Yee Whye Teh  Matthias Seeger  and Michael I. Jordan. Semiparametric latent factor models.
In Robert G. Cowell and Zoubin Ghahramani  editors  Proceedings of the 10th International
Workshop on Artiﬁcial Intelligence and Statistics  pages 333–340. Society for Artiﬁcial Intel-
ligence and Statistics  January 2005.

[3] Edwin V. Bonilla  Felix V. Agakov  and Christopher K. I. Williams. Kernel Multi-task Learning
In Marina Meila and Xiaotong Shen  editors  Proceedings of
using Task-speciﬁc Features.
the 11th International Conference on Artiﬁcial Intelligence and Statistics. Omni Press  March
2007.

[4] Kai Yu  Wei Chu  Shipeng Yu  Volker Tresp  and Zhao Xu. Stochastic Relational Models for
Discriminative Link Prediction. In B. Sch¨olkopf  J. Platt  and T. Hofmann  editors  Advances
in Neural Information Processing Systems 19  Cambridge  MA  2007. MIT Press.

[5] Edwin V. Bonilla  Kian Ming A. Chai  and Christopher K.I. Williams. Multi-task Gaussian
In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in

process prediction.
Neural Information Processing Systems 20. MIT Press  Cambridge  MA  2008.

[6] Jonathan Baxter. A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Re-

[7] Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Re-

search  12:149–198  March 2000.

search  7:117–139  January 2006.

[8] Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable

multiple-task learning guarantees. Machine Learning  73(3):273–287  2008.

[9] Christopher K. I. Williams and Francesco Vivarelli. Upper and lower bounds on the learning

curve for Gaussian processes. Machine Learning  40(1):77–102  2000.

[10] Ya Xue  Xuejun Liao  Lawrence Carin  and Balaji Krishnapuram. Multi-task learning for
classiﬁcation with Dirichlet process prior. Journal of Machine Learning Research  8:35–63 
January 2007.

[11] Klaus Ritter. Average-Case Analysis of Numerical Problems  volume 1733 of Lecture Notes in

[12] Christopher T. H. Baker. The Numerical Treatment of Integral Equations. Clarendon Press 

Mathematics. Springer  2000.

1977.

[13] Peter Sollich and Anason Halees. Learning curves for Gaussian process regression: Approxi-

mations and bounds. Neural Computation  14(6):1393–1428  2002.
[14] Noel A. Cressie. Statistics for Spatial Data. Wiley  New York  1993.
[15] Manfred Opper and Francesco Vivarelli. General bounds on Bayes errors for regression with

Gaussian processes. In Kearns et al. [18]  pages 302–308.

[16] Giancarlo Ferrari Trecate  Christopher K. I. Williams  and Manfred Opper. Finite-dimensional

approximation of Gaussian processes. In Kearns et al. [18]  pages 218–224.

[17] Huaiyu Zhu  Christopher K. I. Williams  Richard Rohwer  and Michal Morciniec. Gaussian
regression and optimal ﬁnite dimensional linear models.
In Christopher M. Bishop  editor 
Neural Networks and Machine Learning  volume 168 of NATO ASI Series F: Computer and
Systems Sciences  pages 167–184. Springer-Verlag  Berlin  1998.

[18] Michael J. Kearns  Sara A. Solla  and David A. Cohn  editors. Advances in Neural Information

Processing Systems 11  1999. The MIT Press.

9

,Jamie Morgenstern
Tim Roughgarden