2008,Stochastic Relational Models for Large-scale Dyadic Data using MCMC,Stochastic relational models provide a rich family of choices for learning and predicting dyadic data between two sets of entities. It generalizes matrix factorization to a supervised learning problem that utilizes attributes of objects in a hierarchical Bayesian framework. Previously empirical Bayesian inference was applied  which is however not scalable when the size of either object sets becomes tens of thousands. In this paper  we introduce a Markov chain Monte Carlo (MCMC) algorithm to scale the model to very large-scale dyadic data. Both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem  which involves tens of thousands users and a half million items.,Stochastic Relational Models for

Large-scale Dyadic Data using MCMC

NEC Laboratories America  Cupertino  CA 95014  USA

Shenghuo Zhu
Yihong Gong
{zsh  kyu  ygong}@sv.nec-labs.com

Kai Yu

Abstract

Stochastic relational models (SRMs) [15] provide a rich family of choices for
learning and predicting dyadic data between two sets of entities. The models gen-
eralize matrix factorization to a supervised learning problem that utilizes attributes
of entities in a hierarchical Bayesian framework. Previously variational Bayes in-
ference was applied for SRMs  which is  however  not scalable when the size of
either entity set grows to tens of thousands. In this paper  we introduce a Markov
chain Monte Carlo (MCMC) algorithm for equivalent models of SRMs in order to
scale the computation to very large dyadic data sets. Both superior scalability and
predictive accuracy are demonstrated on a collaborative ﬁltering problem  which
involves tens of thousands users and half million items.

1 Stochastic Relational Models

Stochastic relational models (SRMs) [15] are generalizations of Gaussian process (GP) models [11]
to the relational domain  where each observation is a dyadic datum  indexed by a pair of entities.
They model dyadic data by a multiplicative interaction of two Gaussian process priors.
Let U be the feature representation (or index) space of a set of entities. A pair-wise similarity in
U is given by a kernel (covariance) function Σ : U × U → R. A Gaussian process (GP) deﬁnes
a random function f : U → R  whose distribution is characterized by a mean function and the
covariance function Σ  denoted by f ∼ N∞(0  Σ)1  where  for simplicity  we assume the mean to
be the constant zero. GP complies with the intuition regarding the smoothness — if two entities ui
and uj are similar according to Σ  then f(ui) and f(uj) are similar with a high probability.
A domain of dyadic data must involve another set of entities  let it be represented (or indexed) by
V. In a similar way  this entity set is associated with another kernel function Ω. For example  in a
typical collaborative ﬁltering domain  U represents users while V represents items  then  Σ measures
the similarity between users and Ω measures the similarity between items.
Being the relation between a pair of entities from different sets  a dyadic variable y is indexed by
the product space U × V. Then an SRM aims to model y(u  v) by the following generative process 
Model 1. The generative model of an SRM:
1. Draw kernel functions Σ ∼ IW∞(δ  Σ◦)  and Ω ∼ IW∞(δ  Ω◦);
2. For k = 1  . . .   d: draw random functions fk ∼ N∞(0  Σ)  and gk ∼ N∞(0  Ω);

1We denote an n dimensional Gaussian distribution with a covariance matrix Σ by Nn(0  Σ). Then

N∞(0  Σ) explicitly indicates that a GP follows an “inﬁnite dimensional” Gaussian distribution.

1

3. For each pair (u  v): draw y(u  v) ∼ p(y(u  v)|z(u  v)  γ)  where

z(u  v) =

1√
d

fk(u)gk(v) + b(u  v).

d(cid:88)

k=1

In this model  IW∞(δ  Σ◦) and IW∞(δ  Ω◦) are hyper priors  whose details will be introduced
later. p(y|z  γ) is the problem-speciﬁc noise model. For example  it can follow a Gaussian noise
distribution y ∼ N1(z  γ) if y is numerical  or  a Bernoulli distribution if y is binary. Function
b(u  v) is the bias function over the U × V. For simplicity  we assume b(u  v) = 0.
In the limit d → ∞  the model converges to a special case where fk and gk can be analytically
marginalized out and z becomes a Gaussian process z ∼ N∞(0  Σ ⊗ Ω) [15]  with the covariance
between pairs being a tensor kernel

K ((ui  vs)  (uj  vt)) = Σ(ui  uj)Ω(vs  vt).

In anther special case  if Σ and Ω are both ﬁxed to be Dirac delta functions  and U  V are ﬁnite sets 
it is easy to see that the model reduces to probabilistic matrix factorization.
The hyper prior IW∞(δ  Σ◦) is called inverted Wishart Process that generalizes the ﬁnite n-
dimensional inverted Wishart distribution [2]

IW n(Σ|δ  Σ◦) ∝ |Σ|− 1

2 (δ+2n) etr(cid:0) − 1

2 Σ−1Σ◦(cid:1) 

where δ is the degree-of-freedom parameter  and Σ◦ is a positive deﬁnite kernel matrix. We note
that the above deﬁnition is different from the popular formulation [3] or [4] in the machine learning
community. The advantage of this new notation is demonstrated by the following theorem [2].
Theorem 1. Let A ∼ IW m(δ  K)  A ∈ R+  K ∈ R+  and A and K be partitioned as

(cid:21)

(cid:20)A11  A12

A21  A22

A =

  K =

(cid:21)

(cid:20)K11  K12

K21  K22

where A11 and K11 are two n × n sub matrices  n < m  then A11 ∼ IW n(δ  K11).
The new formulation of inverted Wishart is consistent under marginalization. Therefore  similar to
the way of deriving GPs from Gaussian distributions  we deﬁne a distribution of inﬁnite-dimensional
kernel functions  denoted by Σ ∼ IW∞(δ  Σ◦)  such that any sub kernel matrix of size m × m
follows Σ ∼ IW m(δ  Σ◦)  where both Σ and Σ◦ are positive deﬁnite kernel functions. In case
when U and V are sets of entity indices  SRMs let Σ◦ and Ω◦ both be Dirac delta functions  i.e.  any
of their sub kernel matrices is an identity matrix.
Similar to GP regression/classiﬁcation  the major application of SRMs is supervised prediction based
on observed relational values and input features of entities. Formally  let YI = {y(u  v)|(u  v) ∈ I}
be the set of noisy observations  where I ⊂ U × V  the model aims to predict the noise-free values
ZO = {z(u  v)|(u  v) ∈ O} on O ⊂ U × V. As our computation is always on a ﬁnite set containing
both I and O  from now on  we only consider the ﬁnite subset U0 × V0  a ﬁnite support subset of
U ×V that contains I∪ O. Accordingly we let Σ be the covariance matrix of Σ on U0  and Ω be the
covariance matrix of Ω on V0.
Previously a variational Bayesian method was applied to SRMs [15]  which computes the maximum
a posterior estimates of Σ and Ω  given YI  and then predicts ZO based on the estimated Σ and Ω.
There are two limitations of this empirical Bayesian approach: (1) The variational method is not a
fully Bayesian treatment. Ideally we wish to integrate Σ and Ω; (2) The more critical issue is  the
algorithm has the complexity O(m3 + n3)  with m = |U0| and n = |V0|  is not scalable to a large
relational domain where m or n exceeds several thousands. In this paper we will introduce a fully
Bayesian inference algorithm using Markov chain Monte Carlo sampling. By deriving equivalent
sampling processes  we show the algorithms can be applied to a dataset  which is 103 times larger
than the previous work [15]  and produce an excellent accuracy.
In the rest of this paper  we present our algorithms for Bayesian inference of SRMs in Section 2.
Some related work is discussed in Section 3  followed by experiment results of SRMs in Section 4.
Section 5 concludes.

2

2 Bayesian Models and MCMC Inference

In this paper  we tackle the scalability issue with a fully Bayesian paradigm. We estimate the expec-
tation of ZO directly from YI using Markov-chain Monte Carlo (MCMC) algorithm (speciﬁcally 
Gibbs sampling)  instead of evaluating that from estimated Σ or Ω. Our contribution is in how to
make the MCMC inference more efﬁcient for large scale data.
We ﬁrst introduce some necessary notation here. Bold capital letters  e.g. X  indicate matrices. I(m)
is an identity matrix of size m × m. Nd  Nm d  IW m  χ−2 are the multivariate normal distribution 
the matrix-variate normal distribution  the inverse-Wishart distribution  and the inverse chi-square
distribution  respectively.

2.1 Models with Non-informative Priors
Let r = |I|  m = |U0| and n = |V0|. It is assumed that d (cid:28) min(m  n)  and the observed set  I  is
sparse  i.e. r (cid:28) mn. First  we consider the case of Σ◦ = αI(m) and Ω◦ = βI(n). Let {fk} on U0
denoted by matrix variate F of size m × d  {gk} on V0 denoted by matrix variate G of size n × d.
Then the generative model is written as Model 2 and depicted in Figure 1.
Model 2. The generative model of a matrix-variate SRM:
1. Draw Σ ∼ IW m(δ  αI(m)) and Ω ∼ IW n(δ  βI(n));
2. Draw F|Σ ∼ Nm d(0  Σ ⊗ I(d)) and G|Ω ∼ Nn d(0  Ω ⊗ I(d));
3. Draw s2 ∼ χ−2(ν  σ2) ;
4. Draw Y|F  G  s2 ∼ Nm n(Z  s2I(m) ⊗ I(n))  where Z = FG(cid:62).
where Nm d is the matrix-variate normal distribution of size m × d; α 
β  δ  ν and σ2 are scalar parameters of the model. A slight difference
between this ﬁnite model and Model 1 is that the coefﬁcient 1/
d is ignored for simplicity because
this coefﬁcient can be absorbed by α or β.
As we can explicitly compute Pr(Σ|F)  Pr(Ω|G)  Pr(F|YI  G  Σ  s2)  Pr(G|YI  F  Ω  s2) 
Pr(s2|YI  F  G)  we can apply Gibbs sampling algorithm to compute ZO. However  the com-
putational time complexity is at least O(m3 + n3)  which is not practical for large scale data.

Figure 1: Model 2

√

2.2 Gibbs Sampling Method

To overcome the inefﬁciency in sampling large covariance matrices  we rewrite the sampling
process using the property of Theorem 2 to take the advantage of d (cid:28) min(m  n).
Theorem 2. If

1. Σ ∼ IW m(δ  αI(m)) and F|Σ ∼ Nm d(0  Σ ⊗ I(d)) 
2. K ∼ IW d(δ  αI(d)) and H|K ∼ Nm d(0  I(m) ⊗ K) 

then  matrix variates  F and H  have the same distribution.

Proof sketch. Matrix variate F follows a matrix variate t distribution 
t(δ  0  αI(m)  I(d))  which is written as

Figure 2: Theorem 2

p(F) ∝ |I(m) + (αI(m))−1F(I(d))−1F(cid:62)|− 1

2 (δ+m+d−1) = |I(m) + α−1FF(cid:62)|− 1

2 (δ+m+d−1)

Matrix variate H follows a matrix variate t distribution  t(δ  0  I(m)  αI(d))  which can be written as
p(H) ∝ |I(m) + (I(m))−1H(αI(d))−1H(cid:62)|− 1
Thus  matrix variates  F and H  have the same distribution.

2 (δ+m+d−1) = |I(m) + α−1HH(cid:62)|− 1

2 (δ+m+d−1)

3

SI(d)WI(d)FGZYs2aI(m)SI(d)F→aI(d)KI(m)FThis theorem allows us to sample a smaller covariance matrix K of size d × d on the column side
instead of sampling a large covariance matrix Σ of size m × m on the row side. The translation is
depicted in Figure 2. This theorem applies to G as well  thus we rewrite the model as Model 3 (or
Figure 3). A similar idea was used in our previous work [16].
Model 3. The alternative generative model of a matrix-variate SRM:
1. Draw K ∼ IW d(δ  αI(d)) and R ∼ IW d(δ  βI(d));
2. Draw F|K ∼ Nm d(0  I(m) ⊗ K)  and G|R ∼ Nn d(0  I(n) ⊗ R) 
3. Draw s2 ∼ χ−2(ν  σ2) ;
4. Draw Y|F  G  s2 ∼ Nm n(Z  s2I(m) ⊗ I(n))  where Z = FG(cid:62).
Let column vector f i be the i-th row of matrix F  and column vector gj
be the j-th row of matrix G. In Model 3  {f i} are independent given K 
G and s2. Similar independence applies to {gj
K  R  {f i}  {gj
(for Bayesian SRM).
We use Gibbs sampling to compute the mean of ZO  which is derived from the samples of FG(cid:62).
Because of the sparsity of I  each iteration in this sampling algorithm can be computed in O(d2r +
d3(m + n)) time complexity2  which is a dramatic reduction from the previous time complexity
O(m3 + n3) .

} as well. The conditional posterior distribution of
} and s2 can be easily computed  thus the Gibbs sampling for SRM is named BSRM

Figure 3: Model 3

2.3 Models with Informative Priors

An important characteristic of SRMs is that it allows the inclusion of certain prior knowledge of
entities into the model. Speciﬁcally  the prior information is encoded as the prior covariance param-
eters  i.e. Σ◦ and Ω◦. In the general case  it is difﬁcult to run sampling process due to the size of Σ◦
and Ω◦. We assume that Σ◦ and Ω◦ have a special form  i.e. Σ◦ = F◦(F◦)(cid:62) + αI(m)  where F◦ is
an m × p matrix  and Ω◦ = G◦(G◦)(cid:62) + βI(n)  where G◦ is an n × q matrix  and the magnitude of
p and q is about the same as or less than that of d. This prior knowledge can be obtained from some
additional features of entities.
Although such an informative Σ◦ prevents us from directly sampling each row of F independently 
as we do in Model 3  we can expand matrix F of size m × d to (F  F◦) of size m × (d + p)  and
derive an equivalent model  where rows of F are conditionally independent given F◦. Figure 4
illustrates this transformation.
Theorem 3. Let δ > p  Σ◦ = F◦(F◦)(cid:62) + αI(m)  where F◦ is an m× p
matrix. If

1. Σ ∼ IW m(δ  Σ◦) and F|Σ ∼ Nm d(0  Σ ⊗ I(d)) 

(cid:19) ∼ IW d+p(δ − p  αI(d+p)) and

(cid:18)K11 K12

2. K =

K21 K22

H|K ∼ Nm d(F◦K−1
where K11·2 = K11 − K12K−1

22 K21  I(m) ⊗ K11·2) 
22 K21  then F and H have the same distribution.

Figure 4: Theorem 3

Proof sketch. Consider the distribution

(H1  H2)|K ∼ Nm d+p(0  I(m) ⊗ K).

(1)
22 K21  I(m) ⊗ K11·2)  p(H) = p(H1|H2 = F◦). On the other
Because H1|H2 ∼ Nm d(H2K−1
hand  we have a matrix-variate t distribution  (H1  H2) ∼ tm d+p(δ − p  0  αI(m)  I(d+p)). By
Theorem 4.3.9 in [4]  we have H1|H2 ∼ tm d(δ  0  αI(m) + H2H(cid:62)
  I(d)) = tm d(δ  0  Σ◦  I(d)) 
which implies p(F) = p(H1|H2 = F◦) = p(H).

2

2|Y − FG(cid:62)|2I can be efﬁciently computed in O(dr) time.

4

KI(m)RI(n)FGZYs2S0SI(d)F→aI(d+p)KI(m)(F F0)The following corollary allows us to compute the posterior distribution of K efﬁciently.
Corollary 4. K|H ∼ IW d+p(δ + m  αI(d+p) + (H  F◦)(cid:62)(H  F◦)).

Proof sketch. Because normal distribution and inverse Wishart distribution are conjugate  we can
derive the posterior distribution K from Eq. (1).

Thus  we can explicitly sample from the conditional posterior distributions  as listed in Algorithm 1
(BSRM/F for BSRM with features) in Appendix. We note that when p = q = 0  Algorithm 1
(BSRM/F) reduces to the exact algorithm for BSRM. Each iteration in this sampling algorithm can
be computed in O(d2r + d3(m + n) + dpm + dqn) time complexity.

2.4 Unblocking for Sampling Implementation

Blocking Gibbs sampling technique is commonly used to improve the sampling efﬁciency by re-
ducing the sample variance according to the Rao-Blackwell theorem (c.f. [9]). However  blocking
Gibbs sampling is not necessary to be computationally efﬁcient. To improve the computational efﬁ-
ciency of Algorithm 1  we use unblocking sampling to reduce the major computational cost is Step 2
and Step 4. We consider sampling each element of F conditionally. The sampling process is written
as Step 4 and Step 9 of Algorithm 2  which is called BSRM/F with conditional Gibss sampling. We
can reduce the computational cost of each iteration to O(dr + d2(m + n) + dpm + dqn)  which is
comparable to other low-rank matrix factorization approaches. Though such a conditional sampling
process increases the sample variance comparing to Algorithm 1  we can afford more samples within
a given amount of time due to its faster speed. Our experiments show that the overall computational
cost of Algorithm 2 is usually less than that of Algorithm 1 when achieving the same accuracy.
Additionally  since {f i} are independent  we can parallelize the for loops in Step 4 and Step 9 of
Algorithm 2.

3 Related Work

SRMs fall into a class of statistical latent-variable relational models that explain relations by latent
factors of entities. Recently a number of such models were proposed that can be roughly put into two
groups  depending on whether the latent factors are continuous or discrete: (1) Discrete latent-state
relational models: a large body of research infers latent classes of entities and explains the entity
relationship by the probability conditioned on the joint state of participating entities  e.g.  [6  14  7 
1]. In another work [10]  binary latent factors are modeled; (2) Continuous latent-variable relational
models: many such models assume relational data underlain by multiplicative effects between latent
variables of entities  e.g. [5]. A simple example is matrix factorization  which recently has become
very popular in collaborative ﬁltering applications  e.g.  [12  8  13].
The latest Bayesian probabilistic matrix factorization [13] reported the state-of-the-art accuracy of
matrix factorization on Netﬂix data. Interestingly  the model turns out to be similar to our Model 3
under the non-informative prior. This paper reveals the equivalence between different models and
offers a more general Bayesian framework that allows informative priors from entity features to
play a role. The framework also generalizes Gaussian processes [11] to a relational domain  where
a nonparametric prior for stochastic relational processes is described.

4 Experiments

Synthetic data: We compare BSRM under noninformative priors against two other algorithms: the
fast max-margin matrix factorization (fMMMF) in [12] with a square loss  and SRM using varia-
tional Bayesian approach (SRM-VB) in [15]. We generate a 30 × 20 random matrix (Figure 5(a)) 
then add Gaussian noise with σ2 = 0.1 (Figure 5(b)). The root mean squared noise is 0.32. We
select 70% elements as the observed data and use the rest of the elements for testing. The recon-
struction matrix and root mean squared errors (RMSEs) of predictions on the test elements are shown
in Figure 5(c)-5(e). BSRM outperforms the variational approach of SRMs and fMMMF. Note that
because of the log-determinant penalty of the inverse Wishart prior  SRM-VB enforces the rank to
be smaller  thus the result of SRM-VB looks smoother than that of BSRM.

5

(a) Original Matrix (b) With Noise(0.32) (c) fMMMF (0.27)

(d) SRM-VB(0.22)

(e) BSRM(0.19)

Figure 5: Experiments on synthetic data. RMSEs are shown in parentheses.

User Mean Movie Mean

RMSE
MAE

1.425
1.141

1.387
1.103

fMMMF [12] VB [8]
1.165
0.915

1.186
0.943

Table 1: RMSE (root mean squared error) and MAE (mean absolute error) of the experiments on
EachMovie data. All standard errors are 0.001 or less.

EachMovie data: We test the accuracy and the efﬁciency of our algorithms on EachMovie. The
dataset contains 74  424 users’ 2  811  718 ratings on 1  648 movies  i.e. about 2.29% are rated by
zero-to-ﬁve stars. We put all the ratings into a matrix  and randomly select 80% as observed data
to predict the remaining ratings. The random selection was carried out 10 times independently. We
compare our approach against several competing methods: 1) User Mean  predicting ratings by the
sample mean of the same user’s ratings; 2) Move Mean  predicting rating by the sample mean of
ratings on the same movie; 3) fMMMF [12]; 4) VB introduced in [8]  which is a probabilistic low-
rank matrix factorization using variational approximation. Because of the data size  we cannot run
the SRM-VB of [15]. We test the algorithms BSRM and BSRM/F  both following Algorithm 2 
which run without and with features  respectively. The features used in BSRM/F are generated from
the PCA result of the binary indicator matrix that indicates whether the user rates the movie. The
10 top factors of both the user side and the movie side are used as features  i.e. p = 10  q = 10. We
run the experiments with different d = 16  32  100  200  300. The hyper parameters are set to some
trivial values  δ = p + 1 = 11  α = β = 1  σ2 = 1  and ν = 1. The results are shown in Table 1
and 2. We ﬁnd that the accuracy improves as the number of d is increased. Once d is greater than
100  the further improvement is not very signiﬁcant within a reasonable amount of running time.

rank (d)

16

BSRM RMSE 1.0983
0.8411
BSRM/F RMSE 1.0952
0.8311

MAE

MAE

32

1.0924
0.8321
1.0872
0.8280

100

1.0905
0.8335
1.0848
0.8289

200

1.0903
0.8340
1.0846
0.8293

300

1.0902
0.8393
1.0852
0.8292

Table 2: RMSE (root mean squared error) and MAE (mean absolute error) of experiments on Each-
Movie data. All standard errors are 0.001 or less.

To compare the overall computational efﬁciency of the two Gibbs sampling procedures  Algorithm 1
and Algorithm 2  we run both algorithms
and record the running time and accuracy
in RMSE. The dimensionality d is set to
be 100. We compute the average ZO and
evaluate it after a certain number of itera-
tions. The evaluation results are shown in
Figure 6. We run both algorithms for 100
iterations as the burn-in period  so that we
can have an independent start sample. Af-
ter the burn-in period  we restart to compute
the averaged ZO and evaluate them  there-
fore there are abrupt points at 100 iterations
in both cases. The results show that the
overall accuracy of Algorithm 2 is better at
any given time.

Figure 6: Time-Accuracy of Algorithm 1 and 2

6

246810121416182051015202530246810121416182051015202530246810121416182051015202530246810121416182051015202530246810121416182051015202530 1.08 1.1 1.12 1.14 1.16 1.18 1.2 0 1000 2000 3000 4000 5000 6000 7000 8000RMSERunning time (sec)burn-in endsburn-in endsAlgorithm 1Algorithm 2Netﬂix data: We also test the algorithms on the large collection of user ratings from netﬂix.com. The
dataset consists of 100  480  507 ratings from 480  189 users on 17  770 movies. In addition  Netﬂix
also provides a set of validation data with 1  408  395 ratings. In order to evaluate the prediction
accuracy  there is a test set containing 2  817  131 ratings whose values are withheld and unknown
for all the participants.
The features used in BSRM/F are generated from the PCA result of a binary matrix that indicates
whether or not the user rated the movie. The top 30 user-side factors are used as features  none of
movie-side factors are used  i.e. p = 30  q = 0. The hyper parameters are set to some trivial values 
δ = p + 1 = 31  α = β = 1  σ2 = 1  and ν = 1. The results on the validation data are shown in
Table 3. The submitted result of BSRM/F(400) achieves RMSE 0.8881 on the test set. The running
time is around 21 minutes per iteration for 400 latent dimensions on an Intel Xeon 2GHz PC.

RMSE

BPMF [13]

VB[8]
0.9141
0.8880
Table 3: RMSE (root mean squared error) of experiments on Netﬂix data.

0.8926

0.8920

0.8930

400

0.8895

100

BSRM

200

0.8910

BSRM/F

200

100

400

0.8874

5 Conclusions

In this paper  we study the fully Bayesian inference for stochastic relational models (SRMs)  for
learning the real-valued relation between entities of two sets. We overcome the scalability issue
by transforming SRMs into equivalent models  which can be efﬁciently sampled. The experiments
show that the fully Bayesian inference outperforms the previously used variational Bayesian infer-
ence on SRMs. In addition  some techniques for efﬁcient computation in this paper can be applied to
other large-scale Bayesian inferences  especially for models involving inverse-Wishart distributions.
Acknowledgment: We thank the reviewers and Sarah Tyler for constructive comments.

References

[1] E. Airodi  D. Blei  S. Fienberg  and E. P. Xing. Mixed membership stochastic blockmodels. In

Journal of Machine Learning Research  2008.

[2] A. P. Dawid. Some matrix-variate distribution theory: notational considerations and a Bayesian

application. Biometrika  68:265–274  1981.

[3] A. Gelman  J. B. Carlin  H. S. Stern  and D. B. Rubin. Bayesian Data Analysis. Chapman &

Hall  New York  1995.

[4] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions. Chapman & Hall/CRC  2000.
[5] P. Hoff. Multiplicative latent factor models for description and prediction of social networks.

Computational and Mathematical Organization Theory  2007.

[6] T. Hofmann. Latent semantic models for collaborative ﬁltering. ACM Trans. Inf. Syst. 

22(1):89–115  2004.

[7] C. Kemp  J. B. Tenenbaum  T. L. Grifﬁths  T. Yamada  and N. Ueda. Learning systems of
concepts with an inﬁnite relational model. In Proceedings of the 21st National Conference on
Artiﬁcial Intelligence (AAAI)  2006.

[8] Y. J. Lim and Y. W. Teh. Variational Bayesian approach to movie rating prediction. In Pro-

ceedings of KDD Cup and Workshop  2007.

[9] J. S. Liu. Monte Carlo Strategies in Scientiﬁc Computing. Springer  2001.
[10] E. Meeds  Z. Ghahramani  R. Neal  and S. T. Roweis. Modeling dyadic data with binary latent

factors. In Advances in Neural Information Processing Systems 19  2007.

[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[12] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative

prediction. In ICML  2005.

7

[13] R. Salakhutdinov and A. Mnih. Bayeisna probabilistic matrix factorization using Markov chain

Monte Carlo. In The 25th International Conference on Machine Learning  2008.

[14] Z. Xu  V. Tresp  K. Yu  and H.-P. Kriegel. Inﬁnite hidden relational models. In Proceedings of

the 22nd International Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2006.

[15] K. Yu  W. Chu  S. Yu  V. Tresp  and Z. Xu. Stochastic relational models for discriminative link

prediction. In Advances in Neural Information Processing Systems 19 (NIPS)  2006.

[16] S. Zhu  K. Yu  and Y. Gong. Predictive matrix-variate t models. In J. Platt  D. Koller  Y. Singer 
and S. Roweis  editors  NIPS ’07: Advances in Neural Information Processing Systems 20 
pages 1721–1728. MIT Press  Cambridge  MA  2008.

Appendix
Before presenting the algorithms  we introduce the necessary notation. Let Ii = {j|(i  j) ∈ I} and
Ij = {i|(i  j) ∈ I}. A matrix with subscripts indicates its submatrix  which consists its entries at the
given indices in the subscripts  for example  XIj  j is a subvector of the j-th column of X whose row
indices are in set Ij  X· j is the j-th column of X (· indicates the full set). Xi j denotes the (i  j)-th
i j. We ﬁll the unobserved
elements in Y with 0 for simplicity in notation

entry of X. |X|2I is the squared sum of elements in set I  i.e. (cid:80)

(i j)∈I X 2

Algorithm 1 BSRM/F: Gibbs sampling for SRM with features
1: Draw K ∼ IW d+p(δ + m  αI(d+p) + (F  F◦)(cid:62)(F  F◦));
2: For each i ∈ U0  draw f i ∼ Nd(K(i)(s−2G(cid:62)Y(cid:62)
i · + K−1
3: Draw R ∼ IW d+q(δ + n  βI(d+q) + (G  G◦)(cid:62)(G  G◦));
4: For each j ∈ V0  draw gj
∼ Nd(R(j)(s−2F(cid:62)Y· j + R−1
5: Draw s2 ∼ χ−2(ν + r  σ2 + |Y − FG(cid:62)|2I ).

where K(i) =(cid:0)s−2(GIi ·)(cid:62)GIi · + K−1
where R(j) =(cid:0)s−2(FIj  ·)(cid:62)FIj  · + R−1
1: ∆i j ← Yi j −(cid:80)

Fi kGj k  for (i  j) ∈ I;

(cid:1)−1;
(cid:1)−1;

11·2

11·2

22 f◦
11·2K12K−1

i

)  K(i)) 

11·2R12R−1

22 g◦

j

)  R(j)) 

Draw f ∼ N1(φ−1(s−2∆i IiGIi k − Fi ·Φ· k)  φ−1)  where φ = s−2(GIi k)(cid:62)GIi k + Φk k;
Update Fi k ← Fi k + f  and ∆i j ← ∆i j − f Gj k  for j ∈ Ii;

k

Algorithm 2 BSRM/F: Conditional Gibbs sampling for SRM with features
2: Draw Φ ∼ Wd+p(δ + m + d + p − 1  (αI(d+p) + (F  F◦)(cid:62)(F  F◦))−1);
3: for each (i  k) ∈ U0 × {1 ···   d} do
4:
5:
6: end for
7: Draw Ψ ∼ Wd+q(δ + n + d + q − 1  (βI(d+q) + (G  G◦)(cid:62)(G  G◦))−1);
8: for each (j  k) ∈ V0 × {1 ···   d} do
9:
10:
11: end for
12: Draw s2 ∼ χ−2(ν + r  σ2 + |∆|2I ).

Draw g ∼ N1(ψ−1(s−2∆(cid:62)
Update Gj k ← Gj k + g and ∆i j ← ∆i j − gFi k  for i ∈ Ij;

Ij  jFIj  k−Gj ·Ψ· k)  ψ−1)  where ψ = s−2(FIj  k)(cid:62)FIj  k +Ψk k;

8

,Boqing Gong
Kristen Grauman
Fei Sha
Lingqiao Liu
Chunhua Shen
Lei Wang
Anton van den Hengel
Chao Wang