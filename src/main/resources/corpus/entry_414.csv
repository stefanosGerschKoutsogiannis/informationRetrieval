2009,Adaptive Regularization for Transductive Support Vector Machine,We discuss the framework of Transductive Support Vector Machine (TSVM) from the perspective of the regularization strength induced by the unlabeled data. In this framework  SVM and TSVM can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data  respectively. Therefore  to supplement this framework of the regularization strength  it is necessary to introduce data-dependant partial regularization. To this end  we reformulate TSVM into a form with controllable regularization strength  which includes SVM and TSVM as special cases. Furthermore  we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption. Experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art TSVM algorithms.,Adaptive Regularization for

Transductive Support Vector Machine

Zenglin Xu †‡
† Cluster MMCI

Saarland Univ. & MPI INF

Saarbrucken  Germany
zlxu@mpi-inf.mpg.de

Rong Jin

Computer Sci. & Eng.
Michigan State Univ.
East Lansing  MI  U.S.
rongjin@cse.msu.edu

Jianke Zhu

Computer Vision Lab

ETH Zurich

Zurich  Switzerland

zhuji@vision.ee.ethz.ch

Irwin King‡

Michael R. Lyu‡

‡ Computer Science & Engineering
The Chinese Univ. of Hong Kong

Shatin  N.T.  Hong Kong

{king lyu}@cse.cuhk.edu.hk

Zhirong Yang

Information & Computer Science

Helsinki Univ. of Technology

Espoo  Finland

zhirong.yang@tkk.fi

Abstract

We discuss the framework of Transductive Support Vector Machine
(TSVM) from the perspective of the regularization strength induced by
the unlabeled data. In this framework  SVM and TSVM can be regarded
as a learning machine without regularization and one with full regular-
ization from the unlabeled data  respectively. Therefore  to supplement
this framework of the regularization strength  it is necessary to introduce
data-dependant partial regularization. To this end  we reformulate TSVM
into a form with controllable regularization strength  which includes SVM
and TSVM as special cases. Furthermore  we introduce a method of adap-
tive regularization that is data dependant and is based on the smooth-
ness assumption. Experiments on a set of benchmark data sets indicate
the promising results of the proposed work compared with state-of-the-art
TSVM algorithms.

1 Introduction

Semi-supervised learning has attracted a lot of research focus in recently years. Most of
the existing approaches can be roughly divided into two categories: (1) the clustering-based
methods [12  4  8  17] assume that most of the data  including both the labeled ones and the
unlabeled ones  should be far away from the decision boundary of the target classes; (2) the
manifold-based methods make the assumption that most of data lie on a low-dimensional
manifold in the input space  which include Label Propagation [21]  Graph Cuts [2]  Spectral
Kernels [9  22]  Spectral Graph Transducer [11]  and Manifold Regularization [1]. The
comprehensive study on semi-supervised learning techniques can be found in the recent
surveys [23  3].

Although semi-supervised learning wins success in many real-world applications  there still
remains two major unsolved challenges. One is whether the unlabeled data can help the
classiﬁcation  and the other is what is the relation between the clustering assumption and
the manifold assumption.

As for the ﬁrst challenge  Singh et al. [16] provided a ﬁnite sample analysis on the usefulness
of unlabeled data based on the cluster assumption. They show that unlabeled data may

be useful for improving the error bounds of supervised learning methods when the margin
between diﬀerent classes satisﬁes some conditions. However  in the real-world problems  it
is hard to identify the conditions that unlabeled data can help.

On the other hand  it is interesting to explore the relation between the low density assump-
tion and the manifold assumption. Narayanan et al. [14] implied that the cut-size of the
graph partition converges to the weighted volume of the boundary which separates the two
regions of the domain for a ﬁxed partition. This makes a step forward for exploring the
connection between graph-based partitioning and the idea surrounding the low density as-
sumption. Unfortunately  this approach cannot be generalized uniformly over all partitions.
Laﬀerty and Wasserman [13] revisited the assumptions of semi-supervised learning from the
perspective of minimax theory  and suggested that the manifold assumption is stronger than
the smoothness assumption for regression. Till now  the underlying relationships between
the cluster assumption and the manifold assumption are still undisclosed. Speciﬁcally  it is
unclear that in what kind of situation the clustering assumption or the manifold assumption
should be adopted.

In this paper  we address these current limitations by a uniﬁed solution from the perspective
of the regularization strength of the unlabeled data. Taking Transductive Support Vector
Machine (TSVM) as an example  we suggest an framework that introduces the regularization
strength of the unlabeled data when estimating the decision boundary. Therefore  we can
obtain a spectrum of models by varying the regularization strength of unlabeled data which
corresponds to changing the models from supervised SVM to Transductive SVM. To select
the optimal model under the proposed framework  we employ the manifold regularization
assumption that enables the prediction function to be smooth over the data space. Further 
the optimal function is a linear combination of supervised models  weakly semi-supervised
models  and semi-supervised models. Additionally  it provides an eﬀective approach towards
combining the cluster assumption and the manifold assumption in semi-supervised learning.

The rest of this paper is organized as follows. In Section 2  we review the background of
Transductive SVM. In Section 3  we ﬁrst present a framework of models with diﬀerent reg-
ularization strength  followed by an integrating approach based on manifold regularization.
In Section 4  we report the experimental results on a series of benchmark data sets. Section
5 concludes the paper.

2 Related Work on TSVM

Before presenting the formulation of TSVM  we ﬁrst describe the notations used in this
paper. Let X = (x1  . . .   xn) denote the entire data set  including both the labeled examples
and the unlabeled ones. We assume that the ﬁrst l examples within X are labeled and the
next n − l examples are unlabeled. We denote the unknown labels by yu = (yu
n).

l+1  . . .   yu

TSVM [12] maximizes the margin in the presence of unlabeled data and keeps the boundary
traversing through low density regions while respecting labels in the input space. Under
the maximum-margin framework  TSVM aims to ﬁnd the classiﬁcation model with the
maximum classiﬁcation margin for both labeled and unlabeled examples  which amounts to
solve the following optimization problem:

min

w∈Rn yu∈Rn−` ξ∈Rn

s. t.

1
2

kwkK + C

l

Xi=1

ξi + C ∗

n

Xi=l+1

ξi

yiw>φ(xi) ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l 
yu
i w>φ(xi) ≥ 1 − ξi  ξi ≥ 0  l + 1 ≤ i ≤ n 

(1)

where C and C ∗ are the trade-oﬀ parameters between the complexity of the function w and
the margin errors. Moreover  the prediction function can be formulated as f (x) = w>φ(x).
Note that we remove the bias term in the above formulation  since it can be taken into
account by introducing a constant element into the input pattern alternatively.

As in [19] and [20]  we can rewrite (1) into the following optimization problem:

min
f  ξ

s. t.

1
2

f >K−1f + C

l

Xi=1

ξi + C ∗

n

Xi=l+1

ξi

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l 
|fi| ≥ 1 − ξi  ξi ≥ 0  l + 1 ≤ i ≤ n.

(2)

The optimization problem held in TSVM is a non-linear non-convex optimization [6]. During
past several years  researchers have devoted a signiﬁcant amount of research eﬀorts to solving
this critical problem. A branch-and-bound method [5] was developed to search for the
optimal solution  which is only limited to solve the problem with a small number of examples
due to involving the heavy computational cost. To apply TSVM for large-scale problems 
Joachims [12] proposed a label-switching-retraining procedure to speed up the optimization
procedure. Later  the hinge loss in TSVM is replaced by a smooth loss function  and a
gradient descent method is used to ﬁnd the decision boundary in a region of low density [4].
In addition  there are some iterative methods  such as deterministic annealing [15]  concave-
convex procedure (CCCP) [8]  and convex relaxation method [19  18]. Despite the success
of TSVM  the unlabeled data not necessarily improve classiﬁcation accuracy.

To better utilize the unlabeled data  unlike existing TSVM approaches  we propose a frame-
work that tries to control the regularization strength of the unlabeled data. To do this  we
intend to learn the optimal regularization strength conﬁguration from the combination of a
spectrum of models: supervised  weakly-supervised  and semi-supervised.

3 TSVM: A Regularization View

For the sake of illustration  we ﬁrst study a model that does not penalize on the classiﬁcation
errors of unlabeled data. Note that the penalization on the margin errors of unlabeled data
can be included if needed. Therefore  we have the following form of TSVM that can be
derived through the duality:

min
f  ξ

s. t.

1
2

f >K−1f + C

l

Xi=1

ξi

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l 
f 2
i ≥ 1  l + 1 ≤ i ≤ n.

(3)

3.1 Full Regularization of Unlabeled Data

In order to adjust the strength of the regularization raised from the unlabeled examples  we
introduce a coeﬃcient ρ ≥ 0  and modify the above problem (3) as below:

min
f  ξ

s. t.

1
2

f >K−1f + C

l

Xi=1

ξi

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l 
f 2
i ≥ ρ  l + 1 ≤ i ≤ n.

(4)

Obviously  it is the standard TSVM for ρ = 1. In particular  the larger the ρ is  the stronger
the regularization of unlabeled data is. It is also important to note that we only take into
account the classiﬁcation errors on the labeled examples in the above equation. Namely  we
only denote ξi for each labeled example.

Further  we write f = (fl; fu) where fl = (f1  . . .   fl) and fu = (fl+1  . . .   fn) represent the
prediction for the labeled and the unlabeled examples  respectively. According to the inverse
lemma of the block matrix  we can write K−1 as follows:

K−1 = (cid:18)

l

−M−1

M−1
u Ku lK−1

l l

−K−1

l l Kl uM−1

u

M−1

u

(cid:19)  

where

Ml = Kl l − Kl uK−1
Mu = Ku u − Ku lK−1

u uKu l 
l l Kl u.

Thus  the term f >K−1f is computed as

f >K−1f = f >

l M−1

l

fl + f >

u M−1

u fu − 2f >

l K−1

l l Kl uM−1

u fu.

When the unlabeled data are loosely correlated to the labeled data  namely when most of
the elements within Ku l are small  this leads to Mu ≈ Ku. We refer to this case as “weakly
unsupervised learning”. Using the above equations  we rewrite TSVM as follows:

min
fl fu ξ

1
2

l M−1
f >

l

fl + C

l

Xi=1

ξi + ω(fl  ρ)

(5)

s. t.

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l 

where ω(fl  ρ) is a regularization function for fl and it is the result of the following optimiza-
tion problem:

f >
u M−1

u fu − f >

l K−1

l l Kl uM−1

u fu

(6)

min
fu

s. t.

1
2
[f u

i ]2 ≥ ρ 

l + 1 ≤ i ≤ n.

To understand the regularization function ω(fl  ρ)  we ﬁrst compute the dual of the problem
(6) by the Lagrangian function:

L =

=

1
2

1
2

f >
u M−1

u fu − f >

l K−1

l l Kl uM−1

λi([f u

i ]2 − ρ)

nu

1
2

u fu −

Xi=1
l l Kl uM−1

u fu +

λ>e 

ρ
2

f >
u (M−1

u − D(λ))fu − f >

l K−1

where D(λ) = diag(λ1  . . .   λn−l) and e denotes a vector with all elements being one. As
the derivatives vanish for optimality  we have

fu = (M−1

u − D(λ))−1M−1

u Ku lK−1
l l fl

= (I − MuD(λ))−1Ku lK−1

l l fl 

where I is an identity matrix.

Replacing fu in (6) with the above equation  we have the following dual problem:

max

λ

−

s. t. M−1

l K−1
f >

l l Kl u(Mu − MuD(λ)Mu)−1Ku lK−1

1
2
u (cid:23) D(λ)  λi ≥ 0  i = 1  . . .   n − l.

l l fl + ρλ>e

(7)

The above formulation allows us to understand how the parameter ρ controls the strength
of regularization from the unlabeled data. In the following  we will show that a series of
learning models can be derived through assigning various values for the coeﬃcient ρ.

3.2 No Regularization from Unlabeled Data

First  we study the case of ρ = 0. We have the following theorem to illustrate the relationship
between the dual problem (7) and the supervised SVM.

Theorem 1 When ρ = 0  the optimization problem is reduced to the standard supervised
SVM.

Proof 1 It is not diﬃcult to see that the optimal solution to (7) is λ = 0. As a result 
ω(fl  ρ) becomes

ω(fl  ρ = 0) = −

1
2

flK−1

l l Kl uM−1

u Ku lK−1
l l fl

Substituting ω(fl  ρ) in (5) with the formulation above  the overall optimization problem
becomes

min
fl ξ

1
2

l (M−1
f >

l − K−1

l l Kl uM−1

u Ku lK−1

l l )fl + C

l

Xi=1

ξi

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l.
According to the matrix inverse lemma  we calculate M−1

s. t.

l

as below:

M−1

l

u uKu l)−1

= (Kl l − Kl uK−1
= K−1
= K−1

l l + K−1
l l + K−1

l l Kl u(Ku u − Ku lK−1
l l Kl uM−1

u Ku lK−1
l l .

l l Kl u)−1Ku lK−1

l l

Finally  the optimization problem is simpliﬁed as

min
fl ξ

1
2

l K−1
f >

l l fl + C

l

Xi=1

ξi

s. t.

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l.

(8)

Clearly  the above optimization is identical to the standard supervised SVM. Hence  the
unlabeled data are not employed to regularize the decision boundary when ρ = 0.

3.3 Partial Regularization of Unlabeled Data

Second  we consider the case when ρ is small. According to (7)  we expect λ to be small
when ρ is small. As a result  we can approximate (Mu − MuD(λ)Mu)−1 as follows:

(Mu − MuD(λ)Mu)−1 ≈ M−1

u + D(λ).

Consequently  we can write ω(fl  ρ) as follows:

ω(fl  ρ) = −

1
2

l K−1
f >

l l Kl uM−1

u Ku lK−1

l l fl + φ(fl  ρ) 

(9)

where φ(fl  ρ) is the output of the following optimization problem

max

λ

ρλ>e −

1
2

l K−1
f >

l l Kl uD(λ)Ku lK−1
l l fl

s. t. M−1

u (cid:23) D(λ)  λi ≥ 0  i = 1  . . .   n − l.

We can simplify the above problem by approximating M−1
u (cid:23) D(λ) as λi ≤ [σ1(Mu)]−1 
i = 1  . . .   n − l  where σ1(Mu) represents the maximum eigenvalue of matrix Mu. The
resulting simpliﬁed problem becomes

max

λ

s. t.

1
2

λ>e −

ρ
l l Kl uD(λ)Ku lK−1
l l fl
2
0 ≤ λi ≤ [σ1(Mu)]−1  1 ≤ i ≤ n − l.

l K−1
f >

As the above problem is a linear programming problem  the solution for λ can be computed
as:

λi = (cid:26)

0

σ(Mu)−1

[Ku lK−1
[Ku lK−1

l l fl]2
l l fl]2

i > ρ 
i ≤ ρ.

From the above formulation  we ﬁnd that ρ plays the role of a threshold of selecting the
unlabeled examples. Since [Ku lK−1
l l fl]i can be regarded as the approximation for the ith

unlabeled example  the above formulation can be interpreted in the way that only the unla-
beled examples with low prediction conﬁdence will be selected for regularizing the decision
boundary. Moreover  all the unlabeled examples with high prediction conﬁdence will be
ignored. From the above discussions  we can conclude that ρ determines the regularization
strength of unlabeled examples.

Then  we rewrite the overall optimization problem as below:

min
fl ξ

max

λ

s. t.

1
2

l K−1
f >

l l fl + C

l

Xi=1

ξi −

1
2

l K−1
f >

l l Kl uD(λ)Ku lK−1
l l fl

(10)

yifi ≥ 1 − ξi  ξi ≥ 0  1 ≤ i ≤ l 
0 ≤ λi ≤ [σ1(Mu)]−1  1 ≤ i ≤ n − l.

This is a min-max optimization problem and thus the global optimal solution can be guar-
anteed. To obtain the optimal solution  we employ an alternating optimization procedure 
which iteratively computes the values of fl and λ. To account for the penalty on the margin
error from the unlabeled data  we just need to add an extra constraint of λi ≤ 2C for
i = 1  . . .   n − l.

By varying the parameter ρ from 0 to 1  we can indeed obtain a series of transductive models
for SVM. When ρ is small  we call the corresponding optimization problem as weakly semi-
supervised learning. Therefore  it is important to ﬁnd an appropriate ρ which adapts for the
input data. However  as the data distribution is usually unknown  it is very challenging to
directly estimate an optimal regularization strength parameter ρ. Instead  we try to explore
an alternative approach to select an appropriate ρ by combining the prediction functions.
Due to the large cost in calculating the inverse of kernel matrices  one can solve the dual
problems according to the Representer theorem.

3.4 Adaptive Regularization

As stated in previous sections  ρ determines the regularization strength of the unlabeled
data. We now try to adapt the parameter ρ according to the unlabeled data information.
Speciﬁcally  we intend to implicitly select the best ρ from a given list  i.e.  Υ = {ρ1  . . .   ρm}
where ρ1 = 0 and ρm = 1. This is equivalent to selecting the optimal f from a list of
prediction functions  i.e.  F = {f1  . . .   fm}. Motivated from the ensemble technique for
semi-supervised learning [7]  we assume that the optimal f comes from a linear combination
of the base functions {fi}. We then have:

f =

m

Xi=1

θifi 

m

Xi=1

θi = 1  θi ≥ 0  i = 1  . . .   m.

where θi is the weight of the prediction function fi and θ ∈ Rm. One can also involve a
priori to θi. For example  if we have more conﬁdences on the semi-supervised classiﬁer 
we can introduce a constraint like θm ≥ 0.5.
It is important to note that the learning
functions in ensemble methods [7] are usually weak learners  while in our approach  the
learning functions are strong learners with diﬀerent degrees of regularization.

In the following  we study how to set the regularization strength adaptive to data. Since
TSVM naturally follows the cluster assumption of semi-supervised learning  in order to
complement the cluster assumption  we adopt another principle in semi-supervised learning 
i.e.  the manifold assumption. From the point of view of manifold assumption in semi-
supervised learning  the prediction function f should be smooth on unlabeled data. To
this end  the approach of manifold regularization is widely adopted as a smoothing term
in semi-supervised learning literatures  e.g. 
[1  10]. In the following  we will employ the
manifold regularization principle for selecting the regularization strength.

The manifold regularization is mainly based on a graph G =< V  E > derived from the whole
data space X  where V = {xi}n
i=1 is the vertex set  and E denotes the edges linking pairs of
nodes. In general  a graph is built in the following four steps: (1) constructing adjacency
graph; (2) calculating the weights on edges; (3) computing the adjacency matrix W; (4)

obtaining the graph Laplacian by L = diag(Pn

regularization term as f >Lf .

j=1 Wij ) − W. Then  we denote the manifold

For simplicity  we denote the predicted values of function fi on the data X as fi  such that
fi = ([fi]1  . . .   [fi]n). F = (f1  . . .   fm)> is used to represent the set of the prediction values
of all prediction functions. Finally  We have the following minimization problem:

min

θ

η(θ>F)L(F>θ) − y>

1
2
θ>e = 1  θi ≥ 0  i = 1  . . .   m 

` (F>

` θ)

(11)

` (F>

s. t.
where the second term  y>
` θ)  is used to strengthen the conﬁdence on the prediction over
the labeled data. η is a trade-oﬀ parameter. The above optimization problem is a simple
quadratic programming problem  which can be solved very eﬃciently. It is important to note
that the above optimization problem is less sensitive to the graph structure than Laplacian
SVM as used in [1]  since the basic learning functions are all strong learners. It also saves
a huge amount of eﬀorts in estimating the parameters compared with Laplacian SVM.

The above approach indeed provides a practical approach towards a combination of both
the cluster assumption and the manifold assumption. It is empirically suggested that com-
bining these two assumptions helps to improve the prediction accuracy of semi-supervised
learning according to the survey paper on semi-supervised SVM [6]. Moreover  when ρ = 0 
supervised models are incorporated in the framework. Thus the usefulness of unlabeled in
naturally considered by the regularization. This therefore provides a practical solution to
the problems described in Section 1.

4 Experiment

In this section  we give details of our implementation and discuss the results on several
benchmark data sets for our proposed approach. To conduct a comprehensive evaluation  we
employ several well-known datasets as the testbed. As summarized in Table 1  three image
data sets and ﬁve text data sets are selected from the recent book (www.kyb.tuebingen.
mpg.de/ssl-book/) and the literature (www.cs.uchicago.edu/~vikass/).

Table 1: Datasets used in our experiments. d represents the data dimensionality  and n
denotes the total number of examples.

Data set
usps
coil
pcmac
link

n

1500
1500
1946
1051

d
241
241
7511
1800

Data set
digit1
ibm vs rest
page
pagelink

n

1500
1500
1051
1051

d
241
11960
3000
4800

For simplicity  our proposed adaptive regularization approach is denoted as ARTSVM.
To evaluate it  we conduct an extensive comparison with several state-of-the-art ap-
proaches  including the label-switching-retraining algorithm in SVM-Light [12]  CCCP [8] 
and ∇TSVM [4]. We employ SVM as the baseline method.

In our experiments  we repeat all the algorithms 20 times for each dataset. In each run 
10% of the data are randomly selected as the training data and the remaining data are used
as the unlabeled data. The value of C in all algorithms are selected from [1  10  100  1000]
using cross-validation. The set of ρ is set to [0  0.01  0.05  0.1  1] and η is ﬁxed to 0.001. As
stated in Section 3.4  ARTSVM is less sensitive to the graph structure. Thus  we adopt a
simple way to construct the graph: for each data  the number of neighbors is set to 20 and
binary weighting is employed. In ARTSVM  the supervised  weakly semi-supervised  and
semi-supervised algorithms are based on implementation in LibSVM (www.csie.ntu.edu.
tw/~cjlin/libsvm/)  MOSEK (www.mosek.org)  and ∇TSVM (www.kyb.tuebingen.mpg.
de/bs/people/chapelle/lds/)  respectively. For the comparison algorithms  we adopt the
original authors’ own implementations.

Table 2 summarizes the classiﬁcation accuracy and the standard deviations of the proposed
ARTSVM method and other competing methods. We can draw several observations from

the results. First of all  we can clearly see that our proposed algorithm performs signif-
icantly better than the baseline SVM method across all the data sets. Note that some
very large deviations in SVM are mainly because the labeled data and the unlabeled data
may have quite diﬀerent distributions after the random sampling. On the other hand  the
unlabeled data capture the underlying distribution and help to correct such random error.
Comparing ARTSVM with other TSVM algorithms  we observe that ARTSVM achieves
the best performance in most cases. For example  for the digital image data sets  espe-
cially digit1  supervised learning usually works well and the advantages of TSVM are very
limited. However  the proposed ARTSVM outperforms both the supervised and other semi-
supervised algorithms. This indicates that the appropriate regularization from the unlabel
data improves the classiﬁcation performance.

Table 2: The classiﬁcation performance of Transductive SVMs on benchmark data sets.

Data Set ARTSVM ∇TSVM
79.44±3.63
usps
80.55±1.94
digit1
79.84±1.88
coil
ibm vs rest
76.83±2.11
95.42±0.95
pcmac
94.78±1.83
page
93.56±1.58
link
pagelink
96.53±1.84

81.30±4.04
82.10±2.11
81.70±2.10
78.04±1.44
95.50±0.88
94.65±1.19
94.27±0.97
97.31±0.68

SVM

CCCP

79.23±8.60
81.70±5.61
78.98±8.07
72.90±2.32
92.57±0.82
75.22±17.38
40.79±3.63
89.41±3.12

80.48±3.20
80.69±2.97
80.15±2.90
77.52±1.51
94.86±1.09
94.47±1.67
92.60±2.10
95.97±2.22

SVM-light
78.16±4.41
77.53±4.24
79.03±2.84
73.99±5.18
91.42±7.24
93.98±2.60
92.18±2.45
94.89±1.81

5 Conclusion

This paper presents a novel framework for semi-supervised learning from the perspective of
the regularization strength from the unlabeled data. In particular  for Transductive SVM 
we show that SVM and TSVM can be incorporated as special cases within this framework.
In more detail  the loss on the unlabeled data can essentially be regarded as an additional
regularizer for the decision boundary in TSVM. To control the regularization strength  we
introduce an alternative method of data-dependant regularization based on the principle of
manifold regularization. Empirical studies on benchmark data sets demonstrate that the
proposed framework is more eﬀective than the previous transductive algorithms and purely
supervised methods.

For future work  we plan to design a controlling strategy that is adaptive to data from
the perspective of low density assumption and manifold regularization of semi-supervised
learning. Finally  it is desirable to integrate the low density assumption and manifold
regularization into a uniﬁed framework.

Acknowledgement

The work was supported by the National Science Foundation (IIS-0643494)  Na-
tional Institute of Health (1R01GM079688-01)  Research Grants Council of Hong Kong
(CUHK4158/08E and CUHK4128/08E)  and MSRA (FY09-RES-OPP-103). It is also aﬃli-
ated with the MS-CUHK Joint Lab for Human-centric Computing & Interface Technologies.

References

[1] Mikhail Belkin  Partha Niyogi  and Vikas Sindhwani. Manifold regularization: A geometric
framework for learning from labeled and unlabeled examples. Journal of Machine Learning
Research  7:2399–2434  2006.

[2] Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph
mincuts. In ICML ’01: Proceedings of the 18th international conference on Machine learning 
pages 19–26. Morgan Kaufmann  San Francisco  CA  2001.

[3] O. Chapelle  B. Sch¨olkopf  and A. Zien  editors. Semi-Supervised Learning. MIT Press  Cam-

bridge  MA  2006.

[4] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Pro-
ceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics  pages
57–64  2005.

[5] Olivier Chapelle  Vikas Sindhwani  and Sathiya Keerthi. Branch and bound for semi-supervised
support vector machines. In B. Sch¨olkopf  J. Platt  and T. Hoﬀman  editors  Advances in Neural
Information Processing Systems 19. MIT Press  Cambridge  MA  2007.

[6] Olivier Chapelle  Vikas Sindhwani  and Sathiya S. Keerthi. Optimization techniques for semi-
supervised support vector machines. Journal of Machine Learning Research  9:203–233  2008.

[7] Ke Chen and Shihai Wang. Regularized boost for semi-supervised learning.

In J.C. Platt 
D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Sys-
tems 20  pages 281–288. MIT Press  Cambridge  MA  2008.

[8] Ronan Collobert  Fabian Sinz  Jason Weston  and L´eon Bottou. Large scale transductive

SVMs. Journal of Machine Learning Reseaerch  7:1687–1712  2006.

[9] S. C. H. Hoi  M. R. Lyu  and E. Y. Chang. Learning the uniﬁed kernel machines for classiﬁca-
tion. In Proceedings of Twentith International Conference on Knowledge Discovery and Data
Mining (KDD-2006)  pages 187–196  New York  NY  USA  2006. ACM Press.

[10] Steven C. H. Hoi  Rong Jin  and Michael R. Lyu. Learning nonparametric kernel matrices
from pairwise constraints. In ICML ’07: Proceedings of the 24th international conference on
Machine learning  pages 361–368  New York  NY  USA  2007. ACM.

[11] T. Joachims. Transductive learning via spectral graph partitioning. In ICML ’03: Proceedings

of the 20th international conference on Machine learning  pages 290–297  2003.

[12] Thorsten Joachims. Transductive inference for text classiﬁcation using support vector ma-
chines. In ICML ’99: Proceedings of the 16th international conference on Machine learning 
pages 200–209  San Francisco  CA  USA  1999. Morgan Kaufmann Publishers Inc.

[13] John Laﬀerty and Larry Wasserman. Statistical analysis of semi-supervised regression. In J.C.
Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing
Systems 20  pages 801–808. MIT Press  Cambridge  MA  2008.

[14] Hariharan Narayanan  Mikhail Belkin  and Partha Niyogi. On the relation between low density
separation  spectral clustering and graph cuts.
In B. Sch¨olkopf  J. Platt  and T. Hoﬀman 
editors  Advances in Neural Information Processing Systems 19  pages 1025–1032. MIT Press 
Cambridge  MA  2007.

[15] Vikas Sindhwani  S. Sathiya Keerthi  and Olivier Chapelle. Deterministic annealing for semi-
supervised kernel machines. In ICML ’06: Proceedings of the 23rd international conference on
Machine learning  pages 841–848  New York  NY  USA  2006. ACM Press.

[16] Aarti Singh  Robert Nowak  and Xiaojin Zhu. Unlabeled data: Now it helps  now it doesn’t. In
D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information
Processing Systems 21  pages 1513–1520. 2009.

[17] Junhui Wang  Xiaotong Shen  and Wei Pan. On eﬃcient large margin semisupervised learning:

Method and theory. Journal of Machine Learning Research  10:719–742  2009.

[18] Linli Xu and Dale Schuurmans. Unsupervised and semi-supervised multi-class support vector

machines. In AAAI  pages 904–910  2005.

[19] Zenglin Xu  Rong Jin  Jianke Zhu  Irwin King  and Michael R. Lyu. Eﬃcient convex relaxation
for transductive support vector machine. In J.C. Platt  D. Koller  Y. Singer  and S. Roweis 
editors  Advances in Neural Information Processing Systems 20  pages 1641–1648. MIT Press 
Cambridge  MA  2008.

[20] T. Zhang and R. Ando. Analysis of spectral kernel design based semi-supervised learning.
In Y. Weiss  B. Sch¨olkopf  and J. Platt  editors  Advances in Neural Information Processing
Systems (NIPS 18)  pages 1601–1608. MIT Press  Cambridge  MA  2006.

[21] Dengyong Zhou  Olivier Bousquet  Thomas Navin Lal  Jason Weston  and Bernhard Sch¨olkopf.
Learning with local and global consistency. In Sebastian Thrun  Lawrence Saul  and Bern-
hard Sch¨olkopf  editors  Advances in Neural Information Processing Systems 16. MIT Press 
Cambridge  MA  2004.

[22] X. Zhu  J. Kandola  Z. Ghahramani  and J. Laﬀerty. Nonparametric transforms of graph
kernels for semi-supervised learning. In Advances in Neural Information Processing Systems
(NIPS 17)  pages 1641–1648  Cambridge  MA  2005. MIT Press.

[23] Xiaojin Zhu. Semi-supervised learning literature survey. Technical report  Computer Sciences 

University of Wisconsin-Madison  2005.

,Zhengdong Lu
Hang Li
Chao Qu
Shie Mannor
Huan Xu
Yuan Qi
Le Song
Junwu Xiong