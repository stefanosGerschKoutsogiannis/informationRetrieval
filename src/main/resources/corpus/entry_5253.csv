2017,Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure,Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. Unfortunately  these techniques are unable to deal with stochastic perturbations of input data  induced for example by data augmentation. In such cases  the objective is no longer a finite sum  and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper  we introduce a variance reduction approach for these settings when the objective is composite and strongly convex. The convergence rate outperforms SGD with a typically much smaller constant factor  which depends on the variance of gradient estimates only due to perturbations on a single example.,Stochastic Optimization with Variance Reduction

for Inﬁnite Datasets with Finite Sum Structure

Alberto Bietti

Inria∗

Julien Mairal

Inria∗

alberto.bietti@inria.fr

julien.mairal@inria.fr

Abstract

Stochastic optimization algorithms with variance reduction have proven successful
for minimizing large ﬁnite sums of functions. Unfortunately  these techniques are
unable to deal with stochastic perturbations of input data  induced for example by
data augmentation. In such cases  the objective is no longer a ﬁnite sum  and the
main candidate for optimization is the stochastic gradient descent method (SGD).
In this paper  we introduce a variance reduction approach for these settings when
the objective is composite and strongly convex. The convergence rate outperforms
SGD with a typically much smaller constant factor  which depends on the variance
of gradient estimates only due to perturbations on a single example.

1

Introduction

Many supervised machine learning problems can be cast as the minimization of an expected loss
over a data distribution with respect to a vector x in Rp of model parameters. When an inﬁnite
amount of data is available  stochastic optimization methods such as SGD or stochastic mirror descent
algorithms  or their variants  are typically used (see [5  11  24  34]). Nevertheless  when the dataset is
ﬁnite  incremental methods based on variance reduction techniques (e.g.  [2  8  15  17  18  27  29])
have proven to be signiﬁcantly faster at solving the ﬁnite-sum problem

min

x∈RpnF (x) := f (x) + h(x) =

1
n

n

Xi=1

fi(x) + h(x)o 

(1)

where the functions fi are smooth and convex  and h is a simple convex penalty that need not be
differentiable such as the ℓ1 norm. A classical setting is fi(x) = ℓ(yi  x⊤ξi) + (µ/2)kxk2  where
(ξi  yi) is an example-label pair  ℓ is a convex loss function  and µ is a regularization parameter.

In this paper  we are interested in a variant of (1) where random perturbations of data are introduced 
which is a common scenario in machine learning. Then  the functions fi involve an expectation over
a random perturbation ρ  leading to the problem

min

x∈RpnF (x) :=

1
n

n

Xi=1

fi(x) + h(x)o. with

fi(x) = Eρ[ ˜fi(x  ρ)].

(2)

Unfortunately  variance reduction methods are not compatible with the setting (2)  since evaluating
a single gradient ∇fi(x) requires computing a full expectation. Yet  dealing with random pertur-
bations is of utmost interest; for instance  this is a key to achieve stable feature selection [23] 
improving the generalization error both in theory [33] and in practice [19  32]  obtaining stable
and robust predictors [36]  or using complex a priori knowledge about data to generate virtually

∗Univ. Grenoble Alpes  Inria  CNRS  Grenoble INP  LJK  38000 Grenoble  France

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Table 1: Iteration complexity of different methods for solving the objective (2) in terms of number of
iterations required to ﬁnd x such that E[f (x) − f (x∗)] ≤ ǫ. The complexity of N-SAGA [14] matches
the ﬁrst term of S-MISO but is asymptotically biased. Note that we always have the perturbation
noise variance σ2
tot and thus S-MISO improves on SGD both in the
ﬁrst term (linear convergence to a smaller ¯ǫ) and in the second (smaller constant in the asymptotic
rate). In many application cases  we also have σ2

p smaller than the total variance σ2

tot (see main text and Table 2).

p ≪ σ2

Method

Asymptotic error

Iteration complexity

SGD

0

N-SAGA [14]

µ !
ǫ0 = O σ2

p

S-MISO

0

O(cid:18) L

µ

log

1
¯ǫ

O(cid:18)(cid:18)n +
µ(cid:19) log

L

O (cid:18)n +

+

L

tot

1

σ2
tot

µǫ(cid:19) with ¯ǫ = O(cid:18) σ2
µ (cid:19)
ǫ(cid:19) with ǫ > ǫ0
µ(cid:19) log
µǫ! with ¯ǫ = O σ2
µ !

1
¯ǫ

σ2
p

+

p

larger datasets [19  26  30]. Injecting noise in data is also useful to hide gradient information for
privacy-aware learning [10].

Despite its importance  the optimization problem (2) has been littled studied and to the best of our
knowledge  no dedicated optimization method that is able to exploit the problem structure has been
developed so far. A natural way to optimize this objective when h = 0 is indeed SGD  but ignoring the
ﬁnite-sum structure leads to gradient estimates with high variance and slow convergence. The goal of
this paper is to introduce an algorithm for strongly convex objectives  called stochastic MISO  which
exploits the underlying ﬁnite sum using variance reduction. Our method achieves a faster convergence
rate than SGD  by removing the dependence on the gradient variance due to sampling the data points i
in {1  . . .   n}; the dependence remains only for the variance due to random perturbations ρ.

To the best of our knowledge  our method is the ﬁrst algorithm that interpolates naturally between in-
cremental methods for ﬁnite sums (when there are no perturbations) and the stochastic approximation
setting (when n = 1)  while being able to efﬁciently tackle the hybrid case.

Related work. Many optimization methods dedicated to the ﬁnite-sum problem (e.g.  [15  29])
have been motivated by the fact that their updates can be interpreted as SGD steps with unbiased
estimates of the full gradient  but with a variance that decreases as the algorithm approaches the
optimum [15]; on the other hand  vanilla SGD requires decreasing step-sizes to achieve this reduction
of variance  thereby slowing down convergence. Our work aims at extending these techniques to the
case where each function in the ﬁnite sum can only be accessed via a ﬁrst-order stochastic oracle.

Most related to our work  recent methods that use data clustering to accelerate variance reduction
techniques [3  14] can be seen as tackling a special case of (2)  where the expectations in fi are
replaced by empirical averages over points in a cluster. While N-SAGA [14] was originally not
designed for the stochastic context we consider  we remark that their method can be applied to (2).
Their algorithm is however asymptotically biased and does not converge to the optimum. On the other
hand  ClusterSVRG [3] is not biased  but does not support inﬁnite datasets. The method proposed
in [1] uses variance reduction in a setting where gradients are computed approximately  but the
algorithm computes a full gradient at every pass  which is not available in our stochastic setting.

Paper organization.
In Section 2  we present our algorithm for smooth objectives  and we analyze
its convergence in Section 3. For space limitation reasons  we present an extension to composite
objectives and non-uniform sampling in Appendix A. Section 4 is devoted to empirical results.

2 The Stochastic MISO Algorithm for Smooth Objectives

In this section  we introduce the stochastic MISO approach for smooth objectives (h = 0)  which
relies on the following assumptions:

• (A1) global strong convexity: f is µ-strongly convex;
• (A2) smoothness: ˜fi(·  ρ) is L-smooth for all i and ρ (i.e.  with L-Lipschitz gradients).

2

tot/σ2

Table 2: Estimated ratio σ2
p  which corresponds to the expected acceleration of S-MISO over
SGD. These numbers are based on feature vectors variance  which is closely related to the gradient
variance when learning a linear model. ResNet-50 denotes a 50 layer network [12] pre-trained on
the ImageNet dataset. For image transformations  the numbers are empirically evaluated from 100
different images  with 100 random perturbations for each image. R2
cluster) denotes
the average squared distance between pairs of points in the dataset (respectively  in a given cluster) 
following [14]. The settings for unsupervised CKN and Scattering are described in Section 4. More
details are given in the main text.

tot (respectively  R2

Type of perturbation Application case

Direct perturbation of
linear model features

Random image
transformations

Data clustering as in [3  14]
Additive Gaussian noise N (0  α2I)
Dropout with probability δ
Feature rescaling by s in U (1 − w  1 + w)
ResNet-50 [12]  color perturbation
ResNet-50 [12]  rescaling + crop
Unsupervised CKN [22]  rescaling + crop
Scattering [6]  gamma correction

tot/σ2
p

Estimated ratio σ2
≈
≈
≈
≈

cluster

R2
tot/R2
1 + 1/α2
1 + 1/δ
1 + 3/w2
21.9
13.6
9.6
9.8

Note that these assumptions are relaxed in Appendix A by supporting composite objectives and
by exploiting different smoothness parameters Li on each example  a setting where non-uniform
sampling of the training points is typically helpful to accelerate convergence (e.g.  [35]).

Complexity results. We now introduce the following quantity  which is essential in our analysis:

σ2
p :=

1
n

n

Xi=1

σ2
i   with σ2

i := Eρhk∇ ˜fi(x∗  ρ) − ∇fi(x∗)k2i  

where x∗ is the (unique) minimizer of f . The quantity σ2
p represents the part of the variance of the
gradients at the optimum that is due to the perturbations ρ. In contrast  another quantity of interest is
the total variance σ2

tot  which also includes the randomness in the choice of the index i  deﬁned as

tot = Ei ρ[k∇ ˜fi(x∗  ρ)k2] = σ2
σ2

p + Ei[k∇fi(x∗)k2]

(note that ∇f (x∗) = 0).

The relation between σ2

tot and σ2

p is obtained by simple algebraic manipulations.

The goal of our paper is to exploit the potential imbalance σ2
tot  occurring when perturbations
on input data are small compared to the sampling noise. The assumption is reasonable: given a data
point  selecting a different one should lead to larger variation than a simple perturbation. From a
theoretical point of view  the approach we propose achieves the iteration complexity presented in
Table 1  see also Appendix D and [4  5  24] for the complexity analysis of SGD. The gain over SGD
is of order σ2
p  which is also observed in our experiments in Section 4. We also compare against
the method N-SAGA; its convergence rate is similar to ours but suffers from a non-zero asymptotic
error.

p ≪ σ2

tot/σ2

Motivation from application cases. One clear framework of application is the data clustering
scenario already investigated in [3  14]. Nevertheless  we will focus on less-studied data augmentation
settings that lead instead to true stochastic formulations such as (2). First  we consider learning a
linear model when adding simple direct manipulations of feature vectors  via rescaling (multiplying
each entry vector by a random scalar)  Dropout  or additive Gaussian noise  in order to improve the
generalization error [33] or to get more stable estimators [23]. In Table 2  we present the potential
gain over SGD in these scenarios. To do that  we study the variance of perturbations applied to
a feature vector ξ. Indeed  the gradient of the loss is proportional to ξ  which allows us to obtain
good estimates of the ratio σ2
p  as we observed in our empirical study of Dropout presented
in Section 4. Whereas some perturbations are friendly for our method such as feature rescaling (a
rescaling window of [0.9  1.1] yields for instance a huge gain factor of 300)  a large Dropout rate
would lead to less impressive acceleration (e.g.  a Dropout with δ = 0.5 simply yields a factor 2).

tot/σ2

Second  we also consider more interesting domain-driven data perturbations such as classical im-
age transformations considered in computer vision [26  36] including image cropping  rescaling 
brightness  contrast  hue  and saturation changes. These transformations may be used to train a linear

3

Algorithm 1 S-MISO for smooth objectives

Input: step-size sequence (αt)t≥1;
initialize x0 = 1
for t = 1  . . . do

i )i=1 ... n;

i for some (z0

nPi z0
i =((1 − αt)zt−1

zt−1
i

zt

 

xt =

1
n

n

Xi=1

zt
i = xt−1 +

1
n

end for

(zt

it − zt−1

it

).

Sample an index it uniformly at random  a perturbation ρt  and update

i + αt(xt−1 − 1

µ ∇ ˜fit (xt−1  ρt)) 

if i = it
otherwise.

(3)

(4)

classiﬁer on top of an unsupervised multilayer image model such as unsupervised CKNs [22] or
the scattering transform [6]. It may also be used for retraining the last layer of a pre-trained deep
neural network: given a new task unseen during the full network training and given limited amount
of training data  data augmentation may be indeed crucial to obtain good prediction and S-MISO
can help accelerate learning in this setting. These scenarios are also studied in Table 2  where the
experiment with ResNet-50 involving random cropping and rescaling produces 224 × 224 images
from 256 × 256 ones. For these scenarios with realistic perturbations  the potential gain varies from
10 to 20.

Description of stochastic MISO. We are now in shape to present our method  described in Algo-
rithm 1. Without perturbations and with a constant step-size  the algorithm resembles the MISO/Finito
algorithms [9  18  21]  which may be seen as primal variants of SDCA [28  29]. Speciﬁcally  MISO
is not able to deal with our stochastic objective (2)  but it may address the deterministic ﬁnite-sum
problem (1). It is part of a larger body of optimization methods that iteratively build a model of the
objective function  typically a lower or upper bound on the objective that is easier to optimize; for
instance  this strategy is commonly adopted in bundle methods [13  25].

More precisely  MISO assumes that each fi is strongly convex and builds a model using lower bounds
Dt(x) = 1

i=1 dt

nPn

i(x)  where each dt
i is a quadratic lower bound on fi of the form
µ
kx − zt
2

i 2 − µhx  zt

dt
i(x) = ct

i k2 = ct

kxk2.

i 1 +

i i +

µ
2

These lower bounds are updated during the algorithm using strong convexity lower bounds at xt−1 of
the form lt

i(x) = fi(xt−1) + h∇fi(xt−1)  x − xt−1i + µ
(x) + αtlt

2 kx − xt−1k2 ≤ fi(x):
i(x) 

(5)

(6)

dt

i(x) =(cid:26)(1 − αt)dt−1

dt−1
i

(x) 

i

if i = it
otherwise 

which corresponds to an update of the quantity zt
i :

zt

i =((1 − αt)zt−1

zt−1
i

 

i + αt(xt−1 − 1

µ ∇fit (xt−1)) 

if i = it
otherwise.

The next iterate is then computed as xt = arg minx Dt(x)  which is equivalent to (4). The original
MISO/Finito algorithms use αt = 1 under a “big data” condition on the sample size n [9  21] 
while the theory was later extended in [18] to relax this condition by supporting smaller constant
steps αt = α  leading to an algorithm that may be interpreted as a primal variant of SDCA (see [28]).

Note that when fi is an expectation  it is hard to obtain such lower bounds since the gradient
∇fi(xt−1) is not available in general. For this reason  we have introduced S-MISO  which can
exploit approximate lower bounds to each fi using gradient estimates  by letting the step-sizes αt
decrease appropriately as commonly done in stochastic approximation. This leads to update (3).

i (y) = supx x⊤y −fi(x).
Separately  SDCA [29] considers the Fenchel conjugates of fi  deﬁned by f ∗
When fi is an expectation  f ∗
i is not available in closed form in general  nor are its gradients  and in
fact exploiting stochastic gradient estimates is difﬁcult in the duality framework. In contrast  [28]
gives an analysis of SDCA in the primal  aka. “without duality”  for smooth ﬁnite sums  and our
work extends this line of reasoning to the stochastic approximation and composite settings.

4

Relationship with SGD in the smooth case. The link between S-MISO in the non-composite
setting and SGD can be seen by rewriting the update (4) as

where

vt := xt−1 −

1
µ

xt = xt−1 +

1
n

(zt

it

it − zt−1

) = xt−1 +

αt
n
∇ ˜fit (xt−1  ρt) − zt−1

.

it

vt 

(7)

Note that E[vt|Ft−1] = − 1
µ ∇f (xt−1)  where Ft−1 contains all information up to iteration t; hence 
the algorithm can be seen as an instance of the stochastic gradient method with unbiased gradients 
which was a key motivation in SVRG [15] and later in other variance reduction algorithms [8  28]. It
is also worth noting that in the absence of a ﬁnite-sum structure (n = 1)  we have zt−1
= xt−1; hence
our method becomes identical to SGD  up to a redeﬁnition of step-sizes. In the composite case (see
Appendix A)  our approach yields a new algorithm that resembles regularized dual averaging [34].

it

Memory requirements and handling of sparse datasets. The algorithm requires storing the
vectors (zt
i )i=1 ... n  which takes the same amount of memory as the original dataset and which
is therefore a reasonable requirement in many practical cases. In the case of sparse datasets  it is
fair to assume that random perturbations applied to input data preserve the sparsity patterns of the
original vectors  as is the case  e.g.  when applying Dropout to text documents described with bag-of-
words representations [33]. If we further assume the typical setting where the µ-strong convexity
comes from an ℓ2 regularizer: ˜fi(x  ρ) = φi(x⊤ξρ
i is the (sparse) perturbed
example and φi encodes the loss  then the update (3) can be written as

i ) + (µ/2)kxk2  where ξρ

zt

i =((1 − αt)zt−1

zt−1
i

 

i − αt

µ φ′

i(x⊤

t−1ξρt

i )ξρt
i  

if i = it
otherwise 

which shows that for every index i  the vector zt
throughout the algorithm (assuming the initialization z0
update (4) has the same cost since vt = zt
it

− zt−1

it

is also sparse.

i preserves the same sparsity pattern as the examples ξρ
i
i = 0)  making the update (3) efﬁcient. The

Limitations and alternative approaches. Since our algorithm is uniformly better than SGD in
terms of iteration complexity  its main limitation is in terms of memory storage when the dataset
cannot ﬁt into memory (remember that the memory cost of S-MISO is the same as the input dataset).
In these huge-scale settings  SGD should be preferred; this holds true in fact for all incremental
methods when one cannot afford to perform more than one (or very few) passes over the data. Our
paper focuses instead on non-huge datasets  which are those beneﬁting most from data augmentation.

We note that a different approach to variance reduction like SVRG [15] is able to trade off storage
requirements for additional full gradient computations  which would be desirable in some situations.
However  we were not able to obtain any decreasing step-size strategy that works for these methods 
both in theory and practice  leaving us with constant step-size approaches as in [1  14] that either
maintain a non-zero asymptotic error  or require dynamically reducing the variance of gradient
estimates. One possible way to explain this difﬁculty is that SVRG and SAGA [8] “forget” past
gradients for a given example i  while S-MISO averages them in (3)  which seems to be a technical
key to make it suitable to stochastic approximation. Nevertheless  the question of whether it is
possible to trade-off storage with computation in a setting like ours is open and of utmost interest.

3 Convergence Analysis of S-MISO

We now study the convergence properties of the S-MISO algorithm. For space limitation reasons 
all proofs are provided in Appendix B. We start by deﬁning the problem-dependent quantities
i := x∗ − 1
z∗

µ ∇fi(x∗)  and then introduce the Lyapunov function

Ct =

1
2

kxt − x∗k2 +

αt
n2

n

Xi=1

kzt

i − z∗

i k2.

(8)

Proposition 1 gives a recursion on Ct  obtained by upper-bounding separately its two terms  and
ﬁnding coefﬁcients to cancel out other appearing quantities when relating Ct to Ct−1. To this end  we
borrow elements of the convergence proof of SDCA without duality [28]; our technical contribution
is to extend their result to the stochastic approximation and composite (see Appendix A) cases.

5

Proposition 1 (Recursion on Ct). If (αt)t≥1 is a positive and non-increasing sequence satisfying

with κ = L/µ  then Ct obeys the recursion

α1 ≤ min(cid:26) 1

2

 

n

2(2κ − 1)(cid:27)  

E[Ct] ≤(cid:16)1 −

αt

n (cid:17) E[Ct−1] + 2(cid:16) αt

n (cid:17)2 σ2

µ2 .

p

(9)

(10)

We now state the main convergence result  which provides the expected rate O(1/t) on Ct based on
decreasing step-sizes  similar to [5] for SGD. Note that convergence of objective function values is
directly related to that of the Lyapunov function Ct via smoothness:

E[f (xt) − f (x∗)] ≤

L
2

E(cid:2)kxt − x∗k2(cid:3) ≤ L E[Ct].

(11)

Theorem 2 (Convergence of Lyapunov function). Let the sequence of step-sizes (αt)t≥1 be deﬁned
by αt = 2n

γ+t with γ ≥ 0 such that α1 satisﬁes (9). For all t ≥ 0  it holds that

E[Ct] ≤

ν

γ + t + 1

where

ν := max( 8σ2

µ2   (γ + 1)C0) .

p

(12)

Choice of step-sizes in practice. Naturally  we would like ν to be small  in particular independent
of the initial condition C0 and equal to the ﬁrst term in the deﬁnition (12). We would like the
dependence on C0 to vanish at a faster rate than O(1/t)  as it is the case in variance reduction
algorithms on ﬁnite sums. As advised in [5] in the context of SGD  we can initially run the algorithm
with a constant step-size ¯α and exploit this linear convergence regime until we reach the level of
noise given by σp  and then start decaying the step-size. It is easy to see that by using a constant
step-size ¯α  Ct converges near a value ¯C := 2¯ασ2

p/nµ2. Indeed  Eq. (10) with αt = ¯α yields

E[Ct − ¯C] ≤(cid:16)1 −

n(cid:17) E[Ct−1 − ¯C].

¯α

Thus  we can reach a precision C ′
start decaying step-sizes as in Theorem 2 with γ large enough so that α1 = ¯α  we have

¯α log C0/¯ǫ) iterations. Then  if we

0 with E[C ′

0] ≤ ¯ǫ := 2 ¯C in O( n

(γ + 1) E[C ′

0] ≤ (γ + 1)¯ǫ = 8σ2

p/µ2 

making both terms in (12) smaller than or equal to ν = 8σ2
an initial step-size ¯α given by (9)  the ﬁnal work complexity for reaching E[kxt − x∗k2] ≤ ǫ is

p/µ2. Considering these two phases  with

O(cid:18)(cid:18)n +

L

µ(cid:19) log

C0

µ2ǫ! .
¯ǫ (cid:19) + O σ2

p

(13)

We can then use (11) in order to obtain the complexity for reaching E[f (xt) − f (x∗)] ≤ ǫ. Note that
following this step-size strategy was found to be very effective in practice (see Section 4).

Acceleration by iterate averaging. When one is interested in the convergence in function values 
the complexity (13) combined with (11) yields O(Lσ2
p/µ2ǫ)  which can be problematic for ill-
conditioned problems (large condition number L/µ). The following theorem presents an iterate
averaging scheme which brings the complexity term down to O(σ2
p/µǫ)  which appeared in Table 1.
Theorem 3 (Convergence under iterate averaging). Let the step-size sequence (αt)t≥1 be deﬁned by

We have

where

αt =

2n

γ + t

for γ ≥ 1 s.t. α1 ≤ min(cid:26) 1

2

 

n

4(2κ − 1)(cid:27) .

E[f (¯xT ) − f (x∗)] ≤

2µγ(γ − 1)C0
T (2γ + T − 1)

+

16σ2
p

µ(2γ + T − 1)

 

¯xT :=

2

T (2γ + T − 1)

(γ + t)xt.

T −1

Xt=0

6

Figure 1: Impact of conditioning for data augmentation on STL-10 (controlled by µ  where µ = 10−4
gives the best accuracy). Values of the loss are shown on a logarithmic scale (1 unit = factor 10).
η = 0.1 satisﬁes the theory for all methods  and we include curves for larger step-sizes η = 1. We
omit N-SAGA for η = 1 because it remains far from the optimum. For the scattering representation 
the problem we study is ℓ1-regularized  and we use the composite algorithm of Appendix A.

Figure 2: Re-training of the last layer of a pre-trained ResNet 50 model  on a small dataset with
random color perturbations (for different values of µ).

The proof uses a similar telescoping sum technique to [16]. Note that if T ≫ γ  the ﬁrst term 
which depends on the initial condition C0  decays as 1/T 2 and is thus dominated by the second
term. Moreover  if we start averaging after an initial phase with constant step-size ¯α  we can consider
C0 ≈ 4¯ασ2
p/nµ2. In the ill-conditioned regime  taking ¯α = α1 = 2n/(γ + 1) as large as allowed
by (9)  we have γ of the order of κ = L/µ ≫ 1. The full convergence rate then becomes

E[f (¯xT ) − f (x∗)] ≤ O σ2

p

µ(γ + T )(cid:16)1 +

γ

T(cid:17)! .

When T is large enough compared to γ  this becomes O(σ2

p/µT )  leading to a complexity O(σ2

p/µǫ).

4 Experiments

We present experiments comparing S-MISO with SGD and N-SAGA [14] on four different scenarios 
in order to demonstrate the wide applicability of our method: we consider an image classiﬁcation
dataset with two different image representations and random transformations  and two classiﬁcation
tasks with Dropout regularization  one on genetic data  and one on (sparse) text data. Figures 1 and 3
show the curves for an estimate of the training objective using 5 sampled perturbations per example.
The plots are shown on a logarithmic scale  and the values are compared to the best value obtained
among the different methods in 500 epochs. The strong convexity constant µ is the regularization
parameter. For all methods  we consider step-sizes supported by the theory as well as larger step-sizes
that may work better in practice. Our C++/Cython implementation of all methods considered in this
section is available at https://github.com/albietz/stochs.

Choices of step-sizes. For both S-MISO and SGD  we use the step-size strategy mentioned in
Section 3 and advised by [5]  which we have found to be most effective among many heuristics

7

050100150200250300350400450epochs10-510-410-310-210-1100f - f*STL-10 ckn  µ=10−3S-MISO η=0.1S-MISO η=1.0N-SAGA η=0.1SGD η=0.1SGD η=1.0050100150200250300350400450epochs10-410-310-210-1100f - f*STL-10 ckn  µ=10−4050100150200250300350400epochs10-310-210-1100f - f*STL-10 ckn  µ=10−5050100150200250300350400epochs10-510-410-310-210-1100F - F*STL-10 scattering  µ=10−3050100150200250300350400epochs10-510-410-310-210-1100101F - F*STL-10 scattering  µ=10−4050100150200250300350400epochs10-410-310-210-1100101F - F*STL-10 scattering  µ=10−5050100150200250300350400epochs10-610-510-410-310-2f - f*ResNet50  µ=10−2S-MISO η=0.1S-MISO η=1.0N-SAGA η=0.1SGD η=0.1SGD η=1.0050100150200250300350400epochs10-710-610-510-410-310-210-1100f - f*ResNet50  µ=10−3050100150200250300350400epochs10-510-410-310-210-1100f - f*ResNet50  µ=10−4Figure 3: Impact of perturbations controlled by the Dropout rate δ. The gene data is ℓ2-normalized;
hence  we consider similar step-sizes as Figure 1. The IMDB dataset is highly heterogeneous;
thus  we also include non-uniform (NU) sampling variants of Appendix A. For uniform sampling 
theoretical step-sizes perform poorly for all methods; thus  we show a larger tuned step-size η = 10.

we have tried: we initially keep the step-size constant (controlled by a factor η ≤ 1 in the ﬁgures)
for 2 epochs  and then start decaying as αt = C/(γ + t)  where C = 2n for S-MISO  C = 2/µ
for SGD  and γ is chosen large enough to match the previous constant step-size. For N-SAGA  we
maintain a constant step-size throughout the optimization  as suggested in the original paper [14].
The factor η shown in the ﬁgures is such that η = 1 corresponds to an initial step-size nµ/(L − µ)
for S-MISO (from (19) in the uniform case) and 1/L for SGD and N-SAGA (with ¯L instead of L in
the non-uniform case when using the variant of Appendix A).

Image classiﬁcation with “data augmentation”. The success of deep neural networks is often
limited by the availability of large amounts of labeled images. When there are many unlabeled
images but few labeled ones  a common approach is to train a linear classiﬁer on top of a deep
network learned in an unsupervised manner  or pre-trained on a different task (e.g.  on the ImageNet
dataset). We follow this approach on the STL-10 dataset [7]  which contains 5K training images
from 10 classes and 100K unlabeled images  using a 2-layer unsupervised convolutional kernel
network [22]  giving representations of dimension 9 216. The perturbation consists of randomly
cropping and scaling the input images. We use the squared hinge loss in a one-versus-all setting. The
vector representations are ℓ2-normalized such that we may use the upper bound L = 1 + µ for the
smoothness constant. We also present results on the same dataset using a scattering representation [6]
of dimension 21 696  with random gamma corrections (raising all pixels to the power γ  where γ is
chosen randomly around 1). For this representation  we add an ℓ1 regularization term and use the
composite variant of S-MISO presented in Appendix A.

Figure 1 shows convergence results on one training fold (500 images)  for different values of µ 
allowing us to study the behavior of the algorithms for different condition numbers. The low variance
induced by data transformations allows S-MISO to reach suboptimality that is orders of magnitude
smaller than SGD after the same number of epochs. Note that one unit on these plots corresponds to
one order of magnitude in the logarithmic scale. N-SAGA initially reaches a smaller suboptimality
than SGD  but quickly gets stuck due to the bias in the algorithm  as predicted by the theory [14] 
while S-MISO and SGD continue to converge to the optimum thanks to the decreasing step-sizes. The
best validation accuracy for both representations is obtained for µ ≈ 10−4 (middle column)  and we
observed relative gains of up to 1% from using data augmentation. We computed empirical variances
of the image representations for these two strategies  which are closely related to the variance in
gradient estimates  and observed these transformations to account for about 10% of the total variance.

Figure 2 shows convergence results when training the last layer of a 50-layer Residual network [12]
that has been pre-trained on ImageNet. Here  we consider the common scenario of leveraging a deep
model trained on a large dataset as a feature extractor in order to learn a new classiﬁer on a different
small dataset  where it would be difﬁcult to train such a model from scratch. To simulate this setting 
we consider a binary classiﬁcation task on a small dataset of 100 images of size 256x256 taken
from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012  which we crop to

8

050100150200250300350400epochs10-510-410-310-210-1100f - f*gene dropout  δ = 0.30S-MISO η=0.1S-MISO η=1.0SGD η=0.1SGD η=1.0N-SAGA η=0.1N-SAGA η=1.0050100150200250300350400epochs10-610-510-410-310-210-1100f - f*gene dropout  δ = 0.10050100150200250300350400epochs10-710-610-510-410-310-210-1100f - f*gene dropout  δ = 0.01050100150200250300350400epochs10-410-310-210-1100f - f*imdb dropout  δ = 0.30S-MISO-NU η=1.0S-MISO η=10.0SGD-NU η=1.0SGD η=10.0N-SAGA η=10.0050100150200250300350400epochs10-510-410-310-210-1100f - f*imdb dropout  δ = 0.10050100150200250300350400epochs10-710-610-510-410-310-210-1100f - f*imdb dropout  δ = 0.01224x224 before performing random adjustments to brightness  saturation  hue and contrast. As in the
STL-10 experiments  the gains of S-MISO over other methods are of about one order of magnitude in
suboptimality  as predicted by Table 2.

Dropout on gene expression data. We trained a binary logistic regression model on the breast
cancer dataset of [31]  with different Dropout rates δ  i.e.  where at every iteration  each coordinate ξj
of a feature vector ξ is set to zero independently with probability δ and to ξj/(1 − δ) otherwise. The
dataset consists of 295 vectors of dimension 8 141 of gene expression data  which we normalize
in ℓ2 norm. Figure 3 (top) compares S-MISO with SGD and N-SAGA for three values of δ  as a
way to control the variance of the perturbations. We include a Dropout rate of 0.01 to illustrate the
impact of δ on the algorithms and study the inﬂuence of the perturbation variance σ2
p  even though
this value of δ is less relevant for the task. The plots show very clearly how the variance induced by
the perturbations affects the convergence of S-MISO  giving suboptimality values that may be orders
of magnitude smaller than SGD. This behavior is consistent with the theoretical convergence rate
established in Section 3 and shows that the practice matches the theory.

Dropout on movie review sentiment analysis data. We trained a binary classiﬁer with a squared
hinge loss on the IMDB dataset [20] with different Dropout rates δ. We use the labeled part of
the IMDB dataset  which consists of 25K training and 250K testing movie reviews  represented as
89 527-dimensional sparse bag-of-words vectors. In contrast to the previous experiments  we do not
normalize the representations  which have great variability in their norms  in particular  the maximum
Lipschitz constant across training points is roughly 100 times larger than the average one. Figure 3
(bottom) compares non-uniform sampling versions of S-MISO (see Appendix A) and SGD (see
Appendix D) with their uniform sampling counterparts as well as N-SAGA. Note that we use a large
step-size η = 10 for the uniform sampling algorithms  since η = 1 was signiﬁcantly slower for
all methods  likely due to outliers in the dataset. In contrast  the non-uniform sampling algorithms
required no tuning and just use η = 1. The curves clearly show that S-MISO-NU has a much faster
convergence in the initial phase  thanks to the larger step-size allowed by non-uniform sampling  and
later converges similarly to S-MISO  i.e.  at a much faster rate than SGD when the perturbations are
small. The value of µ used in the experiments was chosen by cross-validation  and the use of Dropout
gave improvements in test accuracy from 88.51% with no dropout to 88.68 ± 0.03% with δ = 0.1
and 88.86 ± 0.11% with δ = 0.3 (based on 10 different runs of S-MISO-NU after 400 epochs).

Finally  we also study the effect of the iterate averaging scheme of Theorem 3 in Appendix E.

Acknowledgements

This work was supported by a grant from ANR (MACARON project under grant number ANR-
14-CE23-0003-01)  by the ERC grant number 714381 (SOLARIS project)  and by the MSR-Inria
joint center.

References

[1] M. Achab  A. Guilloux  S. Gaïffas  and E. Bacry. SGD with Variance Reduction beyond Empirical Risk

Minimization. arXiv:1510.04822  2015.

[2] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Symposium on the

Theory of Computing (STOC)  2017.

[3] Z. Allen-Zhu  Y. Yuan  and K. Sridharan. Exploiting the Structure: Stochastic Gradient Methods Using

Raw Clusters. In Advances in Neural Information Processing Systems (NIPS)  2016.

[4] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine

learning. In Advances in Neural Information Processing Systems (NIPS)  2011.

[5] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization Methods for Large-Scale Machine Learning.

arXiv:1606.04838  2016.

[6] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE transactions on pattern analysis

and machine intelligence (PAMI)  35(8):1872–1886  2013.

[7] A. Coates  H. Lee  and A. Y. Ng. An Analysis of Single-Layer Networks in Unsupervised Feature Learning.

In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2011.

9

[8] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems (NIPS) 
2014.

[9] A. Defazio  J. Domke  and T. S. Caetano. Finito: A faster  permutable incremental gradient method for big

data problems. In International Conference on Machine Learning (ICML)  2014.

[10] J. C. Duchi  M. I. Jordan  and M. J. Wainwright. Privacy aware learning. In Advances in Neural Information

Processing Systems (NIPS)  2012.

[11] J. C. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. Journal of

Machine Learning Research (JMLR)  10:2899–2934  2009.

[12] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

[13] J.-B. Hiriart-Urruty and C. Lemaréchal. Convex analysis and minimization algorithms I: Fundamentals.

Springer science & business media  1993.

[14] T. Hofmann  A. Lucchi  S. Lacoste-Julien  and B. McWilliams. Variance Reduced Stochastic Gradient

Descent with Neighbors. In Advances in Neural Information Processing Systems (NIPS)  2015.

[15] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems (NIPS)  2013.

[16] S. Lacoste-Julien  M. Schmidt  and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv:1212.2002  2012.

[17] G. Lan and Y. Zhou. An optimal randomized incremental gradient method. Mathematical Programming 

2017.

[18] H. Lin  J. Mairal  and Z. Harchaoui. A Universal Catalyst for First-Order Optimization. In Advances in

Neural Information Processing Systems (NIPS)  2015.

[19] G. Loosli  S. Canu  and L. Bottou. Training invariant support vector machines using selective sampling. In

Large Scale Kernel Machines  pages 301–320. MIT Press  Cambridge  MA.  2007.

[20] A. L. Maas  R. E. Daly  P. T. Pham  D. Huang  A. Y. Ng  and C. Potts. Learning word vectors for sentiment
analysis. In The 49th Annual Meeting of the Association for Computational Linguistics (ACL)  pages
142–150. Association for Computational Linguistics  2011.

[21] J. Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine

Learning. SIAM Journal on Optimization  25(2):829–855  2015.

[22] J. Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. In Advances in

Neural Information Processing Systems (NIPS)  2016.

[23] N. Meinshausen and P. Bühlmann. Stability selection. Journal of the Royal Statistical Society: Series B

(Statistical Methodology)  72(4):417–473  2010.

[24] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust Stochastic Approximation Approach to

Stochastic Programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[25] Y. Nesterov. Introductory Lectures on Convex Optimization. Springer  2004.

[26] M. Paulin  J. Revaud  Z. Harchaoui  F. Perronnin  and C. Schmid. Transformation pursuit for image
classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2014.

[27] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.

Mathematical Programming  162(1):83–112  2017.

[28] S. Shalev-Shwartz. SDCA without Duality  Regularization  and Individual Convexity. In International

Conference on Machine Learning (ICML)  2016.

[29] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research (JMLR)  14:567–599  2013.

10

[30] P. Y. Simard  Y. A. LeCun  J. S. Denker  and B. Victorri. Transformation Invariance in Pattern Recognition
— Tangent Distance and Tangent Propagation. In G. B. Orr and K.-R. Müller  editors  Neural Networks:
Tricks of the Trade  number 1524 in Lecture Notes in Computer Science  pages 239–274. Springer Berlin
Heidelberg  1998.

[31] M. J. van de Vijver et al. A Gene-Expression Signature as a Predictor of Survival in Breast Cancer. New

England Journal of Medicine  347(25):1999–2009  Dec. 2002.

[32] L. van der Maaten  M. Chen  S. Tyree  and K. Q. Weinberger. Learning with marginalized corrupted

features. In International Conference on Machine Learning (ICML)  2013.

[33] S. Wager  W. Fithian  S. Wang  and P. Liang. Altitude Training: Strong Bounds for Single-layer Dropout.

In Advances in Neural Information Processing Systems (NIPS)  2014.

[34] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research (JMLR)  11:2543–2596  2010.

[35] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization  24(4):2057–2075  2014.

[36] S. Zheng  Y. Song  T. Leung  and I. Goodfellow. Improving the robustness of deep neural networks via
stability training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2016.

11

,Atsushi Shibagaki
Yoshiki Suzuki
Masayuki Karasuyama
Ichiro Takeuchi
David Harwath
Antonio Torralba
James Glass
Alberto Bietti
Julien Mairal