2015,Learning Continuous Control Policies by Stochastic Value Gradients,We present a unified framework for learning continuous control policies usingbackpropagation. It supports stochastic control by treating stochasticity in theBellman equation as a deterministic function of exogenous noise. The productis a spectrum of general policy gradient algorithms that range from model-freemethods with value functions to model-based methods without value functions.We use learned models but only require observations from the environment insteadof observations from model-predicted trajectories  minimizing the impactof compounded model errors. We apply these algorithms first to a toy stochasticcontrol problem and then to several physics-based control problems in simulation.One of these variants  SVG(1)  shows the effectiveness of learning models  valuefunctions  and policies simultaneously in continuous domains.,Learning Continuous Control Policies by

Stochastic Value Gradients

Nicolas Heess⇤  Greg Wayne⇤  David Silver  Timothy Lillicrap  Yuval Tassa  Tom Erez

Google DeepMind

{heess  gregwayne  davidsilver  countzero  tassa  etom}@google.com

⇤These authors contributed equally.

Abstract

We present a uniﬁed framework for learning continuous control policies using
backpropagation.
It supports stochastic control by treating stochasticity in the
Bellman equation as a deterministic function of exogenous noise. The product
is a spectrum of general policy gradient algorithms that range from model-free
methods with value functions to model-based methods without value functions.
We use learned models but only require observations from the environment in-
stead of observations from model-predicted trajectories  minimizing the impact
of compounded model errors. We apply these algorithms ﬁrst to a toy stochastic
control problem and then to several physics-based control problems in simulation.
One of these variants  SVG(1)  shows the effectiveness of learning models  value
functions  and policies simultaneously in continuous domains.

Introduction

1
Policy gradient algorithms maximize the expectation of cumulative reward by following the gradient
of this expectation with respect to the policy parameters. Most existing algorithms estimate this gra-
dient in a model-free manner by sampling returns from the real environment and rely on a likelihood
ratio estimator [32  26]. Such estimates tend to have high variance and require large numbers of
samples or  conversely  low-dimensional policy parameterizations.
A second approach to estimate a policy gradient relies on backpropagation instead of likelihood ratio
methods. If a differentiable environment model is available  one can link together the policy  model 
and reward function to compute an analytic policy gradient by backpropagation of reward along a
trajectory [18  11  6  9]. Instead of using entire trajectories  one can estimate future rewards using a
learned value function (a critic) and compute policy gradients from subsequences of trajectories. It
is also possible to backpropagate analytic action derivatives from a Q-function to compute the policy
gradient without a model [31  21  23]. Following Fairbank [8]  we refer to methods that compute
the policy gradient through backpropagation as value gradient methods.
In this paper  we address two limitations of prior value gradient algorithms. The ﬁrst is that  in
contrast to likelihood ratio methods  value gradient algorithms are only suitable for training deter-
ministic policies. Stochastic policies have several advantages: for example  they can be beneﬁcial for
partially observed problems [24]; they permit on-policy exploration; and because stochastic policies
can assign probability mass to off-policy trajectories  we can train a stochastic policy on samples
from an experience database in a principled manner. When an environment model is used  value
gradient algorithms have also been critically limited to operation in deterministic environments. By
exploiting a mathematical tool known as “re-parameterization” that has found recent use for gener-
ative models [20  12]  we extend the scope of value gradient algorithms to include the optimization
of stochastic policies in stochastic environments. We thus describe our framework as Stochastic
Value Gradient (SVG) methods. Secondly  we show that an environment dynamics model  value
function  and policy can be learned jointly with neural networks based only on environment in-
teraction. Learned dynamics models are often inaccurate  which we mitigate by computing value
gradients along real system trajectories instead of planned ones  a feature shared by model-free

1

methods [32  26]. This substantially reduces the impact of model error because we only use models
to compute policy gradients  not for prediction  combining advantages of model-based and model-
free methods with fewer of their drawbacks.
We present several algorithms that range from model-based to model-free methods  ﬂexibly combin-
ing models of environment dynamics with value functions to optimize policies in stochastic or de-
terministic environments. Experimentally  we demonstrate that SVG methods can be applied using
generic neural networks with tens of thousands of parameters while making minimal assumptions
about plants or environments. By examining a simple stochastic control problem  we show that
SVG algorithms can optimize policies where model-based planning and likelihood ratio methods
cannot. We provide evidence that value function approximation can compensate for degraded mod-
els  demonstrating the increased robustness of SVG methods over model-based planning. Finally 
we use SVG algorithms to solve a variety of challenging  under-actuated  physical control problems 
including swimming of snakes  reaching  tracking  and grabbing with a robot arm  fall-recovery for
a monoped  and locomotion for a planar cheetah and biped.
2 Background
We consider discrete-time Markov Decision Processes (MDPs) with continuous states and actions
and denote the state and action at time step t by st 2 RNS and at 2 RNA  respectively. The MDP has
an initial state distribution s0 ⇠ p0(·)  a transition distribution st+1 ⇠ p(·|st  at)  and a (potentially
time-varying) reward function rt = r(st  at  t).1 We consider time-invariant stochastic policies
a ⇠ p(·|s; ✓)  parameterized by ✓. The goal of policy optimization is to ﬁnd policy parameters ✓ that
maximize the expected sum of future rewards. We optimize either ﬁnite-horizon or inﬁnite-horizon

sums  i.e.  J(✓) = EhPT

t=0 trt✓i or J(✓) = E⇥P1t=0 trt✓⇤ where  2 [0  1] is a discount

factor.2 When possible  we represent a variable at the next time step using the “tick” notation  e.g. 
s0   st+1.
In what follows  we make extensive use of the state-action-value Q-function and state-value V-
function.

Qt(s  a) = E"X⌧ =t

⌧tr⌧st = s  at = a ✓# ; V t(s) = E"X⌧ =t

⌧tr⌧st = s ✓# .

For ﬁnite-horizon problems  the value functions are time-dependent  e.g.  V 0   V t+1(s0)  and for
inﬁnite-horizon problems the value functions are stationary  V 0   V (s0). The relevant meaning
should be clear from the context. The state-value function can be expressed recursively using the
stochastic Bellman equation

V t(s) =Z rt + Z V t+1(s0)p(s0|s  a)ds0 p(a|s; ✓)da.

(1)

(2)

We abbreviate partial differentiation using subscripts  gx   @g(x  y)/@x.
3 Deterministic value gradients
The deterministic Bellman equation takes the form V (s) = r(s  a)+V 0(f (s  a)) for a deterministic
model s0 = f (s  a) and deterministic policy a = ⇡(s; ✓). Differentiating the equation with respect
to the state and policy yields an expression for the value gradient

Vs = rs + ra⇡s + V 0s0(fs + fa⇡s) 
V✓ = ra⇡✓ + V 0s0fa⇡✓ + V 0✓ .

(3)
(4)
In eq. 4  the term V 0✓ arises because the total derivative includes policy gradient contributions from
subsequent time steps (full derivation in Appendix A). For a purely model-based formalism  these
equations are used as a pair of coupled recursions that  starting from the termination of a trajectory 
proceed backward in time to compute the gradient of the value function with respect to the state
✓ returns the total policy gradient. When a state-value function is used
and policy parameters. V 0

1We make use of a time-varying reward function only in one problem to encode a terminal reward.
2< 1 for the inﬁnite-horizon case.

2

after one step in the recursion  ra⇡✓ + V 0s0fa⇡✓ directly expresses the contribution of the current
time step to the policy gradient. Summing these gradients over the trajectory gives the total policy
gradient. When a Q-function is used  the per-time step contribution to the policy gradient takes the
form Qa⇡✓.
4 Stochastic value gradients
One limitation of the gradient computation in eqs. 3 and 4 is that the model and policy must be
deterministic. Additionally  the accuracy of the policy gradient V✓ is highly sensitive to modeling
errors. We introduce two critical changes: First  in section 4.1  we transform the stochastic Bellman
equation (eq. 2) to permit backpropagating value information in a stochastic setting. This also
enables us to compute gradients along real trajectories  not ones sampled from a model  making the
approach robust to model error  leading to our ﬁrst algorithm “SVG(1) ” described in section 4.2.
Second  in section 4.3  we show how value function critics can be integrated into this framework 
leading to the algorithms “SVG(1)” and “SVG(0)”  which expand the Bellman recursion for 1 and
0 steps  respectively. Value functions further increase robustness to model error and extend our
framework to inﬁnite-horizon control.
4.1 Differentiating the stochastic Bellman equation

Re-parameterization of distributions Our goal is to backpropagate through the stochastic Bell-
man equation. To do so  we make use of a concept called “re-parameterization”  which permits us to
compute derivatives of deterministic and stochastic models in the same way. A very simple example
of re-parameterization is to write a conditional Gaussian density p(y|x) = N (y|µ(x)  2(x)) as the
function y = µ(x) + (x)⇠  where ⇠ ⇠N (0  1). From this point of view  one produces samples
procedurally by ﬁrst sampling ⇠  then deterministically constructing y. Here  we consider condi-
tional densities whose samples are generated by a deterministic function of an input noise variable
and other conditioning variables: y = f (x ⇠ )  where ⇠ ⇠ ⇢(·)  a ﬁxed noise distribution. Rich
density models can be expressed in this form [20  12]. Expectations of a function g(y) become
Ep(y|x)g(y) =R g(f (x ⇠ ))⇢(⇠)d⇠.

The advantage of working with re-parameterized distributions is that we can now obtain a simple
Monte-Carlo estimator of the derivative of an expectation with respect to x:

rxEp(y|x)g(y) = E⇢(⇠)gyfx ⇡

1
M

.

(5)

MXi=1

gyfx⇠=⇠i

In contrast to likelihood ratio-based Monte Carlo estimators  rx log p(y|x)g(y)  this formula makes
direct use of the Jacobian of g.
Re-parameterization of the Bellman equation We now re-parameterize the Bellman equation.
When re-parameterized  the stochastic policy takes the form a = ⇡(s ⌘ ; ✓)  and the stochastic
environment the form s0 = f (s  a ⇠ ) for noise variables ⌘ ⇠ ⇢(⌘) and ⇠ ⇠ ⇢(⇠)  respectively.
Inserting these functions into eq. (2) yields

V (s) = E⇢(⌘)r(s ⇡ (s ⌘ ; ✓)) + E⇢(⇠)⇥V 0(f (s ⇡ (s ⌘ ; ✓) ⇠ ))⇤.

Differentiating eq. 6 with respect to the current state s and policy parameters ✓ gives

(6)

(7)

(8)

Vs = E⇢(⌘)rs + ra⇡s + E⇢(⇠)V 0s0(fs + fa⇡s) 
V✓ = E⇢(⌘)ra⇡✓ + E⇢(⇠)⇥V 0s0fa⇡✓ + V 0✓⇤.

We are interested in controlling systems with a priori unknown dynamics. Consequently  in the
following  we replace instances of f or its derivatives with a learned model ˆf.
Gradient evaluation by planning A planning method to compute a gradient estimate is to com-
pute a trajectory by running the policy in loop with a model while sampling the associated noise
variables  yielding a trajectory ⌧ = (s1 ⌘ 1  a1 ⇠ 1  s2 ⌘ 2  a2 ⇠ 2  . . . ). On this sampled trajectory  a
Monte-Carlo estimate of the policy gradient can be computed by the backward recursions:

3

vs = [rs + ra⇡s + v0s0(ˆfs + ˆfa⇡s)]⌘ ⇠  
ˆfa⇡✓ + v0✓)]⌘ ⇠  

v✓ = [ra⇡✓ + (v0s0

where have written lower-case v to emphasize that the quantities are one-sample estimates3  and

“x” means “evaluated at x”.

Gradient evaluation on real trajectories An important advantage of stochastic over determinis-
tic models is that they can assign probability mass to observations produced by the real environment.
In a deterministic formulation  there is no principled way to account for mismatch between model
predictions and observed trajectories. In this case  the policy and environment noise (⌘  ⇠ ) that pro-
duced the observed trajectory are considered unknown. By an application of Bayes’ rule  which we
explain in Appendix B  we can rewrite the expectations in equations 7 and 8 given the observations
(s  a  s0) as

(9)
(10)

(11)

(12)

Vs = Ep(a|s)Ep(s0|s a)Ep(⌘ ⇠|s a s0)rs + ra⇡+V 0s0(ˆfs + ˆfa⇡s) 
V✓ = Ep(a|s)Ep(s0|s a)Ep(⌘ ⇠|s a s0)ra⇡✓ + (V 0s0
ˆfa⇡✓ + V 0✓ ) 

where we can now replace the two outer expectations with samples derived from interaction with
the real environment. In the special case of additive noise  s0 = ˆf (s  a) + ⇠  it is possible to use
a deterministic model to compute the derivatives (ˆfs  ˆfa). The noise’s inﬂuence is restricted to the
gradient of the value of the next state  V 0s0  and does not affect the model Jacobian. If we consider it
desirable to capture more complicated environment noise  we can use a re-parameterized generative
model and infer the missing noise variables  possibly by sampling from p(⌘  ⇠|s  a  s0).
4.2 SVG(1)
SVG(1) computes value gradients by backward recursions on ﬁnite-horizon trajectories. After
every episode  we train the model  ˆf  followed by the policy  ⇡. We provide pseudocode for this in
Algorithm 1 but discuss further implementation details in section 5 and in the experiments.

for t = 0 to T do

Apply control a = ⇡(s ⌘ ; ✓)  ⌘ ⇠ ⇢(⌘)
Insert (s  a  r  s0) into D
end for
Train generative model ˆf using D
v0s = 0 (ﬁnite-horizon)
v0✓ = 0 (ﬁnite-horizon)
for t = T down to 0 do

Algorithm 2 SVG(1) with Replay
1: Given empty experience database D
2: for t = 0 to 1 do
3:
Apply control ⇡(s ⌘ ; ✓)  ⌘ ⇠ ⇢(⌘)
Observe r  s0
4:
Insert (s  a  r  s0) into D
5:
6:
// Model and critic updates
Train generative model ˆf using D
7:
Train value function ˆV using D (Alg. 4)
8:
9:
// Policy update
Sample (sk  ak  rk  sk+1) from D (k  t)
10:
w = p(ak|sk;✓t)
11:
p(ak|sk;✓k)
12:
Infer ⇠k|(sk  ak  sk+1) and ⌘k|(sk  ak)
v✓ = w(ra +  ˆV 0s0
13:
14:
Apply gradient-based update using v✓
15: end for

Algorithm 1 SVG(1)
1: Given empty experience database D
2: for trajectory = 0 to 1 do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for
4.3 SVG(1) and SVG(0)
In our framework  we may learn a parametric estimate of the expected value ˆV (s; ⌫) (critic) with
parameters ⌫. The derivative of the critic value with respect to the state  ˆVs  can be used in place
of the sample gradient estimate given in eq. (9). The critic can reduce the variance of the gradient
estimates because ˆV approximates the expectation of future rewards while eq. (9) provides only a

ˆfa)⇡✓⌘k ⇠k

Infer ⇠|(s  a  s0) and ⌘|(s  a)
v✓ = [ra⇡✓ + (v0s0

ˆfa⇡✓ + v0✓)]⌘ ⇠
vs = [rs + ra⇡s + v0s0 (ˆfs + ˆfa⇡s)]⌘ ⇠

end for
Apply gradient-based update using v0
✓

3In the ﬁnite-horizon formulation  the gradient calculation starts at the end of the trajectory for which the
s . After the recursion  the total derivative of the value

only terms remaining in eq. (9) are vT
function with respect to the policy parameters is given by v0

s ⇡ rT

✓  which is a one-sample estimate of r✓J.

s + rT

a ⇡T

4

single-trajectory estimate. Additionally  the value function can be used at the end of an episode to
approximate the inﬁnite-horizon policy gradient. Finally  eq. (9) involves the repeated multiplication
of Jacobians of the approximate model ˆfs  ˆfa. Just as model error can compound in forward planning 
model gradient error can compound during backpropagation. Furthermore  SVG(1) is on-policy.
That is  after each episode  a single gradient-based update is made to the policy  and the policy
optimization does not revisit those trajectory data again. To increase data-efﬁciency  we construct
an off-policy  experience replay [15  29] algorithm that uses models and value functions  SVG(1)
with Experience Replay (SVG(1)-ER). This algorithm also has the advantage that it can perform an
inﬁnite-horizon computation.
To construct an off-policy estimator  we perform importance-weighting of the current policy distri-
bution with respect to a proposal distribution  q(s  a):
ˆV✓ = Eq(s a)Ep(s0|s a)Ep(⌘ ⇠|s a s0)

p(a|s; ✓)

(13)

q(a|s) ra⇡✓ +  ˆV 0s

ˆfa⇡✓.

Speciﬁcally  we maintain a database with tuples of past state transitions (sk  ak  rk  sk+1). Each
proposal drawn from q is a sample of a tuple from the database. At time t  the importance-weight
w   p/q = p(ak|sk;✓t)
p(ak|sk ✓k)  where ✓k comprise the policy parameters in use at the historical time step k.
We do not importance-weight the marginal distribution over states q(s) generated by a policy; this
is widely considered to be intractable.
Similarly  we use experience replay for value function learning. Details can be found in Appendix
C. Pseudocode for the SVG(1) algorithm with Experience Replay is in Algorithm 2.
We also provide a model-free stochastic value gradient algorithm  SVG(0) (Algorithm 3 in the Ap-
pendix). This algorithm is very similar to SVG(1) and is the stochastic analogue of the recently intro-
duced Deterministic Policy Gradient algorithm (DPG) [23  14  4]. Unlike DPG  instead of assuming

a deterministic policy  SVG(0) estimates the derivative around the policy noise Ep(⌘)⇥Qa⇡✓⌘⇤.4

This  for example  permits learning policy noise variance. The relative merit of SVG(1) versus
SVG(0) depends on whether the model or value function is easier to learn and is task-dependent.
We expect that model-based algorithms such as SVG(1) will show the strongest advantages in mul-
titask settings where the system dynamics are ﬁxed  but the reward function is variable. SVG(1)
performed well across all experiments  including ones introducing capacity constraints on the value
function and model. SVG(1)-ER demonstrated a signiﬁcant advantage over all other tested algo-
rithms.
5 Model and value learning
We can use almost any kind of differentiable  generative model. In our work  we have parameterized
the models as neural networks. Our framework supports nonlinear state- and action-dependent noise 
notable properties of biological actuators. For example  this can be described by the parametric
form ˆf (s  a ⇠ ) = ˆµ(s  a) + ˆ(s  a)⇠. Model learning amounts to a purely supervised problem based
on observed state transitions. Our model and policy training occur jointly. There is no “motor-
babbling” period used to identify the model. As new transitions are observed  the model is trained
ﬁrst  followed by the value function (for SVG(1))  followed by the policy. To ensure that the model
does not forget information about state transitions  we maintain an experience database and cull
batches of examples from the database for every model update. Additionally  we model the state-
change by s0 = ˆf (s  a ⇠ ) + s and have found that constructing models as separate sub-networks per
predicted state dimension improved model quality signiﬁcantly.
Our framework also permits a variety of means to learn the value function models. We can use
temporal difference learning [25] or regression to empirical episode returns. Since SVG(1) is model-
based  we can also use Bellman residual minimization [3]. In practice  we used a version of “ﬁtted”
policy evaluation. Pseudocode is available in Appendix C  Algorithm 4.
6 Experiments
We tested the SVG algorithms in two sets of experiments. In the ﬁrst set of experiments (section
6.1)  we test whether evaluating gradients on real environment trajectories and value function ap-

4Note that ⇡ is a function of the state and noise variable.

5

Figure 1: From left to right: 7-Link Swimmer; Reacher; Gripper; Monoped; Half-Cheetah; Walker

proximation can reduce the impact of model error. In our second set (section 6.2)  we show that
SVG(1) can be applied to several complicated  multidimensional physics environments involving
contact dynamics (Figure 1) in the MuJoCo simulator [28]. Below we only brieﬂy summarize the
main properties of each environment: further details of the simulations can be found in Appendix
D and supplement. In all cases  we use generic  2 hidden-layer neural networks with tanh activa-
tion functions to represent models  value functions  and policies. A video montage is available at
https://youtu.be/PYdL7bcn_cM.
6.1 Analyzing SVG
Gradient evaluation on real trajectories vs. planning To demonstrate the difﬁculty of planning
with a stochastic model  we ﬁrst present a very simple control problem for which SVG(1) easily
learns a control policy but for which an otherwise identical planner fails entirely. Our example is
based on a problem due to [16]. The policy directly controls the velocity of a point-mass “hand”
on a 2D plane. By means of a spring-coupling  the hand exerts a force on a ball mass; the ball
additionally experiences a gravitational force and random forces (Gaussian noise). The goal is to
bring hand and ball into one of two randomly chosen target conﬁgurations with a relevant reward
being provided only at the ﬁnal time step.
With simulation time step 0.01s  this demands controlling and backpropagating the distal reward
along a trajectory of 1  000 steps. Because this experiment has a non-stationary  time-dependent
value function  this problem also favors model-based value gradients over methods using value
functions. SVG(1) easily learns this task  but the planner  which uses trajectories from the model 
shows little improvement. The planner simulates trajectories using the learned stochastic model
and backpropagates along those simulated trajectories (eqs. 9 and 10) [18]. The extremely long
time-horizon lets prediction error accumulate and thus renders roll-outs highly inaccurate  leading
to much worse ﬁnal performance (c.f. Fig. 2  left).5
Robustness to degraded models and value functions We investigated the sensitivity of SVG(1)
and SVG(1) to the quality of the learned model on Swimmer. Swimmer is a chain body with multiple
links immersed in a ﬂuid environment with drag forces that allow the body to propel itself [5  27].
We build chains of 3  5  or 7 links  corresponding to 10  14  or 18-dimensional state spaces with 2 
4  or 6-dimensional action spaces. The body is initialized in random conﬁgurations with respect to
a central goal location. Thus  to solve the task  the body must turn to re-orient and then produce an
undulation to move to the goal.
To assess the impact of model quality  we learned to control a link-3 swimmer with SVG(1) and
SVG(1) while varying the capacity of the network used to model the environment (5  10  or 20
hidden units for each state dimension subnetwork (Appendix D); i.e.  in this task we intentionally
shrink the neural network model to investigate the sensitivity of our methods to model inaccuracy.
While with a high capacity model (20 hidden units per state dimension)  both SVG(1) and SVG(1)
successfully learn to solve the task  the performance of SVG(1) drops signiﬁcantly as model ca-
pacity is reduced (c.f. Fig. 3  middle). SVG(1) still works well for models with only 5 hidden units 
and it also scales up to 5 and 7-link versions of the swimmer (Figs. 3  right and 4  left). To compare
SVG(1) to conventional model-free approaches  we also tested a state-of-the-art actor-critic algo-
rithm that learns a V -function and updates the policy using the TD-error  = r + V 0  V as an
estimate of the advantage  yielding the policy gradient v✓ = r✓ log ⇡ [30]. (SVG(1) and the AC
algorithm used the same code for learning V .) SVG(1) outperformed the model-free approach in
the 3-  5-  and 7-link swimmer tasks (c.f. Fig. 3  left  right; Fig. 4  top left). In ﬁgure panels 2 
middle  3  right  and 4  left column  we show that experience replay for the policy can improve the
data efﬁciency and performance of SVG(1).

5We also tested REINFORCE on this problem but achieved very poor results due to the long horizon.

6

Figure 2: Left: Backpropagation through a model along observed stochastic trajectories is able
to optimize a stochastic policy in a stochastic environment  but an otherwise equivalent planning
algorithm that simulates the transitions with a learned stochastic model makes little progress due to
compounding model error. Middle: SVG and DPG algorithms on cart-pole. SVG(1)-ER learns the
fastest. Right: When the value function capacity is reduced from 200 hidden units in the ﬁrst layer to
100 and then again to 50  SVG(1) exhibits less performance degradation than the Q-function-based
DPG  presumably because the dynamics model contains auxiliary information about the Q function.

Figure 3: Left: For a 3-link swimmer  with relatively simple dynamics  the compared methods
yield similar results and possibly a slight advantage to the purely model-based SVG(1). Middle:
However  as the environment model’s capacity is reduced from 20 to 10 then to 5 hidden units
per state-dimension subnetwork  SVG(1) dramatically deteriorates  whereas SVG(1) shows undis-
turbed performance. Right: For a 5-link swimmer  SVG(1)-ER learns faster and asymptotes at
higher performance than the other tested algorithms.

Similarly  we tested the impact of varying the capacity of the value function approximator (Fig. 2 
right) on a cart-pole. The V-function-based SVG(1) degrades less severely than the Q-function-
based DPG presumably because it computes the policy gradient with the aid of the dynamics model.
6.2 SVG in complex environments
In a second set of experiments we demonstrated that SVG(1)-ER can be applied to several chal-
lenging physical control problems with stochastic  non-linear  and discontinuous dynamics due to
contacts. Reacher is an arm stationed within a walled box with 6 state dimensions and 3 action
dimensions and the (x  y) coordinates of a target site  giving 8 state dimensions in total. In 4-Target
Reacher  the site was randomly placed at one of the four corners of the box  and the arm in a random
conﬁguration at the beginning of each trial. In Moving-Target Reacher  the site moved at a random-
ized speed and heading in the box with reﬂections at the walls. Solving this latter problem implies
that the policy has generalized over the entire work space. Gripper augments the reacher arm with a
manipulator that can grab a ball in a randomized position and return it to a speciﬁed site. Monoped
has 14 state dimensions  4 action dimensions  and ground contact dynamics. The monoped begins
falling from a height and must remain standing. Additionally  we apply Gaussian random noise
to the torques controlling the joints with a standard deviation of 5% of the total possible actuator
strength at all points in time  reducing the stability of upright postures. Half-Cheetah is a planar cat
robot designed to run based on [29] with 18 state dimensions and 6 action dimensions. Half-Cheetah
has a version with springs to aid balanced standing and a version without them. Walker is a planar
biped  based on the environment from [22].
Results Figure 4 shows learning curves for several repeats for each of the tasks. We found that
in all cases SVG(1) solved the problem well; we provide videos of the learned policies in the sup-
plemental material. The 4-target reacher reliably ﬁnished at the target site  and in the tracking task
followed the moving target successfully. SVG(1)-ER has a clear advantage on this task as also borne
out in the cart-pole and swimmer experiments. The cheetah gaits varied slightly from experiment to
experiment but in all cases made good forward progress. For the monoped  the policies were able
to balance well beyond the 200 time steps of training episodes and were able to resist signiﬁcantly

7

HandCartpoleCartpoleSwimmer-5Swimmer-3Swimmer-3Figure 4: Across several different domains  SVG(1)-ER reliably optimizes policies  clearly settling
into similar local optima. On the 4-target Reacher  SVG(1)-ER shows a noticeable efﬁciency and
performance gain relative to the other algorithms.

higher adversarial noise levels than used during training (up to 25% noise). We were able to learn
gripping and walking behavior  although walking policies that achieved similar reward levels did not
always exhibit equally good walking phenotypes.

7 Related work

Writing the noise variables as exogenous inputs to the system to allow direct differentiation with
respect to the system state (equation 7) is a known device in control theory [10  7] where the model is
given analytically. The idea of using a model to optimize a parametric policy around real trajectories
is presented heuristically in [17] and [1] for deterministic policies and models. Also in the limit of
deterministic policies and models  the recursions we have derived in Algorithm 1 reduce to those
of [2]. Werbos deﬁnes an actor-critic algorithm called Heuristic Dynamic Programming that uses a
deterministic model to roll-forward one step to produce a state prediction that is evaluated by a value
function [31]. Deisenroth et al. have used Gaussian process models to compute policy gradients that
are sensitive to model-uncertainty [6]  and Levine et al. have optimized impressive policies with the
aid of a non-parametric trajectory optimizer and locally-linear models [13]. Our work in contrast
has focused on using global  neural network models conjoined to value function approximators.

8 Discussion

We have shown that two potential problems with value gradient methods  their reliance on planning
and restriction to deterministic models  can be exorcised  broadening their relevance to reinforce-
ment learning. We have shown experimentally that the SVG framework can train neural network
policies in a robust manner to solve interesting continuous control problems. The framework in-
cludes algorithm variants beyond the ones tested in this paper  for example  ones that combine a
value function with k steps of back-propagation through a model (SVG(k)). Augmenting SVG(1)
with experience replay led to the best results  and a similar extension could be applied to any SVG(k).
Furthermore  we did not harness sophisticated generative models of stochastic dynamics  but one
could readily do so  presenting great room for growth.

Acknowledgements We thank Arthur Guez  Danilo Rezende  Hado van Hasselt  John Schulman  Jonathan
Hunt  Nando de Freitas  Martin Riedmiller  Remi Munos  Shakir Mohamed  and Theophane Weber for helpful
discussions and John Schulman for sharing his walker model.

8

Monoped2D-WalkerGripperAvg. reward (arbitrary units)4-Target ReacherHalf-CheetahAvg. reward (arbitrary units)Swimmer-7References
[1] P. Abbeel  M. Quigley  and A. Y. Ng. Using inaccurate models in reinforcement learning. In ICML  2006.
[2] C. G. Atkeson. Efﬁcient robust policy optimization. In ACC  2012.
[3] L. Baird. Residual algorithms: Reinforcement learning with function approximation. In ICML  1995.
[4] D. Balduzzi and M. Ghifary. Compatible value gradients for reinforcement learning of continuous deep

policies. arXiv preprint arXiv:1509.03005  2015.

[5] R. Coulom. Reinforcement learning using neural networks  with applications to motor control. PhD

thesis  Institut National Polytechnique de Grenoble-INPG  2002.

[6] M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efﬁcient approach to policy search.

In (ICML)  2011.

[7] M. Fairbank. Value-gradient learning. PhD thesis  City University London  2014.
[8] M. Fairbank and E. Alonso. Value-gradient learning. In IJCNN  2012.
[9] I. Grondman. Online Model Learning Algorithms for Actor-Critic Control. PhD thesis  TU Delft  Delft

University of Technology  2015.

[10] D. H. Jacobson and D. Q. Mayne. Differential dynamic programming. 1970.
[11] M. I. Jordan and D. E. Rumelhart. Forward models: Supervised learning with a distal teacher. Cognitive

science  16(3):307–354  1992.

[12] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114  2013.
[13] S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown

dynamics. In NIPS  2014.

[14] T. P. Lillicrap  J. J. Hunt  A. Pritzel  N. Heess  T. Erez  Y. Tassa  D. Silver  and D. Wierstra. Continuous

control with deep reinforcement learning. arXiv preprint arXiv:1509.02971  2015.

[15] L.-J. Lin. Self-improving reactive agents based on reinforcement learning  planning and teaching. Ma-

chine learning  8(3-4):293–321  1992.

[16] R. Munos. Policy gradient in continuous time. Journal of Machine Learning Research  7:771–791  2006.
[17] K. S. Narendra and K. Parthasarathy. Identiﬁcation and control of dynamical systems using neural net-

works. IEEE Transactions on Neural Networks  1(1):4–27  1990.

[18] D. H. Nguyen and B. Widrow. Neural networks for self-learning control systems. IEEE Control Systems

Magazine  10(3):18–23  1990.

[19] R. Pascanu  T. Mikolov  and Y. Bengio. On the difﬁculty of training recurrent neural networks. In ICML 

2013.

[20] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in

deep generative models. In ICML  2014.

[21] Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforcement

learning method. In Machine Learning: ECML 2005  pages 317–328. Springer  2005.

[22] J. Schulman  S. Levine  P. Moritz  M. I. Jordan  and P. Abbeel. Trust region policy optimization. CoRR 

abs/1502.05477  2015.

[23] D. Silver  G. Lever  N. Heess  T. Degris  D. Wierstra  and M. Riedmiller. Deterministic policy gradient

algorithms. In ICML  2014.

[24] S. P. Singh. Learning without state-estimation in partially observable Markovian decision processes. In

ICML  1994.

[25] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning  3(1):9–

44  1988.

[26] R.S. Sutton  D. A. McAllester  S. P. Singh  and Y. Mansour. Policy gradient methods for reinforcement

learning with function approximation. In NIPS  1999.

[27] Y. Tassa  T. Erez  and W.D. Smart. Receding horizon differential dynamic programming. In NIPS  2008.
[28] E. Todorov  T. Erez  and Y. Tassa. Mujoco: A physics engine for model-based control. In IROS  2012.
[29] P. Wawrzy´nski. A cat-like robot real-time learning to run. In Adaptive and Natural Computing Algorithms 

pages 380–390. Springer  2009.

[30] P. Wawrzy´nski. Real-time reinforcement learning by sequential actor–critics and experience replay. Neu-

ral Networks  22(10):1484–1497  2009.

[31] P. J Werbos. A menu of designs for reinforcement learning over time. Neural networks for control  pages

67–95  1990.

[32] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.

Machine learning  8(3-4):229–256  1992.

9

,Nicolas Heess
Gregory Wayne
David Silver
Timothy Lillicrap
Tom Erez
Yuval Tassa
Ben Poole
Subhaneil Lahiri
Maithra Raghu
Jascha Sohl-Dickstein
Surya Ganguli
Oisín Moran
Piergiorgio Caramazza
Daniele Faccio
Roderick Murray-Smith