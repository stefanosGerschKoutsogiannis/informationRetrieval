2009,Replicated Softmax: an Undirected Topic Model,We show how to model documents as bags of words using family of two-layer  undirected graphical models. Each member of the family has the same number of binary hidden units but a different number of ``softmax visible units. All of the softmax units in all of the models in the family share the same weights to the binary hidden units. We describe efficient inference and learning procedures for such a family. Each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than Latent Dirichlet Allocation for modeling the log probabilities of held-out documents. The low-dimensional topic vectors learned by the undirected family are also much better than LDA topic vectors for retrieving documents that are similar to a query document. The learned topics are more general than those found by LDA because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word.,Replicated Softmax: an Undirected Topic Model

Ruslan Salakhutdinov

Brain and Cognitive Sciences and CSAIL

Massachusetts Institute of Technology

Geoffrey Hinton

Department of Computer Science

University of Toronto

rsalakhu@mit.edu

hinton@cs.toronto.edu

Abstract

We introduce a two-layer undirected graphical model  called a “Replicated Soft-
max”  that can be used to model and automatically extract low-dimensional latent
semantic representations from a large unstructured collection of documents. We
present efﬁcient learning and inference algorithms for this model  and show how a
Monte-Carlo based method  Annealed Importance Sampling  can be used to pro-
duce an accurate estimate of the log-probability the model assigns to test data.
This allows us to demonstrate that the proposed model is able to generalize much
better compared to Latent Dirichlet Allocation in terms of both the log-probability
of held-out documents and the retrieval accuracy.

1 Introduction

Probabilistic topic models [2  9  6] are often used to analyze and extract semantic topics from large
text collections. Many of the existing topic models are based on the assumption that each document
is represented as a mixture of topics  where each topic deﬁnes a probability distribution over words.
The mixing proportions of the topics are document speciﬁc  but the probability distribution over
words  deﬁned by each topic  is the same across all documents.

All these models can be viewed as graphical models in which latent topic variables have directed
connections to observed variables that represent words in a document. One major drawback is that
exact inference in these models is intractable  so one has to resort to slow or inaccurate approxima-
tions to compute the posterior distribution over topics. A second major drawback  that is shared by
all mixture models  is that these models can never make predictions for words that are sharper than
the distributions predicted by any of the individual topics. They are unable to capture the essential
idea of distributed representations which is that the distributions predicted by individual active fea-
tures get multiplied together (and renormalized) to give the distribution predicted by a whole set of
active features. This allows individual features to be fairly general but their intersection to be much
more precise. For example  distributed representations allow the topics “government”  ”maﬁa” and
”playboy” to combine to give very high probability to a word “Berlusconi” that is not predicted
nearly as strongly by each topic alone.

To date  there has been very little work on developing topic models using undirected graphical mod-
els. Several authors [4  17] used two-layer undirected graphical models  called Restricted Boltzmann
Machines (RBMs)  in which word-count vectors are modeled as a Poisson distribution. While these
models are able to produce distributed representations of the input and perform well in terms of re-
trieval accuracy  they are unable to properly deal with documents of different lengths  which makes
learning very unstable and hard. This is perhaps the main reason why these potentially powerful
models have not found their application in practice. Directed models  on the other hand  can eas-
ily handle unobserved words (by simply ignoring them)  which allows them to easily deal with
different-sized documents. For undirected models marginalizing over unobserved variables is gen-
erally a non-trivial operation  which makes learning far more difﬁcult. Recently  [13] attempted to
ﬁx this problem by proposing a Constrained Poisson model that would ensure that the mean Poisson

1

rates across all words sum up to the length of the document. While the parameter learning has been
shown to be stable  the introduced model no longer deﬁnes a proper probability distribution over the
word counts.

In the next section we introduce a “Replicated Softmax” model. The model can be efﬁciently trained
using Contrastive Divergence  it has a better way of dealing with documents of different lengths  and
computing the posterior distribution over the latent topic values is easy. We will also demonstrate
that the proposed model is able to generalize much better compared to a popular Bayesian mixture
model  Latent Dirichlet Allocation (LDA) [2]  in terms of both the log-probability on previously
unseen documents and the retrieval accuracy.

2 Replicated Softmax: A Generative Model of Word Counts

Consider modeling discrete visible units v using a restricted Boltzmann machine  that has a two-
layer architecture as shown in Fig. 1. Let v ∈ {1  ...  K}D  where K is the dictionary size and D
is the document size  and let h ∈ {0  1}F be binary stochastic hidden topic features. Let V be a
i = 1 if visible unit i takes on kth value. We deﬁne the energy
K × D observed binary matrix with vk
of the state {V  h} as follows:

E(V  h) = −

D

F

K

Xi=1

Xj=1

Xk=1

W k

ijhjvk

i −

D

K

Xi=1

Xk=1

vk
i bk

i −

F

Xj=1

hjaj 

(1)

ij is a symmetric interaction term between visible
where {W  a  b} are the model parameters: W k
unit i that takes on value k  and hidden feature j  bk
i is the bias of unit i that takes on value k  and aj
is the bias of hidden feature j (see Fig. 1). The probability that the model assigns to a visible binary
matrix V is:

P (V) =

1

Z Xh

exp (−E(V  h))  Z = XV Xh

exp (−E(V  h)) 

(2)

where Z is known as the partition function or normalizing constant. The conditional distributions
are given by softmax and logistic functions:

p(vk

i = 1|h) =

p(hj = 1|V) = σ aj +

j=1 hjW k
ij )
j=1 hjW q

exp (bk

i +PF
PK
i +PF
q=1 exp(cid:0)bq
Xk=1
Xi=1

K

D

vk
i W k

ij!  

ij(cid:1)

(3)

(4)

where σ(x) = 1/(1 + exp(−x)) is the logistic function.
Now suppose that for each document we create a separate RBM with as many softmax units as there
are words in the document. Assuming we can ignore the order of the words  all of these softmax units
can share the same set of weights  connecting them to binary hidden units. Consider a document
that contains D words. In this case  we deﬁne the energy of the state {V  h} to be:

E(V  h) = −

F

K

Xj=1

Xk=1

W k

j hj ˆvk −

ˆvkbk − D

K

Xk=1

hjaj 

F

Xj=1

(5)

i=1 vk

where ˆvk =PD

i denotes the count for the kth word. Observe that the bias terms of the hidden
units are scaled up by the length of the document. This scaling is crucial and allows hidden topic
units to behave sensibly when dealing with documents of different lengths.
Given a collection of N documents {Vn}N
parameters W takes the form:

n=1  the derivative of the log-likelihood with respect to

1
N

N

Xn=1

∂ log P (Vn)

∂W k
j

where EPdata [·] denotes an expectation with respect to the data distribution Pdata(h  V) =
p(h|V)Pdata(V)  with Pdata(V) = 1

= EPdata(cid:2)ˆvkhj(cid:3) − EPModel(cid:2)ˆvkhj(cid:3)  
N Pn δ(V − Vn) representing the empirical distribution 

2

Latent Topics

Latent Topics

W1 W2

W2

W1

W2

W1

W1

W2

W2

W1

W2

h

W1

v

Observed Softmax Visibles

Multinomial Visible

Figure 1: Replicated Softmax model. The top layer represents a vector h of stochastic  binary topic features
and and the bottom layer represents softmax visible units v. All visible units share the same set of weights 
connecting them to binary hidden units. Left: The model for a document containing two and three words.
Right: A different interpretation of the Replicated Softmax model  in which D softmax units with identical
weights are replaced by a single multinomial unit which is sampled D times.

and EPModel [·] is an expectation with respect to the distribution deﬁned by the model. Exact maxi-
mum likelihood learning in this model is intractable because exact computation of the expectation
EPModel [·] takes time that is exponential in min{D  F }  i.e the number of visible or hidden units. To
avoid computing this expectation  learning is done by following an approximation to the gradient of
a different objective function  called the “Contrastive Divergence” (CD) ([7]):

∆W k

j = α(cid:18)EPdata(cid:2)ˆvkhj(cid:3) − EPT (cid:2)ˆvkhj(cid:3)(cid:19) 

(6)

where α is the learning rate and PT represents a distribution deﬁned by running the Gibbs chain 
initialized at the data  for T full steps. The special bipartite structure of RBM’s allows for quite an
efﬁcient Gibbs sampler that alternates between sampling the states of the hidden units independently
given the states of the visible units  and vise versa (see Eqs. 3  4). Setting T = ∞ recovers maximum
likelihood learning.

The weights can now be shared by the whole family of different-sized RBM’s that are created for
documents of different lengths (see Fig. 1). We call this the “Replicated Softmax” model. A pleasing
property of this model is that computing the approximate gradients of the CD objective (Eq. 6) for a
document that contains 100 words is computationally not much more expensive than computing the
gradients for a document that contains only one word. A key observation is that using D softmax
units with identical weights is equivalent to having a single multinomial unit which is sampled D
times  as shown in Fig. 1  right panel. If instead of sampling  we use real-valued softmax proba-
bilities multiplied by D  we exactly recover the learning algorithm of a Constrained Poisson model
[13]  except for the scaling of the hidden biases with D.

3 Evaluating Replicated Softmax as a Generative Model

Assessing the generalization performance of probabilistic topic models plays an important role in
model selection. Much of the existing literature  particularly for undirected topic models [4  17] 
uses extremely indirect performance measures  such as information retrieval or document classiﬁca-
tion. More broadly  however  the ability of the model to generalize can be evaluated by computing
the probability that the model assigns to the previously unseen documents  which is independent of
any speciﬁc application.

For undirected models  computing the probability of held-out documents exactly is intractable  since
computing the global normalization constant requires enumeration over an exponential number of
terms. Evaluating the same probability for directed topic models is also difﬁcult  because there are
an exponential number of possible topic assignments for the words.

Recently  [14] showed that a Monte Carlo based method  Annealed Importance Sampling (AIS) [12] 
can be used to efﬁciently estimate the partition function of an RBM. We also ﬁnd AIS attractive
because it not only provides a good estimate of the partition function in a reasonable amount of
computer time  but it can also just as easily be used to estimate the probability of held-out documents
for directed topic models  including Latent Dirichlet Allocation (for details see [16]). This will
allow us to properly measure and compare generalization capabilities of Replicated Softmax and

3

Algorithm 1 Annealed Importance Sampling (AIS) run.
1: Initialize 0 = β0 < β1 < ... < βS = 1.
2: Sample V1 from p0.
3: for s = 1 : S − 1 do
4:
5: end for
6: Set wAIS = QS

Sample Vs+1 given Vs using Ts(Vs+1 ← Vs).

s−1(Vs).

s(Vs)/p∗

s=1 p∗

LDA models. We now show how AIS can be used to estimate the partition function of a Replicated
Softmax model.

3.1 Annealed Importance Sampling

B(x)/ZB. Typically
Suppose we have two distributions: pA(x) = p∗
pA(x) is deﬁned to be some simple proposal distribution with known ZA  whereas pB represents
our complex target distribution of interest. One way of estimating the ratio of normalizing constants
is to use a simple importance sampling method:

A(x)/ZA and pB(x) = p∗

ZB
ZA

=Xx

p∗
B(x)
p∗
A(x)

pA(x) = EpA(cid:20) p∗

B(x)
p∗

A(x)(cid:21) ≈

1
N

N

Xi=1

B(x(i))
p∗
A(x(i))
p∗

 

(7)

where x(i) ∼ pA. However  if the pA and pB are not close enough  the estimator will be very poor.
In high-dimensional spaces  the variance of the importance sampling estimator will be very large  or
possibly inﬁnite  unless pA is a near-perfect approximation to pB.
Annealed Importance Sampling can be viewed as simple importance sampling deﬁned on a much
higher dimensional state space. It uses many auxiliary variables in order to make the proposal distri-
bution pA be closer to the target distribution pB. AIS starts by deﬁning a sequence of intermediate
probability distributions: p0  ...  pS  with p0 = pA and pS = pB. One general way to deﬁne this
sequence is to set:

pk(x) ∝ p∗

A(x)1−βk p∗

B(x)βk  

(8)

with “inverse temperatures” 0 = β0 < β1 < ... < βK = 1 chosen by the user. For each intermediate
distribution  a Markov chain transition operator Tk(x′; x) that leaves pk(x) invariant must also be
deﬁned.

Using the special bipartite structure of RBM’s  we can devise a better AIS scheme [14] for estimating
the model’s partition function. Let us consider a Replicated Softmax model with D words. Using
Eq. 5  the joint distribution over {V  h} is deﬁned as1:

p(V  h) =

1
Z

F

exp
Xj=1


K

Xk=1

W k

j hj ˆvk
  

(9)

where ˆvk =PD

i denotes the count for the kth word. By explicitly summing out the latent topic
units h we can easily evaluate an unnormalized probability p∗(V). The sequence of intermediate
distributions  parameterized by β  can now be deﬁned as follows:

i=1 vk

ps(V) =

1
Zs

p∗(V) =

p∗
s(V  h) =

1
Zs

1

Zs Xh

F

Yj=1 1 + exp βs

K

Xk=1

W k

j ˆvk!! .

(10)

Note that for s = 0  we have βs = 0  and so p0 represents a uniform distribution  whose partition
function evaluates to Z0 = 2F   where F is the number of hidden units. Similarly  when s = S  we
have βs = 1  and so pS represents the distribution deﬁned by the Replicated Softmax model. For the
intermediate values of s  we will have some interpolation between uniform and target distributions.
Using Eqs. 3  4  it is also straightforward to derive an efﬁcient Gibbs transition operator that leaves
ps(V) invariant.

1We have omitted the bias terms for clarity of presentation

4

A single run of AIS procedure is summarized in Algorithm 1. It starts by ﬁrst sampling from a sim-
ple uniform distribution p0(V) and then applying a series of transition operators T1  T2  . . .   TS−1
that “move” the sample through the intermediate distributions ps(V) towards the target distribution
pS(V). Note that there is no need to compute the normalizing constants of any intermediate distri-
butions. After performing M runs of AIS  the importance weights w(i)
AIS can be used to obtain an
unbiased estimate of our model’s partition function ZS:

ZS
Z0

≈

1
M

M

Xi=1

w(i)

AIS 

(11)

where Z0 = 2F . Observe that the Markov transition operators do not necessarily need to be ergodic.
In particular  if we were to choose dumb transition operators that do nothing  Ts(V′ ← V) =
δ(V′ − V) for all s  we simply recover the simple importance sampling procedure of Eq. 7.
When evaluating the probability of a collection of several documents  we need to perform a separate
AIS run per document  if those documents are of different lengths. This is because each different-
sized document can be represented as a separate RBM that has its own global normalizing constant.

4 Experimental Results

In this section we present experimental results on three three text datasets: NIPS proceedings pa-
pers  20-newsgroups  and Reuters Corpus Volume I (RCV1-v2) [10]  and report generalization per-
formance of Replicated Softmax and LDA models.

4.1 Description of Datasets
The NIPS proceedings papers2 contains 1740 NIPS papers. We used the ﬁrst 1690 documents as
training data and the remaining 50 documents as test. The dataset was already preprocessed  where
each document was represented as a vector containing 13 649 word counts.

The 20-newsgroups corpus contains 18 845 postings taken from the Usenet newsgroup collection.
The corpus is partitioned fairly evenly into 20 different newsgroups  each corresponding to a sepa-
rate topic.3 The data was split by date into 11 314 training and 7 531 test articles  so the training and
test sets were separated in time. We further preprocessed the data by removing common stopwords 
stemming  and then only considering the 2000 most frequent words in the training dataset. As a re-
sult  each posting was represented as a vector containing 2000 word counts. No other preprocessing
was done.
The Reuters Corpus Volume I is an archive of 804 414 newswire stories4 that have been manually
categorized into 103 topics. The topic classes form a tree which is typically of depth 3. For this
dataset  we deﬁne the relevance of one document to another to be the fraction of the topic labels that
agree on the two paths from the root to the two documents. The data was randomly split into 794 414
training and 10 000 test articles. The available data was already in the preprocessed format  where
common stopwords were removed and all documents were stemmed. We again only considered the
10 000 most frequent words in the training dataset.
For all datasets  each word count wi was replaced by log(1 + wi)  rounded to the nearest integer 
which slightly improved retrieval performance of both models. Table 1 shows description of all three
datasets.

4.2 Details of Training
For the Replicated Softmax model  to speed-up learning  we subdivided datasets into minibatches 
each containing 100 training cases  and updated the parameters after each minibatch. Learning
was carried out using Contrastive Divergence by starting with one full Gibbs step and gradually
increaing to ﬁve steps during the course of training  as described in [14]. For all three datasets  the
total number of parameter updates was set to 100 000  which took several hours to train. For the

2Available at http://psiexp.ss.uci.edu/research/programs data/toolbox.htm.
3Available at http://people.csail.mit.edu/jrennie/20Newsgroups (20news-bydate.tar.gz).
4Available at http://trec.nist.gov/data/reuters/reuters.html

5

Data set

NIPS
20-news
Reuters

Number of docs
Test
Train
1 690
50
11 314
794 414

7 531
10 000

K

¯D

St. Dev.

Avg. Test perplexity per word (in nats)

LDA-50

LDA-200 R. Soft-50 Unigram

13 649
2 000
10 000

98.0
51.8
94.6

245.3
70.8
69.3

3576
1091
1437

3391
1058
1142

3405
953
988

4385
1335
2208

Table 1: Results for LDA using 50 and 200 topics  and Replaced Softmax model that uses 50 topics. K is
the vocabulary size  ¯D is the mean document length  St. Dev. is the estimated standard deviation in document
length.

NIPS Proceedings

20-newsgroups

Reuters

5000

4500

4000

A
D
L

3500

3000

2500

2500

1600

1400

A
D
L

1200

1000

800

600

5000

600

2500

2000

1500

1000

500

A
D
L

1600

0
0

1000

800
1400
Replicated Softmax

1200

3000
Replicated Softmax

3500

4000

4500

500

2000

2500

1000

1500

Replicated Softmax

Figure 2: The average test perplexity scores for each of the 50 held-out documents under the learned 50-
dimensional Replicated Softmax and LDA that uses 50 topics.

LDA model  we used the Gibbs sampling implementation of the Matlab Topic Modeling Toolbox5
[5]. The hyperparameters were optimized using stochastic EM as described by [15]. For the 20-
newsgroups and NIPS datasets  the number of Gibbs updates was set to 100 000. For the large
Reuters dataset  it was set to 10 000  which took several days to train.

4.3 Assessing Topic Models as Generative Models

For each of the three datasets  we estimated the log-probability for 50 held-out documents.6 For both
the Replicated Softmax and LDA models we used 10 000 inverse temperatures βs  spaced uniformly
from 0 to 1. For each held-out document  the estimates were averaged over 100 AIS runs. The

average test perplexity per word was then estimated as exp(cid:16)−1/NPN

N is the total number of documents  Dn and vn are the total number of words and the observed
word-count vector for a document n.
Table 1 shows that for all three datasets the 50-dimensional Replicated Softmax consistently outper-
forms the LDA with 50-topics. For the NIPS dataset  the undirected model achieves the average test
perplexity of 3405  improving upon LDA’s perplexity of 3576. The LDA with 200 topics performed
much better on this dataset compared to the LDA-50  but its performance only slightly improved
upon the 50-dimensional Replicated Softmax model. For the 20-newsgroups dataset  even with 200
topics  the LDA could not match the perplexity of the Replicated Softmax model with 50 topic units.

1/Dn log p(vn)(cid:17)  where

n=1

The difference in performance is particularly striking for the large Reuters dataset  whose vocabulary
size is 10 000. LDA achieves an average test perplexity of 1437  substantially reducing it from
2208  achieved by a simple smoothed unigram model. The Replicated Softmax further reduces the
perplexity down to 986  which is comparable in magnitude to the improvement produced by the LDA
over the unigram model. LDA with 200 topics does improve upon LDA-50  achieving a perplexity
of 1142. However  its performance is still considerably worse than that of the Replicated Softmax
model.

5The code is available at http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
6For the 20-newsgroups and Reuters datasets  the 50 held-out documents were randomly sampled from the

test sets.

6

20-newsgroups

Replicated 
Softmax 50−D

LDA 50−D

60

50

40

30

20

10

)

%

(
 
n
o
s

i

i

c
e
r
P

Reuters

Replicated 
Softmax 50−D

LDA 50−D

50

40

30

20

10

)

%

i

(
 
n
o
s
c
e
r
P

i

0.02 0.1 0.4 1.6 6.4 25.6 100 

Recall (%) 

0.001 0.006 0.051 0.4 1.6 6.4 25.6 100 

Recall (%) 

Figure 3: Precision-Recall curves for the 20-newsgroups and Reuters datasets  when a query document from
the test set is used to retrieve similar documents from the training corpus. Results are averaged over all 7 531
(for 20-newsgroups) and 10 000 (for Reuters) possible queries.

Figure 2 further shows three scatter plots of the average test perplexity per document. Observe that
for almost all test documents  the Replicated Softmax achieves a better perplexity compared to the
corresponding LDA model. For the Reuters dataset  as expected  there are many documents that are
modeled much better by the undirected model than an LDA. Clearly  the Replicated Softmax is able
to generalize much better.

4.4 Document Retrieval

We used 20-newsgroup and Reuters datasets to evaluate model performance on a document retrieval
task. To decide whether a retrieved document is relevant to the query document  we simply check if
they have the same class label. This is the only time that the class labels are used. For the Replicated
Softmax  the mapping from a word-count vector to the values of the latent topic features is fast 
requiring only a single matrix multiplication followed by a componentwise sigmoid non-linearity.
For the LDA  we used 1000 Gibbs sweeps per test document in order to get an approximate posterior
over the topics. Figure 3 shows that when we use the cosine of the angle between two topic vectors to
measure their similarity  the Replicated Softmax signiﬁcantly outperforms LDA  particularly when
retrieving the top few documents.

5 Conclusions and Extensions

We have presented a simple two-layer undirected topic model that be used to model and automati-
cally extract distributed semantic representations from large collections of text corpora. The model
can be viewed as a family of different-sized RBM’s that share parameters. The proposed model have
several key advantages: the learning is easy and stable  it can model documents of different lengths 
and computing the posterior distribution over the latent topic values is easy. Furthermore  using
stochastic gradient descent  scaling up learning to billions of documents would not be particularly
difﬁcult. This is in contrast to directed topic models  where most of the existing inference algorithms
are designed to be run in a batch mode. Therefore one would have to make further approximations 
for example by using particle ﬁltering [3]. We have also demonstrated that the proposed model is
able to generalize much better than LDA in terms of both the log-probability on held-out documents
and the retrieval accuracy.

In this paper we have only considered the simplest possible topic model  but the proposed model can
be extended in several ways. For example  similar to supervised LDA [1]  the proposed Replicated
Softmax can be easily extended to modeling the joint the distribution over words and a document
label  as shown in Fig. 4  left panel. Recently  [11] introduced a Dirichlet-multinomial regression
model  where a prior on the document-speciﬁc topic distributions was modeled as a function of
observed metadata of the document. Similarly  we can deﬁne a conditional Replicated Softmax
model  where the observed document-speciﬁc metadata  such as author  references  etc.  can be used

7

Latent Topics

Latent Topics

Label

Metadata

Multinomial Visible

Multinomial Visible

Figure 4: Left: A Replicated Softmax model that models the joint distribution of words and document label.
Right: Conditional Replicated Softmax model where the observed document-speciﬁc metadata affects binary
states of the hidden topic units.

to inﬂuence the states of the latent topic units  as shown in Fig. 4  right panel. Finally  as argued by
[13]  a single layer of binary features may not the best way to capture the complex structure in the
count data. Once the Replicated Softmax has been trained  we can add more layers to create a Deep
Belief Network [8]  which could potentially produce a better generative model and further improve
retrieval accuracy.

Acknowledgments
This research was supported by NSERC  CFI  and CIFAR.

References

[1] D. Blei and J. McAuliffe. Supervised topic models. In NIPS  2007.
[2] D. Blei  A. Ng  and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research 

3:993–1022  2003.

[3] K. Canini  L. Shi  and T. Grifﬁths. Online inference of topics with latent Dirichlet allocation. In Proceed-

ings of the International Conference on Artiﬁcial Intelligence and Statistics  volume 5  2009.

[4] P. Gehler  A. Holub  and M. Welling. The Rate Adapting Poisson (RAP) model for information retrieval
and object recognition. In Proceedings of the 23rd International Conference on Machine Learning  2006.
In Proceedings of the National Academy of

[5] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics.

Sciences  volume 101  pages 5228–5235  2004.

[6] Thomas Grifﬁths and Mark Steyvers. Finding scientiﬁc topics. PNAS  101(suppl. 1)  2004.
[7] G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation 

14(8):1711–1800  2002.

[8] G. Hinton  S. Osindero  and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation 

18(7):1527–1554  2006.

[9] T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of the 15th Conference on Uncertainty

in AI  pages 289–296  San Fransisco  California  1999. Morgan Kaufmann.

[10] D. Lewis  Y. Yang  T. Rose  and F. Li. RCV1: A new benchmark collection for text categorization

research. Journal of Machine Learning Research  5:361–397  2004.

[11] D. Mimno and A. McCallum. Topic models conditioned on arbitrary features with dirichlet-multinomial

regression. In UAI  pages 411–418  2008.

[12] R. Neal. Annealed importance sampling. Statistics and Computing  11:125–139  2001.
[13] R. Salakhutdinov and G. Hinton. Semantic Hashing. In SIGIR workshop on Information Retrieval and

applications of Graphical Models  2007.

[14] R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In Proceedings of

the International Conference on Machine Learning  volume 25  pages 872 – 879  2008.

[15] H. Wallach. Topic modeling: beyond bag-of-words. In ICML  volume 148  pages 977–984  2006.
[16] H. Wallach  I. Murray  R. Salakhutdinov  and D. Mimno. Evaluation methods for topic models.

Proceedings of the 26th International Conference on Machine Learning (ICML 2009)  2009.

In

[17] E. Xing  R. Yan  and A. Hauptmann. Mining associated text and images with dual-wing harmoniums. In

Proceedings of the 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI-2005)  2005.

8

,Karthika Mohan
Judea Pearl
Victor Picheny
Robert Gramacy
Stefan Wild
Sebastien Le Digabel