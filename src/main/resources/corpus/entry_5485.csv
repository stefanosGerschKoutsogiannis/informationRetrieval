2018,The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal,We analyze the Kozachenko–Leonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over H\"{o}lder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the H\"{o}lder ball  we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the H\"{o}lder ball for $s \in (0 2]$ and arbitrary dimension d  rendering it the first estimator that provably satisfies this property.,The Nearest Neighbor Information Estimator is

Adaptively Near Minimax Rate-Optimal

Jiantao Jiao

Department of Electrical Engineering and Computer Sciences

University of California  Berkeley

jiantao@berkeley.edu

Weihao Gao

Department of ECE

Coordinated Science Laboratory

University of Illinois at Urbana-Champaign

wgao9@illinois.edu

Yanjun Han

Department of Electrical Engineering

Stanford University
yjhan@stanford.edu

Abstract

We analyze the Kozachenko–Leonenko (KL) ﬁxed k-nearest neighbor estimator
for the differential entropy. We obtain the ﬁrst uniform upper bound on its perfor-
mance for any ﬁxed k over H¨older balls on a torus without assuming any condi-
tions on how close the density could be from zero. Accompanying a recent mini-
max lower bound over the H¨older ball  we show that the KL estimator for any ﬁxed
k is achieving the minimax rates up to logarithmic factors without cognizance of
the smoothness parameter s of the H¨older ball for s ∈ (0  2] and arbitrary dimen-
sion d  rendering it the ﬁrst estimator that provably satisﬁes this property.

1

Introduction

Information theoretic measures such as entropy  Kullback-Leibler divergence and mutual informa-
tion quantify the amount of information among random variables. They have many applications in
modern machine learning tasks  such as classiﬁcation [48]  clustering [46  58  10  41] and feature
selection [1  17]. Information theoretic measures and their variants can also be applied in several
data science domains such as causal inference [18]  sociology [49] and computational biology [36].
Estimating information theoretic measures from data is a crucial sub-routine in the aforementioned
applications and has attracted much interest in statistics community. In this paper  we study the prob-
lem of estimating Shannon differential entropy  which is the basis of estimating other information
theoretic measures for continuous random variables.
Suppose we observe n independent identically distributed random vectors X = {X1  . . .   Xn}
drawn from density function f where Xi ∈ Rd. We consider the problem of estimating the dif-
ferential entropy

h(f ) = −(cid:82) f (x) ln f (x)dx  

(1)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

from the empirical observations X. The fundamental limit of estimating the differential entropy is
given by the minimax risk

(cid:16)E(ˆh(X) − h(f ))2(cid:17)1/2

inf
ˆh

sup
f∈F

 

(2)

where the inﬁmum is taken over all estimators ˆh that is a function of the empirical data X. Here F
denotes a (nonparametric) class of density functions.
The problem of differential entropy estimation has been investigated extensively in the literature.
As discussed in [2]  there exist two main approaches  where one is based on kernel density esti-
mators [30]  and the other is based on the nearest neighbor methods [56  53  52  11  3]  which is
pioneered by the work of [33].
The problem of differential entropy estimation lies in the general problem of estimating nonpara-
metric functionals. Unlike the parametric counterparts  the problem of estimating nonparametric
functionals is challenging even for smooth functionals. Initial efforts have focused on inference of
linear  quadratic  and cubic functionals in Gaussian white noise and density models and have laid the
foundation for the ensuing research. We do not attempt to survey the extensive literature in this area 
but instead refer to the interested reader to  e.g.  [24  5  12  16  6  32  37  47  8  9  54] and the refer-
ences therein. For non-smooth functionals such as entropy  there is some recent progress [38  26  27]
on designing theoretically minimax optimal estimators  while these estimators typically require the
knowledge of the smoothness parameters  and the practical performances of these estimators are not
yet known.
The k-nearest neighbor differential entropy estimator  or Kozachenko-Leonenko (KL) estimator is
computed in the following way. Let Ri k be the distance between Xi and its k-nearest neighbor
among {X1  . . .   Xi−1  Xi+1  . . .   Xn}. Precisely  Ri k equals the k-th smallest number in the list
{(cid:107)Xi − Xj(cid:107) : j (cid:54)= i  j ∈ [n]}  here [n] = {1  2  . . .   n}. Let B(x  ρ) denote the closed (cid:96)2 ball
centered at x of radius ρ and λ be the Lebesgue measure on Rd. The KL differential entropy
estimator is deﬁned as

i=1 ln(cid:0) n
(cid:80)n
where ψ(x) is the digamma function with ψ(1) = −γ  γ = −(cid:82) ∞

ˆhn k(X) = ln k − ψ(k) + 1

k λ(B(Xi  Ri k))(cid:1)  

Euler–Mascheroni constant.
There exists an intuitive explanation behind the construction of the KL differential entropy estimator.
Writing informally  we have

(3)
0 e−t ln tdt = 0.5772156 . . . is the
n(cid:88)

− ln ˆf (Xi) 

(4)

n

h(f ) = Ef [− ln f (X)] ≈ 1
n

− ln f (Xi) ≈ 1
n

i=1

n(cid:88)

i=1

where the ﬁrst approximation is based on the law of large numbers  and in the second approxima-
tion we have replaced f by a nearest neighbor density estimator ˆf. The nearest neighbor density
estimator ˆf (Xi) follows from the “intuition” 1that

.

ˆf (Xi)λ(B(Xi  Ri k)) ≈ k
n

(5)
Here the ﬁnal additive bias correction term ln k − ψ(k) follows from a detailed analysis of the bias
of the KL estimator  which will become apparent later.
We focus on the regime where k is a ﬁxed: in other words  it does not grow as the number of samples
n increases. The ﬁxed k version of the KL estimator is widely applied in practice and enjoys smaller
computational complexity  see [52].
There exists extensive literature on the analysis of the KL differential entropy estimator  which we
refer to [4] for a recent survey. One of the major difﬁculties in analyzing the KL estimator is that
the nearest neighbor density estimator exhibits a huge bias when the density is small. Indeed  it was
shown in [42] that the bias of the nearest neighbor density estimator in fact does not vanish even
B(Xi Ri k) f (u)du ∼ Beta(k  n − k) [4  Chap. 1.2]. A Beta(k  n − k) distributed
random variable has mean k
n .

1Precisely  we have(cid:82)

2

when n → ∞ and deteriorates as f (x) gets close to zero. In the literature  a large collection of work
assume that the density is uniformly bounded away from zero [23  29  57  30  53]  while others put
various assumptions quantifying on average how close the density is to zero [25  40  56  14  20  52 
11]. In this paper  we focus on removing assumptions on how close the density is to zero.

1.1 Main Contribution
Let Hs
d(L; [0  1]d) be the H¨older ball in the unit cube (torus) (formally deﬁned later in Deﬁnition 2
in Appendix A) and s ∈ (0  2] is the H¨older smoothness parameter. Then  the worst case risk of
the ﬁxed k-nearest neighbor differential entropy estimator over Hs
d(L; [0  1]d) is controlled by the
following theorem.
Theorem 1 Let X = {X1  . . .   Xn} be i.i.d. samples from density function f. Then  for 0 < s ≤ 2 
the ﬁxed k-nearest neighbor KL differential entropy estimator ˆhn k in (3) satisﬁes

(cid:32)

f∈Hs

sup
d(L;[0 1]d)

Ef

(cid:16)ˆhn k(X) − h(f )

(cid:17)2(cid:33) 1

(cid:16)

2 ≤ C

n− s

s+d ln(n + 1) + n− 1

2

.

(6)

where C is a constant depends only on s  L  k and d.

(cid:17)

(cid:17)

The KL estimator is in fact nearly minimax up to logarithmic factors  as shown in the following
result from [26].
Theorem 2 [26] Let X = {X1  . . .   Xn} be i.i.d. samples from density function f. Then  there
exists a constant L0 depending on s  d only such that for all L ≥ L0  s > 0 

(cid:32)

inf
ˆh

f∈Hs

sup
d(L;[0 1]d)

(cid:16)ˆh(X) − h(f )

Ef

(cid:17)2(cid:33) 1

(cid:16)

2 ≥ c

n− s

s+d (ln(n + 1))− s+2d

s+d + n− 1

2

.

(7)

where c is a constant depends only on s  L and d.
Remark 1 We emphasize that one cannot remove the condition L ≥ L0 in Theorem 2. Indeed  if the
H¨older ball has a too small width  then the density itself is bounded away from zero  which makes
the differential entropy a smooth functional  with minimax rates n− 4s
Theorem 1 and 2 imply that for any ﬁxed k  the KL estimator achieves the minimax rates up to
logarithmic factors without knowing s for all s ∈ (0  2]  which implies that it is near minimax
rate-optimal (within logarithmic factors) when the dimension d ≤ 2. We cannot expect the vanilla
version of the KL estimator to adapt to higher order of smoothness since the nearest neighbor density
estimator can be viewed as a variable width kernel density estimator with the box kernel  and it is
well known in the literature (see  e.g.  [55  Chapter 1]) that any positive kernel cannot exploit the
smoothness s > 2. We refer to [26] for a more detailed discussion on this difﬁculty and potential
solutions. The Jackknife idea  such as the one presented in [11  3] might be useful for adapting to
s > 2.
The signiﬁcance of our work is multi-folded:

4s+d + n−1/2 [51  50  43].

• We obtain the ﬁrst uniform upper bound on the performance of the ﬁxed k-nearest neigh-
bor KL differential entropy estimator over H¨older balls without assuming how close the
density could be from zero. We emphasize that assuming conditions of this type  such as
the density is bounded away from zero  could make the problem signiﬁcantly easier. For
example  if the density f is assumed to satisfy f (x) ≥ c for some constant c > 0  then the
differential entropy becomes a smooth functional and consequently  the general technique
for estimating smooth nonparametric functionals [51  50  43] can be directly applied here
to achieve the minimax rates n− 4s
4s+d + n−1/2. The main technical tools that enabled us
to remove the conditions on how close the density could be from zero are the Besicovitch
covering lemma (Lemma. 4) and the generalized Hardy–Littlewood maximal inequality.
• We show that  for any ﬁxed k  the k-nearest neighbor KL entropy estimator nearly achieves
the minimax rates without knowing the smoothness parameter s. In the functional estima-
tion literature  designing estimators that can be theoretically proved to adapt to unknown

3

levels of smoothness is usually achieved using the Lepski method [39  22  45  44  27] 
which is not known to be performing well in general in practice. On the other hand  a sim-
ple plug-in approach can achieves the rate of n−s/(s+d)  but only when s is known [26].
The KL estimator is well known to exhibit excellent empirical performance  but existing
theory has not yet demonstrated its near-“optimality” when the smoothness parameter s is
not known. Recent works [3  52  11] analyzed the performance of the KL estimator under
various assumptions on how close the density could be to zero  with no matching lower
bound up to logarithmic factors in general. Our work makes a step towards closing this gap
and provides a theoretical explanation for the wide usage of the KL estimator in practice.

The rest of the paper is organized as follows. Section 2 is dedicated to the proof of Theorem 1. We
discuss some future directions in Section 3.

1.2 Notations
For positive sequences aγ  bγ  we use the notation aγ (cid:46)α bγ to denote that there exists a universal
≤ C  and aγ (cid:38)α bγ is equivalent to bγ (cid:46)α aγ.
constant C that only depends on α such that supγ
Notation aγ (cid:16)α bγ is equivalent to aγ (cid:46)α bγ and bγ (cid:46)α aγ. We write aγ (cid:46) bγ if the constant is
universal and does not depend on any parameters. Notation aγ (cid:29) bγ means that lim inf γ
= ∞ 
and aγ (cid:28) bγ is equivalent to bγ (cid:29) aγ. We write a ∧ b = min{a  b} and a ∨ b = max{a  b}.

aγ
bγ

aγ
bγ

2 Proof of Theorem 1

In this section  we will prove that

(cid:18)

E(cid:16)ˆhn k(X) − h(f )

(cid:17)2(cid:19) 1

2 (cid:46)s L d k n− s

s+d ln(n + 1) + n− 1
2  

(8)

for any f ∈ Hs
d(L; [0  1]d) and s ∈ (0  2]. The proof consists two parts: (i) the upper bound
of the bias in the form of Os L d k(n−s/(s+d) ln(n + 1)); (ii) the upper bound of the variance is
Os L d k(n−1). Below we show the bias proof and relegate the variance proof to Appendix B.
First  we introduce the following notation

ft(x) =

µ(B(x  t))
λ(B(x  t))

=

1

Vdtd

u:|u−x|≤t

f (u)du .

(9)

(cid:90)

Here µ is the probability measure speciﬁed by density function f on the torus  λ is the Lebesgue
measure on Rd  and Vd = πd/2/Γ(1+d/2) is the Lebesgue measure of the unit ball in d-dimensional
Euclidean space. Hence ft(x) is the average density of a neighborhood near x. We ﬁrst state two
main lemmas about ft(x) which will be used later in the proof.
Lemma 1 If f ∈ Hs

d(L; [0  1]d) for some 0 < s ≤ 2  then for any x ∈ [0  1]d and t > 0  we have

| ft(x) − f (x)| ≤ dLts
s + d

 

(10)

Lemma 2 If f ∈ Hs
x and any t > 0  we have

d(L; [0  1]d) for some 0 < s ≤ 2 and f (x) ≥ 0 for all x ∈ [0  1]d  then for any

(cid:110)

ft(x) (cid:0) ft(x)Vdtd(cid:1)s/(s+d)(cid:111)

 

(11)

f (x) (cid:46)s L d max

Furthermore  f (x) (cid:46)s L d 1.

We relegate the proof of Lemma 1 and Lemma 2 to Appendix C. Now we investigate the bias
of ˆhn k(X). The following argument reduces the bias analysis of ˆhn k(X) to a function analytic
problem. For notation simplicity  we introduce a new random variable X ∼ f independent of

4

(16)

(17)

(cid:105) (cid:46)s L d k n− s

s+d and

s+d ln(n + 1)  which completes the proof.

{X1  . . .   Xn} and study ˆhn+1 k({X1  . . .   Xn  X}). For every x ∈ Rd  denote Rk(x) by the k-
nearest neighbor distance from x to {X1  X2  . . .   Xn} under distance d(x  y) = minm∈Zd (cid:107)m +
x − y(cid:107)  i.e.  the k-nearest neighbor distance on the torus. Then 

E[ˆhn+1 k({X1  . . .   Xn  X})] − h(f )

(cid:18) f (X)λ(B(X  Rk(X)))

= −ψ(k) + E [ ln ( (n + 1)λ(B(X  Rk(X))) )] + E [ln f (X)]
= E

(12)
(13)
+ E [ ln ((n + 1)µ(B(X  Rk(X))) ) ] − ψ(k) (14)

(cid:19)(cid:21)

ln

= E

ln

f (X)

fRk(X)(X)

+ ( E [ ln ((n + 1)µ(B(X  Rk(X))) ) ] − ψ(k) ) .

(15)

We ﬁrst show that the second term E [ln ((n + 1)µ(B(X  Rk(X))))] − ψ(k) can be universally
controlled regardless of the smoothness of f. Indeed  the random variable µ(B(X  Rk(X))) ∼
Beta(k  n + 1 − k) [4  Chap. 1.2] and it was shown in [4  Theorem 7.2] that there exists a universal

µ(B(X  Rk(X)))

(cid:21)

(cid:20)
(cid:20)

(cid:12)(cid:12)(cid:12) ≤ C

n

.

s+d ln(n + 1).

ln

fRk (X)(X)

f (X)

(cid:20)

Hence  it sufﬁces to show that for 0 < s ≤ 2 

constant C > 0 such that(cid:12)(cid:12)(cid:12) E [ln ((n + 1)µ(B(X  Rk(X))))] − ψ(k)
(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) (cid:46)s L d k n− s
(cid:12)(cid:12)(cid:12)(cid:12)E
We split our analysis into two parts. Section 2.1 shows that E(cid:104)
(cid:105) (cid:46)s L d k n− s
Section 2.2 shows that E(cid:104)
2.1 Upper bound on E(cid:104)
(cid:105)

fRk(X)(X)

fRk (X)(X)

fRk (X)(X)

f (X)

f (X)

ln

ln

ln

By the fact that ln y ≤ y − 1 for any y > 0  we have

(cid:20)

E

ln

fRk(X)(X)

f (X)

f (X)

(cid:21)

≤ E

(cid:90)

(cid:20) fRk(X)(X) − f (X)

(cid:21)

f (X)

(cid:0)E[fRk(x)(x)] − f (x)(cid:1) dx.

(18)

(cid:26)

(cid:27)

(19)
Here the expectation is taken with respect to the randomness in Rk(x) = min1≤i≤n m∈Zd (cid:107)m +
Xi − x(cid:107)  x ∈ Rd. Deﬁne function g(x; f  n) as

[0 1]d∩{x:f (x)(cid:54)=0}

=

g(x; f  n) = sup

u ≥ 0 : Vdudfu(x) ≤ 1
n

 

(20)

g(x; f  n) intuitively means the distance R such that the probability mass µ(B(x  R)) within R is
1/n. Then for any x ∈ [0  1]d  we can split E[fRk(x)(x)] − f (x) into three terms as

E[fRk(x)(x)] − f (x) = E[(fRk(x)(x) − f (x))1(Rk(x) ≤ n−1/(s+d))]

(21)
+ E[(fRk(x)(x) − f (x))1(n−1/(s+d) < Rk(x) ≤ g(x; f  n))] (22)
+ E[(fRk(x)(x) − f (x))1(Rk(x) > g(x; f  n) ∨ n−1/(s+d))]
(23)
(24)
= C1 + C2 + C3.
Now we handle three terms separately. Our goal is to show that for every x ∈ [0  1]  Ci (cid:46)s L d
n−s/(s+d) for i ∈ {1  2  3}. Then  taking the integral with respect to x leads to the desired bound.

1. Term C1: whenever Rk(x) ≤ n−1/(s+d)  by Lemma 1  we have

which implies that

|fRk(x)(x) − f (x)| ≤ dLRk(x)s
s + d

C1 ≤ E(cid:104)(cid:12)(cid:12)fRk(x)(x) − f (x)(cid:12)(cid:12) 1(Rk(x) ≤ n−1/(s+d))

(cid:105) (cid:46)s L d n−s/(s+d).

(cid:46)s L d n−s/(s+d) 

(25)

(26)

5

2. Term C2: whenever Rk(x) satisﬁes that n−1/(s+d) < Rk(x) ≤ g(x; f  n)  by deﬁnition of

g(x; f  n)  we have VdRk(x)dfRk(x)(x) ≤ 1

n  which implies that

It follows from Lemma 2 that in this case

fRk(x)(x) ≤

1

nVdRk(x)d ≤

1

nVdn−d/(s+d)

(cid:46)s L d n−s/(s+d).

fRk(x)(x) ∨(cid:0) fRk(x)(x)VdRk(x)d(cid:1)s/(s+d)

n−s/(s+d) ∨ n−s/(s+d) = n−s/(s+d).

f (x) (cid:46)s L d

≤

Hence 
f (x) + fRk(x)(x) (cid:46)s L d

2fRk(x)(x) + (VdRk(x)dfRk(x)(x))s/(s+d)

(37)
(cid:46)s L d VdRk(x)dfRk(x)(x)nd/(s+d) + (VdRk(x)dfRk(x)(x))s/(s+d)
(38)
(39)
where in the last step we have used the fact that VdRk(x)dfRk(x)(x) > n−1 since Rk(x) >
g(x; f  n). Finally  we have

(cid:46)s L d VdRk(x)dfRk(x)(x)nd/(s+d) 

C3 (cid:46)s L d nd/(s+d)E[(VdRk(x)dfRk(x)(x))1(Rk(x) > g(x; f  n))]

nd/(s+d)E(cid:2)(VdRk(x)dfRk(x)(x))1(cid:0)VdRk(x)dfRk(x)(x) > 1/n(cid:1)(cid:3) .(41)

(40)

=

Note that VdRk(x)dfRk(x)(x) ∼ Beta(k  n + 1 − k)  and if Y ∼ Beta(k  n + 1 − k)  we
have

(cid:18) k

k(n + 1 − k)
(n + 1)2(n + 2)
Notice that E[Y 1 (Y > 1/n)] ≤ nE[Y 2]. Hence  we have

E[Y 2] =

n + 1

+

(cid:19)2
nd/(s+d) n E(cid:2)(VdRk(x)dfRk(x)(x))2(cid:3)

1
n2 .

(cid:46)k

C3 (cid:46)s L d
(cid:46)s L d k

= n−s/(s+d).

nd/(s+d)n

n2

6

(cid:17)(cid:105)
(cid:17)(cid:105)

Hence  we have

C2

=
≤

E(cid:104)
E(cid:104)

(cid:16)
(cid:16)

(fRk(x)(x) − f (x))1

(fRk(x)(x) + f (x))1

n−1/(s+d) < Rk(x) ≤ g(x; f  n)
n−1/(s+d) < Rk(x) ≤ g(x; f  n)

(cid:46)s L d n−s/(s+d).

3. Term C3: we have

C3 ≤ E(cid:104)

(fRk(x)(x) + f (x))1

(cid:16)

Rk(x) > g(x; f  n) ∨ n−1/(s+d)(cid:17)(cid:105)

.

For any x such that Rk(x) > n−1/(s+d)  we have

fRk(x)(x) (cid:46)s L d VdRk(x)dfRk(x)(x)nd/(s+d) 

and by Lemma 2 

f (x) (cid:46)s L d

≤

fRk(x)(x) ∨ (VdRk(x)dfRk(x)(x))s/(s+d)
fRk(x)(x) + (VdRk(x)dfRk(x)(x))s/(s+d).

(27)

(28)
(29)

(30)

(31)

(32)

(33)

(34)

(35)
(36)

(42)

(43)

(44)

(cid:35)

(cid:21)

2.2 Upper bound on E(cid:104)
(cid:21)

(cid:20)

f (X)

E

ln

fRk(X)(X)

By splitting the term into two parts  we have

(cid:105)

f (X)

fRk (X)(X)

ln

(cid:34)(cid:90)
(cid:20)(cid:90)
(cid:20)(cid:90)

A

A

= E

= E

+ E

[0 1]d∩{x:f (x)(cid:54)=0}
f (x)

f (x) ln

fRk(x)(x)

f (x)

fRk(x)(x)

f (x) ln

f (x) ln

dx

f (x)

fRk(x)(x)

(cid:21)
(cid:21)
1(fRk(x)(x) > n−s/(s+d))dx
1(fRk(x)(x) ≤ n−s/(s+d))dx

(45)

(46)

(47)

(cid:20)(cid:90)
(cid:20)(cid:90)
(cid:20)(cid:90)

(cid:18) f (x) − fRk(x)(x)

(cid:19)

(48)
here we denote A = [0  1]d ∩ {x : f (x) (cid:54)= 0} for simplicity of notation. For the term C4  we have

= C4 + C5.

A

A

A

+ E

= E

fRk(x)(x)

C4 ≤ E

1(fRk(x)(x) > n−s/(s+d))dx

(cid:21)
f (x)
fRk(x)(x)
(f (x) − fRk(x)(x))2
(cid:21)
1(fRk(x)(x) > n−s/(s+d))dx
(cid:0)f (x) − fRk(x)(x)(cid:1) 1(fRk(x)(x) > n−s/(s+d))dx
(cid:20)(cid:90)
In the proof of upper bound of E(cid:104)
n−s/(s+d) for any x ∈ A. Similarly as in the proof of upper bound of E(cid:104)
E(cid:2)(fRk(x)(x) − f (x))2(cid:3) (cid:46)s L d k n−2s/(s+d) for every x ∈ A. Therefore  we have

(cid:0)f (x) − fRk(x)(x)(cid:1)2
(cid:105)
  we have shown that E[fRk(x)(x) − f (x)] (cid:46)s L d k
  we have

(cid:21)
(cid:0)f (x) − fRk(x)(x)(cid:1) dx
(cid:105)

≤ ns/(s+d)E

(49)

(50)

(cid:20)(cid:90)

fRk (X)(X)

fRk (X)(X)

.

(52)

+ E

(51)

(cid:21)

f (X)

f (X)

dx

ln

ln

A

A

C4 (cid:46)s L d k ns/(s+d)n−2s/(s+d) + n−s/(s+d) (cid:46)s L d k n−s/(s+d).

(53)

Now we consider C5. We conjecture that C5 (cid:46)s L d k n−s/(s+d) in this case  but we were not able
to prove it. Below we prove that C5 (cid:46)s L d k n−s/(s+d) ln(n + 1). Deﬁne the function

M (x) = sup
t>0

1

ft(x)

.

(54)

Since fRk(x)(x) ≤ n−s/(s+d)  we have M (x) = supt>0(1/ft(x)) ≥ 1/fRk(x)(x) ≥ ns/(s+d).
Denote ln+(y) = max{ln(y)  0} for any y > 0  therefore  we have that

(cid:21)

(cid:21)
1(fRk(x)(x) ≤ n−s/(s+d))dx

1(M (x) ≥ ns/(s+d))dx

(cid:19)(cid:21)

C5 ≤ E

f (x) ln+

≤ E

f (x) ln+

(cid:20)

(cid:20)(cid:90)
(cid:20)(cid:90)

A

A

(cid:90)
(cid:90)

(cid:19)
(cid:19)

(cid:18) f (x)
(cid:18) f (x)
(cid:18)

fRk(x)(x)

fRk(x)(x)

A

= C51 + C52 

7

≤

+

A

f (x)E

ln+

f (x)E(cid:2)ln+(cid:0)(n + 1)VdRk(x)df (x)(cid:1)(cid:3) 1(M (x) ≥ ns/(s+d))dx

(n + 1)VdRk(x)dfRk(x)(x)

1(M (x) ≥ ns/(s+d))dx

1

(55)

(56)

(57)

(58)

(59)

where the last inequality uses the fact ln+(xy) ≤ ln+ x + ln+ y for all x  y > 0. As for C51  since
VdRk(x)dfRk(x)(x) ∼ Beta(k  n + 1 − k)  and for Y ∼ Beta(k  n + 1 − k)  we have

(cid:20)

(cid:18)

(cid:19)(cid:21)

0

(cid:90) 1
(cid:20)
(cid:20)
(cid:20)

(cid:18)

1

(n + 1)x

(cid:19)
(cid:19)(cid:21)
(cid:19)(cid:21)
(cid:19)(cid:21)

1

n+1

=

ln

ln

E

ln+

(n + 1)Y

= E

(cid:18)
(cid:18)
(cid:18)
where in the last inequality we used the fact that E(cid:104)
(cid:90)

≤ E
≤ ln(n + 1)

for any k ≥ 1. Hence 

≤ E

ln

ln

1

1

1

C51 (cid:46)s L d

ln(n + 1)

(n + 1)Y

(n + 1)Y

(n + 1)Y

(cid:16)

pY (x)dx

(cid:90) 1

1

n+1

(cid:90) 1

1

n+1

+

ln ((n + 1)x) pY (x)dx

+ ln(n + 1)

pY (x)dx

+ ln(n + 1)

(cid:17)(cid:105)

(64)
= ψ(n+1)−ψ(k)−ln(n+1) ≤ 0

ln

1

(n+1)Y

f (x)1(M (x) ≥ ns/(s+d))dx.

A

(60)

(61)

(62)

(63)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

Now we introduce the following lemma  which is proved in Appendix C.
Lemma 3 Let µ1  µ2 be two Borel measures that are ﬁnite on the bounded Borel sets of Rd. Then 
for all t > 0 and any Borel set A ⊂ Rd 

(cid:18)(cid:26)

µ1

x ∈ A :

sup
0<ρ≤D

(cid:18) µ2(B(x  ρ))

(cid:19)

µ1(B(x  ρ))

(cid:27)(cid:19)

> t

≤ Cd
t

µ2(AD).

Here Cd > 0 is a constant that depends only on the dimension d and

AD = {x : ∃y ∈ A |y − x| ≤ D}.

Applying the second part of Lemma 3 with µ2 being the Lebesgue measure and µ1 being the measure
speciﬁed by f (x) on the torus  we can view the function M (x) as

M (x) = sup

0<ρ≤1/2

µ2(B(x  ρ))
µ1(B(x  ρ))

.

(cid:90)

Taking A = [0  1]d ∩ {x : f (x) (cid:54)= 0}  t = ns/(s+d)  then µ2(A 1

) ≤ 2d  so we know that

2

C51 (cid:46)s L d

=
≤

(cid:16)

f (x)1(M (x) ≥ ns/(s+d))dx

x ∈ [0  1]d  f (x) (cid:54)= 0  M (x) ≥ ns/(s+d)(cid:17)

ln(n + 1) ·
ln(n + 1) · µ1
ln(n + 1) · Cdn−s/(s+d)µ2(A 1

) (cid:46)s L d n−s/(s+d) ln(n + 1).

A

2

Now we deal with C52. Recall that in Lemma 2  we know that f (x) (cid:46)s L d 1 for any x  and
Rk(x) ≤ 1  so ln+((n + 1)VdRk(x)df (x)) (cid:46)s L d ln(n + 1). Therefore 

C52 (cid:46)s L d

ln(n + 1) ·

f (x)1(M (x) ≥ ns/(s+d))dx

(72)

(73)
Therefore  we have proved that C5 ≤ C51 + C52 (cid:46)s L d n−s/(s+d) ln(n + 1)  which completes the

(cid:46)s L d n−s/(s+d) ln(n + 1).

proof of the upper bound on E(cid:104)

(cid:105)

.

ln

f (X)

fRk (X)(X)

(cid:90)

A

8

3 Future directions

It is an tempting question to ask whether one can close the logarithmic gap between Theorem 1 and 2.
We believe that neither the upper bound nor the lower bound are tight. In fact  we conjecture that the
upper bound in Theorem 1 could be improved to n− s
s+d +n−1/2 due to a more careful analysis of the
bias  since Hardy–Littlewood maximal inequalities apply to arbitrary measurable functions but we
have assumed regularity properties of the underlying density. We conjecture that the minimax lower
bound could be improved to (n ln n)− s
s+d +n−1/2  since a kernel density estimator based differential
entropy estimator was constructed in [26] which achieves upper bound (n ln n)− s
s+d + n−1/2 over
Hs
d(L; [0  1]d) with the knowledge of s.
It would be interesting to extend our analysis to that of the k-nearest neighbor based Kullback–
Leibler divergence estimator [59]. The discrete case has been studied recently [28  7].
It is also interesting to analyze k-nearest neighbor based mutual information estimators  such as the
KSG estimator [34]  and show that they are “near”-optimal and adaptive to both the smoothness
and the dimension of the distributions. There exists some analysis of the KSG estimator [21] but we
suspect the upper bound is not tight. Moreover  a slightly revised version of KSG estimator is proved
to be consistent even if the underlying distribution is not purely continuous nor purely discrete [19] 
but the optimality properties are not yet well understood.

9

References
[1] R. Battiti. Using mutual information for selecting features in supervised neural net learning.

Neural Networks  IEEE Transactions on  5(4):537–550  1994.

[2] Jan Beirlant  Edward J Dudewicz  L´aszl´o Gy¨orﬁ  and Edward C Van der Meulen. Nonpara-
metric entropy estimation: An overview. International Journal of Mathematical and Statistical
Sciences  6(1):17–39  1997.

[3] Thomas B Berrett  Richard J Samworth  and Ming Yuan. Efﬁcient multivariate entropy esti-

mation via k-nearest neighbour distances. arXiv preprint arXiv:1606.00304  2016.

[4] G´erard Biau and Luc Devroye. Lectures on the nearest neighbor method. Springer  2015.

[5] Peter J Bickel and Yaacov Ritov. Estimating integrated squared density derivatives: sharp best
order of convergence estimates. Sankhy¯a: The Indian Journal of Statistics  Series A  pages
381–393  1988.

[6] Lucien Birg´e and Pascal Massart. Estimation of integral functionals of a density. The Annals

of Statistics  pages 11–29  1995.

[7] Yuheng Bu  Shaofeng Zou  Yingbin Liang  and Venugopal V Veeravalli. Estimation of KL
divergence between large-alphabet distributions. In 2016 IEEE International Symposium on
Information Theory (ISIT)  pages 1118–1122. IEEE  2016.

[8] T Tony Cai and Mark G Low. A note on nonparametric estimation of linear functionals. Annals

of statistics  pages 1140–1153  2003.

[9] T Tony Cai and Mark G Low. Nonquadratic estimators of a quadratic functional. The Annals

of Statistics  pages 2930–2956  2005.

[10] C. Chan  A. Al-Bashabsheh  J. B. Ebrahimi  T. Kaced  and T. Liu. Multivariate mutual in-
formation inspired by secret-key agreement. Proceedings of the IEEE  103(10):1883–1913 
2015.

[11] Sylvain Delattre and Nicolas Fournier. On the kozachenko–leonenko entropy estimator. Jour-

nal of Statistical Planning and Inference  185:69–93  2017.

[12] David L Donoho and Michael Nussbaum. Minimax quadratic estimation of a quadratic func-

tional. Journal of Complexity  6(3):290–323  1990.

[13] Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics 

pages 586–596  1981.

[14] Fidah El Haje Hussein and Yu Golubev. On entropy estimation by m-spacing method. Journal

of Mathematical Sciences  163(3):290–309  2009.

[15] Lawrence Craig Evans and Ronald F Gariepy. Measure theory and ﬁne properties of functions.

CRC press  2015.

[16] Jianqing Fan. On the estimation of quadratic functionals. The Annals of Statistics  pages

1273–1294  1991.

[17] F. Fleuret. Fast binary feature selection with conditional mutual information. The Journal of

Machine Learning Research  5:1531–1555  2004.

[18] Weihao Gao  Sreeram Kannan  Sewoong Oh  and Pramod Viswanath. Conditional dependence
via shannon capacity: Axioms  estimators and applications. In International Conference on
Machine Learning  pages 2780–2789  2016.

[19] Weihao Gao  Sreeram Kannan  Sewoong Oh  and Pramod Viswanath. Estimating mutual in-
In Advances in Neural Information Processing

formation for discrete-continuous mixtures.
Systems  pages 5988–5999  2017.

10

[20] Weihao Gao  Sewoong Oh  and Pramod Viswanath. Breaking the bandwidth barrier: Geo-
metrical adaptive entropy estimation. In Advances in Neural Information Processing Systems 
pages 2460–2468  2016.

[21] Weihao Gao  Sewoong Oh  and Pramod Viswanath. Demystifying ﬁxed k-nearest neighbor
information estimators. In Information Theory (ISIT)  2017 IEEE International Symposium
on  pages 1267–1271. IEEE  2017.

[22] Evarist Gin´e and Richard Nickl. A simple adaptive estimator of the integrated square of a

density. Bernoulli  pages 47–61  2008.

[23] Peter Hall. Limit theorems for sums of general functions of m-spacings.

In Mathematical
Proceedings of the Cambridge Philosophical Society  volume 96  pages 517–532. Cambridge
University Press  1984.

[24] Peter Hall and James Stephen Marron. Estimation of integrated squared density derivatives.

Statistics & Probability Letters  6(2):109–115  1987.

[25] Peter Hall and Sally C Morton. On the estimation of entropy. Annals of the Institute of

Statistical Mathematics  45(1):69–88  1993.

[26] Yanjun Han  Jiantao Jiao    Tsachy Weissman  and Yihong Wu. Optimal rates of entropy

estimation over lipschitz balls. arXiv preprint arXiv:1711.02141  2017.

[27] Yanjun Han  Jiantao Jiao  Rajarshi Mukherjee  and Tsachy Weissman. On estimation of lr-

norms in gaussian white noise models. arXiv preprint arXiv:1710.03863  2017.

[28] Yanjun Han  Jiantao Jiao  and Tsachy Weissman. Minimax rate-optimal estimation of diver-

gences between discrete distributions. arXiv preprint arXiv:1605.09124  2016.

[29] Harry Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the

Institute of Statistical Mathematics  41(4):683–697  1989.

[30] Kirthevasan Kandasamy  Akshay Krishnamurthy  Barnabas Poczos  Larry Wasserman  et al.
Nonparametric von Mises estimators for entropies  divergences and mutual informations. In
Advances in Neural Information Processing Systems  pages 397–405  2015.

[31] Rhoana J Karunamuni and Tom Alberts. On boundary correction in kernel density estimation.

Statistical Methodology  2(3):191–212  2005.

[32] G´erard Kerkyacharian and Dominique Picard. Estimating nonquadratic functionals of a density

using haar wavelets. The Annals of Statistics  24(2):485–507  1996.

[33] LF Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a random vector.

Problemy Peredachi Informatsii  23(2):9–16  1987.

[34] Alexander Kraskov  Harald St¨ogbauer  and Peter Grassberger. Estimating mutual information.

Physical Review E  69(6):066138  2004.

[35] Akshay Krishnamurthy  Kirthevasan Kandasamy  Barnabas Poczos  and Larry Wasserman.
Nonparametric estimation of R´enyi divergence and friends. In International Conference on
Machine Learning  pages 919–927  2014.

[36] Smita Krishnaswamy  Matthew H Spitzer  Michael Mingueneau  Sean C Bendall  Oren Litvin 
Erica Stone  Dana Pe’er  and Garry P Nolan. Conditional density-based analysis of t cell
signaling in single-cell data. Science  346(6213):1250689  2014.

[37] B´eatrice Laurent. Efﬁcient estimation of integral functionals of a density. The Annals of

Statistics  24(2):659–681  1996.

[38] Oleg Lepski  Arkady Nemirovski  and Vladimir Spokoiny. On estimation of the Lr norm of a

regression function. Probability theory and related ﬁelds  113(2):221–253  1999.

[39] Oleg V Lepski. On problems of adaptive estimation in white gaussian noise. Topics in non-

parametric estimation  12:87–106  1992.

11

[40] Boris Ya Levit. Asymptotically efﬁcient estimation of nonlinear functionals. Problemy

Peredachi Informatsii  14(3):65–72  1978.

[41] Pan Li and Olgica Milenkovic. Inhomogoenous hypergraph clustering with applications. arXiv

preprint arXiv:1709.01249  2017.

[42] YP Mack and Murray Rosenblatt. Multivariate k-nearest neighbor density estimates. Journal

of Multivariate Analysis  9(1):1–15  1979.

[43] Rajarshi Mukherjee  Whitney K Newey  and James M Robins. Semiparametric efﬁcient em-

pirical higher order inﬂuence function estimators. arXiv preprint arXiv:1705.07577  2017.

[44] Rajarshi Mukherjee  Eric Tchetgen Tchetgen  and James Robins. On adaptive estimation of

nonparametric functionals. arXiv preprint arXiv:1608.01364  2016.

[45] Rajarshi Mukherjee  Eric Tchetgen Tchetgen  and James Robins. Lepski’s method and adaptive
estimation of nonlinear integral functionals of density. arXiv preprint arXiv:1508.00249  2015.

[46] A. C. M¨uller  S. Nowozin  and C. H. Lampert. Information theoretic clustering using minimum

spanning trees. Springer  2012.

[47] Arkadi Nemirovski. Topics in non-parametric. Ecole dEt´e de Probabilit´es de Saint-Flour 

28:85  2000.

[48] H. Peng  F. Long  and C. Ding. Feature selection based on mutual information criteria of max-
dependency  max-relevance  and min-redundancy. Pattern Analysis and Machine Intelligence 
IEEE Transactions on  27(8):1226–1238  2005.

[49] David N Reshef  Yakir A Reshef  Hilary K Finucane  Sharon R Grossman  Gilean McVean 
Peter J Turnbaugh  Eric S Lander  Michael Mitzenmacher  and Pardis C Sabeti. Detecting
novel associations in large data sets. science  334(6062):1518–1524  2011.

[50] James Robins  Lingling Li  Rajarshi Mukherjee  Eric Tchetgen Tchetgen  and Aad van der
Vaart. Higher order estimating equations for high-dimensional models. The Annals of Statistics
(To Appear)  2016.

[51] James Robins  Lingling Li  Eric Tchetgen  and Aad van der Vaart. Higher order inﬂuence func-
tions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays
in Honor of David A. Freedman  pages 335–421. Institute of Mathematical Statistics  2008.

[52] Shashank Singh and Barnab´as P´oczos. Finite-sample analysis of ﬁxed-k nearest neighbor
density functional estimators. In Advances in Neural Information Processing Systems  pages
1217–1225  2016.

[53] Kumar Sricharan  Raviv Raich  and Alfred O Hero. Estimation of nonlinear functionals of
densities with conﬁdence. IEEE Transactions on Information Theory  58(7):4135–4159  2012.

[54] Eric Tchetgen  Lingling Li  James Robins  and Aad van der Vaart. Minimax estimation of the

integral of a power of a density. Statistics & Probability Letters  78(18):3307–3311  2008.

[55] A. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag  2008.

[56] Alexandre B Tsybakov and EC Van der Meulen. Root-n consistent estimators of entropy for

densities with unbounded support. Scandinavian Journal of Statistics  pages 75–83  1996.

[57] Bert Van Es. Estimating functionals related to a density by a class of statistics based on spac-

ings. Scandinavian Journal of Statistics  pages 61–72  1992.

[58] G. Ver Steeg and A. Galstyan. Maximally informative hierarchical representations of high-

dimensional data. stat  1050:27  2014.

[59] Qing Wang  Sanjeev R Kulkarni  and Sergio Verd´u. Divergence estimation for multidimen-
sional densities via k-nearest-neighbor distances. Information Theory  IEEE Transactions on 
55(5):2392–2405  2009.

12

,Tatiana Shpakova
Francis Bach
Jiantao Jiao
Weihao Gao
Yanjun Han
Sulaiman Alghunaim
Kun Yuan
Ali Sayed