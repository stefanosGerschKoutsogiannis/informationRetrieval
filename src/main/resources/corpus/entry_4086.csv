2017,Online control of the false discovery rate with decaying memory,In the online multiple testing problem  p-values corresponding to different null hypotheses are presented one by one  and the decision of whether to reject a hypothesis must be made immediately  after which the next p-value is presented. Alpha-investing algorithms to control the false discovery rate were first formulated by Foster and Stine and have since been generalized and applied to various settings  varying from quality-preserving databases for science to multiple A/B tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways : (a) we show how to uniformly improve the power of the entire class of GAI procedures under independence by awarding more alpha-wealth for each rejection  giving a near win-win resolution to a dilemma raised by Javanmard and Montanari  (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be null or non-null  (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more meaningful/important than others  (d) we define a new quantity called the \emph{decaying memory false discovery rate  or $\memfdr$} that may be more meaningful for applications with an explicit time component  using a discount factor to incrementally forget past decisions and alleviate some potential problems that we describe and name ``piggybacking'' and ``alpha-death''. Our GAI++ algorithms incorporate all four generalizations (a  b  c  d) simulatenously  and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity.,Online control of the false discovery rate with

decaying memory

Aaditya Ramdas

Fanny Yang Martin J. Wainwright Michael I. Jordan

{aramdas  fanny-yang  wainwrig  jordan} @berkeley.edu

University of California  Berkeley

Abstract

In the online multiple testing problem  p-values corresponding to different null
hypotheses are observed one by one  and the decision of whether or not to re-
ject the current hypothesis must be made immediately  after which the next p-
value is observed. Alpha-investing algorithms to control the false discovery rate
(FDR)  formulated by Foster and Stine  have been generalized and applied to many
settings  including quality-preserving databases in science and multiple A/B or
multi-armed bandit tests for internet commerce. This paper improves the class
of generalized alpha-investing algorithms (GAI) in four ways: (a) we show how
to uniformly improve the power of the entire class of monotone GAI procedures
by awarding more alpha-wealth for each rejection  giving a win-win resolution to
a recent dilemma raised by Javanmard and Montanari  (b) we demonstrate how
to incorporate prior weights to indicate domain knowledge of which hypotheses
are likely to be non-null  (c) we allow for differing penalties for false discoveries
to indicate that some hypotheses may be more important than others  (d) we de-
ﬁne a new quantity called the decaying memory false discovery rate (mem-FDR)
that may be more meaningful for truly temporal applications  and which alleviates
problems that we describe and refer to as “piggybacking” and “alpha-death.” Our
GAI++ algorithms incorporate all four generalizations simultaneously  and reduce
to more powerful variants of earlier algorithms when the weights and decay are all
set to unity. Finally  we also describe a simple method to derive new online FDR
rules based on an estimated false discovery proportion.

1

Introduction

The problem of multiple comparisons was ﬁrst recognized in the seminal monograph by Tukey [12]:
simply stated  given a collection of multiple hypotheses to be tested  the goal is to distinguish be-
tween the nulls and non-nulls  with suitable control on different types of error. We are given access
to one p-value for each hypothesis  which we use to decide which subset of hypotheses to reject  ef-
fectively proclaiming the rejected hypothesis as being non-null. The rejected hypotheses are called
discoveries  and the subset of these that were truly null—and hence mistakenly rejected—are called
false discoveries. In this work  we measure a method’s performance using the false discovery rate
(FDR) [2]  deﬁned as the expected ratio of false discoveries to total discoveries. Speciﬁcally  we
require that any procedure must guarantee that the FDR is bounded by a pre-speciﬁed constant ↵.
The traditional form of multiple testing is ofﬂine in nature  meaning that an algorithm testing N
hypotheses receives the entire batch of p-values {P1  . . .   PN} at one time instant. In the online
version of the problem  we do not know how many hypotheses we are testing in advance; instead  a
possibly inﬁnite sequence of p-values appear one by one  and a decision about rejecting the null must
be made before the next p-value is received. There are at least two different motivating justiﬁcations
for considering the online setting:

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

M1. We may have the entire batch of p-values available at our disposal from the outset  but we may
nevertheless choose to process the p-values one by one in a particular order.
Indeed  if one
can use prior knowledge to ensure that non-nulls typically appear earlier in the ordering  then
carefully designed online procedures could result in more discoveries than ofﬂine algorithms
(that operate without prior knowledge) such as the classical Benjamini-Hochberg algorithm [2] 
while having the same guarantee on FDR control. This motivation underlies one of the original
online multiple testing paper  namely that of Foster and Stine [5].

M2. We may genuinely conduct a sequence of tests one by one  where both the choice of the next
null hypothesis and the level at which it is tested may depend on the results of the previous tests.
Motivating applications include the desire to provide anytime guarantees for (i) internet compa-
nies running a sequence of A/B tests over time [9]  (ii) pharmaceutical companies conducting a
sequence of clinical trials using multi-armed bandits [13]  or (iii) quality-preserving databases
in which different research teams test different hypotheses on the same data over time [1].

The algorithms developed in this paper apply to both settings  with emphasis on motivation M2.
Let us ﬁrst reiterate the need for corrections when testing a sequence of hypotheses in the online
setting  even when all the p-values are independent. If each hypothesis i is tested independently
of the total number of tests either performed before it or to be performed after it  then we have no
control over the number of false discoveries made over time. Indeed  if our test for every Pi takes
the form 1{Pi  ↵} for some ﬁxed ↵  then  while the type 1 error for any individual test is bounded
by ↵  the set of discoveries could have arbitrarily poor FDR control. For example  under the “global
null” where every hypothesis is truly null  as long as the number of tests N is large and the null
p-values are uniform  this method will make at least one rejection with high probability (w.h.p.)  and
since in this setting every discovery is a false discovery  w.h.p. the FDR will equal one.
A natural alternative that takes multiplicity into account is the Bonferroni correction. If one knew the
total number N of tests to be performed  the decision rule 1{Pi  ↵/N} for each i 2{ 1  . . .   N}
controls the probability of even a single false discovery—a quantity known as the familywise er-
ror rate or FWER—at level ↵  as can be seen by applying the union bound. The natural exten-
sion of this solution to having an unknown and potentially inﬁnite number of tests is called alpha-
spending. Speciﬁcally  we choose any sequence of constants {↵i}i2N such thatPi ↵i  ↵  and on
receiving Pi  our decision is simply 1{Pi  ↵i}. However  such methods typically make very few
discoveries—meaning that they have very low power—when the number of tests is large  because
they must divide their error budget of ↵  also called alpha-wealth  among a large number of tests.
Since the FDR is less stringent than FWER  procedures that guarantee FDR control are generally
more powerful  and often far more powerful  than those controlling FWER. This fact has led to
the wide adoption of FDR as a de-facto standard for ofﬂine multiple testing (note  e.g.  that the
Benjamini-Hochberg paper [2] currently has over 40 000 citations).
Foster and Stine [5] designed the ﬁrst online alpha-investing procedures that use and earn alpha-
wealth in order to control a modiﬁed deﬁnition of FDR. Aharoni and Rosset [1] further extended
this to a class of generalized alpha-investing (GAI) methods  but once more for the modifed FDR. It
was only recently that Javanmard and Montanari [9] demonstrated that monotone GAI algorithms 
appropriately parameterized  can control the (unmodiﬁed) FDR for independent p-values. It is this
last work that our paper directly improves upon and generalizes; however  as we summarize below 
many of our modiﬁcations and generalizations are immediately applicable to all previous algorithms.
Contributions and outline. Instead of presenting the most general and improved algorithms imme-
diately  we choose to present results in a bottom-up fashion  introducing one new concept at a time
so as to lighten the symbolic load on the reader. For this purpose  we set up the problem formally
in Section 2. Our contributions are organized as follows:
1. Power.

In Section 3  we introduce the generalized alpha-investing (GAI) procedures  and
demonstrate how to uniformly improve the power of monotone GAI procedures that control
FDR for independent p-values  resulting in a win-win resolution to a dilemma posed by Javan-
mard and Montanari [9]. This improvement is achieved by a somewhat subtle modiﬁcation that
allows the algorithm to reward more alpha-wealth at every rejection but the ﬁrst. We refer to our
algorithms as improved generalized alpha-investing (GAI++) procedures  and provide intuition
for why they work through a general super-uniformity lemma (see Lemma 1 in Section 3.2). We

2

also provide an alternate way of deriving online FDR procedures by deﬁning and bounding a

natural estimator for the false discovery proportion dFDP.

2. Weights. In Section 5  we demonstrate how to incorporate certain types of prior information
about the different hypotheses. For example  we may have a prior weight for each hypothesis 
indicating whether it is more or less likely to be null. Additionally  we may have a different
penalty weight for each hypothesis  indicating differing importance of hypotheses. These prior
and penalty weights have been incorporated successfully into ofﬂine procedures [3  6  11]. In the
online setting  however  there are some technical challenges that prevent immediate application
of these ofﬂine procedures. For example  in the ofﬂine setting all the weights are constants 
but in the online setting  we allow them to be random variables that depend on the sequence
of past rejections. Further  in the ofﬂine setting all provided weights are renormalized to have
an empirical mean of one  but in the truly online setting (motivation M2) we do not know the
sequence of hypotheses or their random weights in advance  and hence we cannot perform any
such renormalization. We clearly outline and handle such issues and design novel prior- and/or
penalty-weighted GAI++ algorithms that control the penalty-weighted FDR at any time. This
may be seen as an online analog of doubly-weighted procedures for the ofﬂine setting [4  11].
Setting the weights to unity recovers the original class of GAI++ procedures.

3. Decaying memory.

In Section 6  we discuss some implications of the fact that existing al-
gorithms have an inﬁnite memory and treat all past rejections equally  no matter when they oc-
curred. This causes phenomena that we term as “piggybacking” (a string of bad decisions  riding
on past earned alpha-wealth) and “alpha-death” (a permanent end to decision-making when the
alpha-wealth is essentially zero). These phenomena may be desirable or acceptable under mo-
tivation M1 when dealing with batch problems  but are generally undesirable under motivation
M2. To address these issues  we propose a new error metric called the decaying memory false
discovery rate  abbreviated as mem-FDR  that we view as better suited to multiple testing for
truly temporal problems. Brieﬂy  mem-FDR pays more attention to recent discoveries by intro-
ducing a user-deﬁned discount factor  0 <  1  into the deﬁnition of FDR. We demonstrate
how to design GAI++ procedures that control online mem-FDR  and show that they have a sta-
ble and robust behavior over time. Using < 1 allows these procedures to slowly forget their
past decisions (reducing piggybacking)  or they can temporarily “abstain” from decision-making
(allowing rebirth after alpha-death). Instantiating  = 1 recovers the class of GAI++ procedures.
We note that the generalizations to incorporate weights and decaying memory are entirely orthogonal
to the improvements that we introduce to yield GAI++ procedures  and hence these ideas immedi-
ately extend to other GAI procedures for non-independent p-values. We also describe simulations
involving several of the aforementioned generalizations in Appendix C.

2 Problem Setup

Pr{Pt  x} x for any x 2 [0  1].

At time t = 0  before the p-values begin to appear  we ﬁx the level ↵ at which we wish to control the
FDR over time. At each time step t = 1  2  . . .   we observe a p-value Pt corresponding to some null
hypothesis Ht  and we must immediately decide whether to reject Ht or not. If the null hypothesis
is true  p-values are stochastically larger than the uniform distribution (“super-uniform”  for short) 
formulated as follows: if H0 is the set of true null hypotheses  then for any null Ht 2H 0  we have
(1)
We do not make assumptions on the marginal distribution of the p-values for hypotheses that are
non-null / false. Although they can be arbitrary  it is useful to think of them as being stochastically
smaller than the uniform distribution  since only then do they carry signal that differentiates them
from nulls. Our task is to design threshold levels ↵t according to which we deﬁne the rejection
decision as Rt = 1{Pt  ↵t}  where 1{·} is the indicator function. Since the aim is to control
the FDR at the ﬁxed level ↵ at any time t  each ↵t must be set according to the past decisions of
the algorithm  meaning that ↵t = ↵t(R1  . . .   Rt1). Note that  in accordance with past work 
we require that ↵t does not directly depend on the observed p-values but only on past rejections.
Formally  we deﬁne the sigma-ﬁeld at time t as F t = (R1  . . .   Rt)  and insist that
(2)
As studied by Javanmard and Montanari [8]  and as is predominantly the case in ofﬂine multiple
testing  we consider monotone decision rules  where ↵t is a coordinate-wise nondecreasing function:
(3)

↵t 2F t1 ⌘ ↵t is F t1-measurable ⌘ ↵t is predictable.

if ˜Ri  Ri for all i  t  1  then we have ↵t( ˜R1  . . .   ˜Rt1)  ↵t(R1  . . .   Rt1).

3

Existing online multiple testing algorithms control some variant of the FDR over time  as we now
t=1 Rt be the total number of rejections/discoveries made by

deﬁne. At any time T   let R(T ) =PT
the algorithm so far  and let V (T ) =Pt2H0 Rt be the number of false rejections/discoveries. Then 

the false discovery proportion and rate are deﬁned as

··········· and FDR(T ) = E V (T )
···········  
E[R(T )]+⌘   with a special case being mFDR(T ) = E[V (T )]

where we use the dotted-fraction notation corresponds to the shorthand a
b_1. Two vari-
ants of the FDR studied in earlier online FDR works [5  8] are the marginal FDR given by
mFDR⌘(T ) = E[V (T )]
E[R(T )_1]  and the smoothed FDR 
R(T )+⌘i . In Appendix A  we summarize a variety of algorithms and de-

given by sFDR⌘(T ) = Eh V (T )

pendence assumptions considered in previous work.

b··· = a

FDP(T ) :=

V (T )
R(T )

R(T )

3 Generalized alpha-investing (GAI) rules

The generalized class of alpha-investing rules [1] essentially covers most rules that have been pro-
posed thus far  and includes a wide range of algorithms with different behaviors. In this section  we
present a uniform improvement to monotone GAI algorithms for FDR control under independence.
Any algorithm of the GAI type begins with an alpha-wealth of W (0) = W0 > 0  and keeps track
of the wealth W (t) available after t steps. At any time t  a part of this alpha-wealth is used to test
the t-th hypothesis at level ↵t  and the wealth is immediately decreased by an amount t. If the
t-th hypothesis is rejected  that is if Rt := 1{Pt  ↵t} = 1  then we award extra wealth equaling
an amount t. Recalling the deﬁnition F t : = (R1  . . .   Rt)  we require that ↵t  t  t 2F t1 
meaning they are predictable  and W (t) 2F t  with the explicit update W (t) : = W (t  1)  t +
Rt t. The parameters W0 and the sequences ↵t  t  t are all user-deﬁned. They must be chosen
so that the total wealth W (t) is always non-negative  and hence that t  W (t  1) If the wealth
ever equals zero  the procedure is not allowed to reject any more hypotheses since it has to choose
↵t equal to zero from then on. The only real restriction for ↵t  t  t arises from the goal to control
FDR. This condition takes a natural form—whenever a rejection takes place  we cannot be allowed
to award an arbitrary amount of wealth. Formally  for some user-deﬁned constant B0  we must have
(4)
Many GAI rules are not monotone (cf. equation (3))  meaning that ↵t is not always a coordinatewise
nondecreasing function of R1  . . .   Rt1  as mentioned in the last column of Table 2 (Appendix A).
t=1 Rt = k} is the time of the k-th rejection.

Table 1 has some examples  where ⌧k := mins2N 1{Ps

 t  min{t + B0 

+ B0  1}.

t
↵t

Level ↵t

Name
[5] Alpha-investing (AI)
[1] Alpha-spending with rewards
[9] LORD’17

Parameters
—
  1  c
i = 1

1Pi=1

t

1+t

cW (t  1)
t

Table 1: Examples of GAI rules.

Penalty t
 W (t  1)
W (t  1)

tW0 + B0 Pj:⌧j <t

Reward t
t + B0
satisfy (4)

t⌧j B0 = ↵  W0

Improved monotone GAI rules (GAI++) under independence

3.1
In their initial work on GAI rules  Aharoni and Rosset [1] did not incorporate an explicit parameter
B0; rather  they proved that choosing W0 = B0 = ↵ sufﬁces for mFDR1 control. In subsequent
work  Javanmard and Montanari [9] introduced the parameter B0 and proved that for monotone GAI
rules  the same choice W0 = B0 = ↵ sufﬁces for sFDR1 control  whereas the choice B0 = ↵  W0
sufﬁces for FDR control  with both results holding under independence. In fact  their monotone GAI
rules with B0 = ↵  W0 are the only known methods that control FDR. This state of affairs leads
to the following dilemma raised in their paper [9]:

A natural question is whether  in practice  we should choose W0  B0 as to guarantee FDR
control (and hence set B0 = ↵  W0 ⌧ ↵) or instead be satisﬁed with mFDR or sFDR
control  which allow for B0 = ↵ and hence potentially larger statistical power.

4

Our ﬁrst contribution is a “win-win” resolution to this dilemma: more precisely  we prove that we
can choose B0 = ↵ while maintaining FDR control  with a small catch that at the very ﬁrst rejection
only  we need B0 = ↵  W0. Of course  in this case B0 is not constant  and hence we replace
it by the random variable bt 2F t1  and we prove that choosing W0  bt such that bt + W0 = ↵
for the ﬁrst rejection  and simply bt = ↵ for every future rejection  sufﬁces for formally proving
FDR control under independence. This achieves the best of both worlds (guaranteeing FDR control 
and handing out the largest possible reward of ↵)  as posed by the above dilemma. To restate our
contribution  we effectively prove that the power of monotone GAI rules can be uniformly improved
without changing the FDR guarantee.
Formally  we deﬁne our improved generalized alpha-investing (GAI++) algorithm as follows. It sets
W (0) = W0 with 0  W0  ↵  and chooses ↵t 2F t1 to make decisions Rt = 1{Pt  ↵t} and
updates the wealth W (t) = W (t  1)  t + Rt t 2F t using some t  W (t  1) 2F t1 and
some reward t  min{t + bt  t

↵t

+ bt  1}2F t1  using the choice
2F t1.

bt =⇢↵  W0 when R(t  1) = 0

otherwise

↵

As an explicit example  given an inﬁnite nonincreasing sequence of positive constants {j} that
sums to one  the LORD++ algorithm effectively makes the choice:

↵t = tW0 + (↵  W0)t⌧1 + ↵ Xj:⌧j <t ⌧j6=⌧1

t⌧j  

(5)

recalling that ⌧j is the time of the j-th rejection. Reasonable default choices include W0 = ↵/2  and
j = 0.0722 log(j_2)
Any monotone GAI++ rule comes with the following guarantee.

jeplog j   the latter derived in the context of testing if a Gaussian is zero mean [9].

·····················i  ↵ for all T 2 N
Theorem 1. Any monotone GAI++ rule satisﬁes the bound Eh V (T )+W (T )
under independence. Since W (T )  0 for all T 2 N  any such rule (a) controls FDR at level ↵
under independence  and (b) has power at least as large as the corresponding GAI algorithm.

R(T )

The proof of this theorem is provided in Appendix F. Note that for monotone rules  a larger alpha-
wealth reward at each rejection yields a possibly higher power  but never lower power  immediately
implying statement (b). Consequently  we provide only a proof for statement (a) in Appendix F. For
the reader interested in technical details  a key super-uniformity Lemma 1 and associated intuition
for online FDR algorithms is provided in Section 3.2.

Intuition for larger rewards via a super-uniformity lemma

3.2
For the purposes of providing some intuition for why we are able to obtain larger rewards than
Javanmard and Montanari [9]  we present the following lemma. In order to set things up  recall
that Rt = 1{Pt  ↵t} and note that ↵t is F t1-measurable  being a coordinatewise nondecreas-
ing function of R1  . . .   Rt1. Hence  the marginal super-uniformity assumption (1) immediately
implies that for independent p-values  we have

(6)

(7)

PrPt  ↵t F t1  ↵t 

or equivalently  E 1{Pt  ↵t}

························ F t1  1.

↵t

Lemma 1 states that under independence  the above statement remains valid in much more general-
ity.
Given a sequence P1  P2  . . . of independent p-values  deﬁne a ﬁltration via the sigma-ﬁelds
F i1 : = (R1  . . .   Ri1)  where Ri : = 1{Pi  fi(R1  . . .   Ri1)} for some coordinatewise
nondecreasing function fi : {0  1}i1 ! R. With this set-up  we have the following guarantee:
Lemma 1. Let g : {0  1}T ! R be any coordinatewise nondecreasing function such that g(~x) > 0
for any vector ~x6= (0  . . .   0). Then for any index t  T such that Ht 2H 0  we have

····················································· F t1  E ft(R1  . . .   Rt1)
E 1{Pt  ft(R1  . . .   Rt1)}

·································· F t1 .

g(R1  . . .   RT )

g(R1  . . .   RT )

5

This super-uniformity lemma is analogous to others used in ofﬂine multiple testing [4  11]  and will
be needed in its full generality later in the paper. The proof of this lemma in Appendix E is based
on a leave-one-out technique which is common in the multiple testing literature [7  10  11]; ours
speciﬁcally generalizes a lemma in the Appendix of Javanmard and Montanari [9].
As mentioned  this lemma helps to provide some intuition for the condition on t and the unorthodox
condition on bt. Indeed  note that by deﬁnition 

FDR(T ) = E V (T )

R(T )

··········· = EPt2H0 1{Pt  ↵t}

········································  EPT
··················  
PT

t=1 ↵t
t=1 Rt

R(T )

where we applied Lemma 1 to the coordinatewise nondecreasing function g(R1  . . .   RT ) = R(T ).

From this equation  we may infer the following: IfPt Rt = k  then the FDR will be bounded by
↵ as long as the total alpha-wealthPt ↵t that was used for testing is smaller than k↵. In other
words  with every additional rejection that adds one to the denominator  the algorithm is allowed
extra alpha-wealth equaling ↵ for testing.
In order to see where this shows up in the algorithm design  assume for a moment that we choose
our penalty as t = ↵t. Then  our condition on rewards t simply reduces to t  bt. Furthermore 
since we choose bt = ↵ after every rejection except the ﬁrst  our total earned alpha-wealth is
approximately ↵R(T )  which also upper bounds the total alpha-wealth used for testing.
The intuitive reason that bt cannot equal ↵ at the very ﬁrst rejection can also be inferred from the
········· := V (T )
above equation. Indeed  note that because of the deﬁnition of FDR  we have V (T )
R(T )_1  
R(T )
the denominator R(T ) _ 1 = 1 when the number of rejections equals zero or one. Therefore  the
denominator only starts incrementing at the second rejection. Hence  the sum of W0 and the ﬁrst
reward must be at most ↵  following which one may award ↵ at every rejection. This is the central
piece of intuition behind the GAI algorithm design  its improvement in this paper  and the FDR
control analysis. To the best of our knowledge  this is the ﬁrst explicit presentation for the intuition
for online FDR control.

4 A direct method for deriving new online FDR rules

Many ofﬂine FDR procedures can be derived in terms of an estimate dFDP of the false discovery
proportion; see Ramdas et al. [11] and references therein. The discussion in Section 3.2 suggests
that it is also possible to write online FDR rules in this fashion. Indeed  given any non-negative 
predictable sequence {↵t}  we propose the following deﬁnition:

dFDP(t) : = Pt

j=1 ↵j
··················.
R(t)

R(t)

R(t)

················································ = FDP(t).

bypassing the use of wealth  penalties and rewards in GAI. This idea is formalized below.

dFDP(t)  Pjt j2H0 ↵j

···························· ⇡ Pjt j2H0 1{Pj  ↵j}

This deﬁnition is intuitive because dFDP(t) approximately overestimates the unknown FDP(t):
A more direct way to construct new online FDR procedures is to ensure that supt2N dFDP(t)  ↵ 
Theorem 2. For any predictable sequence {↵t} such that supt2N dFDP(t)  ↵  we have:
PrPj  ↵j F j1  ↵j  then the associated procedure has supT2N mFDR(T )  ↵.
(b) If the p-values are independent and if {↵t} is monotone  then we also have supT2N FDR(T )  ↵.
The proof of this theorem is given in Appendix D. In our opinion  it is more transparent to verify
that LORD++ controls both mFDR and FDR using Theorem 2 than using Theorem 1.

the p-values are super-uniform conditional on all past discoveries  meaning that

(a)

If

5

Incorporating prior and penalty weights

Here  we develop GAI++ algorithms that incorporate prior weights wt  which allow the user to
exploit domain knowledge about which hypotheses are more likely to be non-null  as well as penalty
weights ut to differentiate more important hypotheses from the rest. The weights must be strictly
positive  predictable (meaning that wt  ut 2F t1) and monotone (in the sense of deﬁnition (3)).

6

Penalty weights. For many motivating applications  including internet companies running a series
of A/B tests over time  or drug companies doing a series of clinical trials over time  it is natural
to assume that some tests are more important than others  in the sense that some false discoveries
may have more lasting positive/negative effects than others. To incorporate this in the ofﬂine setting 
Benjamini and Hochberg [3] suggested associating each test with a positive penalty weight ui with
hypothesis Hi. Choosing ui > 1 indicates a more impactful or important test  while ui < 1
means the opposite. Although algorithms exist in the ofﬂine setting that can intelligently incorporate
penalty weights  no such ﬂexibility currently exists for online FDR algorithms. With this motivation
in mind and following Benjamini and Hochberg [3]  deﬁne the penalty-weighted FDR as

FDRu(T ) : = E Vu(T )
·············

Ru(T )

(8)

where Vu(T ) : =Pt2H0 utRt = Vu(T  1) + uT RT 1T 2H 0 and Ru(T ) : = Ru(T  1) +

uT RT . One may set ut = 1 to recover the special case of no penalty weights. In the ofﬂine setting  a
given set of penalty weights can be rescaled to make the average penalty weight equal unity  without
affecting the associated procedure. However  in the online setting  we choose penalty weights ut one
at a time  possibly not knowing the total number of hypotheses ahead of time. As a consequence 
these weights cannot be rescaled in advance to keep their average equal to unity. It is important to
note that we allow ut 2F t1 to be determined after viewing the past rejections  another important
difference from the ofﬂine setting. Indeed  if the hypotheses are logically related (even if the p-
values are independent)  then the current hypothesis can be more or less critical depending on which
other ones are already rejected.
Prior weights. In many applications  one may have access to prior knowledge about the underlying
state of nature (that is  whether the hypothesis is truly null or non-null). For example  an older
published biological study might have made signiﬁcant discoveries  or an internet company might
know the results of past A/B tests or decisions made by other companies. This knowledge may
be incorporated by a weight wt which indicates the strength of a prior belief about whether the
hypothesis is null or not—typically  a larger wt > 1 can be interpreted as a greater likelihood
of being a non-null  indicating that the algorithm may be more aggressive in deciding whether to
reject Ht. Such p-value weighting was ﬁrst suggested in the ofﬂine FDR context by [6]  though
earlier work employed it in the context of FWER control. As with penalty weights in the ofﬂine
setting  ofﬂine prior weights are also usually rescaled to have unit mean  and then existing ofﬂine
algorithms simply replace the p-value Pt by the weighted p-value Pt/wt. However  it is not obvious
how to incorporate prior weights in the online setting. As we will see in the sections to come  the
online FDR algorithms we propose will also use p-value reweighting; moreover  the rewards must be
prudently adjusted to accommodate the fact that an a-priori rescaling is not feasible. Furthermore 
as opposed to the ofﬂine case  the weights wt 2F t1 are allowed to depend on past rejections.
This additional ﬂexibility allows one to set the weights not only based on our prior knowledge of
the current hypothesis being tested  but also based on properties of the sequence of discoveries (for
example  whether we recently saw a string of rejections or non-rejections). We point out some
practical subtleties with the use and interpretation of prior weights in Appendix C.4.
Doubly-weighted GAI++ rules. Given a testing level ↵t and weights wt  ut  all three being pre-
dictable and monotone  we make the decision

Rt : = 1{Pt  ↵tutwt} .

(9)
This agrees with the intuition that larger prior weights should be reﬂected in an increased will-
ingness to reject the null  and we should favor rejecting more important hypotheses. As before 
our rejection reward strategy differs before and after ⌧1  the time of the ﬁrst rejection. Starting
with some W (0) = W0  ↵  we update the wealth as W (t) = W (t  1)  t + Rt t  where
wt  ut ↵ t  t  t 2F t1 must be chosen so that t  W (t  1)  and the rejection reward t must
obey the condition

0  t  min⇢t + utbt 

t

utwt↵t

+ utbt  ut   where

bt := ↵ 

1{⌧1 > t  1}2F t1.

W0
ut

(10a)

(10b)

Notice that setting wt = ut = 1 immediately recovers the GAI updates. Let us provide some
intuition for the form of the rewards t  which involves an interplay between the weights wt  ut 

7

the testing levels ↵t and the testing penalties t. First note that large weights ut  wt > 1 result in a
smaller earning of alpha-wealth and if ↵t  t are ﬁxed  then the maximum “common-sense” weights
are determined by requiring t  0. The requirements of lower rewards for larger weights and of
a maximum allowable weight should both seem natural; indeed  there must be some price one must
pay for an easier rejection  otherwise we would always use a high prior weight or penalty weight to
get more power  no matter the hypothesis! We show that such a price does not have to be paid in
terms of the FDR guarantee—we prove that FDRu is controlled for any choices of weights—but a
price is paid in terms of power  speciﬁcally the ability to make rejections in the future. Indeed  the
combined use of ut  wt in both the decision rule Rt and the earned reward t keeps us honest; if
we overstate our prior belief in the hypothesis being non-null or its importance by assigning a large
ut  wt > 1  we will not earn much of a reward (or even a negative reward!)  while if we understate
our prior beliefs by assigining a small ut  wt < 1  then we may not reject this hypothesis. Hence  it
is prudent to not misuse or overuse the weights  and we recommend that the scientist uses the default
ut = wt = 1 in practice unless there truly is prior evidence against the null or a reason to believe the
ﬁnding would be of importance  perhaps due to past studies by other groups or companies  logical
relationships between hypotheses  or due to extraneous reasons suggested by the underlying science.
We are now ready to state a theoretical guarantee for the doubly-weighted GAI++ procedure:

Theorem 3. Under independence  the doubly-weighted GAI++ algorithm satisﬁes the bound

Eh Vu(T )+W (T )
·······················i  ↵ for all T 2 N. Since W (T )  0  we also have FDRu(T )  ↵ for all T 2 N.

Ru(T )

The proof of this theorem is given in Appendix G. It is important to note that although we provide
the proof here only for GAI++ rules under independence  the ideas would actually carry forward in
an analogous fashion for GAI rules under various other forms of dependence.

6 From inﬁnite to decaying memory
Here  we summarize two phenomena : (i) the “piggybacking” problem that can occur with non-
stationary null-proportion  (ii) the “alpha-death” problem that can occur with a sequence of nulls. We
propose a new error metric  the decaying-memory FDR (mem-FDR)  that for truly temporal multiple
testing scenarios  and propose an adjustment of our GAI++ algorithms to control this quantity.
Piggybacking. As outlined in motivation M1  when the full batch of p-values is available ofﬂine 
online FDR algorithms have an inherent asymmetry in their treatment of different p-values  and
make different rejections depending on the order in which they process the batch. Indeed  Foster and
Stine [5] demonstrated that if one knew a reasonably good ordering (with non-nulls arriving earlier) 
then their online alpha-investing procedures could attain higher power than the ofﬂine BH procedure.
This is partly due to a phenomenon that we call “piggybacking”—if a lot of rejections are made early 
these algorithms earn and accumulate enough alpha-wealth to reject later hypotheses more easily by
testing them at more lenient thresholds than earlier ones. In essence  later tests “piggyback” on the
success of earlier tests. While piggybacking may be desirable or acceptable under motivation M1 
such behavior may be unwarranted and unwanted under motivation M2. We argue that piggybacking
may lead to a spike in the false discovery rate locally in time  even though the FDR over all time is
controlled. This may occur when the sequence of hypotheses is non-stationary and clustered  when
strings of nulls may follow strings of non-nulls. For concreteness  consider the setting in Javanmard
and Montanari [8] where an internet company conducts many A/B tests over time. In “good times” 
when a large fraction tests are truly non-null  the company may accumulate wealth due to frequent
rejections. We demonstrate using simulations that such accumulated wealth can lead to a string of
false discoveries when there is a quick transition to a “bad period” where the proportion of non-nulls
is much lower  causing a spike in the false discovery proportion locally in time.
Alpha-death. Suppose we test a long stretch of nulls  followed by a stretch of non-nulls. In this
setting  GAI algorithms will make (almost) no rejections in the ﬁrst stretch  losing nearly all of
its wealth. Thereafter  the algorithm may be effectively condemned to have no power  unless a
non-null with extremely strong signal is observed. Such a situation  from which no recovery is
possible  is perfectly reasonable under motivation M1. The alpha-wealth has been used up fully  and
those are the only rejections we are allowed to make with that batch of p-values. However  for an
internet company operating with motivation M2  it might be unacceptable to inform them that they
essentially cannot run any more tests  or that they may perhaps never make another useful discovery.

8

Both of these problems  demonstrated in simulations in Appendix C.2  are due to the fact that the
process effectively has an inﬁnite memory. In the following  we propose one way to smoothly forget
the past and to some extent alleviate the negative effects of the aforementioned phenomena.
Decaying memory. For a user-deﬁned decay parameter > 0  deﬁne V (0) = R(0) = 0 and
deﬁne the decaying memory FDR as follows:

mem-FDR(T ) : = E V (T )
·············  

R(T )

where V (T ) : = V (T  1) + RT 1T 2H 0 =Pt2H0 TtRt1t 2H 0   and analogously
R(T ) : = R(T  1) + RT = Pt TtRt. This notion of FDR control  which is arguably

natural for modern temporal applications  appears to be novel in the multiple testing literature. The
parameter  is reminiscent of the discount factor in reinforcement learning.
Penalty-weighted decaying-memory FDR. We may naturally extend the notion of decaying-
memory FDR to encompass penalty weights. Setting V 

u(0) = 0  we deﬁne

u (0) = R

mem-FDRu(T ) : = E V 
u (T  1) + uT RT 1T 2H 0 = PT

·············  

u (T )
R
u(T )

u(T ) : = R

u (T ) : = V 

t=1 TtutRt1t 2H 0  
where we deﬁne V 
R
mem-GAI++ algorithms with decaying memory and weights. Given a testing level ↵t  we make
the decision using equation (9) as before  starting with a wealth of W (0) = W0  ↵. Also  recall
that ⌧k is the time of the k-th rejection. On making the decision Rt  we update the wealth as:
(11)

u(T  1) + utRt =PT

W (t) : = W (t  1) + (1  )W01{⌧1 > t  1} t + Rt t 

t=1 TtutRt.

so that W (T ) = W0Tmin{⌧1 T} +

Tt(t + Rt t).

TXt=1

The ﬁrst term in equation (11) indicates that the wealth must decay in order to forget the old earnings
If we were to keep the ﬁrst term and drop the second  then the
from rejections far in the past.
effect of the initial wealth (not just the post-rejection earnings) also decays to zero.
Intuitively 
the correction from the second term suggests that even if one forgets all the past post-rejection
earnings  the algorithm should behave as if it started from scratch  which means that its initial wealth
should not decay. This does not contradict the fact that initial wealth can be consumed because of
testing penalties t  but it should not decay with time—the decay was only introduced to avoid
piggybacking  which is an effect of post-rejection earnings and not the initial wealth.
A natural restriction on t is the bound t  W (t  1) + (1  )W01{⌧1 > t  1}   which
ensures that the wealth stays non-negative. Further  wt  ut ↵ t  t 2F t1 must be chosen so that
the rejection reward t obeys conditions (10a) and (10b). Notice that setting wt = ut =  = 1
recovers the GAI++ updates. As an example  mem-LORD++ would use :
t⌧j t⌧j ⌧j .

↵t = tW0tmin{⌧1 t} + Xj:⌧j <t

u (T )+W (T )

We are now ready to present our last main result.
Theorem 4. Under independence  the doubly-weighted mem-GAI++ algorithm satisﬁes the bound

Eh V 
·······················i  ↵ for all T 2 N. Since W (T )  0  we have mem-FDRu(T )  ↵ for all T 2 N.

See Appendix H for the proof of this claim. Appendix B discusses how to use “abstaining” to
provide a smooth restart from alpha-death  whereas Appendix C contains a numerical simulation
demonstrating the use of decaying memory.

u(T )

R

7 Summary

In this paper  we make four main contributions—more powerful procedures under independence  an
alternate viewpoint of deriving online FDR procedures  incorporation of prior and penalty weights 
and introduction of a decaying-memory false discovery rate to handle piggybacking and alpha-death.
Numerical simulations in Appendix C complement the theoretical results.

9

Acknowledgments
We thank A. Javanmard  R. F. Barber  K. Johnson  E. Katsevich  W. Fithian and L. Lei for related
discussions  and A. Javanmard for sharing code to reproduce experiments in Javanmard and Mon-
tanari [9]. This material is based upon work supported in part by the Army Research Ofﬁce under
grant number W911NF-17-1-0304  and National Science Foundation grant NSF-DMS-1612948.

References
[1] Ehud Aharoni and Saharon Rosset. Generalized ↵-investing: deﬁnitions  optimality results and
application to public databases. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  76(4):771–794  2014.

[2] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and
powerful approach to multiple testing. Journal of the Royal Statistical Society  Series B  57(1):
289–300  1995.

[3] Yoav Benjamini and Yosef Hochberg. Multiple hypotheses testing with weights. Scandinavian

Journal of Statistics  24(3):407–418  1997.

[4] Gilles Blanchard and Etienne Roquain. Two simple sufﬁcient conditions for fdr control. Elec-

tronic journal of Statistics  2:963–992  2008.

[5] Dean P. Foster and Robert A. Stine. ↵-investing: a procedure for sequential control of expected
false discoveries. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
70(2):429–444  2008.

[6] Christopher R Genovese  Kathryn Roeder  and Larry Wasserman. False discovery control with

p-value weighting. Biometrika  93(3):509–524  2006.

[7] Philipp Heesen and Arnold Janssen. Dynamic adaptive multiple tests with ﬁnite sample fdr

control. arXiv preprint arXiv:1410.6296  2014.

[8] Adel Javanmard and Andrea Montanari. On online control of false discovery rate. arXiv

preprint arXiv:1502.06197  2015.

[9] Adel Javanmard and Andrea Montanari. Online rules for control of false discovery rate and

false discovery exceedance. The Annals of statistics  2017.

[10] Ang Li and Rina Foygel Barber. Multiple testing with the structure adaptive benjamini-

hochberg algorithm. arXiv preprint arXiv:1606.07926  2016.

[11] Aaditya Ramdas  Rina Foygel Barber  Martin J. Wainwright  and Michael I. Jordan. A uniﬁed

treatment of multiple testing with prior knowledge. arXiv preprint arXiv:1703.06222  2017.

[12] John Tukey. The Problem of Multiple Comparisons: Introduction and Parts A  B  and C.

Princeton University  1953.

[13] Fanny Yang  Aaditya Ramdas  Kevin Jamieson  and Martin J. Wainwright. A framework for
Multi-A(rmed)/B(andit) testing with online FDR control. Advances in Neural Information
Processing Systems  2017.

10

,Aaditya Ramdas
Fanny Yang
Martin Wainwright
Michael Jordan
Alberto Bernacchia
Mate Lengyel
Guillaume Hennequin
Daniel McDuff
Shuang Ma
Yale Song
Ashish Kapoor