2011,Budgeted Optimization with Concurrent Stochastic-Duration Experiments,Budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs. Typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments  which fail to capture important aspects of many real-world problems. This paper defines a novel problem formulation with the following important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic experiment durations; and 3) placing constraints on both the total number of experiments and the total experimental time. We develop both offline and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks. The results show that our algorithms produce highly effective schedules compared to natural baselines.,Budgeted Optimization with Concurrent

Stochastic-Duration Experiments

Javad Azimi  Alan Fern  Xiaoli Z. Fern
School of EECS  Oregon State University

{azimi  afern  xfern}@eecs.oregonstate.edu

Abstract

Budgeted optimization involves optimizing an unknown function that is costly to evalu-
ate by requesting a limited number of function evaluations at intelligently selected inputs.
Typical problem formulations assume that experiments are selected one at a time with
a limited total number of experiments  which fail to capture important aspects of many
real-world problems. This paper deﬁnes a novel problem formulation with the following
important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic
experiment durations; and 3) placing constraints on both the total number of experiments
and the total experimental time. We develop both ofﬂine and online algorithms for se-
lecting concurrent experiments in this new setting and provide experimental results on a
number of optimization benchmarks. The results show that our algorithms produce highly
effective schedules compared to natural baselines.

Introduction

1
We study the optimization of an unknown function f by requesting n experiments  each specifying an input
x and producing a noisy observation of f (x). In practice  the function f might be the performance of a de-
vice parameterized by x. We consider the setting where running experiments is costly (e.g. in terms of time) 
which renders methods that rely on many function evaluations  such as stochastic search or empirical gra-
dient methods  impractical. Bayesian optimization (BO) [8  4] addresses this issue by leveraging Bayesian
modeling to maintain a posterior over the unknown function based on previous experiments. The posterior is
then used to intelligently select new experiments to trade-off exploring new parts of the experimental space
and exploiting promising parts.
Traditional BO follows a sequential approach where only one experiment is selected and run at a time.
However  it is often desirable to select more than one experiment at a time so that multiple experiments
can be run simultaneously to leverage parallel facilities. Recently  Azimi et al. (2010) proposed a batch BO
algorithm that selects a batch of k ≥ 1 experiments at a time. While this broadens the applicability of BO  it
is still limited to selecting a ﬁxed number of experiments at each step. As such  prior work on BO  both batch
and sequential  completely ignores the problem of how to schedule experiments under ﬁxed experimental
budget and time constraints. Furthermore  existing work assumes that the durations of experiments are
identical and deterministic  whereas in practice they are often stochastic.
Consider one of our motivating applications of optimizing the power output of nano-enhanced Microbial
Fuel Cells (MFCs). MFCs [3] use micro-organisms to generate electricity. Their performance depends

1

strongly on the surface properties of the anode [10]. Our problem involves optimizing nano-enhanced an-
odes  where various types of nano-structures  e.g. carbon nano-wire  are grown directly on the anode surface.
Because there is little understanding of how different nano-enhancements impact power output  optimizing
anode design is largely guess work. Our original goal was to develop BO algorithms for aiding this process.
However  many aspects of this domain complicate the application of BO. First  there is a ﬁxed budget on
the number of experiments that can be run due to limited funds and a ﬁxed time period for the project. Sec-
ond  we can run multiple concurrent experiments  limited by the number of experimental apparatus. Third 
the time required to run each experiment is variable because each experiment requires the construction of a
nano-structure with speciﬁc properties. Nano-fabrication is highly unpredictable and the amount of time to
successfully produce a structure is quite variable. Clearly prior BO models fail to capture critical aspects of
the experimental process in this domain.
In this paper  we consider the following extensions. First  we have l available labs (which may correspond
to experimental stations at one location or to physically distinct laboratories)  allowing up to l concurrent
experiments. Second  experiments have stochastic durations  independently and identically distributed ac-
cording to a known density function pd. Finally  we are constrained by a budget of n total experiments and a
time horizon h by which point we must ﬁnish. The goal is to maximize the unknown function f by selecting
experiments and when to start them while satisfying the constraints.
We propose ofﬂine (Section 4) and online (Section 5) scheduling approaches for this problem  which aim
to balance two competing factors. First  a scheduler should ensure that all n experiments complete within
the horizon h  which encourages high concurrency. Second  we wish to select new experiments given as
many previously completed experiments as possible to make more intelligent experiment selections  which
encourages low concurrency. We introduce a novel measure of the second factor  cumulative prior experi-
ments (CPE) (Section 3)  which our approaches aim to optimize. Our experimental results indicate that these
approaches signiﬁcantly outperform a set of baselines across a range of benchmark optimization problems.

2 Problem Setup
Let X ⊆ (cid:60)d be a d-dimensional compact input space  where each dimension i is bounded in [ai  bi]. An
element of X is called an experiment. An unknown real-valued function f : X → (cid:60) represents the expected
value of the dependent variable after running an experiment. For example  f (x) might be the result of a wet-
lab experiment described by x. Conducting an experiment x produces a noisy outcome y = f (x) +   where
 is a random noise term. Bayesian Optimization (BO) aims to ﬁnd an experiment x ∈ X that approximately
maximizes f by requesting a limited number of experiments and observing their outcomes.
We extend traditional BO algorithms and study the experiment scheduling problem. Assuming a known
density function pd for the experiment durations  the inputs to our problem include the total number of
available labs l  the total number of experiments n  and the time horizon h by which we must ﬁnish. The
goal is to design a policy π for selecting when to start experiments and which ones to start to optimize f.
Speciﬁcally  the inputs to π are the set of completed experiments and their outcomes  the set of currently
running experiments with their elapsed running time  the number of free labs  and the remaining time till the
horizon. Given this information  π must select a set of experiments (possibly empty) to start that is no larger
than the number of free labs. Any run of the policy ends when either n experiments are completed or the
time horizon is reached  resulting in a set X of n or fewer completed experiments. The objective is to obtain
a policy with small regret  which is the expected difference between the optimal value of f and the value of
f for the predicted best experiment in X. In theory  the optimal policy can be found by solving a POMDP
with hidden state corresponding to the unknown function f. However  this POMDP is beyond the reach of
any existing solvers. Thus  we focus on deﬁning and comparing several principled policies that work well
in practice  but without optimality guarantees. Note that this problem has not been studied in the literature
to the best of our knowledge.

2

3 Overview of General Approach

(CPE) of E as:(cid:80)

A policy for our problem must make two types of decisions: 1) scheduling when to start new experiments 
and 2) selecting the speciﬁc experiments to start. In this work  we factor the problem based on these decisions
and focus on approaches for scheduling experiments. We assume a black box function SelectBatch for
intelligently selecting the k ≥ 1 experiments based on both completed and currently running experiments.
The implementation of SelectBatch is described in Section 6.
Optimal scheduling to minimize regret appears to be computationally hard for non-trivial instances of Se-
lectBatch. Further  we desire scheduling approaches that do not depend on the details of SelectBatch  but
work well for any reasonable implementation. Thus  rather than directly optimizing regret for a speciﬁc
SelectBatch  we consider the following surrogate criteria. First  we want to ﬁnish all n experiments within
the horizon h with high probability. Second  we would like to select each experiment based on as much
information as possible  measured by the number of previously completed experiments. These two goals are
at odds  since maximizing the completion probability requires maximizing concurrency of the experiments 
which minimizes the second criterion. Our ofﬂine and online scheduling approaches provide different ways
for managing this trade-off.
To quantify the second criterion  consider a complete execu-
tion E of a scheduler. For any experiment e in E  let priorE(e)
denote the number of experiments in E that completed be-
fore starting e. We deﬁne the cumulative prior experiments
e∈E priorE(e). Intuitively  a scheduler with
a high expected CPE is desirable  since CPE measures the total
amount of information SelectBatch uses to make its decisions.
CPE agrees with intuition when considering extreme policies.
A poor scheduler that starts all n experiments at the same time
(assuming enough labs) will have a minimum CPE of zero.
Further  CPE is maximized by a scheduler that sequentially executes all experiments (assuming enough
time). However  in between these extremes  CPE fails to capture certain intuitive properties. For example 
CPE increases linearly in the number of prior experiments  while one might expect diminishing returns as
the number of prior experiments becomes large. Similarly  as the number of experiments started together
(the batch size) increases  we might also expect diminishing returns since SelectBatch must choose the
experiments based on the same prior experiments. Unfortunately  quantifying these intuitions in a general
way is still an open problem. Despite its potential shortcomings  we have found CPE to be a robust measure
in practice.
To empirically examine the utility of CPE  we conducted experiments on a number of BO benchmarks. For
each domain  we used 30 manually designed diverse schedulers  some started more experiments early on
than later  and vice-versa  while others included random and uniform schedules. We measured the average
regret achieved for each scheduler given the same inputs and the expected CPE of the executions. Figure 1
shows the results for two of the domains (other results are highly similar)  where each point corresponds to
the average regret and CPE of a particular scheduler. We observe a clear and non-trivial correlation between
regret and CPE  which provides empirical evidence that CPE is a useful measure to optimize. Further  as we
will see in our experiments  the performance of our methods is also highly correlated with CPE.

Figure 1: The correlation between CPE and
regret for 30 different schedulers on two BO
benchmarks.

4 Ofﬂine Scheduling

We now consider ofﬂine schedules  which assign start times to all n experiments before the experimental
process begins. Note that while the schedules are ofﬂine  the overall BO policy has online characteristics 
since the exact experiments to run are only speciﬁed when they need to be started by SelectBatch  based

3

0204060801001200.180.20.220.240.260.280.30.32CPERegretCosines0204060801001200.030.040.050.060.070.080.090.1CPERegretHydrogenon the most recent information. This ofﬂine scheduling approach is often convenient in real experimental
domains where it is useful to plan out a static equipment/personnel schedule for the duration of a project.
Below we ﬁrst consider a restricted class of schedules  called staged schedules  for which we present a
solution that optimizes CPE. Next  we describe an approach for a more general class of schedules.

4.1 Staged Schedules

i=1  where 0 < ni ≤ l (cid:80)

i di ≤ h  and(cid:80)

(cid:80)i−1

i=2 ni

The CPE of any safe execution of S (slightly abusing notation) is: CPE(S) =(cid:80)N

A staged schedule deﬁnes a consecutive sequence of N experimental stages  denoted by a sequence of
tuples (cid:104)(ni  di)(cid:105)N
i ni ≤ n. Stage i begins by starting up ni new
experiments selected by SelectBatch using the most recent information  and ends after a duration of di  upon
which stage i + 1 starts. In some applications  staged schedules are preferable as they allow project planning
to focus on a relatively small number of time points (the beginning of each stage). While our approach tries
to ensure that experiments ﬁnish within their stage  experiments are never terminated and hence might run
longer than their speciﬁed duration. If  because of this  at the beginning of stage i there are not ni free labs 
the experiments will wait till labs free up.
We say that an execution E of a staged schedule S is safe if each experiment is completed within its speciﬁed
duration in S. We say that a staged schedule S is p-safe if with probability at least p an execution of S is safe
which provides a probabilistic guarantee that all n experiments complete within the horizon h. Further  it
ensures with probability p that the maximum number of concurrent experiments when executing S is maxi ni
(since experiments from two stages will not overlap with probability p). As such  we are interested in ﬁnding
staged schedules that are p-safe for a user speciﬁed p  e.g. 95%. Meanwhile  we want to maximize CPE.
j=1 nj. Typical
applications will use relative high values of p  since otherwise experimental resources would be wasted  and
thus with high probability we expect the CPE of an execution of S to equal CPE(S).
Our goal is thus to maximize CPE(S) while ensuring p-safeness. It turns out that for any ﬁxed number of
stages N  the schedules that maximize CPE(S) must be uniform. A staged schedule is deﬁned to be uniform
if ∀i  j  |ni − nj| ≤ 1  i.e.  the batch sizes across stages may differ by at most a single experiment.
Proposition 1. For any number of experiments n and labs l  let SN be the set of corresponding N stage
schedules  where N ≥ (cid:100)n/l(cid:101). For any S ∈ SN   CPE(S) = maxS(cid:48)∈SN CPE(S(cid:48)) if and only if S is uniform.
It is easy to verify that for a given n and l  an N
stage uniform schedule achieves a strictly higher
CPE than any N − 1 stage schedule. This im-
plies that we should prefer uniform schedules
with maximum number of stages allowed by the
p-safeness restriction. This motivates us to solve
the following problem: Find a p-safe uniform
schedule with maximum number of stages.
Our approach  outlined in Algorithm 1  considers
N stage schedules in order of increasing N  start-
ing at the minimum possible number of stages
N = (cid:100)n/l(cid:101) for running all experiments. For each
value of N  the call to MaxProbUniform com-
putes a uniform schedule S with the highest prob-
ability of a safe execution  among all N stage uni-
form schedules. If the resulting schedule is p-safe
then we consider N + 1 stages. Otherwise  there
is no uniform N stage schedule that is p-safe and
we return a uniform N − 1 stage schedule  which was computed in the previous iteration.

Algorithm 1 Algorithm for computing a p-safe uniform
schedule with maximum number of stages.
Input:number of experiments (n)  number of labs (l) 
horizon (h)  safety probability (p)
Output:A p-safe uniform schedule with maximum
number of stages

S(cid:48) ← MaxProbUniform(N  n  l  h)
if S(cid:48) is not p-safe then

N = (cid:100)n/l(cid:101)  S ← null
loop

return S

end if
S ← S(cid:48)  N ← N + 1

end loop

4

(cid:2)Pd(d

(cid:48)

)(cid:3)N(cid:48)·n(cid:48)(cid:20)

(cid:18) h − d(cid:48) · N(cid:48)

(cid:19)(cid:21)(N−N(cid:48))·(n(cid:48)−1)

It remains to describe the MaxProbUniform function  which computes a uniform N stage schedule S =
(cid:104)(ni  di)(cid:105)N
i=1 that maximizes the probability of a safe execution. First  any N stage uniform schedule must
have N(cid:48) = (n mod N ) stages with n(cid:48) = (cid:98)n/N(cid:99)+1 experiments and N−N(cid:48) stages with n(cid:48)−1 experiments.
Furthermore  the probability of a safe execution is invariant to the ordering of the stages  since we assume
i.i.d. distribution on the experiment durations. The MaxProbUniform problem is now reduced to computing
the durations di of S that maximize the probability of safeness for each given ni. For this we will assume that
the distribution of the experiment duration pd is log-concave  which allows us to characterize the solution
using the following lemma.
Lemma 1. For any duration distribution pd that is log-concave  if an N stage schedule S = (cid:104)(ni  di)(cid:105)N
is p-safe  then there is a p-safe N stage schedule S(cid:48) = (cid:104)(ni  d(cid:48)
This lemma suggests that any stages with equal ni’s should have equal di’s to maximize the probability of
safe execution. For a uniform schedule  ni is either n(cid:48) or n(cid:48) − 1. Thus we only need to consider schedules
with two durations  d(cid:48) for stages with ni = n(cid:48) and d(cid:48)(cid:48) for stages with ni = n(cid:48) − 1. Since all durations must
sum to h  d(cid:48) and d(cid:48)(cid:48) are deterministically related by: d(cid:48)(cid:48) = h−d(cid:48)·N(cid:48)
. Based on this  for any value of d(cid:48) the
N−N(cid:48)
probability of the uniform schedule using durations d(cid:48) and d(cid:48)(cid:48) is as follows  where Pd is the CDF of pd.

i=1 such that if ni = nj then d(cid:48)

i = d(cid:48)
j.

i)(cid:105)N

i=1

Pd

N − N(cid:48)

Independent Lab Schedules

experiments mi such that(cid:80)

i mi = n. Further  for each lab i a sequence of mi durations Di = (cid:104)d1

(1)
We compute MaxProbUniform by maximizing Equation 1 with respect to d(cid:48) and using the corresponding
duration for d(cid:48)(cid:48). Putting everything together we get the following result.
Theorem 1. For any log-concave pd  computing MaxProbUniform by maximizing Equation 1 over d(cid:48)  if a
p-safe uniform schedule exists  Algorithm 1 returns a maximum-stage p-safe uniform schedule.
4.2
We now consider a more general class of ofﬂine schedules and a heuristic algorithm for computing them.
This class allows the start times of different labs to be decoupled  desirable in settings where labs are run
by independent experimenters. Further  our online scheduling approach is based on repeatedly calling an
ofﬂine scheduler  which requires the ﬂexibility to make schedules for labs in different stages of execution.
An independent lab (IL) schedule S speciﬁes a number of labs k < l and for each lab i  a number of
(cid:105)
i   . . .   dmi
is given. The execution of S runs each lab independently  by having each lab start up experiments whenever
they move to the next stage. Stage j of lab i ends after a duration of dj
i   or after the experiment ﬁnishes
when it runs longer than dj
i (i.e. we do not terminate experiments). Each experiment is selected according
to SelectBatch  given information about all completed and running experiments across all labs.
We say that an execution of an IL schedule is safe if all experiments ﬁnish within their speciﬁed durations 
which also yields a notion of p-safeness. We are again interested in computing p-safe schedules that max-
imizes the CPE. Intuitively  CPE will be maximized if the amount of concurrency during an execution is
minimized  suggesting the use of as few labs as possible. This motivates the problem of ﬁnding a p-safe IL
schedule that use the minimum number of labs. Below we describe our heuristic approach to this problem.
Algorithm Description. Starting with k = 1  we compute a k labs IL schedule with the goal of maximizing
the probability of safe execution. If this probability is less than p  we increment k  and otherwise output the
schedule for k labs. To compute a schedule for each value of k  we ﬁrst allocate the number of experiments
mi across k labs as uniformly as possible. In particular  (n mod k) labs will have (cid:98)n/k(cid:99) + 1 experiments
and k − (n mod k) labs will have (cid:98)n/k(cid:99) experiments. This choice is motivated by the intuition that the
best way to maximize the probability of a safe execution is to distribute the work across labs as uniformly
as possible. Given mi for each lab  we assign all durations of lab i to be h/mi  which can be shown to be
optimal for log-concave pd. In this way  for each value of k the schedule we compute has just two possible
values of mi and labs with the same mi have the same stage durations.

i

5

5 Online Scheduling Approaches
We now consider online scheduling  which selects the start time of experiments online. The ﬂexibility of
the online approaches offers the potential to outperform ofﬂine schedules by adapting to speciﬁc stochastic
outcomes observed during experimental runs. Below we ﬁrst describe two baseline online approaches 
followed by our main approach  policy switching  which aims to directly optimize CPE.
Online Fastest Completion Policy (OnFCP). This baseline policy simply tries to ﬁnish all of the n exper-
iments as quickly as possible. As such  it keeps all l labs busy as long as there are experiments left to run.
Speciﬁcally whenever a lab (or labs) becomes free the policy immediately uses SelectBatch with the latest
information to select new experiments to start right away. This policy will achieve a low value of expected
CPE since it maximizes concurrency.
Online Minimum Eager Lab Policy (OnMEL). One problem with OnFCP is that it does not attempt to
use the full time horizon. The OnMEL policy simply restricts OnFCP to use only k labs  where k is the
minimum number of labs required to guarantee with probability at least p that all n experiments complete
within the horizon. Monte-Carlo simulation is used to estimate p for each k.
Policy Switching (PS). Our policy switching approach decides the number of new experiments to start at
each decision epoch. Decision epochs are assumed to occur every ∆ units of time  where ∆ is a small
constant relative to the expected experiment durations. The motivation behind policy switching is to exploit
the availability of a policy generator that can produce multiple policies at any decision epoch  where at least
one of them is expected to be good. Given such a generator  the goal is to deﬁne a new (switching) policy that
performs as well or better than the best of the generated policies in any state. In our case  the objective is to
improve CPE  though other objectives can also be used. This is motivated by prior work on policy switching
[6] over a ﬁxed policy library  and generalize that work to handle arbitrary policy generators instead of static
policy libraries. Below we describe the general approach and then the speciﬁc policy generator that we use.
Let t denote the number of remaining decision epochs (stages-to-go)  which is originally equal to (cid:98)h/∆(cid:99) and
decremented by one each epoch. We use s to denote the experimental state of the scheduling problem  which
encodes the number of completed experiments and ongoing experiments with their elapsed running time. We
assume access to a policy generator Π(s  t) which returns a set of base scheduling policies (possibly non-
stationary) given inputs s and t. Prior work on policy switching [6] corresponds to the case where Π(s  t)
returns a ﬁxed set of policies regardless of s and t. Given Π(s  t)  ¯π(s  t  π) denotes the resulting switching
policy based on s  t  and the base policy π selected in the previous epoch. The decision returned by ¯π is
computed by ﬁrst conducting N simulations of each policy returned by Π(s  t) along with π to estimate their
CPEs. The base policy with the highest estimated CPE is then selected and its decision is returned by ¯π. The
need to compare to the previous policy π is due to the use of a dynamic policy generator  rather than a ﬁxed
library. The base policy passed into policy switching for the ﬁrst decision epoch can be arbitrary.
Despite its simplicity  we can make guarantees about the quality of ¯π assuming a bound on the CPE estima-
tion error. In particular  the CPE of the switching policy will not be much worse than the best of the policies
produced by our generator given accurate simulations. We say that a CPE estimator is -accurate if it can
estimate the CPE C π
t (s) of any base policy π for any s and t within an accuracy bound of . Below we
denote the expected CPE of ¯π for s  t  and π to be C ¯π
Theorem 2. Let Π(s  t) be a policy generator and ¯π be the switching policy computed with -accurate
t (s) − 2t.
estimates. For any state s  stages-to-go t  and base policy π  C ¯π
We use a simple policy generator Π(s  t) that makes multiple calls to the ofﬂine IL scheduler described
earlier. The intuition is to notice that the produced p-safe schedules are fairly pessimistic in terms of the
experiment runtimes.
In reality many experiments will ﬁnish early and we can adaptively exploit such
situations. Speciﬁcally  rather than follow the ﬁxed ofﬂine schedule we may choose to use fewer labs and
hence improve CPE. Similarly if experiments run too long  we will increase the number of labs.

t (s  π) ≥ maxπ(cid:48)∈Π(s t)∪{π} C π(cid:48)

t (s  π).

6

Cosines(2)[1]

Hartman(3 6)[7]
Shekel(4)[7]

Table 1: Benchmark Functions

Σi=14αi exp(cid:2)−Σd
1 − (u2 + v2 − 0.3cos(3πu) − 0.3cos(3πv)) Rosenbrock(2)[1] 10 − 100(y − x2)2 − (1 − x)2
(cid:16) i.x2
(cid:17)(cid:17)20

j=1Aij(xj − Pij)2(cid:3)

u = 1.6x − 0.5  v = 1.6y − 0.5

Michalewicz(5)[9]−(cid:80)5

(cid:16)

i=1 sin(xi).

sin

i

π

α1×4  A4×d  P4×d are constants

Σ10
i=1

αi+Σj=14(xj−Aji)2

1

α1×10  A4×10 are constants

We deﬁne Π(s  t) to return k + 1 policies  {π(s t 0)  . . .   π(s t k)}  where k is the number of experiments
running in s. Policy π(s t i) is deﬁned so that it waits for i current experiments to ﬁnish  and then uses the
ofﬂine IL scheduler to return a schedule. This amounts to adding a small lookahead to the ofﬂine IL scheduler
where different amounts of waiting time are considered 1. Note that the deﬁnition of these policies depends
on s and t and hence can not be viewed as a ﬁxed set of static policies as used by traditional policy switching.
In the initial state s0  π(s0 h 0) corresponds to the ofﬂine IL schedule and hence the above theorem guarantees
that we will not perform much worse than the ofﬂine IL  with the expectation of performing much better.
Whenever policy switching selects a πi with i > 0 then no new experiments will be started and we wait for
the next decision epoch. For i = 0  it will apply the ofﬂine IL scheduler to return a p-safe schedule to start
immediately  which may require starting new labs to ensure high probability of completing n experiments.

with the RBF kernel and the kernel width = 0.01(cid:80)d

6 Experiments
Implementation of SelectBatch. Given the set of completed experiments O and on-going experiments A 
SelectBatch selects k new experiments. We implement SelectBatch based on a recent batch BO algorithm
[2]  which greedily selects k experiments considering only O. We modify this greedy algorithm to also
consider A by forcing the selected batch to include the ongoing experiments plus k additional experiments.
SelectBatch makes selections based on a posterior over the unknown function f. We use Gaussian Process
i=1 li  where li is the input space length in dimension i.
Benchmark Functions. We evaluate our scheduling policies using 6 well-known synthetic benchmark
functions (shown in Tab. 1 with dimension inside the parenthesis) and two real-world benchmark functions
Hydrogen and FuelCell over [0  1]2 [2]. The Hydrogen data is produced by a study on biosolar hydrogen
production [5]  where the goal was to maximize hydrogen production of a particular bacteria by optimizing
PH and Nitrogen levels. The FuelCell data was collected in our motivating application mentioned in Sect. 1.
In both cases  the benchmark function was created by ﬁtting regression models to the available data.
Evaluation. We consider a p-safeness guarantee of p = 0.95 and the number of available labs l is 10. For
pd(x)  we use one sided truncated normal distribution such that x ∈ (0  inf) with µ = 1  σ2 = 0.1  and we
set the total number of experiments n = 20. We consider three time horizons h of 6  5  and 4.
Given l  n and h  to evaluate policy π using function f (with a set of initial observed experiments)  we execute
π and get a set X of n or fewer completed experiments. We measure the regret of π as the difference between
the optimal value of f (known for all eight functions) and the f value of the predicted best experiment in X.
Results. Table 2 shows the results of our proposed ofﬂine and online schedulers. We also include  as a
reference point  the result of the un-constrained sequential policy (i.e.  selecting one experiment at a time)
using SelectBatch  which can be viewed as an effective upper bound on the optimal performance of any
constrained scheduler because it ignores the time horizon (h = ∞). The values in the table correspond to
the regrets (smaller values are better) achieved by each policy  averaged across 100 independent runs with
the same initial experiments (5 for 2-d and 3-d functions and 20 for the rest) for all policies in each run.

1For simplicity our previous discussion of the IL scheduler did not consider states with ongoing experiments  which
will occur here. To handle this the scheduler ﬁrst considers using already executing labs taking into account how long
they have been running. If more labs are required to ensure p-safeness new ones are added.

7

Functionh = ∞ OnFCP OfStaged OfIL OnMEL PS OfStaged OfIL OnMEL PS OfStaged OfIL OnMEL PS
.156
Cosines
.153
FuelCell
.025
Hydro
Rosen
.009
Hart(3)
.038
.480
Michal
.510
Shekel
Hart(6)
.262
CPE
138

.274
.239
.086
.011
.081
.521
.682
.333
91

.181
.167
.071
.009
.055
.500
.635
.334
100

.275
.258
.123
.013
.096
.525
.688
.354
66

.205
.206
.059
.008
.067
.502
.623
.347
100

.167
.154
.036
.007
.045
.477
.530
.304
133

.270
.230
.064
.010
.070
.502
.576
.301
120

.181
.182
.069
.010
.070
.509
.630
.338
100

.195
.191
.070
.009
.069
.508
.648
.340
100

.142
.160
.025
.008
.037
.465
.427
.265
190

.339
.240
.115
.013
.095
.545
.660
.348
55

.147
.163
.035
.009
.050
.460
.564
.266
137

Table 2: The proposed policies results for different horizons.

h=4

h=5

h=6

.194
.190
.069
.008
.064
.510
.645
.330
100

.150
.185
.042
.008
.045
.494
.540
.297
118

We ﬁrst note that the two ofﬂine algorithms (OfStages and OfIL) perform similarly across all three horizon
settings. This suggests that there is limited beneﬁt in these scenarios to using the more ﬂexible IL schedules 
which were primarily introduced for use in the online scheduling context. Comparing with the two online
baselines (OnFCP and OnMEL)  the ofﬂine algorithms perform signiﬁcantly better. This may seem surpris-
ing at ﬁrst because online policies should offer more ﬂexibility than ﬁxed ofﬂine schedules. However  the
ofﬂine schedules purposefully wait for experiments to complete before starting up new experiments  which
tends to improve the CPE values. To see this  the last row of Table 2 gives the average CPEs of each pol-
icy. Both OnFCP and OnMEL yield signiﬁcantly lower CPEs compared to the ofﬂine algorithms  which
correlates with their signiﬁcantly larger regrets.
Finally  policy switching consistently outperforms other policies (excluding h = ∞) on the medium horizon
setting and performs similarly in the other settings. This makes sense since the added ﬂexibility of PS is not
as critical for long and short horizons. For short horizons  there is less opportunity for scheduling choices and
for longer horizons the scheduling problem is easier and hence the ofﬂine approaches are more competitive.
In addition  looking at Table 2  we see that PS achieves a signiﬁcantly higher CPE than ofﬂine approaches in
the medium horizon  and is similar to them in the other horizons  again correlating with the regret. Further
examination of the schedules produced by PS indicates that although it begins with the same number of labs
as OfIL  PS often selects fewer labs in later steps if early experiments are completed sooner than expected 
which leads to higher CPE and consequently better performance. Note that the variances of the proposed
policies are very small which are shown in the supplementary materials.
7 Summary and Future Work
Motivated by real-world applications we introduced a novel setting for Bayesian optimization that incorpo-
rates a budget on the total time and number of experiments and allows for concurrent  stochastic-duration
experiments. We considered ofﬂine and online approaches for scheduling experiments in this setting  rely-
ing on a black box function to intelligently select speciﬁc experiments at their scheduled start times. These
approaches aimed to optimize a novel objective function  Cumulative Prior Experiments (CPE)  which we
empirically demonstrate to strongly correlate with performance on the original optimization problem. Our
ofﬂine scheduling approaches signiﬁcantly outperformed some natural baselines and our online approach of
policy switching was the best overall performer.
For further work we plan to consider alternatives to CPE  which  for example  incorporate factors such as
diminishing returns. We also plan to study further extensions to the experimental model for BO and also for
active learning. For example  taking into account varying costs and duration distributions across labs and
experiments. In general  we believe that there is much opportunity for more tightly integrating scheduling
and planning algorithms into BO and active learning to more accurately model real-world conditions.
Acknowledgments
The authors acknowledge the support of the NSF under grants IIS-0905678.

8

References
[1] B. S. Anderson  A. Moore  and D. Cohn. A nonparametric approach to noisy and costly optimization. In ICML 

2000.

[2] J. Azimi  A. Fern  and X. Fern. Batch bayesian optimization via simulation matching. In NIPS  2010.
[3] D. Bond and D. Lovley. Electricity production by geobacter sulfurreducens attached to electrodes. Applications of

Environmental Microbiology  69:1548–1555  2003.

[4] E. Brochu  M. Cora  and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions  with appli-
cation to active user modeling and hierarchical reinforcement learning. Technical Report TR-2009-23  Department
of Computer Science  University of British Columbia  2009.

[5] E. H. Burrows  W.-K. Wong  X. Fern  F. W. Chaplen  and R. L. Ely. Optimization of ph and nitrogen for enhanced
hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning methods. Biotechnology
Progress  25:1009–1017  2009.

[6] H. Chang  R. Givan  and E. Chong. Parallel rollout for online solution of partially observable markov decision

processes. Discrete Event Dynamic Systems  14:309–341  2004.

[7] L. Dixon and G. Szeg. The Global Optimization Problem: An Introduction Toward Global Optimization. North-

Holland  Amsterdam  1978.

[8] D. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization 

pages 345–383  2001.

[9] Z. Michalewicz. Genetic algorithms + data structures = evolution programs (2nd  extended ed.). Springer-Verlag

New York  Inc.  New York  NY  USA  1994.

[10] D. Park and J. Zeikus. Improved fuel cell and electrode designs for producing electricity from microbial degrada-

tion. Biotechnol.Bioeng.  81(3):348–355  2003.

9

,Jun Zhu
Junhua Mao
Alan Yuille
Chengguang Xu
Ehsan Elhamifar