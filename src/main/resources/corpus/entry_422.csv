2013,Adaptive Step-Size for Policy Gradient Methods,In the last decade  policy gradient methods have significantly grown in popularity in the reinforcement--learning field. In particular  they have been largely employed in motor control and robotic applications  thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless  the performance of policy gradient methods is determined not only by the gradient direction  since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate  while large values may lead to oscillations or even divergence of the policy parameters. Step--size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper  we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies  we derive a lower bound that is second--order polynomial of the step size  and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear--quadratic regulator problem.,Adaptive Step–Size for Policy Gradient Methods

Matteo Pirotta

Dept. Elect.  Inf.  and Bio.

Politecnico di Milano  ITALY
matteo.pirotta@polimi.it

Marcello Restelli

Dept. Elect.  Inf.  and Bio.

Politecnico di Milano  ITALY
marcello.restelli@polimi.it

Luca Bascetta

Dept. Elect.  Inf.  and Bio.

Politecnico di Milano  ITALY

luca.bascetta@polimi.it

Abstract

In the last decade  policy gradient methods have signiﬁcantly grown in popularity
in the reinforcement–learning ﬁeld. In particular  they have been largely employed
in motor control and robotic applications  thanks to their ability to cope with con-
tinuous state and action domains and partial observable problems. Policy gradient
researches have been mainly focused on the identiﬁcation of effective gradient
directions and the proposal of efﬁcient estimation algorithms. Nonetheless  the
performance of policy gradient methods is determined not only by the gradient di-
rection  since convergence properties are strongly inﬂuenced by the choice of the
step size: small values imply slow convergence rate  while large values may lead
to oscillations or even divergence of the policy parameters. Step–size value is usu-
ally chosen by hand tuning and still little attention has been paid to its automatic
selection. In this paper  we propose to determine the learning rate by maximizing
a lower bound to the expected performance gain. Focusing on Gaussian policies 
we derive a lower bound that is second–order polynomial of the step size  and
we show how a simpliﬁed version of such lower bound can be maximized when
the gradient is estimated from trajectory samples. The properties of the proposed
approach are empirically evaluated in a linear–quadratic regulator problem.

1

Introduction

Policy gradient methods have established as the most effective reinforcement–learning techniques
in robotic applications. Such methods perform a policy search to maximize the expected return of a
policy in a parameterized policy class. The reasons for their success are many. Compared to several
traditional reinforcement–learning approaches  policy gradients scale well to high–dimensional con-
tinuous state and action problems  and no changes to the algorithms are needed to face uncertainty
in the state due to limited and noisy sensors. Furthermore  policy representation can be properly de-
signed for the given task  thus allowing to incorporate domain knowledge into the algorithm useful
to speed up the learning process and to prevent the unexpected execution of dangerous policies that
may harm the system. Finally  they are guaranteed to converge to locally optimal policies.
Thanks to these advantages  from the 1990s policy gradient methods have been widely used to learn
complex control tasks [1]. The research in these years has focused on obtaining good model–free
estimators of the policy gradient using data generated during the task execution. The oldest policy
gradient approaches are ﬁnite–difference methods [2]  that estimate gradient direction by resolving
a regression problem based on the performance evaluation of policies associated to different small
perturbations of the current parameterization. Finite–difference methods have some advantages:
they are easy to implement  do not need assumptions on the differentiability of the policy w.r.t. the
policy parameters  and are efﬁcient in deterministic settings. On the other hand  when used on real
systems  the choice of parameter perturbations may be difﬁcult and critical for system safeness.
Furthermore  the presence of uncertainties may signiﬁcantly slow down the convergence rate. Such
drawbacks have been overcome by likelihood ratio methods [3  4  5]  since they do not need to gen-
erate policy parameters variations and quickly converge even in highly stochastic systems. Several

1

studies have addressed the problem to ﬁnd minimum variance estimators by the computation of op-
timal baselines [6]. To further improve the efﬁciency of policy gradient methods  natural gradient
approaches (where the steepest ascent is computed w.r.t. the Fisher information metric) have been
considered [7  8]. Natural gradients still converge to locally optimal policies  are independent from
the policy parameterization  need less data to attain good gradient estimate  and are less affected by
plateaus.
Once an accurate estimate of the gradient direction is obtained  policy parameters are updated by:
θt+1 = θt + αt∇θJ θ=θt  where αt ∈ R+ is the step size in the direction of the gradient. Although 
given an unbiased gradient estimate  convergence to a local optimum can be guaranteed under mild
conditions over the learning–rate values [9]  their choice may signiﬁcantly affect the convergence
speed or the behavior during the transient. Updating the policy with large step sizes may lead to
policy oscillations or even divergence [10]  while trying to avoid such phenomena by using small
learning rates determines a growth in the number of iterations that is unbearable in most real–world
applications. In general unconstrained programming  the optimal step size for gradient ascent meth-
ods is determined through line–search algorithms [11]  that require to try different values for the
learning rate and evaluate the function value in the corresponding updated points. Such an approach
is unfeasible for policy gradient methods  since it would require to perform a large number of policy
evaluations. Despite these difﬁculties  up to now  little attention has been paid to the study of step–
size computation for policy gradient algorithms. Nonetheless  some policy search methods based
on expectation–maximization have been recently proposed; such methods have properties similar to
the ones of policy gradients  but the policy update does not require to tune the step size [12  13].
In this paper  we propose a new approach to compute the step size in policy gradient methods that
guarantees an improvement at each step  thus avoiding oscillation and divergence issues. Starting
from a lower bound to the difference of performance between two policies  in Section 3 we derive a
lower bound in the case where the new policy is obtained from the old one by changing its parame-
ters along the gradient direction. Such a new bound is a (polynomial) function of the step size  that 
for positive values of the step size  presents a single  positive maximum ( i.e.  it guarantees improve-
ment) which can be computed in closed form. In Section 4  we show how the bound simpliﬁes to a
quadratic function of the step size when Gaussian policies are considered  and Section 5 studies how
the bound needs to be changed in approximated settings (e.g.  model–free case) where the policy
gradient needs to be estimated directly from experience.

2 Preliminaries

A discrete–time continuous Markov decision process
is deﬁned as a 6-tuple
(cid:104)S A P R  γ  D(cid:105)  where S is the continuous state space  A is the continuous action space  P
is a Markovian transition model where P(s(cid:48)|s  a) deﬁnes the transition density between state s and
s(cid:48) under action a  R : S × A → [0  R] is the reward function  such that R(s  a) is the expected
immediate reward for the state-action pair (s  a) and R is the maximum reward value  γ ∈ [0  1) is
the discount factor for future rewards  and D is the initial state distribution. The policy of an agent
is characterized by a density distribution π(·|s) that speciﬁes for each state s the density distribution
over the action space A. To measure the distance between two policies we will use this norm:

(MDP)

(cid:107)π(cid:48) − π(cid:107)∞ = sup
s∈S

A

|π(cid:48)(a|s) − π(a|s)|da 

that is the superior value over the state space of the total variation between the distributions over the
action space of policy π(cid:48) and π.
We consider inﬁnite horizon problems where the future rewards are exponentially discounted with
γ. For each state s  we deﬁne the utility of following a stationary policy π as:

(cid:35)

(cid:90)

(cid:34) ∞(cid:88)

t=0

V π(s) = E at ∼ π
st ∼ P

γtR(st  at)|s0 = s

.

It is known that V π solves the following recursive (Bellman) equation:

(cid:90)

V π(s) =

(cid:90)

S

A

π(a|s)R(s  a) + γ

2

P (s(cid:48)|s  a)V π(s(cid:48))ds(cid:48)da.

(cid:90)

(cid:90)

(cid:90)
D(s) = (1 − γ)(cid:80)∞

J π
D =

S

Policies can be ranked by their expected discounted reward starting from the state distribution D:

π(a|s)R(s  a)dads 

dπ
D(s)

D(s)V π(s)ds) =
t=0 γtP r(st = s|π  D) is the γ–discounted future state distribution
where dπ
for a starting state distribution D [5]. Solving an MDP means to ﬁnd a policy π∗ that maximizes
the expected long-term reward: π∗ ∈ arg maxπ∈Π J π
D. For any MDP there exists at least one
deterministic optimal policy that simultaneously maximizes V π(s)  ∀s ∈ S. For control purposes  it
is better to consider action values Qπ(s  a)  i.e.  the value of taking action a in state s and following
a policy π thereafter:

A

S

Qπ(s  a) = R(s  a) + γ

P(s(cid:48)|s  a)

π(a(cid:48)|s(cid:48))Qπ(s(cid:48)  a(cid:48))da(cid:48)ds(cid:48).

(cid:90)

S

(cid:90)

A

Furthermore  we deﬁne the advantage function:

Aπ(s  a) = Qπ(s  a) − V π(s) 

π (s) = (cid:82)

that quantiﬁes the advantage (or disadvantage) of taking action a in state s instead of following
policy π. In particular  for each state s  we deﬁne the advantage of a policy π(cid:48) over policy π as
A π(cid:48)(a|s)Aπ(s  a)da and  following [14]  we deﬁne its expected value w.r.t. an initial
Aπ(cid:48)
state distribution µ as Aπ(cid:48)
We consider the problem of ﬁnding a policy that maximizes the expected discounted reward over
a class of parameterized policies Πθ = {πθ : θ ∈ Rm}  where πθ is a compact representation of
π(a|s  θ). The exact gradient of the expected discounted reward w.r.t. the policy parameters [5] is:

π µ =(cid:82)

µ(s)Aπ(cid:48)

π (s)ds.

S dπ

(cid:90)

(cid:90)

∇θJµ(θ) =

1
1 − γ

dπθ
µ (s)

S

A

∇θπ(a|s  θ)Qπθ (s  a)dads.

The policy parameters can be updated by following the direction of the gradient of the expected
= θ + α∇θJµ(θ). In the following  we will denote with (cid:107)∇θJµ(θ)(cid:107)1 and
discounted reward: θ
(cid:107)∇θJµ(θ)(cid:107)2 the L1– and L2–norm of the policy gradient vector  respectively.

(cid:48)

3 Policy Gradient Formulation

In this section we provide a lower bound to the improvement obtained by updating the policy pa-
rameters along the gradient direction as a function of the step size. The idea is to start from the
general lower bound on the performance difference between any pair of policies introduced in [15]
and specialize it to the policy gradient framework.
Lemma 3.1 (Continuous MDP version of Corollary 3.6 in [15]). For any pair of stationary poli-
(cid:48) and for any starting state distribution µ  the difference
cies corresponding to parameters θ and θ
between the performance of policy πθ(cid:48) and policy πθ can be bounded as follows

(cid:48)

Jµ(θ

) − Jµ(θ) ≥ 1
1 − γ

µ (s)Aπθ(cid:48)
dπθ
πθ

(s)ds −

γ

2(1 − γ)2 (cid:107)πθ(cid:48) − πθ(cid:107)2∞ (cid:107)Qπθ(cid:107)∞  

where (cid:107)Qπθ(cid:107)∞ is the supremum norm of the Q–function: (cid:107)Qπθ(cid:107)∞ = sup
s∈S a∈A

Qπθ (s  a)

(1)

(cid:90)

S

As we can notice from the above bound  to maximize the performance improvement  we need to
ﬁnd a new policy πθ(cid:48) that is associated to large average advantage Aπθ(cid:48)
πθ  µ  but  at the same time  is
not too different from the current policy πθ. Policy gradient approaches provide search directions
characterized by increasing advantage values and  through the step size value  allow to control the
difference between the new policy and the target one. Exploiting a lower bound to the ﬁrst order
Taylor’s expansion  we can bound the difference between the current policy and the new policy 
whose parameters are adjusted along the gradient direction  as a function of the step size α.
Lemma 3.2. Let the update of the policy parameters be θ

= θ + α∇θJµ(θ). Then

(cid:48)

(cid:32) m(cid:88)

i j=1

(cid:12)(cid:12)(cid:12)(cid:12)θ+c∆θ

∂2π(a|s  θ)

∂θi∂θj

(cid:33)

∆θi ∆θj

1 + I(i = j)

 

) − π(a|s  θ) ≥α∇θπ(a|s  θ)T∇θJµ(θ) + α2

(cid:48)

π(a|s  θ
where ∆θ = α∇θJµ(θ).

inf

c∈(0 1)

3

By combining the two previous lemmas  it is possible to derive the policy performance improvement
obtained following the gradient direction.
= θ + α∇θJµ(θ). Then for any stationary
Theorem 3.3. Let the update of the parameters be θ
policy π(a|s  θ) and any starting state distribution µ  the difference in performance between πθ and
πθ(cid:48) is lower bounded by:

(cid:48)

(cid:48)

Jµ(θ

) − Jµ(θ) ≥ α(cid:107)∇θJµ(θ)(cid:107)2

2

(cid:90)

+

(cid:90)
α2
1 − γ
− γ (cid:107)Qπθ(cid:107)∞
2(1 − γ)2

S

i j=1

∂θi∂θj

∂2π(a|s  θ)

(cid:32) m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)θ+c∆θ
(cid:12)(cid:12)∇θπ(a|s  θ)T∇θJµ(θ)(cid:12)(cid:12) da
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) sup
(cid:32) m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)θ+c∆θ

∂2π(a|s  θ)

c∈(0 1)

∂θi∂θj

i j=1

A

(cid:90)

inf

c∈(0 1)

A

(cid:90)

dπθ
µ (s)

(cid:18)

α sup
s∈S

+α2 sup
s∈S

A

∆θi ∆θj

1 + I(i = j)

Qπθ (s  a)dads

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) da
(cid:33)2

.

∆θi ∆θj

1 + I(i = j)

(cid:33)

The above bound is a forth–order polynomial of the step size  whose stationary points  being the
roots of a third–order polynomial ax3 + bx2 + cx + d  can be expressed in closed form. It is worth to
notice that  for positive values of α  the bound presents a single stationary point that corresponds to
a local maximum. In fact  since a  b ≤ 0 and d ≥ 0  the Descartes’ rule of signs gives the existence
and uniqueness of the real positive root.
In the following section  we will show  in the case of Gaussian policies  how the bound in Theo-
rem 3.3 can be reduced to a second–order polynomial in α  thus obtaining a simpler closed-form
solution for optimal (w.r.t. the bound) step size.

4 The Gaussian Policy Model

In this section we consider the Gaussian policy model with ﬁxed standard deviation σ and the mean
is a linear combination of the state feature vector φ(·) using a parameter vector θ of size m:

(cid:32)

(cid:18) a − θTφ(s)

(cid:19)2(cid:33)

.

σ

π(a|s  θ) =

1√
2πσ2

exp

− 1
2

In the case of Gaussian policies  each second–order derivative of policy πθ can be easily bounded.
Lemma 4.1. For any Gaussian policy π(a|s  θ) ∼ N (θTφ(s)  σ2)  the second order derivative of
the policy can be bounded as follows:

(cid:12)(cid:12)(cid:12)(cid:12) ∂2π(a|s  θ)

∂θi∂θj

(cid:12)(cid:12)(cid:12)(cid:12) ≤ |φi(s)φj(s)|

2πσ3

√

∀θ ∈ Rm ∀a ∈ A.

 

This result allows to restate Lemma 3.2 in the case of Gaussian policies:

π(a|s  θ

(cid:48)

) − π(a|s  θ) ≥ α∇θπ(a|s  θ)T∇θJµ(θ) − α2√

2πσ3

(cid:0)|∇θJµ(θ)|T|φ(s)|(cid:1)2

.

In the following we will assume that features φ are uniformly bounded:
Assumption 4.1. All the basis functions are uniformly bounded by Mφ:
S ∀i = 1  . . .   m.

|φi(s)|< Mφ  ∀s ∈

Exploiting Pinsker’s inequality [16] (which upper bounds the total variation between two distribu-
tions with their Kullback–Liebler divergence)  it is possible to provide the following upper bound to
the supremum norm between two Gaussian policies.
= θ +α∇θJµ(θ)  supremum
Lemma 4.2. For any pair of stationary policies πθ and πθ(cid:48)  so that θ
norm of their difference can be upper bounded as follows:

(cid:48)

(cid:107)πθ(cid:48) − πθ(cid:107)∞ ≤ αMφ
σ

(cid:107)∇θJµ(θ)(cid:107)1 .

4

By plugging the results of Lemmas 4.1 and 4.2 into Equation (1) we can obtain a lower bound to
the performance difference between a Gaussian policy πθ and another policy along the gradient
direction that is quadratic in the step size α.
Theorem 4.3. For any starting state distribution µ  and any pair of stationary Gaussian policies
= θ + α∇θJµ(θ) and under Assump-
πθ ∼ N (θTφ(s)  σ2) and πθ(cid:48) ∼ N (θ
tion 4.1  the difference between the performance of πθ(cid:48) and the one of πθ can be lower bounded as
follows:
(cid:48)

φ(s)  σ2)  so that θ

(cid:48)T

(cid:48)

(cid:18)

) − Jµ(θ) ≥ α(cid:107)∇θJµ(θ)(cid:107)2
2
√
1
(1 − γ)
2(1 − γ)2σ2 (cid:107)∇θJµ(θ)(cid:107)2

− α2

γM 2
φ

2πσ3

dπθ

+

S

(cid:90)

µ (s)(cid:0)|∇θJµ(θ)|T |φ(s)|(cid:1)2(cid:90)

(cid:33)

1 (cid:107)Qπθ(cid:107)∞

.

Qπθ (s  a)dads

A

Jµ(θ

Since the linear coefﬁcient is positive and the quadratic one is negative  the bound in Theorem 4.3
has a single maximum attained for some positive value of α.
Corollary 4.4. The performance lower bound provided in Theorem 4.3 is maximized by choosing
the following step size:

∗

α

=

√

γ

2πσM 2

φ (cid:107)∇θJµ(θ)(cid:107)2

1 (cid:107)Qπθ(cid:107)∞ + 2(1 − γ)(cid:82)

(1 − γ)2

√
2πσ3 (cid:107)∇θJµ(θ)(cid:107)2

µ (s)(cid:0)|∇θJµ(θ)|T |φ(s)|(cid:1)2(cid:82)

S dπθ

2

A Qπθ (s  a)dads

 

that guarantees the following policy performance improvement

Jµ(θ

(cid:48)

) − Jµ(θ) ≥ 1
2

α∗ (cid:107)∇θJµ(θ)(cid:107)2
2 .

5 Approximate Framework

The solution for the tuning of the step size presented in the previous section depends on some
constants (e.g.  discount factor and the variance of the Gaussian policy) and requires to be able to
compute some quantities (e.g.  the policy gradient and the supremum value of the Q–function). In
many real–world applications such quantities cannot be computed (e.g.  when the state–transition
model is unknown or too large for exact methods) and need to be estimated from experience samples.
In this section  we study how the step size can be chosen when the gradient is estimated through
sample trajectories to guarantee a performance improvement in high probability.
For sake of easiness  we consider a simpliﬁed version of the bound in Theorem 4.3  in order to obtain
a bound where the only element that needs to be estimated is the policy gradient ∇θJµ(θ).
Corollary 5.1. For any starting state distribution µ  and any pair of stationary Gaussian policies
πθ ∼ N (θTφ(s)  σ2) and πθ(cid:48) ∼ N (θ
= θ + α∇θJµ(θ) and under Assump-
tion 4.1  the difference between the performance of πθ(cid:48) and πθ is lower bounded by:

φ(s)  σ2)  so that θ

(cid:48)T

(cid:48)

(cid:48)

Jµ(θ

) − Jµ(θ) ≥ α(cid:107)∇θJµ(θ)(cid:107)2

2 − α2 RM 2

φ (cid:107)∇θJµ(θ)(cid:107)2
(1 − γ)2 σ2

1

(cid:18) |A|√

2πσ

+

γ

2(1 − γ)

(cid:19)

 

that is maximized by the following step size value:
√

˜α∗ =

(cid:0)γ

(1 − γ)3

2πσ + 2(1 − γ)|A|(cid:1) RM 2

2πσ3 (cid:107)∇θJµ(θ)(cid:107)2

2

√

φ (cid:107)∇θJµ(θ)(cid:107)2

1

.

Since we are assuming that the policy gradient ∇θJµ(θ) is estimated through trajectory samples 
the lower bound in Corollary 5.1 must take into consideration the associated approximation error.

Given a set of trajectories obtained following policy πθ  we can produce an estimate (cid:98)∇θJµ(θ) of

the policy gradient and we assume to be able to produce a vector  = [1  . . .   m]T  so that the i–th
component of the approximation error is bounded at least with probability 1 − δ:

P(cid:16)(cid:12)(cid:12)(cid:12)∇θiJµ(θ) −(cid:98)∇θi Jµ(θ)
(cid:12)(cid:12)(cid:12) ≥ i

(cid:17) ≤ δ.

5

Given the approximation error vector   we can adjust the bound in Corollary 5.1 to produce a new
bound that holds at least with probability (1 − δ)m. In particular  to preserve the inequality sign 
the estimated approximation error must be used to decrease the L2–norm of the policy gradient in
the ﬁrst term (the one that provides the positive contribution to the performance improvement) and
to increase the L1–norm in the penalization term. To lower bound the L2–norm  we introduce the

vector (cid:98)∇θJµ(θ) whose components are a lower bound to the absolute value of the policy gradient

built on the basis of the approximation error :

(cid:98)∇θJµ(θ) = max(|(cid:98)∇θJµ(θ)| −   0) 

(cid:98)∇θJµ(θ) = |(cid:98)∇θJµ(θ)| + .

where 0 denotes the m–size vector with all zeros  and max denotes the component–wise maximum.

Theorem 5.2. Under the same assumptions of Corollary 5.1  and provided that it is available a

between the performance of πθ(cid:48) and πθ can be lower bounded at least with probability (1 − δ)m:

Similarly  to upper bound the L1–norm of the policy gradient  we introduce the vector (cid:98)∇θJµ(θ):
(cid:12)(cid:12)(cid:12) ≥ i
policy gradient estimate (cid:98)∇θJµ(θ)  so that P(cid:16)(cid:12)(cid:12)(cid:12)∇θiJµ(θ) −(cid:98)∇θiJµ(θ)
(cid:17) ≤ δ  the difference
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:98)∇θJµ(θ)
(cid:18) |A|√
2πσ3(cid:13)(cid:13)(cid:13)(cid:98)∇θJµ(θ)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:98)∇θJµ(θ)
2πσ + 2(1 − γ)|A|(cid:1) RM 2

(cid:13)(cid:13)(cid:13)(cid:98)∇θJµ(θ)
(cid:98)α∗ =

that is maximized by the following step size value:
√

) − Jµ(θ) ≥ α

(1 − γ)2 σ2

2(1 − γ)

(1 − γ)3

(cid:13)(cid:13)(cid:13)2

2

(cid:0)γ

− α2

(cid:48)

Jµ(θ

RM 2
φ

2πσ

(cid:19)

 

1

2

√

+

γ

φ

(cid:13)(cid:13)(cid:13)2

1

.

In the following  we will discuss how the approximation error of the policy gradient can be bounded.
Among the several methods that have been proposed over the years  we focus on two well–
understood policy–gradient estimation approaches: REINFORCE [3] and G(PO)MDP [4]/policy
gradient theorem (PGT) [5].

5.1 Approximation with REINFORCE gradient estimator

The REINFORCE approach [3] is the main exponent of the likelihood–ratio family. The episodic
REINFORCE gradient estimator is given by:

(cid:98)∇θJ RF

µ (θ) =

(cid:32) H(cid:88)

N(cid:88)

n=1

k=1

1
N

∇θ log π (an

k ; sn

k   θ)

γl−1rn

l − b

 

(cid:33)(cid:33)

(cid:32) H(cid:88)

l=1

where N is the number of H–step trajectories generated from a system by roll–outs and b ∈ R is
a baseline that can be chosen arbitrary  but usually with the goal of minimizing the variance of the
gradient estimator. The main drawback of REINFORCE is its variance  that is strongly affected by
the length of the trajectory horizon H.
The goal is to determine the number of trajectories N in order to obtain the desired accuracy of
the gradient estimate. To achieve this  we exploit the upper bound to the variance of the episodic
REINFORCE gradient estimator introduced in [17] for Gaussian policies.
[17]). Given a Gaussian policy π(a|s  θ) ∼
Lemma 5.3 (Adapted from Theorem 2 in

N(cid:0)θTφ(s)  σ2(cid:1)  under the assumption of uniformly bounded rewards and basis functions (Assump-
REINFORCE gradient estimate (cid:98)∇θiJ RF
(cid:16)(cid:98)∇θi J RF

tion 4.1)  we have the following upper bound to the variance of the i–th component of the episodic

φH(cid:0)1 − γH(cid:1)2

(cid:17) ≤ R2M 2

µ (θ):

µ (θ)

V ar

.

N σ2 (1 − γ)2

6

The result in the previous Lemma combined with the Chebyshev’s inequality allows to provide a
high–probability upper bound to the gradient approximation error using the episodic REINFORCE
gradient estimator.

Theorem 5.4. Given a Gaussian policy π(a|s  θ) ∼ N(cid:0)θTφ(s)  σ2(cid:1)  under the assumption of

uniformly bounded rewards and basis functions (Assumption 4.1)  using the following number of
H–step trajectories:

the gradient estimate (cid:98)∇θiJ RF

µ (θ) generated by REINFORCE is such that with probability 1 − δ:

R2M 2
i σ2 (1 − γ)2
δ2

φH(cid:0)1 − γH(cid:1)2
(cid:12)(cid:12)(cid:12) ≤ i.

 

µ (θ) − ∇θiJµ(θ)

N =

(cid:12)(cid:12)(cid:12)(cid:98)∇θiJ RF

5.2 Approximation with G(PO)MDP/PGT gradient estimator

µ

H(cid:88)

(cid:32) H(cid:88)

reason  we can limit our attention to the PGT formulation:

Although the REINFORCE method is guaranteed to converge at the true gradient at the fastest possi-
ble pace  its large variance can be problematic in practice. Advances in the likelihood ratio gradient
estimators have produced new approaches that signiﬁcantly reduce the variance of the estimate. Fo-
cusing on the class of “vanilla” gradient estimator  two main approaches have been proposed: policy
gradient theorem (PGT) [5] and G(PO)MDP [4]. In [6]  the authors show that  while the algorithms
(θ). For this

(θ) = (cid:98)∇θJ G(PO)MDP
look different  their gradient estimate are equal  i.e.  (cid:98)∇θJ P GT
(cid:32) H(cid:88)
(cid:33)(cid:33)

(cid:98)∇θJ P GT
l ∈ R have the objective to reduce the variance of the gradient estimate. Following the
where bn
procedure used to bound the approximation error of REINFORCE  we need an upper bound to the
variance of the gradient estimate of PGT that is provided by the following lemma (whose proof is
similar to the one used in [17] for the REINFORCE case).

Lemma 5.5. Given a Gaussian policy π(a|s  θ) ∼ N(cid:0)θTφ(s)  σ2(cid:1)  under the assumption of uni-
to the variance of the i–th component of the PGT gradient estimate (cid:98)∇θiJ P GT

formly bounded rewards and basis functions (Assumption 4.1)  we have the following upper bound

∇θ log π (an

l − bn

γl−1rn

k ; sn

k   θ)

(θ) =

1
N

n=1

k=1

l=k

µ

µ

l

 

(cid:16)(cid:98)∇θiJ P GT

µ

(cid:17) ≤

(θ)

V ar

(cid:20) 1 − γ2H
1 − γ2 + Hγ2H − 2γH 1 − γH
1 − γ

(θ):

µ

(cid:21)

.

R2M 2
φ

N (1 − γ)2 σ2

As expected  since the variance of the gradient estimate obtained with PGT is smaller than the one
with REINFORCE  also the upper bound of the PGT variance is smaller than REINFORCE one. In
particular  while the variance with REINFORCE grows linearly with the time horizon  using PGT
the dependence on the time horizon is signiﬁcantly smaller. Finally  we can derive the upper bound
for the approximation error of the gradient estimated of PGT.

Theorem 5.6. Given a Gaussian policy π(a|s  θ) ∼ N(cid:0)θTφ(s)  σ2(cid:1)  under the assumption of

uniformly bounded rewards and basis functions (Assumption 4.1)  using the following number of
H–step trajectories:

N =

the gradient estimate (cid:98)∇θiJ P GT

µ

R2M 2
φ
i σ2 (1 − γ)2
δ2

(cid:20) 1 − γ2H
1 − γ2 + Hγ2H − 2γH 1 − γH
1 − γ

(cid:21)

(θ) generated by PGT is such that with probability 1 − δ:

(cid:12)(cid:12)(cid:12)(cid:98)∇θi J P GT

µ

(cid:12)(cid:12)(cid:12) ≤ i.

(θ) − ∇θiJµ(θ)

7

1e − 07
1e − 06
1e − 05
1e − 04
1e − 03
1e − 05
1e − 04

0.50
itmax
itmax
17138
1675
⊥
itmax
itmax
24106

0.75
itmax
itmax
8669
697
⊥
itmax
itmax
7271

1.00
itmax
itmax
5120
499
⊥
itmax
itmax
3279

1.25
itmax
itmax
3348
⊥
⊥
itmax
⊥
1838

αconst

αt = α0
t

α∗

σ
1.50
itmax
23651
2342
⊥
⊥
itmax
⊥
1172

1.75
itmax
17516
1714
⊥
⊥
itmax
⊥
813

2.00
itmax
13480
1287
⊥
⊥
itmax
⊥
598

5.00
21888
2163
⊥
⊥
⊥
⊥
⊥
1

7.50
9740
849
⊥
⊥
⊥
⊥
⊥
58

Table 1: Convergence speed in exact LQG scenario with γ = 0.95. The table reports the number of
iterations required by the exact gradient approach  starting from θ = 0  to learn the optimal policy
parameter θ∗ = −0.6037 with an accuracy of 0.01  for different step–size values. Three different
set of experiments are shown: constant step size  decreasing step size  and the step size proposed in
Corollary 4.4. The table contains itmax when no convergence happens in 30  000 iterations  and ⊥
when the algorithm diverges (θ < −1 or θ > 0). Best performances are reported in boldface.

Number of trajectories

10  000

it
−0.0030
822
29  761 −0.2176

θ

RF
PGT

100  000

it

θ

51  731 −0.3068
63  985 −0.4013

500  000

it

θ

75  345 −0.4088
83  983 −0.4558

Table 2: Convergence speed in approximate LQG scenario with γ = 0.9. The table reports  starting

from θ = 0 and ﬁxed σ = 1  the number of iterations performed before the proposed step size (cid:98)α

t + a2

characterized by a transition model st+1 ∼ N(cid:0)st + at  σ2(cid:1)  Gaussian policy at ∼ N(cid:0)θ · s  σ2(cid:1)

becomes 0 and the last value of the policy parameter. Results are shown for different number of
trajectories (of 20 steps each) used in the gradient estimation by REINFORCE and PGT.
6 Numerical Simulations and Discussion
In this section we show results related to some numerical simulations of policy gradient in the
linear–quadratic Gaussian regulation (LQG) problem as formulated in [6]. The LQG problem is
and quadratic reward rt = −0.5(s2
t ). The range of state and action spaces is bounded to
the interval [−2  2] and the initial state is drawn uniformly at random. This scenario is particularly
instructive since it allows to exactly compute all terms involved in the bounds. We ﬁrst present
results in the exact scenario and then we move toward the approximated one.
Table 1 shows how the number of iterations required to learn a near–optimal value of the policy
parameter changes according to the standard deviation of the Gaussian policy and the step–size
value. As expected  very small values of the step size allow to avoid divergence  but the learning
process needs many iterations to reach a good performance (this can be observed both when the step
size is kept constant and when it decreases). On the other hand  larger step–size values may lead to
divergence. In this example  the higher the policy variance  the lower is the step size value that allows
to avoid divergence  since  in LQG  higher policy variance implies larger policy gradient values.
Using the step size α∗ from Corollary 4.4 the policy gradient algorithm avoids divergence (since
it guarantees an improvement at each iteration)  and the speed of convergence is strongly affected
by the variance of the Gaussian policy. In general  when the policy are nearly deterministic (small
variance in the Gaussian case)  small changes in the parameters lead to large distances between
the policies  thus negatively affecting the lower bound in Equation 1. As we can notice from the
expression of α∗ in Corollary 4.4  considering policies with high variance (that might be a problem in
real–world applications) allows to safely take larger step size  thus speeding up the learning process.
Nonetheless  increasing the variance over some threshold (making policies nearly random) produces
very bad policies  so that changing the policy parameter has a small impact on the performance 
and as a result slows down the learning process. How to identify an optimal variance value is
an interesting future research direction. Table 2 provides numerical results in the approximated
settings  showing the effect of varying the number of trajectories used to estimate the gradient by
REINFORCE and PGT. Increasing the number of trajectories reduces the uncertainty on the gradient
estimates  thus allowing to use larger step sizes and reaching better performances. Furthermore  the
smaller variance of PGT w.r.t. REINFORCE allows the former to achieve better performances.
However  even with a large number of trajectories  the approximated errors are still quite large
preventing to reach very high performance. For this reason  future studies will try to derive tighter
bounds. Further developments include extending these results to other policy models (e.g.  Gibbs
policies) and to other policy gradient approaches (e.g.  natural gradient).

8

References
[1] Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Intelligent Robots and

Systems  2006 IEEE/RSJ International Conference on  pages 2219–2225. IEEE  2006.

[2] James C Spall. Multivariate stochastic approximation using a simultaneous perturbation gra-

dient approximation. Automatic Control  IEEE Transactions on  37(3):332–341  1992.

[3] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist rein-

forcement learning. Machine Learning  8(3-4):229–256  May 1992.

[4] Jonathan Baxter and Peter L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of

Artiﬁcial Intelligence Research  15:319–350  2001.

[5] Richard S Sutton  David McAllester  Satinder Singh  and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. Advances in neural infor-
mation processing systems  12(22)  2000.

[6] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients.

Neural Networks  21(4):682–697  2008.

[7] Sham Kakade. A natural policy gradient. Advances in neural information processing systems 

14:1531–1538  2001.

[8] Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing  71(7):1180–1190  2008.
[9] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Math-

ematical Statistics  pages 400–407  1951.

[10] P. Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy

iteration. Advances in Neural Information Processing Systems  24  2011.

[11] Jorge J Mor´e and David J Thuente. Line search algorithms with guaranteed sufﬁcient decrease.

ACM Transactions on Mathematical Software (TOMS)  20(3):286–307  1994.

[12] J. Kober and J. Peters. Policy search for motor primitives in robotics. In Advances in Neural

Information Processing Systems 22 (NIPS 2008)  Cambridge  MA: MIT Press  2009.

[13] Nikos Vlassis  Marc Toussaint  Georgios Kontes  and Savas Piperidis. Learning model-free

robot control by a monte carlo em algorithm. Autonomous Robots  27(2):123–130  2009.

[14] S.M. Kakade. On the sample complexity of reinforcement learning. PhD thesis  PhD thesis 

University College London  2003.

[15] Matteo Pirotta  Marcello Restelli  Alessio Pecorino  and Daniele Calandriello. Safe policy
In Sanjoy Dasgupta and David McAllester  editors  Proceedings of the 30th In-
iteration.
ternational Conference on Machine Learning (ICML-13)  volume 28  pages 307–315. JMLR
Workshop and Conference Proceedings  May 2013.

[16] S. Pinsker. Information and Information Stability of Random Variable and Processes. Holden-

Day Series in Time Series Analysis. Holden-Day  Inc.  1964.

[17] Tingting Zhao  Hirotaka Hachiya  Gang Niu  and Masashi Sugiyama. Analysis and improve-

ment of policy gradient estimation. Neural Networks  26(0):118 – 129  2012.

9

,Matteo Pirotta
Marcello Restelli
Luca Bascetta