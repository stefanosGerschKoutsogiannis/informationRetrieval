2017,Adaptive Active Hypothesis Testing under Limited Information,We consider the problem of active sequential hypothesis testing where a Bayesian decision maker must infer the true hypothesis from a set of hypotheses.  The decision maker may choose for a set of actions  where the outcome of an action is corrupted by independent noise.    In this paper we consider a special case where the decision maker has limited knowledge about the distribution of observations for each action  in that only a binary value is observed.  Our objective is to infer the true hypothesis with low error  while minimizing the number of action sampled.  Our main results include the derivation of a lower bound on sample size for our system under limited knowledge and the design of an active learning policy that matches this lower bound and outperforms similar known algorithms.,Adaptive Active Hypothesis Testing under Limited

Information

Eindhoven University of Technology  Eindhoven  The Netherlands

Fabio Cecchi

f.cecchi@tue.nl

Nidhi Hegde

Nokia Bell Labs  Paris-Saclay  France

nidhi.hegde@nokia-bell-labs.com

Abstract

We consider the problem of active sequential hypothesis testing where a Bayesian
decision maker must infer the true hypothesis from a set of hypotheses. The
decision maker may choose for a set of actions  where the outcome of an action is
corrupted by independent noise. In this paper we consider a special case where the
decision maker has limited knowledge about the distribution of observations for
each action  in that only a binary value is observed. Our objective is to infer the
true hypothesis with low error  while minimizing the number of action sampled.
Our main results include the derivation of a lower bound on sample size for our
system under limited knowledge and the design of an active learning policy that
matches this lower bound and outperforms similar known algorithms.

1

Introduction

We consider the problem of active sequential hypothesis testing with incomplete information. The
original problem  ﬁrst studied by Chernoff [1]  is one where a Bayesian decision maker must infer
the correct hypothesis from a set of J hypotheses. At each step the decision maker may choose from
W actions where the outcome of an action is a random variable that depends on the action and the
true (hidden) hypothesis. In prior work  the probability distribution functions on the outcomes are
assumed to be known. In the present work we assume that these distributions are not known  and
only some rough information about the outcomes of the actions is known  to be made more precise
further on.
Active hypothesis testing is an increasingly important problem these days  with applications that
include the following. (a) Medical diagnostics ([2]) systems that include clinical trials for testing a
new treatment  or diagnostics of a new disease. (b) Crowdsourcing: online platforms for task-worker
matching such as Amazon’s Mechanical Turk or TaskRabbit  where  as new tasks arrive  they must
be matched to workers capable of working on them. (c) Customer hotline centres or Q&A forums:
online platforms such as StackExchange where questions are submitted  and users with varying
capabilities are available for providing an answer. This includes customer service centres where
customer tickets are submitted and the nature of the problem must be learned before its treatment (an
example where supervised learning techniques are used is [3]). (d) Content search problems where
an incoming image must be matched to known contents  as studied in [4].
We now informally describe our model. In the general instance of our problem  the true hypothesis 
θ∗ is one in a set of J hypotheses  J = {θ1  . . .   θJ}  and a set of W actions is available  where
the outcomes of the actions depend on the true hypothesis. When the true hypothesis is θj and

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

action w is chosen  a noisy outcome Xw j ∈ J is observed  whose distribution  pw j(·) ∈ P(J )  is
given. The objective then is to select an action at each step so as to infer the true hypothesis in a
minimum number of steps  with a given accuracy. In our model  we assume that the decision maker
has limited information about the outcome distributions. We deﬁne the principal set of an action w as
Jw ⊆ J . When action w is sampled  a noisy binary outcome y ∈ {−1  1} is observed  which gives
an indication on whether the action classiﬁes the hypothesis in the set Jw. The quality of action w 
αw is related to the noise in the outcome. Rather than the distributions pw j(·)  we assume that the
decision maker only has knowledge of the principal set Jw and quality αw of each action.

1.1 Related work

Since the seminal work by Chernoff [1]  active hypothesis testing and variants of the problemhave
been studied through various perspectives (see [5] for a brief survey). Chernoff derived a simple
heuristic algorithm whose performance is shown to achieve asymptotic optimality in the regime
where the probability of error vanishes. Speciﬁcally  it is shown that as the probability of error δ
decreases the expected number of samples needed by Chernoff’s algorithm grows as − log(δ). Most
of the past literature in active sequential hypothesis testing has dealt with extensions of Chernoff’s
model  and has shown that Chernoff’s algorithm performs well in more general settings [6  7]. A
notable exception is [8]  where the impact of the number of hypotheses is analyzed and an algorithm
that performs better than Chernoff’s benchmark is provided for the case of large values of J.
Our work differs from prior work in a few ways. First  the hypothesis need not be locally identiﬁable.
While in [1] each action is able to distinguish each pair of hypotheses  we assume that each hypothesis
is globally identiﬁable  i.e.  each pair of hypotheses can be discerned by at least one action. This is a
common assumption in the area of distributed hypothesis testing ([9  10]) and a weaker assumption
than that of Chernoff. Note that dropping this assumption is not novel in itself  and has been done
in other work such as [8]. Second  a novel extension in our work  differing from [8] is that we do
not assume full knowledge on the actions’ statistical parameters. The responses of actions are noisy 
and in past literature the probability distributions governing them was assumed to be known. In
our model  we drop this assumption  and we only require to know a lower bound αw > 1/2 on the
probability that action w will provide a correct response  no matter the hypothesis we want to test. As
far as we know  no previous work in active sequential learning has tackled the problem of incomplete
statistical information and we believe that such an extension may provide a non-negligible impact in
real-life applications.
Active hypothesis testing is similar to the problem of Bayesian active learning. This latter perspec-
tive in considered in [11] where noisy Bayesian active learning setting is used on the hypothesis
testing problem with asymmetric noise and a heuristic based on the extrinsic Jensen-Shannon (EJS)
divergence [12] is proposed. As in [8]  full knowledge of the probability distributions governing
the noise is available. In contrast  in our work we consider a more restricted model where  only a
binary outcome with noise is given by the actions on the large hypothesis space. Inference with
binary responses is considered in work on generalized binary search (GBS) [13]  which is special
case where the label set (outcome of actions) is binary with the case of symmetric  non-peristent
noise. Our work differs from this type of work in that we consider asymmetric label-dependent noise 
that is  αw varies with action w.
We thus position our work between [11  8] and [13]. While the former assumes full knowledge on
the noise distributions  we assume that only a binary response is provided and only a lower bound
on the value that governs the outcome is known  and while the latter considers symmetric noise  we
extend to asymmetric label-dependent noise.

Our contribution. Our main objective is to investigate the minimum sample query size of this
system for a certain level of accuracy in the inference of the true hypothesis  and to design efﬁcient
policies for this inference. Our contributions in the present paper are as follows. First  we consider the
system under limited knowledge of outcome distribution. This restricted scenario adds a signiﬁcant
constraint for the action selection policy  and the belief vector update policy. To the best of our
knowledge  this restricted scenario has not been considered in past literature. Second  under the
limited knowledge constraint  we propose the Incomplete-Bayesian Adaptive Gradient (IBAG) policy
which includes a belief vector update rule that we call Incomplete-Bayesian  and an action selection
rule  named Adaptive Gradient  that follows the drift of the (unknown) coordinate of interest in the

2

belief vector. Third  we derive a lower bound on the sample size for the system under incomplete
information  and show that the performance of IBAG matches this bound. We also carry out numerical
experiments to compare IBAG to prior work.

2 Model

The classic model of the active sequential learning problem consists in sequentially selecting one
of several available sensing actions  in order to collect enough information to identify the true
hypothesis  as considered in [1]. We thus consider a system where a decision maker has at his
disposal a ﬁnite set of actions W = {1  . . .   W}  and there are a set of J = |J | < ∞ possible
hypothesis  J = {θ1  . . .   θJ}. (For the rest of the paper  we refer to a hypothesis only by its index 
i.e.  j for hypothesis θj  for ease of notation.) When the true hypothesis is j and action w is sensed  the
outcome Xw j ∈ J is sampled from the distribution pw j(·) ∈ P(J )  i.e.  P{Xw j = j(cid:48)} = pw j(j(cid:48)).
In our model  we assume to have limited information about the actions and this affects the classic
model in two ways. First  for every sampled action w  a binary outcome y ∈ {−1  1} is observed 
indicating whether the inference of hypothesis by this action is in Jw or not  i.e.  the response
observed is Yw j ∈ {−1  1} where

Yw j =

if Xw j ∈ Jw 
if Xw j /∈ Jw.

−1 

(cid:26)1 
(cid:26)1 

qw j ≥ αw 

∀ j ∈ J   w ∈ W.

(2)

The subset Jw ⊆ J is assumed to be known  and it is described by the matrix g ∈ {−1  1}W×J
where

j belongs is given by qw j := P{Yw j = gw j} =(cid:80)

if j ∈ Jw 
if j /∈ Jw.
Observe that the probability an action w correctly identiﬁes the subset to which the true hypothesis
j(cid:48):gw j =gw j(cid:48) pw j(j(cid:48)). However  as a second
restriction  instead of knowing qw j  the capacity  or quality  of an action w is captured by αw where
we assume that

gw j =

−1 

(1)

We thus characterize each action by its principal set  Jw  and its quality  αw.
Assumption 1. For every action w ∈ W  the principal sets Jw ⊆ J and the quality αw ∈ (1/2  1)
are known. Denote by ∆w = 2αw − 1 where ∆w ∈ [∆m  ∆M ] and ∆m  ∆M ∈ (0  1).
Since each action can only indicate whether the hypothesis belongs to a subset or not  there must exist
an action w ∈ W for which j1 and j2 belong to different subsets  for all pairs j1  j2 ∈ J . Deﬁne the
subset Wj1 j2 ⊆ W as Wj1 j2 = {w ∈ W : gw j1 gw j2 = −1}.
Assumption 2. For every j1  j2 ∈ J   the subset Wj1 j2 is nonempty  i.e.  each hypothesis is globally
identiﬁable.
For every action w ∈ W and hypothesis j ∈ J we deﬁne the subsets Jw +j and Jw −j which are 
respectively  given by the hypotheses that action w cannot and can distinguish from j  i.e. 
Jw −j = {j(cid:48) ∈ J : gw j(cid:48)gw j = −1}.

Jw +j = {j(cid:48) ∈ J : gw j(cid:48)gw j = 1} 

Note that w ∈ Wj1 j2 if and only if j2 ∈ Jw −j1 (or equivalently j1 ∈ Jw −j2).
We aim to design a simple algorithm to infer the correct hypothesis using as few actions as possible.
The true hypothesis will be denoted by j∗ ∈ J . The learning process is captured by the evolution of
the belief vector ν(t) ∈ P(J )  where νj(t) denotes the decision maker’s conﬁdence at time t that
the true hypothesis is j. At the initial step t = 1  the belief vector ν(1) ∈ P(J ) is initialized so that
νj(1) > 0  j ∈ J . Since we assume to initially lack any information on the true hypothesis  without
loss of generality  we set νj(1) = 1/J for every j ∈ J .
At every step t ≥ 1  according to the belief vector ν(t)  the decision maker determines the next
action to sense FW (ν(t)) = w(t) ∈ W according to some selection rule FW (·). The outcome
y(t) ∈ {−1  1} from the chosen action w(t) is used to update the belief vector according to an
update rule F U

(cid:1) = ν(t + 1) ∈ P(J ). The algorithm ends at time T ∗  and the

(cid:0)ν(t)  w(t)  y(t)

3

inferred hypothesis is given by ˆj = arg maxj∈J νj(T ∗) . Sensing actions is stopped when one of
the posteriors is larger than 1 − δ  for some δ > 0:
{max
j∈J νj(t) > 1 − δ}.

T ∗ = inf
t≥0

(3)

3 The Incomplete-Bayesian update rule

We now describe how the decision maker updates the belief vector after he observes the outcome of
an action. Given a belief vector ν ∈ P(J ) and the observation y ∈ {−1  1} obtained from action
w ∈ W  deﬁne

˜f (y  j  w) =

1 − qw j 

y = gw j 
y = −gw j 

f (y  j  w) =

1 − αw 

y = gw j 
y = −gw j.

U j(ν  w  y) =

Note that ˜f (y  j  w) denotes the probability of having outcome y given that the action w is chosen and
the true hypothesis is j. The standard Bayesian update rule is given by the map F B
U (ν  w  y)  where
. In our model  however  the values qw j for w ∈ W are unknown to
F B
the decision maker. Hence  we introduce the Incomplete Bayesian (IB) update rule  which mimics the
Bayesian rule  but with limited knowledge on outcome probailities. The IB update rule is given by
the map F U (ν  w  y)  where

˜f (y j w)νj
i∈J ˜f (y i w)νi

(cid:80)

(cid:26)qw j 

(cid:26)αw 

FU j(ν  w  y) =

.

(4)

(cid:80)

f (y  j  w)νj
i∈J f (y  i  w)νi

Observe that Bayesian and IB update rules are identical when qw j = αw.
In practice  the νj(t) evolves according to both the quality of the chosen action  αw  and the
relation between this action’s principal set Jw and the current state of the belief vector ν(t). This
dependence is formalized in the following lemma whose proof is included in the supplementary
material  Section B.
Lemma 1. Given ν(t) ∈ P(J ) and w(t) ∈ W  then it holds that

νj∗ (t + 1)
νj(t + 1)

=

νj∗ (t)
νj(t)

×

1 
indic1{w(t) /∈ Wj∗ j} 
1+∆w(t)
1−∆w(t)
1−∆w(t)
1+∆w(t)

 

 

w.p.

w.p. 1{w(t) ∈ Wj∗ j}qw(t) j∗  
w.p. 1{w(t) ∈ Wj∗ j}(1 − qw(t) j∗ ).



3.1 A lower bound on the sample size

Note that the IB update rule alone sets some constraints on the performance. In particular  if we
require the error probability to be low  then the expected number of samples is necessarily larger than
a certain quantity depending on the model parameters. We show that this quantity asymptotically
grows as − log δ in the asymptotic regime where δ → 0.
Theorem 1. Assume the IB update rule is applied to the belief vector and that

Then  there exist functions K l

1(δ) such that

P{νj∗ (T ∗) ≤ δ} ≤ ˜γ < 1.

lim
δ→0
0(δ)  K l

E[T ∗] ≥ K l

1(δ) log

1
δ

+ K l

0(δ) 

i(δ) ≥ K l
K l

i > 0 

lim
δ→0

for i = 0  1.

The proof of this result is presented in the supplement  Section A.2. We sketch the proof here. We
ﬁrst deﬁne

and show that  on the one hand  if P{ˆj (cid:54)= j∗} is small  then(cid:80)
probability  and on the other hand  if t is small  then(cid:80)

St(j1  j2) = log

νj1 (t)
νj2 (t)

 

S(j1  j2) = ST ∗ (j1  j2) 

j(cid:54)=j∗ S(j∗  j) is large with high
j(cid:54)=j∗ St(j∗  j) is small with high probability.

4

We use these properties to derive a lower bound on the tail probability of T ∗  and thus on its expected
value.
Further  we can control the belief vector evolution by deriving bounds on the ratio between coordinates
of the belief vector under the IB policy. Speciﬁcally  in the supplementary material Section A.3 
we bound the probability that νj(t) > νj∗ (t) at a certain time  and investigate how this probability
evolves with t.

4 Adaptive Gradient: the action selection policy

4.1 A gradient-based selection policy

We now present an action selection policy that  together with the IB update rule  deﬁnes our active
learning algorithm  which we call the Incomplete-Bayesian Adaptive Gradient (IBAG) policy. We
will then analyze the complete algorithm showing that its performance asymptotically matches the
lower bound provided in Theorem 1 as δ → 0.
We focus on the j∗-th coordinate of the belief vector  and deﬁne the drift at time t as

Dw(ν(t)) = E[νj∗ (t + 1)|ν(t)  w(t) = w] − νj∗ (t).

Simple algebra and (4) yield the following Lemma.
Lemma 2. It holds that

Dw(ν(t)) = 4∆wνj∗ (t)νw −j∗ (t)

where

νw +j =

(cid:16) qw j∗ − αw + ∆wνw −j∗ (t)
(cid:0)1 − 2νw −j∗ (t)(cid:1)2
(cid:88)

1 − ∆2

w

νj 

νw −j =

νj.

j∈Jw −j

(cid:88)

j∈Jw +j

(cid:17)

 

(5)

Assume for a moment that we know the true hypothesis j∗ and qw j∗ for every w ∈ W. Then  in
order to let νj∗ (t) grow as much as possible  we would greedily select the action w which maximizes
Dw(ν(t)). Our worker selection policy will attempt to mimic as closely as possible this greedy
policy  while operating without complete information.
Lemma 3. It holds that Dw(ν(t)) ≥ DL
w(ν(t))  where

DL

w(ν(t)) = 4νj∗ (t)

(6)

and

ν−w(t) = min

∆2

wν2−w(t)

(cid:0)1 − 2ν−w(t)(cid:1)2  
(cid:111)
(cid:88)

νj(t)

1 − ∆2

w

(cid:110) (cid:88)

νj(t) 

j∈Jw

j /∈Jw

The proof follows from the fact that Dw(ν(t)) is increasing both in qw j∗ and νw −j∗ (t) for every
w ∈ W  and the observation that that qw j∗ ≥ αw and νw −j∗ (t) ≥ ν−w(t).
Note that DL
w(ν(t)) provides us a tight lower bound on the expected growth of the coordinate of the
true hypothesis if action w is chosen at step t. Indeed  DL
w(ν(t)) can be decomposed to a part that
uses the j∗-th coordinate of the belief vector and a part than can be computed without knowing j∗.
The Adaptive Gradient (AG) selection rule  then chooses at step t  the action wD(t) ∈ W such that

1 − d2(cid:0)1 − 2v(cid:1)2  

d2v2

(7)

wD(t) = FW (ν(t)) = arg max

w∈W G(ν−w  ∆w) 

G(v  d) =

i.e.  we select the action maximizing the current lower bound on the expected growth of the j∗-
coordinate of the belief vector. Ties are broken uniformly.
Remark: Assume the actions have different costs of sensing. The AG selection rule can then be
generalized as follows:

wD(t) = F c

W (ν(t)) = arg max
w∈W

G(ν−w  ∆w)

cw

.

(8)

5

4.2 An upper bound

We now present our main result. We show that the expected number of samples required by our
algorithm IBAG asymptotically matches the lower bound obtained in Theorem 1.
Theorem 2. Under the IBAG algorithm  there exist constants K u
that

1 > 0 independent of δ such

0   K u

E[T ∗] ≤ K u

1 log

1
δ

+ K u
0 .

The proof is provided in supplementary material  Section A.5. This result is based on the intuition
that IBAG never selects an action that is too uninformative relative to the other actions. Speciﬁcally 
the information provided by an action w at time t depends on its quality αw and outcome over the
subset Jw −j∗. In other words  the value νw −j∗ must decrease to 0  hence the higher this value is
for a given action w  the more we can still learn from sensing this action. As a proxy for νw −j∗ we
use ν−w which also must be as large as possible. The following lemma  whose proof is given in
supplementary material  Section B  provides bounds on the relative quality of ν−wD(t) compared to
ν−w.
Lemma 4. For every w ∈ W  it holds that ν−wD(t) ≥ ∆m

∆M ν−w.

5 Numerical results

We now present numerical results based on simulations. In order to gain practical insight  we will
focus on a task labelling application. A task labelling problem might arise in a crowdsourcing scenrio
such as Amazon’s Mechanical Turk or Content search problems where an incoming image must be
matched to known contents. The mapping to the hypothesis testing problem is as follows. The set
of hypotheses J corresponds to the set of task labels  with j∗ the true hypothesis being the latent
task label that must be inferred. The set of W actions corresponds to W workers who perform the
labelling when sampled  where pw j(j(cid:48)) is the probability that worker w assigns the task the label j(cid:48)
when the true label is j. For each worker w  we will call Jw the expertise of the worker (principal
set of the actions)  and αw will be the quality of the worker. We will ﬁrst investigate the impact of
the lack of exact knowledge  i.e.  the difference between αw and qw j  that we call slack. We then
compare our algorithm to that in [1] and that of [13] for a few scenarios of interest.

5.1 The effect of the slack
Here we present a simulated scenario with J = 100  W = 15  and ﬁxed subsets {Jw}w∈W satisfying
Assumption 2. We set δ ≈ 0.001  and assume the incoming job-type to be j∗ = 1. In Figure 1
we present the results of 1000 runs of the simulation for every instance of respectively the ﬁrst and
second scenario described below. Recalling that the simulation stops as soon as maxj νj(t) > 1 − δ 
we specify that out of the entire set of simulations of these scenarios the algorithm never failed to infer
the correct incoming job type j∗ = 1. For both scenarios  in Figure 1(left) we display the averaged
sample paths of the coordinate νj∗ (t) and in Figure 1(right) the average sample size required for the
decision maker to make an inference.

The performance upper bound is pessimistic.
In the ﬁrst set of simulations  scenario A  we ﬁx
the quality vector α with αw ∈ (0.55  0.6) for every worker w ∈ W. We then let the parameter s
vary in {0  .05  .1  .15  .2  .25  .3} and assume qw j∗ = αw + s for every w ∈ W. In Theorem 2 we
proved an upper bound for E[T ∗] when the IBAG algorithm is employed. It can be observed that
the upper bound does not depend on qw j∗  but only on αw. In fact  the upper bound is obtained by
looking at the worst case scenario  where qw j∗ = αw for every w ∈ W and j ∈ J . As the slack s
grows  the performance of the algorithm drastically improves even if it is not reﬂected in the upper
bound term.

Robustness to perturbations in estimate of worker skills.
In the second set of simulations 
scenario B we ﬁx the quality vector qw j∗ ∈ (0.85  0.9) for every worker w ∈ W. We then let
the parameter s vary in {0  .05  .1  .15  .2  .25  .3} and set αw = qw j∗ − s for every w ∈ W. It is
observed that the IBAG algorithm performs well even when the decision maker’s knowledge of the
skills is not precise  and he decides to play safe by reducing the lower bound α(w).

6

(a) Scenario A

(b) Scenario B

Figure 1: ((a)  (b) left) Empirical average of the sample paths of the process νj∗ (t)  ((a)  (b) right)
Empirical average of the sample size T ∗.

We therefore deduce that the learning process strongly depends on the true skills of the worker qw j
(Figure 1(a))  however their exact knowledge is not fundamental for IBAG to behave well (Figure
1(b)) - it is robust to small perturbations.

5.2 Comparison to existing algorithms

Chernoff algorithm. As we mentioned  most of the existing sequential hypothesis testing algorithms
are based on Chernoff’s algorithm presented in [1]. Such an algorithm  at step t identiﬁes the
job-types j1  j2 ∈ J associated with the two highest values of ν(t) and selects the class of workers
wC that best distinguishes j1 and j2  i.e.  wC = arg maxw∈Wj1 j2
αw. In the asymptotic regime with
δ → 0  the expected sample size required by the Chernoff’s algorithm is of order − log δ  exactly
as with IBAG. This has been proven ([1  8]) in the case with full knowledge of the matrix pw j(·).
What we emphasize here is that by focusing only on the two highest components of ν(t)  the decision
maker loses information that might help him make a better selection of worker w(t). In particular 
Chernoff’s algorithm bases its decision largely on the workers’ skills and thus does not behave as
well as it should when these are not informative enough.
Soft-Decision GBS algorithm. The algorithm proposed in [13] generalizes the intuition behind
optimal GBS algorithms in noiseless environments. This algorithm  given a belief vector ν(t) at
νj −
step t picks the worker ¯w such that ¯w = arg minw

(cid:12)(cid:12) = arg maxw{ν−w}. Intuitively  the Soft-Decision GBS algorithm selects the worker that

(cid:12)(cid:12) = arg minw

(cid:12)(cid:12)(cid:80)

(cid:12)(cid:12)(cid:80)

j∈Jw

(cid:80)

j /∈Jw

νj

j∈J νjgw j

is the most "unsure"  in the sense that the worker splits the belief vector as evenly as possible. Since
the model in [13] does not allow for different qualities of the workers (noise is symmetric there)  this
feature does not play a role on the worker selection policy. Note that when the quality of all workers
are identical  the Soft-Decision GBS and the IBAG algorithms are identical. In [13]  an asymptotic
performance analysis is presented  and under certain constraints on the problem geometry  it is shown
that the sample size required is of order − log δ + log J  and once again the performance in terms of
the error probability matches with IBAG.
We now compare our algorithm IBAG with the Chernoff algorithm under three scenarios and with
Soft-Decision GBS only for the third scenario where the quality αw or workers (noise in GBS) differ
among the workers.
In the ﬁrst scenario  we set J = 32  j∗ = 1  and δ = 0.003. We assume two kinds of worker classes.
We have 5 ‘generalist’ workers  each of whom has |Jw| = J/2 = 16 and moreover for every pair of
job types (j1  j2) there exists a generalist belonging to Wj1 j2. In addition  we have 32 ‘specialist’
workers who can distinguish exactly 1 job-type  i.e.  |Jw| = 1. We assume that there is one specialist
per job-type  and note that among them there is also w∗ such that Jw∗ = {j∗}. We consider two
cases: in case A  the skills of the workers are identical  αw = 0.8 for every w ∈ W  and in case B we
drop the generalists’ skill level to αw = 0.75. We assume qw j = αw for every w ∈ W and j ∈ J .
In the second scenario  we set J = 30 with only specialists present. We set δ = 0.003 and j∗ = 1. In
this scenario we consider two cases as well  in case A αw = 0.7 for every worker  while in case B we
drop the skill level of the specialist on job-type j∗ to 0.65  representing a situation where the system
is ill-prepared for an incoming job. We assume qw j = αw for every w ∈ W and j ∈ J .
We display the results for both scenarios in Figure 2. In Figure 2(top) we display boxplots of the
number of queries required and in Figure 2(bottom) we show the expectation of the number of
queries per kind of worker. In both scenarios  the performance of Chernoff’s algorithm is drastically

7

(a) Scenario 1

(b) Scenario 2

(c) Scenario 3

Figure 2: (top) Boxplot of the sample size T ∗. (bottom) Empirical expected number of times the
different groups of workers are queried.

weakened by only a tiny variation in αw  yielding a very different behavior. In the ﬁrst scenario 
although it is very informative to query the generalists in an early explorative stage  under Chernoff’s
algorithm the selection of the workers relies too much on the skill levels and therefore always queries
the specialists. The IBAG algorithm  on the other hand  sensibly decides at each step on the trade-off
between getting rough information on a larger set of job pairs  or getting more precise information on
a smaller set  and seems to better grasp this quality vs quantity dilemma.
Similarly  in case B of the second scenario  the low-quality workers (the specialist in j∗) are never
selected by Chernoff’s algorithm  even if their responses have a large impact on the growth of νj∗ (t).
For both cases A and B we see that IBAG outperforms Chernoff.
In the third scenario we set J = 32  W = 42  and δ = 0.03. We have ﬁve low-quality generalist
workers with αw = 0.55  ﬁve high-quality generalist workers with αw = 0.75. The remaining
32 workers are specialists with αw = 0.8. The plots comparing all three algorithms is shown in
Figure 2(iii). We observe again that the Chernoff algorithm never queries generalists and performs
the worst. IBAG outperforms Soft-GBS because it queries high-quality workers preferentially while
Soft-GBS doesn’t consider quality.

6 Discussion and conclusion

We have presented and analyzed the IBAG algorithm  an intuitive active sequential learning algorithm
which requires only a rough knowledge of the quality and principal set of each available action.
The algorithm is shown to be competitive and in many cases outperforms Chernoff’s algorithm  the
benchmark in the area.
As far as we know  this is the ﬁrst attempt to analyze a scenario where the decision maker has limited
knowledge of the system parameters. In Section 5 we studied through simulations  the effect of
this lack of exact knowledge on the performances of the system  in order to quantify the tradeoff
between caution  i.e.  how close αw is to qw j  and the cost. The numerical analysis suggests that
a moderate caution does not worsen drastically the performance. In the supplement Section C we
analyze formally this tradeoff and show results on how cautious the decision maker can be while still
ensuring good performance.
A further element of incomplete knowledge would be to allow slight perturbations on the principal
sets of the actions. In the present paper we have assumed to know with certainty  for every w ∈ W
and j ∈ J   whether w has j in its principal set (j ∈ Jw)  or not. In future work we will investigate
the impact of uncertainty in the expertise  for instance having j ∈ Jw with some probability pj w.

8

As a last remark  it would be interesting to analyze the model when the different actions have
heterogeneous costs. Note that the IBAG algorithm naturally extends to such case  as mentioned in
equation (8). The IBAG algorithm in the framework of the task-worker system could give deﬁnitive
answers on whether it is better to sample a response from a cheap worker with a general expertise
and low skill or from more expensive workers with narrow expertise and higher skill.

References
[1] H. Chernoff  “Sequential design of experiments ” The Annals of Mathematical Statistics  vol. 30  no. 3 

pp. 755–770  1959.

[2] S. Berry  B. Carlin  J. Lee  and P. Muller  Bayesian Adaptive Methods for Clinical Trials. CRC press  2010.
[3] S. C. Hui and G. Jha  “Data mining for customer service support ” Information & Management  vol. 38 

no. 1  pp. 1–13  2000.

[4] N. Vaidhiyan  S. P. Arun  and R. Sundaresan  “Active sequential hypothesis testing with application to a
visual search problem ” in 2012 IEEE International Symposium on Information Theory Proceedings (ISIT) 
pp. 2201–2205  IEEE  2012.

[5] B. Ghosh  “A brief history of sequential analysis ” Handbook of Sequential Analysis  vol. 1  1991.
[6] A. Albert  “The sequential design of experiments for inﬁnitely many states of nature ” The Annals of

Mathematical Statistics  vol. 32  pp. 774–799  1961.

[7] J. Kiefer and J. Sacks  “Asymptotically optimum sequential inference and design ” The Annals of Mathe-

matical Statistics  vol. 34  pp. 705–750  1963.

[8] M. Naghshvar and T. Javidi  “Active sequential hypothesis testing ” The Annals of Statistics  vol. 41  no. 6 

pp. 2703–2738  2013.

[9] A. Lalitha  A. Sarwate  and T. Javidi  “Social learning and distributed hypothesis testing ” in Information

Theory (ISIT)  2014 IEEE International Symposium on  pp. 551–555  IEEE  2014.

[10] R. Olfati-Saber  J. Fax  and R. Murray  “Consensus and cooperation in networked multi-agent systems ”

Proceedings of the IEEE  vol. 95  no. 1  pp. 215–233  2007.

[11] M. Naghshvar  T. Javidi  and K. Chaudhuri  “Noisy bayesian active learning ” in Communication  Control 

and Computing (Allerton)  2012 50th Annual Allerton Conference on  pp. 1626–1633  IEEE  2012.

[12] M. Naghshvar and T. Javidi  “Extrinsic jensen-shannon divergence with application in active hypothesis

testing ” in IEEE International Symposium on Information Theory (ISIT)  2012.

[13] R. Nowak  “Noisy generalized binary search ” in Advances in neural information processing systems 

pp. 1366–1374  2009.

9

,Jimmy Ba
Geoffrey Hinton
Volodymyr Mnih
Joel Leibo
Catalin Ionescu
Fabio Cecchi
Nidhi Hegde