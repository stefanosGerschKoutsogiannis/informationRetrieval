2018,Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting,We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework  where we recursively approximate the posterior after every task with a Gaussian  leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode  which is typically intractable for modern architectures. In order to make our method scalable  we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset  substantially outperforming related methods for overcoming catastrophic forgetting.,Online Structured Laplace Approximations for

Overcoming Catastrophic Forgetting

Hippolyt Ritter1∗
1University College London

2Alan Turing Institute

3reinfer.io

Aleksandar Botev1

David Barber1 2 3

Abstract

We introduce the Kronecker factored online Laplace approximation for overcoming
catastrophic forgetting in neural networks. The method is grounded in a Bayesian
online learning framework  where we recursively approximate the posterior after
every task with a Gaussian  leading to a quadratic penalty on changes to the weights.
The Laplace approximation requires calculating the Hessian around a mode  which
is typically intractable for modern architectures. In order to make our method
scalable  we leverage recent block-diagonal Kronecker factored approximations to
the curvature. Our algorithm achieves over 90% test accuracy across a sequence
of 50 instantiations of the permuted MNIST dataset  substantially outperforming
related methods for overcoming catastrophic forgetting.

1

Introduction

Creating an agent that performs well across multiple tasks and continuously incorporates new
knowledge has been a longstanding goal of research on artiﬁcial intelligence. When training on a
sequence of tasks  however  the performance of many machine learning algorithms  including neural
networks  decreases on older tasks when learning new ones. This phenomenon has been termed
‘catastrophic forgetting’ [6  26  33] and has recently received attention in the context of deep learning
[8  16]. Catastrophic forgetting cannot be overcome by simply initializing the parameters for a
new task with optimal ones from the old task and hoping that stochastic gradient descent will stay
sufﬁciently close to the original values to maintain good performance on previous datasets [8].
Bayesian learning provides an elegant solution to this problem. It combines the current data with
prior information to ﬁnd an optimal trade-off in our belief about the parameters. In the sequential
setting  such information is readily available: the posterior over the parameters given all previous
datasets. It follows from Bayes’ rule that we can use the posterior over the parameters after training
on one task as our prior for the next one. As the posterior over the weights of a neural network is
typically intractable  we need to approximate it. This type of Bayesian online learning has been
studied extensively in the literature [31  7  13].
In this work  we combine Bayesian online learning [31] with the Kronecker factored Laplace
approximation [34] to update a quadratic penalty for every new task. The block-diagonal Kronecker
factored approximation of the Hessian [23  2] allows for an expressive scalable posterior that takes
interactions between weights within the same layer into account. In our experiments we show that
this principled approximation of the posterior leads to substantial gains in performance over simpler
diagonal methods  in particular for long sequences of tasks.

∗Corresponding author: j.ritter@cs.ucl.ac.uk

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

2 Bayesian online learning for neural networks

We are interested in optimizing the parameters θ of a single neural network to perform well across
multiple tasks D1  . . .  DT   speciﬁcally ﬁnding a MAP estimate θ∗ = arg maxθ p(θ|D1  . . .  DT ).
However  the datasets arrive sequentially and we can only train on one of them at a time.
In the following  we ﬁrst discuss how Bayesian online learning solves this problem and introduce an
approximate procedure for neural networks. We then review recent Kronecker factored approxima-
tions to the curvature of neural networks and how to use them to obtain a better ﬁt to the posterior.
Finally  we introduce a hyperparameter that acts as a regularizer on the approximation to the posterior.

2.1 Bayesian online learning

Bayesian online learning [31]  or Assumed Density Filtering [25]  is a framework for updating an
approximate posterior when data arrive sequentially. Using Bayes’ rule we would like to simply
incorporate the most recent dataset Dt+1 into the posterior as:

p(θ|D1:t+1) =

(cid:82) dθ(cid:48)p(Dt+1|θ(cid:48))p(θ(cid:48)|D1:t)

p(Dt+1|θ)p(θ|D1:t)

(1)

(2)

where we use the posterior p(θ|D1:t) from the previously observed tasks as the prior over the
parameters for the most recent task. As the posterior given the previous datasets is typically intractable 
Bayesian online learning formulates a parametric approximate posterior q with parameters φt  which
it iteratively updates in two steps:

Update step In the update step  the approximate posterior q with parameters φt from the previous
task is used as a prior to ﬁnd the new posterior given the most recent data:

p(θ|Dt+1  φt) =

(cid:82) dθ(cid:48)p(Dt+1|θ(cid:48))q(θ(cid:48)|φt)

p(Dt+1|θ)q(θ|φt)

Projection step The projection step ﬁnds the distribution within the parametric family of the
approximation that most closely resembles this posterior  i.e. sets φt+1 such that:

q(θ|φt+1) ≈ p(θ|Dt+1  φt)

(3)

Opper and Winther [31] suggest minimizing the KL-divergence between the approximate and the
true posterior  however this is mostly appropriate for models where the update-step posterior and a
solution to the KL-divergence are available in closed form. In the following  we therefore propose
using a Laplace approximation to make Bayesian online learning tractable for neural networks.

2.2 The online Laplace approximation

Neural networks have found wide-spread success and adoption by performing simple MAP inference 
i.e. ﬁnding a mode of the posterior:

θ∗ = arg max

log p(θ|D) = arg max

log p(D|θ) + log p(θ)

θ

θ

(4)

where p(D|θ) is the likelihood of the data and p(θ) the prior. Most commonly used loss functions
and regularizers ﬁt into this framework  e.g. using a categorical cross-entropy with L2-regularization
corresponds to modeling the data with a categorical distribution and placing a zero-mean Gaussian
prior on the network parameters. A local mode of this objective function can easily be found using
standard gradient-based optimizers.
Around a mode  the posterior can be locally approximated using a second-order Taylor expansion 
resulting in a Normal distribution with the MAP parameters as the mean and the Hessian of the
negative log posterior around them as the precision. Using a Laplace approximation for neural
networks was pioneered by MacKay [22].

2

We therefore proceed in two iterative steps similar to Bayesian online learning  using a Gaussian
approximate posterior for q  such that φt = {µt  Λt} consists of a mean µ and a precision matrix Λ:
Update step As the posterior of a neural network is intractable for all but the simplest architectures 
we will work with the unnormalized posterior. The normalization constant is not needed for ﬁnding a
mode or calculating the Hessian. The Gaussian approximate posterior results in a quadratic penalty
encouraging the parameters to stay close to the mean of the previous approximate posterior:

log p(θ|Dt+1  φt) ∝ log p(Dt+1|θ) + log q(θ|φt)

∝ log p(Dt+1|θ) − 1
2

(θ − µt)(cid:62)Λt(θ − µt)

(5)

Projection step In the projection step we approximate the posterior with a Gaussian. We ﬁrst
update the mean of the approximation to a mode of the new posterior:

µt+1 = arg max

θ

log p(Dt+1|θ) + log q(θ|φt)

(6)

and then perform a quadratic approximation around it  which requires calculating the Hessian of the
negative objective. This leads to a recursive update to the precision with the Hessian of the most
recent log likelihood  as the Hessian of the negative log approximate posterior is its precision:

(cid:12)(cid:12)(cid:12)θ=µt+1

Λt+1 = Ht+1(µt+1) + Λt

(7)

∂θ∂θ

where Ht+1(µt+1) = − ∂2 log p(Dt+1|θ)
is the Hessian of the newest negative log likelihood
around the mode. The precision of a Gaussian is required to be positive semi-deﬁnite  which is the
case for the Hessian at a mode. In order to numerically guarantee this in practice  we use the Fisher
Information as an approximation [24] that is positive semi-deﬁnite by construction.
The recursion is initialized with the Hessian of the log prior  which is typically constant. For a
zero-mean isotropic Gaussian prior  corresponding to an L2-regularizer  it is simply the identity
matrix times the prior precision.2
A desirable property of the Laplace approximation is that the approximate posterior becomes peaked
around its current mode as we observe more data. This becomes particularly clear if we think of the
precision matrix as the product of the number of data points and the average precision. By becoming
increasingly peaked  the approximate posterior will naturally allow the parameters to change less for
later tasks. At the same time  even though the Laplace method is a local approximation  we would
expect it to leave sufﬁcient ﬂexibility for the parameters to adapt to new tasks  as the Hessian of
neural networks has been observed to be ﬂat in most directions [36].
We will also compare to ﬁtting the true posterior with a new Gaussian at every task for which we
compute the Hessian of all tasks around the most recent MAP estimate:

t+1(cid:88)

Λt+1 = Hprior +

Hi(µt+1)

(8)

i=1

This procedure differs from the online Laplace approximation only in evaluating all Hessians at
the most recent MAP parameters instead of the respective task’s ones. Technically  this is not a
valid Laplace approximation  as we only optimize an approximation to the posterior. Hence the
optimal parameters for the approximate objective will not exactly correspond to a mode of the true
posterior. However  as we will use a positive semi-deﬁnite approximation to the Hessian  this will
only introduce a small additional approximation error.
Calculating the Hessian across all datasets requires relaxing the sequential learning setting to allowing
access to previous data ‘ofﬂine’  i.e. between tasks. We use this baseline to check if there is any loss
of information in using estimates of the curvature at previous parameter values.

2Huszár [14] recently discussed a similar recursive Laplace approximation for online learning  however with

limited experimental results and in the context of using a diagonal approximation to the Hessian.

3

2.3 Kronecker factored approximation of the Hessian

Modern networks typically have millions of parameters  so the size of the Hessian is several terabytes.
An approximation that is simple to implement with automatic differentiation frameworks is the
diagonal of the Fisher matrix  i.e. the expected square of the gradients  where the expectation is over
the datapoints and the conditional distribution deﬁned by the model. While this approximation has
been used successfully [16]  it ignores interactions between the parameters.
Recent works on second-order optimization [23  2] have developed block-diagonal approximations
to the Hessian. They exploit that  for a single data point  the diagonal blocks of the Hessian of a
feedforward network — corresponding to the weights of a single layer — are Kronecker factored  i.e.
a product of two relatively small matrices.
We denote a neural network as taking an input a0=x and producing an output hL. The input is passed
through layers 1  . . .   L as the linear pre-activations hl=Wlal−1 and the activations al=fl(hl)  where
fl is a non-linear elementwise function. The outputs then parameterize the log likelihood of the data 
and  using the chain rule  we can write the Hessian w.r.t. the weights of a single layer as:

Hl =

∂2 log p(D|hL)

∂ vec(Wl)∂ vec(Wl)

= Ql ⊗ Hl

(9)

where vec(Wl) is the weight matrix of layer l stacked into a vector and we deﬁne Ql = al−1a(cid:62)
l−1
as the covariance of the inputs to the layer. Hl = ∂2 log p(D|θ)
is the pre-activation Hessian  i.e. the
second derivative w.r.t. the pre-activations hl of the layer. We provide the basic derivation of Eq. (9)
and the recursive formula for calculating Hl in Appendix A. To maintain the Kronecker factorization
in expectation  i.e. for an entire dataset  [23] and [2] assume the two factors to be independent and
approximate the expected Kronecker product by the Kronecker product of the expected factors.
The block-diagonal approximation splits the Hessian-vector product in the quadratic penalty across
the layers. Due to the Kronecker factored approximation  it can be calculated efﬁciently for each
layer using the following well-known identity:

∂hl∂hl

(Ql ⊗ Hl) vec(Wl − W ∗

l ) = vec(Hl (Wl − W ∗

l )Ql)

(10)

where vec stacks the columns of a matrix into a vector and we use that H is symmetric.
The block-diagonal Kronecker factored approximation corresponds to assuming independence be-
tween the layers and factorizing the covariance between the weights of a layer into the covariance
of the columns and rows  resulting in a matrix normal distribution [11]. The same approximation
has been used recently to sample from the predictive posterior [34  9]. While it still makes some
independence assumptions about the weights  the most important interactions — the ones within the
same layer — are accounted for. In order to guarantee for the curvature being positive semi-deﬁnite 
we approximate the Hessian with the Fisher Information as in [23] throughout our experiments.

2.4 Regularizing the approximate posterior

Kirkpatrick et al. [16]  who develop a similar method inspired by the Laplace approximation  suggest
using a multiplier λ on the quadratic penalty in Eq. (5). This hyperparameter provides a way of
trading off retaining performance on previous tasks against having sufﬁcient ﬂexibility for learning a
new one. As modifying the objective would propagate into the recursion for the precision matrix  we
instead place the multiplier on the Hessian of each log likelihood and update the precision as:

Λt+1 = λHt+1(µt+1) + Λt

(11)

The multiplier affects the width of the approximate posterior and thus the location of the next MAP
estimate. As it acts directly on the parameter of a probability distribution  its optimal value can
inform us about the quality of our approximation: if it strongly deviates from its natural value of 1 
our approximation is a poor one and over- or underestimates the uncertainty about the parameters.
We visualize the effect of λ in Fig. 5 in Appendix B.

4

2.5 Computational complexity

Our method requires calculating the expectations of the two Kronecker factors from Eq. (9) over the
data of the most recent task after training on it as well as calculating the quadratic penalty in Eq. (5)
using the identity in Eq. (10) for every parameter update.
Calculating the Kronecker factors can efﬁciently be mini-batched and requires the same calculations
as a forward and backward pass through the network plus two additional matrix-matrix products. The
overall cost is thus effectively equivalent to that of an extra training epoch. See [23] for more details.
The computational complexity of calculating the quadratic penalty is dominated by the two matrix-
matrix products in Eq. (10). Assuming that all L layers of the network as well as the inputs and
outputs are of dimensionality d  all weight matrices as well as the Kronecker factors will be of
dimensionality d × d.3 The complexity of calculating the penalty for all layers is then O(Ld3).
Finally  we note that sums of Kronecker products do not add up pairwise  i.e. A ⊗ B + C ⊗ D (cid:54)=
(A + C) ⊗ (B + D)  so the corresponding Kronecker factors of different tasks do not simply add.
In our implementation  we keep an approximate Hessian for every task in memory  similar to how
EWC [16] keeps the MAP parameters for each task. If constant scaling in the number of tasks is
required  one can make a further approximation by adding up the Kronecker factors separately. This
would be comparable to the independence assumption between the factors within the same task.

3 Related work

Our method is closely related to Bayesian online learning [31] and to Laplace propagation [4]. In
contrast to Bayesian online learning  as we cannot update the posterior over the weights in closed
form  we use gradient-based methods to ﬁnd a mode and perform a quadratic approximation around
it  resulting in a Gaussian approximation. Laplace propagation  similar to expectation propagation
[27]  maintains a factor for every task  but approximates each of them with a Gaussian. It performs
multiple updates  whereas we use each dataset only once to update the approximation to the posterior.
The most similar method to ours for overcoming catastrophic forgetting is Elastic Weight Consolida-
tion (EWC) [16]. EWC approximates the posterior after the ﬁrst task with a Gaussian. However  it
continues to add a penalty for every new task [17]. This is more closely related to Laplace propagation 
but may be overcounting early tasks [14] and does not approximate the posterior. Furthermore  EWC
uses a simple diagonal approximation to the Hessian. Lee et al. [20] approximate the posterior around
the mode for each dataset with a diagonal Gaussian in addition to a similar approximation of the
overall posterior. They update this approximation to the posterior as the Gaussian that minimizes the
KL divergence with the individual posterior approximations. Nguyen et al. [30] implement online
variational learning [7  13]  which ﬁts an approximation to the posterior through the variational
lower bound and then uses this approximation as the prior on the next task. Their Gaussian is fully
factorized  hence they do not take weight interactions into account either.
[34] and [9] have independently proposed the use of block-diagonal Kronecker factored curvature
approximations [23  2] to sample from an approximate Gaussian posterior over the weights of a
neural network. They ﬁnd that this requires adding a multiple of the identity to their curvature factors
as an ad-hoc regularizer  which is not necessary for our method. In our work  we use an approximate
posterior with the same Kronecker factored covariance structure as a prior for subsequent tasks. We
iteratively update this approximation for every new dataset. The curvature factors that we accumulate
throughout training could be used on top of our method to approximate the predictive posterior similar
to [34  9]. However  both the curvature factors and the mode that our method ﬁnds will be different to
performing a Laplace approximation in batch mode. Our work links the Kronecker factored Laplace
approximation [34] to Bayesian online learning [31] similar to how Variational Continual Learning
[30] connects Online Variational Learning [7  13] to Bayes-by-Backprop [1].
We discuss additional related methods without a Bayesian motivation in Appendix C.

3In general  the size of the ﬁrst factor is square in the dimensionality of the input to a layer and that of the

second factor square in the number of units  i.e. din × din and dout × dout for a dout × din weight matrix.

5

(a) Kronecker factored

Figure 1: Mean test accuracy on a sequence of
permuted MNIST datasets. We categorize SI
as a diagonal method  as it does not account
for parameter interactions. The dotted black
line shows the performance of a single network
trained on all observed data at each task.

(b) Diagonal

Figure 2: Effect of λ for different curvature ap-
proximations for permuted MNIST. Each plot
shows the mean  minimum and maximum across
the tasks observed so far  as well as the accuracy
on the ﬁrst and most recent task.

4 Experiments

In our experiments we compare our online Laplace approximation to the approximate Laplace
approximation of Eq. (8) as well as EWC [16] and Synaptic Intelligence (SI) [41]  both of which also
add quadratic regularizers to the objective. Further  we investigate the effect of using a block-diagonal
Kronecker factored approximation to the curvature over a diagonal one. We also run EWC with a
Kronecker factored approximation  even though the original method is based on a diagonal one. We
implement our experiments using Theano [39] and Lasagne [3] software libraries.4

4.1 Permuted MNIST

As a ﬁrst experiment  we test on a sequence of permutations of the MNIST dataset [19]. Each
instantiation consists of the 28×28 grey-scale images and labels from the original dataset with a ﬁxed
random permutation of the pixels. This makes the individual data distributions mostly independent of
each other  testing the ability of each method to fully utilize the model’s capacity.
We train a feed-forward network with two hidden layers of 100 units and ReLU nonlinearities on a
sequence of 50 versions of permuted MNIST. Every one of these datasets is equally difﬁcult for a
fully connected network due to its permutation invariance to the input. We stress that our network is
smaller than in previous works as the limited capacity of the network makes the task more challenging.
Further  we train on a longer sequence of datasets. Optimization details are in Appendix D.
Fig. 1 shows the mean test accuracy as new datasets are observed for the optimal hyperparameters
of each method. We refer to the online Laplace approximation as ‘Online Laplace’  to the Laplace
approximation around an approximate mode as ‘Approximate Laplace’ and to adding a quadratic
penalty for every set of MAP parameters as in [16] as ‘Per-task Laplace’. The per-task Laplace
method with a diagonal approximation to the Hessian corresponds to EWC.
We ﬁnd our online Laplace approximation to maintain higher test accuracy throughout training than
placing a quadratic penalty around the MAP parameters of every task  in particular when using a
simple diagonal approximation to the Hessian. However  the main difference between the methods
lies in using a Kronecker factored approximation of the curvature over a diagonal one.5 Using this
approximation  we achieve over 90% average test accuracy across 50 tasks  almost matching the
performance of a network trained jointly on all observed data. Recalculating the curvature for each
task instead of retaining previous estimates does not signiﬁcantly affect performance.

4Our fork with code to calculate the Kronecker factors is available at: www.github.com/BB-UCL/Lasagne
5In earlier work  e.g. [16  41]  diagonal approximations were reported to be effective for a smaller number of

tasks and with substantially larger networks than in our experiments.

6

Beyond simple average performance  we investigate different values of the hyperparameter λ on the
permuted MNIST sequence of datasets for our online Laplace approximation. The goal is to visualize
how it affects the trade-off between remembering previous tasks and being able to learn new ones
for the two approximations of the curvature that we consider. Fig. 2 shows various statistics of the
accuracy on the test set for the smallest and largest value of the hyperparameter on the quadratic
penalty that we tested  as well as the one that optimizes the validation error.
We are particularly interested in the performance on the ﬁrst dataset and the most recent one  as a
measure for memory and ﬂexibility respectively. For all displayed values of the hyperparameter  the
Kronecker factored approximation (Fig. 2a) has higher test accuracy than the diagonal approximation
(Fig. 2b) on both the most recent and the ﬁrst task  as well as on average. For the natural choice of
λ = 1 (leftmost subﬁgure respectively)  the network’s performance decays for the ﬁrst task for both
curvature approximations  yet it is able to learn the most recent task well. The performance on the
ﬁrst task decays more slowly  however  for the more expressive Kronecker factored approximation
of the curvature. Increasing the hyperparameter  corresponding to making the prior more narrow
as discussed in Section 2.4  leads to the network remembering the ﬁrst task much better at the cost
of not being able to achieve optimal performance on the most recently added task. Using λ = 3
(central subﬁgure)  the value that achieves optimal validation error in our experiments  the Kronecker
factored approximation leads to the network performing similarly on the most recent and ﬁrst tasks.
This coincides with optimal average test accuracy. We are not able to ﬁnd such an ideal trade-off for
the diagonal Hessian approximation  resulting in worse average performance and suggesting that the
posterior cannot be matched well without accounting for interactions between the weights. Using
a large value of λ = 100 (rightmost subﬁgure) reverts the order of performance between the most
recent and the ﬁrst task for both approximations: while for small λ the ﬁrst task is ‘forgotten’  the
network’s performance now stays at a high level — for the Kronecker factored approximation it
remembers it perfectly — which comes at the cost of being unable to learn new tasks well.
We conclude from our results that the online Laplace approximation overestimates the uncertainty in
the approximate posterior about the parameters for the permuted MNIST task  in particular with a
diagonal approximation to the Hessian. Overestimating the uncertainty leads to a need for regulariza-
tion in the form of reducing the width of the approximate posterior  as the value that optimizes the
validation error is λ = 3. Only when regularizing too strongly the approximate posterior underesti-
mates the uncertainty about the weights  leading to reduced performance on new tasks for large values
of λ. Using a better approximation to the posterior leads to a drastic increase in performance and a
reduced need for regularization in the subsequent experiments. We note that some regularization is
still necessary  suggesting that even the Kronecker factored approximation overestimates the variance
in the posterior  and a better approximation could lead to further improvements. However  it is also
possible that the Laplace approximation as such requires a large amount of data to estimate the
interaction between the parameters sufﬁciently well; hence it might be best suited for settings where
plenty of data are available.

4.2 Disjoint MNIST

We further experiment with the disjoint MNIST task  which
splits the MNIST dataset into one part containing the digits ‘0’
to ‘4’  and a second part containing ‘5’ to ‘9’ and training a
ten-way classiﬁer on each set separately. Previous work [20]
has found this problem to be challenging for EWC  as during
the ﬁrst half of training the network is encouraged to set the
bias terms for the second set of labels to highly negative values.
This setup makes it difﬁcult to balance out the biases for the
two sets of classes after the ﬁrst task without overcorrecting and
setting the biases for the ﬁrst set of classes to highly negative
values. Lee et al. [20] report just over 50% test accuracy for
EWC  which corresponds to either completely forgetting the
ﬁrst task or being unable to learn the second one  as each task
individually can be solved with around 99% accuracy.
We use an identical network architecture to the previous section
and found stronger regularization of the approximate posterior

7

Figure 3: Disjoint MNIST test ac-
curacy for the Laplace approxima-
tion (hyperparameter: λ) and SI
(hyperparameter: c).
‘Kronecker
factored’ and ‘Diagonal’ refer to
the respective curvature approxima-
tion for the Laplace method.

Figure 4: Test accuracy of a convolutional network on a sequence of vision datasets. We train on
the datasets separately in the order displayed from top to bottom and show the network’s accuracy
on each dataset once training on it has started. The dotted black line indicates the performance of
a network with the same architecture trained separately on the task. The diagonal and Kronecker
factored approximation to the Hessian both use our online Laplace method to prevent forgetting.

λ

to be necessary. For the Laplace methods  we tested values of λ ∈ {1  3  10  . . .   3×105  106}  and
c ∈ {0.1  0.3  1  . . .   3×104  105} for SI. We train using Nesterov momentum with a learning rate
of 0.1 and momentum of 0.9 and decay the learning rate by a factor of 10 every 1000 parameter
updates using a batch size of 250. We decay the initial learning rate for the second task depending on
the hyperparameter to prevent the objective from diverging. We test various decay factors for each
hyperparameter  but as a rule of thumb found λ
10 to perform well for the Kronecker factored  and
1000 for the diagonal approximation. The results are averaged across ten independent runs.
Fig. 3 shows the test accuracy for various hyperparameter values for a Kronecker factored and
a diagonal approximation of the curvature as well as SI. As there are only two datasets  the three
Laplace-based methods are identical  therefore we focus on the impact of the curvature approximation.
Approximating the Hessian with a diagonal corresponds to EWC. While we do not match the
performance of the method developed in [20]  we ﬁnd the Laplace approximation to work signiﬁcantly
better than reported by the authors. The Kronecker factored approximation gives a small improvement
over the diagonal one and requires weaker regularization  which further suggests that it better ﬁts the
true posterior. It also outperforms SI.

4.3 Vision datasets

As a ﬁnal experiment  we test our method on a suite of related vision datasets. Speciﬁcally  we train
and test on MNIST [19]  notMNIST6  Fashion MNIST [40]  SVHN [29] and CIFAR10 [18] in this
order. All ﬁve datasets contain around 50  000 training images from 10 different classes. MNIST
contains hand-written digits from ‘0’ to ‘9’  notMNIST the letters ‘A’ to ‘J’ in different computer
fonts  Fashion MNIST different categories of clothing  SVHN the digits ‘0’ to ‘9’ on street signs
and CIFAR10 ten different categories of natural images. We zero-pad the images of the MNIST-like
datasets to be of size 32×32 and replicate their intensity values over three channels  such that all
images have the same format.
We train a LeNet-like architecture [19] with two convolutional layers with 5×5 convolutions with
20 and 50 channels respectively and a fully connected hidden layer with 500 units. We use ReLU
nonlinearities and perform a 2×2 max-pooling operation after each convolutional layer with stride 2.
An extension of the Kronecker factored curvature approximations to convolutional neural networks is
presented in [10]. As the meaning of the classes in each dataset is different  we keep the weights of the
ﬁnal layer separate for each task. We optimize the networks as in the permuted MNIST experiment
and compare to ﬁve baseline networks with the same architecture trained on each task separately.
Overall  the online Laplace approximation in conjunction with a Kronecker factored approximation
of the curvature achieves the highest test accuracy across all ﬁve tasks (see Appendix E for the
numerical results). However  the difference between the three Laplace-based methods is small in

6Originally published at www.yaroslavvb.blogspot.co.uk/2011/09/notmnist-dataset.html and

downloaded from www.github.com/davidflanagan/notMNIST-to-MNIST

8

comparison to the improvement stemming from the better approximation to the Hessian. We therefore
plot the test accuracy curves through training only for the online Laplace approximation in the main
text in Fig. 4 to show the difference to SI and between the two curvature approximations. The
corresponding ﬁgures for having a separate quadratic penalty for each task and the approximate
Laplace approximation are in Appendix F.
Using a diagonal Hessian approximation for the Laplace approximation  the network mostly remem-
bers the ﬁrst three tasks  but has difﬁculties learning the ﬁfth one. SI  in contrast  shows decaying
performance on the initial tasks  but learns the ﬁfth task almost as well as our method with a Kro-
necker factored approximation of the Hessian. However  using the Kronecker factored approximation 
the network achieves good performance relative to the individual networks across all ﬁve tasks. In
particular  it remembers the easier early tasks almost perfectly while being sufﬁciently ﬂexible to
learn the more difﬁcult later tasks better than the diagonal methods  which suffer from forgetting.

5 Conclusion

We proposed the online Laplace approximation  a Bayesian online learning method for overcoming
catastrophic forgetting in neural networks. By formulating a principled approximation to the posterior 
we were able to substantially improve over EWC [16] and SI [41]  two recent methods that also add
a quadratic regularizer to the objective for new tasks. By further taking interactions between the
parameters into account  we achieved considerable increases in test accuracy on the problems that we
investigated  in particular for long sequences of datasets. Our results demonstrate the importance
of going beyond diagonal approximation methods which only measure the sensitivity of individual
parameters. Dealing with the complex interaction and correlation between parameters is necessary in
moving towards a more complete response to the challenge of continual learning.

Acknowledgements

We thank Raza Habib  Harshil Shah and the anonymous reviewers for their feedback. This work was
supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.

References
[1] C. Blundell  J. Cornebise  K. Kavukcuoglu  and D. Wierstra. Weight Uncertainty in Neural

Networks. In International Conference on Machine Learning  pages 1613–1622  2015.

[2] A. Botev  H. Ritter  and D. Barber. Practical Gauss-Newton Optimisation for Deep Learning. In

International Conference on Machine Learning  pages 557 – 565  2017.

[3] S. Dieleman  J. Schlüter  C. Raffel  E. Olson  S. K. Sønderby  D. Nouri  et al. Lasagne: First

release.  August 2015.

[4] E. Eskin  A. J. Smola  and S. Vishwanathan. Laplace Propagation. In Advances in Neural

Information Processing Systems  pages 441–448  2004.

[5] C. Fernando  D. Banarse  C. Blundell  Y. Zwols  D. Ha  A. A. Rusu  A. Pritzel  and D. Wier-
stra. Pathnet: Evolution Channels Gradient Descent in Super Neural Networks. arXiv preprint
arXiv:1701.08734  2017.

[6] R. M. French. Catastrophic Forgetting in Connectionist Networks. Trends in Cognitive Sciences 

3:128–135  1999.

[7] Z. Ghahramani. Online Variational Bayesian Learning. 2000. Slides from talk presented at NIPS

2000 workshop on Online Learning.

[8] I. J. Goodfellow  M. Mirza  D. Xiao  A. Courville  and Y. Bengio. An Empirical Investigation
of Catastrophic Forgetting in Gradient-based Neural Networks. arXiv preprint arXiv:1312.6211 
2013.

[9] E. Grant  C. Finn  S. Levine  T. Darrell  and T. Grifﬁths. Recasting Gradient-Based Meta-Learning

as Hierarchical Bayes. In International Conference on Learning Representations  2018.

9

[10] R. Grosse and J. Martens. A Kronecker-factored Approximate Fisher Matrix for Convolution

Layers. In International Conference on Machine Learning  pages 573–582  2016.

[11] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions  volume 104. CRC Press  1999.

[12] X. He and H. Jaeger. Overcoming Catastrophic Interference using Conceptor-Aided Backpropa-

gation. In International Conference on Learning Representations  2018.

[13] A. Honkela and H. Valpola. On-line Variational Bayesian Learning. In 4th International
Symposium on Independent Component Analysis and Blind Signal Separation  pages 803–808 
2003.

[14] F. Huszár. Note on the Quadratic Penalties in Elastic Weight Consolidation. Proceedings of the

National Academy of Sciences  2018.

[15] D. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. arXiv preprint

arXiv:1412.6980  2014.

[16] J. Kirkpatrick  R. Pascanu  N. Rabinowitz  J. Veness  G. Desjardins  A. A. Rusu  K. Milan 
J. Quan  T. Ramalho  A. Grabska-Barwinska  D. Hassabis  C. Clopath  D. Kumaran  and R. Hadsell.
Overcoming Catastrophic Forgetting in Neural Networks. Proceedings of the National Academy
of Sciences  pages 3521–3526  2017.

[17] J. Kirkpatrick  R. Pascanu  N. Rabinowitz  J. Veness  G. Desjardins  A. A. Rusu  K. Milan 
J. Quan  T. Ramalho  A. Grabska-Barwinska  D. Hassabis  C. Clopath  D. Kumaran  and R. Hadsell.
Reply to Huszár: The Elastic Weight Consolidation Penalty is Empirically Valid. Proceedings of
the National Academy of Sciences  2018.

[18] A. Krizhevsky and G. Hinton. Learning Multiple Layers of Features from Tiny Images. 2009.

[19] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based Learning Applied to Document

Recognition. In Proceedings of the IEEE  pages 2278 – 2324  1998.

[20] S.-W. Lee  J.-H. Kim  J. Jun  J.-W. Ha  and B.-T. Zhang. Overcoming Catastrophic Forgetting
by Incremental Moment Matching. In Advances in Neural Information Processing Systems  pages
4655–4665  2017.

[21] D. Lopez-Paz and M. Ranzato. Gradient Episodic Memory for Continual Learning. In Advances

in Neural Information Processing Systems  pages 6470–6479  2017.

[22] D. J. C. MacKay. A Practical Bayesian Framework for Backpropagation Networks. Neural

Computation  4:448–472  1992.

[23] J. Martens and R. Grosse. Optimizing Neural Networks with Kronecker-factored Approximate

Curvature. In International Conference on Machine Learning  pages 2408–2417  2015.

[24] J. Martens. New Insights and Perspectives on the Natural Gradient Method. arXiv preprint

arXiv:1412.1193  2014.

[25] P. Maybeck. Stochastic Models  Estimation and Control  chapter 12.7. Academic Press  1982.

[26] M. McCloskey and N. J. Cohen. Catastrophic Interference in Connectionist Networks: The
Sequential Learning Problem. Psychology of Learning and Motivation - Advances in Research
and Theory  24:109–165  1989.

[27] T. P. Minka. Expectation Propagation for Approximate Bayesian Inference. In Proceedings of

the Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence  pages 362–369  2001.

[28] Y. Nesterov. A Method of Solving a Convex Programming Problem with Convergence Rate O

(1/k2). Soviet Mathematics Doklady  27:372–376  1983.

[29] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading Digits in Natural
Images with Unsupervised Feature Learning. In NIPS workshop on deep learning and unsupervised
feature learning  page 5  2011.

10

[30] C. V. Nguyen  Y. Li  T. D. Bui  and R. E. Turner. Variational Continual Learning. International

Conference on Learning Representations  2018.

[31] M. Opper and O. Winther. A Bayesian Approach to On-line Learning. On-line Learning in

Neural Networks  ed. D. Saad  pages 363–378  1998.

[32] B. T. Polyak. Some Methods of Speeding up the Convergence of Iteration Methods. USSR

Computational Mathematics and Mathematical Physics  4:1–17  1964.

[33] R. Ratcliff. Connectionist Models of Recognition Memory: Constraints Imposed by Learning

and Forgetting Functions. Psychological Review  97:285–308  1990.

[34] H. Ritter  A. Botev  and D. Barber. A Scalable Laplace Approximation for Neural Networks.

International Conference on Learning Representations  2018.

[35] A. A. Rusu  N. C. Rabinowitz  G. Desjardins  H. Soyer  J. Kirkpatrick  K. Kavukcuoglu 
R. Pascanu  and R. Hadsell. Progressive Neural Networks. arXiv preprint arXiv:1606.04671 
2016.

[36] L. Sagun  U. Evci  V. U. Guney  Y. Dauphin  and L. Bottou. Empirical Analysis of the Hessian

of Over-Parametrized Neural Networks. arXiv preprint arXiv:1706.04454  2017.

[37] J. Serrà  D. Surís  M. Miron  and A. Karatzoglou. Overcoming Catastrophic Forgetting with

Hard Attention to the Task. arXiv preprint arXiv:1801.01423  2018.

[38] H. Shin  J. K. Lee  J. Kim  and J. Kim. Continual Learning with Deep Generative Replay. arXiv

preprint arXiv:1705.08690  2017.

[39] Theano Development Team. Theano: A Python Framework for Fast Computation of Mathemat-

ical Expressions. arXiv e-prints  abs/1605.02688  May 2016.

[40] H. Xiao  K. Rasul  and R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking

Machine Learning Algorithms. arXiv preprint arXiv:1708.07747  2017.

[41] F. Zenke  B. Poole  and S. Ganguli. Continual Learning through Synaptic Intelligence. In

International Conference on Machine Learning  pages 3987–3995  2017.

11

,Thang Bui
Cuong Nguyen
Richard Turner
Hippolyt Ritter
Aleksandar Botev
David Barber