2019,Convergence Guarantees for Adaptive Bayesian Quadrature Methods,Adaptive Bayesian quadrature (ABQ) is a powerful approach to numerical integration that empirically compares favorably with Monte Carlo integration on problems of medium dimensionality (where non-adaptive quadrature is not competitive).
Its key ingredient is an acquisition function that changes as a function of  previously collected values of the integrand.
While this adaptivity appears to be empirically powerful  it complicates analysis. Consequently  there are no theoretical guarantees so far for this class of methods. In this work  for a broad class of adaptive Bayesian quadrature methods  we prove consistency  deriving non-tight but informative convergence rates. To do so we introduce a new concept we call \emph{weak adaptivity}. Our results identify a large and flexible class of adaptive Bayesian quadrature rules as consistent  within which practitioners can develop empirically efficient methods.,Convergence Guarantees

for Adaptive Bayesian Quadrature Methods

Motonobu Kanagawa† #∗and Philipp Hennig#

†EURECOM  Sophia Antipolis  France

#University of Tübingen and Max Planck Institute for Intelligent Systems  Tübingen  Germany

motonobu.kanagawa@eurecom.fr & philipp.hennig@uni-tuebingen.de

Abstract

Adaptive Bayesian quadrature (ABQ) is a powerful approach to numerical integra-
tion that empirically compares favorably with Monte Carlo integration on problems
of medium dimensionality (where non-adaptive quadrature is not competitive). Its
key ingredient is an acquisition function that changes as a function of previously
collected values of the integrand. While this adaptivity appears to be empirically
powerful  it complicates analysis. Consequently  there are no theoretical guarantees
so far for this class of methods. In this work  for a broad class of adaptive Bayesian
quadrature methods  we prove consistency  deriving non-tight but informative con-
vergence rates. To do so we introduce a new concept we call weak adaptivity. Our
results identify a large and ﬂexible class of adaptive Bayesian quadrature rules as
consistent  within which practitioners can develop empirically efﬁcient methods.

1

Introduction

Numerical integration  or quadrature/cubature  is a fundamental task in many areas of science and en-
gineering. This includes machine learning and statistics  where such problems arise when computing
marginals and conditionals in probabilistic inference problems. In particular in hierarchical Bayesian
inference  quadrature is generally required for the computation of the marginal likelihood  the key
quantity for model selection  and for prediction  for which latent variables are to be marginalized out.
To describe the problem  let Ω be a compact metric space  µ be a ﬁnite positive Borel measure on
Ω (such as the Lebesgue measure on compact Ω ⊂ Rd) that playes the role of reference measure 
π : Ω → R be a known density function  and f : Ω → R be an integrand  a known function such that
the function value f (x) ∈ R can be obtained for any given query x ∈ Ω. The task of quadrature is to
numerically compute the integral (assumed to be intractable analytically)

(cid:90)

f (x)π(x)dµ(x).

a proposal distribution and the integral is approximated as(cid:80)n

This is done by evaluating the function values f (x1)  . . .   f (xn) at design points x1  . . .   xn ∈ Ω
and using them to approximate f and the integral. The points x1  . . .   xn should be “good” in the
sense that f (x1)  . . .   f (xn) provide useful information for computing the the integral.
Monte Carlo methods are the classic alternative  where x1  . . .   xn are randomly generated from
i=1 wif (xi)  with w1  . . .   wn being
importance weights. Such Monte Carlo estimators achieve the convergence rate of order n−1/2 for
n the number of design points  under a mild condition that f is a bounded function. This dimension-
independent rate  and the mild condition about f  would be one of the reasons for the wide popularity
and successes of Monte Carlo methods. However  as has been empirically known for practitioners
∗Most of this work was done when MK was afﬁliated with University of Tübingen and MPI IS  Germany

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

and also theoretically investigated recently [3  10]  practical (i.e. Markov Chain) Monte Carlo can
struggle in high dimensional integration  requiring a huge number of sample points to give a reliable
estimate:2 the curse of dimensionality appears in the constant term in front of the rate n−1/2 [22 
Sec. 2.5] [10  Thm. 2.1 and Sec. 3.4]. Thus  there has been a number of attempts on developing
methods that work better than Monte Carlo for high dimensional integration  such as Quasi Monte
Carlo methods [14].
Adaptive Bayesian quadrature (ABQ) is a recent approach from machine learning that actively 
sequentially and deterministiclaly selects design points to adapt to the target integrand [29  30  16  1 
9]. It is an extension of Bayesian quadrature (BQ) [28  15  8  21]  a probabilistic numerical method
for quadrature that makes use of prior knowledge about the integrand  such as smoothness and
structure  via a Gaussian process (GP) prior. Convergence rates of BQ methods take the form n−s/d
if the integrand f is s-times differentiable  or of the form exp(−Cn1/d) for some constant C > 0 if
f is inﬁnitely smooth [8  20]. While the rates can be faster than Monte Carlo  the dimension d of the
ambient space now appears in the rate  meaning that BQ also suffers from the curse of dimensionality.
ABQ has been developed to improve upon such vanilla BQ methods. One drawback of vanilla BQ is
that the Gaussian process model prevents the use of certain kinds of relevant knowledge about the
integrand  such as it being positive (or non-negative)  because they cannot be encoded in a Gaussian
distribution. Positive integrands are ubiquitous in machine learning and statistics  where integration
tasks emerge in the marginalization and conditioning of probability density functions  which are
positive by deﬁnition. In ABQ such prior knowledge is modelled by describing the integrand as given
by a certain transformation (or warping) of a GP — for instance  an exponentiated GP [30  29  9] or a
squared GP [16]. ABQ methods with such transformations have empirically been shown to improve
upon both standard BQ and Monte Carlo  leading to state-of-the-art wall-clock time performance on
problems of medium dimensionality.
If the transformation is nonlinear  as in the examples above  the transformed GP no longer allows an
analytic expression for its posterior process  and thus approximations are used to obtain a tractable
acquisition function. In contrast to the posterior covariance of GPs  these acquisition functions then
become dependent on previous observations  making the algorithm adaptive. This twist seems to be
critical for ABQ methods’ superior empirical performance  but it complicates analysis. Thus  there
has been no theoretical guarantee for their convergence  rendering them heuristics in practice. This is
problematic since integration is usually an intermediate computational step in a larger system  and
thus must be reliable. This paper provides the ﬁrst convergence analysis for ABQ methods.
In Sec. 2 we review ABQ methods  and formulate a generic class of acquisition functions that cover
those of [16  1  2  9]. Our convergence analysis is done for this class. We also derive an upper-bound
on the quadrature error using a transformed integrand  which is applicable to any design points and
given in terms of the GP posterior variance (Prop. 2.1). In Sec. 3  we establish a connection between
ABQ and certain weak greedy algorithms (Thm. 3.3). This is based on a new result that the scaled
GP posterior variance can be interpreted in terms of a certain projection in a Hilbert space (Lemma
3.1). Using this connection  we derive convergence rates of ABQ methods in Sec. 4. For ease of the
reader  we present a high-level overview of the proof structure in Fig. 1.
The key to our analysis is a relatively general notion for active exploration that we term weak
adaptivity. An ABQ method that satisﬁes weak adaptivity (and a few additional technical constraints)
is consistent  and the conceptual space of weakly adaptive BQ methods is large and ﬂexible. We hope
that our results spark a practical interest in the design of empirically efﬁcient acquisition functions  to
extend the reach of quadrature to problems of higher and higher dimensionality.

Related Work. For standard BQ methods  and the corresponding kernel quadrature rules  conver-
gence properties have been studied extensively [e.g. 7  19  4  40  21  11  8  27  20]. Some of these
works theoretically analyze methods that deterministically generate design points [12  5  17  7  11].
These methods are  however  not adaptive  as design points are generated independently to the
function values of the target integrand.
Our analysis is technically related to the work by Santin and Haasdonk [34]  which analyzed the
so-called P-greedy algorithm  an algorithm to sequentially obtain design points using the GP posterior

2For instance  Wenliang et al. [39  Fig. 3] used 1010 Monte Carlo samples to estimate the the normalizing

constant of their model  on problems with medium dimensionality (10 to 50 dims).

2

Figure 1: Relationships between the various auxiliary results and how they yield the main results.

variance as an acquisition function. Our results can be regarded as a generalization of their result so
that the acquisition function can include i) a scaling and a transformation of the GP posterior variance
and ii) a data-dependent term that takes care of adaptation; see (4) for details.
Adaptive methods have also been theoretically studied in the information-based complexity literature
[23  24  25  26]. The key result is that optimal points for quadrature can be obtained without observing
actual function values  if the hypothesis class of functions is symmetric and convex (e.g. the unit
ball in a Hilbert space): in this case adaptation does not help improve the performance. On the other
hand  it the hypothesis class is either asymmetric or nonconvex  then adaptation may be helpful. For
instance  a class of positive functions is assymetric because only one of f or −f can be positive.
These results thus support the choice of acquisition functions of existing ABQ methods  where the
adaptivity to function values is motivated by modeling the positivity of the integrand.
Notation. N denotes the set of positive integers  R the real line  and Rd the d-dimensional Euclidean
space for d ∈ N. Lp(Ω) for 1 ≤ p < ∞ is the Banach space of p-integrable functions  and L∞(Ω) is
that of essentially bounded functions.

2 Adaptive Bayesian Quadrature (ABQ)

We describe here ABQ methods  and present a generic form of acquisition functions that we analyze.
We also derive an upper-bound on the quadrature error using a transformed integrand in terms of the
GP posterior variance  motivating our analysis in the later sections. Throughout the paper we assume
that the domain Ω is a compact metric space and µ is a ﬁnite positive Borel measure on Ω.

2.1 Bayesian Quadrature with Transformation

ABQ methods deal with an integrand f that is a priori known to satisfy a certain constraint  for
example f (x) > 0 ∀x ∈ Ω. Such a constraint is modeled by considering a certain transformation
T : R → R  and assuming that there exists a latent function g : Ω → R such that the integrand f
is given as the transformation of g  i.e.  f (x) = T (g(x))  x ∈ Ω. Examples of T for modeling the
positivity include i) the square transformation T (y) = α + 1
2 y2  where α > 0 is a small constant such
that 0 < α < inf x∈Ω f (x)  assuming that f is bounded away from 0 [16]; and ii) the exponential
transformation T (y) = exp(y) [30  29  9]. Note that the identity map T (y) = y recovers standard
Bayesian quadrature (BQ) methods [28  15  7  21]. To model the latent function g  a Gaussian process
(GP) prior [32] is placed over g:
(1)
where m : Ω → R is a mean function and k : Ω × Ω is a covariance kernel. Both m and k should be
chosen to capture as much prior knowledge or belief about g (or its transformation f) as possible 
such as smoothness and correlation structure; see e.g. [32  Chap. 4].
Assume that a set of points Xn := {x1  . . .   xn} ⊂ Ω are given  such that the kernel matrix
i j=1 ⊂ Rn×n is invertible. Given the function values f (x1)  . . .   f (xn)  deﬁne
Kn := (k(xi  xj))n

g ∼ GP(m  k)

3

Assumption 1On the integrand  kernel and mean functions Assumption 2On the 𝑞function  the kernel and the domain Assumption 3“Weak adaptivitycondition” Assumption 4On the 𝐹functionProposition 2.1 Bound on the quadrature errorLemma 3.1Scaled posterior variance Lemma 3.2Kernel matrix invertibleTheorem 3.3Main result: ABQ as a weak greedy algorithmProposition 4.2Bound on Kolmogorov width Theorem 4.3Bound on scaled posterior variance Corollary 4.4Convergence rateInfinitely smooth kernelsfinitely smooth kernelsTheorem C.3Bound on scaled posterior variance Corollary C.4Convergence rateProposition C.2Bound on Kolmogorov width Lemma 4.1 (DeVoreet al. 13)Weak greedy algorithm and Kolmogorov widthLemma C.1Kolmogorov width and scaled posterior variancegi(x) := zi ∈ R such that T (zi) = f (xi) for i = 1  . . .   n. Treating g(x1)  . . .   g(xn) as “observed
data without noise ” the posterior distribution of g under the GP prior (1) is again given as a GP

g|(xi  g(xi))n

i=1 ∼ GP(mg Xn  kXn ) 

where mg Xn : Ω → R is the posterior mean function and kXn : Ω × Ω → R is the posterior
covariance kernel given by (see e.g. [32])

mg Xn (x)
kXn (x  x(cid:48))

:= m(x) + kn(x)(cid:62)K−1
:= k(x  x(cid:48)) − kn(x)(cid:62)K−1

(2)
(3)
where kn(x) := (k(x  x1)  . . .   k(x  xn))(cid:62) ∈ Rn  gn := (g(x1)  . . .   g(xn))(cid:62) ∈ Rn and mn =

(m(x1)  . . .   m(xn))(cid:62) ∈ Rn. Then a quadrature estimate3 for the integral(cid:82) f (x)π(x)dµ(x) is given
as the integral(cid:82) T (mg Xn (x))π(x)dµ(x) of the transformed posterior mean function T (mg Xn) 
or as the integral of the posterior expectation of the transformation(cid:82) E´gT (´g(x))π(x)dµ(x)  where
´g ∼ GP(mg Xn   kXn ) is the posterior GP. The posterior covariance for(cid:82) f (x)π(x) is given similarly;

n (gn − mn) 
n kn(x(cid:48)) 

see [9  16] for details.

a(cid:96)(x)  where a(cid:96)(x) = F(cid:0)q2(x)kX(cid:96)(x  x)(cid:1) b(cid:96)(x) 

2.2 A Generic Form of Acquisition Functions
The key remaining question is how to select good design points x1  . . .   xn ∈ Ω. ABQ methods
sequentially and deterministically generate x1  . . .   xn using an acquisition function. Many of the
acquisition functions can be formulated in the following generic form:
x(cid:96)+1 ∈ arg max
((cid:96) = 0  1  . . .   n−1) (4)
x∈Ω
where kX0 (x  x) := k(x  x)  F : [0 ∞) → [0 ∞) is an increasing function such that F (0) = 0 
q : Ω → (0 ∞) and b(cid:96) : Ω → R is a function that may change at each iteration (cid:96). e.g.  it may
depend on the function values f (x1)  . . .   f (x(cid:96)) of the target integrand f. Intuitively  b(cid:96)(x) is a
data-dependent term that makes the point selection adaptive to the target integrand  q(x) may be seen
as a proposal density in importance sampling  and F determines the balance between the uncertainty
sampling part q2(x)kX(cid:96) (x  x) and the adaptation term b(cid:96)(x). We analyse ABQ with this generic
form (4)  aiming for results with wide applicability. Here are some representative choices.
Warped Sequential Active Bayesian Integration (WSABI) [16]: Gunter et al. [16] employ the
square transformation f (x) = T (g(x)) = α + 1
2 g2(x) with two acquisition functions: i) WSABI-L
[16  Eq. 15]  which is based on linearization of T and recovered with F (y) = y  q(x) = π(x) and
(x); and ii) WSABI-M [16  Eq. 14]  the one based on moment matching given by
b(cid:96)(x) = m2
F (y) = y  q(x) = π(x) and b(cid:96)(x) = 1
Moment-Matched Log-Transformation (MMLT) [9]: Chai and Garnett [9  3rd raw in Table 1]
use the exponential transformation f (x) = T (g(x)) = exp(g(x)) with the acquisition function given
by F (y) = exp(y) − 1   q(x) = 1 and b(cid:96)(x) = exp (kX(cid:96)(x  x) + 2mg X(cid:96)(x)).
Variational Bayesian Monte Carlo (VBMC) [1  2]: Acerbi [2  Eq. 2] uses the identity f (x) =
T (g(x)) = g(x) with the acquisition function given by F (y) = yδ1  q(x) = 1 and b(cid:96)(x) =
(cid:96) (x) exp(δ3mg X(cid:96)(x))  where π(cid:96) is the variational posterior at the (cid:96)-th iteration and δ1  δ2  δ3 ≥ 0
πδ2
are constants: setting δ1 = δ2 = δ3 = 1 recovers the original acquisition function [1  Eq. 9]. Acerbi
[1  Sec. 2.1] considers an integrand f that is deﬁned as the logarithm of a joint density  while π is an
intractable posterior that is gradually approximated by the variational posteriors π(cid:96).
For the WSABI and MMLT  the acquisition function (4) is obtained by a certain approximation

for the posterior variance of the integral(cid:82) f (x)π(x)dµ(x) =(cid:82) T (g(x))π(x)dµ(x); thus this is a

2 kX(cid:96)(x  x) + m2

form of uncertainty sampling. Such an approximation is needed because the posterior variance of
the integral is not available in closed form  due to the nonlinear transformation T . The resulting
acquisition function includes the data-dependent term b(cid:96)(x)  which encourages exploration in regions
where the value of g(x) is expected to be large. This makes ABQ methods adaptive to the target
integrand. Alas  it also complicates analysis. Thus there has been no convergence guarantee for these
ABQ methods; which is what we aim to remedy in this paper.

(x).

g X(cid:96)

g X(cid:96)

3The point is that  in contrast to the integral over f  this estimate should be analytically tractable. This
2 y2 with k and π Gaussian 

depends on the choices for T   k and π. For instance  for T (y) = y or T (y) = α + 1
the estimate can be obtained analytically [16]  while for T (y) = exp(y) one needs approximations; [cf. 9].

4

2.3 Bounding the Quadrature Error with Transformation

quadrature estimate(cid:82) T (mg Xn (x))π(x)dµ(x) based on a transformation described in Sec. 2.1. It
estimator(cid:82) E´gT (´g(x))π(x)dµ(x) with ´g ∼ GP(mg Xn   kXn )  which we describe in Appendix A.2.

Our ﬁrst result  which may be of independent interest  is an upper-bound on the error for the
is applicable to any point set Xn = {x1  . . .   xn}  and the bound is given in terms of the posterior
variance kXn (x  x). This gives us a motivation to study the behavior of this quantity for x1  . . .   xn
generated by ABQ (4) in the later sections. Note that the essentially same bound holds for the other

i=1 ⊂ Ω such that (cid:107)h(cid:107)2Hk

=(cid:80)∞

Hk = span{k(·  x) | x ∈ Ω}  meaning that any h ∈ Hk can be written as h =(cid:80)∞

To state the result  we need to introduce the Reproducing Kernel Hilbert Space (RKHS) of the
covariance kernel k of the GP prior. See e.g. [35  36] for details of RKHS’s  and [6  18] for
discussions of their close but subtle relation to the GP notion. Let Hk be the RKHS associated with
the covariance kernel k of the GP prior (1)  with (cid:104)· ·(cid:105)Hk
and (cid:107) · (cid:107)Hk being its inner-product and
norm  respectively. Hk is a Hilbert space consisting of functions on Ω  such that i) k(·  x) ∈ Hk
for all x ∈ Ω  and ii) h(x) = (cid:104)k(·  x)  h(cid:105)Hk
for all h ∈ Hk and x ∈ Ω (the reproducing property) 
where k(·  x) denotes the function of the ﬁrst argument such that y → k(y  x)  with x being ﬁxed.
As a set of functions  Hk is given as the closure of the linear span of such functions k(·  x)  i.e. 
i=1 αik(·  yi) for
i=1 ⊂ R and (yi)∞
i j=1 αiαjk(yi  yj) < ∞. We are now
some (αi)∞
ready to state our assumption:
Assumption 1. T : R → R is continuously differentiable. For f : Ω → R  there exists g : Ω → R
such that f (x) = T (g(x))  x ∈ Ω and that ˜g := g − m ∈ Hk.
It holds that (cid:107)k(cid:107)L∞(Ω) :=
supx∈Ω k(x  x) < ∞ and (cid:107)m(cid:107)L∞(Ω) := supx∈Ω |m(x)| < ∞.
The assumption ˜g := g − m ∈ Hk is common in theoretical analysis of standard BQ methods  where
T (y) = y and m = 0 [see e.g. 7  40  8  and references therein]. This assumption may be weakened
by using proof techniques developed for standard BQ in the misspeciﬁd setting [19  20]  but we leave
it for a future work. The other conditions on T   k and m are weak.
Proposition 2.1. (proof in Appendix A.1) Let Ω be a compact metric space  Xn = {x1  . . .   xn} ⊂
i j=1 ∈ Rn×n is invertible  and π : Ω → [0 ∞)
Ω be such that the kernel matrix Kn = (k(xi  xj))n
Ω π(x)/q(x)dµ(x) < ∞. Suppose
that Assumption 1 is satisﬁed. Then there exists a constant C˜g m k T depending only on ˜g  m  k and
T such that

and q : Ω → [0 ∞) be continuous functions such that Cπ/q :=(cid:82)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
q(x)(cid:112)kXn(x  x).
the convergence behavior of the quantity supx∈Ω q(x)(cid:112)kXn(x  x) for points Xn = {x1  . . .   xn}
To analyze the quantity supx∈Ω q(x)(cid:112)kXn (x  x) for points Xn = {x1  . . .   xn} generated from
{(cid:80)n

ABQ (4)  we show here that the ABQ can be interpreted as a certain weak greedy algorithm studied
by DeVore et al. [13]. To describe this  let H be a (generic) Hilbert space and C ⊂ H be a compact
subset. To deﬁne some notation  let h1  . . .   hn ∈ C be given. Denote by Sn := span(h1  . . .   hn) =
i=1 αihi | α1  . . .   αn ∈ R} ⊂ H the linear subspace spanned by h1  . . .   hn. For a given h ∈ C 

(cid:12)(cid:12)(cid:12)(cid:12) ≤ C˜g m k T Cπ/q(cid:107)˜g(cid:107)Hk sup

x∈Ω

3 Connections to Weak Greedy Algorithms in Hilbert Spaces

Prop. 2.1 shows that to establish convergence guarantees for ABQ methods  it is sufﬁcient to analyze

f (x)π(x)dµ(x) −

T (mg Xn (x)) π(x)dµ(x)

generated from (4). This is what we focus on in the remainder.

(cid:90)

let dist(h  Sn) be the distance between h and Sn deﬁned by

dist(h  Sn) := inf
g∈Sn

(cid:107)h − g(cid:107)H =

α1 ... αn∈R(cid:107)h − n(cid:88)

inf

i=1

αihi(cid:107)H 

where (cid:107) · (cid:107)H denotes the norm of H. Geometrically  this is the distance between h and its orthogonal
projection onto the subspace Sn. The task considered in [13] is to select h1  . . .   hn ∈ C such that
the worst case error in C deﬁned by

dist(h  Sn)

(5)

en(C) := sup
h∈C

5

becomes as small as possible: h1  . . .   hn ∈ C are to be chosen to approximate well the set C.
The following weak greedy algorithm is considered in DeVore et al. [13]. Let γ be a constant
such that 0 < γ ≤ 1  and let n ∈ N. First select h1 ∈ C such that (cid:107)h1(cid:107)H ≥ γ suph∈C (cid:107)h(cid:107)H. For
(cid:96) = 1  . . . n−1  suppose that h1  . . .   h(cid:96) have already been generated  and let S(cid:96) = span(h1  . . .   h(cid:96)).
Then select a next element h(cid:96)+1 ∈ C such that
dist(h(cid:96)+1  S(cid:96)) ≥ γ sup
h∈C

(6)
In this paper we refer to such h1  . . .   hn as a γ-weak greedy approximation of C in H because  γ = 1
recovers the standard greedy algorithm  while γ < 1 weakens the “greediness” of this rule. DeVore
et al. [13] derived convergence rates of the worst case error (5) as n → ∞ for h1  . . .   hn generated
from this weak greedy algorithm.

((cid:96) = 1  . . .   n − 1).

dist(h  S(cid:96)) 

Weak Greedy Algorithms in the RKHS. To establish a connection to ABQ  we formulate the
weak greedy algorithm in an RKHS. Let Hk be the RKHS of the covariance kernel k as in Sec. 2.3 
and q(x) be the function in (4). We deﬁne a subset Ck q ⊂ Hk by

Ck q := {q(x)k(·  x) | x ∈ Ω} ⊂ Hk.

Note that Ck q is the image of the mapping x → q(x)k(·  x) with Ω being the domain. Therefore
Ck q is compact  if k and q are continuous and Ω is compact; this is because in this case the mapping
x → q(x)k(·  x) becomes continuous  and in general the image of a continuous mapping from a
compact domain is compact. Thus  we make the following assumption:
Assumption 2. Ω is a compact metric space  q : Ω → R is continuous with q(x) > 0 for all x ∈ Ω 
and k : Ω × Ω → R is continuous.
The following simple lemma establishes a key connection between weak greedy algorithms and
ABQ. (Note that the the result for the case q(x) = 1 is well known in the literature  and the novelty
lies in that we allow for q(x) to be non-constant.) For a geometric interpretation of (7) in terms of
projections  see Fig.2 in Appendix B.1.
Lemma 3.1. (proof in Appendix B.1) Let x1  . . .   xn ∈ Ω be such that the kernel matrix Kn =
i j=1 ∈ Rn×n is invertible. Deﬁne hx := q(x)k(·  x) for any x ∈ X   and let Sn :=
(k(xi  xj))n
span(hx1  . . .   hxn ) ⊂ Hk. Assume that q(x) > 0 holds for all x ∈ Ω. Then for all x ∈ Ω we have
(7)

q2(x)kXn (x  x) = dist2(hx  Sn) 

where kXn(x  x) is the GP posterior variance function given by (3). Moreover  we have

q(x)(cid:112)kXn (x  x) 

en(Ck q) = sup
x∈Ω

where en(Ck q) is the worst case error deﬁned by (5) with C := Ck q and Sn deﬁned here.

Lemma 3.1 (8) suggests that we can analyze the convergence properties of supx∈Ω q(x)(cid:112)kXn (x  x)

for Xn = {x1  . . .   xn} generated from the ABQ rule (4) by analyzing those of the worst case error
en(Ck q) for the corresponding elements hx1  . . .   hxn  where hxi := q(xi)k(·  xi).
Adaptive Bayesian Quadrature as a Weak Greedy Algorithm. We now show that the ABQ (4)
gives a weak greedy approximation of the compact set Ck q in the RKHS Hk in the sense of (6). We
summarize required conditions in Assumptions 3 and 4. As mentioned in Sec. 1  Assumption 3 is the
crucial one: its implications for certain speciﬁc ABQ methods will be discussed in Sec. 4.2.
Assumption 3 (Weak Adaptivity Condition). There are constants CL  CU > 0 such that CL <
b(cid:96)(x) < CU holds for all x ∈ Ω and for all (cid:96) ∈ N ∪ {0}.
Intuitively  this condition enforces ABQ to not overly focus on a speciﬁc local region in Ω and to
explore the entire domain Ω. For instance  consider the following two situations where Assumption 3
does not hod.: (a) b(cid:96)(x) → +0 as (cid:96) → ∞ for some local region x ∈ A ⊂ Ω  while b(cid:96)(x) remains
bounded from blow for x ∈ Ω\A; (b) b(cid:96)(x) → +∞ as (cid:96) → ∞ for some local region x ∈ B ⊂ Ω 
while b(cid:96)(x) remains bounded from above for x ∈ Ω\B. In case (a)  ABQ will not allocate any points
to this region A at all  after a ﬁnite number of iterations. Thus  the information about the integrand f

(8)

6

on this region A will not be obtained after a ﬁnite number of evaluations  which makes it difﬁcult to
guarantee the consistency of quadrature  unless f has a ﬁnite degree of freedom on A. Similarly  in
case (b)  ABQ will generate points only in the region B and no point in the rest of the region Ω\B 
after a ﬁnite number of iterations. Assumption 3 prevents such problematic situations to occur.
Assumption 4. F : [0 ∞) → [0 ∞) is increasing and continuous  and F (0) = 0. For any
0 < c ≤ 1  there is a constant 0 < ψ(c) ≤ 1 such that F −1 (cy) ≥ ψ(c)F −1(y) holds for all y ≥ 0.
For instance  if F (y) = yδ for δ > 0 then F −1(y) = y1/δ and thus we have ψ(c) = c1/δ
for 0 < c ≤ 1; δ = 1 is the case for the WSABI [16]  and δ > 0 for the VBMC [1  2]. If
F (y) = exp(y) − 1 as in the MMLT [9]  we have F −1(y) = log(y + 1) and it can be shown
that ψ(c) = c for 0 < c ≤ 1; see Appendix B.2. Note that in Assumption 4  the inverse F −1 is
well-deﬁned since F is increasing and continuous.
In our analysis  we allow for the point selection procedure of ABQ itself “weak ” in the sense that the
optimization problem in (4) may be solved approximately.4 That is  for a constant 0 < ˜γ ≤ 1 we
assume that the points x1  . . .   xn satisfy

a(cid:96)(x(cid:96)+1) ≥ ˜γ max
x∈Ω

a(cid:96)(x) 

((cid:96) = 0  1  . . .   n − 1) 

(9)

The case ˜γ = 1 amounts to exactly solving the global optimization problem of ABQ (4).
The following lemma guarantees we can assume without loss of generality that the kernel matrix
Kn for the points x1  . . .   xn generated from the ABQ (4) is invertible under the assumptions above 
since otherwise supx∈Ω kX(cid:96)(x  x) = 0 holds  implying that the quadrature error is 0 from Prop. 2.1.
This guarantees the applicability of Lemma 3.1 for points generated from the ABQ (4).
Lemma 3.2. (proof in Appendix B.3) Suppose that Assumptions 2  3 and 4 are satisﬁed. For a
constant 0 < ˜γ ≤ 1  assume that x1  . . .   xn are generated by a ˜γ-weak version of ABQ (4)  i.e.  (9) is
i j=1 ∈ R(cid:96)×(cid:96) is
satisﬁed. Then either one of the following holds: i) the kernel matrix K(cid:96) = (k(xi  xj))(cid:96)
invertible for all (cid:96) = 1  . . .   n; or ii) there exists some (cid:96) = 1  . . .   n such that supx∈Ω kX(cid:96)(x  x) = 0.

Lemma 3.1 leads to the following theorem  which establishes a connection between ABQ and weak
greedy algorithms.
Theorem 3.3. (proof in Appendix B.4) Suppose that Assumptions 2  3 and 4 are satisﬁed. For a
constant 0 < ˜γ ≤ 1  assume that x1  . . .   xn are generated by a ˜γ-weak version of ABQ (4)  i.e. 
(9) is satisﬁed. Let hxi = q(xi)k(·  xi) for i = 1  . . .   n. Then hx1   . . .   hxn are a γ-weak greedy

approximation of Ck q in Hk with γ =(cid:112)ψ(˜γCL/CU ).

4 Convergence Rates of Adaptive Bayesian Quadrature

We use the connection established in the previous section to derive convergence rates of ABQ. To
this end we introduce a quantity called Kolmogorov n-width  which is deﬁned (for a Hilbert space H
and a compact subset C ⊂ H) by

dn(C) := inf

Un

sup
h∈C

dist(h  Un) 

where the inﬁmum is taken over all n-dimensional subspaces Un of H. This is the worst case error
for the best possible solution using n elements in H; thus dn(C) ≤ en(C) holds for any choice of Sn
that deﬁnes the worst case error en(C) in (5). The following result by DeVore et al. [13  Corollary
3.3] relates the Kolmogorov n-width with the worst case error en(C) of a weak greedy algorithm.
Lemma 4.1. Let H be a Hilbert space and C ⊂ H be a compact subset. For 0 < γ ≤ 1  let
h1  . . .   hn ∈ C be a γ-weak greedy approximation of C in H for n ∈ N  and let en(C) be the worst
case error (5) for the subspace Sn := span(h1  . . .   hn). Then we have:

– Exponential decay: Assume that there exist constants α > 0  C0 > 0 and D0 > 0 such that
2C0γ−1 exp(−D1nα)

dn(C) ≤ C0 exp(−D0nα) holds for all n ∈ N. Then en(C) ≤ √
holds for all n ∈ N with D1 := 2−1−2αD0.

4We thank George Wynne for pointing out that our analysis can be extended to this weak version of ABQ.

7

– Polynomial decay: Assume that there exist constants α > 0 and C0 > 0 such that dn(C) ≤
C0n−α holds for all n ∈ N. Then en(C) ≤ C1n−α holds for all n ∈ N with C1 :=
25α+1γ−2C0.
e2n(C) ≤ √

2γ−1(cid:112)dn(C) holds for all n ∈ N.

– Generic case: We have en(C) ≤ √

2γ−1 min1≤(cid:96)<n (d(cid:96)(C))n−(cid:96) for all n ∈ N.

In particular 

Thus  the key is how to upper-bound the Kolmogorov n-width dn(Ck q) for the RKHS Hk associated
with the covariance kernel k. Given such an upper bound  one can then derive convergence rates for
ABQ using Thm. 3.3.
Below we demonstrate such results in the setting where Ω ⊂ Rd is compact and µ is the Lebesgue
measure  focusing on kernels with inﬁnite smoothness such as Gaussian and (inverse) multiquadric
kernels  using Lemma 4.1 for the case of exponential decay.
In a similar way (using Lemma 4.1
for the polynomial decay case) one can also derive rates for kernels with ﬁnite smoothness  such as
Matérn and Wendland kernels. These additional results are presented in Appendix C.4. We emphasize
that one can also analyze other cases (e.g. kernels on a sphere) by deriving upper-bounds on the
Kolmogorov n-width and using Thm. 3.3.

4.1 Convergence Rates for Kernels with Inﬁnite Smoothness
We consider kernels with inﬁnite smoothness  such as square-exponential kernels k(x  x(cid:48)) =
exp(−(cid:107)x − x(cid:48)(cid:107)2/γ2) with γ > 0  multiquadric kernels k(x  x(cid:48)) = (−1)(cid:100)β(cid:101)(c2 + (cid:107)x − x(cid:48)(cid:107)2)β
with β  c > 0 such that β (cid:54)∈ N  where (cid:100)β(cid:101) denotes the smallest integer greater than β  and inverse
multiquadric kernels k(x  x(cid:48)) = (c2 + (cid:107)x − x(cid:48)(cid:107)2)−β with β > 0. We have the following bound on
the Kolmogorov n-width of the Ck q for these kernels; the proof is in Appendix C.2.
Proposition 4.2. Let Ω ⊂ Rd be a cube  and suppose that Assumption 2 is satisﬁed. Let k be a
square-exponential kernel or an (inverse) multiquadric kernel. Then there exist constants C0  D0 > 0
such that dn(Ck q) ≤ C0 exp(−D0n1/d) holds for all n ∈ N.
The requirement for Ω to be a cube stems from the use of Wendland [38  Thm. 11.22] in our proof 
which requires this condition. In fact  this can be weakened to Ω being a compact set satisfying an
interior cone condition  but the resulting rate weakens to O(exp(−D1n−1/2d)) (note that this is still
exponential); see [38  Sec. 11.4]. This also applies to the following results. Combining Prop. 4.2

with Lemma 3.1  Thm. 3.3 and Lemma 4.1  we now obtain a bound on supx∈Ω q(x)(cid:112)kXn (x  x).

Theorem 4.3. (proof in Appendix C.3) Suppose that Assumptions 2  3 and 4 are satisﬁed. Let
Ω ⊂ Rd be a cube  and k be a square-exponential kernel or an (inverse) multiquadric kernel. For
a constant 0 < ˜γ ≤ 1  assume that Xn = {x1  . . .   xn} ⊂ Ω are generated by a ˜γ-weak version of
ABQ (4)  i.e.  (9) is satisﬁed. Then there exist constants C1  D1 > 0 such that

q(x)(cid:112)kXn (x  x) ≤ C1ψ(˜γCL/CU )−1/2 exp(−D1n1/d)

(n ∈ N).

sup
x∈Ω

(cid:82)

As a directly corollary of Prop. 2.1 and Thm. 4.3  we ﬁnally obtain a convergence rate of the ABQ
with an inﬁnitely smooth kernel  which is exponentially fast.
Corollary 4.4. Suppose that Assumptions 1  2  3 and 4 are satisﬁed  and that Cπ/q
:=
Ω π(x)/q(x)dµ(x) < ∞. Let Ω ⊂ Rd be a cube  and k be a square-exponential kernel or a
(inverse) multiquadric kernel. For a constant 0 < ˜γ ≤ 1  assume that Xn = {x1  . . .   xn} ⊂ Ω are
generated by a ˜γ-weak version of ABQ (4)  i.e.  (9) is satisﬁed. Then there exists a constant D1 > 0
independent of n ∈ N such that

(cid:12)(cid:12)(cid:12)(cid:12) = O(exp(−D1n1/d))

(n → ∞).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:90)

f (x)π(x)dµ(x) −

T (mg Xn (x)) π(x)dµ(x)

4.2 Discussions of the Weak Adaptivity Condition (Assumption 3)

We discuss consequences of our results to individual ABQ methods reviewed in Sec. 2.2. We do this
in particular by discussing the weak adaptivity condition (Assumption 3)  which requires that the
data-dependent term bn(x) in (4) is uniformly bounded away from zero and inﬁnity. (A discussion

8

for VBMC by Acerbi [1  2] is given in Appendix C.8. To summarize  Assumption 3 holds if the
densities of the variational distributions are bounded away uniformly from zero and inﬁnity.)
We ﬁrst consider the WSABI-L approach by Gunter et al. [16]  for which bn(x) = (mg Xn (x))2;
a similar result is presented for the WSABI-M in Appendix C.7. The following bounds for bn(x)
follow from Lemma C.5 in Appendix C.5.
Lemma 4.5. Let bn(x)
and that
inf x∈Ω |m(x)| − 2(cid:107)˜g(cid:107)Hk(cid:107)k(cid:107)1/2

Suppose that Assumption 1 is satisﬁed 
Then Assumption 3 holds for CL
:=
< ∞.

(cid:16)(cid:107)m(cid:107)L∞(Ω) + 2(cid:107)˜g(cid:107)Hk(cid:107)k(cid:107)1/2

:= (mg Xn (x))2.
inf x∈Ω |m(x)| > 2(cid:107)˜g(cid:107)Hk(cid:107)k(cid:107)1/2

> 0 and CU :=

L∞(Ω).

(cid:17)2

L∞(Ω)

(cid:16)

(cid:17)2

L∞(Ω)

Lemma 4.5 implies that WSABI-L may not be consistent when  e.g.  one uses the zero prior mean
function m(x) = 0  since in this case the condition inf x∈Ω |m(x)| > 2(cid:107)˜g(cid:107)Hk(cid:107)k(cid:107)1/2
L∞(Ω) is not
satisﬁed. Intuitively  the inconsistency may happen because the posterior mean mg Xn (x) for inputs
x in regions distant from the current design points x1  . . .   xn would become close to 0  since the
prior mean function is 0; and such regions will never be explored in the subsequent iterations 
because of the form bn(x) = (mg Xn (x))2. One simple way to guarantee the consistency is to
make a modiﬁcation like bn(x) := 1
2 (mg Xn (x))2 + α = T (mg Xn (x)); then we can guarantee
that CL ≥ α > 0  encouraging exploration in the whole region Ω. This then makes the algorithm
consistent.
We next consider
for which bn(x) =
exp (kXn(x  x) + 2mg Xn(x)). Lemma 4.6 below shows that the weak adaptivity condition holds
for the MMLT as long as Assumption 1 is satisﬁed. Therefore different from the WSABI  the MMLT
is consistent without requiring a further assumption.
Lemma 4.6. (proof in Appendix C.6) Let bn(x) := exp(kXn (x  x) + 2mg Xn (x)). Suppose
that Assumption 1 is satisﬁed. Then Assumption 3 holds for CL := exp(−2(cid:107)m(cid:107)L∞(Ω) −
4(cid:107)˜g(cid:107)Hk(cid:107)k(cid:107)1/2

L∞(Ω)) > 0 and CU := exp((cid:107)k(cid:107)L∞(Ω) + 2(cid:107)m(cid:107)L∞(Ω) + 4(cid:107)˜g(cid:107)Hk(cid:107)k(cid:107)1/2

the MMLT method by Chai and Garnett

L∞(Ω)) < 0.

[9] 

5 Conclusion and Outlook

Extending efﬁcient numerical integration beyond the low-dimensional domain remains both a
formidable challenge and a crucial desideratum for many areas. In machine learning  efﬁcient
numerical integration in the high-dimensional domain would be a game-changer for Bayesian learn-
ing. Developed by  and used in  the NeurIPS community  adaptive Bayesian quadrature is a promising
new direction for progress in this fundamental problem class. So far  it has been hindered by the
absence of theoretical guarantees.
In this work  we have provided the ﬁrst known convergence guarantees for ABQ methods  by
analyzing a generic form of their acquisition functions. Of central importance is the notion of weak
adaptivity which  speaking vaguely  ensures that the algorithm asymptotically does not “overly focus”
on some evaluations. It is conceptually related to ideas like detailed balance and ergodicity  which
play a similar role for Markov Chain Monte Carlo methods (where  speaking equally vaguely  they
guard against the same kind of locality) [cf. §6.5 & 6.6 in 33]. Like those of MCMC  our sufﬁcient
conditions for consistency span a ﬂexible class of design options  and can thus act as a guideline for
the design of novel acquisition functions for ABQ  guided by practical and intuitive considerations.
Based on the results presented herein  novel ABQ methods may be proposed for novel domains
other than only positive integrands  for example integrands with discontinuities [31] and those with
spatially inhomogeneous smoothness.
An important theoretical question  however  remains to be addressed: While our results provide
convergence guarantees for ABQ methods  they do not provide a theoretical explanation for why 
how and when ABQ methods should be fundamentally better than non-adaptive methods. In fact 
little is known about theoretical properties of adaptive quadrature methods in general. In applied
mathematics  they remain an open problem [23  24  25  26]. While we have to leave this question of
ABQ’s potential advantages over standard BQ for future research  we consider this area to be highly
promising on account of the fundamental role of high-dimensional integrals of structured functions in
probabilistic machine learning.

9

Acknowledgements

We would like to express our gratitude to the anonymous reviewers for their constructive feedback.
We also thank Alexandra Gessner  Hans Kersting  Tim Sullivan and George Wynne for their comments
and for fruitful discussions. The authors gratefully acknowledge ﬁnancial supports by the European
Research Council through ERC StG Action 757275 / PANAMA  by the DFG Cluster of Excellence
“Machine Learning – New Perspectives for Science”  EXC 2064/1  project number 390727645  by
the German Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center
(FKZ: 01IS18039A  01IS18039B)  and by the Ministry of Science  Research and Arts of the State of
Baden-Württemberg.

References
[1] L. Acerbi. Variational Bayesian Monte Carlo. In S. Bengio  H. Wallach  H. Larochelle  K. Grau-
man  N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing
Systems 31  pages 8213–8223. Curran Associates  Inc.  2018.

[2] L. Acerbi. An Exploration of Acquisition and Mean Functions in Variational Bayesian Monte
Carlo. In Francisco Ruiz  Cheng Zhang  Dawen Liang  and Thang Bui  editors  Proceedings of
The 1st Symposium on Advances in Approximate Bayesian Inference  volume 96 of Proceedings
of Machine Learning Research  pages 1–10. PMLR  02 Dec 2019.

[3] S. Agapiou  O. Papaspiliopoulos  D. Sanz-Alonso  and A. M. Stuart. Importance Sampling:

Intrinsic Dimension and Computational Cost. Statistical Science  32(3):405–431  2017.

[4] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions.

Journal of Machine Learning Research  18(19):1–38  2017.

[5] F. Bach  S. Lacoste-Julien  and G. Obozinski. On the equivalence between herding and
In Proceedings of the 29th International Conference on

conditional gradient algorithms.
Machine Learning (ICML2012)  pages 1359–1366  2012.

[6] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and

statistics. Kluwer Academic Publisher  2004.

[7] F-X. Briol  C. J. Oates  M. Girolami  and M. A. Osborne. Frank-Wolfe Bayesian quadrature:
In Advances in Neural Information

Probabilistic integration with theoretical guarantees.
Processing Systems 28  pages 1162–1170  2015.

[8] F.-X. Briol  C.J. Oates  M. Girolami  M.A. Osborne  and D. Sejdinovic. Probabilistic Integration:
A Role in Statistical Computation? (with Discussion and Rejoinder). Statistical Science  34(1):1–
22; rejoinder: 38–42  2019.

[9] H. R. Chai and R. Garnett. Improving quadrature for constrained integrands. In Kamalika
Chaudhuri and Masashi Sugiyama  editors  Proceedings of the 22nd International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS)  volume 89 of Proceedings of Machine Learning
Research  pages 2751–2759. PMLR  16–18 Apr 2019.

[10] S. Chatterjee and P. Diaconis. The sample size required in importance sampling. Annals of

Applied Probability  28(2):1099–1135  2018.

[11] W. Y. Chen  L. Mackey  J. Gorham  F. X. Briol  and C. Oates. Stein points. In J Dy and
A. Krause  editors  Proceedings of the 35th International Conference on Machine Learning 
volume 80 of Proceedings of Machine Learning Research  pages 844–853. PMLR  2018.

[12] Y. Chen  M. Welling  and A. Smola. Supersamples from kernel-herding. In Proceedings of the

26th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2010)  pages 109–116  2010.

[13] R. DeVore  G. Petrova  and P. Wojtaszczyk. Greedy algorithms for reduced bases in Banach

spaces. Constructive Approximation  37(3):455–466  2013.

[14] J. Dick  F. Y. Kuo  and I. H. Sloan. High dimensional numerical integration - the Quasi-Monte

Carlo way. Acta Numerica  22(133-288)  2013.

10

[15] Z. Ghahramani and C. E. Rasmussen. Bayesian Monte Carlo. In S. Becker  S. Thrun  and
K. Obermayer  editors  Advances in Neural Information Processing Systems 15  pages 505–512.
MIT Press  2003.

[16] T. Gunter  M. A. Osborne  R. Garnett  P. Hennig  and S. J. Roberts. Sampling for Inference in
Probabilistic Models with Fast Bayesian Quadrature. In Z. Ghahramani  M. Welling  C. Cortes 
N. D. Lawrence  and K. Q. Weinberger  editors  Advances in Neural Information Processing
Systems 27  pages 2789–2797. Curran Associates  Inc.  2014.

[17] F. Huszár and D. Duvenaud. Optimally-weighted herding is Bayesian quadrature. In Uncertainty

in Artiﬁcial Intelligence  pages 377–385  2012.

[18] M. Kanagawa  P. Hennig  D. Sejdinovic  and B. K Sriperumbudur. Gaussian processes and
kernel methods: A review on connections and equivalences. ArXiv preprint  1807.02582v1 
July 2018.

[19] M. Kanagawa  B. K. Sriperumbudur  and K. Fukumizu. Convergence guarantees for kernel-
based quadrature rules in misspeciﬁed settings. In D. D. Lee  M. Sugiyama  U. V. Luxburg 
I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages
3288–3296. Curran Associates  Inc.  2016.

[20] M. Kanagawa  B. K. Sriperumbudur  and K. Fukumizu. Convergence analysis of determin-
istic kernel-based quadrature rules in misspeciﬁed settings. Foundations of Computational
Mathematics  2019.

[21] T. Karvonen  C. J. Oates  and S. Sarkka. A Bayes-Sard cubature method.

In S. Bengio 
H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in
Neural Information Processing Systems 31  pages 5882–5893. Curran Associates  Inc.  2018.

[22] J. S. Liu. Monte Carlo Strategies in Scientiﬁc Computing. Springer-Verlag  New York  2001.

[23] E. Novak. The adaption problem for nonsymmetric convex sets. Journal of Approximation

Theory  82:123–134  1995.

[24] E. Novak. Optimal recovery and n-widths for convex classes of functions. Journal of Approxi-

mation Theory  80:390–408  1995.

[25] E. Novak. On the power of adaption. Journal of Complexity  12:199–237  1996.

[26] E. Novak. Some results on the complexity of numerical integration. In R. Cools and D. Nuyens 
editors  Monte Carlo and Quasi-Monte Carlo Methods. Springer Proceedings in Mathematics
& Statistics  volume 163  pages 161–183. Springer  Cham  2016.

[27] C. J. Oates  J. Cockayne  F-X. Briol  and M. Girolami. Convergence rates for a class of

estimators based on Stein’s method. Bernoulli  25(2):1141–1159  2019.

[28] A. O’Hagan. Bayes–Hermite quadrature. Journal of Statistical Planning and Inference  29:245–

260  1991.

[29] M. A. Osborne  D. K. Duvenaud  R. Garnett  C. E. Rasmussen  S. J. Roberts  and Z. Ghahra-
mani. Active learning of model evidence using Bayesian quadrature. In Advances in Neural
Information Processing Systems (NIPS)  pages 46–54  2012.

[30] M. A. Osborne  R. Garnett  S. J. Roberts  C. Hart  S. Aigrain  and N. Gibson. Bayesian
In International Conference on Artiﬁcial Intelligence and Statistics

quadrature for ratios.
(AISTATS)  pages 832–840  2012.

[31] L. Plaskota and G. W. Wasilkowski. The power of adaptive algorithms for functions with

singularities. Journal of Fixed Point Theory and Applications  6:227–248  2009.

[32] E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[33] C. Robert and G. Casella. Monte Carlo Statistical Methods. Springer  2004.

11

[34] G. Santin and B. Haasdonk. Convergence rate of the data-independent P-greedy algorithm in

kernel-based approximation. Dolomites Research Notes on Approximation  10:68–78  2017.

[35] B. Schölkopf and A. J. Smola. Learning with Kernels. MIT Press  2002.

[36] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.

[37] H. Wendland. Piecewise polynomial  positive deﬁnite and compactly supported radial functions

of minimal degree. Advances in Computational Mathematics  4(1):389–396  1995.

[38] H. Wendland. Scattered Data Approximation. Cambridge University Press  Cambridge  UK 

2005.

[39] L. Wenliang  D. Sutherland  H. Strathmann  and A. Gretton. Learning deep kernels for exponen-
tial family densities. In Kamalika Chaudhuri and Ruslan Salakhutdinov  editors  Proceedings of
the 36th International Conference on Machine Learning  volume 97 of Proceedings of Machine
Learning Research  pages 6737–6746  Long Beach  California  USA  09–15 Jun 2019. PMLR.

[40] X. Xi  F. X. Briol  and M. Girolami. Bayesian quadrature for multiple related integrals. In J. Dy
and A. Krause  editors  Proceedings of the 35th International Conference on Machine Learning 
volume 80 of Proceedings of Machine Learning Research  pages 5373–5382. PMLR  2018.

12

,Elad Hazan
Karan Singh
Cyril Zhang
Pablo Moreno-Muñoz
Antonio Artés
Mauricio Álvarez
Motonobu Kanagawa
Philipp Hennig