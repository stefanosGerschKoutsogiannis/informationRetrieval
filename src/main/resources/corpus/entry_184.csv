2019,A Refined Margin Distribution Analysis for Forest Representation Learning,In this paper  we formulate the forest representation learning approach called \textsc{CasDF} as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of the generalization gap from $\mathcal{O}(\sqrt{\ln m/m})$ to $\mathcal{O}(\ln m/m)$  while the margin ratio of the margin standard deviation to the margin mean is sufficiently small. This tighter upper bound inspires us to optimize the ratio. Therefore  we design a margin distribution reweighting approach for deep forest to achieve a small margin ratio by boosting the augmented feature. Experiments confirm the correlation between the margin distribution and generalization performance. We remark that this study offers a novel understanding of \textsc{CasDF} from the perspective of the margin theory and further guides the layer-by-layer forest representation learning.,A Reﬁned Margin Distribution Analysis for Forest

Representation Learning

Shen-Huan Lyu  Liang Yang  Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology

Nanjing University  Nanjing  210023  China
{lvsh yangl zhouzh}@lamda.nju.edu.cn

Abstract

In this paper  we formulate the forest representation learning approach named
casForest as an additive model  and show that the generalization error can be
bounded by O(ln m/m)  when the margin ratio related to the margin standard
deviation against the margin mean is sufﬁciently small. This inspires us to optimize
the ratio. To this end  we design a margin distribution reweighting approach for the
deep forest model to attain a small margin ratio. Experiments conﬁrm the relation
between the margin distribution and generalization performance. We remark that
this study offers a novel understanding of casForest from the perspective of the
margin theory and further guides the layer-by-layer forest representation learning.

1

Introduction

In recent years  deep neural networks have achieved excellent performance in many application
scenarios such as face recognition and automatic speech recognition [19]. It is well known that
deep neural networks are difﬁcult to be interpreted. This severely restricts the development of deep
learning in application scenarios where the interpretability of the model is crucial. Moreover  deep
neural networks are data-hungry and the performance will degrades signiﬁcantly when the size of the
training data is not big enough [12  20]. In real-world tasks  due to the high cost of data collection
and labeling  the amount of labeled training data may be insufﬁcient for deep neural networks.
In such a situation  conventional learning methods such as support-vector machines (SVMs) [7] 
random forests (RFs) [3]  gradient boosting decision trees (GBDTs) [15  5]  etc.  are still good choices.
By realizing that the essence of deep learning lies in the layer-by-layer processing  in-model feature
transformation  and sufﬁcient model complexity  recently Zhou & Feng [32  33] propose the deep
forest model and the gcForest algorithm that incorporate forest representation learning. It can achieve
excellent performance on a broad range of tasks  and even perform well on small or middle-scale
data. Later on  a more efﬁcient improvement is made by Pang et al. [21]. Feng & Zhou [13] show
that forests are able to do auto-encoder which was considered as a specialty of neural networks.
The tree-based multi-layer model can do hierarchical distributed representation learning which was
thought to be a special feature of neural networks [14]. Utkin & Ryabinin [25] propose a Siamese
deep forest as an alternative to the Siamese neural network for metric learning tasks.
The cascade forest (abbr. casForest) structure plays an important role in Deep Forest  and it is crucial
for the layer-by-layer processing. This paper attempts to explain the beneﬁts of casForest from the
perspective of the margin theory.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Our Results

In Section 2  we formulate casForest (see the structure in Figure 1) as an additive model (the additive
casForest model) to optimize the margin distribution:

F (x) =

αtht (x)  

(1)

forests block function φt is the raw feature x and the (t− 1)-th augmented feature ft−1 =(cid:80)t−1

where αt is a scalar determined by the margin distribution loss function (cid:96)md. The input of each random
l=1 αlhl:

t=1

T(cid:88)

(cid:35)(cid:33)

(cid:32)(cid:34)

t−1(cid:88)

l=1

ht(x) = φt ([x  ft−1(x)]) = φt

x 

αlhl(x)

 

(2)

[yF (x) < r] ≤ ln(cid:80)T

so that the t-layer casForest model ht ∈ Ht is deﬁned by such a recursive formula. Unlike all the
weak classiﬁers of traditional boosting are chosen from the same hypothesis set H  the hypothesis
set of the t-layer casForest model contains the (t − 1)-layer1  i.e.  Ht−1 ⊂ Ht ∀t ≥ 2.
In Section 3  we provide a margin distribution upper bound for the generalization error of the additive
model above:

t=1 αt |Ht|
r2

S

· ln m
m

PrD [yF (x) < 0] − Pr
where m is the size of training set  r is a margin parameter  λ =
S [yF (x)] is a ratio related to the
E2
margin standard deviation against the margin mean  and yF (x) denotes the margin of the sample x.
Inspired by our theoretical result  we propose an effective algorithm named margin distribution Deep
Forest (see mdDF in Algorithm 2) to encourage optimizing the margin ratio. Extensive experiments
validate that mdDF can effectively improve the performance on classiﬁcation tasks  especially for
categorical and mixed modeling tasks.

· ln m
m

(3)

 

(cid:115)
ln(cid:80)T
(cid:113) Var[yF (x)]

+ λ

t=1 αt |Ht|
r2

1.2 Related Work

Deep Forest. Deep Forest [32  33] is a non-neural network deep learning model which builds
upon decision trees and does not rely on BP algorithm and gradient-based approach. The earliest
deep forest algorithm gcForest [33]  is constructed by the multi-grained scanning operation and
the casForest structure. The multi-grained scanning operation aims to deal with the raw data with
spatial or sequential relations. The casForest structure aims at the layer-by-layer processing with
in-model feature transformation. It can be viewed as an ensemble approach that utilizes almost all
categories of well-known strategies for diversity enhancement  e.g.  input feature manipulation and
output representation manipulation [30].

Margin theory. The margin theory was used by Schapire et al. [24] to explain the resistance of
AdaBoost to overﬁtting  but then attacked almost to death by the construction of the Arc algorithm by
Breiman [2]. Later on  it was found that the empirical attack to margin theory of Adaboost might
be misleading [22]  and many theoretical studies tried to get more understanding  ended by Gao &
Zhou [16]. They ﬁnally proved that the margin distribution  which can be improved by increasing the
margin mean while decreasing the margin variance  is crucial to the performance of AdaBoost. This
has inspired the birth of a series of new statistical learning algorithms named ODM [31  28  29].

2 Cascade Forest

In Figure 1  the casForest structure is composed of stacked entities named random forests blocks
φts. Each random forests block consists of several forest modules  e.g.  commonly random forests
1The hypothesis of the random forests block in the t-th layer contains that in the (t − 1)-th layer without
updating the augmented features  i.e.  αt = 0. In other words  the in-model transformation [33] is crucial for the
recursive formulation.

2

Figure 1: The standard cascade structure of the deep forest model [33] can be viewed as a layer-
by-layer process. This feature augmentation can achieve feature enrichment by concatenating the
prediction vector with the input feature vector  which is named "PRECONC".

(abbr. RF) [3] and completely-random forests (abbr. CRF) [32]. Suppose f1 denotes the function of
the ﬁrst-layer forests  then given the input x to the ﬁrst layer  the input to the second layer will be
[x  f1(x)]  where [a  b] denotes the concatenation of a and b to form a feature vector. Considering
that the f1(x) is the prediction from the ﬁrst layer  we name this process as “PRECONC” (PREdiction
CONCatenation)  which is crucial for the feature learning process in deep forest. PRECONC is different
from the stacking operation [27  1] in traditional ensemble learning  where the second-level learners
act on the prediction space composed of different base learners  whereas the information of the
original input feature space is ignored. Using the stacking operation with more than two layers would
seriously suffer from overﬁtting  and cannot enable a deep model. In this paper we do not study the
factors which enable deep forest to become a deep model  only focus on the cascade structure.
Firstly we formulate casForest as an additive model in this section. We consider training and test
samples generated i.i.d. from distribution D over X × Y  where X ∈ Rn is the input space and
Y ∈ {1  2  . . .   s} is the output space. We denote a training set of m samples drawn from Dm by S.
The casForest model can be formalized as follows. We use a quadruple form (φ  f  D  h) where

random forests block in the t-th layer which is deﬁned by (4);
(5)  and ht drawn from the hypothesis set Ht;

• Forest block: φ = (φ1  φ2  . . .   φT )  where φt denotes the function computed by the
• casForest: h = (h1  h2  . . .   hT )  where ht denotes the t-layer casForest model deﬁned by
• Augmented feature: f = (f1  f2  . . .   fT )  where ft denotes the output in the t-th layer 
• Sample distribution: D = (D1 D2  . . .  DT )  where Dt is the updated sample distribution

which is deﬁned by (6);
in the t-th layer  and D1 = D.

φt is the function returned by the random forests block (Algorithm 1). The input of the algorithm
is the raw training sample S = {(x1  y1)  . . .   (xm  ym)}  the augmented feature from the previous
layer ft−1(xi)  i ∈ [m]  and the reweighting distribution Dt:

φt =

Arfb ([xi  ft−1(xi); yi]m

i=1 Dt)

i=1 D1)

t = 1 
t > 1.

Using these random forests block functions φts  we can deﬁne the t-layer casForest model as:

ft : X → C is deﬁned as follows:

(cid:26)Arfb ([xi; yi]m
(cid:26)φt(x)
(cid:26)αtht(x)

ht(x) =

ft(x) =

t = 1 
t > 1 

t = 1 
t > 1 

φt ([x  ft−1(x)])

αtht (x) + ft−1(x)

3

(4)

(5)

(6)

RFCRFAve.Input featuresRFCRFRFCRFRFCRFLayer 1Layer 2. . . Layer TRFCRFRFCRFf<latexit sha1_base64="TyiS+MDku4cLxAn4V+BcMEYIhX4=">AAAB6HicdZDLSgMxFIbPeK31VnXpJlgEVyUpYttdwY3LFuwF2qFk0kwbm8kMSUYopU/gxoUibn0kd76NmbaCiv4Q+Pj/c8g5J0ikMBbjD29tfWNzazu3k9/d2z84LBwdt02casZbLJax7gbUcCkUb1lhJe8mmtMokLwTTK6zvHPPtRGxurXThPsRHSkRCkats5rhoFDEJYwxIQRlQCpX2EGtVi2TKiJZ5FSElRqDwnt/GLM04soySY3pEZxYf0a1FUzyeb6fGp5QNqEj3nOoaMSNP1sMOkfnzhmiMNbuKYsW7veOGY2MmUaBq4yoHZvfWWb+lfVSG1b9mVBJarliy4/CVCIbo2xrNBSaMyunDijTws2K2Jhqyqy7Td4d4WtT9D+0yyXiuHlZrOPVOXJwCmdwAQQqUIcbaEALGHB4gCd49u68R+/Fe12WrnmrnhP4Ie/tExKOjQ4=</latexit><latexit sha1_base64="TyiS+MDku4cLxAn4V+BcMEYIhX4=">AAAB6HicdZDLSgMxFIbPeK31VnXpJlgEVyUpYttdwY3LFuwF2qFk0kwbm8kMSUYopU/gxoUibn0kd76NmbaCiv4Q+Pj/c8g5J0ikMBbjD29tfWNzazu3k9/d2z84LBwdt02casZbLJax7gbUcCkUb1lhJe8mmtMokLwTTK6zvHPPtRGxurXThPsRHSkRCkats5rhoFDEJYwxIQRlQCpX2EGtVi2TKiJZ5FSElRqDwnt/GLM04soySY3pEZxYf0a1FUzyeb6fGp5QNqEj3nOoaMSNP1sMOkfnzhmiMNbuKYsW7veOGY2MmUaBq4yoHZvfWWb+lfVSG1b9mVBJarliy4/CVCIbo2xrNBSaMyunDijTws2K2Jhqyqy7Td4d4WtT9D+0yyXiuHlZrOPVOXJwCmdwAQQqUIcbaEALGHB4gCd49u68R+/Fe12WrnmrnhP4Ie/tExKOjQ4=</latexit><latexit sha1_base64="TyiS+MDku4cLxAn4V+BcMEYIhX4=">AAAB6HicdZDLSgMxFIbPeK31VnXpJlgEVyUpYttdwY3LFuwF2qFk0kwbm8kMSUYopU/gxoUibn0kd76NmbaCiv4Q+Pj/c8g5J0ikMBbjD29tfWNzazu3k9/d2z84LBwdt02casZbLJax7gbUcCkUb1lhJe8mmtMokLwTTK6zvHPPtRGxurXThPsRHSkRCkats5rhoFDEJYwxIQRlQCpX2EGtVi2TKiJZ5FSElRqDwnt/GLM04soySY3pEZxYf0a1FUzyeb6fGp5QNqEj3nOoaMSNP1sMOkfnzhmiMNbuKYsW7veOGY2MmUaBq4yoHZvfWWb+lfVSG1b9mVBJarliy4/CVCIbo2xrNBSaMyunDijTws2K2Jhqyqy7Td4d4WtT9D+0yyXiuHlZrOPVOXJwCmdwAQQqUIcbaEALGHB4gCd49u68R+/Fe12WrnmrnhP4Ie/tExKOjQ4=</latexit><latexit sha1_base64="TyiS+MDku4cLxAn4V+BcMEYIhX4=">AAAB6HicdZDLSgMxFIbPeK31VnXpJlgEVyUpYttdwY3LFuwF2qFk0kwbm8kMSUYopU/gxoUibn0kd76NmbaCiv4Q+Pj/c8g5J0ikMBbjD29tfWNzazu3k9/d2z84LBwdt02casZbLJax7gbUcCkUb1lhJe8mmtMokLwTTK6zvHPPtRGxurXThPsRHSkRCkats5rhoFDEJYwxIQRlQCpX2EGtVi2TKiJZ5FSElRqDwnt/GLM04soySY3pEZxYf0a1FUzyeb6fGp5QNqEj3nOoaMSNP1sMOkfnzhmiMNbuKYsW7veOGY2MmUaBq4yoHZvfWWb+lfVSG1b9mVBJarliy4/CVCIbo2xrNBSaMyunDijTws2K2Jhqyqy7Td4d4WtT9D+0yyXiuHlZrOPVOXJwCmdwAQQqUIcbaEALGHB4gCd49u68R+/Fe12WrnmrnhP4Ie/tExKOjQ4=</latexit>Final prediction~concatenateh<latexit sha1_base64="s+UDyPQSSlHZ1avi1ctUAwzU7WA=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY8OKxBdMW2lA220m7drMJuxuhhP4CLx4U8epP8ua/cdvmoK0vLDy8M8POvGEquDau++2UNja3tnfKu5W9/YPDo+rxSVsnmWLos0QkqhtSjYJL9A03ArupQhqHAjvh5G5e7zyh0jyRD2aaYhDTkeQRZ9RYqzUeVGtu3V2IrINXQA0KNQfVr/4wYVmM0jBBte55bmqCnCrDmcBZpZ9pTCmb0BH2LEoaow7yxaIzcmGdIYkSZZ80ZOH+nshprPU0Dm1nTM1Yr9bm5n+1Xmai2yDnMs0MSrb8KMoEMQmZX02GXCEzYmqBMsXtroSNqaLM2GwqNgRv9eR1aF/VPcut61rDLeIowxmcwyV4cAMNuIcm+MAA4Rle4c15dF6cd+dj2VpyiplT+CPn8wfIkYza</latexit><latexit sha1_base64="s+UDyPQSSlHZ1avi1ctUAwzU7WA=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY8OKxBdMW2lA220m7drMJuxuhhP4CLx4U8epP8ua/cdvmoK0vLDy8M8POvGEquDau++2UNja3tnfKu5W9/YPDo+rxSVsnmWLos0QkqhtSjYJL9A03ArupQhqHAjvh5G5e7zyh0jyRD2aaYhDTkeQRZ9RYqzUeVGtu3V2IrINXQA0KNQfVr/4wYVmM0jBBte55bmqCnCrDmcBZpZ9pTCmb0BH2LEoaow7yxaIzcmGdIYkSZZ80ZOH+nshprPU0Dm1nTM1Yr9bm5n+1Xmai2yDnMs0MSrb8KMoEMQmZX02GXCEzYmqBMsXtroSNqaLM2GwqNgRv9eR1aF/VPcut61rDLeIowxmcwyV4cAMNuIcm+MAA4Rle4c15dF6cd+dj2VpyiplT+CPn8wfIkYza</latexit><latexit sha1_base64="s+UDyPQSSlHZ1avi1ctUAwzU7WA=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY8OKxBdMW2lA220m7drMJuxuhhP4CLx4U8epP8ua/cdvmoK0vLDy8M8POvGEquDau++2UNja3tnfKu5W9/YPDo+rxSVsnmWLos0QkqhtSjYJL9A03ArupQhqHAjvh5G5e7zyh0jyRD2aaYhDTkeQRZ9RYqzUeVGtu3V2IrINXQA0KNQfVr/4wYVmM0jBBte55bmqCnCrDmcBZpZ9pTCmb0BH2LEoaow7yxaIzcmGdIYkSZZ80ZOH+nshprPU0Dm1nTM1Yr9bm5n+1Xmai2yDnMs0MSrb8KMoEMQmZX02GXCEzYmqBMsXtroSNqaLM2GwqNgRv9eR1aF/VPcut61rDLeIowxmcwyV4cAMNuIcm+MAA4Rle4c15dF6cd+dj2VpyiplT+CPn8wfIkYza</latexit><latexit sha1_base64="s+UDyPQSSlHZ1avi1ctUAwzU7WA=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY8OKxBdMW2lA220m7drMJuxuhhP4CLx4U8epP8ua/cdvmoK0vLDy8M8POvGEquDau++2UNja3tnfKu5W9/YPDo+rxSVsnmWLos0QkqhtSjYJL9A03ArupQhqHAjvh5G5e7zyh0jyRD2aaYhDTkeQRZ9RYqzUeVGtu3V2IrINXQA0KNQfVr/4wYVmM0jBBte55bmqCnCrDmcBZpZ9pTCmb0BH2LEoaow7yxaIzcmGdIYkSZZ80ZOH+nshprPU0Dm1nTM1Yr9bm5n+1Xmai2yDnMs0MSrb8KMoEMQmZX02GXCEzYmqBMsXtroSNqaLM2GwqNgRv9eR1aF/VPcut61rDLeIowxmcwyV4cAMNuIcm+MAA4Rle4c15dF6cd+dj2VpyiplT+CPn8wfIkYza</latexit><latexit sha1_base64="0ER/x9kyTro+JqZMNUFNahpVLaI=">AAAB63icbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXRZcOOygr1AO5RMmumEJpkhyQhl6Cu4caGIW1/InW9jpp2Ftv4Q+PjPOeScP0wFN9bzvlFlY3Nre6e6W9vbPzg8qh+fdE2Saco6NBGJ7ofEMMEV61huBeunmhEZCtYLp3dFvffEtOGJerSzlAWSTBSPOCW2sIZpzEf1htf0FsLr4JfQgFLtUf1rOE5oJpmyVBBjBr6X2iAn2nIq2Lw2zAxLCZ2SCRs4VEQyE+SLXef4wjljHCXaPWXxwv09kRNpzEyGrlMSG5vVWmH+VxtkNroNcq7SzDJFlx9FmcA2wcXheMw1o1bMHBCqudsV05hoQq2Lp+ZC8FdPXofuVdN3/HDdaHllHFU4g3O4BB9uoAX30IYOUIjhGV7hDUn0gt7Rx7K1gsqZU/gj9PkDDiCOLQ==</latexit><latexit sha1_base64="0ER/x9kyTro+JqZMNUFNahpVLaI=">AAAB63icbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXRZcOOygr1AO5RMmumEJpkhyQhl6Cu4caGIW1/InW9jpp2Ftv4Q+PjPOeScP0wFN9bzvlFlY3Nre6e6W9vbPzg8qh+fdE2Saco6NBGJ7ofEMMEV61huBeunmhEZCtYLp3dFvffEtOGJerSzlAWSTBSPOCW2sIZpzEf1htf0FsLr4JfQgFLtUf1rOE5oJpmyVBBjBr6X2iAn2nIq2Lw2zAxLCZ2SCRs4VEQyE+SLXef4wjljHCXaPWXxwv09kRNpzEyGrlMSG5vVWmH+VxtkNroNcq7SzDJFlx9FmcA2wcXheMw1o1bMHBCqudsV05hoQq2Lp+ZC8FdPXofuVdN3/HDdaHllHFU4g3O4BB9uoAX30IYOUIjhGV7hDUn0gt7Rx7K1gsqZU/gj9PkDDiCOLQ==</latexit><latexit sha1_base64="0ER/x9kyTro+JqZMNUFNahpVLaI=">AAAB63icbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXRZcOOygr1AO5RMmumEJpkhyQhl6Cu4caGIW1/InW9jpp2Ftv4Q+PjPOeScP0wFN9bzvlFlY3Nre6e6W9vbPzg8qh+fdE2Saco6NBGJ7ofEMMEV61huBeunmhEZCtYLp3dFvffEtOGJerSzlAWSTBSPOCW2sIZpzEf1htf0FsLr4JfQgFLtUf1rOE5oJpmyVBBjBr6X2iAn2nIq2Lw2zAxLCZ2SCRs4VEQyE+SLXef4wjljHCXaPWXxwv09kRNpzEyGrlMSG5vVWmH+VxtkNroNcq7SzDJFlx9FmcA2wcXheMw1o1bMHBCqudsV05hoQq2Lp+ZC8FdPXofuVdN3/HDdaHllHFU4g3O4BB9uoAX30IYOUIjhGV7hDUn0gt7Rx7K1gsqZU/gj9PkDDiCOLQ==</latexit><latexit sha1_base64="0ER/x9kyTro+JqZMNUFNahpVLaI=">AAAB63icbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXRZcOOygr1AO5RMmumEJpkhyQhl6Cu4caGIW1/InW9jpp2Ftv4Q+PjPOeScP0wFN9bzvlFlY3Nre6e6W9vbPzg8qh+fdE2Saco6NBGJ7ofEMMEV61huBeunmhEZCtYLp3dFvffEtOGJerSzlAWSTBSPOCW2sIZpzEf1htf0FsLr4JfQgFLtUf1rOE5oJpmyVBBjBr6X2iAn2nIq2Lw2zAxLCZ2SCRs4VEQyE+SLXef4wjljHCXaPWXxwv09kRNpzEyGrlMSG5vVWmH+VxtkNroNcq7SzDJFlx9FmcA2wcXheMw1o1bMHBCqudsV05hoQq2Lp+ZC8FdPXofuVdN3/HDdaHllHFU4g3O4BB9uoAX30IYOUIjhGV7hDUn0gt7Rx7K1gsqZU/gj9PkDDiCOLQ==</latexit>PRECONCAugmented featuresOriginal featuresAlgorithm 1 Random forests block Arfb [33]
Input: A training set S drawn from Dt and the augmented feature ft−1(xi) ∀i ∈ [m].
Output: The function computed by the random forests block in the t-th layer: φt.
1: Divide S to k-fold subsets {S1  . . .   Sk} randomly.
2: for Si in {S1  S2  . . .   Sk} do
Using S/Si to train two random forests and two completely random forests.
3:
Compute the prediction rate pi
t(j) for the j-th leaf node generated by S/Si.
4:
φt([x  ft−1(x)]) ← Ej[pi
5:
6: end for
7: φt([x  ft−1(x)]) ← Ei j[pi
8: return The function computed by the random forests block in the t-th layer: φt.

t(j)]  for any training sample (x  y) ∈ Si.
t(j)]  for any test sample (x  y) ∈ D.

where αt and Dt need to be optimized and updated.
Here  we ﬁnd that the t-layer casForest model is deﬁned by a recursive formula:

(cid:32)(cid:34)

t−1(cid:88)

(cid:35)(cid:33)

x 

αlhl(x)

ht(x) = φt ([x  ft−1(x)]) = φt

(7)
Unlike all the weak classiﬁers of AdaBoost which are chosen from the same hypothesis set H 
the hypothesis set of the t-layer casForest model contains that of the (t − 1)-layer  similar to the
hypothesis sets of the deep neural networks (DNNs) at different depths  i.e.  Ht−1 ⊂ Ht ∀t ≥ 2.
The PRECONC process is difﬁcult to analyze. For simplicity  here we do not consider the inﬂuence
of the feature augmentation process though it is very crucial for deep forest. Instead  we only
consider the hypotheses based on the original feature space  and thus the entire additive cascade
model ˜F : X → Y is deﬁned as follows:

l=1

.

(cid:35)

(cid:34) T(cid:88)

t=1

˜F (x) = ˜σ(F (x)) = arg max
j∈{1 2 ... s}

αthj

t (x)

 

(8)

where F (x) is the ﬁnal prediction vector of the casForest model for classiﬁcation and ˜σ denotes a
map from average prediction score vector to a label.
With such a simplicity  the casForest structure has relation to Cortes et al. [8  9] and Huang et al.
[17]. However  in the next section we will see that we prove that the generalization error of casForest
can be bounded by O(ln m/m + λ
deviation against the margin mean is sufﬁcient small. This bound is tighter than the generalization
bound O(ln m/m) for Deep Boosting [8  9  17].

(cid:112)ln m/m)  when the margin ratio related to the margin standard

3 Generalization Analysis

classiﬁer (the T -layer casForest model) as F (x) =(cid:80)T

In this section  we analyze the generalization error to understand the sample complexity of the
casForest model. For simplicity  we consider the binary classiﬁcation2 task. We deﬁne the strong
t=1 αtht(x)  i.e.  casForest is formulated as an
additive model. Now we deﬁne the margin for sample (x  y) as yF (x) ∈ [−1  1]  which implies the
conﬁdence of prediction. We assume that the hypothesis set H of base classiﬁers {h1  h2  . . .   hT}
can be decomposed as the union of T families H1 H2  . . .  HT ordered by increasing complexity 
where ∀t ≥ 1 Ht ⊂ Ht+1 and ht ∈ Ht. Remarkably  the complexity term of our bound admits an
explicit dependency in terms of the mixture coefﬁcients deﬁning the ensembles. Thus  the ensemble
t=1 Ht)  which is the family of functions F (x) of the form

family we consider is F = conv((cid:83)T
F (x) =(cid:80)T

t=1 αtht(x)  where α = (α1  . . .   αT ) is in the simplex ∆.

For a ﬁxed g = (g1  . . .   gT )  any α ∈ ∆ deﬁnes a distribution over {g1  . . .   gT}. Sampling
from {g1  . . .   gT} according to α and averaging leads to functions G = 1
i=1 ntgt for some
2In the binary classiﬁcation  we can redeﬁne the output of the strong classiﬁer F (x) as a variable in [−1  1] 
e.g. the difference between two prediction scores  where ˜F (x) = sign(F (x)) is the predicted label. The
previous bounds [8  9  17] are based on binary classiﬁcation  therefore  our result is comparable with them.

(cid:80)T

n

4

n

gk j

T(cid:88)

GF  N =

consider the family of functions

t=1 nt = n  and gt ∈ Ht. For any N = (N1  . . .   NT ) with |N| = n  we

n = (n1  . . .   nT )  with(cid:80)T
 1
Nk(cid:88)
and the union of all such families GF  n =(cid:83)|N=n| GF  N. For a ﬁxed N  the size of GF  N can be
bounded as follows: ln|GF  N| ≤ n ln(cid:80)T
Lemma 1. ([16]) For F =(cid:80)T

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∀(k  j) ∈ [T ] × [Nk]  gk j ∈ Hk

bound based on the margin mean and a Bernstein-type bound follows:
t=1 αtht ∈ F and G ∈ GF  n  we have
−n2

t=1 αt|Ht|. Our margin distribution theory is based on a

(10)
Lemma 2. ([16]) For independent random variables X1  X2  . . .   Xm(m ≥ 5) with values in [0  1] 
and for δ ∈ (0  1)  with probability at least 1 − δ we have

S[yF (x)] + 4/3

[yG(x) − yF (x) ≥ ] ≤ exp

  

2 − 2E2

S GF  n

(cid:18)

(cid:19)

(9)

Pr

k=1

j=1

.

m(cid:88)

m(cid:88)
i(cid:54)=j(Xi − Xj)2/2m(m − 1)

E[Xi] − 1
m

1
m

i=1

i=1

where ˆVm =(cid:80)

(cid:115)

Xi ≤

2 ˆVm ln(2/δ)

m

+

7 ln(2/δ)

3m

(11)

Since the gap between the margin of strong classiﬁer yF (x) and that in the union family GF  N
is bounded by a function related to the margin mean ES[yF (x)]  we can further obtain a margin
distribution theorem as follows:
Theorem 1. Let D be a distribution over X × Y and S be a training set of m samples drawn from
D. With probability at least 1 − δ  for r > 0  the strong classiﬁer F (x) (the T -layer casForest model)
satisﬁes that

PrD [yF (x) < 0] ≤ inf
r∈(0 1]

[yF (x) < r] +

Pr
S

1
md +

+

7µ
3m

+ λ

√
3
µ
m3/2

(cid:34)

> 2  µ = ln m ln(2

T(cid:88)

t=1

αt|Ht|)/r2 + ln

2
δ

  λ =

Var[yF (x)]
E2
S[yF (x)]

.

(cid:35)

(cid:114) 3µ
(cid:115)

m

where

S[yF (x)] + r/9

2

d =

1 − E2

Proof. For F = (cid:80)T

Chernoff’s bound gives

t=1 αtht ∈ F and G ∈ GF  n  we have EG∈GF  n[G] = F . For β > 0  the

Pr
D

[yF (x) < 0] = Pr

D GF  n

[yF (x) < 0  yG(x) ≥ β] + Pr
D GF  n

[yF (x) < 0  yG(x) < β]

Recall that |GF  N| ≤(cid:81)T

≤ exp(−nβ2/2) + Pr
(12)
D GF  n
t=1 |Ht|Nt for a ﬁxed N. Therefore  for any δn > 0  combining the union
bound with the Lemma 2 guarantees that with probability at least 1 − δn over sample S  for any
G ∈ GF  N and β > 0

[yG(x) < β].

[yG(x) < β] ≤ Pr

S

Pr
D

[yG(x) < β] +

≤ Pr

S

[yG(x) < β] +

(cid:32)

2
δ

|Ht|Nt

(cid:33)
T(cid:89)
(cid:18) 2|Ht|
(cid:19)

t=1

αt ln

δ

(cid:32)

|Ht|Nt

(cid:33)
T(cid:89)
(cid:18) 2|Ht|
(cid:19)

t=1

αt ln

δ

+

7
3m

ln

2
δ

T(cid:88)

i=1

+

7n
3m

(13)

(14)

(cid:118)(cid:117)(cid:117)(cid:116) 2
(cid:118)(cid:117)(cid:117)(cid:116) 2n

m

m

ˆVm ln

ˆVm

T(cid:88)

i=1

5

(cid:118)(cid:117)(cid:117)(cid:116) 2n

m

(cid:32)

2(cid:80)T

i=1 αt|Ht|

(cid:33)

δ

ˆVm ln

(cid:32)

2(cid:80)T

i=1 αt|Ht|

(cid:33)

δ

+

7n
3m

ln

≤ Pr

S

[yG(x) < β] +

where

(cid:88)

i(cid:54)=j

ˆVm =

(I[yiG(xi) < β] − I[yjG(xj) < β])2

2m(m − 1)

 

(15)

(16)

The inequality (14) is a large probability bound when n is large enough and inequality (15) is
according to the Jensen’s Inequality. Since there are T at most T n possible T -tuples N with |N| = n 
by the union bound  for any δ > 0  with probability at least 1 − δ  for all G ∈ GF  n and β > 0:
i=1 αt|Ht|
δ/T n

[yG(x) < β] ≤ Pr

i=1 αt|Ht|
δ/T n

(cid:118)(cid:117)(cid:117)(cid:116) 2n

2(cid:80)T

[yG(x) < β] +

(cid:32)

2(cid:80)T

ˆVm ln

+

7n
3m

ln

(cid:32)

(cid:33)

(cid:33)

Pr
D

m

S

Meantime  we can rewrite ˆVm

ˆVm =

=

=

(cid:88)

(I[yiG(xi) < β] − I[yjG(xj) < β])2

2m(m − 1)

i(cid:54)=j
2m2 PrS[yG(x) < β] PrS[yG(x) ≥ β]

2m(m − 1)

For any θ1  θ2 > 0  we utilize Chernoff’s bound to get:

m
m − 1

∗
ˆV
m

[yG(x) ≥ β]

∗
ˆV
m = Pr
S

[yG(x) < β] Pr
S

≤ 3 exp(−nθ2
≤ 3 exp(−nθ2

1/2) + Pr
S
1/2) + Pr
S

[yF (x) < β + θ1] Pr
S
[yF (x) < β + θ1 |ES[yF (x)] ≥ β + θ1 + θ2 ]

[yF (x) ≥ β − θ1]

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(cid:19)

(25)

· Pr

[yF (x) ≥ β − θ1|ES[yF (x)] ≥ β + θ1 + θ2]

S

≤ 3 exp(−nθ2

1/2) +

≤ 3 exp(−nθ2

1/2) +

Var[yF (x)]

θ2
2
Var[yF (x)]

According to Chebyshev’s Inequality

(ES[yF (x)] − β + θ1)2 (cid:39) 3 exp(−nθ2

1/2) +

Var[yF (x)]
E2
S[yF (x)]

(24)

S[yF (x)] is the variance of the margins.

where Var[yF (x)] = ES[(yF (x))2] − E2
From Lemma 1  we obtain that
[yG(x) < β] ≤ Pr

Pr
S

(cid:18)

[yF (x) < β + θ1] + exp

S

−nθ2

1

2 − 2E2

S[yF (x)] + 4θ1/3

Let θ1 = r/6  β = 5r/6 and n = ln m/r2  we combine (12)(15)(24)(25)  the proof is completed.

(cid:112)ln m/m + ln m/m)  which is controlled by the
the sample complexity. Then  this bound is tighter than the O((cid:112)ln m/m) rate as demonstrated in

Remark 1. From Theorem 1  we know that the gap between the generalization error and the empirical
margin loss is generally bounded by the term O(λ
ratio related to the margin standard deviation against the margin mean λ. This ratio implies that the
larger margin mean and the smaller margin variance can reduce the generalization error of models
properly  which is crucial to alleviating the overﬁtting problem. When the margin distribution is
good enough (the margin mean is large and the margin variance is small)  O(ln m/m) will dominate

previous theoretical works about Deep Boosting [8  9  17].

6

m  ∀i ∈ [m]

Algorithm 2 mdDF (margin distribution Deep Forest)
Input: Training set S = {(x1  y1)  . . .   (xm  ym)} and random forests block algorithm Arfb.
Output: The ﬁnal additive cascade model ˜F .
1: Initialize α0 ← 1  f0 ← ∅
2: Initialize sample weights: D1(i) ← 1
3: for t = 1  2  . . .   T do
4:
5:
6:
7:

φt ← the random forests block returned by Arfb([xi  ft−1(xi); yi]m
ht(xi) ← φt ([xi  ft−1(xi)])  ∀i ∈ [m].
ES[(cid:96)md((cid:80)t
γt(xi) ← hy
αt ← arg min
9: Dt+1(i) ← (cid:96)md((cid:80)t
ft(xi) ← αtht (xi) + ft−1(xi) ∀i ∈ [m].
(cid:80)m
i=1 (cid:96)md((cid:80)t
(cid:104)(cid:80)T
10: end for
11: return ˜F ← arg max
j∈{1 2 ... s}

t (xi) − maxj(cid:54)=y hj

t (xi) ∀i ∈ [m].

 ∀i ∈ [m].

i=1 Dt).

l=1 αlγl(x))]

t=1 αthj

l=1 αlγl(xi))

l=1 αlγl(xi))

(cid:105)

8:

αt

.

t

the result of Cortes et al. [8]. The cardinality of the hypothesis set F = conv((cid:83)T
by the mixture coefﬁcients αts in (1).(cid:80)T
the expected margin distribution loss ES[(cid:96)md((cid:80)t

Remark 2. As for the overﬁtting risk of the model (due to the large complexity)  our bound inherits
t=1 Ht) is controlled
t=1 αt|Ht| in our bound implies that it is not detrimental to
generalization if the corresponding mixture weight is relatively small  while some hypothesis sets
used for learning could have large complexity. In other words  the coefﬁcients αts need to minimize
l=1 αlγl(x))]  which estimates the generalization

error of the additive casForest model.

4 Optimization

The generalization analysis shows the importance of optimizing the margin ratio λ and the mixture
coefﬁcients αts. Since we formulate casForest as an additive model  we utilize the reweighting
approach to minimize the expected margin distribution loss

(cid:34)

ES

(cid:96)md

(cid:33)(cid:35)

(cid:32) t(cid:88)

l=1

αlγl(x)

 

(26)

t (·) − maxj(cid:54)=y hj

t (·)(cid:3) ∈ C. According to

where the margin distribution loss function (cid:96)md is designed to utilize the ﬁrst- and second-order
statistics of margins  and γl(x) denotes the margin in the l-th layer. The scalar αt is determined by
minimizing the expected loss for the t-layer model.
The mdDF algorithm (Algorithm 2). We denote a prediction score space by C = Rs  where
s is the number of classes. When each sample passes through the forest model  we will get an
Crammer & Singer [10]  we can deﬁne the margin of sample γt(·) for multi-class classiﬁcation as:
γt(·) := hy
The initial sample weights are [1/m  1/m  . . .   1/m]  and we update the i-th weight by

t (·)  h2
t (·) . . .   hs
t (·)  i.e.  the conﬁdence of prediction.
(cid:17)

average prediction vector in each layer: ht(·) = (cid:2)h1
(cid:16)(cid:80)t
(cid:16)(cid:80)t
(cid:80)m
(cid:40) (z−γ)2

where the margin distribution loss function (cid:96)md(·) is deﬁned by Zhang & Zhou [28] to optimize the
ﬁrst- and second-order statistics of margins as follows:

Dt+1(i) =

l=1 αlγl(xi)

l=1 αlγl(xi)

(cid:17)  

i=1 (cid:96)md

(27)

(cid:96)md

z ≤ γ 
z > γ 

(28)

(cid:96)md(z) =

γ2
µ(z−γ)2
(1−γ)2

7

where hyper-parameter γ is a parameter as the margin mean and µ is a parameter to trade off two
different kinds of deviation (keeping the balance on both sides of the margin mean). Obviously  this
margin distribution loss function will enforce the band that has a lower loss to contain the sample
points as many as possible. In practice  we generally choose these two hyper-parameters from the
ﬁnite sets γ ∈ {0.7  0.75  0.8  0.85  0.9  0.95} and µ ∈ {0.01  0.05  0.1}. The algorithm utilizing the
margin distribution optimization is summarized in Algorithm 2.

5 Experiments

Datasets and conﬁguration. We choose eight classiﬁcation benchmark datasets with different
scales. The datasets vary in size: from 1484 up to 78823 instances  from 8 up to 784 features  and
from 2 up to 26 classes. From the literature  these datasets come pre-divided into training and testing
sets. Therefore in our experiments  we use them in their original format. PROTEIN  SENSIT  and
SATIMAGE datasets are obtained from LIBSVM datasets [4]. Except for MNIST [18] dataset  others
come from the UCI Machine Learning Repository [11]. Based on the attribute characteristics of the
dataset  we classify the datasets into three categories: categorical  numerical  and mixed modeling
tasks. We conjecture that some numerical modeling tasks such as image or audio recognition are
very suitable for DNNs. Some operations  such as convolution  exactly ﬁt well with numerical signal
modeling. The deep forest model is not developed to replace DNNs for such tasks; instead  it offers
an alternative when DNNs are not superior  e.g.  deep forests are good at the categorical/symbolic or
mixed modeling tasks especially [33].
In mdDF  we take two random forests and two completely-random forests in each layer  and each
forest contains 100 trees  whose maximum depth of trees in random forests grows with the layer  i.e. 
max ∈ {2t + 2  4t + 4  8t + 8  16t + 16}. To reduce the risk of overﬁtting  the representation learned
d(t)
by each forest is generated by k-fold cross-validation (k = 5 in our experiments). In Algorithm 1 
each instance will be used as training data for (k − 1) times  and produce the ﬁnal class vector as
augmented features for the resulting in (k − 1) class vectors  that are averaged to the next layer.
We compare mdDF with the other four common used algorithms on different datasets: multilayer
perceptron (MLP)  random forest (RF) [3]  XGBoost [5] and gcForest [32]. Here  we set the same
number of forests as mdDF in each layer of gcForest. For random forests  we set 400 × k trees; and
for XGBoost  we also take 400 × k trees. As for other hyper-parameters  we set them as the default
values. For the multilayer perceptron (MLP) conﬁgurations  we use ReLU for the activation function 
cross-entropy for the loss function  adadelta for optimization  no dropout for hidden layers according
to the scale of training data. The network structure hyper-parameters  however  could not be ﬁxed
across tasks. Therefore  for MLP  we examine a variety of architectures on the validation set  and
pick the one with the best performance  then train the whole network again on the training set and
report the test accuracy. The examined architectures are listed as follows: (1) input-1024-512-output;
(2) input-16-8-8-output; (3) input-70-50-output; (4) input-50-30-output; (5) input-30-20-output.

Test accuracy on benchmark datasets. Table 1 shows that mdDF achieves better accuracy than
the other methods on several datasets. Compared with the MLP method  the deep forest models

Table 1: Left: Comparison results between mdDF and the other tree-based algorithms on test accuracy
with different datasets. The best accuracy on each dataset is highlighted in bold type. • indicates the
second best accuracy on each dataset. The average rank is listed at the bottom. Right: Comparison
results between the standard mdDF structure and the other mdDF structures.

Dataset
ADULT Categorical
YEAST Categorical
LETTER Categorical
PROTEIN Categorical
Mixed
Mixed
Numerical
Numerical
-

Attribute MLP
80.597
59.641
96.025
68.660
94.231 •
78.957
91.125
98.621 •
3.650

HAR
SENSIT
SATIMAGE
MNIST
Avg. Rank

RF
85.818
61.886
96.575
68.071
92.569
80.133
91.200
96.831
4.000

XGBoost
85.904
59.161
95.850
71.214 •
93.112
81.874
90.450
97.730
3.750

8

gcForest mdDF mdDFSF mdDFST mdDFNP
86.276 •
63.004 •
97.375 •
71.009
94.224
82.334 •
91.700 •
98.252
2.375

86.560
63.340
97.500
71.247
94.600
82.534
91.750
98.734
1.000

85.650
62.556
96.975
68.509
94.060
80.320
90.800
98.240
-

86.200
63.000
96.475
71.127
93.926
82.014
91.600
98.254
-

85.710
62.780
97.300
70.291
94.290
80.412
91.300
98.101
-

(a) The accuracy (solid line) and the margin
ratio (dotted line) of the mdDF algorithm at
different layers on the HAR dataset.

(b) The multi-layer feature visualization of the mdDF al-
gorithm on HAR training set. The ratios of the intra-class
variance to the inter-class variance SA/SE are (3.88  1.97).

Figure 2: The relation between the margin ratio and learning ability in the different layers.

almost outperform on these datasets and obtain the top 2 test accuracy on categorical or mixed
modeling tasks. Obviously  gcForest and mdDF perform better than the shallow ones  and mdDF
with reweighting and boosting representations outperforms gcForest across these datasets. The
empirical results show that the deep models provide an improvement in performance with in-model
transformation  compared to the shallow models that only have invariant features.

Comparison with the other mdDF structures
In Table 1  we compare our mdDF structure with
the three other mdDF structures on different datasets: (1) mdDF using same forests (use 4 random
forests) named mdDFSF; (2) mdDF using stacking (only transmit the prediction vectors to next layer)
named mdDFST; (3) mdDF without PRECONC (only transmit the input feature vector to next layer)
named mdDFNP. In this way  we explore the importance of internal structures of the mdDF. When
we remove a concrete structure and control other variables  the performance of the mdDF algorithm
will be worse. The empirical results demonstrate the effectiveness of these speciﬁc structures.

Relation between the margin ratio and learning ability. Figure 2(a) plots the accuracy and
margin ratio of mdDF on the HAR dataset. It demonstrates clearly that the performance is consistent
with the margin ratio. When the margin ratio is smaller  i.e.  the margin std/mean is smaller  the
performance is better. Figure 2(b) plots the t-SNE visualization of mdDF on the HAR dataset. We also
use the variance decomposition in the 2D space. The result shows that the intra-class compactness and
inter-class separability are getting better as the layers becomes deeper. Such a correlation validates
the theoretical result of our reﬁned margin distribution analysis.

6 Conclusion

Recent studies propose a few tree-based deep models to learn the representations from a broad range
of tasks and achieve good performance. By formulating casForest as an additive model  we partially
explain the success of it from the perspective of the margin theory. The theoretical results inspire us
to design a margin distribution reweighting approach that improves the generalization performance.
Then  the empirical studies validate our theoretical results. We will explore how to understand the
effectiveness of the PRECONC operation (which is crucial for feature enrichment) in future work.

Acknowledgments

This research was supported by the NSFC (61751306)  National Key R&D Program of China
(2018YFB1004300)  and the Collaborative Innovation Center of Novel Software Technology and
Industrialization. The authors would like to thank the anonymous reviewers for constructive sugges-
tions  as well as Wei Gao  Lijun Zhang  Shengjun Huang  Xizhu Wu  Lu Wang  Peng Zhao  Ming
Pang and Kangle Zhao for helpful discussions.

9

48121620layer index889092949698100accuracy (%)Train accTest acc0.00.10.20.30.40.50.60.70.8std/meanTrain ratioTest ratio10010Layer 11001010010Layer 510010References
[1] Breiman  L. Stacked regressions. Machine Learning  24(1):49–64  1996.

[2] Breiman  L. Prediction games and Arcing algorithms. Neural Computation  11(7):1493–1517 

1999.

[3] Breiman  L. Random forests. Machine Learning  45(1):5–32  2001.

[4] Chang  C.-C. and Lin  C.-J. LIBSVM: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology  2(3):27  2011.

[5] Chen  T. and Guestrin  C. XGBoost: A scalable tree boosting system. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pp.
785–794  2016.

[6] Chernoff  H. et al. A measure of asymptotic efﬁciency for tests of a hypothesis based on the

sum of observations. Annals of Mathematical Statistics  23(4):493–507  1952.

[7] Cortes  C. and Vapnik  V. Support-vector networks. Machine Learning  20(3):273–297  1995.

[8] Cortes  C.  Mohri  M.  and Syed  U. Deep boosting. In Proceedings of the 31st International

Conference on Machine Learning  pp. 1179–1187  2014.

[9] Cortes  C.  Gonzalvo  X.  Kuznetsov  V.  Mohri  M.  and Yang  S. AdaNet: Adaptive structural
learning of artiﬁcial neural networks. In Proceedings of the 34th International Conference on
Machine Learning  pp. 874–883  2017.

[10] Crammer  K. and Singer  Y. On the algorithmic implementation of multiclass kernel-based

vector machines. Journal of Machine Learning Research  2:265–292  2001.

[11] Dheeru  D. and Karra Taniskidou  E. UCI machine learning repository  2017.

[12] Elsayed  G.  Krishnan  D.  Mobahi  H.  Regan  K.  and Bengio  S. Large margin deep networks
for classiﬁcation. In Advances in Neural Information Processing Systems 31  pp. 850–860 
2018.

[13] Feng  J. and Zhou  Z. Autoencoder by forest. In Proceedings of the 32nd AAAI Conference on

Artiﬁcial Intelligence  pp. 2967–2973  2018.

[14] Feng  J.  Yu  Y.  and Zhou  Z.-H. Multi-layered gradient boosting decision trees. In Advances in

Neural Information Processing Systems 31  pp. 3555–3565  2018.

[15] Friedman  J. H. Greedy function approximation: A gradient boosting machine. Annals of

Statistics  pp. 1189–1232  2001.

[16] Gao  W. and Zhou  Z.-H. On the doubt about margin explanation of boosting. Artiﬁcial

Intelligence  203:1–18  2013.

[17] Huang  F.  Ash  J.  Langford  J.  and Schapire  R. Learning deep ResNet blocks sequentially
In Proceedings of the 35th International Conference on Machine

using boosting theory.
Learning  pp. 2058–2067  2018.

[18] LeCun  Y.  Bottou  L.  Bengio  Y.  and Haffner  P. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[19] LeCun  Y.  Bengio  Y.  and Hinton  G. Deep learning. Nature  521(7553):436  2015.

[20] Lyu  S.-H.  Wang  L.  and Zhou  Z.-H. Optimal margin distribution network. CoRR  ab-

s/1812.10761  2018.

[21] Pang  M.  Ting  K.-M.  Zhao  P.  and Zhou  Z.-H. Improving deep forest by conﬁdence screening.
In Proceedings of the 18th IEEE International Conference on Data Mining  pp. 1194–1199 
2018.

10

[22] Reyzin  L. and Schapire  R. E. How boosting the margin can also boost classiﬁer complexity.
In Proceedings of the 23rd International Conference on Machine Learning  pp. 753–760  2006.

[23] Schapire  R. E. and Freund  Y. Boosting: Foundations and Algorithms. MIT press  2012.

[24] Schapire  R. E.  Freund  Y.  Bartlett  P.  Lee  W. S.  et al. Boosting the margin: A new explanation

for the effectiveness of voting methods. Annals of Statistics  26(5):1651–1686  1998.

[25] Utkin  L. V. and Ryabinin  M. A. A siamese deep forest. Knowledge-Based Systems  139:13–22 

2018.

[26] van der Maaten  L. and Hinton  G. Visualizing data using t-SNE. Journal of Machine Learning

Research  9:2579–2605  2008.

[27] Wolpert  D. H. Stacked generalization. Neural Networks  5(2):241–259  1992.

[28] Zhang  T. and Zhou  Z.-H. Multi-class optimal margin distribution machine. In Proceedings of

the 34th International Conference on Machine Learning  pp. 4063–4071  2017.

[29] Zhang  T. and Zhou  Z.-H. Optimal margin distribution machine.

IEEE Transactions on

Knowledge and Data Engineering  2019. doi: 10.1109/TKDE.2019.2897662.

[30] Zhou  Z.-H. Ensemble Methods: Foundations and Algorithms. CRC Press  2012.

[31] Zhou  Z.-H. Large margin distribution learning. In Proceedings of the 6th IAPR International

Workshop on Artiﬁcial Neural Networks in Pattern Recognition  pp. 1–11  2014.

[32] Zhou  Z.-H. and Feng  J. Deep forest: Towards an alternative to deep neural networks. In
Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence  pp. 3553–3559 
2017.

[33] Zhou  Z.-H. and Feng  J. Deep forest. National Science Review  6(1):74–86  2019.

11

,Arya Mazumdar
Barna Saha
Shen-Huan Lyu
Liang Yang
Zhi-Hua Zhou