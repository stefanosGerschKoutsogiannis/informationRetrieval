2018,Leveraging the Exact Likelihood of Deep Latent Variable Models,Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however  the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First  we investigate maximum likelihood estimation for DLVMs: in particular  we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates  and draw useful connections with nonparametric mixture models. Finally  we describe an algorithm for missing data imputation using the exact conditional likelihood of a DLVM. On several data sets  our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.,Leveraging the Exact Likelihood of Deep Latent

Variable Models

Pierre-Alexandre Mattei

Department of Computer Science

IT University of Copenhagen

pima@itu.dk

Jes Frellsen

Department of Computer Science

IT University of Copenhagen

jefr@itu.dk

Abstract

Deep latent variable models (DLVMs) combine the approximation abilities of deep
neural networks and the statistical foundations of generative models. Variational
methods are commonly used for inference; however  the exact likelihood of these
models has been largely overlooked. The purpose of this work is to study the
general properties of this quantity and to show how they can be leveraged in
practice. We focus on important inferential problems that rely on the likelihood:
estimation and missing data imputation. First  we investigate maximum likelihood
estimation for DLVMs: in particular  we show that most unconstrained models
used for continuous data have an unbounded likelihood function. This problematic
behaviour is demonstrated to be a source of mode collapse. We also show how to
ensure the existence of maximum likelihood estimates  and draw useful connections
with nonparametric mixture models. Finally  we describe an algorithm for missing
data imputation using the exact conditional likelihood of a DLVM. On several data
sets  our algorithm consistently and signiﬁcantly outperforms the usual imputation
scheme used for DLVMs.

1

Introduction

Dimension reduction aims at summarizing multivariate data using a small number of features that
constitute a code. Earliest attempts rested on linear projections  leading to Hotelling’s (1933) principal
component analysis (PCA) that has been vastly explored and perfected over the last century (Jolliffe
and Cadima  2016). In recent years  the ﬁeld has been vividly animated by the successes of latent
variable models that probabilistically use the low-dimensional features to deﬁne powerful generative
models. Usually  these latent variable models transform the random code into parameters of a simple
distribution. Linear mappings were initially considered  giving rise to factor analysis (Bartholomew
et al.  2011) and probabilistic principal component analysis (Tipping and Bishop  1999). In recent
years  much work has been done regarding nonlinear mappings parametrised by deep neural networks 
following the seminal papers of Rezende et al. (2014) and Kingma and Welling (2014). These
models have led to impressive empirical performance in unsupervised or semi-supervised generative
modelling of images (Siddharth et al.  2017)  molecular structures (Kusner et al.  2017; Gómez-
Bombarelli et al.  2018)  arithmetic expressions (Kusner et al.  2017)  and single-cell gene expression
data (Grønbech et al.  2018). This paper is an investigation of the statistical properties of these models 
which remain essentially unknown.

1.1 Deep latent variable models

In their most common form  deep latent variable models (DLVMs) assume that we are in the presence
of a data matrix X = (x1  ...  xn)T ∈ X n that we wish to explain using some latent variables
Z = (z1  ...  zn)T ∈ Rn×d. We assume that (xi  zi)i≤n are independent and identically distributed

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(i.i.d.) random variables driven by the following generative model:

(cid:26) z ∼ p(z)

pθ(x|z) = Φ(x|fθ(z)).

(1)

The unobserved random vector z ∈ Rd is called the latent variable and usually follows marginally a
simple distribution p(z) called the prior distribution. The dimension d of the latent space is called
the intrinsic dimension—and is usually smaller than the dimensionality of the data. The collection
(Φ(·|η))η∈H is a parametric family of densities with respect to a dominating measure (usually the
Lebesgue or the counting measure) called the observation model. The function fθ : Rd → H is
called a decoder or a generative network  and is parametrised by a (deep) neural network whose
weights are stored in θ ∈ Θ. The latent structure of these DLVMs leads to the following marginal
distribution of the data:

pθ(x|z)p(z)dz =

Φ(x|fθ(z))p(z)dz.

(2)

(cid:90)

pθ(x) =

Rd

This parametrisation allows to leverage recent advances in deep architectures  such as deep residual
networks (Kingma et al.  2016)  recurrent networks (Bowman et al.  2016; Gómez-Bombarelli et al. 
2018)  or batch normalisation (Sønderby et al.  2016).
Several observation models have been considered: in case of discrete multivariate data  products
of Bernoulli (or multinomial) distributions; multivariate Gaussian distributions for continuous data;
products of Poisson distributions for multivariate count data (Grønbech et al.  2018). Several speciﬁc
proposals for image data have been made  like the discretised logistic mixture of Salimans et al.  2017).
Dirac observation models correspond to deterministic decoders  that are used e.g. within generative
adversarial networks (Goodfellow et al.  2014)  or non-volume preserving transformations (Dinh
et al.  2017). Introduced by both Kingma and Welling (2014) and Rezende et al. (2014)  the Gaussian
and Bernoulli families are the most widely studied  and will be the focus of this article.

1.2 Scalable learning through amortised variational inference
The log-likelihood function of a DLVM is  for all θ ∈ Θ 

(cid:90)

Rd

n(cid:88)

i=1

(cid:96)(θ) = log pθ(X) =

log pθ(xi) 

(3)

which is an extremely challenging quantity to compute that involves potentially high-dimensional
integrals. Estimating θ by maximum likelihood appears therefore out of reach. Consequently 
following Rezende et al. (2014) and Kingma and Welling (2014)  inference in DLVMs is usually
performed using amortised variational inference. Variational inference approximatively maximises
the log-likelihood by maximising a lower bound known as the evidence lower bound (ELBO  see
e.g. Blei et al.  2017):

ELBO(θ  q) = EZ∼q

log

p(X  Z)

q(Z)

= (cid:96)(θ) − KL(q||p(·|X)) ≤ (cid:96)(θ) 

(4)

where the variational distribution q is a distribution over the space of codes Rn×d. The variational
distribution plays the role of a tractable approximation of the posterior distribution of the codes;
when this approximation is perfectly accurate  the ELBO is equal to the log-likelihood. Amortised
inference builds q using a neural network called the inference network gγ : X → K  whose weights
are stored in γ ∈ Γ:

(cid:20)

qγ X(Z) = qγ X(z1  ...  zn) =

(5)
where (Ψ(·|κ))κ∈K is a parametric family of distributions over Rd—such as Gaussians with diagonal
covariances (Kingma and Welling  2014). Other kinds of families—built using e.g. normalising
ﬂows (Rezende and Mohamed  2015; Kingma et al.  2016)  auxiliary variables (Maaløe et al.  2016;
Ranganath et al.  2016)  or importance weights (Burda et al.  2016; Cremer et al.  2017)—have been
considered for amortised inference  but they will not be central focus of in this paper. Variational

Ψ(zi|gγ(xi)) 

n(cid:89)

i=1

(cid:21)

2

inference for DLVMs then solves the optimisation problem maxθ∈Θ γ∈Γ ELBO(θ  qγ X) using
variants of stochastic gradient ascent (see e.g. Roeder et al.  2017  for strategies for computing
gradients estimates of the ELBO).
As emphasised by Kingma and Welling (2014)  the ELBO resembles the objective function of a
popular deep learning model called an autoencoder (see e.g. Goodfellow et al.  2016  Chapter 14).
This motivates the popular denomination of encoder for the inference network gγ and variational
autoencoder (VAE) for the combination of a DLVM with amortised variational inference.

be seen as parsimonious submodels of nonparametric mixture models.

In this work  we revisit DLVMs by asking: Is it possible to leverage the properties

Contributions.
of pθ(x) to understand and improve deep generative modelling? Our main contributions are:
• We show that maximum likelihood is ill-posed for continuous DLVMs and well-posed for discrete
ones. We link this undesirable property of continuous DLVMs to the mode collapse phenomenon 
and illustrate it on a real data set.
• We draw a connection between DLVMs and nonparametric statistics  and show that DLVMs can
• We leverage this connection to provide a way of ﬁnding an upper bound of the likelihood based on
ﬁnite mixtures. Combined with the ELBO  this bound allows us to provide useful “sandwichings”
of the exact likelihood. We also prove that this bound characterises the large capacity behaviour
of DLVMs.
• When dealing with missing data  we show how a simple modiﬁcation of an approximate scheme
proposed by Rezende et al. (2014) allows us to draw according to the exact conditional distribution
of the missing data. On several data sets and missing data scenarios  our algorithm consistently
outperforms the one of Rezende et al. (2014)  while having the same computational cost.

2

Is maximum likelihood well-deﬁned for deep latent variable models?

In this section  we investigate the properties of maximum likelihood estimation for DLVMs with
Gaussian and Bernoulli observation models.

2.1 On the boundedness of the likelihood of deep latent variable models
Deep generative models with Gaussian observation models assume that the data space is X = Rp  and
that the observation model is the family of p-variate full-rank Gaussian distributions. The conditional
distribution of each data point is consequently

pθ(x|z) = N (x|µθ(z)  Σθ(z)) 

(6)

(7)

where µθ : Rd → Rp and Σθ : Rd → S ++
are two continuous functions parametrised by neural
networks whose weights are stored in a parameter θ. These two functions constitute the decoder of
(cid:18)(cid:90)
the model. This leads to the log-likelihood

(cid:19)

p

(cid:96)(θ) =

log

Rd

N (xi|µθ(z)  Σθ(z))p(z)dz

.

n(cid:88)

i=1

This model can be seen as a special case of inﬁnite mixture of Gaussian distributions. However  it
is well-known that maximum likelihood is ill-posed for ﬁnite Gaussian mixtures (see e.g. Le Cam 
1990). Here  by “ill-posed”  we mean that  inside the parameter space  there exists no maximiser
of the likelihood function  which corresponds to the ﬁrst condition given by Tikhonov and Arsenin
(1977  p.7 ). This happens because the likelihood function is unbounded above. Moreover  the inﬁnite
maxima of the likelihood happen to be very poor generative models  whose density collapse around
some of the data points. This problematic behaviour of a model quite similar to DLVMs motivates
the question: is the likelihood function of DLVMs bounded above?
In this section  we will not make any particular parametric assumption about the prior distribution
of the latent variable z. While Kingma and Welling (2014) and Rezende et al. (2014) originally
proposed to use isotropic Gaussian distributions  more complex learnable priors have also been
proposed (e.g. Tomczak and Welling  2018). We simply make the natural assumptions that z is
continuous and has zero mean. Many different neural architectures have been explored regarding

3

the parametrisation of the decoder. For example  Kingma and Welling (2014) consider multilayer
perceptrons (MLPs) of the form

µθ(z) = V tanh (Wz + a) + b  Σθ(z) = Diag (exp (α tanh (Wz + a) + β))  

(8)
where θ = (W  a  V  b  α  β). The weights of the decoder are W ∈ Rh×d  a ∈ Rh  V  α ∈ Rp×h 
and b  β ∈ Rp. The integer h ∈ N∗ is the (common) number of hidden units of the MLPs. Much
more complex parametrisations exist  but we will see that this one  arguably one of the most rigid  is
already too ﬂexible for maximum likelihood. Actually  we will show that an even much less ﬂexible
family of MLPs with a single hidden unit is problematic and leads the model to collapse around a data
point. Let w ∈ Rp and let (αk)k≥1 be a sequence of nonnegative real numbers such that αk → +∞
as k → +∞. Let us consider i∗ ∈ {1  ...  n}: this arbitrary index will represent the observation
around which the model will collapse. Using the parametrisation (8)  we consider the sequences of
∗
parameters θ(i
k

= (αkwT   0  0p  xi∗   αk1p −αk1p). This leads to the simpliﬁed decoders:

 w)

µθ(i∗  w)

k

(z) = xi∗   Σ

θ(i∗  w)

k

(9)

(z) = exp(cid:0)αk tanh(cid:0)αkwT z(cid:1) − αk
(cid:16)

(cid:1) Ip.

(cid:17)

∗

 w)

k

k

θ(i
k

θ(i∗  w)

= +∞.

As shown by next theorem  these sequences of decoders lead to the divergence of the log-likelihood
function.
Theorem 1. For all i∗ ∈ {1  ...  n} and w ∈ Rd \ {0}  we have limk→+∞ (cid:96)
A detailed proof is provided in Appendix A (all appendices of this paper are available as supplementary
converges to a function
material). Its cornerstone is the fact that the sequence of functions Σ
that outputs both singular and nonsingular covariances  leading to the explosion of log pθ(i∗  w)
(xi∗ )
while all other terms of the log-likelihood remain bounded below by a constant.
Using simple MLP-based parametrisations such a the one of Kingma and Welling (2014) therefore
brings about an unbounded log-likelihood function. A natural question that follows is: do these
inﬁnite suprema lead to useful generative models? The answer is no. Actually  none of the functions
considered in Theorem 1 are particularly useful  because of the use of a constant mean function. This
is formalised in the next proposition  that exhibits a strong link between likelihood blow-up and the
mode collapse phenomenon.
Proposition 1. For all k ∈ N∗  i∗ ∈ {1  ...  n}  and w ∈ Rd \ {0}  the distribution pθ(i∗  w)
spherically symmetric and unimodal around xi∗.
A proof is provided in Appendix B. This is a direct consequence of the constant mean function.
The spherical symmetry implies that the distribution of these “optimal” deep generative model will
lead to uncorrelated variables  and the unimodality will lead to poor sample diversity. This behaviour
is symptomatic of mode collapse  which remains one of the most challenging drawbacks of generative
modelling (Arora et al.  2018). While mode collapse has been extensively investigated for adversarial
training (e.g. Arora et al.  2018; Lucas et al.  2018)  this phenomenon is also known to affect VAEs
(Richardson and Weiss  2018).
Unregularised gradient-based optimisation of a tight lower bound of this unbounded likelihood
is therefore likely to follow these (uncountably many) paths to blow-up. This gives a theoretical
foundation to the necessary regularisation of VAEs that was already noted by Rezende et al. (2014)
and Kingma and Welling (2014). For example  using weight decay as in Kingma and Welling
(2014) is likely to help avoiding these inﬁnite maxima. This difﬁculty to learn the variance was also
experimentally noticed by Takahashi et al. (2018)  and may explain the choice made by several authors
to use a constant variance function Σ(z) = σ0Ip  where σ0 can be either ﬁxed (Zhao et al.  2017)
or learned via approximate maximum likelihood (Pu et al.  2016). Dai et al. (2018) independently
showed that the VAE objective is also unbounded in the case where such a constant variance function
is combined with a nonparametric mean function. An interesting feature of our result is that it only
involves a decoder of very low capacity.

is

k

Tackling the unboundedness of the likelihood. Let us go back to a parametrisation which is not
necessarily MLP-based. Even in this general context  it is possible to tackle the unboundedness of the
likelihood using additional constraints on Σθ. Speciﬁcally  for each ξ ≥ 0  we will consider the set
S ξ
p = {A ∈ S +
p   SpA denotes the spectrum of A. Note
that S 0
p = S +

p | min(SpA) ≥ ξ}  where  for all A ∈ S +
p . This simple spectral constraint allows to end up with a bounded likelihood.

4

p for all θ  then the log-likelihood function is upper bounded by −np log

Proposition 2. Let ξ > 0. If the parametrisation of the decoder is such that the image of Σθ is
included in S ξ
Proof. For all i ∈ {1  ...  n}  we have p(xi|µθ  Σθ) ≤ (2πξ)−2p/2  using the fact that the determi-
nant of Σθ(z) is lower bounded by ξp for all z ∈ Rd and that the exponential of a negative number is
smaller than one. Therefore  the likelihood function is bounded above by 1/(2πξ)np/2.

√

2πξ.

Similar constraints have been proposed to solve the ill-posedness of maximum likelihood for ﬁnite
Gaussian mixtures (e.g. Hathaway  1985; Biernacki and Castellan  2011). In practice  implementing
such constraints can be easily done by adding a constant diagonal matrix to the output of the
covariance decoder.

What about other parametrisations? We chose a speciﬁc and natural parametrisation in order to
obtain a constructive proof of the unboundedness of the likelihood. However  virtually any other deep
parametrisation that does not include covariance constraints will be affected by our result  because of
the universal approximation abilities of neural networks (see e.g. Goodfellow et al.  2016  Section
6.4.1).
Bernoulli DLVMs do not suffer from unbounded likelihood. When X = {0  1}p  Bernoulli
DLVMs assume that (Φ(·|η))η∈H is the family of p-variate multivariate Bernoulli distributions (i.e.
the family of products of p univariate Bernoulli distributions). In this case  maximum likelihood is
well-posed.
Proposition 3. Given any possible parametrisation  the log-likelihood function of a deep latent
model with a Bernoulli observation model is everywhere negative.

Proof. This directly follows from the fact that the Bernoulli density is always smaller than one.

2.2 Towards data-dependent likelihood upper bounds

We have determined under which conditions maximum likelihood estimates exist  and have computed
simple upper bounds on the likelihood functions. Since they do not depend on the data  these bounds
are likely to be very loose. A natural follow-up issue is to seek tighter  data-dependent upper bounds
that remain easily computable. Such bounds are desirable because  combined with ELBOs  they
would allow sandwiching the likelihood between two bounds.
To study this problem  let us take a step backwards and consider a more general inﬁnite mixture model.
Precisely  given any distribution G over the generic parameter space H  we deﬁne the nonparametric
mixture model (see e.g. Lindsay  1995  Chapter 1) as:

(cid:90)

H

pG(x) =

Φ(x|η)dG(η).

(10)

i=1 log pG(xi).

Note that there are many ways for a mixture model to be nonparametric (e.g. having some nonpara-
metric components  an inﬁnite but countable number of components  or an uncountable number of
components). In this case  this comes from the fact that the model parameter is the mixing distribution
G  which belongs to the set P of all probability measures over H. The log-likelihood of any G ∈ P

is given by (cid:96)(G) =(cid:80)n

When G has a ﬁnite support of cardinal k ∈ N∗  pG is a ﬁnite mixture model with k components.
When the mixing distribution G is generatively deﬁned by the distribution of a random variable η
such that z ∼ p(z)  η = fθ(z)  we exactly end up with a deep generative model with decoder fθ.
Therefore  the nonparametric mixture is a more general model that the DLVM. The fact that the
mixing distribution of a DLVM is intrinsically low-dimensional leads us to interpret the DLVM as a
parsimonious submodel of the nonparametric mixture model. This also gives us an immediate upper
bound on the likelihood of any decoder fθ: (cid:96)(θ) ≤ maxG∈P (cid:96)(G).
Of course  in many cases  this upper bound will be inﬁnite (for example in the case of an unconstrained
Gaussian observation model). However  under the conditions of boundedness of the likelihood of
deep Gaussian models  the bound is ﬁnite and attained for a ﬁnite mixture model with no more
components than data points.

5

Theorem 2. Assume that (Φ(·|η))η∈H is the family of multivariate Bernoulli distributions or the
family of Gaussian distributions with the spectral constraint of Proposition 2. The likelihood of
the corresponding nonparametric mixture model is maximised for a ﬁnite mixture model of k ≤ n
distributions from the family (Φ(·|η))η∈H.
A detailed proof is provided in Appendix C. The main tool of the proof of this rather surprising
result is Lindsay’s (1983) geometric analysis of the likelihood of nonparametric mixtures  based on
Minkovski’s theorem. Speciﬁcally  Lindsay’s (1983) Theorem 3.1 ensures that  when the trace of
the likelihood curve is compact  the likelihood function is maximised for a ﬁnite mixture. For the
Bernoulli case  compactness of the curve is immediate; for the Gaussian case  we use a compactiﬁca-
tion argument inspired by van der Vaart and Wellner (1992).
Assume now that the conditions of The-
orem 2 are satisﬁed. Let us denote a
maximum likelihood estimate of G as
ˆG. For all θ  we therefore have

(cid:96)(θ) ≤ (cid:96)( ˆG) 

(11)
which gives an upper bound on the like-
lihood. We call the difference (cid:96)( ˆG) −
(cid:96)(θ) the parsimony gap (see Fig. 1).
By sandwiching the exact likelihood
between this bound and an ELBO  we
can also have guarantees on how far a
posterior approximation q is from the
true posterior:
KL(q||p(·|X)) ≤ (cid:96)( ˆG)−ELBO(θ  q).
(12)
Figure 1: The parsimony gap represents the amount of
likelihood lost due to the architecture of the decoder. The
Note that ﬁnding upper bounds of the
approximation gap expresses how far the posterior is from
likelihood of latent variable models
the variational family  and the amortisation gap appears due
is usually harder than ﬁnding lower
to the limited capacity of the encoder (Cremer et al.  2018).
bounds (Grosse et al.  2015; Dieng
et al.  2017). From a computational perspective  the estimate ˆG can be found using the expectation-
maximisation algorithm for ﬁnite mixtures (Dempster et al.  1977)—although it only ensures to ﬁnd
a local optimum. Some strategies guaranteed to ﬁnd a global optimum have also been developed
(e.g. Lindsay  1995  Chapter 6  or Wang  2007).
Now that computationally approachable upper bounds have been derived  the question remains
whether or not these bounds can be tight. Actually  as shown by next theorem  tightness of the
parsimony gap occurs when the decoder has universal approximation abilities. In other words  the
nonparametric upper bound characterises the large capacity limit of the decoder.
Theorem 3 (Tightness of the parsimony gap). Assume that
1. (Φ(·|η))η∈H is the family of multivariate Bernoulli distributions or the family of Gaussian
2. The decoder has universal approximation abilities : for any compact C ⊂ Rd and continuous

distributions with the spectral constraint of Proposition 2.

∞ < ε.

function f : C → H  for all ε > 0  there exists θ such that ||f − fθ ||
Then  for all ε > 0  there exists θ ∈ Θ such that (cid:96)( ˆG) ≥ (cid:96)(θ) ≥ (cid:96)( ˆG) − ε.
A detailed proof is provided in Appendix D. The main idea is to split the code space into a compact
set made of several parts that will represent the mixture components  and an unbounded set of very
small prior mass. The universal approximation property is ﬁnally used for this compact set.
The universal approximation condition is satisﬁed for example by MLPs with nonpolynomial ac-
tivations (Leshno et al.  1993). Combined with the work of Cremer et al. (2018)  who studied the
large capacity limit of the encoder  this result describes the general behaviour of a VAE in the large
capacity limit (see Fig. 1). Note eventually that Rezende and Viola (2018) analysed the large capacity
behaviour of the VAE objective  and also found connections with ﬁnite mixtures.

6

`(✓)`(ˆG)ELBO(✓ q⇤X)ELBO(✓ q X)parsimonygaptightwhenf✓haslargecapacitytightwhenghaslargecapacityamortisationgapapproximationgaptightwhentheposteriorbelongstothevariationalfamily3 Missing data imputation using the exact conditional likelihood

In this section  we assume that a variational autoencoder has been trained  and that some data is
missing at test time. The couple decoder/encoder obtained after training is denoted by fθ and gγ. Let
x ∈ X be a new data point that consists of some observed features xobs and missing data xmiss. Since
we have a probabilistic model pθ of the data  an ideal way of imputing xmiss would be to generate
some data according to the conditional distribution

pθ(xmiss|xobs) =

pθ(xmiss|xobs  z)p(z|xobs)dz.

(13)

(cid:90)

Rd

Again  this distribution appears out of reach because of the integration of the latent variable z.
However  it is reasonable to assume that  for all η  it is easy to sample from the marginals of Φ(·|η).
This is for instance the case for Gaussian observation models and factorised observation models
(like products of Bernoulli or Poisson distributions). A direct consequence of this assumption is that 
for all z  it is easy to sample from pθ(xmiss|xobs  z). Under this simple assumption  we will see that
generating data according to the conditional distribution is actually (asymptotically) possible.

3.1 Pseudo-Gibbs sampling

t

0

t−1)) and ˆxmiss

)t≥1(initialised by randomly imputing the missing data with ˆxmiss

Rezende et al. (2014) proposed a simple way of imputing xmiss by following a Markov chain
). For all t ≥ 1  the chain
(zt  ˆxmiss
alternatively generates zt ∼ Ψ(z|gγ(xobs  ˆxmiss
t ∼ pθ(xmiss|xobs  z) until convergence.
This scheme closely resembles Gibbs sampling (Geman and Geman  1984)  and actually exactly
coincides with Gibbs sampling when the amortised variational distribution Ψ(z|gγ(xobs  ˆxmiss))
is equal to the true posterior distribution pθ(z|xobs  ˆxmiss) for all possible ˆxmiss. Following the
terminology of Heckerman et al. (2000)  we will call this algorithm pseudo-Gibbs sampling. Very
similar schemes have been proposed for more general autoencoder settings (Goodfellow et al.  2016 
Section 20.11). Because of its ﬂexibility  this pseudo-Gibbs approach is routinely used for missing
data imputation using DLVMs (see e.g. Li et al.  2016; Rezende et al.  2016; Du et al.  2018). Rezende
et al. (2014  Proposition F.1) proved that  when these two distributions are close in some sense 
pseudo-Gibbs sampling generates points that approximatively follow the conditional distribution
pθ(xmiss|xobs). Actually  we will see that a simple modiﬁcation of this scheme allows to generate
exactly according to the conditional distribution.

3.2 Metropolis-within-Gibbs sampling

)

Φ(xobs ˆxmiss

  ...  ˆxmiss
T .

1

Algorithm 1 Metropolis-within-Gibbs sampler for missing
data imputation using a trained VAE

Inputs: Observed data xobs  trained VAE (fθ  gγ)  number
of iterations T
Outputs: Markov chain of imputations ˆxmiss
Initialise (z0  ˆxmiss
0
for t = 1 to T do

At each step of the chain  rather
than generating codes according to
the approximate posterior distribu-
tion  we may use this approximation
as a proposal within a Metropolis-
Hastings algorithm (Metropolis
et al.  1953; Hastings  1970)  using
the fact that we have access to the
unnormalised posterior density of
the latent codes.
Speciﬁcally  at each step  we will
generate a new code ˜zt as a pro-
posal using the approximate poste-
rior Ψ(z|gγ(xobs  ˆxmiss
t−1)). This pro-
posal is kept as a valid code with ac-
ceptance probability ρt  deﬁned in
Algorithm 1. This probability corre-
sponds to a ratio of importance ratios  and is equal to one when the posterior approximation is perfect.
This code-generating scheme exactly corresponds to performing a single iteration of an independent
Metropolis-Hastings algorithm. With the obtained code zt  we can now generate a new imputation
using the exact conditional Φ(xmiss|xobs  fθ(zt)). The obtained algorithm  detailed in Algorithm 1 
is a particular instance of a Metropolis-within-Gibbs algorithm. Actually  it exactly corresponds to

˜zt ∼ Ψ(z|gγ(xobs  ˆxmiss
t−1))
Φ(xobs ˆxmiss
˜ρt =
ρt = min{ ˜ρt  1}
zt =
t ∼ pθ(xmiss|xobs  zt)
ˆxmiss
end for

(cid:26) ˜zt
zt−1 with probability 1 − ρt

Ψ(zt−1|gγ (xobs ˆxmiss
t−1))
Ψ(˜zt|gγ (xobs ˆxmiss
t−1))

t−1|fθ (˜zt))p(˜zt)

t−1|fθ (zt−1))p(zt−1)

with probability ρt

7

the algorithm described by Gelman (1993  Section 4.4)  and is ensured to asymptotically produce
samples from the true conditional distribution pθ(xmiss|xobs)  even if the variational approximation is
imperfect. Note that when the variational approximation is perfect  all proposals are accepted and the
algorithm exactly reduces to Gibbs sampling.
The theoretical superiority of the Metropolis-within-Gibbs scheme compared to the pseudo-Gibbs
sampler comes with almost no additional computational cost. Indeed  all the quantities that need
to be computed in order to compute the acceptance probability need also to be computed within
the pseudo-Gibbs scheme—except for prior evaluations  which are assumed to be computationally
negligible. However  a poor initialisation of the missing values might lead to a lot of rejections at the
beginning of the chain  and to slow convergence. A good initialisation heuristic is to perform a few
pseudo-Gibbs iterations at ﬁrst in order to begin with a sensible imputation. Note also that  similarly
to the pseudo-Gibbs sampler  our Metropolis-within-Gibbs scheme can be extended to many other
variational approximations—like normalising ﬂows (Rezende and Mohamed  2015; Kingma et al. 
2016)—in a straightforward manner.

4 Empirical results

In this section  we investigate the empirical realisations of our theoretical ﬁndings on DLVMs. For
architecture and implementation details  see Appendix E (in the supplementary material).

4.1 Witnessing likelihood blow-up

To investigate if the unboundedness
of the likelihood of a DLVM with
a Gaussian observation model has
actual concrete consequences for
variational inference  we train two
DLVMs on the Frey faces data set:
one with no constraints  and one
with the constraint of Proposition
2 (with ξ = 2−4). The results are
presented in Fig. 2. One can notice
that the unconstrained DLVM ﬁnds
models with very high likelihood
but very poor generalisation perfor-
mance. This conﬁrms that the un-
boundedness of the likelihood is not
a merely theoretical concern. We
also display the two upper bounds of
the likelihood. The nonparametric
bound offers a slight but signiﬁcant
improvement over the naive upper
bound. On this example  using the
nonparametric upper bound as an early stopping criterion for the unconstrained ELBO appears to
provide a good regularisation scheme—that perform better than the covariance constraints on this
data set. This illustrates the potential practical usefulness of the connection that we drew between
DLVMs and nonparametric mixtures.

Figure 2: Likelihood blow-up for the Frey Faces data. The
unconstrained ELBO appears to diverge  while ﬁnding increas-
ingly poor models.

4.2 Comparing the pseudo-Gibbs and Metropolis-within-Gibbs samplers

We compare the two samplers for single imputation of the test sets of three data sets: Caltech 101
Silhouettes and statically binarised versions of MNIST and OMNIGLOT. We consider two missing
data scenarios: a ﬁrst one with pixels missing uniformly at random (the fractions of missing data
considered are 40%  50%  60%  70%  and 80%) and one where the top or bottom half of the pixels
was removed. Both samplers use the same trained VAE and perform the same number of iterations.
The imputations are made by computing the means of the chains  which estimate the conditional
expected value of the missing data. Since the imputation of these high-dimensional binary data sets
can be interpreted as imbalanced binary classiﬁcation problems  we use the F1 score (the harmonic

8

−25002505000500010000EpochsLog−likelihood boundsConstrained ELBO (Test)Constrained ELBO (Training)Constrained naive upper boundConstrained nonparametric upper boundUnconstrained ELBO (Test)Unconstrained ELBO (Training)Figure 3: Single imputation results (F1 score between the true and imputed values) for the two Markov
chains. Additional results for the bottom missing and the 50% and 70% MAR cases are provided as
supplementary material. The more the conditional distribution is challenging (high-dimensional in
the MAR cases and highly multimodal in the top/bottom cases)  the more the performance gain of
our Metropolis-within-Gibbs scheme is important.

mean of precision and recall) as a performance metric. For both schemes  we use 50 iterations
of Pseudo-Gibbs as burn-in. In practice  convergence and mixing of the chains can be monitored
using a validation set of complete data. The results are displayed on Fig. 3 and in Appendix F
(see supplementary material). The chains converge much faster for the missing at random (MAR)
situation than for the top/bottom missing scenario. This is probably due to the fact that the conditional
distribution of the missing half of an image is highly multimodal. The Metropolis-within-Gibbs
sampler consistently outperforms the pseudo-Gibbs scheme  especially for the most challenging
scenarios where the top/bottom of the image is missing. One can see that the pseudo-Gibbs sampler
appears to converge quickly to a stationary distribution that gives suboptimal results. Because of the
rejections  the Metropolis-within-Gibbs algorithm converges slower  but to a much more accurate
conditional distribution.

5 Conclusion

Although extremely difﬁcult to compute in practice  the exact likelihood of DLVMs offers several
important insights on deep generative modelling. An important research direction for future work is
the design of principled regularisation schemes for maximum likelihood estimation.
The objective evaluation of deep generative models remains an open question. Missing data imputation
is often used as a performance metric for DLVMs (e.g. Li et al.  2016; Du et al.  2018). Since both
algorithms have essentially the same computational cost  this motivates to replace pseudo-Gibbs
sampling by Metropolis-within-Gibbs when evaluating these models. Upon convergence  the samples
generated by Metropolis-within-Gibbs do not depend on the inference network  and explicitly depend
on the prior  which allows us to evaluate mainly the generative performance of the models.
We interpreted DLVMs as parsimonious submodels of nonparametric mixture models. While we
used this connection to provide upper bounds of the likelihood  many other applications could be
derived. In particular  the important body of work regarding consistency of maximum likelihood
estimates for nonparametric mixtures (e.g. Kiefer and Wolfowitz  1956; van de Geer  2003; Chen 
2017) could be leveraged to study the asymptotics of DLVMs.

9

MNIST40% MAR0.40.50.60.70.80500100015002000F1 scoreMNIST 60% MAR0.40.50.60.70.80500100015002000MNIST 80% MAR0.40.50.60.70.80500100015002000MNIST top missing0.40.50.60.70.80100002000030000OMNIGLOT40% MAR0.20.30.40100020003000F1 scoreOMNIGLOT 60% MAR0.20.30.40100020003000OMNIGLOT 80% MAR0.20.30.40100020003000OMNIGLOT top missing0.20.30.40100002000030000Caltech Silhouettes40% MAR0.750.800.850.900.950100020003000IterationsF1 scoreCaltech Silhouettes 60% MAR0.750.800.850.900.950100020003000IterationsCaltech Silhouettes 80% MAR0.750.800.850.900.950100020003000IterationsCaltech Silhouettes top missing0.750.800.850.900.95025005000750010000IterationsMetropolis−within−GibbsPseudo−Gibbs (Rezende et al.  2014)References
S. Arora  A. Risteski  and Y. Zhang. Do GANs learn the distribution? Some theory and empirics. In

International Conference on Learning Representations  2018.

D. J. Bartholomew  M. Knott  and I. Moustaki. Latent variable models and factor analysis: A uniﬁed

approach  volume 904. John Wiley & Sons  2011.

C. Biernacki and G. Castellan. A data-driven bound on variances for avoiding degeneracy in univariate

Gaussian mixtures. Pub. IRMA Lille  71  2011.

D. M. Blei  A. Kucukelbir  and J. D. McAuliffe. Variational inference: A review for statisticians.

Journal of the American Statistical Association  112(518):859–877  2017.

S. R. Bowman  L. Vilnis  O. Vinyals  A. M. Dai  R. Jozefowicz  and S. Bengio. Generating sentences

from a continuous space. Proceedings of CoNLL  2016.

Y. Burda  R. Grosse  and R. Salakhutdinov.

Importance weighted autoencoders.

Conference on Learning Representations  2016.

International

J. Chen. Consistency of the MLE under mixture models. Statistical Science  32(1):47–63  2017.

C. Cremer  Q. Morris  and D. Duvenaud. Reinterpreting importance-weighted autoencoders. Interna-

tional Conference on Learning Representations (Workshop track)  2017.

C. Cremer  X. Li  and D. Duvenaud.

Inference suboptimality in variational autoencoders.

Proceedings of the 35th International Conference on Machine Learning  2018.

In

B. Dai  Y. Wang  J. Aston  G. Hua  and D. Wipf. Connections with robust PCA and the role of
emergent sparsity in variational autoencoder models. The Journal of Machine Learning Research 
19(1):1573–1614  2018.

A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  pages
1–38  1977.

A. B. Dieng  D. Tran  R. Ranganath  J. Paisley  and D. Blei. Variational inference via chi upper bound
minimization. In Advances in Neural Information Processing Systems  pages 2732–2741  2017.

L. Dinh  J. Sohl-Dickstein  and S. Bengio. Density estimation using real NVP.

Conference on Learning Representations  2017.

International

C. Du  J. Zhu  and B. Zhang. Learning deep generative models with doubly stochastic gradient

MCMC. IEEE Transactions on Neural Networks and Learning Systems  PP(99):1–13  2018.

A. Gelman. Iterative and non-iterative simulation algorithms. Computing science and statistics 

pages 433–433  1993.

S. Geman and D. Geman. Stochastic relaxation  Gibbs distributions  and the Bayesian restoration of

images. IEEE Transactions on Pattern Analysis and Machine Intelligence  (6):721–741  1984.

R. Gómez-Bombarelli  J. N. Wei  D. Duvenaud  J. M. Hernández-Lobato  B. Sánchez-Lengeling 
D. Sheberla  J. Aguilera-Iparraguirre  T. D. Hirzel  R. P. Adams  and A. Aspuru-Guzik. Automatic
chemical design using a data-driven continuous representation of molecules. ACS Central Science 
2018.

I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 
pages 2672–2680  2014.

I. Goodfellow  Y. Bengio  and A. Courville. Deep learning. MIT press  2016.

C. H. Grønbech  M. F. Vording  P. N. Timshel  C. K. Sønderby  T. H. Pers  and O. Winther. scVAE:
Variational auto-encoders for single-cell gene expression data. bioRxiv  2018. URL https:
//www.biorxiv.org/content/early/2018/05/16/318295.

10

R. Grosse  Z. Ghahramani  and R. P. Adams. Sandwiching the marginal likelihood using bidirectional

monte carlo. arXiv preprint arXiv:1511.02543  2015.

W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.

Biometrika  57(1):97–109  1970.

R. J. Hathaway. A constrained formulation of maximum-likelihood estimation for normal mixture

distributions. The Annals of Statistics  13(2):795–800  1985.

D. Heckerman  D. M. Chickering  C. Meek  R. Rounthwaite  and C. Kadie. Dependency networks for
inference  collaborative ﬁltering  and data visualization. Journal of Machine Learning Research  1
(Oct):49–75  2000.

H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal of

educational psychology  24(6):417  1933.

I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philo-
sophical Transactions of the Royal Society of London A: Mathematical  Physical and Engineering
Sciences  374(2065)  2016.

J. Kiefer and J. Wolfowitz. Consistency of the maximum likelihood estimator in the presence of
inﬁnitely many incidental parameters. The Annals of Mathematical Statistics  pages 887–906 
1956.

D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In Proceedings of the International

Conference on Learning Representations  2014.

D. P. Kingma  T. Salimans  R. Jozefowicz  X. Chen  I. Sutskever  and M. Welling. Improved varia-
tional inference with inverse autoregressive ﬂow. In Advances in Neural Information Processing
Systems  pages 4743–4751  2016.

M. J. Kusner  B. Paige  and J. M. Hernández-Lobato. Grammar variational autoencoder. In Proceed-

ings of the 34th International Conference on Machine Learning  pages 1945–1954  2017.

L. Le Cam. Maximum likelihood: an introduction. International Statistical Review/Revue Interna-

tionale de Statistique  pages 153–171  1990.

M. Leshno  V. Y. Lin  A. Pinkus  and S. Schocken. Multilayer feedforward networks with a
nonpolynomial activation function can approximate any function. Neural Networks  6(6):861–867 
1993.

C. Li  J. Zhu  and B. Zhang. Learning to generate with memory.

International Conference on Machine Learning  pages 1177–1186  2016.

In Proceedings of The 33rd

B. Lindsay. The geometry of mixture likelihoods: a general theory. The Annals of Statistics  11(1):

86–94  1983.

B. Lindsay. Mixture Models: Theory  Geometry and Applications  volume 5 of Regional Conference
Series in Probability and Statistics. Institute of Mathematical Statistics and American Statistical
Association  1995.

T. Lucas  C. Tallec  J. Verbeek  and Y. Ollivier. Mixed batches and symmetric discriminators for

GAN training. In International Conference on Machine Learning  2018.

L. Maaløe  C. K. Sønderby  S. K. Sønderby  and O. Winther. Auxiliary deep generative models. In

International Conference on Machine Learning  pages 1445–1453  2016.

N. Metropolis  A. W. Rosenbluth  M. N. Rosenbluth  A. H. Teller  and E. Teller. Equation of state
calculations by fast computing machines. The Journal of Chemical Physics  21(6):1087–1092 
1953.

Y. Pu  Z. Gan  R. Henao  X. Yuan  C. Li  A. Stevens  and L. Carin. Variational autoencoder for deep
learning of images  labels and captions. In Advances in Neural Information Processing Systems 
pages 2352–2360  2016.

11

R. Ranganath  D. Tran  and D. Blei. Hierarchical variational models. In Proceedings of the 33rd

International Conference on Machine Learning  pages 324–333  2016.

D. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In Proceedings of the

32nd International Conference on Machine Learning  pages 1530–1538  2015.

D. Rezende and F. Viola. Taming VAEs. arXiv preprint arXiv:1810.00597  2018.

D. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in
deep generative models. In Proceedings of the 31st International Conference on Machine Learning 
pages 1278–1286  2014.

D. Rezende  S. M. A. Eslami  S. Mohamed  P. Battaglia  M. Jaderberg  and N. Heess. Unsupervised
learning of 3D structure from images. In Advances in Neural Information Processing Systems 
pages 4996–5004  2016.

E. Richardson and Y. Weiss. On GANs and GMMs. In Advances in Neural Information Processing

Systems  2018.

G. Roeder  Y. Wu  and D. Duvenaud. Sticking the landing: Simple  lower-variance gradient estimators
for variational inference. In Advances in Neural Information Processing Systems  pages 6928–6937 
2017.

T. Salimans  A. Karpathy  X. Chen  and D. P. Kingma. Pixelcnn++: Improving the pixelcnn with
discretized logistic mixture likelihood and other modiﬁcations. Proceedings of the International
Conference on Learning Representations  2017.

N. Siddharth  B. Paige  J.-W. van de Meent  A. Desmaison  N. Goodman  P. Kohli  F. Wood  and
P. Torr. Learning disentangled representations with semi-supervised deep generative models. In
Advances in Neural Information Processing Systems  pages 5927–5937  2017.

C. K. Sønderby  T. Raiko  L. Maaløe  S. Kaae Sønderby  and O. Winther. Ladder variational
autoencoders. In Advances in Neural Information Processing Systems  pages 3738–3746  2016.

H. Takahashi  T. Iwata  Y. Yamanaka  M. Yamada  and S. Yagi. Student-t variational autoencoder for
robust density estimation. In Proceedings of the Twenty-Seventh International Joint Conference on
Artiﬁcial Intelligence  pages 2696–2702. International Joint Conferences on Artiﬁcial Intelligence
Organization  2018.

A. N. Tikhonov and V. Y. Arsenin. Solutions of ill-posed problems. New York: Winston  1977.

M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal

Statistical Society: Series B (Statistical Methodology)  61(3):611–622  1999.

J. Tomczak and M. Welling. VAE with a VampPrior. In International Conference on Artiﬁcial

Intelligence and Statistics  pages 1214–1223  2018.

S. van de Geer. Asymptotic theory for maximum likelihood in nonparametric mixture models.

Computational Statistics & Data Analysis  41(3-4):453–464  2003.

A. W. van der Vaart and J. A. Wellner. Existence and consistency of maximum likelihood in upgraded

mixture models. Journal of Multivariate Analysis  43(1):133–146  1992.

Y. Wang. On fast computation of the non-parametric maximum likelihood estimate of a mixing
distribution. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  69(2):
185–198  2007.

S. Zhao  J. Song  and S. Ermon. Learning hierarchical features from deep generative models. In
Proceedings of the 34th International Conference on Machine Learning  pages 4091–4099  2017.

12

,Kai-Yang Chiang
Cho-Jui Hsieh
Inderjit Dhillon
Pierre-Alexandre Mattei
Jes Frellsen
Pierre Bellec
Arun Kuchibhotla