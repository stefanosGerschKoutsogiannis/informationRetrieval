2013,More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server,We propose a parameter server system for distributed ML  which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms  while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables)  and the SSP model allows distributed workers to read older  stale versions of these values from a local cache  instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing  as opposed to waiting. Furthermore  the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP  as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems  compared to fully-synchronous and asynchronous schemes.,More Effective Distributed ML via a Stale
Synchronous Parallel Parameter Server

†Qirong Ho  †James Cipar  §Henggang Cui  †Jin Kyu Kim  †Seunghak Lee 
‡Phillip B. Gibbons  †Garth A. Gibson  §Gregory R. Ganger  †Eric P. Xing
‡Intel Labs

§Electrical and Computer Engineering

Pittsburgh  PA 15213

phillip.b.gibbons@intel.com

†School of Computer Science
Carnegie Mellon University

Pittsburgh  PA 15213

Carnegie Mellon University

Pittsburgh  PA 15213

qho@  jcipar@  jinkyuk@ 

hengganc@  ganger@ece.cmu.edu

seunghak@  garth@ 
epxing@cs.cmu.edu

Abstract

We propose a parameter server system for distributed ML  which follows a Stale
Synchronous Parallel (SSP) model of computation that maximizes the time com-
putational workers spend doing useful work on ML algorithms  while still provid-
ing correctness guarantees. The parameter server provides an easy-to-use shared
interface for read/write access to an ML model’s values (parameters and vari-
ables)  and the SSP model allows distributed workers to read older  stale versions
of these values from a local cache  instead of waiting to get them from a central
storage. This signiﬁcantly increases the proportion of time workers spend com-
puting  as opposed to waiting. Furthermore  the SSP model ensures ML algorithm
correctness by limiting the maximum age of the stale values. We provide a proof
of correctness under SSP  as well as empirical results demonstrating that the SSP
model achieves faster algorithm convergence on several different ML problems 
compared to fully-synchronous and asynchronous schemes.

Introduction

1
Modern applications awaiting next generation machine intelligence systems have posed unprece-
dented scalability challenges. These scalability needs arise from at least two aspects: 1) massive
data volume  such as societal-scale social graphs [10  25] with up to hundreds of millions of nodes;
and 2) massive model size  such as the Google Brain deep neural network [9] containing billions of
parameters. Although there exist means and theories to support reductionist approaches like subsam-
pling data or using small models  there is an imperative need for sound and effective distributed ML
methodologies for users who cannot be well-served by such shortcuts. Recent efforts towards dis-
tributed ML have made signiﬁcant advancements in two directions: (1) Leveraging existing common
but simple distributed systems to implement parallel versions of a limited selection of ML models 
that can be shown to have strong theoretical guarantees under parallelization schemes such as cyclic
delay [17  1]  model pre-partitioning [12]  lock-free updates [21]  bulk synchronous parallel [5]  or
even no synchronization [28] — these schemes are simple to implement but may under-exploit the
full computing power of a distributed cluster. (2) Building high-throughput distributed ML architec-
tures or algorithm implementations that feature signiﬁcant systems contributions but relatively less
theoretical analysis  such as GraphLab [18]  Spark [27]  Pregel [19]  and YahooLDA [2].
While the aforementioned works are signiﬁcant contributions in their own right  a naturally desirable
goal for distributed ML is to pursue a system that (1) can maximally unleash the combined compu-
tational power in a cluster of any given size (by spending more time doing useful computation and
less time waiting for communication)  (2) supports inference for a broad collection of ML methods 
and (3) enjoys correctness guarantees. In this paper  we explore a path to such a system using the

1

idea of a parameter server [22  2]  which we deﬁne as the combination of a shared key-value store
that provides a centralized storage model (which may be implemented in a distributed fashion) with
a synchronization model for reading/updating model values. The key-value store provides easy-
to-program read/write access to shared parameters needed by all workers  and the synchronization
model maximizes the time each worker spends on useful computation (versus communication with
the server) while still providing algorithm correctness guarantees.
Towards this end  we propose a parameter server using a Stale Synchronous Parallel (SSP) model of
computation  for distributed ML algorithms that are parallelized into many computational workers
(technically  threads) spread over many machines. In SSP  workers can make updates  to a param-
eter1 ✓  where the updates follow an associative  commutative form ✓ ✓ + . Hence  the current
true value of ✓ is just the sum over updates  from all workers. When a worker asks for ✓  the SSP
model will give it a stale (i.e. delayed) version of ✓ that excludes recent updates . More formally 
a worker reading ✓ at iteration c will see the effects of all  from iteration 0 to c  s  1  where
s  0 is a user-controlled staleness threshold. In addition  the worker may get to see some recent
updates beyond iteration c  s  1. The idea is that SSP systems should deliver as many updates
as possible  without missing any updates older than a given age — a concept referred to as bounded
staleness [24]. The practical effect of this is twofold: (1) workers can perform more computation
instead of waiting for other workers to ﬁnish  and (2) workers spend less time communicating with
the parameter server  and more time doing useful computation. Bounded staleness distinguishes
SSP from cyclic-delay systems [17  1] (where ✓ is read with inﬂexible staleness)  Bulk Synchronous
Parallel (BSP) systems like Hadoop (workers must wait for each other at the end of every iteration) 
or completely asynchronous systems [2] (workers never wait  but ✓ has no staleness guarantees).
We implement an SSP parameter server with a table-based interface  called SSPtable  that supports
a wide range of distributed ML algorithms for many models and applications. SSPtable itself can
also be run in a distributed fashion  in order to (a) increase performance  or (b) support applications
where the parameters ✓ are too large to ﬁt on one machine. Moreover  SSPtable takes advantage of
bounded staleness to maximize ML algorithm performance  by reading the parameters ✓ from caches
on the worker machines whenever possible  and only reading ✓ from the parameter server when the
SSP model requires it. Thus  workers (1) spend less time waiting for each other  and (2) spend less
time communicating with the parameter server. Furthermore  we show that SSPtable (3) helps slow 
straggling workers to catch up  providing a systems-based solution to the “last reducer” problem on
systems like Hadoop (while we note that theory-based solutions are also possible). SSPtable can
be run on multiple server machines (called “shards”)  thus dividing its workload over the cluster;
in this manner  SSPtable can (4) service more workers simultaneously  and (5) support very large
models that cannot ﬁt on a single machine. Finally  the SSPtable server program can also be run on
worker machines  which (6) provides a simple but effective strategy for allocating machines between
workers and the parameter server.
Our theoretical analysis shows that (1) SSP generalizes the bulk synchronous parallel (BSP) model 
and that (2) stochastic gradient algorithms (e.g. for matrix factorization or topic models) under SSP
not only converge  but do so at least as fast as cyclic-delay systems [17  1] (and potentially even
faster depending on implementation). Furthermore  our implementation of SSP  SSPtable  supports
a wide variety of algortihms and models  and we demonstrate it on several popular ones: (a) Ma-
trix Factorization with stochastic gradient descent [12]  (b) Topic Modeling with collapsed Gibbs
sampling [2]  and (c) Lasso regression with parallelized coordinate descent [5]. Our experimental
results show that  for these 3 models and algorithms  (i) SSP yields faster convergence than BSP (up
to several times faster)  and (ii) SSP yields faster convergence than a fully asynchronous (i.e. no stal-
eness guarantee) system. We explain SSPtable’s better performance in terms of algorithm progress
per iteration (quality) and iterations executed per unit time (quantity)  and show that SSPtable hits a
“sweet spot” between quality and quantity that is missed by BSP and fully asynchronous systems.
2 Stale Synchronous Parallel Model of Computation
We begin with an informal explanation of SSP: assume a collection of P workers  each of which
makes additive updates to a shared parameter x x + u at regular intervals called clocks. Clocks
are similar to iterations  and represent some unit of progress by an ML algorithm. Every worker

1 For example  the parameter ✓ might be the topic-word distributions in LDA  or the factor matrices in a
matrix decomposition  while the updates  could be adding or removing counts to topic-word or document-word
tables in LDA  or stochastic gradient steps in a matrix decomposition.

2

forced to wait for the slowest worker to catch up.

has its own integer-valued clock c  and workers only commit their updates at the end of each clock.
Updates may not be immediately visible to other workers trying to read x — in other words  workers
only see effects from a “stale” subset of updates. The idea is that  with staleness  workers can retrieve
updates from caches on the same machine (fast) instead of querying the parameter server over the
network (slow). Given a user-chosen staleness threshold s  0  SSP enforces the following bounded
staleness conditions (see Figure 1 for a graphical illustration):
• The slowest and fastest workers must be  s clocks apart — otherwise  the fastest worker is
• When a worker with clock c commits an update u  that u is timestamped with time c.
• When a worker with clock c reads x  it will always see effects from all u with timestamp 
c  s  1. It may also see some u with timestamp > c  s  1 from other workers.
• Read-my-writes: A worker p will always see the effects of its own updates up.
Since the fastest and slowest workers are
 s clocks apart  a worker reading x at
clock c will see all updates with times-
tamps in [0  c  s  1]  plus a (possi-
bly empty) “adaptive” subset of updates in
the range [c  s  c + s  1]. Note that
when s = 0  the “guaranteed” range be-
comes [0  c  1] while the adaptive range
becomes empty  which is exactly the Bulk
Synchronous Parallel model of computa-
tion. Let us look at how SSP applies to an
example ML algorithm.
2.1 An example: Stochastic Gradient Descent for Matrix Problems
The Stochastic Gradient Descent (SGD) [17  12] algorithm optimizes an objective function by ap-
plying gradient descent to random subsets of the data. Consider a matrix completion task  which
involves decomposing an N ⇥ M matrix D into two low-rank matrices LR ⇡ D  where L  R have
sizes N ⇥ K and K ⇥ M (for a user-speciﬁed K). The data matrix D may have missing entries 
corresponding to missing data. Concretely  D could be a matrix of users against products  with Dij
representing user i’s rating of product j. Because users do not rate all possible products  the goal is
to predict ratings for missing entries Dab given known entries Dij. If we found low-rank matrices
L  R such that Li· · R·j ⇡ Dij for all known entries Dij  we could then predict Dab = La· · R·b for
unknown entries Dab.
To perform the decomposition  let us minimize the squared difference between each known entry
Dij and its prediction Li· · R·j (note that other loss functions and regularizers are also possible):

Figure 1: Bounded Staleness under the SSP Model

.

(1)

min

L R X(i j)2Data

Dij 

KXk=1

2

LikRkj

= X(a b)2Data

As a ﬁrst step towards SGD  consider solving Eq (1) using coordinate gradient descent on L  R:
@OMF
@Lik

(a = i) [2DabRkb + 2La·R·bRkb]  

(b = j) [2DabLak + 2La·R·bLak]

@OMF
@Rkj

= X(a b)2Data

where OMF is the objective in Eq(1)  and (a = i) equals 1 if a = i  and 0 otherwise. This can be
transformed into an SGD algorithm by replacing the full sum over entries (a  b) with a subsample
(with appropriate reweighting). The entries Dab can then be distributed over multiple workers  and
their gradients computed in parallel [12].
We assume that D is “tall”  i.e. N > M (or transpose D so this is true)  and partition the rows of
D and L over the processors. Only R needs to be shared among all processors  so we let it be the
SSP shared parameter x := R. SSP allows many workers to read/write to R with minimal waiting 
though the workers will only see stale values of R. This tradeoff is beneﬁcial because without
staleness  the workers must wait for a long time when reading R from the server (as our experiments
will show). While having stale values of R decreases convergence progress per iteration  SSP more
than makes up by enabling signiﬁcantly more iterations per minute  compared to fully synchronous
systems. Thus  SSP yields more convergence progress per minute  i.e. faster convergence.

3

Clock 0 1 2 3 4 5 6 7 8 9 SSP: Bounded Staleness and Clocks Updates visible to all workers Worker 1 Worker 2 Worker 3 Worker 4 Staleness Threshold 3 Updates visible to Worker 1  due to read-my-writes Updates not necessarily visible to Worker 1 Here  Worker 1 must wait on further reads  until Worker 2 has reached clock 4 Worker progress Client process

Table server
Table server
Table server
Table server
Table data

Pending 
requests

Application
Application
thread
Application
thread
Application
thread
thread

Thread
Thread
cache
Thread
cache
Thread
cache
cache

Process
cache

Figure 2: Cache structure of SSPtable  with
multiple server shards

Note that SSP is not limited to stochastic gradient matrix algorithms: it can also be applied to parallel
collapsed sampling on topic models [2] (by storing the word-topic and document-topic tables in x) 
parallel coordinate descent on Lasso regression [5] (by storing the regression coefﬁcients  in x)  as
well as any other parallel algorithm or model with shared parameters that all workers need read/write
access to. Our experiments will show that SSP performs better than bulk synchronous parallel and
asynchronous systems for matrix completion  topic modeling and Lasso regression.
3 SSPtable: an Efﬁcient SSP System
An ideal SSP implementation would fully exploit the lee-
way granted by the SSP’s bounded staleness property 
in order to balance the time workers spend waiting on
reads with the need for freshness in the shared data. This
section describes our initial implementation of SSPtable 
which is a parameter server conforming to the SSP model 
and that can be run on many server machines at once (dis-
tributed). Our experiments with this SSPtable implemen-
tation shows that SSP can indeed improve convergence
rates for several ML models and algorithms  while fur-
ther tuning of cache management policies could further
improve the performance of SSPtable.
SSPtable follows a distributed client-server architecture. Clients access shared parameters using a
client library  which maintains a machine-wide process cache and optional per-thread2 thread caches
(Figure 2); the latter are useful for improving performance  by reducing inter-thread synchronization
(which forces workers to wait) when a client ML program executes multiple worker threads on each
of multiple cores of a client machine. The server parameter state is divided (sharded) over multiple
server machines  and a normal conﬁguration would include a server process on each of the client
machines. Programming with SSPtable follows a simple table-based API for reading/writing to
shared parameters x (for example  the matrix R in the SGD example of Section 2.1):
• Table Organization: SSPtable supports an unlimited number of tables  which are divided into
• read row(table row s): Retrieve a table-row with staleness threshold s. The user can
• inc(table row el val): Increase a table-row-element by val  which can be negative.
• clock(): Inform all servers that the current thread/processor has completed one clock  and
Any number of read row() and inc() calls can be made in-between calls to clock(). Differ-
ent thread workers are permitted to be at different clocks  however  bounded staleness requires that
the fastest and slowest threads be no more than s clocks apart. In this situation  SSPtable forces the
fastest thread to block (i.e. wait) on calls to read row()  until the slowest thread has caught up.
To maintain the “read-my-writes” property  we use a write-back policy: all writes are immediately
committed to the thread caches  and are ﬂushed to the process cache and servers upon clock().
To maintain bounded staleness while minimizing wait times on read row() operations  SSPtable
uses the following cache protocol: Let every table-row in a thread or process cache be endowed
with a clock rthread or rproc respectively. Let every thread worker be endowed with a clock c  equal
to the number of times it has called clock(). Finally  deﬁne the server clock cserver to be the
minimum over all thread clocks c. When a thread with clock c requests a table-row  it ﬁrst checks
its thread cache. If the row is cached with clock rthread  c  s  then it reads the row. Otherwise 
it checks the process cache next — if the row is cached with clock rproc  c  s  then it reads the
row. At this point  no network trafﬁc has been incurred yet. However  if both caches miss  then a
network request is sent to the server (which forces the thread to wait for a reply). The server returns
its view of the table-row as well as the clock cserver. Because the fastest and slowest threads can
be no more than s clocks apart  and because a thread’s updates are sent to the server whenever it
calls clock()  the returned server view always satisﬁes the bounded staleness requirements for the

rows  which are further subdivided into elements. These tables are used to store x.

These changes are not propagated to the servers until the next call to clock().

commit all outstanding inc()s to the servers.

then query individual row elements.

2 We assume that every computation thread corresponds to one ML algorithm worker.

4

asking thread. After fetching a row from the server  the corresponding entry in the thread/process
caches and the clocks rthread  rproc are then overwritten with the server view and clock cserver.
A beneﬁcial consequence of this cache protocol is that the slowest thread only performs costly server
reads every s clocks. Faster threads may perform server reads more frequently  and as frequently as
every clock if they are consistently waiting for the slowest thread’s updates. This distinction in work
per thread does not occur in BSP  wherein every thread must read from the server on every clock.
Thus  SSP not only reduces overall network trafﬁc (thus reducing wait times for all server reads)  but
also allows slow  straggler threads to avoid server reads in some iterations. Hence  the slow threads
naturally catch up — in turn allowing fast threads to proceed instead of waiting for them. In this
manner  SSP maximizes the time each machine spends on useful computation  rather than waiting.
4 Theoretical Analysis of SSP
Formally  the SSP model supports operations x x  (z · y)  where x  y are members of a ring
with an abelian operator  (such as addition)  and a multiplication operator · such that z · y = y0
where y0 is also in the ring. In the context of ML  we shall focus on addition and multiplication
over real vectors x  y and scalar coefﬁcients z  i.e. x x + (zy); such operations can be found
in the update equations of many ML inference algorithms  such as gradient descent [12]  coordinate
descent [5] and collapsed Gibbs sampling [2]. In what follows  we shall informally refer to x as the
“system state”  u = zy as an “update”  and to the operation x x + u as “writing an update”.
We assume that P workers write updates at regular time intervals (referred to as “clocks”). Let up c
be the update written by worker p at clock c through the write operation x x + up c. The updates
up c are a function of the system state x  and under the SSP model  different workers will “see”
different  noisy versions of the true state x. Let ˜xp c be the noisy state read by worker p at clock c 
implying that up c = G(˜xp c) for some function G. We now formally re-state bounded staleness 
which is the key SSP condition that bounds the possible values ˜xp c can take:
SSP Condition (Bounded Staleness): Fix a staleness s. Then  the noisy state ˜xp c is equal to

˜xp c = x0 +24
|

cs1Xc0=1

+

up0 c035
}

PXp0=1
{z

24
|

up c035
c1Xc0=cs
}
{z

+24 X(p0 c0)2Sp c
{z
|

up0 c035
}

 

(2)

guaranteed pre-window updates

guaranteed read-my-writes updates

best-effort in-window updates

where Sp c ✓W p c = ([1  P ] \ {p}) ⇥ [c  s  c + s  1] is some subset of the updates u written
in the width-2s “window” Wp c  which ranges from clock c  s to c + s  1 and does not include
updates from worker p. In other words  the noisy state ˜xp c consists of three parts:

updates made by the querying worker3 p.

1. Guaranteed “pre-window” updates from clock 0 to c  s  1  over all workers.
2. Guaranteed “read-my-writes” set {(p  c  s)  . . .   (p  c  1)} that covers all “in-window”
3. Best-effort “in-window” updates Sp c from the width-2s window4 [c  s  c + s  1] (not
counting updates from worker p). An SSP implementation should try to deliver as many
updates from Sp c as possible  but may choose not to depending on conditions.

Notice that Sp c is speciﬁc to worker p at clock c; other workers at different clocks will observe
different S. Also  observe that SSP generalizes the Bulk Synchronous Parallel (BSP) model:
BSP Corollary: Under zero staleness s = 0  SSP reduces to BSP. Proof: s = 0 implies [c  c +
s  1] = ;  and therefore ˜xp c exactly consists of all updates until clock c  1. ⇤
Our key tool for convergence analysis is to deﬁne a reference sequence of states xt  informally
referred to as the “true” sequence (this is different and unrelated to the SSPtable server’s view):

xt = x0 +

ut0 

where ut := ut mod P  bt/Pc.

tXt0=0

In other words  we sum updates by ﬁrst looping over workers (t mod P )  then over clocks bt/Pc.
We can now bound the difference between the “true” sequence xt and the noisy views ˜xp c:
3 This is a “read-my-writes” or self-synchronization property  i.e. workers will always see any updates they

make. Having such a property makes sense because self-synchronization does not incur a network cost.

4 The width 2s is only an upper bound for the slowest worker. The fastest worker with clock cmax has a
width-s window [cmax  s  cmax  1]  simply because no updates for clocks  cmax have been written yet.

5

 

(3)

Lemma 1: Assume s  1  and let ˜xt := ˜xt mod P  bt/Pc  so that
+"Xi2Bt
ui#
}
{z
|

˜xt = xt  "Xi2At
{z
|

ui#
}

missing updates

extra updates

where we have decomposed the difference between ˜xt and xt into At  the index set of updates ui
that are missing from ˜xt (w.r.t. xt)  and Bt  the index set of “extra” updates in ˜xt but not in xt. We
then claim that |At| + |Bt| 2s(P  1)  and furthermore  min(At [B t)  max(1  t  (s + 1)P ) 
and max(At [B t)  t + sP .
Proof: Comparing Eq.
(3) with (2)  we see that the extra updates obey Bt ✓S t mod P  bt/Pc 
while the missing updates obey At ✓ (Wt mod P  bt/Pc \St mod P  bt/Pc). Because |Wt mod P  bt/Pc| =
2s(P  1)  the ﬁrst claim immediately follows. The second and third claims follow from looking at
the left- and right-most boundaries of Wt mod P  bt/Pc. ⇤
Lemma 1 basically says that the “true” state xt and the noisy state ˜xt only differ by at most 2s(P1)
updates ut  and that these updates cannot be more than (s+1)P steps away from t. These properties
can be used to prove convergence bounds for various algorithms; in this paper  we shall focus on
stochastic gradient descent SGD [17]:
Theorem 1 (SGD under SSP): Suppose we want to ﬁnd the minimizer x⇤ of a convex function
t=1 ft(x)  via gradient descent on one component rft at a time. We assume the
f (x) = 1
components ft are also convex. Let ut := ⌘trft(˜xt)  where ⌘t = pt with  =
for
certain constants F  L. Then  under suitable conditions (ft are L-Lipschitz and the distance between
two points D(xkx0)  F 2) 

T PT

Lp2(s+1)P

F

R[X] :=" 1

T

TXt=1

ft(˜xt)#  f (x⇤)  4F Lr 2(s + 1)P

T

This means that the noisy worker views ˜xt converge in expectation to the true view x⇤ (as measured
by the function f ()  and at rate O(T 1/2)). We defer the proof to the appendix  noting that it
generally follows the analysis in Langford et al. [17]  except in places where Lemma 1 is involved.
Our bound is also similar to [17]  except that (1) their ﬁxed delay ⌧ has been replaced by our
staleness upper bound 2(s + 1)P   and (2) we have shown convergence of the noisy worker views
˜xt rather than a true sequence xt. Furthermore  because the constant factor 2(s + 1)P is only an
upper bound to the number of erroneous updates  SSP’s rate of convergence has a potentially tighter
constant factor than Langford et al.’s ﬁxed staleness system (details are in the appendix).
5 Experiments
We show that the SSP model outperforms fully-synchronous models such as Bulk Synchronous
Parallel (BSP) that require workers to wait for each other on every iteration  as well as asynchronous
models with no model staleness guarantees. The general experimental details are:
• Computational models and implementation: SSP  BSP and Asynchronous5. We used SSPtable for the
ﬁrst two (BSP is just staleness 0 under SSP)  and implemented the Asynchronous model using many of the
caching features of SSPtable (to keep the implementations comparable).

• ML models (and parallel algorithms): LDA Topic Modeling (collapsed Gibbs sampling)  Matrix Fac-
torization (stochastic gradient descent) and Lasso regression (coordinate gradient descent). All algorithms
were implemented using SSPtable’s parameter server interface. For TM and MF  we ran the algorithms in a
“full batch” mode (where the algorithm’s workers collectively touch every data point once per clock()) 
as well as a “10% minibatch” model (workers touch 10% of the data per clock()). Due to implementa-
tion limitations  we did not run Lasso under the Async model.

• Datasets: Topic Modeling: New York Times (N = 100m tokens  V = 100k terms  K = 100 topics) 
Matrix Factorization: NetFlix (480k-by-18k matrix with 100m nonzeros  rank K = 100 decomposition) 
Lasso regression: Synthetic dataset (N = 500 samples with P = 400k features6). We use a static data
partitioning strategy explained in the Appendix.

• Compute cluster: Multi-core blade servers connected by 10 Gbps Ethernet  running VMware ESX. We
use one virtual machine (VM) per physical machine. Each VM is conﬁgured with 8 cores (either 2.3GHz
or 2.5GHz each) and 23GB of RAM  running on top of Debian Linux 7.0.
5 The Asynchronous model is used in many ML frameworks  such as YahooLDA [2] and HogWild! [21].
6This is the largest data size we could get the Lasso algorithm to converge on  under ideal BSP conditions.

6

Convergence Speed. Figure 3 shows objective vs. time plots for the three ML algorithms  over
several machine conﬁgurations. We are interested in how long each algorithm takes to reach a given
objective value  which corresponds to drawing horizontal lines on the plots. On each plot  we show
curves for BSP (zero staleness)  Async  and SSP for the best staleness value  1 (we generally
omit the other SSP curves to reduce clutter). In all cases except Topic Modeling with 8 VMs  SSP
converges to a given objective value faster than BSP or Async. The gap between SSP and the other
systems increases with more VMs and smaller data batches  because both of these factors lead to
increased network communication — which SSP is able to reduce via staleness. We also provide a
scalability-with-N-machines plot in the Appendix.
Computation Time vs Network Waiting Time. To understand why SSP performs better  we look
at how the Topic Modeling (TM) algorithm spends its time during a ﬁxed number of clock()s. In
the 2nd row of Figure 3  we see that for any machine conﬁguration  the TM algorithm spends roughly
the same amount of time on useful computation  regardless of the staleness value. However  the time
spent waiting for network communication drops rapidly with even a small increase in staleness 
allowing SSP to execute clock()s more quickly than BSP (staleness 0). Furthermore  the ratio of
network-to-compute time increases as we add more VMs  or use smaller data batches. At 32 VMs
and 10% data minibatches  the TM algorithm under BSP spends six times more time on network
communications than computation. In contrast  the optimal value of staleness  32  exhibits a 1:1
ratio of communication to computation. Hence  the value of SSP lies in allowing ML algorithms
to perform far more useful computations per second  compared to the BSP model (e.g. Hadoop).
Similar observations hold for the MF and Lasso applications (graphs not shown for space reasons).
Iteration Quantity and Quality. The network-compute ratio only partially explains SSP’s behav-
ior; we need to examine each clock()’s behavior to get a full picture. In the 3rd row of Figure 3 
we plot the number of clocks executed per worker per unit time for the TM algorithm  as well as
the objective value at each clock. Higher staleness values increase the number of clocks executed
per unit time  but decrease each clock’s progress towards convergence (as suggested by our theory);
MF and Lasso also exhibit similar behavior (graphs not shown). Thus  staleness is a tradeoff be-
tween iteration quantity and quality — and because the iteration rate exhibits diminishing returns
with higher staleness values  there comes a point where additional staleness starts to hurt the rate of
convergence per time. This explains why the best staleness value in a given setting is some constant
0 < s < 1 — hence  SSP can hit a “sweet spot” between quality/quantity that BSP and Async do
not achieve. Automatically ﬁnding this sweet spot for a given problem is a subject for future work.
6 Related Work and Discussion
The idea of staleness has been explored before: in ML academia  it has been analyzed in the con-
text of cyclic-delay architectures [17  1]  in which machines communicate with a central server (or
each other) under a ﬁxed schedule (and hence ﬁxed staleness). Even the bulk synchronous paral-
lel (BSP) model inherently produces stale communications  the effects of which have been studied
for algorithms such as Lasso regression [5] and topic modeling [2]. Our work differs in that SSP
advocates bounded (rather than ﬁxed) staleness to allow higher computational throughput via local
machine caches. Furthermore  SSP’s performance does not degrade when parameter updates fre-
quently collide on the same vector elements  unlike asynchronous lock-free systems [21]. We note
that staleness has been informally explored in the industrial setting at large scales; our work provides
a ﬁrst attempt at rigorously justifying staleness as a sound ML technique.
Distributed platforms such as Hadoop and GraphLab [18] are popular for large-scale ML. The
biggest difference between them and SSPtable is the programming model — Hadoop uses a stateless
map-reduce model  while GraphLab uses stateful vertex programs organized into a graph. In con-
trast  SSPtable provides a convenient shared-memory programming model based on a table/matrix
API  making it easy to convert single-machine parallel ML algorithms into distributed versions. In
particular  the algorithms used in our experiments — LDA  MF  Lasso — are all straightforward
conversions of single-machine algorithms. Hadoop’s BSP execution model is a special case of SSP 
making SSPtable more general in that regard; however  Hadoop also provides fault-tolerance and
distributed ﬁlesystem features that SSPtable does not cover. Finally  there exist special-purpose
tools such as Vowpal Wabbit [16] and YahooLDA [2]. Whereas these systems have been targeted at
a subset of ML algorithms  SSPtable can be used by any ML algorithm that tolerates stale updates.
The distributed systems community has typically examined staleness in the context of consistency
models. The TACT model [26] describes consistency along three dimensions: numerical error  order
error  and staleness. Other work [24] attempts to classify existing systems according to a number

7

8 VMs

Topic Modeling: Convergence

32 VMs

32 VMs  10% minibatches

Topic Modeling: Computation Time vs Network Waiting Time
8 VMs

32 VMs

32 VMs  10% minibatches

Topic Modeling: Iteration Quantity and Quality

32 VMs  10% minibatches

32 VMs  10% minibatches

Lasso: Convergence

16 VMs

8 VMs

Matrix Factorization: Convergence

32 VMs

32 VMs  10% minibatches

Figure 3: Experimental results: SSP  BSP and Asynchronous parameter servers running Topic Modeling 
Matrix Factorization and Lasso regression. The Convergence graphs plot objective function (i.e. solution
quality) against time. For Topic Modeling  we also plot computation time vs network waiting time  as well as
how staleness affects iteration (clock) frequency (Quantity) and objective improvement per iteration (Quality).
of consistency properties  speciﬁcally naming the concept of bounded staleness. The vector clocks
used in SSPtable are similar to those in Fidge [11] and Mattern [20]  which were in turn inspired
by Lamport clocks [15]. However  SSPtable uses vector clocks to track the freshness of the data 
rather than causal relationships between updates. [8] gives an informal deﬁnition of the SSP model 
motivated by the need to reduce straggler effects in large compute clusters.
In databases  bounded staleness has been applied to improve update and query performance. Lazy-
Base [7] allows staleness bounds to be conﬁgured on a per-query basis  and uses this relaxed stale-
ness to improve both query and update performance. FAS [23] keeps data replicated in a number of
databases  each providing a different freshness/performance tradeoff. Data stream warehouses [13]
collect data about timestamped events  and provide different consistency depending on the freshness
of the data. Staleness (or freshness/timeliness) has also been applied in other ﬁelds such as sensor
networks [14]  dynamic web content generation [3]  web caching [6]  and information systems [4].
Acknowledgments
Qirong Ho is supported by an NSS-PhD Fellowship from A-STAR  Singapore. This work is supported in
part by NIH 1R01GM087694 and 1R01GM093156  DARPA FA87501220324  and NSF IIS1111142 to Eric
P. Xing. We thank the member companies of the PDL Consortium (Actiﬁo  APC  EMC  Emulex  Facebook 
Fusion-IO  Google  HP  Hitachi  Huawei  Intel  Microsoft  NEC  NetApp  Oracle  Panasas  Samsung  Seagate 
Symantec  VMware  Western Digital) for their interest  insights  feedback  and support. This work is supported
in part by Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC) and hardware
donations from Intel and NetApp.

8

-1.30E+09 -1.25E+09 -1.20E+09 -1.15E+09 -1.10E+09 -1.05E+09 -1.00E+09 -9.50E+08 -9.00E+08 0 500 1000 1500 2000 Log-Likelihood Seconds Objective function versus time LDA 8 machines (64 threads)  Full data per iter BSP (stale 0) stale 2 async -1.30E+09 -1.25E+09 -1.20E+09 -1.15E+09 -1.10E+09 -1.05E+09 -1.00E+09 -9.50E+08 -9.00E+08 0 500 1000 1500 2000 Log-Likelihood Seconds Objective function versus time LDA 32 machines (256 threads)  Full data per iter BSP (stale 0) stale 4 async -1.30E+09 -1.25E+09 -1.20E+09 -1.15E+09 -1.10E+09 -1.05E+09 -1.00E+09 -9.50E+08 -9.00E+08 0 500 1000 1500 2000 Log-Likelihood Seconds Objective function versus time LDA 32 machines (256 threads)  10% data per iter BSP (stale 0) stale 32 async 0 200 400 600 800 1000 1200 1400 1600 1800 0 2 4 16 32 48 Seconds Staleness Time Breakdown: Compute vs Network LDA 8 machines  Full data Network waiting time Compute time 0 500 1000 1500 2000 2500 3000 3500 4000 0 2 4 6 8 Seconds Staleness Time Breakdown: Compute vs Network LDA 32 machines  Full data Network waiting time Compute time 0 1000 2000 3000 4000 5000 6000 7000 8000 0 8 16 24 32 40 48 Seconds Staleness Time Breakdown: Compute vs Network LDA 32 machines  10% data Network waiting time Compute time 0 100 200 300 400 500 600 700 800 900 1000 0 2000 4000 6000 8000 Iterations (clocks) Seconds Quantity: iterations versus time LDA 32 machines  10% data BSP (stale 0) stale 8 stale 16 stale 24 stale 32 stale 40 stale 48 -1.30E+09 -1.25E+09 -1.20E+09 -1.15E+09 -1.10E+09 -1.05E+09 -1.00E+09 -9.50E+08 -9.00E+08 0 200 400 600 800 1000 Log-Likelihood Iterations (clocks) Quality: objective versus iterations LDA 32 machines  10% data BSP (stale 0) stale 8 stale 16 stale 24 stale 32 stale 40 stale 48 4.20E-01 4.30E-01 4.40E-01 4.50E-01 4.60E-01 4.70E-01 4.80E-01 0 500 1000 1500 2000 2500 3000 3500 4000 Objective Seconds Objective function versus time Lasso 16 machines (128 threads) BSP (stale 0) stale 10 stale 20 stale 40 stale 80 1.40E+09 1.60E+09 1.80E+09 2.00E+09 2.20E+09 2.40E+09 2.60E+09 0 1000 2000 3000 4000 5000 6000 7000 8000 Objective Seconds Objective function versus time MF 8 machines (64 threads)  Full data per iter BSP (stale 0) stale 4 async 1.60E+09 1.70E+09 1.80E+09 1.90E+09 2.00E+09 2.10E+09 2.20E+09 0 1000 2000 3000 4000 5000 6000 7000 8000 Objective Seconds Objective function versus time MF 32 machines (256 threads)  Full data per iter BSP (stale 0) stale 15 async 1.60E+09 1.70E+09 1.80E+09 1.90E+09 2.00E+09 2.10E+09 2.20E+09 0 1000 2000 3000 4000 5000 6000 7000 8000 Objective Seconds Objective function versus time MF 32 machines (256 threads)  10% data per iter BSP (stale 0) stale 32 async References
[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Decision and Control (CDC) 

2012 IEEE 51st Annual Conference on  pages 5451–5452. IEEE  2012.

[2] A. Ahmed  M. Aly  J. Gonzalez  S. Narayanamurthy  and A. J. Smola. Scalable inference in latent variable

models. In WSDM  pages 123–132  2012.

[3] N. R. Alexandros Labrinidis. Balancing performance and data freshness in web database servers. pages

pp. 393 – 404  September 2003.

[4] M. Bouzeghoub. A framework for analysis of data freshness. In Proceedings of the 2004 international

workshop on Information quality in information systems  IQIS ’04  pages 59–67  2004.

[5] J. K. Bradley  A. Kyrola  D. Bickson  and C. Guestrin. Parallel coordinate descent for l1-regularized loss

minimization. In International Conference on Machine Learning (ICML 2011)  June 2011.

[6] L. Bright and L. Raschid. Using latency-recency proﬁles for data delivery on the web. In Proceedings of

the 28th international conference on Very Large Data Bases  VLDB ’02  pages 550–561  2002.
[7] J. Cipar  G. Ganger  K. Keeton  C. B. Morrey  III  C. A. Soules  and A. Veitch. LazyBase:

trading
freshness for performance in a scalable database. In Proceedings of the 7th ACM european conference on
Computer Systems  pages 169–182  2012.

[8] J. Cipar  Q. Ho  J. K. Kim  S. Lee  G. R. Ganger  G. Gibson  K. Keeton  and E. Xing. Solving the straggler

problem with bounded staleness. In HotOS ’13. Usenix  2013.

[9] J. Dean  G. Corrado  R. Monga  K. Chen  M. Devin  Q. Le  M. Mao  M. Ranzato  A. Senior  P. Tucker 

K. Yang  and A. Ng. Large scale distributed deep networks. In NIPS 2012  2012.

[10] Facebook. www.facebook.com/note.php?note_id=10150388519243859  January 2013.
[11] C. J. Fidge. Timestamps in Message-Passing Systems that Preserve the Partial Ordering. In 11th Aus-

tralian Computer Science Conference  pages 55–66  University of Queensland  Australia  1988.

[12] R. Gemulla  E. Nijkamp  P. J. Haas  and Y. Sismanis. Large-scale matrix factorization with distributed

stochastic gradient descent. In KDD  pages 69–77. ACM  2011.

[13] L. Golab and T. Johnson. Consistency in a stream warehouse. In CIDR 2011  pages 114–122.
[14] C.-T. Huang. Loft: Low-overhead freshness transmission in sensor networks.

In SUTC 2008  pages

241–248  Washington  DC  USA  2008. IEEE Computer Society.

[15] L. Lamport. Time  clocks  and the ordering of events in a distributed system. Commun. ACM  21(7):558–

565  July 1978.

[16] J. Langford  L. Li  and A. Strehl. Vowpal wabbit online learning project  2007.
[17] J. Langford  A. J. Smola  and M. Zinkevich. Slow learners are fast. In Advances in Neural Information

Processing Systems  pages 2331–2339  2009.

[18] Y. Low  G. Joseph  K. Aapo  D. Bickson  C. Guestrin  and M. Hellerstein  Joseph. Distributed GraphLab:

A framework for machine learning and data mining in the cloud. PVLDB  2012.

[19] G. Malewicz  M. H. Austern  A. J. Bik  J. C. Dehnert  I. Horn  N. Leiser  and G. Czajkowski. Pregel: a
system for large-scale graph processing. In Proceedings of the 2010 International Conference on Man-
agement of Data  pages 135–146. ACM  2010.

[20] F. Mattern. Virtual time and global states of distributed systems. In C. M. et al.  editor  Proc. Workshop

on Parallel and Distributed Algorithms  pages 215–226  North-Holland / Elsevier  1989.

[21] F. Niu  B. Recht  C. R´e  and S. J. Wright. Hogwild!: A lock-free approach to parallelizing stochastic

gradient descent. In NIPS  2011.

[22] R. Power and J. Li. Piccolo: building fast  distributed programs with partitioned tables. In Proceedings
of the USENIX conference on Operating systems design and implementation (OSDI)  pages 1–14  2010.
[23] U. R¨ohm  K. B¨ohm  H.-J. Schek  and H. Schuldt. Fas: a freshness-sensitive coordination middleware for

a cluster of olap components. In VLDB 2002  pages 754–765. VLDB Endowment  2002.

[24] D. Terry. Replicated data consistency explained through baseball. Technical Report MSR-TR-2011-137 

Microsoft Research  October 2011.

[25] Yahoo! http://webscope.sandbox.yahoo.com/catalog.php?datatype=g  2013.
[26] H. Yu and A. Vahdat. Design and evaluation of a conit-based continuous consistency model for replicated

services. ACM Transactions on Computer Systems  20(3):239–282  Aug. 2002.

[27] M. Zaharia  M. Chowdhury  M. J. Franklin  S. Shenker  and I. Stoica. Spark: cluster computing with

working sets. In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing  2010.

[28] M. Zinkevich  M. Weimer  A. Smola  and L. Li. Parallelized stochastic gradient descent. Advances in

Neural Information Processing Systems  23(23):1–9  2010.

9

,Qirong Ho
James Cipar
Henggang Cui
Seunghak Lee
Jin Kyu Kim
Phillip B. Gibbons
Garth Gibson
Greg Ganger
Eric Xing