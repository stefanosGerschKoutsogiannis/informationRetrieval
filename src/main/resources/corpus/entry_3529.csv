2013,Moment-based Uniform Deviation Bounds for $k$-means and Friends,Suppose $k$ centers are fit to $m$ points by heuristically minimizing the $k$-means cost; what is the corresponding fit over the source distribution?  This question is resolved here for distributions with $p\geq 4$ bounded moments; in particular  the difference between the sample cost and distribution cost decays with $m$ and $p$ as $m^{\min\{-1/4  -1/2+2/p\}}$.  The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets  cost functions  and source distributions.  To further demonstrate this mechanism  a soft clustering variant of $k$-means cost is also considered  namely the log likelihood of a Gaussian mixture  subject to the constraint that all covariance matrices have bounded spectrum.  Lastly  a rate with refined constants is provided for $k$-means instances possessing some cluster structure.,Moment-based Uniform Deviation Bounds for

k-means and Friends

Matus Telgarsky

Sanjoy Dasgupta

Computer Science and Engineering  UC San Diego

{mtelgars dasgupta}@cs.ucsd.edu

Abstract

Suppose k centers are ﬁt to m points by heuristically minimizing the k-means
cost; what is the corresponding ﬁt over the source distribution? This question is
resolved here for distributions with p  4 bounded moments; in particular  the
difference between the sample cost and distribution cost decays with m and p as
mmin{1/4 1/2+2/p}. The essential technical contribution is a mechanism to uni-
formly control deviations in the face of unbounded parameter sets  cost functions 
and source distributions. To further demonstrate this mechanism  a soft clustering
variant of k-means cost is also considered  namely the log likelihood of a Gaus-
sian mixture  subject to the constraint that all covariance matrices have bounded
spectrum. Lastly  a rate with reﬁned constants is provided for k-means instances
possessing some cluster structure.

1

Introduction

i=1 is selected by approximate minimization of k-means cost; how
Suppose a set of k centers {pi}k
does the ﬁt over the sample compare with the ﬁt over the distribution? Concretely: given m points
sampled from a source distribution ⇢  what can be said about the quantities

1
m


mXj=1

1
m

mXj=1
ln kXi=1

i kxj  pik2
min

2 Z min
↵ip✓i(xj)! Z ln kXi=1



i kx  pik2

2d⇢(x)
↵ip✓i(x)! d⇢(x)

(k-means) 

(1.1)

(soft k-means) 

(1.2)

where each p✓i denotes the density of a Gaussian with a covariance matrix whose eigenvalues lie in
some closed positive interval.
The literature offers a wealth of information related to this question. For k-means  there is ﬁrstly a
consistency result: under some identiﬁability conditions  the global minimizer over the sample will
converge to the global minimizer over the distribution as the sample size m increases [1]. Further-
more  if the distribution is bounded  standard tools can provide deviation inequalities [2  3  4]. For
the second problem  which is maximum likelihood of a Gaussian mixture (thus amenable to EM
[5])  classical results regarding the consistency of maximum likelihood again provide that  under
some identiﬁability conditions  the optimal solutions over the sample converge to the optimum over
the distribution [6].
The task here is thus: to provide ﬁnite sample guarantees for these problems  but eschewing bound-
edness  subgaussianity  and similar assumptions in favor of moment assumptions.

1

1.1 Contribution

The results here are of the following form: given m examples from a distribution with a few bounded
moments  and any set of parameters beating some ﬁxed cost c  the corresponding deviations in cost
(as in eq. (1.1) and eq. (1.2)) approach O(m1/2) with the availability of higher moments.

• In the case of k-means (cf. Corollary 3.1)  p  4 moments sufﬁce  and the rate is
O(mmin{1/4 1/2+2/p}). For Gaussian mixtures (cf. Theorem 5.1)  p  8 moments
sufﬁce  and the rate is O(m1/2+3/p).
• The parameter c allows these guarantees to hold for heuristics. For instance  suppose k
centers are output by Lloyd’s method. While Lloyd’s method carries no optimality guar-
antees  the results here hold for the output of Lloyd’s method simply by setting c to be the
variance of the data  equivalently the k-means cost with a single center placed at the mean.
• The k-means and Gaussian mixture costs are only well-deﬁned when the source distribu-
tion has p  2 moments. The condition of p  4 moments  meaning the variance has a
variance  allows consideration of many heavy-tailed distributions  which are ruled out by
boundedness and subgaussianity assumptions.

The main technical byproduct of the proof is a mechanism to deal with the unboundedness of the
cost function; this technique will be detailed in Section 3  but the difﬁculty and its resolution can be
easily sketched here.
For a single set of centers P   the deviations in eq. (1.1) may be controlled with an application of
Chebyshev’s inequality. But this does not immediately grant deviation bounds on another set of
centers P 0  even if P and P 0 are very close: for instance  the difference between the two costs will
grow as successively farther and farther away points are considered.
The resolution is to simply note that there is so little probability mass in those far reaches that the
cost there is irrelevant. Consider a single center p (and assume x 7! kx  pk2
2 is integrable); the
dominated convergence theorem grants
ZBi kx  pk2
In other words  a ball Bi may be chosen so thatRBc
p0 with kp  p0k2  i. Then
2d⇢(x) ZBc
ZBc

where Bi := {x 2 Rd : kx  pk2  i}.
2d⇢(x)  1/1024. Now consider some

(kx  pk2 + kp  p0k2)2d⇢(x)  4ZBc

2d⇢(x) ! Z kx  pk2

kx  pk2

2d⇢(x) 

kx  p0k2

i

i

i

2d⇢(x) 

i kx  pk2

1
256

.

In this way  a single center may control the outer deviations of whole swaths of other centers. Indeed 
those choices outperforming the reference score c will provide a suitable swath. Of course  it would
be nice to get a sense of the size of Bi; this however is provided by the moment assumptions.
The general strategy is thus to split consideration into outer deviations  and local deviations. The
local deviations may be controlled by standard techniques. To control outer deviations  a single pair
of dominating costs — a lower bound and an upper bound — is controlled.
This technique can be found in the proof of the consistency of k-means due to Pollard [1]. The
present work shows it can also provide ﬁnite sample guarantees  and moreover be applied outside
hard clustering.
The content here is organized as follows. The remainder of the introduction surveys related work 
and subsequently Section 2 establishes some basic notation. The core deviation technique  termed
outer bracketing (to connect it to the bracketing technique from empirical process theory)  is pre-
sented along with the deviations of k-means in Section 3. The technique is then applied in Section 5
to a soft clustering variant  namely log likelihood of Gaussian mixtures having bounded spectra. As
a reprieve between these two heavier bracketing sections  Section 4 provides a simple reﬁnement for
k-means which can adapt to cluster structure.
All proofs are deferred to the appendices  however the construction and application of outer brackets
is sketched in the text.

2

1.2 Related Work

As referenced earlier  Pollard’s work deserves special mention  both since it can be seen as the origin
of the outer bracketing technique  and since it handled k-means under similarly slight assumptions
(just two moments  rather than the four here) [1  7]. The present work hopes to be a spiritual
successor  providing ﬁnite sample guarantees  and adapting technique to a soft clustering problem.
In the machine learning community  statistical guarantees for clustering have been extensively stud-
ied under the topic of clustering stability [4  8  9  10]. One formulation of stability is: if param-
eters are learned over two samples  how close are they? The technical component of these works
frequently involves ﬁnite sample guarantees  which in the works listed here make a boundedness
assumption  or something similar (for instance  the work of Shamir and Tishby [9] requires the cost
function to satisfy a bounded differences condition). Amongst these ﬁnite sample guarantees  the
ﬁnite sample guarantees due to Rakhlin and Caponnetto [4] are similar to the development here after
the invocation of the outer bracket: namely  a covering argument controls deviations over a bounded
set. The results of Shamir and Tishby [10] do not make a boundedness assumption  but the main
results are not ﬁnite sample guarantees; in particular  they rely on asymptotic results due to Pollard
[7].
There are many standard tools which may be applied to the problems here  particularly if a bound-
edness assumption is made [11  12]; for instance  Lugosi and Zeger [2] use tools from VC theory to
handle k-means in the bounded case. Another interesting work  by Ben-david [3]  develops special-
ized tools to measure the complexity of certain clustering problems; when applied to the problems
of the type considered here  a boundedness assumption is made.
A few of the above works provide some negative results and related commentary on the topic of
uniform deviations for distributions with unbounded support [10  Theorem 3 and subsequent discus-
sion] [3  Page 5 above Deﬁnition 2]. The primary “loophole” here is to constrain consideration to
those solutions beating some reference score c. It is reasonable to guess that such a condition en-
tails that a few centers must lie near the bulk of the distribution’s mass; making this guess rigorous
is the ﬁrst step here both for k-means and for Gaussian mixtures  and moreover the same conse-
quence was used by Pollard for the consistency of k-means [1]. In Pollard’s work  only optimal
choices were considered  but the same argument relaxes to arbitrary c  which can thus encapsulate
heuristic schemes  and not just nearly optimal ones. (The secondary loophole is to make moment
assumptions; these sufﬁciently constrain the structure of the distribution to provide rates.)
In recent years  the empirical process theory community has produced a large body of work on the
topic of maximum likelihood (see for instance the excellent overviews and recent work of Wellner
[13]  van der Vaart and Wellner [14]  Gao and Wellner [15]). As stated previously  the choice of the
term “bracket” is to connect to empirical process theory. Loosely stated  a bracket is simply a pair
of functions which sandwich some set of functions; the bracketing entropy is then (the logarithm of)
the number of brackets needed to control a particular set of functions. In the present work  brackets
are paired with sets which identify the far away regions they are meant to control; furthermore 
while there is potential for the use of many outer brackets  the approach here is able to make use of
just a single outer bracket. The name bracket is suitable  as opposed to cover  since the bracketing
elements need not be members of the function class being dominated. (By contrast  Pollard’s use in
the proof of the consistency of k-means was more akin to covering  in that remote ﬂuctuations were
compared to that of a a single center placed at the origin [1].)

2 Notation

The ambient space will always be the Euclidean space Rd  though a few results will be stated for a
general domain X . The source probability measure will be ⇢  and when a ﬁnite sample of size m
is available  ˆ⇢ is the corresponding empirical measure. Occasionally  the variable ⌫ will refer to an
arbitrary probability measure (where ⇢ and ˆ⇢ will serve as relevant instantiations). Both integral and
expectation notation will be used; for example  E(f (X)) = E⇢(f (X) =R f (x)d⇢(x); for integrals 
RB f (x)d⇢(x) =R f (x)1[x 2 B]d⇢(x)  where 1 is the indicator function. The moments of ⇢ are
deﬁned as follows.
Deﬁnition 2.1. Probability measure ⇢ has order-p moment bound M with respect to norm k·k when
E⇢kX  E⇢(X)kl  M for 1  l  p.

3

For example  the typical setting of k-means uses norm k·k 2  and at least two moments are needed for
the cost over ⇢ to be ﬁnite; the condition here of needing 4 moments can be seen as naturally arising
via Chebyshev’s inequality. Of course  the availability of higher moments is beneﬁcial  dropping the
rates here from m1/4 down to m1/2. Note that the basic controls derived from moments  which
are primarily elaborations of Chebyshev’s inequality  can be found in Appendix A.
The k-means analysis will generalize slightly beyond the single-center cost x 7! kx  pk2
Bregman divergences [16  17].
Deﬁnition 2.2. Given a convex differentiable function f : X! R  the corresponding Bregman
divergence is Bf (x  y) := f (x)  f (y)  hrf (y)  x  yi.
Not all Bregman divergences are handled; rather  the following regularity conditions will be placed
on the convex function.
Deﬁnition 2.3. A convex differentiable function f is strongly convex with modulus r1 and has Lip-
schitz gradients with constant r2  both respect to some norm k·k   when f (respectively) satisﬁes

2 via

f (↵x + (1  ↵)y)  ↵f (x) + (1  ↵)f (y) 
krf (x)  rf (y)k⇤  r2kx  yk 

r1↵(1  ↵)

2

kx  yk2 

where x  y 2X   ↵ 2 [0  1]  and k·k ⇤ is the dual of k·k . (The Lipschitz gradient condition is
sometimes called strong smoothness.)

These conditions are a fancy way of saying the corresponding Bregman divergence is sandwiched
between two quadratics (cf. Lemma B.1).
Deﬁnition 2.4. Given a convex differentiable function f : Rd ! R which is strongly convex and
has Lipschitz gradients with respective constants r1  r2 with respect to norm k·k   the hard k-means
cost of a single point x according to a set of centers P is

f (x; P ) := min
p2P

Bf (x  p).

The corresponding k-means cost of a set of points (or distribution) is thus computed as
E⌫(f (X; P ))  and let Hf (⌫; c  k) denote all sets of at most k centers beating cost c  meaning

Hf (⌫; c  k) := {P : |P| k  E⌫(f (X; P ))  c}.

2 (which has r1 = r2 = 2)  the
2  and Eˆ⇢(f (X; P )) denotes the vanilla

For example  choosing norm k·k 2 and convex function f (x) = kxk2
corresponding Bregman divergence is Bf (x  y) = kx  yk2
k-means cost of some ﬁnite point set encoded in the empirical measure ˆ⇢.
The hard clustering guarantees will work with Hf (⌫; c  k)  where ⌫ can be either the source distri-
bution ⇢  or its empirical counterpart ˆ⇢. As discussed previously  it is reasonable to set c to simply
the sample variance of the data  or a related estimate of the true variance (cf. Appendix A).
Lastly  the class of Gaussian mixture penalties is as follows.
Deﬁnition 2.5. Given Gaussian parameters ✓ := (µ  ⌃)  let p✓ denote Gaussian density

p✓(x) =

exp✓

1
2

1

p(2⇡)d|⌃i|

Given Gaussian mixture parameters (↵  ⇥) = ({↵i}k
(written ↵ 2 )  the Gaussian mixture cost at a point x is

g(x; (↵  ⇥)) := g(x;{(↵i ✓ i) = (↵i  µi ⌃ i)}k

i

(x  µi)T ⌃1
i=1 {✓i}k

(x  µi)◆ .
i=1) with ↵  0 andPi ↵i = 1
i=1) := ln kXi=1

↵ip✓i(x)!  

Lastly  given a measure ⌫  bound k on the number of mixture parameters  and spectrum bounds
0 < 1  2  let Smog(⌫; c  k  1  2) denote those mixture parameters beating cost c  meaning
Smog(⌫; c  k  1  2) := {(↵  ⇥) : 1I  ⌃i  2I |↵| k  ↵ 2   E⌫ (g(X; (↵  ⇥)))  c} .
While a condition of the form ⌃ ⌫ 1I is typically enforced in practice (say  with a Bayesian prior 
or by ignoring updates which shrink the covariance beyond this point)  the condition ⌃  2I is
potentially violated. These conditions will be discussed further in Section 5.

4

3 Controlling k-means with an Outer Bracket

First consider the special case of k-means cost.
Corollary 3.1. Set f (x) := kxk2
2  whereby f is the k-means cost. Let real c  0 and probability
measure ⇢ be given with order-p moment bound M with respect to k·k 2  where p  4 is a positive
multiple of 4. Deﬁne the quantities
c1 := (2M )1/p + p2c  M1 := M 1/(p2) + M 2/p  N1 := 2 + 576d(c1 + c2
Then with probability at
the draw of a sample of
max{(p/(2p/4+2e))2  9 ln(1/)}  every set of centers P 2H f (ˆ⇢; c  k) [H f (⇢; c  k) satisﬁes

1 ).
size m 

least 1  3 over

1 + M1 + M 2

Z f (x; P )d⇢(x) Z f (x; P )dˆ⇢(x)
 m1/2+min{1/4 2/p} 4 + (72c2

1 + 32M 2

1 )s 1

2

ln✓ (mN1)dk



◆ +r 2p/4ep
8m1/2 ✓ 2

◆4/p! .

One artifact of the moment approach (cf. Appendix A)  heretofore ignored  is the term (2/)4/p.
While this may seem inferior to ln(2/)  note that the choice p = 4 ln(2/)/ ln(ln(2/)) sufﬁces to
make the two equal.
Next consider a general bound for Bregman divergences. This bound has a few more parameters
than Corollary 3.1. In particular  the term ✏  which is instantiated to m1/2+1/p in the proof of
Corollary 3.1  catches the mass of points discarded due to the outer bracket  as well as the resolution
of the (inner) cover. The parameter p0  which controls the tradeoff between m and 1/  is set to p/4
in the proof of Corollary 3.1.
Theorem 3.2. Fix a reference norm k·k throughout the following. Let probability measure ⇢ be
given with order-p moment bound M where p  4  a convex function f with corresponding constants
r1 and r2  reals c and ✏> 0  and integer 1  p0  p/2  1 be given. Deﬁne the quantities

(M/✏)1/(p2i)  

RB := max⇢(2M )1/p +p4c/r1  max
RC :=pr2/r1⇣(2M )1/p +p4c/r1 + RB⌘ + RB 
B :=x 2 Rd : kx  E(X)k  RB  
C :=x 2 Rd : kx  E(X)k  RC  
2(RB + RC)r2  
⌧ := min⇢r ✏
|N| ✓1 +
⌧ ◆d
least 1  3 over
Cs 1
Z f (x; P )d⇢(x) Z f (x; P )dˆ⇢(x)  4✏+4r2R2

ln✓ 2|N|k

3.1 Compactiﬁcation via Outer Brackets

size m 
Then with probability at
max{p0/(e2p0✏)  9 ln(1/)}  every set of centers P 2H f (⇢; c  k) [H f (ˆ⇢; c  k) satisﬁes
 ◆+r e2p0✏p0
◆1/p0
2m ✓ 2

the draw of a sample of

and let N be a cover of C by k·k -balls with radius ⌧; in the case that k·k is an lp norm  the size of
this cover has bound

 

2r2

2RCd

.

i2[p0]

2m

.

✏

The outer bracket is deﬁned as follows.
Deﬁnition 3.3. An outer bracket for probability measure ⌫ at scale ✏ consists of two triples  one
each for lower and upper bounds.

5

u

` and  2 Z` 

`(x)d⌫(x)| ✏.

`

u(x)d⌫(x)| ✏.

1. The function `  function class Z`  and set B` satisfy two conditions: if x 2 Bc
then `(x)  (x)  and secondly |RBc
2. Similarly  function u  function class Zu  and set Bu satisfy: if x 2 Bc
u(x)  (x)  and secondly |RBc
`(x)d⌫(x) ZBc

Direct from the deﬁnition  given bracketing functions (`  u)  a bracketed function f (·; P )  and the
bracketing set B := Bu [ B` 
✏ ZBc

f (x; P )d⌫(x) ZBc

u and  2 Zu  then

in other words  as intended  this mechanism allows deviations on Bc to be discarded. Thus to
uniformly control the deviations of the dominated functions Z := Zu [ Z` over the set Bc  it
sufﬁces to simply control the deviations of the pair (`  u).
The following lemma shows that a bracket exists for {f (·; P ) : P 2H f (⌫; c  k)} and compact B 
and moreover that this allows sampled points and candidate centers in far reaches to be deleted.
Lemma 3.5. Consider the setting and deﬁnitions in Theorem 3.2  but additionally deﬁne
◆1/p0
M0 := 2p0✏ 
.
The following statements hold with probability at least 1  2 over a draw of size m 
max{p0/(M0e)  9 ln(1/)}.

ˆ⇢ := ✏ +r M0ep0
2m ✓ 2

u(x) := 4r2kx  E(X)k2 ✏

u(x)d⌫(x)  ✏;

(3.4)

`(x) := 0 

1. (u  `) is an outer bracket for ⇢ at scale ✏⇢ := ✏ with sets B` = Bu = B and Z` = Zu =
{f (·; P ) : P 2H f (ˆ⇢; c  k)[H f (⇢; c  k)}  and furthermore the pair (u  `) is also an outer
bracket for ˆ⇢ at scale ✏ˆ⇢ with the same sets.
2. For every P 2H f (ˆ⇢; c  k) [H f (⇢; c  k) 

and

Z f (x; P )d⇢(x) ZB
Z f (x; P )dˆ⇢(x) ZB

f (x; P \ C)d⇢(x)  ✏⇢ = ✏.
f (x; P \ C)dˆ⇢(x)  ✏ˆ⇢.

The proof of Lemma 3.5 has roughly the following outline.

1. Pick some ball B0 which has probability mass at least 1/4. It is not possible for an element
of Hf (ˆ⇢; c  k) [H f (⇢; c  k) to have all centers far from B0  since otherwise the cost is
larger than c. (Concretely  “far from” means at leastp4c/r1 away; note that this term
appears in the deﬁnitions of B and C in Theorem 3.2.) Consequently  at least one center
lies near to B0; this reasoning was also the ﬁrst step in the k-means consistency proof due
to k-means Pollard [1].
2. It is now easy to dominate P 2H f (ˆ⇢; c  k) [H f (⇢; c  k) far away from B0. In particular 
choose any p0 2 B0 \ P   which was guaranteed to exist in the preceding point; since
minp2P Bf (x  p)  Bf (x  p0) holds for all x  it sufﬁces to dominate p0. This domination
proceeds exactly as discussed in the introduction; in fact  the factor 4 appeared there  and
again appears in the u here  for exactly the same reason. Once again  similar reasoning can
be found in the proof by Pollard [1].

3. Satisfying the integral conditions over ⇢ is easy: it sufﬁces to make B huge. To control the
size of B0  as well as the size of B  and moreover the deviations of the bracket over B  the
moment tools from Appendix A are used.

Now turning consideration back to the proof of Theorem 3.2  the above bracketing allows the re-
moval of points and centers outside of a compact set (in particular  the pair of compact sets B and
C  respectively). On the remaining truncated data and set of centers  any standard tool sufﬁces; for
mathematical convenience  and also to ﬁt well with the use of norms in the deﬁnition of moments
as well as the conditions on the convex function f providing the divergence Bf   norm structure
used throughout the other properties  covering arguments are used here. (For details  please see
Appendix B.)

6

4

Interlude: Reﬁned Estimates via Clamping

So far  rates have been given that guarantee uniform convergence when the distribution has a few
moments  and these rates improve with the availability of higher moments. These moment condi-
tions  however  do not necessarily reﬂect any natural cluster structure in the source distribution. The
purpose of this section is to propose and analyze another distributional property which is intended
to capture cluster structure. To this end  consider the following deﬁnition.
Deﬁnition 4.1. Real number R and compact set C are a clamp for probability measure ⌫ and family
of centers Z and cost f at scale ✏> 0 if every P 2 Z satisﬁes

|E⌫(f (X; P ))  E⌫ (min{f (X; P \ C)   R})| ✏.

Note that this deﬁnition is similar to the second part of the outer bracket guarantee in Lemma 3.5 
and  predictably enough  will soon lead to another deviation bound.
Example 4.2. If the distribution has bounded support  then choosing a clamping value R and clamp-
ing set C respectively slightly larger than the support size and set is sufﬁcient: as was reasoned in
the construction of outer brackets  if no centers are close to the support  then the cost is bad. Corre-
spondingly  the clamped set of functions Z should again be choices of centers whose cost is not too
high.
For a more interesting example  suppose ⇢ is supported on k small balls of radius R1  where the
distance between their respective centers is some R2  R1. Then by reasoning similar to the
bounded case  all choices of centers achieving a good cost will place centers near to each ball  and
⌅
thus the clamping value can be taken closer to R1.
Of course  the above gave the existence of clamps under favorable conditions. The following shows
that outer brackets can be used to show the existence of clamps in general. In fact  the proof is very
short  and follows the scheme laid out in the bounded example above: outer bracketing allows the
restriction of consideration to a bounded set  and some algebra from there gives a conservative upper
bound for the clamping value.
Proposition 4.3. Suppose the setting and deﬁnitions of Lemma 3.5  and additionally deﬁne

R := 2((2M )2/p + R2

B).

Then (C  R) is a clamp for measure ⇢ and center Hf (⇢; c  k) at scale ✏  and with probability at least
1  3 over a draw of size m  max{p0/(M0e)  9 ln(1/)}  it is also a clamp for ˆ⇢ and centers
Hf (ˆ⇢; c  k) at scale ✏ˆ⇢.
The general guarantee using clamps is as follows. The proof is almost the same as for Theorem 3.2 
but note that this statement is not used quite as readily  since it ﬁrst requires the construction of
clamps.
Theorem 4.4. Fix a norm k·k . Let (R  C) be a clamp for probability measure ⇢ and empirical
counterpart ˆ⇢ over some center class Z and cost f at respective scales ✏⇢ and ✏ˆ⇢  where f has
corresponding convexity constants r1 and r2. Suppose C is contained within a ball of radius RC 
let ✏> 0 be given  deﬁne scale parameter

⌧ := min⇢r ✏
and let N be a cover of C by k·k -balls of radius ⌧ (as per lemma B.4  if k·k is an lp norm  then
|N|  (1 + (2RCd)/⌧ )d sufﬁces). Then with probability at least 1  over the draw of a sample of
size m  p0/(M0e)  every set of centers P 2 Z satisﬁes

2r2

r1✏

2r2R3  

 

Z f (x; P )d⇢(x) Z f (x; P )dˆ⇢(x)  2✏ + ✏⇢ + ✏ˆ⇢ + R2s 1

2m

Before adjourning this section  note that clamps and outer brackets disagree on the treatment of the
outer regions: the former replaces the cost there with the ﬁxed value R  whereas the latter uses the
value 0. On the technical side  this is necessitated by the covering argument used to produce the
ﬁnal theorem: if the clamping operation instead truncated beyond a ball of radius R centered at each
p 2 P   then the deviations would be wild as these balls moved and suddenly switched the value at a
point from 0 to something large. This is not a problem with outer bracketing  since the same points
(namely Bc) are ignored by every set of centers.

ln✓ 2|N|k
 ◆.

7

5 Mixtures of Gaussians

Before turning to the deviation bound  it is a good place to discuss the condition 1I  ⌃  2I 
which must be met by every covariance matrix of every constituent Gaussian in a mixture.
The lower bound 1I  ⌃  as discussed previously  is fairly common in practice  arising either
via a Bayesian prior  or by implementing EM with an explicit condition that covariance updates are
discarded when the eigenvalues fall below some threshold. In the analysis here  this lower bound is
used to rule out two kinds of bad behavior.

1. Given a budget of at least 2 Gaussians  and a sample of at least 2 distinct points  arbitrarily
large likelihood may be achieved by devoting one Gaussian to one point  and shrinking its
covariance. This issue destroys convergence properties of maximum likelihood  since the
likelihood score may be arbitrarily large over every sample  but is ﬁnite for well-behaved
distributions. The condition 1I  ⌃ rules this out.
2. Another phenomenon is a “ﬂat” Gaussian  meaning a Gaussian whose density is high along
a lower dimensional manifold  but small elsewhere. Concretely  consider a Gaussian over
R2 with covariance ⌃ = diag(  1); as  decreases  the Gaussian has large density on
a line  but low density elsewhere. This phenomenon is distinct from the preceding in that
it does not produce arbitrarily large likelihood scores over ﬁnite samples. The condition
1I  ⌃ rules this situation out as well.

In both the hard and soft clustering analyses here  a crucial early step allows the assertion that good
scores in some region mean the relevant parameter is nearby. For the case of Gaussians  the condition
1I  ⌃ makes this problem manageable  but there is still the possibility that some far away  fairly
uniform Gaussian has reasonable density. This case is ruled out here via 2I ⌫ ⌃.
Theorem 5.1. Let probability measure ⇢ be given with order-p moment bound M according to norm
k·k 2 where p  8 is a positive multiple of 4  covariance bounds 0 < 1  2 with 1  1 for
simplicity  and real c  1/2 be given. Then with probability at least 1  5 over the draw of a
sample of size m  max(p/(2p/4+2e))2  8 ln(1/)  d2 ln(⇡2)2 ln(1/)   every set of Gaussian
mixture parameters (↵  ⇥) 2S mog(ˆ⇢; c  k  1  2) [S mog(⇢; c  k  1  2) satisﬁes

Z g(x; (↵  ⇥))d⇢(x) Z g(x; (↵  ⇥))dˆ⇢(x)

= O⇣m1/2+3/p⇣1 +pln(m) + ln(1/) + (1/)4/p⌘⌘  

where the O(·) drops numerical constants  polynomial terms depending on c  M  d  and k  2/1 
and ln(2/1)  but in particular has no sample-dependent quantities.

The proof follows the scheme of the hard clustering analysis. One distinction is that the outer bracket
now uses both components; the upper component is the log of the largest possible density — indeed 
it is ln((2⇡1)d/2) — whereas the lower component is a function mimicking the log density of
the steepest possible Gaussian — concretely  the lower bracket’s deﬁnition contains the expression
2/1  which lacks the normalization of a proper Gaussian  high-
ln((2⇡2)d/2)  2kx  E⇢(X)k2
lighting the fact that bracketing elements need not be elements of the class. Superﬁcially  a second
distinction with the hard clustering case is that far away Gaussians can not be entirely ignored on
local regions; the inﬂuence is limited  however  and the analysis proceeds similarly in each case.

Acknowledgments
The authors thank the NSF for supporting this work under grant IIS-1162581.

8

References
[1] David Pollard. Strong consistency of k-means clustering. The Annals of Statistics  9(1):135–

140  1981.

[2] Gbor Lugosi and Kenneth Zeger. Rates of convergence in the source coding theorem  in em-
pirical quantizer design  and in universal lossy source coding. IEEE Trans. Inform. Theory  40:
1728–1740  1994.

[3] Shai Ben-david. A framework for statistical clustering with a constant time approximation

algorithms for k-median clustering. In COLT  pages 415–426. Springer  2004.

[4] Alexander Rakhlin and Andrea Caponnetto. Stability of k-means clustering. In NIPS  pages

1121–1128  2006.

[5] Richard O. Duda  Peter E. Hart  and David G. Stork. Pattern Classiﬁcation. Wiley  2 edition 

2001.

[6] Thomas S. Ferguson. A course in large sample theory. Chapman & Hall  1996.
[7] David Pollard. A central limit theorem for k-means clustering. The Annals of Probability  10

(4):919–926  1982.

[8] Shai Ben-david  Ulrike Von Luxburg  and D´avid P´al. A sober look at clustering stability. In In

COLT  pages 5–19. Springer  2006.

[9] Ohad Shamir and Naftali Tishby. Cluster stability for ﬁnite samples. In Annals of Probability 

10(4)  pages 919–926  1982.

[10] Ohad Shamir and Naftali Tishby. Model selection and stability in k-means clustering.

COLT  2008.

In

[11] St´ephane Boucheron  Olivier Bousquet  and G´abor Lugosi. Theory of classiﬁcation: a survey

of recent advances. ESAIM: Probability and Statistics  9:323–375  2005.

[12] St´ephane Boucheron  G´abor Lugosi  and Pascal Massart. Concentration Inequalities: A

Nonasymptotic Theory of Independence. Oxford  2013.

[13] Jon Wellner. Consistency and rates of convergence for maximum likelihood estimators via

empirical process theory. 2005.

[14] Aad van der Vaart and Jon Wellner. Weak Convergence and Empirical Processes. Springer 

1996.

[15] FuChang Gao and Jon A. Wellner. On the rate of convergence of the maximum likelihood
estimator of a k-monotone density. Science in China Series A: Mathematics  52(7):1525–1538 
2009.

[16] Yair Al Censor and Stavros A. Zenios. Parallel Optimization: Theory  Algorithms and Appli-

cations. Oxford University Press  1997.

[17] Arindam Banerjee  Srujana Merugu  Inderjit S. Dhillon  and Joydeep Ghosh. Clustering with

[18] Terence

Tao.

254a

Bregman divergences. Journal of Machine Learning Research  6:1705–1749  2005.
of measure 

Concentration

notes
January
http://terrytao.wordpress.com/2010/01/03/

2010.
254a-notes-1-concentration-of-measure/.

URL

1:

[19] I. F. Pinelis and S. A. Utev. Estimates of the moments of sums of independent random vari-
ables. Teor. Veroyatnost. i Primenen.  29(3):554–557  1984. Translation to English by Bernard
Seckler.

[20] Shai Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis 

The Hebrew University of Jerusalem  July 2007.

[21] Jean-Baptiste Hiriart-Urruty and Claude Lemar´echal. Fundamentals of Convex Analysis.

Springer Publishing Company  Incorporated  2001.

9

,Matus Telgarsky
Sanjoy Dasgupta
Reza Babanezhad Harikandeh
Mark Schmidt
Scott Sallinen
Pan Li
Olgica Milenkovic
Liuyi Yao
Sheng Li
Yaliang Li
Mengdi Huai
Jing Gao
Aidong Zhang