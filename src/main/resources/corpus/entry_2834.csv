2014,Latent Support Measure Machines for Bag-of-Words Data Classification,In many classification problems  the input is represented as a set of features  e.g.  the bag-of-words (BoW) representation of documents. Support vector machines (SVMs) are widely used tools for such classification problems. The performance of the SVMs is generally determined by whether kernel values between data points can be defined properly. However  SVMs for BoW representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation. To overcome the weakness  we propose a kernel-based discriminative classifier for BoW data  which we call the latent support measure machine (latent SMM). With the latent SMM  a latent vector is associated with each vocabulary term  and each document is represented as a distribution of the latent vectors for words appearing in the document. To represent the distributions efficiently  we use the kernel embeddings of distributions that hold high order moment information about distributions. Then the latent SMM finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance. In the experiments  we show that the latent SMM achieves state-of-the-art accuracy for BoW text classification  is robust with respect to its own hyper-parameters  and is useful to visualize words.,Latent Support Measure Machines
for Bag-of-Words Data Classiﬁcation

Yuya Yoshikawa

Nara  630-0192  Japan

Tomoharu Iwata

Kyoto  619-0237  Japan

Nara Institute of Science and Technology

NTT Communication Science Laboratories

yoshikawa.yuya.yl9@is.naist.jp

iwata.tomoharu@lab.ntt.co.jp

Hiroshi Sawada

NTT Service Evolution Laboratories

Kanagawa  239-0847  Japan

sawada.hiroshi@lab.ntt.co.jp

Abstract

In many classiﬁcation problems  the input is represented as a set of features  e.g. 
the bag-of-words (BoW) representation of documents. Support vector machines
(SVMs) are widely used tools for such classiﬁcation problems. The performance
of the SVMs is generally determined by whether kernel values between data points
can be deﬁned properly. However  SVMs for BoW representations have a major
weakness in that the co-occurrence of different but semantically similar words
cannot be reﬂected in the kernel calculation. To overcome the weakness  we pro-
pose a kernel-based discriminative classiﬁer for BoW data  which we call the la-
tent support measure machine (latent SMM). With the latent SMM  a latent vector
is associated with each vocabulary term  and each document is represented as a
distribution of the latent vectors for words appearing in the document. To repre-
sent the distributions efﬁciently  we use the kernel embeddings of distributions that
hold high order moment information about distributions. Then the latent SMM
ﬁnds a separating hyperplane that maximizes the margins between distributions of
different classes while estimating latent vectors for words to improve the classi-
ﬁcation performance. In the experiments  we show that the latent SMM achieves
state-of-the-art accuracy for BoW text classiﬁcation  is robust with respect to its
own hyper-parameters  and is useful to visualize words.

1 Introduction

In many classiﬁcation problems  the input is represented as a set of features. A typical example of
such features is the bag-of-words (BoW) representation  which is used for representing a document
(or sentence) as a multiset of words appearing in the document while ignoring the order of the words.
Support vector machines (SVMs) [1]  which are kernel-based discriminative learning methods  are
widely used tools for such classiﬁcation problems in various domains  e.g.  natural language pro-
cessing [2]  information retrieval [3  4] and data mining [5]. The performance of SVMs generally
depends on whether the kernel values between documents (data points) can be deﬁned properly.
The SVMs for BoW representation have a major weakness in that the co-occurrence of different but
semantically similar words cannot be reﬂected in the kernel calculation. For example  when dealing
with news classiﬁcation  ‘football’ and ‘soccer’ are semantically similar and characteristic words for
football news. Nevertheless  in the BoW representation  the two words might not affect the compu-
tation of the kernel value between documents  because many kernels  e.g.  linear  polynomial and

1

Gaussian RBF kernels  evaluate kernel values based on word co-occurrences in each document  and
‘football’ and ‘soccer’ might not co-occur.
To overcome this weakness  we can consider the use of the low rank representation of each doc-
ument  which is learnt by unsupervised topic models or matrix factorization. By using the low
rank representation  the kernel value can be evaluated properly between documents without shared
vocabulary terms. Blei et al. showed that an SVM using the topic proportions of each document
extracted by latent Dirichlet allocation (LDA) outperforms an SVM using BoW features in terms
of text classiﬁcation accuracy [6]. Another naive approach is to use vector representation of words
learnt by matrix factorization or neural networks such as word2vec [7]. In this approach  each doc-
ument is represented as a set of vectors corresponding to words appearing in the document. To
classify documents represented as a set of vectors  we can use support measure machines (SMMs) 
which are a kernel-based discriminative learning method on distributions [8]. However  these low
dimensional representations of documents or words might not be helpful for improving classiﬁca-
tion performance because the learning criteria for obtaining the representation and the classiﬁers are
different.
In this paper  we propose a kernel-based discriminative learning method for BoW representation
data  which we call the latent support measure machine (latent SMM). The latent SMMs assume
that a latent vector is associated with each vocabulary term  and each document is represented as a
distribution of the latent vectors for words appearing in the document. By using the kernel embed-
dings of distributions [9]  we can effectively represent the distributions without density estimation
while preserving necessary distribution information. In particular  the latent SMMs map each dis-
tribution into a reproducing kernel Hilbert space (RKHS)  and ﬁnd a separating hyperplane that
maximizes the margins between distributions from different classes on the RKHS. The learning pro-
cedure of the latent SMMs is performed by alternately maximizing the margin and estimating the
latent vectors for words. The learnt latent vectors of semantically similar words are located close
to each other in the latent space  and we can obtain kernel values that reﬂect the semantics. As a
result  the latent SMMs can classify unseen data using a richer and more useful representation than
the BoW representation. The latent SMMs ﬁnd the latent vector representation of words useful for
classiﬁcation. By obtaining two- or three-dimensional latent vectors  we can visualize relationships
between classes and between words for a given classiﬁcation task.
In our experiments  we demonstrate the quantitative and qualitative effectiveness of the latent SMM
on standard BoW text datasets. The experimental results ﬁrst indicate that the latent SMM can
achieve state-of-the-art classiﬁcation accuracy. Therefore  we show that the performance of the
latent SMM is robust with respect to its own hyper-parameters  and the latent vectors for words in
the latent SMM can be represented in a two dimensional space while achieving high classiﬁcation
performance. Finally  we show that the characteristic words of each class are concentrated in a single
region by visualizing the latent vectors.
The latent SMMs are a general framework of discriminative learning for BoW data. Thus  the idea
of the latent SMMs can be applied to various machine learning problems for BoW data  which have
been solved by using SVMs: for example  novelty detection [10]  structure prediction [11]  and
learning to rank [12].

2 Related Work

The proposed method is based on a framework of support measure machines (SMMs)  which are
kernel-based discriminative learning on distributions [8]. Muandet et al. showed that SMMs are
more effective than SVMs when the observed feature vectors are numerical and dense in their exper-
iments on handwriting digit recognition and natural scene categorization. On the other hand  when
observations are BoW features  the SMMs coincide with the SVMs as described in Section 3.2.
To receive the beneﬁts of SMMs for BoW data  the proposed method represents each word as a
numerical and dense vector  which is estimated from the given data.
The proposed method aims to achieve a higher classiﬁcation performance by learning a classiﬁer
and feature representation simultaneously. Supervised topic models [13] and maximum margin
topic models (MedLDA) [14] have been proposed based on a similar motivation but using differ-
ent approaches. They outperform classiﬁers using features extracted by unsupervised LDA. There

2

are two main differences between these methods and the proposed method. First  the proposed
method plugs the latent word vectors into a discriminant function  while the existing methods plug
the document-speciﬁc vectors into their discriminant functions. Second  the proposed method can
naturally develop non-linear classiﬁers based on the kernel embeddings of distributions. We demon-
strate the effectiveness of the proposed model by comparing the topic model based classiﬁers in our
text classiﬁcation experiments.

3 Preliminaries

In this section  we introduce the kernel embeddings of distributions and support measure machines.
Our method in Section 4 will build upon these techniques.

3.1 Representations of Distributions via Kernel Embeddings
Suppose that we are given a set of n distributions fPign
i=1  where Pi is the ith distribution on space
X (cid:26) Rq. The kernel embeddings of distributions are to embed any distribution Pi into a reproducing
kernel Hilbert space (RKHS) Hk speciﬁed by kernel k [15]  and the distribution is represented as
element (cid:22)Pi in the RKHS. More precisely  the element of the ith distribution (cid:22)Pi is deﬁned as
follows:

∫

(cid:22)Pi := Ex(cid:24)Pi [k((cid:1); x)] =

k((cid:1); x)dPi 2 Hk;

X

(1)

where kernel k is referred to as an embedding kernel. It is known that element (cid:22)Pi preserves the
properties of probability distribution Pi such as mean  covariance and higher-order moments by
using characteristic kernels (e.g.  Gaussian RBF kernel) [15]. In practice  although distribution Pi
is unknown  we are given a set of samples Xi = fximgMi
m=1 drawn from the distribution. In this
m=1 (cid:14)xim((cid:1))  where (cid:14)x((cid:1))
case  by interpreting sample set Xi as empirical distribution ^Pi = 1
is the Dirac delta function at point x 2 X   empirical kernel embedding ^(cid:22)Pi is given by

∑

Mi

Mi

k((cid:1); xim) 2 Hk;

(2)

Mi∑

m=1

^(cid:22)Pi =

1
Mi

which can be approximated with an error rate of jj^(cid:22)Pi
3.2 Support Measure Machines

(cid:0) (cid:22)Pi

jjHk = Op(M

(cid:0) 1
i

2

) [9].

Now we consider learning a separating hyper-plane on distributions by employing support measure
machines (SMMs). An SMM amounts to solving an SVM problem with a kernel between empirical
gn
embedded distributions f^(cid:22)Pi
i=1  called level-2 kernel. A level-2 kernel between the ith and jth
distributions is given by

Mi∑

Mj∑

1

MiMj

k(xig; xjh);

g=1

h=1

(3)

K(^Pi; ^Pj) = ⟨^(cid:22)Pi ; ^(cid:22)Pj
)
(

⟩Hk =
(

= exp

⟩Hk

jj2Hk

jj^(cid:22)Pi

(cid:0) (cid:21)
2

(cid:0) (cid:21)
2

(cid:0) ^(cid:22)Pj

(⟨^(cid:22)Pi ; ^(cid:22)Pi

where kernel k indicates the embedding kernel used in Eq. (2). Although the level-2 kernel Eq.(3) is
)
linear on the embedded distributions  we can also consider non-linear level-2 kernels. For example 
a Gaussian RBF level-2 kernel with bandwidth parameter (cid:21) > 0 is given by
⟩Hk )
Krbf (^Pi; ^Pj) = exp
(4)
Note that the inner-product ⟨(cid:1);(cid:1)⟩Hk in Eq. (4) can be calculated by Eq. (3). By using these kernels 
we can measure similarities between distributions based on their own moment information.
The SMMs are a generalization of the standard SVMs. For example  suppose that a word is rep-
resented as a one-hot representation vector with vocabulary length  where all the elements are zero
except for the entry corresponding to the vocabulary term. Then  a document is represented by
adding the one-hot vectors of words appearing in the document. This operation is equivalent to
using a linear kernel as its embedding kernel in the SMMs. Then  by using a non-linear kernel
as a level-2 kernel like Eq. (4)  the SMM for the BoW documents is the same as an SVM with a
non-linear kernel.

⟩Hk + ⟨^(cid:22)Pj ; ^(cid:22)Pj

(cid:0) 2⟨^(cid:22)Pi ; ^(cid:22)Pj

:

3

4 Latent Support Measure Machines

In this section  we propose latent support measure machines (latent SMMs) that are effective for
BoW data classiﬁcation by learning latent word representation to improve classiﬁcation perfor-
mance.
The SMM assumes that a set of samples from distribution Pi  Xi  is observed. On the other hand  as
described later  the latent SMM assumes that Xi is unobserved. Instead  we consider a case where
BoW features are given for each document. More formally  we are given a training set of n pairs
of documents and class labels f(di; yi)gn
i=1  where di is the ith document that is represented by a
multiset of words appearing in the document and yi 2 Y is a class variable. Each word is included
in vocabulary set V. For simplicity  we consider binary class variable yi 2 f+1;(cid:0)1g. The proposed
method is also applicable to multi-class classiﬁcation problems by adopting one-versus-one or one-
versus-rest strategies as with the standard SVMs [16].
With the latent SMM  each word t 2 V is represented by a q-dimensional latent vector xt 2 Rq 
and the ith document is represented as a set of latent vectors for words appearing in the document
Xi = fxtgt2di. Then  using the kernel embeddings of distributions described in Section 3.1  we
can obtain a representation of the ith document from Xi as follows: ^(cid:22)Pi = 1jdij
gn
Using latent word vectors X = fxtgt2V and document representation f^(cid:22)Pi
i=1  the primal opti-
mization problem for the latent SMM can be formulated in an analogous but different way from the
original SMMs as follows:

k((cid:1); xt).

∑

t2di

min

w;b;(cid:24);X;(cid:18)

jjwjj2 + C

1
2

(cid:24)i +

(cid:26)
2

jjxtjj2

2

subject to yi (⟨w; (cid:22)Pi

⟩H (cid:0) b) (cid:21) 1(cid:0) (cid:24)i; (cid:24)i (cid:21) 0; (5)

where f(cid:24)ign
i=1 denotes slack variables for handling soft margins. Unlike the primal form of the
SMMs  that of the latent SMMs includes a ℓ2 regularization term with parameter (cid:26) > 0 with respect
to latent word vectors X. The latent SMM minimizes Eq. (5) with respect to the latent word vectors
X and kernel parameters (cid:18)  along with weight parameters w  bias parameter b and f(cid:24)ign
It is extremely difﬁcult to solve the primal problem Eq. (5) directly because the inner term ⟨w; (cid:22)Pi
⟩H
in the constrained conditions is in fact calculated in an inﬁnite dimensional space. Thus  we solve
this problem by converting it into an another optimization problem in which the inner term does not
∑
appear explicitly. Unfortunately  due to its non-convex nature  we cannot derive the dual form for
Eq. (5) as with the standard SVMs. Thus we consider a min-max optimization problem  which is
derived by ﬁrst introducing Lagrange multipliers A = fa1; a2;(cid:1)(cid:1)(cid:1) ; ang and then plugging w =

i=1.

n

i=1 ai ^(cid:22)Pi into Eq (5)  as follows:

n∑

i=1

∑

t2V

L(A; X; (cid:18)) subject to 0 (cid:20) ai (cid:20) C;

min
X;(cid:18)

max

A

n∑

n∑

n∑

ai (cid:0) 1

2

i=1

i=1

j=1

where L(A; X; (cid:18)) =

aiajyiyjK(^Pi; ^Pj; X; (cid:18)) +

∑

t2V

(cid:26)
2

(6a)

jjxtjj2

2; (6b)

where K(^Pi; ^Pj; X; (cid:18)) is a kernel value between empirical distributions ^Pi and ^Pj speciﬁed by
parameters X and (cid:18) as is shown in Eq. (3).
We solve this min-max problem by separating it into two partial optimization problems: 1) maxi-
mization over A given current estimates (cid:22)X and (cid:22)(cid:18)  and 2) minimization over X and (cid:18) given current
estimates (cid:22)A. This approach is analogous to wrapper methods in multiple kernel learning [17].
Maximization over A. When we ﬁx X and (cid:18) in Eq. (6) with current estimate (cid:22)X and (cid:22)(cid:18)  the maxi-
mization over A becomes a quadratic programming problem as follows:

n∑

aiyi = 0;

i=1

n∑

n∑

n∑

ai (cid:0) 1

2

max

A

i=1

aiajyiyjK(^Pi; ^Pj; (cid:22)X; (cid:22)(cid:18)) subject to 0 (cid:20) ai (cid:20) C;

aiyi = 0;

(7)

i=1

j=1

i=1

which is identical to solving the dual problem of the standard SVMs. Thus  we can obtain optimal
A by employing an existing SVM package.

4

n∑

Table 1: Dataset speciﬁcations.
# features
7 770
17 387
70 216

# samples
4 199
7 674
18 821

WebKB
Reuters-21578
20 Newsgroups

# classes
4
8
20

Minimization over X and (cid:18). When we ﬁx A in Eq. (6) with current estimate (cid:22)A  the min-max
problem can be replaced with a simpler minimization problem as follows:

(cid:22)ai(cid:22)ajyiyjK(^Pi; ^Pj; X; (cid:18)) +

(cid:26)
2

jjxtjj2
2:

(8)

∑

t2V

n∑

n∑

i=1

j=1

min
X;(cid:18)

l(X; (cid:18))  where l(X; (cid:18)) = (cid:0) 1
2
n∑

n∑

@l(X; (cid:18))

= (cid:0) 1
2

@xm

(cid:22)ai(cid:22)ajyiyj

i=1

j=1

To solve this problem  we use a quasi-Newton method [18]. The quasi-Newton method needs the
gradient of parameters. For each word m 2 V  the gradient of latent word vector xm is given by

@K(^Pi; ^Pj; X; (cid:18))

@xm

+ (cid:26)xm;

(9)

where the gradient of the kernel with respect to xm depends on the choice of kernels. For example 
when choosing a embedding kernel as a Gaussian RBF kernel with bandwidth parameter (cid:13) > 0:
k(cid:13)(xs; xt) = exp((cid:0) (cid:13)
)  and a level-2 kernel as a linear kernel  the gradient is given by
@K(^Pi; ^Pj; X; (cid:18))

∑

{

2

(cid:13)(xt (cid:0) xs)
(cid:13)(xs (cid:0) xt)

0

(m = s ^ m ̸= t)
(m = t ^ m ̸= s)
(m = t ^ m = s):

jjxs (cid:0) xtjj2Hk
∑
1jdijjdjj

=

s2di

t2dj

k(cid:13)(xs; xt) (cid:2)

@xm

As with the estimation of X  kernel parameters (cid:18) can be obtained by calculating gradient @l(X;(cid:18))
.
By alternately repeating these computations until dual function Eq. (6) converges  we can ﬁnd a
local optimal solution to the min-max problem.
The parameters that need to be stored after learning are latent word vectors X  kernel parameters
(cid:3) is performed by computing
(cid:18) and Lagrange multipliers A. Classiﬁcation for new document d
(cid:3)
i=1 aiyiK(^Pi; ^P(cid:3); X; (cid:18))  where ^P(cid:3) is the distribution of latent vectors for words included
y(d
) =
(cid:3).
in d

∑

@(cid:18)

n

5 Experiments with Bag-of-Words Text Classiﬁcation

Data description. For the evaluation  we used the following three standard multi-class text classi-
ﬁcation datasets: WebKB  Reuters-21578 and 20 Newsgroups. These datasets  which have already
been preprocessed by removing short and stop words  are found in [19] and can be downloaded
from the author’s website1. The speciﬁcations of these datasets are shown in Table 1. For our
experimental setting  we ignored the original training/test data separations.
Setting. In our experiments  the proposed method  latent SMM  uses a Gaussian RBF embedding
kernel and a linear level-2 kernel. To demonstrate the effectiveness of the latent SMM  we compare
it with several methods: MedLDA  SVD+SMM  word2vec+SMM and SVMs. MedLDA is a method
that jointly learns LDA and a maximum margin classiﬁer  which is a state-of-the-art discriminative
learning method for BoW data [14]. We use the author’s implementation of MedLDA2. SVD+SMM
is a two-step procedure: 1) extracting low-dimensional representations of words by using a singular
value decomposition (SVD)  and 2) learning a support measure machine using the distribution of
extracted representations of words appearing in each document with the same kernels as the latent
SMM. word2vec+SMM employs the representations of words learnt by word2vec [7] and uses them
for the SMM as in SVD+SMM. Here we use pre-trained 300 dimensional word representation vec-
tors from the Google News corpus  which can be downloaded from the author’s website3. Note that
word2vec+SMM utilizes an additional resource to represent the latent vectors for words unlike the

1http://web.ist.utl.pt/acardoso/datasets/
2http://www.ml-thu.net/˜jun/medlda.shtml
3https://code.google.com/p/word2vec/

5

(a) WebKB

(b) Reuters-21578

(c) 20 Newsgroups

Figure 1: Classiﬁcation accuracy over number of training samples.

(a) WebKB

(b) Reuters-21578

(c) 20 Newsgroups

Figure 2: Classiﬁcation accuracy over the latent dimensionality.

(cid:0)3; 2

latent SMM  and the learning of word2vec requires n-gram information about documents  which
is lost in the BoW representation. With SVMs  we use a Gaussian RBF kernel with parameter (cid:13)
and a quadratic polynomial kernel  and the features are represented as BoW. We use LIBSVM4 to
estimate Lagrange multipliers A in the latent SMM and to build SVMs and SMMs. To deal with
multi-class classiﬁcation  we adopt a one-versus-one strategy [16] in the latent SMM  SVMs and
SMMs. In our experiments  we choose the optimal parameters for these methods from the following
variations: (cid:13) 2 f10
(cid:0)2;(cid:1)(cid:1)(cid:1) ; 103g in the latent SMM  SVD+SMM  word2vec+SMM and SVM
(cid:0)3; 10
with a Gaussian RBF kernel  C 2 f2
(cid:0)1;(cid:1)(cid:1)(cid:1) ; 25; 27g in all the methods  regularizer parame-
ter (cid:26) 2 f10
(cid:0)1; 100g  latent dimensionality q 2 f2; 3; 4g in the latent SMM  and the latent
(cid:0)2; 10
dimensionality of MedLDA and SVD+SMM ranges f10; 20;(cid:1)(cid:1)(cid:1) ; 50g.
Accuracy over number of training samples. We ﬁrst show the classiﬁcation accuracy when vary-
ing the number of training samples. Here we randomly chose ﬁve sets of training samples  and used
the remaining samples for each of the training sets as the test set. We removed words that occurred
in less than 1% of the training documents. Below  we refer to the percentage as a word occurrence
threshold. As shown in Figure 1  the latent SMM outperformed the other methods for each of the
numbers of training samples in the WebKB and Reuters-21578 datasets. For the 20 Newsgroups
dataset  the accuracies of the latent SMM  MedLDA and word2vec+SMM were proximate and bet-
ter than those of SVD+SMM and SVMs.
The performance of SVD+SMM changed depending on the datasets: while SVD+SMM was the
second best method with the Reuters-21578  it placed fourth with the other datasets. This result
indicates that the usefulness of the low rank representations by SVD for classiﬁcation depends on
the properties of the dataset. The high classiﬁcation performance of the latent SMM for all of the
datasets demonstrates the effectiveness of learning the latent word representations.
Robustness over latent dimensionality. Next we conﬁrm the robustness of the latent SMM over
the latent dimensionality. For this experiment  we changed the latent dimensionality of the latent
SMM  MedLDA and SVD+SMM within f2; 4;(cid:1)(cid:1)(cid:1) ; 12g. Figure 2 shows the accuracy when varying
the latent dimensionality. Here the number of training samples in each dataset was 600  and the
word occurrence threshold was 1%. For all the latent dimensionality  the accuracy of the latent
SMM was consistently better than the other methods. Moreover  even with two-dimensional latent

4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/

6

Figure 3: Classiﬁcation accuracy on WebKB
when varying word occurrence threshold.

Figure 4: Parameter sensitivity on Reuters-21578.

project

course

faculty

student

Figure 5: Distributions of latent vectors for words appearing in documents of each class on WebKB.

vectors  the latent SMM achieved high classiﬁcation performance. On the other hand  MedLDA
and SVD+SMM often could not display their own abilities when the latent dimensionality was low.
One of the reasons why the latent SMM with a very low latent dimensionality q achieves a good
performance is that it can use qjdij parameters to classify the ith document  while MedLDA uses
only q parameters. Since the latent word representation used in SVD+SMM is not optimized for the
given classiﬁcation problem  it does not contain useful features for classiﬁcation  especially when
the latent dimensionality is low.
Accuracy over word occurrence threshold.
In the above experiments  we omit words whose
occurrence accounts for less than 1% of the training document. By reducing the threshold  low
frequency words become included in the training documents. This might be a difﬁcult situation
for the latent SMM and SVD+SMM because they cannot observe enough training data to estimate
their own latent word vectors. On the other hand  it would be an advantageous situation for SVMs
using BoW features because they can use low frequency words that are useful for classiﬁcation to
compute their kernel values. Figure 3 shows the classiﬁcation accuracy on WebKB when varying
the word occurrence threshold within f0:4; 0:6; 0:8; 1:0g. The performance of the latent SMM did
not change when the thresholds were varied  and was better than the other methods in spite of the
difﬁcult situation.
Parameter sensitivity. Figure 4 shows how the performance of the latent SMM changes against
ℓ2 regularizer parameter (cid:26) and C on a Reuters-21578 dataset with 1 000 training samples. Here
the latent dimensionality of the latent SMM was ﬁxed at q = 2 to eliminate the effect of q. The
performance is insensitive to (cid:26) except when C is too small. Moreover  we can see that the perfor-
mance is improved by increasing the C value. In general  the performance of SVM-based methods
is very sensitive to C and kernel parameters [20]. Since kernel parameters (cid:18) in the latent SMM are
estimated along with latent vectors X  the latent SMM can avoid the problem of sensitivity for the
kernel parameters. In addition  Figure 2 has shown that the latent SMM is robust over the latent
dimensionality. Thus  the latent SMM can achieve high classiﬁcation accuracy by focusing only on
tuning the best C  and experimentally the best C exhibits a large value  e.g.  C (cid:21) 25.
Visualization of classes.
In the above experiments  we have shown that the latent SMM can
achieve high classiﬁcation accuracy with low-dimensional latent vectors. By using two- or three-
dimensional latent vectors in the latent SMM  and visualizing them  we can understand the rela-
tionships between classes. Figure 5 shows the distributions of latent vectors for words appearing

7

(a)

(c)

(b)

(d)

Complete view (50% sampling)

Figure 6: Visualization of latent vectors for words on WebKB. The font color of each word indicates
the class in which the word occurs most frequently  and ‘project’  ‘course’  ‘student’ and ‘faculty’
classes correspond to yellow  red  blue and green fonts  respectively.

in documents of each class. Each class has its own characteristic distribution that is different from
those of other classes. This result shows that the latent SMM can extract the difference between
the distributions of the classes. For example  the distribution of ‘course’ is separated from those
of the other classes  which indicates that documents categorized in ‘course’ share few words with
documents categorized in other classes. On the other hand  the latent words used in the ‘project’
class are widely distributed  and its distribution overlaps those of the ‘faculty’ and ‘student’ classes.
This would be because faculty and students work jointly on projects  and words in both ‘faculty’
and ‘student’ appear simultaneously in ‘project’ documents.
Visualization of words. In addition to the visualization of classes  the latent SMM can visualize
words using two- or three-dimensional latent vectors. Unlike unsupervised visualization methods
for documents  e.g.  [21]  the latent SMM can gather characteristic words of each class in a region.
Figure 6 shows the visualization result of words on the WebKB dataset. Here we used the same
learning result as that used in Figure 5. As shown in the complete view  we can see that highly-
frequent words in each class tend to gather in a different region. On the right side of this ﬁgure 
four regions from the complete view are displayed in closeup. Figures (a)  (b) and (c) include words
indicating ‘course’  ‘faculty’ and ‘student’ classes  respectively. For example  ﬁgure (a) includes
‘exercise’  ’examine’ and ‘quiz’ which indicate examinations in lectures. Figure (d) includes words
of various classes  although the ‘project’ class dominates the region as shown in Figure 5. This
means that words appearing in the ‘project’ class are related to the other classes or are general
words  e.g.  ‘occur’ and ‘differ’.

6 Conclusion
We have proposed a latent support measure machine (latent SMM)  which is a kernel-based dis-
criminative learning method effective for sets of features such as bag-of-words (BoW). The latent
SMM represents each word as a latent vector  and each document to be classiﬁed as a distribution
of the latent vectors for words appearing in the document. Then the latent SMM ﬁnds a separating
hyperplane that maximizes the margins between distributions of different classes while estimating
latent vectors for words to improve the classiﬁcation performance. The experimental results can be
summarized as follows: First  the latent SMM has achieved state-of-the-art classiﬁcation accuracy
for BoW data. Second  we have shown experimentally that the performance of the latent SMM is
robust as regards its own hyper-parameters. Third  since the latent SMM can represent each word as
a two- or three- dimensional latent vector  we have shown that the latent SMMs are useful for un-
derstanding the relationships between classes and between words by visualizing the latent vectors.
Acknowledgment. This work was supported by JSPS Grant-in-Aid for JSPS Fellows (259867).

8

References
[1] Corinna Cortes and Vladimir Vapnik. Support-Vector Networks. Machine Learning  20(3):273–297 

September 1995.

[2] Taku Kudo and Yuji Matsumoto. Chunking with Support Vector Machines. Proceedings of the second
meeting of the North American Chapter of the Association for Computational Linguistics on Language
technologies  816  2001.

[3] Dell Zhang and Wee Sun Lee. Question Classiﬁcation Using Support Vector Machines. SIGIR  page 26 

2003.

[4] Changhua Yang  Kevin Hsin-Yih Lin  and Hsin-Hsi Chen. Emotion Classiﬁcation Using Web Blog Cor-

pora. IEEE/WIC/ACM International Conference on Web Intelligence  pages 275–278  November 2007.

[5] Pranam Kolari  Tim Finin  and Anupam Joshi. SVMs for the Blogosphere: Blog Identiﬁcation and Splog

Detection. AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs  2006.

[6] David M. Blei  Andrew Y. Ng  and M. Jordan. Latent Dirichlet Allocation. The Journal of Machine

Learning Research  3(4-5):993–1022  May 2003.

[7] Tomas Mikolov  I Sutskever  and Kai Chen. Distributed Representations of Words and Phrases and their

Compositionality. NIPS  pages 1–9  2013.

[8] Krikamol Muandet and Kenji Fukumizu. Learning from Distributions via Support Measure Machines.

NIPS  2012.

[9] Alex Smola  Arthur Gretton  Le Song  and B Sch¨olkopf. A Hilbert Space Embedding for Distributions.

Algorithmic Learning Theory  2007.

[10] Bernhard Sch¨olkopf  Robert Williamson  Alex Smola  John Shawe-Taylor  and John Platt. Support Vector

Method for Novelty Detection. NIPS  pages 582–588  1999.

[11] Ioannis Tsochantaridis  Thomas Hofmann  Thorsten Joachims  and Yasemin Altun. Support Vector Ma-

chine Learning for Interdependent and Structured Output Spaces. ICML  page 104  2004.

[12] Thorsten Joachims. Optimizing Search Engines Using Clickthrough Data. SIGKDD  page 133  2002.
[13] David M. Blei and Jon D. McAuliffe. Supervised Topic Models. NIPS  pages 1–8  2007.
[14] Jun Zhu  A Ahmed  and EP Xing. MedLDA: Maximum Margin Supervised Topic Models for Regression

and Classiﬁcation. ICML  2009.

[15] BK Sriperumbudur and A Gretton. Hilbert Space Embeddings and Metrics on Probability Measures. The

Journal of Machine Learning Research  11:1517–1561  2010.

[16] Chih-Wei Hsu and Chih-Jen Lin. A Comparison of Methods for Multi-class Support Vector Machines.

Neural Networks  IEEE Transactions on  13(2):415—-425  2002.

[17] S¨oren Sonnenburg and G R¨atsch. Large Scale Multiple Kernel Learning. The Journal of Machine Learn-

ing Research  7:1531–1565  2006.

[18] Dong C. Liu and Jorge Nocedal. On the Limited Memory BFGS Method for Large Scale Optimization.

Mathematical Programming  45(1-3):503–528  August 1989.

[19] Ana Cardoso-Cachopo. Improving Methods for Single-label Text Categorization. PhD thesis  2007.
[20] Vladimir Cherkassky and Yunqian Ma. Practical Selection of SVM Parameters and Noise Estimation for
SVM Regression. Neural networks : the ofﬁcial journal of the International Neural Network Society 
17(1):113–26  January 2004.

[21] Tomoharu Iwata  T Yamada  and N Ueda. Probabilistic Latent Semantic Visualization: Topic Model for

Visualizing Documents. SIGKDD  2008.

9

,Yuya Yoshikawa
Tomoharu Iwata
Hiroshi Sawada
Scott Linderman
Ryan Adams
Jonathan Pillow