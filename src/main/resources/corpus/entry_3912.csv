2017,On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning,We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming   and enjoys both strong computational and statistical guarantees. Specifically  by leveraging a sophisticated characterization of sparse modeling structures (i.e.  local restricted strong convexity and Hessian smoothness)  we prove that within each stage of convex relaxation  our proposed algorithm achieves (local) quadratic convergence  and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory.,On Quadratic Convergence of DC Proximal Newton

Algorithm in Nonconvex Sparse Learning

Xingguo Li1 4 Lin F. Yang2⇤ Jason Ge2

Jarvis Haupt1 Tong Zhang3 Tuo Zhao4†

1University of Minnesota

2Princeton University 3Tencent AI Lab 4Georgia Tech

Abstract

We propose a DC proximal Newton algorithm for solving nonconvex regularized
sparse learning problems in high dimensions. Our proposed algorithm integrates
the proximal newton algorithm with multi-stage convex relaxation based on the
difference of convex (DC) programming  and enjoys both strong computational and
statistical guarantees. Speciﬁcally  by leveraging a sophisticated characterization
of sparse modeling structures (i.e.  local restricted strong convexity and Hessian
smoothness)  we prove that within each stage of convex relaxation  our proposed
algorithm achieves (local) quadratic convergence  and eventually obtains a sparse
approximate local optimum with optimal statistical properties after only a few
convex relaxations. Numerical experiments are provided to support our theory.

Introduction

1
We consider a high dimensional regression or classiﬁcation problem: Given n independent observa-
tions {xi  yi}n
i=1 ⇢ Rd ⇥ R sampled from a joint distribution D(X  Y )  we are interested in learning
the conditional distribution P(Y |X) from the data. A popular modeling approach is the Generalized
Linear Model (GLM) [20]  which assumes

P (Y |X; ✓⇤) / exp✓ Y X>✓⇤  (X>✓⇤)

c()

◆  

where c() is a scaling parameter  and is the cumulant function. A natural approach to estimate
✓⇤ is the Maximum Likelihood Estimation (MLE) [25]  which essentially minimizes the negative
log-likelihood of the data given parameters. However  MLE often performs poorly in parameter
estimation in high dimensions due to the curse of dimensionality [6].
To address this issue  machine learning researchers and statisticians follow Occam’s razor principle 
and propose sparse modeling approaches [3  26  30  32]. These sparse modeling approaches assume
that ✓⇤ is a sparse vector with only s⇤ nonzero entries  where s⇤ < n ⌧ d. This implies that
many variables in X are essentially irrelevant to modeling  which is very natural to many real world
applications such as genomics and medical imaging [7  21]. Many empirical results have corroborated
the success of sparse modeling in high dimensions. Speciﬁcally  many sparse modeling approaches
obtain a sparse estimator of ✓⇤ by solving the following regularized optimization problem 

✓ = argmin

✓2Rd L(✓) + Rtgt(✓) 

(1)

where L : Rd ! R is the convex negative log-likelihood (or pseudo-likelihood) function  Rtgt :
Rd ! R is a sparsity-inducing decomposable regularizer  i.e.  Rtgt(✓) = Pd
j=1 rtgt(✓j) with
: R ! R  and tgt > 0 is the regularization parameter. Many existing sparse modeling
rtgt
approaches can be cast as special examples of (1)  such as sparse linear regression [30]  sparse logistic
regression [32]  and sparse Poisson regression [26].

⇤The work was done while the author was at Johns Hopkins University.
†The authors acknowledge support from DARPA YFA N66001-14-1-4047  NSF Grant IIS-1447639 
and Doctoral Dissertation Fellowship from University of Minnesota. Correspondence to: Xingguo Li
<lixx1661@umn.edu> and Tuo Zhao <tuo.zhao@isye.gatech.edu>.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Given a convex regularizer  e.g.  Rtgt(✓) = tgt||✓||1 [30]  we can obtain global optima in polynomial
time and characterize their statistical properties. However  convex regularizers incur large estimation
bias. To address this issue  several nonconvex regularizers are proposed  including the minimax
concave penalty (MCP  [39])  smooth clipped absolute deviation (SCAD  [8])  and capped `1-
regularization [40]. The obtained estimator (e.g.  hypothetically global optima to (1)) can achieve
faster statistical rates of convergence than their convex counterparts [9  16  22  34].
Despite of these superior statistical guarantees  nonconvex regularizers raise greater computational
challenge than convex regularizers in high dimensions. Popular iterative algorithms for convex
optimization  such as proximal gradient descent [2  23] and coordinate descent [17  29]  no longer
have strong global convergence guarantees for nonconvex optimization. Therefore  establishing
statistical properties of the estimators obtained by these algorithms becomes very challenging  which
explains why existing theoretical studies on computational and statistical guarantees for nonconvex
regularized sparse modeling approaches are so limited until recent rise of a new area named “statistical
optimization”. Speciﬁcally  machine learning researchers start to incorporate certain structures of
sparse modeling (e.g. restricted strong convexity  large regularization effect) into the algorithmic
design and convergence analysis for optimization. This further motivates a few recent progresses:
[16] propose proximal gradient algorithms for a family of nonconvex regularized estimators with a
linear convergence to an approximate local optimum with suboptimal statistical guarantees; [34  43]
further propose homotopy proximal gradient and coordinate gradient descent algorithms with a linear
convergence to a local optimum and optimal statistical guarantees; [9  41] propose a multistage
convex relaxation-based (also known as Difference of Convex (DC) Programming) proximal gradient
algorithm  which can guarantee an approximate local optimum with optimal statistical properties.
Their computational analysis further shows that within each stage of the convex relaxation  the
proximal gradient algorithm achieves a (local) linear convergence to a unique sparse global optimum
for the relaxed convex subproblem.
The aforementioned approaches only consider ﬁrst order algorithms  such as proximal gradient
descent and proximal coordinate gradient descent. The second order algorithms with theoretical
guarantees are still largely missing for high dimensional nonconvex regularized sparse modeling
approaches  but this does not suppress the enthusiasm of applying heuristic second order algorithms
to real world problems. Some evidences have already corroborated their superior computational
performance over ﬁrst order algorithms (e.g. glmnet [10]). This further motivates our attempt
towards understanding the second order algorithms in high dimensions.
In this paper  we study a multistage convex relaxation-based proximal Newton algorithm for noncon-
vex regularized sparse learning. This algorithm is not only highly efﬁcient in practice  but also enjoys
strong computational and statistical guarantees in theory. Speciﬁcally  by leveraging a sophisticated
characterization of local restricted strong convexity and Hessian smoothness  we prove that within
each stage of convex relaxation  our proposed algorithm maintains the solution sparsity  and achieves
a (local) quadratic convergence  which is a signiﬁcant improvement over (local) linear convergence
of proximal gradient algorithm in [9] (See more details in later sections). This eventually allows us to
obtain an approximate local optimum with optimal statistical properties after only a few relaxations.
Numerical experiments are provided to support our theory. To the best of our knowledge  this is the
ﬁrst of second order based approaches for high dimensional sparse learning using convex/nonconvex
regularizers with strong statistical and computational guarantees.

1(vj

Notations: Given a vector v 2 Rd  we denote the p-norm as ||v||p = (Pd
j=1 |vj|p)1/p for
a real p > 0 and the number of nonzero entries as ||v||0 = Pj
6= 0) and v\j =
(v1  . . .   vj1  vj+1  . . .   vd)> 2 Rd1 as the subvector with the j-th entry removed. Given an
index set A✓{ 1  ...  d}  A? = {j | j 2{ 1  ...  d}  j /2A} is the complementary set to A. We use
vA to denote a subvector of v indexed by A. Given a matrix A 2 Rd⇥d  we use A⇤j (Ak⇤) to denote
the j-th column (k-th row) and ⇤max(A) (⇤min(A)) as the largest (smallest) eigenvalue of A. We
2 and ||A||2 =p⇤max(A>A). We denote A\i\j as the submatrix of A
deﬁne ||A||2
with the i-th row and the j-th column removed  A\ij (Ai\j) as the j-th column (i-th row) of A with
its i-th (j-th) entry removed  and AAA as a submatrix of A with both row and column indexed by
A. If A is a PSD matrix  we deﬁne ||v||A = pv>Av as the induced seminorm for vector v. We use
conventional notation O(·)  ⌦(·) ⇥(·) to denote the limiting behavior  ignoring constant  and OP (·)
to denote the limiting behavior in probability. C1  C2  . . . are denoted as generic positive constants.

F =Pj ||A⇤j||2

2

2 DC Proximal Newton Algorithm
Throughout the rest of the paper  we assume: (1) L(✓) is nonstrongly convex and twice continuously
differentiable  e.g.  the negative log-likelihood function of the generalized linear model (GLM);
nPn
(2) L(✓) takes an additive form  i.e.  L(✓) = 1
i=1 `i(✓)  where each `i(✓) is associated with an
observation (xi  yi) for i = 1  ...  n. Take GLM as an example  we have `i(✓) = (x>i ✓)  yix>i ✓ 
where is the cumulant function.
For nonconvex regularization  we use the capped `1 regularizer [40] deﬁned as

Rtgt(✓) =

rtgt(✓j) = tgt

min{|✓j|  tgt} 

dXj=1

dXj=1

where > 0 is an additional tuning parameter. Our algorithm and theory can also be extended to the
SCAD and MCP regularizers in a straightforward manner [8  39]. As shown in Figure 1  rtgt(✓j)
can be decomposed as the difference of two convex functions [5]  i.e. 

=



r(✓j) = |✓j|

 max{|✓j| 2  0}
|
}

{z

convex

.

convex

|{z}

This motivates us to apply the difference
of convex (DC) programming approach
to solve the nonconvex problem. We then
introduce the DC proximal Newton algo-
rithm  which contains three components:
the multistage convex relaxation  warm
initialization  and proximal Newton algo-
rithm.

j

1

Figure 1: The capped `1 regularizer is the difference of two con-
vex functions. This allows us to relax the nonconvex regularizer
based the concave duality.
(I) The multistage convex relaxation is essentially a sequential optimization framework [40]. At

  ...  {K+1}

)>  where {K+1}

✓{K+1} = argmin

simplicity  we deﬁne a regularization vector as {K+1} = ({K+1}

✓2Rd F{K+1}(✓)  where F{K+1}(✓) = L(✓) + ||{K+1}  ✓||1 

the (K + 1)-th stage  we have the output solution from the previous stageb✓{K}. For notational
=
tgt · 1(|b✓{K}
| tgt) for all j = 1  . . .   d. Let  be the Hadamard (entrywise) product. We solve
a convex relaxation of (1) at ✓ =b✓{K} as follows 
where ||{K+1}  ✓||1 = Pd
|✓j|. One can verify that ||{K+1}  ✓||1 is essentially
a convex relaxation of Rtgt(✓) at ✓ = b✓{K} based on the concave duality in DC programming.
We emphasis that ✓{K} denotes the unique sparse global optimum for (2) (The uniqueness will be
elaborated in later sections)  andb✓{K} denotes the output solution for (2) when we terminate the

iteration at the K-th convex relaxation stage. The stopping criterion will be explained later.
(II) The warm initialization is the ﬁrst stage of DC programming  where we solve the `1 regularized
counterpart of (1) 

j=1 {K+1}

(2)

d

j

j

✓{1} = argmin

✓2Rd L(✓) + tgt||✓||1.

(3)

This is an intuitive choice for sparse statistical recovery  since the `1 regularized estimator can give
us a good initialization  which is sufﬁciently close to ✓⇤. Note that this is equivalent to (2) with

{1}j = tgt for all j = 1  . . .   d  which can be viewed as the convex relaxation of (1) atb✓{0} = 0
for the ﬁrst stage.
(III) The proximal Newton algorithm proposed in [12] is then applied to solve the convex sub-
problem (2) at each stage  including the warm initialization (3). For notational simplicity  we omit
the stage index {K} for all intermediate updates of ✓  and only use (t) as the iteration index within
the K-th stage for all K  1. Speciﬁcally  at the K-th stage  given ✓(t) at the t-th iteration of the
proximal Newton algorithm  we consider a quadratic approximation of (2) at ✓(t) as follows 
Q(✓; ✓(t)  {K}) = L(✓(t)) + (✓  ✓(t))>rL(✓(t)) +
r2L(✓(t)) + ||{K}  ✓||1 

1
2||✓  ✓(t)||2

(4)

3

θjθjθjr2L(✓(t)) = (✓  ✓(t))>r2L(✓(t))(✓  ✓(t)). We then take ✓(t+ 1

where ||✓  ✓(t)||2
2 ) =
nPn
i=1 `i(✓) takes an additive form  we can avoid
argmin✓ Q(✓; ✓(t)  {K}). Since L(✓) = 1
directly computing the d by d Hessian matrix in (4). Alternatively  in order to reduce the memory
usage when d is large  we rewrite (4) as a regularized weighted least square problem as follows

Q(✓; ✓(t)) =

1
n

nXi=1

wi(zi  x>i ✓)2 + ||{K}  ✓||1 + constant 

(5)

where wi’s and zi’s are some easy to compute constants depending on ✓(t)  `i(✓(t))’s  xi’s  and yi’s.
Remark 1. Existing literature has shown that (5) can be efﬁciently solved by coordinate descent
algorithms in conjunction with the active set strategy [43]. See more details in [10] and Appendix B.

For the ﬁrst stage (i.e.  warm initialization)  we require an additional backtracking line search
procedure to guarantee the descent of the objective value [12]. Speciﬁcally  we denote

Then we start from ⌘t = 1 and use backtracking line search to ﬁnd the optimal ⌘t 2 (0  1] such that
the Armijo condition [1] holds. Speciﬁcally  given a constant µ 2 (0.9  1)  we update ⌘t = µq from
q = 0 and ﬁnd the smallest integer q such that

✓(t) = ✓(t+ 1

2 )  ✓(t).

where ↵ 2 (0  1

2 ) is a ﬁxed constant and

F{1}(✓(t) + ⌘t✓(t)) F {1}(✓(t)) + ↵⌘tt 
· ✓(t) + ||{1} ⇣✓(t) + ✓(t)⌘||1 || {1}  ✓(t)||1.

t = rL⇣✓(t)⌘>

We then set ✓(t+1) as ✓(t+1) = ✓(t) + ⌘t✓(t). We terminate the iterations when the following
approximate KKT condition holds:

!{1}⇣✓(t)⌘ := min

⇠2@||✓(t)||1 ||rL(✓(t)) + {1}  ⇠||1  " 

where " is a predeﬁned precision parameter. Then we set the output solution asb✓{1} = ✓(t). Note that
b✓{1} is then used as the initial solution for the second stage of convex relaxation (2). The proximal
Newton algorithm with backtracking line search is summarized in Algorithm 2 in Appendix.
Such a backtracking line search procedure is not necessary at K-th stage for all K  2. In other
words  we simply take ⌘t = 1 and ✓(t+1) = ✓(t+ 1
2 ) for all t  0 when K  2. This leads to more
efﬁcient updates for the proximal Newton algorithm from the second stage of convex relaxation (2).
We summarize our proposed DC proximal Newton algorithm in Algorithm 1 in Appendix.

3 Computational and Statistical Theories

Before we present our theoretical results  we ﬁrst introduce some preliminaries  including important
deﬁnitions and assumptions. We deﬁne the largest and smallest s-sparse eigenvalues as follows.
Deﬁnition 2. We deﬁne the largest and smallest s-sparse eigenvalues of r2L(✓) as

⇢+
s = sup
kvk0s

v>r2L(✓)v

v>v

and ⇢s = inf

kvk0s

v>r2L(✓)v

v>v

for any positive integer s. We deﬁne s = ⇢+
s
⇢s

as the s-sparse condition number.

The sparse eigenvalue (SE) conditions are widely studied in high dimensional sparse modeling prob-
lems  and are closely related to restricted strong convexity/smoothness properties and restricted eigen-
value properties [22  27  33  44]. For notational convenience  given a parameter ✓2 Rd and a real con-
stant R > 0  we deﬁne a neighborhood of ✓ with radius R as B(✓  R) := 2 Rd | ||  ✓||2  R .
Our ﬁrst assumption is for the sparse eigenvalues of the Hessian matrix over a sparse domain.
Assumption 1. Given ✓ 2B (✓⇤  R) for a generic constant R  there exists a generic constant C0 such
that r2L(✓) satisﬁes SE with parameters 0 <⇢ s⇤+2es <⇢ +
s⇤+2es s⇤
and s⇤+2es =

s⇤+2es < +1  wherees  C02

.

⇢+

s⇤+2es
⇢s⇤+2es

4

Assumption 1 requires that L(✓) has ﬁnite largest and positive smallest sparse eigenvalues  given ✓ is
sufﬁciently sparse and close to ✓⇤. Analogous conditions are widely used in high dimensional analysis
[13  14  34  35  43]  such as the restricted strong convexity/smoothness of L(✓) (RSC/RSS  [6]). Given
any ✓  ✓0 2 Rd  the RSC/RSS parameter can be deﬁned as (✓0 ✓ ) := L(✓0)L(✓)rL(✓)>(✓0✓).
For notational simplicity  we deﬁne S = {j | ✓⇤j 6= 0} and S? = {j | ✓⇤j = 0}. The following
proposition connects the SE property to the RSC/RSS property.

2 ⇢+

2.

1

Proposition 3. Given ✓  ✓0 2B (✓⇤  R) with ||✓S?||0 es and ||✓0

s⇤+2esk✓0  ✓k2
The proof of Proposition 3 is provided in [6]  and therefore is omitted. Proposition 3 implies that
L(✓) is essentially strongly convex  but only over a sparse domain (See Figure 2).
The second assumption requires r2L(✓) to be smooth over the sparse domain.

S?||0 es  L(✓) satisﬁes

2 ⇢s⇤+2esk✓0  ✓k2

2  (✓0 ✓ )  1

Assumption 2 (Local Restricted Hessian Smoothness). Recall thates is deﬁned in Assumption 1.
There exist generic constants Ls⇤+2es and R such that for any ✓  ✓0 2B (✓⇤  R) with ||✓S?||0 es

S?||0  es  we have supv2⌦  ||v||=1 v>(r2L(✓0)  r2L(✓))v  Ls⇤+2es||✓  ✓0||2

and ||✓0
⌦= {v | supp(v) ✓ (supp(✓) [ supp(✓0))}.
Assumption 2 guarantees that r2L(✓) is Lipschitz continuous within a neighborhood of ✓⇤ over a
sparse domain. The local restricted Hessian sm-
oothness is parallel to the local Hessian smooth-
ness for analyzing the proximal Newton method
in low dimensions [12].
In our analysis  we set the radius R as R :=
⇢s⇤+2es
is the radius of the
2Ls⇤+2es
region centered at the unique global minimizer
of (2) for quadratic convergence of the proximal
Newton algorithm. This is parallel to the radius
in low dimensions [12]  except that we restrict
the parameters over the sparse domain.
The third assumption requires the choice of tgt to be appropriate.
Assumption 3. Given the true modeling parameter ✓⇤  there exists a generic constant C1 such
n  4||rL(✓⇤)||1. Moreover  for a large enough n  we have ps⇤tgt 

Figure 2: An illustrative two dimensional example of
the restricted strong convexity. L(✓) is not strongly
convex. But if we restrict ✓ to be sparse (Black Curve) 
L(✓) behaves like a strongly convex function.

Restricted Strongly Convex

⇢s⇤+2es
Ls⇤+2es

  where 2R =

Nonstrongly Convex

2  where

that tgt = C1q log d
C2R⇢s⇤+2es.

Assumption 3 guarantees that the regularization is sufﬁciently large to eliminate irrelevant coordinates
such that the obtained solution is sufﬁciently sparse [4  22].
In addition  tgt can not be too
large  which guarantees that the estimator is close enough to the true model parameter. The above
assumptions are deterministic. We will verify them under GLM in the statistical analysis.
Our last assumption is on the predeﬁned precision parameter " as follows.
Assumption 4. For each stage of solving the convex relaxation subproblems (2) for all K  1  there
exists a generic constant C3 such that " satisﬁes " = C3pn  tgt
8 .
Assumption 4 guarantees that the output solutionb✓{K} at each stage for all K  1 has a sufﬁcient

precision  which is critical for our convergence analysis of multistage convex relaxation.

3.1 Computational Theory
We ﬁrst characterize the convergence for the ﬁrst stage of our proposed DC proximal Newton
algorithm  i.e.  the warm initialization for solving (3).
Theorem 4 (Warm Initialization  K = 1). Suppose that Assumptions 1 ⇠ 4 hold. After sufﬁciently
many iterations T < 1  the following results hold for all t  T :

||✓(t)  ✓⇤||2  R and F{1}(✓(t)) F {1}(✓⇤) +

5

 

152

tgts⇤

4⇢s⇤+2es

which further guarantee

⌘t = 1  ||✓(t)

Ls⇤+2es
2⇢s⇤+2es||✓(t)  ✓{1}||2
where ✓{1} is the unique sparse global minimizer of (3) satisfying ||✓{1}
Moreover  we need at most

S?||0 es and ||✓(t+1)  ✓{1}||2 
T + log log 3⇢+
" !
s⇤+2es

2 

S? ||0 es and !{1}(✓{1}) = 0.

iterations to terminate the proximal Newton algorithm for the warm initialization (3)  where the

output solutionb✓{1} satisﬁes

18tgtps⇤
⇢s⇤+2es

.

||b✓{1}
S? ||0 es  !{1}(b✓{1})  "  and ||b✓{1}  ✓⇤||2 

The proof of Theorem 4 is provided in Appendix C.1. Theorem 4 implies: (I) The objective value is
sufﬁciently small after ﬁnite T iterations of the proximal Newton algorithm  which further guarantees
sparse solutions and good computational performance in all follow-up iterations. (II) The solution
enters the ball B(✓⇤  R) after ﬁnite T iterations. Combined with the sparsity of the solution  it further
guarantees that the solution enters the region of quadratic convergence. Thus the backtracking line
search stops immediately and output ⌘t = 1 for all t  T . (III) The total number of iterations is at
most O(T + log log 1
" ) to achieve the approximate KKT condition !{1}(✓(t))  "  which serves as
the stopping criterion of the warm initialization (3).
Given these good properties of the output solutionb✓{1} obtained from the warm initialization  we can
further show that our proposed DC proximal Newton algorithm for all follow-up stages (i.e.  K  2)
achieves better computational performance than the ﬁrst stage. This is characterized by the following
theorem. For notational simplicity  we omit the iteration index {K} for the intermediate updates
within each stage for the multistage convex relaxation.
Theorem 5 (Stage K  K  2). Suppose Assumptions 1 ⇠ 4 hold. Then for all iterations t = 1  2  ...
within each stage K  2  we have
||✓(t)
S?||0 es
Ls⇤+2es
2⇢s⇤+2es||✓(t)  ✓{K}||2
log log 3⇢+
" !.
s⇤+2es
S? ||0 es  !{K}(b✓{K})  "  and

where ✓{K} is the unique sparse global minimizer of (2) at the K-th stage satisfying ||✓{K}
and !{K}(✓{K}) = 0. Moreover  we need at most

2  and F{K}(✓(t+1)) < F{K}(✓(t)) 
S? ||0 es

where the output solutionb✓{K} satisﬁes ||b✓{K}

iterations to terminate the proximal Newton algorithm for the K-th stage of convex relaxation (2) 

⌘t = 1  ||✓(t+1)  ✓{K}||2 

and ||✓(t)  ✓⇤||2  R 

which further guarantee

||b✓{K}  ✓⇤||2  C20@krL(✓⇤)Sk2 + tgtsXj2S
+ C30.7K1||b✓{1}  ✓⇤||2 

for some generic constants C2 and C3.

1(|✓⇤j| tgt)2 + "ps⇤1A

The proof of Theorem 5 is provided in Appendix C.2. A geometric interpretation for the computational
theory of local quadratic convergence for our proposed algorithm is provided in Figure 3. From the
second stage of convex relaxation (2)  i.e.  K  2  Theorem 5 implies: (I) Within each stage  the al-

6

Neighborhood of  : B( R)
Initial Solution for Warm Initialization

Output Solution for Warm Initialization {0}

Output Solution for the 2nd Stage

...

Output Solution for the Last Stage

Region of Quadratic Convergence

{1}
{2}



{K}

gorithm maintains a sparse solution
throughout all iterations t  1. The spar-
sity further guarantees that the SE property
and the restrictive Hessian smoothness hold 
which are necessary conditions for the fast
convergence of the proximal Newton algo-
rithm. (II) The solution is maintained in
the region B(✓⇤  R) for all t  1. Com-
bined with the sparsity of the solution  we
have that the solution enters the region of
quadratic convergence. This guarantees that
we only need to set the step size ⌘t = 1 and
the objective value is monotonely decreas-
ing without the sophisticated backtracking
line search procedure. Thus  the proximal
Newton algorithm enjoys the same fast con-
vergence as in low dimensional optimiza-
tion problems [12].

Figure 3: A geometric interpretation of local quadratic con-
vergence: the warm initialization enters the region of quadratic
convergence (orange region) after ﬁnite iterations and the
follow-up stages remain in the region of quadratic conver-

" ) to attain

gence. The ﬁnal estimatorb✓{eK} has a better estimation error
than the estimatorb✓{1} obtained from the convex warm ini-
tialization.
(III) With the quadratic convergence rate  the number of iterations is at most O(log log 1
the approximate KKT condition !{K}(✓(t))  "  which is the stopping criteria at each stage.
3.2 Statistical Theory
Recall that our computational theory relies on deterministic assumptions (Assumptions 1 ⇠ 3).
However  these assumptions involve data  which are sampled from certain statistical distribution.
Therefore  we need to verify that these assumptions hold with high probability under mild data
generation process of (i.e.  GLM) in high dimensions in the following lemma.
Lemma 6. Suppose that xi’s are i.i.d. sampled from a zero-mean distribution with covariance matrix
Cov(xi) =⌃ such that 1 > cmax  ⇤max(⌃)  ⇤min(⌃)  cmin > 0  and for any v 2 Rd 
v>xi is sub-Gaussian with variance at most a||v||2
2  where cmax  cmin  and a are generic constants.
Moreover  for some constant M > 0  at least one of the following two conditions holds: (I) The
Hessian of the cumulant function is uniformly bounded: || 00||1  M   or (II) The covariates are
bounded ||xi||1  1  and E[max|u|1[ 00(x>✓⇤) + u]p]  M for some p > 2. Then Assumption 1
⇠ 3 hold with high probability.
The proof of Lemma 6 is provided in Appendix F. Given that these assumptions hold with high
probability  we know that the proximal Newton algorithm attains quadratic rate convergence within
each stage of convex relaxation with high probability. Then we establish the statistical rate of
convergence for the obtained estimator in parameter estimation.
Theorem 7. Suppose the observations are generated from GLM satisfying the condition in Lemma 6
for large enough n such that n  C4s⇤ log d and  = C5/cmin is a constant deﬁned in Section 2 for
generic constants C4 and C5  then with high probability  the output solutionb✓{K} satisﬁes
+r s0 log d
for generic constants C6 and C7  where s0 =Pj2S

||b✓{K}  ✓⇤||2  C6 r s⇤

n ! + C70.7K r s⇤ log d
n !

Theorem 7 is a direct result combining Theorem 5 and the analysis in [40]. As can be seen  s0
is essentially the number of nonzero ✓j’s with smaller magnitudes than tgt  which are often
considered as “weak” signals. Theorem 7 essentially implies that by exploiting the multi-stage convex
relaxation framework  our proposed DC proximal Newton algorithm gradually reduces the estimation
bias for “strong” signals  and eventually obtains an estimator with better statistical properties than

1(|✓⇤j| tgt)).

the `1-regularized estimator. Speciﬁcally  let eK be the smallest integer such that after eK stages of
convex relaxation we have C70.7eK✓q s⇤ log d
n   which is equivalent
to requiring eK = O(log log d). This implies the total number of the proximal Newton updates being
at most OT + log log 1
" · (1 + log log d). In addition  the obtained estimator attains the optimal

n ◆  C6 max⇢q s⇤

n  q s0 log d

n

7

statistical properties in parameter estimation:

||b✓{eK}  ✓⇤||2 O P✓q s⇤

n +q s0 log d

n ◆ v.s.

||b✓{1}  ✓⇤||2 O P✓q s⇤ log d
n ◆ .

(6)

n

2







}

n

{

K



r
o
r
r
E
n
o
i
t
a
m

i
t
s
E

Percentage of Strong Signals ss
s

Oracle Bound: OP s
n

Recall thatb✓{1} is obtained by the warm initialization (3). As illustrated in Figure 3  this implies
the statistical rate in (6) for ||b✓{eK}  ✓⇤||2 obtained from the multistage convex relaxation for the
nonconvex regularized problem (1) is a signiﬁcant improvement over ||b✓{1}  ✓⇤||2 obtained from
result approaches the oracle bound3 OP⇣q s⇤

the convex problem (3). Especially when s0 is small  i.e.  most of nonzero ✓j’s are strong signals  our

n⌘ [8] as illustrated in Figure 4.

Slow Bound: Convex OP slogd

OP r s⇤
n !
+r s0log d
FastBound:Nonconvex

4 Experiments
We compare our DC Proximal Newton (DC+PN) algo-
rithm with two competing algorithms for solving the
nonconvex regularized sparse logistic regression prob-
lem. They are accelerated proximal gradient algorithm
(APG) implemented in the SPArse Modeling Software
(SPAMS  coded in C++ [18])  and accelerated coordinate
descent (ACD) algorithm implemented in R package
gcdnet (coded in Fortran  [36]). We further optimize
the active set strategy in gcdnet to boost its computa-
tional performance. To integrate these two algorithms
with the multistage convex relaxation framework  we
revise their source code.
To further boost the computational efﬁciency at each
stage of the convex relaxation  we apply the pathwise
optimization [10] for all algorithms in practice. Speciﬁ-
cally  at each stage  we use a geometrically decreas-
ing sequence of regularization parameters {[m] =
m=1  where [0] is the smallest value such that
↵m[0]}M
the corresponding solution is zero  ↵ 2 (0  1) is a shrin-
kage parameter and tgt = [M ]. For each [m]  we apply the corresponding algorithm (DC+PN 
DC+APG  and DC+ACD) to solve the nonconvex regularized problem (1). Moreover  we initialize
the solution for a new regularization parameter [m+1] using the output solution obtained with [m].
Such a pathwise optimization scheme has achieved tremendous success in practice [10  15  42]. We
refer [43] for detailed discussion of pathwise optimization.
Our comparison contains 3 datasets: “madelon” (n = 2000  d = 500  [11])  “gisette” (n = 2000 d =
5000  [11])  and three simulated datasets: “sim_1k” (d=1000)  “sim_5k” (d=5000)  and “sim_10k”

Figure 4: An illustration of the statistical rates
of convergence in parameter estimation. Our
obtained estimator has an error bound between
the oracle bound and the slow bound from the
convex problem in general. When the percentage
of strong signals increases  i.e.  s0 decreases 
then our result approaches the oracle bound.

(d=10000) with the sample size n = 1000 for all three datasets. We set tgt = 0.25plog d/n and
 = 0.2 for all settings here. We generate each row of the design matrix X independently from a
d-dimensional normal distribution N (0  ⌃)  where ⌃jk = 0.5|jk| for j  k = 1  ...  d. We generate
y ⇠ Bernoulli(1/[1 + exp(X✓⇤)])  where ✓⇤ has all 0 entries except randomly selected 20 entries.
These nonzero entries are independently sampled from Uniform(0  1). The stopping criteria for all
algorithms are tuned such that they attain similar optimization errors.
All three algorithms are compared in wall clock time. Our DC+PN algorithm is implemented in
C with double precisions and called from R by a wrapper. All experiments are performed on a
computer with 2.6GHz Intel Core i7 and 16GB RAM. For each algorithm and dataset  we repeat
the algorithm 10 times and report the average value and standard deviation of the wall clock time in
Table 1. As can be seen  our DC+PN algorithm signiﬁcantly outperforms the competing algorithms.
We remark that for increasing d  the superiority of DC+PN over DC+ACD becomes less signiﬁcant
as the Newton method is more sensitive to ill conditioned problems. This can be mitigated by using a
denser sequence of {[m]} along the solutions path.
We then illustrate the quadratic convergence of our DC+PN algorithm within each stage of the
convex relaxation using the “sim” dataset. Speciﬁcally  we plot the gap towards the optimal objective
3The oracle bound assumes that we know which variables are relevant in advance. It is not a realistic bound 

but only for comparison purpose.

8

F{K}(✓{K}) of the K-th stage versus the wall clock time in Figure 5. We see that our proposed DC
proximal Newton algorithm achieves quadratic convergence  which is consistent with our theory.
Table 1: Quantitive timing comparisons for the nonconvex regularized sparse logistic regression. The average
values and the standard deviations (in parenthesis) of the timing performance (in seconds) over 10 random trials
are presented.

DC+PN

DC+ACD

DC+APG

madelon

1.51(±0.01)s
obj value: 0.52
5.83(±0.03)s
obj value: 0.52
1.60(±0.03)s
obj value: 0.52

gisette

5.35(±0.11)s
obj value: 0.01
18.92(±2.25)s
obj value: 0.01
207(±2.25)s
obj value: 0.01

sim_1k

1.07(±0.02)s
obj value: 0.01
9.46(±0.09) s
obj value: 0.01
17.8(±1.23) s
obj value: 0.01

sim_5k

4.53(±0.06)s
obj value: 0.01
16.20(±0.24) s
obj value: 0.01
111(±1.28) s
obj value: 0.01

sim_10k

8.82(±0.04)s
obj value: 0.01
19.1(±0.56) s
obj value: 0.01
222(±5.79) s
obj value: 0.01

(a) Simulated Data

(b) Gissete Data

Figure 5: Timing comparisons in wall clock time. Our proposed DC proximal Newton algorithm demonstrates
superior quadratic convergence and signiﬁcantly outperforms the DC proximal gradient algorithm.

5 Discussions
We provide further discussions on the superior performance of our DC proximal Newton. There exist
two major drawbacks of existing multi-stage convex relaxation based ﬁrst order algorithms:
(I) The ﬁrst order algorithms have signiﬁcant computational overhead in each iteration. For example 
for GLM  computing gradients requires frequently evaluating the cumulant function and its derivatives.
This often involves extensive non-arithmetic operations such as log(·) and exp(·) functions  which
naturally appear in the cumulant function and its derivates  are computationally expensive. To the best
of our knowledge  even if we use some efﬁcient numerical methods for calculating exp(·) in [28  19] 
the computation still need at least 10  30 times more CPU cycles than basic arithmetic operations 
e.g.  multiplications. Our proposed DC Proximal Newton algorithm cannot avoid calculating the
cumulant function and its derivatives  when computing quadratic approximations. The computation 
however  is much less intense  since the convergence is quadratic.
(II) The ﬁrst order algorithms are computationally expensive with the step size selection. Although
for certain GLM  e.g.  sparse logistic regression  we can choose the step size parameter as ⌘ =
i=1 xix>i )  such a step size often leads to poor empirical performance. In contrast  as
4⇤1
our theoretical analysis and experiments suggest  the proposed DC proximal Newton algorithm needs
very few line search steps  which saves much computational effort.
Some recent works on proximal Newton or inexact proximal Newton also demonstrate local quadratic
convergence guarantees [37  38]. However  the conditions there are much more stringent than the
SE property in terms of the dependence on the problem dimensions. Speciﬁcally  their quadratic
convergence can only be guaranteed in a much smaller neighborhood. For example  the constant

nPn

max( 1

in our analysis  is as small as 1
[6]. Therefore  instead of a constant radius as in our analysis  they can only guarantee the quadratic

nullspace strong convexity in [37]  which plays the rule as the smallest sparse eigenvalue ⇢s⇤+2es
d. Note that ⇢s⇤+2es can be (almost) independent of d in our case
convergence in a region with radius O 1
d  which is very small in high dimensions. A similar issue

exists in [38] that the region of quadratic convergence is too small.

9

References
[1] Larry Armijo. Minimization of functions having lipschitz continuous ﬁrst partial derivatives. Paciﬁc

Journal of Mathematics  16(1):1–3  1966.

[2] Amir Beck and Marc Teboulle. Fast gradient-based algorithms for constrained total variation image
denoising and deblurring problems. IEEE Transactions on Image Processing  18(11):2419–2434  2009.

[3] Alexandre Belloni  Victor Chernozhukov  and Lie Wang. Square-root lasso: pivotal recovery of sparse

signals via conic programming. Biometrika  98(4):791–806  2011.

[4] Peter J Bickel  Yaacov Ritov  and Alexandre B Tsybakov. Simultaneous analysis of Lasso and Dantzig

selector. The Annals of Statistics  37(4):1705–1732  2009.

[5] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press  2004.

[6] Peter Bühlmann and Sara Van De Geer. Statistics for high-dimensional data: methods  theory and

applications. Springer Science & Business Media  2011.

[7] Ani Eloyan  John Muschelli  Mary Beth Nebel  Han Liu  Fang Han  Tuo Zhao  Anita D Barber  Suresh
Joel  James J Pekar  Stewart H Mostofsky  et al. Automated diagnoses of attention deﬁcit hyperactive
disorder using magnetic resonance imaging. Frontiers in Systems Neuroscience  6  2012.

[8] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties.

Journal of the American Statistical Association  96(456):1348–1360  2001.

[9] Jianqing Fan  Han Liu  Qiang Sun  and Tong Zhang. TAC for sparse learning: Simultaneous control of

algorithmic complexity and statistical error. arXiv preprint arXiv:1507.01037  2015.

[10] Jerome Friedman  Trevor Hastie  and Rob Tibshirani. Regularization paths for generalized linear models

via coordinate descent. Journal of Statistical Software  33(1):1  2010.

[11] Isabelle Guyon  Steve Gunn  Asa Ben-Hur  and Gideon Dror. Result analysis of the nips 2003 feature

selection challenge. In Advances in neural information processing systems  pages 545–552  2005.

[12] Jason D Lee  Yuekai Sun  and Michael A Saunders. Proximal newton-type methods for minimizing

composite functions. SIAM Journal on Optimization  24(3):1420–1443  2014.

[13] Xingguo Li  Jarvis Haupt  Raman Arora  Han Liu  Mingyi Hong  and Tuo Zhao. A ﬁrst order free lunch

for sqrt-lasso. arXiv preprint arXiv:1605.07950  2016.

[14] Xingguo Li  Tuo Zhao  Raman Arora  Han Liu  and Jarvis Haupt. Stochastic variance reduced optimization
for nonconvex sparse learning. In International Conference on Machine Learning  pages 917–925  2016.

[15] Xingguo Li  Tuo Zhao  Tong Zhang  and Han Liu. The picasso package for nonconvex regularized

m-estimation in high dimensions in R. Technical Report  2015.

[16] Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and

algorithmic theory for local optima. Journal of Machine Learning Research  2015. to appear.

[17] Zhi-Quan Luo and Paul Tseng. On the linear convergence of descent methods for convex essentially

smooth minimization. SIAM Journal on Control and Optimization  30(2):408–425  1992.

[18] Julien Mairal  Francis Bach  Jean Ponce  et al. Sparse modeling for image and vision processing. Founda-

tions and Trends R in Computer Graphics and Vision  8(2-3):85–283  2014.

[19] A Cristiano I Malossi  Yves Ineichen  Costas Bekas  and Alessandro Curioni. Fast exponential computation

on simd architectures. Proc. of HIPEAC-WAPCO  Amsterdam NL  2015.

[20] Peter McCullagh. Generalized linear models. European Journal of Operational Research  16(3):285–292 

1984.

[21] Benjamin M Neale  Yan Kou  Li Liu  Avi Ma’Ayan  Kaitlin E Samocha  Aniko Sabo  Chiao-Feng Lin 
Christine Stevens  Li-San Wang  Vladimir Makarov  et al. Patterns and rates of exonic de novo mutations
in autism spectrum disorders. Nature  485(7397):242–245  2012.

[22] Sahand N Negahban  Pradeep Ravikumar  Martin J Wainwright  and Bin Yu. A uniﬁed framework for high-
dimensional analysis of m-estimators with decomposable regularizers. Statistical Science  27(4):538–557 
2012.

10

[23] Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming 

140(1):125–161  2013.

[24] Yang Ning  Tianqi Zhao  and Han Liu. A likelihood ratio framework for high dimensional semiparametric

regression. arXiv preprint arXiv:1412.2295  2014.

[25] Johann Pfanzagl. Parametric statistical theory. Walter de Gruyter  1994.

[26] Maxim Raginsky  Rebecca M Willett  Zachary T Harmany  and Roummel F Marcia. Compressed sensing
performance bounds under poisson noise. IEEE Transactions on Signal Processing  58(8):3990–4002 
2010.

[27] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Restricted eigenvalue properties for correlated

Gaussian designs. Journal of Machine Learning Research  11(8):2241–2259  2010.

[28] Nicol N Schraudolph. A fast  compact approximation of the exponential function. Neural Computation 

11(4):853–862  1999.

[29] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for `1-regularized loss minimization. Journal

of Machine Learning Research  12:1865–1892  2011.

[30] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  pages 267–288  1996.

[31] Robert Tibshirani  Jacob Bien  Jerome Friedman  Trevor Hastie  Noah Simon  Jonathan Taylor  and Ryan J
Tibshirani. Strong rules for discarding predictors in Lasso-type problems. Journal of the Royal Statistical
Society: Series B (Statistical Methodology)  74(2):245–266  2012.

[32] Sara A van de Geer. High-dimensional generalized linear models and the Lasso. The Annals of Statistics 

36(2):614–645  2008.

[33] Sara A van de Geer and Peter Bühlmann. On the conditions used to prove oracle results for the Lasso.

Electronic Journal of Statistics  3:1360–1392  2009.

[34] Zhaoran Wang  Han Liu  and Tong Zhang. Optimal computational and statistical rates of convergence for

sparse nonconvex learning problems. The Annals of Statistics  42(6):2164–2201  2014.

[35] Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares problem.

SIAM Journal on Optimization  23(2):1062–1091  2013.

[36] Yi Yang and Hui Zou. An efﬁcient algorithm for computing the hhsvm and its generalizations. Journal of

Computational and Graphical Statistics  22(2):396–415  2013.

[37] Ian En-Hsu Yen  Cho-Jui Hsieh  Pradeep K Ravikumar  and Inderjit S Dhillon. Constant nullspace strong
convexity and fast convergence of proximal methods under high-dimensional settings. In Advances in
Neural Information Processing Systems  pages 1008–1016  2014.

[38] Man-Chung Yue  Zirui Zhou  and Anthony Man-Cho So. Inexact regularized proximal newton method:
provable convergence guarantees for non-smooth convex minimization without strong convexity. arXiv
preprint arXiv:1605.07522  2016.

[39] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of

Statistics  38(2):894–942  2010.

[40] Tong Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine

Learning Research  11:1081–1107  2010.

[41] Tong Zhang et al. Multi-stage convex relaxation for feature selection. Bernoulli  19(5B):2277–2293  2013.

[42] Tuo Zhao  Han Liu  Kathryn Roeder  John Lafferty  and Larry Wasserman. The huge package for high-
dimensional undirected graph estimation in R. Journal of Machine Learning Research  13:1059–1062 
2012.

[43] Tuo Zhao  Han Liu  and Tong Zhang. Pathwise coordinate optimization for sparse learning: Algorithm and

theory. arXiv preprint arXiv:1412.7477  2014.

[44] Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv preprint

arXiv:0912.4045  2009.

11

,Qingqing Huang
Sham Kakade
Xingguo Li
Lin Yang
Jason Ge
Jarvis Haupt
Tong Zhang
Tuo Zhao