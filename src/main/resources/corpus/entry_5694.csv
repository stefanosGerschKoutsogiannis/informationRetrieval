2014,Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets,Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms. However  in large-scale applications  the complexity of the regularizers entails great computational challenges. In this paper  we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features  respectively  which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge  TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover  TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.,Two-Layer Feature Reduction for Sparse-Group

Lasso via Decomposition of Convex Sets

Jie Wang  Jieping Ye

Computer Science and Engineering

Arizona State University  Tempe  AZ 85287

{jie.wang.ustc  jieping.ye}@asu.edu

Abstract

Sparse-Group Lasso (SGL) has been shown to be a powerful regression tech-
nique for simultaneously discovering group and within-group sparse patterns by
using a combination of the (cid:96)1 and (cid:96)2 norms. However  in large-scale applications 
the complexity of the regularizers entails great computational challenges. In this
paper  we propose a novel two-layer feature reduction method (TLFre) for SGL
via a decomposition of its dual feasible set. The two-layer reduction is able to
quickly identify the inactive groups and the inactive features  respectively  which
are guaranteed to be absent from the sparse representation and can be removed
from the optimization. Existing feature reduction methods are only applicable
for sparse models with one sparsity-inducing regularizer. To our best knowledge 
TLFre is the ﬁrst one that is capable of dealing with multiple sparsity-inducing
regularizers. Moreover  TLFre has a very low computational cost and can be inte-
grated with any existing solvers. Experiments on both synthetic and real data sets
show that TLFre improves the efﬁciency of SGL by orders of magnitude.

Introduction

1
Sparse-Group Lasso (SGL) [5  16] is a powerful regression technique in identifying important
groups and features simultaneously. To yield sparsity at both group and individual feature levels 
SGL combines the Lasso [18] and group Lasso [28] penalties. In recent years  SGL has found great
success in a wide range of applications  including but not limited to machine learning [20  27]  sig-
nal processing [17]  bioinformatics [14] etc. Many research efforts have been devoted to developing
efﬁcient solvers for SGL [5  16  10  21]. However  when the feature dimension is extremely high 
the complexity of the SGL regularizers imposes great computational challenges. Therefore  there
is an increasingly urgent need for nontraditional techniques to address the challenges posed by the
massive volume of the data sources.
Recently  El Ghaoui et al. [4] proposed a promising feature reduction method  called SAFE screen-
ing  to screen out the so-called inactive features  which have zero coefﬁcients in the solution  from
the optimization. Thus  the size of the data matrix needed for the training phase can be signiﬁcantly
reduced  which may lead to substantial improvement in the efﬁciency of solving sparse models.
Inspired by SAFE  various exact and heuristic feature screening methods have been proposed for
many sparse models such as Lasso [25  11  19  26]  group Lasso [25  22  19]  etc. It is worthwhile
to mention that the discarded features by exact feature screening methods such as SAFE [4]  DOME
[26] and EDPP [25] are guaranteed to have zero coefﬁcients in the solution. However  heuristic fea-
ture screening methods like Strong Rule [19] may mistakenly discard features which have nonzero
coefﬁcients in the solution. More recently  the idea of exact feature screening has been extended
to exact sample screening  which screens out the nonsupport vectors in SVM [13  23] and LAD
[23]. As a promising data reduction tool  exact feature/sample screening would be of great practical
importance because they can effectively reduce the data size without sacriﬁcing the optimality [12].

1

However  all of the existing feature/sample screening methods are only applicable for the sparse
models with one sparsity-inducing regularizer. In this paper  we propose an exact two-layer feature
screening method  called TLFre  for the SGL problem. The two-layer reduction is able to quickly
identify the inactive groups and the inactive features  respectively  which are guaranteed to have zero
coefﬁcients in the solution. To the best of our knowledge  TLFre is the ﬁrst screening method which
is capable of dealing with multiple sparsity-inducing regularizers.
We note that most of the existing exact feature screening methods involve an estimation of the dual
optimal solution. The difﬁculty in developing screening methods for sparse models with multiple
sparsity-inducing regularizers like SGL is that the dual feasible set is the sum of simple convex
sets. Thus  to determine the feasibility of a given point  we need to know if it is decomposable with
respect to the summands  which is itself a nontrivial problem (see Section 2). One of our major
contributions is that we derive an elegant decomposition method of any dual feasible solutions of
SGL via the framework of Fenchel’s duality (see Section 3). Based on the Fenchel’s dual problem
of SGL  we motivate TLFre by an in-depth exploration of its geometric properties and the optimality
conditions. We derive the set of the regularization parameter values corresponding to zero solutions.
To develop TLFre  we need to estimate the upper bounds involving the dual optimal solution. To this
end  we ﬁrst give an accurate estimation of the dual optimal solution via the normal cones. Then 
we formulate the estimation of the upper bounds via nonconvex optimization problems. We show
that these nonconvex problems admit closed form solutions. Experiments on both synthetic and real
data sets demonstrate that the speedup gained by TLFre in solving SGL can be orders of magnitude.
All proofs are provided in the long version of this paper [24].
Notation: Let (cid:107)·(cid:107)1  (cid:107)·(cid:107) and (cid:107)·(cid:107)∞ be the (cid:96)1  (cid:96)2 and (cid:96)∞ norms  respectively. Denote by Bn
1   Bn  and
Bn∞ the unit (cid:96)1  (cid:96)2  and (cid:96)∞ norm balls in Rn (we omit the superscript if it is clear from the context).
For a set C  let intC be its interior. If C is closed and convex  we deﬁne the projection operator as
PC(w) := argminu∈C(cid:107)w − u(cid:107). We denote by IC(·) the indicator function of C  which is 0 on C and
∞ elsewhere. Let Γ0(Rn) be the class of proper closed convex functions on Rn. For f ∈ Γ0(Rn) 
let ∂f be its subdifferential. The domain of f is the set dom f := {w : f (w) < ∞}. For w ∈ Rn 
let [w]i be its ith component. For γ ∈ R  let sgn(γ) = sign(γ) if γ (cid:54)= 0  and sgn(0) = 0. We deﬁne

(cid:26)

SGN(w) =

s ∈ Rn : [s]i ∈

(cid:26)sign([w]i) 

[−1  1] 

if [w]i (cid:54)= 0;
if [w]i = 0.

(cid:27)

We denote by γ+ = max(γ  0). Then  the shrinkage operator Sγ(w) : Rn → Rn with γ ≥ 0 is

[Sγ(w)]i = (|[w]i| − γ)+sgn([w]i)  i = 1  . . .   n.

(1)

2 Basics and Motivation
In this section  we brieﬂy review some basics of SGL. Let y ∈ RN be the response vector and
X ∈ RN×p be the matrix of features. With the group information available  the SGL problem [5] is

min
β∈Rp

(2)
where ng is the number of features in the gth group  Xg ∈ RN×ng denotes the predictors in that
group with the corresponding coefﬁcient vector βg  and λ1  λ2 are positive regularization parame-
ters. Without loss of generality  let λ1 = αλ and λ2 = λ with α > 0. Then  problem (2) becomes:

+ λ1

g=1

Xgβg

ng(cid:107)βg(cid:107) + λ2(cid:107)β(cid:107)1 

1
2

g=1

(cid:13)(cid:13)(cid:13)(cid:13)y −(cid:88)G
(cid:13)(cid:13)(cid:13)(cid:13)y −(cid:88)G
(cid:13)(cid:13) y
λ − θ(cid:13)(cid:13)2

g=1

2

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:88)G

√

(cid:18)

(cid:88)G

g=1

√

(cid:19)

.

(cid:111)

.

(3)

(4)

Xgβg

+ λ

α

√

ng(cid:107)βg(cid:107) + (cid:107)β(cid:107)1

1
2

min
β∈Rp

(cid:110) 1
2(cid:107)y(cid:107)2 − 1

sup

θ

By the Lagrangian multipliers method [24]  the dual problem of SGL is

: XT

g θ ∈ Dα

g := α

ngB + B∞  g = 1  . . .   G

It is well-known that the dual feasible set of Lasso is the intersection of closed half spaces (thus a
polytope); for group Lasso  the dual feasible set is the intersection of ellipsoids. Surprisingly  the
geometric properties of these dual feasible sets play fundamentally important roles in most of the
existing screening methods for sparse models with one sparsity-inducing regularizer [23  11  25  4].
When we incorporate multiple sparse-inducing regularizers to the sparse models  problem (4) indi-
cates that the dual feasible set can be much more complicated. Although (4) provides a geometric

2

√

g θ ∈ Dα

g θ = b1 + b2  with one belonging to α

description of the dual feasible set of SGL  it is not suitable for further analysis. Notice that  even
the feasibility of a given point θ is not easy to determine  since it is nontrivial to tell if XT
g θ can
be decomposed into b1 + b2 with b1 ∈ α
ngB and b2 ∈ B∞. Therefore  to develop screening
methods for SGL  it is desirable to gain deeper understanding of the sum of simple convex sets.
In the next section  we analyze the dual feasible set of SGL in depth via the Fenchel’s Duality
Theorem. We show that for each XT
g   Fenchel’s duality naturally leads to an explicit decom-
ngB and the other one belonging to B∞. This
position XT
lays the foundation of the proposed screening method for SGL.
3 The Fenchel’s Dual Problem of SGL
In Section 3.1  we derive the Fenchel’s dual of SGL via Fenchel’s Duality Theorem. We then
motivate TLFre and sketch our approach in Section 3.2. In Section 3.3  we discuss the geometric
properties of the Fenchel’s dual of SGL and derive the set of (λ  α) leading to zero solutions.
3.1 The Fenchel’s Dual of SGL via Fenchel’s Duality Theorem
To derive the Fenchel’s dual problem of SGL  we need the Fenchel’s Duality Theorem as stated in
Theorem 1. The conjugate of f ∈ Γ0(Rn) is the function f∗ ∈ Γ0(Rn) deﬁned by

√

f∗(z) = supw (cid:104)w  z(cid:105) − f (w).

Theorem 1. [Fenchel’s Duality Theorem] Let f ∈ Γ0(RN )  Ω ∈ Γ0(Rp)  and T (β) = y − Xβ
be an afﬁne mapping from Rp to RN . Let p∗  d∗ ∈ [−∞ ∞] be primal and dual values deﬁned 
respectively  by the Fenchel problems:

p∗ = inf β∈Rp f (y − Xβ) + λΩ(β); d∗ = supθ∈RN −f∗(λθ) − λΩ∗(XT θ) + λ(cid:104)y  θ(cid:105).
One has p∗ ≥ d∗. If  furthermore  f and Ω satisfy the condition 0 ∈ int (dom f − y + Xdom Ω) 
then the equality holds  i.e.  p∗ = d∗  and the supreme is attained in the dual problem if ﬁnite.
We omit the proof of Theorem 1 since it is a slight modiﬁcation of Theorem 3.3.5 in [2].
2(cid:107)w(cid:107)2  and λΩ(β) be the second term in (3). Then  SGL can be written as
Let f (w) = 1

minβ f (y − Xβ) + λΩ(β).

To derive the Fenchel’s dual problem of SGL  Theorem 1 implies that we need to ﬁnd f∗ and Ω∗. It
2(cid:107)z(cid:107)2. Therefore  we only need to ﬁnd Ω∗  where the concept inﬁmal
is well-known that f∗(z) = 1
convolution is needed. Let h  g ∈ Γ0(Rn). The inﬁmal convolution of h and g is deﬁned by

(h(cid:3)g)(ξ) = inf η h(η) + g(ξ − η) 

g=1

√

g = α

1 (β) = α(cid:80)G
1 )∗ (cid:0) (Ω2)∗) (ξ) =(cid:80)G

(ξg)  

and it is exact at a point ξ if there exists a η∗(ξ) such that (h(cid:3)g)(ξ) = h(η∗(ξ)) + g(ξ − η∗(ξ)).
h(cid:3)g is exact if it is exact at every point of its domain  in which case it is denoted by h (cid:0) g.
Lemma 2. Let Ωα
√
over  let Cα

ng(cid:107)βg(cid:107)  Ω2(β) = (cid:107)β(cid:107)1 and Ω(β) = Ωα

1 (β) + Ω2(β). More-

1 )∗(ξ) =(cid:80)G

ngB ⊂ Rng  g = 1  . . .   G. Then  the following hold:
g=1 ICα

(Ω2)∗(ξ) =(cid:80)G

g=1 IB∞ (ξg) 

g=1 IB

(i): (Ωα
(ii): Ω∗(ξ) = ((Ωα
where ξg ∈ Rng is the sub-vector of ξ corresponding to the gth group.
Note that PB∞(ξg) admits a closed form solution  i.e.  [PB∞ (ξg)]i = sgn ([ξg]i) min (|[ξg]i|   1).
Combining Theorem 1 and Lemma 2  the Fenchel’s dual of SGL can be derived as follows.
Theorem 3. For the SGL problem in (3)  the following hold:
(i): The Fenchel’s dual of SGL is given by:

(cid:16) ξg−PB∞ (ξg)

(cid:17)

√
α

ng

 

g

λ − θ(cid:107)2 − 1

g θ − PB∞(XT

inf
θ

(5)
(ii): Let β∗(λ  α) and θ∗(λ  α) be the optimal solutions of problems (3) and (5)  respectively. Then 
(6)
(7)

λθ∗(λ  α) =y − Xβ∗(λ  α) 
g θ∗(λ  α) ∈α
XT

g (λ  α)(cid:107)1  g = 1  . . .   G.

g (λ  α)(cid:107) + ∂(cid:107)β∗

ng∂(cid:107)β∗

√

(cid:8) 1
2(cid:107) y

2(cid:107)y(cid:107)2 :(cid:13)(cid:13)XT

g θ)(cid:13)(cid:13) ≤ α

ng  g = 1  . . .   G(cid:9) .

√

3

Remark 1. We note that the shrinkage operator can also be expressed by

(8)

Therefore  problem (5) can be written more compactly as

Sγ(w) = w − PγB∞(w)  γ ≥ 0.

2(cid:107)y(cid:107)2 :(cid:13)(cid:13)S1(XT

g θ)(cid:13)(cid:13) ≤ α

√

ng  g = 1  . . .   G(cid:9) .

λ − θ(cid:107)2 − 1

(cid:8) 1
2(cid:107) y

inf
θ

(9)
Remark 2. Eq. (6) and Eq. (7) can be obtained by the Fenchel-Young inequality [2  24]. They
are the so-called KKT conditions [3] and can also be obtained by the Lagrangian multiplier method
[24]. Moreover  for the SGL problem  its Lagrangian dual in (4) and Fenchel’s dual in (5) are indeed
equivalent to each other [24].
Remark 3. An appealing advantage of the Fenchel’s dual in (5) is that we have a natural decompo-
sition of all points ξg ∈ Dα
g . As a
result  this leads to a convenient way to determine the feasibility of any dual variable θ by checking
if S1(XT
3.2 Motivation of the Two-Layer Screening Rules
We motive the two-layer screening rules via the KKT condition in Eq. (7). As implied by the name 
there are two layers in our method. The ﬁrst layer aims to identify the inactive groups  and the
second layer is designed to detect the inactive features for the remaining groups.
by Eq. (7)  we have the following cases by noting ∂(cid:107)w(cid:107)1 = SGN(w) and

g : ξg = PB∞ (ξg) +S1(ξg)) with PB∞(ξg) ∈ B∞ and S1(ξg) ∈ Cα

g   g = 1  . . .   G.

g θ) ∈ Cα

(cid:40)(cid:110) w(cid:107)w(cid:107)

(cid:111)

(10)

(11)

(12)

(13)

(R1)

(R2)

if w (cid:54)= 0 
if w = 0.

 

[XT

Case 1. If β∗

{u : (cid:107)u(cid:107) ≤ 1} 

∂(cid:107)w(cid:107) =
(cid:40)
g (λ  α) (cid:54)= 0  we have
√
[β∗
g (λ α)(cid:107) + sign([β∗
g θ∗(λ  α)]i ∈
ng
α
(cid:107)β∗
[−1  1] 
In view of Eq. (10)  we can see that
g θ∗(λ  α)) = α
g θ∗(λ  α]i
g (λ  α) = 0  we have

(cid:12)(cid:12) ≤ 1 then [β∗

(b): If (cid:12)(cid:12)[XT

Case 2. If β∗

(a): S1(XT

g (λ α)]i

√

ng

g (λ  α)]i) 

if [β∗
if [β∗

g (λ  α)]i (cid:54)= 0 
g (λ  α)]i = 0.

β∗
g (λ1 λ2)(cid:107) and (cid:107)S1(XT
g (λ1 λ2)
(cid:107)β∗
g (λ  α)]i = 0.

g θ∗(λ  α))(cid:107) = α

√

ng 

[XT

g θ∗(λ  α)]i ∈ α

ng[ug]i + [−1  1]  (cid:107)ug(cid:107) ≤ 1.

√

The ﬁrst layer (group-level) of TLFre From (11) in Case 1  we have

ng ⇒ β∗

g (λ  α) = 0.

Clearly  (R1) can be used to identify the inactive groups and thus a group-level screening rule.
The second layer (feature-level) of TLFre Let xgi be the ith column of Xg. We have
[XT

θ∗(λ  α). In view of (12) and (13)  we can see that

g θ∗(λ  α)]i = xT

gi

√

g θ∗(λ  α))(cid:13)(cid:13) < α
(cid:13)(cid:13)S1(XT
(cid:12)(cid:12)xT
θ∗(λ  α)(cid:12)(cid:12) ≤ 1 ⇒ [β∗

gi

g (λ  α)]i = 0.

Different from (R1)  (R2) detects the inactive features and thus it is a feature-level screening rule.
However  we cannot directly apply (R1) and (R2) to identify the inactive groups/features because
both need to know θ∗(λ  α).
Inspired by the SAFE rules [4]  we can ﬁrst estimate a region Θ
g θ : θ ∈ Θ}. Then  (R1) and (R2) can be relaxed as follows:
containing θ∗(λ  α). Let XT
(R1∗)
(R2∗)

(cid:8)(cid:107)S1(ξg)(cid:107) : ξg ∈ Ξg ⊇ XT
(cid:8)(cid:12)(cid:12)xT
θ(cid:12)(cid:12) : θ ∈ Θ(cid:9) ≤ 1 ⇒ [β∗

g Θ(cid:9) < α

g Θ = {XT

ng ⇒ β∗

g (λ  α) = 0 

g (λ  α)]i = 0.

supξg
supθ

√

Inspired by (R1∗) and (R2∗)  we develop TLFre via the following three steps:
Step 1. Given λ and α  we estimate a region Θ that contains θ∗(λ  α).
Step 2. We solve for the supreme values in (R1∗) and (R2∗).
Step 3. By plugging in the supreme values from Step 2  (R1∗) and (R2∗) result in the desired

gi

two-layer screening rules for SGL.

4

3.3 The Set of Parameter Values Leading to Zero Solution
√
For notational convenience  let F α
ng}  g = 1  . . .   G; and thus the
g θ)(cid:107) ≤ α
feasible set of the Fenchel’s dual of SGL is F α = ∩g=1 ... G F α
g . In view of problem (5) [or (9)] 
we can see that θ∗(λ  α) is the projection of y/λ on F α  i.e.  θ∗(λ  α) = PF α (y/λ). Thus  if
y/λ ∈ F α  we have θ∗(λ  α) = y/λ. Moreover  by (R1)  we can see that β∗(λ  α) = 0 if y/λ is an
interior point of F α. Indeed  we have the following stronger result.

g = {θ : (cid:107)S1(XT

max = maxg {ρg :(cid:13)(cid:13)S1(XT

g y/ρg)(cid:13)(cid:13) = α

ng}. Then 

√

Theorem 4. For the SGL problem  let λα
λ ∈ F α ⇔ θ∗(λ  α) = y

y

λ ⇔ β∗(λ  α) = 0 ⇔ λ ≥ λα

max.

2

1

1

1√
ng

1
1√
ng

:= maxg

(λ2) = maxg

(cid:107)Sλ2 (XT

g y)(cid:107). Then 

:= (cid:107)XT y(cid:107)∞  then ¯β∗(λ1  λ2) = 0.

ρg in the deﬁnition of λα
max admits a closed form solution [24]. Theorem 4 implies that the optimal
solution β∗(λ  α) is 0 as long as y/λ ∈ F α. This geometric property also leads to an explicit
characterization of the set of (λ1  λ2) such that the corresponding solution of problem (2) is 0. We
denote by ¯β∗(λ1  λ2) the optimal solution of problem (2).
Corollary 5. For the SGL problem in (2)  let λmax
(i): ¯β∗(λ1  λ2) = 0 ⇔ λ1 ≥ λmax
(λ2).
(cid:107)XT
g y(cid:107) or λ2 ≥ λmax
(ii): If λ1 ≥ λmax
4 The Two-Layer Screening Rules for SGL
We follow the three steps in Section 3.2 to develop TLFre. In Section 4.1  we give an accurate
estimation of θ∗(λ  α) via normal cones [15]. Then  we compute the supreme values in (R1∗) and
(R2∗) by solving nonconvex problems in Section 4.2. We present the TLFre rules in Section 4.3.
4.1 Estimation of the Dual Optimal Solution
Because of the geometric property of the dual problem in (5)  i.e.  θ∗(λ  α) = PF α (y/λ)  we have
a very useful characterization of the dual optimal solution via the so-called normal cones [15].
Deﬁnition 1. [15] For a closed convex set C ∈ Rn and a point w ∈ C  the normal cone to C at w is
(14)
max. Thus  we can estimate θ∗(λ  α) in terms of θ∗(¯λ  α).

By Theorem 4  θ∗(¯λ  α) is known if ¯λ = λα
Due to the same reason  we only consider the cases with λ < λα
Remark 4. In many applications  the parameter values that perform the best are usually unknown.
To determine appropriate parameter values  commonly used approaches such as cross validation
and stability selection involve solving SGL many times over a grip of parameter values. Thus  given
{α(i)}I
i=1 and λ(1) ≥ ··· ≥ λ(J )  we can ﬁx the value of α each time and solve SGL by varying the
value of λ. We repeat the process until we solve SGL for all of the parameter values.
Theorem 6. For the SGL problem in (3)  suppose that θ∗(¯λ  α) is known with ¯λ ≤ λα
g = 1  . . .   G  be deﬁned by Theorem 4. For any λ ∈ (0  ¯λ)  we deﬁne

NC(w) = {v : (cid:104)v  w(cid:48) − w(cid:105) ≤ 0  ∀w(cid:48) ∈ C}.

max for θ∗(λ  α) to be estimated.

max. Let ρg 

if ¯λ < λα
if ¯λ = λα

max 
max 

where X∗ = argmaxXg

ρg 

max) 

nα(¯λ) =

(cid:26)y/¯λ − θ∗(¯λ  α) 

X∗S1(XT∗ y/λα
λ − θ∗(¯λ  α) 

vα(λ  ¯λ) = y
vα(λ  ¯λ)⊥ = vα(λ  ¯λ) − (cid:104)vα(λ ¯λ) nα(¯λ)(cid:105)

(cid:107)nα(¯λ)(cid:107)2

nα(¯λ).

Then  the following hold:
(i): nα(¯λ) ∈ NF α (θ∗(¯λ  α)) 
(ii): (cid:107)θ∗(λ  α) − (θ∗(¯λ  α) + 1
For notational convenience  let oα(λ  ¯λ) = θ∗(¯λ  α) + 1
lies inside the ball of radius 1
4.2 Solving for the supreme values via Nonconvex Optimization
We solve the optimization problems in (R1∗) and (R2∗). To simplify notations  let

α (λ  ¯λ)(cid:107) centered at oα(λ  ¯λ).

α (λ  ¯λ)(cid:107).
2 v⊥

α (λ  ¯λ))(cid:107) ≤ 1

2(cid:107)v⊥

2(cid:107)v⊥

2 v⊥

Θ = {θ : (cid:107)θ − oα(λ  ¯λ)(cid:107) ≤ 1

Ξg =(cid:8)ξg : (cid:107)ξg − XT

2(cid:107)v⊥
g oα(λ  ¯λ)(cid:107) ≤ 1

α (λ  ¯λ)(cid:107)} 
2(cid:107)v⊥

α (λ  ¯λ)(cid:107)(cid:107)Xg(cid:107)2

α (λ  ¯λ). Theorem 6 shows that θ∗(λ  α)

(cid:9)   g = 1  . . .   G.

(15)
(16)

5

gi

(cid:0)s∗
g(λ  ¯λ; α)(cid:1)2

(cid:8) 1
2(cid:107)S1(ξg)(cid:107)2 : ξg ∈ Ξg

(cid:9) .

Theorem 6 indicates that θ∗(λ  α) ∈ Θ. Moreover  we can see that XT
g Θ ⊆ Ξg  g = 1  . . .   G. To
develop the TLFre rule by (R1∗) and (R2∗)  we need to solve the following optimization problems:
(17)
(18)

g(λ  ¯λ; α) = supξg {(cid:107)S1(ξg)(cid:107) : ξg ∈ Ξg}  g = 1  . . .   G 
s∗
(λ  ¯λ; α) = supθ {|xT
t∗

θ| : θ ∈ Θ}  i = 1  . . .   ng  g = 1  . . .   G.

gi

Solving problem (17) We consider the following equivalent problem of (17):

1
2

= supξg

(19)
We can see that the objective function of problem (19) is continuously differentiable and the feasible
set is a ball. Thus  (19) is a nonconvex problem because we need to maximize a convex function
subject to a convex set. We derive the closed form solutions of problems (17) and (19) as follows.
2(cid:107)v⊥
α (λ  ¯λ)(cid:107)(cid:107)Xg(cid:107)2 and Ξ∗
Theorem 7. For problems (17) and (19)  let c = XT
the set of the optimal solutions.
(i) Suppose that c /∈ B∞  i.e.  (cid:107)c(cid:107)∞ > 1. Let u = rS1(c)/(cid:107)S1(c)(cid:107). Then 
g = {c + u}.

g(λ  ¯λ; α) = (cid:107)S1(c)(cid:107) + r and Ξ∗
s∗

g oα(λ  ¯λ)  r = 1

g be

(20)

(ii) Suppose that c is a boundary point of B∞  i.e.  (cid:107)c(cid:107)∞ = 1. Then 

s∗
g(λ  ¯λ; α) = r and Ξ∗

g = {c + u : u ∈ NB∞(c) (cid:107)u(cid:107) = r} .

(iii) Suppose that c ∈ intB∞  i.e.  (cid:107)c(cid:107)∞ < 1. Let i∗ ∈ I∗ = {i : |[c]i| = (cid:107)c(cid:107)∞}. Then 

(21)

(22)

Ξg 

g(λ  ¯λ; α) = ((cid:107)c(cid:107)∞ + r − 1)+  
s∗

Ξ∗
g =

{c + r · sgn([c]i∗ )ei∗ : i∗ ∈ I∗}  
{r · ei∗  −r · ei∗ : i∗ ∈ I∗}  

if Ξg ⊂ B∞ 
if Ξg (cid:54)⊂ B∞ and c (cid:54)= 0 
if Ξg (cid:54)⊂ B∞ and c = 0 

where ei is the ith standard basis vector.

gi

gi

gi

2(cid:107)v⊥

(cid:12)(cid:12)(cid:12)xT

ˆgi

g (λ(j+1)  α) = 0.

oα(λ  ¯λ)| + 1

(λ  ¯λ; α) = |xT

(cid:16) y−Xβ∗(λ(j) α)

Solving problem in (18) Problem (18) can be solved directly via the Cauchy-Schwarz inequality.
Theorem 8. For problem (18)  we have t∗
4.3 The Proposed Two-Layer Screening Rules
To develop the two-layer screening rules for SGL  we only need to plug the supreme values
g(λ2  ¯λ2; λ1) and t∗
s∗
Theorem 9. For the SGL problem in (3)  suppose that we are given α and a sequence of parameter
max = λ(0) > λ(1) > . . . > λ(J ). For each integer 0 ≤ j < J   we assume that β∗(λ(j)  α)
values λα
α (λ(j+1)  λ(j)) and s∗
is known. Let θ∗(λ(j)  α)  v⊥
g(λ(j+1)  λ(j); α) be given by Eq. (6)  Theorems 6
and 7  respectively. Then  for g = 1  . . .   G  the following holds
√
(L1)
ng ⇒ β∗

(λ2  ¯λ2; λ1) in (R1∗) and (R2∗). We present the TLFre rule as follows.

s∗
g(λ(j+1)  λ(j); α) < α

α (λ  ¯λ)(cid:107)(cid:107)xgi(cid:107).

ˆg (λ(j+1)  α)]i = 0 if

For the ˆgth group that does not pass the rule in (L1)  we have [β∗
2(cid:107)v⊥

α (λ(j+1)  λ(j))(cid:107)(cid:107)xˆgi(cid:107) ≤ 1.
(L1) and (L2) are the ﬁrst layer and second layer screening rules of TLFre  respectively.
5 Experiments
We evaluate TLFre on both synthetic and real data sets. To measure the performance of TLFre 
we compute the rejection ratios of (L1) and (L2)  respectively. Let m be the number of features
with 0 coefﬁcients in the solution  G be the index set of groups discarded by (L1)  and p be the
number of inactive features detected by (L2). The rejection ratios of (L1) and (L2) are deﬁned by
|p|
m   respectively. We also report the speedup gained by TLFre: the ratio of
r1 =
the running time of solver without screening to the running time of solver with TLFre. The code of
TLFre integrated with the solver from SLEP [9] is available at dpc-screening.github.io.
To determine appropriate values of α and λ by cross validation or stability selection  we can run
TLFre with as many parameter values as we need. Given a data set  for illustrative purposes only 
we select seven values of α from {tan(ψ) : ψ = 5◦  15◦  30◦  45◦  60◦  75◦  85◦}. Then  for each
value of α  we run TLFre along a sequence of 100 values of λ equally spaced on the logarithmic
scale of λ/λα
max from 1 to 0.01. Thus  700 pairs of parameter values of (λ  α) are sampled in total.

(cid:17)(cid:12)(cid:12)(cid:12) + 1

α (λ(j+1)  λ(j))

+ 1

2 v⊥

and r2 =

g∈G ng
m

(L2)

(cid:80)

λ(j)

6

(a)

(b) α = tan(5◦)

(c) α = tan(15◦)

(d) α = tan(30◦)

(e) α = tan(45◦)

(f) α = tan(60◦)

(g) α = tan(75◦)

(h) α = tan(85◦)

Figure 1: Rejection ratios of TLFre on the Synthetic 1 data set.

(a)

(b) α = tan(5◦)

(c) α = tan(15◦)

(d) α = tan(30◦)

(e) α = tan(45◦)

(f) α = tan(60◦)

(g) α = tan(75◦)

(h) α = tan(85◦)

Figure 2: Rejection ratios of TLFre on the Synthetic 2 data set.

5.1 Simulation Studies
We perform experiments on two synthetic data sets that are commonly used in the literature [19  29].
The true model is y = Xβ∗ + 0.01   ∼ N (0  1). We generate two data sets with 250 × 10000
entries: Synthetic 1 and Synthetic 2. We randomly break the 10000 features into 1000 groups. For
Synthetic 1  the entries of the data matrix X are i.i.d. standard Gaussian with pairwise correlation
zero  i.e.  corr(xi  xi) = 0. For Synthetic 2  the entries of the data matrix X are drawn from i.i.d.
standard Gaussian with pairwise correlation 0.5|i−j|  i.e.  corr(xi  xj) = 0.5|i−j|. To construct β∗ 
we ﬁrst randomly select γ1 percent of groups. Then  for each selected group  we randomly select γ2
percent of features. The selected components of β∗ are populated from a standard Gaussian and the
remaining ones are set to 0. We set γ1 = γ2 = 10 for Synthetic 1 and γ1 = γ2 = 20 for Synthetic 2.
(λ2) (see Corollary
The ﬁgures in the upper left corner of Fig. 1 and Fig. 2 show the plots of λmax
5) and the sampled parameter values of λ and α (recall that λ1 = αλ and λ2 = λ). For the other
ﬁgures  the blue and red regions represent the rejection ratios of (L1) and (L2)  respectively. We
can see that TLFre is very effective in discarding inactive groups/features; that is  more than 90%
of inactive features can be detected. Moreover  we can observe that the ﬁrst layer screening (L1)
becomes more effective with a larger α. Intuitively  this is because the group Lasso penalty plays a
more important role in enforcing the sparsity with a larger value of α (recall that λ1 = αλ). The top
and middle parts of Table 1 indicate that the speedup gained by TLFre is very signiﬁcant (up to 30
times) and TLFre is very efﬁcient. Compared to the running time of the solver without screening 
the running time of TLFre is negligible. The running time of TLFre includes that of computing
(cid:107)Xg(cid:107)2  g = 1  . . .   G  which can be efﬁciently computed by the power method [6]. Indeed  this can
be shared for TLFre with different parameter values.
5.2 Experiments on Real Data Set
We perform experiments on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data set
(http://adni.loni.usc.edu/). The data matrix consists of 747 samples with 426040 single

1

7

02004006008000100200300400λ2λ1 λmax1(λ2)α=tan(5◦)α=tan(15◦)α=tan(30◦)α=tan(45◦)α=tan(60◦)α=tan(75◦)α=tan(85◦)0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio050010000200400600λ2λ1 λmax1(λ2)α=tan(5◦)α=tan(15◦)α=tan(30◦)α=tan(45◦)α=tan(60◦)α=tan(75◦)α=tan(85◦)0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection RatioTable 1: Running time (in seconds) for solving SGL along a sequence of 100 tuning parameter
values of λ equally spaced on the logarithmic scale of λ/λα
max from 1.0 to 0.01 by (a): the solver [9]
without screening; (b): the solver combined with TLFre. The top and middle parts report the results
of TLFre on Synthetic 1 and Synthetic 2. The bottom part reports the results of TLFre on the ADNI
data set with the GMV data as response.

Synthetic 1

Synthetic 2

ADNI+GMV

α

solver
TLFre

TLFre+solver

speedup
solver
TLFre

TLFre+solver

speedup
solver
TLFre

TLFre+solver

speedup

tan(5◦) tan(15◦) tan(30◦) tan(45◦) tan(60◦) tan(75◦) tan(85◦)
291.24
298.36
0.77
0.77
10.26
22.53
12.93
29.09
292.24
294.64
0.82
0.79
22.80
11.05
26.66
12.82

301.74
0.78
12.47
24.19
294.92
0.80
12.89
22.88

307.71
0.79
17.69
17.40
297.50
0.81
18.90
15.74

308.69
0.79
15.73
19.63
297.29
0.80
16.08
18.49

311.33
0.81
19.71
15.79
297.59
0.81
20.45
14.55

307.53
0.79
21.95
14.01
295.51
0.81
21.58
13.69

30652.56 30755.63

30838.29

31096.10

30850.78

30728.27

30572.35

64.08
372.04
82.39

64.56
383.17
80.27

64.96
386.80
79.73

65.00
402.72
77.22

64.89
391.63
78.78

65.17
385.98
79.61

65.05
382.62
79.90

(a)

(b) α = tan(5◦)

(c) α = tan(15◦)

(d) α = tan(30◦)

(e) α = tan(45◦)

(h) α = tan(85◦)
Figure 3: Rejection ratios of TLFre on the ADNI data set with grey matter volume as response.

(g) α = tan(75◦)

(f) α = tan(60◦)

1

nucleotide polymorphisms (SNPs)  which are divided into 94765 groups. The response vector is the
grey matter volume (GMV).
The ﬁgure in the upper left corner of Fig. 3 shows the plots of λmax
(λ2) (see Corollary 5) and the
sampled parameter values of α and λ. The other ﬁgures present the rejection ratios of (L1) and (L2)
by blue and red regions  respectively. We can see that almost all of the inactive groups/features are
discarded by TLFre. The rejection ratios of r1 + r2 are very close to 1 in all cases. The bottom part
of Table 1 shows that TLFre leads to a very signiﬁcant speedup (about 80 times). In other words  the
solver without screening needs about eight and a half hours to solve the 100 SGL problems for each
value of α. However  combined with TLFre  the solver needs only six to eight minutes. Moreover 
we can observe that the computational cost of TLFre is negligible compared to that of the solver
without screening. This demonstrates the efﬁciency of TLFre.
6 Conclusion
In this paper  we propose a novel feature reduction method for SGL via decomposition of convex
sets. We also derive the set of parameter values that lead to zero solutions of SGL. To the best
of our knowledge  TLFre is the ﬁrst method which is applicable to sparse models with multiple
sparsity-inducing regularizers. More importantly  the proposed approach provides novel framework
for developing screening methods for complex sparse models with multiple sparsity-inducing regu-
larizers  e.g.  (cid:96)1 SVM that performs both sample and feature selection  fused Lasso and tree Lasso
with more than two regularizers. Experiments on both synthetic and real data sets demonstrate the
effectiveness and efﬁciency of TLFre. We plan to generalize the idea of TLFre to (cid:96)1 SVM  fused
Lasso and tree Lasso  which are expected to consist of multiple layers of screening.

8

050100150050100λ2λ1 λmax1(λ2)α=tan(5◦)α=tan(15◦)α=tan(30◦)α=tan(45◦)α=tan(60◦)α=tan(75◦)α=tan(85◦)0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection Ratio0.010.020.040.10.20.410.10.30.50.70.91λ/λαmaxRejection RatioReferences
[1] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer  2011.

[2] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization  Second Edition. Canadian

Mathematical Society  2006.

[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[4] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination in sparse supervised learning. Paciﬁc

Journal of Optimization  8:667–698  2012.

[5] J. Friedman  T. Hastie  and R. Tibshirani. A note on the group lasso and a sparse group lasso. arX-

iv:1001.0736.

[6] N. Halko  P. Martinsson  and J. Tropp. Finding structure with randomness: Probabilistic algorithms for

constructing approximate matrix decompositions. SIAM Review  53:217–288  2011.

[7] J.-B. Hiriart-Urruty. From convex optimization to nonconvex optimization. necessary and sufﬁcient con-

ditions for global optimality. In Nonsmooth optimization and related topics. Springer  1988.

[8] J.-B. Hiriart-Urruty. A note on the Legendre-Fenchel transform of convex composite functions. In Nons-

mooth Mechanics and Analysis. Springer  2006.

[9] J. Liu  S. Ji  and J. Ye. SLEP: Sparse Learning with Efﬁcient Projections. Arizona State University  2009.
[10] J. Liu and J. Ye. Moreau-Yosida regularization for grouped tree structure learning. In Advances in neural

information processing systems  2010.

[11] J. Liu  Z. Zhao  J. Wang  and J. Ye. Safe screening with variational inequalities and its application to

lasso. In International Conference on Machine Learning  2014.

[12] K. Ogawa  Y. Suzuki  S. Suzumura  and I. Takeuchi. Safe sample screening for Support Vector Machine.

arXiv:1401.6740  2014.

[13] K. Ogawa  Y. Suzuki  and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computa-

tion. In ICML  2013.

[14] J. Peng  J. Zhu  A. Bergamaschi  W. Han  D. Noh  J. Pollack  and P. Wang. Regularized multivariate
regression for indentifying master predictors with application to integrative genomics study of breast
cancer. The Annals of Appliced Statistics  4:53–77  2010.

[15] A. Ruszczy´nski. Nonlinear Optimization. Princeton University Press  2006.
[16] N. Simon  J. Friedman.  T. Hastie.  and R. Tibshirani. A Sparse-Group Lasso. Journal of Computational

and Graphical Statistics  22:231–245  2013.

[17] P. Sprechmann  I. Ram´ırez  G. Sapiro.  and Y. Eldar. C-HiLasso: a collaborative hierarchical sparse

modeling framework. IEEE Transactions on Signal Processing  59:4183–4198  2011.

[18] R. Tibshirani. Regression shringkage and selection via the lasso. Journal of the Royal Statistical Society

Series B  58:267–288  1996.

[19] R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R. Tibshirani. Strong rules for
discarding predictors in lasso-type problems. Journal of the Royal Statistical Society Series B  74:245–
266  2012.

[20] M. Vidyasagar. Machine learning methods in the cocomputation biology of cancer. In Proceedings of the

Royal Society A  2014.

[21] M. Vincent and N. Hansen. Sparse group lasso and high dimensional multinomial classiﬁcation. Compu-

tational Statistics and Data Analysis  71:771–786  2014.

[22] J. Wang  J. Jun  and J. Ye. Efﬁcient mixed-norm regularization: Algorithms and safe screening methods.

arXiv:1307.4156v1.

[23] J. Wang  P. Wonka  and J. Ye. Scaling svm and least absolute deviations via exact data reduction. In

International Conference on Machine Learning  2014.

[24] J. Wang and J. Ye. Two-Layer feature reduction for sparse-group lasso via decomposition of convex sets.

arXiv:1410.4210v1  2014.

[25] J. Wang  J. Zhou  P. Wonka  and J. Ye. Lasso screening rules via dual polytope projection. In Advances

in neural information processing systems  2013.

[26] Z. J. Xiang and P. J. Ramadge. Fast lasso screening tests based on correlations. In IEEE ICASSP  2012.
[27] D. Yogatama and N. Smith. Linguistic structured sparsity in text categorization. In Proceedings of the

Annual Meeting of the Association for Computational Linguistics  2014.

[28] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society Series B  68:49–67  2006.

[29] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society Series B  67:301–320  2005.

9

,Jie Wang
Jieping Ye