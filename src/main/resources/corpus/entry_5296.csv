2014,Structure Regularization for Structured Prediction,While there are many studies on weight regularization  the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However  this trend could have been misdirected  because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting  we propose a structure regularization framework via \emph{structure decomposition}  which decomposes training samples into mini-samples with simpler structures  deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product  the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks  achieving record-breaking accuracies yet with substantially faster training speed.,Structure Regularization for Structured Prediction

∗MOE Key Laboratory of Computational Linguistics  Peking University

†School of Electronics Engineering and Computer Science  Peking University

Xu Sun∗†

xusun@pku.edu.cn

Abstract

While there are many studies on weight regularization  the study on structure reg-
ularization is rare. Many existing systems on structured prediction focus on in-
creasing the level of structural dependencies within the model. However  this trend
could have been misdirected  because our study suggests that complex structures
are actually harmful to generalization ability in structured prediction. To control
structure-based overﬁtting  we propose a structure regularization framework via
structure decomposition  which decomposes training samples into mini-samples
with simpler structures  deriving a model with better generalization power. We
show both theoretically and empirically that structure regularization can effec-
tively control overﬁtting risk and lead to better accuracy. As a by-product  the
proposed method can also substantially accelerate the training speed. The method
and the theoretical results can apply to general graphical models with arbitrary
structures. Experiments on well-known tasks demonstrate that our method can
easily beat the benchmark systems on those highly-competitive tasks  achieving
state-of-the-art accuracies yet with substantially faster training speed.

1 Introduction

Structured prediction models are popularly used to solve structure dependent problems in a wide
variety of application domains including natural language processing  bioinformatics  speech recog-
nition  and computer vision. Recently  many existing systems on structured prediction focus on
increasing the level of structural dependencies within the model. We argue that this trend could
have been misdirected  because our study suggests that complex structures are actually harmful to
model accuracy. While it is obvious that intensive structural dependencies can effectively incorpo-
rate structural information  it is less obvious that intensive structural dependencies have a drawback
of increasing the generalization risk  because more complex structures are easier to suffer from
overﬁtting. Since this type of overﬁtting is caused by structure complexity  it can hardly be solved
by ordinary regularization methods such as L2 and L1 regularization schemes  which is only for
controlling weight complexity.
To deal with this problem  we propose a simple structure regularization solution based on tag struc-
ture decomposition. The proposed method decomposes each training sample into multiple mini-
samples with simpler structures  deriving a model with better generalization power. The proposed
method is easy to implement  and it has several interesting properties: (1) We show both theoretical-
ly and empirically that the proposed method can reduce the overﬁt risk. (2) Keeping the convexity of
the objective function: a convex function with a structure regularizer is still convex. (3) No conﬂict
with the weight regularization: we can apply structure regularization together with weight regular-
ization. (4) Accelerating the convergence rate in training. (5) This method can be used for different
types of models  including CRFs [6] and perceptrons [3].
The term structural regularization has been used in prior work for regularizing structures of features 
including spectral regularization [1]  regularizing feature structures for classiﬁers [23]  and many

1

recent studies on structured sparsity in structured prediction scenarios [13  9]  via adopting mixed
norm regularization [11]  Group Lasso [25]  and posterior regularization [5]. Compared with those
prior work  we emphasize that our proposal on tag structure regularization is novel. This is because
the term structure in all of the aforementioned work refers to structures of feature space  which
is substantially different compared with our proposal on regularizing tag structures (interactions
among tags).
There are other related studies  including the studies of [20] and [12] on piecewise/decomposed
training methods  and the study of [22] on a “lookahead" learning method. Our work differs from
[20  12  22] mainly because our work is built on a regularization framework  with arguments and
justiﬁcations on reducing generalization risk and for better accuracy. Also  our method and the the-
oretical results can ﬁt general graphical models with arbitrary structures  and the detailed algorithm
is quite different. On generalization risk analysis  related studies include [2  14] on non-structured
classiﬁcation and [21  8  7] on structured classiﬁcation.
To the best of our knowledge  this is the ﬁrst theoretical result on quantifying the relation between
structure complexity and the generalization risk in structured prediction  and this is also the ﬁrst
proposal on structure regularization via regularizing tag-interactions. The contributions of this work1
are two-fold:

• On the methodology side  we propose a structure regularization framework for structured
prediction. We show both theoretically and empirically that the proposed method can ef-
fectively reduce the overﬁtting risk  and at the same time accelerate the convergence rate in
training. Our method and the theoretical analysis do not make assumptions based on specif-
ic structures. In other words  the method and the theoretical results can apply to graphical
models with arbitrary structures  including linear chains  trees  and general graphs.
• On the application side  for several important natural language processing tasks  our simple
method can easily beat the benchmark systems on those highly-competitive tasks  achieving
record-breaking accuracies as well as substantially faster training speed.

2 Structure Regularization

A graph of observations (even with arbitrary structures) can be indexed and be denoted by using
an indexed sequence of observations OOO = {o1  . . .   on}. We use the term sample to denote OOO =
{o1  . . .   on}. For example  in natural language processing  a sample may correspond to a sentence
of n words with dependencies of tree structures (e.g.  in syntactic parsing). For simplicity in analysis 
we assume all samples have n observations (thus n tags). In a typical setting of structured prediction 
all the n tags have inter-dependencies via connecting each Markov dependency between neighboring
tags. Thus  we call n as tag structure complexity or simply structure complexity below.
A sample is converted to an indexed sequence of feature vectors xxx = {xxx(1)  . . .   xxx(n)}  where xxx(k) ∈
X is of the dimension d and corresponds to the local features extracted from the position/index k.
We can use an n× d matrix to represent xxx ∈ X n. Let Z = (X n Y n) and let zzz = (xxx  yyy) ∈ Z denote
a sample in the training data. Suppose a training set is S = {zzz1 = (xxx1  yyy1)  . . .   zzzm = (xxxm  yyym)} 
with size m  and the samples are drawn i.i.d. from a distribution D which is unknown. A learning
algorithm is a function G : Z m 7→ F with the function space F ⊂ {X n 7→ Y n}  i.e.  G maps a
training set S to a function GS : X n 7→ Y n. We suppose G is symmetric with respect to S  so that
G is independent on the order of S.
Structural dependencies among tags are the major difference between structured prediction and non-
structured classiﬁcation. For the latter case  a local classiﬁcation of g based on a position k can be
expressed as g(xxx(k−a)  . . .   xxx(k+a))  where the term {xxx(k−a)  . . .   xxx(k+a)} represents a local win-
dow. However  for structured prediction  a local classiﬁcation on a position depends on the whole
input xxx = {xxx(1)  . . .   xxx(n)} rather than a local window  due to the nature of structural dependencies
among tags (e.g.  graphical models like CRFs). Thus  in structured prediction a local classiﬁcation
on k should be denoted as g(xxx(1)  . . .   xxx(n)  k). To simplify the notation  we deﬁne

g(xxx  k)   g(xxx(1)  . . .   xxx(n)  k)

1See the code at http://klcl.pku.edu.cn/member/sunxu/code.htm

2

Figure 1: An illustration of structure regularization in simple linear chain case  which decompose a
training sample zzz with structure complexity 6 into three mini-samples with structure complexity 2.
Structure regularization can apply to more general graphs with arbitrary dependencies.
We deﬁne point-wise cost function c : Y×Y 7→ R+ as c[GS(xxx  k)  yyy(k)]  which measures the cost on
a position k by comparing GS(xxx  k) and the gold-standard tag yyy(k)  and we introduce the point-wise
loss as
Then  we deﬁne sample-wise cost function C : Y n × Y n 7→ R+  which is the cost function with
respect to a whole sample  and we introduce the sample-wise loss as

ℓ(GS  zzz  k)   c[GS(xxx  k)  yyy(k)]

n∑

n∑

L(GS  zzz)   C[GS(xxx)  yyy] =

ℓ(GS  zzz  k) =

c[GS(xxx  k)  yyy(k)]

k=1

k=1

Given G and a training set S  what we are most interested in is the generalization risk in structured
prediction (i.e.  expected average loss) [21  8]:

Since the distribution D is unknown  we have to estimate R(GS) by using the empirical risk:

[L(GS  zzz)
]
m∑

n

n∑

R(GS) = Ezzz

m∑

i=1

Re(GS) =

1
mn

L(GS  zzzi) =

1
mn

ℓ(GS  zzzi  k)

i=1

k=1

To state our theoretical results  we must describe several quantities and assumptions following prior
work [2  14]. We assume a simple real-valued structured prediction scheme such that the class
predicted on position k of xxx is the sign of GS(xxx  k) ∈ D.2 Also  we assume the point-wise cost
function cτ is convex and τ-smooth such that ∀y1  y2 ∈ D ∀y

|cτ (y1  y

∗

) − cτ (y2  y

∗

(1)
Also  we use a value ρ to quantify the bound of |GS(xxx  k) − GS\i (xxx  k)| while changing a single
′ ≤ n) in the training set with respect to the structured input xxx. This ρ-admissible
sample (with size n
assumption can be formulated as ∀k 

∗ ∈ Y
)| ≤ τ|y1 − y2|

|GS(xxx  k) − GS\i(xxx  k)| ≤ ρ||GS − GS\i||2 · ||xxx||2

(2)

where ρ ∈ R+ is a value related to the design of algorithm G.

2.1 Structure Regularization

Most existing regularization techniques are for regularizing model weights/parameters (e.g.  a rep-
resentative regularizer is the Gaussian regularizer or so called L2 regularizer)  and we call such
regularization techniques as weight regularization.
Deﬁnition 1 (Weight regularization) Let Nλ : F 7→ R+ be a weight regularization function on
F with regularization strength λ  the structured classiﬁcation based objective function with general
weight regularization is as follows:

Rλ(GS)   Re(GS) + Nλ(GS)

(3)

2Many popular structured prediction models have a convex and real-valued cost function (e.g.  CRFs).

3

y(1) x(1) y(2) y(3) y(4) y(5) y(6) y(1) y(2) y(3) y(4) y(5) y(6) x(2) x(3) x(4) x(5) x(6) x(1) x(2) x(3) x(4) x(5) x(6) Algorithm 1 Training with structure regularization
1: Input: model weights www  training set S  structure regularization strength α
2: repeat
′ ← ∅
3:
S
for i = 1 → m do
4:
Randomly decompose zzzi ∈ S into mini-samples Nα(zzzi) = {zzz(i 1)  . . .   zzz(i α)}
5:
′ ← S
6:
7:
8:
9:
10:
11:
12: until Convergence
13: return www

end for
for i = 1 → |S
Sample zzz
www ← www − η∇gzzz′(www)

′| do
′ uniformly at random from S

′  with gradient ∇gzzz′(www)

′ ∪ Nα(zzzi)

end for

S

While weight regularization is normalizing model weights  the proposed structure regularization
method is normalizing the structural complexity of the training samples. As illustrated in Figure 1 
our proposal is based on tag structure decomposition  which can be formally deﬁned as follows:
Deﬁnition 2 (Structure regularization) Let Nα : F 7→ F be a structure regularization function
on F with regularization strength α with 1 ≤ α ≤ n  the structured classiﬁcation based objective
function with structure regularization is as follows3:

1
mn

Rα(GS)   Re[GN(cid:11)(S)] =

(4)
where Nα(zzzi) randomly splits zzzi into α mini-samples {zzz(i 1)  . . .   zzz(i α)}  so that the mini-samples
have a distribution on their sizes (structure complexities) with the expected value n
= n/α. Thus 
we get

ℓ[GS′  zzz(i j)  k]

k=1

j=1

j=1

i=1

i=1

1
mn

L[GS′  zzz(i j)] =

′

  . . .   zzz(m 1)  zzz(m 2)  . . .   zzz(m α)

(5)

}

}

m∑

α∑

m∑

α∑

n/α∑

When the structure regularization strength α = 1  we have S
= S and Rα = Re. The structure
regularization algorithm (with the stochastic gradient descent setting) is summarized in Algorithm
1. Recall that xxx = {xxx(1)  . . .   xxx(n)} represents feature vectors. Thus  it should be emphasized that
the decomposition of xxx is the decomposition of the feature vectors  not the original observations.
Actually the decomposition of the feature vectors is more convenient and has no information loss —
decomposing observations needs to regenerate features and may lose some features.
The structure regularization has no conﬂict with the weight regularization  and the structure regular-
ization can be applied together with the weight regularization.

Deﬁnition 3 (Structure & weight regularization) By combining structure regularization in Def-
inition 2 and weight regularization in Deﬁnition 1  the structured classiﬁcation based objective
function is as follows:

Rα λ(GS)   Rα(GS) + Nλ(GS)

(7)

When α = 1  we have Rα λ = Re(GS) + Nλ(GS) = Rλ.

Like existing weight regularization methods  currently our structure regularization is only for the
training stage. Currently we do not use structure regularization in the test stage.

3The notation N is overloaded here. For clarity throughout  N with subscript (cid:21) refers to weight regulariza-

tion function  and N with subscript (cid:11) refers to structure regularization function.

4

with mα mini-samples with expected structure complexity n/α. We can denote S
as S

= {zzz

′
2  . . .   zzz

′
1  zzz

′

′

S

|

|
}
= {zzz(1 1)  z(1 2)  . . .   zzz(1 α)
mα∑
} and Rα(GS) can be simpliﬁed as

{z
mα∑

′
mα

α

{z
n/α∑

α

Rα(GS)   1
mn

i=1

L(GS′  zzz

′
i) =

1
mn

ℓ[GS′   zzz

k=1

i=1
′

′ more compactly

′
i  k]

(6)

2.2 Reduction of Generalization Risk

In contrast to the simplicity of the algorithm  the theoretical analysis is quite technical. In this paper
we only describe the major theoretical result. Detailed analysis and proofs are given in the full
version of this work [16].
Theorem 4 (Generalization vs. structure regularization) Let the structured prediction objective
function of G be penalized by structure regularization with factor α ∈ [1  n] and L2 weight regular-
ization with factor λ  and the penalized function has a minimizer f:
Lτ (g  zzz

(8)
Assume the point-wise loss ℓτ is convex and differentiable  and is bounded by ℓτ (f  zzz  k) ≤ γ.
Assume f (xxx  k) is ρ-admissible. Let a local feature value be bounded by v such that xxx(k q) ≤ v for
q ∈ {1  . . .   d}. Then  for any δ ∈ (0  1)  with probability at least 1 − δ over the random draw of
the training set S  the generalization risk R(f ) is bounded by

g∈F Rα λ(g) = argmin
g∈F

mα∑

f = argmin

||g||2

′
j) +

1
mn

(

)

λ
2

j=1

2

(

(4m − 2)dτ 2ρ2v2n2

mλα

+ γ

)√

ln δ−1
2m

(9)

(

R(f ) ≤ Re(f ) +
)√

2dτ 2ρ2v2n2

mλα

+

mλα

+ γ

(4m−2)dτ 2ρ2v2n2

The proof is given in the full version of this work [16]. We call the term 2dτ 2ρ2v2n2

+
ln δ−1
2m in (9) as “overﬁt-bound"  and reducing the overﬁt-bound is cru-
cial for reducing the generalization risk. Most importantly  we can see from the overﬁt-bound that
the structure regularization factor α is always staying together with the weight regularization factor
λ  working together on reducing the overﬁt-bound. This indicates that the structure regularization is
as important as the weight regularization for reducing the generalization risk.
Theorem 4 also indicates that too simple structures may overkill the overﬁt-bound but with a domi-
nating empirical risk  and too complex structures may overkill the empirical risk but with a dominat-
ing overﬁt-bound. Thus  to achieve the best prediction accuracy  a balanced complexity of structures
should be used for training the model.

mλα

2.3 Accelerating Convergence Rates in Training

We also analyze the impact on the convergence rate of online learning by applying structure regular-
ization. Following prior work [10]  our analysis is based on the stochastic gradient descent (SGD)
with ﬁxed learning rate. Let g(www) be the structured prediction objective function and www ∈ W is the
weight vector. Recall that the SGD update with ﬁxed learning rate η has a form like this:

wwwt+1 ← wwwt − η∇gzzzt (wwwt)

′

g(www

(10)
where gzzz(wwwt) is the stochastic estimation of the objective function based on zzz which is randomly
drawn from S. To state our convergence rate analysis results  we need several assumptions following
(Nemirovski et al. 2009). We assume g is strongly convex with modulus c  that is  ∀www  www

′ ∈ W 

) ≥ g(www) + (www

′ − www)T∇g(www) +

′ − www||2
||www
c
(11)
2
∗. We also assume Lipschitz
When g is strongly convex  there is a global optimum/minimizer www
continuous differentiability of g with the constant q  that is  ∀www  www
′ ∈ W 
′ − www||
(12)
It is also reasonable to assume that the norm of ∇gzzz(www) has almost surely positive correlation with
the structure complexity of zzz 4 which can be quantiﬁed by a bound κ ∈ R+:
||∇gzzz(www)||2 ≤ κ|zzz| almost surely for ∀www ∈ W
where |zzz| denotes the structure complexity of zzz. Moreover  it is reasonable to assume

) − ∇g(www)|| ≤ q||www

||∇g(www

(13)

′

(14)
because even the ordinary gradient descent methods will diverge if ηc > 1. Then  we show that
structure regularization can quadratically accelerate the SGD rates of convergence:

ηc < 1

4Many models (e.g.  CRFs) satisfy this assumption that the gradient of a larger sample is expected to have

a larger norm.

5

Proposition 5 (Convergence rates vs. structure regularization) With the aforementioned as-
sumptions  let the SGD training have a learning rate deﬁned as η = cϵβα2
qκ2n2   where ϵ > 0 is a
convergence tolerance value and β ∈ (0  1]. Let t be a integer satisfying

(15)
where n and α ∈ [1  n] is like before  and a0 is the initial distance which depends on the initialization
∗||2. Then  after t updates of www it
of the weights www0 and the minimizer www
converges to E[g(wwwt) − g(www
The proof is given in the full version of this work [16]. As we can see  using structure regularization
with the strength α can quadratically accelerate the convergence rate with a factor of α2.

∗  i.e.  a0 = ||www0 − www

ϵβc2α2

t ≥ qκ2n2 log (qa0/ϵ)

∗

)] ≤ ϵ.

3 Experiments

Diversiﬁed Tasks. The natural language processing tasks include (1) part-of-speech tagging  (2)
biomedical named entity recognition  and (3) Chinese word segmentation. The signal processing
task is (4) sensor-based human activity recognition. The tasks (1) to (3) use boolean features and
the task (4) adopts real-valued features. From tasks (1) to (4)  the averaged structure complexity
(number of observations) n is very different  with n = 23.9  26.5  46.6  67.9  respectively. The
dimension of tags |Y| is also diversiﬁed among tasks  with |Y| ranging from 5 to 45.
Part-of-Speech Tagging (POS-Tagging). Part-of-Speech (POS) tagging is an important and highly
competitive task. We use the standard benchmark dataset in prior work [3]  with 38 219 training
samples and 5 462 test samples. Following prior work [22]  we use features based on words and
lexical patterns  with 393 741 raw features5. The evaluation metric is per-word accuracy.
Biomedical Named Entity Recognition (Bio-NER). This task is from the BioNLP-2004 shared
task [22]. There are 17 484 training samples and 3 856 test samples. Following prior work [22] 
we use word pattern features and POS features  with 403 192 raw features in total. The evaluation
metric is balanced F-score.
Word Segmentation (Word-Seg). We use the MSR data provided by SIGHAN-2004 contest [4].
There are 86 918 training samples and 3 985 test samples. The features are similar to [18]  with
1 985 720 raw features in total. The evaluation metric is balanced F-score.
Sensor-based Human Activity Recognition (Act-Recog). This is a task based on real-valued sen-
sor signals  with the data extracted from the Bao04 activity recognition dataset [17]. The features
are similar to [17]  with 1 228 raw features in total. There are 16 000 training samples and 4 000
test samples. The evaluation metric is accuracy.
We choose the CRFs [6] and structured perceptrons (Perc) [3]  which are arguably the most popular
probabilistic and non-probabilistic structured prediction models  respectively. The CRFs are trained
using the SGD algorithm 6 and the baseline method is the traditional weight regularization scheme
(WeightReg)  which adopts the most representative L2 weight regularization  i.e.  a Gaussian prior.
We also tested sparsity emphasized regularization methods  including L1 regularization and Group
Lasso regularization [9]. However  although the feature sparsity is improved  we ﬁnd in experiments
that in most cases those sparsity emphasized regularization methods have lower accuracy than the
L2 regularization. For the structured perceptrons  the baseline WeightAvg is the popular implicit
regularization technique based on parameter averaging  i.e.  averaged perceptron [3].
The rich edge features [19  18] are employed for all methods. All methods are based on the 1st-
order Markov dependency. For WeightReg  the L2 regularization strengths (i.e.  λ/2 in Eq.(8)) are
tuned among values 0.1  0.5  1  2  5  and are determined on the development data (POS-Tagging) or
simply via 4-fold cross validation on the training set (Bio-NER  Word-Seg  and Act-Recog). With
this automatic tuning for WeightReg  we set 2  5  1 and 5 for POS-Tagging  Bio-NER  Word-Seg 
and Act-Recog tasks  respectively.

5Raw features are those observation features based only on xxx  i.e.  no combination with tag information.
6In theoretical analysis  following prior work we adopt the SGD with ﬁxed learning rate  as described in
Section 2.3. However  since the SGD with decaying learning rate is more commonly used in practice  in
experiments we use the SGD with decaying learning rate.

6

Figure 2: On the four tasks  comparing the structure regularization method (StructReg) with existing
regularization methods in terms of accuracy/F-score. Row-1 shows the results on CRFs and Row-2
shows the results on structured perceptrons.

Table 1: Comparing our results with the benchmark systems on corresponding tasks.
POS-Tagging (Acc%) Bio-NER (F1%) Word-Seg (F1%)
97.19 (see [4])

Benchmark system

72.28 (see [22])

97.33 (see [15])

72.43

97.50

Our results

97.36

3.1 Experimental Results

′

′

′ with n

= n/α. Actually  n

The experimental results in terms of accuracy/F-score are shown in Figure 2. For the CRF model 
the training is convergent  and the results on the convergence state (decided by relative objective
change with the threshold value of 0.0001) are shown. For the structured perceptron model  the
training is typically not convergent  and the results on the 10’th iteration are shown. For stability of
the curves  the results of the structured perceptrons are averaged over 10 repeated runs.
Since different samples have different size n in practice  we set α being a function of n  so that the
′ is a probabilistic distri-
generated mini-samples are with ﬁxed size n
bution because we adopt randomized decomposition. For example  if n
= 5.5  it means the mini-
samples are a mixture of the ones with the size 5 and the ones with the size 6  and the mean of the
size distribution is 5.5. In the ﬁgure  the curves are based on n
= 1.5  2.5  3.5  5.5  10.5  15.5  20.5.
As we can see  the results are quite consistent. It demonstrates that structure regularization leads to
higher accuracies/F-scores compared with the existing baselines. We also conduct signiﬁcance tests
based on t-test. Since the t-test for F-score based tasks (Bio-NER and Word-Seg) may be unreli-
able7  we only perform t-test for the accuracy-based tasks  i.e.  POS-Tagging and Act-Recog. For
POS-Tagging  the signiﬁcance test suggests that the superiority of StructReg over WeightReg is very
statistically signiﬁcant  with p < 0.01. For Act-Recog  the signiﬁcance tests suggest that both the
StructReg vs. WeightReg difference and the StructReg vs. WeightAvg difference are extremely statis-
tically signiﬁcant  with p < 0.0001 in both cases. The experimental results support our theoretical
analysis that structure regularization can further reduce the generalization risk over existing weight
regularization techniques.
Our method outperforms the benchmark systems on the three important natural language processing
tasks. The POS-Tagging task is a highly competitive task  with many methods proposed  and the best
report (without using extra resources) until now is achieved by using a bidirectional learning model

′

7Indeed we can convert F-scores to accuracy scores for t-test  but in many cases this conversion is unreliable.

For example  very different F-scores may correspond to similar accuracy scores.

7

0510152097.197.1597.297.2597.397.3597.4POS−Tagging: CRFMini−Sample Size (n/α)Accuracy (%) StructRegWeightReg0510152071.87272.272.4Bio−NER: CRFMini−Sample Size (n/α)F−score (%) StructRegWeightReg0510152097.497.4297.4497.4697.4897.5Word−Seg: CRFMini−Sample Size (n/α)F−score (%) StructRegWeightReg0510152092.692.89393.293.493.6Act−Recog: CRFMini−Sample Size (n/α)Accuracy (%) StructRegWeightReg0510152097.197.1597.2POS−Tagging: PercMini−Sample Size (n/α)Accuracy (%) StructRegWeightAvg0510152071.271.471.671.872Bio−NER: PercMini−Sample Size (n/α)F−score (%) StructRegWeightAvg0510152096.99797.197.297.3Word−Seg: PercMini−Sample Size (n/α)F−score (%) StructRegWeightAvg0510152092.59393.5Act−Recog: PercMini−Sample Size (n/α)Accuracy (%) StructRegWeightAvgFigure 3: On the four tasks  comparing the structure regularization method (StructReg) with existing
regularization methods in terms of wall-clock training time.

in [15] 8 with the accuracy 97.33%. Our simple method achieves better accuracy compared with all
of those state-of-the-art systems. Furthermore  our method achieves as good scores as the benchmark
systems on the Bio-NER and Word-Seg tasks. On the Bio-NER task  [22] achieves 72.28% based
on lookahead learning and [24] achieves 72.65% based on reranking. On the Word-Seg task  [4]
achieves 97.19% based on maximum entropy classiﬁcation and our recent work [18] achieves 97.5%
based on feature-frequency-adaptive online learning. The comparisons are summarized in Table 1.
Figure 3 shows experimental comparisons in terms of wall-clock training time. As we can see  the
proposed method can substantially improve the training speed. The speedup is not only from the
faster convergence rates  but also from the faster processing time on the structures  because it is
more efﬁcient to process the decomposed samples with simple structures.

4 Conclusions and Future Work

We proposed a structure regularization framework  which decomposes training samples into mini-
samples with simpler structures  deriving a trained model with regularized structural complexity.
Our theoretical analysis showed that this method can effectively reduce the generalization risk  and
can also accelerate the convergence speed in training. The proposed method does not change the
convexity of the objective function  and can be used together with any existing weight regulariza-
tion methods. The proposed method and the theoretical results can ﬁt general structures including
linear chains  trees  and graphs. Experimental results demonstrated that our method achieved better
results than state-of-the-art systems on several highly-competitive tasks  and at the same time with
substantially faster training speed.
The structure decomposition of structure regularization can naturally used for parallel training 
achieving parallel training among mini-samples. As future work  we will combine structure reg-
ularization with parallel training.

Acknowledgments

This work was supported in part by National Natural Science Foundation of China (No. 61300063)  and
Doctoral Fund of Ministry of Education of China (No. 20130001120004).

8See a collection of the systems at http://aclweb.org/aclwiki/index.php?title=POS_

Tagging_(State_of_the_art)

8

051015200.511.522.5x 104POS−Tagging: CRFMini−Sample Size (n/α)Train−time (sec) StructRegWeightReg0510152010002000300040005000Bio−NER: CRFMini−Sample Size (n/α)Train−time (sec) StructRegWeightReg051015202000250030003500400045005000Word−Seg: CRFMini−Sample Size (n/α)Train−time (sec) StructRegWeightReg0510152010002000300040005000Act−Recog: CRFMini−Sample Size (n/α)Train−time (sec) StructRegWeightReg0510152040060080010001200POS−Tagging: PercMini−Sample Size (n/α)Train−time (sec) StructRegWeightAvg05101520100150200250300350400450Bio−NER: PercMini−Sample Size (n/α)Train−time (sec) StructRegWeightAvg05101520350400450Word−Seg: PercMini−Sample Size (n/α)Train−time (sec) StructRegWeightAvg05101520100150200250300350Act−Recog: PercMini−Sample Size (n/α)Train−time (sec) StructRegWeightAvgReferences
[1] A. Argyriou  C. A. Micchelli  M. Pontil  and Y. Ying. A spectral regularization framework for multi-task

structure learning. In Proceedings of NIPS’07. MIT Press  2007.

[2] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research 

2:499–526  2002.

[3] M. Collins. Discriminative training methods for hidden markov models: Theory and experiments with

perceptron algorithms. In Proceedings of EMNLP’02  pages 1–8  2002.

[4] J. Gao  G. Andrew  M. Johnson  and K. Toutanova. A comparative study of parameter estimation methods

for statistical natural language processing. In Proceedings of ACL’07  pages 824–831  2007.

[5] J. Graça  K. Ganchev  B. Taskar  and F. Pereira. Posterior vs parameter sparsity in latent variable models.

In Proceedings of NIPS’09  pages 664–672  2009.

[6] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data. In ICML’01  pages 282–289  2001.

[7] B. London  B. Huang  B. Taskar  and L. Getoor. Collective stability in structured prediction: General-
ization from one example. In Proceedings of the 30th International Conference on Machine Learning
(ICML-13)  pages 828–836  2013.

[8] B. London  B. Huang  B. Taskar  and L. Getoor. Pac-bayes generalization bounds for randomized struc-

tured prediction. In NIPS Workshop on Perturbation  Optimization and Statistics  2013.

[9] A. F. T. Martins  N. A. Smith  M. A. T. Figueiredo  and P. M. Q. Aguiar. Structured sparsity in structured

prediction. In Proceedings of EMNLP’11  pages 1500–1511  2011.

[10] F. Niu  B. Recht  C. Re  and S. J. Wright. Hogwild: A lock-free approach to parallelizing stochastic

gradient descent. In NIPS’11  pages 693–701  2011.

[11] A. Quattoni  X. Carreras  M. Collins  and T. Darrell. An efﬁcient projection for l1 inﬁnity regularization.

In Proceedings of ICML’09  page 108  2009.

[12] R. Samdani and D. Roth. Efﬁcient decomposed learning for structured prediction. In ICML’12  2012.
[13] M. W. Schmidt and K. P. Murphy. Convex structure learning in log-linear models: Beyond pairwise

potentials. In Proceedings of AISTATS’10  volume 9 of JMLR Proceedings  pages 709–716  2010.

[14] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Learnability and stability in the general

learning setting. In Proceedings of COLT’09  2009.

[15] L. Shen  G. Satta  and A. K. Joshi. Guided learning for bidirectional sequence classiﬁcation. In Proceed-

ings of ACL’07  2007.

[16] X. Sun. Structure regularization for structured prediction: Theories and experiments. In Technical report 

arXiv  2014.

[17] X. Sun  H. Kashima  and N. Ueda. Large-scale personalized human activity recognition using online

multitask learning. IEEE Trans. Knowl. Data Eng.  25(11):2551–2563  2013.

[18] X. Sun  W. Li  H. Wang  and Q. Lu. Feature-frequency-adaptive on-line training for fast and accurate

natural language processing. Computational Linguistics  40(3):563–586  2014.

[19] X. Sun  H. Wang  and W. Li. Fast online training with frequency-adaptive learning rates for chinese word

segmentation and new word detection. In Proceedings of ACL’12  pages 253–262  2012.

[20] C. A. Sutton and A. McCallum. Piecewise pseudolikelihood for efﬁcient training of conditional random

ﬁelds. In ICML’07  pages 863–870. ACM  2007.

[21] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In NIPS’03  2003.
[22] Y. Tsuruoka  Y. Miyao  and J. Kazama. Learning with lookahead: Can history-based models rival globally

optimized models? In Conference on Computational Natural Language Learning  2011.

[23] H. Xue  S. Chen  and Q. Yang. Structural regularized support vector machine: A framework for structural

large margin classiﬁer. IEEE Transactions on Neural Networks  22(4):573–587  2011.

[24] K. Yoshida and J. Tsujii. Reranking for biomedical named-entity recognition.

BioNLP  pages 209–216  2007.

In ACL Workshop on

[25] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society  Series B  68:49–67  2006.

9

,Xu Sun
Shumeet Baluja