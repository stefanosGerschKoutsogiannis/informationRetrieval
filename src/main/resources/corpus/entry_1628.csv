2019,Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries,This paper explores the task of interactive image retrieval using natural language queries  where a user progressively provides input queries to refine a set of retrieval results. Moreover  our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down  an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval.
We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore  we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks  demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries  and interactive image retrieval using real queries from human evaluators.,Drill-down: Interactive Retrieval of Complex Scenes

using Natural Language Queries

Fuwen Tan

University of Virginia

fuwen.tan@virginia.edu

Paola Cascante-Bonilla
University of Virginia
pc9za@virginia.com

Xiaoxiao Guo
IBM Research AI

xiaoxiao.guo@ibm.com

Hui Wu

IBM Research AI
wuhu@us.ibm.com

Song Feng

IBM Research AI

sfeng@us.ibm.com

Vicente Ordonez

University of Virginia

vicente@virginia.edu

Abstract

This paper explores the task of interactive image retrieval using natural language
queries  where a user progressively provides input queries to reﬁne a set of retrieval
results. Moreover  our work explores this problem in the context of complex image
scenes containing multiple objects. We propose Drill-down  an effective framework
for encoding multiple queries with an efﬁcient compact state representation that
signiﬁcantly extends current methods for single-round image retrieval. We show
that using multiple rounds of natural language queries as input can be surprisingly
effective to ﬁnd arbitrarily speciﬁc images of complex scenes. Furthermore  we
ﬁnd that existing image datasets with textual captions can provide a surprisingly
effective form of weak supervision for this task. We compare our method with
existing sequential encoding and embedding networks  demonstrating superior per-
formance on two proposed benchmarks: automatic image retrieval on a simulated
scenario that uses region captions as queries  and interactive image retrieval using
real queries from human evaluators.

1

Introduction

Retrieving images from text-based queries has been an active area of research that requires some level
of visual and textual understanding. Signiﬁcant improvement has been achieved over the past years
with advances in representation learning but ﬁnding very speciﬁc images with detailed speciﬁcations
remains challenging. A common way of speciﬁcation is through natural language queries  where a
user inputs a description of the image and obtains a set of results. We focus on a common scenario
where a user is trying to ﬁnd an exact image  or similarly where the user has a very speciﬁc idea of a
target image  or is deciding on-the-ﬂy while querying. We present empirical evidence that users are
much more successful if they are allowed to reﬁne their search results with subsequent textual queries.
Users might start with a general query about the “concept” of the image they have in mind and then
“drill down” onto more speciﬁc descriptions of objects or attributes in the image to reﬁne the results.
Among previous efforts in image retrieval  a promising paradigm is to learn a visual-semantic
embedding by minimizing the distance between a target image and an input textual query using a joint
feature space. Pioneering approaches such as [17  34  9  21  36  33] have demonstrated remarkable

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: An example of the interactive image retrieval with our Drill-down model  where a user
generated query (Ut) progressively reﬁnes the search results (St) until the target image is among top
search results.

performance on large scale datasets such as Flickr30K [26] and COCO [23]  and domain-speciﬁc
tasks such as outﬁt composition [12]. However  we ﬁnd that these methods are limited in their
capacity for retrieving highly speciﬁc images  because it is either difﬁcult for users to be speciﬁc
enough with a single query or users may not have the full picture in mind beforehand. We show an
example of this type of interaction in Figure 1. While single-query retrieval might be more suited for
domains such as product search where images typically contain only one object  requiring users to
describe a whole scene in one sentence might be too demanding. More recently  dialog based search
has been proposed to overcome some of the limitations of single-query retrieval [22  31  10  7].
In this paper  we propose Drill-down  an interactive image search framework for retrieving complex
scenes  which learns to capture the ﬁne-grained alignments between images and multiple text queries.
Our work is inspired by the observations that: (1) user queries at each turn may not exhaustively
describe all the details of the target image  but focus on some local regions  which provide a natural
decomposition of the whole scene. Therefore  we explicitly represent images as a list of object/stuff
level features extracted from a pre-trained object detector [27]. This is also in line with recent
research [21  36] on learning region-phrase alignments for single-query methods; (2) complex scenes
contain multiple objects that might share the same feature subspace. Particularly  existing state
representations of sequential text queries  such as the hidden states of a RNN  condense all image
properties in a single state vector  which makes it difﬁcult to distinguish entities sharing the same
feature subspace  such as multiple person instances. To address this  we propose to maintain a set
of state vectors  encouraging each of the vectors to encode text queries corresponding to a distinct
image region. Figure 2 shows an overview of our approach  images are represented with local feature
representations  and the query state is represented by a ﬁxed set of vectors that are selectively updated
with each subsequent query.
We demonstrate the effectiveness of our approach on the Visual Genome dataset [20] in two scenarios:
automatic image retrieval using region captions as queries  and interactive image retrieval with real
queries from human evaluators. In both cases  our experimental results show that the proposed model
outperforms existing methods  such as a hierarchical recurrent encoder model [29]  while using less
computational budget.
Our main contributions can be summarized as follows: 1

which leverages region captions as a form of weak supervision during training.

• We propose Drill-down  an interactive image search approach with multiple round queries
• We conduct experiments on a large-scale natural image dataset: Visual Genome [20]  and
demonstrate superior performance of our model on both simulated and real user queries;
• We show that our model  while producing a compact representation  outperforms competing

baseline methods by a signiﬁcant margin.

2 Related Work

Text-based image retrieval has been an active research topic for decades [5  4  28]. Prominent more
contemporary works have recognized the need for richer user interactions in order to obtain higher

1Codes are available at https://github.com/uvavision/DrillDown

2

quality results [30  18  19  2]. Siddiquie et al [30] proposed an approach to use multiple query
attributes. Kovashka et al [18  19] further proposed using user feedback based on individual visual
attributes to progressively improve search results. Arandjelovic et al [2] proposed a multiple query
retrieval system that was used for querying speciﬁc objects within a large set of images. These works
show that multiple independent queries generally outperform methods that jointly model the input set
with a single query. Our work builds on these previous ideas but does not use an explicit notion of
attributes and aims to support more general input text queries.
Remarkable results have been achieved by recent methods based on deep learning [17  34  9]. These
methods typically explore mapping a text query and the target image into a common feature space.
Learned feature representations are designated to capture both visual and semantic information in the
same embedding space. In contrast  besides supporting multiple rounds of queries  our approach also
has a richer region representation to explicitly map individual entities in images to textual phrases.
Another line of recent inquiry are dialog based image search systems [22  10]. Liao et al [22] proposed
to aggregate multi-round user responses from trained agents or human agents in order to iteratively
reﬁne a retrieved set of images using a hierarchical recurrent encoder-decoder framework [29]. We
follow a similar protocol  but we explore a more open-ended domain of images corresponding to
scenes depicting multiple objects. The method Guo et al [10] as in our work  used multiple rounds of
natural language queries  and proposed collecting relative image captions as supervision for a product
search task. In contrast  we pursue a weakly supervised approach where we leverage an image dataset
with region captions that are used to simulate queries during training  thus bypassing the need to
collect extra annotations. We demonstrate that training with simulated queries is surprisingly effective
under human evaluations. As the hierarchical recurrent framework [29] was used in most of the
previous dialog based methods [6  7  31  22  10]  we provide a re-implementation of the hierarchical
encoder (HRE) model with the queries as context and use it as one of our baselines. Different from
the previous dialog based methods where the systems also provide textual responses  we explore a
scenario where the system only responses with retrieved images  so no decoder module is required in
our case.
Also relevant to our research are the existing works on learning image-word [14  11  21] or region-
phrase [25] alignments for vision-language tasks. For instance  Karpathy et al [14] proposed to learn
a bidirectional image-sentence mapping by jointly embedding fragments of images (objects) and
sentences. The image fragments are extracted using a pre-trained object detector  while the sentence
fragments are obtained using a dependency tree relation parser. Niu et al [25] extended this work by
jointly learning hierarchical relations between phrases and image regions in an iterative reﬁnement
framework. Recently  Lee et al [21] developed a stacked cross attention network for word-region
matching. Compared to these models  our proposed query state encoding aims at integrating multiple
round queries while still using a compact representation of ﬁxed size (i.e. independent of the number
of queries)  so that retrieval times do not depend on the number or the length of the queries. We show
our compact representation to be both efﬁcient and effective for interactive image search.
More closely related to our work are Memory Networks [35  32  15]  which perform query and
possibly update operations on a predeﬁned memory space. In contrast to this line of research  we
explore a more challenging scenario where the model needs to create and update the memory (i.e.
the state vectors) on-the-ﬂy so as to maintain the states of the queries.

3 Model

Retrieving images with multi-round reﬁnements offers the potential beneﬁt of reducing the ambiguity
of each query but also raises challenges on how to integrate user queries from multiple rounds. Our
model is inspired by the observation that users naturally underspecify in their queries by referring
to local regions of the target image. We aim to capture these region level alignments by learning to
map text queries {st}T
i=1 and {vj}N
j=1
respectively  and computing the matching score of {st}T
t=1 and I by measuring and aggregating
ﬁne-grained similarities between {xi}M
j=1. Figure 2 provides an overview of our model.

t=1 and the target image I into two sets of latent vectors {xi}M

i=1 and {vj}N

3.1

Image representation

To identify candidate regions referred in the queries  we follow [1  21]. For each image I  we ﬁrst
detect the potential objects and salient stuff using the FasterRCNN detector [27]. Corresponding

3

Figure 2: Overview of our model. Drill-down maintains a ﬁxed set of state vectors X  modeling the
historical context of the user queries. Given a new query qt  our model selects and updates one of
the state vectors. The updated state vectors Xt and image region features are then projected to a
cross-modal embedding space to measure the ﬁne-grained alignment between each region-state pair.

features {cj} are extracted from the ROI pooling layer of the detector. In practice  we leverage the
object detector provided by [1]  which is pre-trained on Visual Genome [20] with 1600 predeﬁned
object and stuff classes. A linear projection vj = WI cj + bI is applied to reduce {cj} into D-
dimensional latent vectors V = {vj}N
j=1  vj ∈ RD. Here N is the number of regions in each image.
The learnable parameters for the image representation {WI   bI} are denoted as θI.

3.2 Query representation

Supporting multi-round retrieval requires a state representation for integrating the queries from
multiple turns. Solutions adopted by existing methods include applying a single recurrent network
to the concatenation of all queries [9] or a hierarchical recurrent network [7  31  22  10] modeling
individual query and historical context in separate recurrent modules. These approaches produce
a single latent vector which aggregates all queries. While state-of-the-art models [22  10] show
remarkable performance on domains such as fashion product search  we demonstrate that currently
used single-vector representations are not the most effective for capturing complex scenes with
multiple objects. Speciﬁcally  as image features used in existing methods are typically extracted from
the penultimate layer of a pre-trained image classiﬁcation or object detection model  input instances
of the same or very similar categories activate the same feature units in the extracted feature space.
Therefore  it is nontrivial for these latent representations to encode and distinguish multiple entities
from the same or very similar categories (i.e. multiple person instances).
We propose to maintain a set of latent representations X = {xi}M
i=1  xi ∈ RD for multiple turn
queries. Here M is the number of latent vectors. This parameter represents the computational
budget  since retrieval time will depend on the compactness of this representation. While users
might provide a general image description in the ﬁrst round of querying  subsequent queries typically
describe more speciﬁc regions. We aim at ﬁnding a good alignment between queries and image region
representations {vj}N
i=1 should learn to group and encode the input queries
into visually discriminative representations referring to distinct image regions. In the remaining of
the section  we ﬁrst introduce the cross modal similarity formula used in our model. We then explain
how to update the state representations {xi}M
t=1 so as to optimize their
matching score with the target image.

i=1 from the queries {st}T

j=1. An ideal set of {xi}M

3.3 Cross modal similarity
To measure the similarity of X = {xi}M
i=1 and V = {vj}N
j=1  we ﬁrst compute the cosine similarity
i vj/(cid:107)xi(cid:107)(cid:107)vj(cid:107)  where (cid:107).(cid:107) denotes the L2
of each possible state-region pair (xi  vj): s(xi  vj) = xT
norm. Given s(xi  vj)  we deﬁne the similarity s(xi  I) between a state vector xi and the target
image I as

N(cid:88)

k=1

1
N

s(xi  I) =

αiks(xi  vk)  αik =

(cid:80)N

exp(s(xi  vk)/σ)
j exp(s(xi  vj)/σ)

(1)

4

(1) red brick of fireplace(2) china plates and glasses…(t-1) flowers on the dining table(t) candle style chandelier hanging down from ceilingQuery EncoderQueriesFasterRCNN!(# %)RegionFeaturesCross Modal SimilarityState Vectors (t)GRU'(1) red brick of fireplace(2) china plates and glasses(3) group of three candle sticks on mantel(4) flowers on the dining table(5) candle style chandelier hanging down from ceiling(6) wooden chairs on the carpetNew QueryState Vectors ()*+GRUSentence Rep.  )State Vectors ()FasterRCNN!(# %)RegionFeaturesCross Modal Similarity'cosine similarity of xi and a context vector(cid:80)N

Here σ is a temperature hyper-parameter. Note that this formulation is similar to measuring the
k=1 αikvk from an attention module [24  21]. The
cross modal similarity between the state vectors X = {xi}M
i=1 and the target image I is deﬁned as
s(X  I) = 1
M

k=1 s(xk  I).

(cid:80)M

3.4 Query encoding

Given a query input st at time t  our model maps each word token wk in st to an E-dimensional
vector via a linear projection: ek = WEwk  ek ∈ RE  k = 1  ···   K  then generates the
sentence embedding via a uni-directional recurrent network φ with gated recurrent units (GRU) as:
hk = φ(ek  hk−1)  hk ∈ RD. The ﬁrst hidden state of φ is initialized as a zero vector  while the last
hidden state is treated as the sentence representation: qt = hK. We also explore using a bidirectional
encoder but ﬁnd no improvement. Given the assumption that each text query describes a sub-region
of the image  each qt only updates a subset of the state vectors. In this work  we focus on a simpliﬁed
k ∈ Xt−1. In detail  given the text query
scenario where each qt only updates a single state vector xt−1
}M
qt at time step t  our model samples xt−1
based on the probability:

from the previous state vector set Xt−1 = {xt−1

i=1

k

i

π(xt−1

k

|Xt−1  qt) =

(cid:80)

1(xt−1

k =∅)
1(xt−1
j =∅)

j

(cid:80)
exp(f (xt−1
j exp(f (xt−1

k

j

 qt))

 qt))

if Xt−1 has an empty vector

otherwise



(2)

(3)

(4)

f (xt−1

k

  qt) = W 3

π (δ(W 2

π (δ(W 1

π [xt−1

k

; qt] + b1

π)) + b2

π)) + b3
π 

j

k

π  ∈ RD×D  W 3

π ∈ RD×2D  W 2

j = ∅) is an indicator function which returns 1 if xt−1
where 1(xt−1
is an empty vector and 0 otherwise.
f (·) is a multilayer perceptron mapping the concatenation of xt−1
and qt into a scalar value. Here
π ∈ RD 
δ is the ReLU activation function  W 1
π ∈ R are model parameters. An empty state vector is initialized with zero values. Ideally  an
b3
expressive sample policy should learn to allocate a new state vector when necessary. However  we
empirically ﬁnd it beneﬁcial to update qt to an empty state vector whenever possible. Once xt−1
is
sampled  we update this state vector using a single uni-directional gated recurrent unit cell (GRU
Cell) τ: xt
). Note that our formulation is similar to a hard attention module [37].
Leveraging a soft attention is possible  but it is more computationally expensive as it would need
to update all state vectors. Our state vector update mechanism is inspired by the knowledge base
methods with external memory [22]. Our method can be interpreted as building a knowledge base
memory online from scratch  only from the query context  which can be trained end-to-end with
other modules. We denote the learnable parameters for the state vector update policy function π(·) as
θπ = {W 1

π}  and for the rest modules as θq = {WE  φ  τ}.

k = τ (qt  xt−1

π ∈ R1×D  b1

π   W 3

π   W 2

π   b1

π  b3

π  b2

π  b2

k

k

3.5 End-to-end training

Our model is trained to optimize θI  θπ and θq so as to achieve high similarity score between the
queries {st}T
t=1 and the target image I. Thus  we follow [9  21] and adopt a triplet loss on s(X  I)
with hard negatives:

Le = argmin

(cid:96)(X  I)
[α + s(X  I(cid:48)) − s(X  I)]+ + max

θI  θq

X I

X(cid:48) [α + s(X(cid:48)  I) − s(X  I)]+

(cid:96)(X  I) = max

I(cid:48)

Here  α is a margin parameter  [·]+ ≡ max(·  0). I(cid:48) and X(cid:48) are decoy images and state vectors
within the same mini-batch as the ground-truth pair (X  I) during training. Note that Le will only
optimize the parameters θI and θq. Directly optimizing θπ is difﬁcult as sampling from Equation 2
is non-differentiable. We propose to train the policy parameters via Reinforcement Learning (RL).

(cid:88)

5

i}M
Formally  the state in our RL formulation is the set of state vectors Xt = {xt
i=1  and the action
k ∈ {1  ...  M} is to select the state vector xt
k from Xt when fusing information from the embedded
query vector qt+1. The RL objective is to maximize the expected cumulative discounted rewards  so
in our case we deﬁne the reward function as the similarity between the state vectors Xt and the image
I  i.e. s(Xt  I). Note that our reward function evaluates the potential similarity at all future time step
instead of only the last step T   encouraging the model to ﬁnd the target image with fewer turns.

Supervised pre-training As optimizing the sampling policy requires reward signals from
the retrieval environment  we pre-train the model by optimizing Le with a ﬁxed policy:
|Xt−1  qt) = 1(k ≡ t (mod M))  where 1(·) is an indicator function and M is the number of
π(xt−1
state vectors. Intuitively  this policy circularly updates the state vectors in order.

k

the policy. Speciﬁcally  we estimate the state-action value Q(Xt  k) =(cid:80)T−1

Joint optimization Given the pre-trained environment  we then jointly optimize the sampling policy
and the other modules (i.e. θI   θq and θπ). Because the next state Xt+1 is a deterministic function
given the current state Xt and action k  we adopt the policy improvement strategy from [10] to update
t(cid:48)=t γt(cid:48)−ts(Xt(cid:48)+1  I) for
each state vector selection action k by sampling one look-ahead trajectory. γ is the discount factor.
The policy is then optimized to predict the most rewarding action k∗ = argmaxk Q(Xt  k) via a
cross entropy loss:

(cid:88)

Lπ = argmin

θπ

Xt qt+1

− log(π(xt

k∗|Xt  qt+1; θπ))

(5)

(cid:80)
X∗ I (cid:96)(X∗  I). The model is trained with the multi-task loss: L = L∗

We also jointly ﬁnetune θI and θq by applying Le on the rollout state vectors X∗: L∗
argminθI  θq
µ is a scalar factor determining the trade-off between the two terms.

e =
e + µLπ  where

4 Experiments

Dataset We evaluate the performance of our method on the Visual Genome dataset [20]. Each
image in Visual Genome is annotated with multiple region captions. We preprocess the data by
removing duplicate region captions (e.g. multiple captions that are exactly the same)  and images
with less than 10 region captions. This preprocessing results in 105 414 image samples  which are
further split into 92 105/5 000/9 896 for training/validation/testing. We also ensure that the images in
the test split are not used for the training of the object detector [1]. All the evaluations  including the
human subject study  are performed on the test split  which contains 9 896 images. We use region
captions as queries to train our model  thus bypassing the challenging issue of data collection for
this task. The vocabulary of the queries is built with the words that appear more than 10 times in all
region captions  resulting in a vocabulary size of 14 284. During training  queries and their orders are
randomly sampled. During validation and testing  the queries and their orders are kept ﬁxed.

Baselines We compare our method with four baseline models: (1) HRE: a hierarchical recurrent
encoder network  which is commonly adopted by recent dialog based approaches [31  22  10]. We
consider the framework using text queries as context  which consists of a sentence encoder  a context
encoder and an image encoder. The sentence encoder has the same word embedding (e.g. the linear
projection WE) and sentence embedding (e.g. the φ function) as the proposed model. The context
encoder is a uni-directional GRU network ψ that sequentially integrates the sentence features qt from
φ and generates the ﬁnal query feature ¯xt : ¯xt = ψ(qt  ¯xt−1). ¯x0 is initialized as a zero vector. The
image encoder maps the mean-pooled features of ResNet152 [13] into a one-dimensional feature
vector ¯v via a linear projection. The ResNet model is pre-trained on ImageNet [8]. The model is
trained to optimize the cosine similarity between ¯xt and ¯v by a triplet loss with hard negatives as
in [9]. (2) R-HRE: a model similar to baseline (1) but is trained with the region features {vj}N
j=1  as
in the proposed method. Speciﬁcally  the model learns to optimize the similarity term s(¯xt  I) deﬁned
in Eq.(1) by a triplet loss with hard negatives similar to Le on one state vector. (3) R-RE: a model
similar to baseline (2) but instead of using a hierarchical text encoder  this baseline uses a single
uni-directional GRU network which encodes the concatenation of the queries. (4) R-RankFusion: a

6

Figure 3: Quantitative evaluation of our models and the baselines. (A) Comparison of models
using query representations of the same memory size; (B) Comparison of the models using query
representations of different memory sizes. The horizontal axis represents the query turn.

Methods
Drill-down3×128 / 3×256 / 5×256 / 10×256
# Query Rep.
# Image Rep. 1280 / 36 × 1280 36×640 / 36 × 1280 36 × 128 / 36 × 256 / 36 × 256 / 36 × 256
# Parameters

4861k / 5830k / 5830k / 5830k

384 / 768 / 1280 / 2560

R-HRE640/1280

640 / 1280

9866k / 22820k

HRE/R-RE1280

1280

22820k

Table 1: Sizes of the query/image representations and the parameters in our models and the baselines.

model where each query is encoded by a uni-directional GRU network and each image is represented
as a set of region features {vj}N
j=1. The ranks of all images are computed separably for each turn.
The ﬁnal ranks of the images are represented as the averages of the per-turn ranks.

Implementation details We try to keep consistent conﬁgurations for all the models in our ex-
periments to better evaluate the contribution of each component. In particular  all the models are
trained with 10-turn queries (T = 10). We use ten turns as we’d like to track and demonstrate
the performance of all methods in both short-term and long-term scenarios. For each image  we
extract the top 36 regions (N = 36) detected by a pretrained Faster RCNN model  following [1].
Each embeded word vector has a dimension of 300 (E = 300). In all our experiments  we set the
temperature parameter σ to 9  the margin parameter α to 0.2  the discount factor γ to 1.0  and the
trade-off factor µ to 0.1. For optimization  we use Adam [16] with an initial learning rate of 2e − 4
and a batch size of 128. We clip the gradients in the back-propagation such that the norm of the
gradients is not larger than 10. All models are trained with at most 300 epochs  validated after each
epoch. The models which perform best on the validation set are used for evaluation.

Evaluation metrics To measure the retrieval performance  we use the common R@K metric  i.e. 
recall at K - the ratio of queries for which the target image is among the top-K retrieved images. The
R@1  R@5 and R@10 scores at each turn are reported as shown in Fig. 3.

4.1 Results on simulated user queries

Due to the lack of existing benchmarks for multiple turn image retrieval  we use the annotated
region captions in Visual Genome to mimic the user queries. As region captions focus more on
invariant information  such as image contents  and convey fewer irrelevant signals  such as different
speaking/writing styles  they could be seen as the common "abstracts" of real queries in different
forms. While we agree that strong supervisory signals such as real user queries could bridge the
domain gap and would like to explore further in this direction  we choose at this stage to use only
"weak but free" signals and investigate their potentials of being generalized to real scenarios. First 
we compare our method against the baseline models when using query representations of the same
memory size. In particular  we use 5 state vectors in our model (M = 5)  each with a dimension of
256. Accordingly  the baseline models use a 1280-d query vector. Figure 3(A) shows the per-turn

7

Figure 4: Qualitative examples of Drill-down3×128. The sequential queries and the corresponding
state vectors used to integrate them are shown on the left; The top-3 regions of the target images
attended by each state vector are shown on the right  with the same color as the corresponding state
vector. Note that all these target images rank top-1 given the input queries.

performance of the models on the test set. Here Drill-down5×256(FP) indicates the supervised pre-
trained model with the ﬁxed policy  and Drill-down5×256 indicates the jointly optimized model with a
learned policy. Both the R-RE1280 and R-HRE1280 baselines perform better than the HRE1280 model 
demonstrating the beneﬁt of incorporating region features. R-HRE1280 is superior to R-RE1280 
demonstrating the beneﬁt of hierarchical context encoding. R-RankFusion1280 performs inferior to
all other models. Note that it also requires more memory to store the ranks of all images at each turn.
Our models signiﬁcantly outperform all baselines by a large margin. On the other hand  we observe
that the performance of our model will degrade when different queries have to share the same state
vector. For example  after the 5th turn  the Drill-down5×256(FP) model gains less improvement from
each new query. Drill-down5×256 further improves Drill-down5×256(FP) by learning to distribute
the queries into the most rewarding state vectors.
To investigate the design space of the query representation  we further explore variants of our model
with different numbers of state vectors and feature dimensions. Table 1 shows the sizes of the
query/image representations and the parameters used in our models and the baselines. Note that the
R-RankFusion and R-RE models have the same size of query/image representations and parameters.
Here Drill-downM×D indicates the model with M state vectors  each with a dimension of D. As
shown in Figure 3(B)  while both Drill-down and the R-HRE baseline can be improved by increasing
the feature dimension  using more state vectors gains signiﬁcantly more improvements with the same 
or even less memory budget. For example  Drill-down3×128 signiﬁcantly outperforms R-HRE1280
with 3 times less query features  10 times less region features and 4 times less parameters. The
highest performance is achieved by the model which stores each query in a distinct state vector: 10
state vectors for 10-turn queries. Integrating multiple queries into the same state vector could make
the model “forget” the responses from earlier turns  especially when they activate the same semantic
space as the new query.
Figure 4 provides qualitative examples of the Drill-down3×128 model. Here the arrows indicate the
predicted state vectors used to incorporate the queries. We show the top-3 regions of the target images
that have the highest similarity scores with each state vector (illustrated with the same color). We
observe that the model tends to group queries with entities that potentially coincide with each other.
However  it could also lead to the “forgetting” of earlier queries. For instance  in the ﬁrst example 
when aggregating the queries “child in a stroller” and “woman in a dress” in order  the model tends
to focus on “woman” while forgetting information about “child”  as “woman” and “child” potentially
activate the same semantic subspace.

8

Figure 5: Examples of real user queries and the top-1 images from Drill-down3×256.

4.2 Results on real user queries

We evaluate our method with the queries from crowdsourced human users via a multi-round in-
teractive system adapted from [3]. Given a target image  a user is asked to search for it by pro-
viding descriptions of the image content. The system shows top-5 retrieved images to the user
per turn as context so that the user can improve the results by providing additional descriptions.
This process is repeated until the image is found or it
reaches 5 turns. We sample 80 random images from
the test set and evaluate HRED1280  R-HRED1280
and Drill-down3×256 on these images respectively.
Each image is viewed by 3 different users. For each
model  the best result on each image is selected across
users to ensure high quality responses. As shown in
Figure 6  most users (> 80%) successfully ﬁnd the
target image within 5 turns  demonstrating the ef-
fectiveness of the multi-round search paradigm and
the quality of using region captions for training. In
particular  Drill-down3×256 consistently outperforms
HRE1280 and R-HRE1280 on all evaluation metrics.
On the other hand  as real user queries have more
ﬂexible forms  e.g.
longer sentences  repeated de-
scriptions of the same region  etc  we also observe
smaller performance gaps between our method and
the baselines. We believe further efforts such as real
query data collection are needed to systematically
ﬁll this domain gap. Figure 5 shows example real user queries and the retrieval sequences using
Drill-down3×256.

Figure 6: Human subject evaluation of
the HRE1280  R-HRE1280 baselines and our
Drill-down3×256 model.

5 Conclusion

We present Drill-down  a framework that is efﬁcient and effective in interactive retrieval of speciﬁc
images of complex scenes. Our method explores in depth and addresses several challenges in multiple
round retrievals with natural language queries such as the compactness of query state representations 
and the need for region-aware features. It also demonstrates the effectiveness of training a retrieval
model with region captions as queries for interactive image search under human evaluations.

Acknowledgements We thank our anonymous reviewers for helpful feedback. This work was
funded by a research grant from SAP Research and generous gift funding from SAP Research. We
thank Tassilo Klein and Moin Nabi from SAP Research for their support.

9

References
[1] Peter Anderson  Xiaodong He  Chris Buehler  Damien Teney  Mark Johnson  Stephen Gould  and Lei
Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[2] Relja Arandjelovic and Andrew Zisserman. Multiple queries for large scale speciﬁc object retrieval. In

British Machine Vision Conference (BMVC)  pages 1–11  2012.

[3] Paola Cascante-Bonilla  Xuwang Yin  Vicente Ordonez  and Song Feng. Chat-crowd: A dialog-based
platform for visual layout composition. In Conference of the North American Chapter of the Association
for Computational Linguistics (NAACL-HLT)  2019.

[4] Ning-San Chang and King-Sun Fu. Query-by-pictorial-example. IEEE Trans. Softw. Eng.  6(6):519–524 

November 1980.

[5] Ning-San Chang and King-Sun Fu. A relational database system for images. In Pictorial Information

Systems  1980.

[6] Abhishek Das  Satwik Kottur  Khushi Gupta  Avi Singh  Deshraj Yadav  José M.F. Moura  Devi Parikh  and
Dhruv Batra. Visual Dialog. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2017.

[7] Abhishek Das  Satwik Kottur  Jose M. F. Moura  Stefan Lee  and Dhruv Batra. Learning cooperative visual
dialog agents with deep reinforcement learning. In IEEE International Conference on Computer Vision
(ICCV)  Oct 2017.

[8] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale hierarchical

image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2009.

[9] Fartash Faghri  David J Fleet  Jamie Ryan Kiros  and Sanja Fidler. Vse++: Improving visual-semantic

embeddings with hard negatives. 2018.

[10] Xiaoxiao Guo  Hui Wu  Yu Cheng  Steven Rennie  Gerald Tesauro  and Rogerio Feris. Dialog-based
interactive image retrieval. In Advances in Neural Information Processing Systems (NeurIPS)  pages
676–686  2018.

[11] Tanmay Gupta  Kevin J. Shih  Saurabh Singh  and Derek Hoiem. Aligned image-word representations
improve inductive transfer across vision-language tasks. In IEEE International Conference on Computer
Vision (ICCV)  2017.

[12] Xintong Han  Zuxuan Wu  Yu-Gang Jiang  and Larry S Davis. Learning fashion compatibility with

bidirectional lstms. In ACM Multimedia  2017.

[13] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

[14] Andrej Karpathy  Armand Joulin  and Li Fei-Fei. Deep fragment embeddings for bidirectional image
sentence mapping. In Advances in Neural Information Processing Systems (NeurIPS)  pages 1889–1897 
2014.

[15] Chloé Kiddon  Luke S. Zettlemoyer  and Yejin Choi. Globally coherent text generation with neural

checklist models. In Empirical Methods in Natural Language Processing (EMNLP)  2016.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations (ICLR)  2015.

[17] Ryan Kiros  Ruslan Salakhutdinov  and Richard S Zemel. Unifying visual-semantic embeddings with

multimodal neural language models. arXiv preprint arXiv:1411.2539  2014.

[18] Adriana Kovashka and Kristen Grauman. Attribute pivots for guiding relevance feedback in image search.

In IEEE International Conference on Computer Vision (ICCV)  December 2013.

[19] Adriana Kovashka  Devi Parikh  and Kristen Grauman. Whittlesearch: Interactive image search with

relative attribute feedback. International Journal of Computer Vision (IJCV)  115(2):185–210  2015.

[20] Ranjay Krishna  Yuke Zhu  Oliver Groth  Justin Johnson  Kenji Hata  Joshua Kravitz  Stephanie Chen 
Yannis Kalantidis  Li-Jia Li  David A Shamma  Michael Bernstein  and Li Fei-Fei. Visual genome:
Connecting language and vision using crowdsourced dense image annotations. 2016.

10

[21] Kuang-Huei Lee  Xi Chen  Gang Hua  Houdong Hu  and Xiaodong He. Stacked cross attention for

image-text matching. In European Conference on Computer Vision (ECCV)  2018.

[22] Lizi Liao  Yunshan Ma  Xiangnan He  Richang Hong  and Tat-Seng Chua. Knowledge-aware multimodal

dialogue systems. In ACM International Conference on Multimedia (ACM MM)  pages 801–809  2018.

[23] Tsung-Yi Lin  Michael Maire  Serge J. Belongie  Lubomir D. Bourdev  Ross B. Girshick  James Hays 
Pietro Perona  Deva Ramanan  Piotr Dollár  and C. Lawrence Zitnick. Microsoft COCO: Common objects
in context. European Conference on Computer Vision (ECCV)  2014.

[24] Minh-Thang Luong  Hieu Pham  and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Empirical Methods in Natural Language Processing (EMNLP)  pages
1412–1421  2015.

[25] Zhenxing Niu  Mo Zhou  Le Wang  Xinbo Gao  and Gang Hua. Hierarchical multimodal lstm for dense

visual-semantic embedding. In IEEE International Conference on Computer Vision (ICCV)  2017.

[26] Bryan A. Plummer  Liwei Wang  Chris M. Cervantes  Juan C. Caicedo  Julia Hockenmaier  and Svetlana
Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence
models. In IEEE International Conference on Computer Vision (ICCV)  pages 2641–2649  2015.

[27] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster R-CNN: Towards real-time object detection
with region proposal networks. In Advances in Neural Information Processing Systems (NeurIPS)  2015.

[28] Yong Rui  Thomas S. Huang  and Shih-Fu Chang.

Image retrieval: Current techniques  promising
directions  and open issues. Journal of Visual Communication and Image Representation  10(1):39 – 62 
1999.

[29] Iulian V. Serban  Alessandro Sordoni  Yoshua Bengio  Aaron Courville  and Joelle Pineau. Building
end-to-end dialogue systems using generative hierarchical neural network models. In AAAI Conference on
Artiﬁcial Intelligence (AAAI)  pages 3776–3783  2016.

[30] Behjat Siddiquie  Rogerio S Feris  and Larry S Davis. Image ranking and retrieval based on multi-attribute
queries. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 801–808  2011.

[31] Alessandro Sordoni  Yoshua Bengio  Hossein Vahabi  Christina Lioma  Jakob Grue Simonsen  and Jian-
Yun Nie. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion. In ACM
International on Conference on Information and Knowledge Management (CIKM)  pages 553–562  2015.

[32] Sainbayar Sukhbaatar  Arthur Szlam  Jason Weston  and Rob Fergus. End-to-end memory networks. In

Advances in Neural Information Processing Systems (NeurIPS)  2015.

[33] Mariya I. Vasileva  Bryan A. Plummer  Krishna Dusad  Shreya Rajpal  Ranjitha Kumar  and David Forsyth.
Learning type-aware embeddings for fashion compatibility. In European Conference on Computer Vision
(ECCV)  2018.

[34] Liwei Wang  Yin Li  and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings.

In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 5005–5013  2016.

[35] Jason Weston  Sumit Chopra  and Antoine Bordes. Memory networks. In International Conference on

Learning Representations (ICLR)  2015.

[36] Hao Wu  Jiayuan Mao  Yufeng Zhang  Yuning Jiang  Lei Li  Weiwei Sun  and Wei-Ying Ma. Uniﬁed
visual-semantic embeddings: Bridging vision and language with structured meaning representations. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2019.

[37] Kelvin Xu  Jimmy Ba  Ryan Kiros  Kyunghyun Cho  Aaron Courville  Ruslan Salakhudinov  Rich Zemel 
and Yoshua Bengio. Show  attend and tell: Neural image caption generation with visual attention. In
International Conference on Machine Learning (ICML)  volume 37  pages 2048–2057  2015.

11

,Fuwen Tan
Paola Cascante-Bonilla
Xiaoxiao Guo
Hui Wu
Song Feng
Vicente Ordonez