2016,High-Rank Matrix Completion and Clustering under Self-Expressive Models,We propose efficient algorithms for simultaneous clustering and completion of incomplete high-dimensional data that lie in a union of low-dimensional subspaces. We cast the problem as finding a completion of the data matrix so that each point can be reconstructed as a linear or affine combination of a few data points. Since the problem is NP-hard  we propose a lifting framework and reformulate the problem as a group-sparse recovery of each incomplete data point in a dictionary built using incomplete data  subject to rank-one constraints. To solve the problem efficiently  we propose a rank pursuit algorithm and a convex relaxation. The solution of our algorithms recover missing entries and provides a similarity matrix for clustering. Our algorithms can deal with both low-rank and high-rank matrices  does not suffer from initialization  does not need to know dimensions of subspaces and can work with a small number of data points. By extensive experiments on synthetic data and real problems of video motion segmentation and completion of motion capture data  we show that when the data matrix is low-rank  our algorithm performs on par with or better than low-rank matrix completion methods  while for high-rank data matrices  our method significantly outperforms existing algorithms.,High-Rank Matrix Completion and Clustering

under Self-Expressive Models

E. Elhamifar∗

College of Computer and Information Science

Northeastern University

Boston  MA 02115

eelhami@ccs.neu.edu

Abstract

We propose efﬁcient algorithms for simultaneous clustering and completion of
incomplete high-dimensional data that lie in a union of low-dimensional subspaces.
We cast the problem as ﬁnding a completion of the data matrix so that each point
can be reconstructed as a linear or afﬁne combination of a few data points. Since the
problem is NP-hard  we propose a lifting framework and reformulate the problem
as a group-sparse recovery of each incomplete data point in a dictionary built using
incomplete data  subject to rank-one constraints. To solve the problem efﬁciently 
we propose a rank pursuit algorithm and a convex relaxation. The solution of our
algorithms recover missing entries and provides a similarity matrix for clustering.
Our algorithms can deal with both low-rank and high-rank matrices  does not suffer
from initialization  does not need to know dimensions of subspaces and can work
with a small number of data points. By extensive experiments on synthetic data
and real problems of video motion segmentation and completion of motion capture
data  we show that when the data matrix is low-rank  our algorithm performs on
par with or better than low-rank matrix completion methods  while for high-rank
data matrices  our method signiﬁcantly outperforms existing algorithms.

1

Introduction

High-dimensional data  which are ubiquitous in computer vision  image processing  bioinformatics
and social networks  often lie in low-dimensional subspaces corresponding to different categories
they belong to [1  2  3  4  5  6]. Clustering and ﬁnding low-dimensional representations of data are
important unsupervised learning problems with numerous applications  including data compression
and visualization  image/video/costumer segmentation  collaborative ﬁltering and more.
A major challenge in real problems is dealing with missing entries in data  due to sensor failure 
ad-hoc data collection  or partial knowledge of relationships in a dataset. For instance  in estimating
object motions in videos  the tracking algorithm may loose the track of features in some video frames
[7]; in the image inpainting problem  intensity values of some pixels are missing due to sensor failure
[8]; or in recommender systems  each user provides ratings for a limited number of products [9].

Prior Work. Existing algorithms that deal with missing entries in high-dimensional data can be
divided into two main categories. The ﬁrst group of algorithms assume that data lie in a single
low-dimensional subspace. Probabilistic PCA (PPCA) [10] and Factor Analysis (FA) [11] optimize
a non-convex function using Expectation Maximization (EM)  estimating low-dimensional model
parameters and missing entries of data in an iterative framework. However  their performance depends
∗E. Elhamifar is an Assistant Professor in the College of Computer and Information Science  Northeastern

University.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

on initialization and degrades as the dimension of the subspace or the percentage of missing entries
increases. Low-rank matrix completion algorithms  such as [12  13  14  15  16  17] recover missing
entries by minimizing the convex surrogate of the rank  i.e.  nuclear norm  of the complete data
matrix. When the underlying subspace is incoherent with standard basis vectors and missing entries
locations are spread uniformly at random  they are guaranteed to recover missing entries.
The second group of algorithms addresses the more general and challenging scenario where data
lie in a union of low-dimensional subspaces. The goals in this case are to recover missing entries
and cluster data according to subspaces. Since the union of low-dimensional subspaces is often
high/full-rank  methods in the ﬁrst category are not effective. Mixture of Probabilistic PCA (MPPCA)
[18  19]  Mixture of Factor Analyzers (MFA) [20] and K-GROUSE [21] address clustering and
completion of multi-subspace data  yet suffer from dependence on initialization and perform poorly
as the dimension/number of subspaces or the percentage of missing entires increases. On the other
hand  [22] requires a polynomial number of data points in the ambient space dimension  which often
cannot be met in high-dimensional datasets. Building on the unpublished abstract in [23]  a clustering
algorithm using expectation completion on the data kernel matrix was proposed in [24]. However  the
algorithm only addresses clustering and the resulting non-convex optimization is dealt with using the
heuristic approach of shifting eigenvalues of the Hessian to nonnegative values. [25] assumes that the
observed matrix corresponds to applying a Lipschitz  monotonic function to a low-rank matrix. While
an important generalization to low-rank regime  [25] cannot cover the case of multiple subspaces.

Paper Contributions. In this paper  we propose an efﬁcient algorithm for the problem of simulta-
neous completion and clustering of incomplete data lying in a union of low-dimensional subspaces.
Building on the Sparse Subspace Clustering (SSC) algorithm [26]  we cast the problem as ﬁnding a
completion of the data so that each complete point can be efﬁciently reconstructed using a few com-
plete points from the same subspace. Since the formulation is non-convex and  in general  NP-hard 
we propose a lifting scheme  where we cast the problem as ﬁnding a group-sparse representation of
each incomplete data point in a modiﬁed dictionary  subject to a set of rank-one constraints. In our
formulation  coefﬁcients in groups correspond to pairwise similarities and missing entries of data.
More speciﬁcally  our group-sparse recovery formulation ﬁnds a few incomplete data points that
well reconstruct a given point and  at the same time  completes the selected data points in a globally
consistent fashion. Our framework has several advantages over the state of the art:

– Unlike algorithms such as [22] that require a polynomial number of points in the ambient-space
dimension  our framework needs about as many points as the subspace dimension not the ambient
space. In addition  we do not need to know dimensions of subspaces a priori.

– While two-stage methods such as [24]  which ﬁrst obtain a similarity graph for clustering and then
apply low-rank matrix completion to each cluster  fail when subspaces intersect or clustering fails 
our method simultaneously recovers missing entries and builds a similarity matrix for clustering 
hence  each goal beneﬁts from the other. Moreover  in scenarios where a hard clustering does not
exist  we can still recover missing entries.

– While we motivate and present our algorithm in the context of clustering and completion of multi-
subspace data  our framework can address any task that relies on the self-expressiveness property of
the data  e.g.  column subset selection in the presence of missing data.

– By experiments on synthetic and real data  we show that our algorithm performs on par with or better
than low-rank matrix completion methods when the data matrix is low-rank  while it signiﬁcantly
outperforms state-of-the-art clustering and completion algorithms when the data matrix is high-rank.

2 Problem Statement
Assume we have L subspaces {S(cid:96)}L
(cid:96)=1 in an n-dimensional ambient space 
Rn. Let {yj}N
j=1 denote a set of N data points lying in the union of subspaces  where we observe only
some entries of each yj
. Assume that we do not know a priori the bases for
subspaces nor do we know which data points belong to which subspace. Given the incomplete data
points  our goal is to recover missing entries and cluster the data into their underlying subspaces.

(cid:96)=1 of dimensions {d(cid:96)}L

(cid:44) [y1j

y2j

. . . ynj]

(cid:62)

2

To set the notation  let Ωj ⊆ {1  . . .   n} and Ωc
j denote  respectively  indices of observed and missing
entries of yj. Let U Ωj ∈ Rn×|Ωj| be the submatrix of the standard basis whose columns are indexed
by Ωj. We denote by P Ωj ∈ Rn×n the projection matrix onto the subspace spanned by U Ωj   i.e. 
Ωj . Hence  xj (cid:44) U(cid:62)
j| corresponds to the vector of missing entries of yj.
P Ωj
We denote by ¯yj an n-dimensional vector whose i-th coordinate is yij for i ∈ Ωj and is zero for
i ∈ Ωc
(cid:44) P Ωj yj ∈ Rn. We can write each yj as the summation of two orthogonal vectors
with observed and unobserved entries  i.e. 

(cid:44) U Ωj U(cid:62)

yj ∈ R|Ωc

j  i.e.  ¯yj

Ωc
j

U(cid:62)

Ωc
j

j

j

yj = ¯yj + U Ωc

yj = P Ωj yj + P Ωc

j=1 and zero-ﬁlled data {¯yj}N

(1)
Finally  we denote by Y ∈ Rn×N and ¯Y ∈ Rn×N matrices whose columns are complete data points
{yj}N
To address completion and clustering of multi-subspace data  we propose a uniﬁed framework to
simultaneously recover missing entries and learn a similarity graph for clustering. To do so  we build
on the SSC algorithm [26  4]  which we review next.

j=1  respectively.

yj = ¯yj + U Ωc

xj.

j

3 Sparse Subspace Clustering Review

The sparse subspace clustering (SSC) algorithm [26  4] addresses the problem of clustering complete
multi-subspace data. It relies on the observation that in a high-dimensional ambient space  while
there are many ways that each data point yj can be reconstructed using the entire dataset  a sparse
representation selects a few data points from the underlying subspace of yj  since each point in S(cid:96)
can be represented using d(cid:96) data points  in general directions  from S(cid:96). This motivates solving2

N(cid:88)

N(cid:88)

min

{c1j  ... cN j}

i=1

i=1

|cij|

s. t.

cijyi = 0  cjj = −1 

(2)

where the constraints express that each yj should be written as a combination of other points. To
infer clustering  one builds a similarity graph using sparse coefﬁcients  by connecting nodes i and j
of the graph  representing  respectively  yi and yj  with an edge with the weight wij = |cij| + |cji|.
Clustering of data is obtained then by applying spectral clustering [27] to the similarity graph.
While [4  26  28] show that  under appropriate conditions on subspace angles and data distribution 
(2) is guaranteed to recover desired representations  the algorithm requires complete data points.

3.1 Naive Extensions of SSC to Deal with Missing Entries

In the presence of missing entries  the (cid:96)1-minimization in (2) becomes non-convex  since coefﬁcients
and a subset of data entries are both unknown. A naive approach is to solve (2) using zero-ﬁlled
data points  {¯yi}N
i=1  to perform clustering and then apply standard matrix completion on each
cluster. However  the drawback of this approach is that not only it does not take advantage of the
known locations of missing entries  but also zero-ﬁlled data will no longer lie in original subspaces 
and deviate more from subspaces as the percentage of missing entries increases. Hence  a sparse
representation does not necessarily ﬁnd points from the same subspace and spectral clustering fails.
An alternative approach to deal with incomplete data is to use standard low-rank matrix completion
algorithms to recover missing values and then apply SSC to cluster data into subspaces. While this
approach works when the union of subspaces is low-rank  its effectiveness diminishes as the number
of subspaces or their dimensions increases and the data matrix becomes high/full-rank.

4 Sparse Subspace Clustering and Completion via Lifting

In this section  we propose an algorithm to recover missing entries and build a similarity graph for
clustering  given observations {yij; i ∈ Ωj}N

2(cid:96)1 is the convex surrogate of the cardinality function (cid:80)N

j=1 for N data points lying in a union of subspaces.
i=1 I(|cij|)  where I(·) is the indicator function.

3

4.1 SSC–Lifting Formulation
To address the problem  we start from the SSC observation that  given complete data {yj}N
solution of

j=1  the

N(cid:88)

N(cid:88)

min{cij}

N(cid:88)

I(|cij|) s. t.

cijyi = 0  cjj = −1  ∀j

(3)

j=1

i=1

i=1

ideally ﬁnds a representation of each yj as a linear combination of a few data points that lie in the
same subspace as of yj. I(·) denotes the indicator function  which is zero when its argument is zero
and is one otherwise. Notice that  using (1)  we can write each yi as

xi =(cid:2)¯yi U Ωc

i

(cid:3)(cid:20) 1

(cid:21)

xi

yi = ¯yi + U Ωc

i

 

(4)

where ¯yi is the i-th data point whose missing entries are ﬁlled with zeros and xi is the vector
containing missing entries of yi. Thus  substituting (4) in the optimization (3)  we would like to solve

(cid:21)

(cid:3)(cid:20) cij

(cid:2)¯yi U Ωc

N(cid:88)
i|+1 are given and known while vectors(cid:2)cij

= 0  cjj = −1  ∀j.

cijxi

i=1

i

(5)

(cid:3)(cid:62) ∈

i=1

j=1

min

{cij} {xi}

N(cid:88)

I(|cij|) s. t.

(cid:3) ∈ Rn×|Ωc

N(cid:88)
Notice that matrices(cid:2)¯yi U Ωc
cij is the same as the number of nonzero blocks(cid:2)cij
(cid:2)cij
N(cid:88)

(cid:3)(cid:62)
N(cid:88)

N(cid:88)

cijx(cid:62)

(cid:33)

j

i

s. t.

min

{cij} {xi}

I

j=1

i=1

i|+1 are unknown. In fact  the optimization (5) has two sources of non-convexity: the (cid:96)0-norm in

cijx(cid:62)
R|Ωc
the objective function and the product of unknown variables {cij} and {xi} in the constraint.
To pave the way for an efﬁcient algorithm  ﬁrst we use the fact that the number of nonzero coefﬁcients
  since cij is nonzero if and only if

cijx(cid:62)

i

is nonzero. Thus  we can write (5) as the equivalent group-sparse optimization

(cid:3)(cid:62)
(cid:3)(cid:20) cij

cijxi

(cid:21)

= 0  cjj = −1  ∀j 

(6)

where (cid:107) · (cid:107)p denotes the (cid:96)p-norm for p > 0. Next  to deal with the non-convexity of the product of
cij and xi  we use the fact that for each i ∈ {1  . . .   N}  the matrix

···
ciN
··· ciN xi

[ci1 ··· ciN ]  

(7)

cijxi

(cid:32)(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) cij
(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)p
(cid:20) ci1

Ai (cid:44)

ci1xi

i

i

(cid:2)¯yi U Ωc
(cid:21)
(cid:20) 1

=

xi

i=1

(cid:21)

is of rank one  since it can be written as the outer product of two vectors. This motivates to use a
lifting scheme where we deﬁne new optimization variables
αij (cid:44) cijxi ∈ R|Ωc
i| 

(8)

and consider the group-sparse optimization program

N(cid:88)

N(cid:88)

j=1

i=1

(cid:33)

(cid:32)(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) cij

αij

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)p

I

(cid:2)¯yi U Ωc

i

N(cid:88)

i=1

s. t.

min

{cij} {αij}
cjj =−1 ∀j

(cid:21)

(cid:3)(cid:20) cij

αij

(cid:18)(cid:20) ci1 ··· ciN

αi1 ··· αiN

(cid:21)(cid:19)

= 0  rk

= 1 ∀i  j 

(9)

where we have replaced cijxi with αij and have introduced rank-one constraints. In fact  we show
that one can recover the solution of (5) using (9) and vice versa.
Proposition 1 Given a solution {cij} and {αij} of (9)  by computing xi’s via the factorization in
(7)  {cij} and {xi} is a solution of (5). Also  given a solution {cij} and {xi} of (5)  {cij} and
{αij (cid:44) cijxi} would be a solution of (9).
Notice that  we have transferred the non-convexity of the product cijxi in (5) into a set of non-convex
rank-one constraints in (9). However  as we will see next  (9) admits an efﬁcient convex relaxation.

4

4.2 Relaxations and Extensions

The optimization program in (9) is  in general  NP-hard  due to the mixed (cid:96)0/(cid:96)p-norm in the objective
function. It is non-convex due to both mixed (cid:96)0/(cid:96)p-norm and rank-one constraints. To solve (9)  we
ﬁrst take the convex surrogate of the objective function  which corresponds to an (cid:96)1/(cid:96)p-norm [29  30] 
where we drop the indicator function and  for p ∈ {2 ∞}  solve

N(cid:88)

N(cid:88)

j=1

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) cij

αij

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)p

(cid:32) N(cid:88)

N(cid:88)

+

ρ

j=1

i=1

(cid:2)¯yi U Ωc

i

(cid:21)(cid:33)

(cid:3)(cid:20) cij

αij

(cid:18)(cid:20) ci1 ··· ciN

αi1 ··· αiN

(cid:21)(cid:19)

= 1 ∀i.

s. t. rk

min

λ
{cij  αij}
{cjj =−1}

(10)
The nonnegative parameter λ is a regularization parameter and the function ρ(·) ∈ {ρe(·)  ρa(·)}
enforces whether the reconstruction of each point should be exact or approximate  where

(cid:26)+∞ if u (cid:54)= 0

0

if u = 0

ρe(u) (cid:44)

 

ρa(u) (cid:44) 1
2

(cid:107)u(cid:107)2
2.

(11)

αi1 ··· αiN

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ci1 ··· ciN
(cid:80)N
(cid:80)N

ˆxi =

More speciﬁcally  when dealing with missing entries from noise-free data  which perfectly lie in
multiple subspaces  we enforce exact reconstruction by selecting ρ(·) = ρe(·). On the other hand 
when dealing with real data where observed entries are corrupted by noise  exact reconstruction is
infeasible or comes at the price of losing the sparsity of the solution  which is undesired. Thus  to
deal with noisy incomplete data  we consider approximate reconstruction by selecting ρ(·) = ρa(·).
Notice that the objective function of (10) is convex for p ≥ 1  while the rank-one constraints are
non-convex. We can obtain a local solution  by solving (10) with an Alternating Direction Method of
Multipliers (ADMM) framework using projection onto the set of rank-one matrices.
To obtain a convex algorithm  we use a nuclear-norm3 relaxation [12  14  15] for the rank-one
constraints  where we replace rank(Ai) = 1 with (cid:107)Ai(cid:107)∗ ≤ τ  for τ > 0. In addition  to reduce
the number of constraints and the complexity of the problem  we choose to bring the nuclear norm
constraints into the objective function using a Lagrange multiple γ > 0. Hence  we propose to solve

N(cid:88)

N(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) cij

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)p

N(cid:88)

+ γ

(cid:32) N(cid:88)

N(cid:88)

+

ρ

(cid:2)¯yi U Ωc

i

(cid:21)(cid:33)

(cid:3)(cid:20) cij

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)∗

min

λ

i=1

j=1

αij

{cij  αij}
{cjj =−1}
which is convex for p ≥ 1 and can be solved efﬁciently using convex solvers. Finally  using the
solution of (10)  we recover missing entries by ﬁnding the best rank-one factorization of each block
Ai as in (7)  which results in4

αij

j=1

i=1

i=1

 

(12)

j=1 cijαij

(13)
In addition  we use the coefﬁcients {cij} to build a similarity graph with weights wij = |cij| + |cji|
(cid:80)N
and obtain clustering of data using graph partitioning. It is important to note that we do not need to
know dimensions of subspaces a priori  since (10) automatically selects the appropriate number of
i=1 |cij| instead

data points from each subspace. Also  it is worth metioning that we can use(cid:80)N

j=1 c2
ij

j=1

.

of the group-sparsity term in (10) and (12).

i = ∅  the rank-
Remark 1 Notice that when all entries of all data points are observed  i.e.  Ωc
one constraints in (9) are trivially satisﬁed. Hence  (10) and (12) with γ = 0 reduce to the (cid:96)1-
minimization of SSC. In other words  our framework is a generalization of SSC  which simultaneously
ﬁnds similarities and missing entries for incomplete data.

Table 1 shows the stable rank5 [31] of blocks Ai of the solution for the synthetic dataset explained in
the experiments in Section 5. As the results show  the penalized optimization successfully recovers
close to rank-one solutions for practical values of γ and λ.

3The nuclear norm of A  denoted by (cid:107)A(cid:107)∗  is the sum of its singular values  i.e.  (cid:107)A(cid:107)∗ =(cid:80)
5Stable rank of B is deﬁned as(cid:80)

4The denominator is always nonzero since cii = −1 for all i.

i   where σi’s are singular values of B.

i / maxi σ2

i σi(A).

i σ2

5

Table 1: Average stable-rank of matrices Ai for high-rank data  n = 100  L = 12  d = 10  N = 600  with
ρ = 0.4  explained in section 5. Notice that rank of Ai is close to one  and as γ increases  it gets closer to one.

γ = 0.001
λ = 0.01 1.015 ± 0.005 1.009 ± 0.005 1.004 ± 0.002
1.021 ± 0.007 1.011 ± 0.006 1.006 ± 0.003
λ = 0.1

γ = 0.01

γ = 0.1

···

···

Figure 1: Subset selection and completion via lifting on the Olivetti face dataset. Top: faces from the dataset
with missing entries. Bottom: solution of our method on the dataset. We successfully recover missing entries
and  at the same time  select a subset of faces as representatives.

nonzero coefﬁcient blocks(cid:2)cij α(cid:62)

ij

(cid:3). In other words  we ﬁnd a representation of each incomplete

Notice that the mixed (cid:96)1/(cid:96)p-norm in the objective function of (10) and (12) promotes selecting a few

data point using a few other incomplete data points  while  at the same time  ﬁnd missing entries
of the selected data points. On the other hand  rank constraints on the sub-blocks of the solution
ensure that recovered missing entries are globally consistent  i.e.  if a data point takes part in the
reconstruction of multiple points  the associated missing entries in each representation are the same.

the self-expressiveness property  i.e.  yj =(cid:80)N

Remark 2 Our lifting framework can also deal with missing entries in other tasks that rely the on
i=1 cijyi. Figure 1 shows results of the extension of
our method to column subset selection [32  33] with missing entries. In fact  simultaneously selecting
a few data points that well reconstruct the entire dataset and recovering missing entires can be cast
as a modiﬁcation of (10) or (12)  where we modify the ﬁrst term in the objective function in order to
select a few nonzero blocks  Ai.

j=1

We implement (10) and (12) with(cid:80)N

5 Experiments
(cid:80)N
We study the performance of our algorithm for completion and clustering of synthetic and real data.
i=1 |cij| instead of the group-sparsity term using the
ADMM framework [34  35]. Unless stated otherwise  we set λ = 0.01 and γ = 0.1. However  the
results are stable for λ ∈ [0.005  0.05] and γ ∈ [0.01  0.5].
We compare our algorithm  SSC-Lifting  with MFA [20]  K-Subspaces with Missing Entries
(KSub-M) [21]  Low-Rank Matrix Completion [13] followed by SSC (LRMC+SSC) or LSA [36]
(LRMC+LSA)  and SSC using Column-wise Expectation Completion (SSC-CEC) [24]. It is worth
mentioning that in all experiments  we found that the performance of SSC-CEC is slightly better
than SSC using zero-ﬁlled data. In addition  as reported in [21]  KSub-M generally outperforms
the high-rank matrix completion algorithm in [22]  since the latter requires a very large number of
samples  which becomes impractical in high-dimensional problems. We compute

Clustering Error =

(14)
where Y and ˆY denote  respectively  the true and recovered matrix and (cid:107) · (cid:107)F is the Frobenius norm.

  Completion Error =

# All points

(cid:107)Y (cid:107)F

# Misclassiﬁed points

(cid:107) ˆY − Y (cid:107)F

 

5.1 Synthetic Experiments
In this section  we evaluate the performance of different algorithms on synthetic data. We generate L
random d-dimensional subspaces in Rn and draw Ng data points  at random  from each subspace. We
consider two scenarios: 1) a low-rank data matrix whose columns lie in a union of low-dimensional
subspaces; 2) a high rank data matrix whose columns lie in a union of low-dimensional subspaces.
Unless stated otherwise  for low-rank matrices  we set L = 3 and d = 5  hence  Ld = 15 < n = 100 
while for high-rank matrices  we set L = 12 and d = 10  hence  Ld = 120 > n = 100.

Completion Performance. We generate missing entries by selecting ρ fraction of entries of the
data matrix uniformly at random and dropping their values. The left and middle left plots in Figure 2

6

Figure 2: Completion errors of different algorithms as a function of ρ. Left: low-rank matrices. Middle left:
high-rank matrices. Middle right: effect of the ambient space dimension  n. Right: effect of the number of data
points in each subspace  Ng  for low-rank (solid lines) and high-rank (dashed lines) matrices.

show completion errors of different algorithms for low-rank and high-rank matrices  respectively  as
a function of the fraction of missing entries  ρ. Notice that in both cases  MFA and KSub-M have
high errors  which rapidly increase as ρ increases  due to dependence on initialization and getting
trapped in local optima. In both cases  SSC-lifting outperforms all methods across all values of
ρ. Speciﬁcally  in the low-rank regime  while LRMC and SSC-lifting have almost zero error for
ρ ≤ 0.35  the performance of LRMC quickly degrades for larger ρ’s  while SSC-lifting performs well
for ρ ≤ 0.6. On the other hand  the performance of LRMC signiﬁcantly degrades for the high-rank
case  with a large gap to SSC-lifting  which performs well for ρ < 0.45. The middle right plot in
Figure 2 demonstrates the effect of the ambient space dimension  n  for L = 7  d = 5  Ng = 100
and ρ = 0.3. Notice that errors of MFA and KSub-M increases as n increases  due to larger number
of local optima. LRMC has a large error for small values of n  where n is smaller than or close to Ld 
i.e.  high-rank regime. As n increases and matrices becomes low-rank  the error decreases. Notice
that SSC-lifting for n ≥ 40 has a low error  demonstrating its effectiveness in handling both low-rank
and high-rank matrices. Finally  the right plot in Figure 2 demonstrates the effect of the number
of points  Ng  for low and high rank matrices with ρ = 0.5. We do not show results of MFA and
KSub-M  since they have large errors for all Ng. Notice that for all values of Ng  SSC-lifting obtains
smaller errors than LRMC  verifying the effectiveness of sparsity principle to complete the data.

Clustering Performance. Next  we compare the clustering performance. To better study the effect
of missing entries  we generate missing entries by selecting a fraction δ of data points and for each
selected data point  we drop the values for a fraction ρ of its entries  both uniformly at random. We
change δ in [0.1  1.0] and ρ in [0.1  0.9] and for each pair (ρ  δ)  record the average clustering and
completion errors over 20 trials  each with different random subspaces and data points. Figure 3
shows the clustering errors of different algorithms for low-rank (top row) and high-rank (bottom
row) data matrices (completion errors provided in supplementary materials). In both cases  MFA
performs poorly  due to local optima. While LRMC+SSC  SSC-CEC and SSC-Lifting perform
similarly for low-rank matrices  SSC-Lifting performs best among all methods for high-rank matrices.
In particular  when the percentage of missing entries  ρ  is more than 70%  SSC-Lifting performs
signiﬁcantly better than other algorithms. It is important to notice that for small values of (ρ  δ)  since
completion errors via SSC-Lifting and LRMC are sufﬁciently small  the recovered matrices will be
noisy versions of the original matrices. As a result  Lasso-type optimizations of SSC and SSC-Lifting
will succeed in recovering subspace-sparse representations  leading to zero clustering errors. In the
high-rank case  SSC-EC has a higher clustering error than LRMC and SSC-Lifting  which is due to
the fact that it relies on a heuristic of shifting eigenvalues of the kernel matrix to non-negative values.

5.2 Real Experiments on Motion Segmentation
We consider the problem of motion segmentation [37  38] with missing entries on the Hopkins 155
dataset  with 155 sequences of 2 and 3 motions. Since the dataset consists of complete feature
trajectories (incomplete trajectories were removed manually to form the dataset)  we select ρ fraction
of feature points across all frames uniformly at random and remove their x − y coordinate values.
Left plot in Figure 4 shows clustering error bars of different algorithms on the dataset as a function of
ρ. Notice that in all cases  MFA and SSC-CEC have large errors  due to  respectively  dependence
on initialization and the heuristic convex reformulation. On the other hand  LRMC+SSC and SSC-
Lifting perform well  achieving less than 5% error for all values of ρ. This comes from the fact that
sequences have at most L = 3 motions and dimension of each motion subspace is at most d = 4 
hence  Ld ≤ 12 (cid:28) 2F   where F is the number of video frames. Since the data matrix is low-rank
and LRMC succeeds  SSC and our method achieve roughly the same errors for different values of ρ.

7

00.10.20.30.40.50.60.70.80.9Missing entries fraction00.20.40.60.81Completion errorMFAKSub-MLRMCSSC-lifting00.10.20.30.40.50.60.70.80.9Missing entries fraction00.20.40.60.81Completion errorMFAKSub-MLRMCSSC-lifting2030405060708090100110120n00.20.40.60.81Completion errorMFAKSub-MLRMCSSC-lifting2030405060708090100110120Ng00.20.40.60.8Completion errorLRMCSSC-liftingFigure 3: Clustering errors for low-rank matrices (top row) with L = 3  d = 5  n = 100 and high-rank matrices
(bottom row) with L = 12  d = 10  n = 100 as a function of (ρ  δ)  where δ is the fraction of data with missing
entires (vertical axis) and ρ is the fraction of missing entries in each affected point (horizontal axis). Left to
Right: MFA  SSC-CEC  LRMC+SSC and SSC-Lifting.

Figure 4: Left: Clustering error bars of MFA  LRMC+LSA  LRMC+SSC  SSC-CEC and SSC-Lifting as a
function of the fraction of missing entries  ρ. Middle: Singular values of CMU Mocap data reveal that each
activity lie in a low-dimensional subspace. Right: Average completion errors of MFA  LRMC and SSC-Lifting
on the CMU Mocap Dataset as a function of ρ. Solid lines correspond to δ = 0.5  i.e.  50% of data have missing
entries  while dashed lines correspond to δ = 1  i.e.  all data have missing entries.

5.3 Real Experiments on Motion Capture Data
We consider completion of time-series trajectories from motion capture sensors  where a trajectory
consists of different human activities  such as running  jumping  squatting  etc. We use the CMU
Mocap dataset  where each data point corresponds to measurements from n sensors at a particular
time instant. Since transition from one activity to another happens gradually  we do not consider
clustering. However  as the middle plot in Figure 4 shows  excluding the transition time periods  data
from each activity lie in a low-rank subspace. Since typically there are L ≈ 7 activities  each having a
dimension of d ≈ 8  and there are n = 42 sensors  the data matrix is full-rank  as Ld ≈ 56 > n = 42.
To evaluate performance of different algorithms  we select δ ∈ {0.5  1.0} fraction of data points and
remove entries of ρ ∈ {0.1  0.2  0.3  0.4  0.5  0.6  0.7} fraction of each selected point  both uniformly
at random. Right plot in Figure 4 shows completion errors of different algorithms as a function of ρ
for δ ∈ {0.5  1.0}. Notice that  unlike the previous experiment  since the data matrix is high-rank 
LRMC has a large completion error  similar to synthetic experiments. On the other hand  SSC-Lifting
error is less than 0.1 for ρ = 0.1 and less than 0.55 for ρ = 0.7. In all cases  for δ = 1  the
performance degrades with respect to δ = 0.5. Lastly  it is important to notice that MFA performs
slightly better than LRMC  demonstrating the importance of the union of low-dimensional subspaces
model for the problem. However  getting trapped in local optima does not allow MFA to take full
advantage of such a model  as opposed to SSC-Lifting.

6 Conclusions
We proposed efﬁcient algorithms  based on lifting  for simultaneous clustering and completion of
incomplete multi-subspace data. By extensive experiments on synthetic and real data  we showed
that for low-rank data matrices  our algorithm performs on par with or better than low-rank matrix
completion methods  while for high-rank data matrices  it signiﬁcantly outperforms existing algo-
rithms. Theoretical guarantees of the proposed method and scaling the algorithm to large data is the
subject of our ongoing research.

8

Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction 0.10.30.50.70.910.80.60.40.200.20.40.60.8Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction0.10.30.50.70.910.80.60.40.2Corrupted data fractionMissing entries fraction 0.10.30.50.70.910.80.60.40.200.20.40.60.80.10.20.30.40.50.60.7Missing entries fraction00.10.20.30.4Clustering errorSSC-CECMFALRMC+LSALRMC+SSCSSC-lifting5101520051015IndexSingular values squatrunstandarm−upjumpdrinkpunch0.10.20.30.40.50.60.7Missing entries fraction00.20.40.60.8Completion errorLRMCMFASSC-liftingReferences
[1] R. Basri and D. Jacobs  “Lambertian reﬂection and linear subspaces ” IEEE Transactions on Pattern Analysis and Machine Intelligence 

vol. 25  2003.

[2] T. Hastie and P. Simard  “Metrics and models for handwritten character recognition ” Statistical Science  1998.
[3] C. Tomasi and T. Kanade  “Shape and motion from image streams under orthography ” International Journal of Computer Vision  vol. 9 

1992.

[4] E. Elhamifar and R. Vidal  “Sparse subspace clustering: Algorithm  theory  and applications ” IEEE Transactions on Pattern Analysis

and Machine Intelligence  2013.

[5] G. Chen and G. Lerman  “Spectral curvature clustering (SCC) ” International Journal of Computer Vision  vol. 81  2009.
[6] A. Zhang  N. Fawaz  S. Ioannidis  and A. Montanari  “Guess who rated this movie: Identifying users through subspace clustering ”

Uncertainty in Artiﬁcial Intelligence (UAI)  2012.

[7] R. Vidal  R. Tron  and R. Hartley  “Multiframe motion segmentation with missing data using PowerFactorization and GPCA ” Interna-

tional Journal of Computer Vision  vol. 79  2008.

[8] J. Mairal  F. Bach  J. Ponce  and G. Sapiro  “Online dictionary learning for sparse coding ” in International Conference on Machine

Learning  2009.

[9] D. Park  J. Neeman  J. Zhang  S. Sanghavi  and I. S. Dhillon  “Preference completion: Large-scale collaborative ranking from pairwise

comparisons ” International Conference on Machine Learning (ICML)  2015.

[10] M. Tipping and C. Bishop  “Probabilistic principal component analysis ” Journal of the Royal Statistical Society  vol. 61  1999.
[11] M. Knott and D. Bartholomew  Latent variable models and factor analysis. London: Edward Arnold  1999.
[12] E. J. Candès and B. Recht  “Exact matrix completion via convex optimization ” Foundations of Computational Mathematics  vol. 9  2008.
[13] E. J. Candès and Y. Plan  “Matrix completion with noise ” Proceedings of the IEEE  2009.
[14] R. Keshavan  A. Montanari  and S. Oh  “Matrix completion from noisy entries ” IEEE Transactions on Information Theory  2010.
[15] Y. Chen  H. Xu  C. Caramanis  and S. Sanghavi  “Robust matrix completion with corrupted columns ” in International Conference on

Machine Learning (ICML)  2011.

[16] S. Bhojanapalli and P. Jain  “Universal matrix completion ” International Conference on Machine Learning (ICML)  2013.
[17] K. Y. Chiang  C. J. Hsieh  and I. S. Dhillon  “Matrix completion with noisy side information ” Neural Information Processing Systems

(NIPS)  2015.

[18] M. Tipping and C. Bishop  “Mixtures of probabilistic principal component analyzers ” Neural Computation  vol. 11  1999.
[19] A. Gruber and Y. Weiss  “Multibody factorization with uncertainty and missing data using the em algorithm ” IEEE Conference on

Computer Vision and Pattern Recognition (CVPR)  2004.

[20] Z. Ghahramani and G. E. Hinton  “The em algorithm for mixtures of factor analyzers ” Technical Report CRG-TR-96-1  Dept. Computer

Science  Univ. of Toronto  1996.

[21] L. Balzano  A. Szlam  B. Recht  and R. Nowak  “K-subspaces with missing data ” IEEE Statistical Signal Processing Workshop  2012.
[22] B. Eriksson  L. Balzano  and R. Nowak  “High rank matrix completion ” International Conference on Artiﬁcial Intelligence and Statistics 

2012.

[23] E. J. Candes  L. Mackey  and M. Soltanolkotabi  “From robust subspace clustering to full-rank matrix completion ” Unpublished abstract 

2014.

[24] C. Yang  D. Robinson  and R. Vidal  “Sparse subspace clustering with missing entries ” International Conference on Machine Learning

(ICML)  2015.

[25] R. Ganti  L. Balzano  and R. Willett  “Matrix completion under monotonic single index models ” Neural Information Processing Systems

(NIPS)  2015.

[26] E. Elhamifar and R. Vidal  “Sparse subspace clustering ” in IEEE Conference on Computer Vision and Pattern Recognition  2009.
[27] A. Ng  Y. Weiss  and M. Jordan  “On spectral clustering: analysis and an algorithm ” in Neural Information Processing Systems  2001.
[28] M. Soltanolkotabi  E. Elhamifar  and E. J. Candes  “Robust subspace clustering ” Annals of Statistics  2014.
[29] B. Zhao  G. Rocha  and B. Yu  “The composite absolute penalties family for grouped and hierarchical selection ” The Annals of Statistics 

vol. 37  2009.

[30] R. Jenatton  J. Y. Audibert  and F. Bach  “Structured variable selection with sparsity-inducing norms ” Journal of Machine Learning

Research  vol. 12  2011.

[31] J. Tropp  “Column subset selection  matrix factorization  and eigenvalue optimization ” in ACM-SIAM Symp. Discrete Algorithms

(SODA)  2009.

[32] E. Elhamifar  G. Sapiro  and S. S. Sastry  “Dissimilarity-based sparse subset selection ” IEEE Transactions on Pattern Analysis and

Machine Intelligence  2016.

[33] E. Elhamifar  G. Sapiro  and R. Vidal  “See all by looking at a few: Sparse modeling for ﬁnding representative objects ” in IEEE

Conference on Computer Vision and Pattern Recognition  2012.

[34] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein  “Distributed optimization and statistical learning via the alternating direction

method of multipliers ” Foundations and Trends in Machine Learning  vol. 3  2010.

[35] D. Gabay and B. Mercier  “A dual algorithm for the solution of nonlinear variational problems via ﬁnite-element approximations ” Comp.

Math. Appl.  vol. 2  1976.

[36] J. Yan and M. Pollefeys  “A general framework for motion segmentation: Independent  articulated  rigid  non-rigid  degenerate and

non-degenerate ” in European Conf. on Computer Vision  2006.

[37] J. Costeira and T. Kanade  “A multibody factorization method for independently moving objects.” Int. Journal of Computer Vision 

vol. 29  1998.

[38] K. Kanatani  “Motion segmentation by subspace separation and model selection ” in IEEE Int. Conf. on Computer Vision  vol. 2  2001.

9

,Ehsan Elhamifar