2007,Sparse Overcomplete Latent Variable Decomposition of Counts Data,An important problem in many fields is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However  they are limited in the number of components they can extract and also do not have a provision to control the expressiveness" of the extracted components. In this paper  we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.",Sparse Overcomplete Latent Variable Decomposition

of Counts Data

Madhusudana Shashanka

Mars  Incorporated
Hackettstown  NJ

Bhiksha Raj

Mitsubishi Electric Research Labs

Cambridge  MA

Paris Smaragdis
Adobe Systems
Newton  MA

shashanka@cns.bu.edu

bhiksha@merl.com

paris@adobe.com

Abstract

An important problem in many ﬁelds is the analysis of counts data to extract mean-
ingful latent components. Methods like Probabilistic Latent Semantic Analysis
(PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this pur-
pose. However  they are limited in the number of components they can extract and
lack an explicit provision to control the “expressiveness” of the extracted compo-
nents. In this paper  we present a learning formulation to address these limitations
by employing the notion of sparsity. We start with the PLSA framework and
use an entropic prior in a maximum a posteriori formulation to enforce sparsity.
We show that this allows the extraction of overcomplete sets of latent components
which better characterize the data. We present experimental evidence of the utility
of such representations.

1 Introduction

A frequently encountered problem in many ﬁelds is the analysis of histogram data to extract mean-
ingful latent factors from it. For text analysis where the data represent counts of word occurrences
from a collection of documents  popular techniques available include Probabilistic Latent Semantic
Analysis (PLSA; [6]) and Latent Dirichlet Allocation (LDA; [2]). These methods extract compo-
nents that can be interpreted as topics characterizing the corpus of documents. Although they are
primarily motivated by the analysis of text  these methods can be applied to analyze arbitrary count
data. For example  images can be interpreted as histograms of multiple draws of pixels  where each
draw corresponds to a “quantum of intensity”. PLSA allows us to express distributions that underlie
such count data as mixtures of latent components. Extensions to PLSA include methods that attempt
to model how these components co-occur (eg. LDA  Correlated Topic Model [1]).

One of the main limitations of these models is related to the number of components they can extract.
Realistically  it may be expected that the number of latent components in the process underlying
any dataset is unrestricted. However  the number of components that can be discovered by LDA
or PLSA is restricted by the cardinality of the data  e.g. by the vocabulary of the documents  or
the number of pixels of the image analyzed. Any analysis that attempts to ﬁnd an overcomplete
set of a larger number of components encounters the problem of indeterminacy and is liable to
result in meaningless or trivial solutions. The second limitation of the models is related to the
“expressiveness” of the extracted components i.e. the information content in them. Although the
methods aim to ﬁnd “meaningful” latent components  they do not actually provide any control over
the information content in the components.

In this paper  we present a learning formulation that addresses both these limitations by employing
the notion of sparsity. Sparse coding refers to a representational scheme where  of a set of compo-
nents that may be combined to compose data  only a small number are combined to represent any
particular instance of the data (although the speciﬁc set of components may change from instance to

1

instance). In our problem  this translates to permitting the generating process to have an unrestricted
number of latent components  but requiring that only a small number of them contribute to the com-
position of the histogram represented by any data instance. In other words  the latent components
must be learned such that the mixture weights with which they are combined to generate any data
have low entropy – a set with low entropy implies that only a few mixture weight terms are signiﬁ-
cant. This addresses both the limitations. Firstly  it largely eliminates the problem of indeterminacy
permitting us to learn an unrestricted number of latent components. Secondly  estimation of low
entropy mixture weights forces more information on to the latent components  thereby making them
more expressive.

The basic formulation we use to extract latent components is similar to PLSA. We use an entropic
prior to manipulate the entropy of the mixture weights. We formulate the problem in a maximum a
posteriori framework and derive inference algorithms. We use an artiﬁcial dataset to illustrate the
effects of sparsity on the model. We show through simulations that sparsity can lead to components
that are more representative of the true nature of the data compared to conventional maximum like-
lihood learning. We demonstrate through experiments on images that the latent components learned
in this manner are more informative enabling us to predict unobserved data. We also demonstrate
that they are more discriminative than those learned using regular maximum likelihood methods.
We then present conclusions and avenues for future work.

2 Latent Variable Decomposition

Consider an F × N count matrix V. We will consider each column of V to be the histogram of an
independent set of draws from an underlying multinomial distribution over F discrete values. Each
column of V thus represents counts in a unique data set. Vf n  the f th row entry of Vn  the nth
column of V  represents the count of f (or the f th discrete symbol that may be generated by the
multinomial) in the nth data set. For example  if the columns of V represent word count vectors
for a collection of documents  Vf n would be the count of the f th word of the vocabulary in the nth
document in the collection.

We model all data as having been generated by a process that is characterized by a set of latent
probability distributions that  although not directly observed  combine to compose the distribution
of any data set. We represent the probability of drawing f from the zth latent distribution by P (f |z) 
where z is a latent variable. To generate any data set  the latent distributions P (f |z) are combined in
proportions that are speciﬁc to that set. Thus  each histogram (column) in V is the outcome of draws
from a distribution that is a column-speciﬁc composition of P (f |z). We can deﬁne the distribution
underlying the nth column of V as

Pn(f ) = X

P (f |z)Pn(z) 

z

(1)

where Pn(f ) represents the probability of drawing f in the nth data set in V  and Pn(z) is the
mixing proportion signifying the contribution of P (f |z) towards Pn(f ).
Equation 1 is functionally identical to that used for Probabilistic Latent Semantic Analysis of text
data [6]1: if the columns Vn of V represent word count vectors for documents  P (f |z) represents
the zth latent topic in the documents. Analogous interpretations may be proposed for other types
of data as well. For example  if each column of V represents one of a collection of images (each
of which has been unraveled into a column vector)  the P (f |z)’s would represent the latent “bases”
that compose all images in the collection. In maintaining this latter analogy  we will henceforth refer
to P (f |z) as the basis distributions for the process.
Geometrically  the normalized columns of V (obtained by scaling the entries of Vn to sum to 1.0) 
¯Vn  which we refer to as data distributions  may be viewed as F -dimensional vectors that lie in an
(F − 1) simplex. The distributions Pn(f ) and basis distributions P (f |z) are also F -dimensional
vectors in the same simplex. The model expresses Pn(f ) as points within the convex hull formed
by the basis distributions P (f |z). The aim of the model is to determine P (f |z) such that the model

1PLSA actually represents the joint distribution of n and f as P (n  f ) = P (n) Pz P (f |z)P (z|n). How-
ever the maximum likelihood estimate of P (n) is simply the fraction of all observations from all data sets that
occurred in the nth data set and does not affect the estimation of P (f |z) and P (z|n).

2

(100)

(100)

2 Basis Vectors

(010)

 

3 Basis Vectors

(010)

 

 

(001)

Simplex Boundary
Data Points
Basis Vectors
Approximation

 

(001)

Simplex Boundary
Data Points
Basis Vectors
Convex Hull

Figure 1: Illustration of the latent variable model. Panels show 3-dimensional data distributions as
points within the Standard 2-Simplex given by {(001)  (010)  (100)}. The left panel shows a set of
2 Basis distributions (compact code) derived from the 400 data points. The right panel shows a set
of 3 Basis distributions (complete code). The model approximates data distributions as points lying
within the convex hull formed by the basis distributions. Also shown are two data points (marked
by + and ×) and their approximations by the model (respectively shown by ♦ and (cid:3)).

Pn(f ) for any data distribution ¯Vn approximates it closely. Since Pn(f ) is constrained to lie within
the simplex deﬁned by P (f |z)  it can only model ¯Vn accurately if the latter also lies within the
hull. Any ¯Vn that lies outside the hull is modeled with error. Thus  the objective of the model
is to identify P (f |z) such that they form a convex hull surrounding the data distributions. This is
illustrated in Figure 1 for a synthetic data set of 400 3-dimensional data distributions.

2.1 Parameter Estimation

Given count matrix V  we estimate P (f |z) and Pn(z) to maximize the likelihood of V. This can be
done through iterations of equations derived using the Expectation Maximization (EM) algorithm:

Pn(z|f ) =

Pn(z)P (f |z)

Pz Pn(z)P (f |z)

 

and

P (f |z) = Pn Vf nPn(z|f )

Pf Pn Vf nPn(z|f )

 

Pn(z) = Pf Vf nPn(z|f )
Pz Pf Vf nPn(z|f )

(2)

(3)

Detailed derivation is shown in supplemental material. The EM algorithm guarantees that the above
multiplicative updates converge to a local optimum.

2.2 Latent Variable Model as Matrix Factorization

We can write the model given by equation (1) in matrix form as pn = Wgn  where pn is a column
vector indicating Pn(f )  gn is a column vector indicating Pn(z)  and W is a matrix with the (f  z)-
th element corresponding to P (f |z). If we characterize V by R basis distributions  W is an F × R
matrix. Concatenating all column vectors pn and gn as matrices P and G respectively  one can
write the model as P = WG  where G is an R × N matrix. It is easy to show (as demonstrated in
the supplementary material) that the maximum likelihood estimator for P (f |z) and Pn(z) attempts
to minimize the Kullback-Leibler (KL) distance between the normalized data distribution Vn and
Pn(f )  weighted by the total count in Vn.
In other words  the model of Equation (1) actually
represents the decomposition

(4)
where D is an N × N diagonal matrix  whose nth diagonal element is the total number of counts
in Vn and H = GD. The astute reader might recognize the decomposition of equation (4) as Non-
negative matrix factorization (NMF; [8]). In fact equations (2) and (3) can be shown to be equivalent
to one of the standard update rules for NMF.

V ≈ WGD = WH

Representing the decomposition in matrix form immediately reveals one of the shortcomings of the
basic model. If R  the number of basis distributions  is equal to F   then a trivial solution exists
that achieves perfect decomposition: W = I; H = V  where I is the identity matrix (although the
algorithm may not always arrive at this solution). However  this solution is no longer of any utility
to us since our aim is to derive basis distributions that are characteristic of the data  whereas the

3

Enclosing triangles for ’+’:
ABG  ABD  ABE  ACG 
ACD  ACE  ACF

(010)

C

(100)

A

B

F

G

DE
(001)

Simplex Boundary
Data Points
Basis Vectors

Figure 2: Illustration of the effect of sparsifying H on the dataset shown in Figure 1. A-G represent
7 basis distributions. The ‘+’ represents a typical data point. It can be accurately represented by
any set of three or more bases that form an enclosing polygon and there are many such polygons.
However  if we restrict the number of bases used to enclose ‘+’ to be minimized  only the 7 enclosing
triangles shown remain as valid solutions. By further imposing the restriction that the entropy of
the mixture weights with which the bases (corners) must be combined to represent ‘+’ must be
minimum  only one triangle is obtained as the unique optimal enclosure.

columns of W in this trivial solution are not speciﬁc to any data  but represent the dimensions of
the space the data lie in. For overcomplete decompositions where R > F   the solution becomes
indeterminate – multiple perfect decompositions are possible.

The indeterminacy of the overcomplete decomposition can  however  be greatly reduced by im-
posing a restriction that the approximation for any ¯Vn must employ minimum number of basis
distributions required. By further imposing the constraint that the entropy of gn must be minimized 
the indeterminacy of the solution can often be eliminated as illustrated by Figure 2. This principle 
which is related to the concept of sparse coding [5]  is what we will use to derive overcomplete sets
of basis distributions for the data.

3 Sparsity in the Latent Variable Model

Sparse coding refers to a representational scheme where  of a set of components that may be com-
bined to compose data  only a small number are combined to represent any particular input. In the
context of basis decompositions  the goal of sparse coding is to ﬁnd a set of bases for any data set
such that the mixture weights with which the bases are combined to compose any data are sparse.
Different metrics have been used to quantify the sparsity of the mixture weights in the literature.
Some approaches minimize variants of the Lp norm of the mixture weights (eg. [7]) while other
approaches minimize various approximations of the entropy of the mixture weights.

In our approach  we use entropy as a measure of sparsity. We use the entropic prior  which has
been used in the maximum entropy literature (see [9]) to manipulate entropy. Given a probability
distribution θ  the entropic prior is deﬁned as Pe(θ) ∝ e−αH(θ)  where H(θ) = − Pi θi log θi is the
entropy of the distribution and α is a weighting factor. Positive values of α favor distributions with
lower entropies while negative values of α favor distributions with higher entropies. Imposing this
prior during maximum a posteriori estimation is a way to manipulate the entropy of the distribution.
The distribution θ could correspond to the basis distributions P (f |z) or the mixture weights Pn(z)
or both. A sparse code would correspond to having the entropic prior on Pn(z) with a positive
value for α. Below  we consider the case where both the basis vectors and mixture weights have the
entropic prior to keep the exposition general.

3.1 Parameter Estimation

We use the EM algorithm to derive the update equations. Let us examine the case where both
P (f |z) and Pn(z) have the entropic prior. The set of parameters to be estimated is given by Λ =
{P (f |z)  Pn(z)}. The a priori distribution over the parameters  P (Λ)  corresponds to the entropic
priors. We can write log P (Λ)  the log-prior  as

α X

X

P (f |z) log P (f |z) + β X

X

Pn(z) log Pn(z) 

(5)

z

f

n

z

4

(100)

(100)

(100)

3 Basis Vectors

(010)

7 Basis Vectors

(010)

10 Basis Vectors

(010)

(001)

(001)

(001)

(100)

(100)

(100)

7 Basis Vectors

(010)

7 Basis Vectors

(010)

7 Basis Vectors

(010)

Sparsity Param = 0.01

Sparsity Param = 0.05

Sparsity Param = 0.3

(001)

(001)

(001)

Figure 3: Illustration of the effect of sparsity on the synthetic data set from Figure 1. For visual
clarity  we do not display the data points.
Top panels: Decomposition without sparsity. Sets of 3 (left)  7 (center)  and 10 (right) basis dis-
tributions were obtained from the data without employing sparsity. In each case  20 runs of the
estimation algorithm were performed from different initial values. The convex hulls formed by the
bases from each of these runs are shown in the panels from left to right. Notice that increasing the
number of bases enlarges the sizes of convex hulls  none of which characterize the distribution of
the data well.
Bottom panels: Decomposition with sparsity. The panels from left to right show the 20 sets of
estimates of 7 basis distributions  for increasing values of the sparsity parameter for the mixture
weights. The convex hulls quickly shrink to compactly enclose the distribution of the data.

where α and β are parameters indicating the degree of sparsity desired in P (f |z) and Pn(z) respec-
tively. As before  we can write the E-step as

The M-step reduces to the equations

Pn(z|f ) =

Pn(z)P (f |z)

Pz Pn(z)P (f |z)

.

ξ

P (f |z)

+ α + α log P (f |z) + ρz = 0 

ω

Pn(z)

+ β + β log Pn(z) + τn = 0

(6)

(7)

where we have let ξ represent Pn Vf nPn(z|f )  ω represent Pf Vf nPn(z|f )  and ρz  τn are La-
grange multipliers. The above M-step equations are systems of simultaneous transcendental equa-
tions for P (f |z) and Pn(z). Brand [3] proposes a method to solve such equations using the Lambert
W function [4]. It can be shown that P (f |z) and Pn(z) can be estimated as

ˆP (f |z) =

−ξ/α

W(−ξe1+ρz /α/α)

 

ˆPn(z) =

−ω/β

W(−ωe1+τn/β/β)

.

(8)

Equations (7)  (8) form a set of ﬁxed-point iterations that typically converge in 2-5 iterations [3].

The ﬁnal update equations are given by equation (6)  and the ﬁxed-point equation-pairs (7)  (8). De-
tails of the derivation are provided in supplemental material. Notice that the above equations reduce
to the maximum likelihood updates of equations (2) and (3) when α and β are set to zero. More
generally  the EM algorithm aims to minimize the KL distance between the true distribution of the
data and that of the model  i.e. it attempts to arrive at a model that conserves the entropy of the data 
subject to the a priori constraints. Consequently  reducing entropy of the mixture weights Pn(z) to
obtain a sparse code results in increased entropy (information) of basis distributions P (f |z).

3.2 Illustration of the Effect of Sparsity

The effect and utility of sparse overcomplete representations is demonstrated by Figure 3. In this
example  the data (from Figure 1) have four distinct quadrilaterally located clusters. This structure
cannot be accurately represented by three or fewer basis distributions  since they can  at best specify

5

A. Occluded Faces

B. Reconstructions

C. Original Test Images

Figure 4: Application of latent variable decomposition for reconstructing faces from occluded im-
ages (CBCL Database). (A). Example of a random subset of 36 occluded test images. Four 6 × 6
patches were removed from the images in several randomly chosen conﬁgurations (corresponding
to the rows). (B). Reconstructed faces from a sparse-overcomplete basis set of 1000 learned compo-
nents (sparsity parameter = 0.1). (C). Original test images shown for comparison.

a triangular simplex  as demonstrated by the top left panel in the ﬁgure. Simply increasing the num-
ber of bases without constraining the sparsity of the mixture weights does not provide meaningful
solutions. However  increasing the sparsity quickly results in solutions that accurately characterize
the distribution of the data.

A clearer intuition is obtained when we consider the matrix form of the decomposition in Equation
4. The goal of the decomposition is often to identify a set of latent distributions that characterize
the underlying process that generated the data V. When no sparsity is enforced on the solution  the
trivial solution W = I  H = V is obtained at R = F . In this solution  the entire information in
V is borne by H and the bases W becomes uninformative  i.e. they no longer contain information
about the underlying process.

However  by enforcing sparsity on H the information V is transferred back to W  and non-trivial
solutions are possible for R > F . As R increases  however  W become more and more data-like.
At R = N another trivial solution is obtained: W = V  and H = D (i.e. G = I). The columns of
W now simply represent (scaled versions) of the speciﬁc data V rather than the underlying process.
For R > N the solutions will now become indeterminate. By enforcing sparsity  we have thus
increased the implicit limit on the number of bases that can be estimated without indeterminacy
from the smaller dimension of V to the larger one.

4 Experimental Evaluation

We hypothesize that if the learned basis distribution are characteristic of the process that generates
the data  they must not only generalize to explain new data from the process  but also enable predic-
tion of components of the data that were not observed. Secondly  the bases for a given process must
be worse at explaining data that have been generated by any other process. We test both these hy-
potheses below. In both experiments we utilize images  which we interpret as histograms of repeated
draws of pixels  where each draw corresponds to a quantum of intensity.

4.1 Face Reconstruction

In this experiment we evaluate the ability of the overcomplete bases to explain new data and predict
the values of unobserved components of the data. Speciﬁcally  we use it to reconstruct occluded
portions of images. We used the CBCL database consisting of 2429 frontal view face images hand-
aligned in a 19 × 19 grid. We preprocessed the images by linearly scaling the grayscale intensities
so that pixel mean and standard deviation was 0.25  and then clipped them to the range [0  1].
2000 images were randomly chosen as the training set. 100 images from the remaining 429 were
randomly chosen as the test set. To create occluded test images  we removed 6 × 6 grids in ten
random conﬁgurations for 10 test faces each  resulting in 100 occluded images. We created 4 sets of
test images  where each set had one  two  three or four 6 × 6 patches removed. Figure 4A represents
the case where 4 patches were removed from each face.

In a training stage  we learned sets of K ∈ {50  200  500  750  1000} basis distributions from the
training data. Sparsity was not used in the compact (R < F ) case (50 and 200 bases) and sparsity

6

Basis Vectors

Mixture Weights

Basis Vectors

Mixture Weights

Pixel Image

Pixel Image

Figure 5: 25 Basis distributions (represented as images) extracted for class “2” from training data
without sparsity on mixture weights (Left Panel  sparsity parameter = 0) and with sparsity on mixture
weights (Right Panel  sparsity parameter = 0.2). Basis images combine in proportion to the mixture
weights shown to result in the pixel images shown.

β = 0

β = 0.2

β = 0.5

Figure 6: 25 basis distributions learned from training data for class “3” with increasing sparsity
parameters on the mixture weights. The sparsity parameter was set to 0  0.2 and 0.5 respectively. In-
creasing the sparsity parameter of mixture weights produces bases which are holistic representations
of the input (histogram) data instead of parts-like features.

was imposed (parameter = 0.1) on the mixture weights in the overcomplete cases (500  750 and 1000
basis vectors).

The procedure for estimating the occluded regions of the a test image has two steps. In the ﬁrst step 
we estimate the distribution underlying the image as a linear combination of the basis distributions.
This is done by iterations of Equations 2 and 3 to estimate Pn(z) (the bases P (f |z)  being already
known  stay ﬁxed) based only on the pixels that are observed (i.e. we marginalize out the occluded
pixels). The combination of the bases P (f |z) and the estimated Pn(z) give us the overall distribution
Pn(f ) for the image. The occluded pixel values at any pixel f is estimated as the expected number
of counts at the pixels  given by Pn(f )(Pf ′∈{Fo} Vf ′ )/(Pf ′∈{Fo} Pn(f ′)) where Vf represents
the value of the image at the f th pixel and {Fo} is the set of observed pixels. Figure 4B shows the
reconstructed faces for the sparse-overcomplete case of 1000 basis vectors. Figure 7A summarizes
the results for all cases. Performance is measured by mean Signal-to-Noise-Ratio (SNR)  where
SNR for an image was computed as the ratio of the sum of squared pixel intensities of the original
image to the sum of squared error between the original image pixels and the reconstruction.

4.2 Handwritten Digit Classiﬁcation

In this experiment we evaluate the speciﬁcity of the bases to the process represented by the training
data set  through a simple example of handwritten digit classiﬁcation. We used the USPS Handwrit-
ten Digits database which has 1100 examples for each digit class. We randomly chose 100 examples
from each class and separated them as the test set. The remaining examples were used for training.
During training  separate sets of basis distributions P k(f |z) were learned for each class  where k
represents the index of the class. Figure 5 shows 25 bases images extracted for the digit “2”. To
classify any test image v  we attempted to compute the distribution underlying the image using the
bases for each class (by estimating the mixture weights P k
v (z)  keeping the bases ﬁxed  as before).
The “match” of the bases to the test instance was indicated by the likelihood Lk of the image com-
puted using P k(f ) = Pz P k(f |z)P k
v (z) as Lk = Pf vf log P k(f ). Since we expect the bases for
the true class of the image to best compose it  we expect the likelihood for the correct class to be
maximum. Hence  the image v was assigned to the class for which likelihood was the highest.

7

A. Reconstruction Experiment

 

1 patch
2 patches
3 patches
4 patches

24

22

20

18

16

14

12

10

 

R
N
S
n
a
e
M

8

 

50

200
750
Number of Basis Components

500

1000

r
o
r
r

 

E
e
g
a

t

n
e
c
r
e
P

5

4.5

4

3.5

3

2.5

 

2
0

B. Classification Experiment

 

25
50
75
100
200

0.05

0.1

Sparsity Parameter

0.2

0.3

Figure 7: (A). Results of the face Reconstruction experiment. Mean SNR of the reconstructions is
shown as a function of the number of basis vectors and the test case (number of deleted patches 
shown in the legend). Notice that the sparse-overcomplete codes consistently perform better than
the compact codes.
(B). Results of the classiﬁcation experiment. The legend shows number of
basis distributions used. Notice that imposing sparsity almost always leads to better classiﬁcation
performance. In the case of 100 bases  error rate comes down by almost 50% when a sparsity
parameter of 0.3 is imposed.

Results are shown in Figure 7B. As one can see  imposing sparsity improves classiﬁcation perfor-
mance in almost all cases. Figure 6 shows three sets of basis distributions learned for class “3” with
different sparsity values on the mixture weights. As the sparsity parameter is increased  bases tend
to be holistic representations of the input histograms. This is consistent with improved classiﬁcation
performance - as the representation of basis distributions gets more holistic  the more unlike they
become when compared to bases of other classes. Thus  there is a lesser chance that the bases of
one class can compose an image in another class  thereby improving performance.

5 Conclusions

In this paper  we have presented an algorithm for sparse extraction of overcomplete sets of latent
distributions from histogram data. We have used entropy as a measure of sparsity and employed
the entropic prior to manipulate the entropy of the estimated parameters. We showed that sparse-
overcomplete components can lead to an improved characterization of data and can be used in appli-
cations such as classiﬁcation and inference of missing data. We believe further improved characteri-
zation may be achieved by the imposition of additional priors that represent known or hypothesized
structure in the data  and will be the focus of future research.

References

[1] DM Blei and JD Lafferty. Correlated Topic Models. In NIPS  2006.
[2] DM Blei  AY Ng  and MI Jordan. Latent Dirichlet Allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[3] ME Brand. Pattern Discovery via Entropy Minimization. In Uncertainty 99: AISTATS 99  1999.
[4] RM Corless  GH Gonnet  DEG Hare  DJ Jeffrey  and DE Knuth. On the Lambert W Function.

Advances in Computational mathematics  1996.

[5] DJ Field. What is the Goal of Sensory Coding? Neural Computation  1994.
[6] T Hofmann. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learn-

ing  42:177–196  2001.

[7] PO Hoyer. Non-negative Matrix Factorization with Sparseness Constraints. Journal of Machine

Learning Research  5  2004.

[8] DD Lee and HS Seung. Algorithms for Non-negative Matrix Factorization. In NIPS  2001.
[9] J Skilling. Classic Maximum Entropy. In J Skilling  editor  Maximum Entropy and Bayesian

Methods. Kluwer Academic  1989.

8

,Jonas Mueller
Tommi Jaakkola
Bo Li
Yining Wang
Aarti Singh
Yevgeniy Vorobeychik