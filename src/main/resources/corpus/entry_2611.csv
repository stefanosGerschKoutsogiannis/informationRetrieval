2014,Incremental Local Gaussian Regression,Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions  is computationally efficient  and can learn continually from very large amounts of incrementally collected data. As an interesting feature  LWR can regress on non-stationary functions  a beneficial property  for instance  in control problems. However  it does not provide a proper generative model for function values  and existing algorithms have a variety of manual tuning parameters that strongly influence bias  variance and learning speed of the results. Gaussian (process) regression  on the other hand  does provide a generative model with rather black-box automatic parameter tuning  but it has higher computational cost  especially for big data sets and if a non-stationary model is required. In this paper  we suggest a path from Gaussian (process) regression to locally weighted regression  where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques  we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale  and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.,Incremental Local Gaussian Regression

Franziska Meier1
fmeier@usc.edu

Philipp Hennig2

phennig@tue.mpg.de

Stefan Schaal1 2

sschaal@usc.edu

1University of Southern California

Los Angeles  CA 90089  USA

2Max Planck Institute for Intelligent Systems

Spemannstra√üe 38  T¬®ubingen  Germany

Abstract

Locally weighted regression (LWR) was created as a nonparametric method that
can approximate a wide range of functions  is computationally efÔ¨Åcient  and can
learn continually from very large amounts of incrementally collected data. As
an interesting feature  LWR can regress on non-stationary functions  a beneÔ¨Åcial
property  for instance  in control problems. However  it does not provide a proper
generative model for function values  and existing algorithms have a variety of
manual tuning parameters that strongly inÔ¨Çuence bias  variance and learning speed
of the results. Gaussian (process) regression  on the other hand  does provide
a generative model with rather black-box automatic parameter tuning  but it has
higher computational cost  especially for big data sets and if a non-stationary model
is required. In this paper  we suggest a path from Gaussian (process) regression to
locally weighted regression  where we retain the best of both approaches. Using
a localizing function basis and approximate inference techniques  we build a
Gaussian (process) regression algorithm of increasingly local nature and similar
computational complexity to LWR. Empirical evaluations are performed on several
synthetic and real robot datasets of increasing complexity and (big) data scale  and
demonstrate that we consistently achieve on par or superior performance compared
to current state-of-the-art methods while retaining a principled approach to fast
incremental regression with minimal manual tuning parameters.

1

Introduction

Besides accuracy and sample efÔ¨Åciency  computational cost is a crucial design criterion for machine
learning algorithms in real-time settings  such as control problems. An example is the modeling of
robot dynamics: The sensors in a robot can produce thousands of data points per second  quickly
amassing a coverage of the task related workspace  but what really matters is that the learning
algorithm incorporates this data in real time  as a physical system can not necessarily stop and
wait in its control ‚Äì e.g.  a biped would simply fall over. Thus  a learning method in such settings
should produce a good local model in fractions of a second  and be able to extend this model as the
robot explores new areas of a very high dimensional workspace that can often not be anticipated
by collecting ‚Äúrepresentative‚Äù training data. Ideally  it should rapidly produce a good (local) model
from a large number N of data points by adjusting a small number M of parameters. In robotics 
local learning approaches such as locally weighted regression [1] have thus been favored over global
approaches such as Gaussian process regression [2] in the past.

Local regression models approximate the function in the neighborhood of a query point x‚àó. Each

local model‚Äôs region of validity is deÔ¨Åned by a kernel. Learning the shape of that kernel [3] is the
key component of locally weighted learning. Schaal & Atkeson [4] introduced a non-memory-based
version of LWR to compress large amounts of data into a small number of parameters. Instead
of keeping data in memory and constructing local models around query points on demand  their

1

algorithm incrementally compresses data into M local models  where M grows automatically to
cover the experienced input space of the data. Each local model can have its own distance metric 
allowing local adaptation to local characteristics like curvature or noise. Furthermore  each local
model is trained independently  yielding a highly efÔ¨Åcient parallelizable algorithm. Both its local

adaptiveness and its low computation cost (linear O(N M)) has made LWR feasible and successful

in control learning. The downside is that LWR requires several tuning parameters  whose optimal
values can be highly data dependent. This is at least partly a result of the strongly localized training 
which does not allow models to ‚Äòcoordinate‚Äô  or to beneÔ¨Åt from other local models in their vicinity.
Gaussian process regression (GPR) [2]  on the other hand  offers principled inference for hyperpa-
rameters  but at high computational cost. Recent progress in sparsifying Gaussian processes [5  6]
has resulted in computationally efÔ¨Åcient variants of GPR . SparsiÔ¨Åcation is achieved either through a
subset selection of support points [7  8] or through sparsiÔ¨Åcation of the spectrum of the GP [9  10].
Online versions of such sparse GPs [11  12  13] have produced a viable alternative for real-time
model learning problems [14]. However  these sparse approaches typically learn one global distance
metric  making it difÔ¨Åcult to Ô¨Åt the non-stationary data encountered in robotics. Moreover  restricting
the resources in a GP also restricts the function space that can be covered  such that with the need to
cover a growing workspace  the accuracy of learning will naturally diminish.
Here we develop a probabilistic alternative to LWR that  like GPR  has a global generative model  but
is locally adaptive and retains LWRs fast incremental training. We start in the batch setting  where
rethinking LWRs localization strategy results in a loss function coupling local models that can be
modeled within the Gaussian regression framework (Section 2). Modifying and approximating the
global model  we arrive at a localized batch learning procedure (Section 3)  which we term Local
Gaussian Regression (LGR). Finally  we develop an incremental version of LGR that processes
streaming data (Section 4). Previous probabilistic formulations of local regression [15  16  17] are
bottom-up constructions‚Äîgenerative models for one local model at a time. Ours is a top-down
approach  approximating a global model to give a localized regression algorithm similar to LWR.

2 Background

L(wm).

Locally weighted regression (LWR) with a Ô¨Åxed set of M local models minimizes the loss function

used  too  but linear models have a favorable bias-variance trade-off [18]. The models are localized

L(w)= NQ
MQ
m=1
n=1
fm(x)= KQ
k=1

Œ∑m(xn)(yn‚àí Œæm(xn)T wm)2= MQ
m=1
Œæmk(x)wmk= Œæm(x)wm

The right hand side decomposesL(w) into independent losses for M models. We assume each
model has K local feature functions Œæmk(x)  so that the m-th model‚Äôs prediction at x is
K= 2  Œæm1(x)= 1  Œæm2(x)=(x‚àí cm) gives a linear model around cm. Higher polynomials can be
by a non-negative  symmetric and integrable weighting Œ∑m(x)  typically the radial basis function
for x‚àà RD  with center cm and length scale Œªm or positive deÔ¨Ånite metric Œõm. Œ∑m(xn) localizes the
The prediction y‚àó at a test point x‚àó is a normalized weighted average of the local predictions y‚àó m:
LWR effectively trains M linear models on M separate datasets ym(xn)=
Œ∑m(xn)yn. These

Œ∑m(x)= exp‚àí(x‚àí cm)2

y‚àó=‚àëM

m=1 Œ∑m(x‚àó)fm(x‚àó)
m=1 Œ∑m(x‚àó)
‚àëM

Œ∑m(x)= exp‚àí 1

(x‚àí cm)Œõ‚àí1

m(x‚àí cm)

effect of errors on the least-squares estimate of wm‚Äîdata points far away from cm have little effect.

(1)

(2)

(3)

(4)

  

or

2Œª2
m

2

models differ from the one of Eq. (4)  used at test time. This smoothes discontinuous transitions
between models  but also means that LWR can not be cast probabilistically as one generative model
for training and test data simultaneously. (This holds for any bottom-up construction that learns local

2

Œ≤y

w

Œ∑n
m

Œæm

œÜn
m

yn

N

M

Œ≤y

Œ≤fm

Œ∑n
m

Œæn
m

wm

f n
m

yn

N

M

m= œÜm(xn)= Œ∑n

Figure 1: Left: Bayesian linear regression with M feature functions œÜn
be a function localizing the effect of the mth input function Œæn
variables f n
create M local models connected only through the latent f n
m.

m can
m towards the prediction of yn. Right: Latent
m placed between the features and yn decouple the M regression parameters wm and effectively

m  where Œ∑n

mŒæn

training point y  similar to how LWR operates during test time (Eq.4). Thus  already during training 

models independently and combines them as above  e.g.  [15  16]). The independence of local models
is key to LWR‚Äôs training: changing one local model does not affect the others. While this lowers cost 
we believe it is also partially responsible for LWR‚Äôs sensitivity to manually tuned parameters.
Here  we investigate a different strategy to achieve localization  aiming to retain the computational
complexity of LWR  while adding a sense of globality. Instead of using Œ∑m to localize the training

error of data points  we localize a model‚Äôs contribution ÀÜym= Œæ(x)T wm towards the global Ô¨Åt of
local models must collaborate to Ô¨Åt a data point ÀÜy=‚àëm=1 Œ∑m(x)Œæ(x)T wm. Our loss function is
combining the localizer Œ∑m(xn) and the mth input function Œæm(xn) to form the feature œÜm(xn)=
Œ∑m(xn)Œæm(xn). This form of localization couples all local models  as in classical radial basis

Œ∑m(xn)Œæm(xn)T wm2= NQ
n=1

œÜm(xn)T wm2

L(w)= NQ
n=1

yn‚àí MQ
m=1

yn‚àí MQ
m=1

(5)

 

function networks [19]. At test time  all local predictions form a joined prediction

y‚àó= MQ
m=1

y‚àóm= MQ
m=1

œÜm(x‚àó)T wm

(6)

(7)
(8)

This loss can be minimized through a regularized least-square estimator for w (the concatenation of all
wm). We follow the probabilistic interpretation of least-squares estimation as inference on the weights

w  from a Gaussian prior p(w)=N(w; ¬µ0  Œ£0) and likelihood p(y œÜ  w)=N(y; œÜw  Œ≤‚àí1
y I).

The probabilistic formulation has additional value as a generative model for all (training and test)
data points y  which can be used to learn hyperparameters (Figure 1  left). The posterior is

p(w y  œÜ)=N(w; ¬µN   Œ£N) with

0 + Œ≤yŒ¶Œ¶)‚àí1(Œ≤yŒ¶y+ Œ£‚àí1
0 ¬µ0)

and

¬µN=(Œ£‚àí1

this framework can be extended nonparametrically by a limit that replaces all inner products

(Heteroscedastic data will be addressed below). The prediction for f(x‚àó) with features œÜ(x‚àó)=‚à∂ œÜ‚àó
is also Gaussian  with p(f(x‚àó) y  œÜ) = N(f(x‚àó); œÜ‚àó¬µN   œÜ‚àóŒ£N œÜ‚àó). As is widely known 
œÜ(xi)Œ£0œÜ(xj) with a Mercer (positive semi-deÔ¨Ånite) kernel k(xi  xj)  corresponding to a Gaus-
0 + Œ≤yŒ¶Œ¶). In general  this requiresO(F 3) operations.
jointly  by inverting the Gram matrix(Œ£‚àí1

sian process prior. The direct connection between Gaussian regression and the elegant theory of
Gaussian processes is a conceptual strength. The main downside  relative to LWR  is computational
cost: Calculating the posterior (7) requires solving the least-squares problem for all F parameters w

Below we propose approximations to lower the computational cost of this operation to a level compa-
rable to LWR  while retaining the probabilistic interpretation  and the modeling robustness of the full
Gaussian model.

Œ£N=(Œ£‚àí1

0 + Œ≤yŒ¶Œ¶)‚àí1

3 Local Parametric Gaussian Regression

The above shows that Gaussian regression with features œÜm(x)= Œ∑m(x)Œæm(x) can be interpreted
as global regression with M models  where Œ∑m(xn) localizes the contribution of the model Œæm(x)

towards the joint prediction of yn. The choice of local parametric model Œæm is essentially free. Local

3

correspond to Gaussian regression with RBF features. Generalizing to M local models with K
parameters each  feature function œÜn

linear regression in a K-dimensional input space takes the form Œæm(xn)= xn‚àí cm  and can be
viewed as the analog of locally weighted linear regression. Locally constant models Œæm(x)= 1
mk combines the kth component of the local model Œækm(xn) 
localized by the m-th weighting function Œ∑m(xn)
mk‚à∂= œÜmk(xn)= Œ∑m(xn)Œækm(xn).
Treating mk as indices of a vector‚àà RM K  Equation (7) gives localized linear Gaussian regression.
N(wm; 0  A‚àí1

Since it will become necessary to prune the model  we adopt the classic idea of automatic relevance
determination [20  21] using a factorizing prior

m) with Am= diag(Œ±m1  . . .   Œ±mK).

Thus every component k of local model m has its own precision  and can be pruned out by setting

Œ±mk(cid:95)‚àû. Section 3.1 assumes a Ô¨Åxed number M of local models with Ô¨Åxed centers cm. The
parameters are Œ∏={Œ≤y {Œ±mk} {Œªmd}}  where K is the dimension of local model Œæ(x) and D is

p(wA)= MM
m=1

the dimension of input x. We propose an approximation for estimating Œ∏. Section 4 then describes
an incremental algorithm allocating local models as needed  adapting M and cm.

(10)

(9)

œÜn

3.1 Learning in Local Gaussian Regression

Exact Gaussian regression with localized features still has cubic cost. However  because of the
localization  correlation between distant local models approximately vanishes  and inference is
approximately independent between local models. To use this near-independence for cheap local
approximate inference  similar to LWR  we introduce a latent variable f n
m for each local model m
and datum xn  as in probabilistic backÔ¨Åtting [22]. Intuitively  the f form approximate local targets 
against which the local parameters Ô¨Åt (Figure 1  right). Moreover  as formalized below  each f n
m has
its own variance parameter  which re-introduces the ability to model hetereoscedastic data.
This modiÔ¨Åed model motivates a factorizing variational bound (Section 3.1.1). Rendering the local
models computationally independent  it allows for fast approximate inference in the local Gaussian
model. Hyperparameters can be learned by approximate maximum likelihood (Section 3.1.2) 

i.e. iterating between constructing a bound q(z Œ∏) on the posterior over hidden variables z (deÔ¨Åned

below) given current parameter estimates Œ∏ and optimizing q with respect to Œ∏.

3.1.1 Variational Bound

m; œÜn

The complete data likelihood of the modiÔ¨Åed model (Figure 1  right) is

N(yn; f n  Œ≤‚àí1
y ) NM
n=1

p(y  f   w Œ¶  Œ∏)= NM
n=1
Our Gaussian model involves the latent variables w and f  the precisions Œ≤={Œ≤y  Œ≤f 1  . . .   Œ≤f M} and
the model parameters Œªm  cm. We treat w and f as probabilistic variables and estimate Œ∏={Œ≤  Œª  c}.
On w  f  we construct a variational bound q(w  f) imposing factorization q(w  f)= q(w)q(f).

N(wm; 0  A‚àí1
m)

f m) MM
m=1

mwm  Œ≤‚àí1

N(f n

MM
m=1

is maximized

The variational free energy is a lower bound on the log evidence for the observations y:

log p(y Œ∏)‚â•S q(w  f) log

distributions are Gaussian in both w and f.The approximation on w is

DKL[q(w  f)p(w  f y  Œ∏)]  the distribution for which log q(w) = Ef[log p(y f   w)p(w  f)]
and log q(f) = Ew[log p(y f   w)p(w  f)]. It is relatively easy to show (e.g. [23]) that these
log q(w)= Ef NQ
n=1
Œ£wm=Œ≤f m

log p(f n œÜn  w)+ log p(w A)= log
T+ Am‚àí1 ‚àà RK√óK and ¬µwm

N(wm; ¬µwm  Œ£wm)
E[f n
m]‚àà RK√ó1

p(y  w  f Œ∏)
q(w  f)
q(w  f) minimizing
MM
m=1
= Œ≤f mŒ£wm NQ
n=1

NQ
n=1

entropy

relative

bound

œÜn

mœÜn

m

by

the

.

the

where

This

(11)

(12)

(13)

(14)

œÜn
m

4

where

¬µf n

m

f m

f m

(15)

(16)

(17)

likelihood under the variational bound

3.1.2 Optimizing Hyperparameters

yn‚àí MQ
m=1

m is the posterior mean of the m-th model‚Äôs virtual target for data

f couples the local models  allowing for a form of message passing between local models.

log q(f n)= Ew[log p(yn f n  Œ≤y)+ log p(f n œÜn

The posterior update equations for the weights are local: each of the local models updates its
parameters independently. This comes at the cost of having to update the belief over the variables f n
m 
which achieves a coupling between the local models. The Gaussian variational bound on f is

m  w)]= logN(f n; ¬µf n  Œ£f) 
Œ£f= B‚àí1‚àí B‚àí11(Œ≤‚àí1
y + 1T B‚àí11)‚àí11T B‚àí1= B‚àí1‚àí B‚àí111T B‚àí1
y + 1T B‚àí11
Œ≤‚àí1
Ew[wm]T œÜn
m
= Ew[wm]T œÜn
m+
Œ≤‚àí1
y +‚àëM
m=1 Œ≤‚àí1
Œ≤‚àí1
and B= diag(Œ≤f 1  . . .   Œ≤f M). ¬µf n
point n. These updates can be performed inO(M K). Note how the posterior over hidden variables
m=1 {Œ±mk}}  we maximize the expected complete log
To set the parameters Œ∏={Œ≤y {Œ≤f m  Œªm}M
y 
 logNyn;
Ef  w[log p(y  f   w Œ¶  Œ∏)]= Ef  w NQ
MQ
m  Œ≤‚àí1
m=1
n=1
logN(f n
f m)+ MQ
+ MQ
m  Œ≤‚àí1
m=1
m=1
(yn‚àí 1¬µf n)2+ 1T Œ£f 1
NQ
n=1
m)2+ œÜn
(¬µf nm‚àí ¬µwm œÜn
NQ
n=1
+ Œ£w kk
= ‚àÇEf  w‚àëN
n=1 logN(f n
‚àÇEf  w[log p(y  f   w Œ¶  Œ∏)]

Setting the gradient of this expression to zero leads to the following update equations for the variances

The gradient with respect to the scales of each local model is completely localized

logN(wm; 0  A‚àí1
m).

y = 1
Œ≤‚àí1
f m= 1
Œ≤‚àí1
mk= ¬µ2
Œ±‚àí1

‚àÇŒªmd

‚àÇŒªmd

and  with the exception of the variance 1~Œ≤y  all hyper-parameter updates are solved independently

We use gradient ascent to optimize the length scales Œªmd. All necessary equations are of low cost

for each local model  similar to LWR. In contrast to LWR  however  these local updates do not
cause a potential catastrophic shrinking in the length scales: In LWR  both inputs and outputs are
weighted by the localizing function  thus reducing the length scale improves the Ô¨Åt. The localization
in Equation (22) only affects the inÔ¨Çuence of regression model m  but the targets still need to be
Ô¨Åt accordingly. Shrinking of local models only happens if it actually improves the Ô¨Åt against the
unweighted targets fnm such that no complex cross validation procedures are required.

f m)(cid:6)
m  Œ≤‚àí1

m+ œÉ2

T Œ£wmœÜn

m

m; wT

mœÜn

m; wT

mœÜn

(19)

(20)

(21)

N

N

(18)

(22)

f n

wmk

f m

3.1.3 Prediction

Predictions at a test point x‚àó arise from marginalizing over both f and w  using
S S N(y‚àó; 1T f‚àó  Œ≤‚àí1
y )N(f‚àó; W T œÜ(x‚àó)  B‚àí1)df‚àóN(w; ¬µw  Œ£w)dw
=Ny‚àó;Q
m  œÉ2(x‚àó) (23)
mœÜ‚àó
f m+‚àëM
y +‚àëM
where œÉ2(x‚àó)= Œ≤‚àí1
m=1 œÜ‚àó
m=1 Œ≤‚àí1

m  which is linear in M and K.

T Œ£wm œÜ‚àó

wT

m

m

5

4

Incremental Local Gaussian Regression

0) and p(Œ±m)=‚àèK

the n-th incoming data point. Following this principle we extend the model presented in Section 3

The above approximate posterior updates apply in the batch setting  assuming the number M and
locations c of local models are Ô¨Åxed. This section constructs an online algorithm for incrementally
incoming data  creating new local models when needed. There has been recent interest in variational
online algorithms for efÔ¨Åcient learning on large data sets [24  25]. Stochastic variational inference
[24] operates under the assumption that the data set has a Ô¨Åxed size N and optimizes the variational
lower bound for N data points via stochastic gradient descent. Here  we follow algorithms for
streaming datasets of unknown size. Probabilistic methods in this setting typically follow a Bayesian

Ô¨Åltering approach [26  25  27] in which the posterior after n‚àí 1 data points becomes the prior for
and treat precision variables{Œ≤f m  Œ±mk} as random variables  assuming Gamma priors p(Œ≤f m)=
G(Œ≤f m aŒ≤
0). Thus  the factorized approximation on the
posterior q(z) over all random variables z={f   w  Œ±  Œ≤f} is changed to
q(z)= q(f   w  Œ≤f   Œ±)= q(f)q(w)q(Œ≤f)q(Œ±)
p(zx1  . . .   xn)‚âà p(xn z)q(z x1  . . . xn‚àí1)
f m and‚àën=1(¬µn

(25)
after n data points.
In essence  this formulates the (approximate) posterior updates in terms
of sufÔ¨Åcient statistics  which are updated with each new incoming data point. The batch up-
dates (listed in [28]) can be rewritten such that they depend on the following sufÔ¨Åcient statistics

(24)
A batch version of this was introduced in [28]. Given that  the recursive application of Bayes‚Äô theorem
results in the approximate posterior

k=1G(Œ±mk aŒ±

 ‚àën=1 œÜn

mœÜn

0   bŒ≤

0   bŒ±

m¬µn



m

are added. Algorithm 1 gives an overview of the entire incremental algorithm.

number M can grow fast initially  before the pruning becomes effective. Thus  we check for each

Finally  we use an extension analogous to incremental training of the relevance vector machine [29] to

each iteration adds one local model in the variational step  and prunes out existing local models

random variables too  here we update them using the noisy (stochastic) gradients produced by each
incoming data point. Due to space limitations  we only summarize these update equations in the

f m)2. Although the length-scales Œªm could be treated as

‚àëN
n=1 œÜn
algorithm below  where we have replaced the expectation operator by‚ãÖ.
iteratively add local models at new  greedily selected locations cM+1. Starting with one local model 
for which all components Œ±mk(cid:95)‚àû. This works well in practice  with the caveat that the model
Œ∑m(cM+1)‚â• wgen  where wgen is a parameter between 0 and 1 and regulates how many parameters
selected location cM+1 whether any of the existing local models c1‚à∂M produces a localizing weight
1: M= 0; C={}  aŒ±
2: for all(xn  yn) do
Algorithm 1 Incremental LGR
~~ for each data point
if Œ∑m(xn)< wgen ‚àÄm= 1  . . .   M then cm(cid:94) xn; C(cid:94) C‚à™{cm}; M= M+ 1 end if
0   aŒ≤
0   Œ≤Œ≤
0   bŒ±
m
yn‚àí‚àëM
Œ£f= B‚àí1‚àí B
= ¬µT
‚àí1
‚àí111T B
‚àí1
m=1 ¬µT
for m= 1 to M do
y +‚àëmŒ≤fm
m=1Œ≤‚àí1
y +‚àëM
Œ≤‚àí1
Œ≤‚àí1
if Œ∑m(xn)< 0.01 then continue end if
+ œÜn
+ œÜn
+ ¬µ2

m(cid:94) Œ∫SœÜmœÜ
  SœÜm¬µfm(cid:94) Œ∫SœÜm¬µfm
fm(cid:94) Œ∫S¬µ2
+Am‚àí1
Œ£wm=Œ≤f mSœÜmœÜT
  ¬µwm=Œ≤f mŒ£wm SœÜm¬µfm
Nm= Œ∫Nm+ 1  aŒ≤
0+ Nm  aŒ±
N m= aŒ≤
0+ 0.5
N m= aŒ±
+ trSœÜmœÜ
N m= S¬µ2
‚àí 2¬µ
(Œ£wm+ ¬µwm¬µ
wm k+ Œ£wm kk
N mk= ¬µ2
Nmk)
Nmk~bŒ±
Nm Am= diag(aŒ±
Œ≤f m= aŒ≤
Nm~bŒ≤
Œªm= Œªm+ ŒΩ(‚àÇ~‚àÇŒªmN(f nm;wT
m Œ≤‚àí1
f m))
ifŒ±mk> 1e3 ‚àÄk= 1  . . .   K then prune local model m  M(cid:94) M‚àí 1 end if

0   forgetting rate Œ∫  learning rate ŒΩ

)(cid:6)+ NmœÉ2

8:
9:
10:

SœÜm¬µfm

SœÜmœÜT

4:
5:
6:
7:

mœÜn

m¬µf n

m

  ¬µf n

m

mœÜn

œÜn
m

wm

œÜn

wm

fm

f n
m

m

m

f n
m

wm

wm

f m

bŒ≤
bŒ±

Œ≤

fm

fm

3:

m

  S¬µ2

11:
12:
13:
14:
15:
16: end for

end for

m

6

Table 1: Datasets for inverse dynamics tasks: KUKA1  KUKA2 are different splits of the same data. Rightmost
column indicates the overlap in input space coverage between ofÔ¨Çine (ISofÔ¨Çine) and online training (ISonline) sets.

ISofÔ¨Çine‚à™ ISonline

NofÔ¨Çine train Nonline train

Ntest

4449
17560
17560

-

44484
180360
180360
1984950

-
-
-

large overlap
small overlap
no overlap

20050

-

Dataset
Sarcos [2]
KUKA1
KUKA2
KUKAsim

freq
100
500
500
500

Motion
rhythmic

rhythmic at various speeds
rhythmic at various speeds

rhythmic + discrete

5 Experiments

We evaluate our LGR on inverse dynamics learning tasks  using data from two robotic platforms:
a SARCOS anthropomorphic arm and a KUKA lightweight arm. For both robots  learning the

inverse dynamics involves learning a map from the joint positions q(rad)  velocities Àôq(rad~s) and
accelerations ¬®q(rad~s2)  to torques œÑ(Nm) for each of 7 joints (degrees of freedom). We compare to

two methods previously used for inverse dynamics learning: LWPR1 ‚Äì an extension of LWR for high
dimensional spaces [31] ‚Äì and I-SSGPR2 [13] ‚Äì an incremental version of Sparse Spectrum GPR.
I-SSGPR differs from LGR and LWPR in that it is a global method and does not learn the distance
metric online. Instead  I-SSGPR needs ofÔ¨Çine training of hyperparameters before it can be used
online. We mimic the procedure used in [13]: An ofÔ¨Çine training set is used to learn an initial model
and hyperparameters  then an online training set is used to evaluate incremental learning. Where
indicated we use initial ofÔ¨Çine training for all three methods. I-SSGPR uses typical GPR optimization
procedures for ofÔ¨Çine training  and is thus only available in batch mode. For LGR  we use the batch
version for pre-training/hyperparameter learning. For all experiments we initialized the length scales

to Œª= 0.3  and used wgen= 0.3 for both LWPR and LGR.

We evaluate on four different data sets  listed in Table 1. These sets vary in scale  types of motion
and how well the ofÔ¨Çine training set represents the data encountered during online learning. All
results were averaged over 5 randomly seeded runs  mean-squared error (MSE) and normalized
mean-squared error (nMSE) are reported on the online training dataset. The nMSE is reported as the
mean-squared error normalized by the variance of the outputs.

with 200(400) features  MSE for 400 features is reported in brackets.

Table 2: Predictive performance on online training data of Sarcos after one sweep. I-SSGPR has been trained

I-SSGPR200(400)

Joint
J1
J2
J3
J4
J5
J6
J7

MSE

13.699 (10.832)
6.158 (4.788)
1.803 (1.415)
1.198 (0.857)
0.034 (0.027)
0.129 (0.096)
0.093 (0.063)

nMSE
0.033
0.027
0.018
0.006
0.036
0.044
0.014

MSE
19.180
9.783
3.595
4.807
0.071
0.248
0.231

LWPR
nMSE # of LM
0.046
0.044
0.036
0.025
0.075
0.085
0.034

461.4
495.0
464.6
382.8
431.2
510.2
378.8

LGR

nMSE # of LM
0.027
0.037
0.023
0.027
0.033
0.034
0.025

321.4
287.4
298.0
303.2
344.2
344.2
348.8

MSE
11.434
8.342
2.237
5.079
0.031
0.101
0.170

Sarcos: Table 2 summarizes results on the popular Sarcos benchmark for inverse dynamics learning
tasks [2]. The traditional test set is used as the ofÔ¨Çine training data to pre-train all three models.

200 features is the optimal design choice according to[13]. We report the (normalized) mean-squared
I-SSGPR is trained with 200 and 400 sparse spectrum features  indicated as I-SSGPR200(400)  where

error on the online training data  after one sweep through it - i.e. each data point has been used once -
has been performed. All three methods perform well on this data  with I-SSGPR and LGR having a
slight edge over LWPR in terms of accuracy; and LGR uses fewer local models than LWPR. The
Sarcos data ofÔ¨Çine training set represents the data encountered during online training very well. Thus 
here online distance metric learning is not necessary to achieve good performance.

1we use the LWPR implementation found in the SL simulation software package [30]
2we use code from the learningMachine library in the RobotCub framework  from http:// eris.liralab.it/iCub

7

Table 3: Predictive performance on online training data of KUKA1 and KUKA2 after one sweep. KUKA2
results are averages across joints. I-SSGPR was trained on 200 and 400 features (results for I-SSGPR400 shown
in brackets).

I-SSGPR200(400)

LWPR

LGR

data

KUKA1

Joint
J1
J2
J3
J4
J5
J6
J7

MSE

7.021 (7.680)
16.385 (18.492)
1.872 (1.824)
3.124 (3.460)
0.095 (0.143)
0.142 (0.296)
0.129 (0.198)

nMSE MSE nMSE # of LM MSE nMSE # of LM
3188.6
0.233
3363.8
0.265
3246.6
0.289
0.256
3333.6
3184.4
0.196
3372.4
0.139
0.174
3232.6

3476.8
3508.6
3477.2
3494.6
3512.4
3561.0
3625.6

2.238
2.738
0.528
0.571
0.017
0.029
0.033

2.362
2.359
0.457
0.503
0.019
0.043
0.023

0.078
0.038
0.071
0.041
0.039
0.042
0.031

0.074
0.044
0.082
0.047
0.036
0.029
0.044

KUKA2

-

9.740 (9.985)

0.507

1.064

0.056

3617.7

1.012

0.054

3290.2

0.06

0.04

0.02

E
S
M
n

LGR
LWPR

5‚ãÖ 105

1‚ãÖ 106

1.5‚ãÖ 106

17 000

16 000

M

15 000

14 000

5‚ãÖ 105

1‚ãÖ 106

1.5‚ãÖ 106

n

n

Figure 2: Right: nMSE on the Ô¨Årst joint of simulated KUKA arm Left: average number of local models used.
KUKA1 and KUKA2: The two KUKA datasets consist of rhythmic motions at various speeds  and
represent a more realistic setting in robotics: While one can collect some data for ofÔ¨Çine training  it is
not feasible to cover the whole state-space. OfÔ¨Çine data of KUKA1 has been chosen to give partial
coverage of the range of available speeds  while KUKA2 consists of motion at only one speed. In this
setting  both LWPR and LGR excel (Table 3). As they can learn local distance metrics on the Ô¨Çy  they
adapt to incoming data in previously unexplored input areas. Performance of I-SSGPR200 degrades
as the ofÔ¨Çine training data is less representative  while LGR and LWPR perform almost equally well
on KUKA1 and KUKA2. While there is little difference in accuracy between LGR and LWPR  LGR
consistently uses fewer local models and does not require careful manual meta-parameter tuning.
Since both LGR and LWPR use more local models on this data (compared to the Sarcos data) we
also tried increasing the feature space of I-SSGPR to 400 features. This did not improve I-SSGPRs
performance on the online data (see Table 3). Finally  it is noteworthy that LGR processes both of

these data sets at‚àº 500Hz (C++ code  on a 3.4GHz Intel Core i7)  making it a realistic alternative for

real-time inverse dynamics learning tasks.

KUKAsim : Finally  we evaluate LGRs ability to learn from scratch on KUKAsim  a large data set
of 2 million simulated data points  collected using [30]. We randomly drew 1% points as a test
set  on which we evaluate convergence during online training. Figure 2 (left) shows convergence
and number of local models used  averaged over 5 randomly seeded runs for joint 1. After the Ô¨Årst
1e5 data points  both LWPR and LGR achieve a normalized mean squared error below 0.07  and

eventually converge to a nMSE of‚àº 0.01. LGR converges slightly faster  while using fewer local

models (Figure 2  right).

6 Conclusion

We proposed a top-down approach to probabilistic localized regression. Local Gaussian Regression
decouples inference over M local models  resulting in efÔ¨Åcient and principled updates for all
parameters  including local distance metrics. These localized updates can be used in batch as well as
incrementally  yielding computationally efÔ¨Åcient learning in either case and applicability to big data
sets. Evaluated on a variety of simulated and real robotic inverse dynamics tasks  and compared to
I-SSGPR and LWPR  incremental LGR shows an ability to add resources (local models) and to update
its distance metrics online. This is essential to consistently achieve high accuracy. Compared to
LWPR  LGR matches or improves precision  while consistently using fewer resources (local models)
and having signiÔ¨Åcantly fewer manual tuning parameters.

8

References
[1] Christopher G Atkeson  Andrew W Moore  and Stefan Schaal. Locally weighted learning for control.

ArtiÔ¨Åcial Intelligence Review  (1-5):75‚Äì113  1997.

[2] Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning. MIT

Press  2006.

[3] Jianqing Fan and Irene Gijbels. Data-driven bandwidth selection in local polynomial Ô¨Åtting: variable

bandwidth and spatial adaptation. Journal of the Royal Statistical Society.  pages 371‚Äì394  1995.

[4] Stefan Schaal and Christopher G Atkeson. Constructive incremental learning from only local information.

Neural Computation  10(8):2047‚Äì2084  1998.

[5] Joaquin QuiÀúnonero Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian

process regression. JMLR  6:1939‚Äì1959  2005.

[6] Krzysztof Chalupka  Christopher KI Williams  and Iain Murray. A framework for evaluating approximation

methods for Gaussian process regression. JMLR  14(1):333‚Äì350  2013.

[7] Michalis K Titsias. Variational learning of inducing variables in sparse Gaussian processes. In International

Conference on ArtiÔ¨Åcial Intelligence and Statistics  pages 567‚Äì574  2009.

[8] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. Advances in

neural information processing systems  18:1257  2006.

[9] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In NIPS  2007.
[10] Miguel L¬¥azaro-Gredilla  Joaquin QuiÀúnonero-Candela  Carl Edward Rasmussen  and An¬¥ƒ±bal R Figueiras-

Vidal. Sparse spectrum Gaussian process regression. JMLR  11:1865‚Äì1881  2010.

[11] Marco F Huber. Recursive Gaussian process: On-line regression and learning. Pattern Recognition Letters 

45:85‚Äì91  2014.

[12] Lehel Csat¬¥o and Manfred Opper. Sparse on-line Gaussian processes. Neural computation  2002.
[13] Arjan Gijsberts and Giorgio Metta. Real-time model learning using incremental sparse spectrum Gaussian

process regression. Neural Networks  41:59‚Äì69  2013.

[14] James Hensman  Nicolo Fusi  and Neil D Lawrence. Gaussian processes for big data. UAI  2013.
[15] Jo-Anne Ting  Mrinal Kalakrishnan  Sethu Vijayakumar  and Stefan Schaal. Bayesian kernel shaping for

learning control. Advances in neural information processing systems  6:7  2008.

[16] Duy Nguyen-Tuong  Jan R Peters  and Matthias Seeger. Local Gaussian process regression for real time
online model learning. In Advances in Neural Information Processing Systems  pages 1193‚Äì1200  2008.
[17] Edward Snelson and Zoubin Ghahramani. Local and global sparse Gaussian process approximations. In

International Conference on ArtiÔ¨Åcial Intelligence and Statistics  pages 524‚Äì531  2007.

[18] Trevor Hastie and Clive Loader. Local regression: Automatic kernel carpentry. Statistical Science  1993.
[19] J. Moody and C. Darken. Learning with localized receptive Ô¨Åelds. In Proceedings of the 1988 Connectionist

Summer School  pages 133‚Äì143. San Mateo  CA  1988.

[20] Radford M Neal. Bayesian Learning for Neural Network  volume 118. Springer  1996.
[21] Michael E Tipping. Sparse Bayesian learning and the relevance vector machine. The Journal of Machine

Learning Research  1:211‚Äì244  2001.

[22] Aaron D‚ÄôSouza  Sethu Vijayakumar  and Stefan Schaal. The Bayesian backÔ¨Åtting relevance vector machine.

In ICML  2004.

[23] Martin J Wainwright and Michael I Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends¬Æ in Machine Learning  2008.

[24] Matthew D. Hoffman  David M. Blei  Chong Wang  and John Paisley. Stochastic variational inference. J.

Mach. Learn. Res.  14(1):1303‚Äì1347  May 2013.

[25] Tamara Broderick  Nicholas Boyd  Andre Wibisono  Ashia C Wilson  and Michael Jordan. Streaming

variational Bayes. In Advances in Neural Information Processing Systems  pages 1727‚Äì1735  2013.

[26] Jan Luts  Tamara Broderick  and Matt Wand. Real-time semiparametric regression. arxiv  2013.
[27] Antti Honkela and Harri Valpola. On-line variational Bayesian learning. In 4th International Symposium

on Independent Component Analysis and Blind Signal Separation  pages 803‚Äì808  2003.

[28] Franziska Meier  Philipp Hennig  and Stefan Schaal. EfÔ¨Åcient Bayesian local model learning for control.

In Proceedings of the IEEE International Conference on Intelligent Robotics Systems (IROS)  2014.

[29] Joaquin QuiÀúnonero-Candela and Ole Winther. Incremental Gaussian processes. In NIPS  2002.
[30] Stefan Schaal. The SL simulation and real-time control software package. Technical report  2009.
[31] Sethu Vijayakumar and Stefan Schaal. Locally weighted projection regression: Incremental real time

learning in high dimensional space. In ICML  pages 1079‚Äì1086  2000.

9

,Franziska Meier
Philipp Hennig
Stefan Schaal
George Papamakarios
Iain Murray
Vatsal Sharan
Sham Kakade
Percy Liang
Gregory Valiant