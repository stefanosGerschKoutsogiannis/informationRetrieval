2018,Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms,Recent methods for learning a linear subspace from data corrupted by outliers are based on convex L1 and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small [27]. In sharp contrast  the recently proposed Dual Principal Component Pursuit (DPCP) method [22] can provably handle subspaces of high dimension by solving a non-convex L1 optimization problem on the sphere. However  its geometric analysis is based on quantities that are difficult to interpret and are not amenable to  statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers  thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Descent method (DPCP-PSGD) for solving the DPCP problem and show it admits linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGD can be more efficient than the traditional RANSAC algorithm  which is one of the most popular methods for such computer vision applications.,Dual Principal Component Pursuit:

Improved Analysis and Efﬁcient Algorithms

Zhihui Zhu

MINDS

Johns Hopkins University

zzhu29@jhu.edu

Yifan Wang

SIST

ShanghaiTech University

wangyf@shanghaitech.edu.cn

Daniel Robinson

AMS

Johns Hopkins University

daniel.p.robinson@jhu.edu

Daniel Naiman

AMS

Johns Hopkins University
daniel.naiman@jhu.edu

Rene Vidal

MINDS

Johns Hopkins University

ShanghaiTech University

rvidal@jhu.edu

mtsakiris@shanghaitech.edu.cn

Manolis C. Tsakiris

SIST

Abstract

Recent methods for learning a linear subspace from data corrupted by outliers are
based on convex (cid:96)1 and nuclear norm optimization and require the dimension of
the subspace and the number of outliers to be sufﬁciently small [27]. In sharp
contrast  the recently proposed Dual Principal Component Pursuit (DPCP) method
[22] can provably handle subspaces of high dimension by solving a non-convex (cid:96)1
optimization problem on the sphere. However  its geometric analysis is based on
quantities that are difﬁcult to interpret and are not amenable to statistical analysis.
In this paper we provide a reﬁned geometric analysis and a new statistical analysis
that show that DPCP can tolerate as many outliers as the square of the number of
inliers  thus improving upon other provably correct robust PCA methods. We also
propose a scalable Projected Sub-Gradient Method (DPCP-PSGM) for solving
the DPCP problem and show that it achieves linear convergence even though the
underlying optimization problem is non-convex and non-smooth. Experiments on
road plane detection from 3D point cloud data demonstrate that DPCP-PSGM can
be more efﬁcient than the traditional RANSAC algorithm  which is one of the most
popular methods for such computer vision applications.

1

Introduction

Fitting a linear subspace to a dataset corrupted by outliers is a fundamental problem in machine
learning and statistics  primarily known as (Robust) Principal Component Analysis (PCA) [10  2].
The classical formulation of PCA  dating back to Carl F. Gauss  is based on minimizing the sum of
squares of the distances of all points in the dataset to the estimated linear subspace. Although this
problem is non-convex  it admits a closed form solution given by the span of the top eigenvectors of
the data covariance matrix. Nevertheless  it is well-known that the presence of outliers can severely
affect the quality of the computed solution because the Euclidean norm is not robust to outliers.
The sensitivity of classical (cid:96)2-based PCA to outliers has been addressed by using robust maximum
likelihood covariance estimators  such as the one in [25]. However  the associated optimization
problems are non-convex and thus difﬁcult to provide global optimality guarantees. Another classical
approach is the exhaustive-search method of Random Sampling And Consensus (RANSAC) [5]  which
given a time budget  computes at each iteration a d-dimensional subspace as the span of d randomly
chosen points  and outputs the subspace that agrees with the largest number of points. Even though
RANSAC is currently one of the most popular methods in many computer vision applications such as

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Table 1: Probabilistic upper bounds for the number M of tolerated outliers as a function of the
number N of inliers  the subspace dimension d  and the ambient dimension D  by different methods
under a random Gaussian or random spherical model.

Method

GGD [16]

REAPER [13]

GMS [30]

d

d

1

Random Gaussian Model

√
D(D−d)

M (cid:46)
M (cid:46) D
M (cid:46)

d ≤ D−1

2

√
d N 
(D−d)D

N

N

(cid:96)2 1-RPCA [27] M (cid:46)

TME [29]

TORP [3]

M (cid:46)

d max(1  log(M +N
M < D−d

d

d N
1

d max(1  log(M +N )

d

N

)

N

)2

Method

FMS [11]

CoP [19]
DPCP

(this paper)

Random Spherical Model
N/M (cid:39) 0  N → ∞  i.e. 

any ratio of outliers when1N → ∞

√
D

M (cid:46) D−d2

d N 

d <

M (cid:46)

1

dD log2 D N 2

multiple view geometry [9]  its performance is sensitive to the choice of a thresholding parameter.
Moreover  the number of required samplings may become prohibitive in cases when the number of
outliers is very large and/or the subspace dimension d is large and close to the dimension D of the
ambient space (i.e.  the high relative dimension case).
As an alternative to traditional robust subspace learning methods  during the last decade ideas from
compressed sensing have given rise to a new class of methods that are based on convex optimization 
and admit elegant theoretical analyses and efﬁcient algorithmic implementations. Prominent examples
are based on decomposing the data matrix into low-rank and column-sparse parts [27]  expressing
each data point as a sparse linear combination of other data points [20  28]  and measuring the
coherence of each point with every other point in the dataset [19]. The main limitation of these
methods is that they are theoretically justiﬁable only for subspaces of low relative dimension d/D.
However  for applications such as 3D point cloud analysis  two/three-view geometry in computer
vision  and system identiﬁcation  a subspace of dimension D − 1 (high relative dimension) is sought
[21  26]. A promising direction towards handling subspaces of high relative dimension is minimizing
the sum of the distances of the points to the subspace  which is a non-convex problem that REAPER
[13] relaxes to a Semi-Deﬁnite Program (SDP). Even though in practice REAPER outperforms
low-rank methods [27  20  28  19] for subspaces of high relative dimension  its theoretical guarantees
still require d < (D − 1)/2. This is improved upon by the recent work of [16]  which studies a
gradient descent algorithm on the Grassmannian  and establishes convergence with high-probability
to the inlier subspace for any d/D  as long as the number of outliers M scales as (D/d)O(N ).
The focus of the present paper is the recently proposed Dual Principal Component Pursuit (DPCP)
method [22  24  23]  which seeks to learn recursively a basis for the orthogonal complement of the
subspace by solving an (cid:96)1 minimization problem on the sphere. In fact  this optimization problem is
precisely the underlying non-convex problem associated to REAPER and [16] for the special case
d = D − 1. As shown in [22  24]  as long as the points are well distributed in a certain deterministic
sense  any global minimizer of this non-convex problem is guaranteed to be a vector orthogonal to the
subspace  regardless of the outlier/inlier ratio and the subspace dimension; a result that agrees with
the earlier ﬁndings of [14]. Indeed  for synthetic data drawn from a hyperplane (d = D − 1)  DPCP
has been shown to be the only method able to correctly recover the subspace with up to 70% outliers
(D = 30). Nevertheless  the analysis of [22  24] involves geometric quantities that are difﬁcult to
analyze in a probabilistic setting  and consequently it has been unclear how the number M of outliers
that can be tolerated scales as a function of the number N of inliers. Moreover  even though [22  24]
show that relaxing the non-convex problem to a sequence of linear programs (LPs) guarantees ﬁnite
convergence to a vector orthogonal to the subspace  this approach is computationally expensive.
Alternatively  while the Iteratively Reweighted Least Squares (IRLS) scheme proposed in [24  23] is
more efﬁcient than the linear programming approach  it comes with no theoretical guarantees and
scales poorly for high-dimensional data  since it involves an SVD at each iteration.
In this paper we make the following speciﬁc contributions:

1This asymptotic result assumes that d and D are ﬁxed  thus these two parameters are omitted.

2

1

• Theory: An improved analysis of global optimality for DPCP that replaces the cumbersome
geometric quantities of [22  24] with new quantities that are both tighter and easier to bound in
probability. Speciﬁcally  employing a spherical random model suggests that DPCP can handle
dD log2 D N 2) outliers. This is in sharp contrast to existing provably correct state-of-the-art
M = O(
robust PCA methods  which as per Table 1 can tolerate at best M = O(N ) outliers.2
• Algorithms: A scalable Projected Sub-Gradient Method algorithm with piecewise geometrically
diminishing step sizes (DPCP-PSGM)  which is proven to solve the non-convex DPCP problem
with linear convergence and using only matrix-vector multiplications. This is in contrast to classic
results in the literature on the PSGM  which usually requires the problem to be convex in order to
establish sub-linear convergence [1]. DPCP-PSGM is orders of magnitude faster than the LP-based
and IRLS schemes proposed in [24]  which allows us to extend the size of the datasets that we can
handle from 103 to 106 data points.
• Experiments: Experiments on road plane detection from 3D point cloud data using the KITTI
dataset [6]  which is an important computer vision task in autonomous car driving systems  show
that for the same computational budget DPCP-PSGM outperforms RANSAC  which is one of the
most popular methods for such computer vision applications.

2 Global Optimality Analysis for Dual Principal Component Pursuit

Review of DPCP Given a unit (cid:96)2-norm dataset (cid:101)X = [X O]Γ ∈ RD×L  where X ∈ RD×N are

inlier points spanning a d-dimensional subspace S of RD  O are outlier points having no linear
structure  and Γ is an unknown permutation  the goal of robust PCA is to recover the inlier space S
or equivalently to cluster the points into inliers and outliers. Towards that end  the main idea of Dual
Principal Component Pursuit (DPCP) [22  24] is to ﬁrst compute a hyperplane H1 that contains all the
inliers X . Such a hyperplane can be used to discard a potentially very large number of outliers  after
which a method such as RANSAC may successfully be applied to the reduced dataset 3. Alternatively 
if d is known  then one may proceed to recover S as the intersection of D − d orthogonal hyperplanes
that contain X . In any case  DPCP computes a normal vector b1 to the ﬁrst hyperplane H1 as follows:
(1)

Notice that the function (cid:107)(cid:101)X (cid:62)b(cid:107)0 being minimized simply counts how many points in the dataset are

(cid:107)(cid:101)X (cid:62)b(cid:107)0 s. t. b (cid:54)= 0.

min
b∈RD

not contained in the hyperplane with normal vector b. Assuming that there are at least d + 1 inliers
and at least D − d outliers (this is to avoid degenerate situations)  and that all points are in general
∗ to (1) must correspond to a hyperplane that contains X   and hence
position4  then every solution b
∗ is orthogonal to S. Since (1) is computationally intractable  it is reasonable to replace it by5
b

f (b) := (cid:107)(cid:101)X (cid:62)b(cid:107)1 s. t. (cid:107)b(cid:107)2 = 1.

min
b∈RD

(2)

Although problem (2) is non-convex (because of the constraint) and non-smooth (because of the (cid:96)1
norm)  the work of [22  24] established conditions suggesting that if the outliers are well distributed
on the unit sphere and the inliers are well distributed on the intersection of the unit sphere with the
subspace S  then global minimizers of (2) are orthogonal to S. Nevertheless  these conditions are
deterministic in nature and difﬁcult to interpret. In this section  we give improved global optimality
conditions that are i) tighter  ii) easier to interpret and iii) amenable to a probabilistic analysis.

Geometry of the critical points The heart of our analysis lies in a tight geometric characterization
of the critical points of (2) (see Lemma 1 below). Before stating the result  we need to introduce
some further notation and deﬁnitions. Letting PS be the orthogonal projection onto S  we deﬁne
2 ] such that cos(φ) = (cid:107)PS (b)(cid:107)2/(cid:107)b(cid:107)2. Since we will
the principal angle of b from S as φ ∈ [0  π
2 Table 1 is an adaptation of Table I from [12]. We note that not all bounds are directly comparable because
different methods might be analyzed under different models  e.g.  for DPCP we use the random spherical model 
while for REAPER the random Gaussian model is used. Nevertheless  the two models are closely related  since
a random vector distributed according to the standard normal distribution tends to concentrate around the sphere.

3Note that if the outliers are in general position  then H1 will contain at most D − d − 1 outliers.
4Every d-tuple of inliers is linearly independent  and every D-tuple of outliers is linearly independent.
5This optimization problem also appears in different contexts (e.g.  [18] and [21]).

3

(cid:48)

)d

consider the ﬁrst-order optimality conditions of (2)  we naturally need to compute the sub-differential
of the objective function in (2). Towards that end  we denote the sign function by sign(a) = a/|a|
when a (cid:54)= 0  and sign(a) = 0 when a = 0. We also require the sub-differential Sgn of the absolute
value function |a| deﬁned as Sgn(a) = sign(a) when a (cid:54)= 0  and Sgn(a) = [−1  1] when a = 0. We
use sign(a) to indicate that we apply the sign function element-wise to the vector a and similarly for
Sgn. Next  global minimizers of (2) are critical points in the following sense:
Deﬁnition 1. A vector b ∈ SD−1 is called a critical point of (2) if there exists d
(cid:48) ∈ ∂f (b) such that
the Riemannian gradient d := (I−bb
(cid:62)
of f at b.

= 0  where ∂f (b) = (cid:101)X Sgn((cid:101)X (cid:62)b) is the sub-differential
be orthogonal to K ≤ D − 1 columns of (cid:101)X   of which at most d − 1 can be inliers (otherwise b ⊥ S).
where ξ =(cid:80)K
i=1 αji ˜xji with ˜xj1  . . .   ˜xjK the columns of (cid:101)X orthogonal to b and αj1   . . .   αjK ∈

We now illustrate the key idea behind characterizing the geometry of the critical points. Let b be a
critical point that is not orthogonal to S. Then  under general position assumptions on the data  b can

It follows that any Riemannian sub-gradient evaluated at b has the form

[−1  1]. Note that (cid:107)ξ(cid:107)2 < D. Since b is a critical point  Deﬁnition 1 implies a choice of αji so
that d = 0. Deﬁne b = cos(φ)s + sin(φ)n  where φ is the principal angle of b from S  and
s = PS (b)/(cid:107)PS (b)(cid:107)2 and n = PS⊥ (b)/(cid:107)PS⊥ (b)(cid:107)2 are the orthonormal projections of b onto S
and S⊥  respectively. Deﬁning g = − sin(φ)s + cos(φ)n and noting that g ⊥ b  it follows that

)O sign(O(cid:62)b) + (I − bb
(cid:62)

)X sign(X (cid:62)b) + ξ 

d = (I − bb

(3)

(cid:62)

which in particular implies that

Thus  we obtain Lemma 1 after deﬁning

0 = g(cid:62)O sign(O(cid:62)b) − sin(φ)(cid:13)(cid:13)X (cid:62)s(cid:13)(cid:13)1 + g(cid:62)ξ 
sin(φ) ≤(cid:0)(cid:12)(cid:12)g(cid:62)O sign(O(cid:62)b)(cid:12)(cid:12) + D(cid:1) /(cid:13)(cid:13)X (cid:62)s(cid:13)(cid:13)1 .

(cid:12)(cid:12)g(cid:62)O sign(O(cid:62)b)(cid:12)(cid:12) and cX  min :=

1
N

(4)

(5)

1
M

(cid:107)X (cid:62)b(cid:107)1.

min

max

ηO :=

b∈S∩SD−1

g b∈SD−1 g⊥b

M O sign(cid:0)O(cid:62)b(cid:1) tends to the quantity cDb  where cD is the

(6)
Lemma 1. Any critical point b of (2) must either be a normal vector of S  or have a principal angle
φ from S smaller than or equal to arcsin (M ηO/N cX  min)  where ηO := ηO + D/M.
Towards interpreting Lemma 1  we ﬁrst give some insight into the quantities ηO and cX  min. First 
we claim that ηO reﬂects how well distributed the outliers are  with smaller values corresponding
to more uniform distributions. This can be seen by noting that as M → ∞ and assuming that O
remains well distributed  the quantity 1
average height of the unit hemi-sphere of RD [22  24]. Since g ⊥ b  in the limit ηO → 0. Second  the
quantity cX  min is the same as the permeance statistic deﬁned in [13]  and for well-distributed inliers
is bounded away from small values  since there is no single direction in S sufﬁciently orthogonal to
X . We thus see that according to Lemma 1  any critical point of (2) is either orthogonal to the inlier
subspace S  or very close to S  with its principal angle φ from S being smaller for well distributed
points and smaller outlier to inlier ratios M/N. Interestingly  Lemma 1 suggests that any algorithm
can be utilized to ﬁnd a normal vector to S as long as the algorithm is guaranteed to ﬁnd a critical
point of (2) and this critical point is sufﬁciently far from the subspace S  i.e.  it has principal angle
larger than arcsin (M ηO/N cX  min). We will utilize this crucial observation in the next section to
derive guarantees for convergence to the global optimum for a new scalable algorithm.

Global optimality
to cX  min but associated with the outliers  namely

In order to characterize the global solutions of (2)  we deﬁne quantities similar

cO min :=

1
M

min
b∈SD−1

(cid:107)O(cid:62)b(cid:107)1 and cO max :=

1
M

max
b∈SD−1

(cid:107)O(cid:62)b(cid:107)1.

(7)

The next theorem  whose proof relies on Lemma 1  provides new deterministic conditions under
which any global solution to (2) must be a normal vector to S.
Theorem 1. Any global solution b(cid:63) to (2) must be orthogonal to the inlier subspace S as long as

< 1.

(8)

(cid:113)
η2O + (cO max − cO min)2

·

M
N

cX  min

4

Towards interpreting Theorem 1  recall that for well distributed inliers and outliers ηO is small  while
the permeance statistics cO max  cO min are bounded away from small values. Now  the quantity
cO max  thought of as a dual permeance statistic  is bounded away from large values for the reason
that there is not a single direction in RD that can sufﬁciently capture the distribution of O. In fact  as
M increases the two quantities cO max  cO min tend to each other and their difference goes to zero
as M → ∞. With these insights  Theorem 1 implies that regardless of the outlier/inlier ratio M/N 
as we have more and more inliers and outliers while keeping D and M/N ﬁxed  and assuming the
points are well-distributed  condition (8) will eventually be satisﬁed and any global minimizer must
be orthogonal to the inlier subspace S.
A similar condition to (8) is given in [22  Theorem 2]. Al-
though the proofs of the two theorems share some common
elements  [22  Theorem 2] is derived by establishing discrep-
ancy bounds between (2) and a continuous analogue of (2) 
and involves quantities difﬁcult to handle such as spherical
cap discrepancies and circumradii of zonotopes. In addition 
as shown in Figure 1  a numerical comparison of the condi-
tions of the two theorems reveals that condition (8) is much
tighter. We attribute this to the quantities in our new analysis

better representing the function (cid:107)(cid:101)X (cid:62)b(cid:107)1 being minimized 

(a) Check [22  (24)] (b) Check [22  (24)]

(c) Check (8)

(d) Check (8)

namely cX  min  cO min  cO max  and ηO  when compared to
the quantities used in the analysis of [22  24]. Moreover  our
quantities are easier to bound under a probabilistic model 
thus leading to the following characterization of the number
of outliers that may be tolerated.
Theorem 2. Consider a random spherical model where the
columns of O are drawn uniformly from the sphere SD−1
and the columns of X are drawn uniformly from SD−1 ∩ S 
where S is a subspace of dimension d < D. Fix any t <
2(cd
S as long as

Figure 1: Check whether the condition
(8) and a similar condition in [22  Theo-
rem 2] are satisﬁed (white) or not (black)
for a ﬁxed number N of inliers while vary-
ing the outlier ratio M/(M + N ) and the
subspace relative dimension d/D: (a)-(c) 
N = 500; (b)-(d)  N = 1000.
N − 2). Then with probability at least 1 − 6e−t2/2  any global solution of (2) is orthogonal to

√

N − (2 + t/2)

√

N

 

(9)

(cid:17)2

(cid:16)√

(cid:17)2

M ≤(cid:16)(cid:114) 2

πd

(4 + t)2M + C0

D log D + t

where C0 is a universal constant that is independent of N  M  D  d and t.

1

dD log2 D N 2) outliers.
Interestingly  Theorem 2 suggests that DPCP can roughly tolerate M = O(
We believe this makes DPCP the ﬁrst method that is able to tolerate O(N 2) outliers when d and D
are ﬁxed  since as per Table 1 current provably correct state-of-the-art methods can handle at best
M = O(N ). For example  REAPER [13] requires M ≤ O( D
d N ). On the other hand  our bound is a
decreasing function of D  which is an artifact of the proof technique used; we conjecture that this can
be mended by a more sophisticated analysis of the term ηO.
Finally  our choice to use a spherical random model as opposed to a Gaussian model is a technical
one: the analysis is more difﬁcult when the functions are both non-Lipschitz and unbounded. That
being said  we believe that this choice does not impose any practical limitations  since one can always
normalize the data without changing the angles of the inliers/outliers to the linear subspace.

3 A Scalable Algorithm for Dual Principal Component Pursuit

Note that the DPCP problem (2) involves a convex objective function and a non-convex feasible
region  which nevertheless is easy to project onto. This structure was exploited in [18  22]  where
in the second case the authors proposed an Alternating Linearization and Projection (ALP) method
that solves a sequence of linear programs (LP) with a linearization of the non-convex constraint
and then projection onto the sphere.6 Although efﬁcient LP solvers (such as Gurobi [8]) may
be used to solve each LP  these methods do not scale well with the problem size (i.e.  D  N and

6Details of the procedure can be found in the supplementary material  where we are also able to provide an

improved analysis for their ALP method.

5

0.10.20.30.40.50.60.70.170.330.5 0.670.830.970.10.20.30.40.50.60.70.170.330.5 0.670.830.970.10.20.30.40.50.60.70.170.330.500.670.830.970.10.20.30.40.50.60.70.170.330.500.670.830.97M). Inspired by Lemma 1  which states that any critical point that has principal angle larger than
arcsin (M ηO/N cX  min) must be a normal vector of S  we now consider solving (2) with a ﬁrst-order
method  speciﬁcally Projected Sub-Gradient Method (DPCP-PSGM)  which is stated in Algorithm 1.
Input: data (cid:101)X ∈ RD×L and initial step size µ0;
Initialization: set(cid:98)b0 = arg minb (cid:107)(cid:101)X (cid:62)b(cid:107)2  s. t. b ∈ SD−1;

Algorithm 1 (DPCP-PSGM) Projected Sub-gradient Method for Solving (2)

update the step size µk according to a certain rule;

bk =(cid:98)bk−1 − µk (cid:101)X sign((cid:101)X (cid:62)(cid:98)bk−1); (cid:98)bk = PSD−1 (bk) = bk/(cid:107)bk(cid:107);

1: for k = 1  2  . . . do
2:
3:
4: end for

Unlike projected gradient descent for smooth problems  the choice of step size for PSGM is more
complicated since a constant step size in general can not guarantee the convergence of PSGM even
to a critical point  though such a choice is often used in practice. For the purpose of illustration 
consider a simple example h(x) = |x| without any constraint  and suppose that µk = 0.08 for all
k and that an initialization of x0 = 0.1 is used. Then  the iterates {xk} will jump between two
points 0.02 and −0.06 and never converge to the global minimum 0. Thus  a widely adopted strategy
√
is to use diminishing step sizes  including those that are not summable (such as µk = O(1/k) or
k)) [1]  or geometrically diminishing (such as µk = O(ρk)  ρ < 1) [7  4  15]. However 
µk = O(1/
for such choices  most of the literature establishes convergence guarantees for PSGM in the context
of convex feasible regions [1  7  4]  and thus can not be directly applied to Algorithm 1.
For the rest of this section  it is more convenient to use the principal angle θ ∈ [0  π
2 ] between b and
the orthogonal subspace S⊥; thus b is a normal vector of S if and only if θ = 0. We also need a
quantity similar to ηO that quantiﬁes how well the inliers are distributed within the subspace S:

ηX :=

1
N

max

g b∈S∩SD−1 g⊥b

(cid:12)(cid:12)g(cid:62)X sign(X (cid:62)b)(cid:12)(cid:12) .

Our next result provides performance guarantees for Algorithm 1 for various choices of step sizes
ranging from constant to geometrically diminishing step sizes  the latter one giving an R-linear
convergence of the sequence of principal angles to zero.

Theorem 3 (Convergence guarantee for PSGM). Let {(cid:98)bk} be the sequence generated by Algorithm 1
with initialization(cid:98)b0  whose principal angle θ0 to S⊥ is assumed to satisfy
(cid:1)/(cid:0)N ηX + M ηO(cid:1)(cid:17)

(10)
4·max{N cX  min M cO max} . Assuming that N cX  min ≥ N ηX + M ηO  the angle θk between

(cid:98)bk and S⊥ satisﬁes the following properties in accordance with various choices of step sizes.

(cid:16)(cid:0)N cX  min

Let µ(cid:48) :=

θ0 < arctan

1

.

(i) (constant step size) With µk = µ ≤ µ(cid:48)  ∀k ≥ 0  we have

(cid:26)max{θ0  θ(cid:5)(µ)}  k < K(cid:5)(µ) 

k ≥ K(cid:5)(µ) 

θk ≤

θ(cid:5)(µ) 

(cid:16) µ√

2µ(cid:48)

(11)

(cid:17)

.

where K(cid:5)(µ) :=

(ii) (diminishing step size) With µk ≤ µ(cid:48)  µk → 0 (cid:80)∞

µ(N cX  min−max{1 tan(θ0)}(N ηX +M ηO )) and θ(cid:5)(µ) := arctan
k=1 µk = ∞  we have θk → 0.

tan(θ0)

(iii) (diminishing step size of O(1/k)) With µ0 ≤ µ(cid:48)  µk = µ0
(iv) (piecewise geometrically diminishing step size) With µ0 ≤ µ(cid:48) and
k < K0 
k ≥ K0 

µ0β(cid:98)(k−K0)/K(cid:99)+1 

(cid:26)

µk =

µ0 

where β ∈ (0  1)  (cid:98)·(cid:99) is the ﬂoor function  and K0  K ∈ N are chosen such that
N cX  min − (N ηX + M ηO)

K0 ≥ K(cid:5)(µ0) and K ≥(cid:16)√

2βµ(cid:48)(cid:16)

(cid:17)(cid:17)−1

k  ∀k ≥ 1  we have tan(θk) = O( 1
k ).

(12)

(13)

6

with K(cid:5)(µ) deﬁned right after (11)  we have

(cid:40)max{tan(θ0)  µ0√

2µ(cid:48) β(cid:98)(k−K0)/K(cid:99) 
µ0√

tan(θk) ≤

2µ(cid:48)}  k < K0 
k ≥ K0.

(14)

normal vector  (11) ensures that after K(cid:5)(µ) iterations (cid:98)bk is close to S⊥ in the sense that θk ≤ θ(cid:5)(µ) 

First note that with the choice of constant step size µ  although PSGM is not guaranteed to ﬁnd a
which can be much smaller than θ0 for a sufﬁciently small µ. The expressions for K(cid:5)(µ) and θ(cid:5)(µ)
indicate that there is a tradeoff in selecting the step size µ. By choosing a larger step size µ  we
have a smaller K(cid:5)(µ) but a larger upper bound θ(cid:5)(µ). We can balance this tradeoff according to
the requirements of speciﬁc applications. For example  in applications where the accuracy of θ (to
zero) is not as important as the convergence speed  it is appropriate to choose a larger step size. An
alternative and more efﬁcient way to balance this tradeoff is to change the step size as the iterations
proceed. For the classical diminishing step sizes that are not summable  Theorem 3(ii) guarantees

convergence of θk to zero (i.e.  all limit points of the sequence of iterates {(cid:98)bk} are normal vectors) 

though the convergence rate depends on the speciﬁc choice of step size. For example  Theorem 3(iii)
guarantees a sub-linear convergence of tan(θk) for step size diminishing at the rate of 1/k.
The approach of piecewise geometrically dimin-
ishing step size (see Theorem 3(iv)) takes ad-
vantage of the tradeoff in Theorem 3(i) by ﬁrst
using a relatively large initial step size µ0 so that
K(cid:5)(µ0) is small (although θ(cid:5)(µ0) is large)  and
then decreasing the step size in a piecewise fash-
ion. As illustrated in Figure 2  with such a piece-
wise geometrically diminishing step size  (14)
establishes a piecewise geometrically decaying
bound for the principal angles. Note that the
curve tan(θk) is not monotone because  as noted
earlier  PSGM is not a descent method. Per-
haps the most surprising aspect in Theorem 3(iv)
is that with the diminishing step size (12)  we
obtain a K-step R-linear convergence rate for
tan(θk). This linear convergence rate relies on
both the choice of the step size and certain ben-
eﬁcial geometric structure in the problem. As
characterized by Lemma 1  one such structure is
that all critical points in a neighborhood of S⊥ are global solutions. Aside from this  other properties
(e.g.  the negative direction of the Riemannian subgradient points toward S⊥) are used to show
the decaying rate of the principal angle. This is different from the recent work [4] in which linear
convergence for PSGM is obtained for sharp and weakly convex objective functions and convex
constraint sets. Thus  we believe the choice of piecewise geometrically diminishing step size is of
independent interest and can be useful for other nonsmooth problems.7

Figure 2: Illustration of Theorem 3(iv): θk is the
principal angle between bk and S⊥ generated by
the PSGM Algorithm 1 with piecewise geometri-
cally diminishing step size. The red dotted line rep-
resents the upper bound on tan(θk) given by (14) 
while the green dashed line indicates the choice of
the step size (12).

4 Experiments on Synthetic Data and Real 3D Point Cloud Road Data

Synthetic Data We ﬁrst use synthetic data to verify the proposed PSGM algorithm. We ﬁx
D = 30  randomly sample a subspace S of dimension d = 29  and uniformly at random sample
N = 500 inliers and M = 1167 outliers (so that the outlier ratio M/(M + N ) = 0.7) with unit
(cid:96)2-norm. Inspired by the Piecewise Geometrically Diminishing (PGD) step size  we also use a
modiﬁed backtracking line search (MBLS) that always uses the previous step size as an initialization
for ﬁnding the current one within a backtracking line search [17  Section 3.1] strategy  which
dramatically reduces the computational time compared with a standard backtracking line search. The

7While smoothing allows one to use gradient-based algorithms with guaranteed convergence  the obtained
solution is a perturbed version of the targeted one and thus a rounding step (such as solving a linear program [18])
is required. However  as illustrated in Figure 3  solving one linear program is more expensive than the PSGM for
(2) when the data set is relatively large  thus indicating that using a smooth surrogate is not always beneﬁcial.

7

10-410-2100Figure 3: (L) Convergence of PSGM for different step sizes. Comparison of PSGM with ALP and IRLS in [24]
in terms of (M) iterations and (R) computing time. Here D = 30 and d = 29  N = 500  M

M +N = 0.7.

corresponding algorithm is denoted by PSGM-MBLS. (This variant does not have any convergence
guarantee for nonsmooth problems but performed well in practice.) We set K0 = 30  K = 4 and
β = 1/2 for the PGD step size with initial step size obtained by one iteration of a backtracking

line search and denote the corresponding algorithm by PSGM-PGD. We deﬁne(cid:98)b0 to be the bottom
eigenvector of (cid:101)X (cid:101)X (cid:62)  which has been demonstrated to be effective in practice [24].

Figure 3(L) displays the convergence of the PSGM (Algorithm 1)
with different choices of step sizes. We observe linear convergence
for both PSGM-PGD and PSGM-MBLS  which converge much
faster than PSGM with constant step size or classical diminishing
step size. In Figure 3(M)/(R) we compare PSGM algorithms with the
ALP and IRLS algorithm (referred to as DPCP-LP and DPCP-IRLS 
respectively  in [24]). First observe that  as expected  although ALP
ﬁnds a normal vector in few iterations  it has the highest time com-
plexity because it solves an LP during each iteration. Figure 3(R)
indicates that one iteration of ALP consumes more time than the
whole procedure for PSGM. We also note that aside from the the-
oretical guarantee for PSGM-PGD  it also converges faster than
IRLS (in terms of computing time)  the latter lacking a convergence
guarantee. Finally  Figure 4 illustrates Theorem 2  using the same
setup  by showing the principal angle from S⊥ of the solution to the
DPCP problem computed by the PSGM-MBLS algorithm: the phase
transition is indeed quadratic  indicating that DPCP can tolerate as
many as O(N 2) outliers as predicted by Theorem 2.

Figure 4: The principal angle θ
between the solution to the DPCP
problem (2) and S⊥: black cor-
responds to π
2 and white corre-
sponds to 0. Here D = 30 and
d = 29. For each M  we ﬁnd the
smallest N (red dots) such that
θ ≤ 0.001. The blue quadratic
curve indicates the least-squares
ﬁt to these points.

Experiments on real 3D point cloud road data We compare
DPCP-PSGM (with a modiﬁed backtracking line search) with RANSAC [5]  (cid:96)2 1-RPCA [27] and
REAPER [13] on the road detection challenge8 of the KITTI dataset [6]  recorded from a moving
platform while driving in and around Karlsruhe  Germany. This dataset consists of image data
together with corresponding 3D points collected by a rotating 3D laser scanner. In this experiment
we use only the 360◦ 3D point clouds with the objective of determining the 3D points that lie on the
road plane (inliers) and those off that plane (outliers). Typically  each 3D point cloud is on the order
of 100 000 points including about 50% outliers. Using homogeneous coordinates this can be cast
as a robust hyperplane learning problem in R4. Since the dataset is not annotated for that purpose 
we manually annotated a few frames (e.g.  see the left column of Fig. 5). Since DPCP-PSGM is
the fastest method (on average converging in about 100 milliseconds for each frame on a 6 core 6
thread Intel (R) i5-8400 machine)  we set the time budget for all methods equal to the running time
of DPCP-PSGM. For RANSAC we also compare with 10 and 100 times that time budget. Since
(cid:96)2 1-RPCA does not directly return a subspace model  we extract the normal vector via SVD on the
low-rank matrix returned by that method. Table 2 reports the area under the Receiver Operator Curve
(ROC)  the latter obtained by thresholding the distances of the points to the hyperplane estimated by
each method  using a suitable range of different thresholds9. As seen  even though a low-rank method 
(cid:96)2 1-RPCA performs reasonably well but not on par with DPCP-PSGM and REAPER  which overall

8Coherence Pursuit [19] is not applicable to this experiment because forming the required correlation matrix

of the thousands of 3D points is prohibitively expensive.

9For RANSAC  we also use each such threshold as its internal thresholding parameter.

8

02004006008001000iteration10-1010-505010015020010-10100PSGM-PGDPSGM-MBLSIRLSALP10-210-110010-10100PSGM-PGDPSGM-MBLSIRLSALP10020030040050060070080005001000150020002500300035004000Fitted 0.001Quadratic fitFigure 5: Frame 21 of dataset KITTI-CITY-48: raw image  projection of annotated 3D point cloud
onto the image  and detected inliers/outliers using a ground-truth threshold on the distance to the
hyperplane for each method. The corresponding F1 measures are DPCP-PSGM (0.933)  REAPER
(0.890)  (cid:96)21-RPCA (0.248)  RANSAC (0.023)  10xRANSAC (0.622)  and 100xRANSAC (0.824).

tend to be the most robust methods. On the contrary  for the same time budget  RANSAC  which is
a popular choice in the computer vision community for such outlier detection tasks  is essentially
failing due to an insufﬁcient number of iterations. Even allowing for a 100 times higher time budget
still does not make RANSAC the best method  as it is outperformed by DPCP-PSGM on ﬁve out of
the seven point clouds (1  45  and 137 in KITTY-CITY-5  and 0 and 21 in KITTY-CITY-48).

Table 2: Area under ROC for annotated 3D point clouds with index 1  45  120  137  153 in KITTY-
CITY-5 and 0  21 in KITTY-CITY-48. The number in parenthesis is the percentage of outliers.

KITTY-CITY-5

Methods
DPCP-PSGM 0.998
REAPER
0.998
0.841
(cid:96)2 1-RPCA
0.596
RANSAC
10xRANSAC
0.911
100xRANSAC 0.991

0.999
0.998
0.953
0.592
0.773
0.983

0.868
0.839
0.610
0.569
0.717
0.965

KITTY-CITY-48
1(37%) 45(38%) 120(53%) 137(48%) 153(67%) 0(56%) 21(57%)
0.991
0.982
0.837
0.531
0.598
0.902

1.000
0.999
0.925
0.551
0.654
0.955

0.994
0.994
0.836
0.534
0.757
0.974

0.749
0.749
0.575
0.521
0.624
0.849

5 Conclusions
We provided an improved analysis for the global optimality of the DPCP method that suggests that
DPCP can handle O((#inliers)2) outliers. We also presented a scalable ﬁrst-order method for solving
the DPCP problem that only uses matrix-vector multiplications  for which we established global
convergence guarantees for various step size selection schemes  regardless of the non-convexity and
non-smoothness of the DPCP problem. Finally  experiments on 3D point cloud road data demonstrate
that the proposed method is able to outperform RANSAC even when RANSAC is allowed to use 100
times the computational budget of the proposed method. Extensions to allow for corrupted data and
multiple subspaces  and further applications in computer vision are the subject of ongoing work.

Acknowledgment

The co-authors from JHU were supported by NSF grant 1704458. We thank Tianyu Ding of JHU for
carefully proof-reading the longer version of this manuscript and catching some mistakes  Yunchen
Yang and Tianjiao Ding of ShanghaiTech for reﬁning the 3D point cloud experiment  Ziyu Liu of
ShanghaiTech for his help in deriving the concentration inequality for ηO  and the three anonymous
reviewers for their constructive comments.

9

References
[1] S. Boyd  L. Xiao  and A. Mutapcic. Subgradient methods. Lecture Notes of EE392o  Stanford University 

Autumn Quarter  2004:2004–2005  2003.

[2] E. Candès  X. Li  Y. Ma  and J. Wright. Robust principal component analysis. Journal of the ACM  58 

2011.

[3] Y. Cherapanamjeri  P. Jain  and P. Netrapalli. Thresholding based efﬁcient outlier robust pca. arXiv preprint

arXiv:1702.05571  2017.

[4] D. Davis  D. Drusvyatskiy  K. J. MacPhee  and C. Paquette. Subgradient methods for sharp weakly convex

functions. arXiv preprint arXiv:1803.02461  2018.

[5] M. A. Fischler and R. C. Bolles. RANSAC random sample consensus: A paradigm for model ﬁtting with
applications to image analysis and automated cartography. Communications of the ACM  26:381–395 
1981.

[6] A. Geiger  P. Lenz  C. Stiller  and R. Urtasun. Vision meets robotics: The kitti dataset. The International

Journal of Robotics Research  32(11):1231–1237  2013.

[7] J.-L. Gofﬁn. On convergence rates of subgradient optimization methods. Mathematical Programming 

13(1):329–347  1977.

[8] I. Gurobi Optimization. Gurobi optimizer reference manual  2015.

[9] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge  2nd edition  2004.

[10] I. Jolliffe. Principal Component Analysis. Springer-Verlag  New York  1986.

[11] G. Lerman and T. Maunu. Fast  robust and non-convex subspace recovery. Information and Inference: A

Journal of the IMA  7(2):277–336  2017.

[12] G. Lerman and T. Maunu. An overview of robust subspace recovery. arXiv:1803.01013 [cs.LG]  2018.

[13] G. Lerman  M. B. McCoy  J. A. Tropp  and T. Zhang. Robust computation of linear models by convex

relaxation. Foundations of Computational Mathematics  15(2):363–410  2015.

[14] G. Lerman and T. Zhang. (cid:96)p-recovery of the most signiﬁcant subspace among multiple subspaces with

outliers. Constructive Approximation  40(3):329–385  2014.

[15] X. Li  Z. Zhu  A. M.-C. So  and R. Vidal. Nonconvex robust low-rank matrix recovery. arXiv preprint

arXiv:1809.09237  2018.

[16] T. Maunu and G. Lerman. A well-tempered landscape for non-convex robust subspace recovery.

arXiv:1706.03896 [cs.LG]  2017.

[17] J. Nocedal and S. J. Wright. Numerical Optimization  second edition. World Scientiﬁc  2006.

[18] Q. Qu  J. Sun  and J. Wright. Finding a sparse vector in a subspace: Linear sparsity using alternating

directions. In Advances in Neural Information Processing Systems  pages 3401–3409  2014.

[19] M. Rahmani and G. Atia. Coherence pursuit: Fast  simple  and robust principal component analysis. arXiv

preprint arXiv:1609.04789  2016.

[20] M. Soltanolkotabi  E. Elhamifar  and E. J. Candès. Robust subspace clustering. Annals of Statistics 

42(2):669–699  2014.

[21] H. Späth and G. Watson. On orthogonal linear (cid:96)1 approximation. Numerische Mathematik  51(5):531–543 

1987.

[22] M. Tsakiris and R. Vidal. Dual principal component pursuit. In ICCV Workshop on Robust Subspace

Learning and Computer Vision  pages 10–18  2015.

[23] M. C. Tsakiris and R. Vidal. Hyperplane clustering via dual principal component pursuit. In International

Conference on Machine Learning  2017.

[24] M. C. Tsakiris and R. Vidal. Dual principal component pursuit. Journal of Machine Learning Research 

19(18):1–50  2018.

10

[25] D. E. Tyler. A distribution-free m-estimator of multivariate scatter. The Annals of Statistics  15(1):234–251 

1987.

[26] Y. Wang  C. Dicle  M. Sznaier  and O. Camps. Self scaled regularized robust regression. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition  pages 3261–3269  2015.

[27] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. In Advances in Neural Information

Processing Systems  pages 2496–2504  2010.

[28] C. You  D. Robinson  and R. Vidal. Provable self-representation based outlier detection in a union of

subspaces. In IEEE Conference on Computer Vision and Pattern Recognition  2017.

[29] T. Zhang. Robust subspace recovery by tyler’s m-estimator. Information and Inference: A Journal of the

IMA  5(1):1–21  2016.

[30] T. Zhang and G. Lerman. A novel m-estimator for robust pca. The Journal of Machine Learning Research 

15(1):749–808  2014.

11

,Marco Cuturi
Zhihui Zhu
Yifan Wang
Daniel Robinson
Daniel Naiman
René Vidal
Manolis Tsakiris