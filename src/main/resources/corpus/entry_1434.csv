2013,Density estimation from unweighted k-nearest neighbor graphs: a roadmap,Consider an unweighted k-nearest neighbor graph   on n points that have been sampled i.i.d. from some unknown density p on R^d. We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph  without knowing the points themselves or their distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate some local function of p  and that integrating this function along shortest paths leads to an estimate of the underlying density.,Density estimation from unweighted k-nearest

neighbor graphs: a roadmap

Ulrike von Luxburg

and

Morteza Alamgir

Department of Computer Science
University of Hamburg  Germany

{luxburg alamgir}@informatik.uni-hamburg.de

Abstract

Consider an unweighted k-nearest neighbor graph on n points that have been sam-
pled i.i.d. from some unknown density p on Rd. We prove how one can estimate
the density p just from the unweighted adjacency matrix of the graph  without
knowing the points themselves or any distance or similarity scores. The key in-
sights are that local differences in link numbers can be used to estimate a local
function of the gradient of p  and that integrating this function along shortest paths
leads to an estimate of the underlying density.

1

Introduction

The problem. Consider an unweighted k-nearest neighbor graph that has been built on a random
sample X1  ...  Xn from some unknown density p on Rd. Assume we are given the adjacency matrix
of the graph  but we do not know the point locations X1  ....  Xn or any distance or similarity scores
between the points. Is it then possible to estimate the underlying density p  just from the adjacency
matrix of the unweighted graph?
Why is this problem interesting for machine learning? Machine learning algorithms on graphs
are abundant  ranging from graph clustering methods such as spectral clustering over label prop-
agation methods for semi-supervised learning to dimensionality reduction methods and manifold
algorithms. In the majority of applications  the graphs that are used as input are similarity graphs:
Given a set of abstract “objects” X1  ...  Xn we ﬁrst compute pairwise similarities s(Xi  Xj) accord-
ing to some suitable similarity function and then build a k-nearest neighbor graph (kNN graph for
short) based on this similarity function. The intuition is that the edges in the graph encode the local
information given by the similarity function  whereas the graph as a whole reveals global properties
of the data distribution such as cluster properties  high- and low-density regions  or manifold struc-
ture. From a computational point of view  kNN graphs are convenient because they lead to a sparse
representation of the data — even more so when the graph is unweighted. From a statistical point of
view the key question is whether this sparse representation still contains all the relevant information
about the original data  in particular the information about the underlying data distribution. It is easy
to see that for suitably weighted kNN graphs this is the case: the original density can be estimated
from the degrees in the graph. However  it is completely unclear whether the same holds true for
unweighted kNN graphs.
Why is the problem difﬁcult?
The naive attempt to estimate the density from vertex degrees
obviously has to fail in unweighted kNN graphs because all vertex degrees are (about) k. Moreover 
unweighted kNN graphs are invariant with respect to rescaling of the underlying distribution by
a constant factor (e.g.  the unweighted kNN graph on a sample from the uniform distribution on
[0  1]2 is indistinguishable from a kNN graph on a sample from the uniform distribution on [0  2]2).
So all we can hope for is an estimate of the density up to some multiplicative constant that cannot
be determined from the kNN graph alone. The main difﬁculty  however  is that a kNN graph “looks

1

the same” in every small neighborhood. To see this  consider the case where the underlying density
is continuous  hence approximately constant in small neighborhoods. Then  if n is large and k/n is
small  local neighborhoods in the kNN graph are all going to look like kNN graphs from a uniform
It is impossible to estimate the density in
distribution. This intuition raises an important issue.
an unweighted kNN graph by local quantities alone. We somehow have to make use of global
properties if we want to be successful. This makes the problem very different and much harder than
more standard density estimation problems.
Our solution. We show that it is indeed possible to estimate the underlying density from an
unweighted kNN graph. The construction is fairly involved. In a ﬁrst step we estimate a pointwise
function of the gradient of the density  and in a second step we integrate these estimates along
shortest paths in the graph to end up with an approximation of the log-density. Our estimate works
as long as the kNN graph is reasonably dense (kd+2/(n2 logd n) → ∞). However  it fails in the
more important sparser regime (e.g.  k ≈ log n). Currently we do not know whether this is due to a
suboptimal proof or whether density estimation is generally impossible in the sparse regime.

of X . For ε > 0 deﬁne the ε-interior Xε := {x ∈ X (cid:12)(cid:12) d(x  ∂X ) ≥ ε}. We assume that X is “full

2 Notation and assumptions
Underlying space. Let X ⊂ Rd be a compact subset of Rd. Denote by ∂X the topological boundary
dimensional” in the sense that there exists some ε0 > 0 such that Xε0 is non-empty and connected.
By ηd we denote the volume of a d-dimensional unit ball  and by vd the volume of the intersection
of two d-dimensional unit balls whose centers have distance 1.
Density. Let p be a continuously differentiable density on X . We assume that there exist constants
pmin and pmax such that 0 < pmin ≤ p(x) ≤ pmax < ∞ for all x ∈ X .
Graph. Given an i.i.d. sample Xn := {X1  ...  Xn} from p  we build a graph Gn = (Vn  En) with
Vn = Xn. We connect Xi by a directed edge to Xj if Xj is among the k-nearest neighbors of Xi.
The resulting graph is called the directed  unweighted kNN graph (in the following  we will often
drop the words “directed” and “unweighted”). By r(x) := rn k(x) we denote the Euclidean distance
of a point x to its kth nearest neighbor. For any vertex x ∈ V we deﬁne the sets

In(x) := Inn k(x) := {y ∈ Xn
Out(x) := Outn k(x) := {y ∈ Xn

(source points of in-links to x)
(target points of out-links from x).

(cid:12)(cid:12) (y  x) ∈ En}
(cid:12)(cid:12) (x  y) ∈ En}
(cid:90) 1

p1/d(x) ds :=

0

(cid:90)

γ

To increase readability we often omit the indices n and k. For a ﬁnite set S we denote by |S| its
number of elements.
Paths. For a rectiﬁable path γ : [0  1] → X we deﬁne its p-weighted length as

(cid:96)p(γ) :=

p1/d(γ(t))|γ(cid:48)(t)| dt

(recall the notational convention of writing “ds” in a line integral). For two points x  y ∈ X we
deﬁne their p-weighted distance as Dp(x  y) = inf γ (cid:96)p(γ) where the inﬁmum is taken over all
rectiﬁable paths γ that connect x to y. As a consequence of the compactness of X   a minimizing
path that realizes Dp always exists (cf. Burago et al.  2001  Section 2.5.2). We call such a path a
Dp-shortest path. Under the given assumptions on p  the Dp-shortest path between any two points
x  y ∈ Xε0 is smooth.
In an unweighted graph  deﬁne the length of a path as its number of edges. For two vertices
x  y denote by Dsp(x  y) their shortest path distance in the graph. It has been proved in Alamgir
and von Luxburg (2012) that for unweighted  undirected kNN graphs  (k/(nηd))1/dDsp(x  y) →
Dp(x  y) almost surely as n → ∞ and k → ∞ appropriately slowly. The proofs extend directly to
the case of directed kNN graphs.

3 Warmup: the 1-dimensional case

To gain some intuition about the problem and its solution  let us consider the 1-dimensional case
X ⊂ R. For any given point x ∈ Xn we deﬁne the following sets:

Right1(x) := |{y ∈ Out(x)(cid:12)(cid:12) y > x}|.

Left1(x) := |{y ∈ Out(x)(cid:12)(cid:12) y < x}|

and

2

Figure 1: Geometric argument (left: 1-dimensional case  right: 2-dimensional case). The difference
Right− Left is approximately proportional to the volume of the grey-shaded area.

The intuition to estimate the density from the directed kNN graph is the following. Consider a point
x in a region where the density has positive slope. The set Out(x) is approximately symmetric
around x  that is it has the form Out(x) = Xn ∩ [x − R  x + R] for some R > 0. When the density
has an increasing slope at x  there tend to be less sample points in [x−R  x] than in [x  x+R]  so the
set Right1(x) tends to contain more sample points than the set Left1(x). This is the effect we want
to exploit. The difference Right1(x)−Left1(x) can be approximated by n·(P ([x  x+R])−P ([x−
R  x]))  and by a simple geometric argument one can see that the latter probability is approximately
R2p(cid:48)(x). See Figure 1 (left side) for an illustration. By standard concentration arguments one can
see that if n is large enough and k chosen appropriately  then R ≈ k/(2np(x)). Plugging these
two things together shows that Right1(x) − Left1(x) ≈ (k2/(4n2)) · p(cid:48)(x)/p2  hence gives an
estimate of p(cid:48)(x)/p2(x). But we are not there yet: it is impossible to directly turn an estimate of
p(cid:48)(x)/p2(x) into an estimate of p(x). This is in accordance with the intuition we mentioned above:
one cannot estimate the density by just looking at a local neighborhood of x in the kNN graph.
Here is now the key trick to introduce a global component to the estimate. We ﬁx one data point
X0 that is going to play the role of an anchor point. To estimate the density at a particular data
point Xs  we now sum the estimates p(cid:48)(x)/p2(x) over all data points x that sit between X0 and Xs.
This corresponds to integrating the function p(cid:48)(x)/p2(x) over the interval [X0  Xs] with respect to
the underlying density p  which in turn corresponds to integrating the function p(cid:48)(x)/p(x) over the
interval [X0  Xs] with respect to the standard Lebesgue measure. This latter integral is well known 
its primitive is log p(x). Hence  for each data point Xs we get an estimate of log p(Xs)− log p(X0).
Then we exponentiate and arrive at an estimate of c · p(x)  where c = 1/p(X0) plays the role of an
unknown constant.

4 A hypothetical estimate in the d-dimensional case

We now generalize our approach to the d-dimensional setting. There are two main challenges: First 
we need to replace the integral over all sample points between X0 and Xs by something more general
in Rd. Our idea is to consider an integral along a path between X0 and Xs  speciﬁcally along a path
that corresponds to a shortest path in the graph Gn. Second  we need a generalization of the concept
of what are “left” and “right” out-links. Our idea is to use the shortest path as reference. For a point
x on the shortest path between X0 and Xs  the “left points” of x should be the ones that are on or
close to the subpath from X0 to x and “right points” the ones on or close to the path from x to Xs.

4.1 Gradient estimates based on link differences

Fix a point x on a simple  continuously differentiable path γ and let T (x) be its tangent vector.
Consider h(y) = (cid:104)w  y(cid:105) + b with normal vector w := T (x)  where the offset b has been chosen such

that the hyperplane H := {y ∈ Rd(cid:12)(cid:12) h(y) = 0} goes through x. Deﬁne

Leftd(x) := Leftd n k(x) := |{x ∈ Out(x)(cid:12)(cid:12) h(x) ≤ 0}|
Rightd(x) := Rightd n k(x) := |{x ∈ Out(x)(cid:12)(cid:12) h(x) > 0}|.

3

xxlxrDensityR Rp'(x)TangentLeftRight11LeftRightTangent space to the density at xRp'(x)RddFigure 2: Deﬁnitions of “left” and “right”in the d-dimensional case.

See Figure 2 (left side) for an illustration. This deﬁnition is a direct generalization of the deﬁnition
of Left1 und Right1 in the 1-dimensional case. It is not yet the end of the story  as the quantities
Leftd and Rightd cannot be evaluated based on the kNN graph alone  but it is a good starting point to
develop the necessary proof concepts. In this section we prove the consistency of a density estimate
based on Leftd and Rightd. In Section 5 we will further generalize the deﬁnition to our ﬁnal estimate.
Theorem 1 (Estimate related to the gradient) Let X and p satisfy the assumptions in Section 2.
Let γ be a differentiable  regular  simple path in Xε0 and x a sample point on this path. Let T be
the tangent direction of γ at x and p(cid:48)
T (x) the directional derivative of the density p in direction T at
point x. Then  if n → ∞  k → ∞  k/n → 0  kd+2/n2 → ∞ 

(cid:16)

2n1/dη1/d
d
k(d+1)/d

Rightd n k(x) − Leftd n k(x)

(cid:17) −→

p(cid:48)
T (x)

p(x)(d+1)/d

a.s.

If kd+2/(n2 logd n) → ∞ the convergence even holds uniformly over all sample points x ∈ Xn.
Proof sketch. The key problem in the proof is that the difference Rightd−Leftd is of a much smaller
order of magnitude than Rightd and Leftd themselves  so controlling the deviations of Rightd−Leftd
is somewhat tricky. Conditioned on rout(x) =: r  Rightd ∼ Bin(k  πr) and Leftd ∼ Bin(k  πl) 
√
where πr = P (right half ball)/P (ball) and πl analogously (cf. Figure 2). By Hoeffding’s inequal-
ity  Rightd− Leftd ≈ E(Rightd− Leftd)± Θ(
k) with high probability. Note that πl and πr tend to
be close to 1/2  thus Hoeffding’s inequality is reasonably tight. A simple geometric argument shows
that if the density in a neighborhood of x is linear  then E(Rightd − Leftd) = n · rdηd/2 · rp(cid:48)
T (x)
(n times the probability mass of the grey area in Figure 1). A similar argument holds approximately
if the density is just differentiable at x. A standard concentration argument for the out-radius shows
that with high probability  rout(x) can be approximated by (k/(nηdp(x)))1/d. Combining all results
we obtain that with high probability 

2n1/dη1/d
d
k(d+1)/d

(Rightd − Leftd) =

p(cid:48)
T (x)

p(x)(d+1)/d

± Θ

(cid:16) n1/d

k1/2+1/d

(cid:17)

.

Convergence takes place if the noise term on the right hand side goes to 0 and the “high probability”
converges to 1  which happens under the conditions on n and k stated in the theorem.

4.2

Integrating the gradient estimates along the shortest path

To deal with the integration part  let us recap some standard results about line integrals.
Proposition 2 (Line integral) Let γ : [0  1] → Rd be a simple  continuously differentiable path
from x0 = γ(0) to x1 = γ(1) parameterized by arc length. For a point x = γ(t) on the path  denote
by T (x) the tangent vector to γ at x  and by p(cid:48)
T (x) the directional derivative of p in the tangent
direction T . Then

(cid:44)

(cid:90)

p(cid:48)
T (x)
p(x)

γ

ds = log(p(x1)) − log(p(x0)).

4

LeftRightpath γOut(x)HddxLeftRightpath γOut(x)In(x  )In(x  )lrγγxlxrProof. We deﬁne the vector ﬁeld

F : Rd → Rd  x (cid:55)→ p(cid:48)(x)

p(x)

=

1

p(x)

(cid:32)∂p/∂x1

(cid:33)

...

.

∂p/∂xd

Observe that F is a continuous gradient ﬁeld with primitive V : Rd → R  x (cid:55)→ log(p(x)). Now
consider the line integral of F along γ:

(cid:90)

(cid:90) 1

(cid:68)

(cid:69)

(cid:90) 1

(cid:68)

F (x) dx

def
=

γ

0

F (γ(t))  γ(cid:48)(t)

dt =

1

p(cid:48)(γ(t))  γ(cid:48)(t)

dt.

(1)

p(γ(t))

0

Note that γ(cid:48)(t) is the tangent vector T (x) of the path γ at point x = γ(t). Hence  the scalar product
(cid:104)p(cid:48)(γ(t))  γ(cid:48)(t)(cid:105) coincides with the directional derivative of p in direction T   so the right hand side
of Equation (1) coincides with the left hand side of the equation in the proposition. On the other
hand  it is well known that the line integral over a gradient ﬁeld only depends on the starting and
end point of γ and is given by

(cid:69)

(cid:90)

γ

F (x) dx = V (x1) − V (x0).

This coincides with the right hand side of the equation in the proposition.
(cid:44)
Now we consider the ﬁnite sample case. The goal is to approximate the integral along the continuous
path γ by a sum along a path γn in the kNN graph Gn. To achieve this  we need to construct a
sequence of paths γn in Gn such that γn converges to some well-deﬁned path γ in the underlying
space and the lengths of γn in Gn converge to (cid:96)p(γ). To this end  we are going to consider paths γn
which are shortest paths in the graph.
Adapting the proof of the convergence of shortest paths in unweighted kNN graphs (Alamgir and
von Luxburg  2012) we can derive the following statement for integrals along shortest paths.
Proposition 3 (Integrating a function along a shortest path) Let X and p satisfy the assumptions
in Section 2. Fix two sample points in Xε0  say X0 and Xs  and let γn be a shortest path between
X0 and Xs in the kNN graph Gn. Let γ ⊂ X be a path that realizes Dp(X0  Xs). Assume that it
is unique and is completely contained in Xε0. Let g : X → R be a continuous function. Then  as
n → ∞  k1+α/n → 0 (for some small α > 0)  k/ log n → ∞ 

(cid:18) k

nηd

(cid:19)1/d · (cid:88)

x∈γn

(cid:90)

γ

g(x) −→

g(x)p(x)1/d ds a.s.

Note that if g(x)p1/d(x) can be written in the form (cid:104)F (γ(t))  γ(cid:48)(t)(cid:105)  then the same statement even
holds if the shortest Dp-path is not unique  because the path integral then only depends on start and
end point. This is the case for our particular function of interest  g(x) = p(cid:48)

T (x)/p1+1/d(x).

4.3 Combining everything to obtain a density estimate
Theorem 4 (Density estimate) Let X and p satisfy the assumptions in Section 2  let X0 ∈ Xε0 be
any ﬁxed sample point. For another sample point Xs  let γn be a shortest path between X0 and Xs
in the kNN graph Gn. Assume that there exists a path γ that realizes Dp(x  y) and that is completely
contained in Xε0. Then  as n → ∞  k → ∞  k/n → 0  kd+2/(n2 logd n) → ∞ 

(Rightd n k(x) − Leftd n k(x)) −→ log p(Xs) − log p(X0) a.s.

(cid:88)

x∈γn

2
k

Proof sketch. By Proposition 2 

log(p(Xs)) − log(p(X0)) =

(cid:90)

γ

p(cid:48)
T (x)
p(x)

ds =

(cid:90)

p(cid:48)
T (x)

p(x)(d+1)/d

γ

p(x)1/d ds.

5

According to Proposition 3  the latter can be approximated by

(cid:18) k

(cid:19)1/d (cid:88)

p(cid:48)
T (x)

nηd

x∈γn

p(x)(d+1)/d

where γn is a shortest path between X0 and Xs in the kNN graph. Proposition 1 shows that this
quantity gets estimated by

(cid:18) k

(cid:19)1/d n1/d

nηd

k(d+1)/d

(cid:88)

x∈γn

· 2η1/d

d

(cid:16)

(cid:17)
Rightd(x) − Leftd(x)

(cid:88)

(cid:16)

x∈γn

=

2
k

Rightd(x) − Leftd(x)

(cid:17)

.

5 The ﬁnal d-dimensional density estimate

(cid:44)

In this section  we ﬁnally introduce an estimate that solely uses quantities available from the kNN
graph. Let x be a vertex on a shortest path γn k in the kNN graph Gn. Let xl and xr be the
predecessor and successor vertices of x on this path (in particular  xl and xr are sample points as
well). Deﬁne

Leftγn k (x) := | Out(x) ∩ In(xl)|

and

Rightγn k

(x) := | Out(x) ∩ In(xr)|.

See Figure 2 (right side) for an illustration. On ﬁrst glance  these sets look quite different from
Leftd and Rightd. But the intuition is that whenever we ﬁnd two sets on the left and right side of
x that have approximately the same volume  then the difference Leftγn k − Rightγn k should be a
function of p(cid:48)
T (x). For a second intuition consider the special case d = 1 and recall the deﬁnition of
R of Section 3. One can show that in expectation  [x − R  x] coincides with Out(x) ∩ In(xl) and
[x  x + R] with Out(x) ∩ In(xr)  so in case d = 1 the deﬁnitions coincide in expectation with the
ones in Section 3. Another insight is that the set Leftγn k (x) counts the number of directed paths of
length 2 from x to xl  and Rightγn k
− Leftγn k can be used as before to construct a density
We conjecture that the difference Rightγn k
estimate. Speciﬁcally  if γn k is a shortest path from the anchor point X0 to Xs  we believe that
under similar conditions on k and n as before 

(x) analogously.

Rightγn k

(x) − Leftγn k (x)

((cid:63))

(cid:88)

x∈γn

ηd
kνd

is a consistent estimator of the quantity log p(Xs) − log p(X0). Our simulations in Section 6 show
that the estimate works  even surprisingly well. So far we do not have a formal proof yet  due
to two technical difﬁculties. The ﬁrst problem is that the set In(xl) is not a ball  but an “egg-
shaped” set. As n → ∞  one can sandwich In(x) between two concentric balls that converge to
each other  but this approximation is too weak to carry the proof. To compute the expected value
(x)− Leftγn k (x)) we would have to integrate the intersection of the “egg” In(xl) with
E(Rightγn k
the ball Out(x)  and so far we have no closed form solution. The second difﬁculty is related to the
shortest path in the graph. While it is clear that “most edges” in this path have approximately the
maximal length (that is  (k/(nηdp(x))1/d for an edge in the neighborhood of x)  this is not true for
all edges. Intuitively it is clear that the contribution of the few violating edges will be washed out in
the integral along the shortest path  but we don’t have a formal proof yet.
What we can prove is the following weaker version. Consider a Dp-shortest path γ ⊂ Rd and a point
x on this path with out-radius rout(x). Deﬁne the points xl and xr as the two points where the path γ
enters resp. leaves the ball B(x  rout(x))  and deﬁne the sets Ln k := Out(x) ∩ B(xl  rout(x)) and
Rn k := Out(x) ∩ B(xr  rout(x)). Then it can be proved that (η1/d
Rn k(x) −
Ln k(x) → log p(Xs) − log p(X0). The proof is similar to the one in Section 4 . It circumvents the
problems mentioned above by using well deﬁned balls instead of In-sets and the continuous path γ
rather than the ﬁnite sample shortest path γn  but the quantities cannot be estimated from the kNN
graph alone.

)(cid:80)

x∈γn

)/(kν1/d

d

d

6

6 Simulations

As a proof of concept  we ran simple experiments to evaluate the behavior of estimator ((cid:63)). We
draw n = 2000 points according to a couple of simple densities on R  R2 and R10  then we build
the directed  unweighted kNN graph with k = 50. We ﬁx a random point as anchor point X0 
compute the quantities Rightγn k and Leftγn k for all sample points  and then sum the differences
− Leftγn k along shortest paths to X0. Rescaling by the constant ηn/(kvd) and exponen-
Rightγn k
tiating then leads to our estimate for p(x)/p(X0). In order to nicely plot our results  we multiply
the resulting estimate by p(X0) to get rid of the scaling constant (this step would not be possible in
applications  but it merely serves for illustration purposes). The results are shown in Figure 3. It is
obvious from these ﬁgures that our estimate “works”  even surprisingly well (note that the sample
size is not very large and we did not perform any parameter tuning). Even in the case of a step
function the estimate recovers the structure of the density. Note that this is a particularly difﬁcult
case in our setting  because within the constant parts of the two steps  the kNN graphs of the left and
right step are indistinguishable. It is only in a small strip around the boundary between the two steps
that kNN graph will reveal non-uniform behavior. The simulations show that this is already enough
to reveal the overall structure of the step function.

7 Extensions

We have seen how to estimate the density in an unweighted  directed kNN graph. It is even possible
to extend this result to more general cases. Here is a sketch of the main ideas.
Estimating the dimension from the graph. The current density estimate requires that we know
the dimension d of the underlying space because we need to be able to compute the constants ηd
(volume of the unit ball) and vd (intersection of two unit balls). The dimension can be estimated
from the directed  unweighted kNN graph as follows. Denote by r the distance of x to its kth-nearest
neighbor  and by K the number of vertices that can be reached from x by a directed shortest path
of length 2. Then k/n ≈ P (B(x  r)) and K/n ≈ P (B(x  2r)). If n is large enough and k small
enough  the density on these balls is approximately constant  which implies K/k ≈ 2d where d is
the dimension of the underlying space.
Recovering the directed graph from the undirected one. The current estimate is based on the
directed kNN graph  but many applications use undirected kNN graphs. However  it is possible to
recover the directed  unweighted kNN graph from the undirected  unweighted graph. Denote by
N (x) the vertices that have an undirected edge to x. If n is large and k small  then for any two
vertices x and y we can approximate |N (x) ∩ N (y)|/n ≈ P (B(x  r) ∩ B(y  r)). The latter is
monotonously decreasing with (cid:107)x − y(cid:107). To estimate the set Out(x) in order to recover the directed
kNN graph  we rank all points y ∈ N (x) according to |N (x) ∩ N (y)| and choose Out(x) as the
ﬁrst k vertices in this ranking.
Point embedding. In this paper we focus on estimating the density from the unweighted kNN graph.
Another interesting problem is to recover an embedding of the vertices to Rd such that the kNN graph
based on the embedded vertices corresponds to the given kNN graph. This problem is closely related
to a classic problem in statistics  namely non-metric multidimensional scaling (Shepard  1966  Borg
and Groenen  2005)  and more speciﬁcally to learning distances and embeddings from ranking and
comparison data (Schultz and Joachims  2004  Agarwal et al.  2007  Ouyang and Gray  2008  McFee
and Lanckriet  2009  Shaw and Jebara  2009  Shaw et al.  2011  Jamieson and Nowak  2011) as well
as to ordinal (monotone) embeddings (Bilu and Linial  2005  Alon et al.  2008  B˘adoiu et al.  2008 
Gutin et al.  2009). However  we are not aware of any approach in the literature that can faithfully
embed unweighted kNN graphs and comes with performance guarantees. Based on our density
estimate  such an embedding can now easily be constructed. Given the unweighted kNN graph 
we assign edge weights w(Xi  Xj) = (ˆp−1/d(Xi) + ˆp−1/d(Xj))/2 where ˆp is the estimate of the
underlying density. Then the shortest paths in this weighted kNN graph converge to the Euclidean
distances in the underlying space  and standard metric multidimensional scaling can be used to
construct an appropriate embedding. In the limit of n → ∞  this approach is going to recover the
original point embedding up to similarity transformations (translation  rotation or rescaling).

7

Figure 3: Densities and their estimates. Density model in the ﬁrst row: the ﬁrst dimension is sampled
from a mixture of Gaussians  the other dimensions from a uniform distribution. The ﬁgures plot the
ﬁrst dimension of the data points versus the true (black) and estimated (green) density values. From
left to right  they show the case of 1  2  and 10 dimensions  respectively. Second and third row:
2-dimensional densities. The left plots show the true log-density (a Gaussian and a step function) 
the middle plots show the estimated log-density. The right ﬁgures plot the ﬁrst coordinate of the
data points against the true (black) and estimated (green) density values. The black star in the left
plot depicts the anchor point X0 of the integration step.

8 Conclusions

In this paper we show how a density can be estimated from the adjacency matrix of an unweighted 
directed kNN graph  provided the graph is dense enough (kd+2/(n2 logd n) → ∞). In this case  the
information about the underlying density is implicitly contained in unweighted kNN graphs  and 
at least in principle  accessible by machine learning algorithms. However  in most applications  k
is chosen much  much smaller  typically on the order k ≈ log(n). For such sparse graphs  our
density estimate fails because it is dominated by sampling noise that does not disappear as n → ∞.
This raises the question whether this failure is just an artifact of our particular construction or of
our proof  or whether a similar phenomenon is true more generally. If yes  then machine learning
algorithms on sparse unweighted kNN graphs would be highly problematic: If the information about
the underlying density is not present in the graph  it is hard to imagine how machine learning algo-
rithms (for example  spectral clustering) could still be statistically consistent. General lower bounds
proving or disproving these speculations are an interesting open problem.

Acknowledgements

We would like to thank Gabor Lugosi for help with the proof of Theorem 1. This research was
partly supported by the German Research Foundation (grant LU1718/1-1 and Research Unit 1735
”Structural Inference in Statistics: Adaptation and Efﬁciency”).

8

−3−2−1012300.20.40.60.81density  n=2000  k=50  dim=1 −3−2−1012300.20.40.60.811.21.4density  n=2000  k=50  dim=2 −4−202400.511.522.5density  n=2000  k=50  dim=10 −202−2−10123log(p) true  n = 2000  k=50  dim=2   −202−2−10123log(p) estimated  n = 2000  k=50  dim=2   −4−202400.050.10.150.2density  n=2000  k=50  dim=2 0.20.40.60.80.20.40.60.8log(p) true  n = 2000  k=50  dim=2   0.20.40.60.80.20.40.60.8  log(p) estimated  n = 2000  k=50  dim=2 00.20.40.60.8100.511.5density  n=2000  k=50  dim=2 References
S. Agarwal  J. Wills  L. Cayton  G. Lanckriet  D. Kriegman  and S. Belongie. Generalized non-

metric multidimensional scaling. In AISTATS  2007.

M. Alamgir and U. von Luxburg. Shortest path distance in random k-nearest neighbor graphs. In

International Conference on Machine Learning (ICML)  2012.

N. Alon  M. B˘adoiu  E. Demaine  M. Farach-Colton  M. Hajiaghayi  and A. Sidiropoulos. Ordinal
embeddings of minimum relaxation: general properties  trees  and ultrametrics. ACM Transac-
tions on Algorithms  4(4):46  2008.

M. B˘adoiu  E. Demaine  M. Hajiaghayi  A. Sidiropoulos  and M. Zadimoghaddam. Ordinal embed-
ding: approximation algorithms and dimensionality reduction. In Approximation  Randomization
and Combinatorial Optimization. Algorithms and Techniques. Springer  2008.

Y. Bilu and N. Linial. Monotone maps  sphericity and bounded second eigenvalue. Journal of

Combinatorial Theory  Series B  95(2):283–299  2005.

I. Borg and P. Groenen. Modern multidimensional scaling: Theory and applications. Springer 

2005.

D. Burago  Y. Burago  and S. Ivanov. A course in metric geometry. American Mathematical Society 

2001.

G. Gutin  E. Kim  M. Mnich  and A. Yeo. Ordinal embedding relaxations parameterized above tight

lower bound. arXiv preprint arXiv:0907.5427  2009.

K. Jamieson and R. Nowak. Low-dimensional embedding using adaptively selected ordinal data. In

Conference on Communication  Control  and Computing  pages 1077–1084  2011.

B. McFee and G. Lanckriet. Partial order embedding with multiple kernels. In International Con-

ference on Machine Learning (ICML)  2009.

H. Ouyang and A. Gray. Learning dissimilarities by ranking: from SDP to QP. In International

Conference on Machine Learning (ICML)  pages 728–735  2008.

M. Schultz and T. Joachims. Learning a distance metric from relative comparisons.

Information Processing Systems (NIPS)  2004.

In Neural

B. Shaw and T. Jebara. Structure preserving embedding. In International Conference on Machine

Learning (ICML)  2009.

B. Shaw  B. Huang  and T. Jebara. Learning a distance metric from a network. Neural Information

Processing Systems (NIPS)  2011.

R. Shepard. Metric structures in ordinal data. Journal of Mathematical Psychology  3(2):287–315 

1966.

9

,Ulrike Von Luxburg
Morteza Alamgir
Sung Ju Hwang
Leonid Sigal
Megasthenis Asteris
Dimitris Papailiopoulos
Alexandros Dimakis
Hisham Husain
Richard Nock
Robert Williamson