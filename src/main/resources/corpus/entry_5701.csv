2017,Unbiased estimates for linear regression via volume sampling,Given a full rank matrix X with more columns than rows consider the task of estimating the pseudo inverse $X^+$ based  on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie  volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\top}$.  Pseudo inverse plays an important part in solving the linear least squares problem  where we try to predict a label for each column of $X$. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from $X$. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all  column labels.   We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.,Unbiased estimates for linear regression

via volume sampling

Michał Derezi´nski

Department of Computer Science
University of California Santa Cruz

mderezin@ucsc.edu

Manfred K. Warmuth

Department of Computer Science
University of California Santa Cruz

manfred@ucsc.edu

Abstract

Given a full rank matrix X with more columns than rows  consider the task of
estimating the pseudo inverse X+ based on the pseudo inverse of a sampled subset
of columns (of size at least the number of rows). We show that this is possible if
the subset of columns is chosen proportional to the squared volume spanned by the
rows of the chosen submatrix (ie  volume sampling). The resulting estimator is
unbiased and surprisingly the covariance of the estimator also has a closed form: It
equals a speciﬁc factor times X+(cid:62)X+.
Pseudo inverse plays an important part in solving the linear least squares problem 
where we try to predict a label for each column of X. We assume labels are
expensive and we are only given the labels for the small subset of columns we
sample from X. Using our methods we show that the weight vector of the solution
for the sub problem is an unbiased estimator of the optimal solution for the whole
problem based on all column labels.
We believe that these new formulas establish a fundamental connection between
linear least squares and volume sampling. We use our methods to obtain an
algorithm for volume sampling that is faster than state-of-the-art and for obtaining
bounds for the total loss of the estimated least-squares solution on all labeled
columns.

Introduction

1
Let X be a wide full rank matrix with d rows and n columns where n ≥ d. Our
goal is to estimate the pseudo inverse X+ of X based on the pseudo inverse
of a subset of columns. More precisely  we sample a subset S ⊆ {1..n} of s
column indices (where s ≥ d). We let XS be the sub-matrix of the s columns
indexed by S (See Figure 1). Consider a version of X in which all but the
columns of S are zero. This matrix equals XIS where IS is an n-dimensional
diagonal matrix with (IS)ii = 1 if i ∈ S and 0 otherwise.
We assume that the set of s column indices of X is selected proportional to
the squared volume spanned by the rows of submatrix XS  i.e. proportional
to det(XSX(cid:62)
S ) and prove a number of new surprising expectation formulas
for this type of volume sampling  such as
E[(XIS)+] = X+ and E[ (XSX(cid:62)

X+(cid:62)X+.

] =

n − d + 1
s − d + 1

(cid:124)

(cid:123)(cid:122)
(cid:125)
S )−1
(XIS )+(cid:62)(XIS )+

xi

S

XS

X

IS

XIS

X+(cid:62)

(XIS)+(cid:62)

(XS)+(cid:62)

Figure 1: Set S may
not be consecutive.
Note that (XIS)+ has the n × d shape of X+ where the s rows indexed by S contain (XS)+ and
the remaining n − s rows are zero. The expectation of this matrix is X+ even though (XS)+ is

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

clearly not a sub-matrix of X+. In addition to the expectation formulas  our new techniques lead
to an efﬁcient volume sampling procedure which beats the state-of-the-art by a factor of n2 in time
complexity.
Volume sampling is useful in numerous applications  from clustering to matrix approximation  but we
focus on the task of solving linear least squares problems: For an n−dimensional label vector y  let
w∗ = argminw ||X(cid:62)w − y||2 = X+y. Assume the entire design matrix X is known to the learner
but labels are expensive and you want to observe as few of them as possible. Let w∗(S) = (XS)+yS
be the solution to the sub-problem based on labels yS. What is the smallest number of labels s
necessary  for which there is a sampling procedure on sets S of size s st the expected loss of w∗(S)
is at most a constant factor larger than the loss of w∗ that uses all n labels (where the constant is
independent of n)? More precisely  using the short hand L(w) = ||X(cid:62)w − y||2 for the loss on all n
labels  what is the smallest size s such that E[L(w∗(S))] ≤ const L(w∗). This question is a version
of the “minimal coresets” open problem posed in [3].
The size has to be at least d and one can show that randomization is necessary in that any deterministic
algorithm for choosing a set of d columns can suffer loss larger by a factor of n. Also any iid sampling
of S (such as the commonly used leverage scores [8]) requires at least Ω(d log d) examples to achieve
a ﬁnite factor. In this paper however we show that with a size d volume sample  E[L(w∗(S))] =
(d + 1)L(w∗) if X is in general position. Note again that we have equality and not just an upper
bound. Also we can show that the multiplicative factor d + 1 is optimal. We further improve this
factor to 1 +  via repeated volume sampling. Moreover  our expectation formulas imply that when S
is size s ≥ d volume sampled  then w∗(S) is an unbiased estimator for w∗  ie E[w∗(S)] = w∗.

2 Related work

Volume sampling is an extension of a determinantal point process [15]  which has been given a lot of
attention in the literature with many applications to machine learning  including recommendation
systems [10] and clustering [13]. Many exact and approximate methods for efﬁciently generating
samples from this distribution have been proposed [6  14]  making it a useful tool in the design of
randomized algorithms. Most of those methods focus on sampling s ≤ d elements. In this paper 
we study volume sampling sets of size s ≥ d  which has been proposed in [1] and motivated with
applications in graph theory  linear regression  matrix approximation and more. The only known
polynomial time algorithm for size s > d volume sampling was recently proposed in [16] with time
complexity O(n4s). We offer a new algorithm with runtime O((n − s + d)nd)  which is faster by a
factor of at least n2.
The problem of selecting a subset of input vectors for solving a linear regression task has been
extensively studied in statistics literature under the terms optimal design [9] and pool-based active
learning [19]. Various criteria for subset selection have been proposed  like A-optimality and D-
S )−1)  which is combinatorially
optimality. For example  A-optimality seeks to minimize tr((XSX(cid:62)
hard to optimize exactly. We show that for size s volume sampling (for s ≥ d)  E[(XSX(cid:62)
S )−1] =
n−d+1
s−d+1 X+(cid:62)X+ which provides an approximate randomized solution for this task.
A related task has been explored in the ﬁeld of computational geometry  where efﬁcient algorithms
are sought for approximately solving linear regression and matrix approximation [17  5  3]. Here 
multiplicative bounds on the loss of the approximate solution can be achieved via two approaches:
Subsampling the vectors of the design matrix  and sketching the design matrix X and the label vector
y by multiplying both by the same suitably chosen random matrix. Algorithms which use sketching
to generate a smaller design matrix for a given linear regression problem are computationally efﬁcient
[18  5]  but unlike vector subsampling  they require all of the labels from the original problem to
generate the sketch  so they do not apply directly to our setting of using as few labels as possible.
The main competitor to volume sampling for linear regression is iid sampling using the statistical
leverage scores [8]. However we show in this paper that any iid sampling method requires sample
size Ω(d log d) to achieve multiplicative loss bounds. On the other hand  the input vectors obtained
from volume sampling are selected jointly and this makes the chosen subset more informative. We
show that just d volume sampled columns are sufﬁcient to achieve a multiplicative bound. Volume
sampling size s ≤ d has also been used in this line of work by [7  11] for matrix approximation.

2

s contains(cid:0)n

(cid:1) nodes for sets S ⊆ {1..n} of size s. Every node S at level s > d has s directed

3 Unbiased estimators
Let n be an integer dimension. For each subset S ⊆ {1..n} of size s we are given a matrix formula
F(S). Our goal is to sample set S of size s using some sampling process and then develop concise
expressions for ES:|S|=s[F(S)]. Examples of formula classes F(S) will be given below.
We represent the sampling by a directed acyclic graph (dag)  with a single root node corresponding
to the full set {1..n}  Starting from the root  we proceed along the edges of the graph  iteratively
removing elements from the set S. Concretely  consider a dag with levels s = n  n − 1  ...  d. Level
edges to the nodes S − {i} at the next lower level. These edges are labeled with a conditional
probability vector P (S−i|S). The probability of a (directed) path is the product of the probabilities
along its edges. The outﬂow of probability from each node on all but the bottom level is 1. We let the
probability P (S) of node S be the probability of all paths from the top node {1..n} to S and set the
probability P ({1..n}) of the top node to 1. We associate a formula F(S) with each set node S in the
dag. The following key equality lets us compute expectations.
Lemma 1 If for all S ⊆ {1..n} of size greater than d we have

s

P (S−i|S)F(S−i) 

S:|S|=s P (S)F(S) = F({1..n}).

(cid:88)

T :|T|=s−1

(cid:88)
(cid:124)

j /∈T

(cid:123)(cid:122)

P (T )

(cid:125)

(cid:88)
then for any s ∈ {d..n}: ES:|S|=s[F(S)] =(cid:80)
(cid:88)

(cid:88)

(cid:88)

F(S) =

i∈S

P (S) F(S) =

P (S)

S:|S|=s

S:|S|=s

i∈S

Proof Sufﬁces to show that expectations at successive layers are equal:

P (S−i|S) F(S−i) =

P (T+j)P (T|T+j)

F(T ).

3.1 Volume sampling
Given a wide full-rank matrix X ∈ Rd×n and a sample size s ∈ {d..n}  volume sampling chooses
subset S ⊆ {1..n} of size s with probability proportional to volume spanned by the rows of submatrix
XS  ie proportional to det(XSX(cid:62)
S ). The following corollary uses the above dag setup to compute the
normalization constant for this distribution. When s = d  the corollary provides a novel minimalist

proof for the Cauchy-Binet formula:(cid:80)

S:|S|=s det(XSX(cid:62)

S ) = det(XX(cid:62)).

Corollary 2 Let X ∈ Rd×n and S ⊆ {1..n} of size n ≥ s ≥ d st det(XSX(cid:62)
set S of size larger than d and i ∈ S  deﬁne the probability of the edge from S to S−i as:

S ) > 0. Then for any

1−x(cid:62)

S )−1xi

i (XSX(cid:62)
s−d

det(XS−iX(cid:62)
)
(s−d) det(XSX(cid:62)
S )

S−i

=

is a proper probability distribution and thus(cid:80)

P (S−i|S) :=
(reverse iterative volume sampling)
where xi is the ith column of X and XS is the sub matrix of columns indexed by S. Then P (S−i|S)
S:|S|=s P (S) = 1 for all s ∈ {d..n}. Furthermore
det(XSX(cid:62)
S )
s−d

(cid:0)n−d
(cid:1) det(XX(cid:62))

(volume sampling)

P (S) =

 

.

Proof First  for any node S st s > d and det(XSX(cid:62)

S ) > 0  the probabilities out of S sum to 1:

P (S−i|S) =

1 − tr((XSX(cid:62)
s − d

S )−1xix(cid:62)
i )

=

s − tr((XSX(cid:62)
s − d

S )−1XSX(cid:62)
S )

s − d
s − d

=

= 1.

(cid:88)

i∈S

(cid:88)

i∈S

It remains to show the formula for the probability P (S) of all paths ending at node S. Consider
any path from the root {1..n} to S. There are (n − s)! such paths. The fractions of determinants in

3

probabilities along each path telescope1 and the additional factors accumulate to the same product.
So the probability of all paths from the root to S is the same and the total probability into S is

(n − s)!

(n − d)(n − d − 1) . . . (n − s + 1)

det(XSX(cid:62)
S )
det(XX(cid:62))

=

3.2 Expectation formulas for volume sampling

1(cid:0)n−d

s−d

(cid:1) det(XSX(cid:62)

S )
det(XX(cid:62))

.

All expectations in the remainder of the paper are wrt volume sampling. We use the short hand
E[F(S)] for expectation with volume sampling where the size of the sampled set is ﬁxed to s. The
expectation formulas for two choices of F(S) are proven in the next two theorems. By Lemma 1 it

sufﬁces to show F(S) =(cid:80)

i∈S P (S−i|S)F(S−i) for volume sampling.

We introduce a bit more notation ﬁrst. Recall that XS is the sub matrix of columns indexed by
S ⊆ {1..n} (See Figure 1). Consider a version of X in which all but the columns of S are zero. This
matrix equals XIS where IS is an n-dimensional diagonal matrix with (IS)ii = 1 if i ∈ S and 0
otherwise.
Theorem 3 Let X ∈ Rd×n be a wide full rank matrix (ie n ≥ d). For s ∈ {d..n}  let S ⊆ 1..n be a
size s volume sampled set over X. Then

E[(XIS)+] = X+.

We believe that this fundamental formula lies at the core of why volume sampling is important in
many applications. In this work  we focus on its application to linear regression. However  [1]
discuss many problems where controlling the pseudo-inverse of a submatrix is essential. For those
applications  it is important to establish variance bounds for the estimator offered by Theorem 3. In
this case  volume sampling once again offers very concrete guarantees. We obtain them by showing
the following formula  which can be viewed as a second moment for this estimator.
Theorem 4 Let X ∈ Rd×n be a full-rank matrix and s ∈ {d..n}. If size s volume sampling over X
has full support  then

(cid:124)

E[ (XSX(cid:62)

(cid:123)(cid:122)
(cid:125)
S )−1
(XIS )+(cid:62)(XIS )+

n − d + 1
s − d + 1

] =

(cid:124)

(cid:123)(cid:122)

(XX(cid:62))−1
X+(cid:62)X+

(cid:125)

.

If volume sampling does not have full support then the matrix equality “=” is replaced by the
positive-deﬁnite inequality “(cid:22)”.
The condition that size s volume sampling over X has full support is equivalent to det(XSX(cid:62)
S ) > 0
for all S ⊆ 1..n of size s. Note that if size s volume sampling has full support  then size t > s also
has full support. So full support for the smallest size d (often phrased as X being in general position)
implies that volume sampling wrt any size s ≥ d has full support.
Surprisingly by combining theorems 3 and 4  we can obtain a “covariance type formula” for the
pseudo-inverse matrix estimator:

E[((XIS)+ − E[(XIS)+])(cid:62) ((XIS)+ − E[(XIS)+])]
= E[(XIS)+(cid:62)(XIS)+] − E[(XIS)+](cid:62) E[(XIS)+]

=

(1)
Theorem 4 can also be used to obtain an expectation formula for the Frobenius norm (cid:107)(XIS)+(cid:107)F of
the estimator:

X+(cid:62)X+ − X+(cid:62)X+ =

n − s
s − d + 1

X+(cid:62)X+.

n − d + 1
s − d + 1

E(cid:107)(XIS)+(cid:107)2

F = E[tr((XIS)+(cid:62)(XIS)+)] =

(cid:107)X+(cid:107)2
F .

(2)

n − d + 1
s − d + 1

This norm formula has been shown in [1]  with numerous applications. Theorem 4 can be viewed as
a much stronger pre trace version of the norm formula. Also our proof techniques are quite different

1Note that 0

0 determinant ratios are avoided along the path because paths with such ratios always lead to sets

of probability 0 and in the corollary we only consider paths to nodes S for which det(XSXS) > 0.

4

i∈S

(cid:80)
Proof of Theorem 4 Choose F(S) = s−d+1
i∈S P (S−i|S)F(S−i) for volume sampling:
1 − x(cid:62)

i (XSX(cid:62)
s − d

To show this we apply Sherman Morrison to (XS−iX(cid:62)

n−d+1 (XSX(cid:62)
(cid:88)
S )−1 =
(cid:18)

s − d + 1
(((((
n − d + 1

(XSX(cid:62)

i∈S

(cid:88)

(1 − x(cid:62)

i (XSX(cid:62)

i∈S
= (s − d)(XSX(cid:62)

S )−1xi)
(XSX(cid:62)
S )−1 + 
S )−1
xix(cid:62)


i∈S


S )−1 +
(cid:88)

(XSX(cid:62)

S )−1xi

)−1

S−i

(XS−i X(cid:62)

s − d

(((((
n − d + 1
)−1 on the rhs:
S )−1xix(cid:62)
i (XSX(cid:62)
S )−1 = (s − d + 1) (XSX(cid:62)

i (XSX(cid:62)
S )−1xi

S )−1

(cid:19)

S−i
(XSX(cid:62)

1 − x(cid:62)
i (XSX(cid:62)

S )−1.

S )−1xi

  ie:

and much simpler. Note that if size s volume sampling for X does not have full support then (1)
becomes a semi-deﬁnite inequality (cid:22) between matrices and (2) an inequality between numbers.
Proof of Theorem 3 We apply Lemma 1 with F(S) = (XIS)+. It sufﬁces to show F(S) =

(cid:80)
i∈S P (S−i|S)F(S−i) for P (S−i|S) := 1−x(cid:62)
i (XSX(cid:62)
s − d

(XIS)+ =

(cid:88)

1 − x(cid:62)

i (XS X(cid:62)
s−d
S )−1xi

.

i∈S

(cid:124)

(cid:88)

(XIS−i)+
(XIS−i )(cid:62)(XS−i X(cid:62)

Proven by applying Sherman Morrison to (XS−iX(cid:62)

(cid:125)
(cid:123)(cid:122)
)−1
S − xix(cid:62)
i )−1 on the rhs:
i (XSX(cid:62)
(XSX(cid:62)
S )−1
S )−1xi
We now expand the last two factors into 4 terms. The expectation of the ﬁrst (XIS)(cid:62)(XSX(cid:62)
S )−1 is
(XIS)+ (which is the lhs) and the expectations of the remaining three terms times s − d sum to 0:
S )−1

S )−1xix(cid:62)
i (XSX(cid:62)

(cid:88)

((XIS)(cid:62) − eix(cid:62)
i )

i (XSX(cid:62)
s − d

)−1 = (XSX(cid:62)

i (XSX(cid:62)

(XSX(cid:62)

S )−1 +

(1 − x(cid:62)

S )−1xi

1 − x(cid:62)

1 − x(cid:62)

(cid:18)

(cid:19)

S−i

S−i

i (XSX(cid:62)
ei(x(cid:62)

S )−1xi) eix(cid:62)
S )−1xi) x(cid:62)

i (XSX(cid:62)
i (XSX(cid:62)

i (XSX(cid:62)

(XSX(cid:62)

S )−1 + (XIS)(cid:62)
S )−1
xix(cid:62)


i∈S

S )−1 = 0.

−(cid:88)
−(cid:88)

i∈S

.

i

S )−1. By Lemma 1 it sufﬁces to show F(S) =

If some denominators 1−x(cid:62)
are positive. In this case the above matrix equality becomes a positive-deﬁnite inequality (cid:22).

S )−1xi are zero  then only sum over i for which the denominators

i (XSX(cid:62)

4 Linear regression with few labels

Our main motivation for studying volume sampling came
from asking the following simple question. Suppose we
want to solve a d-dimensional linear regression problem
with a matrix X ∈ Rd×n of input column vectors and a
label vector y ∈ Rn  ie ﬁnd w ∈ Rd that minimizes the
least squares loss L(w) = (cid:107)X(cid:62)w − y(cid:107)2:

L(w∗(Si))

L(·)

E[L(w∗(S))]

L(w∗(Sj))

d L(w∗)

L(w∗)

w∗ def= argmin
w∈Rd

L(w) = X+(cid:62)y 

w∗(Si)
Figure 2: Unbiased estimator w∗(S) in ex-
but the access to label vector y is restricted. We are al-
pectation suffers loss (d + 1) L(w∗).
lowed to pick a subset S ⊆ {1..n} for which the labels yi
(where i ∈ S) are revealed to us  and then solve the subproblem (XS  yS)  obtaining w∗(S). What is
the smallest number of labels such that for any X  we can ﬁnd w∗(S) for which L(w∗(S)) is only a
multiplicative factor away from L(w∗) (independent of the number of input vectors n)? This question
was posed as an open problem by [3]. It is easy to show that we need at least d labels (when X is
full-rank)  so as to guarantee the uniqueness of solution w∗(S). We use volume sampling to show
that d labels are in fact sufﬁcient (proof in Section 4.1).

w∗ = E(w∗(S))

w∗(Sj)

5

Theorem 5 If the input matrix X ∈ Rd×n is in general position  then for any label vector y ∈ Rn 
the expected square loss (on all n labeled vectors) of the optimal solution w∗(S) for the subproblem
(XS  yS)  with the d-element set S obtained from volume sampling  is given by

E[L(w∗

(S))] = (d + 1) L(w∗).

If X is not in general position  then the expected loss is upper-bounded by (d + 1) L(w∗).

The factor d + 1 cannot be improved when selecting only d labels (we omit the proof):
Proposition 6 For any d  there exists a least squares problem (X  y) with d + 1 vectors in Rd such
that for every d-element index set S ⊆ {1  ...  d + 1}  we have
(S)) = (d + 1) L(w∗).

L(w∗

Note that the multiplicative factor in Theorem 5 does not depend on n. It is easy to see that this
cannot be achieved by any deterministic algorithm (without the access to labels). Namely  suppose
that d = 1 and X is a vector of all ones  whereas the label vector y is a vector of all ones except for a
single zero. No matter which column index we choose deterministically  if that index corresponds to
the label 0  the solution to the subproblem will incur loss L(w∗(S)) = n L(w∗). The fact that volume
sampling is a joint distribution also plays an essential role in proving Theorem 5. Consider a matrix
X with exactly d unique linearly independent columns (and an arbitrary number of duplicates). Any
iid column sampling distribution (like for example leverage score sampling) will require Ω(d log d)
samples to retrieve all d unique columns (ie coupon collector problem)  which is necessary to get any
multiplicative loss bound.
The exact expectation formula for the least squares loss under volume sampling suggests a deep
connection between linear regression and this distribution. We can use Theorem 3 to further strengthen
that connection. Note  that the least squares estimator obtained through volume sampling can be
written as w∗(S) = (XIS)+(cid:62)y. Applying formula for the expectation of pseudo-inverse  we conclude
that w∗(S) is an unbiased estimator of w∗.
Proposition 7 Let X ∈ Rd×n be a full-rank matrix and n ≥ s ≥ d. Let S ⊆ 1..n be a size s volume
sampled set over X. Then  for arbitrary label vector y ∈ Rn  we have
(S)] = E[(XIS)+(cid:62)y] = X+(cid:62)y = w∗.

E[w∗

For size s = d volume sampling  the fact that E[w∗(S)] equals w∗ can be found in an early paper [2].
They give a direct proof based on Cramer’s rule. For us the above proposition is a direct consequence
of the matrix expectation formula given in Theorem 3 that holds for volume sampling of any size
s ≥ d. In contrast  the loss expectation formula of Theorem 5 is limited to sampling of size s = d.
Bounding the loss expectation for s > d remains an open problem. However  we consider a different
strategy for extending volume sampling in linear regression. Combining Proposition 7 with Theorem
5 we can compute the variance of predictions generated by volume sampling  and obtain tighter
multiplicative loss bounds by sampling multiple d-element subsets S1  ...  St independently.

Theorem 8 Let (X  y) be as in Theorem 5. For k independent size d volume samples S1  ...  Sk 

 1
L
k(cid:88)
Proof Denote(cid:98)y def= X(cid:62)w∗ and(cid:98)y(S)

j=1

E

k

 =

(cid:18)

w∗

(Sj)

(cid:19)

1 +

d
k

L(w∗).

def= X(cid:62)w∗(S) as the predictions generated by w∗ and w∗(S)
respectively. We perform bias-variance decomposition of the loss of w∗(S) (for size d volume
sampling):

(S))] = E[(cid:107)(cid:98)y(S) − y(cid:107)2] = E[(cid:107)(cid:98)y(S) −(cid:98)y +(cid:98)y − y(cid:107)2]
n(cid:88)
E(cid:2)((cid:98)y(S)i − E[(cid:98)y(S)i])2(cid:3) + L(w∗) =

= E[(cid:107)(cid:98)y(S) −(cid:98)y(cid:107)2] + E[2((cid:98)y(S) −(cid:98)y)(cid:62)((cid:98)y − y)] + (cid:107)(cid:98)y − y(cid:107)2
n(cid:88)

(∗)
=

Var[(cid:98)y(S)i] + L(w∗) 

E[L(w∗

i=1

i=1

6

Now the expected loss of the average weight vector wrt sampling k independent sets S1  ...  Sk is:

where (∗) follows from Theorem 3. Now  we use Theorem 5 to obtain the total variance of predictions:

L

 1

k

E

k(cid:88)

j=1

w∗

(Sj)

(S))] − L(w∗) = d L(w∗).

i=1

n(cid:88)
Var[(cid:98)y(S)i] = E[L(w∗
 =
 1
k(cid:88)
n(cid:88)
 k(cid:88)

Var

j=1

i=1

=

k

1
k2

d L(w∗)

j=1

 + L(w∗)
(cid:98)y(Sj)i
 + L(w∗) =
(cid:18)

(cid:19)

d
k

L(w∗).

1 +

It is worth noting that the average weight vector used in Theorem 8 is not expected to perform better
than taking the solution to the joint subproblem  w∗(S1:k)  where S1:k = S1 ∪ ... ∪ Sk. However 
theoretical guarantees for that case are not yet available.

4.1 Proof of Theorem 5

We use the following lemma regarding the leave-one-out loss for linear regression [4]:
Lemma 9 Let w∗(−i) denote the least squares solution for problem (X−i  y−i). Then  we have
i w − yi)2.

L(w∗) = L(w∗

(−i)) − x(cid:62)

= (x(cid:62)

(cid:96)i(w)

def

When X has d + 1 columns and X−i is a full-rank d × d matrix  then L(w∗(−i)) = (cid:96)i(w∗(−i)) and
Lemma 9 leads to the following:

det((cid:101)X(cid:101)X(cid:62))

P (S) (cid:96)j(w∗

(S)) =

P (T−j) (cid:96)j(w∗

(T−j)).

(4)

We now use (3) on the matrix XT and test instance xj (assuming rank(XT−j ) = d):

P (T−j) (cid:96)j(w∗

(5)
Since the summand does not depend on the index j ∈ T   the inner summation in (4) becomes a
multiplication by d + 1. This lets us write the expected loss as:

(T−j)) =

(T−j)) =

.

det(XT−j X(cid:62)
T−j
det(XX(cid:62))

)

(cid:96)j(w∗

det((cid:101)XT(cid:101)X(cid:62)

T )
det(XX(cid:62))

(cid:88)

det((cid:101)XT(cid:101)X(cid:62)

T )

T |T|=d+1

det((cid:101)X(cid:101)X(cid:62))

det(XX(cid:62))

(1)
= (d + 1)

(2)

= (d + 1) L(w∗) 

E[L(w∗

(S))] =

d + 1

det(XX(cid:62))

(6)
where (1) follows from the Cauchy-Binet formula and (2) is an application of the “base × height”
formula. If X is not in general position  then for some summands in (5)  rank(XT−j ) < d and
P (T−j) = 0. Thus the left-hand side of (5) is 0  while the right-hand side is non-negative  so (6)
becomes an inequality  completing the proof of Theorem 5.

7

where

(−i)) 

i (XX(cid:62))−1xi (cid:96)i(w∗
(cid:125)(cid:124)
(cid:123)
(cid:122)
where (cid:101)X =
(cid:107)(cid:98)y − y(cid:107)2
i (XX(cid:62))−1xi)(cid:96)i(w∗
(−i)) 

= det(XX(cid:62))

L(w∗)

(1)

(3)

(2)

(cid:19)

(cid:18) X

y(cid:62)
(−i))

= det(XX(cid:62))(1 − x(cid:62)
−i)(cid:96)i(w∗
= det(X−iX(cid:62)
(cid:88)

P (S)L(w∗

(S)) =

n(cid:88)
(cid:88)

j=1

(cid:96)j(w∗

(S))

S |S|=d

P (S)

(cid:88)

T |T|=d+1

j∈T

E[L(w∗

(S))] =

=

(cid:88)
(cid:88)

S |S|=d

(cid:88)

S |S|=d

j /∈S

(3)
where (1) is the “base × height” formula for volume  (2) follows from Lemma 9 and (3) follows
from a standard determinant formula. Returning to the proof  our goal is to ﬁnd the expected loss
E[L(w∗(S))]  where S is a size d volume sampled set. First  we rewrite the expectation as follows:

5 Efﬁcient algorithm for volume sampling

In this section we propose an algorithm for efﬁciently performing exact volume sampling for any
s ≥ d. This addresses the question posed by [1]  asking for a polynomial-time algorithm for the
case when s > d. [6  11] gave an algorithm for the case when s = d  which runs in time O(nd3).
Recently  [16] offered an algorithm for arbitrary s  which has complexity O(n4s). We propose a
new method  which uses our techniques to achieve the time complexity O((n − s + d)nd)  a direct
improvement over [16] by a factor of at least n2. Our algorithm also offers an improvement for s = d
in certain regimes. Namely  when n = o(d2)  then our algorithm runs in time O(n2d) = o(nd3) 
faster than the method proposed by [6].
Our algorithm implements reverse iterative sampling from Corollary 2. After removing q columns 
we are left with an index set of size n− q that is distributed according to volume sampling for column
set size n − q.
Theorem 10 The sampling algorithm runs in time O((n − s + d)nd)  using O(d2 + n) additional
memory  and returns set S which is distributed according to size s volume sampling over X.

Proof For correctness we show the following invariants that hold at the beginning of the while loop:

pi = 1 − x(cid:62)

i (XSX(cid:62)

S )−1xi = (|S| − d) P (S−i|S)

and

Z = (XSX(cid:62)

S )−1.

At the ﬁrst iteration the invariants trivially hold. When updating the pj we use Z and the pi from the
previous iteration  so we can rewrite the update as

pj ← pj − (x(cid:62)
= 1 − x(cid:62)

= 1 − x(cid:62)

j v)2
j (XSX(cid:62)
(cid:18)
j (XSX(cid:62)

S )−1xj −
S )−1xj − x(cid:62)
S )−1 +

(x(cid:62)
j Zxi)2
i (XSX(cid:62)

1 − x(cid:62)
j (XSX(cid:62)

S )−1xj

S )−1xi
S )−1xix(cid:62)
i (XSX(cid:62)

1 − x(cid:62)
S )−1xix(cid:62)
i (XSX(cid:62)

i (XSX(cid:62)
S )−1xi

i (XSX(cid:62)
S )−1xi
S )−1

(cid:19)

(XSX(cid:62)

Reverse iterative volume sampling

i Zxi

Input: X∈Rd×n  s∈{d..n}
Z ← (XX(cid:62))−1
∀i∈{1..n} pi ← 1 − x(cid:62)
S ← {1  ..  n}
while |S| > s
Sample i ∝ pi out of S
S ← S − {i}
√
v ← Zxi/
pi
∀j∈S pj ← pj − (x(cid:62)
Z ← Z + vv(cid:62)

j

S−i

1 − x(cid:62)

(XSX(cid:62)
j (XS−i X(cid:62)

xj
)−1xj = (|S| − 1 − d) P (S−i j|S−i) 

= 1 − x(cid:62)
(∗)
= 1 − x(cid:62)
where (∗) follows from the Sherman-Morrison formula. The update of Z is also an application of
Sherman-Morrison and this concludes the proof of correctness.
Runtime: Computing the initial Z = (XX(cid:62))−1 takes O(nd2)  as does computing the initial values
of pj’s. Inside the while loop  updating pj’s takes O(|S|d) = O(nd) and updating Z takes O(d2).
The overall runtime becomes O(nd2 + (n − s)nd) = O((n − s + d)nd). The space usage (in
addition to the input data) is dominated by the pi values and matrix Z.

end
return S

j v)2

6 Conclusions
We developed exact formulas for E[(XIS)+)] and E[(XIS)+)2] when the subset S of s column
indices is sampled proportionally to the volume det(XSX(cid:62)
S ). The formulas hold for any ﬁxed size
s ∈ {d..n}. These new expectation formulas imply that the solution w∗(S) for a volume sampled
subproblem of a linear regression problem is unbiased. We also gave a formula relating the loss of the
subproblem to the optimal loss (ie E(L(w∗(S))) = (d + 1)L(w∗)). However  this result only holds
(cid:80)
for sample size s = d. It is an open problem to obtain such an exact expectation formula for s > d.
i Si. We
i w∗(Si)) of the average predictor but it is an

A natural algorithm is to draw k samples Si of size d and return w∗(S1:k)  where S1:k =(cid:83)

were able to get exact expressions for the loss L( 1
open problem to get nontrivial bounds for the loss of the best predictor w∗(S1:k).
k

8

We were able to show that for small sample sizes  volume sampling a set jointly has the advantage: It
achieves a multiplicative bound for the smallest sample size d  whereas any independent sampling
routine requires sample size at least Ω(d log d).
We believe that our results demonstrate a fundamental connection between volume sampling and
linear regression  which demands further exploration. Our loss expectation formula has already been
applied by [12] to the task of linear regression without correspondence.

Acknowledgements Thanks to Daniel Hsu and Wojciech Kotłowski for many valuable discussions.
This research was supported by NSF grant IIS-1619271.

References
[1] Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM

Journal on Matrix Analysis and Applications  34(4):1464–1499  2013.

[2] Aharon Ben-Tal and Marc Teboulle. A geometric property of the least squares solution of linear

equations. Linear Algebra and its Applications  139:165 – 170  1990.

[3] Christos Boutsidis  Petros Drineas  and Malik Magdon-Ismail. Rich coresets for constrained

linear regression. CoRR  abs/1202.3505  2012.

[4] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  New York  NY  USA  2006.

[5] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in
input sparsity time. In Proceedings of the Forty-ﬁfth Annual ACM Symposium on Theory of
Computing  STOC ’13  pages 81–90  New York  NY  USA  2013. ACM.

[6] Amit Deshpande and Luis Rademacher. Efﬁcient volume sampling for row/column subset
selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer
Science  FOCS ’10  pages 329–338  Washington  DC  USA  2010. IEEE Computer Society.

[7] Amit Deshpande  Luis Rademacher  Santosh Vempala  and Grant Wang. Matrix approximation
and projective clustering via volume sampling. In Proceedings of the Seventeenth Annual
ACM-SIAM Symposium on Discrete Algorithm  SODA ’06  pages 1117–1126  Philadelphia  PA 
USA  2006. Society for Industrial and Applied Mathematics.

[8] Petros Drineas  Malik Magdon-Ismail  Michael W. Mahoney  and David P. Woodruff. Fast
approximation of matrix coherence and statistical leverage. J. Mach. Learn. Res.  13(1):3475–
3506  December 2012.

[9] Valeri Vadimovich Fedorov  W.J. Studden  and E.M. Klimko  editors. Theory of optimal

experiments. Probability and mathematical statistics. Academic Press  New York  1972.

[10] Mike Gartrell  Ulrich Paquet  and Noam Koenigstein. Bayesian low-rank determinantal point
processes. In Proceedings of the 10th ACM Conference on Recommender Systems  RecSys ’16 
pages 349–356  New York  NY  USA  2016. ACM.

[11] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix re-
construction. In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete
Algorithms  SODA ’12  pages 1207–1214  Philadelphia  PA  USA  2012. Society for Industrial
and Applied Mathematics.

[12] Daniel Hsu  Kevin Shi  and Xiaorui Sun. Linear regression without correspondence. CoRR 

abs/1705.07048  2017.

[13] Byungkon Kang. Fast determinantal point process sampling with application to clustering. In
Proceedings of the 26th International Conference on Neural Information Processing Systems 
NIPS’13  pages 2319–2327  USA  2013. Curran Associates Inc.

[14] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. In Proceed-
ings of the 28th International Conference on Machine Learning  pages 1193–1200. Omnipress 
2011.

9

[15] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now

Publishers Inc.  Hanover  MA  USA  2012.

[16] C. Li  S. Jegelka  and S. Sra. Column Subset Selection via Polynomial Time Dual Volume

Sampling. ArXiv e-prints  March 2017.

[17] Michael W. Mahoney. Randomized algorithms for matrices and data. Found. Trends Mach.

Learn.  3(2):123–224  February 2011.

[18] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science  FOCS
’06  pages 143–152  Washington  DC  USA  2006. IEEE Computer Society.

[19] Masashi Sugiyama and Shinichi Nakajima. Pool-based active learning in approximate linear

regression. Mach. Learn.  75(3):249–274  June 2009.

10

,Michal Derezinski
Manfred K. Warmuth
Simon Ramstedt
Chris Pal