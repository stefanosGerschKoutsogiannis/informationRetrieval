2012,Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand,In many large economic markets  goods are sold through sequential auctions. Such domains include eBay  online ad auctions  wireless spectrum auctions  and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems  as their preferences for outcomes in one auction often depend on the outcomes of other auctions  and bidders have limited information about factors that drive outcomes  such as other bidders' preferences and past actions.   In this work  we formulate the bidder's problem as one of price prediction (i.e.  learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains  and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown.,Approximating Equilibria in Sequential Auctions with

Incomplete Information and Multi-Unit Demand

Amy Greenwald and Eric Sodomka

Department of Computer Science

Brown University

Providence  RI 02912

{amy sodomka}@cs.brown.edu

Jiacui Li

Department of Applied Math/Economics

Brown University

Providence  RI 02912

jiacui li@alumni.brown.edu

Abstract

In many large economic markets  goods are sold through sequential auctions.
Examples include eBay  online ad auctions  wireless spectrum auctions  and the
Dutch ﬂower auctions. In this paper  we combine methods from game theory and
decision theory to search for approximate equilibria in sequential auction domains 
in which bidders do not know their opponents’ values for goods  bidders only par-
tially observe the actions of their opponents’  and bidders demand multiple goods.
We restrict attention to two-phased strategies: ﬁrst predict (i.e.  learn); second 
optimize. We use best-reply dynamics [4] for prediction (i.e.  to predict other bid-
ders’ strategies)  and then assuming ﬁxed other-bidder strategies  we estimate and
solve the ensuing Markov decision processes (MDP) [18] for optimization. We
exploit auction properties to represent the MDP in a more compact state space 
and we use Monte Carlo simulation to make estimating the MDP tractable. We
show how equilibria found using our search procedure compare to known equilib-
ria for simpler auction domains  and we approximate an equilibrium for a more
complex auction domain where analytical solutions are unknown.

1

Introduction

Decision-making entities  whether they are businesses  governments  or individuals  usually interact
in game-theoretic environments  in which the ﬁnal outcome is intimately tied to the actions taken
by others in the environment. Auctions are examples of such game-theoretic environments with
signiﬁcant economic relevance. Internet advertising  of which a signiﬁcant portion of transactions
take place through online auctions  has had spending increase 24 percent from 2010 to 2011  globally
becoming an $85 billion industry [16]. The FCC has conducted auctions for wireless spectrum since
1994  reaching sales of over $60 billion.1 Perishable commodities such as ﬂowers are often sold via
auction; the Dutch ﬂower auctions had about $5.4 billion in sales in 2011.2
A game-theoretic equilibrium  in which each bidder best responds to the strategies of its opponents 
can be used as a means of prescribing and predicting auction outcomes. Finding equilibria in auc-
tions is potentially valuable to bidders  as they can use the resulting strategies as prescriptions that
guide their decisions  and to auction designers  as they can use the resulting strategies as predictions
for bidder behavior. While a rich literature exists on computing equilibria for relatively simple auc-
tion games [11]  auction theory offers few analytical solutions for real-world auctions. Even existing
computational methods for approximating equilibria quickly become intractable as the number of
bidders and goods  and the complexity of preferences and decisions  increase.

1See http://wireless.fcc.gov/auctions/default.htm?job=auctions_all.
2See http://www.floraholland.com/en/.

1

In this paper  we combine methods from game theory and decision theory to approximate equilibria
in sequential auction domains  in which bidders do not know their opponents’ values for goods 
bidders partially observe the actions of their opponents’  and bidders demand multiple goods. Our
method of searching for equilibria is motivated by the desire to reach strategies that real-world
bidders might actually use. To this end  we consider strategies that consist of two parts: a prediction
(i.e.  learning) phase and an optimization phase. We use best-reply dynamics [4] for prediction (i.e. 
to predict other bidders’ strategies)  and then assuming ﬁxed other-bidder strategies  we estimate
and solve a Markov decision processes (MDP) [18] for optimization. We exploit auction properties
to represent the MDPs in a more compact state space  and we use Monte Carlo simulation to make
estimating the MDPs tractable.

2 Sequential Auctions

1  . . .   bk

ok−1

i

1  . . .   ok

i   given bk.

i ∈ {∅ {k}}  and how much she paid  ck

i | bk) express the probability that player i receives signal ok

We focus on sequential sealed-bid auctions  with a single good being sold at each of K rounds. The
number of bidders n and the order in which goods are sold are assumed to be common knowledge.
i ∈ Bi to the auctioneer. We let
During auction round k  each bidder i submits a private bid bk
n(cid:105) denote the vector of bids submitted by all bidders at round k. The bidder who
bk = (cid:104)bk
submits the highest bid wins and is assigned a cost based on a commonly known payment rule.
i ∈ Oi to each bidder i 
At the end of round k  the auctioneer sends a private (or public) signal ok
which is a tuple specifying information about the auction outcome for round k  such as the winning
bid  the bids of all agents  the winner identities  whether or not a particular agent won the good  or
any combination thereof. Bidders only observe opponents’ bids if those bids are announced by the
auctioneer. Regardless  we assume that bidder i is told at least which set of goods she won in the
i ∈ R. We let ψ(ok | bk) ∈ [0  1] denote
kth round  wk
n(cid:105) given bk  and we let
the probability that the auctioneer sends the bidders signals ok = (cid:104)ok
ψ(ok
An auction history at round k consists of past bids plus all information communicated by the auc-
tioneer though round k − 1. Let hk
)(cid:105) be a possible auction history at
round k as observed by bidder i. Let Hi be the set of all possible auction histories for bidder i.
Each bidder i is endowed with a privately known type θi ∈ Θi  drawn from a commonly known
distribution F   that determines bidder i’s valuations for various bundles of goods. A (behavioral)
strategy σi : Θ × Hi (cid:55)→ (cid:52)Bi for bidder i speciﬁes a distribution over bids for each possible type
and auction history. The set Σi contains all possible strategies.
At the end of the K auction rounds  bidder i’s utility is based on the bundle of goods she won
and the amount she paid for those goods. Let X ⊆ {1  . . .   K} be a possible bundle of goods 
and let v(X; θi) denote a bidder’s valuation for bundle X when its type is θi. No assumptions
are made about the structure of this value function. A bidder’s utility for type θi and history hK
after K auction rounds is simply that bidder’s value for the bundle of goods it won minus its cost:
ui(θi  hK
Given a sequential auction Γ (deﬁned by all of the above)  bidder i’s objective is to choose a strategy
that maximizes its expected utility. But this quantity depends on the actions of other bidders. A
strategy proﬁle (cid:126)σ = (σ1 ···   σN ) = (σi  σ−i) deﬁnes a strategy for each bidder. (Throughout
the paper  subscript i refers to a bidder i while −i refers to all bidders except i.) Let Ui((cid:126)σ) =
E
θi hK

i )] denote bidder i’s expected utility given strategy proﬁle (cid:126)σ.

i ; θi) −(cid:80)K

i )  . . .   (bk−1

i

i ) = v(∪K

i = (cid:104)(b1

i   o1

k=1wk

i .
k=1 ck

i |(cid:126)σ[ui(θi  hK

Deﬁnition 1 (-Bayes-Nash Equilibrium (-BNE)). Given a sequential auction Γ  a strategy proﬁle
(cid:126)σ ∈ Σ is an -Bayes-Nash-equilibrium if Ui((cid:126)σ) +  ≥ Ui(σ(cid:48)

i  σ−i) ∀i ∈ {1  . . .   n}  ∀σ(cid:48)

i ∈ Σi.

In an -Bayes-Nash Equilibrium  each bidder has to come within an additive factor () of best-
responding to its opponent strategies. A Bayes-Nash equilibrium is an -Bayes-Nash equilibrium
where  = 0. In this paper  we explore techniques for ﬁnding -BNE in sequential auctions. We also
explain how to experimentally estimate the so-called -factor of a strategy proﬁle:
Deﬁnition 2 (-Factor). Given a sequential auction Γ  the -factor of strategy proﬁle (cid:126)σ for bidder
i  σ−i) − Ui(σi  σ−i). In words  the -factor measures bidder i’s loss in
i is i((cid:126)σ) = maxσ(cid:48)
expected utility for not playing his part of (cid:126)σ when other bidders are playing their parts.

Ui(σ(cid:48)

i

2

3 Theoretical Results

i ) if s = hK
i

is a history of length K; 0 otherwise}  and transition function T .

As the number of rounds  bidders  possible types  or possible actions in a sequential auction in-
creases  it quickly becomes intractable to ﬁnd equilibria using existing computational methods. Such
real-world intractability is one reason bidders often do not attempt to solve for equilibria  but rather
optimize with respect to predictions about opponent behavior. Building on past work [2  8]  our ﬁrst
contribution is to fully represent the decision problem for a single bidder i in a sequential auction Γ
as a Markov decision process (MDP).
Deﬁnition 3 (Full-history MDP). A full-history MDP Mi(Γ  θi  T ) represents the sequential auction
Γ from bidder i’s perspective  assuming i’s type is θi  with states S = Hi  actions A = Bi  rewards
R(s) = {ui(θi  hK
If bidder types are correlated  bidder i’s type informs its beliefs about opponents’ types and thus
opponents’ predicted behavior. For notational and computational simplicity  we assume that bidder
types are drawn independently  in which case there is one transition function T regardless of bidder
i’s type. We also assume that bidders are symmetric  meaning their types are all drawn from the same
distribution. When bidders are symmetric  we can restrict our attention to symmetric equilibria 
where a single set of full-history MDPs  one per type  is solved on behalf of all bidders.
Deﬁnition 4 (MDP Assessment). An MDP assessment (π  T ) for a sequential auction Γ is a set of
policies {πθi | θi ∈ Θi}  one for each full-history MDP Mi(Γ  θi  T ).
We now explain where the transition function T comes from. At a high level  we deﬁne (symmetric)
induced transition probabilities Induced(π) to be the transition probabilities that result from agent
i using Bayesian updating to infer something about its opponents’ private information  and then
reasoning about its opponents’ subsequent actions  assuming they all follow policy π. The following
example provides some intuition for this process.
Example 1. Consider a ﬁrst-price sequential auction with two rounds  two bidders  two possible
types (“H” and “L”) drawn independently from a uniform prior (i.e.  p(H) = 0.5 and p(L) = 0.5) 
and two possible actions (“high” and “low”). Suppose Bidder 2 is playing the following simple
strategy: if type H: bid “high” with probability .9  and bid “low” with probability .1; if type L: bid
“high” with probability .1  and bid “low” with probability .9.
At round k = 1  from the perspective of Bidder 1  the only uncertainty that exists is about Bidder 2’s
type. Bidder 1’s beliefs about Bidder 2’s type is based solely on the type prior  resulting in beliefs
that Bidder 2 will bid “high” and “low” each with equal probability. Suppose Bidder 1 bids “low”
and loses to Bidder 2  who the auctioneer reports as having bid “high”. At round k = 2  Bidder
1 must update its posterior beliefs about Bidder 2 after observing the given outcome. This is done
using Bayes’ rule to ﬁnd that Bidder 2 is of type “H” with probability 0.9. Based on its policy  in
the subsequent round  the probability Bidder 2 bids “high” is 0.9(0.9) + 0.1(0.1) = 0.82  and the
probability it bids “low” is 0.9(0.1) + 0.1(0.9) = 0.18. Given this bid distribution for Bidder 2 
Bidder 1 can compute her probability of transitioning to various future states for each possible bid.

More formally  denoting sk
deﬁne Pr(sk+1
state sk

i as agent i’s state and action at auction round k  respectively 
i was taken in
i . By twice applying the law of total probability and then noting conditional independencies 

given that action ak

i and ak

| sk

i

Pr(sk+1

i

| sk

i   ak

i ) =

i

| sk

Pr(sk+1

i ) to be the probability of reaching state sk+1
i   ak
(cid:88)
(cid:88)
(cid:88)

i   ak−i) Pr(ak−i | sk

(cid:88)
(cid:88)

(cid:88)
(cid:88)

Pr(sk+1

i   ak
i )

i   ak

i   ak

| sk

ak−i

ak−i

sk−i

θ−i

i

i

i

=

=

(cid:124)

θ−i

| sk

i   ak

Pr(sk+1

i   sk−i  θ−i) Pr(sk−i  θ−i | sk

i   ak−i  sk−i  θ−i) Pr(ak−i | sk
(cid:125) Pr(sk−i  θ−i | sk
(cid:124)
(cid:125) Pr(ak−i | sk−i  θ−i)
(cid:124)
(cid:123)(cid:122)
independent given that agent’s state at round k: Pr(ak−i | sk−i  θ−i) = (cid:81)
(cid:81)

The ﬁrst term in Equation 1 is deﬁned by the auction rules and depends only on the actions taken at
i | ak). The second term is a joint distribution over oppo-
round k: Pr(sk+1
nents’ actions given opponents’ private information. Each agent’s action at round k is conditionally
j   θj) =
j ). The third term is the joint distribution over opponents’ private information 

i   ak−i) = ψ(ok

j(cid:54)=i Pr(ak

j | sk

j | sk

j(cid:54)=i πθj (ak

| sk

i   ak−i)

i   ak

i   ak
i )

i   ak
i )

(cid:123)(cid:122)

(cid:123)(cid:122)

i   ak

ak−i

sk−i

(cid:125)

(1)

i

3

given agent i’s observations. This term can be computed using Bayesian updating. We compute
induced transition probabilities Induced(π)(sk
Deﬁnition 5 (δ-Stable MDP Assessment). An MDP assessment (π  T ) for a sequential auction Γ is
called δ-stable if d(T  Induced(π)) < δ  for some symmetric distance function d.

) using Equation 1.

i   sk+1

i   ak

i

θi hK

i |π T [ui(θi  hK

When δ = 0  the induced transition probabilities exactly equal the transition probabilities from the
MDP assessment (π  T )  meaning that if all agents follow (π  T )  the transition function T is correct.
Deﬁne Ui(π  T ) ≡ E
i )] to be the expected utility for following an MDP assess-
ment’s policy π when the transition function is T . (We abbreviate Ui by U because of symmetry.)
Deﬁnition 6 (α-Optimal MDP Assessment). An MDP assessment (π  T ) for a sequential auction Γ
is called α-optimal if for all policies π(cid:48)  U (π  T ) + α ≥ U (π(cid:48)  T ).
If each agent is playing a 0-optimal (i.e.  optimal) 0-stable (i.e.  stable) MDP assessment for the
sequential auction Γ  each agent is best responding to its beliefs  and each agent’s beliefs are correct.
It follows that any optimal stable MDP assessment for the sequential auction Γ corresponds to
a symmetric Bayes-Nash equilibrium for Γ. Corollary 2 (below) generalizes this observation to
approximate equilibria.3
Suppose we have a black box that tells us the difference in perceived versus actual expected utility
for optimizing with respect to the wrong beliefs: i.e.  the wrong transition function. More precisely 
if we were to give the black box two transition functions T and T (cid:48) that differ by at most δ (i.e. 
d(T  T (cid:48)) < δ)  the black box would return maxπ |U (π  T ) − U (π  T (cid:48))| ≡ D(δ).
Theorem 1. Given such a black box  if (π  T ) is an α-optimal δ-stable MDP assessment for the
sequential auction Γ  then π is a symmetric -Bayes-Nash equilibrium for Γ  where  = 2D(δ) + α.
Proof. Let Tπ = Induced(π)  and let π∗ be such that (π∗  Tπ) is an optimal MDP assessment.

U (π  Tπ) ≥ U (π  T ) − D(δ)

≥ U (π∗  T ) − (α + D(δ))
≥ U (π∗  Tπ) − (α + 2D(δ))

(2)
(3)
(4)

Lines 2 and 4 hold because (π  T ) is δ-stable. Line 3 holds because (π  T ) is α-optimal.
Corollary 2. If (π  T ) is an α-optimal δ-stable MDP assessment for the sequential auction Γ  then
π is a symmetric -Bayes-Nash equilibrium for Γ  where  = 2δK + α.

In particlar  when the distance between other-agent bid predictions and the actual other-agent bids
induced by the actual other-agent policies is less than δ  optimizing agents play a 2δK-BNE.
This corollary follows from the simulation lemma in Kakade et al. [9]  which provides us
then |U (π  T ) −
with a black box.4
)| and
i   ak

U (π  Induced(π))| ≤ δK  where d(T  T (cid:48)) = (cid:80)

if MDP assessment (π  T ) is δ-stable 

) − T (cid:48)(sk

In particular 

i   sk+1

i   sk+1

K is the MDP’s horizon.
Wellman et al. [24] show that  for simultaneous one-shot auctions  optimizing with respect to pre-
dictions about other-agent bids is an -Bayes-Nash equilibrium  where  depends on the distance
between other-agent bid predictions and the actual other-agent bids induced by the actual other-
agent strategies. Corollary 2 is an extension of that result to sequential auctions.

sk+1
i

|T (sk

i   ak

i

i

4 Searching for an -BNE

We now know that an optimal  stable MDP assessment is a BNE  and moreover  a near-optimal 
near-stable MDP assessment is nearly a BNE. Hence  we propose to search for approximate BNE
by searching the space of MDP assessments for any that are nearly optimal and nearly stable.

3Note that this result also generalizes to non-symmetric equilibria: we would calculate a vector of induced
transition probabilities (one per bidder)  given a vector of MDP assessments  (one per bidder)  instead of assum-
ing that each bidder abides by the same assessment. Similarly  stability would need to be deﬁned in terms of a
vector of MDP assessments. We present our theoretical results in terms of symmetric equilibria for notational
simplicity  and because we search for symmetric equilibria in Section 5.

4Slightly adjusted since there is error only in the transition probabilities  not in the rewards.

4

Our search uses an iterative two-step learning process. We ﬁrst ﬁnd a set of optimal policies π with
respect to some transition function T (i.e.  π = Solve MDP(T )) using dynamic programming  as
described by Bellman’s equations [1]. We then update the transition function T to reﬂect what would
happen if all agents followed the new policies π (i.e.  T ∗ = Induced(π)). More precisely 

1. Initiate the search from an arbitrary MDP assessment (π0  T 0)
2. Initialize t = 1 and  = ∞
3. While (t < τ ) and ( > κ)

(a) PREDICT: T t = Induced(πt−1)
(b) OPTIMIZE: for all types θi  πt = Solve MDP(θi  T t)
(c) Calculate  ≡ (πτ ) (deﬁned below)
(d) Increment t

4. Return MDP assessment (πτ   T τ ) and 

This learning process is not guaranteed to converge  so upon termination  it could return an optimal 
δ-stable MDP assessment for some very large δ. However  it has been shown to be successful exper-
imentally in simultaneous auction games [24] and other large games of imperfect information [7].

Monte Carlo Simulations Recall how we deﬁne induced transition functions (Equation 1). In
practice  the Bayesian updating involved in this calculation is intractable. Instead  we employ Monte
Carlo simulations. First  we further simplify Equation 1 using the law of total probability and noting
conditional independencies (Equation 5). Second  we exploit some special structure of sequential
auctions: if nothing but the winning price at each round is revealed  conditional on reaching state sk
i  
the posterior distribution over highest opponent bids is sufﬁcient for computing the probability of
that round’s outcome (Equation 6).5 Third  we simulate N auction trajectories for the given policy
π and multiple draws from the agent’s type distribution  and count the number of times each highest
opponent bid occurs at each state (Equation 7):

Induced(π)(sk

i   ak

i   sk+1

i

InducedN (π)(sk

i   ak

i   sk+1

i

) = Pr(sk+1
= Pr(sk+1

i

i

| sk
| sk

i   max ak−i)Pr(max ak−i | sk
i   ak
i )
i   max ak−i)Pr(max ak−i | sk
i )

i   ak
i   ak

) = ψ(ok

i | max(ak−i)  ak
i )

#(max(ak−i)  sk
i )

#(sk
i )

(5)
(6)

(7)

Solving the MDP As previously stated  we solve the MDPs exactly using dynamic programming 
but we can only do so because we exploit the structure of auctions to reduce the number of states
in each MDP. Recall that we assume symmetry: i.e.  all bidders’ types are drawn from the same
distribution. Under this assumption  when the auctioneer announces that an Bidder j has won an
auction for the ﬁrst time  this provides the same information as if a different Bidder k won an auction
for the ﬁrst time. We thus collapse these two outcomes into the same state. This can greatly decrease
the MDP state space  particularly if the number of players n is larger than the number of auctions
K  as is often the case in competitive markets. In fact  by handling this symmetry  the MDP state
space is the same for any number of players n ≥ K.6 Second  we exploit the property of losing bid
symmetry: if a bidder i loses with a bid of b or a bid of b(cid:48)  its beliefs about its opponents bids are
unchanged  and thus it receives the same reward for placing the same bid at either resulting state.

5A distribution over the next round’s highest opponent bid is only sufﬁcient without the possibility of ties.
In ties can occur  a distribution over the number of opponents placing that highest bid is also needed. In our
experiments  we do not maintain such a distribution; if there is a tie  the agent in question wins with probability
0.5 (i.e.  we assume it tied with only one opponent).

6Even when n < K  the state space can still be signiﬁcantly reduced  since instead of n different possible
winner identities in the kth round  there are only min(n; k + 1). In the extreme case of n = 2  there is no
winner identity symmetry to exploit  since n = k + 1 even in the ﬁrst round.

5

θi hK

Ui(π(cid:48)

i  π−i) − Ui(πi  π−i).

i

i |(cid:126)π[ui(θi  hK

-factor Approximation Deﬁne Ui((cid:126)π) = E
i )] to be bidder i’s expected utility
i when each agent plays its part in the vector of MDP assessment policies (cid:126)π. Following Def-
inition 2  the -factor measures bidder i’s loss in expected utility for not playing his part of (cid:126)π
when other bidders are playing their parts: i((cid:126)π) = maxπ(cid:48)
In fact 
since we are only interested in ﬁnding symmetric equilibria  where (cid:126)π = (π  . . .   π)  we calculate
(π) = maxπ(cid:48) U (π(cid:48)  (cid:126)π−i) − U (π  (cid:126)π−i).
The ﬁrst term in this deﬁnition is the expected utility of the best response  π∗  to (cid:126)π−i. This quan-
tity typically cannot be computed exactly  so instead  we compute a near-best response ˆπ∗
N =
Solve MDP(InducedN (π))  which is optimal with respect to InducedN (π) ≈ Induced(π) 
and then measure the gain in expected utility of deviating from π to ˆπ∗
N .
Further  we approximate expected utility through Monte Carlo simulation. Speciﬁcially  we compute
l=1 u(θl  hl) by sampling (cid:126)θ and simulating (πθ  . . .   πθ) L times  and then averaging
ˆUL((cid:126)π) = 1
L
bidder i’s resulting utilities. Thus  we approximate (π) by ˆ(π) ≈ ˆUL(ˆπ∗
The approximation error in ˆ(π) comes from both imprecision in InducedN (π)  which depends on
the sample size N  and imprecision in the expected utility calculation  which depends on the sample
size L. The latter is O(
L) by the central limit theorem  and can be made arbitrarily small. (In
our experiments  we plot the conﬁdence bounds of this error to make sure it is indeed small.) The
former arises because ˆπ∗
N is not truly optimal with respect to Induced(π)  and goes to zero as N
goes to inﬁnity by standard reinforcement learning results [20]. In practice we make sure that N is
large enough so that this error is negligible.

N   (cid:126)π−i) − ˆUL(π  (cid:126)π−i).

(cid:80)L

√

5 Experimental Results

This section presents the results of running our iterative learning method on three auction mod-
els studied in the economics literature: Katzman [10]  Weber [23]  and Menezes and Monteiro
[14]. These models are all two-round  second-price  sequential auctions7  with continuous valua-
tion spaces; they differ only in their speciﬁc choice of valuations. The authors analytically derive
a symmetric pure strategy equilibrium for each model  which we attempt to re-discover using our
iterative method. After discretizing the valuation space  our method is sufﬁciently general to apply
immediately in all three settings.
Although these particular sequential auctions are all second price  our method applies to sequential
auctions with other rules as well. We picked this format because of the abundance of corresponding
theoretical results and the simplicity of exposition in two-round auctions. It is a dominant strategy to
bid truthfully in a one-shot second-price auction [22]; hence  when comparing policies in two-round
second-price auctions it sufﬁces to compare ﬁrst-round policies only.
Static Experiments We ﬁrst run one iteration of our learning procedure to check whether the
derived equilibria are strict. In other words  we check whether Solve MDP(InducedN (πE)) =
πE  where πE is a (discretized) derived equilibrium strategy. For each of the three models  Figures
1(a)–1(c) compare ﬁrst-round bidding functions of the former (blue) with the latter (green).
Our results indicate that the equilibria derived by Weber and Katzman are indeed strict  while that
by Menezes and Monteiro (MM) is not  since there exists a set of best-responses to the equilibrium
strategy  not a unique best-response. We conﬁrm analytically that the set of bids output by our
learning procedure are best-responses to the theoretical equilibrium  with the upper bound being the
known theoretical equilibrium strategy and the lower bound being the black dotted line.8 To our
knowledge  this instability was previously unknown.
Dynamic Experiments Since MM’s theoretical equilibrium is not strict  we apply our iterative
learning procedure to search for more stable approximate equilibria. Our procedure converges within
a small number of iterations to an -BNE with a small  factor  and the convergence is robust across
different initializations. We chose initial strategies π0 parametrized by p ∈ R+ that bid xp when
the marginal value of winning an additional good is x. By varying the exponent p  we initialize the
learning procedure with bidding strategies whose level of aggressiveness varies.

7Weber’s model can be extended to any number of rounds  but is unit  not multi-unit  demand.
8These analytical derivations are included in supplemental material.

6

(a)

(b)

(c)

Figure 1: Comparison of ﬁrst-round bidding functions of theoretical equilibrium strategies (green) and that of
the best response from one step of the iterative learning procedure initialized with those equilibrium strategies
(blue). (a) Weber. (b) Katzman. (c) MM.

(a)

(b)

(c)

(d)

Figure 2: Convergence properties of the learning procedure in two-round MM model with 3 agents. (a) (b)
evaluates convergence through L1 distance of ﬁrst-round bidding functions; (c) compares the learned best
response (blue) with different learning procedure initializations (green). (d) plots evolution of estimated -
factor for learning dynamics with one speciﬁc initialization; plots for other initializations look very similar.
The bracketed values in the legend give the 99% conﬁdence bound for the -factor in the ﬁnal iteration  which
is estimated using more sample points (N = L = 109) than previous iterations (N = L = 106).

Our iterative learning procedure is not guaranteed to converge. Nonetheless  in this experiment 
our procedure not only converges with different initialization parameters p (Figure 2(a))  but also
converges to the same solution regardless of initial conditions (Figure 2(b)). The distance measure
d(π  π(cid:48)) between two strategies π  π(cid:48) in these ﬁgures is deﬁned as the L1 distance of their respective
ﬁrst-round bidding functions. Furthermore  the more economically meaningful measure of (π) 
measured by ˆ(π)  converges quickly to a negligible factor smaller than 1× 10−4  which is less than
0.01% of the expected bidder proﬁt (Figure 2(d)).
All existing theoretical work on Bayesian sequential auctions with multi-unit demand is conﬁned
to two-round cases due to the increased complexity of additional rounds  but our method removes
this constraint. We extend the two-round MM model into a three-round auction model 9 and apply
our learning procedure. It requires more iterations for our algorithm to converge in this set up  but it
again converges to a rather stable -BNE regardless of initial conditions. The ﬁnal -factor is smaller
than 0.5% of expected bidder proﬁt (Figure 3(d)). Although d(π  π(cid:48)) no longer fully summarizes
strategy differences  it still strongly indicates that the learning procedure converges to very similar
strategies regardless of initial conditions (Figure 3(b)).

6 Related Work

On the theoretical side  Weber [23] derived equilibrium strategies for a basic model in which n
bidders compete in k ﬁrst or second price auctions  but bidders are assumed to have unit demand.
F´evrier [6] and Yao [25] studied a model where n bidders have multi-unit demand  but there are
only two auctions and a bidder’s per-good valuation is the same across the two goods. Liu [13]
and Paes Leme et al. [17] studied models of n bidders with multi-unit demand where bidders have

9This model is described in supplemental material.

7

00.5100.20.40.60.81ValuationBidWeber  4 agents 00.5100.20.40.60.81ValuationBidKatzman  2 agents 00.5100.20.40.60.81Menezes  3 agents ValuationBid24681000.0020.0040.0060.0080.012 round  d(πit πit+1)Iteration  p = 0.5p = 1.0p = 2.024681000.0020.0040.0060.0080.012 round  d(πit πjt)Iteration  p=1.0 − p=0.5p=2.0 − p=0.5p=2.0 − p=1.000.5100.20.40.60.81ValuationBidAfter 20 iterations246810−1−0.500.51x 10−3Iteration2 round epsilonp = 1.0  [−2e−05 7e−05](a)

(b)

(c)

(d)

Figure 3: The same set of graphs as in Figure 2 for three round MM model with 3 agents.

complete information about opponents’ valuations and perfect information about opponents’ past
bids. Syrgkanis and Tardos [21] extended to the case of incomplete information with unit demand.
On the computational side  Rabinovich et al. [19] generalized ﬁctitious play to ﬁnite-action incom-
plete information games and applied their technique to simultaneous second-price auctions with
utilities expressible as linear functions over a one-dimensional type space. Cai and Wurman [3] take
a heuristic approach to ﬁnding equilibria for sequential auctions with incomplete information; oppo-
nent valuations are sampled to create complete information games  which are solved with dynamic
programming and a general game solver  and then aggregated into mixed behavior strategies to form
a policy for the original incomplete information game. Fatima et al. [5] ﬁnd equilibrium bidding
strategies in sequential auctions with incomplete information under various rules of information
revelation after each round. Additional methods of computing equilibria have been developed for
sequential games outside the context of auctions: Ganzfried and Sandholm [7] study the problem of
computing approximate equilibria in the context of poker  and Mostafa and Lesser [15] describe an
anytime algorithm for approximating equilibria in general incomplete information games.
From a decision-theoretic perspective  the bidding problem for sequential auctions was previously
formulated as an MDP in related domains. In Boutilier et al. [2]  an MDP is created where distinct
goods are for sold consecutively  complementarities exist across goods  and the bidder is budget-
constrained. A similar formulation was studied in Greenwald and Boyan [8]  but without budget
constraints. There  purchasing costs were models as negative rewards  signiﬁcantly reducing the
size of the MDP’s state space. Lee et al. [12] represent multi-round games as iterated semi-net-
form games  and then use reinforcement learning techniques to ﬁnd K-level reasoning strategies for
those games. Their experiments are for two-player games with perfect information about opponent
actions  but their approach is not conceptually limited to such models.

7 Conclusion

We presented a two step procedure (predict and optimize) for ﬁnding approximate equilibria in a
class of complex sequential auctions in which bidders have incomplete information about opponents’
types  imperfect information about opponents’ bids  and demand multiple goods. Our procedure is
applicable under numerous pricing rules  allocation rules  and information-revelation policies. We
evaluated our method on models with analytically derived equilibria and on an auction domain in
which analytical solutions were heretofore unknown. Our method was able to both show that the
known equilibrium for one model was not strict and guided our own analytical derivation of the
non-strict set of equilibria. For a more complex auction with no known analytical solutions  our
method converged to an approximate equilibria with an -factor less than 10−4  and did so robustly
with respect to initialization of the learning procedure. While we achieved fast convergence in
the MM model  such convergence is not guaranteed. The fact that our procedure converged to
nearly identical approximate equilibria even from different initializations is promising  and further
exploring convergence properties in this domain is a direction for future work.

Acknowledgements This research was supported by U.S. National Science Foundation Grants
CCF-0905139 and IIS-1217761. The authors (and hence  the paper) beneﬁted from lengthy discus-
sions with Michael Wellman  Michael Littman  and Victor Naroditskiy. Chris Amato also provided
useful insights  and James Tavares contributed to the code development.

8

5101520253000.10.20.30.43 round  d(πit πit+1)Iteration  p = 0.5p = 1.0p = 2.0010203000.10.20.30.43 round  d(πit πjt)Iteration  p=1.0 − p=0.5p=2.0 − p=0.5p=2.0 − p=1.000.5100.20.40.60.81ValuationBidAfter 30 iterations102030−0.1−0.0500.050.1Iteration3 round epsilonp = 1.0  [−4e−05 0.003]References
[1] R. E. Bellman. Dynamic Programming. Princeton University Press  Princeton  NJ  1957.
[2] C. Boutilier  M. Goldszmidt  and B. Sabata. Sequential auctions for the allocation of resources with
complementarities. In International Joint Conference on Artiﬁcial Intelligence  volume 16  pages 527–
534. Lawrence Erlbaum Associates LTD  1999.

[3] G. Cai and P. R. Wurman. Monte Carlo approximation in incomplete information  sequential auction

games. Decision Support Systems  39(2):153–168  Apr. 2005.

[4] A. Cournot. Recherches sur les Principes Mathematics de la Theorie la Richesse. Hachette  1838.
[5] S. S. Fatima  M. Wooldridge  and N. R. Jennings. Sequential Auctions in Uncertain Information Settings.

Agent-Mediated Electronic Commerce and Trading Agent Design and Analysis  pages 16—-29  2009.

[6] P. F´evrier. He who must not be named. Review of Economic Design  8(1):99–1  Aug. 2003.
[7] S. Ganzfried and T. Sandholm. Computing Equilibria in Multiplayer Stochastic Games of Imperfect

Information. International Joint Conference on Artiﬁcial Intelligence  pages 140–146  2009.

[8] A. Greenwald and J. Boyan. Bidding under uncertainty: Theory and experiments. In Twentieth Conference

on Uncertainty in Artiﬁcial Intelligence  pages 209–216  Banff  2004.

[9] S. M. Kakade  M. J. Kearns  and J. Langford. Exploration in metric state spaces. In Proceedings of the

20th International Conference on Machine Learning ICML03  2003.

[10] B. Katzman. A Two Stage Sequential Auction with Multi-Unit Demands . Journal of Economic Theory 

86(1):77–99  May 1999.

[11] P. Klemperer. Auctions: theory and practice. Princeton University Press  2004.
[12] R. Lee  S. Backhaus  J. Bono  W. Dc  D. H. Wolpert  R. Bent  and B. Tracey. Modeling Humans as
Reinforcement Learners : How to Predict Human Behavior in Multi-Stage Games. In NIPS 2011  2011.
[13] Q. Liu. Equilibrium of a sequence of auctions when bidders demand multiple items. Economics Letters 

112(2):192–194  2011.

[14] F. M. Menezes and P. K. Monteiro. Synergies and Price Trends in Sequential Auctions. Review of

Economic Design  8:85–98  2003.

[15] H. Mostafa and V. Lesser. Approximately Solving Sequential Games With Incomplete Information. In
Proceedings of the AAMAS08 Workshop on Multi-Agent Sequential Decision Making in Uncertain Multi-
Agent Domains  pages 92–106  2008.

[16] Nielsen Company. Nielsen’s quarterly global adview pulse report  2011.
[17] R. Paes Leme  V. Syrgkanis  and E. Tardos. Sequential Auctions and Externalities. In Proceedings of the

Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms  pages 869–886  2012.

[18] M. Puterman. Markov decision processes: discrete stochastic dynamic programming. Wiley  1994.
[19] Z. Rabinovich  V. Naroditskiy  E. H. Gerding  and N. R. Jennings. Computing pure Bayesian Nash
equilibria in games with ﬁnite actions and continuous types. Technical report  University of Southampton 
2011.

[20] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction  volume 9 of Adaptive computa-

tion and machine learning. MIT Press  1998.

[21] V. Syrgkanis and E. Tardos. Bayesian sequential auctions. In Proceedings of the 13th ACM Conference

on Electronic Commerce  pages 929–944. ACM  2012.

[22] W. Vickrey. Counterspeculation  Auctions  and Competitive Sealed Tenders. Journal of Finance  16(1):

8–37  1961.

[23] R. J. Weber. Multiple-Object Auctions. In R. Engelbrecht-Wiggans  R. M. Stark  and M. Shubik  editors 

Competitive Bidding  Auctions  and Procurement  pages 165–191. New York University Press  1983.

[24] M. Wellman  E. Sodomka  and A. Greenwald. Self-conﬁrming price prediction strategies for simultaneous

one-shot auctions. In The Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2012.

[25] Z. Yao. Sequential First-Price Auctions with Multi-Unit Demand. Technical report  Discussion paper 

UCLA  2007.

9

,Min Xu
Tao Qin
Tie-Yan Liu
Kihyuk Sohn
Wenling Shang
Honglak Lee