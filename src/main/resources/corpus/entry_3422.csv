2019,Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition,This paper studies the task of one-shot fine-grained recognition  which suffers from the problem of data scarcity of novel fine-grained classes. To alleviate this problem  a off-the-shelf image generator can be applied to synthesize additional images to help one-shot learning. However  such synthesized images may not be helpful in one-shot fine-grained recognition  due to a large domain discrepancy between synthesized and original images. To this end  this paper proposes a meta-learning framework to reinforce the generated images by original images so that these images can facilitate one-shot learning. Specifically  the generic image generator is updated by few training instances of novel classes; and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner  and our experiments demonstrate consistent improvement over baseline on one-shot fine-grained image classification benchmarks.,Meta-Reinforced Synthetic Data for One-Shot

Fine-Grained Visual Recognition

Satoshi Tsutsui
Indiana University

USA

Yanwei Fu⇤

Fudan University

China

stsutsui@indiana.edu

yanweifu@fudan.edu.cn

djcran@indiana.edu

David Crandall
Indiana University

USA

Abstract

One-shot ﬁne-grained visual recognition often suffers from the problem of training
data scarcity for new ﬁne-grained classes. To alleviate this problem  an off-the-shelf
image generator can be applied to synthesize additional training images  but these
synthesized images are often not helpful for actually improving the accuracy of
one-shot ﬁne-grained recognition. This paper proposes a meta-learning framework
to combine generated images with original images  so that the resulting “hybrid”
training images can improve one-shot learning. Speciﬁcally  the generic image
generator is updated by a few training instances of novel classes  and a Meta Image
Reinforcing Network (MetaIRNet) is proposed to conduct one-shot ﬁne-grained
recognition as well as image reinforcement. The model is trained in an end-to-end
manner  and our experiments demonstrate consistent improvement over baselines
on one-shot ﬁne-grained image classiﬁcation benchmarks.

1

Introduction

The availability of vast labeled datasets has been crucial for the recent success of deep learning.
However  there will always be learning tasks for which labeled data is sparse. Fine-grained visual
recognition is one typical example: when images are to be classiﬁed into many very speciﬁc categories
(such as species of birds)  it may be difﬁcult to obtain training examples for rare classes  and producing
the ground truth labels may require signiﬁcant expertise (e.g.  ornithologists). One-shot learning is
thus very desirable for ﬁne-grained visual recognition.
A recent approach to address data scarcity is meta-learning [7 10 24 35]  which trains a parameterized
function called a meta-learner that maps labeled training sets to classiﬁers. The meta-learner is trained
by sampling small training and test sets from a large dataset of a base class. Such a meta-learned
model can be adapted to recognize novel categories with a single training instance per class. Another
way to address data scarcity is to synthesize additional training examples  for example by using
off-the-shelf Generative Adversarial Networks (GANs) [3  13]. However  classiﬁers trained from
GAN-generated images are typically inferior to those trained with real images  possibly because the
distribution of generated images may be biased towards frequent patterns (modes) of the original
image distribution [26]. This is especially true in one-shot ﬁne-grained recognition where a tiny
difference (e.g.  beak of a bird) can make a large difference in class.
To address these issues  we develop an approach to apply off-the-shelf generative models to synthesize
training data in a way that improves one-shot ﬁne-grained classiﬁers. We begin by conducting a
pilot study to transfer a generator pre-trained on ImageNet in a one-shot scenario. We show that
the generated images can indeed improve the performance of a one-shot classiﬁer when used with

⇤Y. Fu is with School of Data Science  and MOE Frontiers Center for Brain Science  Shanghai Key Lab of

Intelligent Information Processing  Fudan University.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

a carefully-designed rule to combine the generated images with the originals. Based on these
preliminary results  we propose a meta-learning approach to learn these rules to reinforce the
generated images effectively for few-shot classiﬁcation.
Our approach has two steps. First  an off-the-shelf generator trained from ImageNet is updated
towards the domain of novel classes by using only a single image (Sec. 4.1). Second  since previous
work and our pilot study (Sec. 3) suggest that simply adding synthesized images to the training data
may not improve one-shot learning  the synthesized images are “mixed” with the original images in
order to bridge the domain gap between the two (Sec. 4.2). The effective mixing strategy is learned
by a meta-learner  which essentially boosts the performance of ﬁne-grained categorization with a
single training instance per class. Lastly  we experimentally validate that our approach can achieve
improved performance over baselines on ﬁne-grained classiﬁcation datasets in one-shot situations
(Sec. 5).
To summarize  the contributions of this paper are: (1) a method to transfer a pre-trained generator
with a single image  (2) a method to learn to complement real images with synthetic images in a way
that beneﬁts one-shot classiﬁers  and (3) to experimentally demonstrate that these methods improve
one-shot classiﬁcation accuracy on ﬁne-grained visual recognition benchmarks.

2 Related Work

Image Generation. Learning to generate realistic images has many potential applications  but is
challenging with typical supervised learning. Supervised learning minimizes a loss function between
the predicted output and the desired output but  for image generation  it is not easy to design such
a perceptually-meaningful loss between images. Generative Adversarial Networks (GANs) [13]
address this issue by learning not only a generator but also a loss function — the discriminator — that
helps the generator to synthesize images indistinguishable from real ones. This adversarial learning
is intuitive but is known to often be unstable [14] in practice. Recent progress includes better CNN
architectures [3  21]  training stabilization tips [2  14  19]  and interesting applications (e.g. [38]).
In particular  BigGAN [3] trained on ImageNet has shown visually impressive generated images
with stable performance on generic image generation tasks. Several studies [20  33] have explored
generating images from few examples  but their focus has not been on one shot classiﬁcation. Several
papers [8  9  20] use the idea of adjusting batch normalization layers  which helped inspire our work.
Finally  some work has investigated using GANs to help image classiﬁcation [1  12  26  27  37]; our
work differs in that we apply an off-the-shelf generator pre-trained from a large and generic dataset.
Few-shot Meta-learning. Few shot classiﬁcation [4] is a sub-ﬁeld of meta-learning (or “learning-to-
learn”) problems  in which the task is to train a classiﬁer with only a few examples per class. Unlike
the typical classiﬁcation setup  in few-shot classiﬁcation the labels in the training and test sets have
no overlapping categories. Moreover  the model is trained and evaluated by sampling many few-shot
tasks (or episodes). For example  when training a dog breed classiﬁer  an episode might train to
recognize ﬁve dog species with only a single training image per class — a 5-way-1-shot setting. A
meta-learning method trains a meta-model by sampling many episodes from training classes and is
evaluated by sampling many episodes from other unseen classes. With this episodic training  we can
choose several possible approaches to learn to learn. For example  “learning to compare” methods
learn a metric space (e.g.  [28 29 31])  while other approaches learn to ﬁne-tune (e.g.  [10 11 22 23])
or learn to augment data (e.g.  [6  12  15  25  34]). An advantage of the latter type is that  since it
is data augmentation  we can use it in combination with any other approaches. Our approach also
explores data augmentation by mixing the original images with synthesized images produced by a
ﬁne-tuned generator  but we ﬁnd that the naive approach of simply adding GAN generated images to
the training dataset does not improve performance. But by carefully combining generated images with
the original images  we ﬁnd that we can effectively synthesize examples that contribute to increasing
the performance. Thus meta-learning is employed to learn the proper combination strategy.

3 Pilot Study

To explain how we arrived at our approach  we describe some initial experimentation which motivated
the development of our methods.

2

Table 1: CUB 5-way-1-shot classiﬁcation accuracy (%) using ImageNet features. Simply adding
generated images to the training set does not help  but adding hybrid images  as in Fig. 1 (h)  can.

Training Data
Original
Original + Generated
Original + Mixed

Nearest Neighbor Logistic Regression

Softmax Regression

69.6
70.1
70.6

75.0
74.6
75.5

74.1
73.8
74.8

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 1: Samples described in Sec. 3. (a) Original image. (b) Result of tuning noise only. (c) Result
of tuning the whole network. (d) Result of tuning batch norm only. (e) Result of tuning batch norm
with perceptual loss. (f) Result of slightly disturbing noise from (e). (g) a 3 ⇥ 3 block weight matrix
w. (g) Result of mixing (a) and (f) as w⇥(f) + (1  w)⇥(a).

How can we transfer generative knowledge from pre-trained GANs? We aim to quickly gener-
ate training images for few-shot classiﬁcation. Performing adversarial learning (i.e. training generator
and discriminator initializing with pre-trained weights) is not practical when we only have one or two
examples per class. Instead  we want to develop a method that does not depend on the number of
images at all; in fact  we consider the extreme case where only a single image is available  and want
to generate variants of the image using a pre-trained GAN. We tried ﬁxing the generator weights and
optimizing the noise so that it generates the target image  under the assumption that sightly modifying
the optimized noise would produce a variant of the original. However  naively implementing this idea
with BigGAN did not reconstruct the image well  as shown in the sample in Fig. 1(b). We then tried
ﬁne-tuning the generator weights also  but this produced even worse images stuck in a local minima 
as shown in Fig. 1(c).
We speculate that the best approach may be somewhere between the two extremes of tuning noise
only and tuning both noise and weights. Inspired by previous work [8  9  20]  we propose to ﬁne-tune
only scale and shift parameters in the batch normalization layers. This strategy produces better
images as shown in Fig. 1(d). Finally  again inspired by previous work [20]  we not only minimize
the pixel-level distance but also the distance of a pre-trained CNN representation (i.e. perceptual
loss [17])  and we show the slightly improved results in Fig. 1(e). We can also generate slightly
different versions by adding random perturbations to the tuned noise (e.g.  the “fattened” version of
the same bird in Fig. 1(f)). The entire training process needs fewer than 500 iterations and takes less
than 20 seconds on an NVidia Titan Xp GPU. We explain the resulting generation strategy developed
based on this pilot study in Sec. 4.

Are generated images helpful for few shot learning? Our goal is not to generate images  but
to augment the training data for few shot learning. A naive way to do this is to apply the above
generation technique for each training image  in order to double the training set. We tested this idea on
a validation set (split the same as [4]) from the Caltech-UCSD bird dataset [32] and computed average
accuracy on 100 episodes of 5-way-1-shot classiﬁcation. We used pre-trained ImageNet features
from ResNet18 [16] with nearest neighbor  one-vs-all logistic regression  and softmax regression
(or multi-class logistic regression). As shown in Table 1  the accuracy actually drops for two of the
three classiﬁers when we double the size of our training set by generating synthetic training images 
suggesting that the generated images are harmful for training classiﬁers.
What is the proper way of synthesizing images to help few-shot learning? Given that the syn-
thetic images appear meaningful to humans  we conjecture that they can beneﬁt few shot classiﬁcation
when properly mixed with originals to create hybrid images. To empirically test this hypothesis  we
devised a random 3 ⇥ 3 grid to combine the images. As shown in Fig. 1(h)  images (a) and (f) were
combined by taking a linear combination within each cell of the grid of (g). Finally  we added mixed
images like (h) into the training data  and discovered that this produced a modest increase in accuracy
(last row of Table 1). While the increase is marginal  these mixing weights were binary and manually

3

Augmented Support Set = {(Original Images)  (Fused Images) }

Original Image

Fused Image

z
Noise

G

F

image fusion
network

Query Set

Generated Image

P

prediction

C
one-shot
classification
network

Figure 2: Our Meta Image Reinforcing Network (MetaIRNet) has two modules: an image fusion
network  and a one-shot classiﬁcation network. The image fusion network reinforces generated
images to try to make them beneﬁcial for the one-shot classiﬁer  while the one-shot classiﬁer learns
representations that are suitable to classify unseen examples with few examples. Both networks are
trained end-to-end  so the loss back-propagates from classiﬁer to the fusion network.

selected  and thus likely not optimal. In Sec. 4.2  we show how to learn this mixing strategy in an
end-to-end manner using a meta-learning framework.

4 Method

The results of the pilot study in the last section suggested that producing synthetic images could be
useful for few-shot ﬁne-grained recognition  but only if it is done in a careful way. In this section 
we use these ﬁndings to propose a novel technique for doing this effectively. We propose a GAN
ﬁne-tuning method that works with a single image (Sec. 4.1)  and an effective meta-learning method
to not only learn to classify with few examples  but also to learn to effectively reinforce the generated
images (Sec. 4.2).

4.1 Fine-tuning Pre-trained Generator for Target Images
GANs typically have a generator G and a discriminator D. Given an input signal z ⇠N (0  1) 
a well-trained generator synthesizes an image G(z). In our tasks  we adapt an off-the-shelf GAN
generator G that is pre-trained on the ImageNet-2012 dataset in order to generate more images in a
target  data-scarce domain. Note that we do not use the discriminator  since adversarial training with a
few images is unstable and may lead to model collapse. Formally  we ﬁne-tune z and the generator G
such that G generates an image Iz from an input vector z by minimizing the distance between G(z)
and Iz  where the vector z is randomly initialized. Inspired by previous work [2  5  20]  we minimize
a loss function LG with L1 distance and perceptual loss Lperc with earth mover regularization LEM 
(1)
where LEM is an earth mover distance between z and random noise r ⇠N (0  1) to regularize z to
be sampled from a Gaussian  and p and z are coefﬁcients of each term.
Since only a few training images are available in the target domain  only scale and shift parameters of
the batch normalization of G are updated in practice. Speciﬁcally  only the  and  of each batch
normalization layer are updated in each layer 
x  E(x)
pVar(x) + ✏

LG (G  Iz  z) = L1 (G(z)  Iz) + pLperc (G(z)  Iz) + zLEM (z  r)  

(2)

ˆx =

h =  ˆx +  

4

where x is the input feature from the previous layer  and E and Var indicate the mean and variance
functions  respectively. Intuitively and in principle  updating  and  only is equivalent to adjusting
the activation of each neuron in a layer. Once updated  the G(z) would be synthesized to reconstruct
the image Iz. Empirically  a small random perturbation ✏ is added to z as G (z + ✏). Examples of Iz 
G(z) and G (z + ✏) are illustrated in in Fig. 1 (a)  (e)  and (f)  respectively.

4.2 Meta Reinforced Synthetic Data for Few-shot Learning

We propose a meta-learning method to add synthetic data to the originals.
One-shot Learning. One-shot classiﬁcation is a meta-learning problem that divide a dataset into
two sets: meta-training (or base) set and meta-testing (or novel) set. The classes in the base
set and the novel sets are disjoint. In other words  we have Dbase = {(Ii  yi)   yi 2C base} and
Dnovel = {(Ii  yi)   yi 2C novel} where Cbase [C novel = ;. The task is to train a classiﬁer on Dbase
that can quickly generalize to unseen classes in Cnovel with one or few examples. To do this  a
meta-learning algorithm performs meta-training by sampling many one-shot tasks from Dbase  and
is evaluated by sampling many similar tasks from Dnovel. Each sampled task (called an episode) is
an n-way-m-shot classiﬁcation problem with q queries  meaning that we sample n classes with m
training and q test examples for each class. In other words  an episode has a support (or training) set
S and a query (or test) set Q  where |S| = n ⇥ m and |Q| = n ⇥ q. One-shot learning means m = 1.
The notation Sc means the support examples only belong to the class c  so |Sc| = m.
Meta Image Reinforcing Network (MetaIRNet). We propose a Meta Image Reinforcing Network
(MetaIRNet)  which not only learns a few-shot classiﬁer  but also learns to reinforce generated images
by combining real and generated images. MetaIRNet is composed of two modules: an image fusion
network F   and a one-shot classiﬁcation network C.
The Image Fusion Network F combines a real image I and a corresponding generated image Ig into
a new image Isyn = F (I  Ig) that is beneﬁcial for training a one-shot classiﬁer. Among the many
possible ways to synthesize images  we were inspired by a block augmentation method [6] and use
grid-based linear combination. As shown in Figure 1(g)  we divide the images into a 3 ⇥ 3 grid and
linearly combine the cells with the weights w produced by a CNN conditioned on the two images.
That is 

Isyn = w  I + (1  w)  Ig

(3)
where  is element-wise multiplication  and w is resized to the image size keeping the block structure.
The CNN to produce w extracts the feature vectors of I and Ig  concatenates them  and uses a fully-
connected layer to produce a weight corresponding to each of the nine cells in the 3 ⇥ 3 grid. Finally 
for each real image Ii  we generate naug images  producing naug synthetic images  and assign the
same class label yi to each synthesized image Ii j

syn to obtain an augmented support set 

˜S =nIi  yi  Ii j

syn  yi naug

j=1 on⇥m

.

(4)

i=1

The One-Shot Classiﬁcation Network C maps an input image I into feature maps C (I)  and performs
one-shot classiﬁcation. Although any one-shot classiﬁer can be used  we choose the non-parametric
prototype classiﬁer of Snell et al. [28] due to its superior performance and simplicity. During each
episode  given the sampled S and Q  the image fusion network produces an augmented support set ˜S.
This classiﬁer computes the prototype vector pc for each class c in ˜S as an average feature vector 

pc =

1

| ˜Sc| X(Ii yi)2 ˜Sc

C (Ii) .

For a query image Ii 2 Q  the probability of belonging to a class c is estimated as 

(5)

(6)

P (yi = c|Ii) =

exp (kC (Ii)  pck)
k=1 exp (kC (Ii)  pkk)

Pn

where k·k is the Euclidean distance. Then  for a query image  the class with the highest probability
becomes the ﬁnal prediction of the one-shot classiﬁer.

5

Training In the meta-training phase  we jointly train F and C in an end-to-end manner  minimizing
a cross-entropy loss function 

where ✓F and ✓C are the learnable parameters of F and C.

1

|Q| X(Ii yi)2Q

min
✓F  ✓C

logP (yi | Ii)  

(7)

5 Experiments

To investigate the effectiveness of our approach  we perform 1-shot-5-way classiﬁcation following
the meta-learning experimental setup described in Sec. 4.2. We perform 1000 episodes in meta-
testing  with 16 query images per class per episode  and report average classiﬁcation accuracy and
95% conﬁdence intervals. We use the ﬁne-grained classiﬁcation dataset of Caltech UCSD Birds
(CUB) [32] for our main experiments  and another ﬁne-grained dataset of North American Birds
(NAB) [30] for secondary experiments. CUB has 11 788 images with 200 classes  and NAB has
48 527 images with 555 classes.

Implementation Details

5.1
While our ﬁne-tuning method introduced in Sec. 4.1 can generate images for each step in meta-
training and meta-testing  it takes around 20 seconds per image  so we apply the generation method
ahead of time to make our experiments more efﬁcient. We use a BigGAN pre-trained on ImageNet 
using the publicly-available weights. We set p = 0.1 and z = 0.1  and perform 500 gradient
descent updates with the Adam [18] optimizer with learning rate 0.01 for z and 0.0005 for the
fully connected layers  to produce scale and shift parameters of the batch normalization layers. We
manually chose these hyper-parameters by trying random values from 0.1 to 0.0001 and visually
checking the quality of a few generated images. We only train once for each image  generate 10
random images by perturbing z  and randomly use one of them for each episode (naug = 1). For
image classiﬁcation  we use ResNet18 [16] pre-trained on ImageNet for the two CNNs in F and one
in C. We train F and C with Adam with a default learning rate of 0.001. We select the best model
based on the validation accuracy  and then compute the ﬁnal accuracy on the test set. For CUB  we
use the same train/val/test split used in previous work [4]  and for NAB we randomly split with a
proportion of train:val:test = 2:1:1; see supplementary material for details. Further implementation
details are available as supplemental source code.2

5.2 Comparative and Ablative Study on CUB dataset
Baselines. We compare our MetaIRNet with three types of baselines. (1) Non-meta learning
classiﬁers: We directly train the same ImageNet pre-trained CNN used in F to classify images in
Dbase  and use it as a feature extractor for Dnovel. We then use off-the-shelf classiﬁers nearest
neighbor  logistic regression (one-vs-all classiﬁer)  and softmax regression (also called multi-class
logistic regression). (2) Meta-learning classiﬁers: We try the meta-learning method of prototypical
network (ProtoNet [28]). ProtoNet computes an average prototype vector for each class and performs
nearest neighbor with the prototypes. We note that our MetaIRNet adapts ProtoNet as a choice of
F so this is an ablative version of our model (MetaIRNet without the image fusion module). (3)
Data augmentation: Because our MetaIRNet learns data-augmentation as a sub-module  we also
compare with three data augmentation strategies  Flip  Gaussian  and FinetuneGAN. Flip horizontally
ﬂips the images. Gaussian adds Gaussian noise with standard deviation 0.01 into the CNN features.
FinetuneGAN (introduced in Sec. 4.1) generates augmented images by ﬁne-tuning the ImageNet-
pretrained BigGAN with each support set. Note that we do these augmentations in the meta-testing
stage to increase the support set. For fair comparison  we use ProtoNet as the base classiﬁer of these
data augmentation baselines.

Results. As shown in Table 2  our MetaIRNet is superior to all baselines including the meta-learning
classiﬁer of ProtoNet (84.13% vs. 81.73%) on the CUB dataset. It is notable that while ProtoNet has
worse accuracy when simply using the generated images as data augmentation  our method shows an

2http://vision.soic.indiana.edu/metairnet/

6

Table 2: 5-way-1-shot accuracy (%) on CUB/NAB dataset with ImageNet pre-trained ResNet18

Method
Nearest Neighbor
Logistic Regression
Softmax Regression
ProtoNet
ProtoNet
ProtoNet
ProtoNet
MetaIRNet (Ours)
MetaIRNet (Ours)

Data Augmentation
-
-
-
-
FinetuneGAN
Flip
Gaussian
FinetuneGAN
FinetuneGAN  Flip

CUB Acc.
79.00 ± 0.62
81.17 ± 0.60
80.77 ± 0.60
81.73 ± 0.63
79.40 ± 0.69
82.66 ± 0.61
81.75 ± 0.63
84.13 ± 0.58
84.80 ± 0.56

NAB Acc.
80.58 ± 0.59
82.70 ± 0.57
82.38 ± 0.57
87.91 ± 0.52
85.40 ± 0.59
88.55 ± 0.50
87.90 ± 0.52
89.19 ± 0.51
89.57 ± 0.49

Table 3: 5-way-1-shot accuracy (%) on CUB dataset with Conv4 without ImageNet pre-training

MetaIRNet
65.86 ± 0.72

ProtoNet [28] MatchingNet [31]
61.16 ± 0.89 [4]
63.50 ± 0.70

MAML [10]
55.92 ± 0.95 [4]

RelationNet [29]
62.45 ± 0.98 [4]

accuracy increase from ProtoNet  which is equivalent to MetaIRNet without the image fusion module.
This indicates that our image fusion module can effectively complement the original images while
removing harmful elements from generated ones.
Interestingly  horizontal ﬂip augmentation yields nearly a 1% accuracy increase for ProtoNet. Because
ﬂipping augmentation cannot be learned directly by our method  we conjectured that our method
could also beneﬁt from it. The ﬁnal line of the table shows an additional experiment with our
MetaIRNet combined with random ﬂip augmentation  showing an additional accuracy increase from
84.13% to 84.80%. This suggests that our method provides an improvement that is orthogonal to ﬂip
augmentation.

Case Studies. We show some sample visualizations in Fig. 4. We ob-
serve that image generation often works well  but sometimes completely
fails. An advantage of our technique is that even in these failure cases 
our fused images often maintain some of the object’s shape  even if the
images themselves do not look realistic. In order to investigate the quality
of generated images in more detail  we randomly pick two classes  sample
100 images for each class  and a show t-SNE visualization of real images
(•)  generated images (N)  and augmented fused images (+) in Fig. 3 
with classes shown in red and blue. It is reasonable that the generated
images are closer to the real ones  because our loss function (equation 1)
encourages this to be so. Interestingly  perhaps due to artifacts of 3 ⇥ 3
patches  the fused images are distinctive from the real/generated images 
extending the decision boundary.

Figure 3:

t-SNE plot

Comparing with state-of-the-art meta-learning classiﬁers.
It is a convention in the machine
learning community to compare any new technique with the performance of many state-of-the-art
methods reported in the literature. This is somewhat difﬁcult for us to do fairly  however: we use
ImageNet-pre-trained features as a starting point (which is a natural design decision considering that
our focus is how to use ImageNet pre-trained generators for improving ﬁne-grained one-shot classiﬁ-
cation)  but much of the one/few-shot learning literature focuses on algorithmic improvements and
thus trains from scratch (often with non-ﬁne-grained datasets). The Delta Encoder [25]  which uses
the idea of learning data augmentation in the feature space  reports 82.2% on one-shot classiﬁcation
on the CUB dataset with ImageNet-pre-trained features  but this is an average of only 10 episodes.
To provide more stable comparison  we cite a benchmark study [4] reporting accuracy of other
meta-learners [10  29  31] on the CUB dataset with 600 episodes. To compare with these scores 
we experimented with our MetaIRNet and the ProtoNet baseline using the same four-layered CNN.
As shown in Table 3  our MetaIRNet performs better than the other methods with more than 2%

7

Original Generated

Fused

Weight

Original Generated

Fused

Weight

Figure 4: Samples of original image  generated image  fused image  and mixing weight w. Higher
weight (red) means more original image used  and lower weight (blue) means more generated image
used. We show three types of samples based on the quality of generated images: very good (top row) 
relatively good (middle row)  and very bad or broken (last row).

absolute improvement. We note that this comparison is not totally fair because we use images
generated from a generator pre-trained from ImageNet. However  our contribution is not to establish
a new state-of-the-art score but to present the idea of transferring an ImageNet pre-trained GAN for
improving one shot classiﬁers  so we believe this comparison is still informative.

5.3 Results on NAB Dataset
We also performed similar experiments on the NAB dataset  which is more than four times larger than
CUB  and the results are shown in the last column of Table 2. We observe similar results as CUB 
and that our method improves classiﬁcation accuracy from a ProtoNet baseline (89.19% vs. 87.91%).

6 Conclusion

We introduce an effective way to employ an ImageNet-pre-trained image generator for the purpose
of improving ﬁne-grained one-shot classiﬁcation when data is scarce. As a way to ﬁne-tune the
pre-trained generator  our pilot study ﬁnds that adjusting only scale and shift parameters in batch
normalization can produce a visually realistic images. This technique works with a single image 
making the method less dependent on the number of available images. Furthermore  although naively
adding the generated images into the training set does not improve performance  we show that it can
improve performance if we mix generated with original images to create hybrid training exemplars.
In order to learn the parameters of this mixing  we adapt a meta-learning framework. We implement
this idea and demonstrate a consistent and signiﬁcant improvement over several classiﬁers on two
ﬁne-grained benchmark datasets.

Acknowledgments
We would like to thank Yi Li for drawing Figure 2  and Minjun Li and Atsuhiro Noguchi for helpful
discussions. Part of this work was done while Satoshi Tsutsui was an intern at Fudan University.
Yanwei Fu was supported in part by the NSFC project (#61572138)  and Science and Technology
Commission of Shanghai Municipality Project (#19511120700). David Crandall was supported
in part by the National Science Foundation (CAREER IIS-1253549)  and the Indiana University
Ofﬁce of the Vice Provost for Research  the College of Arts and Sciences  and the Luddy School of
Informatics  Computing  and Engineering through the Emerging Areas of Research Project “Learning:
Brains  Machines  and Children.” Yanwei Fu is the corresponding author.

8

References
[1] Antreas Antoniou  Amos Storkey  and Harrison Edwards. Augmenting image classiﬁers using
data augmentation generative adversarial networks. In International Conference on Artiﬁcial
Neural Networks  2018.

[2] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein gan. arXiv preprint

arXiv:1701.07875  2017.

[3] Andrew Brock  Jeff Donahue  and Karen Simonyan. Large scale gan training for high ﬁdelity
natural image synthesis. In International Conference on Learning Representations (ICLR) 
2019.

[4] Wei-Yu Chen  Yen-Cheng Liu  Zsolt Kira  Yu-Chiang Frank Wang  and Jia-Bin Huang. A
closer look at few-shot classiﬁcation. In International Conference on Learning Representations
(ICLR)  2019.

[5] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems (NIPS)  2016.

[6] Zitian Chen  Yanwei Fu  Kaiyu Chen  and Yu-Gang Jiang. Image block augmentation for

one-shot learning. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2019.

[7] Zitian Chen  Yanwei Fu  Yu-Xiong Wang  Lin Ma  Wei Liu  and Martial Hebert.

Image
deformation meta-networks for one-shot learning. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2019.

[8] Harm De Vries  Florian Strub  Jérémie Mary  Hugo Larochelle  Olivier Pietquin  and Aaron C
Courville. Modulating early visual processing by language. In Advances in Neural Information
Processing Systems (NIPS)  2017.

[9] Vincent Dumoulin  Jonathon Shlens  and Manjunath Kudlur. A learned representation for

artistic style. In International Conference on Learning Representations (ICLR)  2017.

[10] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adapta-

tion of deep networks. In International Conference on Machine Learning (ICML)  2017.

[11] Chelsea Finn  Kelvin Xu  and Sergey Levine. Probabilistic model-agnostic meta-learning. In

Advances in Neural Information Processing Systems (NeurIPS)  2018.

[12] Hang Gao  Zheng Shou  Alireza Zareian  Hanwang Zhang  and Shih-Fu Chang. Low-shot
learning via covariance-preserving adversarial augmentation networks. In Advances in Neural
Information Processing Systems (NeurIPS)  2018.

[13] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NIPS)  2014.

[14] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems
(NIPS)  2017.

[15] Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinat-

ing features. In IEEE International Conference on Computer Vision (ICCV)  2017.

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016.
[17] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer

and super-resolution. In European Conference on Computer Vision (ECCV)  2016.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference on Learning Representations (ICLR)  2015.

[19] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations
(ICLR)  2018.

[20] Atsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics

adaptation. In IEEE International Conference on Computer Vision (ICCV)  2019.

9

[21] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. In International Conference on Learning
Representations (ICLR)  2016.

[22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Interna-

tional Conference on Learning Representations (ICLR)  2017.

[23] Andrei A Rusu  Dushyant Rao  Jakub Sygnowski  Oriol Vinyals  Razvan Pascanu  Simon
Osindero  and Raia Hadsell. Meta-learning with latent embedding optimization. In International
Conference on Learning Representations (ICLR)  2018.

[24] Adam Santoro  Sergey Bartunov  Matthew Botvinick  Daan Wierstra  and Timothy Lillicrap.
In International Conference on

Meta-learning with memory-augmented neural networks.
Machine Learning (ICML)  2016.

[25] Eli Schwartz  Leonid Karlinsky  Joseph Shtok  Sivan Harary  Mattias Marder  Abhishek Kumar 
Rogerio Feris  Raja Giryes  and Alex Bronstein. Delta-encoder: an effective sample synthesis
method for few-shot object recognition. In Advances in Neural Information Processing Systems
(NeurIPS)  2018.

[26] Konstantin Shmelkov  Cordelia Schmid  and Karteek Alahari. How good is my gan?

European Conference on Computer Vision (ECCV)  2018.

In

[27] Ashish Shrivastava  Tomas Pﬁster  Oncel Tuzel  Joshua Susskind  Wenda Wang  and Russell
Webb. Learning from simulated and unsupervised images through adversarial training. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)  2017.

[28] Jake Snell  Kevin Swersky  and Richard S Zemel. Prototypical networks for few-shot learning.

In Advances in Neural Information Processing Systems (NIPS)  2017.

[29] Flood Sung  Yongxin Yang  Li Zhang  Tao Xiang  Philip HS Torr  and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2018.

[30] Grant Van Horn  Steve Branson  Ryan Farrell  Scott Haber  Jessie Barry  Panos Ipeirotis  Pietro
Perona  and Serge Belongie. Building a bird recognition app and large scale dataset with citizen
scientists: The ﬁne print in ﬁne-grained dataset collection. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2015.

[31] Oriol Vinyals  Charles Blundell  Timothy Lillicrap  Daan Wierstra  et al. Matching networks

for one shot learning. In Advances in Neural Information Processing Systems (NIPS)  2016.

[32] C. Wah  S. Branson  P. Welinder  P. Perona  and S. Belongie. The Caltech-UCSD Birds-200-2011

Dataset. Technical Report CNS-TR-2011-001  California Institute of Technology  2011.

[33] Yaxing Wang  Chenshen Wu  Luis Herranz  Joost van de Weijer  Abel Gonzalez-Garcia  and
Bogdan Raducanu. Transferring gans: generating images from limited data. In Proceedings of
the European Conference on Computer Vision (ECCV)  2018.

[34] Yu-Xiong Wang  Ross Girshick  Martial Hebert  and Bharath Hariharan. Low-shot learning
from imaginary data. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2018.

[35] Yuxiong Wang and Martial Hebert. Learning from small sample sets by combining unsupervised
meta-training with cnns. In Advances in Neural Information Processing Systems (NIPS)  2016.
[36] Hongyi Zhang  Moustapha Cisse  Yann N. Dauphin  and David Lopez-Paz. Mixup: Beyond
empirical risk minimization. In International Conference on Learning Representations (ICLR) 
2018.

[37] Ruixiang Zhang  Tong Che  Zoubin Ghahramani  Yoshua Bengio  and Yangqiu Song. MetaGAN:
An Adversarial Approach to Few-Shot Learning. In Advances in Neural Information Processing
Systems (NeurIPS)  2018.

[38] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networkss. In IEEE International Conference on
Computer Vision (ICCV)  2017.

10

,Satoshi Tsutsui
Yanwei Fu
David Crandall