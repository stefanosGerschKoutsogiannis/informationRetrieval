2010,Moreau-Yosida Regularization for Grouped Tree Structure Learning,We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-defined tree structure is based on a group-Lasso penalty  where one group is defined for each node in the tree. Such a regularization can help uncover the structured sparsity  which is desirable for applications with some meaningful tree structures on the features. However  the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper  we develop an efficient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution  and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.,Moreau-Yosida Regularization for Grouped

Tree Structure Learning

Jun Liu

Computer Science and Engineering

Arizona State University

J.Liu@asu.edu

Abstract

Jieping Ye

Computer Science and Engineering

Arizona State University
Jieping.Ye@asu.edu

We consider the tree structured group Lasso where the structure over the features
can be represented as a tree with leaf nodes as features and internal nodes as clus-
ters of the features. The structured regularization with a pre-deﬁned tree structure
is based on a group-Lasso penalty  where one group is deﬁned for each node in
the tree. Such a regularization can help uncover the structured sparsity  which is
desirable for applications with some meaningful tree structures on the features.
However  the tree structured group Lasso is challenging to solve due to the com-
plex regularization. In this paper  we develop an efﬁcient algorithm for the tree
structured group Lasso. One of the key steps in the proposed algorithm is to solve
the Moreau-Yosida regularization associated with the grouped tree structure. The
main technical contributions of this paper include (1) we show that the associated
Moreau-Yosida regularization admits an analytical solution  and (2) we develop
an efﬁcient algorithm for determining the effective interval for the regularization
parameter. Our experimental results on the AR and JAFFE face data sets demon-
strate the efﬁciency and effectiveness of the proposed algorithm.

1 Introduction

Many machine learning algorithms can be formulated as a penalized optimization problem:

min
x

l(x) + λφ(x) 

(1)

where l(x) is the empirical loss function (e.g.  the least squares loss and the logistic loss)  λ > 0
is the regularization parameter  and φ(x) is the penalty term. Recently  sparse learning via (cid:96)1 regu-
larization [20] and its various extensions has received increasing attention in many areas including
machine learning  signal processing  and statistics. In particular  the group Lasso [1  16  22] utilizes
the group information of the features  and yields a solution with grouped sparsity. The traditional
group Lasso assumes that the groups are non-overlapping. However  in many applications the fea-
tures may form more complex overlapping groups. Zhao et al. [23] extended the group Lasso to
the case of overlapping groups  imposing hierarchical relationships for the features. Jacob et al. [6]
considered group Lasso with overlaps  and studied theoretical properties of the estimator. Jenatton et
al. [7] considered the consistency property of the structured overlapping group Lasso  and designed
an active set algorithm.
In many applications  the features can naturally be represented using certain tree structures. For
example  the image pixels of the face image shown in Figure 1 can be represented as a tree  where
each parent node contains a series of child nodes that enjoy spatial locality; genes/proteins may
form certain hierarchical tree structures. Kim and Xing [9] studied the tree structured group Lasso
for multi-task learning  where multiple related tasks follow a tree structure. One challenge in the
practical application of the tree structured group Lasso is that the resulting optimization problem is
much more difﬁcult to solve than Lasso and group Lasso  due to the complex regularization.

1

(a)

(b)

(c)

(d)

Figure 1: Illustration of the tree structure of a two-dimensional face image. The 64 × 64 image (a) can be
divided into 16 sub-images in (b) according to the spatial locality  where the sub-images can be viewed as the
child nodes of (a). Similarly  each 16 × 16 sub-image in (b) can be divided into 16 sub-images in (c)  and such
a process is repeated for the sub-images in (c) to get (d).

Figure 2: A sample index tree for illustration. Root: G0
2 = {3  4  5  6}  G1
1 = {1}  G2
G1

3 = {7  8}. Depth 2: G2

1 = {1  2  3  4  5  6  7  8}. Depth 1: G1
2 = {2}  G2

3 = {3  4}  G2

4 = {5  6}.

1 = {1  2} 

In this paper  we develop an efﬁcient algorithm for the tree structured group Lasso  i.e.  the op-
timization problem (1) with φ(·) being the grouped tree structure regularization (see Equation 2).
One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization [17  21]
associated with the grouped tree structure. The main technical contributions of this paper include:
(1) we show that the associated Moreau-Yosida regularization admits an analytical solution  and the
resulting algorithm for the tree structured group Lasso has a time complexity comparable to Lasso
and group Lasso  and (2) we develop an efﬁcient algorithm for determining the effective interval for
the parameter λ  which is important in the practical application of the algorithm. We have performed
experimental studies using the AR and JAFFE face data sets  where the features form a hierarchical
tree structure based on the spatial locality as shown in Figure 1. Our experimental results demon-
strate the efﬁciency and effectiveness of the proposed algorithm. Note that while the present paper
was under review  we became aware of a recent work by Jenatton et al. [8] which applied block
coordinate ascent in the dual and showed that the algorithm converges in one pass.

2 Grouped Tree Structure Regularization

We begin with the deﬁnition of the so-called index tree:
ni} contain all the node(s)
Deﬁnition 1. For an index tree T of depth d  we let Ti = {Gi
1 = {1  2  . . .   p} and ni ≥ 1  i = 1  2  . . .   d. The
corresponding to depth i  where n0 = 1  G0
nodes satisfy the following conditions: 1) the nodes from the same depth level have non-overlapping
j ∩ Gi
j0 be the parent node
indices  i.e.  Gi
of a non-root node Gi

k = ∅ ∀i = 1  . . .   d  j (cid:54)= k  1 ≤ j  k ≤ ni; and 2) let Gi−1
j  then Gi

j ⊆ Gi−1
j0 .

2  . . .   Gi

1  Gi

Figure 2 shows a sample index tree. We can observe that 1) the index sets from different nodes may
overlap  e.g.  any parent node overlaps with its child nodes; 2) the nodes from the same depth level
do not overlap; and 3) the index set of a child node is a subset of that of its parent node.
The grouped tree structure regularization is deﬁned as:

d(cid:88)

ni(cid:88)

i=0

j=1

2

φ(x) =

j(cid:107)xGi
wi

j

(cid:107) 

(2)

where x ∈ Rp  wi
(cid:107) · (cid:107) is the Euclidean norm  and xGi

j ≥ 0 (i = 0  1  . . .   d  j = 1  2  . . .   ni) is the pre-deﬁned weight for the node Gi
j 
j.
is a vector composed of the entries of x with the indices in Gi

j

G01G11G12G13G21G22G23G24In the next section  we study the Moreau-Yosida regularization [17  21] associated with (2)  develop
an analytical solution for such a regularization  propose an efﬁcient algorithm for solving (1)  and
specify the meaningful interval for the regularization parameter λ.
3 Moreau-Yosida Regularization of φ(·)
The Moreau-Yosida regularization associated with the grouped tree structure regularization φ(·) for
a given v ∈ Rp is given by:

d(cid:88)

ni(cid:88)

i=0

j=1

  

(cid:107)x − v(cid:107)2 + λ

1
2

j(cid:107)xGi
wi

j

(cid:107)

(3)

f(x) =

φλ(v) = min
x

for some λ > 0. Denote the minimizer of (3) as πλ(v). The Moreau-Yosida regularization has many
useful properties: 1) φλ(·) is continuously differentiable despite the fact that φ(·) is non-smooth; 2)
πλ(·) is a non-expansive operator. More properties on the general Moreau-Yosida regularization
can be found in [5  10]. Note that  f(·) in (3) is indeed a special case of the problem (1) with
2(cid:107)x − v(cid:107)2. Our recent study has shown that  the efﬁcient optimization of the Moreau-
l(x) = 1
Yosida regularization is key to many optimization algorithms [13  Section 2]. Next  we focus on the
efﬁcient optimization of (3). For convenience of subsequent discussion  we denote λi

j = λwi
j.

3.1 An Analytical Solution

We show that the minimization of (3) admits an analytical solution. We ﬁrst present the detailed
procedure for ﬁnding the minimizer in Algorithm 1.

Algorithm 1 Moreau-Yosida Regularization of the tree structured group Lasso (MYtgLasso)
Input: v ∈ Rp  the index tree T with nodes Gi

j (i = 0  1  . . .   d  j = 1  2  . . .   ni) that satisfy

j ≥ 0 (i = 0  1  . . .   d  j = 1  2  . . .   ni)  λ > 0  and λi

j = λwi

j

Deﬁnition 1  the weights wi

Output: u0 ∈ Rp
1: Set

2: for i = d to 0 do
3:
4:

for j = 1 to ni do

Compute

end for

5:
6: end for



ui

Gi
j

=

ud+1 = v 

0
(cid:107)−λi
(cid:107) ui+1

Gi
j

j

(cid:107)ui+1
Gi
j
(cid:107)ui+1
Gi
j

Gi
j

(cid:107)ui+1
(cid:107)ui+1

j

(cid:107) ≤ λi
(cid:107) > λi
j 

Gi
j

(4)

(5)

j

j

(cid:80)d

by at most λi

(cid:80)ni

j  we update uGi

j. The time complexity of MYtgLasso is O(
j=1 |Gi

In the implementation of the MYtgLasso algorithm  we only need to maintain a working variable u 
which is initialized with v. We then traverse the index tree T in the reverse breadth-ﬁrst order to up-
according to the operation in (5)  which reduces the
date u. At the traversed node Gi
j|).
Euclidean norm of uGi
j| ≤ p. Therefore  the time complexity of MYtgLasso
By using Deﬁnition 1  we have
is O(pd). If the tree is balanced  i.e.  d = O(log p)  then the time complexity of MYtgLasso is
O(p log p).
MYtgLasso can help explain why the structured group sparsity can be induced. Let us analyze the
tree given in Figure 2  with the solution denoted as x∗. We let wi
2  and v =
[1  2  1  1  4  4  1  1]T. After traversing the nodes of depth 2  we can get that the elements of x∗ with
indices in G2
3 are zero; and when the traversal continues to the nodes of depth 1  the elements
of x∗ with indices in G1
4 are still nonzero. Finally  after
traversing the root node  we obtain x∗ = [0  0  0  0  1  1  0  0]T.

3 are set to zero  but those with G2

j = 1 ∀i  j  λ =

(cid:80)ni

j=1 |Gi

1 and G1

1 and G2

√

i=0

3

Next  we show that MYtgLasso ﬁnds the exact minimizer of (3). The main result is summarized in
the following theorem:
Theorem 1. u0 returned by Algorithm 1 is the unique solution to (3).

Before giving the detailed proof for Theorem 1  we introduce some notations  and present several
technical lemmas.
Deﬁne the mapping φi

j : Rp → R as

j(x) = (cid:107)xGi
φi
ni(cid:88)
d(cid:88)
We can then express φ(x) deﬁned in (2) as:

j

(cid:107).

φ(x) =

λi
jφi

j(x).

i=0

j=1

d(cid:88)

ni(cid:88)

The subdifferential of f(·) deﬁned in (3) at the point x can be written as:

where

∂f(x) = x − v +

i=0

(cid:110)
(cid:189)
y ∈ Rp : (cid:107)y(cid:107) ≤ 1  yGi
y ∈ Rp : yGi
(cid:107)   yGi
j denotes the complementary set of Gi
j.



j(x) =

xGi
j
(cid:107)xGi

∂φi

j=1

=

j

j

j

j

λi
j∂φi

j(x) 

(cid:111)

(cid:190)

= 0

= 0

if xGi

j

if xGi

j

= 0
(cid:54)= 0 

and Gi
Lemma 1. For any 1 ≤ i ≤ d  1 ≤ j ≤ ni  we can ﬁnd a unique path from the node Gi
node G0

1. Let the nodes on this path be Gl
rl
j ⊆ Gl
Gi

rl

  for l = 0  1  . . .   i with r0 = 1 and ri = j. We have
 ∀l = 0  1  . . .   i − 1.

j to the root

(6)

(7)

(8)

(9)
(10)

(11)

(12)

(13)

j ∩ Gl
Gi

r = ∅ ∀r (cid:54)= rl  l = 1  2  . . .   i − 1  r = 1  2  . . .   ni.
Proof: According to Deﬁnition 1  we can ﬁnd a unique path from the node Gi
In addition  based on the structure of the index tree  we have (9) and (10).
Lemma 2. For any i = 1  2  . . .   d  j = 1  2  . . .   ni  we have
j(ui)

∈ ui+1

− λi

(cid:162)

ui

 

(cid:161)
∂φi
j(ui) ⊆ ∂φi

Gi
j

j

j(u0).

Gi
j

Gi
j

∂φi

Proof: We can verify (11) using (5)  (6) and (8).
For (12)  it follows from (6) and (8) that  it is sufﬁcient to verify that
j ≥ 0.

  for some αi

= αi

u0
Gi
j

jui
Gi
j

j to the root node G0
1.
(cid:164)

It follows from Lemma 1 that we can ﬁnd a unique path from Gi
path as: Gl
rl

1. Denote the nodes on the
  where l = 0  1  . . .   i  ri = j  and r0 = 1. We ﬁrst analyze the relationship between
= 0 by using

= 0  which leads to ui−1

and ui−1

j to G0

. If

ri−1  we have ui−1
Gi−1
ri−1

ui

Gi
j

Gi
j

Gi
j

ri−1  we have ui−1
Gi−1
ri−1

=

ui

Gi−1
ri−1

  which leads to

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)ui

G

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)−λi−1
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)

i−1
ri−1

ri−1

i−1
ri−1

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)ui

G

(cid:176)(cid:176)(cid:176)(cid:176) ≤ λi−1
(cid:176)(cid:176)(cid:176)(cid:176) > λi−1

Gi−1
ri−1

Gi−1
ri−1

(cid:176)(cid:176)(cid:176)(cid:176)ui
(cid:176)(cid:176)(cid:176)(cid:176)ui
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)−λi−1
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)

i−1
ri−1

ri−1

(9). Otherwise  if

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)ui

G

i−1
ri−1

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)ui

G

ui−1

Gi
j

=

ui

Gi
j

by using (9). Therefore  we have

ui−1

Gi
j

= βiui
Gi
j

  for some βi ≥ 0.

(14)

4

By a similar argument  we have
ul−1

Gl
rl

Together with (9)  we have

= βlul
Gl
rl

  βl ≥ 0 ∀l = 1  2  . . .   i − 1.

ul−1

Gi
j

= βlul
Gi
j

  βl ≥ 0   ∀l = 1  2  . . .   i − 1.

(15)

(16)

From (14) and (16)  we show (13) holds with αi
We are now ready to prove our main result:
Proof of Theorem 1: It is easy to verify that f(·) deﬁned in (3) is strongly convex  thus it admits a
unique minimizer. Our methodology for the proof is to show that

l=1βl. This completes the proof.

j = Πi

(cid:164)

0 ∈ ∂f(u0) 

which is the sufﬁcient and necessary condition for u0 to be the minimizer of f(·).
According to Deﬁnition 1  the leaf nodes are non-overlapping. We assume that the union of the leaf
nodes equals to {1  2  . . .   p}; otherwise  we can add to the index tree the additional leaf nodes with
weight 0 to satisfy the aforementioned assumption. Clearly  the original index tree and the new index
tree with the additional leaf nodes of weight 0 yield the same penalty φ(·) in (2)  the same Moreau-
Yosida regularization in (3)  and the same solution from Algorithm 1. Therefore  to prove (17) 
it sufﬁces to show 0 ∈ ∂f(u0)Gi
j. Next  we focus on establishing the
following relationship:

  for all the leaf nodes Gi

j

0 ∈ ∂f(u0)Gd

.

1

It follows from Lemma 1 that  we can ﬁnd a unique path from the node Gd
nodes on this path are Gl
rl
we can get that the nodes that contain the index set Gd
other words  ∀x  we have
r(x)

1. Let the
  for l = 0  1  . . .   d with r0 = 1 and rd = 1. By using (10) of Lemma 1 
1 are exactly on the aforementioned path. In

= {0} ∀r (cid:54)= rl  l = 1  2  . . .   d − 1  r = 1  2  . . .   ni

1 to the root G0

(cid:161)

(cid:162)

∂φl

(17)

(18)

(19)

(20)

(21)

(22)

(cid:164)

Gd
1

(cid:161)

by using (6) and (8).
Applying (11) and (12) of Lemma 2 to each node on the aformetioned path  we have

Gl
rl
Making using of (9)  we obtain from (20) the following relationship:

Gl
rl

Gl
rl

∈ λl

rl

∂φl
rl

(ul)

⊆ λl

rl

∂φl
rl

(u0)

− ul

ul+1
Gl
rl

 ∀l = 0  1  . . .   d.

(cid:162)

∈ λl
Adding (21) for l = 0  1  . . .   d  we have

− ul

ul+1
Gd
1

Gd
1

rl

∂φl
rl

(u0)

Gd
1

 ∀l = 0  1  . . .   d.

(cid:162)
(cid:161)

∈ d(cid:88)

l=0

(cid:162)

(u0)

Gd
1

ud+1
Gd
1

− u0

Gd
1

λl
rl

∂φl
rl

(cid:161)
(cid:162)
(cid:161)

It follows from (4)  (7)  (19) and (22) that (18) holds.
Similarly  we have 0 ∈ f(u0)Gi

for the other leaf nodes Gi

j

j. Thus  we have (17).

3.2 The Proposed Optimization Algorithm
With the analytical solution for πλ(·)  the minimizer of (3)  we can apply many existing methods for
solving (1). First  we show in the following lemma that  the optimal solution to (1) can be computed
as a ﬁxed point. We shall show in Section 3.3 that  the result in this lemma can also help determine
the interval for the values of λ.
Lemma 3. Let x∗ be an optimal solution to (1). Then  x∗ satisﬁes:
x∗ = πλτ (x∗ − τ l(cid:48)(x∗)) ∀τ > 0.

(23)

5

Proof: x∗ is an optimal solution to (1)  if and only if

0 ∈ l(cid:48)(x∗) + λ∂φ(x∗) 

(24)

1

0 ∈ x∗ − (x∗ − τ l(cid:48)(x∗)) + λτ ∂φ(x∗) ∀τ > 0.

which leads to
(25)
2(cid:107)x−(x∗− τ l(cid:48)(x∗))(cid:107)2 + λτ φ(x). Recall that πλ(·) is the minimizer
Thus  we have x∗ = arg minx
(cid:164)
of (3). We have (23).
It follows from Lemma 3 that we can apply the ﬁxed point continuation method [4] for solving (1). It
is interesting to note that  with an appropriately chosen τ  the scheme in (23) indeed corresponds to
the gradient method developed for the composite function optimization [2  19]  achieving the global
convergence rate of O(1/k) for k iterations. In addition  the scheme in (23) can be accelerated
to obtain the accelerated gradient descent [2  19]  where the Moreau-Yosidea regularization also
needs to be evaluated in each of its iteration. We employ the accelerated gradient descent developed
in [2] for the optimization in this paper. The algorithm is called “tgLasso”  which stands for the tree
structured group Lasso. Note that  tgLasso includes our previous algorithm [11] as a special case 
when the index tree is of depth 1 and w0

1 = 0.

3.3 The Effective Interval for the Values of λ

When estimating the model parameters via (1)  a key issue is to choose the appropriate values for
the regularization parameter λ. A commonly used approach is to select the regularization parameter
from a set of candidate values  whose values  however  need to be pre-speciﬁed in advance. There-
fore  it is essential to specify the effective interval for the values of λ. An analysis of MYtgLasso
in Algorithm 1 shows that  with increasing λ  the entries of the solution to (3) are monotonically
decreasing. Intuitively  the solution to (3) shall be exactly zero if λ is sufﬁciently large and all the
entries of x are penalized in φ(x). Next  we summarize the main results of this subsection.
Theorem 2. The zero point is a solution to (1) if and only if the zero point is a solution to (3)
with v = −l(cid:48)(0). For the penalty φ(x)  let us assume that all entries of x are penalized  i.e. 
∀l ∈ {1  2  . . .   p}  there exists at least one node Gi
j > 0. Then 
for any 0 < (cid:107) − l(cid:48)(0)(cid:107) < +∞  there exists a unique λmax < +∞ satisfying: 1) if λ ≥ λmax the
zero point is a solution to (1)  and 2) if 0 < λ < λmax  the zero point is not a solution to (1).
Proof: If x∗ = 0 is the solution to (1)  we have (24). Setting τ = 1 in (23)  we obtain that x∗ = 0
is also the solution to (3) with v = −l(cid:48)(0). If x∗ = 0 is the solution to (3) with v = −l(cid:48)(0)  we
have 0 ∈ l(cid:48)(0) + λ∂φ(0)  which indicates that x∗ = 0 is the solution to (1).
The function φ(x) is closed convex. According to [18  Chapater 3.1.5]  ∂φ(0) is a closed convex
and non-empty bounded set. From (8)  it is clear that 0 ∈ ∂φ(0). Therefore  we have (cid:107)x(cid:107) ≤
R ∀x ∈ ∂φ(0)  where R is a ﬁnite radius constant. Let

j that contains l and meanwhile wi

S = {x : x = −αRl(cid:48)(0)/(cid:107)l(cid:48)(0)(cid:107)  α ∈ [0  1]}

be the line segment from 0 to −Rl(cid:48)(0)/(cid:107)l(cid:48)(0)(cid:107). It is obvious that S is closed convex and bounded.
Deﬁne I = S

∂φ(0)  which is clearly closed convex and bounded. Deﬁne

(cid:84)

˜λmax = (cid:107)l(cid:48)(0)(cid:107)/ max
x∈I

(cid:107)x(cid:107).

j that contains l and meanwhile wi

It follows from (cid:107)l(cid:48)(0)(cid:107) > 0 and the boundedness of I that ˜λmax > 0. We ﬁrst show ˜λmax < +∞.
Otherwise  we have I = {0}. Thus  ∀λ > 0  we have −l(cid:48)(0)/λ /∈ ∂φ(0)  which indicates that 0 is
neither the solution to (1) nor (3) with v = −l(cid:48)(0). Recall the assumption that  ∀l ∈ {1  2  . . .   p} 
j > 0. It follows from Algorithm 1
there exists at least one node Gi
that  there exists a ˜λ < +∞ such that when λ > ˜λ  0 is a solution to (3) with v = −l(cid:48)(0)  leading to
a contradiction. Therefore  we have 0 < ˜λmax < +∞. Let λmax = ˜λmax. The arguments hold since
1) if λ ≥ λmax  then −l(cid:48)(0)/λ ∈ I ⊆ ∂φ(0); and 2) if 0 < λ < λmax  then −l(cid:48)(0)/λ /∈ ∂φ(0). (cid:164)
When l(cid:48)(0) = 0  the problem (1) has a trivial zero solution. We next focus on the nontrivial case
l(cid:48)(0) (cid:54)= 0. We present the algorithm for efﬁciently solving λmax in Algorithm 2. In Step 1  λ0 is an
initial guess of the solution. Our empirical study shows that λ0 =
j )2 works quite
well. In Step 2-6  we specify an interval [λ1  λ2] in which λmax resides. Finally  in Step 7-14  we
apply bisection for computing λmax.

(cid:80)ni

(cid:114)

i=0

j=1(wi

(cid:80)d

(cid:107)l(cid:48)(0)(cid:107)2

6

Algorithm 2 Finding λmax via Bisection
Input: l(cid:48)(0)  the index tree T with nodes Gi

(i = 0  1  . . .   d  j = 1  2  . . .   ni)  λ0  and δ = 10−10

j (i = 0  1  . . .   d  j = 1  2  . . .   ni)  the weights wi

j ≥ 0

Set λ2 = λ  and ﬁnd the largest λ1 = 2−iλ  i = 1  2  . . . such that πλ1(−l(cid:48)(0)) (cid:54)= 0
Set λ1 = λ  and ﬁnd the smallest λ2 = 2iλ  i = 1  2  . . . such that πλ2(−l(cid:48)(0)) = 0

Output: λmax
1: Set λ = λ0
2: if φλ(−l(cid:48)(0)) = 0 then
3:
4: else
5:
6: end if
7: while λ2 − λ1 ≥ δ do
8:
9:
10:
11:
12:
13:
14: end while
15: λmax = λ

Set λ = λ1+λ2
if πλ(−l(cid:48)(0)) = 0 then
else

Set λ2 = λ

Set λ1 = λ

end if

2

4 Experiments

We have conducted experiments to evaluate the efﬁciency and effectiveness of the proposed tgLasso
algorithm on the face data sets JAFFE [14] and AR [15]. JAFFE contains 213 images of ten Japanese
actresses with seven facial expressions: neutral  happy  disgust  fear  anger  sadness  and suprise.
We used a subsect of AR that contains 400 images corresponding to 100 subjects  with each subject
containing four facial expression: neutral  smile  anger  and scream. For both data sets  we resize
the image size to 64 × 64  and make use of the tree structure depicted in Figure 1. Our task is
to discriminate each facial expression from the rest ones. Thus  we have seven and four binary
classiﬁcation tasks for JAFFE and AR  respectively. We employ the least squares loss for l(·)  and
set the regularization parameter λ = r × λmax  where λmax is computed using Algorithm 2  and
r = {5× 10−1  2× 10−1  1× 10−1  5× 10−2  2× 10−2  1× 10−2  5× 10−3  2× 10−3}. The source
codes  included in the SLEP package [12]  are available online1.

Table 1: Computational time (seconds) for one binary classiﬁcation task (averaged over 7 and 4 runs for JAFFE
and AR  respectively). The total time for all eight regularization parameters is reported.

tgLasso
alternating algorithm [9]

JAFFE AR
73
30
4054
5155

Efﬁciency of the Proposed tgLasso We compare our proposed tgLasso with the recently proposed
alternating algorithm [9] designed for the tree-guided group Lasso. We report the total computational
time (seconds) for running one binary classiﬁcation task (averaged over 7 and 4 tasks for JAFFE and
AR  respectively) corresponding to the eight regularization parameters in Table 1. We can obseve
that tgLasso is much more efﬁcient than the alternating algorithm. We note that  the key step of
tgLasso in each iteration is the associated Moreau-Yosida regularization  which can be efﬁciently
computed due to the existence of an analytical solution; and the key step of the alternating algorithm
in each iteration is the matrix inversion  which does not scale well to high-dimensional data.
Classiﬁcation Performance We compare the classiﬁcation performance of tgLasso with Lasso. On
AR  we use 50 subjects for training  and the remaining 50 subjects for testing; and on JAFFE  we
use 8 subjects for training  and the remaining 2 subjects for testing. This subject-independent setting
is challenging  as the subjects to be tested are not included in the training set. The reported results
are averaged over 10 runs for randomly chosen subjects. For each binary classiﬁcation task  we
compute the balanced error rate [3] to cope with the unbalanced positive and negative samples. We

1http://www.public.asu.edu/˜jye02/Software/SLEP/

7

Figure 3: Classiﬁcation performance comparison between Lasso and the tree structured group Lasso. The
horizontal axis corresponds to different regularization parameters λ = r × λmax.

Figure 4: Markers obtained by Lasso  and tree structured group Lasso (white pixels correspond to the markers).
First row: face images of four expression from the AR data set; Second row: the markers identiﬁed by tree
structured group Lasso; Third row: the markers identiﬁed by Lasso.

report the averaged results in Figure 3. Results show that tgLasso outperforms Lasso in both cases.
This veriﬁes the effectiveness of tgLasso in incorporating the tree structure in the formulation  i.e. 
the spatial locality information of the face images. Figure 4 shows the markers identiﬁed by tgLasso
and Lasso under the best regularization parameter. We can observe from the ﬁgure that tgLasso
results in a block sparsity solution  and most of the selected pixels are around mouths and eyes.

5 Conclusion

In this paper  we consider the efﬁcient optimization for the tree structured group Lasso. Our main
technical result show the Moreau-Yosida regularization associated with the tree structured group
Lasso admits an analytical solution. Based on the Moreau-Yosida regularization  we an design efﬁ-
cient algorithm for solving the grouped tree structure regularized optimization problem for smooth
convex loss functions  and develop an efﬁcient algorithm for determining the effective interval for
the parameter λ. Our experimental results on the AR and JAFFE face data sets demonstrate the
efﬁciency and effectiveness of the proposed algorithm. We plan to apply the proposed algorithm to
other applications in computer vision and bioinformatics involving the tree structure.

Acknowledgments

This work was supported by NSF IIS-0612069  IIS-0812551  CCF-0811790  IIS-0953662  NGA
HM1582-08-1-0016  NSFC 60905035  61035003  and the Ofﬁce of the Director of National Intelli-
gence (ODNI)  Intelligence Advanced Research Projects Activity (IARPA)  through the US Army.

8

5e−12e−11e−15e−22e−21e−25e−32e−3181920212223242526regularization parameter rbalanced error rate (%)AR tgLassoLasso5e−12e−11e−15e−22e−21e−25e−32e−33636.53737.53838.53939.540regularization parameter rbalanced error rate (%)JAFFE tgLassoLassoNeutralSmileAngerSceamReferences
[1] F. Bach  G. Lanckriet  and M. Jordan. Multiple kernel learning  conic duality  and the SMO algorithm. In

International conference on Machine learning  2004.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[3] I. Guyon  A. B. Hur  S. Gunn  and G. Dror. Result analysis of the nips 2003 feature selection challenge.

In Neural Information Processing Systems  pages 545–552  2004.

[4] E.T. Hale  W. Yin  and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: Methodology and con-

vergence. SIAM Journal on Optimization  19(3):1107–1130  2008.

[5] J. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms I & II. Springer

Verlag  Berlin  1993.

[6] L. Jacob  G. Obozinski  and J. Vert. Group lasso with overlap and graph lasso. In International Conference

on Machine Learning  2009.

[7] R. Jenatton  J.-Y. Audibert  and F. Bach. Structured variable selection with sparsity-inducing norms.

Technical report  arXiv:0904.3523v2  2009.

[8] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for sparse hierarchical dictionary

learning. In International Conference on Machine Learning  2010.

[9] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In

International Conference on Machine Learning  2010.

[10] C. Lemar´echal and C. Sagastiz´abal. Practical aspects of the Moreau-Yosida regularization I: Theoretical

properties. SIAM Journal on Optimization  7(2):367–385  1997.

[11] J. Liu  S. Ji  and J. Ye. Multi-task feature learning via efﬁcient (cid:96)2 1-norm minimization. In Uncertainty

in Artiﬁcial Intelligence  2009.

[12] J. Liu  S. Ji  and J. Ye. SLEP: Sparse Learning with Efﬁcient Projections. Arizona State University  2009.
[13] J. Liu  L. Yuan  and J. Ye. An efﬁcient algorithm for a class of fused lasso problems. In ACM SIGKDD

Conference on Knowledge Discovery and Data Mining  2010.

[14] M. J. Lyons  J. Budynek  and S. Akamatsu. Automatic classiﬁcation of single facial images.

Transactions on Pattern Analysis and Machine Intelligence  21(12):1357–1362  1999.

IEEE

[15] A.M. Martinez and R. Benavente. The AR face database. Technical report  1998.
[16] L. Meier  S. Geer  and P. B¨uhlmann. The group lasso for logistic regression. Journal of the Royal

Statistical Society: Series B  70:53–71  2008.

[17] J.-J. Moreau. Proximit´e et dualit´e dans un espace hilbertien. Bulletin de la Societe mathematique de

France  93:273–299  1965.

[18] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publish-

ers  2004.

[19] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discussion Paper 

2007.

[20] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society

Series B  58(1):267–288  1996.

[21] K. Yosida. Functional Analysis. Springer Verlag  Berlin  1964.
[22] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal Of

The Royal Statistical Society Series B  68(1):49–67  2006.

[23] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and hierarchical

variable selection. Annals of Statistics  37(6A):3468–3497  2009.

9

,Michael Hughes
Erik Sudderth