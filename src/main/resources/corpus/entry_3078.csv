2019,Learning in Generalized  Linear Contextual Bandits with Stochastic Delays,In this paper  we consider online learning in generalized linear contextual bandits where rewards are not immediately observed. Instead  rewards are available to the decision maker only after some delay  which is unknown and stochastic  even though a decision must be made at each time step for an incoming set of contexts. We study the performance of upper confidence bound (UCB) based algorithms adapted to this delayed setting. In particular  we design a delay-adaptive algorithm  which we call Delayed UCB  for generalized linear contextual bandits using UCB-style exploration and establish regret bounds under various delay assumptions. In the important special case of linear contextual bandits  we further modify this algorithm and establish a tighter regret bound under the same delay assumptions. 
Our results contribute to the broad landscape of contextual bandits literature by establishing that UCB algorithms  which are widely deployed in modern recommendation engines  can be made robust to delays.,Learning in Generalized

Linear Contextual Bandits with Stochastic Delays

Zhengyuan Zhou1 2⇤  Renyuan Xu3⇤and Jose Blanchet4
1 Department of Electrical Engineering  Stanford University

2 Bytedance Inc.

3 Department of Industrial Engineering and Operations Research  UC Berkeley
4 Department of Management Science and Engineering  Stanford University

Abstract

In this paper  we consider online learning in generalized linear contextual ban-
dits where rewards are not immediately observed. Instead  rewards are available
to the decision maker only after some delay  which is unknown and stochastic 
even though a decision must be made at each time step for an incoming set of
contexts. We study the performance of upper conﬁdence bound (UCB) based
algorithms adapted to this delayed setting. In particular  we design a delay-adaptive
algorithm  which we call Delayed UCB  for generalized linear contextual bandits
using UCB-style exploration and establish regret bounds under various delay as-
sumptions. In the important special case of linear contextual bandits  we further
modify this algorithm and establish a tighter regret bound under the same delay
assumptions. Our results contribute to the broad landscape of contextual bandits lit-
erature by establishing that UCB algorithms  which are widely deployed in modern
recommendation engines  can be made robust to delays.

1

Introduction

The growing availability of user-speciﬁc data has welcomed the exciting era of personalized rec-
ommendation  a paradigm that uncovers the heterogeneity across individuals and provides tailored
service decisions that lead to improved outcomes. Such heterogeneity is ubiquitous across a va-
riety of application domains (including online advertising  medical treatment assignment  prod-
uct/news recommendation (Li et al. (2010)  Bubeck et al. (2012) Chapelle (2014) Bastani and Bayati
(2015) Schwartz et al. (2017))) and manifests itself as different individuals responding differently
to the recommended items. Rising to this opportunity  contextual bandits have emerged to be the
predominant mathematical formalism that provides an elegant and powerful formulation: its three
core components  the features (representing individual characteristics)  the actions (representing the
recommendation)  and the rewards (representing the observed feedback)  capture the salient aspects
of the problem and provide fertile ground for developing algorithms that balance exploring and
exploiting users’ heterogeneity.
As such  the last decade has witnessed extensive research efforts in developing effective and efﬁcient
contextual bandits algorithms. In particular  two types of algorithms–upper conﬁdence bounds (UCB)
based algorithms (Li et al. (2010); Filippi et al. (2010); Chu et al. (2011); Jun et al. (2017); Li et al.
(2017)) and Thompson sampling (TS) based algorithms (Agrawal and Goyal (2013a b); Russo and
Van Roy (2014  2016); Abeille et al. (2017))–stand out from this ﬂourishing and fruitful line of work:
their theoretical guarantees have been analyzed in many settings  often yielding (near-)optimal regret
bounds; their empirical performance have been thoroughly validated  often providing insights into

⇤These two authors contributed equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

their practical efﬁcacy (including the consensus understanding that TS-based algorithms often suffer
from intensive computation for posterior updates but can leverage a correctly speciﬁed prior and have
superior empirical performance; UCB-based algorithms can often achieve tight theoretical regret
bounds but are often sensitive to hyper-parameter tuning in empirical performance). To a large extent 
these two family of algorithms have been widely deployed in many modern recommendation engines.
However  a key assumption therein–both the algorithm design and their analyses–is that the reward
is immediately available after an action is taken. Although useful as a ﬁrst-step abstraction  this
is a stringent requirement that is rarely satisﬁed in practice  particularly in large-scale systems
where the time-scale of a single recommendation is signiﬁcantly smaller than the time-scale of a
user’s feedback. For instance  in E-commerce  a recommendation is typically made by the engine
in milliseconds  whereas a user’s response time (i.e. to buy a product or conversion) is typically
much larger  ranging from hours to days  sometimes even to weeks. Similarly  in clinical trials  it is
infeasible to immediately observe and hence take into account the medical outcome after applying
a treatment to a patient–collecting medical feedback can be a time-consuming and often random
process; and in general  it is common to have applied trial treatments to a large number of patients 
with individual medical outcomes only available much later at different  random points in time. In
both the E-commerce (Kannan et al. (2001); Chapelle (2014); Vernade et al. (2017))and the clinical
trials cases (Chow and Chang (2011))  a random and often signiﬁcantly delayed reward is present 
thereby requiring adjustments in classical formulations to understand the impact of delays.

1.1 Related Work

The problem of learning on bandits with delays has recently been studied in different settings in the
existing literature  where most of the efforts have concentrated on the multi-armed bandits setting 
including both the stochastic and the adversarial multi-armed bandits. For stochastic multi-armed

bandits with delays  Joulani et al. (2013) show a regret bound O(log T + E[⌧ ] +plog TE[⌧ ]) where

E[⌧ ] is the mean of the iid delays. Desautels et al. (2014) consider Gaussian Process bandits with a
bounded stochastic delay. Mandel et al. (2015) follow the work of Joulani et al. (2013) and propose a
queue-based multi-armed bandit algorithm to handle delays. Pike-Burke et al. (2017) match the same
regret bound as in Joulani et al. (2013) when feedback is not only delayed but also anonymous.
For adversarial multi-armed bandits with delays  Neu et al. (2010) establish the regret bound of
)] for Markov decision process  where ⌧const is the constant delay
E[RT ]  O(⌧const) ⇥ E[R0T ( T
and R0T is the regret without delays. Cesa-Bianchi et al. (2019) consider adversarial bandits with

⌧const

ﬁxed constant delays on the network graph  with a minimax regret of the order ˜O⇣p(K + ⌧const)T⌘ 

where K is the number of arms. Another related line of work is adversarial learning with full
information (all arms’ rewards are observed)  where its different variants in the delayed setting have
been studied by Weinberger and Ordentlich (2002)  Mesterharm (2005)  Quanrud and Khashabi
(2015) and Garrabrant et al. (2016). Very recently  Bistritz et al. (2019) studied adversarial bandits
learning under arbitrary delays using Exp3 and established ﬁnite-sample delay-adaptive regret bounds.
On the other hand  learning in contextual bandits with delays are much less explored. Joulani et al.
(2013) consider learning on adversarial contextual bandits with delays and establish an expected

1+E[M⇤T ]⌘i by using a black-box algorithm  where
regret bound E [RT ]  (1 + E[M⇤T ]) ⇥ EhR0T⇣
M⇤T is the running maximum number of delays up to round T . Dudik et al. (2011) consider stochastic
contextual bandits with a ﬁxed constant delay. The reward model they consider is general (i.e. not
necessarily parametric); however  they require the policy class to be ﬁnite. In particular  they obtain
the regret bound O(pK log N (⌧const + pT ))  where N is the number of policies and ⌧const is again
the ﬁxed constant delay. On a related front  Grover et al. (2018b) studied the problem of best-arm
identiﬁcation under delayed feedback. There  the objective is to identify the best arm using as few
samples as possible  without taking into account the cost incurred along the way (i.e. a different
objective from regret). In closing  we also mention that there is also a growing literature in ofﬂine
contextual bandits learning Swaminathan and Joachims (2015); Kitagawa and Tetenov (2018); Zhou
et al. (2018a). In this domain  delay is typically not a concern as all the data has already been collected
in a single batch before any learning/decision-making takes place.

T

2

1.2 Our Contributions

In this paper  we consider learning on generalized linear (stochastic) contextual bandits with stochastic
delays. More speciﬁcally  we design a delay-adaptive algorithm for generalized linear contextual
bandits using UCB-style exploration  which we call Delayed UCB (DUCB  as given in Algorithm 1).
DUCB requires a carefully designed delay-adaptive conﬁdence parameter  which depends on how
many rewards are missing up to the current time step. Next  we give regret characterizations of
DUCB under independent stochastic  unbounded delays. In particular  as a special case of our
results  when the delays are iid with mean µD  we establish a high-probability regret bound of

the delays and d is the feature/context dimension. For comparison  the state-of-the-art regret bound

˜O⇣pµDd + pGd + dpT⌘ on DUCB  where G is a parameter characterizing the tail bound of
of UCB on generalized linear contextual bandits without delays is ˜O⇣dpT⌘ (Filippi et al. (2010); Li

et al. (2017)). Regret bounds for more general delays are also given. Note that our analysis here does
not assume the number of actions to be ﬁnite  and hence these regret bounds apply to inﬁnite-action
setting as well.
Finally  we consider the important special case of linear contextual bandits with ﬁnitely many actions.
In this setting  we provide a different UCB-based algorithm that estimates the underlying parameters
using a biased estimator (as opposed to the unbiased estimator employed in the generalized linear
contextual bandits setting) and provide a more reﬁned analysis that achieves regret bounds which are a
factor of O(pd) tighter. More speciﬁcally in this setting  as a direct comparison  when the delays are
again iid with mean µD  we establish a high-probability regret bound2 of ˜O⇣(1 + µD + G)pdT⌘.

To the best of our knowledge  these regret bounds provide the ﬁrst theoretical characterizations in
(generalized) linear contextual bandits with large delays and contribute to the broad landscape of
contextual bandits literature by delineating the impact of delays on performance.

2 Problem Setup

In this section  we describe the formulation for learning in generalized linear contextual bandits
(GLCB) in the presence of delays. We start by reviewing the basics of generalized linear contextual
bandits  followed by a description of the delay model. Before proceeding  we ﬁrst ﬁx some notation.
For a vector x 2 Rd  we use kxk to denote its l2-norm and x0 its transpose. Bd := {x 2 Rd : kxk 
1} is the unit ball centered at the origin. The weighted l2-norm associated with a positive-deﬁnite
matrix A is deﬁned by kxkA := px0Ax. The minimum and maximum singular values of a matrix
A are written as min(A) and kAk respectively. For two symmetric matrices A and B the same
dimensions  A ⌫ B means that A-B is positive semi-deﬁnite. For a real-valued function f  we use ˙f
and ¨f to denote its ﬁrst and second derivatives. Finally  [n] := {1  2 ···   n}.
2.1 Generalized Linear Contextual Bandits
Decision procedure. We consider the generalized linear contextual bandits problem with K arms.
At each round t  the agent observes a context consisting of a set of K feature vectors xt := {xt a 2
Rd|a 2 [K]}  which is drawn iid from an unknown distribution  with kxt ak  1. Each feature
vector xt a is associated with an unknown stochastic reward yt a 2 [0  1]. If the agent selects one
action at  there is a reward yt at 2 [0  1] associated with the selected arm at and the associated xt at.
Under the classic setting  the reward is immediately observed after the decision and the information
can be utilized to make decision in the next round.

Relationship between reward Y and context X.
In terms of the relationship between yt at and
xt at (t  1)  we follow the standard generalized linear contextual bandits literature (Filippi et al.
2In this case  the number of actions being ﬁnite is important. In particular  the regret bound has a O(log K)
dependence. Consequently  strictly speaking  if K is not viewed as a constant  we would also need K to not

be too large compared to d in order to retain the same regret bound of ˜O⇣(µD + G + 1)pdT⌘. A common

(and rather weak) assumption is the K is polynomial in d.

3

(2010); Li et al. (2017)). Deﬁne H0
t = {(s  xs  as  ys as)  s  t  1}[{ xt} as the information at
the beginning of round t. The agent maximizes the cumulative expected rewards over T rounds
t at each round t (t  1). Suppose the agent takes action at at round t. Denote
with information H0
by Xt = xt at  Yt = yt at and we assume the conditional distribution of Yt given Xt is from the
exponential family. Therefore its density is given by
P✓⇤(Yt|Xt) = exp✓ YtX0t✓⇤  m(X0t✓⇤)

(1)
Here  ✓⇤ is an unknown number under the frequentist setting; ⌘ 2 R+ is a given parameter; A  m and
h are three normalization functions mapping from R to R.
For exponential families  m is inﬁnitely differentiable 
˙m(X0✓⇤) = E[Y |X]  and ¨m(X0✓⇤) =
V(Y |X). Denote g(X0✓⇤) = E[Y |X]   one can easily verify that g(x0✓) = x0✓ for linear model 
1+exp(x0✓) for logistic model and g(x0✓) = exp(x0✓) for Poisson model. In the general-
g(x0✓) =
ized linear model (GLM) literature (Nelder and Wedderburn (1972); McCullagh (2018))  g is often
referred to as the inverse link function. Note that (1) can be rewritten as the GLCB form 

+ A(Yt ⌘ )◆ .

h(⌘)

1

Yt = g(X0t✓⇤) + ✏t 

where {✏t  t 2 [T ]} are independent zero-mean noise  H0
generated from (1) automatically satisﬁes the sub-Gaussian condition:

t -measurable with E[✏t|H0
t⇤  exp✓ 22
2 ◆ .

E⇥exp(✏t)|H0

Throughout the paper  we denote > 0 as the sub-Gaussian parameter of the noise ✏t.
Remark 1. In this paper  we focus on the GLM with exponential family (1). In general  one can work
with model (2) under the sub-Gaussian assumption (3). Our analysis will still hold by considering
maximum quasi-likelihood estimator for (2). See more explanations in the appendix.

(2)
t ] = 0. Data

(3)

2.2 The Delay Model
Unlike the traditional setting where each reward is immediately observed  here we consider the
case where stochastic and unbounded delays are present in revealing the rewards. Let T be the
number of total rounds. At round t  after the agent takes action at  the reward yt at may not be
available immediately. Instead  it will be observed at the end of round t + Dt where Dt is the delay
at time t. We assume Dt is a non-negative random number which is independent of {Ds}st1 and
{xs  ys as  as}st. First  we deﬁne the available information for the agent at each round.
Information structure under delays. At any round t  if Ds + s  t 1 (reward occurred in round
s is available at the beginning of round t)  then we call (s  xs  ys as  as) the complete information
tuple at round t. If Ds + s  t  we call (s  xs  as) the incomplete information tuple at the beginning
of round t. Deﬁne

Ht = {(s  xs  ys as  as) | s + Ds  t  1}[{ (s  xs  as) | s  t  1  s + Ds  t}[{ xt}  

then Ht is the information (ﬁltration) available at the beginning of round t for the agent to choose
action at. In other words  Ht contains all the incomplete and complete information tuples up to round
t  1 and the content vector xt at round t.
Moreover deﬁne

Ft = {(s  xs  as  ys as) | s + Ds  t}.

(4)
Then Ft contains all the complete information tuples (s  xs  as  ys as) up to the end of round t.
Denote It = Ft F t1  It is the new complete information tuples revealed at the end of round t.
Performance criterion. Under the frequentist setting  assume there exists an unknown true param-
eter ✓⇤ 2 Rd. The agent’s strategy can be evaluated by comparing her rewards to the best reward. To
do so  deﬁne the optimal action at round t by a⇤t = arg maxa2[K] g(x0t a✓⇤). Then  the agent’s total
regret of following strategy ⇡ can be expressed as follows

RT (⇡) :=

TXt=1⇣g⇣x0t a⇤t

✓⇤⌘  gx0t at✓⇤⌘  

where at ⇠ ⇡t and policy ⇡t maps Ht to the probability simplex K := {(p1 ···   pK) | PK
i=1 pi =
1  pi  0}. Note that RT (⇡) is in general a random variable due to the possible randomness in ⇡.

4

Assumptions. Through out the paper  we assume the following assumption on distribution  and
function g  which is standard in the generalized linear bandit literature (Filippi et al. (2010); Li et al.
(2017); Jun et al. (2017)).
Assumption 1 (GLCB).

• min(E[ 1

KPa2[K] xt ax0t a])  2

0 for all t 2 [T ].

•  := inf{kxk1 k✓✓⇤k1} ˙g(x0✓) > 0.
• g is twice differentiable. ˙g and ¨g are upper bounded by Lg and Mg  respectively.
t=1 satisﬁes the following assumption.

In addition  we assume the delay sequence {Dt}T
Assumption 2 (Delay). Assume {Dt}T
t=1 are independent non-negative random variables with tail-
envelope distribution (⇠D  µD  MD). That is  there exists a constant MD > 0 and a distribution ⇠D
with mean µD < 1 such that for any m  MD and t 2 [T ] 

where D ⇠ ⇠D and E[D] = µD. Furthermore  assume there exists q > 0 such that

P(Dt  m)  P(D  m) 
P(D  µD  x)  exp✓x1+q
D ◆ .

22

P(Di  E[Di]  x) ✓x1+q
D ◆  

2˜2

Note that when q = 1  D is sub-Gaussian with parameter D. When q 2 (0  1)  D has near-heavy
tail distribution. When Di’s are iid  the following condition guarantees Assumption 2:

with some ˜D > 0 and q > 0.
For ease of reference (as there are many ﬂoating parameters in this paper)  we summarize all the
parameter deﬁnitions in Table 1.

Notation Deﬁnition

Notation Deﬁnition

K number of arms
d

✓⇤

Lg
Mg
2
0

feature dimension
inf{kxk1 k✓✓⇤k1} ˙g(x0✓)
unknown true parameter
sub-Gaussian parameter for ✏t
upper bound on ˙g
upper bound on ¨g
lower bound on
min(E[ 1

KPa2[K] xt ax0t a])

⇠D tail-envelope distribution for the delays

q

parameter of ⇠D
µD expectation of ⇠D
MD parameter of ⇠D
D parameter of ⇠D
G
µ0D expectation of iid delays

sub-Gaussian parameter of Gt

Dmax

upper bound on bounded delays

Table 1: Parameters in the GLCB model with delays.

3 Delayed Upper Conﬁdence Bound (DUCB) for GLCB

In this section  we propose a UCB type of algorithm for GLCB  adapting the delay information in an
online version. Let us ﬁrst deﬁne some variables and state the main algorithm.

3.1 Algorithm: DUCB-GLCB

Denote Gt =Pt1
s=1 I{s + Ds  t} as the number of missing reward when the agent is making a
prediction at round t. Denote Tt = {s : s  t  1  Ds + s  t  1} as the set containing timestamps
with complete information tuples at the beginning of round t. Further denote Wt =Ps2Tt
XsX0s as
the matrix consisting feature information with timestamps in Tt and Vt =Pt1
s=1 XsX0s as the matrix
consisting all available features at the end of round t  1. The main algorithm is given below.

5

Algorithm 1 DUCB-GLCB
1: Input: the total rounds T   model parameters d and   and tuning parameters ⌧ and .

i=1 XsX0s  T⌧ +1 := {s :

2: Initialization: randomly choose ↵t 2 [K] for t 2 [⌧ ]  set V⌧ +1 =P⌧
s  ⌧  s + Ds  ⌧}  G⌧ +1 = ⌧ | T⌧ +1| and W⌧ +1 =Ps2T⌧ +1
3: for t = ⌧ + 1 ⌧ + 2 ···   T do
Update Statistics: calculate the MLE ˆ✓t by solvingPs2Tt
(Ys  g(X0s✓))Xs = 0
4:
r d
2 log⇣1 + 2(tGt)
⌘ + log( 1
 ) + pGt
Select Action: choose at = arg maxa2[K]⇣x0t a
t ⌘
ˆ✓t + tkxt akV 1
Update Observations: Xt xt at  Vt+1 Vt + XtX0t and Tt+1 Tt [{ s : s + Ds = t} 
Gt+1 = t | Tt+1|  and W⌧ +1 = W⌧ +Ps:s+Ds=t XsX0s

Update Parameter: t = 

8: end for

XsX0s

6:
7:

5:

d

Remark 2. In step 4  we use Maximum Likelihood Estimators (MLEs) for the parameter estimation
step at each round t. For more details on the derivation and explanation  we refer to the appendix.
Remark 3 (Comparison to UCB-GLM Algorithm in Li et al. (2017)). We make several adjustments
to the UCB-GLM Algorithm in Li et al. (2017). First  in step 4 (statistics update)  we only use data
with timestamps in Tt to calculate the estimator using MLE. In this step  using data without reward
will cause bias in the estimation. Second  when selecting the action in step 5  parameter t is updated
adaptively at each round whereas in Li et al. (2017)  the corresponding parameter is constant over
time. Moreover  in step 4  we choose to use Vt to normalize the context vector Xt a instead of Wt.

3.2 Preliminary Analysis
Denote G⇤t = max1st Gs as the running maximum number of missing reward up to round t. The
property of Gt and G⇤t is the key to analyze the regret bound for UCB algorithm. We next characterize
the tail behavior of Gt and G⇤t .
Proposition 1 (Properties of Gt and G?

t ). Assume Assumption 2. Denote G =q I

4 + 2

D(1+q)

with

q

D

I = max⇢ 1+qp2 log(2)2

1+q + 1. Then 
D  qq 22
1. Gt is sub-Gaussian. Moreover  for all t  1 
G◆ .
P (Gt  2(µD + MD) + x)  exp✓x2
G⇤T  2(µD + MD) + Gp2 log(T ) + Gs2 log✓ 1
◆ 

2. With probability 1   

22

where G⇤T = max1sT Gs.

(5)

(6)

3. Deﬁne Wt =Ps2Tt
XsX0s where Xt is drawn iid. from some distribution  with support
in the unit ball Bd. Furthermore  let ⌃:= E[XtX0t] be the second moment matrix  and B
and > 0 be two positive constants. Then there exist positive  universal constants C1 and
C2 such that min(Wt)  B with probability at least 1  2  as long as
t 0@

+ 2(µD + MD) + Gs2 log✓ 1
◆.

C1pd + C2qlog( 1

1A

min(⌃)

min(⌃)

(7)

2B

 )

+

2

The proof of Proposition 1 is deferred to the appendix. This Note that Gt is sub-Gaussian even when
D has near-heavy tail distribution when p 2 (0  1).

6

3.3 Regret Bounds

d

r d
2 log⇣1 + 2(tGt)

and t = 
of the algorithm is upper bounded by

Theorem 2. Assume Assumptions 1-2. Fix any . There exists a universal constant C :=
C(C1  C2  MD  µD  0  G   ) > 0  such that if we run DUCB-GLCB with ⌧ := Cd + log( 1
 )
⌘ + log( 1
 ) +pGt  then  with probability at least 1 5  the regret
◆◆1/4sd log✓ T
RT  ⌧ + Lg"4pµD + MDsT d log✓ T
d◆ T
+ 27/4pG (log (T ))1/4sd log✓ T
d◆pT# .

d◆ + 27/4pG✓log✓ 1
d◆ T +
log✓ T

2d


(8)

For parameter deﬁnition  we refer to Table 1.The proof of Theorem 2 is deferred to the appendix.
Corollary 3 (Expected regret). Assume Assumptions 1-2. The expected regret is bounded by

E[RT ] = O⇣dpT log(T ) +pµD + MDpT d log (T ) + pGpT d (log(T ))3/4⌘ .

(9)

Given the result in (8)  (9) holds by choosing  = 1
The highest order term O(dpT log(T )) does not depend on delays. Delay impacts the expected
regret bound in two folds. First  the sub-Gaussian parameter G appears in the second-highest order
term. Second  the mean-related parameter µD + MD appears in the third-order term. Note that here
we include the log factors in deciding the highest order term  the second higest order term and so on.
If we exclude the log terms  then both delay parameters impact the regret bound multiplicatively.

T and using the fact that RT  T .

3.4 Tighter Regret Bounds for Special Cases
When the sequence {Ds}T
probability bounds on the regret.
Proposition 4. Under Assumption 1  we have:

s=1 satisﬁes some speciﬁc assumptions  we are able to provide tighter high

(10)

2d


1. If there exists a constant Dmax > 0 such that P(Ds  Dmax) = 1 for all s 2 [T ]. Fix .
 )) 

There exists a universal constant C > 0 such that by taking ⌧ = Dmax + C(d + log( 1
with probability 1  3  the regret of the algorithm is upper bounded by

d◆pT! .

RT  ⌧ + Lg 2pDmaxs2T d log✓ T
d◆ +

log✓ T
Therefore  E[RT ] = O⇣pDmaxpdT log(T ) + dpT log(T )⌘ .
2. Assume D1 ···   DT are iid non-negative random variables with mean µ0D that satisfy
Assumption (2). There exists C > 0 such that by taking ⌧ := Cd + log( 1
 )  with
probability 1  5  the regret of the algorithm is upper bounded by
◆◆1/4sd log✓ T
RT  ⌧ + Lg"4qµ0DsT d log✓ T
d◆ + 27/4pG✓log✓ 1
d◆ T
d◆pT# .
log✓ T
Therefore  E[RT ] = O⇣⇣pµ0D + pG log (T )3/4⌘pT d + d log (T )pT⌘

+ 27/4pG (log (T ))1/4sd log✓ T

d◆ T +

2d


7

4 Tighter Regret Bounds on Linear Contextual Bandits with Finite Actions

We now consider the important special case of linear contextual bandits. and tighten the O(d)
dependence from previous bounds to O(pd). This requires two new elements that we incorporate
into DUCB-GLCB in Algorithm 1. First  instead of using MLE which is unbiased  here we use an
unbiased estimator that incorporates all the contexts (including those contexts for which the rewards
have not been received). In the linear contextual bandits setting  one can obtain analytical formulas
for the estimation procedure. Second  we extend the Sup-Base UCB decomposition framework (ﬁrst
devised in Auer (2002) and subsequently adapted in Chu et al. (2011); Li et al. (2017)) to the current
setting in order to resolve the reward dependency issue. This framework is a commonly used one in
the literature that deals with the dependency issue  and provides a O(pdT ) regret bound instead of a
O(dpT ) regret bound. Here we adapt this framework in the delayed reward setting.
In summary  the algorithm has two components  Delayed BaseLinUCB (Algorithm 2) and Delayed
SupLinUCB (Algorithm 3). Delayed BaseLinUCB performs estimation and the conﬁdence bound
computation  using a subset t of the past time steps as opposed to the set of all past time steps (note
that when t = 1  the chosen subset t is necessarily the empty set). This subset is carefully chosen
in Delayed SupLinUCB to make sure rewards are indepenent when conditioned on the past selected
contexts. Delayed SupLinUCB is further responsible for selecting an action at each time step.

Algorithm 2 Delayed BaseLinUCB at Step t
1: Input: t ⇢{ 1  2 ···   t  1}.
2: At = Id +P⌧2 t
xt a⌧ x0t a⌧
3: ct =P⌧2 t 1(D⌧ + ⌧  t  1)y⌧ a⌧ x⌧ a⌧
4: ✓t = A1
5: Observe K arm features  xt 1  xt 2 ···   xt K 2 Rd
6: for a 2 [K] do
wt a = ↵tqxT
7:
8:
9: end for

ˆyt a ✓T

t aA1

t xt a

t xt a

t ct

Algorithm 3 Delayed SupLinUCB
1: Input: T 2 N  S log(T )
1 ; for all s 2 [T ]
2: s
3: for t = 1  2 ···   T do
s 1 and ˆA1 [K]
4:
repeat
5:
Use Delayed BaseLinUCB with s
6:
bound  ˆys
if ws

t a + ws

t a  for all a 2 ˆAs
t a  1/pT for all a 2 ˆAs then
Choose at = arg maxa2 ˆAsˆys
t a  2s for all a 2 ˆAs then
else if ws
ˆAs+1 { a 2 ˆAs | ˆys
t a + ws
else
Choose at 2 ˆAs such that ws
s0

7:
8:
9:
10:
11:
12:

end if

13:
14:
15: end for

until an action at is found.

8

t to calculate the width  ws

t a  and upper conﬁdence

t a + ws

t a  Update s0

t+1 s0

t for all s0 2 [S].
t a0)  21s}  s s + 1

t a0 + ws

(ˆys

t a  maxa02 ˆAs
t at > 2s  Update
s0
t  [{t}
s0
t  

t+1 ⇢

if s = s0
otherwise

Remark 4. There are two modiﬁcations compared to Algorithm 2 in Chu et al. (2011). First  the
estimator ✓t (in step 4) is a biased estimator. We use all the features in matrix At and only use
features with observed rewards in vector ct. In particular  when the indicator 1(D⌧ + ⌧  t  1)
evaluates to 1  the reward corresponding to the action taken at time step ⌧ has been received by
the end of (and possibly prior to) t  1 (and hence available at the beginning of t); all the other
rewards (i.e. those that have not been received by t  1) are excluded. In comparison  Chu et al.
(2011) construct an unbiased estimator in each time step. Second  the width parameter ↵t (in step 7)
is time-dependent and adapts to new information (based on the delays) in each round. In comparison 
the width parameter is constant in Chu et al. (2011) that only depends on the horizon T .
Theorem 5 (Regret on Delayed SupLinUCB-BaseLinUCB). If Delayed SupLinUCB is run with

of the algorithm is

↵t = ¯↵ + Gt + 1  where ¯↵ =r 1
O pT d (G + 1) log3/2(



2 ln⇣ 2T K log(T )



T K log T

) + log(

⌘  then with probability at least 1  2  the regret
)!! . (11)

)(1 + µD + MD + Grlog

1




T K log T

The proof of Theorem 5 requires of modiﬁcation of two lemmas in Chu et al. (2011). Lemma 6 is
a modiﬁcation of (Chu et al.  2011  Lemma 1) and Lemma 7 is a modiﬁcation of (Chu et al.  2011 
Lemma 6). We defer the detailed proofs of Lemmas 6-7 to the appendix. Proof of Theorem 5 is also
given in the appendix.
In the regret bound (11)  the delay parameters (µD  MD  D) appear on the highest order term pT d.
Although the highest order term pT d is removed from (8)  the delay on order O(pT d) is essential
and this is also true for (8).
Lemma 6. Suppose the input index set t in Delayed BaseLinUCB is constructed so that for ﬁxed
x⌧ a⌧ with ⌧ 2 t  the rewards y⌧ a⌧ are independent random variables with means E[y⌧ a⌧ ] =
x0⌧ a⌧ ✓⇤. Suppose {Gt} is ﬁxed and given. Then  with probability at least 1  /T   we have for all
a 2 [K] that

ln✓ 2T K
Lemma 7. Assume G⇤T is ﬁxed and given. For all s 2 [S] 

|ˆyt a  x0t a✓⇤| 1 +s 1

2

 ◆ + Gt! st a.

 s

T +1  5 · 2s⇣p2¯↵(G⇤T + ¯↵)⌘qd| s

T +1|

Remark 5 (Why Assumption 1 can be dropped in Theorem 5). There are essentially two methods to
s=1 XsX0s). One method is to randomly sample actions
for ⌧ rounds. In this way  (Li et al.  2017  Proposition 1) guarantees a positive lower bound on
s=1 XsX0s). This is the method adopted in Algorithm 1 and Theorem 2. The other method
adds a regularization term. This is adopted in the deﬁnition of At (See Algorithm 2 and Theorem 5).
This method corresponds to the Ridge regression when estimating parameter ✓t.

guarantee a positive lower bound min(Pt
min(Pt

5 Conclusion

Beyond contextual bandits and looking at the broader landscape of data-driven decision making  de-
lays have emerged to be an important phenomenon in several domains  including  among other things 
distributed stochastic optimization (Bertsekas and Tsitsiklis (1997); Zhou et al. (2018b))  multi-
agent game-theoretical and reinforcement learning (Zhou et al. (2017); Grover et al. (2018a); Guo
et al. (2019); Mertikopoulos and Zhou (2019))  real-time scheduling in large-scale systems (Pinedo;
Mehdian et al. (2017); Mahdian et al. (2018)). Data-driven decision making with imperfect infor-
mation is an emerging research paradigm and much remains to be understood in regards to how
decision-making needs to be adapted in the presence of delays.

9

References
Abeille  M.  Lazaric  A.  et al. (2017). Linear thompson sampling revisited. Electronic Journal of

Statistics  11(2):5165–5197.

Agrawal  S. and Goyal  N. (2013a). Further optimal regret bounds for thompson sampling. In

Artiﬁcial intelligence and statistics  pages 99–107.

Agrawal  S. and Goyal  N. (2013b). Thompson sampling for contextual bandits with linear payoffs.

In International Conference on Machine Learning  pages 127–135.

Auer  P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3(Nov):397–422.

Bastani  H. and Bayati  M. (2015). Online decision-making with high-dimensional covariates.

Bertsekas  D. P. and Tsitsiklis  J. N. (1997). Parallel and distributed computation: Numerical methods.

Bistritz  I.  Zhou  Z.  Chen  X.  Bambos  N.  and Blanchet  J. (2019). Online exp3 learning in
adversarial bandits with delayed feedback. In Advances in Neural Information Processing Systems.

Bubeck  S.  Cesa-Bianchi  N.  et al. (2012). Regret analysis of stochastic and nonstochastic multi-

armed bandit problems. Foundations and Trends R in Machine Learning  5(1):1–122.

Cesa-Bianchi  N.  Gentile  C.  and Mansour  Y. (2019). Delay and cooperation in nonstochastic

bandits. The Journal of Machine Learning Research  20(1):613–650.

Chapelle  O. (2014). Modeling delayed feedback in display advertising. In Proceedings of the
20th ACM SIGKDD international conference on Knowledge discovery and data mining  pages
1097–1105. ACM.

Chen  K.  Hu  I.  Ying  Z.  et al. (1999). Strong consistency of maximum quasi-likelihood estimators
in generalized linear models with ﬁxed and adaptive designs. The Annals of Statistics  27(4):1155–
1163.

Chow  S.-C. and Chang  M. (2011). Adaptive design methods in clinical trials. Chapman and

Hall/CRC.

Chu  W.  Li  L.  Reyzin  L.  and Schapire  R. (2011). Contextual bandits with linear payoff functions.
In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics 
pages 208–214.

Desautels  T.  Krause  A.  and Burdick  J. W. (2014). Parallelizing exploration-exploitation tradeoffs
in gaussian process bandit optimization. The Journal of Machine Learning Research  15(1):3873–
3923.

Dudik  M.  Hsu  D.  Kale  S.  Karampatziakis  N.  Langford  J.  Reyzin  L.  and Zhang  T. (2011).

Efﬁcient optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369.

Filippi  S.  Cappe  O.  Garivier  A.  and Szepesvári  C. (2010). Parametric bandits: The generalized

linear case. In Advances in Neural Information Processing Systems  pages 586–594.

Garrabrant  S.  Soares  N.  and Taylor  J. (2016). Asymptotic convergence in online learning with

unbounded delays. arXiv preprint arXiv:1604.05280.

Grover  A.  Al-Shedivat  M.  Gupta  J. K.  Burda  Y.  and Edwards  H. (2018a). Learning policy

representations in multiagent systems. arXiv preprint arXiv:1806.06464.

Grover  A.  Markov  T.  Attia  P.  Jin  N.  Perkins  N.  Cheong  B.  Chen  M.  Yang  Z.  Harris  S. 
Chueh  W.  et al. (2018b). Best arm identiﬁcation in multi-armed bandits with delayed feedback.
arXiv preprint arXiv:1803.10937.

Guo  X.  Hu  A.  Xu  R.  and Zhang  J. (2019). Learning mean-ﬁeld games. In Advances in Neural

Information Processing Systems.

10

Joulani  P.  Gyorgy  A.  and Szepesvári  C. (2013). Online learning under delayed feedback. In

International Conference on Machine Learning  pages 1453–1461.

Jun  K.-S.  Bhargava  A.  Nowak  R.  and Willett  R. (2017). Scalable generalized linear bandits:
Online computation and hashing. In Advances in Neural Information Processing Systems  pages
99–109.

Kannan  P.  Chang  A.-M.  and Whinston  A. B. (2001). Wireless commerce: marketing issues
and possibilities. In Proceedings of the 34th Annual Hawaii International Conference on System
Sciences  pages 6–pp. IEEE.

Kitagawa  T. and Tetenov  A. (2018). Who should be treated? empirical welfare maximization

methods for treatment choice. Econometrica  86(2):591–616.

Li  L.  Chu  W.  Langford  J.  and Schapire  R. E. (2010). A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide
web  pages 661–670. ACM.

Li  L.  Lu  Y.  and Zhou  D. (2017). Provably optimal algorithms for generalized linear contextual
bandits. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 2071–2080. JMLR. org.

Mahdian  S.  Zhou  Z.  and Bambos  N. (2018). Robustness of join-the-shortest-queue scheduling to
communication delay. In 2018 Annual American Control Conference (ACC)  pages 3708–3713.
IEEE.

Mandel  T.  Liu  Y.-E.  Brunskill  E.  and Popovi´c  Z. (2015). The queue method: Handling delay 
heuristics  prior data  and evaluation in bandits. In Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence.

McCullagh  P. (2018). Generalized linear models. Routledge.

Mehdian  S.  Zhou  Z.  and Bambos  N. (2017). Join-the-shortest-queue scheduling with delay. In

2017 American Control Conference (ACC)  pages 1747–1752. IEEE.

Mertikopoulos  P. and Zhou  Z. (2019). Learning in games with continuous action sets and unknown

payoff functions. Mathematical Programming  173(1-2):465–507.

Mesterharm  C. (2005). On-line learning with delayed label feedback. In International Conference

on Algorithmic Learning Theory  pages 399–413. Springer.

Nelder  J. A. and Wedderburn  R. W. (1972). Generalized linear models. Journal of the Royal

Statistical Society: Series A (General)  135(3):370–384.

Neu  G.  Antos  A.  György  A.  and Szepesvári  C. (2010). Online markov decision processes under

bandit feedback. In Advances in Neural Information Processing Systems  pages 1804–1812.

Ostrovsky  E. and Sirota  L. (2014). Exact value for subgaussian norm of centered indicator random

variable. arXiv preprint arXiv:1405.6749.

Pike-Burke  C.  Agrawal  S.  Szepesvari  C.  and Grunewalder  S. (2017). Bandits with delayed 

aggregated anonymous feedback. arXiv preprint arXiv:1709.06853.

Pinedo  M. Scheduling  volume 29. Springer.

Quanrud  K. and Khashabi  D. (2015). Online learning with adversarial delays. In Advances in neural

information processing systems  pages 1270–1278.

Russo  D. and Van Roy  B. (2014). Learning to optimize via posterior sampling. Mathematics of

Operations Research  39(4):1221–1243.

Russo  D. and Van Roy  B. (2016). An information-theoretic analysis of thompson sampling. The

Journal of Machine Learning Research  17(1):2442–2471.

11

Schwartz  E. M.  Bradlow  E. T.  and Fader  P. S. (2017). Customer acquisition via display advertising

using multi-armed bandit experiments. Marketing Science  36(4):500–522.

Swaminathan  A. and Joachims  T. (2015). Batch learning from logged bandit feedback through

counterfactual risk minimization. Journal of Machine Learning Research  16(52):1731–1755.

Vernade  C.  Cappé  O.  and Perchet  V. (2017). Stochastic bandit models for delayed conversions.

arXiv preprint arXiv:1706.09186.

Vershynin  R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint

arXiv:1011.3027.

Wainwright  M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint  volume 48.

Cambridge University Press.

Weinberger  M. J. and Ordentlich  E. (2002). On delayed prediction of individual sequences. IEEE

Transactions on Information Theory  48(7):1959–1976.

Zhou  Z.  Athey  S.  and Wager  S. (2018a). Ofﬂine multi-action policy learning: Generalization and

optimization. arXiv preprint arXiv:1810.04778.

Zhou  Z.  Mertikopoulos  P.  Bambos  N.  Glynn  P.  Ye  Y.  Li  L.-J.  and Fei-Fei  L. (2018b).
In

Distributed asynchronous optimization with unbounded delays: How slow can you go?
International Conference on Machine Learning  pages 5965–5974.

Zhou  Z.  Mertikopoulos  P.  Bambos  N.  Glynn  P. W.  and Tomlin  C. (2017). Countering feedback
delays in multi-agent learning. In Advances in Neural Information Processing Systems  pages
6171–6181.

12

,Zhengyuan Zhou
Renyuan Xu
Jose Blanchet