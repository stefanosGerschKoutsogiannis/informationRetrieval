2010,Probabilistic Inference and Differential Privacy,We identify and investigate a strong connection between probabilistic inference and differential privacy  the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We find that probabilistic inference can improve accuracy  integrate multiple observations  measure uncertainty  and even provide posterior distributions over quantities that were not directly measured.,Probabilistic Inference and Differential Privacy

Oliver Williams
Microsoft Research

Mountain View  CA 94043

olliew@microsoft.com

Frank McSherry
Microsoft Research

Mountain View  CA 94043

mcsherry@microsoft.com

Abstract

We identify and investigate a strong connection between probabilistic inference
and differential privacy  the latter being a recent privacy deﬁnition that permits
only indirect observation of data through noisy measurement. Previous research
on differential privacy has focused on designing measurement processes whose
output is likely to be useful on its own. We consider the potential of applying
probabilistic inference to the measurements and measurement process to derive
posterior distributions over the data sets and model parameters thereof. We ﬁnd
that probabilistic inference can improve accuracy  integrate multiple observations 
measure uncertainty  and even provide posterior distributions over quantities that
were not directly measured.

1

Introduction

There has recently been signiﬁcant interest in the analysis of data sets whose individual records are
too sensitive to expose directly  examples of which include medical information  ﬁnancial data  and
personal data from social networking sites. Data like these are rich sources of information from
which models could be learned for a variety of important applications. Although agencies with the
resources to collate such data are unable to grant outside parties direct access to them  they may be
able to safely release aggregate statistics of the data set. Progress in this area has so far been driven
by researchers inventing sophisticated learning algorithms which are applied directly to the data and
output model parameters which can be proven to respect the privacy of the data set. Proving these
privacy properties requires an intricate analysis of each algorithm on a case-by-case basis. While this
does result in many valuable algorithms and results  it is not a scalable solution for two reasons: ﬁrst 
to solve a new learning problem  one must invent and analyze a new privacy-preserving algorithm;
second  one must then convince the owner of the data to run this algorithm. Both of these steps are
challenging.
In this paper  we show a natural connection between differential privacy  one of the leading privacy
deﬁnitions  and probabilistic inference. Speciﬁcally  differential privacy exposes the conditional dis-
tribution of its observable outputs given any input data set. Combining the conditional distributions
of differentially-private observations with generative models for the data permits new inferences
about the data without the need to invent and analyze new differentially-private computations. In
some cases  one can rely on previously reported differentially private measurements. When this is
not sufﬁcient  one can use off-the-shelf differentially-private “primitives” pre-sanctioned by owners
of the data. As well as this ﬂexibility  probabilistic inference can improve the accuracy of existing
approaches  provide a measure of uncertainty in any predictions made  combine multiple observa-
tions in a principled way  and integrate prior knowledge about the data or parameters.
The following section brieﬂy introduces differential privacy. In Section 3 we explore the marginal
likelihood of the differentially-private observations given generative model parameters for the data.
In general this likelihood consists of a high-dimensional integration over the space of all data sets 
however we show that for a rich subclass of differentially private computations this distribution can

1

be efﬁciently approximated via upper and lower bounds  derived using variational techniques. Sec-
tion 4 shows several experimental results validating our hypothesis that probabilistic inference can
be fruitfully applied to differentially-private computation. In particular  we show how the applica-
tion of principled  probabilistic inference to measurements made by an existing  heuristic algorithm
for logistic regression improves performance  as well as providing conﬁdence on the predictions
made.

1.1 Related work

There is a substantial amount of research on privacy  and differential privacy in particular  connected
with machine learning and statistics. Nonetheless  we are unaware of any research that uses exact
knowledge of the conditional distribution over outputs given inputs to perform inference over model
parameters  or other features of the data. Much of the existing statistical literature is concerned with
identifying cases when the differentially-private observations are “as good” as traditional statistical
estimators  in terms of efﬁciency [1]  power [2]  and minimax rates [3]  and also robust estimators
[4]. Instead  we are concerned with the cases where it is valuable to acknowledge and manage the
uncertainty in the observations. As we demonstrate experimentally  such cases abound.
Chaudhuri and Monteleoni [5  6] introduced the NIPS community to the problem of differentially-
private logistic regression. Although we will also consider the problem of logistic regression (and
compare our ﬁndings with theirs) we should stress that the aim of the paper is not speciﬁcally to
attack the problem of logistic regression. Rather  the problem serves as a good example where prior
work on differentially-private logistic regression can be improved through probabilistic inference.

2 Differential Privacy

Differential privacy [7] applies to randomized computations executed against a dataset and returning
an aggregate result for the entire set. It prevents inference about speciﬁc records by requiring that
the result of the computation yield nearly identical distributions for similar data sets. Formally  a
randomized computation M satisﬁes -differential privacy if for any two possible input data sets A
and B  and any subset of possible outputs S 

P (M (A) ∈ S) ≤ P (M (B) ∈ S) × exp( × |A (cid:9) B|)  

(1)
where A (cid:9) B is the set of records in A or B  but not both. When A (cid:9) B is small  the relative
bound on probabilities limits the inference an attacker can make about whether the true underlying
data were actually A or B. Inferences about the presence  absence  or speciﬁc values of individual
records are strongly constrained.
One example of a differentially private computation is the exponential mechanism[8]  characterized
by a function φ : Dn × R → R scoring each pair of data set and possible result with a real value.
When the φ function satisﬁes | ln φ(z  A)−ln φ(z  B)| ≤ |A(cid:9)B| for all z  the following distribution
satisﬁes 2-differential privacy:

(cid:80)

φ(z  X)
z(cid:48)∈Z φ(z(cid:48)  X)

P (M (X) = z) =

(2)

the φ functions can be expressed as φ(z  X) =(cid:81)

The exponential mechanism is fully general for differential privacy; any differentially-private mech-
anism M can be encoded in a φ function using the density of M (X) at z.
While any differentially-private mechanism can be expressed as a φ function  verifying that a func-
tion φ satisﬁes the constraint | ln φ(z  A)− ln φ(z  B)| ≤ |A(cid:9) B| is generally not easy  and requires
some form of proof on a case by case basis. One that does not require a specialized proof  is when
i φ(z  xi). This subclass is useful practically  as
data providers can ensure differential privacy by clamping each φ(z  x) value to the range [e−1  e+1] 
without having to understand the φ function. We will refer to this subclass as the factored exponen-
tial mechanism.
As we can see from the deﬁnition of the exponential mechanism  a differentially-private mechanism
draws its guarantees from its inherent randomness  rather than from secrecy about its speciﬁcation.
Although differential privacy has many other redeeming features  it is this feature alone that we

2

(a)

(b)

Figure 1: Graphical models. (a) If the data X = {xi} are directly observable (shaded nodes)  the
canonical learning task is to infer the posterior over θ given a model relating X and θ. (b) In the
private setting  the data are not observable; instead we observe the private measurement z  related
to X by a known measurement process.

exploit in the remainder of the work. By the same token  although there are many other privacy
deﬁnitions with varying guarantees  we can apply inference to any deﬁnition exhibiting one key
feature: an explicit probabilistic relationship between the input data sets and output observations.

3

Inference and privacy

Differential privacy limits what can be inferred about a single record in a data set  but does not
directly limit inference about larger scale  aggregate properties of data sets. For example  many
tasks in machine learning and statistics infer global parameters describing a model of the data set
without explicit dependence on any single record  and we may still expect to be see a meaningful
relationship between differentially-private measurements and model parameters.
One way to model a data set is to propose a generative probabilistic model for the data p(X|θ).
In Figure 1(a) we show a graphical model for the common case  in which we seek to infer the
parameters θ given the observed iid data X = {xi}. In Figure 1(b) we see a graphical model for
the case considered in this paper  in which the data are not directly observed due to privacy. Instead 
information about X is revealed by the measurement z  which is generated from X according to a
known conditional distribution p(z|X)  for example as given in (2). We therefore reason about θ via
the marginal likelihood

(cid:90)

p(z|θ) =

dX p(X|θ)p(z|X).

(3)

Armed with the marginal likelihood  it is possible to bring all the techniques of probabilistic infer-
ence to bear. This will generally include adding a prior distribution over θ  and combining multiple
measurements to form a posterior

(cid:89)

p(θ|z1 . . . zm  π) = p(θ|π)

p(zj|θ)

(4)

j

where π stands for any non-private information about θ we may have available.
While this is superﬁcially clean  there is a problem: the integration in (3) is over the space of all
data sets and is therefore challenging to compute whenever it cannot be solved analytically. Section
4 will show some results in which we tackle this head-on via MCMC  however this only works for
data sets of moderate size. Therefore  the remainder of this section is devoted to the development of
several bounds on the marginal likelihood for cases in which the measurement is generated via the
factored exponential mechanism. These bounds can be computed without requiring an integration
over all X.

3.1 Factored exponential mechanism

independence in p(X|θ) = (cid:81)

The factored exponential mechanism of Section 2 is a special case of differentially-private mech-
anism that admits efﬁcient approximation of the marginal likelihood. We will be able to use the
i φ(z  xi) to factorize lower and upper

i p(xi|θ) and φ(z  X) = (cid:81)

3

θxii=1..nθxizi=1..nbounds on the integral (3)  resulting in a small number of integrals over only the domain of records 
rather than the domain of data sets. As we will see  the bounds are often quite tight.

(cid:32)(cid:88)

(cid:18)(cid:90)
(cid:18)(cid:90)

p(z|θ) ≥

z(cid:48)∈Z
p(z|θ) ≤ e−H[q]

dx p(x|θ)

φ(z(cid:48)  x)
φ(z  x)

(cid:81)

(cid:19)n(cid:33)−1

(cid:19)n

where the upper bound is deﬁned in terms of a variational distribution q(z) [9] such that(cid:80)

z∈ q(z) =
1. H[q] is the Shannon entropy of q. Notice that the integrations appearing in either bound are over
the space of a single record in a data set and not over the entire dataset as they were in (3).

z(cid:48)∈Z φ(z(cid:48)  x)q(z(cid:48))

dx p(x|θ)

φ(z  x)

(5a)

(5b)

To prove the lower bound  we will apply Jensen’s inequality with the function f (x) = 1/x to the
marginal likelihood of the exponential mechanism.

Proof of lower bound

(cid:90)

(cid:18)

(cid:80)

dX p(X|θ)

φ(z  X)
z(cid:48)∈Z φ(z(cid:48)  X)

which now factorizes  as

(cid:90)

(cid:90)

dx1

dx2 . . .

(cid:90)

(cid:89)

i

dxn

(cid:19)

(cid:32)(cid:90)
(cid:32)(cid:88)

z(cid:48)∈Z

≥

=

(cid:88)

dX p(X|θ)

z(cid:48)∈Z
dX p(X|θ)

φ(z(cid:48)  X)
φ(z  X)
φ(z(cid:48)  X)
φ(z  X)

(cid:33)−1
(cid:33)−1

(cid:90)
(cid:89)
(cid:18)(cid:90)

i

(cid:18)(cid:90)

p(xi|θ)

φ(z(cid:48)  xi)
φ(z  xi) =

=

φ(z(cid:48)  xi)
φ(z  xi)

(cid:19)n

dxi p(xi|θ)

dx p(x|θ)

φ(z(cid:48)  x)
φ(z  x)

(cid:19)

.

(cid:4)

distribution q(z)  and applying Jensen’s inequality with the function f (x) = log x.

z(cid:48)∈Z φ(z(cid:48)  X) in (2) by introducing a variational

Proof of upper bound

We can lower bound the normalizing term (cid:80)
(cid:88)

(cid:88)

φ(z(cid:48)  X) = exp log

z(cid:48)∈Z

q(z(cid:48))
q(z(cid:48))

z(cid:48)∈Z

≥ exp(H[q]) + exp

q(z(cid:48)) log φ(z(cid:48)  X)

(cid:33)

.

Applying this bound to the marginal likelihood gives us the bound

(cid:90)

dX p(X|θ)

φ(z  X)

(cid:80)
z(cid:48)∈Z φ(z(cid:48)  X) ≤ e−H[q]
= e−H[q]

= e−H[q]

φ(z(cid:48)  X)

(cid:32)(cid:88)

z(cid:48)∈Z

(cid:90)
(cid:90)
(cid:18)(cid:90)

dX p(X|θ)

(cid:18)

(cid:89)

dX

i

dx p(x|θ)

(cid:81)

(cid:81)

φ(z  X)

z(cid:48)∈Z φ(z(cid:48)  X)q(z(cid:48))
φ(z  xi)

(cid:81)

z(cid:48)∈Z φ(z(cid:48)  xi)q(z(cid:48))

(cid:19)

p(xi|θ)

(cid:19)n

φ(z  x)

z(cid:48)∈Z φ(z(cid:48)  x)q(z(cid:48))

.

(cid:4)

While the upper bound is true for any q distribution  the tightest bound is found for the q which
minimizes the bound.

4

(a)

(b)

Figure 2: Error in upper and lower bounds for coin-ﬂipping problem. (a) For each epsilon  we
plot the maximum across all θ of the error between the true distribution and each of the upper and
lower bounds is plotted. (b) For n = 100 and  = 0.5  we show the shape of the upper bound  lower
bound  and true distribution when differentially-private measurement returned was z = 0.7.

3.1.1 Chosing a φ function

The upper and lower bounds in (5) are true for any admissible φ function  but leave unanswered the
question of what to chose in this rˆole. In the absence of privacy we might try to ﬁnd a good ﬁt for
the parameters θ by maximum likelihood. In the private setting this is not possible because the data
are not directly observable  but the output of the factored exponential mechanism has a very similar
form:

(cid:89)
(cid:89)

i

i

(6a)

(6b)

Max likelihood:

θ∗ = arg max
θ∈Θ

p(xi|θ)

z∗ = noisy max
z∈Z

φ(z  xi)

Exp. mechanism:

(cid:80)

f (z)

where noisy maxz∈Z f (z) samples from
z(cid:48)∈Z f (z(cid:48)). By making the analogy between (6a) and (6b) 
we might let z range over elements of Θ (or a ﬁnite subset)  and take φ(z  xi) to be the likelihood of
xi under parameters z. The exponential mechanism is then likely to choose parameters z that ﬁt the
data well  informing us that the posterior over θ is likely in the vicinity of z. For φ to be admissible 
we must clamp very small values of φ up to 1/e  limiting the ability of very poorly ﬁt records to
inﬂuence our decisions strongly.

3.2 Evaluation of the bounds

To demonstrate the effectiveness of these bounds we consider a problem in which it is possible to
analytically compute the marginal likelihood. This is the case in which the database X contains
a set of Boolean values corresponding to independent samples from a Bernoulli distribution with
probability θ

p(x|θ) = θx(1 − θ)(1−x).

log 0.1) that is 

(7)
For our test  we took Z to be the nine multiples of 0.1 between 0.1 and 0.9  and log φ(z  xi) =
[log p(x|θ)]log 0.9
the log likelihood clammped such that φ(z  x) lies in the interval
[e−1  e+1]  as required by privacy.
We see in ﬁgure 2a that the error in both the upper and lower bounds  across the entire density
function  is essentially zero for small epsilon. As epsilon increases the bounds deteriorate  but we
are most interested in the case of small values of epsilon  where privacy guarantees are meaningfully
strong. Figure 2b shows the shape of the two bounds  and the true density between  for epsilon =
0.5. This large value was chosen as it is in the region for which the bounds are less tight and the
difference between the bounds and the truth can be seen.

5

Upper boundLower bound-4-3-2-10log10(epsilon)-0.02-0.0100.01Max. errorUpper boundActualLower bound00.20.40.60.8theta0.10.20.30.4p(z|theta)approximately minimized by setting q(z) ∝ exp(cid:0)n(cid:82) dx p(x|θ) log φ(z  x)(cid:1). In general  however 

The upper bound is deﬁned in terms of a variational distribution q. For these experiments q was

these (and other) test show that both bounds are equally good for reasonable values of  and we
therefore use the lower bound for the experiments in this paper  since it is simpler to compute.

4 Experiments

We consider two scenarios for the experimental validation of the utility of probabilistic inference.
First  we consider applying probabilistic inference to an existing differentially-private computation 
speciﬁcally a logistic regression heuristic taken from a suite of differentially-private algorithms. The
heuristic is not representable in the factored exponential mechanism  and as such we must attempt
to approximate the full integral over the space of data sets directly. In our second experiment  we
choose a problem and measurement process appropriate for the factored exponential mechanism 
principal components analysis  previously only ever addressed through noisy observation of the
covariance matrix.

4.1 Logistic Regression

To examine the potential of probabilistic inference to improve the quality of existing differentially-
private computations  we consider a heuristic algorithm for logistic regression included in the Pri-
vacy Integrated Queries distribution [10]. This heuristic uses a noisy sum primitive to repeatedly
compute and step in the direction of an approximate gradient. When the number of records is large
compared to the noise introduced  the approximate gradient is relatively accurate  and the algorithm
performs well. When the records are fewer or the privacy requirements demand more noise  its
performance suffers. Probabilistic inference has the potential to improve performance by properly
integrating the information extracted from the data across the multiple gradient measurements and
managing the uncertainty associated with the noisy measurements.
We test our proposals against three synthetic data sets (CM1 and CM2 from [5] and one of our own:
SYNTH) and two data sets from the UCI repository (PIMA and ADULT) [11]. Details of these data
sets appear in table 4.1. The full ADULT data set was split into training and test sets  chosen so as
to force the marginal frequency of positive and negative examples to 50%.

SYNTH CM1

CM2

PIMA ADULT

Records
Dimensions
Positive examples
Test set records

1000

4
497
1000

17500

10
8770
17500

17500

10
8694
17500

691
8
237
767*

16000

6

7841
8000

Table 1: Data sets used and their statistics. Attribute values in SYNTH are sampled uniformly from
a hypercube of unit volume  centered at the origin. CM1 and CM2 are both sampled uniformly at
random for the surface of the unit hypersphere in 10 dimensions; CM1 is linearly separable  whereas
CM2 is not (see [5]). PIMA and ADULT are standard data sets [11] containing diabetes records 
and census data respectively  both of which correspond to the types of data one might expect to be
protected by differential privacy. The total PIMA data set is so small that we reused the full data set
as test data (indicated by *).

4.1.1 Error Rates and Log-Likelihood

Tables 2 and 3 report the classiﬁcation accuracy of several approaches when the privacy parameter
 is set to 0.1 and 1.0 respectively. These results are computed from 50 executions of the heuristic
gradient descent algorithm.
We can see a trend of general improvement from the heuristic approach to the probabilistic inference 
both in terms of the average error rate and the standard deviation. For the CM1 and CM2 data sets
at epsilon = 0.1  we see substantial improvement over the reported results of [5]. Please note that

6

Heuristic
Inference
Benchmark
NIPS 08 [5]

SYNTH
37.40 ± 15.75
29.14 ± 5.54
16.40

CM1
3.93 ± 1.57
2.72 ± 0.84
0.00
14.26 ± 12.84

CM2
9.32 ± 1.18
8.84 ± 0.79
5.40
19.03 ± 11.05

PIMA
44.26 ± 8.50
45.70 ± 6.31
19.48

ADULT
43.15 ± 7.85
36.07 ± 6.32
26.09

Table 2: Error Rates with  = 0.1 All measurements are in per cent; errors are reported as the
mean ± one standard deviation computed from 50 independent executions with random starting
points. Heuristic corresponds to the last estimate made by noisy gradient ascent. Inference entries
correspond to the expected error  computed over the approximate posterior for θ found via MCMC.
Benchmark is the best maximum likelihood solution found by gradient ascent when the data are
directly observable and forms a baseline for expected performance. NIPS08 corresponds the the
results given in [5]; these values were copied from that paper and are provided for comparison.

Heuristic
Inference
Benchmark

SYNTH
17.31 ± 1.12
17.16 ± 0.94
16.40

CM1
0.00 ± 0.00
0.01 ± 0.02
0.00

CM2
5.67 ± 0.19
5.69 ± 0.13
5.40

PIMA
35.67 ± 6.45
36.47 ± 8.56
19.48

ADULT
31.30 ± 4.16
29.36 ± 1.31
26.09

Table 3: Error Rates with  = 1.0 All measurements are in per cent; see caption for table 2.

the experiments were run on different data than in [5] drawn from the same distribution  and that
different numbers of repetitions were used in [5] for the computation of the standard deviation and
mean.

4.1.2 Exchanging Iterations for Accuracy

The heuristic gradient ascent algorithm has an important conﬁguration parameter determining the
number of iterations of ascent  and consequently the accuracy permitted in each round (which must
be lower if more rounds are to be run  to keep the cumulative privacy cost constant). The perfor-
mance of the algorithm can be very sensitive to this parameter  as too few iterations indicate too little
about the data  and too many render each iteration meaningless. In Figure 3a we consider several
parameterizations of the heuristic  taking varying numbers of steps with varying degrees of accu-
racy in each step. Each colored path describes an execution with a ﬁxed level of accuracy in each
iteration  and all are plotted on the common scale of total privacy consumption. All of these paths
roughly describe a common curve  suggesting that careful conﬁguration is not required for these ap-
proaches: probabilistic inference appears to extract an amount of information that depends mainly
on the total privacy consumption  and less on the speciﬁc details of its collection. This experiment
was performed on the CM2 data set and the corresponding result from [5] is indicated by the ‘X’.

4.1.3 Integrating Auxiliary Information

To further demonstrate the power of the probabilistic inference approach  we consider the plausible
scenario in which we are provided with a limited number of additional data points  obtained without
privacy protection (for example  if we independently run a small survey of our own). These addi-
tional samples are easily incorporated into the graphical model by adding them as descendants of θ
in ﬁgure 1b. Figure 3b shows how the performance on SYNTH (which contains 1000 data points)
improves  as the quantity of additional examples increases. Even with very few additional examples 
probabilistic inference is capable of exploiting this information and performance improves dramati-
cally.

4.2 Principal components

To demonstrate inference on another model  and to highlight the applicability of the factored expo-
nential mechanism  we consider the problem of probabilistically ﬁnding the ﬁrst principal compo-

7

(a)

(b)

Figure 3: (a) Paths of varying . (b) Incorporating non-private observations A compelling beneﬁt
of probabilistic inference is how easily alternate sources of information are added. The horizontal
line indicates the performance of the benchmark maximum likelihood solution computed from the
data without privacy.

 = 0.003

 = 0.01

 = 0.1

Figure 4: Posterior distribution as a function of  The same synthetic data set under differentially-
private measurements with varying epsilon. For each measurement  1000 samples of the full pos-
terior over θ are drawn and overlaid on this ﬁgure to indicate the modes and concentration of the
density. The posterior is noticeably more concentrated and accurate as epsilon increases.

nent of a data set where we model the data as iid draws from a Gaussian

p(x|θ) = N (0  θθT + σ2I).

(8)

An important advantage of our approach is its ability to capture uncertainty in the parameters and
act accordingly. Figure 4 demonstrates three instances of inference applied to the same data set with
three different values of . As  increases  the concentration of the posterior over the parameters
increases. We stress that the posterior and its concentration are returned to the analyst; each image
is the result of a single differentially-private measurement  rather than a visualization of multiple
runs. The measurement associated with  = 0.003 is revealing as it corresponds to the off-axis
mode of the posterior. Although centered on this incorrect answer  the posterior indicates lack of
conﬁdence  and there is non-negligible mass over the correct answer.

5 Conclusions

Most work in the area of learning from private data forms an intrinsic analysis. That is  a complex
algorithm is run by the owner of the data  directly on that data  and a single output is produced which
appropriately indicates the desired parameters (modulo noise). In contrast  this paper has shown that
it is possible to do a great deal with an extrinsic analysis  where standard  primitive  measurements
are made against the data  and a posterior over model parameters is inferred post hoc.
This paper brings together two complementary lines of research:
the design and analysis of
differentially-private algorithms  and probabilistic inference. Our primary goal is not to weigh-in
on new differentially-private algorithms  nor to ﬁnd new methods for probabilistic inferences – it is
to present the observation that the two approaches are complementary in way that can be mutually
enriching.

8

-2.5-2-1.5-1-0.50log10(epsilon)00.20.4Error rate0100200300400Public data0.150.20.250.3Error rate-50510-50510-50510-50510-50510-50510References
[1] A. Smith. Efﬁcient  differentially private point estimators. 2008. arXiv:0809.4794.
[2] A. Slavkovic and D. Vu. Differential privacy for clinical trial data: Preliminary evaluations.
In Proceedings of the International workshop on Privacy Aspects of Data Mining  PADM09 
2009.

[3] L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the

American Statistical Association  105(489):375–389  2010.

[4] C. Dwork and J. Lei. Differential privacy and robust statistics. In STOC  2009.
[5] K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression. In NIPS  pages 289–

296  2008.

[6] K. Chaudhuri  C. Monteleoni  and A.D. Sarwate. Differentially private empirical risk mini-

mization. 2010.

[7] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private

data analysis. In TCC  pages 265–284  2006.

[8] F. McSherry and K. Talwar. Differential privacy via mechanism design. In FOCS  2007.
[9] M.I. Jordan  Z. Ghahramani  T. Jaakkola  and L.K. Saul. An introduction to variational meth-

ods for graphical models. Machine Learning  37(2):183–233  1999.

[10] F. McSherry. Privacy integrated queries. In ACM SIGMOD  2009.
[11] A. Asuncion and D.J. Newman. UCI machine learning repository  2007.

9

,Roger Grosse
Chris Maddison
Russ Salakhutdinov
Zhongwen Xu
Joseph Modayil
Hado van Hasselt
Andre Barreto
David Silver
Tom Schaul