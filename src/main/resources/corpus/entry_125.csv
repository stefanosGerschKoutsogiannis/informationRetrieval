2016,A Minimax Approach to Supervised Learning,Given a task of predicting Y from X  a loss function L  and a set of probability distributions Gamma on (X Y)  what is the optimal decision rule minimizing the worst-case expected loss over Gamma? In this paper  we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on X constrained to be the empirical marginal  we provide a minimax interpretation of the maximum likelihood problem over generalized linear models as well as some popular regularization schemes. For quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models. Moreover  for the 0-1 loss we derive a classifier which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed Gamma by solving a tractable optimization problem. We perform several numerical experiments to show the power of the minimax SVM in outperforming the SVM.,A Minimax Approach to Supervised Learning

Farzan Farnia∗

farnia@stanford.edu

David Tse∗

dntse@stanford.edu

Abstract

Given a task of predicting Y from X  a loss function L  and a set of probability
distributions Γ on (X  Y )  what is the optimal decision rule minimizing the worst-
case expected loss over Γ? In this paper  we address this question by introducing
a generalization of the maximum entropy principle. Applying this principle to
sets of distributions with marginal on X constrained to be the empirical marginal 
we provide a minimax interpretation of the maximum likelihood problem over
generalized linear models as well as some popular regularization schemes. For
quadratic and logarithmic loss functions we revisit well-known linear and logistic
regression models. Moreover  for the 0-1 loss we derive a classiﬁer which we
call the minimax SVM. The minimax SVM minimizes the worst-case expected
0-1 loss over the proposed Γ by solving a tractable optimization problem. We
perform several numerical experiments to show the power of the minimax SVM in
outperforming the SVM.

1

Introduction

Supervised learning  the task of inferring a function that predicts a target Y from a feature vector
X = (X1  . . .   Xd) by using n labeled training samples {(x1  y1)  . . .   (xn  yn)}  has been a problem
of central interest in machine learning. Given the underlying distribution ˜PX Y   the optimal prediction
rules had long been studied and formulated in the statistics literature. However  the advent of high-
dimensional problems raised this important question: What would be a good prediction rule when we
do not have enough samples to estimate the underlying distribution?
To understand the difﬁculty of learning in high-dimensional settings  consider a genome-based
classiﬁcation task where we seek to predict a binary trait of interest Y from an observation of
3  000  000 SNPs  each of which can be considered as a discrete variable Xi ∈ {0  1  2}. Hence  to
estimate the underlying distribution we need O(33 000 000) samples.
With no possibility of estimating the underlying ˜P in such problems  several approaches have been
proposed to deal with high-dimensional settings. The standard approach in statistical learning
theory is empirical risk minimization (ERM) [1]. ERM learns the prediction rule by minimizing an
approximated loss under the empirical distribution of samples. However  to avoid overﬁtting  ERM
restricts the set of allowable decision rules to a class of functions with limited complexity measured
through its VC-dimension.
This paper focuses on a complementary approach to ERM where one can learn the prediction rule
through minimizing a decision rule’s worst-case loss over a larger set of distributions Γ( ˆP ) centered
at the empirical distribution ˆP . In other words  instead of restricting the class of decision rules  we
consider and evaluate all possible decision rules  but based on a more stringent criterion that they will
have to perform well over all distributions in Γ( ˆP ). As seen in Figure 1  this minimax approach can
be broken into three main steps:

1. We compute the empirical distribution ˆP from the data 

∗Department of Electrical Engineering  Stanford University  Stanford  CA 94305.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: Minimax Approach

Figure 2: Minimax-hinge Loss

2. We form a distribution set Γ( ˆP ) based on ˆP  
3. We learn a prediction rule ψ∗ that minimizes the worst-case expected loss over Γ( ˆP ).

Some special cases of this minimax approach  which are based on learning a prediction rule from
low-order marginal/moments  have been addressed in the literature: [2] solves a robust minimax
classiﬁcation problem for continuous settings with ﬁxed ﬁrst and second-order moments; [3] develops
a classiﬁcation approach by minimizing the worst-case hinge loss subject to ﬁxed low-order marginals;
[4] ﬁts a model minimizing the maximal correlation under ﬁxed pairwise marginals to design a robust
classiﬁcation scheme. In this paper  we develop a general minimax approach for supervised learning
problems with arbitrary loss functions.
To formulate Step 3 in Figure 1  given a general loss function L and set of distribution Γ( ˆP ) we
generalize the problem formulation discussed at [3] to

E(cid:2) L(cid:0)Y  ψ(X)(cid:1)(cid:3) .

argmin

ψ∈Ψ

max
P∈Γ( ˆP )

(1)

Here  Ψ is the space of all decision rules. Notice the difference with the ERM problem where Ψ was
restricted to smaller function classes while Γ( ˆP ) = { ˆP}.
If we have to predict Y with no access to X  (1) will reduce to the formulation studied at [5]. There 
the authors propose to use the principle of maximum entropy [6]  for a generalized deﬁnition of
entropy  to ﬁnd the optimal prediction rule minimizing the worst-case expected loss. By the principle
of maximum entropy  we should predict based on a distribution in Γ( ˆP ) that maximizes the entropy
function.
How can we use the principle of maximum entropy to solve (1) when we observe X as well? A
natural idea is to apply the maximum entropy principle to the conditional PY |X=x instead of the
marginal PY . This idea motivates a generalized version of the principle of maximum entropy  which
we call the principle of maximum conditional entropy. In fact  this principle breaks Step 3 into two
smaller steps:

3a. We search for P ∗ the distribution maximizing the conditional entropy over Γ( ˆP ) 
3b. We ﬁnd ψ∗ the optimal decision rule for P ∗.

Although the principle of maximum conditional entropy characterizes the solution to (1)  computing
the maximizing distribution is hard in general. In [7]  the authors propose a conditional version of the
principle of maximum entropy  for the speciﬁc case of Shannon entropy  and draw the principle’s
connection to (1). They call it the principle of minimum mutual information  by which one should
predict based on the distribution minimizing mutual information among X and Y . However  they
develop their theory targeting a broad class of distribution sets  which results in a convex problem 
yet the number of variables is exponential in the dimension of the problem.
To overcome this issue  we propose a speciﬁc structure for the distribution set by matching the
marginal PX of all the joint distributions PX Y in Γ( ˆP ) to the empirical marginal ˆPX while matching
only the cross-moments between X and Y with those of the empirical distribution ˆPX Y. We show
that this choice of Γ( ˆP ) has two key advantages: 1) the minimax decision rule ψ∗ can be computed
efﬁciently; 2) the minimax generalization error can be controlled by allowing a level of uncertainty in
the matching of the cross-moments  which can be viewed as regularization in the minimax framework.
Our solution is achieved through convex duality. For some loss functions  the dual problem turns
out to be equivalent to the maximum likelihood problem for generalized linear models. For example 

2

under quadratic and logarithmic loss functions this minimax approach revisits the linear and logistic
regression models respectively.
On the other hand  for 0-1 loss  the minimax approach leads to a new randomized linear classiﬁer
which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss
over Γ( ˆP ) by solving a tractable optimization problem. In contrast  the classic ERM formulation of
minimizing the 0-1 loss over linear classiﬁers is well-known to be NP-hard [8]. Interestingly  the dual
problem for the 0-1 loss minimax problem corresponds also to an ERM problem for linear classiﬁers 
but with a loss function different from 0-1 loss. This loss function  which we call the minimax-hinge
loss  is also different from the classic hinge loss (Figure 2). We emphasize that while the hinge loss is
an adhoc surrogate loss function chosen to convexify the 0-1 loss ERM problem  the minimax-hinge
loss emerges from the minimax formulation. We also perform several numerical experiments to
demonstrate the power of the minimax SVM in outperforming the standard SVM which minimizes
the surrogate hinge loss.

2 Principle of Maximum Conditional Entropy

In this section  we provide a conditional version of the key deﬁnitions and results developed in [5].
We propose the principle of maximum conditional entropy to break Step 3 into 3a and 3b in Figure 1.
We also deﬁne and characterize Bayes decision rules for different loss functions to address Step 3b.

2.1 Decision Problems  Bayes Decision Rules  Conditional Entropy
Consider a decision problem. Here the decision maker observes X ∈ X from which she predicts a
random target variable Y ∈ Y using an action a ∈ A. Let PX Y = (PX   PY |X ) be the underlying
distribution for the random pair (X  Y ). Given a loss function L : Y ×A → [0 ∞]  L(y  a) indicates
the loss suffered by the decision maker by deciding action a when Y = y. The decision maker uses
a decision rule ψ : X → A to select an action a = ψ(x) from A based on an observation x ∈ X .
We will in general allow the decision rules to be random  i.e. ψ is random. The main purpose of
extending to the space of randomized decision rules is to form a convex set of decision rules. Later in
Theorem 1  this convexity is used to prove a saddle-point theorem.
We call a (randomized) decision rule ψBayes a Bayes decision rule if for all decision rules ψ and for
all x ∈ X :

E[L(Y  ψBayes(X))|X = x] ≤ E[L(Y  ψ(X))|X = x].

It should be noted that ψBayes depends only on PY |X  i.e. it remains a Bayes decision rule under a
different PX. The (unconditional) entropy of Y is deﬁned as [5]
E[L(Y  a)].

(2)

H(Y ) := inf
a∈A

Similarly  we can deﬁne conditional entropy of Y given X = x as

H(Y |X = x) := inf

E[L(Y  ψ(X))|X = x] 

ψ

(3)

(cid:88)

and the conditional entropy of Y given X as

H(Y |X) :=

(4)
Note that H(Y |X = x) and H(Y | X) are both concave in PY |X. Applying Jensen’s inequality  this
concavity implies that

ψ

x

PX (x)H(Y |X = x) = inf

E[L(Y  ψ(X))].

H(Y |X) ≤ H(Y ) 

which motivates the following deﬁnition for the information that X carries about Y  

I(X; Y ) := H(Y ) − H(Y |X) 

(5)
i.e. the reduction of expected loss in predicting Y by observing X. In [9]  the author has deﬁned
the same concept to which he calls a coherent dependence measure. It can be seen that I(X; Y ) =
EPX [ D(PY |X   PY ) ] where D is the divergence measure corresponding to the loss L  deﬁned for
any two probability distributions PY   QY with Bayes actions aP   aQ as [5]

D(PY   QY ) := EP [L(Y  aQ)] − EP [L(Y  aP )] = EP [L(Y  aQ)] − HP (Y ).

(6)

3

2.2 Examples

2.2.1 Logarithmic loss
For an outcome y ∈ Y and distribution QY   deﬁne logarithmic loss as Llog(y  QY ) = − log QY (y).
It can be seen Hlog(Y )  Hlog(Y |X)  Ilog(X; Y ) are the well-known unconditional  conditional
Shannon entropy and mutual information [10]. Also  the Bayes decision rule for a distribution PX Y
is given by ψBayes(x) = PY |X (·|x).
2.2.2 0-1 loss
The 0-1 loss function is deﬁned for any y  ˆy ∈ Y as L0-1(y  ˆy) = I(ˆy (cid:54)= y). Then  we can show

H0-1(Y ) = 1 − max

y∈Y PY (y)  H0-1(Y |X) = 1 −(cid:88)

max
y∈Y PX Y (x  y).

x∈X

The Bayes decision rule for a distribution PX Y is the well-known maximum a posteriori (MAP) rule 
i.e. ψBayes(x) = argmaxy∈Y PY |X (y|x).
2.2.3 Quadratic loss
The quadratic loss function is deﬁned as L2(y  ˆy) = (y − ˆy)2. It can be seen

H2(Y ) = Var(Y )  H2(Y |X) = E [Var(Y |X)] 

I2(X; Y ) = Var (E[Y |X]) .

The Bayes decision rule for any PX Y is the well-known minimum mean-square error (MMSE)
estimator that is ψBayes(x) = E[Y |X = x].
2.3 Principle of Maximum Conditional Entropy & Robust Bayes decision rules

Given a distribution set Γ  consider the following minimax problem to ﬁnd a decision rule minimizing
the worst-case expected loss over Γ

argmin

ψ∈Ψ

max
P∈Γ

EP [L(Y  ψ(X))] 

(7)

where Ψ is the space of all randomized mappings from X to A and EP denotes the expected value
over distribution P . We call any solution ψ∗ to the above problem a robust Bayes decision rule
against Γ. The following results motivate a generalization of the maximum entropy principle to ﬁnd a
robust Bayes decision rule. Refer to the supplementary material for the proofs.
Theorem 1.A. (Weak Version) Suppose Γ is convex and closed  and let L be a bounded loss function.
Assume X  Y are ﬁnite and that the risk set S = { [L(y  a)]y∈Y : a ∈ A} is closed. Then there
exists a robust Bayes decision rule ψ∗ against Γ  which is a Bayes decision rule for a distribution P ∗
that maximizes the conditional entropy H(Y |X) over Γ.
Theorem 1.B. (Strong Version) Suppose Γ is convex and that under any P ∈ Γ there exists a Bayes
decision rule. We also assume the continuity in Bayes decision rules for distributions in Γ (See the
supplementary material for the exact condition). Then  if P ∗ maximizes H(Y |X) over Γ  any Bayes
decision rule for P ∗ is a robust Bayes decision rule against Γ.

Principle of Maximum Conditional Entropy: Given a set of distributions Γ  predict Y based on a
distribution in Γ that maximizes the conditional entropy of Y given X  i.e.

H(Y |X)

argmax

P∈Γ

(8)

Note that while the weak version of Theorem 1 guarantees only the existence of a saddle point for
(7)  the strong version further guarantees that any Bayes decision rule of the maximizing distribution
results in a robust Bayes decision rule. However  the continuity in Bayes decision rules does not hold
for the discontinuous 0-1 loss  which requires considering the weak version of Theorem 1 to address
this issue.

4

3 Prediction via Maximum Conditional Entropy Principle

Consider a prediction task with target variable Y and feature vector X = (X1  . . .   Xd). We do not
require the variables to be discrete. As discussed earlier  the maximum conditional entropy principle
reduces (7) to (8)  which formulate steps 3 and 3a in Figure 1  respectively. However  a general
formulation of (8) in terms of the joint distribution PX Y leads to an exponential computational
complexity in the feature dimension d.
The key question is therefore under what structures of Γ( ˆP ) in Step 2 we can solve (8) efﬁciently. In
this section  we propose a speciﬁc structure for Γ( ˆP )  under which we provide an efﬁcient solution
to Steps 3a and 3b in Figure 1. In addition  we prove a bound on the excess worst-case risk for the
proposed Γ( ˆP ).
To describe this structure  consider a set of distributions Γ(Q) centered around a given distribution
QX Y   where for a given norm (cid:107) · (cid:107)  mapping vector θ(Y )t×1 

Γ(Q) = { PX Y : PX = QX  

∀ 1 ≤ i ≤ t : (cid:107) EP [θi(Y )X] − EQ [θi(Y )X](cid:107) ≤ i }.

(9)

Here θ encodes Y with t-dimensional θ(Y )  and θi(Y ) denotes the ith entry of θ(Y ). The ﬁrst
constraint in the deﬁnition of Γ(Q) requires all distributions in Γ(Q) to share the same marginal on
X as Q; the second imposes constraints on the cross-moments between X and Y   allowing for some
uncertainty in estimation. When applied to the supervised learning problem  we will choose Q to be
the empirical distribution ˆP and select θ appropriately based on the loss function L. However  for
now we will consider the problem of solving (8) over Γ(Q) for general Q and θ.
To that end  we use a similar technique as in the Fenchel’s duality theorem  also used at [11  12  13]
to address divergence minimization problems. However  we consider a different version of convex
conjugate for −H  which is deﬁned with respect to θ. Considering PY as the set of all probability
distributions for the variable Y   we deﬁne Fθ : Rt → R as the convex conjugate of −H(Y ) with
respect to the mapping θ 

Fθ(z) := max
P∈PY

H(Y ) + E[θ(Y )]T z.

(10)

Theorem 2. Deﬁne Γ(Q)  Fθ as given by (9)  (10). Then the following duality holds

H(Y |X) = min
A∈Rt×d

EQ

max
P∈Γ(Q)

(11)
where (cid:107)Ai(cid:107)∗ denotes (cid:107) · (cid:107)’s dual norm of the A’s ith row. Furthermore  for the optimal P ∗ and A∗
(12)

EP ∗ [ θ(Y )| X = x ] = ∇Fθ (A∗x).

(cid:2) Fθ(AX) − θ(Y )T AX(cid:3) +

t(cid:88)

i=1

i(cid:107)Ai(cid:107)∗ 

Proof. Refer to the the supplementary material for the proof.

When applying Theorem 2 on a supervised learning problem with a speciﬁc loss function  θ will be
chosen such that EP ∗ [ θ(Y )| X = x ] provides sufﬁcient information to compute the Bayes decision
rule Ψ∗ for P ∗. This enables the direct computation of ψ∗  i.e. step 3 of Figure 1  without the need
to explicitly compute P ∗ itself. For the loss functions discussed at Subsection 2.2  we choose the
identity θ(Y ) = Y for the quadratic loss and the one-hot encoding θ(Y ) = [ I(Y = i) ]t
i=1 for the
logarithmic and 0-1 loss functions. Later in this section  we will discuss how this theorem applies to
these loss functions.

3.1 Generalization Bounds for the Worst-case Risk

By establishing the objective’s Lipschitzness and boundedness through appropriate assumptions  we
can bound the rate of uniform convergence for the problem in the RHS of (11) [14]. Here we consider
the uniform convergence of the empirical averages  when Q = ˆPn is the empirical distribution of n
samples drawn i.i.d. from the underlying distribution ˜P   to their expectations when Q = ˜P .
In the supplementary material  we prove the following theorem which bounds the excess worst-case
risk. Here ˆψn and ˜ψ denote the robust Bayes decision rules against Γ( ˆPn) and Γ( ˜P )  respectively.

5

Figure 3: Duality of Maximum Conditional Entropy/Maximum Likelihood in GLMs

As explained earlier  by the maximum conditional entropy principle we can learn ˆψn by solving the
RHS of (11) for the empirical distribution of samples and then applying (12).
Theorem 3. Consider a loss function L with the entropy function H and suppose θ(Y ) includes
only one element  i.e. t = 1. Let M = maxP∈PY H(Y ) be the maximum entropy value over PY.
Also  take (cid:107) · (cid:107)/(cid:107) · (cid:107)∗ to be the (cid:96)p/(cid:96)q pair where 1
q = 1  1 ≤ q ≤ 2. Given that (cid:107)X(cid:107)2 ≤ B and
p + 1
|θ(Y )| ≤ L  for any δ > 0 with probability at least 1 − δ

(cid:18)

(cid:114)

(cid:19)

max
P∈Γ( ˜P )

E[L(Y  ˆψn(X))] − max
P∈Γ( ˜P )

E[L(Y  ˜ψ(X))] ≤ 4BLM
n

√



1 +

9 log(4/δ)

8

.

(13)

Theorem 3 states that though we learn the prediction rule ˆψn by solving the maximum conditional
problem for the empirical case  we can bound the excess Γ-based worst-case risk. This result justiﬁes
the speciﬁc constraint of ﬁxing the marginal PX across the proposed Γ(Q) and explains the role of
the uncertainty parameter  in bounding the excess worst-case risk.

3.2 A Minimax Interpretation of Generalized Linear Models

We make the key observation that if Fθ is the log-partition function of an expoenetial-family distribu-
tion  the problem in the RHS of (11)  when i = 0 for all i’s  is equivalent to minimizing the negative
log-likelihood for ﬁtting a generalized linear model [15] given by

• An exponential-family distribution p(y|η) = h(y) exp(cid:0)ηT θ(y) − Fθ(η)(cid:1) with the log-partition

function Fθ and the sufﬁcient statistic θ(Y ) 

• A linear predictor  η(X) = AX 
• A mean function  E[ θ(Y )|X = x] = ∇Fθ(η(x)).
Therefore  Theorem 2 reveals a duality between the maximum conditional entropy problem over
Γ(Q) and the regularized maximum likelihood problem for the speciﬁed generalized linear model.
As a geometric interpretation of this duality  by solving the regularized maximum likelihood problem
in the RHS of (11)  we in fact minimize a regularized KL-divergence

t(cid:88)

i=1

i(cid:107)Ai(PY |X)(cid:107)∗ 

EQX [ DKL( QY |X || PY |X ) ] +

argmin
PY |X∈SF

(14)
where SF = {PY |X(y|x) = h(y) exp( θ(y)T Ax−Fθ(Ax)| A ∈ Rt×s} is the set of all exponential-
family conditional distributions for the speciﬁed generalized linear model. This can be viewed as
projecting Q onto (QX   SF ) (See Figure 3).
Furthermore  for a label-invariant entropy H(Y ) the Bayes act for the uniform distribution UY leads
to the same expected loss under any distribution on Y . Based on the divergence D’s deﬁnition in
(6)  maximizing H(Y |X) over Γ(Q) in the LHS of (11) is therefore equivalent to the following
divergence minimization problem

argmin

PY |X: (QX PY |X)∈Γ(Q)

EQX[ D(PY |X UY |X) ].

(15)

6

Here UY |X denotes the uniform conditional distribution over Y given any x ∈ X . This can be
interpreted as projecting the joint distribution (QX UY |X) onto Γ(Q) (See Figure 3). Then  the
duality shown in Theorem 2 implies the following corollary.
Corollary 1. Given a label-invariant H  the solution to (14) also minimizes (15)  i.e. (14) ⊆ (15).
3.3 Examples

3.3.1 Logarithmic Loss: Logistic Regression
To gain sufﬁcient information for the Bayes decision rule under the logarithmic loss  for Y ∈ Y =
{1  . . .   t + 1}  let θ(Y ) be the one-hot encoding of Y   i.e. θi(Y ) = I(Y = i) for 1 ≤ i ≤ t. Here 

we exclude i = t + 1 as I(Y = t + 1) = 1 −(cid:80)t
Fθ(z) = log(cid:0)1 +

exp(zj)(cid:1) 

∀ 1 ≤ i ≤ t : (cid:0)∇Fθ(z)(cid:1)

t(cid:88)

exp(zj)(cid:1)  (16)

i = exp (zi) /(cid:0)1 +

I(Y = i). Then

t(cid:88)

i=1

j=1

j=1

which is the logistic regression model [16]. Also  the RHS of (11) will be the regularized maximum
likelihood problem for logistic regression. This particular result is well-studied in the literature and
straightforward using the duality shown in [17].

3.3.2 0-1 Loss: Minimax SVM

To get sufﬁcient information for the Bayes decision rule under the 0-1 loss  we again consider the
one-hot encoding θ described for the logarithmic loss. We show in the supplementary material that if
˜z = (z  0) and ˜z(i) denotes the ith largest element of ˜z 

k − 1 +(cid:80)k

j=1 ˜z(j)

(17)
In particular  if Y ∈ Y = {−1  1} is binary the dual problem (11) for learning the optimal linear
predictor α∗ given n samples (xi  yi)n

Fθ(z) = max

1≤k≤t+1

i=1 will be

k

.

min

α

1
n

max

0  

1 − yiαT xi

2

  −yiαT xi

+ (cid:107)α(cid:107)∗.

(18)

(cid:27)

The ﬁrst term is the empirical risk of a linear classiﬁer over the minimax-hinge loss max{0  1−z
2  −z}
as shown in Figure 2. In contrast  the standard SVM is formulated using the hinge loss max{0  1− z}:

max(cid:8)0   1 − yiαT xi

(cid:9) + (cid:107)α(cid:107)∗ 

(19)

n(cid:88)

i=1

min

α

1
n

(cid:26)

n(cid:88)

i=1

We therefore call this classiﬁcation approach the minimax SVM. However  unlike the standard SVM 
the minimax SVM is naturally extended to multi-class classiﬁcation.
Using Theorem 1.A2  we prove that for 0-1 loss the robust Bayes decision rule exists and is randomized
in general  where given the optimal linear predictor ˜z = (A∗x  0) randomly predicts a label according
to the following ˜z-based distribution on labels

∀ 1 ≤ i ≤ t + 1 :

 ˜z(i) +
index k satisfying(cid:80)k
probability min(cid:8)1   max{0   (1 + xT α∗)/2}(cid:9).

pσ(i) =

0

1 −(cid:80)kmax

j=1 ˜z(j)

kmax

Here σ is the permutation sorting ˜z in the ascending order  i.e. ˜zσ(i) = ˜z(i)  and kmax is the largest
i=1[˜z(i) − ˜z(k) ] < 1. For example  in the binary case discussed  the minimax
SVM ﬁrst solves (18) to ﬁnd the optimal α∗ and then predicts label y = 1 vs. label y = −1 with

if σ(i) ≤ kmax 
Otherwise.

(20)

2We show that given the speciﬁc structure of Γ(Q) Theorem 1.A holds whether X is ﬁnite or inﬁnite.

7

Dataset
adult
credit
kr-vs-kp
promoters

votes

hepatitis

mmSVM

17
12
4
5
3
17

SVM
22
16
3
9
5
20

DCC
18
14
10
5
3
19

MPM

22
13
5
6
4
18

TAN
17
17
7
44
8
17

Table 1: Methods Performance (error in %)

DRC
17
13
5
6
3
17

3.3.3 Quadratic Loss: Linear Regression

Based on the Bayes decision rule for the quadratic loss  we choose θ(Y ) = Y . To derive Fθ  note that
if we let PY in (10) include all possible distributions  the maximized entropy (variance for quadratic
loss) and thus the value of Fθ would be inﬁnity. Therefore  given a parameter ρ  we restrict the
second moment of distributions in PY = {PY : E[Y 2] ≤ ρ2} and then apply (10). We show in the
supplementary material that an adjusted version of Theorem 2 holds after this change  and

(cid:26)z2/4

ρ(|z| − ρ)

if |z/2| ≤ ρ
if |z/2| > ρ 

Fθ(z) − ρ2 =

(21)

which is the Huber function [18]. Given the samples of a supervised learning task if we choose the
parameter ρ large enough  by solving the RHS of (11) when Fθ(z) is replaced with z2/4 and set ρ
greater than maxi |A∗xi|  we can equivalently take Fθ(z) = z2/4 + ρ2. Then  by (12) we derive the
linear regression model and the RHS of (11) is equivalent to
– Least squares when  = 0.
– Lasso [19  20] when (cid:107) · (cid:107)/(cid:107) · (cid:107)∗ is the (cid:96)∞/(cid:96)1 pair.
– Ridge regression [21] when (cid:107) · (cid:107) is the (cid:96)2-norm.
– (overlapping) Group lasso [22  23] with the (cid:96)1 p penalty when ΓGL(Q) is deﬁned  given subsets
I1  . . . Ik of {1  . . .   d} and 1/p + 1/q = 1  as

ΓGL(Q) = { PX Y : PX = QX  

∀ 1 ≤ j ≤ k : (cid:107) EP

(cid:2)Y XIj

(cid:3) − EQ

(cid:2)Y XIj

(cid:3)(cid:107)q ≤ j }.

(22)

4 Numerical Experiments

We evaluated the performance of the minimax SVM on six binary classiﬁcation datasets from the
UCI repository  compared to these ﬁve benchmarks: Support Vector Machines (SVM) [24]  Discrete
Chebyshev Classiﬁers (DCC) [3]  Minimax Probabilistic Machine (MPM) [2]  Tree Augmented
Naive Bayes (TAN) [25]  and Discrete Rényi Classiﬁers (DRC) [4]. The results are summarized in
Table 1 where the numbers indicate the percentage of error in the classiﬁcation task.
We implemented the minimax SVM by applying the subgradient descent to (18) with the regularizer
λ(cid:107)α(cid:107)2
2. We determined the parameters by cross validation  where we used a randomly-selected 70%
of the training set for training and the rest 30% for testing. We tested the values in {2−10  . . .   210}.
Using the tuned parameters  we trained the algorithm over all the training set and then evaluated the
error rate over the test set. We performed this procedure in 1000 Monte Carlo runs each training on
70% of the data points and testing on the rest 30% and averaged the results.
As seen in the table  the minimax SVM results in the best performance for ﬁve of the six datasets.
To compare these methods in high-dimensional problems  we ran an experiment over synthetic
data with n = 200 samples and d = 10000 features. We generated features by i.i.d. Bernoulli
with P (Xi = 1) = 0.7  and considered y = sign(γT x + z) where z ∼ N (0  1). Using the above
procedure  we evaluated 19.3% for the mmSVM  19.5% error rate for SVM  19.6% error rate for
DRC  which indicates the mmSVM can outperform SVM and DRC in high-dimensional settings as
well. Also  the average training time for training mmSVM was 0.085 seconds  faster than the training
time for the SVM (using Matlab’s SVM command) with the average 0.105 seconds.
Acknowledgments: We are grateful to Stanford University providing a Stanford Graduate Fellowship 
and the Center for Science of Information (CSoI)  an NSF Science and Technology Center under
grant agreement CCF-0939370   for the support during this research.

8

References

[1] Vladimir Vapnik. The nature of statistical learning theory. Springer Science & Business Media  2013.

[2] Gert RG Lanckriet  Laurent El Ghaoui  Chiranjib Bhattacharyya  and Michael I Jordan. A robust minimax

approach to classiﬁcation. The Journal of Machine Learning Research  3:555–582  2003.

[3] Elad Eban  Elad Mezuman  and Amir Globerson. Discrete chebyshev classiﬁers. In Proceedings of the

31st International Conference on Machine Learning (ICML-14)  pages 1233–1241  2014.

[4] Meisam Razaviyayn  Farzan Farnia  and David Tse. Discrete rényi classiﬁers. In Advances in Neural

Information Processing Systems 28  pages 3258–3266  2015.

[5] Peter D. Grünwald and Philip Dawid. Game theory  maximum entropy  minimum discrepancy and robust

bayesian decision theory. The Annals of Statistics  32(4):1367–1433  2004.

[6] Edwin T Jaynes. Information theory and statistical mechanics. Physical review  106(4):620  1957.

[7] Amir Globerson and Naftali Tishby. The minimum information principle for discriminative learning. In

Proceedings of the 20th conference on Uncertainty in artiﬁcial intelligence  pages 193–200  2004.

[8] Vitaly Feldman  Venkatesan Guruswami  Prasad Raghavendra  and Yi Wu. Agnostic learning of monomials

by halfspaces is hard. SIAM Journal on Computing  41(6):1558–1590  2012.

[9] Philip Dawid. Coherent measures of discrepancy  uncertainty and dependence  with applications to
bayesian predictive experimental design. Technical Report 139  University College London  1998.
http://www.ucl.ac.uk/Stats/research/abs94.html.

[10] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons  2012.

[11] Yasemin Altun and Alexander Smola. Unifying divergence minimisation and statistical inference via

convex duality. In Learning Theory: Conference on Learning Theory COLT 2006  Proceedings  2006.

[12] Miroslav Dudík  Steven J Phillips  and Robert E Schapire. Maximum entropy density estimation with
generalized regularization and an application to species distribution modeling. Journal of Machine Learning
Research  8(6):1217–1260  2007.

[13] Ayse Erkan and Yasemin Altun. Semi-supervised learning via generalized maximum entropy. In AISTATS 

pages 209–216  2010.

[14] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[15] Peter McCullagh and John A Nelder. Generalized linear models  volume 37. CRC press  1989.

[16] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. The elements of statistical learning  volume 1.

Springer  2001.

[17] Adam L Berger  Vincent J Della Pietra  and Stephen A Della Pietra. A maximum entropy approach to

natural language processing. Computational linguistics  22(1):39–71  1996.

[18] Peter J Huber. Robust Statistics. Wiley  1981.

[19] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  pages 267–288  1996.

[20] Scott Shaobing Chen  David L. Donoho  and Michael A. Saunders. Atomic decomposition by basis pursuit.

SIAM Journal on Scientiﬁc Computing  20(1):33–61  1998.

[21] Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthogonal problems.

Technometrics  12(1):55–67  1970.

[22] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of

the Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2006.

[23] Laurent Jacob  Guillaume Obozinski  and Jean-Philippe Vert. Group lasso with overlap and graph lasso. In

Proceedings of the 26th annual international conference on machine learning  pages 433–440  2009.

[24] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning  20(3):273–297  1995.

[25] CK Chow and CN Liu. Approximating discrete probability distributions with dependence trees. Information

Theory  IEEE Transactions on  14(3):462–467  1968.

9

,Farzan Farnia
David Tse