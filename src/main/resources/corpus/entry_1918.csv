2019,Explanations can be manipulated and geometry is to blame,Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper  we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely  we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result  we propose effective mechanisms to enhance the robustness of explanations.,Explanations can be manipulated

and geometry is to blame

Ann-Kathrin Dombrowski1  Maximilian Alber5  Christopher J. Anders1 

Marcel Ackermann2  Klaus-Robert Müller1 3 4  Pan Kessel1

1Machine Learning Group  Technische Universität Berlin  Germany

2Department of Video Coding & Analytics  Fraunhofer Heinrich-Hertz-Institute  Berlin  Germany

3Max-Planck-Institut für Informatik  Saarbrücken  Germany

4Department of Brain and Cognitive Engineering  Korea University  Seoul  Korea

5Charité Berlin  Berlin  Germany

Abstract

Explanation methods aim to make neural networks more trustworthy and inter-
pretable. In this paper  we demonstrate a property of explanation methods which is
disconcerting for both of these purposes. Namely  we show that explanations can
be manipulated arbitrarily by applying visually hardly perceptible perturbations
to the input that keep the network’s output approximately constant. We establish
theoretically that this phenomenon can be related to certain geometrical properties
of neural networks. This allows us to derive an upper bound on the susceptibil-
ity of explanations to manipulations. Based on this result  we propose effective
mechanisms to enhance the robustness of explanations.

Figure 1: Original image with corresponding explanation map on the left. Manipulated image with
its explanation on the right. The chosen target explanation was an image with a text stating "this
explanation was manipulated".

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

OriginalImageManipulatedImage1

Introduction

Explanation methods have attracted signiﬁcant attention over the last years due to their promise to
open the black box of deep neural networks. Interpretability is crucial for scientiﬁc understanding
and safety critical applications.
Explanations can be provided in terms of explanation maps[1–20] that visualize the relevance
attributed to each input feature for the overall classiﬁcation result. In this work  we establish that
these explanation maps can be changed to an arbitrary target map. This is done by applying a visually
hardly perceptible perturbation to the input. We refer to Figure 1 for an example. This perturbation
does not change the output of the neural network  i.e. in addition to the classiﬁcation result also the
vector of all class probabilities is (approximately) the same.
This ﬁnding is clearly problematic if a user  say a medical doctor  is expecting a robustly interpretable
explanation map to rely on in the clinical decision making process.
Motivated by this unexpected observation  we provide a theoretical analysis that establishes a
relation of this phenomenon to the geometry of the neural network’s output manifold. This novel
understanding allows us to derive a bound on the degree of possible manipulation of the explanation
map. This bound is proportional to two differential geometric quantities: the principle curvatures
and the geodesic distance between the original input and its manipulated counterpart. Given this
theoretical insight  we propose efﬁcient ways to limit possible manipulations and thus enhance
resilience of explanation methods.
In summary  this work provides the following key contributions:

• We propose an algorithm which allows to manipulate an image with a hardly perceptible
perturbation such that the explanation matches an arbitrary target map. We demonstrate its
effectiveness for six different explanation methods and on four network architectures as well
as two datasets.

• We provide a theoretical understanding of this phenomenon for gradient-based methods
in terms of differential geometry. We derive a bound on the principle curvatures of the
hypersurface of equal network output. This implies a constraint on the maximal change of
the explanation map due to small perturbations.

• Using these insights  we propose methods to undo the manipulations and increase the
robustness of explanation maps by smoothing the explanation method. We demonstrate
experimentally that smoothing leads to increased robustness not only for gradient but also
for propagation-based methods.

1.1 Related work

In [21]  it was demonstrated that explanation maps can be sensitive to small perturbations in the image.
The authors apply perturbations to the image which lead to an unstructured change in the explanation
map. Speciﬁcally  their approach can increase the overall sum of relevances in a certain region of
the explanation map. Our work focuses on structured manipulations instead  i.e. to reproduce a
given target map on a pixel-by-pixel basis. Furthermore  their attacks only keep the classiﬁcation
result the same which often leads to signiﬁcant changes in the network output. From their analysis 
it is therefore not clear whether the explanation or the network is vulnerable (and the explanation
map simply reﬂects the relevance of the perturbation faithfully). Our method keeps the output of
the network (approximately) constant. We furthermore provide a theoretical analysis in terms of
differential geometry and propose effective defense mechanisms. Another approach [22] adds a
constant shift to the input image  which is then eliminated by changing the bias of the ﬁrst layer.
For some methods  this leads to a change in the explanation map. Contrary to our approach  this
requires to change the network’s biases. In [23]  explanation maps are changed by randomization of
(some of) the network weights and in [24] the complete network is ﬁne-tuned to produce manipulated
explanations while the accuracy remains high. These two approaches are different from our method
as they do not aim to change the explanation to a speciﬁc target explanation map and modify the
parameters of the network. In [25  26]  it is proposed to bound the (local) Lipschitz constant of the
explanation. This has the disadvantage that explanations become insensitive to any small perturbation 
e.g. even those which lead to a substantial change in network output. This is clearly undesirable as
the explanation should reﬂect why the perturbation leads to such a drastic change of the network’s

2

conﬁdence. In this work  we therefore propose to only bound the curvature of the hypersurface of
equal network output.

2 Manipulation of explanations

2.1 Explanation methods
We consider a neural network g : Rd → RK with relu non-linearities which classiﬁes an image
x ∈ Rd in K categories with the predicted class given by k = arg maxi g(x)i. The explanation map
is denoted by h : Rd → Rd and associates an image with a vector of the same dimension whose
components encode the relevance score of each pixel for the neural network’s prediction.
Throughout this paper  we will use the following explanation methods:

• Gradient: The map h(x) = ∂g

∂x (x) is used and quantiﬁes how inﬁnitesimal perturbations in

each pixel change the prediction g(x) [2  1].
• Gradient × Input: This method uses the map h(x) = x (cid:12) ∂g
this measure gives the exact contribution of each pixel to the prediction.
• Integrated Gradients: This method deﬁnes h(x) = (x − ¯x) (cid:12)
is a suitable baseline. See the original reference [13] for more details.
• Guided Backpropagation (GBP): This method is a variation of the gradient explanation
for which negative components of the gradient are set to zero while backpropagating through
the non-linearities [4].

∂x (x) [14]. For linear models 

∂g(¯x+t(x−¯x))

dt where ¯x

(cid:82) 1

∂x

0

• Layer-wise Relevance Propagation (LRP): This method [5  16] propagates relevance

backwards through the network. For the output layer  relevance is deﬁned by1

RL

i = δi k  

which is then propagated backwards through all layers but the ﬁrst using the z+ rule

(cid:88)

j

(cid:80)

Rl

i =

i(W l)+
xl
ji
i xl

i(W l)+

ji

Rl+1

j

 

(1)

(2)

where (W l)+ denotes the positive weights of the l-th layer and xl is the activation vector of
the l-th layer. For the ﬁrst layer  we use the zB rule to account for the bounded input domain

(cid:88)

j

(cid:80)

R0

i =

x0
j W 0
i(x0

ji − lj(W 0)+
ji − lj(W 0)+
j W 0

ji − hj(W 0)−
ji − hj(W 0)−

ji

ji)

R1
j  

(3)

where li and hi are the lower and upper bounds of the input domain respectively.

• Pattern Attribution (PA): This method is equivalent to standard backpropagation upon
element-wise multiplication of the weights W l with learned patterns Al. We refer to the
original publication for more details [17].

These methods cover two classes of attribution methods  namely gradient-based and propagation-
based explanations  and are frequently used in practice [27  28].

2.2 Manipulation Method
For a given explanation method and speciﬁed target ht ∈ Rd  a manipulated image xadv = x + δx
has the following properties:

1. The output of the network stays approximately constant  i.e. g(xadv) ≈ g(x).
2. The explanation is close to the target map  i.e. h(xadv) ≈ ht.

1Here we use the Kronecker symbol δi k =

(cid:40)

1 
0 

for i = k
for i (cid:54)= k

.

3

Figure 2: The explanation map of the cat is used as the target and the image of the dog is perturbed.
The red box contains the manipulated images and the corresponding explanations. The ﬁrst column
corresponds to the original explanations of the unperturbed dog image. The target map  shown in the
second column  is the corresponding explanation of the cat image. The last column visualizes the
perturbations.

L =(cid:13)(cid:13)h(xadv) − ht(cid:13)(cid:13)2

3. The norm of the perturbation δx added to the input image is small  i.e. (cid:107)δx(cid:107) =

(cid:107)xadv − x(cid:107) (cid:28) 1 and therefore not perceptible.

We obtain such manipulations by optimizing the loss function

+ γ (cid:107)g(xadv) − g(x)(cid:107)2  

(4)
with respect to xadv using gradient descent. We clamp xadv after each iteration so that it is a valid
image. The ﬁrst term in the loss function (4) ensures that the manipulated explanation map is close
to the target while the second term encourages the network to have the same output. The relative
weighting of these two summands is controlled by the hyperparameter γ ∈ R+.
Our method therefore requires us to calculate the gradient with respect to the input ∇h(x) of the
explanation. For relu-networks  this gradient often depends on the vanishing second derivative of
non-linearities which leads to problems during optimization of the loss (4). As an example  the
gradient method leads to

(cid:13)(cid:13)h(xadv) − ht(cid:13)(cid:13)2

∂xadv

∂h
∂xadv

∝

=

∂2g
∂x2

adv ∝ relu(cid:48)(cid:48) = 0 .

We therefore replace the relu with softplus non-linearities

log(1 + eβx) .

(5)

softplusβ(x) =

1
β

4

Original MapTarget MapManipulated MapPerturbationsGradientGradientxInputLayerwiseRelevancePropagationIntegratedGradientsGuidedBackpropagationPatternAttributionPerturbed ImageOriginal ImageImage used toproduce TargetFigure 3: Left: Similarity measures between target ht and manipulated explanation map h(xadv).
Right: Similarity measures between original x and perturbed image xadv. For SSIM and PCC large
values indicate high similarity while for MSE small values correspond to similar images. For fair
comparison  we use the same 100 randomly selected images for each explanation method.

For large β values  the softplus approximates the relu closely but has a well-deﬁned second derivative.
After optimization is complete  we test the manipulated image with the original relu network.
Similarity metrics: In our analysis  we assess the similarity between both images and explanation
maps. To this end  we use three metrics following [23]: the structural similarity index (SSIM)  the
Pearson correlation coefﬁcient (PCC) and the mean squared error (MSE). SSIM and PCC are relative
similarity measures with values in [0  1]  where larger values indicate high similarity. The MSE is an
absolute error measure for which values close to zero indicate high similarity. We normalize the sum
of the explanation maps to be one and the images to have values between 0 and 1.

2.3 Experiments

To evaluate our approach  we apply our algorithm to 100 randomly selected images for each explana-
tion method. We use a pre-trained VGG-16 network [29] and the ImageNet dataset [30]. For each run 
we randomly select two images from the test set. One of the two images is used to generate a target
explanation map ht. The other image is perturbed by our algorithm with the goal of replicating the
target ht using a few hundred iterations of gradient descent. We sum over the absolute values of the
channels of the explanation map to get the relevance per pixel. Further details about the experiments
are summarized in Supplement A.
Qualitative analysis: Our method is illustrated in Figure 2 in which a dog image is manipulated
in order to have an explanation resembling a cat. For all explanation methods  the target is closely
emulated and the perturbation of the dog image is small. More examples can be found in the
supplement.
Quantitative analysis: Figure 3 shows similarity measures between the target ht and the manipulated
explanation map h(xadv) as well as between the original image x and perturbed image xadv.2 All
considered metrics show that the perturbed images have an explanation closely resembling the targets.
At the same time  the perturbed images are very similar to the corresponding original images. We
also veriﬁed by visual inspection that the results look very similar. We have uploaded the results of all

2Throughout this paper  boxes denote 25th and 75th percentiles  whiskers denote 10th and 90th percentiles 

and solid lines show the medians

5

0.000.250.500.75MSE×10−9SimilaritiesExplanations0.60.70.80.9SSIMGradientGradientxInputIntegratedGradientsLRPGBPPA0.70.80.9PCC0.000.250.500.75MSE×10−3SimilaritiesImages0.900.95SSIMGradientGradientxInputIntegratedGradientsLRPGBPPA0.9900.9951.000PCCruns so that interested readers can assess their similarity themselves3 and provide code4 to reproduce
them. In addition  the output of the neural network is approximately unchanged by the perturbations 
i.e. the classiﬁcation of all examples is unchanged and the median of (cid:107)g(xadv) − g(x)(cid:107) is of the order
of magnitude 10−3 for all methods. See Supplement B for further details.
Other architectures and datasets: We checked that comparable results are obtained for ResNet-18
[31]  AlexNet [32] and Densenet-121 [33]. Moreover  we also successfully tested our algorithm on
the CIFAR-10 dataset [34]. We refer to the Supplement C for further details.

3 Theoretical considerations

In this section  we analyze the vulnerability of explanations theoretically. We argue that this phe-
nomenon can be related to the large curvature of the output manifold of the neural network. We focus
on the gradient method starting with an intuitive discussion before developing mathematically precise
statements.
We have demonstrated that one can drastically change the explanation map while keeping the output
of the neural network constant

g(x + δx) = g(x) = c

(6)
using only a small perturbation in the input δx. The perturbed image xadv = x + δx therefore lies on
the hypersurface of constant network output S = {p ∈ Rd|g(p) = c}.5 We can exclusively consider
the winning class output  i.e. g(x) := g(x)k with k = arg maxi g(x)i because the gradient method
only depends on this component of the output. Therefore  the hypersurface S is of co-dimension one.
The gradient ∇g for every p ∈ S is normal to this hypersurface. The fact that the normal vector ∇g
can be drastically changed by slightly perturbing the input along the hypersurface S suggests that the
curvature of S is large.
While the latter statement may seem intuitive  it requires non-trivial concepts of differential geometry
to make it precise  in particular the notion of the second fundamental form. We will brieﬂy summarize
these concepts in the following (see e.g. [35] for a standard textbook). To this end  it is advantageous
to consider a normalized version of the gradient method
n(x) = ∇g(x)
(cid:107)∇g(x)(cid:107)

This normalization is merely conventional as it does not change the relative importance of any pixel
with respect to the others. For any point p ∈ S  we deﬁne the tangent space TpS as the vector space
dt γ(t)|t=0 of all possible curves γ : R → S with γ(0) = p.
spanned by the tangent vectors ˙γ(0) = d
For u  v ∈ TpS  we denote their inner product by (cid:104)u  v(cid:105). For any u ∈ TpS  the directional derivative
of a function f is uniquely deﬁned for any choice of γ by

(7)

.

with

γ(0) = p and ˙γ(0) = u.

(8)

(cid:12)(cid:12)(cid:12)(cid:12)t=0

Duf (p) =

d
dt

f (γ(t))

We then deﬁne the Weingarten map as6

L :

(cid:26)TpS → TpS

u

(cid:55)→ −Dun(p)  

where the unit normal n(p) can be written as (7). This map quantiﬁes how much the unit normal
changes as we inﬁnitesimally move away from p in the direction u. The second fundamental form is
then given by

(cid:26)TpS × TpS → R

u  v

L :

(cid:55)→ −(cid:104)v  L(u)(cid:105) = (cid:104)v  Dun(p)(cid:105) .

3https://drive.google.com/drive/folders/1TZeWngoevHRuIw6gb5CZDIRrc7EWf5yb?usp=

4https://github.com/pankessel/adv_explanation_ref
5It is sufﬁcient to consider the hypersurface S in a neighbourhood of the unperturbed input x.
6The fact that Dun(p) ∈ TpS follows by taking the directional derivative with respect to u on both sides of

sharing

(cid:104)n  n(cid:105) = 1 .

6

It can be shown that the second fundamental form is bilinear and symmetric L(u  v) = L(v  u). It is
therefore diagonalizable with real eigenvalues λ1  . . . λd−1 which are called principle curvatures.
We have therefore established the remarkable fact that the sensitivity of the gradient map (7) is
described by the principle curvatures  a key concept of differential geometry.
In particular  this allows us to derive an upper bound on the maximal change of the gradient map
h(x) = n(x) as we move slightly on S. To this end  we deﬁne the geodesic distance dg(p  q) of two
points p  q ∈ S as the length of the shortest curve on S connecting p and q. In the supplement  we
show that:
Theorem 1 Let g : Rd → R be a network with softplusβ non-linearities and U(p) = {x ∈
Rd;(cid:107)x − p(cid:107) < } an environment of a point p ∈ S such that U(p)∩ S is fully connected. Let g have
bounded derivatives (cid:107)∇g(x)(cid:107) ≥ c for all x ∈ U(p) ∩ S. It then follows for all p0 ∈ U(p) ∩ S that
(9)
where λmax is the principle curvature with the largest absolute value for any point in U(p) ∩ S and
the constant C > 0 depends on the weights of the neural network.

(cid:107)h(p) − h(p0)(cid:107) ≤ |λmax| dg(p  p0) ≤ β C dg(p  p0) 

This theorem can intuitively be motivated as follows: for relu non-linearities  the lines of equal
network output are piece-wise linear and therefore have kinks  i.e. points of divergent curvature.
These relu non-linearities are well approximated by softplus non-linearities (5) with large β. Reducing
β smoothes out the kinks and therefore leads to reduced maximal curvature  i.e. |λmax| ≤ β C. For
each point on the geodesic curve connecting p and p0  the normal can at worst be affected by the
maximal curvature  i.e. the change in explanation is bounded by |λmax| dg(p  p0).
There are two important lessons to be learnt from this theorem:
the geodesic distance can be
substantially greater than the Euclidean distance for curved manifolds. In this case  inputs which
are very similar to each other  i.e. the Euclidean distance is small  can have explanations that are
drastically different. Secondly  the upper bound is proportional to the β parameter of the softplus
non-linearity. Therefore  smaller values of β provably result in increased robustness with respect to
manipulations.

Figure 4: Left: β dependence for the correlations of the manipulated explanation (here Gradient and
LRP) with the target and original explanation. Lines denote the medians  10th and 90th percentiles
are shown in semitransparent colour. Center and Right: network input and the respective explanation
maps as β is decreased for Gradient (center) and LRP (right).

4 Robust explanations

Using the fact that the upper bound of the last section is proportional
to the β parameter of the softplus non-linearities  we propose β-
smoothing of explanations. This method calculates an explanation
using a network for which the relu non-linearities are replaced by
softplus with a small β parameter to smooth the principle curvatures.
The precise value of β is a hyperparameter of the method  but we
ﬁnd that a value around one works well in practice.

7

0.00.51.0PCCGradienth(xadv)&hth(xadv)&h(x)102103104β0.00.51.0PCCLRPOriginalImageReLUSoftplusβ=5Softplusβ=0.8ManipulatedTargetOriginalImageReLUSoftplusβ=5Softplusβ=0.8ManipulatedTargetunsmoothedsmoothedAs shown in the supplement  a relation between SmoothGrad [12]
and β-smoothing can be proven for a one-layer neural network:

Theorem 2 For a one-layer neural network g(x) = relu(wT x) and its β-smoothed counterpart
gβ(x) = softplusβ(wT x)  it holds that

E∼pβ [∇g(x − )] = ∇g β(cid:107)w(cid:107)

(x)  

where pβ() =

(eβ/2+e−β/2)2 .

β

N

2π

(cid:80)N
√
β   β-smoothing
Since pβ(x) closely resembles a normal distribution with variance σ = log(2)
can be understood as N → ∞ limit of SmoothGrad h(x) = 1
i=1 ∇g(x − i) where i ∼ gβ ≈
N (0  σ). We emphasize that the theorem only holds for a one-layer neural network  but for deeper
networks we empirically observe that both lead to visually similar maps as they are considerably less
noisy than the gradient map. The theorem therefore suggests that SmoothGrad can similarly be used
to smooth the curvatures and can thereby make explanations more robust.7
Experiments: Figure 4 demonstrates that β-smoothing allows us to recover the original explanation
map by decreasing the value of the β parameter. We stress that this works for all considered methods.
We also note that the same effect can be observed using SmoothGrad by successively increasing the
standard deviation σ of the noise distribution. This further underlines the similarity between the two
smoothing methods.
If an attacker knew that smoothing was used to undo the manipulation  they could try to attack the
smoothed method directly. However  both β-smoothing and SmoothGrad are substantially more
robust than their non-smoothed counterparts  see Figure 5. It is important to note that β-smoothing
achieves this at considerably lower computational cost: β-smoothing only requires a single forward
and backward pass  while SmoothGrad requires as many as the number of noise samples (typically
between 10 to 50).
We refer to Supplement D for more details on these experiments.

Figure 5: Left: markers are clearly left of the diagonal  i.e. explanations are more robust to manipula-
tions when β-smoothing is used. Center: SmoothGrad has comparable results to β-smoothing  i.e.
markers are distributed around the diagonal. Right: β-smoothing has signiﬁcantly lower computa-
tional cost than SmoothGrad.

Figure 6 shows the evolution of the gradient explanation maps when reducing the β parameter of
the softplus activations. We note that for small β the explanation maps tend to become similar
to LRP/GBP/PA explanation maps (see Figure 2 for comparison). Figure 7 demonstrates that β-
smoothing leads to better performance than the gradient method and to comparable performance with
SmoothGrad on the pixel-ﬂipping metric [5  36].

(cid:80)N
i=1 h(x − i).

form  i.e. 1
N

7For explanation methods h(x) other than gradient  SmoothGrad needs to be used in a slightly generalized

8

0.250.500.75β-smoothedGradient0.20.40.60.8GradientPCCbetweenhtandh(xadv)0.250.500.75β-smoothedGradient0.20.40.60.8SmoothGradPCCbetweenhtandh(xadv)β-smoothedSmoothGrad0.0000.0250.0500.0750.100secondsRuntimeFigure 6: Gradient explanation map produced with the original network and a network with softplus
activation functions using various values for β.

Figure 7: Pixelﬂipping performance compared to random baseline (the lower the accuracy the better
the explanation): the metric sorts pixels of images by relevance and incrementally sets the pixels
to zero starting with the most relevant. In each step  the network’s performance is evaluated on the
complete ImageNet validation set.

5 Conclusion

Explanation methods have recently become increasingly popular among practitioners. In this contri-
bution  we show that dedicated imperceptible manipulations of the input data can yield arbitrary and
drastic changes of the explanation map. We demonstrate both qualitatively and quantitatively that
explanation maps of many popular explanation methods can be arbitrarily manipulated. Crucially 
this can be achieved while keeping the model’s output constant. A novel theoretical analysis reveals
that in fact the large curvature of the network’s decision function is one important culprit for this
unexpected vulnerability. Using this theoretical insight  we can profoundly increase the resilience to
manipulations by smoothing only the explanation process while leaving the model itself unchanged.
Future work will investigate possibilities to modify the training process of neural networks itself such
that they can become less vulnerable to manipulations of explanations. Another interesting future
direction is to generalize our theoretical analysis of gradient-based to propagation-based methods.
This seems particularly promising because our experiments strongly suggest that similar theoretical
ﬁndings should also hold for these explanation methods.

Acknowledgments

We want to thank the anonymous reviewers for their helpful feedback. We also thank Kristof Schütt 
Grégoire Montavon and Shinichi Nakajima for useful discussions. This work is supported by the
German Ministry for Education and Research as Berlin Big Data Center (01IS18025A) and Berlin
Center for Machine Learning (01IS18037I). This work is also supported by the Information &
Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government
(No. 2017-0-001779)  as well as by the Research Training Group "Differential Equation- and Data-
driven Models in Life Sciences and Fluid Dynamics (DAEDALUS)" (GRK 2433) and Grant Math+ 
EXC 2046/1  Project ID 390685689 both funded by the German Research Foundation (DFG).

References
[1] David Baehrens  Timon Schroeter  Stefan Harmeling  Motoaki Kawanabe  Katja Hansen  and
Klaus-Robert Müller. How to explain individual classiﬁcation decisions. Journal of Machine
Learning Research  11(Jun):1803–1831  2010.

9

ImageReLUβ=10β=3β=2β=10.00.20.40.60.8Ratioofpixelssettozero0.000.250.500.75top-1accuracyβ-smoothedGradientSmoothGradGradientrandom[2] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep Inside Convolutional Networks:
Visualising Image Classiﬁcation Models and Saliency Maps. In 2nd International Conference
on Learning Representations  ICLR 2014  Banff  AB  Canada  April 14-16  2014  Workshop
Track Proceedings  2014.

[3] Matthew D. Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks.
In Computer Vision - ECCV 2014 - 13th European Conference  Zurich  Switzerland  September
6-12  2014  Proceedings  Part I  pages 818–833  2014.

[4] Jost Tobias Springenberg  Alexey Dosovitskiy  Thomas Brox  and Martin A. Riedmiller. Striving
In 3rd International Conference on Learning
for Simplicity: The All Convolutional Net.
Representations  ICLR 2015  San Diego  CA  USA  May 7-9  2015  Workshop Track Proceedings 
2015.

[5] Sebastian Bach  Alexander Binder  Grégoire Montavon  Frederick Klauschen  Klaus-Robert
Müller  and Wojciech Samek. On Pixel-Wise Explanations for Non-Linear Classiﬁer Decisions
by Layer-Wise Relevance Propagation. PLOS ONE  10(7):1–46  07 2015.

[6] Ramprasaath R. Selvaraju  Abhishek Das  Ramakrishna Vedantam  Michael Cogswell  Devi
Parikh  and Dhruv Batra. Grad-CAM: Why did you say that? Visual Explanations from Deep
Networks via Gradient-based Localization. CoRR  abs/1610.02391  2016.

[7] Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. Why should I trust you?: Explaining
the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining  pages 1135–1144. ACM  2016.

[8] Luisa M. Zintgraf  Taco S. Cohen  Tameem Adel  and Max Welling. Visualizing Deep Neural
In 5th International Conference on
Network Decisions: Prediction Difference Analysis.
Learning Representations  ICLR 2017  Toulon  France  April 24-26  2017  Conference Track
Proceedings  2017.

[9] Avanti Shrikumar  Peyton Greenside  and Anshul Kundaje. Learning Important Features
Through Propagating Activation Differences. In Proceedings of the 34th International Con-
ference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages
3145–3153  2017.

[10] Scott M Lundberg and Su-In Lee. A Uniﬁed Approach to Interpreting Model Predictions. In
I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett 
editors  Advances in Neural Information Processing Systems 30  pages 4765–4774. Curran
Associates  Inc.  2017.

[11] Piotr Dabkowski and Yarin Gal. Real Time Image Saliency for Black Box Classiﬁers. In
I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett 
editors  Advances in Neural Information Processing Systems 30  pages 6967–6976. Curran
Associates  Inc.  2017.

[12] Daniel Smilkov  Nikhil Thorat  Been Kim  Fernanda B. Viégas  and Martin Wattenberg. Smooth-

Grad: removing noise by adding noise. CoRR  abs/1706.03825  2017.

[13] Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic Attribution for Deep Networks.
In Proceedings of the 34th International Conference on Machine Learning  ICML 2017  Sydney 
NSW  Australia  6-11 August 2017  pages 3319–3328  2017.

[14] Avanti Shrikumar  Peyton Greenside  and Anshul Kundaje. Learning Important Features
Through Propagating Activation Differences. In Proceedings of the 34th International Con-
ference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages
3145–3153  2017.

[15] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful
perturbation. In 2017 IEEE international conference on computer vision (ICCV)  pages 3449–
3457. IEEE  2017.

10

[16] Grégoire Montavon  Sebastian Lapuschkin  Alexander Binder  Wojciech Samek  and Klaus-
Robert Müller. Explaining nonlinear classiﬁcation decisions with Deep Taylor Decomposition.
Pattern Recognition  65:211–222  2017.

[17] Pieter-Jan Kindermans  Kristof T Schütt  Maximilian Alber  Klaus-Robert Müller  Dumitru
Erhan  Been Kim  and Sven Dähne. Learning how to explain neural networks: PatternNet and
PatternAttribution. International Conference on Learning Representations  2018.

[18] Been Kim  Martin Wattenberg  Justin Gilmer  Carrie J. Cai  James Wexler  Fernanda B. Viégas 
and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept
activation vectors (TCAV). In Proceedings of the 35th International Conference on Machine
Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  pages 2673–
2682  2018.

[19] Sebastian Lapuschkin  Stephan Wäldchen  Alexander Binder  Grégoire Montavon  Wojciech
Samek  and Klaus-Robert Müller. Unmasking Clever Hans predictors and assessing what
machines really learn. Nature communications  10(1):1096  2019.

[20] Wojciech Samek  Gregoire Montavon  Andrea Vedaldi  Lars Kai Hansen  and Klaus-Robert
Müller. Explainable AI: Interpreting  Explaining and Visualizing Deep Learning. Springer 
2019.

[21] Amirata Ghorbani  Abubakar Abid  and James Y. Zou. Interpretation of neural networks is
fragile. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence  AAAI 2019  The Thirty-
First Innovative Applications of Artiﬁcial Intelligence Conference  IAAI 2019  The Ninth AAAI
Symposium on Educational Advances in Artiﬁcial Intelligence  EAAI 2019  Honolulu  Hawaii 
USA  January 27 - February 1  2019.  pages 3681–3688  2019.

[22] Pieter-Jan Kindermans  Sara Hooker  Julius Adebayo  Maximilian Alber  Kristof T. Schütt  Sven
Dähne  Dumitru Erhan  and Been Kim. The (un)reliability of saliency methods. In Explainable
AI: Interpreting  Explaining and Visualizing Deep Learning  pages 267–280. Springer  2019.

[23] Julius Adebayo  Justin Gilmer  Michael Muelly  Ian J. Goodfellow  Moritz Hardt  and Been
Kim. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018  NeurIPS 2018  3-8
December 2018  Montréal  Canada.  pages 9525–9536  2018.

[24] Juyeon Heo  Sunghwan Joo  and Taesup Moon. Fooling neural network interpretations via
adversarial model manipulation. In H. Wallach  H. Larochelle  A. Beygelzimer  F. d'Alché-Buc 
E. Fox  and R. Garnett  editors  Advances in Neural Information Processing Systems 32  pages
2921–2932. Curran Associates  Inc.  2019.

[25] David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-
explaining neural networks. In Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December
2018  Montréal  Canada.  pages 7786–7795  2018.

[26] David Alvarez-Melis and Tommi S. Jaakkola. On the Robustness of Interpretability Methods.

CoRR  abs/1806.08049  2018.

[27] Maximilian Alber  Sebastian Lapuschkin  Philipp Seegerer  Miriam Hägele  Kristof T. Schütt 
Grégoire Montavon  Wojciech Samek  Klaus-Robert Müller  Sven Dähne  and Pieter-Jan
Kindermans. iNNvestigate neural networks! Journal of Machine Learning Research 20  2019.

[28] Marco Ancona  Enea Ceolini  Cengiz Oztireli  and Markus Gross. Towards better understand-
ing of gradient-based attribution methods for Deep Neural Networks. In 6th International
Conference on Learning Representations (ICLR 2018)  2018.

[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. International Conference on Learning Representations  2014.

11

[30] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV)  115(3):211–252  2015.

[31] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition  CVPR
2016  Las Vegas  NV  USA  June 27-30  2016  pages 770–778  2016.

[32] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E. Hinton. ImageNet Classiﬁcation with Deep
Convolutional Neural Networks. In Proceedings of the 25th International Conference on Neural
Information Processing Systems - Volume 1  NIPS’12  pages 1097–1105  USA  2012. Curran
Associates Inc.

[33] Gao Huang  Zhuang Liu  Laurens van der Maaten  and Kilian Q. Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition 
CVPR 2017  Honolulu  HI  USA  July 21-26  2017  pages 2261–2269  2017.

[34] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images  2009.

[35] Loring W Tu. Differential geometry: connections  curvature  and characteristic classes  volume

275. Springer  2017.

[36] Wojciech Samek  Alexander Binder  Gregoire Montavon  Sebastian Lapuschkin  and Klaus-
Robert Müller. Evaluating the Visualization of What a Deep Neural Network Has Learned.
IEEE Transactions on Neural Networks and Learning Systems  28:2660–2673  11 2017.

12

,Ann-Kathrin Dombrowski
Maximillian Alber
Christopher Anders
Marcel Ackermann
Klaus-Robert Müller
Pan Kessel