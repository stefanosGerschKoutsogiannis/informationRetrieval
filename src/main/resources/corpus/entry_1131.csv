2018,Context-dependent upper-confidence bounds for directed exploration,Directed exploration strategies for reinforcement learning are critical for learning an optimal policy in a minimal number of interactions with the environment. Many algorithms use optimism to direct exploration  either through visitation estimates or upper confidence bounds  as opposed to data-inefficient strategies like e-greedy that use random  undirected exploration. Most data-efficient exploration methods require significant computation  typically relying on a learned model to guide exploration. Least-squares methods have the potential to provide some of the data-efficiency benefits of model-based approaches—because they summarize past interactions—with the computation closer to that of model-free approaches. In this work  we provide a novel  computationally efficient  incremental exploration strategy  leveraging this property of least-squares temporal difference learning (LSTD). We derive upper confidence bounds on the action-values learned by LSTD  with context-dependent (or state-dependent) noise variance. Such context-dependent noise focuses exploration on a subset of variable states  and allows for reduced exploration in other states. We empirically demonstrate that our algorithm can converge more quickly than other incremental exploration strategies using confidence estimates on action-values.,Context-Dependent Upper-Conﬁdence Bounds for

Directed Exploration

Raksha Kumaraswamy1  Matthew Schlegel1  Adam White1 2  Martha White1

1Department of Computing Science  University of Alberta; 2DeepMind

{kumarasw  mkschleg}@ualberta.ca  adamwhite@google.com  whitem@ualberta.ca

Abstract

Directed exploration strategies for reinforcement learning are critical for learning
an optimal policy in a minimal number of interactions with the environment. Many
algorithms use optimism to direct exploration  either through visitation estimates
or upper-conﬁdence bounds  as opposed to data-inefﬁcient strategies like ✏-greedy
that use random  undirected exploration. Most data-efﬁcient exploration methods
require signiﬁcant computation  typically relying on a learned model to guide
exploration. Least-squares methods have the potential to provide some of the
data-efﬁciency beneﬁts of model-based approaches—because they summarize past
interactions—with the computation closer to that of model-free approaches. In
this work  we provide a novel  computationally efﬁcient  incremental exploration
strategy  leveraging this property of least-squares temporal difference learning
(LSTD). We derive upper-conﬁdence bounds on the action-values learned by
LSTD  with context-dependent (or state-dependent) noise variance. Such context-
dependent noise focuses exploration on a subset of variable states  and allows for
reduced exploration in other states. We empirically demonstrate that our algorithm
can converge more quickly than other incremental exploration strategies using
conﬁdence estimates on action-values.

1

Introduction

Exploration is crucial in reinforcement learning  as the data gathering process signiﬁcantly impacts
the optimality of the learned policies and values. The agent needs to balance the amount of time
taking exploratory actions to learn about the world  versus taking actions to maximize cumulative
rewards. If the agent explores insufﬁciently  it could converge to a suboptimal policy; exploring too
conservatively  however  results in many suboptimal decisions. The goal of the agent is data-efﬁcient
exploration: to minimize how many samples are wasted in exploration  particularly exploring parts of
the world that are known  while still ensuring convergence to the optimal policy.
To achieve such a goal  directed exploration strategies are key. Undirected strategies  where random
actions are taken such as in ✏-greedy  are a common default. In small domains these methods are
guaranteed to ﬁnd an optimal policy [35]  because the agent is guaranteed to visit the entire space—
but may take many many steps to do so  as undirected exploration can interfere with improving
policies in incremental control. In this paper we explore the idea of constructing conﬁdence intervals
around the agent’s value estimates. The agent can use these learned conﬁdence intervals to select
actions with the highest upper-conﬁdence bound ensuring actions selected are of high value or whose
values are highly uncertain. This optimistic approach is promising for directed exploration  but as yet
there are few such methods that are model-free  incremental and computationally efﬁcient.
Directed exploration strategies have largely been explored under the framework of “optimism in
the face of uncertainty” [13]. These can generally be categorized into count-based approaches
and conﬁdence-based approaches. Count-based approaches estimate the “known-ness” of a state 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

typically by maintaining counts for ﬁnite state-spaces [16  6  36  37  43] and extensions on counting
for continuous states [14  10  26  19  33  15  32  21]. Conﬁdence interval estimates  on the other
hand  depend on variance of the target  not just on visitation frequency for states. Conﬁdence-based
approaches can be more data-efﬁcient for exploration  because the agent can better direct exploration
where the estimates are less accurate. The majority of conﬁdence-based approaches compute
conﬁdence intervals on model parameters  both for ﬁnite state-spaces [12  47  16  6  2  3  9  43  29]
and continuous state-spaces [11  27  8  1  28]. There is early work quantifying uncertainty for value
estimates directly for ﬁnite state-spaces [22]  describing the difﬁculties with extending the local
measures of uncertainty from the bandit literature to RL  since there are long-term dependencies.
These difﬁculties suggest why using conﬁdence intervals directly on value estimates for exploration
in RL has been less explored  until recently. More approaches are now being developed that maintain
conﬁdence intervals on the value function for continuous state-spaces  by maintaining a distribution
over value functions [8  31]  or by maintaining a randomized set of value functions from which to
sample [46  31  30  34  25]. Though signiﬁcant steps forward  these approaches have limitations
particularly in terms of computational efﬁciency. Delayed Gaussian Process Q-learning (DGPQ)
[8] requires updating two Gaussian processes  which is cubic in the number of basis vectors for
the Gaussian process. RLSVI [31] is relatively efﬁcient  maintaining a Gaussian distribution over
parameters with Thompson sampling to get randomized values. Their staged approach for ﬁnite-
horizon problems  however  does not allow for value estimates to be updated online  as the value
function is ﬁxed per episode to gather an entire trajectory of data. Moerland et al. [25]  on the
other hand  sample a new parameter vector from the posterior distribution each time an action is
considered  which is expensive. The bootstrapping approaches can be efﬁcient  as they simply have
to store several value functions  either for training on a bootstrapped subset of samples—such as in
Bootstrapped DQN [30]—or for maintaining a moving bootstrap around the changing parameters
themselves  for UCBootstrap [46]. For both of these approaches  however  it is unclear how many
value functions would be required  which could be large depending on the problem.
In this paper  we provide an incremental  model-free exploration algorithm with fast converging upper-
conﬁdence bounds  called UCLS: Upper-Conﬁdence Least-Squares. We derive the upper-conﬁdence
bounds for Least-Squares Temporal Difference learning (LSTD)  taking advantage of the fact that
LSTD has an efﬁcient summary of past interaction to facilitate computation of conﬁdence intervals.
Importantly  these upper-conﬁdence bounds have context-dependent variance  where variance is
dependent on state rather than a global estimate  focusing exploration on states with higher-variance.
Computing conﬁdence intervals for action-values in RL has remained an open problem  and we
provide the ﬁrst theoretically sound result for obtaining upper-conﬁdence bounds for policy evaluation
under function approximation  without making strong assumptions on the noise. We demonstrate
in several simulated domains that UCLS outperforms DGPQ  UCBootstrap  and RLSVI. We also
empirically show the beneﬁt of using UCLS to a simpliﬁed version that uses a global variance
estimate  rather than context-dependent variance.
2 Background

R is the reward function; and  : S⇥A⇥S!

We focus on the problem of learning an optimal policy for a Markov decision process  from on-
policy interaction. A Markov decision process consists of (S A  Pr  r  ) where S is the set of
states; A is the set of actions; Pr : S⇥A⇥S!
[0 1) provides the transition probabilities;
[0  1] is the transition-based
r : S⇥A⇥S!
discount function which enables either continuing or episodic problems to be speciﬁed [45]. On each
step  the agent selects action At in state St  and transitions to St+1  according to Pr  receiving reward
def= (St  At  St+1). For a policy ⇡ : S⇥A! [0  1] 
Rt+1
wherePa2A ⇡(s  a) = 1 8s 2S   the value at a given state s  taking action a  is the expected

discounted sum of future rewards  with actions selected according to ⇡ into the future 

def= r(St  At  St+1) and discount t+1

Q⇡(s  a) = EhRt+1 + t+1Xa2A

⇡(St+1  a)Q⇡(St+1  a)St = s  At = ai

For problems in which Q⇡ can be stored in a table  a ﬁxed point for the action-values Q⇡ exists for a
given ⇡. In most domains  Q⇡ must be approximated by Q⇡
In the case of linear function approximation  state-action features x(st  at) are used to approximate
action-values Q⇡
w(st  at) = x(st  at)>w. The weights w can be learned with a stochastic approx-
imation algorithm  called temporal difference (TD) learning [39]. The TD update [39] processes

w  parametrized by w 2W⇢ Rd.

2

def= x(St  At).
samples one at a time  w = w + ↵tzt  with t
The eligibility trace zt = xt + t+1zt1 facilitates multi-step updates via an exponentially weighted
memory of previous feature activations decayed by  2 [0  1] and z0 = 0. Alternatively  we can
directly compute the weight vector found by TD using least-squares temporal difference learning
(LSTD) [5]. The LSTD solution is more data-efﬁcient  and can avoid the need to tune TD’s stepsize
parameter ↵> 0. The LSTD update can be efﬁciently computed incrementally without approximation
or storing the data [5  4]  by maintaining a matrix AT and vector bT  

def= Rt+1 + t+1x>t+1w  x>t w for xt

AT

def=

1
T

zt(xt  t+1xt+1)>

bT

def=

1
T

T1Xt=0

ztRt+1

(1)

T1Xt=0

The value function approximation at time step T is the weights that satisfy the linear system AT w =
bT . In practice  the inverse of the matrix A1 is maintained using a Sherman-Morrison update  with
a small regularizer ⌘ added to the matrix A to guarantee invertibility [41].
One approach to ensure systematic exploration is to initialize the agent’s value estimates optimistically.
The action-value function must be initialized to predict the maximum possible return (or greater)
from each state and action. For example  for cost-to-goal problems  with -1 per step  the values can
be initialized to zero. For continuing problems  with constant discount c < 1  the values can be
initialized to Gmax = Rmax/(1  c)  if the maximum reward Rmax is known. For ﬁxed features that
are non-negative and encode locality—such as tile coding or radial basis functions—the weights w
can be simply set to Gmax  to make Qw optimistic.
More generally  however  it can be problematic to use optimistic initialization. Optimistic initialization
assumes the beginning of time is special—a period when systematic exploration should be performed
after which the agent should more or less exploit its current knowledge. Many problems are
non-stationary—or at least beneﬁt from a tracking approach due to aliasing caused by function
approximation—and beneﬁt from continual exploration. Further  unlike for ﬁxed features  it is
unclear how to set and maintain initial values at Gmax for learned features  such as with neural
networks. Optimistic initialization is also not straightforward for algorithms like LSTD  which
completely overwrite the estimate w on each step with a closed-form solution. In fact  we have found
that this issue with LSTD has been obfuscated  because the regularizer ⌘ has inadvertently played a
role in providing optimism (see Appendix A). Rather  to use optimism in LSTD for control  we need
to explicitly compute upper-conﬁdence bounds.
Conﬁdence intervals around action-values  then  provide another mechanism for exploration in
reinforcement learning. Consider action selection with explicit conﬁdence intervals around mean
estimates ˆQw(St  At)  with estimated radius ˆU (St  At). The action selection is greedy w.r.t. to these
ˆQw(St  a) + ˆU (St  a)  which provides a high-conﬁdence upper bound
optimistic values  argmaxa
on the best possible value for that action. The use of upper-conﬁdence bounds on value estimates for
exploration has been well-studied and motivated theoretically in online learning [7]. In reinforcement
learning  there have only been a few specialized proofs for particular algorithms using optimistic
estimates [8  31]  but the result can be expressed more generally by using the idea of stochastic
optimism. We extract the central argument by Osband et al. [31] to provide a general Optimistic
Values Theorem in Appendix B. In particular  similar to online learning  we can guarantee that
greedy-action selection according to upper-conﬁdence values will converge to the optimal policy 
if the conﬁdence interval radius shrinks to zero  if the algorithm to estimate action-values for a
policy converges to the corresponding actions and if upper-conﬁdence estimates are stochastically
optimal—remain above the optimal action-values in expectation.
Motivated by this result  we pursue principled ways to compute upper-conﬁdence bounds for the
general  online reinforcement learning setting. We make a step towards computing such values
incrementally  under function approximation  by providing upper-conﬁdence bounds for value
estimates made by LSTD  for a ﬁxed policy. We approximate these bounds to create a new algorithm
for control—called Upper-Conﬁdence-Least-Squares (UCLS).
3 Estimating Upper-Conﬁdence Bounds for Policy Evaluation using LSTD

Consider the goal of obtaining a conﬁdence interval around value estimates learned incrementally by
LSTD for a ﬁxed policy ⇡. The value estimate is x>w for state-action features x for the current state
and action. We would like to guarantee  with probability 1  p for a small p > 0  that the conﬁdence

3

interval around this estimate contains the value x>w⇤ given by the optimal w⇤ 2W . To estimate
such an interval without parametric assumptions  we use Chebyshev’s inequality which—unlike other
concentration inequalities like Hoeffding or Bernstein—does not require independent samples.
To use this inequality  we need to determine the variance of the estimate x>w; the variance of the
estimate  given x  is due to the variance of the weights. Let w⇤ be ﬁxed point solution for the
projected Bellman operator for the -return—the TD ﬁxed point  for a ﬁxed policy ⇡. To characterize
the noise for this optimal estimator  let ⌫t be the TD-error for the optimal weights w⇤  where

with E[⌫tzt] = 0.

rt+1 = (xt  xt+1)>w⇤ + ⌫t

(2)
The expectation is taken across all states weighted by the sampling distribution  typically the stationary
distribution d⇡ : S! [0 1) or in the off-policy case the stationary distribution of the behaviour
policy. We know that E[⌫tzt] = 0  by the deﬁnition of the Projected Bellman Error ﬁxed point.
This noise ⌫t is incurred from the variability in the reward  the variability in the transition dynamics
and potentially the capabilities of the function approximator. A common assumption—when using
linear regression for contextual bandits [20] and for reinforcement learning [31]—is that the variance
of the target is a constant value 2 for all contexts x. Such an assumption  however  is likely to
produce larger conﬁdence intervals than necessary. For example  consider a one-state world with
two actions  where one action has a high variance reward and the other has a lower variance reward
(see Appendix A  Figure 4). A global sample variance will encourage both actions to be taken many
times. For data-efﬁcient exploration  however  the agent should take the high-variance action more 
and only needs a few samples from the low-variance action.
We derive a conﬁdence interval for LSTD  in Theorem 1. We also derive the conﬁdence interval
assuming a global variance in Corollary 1  to provide a comparison. We compare to using this global-
variance upper-conﬁdence bound in our experiments  and show that it results in signiﬁcantly worse
performance than using a context-dependent variance. Note that we do not assume AT is invertible;
if we did  the big-O term in (3) below would disappear. We include this term for preciseness of the
result—even though we will not estimate it—because for smaller T   AT is unlikely to be invertible.
However  we expect this big-O term to get small quickly  and be dominated by the other terms. In our
algorithm  therefore  we ignore the big-O term.
Theorem 1. Let ¯⌫T
Let ✏⇤T
is invertible. Assume that the following are all ﬁnite: E[A+
state-action features x. With probability at least 1  p  given state-action features x 
T ¯⌫T ¯⌫>T A+>T ]x + OE[(x>✏⇤T )2]

T is the pseudoinverse of AT .
T AT  I)w⇤ reﬂect the degree to which AT is not invertible; it is zero when AT
T ¯⌫T + ✏⇤T ] and all
x>w⇤  x>wT +q p+1

Proof: First we compute the mean and variance for our learned parameters. Because rt+1 =
(xt  xt+1)>w⇤ + ⌫t 

p qx>E[A+

T PT1

t=0 zt⌫t and wT = A+

T bT where A+

T ¯⌫T + ✏⇤T ]  V[A+

def= (A+

def= 1

(3)

T

wT = 1
T1Xt=0
T 1

= A+

T

ztrt+1!

T

zt(xt  xt+1)>!1 1
T1Xt=0
zt((xt  xt+1)>w⇤ + ⌫t)!
T1Xt=0
T 1

zt⌫t!

T

T1Xt=0

= A+

T AT w⇤ + A+

= w⇤ + A+

T ¯⌫T + ✏⇤T

This estimate has a small amount of bias  that vanishes asymptotically. But  for a ﬁnite sample 

E"A+
T 1

T

T1Xt=0

zt⌫t!# 6= E[A+

T ]E" 1

T

zt⌫t# = 0.

T1Xt=0

Further  because AT may not be invertible  there is an additional error ✏⇤T term which will vanish
with enough samples  i.e.  once AT can be guaranteed to be invertible.

4

For covariance  because

wT  E[wT ] =w⇤ + A+

= A+

the covariance of the weights is

T ¯⌫T + ✏⇤T )⇤

T ¯⌫T + ✏⇤T  E⇥w⇤ + A+

T ¯⌫T + ✏⇤T  E⇥A+
V[wT ] = V⇥A+

T ¯⌫T + ✏⇤T⇤
T ¯⌫T + ✏⇤T⇤
Pr⇣|X  E[X]| <✏pV[X]⌘  1 
Pr⇣|X  E[X]| <q 1
ppV[X]⌘  1  p

1
✏2

The goal for computing variances is to use a concentration inequality. Chebyshev’s inequality1 states
that for a random variable X  if the E[X] and V[X] are bounded  then for any ✏  0:

If we set ✏ =p1/p  then this gives

Now we have characterized the variance of the weights  but what we really want is to characterize the
variance of the value estimates. Notice that the variance of the value-estimate  for state-action x is

V[x>wT|x] = E[x>wT w>T x|x]  E[x>wt|x]2
= x>E[wT w>T ]  E[wT ]E[wT ]> x

= x>V[wT ]x

(4)

def= A+

1

1



=

Therefore  the variance of the estimate is characterized by the variance of the weights. With high
probability 

def= E[A+

T ¯⌫T )> + ✏⇤T ✏⇤>T .

T ¯⌫T ✏⇤>T + ✏⇤T (A+

T ¯⌫T + ✏⇤T ] and ⌃⇤T

T ¯⌫T + ✏⇤T ]

x>wT  x>w⇤ =x>(wT  E[wT ]) + x>(E[wT ]  w⇤)
x>(wT  E[wT ]) +x>(E[wT ]  w⇤)
ppqx>V⇥A+
T ¯⌫T + ✏⇤T⇤ x +x>E[A+
ppqx>E⇥A+
T ¯⌫T ¯⌫>T A+>T + ⌃⇤T⇤  µ⇤T µ⇤>T  x +qx>µ⇤T µ⇤>T x (5)
where Equation 4 uses Chebyshev’s inequality  and the last step is a rewriting of Equation 4 using the
deﬁnitions µ⇤T
To simplify (5)  we need to determine an upper bound for the general formula cpa2  b2 + b where
a  b  0. Because p < 1  we know that c =p1/p  1. Therefore  the extremal points for b  b = a
and b = 0  both result in an upper bound of ca. Taking the derivative of the objective  gives a single
stationary point in-between [0  a]  with b = apc2+1. The value at this point evaluates to be apc2 + 1.
Therefore  this objective is upper-bounded by apc2 + 1.
Now for a2 = x>E⇥A+
T ¯⌫T ¯⌫>T A+>T + ⌃⇤T⇤ x  the term involving x>E [⌃⇤T ] x should quickly
T ¯⌫T )(x>✏⇤T ) + (x>✏⇤T )2⇤  which results in the additional O(E[(x>✏⇤T )2]) in the bound.
E⇥2(x>A+
T PT1
action features x. With probability at least 1  p  given state-action features x 

⌅
Corollary 1. Assume that ⌫t are i.i.d.  with mean zero and bounded variance 2. Let ¯zT =
T ¯zT ¯z>T A+>T ] and all state-
1

disappear  since it is only due to the potential lack of invertibility of AT . This term is equal to

t=0 zt and assume that the following are ﬁnite: E[✏⇤T ]  V[✏⇤T ]  E[A+

x>w⇤  x>wT + q p+1

(6)
1Bernstein’s inequality cannot be used here because we do not have independent samples. Rather  we
characterize behaviour of the random variable w  using variance of w  but cannot use bounds that assume w is
the sum of independent random variables. The bound with Chebyshev will be loose  but we can better control
the looseness of the bound with the selection of p and the constant in front of the square root.

T ¯zT ¯z>T A+>T ]x + OE[(x>✏⇤T )2]

p qx>E[A+

5

Proof: The result follows similarly to above  with some simpliﬁcations due to global-variance:

E⇥A+

T ¯⌫T⇤ = EhEhA+

E[A+

T ¯⌫T ¯⌫>T A+>T ] = 2E[A+

T ¯⌫TS0  ....  STii = E"A+

T ¯zT ¯z>T A+>T ]

T

1
T

T1Xt=0

ztEh⌫tS0  ....  STi# = 0

⌅

4 UCLS: Estimating upper-conﬁdence bounds for LSTD in control

In this section  we present Upper-Conﬁdence-Least-Squares (UCLS)2  a control algorithm  which
incrementally estimates the upper-conﬁdence bounds provided in Theorem 1  for guiding on-policy
exploration. The upper-conﬁdence bounds are sound without requiring i.i.d. assumptions; however 
they are derived for a ﬁxed policy. In control  the policy is slowly changing  and so instead we will be
slowly tracking this upper bound. The general strategy  like policy iteration  is to slowly estimate both
the value estimates and the upper-conﬁdence bounds  under a changing policy that acts greedily with
respect to the upper-conﬁdence bounds. Tracking these upper bounds incurs some approximations;
we identify and address potential issues here. The complete psuedocode for UCLS is given in the
Appendix (Algorithm 2).
First  we are not evaluating one ﬁxed policy; rather  the policy is changing. The estimates AT and bT
will therefore be out-of-date. As is common for LSTD with control  we use an exponential moving
average  rather than a sample average  to estimate AT   bT and the upper-conﬁdence bound. The
exponential moving average uses AT = (1  )AT1 + zT (xt  xt+1)>  for some  2 [0  1].
If  = 1/T   then this reduces to the standard sample average; otherwise  for a ﬁxed   such as
 = 0.01  more recent samples have a higher weight in the average. Because an exponential average
is unbiased  the result in Theorem 1 would still hold  and in practice the update will be more effective
for the control setting.
Second  we cannot obtain samples of the noise ⌫t = rt+1 + t+1x>t+1w⇤  x>t w⇤  which is the
TD-error for the optimal value function parameters w⇤ (see Equation (2)). Instead  we use t as a
proxy. This proxy results in an upper bound that is too conservative—too loose—because t is likely
to be larger than ⌫t. This is likely to ensure sufﬁcient exploration  but may cause more exploration
than is needed. The moving average update

¯⌫t = ¯⌫t1 + t(tzt  ¯⌫t1)

(7)

T ¯⌫T ¯⌫>T A1

should also help mitigate this issue  as older t are likely larger than more recent ones.
Third  the covariance matrix C estimating E[A1
T ] could underestimate covariances  de-
pending on a skewed distribution over states and depending on the initialization. This is particularly
true in early learning  where the distribution over states is skewed to be higher near the start state;
a sample average can result in underestimates in as yet unvisited parts of the space. To see why 
let a = A1
T ¯⌫T . The covariance estimate Cij = E[aiaj] corresponds to feature i and j. The agent
begins in a certain region of the space  and so features that only become active outside of this region
will be zero  providing samples aiaj = 0. As a result  the covariance is artiﬁcially driven down
in unvisited regions of the space  because the covariance accumulates updates of 0. Further  if
the initialization to the covariance Cii is an underestimate  a visited state with high variance will
artiﬁcially look more optimistic than an unvisited state.
We propose two simple approaches to this issue: updating C based on locality and adaptively
adjusting the initialization to Cii. Each covariance estimate Cij for features i and j should only be
updated if the sampled outer-product is relevant  with the agent in the region where i and j are active.
To reﬂect this locality  each Cij is updated with the aiaj only if the eligibility traces is non-zero for i
and j. To adaptively update the initialization  the maximum observed a2
i is stored  as cmax  and the
initialization c0 to each Cii is retroactively updated using

Cii = Cii  (1  )cic0 + (1  )cicmax

2We do not characterize the regret of UCLS  and instead similarly to policy iteration  rely on a sound update
under a ﬁxed policy to motivate incrementally estimating these values as if the policy is ﬁxed and then acting
according to them. The only model-free algorithm that achieves a regret bound is RLSVI  but that bound is
restricted to the ﬁnite horizon  batch  tabular setting. It would be a substantial breakthrough to provide such a
regret bound  and is beyond the scope of this work.

6

where ci is the number of times Cii has been updated. This update is equivalent to having initialized
Cii = cmax. We provide a more stable retroactive update to Cii  in the pseudocode in Algorithm 2 
that is equivalent to this update.
Fourth  to improve the computational complexity of the algorithm  we propose an alternative 
incremental strategy for estimating w  that takes advantage of the fact that we already need to
estimate the inverse of A for the upper bound. In order to do so  we make use of the summarized
information in A to improve the update  but avoid directly computing A1 as it may be poorly
conditioned. Instead  we maintain an approximation B ⇡ A> that uses a simple gradient descent
update  to minimize kA>Bxt  xtk2
2. If B is the inverse of A>  then this loss is zero; otherwise 
minimizing it provides an approximate inverse. This estimate B is useful for two purposes in the
algorithm. First  it is clearly needed to estimate the upper-conﬁdence bound. Second  it also provides
a pre-conditioner for the iterative update w = w + G(b  Aw)  for preconditioner G. The optimal
preconditioner is in fact the inverse of A  if it exists. We use G = B> + ⌘I for a small ⌘> 0
to ensure that the preconditioner is full rank. Developing this stable update for LSTD required
signiﬁcant empirical investigation into alternatives; in addition to providing a more practical UCLS
algorithm  we hope it can improve the use of LSTD in other applications.
5 Experiments

We conducted several experiments to investigate the beneﬁts of UCLS’ directed exploration against
other methods that use conﬁdence intervals for action selection  to evaluate sensitivity of UCLS’s
performance with respect to its key parameter p  and to contrast the advantage contextual variance
estimates offer over global variance estimates in control. Our experiments were intentionally con-
ducted in small—though carefully selected—simulation domains so that we could conduct extensive
parameter sweeps  hundreds of runs for averaging  and compare numerous state-of-the-art exploration
algorithms (many of which are computationally expensive on larger domains). We believe that such
experiments constitute a signiﬁcant contribution  because effectively using conﬁdence bounds for
model free-exploration in RL is still in its infancy—not yet at the large-scale demonstration state–with
much work to be done. This point is highlighted nicely below as we demonstrate that several recently
proposed exploration methods fail on these simple domains.
5.1 Algorithms
We compare UCLS to DGPQ [8]  UCBootstrap [46]  our extension of LSPI-Rmax to an incremental
setting [19] and RLSVI [31]. In-depth descriptions of each algorithm and implementation details
can be found in the Appendix. These algorithms are chosen because they either keep conﬁdence
intervals explicitly  as in UCBootstrap  or implicitly as in DGPQ and RLSVI. In addition  we included
LSPI-Rmax as a natural alternative approach to using LSTD to maintain optimistic value estimates.
We also include Sarsa with ✏-greedy  with ✏ optimized over an extensive parameter sweep. Though
✏-greedy is not a generally practical algorithm  particularly in larger worlds  we include it as a
baseline. We do not include Sarsa with optimistic initialization  because even though it has been a
common heuristic  it is not a general strategy for exploration. Optimistic initialization can converge
to suboptimal solutions if initial optimism fades too quickly [46]. Further  initialization only happens
once  at the beginning of learning.
If the world changes  then an agent relying on systematic
exploration due to its initialization may not react  because it no longer explores. For completeness
comparing to previous work using optimistic initialization  we include such results in Appendix G.
5.2 Environments
Sparse Mountain Car is a version of classic mountain car problem Sutton and Barto [40]  only
differing in the reward structure. The agent only receives a reward of +1 at the goal and 0 otherwise 
and a discounted  episodic  of 0.998. The start state is sampled from the range [0.6 0.4] with
velocity zero. This domain is used to highlight how exploration techniques perform when the reward
signal is sparse  and thus initializing the value function to zero is not optimistic.
Puddle World is a continuous state 2-dimensional world with (x  y) 2 [0  1]2 with 2 puddles: (1)
[0.45  0.4] to [0.45  0.8]  and (2) [0.1  0.75] to [0.45  0.75] - with radius 0.1 and the goal is the region
(x  y) 2 ([0.95  1.0]  [0.95  1.0]). The agent receives a reward of 1400⇤d on each time step  where
d denotes the distance between the agent’s position and the center of the puddle  and an undiscounted 
episodic  of 1.0. The agent can select an action to move 0.05 + ⇣  ⇣ ⇠ N (µ = 0  2 = 0.01).

7

10

Steps per
Episode
x*10^3
(better
perf.)

4

2

RLSVI
LSPI-Rmax

UCBootstrap

ε-Greedy

UCLS

5

10

Episodes

Sparse Mountain Car

12

Negated

Total
Reward
2^x
(better
perf.)

6
5

30

LSPI-Rmax

UCBootstrap

RLSVI

DGPQ

UCLS

50 100

Episodes
Puddle World

350

UCLS

DGPQ

UCBootstrap

Optimal

4
(better
perf.)
Total
Reward
10^x

LSPI-Rmax

RLSVI

ε-Greedy

5

10

1

Steps (x*10^3)

25

River Swim

Figure 1: A comparison of speed of learning in Sparse Mountain Car  Puddle World and River Swim.
In plots (a) and (b) lower on y-axis are better  whereas in (c) curves higher along y-axis are better.
Sparse Mountain Car and Puddle World are episodic problems with a ﬁxed experience budget. Thus
the length of the lines in plots (a) and (b) indicate how many episodes each algorithm completed over
50 000 steps  and the height on the y-axis indicates the quality of the learned policy—lower indicates
better performance. Note RLSVI did not show signiﬁcant learning after 50 000 steps. The RLSVI
result in Puddle World uses a budget of 1 million.
The agent’s initial state is uniformly sampled from (x  y) 2 ([0.1  0.3]  [0.45  0.65]). This domain
highlights a common difﬁculty for traditional exploration methods: high magnitude negative rewards 
which often cause the agent to erroneously decrease its value estimates too quickly.
River Swim is a standard continuing exploration benchmark [42] inspired by a ﬁsh trying to swim
upriver  with high reward (+1) upstream which is difﬁcult to reach and  a lower but still positive
reward (+0.005)  which is easily reachable downstream. We extended this domain to continuous states
in [0  1]  with a stochastic displacement of 0.1 when taking an action up or down  with low-probability
of success for up. The starting position is sampled uniformly in [0  0.1]  and  = 0.99.

5.3 Experimental Setup

We investigate a learning regime where the agents are allowed a ﬁxed budget of interaction steps with
the environment  rather than allowing a ﬁnite number of episodes of unlimited length. Our primary
concern is early learning performance  thus each experiment is restricted to 50 000 steps  with an
episode cutoff (in Sparse Mountain Car and Puddle World) at 10 000 steps. In this regime  an agent
that spends a signiﬁcant time exploring the world during the ﬁrst episode may not be able to complete
many episodes  the cutoff makes exploration easier given the strict budget on experience. Whereas 
in the more common framework of allowing a ﬁxed number of episodes  an agent can consume many
steps during the ﬁrst few episodes exploring  which is difﬁcult to detect in the ﬁnal performance
results. We average over 100 runs in River Swim and 200 runs for the other domains . For all the
algorithms that utilize eligibility traces we set  to be 0.9. For algorithms which use exponential
averaging   is set to 0.001  and the regularizer ⌘ is set to be 0.0001. The parameters for UCLS
are ﬁxed. RLSVI’s weights are recalculated using all experienced transitions at the beginning of
an episode in Puddle World and Sparse Mountain Car  and every 5 000 steps in River Swim. The
parameters of competitors  where necessary  are selected as the best from a large parameter sweep.
All the algorithms except DGPQ use the same representation: (1) Sparse Mountain Car - 8 tilings
of 8x8  hashed to a memory space of 512  (2) River Swim - 4 tilings of granularity 32  hashed to a
memory space of 128  and (3) Puddle World - 5 tilings of granularity 5x5  hashed to a memory space
of 128. DGPQ uses its own kernel-based representation with normalized state information.

5.4 Results & Analysis

Our ﬁrst experiment simply compares UCLS against other control algorithms in all the domains.
Figure 1 shows the early learning results across all three domains. In all three domains UCLS achieves
the best ﬁnal performance. In Sparse Mountain Car  UCLS learns faster than the other methods 
while in River Swim DGPQ learns faster initially. UCBootstrap and UCLS learn at a similar rate in
Puddle World  which is a cost-to-goal domain. UCBootstrap  and bootstrapping approaches generally 
can suffer from insufﬁcient optimism  as they rely on sufﬁciently optimistic or diverse initialization
strategies [46  30]. LSPI-Rmax and RLSVI do not perform well in any of the domains. DGPQ does
not perform as well as UCLS in Puddle World  and exhibits high variance compared with the other
methods. In Puddle World  UCLS goes on to ﬁnish 1200 episodes in the alloted budget of steps 

8

10-1

Optimal

4

4

Optimal

Total
Reward
(10^x)

2

1

Total
Reward
(10^x)

2

1

10-5

1

Suboptimal Random
Steps (x*10^4)

2
UCLS

5

10-5

10-1

Suboptimal Random
Steps (x*10^4)

2

1

5

GV-UCB

Figure 2: The effect of the
conﬁdence parameter p on
the policy 
in River Swim 
using context-dependent vari-
ance (UCLS) and global vari-
ance (GV-UCB). The values
for p are {105  [1  2  . . .   9] ⇥
103  102  101}.

whereas in River Swim both UCLS and DGPQ get close to the optimal policy by the end of the
experiment.
The DGPQ algorithm uses the maximum reward (Rmax) to initialize the Gaussian processes. In
Sparse Mountain Car this effectively converts the problem back into the traditional -1 per-step
formulation. In this traditional variant of Mountain Car UCLS signiﬁcantly outperforms DGPQ
(Appendix G). Sarsa with ✏-greedy learns well in Puddle world as it is a cost-to-goal problem in
which by default Sarsa uses optimistic initialization  and therefore is reported in the Appendix. .
Next we investigated the impact of the conﬁdence level 1  p  on the performance of UCLS in River
Swim. The conﬁdence interval radius is proportional top1 + 1/p; smaller p should correspond to a
higher rate of exploration. In Figure 2  smaller p resulted in a slower convergence rate  but all values
eventually reach the optimal policy.
Finally  we investigate the beneﬁt using contextual variance estimates over global variance estimates
within UCLS. In Figure 2  we also show the effect of various p values on the performance of the
algorithm resulting from Corollary 1  which we call Global Variance-UCB (GV-UCB) (see Appendix
E.1 for more details about this algorithm). For this range of p  UCLS still converges to the optimal
policy  albeit at different rates. Using a global variance estimates (GV-UCB)  on the other hand 
results in signiﬁcant over-estimates of variance  resulting in poor performance.
6 Conclusion and Discussion

This paper develops a sound upper-conﬁdence bound on the value estimates for least-squares tem-
poral difference learning (LSTD)  without making i.i.d. assumptions about noise distributions. In
particular  we allow for context-dependent noise  where variability could be due to noise in rewards 
transition dynamics or even limitations of the function approximator. We then introduce an algorithm 
called UCLS  that estimates these upper-conﬁdence bounds incrementally  for policy iteration. We
demonstrate empirically that UCLS requires far fewer exploration steps to ﬁnd high-quality policies
compared to several baselines  across domains chosen to highlight different exploration difﬁculties.
The goal of this paper is to provide an incremental  model-free  data-efﬁcient  directed exploration
strategy. The upper-conﬁdence bounds for action-values for ﬁxed policies are one of the few available
under function approximation  and so a step towards exploration with optimistic values in the general
case. A next step is to theoretically show that using these upper bounds for exploration ensures
stochastic optimism  and so converges to optimal policies.
One promising aspect of UCLS is that it uses least-squares to efﬁciently summarize past experience 
but is not tied to a speciﬁc state representation. Though we considered a ﬁxed representation for
UCLS  it is feasible that an analysis for the non-stationary case could be used as well for the setting
where the representation is being adapted over time. If the representation drifts slowly  then UCLS
may be able to similarly track the upper-conﬁdence bounds. Recent work has shown that combining
deep Q-learning with Least-squares can result in signiﬁcant performance gains over vanilla DQN[18].
We expect that combining deep networks and UCLS could result in even larger gains  and is a natural
direction for future work.
7 Acknowledgements

We would like to thank Bernardo Ávila Pires and Jian Qian for their helpful comments  alongwith
Calcul Québec (www.calculquebec.ca) and Compute Canada (www.computecanada.ca) for the
computing resources used in this work.

9

References
[1] Y. Abbasi-Yadkori and C. Szepesvari. Bayesian Optimal Control of Smoothly Parameterized Systems: The

Lazy Posterior Sampling Algorithm. In Uncertainty in Artiﬁcial Intelligence  2014.

[2] P. Auer and R. Ortner. Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning.

Advances in Neural Information Processing Systems  2006.

[3] P. L. Bartlett and A. Tewari. REGAL - A Regularization based Algorithm for Reinforcement Learning in

Weakly Communicating MDPs. In Conference on Uncertainty in Artiﬁcial Intelligence  2009.

[4] J. A. Boyan. Technical update: Least-squares temporal difference learning. Machine learning  49(2-3):

233–246  2002.

[5] S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning. Machine

learning  22(1-3):33–57  1996.

[6] R. Brafman and M. Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforce-

ment learning. The Journal of Machine Learning Research  2003.

[7] W. Chu  L. Li  L. Reyzin  and R. E. Schapire. Contextual Bandits with Linear Payoff Functions. In

International Conference on Artiﬁcial Intelligence and Statistics  2011.

[8] R. Grande  T. Walsh  and J. How. Sample Efﬁcient Reinforcement Learning with Gaussian Processes. In

International Conference on Machine Learning  2014.

[9] T. Jaksch  R. Ortner  and P. Auer. Near-optimal Regret Bounds for Reinforcement Learning. The Journal

of Machine Learning Research  2010.

[10] N. Jong and P. Stone. Model-based exploration in continuous state spaces. Abstraction  Reformulation 

and Approximation  2007.

[11] T. Jung and P. Stone. Gaussian processes for sample efﬁcient reinforcement learning with RMAX-like

exploration. In Machine Learning: ECML PKDD  2010.

[12] L. P. Kaelbling. Learning in embedded systems. MIT press  1993.

[13] L. P. Kaelbling  M. L. Littman  and A. W. Moore. Reinforcement learning: A survey. Journal of Artiﬁcial

Intelligence Research  1996.

[14] S. Kakade  M. Kearns  and J. Langford. Exploration in metric state spaces. In International Conference on

Machine Learning  2003.

[15] K. Kawaguchi. Bounded Optimal Exploration in MDP. In AAAI Conference on Artiﬁcial Intelligence 

2016.

[16] M. J. Kearns and S. P. Singh. Near-Optimal Reinforcement Learning in Polynomial Time. Machine

Learning  2002.

[17] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. The Journal of Machine Learning Research 

2003.

[18] N. Levine  T. Zahavy  D. J. Mankowitz  A. Tamar  and S. Mannor. Shallow updates for deep reinforcement

learning. In Advances in Neural Information Processing Systems  pages 3138–3148  2017.

[19] L. Li  M. Littman  and C. Mansley. Online exploration in least-squares policy iteration. In International

Conference on Autonomous Agents and Multiagent Systems  2009.

[20] L. Li  W. Chu  J. Langford  and R. E. Schapire. A contextual-bandit approach to personalized news article

recommendation. In World Wide Web Conference  2010.

[21] J. Martin  S. N. Sasikumar  T. Everitt  and M. Hutter. Count-Based Exploration in Feature Space for

Reinforcement Learning. In International Joint Conference on Artiﬁcial IntelligenceI  2017.

[22] N. Meuleau and P. Bourgine. Exploration of Multi-State Environments - Local Measures and Back-

Propagation of Uncertainty. Machine Learning  1999.

[23] C. D. Meyer  Jr. Generalized inversion of modiﬁed matrices. SIAM Journal on Applied Mathematics  24

(3):315–323  1973.

10

[24] K. S. Miller. On the inverse of the sum of matrices. Mathematics magazine  54(2):67–72  1981.

[25] T. M. Moerland  J. Broekens  and C. M. Jonker. Efﬁcient exploration with Double Uncertain Value

Networks. In Advances in Neural Information Processing Systems  2017.

[26] A. Nouri and M. L. Littman. Multi-resolution Exploration in Continuous Spaces. In Advances in Neural

Information Processing Systems  2009.

[27] R. Ortner and D. Ryabko. Online Regret Bounds for Undiscounted Continuous Reinforcement Learning.

In Advances in Neural Information Processing Systems  2012.

[28] I. Osband and B. Van Roy. Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

In International Conference on Machine Learning  2017.

[29] I. Osband  D. Russo  and B. Van Roy. (More) Efﬁcient Reinforcement Learning via Posterior Sampling. In

Advances in Neural Information Processing Systems  2013.

[30] I. Osband  C. Blundell  A. Pritzel  and B. Van Roy. Deep Exploration via Bootstrapped DQN. In Advances

in Neural Information Processing Systems  2016.

[31] I. Osband  B. Van Roy  and Z. Wen. Generalization and Exploration via Randomized Value Functions. In

International Conference on Machine Learning  2016.

[32] G. Ostrovski  M. G. Bellemare  A. van den Oord  and R. Munos. Count-Based Exploration with Neural

Density Models. In International Conference on Machine Learning  2017.

[33] J. Pazis and R. Parr. PAC optimal exploration in continuous space Markov decision processes. In AAAI

Conference on Artiﬁcial Intelligence  2013.

[34] M. Plappert  R. Houthooft  P. Dhariwal  S. Sidor  R. Y. Chen  X. Chen  T. Asfour  P. Abbeel  and

M. Andrychowicz. Parameter Space Noise for Exploration. arXiv.org  2017.

[35] S. P. Singh  T. S. Jaakkola  M. L. Littman  and C. Szepesvari. Convergence Results for Single-Step

On-Policy Reinforcement-Learning Algorithms. Machine Learning  2000.

[36] A. Strehl and M. Littman. Exploration via model based interval estimation. In International Conference

on Machine Learning  2004.

[37] A. L. Strehl  L. Li  E. Wiewiora  J. Langford  and M. L. Littman. PAC model-free reinforcement learning.

In International Conference on Machine Learning  2006.

[38] R. Sutton  C. Szepesvári  A. Geramifard  and M. Bowling. Dyna-style planning with linear function

approximation and prioritized sweeping. In Conference on Uncertainty in Artiﬁcial Intelligence  2008.

[39] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning  1988.

[40] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press Cambridge  1998.

[41] C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool Publishers  2010.

[42] I. Szita and A. Lorincz. The many faces of optimism. In International Conference on Machine Learning 

2008.

[43] I. Szita and C. Szepesvari. Model-based reinforcement learning with nearly tight exploration complexity

bounds. In International Conference on Machine Learning  2010.

[44] H. van Seijen and R. Sutton. A deeper look at planning as learning from replay. In International Conference

on Machine Learning  2015.

[45] M. White. Unifying task speciﬁcation in reinforcement learning. In International Conference on Machine

Learning  2017.

[46] M. White and A. White. Interval estimation for reinforcement-learning algorithms in continuous-state

domains. In Advances in Neural Information Processing Systems  2010.

[47] M. A. Wiering and J. Schmidhuber. Efﬁcient Model-Based Exploration. In Simulation of Adaptive Behavior

From Animals to Animats  1998.

11

,Raksha Kumaraswamy
Matthew Schlegel
Adam White
Martha White