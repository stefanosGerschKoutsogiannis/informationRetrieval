2018,Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo,Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional  distributions in  Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order ``leapfrog" implementation has long been conjectured to run in $d^{1/4}$ gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data.  Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone.  Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an ``incoherence" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions  which grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover  our simulations on synthetic data suggest that  when our regularity condition is satisfied  leapfrog HMC performs better than its competitors -- both in terms of accuracy and in terms of the number of gradient evaluations it requires.,Dimensionally Tight Bounds for Second-Order

Hamiltonian Monte Carlo

Oren Mangoubi

EPFL

omangoubi@gmail.com

Nisheeth K. Vishnoi

EPFL

nisheeth.vishnoi@gmail.com

Abstract

Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-
dimensional distributions in Statistics and Machine learning. HMC is known to run
very efﬁciently in practice and its popular second-order “leapfrog" implementation

has long been conjectured to run in d1~4 gradient evaluations. Here we show that

this conjecture is true when sampling from strongly log-concave target distributions
that satisfy a weak third-order regularity property associated with the input data.
Our regularity condition is weaker than the Lipschitz Hessian property and allows
us to show faster convergence bounds for a much larger class of distributions than
would be possible with the usual Lipschitz Hessian constant alone. Important
distributions that satisfy our regularity condition include posterior distributions
used in Bayesian logistic regression for which the data satisﬁes an “incoherence"
property. Our result compares favorably with the best available bounds for the class

of strongly log-concave distributions  which grow like d1~2 gradient evaluations

with the dimension. Moreover  our simulations on synthetic data suggest that 
when our regularity condition is satisﬁed  leapfrog HMC performs better than its
competitors – both in terms of accuracy and in terms of the number of gradient
evaluations it requires.

Introduction

1
Sampling problems are ubiquitous in a wide range of scientiﬁc and engineering disciplines and have
received signiﬁcant attention in Machine Learning and Statistics. In a typical sampling problem  one

wants to generate samples from a given target distribution π(x)∝ e
−U(x)  where one is given access
to a function U∶ Rd→ R and possibly its gradient∇U. In many situations  such as when d is large 
short size η  meaning that they typically only travel a distance roughly proportional to√
i× η in i

sampling problems are computationally difﬁcult  and Markov chain Monte Carlo (MCMC) algorithms
are used. MCMC algorithms generate samples by running a Markov chain which converges to the
target distribution π. Unfortunately  many MCMC algorithms work by taking independent steps of

steps  preventing the algorithm from quickly exploring the target distribution.
One MCMC algorithm that can take large steps is the Hamiltonian Monte Carlo (HMC) algorithm.
Each step of the HMC Markov chain involves simulating the trajectory of a particle in the “potential
well” U  with the trajectory determined by Hamilton’s equations from classical mechanics [2  38].
To ensure randomization  the momentum is refreshed after each step by independently sampling from
a multivariate Gaussian. HMC is a natural approach to the sampling problem because Hamilton’s
equations preserve the target distribution π. This convenient property reduces the need for frequent
Metropolis corrections which slow down traditional MCMC algorithms  and allows HMC to take
large steps. HMC was ﬁrst discovered by physicists [14]  was adopted soon afterwards with much
success in Bayesian Statistics and Machine learning [36  37]  and is currently the main algorithm
used in the popular software package Stan [5]. Despite its popularity and the widespread belief
that HMC is faster than its competitor algorithms in a wide range of high-dimensional sampling
problems [11  1  38  3]  its theoretical properties are not as well-understood as its older competitor
MCMC algorithms  such as the random walk Metropolis [35] or Langevin [16  17  13] algorithms.
The lack of theoretical results makes it more difﬁcult to tune the parameters of HMC  and prevents us

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

from having a good understanding of when HMC is faster than its competitor algorithms. Several
recent papers have begun to bridge this gap  showing that HMC is geometrically ergodic for a large
class of problems [30  18] and proving quantitative bounds for the convergence rate of an idealized
version of HMC on Gaussian target distributions [42]. Building on probablistic coupling techniques
developed in [42]  [16] and [13]  [33] later proved a bound of O
ﬁrst-order implementation of HMC when U is m-strongly convex with M-Lipschitz gradient (here
the O
accuracy parameters  and polylogarithmic factors of d). When the dimension is large  computing
O
common to use the second-order “leapfrog" implementation of HMC  which is conjectured to require
O
results [27  39]. Very recently  [33] made some progress towards this conjecture by proving that
O

2) gradient evaluations for a
∗(d 1
∗ notation only includes dependence on d and excludes regularity parameters such as M  m 
2) gradient evaluations can be prohibitively slow. For this reason  in practice it is much more
∗(d 1
4) gradient evaluations based on previous simulation [14] and asymptotic “optimal scaling"
∗(d 1
∗(d 1
4) gradient evaluations are required in the special case where U is separable into orthogonal
O(1)-dimensional strongly convex components satisfying Lipschitz gradient  Lipschitz Hessian and
∗(d 1
4) gradient evaluations. Roughly  our

fourth-order regularity conditions.
Our contributions. We introduce a new  and much weaker  regularity condition that allows us
to show that  in many cases  HMC requires at most O
regularity condition allows the Hessian to change quickly in “bad" directions associated with the data 
while at the same time guaranteeing that the Hessian changes slowly in the directions traveled by
the HMC chain with high probability (Assumption 1). The fact that our regularity condition need
not hold for the “worst-case" directions allows us to show desired bounds on the number of gradient
evaluations for a much larger class of distributions than would be possible with more conventional
regularity conditions such as the Lipschitz Hessian property. Under our regularity condition we show
bounds of O
from a large class of strongly log-concave target distributions (Theorem 1). Next  we show that our
regularity condition is satisﬁed by posterior distributions used in Bayesian logistic “ridge" regression.
Computing these posterior distributions is important in statistics and Machine learning applications
[40  22  32  44  25] and quantitative convergence bounds give insight into which MCMC algorithm
to use for a given application  and how to optimally tune the algorithm’s parameters. Finally  we
perform simulations to evaluate the performance of the HMC algorithm analyzed in this paper  and
show that its performance is competitive in both accuracy and speed with the Metropolis-adjusted
version of HMC despite the lack of a Metropolis ﬁlter  when performing Bayesian logistic regression
on synthetic data.
Related work. Hamiltonian Monte Carlo. The earliest theoretical analyses of HMC were the
asymptotic “optimal scaling" results of [27]  for the special case when the target distribution is
a multivariate Gaussian. Speciﬁcally  they showed that the Metropolis-adjusted implementation
of HMC with leapfrog integrator requires a numerical stepsize of O

∗(d
4) to maintain an Ω(1)
− 1
Metropolis acceptance probability in the limit as the dimension d→∞. They then showed that for
∗(d 1
4) in the large-d limit. More recently 

∗(d 1
4) gradient evaluations for the leapfrog implementation of HMC when sampling

this choice of numerical stepsize the number of numerical steps HMC requires to obtain samples
from Gaussian targets with a small autocorrelation is O
[39] have extended their asymptotic analysis of the acceptance probability to more general classes of
separable distributions.
The earliest non-asymptotic analysis of an HMC Markov chain was provided in [42] for an idealized
version of HMC based on continuous Hamiltonian dynamics  in the special case of Gaussian target
distributions. [33] show that idealized HMC can sample from general m-strongly logconcave target
m (see also [4] for more recent
work on idealized HMC). They also show that an unadjusted implementation of HMC with ﬁrst-order

distributions with M-Lipschitz gradient in ˜O(κ2) steps  where κ∶= M
discretization can sample with Wasserstein error ε> 0 in ˜O(d 1
˜ε∶= ε~√
separable target distributions in ˜O(d 1
total variation (TV) error ε> 0 of roughly ˜O( 1

(non-polynomial) function of m  M  B  if the operator norms of the ﬁrst four Fréchet derivatives of
the restriction of U to the coordinate directions are bounded by B. [28] use the conductance method
to show that an idealized version of the Riemannian variant of HMC (RHMC) has mixing time with
4   where R is a

−1) gradient evaluations  where
−1f(m  M  B)) gradient evaluations  where f is an unknown

ψ2T 2 R log( 1

ε))  for any 0≤ T ≤ d
− 1

regularity parameter for U and ψ is an isoperimetric constant for π.

M. In addition  they show that a second-order discretization of HMC can sample from

2 κ6.5 ˜ε

4 ˜ε

2

2 κ2 ˜ε

[21]. Therefore  since most of the probability of a standard spherical Gaussian lies in a ball of radius

target distribution with TV error ε. Interestingly  the only result [19] we are aware of specialized for

sufﬁciently regular target distribution. One should then be able to apply results such as [31] to show

from [12]  [7  15] show bounds for ULA in KL divergence. [9] show that underdamped Langevin

Langevin Algorithms. [17] show that the unadjusted Langevin algorithm (ULA) can generate a sample

gradient evaluations from a warm start in the TV metric.
Hit and run  ball walk  Random walk Metropolis (RWM). The Hit-and-run  ball walk  and RWM algo-

from π with TV error ε> 0 in ˜O(dκ2ε
−2) gradient evaluations. Using optimization-based techniques
requires ˜O(d 1
−1) gradient evaluations for Wasserstein error ε> 0 (see also [8]). [19] show
that the Metropolis-adjusted Langevin algorithm (MALA) requires ˜O(max(dκ  d 1
2 κ1.5) log( 1
ε))
rithms are all thought to have a step size of roughly Θ(1) on sufﬁciently regular target distributions
d  one would expect all three of these algorithms to take roughly(√
√
d)2= d steps to explore a
ε)) target function evaluations to sample from the
that  from a warm start  RWM requires ˜O(dκ log( 1
ε) target function evaluations for RWM.
the strongly log-concave case gives a bound of d2κ2 log( 1
Hamiltonian Dynamics. A Hamiltonian of a simple system in Rd isH(q  p)= U(q)+ 1
2p2
where q∈ Rd represents the “position” of a particle in this system  p∈ Rd the “momentum ” U the
2p2
2 the “kinetic energy.” For ﬁxed q  p∈ Rd  we denote by{qt(q  p)}t≥0 
{pt(q  p)}t≥0 the solutions to Hamilton’s equations:
dqt(q  p)
= pt(q  p)
with initial conditions q0(q  p)= q and p0(q  p)= p. When the initial conditions(q  p) are clear
from the context  we write qt  pt in place of qt(q  p) and pt(q  p). The gradient−∇U in the second
tinuous Hamiltonian dynamics  with update rule Xi+1= qT(Xi  pi)  where p1  p2  . . .∼ N(0  Id) are
iid. Since solutions to Hamilton’s equations have invariant distribution∝ e
− 1
p2
idealized HMC has stationary distribution π(q)∝ e
−U(q) equal to the target distribution  without

Hamilton equation is thought of as a “force" which acts on the particle.
HMC. We ﬁrst consider an idealized version of the HMC Markov chain X0  X1  . . . based on the con-

2 Hamilton’s equations and the Hamiltonian Monte Carlo algorithm
In this section we present the background and HMC algorithm; see [38  2] for a thorough treatment.

=−∇U(qt(q  p)) 

−H(q p)= e

needing a correction such as Metropolis adjustment. This allows HMC to take much larger steps 
and hence mix faster  than would otherwise be possible. It is not possible to implement an HMC
Markov chain with continuous trajectories  so one must discretize these trajectories using a numerical
integrator  such as the popular second-order leapfrog integrator (Step 5 in the algorithm below).
In this case  one obtains the following unadjusted HMC (UHMC) Markov chain. The number of
gradient evaluations required by UHMC is the main object of study in this paper.

dpt(q  p)

dt

“potential energy ” and 1

2 

(1)

−U(q)

e

2

2 

dt

and

0∈ Rd  oracle for gradient∇U  T> 0  imax∈ N  discretization level η> 0

from the (following) UHMC Markov chain

imax

0  . . .   X†

Algorithm 1 Unadjusted HMC
input: Initial point X†
output: Samples X†

1: for i= 0 to imax− 1 do
2: Sample pi∼ N(0  Id)
3: Set q0= X†
i and p0= pi
ηæ− 1 do
for j= 0 toæ T
5: Set qj+1= qj+ ηpj− 1
i+1= qæ T
æ

end for

4:

2 η2∇U(qj) 

pj+1= pj− 1

2 η∇U(qj)− 1

2 η∇U(qj+1)

η

6:
7: Set X†
8: end for
Initialization: In this paper we prove gradient evaluation bounds for UHMC from both a warm start
and cold start  which we deﬁne as follows:
Deﬁnition 1. (Warm start) Let X0  X1  . . . be a Markov chain  and let π be our target distribution. We
M

say that X has an(ω  ˆδ)-warm start if there is a random variable ˜Y0∼ π such thatX0− ˜Y02< ω~√
with probability 1− ˆδ for some ω  ˆδ> 0.

3

d gradient evaluation bound barrier in prior approaches and present

∀x  y  p∈ Rd
√

√

target function U itself. However  in other applications it can take 2d times as many arithmetic

noting that if one attempts to bound the number of gradient evaluations required by HMC using a
conventional Lipschitz bound on the Hessian

(Hy− Hx)p2≤ L2y− x2×p2

(2)

3 Regularity conditions
In this section we explain the

√

Deﬁnition 2. (Cold start) We say that X has a cold start if X0= x
Since UHMC requires Θ( T
number of gradient evaluations required by UHMC is Θ(imax× T

∶= argminx∈Rd U(x).
  with x
η) gradient evaluations to compute each Markov chain step i  the total
η). Note that the parameters imax 

T   η are chosen by the user  and the optimal choice of these algorithm parameters may depend on the
dimension d and the regularity parameters of U such as M and m.
Remark 1. The number of arithmetic operations required to compute the gradient depends on how
the function U is provided to us in a given application. In the Bayesian logistic regression application
analyzed at the end of Section 4 of this paper  the number of arithmetic operations required to

that is deﬁned with respect to the Euclidean norm  then the bounds that one obtains are no faster
than
d gradient evaluations. The reason is that if we use the usual “Euclidean" Lipschitz Hessian
d  since (from a warm
condition to bound the numerical error  we obtain an error bound of roughly

of these trajectories has Euclidean norm
d with high probability (w.h.p.). To bound the error of a
second-order method such as the leapfrog method used by HMC  we must bound the change of the
directional derivative of the gradient along the path taken by the trajectories of the Markov chain.
In particular  when the leapfrog integrator (step 5 of Algorithm 1) takes a numerical step from qj

compute the gradient∇U is Θ(d)  and is the same number of operations required to evaluate the
operations to compute the gradient∇U as it takes to compute the target function U.
our regularity condition that overcomes it. Let Hx denote the Hessian of U at x∈ Rd. We start by
√
start) the trajectories of HMC travel with momentum roughly N(0  Id)  implying that the momentum
to roughly qj+1≈ qj+ ηpj  one component of the error in computing the continuous Hamiltonian
trajectory can be bounded by the quantity(η2Hqj+ηpj− η2Hqj)pj2. This quantity in turn can be
bounded using the Lipschitz Hessian constant by η2L2ηpj2×pj2. Since pj is roughly N(0  Id)
we havepj2≈√
∗(1). To obtain an error bound of
η2L2d for the error of computing an entire HMC trajectory if T= Θ
d). When computing a trajectory of length Θ
ε we therefore need η= O
∗(1) with this stepsize
∗(√
√
d) numerical steps. To overcome this
grow as quickly with the dimension as the Euclidean norm for a random N(0  Id) momentum vector.
We need a better way to bound the quantity(η2Hqj+ηpj− η2Hqj)pj2. One way to do so would
(Hy−Hx)v2≤ L∞×y−x∞v∞ for some constant L∞> 0. For this norm pj∞= O(log(d))
with high probability since  roughly  pj ∼ N(0  Id)  implying that(η2Hqj+ηpj− η2Hqj)pj2 is
bounded by roughly η2L∞ log(d) rather than η2L2d.
Since for many distributions of interest this condition does not hold for a small value of L∞  we
generalize this condition  to obtain a smaller L∞ constant for a wider class of distributions. Towards
this end  we deﬁne the vector (semi)-norm⋅∞ u with respect to the collection of unit vectors
u∶={u1  . . .   ur} byx∞ u∶= maxi∈{1 ... r}u
i x. The usual inﬁnity norm is just a special case

of this new norm if we set ui = ei to be the coordinate vectors. Under this more general norm 
the magnitude of a random N(0  Id) vector still grows only logarithmically with d  since each
i x is a univariate standard normal. The associated matrix normA∞ u is deﬁned to be

supx∞ u≤1Ax2. Using this norm  and motivated by the discussion above  we arrive at our new
very quickly in r> 0 “bad" directions u1  . . .   ur  as long as it does not change quickly on average in
Assumption 1 (Inﬁnity-norm Lipschitz condition). There exist L∞> 0  r∈ N  and a collection of
unit vectors u={u1  . . .   ur}⊆ Sd  such thatHy− Hx∞ u≤ L∞√ry− x∞ u for all x  y∈ Rd.

d gradient evaluation
η  we therefore need to compute O
barrier  we therefore need to control the change in the Hessian with respect to a norm which does not

regularity condition. Roughly speaking  our new regularity condition allows the Hessian to change

be to replace the Euclidean Lipschitz Hessian condition with an inﬁnity-norm Lipschitz condition

d w.h.p.  which gives an error bound of η3L2d for one leapfrog step and roughly

∗(1~√

component u

a random direction.

4

the target functions used in logistic regression. This condition may also be of independent interest.
Additional conditions for cold starts. When proving bounds from a cold start  roughly speaking
we would still like to guarantee that the HMC trajectories travel with speed O

We expect this assumption to hold when the target function U is of the form U(x)=∑r
i=1 fi(u
i x)

for functions fi∶ R→ R with uniformly bounded third derivatives. In particular  this class includes
∗(1) in any of the “bad"
directions  so thatpt∞ u= O
∗(1). However  unlike from a warm start  we have no guarantee that
the momentum is roughly N(0  Id). To boundpt∞ u we therefore need another way to control
i pt in each “bad" direction ui. To do so  we would like to guarantee
the growth of the quantityu



that the bounds on the “force" acting on our Hamiltonian trajectory in each ui direction depend only
i pt of the position and momentum in that direction  regardless of the
i qt and u
on the components u
Assumption 2 (Gaussian tail bound condition (for cold start only)). There exists a constant b> 0 
component of the momentum orthogonal to ui. Towards this end  we assume the following:
and a collection of unit vectors u={u1  . . .   ur}⊆ Sd  such that min{mu
)}−
(x− x
∇U(x)≤ max{mu
b≤ u

(x− x
)}+ b for all x∈ Rd  u∈ u.

(x− x

)  M u

)  M u

(x− x

sample X†
X†

imax

from π such thatX†

m-strongly convex  M-gradient Lipschitz  and satisﬁes Assumption 1. Then there exist parameters

of gradient evaluations for both a warm and cold start. Here we focus on the warm start result; see the
arXiv version [34] for bounds from a cold start and a more formal statement of our warm start result.

Assumption 2 gaurantees that the component of the gradient in each “bad" direction ui is bounded
solely in terms of the component of the position in that same “bad" direction. This allows us to apply
arguments based on Gronwall’s inequality on the projection of the trajectory in each bad direction
ui in order to bound the magnitude of the position and momentum at time t in the direction ui.
Using Grownwall’s inequality  we bound the component of the initial position and momentum in the
direction ui  without assuming a warm start.
4 Theoretical results
Our main result is a bound on the number of gradient evaluations required by HMC with second-order
leapfrog integrator under the inﬁnity-norm Lipschitz condition (Assumption 1)  when sampling from

π(x)∝ e
−U(x) if U is m-strongly convex with M-Lipschitz gradient. We bound the required number
−U(x) where U∶ Rd→ R is
Theorem 1 (Bounds for second-order HMC  informal). Let π(x)∝ e
T  η  imax  such that from an(ω  δ)-warm start  Algorithm 1 generates an approximate independent
imax− Y2< ε for some Y ∼ π independent of the initial point
0 with probability at least 1− δ. Moreover UHMC requires at most ˜O(d1~4ε
L∞ log1~2( 1
δ))
gradient evaluations whenever m  M  ω= O(1)  r= ˜O(d) and L∞= Ω(1).
More generally  for arbitrary m  M and r  we show (from a warm start with ω = O(1)) that
 ˜L∞κ2.25 ˜ε
2( 1
the number of gradient evaluations is ˜O(maxd1~4κ2.75  r1~4
δ))  where
−1~2 log
˜L∞∶= L∞√
and κ∶= M
˜O(maxd1~4κ3.5  r1~4
M and ˜b∶= b~√
−1~2)  where ˜ε∶= ε~√
If κ= O(1)  L∞ = O(1) and r= O(d)  then our bound on the number of gradient evaluations
2) from a warm start. To the best of our knowledge  our bounds are an improvement
is O(d 1
− 1
√
∗(d 1
4) bounds in the special case of
a statistical model is oftentimes very large [26  22]  and in many cases κ and L∞ do not grow  or only
mT 2)  and
1To obtain bounds from a warm start  we run UHMC with parameters T=(6
η= Θ(min{d
δ)). From a cold start  we run UHMC with param-
−0.75}˜ε
2( 1
−1~2∞ κ
−1~4κ
− 1
−1.25  r
√
mT 2)  and η= Θ(min{d
M κ)−1  imax= Θ( 1
−1~2).
eters T=(6
−2  r
− 1
−1~4 ˜L

over all previous gradient evaluation bounds for sampling in this regime  which all have dimension
dependence
product distributions  unlike in [33] the condition number dependence in our bounds is polynomial.
We are especially interested in the regime where d is large since the number of predictor variables in

grow relatively slowly  with the dimension. We state some concrete examples from Bayesian logistic
regression of regimes where our gradient evaluation bounds are an improvement on the previous best
bounds in the discussion after Theorem 2.

m is the “condition number". From a cold start  under the additional Gaus-
sian tail bound condition (Assumption 2)  we show that the number of gradient evaluations is

√
M κ)−1  imax= Θ( 1
−1~2∞ (κ2.75+ ˜bκ1.75)−1}˜ε1~2M

 ˜L∞κ4.25+ ˜bκ3.25 ˜ε

d or greater. Also note that while [33] obtains O

− 1

1
2 M

2 log

M

4 ˜ε

−1~2√

1

M. 1

−1~4 ˜L

4 κ

5



U(θ)= 1~2θ

In Bayesian logistic “ridge" regression  one would like to

Applications to logistic regression.
sample from the target log-density

i=1Yi log(F(θ

Xi))+(1− Yi) log(F(−θ
−1θ−∑r

where the data vectors X1  . . . Xr ∈ Rd are thought of as independent variables  the binary data
−s+ 1)−1 is the logistic function  and Σ is
Y1  . . .   Yr∈{0  1} are dependent variables  F(s)∶=(e
j=1X
inc(X1  . . . Xr)∶= maxi∈[r]∑r
i Xj.


positive deﬁnite. We deﬁne the incoherence of the data as

Xi)) 

(3)



Σ

√

8 ε

dε

.

4 ε

In all these

since C  M  m
distributed  we have

 ˜L∞ = ˜O(d 1

and the angle between any two of the remaining vectors is greater than π

spectively. To obtain bounds in the cold start setting  we show Assumption 2 is satisﬁed with

The proof of Theorem 2 is given in the arXiv version [34]. In particular  when the incoherence is

We also provide bounds for m and M  showing that our Lipschitz gradient and strong con-

Xi2r
C and “bad" directions u= Xi
i=1

We bound the value of the inﬁnity-Lipschitz constant in terms of the incoherence:
Theorem 2 (Regularity bounds for logistic regression). Let U be the logistic regression target for

r> 0 data vectors X1  . . .   Xr  and let inc(X1  . . .   Xr)≤ C for some C> 0. Then the inﬁnity-norm
Lipschitz assumption is satisﬁed with L∞=√
˜O(1)  the constant L∞ does not grow with dimension: This includes the separable case when the Xi
where r= d and the Xi are unit vectors with the ﬁrst
2− 1
vectors are orthogonal and have unit magnitude. It also includes  for instance  the non-separable case
the number of gradient evaluations required by UHMC under a standard normal prior is ˜O(d 1
2) 
d of the Xi vectors isotropically distributed 
− 1
d. In both these examples
−1  (and therefore κ and ˜L∞) are all ˜O(1). When all r= d vectors are isotropically
8) and require ˜O(d 3
2) gradient evaluations.
− 1
examples we therefore obtain an improvement over the existing ˜O(√
−1) bounds of [9  33].
vexity assumptions are satisﬁed for M = λmaxΣ
k and m = λmin(Σ
−1+∑r
−1)  re-

k=1 XkX
b= 2inc(X1~X12  . . . Xr~Xr2) if Σ is a multiple of the identity (see arXiv version [34] for proofs).
algorithm is given a warm start and where m  M = Θ(1); the general case is proved in the arXiv
in [33]  it is enough for us to bound the approximation errorX†
i − Xi2< ε for all i≤I  where 
roughly I= Θ(log 1
ε) is a bound on the mixing time of X  if each HMC trajectory is run for time
T= Θ(1) .
∗(d 1
4) gradient evaluation bounds  it is enough to show that an error
4)  since the HMC algorithm
i− Xi2< ε holds for a numerical timestep-size η= Ω
∗(d
boundX†
− 1
computesI= O
∗(d 1
4) gradient
∗(1) trajectories and for this choice of η each trajectory takes T
η = O
i− Xi2 is bounded by ε for
evaluations to compute. Our goal is therefore to show that the errorX†
4).
∗(d
all i≤I whenever η= O
− 1
start) the momentum of the HMC trajectories is roughly N(0  Id) to show that the continuous HMC
the momentum of the HMC trajectories satisfypt∞ u= O(log(d)) at every time t∈[0  T]. We

5 Proof overview of Theorem 1
For simplicity of exposition  in this proof overview we consider the special case where the HMC

version [34]. Recall that Algorithm 1 generates a Markov chain X† which approximates the steps
taken by the idealized HMC chain X. Since the idealized HMC chain X was shown to mix quickly

trajectories of the idealized chain X are unlikely to travel quickly in any of the “bad" directions ui
speciﬁed in Assumption 1 (Step 2). Speciﬁcally  we show that at every step i with high probability

The structure of our proof is as follows: We begin by bounding the local error of the leapfrog
integrator accumulated at each numerical step (Step 1). Then  we use the fact that (from a warm

To prove the conjectured O

then combine steps 1 and 2 to show that the numerical HMC chain also does not travel too quickly in
any of the “bad" directions  and use this fact together with our bounds in Step 2 to bound the global
error of the numerical HMC trajectories (Step 3). Finally  we compute the value of η needed to bound
the error ε  and use this to bound the number of gradient evaluations.
Note that when proving bounds from a cold start we use the additional Assumption 2 instead of the
invariant Gibbs distribution to control the behavior of the trajectories. Due to limited space  formal
proofs are deferred to the arXiv version [34] of our paper.

6

(6)

2

≈ pj− η∇U(qj)− 1
√
r≈ tL∞pt2∞ u
√r.

Step 1: Error bounds for leapfrog integrator.
In this subsection we show how to use Assumption
1 to bound the error of the leapfrog integrator. We are unaware of non-asymptotic second-order
bounds for the leapfrog integrator  since the previous error bounds for leapfrog we are aware of
only hold in the limit as the numerical step size η goes to zero [3  33  1  29  24]. For this reason 
we prove new non-asymptotic polynomial time bounds for leapfrog here. Key to our analysis is

2 η2∇U(qj) returned by the leapfrog
the observation that the position estimate qj+1= qj+ ηpj− 1
integrator is exactly the second-order Taylor expansion for qη(qj  pj)  and the momentum estimate
2 η∇U(qj+1) approximates (with third-order error) the second-order Taylor
pj+1∶= pj− 1
expansion for pη(qj  pj) in the following way:
η2∇U(qj+1)−∇U(qj)
trajectory. Roughly  we can use Assumption 1 to bound the error in the Hessian at each time 0≤ t≤ η:
(Hqt− Hq0)pt2≤ L∞pt∞ u×(qt− q0)∞ u

2 η∇U(qj)− 1
pj+1= pj− η∇U(qj)− 1

The error in the Taylor expansion is due to the fact that the Hessian Hqj is not constant over the

η2Hqj pj.

√

(4)

(5)

r.

2

η

Using Equations (4) and (5)  we get an error bound of roughly

pj+1− pt(qj  pj)2≤ η3L∞supt∈[0 η]pt(qj  pj)2∞ u

plished using standard techniques which do not require Assumption 1.

this  we would like to use the fact that the position and momentum of HMC trajectories from an
idealized HMC chain started at the stationary distribution π  are jointly distributed according to the

distribution  it remains stationary distributed at every step and the position qt and momentum pt
2 at any ﬁxed time t. Using the fact

(7)
with high probability for the idealized HMC chain. To do so  we use the fact that (from a warm start)

Finally  we note that bounding the error for the position variableqj+1− qt(qj  pj)2 can be accom-
Step 2: Boundingpt∞ u for the idealized HMC chain. Since the error bound of the leapfrog
integrator depends crucially onpt∞ u  our next task is to show that
pt∞ u≤ O(polylog(d  1~δ)+ ω)
the distribution of the momentum at any point on an HMC trajectory is roughly N(0  Id). To show
Gibbs distribution∝ e
2a: Boundingpt∞ u from a stationary start We ﬁrst consider a copy ˜Yi of the idealized chain
started at the stationary distribution ˜Y0 ∼ π  and show that the momentum pt of its trajectories
satisﬁespt∞ u= O(log(d)) at every time t w.h.p. Since the ˜Y chain is started at the stationary
of its trajectories have Gibbs distribution∝ e
i pt is chi-distributed with 1 degree of freedom  we apply the
that for every bad direction ui u

Hanson-Wright inequality together with a union bound to show thatu
i pt∞ u= O(log(d~ξ)) at any

ﬁxed time t with probability at least 1− ξ for any ξ> 0. However  our goal is to boundu
i pt∞ u

problem  we considerJ = poly(κ  d) equally spaced timepoints on the interval[0  T]  and apply a
union bound to show thatu
i pt∞ u= polylog(d  1~δ) with probability at least 1− δ. We then use the

trajectory  implying that the position and momentum do not change by more that O(1) inside each
time interval of length 1J . This in turn implies thatpt∞ u= polylog(d  1~δ) at every time t∈[0  T].
2b: Boundingpt∞ u from a warm start Unfortunately  we cannot apply our results of step 2a
only assume thatX0− ˜Y02< ω for some ω> 0  where ˜Y0∼ π is at the stationary distribution. To
using the update rule ˜Yi+1= qT( ˜Yi  pi) with the same sequence of initial momenta p1  p2  . . . that
pi at every step  we show that at every continuous time t∈[0  T] the Euclidean distance between the
havept(Xi  pi)− pt( ˜Yi  pi)∞ u≤pt(Xi  pi)− pt( ˜Yi  pi)2≤ ω.

show that the trajectories of our warm-started chain also approximately satisfy this Gibbs distribution
property  we couple the two copies X and ˜Y of the idealized HMC chain by deﬁning the ˜Y chain

simultaneously at every time t  not just at a ﬁxed time. Unfortunately  the trajectories are continuous
paths  so we cannot directly apply a union bound to obtain a bound at every t. To get around this

directly since we are only assuming that X0 has a warm start  not a stationary start. That is  we

were used to deﬁne the X chain. Using the fact that the trajectories share the same initial momentum

position and momentum of the trajectories of the two chains remains bounded by ω. We therefore

“conservation of energy" property to bound the Euclidean norm of the momentum at every time on the

2 at any given time t.

− 1

2

pt2

−U(qt)

− 1

2

pt2

e

−U(qt)

e

7

Eq. 7= polylog(d  1~δ)+ ω.

If we can extend this bound to the numerical chain  we can apply it to Inequality (6) to show that the

Step 3: Bounding the global error and the number of gradient evaluations. So far  we have

pj∞ u≤pjη(Xi  pi)∞ u+ jηε

(8)
Then one can use similar “conservation of energy" arguments as in the previous section to show that

shown that the trajectories of the idealized HMC chain X satisfy a bound onpt∞ u (Equation (7)).
error at each step is O(η3L∞√r) w.h.p. To bound the global error  we use roughly the following
inductive argument: inductively assume that the errorsqj− qjη(Xi  pi)2 andpj− pjη(Xi  pi)2
at numerical step j are bounded by roughly jη× ε. This implies that
pt(qj  pj)∞ u= polylog(d  1~δ) over the short time interval t∈[0  η]. Plugging this bound into
Inequality (6) allows us to bound the error accumulated at step j by O(η3L∞√r)  implying that the
inductive assumption also holds for step j+ 1.
η numerical steps  the global error of each trajectory is therefore bounded by T× η2L∞√r.
The error at step i is bounded by i× T η2L∞√r. Finally  we conclude thatX†
i − Xi2 < ε for
2 T). Since the algorithm uses a total of T
ε)) whenever η
i ≤ I = O(log( 1
−1 = ˜Θ(r 1
− 1
numerical steps to compute Xi for i≤I  for T= Θ(1) the number of gradient evaluations is roughly
 ˜L∞ε
 ˜L∞ε
2). When r= O(d)  the number of gradient evaluations is roughly ˜O(d 1
2).
˜O(r 1
− 1
− 1
Remark 2. More generally  we can consider Hamiltonian trajectories with HamiltonianH(q  p)=
U(q)+ 1

by using a mass matrix Ω= cId for some constant c> 0 and by choosing an appropriate integration
of the form Ω= cId for some constant c is equivalent to rescaling U by a constant factor and tuning
the integration time T   we analyze the case Ω= Id and then “tune" our algorithm by setting T= √
M= 1 and κ= 1

6  
m to determine the number of gradient evaluations. (A more general mass matrix Ω

time T (as well as choosing other parameters such as numerical step-size). Since using a mass matrix

Ωp  where Ω is called the mass matrix. In practice  one tunes the algorithm parameters

 ˜L∞ε

4

After T

4

2 p

4

η

m

is equivalent to applying a pre-conditioner on U.)
6 Simulations
6.1 Accuracy and autocorrelation time of Unadjusted HMC
The purpose of our ﬁrst set of simulations is to show that in practical situations analyzed in this paper
the unadjusted HMC algorithm (UHMC) is competitive with other popular sampling algorithms in
terms of both accuracy and in terms of the number of gradient evaluations required. We compare
UHMC to Metropolis-adjusted HMC (MHMC) [14]  the Metropolis-adjusted Langevin algorithm
(MALA) [41] and the unadjusted Langevin algorithm (ULA) [41]. All simulations were implemented
on MATLAB (see the GitHub repository https://github.com/mangoubi/HMC for our MATLAB
code used to implement these algorithms).
We consider the setting of Bayesian logistic regression with standard normal prior  with synthetic

for Z1  . . .   Zr ∼ N(0  Id) iid  for
“independent variable" data vectors generated as Xi = Zi
Zi2
dimension d= 1000 and r= d. To generate the synthetic “dependent variable" binary data  a vector
β=(β1  . . .   βd) of regression coefﬁcients was ﬁrst generated as β= WW2
where W∼ N(0  Id). The
random variables  setting Yi= 1 with probability
and Yi= 0 otherwise. Each Markov chain
was initialized at a point X0 chosen randomly as X0∼ N(0  Id).

binary dependent variable synthetic data Y1  . . .   Yd were then generated as independent Bernoulli

1+e−βXi

1

Figure 1: Marginal accuracy (left) and autocorrelation time (middle) vs. numerical step size η for MALA  ULA 
MHMC  and UHMC. The log plot (right) compares an estimate for the quantity used to obtain second-order
numerical error bounds using the Euclidean Lipschitz and inﬁnity-norm Lipschitz constants  at different d.
To compare the accuracy  we computed the “marginal accuracy" (MA) of the samples generated
by each chain over a ﬁxed number (50 000) of numerical steps for different step sizes η in the

8

0.10.20.30.40.50.60.950.960.970.980.99Marginal AccuracyMALAULAMHMCUHMC0.10.20.30.40.50.6050100150Autocorellation TimeMALAULAMHMCUHMC1011021030246810u√

1

interval[0.1  0.6] (Figure 1  left). Among all four of the algorithms  we found that UHMC had the
highest accuracy at the accuracy-optimizing step size (the accuracy-optimizing step size was η= 0.35
test function f(x)=x1.2 We found that the autocorrelation time of UHMC was fastest at the
autocorrelation time-optimizing step size (the autocorrelation time-optimizing step size was η= 0.5

for UHMC). To compare the runtime we computed the autocorrelation time of the samples for a

3   rounded down to the nearest multiple of η.

for UHMC) (Figure 1  middle). When running UHMC and MHMC  we used a trajectory time T
equal to π
Remark 3. The marginal accuracy is used as a heuristic to compare accuracy of samplers (see
e.g. [18]  [20] and [10]). The marginal accuracy between the measure µ of a sample and the

target π is M A(µ  π)∶= 1− 1
i=1µi− πiTV  where µi and πi are the marginal distributions
2d∑d

L2pt2 and√

Assumption 1. We performed this comparison for the logistic regression example of the previous

At each value of d we used MATLAB’s “fminunc" function to search for the optimal values of L2 and

of µ and π for the coordinate xi. Since MALA is known to sample from the correct stationary
distribution and is geometrically ergodic for the class of distributions analyzed in this paper  we
used the samples generated after running MALA for a very long time (106 steps) to obtain a more
accurate approximation for π as a benchmark with which to compare the sampling accuracy of the
four different algorithms when run for a much shorter amount of time (50  000 numerical steps).
6.2 Comparing Euclidean and inﬁnity-norm Lipschitz conditions
The goal of our second set of simulations was to compare the optimal values of the usual Euclidean

Lipschitz Hessian constant L2 to the constant L∞ from our inﬁnity-norm Lipschitz condition of
simulation with synthetic data generated in the same way  but for different values of d  with r=
d. The optimal values of L2 and L∞ are L2 = supx y v∈Rd
y−x2v2(Hy− Hx)v2 and L∞ =
ry−x∞ uv∞ u(Hy− Hx)v2  with “sup" taken over points where function is deﬁned.
supx y v∈Rd
of the two quantities√
L∞. Recall that to bound the error of a numerical integrator with momentum pt  one may use one
4pt∞ u. We plot the median value of these quantities
for random momenta pt∼ N(0  Id) (Figure 1  right). Our results show that the median of√
L2pt2
increases with d at a faster rate than the median of√
4pt∞ u over the interval d∈[1  1000].
∗(d1~4) gradient

This suggests that bounds based on our inﬁnity-norm Lipschitz condition can be much tighter for
distributions used in practice than bounds based on the usual Euclidean Lipschitz condition.
7 Conclusions and future directions
In this paper  we show that the conjecture of [11]  which says that HMC requires O
evaluations  is true when sampling from strongly log-concave targets satisfying weak regularity
properties associated with the input data. In doing so  we introduce a new regularity property for the
Hessian (Assumption 1) that is much weaker than the Lipschitz Hessian property  and show that for a
class of functions arising in statistics and machine learning this property holds for natural conditions
on the data. One future direction is to further weaken Assumption 1  which says that the Hessian
does not change too quickly in all but a few ﬁxed bad directions  by instead allowing these directions
to vary based on the position x. Our simulations show that UHMC is competitive with MHMC on
synthetic data that satisﬁes our regularity assumption. Further  we show that the constant in our
regularity assumption grows much more slowly with the dimension than the Euclidean Lipschitz
constant of the Hessian. It would also be interesting to extend our results to non-logconcave targets 
and to kth-order numerical implementations of generalizations of HMC  such as RHMC.
Bounds for MHMC Another open problem is to show tight gradient evaluation bounds for MHMC.
Since the Metropolis-adjusted HMC Markov chain preserves the stationary distribution exactly 
it should be possible to show that the number of gradient evaluations is polylogarithmic in ε
improving on the number of gradient evaluations required by unadjusted HMC which grows like ε
2 .
Unfortunately  the probablistic coupling approach used in our current paper is unlikely to work for
MHMC  since Metropolis “accept/reject" steps tend to break the coupling of the two Markov chains if
the chains have different acceptance probabilities  causing one chain to accept its proposal while the
other rejects the proposal. An alternative approach might be to use a proof based on the conductance
method (see e.g. [43]). Unlike the coupling approach  the conductance method is compatible with
Metropolis “accept/reject" steps.

L∞r 1

2The autocorrelation time can be estimated as 1+ 2∑smax

some large smax[23  6].

s=1 ρs  where ρs is the autocorrelation at lag s for

−1 
− 1

1

L∞r 1

9

References
[1] Alexandros Beskos  Natesh Pillai  Gareth Roberts  Jesus-Maria Sanz-Serna  and Andrew Stuart. Optimal

tuning of the hybrid Monte Carlo algorithm. Bernoulli  19(5A):1501–1534  2013.

[2] Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo.

arXiv:1701.02434  2017.

arXiv preprint

[3] MJ Betancourt  Simon Byrne  and Mark Girolami. Optimizing the integrator step size for Hamiltonian

Monte Carlo. arXiv preprint arXiv:1411.6669  2014.

[4] Nawaf Bou-Rabee  Andreas Eberle  and Raphael Zimmer. Coupling and convergence for Hamiltonian

Monte Carlo. arXiv preprint arXiv:1805.00452  2018.

[5] Bob Carpenter  Andrew Gelman  Matt Hoffman  Daniel Lee  Ben Goodrich  Michael Betancourt  Michael A
Brubaker  Jiqiang Guo  Peter Li  and Allen Riddell. Stan: A probabilistic programming language. Journal
of Statistical Software  20:1–37  2016.

[6] Tianqi Chen  Emily Fox  and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Interna-

tional Conference on Machine Learning  pages 1683–1691  2014.

[7] Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Proceedings of

Algorithmic Learning Theory  pages 186–211  2018.

[8] Xiang Cheng  Niladri S Chatterji  Yasin Abbasi-Yadkori  Peter L Bartlett  and Michael I Jordan. Sharp
convergence rates for Langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648 
2018.

[9] Xiang Cheng  Niladri S. Chatterji  Peter L. Bartlett  and Michael I. Jordan. Underdamped Langevin

MCMC: A non-asymptotic analysis. In Conference on Learning Theory  pages 300–323  2018.

[10] Nicolas Chopin  James Ridgway  et al. Leave Pima indians alone: binary regression as a benchmark for

Bayesian computation. Statistical Science  32(1):64–87  2017.

[11] Michael Creutz. Global Monte Carlo algorithms for many-Fermion systems. Physical Review D  38(4):1228 

1988.

[12] Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte Carlo

and gradient descent. In Conference on Learning Theory  pages 678–689  2017.

[13] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  79(3):651–676 
2017.

[14] Simon Duane  Anthony D Kennedy  Brian J Pendleton  and Duncan Roweth. Hybrid Monte Carlo. Physics

letters B  195(2):216–222  1987.

[15] Alain Durmus  Szymon Majewski  and Bła˙zej Miasojedow. Analysis of Langevin Monte Carlo via convex

optimization. arXiv preprint arXiv:1802.09188  2018.

[16] Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted Langevin

algorithm. arXiv preprint arXiv:1605.01559  2016.

[17] Alain Durmus and Eric Moulines. Non-asymptotic convergence analysis for the Unadjusted Langevin

Algorithm. The Annals of Applied Probability  in press.

[18] Alain Durmus  Eric Moulines  and Eero Saksman. On the convergence of Hamiltonian Monte Carlo. arXiv

preprint arXiv:1705.00166  2017.

[19] Raaz Dwivedi  Yuansi Chen  Martin J Wainwright  and Bin Yu. Log-concave sampling: Metropolis-

Hastings algorithms are fast! In Conference on Learning Theory  pages 793–797  2018.

[20] Christel Faes  John T Ormerod  and Matt P Wand. Variational Bayesian inference for parametric and
nonparametric regression with missing data. Journal of the American Statistical Association  106(495):959–
971  2011.

[21] Andrew Gelman  Walter R Gilks  and Gareth O Roberts. Weak convergence and optimal scaling of random

walk Metropolis algorithms. The Annals of Applied Probability  7(1):110–120  1997.

10

[22] Alexander Genkin  David D Lewis  and David Madigan. Large-scale Bayesian logistic regression for text

categorization. Technometrics  49(3):291–304  2007.

[23] Jonathan Goodman and Jonathan Weare. Ensemble samplers with afﬁne invariance. Communications in

applied mathematics and computational science  5(1):65–80  2010.

[24] Ernst Hairer  Christian Lubich  and Gerhard Wanner. Geometric numerical integration illustrated by the

Störmer–Verlet method. Acta numerica  12:399–450  2003.

[25] Matthew D Hoffman and Andrew Gelman. The no-U-turn sampler: adaptively setting path lengths in

Hamiltonian Monte Carlo. Journal of Machine Learning Research  15(1):1593–1623  2014.

[26] Valen E Johnson. On numerical aspects of Bayesian model selection in high and ultrahigh-dimensional

settings. Bayesian analysis (Online)  8(4):741  2013.

[27] AD Kennedy and Brian Pendleton. Acceptances and autocorrelations in hybrid Monte Carlo. Nuclear

Physics B-Proceedings Supplements  20:118–121  1991.

[28] Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster
polytope volume computation. To appear in Proceedings of STOC 2018  arXiv preprint arXiv:1710.06261 
2017.

[29] Benedict Leimkuhler and Sebastian Reich. Simulating Hamiltonian dynamics  volume 14. Cambridge

university press  2004.

[30] Samuel Livingstone  Michael Betancourt  Simon Byrne  and Mark Girolami. On the geometric ergodicity

of Hamiltonian Monte Carlo. arXiv preprint arXiv:1601.08057  2016.

[31] László Lovász and Santosh Vempala. Hit-and-run is fast and fun. preprint  Microsoft Research  2003.

[32] David Madigan  Alexander Genkin  David D Lewis  and Dmitriy Fradkin. Bayesian multinomial logistic
regression for author identiﬁcation. In AIP Conference Proceedings  volume 803  pages 509–516. AIP 
2005.

[33] Oren Mangoubi and Aaron Smith. Rapid mixing of Hamiltonian Monte Carlo on strongly log-concave

distributions. arXiv preprint arXiv:1708.07114  2017.

[34] Oren Mangoubi and Nisheeth K Vishnoi. Dimensionally tight bounds for second-order Hamiltonian Monte

Carlo. arXiv preprint arXiv:1802.08898  2018.

[35] Jonathan C Mattingly  Natesh S Pillai  Andrew M Stuart  et al. Diffusion limits of the random walk

Metropolis algorithm in high dimensions. The Annals of Applied Probability  22(3):881–930  2012.

[36] Radford M Neal. Bayesian training of backpropagation networks by the hybrid Monte Carlo method.
Technical report  Technical Report CRG-TR-92-1  Dept. of Computer Science  University of Toronto 
1992.

[37] Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag  Berlin  1996.

[38] Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo 

2:113–162  2011.

[39] Natesh S Pillai  Andrew M Stuart  and Alexandre H Thiéry. Optimal scaling and diffusion limits for the

Langevin algorithm in high dimensions. The Annals of Applied Probability  22(6):2320–2356  2012.

[40] Nicholas G Polson  James G Scott  and Jesse Windle. Bayesian inference for logistic models using
Pólya–Gamma latent variables. Journal of the American statistical Association  108(504):1339–1349 
2013.

[41] Gareth O Roberts  Richard L Tweedie  et al. Exponential convergence of Langevin distributions and their

discrete approximations. Bernoulli  2(4):341–363  1996.

[42] Christof Seiler  Simon Rubinstein-Salzedo  and Susan Holmes. Positive curvature and Hamiltonian Monte

Carlo. In Advances in Neural Information Processing Systems  pages 586–594  2014.

[43] Santosh Vempala. Geometric random walks: a survey. Combinatorial and computational geometry 

52(573-612):2  2005.

[44] Tong Zhang and Frank J Oles. Text categorization based on regularized linear classiﬁcation methods.

Information retrieval  4(1):5–31  2001.

11

,Oren Mangoubi
Nisheeth Vishnoi