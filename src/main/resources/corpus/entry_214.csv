2012,A Spectral Algorithm for Latent Dirichlet Allocation,Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by \emph{multiple} latent factors (topics)  as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed  and the topics are hidden.  This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models  including Latent Dirichlet Allocation (LDA). For LDA  the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures  using only trigram statistics (\emph{i.e.}  third order moments  which may be estimated with documents containing just three words). The method  called Excess Correlation Analysis  is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover  the algorithm is scalable  since the SVDs are carried out only on $k \times k$ matrices  where $k$ is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.,A Spectral Algorithm for Latent Dirichlet Allocation

Anima Anandkumar
University of California

Irvine  CA

a.anandkumar@uci.edu

Dean P. Foster

University of Pennsylvania

Philadelphia  PA

dean@foster.net

Daniel Hsu

Microsoft Research

Cambridge  MA

dahsu@microsoft.com

Sham M. Kakade
Microsoft Research

Cambridge  MA

skakade@microsoft.com

National Institute of Standards and Technology∗

Yi-Kai Liu

Gaithersburg  MD

yi-kai.liu@nist.gov

Abstract

Topic modeling is a generalization of clustering that posits that observations
(words in a document) are generated by multiple latent factors (topics)  as op-
posed to just one. This increased representational power comes at the cost of
a more challenging unsupervised learning problem of estimating the topic-word
distributions when only words are observed  and the topics are hidden.
This work provides a simple and efﬁcient learning procedure that is guaranteed to
recover the parameters for a wide class of topic models  including Latent Dirichlet
Allocation (LDA). For LDA  the procedure correctly recovers both the topic-word
distributions and the parameters of the Dirichlet prior over the topic mixtures 
using only trigram statistics (i.e.  third order moments  which may be estimated
with documents containing just three words). The method  called Excess Corre-
lation Analysis  is based on a spectral decomposition of low-order moments via
two singular value decompositions (SVDs). Moreover  the algorithm is scalable 
since the SVDs are carried out only on k × k matrices  where k is the number
of latent factors (topics) and is typically much smaller than the dimension of the
observation (word) space.

1

Introduction

Topic models use latent variables to explain the observed (co-)occurrences of words in documents.
They posit that each document is associated with a (possibly sparse) mixture of active topics  and
that each word in the document is accounted for (in fact  generated) by one of these active topics.
In Latent Dirichlet Allocation (LDA) [1]  a Dirichlet prior gives the distribution of active topics
in documents. LDA and related models possess a rich representational power because they allow
for documents to be comprised of words from several topics  rather than just a single topic. This
increased representational power comes at the cost of a more challenging unsupervised estimation
problem  when only the words are observed and the corresponding topics are hidden.
In practice  the most common unsupervised estimation procedures for topic models are based on
ﬁnding maximum likelihood estimates  through either local search or sampling based methods  e.g. 
Expectation-Maximization [2]  Gibbs sampling [3]  and variational approaches [4]. Another body
of tools is based on matrix factorization [5  6]. For document modeling  a typical goal is to form
a sparse decomposition of a term by document matrix (which represents the word counts in each

∗Contributions to this work by NIST  an agency of the US government  are not subject to copyright laws.

1

document) into two parts: one which speciﬁes the active topics in each document and the other
which speciﬁes the distributions of words under each topic.
This work provides an alternative approach to parameter recovery based on the method of mo-
ments [7]  which attempts to match the observed moments with those posited by the model. Our
approach does this efﬁciently through a particular decomposition of the low-order observable mo-
ments  which can be extracted using singular value decompositions (SVDs). This method is simple
and efﬁcient to implement  and is guaranteed to recover the parameters of a wide class of topic
models  including the LDA model. We exploit exchangeability of the observed variables and  more
generally  the availability of multiple views drawn independently from the same hidden component.

1.1 Summary of contributions

We present an approach called Excess Correlation Analysis (ECA) based on the low-order (cross)
moments of observed variables. These observed variables are assumed to be exchangeable (and 
more generally  drawn from a multi-view model). ECA differs from Principal Component Analysis
and Canonical Correlation Analysis in that it is based on two singular value decompositions: the
ﬁrst SVD whitens the data (based on the correlation between two observed variables) and the second
SVD uses higher-order moments (third- or fourth-order moments) to ﬁnd directions which exhibit
non-Gaussianity  i.e.  directions where the moments are in excess of those suggested by a Gaussian
distribution. The SVDs are performed only on k×k matrices  where k is the number of latent factors;
note that the number of latent factors (topics) k is typically much smaller than the dimension of the
observed space d (number of words).
The method is applicable to a wide class of latent variable models including exchangeable and multi-
view models. We ﬁrst consider the class of exchangeable variables with independent latent factors.
We show that the (exact) low-order moments permit a decomposition that recovers the parameters for
model class  and that this decomposition can be computed using two SVD computations. We then
consider LDA and show that the same decomposition of a modiﬁed third-order moment correctly
recovers both the probability distribution of words under each topic  as well as the parameters of the
Dirichlet prior. We note that in order to estimate third-order moments in the LDA model  it sufﬁces
for each document to contain at least three words.
While the methods described assume exact moments  it is straightforward to write down the ana-
logue “plug-in” estimators based on empirical moments from sampled data. We provide a simple
sample complexity analysis that shows that estimating the third-order moments is not as difﬁcult as
it might na¨ıvely seem since we only need a k × k matrix to be accurate.
Finally  we remark that the moment decomposition can also be obtained using other techniques 
including tensor decomposition methods and simultaneous matrix diagonalization methods. Some
preliminary experiments illustrating the efﬁcacy of one such method is given in the appendix.
Omitted proofs  and additional results and discussion are provided in the full version of the paper [8].

1.2 Related work

Under the assumption that a single active topic occurs in each document  the work of [9] provides the
ﬁrst provable guarantees for recovering the topic distributions (i.e.  the distribution of words under
each topic)  albeit with a rather stringent separation condition (where the words in each topic are
essentially non-overlapping). Understanding what separation conditions permit efﬁcient learning
is a natural question; in the clustering literature  a line of work has focussed on understanding the
relationship between the separation of the mixture components and the complexity of learning. For
clustering  the ﬁrst provable learnability result [10] was under a rather strong separation condition;
subsequent results relaxed [11–18] or removed these conditions [19–21]; roughly speaking  learning
under a weaker separation condition is more challenging  both computationally and statistically.
For the topic modeling problem in which only a single active topic is present per document  [22]
provides an algorithm for learning topics with no separation requirement  but under a certain full
rank assumption on the topic probability matrix.
For the case of LDA (where each document may be about multiple topics)  the recent work of [23]
provides the ﬁrst provable result under a natural separation condition. The condition requires that

2

each topic be associated with “anchor words” that only occur in documents about that topic. This
is a signiﬁcantly milder assumption than the one in [9]. Under this assumption  [23] provide the
ﬁrst provably correct algorithm for learning the topic distributions. Their work also justiﬁes the
use of non-negative matrix (NMF) as a provable procedure for this problem (the original motivation
for NMF was as a topic modeling algorithm  though  prior to this work  formal guarantees as such
were rather limited). Furthermore  [23] provides results for certain correlated topic models. Our
approach makes further progress on this problem by relaxing the need for this separation condition
and establishing a much simpler procedure for parameter estimation.
The underlying approach we take is a certain diagonalization technique of the observed moments.
We know of at least three different settings which use this idea for parameter estimation.
The work in [24] uses eigenvector methods for parameter estimation in discrete Markov models
involving multinomial distributions. The idea has been extended to other discrete mixture models
such as discrete hidden Markov models (HMMs) and mixture models with a single active topic
in each document (see [22  25  26]). For such single topic models  the work in [22] demonstrates
the generality of the eigenvector method and the irrelevance of the noise model for the observations 
making it applicable to both discrete models like HMMs as well as certain Gaussian mixture models.
Another set of related techniques is the body of algebraic methods used for the problem of blind
source separation [27]. These approaches are tailored for independent source separation with addi-
tive noise (usually Gaussian) [28]. Much of the literature focuses on understanding the effects of
measurement noise  which often requires more sophisticated algebraic tools (typically  knowledge
of noise statistics or the availability of multiple views of the latent factors is not assumed). These
algebraic ideas are also used by [29  30] for learning a linear transformation (in a noiseless setting)
and provides a different provably correct algorithm  based on a certain ascent algorithm (rather than
joint diagonalization approach  as in [27])  and a provably correct algorithm for the noisy case was
recently obtained by [31].
The underlying insight exploited by our method is the presence of exchangeable (or multi-view)
variables (e.g.  multiple words in a document)  which are drawn independently conditioned on the
same hidden state. This allows us to exploit ideas both from [24] and from [27]. In particular  we
show that the “topic” modeling problem exhibits a rather simple algebraic solution  where only two
SVDs sufﬁce for parameter estimation.
Furthermore  the exchangeability assumption permits us to have an arbitrary noise model (rather
than an additive Gaussian noise  which is not appropriate for multinomial and other discrete distri-
butions). A key technical contribution is that we show how the basic diagonalization approach can
be adapted for Dirichlet models  through a rather careful construction. This construction bridges the
gap between the single topic models (as in [22  24]) and the independent latent factors model.
More generally  the multi-view approach has been exploited in previous works for semi-supervised
learning and for learning mixtures of well-separated distributions (e.g.  [16 18 32 33]). These previ-
ous works essentially use variants of canonical correlation analysis [34] between the two views. This
work follows [22] in showing that having a third view of the data permits rather simple estimation
procedures with guaranteed parameter recovery.

2 The independent latent factors and LDA models
Let h = (h1  h2  . . .   hk) ∈ Rk be a random vector specifying the latent factors (i.e.  the hidden
state) of a model  where hi is the value of the i-th factor. Consider a sequence of exchangeable ran-
dom vectors x1  x2  x3  x4  . . . ∈ Rd  which we take to be the observed variables. Assume through-
out that d ≥ k; that x1  x2  x3  x4  . . . ∈ Rd are conditionally independent given h. Furthermore 
assume there exists a matrix O ∈ Rd×k such that
for each v ∈ {1  2  3  . . .}. Throughout  we assume the following condition.
Condition 2.1. O has full column rank.

E[xv|h] = Oh

This is a mild assumption  which allows for identiﬁability of the columns of O. The goal is to
estimate the matrix O  sometimes referred to as the topic matrix. Note that at this stage  we have not
made any assumptions on the noise model; it need not be additive nor even independent of h.

3

2.1

Independent latent factors model

In the independent latent factors model  we assume h has a product distribution  i.e.  h1  h2  . . .   hk
are independent. Two important examples of this setting are as follows.
Multiple mixtures of Gaussians: Suppose xv = Oh + η  where η is Gaussian noise and h is a
binary vector (under a product distribution). Here  the i-th column Oi can be considered to be the
mean of the i-th Gaussian component. This generalizes the classic mixture of k Gaussians  as the
model now permits any number of Gaussians to be responsible for generating the hidden state (i.e. 
h is permitted to be any of the 2k vectors on the hypercube  while in the classic mixture problem 
only one component is responsible). We may also allow η to be heteroskedastic (i.e.  the noise may
depend on h  provided the linearity assumption E[xv|h] = Oh holds).
Multiple mixtures of Poissons: Suppose [Oh]j speciﬁes the Poisson rate of counts for [xv]j. For
example  xv could be a vector of word counts in the v-th sentence of a document. Here  O would be
a matrix with positive entries  and hi would scale the rate at which topic i generates words in a sen-
tence (as speciﬁed by the i-th column of O). The linearity assumption is satisﬁed as E[xv|h] = Oh
(note the noise is not additive in this case). Here  multiple topics may be responsible for generating
the words in each sentence. This model provides a natural variant of LDA  where the distribution
over h is a product distribution (while in LDA  h is a probability vector).

2.2 The Dirichlet model

Now suppose the hidden state h is a distribution itself  with a density speciﬁed by the Dirichlet
distribution with parameter α ∈ Rk
>0 (α is a strictly positive real vector). We often think of h as a
distribution over topics. Precisely  the density of h ∈ ∆k−1 (where the probability simplex ∆k−1
denotes the set of possible distributions over k outcomes) is speciﬁed by:

k(cid:89)

pα(h) :=

1

Z(α)

i=1

hαi−1

i

(cid:81)k

i=1 Γ(αi)
Γ(α0)

and α0 := α1 + α2 + ··· + αk. Intuitively  α0 (the sum of the “pseudo-
where Z(α) :=
counts”) characterizes the concentration of the distribution. As α0 → 0  the distribution degenerates
to one over pure topics (i.e.  the limiting density is one in which  almost surely  exactly one coordi-
nate of h is 1  and the rest are 0).
Latent Dirichlet Allocation: LDA makes the further assumption that each random variable
x1  x2  x3  . . . takes on discrete values out of d outcomes (e.g.  xv represents what the v-th word
in a document is  so d represents the number of words in the language). The i-th column Oi of
O is a probability vector representing the distribution over words for the i-th topic. The sampling
process for a document is as follows. First  the topic mixture h is drawn from the Dirichlet dis-
tribution. Then  the v-th word in the document (for v = 1  2  . . . ) is generated by: (i) drawing
t ∈ [k] := {1  2  . . . k} according to the discrete distribution speciﬁed by h  then (ii) drawing xv
according to the discrete distribution speciﬁed by Ot (the t-th column of O). Note that xv is in-
dependent of h given t. For this model to ﬁt in our setting  we use the “one-hot” encoding for xv
from [22]: xv ∈ {0  1}d with [xv]j = 1 iff the v-th word in the document is the j-th word in the
vocabulary. Observe that

E[xv|h] =

Pr[t = i|h] · E[xv|t = i  h] =

hi · Oi = Oh

as required. Again  note that the noise model is not additive.

i=1

i=1

3 Excess Correlation Analysis (ECA)

We now present efﬁcient algorithms for exactly recovering O from low-order moments of the ob-
served variables. The algorithm is based on two singular value decompositions:
the ﬁrst SVD
whitens the data (based on the correlation between two variables)  and the second SVD is carried

4

k(cid:88)

k(cid:88)

Algorithm 1 ECA  with skewed factors

Input: vector θ ∈ Rk; the moments Pairs and Triples.

1. Dimensionality reduction: Find a matrix U ∈ Rd×k such that

range(U ) = range(Pairs).

(See Remark 1 for a fast procedure.)

2. Whiten: Find V ∈ Rk×k so V (cid:62)(U (cid:62) Pairs U )V is the k × k identity matrix. Set:

W = U V.
3. SVD: Let Ξ be the set of left singular vectors of

W (cid:62) Triples(W θ)W

corresponding to non-repeated singular values (i.e.  singluar values with multiplicity
one).

4. Reconstruct: Return the set (cid:98)O := {(W +)(cid:62)ξ : ξ ∈ Ξ}.

out on higher-order moments. We start with the case of independent factors  as these algorithms
make the basic diagonalization approach clear.
Throughout  we use A+ to denote the Moore-Penrose pseudo-inverse.

3.1 Independent and skewed latent factors

Deﬁne the following moments:
µ := E[x1]  Pairs := E[(x1 − µ) ⊗ (x2 − µ)]  Triples := E[(x1 − µ) ⊗ (x2 − µ) ⊗ (x3 − µ)]
(here ⊗ denotes the tensor product  so µ ∈ Rd  Pairs ∈ Rd×d  and Triples ∈ Rd×d×d). It is
convenient to project Triples to matrices as follows:

Triples(η) := E[(x1 − µ)(x2 − µ)(cid:62)(cid:104)η  x3 − µ(cid:105)].

Roughly speaking  we can think of Triples(η) as a re-weighting of a cross covariance (by (cid:104)η  x3 −
µ(cid:105)).
Note that the matrix O is only identiﬁable up to permutation and scaling of columns. To see the
latter  observe the distribution of any xv is unaltered if  for any i ∈ [k]  we multiply the i-th column
of O by a scalar c (cid:54)= 0 and divide the variable hi by the same scalar c. Without further assumptions 
we can only hope to recover a certain canonical form of O  deﬁned as follows.
Deﬁnition 1 (Canonical form). We say O is in a canonical form (relative to h) if  for each i ∈ [k] 

i := E[(hi − E[hi])2] = 1.
σ2

The transformation O ← O diag(σ1  σ2  . . .   σk) (and a rescaling of h) places O in canonical form
relative to h  and the distribution over x1  x2  x3  . . . is unaltered. In canonical form  O is unique up
to a signed column permutation.
Let µi p := E[(hi − E[hi])p] denote the p-th central moment of hi  so the variance and skewness of
hi are given by σ2
i . The ﬁrst result considers the case when the skewness
is non-zero.
i > 0 for each i ∈ [k].
Theorem 3.1 (Independent and skewed factors). Assume Condition 2.1 and σ2
Under the independent latent factor model  the following hold.

i := µi 2 and γi := µi 3/σ3

• No False Positives: For all θ ∈ Rk  Algorithm 1 returns a subset of the columns of O  in

canonical form up to sign.

• Exact Recovery: Assume γi (cid:54)= 0 for each i ∈ [k]. If θ ∈ Rk is drawn uniformly at random
from the unit sphere S k−1  then with probability 1  Algorithm 1 returns all columns of O 
in canonical form up to sign.

5

The proof of this theorem relies on the following lemma.
Lemma 3.1 (Independent latent factors moments). Under the independent latent factor model 

Pairs =

Triples =

i Oi ⊗ Oi = O diag(σ2
σ2

1  σ2

2  . . .   σ2

k)O(cid:62) 

µi 3 Oi ⊗ Oi ⊗ Oi  Triples(η) = O diag(O(cid:62)η) diag(µ1 3  µ2 3  . . .   µk 3)O(cid:62).

k(cid:88)
k(cid:88)

i=1

i=1

Proof. The model assumption E[xv|h] = Oh implies µ = OE[h]. Therefore E[(xv − µ)|h] =
O(h − E[h]). Using the conditional independence of x1 and x2 given h  and the fact that h has a
product distribution 

Pairs = E[(x1 − µ) ⊗ (x2 − µ)] = E[E[(x1 − µ)|h] ⊗ E[(x2 − µ)|h]]

= OE[(h − E[h]) ⊗ (h − E[h])]O(cid:62) = O diag(σ2
An analogous argument gives the claims for Triples and Triples(η).

1  σ2

2  . . .   σ2

k)O(cid:62).

Proof of Theorem 3.1. Assume O is in canonical form with respect to h. By Condition 2.1 
U (cid:62) Pairs U ∈ Rk×k is full rank and hence positive deﬁnite. Thus the whitening step is pos-
sible  and M := W (cid:62)O is orthogonal. Observe that W (cid:62) Triples(W θ)W = M DM (cid:62)  where
D := diag(M (cid:62)θ) diag(γ1  γ2  . . .   γk). Since M is orthogonal  the above is an eigendecompo-
sition of W (cid:62) Triples(W θ)W   and hence the set of left singular vectors corresponding to non-
repeated singular values are uniquely deﬁned up to sign. Each such singular vector ξ is of the
form siM ei = siW (cid:62)Oei = siW (cid:62)Oi for some i ∈ [k] and si ∈ {±1}  so (W +)(cid:62)ξ =
siW (W (cid:62)W )−1W (cid:62)Oi = siOi (because range(W ) = range(U ) = range(O)).
If θ is drawn uniformly at random from S k−1  then so is M (cid:62)θ. In this case  almost surely  the
diagonal entries of D are unique (provided that each γi (cid:54)= 0)  and hence every singular value of
W (cid:62) Triples(W θ)W is non-repeated.
Remark 1 (Finding range(Pairs) efﬁciently). Let Θ ∈ Rd×k be a random matrix with entries sam-
pled independently from the standard normal distribution  and set U := Pairs Θ. Then with proba-
bility 1  range(U ) = range(Pairs).
i ) − 3 (cid:54)= 0 for each i ∈ [k] 
It is easy to extend Algorithm 1 to kurtotic sources where κi := (µi 4/σ4
simply by using fourth-order cumulants in places of Triples(η). The details are given in the full
version of the paper.

3.2 Latent Dirichlet Allocation

2

1

k

hα2−1

··· hαk−1

Now we turn to LDA where h has a Dirichlet density. Even though the distribution on h is propor-
tional to the product hα1−1
  the hi are not independent because h is constrained to
live in the simplex. These mild dependencies suggest using a certain correction of the moments with
ECA.
We assume α0 is known. Knowledge of α0 = α1 + α2 + ··· + αk is signiﬁcantly weaker than
having full knowledge of the entire parameter vector α = (α1  α2  . . .   αk). A common practice
is to specify the entire parameter vector α in a homogeneous manner  with each component being
identical (see [35]). Here  we need only specify the sum  which allows for arbitrary inhomogeneity
in the prior.
Denote the mean and a modiﬁed second moment by

2 ]ηµ(cid:62) + µη(cid:62)E[x1x(cid:62)

2 ] + (cid:104)η  µ(cid:105)E[x1x(cid:62)
2 ]
2α2
0

(cid:104)η  µ(cid:105)µµ(cid:62).

(α0 + 2)(α0 + 1)

+

6

µ = E[x1] 

Pairsα0 := E[x1x(cid:62)

2 ] − α0
α0 + 1

µµ(cid:62) 

and a modiﬁed third moment as

Triplesα0(η) := E[x1x(cid:62)

2 (cid:104)η  x3(cid:105)] − α0
α0 + 2

(cid:16)E[x1x(cid:62)

(cid:17)

1–3. Execute steps 1–3 of Algorithm 1 with Pairsα0 and Triplesα0 in place of Pairs and

Algorithm 2 ECA for Latent Dirichlet Allocation

Input: vector θ ∈ Rk; the modiﬁed moments Pairsα0 and Triplesα0.
(cid:27)

4. Reconstruct and normalize: Return the set

(cid:26) (W +)(cid:62)ξ

Triples.

(cid:98)O :=

: ξ ∈ Ξ

(cid:126)1(cid:62)(W +)(cid:62)ξ

where (cid:126)1 ∈ Rd is a vector of all ones.

Remark 2 (Central vs. non-central moments). In the limit as α0 → 0  the Dirichlet model degen-
erates so that  with probability 1  only one coordinate of h equals 1 and the rest are 0 (i.e.  each
document is about just one topic). In this case  the modiﬁed moments tend to the raw (cross) mo-
ments:

Pairsα0 = E[x1 ⊗ x2] 

lim
α0→0

Triplesα0 = E[x1 ⊗ x2 ⊗ x3].
(cid:88)

Pr[1st word = i  2nd word = j] ei⊗ej 

1≤i j≤d

Note that the one-hot encoding of words in xv implies that
E[x1⊗x2] =

Pr[x1 = ei  x2 = ej] ei⊗ej =

lim
α0→0

(cid:88)

1≤i j≤d

(and a similar expression holds for E[x1 ⊗ x2 ⊗ x3])  so these raw moments in the limit α0 → 0 are
precisely the joint probabilitiy tables of words across all documents.
At the other extreme α0 → ∞  the modiﬁed moments tend to the central moments:
α0→∞ Triplesα0 = E[(x1 − µ)⊗ (x2 − µ)⊗ (x3 − µ)]
α0→∞ Pairsα0 = E[(x1 − µ)⊗ (x2 − µ)] 
lim
(to see this  expand the central moment and use exchangeability: E[x1x(cid:62)
3 ] = E[x1x(cid:62)
3 ]).
Our main result here shows that ECA recovers both the topic matrix O  up to a permutation of the
columns (where each column represents a probability distribution over words for a given topic) and
the parameter vector α  using only knowledge of α0 (which  as discussed earlier  is a signiﬁcantly
less restrictive assumption than tuning the entire parameter vector).
Theorem 3.2 (Latent Dirichlet Allocation). Assume Condition 2.1 holds. Under the LDA model 
the following hold.

2 ] = E[x2x(cid:62)

lim

• No False Positives: For all θ ∈ Rk  Algorithm 2 returns a subset of the columns of O.
• Topic Recovery: If θ ∈ Rk is drawn uniformly at random from the unit sphere S k−1  then

with probability 1  Algorithm 2 returns all columns of O.

• Parameter Recovery:

1)O+ Pairsα0(O+)(cid:62)(cid:126)1  where (cid:126)1 ∈ Rk is a vector of all ones.

The Dirichlet parameter α satisﬁes α = α0(α0 +

The proof relies on the following lemma.
Lemma 3.2 (LDA moments). Under the LDA model 

Pairsα0 =

Triplesα0 (η) =

O diag(α)O(cid:62) 

1

(α0 + 1)α0
2

(α0 + 2)(α0 + 1)α0

O diag(O(cid:62)η) diag(α)O(cid:62).

The proof of Lemma 3.2 is similar to that of Lemma 3.1  except here we must use the speciﬁc
properties of the Dirichlet distribution to show that the corrections to the raw (cross) moments have
the desired effect.

Proof of Theorem 3.2. Note that with the rescaling ˜O :=
αk) 
we have that Pairsα0 = ˜O ˜O(cid:62). This is akin to ˜O being in canonical form as per the skewed factor

α2  . . .  

(α0+1)α0

α1 

1√

√
O diag(

√

√

7

model of Theorem 3.1. Now the proof of the ﬁrst two claims is the same as that of Theorem 3.1; the
only modiﬁcation is that we simply normalize the output of Algorithm 1. Finally  observe that claim
for estimating α holds due to the functional form of Pairsα0.
Remark 3 (Limiting behaviors). ECA seamlessly interpolates between the single topic model (α0 →
0) of [22] and the skewness-based ECA  Algorithm 1 (α0 → ∞).

4 Discussion

4.1 Sample complexity

It is straightforward to derive a “plug-in” variant of Algorithm 2 based on empirical moments rather
than exact population moments. The empirical moments are formed using the word co-occurrence
statistics for documents in a corpus. The following theorem shows that the empirical version of ECA
returns accurate estimates of the topics. The details and proof are left to the full version of the paper.
Theorem 4.1 (Sample complexity for LDA). There exist universal constants C1  C2 > 0 such that
the following hold. Let pmin = mini
and let σk(O) denote the smallest (non-zero) singular
value of O. Suppose that we obtain N ≥ C1 · ((α0 + 1)/(pminσk(O)2))2 independent samples
of x1  x2  x3 in the LDA model  which are used to form empirical moments (cid:91)Pairsα0 and (cid:92)Triplesα0.
With high probability  the plug-in variant of Algorithm 2 returns a set { ˆO1  ˆO2  . . . ˆOk} such that 
for some permutation σ of [k] 

αi
α0

(cid:107)Oi − ˆOσ(i)(cid:107)2 ≤ C2 ·

(α0 + 1)2k3
√
p2
minσk(O)3

N

 

∀i ∈ [k].

4.2 Alternative decomposition methods

position of the tensor Triples = (cid:80)k

Algorithm 1 is a theoretically efﬁcient and simple-to-state method for obtaining the desired decom-
i=1 µi 3Oi ⊗ Oi ⊗ Oi (a similar tensor form for Triplesα0 in
the case of LDA can also be given). However  in practice the method is not particularly stable  due
to the use of internal randomization to guarantee strict separation of singular values. It should be
noted that there are other methods in the literature for obtaining these decompositions  for instance 
methods based on simultaneous diagonalizations of matrices [36] as well as direct tensor decom-
position methods [37]; and that these methods can be signiﬁcantly more stable than Algorithm 1.
In particular  very recent work in [37] shows that the structure revealed in Lemmas 3.1 and 3.2 can
be exploited to derive very efﬁcient estimation algorithms for all the models considered here (and
others) based on a tensor power iteration. We have used a simpliﬁed version of this tensor power it-
eration in preliminary experiments for estimating topic models  and found the results (Appendix A)
to be very encouraging  especially due to the speed and robustness of the algorithm.

Acknowledgements

We thank Kamalika Chaudhuri  Adam Kalai  Percy Liang  Chris Meek  David Sontag  and Tong
Zhang for many invaluable insights. We also give warm thanks to Rong Ge for sharing preliminary
results (in [23]) and early insights into this problem with us. Part of this work was completed while
all authors were at Microsoft Research New England. AA is supported in part by the NSF Award
CCF-1219234  AFOSR Award FA9550-10-1-0310 and the ARO Award W911NF-12-1-0404.

References
[1] David M. Blei  Andrew Ng  and Michael Jordan. Latent dirichlet allocation. JMLR  3:993–1022  2003.
[2] R. A. Redner and H. F. Walker. Mixture densities  maximum likelihood and the EM algorithm. SIAM

Review  26(2):195–239  1984.

[3] A. Asuncion  P. Smyth  M. Welling  D. Newman  I. Porteous  and S. Triglia. Distributed gibbs sam-
pling for latent variable models. In Scaling Up Machine Learning: Parallel and Distributed Approaches.
Cambridge Univ Pr  2011.

[4] M.D. Hoffman  D.M. Blei  and F. Bach. Online learning for latent dirichlet allocation. In NIPS  2010.

8

[5] Thomas Hofmann. Probilistic latent semantic analysis. In UAI  1999.
[6] Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by nonnegative matrix factorization.

Nature  401  1999.

[7] K. Pearson. Contributions to the mathematical theory of evolution. Phil. Trans. of the Royal Society 

London  A.  1894.

[8] A. Anandkumar  D. P. Foster  D. Hsu  S. M. Kakade  and Y.-K. Liu. Two svds sufﬁce: spectral decompo-

sitions for probabilistic topic models and latent dirichlet allocation  2012. arXiv:1204.6703.

[9] Christos H. Papadimitriou  Prabhakar Raghavan  Hisao Tamaki  and Santosh Vempala. Latent semantic

indexing: A probabilistic analysis. J. Comput. Syst. Sci.  61(2)  2000.

[10] S. Dasgupta. Learning mixutres of Gaussians. In FOCS  1999.
[11] S. Dasgupta and L. Schulman. A two-round variant of em for gaussian mixtures. In UAI  2000.
[12] S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC  2001.
[13] S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In FOCS  2002.
[14] R. Kannan  H. Salmasian  and S. Vempala. The spectral method for general mixture models. In COLT 

2005.

[15] D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In COLT  2005.
[16] K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and independence.

In COLT  2008.

[17] S. C. Brubaker and S. Vempala. Isotropic PCA and afﬁne-invariant clustering. In FOCS  2008.
[18] K. Chaudhuri  S. M. Kakade  K. Livescu  and K. Sridharan. Multi-view clustering via canonical correla-

tion analysis. In ICML  2009.

[19] A. T. Kalai  A. Moitra  and G. Valiant. Efﬁciently learning mixtures of two Gaussians. In STOC  2010.
[20] M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS  2010.
[21] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS  2010.
[22] A. Anandkumar  D. Hsu  and S. M. Kakade. A method of moments for mixture models and hidden

markov models. In COLT  2012.

[23] S. Arora    R. Ge  and A. Moitra. Learning topic models — going beyond svd. In FOCS  2012.
[24] J. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identiﬁability and consistency.

Mathematical Biosciences  137:51–73  1996.

[25] E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. Annals of Applied

Probability  16(2):583–614  2006.

[26] D. Hsu  S. M. Kakade  and T. Zhang. A spectral algorithm for learning hidden Markov models. In COLT 

2009.

[27] Jean-Franois Cardoso and Pierre Comon. Independent component analysis  a survey of some algebraic

methods. In IEEE International Symposium on Circuits and Systems  pages 93–96  1996.

[28] P. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component Analysis and

Applications. Academic Press. Elsevier  2010.

[29] Alan M. Frieze  Mark Jerrum  and Ravi Kannan. Learning linear transformations. In FOCS  1996.
[30] P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signatures.

Journal of Cryptology  22(2):139–160  2009.

[31] S. Arora  R. Ge  A. Moitra  and S. Sachdeva. Provable ICA with unknown Gaussian noise  and implica-

tions for Gaussian mixtures and autoencoders. In NIPS  2012.

[32] R. Ando and T. Zhang. Two-view feature generation model for semi-supervised learning. In ICML  2007.
[33] Sham M. Kakade and Dean P. Foster. Multi-view regression via canonical correlation analysis. In COLT 

2007.

[34] H. Hotelling. The most predictable criterion. Journal of Educational Psychology  26(2):139–142  1935.
[35] Mark Steyvers and Tom Grifﬁths. Probabilistic topic models. In T. Landauer  D. Mcnamara  S. Dennis 

and W. Kintsch  editors  Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum  2006.

[36] A. Bunse-Gerstner  R. Byers  and V. Mehrmann. Numerical methods for simultaneous diagonalization.

SIAM Journal on Matrix Analysis and Applications  14(4):927–949  1993.

[37] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and T. Telgarsky. Tensor decompositions for learning

latent variable models  2012. arXiv:1210.7559.

9

,Christopher Meek
Marina Meila