2018,Contextual bandits with surrogate losses: Margin bounds and efficient algorithms,We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss  we derive a new margin-based regret bound in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss  we derive an efficient algorithm with a $\sqrt{dT}$-type mistake bound against benchmark policies induced by $d$-dimensional regressors. Under realizability assumptions  our results also yield classical regret bounds.,Contextual bandits with surrogate losses:
Margin bounds and efﬁcient algorithms

Dylan J. Foster
Cornell University

djfoster@cs.cornell.edu

Akshay Krishnamurthy
Microsoft Research  NYC
akshay@cs.umass.edu

Abstract

We use surrogate losses to obtain several new regret bounds and new algorithms for
contextual bandit learning. Using the ramp loss  we derive new margin-based regret
bounds in terms of standard sequential complexity measures of a benchmark class
of real-valued regression functions. Using the hinge loss  we derive an efﬁcient

algorithm with a√dT -type mistake bound against benchmark policies induced by

d-dimensional regressors. Under realizability assumptions  our results also yield
classical regret bounds.

1

Introduction

We study sequential prediction problems with partial feedback  mathematically modeled as contextual
bandits [29]. In this formalism  a learner repeatedly (a) observes a context  (b) selects an action  and
(c) receives a loss for the chosen action. The objective is to learn a policy for selecting actions with
low loss  formally measured via regret with respect to a class of benchmark policies. Contextual
bandit algorithms have been successfully deployed in online recommendation systems [5]  mobile
health platforms [48]  and elsewhere.
In this paper  we use surrogate loss functions to derive new margin-based algorithms and regret bounds
for contextual bandits. Surrogate loss functions are ubiquitous in supervised learning (cf. [49  8  43]).
Computationally  they are used to replace NP-hard optimization problems with tractable ones  e.g. 
the hinge loss makes binary classiﬁcation amenable to convex programming techniques. Statistically 
they also enable sharper generalization analysis for models including boosting  SVMs  and neural
networks [43  6]  by replacing dependence on dimension in VC-type bounds with distribution-
dependent quantities. For example  to agnostically learn d-dimensional halfspaces the optimal

 ⋅￿1￿n for the -margin loss

rates for excess risk are￿d￿n for the 0￿1 loss benchmark and 1

benchmark [27]  meaning the margin bound removes explicit dependence on dimension. Curiously 
surrogate losses have seen limited use in partial information settings (some exceptions are discussed
below). This paper demonstrates that these desirable computational and statistical properties indeed
extend to contextual bandits.
In the ﬁrst part of the paper we focus on statistical issues  namely whether any algorithm can achieve a
generalization of the classical margin bound from statistical learning [10] in the adversarial contextual
bandit setting. Our aim here is to introduce a theory of learnability for contextual bandits  in analogy
with statistical and online learning  and our results provide an information-theoretic benchmark
for future algorithm designers. We consider benchmark policies induced by a class of real-valued
regression functions and obtain a regret bound in terms of the class’ sequential metric entropy  a

regret is achievable for Lipschitz contextual bandits in d-dimensional metric spaces  improving on a

standard complexity measure in online learning [42]. As a consequence  we show that ˜O(T d
d+1)
recent result of Cesa-Bianchi et al. [14]  and that an ˜O(T 2￿3) mistake bound is achievable for bandit

multiclass prediction in smooth Banach spaces  extending Kakade et al. [26].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Technically  these results build on the non-constructive minimax analysis of Rakhlin et al. [42]  which 
for the online adversarial setting  prescribes a recipe for characterizing statistical behavior of arbitrary
classes  and thus provides a counterpart to empirical risk minimization in statistical learning. Indeed 
for full-information problems  this approach yields regret bounds in terms of sequential analogues
of standard complexity measures including Rademacher complexity and metric entropy. However 
since we work in the contextual bandit setting  we must extend these arguments to incorporate partial
information. To do so  we leverage the adaptive minimax framework of Foster et al. [20] along with a
careful “adaptive" chaining argument.
In the second part of the paper  we focus on computational issues and derive two new algorithms using
the hinge loss as a convex surrogate. The ﬁrst algorithm  HINGE-LMC  provably runs in polynomial

time and achieves a√dT -mistake bound against d-dimensional benchmark regressors with convexity
properties. HINGE-LMC is the ﬁrst efﬁcient algorithm with√dT -mistake bound for bandit multiclass

prediction using a surrogate loss without curvature  and so it provides a new resolution to the open
problem of Abernethy and Rakhlin [2]. This algorithm is based on the exponential weights update 
along with Langevin Monte Carlo for efﬁcient sampling and a careful action selection scheme. The
second algorithm is much simpler: in the stochastic setting  Follow-The-Leader with appropriate
smoothing matches our information-theoretic results for sufﬁciently large classes.

1.1 Preliminaries

In the adversarial

Regret(T  ⇧)￿ T￿t=1

E[`t(at)]− inf
⇡∈⇧

T￿t=1

E[`t(⇡(xt))].

minimize the cumulative loss over the T rounds  and  in particular  we would like to design learning

Our algorithms use importance weighting to form unbiased loss estimates. If at round t  the algorithm

natural generalization of the standard formulation for binary classiﬁcation [8] and appears in Pires

LetX denote a context space andA = {1  . . .   K} a discrete action space.
contextual bandits problem  for each of T rounds  an adversary chooses a pair(xt ` t) where xt∈X
is the context and `t∈[0  1]K ￿L is a loss vector. The learner observes the context xt  chooses
an action at  and incurs loss `t(at)∈[0  1]  which is also observed. The goal of the learner is to
algorithms that achieve low regret against a class ⇧⊂(X→A) of benchmark policies:
In this paper  we always identify ⇧ with a class of vector-valued regression functionsF⊂(X→ RK=0) 
where RK=0￿{s∈ RK ∶∑a sa= 0}. We use the notation f(x)∈ RK to denote the vector-valued
output and f(x)a to denote the ath component. Note that we are assuming∑a f(x)a= 0  which is a
et al. [34]. Deﬁne B￿ supf∈F supx∈X￿f(x)￿∞ to be the maximum value predicted by any regressor.
chooses action at by sampling from a distribution pt∈ (A)  the loss estimate is deﬁned as ˆ`t(a)￿
`t(at)1{at= a}￿pt(a). Given pt  we also deﬁne a smoothed distribution as pµ
t ￿(1− Kµ)pt+ µ for
some parameter µ∈[0  1￿K].
are deﬁned as (s)￿ min(max(1+ s￿  0)  1) and (s)￿ max(1+ s￿  0) respectively  for
> 0. For s∈ RK  (s) and (s) are deﬁned coordinate-wise. We start with a simple lemma 
Lemma 1 (Surrogate Loss Translation). For s ∈ RK=0  deﬁne ⇡ramp(s) ⇡ hinge(s) ∈ (A) by
⇡ramp(s)a∝ (sa) and ⇡hinge(s)a∝ (sa). For any vector `∈ RK+   we have
￿⇡ramp(s) `￿≤￿`  (s)￿≤ ￿a∈A
￿⇡hinge(s) `￿≤ K−1￿`  (s)￿.
t=1∑a∈A `t(a)1{f(xt)a≥−} 
loss” here because these quantities upper bound the cost-sensitive loss: `(argmaxa sa)≤￿`  (s)￿≤
￿`  (s)￿.1 In the sequel  ⇡ramp and ⇡hinge are used by our algorithms  but do not deﬁne the benchmark
function ✓(s)a∶= max{1+(sa− maxa′ sa′)￿  0}  which also satisﬁes `(argmaxa sa)≤￿`  ✓(s)￿. This

Based on this lemma  it will be convenient to deﬁne L
which is the margin-based cumulative loss for the regressor f. L
T should be seen as a cost-sensitive
multiclass analogue of the classical margin loss in statistical learning [10]. We use the term “surrogate

We introduce two surrogate loss functions  the ramp loss and the hinge loss  whose scalar versions

1On a related note  the information-theoretic results we present are also compatible with the surrogate

leads to a perhaps more standard notion of multiclass margin bound but does not lead to efﬁcient algorithms.

`(a)1{sa≥−} 

and

T(f)￿∑T

demonstrating how    act as surrogates for cost-sensitive multiclass losses.

policy class  since we compare directly to L

T or the surrogate loss.

2

Related work. Contextual bandit learning has been the subject of intense investigation over the past
decade. The most natural categorization of these works is between parametric  realizability-based 
and agnostic approaches. Parametric methods (e.g.  [1  16]) assume a (generalized) linear relationship
between the losses and the contexts/actions. Realizability-based methods generalize parametric ones
by assuming the losses are predictable by some abstract regression class [3  21]. Agnostic approaches
(e.g.  [7  29  4  38  46  47]) avoid realizability assumptions and instead compete with VC-type policy
classes for statistical tractability. Our work contributes to all of these directions  as our margin bounds
apply to the agnostic adversarial setting and yield true regret bounds under realizability assumptions.
A special case of contextual bandits is bandit multiclass prediction  where the loss vector is zero
for one action and one for all others [26]. Several recent papers obtain surrogate regret bounds

functions [26  24  9  22]. Our work contributes to this line in two ways: our bounds and algorithms
extend beyond linear/parametric classes  and we consider the more general contextual bandit setting.
Our information-theoretic results on achievability are similar in spirit those of Daniely and Halbertal
[18]  who derive tight generic bounds for bandit multiclass prediction in terms of the Littlestone

and efﬁcient algorithms for this setting when the benchmark regressor classF consists of linear
dimension. This result is incomparable to our own: their bounds are on the 0￿1 loss regret directly

rather than surrogate regret  but the Littlestone dimension is not a tight complexity measure for
real-valued function classes in agnostic settings  which is our focus.
At a technical level  our work builds on several recent results. To derive achievable regret bounds 
we use the adaptive minimax framework of Foster et al. [20]  along with a new adaptive chaining
argument to control the supremum of a martingale process [42]. Our HINGE-LMC algorithm is based
on log-concave sampling [12]  and it uses randomized smoothing [19] and the geometric resampling
trick of Neu and Bartók [33]. We also use several ideas from classiﬁcation calibration [49  8]  and  in
particular  the surrogate hinge loss we work with is studied by Pires et al. [34].

2 Achievable regret bounds

This section provides generic surrogate regret bounds for contextual bandits in terms of the sequential

metric entropy [41] of the regressor classF. Notably  our general techniques apply when the ramp

loss is used as a surrogate  and so  via Lemma 1  they yield the main result of the section—-a
margin-based regret guarantee—as a special case.
To motivate our approach  consider a well-known reduction from bandits to full information online
learning: If a full information algorithm achieves a regret bound in terms of the so-called local

yields an expected regret bound for the bandit setting. For example  when ⇧ is ﬁnite  EXP4 [7] uses
HEDGE [23] as the full information algorithm  and obtains a deterministic regret bound of

t￿  then running the full information algorithm on importance-weighted losses ˆ`t(a)

norms∑t￿pt ` 2

Regret(T  ⇧)≤ ⌘

2

E⇡∼pt￿⇡(xt)  ˆ`t￿2+ log(￿⇧￿)

⌘

T￿t=1

 

(1)

To use this reduction beyond the ﬁnite class case and with surrogate losses we face two challenges:

distribution) for round t. Evaluating conditional expectations and optimizing ⌘ yields a regret bound

where ⌘> 0 is the learning rate and pt is the distribution over policies in ⇧ (inducing an action
ofO(￿KT log(￿⇧￿))  which is optimal for contextual bandits with a ﬁnite policy class.
1. Inﬁnite classes. The natural approach of using a pointwise (or sup-norm) cover for F is
covering number forF  which is the correct generalization of the empirical covering number in
2. Variance control. With surrogate losses  controlling the variance/local norm term E⇡￿⇡(xt)  ˆ`t￿2

insufﬁcient—not only because there are classes that have inﬁnite pointwise covers yet are online-
learnable  but also because it yields sub-optimal rates even when a ﬁnite pointwise cover is
available. Instead  we establish existence of a full-information algorithm for large nonparametric
classes that has 1) strong adaptivity to loss scaling as in (1) and 2) regret scaling with the sequential

in the reduction from bandit to full information is more challenging  since the surrogate loss of a
policy depends on the scale of the underlying regressor  not just the action it selects. To address
this  we develop a new sampling scheme tailored to scale-sensitive losses.

statistical learning to the adversarial online setting. This is achieved via non-constructive methods.

3

T￿t=1

T￿t=1￿g(xt) ` t￿.

the smallest set V of RK-valued trees for which

Est∼pt￿st ` t￿− inf
g∈G

Full-information regret bound. We consider the following full information protocol  which in the

As our complexity measure  we use a multi-output generalization of sequential covering numbers

sequel will be instantiated via reduction from contextual bandits. Let the context spaceX andA be
ﬁxed as in Subsection 1.1  and consider a function classG⊂(X→S)  whereS⊆ RK+ . The reader
may think ofG as representing ○F or ○F  i.e. the surrogate loss composed with the regressor
class  so thatS (which is not necessarily convex) represents the image of the surrogate loss overF.
The online learning protocol is: For time t= 1  . . .   T   (1) the learner observes xt and chooses a
distribution pt∈ (S)  (2) the adversary picks a loss vector `t∈L⊂ RK+   (3) the learner samples
outcome st∼ pt and experiences loss￿st ` t￿. Regret against the benchmark classG is given by
introduced by Rakhlin et al. [41]. Deﬁne aZ-valued tree z to be a sequence of mappings zt ∶
{±1}t−1→Z. The tree z is a complete rooted binary tree with nodes labeled by elements ofZ 
where for any “path” ✏∈{±1}T   zt(✏)￿ zt(✏1∶t−1) is the value of the node at level t on the path ✏.
Deﬁnition 1. For a function classG⊂(X → RK) andX -valued tree x of length T   the L∞￿`∞
sequential covering number2 forG on x at scale "  denoted byN∞ ∞(" G  x)  is the cardinality of
∀g∈G∀✏∈{±1}T∃v∈ V s.t. max
DeﬁneN∞ ∞(" G  T)￿ supx∶length(x)=TN∞ ∞(" G  x).
We refer to logN∞ ∞ as the sequential metric entropy. Note that in the binary case  for learning unit
`2 norm linear functions in d dimensions  the pointwise metric entropy is O(d log(1￿"))  whereas the
sequential metric entropy is O(d log(1￿")∧ "−2 log(d))  leading to improved rates in high dimension.
Theorem 2. Assume3 sup`∈L￿`￿1≤ R and sups∈S￿s￿∞≤ B. Fix any constants ⌘∈(0  1]  > 0 
and > ↵> 0. Then there exists an algorithm with the following deterministic regret guarantee:
logN∞ ∞(￿2 G  T)+ 3e2↵
T￿t=1￿`t￿1
Est∼pt￿s  `t￿− inf
Est∼pt￿st ` t￿2+ 4RB
T￿t=1
g∈G
↵ ￿logN∞ ∞(" G  T)d".
￿￿ 
T￿t=1￿`t￿2
1+ R
Observe that the bound involves the variance/local norms Est∼pt￿st ` t￿2  and has a very mild explicit

dependence on the loss range R; this can be veriﬁed by optimizing over ⌘ and . This adaptivity to the
loss range is crucial for our bandit reduction. Further observe that the bound contains a Dudley-type
entropy integral  which is essential for obtaining sharp rates for complex nonparametric classes.
Bandit reduction and variance control. To lift Theorem 2 to contextual bandits we use the

t∈[T]￿g(xt(✏))− vt(✏)￿∞≤ ".

With this deﬁnition  we can now state our main theorem for full information.

T￿t=1￿g(xt) ` t￿≤ 2⌘

The following lemma shows that this strategy leads to sufﬁciently small variance in the loss estimates.
The deﬁnition of the action distribution P µ

following reduction: First  initialize the full information algorithm from Theorem 2 withG= ○F.
st(a)
For each round t  receive xt  and deﬁne Pt(a)￿ Est∼pt
∑a′∈[K] st(a′) where pt is the full information
algorithm’s distribution. Then sample at∼ P µ
t   observe `t(at)  and pass the importance-weighted
loss ˆ`t(a) back to the algorithm. For the hinge loss we use the same strategy  but withG= ○F.
t(a) in terms of the real-valued predictions is crucial here.
Lemma 3. Deﬁne a ﬁltrationJt= ((x1 ` 1  a1)  . . .  (xt−1 ` t−1  at−1)  xt ` t). Then for any µ∈
[0  1￿K] the importance weighting strategy above guarantees
￿1+ B
￿2
2Sequential coverings for Lp￿`q can be deﬁned similarly  but do not appear in the present paper.
weighted losses  and it enables us to cover the output space in `∞ norm.

t￿Est∼pt￿st  ˆ`t￿2￿Jt￿≤￿￿￿￿￿￿￿￿￿￿￿

3Measuring loss in `1 may seem restrictive  but it is natural when working with the 1-sparse importance-

forS⊂ (A).
forS= ○F.
forS= ○F.

Eat∼P µ

K 
K2 

K2 

RB

T￿t=1
+ 24e￿ 

4R

(2)

⌘

4

Figure 1: Regret bound exponent as a function of
(sequential) metric entropy. The cross marks the

point p= 2 where the exponent from Theorem 4
changes growth rate. “Full information” refers to
p ) for the same setting
the optimal rate of T
under full information feedback [41]. “Square loss”
p+1
p+2 for Lipschitz
refers to the optimal rate of T
contextual bandits over metric spaces of dimension
p  which have sequential metric entropy "−p  under

square loss realizability [44].

2∨( p−1

1

Theorem 2 and Lemma 3 together imply our central theorem: a chaining-based margin bound for
contextual bandits  generalizing classical results in statistical learning (cf. [10]).

p+1

5

`t(at)￿≤ inf

parametric case.

(3)

(4)

in terms of the growth rate for the sequential metric entropy.

`t(at)￿≤ inf

f∈F E[L
+ 8

µ

strategy with expected regret against the -margin benchmark bounded as

there exists a contextual bandit strategy with the following regret guarantee:

We derive an analogous bound for the hinge loss in Appendix C. The hinge loss bound differs only
through stronger dependence on scale parameters.

Theorem 4 (Contextual bandit margin bound). For any ﬁxed constants  > ↵ > 0  smoothing
parameter µ∈(0  1) and margin loss parameter > 0 there exists an adversarial contextual bandit
T(f)]+ 4￿2K 2T logN∞ ∞(￿2 F  T)+ µKT
E￿ T￿t=1
￿￿3e2↵KT+ 24e￿ KT
↵ ￿logN∞ ∞(" F  T)d"￿￿.
logN∞ ∞(￿2 F  T)+ 1
µ ￿ 
Before showing the implications of Theorem 4 for speciﬁc classesF we state a coarse upper bound
Proposition 5. Suppose thatF has sequential metric entropy growth logN∞ ∞(" F  T)∝ "−p for
some p> 0 (nonparametric case)  or that logN∞ ∞(" F  T)∝ d log(1￿") (parametric case). Then
E￿ T￿t=1
Proposition 5 recovers the parametric rate of√dT seen with e.g.  LINUCB [16] but is most interesting
p∈(0  2] and the “high complexity” regime of p≥ 2. This is visualized in Figure 1.

O(K￿dT log(KT￿)) 
p+4) 
˜O((KT) p+2
p+4 − 2p
˜O((KT) p
p+1) 
p+1 − p

T(f)￿+￿￿￿￿￿￿￿￿￿￿￿
f∈F E￿L

Remark 1. Under i.i.d. losses and hinge/ramp loss realizability  the standard tools of classiﬁcation
calibration [8] can be used to deduce a proper policy regret bound from (3). However  these
realizability assumptions are somewhat non-standard  and moreover if one imposes the stronger
assumption of a hard margin it is possible to derive improved rates [18]. See also Appendix B.
Remark 2. Classical margin bounds typically hold for all values of  simultaneously  but Theorem 4
requires that  is chosen in advance. Learning the best value of  online appears challenging.
Rates for speciﬁc classes. We now instantiate our results for concrete classes of interest.

Example 1 (Finite classes). In the ﬁnite class case there is an algorithm with O￿K￿T log￿F￿￿
margin regret. When ⇧⊂(X→A) is a ﬁnite policy class  directly reducing to Theorem 2 yields the
optimal O￿￿KT log￿⇧￿￿ policy regret  hinting at the optimality of our approach.
Example 2 (Lipschitz CB). The class of Lipschitz functions over[0  1]p admits a sequential cover
with metric entropy ˜O("−p)  so Proposition 5 implies an ˜O(T
p+1) regret bound. Since our proof
p+2
p+4∨ p
goes through Lemma 1  it also yields a policy regret bound against the ⇡ramp(⋅) class. Therefore  this
result is directly comparable to the ˜O(T
p+2) bound of Cesa-Bianchi et al. [14]  applied to the ⇡ramp

policy class. Our bound achieves a smaller exponent for all values of p (see Figure 1).

nonparametric w/ p≤ 2.
nonparametric w/ p≥ 2.

for complex classes. The rate exhibits a phase change between the “moderate complexity” regime of

R(G  T)￿ sup

x

E✏ sup

g∈G

T￿t=1

✏tg(xt(✏)).

similar discussion applies to Theorem 4.4 of Lykouris et al. [31].

bound is fairly user-friendly  it yields worse rates than Proposition 5 when translated to sequential

margin regret in full information for binary classiﬁcation. For partial information  BISTRO [38]

Thus  for margin-based contextual bandits  full information learnability is equivalent to bandit

antee of BANDITRON [26] from Euclidean geometry to arbitrary uniformly convex Banach spaces 
essentially the largest linear class for which online learning is possible [45]. The result also general-
izes BANDITRON from multiclass to general contextual bandits and strengthens it from hinge loss to

complexity  which is deﬁned for any scalar-valued function classG⊆(X→ R) as:
Example 3. LetF￿a￿{x￿ f(x)a￿ f∈F} be the scalar restriction ofF to output coordinate a
and suppose that maxa∈[K] R(F￿a  T)≥ 1 and B≤ 1.4 Then there exists an adversarial contextual
bandit algorithm with margin regret bound ˜O￿maxa K(R(F￿a  T)￿)2￿3T 1￿3￿.
learnability. Since the optimal regret in full information is ⌦(maxa R(F￿a  T))  it further shows
that the price of bandit information is at most ˜O￿maxa K(T￿R(F￿a  T))1￿3￿. Note that while this
metric entropy  except when p= 2 [40]. For comparison  Rakhlin et al. [41] obtain ˜O(R(F  T)￿)
has an O(￿KT R(⇧  T)) policy regret bound  which involves the policy complexity and a worse
T dependence than our bound  but our bound (in terms ofF) applies only to the margin regret. A
Instantiating Example 3 with linear classes generalizes the O(T 2￿3) dimension-independent guar-
ramp loss. Note that many subsequent works [2  9  22] obtain dimension-dependent O(√dT) bounds
independent O(T 2￿3)-type rates  which are more appropriate for high-dimensional settings.
Example 4. LetX be the unit ball in a Banach space(B ￿⋅￿)  and letF be induced by stacking K−1
linear predictors5 each in the unit ball of the dual space(B￿ ￿⋅￿￿). Suppose that￿⋅￿ has martingale
type 2 [35]  which means there exists ∶ B→ R such that 1
2￿x￿2≤ (x) and is -smooth w.r.t.
￿⋅￿.6 Then there exists a contextual bandit strategy with margin regret O(K(T￿)2￿3).
Beyond linear classes  we also obtain ˜O(K(T￿)2￿3) margin regret when eachFa is a class of neural
networks with weights in each layer bounded in the(1 ∞) group norm  or when eachFa is a class of
As our last example  we consider `p spaces for p< 2. These spaces fail to satisfy martingale type
dependence  and so have sequential metric entropy of order "− p
admit a pointwise cover with metric entropy O(d log(1￿"))  leading to the following dichotomy.
Example 5. Consider the setting of Example 4  with(B ￿⋅￿)=(Rd ￿⋅￿p) for p≤ 2. Then there
2p−1∧ K￿dT log(KT￿)).
exists a contextual bandit strategy with margin regret ˜O(K(T￿) p

2 in a dimension-independent fashion  but they do satisfy martingale type p without dimension
p−1 [39]. Moreover  in Rd the `p spaces

bounded depth decision trees on ﬁnitely many decision functions. These results follow by appealing
to the existing sequential Rademacher complexity bounds derived in Rakhlin et al. [41].

for bandit multiclass prediction  as we will in the next section  but  none have explored dimension-

Learnability in full information online learning is known to be characterized entirely by the sequential
Rademacher complexity of the hypothesis class [41]  and tight bounds on this quantity are known for
standard classes including linear predictors  decision trees  and neural networks. The next example  a
corollary of Theorem 4  bounds contextual bandit margin regret in terms of sequential Rademacher

3 Efﬁcient Algorithms

We derive two new algorithms for contextual bandits using the hinge loss . The ﬁrst algorithm 
HINGE-LMC  focuses on the parametric setting; it is based on a continuous version of exponential

4This restriction serves only to simplify calculations and can be relaxed.

6Norms that satisfy this property with dimension-independent or logarithmic constants include `p for all

5Only K− 1 predictors are needed due to the sum-to-zero constraint of RK=0.
p≥ 2  Schatten Sp norms for p≥ 2 (including the spectral norm)  and(2  p) group norms for p≥ 2 [27  28].

6

Set pµ

Algorithm 1 HINGE-LMC
Input: Class ⇥  learning rate ⌘  margin parameter .

// Geometric resampling.

Deﬁne w0(✓)￿ 1 for all ✓∈ ⇥.
for t= 1  . . .   T do
Receive xt  set ✓t← LMC(⌘wt−1).
Set pt(⋅; ✓t)∝ (f(xt; ✓t)).
t(⋅; ✓t)￿(1− Kµ)pt+ µ.
Play at∼ pµ
t(⋅; ✓t)  observe `t(at).
for m= 1  . . .   M do
˜✓t← LMC(⌘wt−1).
Sample ˜at∼ pµ
t(⋅; ˜✓t)  if ˜at= at  break.
Set mt= m  and ˜`t(a)￿ `t(at)⋅ mt1{at= a}.
Update wt(✓)← wt−1(✓)+￿˜`t  (f(xt; ✓))￿.

end for

end for

Algorithm 2 Langevin Monte Carlo (LMC)

Draw z1  . . .   zm

// Parameter choices are in Appendix D.

Input: F(⋅)  parameters m  u    N  ↵.
Set ˜✓0← 0∈ Rd
for k= 1  . . .   N do
iid∼ N(0  u2Id). Deﬁne
˜Fk(✓)￿ 1
i=1 F(✓+ zi)+ 
2￿✓￿2
m∑m
Draw ⇠k∼N(0  Id) and update
2∇ ˜Fk(˜✓k−1)+√↵⇠k￿ .
˜✓k←P⇥￿˜✓k−1− ↵

2

end for
Return ˜✓N.

weights using a log-concave sampler. The second  SMOOTHFTL  is simply Follow-The-Leader with
uniform smoothing. SMOOTHFTL applies to the stochastic setting with classes that have “high
complexity” in the sense of Proposition 5.

3.1 Hinge-LMC

contains the centered Euclidean ball of radius 1 and is contained within a Euclidean ball of radius R.

For this section  we identifyF with a compact convex set ⇥⊂ Rd  using the notation f(x; ✓)∈ RK=0
to describe the parametrized function. We assume that (f(x; ✓)a) is convex in ✓ for each(x  a)
pair  supx ✓￿f(x; ✓)￿∞≤ B  f(x;⋅)a is L-Lipschitz in ✓ with respect to the `2 norm  and that ⇥
These assumptions are satisﬁed whenF is a linear class  under appropriate boundedness conditions.

The pseudocode for HINGE-LMC is displayed in Algorithm 1  and all parameters settings are given
in Appendix D. The algorithm is a continuous variant of exponential weights [7]  where at round t 
we deﬁne the exponential weights distribution via its density (w.r.t. the Lebesgue measure over ⇥):

Pt(✓)∝ exp(−⌘wt−1(✓)) 

wt−1(✓)￿ t−1￿s=1￿˜`s  (f(xs; ✓))￿ 

where ⌘ is a learning rate and ˜`s is a loss vector estimate. At a high level  at each iteration the

algorithm samples ✓t∼ Pt  then samples the action at from the induced policy distribution pt(⋅; ✓)=
⇡hinge(f(xt; ✓t))  appropriately smoothed. The algorithm plays at and constructs a loss estimate
˜`t￿ mt⋅ `t(a)1{a= at}  where mt is an approximate importance weight computed by repeatedly
a tractable log-concave sampling problem  by using the induced policy distribution ⇡hinge(⋅)  we are

sampling from Pt. This vector ˜`t is passed to exponential weights to deﬁne the distribution at the next
round. To sample from Pt we use Projected Langevin Monte Carlo (LMC)  displayed in Algorithm 2.
The algorithm has many important subtleties. Apart from passing to the hinge surrogate loss to obtain

also able to control the local norm term in the exponential weights regret bound.7 Then  the analysis
for Projected LMC [12] requires a smooth potential function  which we obtain by convolving with the
gaussian density  also known as randomized smoothing [19]. We also use `2 regularization for strong
convexity and to overcome sampling errors introduced by randomized smoothing. Finally  we use the
geometric resampling technique [33] to approximate the importance weight by repeated sampling.
Here  we state the main guarantee and its consequences. A more complete theorem statement  with
exact parameter speciﬁcations and the precise running time is provided in Appendix D as Theorem 18.

7This seems specialized to surrogates that can be expressed as an inner product between the loss vector and (a
transformation of) the prediction  so it does not apply to standard loss functions in bandit multiclass prediction.

7

E

1
K

E

Theorem 6 (Informal). Under the assumptions of Subsection 3.1  HINGE-LMC with appropriate

parameter settings runs in time poly(T  d  B  K  1

   R  L) and guarantees
T￿t=1￿`t  (f(xt; ✓))￿+ ˜O￿ B

T￿t=1

`t(at)≤ inf
✓∈⇥

Since bandit multiclass prediction is a special case of contextual bandits  Theorem 6 immediately

Corollary 7 (Bandit multiclass). In the bandit multiclass setting  Algorithm 1 enjoys a mistake bound

√dT￿ .
implies a√dT -mistake bound for this setting. See Appendix B for more discussion.
of ˜O((B￿)√dT) against the cost-sensitive -hinge loss and runs in polynomial time.
simplicity in deﬁning the condition  assume that for every(x  `) pair  ` is a random variable with
Corollary 8 (Realizable bound). In addition to the conditions above  assume that there exists ✓￿∈ ⇥
such that for every(x  `) pair and for all a∈A  we have f(x; ✓￿)a￿ K1{¯`(a)≤ mina′ ¯`(a′)}− .

Additionally  under a realizability condition for the hinge loss  we obtain a standard regret bound. For

conditional mean ¯` (chosen by the adversary) and ¯` has a unique action with minimal loss.

Then HINGE-LMC runs in polynomial time and guarantees

T￿t=1

E¯`t(at)≤ T￿t=1

E min
a

¯`(a)+ ˜O￿ B

√dT￿.

A few comments are in order:

1. The use of LMC for sampling is not strictly necessary. Other log-concave samplers do exist for
non-smooth potentials [30]  which will remove the parameters m  u    signiﬁcantly simplify the
algorithm  and even lead to a better run-time guarantee using current theory. However  we prefer
to use LMC due to its success in Bayesian inference and deep learning  and its connections to
incremental optimization methods. Note that more recent results in slightly different settings [36 
17  15] suggest that it may be possible to substantially improve upon the LMC analysis that we
use and even extend it to non-convex settings. We are hopeful that the LMC approach will lead to
a practically useful contextual bandit algorithm and plan to explore this direction further.

our loss is slightly different from the multiclass hinge loss used by Kakade et al. [26] in their

2. Corollary 7 provides a new solution to the open problem of Abernethy and Rakhlin [2]. In

fact  it is the ﬁrst efﬁcient√dT -type regret bound against a hinge loss benchmark  although
T 2￿3-regret BANDITRON algorithm (which motivated the open problem). All prior√dT -regret

algorithms [24  9  22] use losses with curvature such as the multiclass logistic loss or the squared
hinge loss. See Appendix B for a comparison between cost-sensitive and multiclass hinge losses.
3. In Corollary 8  regret is measured relative to the policy that chooses the best action (in expectation)
on every round. As in prior results [1  3]  this is possible because the realizability condition ensures

hence the dependence on K is implicit and in fact slightly worse than the optimal rate [16].

4. For Corollary 8  the best points of comparisons are methods based on square-loss realizability [3 
21]  although our condition is different. Compared with LINUCB and variants [16  1] specialized

guarantees for linear classes.8 Compared with Foster et al. [21]  which is the only other efﬁcient
approach at a comparable level of generality  our assumptions on the regressor class are stronger 
but we obtain better guarantees  in particular removing distribution-dependent parameters.

that this policy is in our class. Note that here  a requirement for realizability is that B≥ K   and
to `2￿`2 geometry  our assumptions are somewhat weaker but these methods have slightly better
To summarize  HINGE-LMC is the ﬁrst efﬁcient√dT -regret algorithm for bandit multiclass predic-
√dT policy regret under hinge-based realizability. Finally  while we lose the theoretical guarantees 
8In the abstract linear setting we takeF to be the set of linear functions in the ball for some norm￿⋅￿ and
contexts to be bounded in the dual norm￿⋅￿￿. The runtime of HINGE-LMC will degrade (polynomially) with
the ratio￿✓￿￿￿✓￿2  but the regret bound is the same for any such norm pair.

tion using the hinge loss. It also represents a new approach to adversarial contextual bandits  yielding

the algorithm easily extends to non-convex classes  which we expect to be practically effective.

8

T￿t=1

T
K

3.2 SMOOTHFTL
A drawback of HINGE-LMC is that it only applies in the parametric regime. We now introduce
an efﬁcient (in terms of queries to a hinge loss minimization oracle) algorithm with a regret bound

ˆfm−1￿ argmin
f∈F

nm−1￿⌧=nm−1￿ˆ`⌧   (f(x⌧))￿.

The algorithm we analyze is simply Follow-The-Leader with uniform smoothing and epoching  which

similar to Theorem 4  but in the stochastic setting  where{(xt ` t)}T
t=1 are drawn i.i.d. from some
joint distributionD overX× RK+ . Here we return to the abstract setting with regression classF  and
for simplicity  we assume B= 1.
we refer to as SMOOTHFTL. We use an epoch schedule where the mth epoch lasts for nm￿ 2m rounds
(starting with m= 0). At the beginning of the mth epoch  we compute the empirical importance
weighted hinge-loss minimizer ˆfm−1 using only the data from the previous epoch. That is  we set
Then  for each round t in the mth epoch  we sample at from pt￿(1− Kµ)⇡hinge( ˆfm−1(xt))+ µ.
The parameter µ∈(0  1￿K] controls the smoothing. At time t= 1 we simply take p1 to be uniform.
Theorem 9. Suppose thatF satisﬁes logN∞ ∞(" F  T) ∝ "−p for some p > 2. Then in the
stochastic setting  with µ= K−1T −1
p+1   SMOOTHFTL enjoys the following expected regret guarantee9
E`t(at)≤ inf
f∈F
This provides an algorithmic counterpart to Proposition 5 in the p≥ 2 regime. The algorithm is quite
the regime p∈(0  2) as an open problem.

similar to EPOCH-GREEDY [29]  and the main contribution here is to provide a careful analysis for
large function classes. We leave obtaining an oracle-efﬁcient algorithm that matches Proposition 5 in

A similar bound can be obtained for the ramp loss by simply replacing the hinge loss ERM. We
analyze the hinge loss version because standard (e.g.
linear) classes admit efﬁcient hinge loss
minimization oracles. Interestingly  the bound in Theorem 9 actually improves on Proposition 5  in
that it is independent of K. This is due to the scaling of the hinge loss in Lemma 1.
In Appendix F  we extend the analysis to the stochastic Lipschitz contextual bandit setting. Here 

E￿`  (f(x))￿+ ˜O￿(T￿) p
p+1￿.

p

p

that SMOOTHFTL achieves T
This improves on the T
lower bound is T

instead of measuring regret against the benchmark ○F we compare to the class of all 1-Lipschitz
functions fromX to (A)  whereX is a metric space of bounded covering dimension. We show

p+1 regret with a p-dimensional context space and ﬁnite action space.
p+1
p+2 bound of Cesa-Bianchi et al. [14]  as in Example 2  yet the best available
[25]. Closing this gap remains an intriguing open problem.

p−1
4 Discussion
This paper initiates a study of the utility of surrogate losses in contextual bandit learning. We
obtain new margin-based regret bounds in terms of sequential complexity notions on the benchmark
class  improving on the best known rates for Lipschitz contextual bandits and providing dimension-
independent bounds for linear classes. On the algorithmic side  we provide the ﬁrst solution to
the open problem of Abernethy and Rakhlin [2] with a non-curved loss and we also show that
Follow-the-Leader with uniform smoothing performs well in nonparametric settings.
Yet  several open problems remain. First  our bounds in Section 2 are likely suboptimal in the
dependence on K  and improving this is a natural direction. Other questions involve deriving stronger
lower bounds (e.g.  for the non-parametric setting) and adapting to the margin parameter. We also hope
to experiment with HINGE-LMC  and develop a better understanding of computational-statistical
tradeoffs with surrogate losses. We look forward to studying these questions in future work.
Acknowledgements. We thank Haipeng Luo  Karthik Sridharan  Chen-Yu Wei  and Chicheng
Zhang for several helpful discussions. D.F. acknowledges the support of the NDSEG PhD fellowship
and Facebook PhD fellowship.

9This result is stated in terms of the sequential coverN∞ ∞ to avoid additional deﬁnitions  but can easily be

improved to depend on the classical (worst-case) covering number seen in statistical learning.

9

References
[1] Yasin Abbasi-Yadkori  Dávid Pál  and Csaba Szepesvári.

Improved algorithms for linear

stochastic bandits. In Advances in Neural Information Processing Systems  2011.

[2] Jacob D. Abernethy and Alexander Rakhlin. An efﬁcient bandit algorithm for O(√T)-regret in

online multiclass prediction? In Conference on Learning Theory  2009.

[3] Alekh Agarwal  Miroslav Dudík  Satyen Kale  John Langford  and Robert E. Schapire. Contex-

tual bandit learning with predictable rewards. In Artiﬁcial Intelligence and Statistics  2012.

[4] Alekh Agarwal  Daniel J. Hsu  Satyen Kale  John Langford  Lihong Li  and Robert E. Schapire.
Taming the monster: A fast and simple algorithm for contextual bandits. In International
Conference on Machine Learning  2014.

[5] Alekh Agarwal  Sarah Bird  Markus Cozowicz  Luong Hoang  John Langford  Stephen Lee 
Jiaji Li  Dan Melamed  Gal Oshri  Oswaldo Ribas  Siddhartha Sen  and Aleksandrs Slivkins.
Making contextual decisions with low technical debt. arXiv:1606.03966  2016.

[6] Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations.

Cambridge University Press  2009.

[7] Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic

multiarmed bandit problem. SIAM Journal on Computing  2002.

[8] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk

bounds. Journal of the American Statistical Association  2006.

[9] Alina Beygelzimer  Francesco Orabona  and Chicheng Zhang. Efﬁcient online bandit multiclass

learning with ˜O(√T) regret. In International Conference on Machine Learning  2017.

[10] Stéphane Boucheron  Olivier Bousquet  and Gábor Lugosi. Theory of classiﬁcation: A survey

of some recent advances. ESAIM: Probability and Statistics  2005.

[11] Stéphane Boucheron  Gábor Lugosi  and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. Oxford University Press  2013.

[12] Sébastien Bubeck  Ronen Eldan  and Joseph Lehec. Sampling from a log-concave distribution

with projected langevin monte carlo. Discrete and Computational Geometry  2018.

[13] Nicolò Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  2006.

[14] Nicolò Cesa-Bianchi  Pierre Gaillard  Claudio Gentile  and Sébastien Gerchinovitz. Algorithmic
chaining and the role of partial feedback in online nonparametric learning. In Conference on
Learning Theory  2017.

[15] Xiang Cheng  Niladri S. Chatterji  Yasin Abbasi-Yadkori  Peter L. Bartlett  and Michael I. Jordan.
Sharp convergence rates for langevin dynamics in the nonconvex setting. arXiv:1805.01648 
2018.

[16] Wei Chu  Lihong Li  Lev Reyzin  and Robert E. Schapire. Contextual bandits with linear payoff

functions. In International Conference on Artiﬁcial Intelligence and Statistics  2011.

[17] Arnak S. Dalalyan and Avetik G. Karagulyan. User-friendly guarantees for the langevin monte

carlo with inaccurate gradient. arXiv:1710.00095  2017.

[18] Amit Daniely and Tom Halbertal. The price of bandit information in multiclass online classiﬁ-

cation. In Conference on Learning Theory  2013.

[19] John C. Duchi  Peter L. Bartlett  and Martin J. Wainwright. Randomized smoothing for

stochastic optimization. SIAM Journal on Optimization  2012.

[20] Dylan J. Foster  Alexander Rakhlin  and Karthik Sridharan. Adaptive online learning. In

Advances in Neural Information Processing Systems  2015.

10

[21] Dylan J. Foster  Alekh Agarwal  Miroslav Dudík  Haipeng Luo  and Robert E. Schapire.
Practical contextual bandits with regression oracles. International Conference on Machine
Learning  2018.

[22] Dylan J. Foster  Satyen Kale  Haipeng Luo  Mehryar Mohri  and Karthik Sridharan. Logistic

regression: The importance of being improper. Conference on Learning Theory  2018.

[23] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning

and an application to boosting. Journal of Computer and System Sciences  1997.

[24] Elad Hazan and Satyen Kale. Newtron: an efﬁcient bandit algorithm for online multiclass

prediction. In Advances in Neural Information Processing Systems  2011.

[25] Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In Conference on

Learning Theory  2007.

[26] Sham M. Kakade  Shai Shalev-Shwartz  and Ambuj Tewari. Efﬁcient bandit algorithms for

online multiclass prediction. In International Conference on Machine learning  2008.

[27] Sham M. Kakade  Karthik Sridharan  and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds  margin bounds  and regularization. In Advances in Neural Information Processing
Systems  2009.

[28] Sham M. Kakade  Shai Shalev-Shwartz  and Ambuj Tewari. Regularization techniques for

learning with matrices. Journal of Machine Learning Research  2012.

[29] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems  2008.

[30] László Lovász and Santosh Vempala. The geometry of logconcave functions and sampling

algorithms. Random Structures & Algorithms  2007.

[31] Thodoris Lykouris  Karthik Sridharan  and Éva Tardos. Small-loss bounds for online learning

with partial information. Conference on Learning Theory  2018.

[32] Hariharan Narayanan and Alexander Rakhlin. Efﬁcient sampling from time-varying log-concave

distributions. Journal of Machine Learning Research  2017.

[33] Gergely Neu and Gábor Bartók. An efﬁcient algorithm for learning with semi-bandit feedback.

In International Conference on Algorithmic Learning Theory  2013.

[34] Bernardo Ávila Pires  Csaba Szepesvari  and Mohammad Ghavamzadeh. Cost-sensitive multi-

class classiﬁcation risk bounds. In International Conference on Machine Learning  2013.

[35] Gilles Pisier. Martingales with values in uniformly convex spaces. Israel Journal of Mathematics 

1975.

[36] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory 
2017.

[37] Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss

functions. arxiv:1501.06598  2015.

[38] Alexander Rakhlin and Karthik Sridharan. BISTRO: An efﬁcient relaxation-based method for

contextual bandits. In International Conference on Machine Learning  2016.

[39] Alexander Rakhlin and Karthik Sridharan. On equivalence of martingale tail bounds and

deterministic regret inequalities. Conference on Learning Theory  2017.

[40] Alexander Rakhlin  Karthik Sridharan  and Ambuj Tewari. Online learning: Random averages 
combinatorial parameters  and learnability. Advances in Neural Information Processing Systems 
2010.

11

[41] Alexander Rakhlin  Karthik Sridharan  and Ambuj Tewari. Online learning via sequential

complexities. Journal of Machine Learning Research  2015.

[42] Alexander Rakhlin  Karthik Sridharan  and Ambuj Tewari. Sequential complexities and uniform

martingale laws of large numbers. Probability Theory and Related Fields  2015.

[43] Robert E. Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press  2012.
[44] Aleksandrs Slivkins. Contextual bandits with similarity information. In Conference on Learning

Theory  2011.

[45] Nathan Srebro  Karthik Sridharan  and Ambuj Tewari. On the universality of online mirror

descent. In Advances in Neural Information Processing Systems  2011.

[46] Vasilis Syrgkanis  Akshay Krishnamurthy  and Robert E. Schapire. Efﬁcient algorithms for

adversarial contextual learning. In International Conference on Machine Learning  2016.

[47] Vasilis Syrgkanis  Haipeng Luo  Akshay Krishnamurthy  and Robert E Schapire. Improved
regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information
Processing Systems  2016.

[48] Ambuj Tewari and Susan A. Murphy. From ads to interventions: Contextual bandits in mobile

health. In Mobile Health  2017.

[49] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  2004.

12

,Eleni Triantafillou
Richard Zemel
Raquel Urtasun
Dylan Foster
Akshay Krishnamurthy