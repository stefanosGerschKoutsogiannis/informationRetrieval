2014,Altitude Training: Strong Bounds for Single-Layer Dropout,Dropout training  originally designed for deep neural networks  has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that  under a generative Poisson topic model with long documents  dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout  it will do very well on the uncorrupted test set. We also show that  under similar conditions  dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.,Altitude Training:

Strong Bounds for Single-Layer Dropout

Stefan Wager⇤  William Fithian⇤  Sida Wang†  and Percy Liang⇤ †

Departments of Statistics⇤ and Computer Science†
Stanford University  Stanford  CA-94305  USA

{swager  wfithian}@stanford.edu  {sidaw  pliang}@cs.stanford.edu

Abstract

Dropout training  originally designed for deep neural networks  has been success-
ful on high-dimensional single-layer natural language tasks. This paper proposes
a theoretical explanation for this phenomenon: we show that  under a generative
Poisson topic model with long documents  dropout training improves the exponent
in the generalization bound for empirical risk minimization. Dropout achieves this
gain much like a marathon runner who practices at altitude: once a classiﬁer learns
to perform reasonably well on training examples that have been artiﬁcially cor-
rupted by dropout  it will do very well on the uncorrupted test set. We also show
that  under similar conditions  dropout preserves the Bayes decision boundary and
should therefore induce minimal bias in high dimensions.

1

Introduction

Dropout training [1] is an increasingly popular method for regularizing learning algorithms. Dropout
is most commonly used for regularizing deep neural networks [2  3  4  5]  but it has also been found
to improve the performance of logistic regression and other single-layer models for natural language
tasks such as document classiﬁcation and named entity recognition [6  7  8]. For single-layer linear
models  learning with dropout is equivalent to using “blankout noise” [9].
The goal of this paper is to gain a better theoretical understanding of why dropout regularization
works well for natural language tasks. We focus on the task of document classiﬁcation using linear
classiﬁers where data comes from a generative Poisson topic model. In this setting  dropout effec-
tively deletes random words from a document during training; this corruption makes the training
examples harder. A classiﬁer that is able to ﬁt the training data will therefore receive an accuracy
boost at test time on the much easier uncorrupted examples. An apt analogy is altitude training 
where athletes practice in more difﬁcult situations than they compete in. Importantly  our analysis
does not rely on dropout merely creating more pseudo-examples for training  but rather on dropout
creating more challenging training examples. Somewhat paradoxically  we show that removing in-
formation from training examples can induce a classiﬁer that performs better at test time.

Main Result Consider training the zero-one loss empirical risk minimizer (ERM) using dropout 
where each word is independently removed with probability  2 (0  1). For a class of Poisson
generative topic models  we show that dropout gives rise to what we call the altitude training phe-
nomenon: dropout improves the excess risk of the ERM by multiplying the exponent in its decay
rate by 1/(1  ). This improvement comes at the cost of an additive term of O(1/p)  where 
is the average number of words per document. More formally  let h⇤ and ˆh0 be the expected and

S. Wager and W. Fithian are supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship and NSF

VIGRE grant DMS–0502385 respectively.

1

empirical risk minimizers  respectively; let h⇤ and ˆh be the corresponding quantities for dropout
training. Let Err(h) denote the error rate (on test examples) of h. In Section 4  we show that:

Err⇣ˆh⌘  Err (h⇤)
|
}

dropout excess risk

{z

(1)

= eOP0BB@⇣Err⇣ˆh0⌘  Err (h⇤)⌘
}

ERM excess risk

{z

|

1

1 +

1
p

1CCA  

where eOP is a variant of big-O in probability notation that suppresses logarithmic factors. If  is

large (we are classifying long documents rather than short snippets of text)  dropout considerably
accelerates the decay rate of excess risk. The bound (1) holds for ﬁxed choices of . The constants
in the bound worsen as  approaches 1  and so we cannot get zero excess risk by sending  to 1.
Our result is modular in that it converts upper bounds on the ERM excess risk to upper bounds on
the dropout excess risk. For example  recall from classic VC theory that the ERM excess risk is

examples. With dropout  = 0.5  our result (1) directly implies that the dropout excess risk is

The intuition behind the proof of (1) is as follows: when  = 0.5  we essentially train on half
documents and test on whole documents. By conditional independence properties of the generative
topic model  the classiﬁcation score is roughly Gaussian under a Berry-Esseen bound  and the error
rate is governed by the tails of the Gaussian. Compared to half documents  the coefﬁcient of variation

eOP (pd/n)  where d is the number of features (vocabulary size) and n is the number of training
eOP (d/n + 1/p).
of the classiﬁcation score on whole documents (at test time) is scaled down by p1   compared to
half documents (at training time)  resulting in an exponential reduction in error. The additive penalty
of 1/p stems from the Berry-Esseen approximation.
Note that the bound (1) only controls the dropout excess risk. Even if dropout reduces the excess
risk  it may introduce a bias Err(h⇤)Err(h⇤)  and thus (1) is useful only when this bias is small. In
Section 5  we will show that the optimal Bayes decision boundary is not affected by dropout under
the Poisson topic model. Bias is thus negligible when the Bayes boundary is close to linear.
It is instructive to compare our generalization bound to that of Ng and Jordan [10]  who showed that
the naive Bayes classiﬁer exploits a strong generative assumption—conditional independence of the

features given the label—to achieve an excess risk of OP (p(log d)/n). However  if the generative
assumption is incorrect  then naive Bayes can have a large bias. Dropout enables us to cut excess risk
without incurring as much bias. In fact  naive Bayes is closely related to logistic regression trained
using an extreme form of dropout with  ! 1. Training logistic regression with dropout rates from
the range  2 (0  1) thus gives a family of classiﬁers between unregularized logistic regression and
naive Bayes  allowing us to tune the bias-variance tradeoff.

Other perspectives on dropout
In the general setting  dropout only improves generalization by
a multiplicative factor. McAllester [11] used the PAC-Bayes framework to prove a generalization
bound for dropout that decays as 1  . Moreover  provided that  is not too close to 1  dropout
behaves similarly to an adaptive L2 regularizer with parameter /(1) [6  12]  and at least in linear
regression such L2 regularization improves generalization error by a constant factor. In contrast  by
leveraging the conditional independence assumptions of the topic model  we are able to improve the
exponent in the rate of convergence of the empirical risk minimizer.
It is also possible to analyze dropout as an adaptive regularizer [6  9  13]: in comparison with L2
regularization  dropout favors the use of rare features and encourages conﬁdent predictions. If we
believe that good document classiﬁcation should produce conﬁdent predictions by understanding
rare words with Poisson-like occurrence patterns  then the work on dropout as adaptive regulariza-
tion and our generalization-based analysis are two complementary explanations for the success of
dropout in natural language tasks.

2 Dropout Training for Topic Models
In this section  we introduce binomial dropout  a form of dropout suitable for topic models  and the
Poisson topic model  on which all our analyses will be based.

2

Binomial Dropout Suppose that we have a binary classiﬁcation problem1 with count features
x(i) 2{ 0  1  2  . . .}d and labels y(i) 2{ 0  1}. For example  x(i)
is the number of times the j-th
word in our dictionary appears in the i-th document  and y(i) is the label of the document. Our goal
is to train a weight vector bw that classiﬁes new examples with features x via a linear decision rule
ˆy = I{bw · x > 0}. We start with the usual empirical risk minimizer:
`⇣w; x(i)  y(i)⌘)

def= argminw2Rd( nXi=1

for some loss function ` (we will analyze the zero-one loss but use logistic loss in experiments [e.g. 
10  14  15]). Binomial dropout trains on perturbed features ˜x(i) instead of the original features x(i):

(2)

j

bw0
def= argminw( nXi=1

bw

Eh`⇣w; ˜x(i)  y(i)⌘i)   where ˜x(i)

j = Binom⇣x(i)

j ; 1  ⌘ .

(3)

In other words  during training  we randomly thin the j-th feature xj with binomial noise. If xj
counts the number of times the j-th word appears in the document  then replacing xj with ˜xj is
equivalent to independently deleting each occurrence of word j with probability . Because we
are only interested in the decision boundary  we do not scale down the weight vector obtained by
dropout by a factor 1   as is often done [e.g.  1].
Binomial dropout differs slightly from the usual deﬁnition of (blankout) dropout  which alters the
feature vector x by setting random coordinates to 0 [6  9  11  12]. The reason we chose to study
binomial rather than blankout dropout is that Poisson random variables remain Poisson even after
binomial thinning; this fact lets us streamline our analysis. For rare words that appear once in the
document  the two types of dropout are equivalent.

j

j according to x(i)

A Generative Poisson Topic Model Throughout our analysis  we assume that the data is drawn
from a Poisson topic model depicted in Figure 1a and deﬁned as follows. Each document i is as-
signed a label y(i) according to some Bernoulli distribution. Then  given the label y(i)  the document
gets a topic ⌧ (i) 2 ⇥ from a distribution ⇢y(i). Given the topic ⌧ (i)  for every word j in the vocabu-
lary  we generate its frequency x(i)
j 2 [0 1)
is the expected number of times word j appears under topic ⌧. Note that k(⌧ )k1 is the average
length of a document with topic ⌧. Deﬁne  def= min⌧2⇥ k(⌧ )k1 to be the shortest average doc-
ument length across topics. If ⇥ contains only two topics—one for each class—we get the naive
Bayes model. If ⇥ is the (K  1)-dimensional simplex where (⌧ ) is a ⌧-mixture over K basis
vectors  we get the K-topic latent Dirichlet allocation [16].2
Note that although our generalization result relies on a generative model  the actual learning algo-
rithm is agnostic to it. Our analysis shows that dropout can take advantage of a generative structure
while remaining a discriminative procedure. If we believed that a certain topic model held exactly
and we knew the number of topics  we could try to ﬁt the full generative model by EM. This 
however  could make us vulnerable to model misspeciﬁcation. In contrast  dropout beneﬁts from
generative assumptions while remaining more robust to misspeciﬁcation.

 ⌧ (i) ⇠ Poisson((⌧ (i))

)  where (⌧ )

j

3 Altitude Training: Linking the Dropout and Data-Generating Measures

Our goal is to understand the behavior of a classiﬁer ˆh trained using dropout. During dropout 
the error of any classiﬁer h is characterized by two measures. In the end  we are interested in the
usual generalization error (expected risk) of h where x is drawn from the underlying data-generating
measure:

Err (h) def= P [y 6= h(x)] .

(4)

1Dropout training is known to work well in practice for multi-class problems [8]. For simplicity  however 

we will restrict our theoretical analysis to a two-class setup.

2 In topic modeling  the vertices of the simplex ⇥ are “topics” and ⌧ is a mixture of topics  whereas we call

⌧ itself a topic.

3

However  since dropout training works on the corrupted data ˜x (see (3))  in the limit of inﬁnite data 
the dropout estimator will converge to the minimizer of the generalization error with respect to the
dropout measure over ˜x:

Err (h) def= P [y 6= h(˜x)] .

(5)

The main difﬁculty in analyzing the generalization of dropout is that classical theory tells us that
the generalization error with respect to the dropout measure will decrease as n ! 1  but we are
interested in the original measure. Thus  we need to bound Err in terms of Err. In this section  we
show that the error on the original measure is actually much smaller than the error on the dropout
measure; we call this the altitude training phenomenon.
Under our generative model  the count features xj are conditionally independent given the topic
⌧. We thus focus on a single ﬁxed topic ⌧ and establish the following theorem  which provides a
per-topic analogue of (1). Section 4 will then use this theorem to obtain our main result.
Theorem 1. Let h be a binary linear classiﬁer with weights w  and suppose that our features are
drawn from the Poisson generative model given topic ⌧. Let c⌧ be the more likely label given ⌧:

Let ˜"⌧ be the sub-optimal prediction rate in the dropout measure

where ˜x(i) is an example thinned by binomial dropout (3)  and P is taken over the data-generating
process. Let "⌧ be the sub-optimal prediction rate in the original measure

(6)

(7)

(8)

(9)

c⌧

˜"⌧

def= arg max
c2{0 1}

Phy(i) = c ⌧ (i) = ⌧i .
def= PhInw · ˜x(i) > 0o 6= c⌧ ⌧ (i) = ⌧i  
def= PhInw · x(i) > 0o 6= c⌧ ⌧ (i) = ⌧i .
"⌧ = eO✓˜"

⌧ +p ⌧◆  

1
1

"⌧

Then:

where ⌧ = maxjw2

j /Pd

j=1 (⌧ )

j w2

j   and the constants in the bound depend only on .

Theorem 1 only provides us with a useful bound when the term ⌧ is small. Whenever the largest
j   then p ⌧ scales as O(1/p)  where  is the average
w2
document length. Thus  the bound (9) is most useful for long documents.

j is not much larger than the average w2

j=1 (⌧ )

j=1 (⌧ )

j w2

j wj and 2

A Heuristic Proof of Theorem 1. The proof of Theorem 1 is provided in the technical appendix.
Here  we provide a heuristic argument for intuition. Given a ﬁxed topic ⌧  suppose that it is optimal

to predict c⌧ = 1  so our test error is "⌧ = P⇥w · x  0 ⌧⇤ . For long enough documents  by
the central limit theorem  the score s def= w · x will be roughly Gaussian s ⇠N µ⌧   2
⌧   where
µ⌧ =Pd
j . This implies that "⌧ ⇡ (µ⌧ /⌧ )   where  is the
cumulative distribution function of the Gaussian. Now  let ˜s def= w · ˜x be the score on a dropout
sample. Clearly  E [˜s] = (1  ) µ⌧ and Var [˜s] = (1  ) 2
⌧   because the variance of a Poisson
random variable scales with its mean. Thus 
⌧◆(1)

˜"⌧ ⇡ ✓p1  

⌧◆ ⇡ ✓

⌧ =Pd

Figure 1b illustrates the relationship between the two Gaussians. This explains the ﬁrst term on the
right-hand side of (9). The extra error term p ⌧ arises from a Berry-Esseen bound that approxi-
mates Poisson mixtures by Gaussian random variables.

⇡ "(1)

(10)

µ⌧

⌧

.

µ⌧

4 A Generalization Bound for Dropout
By setting up a bridge between the dropout measure and the original data-generating measure  The-
orem 1 provides a foundation for our analysis. It remains to translate this result into a statement
about the generalization error of dropout. For this  we need to make a few assumptions.

4

⇢

y



⌧

Original
Dropout

5

.

0

4

.

0

.

0

3

y
t
i
s
n
e
d

2

.

0

1

.

0

0

.

0

x

J

I

−2

−1

0

1

2

3

score

(a) Graphical representation of the Poisson topic model:
Given a document with label y  we draw a document
topic ⌧ from the multinomial distribution with proba-
bilities ⇢y. Then  we draw the words x from the topic’s
Poisson distribution with mean (⌧ ). Boxes indicate re-
peated observations  and greyed-out nodes are observed
during training.

(b) For a ﬁxed classiﬁer w  the probabilities of er-
ror on an example drawn from the original and
dropout measures are governed by the tails of two
Gaussians (shaded). The dropout Gaussian has a
larger coefﬁcient of variation  which means the er-
ror on the original measure (test) is much smaller
than the error on the dropout measure (train) (10).
In this example  µ = 2.5   = 1  and  = 0.5.

Figure 1: (a) Graphical model. (b) The altitude training phenomenon.

Our ﬁrst assumption is fundamental: if the classiﬁcation signal is concentrated among just a few
features  then we cannot expect dropout training to do well. The second and third assumptions 
which are more technical  guarantee that a classiﬁer can only do well overall if it does well on every
topic; this lets us apply Theorem 1. A more general analysis that relaxes Assumptions 2 and 3 may
be an interesting avenue for future work.

Assumption 1: well-balanced weights First  we need to assume that all the signal is not concen-
trated in a few features. To make this intuition formal  we say a linear classiﬁer with weights w is
well-balanced if the following holds for each topic ⌧:

j=1 (⌧ )

j

j Pd

j=1 (⌧ )

maxjw2
Pd

j w2
j

  for some 0 << 1.

(11)

For example  suppose each word was either useful (|wj| = 1) or not (wj = 0); then  is the inverse
expected fraction of words in a document that are useful. In Theorem 2 we restrict the ERM to
well-balanced classiﬁers and assume that the expected risk minimizer h⇤ over all linear rules is also
well-balanced.

Assumption 2: discrete topics Second  we assume that there are a ﬁnite number T of topics  and
that the available topics are not too rare or ambiguous: the minimal probability of observing any
topic ⌧ is bounded below by

and that each topic-conditional probability is bounded away from 1

2 (random guessing):

P [⌧ ]  pmin > 0 

Phy(i) = c ⌧ (i) = ⌧i 

1

2  ↵> 0

for all topics ⌧ 2{ 1  ...  T}. This assumption substantially simpliﬁes our arguments  allowing us
to apply Theorem 1 to each topic separately without technical overhead.

Assumption 3: distinct topics Finally  as an extension of Assumption 2  we require that the topics
be “well separated.” First  deﬁne Errmin = P[y(i) 6= c⌧ (i)]  where c⌧ is the most likely label given
topic ⌧ (6); this is the error rate of the optimal decision rule that sees topic ⌧. We assume that the
best linear rule h⇤ satisfying (11) is almost as good as always guessing the best label c⌧ under the
dropout measure:

(12)

(13)

(14)

Err (h⇤) = Errmin +O✓ 1
p◆  

5

where  as usual   is a lower bound on the average document length. If the dimension d is larger
than the number of topics T   this assumption is fairly weak: the condition (14) holds whenever the
matrix ⇧ of topic centers has full rank  and the minimum singular value of ⇧ is not too small (see
Proposition 6 in the Appendix for details). This assumption is satisﬁed if the different topics can be
separated from each other with a large margin.
Under Assumptions 1–3 we can turn Theorem 1 into a statement about generalization error.
Theorem 2. Suppose that our features x are drawn from the Poisson generative model (Figure 1a) 
and Assumptions 1–3 hold. Deﬁne the excess risks of the dropout classiﬁer ˆh on the dropout and
data-generating measures  respectively:

˜⌘ def= Err⇣ˆh⌘  Err (h⇤) and ⌘ def= Err⇣ˆh⌘  Err (h⇤) .

Then  the altitude training phenomenon applies:

⌘ = eO✓˜⌘

1

1 +

1

p◆ .

The above bound scales linearly in p1

min and ↵1; the full dependence on  is shown in the appendix.
In a sense  Theorem 2 is a meta-generalization bound that allows us to transform generalization
bounds with respect to the dropout measure (˜⌘) into ones on the data-generating measure (⌘) in a

modular way. As a simple example  standard VC theory provides an ˜⌘ = eOP (pd/n) bound which 

together with Theorem 2  yields:
Corollary 3. Under the same conditions as Theorem 2  the dropout classiﬁer ˆh achieves the fol-
lowing excess risk:

Err⇣ˆh⌘  Err (h⇤) = eOP0@ r d
n! 1

1

+

1

p1A .

(15)

(16)

(17)

More generally  we can often check that upper bounds for Err(ˆh)  Err(h⇤) also work as upper
bounds for Err(ˆh)  Err(h⇤); this gives us the heuristic result from (1).
5 The Bias of Dropout
In the previous section  we showed that under the Poisson topic model in Figure 1a  dropout can
achieve a substantial cut in excess risk Err(ˆh) Err(h⇤). But to complete our picture of dropout’s
performance  we must address the bias of dropout: Err(h⇤)  Err(h⇤).
Dropout can be viewed as importing “hints” from a generative assumption about the data. Each ob-
served (x  y) pair (each labeled document) gives us information not only about the conditional class
probability at x  but also about the conditional class probabilities at numerous other hypothetical
values ˜x representing shorter documents of the same class that did not occur. Intuitively  if these ˜x
are actually good representatives of that class  the bias of dropout should be mild.
For our key result in this section  we will take the Poisson generative model from Figure 1a  but
further assume that document length is independent of the topic. Under this assumption  we will
show that dropout preserves the Bayes decision boundary in the following sense:
Proposition 4. Let (x  y) be distributed according to the Poisson topic model of Figure 1a. Assume
that document length is independent of topic: k(⌧ )k1 =  for all topics ⌧. Let ˜x be a binomial
dropout sample of x with some dropout probability  2 (0  1). Then  for every feature vector v 2 Rd 
we have:
(18)

P⇥y = 1 ˜x = v⇤ = P⇥y = 1 x = v⇤ .

If we had an inﬁnite amount of data (˜x  y) corrupted under dropout  we would predict according to
2}. The signiﬁcance of Proposition 4 is that this decision rule is identical to
the true Bayes decision boundary (without dropout). Therefore  the empirical risk minimizer of a
sufﬁciently rich hypothesis class trained with dropout would incur very small bias.

I{P⇥y = 1 ˜x = v⇤ > 1

6

0
0
3

0
0
2

2
X

0
0
1

0

LogReg Boundary
Dropout Boundary
Bayes Boundary

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
● ●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●● ●
●●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●●●
●
●
●
●
● ●
●●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
● ●
●
●●●
●
● ●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
● ●●
●
●
●● ●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
● ●
●
●●
● ●
●
●
● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●●
● ●
●
●
●
●● ●
● ●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●●
● ●
●
●
●
●
●
●
●
●
● ●
●
● ●
●
●●
● ●●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●●
● ●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●●●
● ●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●●●
●●
●●●●
● ●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
● ●●●
●●
● ●
● ●●
● ●●
●●
●●
●●
●●●
●
●
●
●
●
●
● ●
● ●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●●
● ●
● ●●●
●●
● ●●
● ●
●●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●●●●
●●
●●●
●●
●
●● ●
●●●●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●●
●●●
●●
●●●
●●●
●
●
●
●●●
●
●
● ● ●
●
●
● ●●●
●
●
●
●
●
● ●●●
●
●
●
●
● ● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●●
●●
●
●
●
● ●
●●
●●
● ●
●●
●
●
●
● ●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
● ●
●●
●
●●
●
●
●
●●
●●
●
●●
●
●
●●●
●
● ●
●
●
●●
●●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●●●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●
●
● ●
●
●● ●
●
●
●
●
●
●
●●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●
●●
●
●
●●
●
●
● ●●
●
●●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●●●
● ●
●● ●●●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●
●
●
●
●
●●
●
●●●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●●●
●
●
● ●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●●
●
●● ●
●
●●
●
●●
●
●
●
●
●
●●
●●
●●
●●
●●
●
●
●●
●● ●
● ● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●
●●
●
● ●
●●
●
●●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●● ●●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●●
●
●
●
●●
●
●
●
●●
● ●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
● ●
●●
●●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
● ●●
●
●● ●
● ●
●
●●
●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●●●
●
●
●● ●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
● ●
●●
●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●● ●
●
●
● ●
●
●
● ●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●● ●
●●
●●● ●●●●
●
●
●●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
● ●●●● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
●
●
●
●●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
● ●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●
●
●
●●
●
● ●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●
●●
●●
●●
●
●
●
●
●
●
●●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
● ●
● ●
●●
● ●
●●
●
●
●
●
●
● ●
●
●
●
●●●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
● ●●
●●
●●
●●
● ●●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
● ●●●
●
●
●
●
● ●
● ●
●●
●●
●
●
●
●●
● ●● ●
●
●
●
●
●
●
● ●
●●●
●
●
●
●
●
●
●
●
●
●●●● ●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●● ●
●
●
●
●
● ●
● ●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●
●
●
●
●
●
●●●
● ●
●●●
● ●●
●
●●
●
● ●
● ●
●
●
●
● ●
●
●
●●
●
●
●●
●●
●
●●
●●
●
●●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●● ●●
●
●●
●
●
●
● ●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
● ●●
●
● ●
●●
●●
●● ●
●
● ●
● ● ●
●
●
●●●●
●
●
●
● ●
● ● ●
●
●
● ●
● ●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●●
●●●
●
●
●●
●
●●●
●
●
●
●●
●
●●
●
●
●●●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●●●●
●
●
●
●
●
●
●
●
●
●●● ● ●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
● ●
●●
●●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ● ●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●●
●
●
●
●●
● ●
●
●
●
●
●●●●
●●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
● ●
●
●
●●●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●
●●
●
● ●
●
●
●
●●
●
●
●●●
●●
●
●●
● ●
●●
●
●●
●●
●
●
● ●
●
●
●
●
●●
●
●
●
●●●●
●
●
●
●
●
●●
●
●●● ●
● ● ●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●●
●
●
●
●
●
●
● ●
●
●
●●
●●●
●●
●●
●
●
● ●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●●
●●●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●●
●●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●●
● ●
●
●
●
●
●
●
●●
●
●●
●
●
●
●● ●
●●●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●●●
●●
●
● ●
●
●
●
●
●
●●●●
●
●
●
●
●
●
● ●●●
●
● ●
●●
●
●
●
●
●
●
●●
●
●●
●
●●
● ●●
●
●●
●
●
●●●
●
●
●
● ●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
● ● ●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
● ●
● ●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●●
●●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●
●
●
●
●
●●
● ●
●
●●
●
●
●
●
●
●
● ●
●
●
●●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

o Long Documents

Short (Dropout) Documents

0

100

200

300

400

500

X1

 0 

)

%

(
 
e
t
a
R

 
r
o
r
r

E

 
t
s
e
T

0
2

0
1

5

2

1

5
.
0

LR
0.25
0.5
0.75
0.9
0.95
0.99
NB

50

100

200

500

1000 2000

n

(a) Dropout ( = 0.75) with d = 2. For long doc-
uments (circles in the upper-right)  logistic regres-
sion focuses on capturing the small red cluster; the
large red cluster has almost no inﬂuence. Dropout
(dots in the lower-left) distributes inﬂuence more
equally between the two red clusters.

(b) Learning curves for the synthetic experiment.
Each axis is plotted on a log scale. Here the dropout
rate  ranges from 0 (logistic regression) to 1 (naive
Bayes) for multiple values of training set sizes n.
As n increases  less dropout is preferable  as the
bias-variance tradeoff shifts.

Figure 2: Behavior of binomial dropout in simulations. In the left panel  the circles are the original
data  while the dots are dropout-thinned examples. The Monte Carlo error is negligible.

However  Proposition 4 does not guarantee that dropout incurs no bias when we ﬁt a linear classiﬁer.
In general  the best linear approximation for classifying shorter documents is not necessarily the
best for classifying longer documents. As n ! 1  a linear classiﬁer trained on (x  y) pairs will
eventually outperform one trained on (˜x  y) pairs.

Dropout for Logistic Regression To gain some more intuition about how dropout affects linear
classiﬁers  we consider logistic regression. A similar phenomenon should also hold for the ERM 
but discussing this solution is more difﬁcult since the ERM solution does not have have a simple
characterization. The relationship between the 0-1 loss and convex surrogates has been studied

i=1y(i)  ˆpi x(i)  where
by  e.g.  [14  15]. The score criterion for logistic regression is 0 = Pn
ˆpi = (1 + ebw·x(i))1 are the ﬁtted probabilities. Note that easily-classiﬁed examples (where ˆpi is

close to y(i)) play almost no role in driving the ﬁt. Dropout turns easy examples into hard examples 
giving more examples a chance to participate in learning a good classiﬁcation rule.
Figure 2a illustrates dropout’s tendency to spread inﬂuence more democratically for a simple classi-
ﬁcation problem with d = 2. The red class is a 99:1 mixture over two topics  one of which is much
less common  but harder to classify  than the other. There is only one topic for the blue class. For
long documents (open circles in the top right)  the infrequent  hard-to-classify red cluster dominates
the ﬁt while the frequent  easy-to-classify red cluster is essentially ignored. For dropout documents
with  = 0.75 (small dots  lower left)  both red clusters are relatively hard to classify  so the infre-
quent one plays a less disproportionate role in driving the ﬁt. As a result  the ﬁt based on dropout is
more stable but misses the ﬁner structure near the decision boundary. Note that the solid gray curve 
the Bayes boundary  is unaffected by dropout  per Proposition 4. But  because it is nonlinear  we
obtain a different linear approximation under dropout.

6 Experiments and Discussion
Synthetic Experiment Consider the following instance of the Poisson topic model: We choose
the document label uniformly at random: P⇥y(i) = 1⇤ = 1
2. Given label 0  we choose topic ⌧ (i) = 0
deterministically; given label 1  we choose a real-valued topic ⌧ (i) ⇠ Exp(3). The per-topic Poisson
intensities (⌧ ) are deﬁned as follows:
✓(⌧ ) =8<:

(1  . . .   1  0  . . .   0  0  . . .   0)
 0  . . .   0
| {z }
| {z }

The ﬁrst block of 7 independent words are indicative of label 0  the second block of 7 correlated
words are indicative of label 1  and the remaining 486 words are indicative of neither.

 ⌧  . . .   ⌧
|
{z
}

(⌧ )
j = 1000 ·

j

e✓(⌧ )
j0=1 e✓(⌧ )

j0

if ⌧ = 0 
otherwise 

P500

(0  . . .   0

486

.

(19)

7

7

)

7

e
t
a
r
 
r
o
r
r

E

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

 
0

 

Log.Reg.
Naive Bayes
Dropout−0.8
Dropout−0.5
Dropout−0.2

 

Log.Reg.
Naive Bayes
Dropout−0.8
Dropout−0.5
Dropout−0.2

0.26

0.24

0.22

0.2

0.18

0.16

0.14

0.12

e
t
a
r
 
r
o
r
r

E

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Fraction of data used

0.1

 
0

0.2

0.6
0.4
Fraction of data used

0.8

1

(a) Polarity 2.0 dataset [17].

(b) IMDB dataset [18].

Figure 3: Experiments on sentiment classiﬁcation. More dropout is better relative to logistic regres-
sion for small datasets and gradually worsens with more training data.

We train a model on training sets of various size n  and evaluate the resulting classiﬁers’ error rates
on a large test set. For dropout  we recalibrate the intercept on the training set. Figure 2b shows
the results. There is a clear bias-variance tradeoff  with logistic regression ( = 0) and naive Bayes
( = 1) on the two ends of the spectrum. For moderate values of n  dropout improves performance 
with  = 0.95 (resulting in roughly 50-word documents) appearing nearly optimal for this example.

Sentiment Classiﬁcation We also examined the performance of dropout as a function of training
set size on a document classiﬁcation task. Figure 3a shows results on the Polarity 2.0 task [17] 
where the goal is to classify positive versus negative movie reviews on IMDB. We divided the
dataset into a training set of size 1 200 and a test set of size 800  and trained a bag-of-words logistic
regression model with 50 922 features. This example exhibits the same behavior as our simulation.
Using a larger  results in a classiﬁer that converges faster at ﬁrst  but then plateaus. We also
ran experiments on a larger IMDB dataset [18] with training and test sets of size 25 000 each and
approximately 300 000 features. As Figure 3b shows  the results are similar  although the training
set is not large enough for the learning curves to cross. When using the full training set  all but three
pairwise comparisons in Figure 3 are statistically signiﬁcant (p < 0.05 for McNemar’s test).

Dropout and Generative Modeling Naive Bayes and empirical risk minimization represent two
divergent approaches to the classiﬁcation problem. ERM is guaranteed to ﬁnd the best model as n !
1 but can have suboptimal generalization error when n is not large relative to d. Conversely  naive
Bayes has very low generalization error  but suffers from asymptotic bias. In this paper  we showed
that dropout behaves as a link between ERM and naive Bayes  and can sometimes achieve a more
favorable bias-variance tradeoff. By training on randomly generated sub-documents rather than on
whole documents  dropout implicitly codiﬁes a generative assumption about the data  namely that
excerpts from a long document should have the same label as the original document (Proposition 4).
Logistic regression with dropout appears to have an intriguing connection to the naive Bayes SVM
[NBSVM  19]  which is a way of using naive Bayes generative assumptions to strengthen an SVM.
In a recent survey of bag-of-words classiﬁers for document classiﬁcation  NBSVM and dropout often
obtain state-of-the-art accuracies [e.g.  7]. This suggests that a good way to learn linear models for
document classiﬁcation is to use discriminative models that borrow strength from an approximate
generative assumption to cut their generalization error. Our analysis presents an interesting contrast
to other work that directly combine generative and discriminative modeling by optimizing a hybrid
likelihood [20  21  22  23  24  25]. Our approach is more guarded in that we only let the generative
assumption speak through pseudo-examples.

Conclusion We have presented a theoretical analysis that explains how dropout training can be
very helpful under a Poisson topic model assumption. Speciﬁcally  by making training examples
artiﬁcially difﬁcult  dropout improves the exponent in the generalization bound for ERM. We believe
that this work is just the ﬁrst step in understanding the beneﬁts of training with artiﬁcially corrupted
features  and we hope the tools we have developed can be extended to analyze other training schemes
under weaker data-generating assumptions.

8

References
[1] Geoffrey E Hinton  Nitish Srivastava  Alex Krizhevsky  Ilya Sutskever  and Ruslan R Salakhutdinov.

Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580  2012.

[2] Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances in Neural

Information Processing Systems  2013.

[3] Ian J Goodfellow  David Warde-Farley  Mehdi Mirza  Aaron Courville  and Yoshua Bengio. Maxout

networks. In Proceedings of the International Conference on Machine Learning  2013.

[4] Alex Krizhevsky  Ilya Sutskever  and Geoff Hinton.

Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in Neural Information Processing Systems  2012.

[5] Li Wan  Matthew Zeiler  Sixin Zhang  Yann L Cun  and Rob Fergus. Regularization of neural networks

using dropconnect. In Proceedings of the International Conference on Machine Learning  2013.

[6] Stefan Wager  Sida Wang  and Percy Liang. Dropout training as adaptive regularization. In Advances in

Neural Information Processing Systems  2013.

[7] Sida I Wang and Christopher D Manning. Fast dropout training.

Conference on Machine Learning  2013.

In Proceedings of the International

[8] Sida I Wang  Mengqiu Wang  Stefan Wager  Percy Liang  and Christopher D Manning. Feature noising

for log-linear structured prediction. In Empirical Methods in Natural Language Processing  2013.

[9] Laurens van der Maaten  Minmin Chen  Stephen Tyree  and Kilian Q Weinberger. Learning with

marginalized corrupted features. In International Conference on Machine Learning  2013.

[10] Andrew Ng and Michael Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic

regression and naive Bayes. Advances in Neural Information Processing Systems  14  2001.
[11] David McAllester. A PAC-Bayesian tutorial with a dropout bound. arXiv:1307.2118  2013.
[12] Pierre Baldi and Peter Sadowski. The dropout learning algorithm. Artiﬁcial Intelligence  210:78–122 

2014.

[13] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Pro-

ceedings of the International Conference on Machine Learning  2006.

[14] Peter L Bartlett  Michael I Jordan  and Jon D McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[15] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk mini-

mization. Annals of Statistics  32(1):56–85  2004.

[16] David M Blei  Andrew Y Ng  and Michael I Jordan. Latent Dirichlet allocation. Journal of Machine

Learning Research  3:993–1022  2003.

[17] Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization

based on minimum cuts. In Proceedings of the Association for Computational Linguistics  2004.

[18] Andrew L Maas  Raymond E Daly  Peter T Pham  Dan Huang  Andrew Y Ng  and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the Association for Computational Lin-
guistics  2011.

[19] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple  good sentiment and topic clas-

siﬁcation. In Proceedings of the Association for Computational Linguistics  2012.

[20] R. Raina  Y. Shen  A. Ng  and A. McCallum. Classiﬁcation with hybrid generative/discriminative models.

In Advances in Neural Information Processing Systems  2004.

[21] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classiﬁers. In Interna-

tional Conference on Computational Statistics  2004.

[22] J. A. Lasserre  C. M. Bishop  and T. P. Minka. Principled hybrids of generative and discriminative models.

In Computer Vision and Pattern Recognition  2006.

[23] Guillaume Bouchard. Bias-variance tradeoff in hybrid generative-discriminative models. In International

Conference on Machine Learning and Applications. IEEE  2007.

[24] A. McCallum  C. Pal  G. Druck  and X. Wang. Multi-conditional learning: Generative/discriminative
training for clustering and classiﬁcation. In Association for the Advancement of Artiﬁcial Intelligence 
2006.

[25] Percy Liang and Michael I Jordan. An asymptotic analysis of generative  discriminative  and pseudolike-

lihood estimators. In Proceedings of the International Conference on Machine Learning  2008.

[26] Willliam Feller. An introduction to probability theory and its applications  volume 2. John Wiley & Sons 

1971.

[27] Olivier Bousquet  St´ephane Boucheron  and G´abor Lugosi. Introduction to statistical learning theory. In

Advanced Lectures on Machine Learning  pages 169–207. Springer  2004.

9

,Stefan Wager
William Fithian
Sida Wang
Percy Liang
El Mahdi El Mhamdi