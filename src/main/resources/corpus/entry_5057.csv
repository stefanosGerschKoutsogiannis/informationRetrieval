2012,Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models,Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics  i.e.  letting the variance of particular distributions in a graphical model go to zero. For instance  in the context of clustering  such an approach yields precise connections between the k-means and EM algorithms.  In this paper  we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences  we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems  including topic modeling  and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis.,Small-Variance Asymptotics for Exponential Family

Dirichlet Process Mixture Models

Ke Jiang  Brian Kulis
Department of CSE

The Ohio State University

{jiangk kulis}@cse.ohio-state.edu

Michael I. Jordan

Departments of EECS and Statistics
University of California at Berkeley
jordan@cs.berkeley.edu

Abstract

Sampling and variational inference techniques are two standard methods for in-
ference in probabilistic models  but for many problems  neither approach scales
effectively to large-scale data. An alternative is to relax the probabilistic model
into a non-probabilistic formulation which has a scalable associated algorithm.
This can often be fulﬁlled by performing small-variance asymptotics  i.e.  letting
the variance of particular distributions in the model go to zero. For instance  in
the context of clustering  such an approach yields connections between the k-
means and EM algorithms. In this paper  we explore small-variance asymptotics
for exponential family Dirichlet process (DP) and hierarchical Dirichlet process
(HDP) mixture models. Utilizing connections between exponential family distri-
butions and Bregman divergences  we derive novel clustering algorithms from the
asymptotic limit of the DP and HDP mixtures that features the scalability of exist-
ing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric
models. We focus on special cases of our analysis for discrete-data problems  in-
cluding topic modeling  and we demonstrate the utility of our results by applying
variants of our algorithms to problems arising in vision and document analysis.

Introduction

1
An enduring challenge for machine learning is in the development of algorithms that scale to truly
large data sets. While probabilistic approaches—particularly Bayesian models—are ﬂexible from
a modeling perspective  lack of scalable inference methods can limit applicability on some data.
For example  in clustering  algorithms such as k-means are often preferred in large-scale settings
over probabilistic approaches such as Gaussian mixtures or Dirichlet process (DP) mixtures  as the
k-means algorithm is easy to implement and scales to large data sets.
In some cases  links between probabilistic and non-probabilistic models can be made by applying
asymptotics to the variance (or covariance) of distributions within the model. For instance  con-
nections between probabilistic and standard PCA can be made by letting the covariance of the data
likelihood in probabilistic PCA tend toward zero [1  2]; similarly  the k-means algorithm may be
obtained as a limit of the EM algorithm when the covariances of the Gaussians corresponding to
each cluster goes to zero. Besides providing a conceptual link between seemingly quite different
approaches  small-variance asymptotics can yield useful alternatives to probabilistic models when
the data size becomes large  as the non-probabilistic models often exhibit more favorable scaling
properties. The use of such techniques to derive scalable algorithms from rich probabilistic models
is still emerging  but provides a promising approach to developing scalable learning algorithms.
This paper explores such small-variance asymptotics for clustering  focusing on the DP mixture.
Existing work has considered asymptotics over the Gaussian DP mixture [3]  leading to k-means-
like algorithms that do not ﬁx the number of clusters upfront. This approach  while an important
ﬁrst step  raises the question of whether we can perform similar asymptotics over distributions other

1

than the Gaussian. We answer in the afﬁrmative by showing how such asymptotics may be applied
to the exponential family distributions for DP mixtures; such analysis opens the door to a new class
of scalable clustering algorithms and utilizes connections between Bregman divergences and expo-
nential families. We further extend our approach to hierarchical nonparametric models (speciﬁcally 
the hierarchical Dirichlet process (HDP) [4])  and we view a major contribution of our analysis to
be the development of a general hard clustering algorithm for grouped data.
One of the primary advantages of generalizing beyond the Gaussian case is that it opens the door
to novel scalable algorithms for discrete-data problems. For instance  visual bag-of-words [5] have
become a standard representation for images in a variety of computer vision tasks  but many existing
probabilistic models in vision cannot scale to the size of data sets now commonly available. Simi-
larly  text document analysis models (e.g.  LDA [6]) are almost exclusively discrete-data problems.
Our analysis covers such problems; for instance  a particular special case of our analysis is a hard
version of HDP topic modeling. We demonstrate the utility of our methods by exploring applications
in text and vision.
Related Work: In the non-Bayesian setting  asymptotics for the expectation-maximization algo-
rithm for exponential family distributions were studied in [7]. The authors showed a connection be-
tween EM and a general k-means-like algorithm  where the squared Euclidean distance is replaced
by the Bregman divergence corresponding to exponential family distribution of interest. Our results
may be viewed as generalizing this approach to the Bayesian nonparametric setting. As discussed
above  our results may also be viewed as generalizing the approach of [3]  where the asymptotics
were performed for the DP mixture with a Gaussian likelihood  leading to a k-means-like algo-
rithm where the number of clusters is not ﬁxed upfront. Note that our setting is considerably more
involved than either of these previous works  particularly since we will require an appropriate tech-
nique for computing an asymptotic marginal likelihood. Other connections between hard clustering
and probabilistic models were explored in [8]  which proposes a “Bayesian k-means” algorithm by
performing a maximization-expectation algorithm.

2 Background
In this section  we brieﬂy review exponential family distributions  Bregman divergences  and the
Dirichlet process mixture model.

2.1 The Exponential Family
Consider the exponential family with natural parameter θ = {θj}d
family probability density function can be written as [9]:

p(x| θ) = exp(cid:0)(cid:104)x  θ(cid:105) − ψ(θ) − h(x)(cid:1) 

j=1 ∈ Rd; then the exponential

where ψ(θ) = log(cid:82) exp((cid:104)x  θ(cid:105) − h(x))dx is the log-partition function. Here we assume for

simplicity that x is a minimal sufﬁcient statistic for the natural parameter θ. ψ(θ) can be utilized to
compute the mean and covariance of p(x| θ); in particular  the expected value is given by ∇ψ(θ) 
and the covariance is ∇2ψ(θ).
Conjugate Priors: In a Bayesian setting  we will require a prior distribution over the natural pa-
rameter θ. A convenient property of the exponential family is that a conjugate prior distribution of
θ exists; in particular  given any speciﬁc distribution in the exponential family  the conjugate prior
can be parametrized as [11]:

p(θ | τ  η) = exp(cid:0)(cid:104)θ  τ(cid:105) − ηψ(θ) − m(τ  η)(cid:1).

Here  the ψ(·) function is the same as that of the likelihood function. Given a data point xi  the
posterior distribution of θ has the same form as the prior  with τ → τ + xi and η → η + 1.
Relationship to Bregman Divergences: Let φ : S → R be a differentiable  strictly convex function
deﬁned on a convex set S ⊆ Rd. The Bregman divergence for any pair of points x  y ∈ S is deﬁned
as Dφ(x  y) = φ(x)− φ(y)−(cid:104)x− y ∇φ(y)(cid:105)  and can be viewed as a generalized distortion mea-
sure. An important result connecting Bregman divergences and exponential families was discussed
in [7] (see also [10  11])  where a bijection between the two was established. A key consequence
of this result is that we can equivalently parameterize both p(x| θ) and p(θ | τ  η) in terms of the

2

expectation µ:

p(x| θ) = p(x| µ) = exp(−Dφ(x  µ))fφ(x) 
p(θ | τ  η) = p(µ| τ  η) = exp

− ηDφ

  µ

(cid:18)

(cid:19)(cid:19)

(cid:18) τ

η

gφ(τ  η) 

where φ(·) is the Legendre-conjugate function of ψ(·) (denoted as φ = ψ∗)  fφ(x) = exp(φ(x) −
h(x))  and µ is the expectation parameter which satisﬁes µ = ∇ψ(θ) (and also µ = θ∗). The
Bregman divergence representation provides a natural way to parametrize the exponential family
distributions with its expectation parameter and  as we will see  we will ﬁnd it convenient to work
with this form.

2.2 Dirichlet Process Mixture Models
The Dirichlet Process (DP) mixture model is a Bayesian nonparametric mixture model [12]; unlike
most parametric mixture models (Bayesian or otherwise)  the number of clusters in a DP mixture is
not ﬁxed upfront. Using the exponential family parameterized by the expectation µc  the likelihood
for a data point can be expressed as the following inﬁnite mixture:

∞(cid:88)

∞(cid:88)

p(x) =

πcp(x| µc) =

πc exp(−Dφ(x  µc))fφ(x).

c=1

c=1

Even though there are conceptually an inﬁnite number of clusters  the nonparametric prior over the
mixing weights causes the weights πc to decay exponentially. Moreover  a simple collapsed Gibbs
sampler can be employed for performing inference in this model [13]; this Gibbs sampler will form
the basis of our asymptotic analysis. Given a data set {x1  ...  xn}  the state of the Markov chain
is the set of cluster indicators {z1  ...  zn} as well as the cluster means of the currently-occupied
clusters (the mixing weights have been integrated out). The Gibbs updates for zi  (i = 1  . . .   n) 
are given by the following conditional probabilities:

P (zi = c| z−i  xi  µ) =
P (zi = cnew | z−i  xi  µ) =

n−i c

Z(n − 1 + α)

α

Z(n − 1 + α)

p(xi | µc)

(cid:90)

p(xi | µ)dG0 

where Z is the normalizing constant  n−i c is the number of data points (excluding xi) that are
currently assigned to cluster c  G0 is a prior over µ  and α is the concentration parameter that
determines how likely we are to start a new cluster. If we choose to start a new cluster during the
Gibbs update  we sample its mean from the posterior distribution obtained from the prior distribution
G0 and the single observation xi. After performing Gibbs moves on the cluster indicators  we update
the cluster means µc by sampling from the posterior of µc given the data points assigned to cluster c.

3 Hard Clustering for Exponential Family DP Mixtures
Our goal is to analyze what happens as we perform small-variance asymptotics on the exponential
family DP mixture when running the collapsed Gibbs sampler described earlier  and we begin by
considering how to scale the covariance in an exponential family distribution. Given an exponential
family distribution p(x| θ) with natural parameter θ and log-partition function ψ(θ)  consider a
scaled exponential family distribution whose natural parameter is ˜θ = βθ and log-partition function
is ˜ψ( ˜θ) = βψ( ˜θ/β)  where β > 0. The following result characterizes the relationship between the
mean and covariance of the original and scaled exponential family distributions.
Lemma 3.1. Denote µ(θ) as the mean  and cov(θ) as the covariance  of p(x| θ) with log-partition
ψ(θ). Given a scaled exponential family with ˜θ = βθ and ˜ψ( ˜θ) = βψ( ˜θ/β)  the mean ˜µ( ˜θ) of the
scaled distribution is µ(θ) and the covariance  ˜cov( ˜θ)  is cov(θ)/β.
This lemma follows directly from ˜µ( ˜θ) = ∇˜θ
˜ψ( ˜θ) = β∇˜θψ( ˜θ/β) = ∇θψ( ˜θ/β) = ∇θψ(θ) =
µ(θ)  and ˜cov( ˜θ) = ∇2
θψ(θ) =
˜θ
cov(θ)/β. It is perhaps intuitively simpler to observe what happens to the distribution using the

( ˜ψ( ˜θ)) = β∇˜θ(∇˜θψ( ˜θ/β)) = 1

θψ( ˜θ/β) = 1

β × ∇2

β × ∇2

3

Bregman divergence representation. Recall that the generating function φ for the Bregman diver-
gence is given by the Legendre-conjugate of ψ. Using standard properties of convex conjugates  we
see that the conjugate of ˜ψ is simply ˜φ = βφ. The Bregman divergence representation for the scaled
distribution is given by

p(x| ˜θ) = p(x| ˜µ) = exp(−D ˜φ(x  ˜µ))f ˜φ(x) = exp(−βDφ(x  µ))fβφ(x) 

where the last equality follows from Lemma 3.1 and the fact that  for a Bregman divergence 
Dβφ(· ·) = βDφ(· ·). Thus  as β increases under the above scaling  the mean is ﬁxed while the
distribution becomes increasingly concentrated around the mean.
Next we consider the prior distribution under the scaled exponential family. When scaling by β  we
also need to scale the hyper-parameters τ and η  namely τ → τ /β and η → η/β. This gives the
following prior written using the Bregman divergence  where we are now explicitly conditioning on
β:
p( ˜θ | τ  η  β) = exp

(cid:18) τ /β

(cid:19)(cid:19)

(cid:19)(cid:19)

(cid:18) τ

(cid:18) τ

(cid:18) τ

(cid:18)

(cid:19)

(cid:19)

(cid:18)

= exp

− ηDφ

  µ

  µ

 

.

 

− η
β

D ˜φ

η/β

g ˜φ

β

η
β

g ˜φ

β

η
β

η

Finally  we compute the marginal likelihood for x by integrating out ˜θ  as it will be necessary for
the Gibbs sampler. Standard algebraic manipulations yield the following:
p(x| τ  η  β) =

p(x| ˜θ) × p( ˜θ | τ  η  β)d ˜θ

(cid:90)

(cid:18) βx + τ

(cid:19)(cid:19)

exp

d ˜θ

β + η

  ˜µ( ˜θ)

− (β + η)Dφ

− (β + η)Dφ

(cid:18) βx + τ

(cid:19)(cid:19)
(cid:18)
β+η ))(cid:1)  which arises when combining

  µ(θ)

β + η

(1)

dθ.

(cid:18) τ
(cid:18) τ

β

(cid:19)
(cid:19)

 

 

η
β
η
β

(cid:90)

(cid:18)
(cid:90)

exp

A( ˜φ τ η β)(x)
A( ˜φ τ η β)(x) · βd ·

= f ˜φ(x) · g ˜φ
= f ˜φ(x) · g ˜φ

Here  A( ˜φ τ η β)(x) = exp(cid:0)− (βφ(x) + ηφ( τ

β

η )− (β + η)φ( βx+τ

=

  ˆµ

I = exp

β+η   ˆµ)

(cid:18) 1

β

(cid:12)(cid:12)(cid:12)(cid:12)−1/2

(cid:18)
(cid:18) 2π

the Bregman divergences from the likelihood and the prior.
Now we make the following key insight  which will allow us to perform the necessary asymptotics.
We can write the integral from the last line above (denoted I below) via Laplace’s method. Since
(cid:19)
β+η )∗  we have:
Dφ( βx+τ

β+η   µ) has a local minimum (which is global in this case) at ˆθ = ˆµ∗ = ( βx+τ

− (β + η)Dφ

(cid:18) βx + τ
(cid:19)d/2(cid:12)(cid:12)(cid:12)(cid:12) ∂2Dφ( βx+τ

β + η
β+η   ˆµ)

∂θ∂θT

(cid:19)d/2(cid:12)(cid:12)(cid:12)(cid:12) ∂2Dφ( βx+τ
(cid:19)(cid:19)(cid:18) 2π
(cid:12)(cid:12)(cid:12)(cid:12)−1/2
(cid:18) 1
(cid:19)

∂θ∂θT

β + η

+ O

β

∂θ∂θT

β+η   ˆµ)

β + η
where ∂2Dφ( βx+τ
= cov( ˆθ) is the covariance matrix of the likelihood function instantiated at ˆθ
and approaches cov(x∗) when β goes to ∞. Note that the exponential term equals one since the
divergence inside is 0.
3.1 Asymptotic Behavior of the Gibbs Sampler
We now have the tools to consider the Gibbs sampler for the exponential family DP mixture as we
let β → ∞. As we will see  we will obtain a general k-means-like hard clustering algorithm which
utilizes the appropriate Bregman divergence in place of the squared Euclidean distance  and also can
vary the number of clusters. Recall the conditional probabilities for performing Gibbs moves on the
cluster indicators zi  where we now are considering the scaled distributions:

+ O

(2)

P (zi = c| z−i  xi  β  µ) =
P (zi = cnew | z−i  xi  β  µ) =

n−i c

exp(−βDφ(xi  µc))f ˜φ(xi)

Z
p(xi | τ  η  β) 

α
Z

where Z is a normalization factor  and the marginal probability p(xi | τ  η  β) is given by the deriva-
tions in (1) and (2). Now  we consider the asymptotic behavior of these probabilities as β → ∞. We

4

note that

lim
β→∞

βxi + τ
β + η

= xi 

and

and that the Laplace approximation error term goes to zero as β → ∞. Further  we deﬁne α as a
function of β  η  and τ (but independent of the data):

lim

β→∞ A( ˜φ τ η β)(xi) = exp(−η(φ(τ /η) − φ(xi))) 
(cid:18) 2π
(cid:19)

(cid:19)−1 · exp(−βλ) 

(cid:19)d/2 · βd

·

β + η

(cid:18)

(cid:18) τ

α =

g ˜φ

 

η
β

β

for some λ. After canceling out the f ˜φ(xi) terms from all probabilities  we can then write the Gibbs
probabilities as

P (zi = c| z−i  xi  β  µ) =

P (zi = cnew | z−i  xi  β  µ) =

Cxi · exp(−βλ) +(cid:80)k
Cxi · exp(−βλ) +(cid:80)k

n−i c · exp(−βDφ(xi  µc))

j=1 n−i j · exp(−βDφ(xi  µj))

Cxi · exp(−βλ)

j=1 n−i j · exp(−βDφ(xi  µj))

 

where Cxi approaches a positive  ﬁnite constant for a given xi as β → ∞. Now  all of the above
probabilities will become binary as β → ∞. More speciﬁcally  all the k + 1 values will be in-
creasingly dominated by the smallest value of {Dφ(xi  µ1)  . . .   Dφ(xi  µk)  λ}. As β → ∞  only
the smallest of these values will receive a non-zero probability. That is  the data point xi will be
assigned to the nearest cluster with a divergence at most λ. If the closest mean has a divergence
greater than λ  we start a new cluster containing only xi.
Next  we show that sampling µc from the posterior distribution is achieved by simply computing
the empirical mean of a cluster in the limit. During Gibbs sampling  once we have performed one
complete set of Gibbs moves on the cluster assignments  we need to sample the µc conditioned on
all assignments and observations. If we let nc be the number of points assigned to cluster c  then the
posterior distribution (parameterized by the expectation parameter) for cluster c is
p(µc | X  z  τ  η  β) ∝ p(Xc | µc  β)×p(µc | τ  η  β) ∝ exp
} is the set of points currently assigned to cluster c  and z
where X is all the data  Xc = {xc
is the set of all current assignments. We can see that the mass of the posterior distribution becomes
as β → ∞. In other words  after we determine the
concentrated around the sample mean
assignments of data points to clusters  we update the means as the sample mean of the data points in
each cluster. This is equivalent to the standard k-means cluster mean update step.
3.2 Objective function and algorithm
From the above asymptotic analysis of the Gibbs sampler  we observe a new algorithm which can
be utilized for hard clustering. It is as simple as the popular k-means algorithm  but also provides
the ability to adapt the number of clusters depending on the data as well as incorporate different
distortion measures. The algorithm description is as follows:

1  ...  xc
(cid:80)nc
nc

−(βnc+η)Dφ

(cid:18)(cid:80)nc

i=1 βxc
βnc + η

i=1 xi
nc

(cid:18)

i + τ

  µ

(cid:19)(cid:19)

 

• Initialization: input data x1  . . .   xn  λ > 0  and µ1 = 1
• Assignment: for each data point xi  compute the Bregman divergence Dφ(xi  µc) to all
existing clusters. If minc Dφ(xi  µc) ≤ λ  then zi c0 = 1 where c0 = argmincDφ(xi  µc);
otherwise  start a new cluster and set zi cnew = 1;
• Mean Update: compute the cluster mean for each cluster  µj = 1|lj|
the set of points in the j-th cluster.

x  where lj is

(cid:80)

i=1 xn

x∈lj

n

(cid:80)n

We iterate between the assignment and mean update steps until local convergence. Note that the
initialization used here—placing all data points into a single cluster—is not necessary  but is one
natural way to initialize the algorithm. Also note that the algorithm depends heavily on the choice
of λ; heuristics for selecting λ were brieﬂy discussed for the Gaussian case in [3]  and we will follow
this approach (generalized in the obvious way to Bregman divergences) for our experiments.

5

We can easily show that the underlying objective function for our algorithm is quite similar to that
in [3]  replacing the squared Euclidean distance with an appropriate Bregman divergence. Recall
that the squared Euclidean distance is the Bregman divergence corresponding to the Gaussian distri-
bution. Thus  the objective function in [3] can be seen as a special case of our work. The objective
function optimized by our derived algorithm is the following:

k(cid:88)

(cid:88)

c=1

x∈lc

min
{lc}k

c=1

Dφ(x  µc) + λk

(3)

where k is the total number of clusters  φ is the conjugate function of the log-partition function of
the chosen exponential family distribution  and µc is the sample mean of cluster c. The penalty term
λ controls the tradeoff between the likelihood and the model complexity  where a large λ favors
small model complexity (i.e.  fewer clusters) while a small λ favors more clusters. Given the above
objective function  our algorithm can be shown to monotonically decrease the objective function
value until convergence to some local minima. We omit the proof here as it is almost identical as the
proof for Theorem 3.1 in [3].

4 Extension to Hierarchies
A key beneﬁt of the Bayesian approach is its natural ability to form hierarchical models. In the con-
text of clustering  a hierarchical mixture allows one to cluster multiple groups of data—each group
is clustered into a set of local clusters  but these local clusters are shared among the groups (i.e. 
sets of local clusters across groups form global clusters  with a shared global mean). For Bayesian
nonparametric mixture models  one way of achieving such hierarchies arises via the hierarchical
Dirichlet Process (HDP) [4]  which provides a nonparametric approach to allow sharing of clusters
among a set of DP mixtures.
In this section  we will brieﬂy sketch out the extension of our analysis to the HDP mixture  which
yields a natural extension of our methods to groups of data. Given space considerations  and the fact
that the resulting algorithm turns out to reduce to Algorithm 2 from [3] with the squared Euclidean
distance replaced by an appropriate Bregman divergence  we will omit the full speciﬁcation of the
algorithm here. However  despite the similarity to the existing Gaussian case  we do view the ex-
tension to hierarchies as a promising application of our analysis. In particular  our approach opens
the door to hard hierarchical algorithms over discrete data  such as text  and we brieﬂy discuss an
application of our derived algorithm to topic modeling.
We assume that there are J data sets (groups) which we index by j = 1  ...  J. Data point xij
refers to data point i from set j. The HDP model can be viewed as clustering each data set into
local clusters  but where each local cluster is associated to a global mean. Global means may be
shared across data sets. When performing the asymptotics  we require variables for the global means
(µ1  ...  µg)  the associations of data points to local clusters  zij  and the associations of local clusters
to global means  vjt  where t indexes the local clusters for a data set. A standard Gibbs sampler
considers updates on all of these variables  and in the nonparametric setting does not ﬁx the number
of local or global clusters.
The tools from the previous section may be nearly directly applied to the hierarchical case. As
opposed to the ﬂat model  the hard HDP requires two parameters: a value λtop which is utilized
when starting a global (top-level) cluster  and a value λbottom which is utilized when starting a local
cluster. The resulting hard clustering algorithm ﬁrst performs local assignment moves on the zij 
then updates the local cluster assignments  and ﬁnally updates all global means.
The resulting objective function that is monotonically minimized by our algorithm is given as fol-
lows:

k(cid:88)

(cid:88)

c=1

xij∈lc

min
{lc}k

c=1

Dφ(xij  µc) + λbottomt + λtopk 

(4)

where k is the total number of global clusters and t is the total number of local clusters. The bottom-
level penalty term λbottom controls both the number of local and top-level clusters  where larger
λbottom tends to give fewer local clusters and more top-level clusters. Meanwhile  the top-level
penalty term λtop  as in the one-level case  controls the tradeoff between the likelihood and model
complexity.

6

Figure 1: (Left) Example images from the ImageNet data (Persian cat and elephant categories). Each
image is represented via a discrete visual-bag-of-words histogram. Clustering via an asymptotic
multinomial DP mixture considerably outperforms the asymptotic Gaussian DP mixture; see text
for details. (Right) Elapsed time per iteration in seconds of our topic modeling algorithm when
running on the NIPS data  as a function of the number of topics.

5 Experiments
We conclude with a brief set of experiments highlighting applications of our analysis to discrete-data
problems  namely image clustering and topic modeling. For all experiments  we randomly permute
the data points at each iteration  as this tends to improve results (as discussed previously  unlike
standard k-means  the order in which the data points are processed impacts the resulting clusters).

Image Clustering. We ﬁrst explore an application of our techniques to image clustering  focusing
on the ImageNet data [14]. We utilize a subset of this data for quantitative experiments  sampling
100 images from 10 different categories of this data set (Persian cat  African elephant  ﬁre engine 
motor scooter  wheelchair  park bench  cello  French horn  television  and goblet)  for a total of 1000
images. Each image is processed via standard visual-bag-of-words: SIFT is densely applied on top
of image patches in image  and the resulting SIFT vectors are quantized into 1000 visual words.
We use the resulting histograms as our discrete representation for an image  as is standard. Some
example images from this data set are shown in Figure 1.
We explore whether the discrete version of our hard clustering algorithm based on a multinomial
DP mixture outperforms the Gaussian mixture version (i.e.  DP-means); this will validate our gen-
eralization beyond the Gaussian setting. For both the Gaussian and multinomial cases  we utilize a
farthest-ﬁrst approach for both selecting λ as well as initializing the clusters (see [3] for a discussion
of farthest-ﬁrst for selecting λ).
We compute the normalized mutual information (NMI) between the true clusters and the results of
the two algorithms on this difﬁcult data set. The Gaussian version performs poorly  achieving an
NMI of .06 on this data  whereas the hard multinomial version achieves a score of .27. While the
multinomial version is far from perfect  it performs signiﬁcantly better than DP-means. Scalability
to large data sets is clearly feasible  given that the method scales linearly in the number of data
points. Note that comparisons between the Gibbs sampler and the corresponding hard clustering
algorithm for the Gaussian case were considered in [3]  where experiments on several data sets
showed comparable clustering accuracy results between the sampler and the hard clustering method.
Furthermore  for a fully Bayesian model that places a prior on the concentration parameter  the
sampler was shown to be considerably slower than the corresponding hard clustering method. Given
the similarity of the sampler for the Gaussian and multinomial case  we expect similar behavior
with the multinomial Gibbs sampler.

Illustration: Scalable Hard Topic Models. We also highlight an application to topic modeling 
by providing some qualitative results over two common document collections. Utilizing our general
algorithm for a hard version of the multinomial HDP is straightforward. In order to apply the hard
hierarchical algorithm to topic modeling  we simply utilize the discrete KL-divergence in the hard
exponential family HDP  since topic modeling for text uses a multinomial distribution for the data
likelihood.
To test topic modeling using our asymptotic approach  we performed analyses using the NIPS 1-121
and the NYTimes [15] datasets. For the NIPS dataset  we use the whole dataset  which contains
1740 total documents  13649 words in the vocabulary  and 2 301 375 total words. For the NYTimes

1http://www.cs.nyu.edu/ roweis/data.html

7

1

2

3

4

5

6

NIPS
neurons  memory  patterns  activity  re-
sponse  neuron  stimulus  ﬁring  cortex  re-
current  pattern  spike  stimuli  delay  re-
sponses
neural  networks  state  weight  states  re-
sults  synaptic  threshold  large  time  sys-
tems  activation  small  work  weights
training  hidden  recognition  layer  per-
formance  probability  parameter  error 
speech  class  weights  trained  algorithm 
approach  order
cells  visual  cell  orientation  cortical  con-
nection  receptive  ﬁeld  center 
tuning 
low  ocular  present  dominance  ﬁelds
energy  solution  methods  function  solu-
tions  local  equations  minimum  hopﬁeld 
temperature  adaptation 
term  optimiza-
tion  computational  procedure
noise  classiﬁer  classiﬁers  note  margin 
noisy  regularization  generalization  hy-
pothesis  multiclasses  prior  cases  boost-
ing  ﬁg  pattern

NYTimes
team  game  season  play  games  point 
player  coach  win  won  guy  played  play-
ing  record  ﬁnal

percent  campaign  money  fund  quarter 
federal  public  pay  cost  according  in-
come  half  term  program  increase
president  power  government  country 
peace  trial  public  reform  patriot  eco-
nomic  past  clear 
interview  religious 
early
family  father  room  line  shares  recount 
told  mother  friend  speech  expression 
won  offer  card  real
company  companies  stock  market  busi-
ness  billion  ﬁrm  computer  analyst  in-
dustry 
technology  cus-
tomer  number
right  human  decision  need  leadership 
foundation  number  question  country 
strike  set  called  support  law  train

internet  chief 

Table 1: Sample topics inferred from the NIPS and NYTimes datasets by our hard multinomial HDP
algorithm.

dataset  we randomly sampled 2971 documents with 10171 vocabulary words  and 853 451 words in
total; we also eliminated low-frequency words (those with less than ten occurrences). The prevailing
metric to measure the goodness of topic models is perplexity; however  this is based on the predictive
probability  which has no counterpart in the hard clustering case. Furthermore  ground truth for topic
models is difﬁcult to obtain. This makes quantitative comparisons difﬁcult for topic modeling  and
so we therefore focus on qualitative results. Some sample topics (with the corresponding top 15
terms) discovered by our approach from both the NIPS and NYTimes datasets are given in Table 1;
we can see that the topics appear to be quite reasonable. Also  we highlight the scalability of our
approach: the number of iterations needed for convergence on these data sets ranges from 13 to 25 
and each iteration completes in under one minute (see the right side of Figure 1). In contrast  for
sampling methods  it is notoriously difﬁcult to detect convergence  and generally a large number of
iterations is required. Thus  we expect this approach to scale favorably to large data sets.

6 Conclusion
We considered a general small-variance asymptotic analysis for the exponential family DP and
HDP mixture model. Crucially  this analysis allows us to move beyond the Gaussian distribution
in such models  and opens the door to new clustering applications  such as those involving discrete
data. Our analysis utilizes connections between Bregman divergences and exponential families 
and results in a simple and scalable hard clustering algorithm which may be viewed as generalizing
existing non-Bayesian Bregman clustering algorithms [7] as well as the DP-means algorithm [3].
Due to the prevalence of discrete data in modern computer vision and information retrieval  we
hope our algorithms will ﬁnd use for a variety of large-scale data analysis tasks. We plan to
continue to focus on the difﬁcult problem of quantitative evaluations comparing probabilistic and
non-probabilistic methods for clustering  particularly for topic models. We also plan to compare
our algorithms with recent online inference schemes for topic modeling  particularly the online
LDA [16] and online HDP [17] algorithms.

Acknowledgements. This work was supported by NSF award IIS-1217433 and by the ONR under
grant number N00014-11-1-0688.

8

References
[1] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the

Royal Statistical Society  Series B  21(3):611–622  1999.

[2] S. Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing

Systems  1998.

[3] B. Kulis and M. I. Jordan. Revisiting k-means: New algorithms via Bayesian nonparametrics.

In Proceedings of the 29th International Conference on Machine Learning  2012.

[4] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal

of the American Statistical Association  101(476):1566–1581  2006.

[5] L. Fei-Fei and P. Perona. A Bayesian hierarchical model for learning natural scene categories.

In IEEE Conference on Computer Vision and Patterns Recognition  2005.

[6] D. Blei  A. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[7] A. Banerjee  S. Merugu  I. S. Dhillon  and J. Ghosh. Clustering with Bregman divergences.

Journal of Machine Learning Research  6:1705–1749  2005.

[8] K. Kurihara and M. Welling. Bayesian k-means as a “Maximization-Expectation” algorithm.

Neural Computation  21(4):1145–1172  2008.

[9] O. Barndorff-Nielsen.

Publishers  1978.

Information and Exponential Families in Statistical Theory. Wiley

[10] J. Forster and M. K. Warmuth. Relative expected instantaneous loss bounds. In Proceedings

of 13th Conference on Computational Learning Theory  2000.

[11] A. Agarwal and H. Daume. A geometric view of conjugate priors. Machine Learning 

81(1):99–113  2010.

[12] N. Hjort  C. Holmes  P. Mueller  and S. Walker. Bayesian Nonparametrics: Principles and

Practice. Cambridge University Press  Cambridge  UK  2010.

[13] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of

Computational and Graphical Statistics  9:249–265  2000.

[14] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. ImageNet: A large-scale hier-
archical image database. In IEEE Conference on Computer Vision and Patterns Recognition 
2009.

[15] A. Frank and A. Asuncion. UCI Machine Learning Repository  2010.
[16] M. D. Hoffman  D. M. Blei  and F. Bach. Online learning for Latent Dirichlet Allocation. In

Advances in Neural Information Processing Systems  2010.

[17] C. Wang  J. Paisley  and D. M. Blei. Online variational inference for the hierarchical Dirichlet
In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and

process.
Statistics  2011.

9

,Si Si
Donghyuk Shin
Inderjit Dhillon
Beresford Parlett
Gustavo Malkomes
Charles Schaff
Roman Garnett