2017,Universal consistency and minimax rates for online Mondrian Forests,We establish the consistency of an algorithm of Mondrian Forests~\cite{lakshminarayanan2014mondrianforests lakshminarayanan2016mondrianuncertainty}  a randomized classification algorithm that can be implemented online.  First  we amend the original Mondrian Forest algorithm proposed in~\cite{lakshminarayanan2014mondrianforests}  that considers a \emph{fixed} lifetime parameter.  Indeed  the fact that this parameter is fixed actually hinders statistical consistency of the original procedure.  Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters $\lambda_n$  and uses an alternative updating rule  allowing to work also in an online fashion.   Second  we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function  which is a strong extension of previous results~\cite{arlot2014purf_bias} to an \emph{arbitrary dimension}.,Universal consistency and minimax rates for online

Mondrian Forests

Jaouad Mourtada

Centre de Mathématiques Appliquées
École Polytechnique  Palaiseau  France

jaouad.mourtada@polytechnique.edu

Stéphane Gaïffas

Centre de Mathématiques Appliquées
École Polytechnique Palaiseau  France

stéphane.gaiffas@polytechnique.edu

Erwan Scornet

Centre de Mathématiques Appliquées
École Polytechnique Palaiseau  France
erwan.scornet@polytechnique.edu

Abstract

We establish the consistency of an algorithm of Mondrian Forests [LRT14  LRT16] 
a randomized classiﬁcation algorithm that can be implemented online. First  we
amend the original Mondrian Forest algorithm proposed in [LRT14]  that considers
a ﬁxed lifetime parameter. Indeed  the fact that this parameter is ﬁxed hinders
the statistical consistency of the original procedure. Our modiﬁed Mondrian
Forest algorithm grows trees with increasing lifetime parameters λn  and uses
an alternative updating rule  allowing to work also in an online fashion. Second 
we provide a theoretical analysis establishing simple conditions for consistency.
Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the
minimax rate (optimal rate) for the estimation of a Lipschitz regression function 
which is a strong extension of previous results [AG14] to an arbitrary dimension.

1

Introduction

Random Forests (RF) are state-of-the-art classiﬁcation and regression algorithms that proceed by
averaging the forecasts of a number of randomized decision trees grown in parallel (see [Bre01  Bre04 
GEW06  BDL08  Bia12  BS16  DMdF14  SBV15]). Despite their widespread use and remarkable
success in practical applications  the theoretical properties of such algorithms are still not fully
understood [Bia12  DMdF14]. Among these methods  purely random forests [Bre00  BDL08  Gen12 
AG14] that grow the individual trees independently of the sample  are particularly amenable to
theoretical analysis; the consistency of such classiﬁers was obtained in [BDL08].
An important limitation of the most commonly used random forests algorithms  such as Breiman’s
Random Forest [Bre01] and the Extra-Trees algorithm [GEW06]  is that they are typically trained in
a batch manner  using the whole dataset to build the trees. In order to enable their use in situations
when large amounts of data have to be incorporated in a streaming fashion  several online adaptations
of the decision trees and RF algorithms have been proposed [DH00  TGP11  SLS+09  DMdF13].
Of particular interest in this article is the Mondrian Forest algorithm  an efﬁcient and accurate online
random forest classiﬁer [LRT14]. This algorithm is based on the Mondrian process [RT09  Roy11]  a
natural probability distribution on the set of recursive partitions of the unit cube [0  1]d. An appealing
property of Mondrian processes is that they can be updated in an online fashion: in [LRT14]  the use
of the conditional Mondrian process enabled to design an online algorithm that matched its batch
counterpart. While Mondrian Forest offer several advantages  both computational and in terms of

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

predictive performance  the algorithm proposed in [LRT14] depends on a ﬁxed lifetime parameter λ
that guides the complexity of the trees. Since this parameter has to be set in advance  the resulting
algorithm is inconsistent  as the complexity of the randomized trees remains bounded. Furthermore 
an analysis of the learning properties of Mondrian Forest – and in particular of the inﬂuence and
proper theoretical tuning of the lifetime parameter λ – is still lacking.
In this paper  we propose a modiﬁed online random forest algorithm based on Mondrian processes.
Our algorithm retains the crucial property of the original method [LRT14] that the decision trees
can be updated incrementally. However  contrary to the original approach  our algorithm uses an
increasing sequence of lifetime parameters (λn)n(cid:62)1  so that the corresponding trees are increasingly
complex  and involves an alternative online updating algorithm. We study such classiﬁcation rules
theoretically  establishing simple conditions on the sequence (λn)n(cid:62)1 to achieve consistency  see
Theorem 1 from Section 5 below.
In fact  Mondrian Forests achieve much more than what they were designed for: while they were
primarily introduced to derive an online algorithm  we show in Theorem 2 (Section 6) that they
actually achieve minimax convergence rates for Lipschitz conditional probability (or regression)
functions in arbitrary dimension. To the best of our knowledge  such results have only been proved
for very speciﬁc purely random forests  where the covariate dimension is equal to one.

Related work. While random forests were introduced in the early 2000s [Bre01]  as noted
by [DMdF14] the theoretical analysis of these methods is outpaced by their practical use. The consis-
tency of various simpliﬁed random forests algorithms is ﬁrst established in [BDL08]  as a byproduct
of the consistency of individual tree classiﬁers. A recent line of research [Bia12  DMdF14  SBV15]
has sought to obtain theoretical guarantees (i.e. consistency) for random forests variants that more
closely resembled the algorithms used in practice. Another aspect of the theoretical study of random
forests is the bias-variance analysis of simpliﬁed versions of random forests [Gen12  AG14]  such
as the purely random forests (PRF) model that performs splits independently of the data. In particu-
lar  [Gen12] shows that some PRF variants achieve the minimax rate for the estimation of a Lipschitz
regression functions in dimension 1. Additionally  the bias-variance analysis is extended in [AG14] 
showing that PRF can also achieve minimax rates for C 2 regression functions in dimension one  and
considering higher dimensional models of PRF that achieve suboptimal rates.
Starting with [SLS+09]  online variants of the random forests algorithm have been considered.
In [DMdF13]  the authors propose an online random forest algorithm and prove its consistency. The
procedure relies on a partitioning of the data into two streams: a structure stream (used to grow the
tree structure) and an estimation stream (used to compute the prediction in each leaf). This separation
of the data into separate streams is a way of simplifying the proof of consistency  but leads to a
non-realistic setting in practice.
A major development in the design of online random forests is the introduction of the Mondrian
Forest (MF) classiﬁer [LRT14  LRT16]. This algorithm makes an elegant use of the Mondrian
Process  introduced in [RT09]  see also [Roy11  OR15]  to draw random trees. Indeed  this process
provides a very convenient probability distribution over the set of recursive  tree-based partitions of
the hypercube. In [BLG+16]  the links between the Mondrian process and the Laplace kernel are
used to design random features in order to efﬁciently approximate kernel ridge regression  leading to
the so-called Mondrian kernel algorithm.
Our approach differs from the original Mondrian Forest algorithm [LRT14]  since it introduces a
“dual” construction  that works in the “time” domain (lifetime parameters) instead of the “space”
domain (features range). Indeed  in [LRT14]  the splits are selected using a Mondrian process on the
range of previously observed features vectors  and the online updating of the trees is enabled by the
possibility of extending a Mondrian process to a larger cell using conditional Mondrian processes.
Our algorithm incrementally grows the trees by extending the lifetime; the online update of the trees
exploits the Markov property of the Mondrian process  a consequence of its formulation in terms of
competing exponential clocks.

2 Setting and notation

We ﬁrst explain the considered setting allowing to state consistency of our procedure  and we describe
and set notation for the main concepts used in the paper  namely trees  forests and partitions.

2

Considered setting. Assume we are given an i.i.d. sequence (X1  Y1)  (X2  Y2) . . . of [0  1]d ×
{0  1}-valued random variables that come sequentially  such that each (Xi  Yi) has the same distribu-
tion as (X  Y ). This unknown distribution is characterized by the distribution µ of X on [0  1]d and
the conditional probability η(x) = P(Y = 1| X = x).
At each time step n (cid:62) 1  we want to output a 0-1-valued randomized classiﬁcation rule gn(·  Z  Dn) :

[0  1]d → {0  1}  where Dn =(cid:0)(X1  Y1)  . . .   (Xn  Yn)(cid:1) and Z is a random variable that accounts for
the randomization procedure; to simplify notation  we will generally denote(cid:98)gn(x  Z) = gn(x  Z  Dn).

The quality of a randomized classiﬁer gn is measured by its probability of error

L(gn) = P(gn(X  Z  Dn) (cid:54)= Y | Dn) = P(X Y ) Z(gn(X  Z  Dn) (cid:54)= Y )

(1)
where P(X Y ) Z denotes the integration with respect to (X  Y )  Z alone. The quantity of Equation (1)
is minimized by the Bayes classiﬁer g∗(x) = 1{η(x)> 1
2}  and its loss  the Bayes error  is denoted
L∗ = L(g∗). We say that a sequence of classiﬁcation rules (gn)n(cid:62)1 is consistent whenever L(gn) →
L∗ in probability as n → ∞.
Remark 1. We restrict ourselves to binary classiﬁcation  note however that our results and proofs can
be extended to multi-class classiﬁcation.

Trees and Forests. The classiﬁcation rules (gn)n(cid:62)1 we consider take the form of a random forest 
deﬁned by averaging randomized tree classiﬁers. More precisely  let K (cid:62) 1 be a ﬁxed number of

randomized classiﬁers(cid:98)gn(x  Z1)  . . .  (cid:98)gn(x  ZK) associated to the same randomized mechanism 
where the Zk are i.i.d. Set Z (K) = (Z1  . . .   ZK). The averaging classiﬁer(cid:98)g(K)

by taking the majority vote among the values gn(x  Zk)  k = 1  . . .   K.
Our individual randomized classiﬁers are decision trees. A decision tree (T  Σ) is composed of the
following components:

n (x  Z (K)) is deﬁned

• A ﬁnite rooted ordered binary tree T   with nodes N (T )  interior nodes N ◦(T ) and leaves
L(T ) (so that N (T ) is the disjoint union of N ◦(T ) and L(T )). Each interior node η has a
left child left(η) and a right child right(η);
• A family of splits Σ = (ση)η∈N ◦(T ) at each interior node  where each split ση = (dη  νη)
is characterized by its split dimension dη ∈ {1  . . .   d} and its threshold νη.

Each randomized classiﬁer(cid:98)gn(x  Zk) relies on a decision tree T   the random variable Zk is the

random sampling of the splits (ση) deﬁning T . This sampling mechanism  based on the Mondrian
process  is deﬁned in Section 3.
We associate to M = (T  Σ) a partition (Aφ)φ∈L(T ) of the unit cube [0  1]d  called a tree partition (or
guillotine partition). For each node η ∈ N (T )  we deﬁne a hyper-rectangular region Aη recursively:

• The cell associated to the root of T is [0  1]d;
• For each η ∈ N ◦(T )  we deﬁne

Aleft(η) := {x ∈ Aη : xdη

(cid:54) νη}

and Aright(η) := Aη \ Aleft(η).

The leaf cells (Aφ)φ∈L(T ) form a partition of [0  1]d by construction. In the sequel  we will identify
a tree with splits (T  Σ) with the associated tree partition M (T  Σ)  and a node η ∈ N (T ) with the
cell Aη ⊂ [0  1]d. The decision tree classiﬁer outputs a constant prediction of the label in each leaf
cell Aη using a simple majority vote of the labels Yi (1 (cid:54) i (cid:54) n) such that Xi ∈ Aη.

3 A new online Mondrian Forest algorithm

We describe the Mondrian Process in Section 3.1  and recall the original Mondrian Forest procedure
in Section 3.2. Our procedure is introduced in Section 3.3.

3.1 The Mondrian process

Mondrian process  introduced in [RT09]. Given a rectangular box C =(cid:81)d

The probability distribution we consider on tree-based partitions of the unit cube [0  1]d is the
j=1[aj  bj]  we denote

3

|C| :=(cid:80)d

j=1(bj−aj) its linear dimension. The Mondrian process distribution MP(λ  C) is the distri-
bution of the random tree partition of C obtained by the sampling procedure SampleMondrian(λ  C)
from Algorithm 1.

Algorithm 1 SampleMondrian(λ  C) ; Samples a tree partition distributed as MP(λ  C).
1: Parameters: A rectangular box C ⊂ Rd and a lifetime parameter λ > 0.
2: Call SplitCell(C  τC := 0  λ).

1: Parameters: A cell A =(cid:81)

Algorithm 2 SplitCell(A  τ  λ) ; Recursively split a cell A  starting from time τ  until λ
1(cid:54)j(cid:54)d[aj  bj]  a starting time τ and a lifetime parameter λ.

2: Sample an exponential random variable EA with intensity |A|.
3: if τ + EA (cid:54) λ then
4:

Draw at random a split dimension J ∈ {1  . . .   d}  with P(J = j) = (bj − aj)/|A|  and a
split threshold νJ uniformly in [aJ   bJ ].
Split A along the split (J  νJ ).
Call SplitCell(left(A)  τ + EA  λ) and SplitCell(right(A)  τ + EA  λ).

5:
6:
7: else
8:
9: end if

Do nothing.

3.2 Online tree growing: the original scheme

In order to implement an online algorithm  it is crucial to be able to “update” the tree partitions
grown at a given time step. The approach of the original Mondrian Forest algorithm [LRT14] uses
a slightly different randomization mechanism  namely a Mondrian process supported in the range
deﬁned by the past feature points. More precisely  this modiﬁcation amounts to replacing each call
to SplitCell(A  τ  λ) by a call to SplitCell(Arange(n)  τ  λ)  where Arange(n) is the range of the
feature points X1  . . .   Xn that fall in A (i.e. the smallest box that contains them).
When a new training point (Xn+1  Yn+1) arrives  the ranges of the training points may change. The
online update of the tree partition then relies on the extension properties of the Mondrian process:
given a Mondrian partition M1 ∼ MP(λ  C1) on a box C1  it is possible to efﬁciently sample a
Mondrian partition M0 ∼ MP(λ  C0) on a larger box C0 ⊃ C1 that restricts to M1 on the cell C1
(this is called a “conditional Mondrian”  see [RT09]).
Remark 2. In [LRT14] a lifetime parameter λ = ∞ is actually used in experiments  which essentially
amounts to growing the trees completely  until the leaves are homogeneous. We will not analyze this
variant here  but this illustrates the problem of using a ﬁxed  ﬁnite budget λ in advance.

3.3 Online tree growing: a dual approach

An important limitation of the original scheme is the fact that it requires to ﬁx the lifetime parameter
λ in advance. In order to obtain a consistent algorithm  it is required to grow increasingly complex
trees. To achieve this  we propose to adopt a “dual” point of view: instead of using a Mondrian
process with ﬁxed lifetime on a domain that changes as new data points are added  we use a Mondrian
process on a ﬁxed domain (the cube [0  1]d) but with a varying lifetime λn that grows with the sample
size n. The rationale is that  as more data becomes available  the classiﬁers should be more complex
and precise. Since the lifetime  rather than the domain  is the parameter that guides the complexity of
the trees  it should be this parameter that dynamically adapts to the amount of training data.
It turns out that in this approach  quite surprisingly  the trees can be updated incrementally  leading to
an online algorithm. The ability to extend a tree partition Mλn ∼ MP(λn  [0  1]d) into a ﬁner tree
partition Mλn+1 ∼ MP(λn+1  [0  1]d) relies on a different property of the Mondrian process  namely
the fact that for λ < λ(cid:48)  it is possible to efﬁciently sample a Mondrian tree partition Mλ(cid:48) ∼ MP(λ(cid:48)  C)
given its pruning Mλ ∼ MP(λ  C) at time λ (obtained by dropping all splits of Mλ(cid:48) performed at a
time τ > λ).
The procedure ExtendMondrian(Mλ  λ  λ(cid:48)) from Algorithm 3 extends a Mondrian tree partition
Mλ ∼ MP(λ  C) to a tree partition Mλ(cid:48) ∼ MP(λ(cid:48)  C). Indeed  for each leaf cell A of Mλ  the fact

4

Algorithm 3 ExtendMondrian(Mλ  λ  λ(cid:48)) ; Extend Mλ ∼ MP(λ  C) to Mλ(cid:48) ∼ MP(λ(cid:48)  C)
1: Parameters: A tree partition Mλ  and lifetimes λ (cid:54) λ(cid:48).
2: for A in L(Mλ) do
3:
4: end for

Call SplitCell(A  λ  λ(cid:48))

that A is a leaf of Mλ means that during the sampling of Mλ  the time of the next candidate split
τ + EA (where τ is the time A was formed and EA ∼ Exp(|A|)) was strictly larger than λ. Now in
the procedure ExtendMondrian(Mλ  λ  λ(cid:48))  the time of the next candidate split is λ + E(cid:48)
A  where
A ∼ Exp(|A|). This is precisely the where the trick resides: by the memory-less property of the
E(cid:48)
exponential distribution  the distribution of τA + EA conditionally on EA > λ − τA is the same as
that of λ + E(cid:48)
A. The procedure ExtendMondrian can be replaced by the following more efﬁcient
implementation:

• Time of the next split of the tree is sampled as λ+EMλ with EMλ ∼ Exp((cid:80)

φ∈L(Mλ) |Aφ|);
• Leaf to split is chosen using a top-down path from the root of the tree  where the choice
between left or right child for each interior node is sampled at random  proportionally to the
linear dimension of all the leaves in the subtree deﬁned by the child.

Remark 3. While we consider Mondrian partitions on the ﬁxed domain [0  1]d  our increasing
lifetime trick can be used in conjunction with a varying domain based on the range of the data (as
in the original MF algorithm)  simply by applying ExtendMondrian(Mλn   λn  λn+1) after having
extended the Mondrian to the new range. In order to keep the analysis tractable and avoid unnecessary
complications in the analysis  we will study the procedure on a ﬁxed domain only.

Given an increasing sequence (λn)n(cid:62)1 of lifetime parameters  our modiﬁed MF algorithm incremen-
tally updates the trees M (k)
  λn  λn+1)  and
combines the forecasts of the given trees  as explained in Algorithm 4.

for k = 1  . . .   K by calling ExtendMondrian(M (k)
λn

λ

Algorithm 4 MondrianForest(K  (λn)n(cid:62)1) ; Trains a Mondrian Forest classiﬁer.
1: Parameters: The number of trees K and the lifetime sequence (λn)n(cid:62)1.
2: Initialization: Start with K trivial partitions M (k)
λ0

the training labels in each cell to 0  and the labels e.g. to 0.

  λ0 := 0  k = 1  . . .   K. Set the counts of

Receive the training point (Xn  Yn).
for k = 1  . . .   K do

3: for n = 1  2  . . . do
4:
5:
6:
7:
8:
9:
10: end for

end for

Update the counts of 0 and 1 (depending on Yn) in the leaf cell of Xn in Mλn.
Call ExtendMondrian(M (k)
λn−1
Fit the newly created leaves.

  λn−1  λn).

For the prediction of the label given a new feature vector  our algorithm uses a majority vote
over the predictions given by all K trees. However  other choices are possible. For instance  the
original Mondrian Forest algorithm [LRT14] places a hierarchical Bayesian prior over the label
distribution on each node of the tree  and performs approximate posterior inference using the so-
called interpolated Kneser-Ney (IKN) smoothing. Another possibility  that will be developed in an
extended version of this work  is tree expert aggregation methods  such as the Context-Tree Weighting
(CTW) algorithm [WST95  HS97] or specialist aggregation methods [FSSW97] over the nodes of
the tree  adapting them to increasingly complex trees.
Our modiﬁcation of the original Mondrian Forest replaces the process of online tree growing with a
ﬁxed lifetime by a new process  that allows to increase lifetimes. This modiﬁcation not only allows
to prove consistency  but more surprisingly leads to an optimal estimation procedure  in terms of
minimax rates  as illustrated in Sections 5 and 6 below.

5

4 Mondrian Forest with ﬁxed lifetime are inconsistent

We state in Proposition 1 the inconsistency of ﬁxed-lifetime Mondrian Forests  such as the original
algorithm [LRT14]. This negative result justiﬁes our modiﬁed algorithm based on an increasing
sequence of lifetimes (λn)n(cid:62)1.
Proposition 1. The Mondrian Forest algorithm (Algorithm 4) with a ﬁxed lifetime sequence λn = λ
is inconsistent: there exists a distribution of (X  Y ) ∈ [0  1] × {0  1} such that L∗ = 0 and
L(gn) = P(gn(X) (cid:54)= Y ) does not tend to 0. This result also holds true for the original Mondrian
Forest algorithm with lifetime λ.

Proposition 1 is established in Appendix C. The proof uses a result of independent interest (Lemma 3) 
which states that asymptotically over the sample size  for ﬁxed λ  the restricted domain does not
affect the randomization procedure.

5 Consistency of Mondrian Forest with lifetime sequence (λn)

The consistency of the Mondrian Forest used with a properly tuned sequence (λn) is established in
Theorem 1 below.
n/n → 0. Then  the online Mondrian Forest described
Theorem 1. Assume that λn → ∞ and that λd
in Algorithm 4 is consistent.

This consistency result is universal  in the sense that it makes no assumption on the distribution of
X nor on the conditional probability η. This contrasts with some consistency results on Random
forests  such as Theorem 1 of [DMdF13]  which assumes that the density of X is bounded by above
and below.
Theorem 1 does not require an assumption on K (number of trees). It is well-known for batch
Random Forests that this meta-parameter is not a sensitive tuning parameter  and that it sufﬁces to
choose it large enough to obtain good accuracy. The only important parameter is the sequence (λn) 
that encodes the complexity of the trees. Requiring an assumption on this meta-parameter is natural 
and conﬁrmed by the well-known fact that the tree-depth is the most important tuning parameter for
batch Random Forests  see for instance [BS16].
The proof of Theorem 1 can be found in the supplementary material (see Appendix D). The core of
the argument lies in two lemmas describing two novel properties of Mondrian trees. Lemma 1 below
provides an upper bound of order O(λ−1) on the diameter of the cell Aλ(x) of a Mondrian partition
Mλ ∼ MP(λ  [0  1]d). This is the key to control the bias of Mondrian Forests with lifetime sequence
that tend to inﬁnity.
Lemma 1 (Cell diameter). Let x ∈ [0  1]d  and let Dλ(x) be the (cid:96)2-diameter of the cell containing
x in a Mondrian partition Mλ ∼ MP(λ  [0  1]d). If λ → ∞  then Dλ(x) → 0 in probability. More
precisely  for every δ  λ > 0  we have

P(Dλ(x) (cid:62) δ) (cid:54) d

1 +

exp

− λδ√
d

and

(cid:18)

(cid:19)

(cid:18)

(cid:19)
E(cid:2)Dλ(x)2(cid:3) (cid:54) 4d

λδ√
d

λ2 .

(2)

(3)

The proof of Lemma 1 is provided in the supplementary material (see Appendix A). The second
important property needed to carry out the analysis is stated in Lemma 2 and helps to control the
“variance” of Mondrian forests. It consists in an upper bound of order O(λd) on the total number of
splits performed by a Mondrian partition Mλ ∼ MP(λ  [0  1]d). This ensures that enough data points
fall in each cell of the tree  so that the labels of the tree are well estimated. The proof of Lemma 2 is
to be found in the supplementary material (see Appendix B).
Lemma 2 (Number of splits). If Kλ denotes the number of splits performed by a Mondrian tree
partition Mλ ∼ MP(λ  [0  1]d)  we have E(Kλ) (cid:54) (e(λ + 1))d.
Remark 4. It is worth noting that controlling the total number of splits ensures that the cell Aλn(X)
in which a new random X ∼ µ ends up contains enough training points among X1  . . .   Xn

6

(see Lemma 4 in appendix D). This enables to get a distribution-free consistency result. Another
approach consists in lower-bounding the volume Vλn (x) of Aλn (x) in probability for any x ∈ [0  1]d 
which shows that the cell Aλn (x) contains enough training points  but this would require the extra
assumption that the density of X is lower-bounded.
Remarkably  owing to the nice restriction properties of the Mondrian process  Lemmas 1 and 2
essentially provide matching upper and lower bounds on the complexity of the partition. Indeed 
in order to partition the cube [0  1]d in cells of diameter O(1/λ)  at least Θ(λd) cells are needed;
Lemma 2 shows that the Mondrian partition in fact contains only O(λd) cells.

6 Minimax rates over the class of Lipschitz functions

The estimates obtained in Lemmas 1 and 2 are quite explicit and sharp in their dependency on λ  and
allow to study the convergence rate of our algorithm. Indeed  it turns out that our modiﬁed Mondrian
Forest  when properly tuned  can achieve the minimax rate in classiﬁcation over the class of Lipschitz
functions (see e.g. Chapter I.3 in [Nem00] for details on minimax rates). We provide two results: a
convergence rate for the estimation of the conditional probabilities  measured by the quadratic risk 
see Theorem 2  and a control on the distance between the classiﬁcation error of our classiﬁer and
the Bayes error  see Theorem 3. We provide also similar minimax bounds for the regression setting
instead of the classiﬁcation one in the supplementary material  see Proposition 4 in Appendix E.

Let(cid:98)ηn be the estimate of the conditional probability η based on the Mondrian Forest (see Algorithm 4)

in which:

(i) Each leaf label is computed as the proportion of 1 in the corresponding leaf;
(ii) Forest prediction results from the average of tree estimates instead of a majority vote.

[0  1]d. Let(cid:98)ηn be a Mondrian Forest as deﬁned in Points (i) and (ii)  with a lifetimes sequence that

Theorem 2. Assume that the conditional probability function η : [0  1]d → [0  1] is Lipschitz on
satisﬁes λn (cid:16) n1/(d+2). Then  the following upper bound holds

E(η(X) −(cid:98)ηn(X))2 = O(n−2/(d+2))

for n large enough  which correspond to the minimax rate over the set of Lipschitz functions.

(4)

To the best of our knowledge  Theorem 2 is the ﬁrst to exhibit the fact that a classiﬁcation method
based on a purely random forest can be minimax optimal in an arbitrary dimension. The same
kind of result is stated for regression estimation in the supplementary material (see Proposition 4 in
Appendix E).
Minimax rates  but only for d = 1  were obtained in [Gen12  AG14] for models of purely random
forests such as Toy-PRF (where the individual partitions corresponded to randomly shifts of the
regular partition of [0  1] in k intervals) and PURF (Purely Uniformly Random Forests  where the
partitions were obtained by drawing k random thresholds at random in [0  1]).
However  for d = 1  tree partitions reduce to partitions of [0  1] in intervals  and do not possess the
recursive structure that appears in higher dimensions and makes their precise analysis difﬁcult. For
this reason  the analysis of purely random forests for d > 1 has typically produced sub-optimal
results: for example  [BDL08] show consistency for UBPRF (Unbalanced Purely Random Forests 
that perform a ﬁxed number of splits and randomly choose a leaf to split at each step)  but with no
rate of convergence. A further step was made by [AG14]  who studied the BPRF (Balanced Purely
Random Forests algorithm  where all leaves were split  so that the resulting tree was complete)  and
obtained suboptimal rates. In our approach  the convenient properties of the Mondrian process enable
to bypass the inherent difﬁculties met in previous attempts  thanks to its recursive structure  and allow
to obtain the minimax rate with transparent proof.

Now  note that the Mondrian Forest classiﬁer corresponds to the plugin classiﬁer (cid:98)gn(x) =
1{(cid:98)ηn(x)>1/2}  where(cid:98)ηn is deﬁned in Points (i) and (ii). A general theorem (Theorem 6.5 in [DGL96])
allows us to derive upper bounds on the distance between the classiﬁcation error of(cid:98)gn and the Bayes
Theorem 3. Under the same assumptions as in Theorem 2  the Mondrian Forest classiﬁer(cid:98)gn with

error  thanks to Theorem 2.
lifetime sequence λn (cid:16) n1/(d+2) satisﬁes

L((cid:98)gn) − L∗ = o(n−1/(d+2)).

(5)

7

The rate of convergence o(n−1/(d+2)) for the error probability with a Lipschitz conditional probability
η turns out to be optimal  as shown by [Yan99]. Note that faster rates can be achieved in classiﬁcation
under low noise assumptions such as the margin assumption [MT99] (see e.g. [Tsy04  AT07  Lec07]).
Such specializations of our results are to be considered in a future work  the aim of the present paper
being an emphasis on the appealing optimal properties of our modiﬁed Mondrian Forest.

7 Experiments

We now turn to the empirical evaluation of our algorithm  and examine its predictive performance
(test error) as a function of the training size. More precisely  we compare the modiﬁed Mondrian
Forest algorithm (Algorithm 4) to batch (Breiman RF [Bre01]  Extra-Trees-1 [GEW06]) and online
(the Mondrian Forest algorithm [LRT14] with ﬁxed lifetime parameter λ) Random Forests algorithms.
We compare the prediction accuracy (on the test set) of the aforementioned algorithms trained on
varying fractions of the training data from 10% to 100%.
Regarding our choice of competitors  we note that Breiman’s RF is well-established and known
to achieve state-of-the-art performance. We also included the Extra-Trees-1 (ERT-1) algorithm
[GEW06]  which is most comparable to the Mondrian Forest classiﬁer since it also draws splits
randomly (we note that the ERT-k algorithm [GEW06] with the default tuning k =
d in the
scikit-learn implementation [PVG+11] achieves scores very close to those of Breiman’s RF).
In the case of online Mondrian Forests  we included our modiﬁed Mondrian Forest classiﬁer with
an increasing lifetime parameter λn = n1/(d+2) tuned according to the theoretical analysis (see
Theorem 3)  as well as a Mondrian Forest classiﬁer with constant lifetime parameter λ = 2. Note that
while a higher choice of λ would have resulted in a performance closer to that of the modiﬁed version
(with increasing λn)  our inconsistency result (Proposition 1) shows that its error would eventually
stagnate given more training samples. In both cases  the splits are drawn within the range of the
training feature  as in the original Mondrian Forest algorithm. Our results are reported in Figure 1.

√

.

Figure 1: Prediction accuracy as a function of the fraction of data used on several datasets. Modiﬁed
MF (Algorithm 4) outperforms MF with a constant lifetime  and is better than the batch ERT-1
algorithm. It also performs almost as well as Breiman’s RF (a batch algorithm that uses the whole
training dataset in order to choose each split) on several datasets  while being incremental and
much faster to train. On the dna dataset  as noted in [LRT14]  Breiman’s RF outperforms the other
algorithms because of the presence of a large number of irrelevant features.

8

0.20.40.60.81.00.650.700.750.800.850.90letterBreiman_RFExtra_Trees_1Mondrian_increasingMondrian_fixed0.20.40.60.81.00.740.760.780.800.820.840.860.88satimage0.20.40.60.81.00.7500.7750.8000.8250.8500.8750.900usps0.20.40.60.81.00.550.600.650.700.750.800.850.90dna8 Conclusion and future work

Despite their widespread use in practice  the theoretical understanding of Random Forests is still
incomplete. In this work  we show that amending the Mondrian Forest classiﬁer  originally introduced
to provide an efﬁcient online algorithm  leads to an algorithm that is not only consistent  but in fact
minimax optimal for Lipschitz conditional probabilities in arbitrary dimension. This new result
suggests promising improvements in the understanding of random forests methods.
A ﬁrst  natural extension of our results  that will be addressed in a future work  is the study of the
rates for smoother regression functions. Indeed  we conjecture that through a more reﬁned study of
the local properties of the Mondrian partitions  it is possible to describe exactly the distribution of the
cell of a given point. In the spirit of the work of [AG14] in dimension one  this could be used to show
improved rates for the bias of forests (e.g. for C 2 regression functions) compared to the tree bias  and
hence give some theoretical insight to the empirically well-known fact that a forest performs better
than individual trees.
Second  the optimal upper bound O(n−1/(d+2)) obtained in this paper is very slow when the number
of features d is large. This comes from the well-known curse of dimensionality phenomenon  a
problem affecting all fully nonparametric algorithms. A standard technique used in high-dimensional
settings is to work under a sparsity assumption  where only s (cid:28) d features are informative (i.e.
affect the distribution of Y ). In such settings  a natural strategy is to select the splits using the
labels Y1  . . .   Yn  as most variants of Random Forests used in practice do. For example  it would be
interesting to combine a Mondrian process-based randomization with a choice of the best split among
several candidates  as performed by the Extra-Tree algorithm [GEW06]. Since the Mondrian Forest
guarantees minimax rates  we conjecture that it should improve feature selection of batch random
forest methods  and improve the underlying randomization mechanism of these algorithms. From a
theoretical perspective  it could be interesting to see how the minimax rates obtained here could be
coupled with results on the ability of forests to select informative variables  see for instance [SBV15].

References

[AG14] Sylvain Arlot and Robin Genuer. Analysis of purely random forests bias. arXiv preprint

arXiv:1407.3939  2014.

[AT07] Jean-Yves Audibert and Alexandre B. Tsybakov. Fast learning rates for plug-in classiﬁers. The

Annals of Statistics  35(2):608–633  2007.

[BDL08] Gérard Biau  Luc Devroye  and Gábor Lugosi. Consistency of random forests and other averaging

classiﬁers. Journal of Machine Learning Research  9:2015–2033  2008.

[Bia12] Gérard Biau. Analysis of a random forests model. Journal of Machine Learning Research 

13(1):1063–1095  2012.

[BLG+16] Matej Balog  Balaji Lakshminarayanan  Zoubin Ghahramani  Daniel M. Roy  and Yee W. Teh. The

Mondrian kernel. In 32nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2016.

[Bre00] Leo Breiman. Some inﬁnity theory for predictor ensembles. Technical Report 577  Statistics

departement  University of California Berkeley  2000.

[Bre01] Leo Breiman. Random forests. Machine Learning  45(1):5–32  2001.

[Bre04] Leo Breiman. Consistency for a simple model of random forests. Technical Report 670  Statistics

departement  University of California Berkeley  2004.

[BS16] Gérard Biau and Erwan Scornet. A random forest guided tour. TEST  25(2):197–227  2016.

[DGL96] Luc Devroye  László Györﬁ  and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition 

volume 31 of Applications of Mathematics. Springer-Verlag  1996.

[DH00] Pedro Domingos and Geoff Hulten. Mining high-speed data streams. In Proceedings of the 6th
SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 71–80  2000.

[DMdF13] Misha Denil  David Matheson  and Nando de Freitas. Consistency of online random forests. In
Proceedings of the 30th Annual International Conference on Machine Learning (ICML)  pages
1256–1264  2013.

9

[DMdF14] Misha Denil  David Matheson  and Nando de Freitas. Narrowing the gap: Random forests in theory
and in practice. In Proceedings of the 31st Annual International Conference on Machine Learning
(ICML)  pages 665–673  2014.

[FSSW97] Yoav Freund  Robert E. Schapire  Yoram Singer  and Manfred K. Warmuth. Using and combining
In Proceedings of the 29th Annual ACM Symposium on Theory of

predictors that specialize.
Computing  pages 334–343  1997.

[Gen12] Robin Genuer. Variance reduction in purely random forests. Journal of Nonparametric Statistics 

24(3):543–562  2012.

[GEW06] Pierre Geurts  Damien Ernst  and Louis Wehenkel. Extremely randomized trees. Machine learning 

63(1):3–42  2006.

[HS97] David P. Helmbold and Robert E. Schapire. Predicting nearly as well as the best pruning of a

decision tree. Machine Learning  27(1):51–68  1997.

[Lec07] Guillaume Lecué. Optimal rates of aggregation in classiﬁcation under low noise assumption.

Bernoulli  13(4):1000–1022  2007.

[LRT14] Balaji Lakshminarayanan  Daniel M. Roy  and Yee W. Teh. Mondrian forests: Efﬁcient online
random forests. In Advances in Neural Information Processing Systems 27  pages 3140–3148.
Curran Associates  Inc.  2014.

[LRT16] Balaji Lakshminarayanan  Daniel M. Roy  and Yee W. Teh. Mondrian forests for large-scale
regression when uncertainty matters. In Proceedings of the 19th International Workshop on Artiﬁcial
Intelligence and Statistics (AISTATS)  2016.

[MT99] Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. The Annals of

Statistics  27(6):1808–1829  1999.

[Nem00] Arkadi Nemirovski. Topics in non-parametric statistics. Lectures on Probability Theory and

Statistics: Ecole d’Ete de Probabilites de Saint-Flour XXVIII-1998  28:85–277  2000.

[OR15] Peter Orbanz and Daniel M. Roy. Bayesian models of graphs  arrays and other exchangeable random
structures. IEEE transactions on pattern analysis and machine intelligence  37(2):437–461  2015.

[PVG+11] Fabian Pedregosa  Gaël Varoquaux  Alexandre Gramfort  Vincent Michel  Bertrand Thirion  Olivier
Grisel  Mathieu Blondel  Peter Prettenhofer  Ron Weiss  Vincent Dubourg  Jake Vanderplas  Alexan-
dre Passos  David Cournapeau  Matthieu Brucher  Matthieu Perrot  and Édouard Duchesnay. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research  12:2825–2830  2011.

[Roy11] Daniel M. Roy. Computability  inference and modeling in probabilistic programming. PhD thesis 

Massachusetts Institute of Technology  2011.

[RT09] Daniel M. Roy and Yee W. Teh. The Mondrian process.

In Advances in Neural Information

Processing Systems 21  pages 1377–1384. Curran Associates  Inc.  2009.

[SBV15] Erwan Scornet  Gérard Biau  and Jean-Philippe Vert. Consistency of random forests. The Annals of

Statistics  43(4):1716–1741  2015.

[SLS+09] Amir Saffari  Christian Leistner  Jacob Santner  Martin Godec  and Horst Bischof. On-line random

forests. In 3rd IEEE ICCV Workshop on On-line Computer Vision  2009.

[TGP11] Matthew A. Taddy  Robert B. Gramacy  and Nicholas G. Polson. Dynamic trees for learning and

design. Journal of the American Statistical Association  106(493):109–123  2011.

[Tsy04] Alexandre B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics 

32(1):135–166  2004.

[WST95] Frans M. J. Willems  Yuri M. Shtarkov  and Tjalling J. Tjalkens. The context-tree weighting method:

Basic properties. IEEE Transactions on Information Theory  41(3):653–664  1995.

[Yan99] Yuhong Yang. Minimax nonparametric classiﬁcation. I. Rates of convergence. IEEE Transactions

on Information Theory  45(7):2271–2284  1999.

10

,Jaouad Mourtada
Stéphane Gaïffas
Erwan Scornet