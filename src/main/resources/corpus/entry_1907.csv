2017,Expectation Propagation for t-Exponential Family Using q-Algebra,Exponential family distributions are highly useful in machine learning since their calculation can be performed efficiently through natural parameters. The exponential family has recently been extended to the t-exponential family  which contains Student-t distributions as family members and thus allows us to handle noisy data well. However  since the t-exponential family is defined by the deformed exponential  an efficient learning algorithm for the t-exponential family such as expectation propagation (EP) cannot be derived in the same way as the ordinary exponential family. In this paper  we borrow the mathematical tools of q-algebra from statistical physics and show that the pseudo additivity of distributions allows us to perform calculation of t-exponential family distributions through natural parameters. We then develop an expectation propagation (EP) algorithm for the t-exponential family  which provides a deterministic approximation to the posterior or predictive distribution with simple moment matching. We finally apply the proposed EP algorithm to the Bayes point machine and Student-t process classification  and demonstrate their performance numerically.,Expectation Propagation for t-Exponential Family

Using q-Algebra

Futoshi Futami

The University of Tokyo  RIKEN
futami@ms.k.u-tokyo.ac.jp

Issei Sato

The University of Tokyo  RIKEN

sato@k.u-tokyo.ac.jp

Masashi Sugiyama

RIKEN  The University of Tokyo

sugi@k.u-tokyo.ac.jp

Abstract

Exponential family distributions are highly useful in machine learning since their
calculation can be performed eﬃciently through natural parameters. The exponen-
tial family has recently been extended to the t-exponential family  which contains
Student-t distributions as family members and thus allows us to handle noisy data
well. However  since the t-exponential family is deﬁned by the deformed exponen-
tial  an eﬃcient learning algorithm for the t-exponential family such as expectation
propagation (EP) cannot be derived in the same way as the ordinary exponential
family. In this paper  we borrow the mathematical tools of q-algebra from statisti-
cal physics and show that the pseudo additivity of distributions allows us to perform
calculation of t-exponential family distributions through natural parameters. We
then develop an expectation propagation (EP) algorithm for the t-exponential fam-
ily  which provides a deterministic approximation to the posterior or predictive
distribution with simple moment matching. We ﬁnally apply the proposed EP
algorithm to the Bayes point machine and Student-t process classiﬁcation  and
demonstrate their performance numerically.

1

Introduction

Exponential family distributions play an important role in machine learning  due to the fact that their
calculation can be performed eﬃciently and analytically through natural parameters or expected
suﬃcient statistics [1]. This property is particularly useful in the Bayesian framework since a
conjugate prior always exists for an exponential family likelihood and the prior and posterior are
often in the same exponential family. Moreover  parameters of the posterior distribution can be
evaluated only through natural parameters.
As exponential family members  Gaussian distributions are most commonly used because their
moments  conditional distribution  and joint distribution can be computed analytically. Gaussian
processes are a typical Bayesian method based on Gaussian distributions  which are used for various
purposes such as regression  classiﬁcation  and optimization [8]. However  Gaussian distributions are
sensitive to outliers and heavier-tailed distributions are often more preferred in practice. For example 
Student-t distributions and Student-t processes are good alternatives to Gaussian distributions [4]
and Gaussian processes [10]  respectively.
A technical problem of the Student-t distribution is that it does not belong to the exponential family
unlike the Gaussian distribution and thus cannot enjoy good properties of the exponential family. To
cope with this problem  the exponential family was recently generalized to the t-exponential family [3] 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

which contains Student-t distributions as family members. Following this line  the Kullback-Leibler
divergence was generalized to the t-divergence  and approximation methods based on t-divergence
minimization have been explored [2]. However  the t-exponential family does not allow us to employ
standard useful mathematical tricks  e.g.  logarithmic transformation does not reduce the product of t-
exponential family functions into summation. For this reason  the t-exponential family unfortunately
does not inherit an important property of the original exponential family  that is  calculation can be
performed through natural parameters. Furthermore  while the dimensionality of suﬃcient statistics
is the same as that of the natural parameters in the exponential family and thus there is no need to
increase the parameter size to incorporate new information [9]  this useful property does not hold in
the t-exponential family.
The purpose of this paper is to further explore mathematical properties of natural parameters of the
t-exponential family through pseudo additivity of distributions based on q-algebra used in statistical
physics [7  11]. More speciﬁcally  our contributions in this paper are three-fold:
1. We show that  in the same way as ordinary exponential family distributions  q-algebra allows us
to handle the calculation of t-exponential family distributions through natural parameters.
2. Our q-algebra based method enables us to extend assumed density ﬁltering (ADF) [2] and develop
an algorithm of expectation propagation (EP) [6] for the t-exponential family. In the same way as
the original EP algorithm for ordinary exponential family distributions  our EP algorithm provides
a deterministic approximation to the posterior or predictive distribution for t-exponential family
distributions with simple moment matching.
3. We apply the proposed EP algorithm to the Bayes point machine [6] and Student-t process classiﬁ-
cation  and we demonstrate their usefulness as alternatives to the Gaussian approaches numerically.

2

t-exponential Family

In this section  we review the t-exponential family [3  2]  which is a generalization of the exponential
family.
The t-exponential family is deﬁned as 

where expt(x) is the deformed exponential function deﬁned as

p(x; (cid:18)) = expt(⟨(cid:8)(x); (cid:18)⟩ (cid:0) gt((cid:18)));

{

expt(x) =

exp(x)
[1 + (1 (cid:0) t)x]

1
1(cid:0)t

if t = 1;
otherwise;

(1)

(2)

(4)

and gt((cid:18)) is the log-partition function that satisﬁes

(3)
The notation Epes denotes the expectation over pes(x)  where pes(x) is the escort distribution of p(x)
deﬁned as

∇(cid:18)gt((cid:18)) = Epes[(cid:8)(x)]:

p(x)t
p(x)tdx
We call (cid:18) a natural parameter and (cid:8)(x) suﬃcient statistics.
Let us express the k-dimensional Student-t distribution with v degrees of freedom as

pes(x) =

(cid:0)((v + k)=2)

St(x; v; (cid:22); (cid:6)) =

((cid:25)v)k=2(cid:0)(v=2)j(cid:6)j1=2

(cid:0)1(x (cid:0) (cid:22))
(5)
where (cid:0)(x) is the gamma function  jAj is the determinant of matrix A  and A
⊤ is the transpose of
) 1
matrix A. We can conﬁrm that the Student-t distribution is a member of the t-exponential family as
follows. First  we have
)1(cid:0)t

(cid:9) + (cid:9) (cid:1) (x (cid:0) (cid:22))
⊤

(cid:0)1(x (cid:0) (cid:22))

St(x; v; (cid:22); (cid:6)) =

(

(v(cid:6))

(v(cid:6))

1(cid:0)t ;

(

(6)

:

∫
(
1 + (x (cid:0) (cid:22))
⊤

)(cid:0) v+k

2

;

(cid:0)((v + k)=2)

((cid:25)v)k=2(cid:0)(v=2)j(cid:6)j1=2

:

(7)

where (cid:9) =

2

Note that relation (cid:0)(v + k)=2 = 1=(1 (cid:0) t) holds  from which we have

(
(
(cid:9)
1 (cid:0) t

)
)

(x

⟨(cid:8)(x); (cid:18)⟩ =

⊤

Kx (cid:0) 2(cid:22)

⊤

Kx);

gt((cid:18)) = (cid:0)

(cid:9)
1 (cid:0) t

⊤

((cid:22)

K(cid:22) + 1) +

1
1 (cid:0) t

;

(8)

(9)

where K = (v(cid:6))
family as:

(cid:0)1. Then  we can express the Student-t distribution as a member of the t-exponential

(

) 1
1 + (1 (cid:0) t)⟨(cid:8)(x); (cid:18)⟩ (cid:0) gt((cid:18))

(⟨(cid:8)(x); (cid:18)⟩ (cid:0) gt((cid:18))

)

St(x; v; (cid:22); (cid:6)) =

(10)
If t = 1  the deformed exponential function is reduced to the ordinary exponential function  and
therefore the t-exponential family is reduced to the ordinary exponential family  which corresponds
to the Student-t distribution with inﬁnite degrees of freedom. For t-exponential family distributions 
the t-divergence is deﬁned as follows [2]:

1(cid:0)t = expt

:

Dt(p∥ep) =

∫ (

)
pes(x) lnt p(x) (cid:0) pes(x) lntep(x)

dx;

(11)

where lnt x := x1(cid:0)t(cid:0)1
1(cid:0)t

(x (cid:21) 0; t 2 R+) and pes(x) is the escort function of p(x).

3 Assumed Density Filtering and Expectation Propagation

We brieﬂy review the assumed density ﬁltering (ADF) and expectation propagation (EP) [6].
∏
Let D = f(x1; y1); : : : ; (xi; yi)g be input-output paired data. We denote the likelihood for the i-th
data as li(w) and the prior distribution of parameter w as p0(w). The total likelihood is given as
i li(w) and the posterior distribution can be expressed as p(wjD) / p0(w)
3.1 Assumed Density Filtering
ADF is an online approximation method for the posterior distribution.
imation to the posterior distribution  epi(cid:0)1(w)  has already been obtained. Given the i-th sample
Suppose that i (cid:0) 1 samples (x1; y1); : : : ; (xi(cid:0)1; yi(cid:0)1) have already been processed and an approx-
(xi; yi)  the posterior distribution pi(w) can be obtained as

i li(w).

∏

(12)
Since the true posterior distribution pi(w) cannot be obtained analytically  it is approximated in ADF
by minimizing the Kullback-Leibler (KL) divergence from pi(w) to its approximation:

Note that if pi and ep are both exponential family members  the above calculation is reduced to

(13)

pi(w) /epi(cid:0)1(w)li(w):
KL(pi∥ep):
epi = arg minep

moment matching.

3.2 Expectation Propagation
Although ADF is an eﬀective method for online learning  it is not favorable for non-online situations 
because the approximation quality depends heavily on the permutation of data [6]. To overcome this
problem  EP was proposed [6].
In EP  an approximation of the posterior that contains whole data terms is prepared beforehand 
typically as a product of data-corresponding terms:

(14)

ep(w) =

∏

i

eli(w);

1
Z

3

approximation as

family member  the total approximation also belongs to the exponential family.
Diﬀerently from ADF  EP has these site approximation with the following four steps  which is

where Z is the normalizing constant. In the above expression  factoreli(w)  which is often called
a site approximation [9]  corresponds to the local likelihood li(w). If eacheli(w) is an exponential
iteratively updated. First  when we update siteelj(w)  we eliminate the eﬀect of site j from the total
ep
(15)
whereep
nj(w) is often called a cavity distribution [9]. If an exponential family distribution is used  the
lj(w) by minimizing the divergence KL(ep
above calculation is reduced to subtraction of natural parameters. Second  we incorporate likelihood
nj is the normalizing
After this step  we obtainep(w). Third  we exclude the eﬀect of terms other than j  which is equivalent
to calculating a cavity distribution aselj(w)new / ep(w)
constant. Note that this minimization is reduced to moment matching for the exponential family.
by replacingelj(w) byelj(w)new.
epnj (w). Finally  we update the site approximation

nj∥ep(w))  where Z

ep(w)
elj(w)

nj(w)lj(w)=Z

nj(w) =

;

Note again that calculation of EP is reduced to addition or subtraction of natural parameters for the
exponential family.

3.3 ADF for t-exponential Family
ADF for the t-exponential family was proposed in [2]  which uses the t-divergence instead of the KL
divergence:

Dt(p∥p

′

) =

pes(x) lnt p(x) (cid:0) pes(x) lnt p
′

(x; (cid:18))

dx:

(16)

p′

∇(cid:18)gt((cid:18)) = Efpes ((cid:8)(x))  where fpes is the escort function of ep(x). Then  minimization of the

When an approximate distribution is chosen from the t-exponential family  we can utilize the property

ep = arg min

∫ (

)

t-divergence yields

Epes[(cid:8)(x)] = Efpes[(cid:8)(x)]:

(17)
This is moment matching  which is a celebrated property of the exponential family. Since the
expectation is taken with respect to the escort function  this is called escort moment matching.
=ep(w) = St(w;e(cid:22);e(cid:6); v). Then the
approximated posteriorepi(w) = St(w;e(cid:22)(i);e(cid:6)i; v) can be obtained by minimizing the t-divergence
As an example  let us consider the situation where the prior is the Student-t distribution and the
posterior is approximated by the Student-t distribution: p(wjD) (cid:24)
from pi(w) /epi(cid:0)1(w)eli(w) as

Dt(pi(w)∥St(w; (cid:22)

′

′

; (cid:6)

; v)):

(18)

arg min

(cid:22)′;(cid:6)′

This allows us to obtain an analytical update expression for t-exponential family distributions.

4 Expectation Propagation for t-exponential Family

As shown in the previous section  ADF has been extended to EP (which resulted in moment matching
for the exponential family) and to the t-exponential family (which yielded escort moment matching
for the t-exponential family). In this section  we combine these two extensions and propose EP for
the t-exponential family.

4.1 Pseudo Additivity and q-Algebra
Diﬀerently from ordinary exponential functions  deformed exponential functions do not satisfy the
product rule:

expt(x) expt(y) ̸= expt(x + y):

(19)

4

The q-division is the inverse of the q-product (and visa versa)  and the q-logarithm is the inverse of
the q-exponential (and visa versa). From the above deﬁnitions  the q-logarithm and q-exponential
satisfy the following relations:

which are called the q-product rules. Also for the q-division  similar properties hold:

lnq x :=

x1(cid:0)q (cid:0) 1
1 (cid:0) q

(x (cid:21) 0; q 2 R+):

lnq(x (cid:10)q y) = lnq x + lnq y;
expq(x) (cid:10)q expq(y) = expq(x + y);
lnq(x ⊘q y) = lnq x (cid:0) lnq y;
expq(x) ⊘q expq(y) = expq(x (cid:0) y);

(20)

(21)

(22)

(23)

(24)
(25)

(26)
(27)

(30)

(31)

For this reason  the cavity distribution cannot be computed analytically for the t-exponential family.
On the other hand  the following equality holds for the deformed exponential functions:

expt(x) expt(y) = expt(x + y + (1 (cid:0) t)xy);

which is called pseudo additivity.
In statistical physics [7  11]  a special algebra called q-algebra has been developed to handle a system
with pseudo additivity. We will use the q-algebra for eﬃciently handling t-exponential distributions.
Deﬁnition 1 (q-product) Operation (cid:10)q called the q-product is deﬁned as

x (cid:10)q y :=

[x1(cid:0)q + y1(cid:0)q (cid:0) 1]
0

1
1(cid:0)q

if x > 0; y > 0; x1(cid:0)q + y1(cid:0)q (cid:0) 1 > 0;
otherwise:

Deﬁnition 2 (q-division) Operation ⊘q called the q-division is deﬁned as

x ⊘q y :=

if x > 0; y > 0; x1(cid:0)q (cid:0) y1(cid:0)q (cid:0) 1 > 0;
otherwise:
Deﬁnition 3 (q-logarithm) The q-logarithm is deﬁned as

[x1(cid:0)q (cid:0) y1(cid:0)q (cid:0) 1]
0

1
1(cid:0)q

{
{

which are called the q-division rules.

4.2 EP for t-exponential Family
The q-algebra allows us to recover many useful properties from the ordinary exponential family. For
example  the q-product of t-exponential family distributions yields an unnormalized t-exponential
distribution:

expt(⟨(cid:8)(x); (cid:18)1⟩ (cid:0) gt((cid:18)1))(cid:10)t expt(⟨(cid:8)(x); (cid:18)2⟩ (cid:0) gt((cid:18)2))

= expt(⟨(cid:8)(x); ((cid:18)1 + (cid:18)2)⟩ (cid:0)egt((cid:18)1; (cid:18)2)):

(28)

Based on this q-product rule  we develop EP for the t-exponential family.
Consider the situation where prior distribution p(0)(w) is a member of the t-exponential family. As
an approximation to the posterior  we choose a t-exponential family distribution

(29)
In the original EP for the ordinary exponential family  we considered an approximate posterior of the
form

that is  we factorized the posterior to a product of site approximations corresponding to data. On the
other hand  in the case of the t-exponential family  we propose to use the following form called the
t-factorization:

∏

ep(w; (cid:18)) = expt(⟨(cid:8)(w); (cid:18)⟩ (cid:0) gt((cid:18))):
eli(w);
ep(w) / p(0)(w)
∏

ep(w) / p(0)(w) (cid:10)t

eli(w):

(cid:10)t

i

i

5

The t-factorization is reduced to the original factorization form when t = 1.
This t-factorization enables us to calculate EP update rules through natural parameters for the t-
exponential family in the same way as the ordinary exponential family. More speciﬁcally  consider
the case where factor j of the t-factorization is updated in four steps in the same way as original EP.
(I) First  we calculate the cavity distribution by using the q-division as
(cid:10)t

elj(w) / p(0)(w) (cid:10)t

ep
nj(w) /ep(w) ⊘t

eli(w):

∏

(32)

i̸=j

nj = (cid:18) (cid:0) (cid:18)(j):

(II) The second step is inclusion of site likelihood lj(w)  which can be performed byep

The above calculation is reduced to subtraction of natural parameters by using the q-algebra rules:
(33)
nj(w)lj(w).
The site likelihood lj(w) is incorporated to approximate the posterior by the ordinary product not the
q-product. Thus moment matching is performed to obtain a new approximation. For this purpose 
the following theorem is useful.
Theorem 1 The expected suﬃcient statistic 

(cid:18)

(cid:17) = ∇(cid:18)gt((cid:18)) = Efpes[(cid:8)(w)];
∫ fpes

nj(w)(lj(w))tdw; Z2 =

(34)

(35)

(36)

nj

(w)(lj(w))tdw:

A proof of Theorem 1 is given in Appendix A of the supplementary material. After moment matching 

(III) Third  we exclude the eﬀect of sites other than j. This is achieved by

can be derived as

∇

(cid:17) = (cid:17)

1
Z2

(cid:18)nj Z1;

nj +

where Z1 =

∫ ep
we obtain an approximation epnew(w).
elnew

j

(w) /epnew(w) ⊘tep
new = (cid:18)new (cid:0) (cid:18)
nj

nj:

nj(w);

(37)

(38)

which is reduced to subtraction of natural parameter

(IV) Finally  we update the site approximation by replacingelj(w) withelj(w)new.

(cid:18)

These four steps are our proposed EP method for the t-exponential family. As we have seen  these
steps are reduced to the ordinary EP steps if t = 1. Thus  the proposed method can be regarded as
an extention of the original EP to the t-exponential family.

4.3 Marginal Likelihood for t-exponential Family
In the above  we omitted the normalization term of the site approximation to simplify the derivation.
Here  we derive the marginal likelihood  which requires us to explicitly take into account the

normalization term eCi:

eli(wjeCi;e(cid:22)i;e(cid:27)2

i ) = eCi (cid:10)t expt(⟨(cid:8)(w); (cid:18)⟩):

(39)
We assume that this normalizer corresponds to Z1  which is the same assumption as that for the
ordinary EP. To calculate Z1  we use the following theorem (its proof is available in Appendix B of
the supplementary material):
Theorem 2 For the Student-t distribution  we have
expt(⟨(cid:8)(w); (cid:18)⟩ (cid:0) g)dw =

expt(gt((cid:18))=(cid:9) (cid:0) g=(cid:9))

) 3(cid:0)t

(

(40)

∫

;

2

where g is a constant  g((cid:18)) is the log-partition function and (cid:9) is deﬁned in (7).

6

Figure 1: Boundaries obtained by ADF (left two  with diﬀerent sample orders) and EP (right).

This theorem yields

2
3(cid:0)t

1 = gt((cid:18))=(cid:9) (cid:0) g

nj
t ((cid:18))=(cid:9) + logt

logt Z

(41)

and therefore the marginal likelihood can be calculated as follows (see Appendix C for details):

∫
(

ZEP =

p(0)(w) (cid:10)t
(∑

(cid:10)t

∏
eli(w)dw
eCi=(cid:9) + gt((cid:18))=(cid:9) (cid:0) gprior

i

eCj=(cid:9);

)) 3(cid:0)t

2

i

t

=

logt

expt

((cid:18))=(cid:9)

By substituting eCi in Eq.(42)  we obtain the marginal likelihood. Note that  if t = 1  the above

expression of ZEP is reduced to the ordinary marginal likelihood expression [9]. Therefore  this
marginal likelihood can be regarded as a generalization of the ordinary exponential family marginal
likelihood to the t-exponential family.
In Appendices D and E of the supplementary material  we derive speciﬁc EP algorithms for the
Bayes point machine (BPM) and Student-t process classiﬁcation.

:

(42)

5 Numerical Experiments

In this section  we numerically illustrate the behavior of our proposed EP applied to BPM and Student-
t process classiﬁcation. Suppose that data (x1; y1); : : : ; (xn; yn) are given  where yi 2 f+1;(cid:0)1g
expresses a class label for covariate xi. We consider a model whose likelihood term can be expressed
as

li(w) = p(yijxi; w) = ϵ + (1 (cid:0) 2ϵ)(cid:2)(yi⟨w; xi⟩);

(43)

where (cid:2)(x) is the step function taking 1 if x > 0 and 0 otherwise.

5.1 BPM

⊤

⊤

⊤

; 0:05I) + 0:25N (x; [(cid:0)1; 1]

; 0:05I) + 0:45N (x; [(cid:0)1;(cid:0)1]

We compare EP and ADF to conﬁrm that EP does not depend on data permutation. We gen-
erate a toy dataset in the following way: 1000 data points x are generated from Gaussian mix-
ture model 0:05N (x; [1; 1]
; 0:05I) +
0:25N (x; [1;(cid:0)1]
⊤
; 0:05I)  where N (x; (cid:22); (cid:6)) denotes the Gaussian density with respect to x with
mean (cid:22) and covariance matrix (cid:6)  and I is the identity matrix. For x  we assign label y = +1 when
; 0:05I) and label y = (cid:0)1 when x comes from
x comes from N (x; [1; 1]
N (x; [(cid:0)1; 1]
; 0:05I). We evaluate the dependence of the performance
of BPM (see Appendix D of the supplementary material for details) on data permutation.
Fig.1 shows labeled samples by blue and red points  decision boundaries by black lines which are
derived from ADF and EP for the Student-t distribution with v = 10 by changing data permutations.
The top two graphs show obvious dependence on data permutation by ADF (to clarify the dependence
on data permutation  we showed the most diﬀerent boundary in the ﬁgure)  while the bottom graph
exhibits almost no dependence on data permutations by EP.

; 0:05I) or N (x; [(cid:0)1;(cid:0)1]

; 0:05I) or N (x; [1;(cid:0)1]

⊤

⊤

⊤

⊤

7

Figure 2: Classiﬁcation boundaries.

5.2 Student-t Process Classiﬁcation
We compare the robustness of Student-t process classiﬁcation (STC) and Gaussian process classiﬁ-
cation (GPC) visually.
We apply our EP method to Student-t process binary classiﬁcation  where the latent function follows
the Student-t process (see Appendix E of the supplementary material for details). We compare this
model with Gaussian process binary classiﬁcation with the likelihood expressed Eq.(43). This kind
of model is called robust Gaussian process classiﬁcation [5]. Since the posterior distribution cannot
be obtained analytically even for the Gaussian process  we use EP for the ordinary exponential family
to approximate the posterior.
We use a two-dimensional toy dataset  where we generate a two-dimensional data point xi
(i = 1; : : : ; 300) following the normal distributions: p(xjyi = +1) = N (x; [1:5; 1:5]
⊤
; 0:5I)
and p(xjyi = (cid:0)1) = N (x; [(cid:0)1;(cid:0)1]
; 0:5I). We add eight outliers to the dataset and evaluate the
robustness against outliers (about 3% outliers). In the experiment  we used v = 10 for Student-t
processes. We furthermore used the following kernel:

⊤

{
(cid:0) D∑

}

k(xi; xj) = (cid:18)0 exp

(cid:18)d
1(xd
i

(cid:0) xd
j )2

+ (cid:18)2 + (cid:18)3(cid:14)i;j;

(44)

d=1

i is the dth element of xi  and (cid:18)0; (cid:18)1; (cid:18)2; (cid:18)3 are hyperparameters to be optimized.

where xd
Fig.2 shows the labeled samples by blue and red points  the obtained decision boundaries by black
lines  and added outliers by blue and red stars. As we can see  the decision boundaries obtained by
the Gaussian process classiﬁer is heavily aﬀected by outliers  while those obtained by the Student-t
process classiﬁer are more stable. Thus  as expected  Student-t process classiﬁcation is more robust

8

Table 1: Classiﬁcation Error Rates (%)

Table 2: Approximate log evidence

Dataset Outliers GPC
Pima

STC

Dataset Outliers
Pima

Ionosphere

Thyroid

Sonar

34.0(3.0) 32.3(2.6)
0
5% 34.9(3.1) 32.9(3.1)
10% 36.2(3.3) 34.4(3.5)
7.5(2.0)
9.6(1.7)
0
9.6(3.2)
9.9(2.8)
5%
10% 13.0(5.2) 11.9(5.4)
4.3(1.3)
0
4.4(1.3)
4.8(1.8)
5%
5.5(2.3)
10% 5.4(1.4)
7.2(3.4)
15.4(3.6) 15.0(3.2)
0
5% 18.3(4.4) 17.5(3.3)
10% 19.4(3.8) 19.4(3.1)

GPC

STC

0
-74.1(2.4) -37.1(6.1)
5% -77.8(2.9) -37.2(6.5)
10% -78.6(1.8) -36.8(6.5)
0
-59.5(5.2) -36.9(7.4)
5% -75.0(3.6) -35.8(7.0)
10% -90.3(5.2) -37.4(7.2)
0
-32.5(1.6) -41.2(4.3)
5% -39.1(2.3) -45.8(5.5)
10% -46.9(1.8) -45.8(4.5)
0
-55.8(1.2) -41.6(1.2)
5% -59.4(2.5) -41.3(1.6)
10% -65.8(1.1) -67.8(2.1)

Ionosphere

Thyroid

Sonar

against outliers compared to Gaussian process classiﬁcation  thanks to the heavy-tailed structure of
the Student-t distribution.

5.3 Experiments on the Benchmark dataset

We compared the performance of Gaussian process and Student-t process classiﬁcation on the UCI
datasets1. We used the kernel given in Eq.(44). The detailed explanation about experimental settings
are given in Appendix F. Results are shown in Tables 1 and 2  where outliers mean how many
percentages we randomly ﬂip training dataset labels to make additional outliers. As we can see
Student-t process classiﬁcation outperforms Gaussian process classiﬁcation in many cases.

6 Conclusions

In this work  we enabled the t-exponential family to inherit the important property of the exponential
family whose calculation can be eﬃciently performed thorough natural parameters by using the
q-algebra. With this natural parameter based calculation  we developed EP for the t-exponential
family by introducing the t-factorization approach. The key concept of our proposed approach is that
the t-exponential family has pseudo additivity. When t = 1  our proposed EP for the t-exponential
family is reduced to the original EP for the ordinary exponential family and t-factorization yields
the ordinary data-dependent factorization. Therefore  our proposed EP method can be viewed as a
generalization of the original EP. Through illustrative experiments  we conﬁrmed that our proposed
EP applied to the Bayes point machine can overcome the drawback of ADF  i.e.  the proposed EP
method is independent of data permutations. We also experimentally illustrated that proposed EP
applied to Student-t process classiﬁcation exhibited high robustness to outliers compared to Gaussian
process classiﬁcation. Experiments on benchmark data also demonstrated superiority of Student-t
process.
In our future work  we will further extend the proposed EP method to more general message passing
methods or double-loop EP. We would like also to make our method more scalable to large datasets
and develop another approximation method such as variational inference.

Acknowledgement

FF acknowledges support by JST CREST JPMJCR1403 and MS acknowledges support by KAKENHI
17H00757.

1https://archive.ics.uci.edu/ml/index.php

9

References
[1] Christopher M Bishop. Pattern Recognition and Machine Learning. Springer  2006.
[2] Nan Ding  Yuan Qi  and SVN Vishwanathan.

t-divergence based approximate inference. In

Advances in Neural Information Processing Systems  pages 1494–1502  2011.

[3] Nan Ding and SVN Vishwanathan. t-logistic regression. In Advances in Neural Information

Processing Systems  pages 514–522  2010.

[4] Pasi Jylänki  Jarno Vanhatalo  and Aki Vehtari. Robust Gaussian process regression with a

student-t likelihood. Journal of Machine Learning Research  12(Nov):3227–3257  2011.

[5] Hyun-Chul Kim and Zoubin Ghahramani. Outlier robust Gaussian process classiﬁcation. Struc-

tural  Syntactic  and Statistical Pattern Recognition  pages 896–905  2008.

[6] Thomas Peter Minka. A family of algorithms for approximate Bayesian inference. PhD Thesis 

Massachusetts Institute of Technology  2001.

[7] Laurent Nivanen  Alain Le Mehaute  and Qiuping A Wang. Generalized algebra within a

nonextensive statistics. Reports on Mathematical Physics  52(3):437–444  2003.

[8] Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine

Learning  volume 1. MIT press Cambridge  2006.

[9] Matthias Seeger. Expectation propagation for exponential families. Technical Report  2005.

URL https://infoscience.epfl.ch/record/161464/files/epexpfam.pdf

[10] Amar Shah  Andrew Wilson  and Zoubin Ghahramani. Student-t processes as alternatives to

gaussian processes. In Artiﬁcial Intelligence and Statistics  pages 877–885  2014.

[11] Hiroki Suyari and Makoto Tsukada. Law of error in Tsallis statistics. IEEE Transactions on

Information Theory  51(2):753–757  2005.

10

,Futoshi Futami
Issei Sato
Masashi Sugiyama