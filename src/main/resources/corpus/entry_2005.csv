2019,Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay,Belief propagation is a fundamental message-passing algorithm for probabilistic reasoning and inference in graphical models. While it is known to be exact on trees  in most applications belief propagation is run on graphs with cycles. Understanding the behavior of ``loopy'' belief propagation has been a major challenge for researchers in machine learning  and several positive convergence results for BP are known under strong assumptions which imply the underlying graphical model exhibits decay of correlations. We show that under a natural initialization  BP converges quickly to the global optimum of the Bethe free energy for Ising models on arbitrary graphs  as long as the Ising model is \emph{ferromagnetic} (i.e. neighbors prefer to be aligned). This holds even though such models can exhibit long range correlations and may have multiple suboptimal BP fixed points. We also show an analogous result for iterating the (naive) mean-field equations; perhaps surprisingly  both results are ``dimension-free'' in the sense that a constant number of iterations already provides a good estimate to the Bethe/mean-field free energy.,Fast Convergence of Belief Propagation to Global

Optima: Beyond Correlation Decay

Frederic Koehler

Department of Mathematics

Massachusetts Institute of Technology

Cambridge  MA 02141
fkoehler@mit.edu

Abstract

Belief propagation is a fundamental message-passing algorithm for probabilistic
reasoning and inference in graphical models. While it is known to be exact on
trees  in most applications belief propagation is run on graphs with cycles. Under-
standing the behavior of “loopy” belief propagation has been a major challenge for
researchers in machine learning and other ﬁelds  and positive convergence results
for BP are known under strong assumptions which imply the underlying graphi-
cal model exhibits decay of correlations. We show  building on previous work of
Dembo and Montanari  that under a natural initialization BP converges quickly to
the global optimum of the Bethe free energy for Ising models on arbitrary graphs 
as long as the Ising model is ferromagnetic (i.e. neighbors prefer to be aligned).
This holds even though such models can exhibit long range correlations and may
have multiple suboptimal BP ﬁxed points. We also show an analogous result for
iterating the (naive) mean-ﬁeld equations; perhaps surprisingly  both results are
dimension-free in the sense that a constant number of iterations already provides
a good estimate to the Bethe/mean-ﬁeld free energy.

1

Introduction

Undirected graphical models  also known as Markov Random Fields  are a general  powerful  and
popular framework for modeling and reasoning about high dimensional distributions. These models
explain the dependency structure of a probability distribution in terms of interactions along the edges
of a (hyper-) graph  which gives rise to a factorization of the joint probability distribution  and the
absence of edges in this graph encodes conditional independence relations.
Ising models are a special class of graphical models with a long history and which are popular in
applications; they model the interaction of random variables valued in a binary alphabet ({±1}) with
exclusively pairwise interactions. Explicitly  the joint pmf of an Ising model is

Pr(X = x) = exp

xT Jx + h · x − log Z

(1)

(cid:18) 1

2

(cid:19)

where x ∈ {±1}n  J : n × n is an arbitrary matrix describing the pairwise interactions between
nodes (with zero diagonal)  h is an arbitrary vector encoding node biases  and Z is a proportionality
constant called the partition function. Historically  Ising models originated in the statistical physics
community as a way to model and study phase transition phenomena in magnetic materials; since
then  they have attracted signiﬁcant interest in the machine learning community and have been ap-
plied in a wide variety of domains including ﬁnance  social networks  neuroscience  and computer
vision (see e.g. references in [18  23  30  11].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Performing sampling and inference on an Ising model is a major computational challenge for which
a wide variety of approaches have been developed. One family of methods are Markov-Chain Monte
Carlo (MCMC) algorithms  the most popular of which is Gibbs sampling (also known as Glauber
dynamics) which resamples the spin of one node at a time from its conditional distribution. When
run sufﬁciently long  MCMC methods will draw samples from the true distribution (1); unfortu-
nately  it is well-known in both theory and practice that MCMC methods may become stuck when
the probability distribution (1) exhibits multi-modal structure; for example  on an n×n square lattice
the Glauber dynamics required exponential time to mix in the low temperature phase [20].
A popular alternative to markov chain methods are variational methods  which typically make some
approximation to the distribution (1) but often run much faster than MCMC. These methods usually
reduce inference on the Ising model to some (typically non-convex) optimization problem  which is
solved either by standard optimization methods (e.g. gradient ascent) or by more specialized meth-
ods like message-passing algorithms (e.g. Belief Propagation). Because of the non-convexity  these
methods are typically not guaranteed to return global optimizers of the corresponding variational
problem. Indeed  these optimization problems are NP-hard to approximate for general Ising models
(see e.g. [12] for the case of mean-ﬁeld approximation).
Belief propagation (BP) is a celebrated message passing algorithm which is known to be closely
related to the Bethe approximation. It is a fundamental algorithm for probabilistic inference [25]
which plays a fundamental role in a variety of applications like phylogenetic reconstruction  coding 
constraint satisfaction problems  and community detection in the stochastic block model (see e.g.
[21  23  6]); it is also closely connected to the “cavity method” in statistical physics [21]. Although
BP is observed to works well for many problems  there are few settings on general graphs (i.e.
with loops) where it provably works. For example  BP with random initialization is conjectured to
achieve optimal reconstruction in the 2-community SBM [6] but no rigorous proof of this result is
known.
In this work  we consider two popular variational approximations  the naive mean-ﬁeld approxi-
mation and the Bethe approximation to the Ising model  and the corresponding heuristic message-
passing algorithms which are usually used to solve these optimization problems: mean-ﬁeld iteration
and belief propagation. We show that under a natural and popular assumption of ferromagneticity
(that Jij ≥ 0 and h has consistent signs; a.k.a. as an attractive graphical model) that these methods
do indeed converge to global optimizers of their optimization problems  under a natural initializa-
tion  and moreover that their convergence rate is fast and dimension-free in the appropriate sense.

1.1 Background: Variational methods and belief propagation

We can describe the variational methods we consider in terms of optimization problems whose goal
is to estimate Φ := log Z  the log partition function or free energy of the Ising model. This is natural
to consider because other important quantities can be recovered by differentiating log Z in some
parameter  and because the ability to construct sufﬁciently precise estimates for Z is equivalent to
approximate sampling for any self-reducible family of models [15]. We note throughout this section
we specialize to Ising models  but all of these notions generalize straightforwardly to general Markov
random ﬁelds (a.k.a. factor models) — see [21] for a more detailed discussion.
The starting point for these variational methods is the Gibbs variational principle [21] which states

EP [

1
2

xT Jx + h · x] + HP (X)

max

log Z =

P∈P({±1}n)

(2)
where P ranges over probability distributions on {±1}n and HP (X) is the entropy of X under P .
This formula is derived by observing the Gibbs measure minimizes the KL divergence to itself and
expanding.
The (naive) mean-ﬁeld approximation is given by restricting (2) to product distributions and ﬁnding
the maximum of the functional

(cid:18)

(cid:88)

i

(cid:18) 1 + xi

(cid:19)(cid:19)

2

ΦM F (x) :=

xT Jx + h · x +

1
2

H

Ber

(3)

where H(Ber(p)) = −p log p − (1 − p) log(1 − p) is the entropy of a Bernoulli random vari-
able. Information-theoretically  the optimizer(s) x∗ of this optimization problem corresponds to the

2

marginals of a product distribution ν which minimizes the KL-divergence from the Gibbs measure
µ (deﬁned by (1)) to ν. Note that this always gives a lower bound on log Z.
By considering the ﬁrst-order optimality conditions for (3)  one arises at the mean-ﬁeld equations

⊗n(J · x + h)

x = tanh

(4)
⊗n denotes entry-wise application of tanh. The mean-ﬁeld iteration is the natural
where tanh
iterative algorithm which starts with some x0 and applies (4) iteratively to search for a ﬁxed
point. The error of the mean-ﬁeld approximation has been extensively studied; the approxima-
tion is guaranteed to be accurate when (cid:107)J(cid:107)F = o(n) (informally  in unfrustrated models with
√
[2  13  1]. For example  the recent result of [1] shows that
large average degree); see e.g.
| log Z − maxx ΦM F (x)| = O(
n(cid:107)J(cid:107)F ) and the result of [10] gives even better bounds for some
models.
The naive mean-ﬁeld approximation can be inaccurate on very sparse graphs; the Bethe approxima-
tion is a more sophisticated approach which has the beneﬁt of being exact on trees [21]  and which
is always at least as accurate as the mean-ﬁeld approximation in ferromagnetic models [26]. The
Bethe free energy is the maximum of the (typically non-convex) functional

ΦBethe(P ) :=

JijEPij [XiXj]+

(deg(i)−1)HPi (Xi)
(5)
where P lies in the polytope of locally consistent distributions (equivalently  SA(2) in the Shereli-
Adams hierarchy1). Explicitly this polytope is given by constraints:

hiEPi[Xi]+

i

i

HPij (Xi  Xj)−(cid:88)

(cid:88)

i∼j

(cid:88)

(cid:88)

i∼j

(cid:88)

xi

(cid:88)

xi

Pij(xi  xj) = Pj(xj)

Pi(xi) = 1
Pi(xi) ≥ 0

for all i  j neighbors

for all i

for all i  xi

One can derive the Bethe-Peierls (BP) equations from the ﬁrst-order optimality conditions for this
optimization problem; this connection is involved and is discussed further in Section 3.1. Just like
the mean-ﬁeld equations  the BP equations can be iterated to search for a ﬁxed point  in which case
one recovers the belief propagation updates for this setting. Explicitly  for edge messages νi→j with
νi→j ∈ [−1  1] the consistency equation is

νi→j = tanh(hi +

tanh

−1(tanh(Jik)νk→i))

(6)

(cid:88)

k∈∂i\j

tions  the BP estimate for E[Xi] can be written as νi := tanh(hi+(cid:80)

Intuitively  this equation describes the expected
where ∂i denotes the neighborhood of node i.
marginal of node Xj in the absence of the edge between i and j. Given ν which solves these equa-
−1(tanh(Jik)νk→i)).
BP also gives an estimate for the free energy in terms of its messages (see equation (7) in Sec-
tion 3.1).
The above derivation of (“loopy”) belief propagation from the the Bethe free energy for general
graphical models is due to [36]. Alternatively  belief propagation can also be derived as the exact
solution to computing the partition function of a tree graphical model and this can be found in a
variety of places — see e.g. [25].

k∈∂i tanh

1.2 Our Results

We analyze the behavior of mean-ﬁeld iteration and belief propagation in ferromagnetic (a.k.a. at-
tractive) models on arbitrary graphs.
Deﬁnition 1.1. An Ising model is ferromagnetic (with consistent ﬁeld) if Jij ≥ 0 for all i and
hi ≥ 0 for every i. (We also can allow hi ≤ 0 for all i  but this is equivalent after ﬂipping signs.)

1The equivalence is given by treating the underlying graph as complete with Jij = 0 for unconnected nodes 

where the optimal coupling of these non-neighbors is when they are independent.

3

We show that in ferromagnetic Ising models  belief propagation and mean-ﬁeld iteration always
converge to the true global optimum of the Bethe free energy and mean-ﬁeld free energy respectively 
as long as they start from the all-1s initialization. Moreover we show that these algorithms converge
quickly  which makes them fast and practical algorithms for estimating the corresponding objective.
We note that these results cannot hold for arbitrary Ising models  as even approximating the mean-
ﬁeld free energy is NP-hard in general Ising models with anti-ferromagnetic interactions [12].
Theorem 1.2. Fix an arbitrary ferromagnetic Ising model parameterized by J  h and let x∗ be a
global maximizer of ΦM F . Initializing with x(0) = (cid:126)1 and deﬁning x(1)  x(2)  . . . by iterating the
mean-ﬁeld equations  we have that2 for every t ≥ 1 

(cid:40)(cid:107)J(cid:107)1 + (cid:107)h(cid:107)1

(cid:18)(cid:107)J(cid:107)1 + (cid:107)h(cid:107)1

(cid:19)4/3(cid:41)

.

  2

t

t

0 ≤ ΦM F (x∗) − ΦM F (x(t)) ≤ min

Theorem 1.3. Fix an arbitrary ferromagnetic Ising model parameterized by J  h  and let P ∗ be a
i→j = 1 for all i ∼ j and deﬁning ν(1)  ν(2)  . . . by BP
global maximizer of ΦBethe. Initializing ν(0)
iteration on a graph with m edges we have for every t ≥ 1 

0 ≤ ΦBethe(P ∗) − Φ∗

Bethe(ν(t)) ≤

(cid:114)

8mn(1 + (cid:107)J(cid:107)∞)

t

We also give simple lower bound examples showing these bounds are not too far from optimal;
for example  for both algorithms we show the optimal asymptotic rate in t is lower bounded by at
least Ω(1/t2). We refer to these bounds as dimension-independent because under the typical scaling
of the entries of J in a ferromagnetic model  they show that mean-ﬁeld iteration/BP achieve good
estimates to the variational free energy density after a constant number of iterations. We explain this
more precisely in the next remark:
Remark 1.4 (Scaling and Dimension-Free Nature). In ferromagnetic models  we usually expect
the scaling (cid:107)J(cid:107)1 = Θ(n) (cid:107)h(cid:107)1 = Θ(n) so that all of the terms in the Gibbs variational principle
(2) are on the same order  since the entropy scales like Θ(n) (e.g. the entropy of U ni({±1}n) is
n log(2)). Then the free energy log Z and its variational approximations all grow like Θ(n)  so
when considering the scaling in n one should consider the free energy density 1
n log Z. Writing
the guarantee of Theorem 1.2 for the free energy density when picking the O(1/t) bound  we see
0 ≤ 1
and the rhs is Θ(1/t) under the assumption (cid:107)J(cid:107)1 =
Θ(n) (cid:107)h(cid:107)1 = Θ(n). We get a similar dimension-free guarantee for BP as long as m = Θ(n)  i.e.
the model is sparse  and (cid:107)J(cid:107)∞ = O(1) which is a very rarely violated assumption.
Remark 1.5 (Importance of initialization). The fast convergence rates we show do not hold for
other seemingly natural choices of initialization; e.g. if we start BP with initial messages near zero.
For a concrete illustration of this  see Figure 2 in the Appendix.

n ΦM F (xt) ≤ (cid:107)J(cid:107)1+(cid:107)h(cid:107)1

n ΦM F (x∗) − 1

nt

Finally  we build on ideas developed in our analysis of BP to give a different method  based on
convex programming  which has worse dependence on n but converges exponentially fast  i.e. can
compute the optimal BP solution to error  in time poly(n  log(1/)). This is described in Ap-
pendix H; as we explain there  such a method is useful when we care about computing the optimal
BP solution accurately in parameter space  as there can be exponentially ﬂat (in terms of β) direc-
tions in the objective.

1.3 Related Work

As mentioned above  the general connection between the Bethe free energy and Belief Propagation
was established in the work of Yedidia  Freeman and Weiss [36]. They showed that in any graphical
model  the ﬁxed points of BP correspond to the critical points of the Bethe Free Energy. However 
their theory by itself does not say anything about the behavior of BP when it is not at a ﬁxed point
(as is the case during the BP iteration)  or which ﬁxed point (if any) it will converge to. In the
special case that the edge constraints are relatively weak  e.g. if they satisfy Dobrushin’s uniqueness
condition [9]  one can show that BP converges to a unique ﬁxed point by comparing to what happens
2In this theorem and throughout  we use the notation (cid:107)J(cid:107)1 (cid:107)J(cid:107)∞ to refer to the corresponding (cid:96)1  (cid:96)∞

norms of J when viewed as a vector of entries.

4

on a tree (see [29  22]). BP is also known to converge if the graph has at most one cycle [31].
Stronger results are known for BP in Gaussian graphical models  in which case BP can be viewed
as an iterative method for solving linear equations [32  19].
This work builds upon previous work of Dembo and Montanari [7]  who studied the convergence
of Belief Propagation in ferromagnetic Ising models with a positive external ﬁeld strictly bounded
away from 0. In their analysis they crucially showed that in all such models  BP converges at an
asymptotically exponential rate to a unique ﬁxed point if initialized with non-negative messages
(other ﬁxed points may exist  but they have at least one negative coordinate). As discussed in [7] 
this can be thought of as establishing an “average-case” version of correlation decay which goes
well beyond the usual “worst-case” setting (which would require there to be a unique global ﬁxed
point). From this they derived analytic results for graphs for Ising models which converge locally
to (random) trees: these models exhibit an average-case version of correlation decay  BP correctly
estimates the marginals (e.g. E[Xi]) of the Ising model  and from this derive that the “cavity predic-
tion” for the free energy  which is determined by the tree the graph locally converges to  is correct to
leading order. These analytic results were generalized in [8] beyond the Ising case (i.e. to non-binary
spins) using some new techniques.
In contrast  in this work we allow for the complete absence of external ﬁeld  in which case these
models may have multiple ﬁxed points  even in the space of nonnegative messages. Furthermore 
the optimal BP ﬁxed point often does not correspond to the true marginals of the underlying Ising
model3. Despite this  we show that BP converges quickly in objective value4 to the optimal ﬁxed
point as long as we start from the all-1s initialization. Another key difference is we are interested
in the behavior of BP on general graphs  where the BP estimate cannot always be related to the true
free energy; we get around this issue by building on the connection established in [36] to show that
the BP result is always equal to the Bethe free energy on any graph  not necessarily locally tree-like.
A very different line of work studies the dense limit of BP in spin glasses and related models  in
the form of the TAP approximation and Approximate Message Passing (AMP); for example  see
[4  3]. These results are more concerned with dense models with random edge weights and are mo-
tivated by CLT-type considerations; the models they consider are typically far from ferromagnetic
and thus require in the dense limit the TAP approximation instead of the naive mean ﬁeld approxi-
mation. Since we consider arbitrary graphs instead of (dense) random graphs  these techniques are
not applicable.
Outside of message passing algorithms  we note that in ferromagnetic Ising models it is actually
possible to sample efﬁciently from the Boltzmann distribution by using a special Markov chain
which performs non-local updates: this was proved in a landmark work of Jerrum and Sinclair [14].
This result can be used to give an algorithm for approximating the mean-ﬁeld free energy using
a graph blow-up reduction [12]. Also  for ferromagnetic models it was shown previously that the
Bethe free energy can be computed in polynomial time using discretization with submodularity-
based methods in [16  33]. The polynomials in the runtime guarantees for both methods are fairly
large compared to the message-passsing algorithms discussed in this work.

2 Convergence of Mean-Field Iteration

In this section  we give the proof of Theorem 1.2 by analyzing the mean-ﬁeld iteration. Organiza-
tionally  we split this theorem into two (corresponding to the two seperate bounds implied by the
min): we prove the ﬁrst bound in the theorem as Theorem 2.4 and the second O(1/t4/3) bound as
Theorem 2.6. Omitted proofs are deferred to Appendices A-C corresponding to each subsection.

3For example  if there is no external ﬁeld then E[Xi] = 0 for all i by symmetry  but e.g. on a 2D lattice
at low temperature one can see the optimal BP solution has different marginals [21]. Roughly  this kind of
behavior correspond to the existence of phase transitions at zero external ﬁeld (for say the random d-regular
graph)  which are ruled out in the case of strictly positive external ﬁeld by the Lee-Yang theorem [17].

4The convergence in parameter space may be slower due to ﬂat directions of the objective: see Appendix G.

This differs from the setting where the external ﬁeld is lower bounded by a positive constant [7].

5

2.1 Main convergence bound

In this section we prove the ﬁrst (O(1/t)) bound appearing in Theorem 1.2  the bound which is
better for small t; we consider this to be the more signiﬁcant bound because it gives a meaningful
convergence result even when t = O(1) (see Remark 1.4). A key observation in the proof is that the
functional ΦM F is actually concave on a certain subset of the space of product distributions  and that
the iteration stays in this region because the iteration is monotone w.r.t. the partial order structure;
this allows us to show progress at each step.
For the analysis of mean-ﬁeld iteration  it will be very helpful to split the updates up into two steps:

y(t+1) := Jx(t) + h
x(t+1) := tanh

⊗n(y(t+1)).

Lemma 2.1. A global maximizer of ΦM F is in [0  1]n.
Proof. For any x  if |x| denotes the coordinate wise absolute value then we observe ΦM F (x) ≤
ΦM F (|x|) since J  h are entrywise nonnegative and the entropy term is preserved. Therefore if x
is a global maximizer then so is |x|  and by compactness of [−1  1]n there exists at least one global
maximizer.
Lemma 2.2. There exists at most one critical point of ΦM F in (0  1]n.
Based on these lemmas  we deﬁne x∗ to be the global maximizer of ΦM F in [0  1]n. Deﬁne S :=
{x ∈ (0  1]n : xi ≥ x∗
i }.
Lemma 2.3. The mean-ﬁeld free energy functional ΦM F is concave on S.
Theorem 2.4 (Main bound in Theorem 1.2). Suppose that x0 ∈ S and deﬁne (x(t)  y(t))∞
iterating the mean-ﬁeld equations. Then for every t  x(t) ∈ S. Furthermore

t=1 by

ΦM F (x∗) − ΦM F (x(t)) ≤ (cid:107)J(cid:107)1 + (cid:107)h(cid:107)1

.

t

⊗n(Jx+h) ≤ tanh

Proof. To show that x(t) ∈ S  observe that the mean-ﬁeld iteration is monotone: if x ≤ x(cid:48)  then
⊗n(Jx∗+
tanh
h) ≤ tanh
To prove the convergence bound  ﬁrst note that ∂
∂xi
observe by Lemma 2.3 and concavity that

⊗n(Jx(cid:48)+h). Therefore  because x∗ ≤ x0 we see that x∗ = tanh

⊗n(Jx(0) + h) = x(1) and so on iteratively.

ΦM F (x) = Ji · x + hi − tanh

−1(xi) and then

ΦM F (x∗) − ΦM F (x(t)) ≤ (cid:104)∇ΦM F (x(t))  x∗ − xt(cid:105)

(cid:88)

≤ (cid:107)∇ΦM F (x(t))(cid:107)1
−1(x(t)
=

| tanh

i

i ) − (Jx(t) + h)i| =

(cid:88)

i

i − y(t+1)
y(t)

i

where the second inequality was by Hölder’s inequality and (cid:107)x∗ − x(t)(cid:107)∞ ≤ 1  and the last equality
follows from the deﬁnition of y(t) and because y(t+1) ≤ y(t) coordinate-wise. We can also see that
ΦM F (x(t)) is a monotonically increasing function of t by considering the path between x(t) and
x(t+1) which updates one coordinate at a time  as the gradient always has non-positive entries along
this path. Therefore if we sum over t we ﬁnd that

ΦM F (x∗)−ΦM F (x(T )) ≤ 1
T

since y(T +1)

i

≥ 0 and y(1)

T(cid:88)
i ≤(cid:80)
j Jij + hi ≤ (cid:107)Ji(cid:107)1 + hi.

(ΦM F (x∗)−ΦM F (x(t))) ≤ 1
T

t=1

n(cid:88)

i=1

i −y(T +1)
(y(1)

i

) ≤ (cid:107)J(cid:107)1 + (cid:107)h(cid:107)1

T

The following simple example shows that the above result is not too far from optimal  in the sense
that an asymptotic rate of o(1/t2) is impossible. We take advantage of the fact that when the model
is completely symmetrical  the behavior of the update can be reduced to a 1-dimensional recursion 
which is a standard trick (see e.g. [21  24]).

6

Example 2.5. Consider any d-regular graph with no external ﬁeld and edge weight β = 1/d  which
corresponds to the naive mean ﬁeld prediction for the critical temperature. By symmetry  analyzing
the mean ﬁeld iteration reduces to the 1d recursion x (cid:55)→ tanh(x) which behaves like x (cid:55)→ x− x3/3
√
near the ﬁxed point x = 0. Solving this recurrence  we see that x converges to 0 at rate Θ(1/
t).
In terms of x  the estimated mean ﬁeld free energy is (n/2)x2 + nH( 1+x
2 )  so by expanding we see
that the estimated free energy converges at a Θ(1/t2) rate in this example.

2.2 A Faster Asymptotic Rate

The above theorem and lower bound leave a gap between O(1/t) and Ω(1/t2) for the asymptotic
rate of the mean-ﬁeld iteration. This section is devoted to showing that for large t  we can obtain an
improved asymptotic rate of O(1/t4/3) for the mean-ﬁeld iteration using a slightly more involved
variant of the argument from the previous section. The key insight is that we can obtain some control
of (cid:107)x − x∗(cid:107)∞ by consider the behavior of higher-order terms when expanding around x∗  and this
can be used to get better bounds on the convergence in objective.
Theorem 2.6 (Second bound in Theorem 1.2). Suppose that x0 ∈ S and deﬁne (xt  yt)∞
iterating the mean-ﬁeld equations. Then for every t  xt ∈ S. Furthermore for any t ≥ 1 

t=1 by

and

(cid:107)xt − x∗(cid:107)3∞ ≤ (cid:107)J(cid:107)1 + (cid:107)h(cid:107)1
(cid:18)(cid:107)J(cid:107)1 + (cid:107)h(cid:107)1

t

(cid:19)4/3

.

ΦM F (x∗) − ΦM F (x2t) ≤

t

2.3 Aside: Computing the Mean-Field Optimum given Inconsistent Fields

In this section we describe a polynomial time algorithm to compute the optimal mean-ﬁeld approx-
imation even in the situation when the external ﬁelds have inconsistent signs (i.e. some of the hi
are negative  some are positive). This is by reduction to the following algorithmic result of [27] 
following the same strategy as [16  33] for the case of the Bethe free energy. We include this result
as we were not aware of it appearing explicitly in the literature  though it is known at least as a
“folk-lore” result.
Theorem 2.7. Fix an Ising model with ferromagnetic interactions (Jij ≥ 0) and arbitrary (not
necessarily consistent) external ﬁeld h. Then the mean-ﬁeld free energy maxx ΦM F (x) can be
approximated within error n in time poly(1/  n (cid:107)J(cid:107)1 (cid:107)h(cid:107)1).

3 Rapid Convergence of Belief Propagation

In this section  we give the proof of our main result Theorem 1.3 by analyzing belief propagation.
This proof is considerably more involved than the case of the mean-ﬁeld iteration; a major concep-
tual difference between the two iterations is that the mean-ﬁeld iteration always maintains a valid
product distribution  and so can be understood in terms of the landscape of ΦM F   whereas BP oper-
ates on “dual” variables which do not correspond to valid pseudodistributions except at ﬁxed points 
so analyzing the landscape of ΦBethe by itself does not sufﬁce. We get around this by also consid-
ering the behavior of a dual functional Φ∗
Bethe which is well-deﬁned for every set of BP messages;
however this functional is poorly behaved in general (it can be unbounded and its critical points are
typically saddle points). We are able to handle these difﬁculties by identifying the special behavior
of BP and Φ∗
Bethe on two special subsets of the BP messages arising from the partial order struc-
ture: the pre-ﬁxpoints and post-ﬁxpoints. Finally  when analyzing BP in these regions we are able to
relate in a useful way its behavior at different values of external ﬁeld  enabling us to use a convexity
argument from [7] which cannot be directly applied to our setting.
Omitted proofs are deferred to Appendices D-F corresponding to each subsection.

3.1 Background: BP and the Bethe Free Energy

In this section we recall the necessary facts we need about the relationship derived in [36] between
the Bethe free energy and BP. This relationship and corresponding formulas are a bit involved so we
sketch the derivation in Appendix D.

7

Following the correspondence outlined in [36]  by writing down the Lagrangian corresponding to
the optimization problem (5) over the polytope of locally consistent distributions  one can derive
an expression for the Bethe free energy at a critical point of the Lagrangian in terms of the dual
variables (Lagrange multipliers) — see [21  35]. After a change of variables to ν  this lets us deﬁne
(for all ν  not necessarily ﬁxed points of the BP equations)  the dual Bethe free energy

where

Fi(ν) = log

ehi (cid:89)

j∈∂i

Φ∗
Bethe(ν) :=

(1 + tanh(Jij)νj→i) + e−hi (cid:89)

(1 − tanh(Jij)νj→i)

j∈∂i

 +

(7)

(cid:88)

j∈∂i

log cosh(Jij)

(cid:88)

Fi(ν) −(cid:88)

i

Fij(ν).

i∼j

and Fij(ν) = log(1 + tanh(Jij)νi→jνj→i) + log cosh(Jij). We remark (see [21]) that in the case
the graph is a tree  it’s known that the Bethe free energy is a convex function  so Φ∗
Bethe(ν) plus the
Lagrange multiplier terms is actually the Lagrangian dual and thus has a natural interpretation for
all ν. This is not true in general  however we will soon see that Φ∗
Bethe does have useful properties
on some special subsets of the space of messages.

3.2 Properties of BP and Optimization Landscape

Throughout the remainder of the paper we will use the notation φ(ν) to denote the result of perform-
ing a single iteration of belief propagation from ν  i.e.

hi +
(cid:88)
Lemma 3.1. Suppose that f (x) = tanh(h +(cid:80)

φ(ν)i→j := tanh

k∈∂i\j

 .

tanh

−1(tanh(Jik)νk→i)

The following lemma implies that φ(ν)i→j is a concave monotone function for nonnegative ν.

−1(xi)) for any h ≥ 0. Then f is a concave
monotone function on the domain [0  1)n. Furthermore ∇2f (x) ≺ 0 unless h = 0 and | supp(x)| ≤
1.

i tanh

There are two special subsets of the nonnegative messages which will play key roles in our analysis.
They are the set of pre-ﬁxpoints and post-ﬁxpoints (following standard poset terminology)  deﬁned
by

Spre = {ν : 0 ≤ φ(ν)i→j ≤ νi→j}  Spost = {ν : 0 ≤ νi→j ≤ φ(ν)i→j}.

Bethe(ν)(cid:107)∞ ≤ 1. Furthermore  if ν ∈ Spre then ∇Φ∗

Note that both sets contain the nonnegative ﬁxed points; also we note from Lemma 3.1 that Spost is
a convex set  whereas Spre is typically non-convex and even disconnected. The gradient of Φ∗
Bethe
is well-behaved on these sets:
Lemma 3.2. For any ν ≥ 0  (cid:107)∇Φ∗
and if ν ∈ Spost then ∇Φ∗
The Knaster-Tarski theorem [28] shows that the ﬁxed points of φ must form a complete lattice  and
in particular shows that a greatest ﬁxed point must exist; the following lemma identiﬁes it explicitly.
Lemma 3.3. Suppose that BP is run from initial messages ν(0)
i→j = 1. The messages converge to a
ﬁxed point ν∗ of the BP equations such that for any other ﬁxed point µ  µi→j ≤ ν∗
i→j for all i  j.
Furthermore

Bethe(ν) ≥ 0.

Bethe(ν) ≤ 0

Bethe(ν∗) = max
Φ∗
ν∈Spost

Φ∗
Bethe(ν)

The following key theorem states that ν∗ is a global optimum; its proof involves intricate analysis
of the Bethe free energy objective and is left to the appendix. The high level idea is similar to the
previous situation with mean ﬁeld approximation — since J  h are nonnegative  based on the form of
the Bethe free energy we would guess that the optimizing pseudodistribution exhibits only positive
correlations  and that this should be reﬂected in the optimum BP ﬁxed point having nonnegative
messages.
Theorem 3.4. The maximal ﬁxed point ν∗ (as deﬁned in Lemma 3.3) corresponds to a global max-
imizer of the Bethe free energy.

8

Bethe(ν(0)) ≤ Φ∗

Bethe(ν(T )).

Bethe(µ) ≤ Φ∗

Bethe behaves nicely with respect to BP for messages in Spre:

3.3 Convergence rate of belief propagation
A priori  there is no signiﬁcance to the value of Φ∗
Bethe on a general (non-ﬁxed point) ν. However 
we observe that Φ∗
Lemma 3.5. Suppose that ν(0) ∈ Spre and deﬁne the BP iterates ν(t+1) := φ(ν(t)). Then for
any T ≥ 0 and any µ such that ν(T ) ≤ µ ≤ ν(0) it follows that Φ∗
Bethe(ν(T )). In
particular  Φ∗
In order to give a quantitative bound on the convergence of BP  we are faced with an important
conceptual difﬁculty: the BP messages may not converge quickly in parameter space  but if the BP
messages are far from a ﬁxed point in parameter space it is hard to show anything about the quality
of their estimate to the free energy. We overcome this difﬁculty by relating the behavior of BP at
zero external ﬁeld and with additional positive external ﬁeld  which allows us to take advantage
of the smoothness of the primal objective ΦBethe — Lemma 3.5 and Theorem 3.4 are key tools
needed to make this connection work. This trick is similar in spirit to the use of monotone couplings
in the proof of various correlation inequalities for the Ising model. This  combined with a useful
concavity argument from [7] for analyzing the positive external ﬁeld setting  allow us to ultimately
prove Theorem 1.3. The detailed proof is in Appendix F.
Lower bounds and examples: We give examples which illustrate the importance of distinguishing
rates in parameter space vs. objective space  the importance of initialization at all-ones  and also
give lower bounds on the asymptotic convergence rate of BP in Appendix G.
A method with exponentially fast convergence: As mentioned in the introduction  we use the
insights developed in our analysis of BP (especially  the nice structure of the set of the set of post-
ﬁxpoints Spost) to give a different method with much faster asymptotic convergence (but with a
runtime that depends more signiﬁcantly on the dimension): this is in Appendix H.
Acknowledgements: The author thanks Vishesh Jain for suggesting the argument in Section 2.2 
Elchanan Mossel  Nike Sun  Matthew Brennan  and Enric Boix for helpful discussions related to
this work  and Ankur Moitra and Andrej Risteski for useful discussions on related topics.

References
[1] Fanny Augeri. A transportation approach to the mean-ﬁeld approximation. arXiv preprint

arXiv:1903.08021  2019.

[2] Anirban Basak and Sumit Mukherjee. Universality of the mean-ﬁeld for the potts model.

Probability Theory and Related Fields  168(3-4):557–600  2017.

[3] Mohsen Bayati and Andrea Montanari.

The dynamics of message passing on dense
graphs  with applications to compressed sensing. IEEE Transactions on Information Theory 
57(2):764–785  2011.

[4] Erwin Bolthausen. An iterative construction of solutions of the tap equations for the
sherrington–kirkpatrick model. Communications in Mathematical Physics  325(1):333–366 
2014.

[5] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and

Trends R(cid:13) in Machine Learning  8(3-4):231–357  2015.

[6] A. Decelle  F. Krzakala  C. Moore  and L. Zdeborová. Asymptotic analysis of the stochas-
tic block model for modular networks and its algorithmic applications. Physics Review E 
84:066106  Dec 2011.

[7] Amir Dembo and Andrea Montanari.

Probab.  20(2):565–592  2010.

Ising models on locally tree-like graphs. Ann. Appl.

[8] Amir Dembo  Andrea Montanari  Nike Sun  et al. Factor models on locally tree-like graphs.

The Annals of Probability  41(6):4162–4213  2013.

[9] Roland Lvovich Dobrushin. The description of a random ﬁeld by means of conditional proba-

bilities and conditions of its regularity. Theor. Prob. Appl.  13:197–224  1968.

9

[10] Ronen Eldan. Taming correlations through entropy-efﬁcient measure decompositions with

applications to mean-ﬁeld approximation. arXiv preprint arXiv:1811.11530  2018.

[11] Geoffrey E Hinton. A practical guide to training restricted boltzmann machines. In Neural

networks: Tricks of the trade  pages 599–619. Springer  2012.

[12] Vishesh Jain  Frederic Koehler  and Elchanan Mossel. The mean-ﬁeld approximation: Infor-
mation inequalities  algorithms  and complexity. In Conference on Learning Theory (COLT) 
2018.

[13] Vishesh Jain  Frederic Koehler  and Andrej Risteski. Mean-ﬁeld approximation  convex hier-
archies  and the optimality of correlation rounding: a uniﬁed perspective. In Proceedings of
the Symposium on Theory of Computing (STOC)  2019.

[14] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for ising model (ex-

tended abstract). In Automata  Languages and Programming  pages 462–475  1990.

[15] Mark R Jerrum  Leslie G Valiant  and Vijay V Vazirani. Random generation of combinatorial

structures from a uniform distribution. Theoretical Computer Science  43:169–188  1986.

[16] Filip Korˇc  Vladimir Kolmogorov  Christoph H Lampert  et al. Approximating marginals using
discrete energy minimization. In ICML Workshop on Inferning: Interactions between Inference
and Learning. Citeseer  2012.

[17] Tsung-Dao Lee and Chen-Ning Yang. Statistical theory of equations of state and phase transi-

tions. ii. lattice gas and ising model. Physical Review  87(3):410  1952.

[18] Stan Z Li. Markov random ﬁeld modeling in image analysis. Springer Science & Business

Media  2009.

[19] Dmitry M Malioutov  Jason K Johnson  and Alan S Willsky. Walk-sums and belief propagation
in gaussian graphical models. Journal of Machine Learning Research  7(Oct):2031–2064 
2006.

[20] Fabio Martinelli. Lectures on glauber dynamics for discrete spin models.

probability theory and statistics  pages 93–191. Springer  1999.

In Lectures on

[21] M. Mézard and A. Montanari.

Press  USA  2009.

Information  physics  and computation. Oxford University

[22] J. M. Mooij and H. J. Kappen. Sufﬁcient conditions for convergence of the sum-product algo-

rithm. IEEE Transactions on Information Theory  53(12):4422–4437  Dec 2007.

[23] Elchanan Mossel. Survey-information ﬂow on trees. DIMACS series in discrete mathematics

and theoretical computer science  63:155–170  2004.

[24] Giorgio Parisi. Statistical ﬁeld theory. New York: Addison-Wesley  1988.

[25] J. Pearl. Probabilistic reasoning in intelligent systems. Morgan Kaufman  San Mateo  1988.

[26] Nicholas Ruozzi. The bethe partition function of log-supermodular graphical models.

Advances in Neural Information Processing Systems  pages 117–125  2012.

In

[27] Dmitrij Schlesinger and Boris Flach. Transforming an arbitrary minsum problem into a binary

one. TU  Fak. Informatik  2006.

[28] Alfred Tarski et al. A lattice-theoretical ﬁxpoint theorem and its applications. Paciﬁc journal

of Mathematics  5(2):285–309  1955.

[29] S. Tatikonda and M. I. Jordan. Loopy belief propagation and gibbs measures. In Uncertainty

in Artiﬁcial Intelligence (UAI)  Proceedings of the Eighteenth Conference. 2002.

[30] Martin J. Wainwright and Michael I. Jordan. Graphical models  exponential families  and

variational inference. Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

10

[31] Yair Weiss. Correctness of local probability propagation in graphical models with loops. Neu-

ral computation  12(1):1–41  2000.

[32] Yair Weiss and William T Freeman. Correctness of belief propagation in gaussian graphical
models of arbitrary topology. In Advances in neural information processing systems  pages
673–679  2000.

[33] Adrian Weller and Tony Jebara. Bethe bounds and approximating the global optimum.

Artiﬁcial Intelligence and Statistics  pages 618–631  2013.

In

[34] Max Welling and Yee Whye Teh. Belief optimization for binary networks: A stable alternative
to loopy belief propagation. In Proceedings of the Seventeenth conference on Uncertainty in
artiﬁcial intelligence  pages 554–561. Morgan Kaufmann Publishers Inc.  2001.

[35] Tomáš Werner. Primal view on belief propagation. In Proceedings of the Twenty-Sixth Con-

ference on Uncertainty in Artiﬁcial Intelligence  pages 651–657. AUAI Press  2010.

[36] J. S. Yedidia  W. T. Freeman  and Y. Weiss. Understading belief propogation and its gener-
In Exploring Artiﬁcial Intelligence in the New Millenium  Annals of mathemat-
alizations.
ics studies  no. 34  pages 239–326. Science & Technology Books  2003. Availible online at
http://www.merl.com/papers/TR2002-35/.

11

,Frederic Koehler