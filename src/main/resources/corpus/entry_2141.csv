2017,Flexible statistical inference for mechanistic models of neural dynamics,Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However  identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation  ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore  we propose an efficient approach for handling missing features and parameter settings for which the simulator fails  as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data  our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages  we recover multivariate posteriors over biophysical parameters  which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms  closing the gap between mechanistic and statistical approaches to single-neuron modelling.,Flexible statistical inference for mechanistic models of

neural dynamics

Jan-Matthis Lueckmann∗ 1  Pedro J. Gonçalves∗ 1  Giacomo Bassetto1 

Kaan Öcal1 2  Marcel Nonnenmacher1  Jakob H. Macke†1

1 research center caesar  an associate of the Max Planck Society  Bonn  Germany

2 Mathematical Institute  University of Bonn  Bonn  Germany

{jan-matthis.lueckmann  pedro.goncalves  giacomo.bassetto 
kaan.oecal  marcel.nonnenmacher  jakob.macke}@caesar.de

Abstract

Mechanistic models of single-neuron dynamics have been extensively studied in
computational neuroscience. However  identifying which models can quantitatively
reproduce empirically measured data has been challenging. We propose to over-
come this limitation by using likelihood-free inference approaches (also known
as Approximate Bayesian Computation  ABC) to perform full Bayesian inference
on single-neuron models. Our approach builds on recent advances in ABC by
learning a neural network which maps features of the observed data to the poste-
rior distribution over parameters. We learn a Bayesian mixture-density network
approximating the posterior over multiple rounds of adaptively chosen simulations.
Furthermore  we propose an efﬁcient approach for handling missing features and
parameter settings for which the simulator fails  as well as a strategy for automati-
cally learning relevant features using recurrent neural networks. On synthetic data 
our approach efﬁciently estimates posterior distributions and recovers ground-truth
parameters. On in-vitro recordings of membrane voltages  we recover multivariate
posteriors over biophysical parameters  which yield model-predicted voltage traces
that accurately match empirical data. Our approach will enable neuroscientists to
perform Bayesian inference on complex neuron models without having to design
model-speciﬁc algorithms  closing the gap between mechanistic and statistical
approaches to single-neuron modelling.

Introduction

1
Biophysical models of neuronal dynamics are of central importance for understanding the mechanisms
by which neural circuits process information and control behaviour. However  identifying which
models of neural dynamics can (or cannot) reproduce electrophysiological or imaging measurements
of neural activity has been a major challenge [1]. In particular  many models of interest – such as
multi-compartment biophysical models [2]  networks of spiking neurons [3] or detailed simulations
of brain activity [4] – have intractable or computationally expensive likelihoods  and statistical
inference has only been possible in selected cases and using model-speciﬁc algorithms [5  6  7]. Many
models are deﬁned implicitly through simulators  i.e. a set of dynamical equations and possibly a
description of sources of stochasticity [1]. In addition  it is often of interest to identify models which
can reproduce particular features in the data  e.g. a ﬁring rate or response latency  rather than the full
temporal structure of a neural recording.

∗Equal contribution
†Current primary afﬁliation: Centre for Cognitive Science  Technical University Darmstadt

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Flexible likelihood-free inference for models of neural dynamics. A. We want to
ﬂexibly and efﬁciently infer the posterior over model parameters given observed data  on a wide
range of models of neural dynamics. B. Our method approximates the true posterior on θ around the
observed data xo by performing density estimation on data simulated using a proposal prior. C. We
train a Bayesian mixture-density network (MDN) for posterior density estimation.

In the absence of likelihoods  the standard approach in neuroscience has been to use heuristic
parameter-ﬁtting methods [2  8  9]: distance measures are deﬁned on multiple features of interest 
and brute-force search [10  11] or evolutionary algorithms [2  9  12  13] (neither of which scales to
high-dimensional parameter spaces) are used to minimise the distances between observed and model-
derived features. As it is difﬁcult to trade off distances between different features  the state-of-the-art
methods optimise multiple objectives and leave the ﬁnal choice of a model to the user [2  9]. As
this approach is not based on statistical inference  it does not provide estimates of the full posterior
distribution – thus  while this approach has been of great importance for identifying ‘best ﬁtting’
parameters  it does not allow one to identify the full space of parameters that are consistent with data
and prior knowledge  or to incrementally reﬁne and reject models.
Bayesian inference for likelihood-free simulator models  also known as Approximate Bayesian
Computation [14  15  16]  provides an attractive framework for overcoming these limitations: like
parameter-ﬁtting approaches in neuroscience [2  8  9]  it is based on comparing summary features
between simulated and empirical data. However  unlike them  it provides a principled framework for
full Bayesian inference and can be used to determine how to trade off goodness-of-ﬁt across summary
statistics. However  to the best of our knowledge  this potential has not been realised yet  and ABC
approaches are not used for linking mechanistic models of neural dynamics with experimental data
(for an exception  see [17]). Here  we propose to use ABC methods for statistical inference of
mechanistic models of single neurons. We argue that ABC approaches based on conditional density
estimation [18  19] are particularly suited for neuroscience applications.
We present a novel method (Sequential Neural Posterior Estimation  SNPE) in which we sequentially
train a mixture-density network across multiple rounds of adaptively chosen simulations1. Our
approach is directly inspired by prior work [18  19]  but overcomes critical limitations: ﬁrst  a
ﬂexible mixture-density network trained with an importance-weighted loss function enables us to
use complex proposal distributions and approximate complex posteriors. Second  we represent a full
posterior over network parameters of the density estimator (i.e. a “posterior on posterior-parameters”)
which allows us to take uncertainty into account when adjusting weights. This enables us to perform
‘continual learning’  i.e. to effectively utilise all simulations without explicitly having to store them.
Third  we introduce an approach for efﬁciently dealing with simulations that return missing values 
or which break altogether – a common situation in neuroscience and many other applications of
simulator-based models – by learning a model that predicts which parameters are likely to lead to
breaking simulations  and using this knowledge to modify the proposal distribution. We demonstrate
the practical effectiveness and importance of these innovations on biophysical models of single
neurons  on simulated and neurophysiological data. Finally  we show how recurrent neural networks
can be used to directly learn relevant features from time-series data.

1Code available at https://github.com/mackelab/delﬁ

2

θproposal priorpriortrue posteriorposteriorxoxα1αK…μ1μK…λ1λK…f1(s)f2(s)fI(s)…h2h3hH…h1MixtureweightsMeansPrecisionfactorsforward passfeature 1…ABC1.1 Related work using likelihood-free inference for simulator models
Given experimental data xo (e.g. intracellular voltage measurements of a single neuron  or extra-
cellular recordings from a neural population)  a model p(x|θ) parameterised by θ (e.g. biophysical
parameters  or connectivity strengths in a network simulation) and a prior distribution p(θ)  our goal
is to perform statistical inference  i.e. to ﬁnd the posterior distribution ˆp(θ|x = xo). We assume that
the model p(x|θ) is only deﬁned through a simulator [14  15]: we can generate samples xn ∼ x|θ
from it  but not evaluate p(x|θ) (or its gradients) explicitly. In neural modelling  many models are
deﬁned through speciﬁcation of a dynamical system with external or intrinsic noise sources or even
through a black-box simulator (e.g. using the NEURON software [20]).
In addition  and in line with parameter-ﬁtting approaches in neuroscience and most ABC techniques
[14  15  21]  we are often interested in capturing summary statistics of the experimental data (e.g.
ﬁring rate  spike-latency  resting potential of a neuron). Therefore  we can think of x as resulting
from applying a feature function f to the raw simulator output s  x = f (s)  with dim(x) (cid:28) dim(s).
Classical ABC algorithms simulate from multiple parameters  and reject parameter sets which yield
data that are not within a speciﬁed distance from the empirically observed features. In their basic
form  proposals are drawn from the prior (‘rejection-ABC’ [22]). More efﬁcient variants make
use of a Markov-Chain Monte-Carlo [23  24] or Sequential Monte-Carlo (SMC) samplers [25  26].
Sampling-based ABC approaches require the design of a distance metric on summary features  as
well as a rejection criterion (ε)  and are exact only in the limit of small ε (i.e. many rejections) [27] 
implying strong trade-offs between accuracy and scalability. In SMC-ABC  importance sampling is
used to sequentially sample from more accurate posteriors while ε is gradually decreased.
Synthetic-likelihood methods [28  21  29] approximate the likelihood p(x|θ) using multivariate
Gaussians ﬁtted to repeated simulations given θ (see [30  31] for generalisations). While the
Gaussianity assumption is often motivated by the central limit theorem  distributions over features can
in practice be complex and highly non-Gaussian [32]. For example  neural simulations sometimes
result in systematically missing features (e.g. spike latency is undeﬁned if there are no spikes)  or
diverging ﬁring rates.
Finally  methods originating from regression correction [33  18  19] simulate multiple data xn from
different θn sampled from a proposal distribution ˜p(θ)  and construct a conditional density estimate
q(θ|x) by performing a regression from simulated data xn to θn. Evaluating this density model at
the observed data xo  q(θ|xo) yields an estimate of the posterior distribution. These approaches do
not require parametric assumptions on likelihoods or the choice of a distance function and a tolerance
(ε) on features. Two approaches are used for correcting the mismatch between prior and proposal
distributions: Blum and François [18] proposed the importance weights p(θ)/˜p(θ)  but restricted
themselves to proposals which were truncated priors (i.e. all importance weights were 0 or 1)  and did
not sequentially optimise proposals over multiple rounds. Papamakarios and Murray [19] recently
used stochastic variational inference to optimise the parameters of a mixture-density network  and a
post-hoc division step to correct for the effect of the proposal distribution. While highly effective in
some cases  this closed-form correction step can be numerically unstable and is restricted to Gaussian
and uniform proposals  limiting both the robustness and ﬂexibility of this approach. SNPE builds on
these approaches  but overcomes their limitations by introducing four innovations: a highly ﬂexible
proposal distribution parameterised as a mixture-density network  a Bayesian approach for continual
learning from multiple rounds of simulations  and a classiﬁer for predicting which parameters will
result in aborted simulations or missing features. Fourth  we show how this approach  when applied
to time-series data of single-neuron activity  can automatically learn summary features from data.
2 Methods
2.1 Sequential Neural Posterior Estimation for likelihood-free inference
In SNPE  our goal is to learn the parameters φ of a posterior model qφ(θ|x = f (s)) which  when
evaluated at xo  approximates the true posterior p(θ|xo) ≈ qφ(θ|x = xo). Given a prior p(θ)  a
proposal prior ˜p(θ)  pairs of samples (θn  xn) generated from the proposal prior and the simulator 
and a calibration kernel Kτ   the posterior model can be trained by minimising the importance-
weighted log-loss

(cid:88)

n

L(φ) = − 1
N

p(θn)
˜p(θn)

Kτ (xn  xo) log qφ(θn|xn) 

(1)

3

We sequentially optimise the density estimator qφ(θ|x) =(cid:80)

as is shown by extending the argument in [19] with importance-weights p(θn)/˜p(θn) and a kernel
Kτ in Appendix A.
Sampling from a proposal prior can be much more effective than sampling from the prior. By
including the importance weights in the loss  the analytical correction step of [19] (i.e. division by
the proposal prior) becomes unnecessary: SNPE directly estimates the posterior density rather than
a conditional density that is reweighted post-hoc. The analytical step of [19] has the advantage of
side-stepping the additional variance brought about by importance-weights  but has the disadvantages
of (1) being restricted to Gaussian proposals  and (2) the division being unstable if the proposal prior
has higher precision than the estimated conditional density.
The calibration kernel Kτ (x  xo) can be used to calibrate the loss function by focusing it on simulated
data points x which are close to xo [18]. Calibration kernels Kτ (x  xo) are to be chosen such that
Kτ (xo  xo) = 1 and that Kτ decreases with increasing distance (cid:107)x − xo(cid:107)  given a bandwidth τ 2.
Here  we only used calibration kernels to exclude bad simulations by assigning them kernel value
zero. An additional use of calibration kernels would be to limit the accuracy of the posterior density
estimation to a region near xo. Choice of the bandwidth implies a bias-variance trade-off [18]. For
the problems we consider here  we assumed our posterior model qφ(θ|x) based on a multi-layer
neural network to be sufﬁciently ﬂexible  such that limiting bandwidth was not necessary.
k αkN (θ|µk  Σk) by training a mixture-
density network (MDN) [19] with parameters φ over multiple ‘rounds’ r with adaptively chosen
proposal priors ˜p(r)(θ) (see Fig. 1). We initialise the proposal prior at the prior  ˜p(1)(θ) = p(θ) 
and subsequently take the posterior of the previous round as the next proposal prior (Appendix B).
Our approach is not limited to Gaussian proposals  and in particular can utilise multi-modal and
heavy-tailed proposal distributions.
2.2 Training the posterior model with stochastic variational inference
To make efﬁcient use of simulation time  we want the posterior network qφ(θ|x) to use all simulations 
including ones from previous rounds. For computational and memory efﬁciency  it is desirable to
avoid having to store all old samples  or having to train a new model at each round. To achieve this
goal  we perform Bayesian inference on the weights w of the MDN across rounds. We approximate
the distribution over weights as independent Gaussians [34  35]. Note that the parameters φ of this
Bayesian MDN are are means and standard deviations per each weight  i.e.  φ = {φm  φs}. As an
extension to the approach of [19]  rather than assuming a zero-centred prior over weights  we use
the posterior over weights of the previous round  πφ(r−1) (w)  as a prior for the next round. Using
stochastic variational inference  in each round  we optimise the modiﬁed loss

(cid:88)

n

DKL

L(φ(r)) = − 1
N

+

1
N

Kτ (xn  xo)(cid:10) log qw(θn|xn)(cid:11)

p(θn)
˜p(r)(θn)

(cid:0)πφ(r)(w)||πφ(r−1) (w)(cid:1) .

π

φ(r) (w)

(2)

Here  the distributions π(w) are approximated by multivariate normals with diagonal covariance. The
continuity penalty ensures that MDN parameters that are already well constrained by previous rounds
are less likely to be updated than parameters with large uncertainty (see Appendix C). In practice 
gradients of the expectation over networks are approximated using the local reparameterisation trick
[36].
2.3 Dealing with bad simulations and bad features  and learning features from time series
Bad simulations: Simulator-based models  and single-neuron models in particular  frequently
generate nonsensical data (which we name ‘bad simulations’)  especially in early rounds in which the
relevant region of parameter space has not yet been found. For example  models of neural dynamics
can easily run into self-excitation loops with diverging ﬁring rates [37] (Fig. 4A). We introduce
a feature b(s) = 1 to indicate that s and x correspond to a bad simulation. We set K(xn  xo) = 0

2While we did not investigate this here  an attractive idea would be to base the kernel of the dis-
tance between xn and xo on the divergence between the associated posteriors  e.g. Kτ (xn  xo) =
exp(−1/τ DKL(q(r−1)(θ|xn)||q(r−1)(θ|xo))) – in this case  two data would be regarded as similar if the
current estimation of the density network assigns similar posterior distributions to them  which is a natural
measure of similarity in this context.

4

whenever b(xn) = 1 since the density estimator should not spend resources on approximating the
posterior for bad data. With this choice of calibration kernel  bad simulations are ignored when
updating the posterior model – however  this results in inefﬁcient use of simulations.
We propose to learn a model ˆg : θ → [0  1] to predict the probability that a simulation from θ will
break. While any probabilistic classiﬁer could be used  we train a binary-output neural network with
log-loss on (θn  b(sn)). For each proposed θ  we reject θ with probability ˆg(θ)  and do not carry out
the expensive simulation3. The rejections could be incorporated into the importance weights (which
would require estimating the corresponding partition function  or assuming it to be constant across
rounds)  but as these rejections do not depend on the data xo  we interpret them as modifying the
prior: from an initially speciﬁed prior p(θ)  we obtain a modiﬁed prior excluding those parameters
which likely will lead to nonsensical simulations. Therefore  the predictive model ˆg(θ) does not only
lead to more efﬁcient inference (especially in strongly under-constrained scenarios)  but is also useful
in identifying an effective prior – the space of parameters deemed plausible a priori intersected with
the space of parameters for which the simulator is well-behaved.
Bad features:
It is frequently observed that individual features of interest for ﬁtting single-neuron
models cannot be evaluated: for example  the spike latency cannot be evaluated if a simulation
does not generate spikes  but the fact that this feature is missing might provide valuable information
(Fig. 4C). SNPE can be extended to handle ‘bad features’ by using a carefully designed posterior
network. For each feature fi(s)  we introduce a binary feature mi(s) which indicates whether fi
is missing. We parameterise the input layer of the posterior network with multiplicative terms of
the form hi(s) = fi(s) · (1 − mi(s)) + ci · mi(s) where the term ci is to be learned. This approach
effectively learns an imputation value ci for each missing feature. For a more expressive model  one
could also include terms which learn interactions across different missing-feature indicators and/or
features  but we did not explore this here.
Learning features: Finally  we point out that using a neural network for posterior estimation yields
a straightforward way of learning relevant features from data [38  39  40]. Rather than feeding
summary features f (s) into the network  we directly feed time-series recordings of neural activity
into the network. The ﬁrst layer of the MDN becomes a recurrent layer instead of a fully-connected
one. By minimising the variational objective (Eq.2)  the network learns informative summary features
about posterior densities.
3 Results
While SNPE is in principle applicable to any simulator-based model  we designed it for performing
inference on models of neural dynamics. In our applications  we concentrate on single-neuron models.
We demonstrate the ability of SNPE to recover ground-truth posteriors in Gaussian Mixtures and
Generalised Linear Models (GLMs) [41]  and apply SNPE to a Hodgkin-Huxley neuron model and
an autapse model  which can have parameter regimes of unstable behaviour and missing features.
3.1 Statistical inference on simple models
Gaussian mixtures: We ﬁrst demonstrate the effectiveness of SNPE for inferring the posterior of
mixtures of two Gaussians  for which we can analytically compute true posteriors. We are interested
in the numerical stability of the method (‘robustness’) and the ‘ﬂexibility’ to approximate multi-modal
posteriors. To illustrate the robustness of SNPE  we apply SNPE and the method proposed by [19]
(which we refer to by Conditional Density Estimation for Likelihood-free Inference  CDE-LFI) to
infer the common mean of a mixture of two Gaussians  given samples from the mixture distribution
(Fig. 2A; details in Appendix D.1). Whereas SNPE works robustly across multiple algorithmic
rounds  CDE-LFI can become unstable: its analytical correction requires a division by a Gaussian
which becomes unstable if the precision of the Gaussian does not increase monotonically across
rounds (see 2.1). Constraining the precision-matrix to be non-decreasing ﬁxes the numerical issue 
but leads to biased estimates of the posterior. Second  we apply both SNPE and CDE-LFI to infer
the two means of a mixture of two Gaussians  given samples x from the mixture distribution (Fig.
2B; Appendix D.1). While SNPE can use bi-modal proposals  CDE-LFI cannot  implying reduced
efﬁciency of proposals on strongly non-Gaussian or multi-modal problems.

3An alternative approach would be to ﬁrst learn p(θ|b(s) = 0) by applying SNPE to a single feature 
f1(s) = b(s)  and to subsequently run SNPE on the full feature-set  but using p(θ|b(s) = 0) as prior – however 
this would ‘waste’ simulations for learning p(θ|b(s) = 1).

5

Figure 2: Inference on simple statistical models. A. Robustness of posterior inference on 1-D
Gaussian Mixtures (GMs). Left: true posterior given observation at xo = 0. Middle: percentage
of completed runs as a function of number of rounds; SNPE is robust. Right: Gaussian proposal
priors tend to underestimate tails of posterior (red). B. Flexibility of posterior inference. Left: True
posterior for 1-D bimodal GM and observation xo. Middle and right: First round proposal priors
(dotted)  second round proposal priors (dashed) and estimated posteriors (solid) for CDE-LFI and
SNPE respectively (true posterior red). SNPE allows multi-modal proposals. C  F. Application to
GLM. Posterior means and variances are recovered well by both CDE-LFI and SNPE. For reference 
we approximate the posterior using likelihood-based PG-MCMC. D. Covariance matrices for SNPE
and PG-MCMC. E. Partial view of the posterior for 3 out of 10 parameters (all 10 parameters in
Appendix G). Ground-truth parameters in red. 2-D marginals for SNPE (lines) and PG-MCMC
(histograms). White and yellow contour lines correspond to 68% and 95% of the mass  respectively.

Generalised linear models: Generalised linear models (GLM) are commonly used to model
neural responses to sensory stimuli. For these models  several techniques are available to estimate the
posterior distribution over parameters  making them ideally suited to test SNPE in a single-neuron
model. We evaluated the posterior distribution over the parameters of a GLM using a Pólya-Gamma
sampler (PG-MCMC  [42  43]) and compared it to the posterior distributions estimated by SNPE
(Appendix D.2 for details). We found a good agreement of the posterior means and variances (Fig.
2C)  covariances (Fig. 2D)  as well as pairwise marginals (Fig. 2E). We note that  since GLMs have
close-to-Gaussian posteriors  the CDE-LFI method works extremely well on this problem (Fig. 2F).
In summary  SNPE leads to accurate and robust estimation of the posterior in simple models. It works
effectively even on multi-modal posteriors on which CDE-LFI exhibits worse performance. On a
GLM-example with an (almost) Gaussian posterior  the CDE-LFI method works extremely well 
but SNPE yields very similar posterior estimates (see Appendix F for additional comparison with
SMC-ABC).

3.2 Statistical inference on Hodgkin-Huxley neuron models
Simulated data:
The Hodgkin-Huxley equations [44] describe the dynamics of a neuron’s mem-
brane potential and ion channels given biophysical parameters (e.g. concentration of sodium and
potassium channels) and an injected input current (Fig. 3A  see Appendix D.3). We applied SNPE
to a Hodgkin-Huxley model with channel kinetics as in [45] and inferred the posterior over 12
biophysical parameters  given 20 voltage features of the simulated data. The true parameter values are
close to the mode of the inferred posterior (Fig. 3B  D)  and in a region of high posterior probability.
Samples from the posterior lead to voltage traces that are similar to the original data  supporting the
correctness of the approach (Fig. 3C).

6

−202θ012pθxx*(|=)o23456# of rounds050100% completed runsSNPECDE-LFI−202θdensitypθ()(2)~pθ()(6)~0xo8x-10010θ−10010θdensityCDE-LFI−10010θdensitySNPE1510parameter−202valuetrue valueSNPEPG-MCMC-0.00.1PG-MCMC covariance-0.00.1SNPE covariance-3.0-0.5b0...-0.71.5h1...-0.32.4h2......1510parameter−202valuetrue valueCDE-LFIPG-MCMCABCDEFFigure 3: Application to Hodgkin-Huxley model: A. Simulation of Hodgkin-Huxley model with
current injection. B. Posterior over 3 out of 12 parameters inferred with SNPE (12 parameters in
Appendix G). True parameters have high posterior probabilities (red). C. Traces for the mode (cyan)
of and samples (orange) from the inferred posterior match the original data (blue). D. Comparison
between SNPE and a standard parameter-ﬁtting procedure based on a genetic algorithm  IBEA:
difference between the mode of SNPE or IBEA best parameter set  and the ground-truth parameters 
normalised by the standard deviations obtained by SNPE. E-G. Application to real data from Allen
Cell Type Database. Inference over 12 parameters for cell 464212183. Results presented as in A-C.

Biophysical neuron models are typically ﬁt to data with genetic algorithms applied to the distance
between simulated and measured data-features [2  8  9  46]. We compared the performance of SNPE
with a commonly used genetic algorithm (Indicator Based Evolutionary Algorithm  IBEA  from the
BluePyOpt package [9])  given the same number of model simulations (Fig. 3D). SNPE is comparable
to IBEA in approximating the ground-truth parameters – note that deﬁning an objective measure to
compare the two approaches is difﬁcult  as they both minimise different criteria. However  unlike
IBEA  SNPE also returns a full posterior distribution  i.e. the space of all parameters consistent with
the data  rather than just a ‘best ﬁt’.
In-vitro recordings: We also applied the approach to in vitro recordings from the mouse visual
cortex (see Appendix D.4  Fig. 3E-G). The posterior mode over 12 parameters of a Hodgkin-Huxley
model leads to a voltage trace which is similar to the data  and the posterior distribution shows the
space of parameters for which the output of the model is preserved. These posteriors could be used to
motivate further experiments for constraining parameters  or to study invariances in the model.
3.3 Dealing with bad simulations and features
Bad simulations: We demonstrate our approach (see Section 2.3) for dealing with ‘bad simulations’
(e.g. for which ﬁring rates diverge) using a simple  two-parameter ‘autapse’ model for which the region
of stability is known. During SNPE  we concurrently train a classiﬁer to predict ‘bad simulations’ and
update the prior accordingly. This approach does not only lead to a more efﬁcient use of simulations 
but also identiﬁes the parameter space for which the simulator is well-deﬁned  information that could
be used for further model analysis (Fig. 4A  B).
Bad features: Many features of interest in neural models  e.g. the latency to ﬁrst spike after the
injection of a current input  are only well deﬁned in the presence of other features  e.g. the presence
of spikes (Fig. 4C). Given that large parts of the parameter space can lead to non-spiking behaviour 
missing features occur frequently and cannot simply be ignored. We enriched our MDN with an extra
layer which imputes values to the absent features  values which are optimised alongside the rest of
the parameters of the network (Fig. 4D; Appendix E). Such imputation has marginal computational

7

−80−2040voltage (mV)060120time (ms)0.000.55input (nA)3.24.3lng()Na...0.92.0lng()K...-3.0-1.9lng()l......060120time (ms)−80−2040voltage (mV)gNagKENakbn1VTnoisegM−Eltmaxglkbn2−EK0.01.22.3|| - || / θθσ*θSNPE meanbest IBEA−80−2040voltage (mV)06251250time (ms)0.000.19input (nA)3.24.3lng()Na...0.92.0lng()K...-3.0-1.9lng()l......06251250time (ms)−80−2040voltage (mV)ABCDEFGInference on neural dynamics has to deal with diverging simulations and missing
Figure 4:
features. A. Firing rate of a model neuron connected to itself (autapse). If the strength of the self-
connection (parameter J) is bigger than 1  the dynamics are unstable (orange line - bad simulation).
B. Portion of parameter space leading to diverging simulations learned by the classiﬁer (yellow: low
probability of bad simulation  blue: high probability)  and comparison with analytically computed
boundaries (white  see Appendix D.5). C. Illustration of a model neuron in two parameter regimes 
spiking (grey trace) and non-spiking (blue). When the neuron does not spike  features that depend on
the presence of spiking  such as the latency to ﬁrst spike  are not deﬁned. D. Our MDN is augmented
with a multiplicative layer which imputes values for missing features.

cost and grants us the convenience of not having to hand-tune imputation values  or to reject all
simulations for which any individual feature might be missing.
Learning features with recurrent neural networks (RNNs):
In neural modelling  it is often
of interest to work with hand-designed features that are thought to be particularly important or
informative for particular analysis questions [2]. For instance  the shape of the action potential is
intimately related to the dynamics of sodium and potassium channels in the Hodgkin-Huxley model.
However  the space of possible features is immense  and given the highly non-linear nature of many of
the neural models in question  it can sometimes be of interest to simply perform statistical inference
without having to hand-design features. Our approach provides a straightforward means of doing that:
we augment the MDN with a RNN which runs along the recorded voltage trace (and stimulus  here a
coloured-noise input) to learn appropriate features to constrain the model parameters. As illustrated in
ﬁgure 5B  the ﬁrst layer of the network  which previously received pre-computed summary statistics
as inputs  is replaced by a recurrent layer that receives full voltage and current traces as inputs. In
order to capture long-term dependencies in the sequence input  we use gated-recurrent units (GRUs)
for the RNN [47]. Since we are using 25 GRU units and only keep the ﬁnal output of the unrolled
RNN (many-to-one)  we introduce a bottleneck. The RNN thus transforms the voltage trace and
stimulus into a set of 25 features  which allow SNPE to recover the posterior over the 12 parameters
(Fig. 5C). As expected  the presence of spikes in the observed data leads to a tighter posterior for
parameters associated to the main ion channels involved in spike generation  ENa  EK  gNa and gK.
4 Discussion
Quantitatively linking models of neural dynamics to data is a central problem in computational
neuroscience. We showed that likelihood-free inference is at least as general and efﬁcient as ‘black-
box’ parameter ﬁtting approaches in neuroscience  but provides full statistical inference  suggesting
it to be the method of choice for inference on single-neuron models. We argued that ABC approaches
based on density estimation are particularly useful for neuroscience  and introduced a novel algorithm
(SNPE) for estimating posterior distributions. We can ﬂexibly and robustly estimate posterior
distributions  even when large regions of the parameter space correspond to unstable model behaviour 
or when features of choice are missing. Furthermore  we have extended our approach with RNNs to
automatically deﬁne features  thus increasing the potential for better capturing salient aspects of the
data with highly non-linear models. SNPE is therefore equipped to estimate posterior distributions
under common constraints in neural models.
Our approach directly builds on a recent approach for density estimation ABC (CDE-LFI  [19]).
While we found CDE-LFI to work well on problems with unimodal  close-to-Gaussian posteriors and
stable simulators  our approach extends the range of possible applications  and these extensions are
critical for the application to neuron models. A key component of SNPE is the proposal prior  which
guides the sampling on each round of the algorithm. Here  we used the posterior on the previous
round as the proposal for the next one  as in CDE-LFI and in many Sequential-MC approaches. Our

8

050100time (ms)10−1100101102103104rate (Hz)observed databad simulation0.012.0J-102.5τgθ()^1.00.0−80−2040voltage (mV)060120time (ms)0.03.6input (mA)…………h2h3hH…h1m1(s)…c11-m1(s)…m2(s)f1(s)f2(s)+ABCDFigure 5: We can learn informative features using a recurrent mixture-density network (R-
MDN). A. We consider a neuron driven by a colored-noise input current. B. Rather than engineering
summary features to reduce the dimensionality of observations  we provide the complete voltage
trace and input current as input to an R-MDN. The unrolled forward pass is illustrated  where a
many-to-one recurrent network reduces the dimensionality of the inputs (T time steps long) to a
feature vector of dimensionality N. C. Our goal is to infer the posterior density for two different
observations: (1) the full 240ms trace shown in panel A; and (2) the initial 60ms of its duration  which
do not show any spike. We show the obtained marginal posterior densities for the two observations 
using a 25-dimensional feature vector learned by the RNN. In the presence of spikes  the posterior
uncertainty gets tighter around the true parameters related to spiking.

method could be extended by alternative approaches to designing proposal priors [48  49]  e.g. by
exploiting the fact that we also represent a posterior over MDN parameters: for example  one could
design proposals that guide sampling towards regions of the parameter space where the uncertainty
about the parameters of the posterior model is highest. We note that  while here we concentrated
on models of single neurons  ABC methods and our approach will also be applicable to models of
populations of neurons. Our approach will enable neuroscientists to perform Bayesian inference on
complex neuron models without having to design model-speciﬁc algorithms  closing the gap between
mechanistic and statistical models  and enabling theory-driven data-analysis [50].

Acknowledgements
We thank Maneesh Sahani  David Greenberg and Balaji Lakshminarayanan for useful comments
on the manuscript. This work was supported by SFB 1089 (University of Bonn) and SFB 1233
(University of Tübingen) of the German Research Foundation (DFG) to JHM and by the caesar
foundation.

References

[1] W Gerstner  W M Kistler  R Naud  and L Paninski. Neuronal dynamics: From single neurons to networks

and models of cognition. Cambridge University Press  2014.

[2] S Druckmann  Y Banitt  A Gidon  F Schürmann  H Markram  and I Segev. A novel multiple objective
optimization framework for constraining conductance-based neuron models by experimental data. Front
Neurosci  1  2007.

[3] C van Vreeswijk and H Sompolinsky. Chaos in neuronal networks with balanced excitatory and inhibitory

activity. Science  274(5293)  1996.

[4] H Markram et al. Reconstruction and Simulation of Neocortical Microcircuitry. Cell  163(2)  2015.
[5] Q J M Huys and L Paninski. Smoothing of  and parameter estimation from  noisy biophysical recordings.

PLoS Comput Biol  5(5)  2009.

[6] L Meng  M A Kramer  and U T Eden. A sequential monte carlo approach to estimate biophysical neural

models from spikes. J Neural Eng  8(6)  2011.

[7] C D Meliza  M Kostuk  H Huang  A Nogaret  D Margoliash  and H D I Abarbanel. Estimating parameters
and predicting membrane voltages with conductance-based neuron models. Biol Cybern  108(4)  2014.
[8] C Rossant  D F M Goodman  B Fontaine  J Platkiewicz  A K Magnusson  and R Brette. Fitting neuron

models to spike trains. Front Neurosci  5:9  2011.

[9] W Van Geit  M Gevaert  G Chindemi  C Rössert  J Courcol  E B Muller  F Schürmann  I Segev  and
H Markram. Bluepyopt: Leveraging open source software and cloud infrastructure to optimise model
parameters in neuroscience. Front Neuroinform  10:17  2016.

[10] A A Prinz  C P Billimoria  and E Marder. Alternative to hand-tuning conductance-based models: Con-

struction and analysis of databases of model neurons. J Neurophysiol  90(6)  2003.

9

−80−2040voltage (mV)602400120240time (ms)0.002.55input (nA)v1GRUsi1GRUsGRUsGRUs…v2i2v3i3vTiT…f1f2fN……FeaturesMixture Density NetworkgNagKglENa60240−EK−ElgMtmaxkbn1kbn2VTnoiseABC[11] C Stringer  M Pachitariu  N A Steinmetz  M Okun  P Bartho  K D Harris  M Sahani  and N A Lesica.

Inhibitory control of correlated intrinsic variability in cortical networks. Elife  5  2016.

[12] Kristofor D Carlson  Jayram Moorkanikara Nageswaran  Nikil Dutt  and Jeffrey L Krichmar. An efﬁ-
cient automated parameter tuning framework for spiking neural networks. Front Neurosci  8:10  2014.
doi:10.3389/fnins.2014.00010.

[13] P Friedrich  M Vella  A I Gulyás  T F Freund  and S Káli. A ﬂexible  interactive software tool for ﬁtting

the parameters of neuronal models. Frontiers in neuroinformatics  8  2014.

[14] P J Diggle and R J Gratton. Monte carlo methods of inference for implicit statistical models. J R Stat Soc

B Met  1984.

[15] F Hartig  J M Calabrese  B Reineking  T Wiegand  and A Huth. Statistical inference for stochastic

simulation models–theory and application. Ecol Lett  14(8)  2011.

[16] J Lintusaari  M U Gutmann  R Dutta  S Kaski  and J Corander. Fundamentals and recent developments in

approximate bayesian computation. Syst Biol  2016.

[17] Aidan C Daly  David J Gavaghan  Chris Holmes  and Jonathan Cooper. Hodgkin–huxley revisited:
reparametrization and identiﬁability analysis of the classic action potential model with approximate
bayesian methods. Royal Society open science  2(12):150499  2015.

[18] M G B Blum and O François. Non-linear regression models for approximate bayesian computation. Stat

Comput  20(1)  2010.

[19] G Papamakarios and I Murray. Fast epsilon-free inference of simulation models with bayesian conditional

density estimation. In Adv in Neur In  2017.

[20] N T Carnevale and M L Hines. The NEURON Book. Cambridge University Press  2009.
[21] E Meeds  M Welling  et al. Gps-abc: Gaussian process surrogate approximate bayesian computation. UAI 

2014.

[22] J K Pritchard  M T Seielstad  A Perez-Lezaun  and M W Feldman. Population growth of human y

chromosomes: a study of y chromosome microsatellites. Mol Biol Evol  16(12)  1999.

[23] P Marjoram  J Molitor  V Plagnol  and S Tavare. Markov chain monte carlo without likelihoods. Proc

Natl Acad Sci U S A  100(26)  2003.

[24] E Meeds  R Leenders  and M Welling. Hamiltonian abc. arXiv preprint arXiv:1503.01916  2015.
[25] M A Beaumont  J Cornuet  J Marin  and C P Robert. Adaptive approximate bayesian computation.

Biometrika  2009.

[26] F V Bonassi  M West  et al. Sequential monte carlo with adaptive weights for approximate bayesian

computation. Bayesian Anal  10(1)  2015.

[27] R Wilkinson. Accelerating abc methods using gaussian processes. In AISTATS  2014.
[28] S N Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature  466(7310)  2010.
[29] V M H Ong  D J Nott  M Tran  S A Sisson  and C C Drovandi. Variational bayes with synthetic likelihood.

[30] Y Fan  D J Nott  and S A Sisson. Approximate bayesian computation via regression density estimation.

[31] B M Turner and P B Sederberg. A generalized  likelihood-free method for posterior estimation. Psycho-

nomic Bulletin & Review  21(2)  2014.

[32] L F Price  C C Drovandi  A Lee  and David J N. Bayesian synthetic likelihood. J Comput Graph Stat 

[33] M Beaumont  W Zhang  and D J Balding. Approximate bayesian computation in population genetics.

arXiv:1608.03069  2016.

Stat  2(1)  2013.

(just-accepted)  2017.

Genetics  162(4)  2002.

[34] G E Hinton and D Van Camp. Keeping the neural networks simple by minimizing the description length
of the weights. In Proceedings of the sixth annual conference on Computational learning theory  1993.

[35] A Graves. Practical variational inference for neural networks. In Adv Neur In  2011.
[36] D P Kingma  T Salimans  and M Welling. Neural adaptive sequential monte carlo. In Variational Dropout

and the Local Reparameterization Trick  pages 2575–2583  2015.

[37] F Gerhard  M Deger  and W Truccolo. On the stability and dynamics of stochastic spiking neuron models:

Nonlinear hawkes process and point process glms. PLoS Comput Biol  13(2)  2017.

[38] K Cho  B Van Merriënboer  C Gulcehre  D Bahdanau  F Bougares  H Schwenk  and Y Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint
arXiv:1406.1078  2014.

[39] M G B Blum  M A Nunes  Ds Prangle  S A Sisson  et al. A comparative review of dimension reduction

methods in approximate bayesian computation. Statistical Science  28(2)  2013.

[40] B Jiang  T Wu  Cs Zheng  and W H Wong. Learning summary statistic for approximate bayesian

computation via deep neural network. arXiv preprint arXiv:1510.02175  2015.

[41] J W Pillow  J Shlens  L Paninski  A Sher  A M Litke  E J Chichilnisky  and E P Simoncelli. Spatio-temporal

correlations and visual signalling in a complete neuronal population. Nature  454(7207)  2008.

[42] N G Polson  J G Scott  and J Windle. Bayesian inference for logistic models using pólya–gamma latent

variables. J Am Stat Assoc  108(504)  2013.

10

[43] S Linderman  R P Adams  and J W Pillow. Bayesian latent structure discovery from multi-neuron

recordings. In Advances in Neural Information Processing Systems  2016.

[44] A L Hodgkin and A F Huxley. A quantitative description of membrane current and its application to

conduction and excitation in nerve. J Physiol  117(4)  1952.

[45] M Pospischil  M Toledo-Rodriguez  C Monier  Z Piwkowska  T Bal  Y Frégnac  H Markram  and
A Destexhe. Minimal hodgkin-huxley type models for different classes of cortical and thalamic neurons.
Biol Cybern  99(4-5)  2008.

[46] E Hay  S Hill  F Schürmann  H Markram  and I Segev. Models of neocortical layer 5b pyramidal cells

capturing a wide range of dendritic and perisomatic active properties. PLoS Comput Biol  7(7)  2011.

[47] J Chung  C Gulcehre  K H Cho  and Y Bengio. Empirical evaluation of gated recurrent neural networks

on sequence modeling. arXiv preprint arXiv:1412.3555  2014.

[48] Marko Järvenpää  Michael U Gutmann  Aki Vehtari  and Pekka Marttinen. Efﬁcient acquisition rules for

model-based approximate bayesian computation. arXiv preprint arXiv:1704.00520  2017.

[49] S Gu  Z Ghahramani  and R E Turner. Neural adaptive sequential monte carlo. In Advances in Neural

[51] G De Nicolao  G Sparacino  and C Cobelli. Nonparametric input estimation in physiological systems:

problems  methods  and case studies. Automatica  33(5)  1997.

Information Processing Systems  pages 2629–2637  2015.

[50] S W Linderman and S J Gershman. Using computational theory to constrain statistical models of neural

data. bioRxiv  2017.

11

,Jan-Matthis Lueckmann
Pedro Goncalves
Giacomo Bassetto
Kaan Öcal
Marcel Nonnenmacher
Jakob Macke
Zhize Li