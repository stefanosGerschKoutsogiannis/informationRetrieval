2013,Generalizing Analytic Shrinkage for Arbitrary Covariance Structures,Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency implies bounds on the growth rates of eigenvalues and their dispersion  which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition  we propose an extension of analytic shrinkage --orthogonal complement shrinkage-- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance  spoken letter and optical character recognition  and neuroscience.,Generalizing Analytic Shrinkage for Arbitrary

Covariance Structures

Daniel Bartz

Department of Computer Science

TU Berlin  Berlin  Germany

daniel.bartz@tu-berlin.de

klaus-robert.mueller@tu-berlin.de

Klaus-Robert M¨uller

TU Berlin  Berlin  Germany
Korea University  Korea  Seoul

Abstract

Analytic shrinkage is a statistical technique that offers a fast alternative to cross-
validation for the regularization of covariance matrices and has appealing con-
sistency properties. We show that the proof of consistency requires bounds on
the growth rates of eigenvalues and their dispersion  which are often violated in
data. We prove consistency under assumptions which do not restrict the covariance
structure and therefore better match real world data. In addition  we propose an
extension of analytic shrinkage –orthogonal complement shrinkage– which adapts
to the covariance structure. Finally we demonstrate the superior performance of
our novel approach on data from the domains of ﬁnance  spoken letter and optical
character recognition  and neuroscience.

1

Introduction

The estimation of covariance matrices is the basis of many machine learning algorithms and estima-
tion procedures in statistics. The standard estimator is the sample covariance matrix: its entries are
unbiased and consistent [1]. A well-known shortcoming of the sample covariance is the systematic
error in the spectrum. In particular for high dimensional data  where dimensionality p and number
of observations n are often of the same order  large eigenvalues are over- und small eigenvalues
underestimated. A form of regularization which can alleviate this bias is shrinkage [2]: the convex
combination of the sample covariance matrix S and a multiple of the identity T = p−1trace(S)I 

Csh = (1 − λ)S + λT 

(1)
has potentially lower mean squared error and lower bias in the spectrum [3]. The standard procedure
for chosing an optimal regularization for shrinkage is cross-validation [4]  which is known to be
time consuming. For online settings CV can become unfeasible and a faster model selection method
is required. Recently  analytic shrinkage [3] which provides a consistent analytic formula for the
above regularization parameter λ has become increasingly popular. It minimizes the expected mean
squared error of the convex combination with a computational cost of O(p2)  which is negligible
when used for algorithms like Linear Discriminant Analysis (LDA) which are O(p3).
The consistency of analytic shrinkage relies on assumptions which are rarely tested in practice [5].
This paper will therefore aim to render the analytic shrinkage framework more practical and usable
for real world data. We contribute in three aspects: ﬁrst  we derive simple tests for the applicability
of the analytic shrinkage framework and observe that for many data sets of practical relevance the
assumptions which underly consistency are not fullﬁlled. Second  we design assumptions which
better ﬁt the statistical properties observed in real world data which typically has a low dimen-
sional structure. Under these new assumptions  we prove consistency of analytic shrinkage. We
show a counter-intuitive result: for typical covariance structures  no shrinkage –and therefore no
regularization– takes place in the limit of high dimensionality and number of observations. In prac-
tice  this leads to weak shrinkage and degrading performance. Therefore  third  we propose an ex-
tension of the shrinkage framework: automatic orthogonal complement shrinkage (aoc-shrinkage)

1

takes the covariance structure into account and outperforms standard shrinkage on real world data at
a moderate increase in computation time. Note that proofs of all theorems in this paper can be found
in the supplemental material.

2 Overview of analytic shrinkage

To derive analytic shrinkage  the expected mean squared error of the shrinkage covariance matrix
eq. (1) as an estimator of the true covariance matrix C is minimized:

(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:20)(cid:13)(cid:13)(cid:13)C − (1 − λ)S − λT
(cid:17) − Var
(cid:17)(cid:111)
+ λ2E(cid:104)(cid:0)Sij − Tij
(cid:16)
(cid:1)(cid:111)

Sij

(cid:1)2(cid:105)

(cid:16)

+ Var

Sij

(cid:17)(cid:41)

(2)

(3)

λ(cid:63) = arg min

= arg min

(cid:80)

i j

=

λ

λ

λ

i j

i j

E

2λ

Cov

Sij  Tij

R(λ) := arg min

(cid:40)
(cid:110)
(cid:16)
(cid:88)
(cid:110)
Var(cid:0)Sij
(cid:1) − Cov(cid:0)Sij  Tij
E(cid:104)(cid:0)Sij − Tij
(cid:1)2(cid:105)
(cid:80)
(cid:16)
(cid:88)
(cid:1) =
(cid:100)Var(cid:0)Sij
(cid:40)(cid:88)
(cid:88)
(cid:1) =
(cid:100)Cov(cid:0)Sii  Tii
(cid:98)E(cid:2)(Sij − Tij)2(cid:3) = (Sij − Tij)2

(n − 1)np

(n − 1)n

1

k

s

1

.

s

(cid:88)

t

xisxjs − 1
n

(cid:17)2

xitxjt

(cid:88)

x2
isx2

ks − 1
n

x2
it

t

(cid:41)

(cid:88)

t(cid:48)

x2
it(cid:48)

The analytic shrinkage estimator ˆλ is obtained by replacing expectations with sample estimates:

Theoretical results on the estimator ˆλ are based on analysis of a sequence of statistical models
indexed by n. Xn denotes a pn × n matrix of n iid observations of pn variables with mean zero
and covariance matrix Σn. Yn = ΓT
nXn denotes the same observations in their eigenbasis  having
diagonal covariance Λn = ΓT
it denote the entries of Xn and
Yn  respectively1. The main theoretical result on the estimator ˆλ is its consistency in the large n  p
limit [3]. A decisive role is played by an assumption on the eighth moments2 in the eigenbasis:
Assumption 2 (A2  Ledoit/Wolf 2004 [3]). There exists a constant K2 independent of n such that

nΣnΓn. Lower case letters xn

it and yn

pn(cid:88)

p−1

n

E[(yn

i1)8] ≤ K2.

3

Implicit assumptions on the covariance structure

i=1

From the assumption on the eighth moments in the eigenbasis  we derive requirements on the eigen-
values which facilitate an empirical check:
Theorem 1 (largest eigenvalue growth rate). Let A2 hold. Then  there exists a limit on the growth
rate of the largest eigenvalue

(cid:16)

(cid:17)

γn
1 = max

i

Var(yn

i ) = O

p1/4
n

.

Theorem 2 (dispersion growth rate). Let A2 hold. Then  there exists a limit on the growth rate of
the normalized eigenvalue dispersion
dn = p−1

γj)2 = O (1) .

(γi − p−1

(cid:88)

(cid:88)

n

n

i

j

1We shall often drop the sequence index n and the observation index t to improve readability of formulas.
2eighth moments arise because Var(Sij)  the variance of the sample covariance  is of fourth order and has

to converge. Nevertheless  even for for non-Gaussian data convergence is fast.

2

Figure 1: Covariance matrices and dependency of the largest eigenvalue/dispersion on the dimen-
sionality. Average over 100 repetitions.

Figure 2: Dependency of the largest eigenvalue/dispersion on the dimensionality. Average over 100
random subsets.

The theorems restrict the covariance structure of the sequence of models when the dimensionality
increases. To illustrate this  we design two sequences of models A and B indexed by their dimen-
sionality p  in which dimensions xp

i are correlated with a signal sp:

(cid:26)(0.5 + bp

(0.5 + bp

xp
i =

i ) · εp
i + αcp
i ) · εp
i  

else.

i sp  with probability PsA/B (i) 

(4)

i and cp

i are uniform random from [0  1]  sp and p

where bp
i are standard normal  α = 1  PsB (i) = 0.2
and PsA (i) = (i/10 + 1)−7/8 (power law decay). To avoid systematic errors  we hold the ratio of
observations to dimensions ﬁxed: np/p = 2.
To the left in Figure 1  covariance matrices are shown: For model A  the matrix is dense in the
upper left corner  the more dimensions we add the more sparse the matrix gets. For model B 
correlations are spread out evenly. To the right  normalized sample dispersion and largest eigenvalue
are shown. For model A  we see the behaviour from the theorems: the dispersion is bounded  the
largest eigenvalue grows with the fourth root. For model B  there is a linear dependency of both
dispersion and largest eigenvalue: A2 is violated.
For real world data  we measure the dependency of the largest eigenvalue/dispersion on the dimen-
sionality by averaging over random subsets. Figure 2 shows the results for four data sets3: (1) New
York Stock Exchange  (2) USPS hand-written digits  (3) ISOLET spoken letters and (4) a Brain
Computer Interface EEG data set. The largest eigenvalues and the normalized dispersions (see Fig-
ure 2) closely resemble model B; a linear dependence on the dimensionality which violates A2 is
visible.

3for details on the data sets  see section 5.

3

 model A  model B10020030040050022.533.54normalized sample dispersion   model A  100200300400500010203040sample dispersionmax(EV)10020030040050001020    model Bcovariance matricesdispersion and largest EVdimensionality100200300400500050100max(EV)05001000050100150normalized sample dispersion#assetsUS stock market  sample dispersionmax(EV)05001000020040060001002000510#pixels USPS hand−written digits010020002040020040060002040#featuresISOLET spoken letters020040060001002000200400050100#featuresBCI EEG data02004000100200max(EV)dimensionality4 Analytic shrinkage for arbitrary covariance structures

We replace A2 by a weaker assumption on the moments in the basis of the observations X which
does not impose any constraints on the covariance structure4:
Assumption 2(cid:48) (A2(cid:48)). There exists a constant K2 independent of p such that

p(cid:88)

p−1

E[(xp

i1)8] ≤ K2.

i=1

Standard assumptions For the proof of consistency  the relationship between dimensionality and
number of observations has to be deﬁned and a weak restriction on the correlation of the products
of uncorrelated variables is necessary. We use slightly modiﬁed versions of the original assump-
tions [3].
Assumption 1(cid:48) (A1(cid:48)  Kolmogorov asymptotics). There exists a constant K1  0 ≤ K1 ≤ ∞ inde-
pendent of p such that

lim
p→∞ p/np = K1.

Assumption 3(cid:48) (A3(cid:48)).

(cid:80)

i j kl l∈Qp

lim
p→∞

(cid:0)Cov[yp

|Qp|

l1](cid:1)2

i1yp

j1  yp

k1yp

= 0

where Qp is the set of all quadruples consisting of distinct integers between 1 and p.

Additional Assumptions A1(cid:48) to A3(cid:48) subsume a wide range of dispersion and eigenvalue conﬁg-
urations. To investigate the role which this plays  we categorize sequences by adding an additional
parameter k. It will prove essential for the limit behavior of optimal shrinkage and the consistency
of analytic shrinkage:
Assumption 4 (A4  growth rate of the normalized dispersion). Let γi denote the eigenvalues of C.
Then  the limit behaviour of the normalized dispersion is parameterized by k:

p−1(cid:88)

(γi − p−1(cid:88)

γj)2 = Θ(cid:0)max(1  p2k−1)(cid:1)  

i

j

where Θ is the Landau Theta.
In sequences of models with k ≤ 0.5 the normalized dispersion is bounded from above and below  as
in model A in the last section. For k > 0.5 the normalized dispersion grows with the dimensionality 
for k = 1 it is linear in p  as in model B.
We make two technical assumptions to rule out degenerate cases. First  we assume that  on average 
additional dimensions make a positive contribution to the mean variance:
Assumption 5 (A5). There exists a constant K3 such that

p(cid:88)

p−1

E[(xp

i1)2] ≥ K3.

i=1

Second  we assume that limits on the relation between second  fourth and eighth moments exist:
Assumption 6 (A6  moment relation). ∃α4  α8  β4 and β8:
E[y4
E[y4

i ] ≤ (1 + α8)E2[y4
i ]
i ] ≥ (1 + β8)E2[y4
i ]

i ] ≤ (1 + α4)E2[y2
i ]
i ] ≥ (1 + β4)E2[y2
i ]

E[y8
E[y8

4For convenience  we index the sequence of statistical models by p instead of n.

4

Figure 3: Illustration of orthogonal complement shrinkage.

Theoretical results on limit behaviour and consistency We are able to derive a novel theorem
which shows that under these wider assumptions  shrinkage remains consistent:
Theorem 3 (Consistency of Shrinkage). Let A1(cid:48)  A2(cid:48)  A3(cid:48)  A4  A5  A6 hold and

m = E

(cid:20)(cid:16)

(λ∗ − ˆλ)/λ∗(cid:17)2(cid:21)

denote the expected squared relative error of the estimate ˆλ. Then  independently of k 

lim
p→∞ m = 0.

An unexpected caveat accompanying this result is the limit behaviour of the optimal shrinkage
strength λ∗:
Theorem 4 (Limit behaviour). Let A1(cid:48)  A2(cid:48)  A3(cid:48)  A4  A5  A6 hold. Then  there exist 0 < bl <
bu < 1

k ≤ 0.5
k > 0.5

⇒
⇒

∀n : bl ≤ λ∗ ≤ bu
p→∞ λ∗ = 0
lim

The theorem shows that there is a fundamental problem with analytic shrinkage:
than 0.5 (all data sets in the last section had k = 1) there is no shrinkage in the limit.

if k is larger

5 Automatic orthogonal complement shrinkage

Orthogonal complement shrinkage To obtain a ﬁnite shrinkage strength  we propose an exten-
sion of shrinkage we call oc-shrinkage: it leaves the ﬁrst eigendirection untouched and performs
shrinkage on the orthogonal complement oc of that direction. Figure 3 illustrates this approach. It
shows a three dimensional true covariance matrix with a high dispersion that makes it highly ellip-
soidal. The result is a high level of discrepancy between the spherical shrinkage target and the true
covariance. The best convex combination of target and sample covariance will put extremely low
weight on the target. The situation is different in the orthogonal complement of the ﬁrst eigendirec-
tion of the sample covariance matrix: there  the discrepancy between sample covariance and target
is strongly reduced.
To simplify the theoretical analysis  let us consider the case where there is only a single growing
eigenvalue while the remainder stays bounded:

5

Assumption 4(cid:48) (A4(cid:48) single large eigenvalue). Let us deﬁne

zi = yi 
z1 = p−k/2y1.

2 ≤ i ≤ p 

There exist constants Fl and Fu such that Fl ≤ E[z8
A recent result from Random Matrix Theory [6] allows us to prove that the projection on the empir-

ical orthogonal complement (cid:98)oc does not affect the consistency of the estimator ˆλ(cid:98)oc:

Theorem 5 (consistency of oc-shrinkage). Let A1(cid:48)  A2(cid:48)  A3(cid:48)  A4(cid:48)  A5  A6 hold. In addition  assume
that 16th moments5 of the yi exist and are bounded. Then  independently of k 

i ] ≤ Fu

(cid:18)

ˆλ(cid:98)oc − arg min

Q(cid:98)oc(λ)

λ

lim
p→∞

(cid:19)2

= 0 

where Q denotes the mean squared error (MSE) of the convex combination (cmp. eq. (2)).

Automatic model selection Orthogonal complement shrinkage only yields an advantage if the ﬁrst
eigenvalue is large enough. Starting from eq. (2)  we can consistently estimate the error of standard
shrinkage and orthogonal complement shrinkage and only use oc-shrinkage when the difference

(cid:98)∆R (cid:98)oc is positive. In the supplemental material  we derive a formula of a conservative estimate:

(cid:98)∆R cons. (cid:98)oc = (cid:98)∆R (cid:98)oc − m∆ ˆσ(cid:98)∆R (cid:99)oc

− mE

ˆλ2(cid:98)oc ˆσ ˆE.

Usage of m∆ = 0.45 corresponds to 75% probability of improvement under gaussianity and yields
good results in practice. The second term is relevant in small samples  setting mE = 0.1 is sufﬁcient.
A dataset may have multiple large eigenvalues. It is straightforward to iterate the procedure and thus
automatically select the number of retained eigendirections ˆr. We call this automatic orthogonal
complement shrinkage. An algorithm listing can be found in the supplemental.
The computational cost of aoc-shrinkage is larger than that of standard shrinkage as it additionally
requires an eigendecomposition O(p3) and some matrix multiplications O(ˆrp2). In the applications
considered here  this additional cost is negligible: ˆr (cid:28) p and the eigendecomposition can replace
matrix inversions for LDA  QDA or portfolio optimization.

Figure 4: Automatic selection of the number of eigendirections. Average over 100 runs.

6 Empirical validation

Simulations To test the method  we extend model B (eq. (4)  section 3) to three signals  Psi = (0.1 
0.25  0.5). Figure 4 reports the percentage improvement in average loss over the sample covariance
matrix 

PRIAL(cid:0)Csh/oc−sh/aoc−sh(cid:1) =

E(cid:107)S − C(cid:107) − E(cid:107)Csh/oc−sh/aoc−sh − C(cid:107)

E(cid:107)S − C(cid:107)

 

5The existence of 16th moments is needed because we bound the estimation error in each direction by the

maximum over all directions  an extremely conservative approximation.

6

1011020.40.50.60.70.80.91dimensionality pPRIAL  Shrinkageoc(1)−Shrinkageoc(2)−Shrinkageoc(3)−Shrinkageoc(4)−Shrinkageaoc−ShrinkageTable 1: Portfolio risk. Mean absolute deviations·103 (mean squared deviations·106) of the resulting
portfolios for the different covariance estimators and markets. † := aoc-shrinkage signiﬁcantly
better than this model at the 5% level  tested by a randomization test.

sample covariance
standard shrinkage

ˆλ

shrinkage to a factor model

ˆλ

aoc-shrinkage

ˆλ
average ˆr

US
8.56† (156.1†)
6.27† (86.4†)
5.56† (69.6†)

0.09

EU
5.93† (78.9†)
4.43† (46.2†)
4.00† (39.1†)

0.12

HK
6.57† (81.2†)
6.32† (76.2†)
6.17† (72.9†)

0.10

0.41

5.41 (67.0)

0.44

3.83 (36.3)

0.42

6.11 (71.8)

0.75
1.64

0.79
1.17

0.75
1.41

Table 2: Accuracies for classiﬁcation tasks on ISOLET and USPS data. ∗ := signiﬁcantly better
than all compared methods at the 5% level  tested by a randomization test.

ntrain
LDA
LDA (shrinkage)
LDA (aoc)
QDA
QDA (shrinkage)
QDA (aoc)

ISOLET
500
75.77%
88.92%
89.69%
2.783%
58.57%
59.51%

∗

∗

2000
92.29%
93.25%
93.42%
4.882%
75.4%
80.84%

∗

5000
94.1%
94.3%
94.33%
14.09%
79.25%
87.35%

USPS
500
72.31%
83.77%
83.95%
10.11%
82.2%
83.31%

∗

5000

2000
87.45% 89.56%
88.37% 89.77%
88.37% 89.77%
49.45% 72.43%
88.85% 89.67%
89.4%
90.07%

∗

of standard shrinkage  oc-shrinkage for one to four eigendirections and aoc-shrinkage.
Standard shrinkage behaves as predicted by Theorem 4: ˆλ and therefore the PRIAL tend to zero in
the large n  p limit. The same holds for orders of oc-shrinkage –oc(1) and oc(2)– lower than the
number of signals  but performance degrades more slowly. For small dimensionalities eigenvalues
are small and therefore there is no advantage for oc-shrinkage. On the contrary  the higher the order
of oc-shrinkage  the larger the error by projecting out spurious large eigenvalues which should have
been subject to regularization. The automatic order selection aoc-shrinkage leads to close to optimal
PRIAL for all dimensionalities.

Real world data I: portfolio optimization Covariance estimates are needed for the minimization
of portfolio risk [7]. Table 1 shows portfolio risk for approximately eight years of daily return data
from 1200 US  600 European and 100 Hong Kong stocks  aggregated from Reuters tick data [8].
Estimation of covariance matrices is based on short time windows (150 days) because of the data’s
nonstationarity. Despite the unfavorable ratio of observations to dimensionality  standard shrinkage
has very low values of ˆλ: the stocks are highly correlated and the spherical target is highly inappro-
priate. Shrinkage to a ﬁnancial factor model incorporating the market factor [9] provides a better
target; it leads to stronger shrinkage and better portfolios. Our proposed aoc-shrinkage yields even
stronger shrinkage and signiﬁcantly outperforms all compared methods.

Table 3: Accuracies for classiﬁcation tasks on BCI data. Artiﬁcially injected noise in one electrode.
∗ := signiﬁcantly better than all compared methods at the 5% level  tested by a randomization test.

σnoise
LDA
LDA (shrinkage)
LDA (aoc)
average ˆr

0
92.28%
92.39%
∗
93.27%
2.0836

10
92.28%
92.94%
∗
93.27%
3.0945

30
92.28%
92.18%
∗
93.24%
3.0891

100
92.28%
88.04%
∗
92.88%
3.0891

300
92.28%
82.15%
∗
93.16%
3.0891

1000
92.28%
73.79%
93.19%

∗

3.09

7

Figure 5: High variance components responsible for failure of shrinkage in BCI. σnoise = 10.
Subject 1.

Real world data II: USPS and ISOLET We applied Linear and Quadratic Discriminant Analysis
(LDA and QDA) to hand-written digit recognition (USPS  1100 observations with 256 pixels for
each of the 10 digits [10]) and spoken letter recognition (ISOLET  617 features  7797 recordings of
26 spoken letters [11]  obtained from the UCI ML Repository [12]) to assess the quality of standard
and aoc-shrinkage covariance estimates.
Table 2 shows that aoc-shrinkage outperforms standard shrinkage for QDA and LDA on both data
sets for different training set sizes. Only for LDA and large sample sizes on the relatively low
dimensional USPS data  there is no difference between standard and aoc-shrinkage: the automatic
procedure decides that shrinkage on the whole space is optimal.

Real world data III: Brain-Computer-Interface The BCI data was recorded in a study in which
11 subjects had to distinguish between noisy and noise-free phonemes [13  14]. We applied LDA
on 427 standardized features calculated from event related potentials in 61 electrodes to classify two
conditions: correctly identiﬁed noise-free and correctly identiﬁed noisy phonemes (ntrain = 1000).
For Table 3  we simulated additive noise in a random electrode (100 repetitions). With and without
noise  our proposed aoc-shrinkage outperforms standard shrinkage LDA. Without noise  ˆr ≈ 2 high
variance directions –probably corresponding to ocular and facial muscle artefacts  depicted to the
left in Figure 5– are left untouched by aoc-shrinkage. With injected noise  the number of directions
increases to ˆr ≈ 3  as the procedure detects the additional high variance component –to the right
in Figure 5– and adapts the shrinkage procedure such that performance remains unaffected. For
standard shrinkage  noise affects the analytic regularization and performance degrades as a result.

7 Discussion

Analytic shrinkage is a fast and accurate alternative to cross-validation which yields comparable
performance  e.g. in prediction tasks and portfolio optimization. This paper has contributed by clar-
ifying the (limited) applicability of the analytic shrinkage formula. In particular we could show that
its assumptions are often violated in practice since real world data has complex structured depen-
dencies. We therefore introduced a set of more general assumptions to shrinkage theory  chosen
such that the appealing consistency properties of analytic shrinkage are preserved. We have shown
that for typcial structure in real world data  strong eigendirections adversely affect shrinkage by
driving the shrinkage strength to zero. Therefore  ﬁnally  we have proposed an algorithm which
automatically restricts shrinkage to the orthogonal complement of the strongest eigendirections if
appropriate. This leads to improved robustness and signiﬁcant performance enhancement in sim-
ulations and on real world data from the domains of ﬁnance  spoken letter and optical character
recognition  and neuroscience.

Acknowledgments

This work was supported in part by the World Class University Program through the National Re-
search Foundation of Korea funded by the Ministry of Education  Science  and Technology  under
Grant R31-10008. We thank Gilles Blanchard  Duncan Blythe  Thorsten Dickhaus  Irene Winkler
and Anne Porbadnik for valuable comments and discussions.

8

  −0.0932−0.046600.04660.0932  −0.0631−0.031600.03160.0631  −0.2532−0.126600.12660.2532References
[1] Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The Elements of Statistical Learning. Springer 

2008.

[2] Charles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution.

In Proc. 3rd Berkeley Sympos. Math. Statist. Probability  volume 1  pages 197–206  1956.

[3] Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance matri-

ces. Journal of Multivariate Analysis  88(2):365–411  2004.

[4] Jerome. H. Friedman. Regularized discriminant analysis. Journal of the American Statistical Association 

84(405):165–175  1989.

[5] Juliane Sch¨afer and Korbinian Strimmer. A shrinkage approach to large-scale covariance matrix esti-
mation and implications for functional genomics. Statistical Applications in Genetics and Molecular
Biology  4(1):1175–1189  2005.

[6] Boaz Nadler. Finite sample approximation results for principal component analysis: A matrix perturbation

approach. The Annals of Statistics  36(6):2791–2817  2008.

[7] Harry Markowitz. Portfolio selection. Journal of Finance  VII(1):77–91  March 1952.
[8] Daniel Bartz  Kerr Hatrick  Christian W. Hesse  Klaus-Robert M¨uller  and Steven Lemm. Directional
Variance Adjustment: Bias reduction in covariance matrices based on factor analysis with an application
to portfolio optimization. PLoS ONE  8(7):e67503  07 2013.

[9] Olivier Ledoit and Michael Wolf. Improved estimation of the covariance matrix of stock returns with an

application to portfolio selection. Journal of Empirical Finance  10:603–621  2003.

[10] Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern

Analysis and Machine Intelligence  16(5):550–554  May 1994.

[11] Mark A Fanty and Ronald Cole. Spoken letter recognition. In Advances in Neural Information Processing

Systems  volume 3  pages 220–226  1990.

[12] Kevin Bache and Moshe Lichman. UCI machine learning repository. University of California  Irvine 

School of Information and Computer Sciences  2013.

[13] Anne Kerstin Porbadnigk  Jan-Niklas Antons  Benjamin Blankertz  Matthias S Treder  Robert Schleicher 
Sebastian M¨oller  and Gabriel Curio. Using ERPs for assessing the (sub)conscious perception of noise.
In 32nd Annual Intl Conf. of the IEEE Engineering in Medicine and Biology Society  pages 2690–2693 
2010.

[14] Anne Kerstin Porbadnigk  Matthias S Treder  Benjamin Blankertz  Jan-Niklas Antons  Robert Schleicher 
Sebastian M¨oller  Gabriel Curio  and Klaus-Robert M¨uller. Single-trial analysis of the neural correlates
of speech quality perception. Journal of neural engineering  10(5):056003  2013.

9

,Daniel Bartz
Klaus-Robert Müller
Yuanyuan Liu
Fanhua Shang
Wei Fan
James Cheng
Hong Cheng
Kai-Wei Chang
He He
Stephane Ross
Hal Daume III
John Langford