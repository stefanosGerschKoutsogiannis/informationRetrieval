2017,Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification,We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy  which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority  of  large-scale  multi-class classification problems and to reduce the number of pairs of examples in the expanded data.  We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10 000 to 100 000 classes where we show the efficiency of the proposed approach in terms of training and prediction time  memory consumption  and predictive performance with respect to state-of-the-art approaches.,Aggressive Sampling for Multi-class to Binary

Reduction with Applications to Text Classiﬁcation

Bikash Joshi

Univ. Grenoble Alps  LIG

Grenoble  France

Massih-Reza Amini

Univ. Grenoble Alps  LIG

Grenoble  France

Ioannis Partalas
Expedia EWE

Geneva  Switzerland

bikash.joshi@imag.fr

massih-reza.amini@imag.fr

ipartalas@expedia.com

Franck Iutzeler

Univ. Grenoble Alps  LJK

Grenoble  France

franck.iutzeler@imag.fr

Los Alamos National Laboratory and Skolkovo IST 

Yury Maximov

USA and Moscow  Russia

yury@lanl.gov

Abstract

We address the problem of multi-class classiﬁcation in the case where the number of
classes is very large. We propose a double sampling strategy on top of a multi-class
to binary reduction strategy  which transforms the original multi-class problem into
a binary classiﬁcation problem over pairs of examples. The aim of the sampling
strategy is to overcome the curse of long-tailed class distributions exhibited in
majority of large-scale multi-class classiﬁcation problems and to reduce the number
of pairs of examples in the expanded data. We show that this strategy does not
alter the consistency of the empirical risk minimization principle deﬁned over the
double sample reduction. Experiments are carried out on DMOZ and Wikipedia
collections with 10 000 to 100 000 classes where we show the efﬁciency of the
proposed approach in terms of training and prediction time  memory consumption 
and predictive performance with respect to state-of-the-art approaches.

1

Introduction

Large-scale multi-class or extreme classiﬁcation involves problems with extremely large number of
classes as it appears in text repositories such as Wikipedia  Yahoo! Directory (www.dir.yahoo.com) 
or Directory Mozilla DMOZ (www.dmoz.org); and it has recently evolved as a popular branch of
machine learning with many applications in tagging  recommendation and ranking. The most common
and popular baseline in this case is the one-versus-all approach (OVA) [18] where one independent
binary classiﬁer is learned per class. Despite its simplicity  this approach suffers from two main
limitations; ﬁrst  it becomes computationally intractable when the number of classes grow large 
affecting at the same time the prediction. Second  it suffers from the class imbalance problem by
construction.Recently  two main approaches have been studied to cope with these limitations. The
ﬁrst one  broadly divided in tree-based and embedding-based methods  have been proposed with
the aim of reducing the effective space of labels in order to control the complexity of the learning
problem. Tree-based methods [4  3  6  7  9  21  5  15] rely on binary tree structures where each
leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a
binary classiﬁer being used at each node to determine the child node to develop. These methods have
logarithmic time complexity with the drawback that it is a challenging task to ﬁnd a balanced tree
structure which can partition the class labels. Further  even though different heuristics have been
developed to address the unbalanced problem  these methods suffer from the drawback that they have
to make several decisions prior to reaching a ﬁnal category  which leads to error propagation and

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

thus a decrease in accuracy. On the other hand  label embedding approaches [11  5  19] ﬁrst project
the label-matrix into a low-dimensional linear subspace and then use an OVA classiﬁer. However 
the low-rank assumption of the label-matrix is generally transgressed in the extreme multi-class
classiﬁcation setting  and these methods generally lead to high prediction error.The second type of
approaches aim at reducing the original multi-class problem into a binary one by ﬁrst expanding the
original training set using a projection of pairs of observations and classes into a low dimensional
dyadic space  and then learning a single classiﬁer to separate between pairs constituted with examples
and their true classes and pairs constituted with examples with other classes [1  28  16]. Although
prediction in the new representation space is relatively fast  the construction of the dyadic training
observations is generally time consuming and prevails over the training and prediction times.
Contributions. In this paper  we propose a scalable multi-class classiﬁcation method based on
an aggressive double sampling of the dyadic output prediction problem. Instead of computing all
possible dyadic examples  our proposed approach consists ﬁrst in drawing a new training set of much
smaller size from the original one by oversampling the most small size classes and by sub-sampling
the few big size classes in order to avoid the curse of long-tailed class distributions common in the
majority of large-scale multi-class classiﬁcation problems [2]. The second goal is to reduce the
number of constructed dyadic examples. Our reduction strategy brings inter-dependency between the
pairs containing the same observation and its true class in the original training set. Thus  we derive
new generalization bounds using local fractional Rademacher complexity showing that even with a
shift in the original class distribution and also the inter-dependency between the pairs of example  the
empirical risk minimization principle over the transformation of the sampled training set remains
consistent. We validate our approach by conducting a series of experiments on subsets of DMOZ and
the Wikipedia collections with up to 100 000 target categories.

2 A doubly-sampled multi-class to binary reduction strategy
We address the problem of monolabel multi-class classiﬁcation deﬁned on joint space X × Y
where X ⊆ Rd is the input space and Y = {1  . . .   K} .
= [K] the output space  made of K
classes. Elements of X × Y are denoted as xy = (x  y). Furthermore  we assume the training set
S = (xyi
i=1 is made of m i.i.d examples/class pairs distributed according to a ﬁxed but unknown
probability distribution D  and we consider a class of predictor functions G = {g : X × Y → R}.
We deﬁne the instantaneous loss for predictor g ∈ G on example xy as:

i )m

e(g  xy) =

1

K − 1

(cid:88)

y(cid:48)∈Y\{y}

1

g(xy)≤g(xy(cid:48)

) 

(1)

y(cid:54)=argmaxy(cid:48)∈Y g(xy(cid:48)

where 1π is the indicator function equal to 1 if the predicate π is true and 0 otherwise. Compared to
the classical multi-class error  e(cid:48)(g  xy) = 1
)  the loss of (1) estimates the average
number of classes  given any input data  that get a greater scoring by g than the correct class. The
loss (1) is hence a ranking criterion  and the multi-class SVM of [29] and AdaBoost.MR [24] optimize
convex surrogate functions of this loss. It is also used in label ranking [12]. Our objective is to ﬁnd a
function g ∈ G with a small expected risk R(g) = Exy∼D [e(g  xy)]  by minimizing the empirical
i ∈ S which  in mean  are scored lower
error deﬁned as the average number of training examples xyi
than xy(cid:48)
(cid:88)
m(cid:88)

i   for y(cid:48) ∈ Y\{yi} :

m(cid:88)

1

e(g  xyi

i ) =

m(K − 1)

i=1

i=1

y(cid:48)∈Y\{yi}

1

g(xyi

i )−g(xy(cid:48)

i )≤0

.

(2)

˜Rm(g S) =

1
m

2.1 Binary reduction based on dyadic representations of examples and classes
In this work  we consider prediction functions of the form g = f ◦ φ  where φ : X × Y → Rp
is a projection of the input and the output space into a joint feature space of dimension p; and
f ∈ F = {f : Rp → R} is a function that measures the adequacy between an observation x and
a class y using their corresponding representation φ(xy). The projection function φ is application-
dependent and it can either be learned [28]  or deﬁned using some heuristics [27  16].

2

Further  consider the following dyadic transformation

(cid:18)(cid:26) (cid:0)zj =(cid:0)φ(xk
(cid:0)zj =(cid:0)φ(xyi

i )(cid:1)   ˜yj = −1(cid:1)
i )(cid:1)   ˜yj = +1(cid:1)

i )  φ(xyi
i )  φ(xk

T (S) =

(cid:19)

(3)
where j = (i − 1)(K − 1) + k with i ∈ [m]  k ∈ [K − 1]; that expands a K-class labeled set S of
size m into a binary labeled set T (S) of size N = m(K − 1) (e.g. Figure 1 over a toy problem).
With the class of functions

.
=(i−1)(K−1)+k

 

j

H = {h : Rp × Rp → R; (φ(xy)  φ(xy(cid:48)

)) (cid:55)→ f (φ(xy)) − f (φ(xy(cid:48)

))  f ∈ F} 

(4)

if k < yi
elsewhere

the empirical loss (Eq. (2)) can be rewritten as :

N(cid:88)

1
N

= (cid:80)

.

S

xy4
4

xy3
3

xy1
1

xy2
2

j=1

˜RT (S)(h) =

1˜yj h(zj )≤0.

(5)
the minimization of Eq. (5) over the transformation T (S) of a training set S

Hence 
deﬁnes a binary classiﬁcation over the pairs of
dyadic examples. However  this binary problem
takes as examples dependent random variables 
as for each original example xy ∈ S  the K − 1
pairs in {(φ(xy)  φ(xy(cid:48)
)); ˜y} ∈ T (S) all de-
pend on xy. In [16] this problem is studied by
bounding the generalization error associated to
(5) using the fractional Rademacher complex-
ity proposed in [25]. In this work  we derive
a new generalization bounds based on Local
Rademacher Complexities introduced in [22]
that implies second-order (i.e. variance) information inducing faster convergence rates (Theorem 1).
Our analysis relies on the notion of graph covering introduced in [14] and deﬁned as :
Deﬁnition 1 (Exact proper fractional cover of G  [14]). Let G = (V E) be a graph. C =
{(Ck  ωk)}k∈[J]  for some positive integer J  with Ck ⊆ V and ωk ∈ [0  1] is an exact proper
fractional cover of G  if: i) it is proper: ∀k  Ck is an independent set  i.e.  there is no connections

between vertices in Ck; ii) it is an exact fractional cover of G: ∀v ∈ V  (cid:80)

Figure 1: A toy example depicting the transforma-
tion T (Eq. (3)) applied to a training set S of size
m = 4 and K = 4.

1 ))  +1) (z3 = (φ(xy1
2 ))  +1) (z6 = (φ(xy2
3 )) −1) (z9 = (φ(xy3
4 )) −1) (z12 = (φ(xy3

1 ))  +1) (z2 = (φ(xy1
2 )) −1) (z5 = (φ(xy2
3 )) −1) (z8 = (φ(xy2
4 )) −1) (z11 = (φ(xy2

(z1 = (φ(xy1
(z4 = (φ(xy1
(z7 = (φ(xy1
(z10 = (φ(xy1

1 )  φ(xy4
2 )  φ(xy4
3 )  φ(xy4
4 )  φ(xy4

1 )  φ(xy2
2 )  φ(xy2
3 )  φ(xy3
4 )  φ(xy4

1 )  φ(xy3
2 )  φ(xy3
3 )  φ(xy3
4 )  φ(xy4

1 ))  +1)
2 ))  +1)
3 ))  +1)
4 )) −1)

ωk = 1.

T

k:v∈Ck

From this statement 

The weight W (C) of C is given by: W (C)
k∈[J] ωk and the minimum weight
χ∗(G) = minC∈K(G) W (C) over the set K(G) of all exact proper fractional covers of
G is the fractional chromatic number of G.
[14] extended Ho-
effding’s inequality and proposed large deviation bounds for sums of dependent
ran-
dom variables which was the precursor of new generalisation bounds 
including a Tala-
grand’s type inequality for empirical processes in the dependent case presented in [22].
With the classes of functions G and H intro-
duced previously  consider the parameterized
family Hr which  for r > 0  is deﬁned as:
Hr = {h : h ∈ H  V[h]
= Vz ˜y[1˜yh(z)] ≤ r} 
.
where V denotes the variance.
The fractional Rademacher complexity intro-
duced in [25] entails our analysis :
RT (S)(H)
.
ωkECksup
=
h∈H

Figure 2: The dependency graph G = {1  . . .   12}
corresponding to the toy problem of Figure 1 
with (ξi)N
independent
Rademacher variables verifying P(ξn = 1) =
where dependent nodes are connected with ver-
P(ξn=−1) = 1
tices in blue double-line. The exact proper frac-
2. If other is not speciﬁed explic-
tional cover C1  C2 and C3 is shown in dashed.
itly we assume below all ωk = 1. Our ﬁrst result
The fractional chromatic number is in this case
that bounds the generalization error of a function
χ∗(G) = K − 1 = 3.
h ∈ H; R(h) = ET (S)[ ˜RT (S)(h)]  with respect
to its empirical error ˜RT (S)(h) over a transformed training set  T (S)  and the fractional Rademacher
complexity  RT (S)(H)  is stated below.

i=1 a sequence of

Eξ
k∈[K−1]

(cid:88)

(cid:88)

ξαh(zα) 

zα∈T (S)

2
N

α∈Ck

3

i=1 ∈ (X × Y)m be a dataset of m examples drawn i.i.d. according to a
Theorem 1. Let S = (xyi
probability distribution D over X × Y and T (S) = ((zi  ˜yi))N
i=1 the transformed set obtained as
in Eq. (3). Then for any 1 > δ > 0 and 0/1 loss (cid:96) : {−1  +1} × R → [0  1]  with probability at
least (1 − δ) the following generalization bound holds for all h ∈ Hr :

i )m

R(h) ≤ ˜RT (S)(h) + RT (S)((cid:96) ◦ Hr) +

5
2

RT (S)((cid:96) ◦ Hr) +

log 1
δ
m

+

25
48

log 1
δ
m

.

(cid:19)(cid:115)

(cid:114) r

2

(cid:18)(cid:113)

The proof is provided in the supplementary material  and it relies on the idea of splitting up the
sum (5) into several parts  each part being a sum of independent variables.

2.2 Aggressive Double Sampling

i )m
i=1

Algorithm: (π  κ)-DS
Input: Labeled training set S = (xyi
initialization: Sπ ← ∅;
Tκ(Sπ) ← ∅ ;
for k = 1..K do

Even-though the previous multi-class to binary transformation T with a proper projection function
φ allows to redeﬁne the learning problem in a dyadic feature space of dimension p (cid:28) d  the
increased number of examples can lead to a large computational overhead. In order to cope with
this problem  we propose a (π  κ)-double subsampling of T (S)  which ﬁrst aims to balance the
presence of classes by constructing a new training set Sπ from S with probabilities π = (πk)K
k=1.
The idea here is to overcome
the curse of long-tailed class
distributions exhibited in ma-
jority of large-scale multi-
class classiﬁcation problems
[2] by oversampling the most
small size classes and by sub-
sampling the few big size
classes in S. The hyperpa-
rameters π are formally de-
ﬁned as ∀k  πk = P (xy ∈
Sπ|xy ∈ S).
In practice
we set them inversely pro-
portional to the size of each
class in the original training
set; ∀k  πk ∝ 1/µk where
µk is the proportion of class
k in S. The second aim is to
reduce the number of dyadic
examples controlled by the
hyperparameter κ. The pseudo-code of this aggressive double sampling procedure  referred to as
(π  κ)-DS  is depicted above and it is composed of two main steps.

Draw randomly a set Sπk of examples of class k from S with
probability πk;
Sπ ← Sπ ∪ Sπk;
forall xy ∈ Sπ do
Draw uniformly a set Yxy of κ classes from Y\{y} (cid:46) κ (cid:28) K;
forall k ∈ Yxy do
if k < y then

Tκ(Sπ) ← Tκ(Sπ) ∪(cid:0)z =(cid:0)φ(xk)  φ(xy)(cid:1)  ˜y = −1(cid:1);
Tκ(Sπ) ← Tκ(Sπ) ∪(cid:0)z =(cid:0)φ(xy)  φ(xk)(cid:1)  ˜y = +1(cid:1);

return Tκ(Sπ)

else

1. For each class k ∈ {1  . . .   K}  draw randomly a set Sπk of examples from S of that class

with probability πk  and let Sπ =

Sπk;

2. For each example xy in Sπ  draw uniformly κ adversarial classes in Y\{y}.

k=1

K(cid:91)

After this double sampling  we apply the transformation T deﬁned as in Eq. (3)  leading to a set
Tκ(Sπ) of size κ|Sπ| (cid:28) N.
In Section 3  we will show that this procedure practically leads to dramatic improvements in terms of
memory consumption  computational complexity  and a higher multi-class prediction accuracy when
the number of classes is very large. The empirical loss over the transformation of the new subsampled
training set Sπ of size M  outputted by the (π  κ)-DS algorithm is :

˜RTκ(Sπ)(h) =

1
κM

1˜yαh(zα)≤0 =

1
κM

(˜yα zα)∈Tκ(Sπ)

1

g(xy)−g(xy(cid:48)

)≤0 

(6)

which is essentially the same empirical risk as the one deﬁned in Eq. (2) but taken with respect to the
training set outputted by the (π  κ)-DS algorithm. Our main result is the following theorem which
bounds the generalization error of a function h ∈ H learned by minimizing ˜RTκ(Sπ).

4

(cid:88)

(cid:88)

xy∈Sπ

y(cid:48)∈Yxy

(cid:88)

i )m

i=1 ∈ (X × Y)m be a training set of size m i.i.d. according to a
Theorem 2. Let S = (xyi
probability distribution D over X × Y  and T (S) = ((zi  ˜yi))N
i=1the transformed set obtained with
the transformation function T deﬁned as in Eq. (3). Let Sπ ⊆ S  |Sπ| = M  be a training set
outputted by the algorithm (π  κ)-DS and T (Sπ) ⊆ T (S) its corresponding transformation. Then for
any 1 > δ > 0 with probability at least (1 − δ) the following risk bound holds for all h ∈ H :

R(h) ≤ α ˜RTκ(Sπ)(h) + αRTκ(Sπ)((cid:96) ◦ H) + α
.
Furthermore  for all functions in the class Hr  we have the following generalization bound that holds
with probability at least (1 − δ) :

2α log 4K
δ
β(m − 1)

7β log 4K
δ
3(m − 1)

2M κ

+

+

δ

(K − 1) log 2

(cid:115)

(cid:115)

(cid:115)
(cid:19)(cid:115)

(cid:114) r

2

+

2α log 4K
δ
β(m − 1)
(K − 1) log 2

δ

M κ

7β log 4K
δ
3(m − 1)

+

25α
48

log 2
δ
M

 

R(h) ≤α ˜RTκ(Sπ)(h) + αRTκ(Sπ)((cid:96) ◦ Hr) +

(cid:18)(cid:113)

+

5α
2

RTκ(Sπ)((cid:96) ◦ Hr) +

where (cid:96) : {−1  +1} × R → [0  1] 0/1 is an instantaneous loss  and α = maxy: 1≤y≤K ηy/πy 
β = maxy: 1≤y≤K 1/πy and ηy > 0 is the proportion of class y in S.
The proof is provided in the supplementary material. This theorem hence paves the way for the
consistency of the empirical risk minimization principle [26  Th. 2.1  p. 38] deﬁned over the double
sample reduction strategy we propose.

2.3 Prediction with Candidate Selection

the

the

out

in

the

that

score

dyadic

feature

highest

learned

space 
the

by ﬁrst
classes 

consider-
and then
classiﬁer.

carried

is
constituted by a
leads
class

test observation and all
by
to

The
prediction
ing the pairs
choosing
the
In the large scale scenario  com-
puting the feature representations
for all classes may require a huge
amount of time. To overcome this
problem we sample over classes
by choosing just those that are the
nearest to a test example  based on
its distance with class centroids.
Here we propose to consider class
centroids as the average of vectors
within that class. Note that class centroids are computed once in the preliminary projection of training
examples and classes in the dyadic feature space and thus represent no additional computation at this
stage. The algorithm above presents the pseudocode of this candidate based selection strategy 1.

Algorithm: Prediction with Candidate Selection Algorithm
Input: Unlabeled test set T ;
Learned function f∗ : Rp → R;
initialization: Ω ← ∅;
forall x ∈ T do

Select Yx ⊆ Y candidate set of q nearest-centroid classes;
Ω ← Ω ∪ argmaxk∈Yx f∗(φ(xk)) ;

return predicted classes Ω

3 Experiments

In this section  we provide an empirical evaluation of the proposed reduction approach with the (π  κ)-
DS sampling strategy for large-scale multi-class classiﬁcation of document collections. First  we
present the mapping φ : X × Y → Rp. Then  we provide a statistical and computational comparison
of our method with state-of-the-art large-scale approaches on popular datasets.

3.1

a Joint example/class representation for text classiﬁcation

The particularity of text classiﬁcation is that documents are represented in a vector space induced by
the vocabulary of the corresponding collection [23]. Hence each class can be considered as a mega-
document  constituted by the concatenation of all of the documents in the training set belonging to it 

1The number of classes pre-selected can be tuned to offer a prediction time/accuracy tradeoff if the prediction

time is more critical.

5

Features in the joint example/class representation representation φ(xy).

3. (cid:88)
6. (cid:88)

t∈y∩x

It

(cid:18)

log

1 +

yt|y| .It
9. d(xy  centroid(y))

t∈y∩x

(cid:19)

(cid:19)
(cid:19)

lS
Ft

yt|y|

(cid:18)
(cid:18)

log

1 +

t∈y∩x

t∈y∩x

t∈y∩x

log(1 + yt)

2. (cid:88)
5. (cid:88)
8. (cid:88)

1. (cid:88)
4. (cid:88)
(cid:18)
yt|y| .It
7. (cid:88)
10. BM25 =(cid:80)
within S  and xt is the frequency of term t in x  yt =(cid:80)
lS =(cid:80)

yt+(0.25+0.75·len(y)/avg(len(y))

t∈y∩x It.

yt|y| .

t∈y∩x

t∈y∩x

log

1 +

log

1 +

t∈y∩x

2×yt

(cid:19)

lS
Ft

1

Table 1: Joint example/class representation for text classiﬁcation  where t ∈ y ∩ x are terms that are
present in both the class y’s mega-document and document x. V represents the set of distinct terms
x∈S xt 
t∈V St. Finally  It is the inverse document frequency of term t  len(y) is number of terms of

x∈y xt  |y| =(cid:80)

t∈V yt  Ft =(cid:80)

documents in class y  and avg(len(y)) is the average of document lengths for all the classes.

and simple feature mapping of examples and classes can be deﬁned over their common words. Here
we used p = 10 features inspired from learning to rank [17] by resembling a class and a document to
respectively a document and a query (Table 1). All features except feature 9  that is the distance of
an example x to the centroid of all examples of a particular class y  are classical. In addition to its
predictive interest  the latter is also used in prediction for performing candidate preselection. Note
that for other large-scale multi-class classiﬁcation applications like recommendation with extremely
large number of offer categories or image classiﬁcation  a same kind of mapping can either be learned
or deﬁned using their characteristics [27  28].

3.2 Experimental Setup

Datasets. We evaluate the proposed method using popular datasets from the Large Scale Hierarchical
Text Classiﬁcation challenge (LSHTC) 1 and 2 [20]. These datasets are provided in a pre-processed
format using stop-word removal and stemming. Various characteristics of these datesets including the
statistics of train  test and heldout are listed in Table 2. Since  the datasets used in LSHTC2 challenge
were in multi-label format  we converted them to multi-class format by replicating the instances
belonging to different class labels. Also  for the largest dataset (WIKI-large) used in LSHTC2
challenge  we used samples with 50 000 and 100 000 classes. The smaller dataset of LSHTC2
challenge is named as WIKI-Small  whereas the two 50K and 100K samples of large dataset are
named as WIKI-50K and WIKI-100K in our result section.

# of classes  K Train Size Test Size Heldout Size Dimension  d

Datasets
LSHTC1
DMOZ

WIKI-Small
WIKI-50K
WIKI-100K

12294
27875
36504
50000
100000

126871
381149
796617
1102754
2195530

31718
95288
199155
276939
550133

5000
34506
5000
5000
5000

409774
594158
380078
951558
1271710

Table 2: Characteristics of the datasets used in our experiments

Baselines. We compare the proposed approach 2 denoted as the sampling strategy by (π  κ)-DS 
with popular baselines listed below:

• OVA: LibLinear [10] implementation of one-vs-all SVM.
• M-SVM: LibLinear implementation of multi-class SVM proposed in [8].
• RecallTree [9]: A recent tree based multi-class classiﬁer implemented in Vowpal Wabbit.

2Source code and datasets can be found in the following repository https://github.com/bikash617/Aggressive-

Sampling-for-Multi-class-to-BinaryReduction

6

Data

LSHTC1
m = 163589
d = 409774
K = 12294

DMOZ

m = 510943
d = 594158
K = 27875

WIKI-Small
m = 1000772
d = 380078
K = 36504

WIKI-50K
m = 1384693
d = 951558
K = 50000

WIKI-100K
m = 2750663
d = 1271710
K = 100000

train time
predict time
total memory

Accuracy

MaF1

train time
predict time
total memory

Accuracy

MaF1

train time
predict time
total memory

Accuracy

MaF1

train time
predict time
total memory

Accuracy

MaF1

train time
predict time
total memory

Accuracy

MaF1

OVA

23056s
328s
40.3G
44.1%
27.4%
180361s
2797s
131.9G
37.7%
22.2%
212438s
2270s
109.1G
15.6%
8.8 %
NA
NA
330G
NA
NA
NA
NA

1017G

NA
NA

M-SVM
48313s
314s
40.3G
36.4%
18.8%
212356s
3981s
131.9G
32.2%
14.3%
>4d
NA

109.1G

NA
NA
NA
NA
330G
NA
NA
NA
NA

1017G

NA
NA

701s
21s
122M
18.1%
3.8%
2212s
47s
256M
16.9%
1.75%
1610s
24s
178M
7.9%
<1%
4188s
45s
226M
17.9%
5.5%
8593s
90s
370M
8.4%
1.4%

8564s
339s
470M
39.3%
21.3%
14334s
424s
1339M
33.4%
15.1%
10646s
453s
949M
11.1%
4.6%
30459s
1110s
1327M
25.8%
14.6%
42359s
1687s
2622M
15%
8%

3912s
164s
471M
39.8%
22.4%
15492s
505s
1242M
33.7%
15.9%
21702s
871s
947M
12.1%
5.63%
48739s
2461s
1781M
27.3%
16.3%
73371s
3210s
2834M
16.1%

9%

5105s
67s
10.5G
45.7%
27.7%
63286s
482s
28.1G
40.8%
22.7%
16309s
382s
12.4G
15.6%
9.91%
41091s
790s
35G
33.8%
23.4%
155633s
3121s
40.3G
22.2%
15.1%

321s
544s
2G

37.4%
26.5%
1060s
2122s
5.3G
27.8%
20.5%
1290s
2577s
3.6G
21.5%
13.3%
3723s
4083s
5G

33.4%
24.5%
9264s
20324s
9.8G
25%
17.8%

RecallTree

FastXML

PfastReXML

PD-Sparse

(π  κ)-DS

Table 3: Comparison of the result of various baselines in terms of time  memory  accuracy  and
macro F1-measure

• FastXML [21]: An extreme multi-class classiﬁcation method which performs partitioning in

the feature space for faster prediction.

• PfastReXML [13]: Tree ensemble based extreme classiﬁer for multi-class and multilabel

problems.

• PD-Sparse [30]: A recent approach which uses multi-class loss with (cid:96)1-regularization.

Referring to the work [30]  we did not consider other recent methods SLEEC [5] and LEML [31] in our
experiments  since they have been shown to be consistently outperformed by the above mentioned
state-of-the-art approaches.
Platform and Parameters. In all of our experiments  we used a machine with an Intel Xeon 2.60GHz
processor with 256 GB of RAM. Each of these methods require tuning of various hyper-parameters
that inﬂuence their performance. For each methods  we tuned the hyperparameters over a heldout set
and used the combination which gave best predictive performance. The list of used hyperparameters
for the results we obtained are reported in the supplementary material (Appendix B).
Evaluation Measures. Different approaches are evaluated over the test sets using accuracy and
the macro F1 measure (MaF1)  which is the harmonic average of macro precision and macro recall;
higher MaF1thus corresponds to better performance. As opposed to accuracy  macro F1 measure is
not affected by the class imbalance problem inherent to multi-class classiﬁcation  and is commonly
used as a robust measure for comparing predictive performance of classiﬁcation methods.

4 Results

The parameters of the datasets along with the results for compared methods are shown in Table 3.
The results are provided in terms of train and predict times  total memory usage  and predictive
performance measured with accuracy and macro F1-measure (MaF1). For better visualization and
comparison  we plot the same results as bar plots in Fig. 3 keeping only the best ﬁve methods while
comparing the total runtime and memory usage. First  we observe that the tree based approaches
(FastXML  PfastReXML and RecallTree) have worse predictive performance compared to the other
methods. This is due to the fact that the prediction error made at the top-level of the tree cannot be
corrected at lower levels  also known as cascading effect. Even though they have lower runtime and
memory usage  they suffer from this side effect.
For large scale collections (WIKI-Small  WIKI-50K and WIKI-100K)  the solvers with competitive
predictive performance are OVA  M-SVM  PD-Sparse and (π  κ)-DS. However  standard OVA and

7

Figure 3: Comparisons in Total (Train and Test) Time (min.)  Total Memory usage (GB)  and MaF1 of
the ﬁve best performing methods on LSHTC1  DMOZ  WIKI-Small  WIKI-50K and WIKI-100K.

M-SVM have a complexity that grows linearly with K thus the total runtime and memory usage
explodes on those datasets  making them impossible. For instance  on Wiki large dataset sample
of 100K classes  the memory consumption of both approaches exceeds the Terabyte and they take
several days to complete the training. Furthermore  on this data set and the second largest Wikipedia
collection (WIKI-50K and WIKI-100K) the proposed approach is highly competitive in terms of
Time  Total Memory and both performance measures comparatively to all the other approaches.
These results suggest that the method least affected by long-tailed class distributions is (π  κ)-DS 
mainly because of two reasons: ﬁrst  the sampling tends to make the training set balanced and
second  the reduced binary dataset contains similar number of positive and negative examples. Hence 
for the proposed approach  there is an improvement in both accuracy and MaF1 measures. The
recent PD-Sparse method also enjoys a competitive predictive performance but it requires to store
intermediary weight vectors during optimization which prevents it from scaling well. The PD-Sparse
solver provides an option for hashing leading to fewer memory usage during training which we used
in the experiments; however  the memory usage is still signiﬁcantly high for large datasets and at the
same time this option slows down the training process considerably. In overall  among the methods
with competitive predictive performance  (π  κ)-DS seems to present the best runtime and memory
usage; its runtime is even competitive with most of tree-based methods  leading it to provide the best
compromise among the compared methods over the three time  memory and performance measures.

5 Conclusion

We presented a new method for reducing a multiclass classiﬁcation problem to binary classiﬁcation.
We employ similarity based feature representation for class and examples and a double sampling
stochastic scheme for the reduction process. Even-though the sampling scheme shifts the distribution
of classes and that the reduction of the original problem to a binary classiﬁcation problem brings
inter-dependency between the dyadic examples; we provide generalization error bounds suggesting
that the Empirical Risk Minimization principle over the transformation of the sampled training set
still remains consistent. Furthermore  the characteristics of the algorithm contribute for its excellent
performance in terms of memory usage and total runtime and make the proposed approach highly
suitable for large class scenario.

Acknowledgments

This work has been partially supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01)
funded by the French program Investissement d’avenir  and by the U.S. Department of Energy’s
Ofﬁce of Electricity as part of the DOE Grid Modernization Initiative.

8

04590135180Time(min.)LSHTC103006009001200DMOZ0150300450WIKI-Small03006009001200WIKI-50K0100020003000WIKI-100K04812TotalMemory(GB)01020300.02.55.07.510.0012243601428420153045Accuracy(%)01530450153045015304501530450102030MaF(%)0102030010203001020300102030RecallTreeFastXMLPfastReXMLPD-SparseProposed(π κ)-DSReferences
[1] Naoki Abe  Bianca Zadrozny  and John Langford. An iterative method for multi-class cost-sensitive

learning. In Proceedings of the 10th ACM SIGKDD  KDD ’04  pages 3–11  2004.

[2] Rohit Babbar  Cornelia Metzig  Ioannis Partalas  Eric Gaussier  and Massih R. Amini. On power law

distributions in large-scale taxonomies. SIGKDD Explorations  16(1)  2014.

[3] Samy Bengio  Jason Weston  and David Grangier. Label embedding trees for large multi-class tasks. In

Advances in Neural Information Processing Systems  pages 163–171  2010.

[4] Alina Beygelzimer  John Langford  and Pradeep Ravikumar. Error-correcting tournaments. In Proceedings

of the 20th International Conference on Algorithmic Learning Theory  ALT’09  pages 247–262  2009.

[5] Kush Bhatia  Himanshu Jain  Purushottam Kar  Manik Varma  and Prateek Jain. Sparse local embeddings
for extreme multi-label classiﬁcation. In Advances in Neural Information Processing Systems  pages
730–738  2015.

[6] Anna Choromanska  Alekh Agarwal  and John Langford. Extreme multi class classiﬁcation. In NIPS

Workshop: eXtreme Classiﬁcation  submitted  2013.

[7] Anna Choromanska and John Langford. Logarithmic time online multiclass prediction. CoRR 

abs/1406.1822  2014.

[8] Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector

machines. J. Mach. Learn. Res.  2:265–292  2002.

[9] Hal Daume III  Nikos Karampatziakis  John Langford  and Paul Mineiro. Logarithmic time one-against-

some. arXiv preprint arXiv:1606.04988  2016.

[10] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. Liblinear: A library

for large linear classiﬁcation. J. Mach. Learn. Res.  9:1871–1874  2008.

[11] Daniel J Hsu  Sham M Kakade  John Langford  and Tong Zhang. Multi-label prediction via compressed

sensing. In Advances in Neural Information Processing Systems 22 (NIPS)  pages 772–780  2009.

[12] Eyke Hüllermeier and Johannes Fürnkranz. On minimizing the position error in label ranking. In Machine

Learning: ECML 2007  pages 583–590. Springer  2007.

[13] Himanshu Jain  Yashoteja Prabhu  and Manik Varma. Extreme multi-label loss functions for recommen-
dation  tagging  ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 935–944. ACM  2016.

[14] S. Janson. Large deviations for sums of partly dependent random variables. Random Structures and

Algorithms  24(3):234–248  2004.

[15] Kalina Jasinska and Nikos Karampatziakis. Log-time and log-space extreme classiﬁcation. arXiv preprint

arXiv:1611.01964  2016.

[16] Bikash Joshi  Massih-Reza Amini  Ioannis Partalas  Liva Ralaivola  Nicolas Usunier  and Éric Gaussier.
On binary reduction of large-scale multiclass classiﬁcation problems. In Advances in Intelligent Data
Analysis XIV - 14th International Symposium  IDA 2015  pages 132–144  2015.

[17] Tie-Yan Liu  Jun Xu  Tao Qin  Wenying Xiong  and Hang Li. Letor: Benchmark dataset for research on
learning to rank for information retrieval. In Proceedings of SIGIR 2007 workshop on learning to rank for
information retrieval  pages 3–10  2007.

[18] Ana Carolina Lorena  André C. Carvalho  and João M. Gama. A review on the combination of binary

classiﬁers in multiclass problems. Artif. Intell. Rev.  30(1-4):19–37  2008.

[19] Paul Mineiro and Nikos Karampatziakis. Fast label embeddings via randomized linear algebra. In Machine
Learning and Knowledge Discovery in Databases - European Conference  ECML PKDD 2015  Porto 
Portugal  September 7-11  2015  Proceedings  Part I  pages 37–51  2015.

[20] I. Partalas  A. Kosmopoulos  N. Baskiotis  T. Artieres  G. Paliouras  E. Gaussier  I. Androutsopoulos  M.-R.
Amini  and P. Galinari. LSHTC: A Benchmark for Large-Scale Text Classiﬁcation. ArXiv e-prints  March
2015.

[21] Yashoteja Prabhu and Manik Varma. Fastxml: A fast  accurate and stable tree-classiﬁer for extreme
multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 263–272. ACM  2014.

[22] Liva Ralaivola and Massih-Reza Amini. Entropy-based concentration inequalities for dependent variables.
In Proceedings of the 32nd International Conference on Machine Learning  ICML 2015  Lille  France 
6-11 July 2015  pages 2436–2444  2015.

[23] G. Salton  A. Wong  and C. S. Yang. A vector space model for automatic indexing. Commun. ACM 

18(11):613–620  November 1975.

9

[24] Robert E Schapire and Yoram Singer. Improved boosting algorithms using conﬁdence-rated predictions.

Machine learning  37(3):297–336  1999.

[25] Nicolas Usunier  Massih-Reza Amini  and Patrick Gallinari. Generalization error bounds for classiﬁers
trained with interdependent data. In Advances in Neural Information Processing Systems 18 (NIPS)  pages
1369–1376  2005.

[26] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.
[27] Maksims Volkovs and Richard S. Zemel. Collaborative ranking with 17 parameters. In Advances in Neural

Information Processing Systems 25  pages 2294–2302  2012.

[28] Jason Weston  Samy Bengio  and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image
annotation. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence  IJCAI  2011.
[29] Jason Weston and Chris Watkins. Multi-class support vector machines. Technical report  Technical Report

CSD-TR-98-04  Department of Computer Science  Royal Holloway  University of London  1998.

[30] Ian EH Yen  Xiangru Huang  Kai Zhong  Pradeep Ravikumar  and Inderjit S Dhillon. Pd-sparse: A primal
and dual sparse approach to extreme multiclass and multilabel classiﬁcation. In Proceedings of the 33nd
International Conference on Machine Learning  2016.

[31] Hsiang-Fu Yu  Prateek Jain  Purushottam Kar  and Inderjit Dhillon. Large-scale multi-label learning with

missing labels. In International Conference on Machine Learning  pages 593–601  2014.

10

,Bikash Joshi
Massih R. Amini
Ioannis Partalas
Franck Iutzeler
Yury Maximov