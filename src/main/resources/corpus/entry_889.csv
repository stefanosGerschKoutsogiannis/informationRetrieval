2019,CXPlain: Causal Explanations for Model Interpretation under Uncertainty,Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding  validating  and interpreting machine-learning models. However  providing fast and accurate estimates of feature importance for high-dimensional data  and quantifying the uncertainty of such estimates remain open challenges. Here  we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task  and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can  once trained  be used to explain the target model in little time  and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition  we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.,CXPlain: Causal Explanations for Model

Interpretation under Uncertainty

Patrick Schwab and Walter Karlen

Institute of Robotics and Intelligent Systems  ETH Zurich

patrick.schwab@hest.ethz.ch

Abstract

Feature importance estimates that inform users about the degree to which given
inputs inﬂuence the output of a predictive model are crucial for understanding 
validating  and interpreting machine-learning models. However  providing fast and
accurate estimates of feature importance for high-dimensional data  and quantifying
the uncertainty of such estimates remain open challenges. Here  we frame the task
of providing explanations for the decisions of machine-learning models as a causal
learning task  and train causal explanation (CXPlain) models that learn to estimate
to what degree certain inputs cause outputs in another machine-learning model.
CXPlain can  once trained  be used to explain the target model in little time  and
enables the quantiﬁcation of the uncertainty associated with its feature importance
estimates via bootstrap ensembling. We present experiments that demonstrate that
CXPlain is signiﬁcantly more accurate and faster than existing model-agnostic
methods for estimating feature importance.
In addition  we conﬁrm that the
uncertainty estimates provided by CXPlain ensembles are strongly correlated with
their ability to accurately estimate feature importance on held-out data.

1

Introduction

Explanation methods for machine-learning models play an important role in researching  developing 
and using predictive models as information on what features were important for a given output enable
us to better understand  validate  and interpret model decisions [1–5]. However  complex models 
such as ensemble models and deep neural networks  are often difﬁcult to interrogate. To address
this apparent dichotomy between performance and interpretability [6]  researchers have developed a
number of attribution methods that provide estimates of the importance of input features towards a
model’s output for speciﬁc types of models [4  7–15]  and for any machine-learning model [6  16].
However  providing fast and accurate feature importance estimates for any machine-learning model
is challenging because there exists a wide variety of intricate machine-learning models with different
underlying model structures  algorithms  and decision functions  which makes it difﬁcult to develop
an optimised and uniﬁed approach to importance attribution. Furthermore  importance estimates
of state-of-the-art methods are typically associated with signiﬁcant uncertainty [3  17–19]  and it is
therefore difﬁcult for users to judge when importance estimates can be expected to be accurate.
In this work  we present a new approach to estimating feature importance for any machine-learning
model using causal explanation (CXPlain) models. CXPlain uses a causal objective to train a super-
vised model to learn to explain another machine-learning model. This approach can be applied to any
machine-learning model  since it has no requirements on the predictive model to be explained. In par-
ticular  it does not require retraining or adapting the original model. We demonstrate experimentally
that CXPlain is signiﬁcantly more accurate than most existing methods  fast  and able to produce
accurate uncertainty estimates. Source code is available at https://github.com/d909b/cxplain.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

estimate feature importance for any machine-learning model.

Contributions. This work contains the following contributions:
• We introduce causal explanation (CXPlain) models  a new method for learning to accurately
• We present a methodology based on bootstrap resampling for deriving uncertainty estimates for
• Our experiments show that CXPlain is signiﬁcantly more accurate and signiﬁcantly faster (at
evaluation time) than existing model-agnostic methods  and that the uncertainty estimates for
its assigned feature importance scores are strongly correlated with the accuracy of the provided
importance scores on previously unseen test data.

the feature importance scores provided by CXPlain.

2 Related Work

Feature Importance Estimation. Existing methods for feature importance estimation can be
subdivided into (1) gradient-based methods  (2) methods based on sensitivity analysis  (3) methods
that measure the change in model conﬁdence when removing input features  and (4) mimic models.
Simple Gradient (SG) [8]  Integrated Gradients (IG) [10]  DeepLIFT [1]  and DeepSHAP [6] are
examples of gradient-based methods. Gradient-based methods are only applicable to differentiable
models  such as neural networks  and their computation is typically fast. Methods that quantify a
model’s sensitivity to changes in the input  such as LIME [16] or SHAP [6]  and more speciﬁcally
Kernel SHAP  are applicable to any machine-learning model but typically slow to compute  as large
numbers of model evaluations are necessary to assess a model’s sensitivity. Methods based on
masking parts of the input and measuring the model’s resulting change in conﬁdence [20] include
conditional multivariate models for visualising deep neural networks [21]  analysing the effects
of erasing parts of their representations [22]  image interpretation by identifying the regions for
which the model most strongly responds to perturbations [23]  and image masking models trained to
manipulate the outputs of a predictive model by occluding parts of the input [24]. The fourth main
category of approaches to explaining model decisions is to train interpretable models that mimic the
decisions of a black-box model that we wish to explain. Tree- [25–27] and rule-based [28] models
have been used as mimic models. However  mimic models are not guaranteed to match the behavior
of the original model. Besides these four established categories of feature importance estimation
methods  structural causal models (SCMs) [29] and Deep Taylor Decomposition (DTD) [30] have
also recently been proposed as explanation methods. However  these methods are designed for
speciﬁc types of models. In addition  the L2X method that uses a variational approximation of mutual
information [31] and Bayesian nonparametrics [32] have been proposed to explain a target model.
Tsang et al. [33] detected statistical interactions by interpreting the weights learned in neural networks.
Beyond feature attribution  testing with concept activation vectors (TCAV) [34] was proposed to
visualise the internal state of deep learning models  and inﬂuence functions [35] have been used
to identify the training data most responsible for a given model decision. A major limitation of
most existing methods for feature importance estimation is that they do not inform users when their
estimates are signiﬁcantly uncertain and can not be expected to be accurate.

Uncertainty and Reliability of Explanations. Although reliability is necessary for model expla-
nations to be trustworthy  relatively few studies have been concerned with quantifying the uncertainty
and robustness of explanation methods. For example  it has been shown that multiple importance
estimation methods incorrectly attribute when a constant vector shift is applied to the input [3]  that
the attributions provided by interpretation methods may themselves contain signiﬁcant uncertainty
[18]  that some explanation methods are independent of both the model and the data-generating
process and  thus  can not be relied upon for important interpretation tasks [17]  and that imperceptibly
small perturbations of the input can signiﬁcantly alter the explanations provided by state-of-the-art

Table 1: Comparison of CXPlain to several representative methods for feature importance estimation.

Accuracy
Model-agnostic
Uncertainty estimates
Computation time

CXPlain
high


fast

SG [8] / IG [10] DeepSHAP [1  6]
high


fast

moderate


fast

LIME [16]
high


slow

SHAP [6]
high


slow

2

explanation methods without changing the explained model’s prediction [19]. These studies highlight
the importance of informing users when a given explanation is uncertain and should be discounted.
In contrast to existing works  CXPlain is an explanation model trained with a causal objective to
learn to explain the decisions of any machine-learning model without the need to retrain  adapt  or
have in-depth knowledge of the explained model. To the best of our knowledge  CXPlain is the ﬁrst
feature importance estimation method that is simultaneously (1) signiﬁcantly more accurate than
most existing methods  (2) compatible with any machine-learning model and data modality  (3) able
to provide uncertainty estimates via bootstrap resampling  and (4) fast at evaluation time (Table 1).

3 Methodology

Problem Setting. We consider a setting in which we are given a predictive model ˆf which processes
inputs X consisting of p input features  or groups of features  xi with i ∈ [0 . . p − 1] to produce
outputs ˆy ∈ Rk of any dimensionality k. The predictive model ˆf is scored according to an objective
function L : y × ˆy → s that computes a scalar loss s ∈ R after comparing the model’s predictive
output ˆy to a ground-truth output y ∈ Rk. The mean squared error (MSE) for regression models and
the categorical crossentropy for classiﬁcation models are commonly used examples of such objectives.
We note that we speciﬁcally do not require access to  or knowledge of  the process by which ˆf
produces its output  nor do we require ˆf to be differentiable or of any speciﬁc form. Additionally  we
are given N ∈ N independent and identically distributed (i.i.d.) pairs of sample covariates X and
ground-truth outputs y as training data. Given this setting  our goal is to train an explanation model
ˆfexp that produces accurate estimates ˆA with elements ˆai corresponding to the importances assigned
to each of the p input features xi to the predictive model ˆf.

Causal Explanations (CXPlain). The main idea be-
hind CXPlain is to train a separate explanation model
ˆfexp to explain the predictive model ˆf (Figure 1). This
ﬂexible framework has the advantage that we do not need
to retrain or adapt the predictive model ˆf to explain its de-
cisions. To train the explanation model  we utilise a causal
objective function that quantiﬁes the marginal contribution
of either a single input feature or group of input features
towards the predictive model’s accuracy [14  20]. This
approach  in essence  transforms the task of producing
feature importance estimates for a given predictive model
into a supervised learning task that we can address with existing supervised machine-learning models.

Figure 1: CXPlain trains an explanation
model ˆfexp (bottom) to learn to estimate
importance scores ˆA for a predictive tar-
get model ˆf (top) given features X.

Causal Objective. The core component of CXPlain is the causal objective that enables us to
optimise explanation models to learn to explain another predictive model. The causal objective we
build on was ﬁrst introduced to jointly learn to produce accurate predictions and estimates of feature
importance in a single neural network model [14]. However  the original formulation of the causal
objective required a speciﬁc attentive mixture of experts architecture. In this work  we contribute
an adapted version of the causal objective from [14] that does not require a speciﬁc model structure 
and that can be used to train explanation models to learn to explain any machine-learning model.
The causal objective introduced in [14] was based on the Humean deﬁnition of causality used by
Granger [36]  who deﬁned a causal relationship xi → ˆy between random variables xi and ˆy to
exist if we are better able to predict ˆy using all available information than if the information apart
from xi had been used [14]. i.e. if the absence of xi as a feature decreases our ability to predict
ˆy. Granger [36]’s deﬁnition of causality was based on two key assumptions: (1) That our set of
available variables X contains all relevant variables for the causal problem being modelled  and (2)
that xi temporally precedes ˆy [36]. In the general setting  these assumptions can not be veriﬁed from
observational data [37]. However  in our speciﬁc setting  we know a priori that the inputs of the
predictive model ˆf mathematically always precede its output  and that the explained model’s output 
on deterministic hardware and software  is not inﬂuenced by variables other than those present in its
set of input features. We can therefore use the given deﬁnition to quantify the degree to which an
input feature caused a marginal improvement in the predictive performance of the predictive model

3

Xy^f^predictive modelfexp^explanation modelAinputmodeloutput^(1)

ˆyX\{i} = ˆf (X \ {i})

ˆf. Given input covariates X  we therefore denote εX\{i} as the predictive model’s error without
including any information from the ith input feature and εX as the predictive model’s error when
considering all available input features. To calculate εX\{i} and εX  we ﬁrst compute the outputs
ˆyX\{i} and ˆyX of the predictive model ˆf without and with the ith input feature xi  respectively:
(2)
There are several different approaches to obtaining X \ {i} from the full set of input features 
depending on the type of input data. For most types of data  masking the respective input feature xi at
index i with zeroes  when the zero value has no special meaning  or replacing it with the mean value
across the entire data set are both valid choices [20  21  24]. More sophisticated feature masking
schemes that consider the masked feature’s distribution [38  39] could be a more principled alternative
to masking with point-wise estimates. Given X \ {i}  we compare the predictions ˆyX\{i} and ˆyX
with the ground-truth labels y using the predictive model’s loss function L to calculate εX\{i} and εX:
(4)
Following Granger [36]’s deﬁnition of causality  we deﬁne the degree ∆εi to which the ith input
feature causally contributed to the predictive model’s output ˆy as the decrease in error  as measured
by its loss L  associated with adding that feature to the set of available information sources [14]:

εX\{i} = L(y  ˆyX\{i})

εX = L(y  ˆyX )

ˆyX = ˆf (X)

(3)

∆εX i = εX\{i} − εX

∆εX i
j=0 ∆εX j

(cid:80)p−1
(cid:80)N−1

(5)
Lastly  we normalise the importance scores ωi to relative contributions ∈ [0  1] with Σiωi = 1 [14]:
(6)

ωi(X) =
We then arrive at our causal objective Lcausal = 1
l=0 KL(ΩXl   ˆAXl ) [14] that aims to minimise
the Kullback-Leibler (KL) divergence [40] between the target importance distribution Ω with Ω(i) =
ωi(X) for a given sample X  and the distribution of importance scores ˆA with ˆA(i) = ˆai as estimated
by ˆfexp based on X. Using Lcausal  we can train supervised learning models to learn to explain
any other machine-learning model based solely on its outputs  and without the need to retrain the
model to be explained. Precomputing the importances Ω for each training sample X takes N (p + 1)
evaluations of the target predictive model at training time. For high-dimensional images  it is sensible
to group non-overlapping regions of adjacent pixels into feature groups  since removing single pixels
in high-dimensional images is unlikely to strongly affect a predictive model’s output [21]. This also
signiﬁcantly limits the number of feature groups p for which importances ωi have to be precomputed.
We note that estimating ˆA is not necessary in situations in which ground truth labels are readily
available  e.g. during model development. In those situations  Ω can directly be used to explain ˆf.

N

Explanation Models.
In principle  any supervised machine learning model that can be trained with
a custom objective could be used as a causal explanation model. In this work  we focus on neural
explanation models. Using deep neural networks as causal explanation models has the advantage
that these models are able to extract high-level feature representations from high-dimensional and
unstructured data [41]  and thus remove the need to perform manual feature engineering. We leave
the exploration of other classes of explanation models to future work. A priori  it is not clear which
architectures would be most suitable to be used in neural explanation models. Absent any prior
knowledge about the structure of the input data  multilayer perceptrons (MLPs) are likely a sensible
default choice. However  since architectures that exploit the spatial or temporal structure of input
data have been shown to be efﬁcacious  we reason that  depending on the data modality of the input
features of the model to be explained  special-purpose architectures  such as convolutional neural
networks [42] for images and attentive neural networks for texts [43]  could perform better than
MLPs. In particular  U-nets [44] that have been designed for image segmentation  a task that involves
mapping input pixels to segmentation labels  may perform well as causal explanation models for
images since segmentation is semantically similar to explanation  which involves mapping input
pixels to importance scores. To determine whether or not specialised model architectures can achieve
better performances in neural explanation models  we experimentally evaluate both MLPs and U-nets.

Uncertainty of Importance Estimates.
In addition to producing accurate estimates of feature
importance  we wish to provide uncertainty estimates ui that quantify the uncertainty associated
with each individual feature importance estimate ˆai produced by a CXPlain model. In particular  we

4

Figure 2: Comparison of the distributions of the
changes in log odds ∆log-odds after masking the
top 10% most important pixels according to sev-
eral feature importance estimation methods across
N = 100 MNIST test images (higher is better).
*** = signiﬁcantly different (p < 0.001  MWW).

Figure 3: Comparison of the distributions of the
changes in log odds ∆log-odds after masking the
top 30% most important pixels according to sev-
eral feature importance estimation methods across
N = 100 test ImageNet images (higher is better).
** = signiﬁcantly different (p < 0.01  MWW).

2

2

  ci 1− α

2

2

2

− ci  α

2

] with lower bounds ci  α

and upper
would like to calculate conﬁdence intervals CIi γ = [ci  α
at conﬁdence level γ = 1 − α for each assigned feature importance estimate ˆai. The
bounds ci 1− α
width ui = ci 1− α
of CIi γ can subsequently be used to quantify the uncertainty of ˆai. To
derive uncertainty estimates for causal explanation models  we propose the use of bootstrap ensemble
methods  speciﬁcally using bootstrap resampling [45  46]. To train bootstrap ensembles of causal
explanation models  we ﬁrst draw N training samples X at random with repeats from the original
training set. We then train an explanation model using the before-mentioned causal objective until
convergence on the selected subset of the training set. We repeat this process M times to obtain a
bootstrap ensemble of M explanation models (Algorithm in Appendix B). We use the median of the
attributions ˆai of the ensemble members as the assigned importance of the bootstrap ensemble  and
the α
2 quantiles as lower and upper bounds of its CI  respectively. The efﬁcacy of bootstrap
ensembles for estimating the uncertainty in outputs of neural networks has been demonstrated in  e.g. 
[47]  but this work is  to the best of our knowledge  the ﬁrst to consider using bootstrap ensembles of
explanation models to quantify the uncertainty in assigned importance scores. We note that Monte
Carlo dropout [48]  which uses dropout [49] at evaluation time  is an alternative method for estimating
uncertainty for the outputs of neural networks that does not require explicitly training an ensemble of
models  but may not always produce uncertainty estimates of the same quality as ensembles [47].

2 and 1− α

4 Experiments

Our experiments aimed to answer the following questions:
1 How does the feature importance estimation performance of CXPlain compare to that of existing

state-of-the-art methods?

2 How does the computational performance of CXPlain compare to existing model-agnostic and

model-speciﬁc methods for feature importance estimation?

3 Are uncertainty estimates computed via bootstrap resampling of CXPlain models qualitatively and

quantitatively correlated with their ability to accurately determine feature importance?

To answer these questions  we performed extensive experiments on several benchmarks that compare
both the computational as well as the estimation performance of CXPlain to existing state-of-the-art
methods for feature importance estimation. To enable a meaningful comparison  we focus most of
our experiments on image classiﬁcation tasks  where we are best able to visualise and quantify the
performance of feature importance estimation methods  and on neural network models as models to
be explained  since most existing model-speciﬁc attribution methods that we wish to compare to were
developed exclusively for neural networks. However  we note that CXPlain as a method is compatible
with any machine-learning model  data modality  and both regression as well as classiﬁcation tasks.
We used Mann–Whitney–Wilcoxon (MWW) tests [50] to calculate p-values for the main comparisons.

4.1 Determining Important Features in MNIST and ImageNet

To compare the accuracy of CXPlain to existing state-of-the-art methods for feature importance
estimation  we evaluated its ability to identify important features in MNIST [51] and ImageNet [52]
images. To do so  we followed the experimental design ﬁrst proposed by Shrikumar et al. [1]  and

5

***0102030IntegratedGradientsSimpleGradientSHAPLIMECXPlain(MLP)Deep−SHAPCXPlain(U−net)D log oddsMNIST**−5.0−2.50.02.5RandomIntegratedGradientsSimpleGradientSHAPDeep−SHAPLIMECXPlain(U−net)D log oddsImageNetFigure 4: A comparison of the top 10% most im-
portant pixels (= Mask) as identiﬁed by CXPlain
(U-net)  DeepSHAP  SHAP  and LIME on the
same sample test set image (Source) of the 8 vs. 3
MNIST benchmark. With accurate estimates  the
Masked image should more closely resemble a 3
than an 8  since the pixels that most distinguished
an 8 as an 8 should have been removed.

Figure 5: A comparison of the feature importance
scores (= Attribution) as estimated by CXPlain
(U-net)  SHAP  and LIME on the same sample test
set image (Source) of the Gorilla vs. Zebra Ima-
geNet benchmark. We found that CXPlain (U-net)
produces attribution maps that are  subjectively
and qualitatively  more semantically focused on
the most salient regions of the image.

trained binary classiﬁcation models to distinguish between two digit types (8 vs. 3) on MNIST (model
accuracy: 99.85%)  and two object categories (Gorilla vs. Zebra) on ImageNet (model accuracy:
96.73%). As a preprocessing step  pixel values were scaled to be in the range of [0  1] prior to training.
We then used several importance estimation methods to determine which input pixels were most
important for the classiﬁcation models’ decisions on N = 100 test images. We masked the top 10
and 30% of those most important pixels for MNIST and ImageNet  respectively  and measured the
resulting change in the classiﬁcation models’ conﬁdences by computing the difference in log odds

∆log-odds = log-odds(poriginal) − log-odds(pmasked)

(7)
1−p )  and poriginal and pmasked are the classiﬁcation models’ outputs p ∈
where log-odds(p) = log( p
[0  1] for the original image and the masked image with the top pixels removed  respectively. To
ensure that the explanations ei of all methods are on the same scale  we normalised them to the range
of [0  1] using the transformation ˆai = |ei|/ΣN
i=0|ei|. We plotted the assigned importances and the
resulting masked images to qualitatively assess each methods’ ability to determine the salient features
in the original image (Figures 4 and 5). We additionally recorded the mean and standard deviation of
the time taken (in seconds) to compute the feature importance estimates for each method on the same
hardware (Appendix C) over 10 and 5 runs with the same parameters and random seed for MNIST
and ImageNet  respectively (Figures 6 and 7). Further training details are given in Appendix A.

4.2 Quantifying Uncertainty in Estimates of Feature Importance

To quantitatively and qualitatively assess the accuracy of the uncertainty estimates provided by
bootstrap ensembles of CXPlain models  we analysed whether their uncertainty estimates ui are
correlated with their errors in feature importance estimation on held-out MNIST test samples. We
evaluated several numbers M of bootstrap resampled models in order to determine how the number of
ensemble members affects the uncertainty estimation performance of bootstrap ensembles of CXPlain
models. In addition  we also evaluated the performance of randomly selected uncertainty estimates
as a baseline for comparison. In general settings  it is difﬁcult to evaluate uncertainty estimates
for feature importance estimation methods  since we typically do not have per-feature ground-truth
attributions to evaluate against. However  by comparing the ranking implied by the ground-truth
change in log-odds to the ranking implied by the explanation model we are able to deﬁne a rank error
REi for each xi. Formally  the rank error REi = |rank∆log-odds(i) − rank ˆfexp
(i)| is the difference in
rank between the true rank∆log-odds implied by ∆log-odds  and the estimated rank ˆfexp
implied by the
explanation model  where rankb(i) deﬁnes the rank of xi from 0 to p − 1 implied by b.
As correlation metric  we used Pearson’s ρ to measure the correlation between the rank error REi and
the uncertainty estimates ui = ci 95% − ci 5% deﬁned by the bootstrap resampled γ = 90% CIs for
each importance estimate ˆai in the top 2.5% of pixels by ∆log-odds across N = 100 unseen images
from the MNIST test set. We limited the evaluation to all pixels with a ∆log-odds greater than 0.

6

SourceMaskedMaskssh://d909b@ssh.schwabpatrick.com:909/usr/bin/python -u /home/d909b/bin/causal_explanations/causal_explanations/apps/main.py --output_directory=/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty --load_existing=/home/d909b/models/cex_main_1/model.npz --load_existing_cxplain=/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz --dataset=mnist --num_epochs=50 --num_units=75 --batch_size=100 --num_layers=2 --dropout=0.0 --learning_rate=0.001 --l2_weight=0.000 --model_type=resnet --explanation_type=cxplain --do_not_save_attributions --source_digit=8 --target_digit=3 --attack_method=random --attack_epsilon=1 --attack_num_samples=100 --defence_method=none --discrete_attack_type=lsga --do_not_calculate_log_odds --do_not_calculate_robustness --num_explanation_samples=1000 --num_boostrap_samples=5Using TensorFlow backend./usr/local/lib/python2.7/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1dINFO: Args are: {'num_boostrap_samples': 5  'do_adversarial_training': False  'n_jobs': 4  'explanation_type': 'cxplain'  'calculate_uncertainties': True  'num_adversarial_samples': 1000  'hyperopt_against_eval_set': False  'num_epochs': 50  'explanation_model_name': 'explanation.npz'  'copy_to_local': False  'attack_iterations': 100  'seed': 909  'precomputed_attributions_train': ''  'num_layers': 2  'fraction_of_data_set': 1  'precomputed_attributions_val': ''  'calculate_log_odds': False  'do_evaluate': False  'with_tensorboard': False  'do_augment': False  'with_bn': False  'attack_num_samples': 100  'do_hyperopt': False  'attack_epsilon': 1  'save_attributions': False  'save_predictions': True  'target_digit': 3  'load_existing': '/home/d909b/models/cex_main_1/model.npz'  'source_digit': 8  'calculate_robustness': False  'discrete_attack_type': 'lsga'  'num_hyperopt_runs': 35  'thermometer_encoding_levels': 16  'learning_rate': 0.001  'batch_size': 100  'output_directory': '/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty'  'load_existing_cxplain': '/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz'  'do_merge_lsf': False  'slic_compactness': 10  'do_hyperopt_on_lsf': False  'l2_weight': 0.0  'hyperopt_o(cid:31)set': 0  'dataset': 'mnist'  'dropout': 0.0  'defence_method': 'none'  'num_gpus': 1  'imagenet_folder': '/home/d909b/backup/imagenettrain-prep/'  'attack_method': 'random'  'num_layers_cxplain': -1  'early_stopping_patience': 12  'do_train': False  'num_explanation_samples': 1000  'num_units': 75  'test_set_fraction': 0.2  'model_type': 'resnet'  'validation_set_fraction': 0.2  'num_units_cxplain': -1  'model_name': 'model.npz'}INFO: Running at 2019-05-10 16:48:02.083797INFO: Seed is 9092019-05-10 16:48:02.092153: I tensor(cid:30)ow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2019-05-10 16:48:02.200813: I tensor(cid:30)ow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1)  but there must be at least one NUMA node  so returning NUMA node zero2019-05-10 16:48:02.201281: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582pciBusID: 0000:01:00.0totalMemory: 11.90GiB freeMemory: 11.55GiB2019-05-10 16:48:02.201296: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02019-05-10 16:48:02.396536: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2019-05-10 16:48:02.396563: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:988] 0 2019-05-10 16:48:02.396569: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2019-05-10 16:48:02.396735: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11180 MB memory) -> physical GPU (device: 0  name: TITAN Xp  pci bus id: 0000:01:00.0  compute capability: 6.1)INFO: Loading MNIST data.INFO: Run with args: {'num_boostrap_samples': 5  'do_adversarial_training': False  'n_jobs': 4  'explanation_type': 'cxplain'  'calculate_uncertainties': True  'num_adversarial_samples': 1000  'hyperopt_against_eval_set': False  'num_epochs': 50  'explanation_model_name': 'explanation.npz'  'copy_to_local': False  'attack_iterations': 100  'seed': 909  'precomputed_attributions_train': ''  'num_layers': 2  'fraction_of_data_set': 1  'precomputed_attributions_val': ''  'calculate_log_odds': False  'do_evaluate': False  'with_tensorboard': False  'do_augment': False  'with_bn': False  'attack_num_samples': 100  'do_hyperopt': False  'attack_epsilon': 1  'save_attributions': False  'save_predictions': True  'target_digit': 3  'load_existing': '/home/d909b/models/cex_main_1/model.npz'  'source_digit': 8  'calculate_robustness': False  'discrete_attack_type': 'lsga'  'num_hyperopt_runs': 35  'thermometer_encoding_levels': 16  'learning_rate': 0.001  'batch_size': 100  'output_directory': '/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty'  'load_existing_cxplain': '/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz'  'do_merge_lsf': False  'slic_compactness': 10  'do_hyperopt_on_lsf': False  'l2_weight': 0.0  'hyperopt_o(cid:31)set': 0  'dataset': 'mnist'  'dropout': 0.0  'defence_method': 'none'  'num_gpus': 1  'imagenet_folder': '/home/d909b/backup/imagenettrain-prep/'  'attack_method': 'random'  'num_layers_cxplain': -1  'early_stopping_patience': 12  'do_train': False  'num_explanation_samples': 1000  'num_units': 75  'test_set_fraction': 0.2  'model_type': 'resnet'  'validation_set_fraction': 0.2  'num_units_cxplain': -1  'model_name': 'model.npz'}INFO: Loaded generator with 11982 samples. Doing 120 steps of size 100INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Built generators with 120 training samples  20 validation samples and 20 test samples.INFO: Started training feature extraction.__________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to ==================================================================================================input_1 (InputLayer) (None  28  28  1) 0 __________________________________________________________________________________________________conv2d_1 (Conv2D) (None  28  28  16) 160 input_1[0][0] __________________________________________________________________________________________________activation_1 (Activation) (None  28  28  16) 0 conv2d_1[0][0] __________________________________________________________________________________________________conv2d_2 (Conv2D) (None  28  28  16) 2320 activation_1[0][0] __________________________________________________________________________________________________activation_2 (Activation) (None  28  28  16) 0 conv2d_2[0][0] __________________________________________________________________________________________________conv2d_3 (Conv2D) (None  28  28  16) 2320 activation_2[0][0] __________________________________________________________________________________________________activation_3 (Activation) (None  28  28  16) 0 conv2d_3[0][0] __________________________________________________________________________________________________add_1 (Add) (None  28  28  16) 0 activation_1[0][0] activation_3[0][0] __________________________________________________________________________________________________activation_4 (Activation) (None  28  28  16) 0 add_1[0][0] __________________________________________________________________________________________________conv2d_4 (Conv2D) (None  28  28  16) 2320 activation_4[0][0] __________________________________________________________________________________________________activation_5 (Activation) (None  28  28  16) 0 conv2d_4[0][0] __________________________________________________________________________________________________conv2d_5 (Conv2D) (None  28  28  16) 2320 activation_5[0][0] __________________________________________________________________________________________________activation_6 (Activation) (None  28  28  16) 0 conv2d_5[0][0] __________________________________________________________________________________________________add_2 (Add) (None  28  28  16) 0 activation_4[0][0] activation_6[0][0] __________________________________________________________________________________________________activation_7 (Activation) (None  28  28  16) 0 add_2[0][0] __________________________________________________________________________________________________conv2d_6 (Conv2D) (None  28  28  16) 2320 activation_7[0][0] __________________________________________________________________________________________________activation_8 (Activation) (None  28  28  16) 0 conv2d_6[0][0] __________________________________________________________________________________________________conv2d_7 (Conv2D) (None  28  28  16) 2320 activation_8[0][0] __________________________________________________________________________________________________activation_9 (Activation) (None  28  28  16) 0 conv2d_7[0][0] __________________________________________________________________________________________________add_3 (Add) (None  28  28  16) 0 activation_7[0][0] activation_9[0][0] __________________________________________________________________________________________________activation_10 (Activation) (None  28  28  16) 0 add_3[0][0] __________________________________________________________________________________________________conv2d_8 (Conv2D) (None  14  14  32) 4640 activation_10[0][0] __________________________________________________________________________________________________activation_11 (Activation) (None  14  14  32) 0 conv2d_8[0][0] __________________________________________________________________________________________________conv2d_10 (Conv2D) (None  14  14  32) 544 activation_10[0][0] __________________________________________________________________________________________________conv2d_9 (Conv2D) (None  14  14  32) 9248 activation_11[0][0] __________________________________________________________________________________________________activation_13 (Activation) (None  14  14  32) 0 conv2d_10[0][0] __________________________________________________________________________________________________activation_12 (Activation) (None  14  14  32) 0 conv2d_9[0][0] __________________________________________________________________________________________________add_4 (Add) (None  14  14  32) 0 activation_13[0][0] activation_12[0][0] __________________________________________________________________________________________________activation_14 (Activation) (None  14  14  32) 0 add_4[0][0] __________________________________________________________________________________________________conv2d_11 (Conv2D) (None  14  14  32) 9248 activation_14[0][0] __________________________________________________________________________________________________activation_15 (Activation) (None  14  14  32) 0 conv2d_11[0][0] __________________________________________________________________________________________________conv2d_12 (Conv2D) (None  14  14  32) 9248 activation_15[0][0] __________________________________________________________________________________________________activation_16 (Activation) (None  14  14  32) 0 conv2d_12[0][0] __________________________________________________________________________________________________add_5 (Add) (None  14  14  32) 0 activation_14[0][0] activation_16[0][0] __________________________________________________________________________________________________activation_17 (Activation) (None  14  14  32) 0 add_5[0][0] __________________________________________________________________________________________________conv2d_13 (Conv2D) (None  14  14  32) 9248 activation_17[0][0] __________________________________________________________________________________________________activation_18 (Activation) (None  14  14  32) 0 conv2d_13[0][0] __________________________________________________________________________________________________conv2d_14 (Conv2D) (None  14  14  32) 9248 activation_18[0][0] __________________________________________________________________________________________________activation_19 (Activation) (None  14  14  32) 0 conv2d_14[0][0] __________________________________________________________________________________________________add_6 (Add) (None  14  14  32) 0 activation_17[0][0] activation_19[0][0] __________________________________________________________________________________________________activation_20 (Activation) (None  14  14  32) 0 add_6[0][0] __________________________________________________________________________________________________conv2d_15 (Conv2D) (None  7  7  64) 18496 activation_20[0][0] __________________________________________________________________________________________________activation_21 (Activation) (None  7  7  64) 0 conv2d_15[0][0] __________________________________________________________________________________________________conv2d_17 (Conv2D) (None  7  7  64) 2112 activation_20[0][0] __________________________________________________________________________________________________conv2d_16 (Conv2D) (None  7  7  64) 36928 activation_21[0][0] __________________________________________________________________________________________________activation_23 (Activation) (None  7  7  64) 0 conv2d_17[0][0] __________________________________________________________________________________________________activation_22 (Activation) (None  7  7  64) 0 conv2d_16[0][0] __________________________________________________________________________________________________add_7 (Add) (None  7  7  64) 0 activation_23[0][0] activation_22[0][0] __________________________________________________________________________________________________activation_24 (Activation) (None  7  7  64) 0 add_7[0][0] __________________________________________________________________________________________________conv2d_18 (Conv2D) (None  7  7  64) 36928 activation_24[0][0] __________________________________________________________________________________________________activation_25 (Activation) (None  7  7  64) 0 conv2d_18[0][0] __________________________________________________________________________________________________conv2d_19 (Conv2D) (None  7  7  64) 36928 activation_25[0][0] __________________________________________________________________________________________________activation_26 (Activation) (None  7  7  64) 0 conv2d_19[0][0] __________________________________________________________________________________________________add_8 (Add) (None  7  7  64) 0 activation_24[0][0] activation_26[0][0] __________________________________________________________________________________________________activation_27 (Activation) (None  7  7  64) 0 add_8[0][0] __________________________________________________________________________________________________conv2d_20 (Conv2D) (None  7  7  64) 36928 activation_27[0][0] __________________________________________________________________________________________________activation_28 (Activation) (None  7  7  64) 0 conv2d_20[0][0] __________________________________________________________________________________________________conv2d_21 (Conv2D) (None  7  7  64) 36928 activation_28[0][0] __________________________________________________________________________________________________activation_29 (Activation) (None  7  7  64) 0 conv2d_21[0][0] __________________________________________________________________________________________________add_9 (Add) (None  7  7  64) 0 activation_27[0][0] activation_29[0][0] __________________________________________________________________________________________________activation_30 (Activation) (None  7  7  64) 0 add_9[0][0] __________________________________________________________________________________________________average_pooling2d_1 (AveragePoo (None  1  1  64) 0 activation_30[0][0] __________________________________________________________________________________________________(cid:30)atten_1 (Flatten) (None  64) 0 average_pooling2d_1[0][0] __________________________________________________________________________________________________dense_1 (Dense) (None  2) 130 (cid:30)atten_1[0][0] __________________________________________________________________________________________________activation_31 (Activation) (None  2) 0 dense_1[0][0] ==================================================================================================Total params: 270 882Trainable params: 270 882Non-trainable params: 0__________________________________________________________________________________________________INFO: Loading existing model from /home/d909b/models/cex_main_1/model.npz_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_3 (InputLayer) (None  28  28  1) 0 _________________________________________________________________(cid:30)atten_3 (Flatten) (None  784) 0 _________________________________________________________________dense_3 (Dense) (None  75) 58875 _________________________________________________________________activation_63 (Activation) (None  75) 0 _________________________________________________________________dense_4 (Dense) (None  75) 5700 _________________________________________________________________activation_64 (Activation) (None  75) 0 _________________________________________________________________dense_5 (Dense) (None  196) 14896 _________________________________________________________________activation_65 (Activation) (None  196) 0 _________________________________________________________________reshape_1 (Reshape) (None  14  14  1) 0 _________________________________________________________________lambda_2 (Lambda) (None  28  28  1) 0 _________________________________________________________________reshape_2 (Reshape) (None  784) 0 =================================================================Total params: 79 471Trainable params: 79 471Non-trainable params: 0_________________________________________________________________INFO: Loading existing CXPlain model from /home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npzbuild_explanation_model : took 1.81394100189 seconds.INFO: Saving loss history to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/losses.pickleINFO: Saving model predictions.INFO: Loaded generator with 11982 samples. Doing 120 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/train_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/val_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/test_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Performance on test AUROC (weighted) = 0.999994917356212   with AUPRC (weighted) = 0.9999949280930339   with r^2 (weighted) = 0.9957963109416453   with f1 (weighted) = 0.9984879166751001   with accuracy = 0.9984879032258065   with error = 0.0015120967741935054INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Performance on test [n= 100 ] targets = [0.9] observed = [(1.1436718599626305  '+-'  0.5520538782615332)]INFO: Performance on test [n= 100 ] for target 0.9observed = [1.3892620290955573  1.1377237947530967  0.8350603218815686  0.8713871427716444  1.8772789577296631  1.0421385269639882  1.2734869085839489  0.5600630255113783  1.2435160902712874  2.0858731684436953  0.6621914099533671  0.6070763678776874  0.8661585671984602  1.0517880297070337  2.9146049149183884  0.6235869496235044  0.9226209148366383  0.896227432532063  0.2670657996896821  0.013821244358016503  2.189807259704378  0.8175433055816894  1.19488319718703  1.8930241978639137  0.7730931068420339  1.27811588137019  0.9850174716812721  1.1216892414631758  0.573120152159161  1.0232491128492835  0.8987605463776784  1.037649214136879  0.5118367176339356  2.083070000264211  1.302903762457773  0.8204965828874214  1.6789029079041142  1.623794531947206  1.2545555949371687  1.0012906768156877  1.2462736163330106  0.9057037790232868  0.9469323067092333  0.4383414096479612  0.8104805348408998  0.8851755920050628  1.132267732268079  1.6621074239888216  0.934013287760548  0.8349687929106586  0.5289153327173624  1.4440371769826026  2.138591203347149  1.2270851493051227  2.142648193073443  0.3385335850217482  0.46770572775341435  0.8527171407093351  0.47953222713471033  1.8026274831409246  1.1812909273907717  0.9314364881634934  0.2302455728277166  1.5633736254880761  2.3031484613717335  2.4174470115754003  1.1201235152098241  2.266265571565306  1.3994960435747492  0.7860688071695486  1.2776682986304249  1.6580154383934487  0.757406483017344  1.0043128902083016  0.7231292579067669  0.740787741347223  0.7141079559712596  0.6734593179701929  0.6616112633542277  0.9468177183380312  0.7690791699258368  1.0155653037702255  0.6512117579059203  0.9884971489035145  0.6675223757172929  1.7234492486644164  0.7886172116728509  1.1807222822302033  0.8768496962883034  1.2127882377541048  1.0925325778269959  0.5464445929844274  2.096737223622148  0.8172319369500524  1.8495334324613373  1.2724899666210199  1.0701363404910589  1.8738202234307906  2.195503699637992  1.8998484264924382]Process (cid:29)nished with exit code 08 → 3DeepSHAPSHAPCXPlain (U-net)LIMESourceMaskedAttributionssh://d909b@ssh.schwabpatrick.com:909/usr/bin/python -u /home/d909b/bin/causal_explanations/causal_explanations/apps/main.py --output_directory=/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty --load_existing=/home/d909b/models/cex_main_1/model.npz --load_existing_cxplain=/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz --dataset=mnist --num_epochs=50 --num_units=75 --batch_size=100 --num_layers=2 --dropout=0.0 --learning_rate=0.001 --l2_weight=0.000 --model_type=resnet --explanation_type=cxplain --do_not_save_attributions --source_digit=8 --target_digit=3 --attack_method=random --attack_epsilon=1 --attack_num_samples=100 --defence_method=none --discrete_attack_type=lsga --do_not_calculate_log_odds --do_not_calculate_robustness --num_explanation_samples=1000 --num_boostrap_samples=5Using TensorFlow backend./usr/local/lib/python2.7/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1dINFO: Args are: {'num_boostrap_samples': 5  'do_adversarial_training': False  'n_jobs': 4  'explanation_type': 'cxplain'  'calculate_uncertainties': True  'num_adversarial_samples': 1000  'hyperopt_against_eval_set': False  'num_epochs': 50  'explanation_model_name': 'explanation.npz'  'copy_to_local': False  'attack_iterations': 100  'seed': 909  'precomputed_attributions_train': ''  'num_layers': 2  'fraction_of_data_set': 1  'precomputed_attributions_val': ''  'calculate_log_odds': False  'do_evaluate': False  'with_tensorboard': False  'do_augment': False  'with_bn': False  'attack_num_samples': 100  'do_hyperopt': False  'attack_epsilon': 1  'save_attributions': False  'save_predictions': True  'target_digit': 3  'load_existing': '/home/d909b/models/cex_main_1/model.npz'  'source_digit': 8  'calculate_robustness': False  'discrete_attack_type': 'lsga'  'num_hyperopt_runs': 35  'thermometer_encoding_levels': 16  'learning_rate': 0.001  'batch_size': 100  'output_directory': '/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty'  'load_existing_cxplain': '/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz'  'do_merge_lsf': False  'slic_compactness': 10  'do_hyperopt_on_lsf': False  'l2_weight': 0.0  'hyperopt_o(cid:31)set': 0  'dataset': 'mnist'  'dropout': 0.0  'defence_method': 'none'  'num_gpus': 1  'imagenet_folder': '/home/d909b/backup/imagenettrain-prep/'  'attack_method': 'random'  'num_layers_cxplain': -1  'early_stopping_patience': 12  'do_train': False  'num_explanation_samples': 1000  'num_units': 75  'test_set_fraction': 0.2  'model_type': 'resnet'  'validation_set_fraction': 0.2  'num_units_cxplain': -1  'model_name': 'model.npz'}INFO: Running at 2019-05-10 16:48:02.083797INFO: Seed is 9092019-05-10 16:48:02.092153: I tensor(cid:30)ow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2019-05-10 16:48:02.200813: I tensor(cid:30)ow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1)  but there must be at least one NUMA node  so returning NUMA node zero2019-05-10 16:48:02.201281: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582pciBusID: 0000:01:00.0totalMemory: 11.90GiB freeMemory: 11.55GiB2019-05-10 16:48:02.201296: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02019-05-10 16:48:02.396536: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2019-05-10 16:48:02.396563: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:988] 0 2019-05-10 16:48:02.396569: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2019-05-10 16:48:02.396735: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11180 MB memory) -> physical GPU (device: 0  name: TITAN Xp  pci bus id: 0000:01:00.0  compute capability: 6.1)INFO: Loading MNIST data.INFO: Run with args: {'num_boostrap_samples': 5  'do_adversarial_training': False  'n_jobs': 4  'explanation_type': 'cxplain'  'calculate_uncertainties': True  'num_adversarial_samples': 1000  'hyperopt_against_eval_set': False  'num_epochs': 50  'explanation_model_name': 'explanation.npz'  'copy_to_local': False  'attack_iterations': 100  'seed': 909  'precomputed_attributions_train': ''  'num_layers': 2  'fraction_of_data_set': 1  'precomputed_attributions_val': ''  'calculate_log_odds': False  'do_evaluate': False  'with_tensorboard': False  'do_augment': False  'with_bn': False  'attack_num_samples': 100  'do_hyperopt': False  'attack_epsilon': 1  'save_attributions': False  'save_predictions': True  'target_digit': 3  'load_existing': '/home/d909b/models/cex_main_1/model.npz'  'source_digit': 8  'calculate_robustness': False  'discrete_attack_type': 'lsga'  'num_hyperopt_runs': 35  'thermometer_encoding_levels': 16  'learning_rate': 0.001  'batch_size': 100  'output_directory': '/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty'  'load_existing_cxplain': '/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz'  'do_merge_lsf': False  'slic_compactness': 10  'do_hyperopt_on_lsf': False  'l2_weight': 0.0  'hyperopt_o(cid:31)set': 0  'dataset': 'mnist'  'dropout': 0.0  'defence_method': 'none'  'num_gpus': 1  'imagenet_folder': '/home/d909b/backup/imagenettrain-prep/'  'attack_method': 'random'  'num_layers_cxplain': -1  'early_stopping_patience': 12  'do_train': False  'num_explanation_samples': 1000  'num_units': 75  'test_set_fraction': 0.2  'model_type': 'resnet'  'validation_set_fraction': 0.2  'num_units_cxplain': -1  'model_name': 'model.npz'}INFO: Loaded generator with 11982 samples. Doing 120 steps of size 100INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Built generators with 120 training samples  20 validation samples and 20 test samples.INFO: Started training feature extraction.__________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to ==================================================================================================input_1 (InputLayer) (None  28  28  1) 0 __________________________________________________________________________________________________conv2d_1 (Conv2D) (None  28  28  16) 160 input_1[0][0] __________________________________________________________________________________________________activation_1 (Activation) (None  28  28  16) 0 conv2d_1[0][0] __________________________________________________________________________________________________conv2d_2 (Conv2D) (None  28  28  16) 2320 activation_1[0][0] __________________________________________________________________________________________________activation_2 (Activation) (None  28  28  16) 0 conv2d_2[0][0] __________________________________________________________________________________________________conv2d_3 (Conv2D) (None  28  28  16) 2320 activation_2[0][0] __________________________________________________________________________________________________activation_3 (Activation) (None  28  28  16) 0 conv2d_3[0][0] __________________________________________________________________________________________________add_1 (Add) (None  28  28  16) 0 activation_1[0][0] activation_3[0][0] __________________________________________________________________________________________________activation_4 (Activation) (None  28  28  16) 0 add_1[0][0] __________________________________________________________________________________________________conv2d_4 (Conv2D) (None  28  28  16) 2320 activation_4[0][0] __________________________________________________________________________________________________activation_5 (Activation) (None  28  28  16) 0 conv2d_4[0][0] __________________________________________________________________________________________________conv2d_5 (Conv2D) (None  28  28  16) 2320 activation_5[0][0] __________________________________________________________________________________________________activation_6 (Activation) (None  28  28  16) 0 conv2d_5[0][0] __________________________________________________________________________________________________add_2 (Add) (None  28  28  16) 0 activation_4[0][0] activation_6[0][0] __________________________________________________________________________________________________activation_7 (Activation) (None  28  28  16) 0 add_2[0][0] __________________________________________________________________________________________________conv2d_6 (Conv2D) (None  28  28  16) 2320 activation_7[0][0] __________________________________________________________________________________________________activation_8 (Activation) (None  28  28  16) 0 conv2d_6[0][0] __________________________________________________________________________________________________conv2d_7 (Conv2D) (None  28  28  16) 2320 activation_8[0][0] __________________________________________________________________________________________________activation_9 (Activation) (None  28  28  16) 0 conv2d_7[0][0] __________________________________________________________________________________________________add_3 (Add) (None  28  28  16) 0 activation_7[0][0] activation_9[0][0] __________________________________________________________________________________________________activation_10 (Activation) (None  28  28  16) 0 add_3[0][0] __________________________________________________________________________________________________conv2d_8 (Conv2D) (None  14  14  32) 4640 activation_10[0][0] __________________________________________________________________________________________________activation_11 (Activation) (None  14  14  32) 0 conv2d_8[0][0] __________________________________________________________________________________________________conv2d_10 (Conv2D) (None  14  14  32) 544 activation_10[0][0] __________________________________________________________________________________________________conv2d_9 (Conv2D) (None  14  14  32) 9248 activation_11[0][0] __________________________________________________________________________________________________activation_13 (Activation) (None  14  14  32) 0 conv2d_10[0][0] __________________________________________________________________________________________________activation_12 (Activation) (None  14  14  32) 0 conv2d_9[0][0] __________________________________________________________________________________________________add_4 (Add) (None  14  14  32) 0 activation_13[0][0] activation_12[0][0] __________________________________________________________________________________________________activation_14 (Activation) (None  14  14  32) 0 add_4[0][0] __________________________________________________________________________________________________conv2d_11 (Conv2D) (None  14  14  32) 9248 activation_14[0][0] __________________________________________________________________________________________________activation_15 (Activation) (None  14  14  32) 0 conv2d_11[0][0] __________________________________________________________________________________________________conv2d_12 (Conv2D) (None  14  14  32) 9248 activation_15[0][0] __________________________________________________________________________________________________activation_16 (Activation) (None  14  14  32) 0 conv2d_12[0][0] __________________________________________________________________________________________________add_5 (Add) (None  14  14  32) 0 activation_14[0][0] activation_16[0][0] __________________________________________________________________________________________________activation_17 (Activation) (None  14  14  32) 0 add_5[0][0] __________________________________________________________________________________________________conv2d_13 (Conv2D) (None  14  14  32) 9248 activation_17[0][0] __________________________________________________________________________________________________activation_18 (Activation) (None  14  14  32) 0 conv2d_13[0][0] __________________________________________________________________________________________________conv2d_14 (Conv2D) (None  14  14  32) 9248 activation_18[0][0] __________________________________________________________________________________________________activation_19 (Activation) (None  14  14  32) 0 conv2d_14[0][0] __________________________________________________________________________________________________add_6 (Add) (None  14  14  32) 0 activation_17[0][0] activation_19[0][0] __________________________________________________________________________________________________activation_20 (Activation) (None  14  14  32) 0 add_6[0][0] __________________________________________________________________________________________________conv2d_15 (Conv2D) (None  7  7  64) 18496 activation_20[0][0] __________________________________________________________________________________________________activation_21 (Activation) (None  7  7  64) 0 conv2d_15[0][0] __________________________________________________________________________________________________conv2d_17 (Conv2D) (None  7  7  64) 2112 activation_20[0][0] __________________________________________________________________________________________________conv2d_16 (Conv2D) (None  7  7  64) 36928 activation_21[0][0] __________________________________________________________________________________________________activation_23 (Activation) (None  7  7  64) 0 conv2d_17[0][0] __________________________________________________________________________________________________activation_22 (Activation) (None  7  7  64) 0 conv2d_16[0][0] __________________________________________________________________________________________________add_7 (Add) (None  7  7  64) 0 activation_23[0][0] activation_22[0][0] __________________________________________________________________________________________________activation_24 (Activation) (None  7  7  64) 0 add_7[0][0] __________________________________________________________________________________________________conv2d_18 (Conv2D) (None  7  7  64) 36928 activation_24[0][0] __________________________________________________________________________________________________activation_25 (Activation) (None  7  7  64) 0 conv2d_18[0][0] __________________________________________________________________________________________________conv2d_19 (Conv2D) (None  7  7  64) 36928 activation_25[0][0] __________________________________________________________________________________________________activation_26 (Activation) (None  7  7  64) 0 conv2d_19[0][0] __________________________________________________________________________________________________add_8 (Add) (None  7  7  64) 0 activation_24[0][0] activation_26[0][0] __________________________________________________________________________________________________activation_27 (Activation) (None  7  7  64) 0 add_8[0][0] __________________________________________________________________________________________________conv2d_20 (Conv2D) (None  7  7  64) 36928 activation_27[0][0] __________________________________________________________________________________________________activation_28 (Activation) (None  7  7  64) 0 conv2d_20[0][0] __________________________________________________________________________________________________conv2d_21 (Conv2D) (None  7  7  64) 36928 activation_28[0][0] __________________________________________________________________________________________________activation_29 (Activation) (None  7  7  64) 0 conv2d_21[0][0] __________________________________________________________________________________________________add_9 (Add) (None  7  7  64) 0 activation_27[0][0] activation_29[0][0] __________________________________________________________________________________________________activation_30 (Activation) (None  7  7  64) 0 add_9[0][0] __________________________________________________________________________________________________average_pooling2d_1 (AveragePoo (None  1  1  64) 0 activation_30[0][0] __________________________________________________________________________________________________(cid:30)atten_1 (Flatten) (None  64) 0 average_pooling2d_1[0][0] __________________________________________________________________________________________________dense_1 (Dense) (None  2) 130 (cid:30)atten_1[0][0] __________________________________________________________________________________________________activation_31 (Activation) (None  2) 0 dense_1[0][0] ==================================================================================================Total params: 270 882Trainable params: 270 882Non-trainable params: 0__________________________________________________________________________________________________INFO: Loading existing model from /home/d909b/models/cex_main_1/model.npz_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_3 (InputLayer) (None  28  28  1) 0 _________________________________________________________________(cid:30)atten_3 (Flatten) (None  784) 0 _________________________________________________________________dense_3 (Dense) (None  75) 58875 _________________________________________________________________activation_63 (Activation) (None  75) 0 _________________________________________________________________dense_4 (Dense) (None  75) 5700 _________________________________________________________________activation_64 (Activation) (None  75) 0 _________________________________________________________________dense_5 (Dense) (None  196) 14896 _________________________________________________________________activation_65 (Activation) (None  196) 0 _________________________________________________________________reshape_1 (Reshape) (None  14  14  1) 0 _________________________________________________________________lambda_2 (Lambda) (None  28  28  1) 0 _________________________________________________________________reshape_2 (Reshape) (None  784) 0 =================================================================Total params: 79 471Trainable params: 79 471Non-trainable params: 0_________________________________________________________________INFO: Loading existing CXPlain model from /home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npzbuild_explanation_model : took 1.81394100189 seconds.INFO: Saving loss history to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/losses.pickleINFO: Saving model predictions.INFO: Loaded generator with 11982 samples. Doing 120 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/train_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/val_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/test_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Performance on test AUROC (weighted) = 0.999994917356212   with AUPRC (weighted) = 0.9999949280930339   with r^2 (weighted) = 0.9957963109416453   with f1 (weighted) = 0.9984879166751001   with accuracy = 0.9984879032258065   with error = 0.0015120967741935054INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Performance on test [n= 100 ] targets = [0.9] observed = [(1.1436718599626305  '+-'  0.5520538782615332)]INFO: Performance on test [n= 100 ] for target 0.9observed = [1.3892620290955573  1.1377237947530967  0.8350603218815686  0.8713871427716444  1.8772789577296631  1.0421385269639882  1.2734869085839489  0.5600630255113783  1.2435160902712874  2.0858731684436953  0.6621914099533671  0.6070763678776874  0.8661585671984602  1.0517880297070337  2.9146049149183884  0.6235869496235044  0.9226209148366383  0.896227432532063  0.2670657996896821  0.013821244358016503  2.189807259704378  0.8175433055816894  1.19488319718703  1.8930241978639137  0.7730931068420339  1.27811588137019  0.9850174716812721  1.1216892414631758  0.573120152159161  1.0232491128492835  0.8987605463776784  1.037649214136879  0.5118367176339356  2.083070000264211  1.302903762457773  0.8204965828874214  1.6789029079041142  1.623794531947206  1.2545555949371687  1.0012906768156877  1.2462736163330106  0.9057037790232868  0.9469323067092333  0.4383414096479612  0.8104805348408998  0.8851755920050628  1.132267732268079  1.6621074239888216  0.934013287760548  0.8349687929106586  0.5289153327173624  1.4440371769826026  2.138591203347149  1.2270851493051227  2.142648193073443  0.3385335850217482  0.46770572775341435  0.8527171407093351  0.47953222713471033  1.8026274831409246  1.1812909273907717  0.9314364881634934  0.2302455728277166  1.5633736254880761  2.3031484613717335  2.4174470115754003  1.1201235152098241  2.266265571565306  1.3994960435747492  0.7860688071695486  1.2776682986304249  1.6580154383934487  0.757406483017344  1.0043128902083016  0.7231292579067669  0.740787741347223  0.7141079559712596  0.6734593179701929  0.6616112633542277  0.9468177183380312  0.7690791699258368  1.0155653037702255  0.6512117579059203  0.9884971489035145  0.6675223757172929  1.7234492486644164  0.7886172116728509  1.1807222822302033  0.8768496962883034  1.2127882377541048  1.0925325778269959  0.5464445929844274  2.096737223622148  0.8172319369500524  1.8495334324613373  1.2724899666210199  1.0701363404910589  1.8738202234307906  2.195503699637992  1.8998484264924382]Process (cid:29)nished with exit code 0Gorilla or Zebra?SHAPCXPlain (U-net)LIMEFigure 6: A comparison of the compute time (in
log2 seconds) needed to produce feature impor-
tance estimates using several state-of-the-art fea-
ture importance estimation methods on the same
hardware for the same N = 100 sample test im-
ages from the MNIST benchmark (lower is better).
*** = signiﬁcantly different (p < 0.001  MWW)

Figure 7: A comparison of the compute time (in
log2 seconds) needed to produce feature impor-
tance estimates using state-of-the-art feature im-
portance estimation methods on the same hard-
ware for the same N = 100 sample test images
from the ImageNet benchmark (lower is better).
*** = signiﬁcantly different (p < 0.001  MWW)

If our uncertainty estimates are well calibrated  we would expect to see a high correlation between
the uncertainty estimates ui and the magnitude of rank errors REi  since that would indicate that the
uncertainty estimates ui accurately quantify how certain the feature importance estimates ˆai are on
previously unseen sample images. For the comparison of the resulting distributions of correlation
scores  we applied the Fisher z-transform to the correlation scores in order to correct for the skew
in the distribution of the sample correlation [53]. Figure 9 depicts visualisations of the calculated
ground-truth log odds  the rank errors of the explanation model’s importance estimates  and the
uncertainty for each importance estimate for three test set images. We used the same hyperparameters
as in the previous experiment to train the ensembled CXPlain (MLP) models (Appendix A).

5 Results and Discussion

Predictive Performance. We found that  on the MNIST benchmark  CXPlain (U-net) was compet-
itive with the best competing state-of-the-art feature importance estimation method  DeepSHAP. We
also found that CXPlain (U-net) produced signiﬁcantly (p < 0.001  MWW) more accurate feature
importance estimates than CXPlain (MLP) - indicating that model architectures speciﬁcally tailored
for the image domain are more effective than MLPs in neural explanation models (Figure 2). On the
ImageNet benchmark  CXPlain signiﬁcantly (p < 0.01  MWW) outperformed the best competing
feature importance estimation method  LIME (Figure 3). We also found that the model-speciﬁc
attribution methods Simple Gradient and Integrated Gradients performed relatively poorly across
both benchmarks  and were consistently outperformed by the model-agnostic attribution methods 
CXPlain  and DeepSHAP. Qualitatively  we found that the estimates of feature importance provided
by CXPlain were more focused on the subjectively more important semantic regions of the sample
images from both MNIST and ImageNet (Figures 4 and 5; more in Appendix D). Other methods  in
contrast  produced more superﬂuous attributions. This behavior is exhibited in Figure 5 where SHAP
and LIME both attribute signiﬁcant importance to the wall behind the gorilla  whereas CXPlain
focused nearly all its attention on the gorilla itself  with the exception of the window frame receiving
some importance outside the top 30% of importances of that sample image. We believe this could be
due to the fact that the causal objective strongly penalises attributions outside regions of interest -
leading to qualitatively more focused estimates of importance.

Computational Performance.
In terms of computational performance  we found that CXPlain
computed feature importance estimates signiﬁcantly faster than the state-of-the-art model-agnostic
attribution methods  LIME and SHAP  on both the MNIST and ImageNet benchmarks (Figures 6
and 7). Gradient-based attribution methods and CXPlain performed similarly. On ImageNet  the gap
between LIME and SHAP and the faster methods was considerably larger than on MNIST  since the
large numbers of model evaluations for LIME and SHAP were slower on higher-dimensional images.

Quality of Uncertainty Estimates. We found that  quantitatively  even relatively small CXPlain
ensembles with just M = 5 bootstrap resampled models produce uncertainty estimates that are
signiﬁcantly (p < 0.001  MWW  compared to Random) correlated with its ability to accurately
estimate feature importances on N = 100 previously unseen test images (Figure 8). We also found

7

***28.90(±0.12)28.83(±0.15)508.82(±14.20)373.69(±8.50)33.81(±0.17)28.55(±0.16)30.43(±0.33)222528CXPlain(MLP)IntegratedGradientsSimpleGradientCXPlain(U−net)Deep−SHAPSHAPLIMEcompute time [log2 (s)]MNIST***35.87(±0.11)35.78(±0.05)8366.36(±58.69)9230.00(±24.42)69.08(±0.11)34.19(±0.10)222528213CXPlain(U−net)IntegratedGradientsSimpleGradientDeep−SHAPLIMESHAPcompute time [log2 (s)]ImageNetFigure 8: A comparison of the distributions of the
z-transformed Pearson’s correlations ρ between
the uncertainty estimates ui produced by various
numbers M of bootstrapped ensembles of CX-
Plain models and the Random baseline  and the
ground-truth rank errors of the top 2.5% most im-
portant pixels across N = 100 unseen test images
from the MNIST benchmark (higher is better).
*** = signiﬁcantly different (p < 0.001  MWW)

Figure 9: Visualisations of the calculated ground-
truth change in log odds ∆log-odds  the Rank
Errors of the explanation model’s feature impor-
tance estimates  and the Estimated Uncertainty
for each feature importance estimate as obtained
via bootstrap resampling (M = 100) for three
unseen sample test set images (Input) from the
MNIST benchmark. Note the visual similarity of
the Rank Error and the Estimated Uncertainty.

that increasing the size M of the bootstrap ensemble further signiﬁcantly (p < 0.001 for M = 5
to M = 100  MWW) increases this correlation  and  thus  the quality of the provided uncertainty
estimates. Qualitatively  there was a high visual similarity between the uncertainty estimates ui
provided by the CXPlain ensembles for each input feature xi and the magnitude of rank errors REi
committed by its importance estimates ˆai (Figure 9). The large differences in importance estimation
accuracy between state-of-the-art feature importance estimation methods shown in the MNIST and
ImageNet benchmarks indicate that many of the importance estimates they provide are not truthful
to the predictive model ˆf to be explained  and that measures of uncertainty are necessary to fully
understand the expected reliability of feature importance estimates.

Limitations. While they are fast at evaluation time  a limitation of CXPlain models is that they
have to be trained to learn to explain a predictive model. However  this one-off compute cost typically
amortises quickly  since CXPlain is signiﬁcantly faster at evaluation time than existing model-agnostic
importance estimation methods. Another important point to note is that the associations identiﬁed
by CXPlain models are only causal in the sense that they quantify the degree to which the input
features xi caused a marginal improvement in the predictive performance of the predictive model
ˆf. Associations reported by CXPlain  in particular  do not in any way indicate that there is a causal
relationship between the explained model’s input and output variables in the real world.

6 Conclusion

We presented CXPlain  a new method for learning to estimate feature importance for any machine-
learning model. CXPlain is based on the idea of training a separate explanation model to learn
to estimate which features are important for a given output of a target predictive model using a
causal objective. This approach has several advantages over existing ones: It is compatible with
any machine-learning model  can produce estimates of feature importance quickly after training 
and may be combined with bootstrap resampling to obtain uncertainty estimates for the provided
feature importance scores. We showed experimentally that CXPlain is signiﬁcantly more accurate in
estimating feature importance than existing model-agnostic methods on both MNIST and ImageNet
benchmarks  while being orders of magnitude faster at providing importance estimates than state-
of-the-art model-agnostic methods. We also found that  analogous to standard supervised learning
tasks  special-purpose model architectures may improve the performance of neural explanation
models in images  and that the bootstrap resampled uncertainty estimates for the importance scores
of an explanation model are signiﬁcantly correlated with CXPlain’s ability to accurately estimate
feature importance - indicating that bootstrap resampling is a suitable approach for quantifying
the uncertainty of importance estimates. Causal explanation models that both produce accurate
estimates of feature importance and their uncertainties quickly for any machine-learning model and
data modality may enable users to better understand  validate  and interpret machine-learning models 
while also informing them when their explanations can not be expected to be accurate.

8

******−101234RandomCXPlain(M=5)CXPlain(M=10)CXPlain(M=50)CXPlain(M=100)z−transformed Pearson¢s rUncertainty Estimation AccuracyInputRank ErrorEstimatedUncertaintylog-oddsssh://d909b@ssh.schwabpatrick.com:909/usr/bin/python -u /home/d909b/bin/causal_explanations/causal_explanations/apps/main.py --output_directory=/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty --load_existing=/home/d909b/models/cex_main_1/model.npz --load_existing_cxplain=/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz --dataset=mnist --num_epochs=50 --num_units=75 --batch_size=100 --num_layers=2 --dropout=0.0 --learning_rate=0.001 --l2_weight=0.000 --model_type=resnet --explanation_type=cxplain --do_not_save_attributions --source_digit=8 --target_digit=3 --attack_method=random --attack_epsilon=1 --attack_num_samples=100 --defence_method=none --discrete_attack_type=lsga --do_not_calculate_log_odds --do_not_calculate_robustness --num_explanation_samples=1000 --num_boostrap_samples=5Using TensorFlow backend./usr/local/lib/python2.7/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1dINFO: Args are: {'num_boostrap_samples': 5  'do_adversarial_training': False  'n_jobs': 4  'explanation_type': 'cxplain'  'calculate_uncertainties': True  'num_adversarial_samples': 1000  'hyperopt_against_eval_set': False  'num_epochs': 50  'explanation_model_name': 'explanation.npz'  'copy_to_local': False  'attack_iterations': 100  'seed': 909  'precomputed_attributions_train': ''  'num_layers': 2  'fraction_of_data_set': 1  'precomputed_attributions_val': ''  'calculate_log_odds': False  'do_evaluate': False  'with_tensorboard': False  'do_augment': False  'with_bn': False  'attack_num_samples': 100  'do_hyperopt': False  'attack_epsilon': 1  'save_attributions': False  'save_predictions': True  'target_digit': 3  'load_existing': '/home/d909b/models/cex_main_1/model.npz'  'source_digit': 8  'calculate_robustness': False  'discrete_attack_type': 'lsga'  'num_hyperopt_runs': 35  'thermometer_encoding_levels': 16  'learning_rate': 0.001  'batch_size': 100  'output_directory': '/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty'  'load_existing_cxplain': '/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz'  'do_merge_lsf': False  'slic_compactness': 10  'do_hyperopt_on_lsf': False  'l2_weight': 0.0  'hyperopt_o(cid:31)set': 0  'dataset': 'mnist'  'dropout': 0.0  'defence_method': 'none'  'num_gpus': 1  'imagenet_folder': '/home/d909b/backup/imagenettrain-prep/'  'attack_method': 'random'  'num_layers_cxplain': -1  'early_stopping_patience': 12  'do_train': False  'num_explanation_samples': 1000  'num_units': 75  'test_set_fraction': 0.2  'model_type': 'resnet'  'validation_set_fraction': 0.2  'num_units_cxplain': -1  'model_name': 'model.npz'}INFO: Running at 2019-05-10 16:48:02.083797INFO: Seed is 9092019-05-10 16:48:02.092153: I tensor(cid:30)ow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2019-05-10 16:48:02.200813: I tensor(cid:30)ow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1)  but there must be at least one NUMA node  so returning NUMA node zero2019-05-10 16:48:02.201281: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582pciBusID: 0000:01:00.0totalMemory: 11.90GiB freeMemory: 11.55GiB2019-05-10 16:48:02.201296: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02019-05-10 16:48:02.396536: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2019-05-10 16:48:02.396563: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:988] 0 2019-05-10 16:48:02.396569: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2019-05-10 16:48:02.396735: I tensor(cid:30)ow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11180 MB memory) -> physical GPU (device: 0  name: TITAN Xp  pci bus id: 0000:01:00.0  compute capability: 6.1)INFO: Loading MNIST data.INFO: Run with args: {'num_boostrap_samples': 5  'do_adversarial_training': False  'n_jobs': 4  'explanation_type': 'cxplain'  'calculate_uncertainties': True  'num_adversarial_samples': 1000  'hyperopt_against_eval_set': False  'num_epochs': 50  'explanation_model_name': 'explanation.npz'  'copy_to_local': False  'attack_iterations': 100  'seed': 909  'precomputed_attributions_train': ''  'num_layers': 2  'fraction_of_data_set': 1  'precomputed_attributions_val': ''  'calculate_log_odds': False  'do_evaluate': False  'with_tensorboard': False  'do_augment': False  'with_bn': False  'attack_num_samples': 100  'do_hyperopt': False  'attack_epsilon': 1  'save_attributions': False  'save_predictions': True  'target_digit': 3  'load_existing': '/home/d909b/models/cex_main_1/model.npz'  'source_digit': 8  'calculate_robustness': False  'discrete_attack_type': 'lsga'  'num_hyperopt_runs': 35  'thermometer_encoding_levels': 16  'learning_rate': 0.001  'batch_size': 100  'output_directory': '/home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty'  'load_existing_cxplain': '/home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npz'  'do_merge_lsf': False  'slic_compactness': 10  'do_hyperopt_on_lsf': False  'l2_weight': 0.0  'hyperopt_o(cid:31)set': 0  'dataset': 'mnist'  'dropout': 0.0  'defence_method': 'none'  'num_gpus': 1  'imagenet_folder': '/home/d909b/backup/imagenettrain-prep/'  'attack_method': 'random'  'num_layers_cxplain': -1  'early_stopping_patience': 12  'do_train': False  'num_explanation_samples': 1000  'num_units': 75  'test_set_fraction': 0.2  'model_type': 'resnet'  'validation_set_fraction': 0.2  'num_units_cxplain': -1  'model_name': 'model.npz'}INFO: Loaded generator with 11982 samples. Doing 120 steps of size 100INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Built generators with 120 training samples  20 validation samples and 20 test samples.INFO: Started training feature extraction.__________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to ==================================================================================================input_1 (InputLayer) (None  28  28  1) 0 __________________________________________________________________________________________________conv2d_1 (Conv2D) (None  28  28  16) 160 input_1[0][0] __________________________________________________________________________________________________activation_1 (Activation) (None  28  28  16) 0 conv2d_1[0][0] __________________________________________________________________________________________________conv2d_2 (Conv2D) (None  28  28  16) 2320 activation_1[0][0] __________________________________________________________________________________________________activation_2 (Activation) (None  28  28  16) 0 conv2d_2[0][0] __________________________________________________________________________________________________conv2d_3 (Conv2D) (None  28  28  16) 2320 activation_2[0][0] __________________________________________________________________________________________________activation_3 (Activation) (None  28  28  16) 0 conv2d_3[0][0] __________________________________________________________________________________________________add_1 (Add) (None  28  28  16) 0 activation_1[0][0] activation_3[0][0] __________________________________________________________________________________________________activation_4 (Activation) (None  28  28  16) 0 add_1[0][0] __________________________________________________________________________________________________conv2d_4 (Conv2D) (None  28  28  16) 2320 activation_4[0][0] __________________________________________________________________________________________________activation_5 (Activation) (None  28  28  16) 0 conv2d_4[0][0] __________________________________________________________________________________________________conv2d_5 (Conv2D) (None  28  28  16) 2320 activation_5[0][0] __________________________________________________________________________________________________activation_6 (Activation) (None  28  28  16) 0 conv2d_5[0][0] __________________________________________________________________________________________________add_2 (Add) (None  28  28  16) 0 activation_4[0][0] activation_6[0][0] __________________________________________________________________________________________________activation_7 (Activation) (None  28  28  16) 0 add_2[0][0] __________________________________________________________________________________________________conv2d_6 (Conv2D) (None  28  28  16) 2320 activation_7[0][0] __________________________________________________________________________________________________activation_8 (Activation) (None  28  28  16) 0 conv2d_6[0][0] __________________________________________________________________________________________________conv2d_7 (Conv2D) (None  28  28  16) 2320 activation_8[0][0] __________________________________________________________________________________________________activation_9 (Activation) (None  28  28  16) 0 conv2d_7[0][0] __________________________________________________________________________________________________add_3 (Add) (None  28  28  16) 0 activation_7[0][0] activation_9[0][0] __________________________________________________________________________________________________activation_10 (Activation) (None  28  28  16) 0 add_3[0][0] __________________________________________________________________________________________________conv2d_8 (Conv2D) (None  14  14  32) 4640 activation_10[0][0] __________________________________________________________________________________________________activation_11 (Activation) (None  14  14  32) 0 conv2d_8[0][0] __________________________________________________________________________________________________conv2d_10 (Conv2D) (None  14  14  32) 544 activation_10[0][0] __________________________________________________________________________________________________conv2d_9 (Conv2D) (None  14  14  32) 9248 activation_11[0][0] __________________________________________________________________________________________________activation_13 (Activation) (None  14  14  32) 0 conv2d_10[0][0] __________________________________________________________________________________________________activation_12 (Activation) (None  14  14  32) 0 conv2d_9[0][0] __________________________________________________________________________________________________add_4 (Add) (None  14  14  32) 0 activation_13[0][0] activation_12[0][0] __________________________________________________________________________________________________activation_14 (Activation) (None  14  14  32) 0 add_4[0][0] __________________________________________________________________________________________________conv2d_11 (Conv2D) (None  14  14  32) 9248 activation_14[0][0] __________________________________________________________________________________________________activation_15 (Activation) (None  14  14  32) 0 conv2d_11[0][0] __________________________________________________________________________________________________conv2d_12 (Conv2D) (None  14  14  32) 9248 activation_15[0][0] __________________________________________________________________________________________________activation_16 (Activation) (None  14  14  32) 0 conv2d_12[0][0] __________________________________________________________________________________________________add_5 (Add) (None  14  14  32) 0 activation_14[0][0] activation_16[0][0] __________________________________________________________________________________________________activation_17 (Activation) (None  14  14  32) 0 add_5[0][0] __________________________________________________________________________________________________conv2d_13 (Conv2D) (None  14  14  32) 9248 activation_17[0][0] __________________________________________________________________________________________________activation_18 (Activation) (None  14  14  32) 0 conv2d_13[0][0] __________________________________________________________________________________________________conv2d_14 (Conv2D) (None  14  14  32) 9248 activation_18[0][0] __________________________________________________________________________________________________activation_19 (Activation) (None  14  14  32) 0 conv2d_14[0][0] __________________________________________________________________________________________________add_6 (Add) (None  14  14  32) 0 activation_17[0][0] activation_19[0][0] __________________________________________________________________________________________________activation_20 (Activation) (None  14  14  32) 0 add_6[0][0] __________________________________________________________________________________________________conv2d_15 (Conv2D) (None  7  7  64) 18496 activation_20[0][0] __________________________________________________________________________________________________activation_21 (Activation) (None  7  7  64) 0 conv2d_15[0][0] __________________________________________________________________________________________________conv2d_17 (Conv2D) (None  7  7  64) 2112 activation_20[0][0] __________________________________________________________________________________________________conv2d_16 (Conv2D) (None  7  7  64) 36928 activation_21[0][0] __________________________________________________________________________________________________activation_23 (Activation) (None  7  7  64) 0 conv2d_17[0][0] __________________________________________________________________________________________________activation_22 (Activation) (None  7  7  64) 0 conv2d_16[0][0] __________________________________________________________________________________________________add_7 (Add) (None  7  7  64) 0 activation_23[0][0] activation_22[0][0] __________________________________________________________________________________________________activation_24 (Activation) (None  7  7  64) 0 add_7[0][0] __________________________________________________________________________________________________conv2d_18 (Conv2D) (None  7  7  64) 36928 activation_24[0][0] __________________________________________________________________________________________________activation_25 (Activation) (None  7  7  64) 0 conv2d_18[0][0] __________________________________________________________________________________________________conv2d_19 (Conv2D) (None  7  7  64) 36928 activation_25[0][0] __________________________________________________________________________________________________activation_26 (Activation) (None  7  7  64) 0 conv2d_19[0][0] __________________________________________________________________________________________________add_8 (Add) (None  7  7  64) 0 activation_24[0][0] activation_26[0][0] __________________________________________________________________________________________________activation_27 (Activation) (None  7  7  64) 0 add_8[0][0] __________________________________________________________________________________________________conv2d_20 (Conv2D) (None  7  7  64) 36928 activation_27[0][0] __________________________________________________________________________________________________activation_28 (Activation) (None  7  7  64) 0 conv2d_20[0][0] __________________________________________________________________________________________________conv2d_21 (Conv2D) (None  7  7  64) 36928 activation_28[0][0] __________________________________________________________________________________________________activation_29 (Activation) (None  7  7  64) 0 conv2d_21[0][0] __________________________________________________________________________________________________add_9 (Add) (None  7  7  64) 0 activation_27[0][0] activation_29[0][0] __________________________________________________________________________________________________activation_30 (Activation) (None  7  7  64) 0 add_9[0][0] __________________________________________________________________________________________________average_pooling2d_1 (AveragePoo (None  1  1  64) 0 activation_30[0][0] __________________________________________________________________________________________________(cid:30)atten_1 (Flatten) (None  64) 0 average_pooling2d_1[0][0] __________________________________________________________________________________________________dense_1 (Dense) (None  2) 130 (cid:30)atten_1[0][0] __________________________________________________________________________________________________activation_31 (Activation) (None  2) 0 dense_1[0][0] ==================================================================================================Total params: 270 882Trainable params: 270 882Non-trainable params: 0__________________________________________________________________________________________________INFO: Loading existing model from /home/d909b/models/cex_main_1/model.npz_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_3 (InputLayer) (None  28  28  1) 0 _________________________________________________________________(cid:30)atten_3 (Flatten) (None  784) 0 _________________________________________________________________dense_3 (Dense) (None  75) 58875 _________________________________________________________________activation_63 (Activation) (None  75) 0 _________________________________________________________________dense_4 (Dense) (None  75) 5700 _________________________________________________________________activation_64 (Activation) (None  75) 0 _________________________________________________________________dense_5 (Dense) (None  196) 14896 _________________________________________________________________activation_65 (Activation) (None  196) 0 _________________________________________________________________reshape_1 (Reshape) (None  14  14  1) 0 _________________________________________________________________lambda_2 (Lambda) (None  28  28  1) 0 _________________________________________________________________reshape_2 (Reshape) (None  784) 0 =================================================================Total params: 79 471Trainable params: 79 471Non-trainable params: 0_________________________________________________________________INFO: Loading existing CXPlain model from /home/d909b/models/cex_main_1_cxplain_20.1h/best_explanation.npzbuild_explanation_model : took 1.81394100189 seconds.INFO: Saving loss history to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/losses.pickleINFO: Saving model predictions.INFO: Loaded generator with 11982 samples. Doing 120 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/train_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/val_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Saved model predictions to /home/d909b/models/cex_main_1_cxplain_20.1h_uncertainty/test_predictions.csvINFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Performance on test AUROC (weighted) = 0.999994917356212   with AUPRC (weighted) = 0.9999949280930339   with r^2 (weighted) = 0.9957963109416453   with f1 (weighted) = 0.9984879166751001   with accuracy = 0.9984879032258065   with error = 0.0015120967741935054INFO: Loaded generator with 1984 samples. Doing 20 steps of size 100INFO: Performance on test [n= 100 ] targets = [0.9] observed = [(1.1436718599626305  '+-'  0.5520538782615332)]INFO: Performance on test [n= 100 ] for target 0.9observed = [1.3892620290955573  1.1377237947530967  0.8350603218815686  0.8713871427716444  1.8772789577296631  1.0421385269639882  1.2734869085839489  0.5600630255113783  1.2435160902712874  2.0858731684436953  0.6621914099533671  0.6070763678776874  0.8661585671984602  1.0517880297070337  2.9146049149183884  0.6235869496235044  0.9226209148366383  0.896227432532063  0.2670657996896821  0.013821244358016503  2.189807259704378  0.8175433055816894  1.19488319718703  1.8930241978639137  0.7730931068420339  1.27811588137019  0.9850174716812721  1.1216892414631758  0.573120152159161  1.0232491128492835  0.8987605463776784  1.037649214136879  0.5118367176339356  2.083070000264211  1.302903762457773  0.8204965828874214  1.6789029079041142  1.623794531947206  1.2545555949371687  1.0012906768156877  1.2462736163330106  0.9057037790232868  0.9469323067092333  0.4383414096479612  0.8104805348408998  0.8851755920050628  1.132267732268079  1.6621074239888216  0.934013287760548  0.8349687929106586  0.5289153327173624  1.4440371769826026  2.138591203347149  1.2270851493051227  2.142648193073443  0.3385335850217482  0.46770572775341435  0.8527171407093351  0.47953222713471033  1.8026274831409246  1.1812909273907717  0.9314364881634934  0.2302455728277166  1.5633736254880761  2.3031484613717335  2.4174470115754003  1.1201235152098241  2.266265571565306  1.3994960435747492  0.7860688071695486  1.2776682986304249  1.6580154383934487  0.757406483017344  1.0043128902083016  0.7231292579067669  0.740787741347223  0.7141079559712596  0.6734593179701929  0.6616112633542277  0.9468177183380312  0.7690791699258368  1.0155653037702255  0.6512117579059203  0.9884971489035145  0.6675223757172929  1.7234492486644164  0.7886172116728509  1.1807222822302033  0.8768496962883034  1.2127882377541048  1.0925325778269959  0.5464445929844274  2.096737223622148  0.8172319369500524  1.8495334324613373  1.2724899666210199  1.0701363404910589  1.8738202234307906  2.195503699637992  1.8998484264924382]Process (cid:29)nished with exit code 0ΔUncertainty Estimation SamplesAcknowledgments

This work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302
within the National Research Program (NRP) 75 “Big Data”. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research. Patrick
Schwab is an afﬁliated PhD fellow at the Max Planck ETH Center for Learning Systems. We
additionally thank the anonymous reviewers whose comments helped improve this manuscript.

References
[1] Avanti Shrikumar  Peyton Greenside  Anna Shcherbina  and Anshul Kundaje. Learning important features

through propagating activation differences. International Conference of Machine Learning  2017.

[2] Zachary C Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490  2016.

[3] Pieter-Jan Kindermans  Sara Hooker  Julius Adebayo  Maximilian Alber  Kristof T Schütt  Sven Dähne 
Dumitru Erhan  and Been Kim. The (un) reliability of saliency methods. arXiv preprint arXiv:1711.00867 
2017.

[4] Daniel Smilkov  Nikhil Thorat  Been Kim  Fernanda Viégas  and Martin Wattenberg. Smoothgrad:

removing noise by adding noise. arXiv preprint arXiv:1706.03825  2017.

[5] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv

preprint arXiv:1702.08608  2017.

[6] Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In Advances in

Neural Information Processing Systems  pages 4768–4777  2017.

[7] David Baehrens  Timon Schroeter  Stefan Harmeling  Motoaki Kawanabe  Katja Hansen  and Klaus-Robert
Müller. How to explain individual classiﬁcation decisions. Journal of Machine Learning Research  11
(Jun):1803–1831  2010.

[8] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks: Visualising
image classiﬁcation models and saliency maps. International Conference on Learning Representations 
2014.

[9] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European

Conference on Computer Vision  pages 818–833. Springer  2014.

[10] Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic attribution for deep networks. International

Conference on Machine Learning  2017.

[11] Kelvin Xu  Jimmy Ba  Ryan Kiros  Kyunghyun Cho  Aaron Courville  Ruslan Salakhudinov  Rich Zemel 
and Yoshua Bengio. Show  attend and tell: Neural image caption generation with visual attention. In
International Conference on Machine Learning  pages 2048–2057  2015.

[12] Edward Choi  Mohammad Taha Bahadori  Jimeng Sun  Joshua Kulas  Andy Schuetz  and Walter Stewart.
Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In
Advances in Neural Information Processing Systems  pages 3504–3512  2016.

[13] Patrick Schwab  Gaetano C. Scebba  Jia Zhang  Marco Delai  and Walter Karlen. Beat by Beat: Classifying

Cardiac Arrhythmias with Recurrent Neural Networks. In Computing in Cardiology  2017.

[14] Patrick Schwab  Djordje Miladinovic  and Walter Karlen. Granger-causal Attentive Mixtures of Experts:
Learning Important Features with Neural Networks. In AAAI Conference on Artiﬁcial Intelligence  2019.

[15] Patrick Schwab and Walter Karlen. PhoneMD: Learning to diagnose Parkinson’s disease from smartphone

data. In AAAI Conference on Artiﬁcial Intelligence  2019.

[16] Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  pages 1135–1144. ACM  2016.

[17] Julius Adebayo  Justin Gilmer  Michael Muelly  Ian Goodfellow  Moritz Hardt  and Been Kim. Sanity
checks for saliency maps. In Advances in Neural Information Processing Systems  pages 9505–9515  2018.

[18] Hui Fen  Kuangyan Song  Madeilene Udell  Yiming Sun  Yujia Zhang  et al. Why should you trust my
interpretation? Understanding uncertainty in LIME predictions. arXiv preprint arXiv:1904.12991  2019.

9

[19] Amirata Ghorbani  Abubakar Abid  and James Zou. Interpretation of neural networks is fragile. AAAI

Conference on Artiﬁcial Intelligence  2019.

[20] Erik Štrumbelj  Igor Kononenko  and M Robnik Šikonja. Explaining instance classiﬁcations with interac-

tions of subsets of feature values. Data & Knowledge Engineering  68(10):886–904  2009.

[21] Luisa M Zintgraf  Taco S Cohen  Tameem Adel  and Max Welling. Visualizing deep neural network
decisions: Prediction difference analysis. In International Conference on Learning Representations  2017.

[22] Jiwei Li  Will Monroe  and Dan Jurafsky. Understanding neural networks through representation erasure.

arXiv preprint arXiv:1612.08220  2016.

[23] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation.

In IEEE International Conference on Computer Vision  2017.

[24] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classiﬁers. In Advances in Neural

Information Processing Systems  pages 6967–6976  2017.

[25] Patrick Schwab and Helmut Hlavacs. Capturing the essence: Towards the automated generation of
In AAAI Conference on Artiﬁcial Intelligence and Interactive Digital

transparent behavior models.
Entertainment  2015.

[26] Zhengping Che  Sanjay Purushotham  Robinder Khemani  and Yan Liu. Interpretable deep models for ICU
outcome prediction. In AMIA Annual Symposium Proceedings  volume 2016  page 371. American Medical
Informatics Association  2016.

[27] Osbert Bastani  Carolyn Kim  and Hamsa Bastani. Interpreting blackbox models via model extraction.

arXiv preprint arXiv:1705.08504  2017.

[28] Robert Andrews  Joachim Diederich  and Alan B Tickle. Survey and critique of techniques for extracting

rules from trained artiﬁcial neural networks. Knowledge-based Systems  8(6):373–389  1995.

[29] Aditya Chattopadhyay  Piyushi Manupriya  Anirban Sarkar  and Vineeth N Balasubramanian. Neural

network attributions: A causal perspective. arXiv preprint arXiv:1902.02302  2019.

[30] Grégoire Montavon  Sebastian Lapuschkin  Alexander Binder  Wojciech Samek  and Klaus-Robert Müller.
Explaining nonlinear classiﬁcation decisions with deep taylor decomposition. Pattern Recognition  65:
211–222  2017.

[31] Jianbo Chen  Le Song  Martin J Wainwright  and Michael I Jordan. Learning to explain: An information-

theoretic perspective on model interpretation. International Conference on Machine Learning  2018.

[32] Wenbo Guo  Sui Huang  Yunzhe Tao  Xinyu Xing  and Lin Lin. Explaining deep learning models–
a bayesian non-parametric approach. In Advances in Neural Information Processing Systems  pages
4514–4524  2018.

[33] Michael Tsang  Dehua Cheng  and Yan Liu. Detecting statistical interactions from neural network weights.

International Conference on Learning Representations  2017.

[34] Been Kim  Martin Wattenberg  Justin Gilmer  Carrie Cai  James Wexler  Fernanda Viegas  and Rory Sayres.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV).
International Conference on Machine Learning  2018.

[35] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions. International

Conference of Machine Learning  2017.

[36] Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.

Econometrica: Journal of the Econometric Society  pages 424–438  1969.

[37] Richard Stone. The assumptions on which causal inferences rest. Journal of the Royal Statistical Society.

Series B (Methodological)  pages 455–466  1993.

[38] Dominik Janzing  David Balduzzi  Moritz Grosse-Wentrup  and Bernhard Schölkopf. Quantifying causal

inﬂuences. The Annals of Statistics  41(5):2324–2358  2013.

[39] Pasha Khosravi  Yitao Liang  YooJung Choi  and Guy Van den Broeck. What to expect of classiﬁers?

Reasoning about logistic regression with missing features. arXiv preprint arXiv:1903.01620  2019.

[40] Solomon Kullback. Information theory and statistics. Courier Corporation  1997.

10

[41] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep learning. MIT Press  2016.

[42] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking
the Inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern
Recognition  pages 2818–2826  2016.

[43] Lukasz Kaiser  Aidan N Gomez  Noam Shazeer  Ashish Vaswani  Niki Parmar  Llion Jones  and Jakob

Uszkoreit. One Model To Learn Them All. arXiv preprint arXiv:1706.05137  2017.

[44] Olaf Ronneberger  Philipp Fischer  and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical Image Computing and Computer-assisted
Intervention  pages 234–241. Springer  2015.

[45] Bradley Efron. The jackknife  the bootstrap  and other resampling plans  volume 38. Siam  1982.

[46] Leo Breiman. Random forests. Machine Learning  45(1):5–32  2001.

[47] Balaji Lakshminarayanan  Alexander Pritzel  and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems 
pages 6402–6413  2017.

[48] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty

in deep learning. In International Conference on Machine Learning  pages 1050–1059  2016.

[49] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research  15
(1):1929–1958  2014.

[50] Myles Hollander and Douglas A Wolfe. Nonparametric statistical methods. Wiley New York  NY  USA 

1973.

[51] Yann LeCun  Corinna Cortes  and CJ Burges. MNIST handwritten digit database. AT&T Labs [Online].

Available: http://yann.lecun.com/exdb/mnist  2:18  2010.

[52] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In IEEE Conference on Computer Vision and Pattern Recognition  pages 248–255. IEEE 
2009.

[53] N Clayton Silver and William P Dunlap. Averaging correlation coefﬁcients: Should Fisher’s z transforma-

tion be used? Journal of Applied Psychology  72(1):146  1987.

[54] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467  2016.

[55] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. In International Conference on Machine Learning  pages 448–456  2015.

[56] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference

on Learning Representations  2015.

[57] Günter Klambauer  Thomas Unterthiner  Andreas Mayr  and Sepp Hochreiter. Self-normalizing neural

networks. In Advances in Neural Information Processing Systems  pages 971–980  2017.

11

,Patrick Schwab
Walter Karlen