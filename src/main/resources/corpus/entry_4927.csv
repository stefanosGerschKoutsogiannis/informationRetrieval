2012,Selecting Diverse Features via Spectral Regularization,We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability  robustness to noise  etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers  when added to the objective function for linear regression  result in approximately submodular functions  which can then be maximized approximately by efficient greedy and local search algorithms  with provable guarantees.  We compare our algorithms to traditional greedy and $\ell_1$-regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations.,Selecting Diverse Features via Spectral

Regularization

Abhimanyu Das∗
Microsoft Research

Mountain View

abhidas@microsoft.com

Anirban Dasgupta

Yahoo! Labs
Sunnyvale

anirban@yahoo-inc.com

ravi.k53@gmail.com

Ravi Kumar∗

Google

Mountain View

Abstract

We study the problem of diverse feature selection in linear regression: selecting
a small subset of diverse features that can predict a given objective. Diversity is
useful for several reasons such as interpretability  robustness to noise  etc. We pro-
pose several spectral regularizers that capture a notion of diversity of features and
show that these are all submodular set functions. These regularizers  when added
to the objective function for linear regression  result in approximately submodu-
lar functions  which can then be maximized by efﬁcient greedy and local search
algorithms  with provable guarantees. We compare our algorithms to traditional
greedy and (cid:96)1-regularization schemes and show that we obtain a more diverse set
of features that result in the regression problem being stable under perturbations.

1

Introduction

Feature selection is a key component in many machine learning settings. The process involves
choosing a small subset of features in order to build a model to approximate the target concept
well. Feature selection offers several advantages in practice. This includes reducing the dimension
of the data and hence the space requirements  enhancing the interpretability of the learned model 
mitigating over-ﬁtting  decreasing generalization error  etc.
In this paper we focus on feature selection for linear regression  which is the prediction model of
choice for many practitioners. The goal is to obtain a linear model using a subset of k features (where
k is user-speciﬁed)  to minimize the prediction error or  equivalently  maximize the squared multiple
correlation [16]. In general  feature selection techniques can be categorized into two approaches.
In the ﬁrst  features are greedily selected one by one up to the pre-speciﬁed budget k; the Forward
or Backward greedy methods[19] fall into this type. In the second  the feature selection process is
intimately coupled with the regression objective itself by adding a (usually convex) regularizer. For
example  the Lasso [20] uses the (cid:96)1-norm of the coefﬁcients as a regularizer to promote sparsity.
In this work we consider the feature selection problem of choosing the best set of features for pre-
dicting a speciﬁed target  coupled with the desire to choose as “diverse” features as possible; our
goal will be to construct a regularizer that can capture diversity. Diversity among the chosen features
is a useful property for many reasons. Firstly  it increases the interpretability of the chosen features 
since we are assured that they not redundant and are more representative of the feature space cov-
ered by the entire dataset (see e.g. [7]). Secondly  as we show  the right notion of diversity can also
make the feature selection task resistant to noise in the data. Thirdly  it is well known that correlated
features can slow down the convergence of algorithms such as the stochastic gradient (e.g.  [2]); by
demanding diversity  one can potentially obviate this slowdown.

∗This work was done while the author was at Yahoo! Labs.

1

Unfortunately  the traditional greedy and (cid:96)1-relaxation approaches to feature-selection do not ex-
plictly address feature diversity1. In this paper  we address this problem of diverse feature selection
using an approach that falls between that of greedy methods and convex-regularization methods. In
particular  we construct regularizers that capture a notion of diversity — unlike regularizers such as
Lasso  our regularizers are set functions as opposed to functions of the regression coefﬁcient vector.
Our objective function are thus a combination of the linear regression objective and the regular-
izer. We then design provable approximation algorithms for such objectives using a combination of
greedy and local search techniques. While there is no unique way to deﬁne feature diversity  we take
a spectral approach. By deﬁning diversity to be a carefully chosen function of the spectrum of the
chosen features  we tap into notions of submodularity and consequently into the rich literature for
maximizing submodular functions [5  9  14].
Our contributions are as follows: (i) We formulate an optimization problem for diverse feature
selection and construct a family of submodular spectral regularizers that capture diversity notions.
(ii) We use a novel approach of combining the diversity regularizers with the optimization objective
to obtain (approximately) submodular maximization problems  and optimize them using greedy
and local search algorithms with provable guarantees.
(iii) We validate the performance of our
algorithms using experiments on real and synthetic data sets.

2 Related work

Feature selection and the closely related problems of sparse approximation/recovery have been ex-
tensively studied using two broad classes of methods: greedy [5  19  21  11  24] and convex relax-
ation [20  25  3  22  8]. None of these methods  however  takes feature diversity into the account
during selection. The (greedy) methods in our paper are inspired by those of Das and Kempe [5] 
who provide prediction error bounds using a notion of approximate submodularity. However  they
do not incorporate any notion of feature diversity; they also require monotonicity  which does not
hold for several regularizers we construct. A related convex relaxation based approach is that of
Grave et al. [12]  who address the unstable behavior of Lasso in the presence of correlated features
and propose adding a trace norm regularizer to the error objective. The focus is to select groups
of correlated variables together instead of selecting only one variable from each group. Our goal is
different: select variables that are relatively uncorrelated with each other.
Previous work on diverse feature selection includes greedy heuristics for trading-off information-
theoretic feature relevance and feature redundancy criteria when selecting features [7  23]. However
the heuristics presented do not carry any theoretical guarantees.
There has been some work on selecting a diverse set of features to maximize the mutual information
or the entropy of a set of variables [13  17]. But  the problem deﬁnition in these works does not
specify a target prediction vector or variable; the goal instead is to select diverse features regardless
of whether the features are relevant for predicting a particular target variable. On the other hand  our
work requires us to simultaneously optimize for both feature selection and diversity objectives.
If we consider orthogonality as a loose proxy for diversity  methods such Principal Component
Analysis and Singular Value Decomposition [15] become relevant. However  these methods do not
return elements from the original set of features and instead output linear combinations of the feature
vectors; this might not be desirable for many applications.

3 Preliminaries
For any symmetric positive semideﬁnite n × n matrix A  we denote its eigenvalues by λmin(A) =
λ1(A) ≤ ··· ≤ λn(A) = λmax(A). We use det(A) = Πn
i=1λi(A) to denote the determinant of A.

Recall the vector and matrix two-norms: (cid:107)x(cid:107)2 =(cid:112)(cid:80)

Let X = {X1  . . .   Xn} be the set of feature vectors (or random variables) where each Xi ∈ Rm and
let Z ∈ Rm be the target vector. By appropriate normalization  we can assume (cid:107)Xi(cid:107)2 = 1 = (cid:107)Z(cid:107)2.
We wish to predict Z using linear regression on a small subset of X. The matrix of inner products (or

i |xi|2 and (cid:107)A(cid:107)2 = λmax(A).

1discussed in the supplementary material at http://cs.usc.edu/∼abhimand/nips12supplementary.pdf

2

covariances) between the Xi and Xj is denoted by C  with entries Ci j = Cov(Xi  Xj). Similarly 
we use b to denote the inner products between Z and the Xi’s  with bi = Cov(Z  Xi).
For a n-dimensional Gaussian random vector v with covariance matrix C  we use H(v) =
2 log((2πe)ndet(C)) to denote the differential entropy of v.
1
For a set S ⊆ X  if Z(cid:48)(S) is the optimal linear predictor of Z using the vectors in S  then the
squared multiple correlation [6  16] is deﬁned as R2
2. This is a widely
used goodness-of-ﬁt measure; it captures the length of the projection of Z on the subspace spanned
by the vectors in S.
Deﬁnition 1 (Diverse feature selection) Given k > 0  ﬁnd a set S ⊆ X satisfying

Z(S) = 1 − (cid:107)(Z − Z(cid:48)(S))(cid:107)2

argmax
S:|S|≤k

g(S) ∆= R2

Z(S) + νf (S) 

(1)

where ν > 0 is the regularization constant and f (S) is some “diversity-promoting” regularizer.

Note that diversity is not a uniquely-deﬁned notion  however  we call a regularizer f to be diversity-
promoting if the following two conditions are satisﬁed: for a ﬁxed k  f (S) is maximized when S is
an orthogonal set of vectors and is minimized when S has the lowest rank  where |S| ≤ k.
For convenience  we do not distinguish between the index set S and the variables {Xi | i ∈ S}. We
use CS to denote the submatrix of C with row and column set S  and bS to denote the vector with
only entries bi for i ∈ S. Given the subset S of vectors used for prediction  the optimal regression
coefﬁcients αi are (αi)i∈S = C−1
Many of our results are phrased in terms of eigenvalues of the inner product matrix C and its subma-
trices. Since such matrices are positive semideﬁnite  their eigenvalues are real  non-negative [16].

S bS (e.g.  [16]) and hence R2

Z(S) = bT

S bS. 2

S C−1

Submodularity ratio. Das and Kempe [5] introduced the notion of submodularity ratio for a general
set function to capture how close is the function to being submodular.

Deﬁnition 2 (Submodularity ratio) Let
f
submodularity ratio of f with respect

The
be
to a set U and a parameter k ≥ 1 is

non-negative

function.

set

γU k(f ) =

min

L⊆U S:|S|≤k S∩L=∅

(cid:80)
x∈S f (L ∪ {x}) − f (L)
f (L ∪ S) − f (L)

a

.

Thus  it captures how much f can increase by adding any subset S of size k to L  compared to the
combined beneﬁts of adding its individual elements to L. In particular  [5] deﬁnes the submodularity
ratio for the R2 function and relates it to the smallest eigenvalue of the covariance matrix of the data.
They also show that  in practice  the submodularity ratio for R2 is often quite close to 1  and hence
a greedy algorithm is a good approximation to maximizing R2 subject to a cardinality constraint.

Theorem 3 (from [4]) Let f be a non-negative  monotone set function and let OPT be the maxi-
mum value of f value obtained by any set of size k. Then  the set ˜S selected by the Greedy Algorithm
has the following approximation guarantee: f ( ˜S) ≥ (1 − e−γ ˜S k(f )) · OPT.

3.1 Robustness to perturbations

As mentioned earlier  in addition to providing better interpretability  another beneﬁt of diverse fea-
ture selection is robustness to feature and label perturbations. Given a selected subset S  we now
obtain a connection between the robustness of the estimated regression coefﬁcients and the spectrum
of CS  in the presence of noise. Suppose S  a subset of size k  is used to predict the target vector
Z ∈ Rn. Let A ∈ Rn×k be the vectors from X corresponding to S. Then CS = AT A and the
optimal regression coefﬁcients are α = C−1
Now suppose the target vector is perturbed with an i.i.d. Gaussian noise  i.e.  Z(cid:48) = Z + η  where
η ∼ N (0  σ2In) is a random vector corresponding to measurement errors. Let the corresponding
2We assume throughout that CS is non-singular. For some of our results  an extension to singular matrices

S AT Z.

is possible using the Moore–Penrose generalized inverse.

3

regression coefﬁcient vector be α(cid:48) = C−1
S AT Z(cid:48). We show the following perturbation result relating
the differential entropy of the perturbation error in the regression coefﬁcients to the spectrum of CS.

Lemma 4 H(α(cid:48) − α) = k log(2σ2πe) −(cid:80)k
k log(2σ2πe) −(cid:80)k
Thus the perturbation error entropy is minimized by maximizing(cid:80)k

Proof. Let δ = α(cid:48) − α = C−1
σ2In×n · (C−1

S AT )T ). Or  δ ∼ N (0  σ2C−1

i=1 log(λi(CS)).

i=1 log(λi(CS)).

S AT η. Since η ∼ N (0  σ2In×n)  we have that δ ∼ N (0  C−1
S ). Thus  H(δ) = log((2σ2πe)kdet(C−1

S AT ·
S )) =

i=1 log(λi(CS))  which moti-

vates the smoothed differential-entropy regularizer used in Section 5.1.
We can also show (supplementary material) that the two-norm of the perturbation error in the re-
the expected noise in the regression
gression coefﬁcients is also related to the spectrum of CS:
coefﬁcients depends on the sum of the eigenvalues of C−1
λi(CS ) as
a diversity-promoting regularizer in Deﬁnition 1. Unfortunately  this regularization function is not
submodular and is thus hard to use directly. However  as seen in Sections 5.2 and 5.3  there are other
related spectral functions that are indeed submodular and can thus be used as efﬁcient regularizers.

S . This suggests the use of −(cid:80)

1

i

4 Algorithms

In this section we present a greedy and local-search based (GLS) approximation algorithm for solv-
ing (1) when f (S) is a non-negative (but not necessarily monotone) submodular function (w.l.o.g. 
ν = 1). In order to give an approximation algorithm for argmaxS:|S|≤k g(S)  we need to follow a
sequence of steps. First we show a technical result (Theorem 5) that says that though the approxi-
mation guarantees of [5] do not carry over to the non-monotone case  we can still prove a weaker
result that relates the solution obtained by a greedy algorithm with any feasible solution  as long
as g(S) is approximately submodular and non-negative (which holds if f (S) is a non-negative sub-
modular function). Next  we modify a local-search based algorithm for unconstrained submodular
maximization to give an approximation of argmaxS g(S) (Theorem 7). We put these together using
the framework of [9] to show (Theorem 9) a constant factor approximation for solving (1).
The greedy Forward Regression (FR) algorithm is the following.
1: S0 ← ∅ and U ← {X1  . . .   Xn}.
2: In each step i + 1  select Xj ∈ U \ Si maximizing g(Si ∪ {Xj}). Set Si+1 ← Si ∪ {Xj} and

U ← U \ {Xj}.

3: Output Sk.
Theorem 5 For any set T such that |T| ≤ k  the set S selected by the greedy FR algorithm satisﬁes
g(S) = R2

Z(S) + f (S) ≥ (1 − e− γS 2k

)g(S ∪ T ).

2

The proof is very similar to that of [5  Theorem 3.2] and is omitted due to space constraints. Next 
we consider the problem of unconstrained maximization of the function g(S) = R2
Z(S) + f (S). For
this  we use a local search (LS) algorithm similar to [9].
1: S ← argmaxif (Xi) and U ← {X1  . . .   Xn}.
2: If there exists an element x ∈ U\S such that f (S∪{x}) ≥ (1+ 

n2 )f (S)  then set S ← S∪{x} 

and go back to Step 2.

3: Output argmaxT∈{S U\S U} g(T ).

Notice that even though we are interested in maximizing g(S)  our LS algorithm ﬁnds a local optima
using f  but then uses g to compute the maximum in the last step. To analyze the performance
guarantees of LS  we ﬁrst use the following result of [9  Theorem 3.4].
Lemma 6 If f is non-negative and submodular  then for any set T ⊆ U and any  > 0  the LS
n )f (S)+f (U\S) ≥ f (T ).
algorithm takes O( 1

 n3 log n) time and outputs solution S such that (2+ 2

Using the above  we prove an approximation guarantee for unconstrained maximization of g(S).

4

1

4+ 4
n

approximation for solving argmaxS g(S).

Theorem 7 The LS algorithm is a
Proof. Suppose the optimal solution is C∗ such that g(C∗) = OPT. Consider the set S obtained
by the LS algorithm when it terminates. We obtain g(C∗) = f (C∗) + R2(C∗) ≤ (2 + 2/n)f (S) +
f (U \ S) + R2(U ) ≤ (2 + 2/n)g(S) + g(U \ S) + g(U )  where the second step follows from
Lemma 6 and the monotonicity of R2 and the last step follows from the non-negativity of f and R2.
Thus  max(g(S)  g(U \ S)  g(U )) ≥ 1

g(C∗).

4+ 4
n

S2 ← FR(U \ S1).

We now present the greedy and local search (GLS) algorithm for solving (1) for any submodular 
non-monotone  non-negative regularizer.
1: U ← {X1  . . .   Xn}.
1 ← LS(S1) 
2: S1 ← FR(U ) 
S(cid:48)
3: Output argmaxS∈{S1 S(cid:48)
1 S2} g(S).
Next  we prove a multiplicative approximation guarantee for the GLS algorithm.
Lemma 8 Given sets C  S1 ⊆ U  let C(cid:48) = C \ S1 and S2 ⊆ U \ S1. Then g(S1 ∪ C) + g(S2 ∪
C(cid:48)) + g(S1 ∩ C) ≥ g(C).
Z(S)  we obtain g(S1∪C)+g(S2∪
Proof. Using the submodularity of f and the monotonicity of R2
Z(C) + f (S1∪ S2∪ C) + f (C(cid:48)).
C(cid:48)) = R2
Now  f (C(cid:48)) + f (S1 ∩ C) ≥ f (C) + f (∅) ≥ f (C)  or f (C(cid:48)) ≥ f (C)− f (S1 ∩ C). Hence  we have
g(S1 ∪ C) + g(S2 ∪ C(cid:48)) + f (S1 ∩ C) ≥ R2
Theorem 9 If f is non-negative and submodular and  < n

Z(S2∪ C(cid:48)) + f (S1∪ C) + f (S2∪ C(cid:48)) ≥ R2
Z(C) + f (C) = g(C).

4   the set ˜S selected by the GLS algorithm

Z(S1∪ C) + R2

approximation for solving argmaxS:|S|≤k g(S).

gives a

− γ ˜S 2k

2

1−e
− γ ˜S 2k

2+(1−e

≥ 1−e

− γ ˜S 2k
7

2

2

)(4+4/n)

2

Proof. Let C∗ be the optimal solution with g(C∗) = OPT. Then g(S1) ≥ κg(S1 ∪ C∗)  where
). If g(S1 ∩ C∗) ≥ OPT  then using the LS algorithm on S1  we get (using
κ = (1 − e− γS1 2k
n . Else  g(S1) ≥ κg(S1 ∪
Theorem 7) a solution of value at least 
C∗) + κg(S1 ∩ C∗) − κOPT. Also  g(S2) ≥ κg(S2 ∪ (C∗ \ S1)). Thus  g(S1) + g(S2) ≥
κg(S1∪ C∗) + κg(S1∩ C∗)− κOPT + κg(S2∪ (C∗\ S1)) ≥ κg(C∗)− κOPT ≥ κ(1− )OPT 
where the last inequality follows from Lemma 8. Thus  max(g(S1)  g(S2)) ≥ κ(1−)OPT
. Hence 
the approximation factor is max( 

α g(C∗)  where α = 4 + 4

). Setting  = κα

κα+2-approximation.

κα+2  we get a

κ

α   κ(1−)

2

2

When f (S) is a monotone  non-negative  submodular function  the problem becomes much easier
due to the proposition below that follows directly from the deﬁnition of the submodularity ratio.

Proposition 10 For any submodular set function f (S)  the function g(S) = R2
γU k(g) ≥ γU k(R2) for any U and k.
Thus  since g(S) is monotone and approximately submodular  we can directly apply [4  Theorem 3]
to show that the greedy FR algorithm gives a (1 − e−γ ˜S k(f ))-approximation.

Z(S)+f (S) satisﬁes

5 Spectral regularizers for diversity

In this section we propose a number of diversity-promoting regularizers for the feature selection
problem. We then prove that our algorithms in the previous section can obtain provable guarantees
for each of the corresponding regularized feature selection problems.
Most of our analysis requires the notion of operator antitone function [1] and its connection with
submodularity that was recently obtained by Friedland and Gaubert [10].

Deﬁnition 11 (Operator antitone functions [1]) A real valued function h is operator antitone on
the interval Γ ∈ R if for all n ≥ 1 and for all n × n Hermitian matrices A and B  we have
A (cid:22) B =⇒ h(B) (cid:22) h(A)  where A (cid:22) B denotes that B − A is positive semideﬁnite; the function
h is called operator monotone if −h is operator antitone.

5

Theorem 12 ([10]) Let f be a real continuous function deﬁned on an interval Γ of R. If the deriva-
tive of f is operator antitone on the interior of Γ  then for every n × n Hermitian matrix C with

spectrum in Γ  the set function (from 2n −→ R) tr(f (S)) =(cid:80)n

i=1 f (λi(CS)) is submodular.

We will frequently use the following lemma for proving monotonicity of set functions. The proof is
given in the supplementary material.
Lemma 13 If f is a monotone and non-negative function deﬁned on R  then for every n × n Her-

mitian matrix C  the set function tr(f (S)) =(cid:80)n

i=1 f (λi(CS)) is monotone.

5.1 Smoothed differential entropy regularizer

entropy regularizer as fde(S) =(cid:80)|S|
(cid:80)|S|

For any set S with the corresponding covariance matrix CS  we deﬁne the smoothed differential
i=1 log2(δ + λi(CS))− 3k log2 δ  where δ > 0 is the smoothing
constant. This is a smoothed version of the log-determinant function fld(S) = log(det(CS)) =
i=1 log(λi(CS))  that is also normalized by an additive term of 3k log2 δ in order to make the

regularizer non-negative 3.
As shown in Lemma 4  this regularizer also helps improve the robustness of the regression model to
noise since maximizing fld(S) minimizes the entropy of the perturbation error. For a multivariate
Gaussian distribution  fld(S) also equivalent (up to an additive |S| factor) to the differential entropy
of S. However  fld(S) is undeﬁned if S is rank-deﬁcient and might also take negative values; the
smoothed version fde(S) overcomes these issues. It is also easy to show that fde(S) is a diversity-
promoting regularizer. We now show that the GLS algorithm to solve (1) with f (S) = fde(S) gives
a constant-factor approximation algorithm.

Theorem 14 The set ˜S selected by the GLS algorithm gives a 1−e
tion guarantee for (1) using the smoothed differential entropy regularizer fde(S).

2

− γ ˜S 2k
7

multiplicative approxima-

Proof. We ﬁrst prove that fde(S) is non-negative and submodular. Consider the real-valued func-
tion ˜f (t) = log(δ + t) deﬁned on the appropriate interval of R. We will show that the derivative of
˜f is operator antitone. Let A  B be k × k Hermitian matrices  such that 0 ≺ A (cid:22) B. Let I denote
the identity matrix. Then A + δI (cid:22) B + δI. Taking inverses  (B + δI)−1 (cid:22) (A + δI)−1. Thus 
δ+t is operator antitone. Since h(t) is the derivative of ˜f (t) 
by Deﬁnition 11  the function h(t) = 1
a straightforward application of Theorem 12 gives us that fde(S) is submodular. By Proposition 10 
we obtain that g(S) is approximately submodular  with submodularity ratio at least γ ˜S k(R2) . Since
g(S) is also non-negative  we can now apply Theorem 9 to obtain the approximation guarantee.

Notice that since fde(S) is not monotone in general [13]  we cannot use Theorem 3. However  in
the case when δ ≥ 1  a simple application of Lemma 13 shows that fde(S) becomes monotonically
increasing and we can then use Theorem 3 to obtain a tighter approximation bound.

eralized rank regularizer as fgr(S) =(cid:80)|S|

5.2 Generalized rank regularizer
For any set S with covariance matrix CS  and constant α such that 0 ≤ α ≤ 1  we deﬁne the gen-
i=1 λi(CS)α. Notice that for α = 0  fgr(S) = rank(CS).
The rank function however  does not discriminate between a full-rank matrix and an orthogonal ma-
trix  and hence we deﬁne fgr(S) as a generalization of the rank function. It is easy to show that
fgr(S) is diversity-promoting. We prove that fgr(S) is also monotone and submodular  and hence
obtain approximation guarantees for the greedy FR algorithm for (1) with f (S) = fgr(S).
Theorem 15 The set ˜S selected by the greedy FR algorithm gives a (1 − e−γ ˜S k(R2)) multiplicative
approximation guarantee for (1) using the generalized rank regularizer fgr(S).

3we need this regularizer to be non-negative for sets of size up to 3k  because of the use of f (S1 ∪ S2 ∪ C)

in the proof of Lemma 8

6

Proof. Consider the real-valued function ˜f (t) = tα deﬁned on t ∈ R. It is well known [1] that
the derivative of ˜f is operator antitone. Thus  Theorem 12 gives us that fgr(S) is submodular.
Hence  by applying Lemma 10  we obtain that g(S) is an approximately submodular function  with
submodularity ratio at least γ ˜S k(R2) . Also  by deﬁnition ˜f (t) is non-negative and monotone.
Thus  using Lemma 13  we get that fgr(S) and consequently g(S) is a monotonically increasing set
function. Since g(S) is non-negative  monotone  and submodular  we can now apply Theorem 3 to
obtain a (1 − e−γ ˜S k(R2)) approximation ratio.

5.3 Spectral variance regularizer

−(cid:80)|S|
9k2−(cid:80)|S|

For a set S with covariance matrix CS  we deﬁne the spectral variance regularizer as
i=1(λi(CS) − 1)2. This regularizes the variance of the eigenvalues of the matrix (recall that
for an orthogonal matrix  all the eigenvalues are equal to 1) and can be shown to be diversity-
promoting. For non-negativity  we add a constant 9k2 term4 to the regularizer and deﬁne fsv(S) =
i=1(λi(CS)− 1)2. As with fde(S)  we can show (proof relegated to the supplementary ma-
terial) that fsv(S) is submodular  but it is not monotonically increasing in general. Hence  appealing
to Theorem 9  we obtain the following.

− γ ˜S 2k
Theorem 16 The set ˜S selected by the GLS algorithm gives a 1−e
7
tion guarantee for (1) using the spectral variance regularizer fsv(S).

2

multiplicative approxima-

6 Experiments and results

In this section we conduct experiments in different settings to validate the robustness of our spectral
regularizers. We compare our approach against two baselines: Lasso and greedy FR. We use two
different datasets for the experiments  the mnist data (http://yann.lecun.com/exdb/
mnist/) and a simulation data (for which  results are presented in the supplementary material).
The way we synthesize a regression problem out of the mnist dataset is as follows. Each image
is regarded as a feature vector (of size 784) consisting of the pixel intensities. The target vector for
the regression problem consists of the vector of labels. We only sample 1000 images out of the set 
and thus have a regression problem with X ∈ R1000×784 and Z ∈ R1000. We then preprocess the
columns of matrix X and the target vector Z to have unit (cid:96)2-length.
We use two baselines: lasso and no-reg  the greedy FR with no regularizer. We also use four
different spectral regularizers: ld (Section 5.1  with δ = 1)  ld-0.1 (Section 5.1  with δ = 0.1) 
sv (Section 5.3)  and gr (Section 5.2). We considered two different types of perturbations: perturb-
ing Z and X. In order to perturb Z  we ﬁrst sample a random vector η ∈ R1000  ηi ∼ N (0  1). We
then create Z(cid:48) = Z + σ η(cid:107)η(cid:107)  where σ is varied in [0  1]5. If S is the set of features selected  then the
unperturbed regression coefﬁcients are deﬁned as α = C−1
S Z  and the perturbed coefﬁcients as
S Z(cid:48). The error that we measure is (cid:107)α − α(cid:48)(cid:107)2. Similarly  in order to perturb X  we ﬁrst
α(cid:48) = C−1
sample E ∈ R1000×784. Let E(cid:63)i denote the ith column of E. Then  we create X(cid:48)  the perturbed
version of X columnwise as X(cid:48)
(cid:107)E(cid:63)i(cid:107). Here again  the perturbed regression coefﬁcients
S and the error is measured as (cid:107)α − α(cid:48)(cid:107)2. For our
are α(cid:48) = C(cid:48)
T y where C(cid:48)
experiments  we apply each random perturbation 5 times and then take the average error. Note that
the differential entropy of α−α(cid:48) is directly given by Lemma 4; we will directly measure the quantity
on the RHS of the equation of Lemma 4.

(cid:63)i = X(cid:63)i + σ E(cid:63)i
S)T X(cid:48)
S = (X(cid:48)

S X T

S X T

−1X(cid:48)

S

S

Results. Figure 1 summarizes the result for the mnist data. For clarity of presentation  we have
only shown the results of greedy FR for monotone regularizers (ld and gr) and GLS for non-
monotone (ld-0.1  sv). We also show the results only for σ = 0.1; results for other values
of σ are similar. The way we decided on the regularization parameters λ is as follows. First we
run the lasso using a regularization path approach  and obtain a set of solutions for a range of

4as before  we need this regularizer to be non-negative for sets of size up to 3k due to the proof of Lemma 8
5Strictly speaking  normalizing η makes it non-Gaussian  but for a high dimensional vector (cid:107)η(cid:107) is highly

concentrated.

7

Figure 1: All plots on mnist data. (a) Error when Z is perturbed (σ = 0.1). (b) Error when X is
perturbed (σ = 0.1). (c) Diversity comparison for ld. (d) Diversity comparison for ld-0.1. (e)
Diversity comparison for sv. (f) Diversity comparison for gr.

.

regularization parameter values and corresponding sparsity (k) values. For the other algorithms  we
use each of this set of sparsity values as the target number of features to be selected. We chose the
regularization constant (ν) to be the maximum subject to the condition that the R2 value for that
solution should be greater than that obtained by the lasso solution with corresponding sparsity.
This ensures we are not sacriﬁcing diversity for solution quality.
Figure 1(a) shows the errors obtained when perturbing the Z vector. As is obvious from the ﬁg-
ure  the coefﬁcient vector obtained by lasso is very susceptible to perturbation  and the effect of
perturbation increases with the number of features used by lasso. This indicates that as lasso
starts incorporating more features  it does not ensure that the features are diverse enough so as to be
robust to perturbation. Greedy with no regularization seems more stable than lasso but still shows
an increasing trend. On the other hand  the errors obtained by perturbing is much less for any of the
regularizers  and is only very mildly increasing with k: it does not seem to matter which regularizer
we employ. Figure 1(b) shows the error obtained when perturbing the X matrix; the same story is
true here also. In both cases  using any of the regularizers we are able to pick a set of features that
are more robust to perturbation.
Figures 1(c)- 1(f) show that our features are also more diverse than the ones obtained by both lasso
and no-reg. Since there is no one deﬁnition of diversity  in each of the plots  we take one of the
deﬁnitions of diversity value corresponding to the four regularizers we use. In order to be able to
compare  the regularizer values for each k are normalized by the maximum value possible for that
k. For each of the plots we show the values of the diversity value for solutions at different levels of
sparsity. It is obvious that we get more diverse solutions than both lasso and no-reg. The lines
corresponding to lasso or no-reg show an increasing trend because of the normalization.

7 Conclusions

In this paper we proposed submodular spectral regularizers for diverse feature selection and obtained
efﬁcient approximation algorithms using greedy and local search algorithms. These algorithms
obtain a more diverse and noise-insensitive set of features. It would be interesting to see whether we
can design convex relaxations for such approaches  and to compare our approach with related ones
e.g. CLASH [18] that presents a general framework for merging combinatorial constraints with the
L1-norm constraint for LASSO  or with Elastic-Net that provides stability to the features selected
when groups of correlated variables are present.

8

10203040506070809000.020.040.060.080.10.12Number of features selectedError in beta lassono−reglogdetlogdet−0.1spec−variancegen−rank10203040506070809000.10.20.30.40.50.60.7Number of features selectedError in beta lassono−reglogdetlogdet−0.1spec−variancegen−rank1020304050607080900.550.60.650.70.750.80.850.90.951Number of features selectedRegularizer value(logdet) lassono−reglogdet1020304050607080900.40.50.60.70.80.91Number of features selectedRegularizer value(logdet−0.1) lassono−reglogdet−0.11020304050607080900.930.940.950.960.970.980.991Number of features selectedRegularizer value(spec−var) lassono−regspec−var1020304050607080900.650.70.750.80.850.90.951Number of features selectedRegularizer value(gen−rank) lassono−reggen−rankReferences
[1] R. Bhatia. Matrix Analysis. Springer  1997.
[2] J. K. Bradley  A. Kyrola  D. Bickson  and C. Guestrin. Parallel coordinate descent for l1-regularized loss

minimization. In ICML  pages 321–328  2011.

[3] E. J. Candes  J. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate measure-

ments. CPAM  59:1207–1223  2005.

[4] A. Das. Subset Selection Algorithms for Prediction. PhD thesis  University of Southern California  2011.
[5] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection  sparse ap-

proximation and dictionary selection. In ICML  pages 1057–1064  2011.

[6] G. Diekhoff. Statistics for the Social and Behavioral Sciences. Wm. C. Brown Publishers  2002.
[7] C. Ding and H. Peng. Minimum redundancy feature selection from microarray gene expression data. In

J. Bioinform. Comput. Biol.  pages 523–529  2003.

[8] D. Donoho. For most large underdetermined systems of linear equations  the minimal 11-norm near-

solution approximates the sparsest near-solution. CPAM  59:1207–1223  2005.

[9] U. Feige  V. S. Mirrokni  and J. Vondrak. Maximizing non-monotone submodular functions. SIAM J.

Comput  40(4):1133–1153  2011.

[10] S. Friedland and S. Gaubert. Submodular spectral functions of principal submatrices of a Hermitian

matrix  extensions and applications. Linear Algebra and its Applications  2011.

[11] A. Gilbert  S. Muthukrishnan  and M. Strauss. Approximation of functions over redundant dictionaries

using coherence. In SODA  2003.

[12] E. Grave  G. Obozinski  and F. R. Bach. Trace Lasso: a trace norm regularization for correlated designs.

In NIPS  2011.

[13] C. Guestrin  A. Krause  and A. Singh. Near-optimal sensor placements in Gaussian processes. In ICML 

2005.

[14] A. Gupta  A. Roth  G. Schoenebeck  and K. Talwar. Constrained non-monotone submodular maximiza-

tion: Ofﬂine and secretary algorithms. In WINE  pages 246–257  2010.

[15] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press  1999.
[16] R. A. Johnson and D. W. Wichern. Applied Multivariate Statistical Analysis. Prentice Hall  2002.
[17] C.-W. Ko  J. Lee  and M. Queyranne. An exact algorithm for maximum entropy sampling. OR  43(4):684–

691  1995.

[18] A. Kyrillidis and V. Cevher. Combinatorial selection and least absolute shrinkage via the clash algorithm.
In Information Theory Proceedings (ISIT)  2012 IEEE International Symposium on  pages 2216 –2220 
july 2012.

[19] A. Miller. Subset Selection in Regression. Chapman and Hall  second edition  2002.
[20] R. Tibshirani. Regression shrinkage and selection via the Lasso. JRSS  58:267–288  1996.
[21] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Information Theory 

50:2231–2242  2004.

[22] J. Tropp. Just relax: Convex programming methods for identifying sparse signals. IEEE TOIT  51:1030–

1051  2006.

[23] L. Yu. Redundancy based feature selection for microarray data. In SIGKDD  pages 737–742  2004.
[24] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In NIPS 

2008.

[25] S. Zhou. Thresholding procedures for high dimensional variable selection and statistical estimation. In

NIPS  2009.

9

,Chao Chen
Han Liu
Dimitris Metaxas
Tianqi Zhao
Maxim Rabinovich
Elaine Angelino
Michael Jordan
Maria-Florina Balcan
Travis Dick
Ritesh Noothigattu
Ariel Procaccia