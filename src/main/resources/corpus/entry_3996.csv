2013,On the Representational Efficiency of Restricted Boltzmann Machines,This paper examines the question: What kinds of distributions can be efficiently represented by Restricted Boltzmann Machines (RBMs)?   We characterize the RBM's unnormalized log-likelihood function as a type of neural network (called an RBM network)  and through a series of simulation results relate these networks to types that are better understood.  We show the surprising result that RBM networks can efficiently compute any function that depends on the number of 1's in the input  such as parity.  We also provide the first known example of a particular type of distribution which provably cannot be efficiently represented by an RBM (or equivalently  cannot be efficiently computed by an RBM network)  assuming a realistic exponential upper bound on the size of the weights.  By formally demonstrating that a relatively simple distribution cannot be represented efficiently by an RBM our results provide a new rigorous justification for the use of potentially more expressive generative models  such as deeper ones.,On the Representational Efﬁciency of Restricted

Boltzmann Machines

James Martens∗

Arkadev Chattopadhyay+

Toniann Pitassi∗

Richard Zemel∗

∗Department of Computer Science

University of Toronto

{jmartens toni zemel}@cs.toronto.edu

+School of Technology & Computer Science
Tata Institute of Fundamental Research

arkadev.c@tifr.res.in

Abstract

This paper examines the question: What kinds of distributions can be efﬁciently
represented by Restricted Boltzmann Machines (RBMs)? We characterize the
RBM’s unnormalized log-likelihood function as a type of neural network  and
through a series of simulation results relate these networks to ones whose repre-
sentational properties are better understood. We show the surprising result that
RBMs can efﬁciently capture any distribution whose density depends on the num-
ber of 1’s in their input. We also provide the ﬁrst known example of a particular
type of distribution that provably cannot be efﬁciently represented by an RBM  as-
suming a realistic exponential upper bound on the weights. By formally demon-
strating that a relatively simple distribution cannot be represented efﬁciently by
an RBM our results provide a new rigorous justiﬁcation for the use of potentially
more expressive generative models  such as deeper ones.

1

Introduction

Standard Restricted Boltzmann Machines (RBMs) are a type of Markov Random Field (MRF) char-
acterized by a bipartite dependency structure between a group of binary visible units x ∈ {0  1}n
and binary hidden units h ∈ {0  1}m. Their energy function is given by:

Eθ(x  h) = −x(cid:62)W h − c(cid:62)x − b
(cid:62)

h

by(cid:80)

where W ∈ Rn×m is the matrix of weights  c ∈ Rn and b ∈ Rm are vectors that store the
input and hidden biases (respectively) and together these are referred to as the RBM’s parameters
θ = {W  c  b}. The energy function speciﬁes the probability distribution over the joint space (x  h)
exp(−Eθ(x  h)) with the partition function Zθ given
via the Boltzmann distribution p(x  h) = 1
x h exp(−Eθ(x  h)). Based on this deﬁnition  the probability for any subset of variables can
Zθ
be obtained by conditioning and marginalization  although this can only be done efﬁciently up to a
multiplicative constant due to the intractability of the RBM’s partition function (Long and Servedio 
2010).
RBMs have been widely applied to various modeling tasks  both as generative models (e.g. Salakhut-
dinov and Murray  2008; Hinton  2000; Courville et al.  2011; Marlin et al.  2010; Tang and
Sutskever  2011)  and for pre-training feed-forward neural nets in a layer-wise fashion (Hinton and
Salakhutdinov  2006). This method has led to many new applications in general machine learning
problems including object recognition and dimensionality reduction. While promising for practical
applications  the scope and basic properties of these statistical models have only begun to be studied.
As with any statistical model  it is important to understand the expressive power of RBMs  both
to gain insight into the range of problems where they can be successfully applied  and to provide
justiﬁcation for the use of potentially more expressive generative models.
In particular  we are
interested in the question of how large the number of hidden units m must be in order to capture a
particular distribution to arbitrarily high accuracy. The question of size is of practical interest  since
very large models will be computationally more demanding (or totally impractical)  and will tend to
overﬁt a lot more during training.

1

construct simple distributions where the mass is a function of(cid:80)

It was shown by Freund and Haussler (1994)  and later by Le Roux and Bengio (2008) that for
binary-valued x  any distribution over x can be realized (up to an approximation error which van-
ishes exponentially quickly in the magnitude of the parameters) by an RBM  as long as m is allowed
to grow exponentially fast in input dimension (n). Intuitively  this construction works by instantiat-
ing  for each of the up to 2n possible values of x that have support  a single hidden unit which turns
on only for that particular value of x (with overwhelming probability)  so that the corresponding
probability mass can be individually set by manipulating that unit’s bias parameter. An improve-
ment to this result was obtained by Montufar and Ay (2011); however this construction still requires
that m grow exponentially fast in n.
Recently  Montufar et al. (2011) generalized the construction used by Le Roux and Bengio (2008)
so that each hidden unit turns on for  and assigns probability mass to  not just a single x  but a
“cubical set” of possible x’s  which is deﬁned as a subset of {0  1}n where some entries of x are
ﬁxed/determined  and the rest are free. By combining such hidden units that are each specialized to
a particular cubic set  they showed that any k-component mixture of product distributions over the
free variables of mutually disjoint cubic sets can be approximated arbitrarily well by an RBM with
m = k hidden units.
Unfortunately  families of distributions that are of this specialized form (for some m = k bounded by
a polynomial function of n) constitute only a very limited subset of all distributions that have some
kind of meaningful/interesting structure. For example  this result would not allow us to efﬁciently
i xi (e.g.  for p(x) ∝ PARITY(x)).
In terms of what kinds of distributions provably cannot be efﬁciently represented by RBMs  even
less is known. Cueto et al. (2009) characterized the distributions that can be realized by a RBM with
k parameters as residing within a manifold inside the entire space of distributions on {0  1}n whose
dimension depends on k. For sub-exponential k this implies the existence of distributions which
cannot be represented. However  this kind of result gives us no indication of what these hard-to-
represent distributions might look like  leaving the possibility that they might all be structureless or
otherwise uninteresting.
In this paper we ﬁrst develop some tools and simulation results which relate RBMs to certain easier-
to-analyze approximations  and to neural networks with 1 hidden layer of threshold units  for which
many results about representational efﬁciency are already known (Maass  1992; Maass et al.  1994;
Hajnal et al.  1993). This opens the door to a range of potentially relevant complexity results  some
of which we apply in this paper.
Next  we present a construction that shows how RBMs with m = n2 + 1 can produce arbitrarily
good approximations to any distribution where the mass is a symmetric function of the inputs (that
i xi). One example of such a function is the (in)famous PARITY function  which
was shown to be hard to compute in the perceptron model by the classic Minsky and Papert book
from 1968. This distribution is highly non-smooth and has exponentially many modes.
Having ruled out distributions with symmetric mass functions as candidates for ones that are hard for
RBMs to represent  we provide a concrete example of one whose mass computation involves only
one additional operation vs computing PARITY  and yet whose reprentation by an RBM provably
requires m to grow exponentially with n (assuming an exponental upper bound on the size of the
RBM’s weights). Because this distribution is particularly simple  it can be viewed as a special case
of many other more complex types of distributions  and thus our results speak to the hardness of
representing those distributions with RBMs as well.
Our results provide a ﬁne delineation between what is “easy” for RBMs to represent  and what is
“hard”. Perhaps more importantly  they demonstrate that the distributions that cannot be efﬁciently
represented by RBMs can have a relatively basic structure  and are not simply random in appearance
as one might hope given the previous results. This provides perhaps the ﬁrst completely rigorous
justiﬁcation for the use of deeper generative models such as Deep Boltzmann Machines (Salakhutdi-
nov and Hinton  2009)  and contrastive backpropagation networks (Hinton et al.  2006) over standard
RBMs.
The rest of the paper is organized as follows. Section 2 characterizes the unnormalized log-
likelihood as a type of neural network (called an “RBM network”) and shows how this type is related
to single hidden layer neural networks of threshold neurons  and to an easier-to-analyze approxima-
tion (which we call a “hardplus RBM network”). Section 3 describes a m = n2 + 1 construction for

distributions whose mass is a function of(cid:80) xi  and in Section 4 we present an exponential lower

is  it depends on(cid:80)

bound on m for a slightly more complicated class of explicit distributions. Note that all proofs can
be found in the Appendix.

2

Figure 1: Left: An illustration of a basic RBM network with n = 3 and m = 5. The hidden biases are omitted
to avoid clutter. Right: A plot comparing the soft and hard activation functions.

2 RBM networks

2.1 Free energy function

In an RBM  the (negative) unnormalized log probability of x  after h has been marginalized out 
is known as the free energy. Denoted by Fθ(x)  the free energy satisﬁes the property that p(x) =
exp(−Fθ(x))/Zθ where Zθ is the usual partition function.
It is well known (see Appendix A.1 for a derivation) that  due to the bipartite structure of RBMs 
computing F is tractable and has a particularly nice form:

Fθ(x) = −c(cid:62)x −(cid:88)

log(1 + exp(x(cid:62)[W ]j + bj))

(1)

j

where [W ]j is the j-th column of W .
Because the free energy completely determines the log probability of x  it fully characterizes an
RBM’s distribution. So studying what kinds of distributions an RBM can represent amounts to
studying the kinds of functions that can be realized by the free energy function for some setting of
θ.

2.2 RBM networks

The form of an RBM’s free energy function can be expressed as a standard feed-forward neural
network  or equivalently  a real-valued circuit  where instead of using hidden units with the usual
sigmoidal activation functions  we have m “neurons” (a term we will use to avoid confusion with
the original meaning of a “unit” in the context of RBMs) that use the softplus activation function:

soft(y) = log(1 + exp(y))

Note that at the cost of increasing m by one (which does not matter asymptotically) and introducing
an arbitrarily small approximation error  we can assume that the visible biases (c) of an RBM are
all zero. To see this  note that up to an additive constant  we can very closely approximate c(cid:62)x by
soft(K + c(cid:62)x) ≈ K + c(cid:62)x for a suitably large value of K (i.e.  K (cid:29) (cid:107)c(cid:107)1 ≥ maxx(c(cid:62)x)).
Proposition 11 in the Appendix quantiﬁes the very rapid convergence of this approximation as K
increases.
These observations motivate the following deﬁnition of an RBM network  which computes functions
with the same form as the negative free energy function of an RBM (assumed to have c = 0)  or
equivalently the log probability (negative energy) function of an RBM. RBM networks are illustrated
in Figure 1.

Deﬁnition 1 A RBM network with parameters W  b is deﬁned as a neural network with one hidden
layer containing m softplus neurons and weights and biases given by W and b  so that each neuron
j’s output is soft([W ]j + bj). The output layer contains one neuron whose weights and bias are
given by 1 ≡ [11...1](cid:62) and the scalar B  respectively.
For convenience  we include the bias constant B so that RBM networks shift their output by an
additive constant (which does not affect the probability distribution implied by the RBM network
since any additive constant is canceled out by log Z in the full log probability).

3

−4−202460123456Functionvaluesysoftplushardplus2.3 Hardplus RBM networks

A function which is somewhat easier to analyze than the softplus function is the so-called hardplus
function (aka ‘plus’ or ‘rectiﬁcation’)  deﬁned by:

hard(y) = max(0  y)

As their names suggest  the softplus function can be viewed as a smooth approximation of the
hardplus  as illustrated in Figure 1. We deﬁne a hardplus RBM network in the obvious way: as an
RBM network with the softplus activation functions of the hidden neurons replaced with hardplus
functions.
The strategy we use to prove many of the results in this paper is to ﬁrst establish them for hardplus
RBM networks  and then show how they can be adapted to the standard softplus case via simulation
results given in the following section.

2.4 Hardplus RBM networks versus (Softplus) RBM networks

In this section we present some approximate simulation results which relate hardplus and standard
(softplus) RBM networks.
The ﬁrst result formalizes the simple observation that for large input magnitudes  the softplus and
hardplus functions behave very similarly (see Figure 1  and Proposition 11 in the Appendix).
Lemma 2. Suppose we have a softplus and hardplus RBM networks with identical sizes and pa-
rameters. If  for each possible input x ∈ {0  1}n  the magnitude of the input to each neuron is
bounded from below by C  then the two networks compute the same real-valued function  up to an
error (measured by | · |) which is bounded by m exp(−C).
The next result demonstrates how to approximately simulate a RBM network with a hardplus RBM
network while incurring an approximation error which shrinks as the number of neurons increases.
The basic idea is to simulate individual softplus neurons with groups of hardplus neurons that com-
pute what amounts to a piece-wise linear approximation of the smooth region of a softplus function.
Theorem 3. Suppose we have a (softplus) RBM network with m hidden neurons with parame-
ters bounded in magnitude by C. Let p > 0. Then there exists a hardplus RBM network with
≤ 2m2p log(mp) + m hidden neurons and with parameters bounded in magnitude by C which
computes the same function  up to an approximation error of 1/p.

Note that if p and m are polynomial functions of n  then the simulation produces hardplus RBM
networks whose size is also polynomial in n.

2.5 Thresholded Networks and Boolean Functions

Many relevant results and proof techniques concerning the properties of neural networks focus on
the case where the output is thresholded to compute a Boolean function (i.e. a binary classiﬁcation).
In this section we deﬁne some key concepts regarding output thresholding  and present some basic
propositions that demonstrate how hardness results for computing Boolean functions via threshold-
ing yield analogous hardness results for computing certain real-valued functions.
We say that a real-valued function g represents a Boolean function f with margin δ if for all x g
satisﬁes |g(x)| ≥ δ and thresh(g(x)) = f (x)  where thresh is the 0/1 valued threshold function
deﬁned by:

(cid:26) 1

0

thresh(a) =

: a ≥ 0
: a < 0

We deﬁne a thresholded neural network (a distinct concept from a “threshold network”  which is
a neural network with hidden neurons whose activation function is thresh) to be a neural network
whose output is a single real value  which is followed by an application of the threshold function.
Such a network will be said to compute a given Boolean function f with margin δ (similar to the
concept of “separation” from Maass et al. (1994)) if the real valued input g to the ﬁnal threshold
represents f according to the above deﬁnition.
While the output of a thresholded RBM network does not correspond to the log probability of an
RBM  the following observation spells out how we can use thresholded RBM networks to establish
lower bounds on the size of an RBM network required to compute certain simple functions (i.e. 
real-valued functions that represent certain Boolean functions):

4

Proposition 4. If an RBN network of size m can compute a real-valued function g which represents
f with margin δ  then there exists a thresholded RBM network that computes f with margin δ.

This statement clearly holds if we replace each instance of “RBM network” with “hardplus RBM
network” above.
Using Theorem 3 we can prove a more interesting result which states that any lower bound result
for thresholded hardplus RBMs implies a somewhat weaker lower bound result for standard RBM
networks:
Proposition 5. If an RBM network of size ≤ m with parameters bounded in magnitude by C com-
putes a function which represents a Boolean function f with margin δ  then there exists a thresholded
hardplus RBM network of size ≤ 4m2 log(2m/δ)/δ + m with parameters bounded in magnitude by
C (C can be ∞) that computes f (x) with margin δ/2

This proposition implies that any exponential lower bound on the size of a thresholded hardplus
RBM network will yield an exponential lower bound for (softplus) RBM networks that compute
functions of the given form  provided that the margin δ is bounded from below by some function of
the form 1/poly(n).
Intuitively  if f is a Boolean function and no RBM network of size m can compute a real-valued
function that represents f (with a margin δ)  this means that no RBM of size m can represent any
distribution where the log probability of each member of {x|f (x) = 1} is at least 2δ higher than
each member of {x|f (x) = 0}. In other words  RBMs of this size cannot generate any distribution
where the two “classes” implied by f are separated in log probability by more than 2δ.

2.6 RBM networks versus standard neural networks

Viewing the RBM log probability function through the formalism of neural networks (or real-valued
circuits) allows us to make use of known results for general neural networks  and helps highlight
important differences between what an RBM can effectively “compute” (via its log probability) and
what a standard neural network can compute.
There is a rich literature studying the complexity of various forms of neural networks  with diverse
classes of activation functions  e.g.  Maass (1992); Maass et al. (1994); Hajnal et al. (1993). RBM
networks are distinguished from these  primarily because they have a single hidden layer and because
the upper level weights are constrained to be 1.
For some activation functions this restriction may not be signiﬁcant  but for soft/hard-plus neurons 
whose output is always positive  it makes particular computations much more awkward (or perhaps
impossible) to express efﬁciently. Intuitively  the jth softplus neuron acts as a “feature detector” 
which when “activated” by an input s.t. x(cid:62)wj + bj (cid:29) 0  can only contribute positively to the log
probability of x  according to an (asymptotically) afﬁne function of x given by that neuron’s input.
For example  it is easy to design an RBM network that can (approximately) output 1 for input x = (cid:126)0
and 0 otherwise (i.e.  have a single hidden neuron with weights −M 1 for a large M and bias b such
that soft(b) = 1)  but it is not immediately obvious how an RBM network could efﬁciently compute
(or approximate) the function which is 1 on all inputs except x = (cid:126)0  and 0 otherwise (it turns out
that a non-obvious construction exists for m = n). By comparison  standard threshold networks
only requires 1 hidden neuron to compute such a function.
In fact  it is easy to show1 that without the constraint on upper level weights  an RBM network
would be  up to a linear factor  at least as efﬁcient at representing real-valued functions as a neural
network with 1 hidden layer of threshold neurons. From this  and from Theorem 4.1 of Maass et al.
(1994)  it follows that a thresholded RBM network is  up to a polynomial increase in size  at least as
efﬁcient at computing Boolean functions as 1-hidden layer neural networks with any “sigmoid-like”
activation function2  and polynomially bounded weights.

1To see this  note that we could use 2 softplus neurons to simulate a single neuron with a “sigmoid-like”
activation function (i.e.  by setting the weights that connect them to the output neuron to have opposite signs).
Then  by increasing the size of the weights so the sigmoid saturates in both directions for all inputs  we could
simulate a threshold function arbitrarily well  thus allowing the network to compute any function computable
by a one hidden layer threshold network while only using only twice as many neurons.

2This is a broad class and includes the standard logistic sigmoid. See Maass et al. (1994) for a precise

technical deﬁnition

5

Figure 2: Left: The functions computed by the 5 building-blocks as constructed by Theorem 7 when applied to
the PARITY function for n = 5. Right: The output total of the hardplus RBM network constructed in Theorem
7. The dotted lines indicate the target 0 and 1 values. Note: For purposes of illustration we have extended the
function outputs over all real-values of X in the obvious way.

2.7 Simulating hardplus RBM networks by a one-hidden-layer threshold network

Here we provide a natural simulation of hardplus RBM networks by threshold networks with one
hidden layer. Because this is an efﬁcient (polynomial) and exact simulation  it implies that a hardplus
RBM network can be no more powerful than a threshold network with one hidden layer  for which
several lower bound results are already known.
Theorem 6. Let f be a real-valued function computed by a hardplus RBM network of size m.
Then f can be computed by a single hidden layer threshold network  of size mn. Furthermore  if
the weights of the RBM network have magnitude at most C  then the weights of the corresponding
threshold network have magnitude at most (n + 1)C.

3 n2 + 1-sized RBM networks can compute any symmetric function

In this section we present perhaps the most surprising results of this paper: a construction of an
n2-sized RBM network (or hardplus RBM network) for computing any given symmetric function of
x. Here  a symmetric function is deﬁned as any real-valued function whose output depends only on
i xi. A well-known example of

the number of 1-bits in the input x. This quantity is denoted X ≡(cid:80)

a symmetric function is PARITY.
Symmetric functions are already known3 to be computable by single hidden layer threshold networks
(Hajnal et al.  1993) with m = n. Meanwhile (qualiﬁed) exponential lower bounds on m exist for
functions which are only slightly more complicated (Hajnal et al.  1993; Forster  2002).
Given that hardplus RBM networks appear to be strictly less expressive than such threshold networks
(as discussed in Section 2.6)  it is surprising that they can nonetheless efﬁciently compute functions
that test the limits of what those networks can compute efﬁciently.

Theorem 7. Let f : {0  1}n → R be a symmetric function deﬁned by f (x) = tk for(cid:80)

i xi = k.
Then (i) there exists a hardplus RBM network  of size n2 + 1  and with weights polynomial in n and
t1  . . .   tk that computes f exactly  and (ii) for every  there is a softplus RBM network of size n2 +1 
and with weights polynomial in n  t0  . . .   tn and log(1/) that computes f within an additive error
.

The high level idea of this construction is as follows. Our hardplus RBM network consists of n
“building blocks”  each composed of n hardplus neurons  plus one additional hardplus neuron  for
a total size of m = n2 + 1. Each of these building blocks is designed to compute a function of the
form:

max(0  γX(e − X))

for parameters γ > 0 and e > 0. This function  examples of which are illustrated in Figure 2  is
quadratic from X = 0 to X = e and is 0 otherwise.
The main technical challenge is then to choose the parameters of these building blocks so that the
sum of n of these “rectiﬁed quadratics”  plus the output of the extra hardplus neuron (which handles

3The construction in Hajnal et al. (1993) is only given for Boolean-valued symmetric functions but can be

generalized easily.

6

012345050100150OutputfromBuildingBlocksX012345−15−10−50NetworkOutputXthe X = 0 case)  yields a function that matches f  up to a additive constant (which we then ﬁx by
setting the bias B of the output neuron). This would be easy if we could compute more general
rectiﬁed quadratics of the form max(0  γ(X − g)(e − X))  since we could just take g = k − 1/2
and e = k + 1/2 for each possible value k of X. But the requirement that g = 0 makes this more
difﬁcult since signiﬁcant overlap between non-zero regions of these functions will be unavoidable.
Further complicating the situation is the fact that we cannot exploit linear cancelations due to the
restriction on the RBM network’s second layer weights. Figure 2 depicts an example of the solution
to this problem as given in our proof of Theorem 7.
Note that this construction is considerably more complex than the well-known construction used for
computing symmetric functions with 1 hidden layer threshold networks Hajnal et al. (1993). While
we cannot prove that ours is the most efﬁcient possible construction RBM networks  we can prove
that a construction directly analogous to the one used for 1 hidden layer threshold networks—where
each individual neuron computes a symmetric function—cannot possibly work for RBM networks.
To see this  ﬁrst observe that any neuron that computes a symmetric function must compute a func-
tion of the form g(βX + b)  where g is the activation function and β is some scalar. Then noting that
both soft(y) and hard(y) are convex functions of y  and that the composition of an afﬁne function
and a convex function is convex  we have that each neuron computes a convex function of X. Then
because the positive sum of convex functions is convex  the output of the RBM network (which is
the unweighted sum of the output of its neurons  plus a constant) is itself convex in X. Thus the
symmetric functions computable by such RBM networks must be convex in X  a severe restriction
which rules out most examples.
4 Lower bounds on the size of RBM networks for certain functions
4.1 Existential results

In this section we prove a result which establishes the existence of functions which cannot be com-
puted by RBM networks that are not exponentially large.
Instead of identifying non-representable distributions as lying in the complement of some low-
dimensional manifold (as was done previously)  we will establish the existence of Boolean functions
which cannot be represented with a sufﬁciently large margin by the output of any sub-exponentially
large RBM network. However  this result  like previous such existential results  will say nothing
about what these Boolean functions actually look like.
To prove this result  we will make use of Proposition 5 and a classical result of Muroga (1971) which
allows us to discretize the incoming weights of a threshold neuron (without changing the function
it computes)  thus allowing us to bound the number of possible Boolean functions computable by
1-layer threshold networks of size m.
Theorem 8. Let Fm δ n represent the set of those Boolean functions on {0  1}n that can be computed
by a thresholded RBM network of size m with margin δ. Then  there exists a ﬁxed number K such
that 

(cid:12)(cid:12)Fm δ n

(cid:12)(cid:12) ≤ 2poly(s m n δ)  where

In particular  when m2 ≤ δ2αn  for any constant α < 1/2  the ratio of the size of the set Fm δ n to
the total number of Boolean functions on {0  1}n (which is 22n)  rapidly converges to zero with n.

(cid:18) 2m

(cid:19)

δ

4m2n

δ

s(m  δ  n) =

log

+ m.

4.2 Qualiﬁed lower bound results for the IP function

While interesting  existential results such as the one above does not give us a clear picture of what
a particular hard-to-compute function for RBM networks might look like. Perhaps these functions
will resemble purely random maps without any interesting structure. Perhaps they will consist only
of functions that require exponential time to compute on a Turing machine  or even worse  ones that
are non-computable. In such cases  not being able to compute such functions would not constitute a
meaningful limitation on the expressive efﬁciency of RBM networks.
In this sub-section we present strong evidence that this is not the case by exhibiting a simple Boolean
function that provably requires exponentially many neurons to be computed by a thresholded RBM
network  provided that the margin is not allowed to be exponentially smaller than the weights. Prior
to these results  there was no formal separation between the kinds of unnormalized log-likelihoods
realizable by polynomially sized RBMs  and the class of functions computable efﬁciently by almost
any reasonable model of computation  such as arbitrarily deep Boolean circuits.

7

The Boolean function we will consider is the well-known “inner product mod 2” function  denoted
IP (x)  which is deﬁned as the parity of the the inner product of the ﬁrst half of x with the second
half (we assume for convenience that n is even). This function can be thought of as a strictly harder
to compute version of PARITY (since PARITY is trivially reducible to it)  which as we saw in
Section 7  can be efﬁciently computed by thresholded RBM network (indeed  an RBM network can
efﬁciently compute any possible real-valued representation of PARITY). Intuively  IP (x) should be
harder than PARITY  since it involves an extra “stage” or “layer” of sequential computation  and our
formal results with RBMs agree with this intuition.
There are many computational problems that IP can be reduced to  so showing that RBM networks
cannot compute IP thus proves that RBMs cannot efﬁciently model a wide range of distributions
whose unnormalized log-likelihoods are sufﬁciently complex in a computational sense. Examples
of such log-likelihoods include ones given by the multiplication of binary-represented integers  or
the evaluation of the connectivity of an encoded graph. For other examples  see see Corollary 3.5 of
Hajnal et al. (1993).
Using the simulation of hardplus RBM networks by 1 hidden layer threshold networks (Theorem
6)  and Proposition 5  and an existing result about the hardness of computing IP by 1 hidden layer
thresholded networks of bounded weights due to Hajnal et al. (1993)  we can prove the following
basic result:

Theorem 9. If m < min
then no RBM network of size
m  whose weights are bounded in magnitude by C  can compute a function which represents n-
dimensional IP with margin δ. In particular  for C and 1/δ bounded by polynomials in n  for n
sufﬁciently large  this condition is satisﬁed whenever m < 2(1/9−)n for some  > 0.

4C log(2/δ)   2n/9 3

4C

(cid:26)

C   2n/6(cid:113) δ

2n/3

(cid:27)

(cid:113) δ

Translating the deﬁnitions  this results says the following about the limitations of efﬁcient repre-
sentation by RBMs: Unless either the weights  or the number units of an RBM are exponentially
large in n  an RBM cannot capture any distribution that has the property that x’s s.t. IP(x) = 1 are
signiﬁcantly more probable than the remaining x’s.
While the above theorem is easy to prove from known results and the simulation/hardness results
given in previous sections  by generalizing the techniques used in Hajnal et al. (1993)  we can (with
much more effort) derive a stronger result. This gives an improved bound on m and lets us partially
relax the magnitude bound on parameters so that they can be arbitrarily negative:
2·max{log 2 nC+log 2} · 2n/4  then no RBM network of size m  whose weights
Theorem 10. If m <
are upper bounded in value by C  can compute a function which represents n-dimensional IP with
margin δ. In particular  for C and 1/δ bounded by polynomials in n  for n sufﬁciently large  this
condition is satisﬁed whenever m < 2(1/4−)n for some  > 0.
The general theorem we use to prove this second result (Theorem 17 in the Appendix) requires only
that the neural network have 1 hidden layer of neurons with activation functions that are monotonic
and contribute to the top neuron (after multiplication by the outgoing weight) a quantity which can
be bounded by a certain exponentially growing function of n (that also depends on δ). Thus this
technique can be applied to produce lower bounds for much more general types of neural networks 
and thus may be independently interesting.

δ

5 Conclusions and Future Work
In this paper we signiﬁcantly advanced the theoretical understanding of the representational efﬁ-
ciency of RBMs. We treated the RBM’s unnormalized log likelihood as a neural network which
allowed us to relate an RBM’s representational efﬁciency to that of threshold networks  which are
much better understood. We showed that  quite suprisingly  RBMs can efﬁciently represent distribu-
tions that are given by symmetric functions such as PARITY  but cannot efﬁciently represent distri-
butions which are slightly more complicated  assuming an exponential bound on the weights. This
provides rigorous justiﬁcation for the use of potentially more expressive/deeper generative models.
Going forward  some promising research directions and open problems include characterizing the
expressive power of Deep Boltzmann Machines and more general Boltzmann machines  and proving
an exponential lower bound for some speciﬁc distribution without any qualiﬁcations on the weights.

Acknowledgments
This research was supported by NSERC. JM is supported by a Google Fellowship; AC by a Ra-
manujan Fellowship of the DST  India.

8

References
Aaron Courville  James Bergstra  and Yoshua Bengio. Unsupervised models of images by spike-
and-slab RBMs. In Proceedings of the 28th International Conference on Machine Learning  pages
952–960  2011.

Maria Anglica Cueto  Jason Morton  and Bernd Sturmfels. Geometry of the Restricted Boltzmann

Machine. arxiv:0908.4425v1  2009.

J. Forster. A linear lower bound on the unbounded error probabilistic communication complexity. J.

Comput. Syst. Sci.  65(4):612–625  2002.

Yoav Freund and David Haussler. Unsupervised learning of distributions on binary vectors using

two layer networks  1994.

A. Hajnal  W. Maass  P. Pudl´ak  M. Szegedy  and G. Tur´an. Threshold circuits of bounded depth. J.

Comput. System. Sci.  46:129–154  1993.

G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science  313(5786):504–507  2006. ISSN 1095-9203.

Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural Com-

putation  14:2002  2000.

Geoffrey E. Hinton  Simon Osindero  Max Welling  and Yee Whye Teh. Unsupervised discovery of
nonlinear structure using contrastive backpropagation. Cognitive Science  30(4):725–731  2006.
Nicolas Le Roux and Yoshua Bengio. Representational power of Restricted Boltzmann Machines

and deep belief networks. Neural Computation  20(6):1631–1649  2008.

Philip Long and Rocco Servedio. Restricted Boltzmann Machines are hard to approximately evalu-
ate or simulate. In Proceedings of the 27th International Conference on Machine Learning  pages
952–960  2010.

Wolfgang Maass. Bounds for the computational power and learning complexity of analog neural
nets (extended abstract). In Proc. of the 25th ACM Symp. Theory of Computing  pages 335–344 
1992.

Wolfgang Maass  Georg Schnitger  and Eduardo D. Sontag. A comparison of the computational
power of sigmoid and boolean threshold circuits. In Theoretical Advances in Neural Computation
and Learning  pages 127–151. Kluwer  1994.

Benjamin M. Marlin  Kevin Swersky  Bo Chen  and Nando de Freitas.

Inductive principles for
Restricted Boltzmann Machine learning. Journal of Machine Learning Research - Proceedings
Track  9:509–516  2010.

G. Montufar  J. Rauh  and N. Ay. Expressive power and approximation errors of Restricted Boltz-

mann Machines. In Advances in Neural Information Processing Systems  2011.

Guido Montufar and Nihat Ay. Reﬁnements of universal approximation results for deep belief net-

works and Restricted Boltzmann Machines. Neural Comput.  23(5):1306–1319  May 2011.

Saburo Muroga. Threshold logic and its applications. Wiley  1971.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep boltzmann machines. Journal of Machine

Learning Research - Proceedings Track  5:448–455  2009.

Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of Deep Belief Networks.
In Andrew McCallum and Sam Roweis  editors  Proceedings of the 25th Annual International
Conference on Machine Learning (ICML 2008)  pages 872–879. Omnipress  2008.

Yichuan Tang and Ilya Sutskever. Data normalization in the learning of Restricted Boltzmann
Machines. Technical Report UTML-TR-11-2  Department of Computer Science  University of
Toronto  2011.

9

A Appendix

A.1 Free-energy derivation

The following is a derivation of the well-known formula for the free-energy of an RBM. This
tractable form is made possible by the bipartite interaction structure of the RBM’s units:

(cid:88)

h
1
Zθ

1
Zθ

1
Zθ

1
Zθ

=

=

=

=

p(x) =

(cid:62)
exp(x(cid:62)W h + c(cid:62)x + b

h)

1
Zθ
exp(c(cid:62)x)

(cid:89)

j

hj∈{0 1}

(cid:88)
(cid:88)
(cid:88)

j

(cid:88)

hj∈{0 1}

exp(c(cid:62)x +

log[1 + exp(x(cid:62)[W ]j + bj)])

exp(x(cid:62)[W ]jhj + bjhj)

exp(c(cid:62)x) exp(

(log

exp(x(cid:62)[W ]jhj + bjhj)))

j

exp(−Fθ(x))

A.2 Proofs for Section 2.4

We begin with a useful technical result:
Proposition 11. For arbitrary y ∈ R the following basic facts for the softplus function hold:

y − soft(y) = − soft(−y)

soft(y) ≤ exp(y)

Proof. The ﬁrst fact follows from:

y− soft(y) = log(exp(y)) − log(1 + exp(y)) = log

(cid:18)

(cid:19)

= log

1

exp(−y) + 1

1 + exp(y)
= − log(1 + exp(y)) = − soft(−y)

(cid:18) exp(y)

(cid:19)

To prove the second fact  we will show that the function f (y) = exp(y) − soft(y) is positive. Note
that f tends to 0 as y goes to −∞ since both exp(y) and soft(y) do. It remains to show that f is
monotonically increasing  which we establish by showing that its derivative is positive:

f(cid:48)(y) = exp(y) −

1

1 + exp(−y)

> 0

⇔ exp(y)(1 + exp(−y)) − 1 + exp(−y)
1 + exp(−y)
⇔ exp(y) + 1 − 1 > 0 ⇔ exp(y) > 0

> 0

Proof of Lemma 2. Consider a single neuron in the RBM network and the corresponding neuron
in the hardplus RBM network  whose net-input are given by y = w(cid:62)x + b.
For each x  there are two cases for y. If y ≥ 0  we have by hypothesis that y ≥ C  and so:

| hard(y) − soft(y)| = |y − soft(y)| = | − soft(−y)| = soft(−y)

And if y < 0  we have by hypothesis that y ≤ −C and so:

| hard(y) − soft(y)| = |0 − soft(y)| = soft(y)

≤ exp(y) ≤ exp(−C)

≤ exp(−y) ≤ exp(−C)

10

Thus  each corresponding pair of neurons computes the same function up to an error bounded by
exp(−C). From this it is easy to show that the entire circuits compute the same function  up to an
error bounded by m exp(−C)  as required.

Proof of Theorem 3. Suppose we have a softplus RBM network with a number of hidden neurons
given by m. To simulate this with a hardplus RBM network we will replace each neuron with a group
of hardplus neurons with weights and biases chosen so that the sum of their outputs approximates the
output of the original softplus neuron  to within a maximum error of 1/p where p is some constant
> 0.
First we describe the construction for the simulation of a single softplus neurons by a group of
hardplus neurons.
Let g be a positive integer and a > 0. We will deﬁne these more precisely later  but for what follows
their precise value is not important.
At a high level  this construction works by approximating soft(y)  where y is the input to the neuron 
by a piece-wise linear function expressed as the sum of a number of hardplus functions  whose
“corners” all lie inside [−a  a]. Outside this range of values  we use the fact that soft(y) converges
exponentially fast (in a) to 0 on the left  and y on the right (which can both be trivially computed by
hardplus functions).
Formally  for i = 1  2  ...  g  g + 1  let:

For i = 1  2  ...  g  let:

qi = (i − 1)

− a

2a
g

νi =

soft(qi+1) − soft(qi)

qi+1 − qi

and also let ν0 = 0 and νg+1 = 1. Finally  for i = 1  2  ...  g  g + 1  let:

ηi = νi − νi−1

With these deﬁnitions it is straightforward to show that 1 ≥ νi > 0  νi > νi−1 and consequently
0 < ηi < 1 for each i. It is also easy to show that qi > qi−1  q0 = −a and qg+1 = a.
For i = 1  2  ...  g  g + 1  we will set the weight vector wi and bias bi of the i-th hardplus neuron in
our group so that the neuron outputs hard(ηi(y − qi)). This is accomplished by taking wi = ηiw
and bi = ηi(b − qi)  where w and b (without the subscripts)  are the weight vector and bias of the
original softplus neuron.
Note that since |ηi| ≤ 1 we have that the weights of these hard neurons are smaller in magnitude
than the weights of the original soft neuron and thus bounded by C as required.
The total output (sum) for this group is:

g+1(cid:88)

T (y) =

hard(ηi(y − qi))

i=1

We will now bound the approximation error |T (y) − soft(y)| of our single neuron simulation.
Note that for a given y we have that the i-th hardplus neuron in the group has a non-negative input
iff y ≥ qi. Thus for y < −a all of the neurons have a negative input. And for y ≥ −a   if we take
j to be the largest index i s.t. qi ≤ y  then each neuron from i = 1 to i = j will have positive input
and each neuron from i = j + 1 to i = g + 1 will have negative input.
Consider the case that y < −a. Since the input to each neuron is negative  they each output 0 and
thus T (y) = 0. This results in an approximation error ≤ exp(−a):

|T (y) − soft(y)| = |0 − soft(y)| = soft(y) < soft(−a) ≤ exp(−a)

11

Next  consider the case that y ≥ −a  and let j be as given above. In such a case we have:

T (y) =

=

i=1

g+1(cid:88)
j(cid:88)
j(cid:88)

i=1

i=1

= y

j(cid:88)

i=1

(νi − νi−1)qi

hard(ηi(y − qi)) =

ηi(y − qi) + 0

(νi − νi−1)(y − qi)

(νi − νi−1) − j(cid:88)
j−1(cid:88)

i=1

j−1(cid:88)

i=1

= yνj − yν0 − νjqj +

νi(qi+1 − qi) + ν0q1

= νj(y − qj) +
= νj(y − qj) + soft(qj) − soft(q1)

(soft(qi+1) − soft(qi))

i=1

For y ≤ a we note that νj(y − qj) + soft(qj) is a secant approximation to soft(y) generated by the
secant from qj to qj+1 and upperbounds soft(y) for y ∈ [qj  qj+1]. Thus a crude bound on the error
is soft(qj+1) − soft(qj)  which only makes use of the fact that soft(y) is monotonic. Then because
the slope (derivative) of soft(y) is σ(y) = 1/(1 + exp(−y)) < 1  we can further (crudely) bound
this by qj+1 − qj. Thus the approximation error at such y’s may be bounded as:

|T (y)− soft(y)| = |(νj(y − qj) + soft(qj) − soft(q1)) − soft(y)|

≤ max{|νj(y − qj) + soft(qj) − soft(y)|  soft(q1)}
  exp(−a)
≤ max{qj+1 − qj  exp(−a)} = max

(cid:27)

(cid:26) 2a

g

where we have also used soft(q1) = soft(−a) ≤ exp(−a).
For the case y > a  all qi > y and the largest index j such that qj ≤ y is j = g + 1. So
νj(y − qj) + soft(qj) − soft(q1) = y − a + soft(a) − soft(−a) = y. Thus the approximation error
at such y’s is:

|y − soft(y)| = | − soft(−y)| = soft(−y) ≤ soft(−a) ≤ exp(−a)

Having covered all cases for y we conclude that the general approximation error for a single softplus
neuron satisﬁes the following bound:

|y − soft(y)| ≤ max

  exp(−a)

(cid:26) 2a

g

(cid:27)

For a softplus RBM network with m neurons  our hardplus RBM neurons constructed by replacing
each neuron with a group of hardplus neurons as described above will require a total of m(g + 1)
neurons  and have an approximation error bounded by the sum of the individual approximation
errors  which is itself bounded by:

Taking a = log(mp)  g = (cid:100)2mpa(cid:101). This gives:
1
mp

(cid:100)2mpa(cid:101)  

m max

(cid:26) 2a

m max

  exp(−a)

(cid:26) 2a
(cid:27)

g

(cid:27)
(cid:26) 2a
(cid:26) 1
(cid:27)

2mpa
1
p

=

 

p

 

(cid:27)

1
mp
1
p

≤ m max

= max

Thus we see that with m(g + 1) = m((cid:100)2mp log(mp)(cid:101) + 1) ≤ 2m2p log(mp) + m neurons we
can produce a hardplus RBM network which approximates the output of our softplus RBM network
with error bounded by 1/p.

12

Remark 12. Note that the construction used in the above lemma is likely far from optimal  as the
placement of the qi’s could be done more carefully. Also  the error bound we proved is crude and
does not make strong use of the properties of the softplus function. Nonetheless  it seems good
enough for our purposes.

A.3 Proofs for Section 2.5

Proof of Proposition 5. Suppose that there is an RBM network of size m with weights bounded in
magnitude by C computes a function g which represent f with margin δ.
Then taking p = 2/δ and applying Theorem 3 we have that there exists an hardplus RBM network
of size 4m2 log(2m/δ)/δ + m which computes a function g(cid:48) s.t. |g(x) − g(cid:48)(x)| ≤ 1/p = δ/2 for
all x.
Note that f (x) = 1 ⇒ thresh(g(x)) = 1 ⇒ g(x) ≥ δ ⇒ g(cid:48)(x) ≥ δ − δ/2 = δ/2 and similarly 
f (x) = 0 ⇒ thresh(g(x)) = 0 ⇒ g(x) ≤ −δ ⇒ g(cid:48)(x) ≤ −δ + δ/2 = −δ/2. Thus we conclude
that g(cid:48) represents f with margin δ/2.

A.4 Proofs for Section 2.7

Proof of Theorem 6. Let f be a Boolean function on n variables computed by a size s hardplus RBM
network  with parameters (W  b  d) . We will ﬁrst construct a three layer hybrid Boolean/threshold
circuit/network where the output gate is a simple weighted sum  the middle layer consists of AND
gates  and the bottom hidden layer consists of threshold neurons. There will be n·m AND gates  one
for every i ∈ [n] and j ∈ [m]. The (i  j)th AND gate will have inputs: (1) xi and (2) (x(cid:62)[W ]j ≥ bj).
The weights going from the (i  j)th AND gate to the output will be given by [W ]i j. It is not hard to
see that our three layer netork computes the same Boolean function as the original hardplus RBM
network.
In order to obtain a single hidden layer threshold network  we replace each sub-network rooted at
an AND gate of the middle layer by a single threshold neuron. Consider a general sub-network
i=1 aixi ≥ b).
Let Q be some number greater than the sum of all the ai’s. We replace this sub-network by a single
i=1 aixi + Qxj ≥ b + Q). Note that if the input x is such that
i aixi + Qαj will be at least b + Q  so the threshold gate will
i aixi < b  then even if xj = 1 
i aixi is never greater than

consisting of an AND of: (1) a variable xj and (2) a threshold neuron computing ((cid:80)n
threshold gate that computes ((cid:80)n
(cid:80)
i aixi ≥ b and xj = 1  then(cid:80)
output 1. In all other cases  the threshold will output zero. (If(cid:80)
the sum will still be less than Q + b. Similarly  if xj = 0  then since(cid:80)
(cid:80)
i ai  the total sum will be less than Q ≤ (n + 1)C.)

A.5 Proof of Theorem 7

Proof. We will ﬁrst describe how to construct a hardplus RBM network which satisﬁes the properties
required for part (i). It will be composed of n special groups of hardplus neurons (which are deﬁned
and discussed below)  and one additional one we call the “zero-neuron”  which will be deﬁned later.

Deﬁnition 13 A “building block” is a group of n hardplus neurons  parameterized by the scalars γ
and e  where the weight vector w ∈ Rn between the i-th neuron in the group and the input layer is
given by wi = M − γ and wj = −γ for j (cid:54)= i and the bias will be given by b = γe − M  where M
is a constant chosen so that M > γe.
For a given x  the input to the i-th neuron of a particular building block is given by:

(cid:88)

j(cid:54)=i

13

n(cid:88)

j=1

wjxj + b = wixi +

wjxj + b

= (M − γ)xi − γ(X − xi) + γe − M
= γ(e − X) − M (1 − xi)

When xi = 0  this is γ(e − X) − M < 0  and so the neuron will output 0 (by deﬁnition of the
hardplus function). On the other hand  when xi = 1  the input to the neuron will be γ(e − X) and
thus the output will be max(0  γ(e − X)).
In general  we have that the output will be given by:

xi max(0  γ(e − X))

From this it follows that the combined output from the neurons in the building block is:

n(cid:88)

(xi max(0  γ(e − X))) = max(0  γ(e − X))

xi

n(cid:88)

i=1

= max(0  γ(e − X))X = max(0  γX(e − X))

i=1

Note that whenever X is positive  the output is a concave quadratic function in X  with zeros at
X = 0 and X = e  and maximized at X = e/2  with value γe2/4.
Next we show how the parameters of the n building blocks used in our construction can be set to
produce a hardplus RBM network with the desired output.

First  deﬁne d to be any number greater than or equal to 2n2(cid:80)

Indexing the building blocks by j for 1 ≤ j ≤ n we deﬁne their respective parameters γj  ej as
follows:

j |tj|.

γn =

tn + d

n2

 

γj =

en = 2n 

ej =

2
γj

tj + d

(cid:18) tj + d
j2 − tj+1 + d

(j + 1)2
− tj+1 + d
j + 1

j

(cid:19)

where we have assumed that γj (cid:54)= 0 (which will be established  along with some other properties of
these deﬁnitions  in the next claim).

Claim 1. For all j  1 ≤ j ≤ n  (i) γj > 0 and (ii) for all j  1 ≤ j ≤ n − 1  j ≤ ej ≤ j + 1.

2n2(cid:80)

Proof of Claim 1. Part (i): For j = n  by deﬁnition we know that γn = tn+d

j |tj| > |tn|  the numerator will be positive and therefore γn will be positive.

n2 . For d ≥

For j < n  we have:

j2 >

tj+1 + d
(j + 1)2

γj > 0
⇔ tj + d
⇔ (j + 1)2(tj + d) > j2(tj+1 + d)
⇔ d((j + 1)2 − j2) > j2tj+1 − (j + 1)2tj
⇔ d >

j2tj+1 − (j + 1)2tj

2j + 1

The right side of the above inequality is less than or equal to (j+1)2(|tj+1|+|tj|)

which is strictly upper bounded by 2n2(cid:80)

≤ (j+1)(|tj+1|+|tj|)
j |tj|  and thus by d. So it follows that γj > 0 as needed.

2j+1

Part (ii):

14

j ≤ ej =

⇔ jγj ≤ 2

⇔ tj + d

j

− tj+1 + d
j + 1

(cid:19)
(cid:19)
(cid:18) tj + d

2
γj

(cid:18) tj + d
(cid:18) tj + d
j
− tj+1 + d
j + 1
(j + 1)2 ≤ 2
− j(tj+1 + d)
(j + 1)2 ≤ tj + d

j

j

(cid:19)

− tj+1 + d
j + 1

− 2

tj+1 + d

⇔ − j(tj+1 + d)
⇔ − (tj+1 + d)j2 ≤ (tj + d)(j + 1)2 − 2(tj+1 + d)j(j + 1)
⇔ d(j2 − 2j(j + 1) + (j + 1)2) ≥ −j2tj+1 + 2j(j + 1)tj+1 − (j + 1)2tj
⇔ d ≥ −j2tj+1 + 2j(j + 1)tj+1 − (j + 1)2tj

j + 1

j

where we have used j2 − 2j(j + 1) + (j + 1)2 = (j − (j + 1))2 = 12 = 1 at the last line. Thus it
sufﬁces to make d large enough to ensure that j ≤ ej. For our choice of d  this will be true.
For the upper bound we have:

= ej ≤ j + 1

≤ (j + 1)γj =

(j + 1)(tj + d)

j2

− tj+1 + d
j + 1

2
γj
⇔ 2

(cid:19)
(cid:19)

(cid:18) tj + d
(cid:18) tj + d

j

j
tj + d

j

− tj+1 + d
j + 1
− tj+1 + d
j + 1
− tj+1 + d
j + 1

j2

≤ (j + 1)(tj + d)

⇔ 2
⇔ 2(tj + d)j(j + 1) − (tj+1 + d)j2 ≤ (j + 1)2(tj + d)
⇔ −(d − tj+1)
⇔ − j2(d + tj+1) + 2j(j + 1)(d + tj) ≤ (j + 1)2(d + tj)
⇔ d(j2 − 2j(j + 1) + (j + 1)2)

≤ (j + 1)

(d + tj)

(d + tj)

j + 1

+ 2

j2

j

≥ −j2tj+1 + 2j(j + 1)tj − (j + 1)2tj

⇔ d ≥ −j2tj+1 + 2j(j + 1)tj − (j + 1)2tj

where we have used j2 − 2j(j + 1) + (j + 1)2 = 1 at the last line. Again  for our choice of d the
above inequality is satisﬁed.

Finally  deﬁne M to be any number greater than max(t0 + d  maxi{γiei}).
In addition to the n building blocks  our hardplus RBM will include an addition unit that we will call
the zero-neuron  which handles x = 0. The zero-neuron will have weights w deﬁned by wi = −M
for each i  and b = t0 + d.
Finally  the output bias B of our hardplus RBM network will be set to −d.
The total output of the network is simply the sum of the outputs of the n different building blocks 
the zero neuron  and constant bias −d.
To show part (i) of the theorem we want to prove that for all k  whenever X = k  our circuit outputs
the value tk.
We make the following deﬁnitions:

ak ≡ − n(cid:88)

γj

bk ≡ n(cid:88)

γjej

j=k

j=k

15

Claim 2.

ak =

−(tk + d)

k2

bk =

2(tk + d)

k

bk = −2kak

This claim is self-evidently true by examining basic deﬁnitions of γj and ej and realizing that ak
and bk are telescoping sums.
Given these facts  we can prove the following:

Claim 3. For all k  1 ≤ k ≤ n  when X = k the sum of the outputs of all the n building blocks is
given by tk + d.

the (γn  en)-block computes max(0  γnX(en − X)) =
Proof of Claim 3. For X = n 
max(0 −γnX 2 +γnenX). By the deﬁnition of en  n ≤ en  and thus when X ≤ n  γnX(en−X) ≥
0. For all other building blocks (γj  ej)  j < n  since ej ≤ j + 1  this block outputs zero since
γjX(ej − X) is less than or equal to zero. Thus the sum of all of the building blocks when X = n
is just the output of the (γn  en)-block which is

γn · n(en − n) = −γn · n2 + γnen · n = −(tn + d) + 2(tn + d) = tn + d

as desired.
For X = k  1 ≤ k < n the argument is similar. For all building blocks j ≥ k  by Claim 1 we know
that ej ≥ j and therefore this block on X = k is nonnegative and therefore contributes to the sum.
On the other hand  for all building blocks j < k  by Claim 1 we know that ej ≤ j + 1 and therefore
this outputs 0 and so does not contribute to the sum.
Thus the sum of all of the building blocks is equal to the sum of the non-zero regions of the building
blocks j for j ≥ k. Since each of this is a quadratic function of X  it can written as a single quadratic
polynomial of the form akX 2 + bkX where ak and bk are deﬁned as before.
Plugging in the above expressions for ak and bk from Claim 2  we see that the value of this polyno-
mial at X = k is:

akk2 + bkk =

−(tk + d)

k2

k2 +

2(tk + d)

k

k = −(tk + d) + 2(tk + d) = tk + d

Finally  it remains to ensure that our hardplus RBM network outputs t0 for X = 0. Note that the
sum of the outputs of all n building blocks and the output bias is −d at X = 0. To correct this  we
set the incoming weights and the bias of the zero-neuron according to wi = −M for each i  and
b = t0 + d. When X = 0  this neuron will output t0 + d  making the total output of the network
−d + t0 + d = t0 as needed. Furthermore  note that the addition of the zero-neuron does not affect
the output of the network when X = k > 0 because the zero-neuron outputs 0 on all of these inputs
as long as M ≥ t0 + d.
This completes the proof of part (i) of the theorem and it remains to prove part (ii).
Observe that the size of the weights grows linearly in M and d  which follows directly from their def-
initions. And note that the magnitude of the input to each neuron is lower bounded by a positive lin-
ear function of M and d (a non-trivial fact which we will prove below). From these two observations
it follows that to achieve the condition that the magnitude of the input to each neuron is greater than
C(n) for some function C of n  the weights need to grow linearly with C. Noting that error bound
condition  ≤ (n2 + 1) exp(−C) in Lemma 2 can be rewritten as C ≤ log((n2 + 1)) + log(1/) 
from which part (ii) of the theorem then follows.
There are two cases where a hardplus neuron in building block j has a negative input. Either the
input is γj(ej − X) − M  or it is γj(ej − X) for X ≥ j + 1. In the ﬁrst case it is clear that as M
grows the net input becomes more negative since ej doesn’t depend on M at all.

16

The second case requires more work. First note that from its deﬁntion  ej can be rewritten as
2 (j+1)aj+1−jaj

. Then for any X ≥ j + 1 and j ≤ n − 1 we have:

γj

(cid:18)

(cid:19)

γj(ej − X) ≤ γj(ej − (j + 1))

2

(j + 1)aj+1 − jaj

− (j + 1)
= γj
= 2(j + 1)aj+1 − 2jaj − (j + 1)γj
= 2(j + 1)aj+1 − 2jaj − (j + 1)(aj+1 − aj)
= (j + 1)aj+1 − 2jaj + (j + 1)aj

γj

−(d − tj+1)

j + 1

(d + tj+1)

− (j + 1)

d + tj+1

+ 2

j

j2

−j2(d + tj+1) + 2j(j + 1)(d + tj) − (j + 1)2(d + tj)

−(j2 − 2j(j + 1) + (j + 1)2)d − j2tj + 2j(j + 1)tj

j2(j + 1)

−(j − (j + 1))2d − j2tj + 2j(j + 1)tj

j2(j + 1)

j2(j + 1)
−d − j2tj + 2j(j + 1)tj

j2(j + 1)

−d

j2(j + 1)

+

−j2tj + 2j(j + 1)tj

j2(j + 1)

=

=

=

=

=

=

So we see that as d increases  this bound guarantees that γj(ej − X) becomes more negative for
each X ≥ j + 1. Also note that for the special zero-neuron  for X ≥ 1 the net input will be
−M X + t0 + d ≤ −M + t0 + d  which will shrink as M grows.
For neurons belonging to building block j which have a positive valued input  we have that X < ej.
Note that for any X ≤ j and j < n we have:
γj(ej − X) ≥ γj(ej − j) = γj
= 2(j + 1)aj+1 − 2jaj − jγj
= 2(j + 1)aj+1 − 2jaj − j(aj+1 − aj)
= 2(j + 1)aj+1 − jaj − jaj+1

(j + 1)aj+1 − jaj

(cid:18)

(cid:19)

− j

γj

2

−(d + tj+1)

j + 1

= 2

(d + tj)

+

j

+ j

(d + tj+1)
(j + 1)2

−2j(j + 1)(d + tj+1) + (j + 1)2(d + tj) + j2(d + tj+1)

((j + 1)2 − 2j(j + 1) + j2)d + (j + 1)2tj − 2j(j + 1)tj+1 + j2tj+1

j(j + 1)2

(j + 1 − j)2d + (j + 1)2tj − 2j(j + 1)tj+1 + j2tj+1

j(j + 1)2

d + (j + 1)2tj − 2j(j + 1)tj+1 + j2tj+1

j(j + 1)2

d

j(j + 1)2 +

j(j + 1)2

(j + 1)2tj − 2j(j + 1)tj+1 + j2tj+1

j(j + 1)2

=

=

=

=

=

And for the case j = n  we have for X ≤ j that:
γj(ej − X) ≥ γj(ej − j) =

d + tn

n2

(2n − n) =

d
n

+

tn
n

17

So in all cases we see that as d increases  this bound guarantees that γj(ej − X) grows linearly.
Also note that for the special zero-neuron  the net input will be t0 + d for X = 0  which will grow
linearly as d increases.

A.6 Proofs for Section 4

A.6.1 Proof of Theorem 8

We ﬁrst state some basic facts which we need.
Fact 14 (Muroga (1971)). Let f : {0  1}n → {0  1} be a Boolean function computed by a threshold
neuron with arbitrary real incoming weights and bias. There exists a constant K and another
threshold neuron computing f  all of whose incoming weights and bias are integers with magnitude
at most 2Kn log n.

A direct consequence of the above fact is the following fact  by now folklore  whose simple proof
we present for the sake of completeness.
Fact 15. Let fn be the set of all Boolean functions on {0  1}n. For each 0 < α < 1  let fα n be the
subset of such Boolean functions that are computable by threshold networks with one hidden layer
with at most s neurons. Then  there exits a constant K such that 

(cid:12)(cid:12)fα n

(cid:12)(cid:12) ≤ 2K(n2s log n+s2 log s).

Proof. Let s be the number of hidden neurons in our threshold network. By using Fact 14 repeatedly
for each of the hidden neurons  we obtain another threshold network having still s hidden units com-
puting the same Boolean function such that the incoming weights and biases of all hidden neurons
is bounded by 2Kn log n. Finally applying Fact 14 to the output neuron  we convert it to a threshold
gate with parameters bounded by 2Ks log s. Henceforth  we count only the total number of Boolean
functions that can be computed by such threshold networks with integer weights. We do this by
establishing a simple upper bound on the total number of distinct such networks. Clearly  there are
at most 2Kn2 log n ways to choose the incoming weights of a given neuron in the hidden layer. There
are s incoming weights to choose for the output threshold  each of which is an integer of magnitude
at most 2Ks log s. Combining these observations  there are at most 2Ks·n2 log n × 2Ks2 log s distinct
networks. Hence  the total number of distinct Boolean functions that can be computed is at most
2K(n2s log n+s2 log s).

With these basic facts in hand  we prove below Theorem 8 using Proposition 5 and Theorem 6.

Proof of Theorem 8. Consider any thresholded RBM network with m hidden units that is computing
a n-dimensional Boolean function with margin δ. Using Proposition 5  we can obtain a thresholded
hardplus RBM network of size 4m2/δ · log(2m/δ) + m that computes the same Boolean function
as the thresholded original RBM network. Applying Theorem 6 and thresholding the output  we
obtain a thresholded network with 1 hidden layer of thresholds which is the same size and computes
the same Boolean function. This argument shows that the set of Boolean functions computed by
thresholded RBM networks of m hidden units and margin δ is a subset of the Boolean functions
computed by 1-hidden-layer threshold networks of size 4m2n/δ·log(2m/δ)+mn. Hence  invoking
Fact 15 establishes our theorem.

A.6.2 Proof of Theorem 9

Note that the theorems from Hajnal et al. (1993) assume integer weights  but this hypthosis can
be easily removed from their Theorem 3.6. In particular  Theorem 3.6 assumes nothing about the
lower weights  and as we will see  the integrality assumption on the top level weights can be easily
replaced with a margin condition.
First note that their Lemma 3.3 only uses the integrality of the upper weights to establish that the
margin must be ≥ 1. Otherwise it is easy to see that with a margin δ  Lemma 3.3 implies that
α -discriminator (α is the sum of the
a threshold neuron in a thresholded network of size m is a 2δ

18

absolute values of the 2nd-level weights in their notation). Then Theorem 3.6’s proof gives m ≥
δ2(1/3−)n for sufﬁciently large n (instead of just m ≥ 2(1/3−)n). A more precise bound that they
implictly prove in Theorem 3.6 is m ≥ 6δ2n/3
C .
Thus we have the following fact adapted from Hajnal et al. (1993):
Fact 16. For a neural network of size m with a single hidden layer of threshold neurons and weights
bounded by C that computes a function that represents IP with margin δ  we have m ≥ 6δ2n/3
C .

Proof of Theorem 9. By Proposition 5 it sufﬁces to show that no thresholded hardplus RBM network
of size ≤ 4m2 log(2m/δ)/δ + m with parameters bounded by C can compute IP with margin δ/2.
Well  suppose by contradiction that such a thresholded RBM network exists. Then by Theorem 6
there exists a single hidden layer threshold network of size ≤ 4m2n log(2m/δ)/δ+mn with weights
bounded in magnitude by (n + 1)C that computes the same function  i.e. one which represents IP
with margin δ/2.
Applying the above Fact we have 4m2n log(2m/δ)/δ + mn ≥ 3δ2n/3
(n+1)C .
It is simple to check that this bound is violated if m is bounded as in the statement of this theorem.

A.6.3 Proof of Theorem 10

We prove a more general result here from which we easily derive Theorem 10 as a special case.
To state this general result  we introduce some simple notions. Let h : R → R be an activation
function. We say h is monotone if it satisﬁes the following: Either h(x) ≤ h(y) for all x < y OR
h(x) ≥ h(y) for all x < y. Let (cid:96) : {0  1}n → R be an inner function. An (h  (cid:96)) gate/neuron Gh (cid:96)
is just one that is obtained by composing h and (cid:96) in the natural way  i.e. Gh (cid:96)

(cid:0)x(cid:1) = h(cid:0)(cid:96)(x)(cid:1). We

notate(cid:12)(cid:12)(cid:12)(cid:12)(h  (cid:96))(cid:12)(cid:12)(cid:12)(cid:12)∞ = maxx∈{0 1}n

(cid:12)(cid:12)Gh (cid:96)(x)(cid:12)(cid:12).

We assume for the discussion here that the number of input variables (or observables) is even and is
divided into two halves  called x and y  each being a Boolean string of n bits. In this language  the in-
ner production Boolean function  denoted by IP (x  y)  is just deﬁned as x1y1 +···+xnyn (mod 2).
We call an inner function of a neuron/gate to be (x  y)-separable if it can be expressed as g(x)+f (y).
For instance  all afﬁne inner functions are (x  y)-separable. Finally  given a set of activation func-
tions H and a set of inner functions I  an (H  I)- network is one each of whose hidden unit is a

neuron of the form Gh (cid:96) for some h ∈ H and (cid:96) ∈ I. Let(cid:12)(cid:12)(cid:12)(cid:12)(H  I)(cid:12)(cid:12)(cid:12)(cid:12)∞ = sup(cid:8)(cid:12)(cid:12)(cid:12)(cid:12)(h  (cid:96))(cid:12)(cid:12)(cid:12)(cid:12)∞ : h ∈
H  (cid:96) ∈ I(cid:9).

Theorem 17. Let H be any set of monotone activation functions and I be a set of (x  y) separable
inner functions. Then  every (H  I) network with one layer of m hidden units computing IP with a
margin of δ must satisfy the following:

m ≥

2(cid:12)(cid:12)(cid:12)(cid:12)(H  I)(cid:12)(cid:12)(cid:12)(cid:12)∞

δ

2n/4.

In order to prove Theorem 17  it would be convenient to consider the following 1/-1 valued function:
(−1)IP(x y) = (−1)x1y1+···+xnyn. Please note that when IP evaluates to 0  (−1)IP evaluates to 1 and
when IP evaluates to 1  (−1)IP evaluates to -1.
We also consider a matrix Mn with entries in {1 −1} which has 2n rows and 2n columns. Each
row of Mn is indexed by a unique Boolean string in {0  1}n. The columns of the matrix are also
indexed similarly. The entry Mn[x  y] is just the 1/-1 value of (−1)IP(x y). We need the following
fact that is a special case of the classical result of Lindsey.
Lemma 18 (Chor and Goldreich 1988). The magnitude of the sum of elements in every r × s sub-
matrix of Mn is at most

rs2n.

√

We use Lemma 18 to prove the following key fact about monotone activation functions:
Lemma 19. Let Gh (cid:96) be any neuron with a monotone activation function h and inner function (cid:96) that
is (x  y)-separable. Then 

19

(cid:12)(cid:12)(cid:12)(cid:12) Ex y

(cid:20)

Gh (cid:96)

(cid:0)x  y(cid:1)(−1)IP(cid:0)x y(cid:1)(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ||(h  (cid:96))||∞ · 2−Ω(n).

(2)

Proof. Let (cid:96)(x  y) = g(x) + f (y) and let 0 < α < 1 be some constant speciﬁed later. Deﬁne a
total order ≺g on {0  1}n by setting x ≺g x(cid:48) whenever g(x) ≤ g(x(cid:48)) and x occurs before x(cid:48) in the
lexicographic ordering. We divide {0  1}n into t = 2(1−α)n groups of equal size as follows: the ﬁrst
group contains the ﬁrst 2αn elements in the order speciﬁed by ≺g  the second group has the next
2αn elements and so on. The ith such group is denoted by Xi for i ≤ 2(1−α)n. Likewise  we deﬁne
the total order ≺f and use it to deﬁne equal sized blocks Y1  . . .   Y2(1−α)n.
The way we estimate the LHS of (2) is to pair points in the block (Xi  Yj) with (Xi+1  Yj+1)
in the following manner: wlog assume that the activation function h in non-decreasing. Then 
Gh (cid:96)(x  y) ≤ Gh (cid:96)(x(cid:48)  y(cid:48)) for each (x  y) ∈ (Xi  Yj) and (x(cid:48)  y(cid:48)) ∈ (Xi+1  Yj+1). Further  applying
Lemma 18  we will argue that the total number of points in (Xi  Yj) at which the product in the
LHS evaluates negative (positive) is very close to the number of points in (Xi+1  Yj+1) at which
the product evaluates to positive (negative). Moreover  by assumption  the composed function (h  (cid:96))
does not take very large values in our domain by assumption. These observations will be used to
show that the points in blocks that are diagonally across each other will almost cancel each other’s
contribution to the LHS. There are too few uncancelled blocks and hence the sum in the LHS will
be small. Forthwith the details.
i j = {(x  y) ∈ (Xi  Yi)| IP(x  y) = −1}.
Let P +
Let t = 2(1−α)n. Let hi j be the max value that the gate takes on points in (Xi  Yj). Note that the
non-decreasing assumption on h implies that hi j ≤ hi+1 j+1. Using this observation  we get the
following:

i j = {(x  y) ∈ (Xi  Yj)| IP(x  y) = 1} and P −

(cid:12)(cid:12)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:18)(cid:12)(cid:12)P +
(cid:12)(cid:12) −(cid:12)(cid:12)P −
(cid:12)(cid:12) −(cid:12)(cid:12)P −
(cid:12)(cid:12) is at most 2 · 2(α+1/2)n. Thus  we get
2 )n + 4 · 2−(1−α)n(cid:1).
RHS of (3) ≤ ||(h  (cid:96))||∞ ·(cid:0)2 · 2−(α− 1
(cid:12)(cid:12)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) − 1
(cid:18)(cid:12)(cid:12)P +

(cid:12)(cid:12) −(cid:12)(cid:12)P −

(4)
Thus  setting α = 3/4 gives us the bound that the RHS above is arbitrarily close to ||(h  (cid:96))||∞·2−n/4.
Similarly  pairing things slightly differently  we get

Ex y

(cid:0)x  y(cid:1)(−1)IP(cid:0)x y(cid:1)(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12) (cid:88)
We apply Lemma 18 to conclude that(cid:12)(cid:12)P +

≤ 1
4n

(cid:0)x  y(cid:1)(−1)IP(cid:0)x y(cid:1)(cid:21)

hi j|Pi j|

(cid:88)

(cid:88)

(cid:88)

≥ 1
4n

i j

i+1 j+1

Ex y

Gh (cid:96)

i+1 j+1

i j

i+1 j+1

i j

hi+1 j+1

i=tORj=t

hi j

(i j)<t

(cid:20)

Gh (cid:96)

1
4n

(5)
Again similar conditions and settings of α imply that RHS of (5) is no smaller than −||(h  (cid:96))||∞ ·
2−n/4  thus proving our lemma.

(i j)<t

(cid:20)

(3)

|hi j| · |Pi j|

4n

i=tORj=t

We are now ready to prove Theorem 17.

Proof of Theorem 17. Let C be any (H  I) network having m hidden units  Gh1 (cid:96)1  . . .   Ghm (cid:96)m 
where each hi ∈ H and each (cid:96)i ∈ I is (x  y)-separable. Further  let the output threshold gate
be such that whenever the sum is at least b  C outputs 1 and whenever it is at most a  C outputs
-1. Then  let f be the sum total of the function feeding into the top threshold gate of C. Deﬁne
t = f − (a + b)/2. Hence 

(cid:2)f (x  y)(−1)IP(x y)(cid:3) = Ex y

(cid:2)t(x  y)(−1)IP(x  y)(cid:3) +

Ex y

(cid:2)(−1)IP(x y)(cid:3)

Ex y

a + b

(cid:2)(−1)IP(x y)(cid:3).

2

≥ (b − a)/2 +

a + b

2

Ex y

20

On the other hand  by linearity of expectation and applying Lemma 19  we get

(cid:2)f (x  y)(−1)IP(x y)(cid:3)(cid:12)(cid:12)(cid:12)(cid:12) ≥ b − a

(cid:12)(cid:12)a + b(cid:12)(cid:12)
Thus  it follows easily (cid:12)(cid:12)(cid:12)(cid:12)Ex y
(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) ≤ m ·(cid:12)(cid:12)(cid:12)(cid:12)(H  I)(cid:12)(cid:12)(cid:12)(cid:12)∞ · 2−n/4.
(cid:12)(cid:12)(cid:12)(cid:12)Ex y
(cid:0)x  y(cid:1)(−1)IP(x y)
Comparing (6) and (7)  observing that each of |a| and |b| is at most m(cid:12)(cid:12)(cid:12)(cid:12)(H  I)(cid:12)(cid:12)(cid:12)(cid:12)∞ and recalling that

2−n.

(cid:2)f (x  y)(−1)IP(x y)(cid:3)(cid:12)(cid:12)(cid:12)(cid:12) ≤ m(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)Ex y

(cid:20)

Ghj  (cid:96)j

−

2

2

(6)

(7)

j=1

δ = (b − a)  our desired bound on m follows.

Proof of Theorem 10. The proof follows quite simply by noting that the set of activation functions in
this case is just the singleton set having only the monotone function sof t(y) = log(1+exp(y)). The
set of inner functions are all afﬁne functions with each coefﬁcient having value at most C. As the

afﬁne functions are (x  y)-separable  we can apply Theorem 17. We do so by noting(cid:12)(cid:12)(cid:12)(cid:12)(H  I)(cid:12)(cid:12)(cid:12)(cid:12)∞ ≤
log(1 + exp(nC)) ≤ max(cid:8) log 2  nC + log 2(cid:9). That yields our result.

Remark 20. It is also interesting to note that Theorem 17 appears to be tight in the sense that
none of the hypotheses can be removed. That is  for neurons with general non-montonic activation
functions  or for neurons with monotonic activation functions whose output magnitude violates the
aforementioned bounds  there are example networks that can efﬁciently compute any real-valued
function. Thus  to improve this result (e.g. removing the weight bounds) it appears one would need
to use a stronger property of the particular activation function than monotonicity.

21

,James Martens
Arkadev Chattopadhya
Toni Pitassi
Richard Zemel
Vitaly Kuznetsov
Mehryar Mohri
Umar Syed
Zhe Li
Boqing Gong
Tianbao Yang
Xiangyu Zheng
Song Xi Chen