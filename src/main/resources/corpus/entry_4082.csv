2013,Online Variational Approximations to non-Exponential Family Change Point Models: With Application to Radar Tracking,The Bayesian online change point detection (BOCPD) algorithm provides an efficient way to do exact inference when the parameters of an underlying model may suddenly change over time. BOCPD requires computation of the underlying model's posterior predictives  which can only be computed online in $O(1)$ time and memory for exponential family models. We develop variational approximations to the posterior on change point times (formulated as run lengths) for efficient inference when the underlying model is not in the exponential family  and does not have tractable posterior predictive distributions. In doing so  we develop improvements to online variational inference. We apply our methodology to a tracking problem using radar data with a signal-to-noise feature that is Rice distributed. We also develop a variational method for inferring the parameters of the (non-exponential family) Rice distribution.,Online Variational Approximations to

non-Exponential Family Change Point Models:

With Application to Radar Tracking

Ryan Turner

Northrop Grumman Corp.
ryan.turner@ngc.com

Steven Bottone

Northrop Grumman Corp.

steven.bottone@ngc.com

Clay Stanek

Northrop Grumman Corp.
clay.stanek@ngc.com

Abstract

The Bayesian online change point detection (BOCPD) algorithm provides an ef-
ﬁcient way to do exact inference when the parameters of an underlying model
may suddenly change over time. BOCPD requires computation of the underly-
ing model’s posterior predictives  which can only be computed online in O(1)
time and memory for exponential family models. We develop variational approx-
imations to the posterior on change point times (formulated as run lengths) for
efﬁcient inference when the underlying model is not in the exponential family 
and does not have tractable posterior predictive distributions. In doing so  we de-
velop improvements to online variational inference. We apply our methodology
to a tracking problem using radar data with a signal-to-noise feature that is Rice
distributed. We also develop a variational method for inferring the parameters of
the (non-exponential family) Rice distribution.

Change point detection has been applied to many applications [5; 7]. In recent years there have been
great improvements to the Bayesian approaches via the Bayesian online change point detection
algorithm (BOCPD) [1; 23; 27]. Likewise  the radar tracking community has been improving in its
use of feature-aided tracking [10]: methods that use auxiliary information from radar returns such
as signal-to-noise ratio (SNR)  which depend on radar cross sections (RCS) [21]. Older systems
would often ﬁlter only noisy position (and perhaps Doppler) measurements while newer systems use
more information to improve performance. We use BOCPD for modeling the RCS feature. Whereas
BOCPD inference could be done exactly when ﬁnding change points in conjugate exponential family
models the physics of RCS measurements often causes them to be distributed in non-exponential
family ways  often following a Rice distribution. To do inference efﬁciently we call upon variational
Bayes (VB) to ﬁnd approximate posterior (predictive) distributions. Furthermore  the nature of both
BOCPD and tracking require the use of online updating. We improve upon the existing and limited
approaches to online VB [24; 13]. This paper produces contributions to  and builds upon background
from  three independent areas: change point detection  variational Bayes  and radar tracking.
Although the emphasis in machine learning is on ﬁltering  a substantial part of tracking with radar
data involves data association  illustrated in Figure 1. Observations of radar returns contain mea-
surements from multiple objects (targets) in the sky. If we knew which radar return corresponded
to which target we would be presented with NT ∈ N0 independent ﬁltering problems; Kalman
ﬁlters [14] (or their nonlinear extensions) are applied to “average out” the kinematic errors in the
measurements (typically positions) using the measurements associated with each target. The data
association problem is to determine which measurement goes to which track. In the classical setup 
once a particular measurement is associated with a certain target  that measurement is plugged into
the ﬁlter for that target as if we knew with certainty it was the correct assignment. The association
algorithms  in effect  ﬁnd the maximum a posteriori (MAP) estimate on the measurement-to-track
association. However  approaches such as the joint probabilistic data association (JPDA) ﬁlter [2]
and the probability hypothesis density (PHD) ﬁlter [16] have deviated from this.

1

To ﬁnd the MAP estimate a log likelihood of the data under each possible assignment vector a must
be computed. These are then used to construct cost matrices that reduce the assignment problem to a
particular kind of optimization problem (the details of which are beyond the scope of this paper). The
motivation behind feature-aided tracking is that additional features increase the probability that the
MAP measurement-to-track assignment is correct. Based on physical arguments the RCS feature
(SNR) is often Rice distributed [21  Ch. 3]; although  in certain situations RCS is exponential or
gamma distributed [26]. The parameters of the RCS distribution are determined by factors such as
the shape of the aircraft facing the radar sensor. Given that different aircraft have different RCS
characteristics  if one attempts to create a continuous track estimating the path of an aircraft  RCS
features may help distinguish one aircraft from another if they cross paths or come near one another 
for example. RCS also helps distinguish genuine aircraft returns from clutter: a ﬂock of birds or
random electrical noise  for example. However  the parameters of the RCS distributions may also
change for the same aircraft due to a change in angle or ground conditions. These must be taken into
account for accurate association. Providing good predictions in light of a possible sudden change in
the parameters of a time series is “right up the alley” of BOCPD and change point methods.
The original BOCPD papers [1; 11] studied sudden changes in the parameters of exponential family
models for time series. In this paper  we expand the set of applications of BOCPD to radar SNR
data which often has the same change point structure found in other applications  and requires online
predictions. The BOCPD model is highly modular in that it looks for changes in the parameters of
any underlying process model (UPM). The UPM merely needs to provide posterior predictive prob-
abilities  the UPM can otherwise be a “black box.” The BOCPD queries the UPM for a prediction
of the next data point under each possible run length  the number of points since the last change
point. If (and only if by Hipp [12]) the UPM is exponential family (with a conjugate prior) the
posterior is computed by accumulating the sufﬁcient statistics since the last potential change point.
This allows for O(1) UPM updates in both computation and memory as the run length increases.
We motivate the use of VB for implementing UPMs when the data within a regime is believed to
follow a distribution that is not exponential family. The methods presented in this paper can be used
to ﬁnd variational run length posteriors for general non-exponential family UPMs in addition to the
Rice distribution. Additionally  the methods for improving online updating in VB (Section 2.2) are
applicable in areas outside of change point detection.

Figure 1: Illustrative example of a tracking scenario: The black lines (−) show the true tracks while the red
stars (∗) show the state estimates over time for track 2 and the blue stars for track 1. The 95% credible regions
on the states are shown as blue ellipses. The current (+) and previous (×) measurements are connected to their
associated tracks via red lines. The clutter measurements (birds in this case) are shown with black dots (·). The
distributions on the SNR (RCS) for each track (blue and red) and the clutter (black) are shown on the right.

To our knowledge this paper is the ﬁrst to demonstrate how to compute Bayesian posterior distri-
butions on the parameters of a Rice distribution; the closest work would be Lauwers et al. [15] 
which computes a MAP estimate. Other novel factors of this paper include: demonstrating the use-
fulness (and advantages over existing techniques) of change point detection for RCS estimation and
tracking; and applying variational inference for UPMs where analytic posterior predictives are not
possible. This paper provides four main technical contributions: 1) VB inference for inferring the
parameters of a Rice distribution. 2) General improvements to online VB (which is then applied to
updating the UPM in BOCPD). 3) Derive a VB approximation to the run length posterior when the
UPM posterior predictive is intractable. 4) Handle censored measurements (particularly for a Rice
distribution) in VB. This is key for processing missed detections in data association.

2

clutter (birds)track 1 (747)track 2 (EMB 110)05101520SNRLikelihood1 Background

In this section we brieﬂy review the three areas of background: BOCPD  VB  and tracking.

1.1 Bayesian Online Change Point Detection

We brieﬂy summarize the model setup and notation for the BOCPD algorithm; see [27  Ch. 5] for a
detailed description. We assume we have a time series with n observations so far y1  . . .   yn ∈ Y. In
effect  BOCPD performs message passing to do online inference on the run length rn ∈ 0:n − 1  the
number of observations since the last change point. Given an underlying predictive model (UPM)
and a hazard function h  we can compute an exact posterior over the run length rn. Conditional on a
run length  the UPM produces a sequential prediction on the next data point using all the data since
the last change point: p(yn|y(r)  Θm) where (r) := (n − r):(n − 1). The UPM is a simpler model
where the parameters θ change at every change point and are modeled as being sampled from a prior
with hyper-parameters Θm. The canonical example of a UPM would be a Gaussian whose mean
and variance change at every change point. The online updates are summarized as:

(cid:88)

rn−1

(cid:124)

hazard

(cid:123)(cid:122)
(cid:88)

msgn := p(rn  y1:n) =

P (rn|rn−1)

(cid:125)

(cid:124)
(cid:125)
p(yn|rn−1  y(r))

(cid:123)(cid:122)

(cid:124)

UPM

(cid:123)(cid:122)

msgn−1

(cid:125)

p(rn−1  y1:n−1)

.

(1)

Unless rn = 0  the sum in (1) only contains one term since the only possibility is that rn−1 = rn−1.
The indexing convention is such that if rn = 0 then yn+1 is the ﬁrst observation sampled from the
new parameters θ. The marginal posterior predictive on the next data point is easily calculated as:

p(yn+1|y1:n) =

p(yn+1|y(r))P (rn|y1:n) .

(2)

Thus  the predictions from BOCPD fully integrate out any uncertainty in θ. The message updates
(1) perform exact inference under a model where the number of change points is not known a priori.

rn

yn ∼ Rice(ν  σ)  

=⇒ p(yn|ν  σ) = ynτ exp(−τ (y2

ν ∼ N (µ0  σ2/λ0)  
n + ν2)/2)I0(ynντ )I{yn ≥ 0}

BOCPD RCS Model We show the Rice UPM as an example as it is required for our application.
The data within a regime are assumed to be iid Rice observations  with a normal-gamma prior:
σ−2 =: τ ∼ Gamma(α0  β0)

(3)
(4)
where I0(·) is a modiﬁed Bessel function of order zero  which is what excludes the Rice distribution
from the exponential family. Although the normal-gamma is not conjugate to a Rice it will enable
us to use the VB-EM algorithm. The UPM parameters are the Rice shape1 ν ∈ R and scale σ ∈ R+ 
θ := {ν  σ}  and the hyper-parameters are the normal-gamma parameters Θm := {µ0  λ0  α0  β0}.
Every change point results in a new value for ν and σ being sampled. A posterior on θ is maintained
for each run length  i.e. every possible starting point for the current regime  and is updated at each
new data point. Therefore  BOCPD maintains n distinct posteriors on θ  and although this can be
reduced with pruning  it necessitates posterior updates on θ that are computationally efﬁcient.
Note that the run length updates in (1) require the UPM to provide predictive log likelihoods at all
sample sizes rn (including zero). Therefore  UPM implementations using such approximations as
plug-in MLE predictions will not work very well. The MLE may not even be deﬁned for run lengths
smaller than the number of UPM parameters |θ|. For a Rice UPM  the efﬁcient O(1) updating in
exponential family models by using a conjugate prior and accumulating sufﬁcient statistics is not
possible. This motivates the use of VB methods for approximating the UPM predictions.

1.2 Variational Bayes

We follow the framework of VB where when computation of the exact posterior distribution
p(θ|y1:n) is intractable it is often possible to create a variational approximation q(θ) that is lo-
cally optimal in terms of the Kullback-Leibler (KL) divergence KL(q(cid:107)p) while constraining q to be
in a certain family of distributions Q. In general this is done by optimizing a lower bound L(q) on
the evidence log p(y1:n)  using either gradient based methods or standard ﬁxed point equations.

1 The shape ν is usually assumed to be positive (∈ R+); however  there is nothing wrong with using a

negative ν as Rice(x|ν  σ) = Rice(x|−ν  σ). It also allows for use of a normal-gamma prior.

3

The VB-EM Algorithm In many cases  such as the Rice UPM  the derivation of the VB ﬁxed point
equations can be simpliﬁed by applying the VB-EM algorithm [3]. VB-EM is applicable to models
that are conjugate-exponential (CE) after being augmented with latent variables x1:n. A model is
CE if: 1) The complete data likelihood p(x1:n  y1:n|θ) is an exponential family distribution; and 2)
the prior p(θ) is a conjugate prior for the complete data likelihood p(x1:n  y1:n|θ). We only have
to constrain the posterior q(θ  x1:n) = q(θ)q(x1:n) to factorize between the latent variables and the
parameters; we do not constrain the posterior to be of any particular parametric form. Requiring the
complete likelihood to be CE is a much weaker condition than requiring the marginal on the observed
data p(y1:n|θ) to be CE. Consider a mixture of Gaussians: the model becomes CE when augmented
with latent variables (class labels). This is also the case for the Rice distribution (Section 2.1).
Like the ordinary EM algorithm [9] the VB-EM algorithm alternates between two steps: 1) Find the
posterior of the latent variables treating the expected natural parameters ¯η := Eq(θ)[η] as correct:
q(xi)← p(xi|yi  η = ¯η). 2) Find the posterior of the parameters using the expected sufﬁcient statis-
tics ¯S := Eq(x1:n)[S(x1:n  y1:n)] as if they were the sufﬁcient statistics for the complete data set:
q(θ)← p(θ|S(x1:n  y1:n) = ¯S). The posterior will be of the same exponential family as the prior.

1.3 Tracking

i=1

n maps tracks to measurements: meaning a−1

In this section we review data association  which along with ﬁltering constitutes tracking. In data
association we estimate the association vectors a which map measurements to tracks. At each time
step  n ∈ N1  we observe NZ(n) ∈ N0 measurements  Zn = {zi n}NZ (n)
  which includes returns
from both real targets and clutter (spurious measurements). Here  zi n ∈ Z is a vector of kinematic
measurements (positions in R3  or R4 with a Doppler)  augmented with an RCS component R ∈ R+
for the measured SNR  at time tn ∈ R. The assignment vector at time tn is such that an(i) = j
if measurement i is associated with track j > 0; an(i) = 0 if measurement i is clutter. The
n (an(i)) = i if an(i) (cid:54)= 0; and
inverse mapping a−1
n (i) = 0⇔ an(j) (cid:54)= i for all j. For example  if NT = 4 and a = [2 0 0 1 4] then NZ = 5 
a−1
Nc = 2  and a−1 = [4 1 0 5]. Each track is associated with at most one measurement  and vice-versa.
In ND data association we jointly ﬁnd the MAP estimate of the association vectors over a sliding
window of the last N − 1 time steps. We assume we have NT (n) ∈ N0 total tracks as a known
parameter: NT (n) is adjusted over time using various algorithms (see [2  Ch. 3]). In the generative
process each track places a probability distribution on the next N − 1 measurements  with both
kinematic and RCS components. However  if the random RCS R for a measurement is below R0
then it will not be observed. There are Nc(n) ∈ N0 clutter measurements from a Poisson process
with λ := E[Nc(n)] (often with uniform intensity). The ordering of measurements in Zn is assumed
to be uniformly random. For 3D data association the model joint p(Zn−1:n  an−1  an|Z1:n−2) is:

λNc(i) exp(−λ)/|Zi|!

p0(zj i)

I{ai(j)=0}  

(5)

NT(cid:89)

i=1

n−1(i) n−1) × n(cid:89)

i=n−1

pi(za

n (i) n  za
−1
−1

|Zi|(cid:89)

j=1

where pi is the probability of the measurement sequence under track i; p0 is the clutter distribution.
The probability pi is the product of the RCS component predictions (BOCPD) and the kinematic
components (ﬁlter); informally  pi(z) = pi(positions)× pi(RCS). If there is a missed detection  i.e.
a−1
n (i) = 0  we then use pi(za
n (i) n) = P (R < R0) under the RCS model for track i with no con-
−1
tribution from positional (kinematic) component. Just as BOCPD allows any black box probabilistic
predictor to be used as a UPM  any black box model of measurement sequences can used in (5).
The estimation of association vectors for the 3D case becomes an optimization problem of the form:
(6)

log P (an−1  an|Z1:n) = argmax

log p(Zn−1:n  an−1  an|Z1:n−2)  

(ˆan−1  ˆan) = argmax
(an−1 an)

(an−1 an)

which is effectively optimizing (5) with respect to the assignment vectors. The optimization given
in (6) can be cast as a multidimensional assignment (MDA) problem [2]  which can be solved ef-
ﬁciently in the 2D case. Higher dimensional assignment problems  however  are NP-hard; approx-
imate  yet typically very accurate  solvers must be used for real-time operation  which is usually
required for tracking systems [20].
If a radar scan occurs at each time step and a target is not detected  we assume the SNR has not
exceeded the threshold  implying 0 ≤ R < R0. This is a (left) censored measurement and is treated
differently than a missing data point. Censoring is accounted for in Section 2.3.

4

n(cid:88)

¯x :=

E[xi]/n   µn =

i=1
βn = β0 +

1
2

(E[xi] − ¯x)2 +

n(cid:88)

i=1

λ0µ0 +(cid:80)

E[xi]

i
λ0 + n
1
nλ0
2

λ0 + n

 

λn = λ0 + n   αn = α0 + n  

(¯x − µ0)2 +

1
2

i − E[xi]2 .
R2

n(cid:88)

i=1

(9)

(10)

2 Online Variational UPMs

We cover the four technical challenges for implementing non-exponential family UPMs in an efﬁ-
cient and online manner. We drop the index of the data point i when it is clear from context.

2.1 Variational Posterior for a Rice Distribution

The Rice distribution has the property that

x ∼ N (ν  σ2)  

y(cid:48) ∼ N (0  σ2) =⇒ R =

(cid:112)

x2 + y(cid:48)2 ∼ Rice(ν  σ) .

(7)

For simplicity we perform inference using R2  as opposed to R  and transform accordingly:

x ∼ N (ν  σ2)   R2 − x2 ∼ Gamma( 1
=⇒ p(R2  x) = p(R2|x)p(x) = Gamma(R2 − x2| 1

τ := 1/σ2 ∈ R+

2   τ
2   τ

2 )  
2 )N (x|ν  σ2) .

(8)
The complete likelihood (8) is the product of two exponential family models and is exponential
family itself  parameterized with base measure h and partition factor g:
η = [ντ  −τ /2](cid:62)  S = [x  R2](cid:62)  h(R2  x) = (2π
g(ν  τ ) = τ exp(−ν2τ /2) .
R2 − x2)−1 
By inspection we see that the natural parameters η and sufﬁcient statistics S are the same as a
Gaussian with unknown mean and variance. Therefore  we apply the normal-gamma prior on (ν  τ )
as it is the conjugate prior for the complete data likelihood. This allows us to apply the VB-EM
i as the VB observation  not Ri as in (3). In (5)  z· ·(end) is the RCS R.
algorithm. We use yi := R2

(cid:112)

VB M-Step We derive the posterior updates to the parameters given expected sufﬁcient statistics:

This is the same as an observation from a Gaussian and a gamma that share a (inverse) scale τ.
VB E-Step We then must ﬁnd both expected sufﬁcient statistics ¯S. The expectation E[R2
i|R2
i trivially; leaving E[xi|R2
R2
the radius to R  the angle ω will be distributed by a von Mises (VM) distribution. Therefore 
ω := arccos(x/R) ∼ VM(0  κ)   κ = R E[ντ ] =⇒ E[x] = R E[cos ω] = RI1(κ)/I0(κ)  
(11)
where computing κ constitutes the VB E-step and we have used the trigonometric moment on ω [18].
This completes the computations required to do the VB updates on the Rice posterior.

i ] =
i ]. Recall that the joint on (x  y(cid:48)) is a bivariate normal; if we constrain

n(cid:88)
Variational Lower Bound For completeness  and to assess convergence  we derive the VB lower
bound L(q). Using the standard formula [4] for L(q) = Eq[log p(y1:n  x1:n  θ)] + H[q] we get:

(12)
where p in the KL is the prior on (ν  τ ) which is easy to compute as q and p are both normal-gamma.
Equivalently  (12) can be optimized directly instead of using the VB-EM updates.

E[ν2τ ] + log I0(κi) − KL(q(cid:107)p)  

i + (E[ντ ] − κi/Ri)E[xi] − 1

E[log τ /2] − 1

E[τ ]R2

i=1

2

2

2.2 Online Variational Inference

In Section 2.1 we derived an efﬁcient way to compute the variational posterior for a Rice distribution
for a ﬁxed data set. However  as is apparent from (1) we need online predictions from the UPM;
we must be able to update the posterior one data point at a time. When the UPM is exponential
family and we can compute the posterior exactly  we merely use the posterior from the previous step
as the prior. However  since we are only computing a variational approximation to the posterior 
using the previous posterior as the prior does not give the exact same answer as re-computing the
posterior from batch. This gives two obvious options: 1) recompute the posterior from batch every
update at O(n) cost or 2) use the previous posterior as the prior at O(1) cost and reduced accuracy.

5

i=1

i=1

¯S =(cid:80)n
posterior effectively uses ¯S =(cid:80)n

The difference between the options is encapsulated by looking at the expected sufﬁcient statistics:
Eq(xi|y1:n)[S(xi  yi)]. Naive online updating uses old expected sufﬁcient statistics whose
Eq(xi|y1:i)[S(xi  yi)]. We get the best of both worlds if we
adjust those estimates over time. We in fact can do this if we project the expected sufﬁcient statistics
into a “feature space” in terms of the expected natural parameters. For some function f 

q(xi) = p(xi|yi  η = ¯η) =⇒ Eq(xi|y1:n)[S(xi  yi)] = f (yi  ¯η) .

f (yi  ¯η) = φ(¯η)(cid:62)ψ(yi) =⇒ ¯S =(cid:80)n

If f is piecewise continuous then we can represent it with an inner product [8  Sec. 2.1.6]
i=1ψ(yi)  

(14)
where an inﬁnite dimensional φ and ψ may be required for exact representation  but can be approx-
imated by a ﬁnite inner product. In the Rice distribution case we use (11)

i=1φ(¯η)(cid:62)ψ(yi) = φ(¯η)(cid:62)(cid:80)n

I(cid:48)(·) := I1(·)/I0(·)  

f (yi  ¯η) = E[xi] = RiI(cid:48)(Ri E[ντ ]) = RiI(cid:48)((Ri/µ0) µ0E[ντ ])  

(15)
i and ¯η1 = E[ντ ]. We can easily represent f with an inner product if we can
where recall that yi = R2
represent I(cid:48) as an inner product: I(cid:48)(uv) = φ(u)(cid:62)ψ(v). We use unitless φi(u) = I(cid:48)(ciu) with c1:G
as a log-linear grid from 10−2 to 103 and G = 50. We use a lookup table for ψ(v) that was trained to
match I(cid:48) using non-negative least squares  which left us with a sparse lookup table. Online updating
for VB posteriors was also developed in [24; 13]. These methods involved introducing forgetting
factors to forget the contributions from old data points that might be detrimental to accuracy. Since
the VB predictions are “embedded” in a change point method  they are automatically phased out if
the posterior predictions become inaccurate making the forgetting factors unnecessary.

(13)

2.3 Censored Data

As mentioned in Section 1.3  we must handle censored RCS observations during a missed detection.
In the VB-EM framework we merely have to compute the expected sufﬁcient statistics given the
censored measurement: E[S|R < R0]. The expected sufﬁcient statistic from (11) is now:
E[x|R < R0] =
σ ))/(1 − Q1( ν
σ ))  
where QM is the Marcum Q function [17] of order M. Similar updates for E[S|R < R0] are
possible for exponential or gamma UPMs  but are not shown as they are relatively easy to derive.

(cid:90) R0
E[x|R]p(R)dR(cid:14) RiceCDF(R0|ν  τ ) = ν(1 − Q2( ν

σ   R0

σ   R0

0

2.4 Variational Run Length Posteriors: Predictive Log Likelihoods

Both updating the BOCPD run length posterior (1) and ﬁnding the marginal predictive log like-
lihood of the next point (2) require calculating the UPM’s posterior predictive log likelihood
log p(yn+1|rn  y(r)). The marginal posterior predictive from (2) is used in data association (6) and
benchmarking BOCPD against other methods. However  the exact posterior predictive distribution
obtained by integrating the Rice likelihood against the VB posterior is difﬁcult to compute.
We can break the BOCPD update (1) into a time and measurement update. The measurement update
corresponds to a Bayesian model comparison (BMC) calculation with prior p(rn|y1:n):

p(rn|y1:n+1) ∝ p(yn+1|rn  y(r))p(rn|y1:n) .

serves as a joint VB lower bound: L(q) = log(cid:80)

(16)
Using the BMC results in Bishop [4  Sec. 10.1.4] we ﬁnd a variational posterior on the run length by
using the variational lower bound for each run length Li(q) ≤ log p(yn+1|rn = i  y(r))  calculated
using (12)  as a proxy for the exact UPM posterior predictive in (16). This gives the exact VB
posterior if the approximating family Q is of the form:
q(rn  θ  x) = qUPM(θ  x|rn)q(rn) =⇒ q(rn = i) = exp(Li(q))p(rn = i|y1:n)/ exp(L(q))   (17)
where qUPM contains whatever constraints we used to compute Li(q). The normalizer on q(rn)
i exp(Li(q))p(rn = i|y1:n) ≤ log p(yn+1|y1:n).
Note that the conditional factorization is different than the typical independence constraint on q.
Furthermore  we derive the estimation of the assignment vectors a in (6) as a VB routine. We use
a similar conditional constraint on the latent BOCPD variables given the assignment and constrain
the assignment posterior to be a point mass. In the 2D assignment case  for example 
q(an X1:NT ) = q(X1:NT |an)q(an) = q(X1:NT |an)I{an = ˆan}  

(18)

6

(a) Online Updating

(b) Exponential RCS

(c) Rice RCS

Figure 2: Left: KL from naive updating ((cid:52))  Sato’s method [24] ((cid:3))  and improved online VB (◦) to the
batch VB posterior vs. sample size n; using a standard normal-gamma prior. Each curve represents a true ν
in the generating Rice distribution: ν = 3.16 (red)  ν = 10.0 (green)  ν = 31.6 (blue) and τ = 1. Middle:
The RMSE (dB scale) of the estimate on the mean RCS distribution E[Rn] is plotted for an exponential RCS
model. The curves are BOCPD (blue)  IMM (black)  identity (magenta)  α-ﬁlter (green)  and median ﬁlter
(red). Right: Same as the middle but for the Rice RCS case. The dashed lines are 95% conﬁdence intervals.

where each track’s Xi represents all the latent variables used to compute the variational lower bound
on log p(zj n|an(j) = i).
:= {rn  x  θ}. The resulting VB ﬁxed point
equations ﬁnd the posterior on the latent variables Xi by taking ˆan as the true assignment and solving
the VB problem of (17); the assignment ˆan is found by using (6) and taking the joint BOCPD lower
bound L(q) as a proxy for the BOCPD predictive log likelihood component of log pi in (5).

In the BOCPD case  Xi

3 Results

3.1

Improved Online Solution

We ﬁrst demonstrate the accuracy of the online VB approximation (Section 2.2) on a Rice estima-
tion example; here  we only test the VB posterior as no change point detection is applied. Fig-
ure 2(a) compares naive online updating  Sato’s method [24]  and our improved online updating in
KL(online(cid:107)batch) of the posteriors for three different true parameters ν as sample size n increases.
The performance curves are the KL divergence between these online approximations to the posterior
and the batch VB solution (i.e. restarting VB from “scratch” every new data point) vs sample size.
The error for our method stays around a modest 10−2 nats while naive updating incurs large errors
of 1 to 50 nats [19  Ch. 4]. Sato’s method tends to settle in around a 1 nat approximation error. The
recommended annealing schedule  i.e. forgetting factors  in [24] performed worse than naive updat-
ing. We did a grid search over annealing exponents and show the results for the best performing
schedule of n−0.52. By contrast  our method does not require the tuning of an annealing schedule.

3.2 RCS Estimation Benchmarking

We now compare BOCPD with other methods for RCS estimation. We use the same experimental
example as Slocumb and Klusman III [25]  which uses an augmented interacting multiple model
(IMM) based method for estimating the RCS; we also compare against the same α-ﬁlter and median
ﬁlter used in [25]. As a reference point  we also consider the “identity ﬁlter” which is merely an
unbiased ﬁlter that uses only yn to estimate the mean RCS E[Rn] at time step n. We extend this
example to look at Rice RCS in addition to the exponential RCS case. The bias correction constants
in the IMM were adjusted for the Rice distribution case as per [25  Sec. 3.4].
The results on exponential distributions used in [25] and the Rice distribution case are shown in Fig-
ures 2(b) and 2(c). The IMM used in [25] was hard-coded to expect jumps in the SNR of multiples
of ±10 dB  which is exactly what is presented in the example (a sequence of 20  10  30  and 10 dB).
In [25] the authors mention that the IMM reaches an RMSE “ﬂoor” at 2 dB  yet BOCPD continues
to drop as low as 0.56 dB. The RMSE from BOCPD does not spike nearly as high as the other
methods upon a change in E[Rn]. The α-ﬁlter and median ﬁlter appear worse than both the IMM
and BOCPD. The RMSE and conﬁdence intervals are calculated from 5000 runs of the experiment.

7

102030405010−210−1100101102Sample SizeKL (nats)01002003004000246810TimeRCS RMSE (dBsm)0100200300400012345TimeRCS RMSE (dBsm)(a) SIAP Metrics

(b) Heathrow (LHR)

Figure 3: Left: Average relative improvements (%) for SIAP metrics: position accuracy (red (cid:52))  velocity
accuracy (green (cid:3))  and spurious tracks (blue ◦) across difﬁculty levels. Right: LHR: true trajectories shown
as black lines (−)  estimates using a BOCPD RCS model for association shown as blue stars (∗)  and the
standard tracker as red circles (◦). The standard tracker has spurious tracks over east London and near Ipswich.

Background map data: Google Earth (TerraMetrics  Data SIO  NOAA  U.S. Navy  NGA  GEBCO  Europa Technologies)

3.3 Flightradar24 Tracking Problem

Finally  we used real ﬂight trajectories from ﬂightradar24 and plugged them into our 3D tracking
algorithm. We compare tracking performance between using our BOCPD model and the relatively
standard constant probability of detection (no RCS) [2  Sec. 3.5] setup. We use the single integrated
air picture (SIAP) metrics [6] to demonstrate the improved performance of the tracking. The SIAP
metrics are a standard set of metrics used to compare tracking systems. We broke the data into 30
regions during a one hour period (in Sept. 2012) sampled every 5 s  each within a 200 km by 200 km
area centered around the world’s 30 busiest airports [22]. Commercial airport trafﬁc is typically very
orderly and does not allow aircraft to ﬂy close to one another or cross paths. Feature-aided tracking
is most necessary in scenarios with a more chaotic air situation. Therefore  we took random subsets
of 10 ﬂight paths and randomly shifted their start time to allow for scenarios of greater interest.
The resulting SIAP metric improvements are shown in Figure 3(a) where we look at performance by
a difﬁculty metric: the number of times in a scenario any two aircraft come within ∼400 m of each
other. The biggest improvements are seen for difﬁculties above three where positional accuracy
increases by 30%. Signiﬁcant improvements are also seen for velocity accuracy (11%) and the
frequency of spurious tracks (6%). Signiﬁcant performance gains are seen at all difﬁculty levels
considered. The larger improvements at level three over level ﬁve are possibly due to some level ﬁve
scenarios that are not resolvable simply through more sophisticated models. We demonstrate how
our RCS methods prevent the creation of spurious tracks around London Heathrow in Figure 3(b).

4 Conclusions

We have demonstrated that it is possible to use sophisticated and recent developments in machine
learning such as BOCPD  and use the modern inference method of VB  to produce demonstrable
improvements in the much more mature ﬁeld of radar tracking. We ﬁrst closed a “hole” in the
literature in Section 2.1 by deriving variational inference on the parameters of a Rice distribution 
with its inherent applicability to radar tracking. In Sections 2.2 and 2.4 we showed that it is possible
to use these variational UPMs for non-exponential family models in BOCPD without sacriﬁcing its
modular or online nature. The improvements in online VB are extendable to UPMs besides a Rice
distribution and more generally beyond change point detection. We can use the variational lower
bound from the UPM and obtain a principled variational approximation to the run length posterior.
Furthermore  we cast the estimation of the assignment vectors themselves as a VB problem  which is
in large contrast to the tracking literature. More algorithms from the tracking literature can possibly
be cast in various machine learning frameworks  such as VB  and improved upon from there.

8

12345−5051015202530354045DifficultyImprovement (%)Easting (km)Northing (km)020406080100−20020406080References
[1] Adams  R. P. and MacKay  D. J. (2007). Bayesian online changepoint detection. Technical report  Univer-

sity of Cambridge  Cambridge  UK.

[2] Bar-Shalom  Y.  Willett  P.  and Tian  X. (2011). Tracking and Data Fusion: A Handbook of Algorithms.

YBS Publishing.

[3] Beal  M. and Ghahramani  Z. (2003). The variational Bayesian EM algorithm for incomplete data: with

application to scoring graphical model structures. In Bayesian Statistics  volume 7  pages 453–464.

[4] Bishop  C. M. (2007). Pattern Recognition and Machine Learning. Springer.
[5] Braun  J. V.  Braun  R.  and M¨uller  H.-G. (2000). Multiple changepoint ﬁtting via quasilikelihood  with

application to DNA sequence segmentation. Biometrika  87(2):301–314.

[6] Byrd  E. (2003). Single integrated air picture (SIAP) attributes version 2.0. Technical Report 2003-029 

DTIC.

[7] Chen  J. and Gupta  A. (1997). Testing and locating variance changepoints with application to stock prices.

Journal of the Americal Statistical Association  92(438):739–747.

[8] Courant  R. and Hilbert  D. (1953). Methods of Mathematical Physics. Interscience.
[9] Dempster  A. P.  Laird  N. M.  and Rubin  D. B. (1977). Maximum likelihood from incomplete data via the

EM algorithm. Journal of the Royal Statistical Society  Series B  39(1):1–38.

[10] Ehrman  L. M. and Blair  W. D. (2006). Comparison of methods for using target amplitude to improve
measurement-to-track association in multi-target tracking. In Information Fusion  2006 9th International
Conference on  pages 1–8. IEEE.

[11] Fearnhead  P. and Liu  Z. (2007). Online inference for multiple changepoint problems. Journal of the

Royal Statistical Society  Series B  69(4):589–605.

[12] Hipp  C. (1974). Sufﬁcient statistics and exponential families. The Annals of Statistics  2(6):1283–1292.
[13] Honkela  A. and Valpola  H. (2003). On-line variational Bayesian learning. In 4th International Sympo-

sium on Independent Component Analysis and Blind Signal Separation  pages 803–808.

[14] Kalman  R. E. (1960). A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME — Journal of Basic Engineering  82(Series D):35–45.

[15] Lauwers  L.  Barb´e  K.  Van Moer  W.  and Pintelon  R. (2009). Estimating the parameters of a Rice
In Instrumentation and Measurement Technology Conference  2009.

distribution: A Bayesian approach.
I2MTC’09. IEEE  pages 114–117. IEEE.

[16] Mahler  R. (2003). Multi-target Bayes ﬁltering via ﬁrst-order multi-target moments. IEEE Trans. AES 

39(4):1152–1178.

[17] Marcum  J. (1950). Table of Q functions. U.S. Air Force RAND Research Memorandum M-339  Rand

Corporation  Santa Monica  CA.

[18] Mardia  K. V. and Jupp  P. E. (2000). Directional Statistics. John Wiley & Sons  New York.
[19] Murray  I. (2007). Advances in Markov chain Monte Carlo methods. PhD thesis  Gatsby computational

neuroscience unit  University College London  London  UK.

[20] Poore  A. P.  Rijavec  N.  Barker  T. N.  and Munger  M. L. (1993). Data association problems posed as
multidimensional assignment problems: algorithm development. In Optical Engineering and Photonics in
Aerospace Sensing  pages 172–182. International Society for Optics and Photonics.

[21] Richards  M. A.  Scheer  J.  and Holm  W. A.  editors (2010). Principles of Modern Radar: Basic Princi-

ples. SciTech Pub.

[22] Rogers  S. (2012). The world’s top 100 airports: listed  ranked and mapped. The Guardian.
[23] Saatc¸i  Y.  Turner  R.  and Rasmussen  C. E. (2010). Gaussian process change point models. In 27th

International Conference on Machine Learning  pages 927–934  Haifa  Israel. Omnipress.

[24] Sato  M.-A. (2001). Online model selection based on the variational Bayes. Neural Computation 

13(7):1649–1681.

[25] Slocumb  B. J. and Klusman III  M. E. (2005). A multiple model SNR/RCS likelihood ratio score for
In Optics & Photonics 2005  pages 59131N–59131N. International

radar-based feature-aided tracking.
Society for Optics and Photonics.

[26] Swerling  P. (1954). Probability of detection for ﬂuctuating targets. Technical Report RM-1217  Rand

Corporation.

[27] Turner  R. (2011). Gaussian Processes for State Space Models and Change Point Detection. PhD thesis 

University of Cambridge  Cambridge  UK.

9

,Ryan Turner
Steven Bottone
Clay Stanek