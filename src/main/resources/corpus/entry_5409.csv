2008,Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation,Actor-critic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail (e.g.  when function approximation is involved). Interestingly  there is growing evidence that actor-critic approaches based on phasic dopamine signals play a key role in biological learning through the cortical and basal ganglia. We derive a temporal difference based actor critic learning algorithm  for which convergence can be proved without assuming separate time scales for the actor and the critic. The approach is demonstrated by applying it to networks of spiking neurons. The established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms.,Temporal Difference Based Actor Critic Learning -

Convergence and Neural Implementation

Dotan Di Castro  Dmitry Volkinshtein and Ron Meir

Department of Electrical Engineering

Technion  Haifa 32000  Israel

{dot@tx} {dmitryv@tx} {rmeir@ee}.technion.ac.il

Abstract

Actor-critic algorithms for reinforcement learning are achieving renewed popular-
ity due to their good convergence properties in situations where other approaches
often fail (e.g.  when function approximation is involved). Interestingly  there is
growing evidence that actor-critic approaches based on phasic dopamine signals
play a key role in biological learning through cortical and basal ganglia loops.
We derive a temporal difference based actor critic learning algorithm  for which
convergence can be proved without assuming widely separated time scales for the
actor and the critic. The approach is demonstrated by applying it to networks
of spiking neurons. The established relation between phasic dopamine and the
temporal difference signal lends support to the biological relevance of such algo-
rithms.

1 Introduction

Actor-critic (AC) algorithms [22] were probably among the ﬁrst algorithmic approaches to reinforce-
ment learning (RL). In recent years much work focused on state  or state-action  value functions as a
basis for learning. These methods  while possessing desirable convergence attributes in the context
of table lookup representation  led to convergence problems when function approximation was in-
volved. A more recent line of research is based on directly (and usually parametrically) representing
the policy  and performing stochastic gradient ascent on the expected reward  estimated through try-
ing out various actions and sampling trajectories [3  15  23]. However  such direct policy methods
often lead to very slow convergence due to large estimation variance. One approach suggested in
recent years to remedy this problem is the utilization of AC approaches  where the value function
is estimated by a critic  and passed to an actor which selects an appropriate action  based on the
approximated value function. The ﬁrst convergence result for a policy gradient AC algorithm based
on function approximation was established in [13]  and extended recently in [5  6]. At this stage
it seems that AC based algorithms provide a solid foundation for provably effective approaches to
RL based on function approximation. Whether these methods will yield useful solutions to practical
problems remains to be seen.
RL has also been playing an increasingly important role in neuroscience  and experimentalists have
directly recorded the activities of neurons while animals perform learning tasks [20]  and used imag-
ing techniques to characterize human brain activities [17  24] during learning. It was suggested long
ago that the basal ganglia  a set of ancient sub-cortical brain nuclei  are implicated in RL. Moreover 
these nuclei are naturally divided into two components  based on the separation of the striatum (the
main input channel to the basal ganglia) into the ventral and dorsal components. Several imaging
studies [17  24] have suggested that the ventral stream is associated with value estimation by a so
called critic  while the dorsal stream has been implicated in motor output  action selection  and
learning by a so called actor. Two further experimental ﬁndings support the view taken in this work.

First  it has been observed [20] that the short latency phasic response of the dopamine neurons in
the midbrain strongly resembles the temporal difference (TD) signal introduced in theory of TD-
learning [22]  which can be used by AC algorithms for both the actor and the critic. Since mid-brain
dopaminergic neurons project diffusively to both the ventral and dorsal components of the striatum 
these results are consistent with a TD-based AC learning interpretation of the basal ganglia. Second 
recent results suggest that synaptic plasticity occurring at the cortico-striatal synapses is strongly
modulated by dopamine [18]. Based on these observations it has been suggested that the basal gan-
glia take part in TD based RL  with the (global) phasic dopamine signal serving as the TD signal
[16] modulating synaptic plasticity.
Some recent work has been devoted to implementing RL in networks of spiking neurons (e.g.  [1 
9  12]). Such an approach may lead to speciﬁc and experimentally veriﬁable hypotheses regarding
the interaction of known synaptic plasticity rules and RL. In fact  one tantalizing possibility is to
test these derived rules in the context of ex-vivo cultured neural networks (e.g.  [19])  which are
connected to the environment through input (sensory) and output (motor) channels. We then envision
dopamine serving as a biological substrate for implementing the TD signal in such a system. The
work cited above is mostly based on direct policy gradient algorithms  (e.g.  [3])  leading to non-
AC approaches. Moreover  these algorithms were based directly on the reward  rather than on the
biologically better motivated TD signal  which provides more information than the reward itself 
and is expected to lead to improved convergence.

2 A Temporal Difference Based Actor-Critic Algorithm

The TD-based AC algorithm developed in this section is related to the one presented in [5  6]. While
the derivation of the present algorithm differs from the latter work (which also stressed the issue of
the natural gradient)   the essential novel theoretical feature here is the establishment of convergence1
without the restriction to two time scales which was used in [5  6  13]. This result is also important
in a biological context  where  as far as we are aware  there is no evidence for such a time scale
separation.

2.1 Problem Formulation
We consider a ﬁnite Markov Decision Process (MDP) in discrete time with a ﬁnite state set X of
size |X| and a ﬁnite action set U. The MDP models the environment in which the agent acts. Each
selected action u ∈ U determines a stochastic matrix P (u) = [P (y|x  u)]x y∈X where P (y|x  u) is
the transition probability from a state x ∈ X to a state y ∈ X given the control u. A parameterized
policy is described by a conditional probability function  denoted by µ(u|x  θ)  which maps obser-
vation x ∈ X into a control u ∈ U given a parameter θ ∈ RK. For each state x ∈ X the agent
receives a corresponding reward r(x). The agent’s goal is to adjust the parameter θ in order to attain
maximum average reward over time.
For each θ ∈ RK  we have a Markov Chain (MC) induced by P (y|x  u) and µ(u|x  θ). The state
(cid:82)
transitions of the MC are obtained by ﬁrst generating an action u according to µ(u|x  θ)  and then
generating the next state according to P (y|x  u)]x y∈X . Thus  the MC has a transition matrix P (θ) =
[P (y|x  θ)]x y∈X which is given by P (y|x  θ) =
U P (y|x  u)dµ(u|x  θ). We denote the set of these
transition probabilities by P = {P (θ)|θ ∈ RK}  and its closure by ¯P. We denote by P (x  u  y)
the stationary probability to be in state x  choose action u and go to state y. Several technical
assumptions are required in the proofs below.
Assumption 2.1. (i) Each MC P (θ)  P (θ) ∈ ¯P  is aperiodic  recurrent  and contains a single
equivalence class. (ii) The function µ(u|x  θ) is twice differentiable. Moreover  there exist positive
constants Br and Bµ  such that for all x ∈ X   u ∈ U  θ ∈ RK and 1 ≤ k1  k2 ≤ K  we have
|r(x)| ≤ Br  |∂µ(u|x  θ)/∂θk| ≤ Bµ  |∂2µ(u|x  θ)/∂θk1θk2| ≤ Bµ.
As a result of assumption 2.1(i)  we have the following lemma regarding the stationary distribution
(Theorem 3.1 in [8]).

1Throughout this paper convergence refers to convergence to a small ball around a stationary point; see

Theorem 2.6 for a precise deﬁnition.

Lemma 2.1. Under Assumption 2.1(i)  each MC  P (θ) ∈ ¯P  has a unique stationary distribution 
denoted by π(θ)  satisfying π(θ)(cid:48)P (θ) = π(θ)(cid:48)  where x(cid:48) is the transpose of vector x.

Next  we deﬁne a measure for performance of an agent in an environment. The average reward per
stage of a MC starting from an initial state x0 ∈ X is deﬁned by

(cid:34)

T(cid:88)

n=0

1
T

(cid:35)
(cid:175)(cid:175)(cid:175)x0 = x

 

r(xn)

J(x|θ) (cid:44) lim

T→∞ Eθ

where Eθ[·] denotes the expectation under the probability measure P (θ)  and xn is the state at time
n. The agent’s goal is to ﬁnd θ ∈ RK which maximizes J(x|θ). The following lemma shows
that under Assumption 2.1  the average reward per stage does not depend on the initial states (see
Theorem 4.7 in [10]).
Lemma 2.2. Under Assumption 2.1 and Lemma 2.1  the average reward per stage  J(x|θ)  is inde-
pendent of the starting state  is denoted by η(θ)  and satisﬁes η(θ) = π(θ)(cid:48)r.

Based on Lemma 2.2  the agent’s goal is to ﬁnd a parameter vector θ  which maximizes the average
reward per stage η(θ). Performing the maximization directly on η(θ) is hard. In the sequel we
show how this maximization can be performed by optimizing η(θ)  using ∇η(θ). A consequence of
Assumption 2.1 and the deﬁnition of η(θ) is the following lemma (see Lemma 1 in [15]).
Lemma 2.3. For each x  y ∈ X and for each θ ∈ RK  the functions P (y|x  θ)  π(x|θ)  and η(θ) 
are bounded  twice differentiable  and have bounded ﬁrst and second derivatives.
Next  we deﬁne the differential value function of state x ∈ X which represents the average reward
the agent receives upon starting from a state x0 and reaching a recurrent state x∗ for the ﬁrst time.
Mathematically 

h(x|θ) (cid:44) Eθ

(r(xn) − η(θ))

(1)
where T (cid:44) min{k > 0|xk = x∗}. We deﬁne h(θ) (cid:44) (h(x1|θ)  . . .   h(x|X||θ)) ∈ R|X|. For each
θ ∈ RK and x ∈ X   h(x|θ)  r(x)  and η(θ) satisfy Poisson’s equation (see Theorem 7.4.1 in [4]) 
(2)

h(x|θ) = r(x) − η(θ) +

P (y|x  θ)h(y|θ).

n=0

(cid:35)
(cid:175)(cid:175)(cid:175)x0 = x

 

(cid:34)

T(cid:88)

(cid:88)

y∈X

Based on the differential value deﬁnition we deﬁne the temporal difference (TD) between the states
x ∈ X and y ∈ X . Formally 

d(x  y) (cid:44) r(x) − η(θ) + h(y|θ) − h(x|θ).

(3)
The TD measures the difference between the differential value estimate following the receipt of
reward r(x) and a move to a new state y  and the estimate of the current differential state value at
state x.

2.2 Algorithmic details and single time scale convergence
We start with a deﬁnition of the likelihood ratio derivative  ψ(x  u|θ) (cid:44) ∇µ(u|x  θ)/µ(u|x  θ) 
which we assume to be bounded.
Assumption 2.2. For all x ∈ X   u ∈ U  and θ ∈ RK  there exists a positive constant  Bψ  such
that |ψ(x  u|θ)| ≤ Bψ < ∞.
In order to improve the agent’s performance  we need to follow the gradient direction. The following
theorem shows how the gradient of the average reward per stage can be calculated by the TD signal.
Similar variants of the theorem were proved using the Q-value [23] or state value [15] instead of the
TD-signal.
Theorem 2.4. The gradient of the average reward per stage for θ ∈ RK can be expressed by

∇η(θ) =

P (x  u  y)ψ(x  u|θ) (d(x  y) + f(x))

(f(x) arbitrary).

(4)

(cid:88)

x y∈X  u∈U

The theorem was proved using an advantage function argument in [6]. We provide a direct proof
in section A of the supplementary material. The ﬂexibility resulting from the function f(x) allows
us to encode the TD signal using biologically realistic positive values only  without inﬂuencing the
convergence proof. In this paper  for simplicity  we use f(x) = 0.
Based on Theorem 2.4  we suggest an TD-based AC algorithm. This algorithm is motivated by [15]
where an actor only algorithm was proposed. In [15] the differential value function was re-estimated
afresh for each regenerative cycle leading to a large estimation variance. Using the continuity of the
actor’s policy function in θ  the difference between the estimates between regenerative cycles is
small. Thus  the critic has a good initial estimate at the beginning of each cycle  which is used
here in order to reduce the variance. A related AC algorithm was proposed in [5  6]  where two
time scales were assumed in order to use Borkar’s two time scales convergence theorem [7]. In our
proposed algorithm  and associated convergence theorem  we do not assume different time scales
for the actor and for the critic.
We present batch mode update equations2 in Algorithm 1 for the actor and the critic. The algorithm
is based on some recurrent state x∗; the visit times to this state are denoted by t0  t1  . . .. Updated
occur only at these times (batch mode). We deﬁne a cycle of the algorithm by the time indices which
satisfy tm ≤ n < tm+1. The variables ˜d  ˜h(x)  and ˜η are the critic’s estimates for d  h(x|θ)  and
η(θ) respectively.

Algorithm 1 Temporal Difference Based Actor Critic Algorithm
1: Given

(cid:80)∞

(cid:80)∞
• An MDP with ﬁnite set X of states and a recurrent state x∗  satisfying 2.1(i).
• Hitting times t0 < t1 < t2 < ··· for the state x∗.
m=1 γm = ∞ and
• Step coefﬁcients γm such that
• A parameterized policy µ(u|x  θ)  θ ∈ RK  which satisﬁes Assumption 2.1(ii).
• A set H  constants B˜h and Bθ  and an operator ΠH according to Assumption B.1.
• Step parameters Γη and Γh satisfying Theorem 2.6.
• ˜η0 = 0 (the estimate of the average reward per stage)
• ˜h0(x) = 0 

∀x ∈ X (the estimate of the differential value function)

m < ∞.

m=1 γ2

2: Initiate the critic’s variables:

3: Initiate the actor: θ0 = 0 and choose f(x) (see (4))
4: for each state xtm+1 visited do
5:

Critic: For all x ∈ X   Nm(x) (cid:44) min{tm < k < tm+1|xk = x}  (min(∅) = ∞)

˜d(xn  xn+1) = r(xn) − ˜ηm + ˜hm(xn+1) − ˜hm(xn) 

˜hm+1(x) = ˜hm(x) + γmΓh

˜d(xn  xn+1)

∀x ∈ X  

  

 tm+1−1(cid:88)
tm+1−1(cid:88)

n=Nm(x)

n=tm

(r(xn) − ˜ηm).

˜ηm+1 = ˜ηm + γmΓη

(cid:80)tm+1−1

ψ(xn  un|θm)( ˜d(xn  xn+1) + f(xn))
Actor: θm+1 = θm + γm
Project each component of ˜hm+1 and θm+1 onto H (see Assumption B.1.).

n=tm

6:
7:
8: end for

In order to prove the convergence of Algorithm 1  we establish two basic results. The ﬁrst shows that
the algorithm converges to the set of ordinary differential equations (5)  and the second establishes
conditions under which the differential equations converge locally.

2In order to prove convergence certain boundedness conditions need to be imposed  which appear as step
7 in the algorithm. For lack of space  the precise deﬁnition of the set H is given in Assumption B.1 of the
supplementary material.



(cid:34)

T−1(cid:88)

(cid:179)

x ∈ X
(cid:34)
T−1(cid:88)

n=0

(cid:180)

 

(5)

(cid:35)
(cid:175)(cid:175)(cid:175)x0 = x
(cid:35)
(cid:175)(cid:175)(cid:175)x0 = x

∗

∗

 

 

Theorem 2.5. Under Assumptions 2.1 and B.1  Algorithm 1 converges to the following set of ODE’s

(cid:88)

x∈X

˙θ = T (θ)∇η(θ) + C(θ) (η(θ) − ˜η) +

D(x)(θ)

h(x|θ) − ˜h(x)

(cid:179)

(cid:180)

˙˜h(x) = Γh

h(x|θ) − ˜h(x)
˙˜η = ΓηT (θ) (η(θ) − ˜η)  

+ ΓhT (θ) (η(θ) − ˜η)  

with probability 1  where

T = min{k > 0|x0 = x
∗

  xk = x

∗}  T (θ) = Eθ[T ]  C(θ) = Eθ

ψ(xn  un|θ)

(cid:35)

(cid:175)(cid:175)(cid:175)x0 = x

∗

(cid:34)
T−1(cid:88)

D(x)(θ) = Eθ

1{xn+1 = x} ψ(xn  un|θ)

+ Eθ

(1{xn = x} ψ(xn  un|θ)

n=0

n=0

and where T (θ)  C(θ)  and D(x)(θ) are continuous with respect to θ.

Theorem 2.5 is proved in section B of the supplementary material  based on the theory of stochas-
tic approximation  and more speciﬁcally  on Theorem 5.2.1 in [14]. An advantage of the proof
technique is that it does not need to assume two time scales.
The second theorem  proved in section C of the supplementary material  states the conditions for
which η(θt) converges to a ball around the local optimum.
Theorem 2.6. If we choose Γη ≥ B2
˙η/η and Γh ≥ B2
/h  for some positive constants h and η 
then lim supt→∞ (cid:107)∇η(θ(t))(cid:107) ≤   where  (cid:44) BCη + |X|BDh. The constants B ˙η and B ˙h are
deﬁned in Section C of the supplementary material.

˙h

3 A Neural Algorithm for the Actor Using McCulloch-Pitts Neurons

In this section we apply the previously developed algorithm to the case of neural networks. We start
with the classic binary valued McCulloch-Pitts neuron  and then consider a more realistic spiking
neuron model. While the algorithm presented in Section 2 was derived and proved to converge in
batch mode  we apply it here in an online fashion. The derivation of an online learning algorithm
from the batch version is immediate (e.g.  [15])  and a proof of convergence in this setting is currently
underway.

A McCulloch-Pitts actor network
The dynamics of the binary valued neurons  given at time n by {ui(n)}N
assumed to be based on stochastic discrete time parallel updates  given by

i=1  ui(n) ∈ {0  1}  is

Pr(ui(n) = 1) = σ(vi(n)) where

vi(n) =

wijuj(n − 1)

(i = 1  2  . . .   N).

j=1

Here σ(v) = 1/(1 + exp(−v))  and the parameters θ in Algorithm 1 are given by {wij}  where
wij(n) is the j (cid:55)→ i synaptic weight at time n. Each neuron’s stochastic output ui is viewed as an
action.
Applying the actor update from Algorithm 1 we obtain the following online learning rule
wij(n + 1) = wij(n) + γd(x(n)  x(n + 1)) (ui(n) − σ(vi(n))) uj(n − 1).

(6)

where d(x(n)  x(n + 1)) is the TD signal.
The update (6) can be interpreted as an error-driven Hebbian-like learning rule modulated by the
TD signal. It resembles the direct policy update rule presented in [2]  except that in this rule the
reward signal is replaced by the TD signal (computed by the critic). Moreover  the eligibility trace
formalism in [2] differs from our formulation.

N(cid:88)

We describe a simulation experiment conducted using a one layered feed-forward artiﬁcial neu-
ral network which functions as an actor  combined with a non biologically motivated critic. The
purpose of the experiment is to examine a simple neuronal model  using different actor and critic
architectures. The actor network consists of a single layered feed-forward network of McCulloch-
Pitts neurons  and TD modulated synapses as described above  where the TD signal is calculated by
a critic. The environment is a maze with barriers consisting of 36 states  see Figure 1(b)  where a
reward of value 1 is provided at the top right corner  and is zero elsewhere. Every time the agent
receives a reward  it is transferred randomly to a different location in the maze. At each time step 
the agent is given an input vector which represents the state. The output layer consists of 4 output
neurons where each neuron represents an action from the action set U = {up  down  left  right}. We
used two different input representations for the actor  consisting either of 12 or 36 neurons (note that
the minimum number of input neurons to represent 36 states is 6  and the maximum number is 36).
The architecture with 36 input neurons represents each maze state with one exclusive neuron  thus 
there is no overlap between input vectors. The architecture with 12 input neurons uses a representa-
tion where each state is represented by two neurons  leading to overlaps between the input vectors.
We tested two types of critic: a table based critic which performs iterates according to Algorithm 1 
and an exact TD which provides the TD of the optimal policy. The results are shown in Figure 1(c) 
averaged over 25 runs  and demonstrate the importance of good input representations and precise
value estimates.

(a)

(b)

(c)

Figure 1: (a) A illustration of the McCulloch-Pitts network. (b) A diagram of the maze where the agent needs
to reach the reward at the upper right corner. (c) The average reward per stage in four different cases: an actor
consisting of 12 input neurons and a table based critic (blue crosses)  an actor consisting of 36 input neurons
and a table based critic (green stars)  an actor consisting of 12 input neurons and exact critic (red circles)  and
an actor consisting of 36 input neurons and an exact TD (black crosses). The optimal average reward per stage
is denoted by the dotted line  while a random agent achieves a reward of 0.005.

A spiking neuron actor

Actual neurons function in continuous time producing action potentials. In extension of [1  9]  we
developed an update rule which is based on the Spike Response Model (SRM) [11]. For each neuron
we deﬁne a state variable vi(t) which represents the membrane potential. The dynamics of vi(t) is
given by

N(cid:88)

(cid:88)

vi(t) = ϑi(t − ˆti) +

wij(t)

ij(t − ˆti  t − tf
j ) 

(7)

j=1

tf
j

where wij(t) is the synaptic efﬁcacy  ˆti is the last spike time of neuron i prior o t  ϑi(t) is the
j are the times of the presynaptic spikes emitted prior to time t  and ij(t −
refractory response  tf
ˆti  t − tf
j ) is the response induced by neuron j at neuron i. The second summation in (7) is over
all spike times of neuron j emitted prior to time t. The neuron model is assumed to have a noisy
threshold  which we model by an escape noise model [11]. According to this model  the neuron ﬁres
in the time interval [t  t + δt) with probability ui(t)δt = ρi(vi(t) − vth)δt  where vth is the ﬁring
threshold and ρi(·) is a monotonically increasing function. When the neuron reaches the threshold
it is assumed to ﬁre and the membrane potential is reset to vr.

051015x 10500.020.040.060.080.10.12Number of StepsAverage Rewardper StageWe consider a network of continuous time neurons and synapses. Based on Algorithm 1  using a
small time step δt  we ﬁnd

(8)
We deﬁne the output of the neuron (interpreted as an action) at time t by ui(t). We note that the
neuron’s output is discrete and that at each time t  a neuron can ﬁre  ui(t) = 1  or be quiescent 
ui(t) = 0. Using the deﬁnition of ψ from Section 2.2  yields (similar to [9])

wij(t + δt) = wij(t) + γd(t)ψij(t).

ψij(t) =

i(t)
ρi(t)

Ht
i(t)
1−δtρi(t)

− δtρ(cid:48)

j

ij(t − ˆti  t − tf
j ) 

ij(t − ˆti  t − tf
j ) 

Ht

j

if ui(t) = 1
if ui(t) = 0

Taking the limit δt → 0  yields the following continuous time update rule

(cid:80)

 ρ(cid:48)
(cid:122)
(cid:195)

(cid:80)

(cid:88)

Hi

dwij(t)

dt

= γd(t)

(1/ρi(t))

δ(t − tf

i ) − 1

ρ(cid:48)
i(t)

ij(t − ˆti  t − tf

j ) .

(9)

(cid:125)(cid:124)

Fpost({tf

i })

(cid:33)

(cid:125)(cid:124)

Fpre({tf

j })

(cid:123)

(cid:123)

(cid:122)
(cid:88)

Ht

j

Similarly to [1  9] we interpret the update rule (9) as a TD modulated spike time dependent plasticity
rule. A detailed discussion and interpretation of this update in a more biological context will be left
to the full paper.
We applied the update rule (9) to an actor network consisting of spiking neurons based on (7). The
network’s goal was to reach a circle at the center of a 2D plain =  where the agent can move  using
Newtonian dynamics  in the four principle directions. The actor is composed of an input layer and
a single layer of modiﬁable weights. The input layer consists of ‘sensory’ neurons which ﬁre ac-
cording to the agent’s location in the environment. The synaptic dynamics of the actor is determined
by (9). The critic receives the same inputs as the actor  but uses a linear function approximation
architecture rather than the table lookup used in Algorithm 1. A standard parameter update rule
appropriate for this architecture (e.g.  ch. 8 in [22]) was used to update the critic’s parameters3. The
output layer of the actor consists of four neuronal groups  representing the directions in which the
agent can move  coded based on a ﬁring rate model using Gaussian tuning curves. The TD signal
is calculated according to (3). Whenever it reaches the centered circle  it receives a reward  and is
transferred randomly to a new position in the environment.
Results of such a simulation are presented in Figure 3. Figure 3-a displays the agent’s typical random
walk like behavior prior to learning  . Figure 3-b depicts four typical trajectories representing the
agent’s actions after a learning phase. Finally  Figure 3-c demonstrates the increase of the average
reward per stage  η  vs. time.

(a)

(b)

(c)

Figure 2: (a) Typical agent tracks prior to learning. (b) Agent trajectories following learning. (c) Average
reward per stage plotted against time.

4 Discussion

We have presented a temporal difference based actor critic learning algorithm for reinforcement
learning. The algorithm was derived from ﬁrst principles based on following a noisy gradient of the

3Algorithm 1 relies on a table lookup critic  while in this example we used a function approximation based

critic  due to the large (continuous) state space.

01020051015200102005101520020040060000.0050.010.0150.02time[sec]ηaverage reward  and a convergence proof was presented without relying on the widely used two time
scale separation for the actor and the critic. The derived algorithm was applied to neural networks 
demonstrating their effective operation in maze problems. The motivation for the proposed algo-
rithm was biological  providing a coherent computational explanation for several recently observed
phenomena: actor critic architectures in the basal ganglia  the relation of phasic dopaminergic neu-
romodulators to the TD signal  and the modulation of the spike time dependent plasticity rules by
dopamine. While a great deal of further work needs to be done on both the theoretical and biologi-
cal components of the framework  we hope that these results provide a tentative step in the (noisy!)
direction of explaining biological RL.

References
[1] D. Baras and R. Meir. Reinforcement learning  spike time dependent plasticity and the bcm rule. Neural

Comput.  19(8):22452279  2007

[2] J. Baxter and P.L. Bartlett. Hebbian synaptic modiﬁcations in spiking neurons that learn. (Technical rep.).
Canberra: Research School of Information Sciences and Engineering  Australian National University 
1999.

[3] J. Baxter and P.L. Bartlett. Inﬁnite-Horizon Policy-Gradient Estimation. J. of Artiﬁcial Intelligence Re-

search  15:319–350  2001.

[4] D.P. Bertsekas. Dynamic Programming and Optimal Control  Vol I.  3rd Ed. Athena Scinetiﬁc  2006.
[5] S. Bhatnagar  R. Sutton  M. Ghavamzadeh  and M. Lee. Incremental natural actor-critic algorithms. In J.C.
Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 105–112. MIT Press  Cambridge  MA  2008.

[6] S. Bhatnagar  R.S. Sutton  M. Ghavamzadeh  and M. Lee. Natural actor-critic algorithms. Automatica  To

appear  2008.

[7] V.S. Borkar. Stochastic approximation with two time scales. Syst. Control Lett.  29(5):291294  1997.
[8] P. Bremaud. Markov Chains: Gibbs Fields  Monte Carlo Simulation  and Queues. Springer  1999.
[9] R.V. Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity.

Neural Computation  19:14681502  2007.

[10] R.G. Gallager. Discrete Stochastic Processes. Kluwer Academic Publishers  1995.
[11] W. Gerstner and W.M. Kistler. Spinking Neuron Models. Cambridge University Press  Cambridge  2002.
[12] E.M. Izhikevich. Solving the Distal Reward Problem through Linkage of STDP and Dopamine Signaling.

Cerebral Cortex  17(10):2443-52  2007.

[13] V.R. Konda and J. Tsitsiklis. On actor critic algorithms. SIAM J. Control Optim.  42(4):11431166  2003.
[14] H.J. Kushner and G.G. Yin. Stochastic Approximation Algorithms and Applications. Springer  1997.
[15] P. Marbach and J. Tsitsiklis. Simulation-Based Optimization of Markov Reward Processes. IEEE. Trans.

Auto. Cont.  46:191–209  1998.

[16] P.R. Montague  P. Dayan  and T.J. Sejnowski. A framework for mesencephalic dopamine systems based

on predictive hebbian learning. Journal of Neuroscience  16:19361947  1996.

[17] J. ODoherty  P. Dayan  J. Schultz  R. Deichmann  K. Friston  and R.J. Dolan. Dissociable roles of ventral

and dorsal striatum in instrumental conditioning. Science  304:452454  2004.

[18] J.N.J. Reynolds and J.R. Wickens. Dopamine-dependent plasticity of corticostriatal synapses. Neural Net-

works  15(4-6):507521  2002.

[19] S. Marom and G. Shahaf. Development  learning and memory in large random networks of cortical neu-

rons: lessons beyond anatomy. Quarterly Reviews of Biophysics  35:6387  2002.

[20] W. Schultz. Multiple reward signals in the brain. Nature Reviews Neuroscience  1:199207  Dec. 2000.
[21] S. Singh and P. Dayan. Analytical mean squared error curves for temporal difference learning. Machine

Learning  32:540  1998.

[22] R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT Press  1998.
[23] R. Sutton  D. McAllester  S. Singh and Y. Mansour. Policy-Gradient Methods for Reinforcement Learning
with Function Approximation. Advances in Neural Information Processing Systems  12:1057–1063  2000.
[24] E.M. Tricomi  M.R. Delgado  and J.A. Fiez. Modulation of caudate activity by action contingency. Neu-

ron  41(2):281292  2004.

,Gunwoong Park
Garvesh Raskutti