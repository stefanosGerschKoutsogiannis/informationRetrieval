2017,Geometric Descent Method for Convex Composite Minimization,In this paper  we extend the geometric descent method recently proposed by Bubeck  Lee and Singh to tackle nonsmooth and strongly convex composite problems. We prove that our proposed algorithm  dubbed geometric proximal gradient method (GeoPG)  converges with a linear rate $(1-1/\sqrt{\kappa})$ and thus achieves the optimal rate among first-order methods  where $\kappa$ is the condition number of the problem. Numerical results on linear regression and logistic regression with elastic net regularization show that GeoPG compares favorably with Nesterov's accelerated proximal gradient method  especially when the problem is ill-conditioned.,Geometric Descent Method for
Convex Composite Minimization

Shixiang Chen1  Shiqian Ma2  and Wei Liu3

1Department of SEEM  The Chinese University of Hong Kong  Hong Kong

2Department of Mathematics  UC Davis  USA

3Tencent AI Lab  China

Abstract

In this paper  we extend the geometric descent method recently proposed by Bubeck 
Lee and Singh [1] to tackle nonsmooth and strongly convex composite problems.
√
We prove that our proposed algorithm  dubbed geometric proximal gradient method
(GeoPG)  converges with a linear rate (1 − 1/
κ) and thus achieves the optimal
rate among ﬁrst-order methods  where κ is the condition number of the problem.
Numerical results on linear regression and logistic regression with elastic net
regularization show that GeoPG compares favorably with Nesterov’s accelerated
proximal gradient method  especially when the problem is ill-conditioned.

1

Introduction

Recently  Bubeck  Lee and Singh proposed a geometric descent method (GeoD) for minimizing a
smooth and strongly convex function [1]. They showed that GeoD achieves the same optimal rate
as Nesterov’s accelerated gradient method (AGM) [2  3]. In this paper  we provide an extension of
GeoD that minimizes a nonsmooth function in the composite form:

min
x∈Rn

F (x) := f (x) + h(x) 

(1.1)
where f is α-strongly convex and β-smooth (i.e.  ∇f is Lipschitz continuous with Lipschitz constant
β)  and h is a closed nonsmooth convex function with simple proximal mapping. Commonly seen
examples of h include (cid:96)1 norm  (cid:96)2 norm  nuclear norm  and so on.
√
If h vanishes  then the objective function of (1.1) becomes smooth and strongly convex. In this case 
it is known that AGM converges with a linear rate (1− 1/
κ)  which is optimal among all ﬁrst-order
methods  where κ = β/α is the condition number of the problem. However  AGM lacks a clear
geometric intuition  making it difﬁcult to interpret. Recently  there has been much work on attempting
to explain AGM or designing new algorithms with the same optimal rate (see  [4  5  1  6  7]). In
particular  the GeoD method proposed in [1] has a clear geometric intuition that is in the ﬂavor
of the ellipsoid method [8]. The follow-up work [9  10] attempted to improve the performance of
GeoD by exploiting the gradient information from the past with a “limited-memory” idea. Moreover 
Drusvyatskiy  Fazel and Roy [10] showed how to extend the suboptimal version of GeoD (with the
convergence rate (1 − 1/κ)) to solve the composite problem (1.1). However  it was not clear how to
extend the optimal version of GeoD to address (1.1)  and the authors posed this as an open question.
In this paper  we settle this question by proposing a geometric proximal gradient (GeoPG) algorithm
which can solve the composite problem (1.1). We further show how to incorporate various techniques
to improve the performance of the proposed algorithm.

Notation. We use B(c  r2) = (cid:8)x|(cid:107)x − c(cid:107)2 ≤ r2(cid:9) to denote the ball with center c and radius r.

We use Line(x  y) to denote the line that connects x and y  i.e.  {x + s(y − x)  s ∈ R}. For ﬁxed
t ∈ (0  1/β]  we denote x+ := Proxth(x − t∇f (x))  where the proximal mapping Proxh(·) is

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

2(cid:107)z − x(cid:107)2. The proximal gradient of F is deﬁned as
deﬁned as Proxh(x) := argminz h(z) + 1
Gt(x) := (x−x+)/t. It should be noted that x+ = x−tGt(x). We also denote x++ := x−Gt(x)/α.
Note that both x+ and x++ are related to t  and we omit t whenever there is no ambiguity.
The rest of this paper is organized as follows. In Section 2  we brieﬂy review the GeoD method for
solving smooth and strongly convex problems. In Section 3  we provide our GeoPG algorithm for
solving nonsmooth problem (1.1) and analyze its convergence rate. We address two practical issues of
the proposed method in Section 4  and incorporate two techniques: backtracking and limited memory 
to cope with these issues. In Section 5  we report some numerical results of comparing GeoPG with
Nesterov’s accelerated proximal gradient method in solving linear regression and logistic regression
problems with elastic net regularization. Finally  we conclude the paper in Section 6.

2 Geometric Descent Method for Smooth Problems
The GeoD method [1] solves (1.1) when h ≡ 0  in which the problem reduces to a smooth and
strongly convex problem min f (x). We denote its optimal solution and optimal value as x∗ and
f∗  respectively. Throughout this section  we ﬁx t = 1/β  which together with h ≡ 0 implies that
x+ = x − ∇f (x)/β and x++ = x − ∇f (x)/α. We ﬁrst brieﬂy describe the basic idea of the
suboptimal GeoD. Since f is α-strongly convex  the following inequality holds
(cid:107)y − x(cid:107)2 ≤ f (y)  ∀x  y ∈ Rn.

f (x) + (cid:104)∇f (x)  y − x(cid:105) +
By letting y = x∗ in (2.1)  it is easy to obtain

x∗ ∈ B(cid:0)x++ (cid:107)∇f (x)(cid:107)2/α2 − 2(f (x) − f∗)/α(cid:1) ∀x ∈ Rn.

(2.1)

α
2

(2.2)

Note that the β-smoothness of f implies

Combining (2.2) and (2.3) yields x∗ ∈ B(cid:0)x++  (1 − 1/κ)(cid:107)∇f (x)(cid:107)2/α2 − 2(f (x+) − f∗)/α(cid:1). As

f (x+) ≤ f (x) − (cid:107)∇f (x)(cid:107)2/(2β) ∀x ∈ Rn.

(2.3)

x∗ ∈ B(cid:0)x0  R2

(cid:1) ∩ B(cid:0)x++

0) that contains x∗  then it follows that

a result  suppose that initially we have a ball B(x0  R2

0

0

  (1 − 1/κ)(cid:107)∇f (x0)(cid:107)2/α2 − 2(f (x+

that x∗ ∈ B(cid:0)x1  R2

0(1 − 1/κ)(cid:1). Therefore  the squared radius of the initial ball shrinks by a factor

(2.4)
Some simple algebraic calculations show that the squared radius of the minimum enclosing ball
0(1 − 1/κ)  i.e.  there exists some x1 ∈ Rn such
of the right hand side of (2.4) is no larger than R2
(1 − 1/κ). Repeating this process yields a linear convergent sequence {xk} with the convergence
rate (1 − 1/κ): (cid:107)xk − x∗(cid:107)2 ≤ (1 − 1/κ)kR2
0.
The optimal GeoD (with the linear convergence rate (1 − 1/
κ)) maintains two balls containing
x∗ in each iteration  whose centers are ck and x++
k+1  respectively. More speciﬁcally  suppose that in
the k-th iteration we have ck and xk  then ck+1 and xk+1 are obtained as follows. First  xk+1 is the
minimizer of f on Line(ck  x+
k+1) is the center (resp. squared radius) of
the ball (given by Lemma 2.1) that contains

k ). Second  ck+1 (resp. R2

k − (cid:107)∇f (xk+1)(cid:107)2/(α2κ)(cid:1) ∩ B(cid:0)x++

k+1  (1 − 1/κ)(cid:107)∇f (xk+1)(cid:107)2/α2(cid:1).

B(cid:0)ck  R2

√

0 ) − f∗)/α(cid:1).

√
k+1 = (1 − 1/

Calculating ck+1 and Rk+1 is easy and we refer to Algorithm 1 of [1] for details. By applying
k ) − f (x∗)) 
Lemma 2.1 with xA = ck  rA = Rk  rB = (cid:107)∇f (xk+1)(cid:107)/α   = 1/κ and δ = 2
k  which further implies (cid:107)x∗ − ck(cid:107)2 ≤ (1 − 1/
0  i.e.  the
√
we obtain R2
optimal GeoD converges with the linear rate (1 − 1/
B > 0. Also ﬁx  ∈ (0  1)
Lemma 2.1 (see [1  10]). Fix centers xA  xB ∈ Rn and squared radii r2
B. There exists a new center c ∈ Rn such that for any δ > 0  we have
and suppose (cid:107)xA − xB(cid:107)2 ≥ r2

α (f (x+
√

κ)kR2

κ)R2

B − δ) ∩ B(cid:0)xB  r2

B(1 − ) − δ(cid:1) ⊂ B(cid:0)c  (1 − √

A  r2

A − δ(cid:1).

A − r2

B(xA  r2

)r2

κ).

3 Geometric Descent Method for Nonsmooth Convex Composite Problems

Drusvyatskiy  Fazel and Roy [10] extended the suboptimal GeoD to solve the composite problem
(1.1). However  it was not clear how to extend the optimal GeoD to solve problem (1.1). We resolve
this problem in this section.
The following lemma is useful to our analysis. Its proof is in the supplementary material.

2

Lemma 3.1. Given point x ∈ Rn and step size t ∈ (0  1/β]  denote x+ = x− tGt(x). The following
inequality holds for any y ∈ Rn:

F (y) ≥ F (x+) + (cid:104)Gt(x)  y − x(cid:105) +

(cid:107)Gt(x)(cid:107)2 +

t
2

(cid:107)y − x(cid:107)2.

α
2

(3.1)

3.1 GeoPG Algorithm

In this subsection  we describe our proposed geometric proximal gradient method (GeoPG) for
solving (1.1). Throughout Sections 3.1 and 3.2  t ∈ (0  1/β] is a ﬁxed scalar. The key observation for
designing GeoPG is that in the k-th iteration one has to ﬁnd xk that lies on Line(x+
k−1  ck−1) such
that the following two inequalities hold:

F (x+

k ) ≤ F (x+

k−1) − t
2

(cid:107)Gt(xk)(cid:107)2  and (cid:107)x++

k − ck−1(cid:107)2 ≥ 1

α2(cid:107)Gt(xk)(cid:107)2.

(3.2)

Intuitively  the ﬁrst inequality in (3.2) requires that there is a function value reduction on x+
k from
x+
k−1  and the second inequality requires that the centers of the two balls are far away from each other
so that Lemma 2.1 can be applied.
The following lemma gives a sufﬁcient condition for (3.2). Its proof is in the supplementary material.
Lemma 3.2. (3.2) holds if xk satisﬁes

(cid:104)x+
k − xk  x+

k−1 − xk(cid:105) ≤ 0  and (cid:104)x+

k − xk  xk − ck−1(cid:105) ≥ 0.

(3.3)

Therefore  we only need to ﬁnd xk such that (3.3) holds. To do so  we deﬁne the following functions
for given x  c (x (cid:54)= c) and t ∈ (0  β]:

φt x c(z) = (cid:104)z+ − z  x − c(cid:105) ∀z ∈ Rn  and ¯φt x c(s) = φt x c

(cid:0)x + s(c − x)(cid:1) ∀s ∈ R.

The functions φt x c(z) and ¯φt x c(s) have the following properties. Its proof can be found in the
supplementary material.
Lemma 3.3. (i) φt x c(z) is Lipschitz continuous. (ii) ¯φt x c(s) strictly monotonically increases.
We are now ready to describe how to ﬁnd xk such that (3.3) holds. This is summarized in Lemma 3.4.
Lemma 3.4. The following two ways ﬁnd xk satisfying (3.3).

(i) If ¯φt x+

k−1 ck−1

(1) ≤ 0  then (3.3) holds by setting xk := ck−1; if ¯φt x+
(1) > 0 and ¯φt x+

(0) ≥ 0  then
(0) < 0  then
(s) = 0. As a result  (3.3) holds by setting

k−1; if ¯φt x+

k−1 ck−1

k−1 ck−1

k−1 ck−1

(3.3) holds by setting xk := x+
there exists s ∈ [0  1] such that ¯φt x+
xk := x+

k−1 + s(ck−1 − x+

k−1).

k−1 ck−1

(ii) If ¯φt x+

(0) ≥ 0  then (3.3) holds by setting xk := x+

then there exists s ≥ 0 such that ¯φt x+
xk := x+

k−1 ck−1
k−1 + s(ck−1 − x+

k−1).

k−1 ck−1

(0) < 0 
(s) = 0. As a result  (3.3) holds by setting

k−1; if ¯φt x+

k−1 ck−1

Proof. Case (i) directly follows from the Mean-Value Theorem. Case (ii) follows from the mono-
tonicity and continuity of ¯φt x+

from Lemma 3.3.

k−1 ck−1

k−1 ck−1

It is indeed very easy to ﬁnd xk satisfying the two cases in Lemma 3.4  since we are tackling a
univariate Lipschitz continuous function ¯φt x c(s) . Speciﬁcally  for case (i) of Lemma 3.4  we can
use the bisection method to ﬁnd the zero of ¯φt x+
in the closed interval [0  1]. In practice  we
found that the Brent-Dekker method [11  12] performs much better than the bisection method  so
we use the Brent-Dekker method in our numerical experiments. For case (ii) of Lemma 3.4  we
in the interval [0  +∞).
can use the semi-smooth Newton method to ﬁnd the zero of ¯φt x+
In our numerical experiments  we implemented the global semi-smooth Newton method [13  14]
and obtained very encouraging results. These two procedures are described in Algorithms 1 and 2 
respectively. Based on the discussions above  we know that xk generated by these two algorithms
satisﬁes (3.3) and hence (3.2).
We are now ready to present our GeoPG algorithm for solving (1.1) as in Algorithm 3.

k−1 ck−1

3

k−1 and ck−1.

k−1)+ − x+
set xk := x+
k−1;

k−1  x+
k−1 − ck−1  x+

Algorithm 1 : The ﬁrst procedure for ﬁnding xk from given x+
1: if (cid:104)(x+
2:
3: else if (cid:104)c+
4:
5: else
6:

set xk := ck−1;
use the Brent-Dekker method to ﬁnd s ∈ [0  1] such that ¯φt x+
xk := x+

k−1 − ck−1(cid:105) ≥ 0 then
k−1 − ck−1(cid:105) ≤ 0 then

k−1 + s(ck−1 − x+

k−1);

7: end if

k−1 ck−1

(s) = 0  and set

k−1  x+

k−1 − ck−1(cid:105) ≥ 0 then

Algorithm 2 : The second procedure for ﬁnding xk from given x+
1: if (cid:104)(x+
2:
3: else
4:

k−1)+ − x+
set xk := x+
k−1;
use the global semi-smooth Newton method [13  14] to ﬁnd the root s ∈ [0  +∞) of
¯φt x+
5: end if

k−1 + s(ck−1 − x+

(s)  and set xk := x+

k−1 and ck−1.

k−1 ck−1

k−1);

3.2 Convergence Analysis of GeoPG

We are now ready to present our main convergence result for GeoPG.
Theorem 3.5. Given initial point x0 and step size t ∈ (0  1/β]  we set R2
(1 − αt).
Suppose that sequence {(xk  ck  Rk)} is generated by Algorithm 3  and that x∗ is the optimal
solution of (1.1) and F ∗ is the optimal objective value. For any k ≥ 0  one has x∗ ∈ B(ck  R2
k+1 ≤ (1 − √
k) and
R2

(cid:107)Gt(x0)(cid:107)2

0 =

α2

αt)R2

k  and thus
(cid:107)x∗ − ck(cid:107)2 ≤ (1 − √

αt)kR2

0  and F (x+

k+1) − F ∗ ≤ α
2

αt)kR2
0.

(3.4)

(1 − √
√

Note that when t = 1/β  (3.4) implies the linear convergence rate (1 − 1/
Proof. We prove a stronger result by induction that for every k ≥ 0  one has

κ).

x∗ ∈ B(cid:0)ck  R2

k ) − F ∗)/α(cid:1).

k − 2(F (x+

(3.5)
Let y = x∗ in (3.1). We have (cid:107)x∗ − x++(cid:107)2 ≤ (1− αt)(cid:107)Gt(x)2(cid:107)/α2 − 2(F (x+)− F ∗)/α  implying
(3.6)
Setting x = x0 in (3.6) shows that (3.5) holds for k = 0. We now assume that (3.5) holds for some
k ≥ 0  and in the following we will prove that (3.5) holds for k + 1. Combining (3.5) and the ﬁrst
inequality of (3.2) yields

x∗ ∈ B(cid:0)x++ (cid:107)Gt(x)(cid:107)2(1 − αt)/α2 − 2(F (x+) − F ∗)/α(cid:1).
k+1) − F ∗)/α(cid:1).
x∗ ∈ B(cid:0)ck  R2
k+1) − F ∗)/α(cid:1).
x∗ ∈ B(cid:0)x++

k+1 (cid:107)Gt(xk+1)(cid:107)2(1 − αt)/α2 − 2(F (x+

k − t(cid:107)Gt(xk+1)(cid:107)2/α − 2(F (x+

By setting x = xk+1 in (3.6)  we obtain

(3.7)

We now apply Lemma 2.1 to (3.7) and (3.8). Speciﬁcally  we set xB = x++
rA = Rk  rB = (cid:107)Gt(xk+1)(cid:107)/α  δ = 2
k ) − F ∗)  and note that (cid:107)xA − xB(cid:107)2 ≥ r2
x∗ ∈ B(cid:0)ck+1  (1 − 1/
α (F (x+
the second inequality of (3.2). Then Lemma 2.1 indicates that there exists ck+1 such that
√
k+1 ≤ (1 − √

k − 2(F (x+
i.e.  (3.5) holds for k + 1 with R2
k. Note that ck+1 is the center of the minimum
enclosing ball of the intersection of the two balls in (3.7) and (3.8)  and can be computed in the
k ≤

same way as Algorithm 1 of [1]. From (3.9) we obtain that (cid:107)x∗ − ck+1(cid:107)2 ≤ (1 − √
(1 − √

k+1) − F ∗)/α(cid:1) 

0. Moreover  (3.7) indicates that F (x+

k+1) − F ∗ ≤ α

2 (1 − √

k ≤ α

0.
αt)kR2

αt)k+1R2

αt)R2

αt)R2

κ)R2

2 R2

(3.9)

(3.8)
k+1  xA = ck   = αt 
B because of

4

0

0 = (cid:107)Gt(x0)(cid:107)2(1 − αt)/α2;

Algorithm 3 : GeoPG: geometric proximal gradient descent for convex composite minimization.
Require: Parameters α  β  initial point x0 and step size t ∈ (0  1/β].
1: Set c0 = x++
  R2
2: for k = 1  2  . . . do
3:
4:
5:
6:

k = xk − Gt(xk)/α  and R2
k−1 − 2(F (x+

k ))/α;
k): the minimum enclosing ball of B(xA  R2

Use Algorithm 1 or 2 to ﬁnd xk;
Set xA := x++
Set xB := ck−1  and R2
Compute B(ck  R2
done using Algorithm 1 in [1];

A = (cid:107)Gt(xk)(cid:107)2(1 − αt)/α2;
k−1) − F (x+

A) ∩ B(xB  R2

B = R2

B)  which can be

7: end for

4 Practical Issues

4.1 GeoPG with Backtracking

In practice  the Lipschitz constant β may be unknown to us. In this subsection  we describe a
backtracking strategy for GeoPG in which β is not needed. From the β-smoothness of f  we have

f (x+) ≤ f (x) − t(cid:104)∇f (x)  Gt(x)(cid:105) + t(cid:107)Gt(x)(cid:107)2/2.

(4.1)
Note that inequality (3.1) holds because of (4.1)  which holds when t ∈ (0  1/β]. If β is unknown 
we can perform backtracking on t such that (4.1) holds  which is a common practice for proximal
gradient method  e.g.  [15–17]. Note that the key step in our analysis of GeoPG is to guarantee that
the two inequalities in (3.2) hold. According to Lemma 3.2  the second inequality in (3.2) holds as
long as we use Algorithm 1 or Algorithm 2 to ﬁnd xk  and it does not need the knowledge of β.
However  the ﬁrst inequality in (3.2) requires t ≤ 1/β  because its proof in Lemma 3.2 needs (3.1).
Thus  we need to perform backtracking on t until (4.1) is satisﬁed  and use the same t to ﬁnd xk by
Algorithm 1 or Algorithm 2. Our GeoPG algorithm with backtracking (GeoPG-B) is described in
Algorithm 4.

Algorithm 4 : GeoPG with Backtracking (GeoPG-B)
Require: Parameters α  γ ∈ (0  1)  η ∈ (0  1)  initial step size t0 > 0 and initial point x0.

Repeat t0 := ηt0 until (4.1) holds for t = t0;
(1 − αt0);
Set c0 = x++
  R2
0 =
for k = 1  2  . . . do

(cid:107)Gt0(x0)(cid:107)2

α2

0

if no backtracking was performed in the (k − 1)-th iteration then

Set tk := tk−1/γ;

else

Set tk := tk−1;

end if
Compute xk by Algorithm 1 or Algorithm 2 with t = tk;
while f (x+

k ) > f (xk) − tk(cid:104)∇f (xk)  Gtk (xk)(cid:105) + tk

Set tk := ηtk (backtracking);
Compute xk by Algorithm 1 or Algorithm 2 with t = tk;

2 (cid:107)Gtk (xk)(cid:107)2 do

end while
Set xA := x++
Set xB := ck−1  R2
Compute B(ck  R2

(cid:107)Gtk (xk)(cid:107)2
k = xk − Gtk (xk)/α  R2
A =
k−1) − F (x+
α (F (x+

B = R2
k): the minimum enclosing ball of B(xA  R2

k−1 − 2

(1 − αtk);

k ));

α2

end for

A) ∩ B(xB  R2
B);

Note that the sequence {tk} generated in Algorithm 4 is uniformly bounded away from 0. This is
because (4.1) always holds when tk ≤ 1/β. As a result  we know tk ≥ tmin := mini=0 ... k ti ≥ η/β.
It is easy to see that in the k-th iteration of Algorithm 4  x∗ is contained in two balls:

x∗ ∈ B(cid:0)ck−1  R2
x∗ ∈ B(cid:0)x++

k

k−1 − tk(cid:107)Gtk (xk)(cid:107)2/α − 2(F (x+
 (cid:107)Gtk (xk)(cid:107)2(1 − αtk)/α2 − 2(F (x+

k ) − F ∗)/α(cid:1) 
k ) − F ∗)/α(cid:1).

5

Therefore  we have the following convergence result for Algorithm 4  whose proof is similar to that
for Algorithm 3. We thus omit the proof for succinctness.
Theorem 4.1. Suppose that {(xk  ck  Rk  tk)} is generated by Algorithm 4. For any k ≥ 0  one
has x∗ ∈ B(ck  R2
0 ≤
k) and R2
(1 − √
αtmin)kR2
0.
4.2 GeoPG with Limited Memory

k  and thus (cid:107)x∗ − ck(cid:107)2 ≤(cid:81)k

k+1 ≤ (1 − √

i=0(1 − √

αti)iR2

αtk)R2

2) that
The basic idea of GeoD is that in each iteration we maintain two balls B(y1  r2
both contain x∗  and then compute the minimum enclosing ball of their intersection  which is expected
to be smaller than both B(y1  r2
2). One very intuitive idea that can possibly improve
the performance of GeoD is to maintain more balls from the past  because their intersection should be
smaller than the intersection of two balls. This idea has been proposed by [9] and [10]. Speciﬁcally 
Bubeck and Lee [9] suggested to keep all the balls from past iterations and then compute the minimum
enclosing ball of their intersection. For a given bounded set Q  the center of its minimum enclosing
ball is known as the Chebyshev center  and is deﬁned as the solution to the following problem:

1) and B(y2  r2

1) and B(y2  r2

min

y

max
x∈Q

(cid:107)y − x(cid:107)2 = min

y

max
x∈Q

(cid:107)y(cid:107)2 − 2y(cid:62)x + Tr(xx(cid:62)).

(4.2)

(4.2) is not easy to solve for a general set Q. However  when Q := ∩m
i )  Beck [18] proved
that the relaxed Chebyshev center (RCC) [19]  which is a convex quadratic program  is equivalent to
(4.2) if m < n. Therefore  we can solve (4.2) by solving a convex quadratic program (RCC):

i=1B(yi  r2

min

y

max
(x (cid:52))∈Γ

(cid:107)y(cid:107)2−2y(cid:62)x+Tr((cid:52)) = max
(x (cid:52))∈Γ

min

y

(cid:107)y(cid:107)2−2y(cid:62)x+Tr((cid:52)) = max
(x (cid:52))∈Γ

−(cid:107)x(cid:107)2+Tr((cid:52)) 
(4.3)

i )  then the dual of (4.3) is
λi ≥ 0  i = 1  . . .   m 

(4.4)

where Γ = {(x (cid:52)) : x ∈ Q (cid:52) (cid:23) xx(cid:62)}. If Q = ∩m

i=1B(ci  r2

m(cid:88)

m(cid:88)

λi(cid:107)ci(cid:107)2 +

λir2

i   s.t.

λi = 1 

min(cid:107)Cλ(cid:107)2 − m(cid:88)

i=1

i=1

i=1

where C = [c1  . . .   cm] and λi  i = 1  2  . . .   m are the dual variables. Beck [18] proved that the
optimal solutions of (4.2) and (4.4) are linked by x∗ = Cλ∗ if m < n.
Now we can give our limited-memory GeoPG algorithm (L-GeoPG) as in Algorithm 5.

0

  r2

0 = R2

0 = (cid:107)Gt(x0)(cid:107)2(1 − 1/κ)/α2  and t = 1/β;

Algorithm 5 : L-GeoPG: Limited-memory GeoPG
Require: Parameters α  β  memory size m > 0 and initial point x0.
1: Set c0 = x++
2: for k = 1  2  . . . do
3:
4:
5:

Use Algorithm 1 or 2 to ﬁnd xk;
Compute r2
Compute B(ck  R2
k): an enclosing ball of the intersection of B(ck−1  R2
i ) (if k ≤ m  then set Qk := ∩k
∩k
i=k−m+1B(x++
  r2
ck = Cλ∗  where λ∗ is the optimal solution of (4.4);

k = (cid:107)Gt(xk)(cid:107)2(1 − 1/κ)/α2;

i=1B(x++

  r2

i

i

6: end for

k−1) and Qk :=
i )). This is done by setting

Remark 4.2. Backtracking can also be incorporated into L-GeoPG. We denote the resulting algo-
rithm as L-GeoPG-B.

√

κ)R2

L-GeoPG has the same linear convergence rate as GeoPG  as we show in Theorem 4.3.
Theorem 4.3. Consider the L-GeoPG algorithm. For any k ≥ 0  one has x∗ ∈ B(ck  R2
k ≤ (1 − 1/
R2
Proof. Note that Qk := ∩k
k−1)∩ B(x++
of B(ck−1  R2
from the proof of Theorem 3.5  and we omit it for brevity.

k−1  and thus (cid:107)x∗ − ck(cid:107)2 ≤ (1 − 1/
i ) ⊂ B(x++
  r2

k). Thus  the minimum enclosing ball
k−1)∩Qk. The proof then follows

k) is an enclosing ball of B(ck−1  R2

i=k−m+1B(x++
  r2

k) and

κ)kR2
0.

  r2

√

k

i

k

6

5 Numerical Experiments

In this section  we compare our GeoPG algorithm with Nesterov’s accelerated proximal gradient
(APG) method for solving two nonsmooth problems: linear regression and logistic regression  both
with elastic net regularization. Because of the elastic net term  the strong convexity parameter α is
known. However  we assume that β is unknown  and implement backtracking for both GeoPG and
APG  i.e.  we test GeoPG-B and APG-B (APG with backtracking). We do not target at comparing
with other efﬁcient algorithms for solving these two problems. Our main purpose here is to illustrate
the performance of this new ﬁrst-order method GeoPG. Further improvement of this algorithm and
comparison with other state-of-the-art methods will be a future research topic.
The initial points were set to zero. To obtain the optimal objective function value F ∗  we ran
APG-B and GeoPG-B for a sufﬁciently long time and the smaller function value returned by the two
algorithms is selected as F ∗. APG-B was terminated if (F (xk) − F ∗)/F ∗ ≤ tol  and GeoPG-B was
k ) − F ∗)/F ∗ ≤ tol  where tol = 10−8 is the accuracy tolerance. The parameters
terminated if (F (x+
used in backtracking were set to η = 0.5 and γ = 0.9. In GeoPG-B  we used Algorithm 2 to ﬁnd xk 
because we found that the performance of Algorithm 2 is slightly better than Algorithm 1 in practice.
In the experiments  we ran Algorithm 2 until the absolute value of ¯φ is smaller than 10−8. The code
was written in Matlab and run on a standard PC with 3.20 GHz I5 Intel microprocessor and 16GB of
memory. In all ﬁgures we reported  the x-axis denotes the CPU time (in seconds) and y-axis denotes
(F (x+

k ) − F ∗)/F ∗.

5.1 Linear regression with elastic net regularization

In this subsection  we compare GeoPG-B and APG-B in terms of solving linear regression with
elastic net regularization  a popular problem in machine learning and statistics [20]:

α
2

1
2p

min
x∈Rn

(cid:107)Ax − b(cid:107)2 +

(cid:107)x(cid:107)2 + µ(cid:107)x(cid:107)1 
where A ∈ Rp×n  b ∈ Rp  and α  µ > 0 are the weighting parameters.
We conducted tests on two real datasets downloaded from the LIBSVM repository: a9a  RCV1. The
results are reported in Figure 1. In particular  we tested α = 10−8 and µ = 10−3  10−4  10−5. Note
that since α is very small  the problems are very likely to be ill-conditioned. We see from Figure 1
that GeoPG-B is faster than APG-B on these real datasets  which indicates that GeoPG-B is preferable
than APG-B. In the supplementary material  we show more numerical results on varying α  which
further conﬁrm that GeoPG-B is faster than APG-B when the problems are more ill-conditioned.

(5.1)

(a) Dataset a9a
Figure 1: GeoPG-B and APG-B for solving (5.1) with α = 10−8.

(b) Dataset RCV1

5.2 Logistic regression with elastic net regularization

In this subsection  we compare the performance of GeoPG-B and APG-B in terms of solving the
following logistic regression problem with elastic net regularization:

p(cid:88)

log(cid:0)1 + exp(−bi · a(cid:62)

i x)(cid:1) +

1
p

min
x∈Rn

(5.2)
where ai ∈ Rn and bi ∈ {±1} are the feature vector and class label of the i-th sample  respectively 
and α  µ > 0 are the weighting parameters.

i=1

(cid:107)x(cid:107)2 + µ(cid:107)x(cid:107)1 

α
2

7

0102030405010−1210−1010−810−610−410−2100102CPU(s)(Fk−F*)/F* GeoPG−B: µ =10−3APG−B: µ =10−3GeoPG−B: µ =10−4APG−B: µ =10−4GeoPG−B: µ =10−5APG−B: µ =10−505101520253010−1010−810−610−410−2100102CPU(s)(Fk−F*)/F* GeoPG−B: µ =10−3APG−B: µ =10−3GeoPG−B: µ =10−4APG−B: µ =10−4GeoPG−B: µ =10−5APG−B: µ =10−5We tested GeoPG-B and APG-B for solving (5.2) on the three real datasets a9a  RCV1 and Gisette
from LIBSVM  and the results are reported in Figure 2. In particular  we tested α = 10−8 and
µ = 10−3  10−4  10−5. Figure 2 shows that with the same µ  GeoPG-B is much faster than APG-B.
More numerical results are provided in the supplementary material  which also indicate that GeoPG-B
is much faster than APG-B  especially when the problems are more ill-conditioned.

Figure 2: GeoPG-B and APG-B for solving (5.2) with α = 10−8. Left: dataset a9a; Middle: dataset
RCV1; Right: dataset Gisette.

5.3 Numerical results of L-GeoPG-B

In this subsection  we test GeoPG with limited memory described in Algorithm 5 in solving (5.2)
on the Gisette dataset. Since we still need to use the backtracking technique  we actually tested
L-GeoPG-B. The results with different memory sizes m are reported in Figure 3. Note that m = 0
corresponds to the original GeoPG-B without memory. The subproblem (4.4) is solved using the
function “quadprog” in Matlab. From Figure 3 we see that roughly speaking  L-GeoPG-B performs
better for larger memory sizes  and in most cases  the performance of L-GeoPG-B with m = 100
is the best among the reported results. This indicates that the limited-memory idea indeed helps
improve the performance of GeoPG.

Figure 3: L-GeoPG-B for solving (5.2) on the dataset Gisette with α = 10−8. Left: µ = 10−3;
Middle: µ = 10−4; Right: µ = 10−5.

6 Conclusions

In this paper  we proposed a GeoPG algorithm for solving nonsmooth convex composite problems 
which is an extension of the recent method GeoD that can only handle smooth problems. We proved
that GeoPG enjoys the same optimal rate as Nesterov’s accelerated gradient method for solving
strongly convex problems. The backtracking technique was adopted to deal with the case when the
Lipschitz constant is unknown. Limited-memory GeoPG was also developed to improve the practical
performance of GeoPG. Numerical results on linear regression and logistic regression with elastic net
regularization demonstrated the efﬁciency of GeoPG. It would be interesting to see how to extend
GeoD and GeoPG to tackle non-strongly convex problems  and how to further accelerate the running
time of GeoPG. We leave these questions in future work.
Acknowledgements. Shixiang Chen is supported by CUHK Research Postgraduate Student Grant
for Overseas Academic Activities. Shiqian Ma is supported by a startup funding in UC Davis.

8

010203040506070809010−1010−810−610−410−2100102CPU(s)(Fk−F*)/F* GeoPG−B: µ =10−3APG−B: µ =10−3GeoPG−B: µ =10−4APG−B: µ =10−4GeoPG−B: µ =10−5APG−B: µ =10−501234567810−1010−810−610−410−2100102CPU(s)(Fk−F*)/F* GeoPG−B: µ =10−3APG−B: µ =10−3GeoPG−B: µ =10−4APG−B: µ =10−4GeoPG−B: µ =10−5APG−B: µ =10−5050010001500200025003000350010−1010−810−610−410−2100102104CPU(s)(Fk−F*)/F* GeoPG−B: µ =10−3APG−B: µ =10−3GeoPG−B: µ =10−4APG−B: µ =10−4GeoPG−B: µ =10−5APG−B: µ =10−505010015020025010−1010−810−610−410−2100102CPU(s)(Fk−F*)/F* memorysize=0memorysize=5memorysize=20memorysize=10005010015020025030035010−1010−810−610−410−2100102CPU(s)(Fk−F*)/F* memorysize=0memorysize=5memorysize=20memorysize=10005010015020025030035040010−1010−810−610−410−2100102104CPU(s)(Fk−F*)/F* memorysize=0memorysize=5memorysize=20memorysize=100References
[1] S. Bubeck  Y.-T. Lee  and M. Singh. A geometric alternative to Nesterov’s accelerated gradient

descent. arXiv preprint arXiv:1506.08187  2015.

[2] Y. E. Nesterov. A method for unconstrained convex minimization problem with the rate of

convergence O(1/k2). Dokl. Akad. Nauk SSSR  269:543–547  1983.

[3] Y. E. Nesterov.

Introductory lectures on convex optimization: A basic course. Applied

Optimization. Kluwer Academic Publishers  Boston  MA  2004. ISBN 1-4020-7553-7.

[4] W. Su  S. Boyd  and E. J. Candès. A differential equation for modeling Nesterov’s accelerated

gradient method: Theory and insights. In NIPS  2014.

[5] H. Attouch  Z. Chbani  J. Peypouquet  and P. Redont. Fast convergence of inertial dynamics

and algorithms with asymptotic vanishing viscosity. Mathematical Programming  2016.

[6] L. Lessard  B. Recht  and A. Packard. Analysis and design of optimization algorithms via

integral quadratic constraints. SIAM Journal on Optimization  26(1):57–95  2016.

[7] A. Wibisono  A. Wilson  and M. I. Jordan. A variational perspective on accelerated methods in

optimization. Proceedings of the National Academy of Sciences  133:E7351–E7358  2016.

[8] R. G. Bland  D. Goldfarb  and M. J. Todd. The ellipsoid method: A survey. Operations

Research  29:1039–1091  1981.

[9] S. Bubeck and Y.-T. Lee. Black-box optimization with a politician. ICML  2016.

[10] D. Drusvyatskiy  M. Fazel  and S. Roy. An optimal ﬁrst order method based on optimal quadratic

averaging. SIAM Journal on Optimization  2016.

[11] R. P. Brent. An algorithm with guaranteed convergence for ﬁnding a zero of a function. In
Algorithms for Minimization without Derivatives. Englewood Cliffs  NJ: Prentice-Hall  1973.

[12] T. J. Dekker. Finding a zero by means of successive linear interpolation. In Constructive Aspects

of the Fundamental Theorem of Algebra. London: Wiley-Interscience  1969.

[13] M. Gerdts  S. Horn  and S. Kimmerle. Line search globalization of a semismooth Newton
method for operator equations in Hilbert spaces with applications in optimal control. Journal of
Industrial And Management Optimization  13(1):47–62  2017.

[14] E. Hans and T. Raasch. Global convergence of damped semismooth Newton methods for L1

Tikhonov regularization. Inverse Problems  31(2):025005  2015.

[15] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Imaging Sciences  2(1):183–202  2009.

[16] K. Scheinberg  D. Goldfarb  and X. Bai. Fast ﬁrst-order methods for composite convex
optimization with backtracking. Foundations of Computational Mathematics  14(3):389–417 
2014.

[17] Y. E. Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming  140(1):125–161  2013.

[18] A. Beck. On the convexity of a class of quadratic mappings and its application to the problem of
ﬁnding the smallest ball enclosing a given intersection of balls. Journal of Global Optimization 
39(1):113–126  2007.

[19] Y. C. Eldar  A. Beck  and M. Teboulle. A minimax Chebyshev estimator for bounded error

estimation. IEEE Transactions on Signal Processing  56(4):1388–1397  2008.

[20] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the

Royal Statistical Society  Series B  67(2):301–320  2005.

9

,Shixiang Chen
Shiqian Ma
Wei Liu