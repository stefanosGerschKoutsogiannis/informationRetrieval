2013,Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions,We propose a compressed sensing (CS) calcium imaging framework for monitoring large neuronal populations  where we image randomized projections of the spatial calcium concentration at each timestep  instead of measuring the concentration at individual locations. We develop scalable nonnegative deconvolution methods for extracting the neuronal spike time series from such observations. We also address the problem of demixing the spatial locations of the neurons using rank-penalized matrix factorization methods. By exploiting the sparsity of neural spiking we demonstrate that the number of measurements needed per timestep is significantly smaller than the total number of neurons  a result that can potentially enable imaging of larger populations at considerably faster rates compared to traditional raster-scanning techniques. Unlike traditional CS setups  our problem involves a block-diagonal sensing matrix and a non-orthogonal sparse basis that spans multiple timesteps. We study the effect of these distinctive features in a noiseless setup using recent results relating conic geometry to CS. We provide tight approximations to the number of measurements needed for perfect deconvolution for certain classes of spiking processes  and show that this number displays a phase transition " similar to phenomena observed in more standard CS settings; however  in this case the required measurement rate depends not just on the mean sparsity level but also on other details of the underlying spiking process.",Sparse nonnegative deconvolution for compressive
calcium imaging: algorithms and phase transitions

Eftychios A. Pnevmatikakis and Liam Paninski

Department of Statistics  Center for Theoretical Neuroscience

Grossman Center for the Statistics of Mind  Columbia University  New York  NY

{eftychios  liam}@stat.columbia.edu

Abstract

We propose a compressed sensing (CS) calcium imaging framework for monitoring
large neuronal populations  where we image randomized projections of the spatial
calcium concentration at each timestep  instead of measuring the concentration at
individual locations. We develop scalable nonnegative deconvolution methods for
extracting the neuronal spike time series from such observations. We also address
the problem of demixing the spatial locations of the neurons using rank-penalized
matrix factorization methods. By exploiting the sparsity of neural spiking we
demonstrate that the number of measurements needed per timestep is signiﬁcantly
smaller than the total number of neurons  a result that can potentially enable
imaging of larger populations at considerably faster rates compared to traditional
raster-scanning techniques. Unlike traditional CS setups  our problem involves a
block-diagonal sensing matrix and a non-orthogonal sparse basis that spans multiple
timesteps. We provide tight approximations to the number of measurements needed
for perfect deconvolution for certain classes of spiking processes  and show that
this number undergoes a “phase transition ” which we characterize using modern
tools relating conic geometry to compressed sensing.

1

Introduction

Calcium imaging methods have revolutionized data acquisition in experimental neuroscience; we
can now record from large neural populations to study the structure and function of neural circuits
(see e.g. Ahrens et al. (2013))  or from multiple locations on a dendritic tree to examine the detailed
computations performed at a subcellular level (see e.g. Branco et al. (2010)). Traditional calcium
imaging techniques involve a raster-scanning protocol where at each cycle/timestep the microscope
scans the image in a voxel-by-voxel fashion  or some other predetermined pattern  e.g. through
random access multiphoton (RAMP) microscopy (Reddy et al.  2008)  and thus the number of
measurements per timestep is equal to the number of voxels of interest. Although this protocol
produces “eye-interpretable” measurements  it introduces a tradeoff between the size of the imaged
ﬁeld and the imaging frame rate; very large neural populations can be imaged only with a relatively
low temporal resolution.
This unfavorable situation can potentially be overcome by noticing that many acquired measurements
are redundant; voxels can be “void” in the sense that no neurons are located there  and active voxels
at nearby locations or timesteps will be highly correlated. Moreover  neural activity is typically
sparse; most neurons do not spike at every timestep. During recent years  imaging practitioners have
developed specialized techniques to leverage this redundancy. For example  Nikolenko et al. (2008)
describe a microscope that uses a spatial light modulator and allows for the simultaneous imaging
of different (predeﬁned) image regions. More broadly  the advent of compressed sensing (CS) has
found many applications in imaging such as MRI (Lustig et al.  2007)  hyperspectral imaging (Gehm
et al.  2007)  sub-diffraction microscopy (Rust et al.  2006) and ghost imaging (Katz et al.  2009) 

1

with available hardware implementations (see e.g. Duarte et al. (2008)). Recently  Studer et al.
(2012) presented a ﬂuorescence microscope based on the CS framework  where each measurement
is obtained by projection of the whole image on a random pattern. This framework can lead to
signiﬁcant undersampling ratios for biological ﬂuorescence imaging.
In this paper we propose the application of the imaging framework of Studer et al. (2012) to the case
of neural population calcium imaging to address the problem of imaging large neural populations with
high temporal resolution. The basic idea is to not measure the calcium at each location individually 
but rather to take a smaller number of “mixed” measurements (based on randomized projections of
the data). Then we use convex optimization methods that exploit the sparse structure in the data in
order to simultaneously demix the information from the randomized projection observations and
deconvolve the effect of the slow calcium indicator to recover the spikes. Our results indicate that the
number of required randomized measurements scales merely with the number of expected spikes
rather than the ambient dimension of the signal (number of voxels/neurons)  allowing for the fast
monitoring of large neural populations. We also address the problem of estimating the (potentially
overlapping) spatial locations of the imaged neurons and demixing these locations using methods for
nuclear norm minimization and nonnegative matrix factorization. Our methods scale linearly with
the experiment length and are largely parallelizable  ensuring computational tractability. Our results
indicate that calcium imaging can be potentially scaled up to considerably larger neuron populations
and higher imaging rates by moving to compressive signal acquisition.
In the traditional static compressive imaging paradigm the sensing matrix is dense; every observation
comes from the projection of all the image voxels to a random vector/matrix. Moreover  the underlying
image can be either directly sparse (most of the voxels are zero) or sparse in some orthogonal basis
(e.g. Fourier  or wavelet). In our case the sensing matrix has a block-diagonal form (we can only
observe the activity at one speciﬁc time in each measurement) and the sparse basis (which corresponds
to the inverse of the matrix implementing the convolution of the spikes from the calcium indicator) is
non-orthogonal and spans multiple timelags. We analyze the effect of these distinctive features in
Sec. 3 in a noiseless setting. We show that as the number of measurements increases  the probability of
successful recovery undergoes a phase transition  and study the resulting phase transition curve (PTC) 
i.e.  the number of measurements per timestep required for accurate deconvolution as a function of
the number of spikes. Our analysis uses recent results that connect CS with conic geometry through
the “statistical dimension” (SD) of descent cones (Amelunxen et al.  2013). We demonstrate that in
many cases of interest  the SD provides a very good estimate of the PTC.

2 Model description and approximate maximum-a-posteriori inference

See e.g. Vogelstein et al. (2010) for background on statistical models for calcium imaging data. Here
we assume that at every timestep an image or light ﬁeld (either two- or three-dimensional) is observed
for a duration of T timesteps. Each observed ﬁeld contains a total number of d voxels and can be
vectorized in a single column vector. Thus all the activity can be described by d × T matrix F . Now
assume that the ﬁeld contains a total number of N neurons  where N is in general unknown. Each
spike causes a rapid increase in the calcium concentration which then decays with a time constant
that depends on the chemical properties of the calcium indicator. For each neuron i we assume that
the “calcium activity” ci can be described as a stable autoregressive process AR(1) process1 that
ﬁlters the neuron’s spikes si(t) according to the fast-rise slow-decay procedure described before:

ci(t) = γci(t − 1) + si(t) 

(1)
where γ is the discrete time constant which satisﬁes 0 < γ < 1 and can be approximated as
γ = 1 − exp(−∆t/τ )  where ∆t is the length of each timestep and τ is the continuous time constant
of the calcium indicator. In general we assume that each si(t) is binary due to the small length of
the timestep in the proposed compressive imaging setting  and we use an i.i.d. prior for each neuron
p(si(t) = 1) = πi.2 Moreover  let ai ∈ Rd
+ the (nonnegative) location vector for neuron i  and
b ∈ Rd
+ the (nonnegative) vector of baseline concentration for all the voxels. The spatial calcium
concentration proﬁle at time t can be described as

(cid:88)N

i=1

f (t) =

aici(t) + b.

(2)

1Generalization to general AR(p) processes is straightforward  but we keep p = 1 for simplicity.
2This choice is merely for simplicity; more general prior distributions can be incorporated in our framework.

2

In conventional raster-scanning experiments  at each timestep we observe a noisy version of the d-
dimensional image f (t). Since d is typically large  the acquisition of this vector can take a signiﬁcant
amount of time  leading to a lengthy timestep ∆t and low temporal resolution. Instead  we propose
to observe the projections of f (t) onto a random matrix Bt ∈ Rn×d (e.g. each entry of Bt could be
chosen as 0 or 1 with probability 0.5):

y(t) = Btf (t) + εt 

εt ∼ N (0  Σt) 

(3)

where εt denotes measurement noise (Gaussian  with diagonal covariance Σt  for simplicity). If
n = dim(y(t)) satisﬁes n (cid:28) d  then y(t) represents a compression of f (t) that can potentially be
obtained more quickly than the full f (t). Now if we can use statistical methods to recover f (t) (or
equivalently the location ai and spikes si of each neuron) from the compressed measurements y(t) 
the total imaging throughput will be increased by a factor proportional to the undersampling ratio
d/n. Our assumption here is that the random projection matrices Bt can be constructed quickly.
Recent technological innovations have enabled this fast construction by using digital micromirror
devices that enable spatial light modulation and can construct different excitation patterns with a high
frequency (order of kHz). The total ﬂuorescence can then be detected with a single photomultiplier
tube. For more details we refer to Duarte et al. (2008); Nikolenko et al. (2008); Studer et al. (2012).
We discuss the statistical recovery problem next. For future reference  note that eqs. (1)-(3) can be
written in matrix form as (vec(·) denotes the vectorizing operator)

 1

−γ
...

0

0
. . .
1
. . .
...
...
. . . −γ

0
0

...

1

   B = blkdiag{B1  . . .   BT}.

(4)

S = CGT
F = AC + b1T
T

with G =

vec(Y ) = Bvec(F ) + ε 

2.1 Approximate MAP inference with an interior point method

For now we assume that A is known. In general MAP inference of S is difﬁcult due to the discrete
nature of S. Following Vogelstein et al. (2010) we relax S to take continuous values in the interval
[0  1] (remember that we assume binary spikes)  and appropriately modify the prior for si(t) to
log p(si(t)) ∝ −(λisi(t))1(0 ≤ si(t) ≤ 1)  where λi is chosen such that the relaxed prior has the
same mean πi. To exploit the banded structure of G we seek the MAP estimate of C (instead of S)
by solving the following convex quadratic problem (we let ¯y(t) = y(t) − Btb)

(cid:88)T

1
2

minimize
subject to 0 ≤ CGT ≤ 1  c(1) ≥ 0 

t=1

C

( ¯y(t) − BtAc(t))T Σ−1

t ( ¯y(t) − BtAc(t)) − log p(C)

(P-QP)

Using the prior on S and the relation S = CGT   the log-prior of C can be written as log p(C) ∝
−λT CGT 1T .We can solve (P-QP) efﬁciently using an interior point method with a log-barrier
(Vogelstein et al.  2010). The contribution of the likelihood term to the Hessian is a block-diagonal
matrix  whereas the barrier-term will contribute a block-tridiagonal matrix where each non-zero
block is diagonal. As a result the Newton search direction −H−1∇ can be computed efﬁciently in
O(T N 3) time using a block version of standard forward-backward methods for tridiagonal systems
of linear equations. We note that if N is large this can be inefﬁcient. In this case we can use an
augmented Lagrangian method (Boyd et al.  2011) to derive a fully parallelizable ﬁrst order method 
with O(T N ) complexity per iteration. We refer to the supplementary material for additional details.
As a ﬁrst example we consider a simple setup where all the parameters are assumed to be known.
We consider N = 50 neurons observed over T = 1000 timesteps. We assume that A  b are known 
with A = IN (corresponding to non-overlapping point neurons  with one neuron in each voxel) and
b = 0  respectively. This case of known point neurons can be thought as the compressive analog
of RAMP microscopy where the neuron locations are predetermined and then imaged in a serial
manner. (We treat the case of unknown and possibly overlapping neuron locations in section 2.2.)
Each neuron was assumed to ﬁre in an i.i.d. fashion with probability per timestep p = 0.04. Each
measurement was obtained by projecting the spatial ﬂuorescence vector at time t  f (t)  onto a random
matrix Bt. Each row of Bt is taken as an i.i.d. normalized vector 2β/
N  where β has i.i.d. entries
following a fair Bernoulli distribution. For each set of measurements we assume that Σt = σ2In  and

√

3

Figure 1: Performance of proposed algorithm under different noise levels. A: True traces  B:
Estimated traces with n = 5 (10× undersampling)  SNR = 20dB. C: Estimated traces with n = 20
(2.5× undersampling)  SNR = 20dB. D: True and estimated spikes from the traces shown in panels
B and C for a randomly selected neuron. E: Relative error between true and estimated traces for
different number of measurements per timestep under different noise levels. The error decreases with
the number of observations and the reconstruction is stable with respect to noise.

the signal-to-noise ratio (SNR) in dB is deﬁned as SNR = 10 log10(Var[βT f (t)]/N σ2); a quick
calculation reveals that SNR = 10 log10(p(1 − p)/(1 − γ2)σ2).
Fig. 1 examines the solution of (P-QP) when the number of measurements per timestep n varied from
1 to N and for 8 different SNR values 0  5  . . .   30 plus the noiseless case (SNR = ∞). Fig. 1A
shows the noiseless traces for all the neurons and panels B and C show the reconstructed traces for
SNR = 20dB and n = 5  20 respectively. Fig. 1D shows the estimated spikes for these cases for a
randomly picked neuron. For very small number of measurements (n = 5  i.e.  10× undersampling)
the inferred calcium traces (Fig. 1B) already closely resemble the true traces. However  the inferred
MAP values of the spikes (computed by S = CGT   essentially a differencing operation here) lie
in the interior of [0  1]  and the results are not directly interpretable at a high temporal resolution.
As n increases (n = 20  red) the estimated spikes lie very close to {0  1} and a simple thresholding
procedure can recover the true spike times. In Fig. 1E the relative error between the estimated and
true traces ((cid:107)C − ˆC(cid:107)F /(cid:107)C(cid:107)F   with (cid:107)·(cid:107)F denoting the the Frobenius norm) is plotted. In general the
error decreases with the number of observations and the reconstruction is robust with noise. Finally 
by observing the noiseless case (dashed curve) we see that when n ≥ 13 the error becomes practically
zero indicating fully compressed acquisition of the calcium traces with a roughly 4× undersampling
factor. We will see below that this undersampling factor is inversely proportional to the ﬁring rate:
we can recover highly sparse spike signals S using very few measurements n.

2.2 Estimation of the spatial matrix A

The above algorithm assumes that the underlying neurons have known locations  i.e.  the matrix
A is known. In some cases A can be estimated a-priori by running a conventional raster-scanning
experiment at a high spatial resolution and locating the active voxels. However this approach is
expensive and can still be challenging due to noise and possible spatial overlap between different
neurons. To estimate A within the compressive framework we note that the baseline-subtracted
spatiotemporal calcium matrix F (see eqs. (2) and (4)) can be written as ¯F = F − b1T
T = AC; thus
rank( ¯F ) ≤ N where N is the number of underlying neurons  with typically N (cid:28) d. Since N is also
in general unknown we estimate ¯F by solving a nuclear norm penalized problem (Recht et al.  2010)

( ¯y(t) − Bt

¯f (t))T Σ−1

t ( ¯y(t) − Bt

¯f (t)) − log p( ¯F ) + λN N(cid:107) ¯F(cid:107)∗

1
2

(P-NN)

T(cid:88)

t=1

minimize

¯F

subject to

¯F GT ≥ 0  ¯f (1) ≥ 0 

4

True tracesA1020304050Neuron idEstimated traces  5 meas.  SNR = 20dBB1020304050TimestepEstimated traces  20 meas.  SNR = 20dBC100200300400500600700800900100010203040500100200300400500600700800900100001TimestepEstimated Spikes  SNR = 20db  DTrue5 meas.20 meas.510152025303540455010−410−310−210−1100 # of measurements per timestep Relative error  EInf30 dB25 dB20 dB15 dB10 dB5 dB0 dBF

where (cid:107) · (cid:107)∗ denotes the nuclear norm (NN) of a matrix (i.e.  the sum of its singular values)  which is
a convex approximation to the nonconvex rank function (Fazel  2002). The prior of ¯F can be chosen
in a similar fashion as log p(C)  i..e  log p( ¯F ) ∝ −λT
¯F GT 1T   where λF ∈ Rd. Although more
complex than (P-QP)  (P-NN) is again convex and can be solved efﬁciently using e.g. the ADMM
method of Boyd et al. (2011). From the solution of (P-NN) we can estimate N by appropriately
thresholding the singular values of the estimated ¯F .3 Having N we can then use appropriately
constrained nonnegative matrix factorization (NMF) methods to alternately estimate A and C. Note
that during this NMF step the baseline vector b can also be estimated jointly with A. Since NMF
methods are nonconvex  and thus prone to local optima  informative initialization is important. We
can use the solution of (P-NN) to initialize the spatial component A using clustering methods  similar
to methods typically used in neuronal extracellular spike sorting (Lewicki  1998). Details are given
in the supplement (along with some discussion of the estimation of the other parameters in this
problem); we refer to Pnevmatikakis et al. (2013) for full details.

Figure 2: Estimating locations and calcium concentration from compressive calcium imaging mea-
surements. A: True spatiotemporal concentration B: estimate by solving (P-NN) C: estimate by using
NMF methods. D: Logarithmic plot of the ﬁrst singular values of the solution of (P-NN)  E: Estima-
tion of baseline vector  F: true spatial locations G: estimated spatial locations. The NN-penalized
method estimates the number of neurons and the NMF algorithm recovers the spatial and temporal
components with high accuracy.

In Fig. 2 we present an application of this method to an example with N = 8 spatially overlapping
neurons. For simplicity we consider neurons in a one-dimensional ﬁeld with total number of voxels
d = 128 and spatial positions shown in Fig. 2E. At each timestep we obtain just n = 5 noisy
measurements using random projections on binary masks. From the solution to the NN-penalized
problem (P-NN) (Fig. 2B) we threshold the singular values (Fig. 2D) and estimate the number of
underlying neurons (note the logarithmic gap between the 8th and 9th largest singular values that
enables this separation). We then use the NMF approach to obtain ﬁnal estimates of the spatial
locations (Fig. 2G)  the baseline vector (Fig. 2E)  and the full spatiotemporal concentration (Fig. 2C).
The estimates match well with the true values. Note that n < N (cid:28) d showing that compressive
imaging with signiﬁcant undersampling factors is possible  even in the case of classical raster scanning
protocol where the spatial locations are unknown.

3 Estimation of the phase transition curve in the noiseless case

The results presented above indicate that reconstruction of the spikes is possible even with signiﬁcant
undersampling. In this section we study this problem from a compressed sensing (CS) perspective
in the idealized case where the measurements are noiseless. For simplicity  we also assume that
A = I (similar to a RAMP setup). Unlike the traditional CS setup  where a sparse signal (in some
basis) is sensed with a dense fully supported random matrix  in our case the sensing matrix B has a
block-diagonal form. A standard justiﬁcation of CS approaches proceeds by establishing that the
sensing matrix satisﬁes the “restricted isometry property” (RIP) for certain classes of sparse signals

3To reduce potential shrinkage but promote low-rank solutions this “global” NN penalty can be replaced by a

series of “local” NN penalties on spatially overlapping patches.

5

True ConcentrationTimestepVoxel #A100200300400500600700800900100020406080100120NN EstimateTimestepB1002003004005006007008009001000NMF EstimateTimestepC10020030040050060070080090010002468101214100101102Singular Value ScalingD00.5100.20.40.60.81TrueEstimateBaseline estimationE20406080100120True LocationsVoxel #F20406080100120Estimated LocationsVoxel #Gwith high probability (w.h.p.); this property in turn guarantees the correct recovery of the parameters
of interest (Candes and Tao  2005). Yap et al. (2011) showed that for signals that are sparse in some
orthogonal basis  the RIP holds for random block-diagonal matrices w.h.p. with a number of sufﬁcient
measurement that scales with the squared coherence between the sparse basis and the elementary
(identity) basis. For non-orthogonal basis the RIP property has only been established for fully dense
sensing matrices (Candes et al.  2011). For signals with sparse variations Ba et al. (2012) established
perfect and stable recovery conditions under the assumption that the sensing matrix at each timestep
satisﬁes certain RIPs  and the sparsity level at each timestep has known upper bounds.
While the RIP is a valuable tool for the study of convex relaxation approaches to compressed sensing
problems  its estimates are usually up to a constant and can be relatively loose (Blanchard et al. 
2011). An alternative viewpoint is offered from conic geometric arguments (Chandrasekaran et al. 
2012; Amelunxen et al.  2013) that examine how many measurements are required such that the
convex relaxed program will have a unique solution which coincides with the true sparse solution.
We use this approach to study the theoretical properties of our proposed compressed calcium imaging
framework in an idealized noiseless setting. When noise is absent  the quadratic program (P-QP) for
the approximate MAP estimate converges to a linear program4:

minimize

(cid:26) (v ⊗ 1N )T vec(C) 

C

(G ⊗ Id)vec(C) ≥ 0

f (C)  subject to: Bvec(C) = vec(Y )

(P-LP)

f (C) =

with
Here ⊗ denotes the Kronecker product and we used the identity vec(CGT ) = (G ⊗ Id)vec(C). To
examine the properties of (P-LP) we follow the approach of Amelunxen et al. (2013): For a fully
dense sensing i.i.d. Gaussian (or random rotation) matrix B  the linear program (P-LP) will succeed
w.h.p. to reconstruct the true solution C0  if the total number of measurements nT satisﬁes

∞  otherwise

  and v = GT 1T .

(5)
D(f  C0) is the descent cone of f at C0  induced by the set of non-increasing directions from C0  i.e. 

T N ).

nT ≥ δ(D(f  C0)) + O(

√

D(f  C0) = ∪τ≥0

(6)
and δ(C) is the “statistical dimension” (SD) of a convex cone C ⊆ Rm  deﬁned as the expected
squared length of a standard normal Gaussian vector projected onto the cone

(cid:8)y ∈ RN×T : f (C0 + τ y) ≤ f (C0)(cid:9)  

δ(C) = Eg(cid:107)ΠC(g)(cid:107)2  with g ∼ N (0  Im).

Eq. (5)  and the analysis of Amelunxen et al. (2013)  state that as T N → ∞  the probability that
(P-LP) will succeed to ﬁnd the true solution undergoes a phase transition  and that the phase transition
curve (PTC)  i.e.  the number of measurements required for perfect reconstruction normalized by
the ambient dimension N T (Donoho and Tanner  2009)  coincides with the normalized SD. In our
case B is a block-diagonal matrix (not a fully-dense Gaussian matrix)  and the SD only provides an
estimate of the PTC. However  as we show below  this estimate is tight in most cases of interest.

3.1 Computing the statistical dimension

Using a result from Amelunxen et al. (2013) the statistical dimension can also be expressed as the
expected squared distance of a standard normal vector from the cone induced by the subdifferential
(Rockafellar  1970) ∂f of f at the true solution C0:

δ(D(f  C0) = Eg inf

min

u∈τ ∂f (C0)

τ >0

(cid:107)g − u(cid:107)2  with g ∼ N (0  IN T ).

(7)

Although in general (7) cannot be solved in closed form  it can be easily estimated numerically; in the
supplementary material we show that the subdifferential ∂f (C0) takes the form of a convex polytope 
i.e.  an intersection of linear half spaces. As a result  the distance of any vector g from ∂f (C0) can
be found by solving a simple quadratic program  and the statistical dimension can be estimated with
a simple Monte-Carlo simulation (details are presented in the supplement). The characterization
of (7) also explains the effect of the sparsity pattern on the SD. In the case where the sparse basis

4To illustrate the generality of our approach we allow for arbitrary nonnegative spike values in this analysis 

but we also discuss the binary case that is of direct interest to our compressive calcium framework.

6

is the identity then the cone induced by the subdifferential can be decomposed as the union of the
respective subdifferential cones induced by each coordinate. It follows that the SD is invariant to
coordinate permutations and depends only on the sparsity level  i.e.  the number of nonzero elements.
However  this result is in general not true for a nonorthogonal sparse basis  indicating that the precise
location of the spikes (sparsity pattern) and not just their number has an effect on the SD. In our case
the calcium signal is sparse in the non-orthogonal basis described by the matrix G from (4).

3.2 Relation with the phase transition curve

In this section we examine the relation of the SD with the PTC for our compressive calcium imaging
problem. Let S denote the set of spikes  Ω = supp(S)  and C the induced calcium traces C = SG−T .
As we argued  the statistical dimension of the descent cone D(f  C) depends both on the cardinality
of the spike set |Ω| (sparsity level) and the location of the spikes (sparsity pattern). To examine
the effects of the sparsity level and pattern we deﬁne the normalized expected statistical dimension
(NESD) with respect to a certain distribution (e.g. Bernoulli) χ from which the spikes S are drawn.

˜δ(k/N T  χ) = EΩ∼χ [δ(D(f  C))/N T ]   with supp(S) = Ω  and EΩ∼χ|Ω| = k.

In Fig. 3 we examine the relation of the NESD with the phase transition curve of the noiseless problem
(P-LP). We consider a setup with N = 40 point neurons (A = Id  d = N ) observed over T = 50
timesteps and chose discrete time constant γ = 0.99. The spike-times of each neuron came from
the same distribution and we considered two different distributions: (i) Bernoulli spiking  i.e.  each
neuron ﬁres i.i.d. spikes with the probability k/T   and (ii) desynchronized periodic spiking where
each neuron ﬁres deterministically spikes with discrete frequency k/T timesteps−1  and each neuron
has a random phase. We considered two forms of spikes: (i) with nonnegative values (si(t) ≥ 0) 
and (ii) with binary values (si(t) = {0  1})  and we also considered two forms of sensing matrices:
(i) with time-varying matrix Bt  and (ii) with constant  fully supported matrices B1 = . . . = BT .
The entries of each Bt are again drawn from an i.i.d. fair Bernoulli distribution. For each of these
8 different conditions we varied the expected number of spikes per neuron k from 1 to T and the
number of observed measurements n from 1 to N. Fig. 3 shows the empirical probability that the
program (P-LP) will succeed in reconstructing the true solution averaged over a 100 repetitions.
Success is declared when the reconstructed spike signal ˆS satisﬁes5 (cid:107) ˆS − S(cid:107)F /(cid:107)S(cid:107)F < 10−3. We
also plot the empirical PTC (purple dashed line)  i.e.  the empirical 50% success probability line  and
the NESD (solid blue line)  approximated with a Monte Carlo simulation using 200 samples  for each
of the four distinct cases (note that the SD does not depend on the structure of the sensing matrix B).
In all cases  our problem undergoes a sharp phase transition as the number of measurements per
timestep varies: in the white regions of Fig. 3  S is recovered essentially perfectly  with a transition
to a high probability of at least some errors in the black regions. Note that the phase transitions are
deﬁned as functions of the sparsity index k/T ; the signal sparsity sets the compressibility of the data.
In addition  in the case of time-varying Bt  the NESD provides a surprisingly good estimate of the
PTC  especially in the binary case or when the spiking signal is actually sparse (k/T < 0.5)  a result
that justiﬁes our overall approach. Although using time-varying sensing matrices Bt leads to better
results  compression is also possible with a constant B. This is an important result for implementation
purposes where changing the sensing matrix might be a costly or slow operation. On a more technical
side we also observe the following interesting properties:
• Periodic spiking requires more measurements for accurate deconvolution  a property again
predicted by the SD. This comes from the fact that the sparse basis is not orthogonal and
shows that for a ﬁxed sparsity level k/T the sparsity pattern also affects the number of required
measurements. This difference depends on the time constant γ. As γ → 0  G → I; the problem
becomes equivalent to a standard nonnegative CS problem  where the spike pattern is irrelevant.
• In the Bernoulli spiking nonnegative case  the SD is numerically very close to the PTC of the
standard nonnegative CS problem (not shown here)  adding to the growing body of evidence for
universal behavior of convex relaxation approaches to CS (Donoho and Tanner  2009).
• In the binary case the results exhibit a symmetry around the axis k/T = 0.5. In fact this symmetry
becomes exact as γ → 1. In the supplement we prove that this result is predicted by the SD.
5When calculating this error we excluded the last 10 timesteps. As every spike is ﬁltered by the AR process
it has an effect for multiple timelags in the future and an optimal encoder has to sense it over multiple timelags.
This number depends only on γ and not on the length T   and thus this behavior becomes negligible as T → ∞.

7

Figure 3: Relation of the statistical dimension with the phase transition curve for two different
spiking distributions (Bernouli  periodic)  two different spike values (nonnegative  binary)  and two
classes of sensing matrices (time-varying  constant). For each panel: x-axis normalized sparsity k/T  
y-axis undersampling index n/N. Each panel shows the empirical success probability for each pair
(k/T  n/N )  the empirical 50%-success line (dashed purple line) and the SD (blue solid line). When
B is time-varying the SD provides a good estimate of the empirical PTC.

As mentioned above  our analysis is only approximate since B is block-diagonal and not fully
dense. However  this approximation is tight in the time-varying case. Still  it is possible to construct
adversarial counterexamples where the SD approach fails to provide a good estimate of the PTC.
For example  if all neurons ﬁre in a completely synchronized manner then the required number
of measurements grows at a rate that is not predicted by (5). We present such an example in the
supplement and note that more research is needed to understand such extreme cases.

4 Conclusion

We proposed a framework for compressive calcium imaging. Using convex relaxation tools from
compressed sensing and low rank matrix factorization  we developed an efﬁcient method for extracting
neurons’ spatial locations and the temporal locations of their spikes from a limited number of
measurements  enabling the imaging of large neural populations at potentially much higher imaging
rates than currently available. We also studied a noiseless version of our problem from a compressed
sensing point of view using newly introduced tools involving the statistical dimension of convex
cones. Our analysis can in certain cases capture the number of measurements needed for perfect
deconvolution  and helps explain the effects of different spike patterns on reconstruction performance.
Our approach suggests potential improvements over the standard raster scanning protocol (unknown
locations) as well as the more efﬁcient RAMP protocol (known locations). However our analysis is
idealistic and neglects several issues that can arise in practice. The results of Fig. 1 suggest a tradeoff
between effective compression and SNR level. In the compressive framework the cycle length can be
relaxed more easily due to the parallel nature of the imaging (each location is targeted during the
whole “cycle”). The summed activity is then collected by the photomultiplier tube that introduces the
noise. While the nature of this addition has to be examined in practice  we expect that the observed
SNR will allow for signiﬁcant compression. Another important issue is motion correction for brain
movement  especially in vivo conditions. While new approaches have to be derived for this problem 
the novel approach of Cotton et al. (2013) could be adaptable to our setting. We hope that our work
will inspire experimentalists to leverage the proposed advanced signal processing methods to develop
more efﬁcient imaging protocols.

Acknowledgements
LP is supported from an NSF career award. This work is also supported by ARO MURI W911NF-12-
1-0594.

8

Bernouli spikingUndersampling Index0.10.20.30.40.50.60.70.80.91Nonnegative SpikesSparsity Index  0.10.20.30.40.50.60.70.80.91Statistical dimensionEmpirical PTCBinary SpikesSparsity IndexPeriodic spikingUndersampling IndexTime−varying B0.20.40.60.810.10.20.30.40.50.60.70.80.91Constant B0.20.40.60.81Time−varying B0.20.40.60.810.10.20.30.40.50.60.70.80.91Constant B  0.20.40.60.8100.10.20.30.40.50.60.70.80.91References
Ahrens  M. B.  M. B. Orger  D. N. Robson  J. M. Li  and P. J. Keller (2013). Whole-brain functional imaging at

cellular resolution using light-sheet microscopy. Nature methods 10(5)  413–420.

Amelunxen  D.  M. Lotz  M. B. McCoy  and J. A. Tropp (2013). Living on the edge: A geometric theory of

phase transitions in convex optimization. arXiv preprint arXiv:1303.6672.

Ba  D.  B. Babadi  P. Purdon  and E. Brown (2012). Exact and stable recovery of sequences of signals with
sparse increments via differential l1-minimization. In Advances in Neural Information Processing Systems
25  pp. 2636–2644.

Blanchard  J. D.  C. Cartis  and J. Tanner (2011). Compressed sensing: How sharp is the restricted isometry

property? SIAM review 53(1)  105–125.
Boyd  S.  N. Parikh  E. Chu  B. Peleato  and J. Eckstein (2011). Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning 3(1) 
1–122.

Branco  T.  B. A. Clark  and M. H¨ausser (2010). Dendritic discrimination of temporal input sequences in cortical

neurons. Science 329  1671–1675.

Candes  E. J.  Y. C. Eldar  D. Needell  and P. Randall (2011). Compressed sensing with coherent and redundant

dictionaries. Applied and Computational Harmonic Analysis 31(1)  59–73.

Candes  E. J. and T. Tao (2005). Decoding by linear programming. Information Theory  IEEE Transactions

on 51(12)  4203–4215.

Chandrasekaran  V.  B. Recht  P. A. Parrilo  and A. S. Willsky (2012). The convex geometry of linear inverse

problems. Foundations of Computational Mathematics 12(6)  805–849.

Cotton  R. J.  E. Froudarakis  P. Storer  P. Saggau  and A. S. Tolias (2013). Three-dimensional mapping of

microcircuit correlation structure. Frontiers in Neural Circuits 7.

Donoho  D. and J. Tanner (2009). Observed universality of phase transitions in high-dimensional geometry  with
implications for modern data analysis and signal processing. Philosophical Transactions of the Royal Society
A: Mathematical  Physical and Engineering Sciences 367(1906)  4273–4293.

Duarte  M. F.  M. A. Davenport  D. Takhar  J. N. Laska  T. Sun  K. F. Kelly  and R. G. Baraniuk (2008).

Single-pixel imaging via compressive sampling. Signal Processing Magazine  IEEE 25(2)  83–91.

Fazel  M. (2002). Matrix rank minimization with applications. Ph. D. thesis  Stanford University.
Gehm  M.  R. John  D. Brady  R. Willett  and T. Schulz (2007). Single-shot compressive spectral imaging with a

dual-disperser architecture. Opt. Express 15(21)  14013–14027.

Katz  O.  Y. Bromberg  and Y. Silberberg (2009). Compressive ghost imaging. Applied Physics Letters 95(13).
Lewicki  M. (1998). A review of methods for spike sorting: the detection and classiﬁcation of neural action

potentials. Network: Computation in Neural Systems 9  R53–R78.

Lustig  M.  D. Donoho  and J. M. Pauly (2007). Sparse MRI: The application of compressed sensing for rapid

MR imaging. Magnetic resonance in medicine 58(6)  1182–1195.

Nikolenko  V.  B. Watson  R. Araya  A. Woodruff  D. Peterka  and R. Yuste (2008). SLM microscopy: Scanless
two-photon imaging and photostimulation using spatial light modulators. Frontiers in Neural Circuits 2  5.
Pnevmatikakis  E.  T. Machado  L. Grosenick  B. Poole  J. Vogelstein  and L. Paninski (2013). Rank-penalized
nonnegative spatiotemporal deconvolution and demixing of calcium imaging data. In Computational and
Systems Neuroscience Meeting COSYNE. (journal paper in preparation for PLoS Computational Biology).
Recht  B.  M. Fazel  and P. Parrilo (2010). Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization. SIAM review 52(3)  471–501.

Reddy  G.  K. Kelleher  R. Fink  and P. Saggau (2008). Three-dimensional random access multiphoton

microscopy for functional imaging of neuronal activity. Nature Neuroscience 11(6)  713–720.

Rockafellar  R. (1970). Convex Analysis. Princeton University Press.
Rust  M. J.  M. Bates  and X. Zhuang (2006). Sub-diffraction-limit imaging by stochastic optical reconstruction

microscopy (STORM). Nature methods 3(10)  793–796.

Studer  V.  J. Bobin  M. Chahid  H. S. Mousavi  E. Candes  and M. Dahan (2012). Compressive ﬂuorescence mi-
croscopy for biological and hyperspectral imaging. Proceedings of the National Academy of Sciences 109(26) 
E1679–E1687.

Vogelstein  J.  A. Packer  T. Machado  T. Sippy  B. Babadi  R. Yuste  and L. Paninski (2010). Fast non-negative
deconvolution for spike train inference from population calcium imaging. Journal of Neurophysiology 104(6) 
3691–3704.

Yap  H. L.  A. Eftekhari  M. B. Wakin  and C. J. Rozell (2011). The restricted isometry property for block
diagonal matrices. In Information Sciences and Systems (CISS)  2011 45th Annual Conference on  pp. 1–6.

9

,Eftychios Pnevmatikakis
Liam Paninski