2014,Multitask learning meets tensor factorization: task imputation via convex optimization,We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices  which can be e.g  (consumer  time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However  we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm  which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion  multitask learning  and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.,Multitask learning meets tensor factorization: task

imputation via convex optimization

Kishan Wimalawarne

Tokyo Institute of Technology

Meguro-ku  Tokyo  Japan

Masashi Sugiyama

The University of Tokyo
Bunkyo-ku  Tokyo  Japan

kishan@sg.cs.titech.ac.jp

sugi@k.u-tokyo.ac.jp

Ryota Tomioka

TTI-C

Illinois  Chicago  USA
tomioka@ttic.edu

Abstract

We study a multitask learning problem in which each task is parametrized by a
weight vector and indexed by a pair of indices  which can be e.g  (consumer 
time). The weight vectors can be collected into a tensor and the (multilinear-)rank
of the tensor controls the amount of sharing of information among tasks. Two
types of convex relaxations have recently been proposed for the tensor multilin-
ear rank. However  we argue that both of them are not optimal in the context of
multitask learning in which the dimensions or multilinear rank are typically het-
erogeneous. We propose a new norm  which we call the scaled latent trace norm
and analyze the excess risk of all the three norms. The results apply to various set-
tings including matrix and tensor completion  multitask learning  and multilinear
multitask learning. Both the theory and experiments support the advantage of the
new norm when the tensor is not equal-sized and we do not a priori know which
mode is low rank.

1 Introduction

We consider supervised multitask learning problems [1  6  7] in which the tasks are indexed by a
pair of indices known as multilinear multitask learning (MLMTL) [17  19]. For example  when we
would like to predict the ratings of different aspects (e.g.  quality of service  food  etc) of restaurants
by different customers  the tasks would be indexed by aspects (cid:2) customers. When each task is
parametrized by a weight vector over features  the goal would be to learn a features (cid:2) aspects (cid:2)
customers tensor. Another possible task dimension would be time  since the ratings may change
over time.
This setting is interesting  because it would allow us to exploit the similarities across different cus-
tomers as well as similarities across different aspects or time-points. Furthermore this would allow
us to perform task imputation  that is to learn weights for tasks for which we have no training exam-
ples. On the other hand  the conventional matrix-based multitask learning (MTL) [2  3  13  16] may
fail to capture the higher order structure if we consider learning a ﬂat features (cid:2) tasks matrix and
would require at least r samples  where r is the rank of the matrix to be learned  for each task.
Recently several norms that induce low-rank tensors in the sense of Tucker decomposition or multi-
linear singular value decomposition [8  9  14  25] have been proposed. The mean squared error for
recovering a n1 (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) nK tensor of multilinear rank (r1; : : : ; rK) from its noisy version scale as
nk)2) for the overlapped trace norm [23]. On the other hand  the
O(( 1
K
error of the latent trace norm scales as O(mink rk= mink nk) in the same setting [21]. Thus while
the latent trace norm has the better dependence in terms of the multilinear rank rk  it has the worse
dependence in terms of the dimensions nk.
Tensors that arise in multitask learning typically have heterogeneous dimensions. For example 
the number of aspects for a restaurant (quality of service  food  atmosphere  etc.) would be much

p
K
k=1 1=

rk)2( 1
K

∑

p

K
k=1

∑

1

Table 1: Tensor denoising performance using different norms. The mean squared error jjj ^W (cid:0)
W(cid:3)jjj2

F =N is shown for the denoising algorithms (3) using different norms for tensors.

Overlapped trace norm
p
nk
1=

p

rk

1
K

Op

1
K

)2(

K∑

k=1

((

K∑

k=1

)

)2

(

)

(

)

Latent trace norm

Scaled latent trace norm

Op

min

k

rk= min

k

nk

Op

min

k

(rk=nk)

smaller than the number of customers or the number of features. In addition  it is a priori unclear
which mode (or dimension) would have the most redundancy or sharing that could be exploited by
multitask learning. Some of the modes may have full ranks if there is no sharing of information
along them. Therefore  both the latent trace norm and the overlapped trace norm would suffer either
from the heterogeneous multilinear rank or the heterogeneous dimensions in this context.
In this paper  we propose a modiﬁcation to the latent trace norm whose mean squared error scales as
O(mink(rk=nk)) in the same setting  which is better than both the previously proposed extensions
of trace norm for tensors. We study the excess risk of the three norms through their Rademacher
complexities in various settings including matrix completion  multitask learning  and MLMTL. The
new analysis allows us to also study the tensor completion setting  which was only empirically
studied in [22  23]. Our analysis consistently shows the advantage of the proposed scaled latent
trace norm in various settings in which the dimensions or ranks are heterogeneous. Experiments on
both synthetic and real data sets are also consistent with our theoretical ﬁndings.

2 Norms for tensors and their denoising performance
Let W 2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK be a K-way tensor. We denote the total number of entries by N :=
k=1 nk.
A mode-k ﬁber of W is an nk dimensional vector we obtain by ﬁxing all but the kth index. The
mode-k unfolding W (k) of W is the nk(cid:2)N=nk matrix formed by concatenating all the N=nk mode-
k ﬁbers along columns. We say that W has multilinear rank (r1; : : : ; rK) if rk = rank(W (k)).

K

∏

2.1 Existing norms for tensors

First we review two norms proposed in literature in order to convexify tensor decomposition.
The overlapped trace norm (see [12  15  18  22]) is deﬁned as the sum of the trace norms of the
mode-k unfoldings as follows:

jjjWjjj

∥W (k)∥tr;

overlap =

(1)
where ∥ (cid:1) ∥tr is the trace norm (also known as the nuclear norm) [10  20]  which is deﬁned as the
absolute sum of singular values. Romera-Paredes et al. [17] has used the overlapped trace norm in
MLMTL.
The latent trace norm [21  22] is deﬁned as the inﬁmum over K tensors as follows:

k=1

∑K

jjjWjjj

latent =

inf

W (1)+(cid:1)(cid:1)(cid:1)+W (K)=W

∥W (k)

(k)

∥tr:

(2)

∑K

k=1

(

Table 1 summarizes the denoising performance in mean squared error analyzed in Tomioka and
Suzuki [21] for the above two norms. The setting is as follows: we observe a noisy version Y of a
tensor W(cid:3) with multilinear rank (r1; : : : ; rK) and would like to recover W(cid:3) by solving

^W = argminW

1
2

jjjW (cid:0) Yjjj2

F + (cid:21)jjjWjjj

⋆

;

(3)

where jjj(cid:1)jjj
⋆ is either the overlapped trace norm or the latent trace norm. We can see that while the
latent trace norm has the better dependence in terms of the multilinear rank  it has the worse de-
pendence in terms of the dimensions. Intuitively  the latent trace norm recognizes the mode with
the lowest rank. However  it does not have a good control of the dimensions; in fact  the factor

)

2

√
1= mink nk comes from the fact that for a random tensor X with i.i.d. Gaussian entries  the expecta-
maxk N=nk)  where ∥(cid:1)∥op
tion of the dual norm ∥X∥latent(cid:3) = maxk ∥X (k)∥op behaves like Op(
is the operator norm.

2.2 A new norm

p
In order to correct the unfavorable behavior of the dual norm  we propose the scaled latent trace
norm. It is deﬁned similarly to the latent trace norm with weights 1=

K∑
Now the expectation of the dual norm ∥X∥scaled(cid:3) = maxk
for X with random i.i.d. Gaussian entries and combined with the following relation

∥W (k)
1p
nk
p
p
nk∥X (k)∥op behaves like Op(
N )

nk as follows:
∥tr:

W (1)+(cid:1)(cid:1)(cid:1)+W (K)=W

scaled =

jjjWjjj

(4)

inf

k=1

(k)

√

jjjWjjj

scaled

(cid:20) min

k

jjjWjjj

F ;

rk
nk

(5)

we obtain the scaling of the mean squared error in the last column of Table 1. We can see that the
scaled latent norm recognizes the mode with the lowest rank relative to its dimension.

3 Theory for multilinear multitask learning

i=1 ((p; q) 2 S)
We consider T = P Q supervised learning tasks. Training samples (xipq; yipq)mpq
are provided for a relatively small fraction of the task index pairs S (cid:26) [P ] (cid:2) [Q]. Each task is
parametrized by a weight vector wpq 2 Rd  which can be collected into a 3-way tensor W =
(wpq) 2 Rd(cid:2)P(cid:2)Q whose (p; q) ﬁber is wpq. We deﬁne the learning problem as follows:

^W = argmin
W2Rd(cid:2)P(cid:2)Q

subject to

jjjWjjj

⋆

(cid:20) B0;

(6)

where the norm jjj(cid:1)jjj
norm  and the empirical risk ^L is deﬁned as follows:

⋆ is either the overlapped trace norm  latent trace norm  or the scaled latent trace

^L(W) =

ℓ (⟨xipq; wpq⟩ (cid:0) yipq) :

^L(W);
∑
∑

1jSj

1

(p;q)2S

P Q

p;q

mpq∑

i=1

1
mpq

The true risk we are interested in minimizing is deﬁned as follows:

L(W) =

E(x;y)(cid:24)Ppq ℓ (⟨x; wpq⟩ (cid:0) y) ;

where Ppq is the distribution from which the samples (xipq; yipq)mpq
The next lemma relates the excess risk L( ^W) (cid:0) L(W(cid:3)
) with the expected dual norm EjjjDjjj
⋆(cid:3)
through Rademacher complexity.
Lemma 1. We assume that the output yipq is bounded as jyipqj (cid:20) b  and the number of samples
mpq (cid:21) m > 0 for the observed tasks. We also assume that the loss function ℓ is Lipschitz continuous
with the constant (cid:3)  bounded in [0; c] and ℓ(0) = 0. Let W(cid:3) be any tensor such that jjjW(cid:3)jjj
(cid:20) B0.
Then with probability at least 1 (cid:0) (cid:14)  any minimizer of (6) satisﬁes the following bound:

i=1 are drawn from.

⋆

√

)

) (cid:20) 2(cid:3)

(
2B0jSj EjjjDjjj
∑mpq
⋆(cid:3) is the dual norm of jjj(cid:1)jjj
{
Z ipq  where Z ipq 2 Rd(cid:2)P(cid:2)Q is deﬁned as

(cid:26)√jSjm
∑

L( ^W) (cid:0) L(W(cid:3)
∑
)th ﬁber ofZ ipq =

log(2=(cid:14))
2jSjm

′ and q = q

⋆  (cid:26) := 1jSj

(cid:27)ipqxipq;

(p;q)2S

(p;q)2S

′
+ c

⋆(cid:3) +

′
(p

p

′

;

′

; q

i=1

b

;

= c + 1  jjj(cid:1)jjj

′
where c
is deﬁned as the sum D =

mpq

m . The tensor D 2 Rd(cid:2)P(cid:2)Q

1

mpq
0;

if p = p
otherwise:

Here (cid:27)ipq 2 f(cid:0)1; +1g are Rademacher random variables and the expectation in the above inequal-
ity is with respect to (cid:27)ipq  the random draw of tasks S  and the training samples (xipq; yipq)mpq
i=1 .

3

Proof. The proof is a standard one following the line of [5] and it is presented in Appendix A.
The next theorem computes the expected dual norm EjjjDjjj
proof can be found in Appendix B).
Theorem 1. We assume that Cpq := E[xipqxipq
∥xipq∥ (cid:20) R almost surely. Let us deﬁne

d I d and there is a constant R > 0 such that

⋆(cid:3) for the three norms for tensors (the

] ⪯ (cid:20)

⊤

D1 := d + P Q; D2 := P + dQ; D3 := Q + dP:

In order to simplify the presentation  we assume that maxk Dk (cid:21) 3 and dP Q (cid:21) max(d2; P 2; Q2).
For the overlapped trace norm  the latent trace norm  and the scaled latent trace norm  the expecta-
tion EjjjDjjj

)

⋆(cid:3) can be bounded as follows:
1jSjEjjjDjjj
1jSjEjjjDjjj
1jSjEjjjDjjj

(√
(√
(cid:3) (cid:20) C min
(√
(cid:3) (cid:20) C
mjSjdP Q
(cid:3) (cid:20) C
(cid:20)
mjSj log(max

mjSjdP Q
(cid:20)

overlap

scaled

latent

(cid:20)

′′

k

k

k

′

max

Dk log Dk +

R
mjSj log Dk

;

(Dk log Dk) +

R
mjSj log(max

k

Dk)

)
)

(7)

(8)

;

R

p
maxk nk
mjSj

;

′

k

; C

Dk)

Dk) +

log(max

(9)
′′ are constants  n1 = d; n2 = P   and n3 = Q. Furthermore  if mjSj (cid:21)

where C; C
R2(maxk nk) log(maxk Dk)=(cid:20)  the O(1=mjSj) terms in the above inequalities can be dropped.
Note that the assumption that the norm of xipq is bounded is natural because the target yipq is also
bounded. The parameter (cid:20) in the assumption Cpq ⪯ (cid:20)=dI d controls the amount of correlation in
the data. Since Tr(C) = E∥xipq∥2 (cid:20) R2  we have (cid:20) = O(1) when the features are uncorrelated;
on the other hand  we have (cid:20) = O(d)  if they lie in a one dimensional subspace. The number of
samples mjSj = ~O(maxk nk) is enough to drop the O(1=mjSj) term even if (cid:20) = O(1).
Now we state the consequences of Theorem 1 for the three norms for tensors. The com-
mon assumptions are the same as in Lemma 1 and Theorem 1. We also assume mjSj (cid:21)
R2(maxk nk) log(maxk Dk)=(cid:20) to drop the O(1=mjSj) terms. Let W(cid:3) be any d (cid:2) P (cid:2) Q tensor
with multilinear-rank (r1; r2; r3) and bounded element-wise as jjjW(cid:3)jjj
Corollary 1 (Overlapped trace norm). With probability at least 1 (cid:0) (cid:14)  any minimizer of (6) with
jjjWjjj

√∥r∥1=2dP Q satisﬁes the following inequality:
(cid:20) B
∑
) (cid:20) c1(cid:3)B
L( ^W) (cid:0) L(W(cid:3)
p
where ∥r∥1=2 = (
p
Note that Tomioka et al.
Dk=3)2 instead of
min(Dk log Dk). Although the minimum may look better than the average  our bound has the
worse constant K = 3 hidden in c1. The log Dk factor allows us to apply the above result to the
setting of tensor completion as we show below.
Corollary 2 (Latent trace norm). With probability at least 1 (cid:0) (cid:14)  any minimizer of (6) with
jjjWjjj

(cid:26)
mjSj + c3
∑

[23] obtained a bound that depends on (

ℓ1 (cid:20) B.
√

rk=3)2 and c1; c2; c3 are constants.

mjSj∥r∥1=2 min

(Dk log Dk) + c2(cid:3)b

log(2=(cid:14))
mjSj

√

√

3
k=1

3
k=1

overlap

(cid:20)

k

;

p
mink rkdP Q satisﬁes the following inequality:
) (cid:20) c

√

rk max

′
1(cid:3)B

(cid:20)
mjSj min

k

k

(Dk log Dk) + c2(cid:3)b

√

(cid:26)
mjSj + c3

(cid:20) B

latent

√

′
where c
1; c2; c3 are constants.
Corollary 3 (Scaled latent trace norm). With probability at least 1 (cid:0) (cid:14)  any minimizer of (6) with
jjjWjjj

mink(rk=nk)dP Q satisﬁes the following inequality:

;

log(2=(cid:14))
mjSj
√

√

(

)

L( ^W) (cid:0) L(W(cid:3)
√

scaled

(cid:20) B
L( ^W) (cid:0) L(W(cid:3)

) (cid:20) c
′′
1 (cid:3)B

(cid:20)
mjSj min
′′
where n1 = d  n2 = P   n3 = Q  and c
1 ; c2; c3 are constants.

dP Q log(max

rk
nk

k

k

Dk) + c2(cid:3)b

(cid:26)
mjSj + c3

log(2=(cid:14))
mjSj

;

√

4

)
T
d
(
g
o
l

d
r

)
T
;
d
(
x
a
m

)
T
d
(
g
o
l
d

)
T
+
d
(
g
o
l

)
T

;
d

=
1
;
d
(

)
1
;
r
;
r
(

We summarize the implications of the above
corollaries for different settings in Table
2. We almost recover the settings for ma-
trix completion [11] and multitask learning
(MTL) [16]. Note that these simpler prob-
lems sometimes disguise themselves as the
more general tensor completion or multilin-
ear multitask learning problems. Therefore it
is important that the new tensor based norms
adapts to the simplicity of the problems in
these cases.
Matrix completion is when d = (cid:20) = m =
r1 = 1  and we assume that r2 = r3 =
r < P; Q. The sample complexities are the
number of samples jSj that we need to make
the leading term in Corollaries 1  2  and 3
equal ϵ. We can see that the overlapped trace
norm and the scaled latent trace norm recover
the known result for matrix completion [11].
The plain latent trace norm requires O(P Q)
samples because it recognizes the ﬁrst mode
as the mode with the lowest rank 1. Although
the rank r of the last two modes are low rela-
tive to their dimensions  the latent trace norm
fails to recognize this. Note that ∥r∥1=2 (cid:20) r.
This is not a contradiction  because in Cor. 1 
we assume that the overlapped trace norm is
bounded  which may or may not be true for
matrix completion. In fact  in this case  the
overlapped trace norm is an Elastic-net-type
regularizer (trace norm + Frobenius norm).
In multitask learning (MTL)  we set P = T
(the number of tasks) and Q = 1. The ﬁrst
and the second modes have a low rank r.
We also assume that all the pairs (p; q) are
observed (jSj = T ) as in [16]. The sam-
ple complexities are deﬁned the same way
as above with respect to the number of sam-
ples m because jSj is ﬁxed. Our bound for
the overlapped trace norm is almost as good
as the one in [16] but has an multiplicative
log(d + T ) factor (as oppose to their additive
log(mT ) term). Also note that the results in
[16] can be applied when d is much larger
than T . Turning back to our bounds  the
scaled latent trace norm performs as well as
knowing the mode with the lowest rank (the
ﬁrst and the second modes; see also [21]).
However  similarly to the matrix completion
case above  the plain latent trace norm fails to
recognize the low-rank-ness of the ﬁrst two
modes  and requires O(d) samples  because
the third mode has the lowest rank.

e
r
a

e
h
t

n
o
m
m
o
c

=
2
=
1

k
s
a
t
i
t
l
∥
u
r
m
∥

e
h
T

r
o
f

m

e
n
ﬁ
e
d

′

.

e

W

 
n
o
i
t
e
l
p
m
o
c

r
<
r
x
(cid:20)
i
r
t
a
m

r
o
f

j

S

j

.
s
g
n
i
t
t
e
s

s
u
o
i
r
a
v

n
i

m
r
o
n

e
c
a
r
t

t
n
e
t
a
l

d
e
l
a
c
s

e
h
t

d
n
a

 

m
r
o
n

e
c
a
r
t

t
n
e
t
a
l

 

m
r
o
n

o
t

t
c
e
p
s
e
r

h
t
i

w
d
e
n
ﬁ
e
d

s
e
i
t
i
x
e
l
p
m
o
c

e
l
p
m
a
s

e
h
T

e
c
a
r
t

d
e
p
p
a
l
r
e
v
o

e
h
t

f
o

s
e
i
t
i
x
e
l
p
m
o
c

e
l
p
m
a
S

.
s
e
i
t
i
x
e
l
p
m
o
c

e
l
p
m
a
s

e
h
t

m
o
r
f

d
e
t
t
i

m
o

s
i

2
ϵ
=
1

:
2

P
e
m
u
s
s
a

e
w

 
e
s
a
c

s
u
o
e
n
e
g
o
r
e
t
e
h

n
I

.

g
n
i
n
r
a
e
l

k
s
a
t
i
t
l
u
m

r
a
e
n
i
l
i
t
l
u
m
d
n
a

n
o
i
t
e
l
p
m
o
c

r
o
s
n
e
t

r
o
f

d
e
l
a
c
S

)
2
ϵ
=
1
r
e
p
(

t
n
e
t
a
L

s
e
i
t
i
x
e
l
p
m
o
c

e
l
p
m
a
S

p
a
l
r
e
v
O

)
j

S

j

;

B

;

(cid:20)
(

)
3
r
;
2
r
;
1
r
(

)
3
n

;
2
n

;
1
n
(

.
3
n
2
n
1
n
=

:

N
d
n
a

)

Q
P
(
g
o
l
)

Q
+
P
(
r

)

Q
P
(
g
o
l

Q
P

)

Q
+
P
(
g
o
l
)

Q
+
P
(
2
=
1

∥
r
∥

)
j

S

j

;
1
;
1
(

)
r
;
r
;
1
(

]
1
1
[

n
o
i
t
e
l
p
m
o
c
x
i
r
t
a

M

j

j

2
)

K
=
k
r

S
m
d
n
a
p
 
g
n
i
n
r
a
e
l

e
l
b
∑
a
T

r
o
t
c
a
f

1
=
3 k

(

5

)
2
d
(
g
o
l

2
d
)
k
r
n
m
(
(cid:20)

i

k

)
2
d
(
g
o
l

2
d
)
k
r
n
m
(
(cid:20)

i

k

)
2
d
(
g
o
l

2
d
2
=
1

)

Q
d
(
g
o
l
)

′

r
P
d
;

Q
P
r
(
n
m
(cid:20)

i

)

Q
d
(
g
o
l

Q
P
d
(cid:20)

)

Q
P
(
g
o
l

Q
P
2
=
1

∥
r
∥
(cid:20)

)
k
D
x
a
m
(
g
o
l

k

N

)

k
r

k
n

(
n
m

i

k

)
k
D
g
o
l

k
D
(
x
a
m
k
r
n
m

i

k

k

)
k
D
g
o
l

k
D
(
n
m

i

k

2
=
1

∥
r
∥

)
T
+
d
(
2
=
1

∥
r
∥

T

∥
r
∥
(cid:20)

)
j

S

)
j

S

j

j

)
j

S

j

p

;
1
;

(cid:20)
(

)
3
r
;
2
r
;
1
r
(

;
1
;

(cid:20)
(

)

′

r
;

P

;
r
(

-
o
m
o
h
(

-
o
r
e
t
e
h
(

]
7
1
[

)
e
s
a
c

L
T
M
L
M

s
u
o
e
n
e
g

]
7
1
[

)
e
s
a
c

L
T
M
L
M

s
u
o
e
n
e
g

]
6
1
[
L
T
M

;
1
;
1
(

)
3
r
;
2
r
;
1
r
(

n
o
i
t
e
l
p
m
o
c

r
o
s
n
e
T

P!Q!1!d!T!d!d!d!P!Q!d!n2!n3!n1!In multilinear multitask learning (MLMTL) [17]  any mode could possibly be low rank but it is
a priori unknown. The sample complexities are deﬁned the same way as above with respect to
mjSj. The homogeneous case is when d = P = Q. The heterogeneous case is when the ﬁrst
mode or the third mode is low rank but P (cid:20) r < d. Similarly to the above two settings  the
overlapped trace norm has a mild dependence on the dimensions but a higher dependence on the
rank ∥r∥1=2 (cid:21) mink rk. The latent trace norm performs as well as knowing the mode that has
the lowest rank in the homogeneous case. However  it fails to recognize the mode with the lowest
rank relative to its dimension. The scaled latent trace norm does this and although it has a higher
logarithmic dependence  it is competitive in both cases.
Finally  our bounds also hold for tensor completion. Although Tomioka et al. [22  23] studied
tensor completion algorithms  their analysis assumed that the inputs xipq are drawn from a Gaussian
distribution  which does not hold for tensor completion. Note that in our setting xipq can be an
indicator vector that has one in the jth position uniformly over 1; : : : ; d. In this case  (cid:20) = 1. The
sample complexities of different norms with respect to mjSj is shown in the last row of Table 2. The
sample complexity for the overlapped trace norm is the same as the one in [23] with a logarithmic
factor. The sample complexities for the latent and scaled latent trace norms are new. Again we can
see that while the latent trace norm recognize the mode with the lowest rank  the scaled latent trace
norm is able to recognize the mode with the lowest rank relative to its dimension.

4 Experiments

We conducted several experiments to evaluate performances of tensor based multitask learning set-
ting we have discussed in Section 3. In Section 4.1  we discuss simulation we conducted using
synthetic data sets. In Sections 4.2 and 4.3  we discuss experiments on two real world data sets 
namely the Restaurant data set [26] and School Effectiveness data set [3  4]. Both of our real world
data sets have heterogeneous dimensions (see Figure 2) and it is a priori unclear across which mode
has the most amount of information sharing.

4.1 Synthetic data sets
The true d (cid:2) P (cid:2) Q tensor W(cid:3) was generated by ﬁrst sampling a r1 (cid:2) r2 (cid:2) r3 core tensor and then
multiplying random orthonormal matrix to each of its modes. For each task (p; q) 2 [P ] (cid:2) [Q]  we
i=1 by ﬁrst sampling xipq from the standard normal
generated training set of m vectors (xipq; yipq)m
distribution and then computing yipq = ⟨xipq; wpq⟩ + (cid:23)i  where (cid:23)i was drawn from a zero-mean
normal distribution with variance 0:1. We used the penalty formulation of (6) with the squared loss
and selected the regularization parameter (cid:21) using two-fold cross validation on the training set from
the range 0:01 to 10 with the interval 0:1.
In addition to the three norms for tensors we discussed in the previous section  we evaluated the
matrix-based multitask learning approaches that penalizes the trace norm of the unfolding of W at
speciﬁc modes. The conventional convex multitask learning [2  3  16] corresponds to one of these
approaches that penalizes the trace norm of the ﬁrst unfolding ∥W (1)∥tr. The convex MLMTL in
[17] corresponds to the overlapped trace norm.
In the ﬁrst experiment  we chose d = P = Q = 10 and r1 = r2 = r3 = 3. Therefore  both
the dimensions and the multilinear rank are homogeneous. The result is shown in Figure 1(a). The
overlapped trace norm performed the best  the matrix-based approaches performed next  and the
latent trace norm and the scaled latent trace norm were the worst. The scaling of the latent trace
norm had no effect because the dimensions were homogeneous. Since the sample complexities for
all the methods were the same in this setting (see Table 2)  the difference in the performances could
be explained by a constant factor K(= 3) that is not shown in the sample complexities.
In the second experiment  we chose the dimensions to be homogeneous as d = P = Q = 10  but
(r1; r2; r3) = (3; 6; 8). The result is shown in Figure 1(b). In this setting  the (scaled) latent trace
norm and the mode-1 regularization performed the best. The lower the rank of the corresponding
mode  the lower were the error of the matrix-based MTL approaches. The overlapped trace norm
was somewhat in the middle of the three matrix-based approaches.

6

(a) Synthetic experiment for the
case when both the dimensions and
the ranks are homogeneous. The
true tensor is 10(cid:2)10(cid:2)10 with mul-
tilinear rank (3; 3; 3).

(b) Synthetic experiment for the
case when the dimensions are ho-
mogeneous but the ranks are het-
erogeneous.
The true tensor is
10 (cid:2) 10 (cid:2) 10 with multilinear rank
(3; 6; 8).

(c) Synthetic experiment for the
case when both the dimensions and
the ranks are heterogeneous. The
true tensor is 10(cid:2) 3(cid:2) 10 with mul-
tilinear rank (3; 3; 8).

Figure 1: Results for the synthetic data sets.

In the last experiment  we chose both the dimensions and the multilinear rank to be heterogeneous
as (d; P; Q) = (10; 3; 10) and (r1; r2; r3) = (3; 3; 8). The result is shown in Figure 1(c). Clearly the
ﬁrst mode had the lowest rank relative to its dimension. However  the latent trace norm recognizes
the second mode as the mode with the lowest rank and performed similarly to the mode-2 regulariza-
tion. The overlapped trace norm performed better but it was worse than the mode-1 regularization.
The scaled latent trace norm performed comparably to the mode-1 regularization.

4.2 Restaurant data set

The Restaurant data set contains data for a recommendation system for restaurants where different
customers have given ratings to different aspects of each restaurant. Following the same approach
as in [17] we modelled the problem as a MLMTL problem with d = 45 features  P = 3 aspects 
and Q = 138 customers.
The total number of instances for all the tasks were 3483 and we randomly selected training set of
sizes 400  800  1200  1600  2000  2400  and 2800. When the size was small many tasks contained
no training example. We also selected 250 instances as the validation set and the rest was used as the
test set. The regularization parameter for each norm was selected by minimizing the mean squared
error on the validation set from the candidate values in the interval [50  1000] for the overlapped 
[0.5  40] for the latent  [6000  20000] for the scaled latent norms  respectively.
We also evaluated matrix-based MTL approaches on different modes and ridge regression (Frobe-
nius norm regularization; abbreviated as RR) as baselines. The convex MLMTL in [17] corresponds
to the overlapped trace norm.
The result is shown in Figure 2(a). We found the multilinear rank of the solution obtained by the
overlapped trace norm to be typically (1; 3; 3). This was consistent with the fact that the perfor-
mances of the mode-1 regularization and the ridge regression were equal. In other words  the effec-
tive dimension of the ﬁrst mode (features) was one instead of 45. The latent trace norm recognized
the ﬁrst mode as the mode with the lowest rank and it failed to take advantage of the low-rank-ness
of the second and the third modes. The scaled latent trace norm was able to perform the best match-
ing with the performances of mode-2 and mode-3 regularization. When the number of samples was
above 2400  the latent trace norm caught up with other methods  probably because the effective
dimension became higher in this regime.

4.3 School data set

The data set comes from the inner London Education Authority (ILEA) consisting of examination
records from 15362 students at 139 schools in years 1985  1986  and 1987. We followed [4] for
the preprocessing of categorical attributes and obtained 24 features. Previously Argyriou et al. [3]
modeled this data set as a 27 (cid:2) 139 matrix-based MTL problem in which the year was modeled as a

7

1020304050607080901001100.010.0110.0120.0130.0140.0150.016Sample sizeMSE Overlapped trace normLatent trace normScaled Latent trace normMode 1Mode 2Mode 31020304050607080901001100.010.0120.0140.0160.0180.020.022Sample sizeMSE Overlapped trace normLatent trace normScaled Latent trace normMode 1Mode 2Mode 31020304050607080901001100.010.0120.0140.0160.0180.020.0220.024Sample sizeMSE Overlapped trace normLatent trace normScaled Latent trace normMode 1Mode 2Mode 3(a) Result for the 45 (cid:2) 3 (cid:2) 138 Restaurant data set.

(b) Result for the 24 (cid:2) 139 (cid:2) 3 School data set.

Figure 2: Results for the real world data sets.

trinomial attribute. Instead here we model this data set as a 24(cid:2) 139(cid:2) 3 MLMTL problem in which
the third mode corresponds to the year. Following earlier papers  [3  4]  we used the percentage of
explained variance  deﬁned as 100 (cid:1) (1 (cid:0) (test MSE)=(variance of test y))  as the evaluation metric.
We note that the variance of test y was roughly one because we standardized the data to have zero
mean and unit variance [4].
The results are shown in Figure 2(b). First  ridge regression performed the worst because it was
not able to take advantage of the low-rank-ness of any mode. Second  the plain latent trace norm
performed similarly to the mode-3 regularization probably because the dimension 3 was lower than
the rank of the other two modes. Clearly the scaled latent trace norm performed the best matching
with the performance of the mode-2 regularization; probably the second mode had the most redun-
dancy. The performance of the overlapped trace norm was comparable or slightly better than the
mode-1 regularization. The percentage of the explained variance of the latent trace norm exceeds
30 % around sample size 4000 (around 30 samples per school)  which is higher than the Hierarchical
Bayes [4] (around 29.5 %) and matrix-based MTL [3] (around 26.7 %) that used around 80 samples
per school.

5 Discussion

Using tensors for modeling multitask learning [17  19] is a promising direction that allows us to
take advantage of similarity of tasks in multiple dimensions and even make prediction for a task
with no training example. However  having multiple modes  we would have to face with more
hyperparameters to choose in the conventional nonconvex tensor decomposition framework. Convex
relaxation of tensor multilinear rank allows us to side-step this issue. In fact  we have shown that the
sample complexity of the latent trace norm is as good as knowing the mode with the lowest rank.
This is consistent with the analysis of [21] in the tensor denoising setting (see Table 1).
In the setting of tensor-based MTL  however  the notion of mode with the lowest rank may be
vacuous because some modes may have very low dimension. In fact  the sample complexity of
the latent trace norm can be as bad as not using any low-rank-ness at all if there is a mode with
dimension lower than the rank of the other modes. The scaled latent trace norm we proposed in this
paper recognizes the mode with the lowest rank relative to its dimension and lead to the competitive
sample complexities in various settings we have shown in Table 2.
Acknowledgment: MS acknowledges support from the JST CREST program.

References
[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unla-

beled data. J. Mach. Learn. Res.  6:1817–1853  2005.

8

0500100015002000250030000.350.40.450.50.550.60.650.7Sample sizeMSE Overlapped trace normLatent trace normScaled latent trace normMode 1Mode 2Mode 3RR02000400060008000100001200010152025303540Sample sizeExplained variance Overlapped trace normLatent trace normScaled latent trace normMode 1Mode 2Mode 3RR[2] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning.

In B. Sch¨olkopf  J. Platt  and
T. Hoffman  editors  Adv. Neural. Inf. Process. Syst. 19  pages 41–48. MIT Press  Cambridge  MA  2007.
[3] A. Argyriou  M. Pontil  Y. Ying  and C. A. Micchelli. A spectral regularization framework for multi-task
structure learning. In J. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Adv. Neural. Inf. Process. Syst.
20  pages 25–32. Curran Associates  Inc.  2008.

[4] B. Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. J. Mach. Learn.

Res.  4:83–99  2003.

[5] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural

results. J. Mach. Learn. Res.  3:463–482  2002.

[6] J. Baxter. A model of inductive bias learning. J. Artif. Intell. Res.  12:149–198  2000.
[7] R. Caruana. Multitask learning. Machine learning  28(1):41–75  1997.
[8] L. De Lathauwer  B. De Moor  and J. Vandewalle. A multilinear singular value decomposition. SIAM J.

Matrix Anal. Appl.  21(4):1253–1278  2000.

[9] L. De Lathauwer  B. De Moor  and J. Vandewalle. On the best rank-1 and rank-(R1; R2; : : : ; RN ) ap-

proximation of higher-order tensors. SIAM J. Matrix Anal. Appl.  21(4):1324–1342  2000.

[10] M. Fazel  H. Hindi  and S. P. Boyd. A Rank Minimization Heuristic with Application to Minimum Order

System Approximation. In Proc. of the American Control Conference  2001.

[11] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. Arxiv

preprint arXiv:1102.3923  2011.

[12] S. Gandy  B. Recht  and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex opti-

mization. Inverse Problems  27:025010  2011.

[13] S. M. Kakade  S. Shalev-Shwartz  and A. Tewari. Regularization techniques for learning with matrices.

J. Mach. Learn. Res.  13(1):1865–1890  2012.

[14] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review  51(3):455–500 

2009.

[15] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual data.

In Proc. ICCV  2009.

[16] A. Maurer and M. Pontil. Excess risk bounds for multitask learning with trace norm regularization. In

JMLR W&CP 30 (COLT2013)  pages 55–76. MIT Press  2013.

[17] B. Romera-Paredes  H. Aung  N. Bianchi-Berthouze  and M. Pontil. Multilinear multitask learning. In

Proceedings of the 30th International Conference on Machine Learning  pages 1444–1452  2013.

[18] M. Signoretto  L. De Lathauwer  and J. Suykens. Nuclear norms for tensors and their use for convex

multilinear estimation. Technical Report 10-186  ESAT-SISTA  K.U.Leuven  2010.

[19] M. Signoretto  L. De Lathauwer  and J. A. K. Suykens. Learning tensors in reproducing kernel hilbert

spaces with multilinear spectral penalties. Technical report  arXiv:1310.4977  2013.

[20] N. Srebro  J. D. M. Rennie  and T. S. Jaakkola. Maximum-margin matrix factorization. In L. K. Saul 
Y. Weiss  and L. Bottou  editors  Adv. Neural. Inf. Process. Syst. 17  pages 1329–1336. MIT Press  Cam-
bridge  MA  2005.

[21] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured Schatten norm regularization. In
C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Weinberger  editors  Adv. Neural. Inf. Process.
Syst. 26  pages 1331–1339. 2013.

[22] R. Tomioka  K. Hayashi  and H. Kashima. Estimation of low-rank tensors via convex optimization.

Technical report  arXiv:1010.0789  2011.

[23] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor decompo-

sition. In Adv. Neural. Inf. Process. Syst. 24  pages 972–980. 2011.

[24] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput. Math.  12(4):

389–434  2012.

[25] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika  31(3):279–311 

1966.

[26] B. Vargas-Govea  G. Gonz´alez-Serna  and R. Ponce-Medellın. Effects of relevant contextual features in
the performance of a restaurant recommender system. In Proceedings of 3rd Workshop on Context-Aware
Recommender Systems. 2011.

9

,Kishan Wimalawarne
Masashi Sugiyama
Ryota Tomioka
Greg Van Buskirk
Benjamin Raichel
Nicholas Ruozzi
Jangho Kim
Seonguk Park
Nojun Kwak