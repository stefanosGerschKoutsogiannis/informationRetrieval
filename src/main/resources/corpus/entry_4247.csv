2019,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph,Despite some exciting progress on high-quality image generation from structured (scene graphs) or free-form (sentences) descriptions  most of them only guarantee the image-level semantical consistency  i.e. the generated image matching the semantic meaning of the description. They still lack the investigations on synthesizing the images in a more controllable way  like finely manipulating the visual appearance of every object. Therefore  to generate the images with preferred objects and rich interactions  we propose a semi-parametric method  PasteGAN  for generating the image from the scene graph and the image crops  where spatial arrangements of the objects and their pair-wise relationships are defined by the scene graph and the object appearances are determined by the given object crops. To enhance the interactions of the objects in the output  we design a Crop Refining Network and an Object-Image Fuser to embed the objects as well as their relationships into one map. Multiple losses work collaboratively to guarantee the generated images highly respecting the crops and complying with the scene graphs while maintaining excellent image quality. A crop selector is also proposed to pick the most-compatible crops from our external object tank by encoding the interactions around the objects in the scene graph if the crops are not provided. Evaluated on Visual Genome and COCO-Stuff dataset  our proposed method significantly outperforms the SOTA methods on Inception Score  Diversity Score and Fréchet Inception Distance. Extensive experiments also demonstrate our method’s ability to generate complex and diverse images with given objects. The code is available at https://github.com/yikang-li/PasteGAN.,PasteGAN: A Semi-Parametric Method to Generate

Image from Scene Graph

Yikang Li1∗  Tao Ma2∗  Yeqi Bai3  Nan Duan4  Sining Wei4  Xiaogang Wang1

1The Chinese University of Hong Kong  2Northwestern Polytechnical University

3Nanyang Technological University  4Microsoft

{ykli  xgwang}@ee.cuhk.edu.hk  taoma@mail.nwpu.edu.cn

baiyeqi@gmail.com  {sinwei  nanduan}@microsoft.com

Abstract

Despite some exciting progress on high-quality image generation from struc-
tured (scene graphs) or free-form (sentences) descriptions  most of them only
guarantee the image-level semantical consistency  i.e. the generated image match-
ing the semantic meaning of the description. They still lack the investigations on
synthesizing the images in a more controllable way  like ﬁnely manipulating the
visual appearance of every object. Therefore  to generate the images with preferred
objects and rich interactions  we propose a semi-parametric method  PasteGAN 
for generating the image from the scene graph and the image crops  where spatial
arrangements of the objects and their pair-wise relationships are deﬁned by the
scene graph and the object appearances are determined by the given object crops.
To enhance the interactions of the objects in the output  we design a Crop Reﬁning
Network and an Object-Image Fuser to embed the objects as well as their relation-
ships into one map. Multiple losses work collaboratively to guarantee the generated
images highly respecting the crops and complying with the scene graphs while
maintaining excellent image quality. A crop selector is also proposed to pick the
most-compatible crops from our external object tank by encoding the interactions
around the objects in the scene graph if the crops are not provided. Evaluated
on Visual Genome and COCO-Stuff dataset  our proposed method signiﬁcantly
outperforms the SOTA methods on Inception Score  Diversity Score and Fréchet
Inception Distance. Extensive experiments also demonstrate our method’s ability
to generate complex and diverse images with given objects. The code is available
at https://github.com/yikang-li/PasteGAN.

1

Introduction

Image generation from a scene description with multiple objects and complicated interactions between
them is a frontier and pivotal task. With such algorithms  everyone can become an artist: you just
need to deﬁne the objects and how they interact with each other  and then the machine will produce
the image following your descriptions. However  it is a challenging problem as it requires the model
to have a deep visual understanding of the objects as well as how they interact with each other.
There have been some excellent works on generating the images conditioned on the textual descrip-
tion [1  2]  semantic segmentations [3] and scene graphs [4]. Among these forms  scene graphs
are powerful structured representations of the images that encode objects and their interactions.
Nevertheless  nearly all the existing methods focus on the semantical compliance with the description
on the image level but lack the object-level control. To truly paint the images in our mind  we need

∗Equally contributed to the work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: The image generation process is guided by both the scene graph and object crop images. Our
proposed PasteGAN encodes the scene graph and given object crops  and generates the corresponding
scene images. One group is in blue box and the other group is in the box with pale yellow background
color. The appearance of the output scene image can be ﬂexibly adjusted by the object crops.

to control the image generation process in a more ﬁne-grained way  not only regulating the object
categories and their interactions but also deﬁning the appearance of every object.
Apart from the scene graph description  a corresponding anchor crop image is also provided for
each deﬁned object  which depicts how the object looks like. The synthesized image should follow
the requirements: 1) the image as a whole should comply with the scene graph deﬁnition (denoted
as image-level matching); 2) the objects should be the ones shown in the crop images (denoted
as object-level control). Therefore  the original task is reformulated to a semi-parametric image
generation from the scene graph  where the given object crops provide supervision on object-level
appearance and the scene graph control the image-level arrangement.
In order to integrate the objects in the expected way deﬁned by the scene graph as well as maintaining
the visual appearance of the objects  we designed a Crop Reﬁning Network as well as an attention-
based Object-Image Fuser  which can encode the spatial arrangements and visual appearance of the
objects as well as their pair-wise interactions into one scene canvas. Therefore  it can encode the
complicated interactions between the objects. Then the Object-Image Fuser fuses all object integral
features into a latent scene canvas  which is fed into an Image Decoder to generate the ﬁnal output.
Sometimes  we just deﬁne the scene graph and don’t want to specify the object appearance. To handle
such situations  we introduce a Crop Selector to automatically select the most-compatible object
crops from our external object tank. It is pre-trained on a scene graph dataset  which aims to learn to
encode the entire scene graph and infer the visual appearance of the objects in it (termed as visual
codes). Then the visual codes can be used to ﬁnd the most matching object from the tank  where all
the visual codes of external objects have been extracted ofﬂine using the scene graph they belong to.
Our main contributions can be summarized three folds: 1) we propose a semi-parametric method 
PasteGAN  to generate realistic images from a scene graph  which uses the external object crops as
anchors to guide the generation process; 2) to make the objects in crops appear on the ﬁnal image in
the expected way  a scene-graph-guided Crop Reﬁning Network and an attention based Object-Image
Fuser are also proposed to reconcile the isolated crops into an integrated image; 3) a Crop Selector is
also introduced to automatically pick the most-compatible crops from our object tank by encoding
the interactions around the objects in the scene graph.
Evaluated on Visual Genome and COCO-Stuff dataset  our proposed method signiﬁcantly outper-
forms the SOTA methods both quantitatively (Inception Score  Diversity Score and Fréchet Inception
Distance) and qualitatively (preference user study). In addition  extensive experiments also demon-
strate our method’s ability to generate complex and diverse images complying the deﬁnition given by
scene graphs and object crops.

2 Related Works

Generative models. Generative models have been widely studied in recent years. Autoregressive
approaches such as PixelRNN and PixelCNN [5] synthesize images pixel by pixel  based on the
sequential distribution pattern of pixels. Variational Autoencoders [6  7] jointly train an encoder
that maps the input into a latent distribution and a decoder that generates images based on the latent
distribution. In Generative Adversarial Networks (GANs) [8  9]  a pair of generator and discriminator
are adversely optimized against each other to synthesize images and distinguish model synthesized
images from the real image. In this work  we propose a novel framework using adversarial training
strategy for image generation from scene graph and object crops.

2

elephantelephantskybelowaboveright oftreegrassonFigure 2: Overview of the training process of our proposed PasteGAN. The two branches are trained
simultaneously with the same scene graph: the top branch focuses on generating the diverse images
with the crops retrieved from the external memory  the bottom branch aims at reconstructing the
ground-truth image using the original crops. The model is trained adversarially against a pair of
discriminators and a number of objectives. L1  CML  IPL and OPL mean image reconstruction loss 
crop matching loss  image perceptual loss and object perceptual loss respectively.

Conditional Image Synthesis. Conditional image synthesis aims to generate images according to
additional information. Such information could be of different forms  like image labels [10  11  12] 
textual descriptions [1  2  13]  scene graphs [4] and semantic segmentations [14  3]. To synthesize
photographic image based on semantic layout  Chen and Koltun [14] train a Cascaded Reﬁnement
Network (CRN) by minimizing Perceptual Loss [15  16]  which measures the Euclidean distance
between the encoded features of a real image and a synthesized image. Qi et al. [3] extend this
approach by ﬁlling segments from other training images into a raw canvas and reﬁne the canvas into
a photographic image with a similar approach. Different from the previous canvas building by simply
stacking or averaging the objects [3  4  17]  our integration process is implemented with a learnable
2D graph convolution architecture.
Scene Graphs. Scene graphs are directed graphs which represent objects in a scene as nodes and
relationships between objects as edges. Scene graphs have been employed for various tasks  e.g. 
image retrieval [18]  image captioning evaluation [19]  sentence-scene graph translation [20]  and
image-based scene graph prediction [21  22  23]. Li et al. [24] proposes a convolutional structure
guided by visual phrases in scene graphs. Visual Genome [25] is a dataset widely used by works on
scene graphs  where each image is associated with a human-annotated scene graph.
Most closely related to our work  sg2im [4] generates image based on scene graph  graph convolution
network [26] is employed to process scene graph information into object latent vectors and image
layouts  which is then reﬁned with a CRN [15] to optimize GAN losses [8  9  12] and pixel
loss between synthesized and ground-truth images. Hong et al. [13] uses the textual information
to generate object masks and heuristically aggregates them to a soft semantic map for the image
decoding. Also  Zhao et al. [17] formulate image generation from layout as a task whose input
is bounding boxes and categories of objects in an image. We further extend previous works by
innovating the PasteGAN pipeline  where object crops are fused into images tractably; canvas
generated from scene graph and object crops serves as enriched information and arms generated
images with outstanding tractability and diversity.

3 The PasteGAN

The overall pipeline of our proposed PasteGAN is illustrated in Figure 2. Given a scene graph
and selected object crops  our model generates a realistic image respecting the scene graph and
the appearance of selected object crops. The training process involves two branches  one aims to
reconstruct the ground-truth image using the original crops mori
(the bottom branch)  the other
i
focuses on generating the diversiﬁed images with the crops msel
retrieved from the external memory
tank M (the top branch). The scene graph is ﬁrstly processed with Graph Convolution Network to get

i

3

elephantelephantskybelowaboveright oftreegrasson(cid:4)(cid:19)(cid:8)(cid:18)(cid:12)(cid:1)(cid:2)(cid:17)(cid:16)(cid:22)(cid:17)(cid:14)(cid:21)(cid:20)(cid:13)(cid:17)(cid:16) (cid:23)(cid:20)(cid:9)(cid:19)(cid:16)(cid:8)(cid:14)(cid:1)(cid:5)(cid:9)(cid:15)(cid:17)(cid:19)(cid:24)Box Regressor!"#$%&'RefinerCrop Selector(cid:5)(cid:10)(cid:9)()*+( -.(cid:11)(cid:12)(cid:9)(cid:9) (cid:8)(cid:12)(cid:9)(cid:7)(cid:18)1(cid:18)(cid:26)(cid:15)(cid:28)(cid:21)(cid:24)1(cid:1)(cid:30)(cid:21)(cid:28)(cid:20)(cid:1)(cid:11)(cid:26)(cid:21)-(cid:21)1(cid:15)(cid:22)(cid:1)(cid:5)(cid:26)(cid:24)(cid:25)(cid:27)(cid:9)(cid:24)(cid:27)(cid:27)(cid:9)(cid:24)(cid:27)(cid:27)(cid:1)(cid:12)(cid:15)(cid:28)(cid:20)(cid:7)(cid:18)1(cid:18)(cid:26)(cid:15)(cid:28)(cid:21)(cid:24)1(cid:1)(cid:30)(cid:21)(cid:28)(cid:20)(cid:1)(cid:14)(cid:18)(cid:22)(cid:18)(cid:16)(cid:28)(cid:18)(cid:17)(cid:1)(cid:5)(cid:26)(cid:24)(cid:25)(cid:27)()*+( -.Obj-ImgFuser(cid:13)(cid:18)(cid:25)(cid:22)(cid:21)(cid:16)(cid:15)(cid:28)(cid:21)(cid:24)1(cid:1)(cid:6)(cid:18)(cid:15)(cid:28)(cid:29)(cid:26)(cid:18)(cid:1)(cid:30)(cid:21)(cid:28)(cid:20)(cid:1)(cid:4)(cid:2)(cid:4)(cid:24)(cid:31)Crop Encoder(cid:2)(cid:19)(cid:17)(cid:18)(cid:1)(cid:7)(cid:9)(cid:10)(cid:13)(cid:16)(cid:13)(cid:16)(cid:11)(cid:1)(cid:6)(cid:9)(cid:20)Image DecoderCrop Encoder/)0")1)2341) 5)66′86Figure 3: Top-2 retrieved person crops using our Crop Selector with different scene graphs during
inference on COCO-Stuff Dataset.

a latent vector z containing the context information for each object which is used for regressing the
object location ˆbi and retrieving the most context-matching object crop by Crop Selector. Then Crop
Encoder processes the object crops to encode their visual appearances. Afterwards  the crop feature
maps and predicate vectors are fed into Object2 Reﬁner to incorporate the pair-wise relationships into
the visual features v. Object-Image Fuser takes the reﬁned object feature maps concatenated with the
expanded latent vectors and predicted bounding boxes as inputs to generate a latent scene canvas L.
Similarly  the other latent scene canvas ˆL is constructed along the reconstruction path using mori
.
Finally  Image Decoder reconstructs the ground-truth image ˆI and generates a new image I(cid:48) based
on ˆL and L respectively. The model is trained adversarially end-to-end with a pair of discriminators
Dimg and Dobj.
Scene Graphs. Given a set of object categories C and a set of relationship categories R  a scene
graph can be deﬁned as a tuple (O  E)  where O = {o1  ...  on} is a set of objects with each oi ∈ C 
and E ∈ O × R × O is a set of directed edges of the form (oi  pj  ok) where oi  ok ∈ O and pj ∈ R.
External Memory Tank. The external memory tank M  playing a role of source materials for image
generation  is a set of object crop images
. In this work  the object crops are
extracted by the ground-truth bounding box from the training dataset. Additionally  once the training
of our model has been ﬁnished  if the users do not want to specify the object appearance  M will
provide the most-compatible object crops for inference. Note that the object crops on COCO-Stuff
dataset are segmented by the ground-truth masks. The number of object crops on Visual Genome and
COCO-Stuff dataset is shown in Table 1.

mi ∈ R3× H

2 × W

(cid:110)

(cid:111)

i

2

3.1 Graph Convolution Network

oi

pj

ok

v(cid:48)

predicate vectors zp are concatenated as a triple(cid:0)voi  vpj   vok

Following [4]  we use a graph convolution network composed of several graph convolution layers to
process scene graphs. Additionally  we extend this approach on 2-D feature maps to achieve message
propagation among feature maps along edges while maintaining the spatial information. Speciﬁcally 
feature maps vi ∈ RDin×w×h for all object crops and predicate feature maps vp expanded from
∈ RDout×w×h as new feature maps for the subject oi  predicate pj 
and go. We compute v(cid:48)
and object ok respectively. gs  gp  and go are implemented with convolutional networks.
The process of updating object feature maps exists more complexities  because an object may
participate in many relationships and there is a large probability that these objects overlap. To this
end  the edges connected to oi can be divided into two folds: the subject edges starting at oi and the
i and V o
object edges terminating at oi. Correspondingly  we have two sets of candidate features V s
i  

(cid:1) and fed into three functions gs  gp 

  v(cid:48)

(cid:1) : (oi  pj  ok) ∈ E(cid:9) and V o

i =(cid:8)g(cid:0)vok   vpj   voi

(cid:1) : (ok  pj  oi) ∈ E(cid:9) (1)

i =(cid:8)g(cid:0)voi  vpj   vok

The output feature maps for object oi is then computed as v(cid:48)
i ) where h is a symmetric
function which pools an input set of feature maps to a single output feature map (we use average
pooling in our experiments). An example computational graph of a single graph convolution layer for
feature maps is shown in Figure 4.

= h (V s

i ∪ V o

oi

V s

3.2 Crop Selector

Selecting a good crop for object oi is very crucial for generating a great image. A good crop is not
simply matching the category  but also of the similar scene. Thus  to retrieve the good crop  we
should also consider the context information of the object  i.e. the entire scene graph it belongs to.
Compared to hard-matching the edges connected to oi  pre-trained sg2im [4] provides us a learning-
based method to achieve this goal. It adopts a graph convolution network (GCN) to process scene
graphs and a decoder to generate images. In order to generate the expected images  the output feature

4

horsepersongrassbelowsurroundingpersonseasurfboardinsideinsidepersonplayingfieldfenceinsideabovesnowtreepersonabovebelowhorsepersongrassbelowsurroundingpersonseasurfboardinsideinsidepersonplayingfieldfenceinsideabovesnowtreepersonabovebelowof GCN should encode the object visual appearance as well as its context information (how to interact
with other objects). So  after deprecating the image decoder part  the remaining GCN can be utilized
as the Crop Selector to incorporating the context information into the visual code of oi.
For each object oi  we can compute its visual code by encoding its scene graph  and select the most
matching crops in M based on some similarity metric (e.g. L2 norm). The visual codes of the object
crops in external memory tank can be extracted ofﬂinely. In our experiments  we randomly sample
the crop from top-k matching ones to improve the model robustness and the image diversity. As
shown in Figure 3  by encoding the visual code  our Crop Selector can retrieve different persons with
different poses and uniforms for different scene graphs. These scene-compatible crops will simplify
the generation process and signiﬁcantly improve the image quality over the random selection.

3.3 Crop Reﬁning Network

2 × W

As shown is Figure 2  the crop reﬁning network is composed of Crop Encoder and Object2 Reﬁner.
Crop encoder. Crop encoder  aiming to extract the main visual features of object crops  takes as
input a selected object crop mi ∈ R3× H
2 and output a feature map vi ∈ RD×w×h. The object
crops selected from external memory M are passed to into several 3×3 convolutional layers followed
by batch normalization and ReLU layers instead of last convolutional layer.
Object2 Reﬁner. Object2 Reﬁner  consisting of two 2-D graph convolution layers  fuses the visual
appearance of a series of object crops which are connected with relationships deﬁned in the scene
graph. As shown in Figure 4  for a single layer  we expand the dimension of predicate vector zpj
to the dimension of D × h × w to get a predicate feature map vpj . Then  a tuple of feature maps
(voi  vpj   vok ) is fed into gs  gp  go  ﬁnally avg averages the information and a new tuple of feature
maps (v(cid:48)
) is produced. The new object crop feature map encodes the visual appearance of
both itself and others  and contains the relationship information as well.

  v(cid:48)

pj

  v(cid:48)

ok

oi

3.4 Object-Image Fuser

Object-Image Fuser focuses on fusing all the object crops into a latent scene canvas L. As we have
faked an (cid:48)image(cid:48) object  which is connected to every object through (cid:48)in_image(cid:48)  we further extend
the GCN to pass the objects to (cid:48)image(cid:48). Firstly  we concatenate expanded object feature vector zoi
and object crop feature map voi to get an integral latent feature representation and replicate it with
corresponding bounding boxes ˆbi to get ui ∈ RD×w×h. Similarly  the new predicate feature map upi
are concatenated through vpi and expanded predicate vector zpi. Next  we select upi corresponding
to (cid:48)in_image(cid:48) relationship and ui to calculate the attention map  where f (·) = Wf u  q(·) = Wqup 

βi =

  where ti = f (ui)T q(upi) 

(2)

exp(ti)
j=1 exp(tj)

(cid:80) N
(cid:88) N

1

and βi indicates the extent to which the fuser attends the ith object at every pixel. Then the output of
the attention module is uattn = (uattn

  ...  uattn

  ...  uattn

  uattn

N )  where

2

j

uattn =

i=1βil(ui)  where l(ui) = Wlui.

(3)
Finally  the last operation layer hsum aggregates all the object features into the (cid:48)image(cid:48) object feature
map by
(4)
where λattn is a balancing parameter. The latent canvas L is formed by upsampling y to D × H × W .

y = λattnuattn + uimg 

3.5

Image Decoder

The image decoder  based on a Cascaded Reﬁnement Network (CRN)  takes as input the latent scene
canvas L and generates an image I(cid:48) that respects the object positions given in L. A CRN consists of a
series of cascaded reﬁnement modules  with spatial resolution doubling between consecutive modules.
The input to each reﬁnement module is a channelwise concatenation of the latent scene canvas L
(downsampled to the input resolution of the module) and the feature map output by the previous
reﬁnement module. The input is processed by a pair of 3×3 convolution layers followed by batch

5

Figure 4: (a) A single layer of Object2 Reﬁner. A tuple of feature maps(cid:0)voi   vpj   vok
(cid:1) is fed into
(symbol(cid:76)) with corresponding object crop feature map vi to form an object integral representation.
Then the feature map is reproduced by ﬁlling the region (symbol(cid:78)) within the object bounding

gs  gp  go for better visual appearance fusion. The last operation avg averages all the information
to form a new crop feature map. (b) Object-Image Fuser. Object latent vector zi is concatenated

box ˆbi to get ui  the rest of feature map are all zeros. Next  Attn takes as input ui and (cid:48)in_image(cid:48)
predicate feature map upi to calculate the attention and form a new feature map uattn
. Finally  hsum
sums all feature maps uattn

to (cid:48)image(cid:48) object feature map uimg to form the latent scene canvas.

i

i

normalization and ReLU; the output feature map is upsampled using nearest-neighbor interpolation
before being passed to the next module. The ﬁrst reﬁnement module takes Gaussian noise as input
for the purpose of increasing diversity  and the output from the ﬁnal module is processed with two
ﬁnal convolution layers to produce the output image I(cid:48).

3.6 Discriminators

LGAN = E

log D(1 − D(x))

We adopt a pair of discriminator Dimg and Dobj to generate realistic images and recognizable objects
by training the generator network adversarially. The discriminator D tries to classify the input x as
real or fake by maximizing the objective

log D(x) + E

x∼preal

x∼pfake

(5)
where x ∼ preal represents the real images and x ∼ pfake represents the generated images. Meanwhile 
the generator network is optimized to fool the discriminators by minimizing LGAN .
Dimg plays a role of promoting the images to be realistic through classifying the input images  real
images I  reconstructed ones ˆI and generated ones I(cid:48)  as real or fake. Dobj takes as input the resized
object crops cropped from real images and generated ones  and encourages that each object in the
generated images appears realistic and clear. In addition  we also add an auxiliary object classiﬁer
which predicts the category of the object to ensure that the objects are recognizable.

3.7 Training

We end-to-end train the generator and the two discriminators Dimg and Dobj in an adversarial manner.
The generator is trained to minimize the weighted sum of eight losses:
Image Reconstruction Loss. Limg
truth image I and the reconstructed image ˆI  which is helpful for stable convergence of training.
Crop Matching Loss. For the purpose of respecting the object crops’ appearance  Llatent

1 = (cid:107)I − ˆI(cid:107)1 penalizes the L1 differences between the ground-

=
i(cid:107)1 penalizes the L1 difference between the object crop feature map and the feature

(cid:80) n
i=1(cid:107)vi − v(cid:48)

GAN from Dimg and object adversarial loss Lobj

map of object re-extracted from the generated images.
Adversarial Loss. Image adversarial loss Limg
Dobj encourage generated image patches and objects to appear realistic respectively.
Auxiliary Classiﬁer Loss. Lobj
AC ensures generated objects to be recognizable and classiﬁed by Dobj.
Perceptual Loss. Image perceptual loss Limg
penalizes the L1 difference in the global feature space
between the ground-truth image I and the reconstructed image ˆI  while object perceptual loss Lobj
penalizes that between the original crop and the object crop re-extracted from ˆI. Mittal et al. [27]

GAN from

P

P

1

6

!"!#!$avgavg%#&%$'%#(%′#&%′#(%′$'ℎ"+ ⋮⋮⋮./%#0(a)(b)Attn1/1$01/233456/<.8 %#& 568 1$&>18233418 ;Dataset
Train
Val.
Test
# Obj.
# Crops

COCO
74 121
1 024
2 048
171

411 682

VG

62 565
5 506
5 088
178

606 319

Table 1: Statistics of COCO-Stuff and
VG dataset. # Obj. denotes the number
of object categories. # Crops denotes the
number of crops in the external memory.

Method
Real Images
w/o Crop Selection
w/o Object2 Reﬁner
w/o Obj-Img Fuser
full model
full model (GT)

IS ↑

16.3 ± 0.4
7.1 ± 0.3
8.3 ± 0.3
8.7 ± 0.2
9.1 ± 0.2
10.2 ± 0.2

FID ↓

-

96.75
61.28
56.14

50.94
38.29

Table 2: Ablation Study using Inception Score (IS)
and Fréchet Inception Distance (FID) on COCO-Stuff
dataset.

used perceptual loss for images generated in the intermediate steps to enforce the images to be
perceptually similar to the ground truth ﬁnal image. So we add a light weight perceptual loss between
generated images and ground-truth images to keep the perceptual similarity. And we found this loss
would help to balance the training process.

Box Regression Loss. Lbox =(cid:80) n

i=1(cid:107)bi − ˆbi(cid:107) penalizes the L1 difference between ground-truth

and predicted boxes.
Therefore  the ﬁnal loss function of our model is deﬁned as:
L = λ1Limg
where  λi are the parameters balancing losses.

1 + λ2Llatent

GAN + λ4Lobj

+ λ3Limg

1

GAN + λ5Lobj

AC + λ6Limg

P + λ7Lobj

P + λ8Lbox (6)

4 Experiments
We trained our model to generate 64 × 64 images  as an comparison to previous works on scene
image generation [4  17]. Apart from the substantial improvements to Inception Score  Diversity
Score and Fréchet Inception Distance  we aim to show that images generated by our model not only
respect the relationships provided by the scene graph  but also high respect the original appearance of
the object crops.

4.1 Experiment Settings

Datasets. COCO-Stuff [28] and Visual Genome [25] are two datasets used by previous scene image
generation models [4  17]. We apply the preprocessing and data splitting strategy used by [4]  the
version of COCO-Stuff annotations is 2017 latest. Table 1 displays the attributes of the datasets.
Implementation Details. Scene graphs are argumented with a special (cid:48)image(cid:48) object  and special
(cid:48)in_image(cid:48) relationships connecting each true object with the image object; ReLU is applied for
graph convolution and CRN; discriminators use LeakyReLU activation and batch normalization. The
image and crop size are set to 64 × 64 and 32 × 32 correspondingly. We train all models using
Adam [29] with learning rate 5e-4 and batch size of 32 for 200 000 iterations; training takes about 3 ∼
4 days on a single Tesla Titan X. The λ1 ∼ λ8 are set to 1  10  1  1  1  1  0.5 and 10 respectively. More
details and qualitative results for ablation comparison can be found in the supplementary material.

4.2 Evaluation Metrics

Inception Score. Inception Score [30] computes the quality and diversity of the synthesized images.
Same as previous work  we employed Inception V3 [31] to compute Inception Score.
Diversity Score. Different from the Inception Score that calculates the diversity of the entire set
of generated images  Diversity Score measures the perceptual difference between a pair of images.
Same as the former approach of calculating Diversity Score [17]  we use the Alex-lin metric [32] 
which inputs a pair of images into an AlexNet and compute the L2 distance between their scaled
activations.
Fréchet Inception Distance. FID is a more robust measure because it penalizes lack of variety but
also rewards IS  and is analogous to human qualitative evaluation.

7

Figure 5: Examples of 64 × 64 generated images using sg2im and our proposed PasteGAN on the
test sets of VG (left 4 columns) and COCO (right 4 columns). For each example we show the input
scene graph and object crops selected by Crop Selector. Some scene graphs have deprecated similar
relationships and we show at most 6 object crops here. Please zoom in to see the details between
crops and generated images.

4.3 Comparison with Existing Methods

Two state-of-the-art scene image generation models are compared with our work. sg2im [4]: Most
lay-
related to our work  we take the code2 released by sg2im to train a model for evaluation.
out2im [17]: We list the Inception Score and Diversity Score reported by layout2im. For sake of
fairness  we provide our model with ground truth boxes in comparison with layout2im. Table 3 shows
the performance of our model compared to the SOTA methods and real images.

Method
Real Imgs

Inception Score ↑
COCO
VG
16.3 ± 0.4 13.9 ± 0.5
6.7 ± 0.1
9.1 ± 0.2
7.3 ± 0.1
9.1 ± 0.1

Diversity Score ↑
VG
COCO
-

-

FID ↓

COCO VG
-

-

sg2im

5.5 ± 0.1 0.02 ± 0.01 0.12 ± 0.06 82.75 71.27
6.9 ± 0.2 0.27 ± 0.11 0.24 ± 0.09 50.94 58.53
6.3 ± 0.2 0.02 ± 0.01 0.15 ± 0.12 63.28 52.96
8.1 ± 0.1 0.15 ± 0.06 0.17 ± 0.09
PasteGAN (GT) 10.2 ± 0.2 8.2 ± 0.2 0.32 ± 0.09 0.29 ± 0.08 38.29 35.25

PasteGAN
sg2im (GT)
layout2im

-

-

Table 3: Performance on COCO-Stuff and VG datsaset in Inception Score  Diversity Score and
Fréchet Inception Distance (FID).

4.4 Qualitative Results

We use sg2im model released to generate images for comparison with ours. Each scene graph is
paired with two sets of object crops for our PasteGAN. Figure 5 shows example scene graphs from
the VG and COCO-Stuff test sets and corresponding generated images using sg2im and our method.

2https://github.com/google/sg2im

8

buildingtreemountainreflectionroofboatin front ofhashasonriverOurs BOurs Asg2imcCrops BCrops AGraphbehindtreeroadbuswindowonhasskyabovetreefieldcloudhorsefenceoninstonesheephillgrassskysheepsheepstanding inbehindeatingin front ofabovepersoninsideplayfield(a)(b)(c)(d)(e)(f)(g)(h)insideairplaneskytreebelowpersonWall-panelbelowpersonleft ofright oftablesurroundingdining tableinsideforkinsidecakepersonabovesnowskistreeabovecloudsaboveaboveBoth two methods can generate scenes with multiple objects  and respect the relationships in the
scene graph; for example in all the three images in Figure 5 (a) we see a boat on the river  which has
its own reﬂection. More importantly  these results indicate that with our method  the appearance of
the output scene image can be ﬂexibly adjusted by the object crops. In (a)  the ship in crop set A has
a white bow and the ship in crop set B has a black bow  and this is highly respected in our generated
images. Similarly  as shown in (b)  our generated image A contains a bus in white and red while
generated image B contains a bus in red; this respects the color of the bus crops A and B.
It is clear that our model achieves much better diversity. For example in (d)  the sheeps generated by
sg2im looks almost the same  however  our model generates four distinct ships with different colors
and appearances. This is because sg2im forces their model to learn a more general representation
of the sheep object  and stores the learned information in a single and ﬁxed word embedding.
Additionally  the selected crops also clearly proves Crop Selector’s powerful ability of capturing and
utilizing the information provided by scene graphs and object crops’ visual appearance. Our model
represent an object with both the word embedding and the object crop  this provides PasteGAN the
ﬂexibility to generate image according to the input crops.

4.5 Ablation Study

We demonstrate the necessity of all components of our model by comparing the image quality of
several ablated versions of our model  shown in table 2. We measure the images with Inception Score
and Fréchet Inception Distance. The following ablations of our model is tested:
No Crop Selection omits Crop Selector and makes our model utilize random crops from same
categories. This signiﬁcantly hurts the Inception Score  as well as FID  and visible qualities  since
irrelevant or unsuitable object crops are forced to be merged into scene image  which confuses the
image generation model. Diversity Score doesn’t decrease because the random crops preserve the
complexity accidentally. The results in supplementary material further demonstrate the powerful
ability of Crop Selector to capture the comprehensive representation of scene graphs for selecting
crops.
No Object2 Reﬁner omits the graph convolution network for feature map fusing  which makes the
model fail to utilize the relationship between objects to better fuse the visual appearance in the image
generation. That Inception Score decreases to 8.3 and FID becomes 61.28 indicate using the feature
maps of crops straightly without fusion is harmful to the generation. It tends to produce overrigid and
strange images.
No Object-Image Fuser omits the last fusion module  generating the latent canvas only by replicating
features within bounding boxes. We observe worse results on both the visual quality and the
quantitative metrics. More qualitative results are shown in supplementary material.

5 Conclusion

In this paper we have introduced a novel method for semi-parametrically generating images from
scene graphs and object crops. Compared to leading scene image generation algorithm which generate
image from scene graph  our method parametrically controls the appearance of the objects in the
image  while maintaining a high image quality. Qualitative results  quantitive results  comparison to a
strong baseline and ablation study justify the performance of our method.

Acknowledgments

This work is supported in part by SenseTime Group Limited  in part by Microsoft Research  and in
part by the General Research Fund through the Research Grants Council of Hong Kong under Grants
CUHK14202217  CUHK14203118  CUHK14207319.

References
[1] Han Zhang  Tao Xu  Hongsheng Li  Shaoting Zhang  Xiaogang Wang  Xiaolei Huang  and Dimitris N
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.
In Proceedings of the IEEE International Conference on Computer Vision  pages 5907–5915  2017.

9

[2] Han Zhang  Tao Xu  Hongsheng Li  Shaoting Zhang  Xiaogang Wang  Xiaolei Huang  and Dimitris
Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. arXiv
preprint arXiv:1710.10916  2017.

[3] Xiaojuan Qi  Qifeng Chen  Jiaya Jia  and Vladlen Koltun. Semi-parametric image synthesis. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  pages 8808–8816  2018.

[4] Justin Johnson  Agrim Gupta  and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition  pages 1219–1228  2018.

[5] Aaron van den Oord  Nal Kalchbrenner  and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv

preprint arXiv:1601.06759  2016.

[6] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

[7] Yunchen Pu  Zhe Gan  Ricardo Henao  Xin Yuan  Chunyuan Li  Andrew Stevens  and Lawrence Carin.
Variational autoencoder for deep learning of images  labels and captions. In D. D. Lee  M. Sugiyama  U. V.
Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages
2352–2360. Curran Associates  Inc.  2016.

[8] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680  2014.

[9] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[10] Jon Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project for
Stanford CS231N: Convolutional Neural Networks for Visual Recognition  Winter semester  2014(5):2 
2014.

[11] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 

2014.

[12] Augustus Odena  Christopher Olah  and Jonathon Shlens. Conditional image synthesis with auxiliary
classiﬁer gans. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 2642–2651. JMLR. org  2017.

[13] Hong Seunghoon  Yang Dingdong  Choi Jongwook  and Lee Honglak. Inferring semantic layout for

hierarchical text-to-image synthesis. In CVPR  2018.

[14] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded reﬁnement networks. In

Proceedings of the IEEE International Conference on Computer Vision  pages 1511–1520  2017.

[15] Leon A Gatys  Alexander S Ecker  and Matthias Bethge. Image style transfer using convolutional neural
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
2414–2423  2016.

[16] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer and

super-resolution. In European conference on computer vision  pages 694–711. Springer  2016.

[17] Bo Zhao  Lili Meng  Weidong Yin  and Leonid Sigal. Image generation from layout. In CVPR  2019.

[18] Justin Johnson  Ranjay Krishna  Michael Stark  Li-Jia Li  David Shamma  Michael Bernstein  and Li Fei-
Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 3668–3678  2015.

[19] Peter Anderson  Basura Fernando  Mark Johnson  and Stephen Gould. Spice: Semantic propositional
image caption evaluation. In European Conference on Computer Vision  pages 382–398. Springer  2016.

[20] Danfei Xu  Yuke Zhu  Christopher B Choy  and Li Fei-Fei. Scene graph generation by iterative message
passing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
5410–5419  2017.

[21] Yikang Li  Wanli Ouyang  Zhou Bolei  Shi Jianping  Zhang Chao  and Xiaogang Wang. Factorizable net:

An efﬁcient subgraph-based framework for scene graph generation. In ECCV  2018.

[22] Alejandro Newell and Jia Deng. Pixels to graphs by associative embedding. In Advances in neural

information processing systems  pages 2171–2180  2017.

10

[23] Yikang Li  Wanli Ouyang  Bolei Zhou  Kun Wang  and Xiaogang Wang. Scene graph generation from

objects  phrases and region captions. In ICCV  2017.

[24] Yikang Li  Wanli Ouyang  Bolei Zhou  Kun Wang  and Xiaogang Wang. Vip-cnn: Visual phrase guided

convolutional neural network. In CVPR  2017.

[25] Ranjay Krishna  Yuke Zhu  Oliver Groth  Justin Johnson  Kenji Hata  Joshua Kravitz  Stephanie Chen 
Yannis Kalantidis  Li-Jia Li  David A Shamma  et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. International Journal of Computer Vision  123(1):32–73 
2017.

[26] Mikael Henaff  Joan Bruna  and Yann LeCun. Deep convolutional networks on graph-structured data.

arXiv preprint arXiv:1506.05163  2015.

[27] Mittal Gaurav  Agrawal Shubham  Agrawal Anuva  Mehta Sushant  and Marwah Tanya. Interactive image

generation using scene graphs. arXiv preprint arXiv:1905.03743  2019.

[28] Holger Caesar  Jasper Uijlings  and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In The

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2018.

[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR  2015.

[30] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  Xi Chen  and Xi Chen.
Improved techniques for training gans. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett 
editors  Advances in Neural Information Processing Systems 29  pages 2234–2242. Curran Associates 
Inc.  2016.

[31] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 2818–2826  2016.

[32] Richard Zhang  Phillip Isola  Alexei A Efros  Eli Shechtman  and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 586–595  2018.

11

,Haitian Sun
William Cohen
Lidong Bing
Yikang LI
Tao Ma
Yeqi Bai
Nan Duan
Sining Wei
Xiaogang Wang