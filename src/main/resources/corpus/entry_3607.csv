2018,Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations,We present the Multi-value Rule Set (MRS) for interpretable
classification with feature efficient presentations. Compared to
rule sets built from single-value rules  MRS adopts a more
generalized form of association rules that allows multiple values
in a condition. Rules of this form are more concise than classical
single-value rules in capturing and describing patterns in data.
Our formulation also pursues a higher efficiency of feature utilization 
which reduces possible cost in data collection and storage.
We propose a Bayesian framework for formulating an MRS model
and develop an efficient inference method for learning a maximum
a posteriori  incorporating theoretically grounded bounds to iteratively
reduce the search space and improve the search efficiency.
Experiments on synthetic and real-world data demonstrate that
MRS models have significantly smaller complexity and fewer features
than baseline models while being competitive in predictive
accuracy.,Multi-value Rule Sets for Interpretable Classiﬁcation

with Feature-Efﬁcient Representations

Tong Wang

Tippie School of Business

University of Iowa
Iowa City  IA 52242

tong-wang@uiowa.edu

Abstract

We present the Multi-value Rule Set (MRS) for interpretable classiﬁcation with
feature efﬁcient presentations. Compared to rule sets built from single-value rules 
MRS adopts a more generalized form of association rules that allows multiple
values in a condition. Rules of this form are more concise than classical single-
value rules in capturing and describing patterns in data. Our formulation also
pursues a higher efﬁciency of feature utilization  which reduces possible cost in
data collection and storage. We propose a Bayesian framework for formulating an
MRS model and develop an efﬁcient inference method for learning a maximum
a posteriori  incorporating theoretically grounded bounds to iteratively reduce the
search space and improve the search efﬁciency. Experiments on synthetic and real-
world data demonstrate that MRS models have signiﬁcantly smaller complexity
and fewer features than baseline models while being competitive in predictive
accuracy. Human evaluations show that MRS is easier to understand and use
compared to other rule-based models.

1

Introduction

In many real-world applications of machine learning  human experts desire the interpretability of a
model as much as the predictive accuracy. As opposed to “black box” models  interpretable models
are easy for humans to understand and extract insights  which is imperative in domains such as
healthcare  law enforcement  etc. In some occasions  the need for interpretability even outweighs
that for accuracy due to legal or ethnic concerns. Among different forms of interpretable models  we
are particularly interested in rule-based models in this paper. This type of models produce decisions
based on a set of rules following simple “if-else” logic: if a rule (or a set of rules) is satisﬁed  the
model outputs the corresponding decision. The set of rules can be either ordered [17  35  5] or
unordered [15  30  19  24]  depending on the speciﬁc model structure.
Prior rule-based models in the literature are from built single-value rules [15  30  19]. For example 
[State = California] AND [Marital status = married]  where a condition (e.g.  [state=California]) is
a pair of a feature (e.g.  state) and a single value (e.g.  California). However  while single-value
rules can express primitive concepts  they are inadequate in capturing more general trends in the
underlying data  especially when working with features with a medium to high cardinality. Rules
built from these features tend to have too small support. They are either less likely to be selected
in the ﬁnal output  introducing selection bias in the model [7]  or induce a large model if selected 
hurting the model interpretability. For example  to capture a set of married or divorced people
who live in California  Texas  Arizona  and Oregon  a model needs to include eight rules  each
rule being a combination of a state and a marital status  yielding an overly complicated model. As
modern machine learning has in part moved on to pursue a better and more concise way of model

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

presentation as well as predictive accuracy  single-value rules do not sufﬁce for some applications 
neither do models built from them.
To mitigate this problem  rules in a more generalized form have been proposed in the literature that
allow multiple values [26  22]  also called internal disjunctions of values in a condition [6]. For
example: [state = California or Texas or Arizona or Oregon] AND [marital status = married or
divorced]. In this case  we only need one rule instead of eight single-value rules  yielding a more
concise presentation while preserving the information. We refer to rules of this form multi-value
rules  which will serve as the building blocks of our proposed model in this paper. The prior efforts
on multi-value rules have mainly focused on ﬁnding individual rules and using heuristics such as
interestingness  conﬁdence  etc.  instead of building a principled classiﬁcation model with a global
objective function that considers predictive accuracy and model complexity.
Another important aspect that has been overlooked by previous rule-based models is the need to
control the total number of unique features. The number of different entities humans need to com-
prehend is directly associated with how easy it is to understand the model  as conﬁrmed by the
conclusions of Miller [21] relative to the magical number seven. With fewer features involved  it
also becomes easier for domain experts to gain clear insights into the data. In practice  models using
fewer features are easier to understand and bring down the overall cost in data collection.
To combine the factors considered above  we propose a novel rule-based classiﬁer  Multi-value Rule
Set (MRS)  which is a set of multi-value rules. An instance is classiﬁed as positive if it satisﬁes at
least one of the rules. A MRS has great advantages over models built from single-value rules in (i)
a more concise presentation of information and (ii) using a smaller number of features in the model.
We develop a Bayesian framework for learning MRS which provides a uniﬁed framework to jointly
optimize data ﬁtting and model complexity without directly “hard” controlling either. We propose
a principled objective combining the interpretability and the predictive accuracy where we devise a
prior model that promotes a small set of short rules using a few features. We propose an efﬁcient
inference algorithm for learning a maximum a posteriori model. We show with experiments on
standard data sets that MRS produces predictive accuracy comparable to or better than prior art with
lower complexity and fewer overall features.

2 Related Work

There has been a series of research on developing rule-based models for classiﬁcation [31  15  12 
3  28  19  5  24  24]. Various structures and formats of models were proposed  from the earlier
work on Classiﬁcation based on Association Rules (CBA) [18] and Repeated Incremental Pruning
to Produce Error Reduction (Ripper) [5] to more recent work on rule sets [31  15  19] and rule lists
[16  29]. A major development along this line of work is that interpretability has been recognized
and emphasized. Therefore controlling model complexity for easier interpretation is becoming an
important component in the modeling. However  previously mentioned models rely on single-value
rules and are limited in the expressive power  leaving redundancy in the model. In addition  learning
in previous methods is mostly a two-step procedure[31  18  16]  that ﬁrst uses off-the-shelf data
mining algorithms to generate a set of rules and then chooses a set from them to form the ﬁnal
model. This in practice will encounter the bottleneck of mining rules of a large maximum length
(millions of rules can be generated from a medium size data set if the maximum length is set to
only 3 [31]). Furthermore  few of the previous works consider limiting the number of features. Our
model aims to combines rule learning and feature assignment into the same process.
Our work is broadly related to generalized association rules that consider disjunctive relationships.
Among various works along this line  some consider disjunction in the rule level  using the dis-
junction connector instead of a conjunction connector as used by classical rule form. For example 
a1_a2_(cid:1)(cid:1)(cid:1) ! Y   where a1 is a rule. Representative works include [10  9  8]. This primitive form of
rules was extended to consider disjunctions in the condition / literal level [22]  yielding multi-value
rules of the form (a1 _ a2 _ (cid:1)(cid:1)(cid:1) ) ^ (b1 _ b2 (cid:1)(cid:1)(cid:1) ) ! Y . Prior efforts have mainly focused on mining
individual multi-value rules [25  10] using heuristics such as interestingness. Some works built clas-
siﬁers comprised of multi-value rules [1  2  20]. However  they still rely on greedy methods such as
greedy induction to build a model  and they do not consider model complexity or restrict the number
of features. Here  we optimize a global objective that considers predictive accuracy  model size  and
the total number of features. By tuning the parameters in the Bayesian framework  our model can

2

strike a nice balance between the different aspects of the model to suit the domain speciﬁc need of
users.

3 Multi-value Rule Sets
We work with standard classiﬁcation data set S that consists of N observations fxn; yngN
n=1. Let
y represent the set of labels.Each observation has J features  and we denote the jth feature of the
nth observation as xnj. Let Vj represent a set of values the jth feature takes. This notation can be
adapted to continuous attributes by discretizing the values.

3.1 Multi-value Rules

Now we introduce the basic components in Multi-value Rule Set model.
Deﬁnition 1 An item is a pair of a feature j and a value v  where j 2 f1; 2;(cid:1)(cid:1)(cid:1) ; Jg and v 2 Vj.
Deﬁnition 2 A condition is a collection of items with the same feature j  denoted as c = (j; V ) 
where j 2 f1; 2;(cid:1)(cid:1)(cid:1) ; Jg and V (cid:26) Vj. V is a union of values in the items.
Deﬁnition 3 A multi-vale rule is a conjunction of conditions  denoted as r = fckgk.
Interchangeable values are grouped into a value set in a condition  such as [state = California or
Texas or Arizona or Oregon]. Following the deﬁnitions  an item is the atom in a multi-value rule. It
is also a special case of a condition with a single value  for example  [state = California].
Now we deﬁne a classiﬁer built from multi-value rules. By an abuse of notation  we use r((cid:1)) to
represent a Boolean function that indicates if an observation satisﬁes rule r: r((cid:1)) : X ! f0; 1g: Let
R denote a Multi-value Rule Set. We deﬁne a classiﬁer R((cid:1)):

{

R(x) =

1 9r 2 R; r(x) = 1
0

otherwise.

(1)

x is classiﬁed as positive if it satisﬁes at least one rule in R and we say x is covered by r.

3.2 MRS Formulation

Our proposed framework considers two aspects of a model: 1) interpretability  characterized by a
prior model for MRS  which considers the complexity (number of rules and lengths of rules) and
feature assignment. 2) predictive accuracy  represented by the conditional likelihood of data given
an MRS model. Both components have tunable parameters to trade off between interpretability and
predictive accuracy. Now we formulate the model.
Prior for MRS The prior model for MRS jointly determines the number of rules M  lengths of
m=1 and feature assignment fzmgM
rules fLmgM
m=1  where m is the rule index. We propose a two-
step process for constructing a rule set  where the ﬁrst step determines the size and shape of an MRS
model and the second step ﬁlls in the empty “boxes” with items.
Creating empty “boxes” - complexity assignment: First  we draw the number of rules M from a
Poisson distribution  where (cid:21)M (cid:24) Gamma((cid:11)M ; (cid:12)M ). Second  we determine the number of items
in each rule  denoted as Lm. Lm (cid:24) Poisson((cid:21)L)  which is a Poisson distribution truncated to only
allow positive outcomes. The arrival rate for this Poisson distribution  (cid:21)L  is governed by a Gamma
distribution with parameters (cid:11)L; (cid:12)L. Since we favor simpler models for interpretability purposes 
we set (cid:11)L < (cid:12)L and (cid:11)M < (cid:12)M to encourage a small set of short rules. These two steps together
determine the size and shape of an MRS model. Therefore  we call parameters (cid:11)M ; (cid:12)M ; (cid:11)L; (cid:12)L
shape parameters. Hs = f(cid:11)M ; (cid:12)M ; (cid:11)L; (cid:12)L; (cid:18)g. This step creates empty “boxes” to be ﬁlled with
items in the following step and assigns overall complexity to the model.
Filling “boxes” - feature assignment: A m-th rule is a collection of Lm “boxes”  each containing
an item. Let zmk represent the feature assigned to the kth box in the m-th rule  where zmk 2
f1; :::; Jg and zm represent the set of feature assignments in the m-th rule. We sample zm from
a multinomial distribution with weights p drawn from a Dirichlet distribution parameterized by

3

∑

k

(zmk = j) and

hyperparameter (cid:18) = f(cid:18)jgJ
j=1. Let lmj denote the number of items with attribute j in the m-th
rule  i.e.  lmj =
j lmj = Lm. It means lmj items share the same feature
j and therefore can be merged into a condition. We truncate the multinomial distribution to only
allow lmj (cid:20) jVjj. Remarks: we use Multinomial-Dirichlet distribution for feature assignment for
its clustering property of the outcomes. The prior model will tend to re-use features already in the
rule. This is consistent with the interpretability goal of our model: we would like to form a MRS
model with fewer features so that multiple items can be merged in to one condition. The prior does
not consider values in each item since they do not affect the size and the shape of the model and
therefore have no effect on the interpretability. In summary  the prior for MRS model follows a
distribution below  where C is a function of Hs and (cid:0)((cid:1)) is a gamma function.

∏

J
j=1 (cid:0)(lmj + (cid:18)j)

∑

∑

M∏

m=1

p(R; Hs) / (cid:0)(M

+ (cid:11)M )CM

(cid:3)
(cid:0)(M(cid:3) + 1)

(cid:0)(Lm + (cid:11)L)
(cid:0)(Lm + 1)

((cid:12)L + 1)Lm (cid:0)(Lm +

j=1 (cid:18)j)

:

(2)

Conditional Likelihood Now we consider the predictive accuracy of a MRS by modeling the condi-
tional likelihood of labels y given features x and a MRS model R. Our prediction on the outcomes
are based on the coverage of MRS. According to formula (1)  if an observation satisﬁes R (covered
by R)  it is predicted to be positive  otherwise  it’s negative. We assume label yn is drawn from
a Bernoulli distribution with probabilities (cid:26)+ or (cid:26)(cid:0) to be consistent with the predicted outcome.
Speciﬁcally  when R(xn) = 1  i.e.  xn satisﬁes the rule set  yn has probability (cid:26)+ to be positive 
and when R(xn) = 0  yn has probability (cid:26)(cid:0) to be negative. (cid:26)+; (cid:26)(cid:0) govern the predictive accuracy
on the training data. We assume that they are drawn from two Beta distributions with hyperparam-
eters ((cid:11)+; (cid:12)+) and ((cid:11)(cid:0); (cid:12)(cid:0))  respectively  which control the predictive power of the model. The
conditional likelihood is shown below  given parameters Hc = f(cid:11)+; (cid:12)+; (cid:11)(cid:0); (cid:12)(cid:0)g:
p(yjx; R; Hc) / B(TP + (cid:11)+; FP + (cid:12)+)B(TN + (cid:11)(cid:0); FN + (cid:12)(cid:0));

(3)
where TP  FP  TN and FN represent the number of true positives  false positives  true negatives and
false negatives  respectively. B((cid:1)) is a Beta function which comes from integrating out (cid:26)+; (cid:26)(cid:0) in the
conditional likelihood function.
We will write p(R; Hs) as p(R) and p(yjx; R; Hc) as p(yjx)  ignoring dependence on parameters
when necessary. Regarding setting hyperparameters Hs; Hc  there are natural settings for (cid:18) (all
entries being 1). This means there’s no prior preference for features. For Gamma distributions 
we set (cid:11)M and (cid:11)L to 1. Then the strength of the prior for constructing a simple MRS depends
on (cid:12)M and (cid:12)L. Increasing (cid:12)M and (cid:12)L decreases the expected number of rules and the expected
length of rules  thus penalizing more on larger models. There are four real-valued parameters in the
conditional likelihood to set  (cid:11)+; (cid:12)+; (cid:11)(cid:0); (cid:12)(cid:0). They jointly control the probability that a prediction
of MRS model is correct. Therefore we should always set (cid:11)+ > (cid:12)+; (cid:11)(cid:0) > (cid:12)(cid:0). The ratios of (cid:11)+; (cid:12)+
and (cid:11)(cid:0); (cid:12)(cid:0) are associated with the expected predictive accuracy. Setting values of the parameters
can be done through cross-validation  another layer of hierarchy with more diffuse hyperparameters 
or plain intuition.

3.3 Clustering of Features

′. Every rule in R

′ remains the same as R except in the m-th rule. Let l

; l
(cid:0) 1. We claim this ﬂip of feature increases the prior probability of the model  i.e. 

We use Multinomial-Dirichlet in the prior model to take advantage of the “clustering” effect in
feature assignment. Our goal is to formulate a model which favors rules with fewer features. Here
we prove this effect. Let R denote a MRS model and lmj represent the number of items in rule
m taking feature j. Now we do a small change in R: pick two features j1; j2 in rule m where
(cid:21) lmj2 and replace an item taking feature j2 with an item taking feature j1  and we denote the
lmj1
′
new rule set as R
mj2
= lmj1 + 1 and
denote the number of items taking feature j1 and j2 in the new model  and l
lmj2 = l
Theorem 1 If lmj1 + (cid:18)j1
When we choose uniform prior where all (cid:18)j are equal  the theorem will be reduced to a simpler
form  that the model always tends to reuse the most prevalent features. For example  given two
rules [state = California or Texas] AND [marital status = married] and [state = California] AND
[marital status = married] AND [age (cid:21) 45]  our model will favor rule sets containing the former  if
everything else being equal. (All proofs are in the supplementary material.)

(cid:21) lmj2 + (cid:18)j2  then p(R

′
mj2

′

) (cid:21) p(R).

′
mj1

′
mj1

4

4

Inference Method

Inference for rule-based models is challenging because it involves a search over exponentially many
possible sets of rules: since each rule is a conjunction of conditions  the number of rules increases
exponentially with the number of features in a data set  and the solution space (all possible rule sets)
is a powerset of the rule space. To obtain a maximum a posteriori (MAP) model within this solution
space  Gibbs Sampling takes tens of thousands of iterations or more to converge even searching
within a reduced space of only a couple of thousands of pre-mined and pre-selected rules [16  29].
Here we propose an efﬁcient inference algorithm that adopts the basic search procedure in simu-
lated annealing. Given an objective function p(RjS) over discrete search space of different rule sets
and a temperature schedule function over time steps  T [t]  a simulated annealing [13] procedure is
a discrete time  discrete state Markov Chain where at step t  given the current state R[t]  the next
state R[t+1] is chosen by ﬁrst proposing a neighbor and accepting it with probability that gradually
decreases with time. In this framework  we also incorporate the following strategies for faster com-
putation. 1) we use theoretical bounds for bounding the sampling chain to reduce computation. 2)
instead of randomly proposing a neighboring solution  we aim to improve from the current solution
by evaluating neighbors and pick the right one to move on to.

4.1 Theoretical bounds on MAP models

We exploit the model formulation to guide us in the search. We start by looking at MRS models
with one rule removed. Removing a rule will yield a simpler model but may lower the likelihood.
However  we can prove that the loss in likelihood is bounded as a function of the support. For a rule
set R and index z  we use Rnz to represent a set that contains all rules from R except the zth rule 
i.e.  Rnz = frmjrm 2 R; m ̸= zg: Deﬁne

(cid:12)(cid:0)(N+ + (cid:11)+ + (cid:12)+ (cid:0) 1)

(N(cid:0) + (cid:11)(cid:0) + (cid:12)(cid:0))(N+ + (cid:11)+ (cid:0) 1)

;

(cid:7) =

∑

n r(xn): Then the following holds:

where N+; N(cid:0) are the number of positive and negative examples  respectively. Notate the support
of a rule as supp(r) =
Lemma 1 If (cid:11)+ > (cid:12)+; (cid:11)(cid:0) > (cid:12)(cid:0)  then p(yjx; R) (cid:21) (cid:7)supp(z)p(yjx; Rnz).
(cid:7) is meaningful if (cid:7) (cid:20) 1  otherwise this lemma means adding a rule always increases the conditional
likelihood. This condition almost always holds since (cid:11)+ > (cid:12)+; (cid:11)(cid:0) > (cid:12)(cid:0) and we do not set (cid:12)+ to
a signiﬁcantly large value. In practice it is recommended to set (cid:12)+; (cid:12)(cid:0) to 1.
We then introduce some notations that will be used later. Let L(cid:3) denote the maximum likelihood
of data S  which is achieved when all data are classiﬁed correctly (this holds when (cid:11)+ > (cid:12)+ and
(cid:11)(cid:0) > (cid:12)(cid:0))  i.e. TP = N+  FP = 0  TN = N(cid:0)  and FN = 0  giving: L(cid:3)
:= B(N+ +(cid:11)+; (cid:12)+)B(N(cid:0) +
(cid:11)(cid:0); (cid:12)(cid:0)): Let v[t] denote the best solution found until iteration t  i.e. 

v[t] = max
(cid:28)(cid:20)t

p(R[(cid:28) ]jS):

According to the prior model  containing too many rules penalizes the model due to the large com-
plexity. Therefore  to hold a spot in the model  each rule needs to make enough “contribution” to the
objective  i.e.  capturing enough of the positive class  to cancel off the decrease in the prior. There-
fore  we claim that the support of rule in the MAP model is lower bounded  and the bound becomes
tighter as v[t] increases along the iterations.

H =

Theorem 2 Take a data set S and a MRS model with parameters

{
(cid:11)M ; (cid:12)M ; (cid:11)L; (cid:12)L; (cid:11)+; (cid:12)+; (cid:11)(cid:0); (cid:12)(cid:0);f(cid:18)jgJ
where H 2 (N+)J+8. Deﬁne R
2666 log M [t](cid:11)M Ω
(cid:12)L; (cid:11)+ > (cid:12)+; (cid:11)(cid:0) > (cid:12)(cid:0) and (cid:7) (cid:20) 1  we have:
M [t]+(cid:11)M(cid:0)1
log 1
(cid:7)

(cid:3) 2 arg maxR p(RjS; H) and M
⌊

3777 ; and M [t] =

; supp(r) (cid:21)

8r 2 R

log L(cid:3)

(cid:3)

}
(cid:3)j. If (cid:11)M < (cid:12)M ; (cid:11)L <
= jR

j=1

;

(cid:3)

⌋

;

+ log p(∅) (cid:0) v[t]
log Ω

5

∑

((cid:12)M +1)((cid:12)L+1)(cid:11)L+1

J
j=1 (cid:18)j

.

(cid:11)M (cid:12)(cid:11)L

L (cid:11)L max((cid:18))

where Ω =
p(∅) is the prior of an empty set. L(cid:3) and p(∅) upper bound the conditional likelihood and prior 
respectively. The difference between log L(cid:3)
+ log p(∅) and v[t]  the numerator in M [t]  represents
the room for improvement from the current solution v[t]. The smaller the difference  the smaller the
M [t]. When we choose (cid:11)M = 1  then the bound on support is reduced to

⌈

⌉

supp(r) (cid:21)

log Ω
log 1
(cid:7)

:

We can control the bounds by changing parameters in H to increase or decrease Ω. As Ω increases 
the bound M [t] decreases  which indicates a stronger preference for a simpler model with a smaller
number of rules. Simultaneously  the lower bound for support increases  which is equivalent to
reducing the search space. To increase Ω  one can increase (cid:12)M
  which is the expected number of
(cid:11)M
rules from the prior distribution  or increase (cid:12)L
  which is the expected number of items in each rule.
(cid:11)L
We incorporate the bound on the support in the search algorithm to check if a rule qualiﬁes to be
included.

4.2 Proposing step

Here we detail the proposing step at each iteration in the search algorithm. We simultaneously deﬁne
the set of neighbors and the process to choose a neighbor to propose. A “next state” is proposed by
ﬁrst selecting an action to alter the current MRS and then choosing from “neighboring” models
generated by that action. To improve the search efﬁciency  we do not perform a random action  but
instead  we sample from misclassiﬁed examples to choose an action that can improve the current
state R[t]. If the misclassiﬁed example is positive  it means R[t] fails to “cover” it and therefore
needs to increase the coverage by randomly choosing one of the following actions.

• Add a value: Choose a rule rm 2 R[t]  a condition ck 2 rm and then a candidate value
v 2 Vzmk
n(cid:23)(ck)  then ck (zmk; (cid:23)(ck) [ v). (cid:23)(ck) indicates the value(s) in condition ck.
• Remove a condition: Choose a rule rm 2 R[t] and a condition ck 2 rm  then rm = fck′ 2
rmjck′ ̸= ckg
• Add a rule: Generate a new rule r
R[t+1] R[t] [ r

) satisﬁes the bound in Theorem 2 

′ where supp(r

′

′

where we use (cid:23)((cid:1)) to access the feature in a condition.
On the other hand  if the misclassiﬁed example is negative  it means R[t] covers more than it should
and therefore needs to reduce the coverage by randomly choosing one of the following actions.

• Add a condition: Choose a rule rm 2 R[t] ﬁrst  choose a feature j
then a set of values V
• Remove a rule: Choose a rule rm 2 R[t]  then R[t+1] = fr 2 R[t]jr ̸= rmg

′ 2 Vj′  then update rm rm [ (j

; V

)

′

′

′ 2 f1;(cid:1)(cid:1)(cid:1) ; Jgnzm and

The above actions involve choosing a value  a condition  or a rule to perform the action on. Different
choices result in different neighboring candidate models. To select one from them  we evaluate
p((cid:1)jS) on every model. Then a choice is made between exploration (choosing a random model) and
exploitation (choosing the best model). This randomness helps to avoid local minima and helps the
Markov Chain to converge to a global optimum.
See the supplementary material for the complete algorithm.

5 Experimental Evaluation

We perform a detailed experimental evaluation of MRS models on simulated and real-world data sets.
The ﬁrst part of our experiments is designed to study the effect of hyperparameters on interpretability
and predictive accuracy. The second part of the experiments compares MRS with classic and state-
of-the-art benchmark baselines.

6

5.1 Accuracy & Interpretability Trade-off

(cid:3). Then we generate labels y from R

We generate ten data sets of 100k observations with 50 arbitrary numerical features uniformly drawn
from 0 to 1. For each data set  we construct a set of 10 rules by ﬁrst drawing the number of conditions
uniformly from 1 to 10 for each rule and then ﬁlling conditions with randomly selected features.
Since the data are numeric  we generate a range for each feature by randomly selecting two values
from 0 to 1  one as the lower boundary and the other as the upper boundary. These ten rules are
(cid:3): observations that
the ground truth rule set denoted as R
(cid:3) are positive. Then each data set is partitioned into 75% training and 25% testing. To
satisfy R
apply the MRS model  we discretize each feature into ten intervals and obtain a binary data set of
size 100k by 500 on which we run the proposed model. We set entries in (cid:18) to 1  (cid:11)+ = (cid:11)(cid:0) = 100
and (cid:12)+ = (cid:12)(cid:0) = 1. Out of the four shape parameters (cid:11)M ; (cid:12)M ; (cid:11)L; (cid:12)L  we ﬁx (cid:11)M ; (cid:11)L to 1 and only
vary (cid:12)M ; (cid:12)L. Larger values of (cid:12)M ; (cid:12)L indicate a stronger prior preference for simpler models. Let
(cid:12)M ; (cid:12)L take values from f1; 10; 100; 1000; 10000g  giving a total of 25 sets of parameters. On each
training data set  we run the MRS model with the 25 sets of parameters and then evaluate the output
model on the test set. We repeat the process for ten data sets. Figure 1 shows the hold-out error  the
number of conditions and the number of features used in the model. Each block corresponds to a
parameter set. The values are averaged over ten data sets.

(a) the avg error rate.
Figure 1: Effect of shape parameters on predictive accuracy and interpretability.

(b) the avg number of conditions. (c) the avg number of features.

The left-bottom corner represents models with the least constraint on complexity ((cid:12)M = 1; (cid:12)L = 1)
and they achieve the lowest error but at the cost of the highest complexity and the largest feature set.
As (cid:12)M and (cid:12)L increase  the model becomes less complex  with fewer conditions and fewer features 
but at the cost of predictive accuracy. The right-top corner represents models with the strongest
preference for simplicity: the smallest model with the largest error. The three ﬁgures show a clear
pattern of the trade-off between interpretability and predictive accuracy.
5.2 Real World data sets

We then evaluate the performance of MRS on six real-world data sets from law enforcement  health-
care  and demography where interpretability is most desired. The data sets are publicly available at
UCI Machine Learning Repository or ICPSR. Among these  medical data sets are especially suitable
for MRS since many features such as diagnose categories have very high cardinalities.

Table 1: A summary of data sets

data set
Juvenile Delinquency [23]
Credit card [34]
Census [14]
Recidivism
Hospital Readmission [27]
In-hospital Mortality

N
4 023
30 000
48 842
11 645
100 000
200 000

d
69
24
14
106
55
14

Y = 1
delinquency
credit card default
income(cid:21) 50k
recidivism
readmitted
death in hospital

Features
exposure to violence  demo  etc
gender  history of past payment  etc
gender  age  occupation  etc
conviction  employment  demo  etc
diagnose history  symptoms  etc
diagnoses  medical history  etc

Baselines We benchmark the performance of MRS against the following rule-based models for clas-
siﬁcation: Scalable Bayesian Rule Lists (SBRL) [33]  Classiﬁcation Based on Associations (CBA)
[18]  Repeated Incremental Pruning to Produce Error Reduction (Ripper) [5] and Bayesian Rule
Sets (BRS) [31]. CBA and Ripper were designed to bridge the gap between association rule mining

7

and classiﬁcation and thus focused mostly on optimizing for predictive accuracy. They are among
the earliest and most-cited work on rule-based classiﬁers. On the other hand  BRS and SBRL  two
recently proposed frameworks aim to achieve simpler models as well as high predictive accuracy.
All of the four rule-based models use classical single value rules. Additionally  we would like to
quantify the possible loss (if any) in predictive accuracy for gaining interpretability. Therefore  we
also use two black-boxes  random forest and XGBoost to benchmark the performance without ac-
counting for interpretability.
Experimental Setup We performed 5-fold cross validation for each method. In each fold  we set
aside 20% of data during training for parameter tuning and used a grid search to locate the best set
of parameters. We use R and python packages for the random forest  SBRL  CBA and Ripper [11]
and use the publicly available code for BRS 1. The MRS model has a set of hyperparameters Hs; Hc.
We set entries in (cid:18) to 1  (cid:11)+ = (cid:11)(cid:0) = 100 and (cid:12)+ = (cid:12)(cid:0) = 1. (cid:11)M ; (cid:12)M control the number of rules
and (cid:11)L; (cid:12)L control lengths of rules. We set (cid:11)M ; (cid:11)L to 1 and vary (cid:12)M ; (cid:12)L. We report in Table 2 the
average test error  the average number of conditions in the output model  and the average number
of unique features used in each model  computed from the 5 folds. The standard deviations are also
reported.

Table 2: Evaluation of predictive performance and model complexity over 5-fold cross validation

Juvenile

Credit card

Census

Recidivism

Readmission

Mortality

Task
Method
Ripper
CBA
SBRL
BRS
MRS

accuracy ncond nfeat
.88(.01) 35(13) 23(5)
.88(.01) 27(22) 18(12)
.88(.01) 10(2)
9(2)
.88(.01) 21(4) 11(3)
.89(.00) 18(3)
6(2)
nval:
19(1)
RF
.90(.00)
XGBoost .91(.01)

–
–

–
–

accuracy ncond nfeat accuracy ncond nfeat accuracy ncond nfeat accuracy ncond nfeat
.82(.01) 23(8) 12(2)
.80(.01) 35(3) 6(0)
.82(.00) 15(2) 10(2)
.81(.01) 17(2) 8(2)
.82(.01) 10(7) 5(3)
nval:
.82(.00)
.83(.01)

ncond nfeat
.58(.01) 35(9) 12(1) .26(.01) 115(6) 9(1)
.61(.01) 39(10) 13(1) .28(.02) 435(18) 10(2)
4(1)
.61(.01) 21(1) 7(1)
6(1)
4(0)
.59(.01) 9(11) 5(3). 39(.01) 10(1)
.60(.00)
6(2)
3(1)
8(2)
nval:
–
.61(.00)
.60(.00)
–

.84(.01) 67(11) 7(0)
.79(.01) 13(12) 6(2)
.82(.00) 32(2) 10(1)
.79(.01) 33(11) 11(2)
.80(.00) 14(8) 5(3)
nval:
.86(.00)
.87(.01)

.39(.00)
nval:
.41(.01)
.41(.02)

.78(.00) 78(18) 32(4)
.72(.01) 87(25) 27(5)
.75(.00) 10(1) 9(1)
.73(.01) 16(11) 8 (3)
.74(.02)
3(1)
nval:
.74(.00)
.75(.05)

6(3)
8(3)

6(3)
8(4)
–
–

.30(.01)

–

–

13(5)

–
–

3(0)

–
–

F1

–
–

–
–

29(17)

–
–

–
–

Results We evaluate the predictive performance and interpretability performance by measuring three
metrics: i) the accuracy on the test set (we report F1 score for the mortality data set since it is highly
unbalanced)  ii) the total number of conditions in the output model (for MRS models  we also report
the total number of values)  and iii) the average number of unique features in the model. MRS
achieves consistently competitive predictive accuracy using signiﬁcantly fewer conditions and fewer
features. On data sets credit card and mortality  MRS is the best performing model: highest accuracy 
smallest complexity  and fewest features. On juvenile data set  MRS achieves the highest accuracy
while using the second smallest number of conditions. On readmission data set  MRS loses slightly
in accuracy compared to CBA and SBRL but only uses 6 conditions while CBA used 39 and SBRL
used 21. In summary  MRS models use the fewest conditions on ﬁve out of six data sets. They use
the smallest number of features on all six data sets  even for juvenile data set where MRS has more
conditions than SBRL model but still wins in the number of features.
We show an MRS model learned from data set juvenile to inspect if the grouping of categories
is meaningful. It consists of two rules  and if a teenager satisﬁes either of them  then the model
predicts the teenager will conduct delinquency in the future. In this data set  features are questions
and feature values are choices for the questions.
1: [Have your friends ever hit or threatened to hit someone without any reason? = “All of them” or
“Not sure” or “Refused to Answer”]
2: [Have your friends purposely damaged or destroyed property that did not belong to them? = “All
of them” or “Most of them” or “Some of them”] AND [Did any of your family members use hard
drugs? = “Yes”] AND [“Has any of your family members or friends ever beat you up with their ﬁsts
so hard that you were hurt pretty bad? = “Yes”]
It is interesting to notice that MRS grouped three values in the ﬁrst rule together and the three values
in the ﬁrst condition in the second rule. Grouped values are considered interchangeable by the model.
It is intuitive to explain the grouping with common sense. People avoid answering when they feel
alerted or uncomfortable with the question [4  32]. In this case  this question concerns the privacy of
their friends  making people more reserved and hesitant to provide a deﬁnite answer. So they would
rather say they are not sure or refuse to answer than directly say yes.

1https://github.com/wangtongada/BOA

8

5.3

Interpretability Evaluation by Humans

To further evaluate the model interpretability  in addition to quantitively measuring the size of the
model  we would like to understand how quickly and how correctly humans understand a machine
learning model. We designed a short survey and sent it to a group of 70 undergraduate students.
The survey was designed as an online quiz with credit to motivate students to do it as accurately as
possible. The students have been enrolled in a machine learning class for a couple of weeks and
have some knowledge about predictive models.
We chose to show models built from data set “credit card” since output models are smallest compared
to other data sets  so it’s easier for humans to understand. The students were asked to use the models
to make predictions on given instances. Every method has ﬁve models  each from one of the ﬁve
folds. Therefore  each student was shown with one model for every one of the ﬁve methods. The
survey ﬁrst taught them how to use a model with instructions and an example  and then asked them
to use the model to make predictions on two instances. Their answers and response time were
recorded.
Since all competing methods are rule-based models  it is important that students understand the
notion of rules before working with any of the models. Therefore  we designed a screening question
on rules and students can only proceed with the survey if they answered the question correctly. 66
students passed the test.
We report in Figure 2 the accuracy and response time of each method averaged over ﬁve folds.
Note that response time refers to the total time for understanding the model and using the model to
predict two instances. Accuracy was evaluated against the predictions of a model  not the true labels.
Methods MRS and BRS achieve the highest accuracy  and SBRL achieves the lowest accuracy. We
hypothesize this is because SBRL uses an ordered set of rule connected by “else-if” which makes
it a little more difﬁcult to understand compared to un-ordered rules in the other methods. For the
response time  MRS uses a signiﬁcantly small amount of time  less than half of that of CBA and
Ripper  due to the Bayesian prior to favor small models and a concise presentation allowing multiple
conditions in a rule. BRS also takes a very short time  a bit longer than MRS  followed by SBRL.
MRS  BRS  and SBRL all have a Bayesian component to favor small models while CBA and Ripper
do not  thus taking signiﬁcantly longer to understand and use.

Figure 2: Effect of shape parameters on predictive accuracy and interpretability

6 Conclusions

We proposed a Multi-value Rule Set (MRS) which provides a more concise and feature-efﬁcient
model form to classify and explain. We developed an inference algorithm that incorporates theoret-
ically grounded bounds to reduce computation. Compared with state-of-the-art rule-based models 
MRS showed competitive predictive accuracy while achieving a signiﬁcant reduction in complexity
and feature sets  thus improving the interpretability  demonstrated by human evaluation. A major
contribution is that we demonstrated the possibility of using fewer features without hurting too much
(if any) predictive performance.
Note that we do not claim that multi-value rules are more interpretable than single-value rules since
it is well-known that interpretability comes in different forms for different domains. However  our
model provides a more ﬂexible solution for interpretable models since  after all  a single-value rule
is just a special case of multi-value rules. We believe the potential in the proposed multi-value rules
is not only limited to MRS. They can be adopted in other rule-based models.
Code: The MRS code is available at https://github.com/wangtongada/MRS.

9

References
[1] M. R. Berthold. Mixed fuzzy rule formation. International journal of approximate reasoning 

32(2-3):67–84  2003.

[2] V. Bombardier  C. Mazaud  P. Lhoste  and R. Vogrig. Contribution of fuzzy reasoning method
to knowledge integration in a defect recognition system. Computers in industry  58(4):355–
366  2007.

[3] Z. Chi  H. Yan  and T. Pham. Fuzzy algorithms: with applications to image processing and

pattern recognition  volume 10. World Scientiﬁc  1996.

[4] P. M. Chisnall. Questionnaire design  interviewing and attitude measurement. Journal of the

Market Research Society  35(4):392–393  1993.

[5] W. W. Cohen. Fast effective rule induction. In Proceedings of the twelfth international confer-

ence on machine learning  pages 115–123  1995.

[6] K. A. DeJong and W. M. Spears. Learning concept classiﬁcation rules using genetic algorithms.

Technical report  GEORGE MASON UNIV FAIRFAX VA  1990.

[7] H. Deng  G. Runger  and E. Tuv. Bias of importance measures for multi-valued attributes
and solutions. Artiﬁcial neural networks and machine Learning–ICANN 2011  pages 293–300 
2011.

[8] T. Hamrouni  S. B. Yahia  and E. M. Nguifo. Sweeping the disjunctive search space towards
mining new exact concise representations of frequent itemsets. Data & Knowledge Engineer-
ing  68(10):1091–1111  2009.

[9] T. Hamrouni  S. B. Yahia  and E. M. Nguifo. Generalization of association rules through

disjunction. Annals of Mathematics and Artiﬁcial Intelligence  59(2):201–222  2010.

[10] I. Hilali  T.-Y. Jen  D. Laurent  C. Marinica  and S. B. Yahia. Mining interesting disjunctive
In International Workshop on Information Search 

association rules from unfrequent items.
Integration  and Personalization  pages 84–99. Springer  2013.

[11] K. Hornik  C. Buchta  and A. Zeileis. Open-source machine learning: R meets Weka. Compu-

tational Statistics  24(2):225–232  2009.

[12] H. Ishibuchi and T. Nakashima. Effect of rule weights in fuzzy rule-based classiﬁcation sys-

tems. IEEE Transactions on Fuzzy Systems  9(4):506–515  2001.

[13] S. Kirkpatrick  C. D. Gelatt  M. P. Vecchi  et al. Optimization by simulated annealing. science 

220(4598):671–680  1983.

[14] R. Kohavi. Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree hybrid. In KDD 

volume 96  pages 202–207. Citeseer  1996.

[15] H. Lakkaraju  S. H. Bach  and J. Leskovec. Interpretable decision sets: A joint framework for

description and prediction. In ACM SIGKDD  pages 1675–1684. ACM  2016.

[16] B. Letham  C. Rudin  T. H. McCormick  D. Madigan  et al.

Interpretable classiﬁers using
rules and bayesian analysis: Building a better stroke prediction model. The Ann of Appl Stats 
9(3):1350–1371  2015.

[17] W. Li  J. Han  and J. Pei. Cmar: Accurate and efﬁcient classiﬁcation based on multiple class-

association rules. In ICDM  pages 369–376. IEEE  2001.

[18] B. L. W. H. Y. Ma and B. Liu. Integrating classiﬁcation and association rule mining. In KDD 

1998.

[19] D. Malioutov and K. Varshney. Exact rule learning via boolean compressed sensing. In Inter-

national Conference on Machine Learning  pages 765–773  2013.

10

[20] M. Mampaey  S. Nijssen  A. Feelders  R. Konijn  and A. Knobbe. Efﬁcient algorithms for ﬁnd-
ing optimal binary features in numeric and nominal labeled data. Knowledge and Information
Systems  42(2):465–492  2015.

[21] G. A. Miller. The magical number seven  plus or minus two: some limits on our capacity for

processing information. Psychological review  63(2):81  1956.

[22] A. A. Nanavati  K. P. Chitrapura  S. Joshi  and R. Krishnapuram. Mining generalised disjunc-
tive association rules. In Proceedings of the tenth international conference on Information and
knowledge management  pages 482–489. ACM  2001.

[23] J. D. Osofsky. The effect of exposure to violence on young children. American Psychologist 

50(9):782  1995.

[24] P. R. Rijnbeek and J. A. Kors. Finding a short and accurate decision rule in disjunctive normal

form by exhaustive search. Machine learning  80(1)  2010.

[25] R. Srikant and R. Agrawal. Mining generalized association rules. 1995.

[26] M. Steinbach and V. Kumar. Generalizing the notion of conﬁdence. Knowledge and Informa-

tion Systems  12(3):279–299  2007.

[27] B. Strack  J. P. DeShazo  C. Gennings  J. L. Olmo  S. Ventura  K. J. Cios  and J. N. Clore. Im-
pact of hba1c measurement on hospital readmission rates: analysis of 70 000 clinical database
patient records. BioMed research international  2014  2014.

[28] T. Tran  W. Luo  D. Phung  J. Morris  K. Rickard  and S. Venkatesh. Preterm birth prediction:
Stable selection of interpretable rules from high dimensional data. In Proceedings of the 1st
Machine Learning for Healthcare Conference  volume 56 of Proceedings of Machine Learn-
ing Research  pages 164–177  Northeastern University  Boston  MA  USA  18–19 Aug 2016.
PMLR.

[29] F. Wang and C. Rudin. Falling rule lists. In Artiﬁcial Intelligence and Statistics  pages 1013–

1022  2015.

[30] T. Wang  C. Rudin  F. Doshi  Y. Liu  E. Klampﬂ  and P. MacNeille. A bayesian framework
for learning rule sets for interpretable classiﬁcation. Journal of Machine Learning Research 
2017.

[31] T. Wang  C. Rudin  F. Velez-Doshi  Y. Liu  E. Klampﬂ  and P. MacNeille. Bayesian rule sets

for interpretable classiﬁcation. ICDM  2016.

[32] G. B. Willis. Cognitive interviewing: A tool for improving questionnaire design. Sage Publi-

cations  2004.

[33] H. Yang  C. Rudin  and M. Seltzer. Scalable bayesian rule lists. ICML  2017.

[34] I.-C. Yeh and C.-h. Lien. The comparisons of data mining techniques for the predictive
accuracy of probability of default of credit card clients. Expert Systems with Applications 
36(2):2473–2480  2009.

[35] X. Yin and J. Han. Cpar: Classiﬁcation based on predictive association rules. In SIAM Inter-

national Conference on Data Mining  pages 331–335. SIAM  2003.

11

,Tong Wang
Mickaël Chen
Thierry Artières
Ludovic Denoyer