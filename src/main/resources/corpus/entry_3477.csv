2012,Mixability in Statistical Learning,Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction  the mixability of a loss  has a natural counterpart in the statistical setting  which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction  stochastic mixability characterizes fast rates in statistical learning. We show that  in the special case of log-loss  stochastic mixability reduces to a well-known (but usually unnamed) martingale condition  which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss  it reduces to the margin condition of Mammen and Tsybakov  and in the case that the model under consideration contains all possible predictors  it is equivalent to ordinary mixability.,Mixability in Statistical Learning

Tim van Erven

Universit´e Paris-Sud  France
tim@timvanerven.nl

Mark D. Reid

ANU and NICTA  Australia
Mark.Reid@anu.edu.au

Peter D. Gr¨unwald

CWI and Leiden University  the Netherlands

pdg@cwi.nl

Robert C. Williamson

ANU and NICTA  Australia

Bob.Williamson@anu.edu.au

Abstract

Statistical learning and sequential prediction are two different but related for-
malisms to study the quality of predictions. Mapping out their relations and trans-
ferring ideas is an active area of investigation. We provide another piece of the
puzzle by showing that an important concept in sequential prediction  the mixa-
bility of a loss  has a natural counterpart in the statistical setting  which we call
stochastic mixability. Just as ordinary mixability characterizes fast rates for the
worst-case regret in sequential prediction  stochastic mixability characterizes fast
rates in statistical learning. We show that  in the special case of log-loss  stochastic
mixability reduces to a well-known (but usually unnamed) martingale condition 
which is used in existing convergence theorems for minimum description length
and Bayesian inference. In the case of 0/1-loss  it reduces to the margin condition
of Mammen and Tsybakov  and in the case that the model under consideration
contains all possible predictors  it is equivalent to ordinary mixability.

1

Introduction

learning (also called batch learning)

In statistical
[1] one obtains a random sample
(X1  Y1)  . . .   (Xn  Yn) of independent pairs of observations  which are all distributed according
to the same distribution P ⇤. The goal is to select a function ˆf that maps X to a prediction ˆf (X) of
Y for a new pair (X  Y ) from the same P ⇤. The quality of ˆf is measured by its excess risk  which
is the expectation of its loss `(Y  ˆf (X)) minus the expected loss of the best prediction function f⇤
in a given class of functions F. Analysis in this setting usually involves giving guarantees about the
performance of ˆf in the worst case over the choice of the distribution of the data.
In contrast  the setting of sequential prediction (also called online learning) [2] makes no proba-
bilistic assumptions about the source of the data. Instead  pairs of observations (xt  yt) are assumed
to become available one at a time  in rounds t = 1  . . .   n  and the goal is to select a function ˆft
just before round t  which maps xt to a prediction of yt. The quality of predictions ˆf1  . . .   ˆfn is
evaluated by their regret  which is the sum of their losses `(y1  ˆf1(x1))  . . .  ` (yn  ˆfn(xn)) on the
actual observations minus the total loss of the best ﬁxed prediction function f⇤ in a class of functions
F. In sequential prediction the usual analysis involves giving guarantees about the performance of
ˆf1  . . .   ˆfn in the worst case over all possible realisations of the data. When stating rates of conver-
gence  we will divide the worst-case regret by n  which makes the rates comparable to rates in the
statistical learning setting.
Mapping out the relations between statistical learning and sequential prediction is an active area of
investigation  and several connections are known. For example  using any of a variety of online-

1

to-batch conversion techniques [3]  any sequential predictions ˆf1  . . .   ˆfn may be converted into a
single statistical prediction ˆf and the statistical performance of ˆf is bounded by the sequential pre-
diction performance of ˆf1  . . .   ˆfn. Moreover  a deep understanding of the relation between worst-
case rates in both settings is provided by Abernethy  Agarwal  Bartlett and Rakhlin [4]. Amongst
others  their results imply that for many loss functions the worst-case rate in sequential prediction
exceeds the worst-case rate in statistical learning.

Fast Rates
In sequential prediction with a ﬁnite class F  it is known that the worst-case regret can
be bounded by a constant if and only if the loss ` has the property of being mixable [5  6] (subject
to mild regularity conditions on the loss). Dividing by n  this corresponds to O(1/n) rates  which is
fast compared to the usual O(1/pn) rates.
In statistical learning  there are two kinds of conditions that are associated with fast rates. First 
for 0/1-loss  fast rates (faster than O(1/pn)) are associated with Mammen and Tsybakov’s margin
condition [7  8]  which depends on a parameter .
In the nicest case   = 1 and then O(1/n)
rates are possible. Second  for log(arithmic) loss there is a single supermartingale condition that
is essential to obtain fast rates in all convergence proofs of two-part minimum description length
(MDL) estimators  and in many convergence proofs of Bayesian estimators. This condition  used
by e.g. [9  10  11  12  13  14]  sometimes remains implicit (see Example 1 below) and usually goes
unnamed. A special case has been called the ‘supermartingale property’ by Chernov  Kalnishkan 
Zhdanov and Vovk [15]. Audibert [16] also introduced a closely related condition  which does seem
subtly different however.

Our Contribution We deﬁne the notion of stochastic mixability of a loss `  set of predictors F 
and distribution P ⇤  which we argue to be the natural analogue of mixability for the statistical setting
on two grounds: ﬁrst  we show that it is closely related to both the supermartingale condition and the
margin condition  the two properties that are known to be related to fast rates; second  we show that
it shares various essential properties with ordinary mixability and in speciﬁc cases is even equivalent
to ordinary mixability.
To support the ﬁrst part of our argument  we show the following: (a) for bounded losses (includ-
ing 0/1-loss)  stochastic mixability is equivalent to the best case ( = 1) of a generalization of
the margin condition; other values of  may be interpreted in terms of a slightly relaxed version of
stochastic mixability; (b) for log-loss  stochastic mixability reduces to the supermartingale condi-
tion; (c) in general  stochastic mixability allows uniform O(log |Fn|/n)-statistical learning rates to
be achieved  where |Fn| is the size of a sub-model Fn ⇢ F considered at sample size n. Finally  (d)
if stochastic mixability does not hold  then in general O(log |Fn|/n)-statistical learning rates cannot
be achieved  at least not for 0/1-loss or for log-loss.
To support the second part of our argument  we show: (e) if the set F is ‘full’  i.e. it contains all
prediction functions for the given loss  then stochastic mixability turns out to be formally equivalent
to ordinary mixability (if F is not full  then either condition may hold without the other). We choose
to call our property stochastic mixability rather than  say  ‘generalized margin condition for  = 1’
or ‘generalized supermartingale condition’  because (f) we also show that the general condition can
be formulated in an alternative way (Theorem 2) that directly indicates a strong relation to ordinary
mixability  and (g) just like ordinary mixability  it can be interpreted as the requirement that a set of
so-called pseudo-likelihoods is (effectively) convex.
We note that special cases of results (a)–(e) already follow from existing work of many other authors;
we provide a detailed comparison in Section 7. Our contributions are to generalize these results  and
to relate them to each other  to the notion of mixability from sequential prediction  and to the inter-
pretation in terms of convexity of a set of pseudo-likelihoods. This leads to our central conclusion:
the concept of stochastic mixability is closely related to mixability and plays a fundamental role in
achieving fast rates in the statistical learning setting.

Outline
In §2 we deﬁne both ordinary mixability and stochastic mixability. We show that two of
the standard ways to express mixability have natural analogues that express stochastic mixability
(leading to (f)). In example 1 we specialize the deﬁnition to log-loss and explain its importance in
the literature on MDL and Bayesian inference  leading to (b). A third interpretation of mixability
and standard mixability in terms of sets (g) is described in §3. The equivalence between mixability

2

and stochastic mixability if F is full is presented in §4 where we also show that the equivalence need
not hold if F is not full (e). In §5  we turn our attention to a version of the margin condition that
does not assume that F contains the Bayes optimal predictor and we show that (a slightly relaxed
version of) stochastic mixability is equivalent to the margin condition  taking care of (a). We show
(§6) that if stochastic mixability holds  O(log |Fn|/n)-rates can always be achieved (c)  and that in
some cases in which it does not hold  O(log |Fn|/n)-rates cannot be achieved (d). Finally (§7) we
connect our results to previous work in the literature. Proofs omitted from the main body of the
paper are in the supplementary material.

2 Mixability and Stochastic Mixability

We now introduce the notions of mixability and stochastic mixability  showing two equivalent for-
mulations of the latter.

2.1 Mixability
A loss function ` : Y⇥A ! [0 1] is a nonnegative function that measures the quality of a prediction
a 2 A when the true outcome is y 2 Y by `(y  a). We will assume that all spaces come equipped
with appropriate -algebras  so we may deﬁne distributions on them  and that the loss function ` is
measurable.
Deﬁnition 1 (Mixability). For ⌘> 0  a loss ` is called ⌘-mixable if for any distribution ⇡ on A there
exists a single prediction a⇡ such that

(1)

(2)

`(y  a⇡)  

1
⌘

lnZ e⌘`(y a) ⇡(da)

for all y.

It is called mixable if there exists an ⌘> 0 such that it is ⌘-mixable.

Let A be a random variable with distribution ⇡. Then (1) may be rewritten as

E⇡ e⌘`(y A)

e⌘`(y a⇡)  1

for all y.

2.2 Stochastic Mixability
Let F be a set of predictors f : X ! A  which are measurable functions that map any input x 2 X
to a prediction f (x). For example  if A = Y = {0  1} and the loss is the 0/1-loss  `0/1(y  a) =
1{y 6= a}  then the predictors are classiﬁers. Let P ⇤ be the distribution of a pair of random variables
(X  Y ) with values in X ⇥ Y. Most expectations in the paper are with respect to P ⇤. Whenever this
is not the case we will add a subscript to the expectation operator  as in (2).
Deﬁnition 2 (Stochastic Mixability). For any ⌘  0  we say that (`  F  P ⇤) is ⌘-stochastically
mixable if there exists an f⇤ 2 F such that
E e⌘`(Y f (X))
e⌘`(Y f⇤(X))  1

for all f 2 F.

We call (`  F  P ⇤) stochastically mixable if there exists an ⌘> 0 such that it is ⌘-stochastically
mixable.

(3)

By Jensen’s inequality  we see that (3) implies 1  Eh e⌘`(Y f (X))

e⌘`(Y f⇤(X))i  eE[⌘(`(Y f⇤(X))`(Y f (X)))] 

so that

E[`(Y  f⇤(X))]  E[`(Y  f (X)))]

for all f 2 F 

and hence the deﬁnition of stochastic mixability presumes that f⇤ minimizes E[`(Y  f (X))] over all
f 2 F. We will assume throughout the paper that such an f⇤ exists  and that E[`(Y  f⇤(X))] < 1.
The larger ⌘  the stronger the requirement of ⌘-stochastic mixability:
Proposition 1. Any triple (`  F  P ⇤) is 0-stochastically mixable. And if 0 <<⌘   then ⌘-stochastic
mixability implies -stochastic mixability.

3

Example 1 (Log-loss). Let F be a set of conditional probability densities and let `log be log-loss 
i.e. A is the set of densities on Y  f (x)(y) is written  as usual  as f (y | x)  and `log(y  f (x)) :=
 ln f (y | x). For log-loss  statistical learning becomes equivalent to conditional density estimation
with random design (see  e.g.  [14]). Equation 3 now becomes equivalent to

A⌘(f⇤kf ) := E✓ f (Y | X)
f⇤(Y | X)◆⌘

 1.

(4)

A⌘ has been called the generalized Hellinger afﬁnity [12] in the literature. If the model is correct 
i.e. it contains the true conditional density p⇤(y | x)  then  because the log-loss is a proper loss [17]
we must have f⇤ = p⇤ and then  for ⌘ = 1  trivially A⌘(fkf⇤) = 1 for all f 2 F. Thus if the model
F is correct  then the log-loss is ⌘-stochastically mixable for ⌘ = 1. In that case  for ⌘ = 1/2  A⌘
turns into the standard deﬁnition of Hellinger afﬁnity [10].
Equation 4 — which just expresses 1-stochastic mixability for log-loss — is used in all previous
convergence theorems for 2-part MDL density estimation [10  12  11  18]  and  more implicitly  in
various convergence theorems for Bayesian procedures  including the pioneering paper by Doob
[9]. All these results assume that the model F is correct  but  if one studies the proofs  one ﬁnds that
the assumption is only needed to establish that (4) holds for ⌘ = 1. For example  as ﬁrst noted by
[12]  if F is a convex set of densities  then (4) also holds for ⌘ = 1  even if the model is incorrect 
and  indeed  two-part MDL converges at fast rates in such cases (see [14] for a precise deﬁnition of
what this means  as well as more general treatment of (4)). Kleijn and Van der Vaart [13]  in their
extensive analysis of Bayesian nonparametric inference if the model is wrong  also use the fact that
(4) holds with ⌘ = 1 for convex models to show that fast posterior concentration rates hold for such
models even if they do not contain the true p⇤.

The deﬁnition of stochastic mixability looks similar to (2)  but whereas ⇡ is a distribution on pre-
dictions  P ⇤ is a distribution on outcomes (X  Y ). Thus at ﬁrst sight the resemblance appears to be
only superﬁcial. It is therefore quite surprising that stochastic mixability can also be expressed in a
way that looks like (1)  which provides a ﬁrst hint that the relation goes deeper.
Theorem 2. Let ⌘> 0. Then (`  F  P ⇤) is ⌘-stochastically mixable if and only if for any distribution
⇡ on F there exists a single predictor f⇤ 2 F such that

E⇥`(Y  f⇤(X))⇤  E

1
⌘

lnZ e⌘`(Y f (X)) ⇡(df ) .

(5)

Notice that  without
E[`(Y  f (X))]. Then f⇤ does not depend on ⇡.

loss of generality  we can always choose f⇤ to be the minimizer of

3 The Convexity Interpretation

There is a third way to express mixability  as the convexity of a set of so-called pseudo-likelihoods.
We will now show that stochastic mixability can also be interpreted as convexity of the correspond-
ing set in the statistical learning setting.
Following Chernov et al. [15]  we ﬁrst note that the essential feature of a loss ` with corresponding
set of predictions A is the set of achievable losses they induce:

L = {l : Y ! [0 1] |9 a 2 A : l(y) = `(y  a) for all y 2 Y}.

If we would reparametrize the loss by a different set of predictions A0  while keeping L the same 
then essentially nothing would change. For example  for 0/1-loss standard ways to parametrize
predictions are by A = {0  1}  by A = {1  +1} or by A = R with the interpretation that predicting
a  0 maps to the prediction 1 and a < 0 maps to the prediction 0. Of course these are all equivalent 
because L is the same.
It will be convenient to consider the set of functions that lie above the achievable losses in L:

S = S` = {l : Y ! [0 1] |9 l0 2 L : l(y)  l0(y) for all y 2 Y} 

Chernov et al. call this the super prediction set. It plays a role similar to the role of the epigraph
of a function in convex analysis. Let ⌘> 0. Then with each element l 2 S in the super prediction

4

P ⇤

coPF(⌘)

f⇤

PF(⌘)

P ⇤

f⇤

PF(⌘)

coPF(⌘)

Not stochastically mixable

Stochastically mixable

Figure 1: The relation between convexity and stochastic mixability for log-loss  ⌘ = 1 and X = {x}
a singleton  in which case P ⇤ and the elements of PF(⌘) can all be interpreted as distributions on Y.

set  we associate a pseudo-likelihood p(y) = e⌘l(y). Note that 0  p(y)  1  but it is generally
not the case thatR p(y) µ(dy) = 1 for some reference measure µ on Y  so p(y) is not normalized.
Let e⌘S = {e⌘l | l 2 S} denote the set of all such pseudo-likelihoods. By multiplying (1) by ⌘
and exponentiating  it can be shown that ⌘-mixability is exactly equivalent to the requirement that
e⌘S is convex [2  15]. And like for the ﬁrst two expressions of mixability  there is an analogous
convexity interpretation for stochastic mixability.
In order to deﬁne pseudo-likelihoods in the statistical setting  we need to take into account that the
predictions f (X) of the predictors in F are not deterministic  but depend on X. Hence we deﬁne
conditional pseudo-likelihoods p(Y |X) = e⌘`(Y f (X)). (See also Example 1.) There is no need to
introduce a conditional analogue of the super prediction set. Instead  let PF(⌘) = {e⌘`(Y f (X)) |
f 2 F} denote the set of all conditional pseudo-likelihoods. For  2 [0  1]  a convex combination
of any two p0  p1 2 PF(⌘) can be deﬁned as p(Y |X) = (1  )p0(Y |X) + p1(Y |X). And
consequently  we may speak of the convex hull co PF(⌘) = {p | p0  p1 2 PF(⌘)  2 [0  1]} of
PF(⌘).
Corollary 3. Let ⌘> 0. Then ⌘-stochastic mixability of (`  F  P ⇤) is equivalent to the requirement
that

min

p2PF(⌘)

E⇥1
⌘ ln p(Y |X)⇤ = min

p2co PF(⌘)

E⇥1
⌘ ln p(Y |X)⇤.

Proof. This follows directly from Theorem 2 after rewriting it in terms of conditional pseudo-
likelihoods.

(6)

Notice that the left-hand side of (6) equals E[`(Y  f⇤(X))]  which does not depend on ⌘.
Equation 6 expresses that the convex hull operator has no effect  which means that PF(⌘) looks
convex from the perspective of P ⇤. See Figure 1 for an illustration for log-loss. Thus we obtain
an interpretation of ⌘-stochastic mixability as effective convexity of the set of pseudo-likelihoods
PF(⌘) with respect to P ⇤.
Figure 1 suggests that f⇤ should be unique if the loss is stochastically mixable  which is almost
right. It is in fact the loss `(Y  f⇤(X)) of f⇤ that is unique (almost surely):
Corollary 4.
If (`  F  P ⇤) is stochastically mixable and there exist f⇤  g⇤ 2 F such that
E[`(Y  f⇤(X))] = E[`(Y  g⇤(X))] = minf2F E[`(Y  f (X))]  then `(Y  f⇤(X)) = `(Y  g⇤(X))
almost surely.

Proof. Let ⇡(f⇤) = ⇡(g⇤) = 1/2. Then  by Theorem 2 and (strict) convexity of  ln 

min
f2F

E[`(Y  f (X))]  E
 E 1

2

1
⌘

ln 1

2

`(Y  f⇤(X)) +

e⌘`(Y f⇤(X)) +

1
2

e⌘`(Y g⇤(X))
`(Y  g⇤(X)) = min

f2F

1
2

E[`(Y  f (X))].

5

Hence both inequalities must hold with equality. For the second inequality this is only the case if
`(Y  f⇤(X)) = `(Y  g⇤(X)) almost surely  which was to be shown.

4 When Mixability and Stochastic Mixability Are the Same

Having observed that mixability and stochastic mixability of a loss share several common features 
we now show that in speciﬁc cases the two concepts even coincide. More speciﬁcally  Theorem 5
below shows that a loss ` (meeting two requirements) is ⌘-mixable if and only if it is ⌘-stochastically
mixable relative to Ffull  the set of all functions from X to A  and all distributions P ⇤. To avoid
measurability issues  we will assume that X is countable throughout this section.
The two conditions we assume of ` are both related to its set of pseudo-likelihoods := e⌘S 
which was deﬁned in Section 3. The ﬁrst condition is that  is closed. When Y is inﬁnite  we mean
closed relative to the topology for the supremum norm kpk1 = supy2Y |p(y)|. The second  more
technical condition is that  is pre-supportable. That is  for every pseudo-likelihood p 2   its
pre-image s 2 S (deﬁned for each y 2 Y by s(y) :=  1
⌘ ln p(y)) is supportable. Here  a point s 2 S
is supportable if it is optimal for some distribution P ⇤Y over Y – that is  if there exists a distribution
P ⇤Y over Y such that EP ⇤Y [s(Y )]  EP ⇤Y [t(Y )] for all t 2 S. This is the case  for example  for all
proper losses [17].
We say (`  F) is ⌘-stochastically mixable if (`  F  P ⇤) is ⌘-stochastically mixable for all distributions
P ⇤ on X ⇥ Y.
Theorem 5. Suppose X is countable. Let ⌘> 0 and suppose ` is a loss such that its pseudo-
likelihood set e⌘S is closed and pre-supportable. Then (`  Ffull) is ⌘-stochastically mixable if and
only if ` is ⌘-mixable.

This result generalizes Theorem 9 and Lemma 11 by Chernov et al. [15] from ﬁnite Y to arbitrary
continuous Y  which they raised as an open question.
In their setting  there are no explanatory
variables x  which may be emulated in our framework by letting X contain only a single element.
Their conditions also imply (by their Lemma 10) that the loss ` is proper  which implies that e⌘S is
closed and pre-supportable. We note that for proper losses ⌘-mixability is especially well understood
[19].
The proof of Theorem 5 is broken into two lemmas (the proofs of which are in the supplemen-
tary material). The ﬁrst establishes conditions for when mixability implies stochastic mixability 
borrowing from a similar result for log-loss by Li [12].
Lemma 6. Let ⌘> 0. Suppose the Bayes optimal predictor f⇤B(x) 2 arg mina2A E[`(Y  a)|X = x]
is in the model: f⇤B = f⇤ 2 F. If ` is ⌘-mixable  then (`  F  P ⇤) is ⌘-stochastically mixable.
The second lemma shows that stochastic mixability implies mixability.
Lemma 7. Suppose the conditions of Theorem 5 are satisﬁed. If (`  Ffull) is ⌘-stochastically mixable 
then it is ⌘-mixable.

The above two lemmata are sufﬁcient to prove the equivalence of stochastic and ordinary mixability.

Proof of Theorem 5. In order to show that ⌘-mixability of ` implies ⌘-stochastic mixability of
(`  Ffull) we note that the Bayes-optimal predictor f⇤B for any ` and P ⇤ must be in Ffull and so
Lemma 6 implies (`  Ffull  P ⇤) is ⌘-stochastically mixable for any distribution P ⇤. Conversely 
that ⌘-stochastic mixability of (`  Ffull) implies the ⌘-mixability of ` follows immediately from
Lemma 7.
Example 2 (if F is not full). In this case  we can have either stochastic mixability without ordinary
mixability or the converse. Consider a loss function ` that is not mixable in the ordinary sense 
e.g. ` = `0/1  the 0/1-loss [6]  and a set F consisting of just a single predictor. Then clearly ` is
stochastically mixable relative to F. This is  of course  a trivial case. We do not know whether we
can have stochastic mixability without ordinary mixability in nontrivial cases  and plan to investigate
this for future work. For the converse  we know that it does hold in nontrivial cases: consider
the log-loss `log which is 1-mixable in the standard sense (Example 1). Let Y = {0  1} and let
the model F be a set of conditional probability mass functions {f✓ | ✓ 2 ⇥} where ⇥ is the

6

set of all classiﬁers  i.e. all functions X ! Y  and f✓(y | x) := e`0/1(y ✓(x))/(1 + e1) where
`0/1(y  ˆy) = 1{y 6= ˆy} is the 0/1-loss. Then log-loss becomes an afﬁne function of 0/1-loss: for
each ✓ 2 ⇥  `log(Y  f✓(X)) = `0/1(Y  ✓(X)) + C with C = ln(1 + e1) [14]. Because 0/1-loss is
not standard mixable  by Theorem 5  0/1-loss is not stochastically mixable relative to ⇥. But then
we must also have that log-loss is not stochastically mixable relative to F.

5 Stochastic Mixability and the Margin Condition

The excess risk of any f compared to f⇤ is the mean of the excess loss `(Y  f (X))  `(Y  f⇤(X)):

We also deﬁne the expected square of the excess loss  which is closely related to its variance:

d(f  f⇤) = E⇥`(Y  f (X))  `(Y  f⇤(X))⇤.
V (f  f⇤) = E⇣`(Y  f (X))  `(Y  f⇤(X))⌘2

.

Note that  for 0/1-loss  V (f  f⇤) = P ⇤(f (X) 6= f⇤(X)) is the probability that f and f⇤ disagree.
The margin condition  introduced by Mammen and Tsybakov [7  8] for 0/1-loss  is satisﬁed with
constants   1 and c0 > 0 if

for all f 2 F.

c0V (f  f⇤)  d(f  f⇤)

(7)
Unlike Mammen and Tsybakov  we do not assume that F necessarily contains the Bayes predictor 
which minimizes the risk over all possible predictors. The same generalization has been used in the
context of model selection by Arlot and Bartlett [20].
Remark 1. In some practical cases  the margin condition only holds for a subset of the model such
that V (f  f⇤)  ✏0 for some ✏0 > 0 [8]. In such cases  the discussion below applies to the same
subset.
Stochastic mixability  as we have deﬁned it  is directly related to the margin condition for the case
 = 1. In order to relate it to other values of   we need a little more ﬂexibility: for given ✏  0 and
(`  F  P ⇤)  we deﬁne
(8)
which excludes a band of predictors that approximate the best predictor in the model to within excess
risk ✏.
Theorem 8. Suppose a loss ` takes values in [0  V ] for 0 < V < 1. Fix a model F and distribution
P ⇤. Then the margin condition (7) is satisﬁed if and only if there exists a constant C > 0 such that 
for all ✏> 0  (`  F✏  P ⇤) is ⌘-stochastically mixable for ⌘ = C✏(1)/. In particular  if the margin
condition is satisﬁed with constants  and c0  we can take C = min V 2c1/

This theorem gives a new interpretation of the margin condition as the rate at which ⌘ has to go
to 0 when the model F is approximated by ⌘-stochastically mixable models F✏. By the following
corollary  proved in the additional material  stochastic mixability of the whole model F is equivalent
to the best case of the margin condition.
Corollary 9. Suppose ` takes values in [0  V ] for 0 < V < 1. Then (`  F  P ⇤) is stochastically
mixable if and only if there exists a constant c0 > 0 such that the margin condition (7) is satisﬁed
with  = 1.

F✏ = {f⇤}[{ f 2 F | d(f  f⇤)  ✏} 

1

V (1)/ .

0

eV V 1  

6 Connection to Uniform O(log |Fn|/n) Rates
Let ` be a bounded loss function. Assume that  at sample size n  an estimator ˆf (statistical learning
algorithm) is used based on a ﬁnite model Fn  where we allow the size |Fn| to grow with n. Let 
for all n  Pn be any set of distributions on X ⇥ Y such that for all P ⇤ 2 Pn  the generalized
margin condition (7) holds for  = 1 and uniform constant c0 not depending on n  with model
Fn. In the case of 0/1-loss  the results of e.g. Tsybakov [8] suggest that there exist estimators

7

ˆfn : (X ⇥ Y)n ! Fn that achieve a convergence rate of O(log |Fn|/n)  uniformly for all P ⇤ 2 P;
that is 
(9)

EP ⇤[d( ˆfn  f⇤)] = O(log |Fn|/n).

sup
P ⇤2Pn

This can indeed be proven  for general loss functions  using Theorem 4.2. of Zhang [21] and with ˆfn
set to Zhang’s information-risk-minimization estimator (to see this  at sample size n apply Zhang’s
result with ↵ set to 0 and a prior ⇡ that is uniform on Fn  so that  log ⇡(f ) = log |Fn| for any
f 2 Fn). By Theorem 8  this means that  for any bounded loss function `  if  for some ⌘> 0  all n 
we have that (`  Fn  P ⇤) is ⌘-stochastically mixable for all P ⇤ 2 Pn  then Zhang’s estimator satisﬁes
(9). Hence  for bounded loss functions  stochastic mixability implies a uniform O(log |Fn|/n) rate.
A connection between stochastic mixability and fast rates is also made by Gr¨unwald [14]  who
introduces some slack in the deﬁnition (allowing the number 1 in (3) to be slightly larger) and uses
the convexity interpretation from Section 3 to empirically determine the largest possible value for
⌘. His Theorem 2  applied with a slack set to 0  implies an in-probability version of Zhang’s result
above.
Example 3. We just explained that  if ` is stochastically mixable relative to Fn  then uniform
O(log |Fn|/n) rates can be achieved. We now illustrate that if this is not the case  then  at least
if ` is 0/1-loss or if ` is log-loss  uniform O(log |Fn|/n) rates cannot be achieved in general. To see
this  let ⇥n be a ﬁnite set of classiﬁers ✓ : X ! Y  Y = {0  1} and let ` be 0/1-loss. Let for each
n  ˆfn : (X ⇥ Y)n ! Fn be some arbitrary estimator. It is known from e.g. the work of Vapnik [22]
that for every sequence of estimators ˆf1  ˆf2  . . .  there exist a sequence ⇥1  ⇥2  . . .  all ﬁnite  and a
sequence P ⇤1   P ⇤2   . . . such that

EP ⇤n [d( ˆfn  f⇤)]
log |⇥n|/n ! 1.

Clearly then  by Zhang’s result above  there cannot be an ⌘ such that for all n  (`  ⇥n  P ⇤n) is ⌘-
stochastically mixable. This establishes that if stochastic mixability does not hold  then uniform rates
of O(log |Fn|/n) are not achievable in general for 0/1-loss. By the construction of Example 2  we
can modify ⇥n into a set of corresponding log-loss predictors Fn so that the log-loss `log becomes an
afﬁne function of the 0/1-loss; thus  there still is no ⌘ such that for all n  (`log  Fn  P ⇤n) is ⌘-mixable 
and the sequence of estimators still cannot achieve uniform a O(log |Fn|/n) rate with log-loss either.
7 Discussion — Related Work

Let us now return to the summary of our contributions which we provided as items (a)—(g) in §1.
We note that slight variations of our formula (3) for stochastic mixability already appear in [14]
(but there no connections to ordinary mixability are made) and [15] (but there it is just a tool for
the worst-case sequential setting  and no connections to fast rates in statistical learning are made).
Equation 3 looks completely different from the margin condition  yet results connecting the two 
somewhat similar to (a)  albeit very implicitly  already appear in [23] and [24]. Also  the paper by
Gr¨unwald [14] contains a connection between the margin condition somewhat similar to Theorem 8 
but involving a signiﬁcantly weaker version of stochastic mixability in which the inequality (3) only
holds with some slack. Result (b) is trivial given Deﬁnition 2; (c) is a consequence of Theorem 4.2.
of [21] when combined with (a) (see Section 6). Result (d) (Theorem 5) is a signiﬁcant extension of
a similar result by Chernov et al. [15]. Yet  our proof techniques and interpretation are completely
different from those in [15]. Result (e)  Example 3  is a direct consequence of (a). Result (f)
(Theorem 2) is completely new  but the proof is partly based on ideas which already appear in [12]
in a log-loss/MDL context  and (g) is a consequence of (f). Finally  Corollary 3 can be seen as
analogous to the results of Lee et al. [25]  who showed the role of convexity of F for fast rates in the
regression setting with squared loss.

Acknowledgments
This work was supported by the ARC and by NICTA  funded by the Australian Government. It
was also supported in part by the IST Programme of the European Community  under the PASCAL
Network of Excellence  IST-2002-506778  and by NWO Rubicon grant 680-50-1112.

8

References
[1] O. Bousquet  S. Boucheron  and G. Lugosi.

In
O. Bousquet  U. von Luxburg  and G. R¨atsch  editors  Advanced Lectures on Machine Learn-
ing  volume 3176 of Lecture Notes in Computer Science  pages 169–207. Springer Berlin /
Heidelberg  2004.

Introduction to statistical learning theory.

[2] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[3] O. Dekel and Y. Singer. Data-driven online to batch conversions. In Y. Weiss  B. Sch¨olkopf 
and J. Platt  editors  Advances in Neural Information Processing Systems 18 (NIPS)  pages
267–274  Cambridge  MA  2006. MIT Press.

[4] J. Abernethy  A. Agarwal  P. L. Bartlett  and A. Rakhlin. A stochastic view of optimal regret
through minimax duality. In Proceedings of the 22nd Conference on Learning Theory (COLT) 
2009.

[5] Y. Kalnishkan and M. V. Vyugin. The weak aggregating algorithm and weak mixability. Jour-

nal of Computer and System Sciences  74:1228–1244  2008.

[6] V. Vovk. A game of prediction with expert advice. In Proceedings of the 8th Conference on

Learning Theory (COLT)  pages 51–60. ACM  1995.

[7] E. Mammen and A. B. Tsybakov. Smooth discrimination analysis. The Annals of Statistics 

[8] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statis-

27(6):1808–1829  1999.

tics  32(1):135–166  2004.

[9] J. L. Doob. Application of the theory of martingales.

In Le Calcul de Probabilit´es et ses
Applications. Colloques Internationaux du Centre National de la Recherche Scientiﬁque  pages
23–27  Paris  1949.

[10] A. Barron and T. Cover. Minimum complexity density estimation.

IEEE Transactions on

Information Theory  37(4):1034–1054  1991.

[11] T. Zhang. From ✏-entropy to KL entropy: analysis of minimum information complexity density

estimation. Annals of Statistics  34(5):2180–2210  2006.

[12] J. Li. Estimation of Mixture Models. PhD thesis  Yale University  1999.
[13] B. Kleijn and A. van der Vaart. Misspeciﬁcation in inﬁnite-dimensional Bayesian statistics.

Annals of Statistics  34(2)  2006.

[14] P. Gr¨unwald. Safe learning: bridging the gap between Bayes  MDL and statistical learning
theory via empirical convexity. In Proceedings of the 24th Conference on Learning Theory
(COLT)  2011.

[15] A. Chernov  Y. Kalnishkan  F. Zhdanov  and V. Vovk. Supermartingales in prediction with

expert advice. Theoretical Computer Science  411:2647–2669  2010.

[16] J.-Y. Audibert. Fast learning rates in statistical inference through aggregation. Annals of

Statistics  37(4):1591–1646  2009.

[17] E. Vernet  R. C. Williamson  and M. D. Reid. Composite multiclass losses. In Advances in

Neural Information Processing Systems 24 (NIPS)  2011.

[18] P. Gr¨unwald. The Minimum Description Length Principle. MIT Press  Cambridge  MA  2007.
[19] T. van Erven  M. Reid  and R. Williamson. Mixability is Bayes risk curvature relative to log

loss. In Proceedings of the 24th Conference on Learning Theory (COLT)  2011.

[20] S. Arlot and P. L. Bartlett. Margin-adaptive model selection in statistical learning. Bernoulli 

17(2):687–713  2011.

[21] T. Zhang.

Information theoretical upper and lower bounds for statistical estimation.

IEEE

Transactions on Information Theory  52(4):1307–1321  2006.

[22] V. Vapnik. Statistical Learning Theory. Wiley  New York  1998.
[23] J.-Y. Audibert. PAC-Bayesian statistical learning theory. PhD thesis  Universit´e Paris VI 

[24] O. Catoni. PAC-Bayesian Supervised Classiﬁcation. Lecture Notes-Monograph Series. IMS 

2004.

2007.

[25] W. Lee  P. Bartlett  and R. Williamson. The importance of convexity in learning with squared
loss. IEEE Transactions on Information Theory  44(5):1974–1980  1998. Correction  Volume
54(9)  4395 (2008).

[26] A. N. Shiryaev. Probability. Springer-Verlag  1996.
[27] J.-Y. Audibert. A better variance control for PAC-Bayesian classiﬁcation. Preprint 905  Labo-

ratoire de Probabilit´es et Mod`eles Al´eatoires  Universit´es Paris 6 and Paris 7  2004.

9

,Nan Li
Rong Jin
Zhi-Hua Zhou
Brenda Betancourt
Giacomo Zanella
Jeffrey Miller
Hanna Wallach
Abbas Zaidi
Rebecca Steorts