2019,Learning from Label Proportions with Generative Adversarial Networks,In this paper  we leverage generative adversarial networks (GANs) to derive an effective algorithm LLP-GAN for learning from label proportions (LLP)  where only the bag-level proportional information in labels is available. Endowed with end-to-end structure  LLP-GAN performs approximation in the light of an adversarial learning mechanism  without imposing restricted assumptions on distribution. Accordingly  we can directly induce the final instance-level classifier upon the discriminator. Under mild assumptions  we give the explicit generative representation and prove the global optimality for LLP-GAN. Additionally  compared with existing methods  our work empowers LLP solver with capable scalability inheriting from deep models. Several experiments on benchmark datasets demonstrate vivid advantages of the proposed approach.,Learning from Label Proportions with

Generative Adversarial Networks

Samsung Research China - Beijing

University of International Business and Economics

Bo Wang∗

Beijing 100029  China
wangbo@uibe.edu.cn

Jiabin Liu∗

Beijing 100028  China

liujiabin008@126.com

Zhiquan Qi†

Yingjie Tian

Yong Shi

University of Chinese Academy of Sciences

Beijing 100190  China

qizhiquan@foxmail.com  {tyj yshi}@ucas.ac.cn

Abstract

In this paper  we leverage generative adversarial networks (GANs) to derive an
effective algorithm LLP-GAN for learning from label proportions (LLP)  where
only the bag-level proportional information in labels is available. Endowed with
end-to-end structure  LLP-GAN performs approximation in the light of an adver-
sarial learning mechanism  without imposing restricted assumptions on distribution.
Accordingly  we can directly induce the ﬁnal instance-level classiﬁer upon the dis-
criminator. Under mild assumptions  we give the explicit generative representation
and prove the global optimality for LLP-GAN. Additionally  compared with exist-
ing methods  our work empowers LLP solver with capable scalability inheriting
from deep models. Several experiments on benchmark datasets demonstrate vivid
advantages of the proposed approach.

1

Introduction

Deep learning beneﬁts from end-to-end design philosophy  which emphasizes minimal a priori
representational and computational assumption  and greatly avoids explicit structure and “hand-
engineering” [4]. Doubtless  most of its achievements are afﬁrmed by the access to the abundance of
fully supervised data [14  13  26]. One reason is that a large amount of complete data can alleviate
the over-ﬁtting problem without deteriorating the hypothesis set complexity (e.g.  a sophisticated
network design with vast parameters).
Unfortunately  fully labeled data is not always handy to utilize. Firstly  it is infeasible or labor-
intensive to obtain abundant accurate labeled data [31]. Secondly  labels are not accessible under
certain circumstances  such as privacy constraints [22]. Therefore  the community begins to pay
attention to weakly supervised learning (a.k.a.  weakly labeled learning  WeLL)  concerning any
corrosion in supervised information. For example  semi-supervised learning (SSL) is a WeLL
problem by concealing most of the labels in the training stage. Another widespread WeLL problem is
multi-instance learning (MIL) [19]  which is also a representative in learning with bags.
Furthermore  generative adversarial networks (GANs) [11]  which is originally proposed to synthesize
high ﬁdelity data in line with the estimation of underlying data distribution  can be potentially applied
to WeLL. For instance  GANs can deal with K-class SSL [28]  by treating the generated data as

∗Joint ﬁrst authors with equal contribution
†Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: An illustration of multi-class learning from label proportions. In detail  the data belongs to
three categories and is partitioned into four non-overlapping groups. In each group  the sizes of green 
blue  and orange rectangles respectively denote available label proportions in different categories.
We only know the sample feature information and class proportions in every group.

class K +1 and exploiting feature matching (FM) as the generator objective. Unlike performing
maximizing log-likelihood on the variational lower bound of unlabeled data [16  17]  GANs seeks
the equilibrium between two networks (discriminator and generator) by alternatively upgrading in an
adversarial game  and directly obtains the ﬁnal classiﬁer upon the discriminator.
In this paper  we push the envelope further by focusing on applying GANs to another WeLL problem:
learning from label proportions (LLP) (see [23  27  33] for real-life applications). We illustrate
multi-class LLP problem in Figure 1. By referring group as bag  LLP also ﬁts for learning with bags
settings  which is primarily established in MIL [9]. In LLP  we strive for an instance-level multi-class
classiﬁer merely with multi-bag proportional information and instance features (inputs). On the right 
instances from different categories are classiﬁed based on a well-trained multi-class classiﬁer.
The main challenge for LLP is to shrink the uncertainty in label inference based on the bag-level
proportional information. Before deep learning making its appearance  several shallow methods have
been proposed  such as probability estimation methods (e.g.  MeanMap [23] and Laplacian MeanMap
[21]) and SVM-based methods (e.g.  InvCal [27] and alter-∝SVM [34  22]). However  statistical
approaches are extremely constrained by strict assumption on data distribution and prior knowledge 
while the SVM-based methods suffer from the NP-hard combinatorial optimization issue  thus is lack
of scalability.
The motivation of our work mainly lies in the following three aspects. Firstly  as introduced above 
GAN is an elegant recipe for solving WeLL problems  especially SSL [28]. From this viewpoint  our
approach is in line with the idea of applying GAN to incomplete label scenarios. More importantly 
the success of generative models for WeLL stems from the explicit or implicit representation learning 
which has been an essential method for unsupervised learning for a long time [5  24]  e.g.  VAE
[16]. In our approach  the convolution layers in discriminator can perform as a feature extractor for
downstream tasks  which is proved to be efﬁcient [24]. Hence  our work can be regarded as solving
LLP based on representation learning with GANs. In this scheme  generated fake samples encourage
the discriminator to not only detect the difference between the real and the fake instances  but also
distinguish true K classes for real samples (through K + 1 classiﬁer). Thirdly  most LLP methods
assume that the bags are i.i.d. [23  34]  which cannot sufﬁciently explore the underlying distribution
in the data and may be contradicted in certain applications. Instead  the generator in LLP-GAN is
designated to learn data distributions through the adversarial scheme without this assumption.
The remainder of this paper is organized as follows:

ment based on entropy regularization for the existing deep LLP solver.

• In Section 2  we give preliminaries regarding LLP problem and propose a simple improve-
• In Section 3  we describe our adversarial learning framework for LLP  especially the
lower bound of discriminator. In particular  we reveal the relationship between prior class
proportions and posterior class likelihoods. More importantly  we offer a decomposition
representation of the class likelihood with respect to the prior class proportions  which
veriﬁes the existence of the ﬁnal classiﬁer.
• In Section 4  we empirically show that our method can achieve SOTA performance on

large-scale LLP problems with a low computational complexity.

2

dolphinpandabutterflydolphinpandabutterflydolphinpandabutterfly45%25%30%25%45%50%30%50%17%33%30%20%Bag1Bag2Bag3Bag4dolphinbutterflypandadolphinbutterflypandaLLP2 Preliminaries

This section offers necessary preliminaries for our approach  including the formal problem setting
and related work with simple extensions.

2.1 The Multi-class LLP

Before further discussion  we formally describe multi-class LLP. For simplicity  we assume that all
i }  i = 1  2 ···  n be the bags in training set. Then 
the bags are disjoint and let Bi ={x1
training data is D =B1 ∪ B2 ∪···∪ Bn Bi ∩ Bj =∅ ∀i (cid:54)= j  where the total number of bags is n.
Assuming we have K classes  for Bi  let pi be a K-element vector  where the kth element pk

proportion of instances belonging to the class k  with the constraint(cid:80)K

i  ···  xNi

i is the

i   x2

k=1 pk

i = 1  i.e. 

i ∈Bi  yj∗
|{j∈ [1 : Ni]|xj
|Bi|

i = k}|

pk
i :=
Here  [1 : Ni] ={1  2 ···  Ni} and yj∗
this way  we can denote the available training data as L ={(Bi  pi)}n
an instance-level classiﬁer based on L.

i

.

is the unaccessible ground-truth instance-level label of xj

i . In
i=1. The goal of LLP is to learn

(1)

2.2 Deep LLP Approach

In terms of deep learning  DLLP ﬁrstly leverages DNNs to solve multi-class LLP problem [1]. Using
DNN’s probabilistic classiﬁcation outputs  it is straightforward to adapt cross-entropy loss into a
bag-level version by averaging the probability outputs in every bag as the proportion estimation. To
this end  inspired by [31]  DLLP reshapes standard cross-entropy loss by substituting instance-level
label with label proportion  in order to meet the requirement of proportion consistency.
In detail  suppose that ˜pj
i   where θ is the network
parameter. Let ⊕ be element-wise summation operator. Then  the bag-level label proportion in the
ith bag is obtained by incorporating the element-wise posterior probability:

i ) is the vector-valued DNNs output for xj

i = pθ(y|xj

Ni(cid:77)

j=1

Ni(cid:77)

j=1

pi =

1
Ni

˜pj

i =

1
Ni

pθ(y|xj
i ).

(2)

Different from the discriminant approaches  in order to smooth max function [6]  ˜pj
i is in a vector-type
softmax manner to produce the probability distribution for classiﬁcation. Taking log as element-wise
logarithmic operator  the objective of DLLP can be intuitively formulated using cross-entropy loss
(cid:124)
i log(pi). It penalizes the difference between prior and posterior probabilities in
i=1 p

Lprop =−(cid:80)n

bag-level  and commonly exists in GAN-based SSL [29].

2.3 Entropy Regularization for DLLP

Following the entropy regularization strategy [12]  we can introduce an extra loss Ein with a trade-off
hyperparameter λ to constrain instance-level output distribution in a low entropy accordingly:

L = Lprop + λEin = − n(cid:88)

n(cid:88)

Ni(cid:88)

(cid:124)
i log(pi) − λ
p

(˜pj
i )

(cid:124)

log(˜pj

i ).

(3)

i=1

i=1

j=1

This straightforward extension of DLLP is similar to a KL divergence  taking care of bag-level and
instance-level consistencies simultaneously. It takes advantage of DNN’s output distribution to cater
to the label proportions requirement  as well as minimizing output entropy as a regularization term to
guarantee high true-fake belief. This is believed to be linked with an inherent maximum a posteriori
(MAP) estimation [6] with certain prior distribution in network parameters. However  we will not
look at the performance of this extension and consider not to include it as a baseline  because the
experimental results empirically suggest that the original DLLP has already converged to the solution
with fairly low instance-level entropy  which makes the proposed regularization term redundant. We
offer results of this empirical study in the Supplementary Material.

3

3 Adversarial Learning for LLP

In this section  we focus on LLP based on adversarial learning and propose LLP-GAN  which devotes
GANs to harnessing LLP problem.
We illustrate the LLP-GAN framework in Figure 2. Firstly  the generator is employed to generate
image with input noise  which is labeled as fake  and the discriminator yields class conﬁdence maps
for each class (including the fake one) by taking both fake and real data as its inputs. This results in
the adversarial loss. Secondly  we incorporate the proportions by adding the cross entropy loss.

Figure 2: An illustration of our LLP-GAN framework.

3.1 The Objective Function of Discriminator

max

D

n(cid:88)

(cid:104)

(cid:105)

(cid:104)

(cid:105)

n(cid:88)

In LLP-GAN  our discriminator is not only to identify whether a sample is from the real data or not 
but also to elaborately distinguish each real input’s label assignment as a K classes classiﬁer. We
incorporate the unsupervised adversarial learning into the Lunsup term.
Next  the main issue becomes how to exploit the proportional information to guide this unsupervised
learning correctly. To this end  we replace the supervised information in semi-supervised GANs with
label proportions  resulting in Lsup  same as Lprop in (3).
Deﬁnition 1. Suppose that P is a partition to divide the data space into n disjoint sections. Let
d(x)  i = 1  2 ···   n be marginal distributions with respect to elements in P respectively. Accord-
pi
d(x)  i = 1  2 ···   n. In the meantime 
ingly  n bags in LLP training data spring from sampling upon pi
let p(x  y) be the unknown holistic joint distribution.
We normalize the ﬁrst K classes in PD(·|x) into the instance-level posterior probability ˜pD(·|x) and
compute p based on (2). Then  the ideal optimization problem for the discriminator of LLP-GAN is:

V (G  D) = Lunsup +Lsup = Lreal +Lf ake−λCEL(p  p)

=

Ex∼pi

d

logPD(y≤ K|x)

+Ex∼pg

logPD(K +1|x)

+λ

(cid:124)
p
i log(pi).

(4)

i=1

i=1

PD(k|x)

Here  pg(x) is the distribution of the synthesized data.
Remark 1. When PD(K +1|x)(cid:54)= 1  the normalized instance-level posterior probability ˜pD(·|x) is:
˜pD(k|x) =
(5)
K   k = 1  2 ···   K. Note that weight λ in (4) is added to balance
If PD(K+1|x) = 1  let ˜pD(k|x) = 1
between supervised and unsupervised terms  which is a slight revision of SSL with GANs [28  8].
Intuitively  we reckon that the proportional information is too weak to fulﬁll supervised learning
pursuit. Hence  a relatively large weight should be preferable in the experiments. However  large λ
may result in unstable GANs training. For simplicity  we ﬁx λ = 1 in the following theoretical analysis
on discriminator.

  k = 1  2 ···   K.

1−PD(K +1|x)

Aside from identifying the ﬁrst two terms in (4) as that in semi-supervised GANs  the cross-entropy
term harnesses the label proportions consistency. In order to justify the non-triviality of this loss  we
ﬁrst look at its lower bound. More importantly  it is easier to perform the gradient method on the
lower bound  because it swaps the order of log and the summation operation. For brevity  the analysis
will be done in a non-parametric setting  i.e.  we assume that both D and G have inﬁnite capacity.

4

DiscriminatorDiscriminatorC1 C2C3GeneratorGeneratorNoiseC4 Real DataFake Data[5/9 2/9 2/9][3/9 2/9 4/9][4/9 2/9 3/9][2/9 4/9 3/9]Predicted ProportionsAdversarial LossCross Entropy LossRemark 2 (The Lower Bound Approximation). Let pi(k) be the class k proportion in the ith bag.
According to the idea of sampling methods and Jensen’s inequality  we have:

−CEL(p  p) =

pi(k)log

˜pD(k|xj
i )

n(cid:88)
K(cid:88)
(cid:119) n(cid:88)
K(cid:88)

k=1

i=1

Ni(cid:88)

j=1

(cid:104) 1
(cid:104)(cid:90)

Ni

(cid:105)

pi(k)log

d(x)˜pD(k|x)dx
pi

pi(k)Ex∼pi

d

log ˜pD(k|x)

(cid:105)(cid:62) n(cid:88)

K(cid:88)

(cid:104)

(cid:105)

(6)

.

i=1

k=1

i=1

k=1

The expectation in the last term can be approximated by sampling. Similar to EM mechanism [20]
for mixture models  by approximating −CEL(p  p) with its lower bound  we can perform gradient
ascend independently on every sample. Hence  SGD can be applied.

As shown in (6)  in order to facilitate the gradient computation  we substitute cross entropy in (4) by

its lower bound and denote this approximate objective function for discriminator by (cid:101)V (G  D).

3.2 The Optimal Discriminator and LLP Classiﬁer

Now  we give the optimal discriminator and the ﬁnal classiﬁer for LLP based on the analysis of

(cid:101)V (G  D). Firstly  we have the following result of the lower bound in (6).

Lemma 1. The maximization on the lower bound in (6) induces an optimal discriminator D∗ with a
posterior distribution ˜pD∗ (y|x)  which is consistent with the prior distribution pi(y) in each bag.

(cid:90)

d

d

d

d

k=1

dy

+

k=1

(7)

(cid:105)

(cid:105)

(cid:105)

log

d

(cid:105)

(cid:105)

.

(cid:104)

(cid:90)

p(yi)

log
d

Ex∼pi

d

pi(y)log

= Ex∼pi

K(cid:88)

pi(k)Ex∼pi

pi(k)Ex∼pi

˜pD(y|x)

dy+Ex∼pi

[logp(x)] = Ex∼pi

(cid:62) K(cid:88)

˜pD(y|x)
p(y|x)

log ˜pD(y|x)+log

= Ex∼pi
p(x|y)
p(y|x)

(cid:104) pi(y)p(x|y)

Proof. Taking the aggregation with respect to one bag  for example  the ith bag  we have:
˜pD(y|x)
p(y|x)

(cid:104) p(x  y)
(cid:104)
(cid:104)
log ˜pD(k|x)
Here  because we only consider x ∼ pi
d  p(x  y) = pi(y)p(y|x) holds. Note that the last term in (7)
is free of the discriminator  and the aggregation can be independently performed within every bag
due to the disjoint assumption on bags. Then  maximizing the lower bound in (6) is equivalent to
minimizing the expectation of KL-divergence between pi(y) and ˜pD(y|x). Because of the inﬁnite
capacity assumption on discriminator and the non-negativity of KL-divergence  we have:
d(x).

˜pD(y|x)
KL(pi(y)(cid:107)˜pD(y|x))
p(x|k)
p(k|x)

D∗ = arg min
That concludes the proof.
Lemma 1 tells us that if there is only one bag  then the ﬁnal classiﬁer ˜pD∗ (y|x) a.e.= p(y). However 
there are normally multiple bags in LLP problem  the ﬁnal classiﬁer will somehow be a trade-off
among all the prior proportions pi(y)  i = 1  2 ···  n. Next  we will show how the adversarial learning
on the discriminator helps to determine the formulation of this trade-off in a weighted aggregation.

Theorem 1. For ﬁxed G  the optimal discriminator D∗ for (cid:101)V (G  D) satisﬁes:

KL(pi(y)(cid:107)˜pD(y|x)) ⇔ ˜pD∗ (y|x) a.e.= pi(y)  x ∼ pi

Ex∼pi

d

(8)

D

Proof. According to (4) and (6) and given any generator G  we have:

(cid:101)V (G  D) =
(cid:90) (cid:110) n(cid:88)

=

n(cid:88)

i=1

Ex∼pi

(cid:104)
(cid:104)
log(cid:2) K(cid:88)

d

pi
d(x)

PD(k|x)(cid:3)+

K(cid:88)

pi(k)log

PD(k|x)

1−PD(K +1|x)

i=1

k=1

k=1

log(1−PD(K +1|x))

+Ex∼pg

logPD(K +1|x)

pi(k)Ex∼pi

d

log ˜pD(k|x)

(cid:105)

  k = 1  2 ···   K.
K(cid:88)
(cid:104)
1− K(cid:88)

n(cid:88)

+pg(x)log

(cid:105)

k=1

i=1

+

(9)

(cid:105)

(cid:104)

(cid:105)(cid:111)

PD(k|x)

dx.

k=1

(10)

PD∗ (y = k|x) =

i=1 pi(k)pi
i=1 pi

d(x)
d(x)+pg(x)

(cid:80)n
(cid:80)n
(cid:105)

(cid:104)

By taking the derivative of the integrand  we ﬁnd the solution in [0  1] for maximization as (9).

5

Remark 3 (Beyond the Incontinuity of pg). According to [2]  the problematic scenario is that the
generator is a mapping from a low dimensional space to a high dimensional one. This will result in
the density of pg(x) infeasible. However  based on the deﬁnition of ˜pD(y|x) in (5)  we have:

(cid:80)n
(cid:80)n

˜pD∗ (y|x) =

i=1 pi(y)pi
d(x)

i=1 pi

d(x)

=

n(cid:88)

i=1

wi(x)pi(y).

(11)

Hence  our ﬁnal classiﬁer does not depend on pg(x). Furthermore  (11) explicitly expresses the
(cid:80)n
normalized weights of the aggregation with wi(x) = pi
Remark 4 (Relationship to One-side Label Smoothing). Notice that the optimal discriminator
D∗ is also related to the one-sided label smoothing mentioned in [28]  which is inspirited by [30]
and shown to reduce the vulnerability of neural networks to adversarial examples [32].
In particular  in our model  we only smooth labels of real data (multi-class) in the discriminator  by
setting the targets as the prior proportions pi(y) in corresponding bags.

d(x)
i=1 pi

d(x) .

3.3 The Objective Function of Generator

(cid:101)V (G  D∗) = min

Normally  for the generator  we should solve the following optimization problem with respect to pg.

Ex∼pg logPD∗ (K + 1|x).

G

G

min

of a set of convex functions is still convex  we have the following sufﬁcient and necessary condition
of global optimality.
Theorem 2. The global minimum of C(G) is achieved if and only if pg = 1
n

Denoting C(G) = maxD(cid:101)V (G  D) =(cid:101)V (G  D∗)  because(cid:101)V (G  D) is convex in pg and the supremum
Proof. Denote pd =(cid:80)n
(cid:104)

d. Hence  according to Theorem 1  we can reformulate C(G) as:

(cid:80)n

d.
i=1 pi

i=1 pi

(12)

(cid:104)

(cid:105)

(cid:105)

K(cid:88)

pg(x)

+

pi(k)Ex∼pi

d

log ˜pD∗ (k|x)

C(G) =

+Ex∼pg

log

pd(x)+pg(x)

k=1

(cid:105)
n(cid:88)
= 2 · JSD(pd(cid:107)pg)−2log(2)− n(cid:88)

pd(x)+pg(x)

Ex∼pi

pd(x)

log

i=1

d

n(cid:88)
(cid:105)

i=1

 

(cid:104)
(cid:104)

CE(pi(y)  ˜pD∗ (y|x))

Ex∼pi

d

i=1

(13)
where JSD(·(cid:107)·) and CE(· ·) are the Jensen-Shannon divergence and cross entropy between two
distributions  respectively. However  note that pd is a summation of n independent distributions  so
n pd is a well-deﬁned probabilistic density. Then  we have:
1

C(G) = nlog(n)−(n+1)log(n+1)− n(cid:88)

∗
C(G

) = min

G

CE(pi(y)  ˜pD∗ (y|x))

Ex∼pi

d

i=1

(cid:105) ⇐⇒ pg∗ a.e.

=

1
n

pd.

(14)

(cid:104)

That concludes the proof.
Remark 5. When there is only one bag  the ﬁrst two terms in (14) will degenerate as nlog(n)−(n+
1)log(n+1) =−2log2  which adheres to results in original GANs. On the other hand  the third term
manifests the uncertainty on instance label  which is concealed in the form of proportion.
Remark 6. According to the analysis above  ideally  we can obtain the Nash equilibrium between
the discriminator and the generator  i.e.  the solution pair (G∗  D∗) satisﬁes:
) ∀G.

  D) ∀D;(cid:101)V (G

) (cid:54) (cid:101)V (G  D

) (cid:62) (cid:101)V (G

(cid:101)V (G

(15)

  D

  D

∗

∗

∗

∗

∗

∗

However  as shown in [8]  a well-trained generator would lead to the inefﬁciency of supervised
information. In other words  the discriminator would possess the same generalization ability as merely
training it on Lprop. Hence  we apply feature matching (FM) to the generator and obtain its alternative
objective by matching the expected value of the features (statistics) on an intermediate layer of the
discriminator [28]: L(G) =(cid:107)Ex∼ 1
2. In fact  FM is similar to the perceptual
loss for style transfer in a concurrent work [15]  and the goal of this improvement is to impede the
“perfect” generator resulting in unstable training and discriminator with low generalization.

f (x)−Ex∼pg f (x)(cid:107)2

n pd

6

3.4 LLP-GAN Algorithm

So far  we have clariﬁed the objective functions of both discriminator and generator in LLP-GAN.
When accomplishing the training stage  the discriminator can be put into effect as the ﬁnal classiﬁer.

The strict proof for algorithm convergence is similar to that in [11]. Because maxD(cid:101)V (G  D) is
convex in G  and the subdifferential of maxD(cid:101)V (G  D) contains that of (cid:101)V (G  D∗) in every step  the

line search method (stochastic) gradient descent converges [7].
We present the LLP-GAN algorithm  which coincides with the algorithm of the original GAN [11].

Algorithm 1: LLP-GAN Training Algorithm
Input: The training set L ={(Bi  pi)}n
Output: The parameters of the ﬁnal discriminator D.
Set m to the total number of training data points.
for i=1:L do

i=1; L: number of total iterations; λ: weight parameter.

Draw m samples {z(1)  z(2) ···  z(m)} from a simple-to-sample noise prior p(z) (e.g.  N (0  I)).
Compute {G(z(1))  G(z(2)) ···   G(z(m))} as sampling from pg(x).

Fix the generator G and perform gradient ascent on parameters of D in (cid:101)V (G  D) for one step.

Fix the discriminator D and perform gradient descent on parameters of G in L(G) for one step.

end
Return parameters of the discriminator D in the last step.

4 Experiments

Four benchmark datasets  MNIST  SVHN  CIFAR-10  and CIFAR-100 are investigated in our
experiments1. In addition to test error comparison  three issues are discussed: the generated samples 
the performance under different selections of hyperparameter λ  and the algorithm scalability.

4.1 Experimental Setup

To keep up the same settings in previous work  bag size is ﬁxed as 16  32  64  and 128. We divide
training data into bags. MNIST data can be found in the code in the Supplementary Material. We
conceal the accessible instance-level labels by replacing them with bag-level label proportions. Note
that we still need the instance-level labels in test data to justify the effectiveness of the obtained
classiﬁer.

4.2 Results on CIFAR-10

Firstly  we perform both DLLP and LLP-GAN on CIFAR-10  which is a computer-vision dataset
used for object recognition with 60 000 color images belonging to 10 categories  respectively. In the
experimental setting  the training data is equally divided into ﬁve minibatches  with 10 000 images in
each one  and the test data with exactly 1 000 images in every category.

4.2.1 Convergence Analysis

We report the convergence curves of test error (y-axis) with respect to the epoch (x-axis) under
different bag sizes in Figure 3. As shown  our results are highly superior to DLLP in most of the
epochs  with signiﬁcant convergence in test error. In contrast  DLLP fails to converge under relatively
large bag sizes (i.e.  64 and 128). Also  our method achieves a better performance in accuracy.

4.2.2 Generated Samples

The original GAN suffers from inefﬁcient training on the generator [2]. It suggests that the dis-
criminator and generator cannot simultaneously perform well [8]. In LLP-GAN  although it is the
discriminator that we are interested in  we still expect a competent generator to construct efﬁcient
adversarial learning paradigm. As a result  we look at the generated samples of original GANs with

1Code is available at https://github.com/liujiabin008/LLP-GAN.

7

(a) Bag size: 16

(b) Bag size: 32

(c) Bag size: 64

(d) Bag size: 128

Figure 3: The convergence curves on CIFAR-10 w/ different bag sizes.

(a) GANs with FM

(b) Ours after 50 epochs

(c) Ours after 60 epochs

(d) Ours after 70 epochs

Figure 4: Generated samples on CIFAR-10.

FM in Figure 4(a) and our method in Figure 4(b)  4(c) and 4(d). It demonstrates that our approach
can stably learn a comparable generator to produce similar samples to that of GANs.

4.3 The Results of Error Rate

Secondly  DLLP and LLP-GAN are carried out on four bench-
mark datasets with different bag sizes in Table 1. We also give
the fully supervised learning results as the baselines. In detail 
baseline for MNIST and CIFAR-10 is offered by [25]. We de-
scribe its architecture in the Supplementary Material. Network
in [18] is used as the baseline for SVHN and CIFAR-100.
In terms of test error  our method reaches a relatively better
result  except for the simplest task MNIST  where both algo-
rithms can attain satisfying results. However  DLLP becomes
unacceptable when the bag size increases  while our method
can properly tackle relatively large bag size. Besides  for each
dataset  LLP becomes extremely difﬁcult as bag size soaring  which is consistent with our intuition.
Again  the architectures of our network are given in the Supplementary Material.

Figure 5: The average error rates w/
different bag sizes.

Table 1: Test error rates (%) on benchmark datasets w/ different bag sizes.

Dataset

Algorithm

MNIST

SVHN

CIFAR-10

CIFAR-100

16

DLLP

DLLP

1.23 (0.100)
LLP-GAN 1.10 (0.026)
4.45 (0.069)
LLP-GAN 4.03 (0.021)
19.70 (0.77)
LLP-GAN 13.68 (0.35)
53.24 (0.77)
LLP-GAN 50.95 (0.67)

DLLP

DLLP

Bag Size

32

1.33 (0.094)
1.23 (0.088)
5.29 (0.54)
4.83 (0.51)
34.39 (0.82)
16.23 (0.43)
98.38 (0.11)
56.44 (0.78)

64

1.57 (0.088)
1.40 (0.089)
5.80 (0.91)
5.42 (0.59)
68.32 (1.34)
21.03 (1.82)
98.65 (0.09)
64.37 (1.52)

128

3.55 (0.27)
3.49 (0.27)
39.73 (1.60)
11.17 (1.12)
82.89 (2.66)
27.39 (4.31)
98.98 (0.08)
85.01 (1.81)

Baseline
CNNs
0.36

2.35

9.27

35.68

Because InvCal and alter-∝SVM are originally designed for binary problem  we randomly select two
classes and merely conduct binary classiﬁcation on all datasets. The detailed results are provided
in the Supplementary Material. The average error rates with different bag sizes are displayed in
Figure 5. From the results  we can conﬁdently tell the advantage of our algorithm in performance 

8

02004006008001000Epoch0.20.40.60.81ErrorDLLPLLP-GAN02004006008001000Epoch0.20.40.60.81ErrorDLLPLLP-GAN02004006008001000Epoch0.20.40.60.81ErrorDLLPLLP-GAN02004006008001000Epoch0.20.40.60.81ErrorDLLPLLP-GAN163264128Bag Size00.050.10.15ErrorInvCalalter-/SVMDLLPLLP-GANespecially when the bag size is relatively large. Indeed  at this moment  we cannot clarify to what
extent this advantage attributes to the deep learning model. However  in spite of both using deep
learning models  our method constantly performs better than DLLP.

4.4 Hyperparameter Analysis and Complexity with Sample Size

Thirdly  we illustrate the convergence curves of MNIST  SVHN  and CIFAR-10 under different λs in
Figure 6(a)  6(b) and 6(c). For simpler task (MNIST)  the performance is not sensitive to λ. However 
for harder task (CIFAR-10)  the performance becomes sensitive to λ. On the other hand  smaller λ
demonstrates more ﬂuctuations  which is much severer in simpler tasks (MNIST and SVHN). Besides 
Figure 6(b) indicates that the convergence speed may be sensitive to the choice of λ. In most of the
cases  λ(cid:62) 1 is a good choice  leading to a comparable performance within limited training time.
In addition  ﬁxing the bag size  we provide the relative training time (training time per bag) to
the relative sample size in Figure 6(d). We take logarithmic operation on sample size (x-axis). It
demonstrates that the relative training time is asymptotically linear to the logarithmic sample size m.
Denote the total training time as t  then t ≈ O(mlnm) < O(m2). Here  we assume that sample size
and # of bags are with same magnitude  due to the relative small bag sizes involved in our study.

(a) λ on MNIST

(b) λ on SVHN

(c) λ on CIFAR-10

(d) Training time w/ differ-
ent sample sizes

Figure 6: Analysis on hyperparameter and complexity.

4.5 Discussion on Experimental Results

Two issues should be clariﬁed for experiments. Firstly  as shown in Figure 3  the results demonstrate
oscillation as bag size soaring. This phenomenon indicates a common drawback of deep models: For
more complex objective surfaces (more possible label candidates)  normally the convergence will
be dramatically getting worse  due to more chances to attain local minima or saddle points of the
objective. Secondly  because our results are based on original datasets without data augmentation  the
reported DLLP performance is worse than that in the concurrent [10].

5 Conclusion

This paper proposed a new algorithm LLP-GAN for LLP problem in virtue of the adversarial learning
based on GANs. Consequently  our method is superior to existing methods in the following three
aspects. Firstly  it demonstrates nice theoretical properties that are innately in accordance with
GANs. Secondly  LLP-GAN can produce a probabilistic classiﬁer  which beneﬁts from the generative
model and meets the proportion consistency naturally. Thirdly  on account of equipping CNNs  our
algorithm is suitable for the large-scale problem  especially for image datasets. Additionally  the
experiments on four benchmark datasets have veriﬁed all these advantages of our approach.
Nevertheless  limitations in our method can be summarized in four aspects. Firstly  learning com-
plexity in the sense of PAC has not been involved in this study. That is to say  we cannot evaluate
the performance under limited data. Secondly  there is no guarantee on algorithm robustness to data
perturbations  notably when the proportions are imprecisely provided. Thirdly  varying GAN models
(such as WGAN [3]) are not fully considered  and their performance is still unknown. In addition 
in many real-world applications  the bags are built based on certain features  such as the education
levels and job titles  rather than randomly established. Hence  a practical issue will be to ensure good
performance under these non-random bag assignments. To overcome these drawbacks will shed light
on the promising improvement of our current work.

9

Acknowledgements

This work is supported by grants from: National Natural Science Foundation of China (No.61702099 
71731009  61472390  71932008  91546201  and 71331005)  Science and Technology Service
Network Program of Chinese Academy of Sciences (STS Program  No.KFJ-STS-ZDTP-060)  and
the Fundamental Research Funds for the Central Universities in UIBE (No.CXTD10-05). Bo Wang
would like to acknowledge that this research was conducted during his visit at Texas A&M University
and thank Dr. Xia Hu for his hosting and insightful discussions.

References
[1] Ehsan M. Ardehaly and Aron Culotta. Co-training for demographic classiﬁcation using deep
learning from label proportions. In International Conference on Data Mining Workshops  pages
1017–1024. IEEE  2017.

[2] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-

sarial networks. In International Conference on Learning Representations  2016.

[3] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial

networks. In International Conference on Machine Learning  pages 214–223  2017.

[4] Peter W Battaglia  Jessica B Hamrick  Victor Bapst  et al. Relational inductive biases  deep

learning  and graph networks. arXiv preprint arXiv:1806.01261  2018.

[5] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE transactions on pattern analysis and machine intelligence  35(8):1798–
1828  2013.

[6] Christopher Bishop. Pattern Recognition and Machine Learning. Springer  January 2006.

[7] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[8] Zihang Dai  Zhilin Yang  Fan Yang  William W Cohen  and Ruslan R Salakhutdinov. Good
semi-supervised learning that requires a bad gan. In Advances in neural information processing
systems  pages 6510–6520  2017.

[9] Thomas G. Dietterich  Richard H. Lathrop  and Tomás Lozano-Pérez. Solving the multiple

instance problem with axis-parallel rectangles. Artiﬁcial Intelligence  89(1-2):31–71  1997.

[10] Gabriel Dulac-Arnold  Neil Zeghidour  Marco Cuturi  Lucas Beyer  and Jean-Philippe Vert.

Deep multi-class learning from label proportions. arXiv preprint arXiv:1905.12909  2019.

[11] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  et al. Generative adversarial nets. In

Advances in Neural Information Processing Systems  pages 2672–2680  2014.

[12] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In

Advances in neural information processing systems  pages 529–536  2005.

[13] Kaiming He  Xiangyu Zhang  Shaoqing Ren  et al. Deep residual learning for image recognition.

In Computer Vision and Pattern Recognition  pages 770–778  2016.

[14] Geoffrey Hinton  Li Deng  Dong Yu  et al. Deep neural networks for acoustic modeling in

speech recognition. IEEE Signal Processing Magazine  29(6):82–97  2012.

[15] Justin Johnson  Alexandre Alahi  and Fei-Fei Li. Perceptual losses for real-time style transfer
and super-resolution. In European Conference on Computer Vision  pages 694–711. Springer 
2016.

[16] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

10

[17] Durk P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems 
pages 3581–3589  2014.

[18] Min Lin  Qiang Chen  and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400 

2013.

[19] Oded Maron and Tomás Lozano-Pérez. A framework for multiple-instance learning.

Advances in Neural Information Processing Systems  pages 570–576  1998.

In

[20] Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine 

13(6):47–60  1996.

[21] Giorgio Patrini  Richard Nock  Paul Rivera  and Tiberio Caetano. (Almost) no label no cry. In

Advances in Neural Information Processing Systems  pages 190–198  2014.

[22] Zhiquan Qi  Bo Wang  Fan Meng  et al. Learning with label proportions via NPSVM. IEEE

Transactions on Cybernetics  47(10):3293–3305  2017.

[23] Novi Quadrianto  Alex J. Smola  Tiberio S. Caetano  et al. Estimating labels from label

proportions. Journal of Machine Learning Research  10(Oct):2349–2374  2009.

[24] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[25] Antti Rasmus  Mathias Berglund  Mikko Honkala  Harri Valpola  and Tapani Raiko. Semi-
supervised learning with ladder networks. In Advances in neural information processing systems 
pages 3546–3554  2015.

[26] Joseph Redmon  Santosh Divvala  Ross Girshick  et al. You only look once: Uniﬁed  real-time

object detection. In Computer Vision and Pattern Recognition  pages 779–788  2016.

[27] Stefan Rueping. SVM classiﬁer estimation from group probabilities. In International Conference

on Machine Learning  pages 911–918  2010.

[28] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  et al. Improved techniques for training

GANs. In Advances in Neural Information Processing Systems  pages 2234–2242  2016.

[29] Jost T. Springenberg. Unsupervised and semi-supervised learning with categorical generative

adversarial networks. arXiv preprint arXiv:1511.06390  2015.

[30] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  et al. Rethinking the inception architecture

for computer vision. In Computer Vision and Pattern Recognition  pages 2818–2826  2016.

[31] Zilei Wang and Jiashi Feng. Multi-class learning from class proportions. Neurocomputing 

119(16):273–280  2013.

[32] David Warde-Farley and Ian Goodfellow. Adversarial perturbations of deep neural networks. In

Perturbations  Optimization  and Statistics  page 311. MIT Press  2016.

[33] Felix X. Yu  Liangliang Cao  Michele Merler  et al. Modeling attributes from category-attribute

proportions. In International Conference on Multimedia  pages 977–980. ACM  2014.

[34] Felix X. Yu  Dong Liu  Sanjiv Kumar  et al. ∝-SVM for learning with label proportions. In

International Conference on Machine Learning  pages 504–512  2013.

11

,Jiabin Liu
Bo Wang
Zhiquan Qi
YingJie Tian
Yong Shi