2013,Large Scale Distributed Sparse Precision Estimation,We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator  which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME  and establish rates of convergence for both the objective and optimality conditions. Further  we develop a large scale distributed framework for the computations  which scales to millions of dimensions and trillions of parameters  using hundreds of cores. The proposed framework solves CLIME in column-blocks and only involves elementwise operations and parallel matrix multiplications.  We evaluate our algorithm on both shared-memory and distributed-memory architectures  which can use block cyclic distribution of data and parameters to achieve load balance and improve the efficiency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores.,Large Scale Distributed Sparse Precision Estimation

Dept. of Computer Science & Engg  University of Minnesota  Twin Cities

Huahua Wang  Arindam Banerjee
{huwang banerjee}@cs.umn.edu

Cho-Jui Hsieh  Pradeep Ravikumar  Inderjit S. Dhillon
Dept. of Computer Science  University of Texas  Austin
{cjhsieh pradeepr inderjit}@cs.utexas.edu

Abstract

We consider the problem of sparse precision matrix estimation in high dimensions
using the CLIME estimator  which has several desirable theoretical properties. We
present an inexact alternating direction method of multiplier (ADMM) algorithm
for CLIME  and establish rates of convergence for both the objective and opti-
mality conditions. Further  we develop a large scale distributed framework for the
computations  which scales to millions of dimensions and trillions of parameters 
using hundreds of cores. The proposed framework solves CLIME in column-
blocks and only involves elementwise operations and parallel matrix multiplica-
tions. We evaluate our algorithm on both shared-memory and distributed-memory
architectures  which can use block cyclic distribution of data and parameters to
achieve load balance and improve the efﬁciency in the use of memory hierarchies.
Experimental results show that our algorithm is substantially more scalable than
state-of-the-art methods and scales almost linearly with the number of cores.

0 ∈ S p

(cid:80)

Introduction

1
Consider a p-dimensional probability distribution with true covariance matrix Σ0 ∈ S p
++ and true
++. Let [R1 ··· Rn] ∈ (cid:60)p×n be n indepen-
precision (or inverse covariance) matrix Ω0 = Σ−1
dent and identically distributed random samples drawn from this p-dimensional distribution. The
centered normalized sample matrix A = [a1 ··· an] ∈ (cid:60)p×n can be obtained as ai = 1√
n (Ri − ¯R) 
where ¯R = 1
i Ri  so that the sample covariance matrix can be computed as C = AAT . In
n
recent years  considerable effort has been invested in obtaining an accurate estimate of the precision
matrix ˆΩ based on the sample covariance matrix C in the ‘low sample  high dimensions’ setting 
i.e.  n (cid:28) p  especially when the true precision Ω0 is assumed to be sparse [28]. Suitable estima-
tors and corresponding statistical convergence rates have been established for a variety of settings 
including distributions with sub-Gaussian tails  polynomial tails [25  3  19]. Recent advances have
also established parameter-free methods which achieve minimax rates of convergence [4  19].
Spurred by these advances in the statistical theory of precision matrix estimation  there has been
considerable recent work on developing computationally efﬁcient optimization methods for solving
the corresponding statistical estimation problems: see [1  8  14  21  13]  and references therein.
While these methods are able to efﬁciently solve problems up to a few thousand variables  ultra-
large-scale problems with millions of variables remain a challenge. Note further that in precision
matrix estimation  the number of parameters scales quadratically with the number of variables; so
that with a million dimensions p = 106  the total number of parameters to be estimated is a trillion 
p2 = 1012. The focus of this paper is on designing an efﬁcient distributed algorithm for precision
matrix estimation under such ultra-large-scale dimensional settings.
We focus on the CLIME statistical estimator [3]  which solves the following linear program (LP):

min(cid:107) ˆΩ(cid:107)1

s.t.

(cid:107)C ˆΩ − I(cid:107)∞ ≤ λ  

(1)

1

where λ > 0 is a tuning parameter. The CLIME estimator not only has strong statistical guar-
antees [3]  but also comes with inherent computational advantages. First  the LP in (1) does not
explicitly enforce positive deﬁniteness of ˆΩ  which can be a challenge to handle efﬁciently in high-
dimensions. Secondly  it can be seen that (1) can be decomposed into p independent LPs  one
for each column of ˆΩ. This separable structure has motivated solvers for (1) which solve the LP
column-by-column using interior point methods [3  28] or the alternating direction method of multi-
pliers (ADMM) [18]. However  these solvers do not scale well to ultra-high-dimensional problems:
they are not designed to run on hundreds to thousands of cores  and in particular require the entire
sample covariance matrix C to be loaded into the memory of a single machine  which is impractical
even for moderate sized problems.
In this paper  we present an efﬁcient CLIME-ADMM variant along with a scalable distributed frame-
work for the computations [2  26]. The proposed CLIME-ADMM algorithm can scale up to millions
of dimensions  and can use up to thousands of cores in a shared-memory or distributed-memory ar-
chitecture. The scalability of our method relies on the following key innovations. First  we propose
an inexact ADMM [27  12] algorithm targeted to CLIME  where each step is either elementwise
parallel or involves suitable matrix multiplications. We show that the rates of convergence of the
objective to the optimum as well as residuals of constraint violation are both O(1/T ). Second  we
solve (1) in column-blocks of the precision matrix at a time  rather than one column at a time. Since
(1) already decomposes columnwise  solving multiple columns together in blocks might not seem
worthwhile. However  as we show our CLIME-ADMM working with column-blocks uses matrix-
matrix multiplications which  building on existing literature [15  5  11] and the underlying low rank
and sparse structure inherent in the precision matrix estimation problem  can be made substantially
more efﬁcient than repeated matrix-vector multiplications. Moreover  matrix multiplication can be
further simpliﬁed as block-by-block operations  which allows choosing optimal block sizes to min-
imize cache misses  leading to high scalability and performance [16  5  15]. Lastly  since the core
computations can be parallelized  CLIME-ADMM scales almost linearly with the number of cores.
We experiment with shared-memory and distributed-memory architectures to illustrate this point.
Empirically  CLIME-ADMM is shown to be much faster than existing methods for precision esti-
mation  and scales well to high-dimensional problems  e.g.  we estimate a precision matrix of one
million dimension and one trillion parameters in 11 hours by running the algorithm on 400 cores.
Our framework can be positioned as a part of the recent surge of effort in scaling up machine learn-
ing algorithms [29  22  6  7  20  2  23  9] to “Big Data”. Scaling up machine learning algorithms
through parallelization and distribution has been heavily explored on various architectures  includ-
ing shared-memory architectures [22]  distributed memory architectures [23  6  9] and GPUs [24].
Since MapReduce [7] is not efﬁcient for optimization algorithms  [6] proposed a parameter server
that can be used to parallelize gradient descent algorithms for unconstrained optimization problems.
However  this framework is ill-suited for the constrained optimization problems we consider here 
because gradient descent methods require the projection at each iteration which involves all vari-
ables and thus ruins the parallelism. In other recent related work based on ADMM  [23] introduce
graph projection block splitting (GPBS) to split data into blocks so that examples and features can
be distributed among multiple cores. Our framework uses a more general blocking scheme (block
cyclic distribution)  which provides more options in choosing the optimal block size to improve the
efﬁciency in the use of memory hierarchies and minimize cache misses [16  15  5]. ADMM has
also been used to solve constrained optimization in a distributed framework [9] for graphical model
inference  but they consider local constraints  in contrast to the global constraints in our framework.
Notation: A matrix is denoted by a bold face upper case letter  e.g.  A. An element of a matrix
is denoted by a upper case letter with row index i and column index j  e.g.  Aij is the ij-th el-
ement of A. A block of matrix is denoted by a bold face lower case letter indexed by ij  e.g. 
Aij. (cid:126)Aij represents a collection of blocks of matrix A on the ij-th core (see block cyclic distri-
bution in Section 4). A(cid:48) refers the transpose of A. Matrix norms used are all elementwise norms 
(cid:80)n
ij (cid:107)A(cid:107)∞ = max1≤i≤p 1≤j≤n |Aij|. The
j=1 AijBij. X ∈ (cid:60)p×k de-
notes k(1 ≤ k ≤ p) columns of the precision matrix ˆΩ  and E ∈ (cid:60)p×k denotes the same k columns
of the identity matrix I ∈ (cid:60)p×p. Let λmax(C) be the largest eigenvalue of covariance matrix C.

e.g.  (cid:107)A(cid:107)1 =(cid:80)p
matrix inner product is deﬁned in elementwise  e.g.  (cid:104)A  B(cid:105) =(cid:80)p

(cid:80)n

(cid:80)n
j=1 |Aij| (cid:107)A(cid:107)2

2 =(cid:80)p

i=1

i=1

i=1

j=1 A2

2

Algorithm 1 Column Block ADMM for CLIME
1: Input: C  λ  ρ  η
2: Output: X
3: Initialization: X0  Z0  Y0  V0  ˆV0 = 0
4: for t = 0 to T − 1 do
5: X-update: Xt+1 = soft(Xt − Vt  1
6: Mat-Mul:

Ut+1 = CXt+1

low rank : Ut+1 = A(A(cid:48)Xt+1)
Z-update: Zt+1 = box(Ut+1 + Yt  λ)  where

7:
8: Y-update: Yt+1 = Yt + Ut+1 − Zt+1
9: Mat-Mul:

(cid:26) sparse :
(cid:26) sparse :

ˆVt+1 = CYt+1
ˆVt+1 = A(A(cid:48)Yt+1)

η )  where

low rank :

10: V-update: Vt+1 = ρ
11: end for

η (2 ˆVt+1 − ˆVt)

(cid:40) Xij − γ  
(cid:40) Eij + λ 

Xij + γ  
0  

Xij 
Eij − λ 

if Xij > γ  
if Xij < −γ  
otherwise
if Xij − Eij > λ 
if |Xij − Eij| ≤ λ 
if Xij − Eij < −λ 

soft(X  γ) =

box(X  E  λ) =

(2)

2 Column Block ADMM for CLIME
In this section  we propose an algorithm to estimate the precision matrix in terms of column blocks
instead of column-by-column. Assuming a column block contains k(1 ≤ k ≤ p) columns  the
sparse precision matrix estimation amounts to solving (cid:100)p/k(cid:101) independent linear programs. Denoting
X ∈ (cid:60)p×k be k columns of ˆΩ  (1) can be written as

(cid:107)CX − E(cid:107)∞ ≤ λ  
which can be rewritten in the following equality-constrained form:

min(cid:107)X(cid:107)1

s.t.

min(cid:107)X(cid:107)1

(3)
Through the splitting variable Z ∈ (cid:60)p×k  the inﬁnity norm constraint becomes a box constraint and
is separated from the (cid:96)1 norm objective. We use ADMM to solve (3). The augmented Lagrangian
of (3) is

s.t.

(cid:107)Z − E(cid:107)∞ ≤ λ  CX = Z .

Lρ = (cid:107)X(cid:107)1 + ρ(cid:104)Y  CX − Z(cid:105) +

(cid:107)CX − Z(cid:107)2
2  

(4)

ρ
2

where Y ∈ (cid:60)p×k is a scaled dual variable and ρ > 0. ADMM yields the following iterates [2]:

ρ
2

(cid:107)CX − Zt + Yt(cid:107)2
2  

Xt+1 = argminX (cid:107)X(cid:107)1 +
Zt+1 = argmin
Yt+1 = Yt + CXt+1 − Zt+1 .
(7)
(5) can be solved using exisiting Lasso algorithms  but that will lead to a
As a Lasso problem 
double-loop algorithm.
(5) does not have a closed-form solution since C in the quadratic penalty
term makes X coupled. We decouple X by linearizing the quadratic penalty term and adding a
proximal term as follows:

(cid:107)CXt+1 − Z + Yt(cid:107)2
2  

(cid:107)Z−E(cid:107)∞≤λ

(5)

(6)

ρ
2

Xt+1 = argminX (cid:107)X(cid:107)1 + η(cid:104)Vt  X(cid:105) +

(8)
η C(Yt + CXt − Zt) and η > 0. (8) is usually called an inexact ADMM update.
η (2 ˆVt − ˆVt−1) . (8) has the

η C(2Yt − Yt−1). Let ˆVt = CYt  we have Vt = ρ

(cid:107)X − Xt(cid:107)2
2  

η
2

where Vt = ρ
Using (7)  Vt = ρ
following closed-form solution:

Xt+1 = soft(Xt − Vt 

1
η

)  

(9)

where soft denotes the soft-thresholding and is deﬁned in Step 5 of Algorithm 1.
Let Ut+1 = CXt+1. (6) is a box constrained quadratic programming which has the following
closed-form solution:

Zt+1 = box(Ut+1 + Yt  E  λ)  

(10)

3

where box denotes the projection onto the inﬁnity norm constraint (cid:107)Z − E(cid:107)∞ ≤ λ and is deﬁned
in Step 7 of Algorithm 1. In particular  if (cid:107)Ut+1 + Yt − E(cid:107)∞ ≤ λ  Zt+1 = Ut+1 + Yt and thus
Yt+1 = Yt + Ut+1 − Zt+1 = 0.
The ADMM algorithm for CLIME is summarized in Algorithm 1. In Algorithm 1  while step 5  7  8
and 10 amount to elementwise operations which cost O(pk) operations  steps 6 and 9 involve matrix
multiplication which is the most computationally intensive part and costs O(p2k) operations. The
memory requirement includes O(pn) for A and O(pk) for the other six variables.
As the following results show  Algorithm 1 has a O(1/T ) convergence rate for both the objective
function and the residuals of optimality conditions. The proof technique is similar to [26]. [12]
shows a similar result as Theorem 2 but uses a different proof technique. For proofs  please see
Appendix A in the supplement.
Theorem 1 Let {Xt  Zt  Yt} be generated by Algorithm 1 and ¯XT = 1
Z0 = Y0 = 0 and η ≥ ρλ2

t=1 Xt. Assume X0 =

(cid:80)T

T

max(C). For any CX = Z  we have
(cid:107) ¯XT(cid:107)1 − (cid:107)X(cid:107)1 ≤ η(cid:107)X(cid:107)2

2

2T

.

(11)

Theorem 2 Let {Xt  Zt  Yt} be generated by Algorithm 1 and {X∗  Z∗  Y∗} be a KKT point for
the Lagrangian of (3). Assume X0 = Z0 = Y0 = 0 and η ≥ ρλ2

(cid:107)CXT − ZT(cid:107)2

2 + (cid:107)ZT − ZT−1(cid:107)2

2 + (cid:107)XT − XT−1(cid:107)2

.

(12)

max(C). We have
2 + η
T

(cid:107)Y∗(cid:107)2

ρ I−C2 ≤

η

ρ(cid:107)X∗(cid:107)2

2

3 Leveraging Sparse  Low-Rank Structure
In this section  we consider a few possible directions that can further leverage the underlying struc-
ture of the problem; speciﬁcally sparse and low-rank structure.

3.1 Sparse Structure
As we detail here  there could be sparsity in the intermediate iterates  or the sample covariance
matrix itself (or a perturbed version thereof); which can be exploited to make our CLIME-ADMM
variant more efﬁcient.
Iterate Sparsity: As the iterations progress  the soft-thresholding operation will yield a sparse
Xt+1  which can help speed up step 6: Ut+1 = CX t+1  via sparse matrix multiplication. Further 
the box-thresholding operation will yield a sparse Yt+1. In the ideal case  if (cid:107)Ut+1+Yt−E(cid:107)∞ ≤ λ
in step 7  then Zt+1 = Ut+1 + Yt. Thus  ˆYt+1 = Yt + Ut+1 − Zt+1 = 0. More generally  Yt+1
will become sparse as the iterations proceed  which can help speed up step 9: ˆVt+1 = CYt+1.
Sample Covariance Sparsity: We show that one can “perturb” the sample covariance to obtain a
sparse and coarsened matrix  solve CLIME with this pertubed matrix  and yet have strong statistical
guarantees. The statistical guarantees for CLIME [3]  including convergence in spectral  matrix
L1  and Frobenius norms  only require from the sample covariance matrix C a deviation bound of

the form (cid:107)C − Σ0(cid:107)∞ ≤ c(cid:112)log p/n  for some constant c. Accordingly  if we perturb the matrix

C with a perturbation matrix ∆ so that the perturbed matrix (C + ∆) continues to satisfy the
deviation bound  the statistical guarantees for CLIME would hold even if we used the perturbed
matrix (C + ∆). The following theorem (for details  please see Appendix B in the supplement)
illustrates some perturbations ∆ that satisfy this property:

Theorem 3 Let the original random variables Ri be sub-Gaussian  with sample covariance C. Let
∆ be a random perturbation matrix  where ∆ij are independent sub-exponential random variables.
Then  for positive constants c1  c2  c3  P ((cid:107)C + ∆ − Σ0(cid:107)∞ ≥ c1

(cid:113) log p
n ) ≤ c2p−c3.

As a special case  one can thus perturb elements of Cij with suitable constants ∆ij with |∆ij| ≤

c(cid:112)log p/n  so that the perturbed matrix is sparse  i.e.  if |Cij| ≤ c(cid:112)log p/n  then it can be safely

4

truncated to 0. Thus  in practice  even if sample covariance matrix is only close to a sparse ma-
trix [21  13]  or if it is close to being block diagonal [21  13]  the complexity of matrix multiplication
in steps 6 and 9 can be signiﬁcantly reduced via the above perturbations.

3.2 Low Rank Structure
Although one can use sparse structures of matrices participating in the matrix multiplication to
accelerate the algorithm  the implementation requires substantial work since dynamic sparsity of
X and Y is unknown upfront and static sparsity of the sample covariance matrix may not exist.
Since the method will operate in a low-sample setting  we can alternatively use the low rank of the
sample covariance matrix to reduce the complexity of matrix multiplication. Since C = AAT and
p (cid:29) n  CX = A(AT X)  and thus the computational complexity of matrix multiplication reduces
from O(p2k) to O(npk)  which can achieve signiﬁcant speedup for small n. We use such low-rank
multiplications for the experiments in Section 5.

4 Scalable Parallel Computation Framework

In this section  we elaborate on scalable frameworks for CLIME-ADMM in both shared-memory
and distributed-memory achitectures.
In a shared-memory architecture (e.g.  a single machine)  data A is loaded to the memory and shared
by q cores  as shown in Figure 1(a). Assume the p × p precision matrix ˆΩ is evenly divided into
l = p/k (≥ q) column blocks  e.g.  X1 ···   Xq ···   Xl  and thus each column block contains k
columns. The column blocks are assigned to q cores cyclically  which means the j-th column block
is assigned to the mod(j  q)-th core. The q cores can solve q column blocks in parallel without com-
munication and synchronization  which can be simply implemented via multithreading. Meanwhile 
another q column blocks are waiting in their respective queues. Figure 1(a) gives an example of how
to solve 8 column blocks on 4 cores in a shared-memory environment. While the 4 cores are solving
the ﬁrst 4 column blocks  the next 4 column blocks are waiting in queues (red arrows).
Although the shared-memory framework is free from communication and synchronization  the lim-
ited resources prevent it from scaling up to datasets with millions of dimensions  which can not be
loaded to the memory of a single machine or solved by tens of cores in a reasonble time. As more
memory and computing power are needed for high dimensional datasets  we implement a frame-
work for CLIME-ADMM in a distributed-memory architecture  which automatically distributes
data among machines  parallelizes computation  and manages communication and synchronization
among machines  as shown in Figure 1(b). Assume q processes are formed as a r × c process
grid and the p × p precision matrix ˆΩ is evenly divided into l = p/k (≥ q) column blocks  e.g. 
Xj  1 ≤ j ≤ l. We solve a column block Xj at a time in the process grid. Assume the data matrix
A has been evenly distributed into the process grid and (cid:126)Aij is the data on the ij-th core  i.e.  A is
colletion of (cid:126)Aij under a mapping scheme  which we will discuss later. Figure 1(b) illustrates that
the 2 × 2 process grid is computing the ﬁrst column block X1 while the second column block X2
is waiting in queues (red lines)  assuming X1  X2 are distributed into the process grid in the same
way as A and (cid:126)X1
A typical issue in parallel computation is load imbalance  which is mainly caused by the compu-
tational disparity among cores and leads to unsatisfactory speedups. Since each step in CLIME-
ADMM are basic operations like matrix multiplication  the distribution of sub-matrices over pro-
cesses has a major impact on the load balance and scalability. The following discussion focuses on
the matrix multiplication in the step 6 in Algorithm 1. Other steps can be easily incorporated into
the framework. The matrix multiplication U = A(A(cid:48)X1) can be decomposed into two steps  i.e. 
W = A(cid:48)X1 and U = AW  where A ∈ (cid:60)n×p  X1 ∈ (cid:60)p×k  W ∈ (cid:60)n×k and U ∈ (cid:60)n×k. Divid-
ing matrices A  X evenly into r × c large consecutive blocks like [23] will lead to load imbalance.
First  since the sparse structure of X changes over time (Section 3.1)  large consecutive blocks may
assign dense blocks to some processes and sparse blocks to the other processes. Second  there will
be no blocks in some processes after the multiplication using large blocks since W is a small matrix
compared to A  X  e.g.  p could be millions and n  k are hundreds. Third  large blocks may not be
ﬁt in the cache  leading to cache misses. Therefore  we use block cyclic data distribution which uses
a small nonconsecutive blocks and thus can largely achieve load balance and scalability. A matrix
is ﬁrst divided into consecutive blocks of size pb × nb. Then blocks are distributed into the process

ij is the block of X1 assigned to the ij-th core.

5

(a) Shared-Memory

(b) Distributed-Memory

(c) Block Cyclic

Figure 1: CLIME-ADMM on shared-memory and distribtued-memory architectures.

grid cyclically. Figure 1(c) illustrates how to distribute the matrix to a 2 × 2 process grid. A is
divided into 3× 2 consecutive blocks  where each block is of size pb × nb. Blocks of the same color
will be assigned to the same process. Green blocks will be assigned to the upper left process  i.e. 
(cid:126)A11 = {a11  a13  a31  a33  a51  a53} in Figure 1(b). The distribution of X1 can be done in a similar
way except the block size should be pb × kb  where pb is to guarantee that matrix multiplication
A(cid:48)X1 works. In particular  we denote pb × nb × kb as the block size for matrix multiplication.
To distribute the data in a block cyclic manner  we use a parallel I/O scheme  where processes can
access the data in parallel and only read/write the assigned blocks.
5 Experimental Results
In this section  we present experimental results to compare CLIME-ADMM with existing al-
gorithms and show its scalability.
In all experiments  we use the low rank property of the
sample covariance matrix and do not assume any other special structures. Our algorithm is
implemented in a shared-memory architecture using OpenMP (http://openmp.org/wp/) and a
distributed-memory architecture using OpenMPI (http://www.open-mpi.org) and ScaLAPACK [15]
(http://www.netlib.org/scalapack/).
5.1 Comparision with Existing Algorithms
We compare CLIME-ADMM with three other methods for estimating the inverse covariance matrix 
including CLIME  Tiger in package ﬂare1 and divide and conquer QUIC (DC-QUIC) [13]. The
comparisons are run on an Intel Zeon E5540 2.83GHz CPU with 32GB main memory.
We test the efﬁciency of the above methods on both synthetic and real datasets. For synthetic
datasets  we generate the underlying graphs with random nonzero pattern by the same way as in [14].
We control the sparsity of the underlying graph to be 0.05  and generate random graphs with var-
ious dimension. Since each estimator has different parameters to control the sparsity  we set them
individually to recover the graph with sparsity 0.05  and compare the time to get the solution. The
column block size k for CLIME-ADMM is 100. Figure 2(a) shows that CLIME-ADMM is the most
scalable estimator for large graphs. We compare the precision and recall for different methods on
recovering the groud truth graph structure. We run each method using different parameters (which
controls the sparsity of the solution)  and plot the precision and recall for each solution in Figure
2(b). As Tiger is parameter tuning free and achieves the minimax optimal rate [19]  it achieves the
best performance in terms of recall. The other three methods have the similar performance. CLIME
can also be free of parameter tuning and achieve the optimal minimax rate by solving an additional
linear program which is similar to (1) [4]. We refer the readers to [3  4  19] for detailed comparisons
between the two models CLIME and Tiger  which is not the focus of this paper.
We further test the efﬁciency of the above algorithms on two real datasets  Leukemia and Climate
(see Table 1). Leukemia is gene expression data provided by [10]  and the pre-processing was done
by [17]. Climate dataset is the temperature data in year 2001 recorded by NCEP/NCAR Reanalysis
data2and preprocessed by [13]. Since the ground truth for real datasets are unknown  we test the
time taken for each method to recover graphs with 0.1 and 0.01 sparsity. The results are presented
in Table 1. Although Tiger is faster than CLIME-ADMM on small dimensional dataset Leukemia 

1The interior point method in [3] is written in R and extremely slow. Therefore  we use ﬂare which is

implemented in C with R interface. http://cran.r-project.org/web/packages/ﬂare/index.html

2www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.surface.html

6

2X1X6X5X4X3X8X7XA21A(cid:38)121X(cid:38)11A(cid:38)111X(cid:38)122X(cid:38)22A(cid:38)112X(cid:38)12A(cid:38)211X(cid:38)221X(cid:38)222X(cid:38)222X(cid:38)Parallel IO13A12A14A11A23A22A24A21A33A32A34A31A43A42A44A41A53A52A54A51A63A62A64A61A(a) Runtime

(a) Speedup Scol
k

(a) Speedup Scol
k

(b) Precision and recall

(b) Speedup Score

q

(b) Speedup Score

q

  i.e.  Score

q = T core

1 /T core

q

q

over q cores T core

Figure 3: Shared-Memory.

Figure 4: Distributed-Memory.

Figure 2: Synthetic datasets
it does not scale well on the high dimensional dataset as CLIME-ADMM  which is mainly due
to the fact that ADMM is not competitive with other methods on small problems but has superior
scalability on big datasets [2]. DC-QUIC runs faster than other methods for small sparsity but
dramatically slows down when sparsity increases. DC-QUIC essentially works on a block-diagonal
matrix by thresholding the off-diagonal elements of the sample covariance matrix. A small sparsity
generally leads to small diagonal blocks  which helps DC-QUIC to make a giant leap forward in the
computation. A block-diagonal structure in the sample covariance matrix can be easily incorporated
into the matrix multiplication in CLIME-ADMM to achieve a sharp computational gain. On a single
core  CLIME-ADMM is faster than ﬂare ADMM. We also show the results of CLIME-ADMM on 8
cores  showing CLIME-ADMM achieves a linear speedup (more results will be seen in Section 5.2).
Note Tiger can estimate the spase precision matrix column-by-column in parallel  while CLIME-
ADMM solves CLIME in column-blocks in parallel.
5.2 Scalability of CLIME ADMM
We evaluate the scalability of CLIME-ADMM in a shared memory and a distributed memory ar-
chitecture in terms of two kinds of speedups. The ﬁrst speedup is deﬁned as the time on 1 core
T core
. The second speedup is caused by the use of col-
1
umn blocks. Assume the total time for solving CLIME column-by-column (k = 1) is T col
1   which
is considered as the baseline. The speedup of solving CLIME in column block with size k over a
single column is deﬁned as Scol
k . The experiments are done on synthetic data which is
generated in the same way as in Section 5.1. The number of samples is ﬁxed to be n = 200.
Shared-memory We estimate a precision matrix with p = 104 dimensions on a server with 20
cores and 64G memory. We use OpenMP to parallelize column blocks. We run the algorithm on
different number of cores q = 1  5  10  20  and with different column block size k. The speedup
Scol
is plotted in Figure 3(a)  which shows the results on three different number of cores. When
k ≤ 20  the speedups keep increasing with increasing number of columns k in each block. For
k
k ≥ 20  the speedups are maintained on 1 core and 5 cores  but decreases on 10 and 20 cores. The
total number of columns in the shared-memory is k× q. For a ﬁxed k  more columns are involved in
the computation when more cores are used  leading to more memory consumption and competition
for the usage of shared cache. The speedup Score
is the time
on a single core. The ideal linear speedups are archived on 5 cores for all block sizes k. On 10
cores  while small and medium column block sizes can maintain the ideal linear speedups  the large
column block sizes fail to scale linearly. The failure to achieve a linear speedup propagate to small
and medium column block sizes on 20 cores  although their speedups are larger than large column
block size. As more and more column blocks are participating in the computation  the speed-ups
decrease possibly because of the competition for resources (e.g.  L2 cache) in the shared-memory
environment.

is plotted in Figure 3(b)  where T core

k = T col

1 /T col

1

q

7

Table 1: Comparison of runtime (sec) on real datasets.
sparsity

CLIME-ADMM

DC-QUIC

Dataset
Leukemia
(1255 × 72)
Climate
(10512 × 1464)

0.1
0.01
0.1
0.01

1 core
48.64
44.98

8 cores
6.27
5.83

4.76 hours
4.46 hours

0.6 hours
0.56 hours

93.88
21.59

Tiger
34.56
17.10
10.51 hours > 1 day
2.12 hours > 1 day

ﬂare CLIME

142.5
87.60
> 1 day
> 1 day

Table 2: Effect (runtime (sec)) of using different number of cores in a node with p = 106.
Using one core per node is the most efﬁcient as there is no resource sharing with other cores.

node ×core

100×1
25× 4
200×1
50×4

k = 1
0.56
1.02
0.37
0.74

k = 5
1.26
2.40
0.68
1.44

k = 10
2.59
3.42
1.12
2.33

k = 50
6.98
8.25
3.48
4.49

k = 100
13.97
16.44
6.76
8.33

k = 500
62.35
84.08
33.95
48.20

k = 1000
136.96
180.89
70.59
103.87

q

1

Distributed-memory We estimate a precision matrix with one million dimensions (p = 106)  which
contains one trillion parameters (p2 = 1012). The experiments are run on a cluster with 400 com-
puting nodes. We use 1 core per node to avoid the competition for the resources as we observed in
2 × 2 since p (cid:29) n. The block size
the shared-memory case. For q cores  we use the process grid q
pb×nb×kb for matrix multiplication is 10×10×1 for k ≤ 10 and 10×10×10 for k > 10. Since the
column block CLIME problems are totally independent  we report the speedups on solving a single
column block. The speedup Scol
k is plotted in Figure 4(a)  where the speedups are larger and more
stable than that in the shared-memory environment. The speedup keeps increasing before arriving
at a certain number as column block size increases. For any column block size  the speedup also
increases as the number of cores increases. The speedup Score
is plotted in Figure 4(b)  where T core
is the time on 50 cores. A single column (k = 1) fails to achieve linear speedups when hundreds of
cores are used. However  if using a column block k > 1  the ideal linear speedups are achieved with
increasing number of cores. Note that due to distributed memory  the larger column block sizes also
scale linearly  unlike in the shared memory setting  where the speedups were limited due to resource
sharing. As we have seen  k depends on the size of process grid  block size in matrix multiplication 
cache size and probably the sparsity pattern of matrices. In Table 2  we compare the performance
of 1 core per node to that of using 4 cores per node  which mixes the effects of shared-memory and
distributed-memory architectures. For small column block size (k = 1  5)  the use of multiple cores
in a node is almost two times slower than the use of a single core in a node. For other column block
sizes  it is still 30% slower. Finally  we ran CLIME-ADMM on 400 cores with one node per core
and block size k = 500  and the entire computation took about 11 hours.
6 Conclusions
In this paper  we presented a large scale distributed framework for the estimation of sparse precision
matrix using CLIME. Our framework can scale to millions of dimensions and run on hundreds of
machines. The framework is based on inexact ADMM  which decomposes the constrained optimiza-
tion problem into elementary matrix multiplications and elementwise operations. Convergence rates
for both the objective and optimality conditions are established. The proposed framework solves the
CLIME in column-blocks and uses block cyclic distribution to achieve load balancing. We evaluate
our algorithm on both shared-memory and distributed-memory architectures. Experimental results
show that our algorithm is substantially more scalable than state-of-the-art methods and scales al-
most linearly with the number of cores. The framework presented can be useful for a variety of other
large scale constrained optimization problems and will be explored in future work.
Acknowledgment
H. W. and A. B. acknowledge the support of NSF via IIS-0953274  IIS-1029711  IIS- 0916750 
IIS-0812183  and the technical support from the University of Minnesota Supercomputing Institute.
H. W. acknowledges the support of DDF (2013-2014) from the University of Minnesota. C.-J.H.
and I.S.D was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H also acknowledge
the support of IBM PhD fellowship. P.R. acknowledges the support of NSF via IIS-1149803  DMS-
1264033 and ARO via W911NF-12-1-0390.

8

References
[1] O. Banerjee  L. E. Ghaoui  and A. dAspremont. Model selection through sparse maximum likelihood

estimation for multivariate Gaussian or binary data. JMLR  9:2261–2286  2008.

[2] S. Boyd  E. Chu N. Parikh  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundation and Trends Machine Learning  3(1)  2011.
[3] T. Cai  W. Liu  and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision matrix estimation.

Journal of American Statistical Association  106:594–607  2011.

[4] T. Cai  W. Liu  and H. Zhou. Estimating sparse precision matrix: Optimal rates of convergence and

adaptive estimation. Preprint  2012.

[5] J. Choi. A new parallel matrix multiplication algorithm on distributed-memory concurrent computers. In

High Performance Computing on the Information Superhighway  1997.

[6] J. Dean  G. Corrado  R. Monga  K. Chen  M. Devin  Q. Le  M. Mao  M. Ranzato  A. Senior  P. Tucker 

K. Yang  and A. Y. Ng. Large scale distributed deep networks. In NIPS  2012.

[7] J. Dean and S. Ghemawat. Map-Reduce: simpliﬁed data processing on large clusters. In CACM  2008.
[8] J. Friedman  T. Hastie  and R. Tibshirani. Model selection through sparse maximum likelihood estimation

for multivariate gaussian or binary data. Biostatistics  9:432–441  2008.

[9] Q. Fu  H. Wang  and A. Banerjee. Bethe-ADMM for tree decomposition based parallel MAP inference.

In UAI  2013.

[10] T. R. Golub  D. K. Slonim  P. Tamayo  C. Huard  M. Gaasenbeek  J. P. Mesirov  H. Coller  M. L. Loh 
J. R. Downing  M. A. Caligiuri  and C. D. Bloomﬁeld. Molecular classication of cancer: class discovery
and class prediction by gene expression monitoring. Science  pages 531–537  1999.

[11] K. Goto and R. Van De Geijn. Highperformance implementation of the level-3 BLAS. ACM Transactions

on Mathematical Software  35:1–14  2008.

[12] B. He and X. Yuan. On non-ergodic convergence rate of Douglas-Rachford alternating direction method

of multipliers. Preprint  2012.

[13] C. Hsieh  I. Dhillon  P. Ravikumar  and A. Banerjee. A divide-and-conquer method for sparse inverse

covariance estimation. In NIPS  2012.

[14] C. Hsieh  M. Sustik  I. Dhillon  and P. Ravikumar. Sparse inverse covariance matrix estimation using

quadratic approximation. In NIPS  2011.

[15] A. Cleary J. Demmel I. S. Dhillon J. Dongarra S. Hammarling G. Henry A. Petitet K. Stanley D. Walker

L. Blackford  J. Choi and R.C. Whaley. ScaLAPACK Users’ Guide. SIAM  1997.

[16] M. Lam  E. Rothberg  and M. Wolf. The cache performance and optimization of blocked algorithms. In

Architectural Support for Programming Languages and Operating Systems  1991.

[17] L. Li and K.-C. Toh. An inexact interior point method for L1-reguarlized sparse covariance selection.

Mathematical Programming Computation  2:291–315  2010.

[18] X. Li  T. Zhao  X. Yuan  and H. Liu. An R package ﬂare for high dimensional linear regression and

precision matrix estimation. http://cran.r-project.org/web/packages/ﬂare  2013.

[19] H. Liu and L. Wang. Tiger: A tuning-insensitive approach for optimally estimating Gaussian graphical

models. Preprint  2012.

[20] Y. Low  J. Gonzalez  A. Kyrola  D. Bickson  C. Guestrin  and J. Hellerstein. Distributed graphlab: A

framework for machine learning in the cloud. In VLDB  2012.

[21] R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for large-scale

graphical lasso. JMLR  13:723–736  2012.

[22] F. Niu  B. Retcht  C. Re  and S. J. Wright. Hogwild! a lock-free approach to parallelizing stochastic

gradient descent. In NIPS  2011.

[23] N. Parikh and S. Boyd. Graph projection block splitting for distributed optimization. Preprint  2012.
[24] R. Raina  A. Madhavan  and A. Y. Ng. Large-scale deep unsupervised learning using graphics processors.

In ICML  2009.

[25] P. Ravikumar  M. J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance estimation by
minimizing l1-penalized log-determinant divergence. Electronic Journal of Statistics  5:935–980  2011.

[26] H. Wang and A. Banerjee. Online alternating direction method. In ICML  2012.
[27] J. Yang and Y. Zhang. Alternating direction algorithms for L1-problems in compressive sensing. ArXiv 

2009.

[28] M. Yuan. Sparse inverse covariance matrix estimation via linear programming. JMLR  11  2010.
[29] M. Zinkevich  M. Weimer  A. Smola  and L. Li. Parallelized stochastic gradient descent. In NIPS  2010.

9

,Huahua Wang
Arindam Banerjee
Cho-Jui Hsieh
Pradeep Ravikumar
Inderjit Dhillon