2019,Nonstochastic Multiarmed Bandits with Unrestricted Delays,We investigate multiarmed bandits with delayed feedback  where the delays need neither be identical nor bounded. We first prove that "delayed" Exp3 achieves the $O(\sqrt{(KT + D)\ln K})$ regret bound conjectured by Cesa-Bianchi et al. [2016] in the case of variable  but bounded delays. Here  $K$ is the number of actions and $D$ is the total delay over $T$ rounds. We then introduce a new algorithm that lifts the requirement of bounded delays by using a wrapper that skips rounds with excessively large delays. 
The new algorithm maintains the same regret bound  but similar to its predecessor requires prior knowledge of $D$ and $T$. 
For this algorithm we then construct a novel doubling scheme that forgoes the prior knowledge requirement under the assumption that the delays are available at action time (rather than at loss observation time). This assumption is satisfied in a broad range of applications  including interaction with servers and service providers. 
The resulting oracle regret bound is of order $\min_\beta (|S_\beta|+\beta \ln K + (KT + D_\beta)/\beta)$  where $|S_\beta|$ is the number of observations with delay exceeding $\beta$  and $D_\beta$ is the total delay of observations with delay below $\beta$. The bound relaxes to $O(\sqrt{(KT + D)\ln K})$  but we also provide examples where $D_\beta \ll D$ and the oracle bound has a polynomially better dependence on the problem parameters.,Nonstochastic Multiarmed Bandits

with Unrestricted Delays

Tobias Sommer Thune∗
University of Copenhagen

Copenhagen  Denmark

tobias.thune@di.ku.dk

Nicolò Cesa-Bianchi

DSRC & Univ. degli Studi di Milano

Milan  Italy

nicolo.cesa-bianchi@unimi.it

Yevgeny Seldin

University of Copenhagen

Copenhagen  Denmark

seldin@di.ku.dk

Abstract

We investigate multiarmed bandits with delayed feedback  where the delays need
neither be identical nor bounded. We ﬁrst prove that "delayed" Exp3 achieves the

O(cid:0)(cid:112)(KT + D) ln K(cid:1) regret bound conjectured by Cesa-Bianchi et al. [2019]

in the case of variable  but bounded delays. Here  K is the number of actions
and D is the total delay over T rounds. We then introduce a new algorithm that
lifts the requirement of bounded delays by using a wrapper that skips rounds with
excessively large delays. The new algorithm maintains the same regret bound  but
similar to its predecessor requires prior knowledge of D and T . For this algorithm
we then construct a novel doubling scheme that forgoes the prior knowledge
requirement under the assumption that the delays are available at action time (rather
than at loss observation time). This assumption is satisﬁed in a broad range of
applications  including interaction with servers and service providers. The resulting
oracle regret bound is of order minβ
is the number of observations with delay exceeding β  and Dβ is the total delay of
we also provide examples where Dβ (cid:28) D and the oracle bound has a polynomially
better dependence on the problem parameters.

(cid:0)|Sβ| + β ln K + (KT + Dβ)/β(cid:1)  where |Sβ|
observations with delay below β. The bound relaxes to O(cid:0)(cid:112)(KT + D) ln K(cid:1)  but

1

Introduction

Multiarmed bandits is an algorithmic paradigm for sequential decision making with a growing
range of industrial applications  including content recommendation  computational advertising 
and many more. In the multiarmed bandit framework an algorithm repeatedly takes actions (e.g. 
recommendation of content to a user) and observes outcomes of these actions (e.g.  whether the
user engaged with the content)  whereas the outcome of alternative actions (e.g.  alternative content
that could have been recommended) remains unobserved. In many real-life situations the algorithm
experience delay between execution of an action and observation of its outcome. Within the delay
period the algorithm may be forced to make a series of other actions (e.g.  interact with new users)
before observing the outcomes of all the previous actions. This setup falls outside of the classical
multiarmed bandit paradigm  where observations happen instantaneously after the actions  and
motivates the study of bandit algorithms that are provably robust in the presence of delays.

∗Part of this work was done while visiting Università degli Studi di Milano  Milan  Italy

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We focus on the nonstochastic (a.k.a. oblivious adversarial) bandit setting  where the losses faced
by the algorithm are generated by an unspeciﬁed deterministic mechanism. Though it might be of
adversarial intent  the mechanism is oblivious to internal randomization of the algorithm. In the
delayed version  the loss of an action executed at time t is observed at time t+dt  where the delay dt is
also chosen deterministically and obliviously. Thus  at time step t the algorithm receives observations
from time steps s ≤ t for which s + ds = t. This delay is the independent of the action chosen.
The algorithm’s performance is evaluated by regret  which is the difference between the algorithm’s
cumulative loss and the cumulative loss of the best static action in hindsight. The regret deﬁnition is
the same as in the ordinary setting without delays. When all the delays are constant (dt = d for all t) 

the number of actions [Cesa-Bianchi et al.  2019]. Remarkably  this bound is achieved by “delayed”
Exp3  which is a minor modiﬁcation of the standard Exp3 algorithm performing updates as soon as
the losses become available.
The case of variable delays has previously been studied in the full information setting by Joulani et al.
t=1 dt is the total delay.
Their proof is based on a generic reduction from delayed full information feedback to full information
with no delay. The applicability of this technique to the bandit setting is unclear (see Appendix A).

the optimal regret is known to scale as O(cid:0)(cid:112)(K + d)T ln K(cid:1)  where T is the time horizon and K is
[2016]. They prove a regret bound of order(cid:112)(D + T ) ln K  where D =(cid:80)T
Cesa-Bianchi et al. [2019] conjecture an upper bound of order(cid:112)(KT + D) ln K for the bandit
the lower bound Ω(cid:0)(cid:112)(K + d)T(cid:1)  which holds for any d. In a recent paper  Li et al. [2019] study
and dmax  Li et al. [2019] prove a regret bound of (cid:101)O(cid:0)(cid:112)dmaxK(T + D)(cid:1). Cesa-Bianchi et al. [2018]
in the last dmax rounds. In this setting Cesa-Bianchi et al. [2018] obtain an O(cid:0)√
dmaxKT ln K(cid:1)
der(cid:112)(KT + D)/(ln K)  "delayed" Exp3 achieves the conjectured bound of O(cid:0)(cid:112)(KT + D) ln K(cid:1).
β that attains the desired O(cid:0)(cid:112)(KT + D) ln K(cid:1) regret bound with "delayed" Exp3 wrapped within

regret bound (which is tight to within the ln K factor  and in fact tighter than the bound of Li et al.
[2019] for an easier problem).
Our paper is structured in the following way. We start by investigating the regret of Exp3 in the vari-
able delay setting. We prove that for known T   D  and dmax  and assuming that dmax is at most of or-

In order to remove the restriction on dmax and eliminate the need of its knowledge we introduce a
wrapper algorithm  Skipper. Skipper prevents the wrapped bandit algorithm from making updates
using observations with delay exceeding a given threshold β. This threshold acts as a tunable upper
bound on the delays observed by the underlying algorithm  so if T and D are known we can choose

setting with variable delays. Note that this bound cannot be improved in the general case because of

a harder variant of bandits  where the delays dt remain unknown. As a consequence  if an action
is played at time s and then more times in between time steps s and s + ds  the learner cannot tell
which speciﬁc round the loss observed at time s + ds refers to. In this harder setting  for known T   D 

further study an even harder setting of bandits with anonymous composite feedback. In this setting at
time step t the learner observes feedback  which is a composition of partial losses of the actions taken

Skipper.
To dispense of the need for knowing T and D  the ﬁrst approach coming to mind is the doubling
trick. However  applying the standard doubling to D is problematic  because the event that the actual
total delay d1 + ··· + dt exceeds an estimate D is observed at time t + dt rather than at time t. In
order to address this issue  we consider a setting in which the algorithm observes the delay dt at time
t rather than at time t + dt. To distinguish between this setting and the previous one we say that "the
delay is observed at action time" if it is observed at time t and "the delay is observed at observation
time" if it is observed at time t + dt. Observing the delay at action time is motivated by scenarios in
which a learning agent depends on feedback from a third party  for instance a server or laboratory
that processes the action in order to evaluate it. In such cases  the third party might partially control
the delay  and provide the agent with a delay estimate based on contingent and possibly private
information. In the server example the delay could depend on workload  while the laboratory might
have processing times and an order backlog. Other examples include medical imaging where the
availability of annotations depends on medical professionals work schedule. Common for these
examples is that the third party knows the delay before the action is taken.
Within the "delay at action time" setting we achieve a much stronger regret bound. We show that
Skipper wrapping delayed Exp3 and combined with a carefully designed doubling trick enjoys an
implicit regret bound of order minβ
of observations with delay below β. This bound is attained without any assumptions on the sequence

(cid:0)|Sβ| + β ln K + (KT + Dβ)/β(cid:1)  where Dβ is the total delay

2

Table 1: Spectrum of delayed feedback settings and the corresponding regret bounds  progressing
from easier to harder settings. Results marked by (*) have matching lower bounds up to the
ln K
factor. If all the delays are identical  then D = dT and (**) has a lower bound following from
Cesa-Bianchi et al. [2019] and matching up to the
ln K factor. However  for non-identical delays
the regret can be much smaller  as we show in Example 8.

√

√

Regret Bound

O(cid:0)(cid:112)(K + d)T ln K(cid:1)
O(cid:16)
(cid:16)|Sβ| + β ln K + KT +Dβ
O(cid:0)(cid:112)(KT + D) ln K(cid:1)
dmaxKT ln K(cid:1)
O(cid:0)√

minβ

(**)

(*)

(*)

β

Setting
Fixed delay

Delay at action time
Delay at observation
time with known T  D
Anonymous  composite
with known dmax

(cid:17)(cid:17)

Reference
Cesa-Bianchi et al. [2019]

This paper
This paper

Cesa-Bianchi et al. [2018]

an explicit bound of O(cid:0)(cid:112)(KT + D) ln K(cid:1)  however if Dβ (cid:28) D it can be much tighter. We provide

of delays dt and with no need for prior knowledge of T and D. The implicit bound can be relaxed to

an instance of such a problem in Example 8  where we get a polynomially tighter bound.
Table 1 summarizes the spectrum of delayed feedback models in the bandit case and places our results
in the context of prior work.

1.1 Additional related work

Online learning with delays was pioneered by Mesterharm [2005] — see also [Mesterharm  2007 
Chapter 8]. More recent work in the full information setting include [Zinkevich et al.  2009  Quanrud
and Khashabi  2015  Ghosh and Ramchandran  2018]. The theme of large or unbounded delays in
the full information setting was also investigated by Mann et al. [2018] and Garrabrant et al. [2016].
Other related approaches are the works by Shamir and Szlak [2017]  who use a semi-adversarial
model  and Chapelle [2014]  who studies the role of delays in the context of onlne advertising.
Chapelle and Li [2011] perform an empirical study of the impact of delay in bandit models. This is
extended in [Mandel et al.  2015]. The analysis of Exp3 in a delayed setting was initiated by Neu
et al. [2014]. In the stochastic case  bandit learning with delayed feedback was studied in [Dudík
et al.  2011  Vernade et al.  2017]. The results were extended to the anonymous setting by Pike-Burke
et al. [2018] and by Garg and Akash [2019]  and to the contextual setting by Arya and Yang [2019].

2 Setting and notation

We consider an oblivious adversarial multiarmed bandit setting  where K sequences of losses are
generated in an arbitrary way prior to the start of the game. The losses are denoted by (cid:96)a
t   where t
indexes the game rounds and a ∈ {1  . . .   K} indexes the sequences. We assume that all losses are in
the [0  1] interval. We use the notation [K] = {1  . . .   K} for brevity. At each round of the game the
learner picks an action At and suffers the loss of that action. The loss (cid:96)At
is observed by the learner
t
after dt rounds  where the sequence of delays d1  d2  . . . is determined in an arbitrary way before the
game starts. Thus  at round t the learner observes the losses of prior actions As for which s + ds = t.
We assume that the losses are observed "at the end of round t"  after the action At has been selected.
We consider two different settings for receiving information about the delays dt:
Delay available at observation time The delay dt is observed when the feedback (cid:96)At
t

arrives at the

end of round t + dt. This corresponds to the feedback being timestamped.

Delay available at action time The delay dt is observed at the beginning of round t  prior to select-

ing the action At.

The following learning protocol provides a formal description of our setting.

3

Protocol for bandits with delayed feedback
For t = 1  2  . . .

3. Pairs(cid:0)s  (cid:96)As

1. If delay is available at action time  then dt ≥ 0 is revealed to the learner
2. The learner picks an action At ∈ {1  . . .   K} and suffers the loss (cid:96)At

(cid:1) for all s ≤ t such that s + ds = t are observed

t ∈ [0  1]

s

We measure the performance of the learner by her expected regret ¯RT   which is deﬁned as the
difference between the expected cumulative loss of the learner and the loss of the best static strategy
in hindsight:

(cid:35)

(cid:34) T(cid:88)

t=1

T(cid:88)

t=1

¯RT = E

(cid:96)At
t

− min

a

(cid:96)a
t .

This regret deﬁnition is the same as the one used in the standard multiarmed bandit setting without
delay.

3 Delay available at observation time: Algorithms and results

This section deals with the ﬁrst of our two settings  namely when delays are observed together
with the losses. We ﬁrst introduce a modiﬁed version of "delayed" Exp3  which we name Delayed
Exponential Weights (DEW) and which is capable of handling variable delays. We then introduce a
wrapper algorithm  Skipper  which ﬁlters out excessively large delays. The two algorithms also
serve as the basis for the next section  where we provide yet another wrapper for tuning the parameters
of Skipper.

3.1 Delayed Exponential Weights (DEW)

DEW is an extension of the standard exponential weights approach to handle delayed feedback. The
algorithm  laid out in Algorithm 1  performs an exponential update using every individual feedback
as it arrives  which means that between each prediction either zero  one  or multiple updates might
occur. The algorithm assumes that the delays are bounded and that an upper bound dmax ≥ maxt dt
on the delays is known.

Algorithm 1: Delayed exponential weights (DEW)
Input : Learning rate η; upper bound on the delays dmax
Truncate the learning rate: η(cid:48) = min{η  (4edmax)−1};
Initialize wa
for t = 1  2  . . . do
wa
b wb

0 = 1 for all a ∈ [K];
t−1(cid:80)
for a ∈ [K];

t =

t−1

Let pa
Draw an action At ∈ [K] according to the distribution pt and play it;
Observe feedback (s  (cid:96)As
Update wa

s ) for all {s : s + ds = t} and construct estimators ˆ(cid:96)a

(cid:16)−η(cid:48)(cid:80)

(cid:17)

;

t = wa

t−1 exp

ˆ(cid:96)a
s

s:s+ds=t

s = (cid:96)a

s

1(a=As)

pa
s

;

end

The following theorem provides a regret bound for Algorithm 1. The bound is a generalization of a
similar bound in Cesa-Bianchi et al. [2019].
Theorem 1. Under the assumption that an upper bound on the delays dmax is known  the regret of
Algorithm 1 with a learning rate η against an oblivious adversary satisﬁes

(cid:26) ln K

η

¯RT ≤ max

(cid:27)

(cid:18) KT e

2

(cid:19)

+ D

 

  4edmax ln K

+ η

4

where D =(cid:80)T

t=1 dt. In particular  if T and D are known and η =

(cid:115)(cid:18) KT e

2

(cid:19)

¯RT ≤ 2

+ D

ln K.

(cid:113) ln K

KT e

2 +D

≤ 1

4edmax

  we have

(1)

The proof of Theorem 1 is based on proving the stability of the algorithm across rounds. The proof is
sketched out in Section 5. As Theorem 1 shows  Algorithm 1 performs well if dmax is small and we
also have preliminary knowledge of dmax  T   and D. However  a single delay of order T increases
dmax up to order T   which leads to a linear regret bound in Theorem 1. This is an undesired property 
which we address with the skipping scheme presented next.

3.2 Skipping scheme

We introduce a wrapper for Algorithm 1  called Skipper  which disregards feedback from rounds
with excessively large delays. The regret in the skipped rounds is trivially bounded by 1 (because the
losses are assumed to be in [0  1]) and the rounds are taken out of the analysis of the regret of DEW.
Skipper operates with an externally provided threshold β and skips all rounds where dt ≥ β. The
advantage of skipping is that it provides a natural upper bound on the delays for the subset of rounds
processed by DEW  dmax = β. Thus  we eliminate the need of knowledge of the maximal delay
in the original problem. The cost of skipping is the number of skipped rounds  denoted by |Sβ|  as
captured in Lemma 2. Below we provide a regret bound for the combination of Skipper and DEW.

Algorithm 2: Skipper
Input : Threshold β; Algorithm A.
for t = 1  2  . . . do

Get prediction At from A and play it;
Observe feedback (s  (cid:96)As

end

s ) for all {s : s + ds = t}  and feed it to A for each s with ds < β;

Lemma 2. The expected regret of Skipper with base algorithm A and threshold parameter β
satisﬁes

¯RT ≤ |Sβ| + ¯RT\Sβ  

(2)
where |Sβ| is the number of skipped rounds (those for which dt ≥ β) and ¯RT\Sβ is a regret bound
for running A on the subset of rounds [T ]\Sβ (those  for which dt < β).
A proof of the lemma is found in Appendix C. When combined with the previous analysis for DEW 
Lemma 2 gives us the following regret bound.
Theorem 3. The expected regret of Skipper(β  DEW(η  β)) against an oblivious adversary satisﬁes

(cid:26) ln K

(cid:27)

(cid:18) KT e

(cid:19)

  4eβ ln K

+ η

+ Dβ

 

(3)

where Dβ =(cid:80)

t /∈Sβ

¯RT ≤ |Sβ| + max

2
dt is the cumulative delay experienced by DEW.

η

Proof. Theorem 1 holds for parameters (η  β) for DEW run under Skipper. We then apply Lemma 2.

Corollary 4. Assume that T and D are known and take

Then the expected regret of Skipper(β  DEW(η  β)) against an oblivious adversary satisﬁes

(cid:115)

η =

¯RT ≤ 2

1
4eβ

 

β =

(cid:115)(cid:18) KT e

2

eKT /2+D

+ D

.

4e
4e ln K

(cid:19)

+ (1 + 4e)D

ln K.

5

Proof. Note that D ≥ β|Sβ| ⇒ |Sβ| ≤ D
and substituting the values of η and β we obtain the result.

β . By substituting this into (3)  observing that Dβ ≤ D 

Note that Corollary 4 recovers the regret scaling in Theorem 1  equation (1) within constant factors
in front of D without the need of knowledge of dmax. Similar to Theorem 1  Corollary 4 is tight in
the worst case. The tuning of β still requires the knowledge of T and D. In the next section we get
rid of this requirement.

4 Delay available at action time: Oracle tuning and results

This section deals with the second setting  where the delays are observed before taking an action.
The combined algorithm introduced in the previous section relies on prior knowledge of T and D
for tuning the parameters. In this section we eliminate this requirement by leveraging the added
information about the delays at the time of action. The information is used in an implicit doubling
scheme for tuning Skipper’s threshold parameter β. Additionally  the new bound scales with the
experienced delay Dβ rather than the full delay D and is signiﬁcantly tighter when Dβ (cid:28) D. This
is achieved through direct optimization of the regret bound in terms of |Sβ| and Dβ  as opposed to
Corollary 4  which tunes β using the potentially loose inequality |Sβ| ≤ D/β.

4.1 Setup

Let m index the epochs of the doubling scheme. In each epoch we restart the algorithm with new
parameters and continually monitor the termination condition in equation (6). The learning rate
within epoch m is set to ηm = 1
  where βm is the threshold parameter of the epoch. Theorem 3
provides a regret bound for epoch m denoted by

4eβm

Boundm(βm) := |Sm

βm

| + 4eβm ln K +

σ(m)eK/2 + Dm
βm

 

4eβm
are  respectively  the number of

(4)

where σ(m) denotes the length of epoch m and |Sm
skipped rounds and the experienced delay within epoch m.
Let ωm = 2m. In epoch m we set

βm

| and Dm

βm

and we stay in epoch m as long as the following condition holds:

max

|Sm

βm

|2 

+ Dm
βm

ln K

(cid:26)

√

ωm
4e ln K

βm =

(cid:18) eKσ(m)

2

(cid:19)

(cid:27)

≤ ωm.

(5)

(6)

Since dt is observed at the beginning of round t  we are able to evaluate condition (6) and start a
new epoch before making the selection of At. This provides the desired tuning of βm for all rounds
without the need of a separate treatment of epoch transition points.
While being more elaborate  this doubling scheme maintains the intuition of standard approaches.
First of all  the condition for doubling (6) ensures that the regret bound in each period is optimized
by explicitly balancing the contribution of each term in equation (4). Secondly  the geometric
progression of the tuning (5) —and thus of the resulting regret bounds— means that the total regret
bound summed over the epochs can be bounded in relation to the bound in the ﬁnal completed epoch.
In the following we refer to the doubling scheme deﬁned by (5) and (6) as Doubling.

4.2 Results

The following results show that the proposed doubling scheme works as well as oracle tuning of β
when the learning rate is ﬁxed at η = 1/4eβ. We ﬁrst compare our performance to the optimal tuning
in a single epoch  where we let

β∗
m = arg min

βm

Boundm(βm)

(7)

be the minimizer of (4).

6

Lemma 5. The regret bound (4) for any non-ﬁnal epoch m  with the epochs and βm controlled by
Doubling satisﬁes

√
Boundm(βm) ≤ 3

ωm ≤ 3 Boundm(β∗

m) + 2e2K ln K + 1.

(8)

The lemma is the main machinery of the analysis of Doubling and its proof is provided in Appendix C.
Applying it to Skipper(β  DEW(η β)) leads to the following main result.
Theorem 6. The expected regret of Skipper(β  DEW(η  β)) tuned by Doubling satisﬁes for any T

¯RT ≤ 15 min

β

|Sβ| + 4eβ ln K +

KT + Dβ

4eβ

+ 10e2K ln K + 5.

(cid:26)

(cid:27)

The proof of Theorem 6 is based on Lemma 5 and is provided in Appendix C.
Corollary 7. The expected regret of Skipper(β  DEW(η  β)) tuned by Doubling can be relaxed for
any T to

(cid:115)(cid:18) KT e

2

¯RT ≤ 30

(cid:19)

+ (1 + 4e)D

ln K + 10e2K ln K + 5.

(9)

Proof. The ﬁrst term in the bound of Theorem 6 can be directly bounded using Corollary 4.

Note that both Theorem 6 and Corollary 7 require no knowledge of T and D.

4.3 Comparison of the oracle and explicit bounds

We ﬁnish the section with a comparison of the oracle bound in Theorem 6 and the explicit bound in
Corollary 7. Ignoring the constant and additive terms  the bounds are

explicit

: O(cid:16)(cid:112)(KT + D) ln K

(cid:17)

(cid:18)

(cid:26)

 
|Sβ| + β ln K +

oracle

: O

min

β

(cid:27)(cid:19)

.

KT + Dβ

β

Note that the oracle bound is always as strong as the explicit bound. There are  however  cases where
it is much tighter. Consider the following example.

Example 8. For t < (cid:112)KT / ln K let dt = T − t and for t ≥ (cid:112)KT / ln K let dt = 0. Take
β =(cid:112)KT / ln K. Then D = Θ(T(cid:112)KT / ln K)  but Dβ = 0 (assuming that T ≥ K ln K) and
|Sβ| <(cid:112)KT / ln K. The corresponding regret bounds are

(cid:18)(cid:113)
: O(cid:16)√

: O

√

(cid:19)
= O(cid:0)T 1/2(cid:1).

KT

(cid:17)

KT ln K + T

KT ln K

= O(cid:0)T 3/4(cid:1) 

explicit

oracle

5 Analysis of Algorithm 1

This section contains the main points of the analysis of Algorithm 1 leading to the proof of Theorem 1
which were postponed from Section 3. Full proofs are found in Appendix B.
The analysis is a generalization of the analysis of delayed Exp3 in Cesa-Bianchi et al. [2019]  and
consists of a general regret analysis and two stability lemmas.

5.1 Additional notation
We let Nt = |{s : s+ds ∈ [t  t+dt)}| denote the stability-span of t  which is the amount of feedback
that arrives between playing action At and observing its feedback. Note that letting N = maxt Nt
we have N ≤ 2 maxt dt ≤ 2dmax  since this may include feedback from up to maxs ds rounds prior
to round t and up to dt rounds after round t.

7

We introduce Z = (z1  ...  zT ) to be a permutation of [T ] = {1  ...  T} sorted in ascending order
according to the value of z + dz with ties broken randomly  and let Ψi = (z1  ...  zi) be its ﬁrst
i elements. Similarly  we also introduce Z(cid:48)
) as an enumeration of {s : s + ds ∈
[t  t + dt)}.
For a subset the integers C  corresponding to timesteps  we also introduce

t = (z(cid:48)

1  ...  z(cid:48)

Nt

(cid:17)

(cid:16)−η(cid:48)(cid:80)
(cid:16)−η(cid:48)(cid:80)

exp

(cid:80)

b exp

s∈C

ˆ(cid:96)a
s

s∈C

ˆ(cid:96)b
s

qa(C) =

(cid:17) .

(10)

The nominator and denominator in the above expression will also be denoted by wa(C) and W (C)
corresponding to the deﬁnition of pa
t .
By ﬁnally letting Ct−1 = {s : s + ds < t} we have pa

t = qa(Ct−1).

5.2 Analysis of delayed exponential weights

The starting point is the following modiﬁcation of the basic lemma within the Exp3 analysis that
takes care of delayed updates of the weights.
Lemma 9. Algorithm 1 satisﬁes

T(cid:88)

K(cid:88)

t=1

a=1

(cid:88)

t

T(cid:88)

K(cid:88)

t=1

a=1

(cid:17)2

(cid:16)ˆ(cid:96)a

t

pa
t+dt

t − min
ˆ(cid:96)a
a∈[K]

t ≤ ln K
ˆ(cid:96)a

η(cid:48) +

η
2

pa
t+dt

.

(11)

To make use of Lemma 9  we need to ﬁgure out the relationship between pa
t . This is
achieved by the following two lemmas  which are generalizations and reﬁnements of Lemmas 1 and
2 in Cesa-Bianchi et al. [2019].
Lemma 10. When using Algorithm 1 the resulting probabilities fulﬁl for every t and a

and pa

t+dt

t ≥ −η(cid:48) Nt(cid:88)

qa(cid:0)Ct−1 ∪ {z(cid:48)

j : j < i}(cid:1) ˆ(cid:96)a

z(cid:48)

i

− pa

pa
t+dt

 

(12)

where z(cid:48)

j is an enumeration of {s : s + ds ∈ [t  t + dt)}.

i=1

The above lemma allows us to bound pa
to upper bound the probability  which is captured in the second probability drift lemma.
Lemma 11. The probabilities deﬁned by (10) satisfy for any i

from below in terms of pa

t+dt

t . We similarly need to be able

(cid:18)

1 +

1

2N − 1

(cid:19)

qa(Ψi) ≤

5.3 Proof sketch of Theorem 1

qa(Ψi−1).

(13)

By using Lemma 10 to bound the left hand side of (11) we have

(cid:88)

(cid:88)

t

a

(cid:88)

t

pa
t

t − min
ˆ(cid:96)a

a

t ≤ ln K
ˆ(cid:96)a

η(cid:48) +

η(cid:48)
2

T(cid:88)
K(cid:88)
(cid:88)
+ η(cid:48)(cid:88)

a=1

t=1

pa
t+dt

Nt(cid:88)

ˆ(cid:96)a
t

(cid:16)ˆ(cid:96)a
(cid:17)2
qa(cid:0)Ct−1 ∪ {z(cid:48)

t

t

a

i=1

j : j < i}(cid:1) ˆ(cid:96)a

.

z(cid:48)

i

Repeated use of Lemma 11 bounds the second term on the right hand side by η(cid:48)T Ke/2 in expectation.
The third term on the right hand side can be bounded by D. Taking the maximum over the two
possible values of the truncated learning rate ﬁnishes the proof.

8

6 Discussion

achieves the O(cid:0)(cid:112)(KT + D) ln K(cid:1) regret bound conjectured by Cesa-Bianchi et al. [2019]. The

We have presented an algorithm for multiarmed bandits with variably delayed feedback  which

algorithm is based on a procedure for skipping rounds with excessively large delays and reﬁned
analysis of the exponential weights algorithm with delayed observations. At the moment the skipping
procedure requires prior knowledge of T and D for tuning the skipping threshold. However  if the
delay information is available "at action time"  as in the examples described in the introduction  we
provide a sophisticated doubling scheme for tuning the skipping threshold that requires no prior
knowledge of T and D. Furthermore  the reﬁned tuning also leads to a reﬁned regret bound of order

(cid:1)(cid:1)  which is polynomially tighter when Dβ (cid:28) D. We provide

(cid:0)|Sβ| + β ln K + KT +Dβ

O(cid:0) minβ

β

an example of such a problem in the paper.
Our work leads to a number of interesting research questions. The main one is whether the two regret
bounds are achievable when the delays are available "at observation time" without prior knowledge
of D and T . Alternatively  is it possible to derive lower bounds demonstrating the impossibility
of further relaxation of the assumptions? More generally  it would be interesting to have reﬁned
lower bounds for problems with variably delayed feedback. Another interesting direction is a design
of anytime algorithms  which do not rely on the doubling trick. Such algorithms can be used  for
example  for achieving simultaneous optimality in stochastic and adversarial setups [Zimmert and
Seldin  2019a]. While a variety of anytime algorithms is available for non-delayed bandits  the
extension to delayed feedback does not seem trivial. Some of these questions are addressed in a
follow-up work by Zimmert and Seldin [2019b].

Acknowledgments

Nicolò Cesa-Bianchi gratefully acknowledges partial support by the Google Focused Award Al-
gorithms and Learning for AI (ALL4AI) and by the MIUR PRIN grant Algorithms  Games  and
Digital Markets (ALGADIMAR). Yevgeny Seldin acknowledges partial support by the Independent
Research Fund Denmark  grant number 9040-00361B.

References
Sakshi Arya and Yuhong Yang. Randomized allocation with nonparametric estimation for contextual

multi-armed bandits with delayed rewards. arXiv preprint  arXiv:1902.00819  2019.

Nicolò Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. Nonstochastic bandits with composite
anonymous feedback. In Proceedings of the International Conference on Computational Learning
Theory (COLT)  2018.

Nicolo Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. Delay and cooperation in nonstochastic

bandits. The Journal of Machine Learning Research  20(1):613–650  2019.

Olivier Chapelle. Modeling delayed feedback in display advertising. In Proceedings of the Interna-

tional Conference on Knowledge Discovery and Data Mining (ACM SIGKDD)  2014.

Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in

Neural Information Processing Systems (NeurIPS)  2011.

Miroslav Dudík  Daniel J. Hsu  Satyen Kale  Nikos Karampatziakis  John Langford  Lev Reyzin  and
Tong Zhang. Efﬁcient optimal learning for contextual bandits. In Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence  2011.

Siddhant Garg and Aditya Kumar Akash. Stochastic bandits with delayed composite anonymous

feedback. arXiv preprint  arXiv:1910.01161  2019.

Scott Garrabrant  Nate Soares  and Jessica Taylor. Asymptotic convergence in online learning with

unbounded delays. arXiv preprint  arXiv:1604.05280  2016.

Avishek Ghosh and Kannan Ramchandran. Online scoring with delayed information: A convex
optimization viewpoint. In the proceedings of the Annual Allerton Conference on Communication 
Control  and Computing (Allerton)  2018.

9

Pooria Joulani  Andras Gyorgy  and Csaba Szepesvári. Delay-tolerant online convex optimization:
Uniﬁed analysis and adaptive-gradient algorithms. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence  2016.

Bingcong Li  Tianyi Chen  and Georgios B. Giannakis. Bandit online learning with unknown delays.
In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
2019.

Travis Mandel  Yun-En Liu  Emma Brunskill  and Zoran Popovi´c. The queue method: Handling
delay  heuristics  prior data  and evaluation in bandits. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence  2015.

Timothy A Mann  Sven Gowal  Ray Jiang  Huiyi Hu  Balaji Lakshminarayanan  and Andras Gyorgy.
Learning from delayed outcomes with intermediate observations. arXiv preprint  arXiv:1807.09387 
2018.

Chris Mesterharm. On-line learning with delayed label feedback. In Proceedings of the International

Conference on Algorithmic Learning Theory (ALT)  2005.

Chris Mesterharm. Improving Online Learning. PhD thesis  Department of Computer Science 

Rutgers University  2007.

Gergely Neu  Andras Gyorgy  Csaba Szepesvari  and Andras Antos. Online markov decision

processes under bandit feedback. IEEE Transactions on Automatic Control  59(3)  2014.

Ciara Pike-Burke  Shipra Agrawal  Csaba Szepesvari  and Steffen Grünewälder. Bandits with delayed 
aggregated anonymous feedback. In Proceedings of the International Conference on Machine
Learning (ICML)  2018.

Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. In Advances in Neural

Information Processing Systems (NeurIPS)  2015.

Ohad Shamir and Liran Szlak. Online learning with local permutations and delayed feedback. In

Proceedings of the International Conference on Machine Learning (ICML)  2017.

Claire Vernade  Olivier Cappé  and Vianney Perchet. Stochastic bandit models for delayed conversions.

In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence  2017.

Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
2019a.

Julian Zimmert and Yevgeny Seldin. An optimal algorithm for adversarial bandits with arbitrary

delays. arXiv preprint  arXiv:1910.06054  2019b.

Martin Zinkevich  John Langford  and Alex J Smola. Slow learners are fast. In Advances in Neural

Information Processing Systems (NeurIPS)  2009.

10

,Tobias Sommer Thune
Nicolò Cesa-Bianchi
Yevgeny Seldin