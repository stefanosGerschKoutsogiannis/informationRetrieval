2013,Embed and Project: Discrete Sampling with Universal Hashing,We consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set  specified for instance by a graphical model. We propose a sampling algorithm  called PAWS  based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods. Our scheme can leverage fast combinatorial optimization tools as a blackbox and  unlike MCMC methods  samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatorial search tools  PAWS can efficiently sample from Ising grids with strong interactions and from software verification instances  while MCMC and variational methods fail in both cases.,Embed and Project:

Discrete Sampling with Universal Hashing

Stefano Ermon  Carla P. Gomes

Dept. of Computer Science

Cornell University

Ithaca NY 14853  U.S.A.

Ashish Sabharwal

Bart Selman

IBM Watson Research Ctr.

Dept. of Computer Science

Yorktown Heights
NY 10598  U.S.A.

Cornell University

Ithaca NY 14853  U.S.A.

Abstract

We consider the problem of sampling from a probability distribution deﬁned over
a high-dimensional discrete set  speciﬁed for instance by a graphical model. We
propose a sampling algorithm  called PAWS  based on embedding the set into
a higher-dimensional space which is then randomly projected using universal
hash functions to a lower-dimensional subspace and explored using combinatorial
search methods. Our scheme can leverage fast combinatorial optimization tools
as a blackbox and  unlike MCMC methods  samples produced are guaranteed to
be within an (arbitrarily small) constant factor of the true probability distribution.
We demonstrate that by using state-of-the-art combinatorial search tools  PAWS
can efﬁciently sample from Ising grids with strong interactions and from software
veriﬁcation instances  while MCMC and variational methods fail in both cases.

1

Introduction

Sampling techniques are one of the most widely used approaches to approximate probabilistic rea-
soning for high-dimensional probability distributions where exact inference is intractable. In fact 
many statistics of interest can be estimated from sample averages based on a sufﬁciently large num-
ber of samples. Since this can be used to approximate #P-complete inference problems  sampling is
also believed to be computationally hard in the worst case [1  2].

Sampling from a succinctly speciﬁed combinatorial space is believed to much harder than searching
the space. Intuitively  not only do we need to be able to ﬁnd areas of interest (e.g.  modes of the
underlying distribution) but also to balance their relative importance. Typically  this is achieved
using Markov Chain Monte Carlo (MCMC) methods. MCMC techniques are a specialized form
of local search that only allows moves that maintain detailed balance  thus guaranteeing the right
occupation probability once the chain has mixed. However  in the context of hard combinatorial
spaces with complex internal structure  mixing times are often exponential. An alternative is to
use complete or systematic search techniques such as Branch and Bound for integer programming 
DPLL for SATisﬁability testing  and constraint and answer-set programming (CP & ASP)  which
are preferred in many application areas  and have witnessed a tremendous success in the past few
decades. It is therefore a natural question whether one can construct sampling techniques based on
these more powerful complete search methods rather than local search.

Prior work in cryptography by Bellare et al. [3] showed that it is possible to uniformly sample
witnesses of an NP language leveraging universal hash functions and using only a small number
of queries to an NP-oracle. This is signiﬁcant because samples can be used to approximate #P-
complete (counting) problems [2]  a complexity class believed to be much harder than NP. Practical
algorithms based on these ideas were later developed [4–6] to near-uniformly sample solutions of
propositional SATisﬁability instances  using a SAT solver as an NP-oracle. However  unlike SAT 

1

most models used in Machine Learning  physics  and statistics are weighted (represented  e.g.  as
graphical models) and cannot be handled using these techniques.

We ﬁll this gap by extending this approach  based on hashing-based projections and NP-oracle
queries  to the weighted sampling case. Our algorithm  called PAWS  uses a form of approximation
by quantization [7] and an embedding technique inspired by slice sampling [8]  before applying
projections. This parallels recent work [9] that extended similar ideas for unweighted counting to
the weighted counting world  addressing the problem of discrete integration. Although in theory
one could use that technique to produce samples by estimating ratios of discrete integrals [1  2] 
the general sampling-by-counting reduction requires a large number of such estimates (proportional
to the number of variables) for each sample. Further  the accuracy guarantees on the sampling
probability quickly become loose when taking ratios of estimates. In contrast  PAWS is a more
direct and practical sampling approach  providing better accuracy guarantees while requiring a much
smaller number of NP-oracle queries per sample.

Answering NP-oracle queries  of course  requires exponential time in the worst case  in accordance
with the hardness of sampling. We rely on the fact that combinatorial search tools  however  are
often extremely fast in practice  and any complete solver can be used as a black box in our sampling
scheme. Another key advantage is that when combinatorial search succeeds  our analysis provides a
certiﬁcate that  with high probability  any samples produced will be distributed within an (arbitrarily
small) constant factor of the desired probability distribution. In contrast  with MCMC methods it
is generally hard to assess whether the chain has mixed. We empirically demonstrate that PAWS
outperforms MCMC as well as variational methods on hard synthetic Ising Models and on a real-
world test case generation problem for software veriﬁcation.

2 Setup and Problem Deﬁnition

We are given a probability distribution p over a (high-dimensional) discrete set X   where the proba-
bility of each item x ∈ X is proportional to a weight function w : X → R+  with R+ being the set
of non-negative real numbers. Speciﬁcally  given x ∈ X   its probability p(x) is given by

p(x) =

w(x)

Z

  Z = Xx∈X

w(x)

where Z is a normalization constant known as the partition function. We assume w is speciﬁed
compactly  e.g.  as the product of factors or in a conjunctive normal form. As our driving example 
we consider the case of undirected discrete graphical models [10] with n = |V | random variables
{xi  i ∈ V } where each xi takes values in a ﬁnite set Xi. We consider a factor graph representation
for a joint probability distribution over elements (or conﬁgurations) x ∈ X = X1 × ··· × Xn:

p(x) =

w(x)

Z

=

1

Z Yα∈I

ψα({x}α).

(1)

This is a compact representation for p(x) based on the weight function w(x) = Qα∈I ψα({x}α) 
deﬁned as the product of potentials or factors ψα : {x}α 7→ R+  where I is an index set and
{x}α ⊆ V the subset of variables factor ψα depends on. For simplicity of exposition  without loss
of generality  we will focus on the case of binary variables  where X = {0  1}n.
We consider the fundamental problem of (approximately) sampling from p(x)  i.e.  designing a
randomized algorithm that takes w as input and outputs elements x ∈ X according to the probability
distribution p. This is a hard computational problem in the worst case. In fact  it is more general
than NP-complete decision problems (e.g.  sampling solutions of a SATisﬁability instance speciﬁed
as a factor graph entails ﬁnding at least one solution  or deciding there is none). Further  samples
can be used to approximate #P-complete problems [2]  such as estimating a marginal probability.

3 Sampling by Embed  Project  and Search

Conceptually  our sampling strategy has three steps  described in Sections 3.1  3.2  and 3.3  resp.
(1) From the input distribution p we construct a new distribution p′ that is “close” to p but more

2

discrete. Speciﬁcally  p′ is based on a new weight function w′ that takes values only in a discrete
set of geometrically increasing weights. (2) From p′  we deﬁne a uniform probability distribution
p′′ over a carefully constructed higher-dimensional embedding of X = {0  1}n. The previous
discretization step allows us to specify p′′ in a compact form  and sampling from p′′ can be seen
to be precisely equivalent to sampling from p′. (3) Finally  we indirectly sample from the desired
distribution p by sampling uniformly from p′′  by randomly projecting the embedding onto a lower-
dimensional subspace using universal hash functions and then searching for feasible states.

The ﬁrst and third steps involve a bounded loss of accuracy  which we can trade off with computa-
tional efﬁciency by setting hyper-parameters of the algorithm. A key advantage is that our technique
reduces the weighted sampling problem to that of solving one MAP query (i.e.  ﬁnding the most likely
state) and a polynomial number of feasibility queries (i.e.  ﬁnding any state with non-zero probabil-
ity) for the original graphical model augmented (through an embedding) with additional variables
and carefully constructed factors. In practice  we use a combinatorial optimization package  which
requires exponential time in the worst case (consistent with the hardness of sampling) but is often fast
in practice. Our analysis shows that whenever the underlying combinatorial search and optimization
queries succeed  the samples produced are guaranteed  with high probability  to be coming from an
approximately accurate distribution.

3.1 Weight Discretization

We use a geometric discretization of the weights into “buckets”  i.e.  a uniform discretization of the
log-probability. As we will see  Θ(n) buckets are sufﬁcient to preserve accuracy.
Deﬁnition 1. Let M = maxx w(x)  r > 1  ǫ > 0  and ℓ = ⌈logr(2n/ǫ)⌉. Partition the con-
ﬁgurations into the following weight based disjoint buckets: Bi = {x | w(x) ∈ (cid:0) M
ri(cid:3)}  i =
0  . . .   ℓ − 1 and Bℓ = {x | w(x) ∈ [0  M
rℓ ]}. The discretized weight function w′ : {0  1}n → R+ is
deﬁned as follows: w′(x) = M
ri+1 if x ∈ Bi for i < ℓ and w′(x) = 0 if x ∈ Bℓ. The corresponding
discretized probability distribution p′(x) = w′(x)/Z ′ where Z ′ is the normalization constant.
Lemma 1. Let ρ = r2/(1 − ǫ). For all x ∈ ∪l−1
i=0Bℓ  p(x) and p′(x) are within a factor of ρ of each
other. Furthermore Px∈Bℓ p(x) ≤ ǫ.
Proof. Since w maps to non-negative values  we have Z ≥ M. Further 
ǫM
Z Xx∈Bℓ
Z ≤

This proves the second part of the claim. For the ﬁrst part  note that by construction  Z ′ ≤ Z and

rℓ = |Bℓ|

ǫM
Z ≤ ǫ.

1
Z |Bℓ|

Xx∈Bℓ

w(x) ≤

ri+1   M

p(x) =

2n

M

1

Z ′ =

ℓ

Xi=0 Xx∈Bi

w′(x) ≥

ℓ−1

Xi=0 Xx∈Bi

1
r

w(x) =

1

r Z − Xx∈Bℓ

w(x)! ≥ (1 − ǫ)Z.

1
ρ

p(x) ≤

w(x)
rZ ′ ≤

w(x)
rZ ≤

Thus Z and Z ′ are within a factor of r/(1−ǫ) of each other. For all x such that w(x) /∈ Bn  recalling
that r > 1 > 1 − ǫ and that w(x)/r ≤ w′(x) ≤ rw(x)  we have

w′(x)
Z ′ = p′(x) =

r2
1 − ǫ
This ﬁnishes the proof that p(x) and p′(x) are within a factor of ρ of each other.
Remark 1. If the weights w deﬁned by the original graphical model are represented in ﬁnite preci-
sion (e.g.  there are 264 possible weights in double precision ﬂoating point)  for every b ≥ 1 there
is a possibly large but ﬁnite value of ℓ (such that M/rℓ is smaller than the smallest representable
weight) such that Bℓ is empty and the discretization error ǫ is effectively zero.
3.2 Embed: From Weighted to Uniform Sampling

w′(x)
Z ′ ≤

rw(x)
Z ′ ≤

= ρp(x).

w(x)

Z

We now show how to reduce the problem of sampling from the discrete distribution p′ (weighted
sampling) to the problem of uniformly sampling  without loss of accuracy  from a higher-
dimensional discrete set into which X = {0  1}n is embedded. This is inspired by slice sampling [8] 
and can be intuitively understood as its discrete counterpart where we uniformly sample points (x  y)
from a discrete representation of the area under the (y vs. x) probability density function of p′ .

3

w(x) ≤

M
ri ⇒

M

k=1 yk

b

_k=1

ℓ−1(cid:1)(cid:12)(cid:12)(cid:12)

1  y2

1  . . .   yb−1

ℓ−1   yb

yk
i   1 ≤ i ≤ ℓ − 1; w(x) >

i may alternatively be thought of as the linear constraintPb

Deﬁnition 2. Let w : X → R+  M = maxx w(x)  and r = 2b/(2b − 1). Then the embedding
S(w  ℓ  b) of X in X × {0  1}(ℓ−1)b is deﬁned as:
rℓ) .
i ≥ 1. Further  let

S(w  ℓ  b) =((cid:0)x  y1
whereWb
k=1 yk
p′′ denote a uniform probability distribution over S(w  ℓ  b) and n′ = n + (ℓ − 1)b.
Given a compact representation of w within a combinatorial search or optimization framework  the
set S(w  ℓ  b) can often be easily encoded using the disjunctive constraints on the y variables.
Lemma 2. Let (x  y) = (x  y1
i.e.  a uniformly sampled element from S(w  ℓ  b). Then x is distributed according to p′.
Informally  given x ∈ Bi and x′ ∈ Bi+1 with i + 1 ≤ l − 1  there are precisely r = 2b/(2b − 1)
times more valid conﬁgurations (x  y) than (x′  y′). Thus x is sampled r times more often than x′.
A formal proof may be found in the Appendix.

ℓ−1) be a sample from p′′ 

2 ···   yb

2 ···   y1

ℓ−1 ···   yb

1  y2

1 ···   yb

1  y1

3.3 Project and Search: Uniform Sampling with Hash Functions and an NP-oracle

In principle  using the technique of Bellare et al. [3] and n′-wise independent hash functions we can
sample purely uniformly from S(w  ℓ  b) using an NP oracle to answer feasibility queries. However 
such hash functions involve constructions that are difﬁcult to implement and reason about in exist-
ing combinatorial search methods. Instead  we use a more practical algorithm based on pairwise
independent hash functions that can be implemented using parity constraints (modular arithmetic)
and still provides accuracy guarantees. The approach is similar to [5]  but we include an algorithmic
way to estimate the number of parity constraints to be used. We also use the pivot technique from
[6] but extend that work in two ways: we introduce a parameter α (similar to [5]) that allows us to
trade off uniformity against runtime and also provide upper bounds on the sampling probabilities.

We refer to our algorithm as PArity-basedWeightedSampler (PAWS) and provide its pseudocode as
Algorithm 1. The idea is to project by randomly constraining the conﬁguration space using a family
of universal hash functions  search for up to P “surviving” conﬁgurations  and then  if fewer than P
survive  perform rejection sampling to choose one of them. The number k of constraints or factors
(encoding a randomly chosen hash function) to add is determined ﬁrst; this is where we depart from
both Gomes et al. [5]  who do not provide a way to compute k  and Chakraborty et al. [6]  who do
not ﬁx k or provide upper bounds. Then we repeatedly add k such constraints  check whether fewer
than P conﬁgurations survive  and if so output one conﬁguration chosen using rejection sampling.
Intuitively  we need the hashed space to contain no more than P solutions because that is a base case
where we know how to produce uniform samples via enumeration. k is a guess (accurate with high
probability) of the number of constraints that is likely to reduce (by hashing) the original problem
to a situation where enumeration is feasible. If too many or too few conﬁgurations survive  the
algorithm fails and is run again. The small failure probability  accounting for a potentially poor
choice of random hash functions  can be bounded irrespective of the underlying graphical model.

A combinatorial optimization procedure is used once in order to determine the maximum weight
M through MAP inference. M is used in the discretization step. Subsequently  several feasibility
queries are issued to the underlying combinatorial search procedure in order to  e.g.  count the
number of surviving conﬁgurations and produce one as a sample.

We brieﬂy review the construction and properties of universal hash functions [11  12].
Deﬁnition 3. H = {h : {0  1}n → {0  1}m} is a family of pairwise independent hash functions
if the following two conditions hold when a function H is chosen uniformly at random from H: 1)
∀x ∈ {0  1}n  the random variable H(x) is uniformly distributed in {0  1}m; 2) ∀x1  x2 ∈ {0  1}n
x1 6= x2  the random variables H(x1) and H(x2) are independent.
Proposition 1. Let A ∈ {0  1}m×n  c ∈ {0  1}m. The family H = {hA c(x) : {0  1}n → {0  1}m}
where hA c(x) = Ax + c mod 2 is a family of pairwise independent hash functions.
Further  H is also known to be a family of three-wise independent hash functions [5].

4

k ← −1 ;

count ← 0

count ← 0

k ← k + 1 ;
for t = 1  · · ·   T do

T ← 24 ⌈ln (n′/δ)⌉ ;
repeat

Algorithm 1 Algorithm PAWS for sampling conﬁgurations σ according to w
1: procedure COMPUTEK(n′  δ  P   S)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end procedure

Sample hash function hk
Let S k t   {(x  y) ∈ S  hk
if |S k t| < P then

until count ≥ ⌈T /2⌉ or k = n′
return k

count ← count + 1

A c : {0  1}n′

A c(x  y) = 0}

→ {0  1}k

end for

/* search for ≥ P different elements */

/* compute with one MAP inference query on w */
/* as in Definition 2 */

→ {0  1}i  i.e.  uniformly choose A ∈ {0  1}i×n′  c ∈ {0  1}i

A c(x  y) = 0}

A c : {0  1}n′

M ← maxx w(x)
S ← S(w  ℓ  b); n′ ← n + b(ℓ − 1)
i ← COMPUTEK(n′  δ  γ  P   S) + α
Sample hash fn. hi
Let S i   {(x  y) ∈ S  hi
Check if |S i| ≥ P by searching for at least P different elements
if |S i| ≥ P or |S i| = 0 then

14: procedure PAWS(w : {0  1}n → R+  ℓ  b  δ  P   α)
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30: end procedure

Fix an arbitrary ordering of S i
Uniformly sample p from {0  1  . . .   P − 1}
if p ≤ |S i| then

Select p-th element (x  y) of S i ;

return ⊥

return ⊥

return x

else

else

/* failure */

/* for rejection sampling */

/* failure */

Lemma 3 (see Appendix for a proof) shows that the subroutine COMPUTEK in Algorithm 1 outputs
with high probability a value close to log(|S(w  ℓ  b)|/P ). The idea is similar to an unweighted
version of the WISH algorithm [9] but with tighter guarantees and using more feasibility queries.
Lemma 3. Let S = S(w  ℓ  b) ⊆ {0  1}n′  δ > 0  and γ > 0. Further  let P ≥ min{2  2γ+2/(2γ −
P = log(Z/P )  and k be the output of procedure COMPUTEK(n′  δ  P S). Then 
1)2}  Z = |S|  k∗
P + 1 + γ] ≥ 1 − δ and COMPUTEK uses O(n′ ln (n′/δ)) feasibility queries.
P[k∗
P − γ ≤ k ≤ k∗
Lemma 4. Let S = S(w  ℓ  b) ⊆ {0  1}n′  δ > 0  P ≥ 2  and γ = log(cid:0)(P + 2√P + 1 + 2)/P(cid:1).
For any α ∈ Z  α > γ  let c(α  P ) = 1 − 2γ−α/(1 − 1
P − 2γ−α)2. Then with probability
at least 1 − δ the following holds: PAWS(w  ℓ  b  δ  P   α) outputs a sample with probability
at least c(α  P )2−(γ+α+1) P
P −1 and  conditioned on outputting a sample  every element (x  y) ∈
S(w  ℓ  b) is selected (Line 27) with probability p′
s(x  y) within a constant factor c(α  P ) of the
uniform probability p′′(x  y) = 1/|S|.
Proof Sketch. For lack of space  we defer details to the Appendix. Brieﬂy  the probability P[σ ∈ S i]
that σ = (x  y) survives is 2−i by the properties of the hash functions in Deﬁnition 3  and the
probability of being selected by rejection sampling is 1/(P − 1). Conditioned on σ surviving  the
mean and variance of the size of the surviving set |S i| are independent of σ because of 3-wise
independence. When k∗
P + 1 + γ and i = k + α  α > γ  on average |S i| < P and
the size is concentrated around the mean. Using Chebychev’s inequality  one can upper bound by
1 − c(α  P ) the probability P[Si ≥ P | σ ∈ S i] that the algorithm fails because |Si| is too large.
Note that the bound is independent of σ and lets us bound the probability ps(σ) that σ is output:

P − γ ≤ k ≤ k∗

c(α  P )

2−i
P − 1

=(cid:18)1 −

2γ−α

P − 2γ−α)2(cid:19) 2−i

P − 1 ≤ ps(σ) ≤

(1 − 1

2−i
P − 1

.

(2)

5

P + 1 + γ + α and summing the lower bound of ps(σ) over all σ  we obtain the
From i = k + α ≤ k∗
desired lower bound on the success probability. Note that given σ  σ′  ps(σ) and ps(σ′) are within
s(σ) (for various σ)
a constant factor c(α  P ) of each other from (2). Therefore  the probabilities p′
that σ is output conditioned on outputting a sample are also within a constant factor of each other.
s(x  y) is within a constant

s(σ) = 1  one gets the desired result that p′

From the normalizationPσ p′
factor c(α  P ) of the uniform probability p′′(x  y) = 1/|S|.
3.4 Main Results: Sampling with Accuracy Guarantees

Combining pieces from the previous three sections  we have the following main result:
Theorem 1. Let w : {0  1}n → R+  ǫ > 0  b ≥ 1  δ > 0  and P ≥ 2. Fix α ∈ Z as in Lemma 4 
r = 2b/(2b−1)  ℓ = ⌈logr(2n/ǫ)⌉  ρ = r2/(1−ǫ)  bucket Bℓ as in Deﬁnition 1  and κ = 1/c(α  P ).
P −1   PAWS(w  ℓ  b 
ThenPx∈Bℓ p(x) ≤ ǫ and with probability at least (1 − δ)c(α  P )2−(γ+α+1) P
δ  P   α) succeeds and outputs a sample σ from {0  1}n \ Bℓ. Upon success  each σ ∈ {0  1}n \ Bℓ
is output with probability p′
s(σ) within a constant factor ρκ of the desired probability p(σ) ∝ w(σ).
Proof. Success probability follows from Lemma 4. For x ∈ {0  1}n \ Bℓ  combining Lemmas 1  2 
4 we obtain

1
ρκ

p(x) ≤

1
κ

p′(x) = Xy:(x y)∈S(w ℓ b)

1
κ

p′
s(x  y) = p′

s(x)

p′′(x  y) ≤ Xy|(x y)∈S(w ℓ b)
≤ Xy:(x y)∈S(w ℓ b)

κp′′(x  y) = κp′(x) ≤ ρκp(x)

where the ﬁrst inequality accounts for discretization error from p(x) to p′(x) (Lemma 1)  equality
s is bounded by Lemma 4. The rest
follows from Lemma 2  and the sampling error between p′′ and p′
is proved in Lemmas 1  2.

Remark 2. By appropriately setting the hyper-parameters b and ℓ we can make the discretization
errors ρ and ǫ arbitrarily small. Although this does not change the number of required feasibility
queries  it can signiﬁcantly increase the runtime of combinatorial search because of the increased
search space size |S(w  ℓ  b)|. Practically  one should set these parameters as large as possible  while
ensuring combinatorial searches can be completed within the available time budget. Increasing pa-
rameter P improves the accuracy as well  but also increases the number of feasibility queries issued 
which is proportional to P (but does not affect the structure of the search space). Similarly  by
increasing α we can make κ arbitrarily small. However  the probability of success of the algorithm
decreases exponentially as α is increased. We will demonstrate in the next section that a practi-
cal tradeoff between computational complexity and accuracy can be achieved for reasonably sized
problems of interest.
Corollary 2. Let w  b  ǫ  ℓ  δ  P  α  and Bℓ be as in Theorem 1  and p′
of PAWS(w  ℓ  b  δ  P   α). Let φ : {0  1}n → R and ηφ = maxx∈Bℓ |φ(x)| ≤ kφk∞. Then 

s(σ) be the output distribution

1
ρκ

Ep′

s [φ] − ǫηφ ≤ Ep[φ] ≤ ρκEp′

s [φ] + ǫηφ

where Ep′

s [φ] can be approximated with a sample average using samples produced by PAWS.

4 Experiments

We evaluate PAWS on synthetic Ising Models and on a real-world test case generation problem for
software veriﬁcation. All experiments used Intel Xeon 5670 3GHz machines with 48GB RAM.

4.1

Ising Grids Models

We ﬁrst consider the marginal computation task for synthetic grid-structured Ising models with
random interactions (attractive and mixed). Speciﬁcally  the corresponding graphical model has n
binary variables xi  i = 1 ···   n  with single node potentials ψi(xi) = exp(fixi) and pairwise

6

i

l

s
a
n
g
r
a
m
d
e

 

t

a
m

i
t
s
E

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
 
0

0.1

0.2

0.3

 

Gibbs
Belief Propagation
WISH
PAWS b=1
PAWS b=2

 

Gibbs
Belief Propagation
WISH
PAWS b=1
PAWS b=2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

i

s
a
n
g
r
a
m
d
e
a
m

 

t

i
t
s
E

0.4

0.5

0.6

True marginals

0.7

0.8

0.9

1

0
 
0.16

0.165

0.17

0.175

True marginals

0.18

0.185

0.19

(a) Mixed (w = 4.0 f = 0.6)

(b) Attractive (w = 3.0 f = 0.45)

Figure 1: Estimated marginals vs. true marginals on 8 × 8 Ising Grid models. Closeness to the 45
degree line indicates accuracy. PAWS is run with b ∈ {1  2}  P = 4  α = 1  and ℓ = 25 (mixed
case) or ℓ = 40 (attractive case).

interactions ψij(xi  xj) = exp(wijxixj)  where fi ∈R [−f  f ] and wij ∈R [−w  w] in the mixed
case  while wij ∈R [0  w] in the attractive case.
Our implementation of PAWS uses the open source solver ToulBar2 [13] to compute M =
maxx w(x) and as an oracle to check the existence of at least P different solutions. We aug-
mented ToulBar2 with the IBM ILOG CPLEX CP Optimizer 12.3 [14] based on techniques bor-
rowed from [15] to efﬁciently reason about parity constraints (the hash functions) using Gauss-
Jordan elimination. We run the subroutine COMPUTEK in Algorithm 1 only once at the beginning 
and then generate all the samples with the same value of i (Line 17). The comparison is with Gibbs
sampling  Belief Propagation  and the recent WISH algorithm [9]. Ground truth is obtained using
the Junction Tree method [16].

In Figure 1  we show a scatter plot of the estimated vs. true marginal probabilities for two Ising
grids with mixed and attractive interactions  respectively  representative of the general behavior in
the large-weights regime. Each sampling method is run for 10 minutes. Marginals computed with
Gibbs sampling (run for about 108 iterations) are clearly very inaccurate (far from the 45 degree
line)  an indication that the Markov Chain had not mixed as an effect of the relatively large weights
that tend to create barriers between modes which are hard to traverse. In contrast  samples from
PAWS provide much more accurate marginals  in part because it does not rely on local search and
hence is not directly affected by the energy landscape (with respect to the Hamming metric). Further 
we see that we can improve the accuracy by increasing the hyper-parameter b. These results highlight
the practical value of having accuracy guarantees on the quality of the samples after ﬁnite amounts
of time vs. MCMC-style guarantees that hold only after a potentially exponential mixing time.

Belief Propagation can be seen from Figure 1 to be quite inaccurate in this large-weights regime. Fi-
nally  we also compare to the recent WISH algorithm [9] which uses similar hash-based techniques
to estimate the partition function of graphical models. Since producing samples with the general
sampling-by-counting reduction [1  2] or estimating each marginal as the ratio of two partition func-
tions (with and without a variable clamped) would be too expensive (requiring n + 1 calls to WISH)
we heuristically run it once and use the solutions of the optimization instances it solves in the inner
loop as samples. We see in Figure 1 that while samples produced by WISH can sometimes produce
fairly accurate marginal estimates  these estimates can also be far from the true value because of an
inherent bias introduced by the arg max operator.

4.2 Test Case Generation for Software Veriﬁcation

Hardware and software veriﬁcation tools are becoming increasingly important in industrial system
design. For example  IBM estimates $100 million savings over the past 10 years from hardware ver-
iﬁcation tools alone [17]. Given that complete formal veriﬁcation is often infeasible  the paradigm
of choice has become that of randomly generating “interesting” test cases to stress the code or chip

7

Instance
bench1039
bench431
bench115
bench97
bench590
bench105

785
410
458
401
527
524

Vars Factors Time (s) MSE (×10−5)
330
173
189
170
244
243

1710
34.97
52.75
67.03
593.71
842.35

5.76
4.35
20.74
45.57
8.11
8.56

 

Theoretical
Sample Frequency

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

y
c
n
e
u
q
e
r
F

0

 
0

100

200

300

400

Solution

500

600

700

800

(a) Marginals: runtime and mean squared error

(b) True vs. observed sampling frequencies.

Figure 2: Experiments on software veriﬁcation benchmark.

with the hope of uncovering bugs. Typically  a model based on hard constraints is used to specify
consistent input/output pairs  or valid program execution traces. In addition  in some systems  do-
main knowledge can be speciﬁed by experts in the form of soft constraints  for instance to introduce
a preference for test cases where operands are zero and bugs are more likely [17].

For our experiments  we focus on software (SW) veriﬁcation  using an industrial benchmark [18]
produced by Microsoft’s SAGE system [19  20]. Each instance deﬁnes a uniform probability distri-
bution over certain valid traces of a computer program. We modify this benchmark by introducing
soft constraints deﬁning a weighted distribution over valid traces  indicating traces that meet certain
criteria should be sampled more often. Speciﬁcally  following Naveh et al. [17] we introduce a pref-
erence towards traces where certain registers are zero. The weight is chosen to be a power of two 
so that there is no loss of accuracy due to discretization using the previous construction with b = 1.
These instances are very difﬁcult for MCMC methods because of the presence of very large regions
of zero probability that cannot be traversed and thus can break the ergodicity assumption. Indeed we
observed that Gibbs sampling often fails to ﬁnd a non-zero probability state  and when it ﬁnds one
it gets stuck there  because there might not be a non-zero probability path from one feasible state
to another. In contrast  our sampling strategy is not affected and does not require any ergodicity
assumption. Table 2a summarizes the results obtained using the propositional satisﬁability (SAT)
solver CryptoMiniSAT [21] as the feasibility query oracle for PAWS. CryptoMiniSAT has built-in
support for parity constraints Ax = c mod 2. We report the time to collect 1000 samples and
the Mean Squared Error (MSE) of the marginals estimated using these samples. We report results
only on the subset of instances where we could enumerate all feasible states using the exact model
counter Relsat [22] in order to obtain ground truth marginals for MSE computation. We see that
PAWS scales to fairly large instances with hundreds of variables and gives accurate estimates of
the marginals. Figure 2b shows the theoretical vs. observed sampling frequencies (based on 50000
samples) for a small instance with 810 feasible states (execution traces)  where we see that the output
distribution p′

s is indeed very close to the target distribution p.

5 Conclusions

We introduced a new approach  called PAWS  to the fundamental problem of sampling from a dis-
crete probability distribution speciﬁed  up to a normalization constant  by a weight function  e.g.  by
a discrete graphical model. While traditional sampling methods are based on the MCMC paradigm
and hence on some form of local search  PAWS can leverage more advanced combinatorial search
and optimization tools as a black box. A signiﬁcant advantage over MCMC methods is that PAWS
comes with a strong accuracy guarantee: whenever combinatorial search succeeds  our analysis
provides a certiﬁcate that  with high probability  the samples are produced from an approximately
correct distribution. In contrast  accuracy guarantees for MCMC methods hold only in the limit  with
unknown and potentially exponential mixing times. Further  the hyper-parameters of PAWS can be
tuned to trade off runtime with accuracy. Our experiments demonstrate that PAWS outperforms
competing sampling methods on challenging domains for MCMC.

8

References
[1] N.N. Madras. Lectures on Monte Carlo Methods. American Mathematical Society  2002.

ISBN 0821829785.

[2] M. Jerrum and A. Sinclair. The Markov chain Monte Carlo method: an approach to approxi-
mate counting and integration. Approximation algorithms for NP-hard problems  pages 482–
520  1997.

[3] Mihir Bellare  Oded Goldreich  and Erez Petrank. Uniform generation of NP-witnesses using

an NP-oracle. Information and Computation  163(2):510–526  2000.

[4] Stefano Ermon  Carla P. Gomes  and Bart Selman. Uniform solution sampling using a con-

straint solver as an oracle. In UAI  pages 255–264  2012.

[5] C.P. Gomes  A. Sabharwal  and B. Selman. Near-uniform sampling of combinatorial spaces

using XOR constraints. In NIPS-2006  pages 481–488  2006.

[6] S. Chakraborty  K. Meel  and M. Vardi. A scalable and nearly uniform generator of SAT

witnesses. In CAV-2013  2013.

[7] Vibhav Gogate and Pedro Domingos. Approximation by quantization. In UAI  pages 247–255 

2011.

[8] Radford M Neal. Slice sampling. Annals of statistics  pages 705–741  2003.
[9] Stefano Ermon  Carla Gomes  Ashish Sabharwal  and Bart Selman. Taming the curse of di-

mensionality: Discrete integration by hashing and optimization. In ICML  2013.

[10] M.J. Wainwright and M.I. Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

[11] S. Vadhan. Pseudorandomness. Foundations and Trends in Theoretical Computer Science 

2011.

[12] O. Goldreich. Randomized methods in computation. Lecture Notes  2011.
[13] D. Allouche  S. de Givry  and T. Schiex. Toulbar2  an open source exact cost function network

solver. Technical report  INRIA  2010.

[14] IBM ILOG. IBM ILOG CPLEX Optimization Studio 12.3  2011.
[15] Carla P. Gomes  Willem Jan van Hoeve  Ashish Sabharwal  and Bart Selman. Counting CSP

solutions using generalized XOR constraints. In AAAI  2007.

[16] Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graph-
ical structures and their application to expert systems. Journal of the Royal Statistical Society.
Series B (Methodological)  pages 157–224  1988.

[17] Yehuda Naveh  Michal Rimon  Itai Jaeger  Yoav Katz  Michael Vinov  Eitan s Marcu  and Gil
Shurek. Constraint-based random stimuli generation for hardware veriﬁcation. AI magazine 
28(3):13  2007.

[18] Clark Barrett  Aaron Stump  and Cesare Tinelli. The Satisﬁability Modulo Theories Library

(SMT-LIB). www.SMT-LIB.org  2010.

[19] Patrice Godefroid  Michael Y Levin  David Molnar  et al. Automated whitebox fuzz testing.

In NDSS  2008.

[20] Patrice Godefroid  Michael Y. Levin  and David Molnar. Sage: Whitebox fuzzing for security

testing. Queue  10(1):20:20–20:27  January 2012. ISSN 1542-7730.

[21] M. Soos  K. Nohl  and C. Castelluccia. Extending SAT solvers to cryptographic problems. In

SAT-2009. Springer  2009.

[22] Robert J Bayardo and Joseph Daniel Pehoushek. Counting models using connected compo-

nents. In AAAI-2000  pages 157–162  2000.

9

,Stefano Ermon
Carla Gomes
Ashish Sabharwal
Bart Selman
Xuezhi Wang
Jeff Schneider
Christopher Dance
Tomi Silander