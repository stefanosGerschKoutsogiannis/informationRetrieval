2013,Exact and Stable Recovery of Pairwise Interaction Tensors,Tensor completion from incomplete observations is a problem of significant practical interest.  However  it is unlikely that there exists an efficient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper  we study the recovery algorithm for pairwise interaction tensors  which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Specifically  in the absence of noise  we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from $O(nr\log^2(n))$ observations.  For the noisy cases  we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition  we apply our algorithm on a temporal collaborative filtering task and obtain state-of-the-art results.,Exact and Stable Recovery of Pairwise Interaction

Tensors

Shouyuan Chen Michael R. Lyu
{sychen lyu king}@cse.cuhk.edu.hk

The Chinese University of Hong Kong

Irwin King

Zenglin Xu

Purdue University

xu218@purdue.edu

Abstract

Tensor completion from incomplete observations is a problem of signiﬁcant prac-
tical interest. However  it is unlikely that there exists an efﬁcient algorithm with
provable guarantee to recover a general tensor from a limited number of obser-
vations. In this paper  we study the recovery algorithm for pairwise interaction
tensors  which has recently gained considerable attention for modeling multiple
attribute data due to its simplicity and effectiveness. Speciﬁcally  in the absence
of noise  we show that one can exactly recover a pairwise interaction tensor by
solving a constrained convex program which minimizes the weighted sum of nu-
clear norms of matrices from O(nr log2(n)) observations. For the noisy cases 
we also prove error bounds for a constrained convex program for recovering the
tensors. Our experiments on the synthetic dataset demonstrate that the recovery
performance of our algorithm agrees well with the theory. In addition  we apply
our algorithm on a temporal collaborative ﬁltering task and obtain state-of-the-art
results.

1

Introduction

Many tasks of recommender systems can be formulated as recovering an unknown tensor (multi-
way array) from a few observations of its entries [17  26  25  21]. Recently  convex optimization
algorithms for recovering a matrix  which is a special case of tensor  have been extensively studied
[7  22  6]. Moreover  there are several theoretical developments that guarantee exact recovery of
most low-rank matrices from partial observations using nuclear norm minimization [8  5]. These
results seem to suggest a promising direction to solve the general problem of tensor recovery.
However  there are inevitable obstacles to generalize the techniques for matrix completion to tensor
recovery  since a number of fundamental computational problems of matrix is NP-hard in their
tensorial analogues [10]. For instance  H˚astad showed that it is NP-hard to compute the rank of a
given tensor [9]; Hillar and Lim proved the NP-hardness to decompose a given tensor into sum of
rank-one tensors even if a tensor is fully observed [10]. The existing evidence suggests that it is
very unlikely that there exists an efﬁcient exact recovery algorithm for general tensors with missing
entries. Therefore  it is natural to ask whether it is possible to identify a useful class of tensors for
which we can devise an exact recovery algorithm.
In this paper  we focus on pairwise interaction tensors  which have recently demonstrated strong
performance in several recommendation applications  e.g. tag recommendation [19] and sequential
data analysis [18]. Pairwise interaction tensors are a special class of general tensors  which directly
model the pairwise interactions between different attributes. Take movie recommendation as an ex-
ample  to model a user’s ratings for movies varying over time  a pairwise interaction tensor assumes
that each rating is determined by three factors: the user’s inherent preference on the movie  the
movie’s trending popularity and the user’s varying mood over time. Formally  pairwise interaction
tensor assumes that each entry Tijk of a tensor T of size n1 × n2 × n3 is given by following
for all (i  j  k) ∈ [n1] × [n2] × [n3] 

Tijk =

  v(a)

(cid:69)

(cid:69)

(cid:68)

(cid:69)

(cid:68)

(cid:68)

(1)

+

u(b)

j

  v(b)
k

+

u(c)
k   v(c)

i

 

u(a)

i

j

1

j }j∈[n2] {v(b)

k }k∈[n3] {v(c)

k }k∈[n3] are r2 di-

i }j∈[n2] are r1 dimensional vectors  {u(b)

i }i∈[n1] are r3 dimensional vectors  respectively. 1

where {u(a)
i }i∈[n1] {v(a)
mensional vectors and {u(c)
The existing recovery algorithms for pairwise interaction tensor use local optimization methods 
which do not guarantee the recovery performance [18  19]. In this paper  we design efﬁcient re-
covery algorithms for pairwise interaction tensors with rigorous guarantee. More speciﬁcally  in the
absence of noise  we show that one can exactly recover a pairwise interaction tensor by solving a
constrained convex program which minimizes the weighted sum of nuclear norms of matrices from
O(nr log2(n)) observations  where n = max{n1  n2  n3} and r = max{r1  r2  r3}. For noisy
cases  we also prove error bounds for a constrained convex program for recovering the tensors.
In the proof of our main results  we reformulated the recovery problem as a constrained matrix
completion problem with a special observation operator. Previously  Gross et al. [8] have showed
that the nuclear norm heuristic can exactly recover low rank matrix from a sufﬁcient number of
observations of an orthogonal observation operator. We note that the orthogonality is critical to their
argument. However  the observation operator  in our case  turns out to be non-orthogonal  which
becomes a major challenge in our proof. In order to deal with the non-orthogonal operator  we have
substantially extended their technique in our proof. We believe that our technique can be generalized
to handle other matrix completion problem with non-orthogonal observation operators.
Moreover  we extend existing singular value thresholding method to develop a simple and scalable
algorithm for solving the recovery problem in both exact and noisy cases. Our experiments on the
synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the
theory. Finally  we apply our algorithm on a temporal collaborative ﬁltering task and obtain state-
of-the-art results.

2 Recovering pairwise interaction tensors

In this section  we ﬁrst introduce the matrix formulation of pairwise interaction tensors and specify
the recovery problem. Then we discuss the sufﬁcient conditions on pairwise interaction tensors
for which an exact recovery would be possible. After that we formulate the convex program for
solving the recovery problem and present our theoretical results on the sample bounds for achieving
an exact recovery. In addition  we also show a quadratically constrained convex program is stable
for the recovery from noisy observations.
A matrix formulation of pairwise interaction tensors. The original formulation of pairwise inter-
action tensors by Rendle et al. [19] is given by Eq. (1)  in which each entry of a tensor is the sum of
inner products of feature vectors. We can reformulate Eq. (1) more concisely using matrix notations.
In particular  we can rewrite Eq. (1) as follows

Tijk = Aij + Bjk + Cki 

(cid:69)

(cid:68)

(cid:68)

(cid:69)

for all (i  j  k) ∈ [n1] × [n2] × [n3] 
u(c)
k   v(c)
u(b)

  and Cki =

  v(b)
k

j

i

(cid:68)

(cid:69)

(2)

u(a)

  v(a)

i

j

  Bjk =

for all (i  j  k).

where we set Aij =
Clearly  matrices A  B and C are rank r1  r2 and r3 matrices  respectively.
We call tensor T ∈ Rn1×n2×n3 a pairwise interaction tensor  which is denoted as T =
Pair(A  B  C)  if T obeys Eq. (2). We note that this concise deﬁnition is equivalent to the original
one. In the rest of this paper  we will exclusively use the matrix formulation of pairwise interaction
tensors.
Recovery problem. Suppose we have partial observations of a pairwise interaction tensor T =
Pair(A  B  C). We write Ω ⊆ [n1] × [n2] × [n3] to be the set of indices of m observed entries. In
this work  we shall assume Ω is sampled uniformly from the collection of all sets of size m. Our goal
is to recover matrices A  B  C and therefore the entire tensor T from exact or noisy observations of
{Tijk}(ijk)∈Ω.
Before we proceed to the recovery algorithm  we ﬁrst discuss when the recovery is possible.
Recoverability: uniqueness. The original recovery problem for pairwise interaction tensors is ill-
posed due to a uniqueness issue. In fact  for any pairwise interaction tensor T = Pair(A  B  C) 

1For simplicity  we only consider three-way tensors in this paper.

2

we can construct inﬁnitely manly different sets of matrices A(cid:48)  B(cid:48)  C(cid:48) such that Pair(A  B  C) =
Pair(A(cid:48)  B(cid:48)  C(cid:48)). For example  we have Tijk = Aij + Bjk + Cki = (Aij + δai) + Bjk + (Cki +
(1 − δ)ai)  where δ (cid:54)= 0 can be any non-zero constant and a is an arbitrary non-zero vector of
size n1. Now  we can construct A(cid:48)  B(cid:48) and C(cid:48) by setting A(cid:48)
jk = Bjk and
ki = Cki + (1 − δ)ai. It is clear that T = Pair(A(cid:48)  B(cid:48)  C(cid:48)).
C(cid:48)
This ambiguity prevents us to recover A  B  C even if T is fully observed  since it is entirely
possible to recover A(cid:48)  B(cid:48)  C(cid:48) instead of A  B  C based on the observations.
In order to avoid
this obstacle  we construct a set of constraints such that  given any pairwise interaction ten-
sor Pair(A  B  C)  there exists unique matrices A(cid:48)  B(cid:48)  C(cid:48) satisfying the constraints and obeys
Pair(A  B  C) = Pair(A(cid:48)  B(cid:48)  C(cid:48)). Formally  we prove the following proposition.
Proposition 1. For any pairwise interaction tensor T = Pair(A  B  C)  there exists unique A(cid:48) ∈
SA  B(cid:48) ∈ SB  C(cid:48) ∈ SC such that Pair(A  B  C) = Pair(A(cid:48)  B(cid:48)  C(cid:48)) where we deﬁne SB = {M ∈
Rn2×n3 : 1T M = 0T} SC = {M ∈ Rn3×n1 : 1T M = 0T} and SA = {M ∈ Rn1×n2 : 1T M =

ij = Aij + δai  B(cid:48)

(cid:16) 1

n2

(cid:17)

1T M1

1T}.

(cid:80)

r

k∈[r] U 2

r

(cid:80)

k∈[r] V 2

jk ≤ µ0.

ik ≤ µ0 and n2

We point out that there is a natural connection between the uniqueness issue and the “bias” compo-
nents  which is a quantity of much attention in the ﬁeld of recommender system [13]. Due to lack
of space  we defer the detailed discussion on this connection and the proof of Proposition 1 to the
supplementary material.
Recoverability: incoherence. It is easy to see that recovering a pairwise tensor T = Pair(A  0  0)
is equivalent to recover the matrix A from a subset of its entries. Therefore  the recovery problem of
pairwise interaction tensors subsumes matrix completion problem as a special case. Previous studies
have conﬁrmed that the incoherence condition is an essential requirement on the matrix in order to
guarantee a successful recovery of matrices. This condition can be stated as follows.
Let M = UΣVT be the singular value decomposition of a rank r matrix M. We call matrix M is
(µ0  µ1)-incoherent if M satisﬁes:
A0. For all i ∈ [n1] and j ∈ [n2]  we have n1
A1. The maximum entry of UVT is bounded by µ1
It is well known the recovery is possible only if the matrix is (µ0  µ1)-incoherent for bounded µ0  µ1
(i.e  µ0  µ1 is poly-logarithmic with respect to n). Since the matrix completion problem is reducible
to the recovery problem for pairwise interaction tensors  our theoretical result will inherit the inco-
herence assumptions on matrices A  B  C.
Exact recovery in the absence of noise. We ﬁrst consider the scenario where the observations are
exact. Speciﬁcally  suppose we are given m observations {Tijk}(ijk)∈Ω  where Ω is sampled from
uniformly at random from [n1]× [n2]× [n3]. We propose to recover matrices A  B  C and therefore
tensor T = Pair(A  B  C) using the following convex program 
n1 (cid:107)Y(cid:107)∗ +
subject to Xij + Yjk + Zki = Tijk 

(cid:112)r/(n1n2) in absolute value.

n2 (cid:107)Z(cid:107)∗
(i  j  k) ∈ Ω 

where (cid:107)M(cid:107)∗ denotes the nuclear norm of matrix M  which is the sum of singular values of M  and
SA  SB  SC is deﬁned in Proposition 1.
We show that  under the incoherence conditions  the above nuclear norm minimization method suc-
cessful recovers a pairwise interaction tensor T when the number of observations m is O(nr log2 n)
with high probability.
Theorem 1. Let T ∈ Rn1×n2×n3 be a pairwise interaction tensor T = Pair(A  B  C) and A ∈
SA  B ∈ SB  C ∈ SC as deﬁned in Proposition 1. Without loss of generality assume that 9 ≤ n1 ≤
n2 ≤ n3. Suppose we observed m entries of T with the locations sampled uniformly at random
from [n1] × [n2] × [n3] and also suppose that each of A  B  C is (µ0  µ1)-incoherent. Then  there
exists a universal constant C  such that if

minimize

X∈SA Y∈SB  Z∈SC

n3 (cid:107)X(cid:107)∗ +

√

√

√

(3)

m > C max{µ2

1  µ0}n3rβ log2(6n3) 

where r = max{rank(A)  rank(B)  rank(C)} and β > 2 is a parameter  the minimizing solution
X  Y  Z for program Eq. (3) is unique and satisﬁes X = A  Y = B  Z = C with probability at
least 1 − log(6n3)6n2−β

3 − 3n2−β

.

3

3

Stable recovery in the presence of noise. Now  we move to the case where the observations are
perturbed by noise with bounded energy. In particular  our noisy model assumes that we observe

for all (i  j  k) ∈ Ω 

ˆTijk = Tijk + σijk 

(4)
where σijk is a noise term  which maybe deterministic or stochastic. We assume σ has bounded
energy on Ω and speciﬁcally that (cid:107)PΩ(σ)(cid:107)F ≤ 1 for some 1 > 0  where PΩ(·) denotes the
restriction on Ω. Under this assumption on the observations  we derive the error bound of the
following quadratically-constrained convex program  which recover T from the noisy observations.
(5)

minimize

√

√

√

n2 (cid:107)Z(cid:107)∗

X∈SA Y∈SB  Z∈SC
subject to

n3 (cid:107)X(cid:107)∗ +

n1 (cid:107)Y(cid:107)∗ +

(cid:13)(cid:13)(cid:13)PΩ(Pair(X  Y  Z)) − PΩ( ˆT )

(cid:13)(cid:13)(cid:13)F

≤ 2.

2rn1n2
2

(1 + 2) 

8β log(n1)

3 − 3n2−β

Theorem 2. Let T = Pair(A  B  C) and A ∈ SA  B ∈ SB  C ∈ SC. Let Ω be the set of
observations as described in Theorem 1. Suppose we observe ˆTijk for (i  j  k) ∈ Ω as deﬁned in
Eq. (4) and also assume that (cid:107)PΩ(σ)(cid:107)F ≤ 1 holds. Denote the reconstruction error of the optimal
solution X  Y  Z of convex program Eq. (5) as E = Pair(X  Y  Z) − T . Also assume that 1 ≤ 2.
Then  we have

(cid:115)
(cid:107)E(cid:107)∗ ≤ 5
with probability at least 1 − log(6n3)6n2−β
The proof of Theorem 1 and Theorem 2 is available in the supplementary material.
Related work. Rendle et al. [19] proposed pairwise interaction tensors as a model used for tag rec-
ommendation. In a subsequent work  Rendle et al. [18] applied pairwise interaction tensors in the
sequential analysis of purchase data. In both applications  their methods using pairwise interaction
tensor demonstrated excellent performance. However  their algorithms are prone to local optimal
issues and the recovered tensor might be very different from its true value. In contrast  our main re-
sults  Theorem 1 and Theorem 2  guarantee that a convex program can exactly or accurately recover
the pairwise interaction tensors from O(nr log2(n)) observations. In this sense  our work can be
considered as a more effective way to recover pairwise interaction tensors from partial observations.
In practice  various tensor factorization methods are used for estimating missing entries of tensors
[12  20  1  26  16]. In addition  inspired by the success of nuclear norm minimization heuristics in
matrix completion  several work used a generalized nuclear norm for tensor recovery [23  24  15].
However  these work do not guarantee exact recovery of tensors from partial observations.

.

3

3 Scalable optimization algorithm

There are several possible methods to solving the optimization problems Eq. (3) and Eq. (5). For
small problem sizes  one may reformulate the optimization problems as semi-deﬁnite programs and
solve them using interior point method. The state-of-the-art interior point solvers offer excellent
accuracy for ﬁnding the optimal solution. However  these solvers become prohibitively slow for
pairwise interaction tensors larger than 100 × 100 × 100. In order to apply the recover algorithms
on large scale pairwise interaction tensors  we use singular value thresholding (SVT) algorithm
proposed recently by Cai et al. [3]  which is a ﬁrst-order method with promising performance for
solving nuclear norm minimization problems.
We ﬁrst discuss the SVT algorithm for solving the exact completion problem Eq. (3). For conve-
nience  we reformulate the original optimization objective Eq. (3) as follows 

minimize

X∈SA Y∈SB  Z∈SC

(cid:107)X(cid:107)∗ + (cid:107)Y(cid:107)∗ + (cid:107)Z(cid:107)∗
Zki√
n2

Yjk√
n1

+

+

subject to Xij√
n3

= Tijk 

(6)

(i  j  k) ∈ Ω 

where we have incorporated coefﬁcients on the nuclear norm terms into the constraints. It is easy
to see that the recovered tensor is given by Pair(n
Z)  where X  Y  Z is the

−1/2
−1/2
1 Y  n
2

−1/2
3 X  n

4

optimal solution of Eq. (6). Our algorithm solves a slightly relaxed version of the reformulated
objective Eq. (6) 

minimize

X∈SA Y∈SB  Z∈SC

τ ((cid:107)X(cid:107)∗ + (cid:107)Y(cid:107)∗ + (cid:107)Z(cid:107)∗) +

1
2

F + (cid:107)Y(cid:107)2

F + (cid:107)Z(cid:107)2

F

(7)

(cid:16)(cid:107)X(cid:107)2

(cid:17)

subject to Xij√
n3

+

Yjk√
n1

+

Zki√
n2

= Tijk 

(i  j  k) ∈ Ω.

It is easy to see that Eq. (7) is closely related to Eq. (6) and the original problem Eq. (3)  as the
relaxed problem converges to the original one as τ → ∞. Therefore by selecting a large value the
parameter τ  a minimizing solution to Eq. (7) nearly minimizes Eq. (3).
Our algorithm iteratively minimizes Eq. (7) and produces a sequence of matrices {Xk  Yk  Zk}
converging to the optimal solution (X  Y  Z) that minimizes Eq. (7). We begin with several def-
initions. For observations Ω = {ai  bi  ci|i ∈ [m]}  let operators PΩA : Rn1×n2 → Rm 
PΩB : Rn2×n3 → Rm and PΩC : Rn3×n1 → Rm represents the inﬂuence of X  Y  Z on the
m observations. In particular 
PΩA (X) =

Ybiciδi  and PΩC (Z) =

Xaibiδi  PΩB (Y) =

m(cid:88)

m(cid:88)

m(cid:88)

Zciaiδi.

1√
n3

i=1

1√
n1

i=1

−1/2
3 X  n
be the adjoint operator of PΩA and similarly deﬁne P∗

It is easy to verify that PΩA (X) + PΩB (Y) + PΩC (Z) = PΩ(Pair(n
We also denote P∗
a matrix X for size n1 × n2  we deﬁne center(X) = X− 1
that removes the mean of each n2 columns  i.e.  1T center(X) = 0T .
Starting with y0 = 0 and k = 1  our algorithm iteratively computes

Z)).
. Finally  for
11T X as the column centering operator

ΩB

ΩA

ΩC

n1

1√
n2
−1/2
−1/2
1 Y  n
and P∗
2

i=1

Step (1). Xk = shrinkA(P∗
Yk = shrinkB(P∗
Zk = shrinkC(P∗

ΩA

ΩB

ΩC

(yk−1)  τ ) 
(yk−1)  τ ) 
(yk−1)  τ ) 
−1/2
3 X  n

Step (2e). ek = PΩ(T ) − PΩ(Pair(n

−1/2
−1/2
1 Y  n
2

Z))

yk = yk−1 + δek.

Here shrinkA is a shrinkage operator deﬁned as follows

shrinkA(M  τ ) (cid:44) arg min
˜M∈SA

1
2

(cid:13)(cid:13)(cid:13) ˜M − M
(cid:13)(cid:13)(cid:13)2

F

(cid:13)(cid:13)(cid:13) ˜M
(cid:13)(cid:13)(cid:13)∗ .

+ τ

(8)

Shrinkage operators shrinkB and shrinkC are deﬁned similarly except they require ˜M belongs SB
and SC  respectively. We note that our deﬁnition of the shrinkage operators shrinkA  shrinkB and
shrinkC are slightly different from that of the original SVT [3] algorithm  where ˜M is unconstrained.
We can show that our constrained version of shrinkage operators can also be calculated using singu-
lar value decompositions of column centered matrices.
Let the SVD of the column centered matrix center(M) be center(M) = UΣVT   Σ =
diag({σi}). We can prove that the shrinkage operator shrinkB is given by

shrinkB(M  τ ) = U diag({σi − τ}+)VT  

(9)
where s+ is the positive part of s  that is  s+ = max{0  s}. Since subspace SC is structurally
identical to SB  it is easy to see that the calculation of shrinkC is identical to that of shrinkB. The
computation of shrinkA is a little more complicated. We have

shrinkA(M  τ ) = U diag({σi − τ}+)VT +

({δ − τ}+ + {δ + τ}−) 11T  
(10)
1T M1 is a constant and s− = min{0  s}
where UΣVT is still the SVD of center(M)  δ = 1√
is the negative part of s. The algorithm iterates between Step (1) and Step (2e) and produces a series
of (Xk  Yk  Zk) converging to the optimal solution of Eq. (7). The iterative procedure terminates

1√
n1n2

n1n2

5

when the training error is small enough  namely (cid:13)(cid:13)ek(cid:13)(cid:13)F ≤ . We refer interested readers to [3] for

a convergence proof of the SVT algorithm.
The optimization problem for noisy completion Eq. (5) can be solved in a similar manner. We only
need to modify Step (2e) to incorporate the quadratical constraint of Eq. (5) as follows

Step (2n).

ek = PΩ( ˆT ) − PΩ(Pair(n

−1/2
−1/2
1 Y  n
2

Z))

(cid:21)

(cid:20) yk

sk

= PK

(cid:21)

+ δ

(cid:21)(cid:19)

(cid:20) ek

−1/2
3 X  n
−

 

sk−1

(cid:18)(cid:20) yk−1
(x  t)

(0  0)

where PΩ( ˆT ) is the noisy observations and the cone projection operator PK can be explicitly com-
puted by

PK : (x  t) →

(cid:107)x(cid:107)+t
2(cid:107)x(cid:107) (x (cid:107)x(cid:107))

if (cid:107)x(cid:107) ≤ t 
if − (cid:107)x(cid:107) ≤ t ≤ (cid:107)x(cid:107)  
if t ≤ −(cid:107)x(cid:107) .

By iterating between Step (1) and Step (2n) and selecting a sufﬁciently large τ  the algorithm gener-
ates a sequence of {Xk  Yk  Zk} that converges to a nearly optimal solution to the noisy completion
program Eq. (5) [3]. We have also included a detailed description of both algorithms in the supple-
mentary material.
At each iteration  we need to compute one singular value decomposition and perform a few elemen-
tary matrix additions. We can see that for each iteration k  Xk vanishes outside of ΩA = {aibi} and
is sparse. Similarly Yk Zk are also sparse matrices. Previously  we showed that the computation of
shrinkage operators requires a SVD of a column centered matrix center(M) − 1
11T X  which is
the sum of a sparse matrix M and a rank-one matrix. Clearly the matrix-vector multiplication of the
form center(M)v can be computed with time O(n + m). This enables the use of Lanczos method
based SVD implementations for example PROPACK [14] and SVDPACKC [2]  which only needs
subroutine of calculating matrix-vector products. In our implementation  we develop a customized
version of SVDPACKC for computing the shrinkage operators. Further  for an appropriate choice
of τ  {Xk  Yk  Zk} turned out to be low rank matrices  which matches the observations in the orig-
inal SVT algorithm [3]. Hence  the storage cost Xk  Yk  Zk can be kept low and we only need to
perform a partial SVD to get the ﬁrst r singular vectors. The estimated rank r is gradually increased
during the iterations using a similar method suggested in [3  Section 5.1.1]. We can see that  in sum 
the overall complexity per iteration of the recovery algorithm is O(r(n + m)).

n1

4 Experiments

Phase transition in exact recovery. We investigate how the number of measurements affects the
success of exact recovery. In this simulation  we ﬁxed n1 = 100  n2 = 150  n3 = 200 and r1 =
r2 = r3 = r. We tested a variety of choices of (r  m) and for each choice of (r  m)  we repeat the
procedure for 10 times. At each time  we randomly generated A ∈ SA  B ∈ SB  C ∈ SC of rank
r. We generated A ∈ SA by sampling two factor matrices UA ∈ Rn1×r  VA ∈ Rn2×r with i.i.d.
standard Gaussian entries and setting A = PSA(UAVT
A)  where PSA is the orthogonal projection
onto subspace SA. Matrices B ∈ SB and C ∈ SC are sampled in a similar way. We uniformly
sampled a subset Ω of m entries and reveal them to the recovery algorithm. We deemed A  B  C
successfully recovered if ((cid:107)A(cid:107)F + (cid:107)B(cid:107)F + (cid:107)C(cid:107)F )−1((cid:107)X − A(cid:107)F + (cid:107)Y − B(cid:107)F + (cid:107)Z − C(cid:107)F ) ≤
10−3  where X  Y and Z are the recovered matrices. Finally  we set the parameters τ  δ of the exact
√
recovery algorithm by τ = 10
Figure 1 shows the results of these experiments. The x-axis is the ratio between the number of
measurements m and the degree of freedom d = r(n1 + n2 − r) + r(n2 + n3 − r) + r(n3 + n1 − r).
Note that a value of x-axis smaller than one corresponds to a case where there is inﬁnite number of
solutions satisfying given entries. The y-axis is the rank r of the synthetic matrices. The color of
each grid indicates the empirical success rate. White denotes exact recovery in all 10 experiments 
and black denotes failure for all experiments. From Figure 1 (Left)  we can see that the algorithm
succeeded almost certainly when the number of measurements is 2.5 times or larger than the degree
of freedom for most parameter settings. We also observe that  near the boundary of m/d ≈ 2.5 
there is a relatively sharp phase transition. To verify this phenomenon  we repeated the experiments 

n1n2n3 and δ = 0.9m(n1n2n3)−1.

6

Figure 1: Phase transition with respect to rank and degree of freedom. Left: m/d ∈ [1  5]. Right:
m/d ∈ [1.5  3.0].

but only vary m/d between 1.5 and 3.0 with ﬁner steps. The results on Figure 1 (Right) shows that
the phase transition continued to be sharp at a higher resolution.
Stability of recovering from noisy data. In this simulation  we show the recovery performance
with respect to noisy data. Again  we ﬁxed n1 = 100  n2 = 150  n3 = 200 and r1 = r2 = r3 = r
and tested against different choices of (r  m). For each choice of (r  m)  we sampled the ground
truth A  B  C using the same method as in the previous simulation. We generated Ω uniformly at
random. For each entry (i  j  k) ∈ Ω  we simulated the noisy observation ˆTijk = Tijk + ijk  where
n. Then  we revealed { ˆTijk}(ijk)∈Ω to
ijk is a zero-mean Gaussian random variable with variance σ2
the noisy recovery algorithm and collect the recovered matrix X  Y  Z. The error of recovery result
is measured by ((cid:107)X − A(cid:107)F +(cid:107)Y − B(cid:107)F +(cid:107)Z − C(cid:107)F )/((cid:107)A(cid:107)F +(cid:107)B(cid:107)F +(cid:107)C(cid:107)F ). We tested the
algorithm with a range of noise levels and for each different conﬁguration of (r  m  σ2
n)  we repeated
the experiments for 10 times and recorded the mean and standard deviation of the relative error.

noise level

0.1
0.2
0.3
0.4
0.5

relative error

0.1020 ± 0.0005
0.1972 ± 0.0007
0.2877 ± 0.0011
0.3720 ± 0.0015
0.4524 ± 0.0015

observations m

m = 3d
m = 4d
m = 5d
m = 6d
m = 7d

relative error

0.1445 ± 0.0008
0.1153 ± 0.0006
0.1015 ± 0.0004
0.0940 ± 0.0007
0.0920 ± 0.0011

rank r

10
20
30
40
50

relative error

0.1134 ± 0.0006
0.1018 ± 0.0007
0.0973 ± 0.0037
0.1032 ± 0.0212
0.1520 ± 0.0344

(a) Fix r = 20  m = 5d and
noise level varies.

(b) Fix r = 20  0.1 noise level
and m varies.

(c) Fix m = 5d  0.1 noise level
and r varies.

Table 1: Simulation results of noisy data.

We present the result of the experiments in Table 1. From the results in Table 1(a)  we can see that
the error in the solution is proportional to the noise level. Table 1(b) indicates that the recovery is not
reliable when we have too few observations  while the performance of the algorithm is much more
stable for a sufﬁcient number of observations around four times of the degree of freedom. Table 1(c)
shows that the recovery error is not affected much by the rank  as the number of observations scales
with the degree of freedom in our setting.
Temporal collaborative ﬁltering. In order to demonstrate the performance of pairwise interaction
tensor on real world applications  we conducted experiments on the Movielens dataset. The Movie-
Lens dataset contains 1 000 209 ratings from 6 040 users and 3 706 movies from April  2000 and
February  2003. Each rating from Movielens dataset is accompanied with time information provided
in seconds. We transformed each timestamp into its corresponding calendar month. We randomly
select 10% ratings as test set and use the rest of the ratings as training set. In the end  we obtained
a tensor T of size 6040 × 3706 × 36  in which the axes corresponded to user  movie and times-
tamp respectively  with 0.104% observed entries as the training set. We applied the noisy recovery
algorithm on the training set. Following previous studies which applies SVT algorithm on movie
recommendation datasets [11]  we used a pre-speciﬁed truncation level r for computing SVD in
each iteration  i.e.  we only kept top r singular vectors. Therefore  the rank of recovered matrices
are at most r.

7

We evaluated the prediction performance in terms of root mean squared error (RMSE). We com-
pared our algorithm with noisy matrix completion method using standard SVT optimization algo-
rithm [3  4] to the same dataset while ignore the time information. Here we can regard the noisy
matrix completion algorithm as a special case of the recover a pairwise interaction tensor of size
6040 × 3706 × 1  i.e.  the time information is ignored. We also noted that the training tensor had
more than one million observed entries and 80 millions total entries. This scale made a number of
tensor recovery algorithms  for example Tucker decomposition and PARAFAC [12]  impractical to
apply on the dataset. In contrast  our recovery algorithm took 2430 seconds to ﬁnish on a standard
workstation for truncation level r = 100.
The experimental result is shown in Figure 2. The empirical result of Figure 2(a) suggests that  by
incorporating the temporal information  pairwise interaction tensor recovery algorithm consistently
outperformed the matrix completion method.
Interestingly  we can see that  for most parameter
settings in Figure 2(b)  our algorithm recovered a rank 2 matrix Y representing the change of movie
popularity over time and a rank 15 matrix Z that encodes the change of user interests over time. The
reason of the improvement on the prediction performance may be that the recovered matrix Y and
Z provided meaningful signal. Finally  we note that our algorithm achieves a RMSE of 0.858 when
the truncation level is set to 50  which slightly outperforms the RMSE=0.861 (quote from Figure 7
of the paper) result of 30-dimensional Bayesian Probabilistic Tensor Factorization (BPTF) on the
same dataset  where the authors predict the ratings by factorizing a 6040 × 3706 × 36 tensor using
BPTF method [26]. We may attribute the performance gain to the modeling ﬂexibility of pairwise
interaction tensor and the learning guarantees of our algorithm.

(a)

(b)

Figure 2: Empirical results on the Movielens dataset. (a) Comparison of RMSE with different trun-
cation levels. MC: Matrix completion algorithm. RPIT: Recovery algorithm for pairwise interaction
tensor. (b) Rank of recovered matrix X  Y  Z. r1 = rank(X)  r2 = rank(Y)  r3 = rank(Z).

5 Conclusion

In this paper  we proved rigorous guarantees for convex programs for recovery of pairwise interac-
tion tensors with missing entries  both in the absence and in the presence of noise. We designed a
scalable optimization algorithm for solving the convex programs. We supplemented our theoretical
results with simulation experiments and a real-world application to movie recommendation. In the
noiseless case  simulations showed that the exact recovery almost always succeeded if the number of
observations is a constant time of the degree of freedom  which agrees asymptotically with the the-
oretical result. In the noisy case  the simulation results conﬁrmed that the stable recovery algorithm
is able to reliably recover pairwise interaction tensor from noisy observations. Our results on the
temporal movie recommendation application demonstrated that  by incorporating the temporal in-
formation  our algorithm outperforms conventional matrix completion and achieves state-of-the-art
results.

Acknowledgments

This work was fully supported by the Basic Research Program of Shenzhen (Project No.
JCYJ20120619152419087 and JC201104220300A)  and the Research Grants Council of the Hong
Kong Special Administrative Region  China (Project No. CUHK 413212 and CUHK 415212).

8

 020406080100 0.860.880.90.920.940.960.981SVD truncation levelRMSEMCRPIT120 20406080100 020406080100120SVD Truncation Levelr1r3r2References
[1] Evrim Acar  Daniel M Dunlavy  Tamara G Kolda  and Morten Mørup. Scalable tensor factorizations for

incomplete data. Chemometrics and Intelligent Laboratory Systems  106(1):41–56  2011.

[2] M Berry et al. Svdpackc (version 1.0) user’s guide  university of tennessee tech. Report (393-194  1993

(Revised October 1996).  1993.

[3] Jian-Feng Cai  Emmanuel J Cand`es  and Zuowei Shen. A singular value thresholding algorithm for matrix

completion. SIAM Journal on Optimization  20(4):1956–1982  2010.

[4] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925–

936  2010.

[5] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational mathematics  9(6):717–772  2009.

[6] A Evgeniou and Massimiliano Pontil. Multi-task feature learning. 2007.
[7] Maryam Fazel  Haitham Hindi  and Stephen P Boyd. A rank minimization heuristic with application to

minimum order system approximation. In American Control Conference  2001  2001.

[8] David Gross  Yi-Kai Liu  Steven T Flammia  Stephen Becker  and Jens Eisert. Quantum state tomography

via compressed sensing. Physical review letters  105(15):150401  2010.

[9] Johan H˚astad. Tensor rank is np-complete. Journal of Algorithms  11(4):644–654  1990.
[10] Christopher Hillar and Lek-Heng Lim. Most tensor problems are np hard. JACM  2013.
[11] Prateek Jain  Raghu Meka  and Inderjit Dhillon. Guaranteed rank minimization via singular value projec-

tion. In NIPS  2010.

[12] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review  51(3):455–

500  2009.

[13] Yehuda Koren  Robert Bell  and Chris Volinsky. Matrix factorization techniques for recommender sys-

tems. Computer  42(8):30–37  2009.

[14] Rasmus Munk Larsen. Propack-software for large and sparse svd calculations. Available online.  2004.
[15] Ji Liu  Przemyslaw Musialski  Peter Wonka  and Jieping Ye. Tensor completion for estimating missing

values in visual data. In ICCV  2009.

[16] Ian Porteous  Evgeniy Bart  and Max Welling. Multi-hdp: A non-parametric bayesian model for tensor

factorization. In AAAI  2008.

[17] Steffen Rendle  Leandro Balby Marinho  Alexandros Nanopoulos  and Lars Schmidt-Thieme. Learning

optimal ranking with tensor factorization for tag recommendation. In SIGKDD  2009.

[18] Steffen Rendle  Christoph Freudenthaler  and Lars Schmidt-Thieme. Factorizing personalized markov

chains for next-basket recommendation. In WWW  2010.

[19] Steffen Rendle and Lars Schmidt-Thieme. Pairwise interaction tensor factorization for personalized tag

recommendation. In ICDM  2010.

[20] Amnon Shashua and Tamir Hazan. Non-negative tensor factorization with applications to statistics and

computer vision. In ICML  2005.

[21] Yue Shi  Alexandros Karatzoglou  Linas Baltrunas  Martha Larson  Alan Hanjalic  and Nuria Oliver.

Tfmap: Optimizing map for top-n context-aware recommendation. In SIGIR  2012.

[22] Nathan Srebro  Jason DM Rennie  and Tommi Jaakkola. Maximum-margin matrix factorization. NIPS 

2005.

[23] Ryota Tomioka  Kohei Hayashi  and Hisashi Kashima. Estimation of low-rank tensors via convex opti-

mization. arXiv preprint arXiv:1010.0789  2010.

[24] Ryota Tomioka  Taiji Suzuki  Kohei Hayashi  and Hisashi Kashima. Statistical performance of convex

tensor decomposition. NIPS  2011.

[25] Jason Weston  Chong Wang  Ron Weiss  and Adam Berenzweig. Latent collaborative retrieval. ICML 

2012.

[26] Liang Xiong  Xi Chen  Tzu-Kuo Huang  Jeff Schneider  and Jaime G Carbonell. Temporal collaborative

ﬁltering with bayesian probabilistic tensor factorization. In SDM  2010.

9

,Shouyuan Chen
Michael Lyu
Irwin King
Zenglin Xu
Pan Zhou
Xiaotong Yuan
Huan Xu
Shuicheng Yan
Jiashi Feng