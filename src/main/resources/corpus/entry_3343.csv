2019,Markov Random Fields for Collaborative Filtering,In this paper  we model the dependencies among the items that are recommended to a user in a collaborative-filtering problem via a Gaussian Markov Random Field (MRF). We build upon Besag's auto-normal parameterization and pseudo-likelihood  which not only enables computationally efficient learning  but also connects the areas of MRFs and sparse inverse covariance estimation with autoencoders and neighborhood models  two successful approaches in collaborative filtering. We propose a novel approximation for learning sparse MRFs  where the trade-off between recommendation-accuracy and training-time can be controlled. At only a small fraction of the training-time compared to various baselines  including deep nonlinear models  the proposed approach achieved competitive ranking-accuracy on all three well-known data-sets used in our experiments  and notably a 20% gain in accuracy on the data-set with the largest number of items.,Markov Random Fields for Collaborative Filtering

Harald Steck

Netﬂix

Los Gatos  CA 95032
hsteck@netflix.com

Abstract

In this paper  we model the dependencies among the items that are recommended
to a user in a collaborative-ﬁltering problem via a Gaussian Markov Random
Field (MRF). We build upon Besag’s auto-normal parameterization and pseudo-
likelihood [7]  which not only enables computationally efﬁcient learning  but also
connects the areas of MRFs and sparse inverse covariance estimation with au-
toencoders and neighborhood models  two successful approaches in collaborative
ﬁltering. We propose a novel approximation for learning sparse MRFs  where
the trade-off between recommendation-accuracy and training-time can be con-
trolled. At only a small fraction of the training-time compared to various baselines 
including deep nonlinear models  the proposed approach achieved competitive
ranking-accuracy on all three well-known data-sets used in our experiments  and
notably a 20% gain in accuracy on the data-set with the largest number of items.

1

Introduction

Collaborative ﬁltering has witnessed signiﬁcant improvements in recent years  largely due to models
based on low-dimensional embeddings  like weighted matrix factorization (e.g.  [26  39]) and deep
learning [23  22  33  47  62  58  20  11]  including autoencoders [58  33]. Also neighborhood-based
approaches are competitive in certain regimes (e.g.  [1  53  54])  despite being simple heuristics based
on item-item (or user-user) similarity matrices (like cosine similarity). In this paper  we outline that
Markov Random Fields (MRF) are closely related to autoencoders as well as to neighborhood-based
approaches. We build on the enormous progress made in learning MRFs  in particular in sparse
inverse covariance estimation (e.g.  [36  59  15  2  60  44  45  63  55  24  25  52  56  51]). Much of
the literature on sparse inverse covariance estimation focuses on the regime where the number of data
points n is much smaller than the number of variables m in the model (n < m).
This paper is concerned with a different regime  where the number n of data-points (i.e.  users) and
the number m of variables (i.e.  items) are both large as well as n > m  which is typical for many
collaborative ﬁltering applications. We use an MRF as to model the dependencies (i.e.  similarities)
among the items that are recommended to a user  while a user corresponds to a sample drawn from
the distribution of the MRF. In this regime (n > m)  learning a sparse model may not lead to
signiﬁcant improvements in prediction accuracy (compared to a dense model). Instead  we exploit
model-sparsity as to reduce the training-time considerably  as computational cost is a main concern
when both n and m are large. To this end  we propose a novel approximation that enables one to trade
prediction-accuracy for training-time. This trade-off subsumes the two extreme cases commonly
considered in the literature  namely regressing each variable against its neighbors in the MRF and
inverting the covariance matrix.
This paper is organized as follows. In the next section  we review Besag’s auto-normal parameteriza-
tion and pseudo-likelihood  and the resulting closed-form solution for the fully-connected MRF. We
then state the key Corollary in Section 2.2.2  which is the basis of our novel sparse approximation
outlined in Section 3. We discuss the connections to various related approaches in Section 4. The

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

empirical evaluation on three well-known data-sets in Section 5 demonstrates the high accuracy
achieved by this approach  while requiring only a small faction of the training-time as well as of the
number of parameters compared to the best competing model.

2 Pseudo-Likelihood

In this section  we lay the groundwork for the novel sparse approximation outlined in Section 3.

2.1 Model Parameterization
In this section  we assume that the graph G of the MRF is given  and each node i ∈ I in G corresponds
to an item in the collaborative-ﬁltering problem. The m = |I| nodes are associated with the (row)
vector of random variables X = (X1  ...  Xm) that follows a zero-mean1 multivariate Gaussian
distribution N (0  Σ).2 A user corresponds to a sample drawn from this distribution. As to make
recommendations  we use the expectation E[Xi|XI\{i} = xI\{i}] as the predicted score for item i
given a user’s observed interactions xI\{i} with all the other items I \ {i}. When learning the MRF 
maximizing the (L1/L2 norm regularized) likelihood of the MRF that is parameterized according to
the Hammersley-Clifford theorem [18] can become computationally expensive  given that the number
of items and users is often large in collaborative ﬁltering applications. For this reason  we use Besag’s
auto-normal parameterization of the MRF [7  8]: the conditional mean of each Xi is parameterized
in terms of a regression against the remaining nodes:

E[Xi|XI\{i} = xI\{i}] =

βj ixj = x · B· i

(1)

(cid:88)

j∈I\{i}

where βj i = 0 if the edge between nodes i and j is absent in G  i.e.  the regression of each node
i involves only its neighbors in G. In the last equality in Eq. 1  we switched to matrix notation 
where B ∈ Rm×m with Bj i := βj i for i (cid:54)= j  and with a zero diagonal  Bi i = 0  as to exclude Xi
from the covariates in the regression regarding its own mean in Eq. 1. B· i denotes the ith column
of B  and x a realization of X regarding a user. Besides B  the vector of conditional variances
i . The fact that
(cid:126)σ2 := (σ2
the covariance matrix Σ is symmetric imposes the constraint σ2
j βj i on the auto-normal
parameterization [7]. Moreover  the positive deﬁniteness of Σ gives rise to an additional constraint 
which in general can only be veriﬁed if the numerical values of the parameters are known [7]. For
computational efﬁciency  we will not explicitly enforce either one of these constraints in this paper.

m) are further model parameters: var(Xi|XI\{i} = xI\{i}) = σ2

i βi j = σ2

1  ...  σ2

2.2 Parameter Fitting

i∈I L(X· i|X· I\{i}; B· i  σ2

likelihoods of the items: L(pseudo)(X|B  (cid:126)σ2) =(cid:80)

Besag’s pseudo-likelihood yields asymptotically consistent estimates when the auto-normal param-
eterization is used [7].3 The log pseudo-likelihood is deﬁned as the sum of the conditional log
i )   where X is the
given user-item-interaction data-matrix X ∈ Rn×m regarding n users and m items. While this
approach allows for a real-valued data-matrix X (e.g.  the duration that a user listened to a song) 
in our experiments in Section 5  following the experimental set-up in [33]  we use a binary matrix
X  where 1 indicates an observed user-item interaction (e.g.  a user listened to a song). X· i denotes
column i of matrix X  while column i is dropped in X· I\{i}. Note that we assume i.i.d. data.
Substituting the Gaussian density function for each (univariate) conditional likelihood  results in
. If the symmetry-constraint
j βj i in previous section) is dropped  the parameters ˆB that maximize this pseudo-

L(pseudo)(X|B  (cid:126)σ2) = −(cid:80)

||X· i − XB· i||2

(cid:110) 1

(cid:111)

i∈I

2 + 1

2 log 2πσ2

i

2σ2
i

i βi j = σ2

(cf. σ2
likelihood also maximize the decoupled pseudo-likelihood

L(decoupled pseudo)(X|B) = −(cid:88)

||X· i − XB· i||2

2 = −||X − XB||2
F  

(2)

i∈I

1In fact  it is possible to drop this common assumption in certain cases  see Appendix.
2We use ’item’  ’node’ and ’random variable’ interchangeably in this paper.

3Interestingly  despite its similarity to Eq. 1  the parameterization Xi =(cid:80)

j∈I\{i} βi jXj + i  where i is

independent Gaussian noise with zero mean and variance σ2

i   does not lead to consistent estimates [7].

2

i ) and wi = 1  in(cid:80)

where || · ||F denotes the Frobenius norm of a matrix. Note that any weighting scheme wi > 0 
2 results in the same optimum ˆB.
including wi = 1/(2σ2
This is obvious from the fact that this sum is optimized by optimizing each column B· i independently
of the other columns  as they are decoupled in the absence of the symmetry constraint. Note that 
unlike the pseudo-likelihood  Eq. 2 becomes independent of the (unknown) conditional variances σ2
i .

i∈I wi||X· i − XB· i||2

2.2.1 Complete Graph

The result for the complete graph is useful for the next section. Starting from Eq. 2  we add L2-norm
regularization with hyper-parameter λ > 0:
||X − XB||2

F where diag(B) = 0

F + λ · ||B||2

(3)

ˆB = arg min
B

where we explicitly re-stated the zero-diagonal constraint  see Section 2.1. The method of Lagrangian
multipliers immediately yields the closed-form solution (see derivation below):

λ

and Sλ = n−1(X(cid:62)X + λ · I) 

ˆB = I − ˆC · dMat(1 (cid:11) diag( ˆC)) where ˆC = S−1

(4)
where I denotes the identity matrix  dMat(·) a diagonal matrix  (cid:11) the elementwise division  and
diag(·) the diagonal of the estimated concentration matrix ˆC  which is the inverse of the L2-norm
regularized empirical covariance matrix Sλ. Note that ˆBi j = − ˆCi j/ ˆCj j for i (cid:54)= j.
Derivation: Eq. 3 can be solved via the method of Lagrangian multipliers: setting the derivative
of the Lagrangian ||X − XB||2
F + 2γ(cid:62) · diag(B) to zero  where γ ∈ Rm is the
vector of Lagrangian multipliers regarding the equality constraint diag(B) = 0  it follows after re-

arranging terms: ˆB = (X(cid:62)X + λ· I)−1(cid:0)X(cid:62)X − dMat(γ)(cid:1) = n−1 ˆC(n ˆC−1 − λ· I− dMat(γ)) =

I − n−1 ˆC · dMat(γ + λ)  where γ is determined by the constraint 0 = diag( ˆB) = diag(I) −
n−1diag( ˆC)(cid:12) (γ + λ)  where (cid:12) denotes the elementwise product. Hence  γ + λ = n(cid:11) diag( ˆC). (cid:3)

F + λ · ||B||2

2.2.2 Subgraphs

The result from the previous section carries immediately over to certain subgraphs:
Corollary: Let D ⊆ I be a subset of nodes that forms a fully connected subgraph in G. Let C be the
Markov blanket of D in graph G such that each j ∈ C is connected to each i ∈ D. Then the non-zero
parameter-estimates in the columns i ∈ D of ˆB based on the pseudo-likelihood are asymptotically
consistent  and given by ˆBj i = − ˆCj i/ ˆCi i for all i ∈ D and j ∈ C ∪ D \ {i}  where the submatrix
of matrix ˆC ∈ R|I|×|I| regarding the nodes C ∪ D is determined by the inverse of the submatrix of
the empirical covariance matrix:4 ˆC[C ∪ D;C ∪ D] = Sλ[C ∪ D;C ∪ D]−1.
Proof: This follows trivially when considering the nodes in D as the so-called dependents and the
nodes in C as the conditioners in the coding technique used in [7]. The estimate in Eq. 4 for the
complete graph carries over to the nodes D  as each i ∈ D is connected to all j ∈ C ∪ D  and D given
the conditioners C is independent of all remaining nodes in graph G. (cid:3)

3 Sparse Approximation

In collaborative-ﬁltering problems with a large number of items  the graph G can be expected to
be (approximately) sparse  where related items form densely connected subgraphs  while items in
different subgraphs are only sparsely connected. An absent edge in graph G is equivalent to a zero
entry in the concentration matrix C [31  36] and in the matrix of regression coefﬁcients B (see Eq. 4).
In our approach  the goal is to trade accuracy for training-time  rather than to learn the ’true’ graph G
and the most accurate parameters at any computational cost. This is important in practical applications 
where recommender systems have to be re-trained regularly under time-constraints as to ingest the
most recent data. To this end  we use model-sparsity as a means for speeding up the training (rather
than for improving accuracy)  as it reduces the number of parameters that need to be learned. The
Corollary outlined above can be used for an approximation where a large number of small submatrices

4When used as indices regarding a (sub-)matrix  a set of indices is used as if it were a list of sorted indices.

3

of the concentration matrix ˆC has to be inverted  each regarding a set of related items D conditioned
on their Markov blanket C. This can be computationally much more efﬁcient (1) compared to
inverting the entire concentration matrix ˆC at once (like in Eq. 4  which can be computationally
expensive if the number of items is large)  or (2) compared to regressing each individual node against
its neighbors as is commonly done in the literature (e.g.  [7  21  36  38]).
Our approximation is comprised of two parts: ﬁrst  the empirical covariance matrix is thresholded
(see next section)  resulting in a sparsity pattern that has to be sufﬁciently sparse as to enable
computationally efﬁcient estimation of the (approximate) regression coefﬁcients in B in the second
part (see Section 3.2). An implementation of the algorithm  and the used values of the hyper-
parameters  are publicly available at https://github.com/hasteck/MRF_NeurIPS_2019.

3.1 Approximate Graph Structure

Numerous approaches for learning the sparse graph structure (and parameters) have been proposed in
recent years  e.g.  [36  59  15  2  60  44  45  63  55  24  25  52  56  51]. Interestingly  simply applying
a threshold to the empirical covariance matrix (in absolute value) [10  9  17  57  35  48  14  61  13]
can recover the same sparsity pattern as the graphical lasso does [59  15  2] under certain assumptions 
regarding the connected components [57  35]  as well as the edges [48  14  61  13] in the graph.
While it may be computationally expensive to verify that the underlying assumptions are met by a
given empirical covariance matrix  the rule of thumb given in [48] is that the assumptions can be
expected to hold if the resulting matrix is ’very’ sparse. For computational efﬁciency  we hence apply
a threshold to Sλ as to obtain a sufﬁciently sparse matrix A ∈ R|I|×|I| reﬂecting the sparsity pattern.
Additionally  we apply an upper limit on the number of non-zero entries per column in A (retaining
the entries with the largest values in Sλ)  as to bound the maximal training-cost of each iteration
(see second to last paragraph in the in the next section). We allow at most 1 000 non-zero entries per
column in our experiments in Section 5  based on the trade-off between training time and prediction
accuracy: a smaller value tends to reduce the training-time  but it might also degrade the prediction
accuracy of the learned sparse model. In Table 2  this threshold actually affects only about 2% of the
items when using the sparsity level of 0.5%  while it has no effect at the sparsity level of 0.1%. Apart
from that  allowing an item to have up to 1 000 similar items (e.g.  songs in the MSD data) seems a
reasonably large number in practice.

3.2 Approximate Parameter-Estimates

In this section  we outline our novel approach for approximate parameter-estimation given the sparsity
pattern A from the previous section. In this approach  the trade-off between approximation-accuracy
and training-time is controlled by the value of the hyper-parameter r ∈ [0  1] used in step 2 below.
Given the sparsity pattern A  we ﬁrst create a list L of all items i ∈ I  sorted in descending order
by each item’s number of neighbors in A  i.e.  number of non-zero entries in column i (ties may be
broken according to the items’ popularities). Our iterative approach is based on this list L  which
gets modiﬁed until it is empty  which marks the end of the iterations. We also use a set S  initialized
to be empty. Each iteration is comprised of the following four steps:
Step 1: We take the ﬁrst element from list L  say item i  and insert it into set S. Then we determine
its neighbors N (i) based on the ith column of the sparsity pattern in matrix A.
Step 2: We now split the set N (i) ∪ {i} into two disjoint sets such that set D(i) contains node
i as well as the m(i) = round(r · |N (i)|) nodes that have the largest empirical covariances with
node i (in absolute value)  where r ∈ [0  1] is the chosen hyper-parameter. The second set is
C(i) := (N (i) ∪ {i}) \ D(i). We now make the key assumption of this approach (and do not verify
it as to save computation time)  namely that C(i) is a Markov blanket of D(i) in the sparse graph G.
Obviously  this is a strong assumption  and cannot be expected to hold in general. It may not be
unreasonable  however  to expect this to be an approximation in the sense that C(i) contains many
nodes of the (actual) Markov blanket of D(i) in graph G for two reasons: (1) if m(i) = 0  then
C(i) = N (i) is indeed the Markov blanket of D(i) = {i}; (2) given that we chose D(i) to contain
the variables with the largest covariances to node i  their Markov blankets likely have many nodes
in common with the Markov blanket of node i. As we increase the value of m(i) ≤ |N (i)|  the

4

approximation-accuracy obviously deteriorates (except for the case that N (i) ∪ {i} is a connected
component in graph G). For these reasons  the value of m(i) (which is controlled by the chosen value
of r) allows one to control the trade-off between approximation accuracy and computational cost.
Step 3: Given set D(i) and its (assumed) Markov blanket C(i)  we now assume that these nodes are
connected as required by the Corollary above. Note that this may assume additional edges to be
present  resulting in additional regression parameters that need to be estimated. Obviously  this is a
further approximation. However  the decrease in statistical efﬁciency can be expected to be rather
small in the typical setting of collaborative ﬁltering  where the number of data points (i.e.  users)
usually is much larger than the number of nodes (i.e.  items) in the (typically small) subset D(i) ∪C(i).
We now can apply the Corollary in Section 2.2.2  and obtain the estimates for all the columns j ∈ D(i)
in matrix ˆB at once. This is the key to the computational efﬁciency of this approach: for about the
same computational cost as estimating the single column i in ˆB  we now obtain the (approximate)
estimates for 1 + m(i) columns (see Section 2.2.2 for details).
Step 4: Finally  we remove all the 1 + m(i) items in D(i) from the sorted list L  and go to step 1
unless L is empty. Obviously  as we increase the value of r (and hence m(i))  the size of list L
decreases by a larger number in each iteration  eventually requiring fewer iterations  which reduces
the training-time. If we choose m(i) = 0  then D(i) = {i}  and there is no computational speed-up
(and also no approximation) compared to the baseline of solving one regression problem per column
in ˆB w.r.t. the pseudo-likelihood.
Upon completion of the iterations  we have estimates for all columns of ˆB. In fact  for many entries
(j  i)  there may be multiple estimates; for instance if node i ∈ D(k) and node j ∈ D(k) ∪ C(k) for
several different nodes k ∈ S. As to aggregate possibly multiple estimates for entry ˆBj i into a single
value  we simply use their average in our experiments.
The computational complexity of this iterative scheme can be controlled by the sparsity level chosen
in Section 3.1  as well as the chosen value r ∈ [0  1]  which determines the values m(i) (see
step 2). When using the Coppersmith-Winograd algorithm for matrix inversion  it is given by
i∈S (1 + |N (i)|)2.376)  where the size of S depends on the chosen values r. Note that the sum
may be dominated by the largest value |N (i)|  which motivated us to cap this value in Section 3.1.
Note that set S can be computed in linear time in |I| by iterating through steps 1  2  and 4 (skipping
step 3). Once S is determined  the computation of step 3 for different i ∈ S is embarrassingly parallel.
In comparison  in the standard approach of separately regressing each item i against its neighbors
i∈I |N (i)|2.376)  i.e.  the sum here extends over all i ∈ I (instead of subset
S ⊆ I only). In the other extreme  inverting the entire covariance matrix incurs the cost O(|I|2.376).
Lacking an analytical error-bound  the accuracy of this approximation may be assessed empirically 
by simply learning ˆB under different choices regarding the sparsity level (see Section 3.1) and the
value r (see step 2 above). Given that recommender systems are re-trained frequently as to ingest the
most recent data  only these regular updates require efﬁcient computations in practice. Additional
models with higher accuracy (and increased training-time) may be learned occasionally as to assess
the accuracy of the models that get trained regularly.

O((cid:80)
N (i)  we have O((cid:80)

4 Related Work

We discuss the connections to various related approaches in this section.
Several non-Gaussian distributions are also covered by Besag’s auto-models [5  6]  including the
logistic auto-model for binary data. Binary data were also considered in [2  42]. While we rely on
the Gaussian distribution for computational efﬁciency  note that  regarding model-ﬁt  Eq. 4 and the
Corollary provide the best least-squares ﬁt of a linear model for any distribution of X  as Eq. 4 is the
solution of Eq. 3. The empirical results in Section 5 corroborate that this is an effective trade-off
between accuracy and training-time.
Sparse Inverse Covariance Estimation has seen tremendous progress beyond the graphical lasso
[59  15  2]. A main focus was computationally efﬁcient optimization of the full likelihood [24 
25  52  44  51  55  45]  often in the regime where n < m (e.g.  [24  52  51])  or the regime of
small data (e.g.  [63  56]). Node-wise regression was considered for structure-learning in [36] and

5

for parameter-learning in [60]  which is along the lines of Besag’s pseudo-likelihood [7  8]. The
pseudo-likelihood was generalized in [34]. Our paper focuses on a different regime  with large n  m
and n > m  as typical for collaborative ﬁltering. In Section 3.2  we outlined a novel kind of sparse
approximation  using set-wise rather than the node-wise regression  which is commonly used in the
literature.
Dependency Networks [21] also regress each node against its neighbors. As this may result in
inconsistencies when learning from ﬁnite data  a kind of Gibbs sampling is used as to obtain a
consistent joint distribution. This increases the computational cost. Given that collaborative ﬁltering
typically operates in the regime of large n  m and n > m  we rely on the asymptotic consistency of
Besag’s pseudo-likelihood for computational efﬁciency.
In SLIM [38]  the objective is similar to Eq. 3  but is comprised of two additional terms: (1) sparsity-
promoting L1-norm regularization and (2) a non-negativity constraint on the learned regression
parameters. As we can see in Table 1  this not only reduces accuracy but also increases training-time 
compared to ˆB(dense). In [38]  also the variant fsSLIM was proposed  where ﬁrst the sparsity pattern
was determined via a k-nearest-neighbor approach  and then a separate regression problem was solved
for each node. This node-wise regression is a special case (for r = 0) of our set-based approximation
outlined in Section 3.2. The variants proposed in [46  32] drop the constraint of a zero diagonal for
computational efﬁciency  which however is an essential property of Besag’s auto-models [7]. The
logistic loss is used in [46]  which requires one to solve a separate logistic regression problem for
each node  which is computationally expensive.
Autoencoders and Deep Learning have led to many improvements in collaborative ﬁltering [23  22 
33  47  62  58  20  11]. In the pseudo-likelihood of the MRF in Eq. 3  the objective is to reproduce
X from X (using B)  like in an autoencoder  cf. also our short paper [50]. However  there is no
encoder  decoder or hidden layer in the MRF in Eq. 3. The learned B is typically of full rank  and the
constraint diag(B) = 0 is essential for generalizing to unseen data. The empirical evidence in Section
5 corroborates that this is a viable alternative to using low-dimensional embeddings  as in typical
autoencoders  as to generalize to unseen data. In fact  recent work on deep collaborative ﬁltering
combines low-rank and full-rank models for improved recommendation accuracy [11]. Moreover 
recent progress in deep learning has also led to full-rank models  like invertible deep networks [27]  as
well as ﬂow-based generative models [16  30  40  12  29]. Adapting these approaches to collaborative
ﬁltering appears to be promising future work in light of the experimental results obtained by the
full-rank shallow model in this paper.
Neighborhood Approaches are typically based on a heuristic item-item (or user-user) similarity
matrix (e.g. cosine similarity)  e.g.  [1  53  54] and references therein. Our approach yields three key
differences to cosine-similarity and the like: (1) a principled way of learning/optimizing the similarity
matrix ˆB from data; (2) Eq. 4 shows that the conceptually correct similarity matrix is not based on (a
re-scaled version of) the covariance matrix  but on its inverse; (3) the similarity matrix is asymmetric
(cf. Eq. 4) rather than symmetric.

5 Experiments

In our experiments  we empirically evaluate the closed-form (dense) solution ˆB(dense) (see Eq. 4)
as well as the sparse approximation outlined in Section 3. We follow the experimental set-up in
[33] and use their publicly available code for reproducibility.5 Three well-known data sets were
used in the experiments in [33]: MovieLens 20 Million (ML-20M) [19]  Netﬂix Prize (Netﬂix) [3] 
and the Million Song Data (MSD) [4]. They were pre-processed and ﬁltered for items and users
with a certain activity level in [33]  resulting in the data-set sizes shown in Table 1. We use all the
approaches evaluated on these three data-sets in [33] as baselines:

• Sparse Linear Method (SLIM) [38] as discussed in Section 4.
• Weighted Matrix Factorization (WMF ) [26  39]: A linear model with a latent representation
of users and items. Variants like NSVD [41] or FISM [28] obtained very similar accuracies.
• Collaborative Denoising Autoencoder (CDAE) [58]: nonlinear model with 1 hidden layer.
5The code regarding ML-20M in [33] is publicly available at https://github.com/dawenl/vae_cf  and

can be modiﬁed for the other two data-sets as described in [33].

6

• Denoising Autoencoder (MULT-DAE) and Variational Autoencoder (MULT-VAE) [33]:
deep nonlinear models  trained w.r.t.
the multinomial likelihood. Three hidden layers
were found to obtain the best accuracy on these data-sets  see Section 4.3 in [33]. Note
that rather shallow architectures are commonly found to obtain the highest accuracy in
collaborative-ﬁltering (which is different from other application areas of deep learning  like
image classiﬁcation  where deeper architectures often achieve higher accuracy).

We do not compare to Neural Collaborative Filtering (NCF)  its extension NeuCF [20] and to Bayesian
Personalized Ranking (BPR) [43]  as their accuracies were found to be below par on the three data-
sets ML-20M  Netﬂix  and MSD in [33]. NCF and NeuCF [20] was competitive only on unrealistically
small data-sets in [33].
We follow the evaluation protocol used in [33] 5 which is based on strong generalization  i.e.  the
training  validation and test sets are disjoint in terms of the users. Normalized Discounted Cumulative
Gain (nDCG@100) and Recall (@20 and @50) served as ranking metrics for evaluation in [33]. For
further details of the experimental set-up  the reader is referred to [33].
Note that the training-data matrix X here is binary  where 1 indicates an observed user-item interac-
tion. This obviously violates our assumption of a Gaussian distribution  which we made for reasons
of computational efﬁciency. In this case  our approach yields the best least-squares ﬁt of a linear
model  as discussed in Section 4. The empirical results corroborate that this is a viable trade-off
between accuracy and training-time  as discussed in the following.
Closed-Form Dense Solution: Table 1 summarizes the experimental results across the three data
sets. It shows that the closed-form solution ˆB(dense) (see Eq. 4) obtains nDCG@100 that is about 1%
lower on ML-20M  about 3% better on Netﬂix  and a remarkable 24% better on MSD than the best
competing model  MULT-VAE.
It is an interesting question as to why this simple full-rank model outperforms the deep nonlinear
MULT-VAE by such a large margin on the MSD data. We suspect that the hourglass architecture
of MULT-VAE (where the smallest hidden layer has 200 dimensions in [33]) severely restricts the
information that can ﬂow between the 41 140-dimensional input and output layers (regarding the
41 140 items in MSD data)  so that many relevant dependencies between items may get lost. For
instance  compared to the full-rank model  MULT-VAE recommends long-tail items considerably
less frequently among the top-N items  on average across all test users in the MSD data  see also
[50]. As the MSD data contain about twice as many items as the other two data sets  this would

Table 1: The closed-form dense solution ˆB(dense) (see Eq. 4) obtains competitive ranking-accuracy
while requiring only a small fraction of the training time  compared to the various models empirically
evaluated in [33]. The standard errors of the ranking-metrics are about 0.002  0.001  and 0.001 on
ML-20M  Netﬂix  and MSD data [33]  respectively.

ML-20M

Netﬂix

MSD

0.392

0.522

0.397

0.364

0.448

0.391

0.334

nDCG Recall Recall

nDCG Recall Recall

nDCG Recall Recall
@100 @20 @50 @100 @20 @50 @100 @20 @50
0.423
0.430
reproduced from [33]:
0.426
0.419
0.418
0.401
0.386

0.364
0.316
0.363
0.313
0.237
0.283
–did not ﬁnish in [33]–
0.257
0.312

0.537
0.524
0.523
0.495
0.498

0.351
0.344
0.343
0.347
0.316

0.395
0.387
0.391
0.370
0.360

0.444
0.438
0.428
0.428
0.404

0.386
0.380
0.376
0.379
0.351

0.266
0.266
0.188

0.211

models
ˆB(dense)

MULT-VAE
MULT-DAE
CDAE
SLIM
WMF
training times
ˆB(dense)
MULT-VAE
data-set
properties

2 min 0 sec
28 min 10 sec
136 677 users
20 108 movies
10 million interactions

1 min 30 sec
1 hour 26 min
463 435 users
17 769 movies
57 million interactions

15 min 45 sec
4 hours 30 min
571 355 users
41 140 songs
34 million interactions

7

also explain why the difference in ranking accuracy between MULT-VAE and the proposed full-rank
model is the largest on the MSD data. While the ranking accuracy of MULT-VAE may be improved by
considerably increasing the number of dimensions  note that this would prolong the training time at
least linearly  which is already 4 hours 30 minutes for MULT-VAE on MSD data (see Table 1). Apart
from that  as a simple sanity check  once the full-rank matrix ˆB(dense) was learned  we applied a
low-rank approximation (SVD)  and found that even 3 000 dimensions resulted in about a 10% drop
in nDCG@100 on MSD data. This motivated us to pursue sparse full-rank rather than dense low-rank
approximations in this paper  which is naturally facilitated by MRFs.
Besides the differences in accuracy  Table 1 also shows that the training-times of MULT-VAE are
more than ten times larger than the few minutes required to learn ˆB(dense) on all three data-sets. The
reasons are that MULT-VAE is trained on the user-item data-matrix X and uses stochastic gradient
descent to optimize ELBO (which involves several expensive computations in each step)–in contrast 
the proposed MRF uses a closed-form solution  and is trained on the item-item data-matrix (note
that #items (cid:28) #users in our experiments). Also note that the training-times of MULT-VAE reported
in Table 1 are optimistic  as they are based on only 50 iterations  where the training of MULT-VAE
may not have fully converged yet (the reported accuracies of MULT-VAE are based on 200 iterations).
These times were obtained on an AWS instance with 64 GB memory and 16 vCPUs for learning
ˆB(dense)  and with a GPU for training MULT-VAE (which was about ﬁve times faster than training
MULT-VAE on 16 vCPUs).
Sparse Approximation: Given the short training-times of the closed-form solution on these three
data-sets  we demonstrate the speed-up obtained by the sparse approximation (see Section 3) on the
MSD data  where the training of the closed-form solution took the longest: Table 2 shows that the
training-time can be reduced from about 16 minutes for the closed-form solution to under a minute
with only a relatively small loss in accuracy: while the loss in accuracy is statistically signiﬁcant
(standard error is about 0.001)  it is still very small compared to the difference to MULT-VAE  the
most accurate competing model in Table 1.
We can also see in Table 2 that different trade-offs between accuracy and training-time can be
obtained by using a sparser model and/or a larger hyper-parameter r (which increases the sizes of the
subsets of items in step 2 in Section 3.2): ﬁrst  the special case r = 0 corresponds to regressing each
individual item (instead of a subset of items) against its neighbors in the MRF  which is commonly
done in the literature (e.g.  [7  21  36  38]). Table 2 illustrates that this can be computationally more
expensive than inverting the entire covariance matrix at once (cf. MRF with sparsity 0.5% and
r = 0 vs. the dense solution). Second  comparing the MRF with sparsity 0.5% and r = 0.5 vs. the
model with sparsity 0.1% and r = 0  we can see that the former obtains a better Recall@50 than
the latter does  and also requires less training time. This illustrates that it can be beneﬁcial to learn
a denser model (0.5% vs. 0.1% sparsity here) but with a larger value r (0.5 vs. 0 here). Note that
optimizing Recall@50 (vs. @20) is important in applications where a large number of items has to

Table 2: Sparse Approximation (see Section 3)  on MSD data (standard error ≈ 0.001): ranking-
accuracy can be traded for training-time  controlled by the sparsity-level and the parameter r ∈ [0  1]
(deﬁned in Section 3). For comparison  also the closed-form solution ˆB(dense) and the best competing
model  MULT-VAE  from Table 1 are shown.

nDCG@100 Recall@20 Recall@50
0.391

0.334

0.430

Training Time
15 min 45 sec

21 min 12 sec
3 min 27 sec
2 min 1 sec

3 min 7 sec
1 min 10 sec
39 sec
4 hours 30 min

0.390
0.387
0.385

ˆB(dense)
0.5% sparse approximation (see Section 3)
r = 0
r = 0.1
r = 0.5
0.1% sparse approximation (see Section 3)
r = 0
r = 0.1
r = 0.5
MULT-VAE

0.385
0.382
0.381
0.316

0.330
0.327
0.327
0.266

0.427
0.424
0.424

0.421
0.417
0.417
0.364

0.333
0.331
0.330

8

be recommended  like for instance on the homepages of video streaming services  where typically
hundreds of videos are recommended. The proposed sparse approximation enables one to choose the
optimal trade-off between training-time and ranking-accuracy for a given real-world application.
Also note that  at sparsity levels 0.5% and 0.1%  our sparse model contains about the same number
of parameters as a dense matrix of size 41 140×200 and 41 140×40  respectively. In comparison 
MULT-VAE in [33] is comprised of layers with dimensions 41 140 → 600 → 200 → 600 → 41 140
regarding the 41 140 items in the MSD data  i.e.  it uses two matrices of size 41 140×600. Hence 
our sparse approximation (1) has only a fraction of the parameters  (2) requires orders of magnitude
less training time  and (3) still obtains about 20% better a ranking-accuracy than MULT-VAE in Table
2  the best competing model (see also Table 1).
Popularity Bias: The popularity bias in the model’s predictions is very important for obtaining high
recommendation accuracy  see also [49]. The different item-popularities affect the means and the
covariances in the Gaussian MRF  and we used the standard procedure of centering the user-item
matrix X (zero mean) and re-scaling the columns of X prior to training (once the training was
completed  and when making predictions  we scaled the predicted values back to the original space 
so that the predictions reﬂected the full popularity bias in the training data  which can be expected
to be the same as the popularity bias in the test data due to the way the data were split). This is
particularly important when learning the sparse model: theoretically  its sparsity pattern is determined
by the correlation matrix (which quantiﬁes the strength of statistical dependence between the nodes
in the Gaussian MRF)  while the values (after scaling them back to the original space) of the non-zero
entries are determined by the covariance matrix. In practice  we divided each column i in X by
si = stdα
i   where stdi is the column’s empirical standard deviation; the grid search regarding the
exponent α ∈ {0  1/4  1/2  3/4  1} yielded the best accuracy for α = 3/4 on MSD data (note that
α = 1 would result in the correlation matrix)  which coincidentally is the same value as was used in
word2vec [37] to remove the word-popularities in text-data as to learn word-similarities.

Conclusions

Geared toward collaborative ﬁltering  where typically the number of users n (data points) and items m
(variables) are large and n > m  we presented a computationally efﬁcient approximation to learning
sparse Gaussian Markov Random Fields (MRF). The key idea is to solve a large number of regression
problems  each regarding a small subset of items. The size of each subset can be controlled  and
this enables one to trade accuracy for training-time. As special (and extreme) cases  it subsumes the
approaches commonly considered in the literature  namely regressing each item (i.e.  set of size one)
against its neighbors in the MRF  as well as inverting the entire covariance matrix at once. Apart from
that  the auto-normal parameterization of MRFs prevents self-similarity of items (i.e.  zero-diagonal
in the weight-matrix)  which we found an effective alternative to using low-dimensional embeddings
in autoencoders in our experiments  as to enable the learned model to generalize to unseen data.
Requiring several orders of magnitude less training time  the proposed sparse approximation resulted
in a model with fewer parameters than the competing models  while obtaining about 20% better
ranking accuracy on the data-set with the largest number of items in our experiments.

Appendix
Let ˆµ(cid:62) (cid:54)= 0 denote the row vector of the (empirical) column-means of the given user-item data-matrix
X. If we assume that matrix B fulﬁlls the eigenvector-constraint ˆµ(cid:62) · B = ˆµ(cid:62)  then it holds that
(X − 1 · ˆµ(cid:62)) − (X − 1 · ˆµ(cid:62)) · B = X − X · B (where 1 denotes a column vector of ones in the
outer product with ˆµ(cid:62)). In other words  the learned ˆB is invariant under centering the columns of the
training data X. When the constraint ˆµ(cid:62) · B = ˆµ(cid:62) is added to the training objective in Eq. 3  the
method of Lagrangian multipliers again yields the closed-form solution:
· ˆC · dMat(˜γ) 

ˆB = I −

(cid:33)

(cid:32)
I − ˆCˆµˆµ(cid:62)
ˆµ(cid:62) ˆCˆµ

where now ˜γ = 1 (cid:11) diag((I − ˆCˆµˆµ(cid:62)
ˆµ(cid:62) ˆCˆµ
is merely the additional factor I − ˆCˆµˆµ(cid:62)
ˆµ(cid:62) ˆCˆµ
however  we did not observe this to cause any signiﬁcant effect regarding the ranking metrics.

) · ˆC) for the zero diagonal of ˆB. The difference to Eq. 4
due to the constraint ˆµ(cid:62) · B = ˆµ(cid:62). In our experiments 

9

Acknowledgments

I am very grateful to Tony Jebara for his encouragement  and to Dawen Liang for providing the code
for the experimental setup of all three data-sets.

References
[1] F. Aiolli. Efﬁcient top-N recommendation for very large scale binary rated datasets. In ACM Conference

on Recommender Systems (RecSys)  2013.

[2] O. Banerjee  L.E. Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood

estimation for multivariate Gaussian or binary data. Journal of Machine Learning Research  9  2008.

[3] J. Bennet and S. Lanning. The Netﬂix Prize. In Workshop at SIGKDD-07  ACM Conference on Knowledge

Discovery and Data Mining  2007.

[4] T. Bertin-Mahieux  D.P.W. Ellis  B. Whitman  and P. Lamere. The million song dataset. In International

Society for Music Information Retrieval Conference (ISMIR)  2011.

[5] J. Besag. Nearest-neighbor systems and the auto-logistic model for binary data. Journal of the Royal

Statistical Society  Series B  34:75–83  1972.

[6] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical

Society  Series B  36:192–236  1974.

[7] J. Besag. Statistical analysis of non-lattice data. The Statistician  24:179–95  1975.

[8] J. Besag. Efﬁciency of pseudo-likelihood estimation for simple Gaussian ﬁelds. Biometrika  64  1977.

[9] T. Blumensath and M.E. Davis. Iterative hard thresholding for compressed sensing  2008. arXiv:0805.0510.

[10] T. Blumensath and M.E. Davis. Iterative thresholding for sparse approximations. Journal for Fourier

Analysis and Applications  14  2008.

[11] H.T. Cheng  L. Koc  J. Harmsen  T. Shaked  T. Chandra  H. Aradhye  G. Anderson  G. Corrado  W. Chai 
M. Ispir  R. Anil  Z. Haque  L. Hong  V. Jain  X. Liu  and H. Shah. Wide & deep learning for recommender
systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS)  pages
7–10  2016.

[12] L. Dinh  J. Sohl-Dickstein  and S. Bengio. Density estimation using Real NVP. In Int. Conference on

Learning Representations (ICLR)  2017.

[13] S. Fattahi and S. Sojoudi. Graphical lasso and thresholding: Equivalence and closed form solution. Journal

of Machine Learning Research  20  2019.

[14] S. Fattahi  R.Y. Zhang  and S. Sojoudi. Sparse inverse covariance estimation for chordal structures. In

European Control Conference (ECC)  2018.

[15] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

Biostatistics  9  2008.

[16] M. Germain  K. Gregor  I. Murray  and H. Larochelle. MADE: Masked autoencoder for distribution

estimation. In International Conference on Machine Learning (ICML)  2015.

[17] D. Guillot  B. Rajaratnam  B. Rolfs  A. Maleki  and I. Wong. Iterative thresholding algorithm for sparse

inverse covariance estimation. In Advances in Neural Information Processing Systems (NIPS)  2012.

[18] J. M. Hammersley and P. E. Clifford. Markov ﬁelds on ﬁnite graphs and lattices. Unpublished manuscript 

1971.

[19] F. M. Harper and J. A. Konstan. The MovieLens datasets: History and context. ACM Transactions on

Interactive Intelligent Systems (TiiS)  5  2015.

[20] X. He  L. Liao  H. Zhang  L. Nie  X. Hu  and T.-S. Chua. Neural collaborative ﬁltering. In International

World Wide Web Conference (WWW)  2017.

[21] D. Heckerman  D.M. Chickering  C. Meek  R. Rounthwaite  and C. Kadie. Dependency networks for
inference  collaborative ﬁltering  and data visualization. Journal of Machine Learning Research  1:49–75 
2000.

10

[22] B. Hidasi and A. Karatzoglou. Recurrent neural networks with top-k gains for session-based recom-
mendations. In International Conference on Information and Knowledge Management (CIKM)  2017.
arXiv:1706.03847.

[23] B. Hidasi  A. Karatzoglou  L. Baltrunas  and D. Tikk. Session-based recommendations with recurrent

neural networks  2015. arXiv:1511.06939.

[24] C.-J. Hsieh  M.A. Sustik  I.S. Dhillon  P.K. Ravikumar  and R. Poldrack. BIG & QUIC: Sparse inverse
covariance estimation for a million variables. In Advances in Neural Information Processing Systems
(NIPS)  2013.

[25] C.-J. Hsieh  M.A. Sustik  I.S. Dhillon  P.K. Ravikumar  and R. Poldrack. QUIC: Quadratic approximation

for sparse inverse covariance matrix estimation. Journal of Machine Learning Research  2014.

[26] Y. Hu  Y. Koren  and C. Volinsky. Collaborative ﬁltering for implicit feedback datasets.

International Conference on Data Mining (ICDM)  2008.

In IEEE

[27] J.-H. Jacobsen  A. Smeulders  and E. Oyallon. i-RefNet: Deep invertible networks. In Int. Conference on

Learning Representations (ICLR)  2018.

[28] S. Kabbur  X. Ning  and G. Karypis. FISM: Factored item similarity models for top-N recommender

systems. In ACM Conference on Knowledge Discovery and Data Mining (KDD)  2013.

[29] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions  2018.

arXiv:1807:03039.

[30] D. P. Kingma  T. Salimans  R. Jozefowicz  X. Chen  I. Sutskever  and M. Welling. Improving variational
inference with inverse autoregressive ﬂow. In Advances in Neural Information Processing Systems (NIPS) 
2016.

[31] S. L. Lauritzen. Graphical Models. Oxford University Press  1996.

[32] M. Levy and K. Jack. Efﬁcient top-N recommendation by linear regression. In RecSys Large Scale

Recommender Systems Workshop  2013.

[33] D. Liang  R. G. Krishnan  M. D. Hoffman  and T. Jebara. Variational autoencoders for collaborative

ﬁltering. In International World Wide Web Conference (WWW)  2018.

[34] B.G. Lindsay. Composite likelihood methods. Contemporary Mathematics  80  1988.

[35] R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for large-scale

graphical lasso. Journal of Machine Learning Research  13:781–94  2012.

[36] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable selection with the lasso. Annals

of Statistics  34  2006.

[37] T. Mikolov  I. Sutskever  K. Chen  G. Corrado  and J. Dean. Distributed representations of words and
phrases and their compositionality. In Conference on Neural Information Processing Systems (NIPS)  2013.

[38] X. Ning and G. Karypis. SLIM: Sparse linear methods for top-N recommender systems.

International Conference on Data Mining (ICDM)  pages 497–506  2011.

In IEEE

[39] R. Pan  Y. Zhou  B. Cao  N. Liu  R. Lukose  M. Scholz  and Q. Yang. One-class collaborative ﬁltering. In

IEEE International Conference on Data Mining (ICDM)  2008.

[40] G. Papamakarios  I. Murray  and T. Pavlakou. Masked autoregressive ﬂow for density estimation. In

Advances in Neural Information Processing Systems (NIPS)  2017.

[41] A. Paterek. Improving regularized singular value decomposition for collaborative ﬁltering. In KDDCup 

2007.

[42] P. Ravikumar  M. Wainwright  and J. Lafferty. High-dimensional Ising model selection using (cid:96)1-regularized

logistic regression. Annals of Statistics  38  2010.

[43] S. Rendle  Ch. Freudenthaler  Z. Gantner  and L. Schmidt-Thieme. BPR: Bayesian personalized ranking
from implicit feedback. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 452–61  2009.

[44] K. Scheinberg  S. Ma  and D. Goldfarb. Sparse inverse covariance selection via alternating linearization

methods. In Advances in Neural Information Processing Systems (NIPS)  2010.

11

[45] M. Schmidt. Graphical Model Structure Learning with L1-Regularization. PhD thesis  University of

British Columbia  Vancouver  Canada  2011.

[46] S. Sedhain  A. K. Menon  S. Sanner  and D. Braziunas. On the effectiveness of linear models for one-class

collaborative ﬁltering. In AAAI Conference on Artiﬁcial Intelligence  2016.

[47] S. Sedhain  A. K. Menon  S. Sanner  and L. Xie. AutoRec: Autoencoders meet collaborative ﬁltering. In

International World Wide Web Conference (WWW)  2015.

[48] S. Sojoudi. Equivalence of graphical lasso and thresholding for sparse graphs. Journal of Machine Learning

Research  2016.

[49] H. Steck. Item popularity and recommendation accuracy. In ACM Conference on Recommender Systems

(RecSys)  pages 125–32  2011.

[50] H. Steck. Embarrassingly shallow autoencoders for sparse data.

Conference (WWW)  2019.

In International World Wide Web

[51] I. Stojkovic  V. Jelisavcic  V. Milutinovic  and Z. Obradovic. Fast sparse Gaussian Markov random ﬁelds

learning based on Cholesky factorization. In Int. Joint Conf. on Artiﬁcial Intelligence (IJCAI)  2017.

[52] E. Treister and J.S. Turek. A block-coordinate descent approach for large-scale sparse inverse covariance

estimation. In Advances in Neural Information Processing Systems (NIPS)  2014.

[53] K. Verstrepen and B. Goethals. Unifying nearest neighbors collaborative ﬁltering. In ACM Conference on

Recommender Systems (RecSys)  2014.

[54] M. N. Volkovs and G. W. Yu. Effective latent models for binary feedback in recommender systems. In

ACM Conference on Research and Development in Information Retrieval (SIGIR)  2015.

[55] H. Wang  A. Banerjee  C.-J. Hsieh  P.K. Ravikumar  and I.S. Dhillon. Large-scale distributed sparse

precision estimation. In Advances in Neural Information Processing Systems (NIPS)  2013.

[56] L. Wang  X. Ren  and Q. Gu. Precision matrix estimation in high dimensional Gaussian graphical models

with faster rates. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2016.

[57] D.M. Witten  J.H. Friedman  and N. Simon. New insights and faster computations for the graphical lasso.

Journal of Computational and Graphical Statistics  20:892–900  2011.

[58] Y. Wu  C. DuBois  A. X. Zheng  and M. Ester. Collaborative denoising auto-encoders for top-N recom-

mender systems. In ACM Conference on Web Search and Data Mining (WSDM)  2016.

[59] M. Yuan. Model selection and estimation in the Gaussian graphical model. Biometrika  2007.

[60] M. Yuan. High dimensional inverse covariance matrix estimation via linear programming. Journal of

Machine Learning Research  11:2261–86  2010.

[61] R.Y. Zhang  S. Fattahi  and S. Sojoudi. Large-scale sparse inverse covariance estimation via thresholding

and max-det matrix completion. In International Conference on Machine Learning (ICML)  2018.

[62] Y. Zheng  B. Tang  W. Ding  and H. Zhou. A neural autoregressive approach to collaborative ﬁltering. In

International Conference on Machine Learning (ICML)  2016.

[63] S. Zhou  P. Rütimann  M. Xu  and P. Bühlmann. High-dimensional covariance estimation based on

Gaussian graphical models. Journal of Machine Learning Research  12  2011.

12

,yunlong yu
Zhong Ji
Yanwei Fu
Jichang Guo
Yanwei Pang
Zhongfei (Mark) Zhang
Harald Steck