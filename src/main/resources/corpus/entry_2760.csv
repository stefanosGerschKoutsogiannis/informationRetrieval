2011,Kernel Embeddings of Latent Tree Graphical Models,Latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision  bioinformatics and natural language processing problems. However  existing models are largely restricted to discrete and Gaussian variables due to computational constraints; furthermore  algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search. We present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-Gaussian variables. Our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efficient inference. Experiments on simulated and real data show the advantage of our proposed approach.,Kernel Embeddings of Latent Tree Graphical Models

Le Song

College of Computing

Georgia Institute of Technology

lsong@cc.gatech.edu

Ankur P. Parikh

School of Computer Science
Carnegie Mellon University
apparikh@cs.cmu.edu

Eric P. Xing

School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu

Abstract

Latent tree graphical models are natural tools for expressing long range and hi-
erarchical dependencies among many variables which are common in computer
vision  bioinformatics and natural language processing problems. However  exist-
ing models are largely restricted to discrete and Gaussian variables due to com-
putational constraints; furthermore  algorithms for estimating the latent tree struc-
ture and learning the model parameters are largely restricted to heuristic local
search. We present a method based on kernel embeddings of distributions for
latent tree graphical models with continuous and non-Gaussian variables. Our
method can recover the latent tree structures with provable guarantees and per-
form local-minimum free parameter learning and efﬁcient inference. Experiments
on simulated and real data show the advantage of our proposed approach.

Introduction

1
Real world problems often produce high dimensional features with sophisticated statistical depen-
dency structures. One way to compactly model these statistical structures is to use probabilistic
graphical models that relate the observed features to a set of latent or hidden variables. By deﬁn-
ing a joint probabilistic model over observed and latent variables  the marginal distribution of the
observed variables is obtained by integrating out the latent ones. This allows complex distributions
over observed variables (e.g.  clique models) to be expressed in terms of more tractable joint models
(e.g.  tree models) over the augmented variable space. Probabilistic models with latent variables
have been deployed successfully to a diverse range of problems such as in document analysis [3] 
social network modeling [10]  speech recognition [18] and bioinformatics [5].
In this paper  we will focus on latent variable models where the latent structures are trees (we call it
a “latent tree” for short). In these tree-shaped graphical models  the leaves are the set of observed
variables (e.g.  taxa  pixels  words) while the internal nodes are hidden and intuitively “represent”
the common properties of their descendants (e.g.  distinct ancestral species  objects in an image 
latent semantics). This class of models strike a nice balance between their representation power
(e.g.  ability to model cliques) and the complexity of learning and inference processes on these
structures (e.g.  message passing is exact on trees). In particular  we will study the problems of
estimating the latent tree structures  learning the model parameters and performing inference on
these models for continuous and non-Gaussian variables where it is not easy to specify a parametric
family.
In previous works  the challenging problem of estimating the structure of latent trees has largely
been tackled by heuristics since the search space of structures is intractable. For instance  Zhang et
al. [28] proposed a search heuristic for hierarchical latent class models by deﬁning a series of local
search operations and using EM to compute the likelihood of candidate structures. Harmeling and
Williams [8] proposed a greedy algorithm to learn binary trees by joining two nodes with a high
mutual information and iteratively performing EM to compute the mutual information among newly
added hidden nodes. Alternatively  Bayesian hierarchical clustering [9] is an agglomerative cluster-
ing technique that merges clusters based on a statistical hypothesis test. Many other local search
heuristics based on maximum parsimony and maximum likelihood methods can also be found from

1

the phylogenetic community [21]. However  none of these methods extend easily to the nonpara-
metric case since they require the data to be discrete or to have a parametric form such that statistical
tests or likelihoods/EM can be easily computed.
Given the structures of the latent trees  learning the model parameters has predominantly relied on
likelihood maximization and local search heuristics such as expectation maximization (EM) [6].
Besides the problem of local minima  non-Gaussian statistical features such as multimodality and
skewness may pose additional challenges for EM. For instance  parametric models such as mixture
of Gaussians may lead to an exponential blowup in terms of representation during the inference
stage of EM  so further approximations may be needed to make these cases tractable. Furthermore 
EM can require many iterations to reach a prescribed training precision.
In this paper  we propose a method for latent tree models with continuous and non-Gaussian ob-
servation based on the concept of kernel embedding of distributions [23]. The problems we try to
address are: how to estimate the structures of latent trees with provable guarantees  and how to
perform local-minimum-free parameter learning and efﬁcient inference given the tree structures  all
in nonparametric fashion. The main ﬂavor of our method is to exploit the spectral properties of
the joint embedding (or covariance operators) in both the structure recovery and learning/inference
stage. For the former  we deﬁne a distance measure between variables based on the singular value
decomposition of covariance operators. This allows us to generalize some of the distance based
latent tree learning procedures such as neighbor joining [20] and the recursive grouping methods [4]
to the nonparametric setting. These distance based methods come with strong statistical guarantees
which carry over to our nonparametric generalization. After the structure is recovered  we further
use the covariance operator and its principal singular vectors to design surrogates for parameters
of the latent variables (called a “spectral algorithm”). One advantage of our spectral algorithm is
that it is local-minimum-free and hence amenable for further statistical analysis (see [11  25  16] for
previous work on spectral algorithms). Last  we will demonstrate the advantage of our method over
existing approaches in both simulation and real data experiments.
2 Latent Tree Graphical Models
We will focus on latent variable models where the observed variables are continuous and non-
Gaussian and the conditional independence structures are speciﬁed by trees. We will use uppercase
letters to denote random variables (e.g.  Xi) and lowercase letters their instantiations (e.g.  xi). A
latent tree model deﬁnes a joint distribution over a set  O = {X1  . . .   XO}  of O observed vari-
ables and a set  H = {XO+1  . . .   XO+H}  of H hidden variables. The complete set of variables is
denoted by X = O ∪ H . For simplicity  we will assume that all observed variables have the same
domain XO  and all hidden variables take values from XH and have ﬁnite dimension d.
The joint distribution of X in a latent tree model is fully characterized by a set of conditional distri-
butions (CD). More speciﬁcally  we can select an arbitrary latent node in the tree as the root  and re-
orient all edges away from the root. Then the set of CDs between nodes and their parents P(Xi|Xπi)
are sufﬁcient to characterize the joint distribution (for the root node Xr  we set P(Xr|Xπr ) =
P(Xi|Xπi ). Com-
pared to tree models which are deﬁned solely on observed variables  latent tree models encompass
a much larger classes of models  allowing more ﬂexibility in modeling observed variables. This is
evident if we sum out the latent variables in the joint distribution 

P(Xr); and we use P to refer to density in continuous case)  P(X ) = (cid:81)O+H

i=1

H

i=1

(1)
This expression leads to complicated conditional independence structures between observed vari-
ables depending on the tree topology. In other words  latent tree models allow complex distributions
over observed variables (e.g.  clique models) to be expressed in terms of more tractable joint models
over the augmented variable space. This can lead to a signiﬁcant saving in model parametrization.
For simplicity of explanation  we will focus on latent tree structures where each internal node has
exactly 3 neighbors. We can reroot the tree and redirect all the edges away from the root. For a
variable Xs  we use αs to denote its sibling  πs to denote its parent  ιs to denote its left child and ρs
to denote its right child; the root node will have 3 children  and we use ωs to denote the extra child.
All the observed variables are leaves in the tree  and we will use ι∗
s to denote an observed
variable which is found by tracing in the direction from node s to its left child ιs  right child ρs  and
its parent πs respectively. s∗ denotes any observed variable in the subtree rooted at node s.

s  π∗

s  ρ∗

(cid:88)

(cid:89)O+H

P(O) =

P(Xi|Xπi ).

2

2πσ

j=1

3 Kernel Density Estimator and Hilbert Space Embedding
Kernel density estimation (KDE) is a nonparametric way of ﬁtting the density of continuous random
variables with non-Gaussian statistical features such as multi-modality and skewness [22]. However 
traditional KDE cannot model the latent tree structure. In this paper  we will show that the kernel
density estimator can be augmented to deal with latent tree structures using a recent concept called
Hilbert space embedding of distributions [23]. Next  we will ﬁrst explain the basic idea of KDE and
distribution embeddings  and show how they are related.

Kernel density estimator. Given a set of i.i.d. samples S = (cid:8)(xi

O)(cid:9)n

1  . . .   xi

i=1 from

P(X1  . . .   XO)  KDE estimates the density via
1
n

(cid:98)P(x1  . . .   xO) =

(cid:88)n

(cid:89)O

i=1

j=1

j) 

(cid:21)

ES

(cid:80)n

i=1

= EO

k(xj  Xj)

(cid:20)(cid:89)O

k(xj  xi

(2)
where k(x  x(cid:48)) is a kernel function. A commonly used kernel function  which we will focus on  is
exp(−(cid:107)x − x(cid:48)(cid:107)2/2σ2). For Gaussian RBF kernel  there
the Gaussian RBF kernel k(x  x(cid:48)) = 1√
exists a feature map φ : R (cid:55)→ F such that k(x  x(cid:48)) = (cid:104)φ(x)  φ(x(cid:48))(cid:105)F   and the feature space has the
reproducing property  i.e. for all f ∈ F  f (x) = (cid:104)f  φ(x)(cid:105)F . Products of kernels are also kernels 
F O.
Here ⊗O
j=1(cid:63) denotes the tensor product of O feature vectors which results in a rank-1 tensor of
order O. This inner product can be understood by analogy to the ﬁnite dimensional case: given
x  y  z  x(cid:48)  y(cid:48)  z(cid:48) ∈ Rd  (x(cid:62)x(cid:48))(y(cid:62)y(cid:48))(z(cid:62)z(cid:48)) = (cid:104)x ⊗ y ⊗ z  x(cid:48) ⊗ y(cid:48) ⊗ z(cid:48)(cid:105)Rd3 .

j) as a single inner product (cid:10)⊗O
j)(cid:11)
j=1φ(Xj)(cid:3) is called the Hilbert space embedding of dis-

which allow us to write (cid:81)O
Hilbert space embedding. CO := EO(cid:2)⊗O

j=1φ(xj) ⊗O

j=1 k(xj  x(cid:48)

j=1φ(x(cid:48)

j=1φ(xj)(cid:11)

=(cid:10)EO(cid:2)⊗O

j=1φ(Xj)(cid:3)  ⊗O

F O = CO ¯×O φ(xO) . . . ¯×2 φ(x2) ¯×1 φ(x1)

tribution P(O) with tensor features ⊗O
j=1φ(Xj). In other words  the embedding of a distribution
is simply the expected feature of that distribution. The essence of Hilbert space embedding is to
represent distributions as elements in Hilbert spaces  and then subsequent manipulation of the distri-
butions can be carried out via Hilbert space operations such as inner product and distance. We next
show how to represent a KDE using distribution embeddings.
Taking the expected value of a KDE with respect to the random sample S  

(3)
we see that this expected value is the inner product between the embedding CO and tensor
features ⊗O

(cid:104)(cid:98)P(x1  . . .   xO)
(cid:105)
If we replace the embedding CO by its ﬁnite sample estimate (cid:98)CO :=
j)(cid:1)  we recover the density estimator in (2). Alternatively  using tensor nota-
(cid:0)⊗O
j=1φ(Xj)(cid:3)  ⊗O
(cid:10)EO(cid:2)⊗O

1
n
tion (described in supplemental)  we can rewrite equation (3) as

(4)
where CO is a big tensor of order O which can be difﬁcult to store and maintain. While traditional
KDE can not make use of the fact that the embedding CO originates from a distribution with latent
tree structure  the embedding view actually allows us to exploit this special structure and further
decompose CO to simpler tensors of much lower orders.
4 Kernel Embedding of Latent Tree Graphical Models
In this section  we assume that the structures of the latent tree graphical models are given  and we will
deal with structure learning in the next section. We will show that the tensor expression of KDE in (4)
can be computed recursively using a collection of lower order tensors. Essentially  these lower order
tensors correspond to the conditional densities in the latent tree graphical models; and the recursive
computations try to integrate out the latent variables in the model  and they correspond to the steps
in the message passing algorithm for graphical model inference. The challenge is that message
passing algorithm becomes nontrivial to represent and implement in continuous and nonparametric
settings. Previous methods may lead to exponential blowup in their message representation and
hence various approximations are needed  such as expectation propagation [15]  mixture of Gaussian
simpliﬁcation [27]  and sampling [12]. In contrast  the distribution embedding view allows us to
represent and implement message passing algorithm efﬁciently without resorting to approximations.
Furthermore  it also allows us to develop a local-minimum-free algorithm for learning the parameters
of latent tree graphical models.

j=1φ(xj)(cid:11)

j=1φ(xj).
j=1φ(xi

F O  

3

4.1 Covariance Operator and Conditional Embedding Operator
We will ﬁrst explain the concept of conditional embedding operators which are the nonparametric
counterparts for conditional probability tables in the discrete case. Conditional embedding operators
will be the key building blocks to a nonparametric message passing algorithm as much as conditional
probability tables are to the ordinary message passing algorithm.
Following [7]  we ﬁrst deﬁne the covariance operator CXsXt which allows us to compute the expec-
tation of the product of function f (Xs) and g(Xt)  i.e.  EXsXt[f (Xs)g(Xt)]  using linear operations
in the RKHS. More formally  let CXsXt : F (cid:55)→ F such that for all f  g ∈ F 
EXsXt[f (Xs)g(Xt)] = (cid:104)f  EXsXt [φ(Xs) ⊗ φ(Xt)] g(cid:105)F = (cid:104)f  CXsXtg(cid:105)F = Cst ¯×2 g ¯×1 f (5)
where we abbreviate the notation CXsXt as Cst  and will follow such abbreviation in the rest of
the paper (e.g. Cs2 is an abbreviation for CXsXs) . This can be understood by analogy with the
ﬁnite dimensional case: if x  y  z  v ∈ Rd  then x(cid:62)(yz(cid:62))v = (yz(cid:62)) ¯×2 v ¯×1 x where we use the
tensor-vector multiplication notation from [13] (see supplemental for details). In other words  the
covariance operator is also the embedding of the joint distribution P(Xs  Xt).
Then the conditional embedding operator can be deﬁned via covariance operators according to
Song et al. [26]. A conditional embedding operator allows us to compute conditional expectations
EXt|xs [f (Xt)] as linear operations in the RKHS. Let Ct|s := CtsC−1
(6)
In other words  the operator Ct|s takes the feature map φ(xs) of the point on which we condition 
and outputs the conditional expectation of the feature φ(Xt) with respect to P(Xt|xs). Although the
formula looks similar to the Gaussian case  it is important to note that the conditional embedding
operator allows us to compute the conditional expectation of any f ∈ F  regardless of the distribu-
tion of the random variable in feature space (aside from the condition that h(·) := EXt|Xs=·[f (Xt)]
is in the RKHS on Xs  as noted by Song et al.). In particular  we do not need to assume the random
variables have a Gaussian distribution in feature space.
4.2 Representation for Message Passing Algorithm
For simplicity  we will focus on latent trees where all latent variables have degree 3 (but our
method can be generalized to higher degrees). We ﬁrst introduce latent variables into equation (3) 
EO∪H
; Then we integrate out the latent variables according to the latent tree
structure using a message passing algorithm [17] 

EXt|xs[f (Xt)] = (cid:10)f  EXt|xs [φ(Xt)(cid:11)

ss such that for all f ∈ F 
F = Ct|s ¯×2 φ(xs) ¯×1 f.

F = (cid:10)f  Ct|sφ(xs)(cid:11)

(cid:104)(cid:81)O

j=1 k(xj  Xj)

(cid:105)

* At a leaf node (always observed variable) we pass the following message to its parent

** An internal latent variable aggregates incoming messages from its two children and then

ms(Xπs ) = EXs|Xπs
sends an outgoing message to its own parent ms(Xπs) = EXs|Xπs

[k(xs  Xs)].

[mιs (Xs)mρs(Xs)].

is integrated out br := EO[(cid:81)O

*** Finally  at the root node  all incoming messages are multiplied together and the root variable

j=1 k(xj  Xj)] = EXr [mιs (Xr)mρs(Xr)mωr (Xr)].

The challenge is that message passing becomes nontrivial to represent and implement in continuous
and nonparametric settings. Previous methods may lead to exponential blowup in their message
representation and hence various approximations are needed  such as expectation propagation [15] 
mixture of Gaussian simpliﬁcation [27]  and sampling [12].
Song et al. [24] show that the above 3 message update operations can be expressed using Hilbert
space embeddings [26]  and no further approximation is needed in the message computation. Basi-
cally  the embedding approach assume that messages are functions in the reproducing kernel Hilbert
space  and message update is an operator that takes several functions as inputs and output another
function in the reproducing kernel Hilbert space. More speciﬁcally  message updates are linear (or
multi-linear) operations in feature space 

¯×1 φ(xs)
* At leaf nodes  we have mts(·) = EXs|Xπs =·[k(xs  Xs)] = C(cid:62)
s|πs
** At internal nodes  we deﬁne a tensor product reproducing kernel Hilbert space H := F⊗F 
under which the product of incoming messages can be written as a single inner product 
mιs(Xs) mρs(Xs) = (cid:104)mιs   φ(Xs)(cid:105)(cid:104)mρs  φ(Xs)(cid:105) = (cid:104)mιs ⊗ mρs  φ(Xs) ⊗ φ(Xs)(cid:105)H
Then the message update becomes

ms(·) =(cid:10)mιs ⊗ mρs   EXs|Xπs =· [φ(Xs) ⊗ φ(Xs)](cid:11)

φ(xs) = Cs|πs

H = Cs2|πs

¯×2 mρs

¯×1 mιs

(7)

4

πsπs.

¯×1 mιr

¯×2 mρr

= Cr3 ¯×3 mωr

[φ(Xs) ⊗ φ(Xs) ⊗ φ(Xs)]  and the operator C−1

*** Finally  at the root nodes  we use the property of tensor product features and arrives at:

where we deﬁne the conditional embedding operator for the tensor features φ(Xs)⊗φ(Xs).
By analogy with (6))  Cs2|πs is deﬁned in terms of a covariance operator Cs2πs
:=
EXsXπs
Er[mιr (Xr) mρr (Xr) mωr (Xr)] = (cid:104)mιr ⊗ mρr ⊗ mωr   EXr [φ(Xr) ⊗ φ(Xr) ⊗ φ(Xr)](cid:105)
(8)
We note that the traditional kernel density estimator needs to estimate a tensor of order O involving
all observed variables (equation (4)). By making use of the conditional independence structure of
latent tree models  we only need to estimate tensors of much smaller orders. Particularly  we only
need to estimate tensors involving two variables (for each parent-child pair)  and then the density
can be estimated via message passing algorithms using these tensors of much smaller order.
The drawback of the representations in (7) and (8) is that they require exact knowledge of conditional
embedding operators associated with latent variables  but none of these are available in training.
Next we will show that we can still make use of the tensor decomposition representation without the
need for recovering the latent variables explicitly.
4.3 Spectral Algorithm for Learning Latent Tree Parameters
Our observation from (7) and (8) is that if we can recover the conditional embedding operators
associated with latent variables up to some invertible transformations  we will still be able to com-

pute latent tree density correctly. For example  we can transform the messages: (cid:101)mιs = Tιs mιs 
(cid:101)mρs = Tρsmρs  and (cid:101)mωs = Tωsmωs  and we can update these transformed messages:
¯×1 (cid:101)mιs
¯×1 (cid:101)mιr

* At leaf nodes  (cid:101)ms = T (cid:62)
** At internal nodes  (cid:101)ms = (Cs2|πs ×1 T −1

s C(cid:62)
s|πs
*** At the root  br = (Cr3 ×1 T −1

s ) ¯×2 (cid:101)mρs
¯×2 (cid:101)mρr

) ¯×3 (cid:101)mωr

×2 T −1
×3 T −1

×2 T −1

×3 T (cid:62)

φ(xs)

ωr

ρr

ρs

ιr

ιs

without changing the ﬁnal br. Basically  all the invertible transformations T cancel out with each
other. These transformations provide us an additional degree of freedom for algorithm design: we
can choose the invertible transforms cleverly  such that the transformed representation can be recov-
ered from observed quantities without the need for accessing the latent variables. This representation
is related to but different from that of [16] for discrete variables which uses only 3rd order tensors.
The kernel case is more challenging and requires qth order tensors (where q is the degree of a node).
More speciﬁcally  these transformations T can be constructed from cross covariance operators of
−1 and let
certain pairs of observed variables and their singular vectors U. We set Ts = (U(cid:62)
Us be the top d right eigenvectors of Cπ∗
s s∗. Consider the simple case for the leaf node (∗). In this
s = U(cid:62)
case  we can set s∗ = s and get that T −1
s Cs|πs )
Cπ∗

s Cs∗|πs)
s Cs|πs. Consider the following expansion:
) = φ(xs)(cid:62)Csπ∗

(9)
(10)
Here † denotes pseudo-inverse. The general pattern is that we can relate the transformed latent
quantity to observed quantities in two different ways such that we can solve for the transformed
s in the
ωr at the root. We summarize the

latent quantity. A similar strategy can be applied to (cid:101)Cs2|πs := Cs2|πs ×1 T −1
internal message update  and the (cid:101)Cr3 := Cr3 ×1 T −1

⇒ (cid:101)ms = (Cπ∗
) = φT (xs)Cs|πs (U(cid:62)
†
s sUs)

(U(cid:62)
s sφ(xs)

(cid:101)m(cid:62)
s (U(cid:62)

s Cs|πs)(Cπ2

results on how to compute the transformed quantities below (see supplemental for details).

×2 T −1

×3 T −1

×2 T −1

C(cid:62)
s|πs
π∗

×3 T (cid:62)

s Csπ∗

−1

ρs

ρs

ιs

ιs

s

s

s

* At leaf nodes  (cid:101)ms = (Cπ∗
** At internal nodes  (cid:101)Cs2|πs = Cι∗
*** At the root  (cid:101)Cr3 = Cι∗

†
s sUs)

Cπ∗
s ρ∗
s π∗
×1 U(cid:62)

r ρ∗

r ω∗

ιr

r

s

s sφ(xs).
×1 U(cid:62)
×2 U(cid:62)

ιs

ρr

×2 U(cid:62)
ρs
×3 U(cid:62)
.

ωr

×3 (Cπ∗
s ι∗

s

Us)†.

The above results give us an efﬁcient algorithm for computing the expected kernel density br which
can take into account the latent tree structures while at the same time avoiding the local minimum
problems associated with explicitly recovering latent parameters. The main computation only in-
volves tensor-matrix and tensor-vector multiplications  and a sequence of singular value decompo-
sitions of pairwise cross covariance operators. After we obtain the transformed quantities  we can
then use them in the message passing algorithm to obtain the ﬁnal belief br.

i=1 drawn i.i.d. from a P(O)  the spectral algorithm for latent
trees proceeds by replacing all population quantities by their empirical counterpart. For instance 

1  . . .   xi

Given a sample S =(cid:8)(xi

O)(cid:9)n

5

the SVD of covariance operators between Xs and Xt can be estimated by ﬁrst forming matrices
value decomposition of (cid:98)C can be carried out to obtain an estimate for (cid:98)U (See [25] for more details).
n ΦΥ(cid:62). Then a singular
Υ = (φ(x1

s )) and Φ = (φ(x1

t )  . . .   φ(xn

s)  . . .   φ(xn

t ))  and estimate (cid:98)Cts = 1

5 Structure Learning of Latent Tree Graphical Models
The last section focused on density estimation where the structure of the latent tree is known. In this
section  we focus on learning the structure of the latent tree. Structure learning of latent trees is a
challenging problem that has largely been tackled by heuristics since the search space of structures
is intractable. The additional challenge in our case is that the observed variables are continuous and
non-Gaussian  which we are not aware of any existing methods for this problem.
Structure learning algorithm We develop a distance based method for constructing latent trees of
continuous  non-Gaussian variables. The idea is that if we have a tree metric (distance) between dis-
tributions on observed nodes  we can use the property of the tree metric to reconstruct the latent tree
structure using algorithms such as neighbor joining [20] and the recursive grouping algorithm [4].
These methods take a distance matrix among all pairs of observed variables as input and output a
tree by iteratively adding hidden nodes. While these methods are iterative  they have strong theo-
retical guarantees on structure recovery when the true distance matrix forms an additive tree metric.
However  most previously known tree metrics are deﬁned for discrete and Gaussian variables. The
additional challenge in our case is that the observed variables are continuous and non-Gaussian. We
propose a tree metric below which works for continuous non-Gaussian cases.
Tree metric and pseudo-determinant We will ﬁrst explain some basic concepts of a tree metric.
If the joint probability distribution P(X ) has a latent tree structure  then a distance measure dst
between an arbitrary variables pairs Xs and Xt are called tree metric if it satisﬁes the following path
(u v)∈P ath(s t) duv. For discrete and Gaussian variables  tree metric can
be deﬁned via the determinant | · | [4]
2 log |CstC(cid:62)

(11)
where Cst denotes joint probability matrix in the discrete case and the covariance in the Gaussian
case; Css is the diagonalized marginal probability vector in the discrete case and variance in the
Gaussian case. However  this deﬁnition of tree metric is restricted in the sense that it requires
all discrete variables to have the same number of states and all Gaussian variables have the same
dimension. This is because determinant is only deﬁned (and non-zero) for square and non-singular
matrices. For our more general scenario  where the observed variables are continuous non-Gaussian
but the hidden variables have dimension d  we will deﬁne a tree metric based on pseudo-determinant
which works for our operators.
Nonparametric tree metric The pseudo-determinant is deﬁned as the product of non-zero singular
i=1 σi(C). In our case  since we assume that the dimension of the
hidden variables is d  the pseudo-determinant is simply the product of top d singular values. Then
we deﬁne the distance metric between two continuous non-Gaussian variables Xs and Xt as

values of an operator |C|(cid:63) = (cid:81)d

additive condition: dst =(cid:80)

4 log |CttC(cid:62)
tt| 

4 log |CssC(cid:62)

dst = − 1

ss| + 1

st| + 1

4 log |CssC(cid:62)

ss|(cid:63) + 1

4 log |CttC(cid:62)

tt|(cid:63).

(12)
One can prove that (12) deﬁnes a tree metric by inducting on the path length. Here we only
show the additive property for the simplest path Xs − Xu − Xt involving only a single hidden
s|u|(cid:63) according
variable Xu.
to the Markov property. Then using Sylvester’s determinant theorem  the latter is also equal to
s|u to the front. Next  introducing two copies of |Cuu| and
|C(cid:62)
s|uCs|uCuuC(cid:62)
rearranging terms  we have
s|u|(cid:63)|Ct|uCuuCuuC(cid:62)
|Cuu|(cid:63)|Cuu|(cid:63)
tu|(cid:63) + 1

In this case  we ﬁrst factorize |CstC(cid:62)
t|uCt|uCuu|(cid:63) by ﬂipping C(cid:62)
|Cs|uCuuCuuC(cid:62)

su|(cid:63)|CtuC(cid:62)
tu|(cid:63)
|CuuCuu|(cid:63)
4 log |CssC(cid:62)

Last  we plug this into (12) and we have the desired path additive property

st|(cid:63) into |Cs|uCuuC(cid:62)

t|uCt|uCuuC(cid:62)

2 log |CuuC(cid:62)

tt|
4 log |CttC(cid:62)

2 log |CsuC(cid:62)

2 log |CtuC(cid:62)

dst = − 1

uu|(cid:63) + 1

su|(cid:63) − 1

ss|(cid:63) + 1

|CsuC(cid:62)

|CstC(cid:62)

st|(cid:63) =

2 log(cid:12)(cid:12)CstC(cid:62)

st

(cid:12)(cid:12)(cid:63) + 1

t|u|(cid:63)

=

dst = − 1

.

(13)

= dsu + dut

6 Experiments
We evaluate our method on synthetic data as well as a real-world crime/communities dataset [1  19].
For all experiments we compare to 2 existing approaches. The ﬁrst is to assume the data are

6

(a) balanced binary tree

(b) skewed HMM-like tree

(c) random trees

Figure 1: Comparison of our kernel structure learning method to the Gaussian and Nonparanormal
methods on different tree structures.

Figure 2: Histogram of the differences between the estimated number of hidden states and the true
number of states.
multivariate Gaussians and use the tree metric deﬁned in [4] (which is essentially a function of
the correlation coefﬁcient). The second existing approach we compare to is the Nonparanor-
mal (NPN) [14] which assumes that there exist marginal transformations f1  . . .   fp such that
f (X1)  . . .   f (Xp) ∼ N (µ  Σ).
If the data comes from a Nonparanormal distribution  then the
transformed data are assumed to be multivariate Gaussians and the same tree metric as the Gaussian
case can be used on the transformed data. Our approach makes much fewer assumptions about the
data than either of these two methods which can be more favorably in practice.
To perform learning and inference in our approach  we use the spectral algorithm and message
passing algorithm described earlier in the paper. For inference in the Gaussian (and nonparanormal)
cases  we use the technique in [4] to learn the model parameters (covariance matrix). Once the
covariance matrix has been estimated  computing the marginal of one variable given a set of evidence
reduces to solving a linear equation of one variable [2].
Synthetic data: structure recovery. The ﬁrst experiment is to demonstrate how our method com-
pares to the Gaussian and Nonparanormal methods in terms of structure recovery. We experiment
with 3 different tree types (each with 64 leaves or observed variables): a balanced binary tree  a
completely binary skewed tree (like an HMM)  and randomly generated binary trees. For all trees 
we use the following generative process to generate the n-th sample from a node s (denoted x(n)
): If
2 sample from a Gaussian
s is the root  sample from a mixture of 2 Gaussians. Else  with probability 1
with mean −x(n)
We vary the training sample size from 200 to 100 000. Once we have computed the empirical tree
distance matrix for each algorithm  we use the neighbor joining algorithm [20] to learn the trees.
For evaluation we compare the number of hops between each pair of leaves in the true tree to the
estimated tree. For a pair of leaves i  j the error is deﬁned as: error(i  j) =
+
|hops∗(i j)−(cid:91)hops(i j)|
  where hops∗ is the true number of hops and (cid:91)hops is the estimated number of

2 sample from a Gaussian with mean x(n)
πs .

πs and with probability 1

|hops∗(i j)−(cid:91)hops(i j)|

hops∗(i j)

s

(cid:91)hops(i j)

hops. The total error is then computed by adding the error for each pair of leaves.
The performance of our method depends on the number of singular values chosen and we experi-
mented with 2  5 and 8 singular values. Furthermore  we choose the bandwidth σ for the Gaussian
RBF kernel needed for the covariance operators using median distance between pairs of training
points. For all these choices our method performs better than the Gaussian and Nonparanormal
methods. This is to be expected  since the data we generated is neither Gaussian or Nonparamnor-
mal  yet our method is able to learn the structure correctly. We also note that balanced binary trees
are the easiest to learn while the skewed trees are the hardest (Figure 1).
Synthetic data: model selection. Next we evaluate the ability of our model to select the correct
number of singular values via held-out likelihood. For this experiment we use a balanced binary tree
with 16 leaves (total of 31 nodes) and 100000 samples. A different generative process is used so

7

0.20.51251020501001050Training Sample Size (x103)ErrorKernel−2Kernel−8Kernel−5GaussianNPN0.20.51251020501001050100200Training Sample Size (x103)ErrorKernel−2Kernel−5Kernel−8GaussianNPN0.20.51251020501001050Training Sample Size (x103)ErrorKernel−2Kernel−8Kernel−5NPNGaussian−3−2−1012305101520Diff from true # of hidden statesFrequencyTrue # of hidden states = 2−3−2−1012305101520Diff from true # of hidden statesFrequencyTrue # of hidden states = 3−3−2−10123051015Diff from true # of hidden statesFrequencyTrue # of hidden states = 4−3−2−10123051015Diff from true # of hidden statesFrequencyTrue # of hidden states = 5(a)

(b)

Figure 3: (a) visualization of kernel latent tree learned from crime data (b) Comparison of our
method to Gaussian and NPN in predictive task.
that it is clear what the correct number of singular values should be (When the hidden state space is
continuous like in our ﬁrst synthetic experiment this is unclear). Each internal node is discrete and
takes on d values. The leaf is a mixture of d Gaussians where which Gaussian to sample from is
dictated by the discrete value of the parent.
We vary d from 2 through 5 and then run our method for a range of 2 through 8 singular values.
We select the model that has the highest likelihood computed using our spectral algorithm on a
hold-out set of 500 examples. We then take the difference between the number of singular values
chosen and the true singular values  and plot histograms of this difference (Ideally all the trials
should be in the zero bin). The experiment is run for 20 trials. As we can see in Figure 2  when
d is low  the held-out likelihood computed by our method does a fairly good job in recovering the
correct number. However  as the true number of eigenvalues rises our method underestimates the
true number (although it is still fairly close).
Crime Data. Finally  we explore the performance of our method on a communities and crime
dataset from the UCI repository [1  19]. In this dataset several real valued attributes are collected
for several communities  such as ethnicity proportions  income  poverty rate  divorce rate etc.  and
the goal is to predict the number of violent crimes (proportional to the size of the community) that
occur based on these attributes. In general these attributes are highly skewed and therefore not well
characterized by a Gaussian model.
We divide the data into 1400 samples for training  300 samples for model selection (held-out like-
lihood)  and 300 samples for testing. We pick the ﬁrst 50 of these attributes  plus the violent crime
variable and construct a latent tree using our tree metric and the neighbor joining algorithm [20].
We depict the tree in Figure 3 and highlight a few coherent groupings. For example  the “elderly”
group attributes are those related to retirement and social security (and thus correlated). The large
clustering in the center is where the class variable (violent crimes) is located next to the poverty
rate and the divorce rate among other relevant variables. Other groupings include type of occupa-
tion and education level as well as ethnic proportions. Thus  overall our method captures sensible
relationships.
For a more quantitative evaluation  we condition on a set of E of evidence variables and predict the
violent crimes class label. We experiment with a varying number of sizes of evidence sets from 5
to 40 and repeat for 40 randomly chosen evidence sets of a ﬁxed size. Since the crime variable is a
number between 0 and 1  our error measure is simply err(ˆc) = |ˆc − c∗| (where ˆc is the predicted
value and c∗ is the true value. As one can see in Figure 3 our method outperforms both the Gaussian
and the nonparanormal for the range of query sizes. Thus  in this case our method is better able to
capture the skewed distributions of the variables than the other methods.
Acknowledgments
This work was partially done when LS was at Carnegie Mellon University and Google Research. This
work is also supported by an NSF Graduate Research Fellowship (under Grant No. 0750271) to APP  NIH
1R01GM093156  NIH 1RC2HL101487  NSF DBI-0546594  and an Alfred P. Sloan Fellowship to EPX.

8

elderly Urban/rural Education/job Divorce/crime/poverty race 5102030400.150.20.25query sizeErrorKernelNPNGaussianReferences

[1] A. Asuncion and D.J. Newman. Uci machine learning repository  2007.
[2] D. Bickson. Gaussian belief propagation: Theory and application. Arxiv preprint arXiv:0811.2518  2008.
[3] D. Blei  A. Ng  and M. Jordan. Latent dirichlet allocation. In NIPS  2002.
[4] M.J. Choi  V.Y.F. Tan  A. Anandkumar  and A.S. Willsky. Learning latent tree graphical models. Arxiv

preprint arXiv:1009.2722  2010.

[5] A. Clark. Inference of haplotypes from pcr-ampliﬁed samples of diploid populations. Molecular Biology

and Evolution  7(2):111–122  1990.

[6] A. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the em algorithm.

Journal of the Royal Statistical Society B  39(1):1–22  1977.

[7] K. Fukumizu  F. R. Bach  and M. I. Jordan. Dimensionality reduction for supervised learning with repro-

ducing kernel Hilbert spaces. J. Mach. Learn. Res.  5:73–99  2004.

[8] S. Harmeling and C.K.I. Williams. Greedy learning of binary latent trees. IEEE Transactions on Pattern

Analysis and Machine Intelligence  2010.

[9] K.A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In Proceedings of the 22nd interna-

tional conference on Machine learning  pages 297–304. ACM  2005.

[10] Peter D. Hoff  Adrian E. Raftery  and Mark S. Handcock. Latent space approaches to social network

analysis. JASA  97(460):1090–1098  2002.

[11] D. Hsu  S. Kakade  and T. Zhang. A spectral algorithm for learning hidden markov models. In COLT 

2009.

[12] A. Ihler and D. McAllester. Particle belief propagation. In AISTATS  pages 256–263  2009.
[13] Tamara. Kolda and Brett Bader. Tensor decompositions and applications. SIAM Review  51(3):455–500 

2009.

[14] H. Liu  J. Lafferty  and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-

sional undirected graphs. The Journal of Machine Learning Research  10:2295–2328  2009.

[15] T. Minka. Expectation Propagation for approximative Bayesian inference. PhD thesis  MIT Media Labs 

Cambridge  USA  2001.

[16] A. Parikh  L. Song  and E. Xing. A spectral algorithm for latent tree graphical models. In ICML  2011.
[17] J. Pearl. Causality: Models  Reasoning and Inference. Cambridge University Press  2001.
[18] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models.

IEEE ASSP Magazine 

3(1):4–16  January 1986.

[19] M. Redmond and A. Baveja. A data-driven software tool for enabling cooperative information sharing

among police departments. European Journal of Operational Research  141(3):660–678  2002.

[20] N. Saitou  M. Nei  et al. The neighbor-joining method: a new method for reconstructing phylogenetic

trees. Mol Biol Evol  4(4):406–425  1987.

[21] C. Semple and M.A. Steel. Phylogenetics  volume 24. Oxford University Press  USA  2003.
[22] B. W. Silverman. Density Estimation for Statistical and Data Analysis. Monographs on statistics and

applied probability. Chapman and Hall  London  1986.

[23] A.J. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A hilbert space embedding for distributions. In E.

Takimoto  editor  Algorithmic Learning Theory  Lecture Notes on Computer Science. Springer  2007.

[24] L. Song  A. Gretton  and C. Guestrin. Nonparametric tree graphical models.

In 13th Workshop on
Artiﬁcial Intelligence and Statistics  volume 9 of JMLR workshop and conference proceedings  pages
765–772  2010.

[25] Le Song  Byron Boots  Sajid Siddiqi  Geoffrey Gordon  and Alex Smola. Hilbert space embeddings of

hidden markov models. In International Conference on Machine Learning  2010.

[26] Le Song  Jonathan Huang  Alex Smola  and Kenji Fukumizu. Hilbert space embeddings of conditional

distributions. In ICML  2009.

[27] E. Sudderth  A. Ihler  W. Freeman  and A. Willsky. Nonparametric belief propagation. In CVPR  2003.
[28] N.L. Zhang. Hierarchical latent class models for cluster analysis. The Journal of Machine Learning

Research  5:697–723  2004.

9

,Alyson Fletcher
Sundeep Rangan
Maria-Florina Balcan
Hongyang Zhang