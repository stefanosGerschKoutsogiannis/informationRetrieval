2019,Reflection Separation using a Pair of Unpolarized and Polarized Images,When we take photos through glass windows or doors  the transmitted background scene is often blended with undesirable reflection. Separating two layers apart to enhance the image quality is of vital importance for both human and machine perception. In this paper  we propose to exploit physical constraints from a pair of unpolarized and polarized images to separate reflection and transmission layers. Due to the simplified capturing setup  the system becomes more underdetermined compared with existing polarization based solutions that take three or more images as input. We propose to solve semireflector orientation estimation first to make the physical image formation well-posed and then learn to reliably separate two layers using a refinement network with gradient loss. Quantitative and qualitative experimental results show our approach performs favorably over existing polarization and single image based solutions.,Reï¬‚ection Separation using a Pair of
Unpolarized and Polarized Images

Youwei Lyu1(cid:93)â€  Zhaopeng Cui2(cid:93) Si Li1âˆ— Marc Pollefeys2 Boxin Shi3 4âˆ—

1Beijing University of Posts and Telecommunications

2Department of Computer Science  ETH ZÃ¼rich

3National Engineering Laboratory for Video Technology  Peking University

4Peng Cheng Laboratory

{youweilv  zhpcui}@gmail.com  lisi@bupt.edu.cn 
marc.pollefeys@inf.ethz.ch  shiboxin@pku.edu.cn

Abstract

When we take photos through glass windows or doors  the transmitted background
scene is often blended with undesirable reï¬‚ection. Separating two layers apart
to enhance the image quality is of vital importance for both human and machine
perception. In this paper  we propose to exploit physical constraints from a pair of
unpolarized and polarized images to separate reï¬‚ection and transmission layers.
Due to the simpliï¬ed capturing setup  the system becomes more underdetermined
compared with existing polarization based solutions that take three or more images
as input. We propose to solve semireï¬‚ector orientation estimation ï¬rst to make the
physical image formation well-posed and then learn to reliably separate two layers
using a reï¬nement network with gradient loss. Quantitative and qualitative experi-
mental results show our approach performs favorably over existing polarization
and single image based solutions.

1

Introduction

Taking photos of a scene behind semireï¬‚ectors (e.g.  glass windows and doors) without reï¬‚ection
contamination is not an easy task for photographers  because the captured image often contains two
layers of the scene: the layer transmitting through the surface and the other layer reï¬‚ected by the
surface. To separate the reï¬‚ection and transmission layers is not an easy task for computer vision
researchers either  because recovering two images from a single mixture image is highly ill-posed
and the number of unknowns is twice as many as that of given measurements. Strong priors crafted
from natural image statistics (e.g.  gradient sparsity [14]) or learned from deep neural networks
(e.g.  [6]) can solve the problem if the assumed priors are well observed in the input. The problem
naturally becomes less ill-posed if multiple images are captured from different viewpoints (e.g.  ï¬ve
images in [15]) or different polarization angles (e.g.  at least three images in [12]). The motions
between the layers present in multiple images provide a strong and effective constraint  but aligning
multiple-view images contaminated by reï¬‚ections is not a trivial task [15]. Rotating a polarizer to
capture multiple images doesnâ€™t suffer from the alignment issue [12]  but it requires skillful operations
and the polarized images always ï¬lter part of the incoming light.
In this paper  we propose to separate reï¬‚ection and transmission layers using a pair of unpolarized
and polarized images. Such a setup takes fewer images than existing polarization based solutions

(cid:93)Authors contributed equally to this work.
â€ Part of this work was ï¬nished as a visiting student at Peking University.
âˆ—Corresponding authors.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

[18  12  22] and keeps an unpolarized image to maintain high light energy throughput. Directly
solving the two layers is still an ill-posed problem  but we ï¬nd that the problem has a closed-
form solution when the semireï¬‚ector surface normal is known. By assuming the semireï¬‚ector is
mostly planar  we can use only two parameters to determine the complete physical image formation
model that encodes the solution to layer separation. Based on these physical and mathematical
deductions  we propose an end-to-end deep neural network for reï¬‚ection separation using two
(un)polarized images. More speciï¬cally  we design a cascaded architecture consisting of three
modules: semireï¬‚ector orientation estimation to determine key variables for a well-posed physical
image formation model  polarization-guided separation based on the physical model  and separated
layers reï¬nement with gradient loss to enhance the sharpness. The code and test data are available at
https://github.com/YouweiLyu/reflection_separation_with_un-polarized_images.
The main contributions of this paper can be summarized as follows:

â€¢ We propose to solve reï¬‚ection separation using a pair of unpolarized and polarized images
for the ï¬rst time  which integrates polarization cues with a simpler and light-efï¬cient setup.
â€¢ We derive a new formulation based on semireï¬‚ector orientation estimation  which induces a

well-posed physical image formation model to be reliably learned for layer separation.

â€¢ We design an end-to-end deep neural network with gradient loss to solve the separation
problem and show superior performance over existing polarization and single image based
solutions.

2 Related Work

In terms of input  reï¬‚ection separation can take a single image or multiple images. The single image
problem has the most relaxed requirement  since it only needs an image captured by an ordinary
camera in the wild. But such a problem is also highly ill-posed  priors formulated using hand
crafted priors [13  14  16  19  21  1] or features learned from large-scale training data [6  20  25  24]
are explored to facilitate the separation. By taking multiple images from different viewpoints  the
difference of projected motion from reï¬‚ection and transmission layers due to the visual parallax
provides useful cues to the separation [2  8  23]. By taking multiple images under different polarization
angles  the differently polarized images provide "independent" representations of reï¬‚ection and
transmission layers based on physical image formation model to leverage the separation using
independent component analysis [7  10  3]  closed form expressions [18  12]  or deep learning [22].
Multiple images usually bring more promising separation quality than relying on only a single image 
but request more complicated and careful image capturing operations.
In terms of solutions  reï¬‚ection separation can be solved by non-learning based methods or learning
methods. Adopted priors of reï¬‚ection and transmission layers by non-learning based methods include
the sparse gradient prior [14  13]  blur level differences between two layers [16]  the ghosting effect
due to thick glass [19  5]  and the Laplacian data ï¬delity term [1]. Such handcrafted priors may get
violated in various real scenarios when expected properties are weakly observed. Learning based
methods are beneï¬ted by the comprehensive modeling ability of deep neural networks. It can be
solved by learning the gradient inference and image restoration sequentially [4  6] or concurrently
[20]  by incorporating perceptual losses [25]  and by considering bidirectional constraints [24]. With
differently polarized images available  a simple encoder-decoder architecture is shown to be effective
for separating two layers using physics based image formation model [22].
Our work belongs to the learning based approach using multiple images and physical constraints.
Different from previous works exploring polarization cues [18  12  22] that require at least three
images with different polarization angles  we take a pair of unpolarized and polarized images and
learn to solve a more underdetermined system.

3 Physical Image Formation Model

Given a pair of unpolarized and polarized images captured at the same view  we aim to separate
the reï¬‚ection layer and the transmission layer. In this section  we will ï¬rst review the reï¬‚ection
and transmission model  and then describe the relationship between polarization properties and

2

Figure 1: Illustration of physical image formation model.

semireï¬‚ector surface geometry. By assuming the medium is planar  we prove that the separation
tightly relies on only two parameters of the plane.

3.1 Reï¬‚ection and Transmission Image Formation

Suppose It  the intensity of light from the transmission scene  and Ir  the intensity of light from
the reï¬‚ection scene  are both unpolarized. After being reï¬‚ected or transmitted  the intensity of light
observed at pixel x changes depending on Î¸(x)  the angle of incidence (AoI) at the reï¬‚ected point
corresponding to pixel x  as the following [12]:

Iunpol(x) =

RâŠ¥(Î¸(x)) + R(cid:107)(Î¸(x))

2

Â· Ir(x) +

TâŠ¥(Î¸(x)) + T(cid:107)(Î¸(x))

2

Â· It(x) 

(1)

where R represents the relative strength of light reï¬‚ected off a glass surface  T represents the relative
strength of light transmitted through glass  and subscripts âŠ¥ and (cid:107) correspond to the polarized
components perpendicular and parallel to the plane of incidence (PoI)  respectively.
When we place a linear polarizer with a polarization angle Ï† in front of the camera  according to
Malusâ€™ law [9]  the intensity at pixel x is

Ipol(x) =

RâŠ¥(Î¸(x)) cos2 (Ï† âˆ’ Ï†âŠ¥(x)) + R(cid:107)(Î¸(x)) sin2 (Ï† âˆ’ Ï†âŠ¥(x))
TâŠ¥(Î¸(x)) cos2 (Ï† âˆ’ Ï†âŠ¥(x)) + T(cid:107)(Î¸(x)) sin2 (Ï† âˆ’ Ï†âŠ¥(x))

2

Â· Ir(x)+
Â· It(x) 

(2)

2

where Ï†âŠ¥(x) is the orientation of the polarizer for the best transmission of the component perpendic-
ular to the PoI. For easy representation  we denote

Î¾(x) = RâŠ¥(Î¸(x)) + R(cid:107)(Î¸(x)) 

(3)

Î¶(x) = RâŠ¥(Î¸(x)) cos2 (Ï† âˆ’ Ï†âŠ¥(x)) + R(cid:107)(Î¸(x)) sin2 (Ï† âˆ’ Ï†âŠ¥(x)).

(4)
The glass can be considered as a double-surfaced semireï¬‚ector  and we have RâŠ¥(Î¸(x))+TâŠ¥(Î¸(x)) =
1 and R(cid:107)(Î¸(x)) + T(cid:107)(Î¸(x)) = 1 for each pixel x approximately [12]. Then Equation (1) and
Equation (2) can be rewritten as

Iunpol(x) =

Î¾(x)

2

Â· Ir(x) +

2 âˆ’ Î¾(x)

2

Â· It(x) 

Ipol(x) =

Î¶(x)

2

Â· Ir(x) +

1 âˆ’ Î¶(x)

2

Â· It(x) 

3

(5)

(6)

Normal ofglass GlassPoINormal of PoIâŠ¥âˆ¥Transmission layerReflection layerCaptured imageâŠ¥âˆ¥CameraIğ¼ğ‘Ÿğ¼ğ‘¡ğœƒwhere Î¾(x) âˆˆ (0  2) and Î¶(x) âˆˆ (0  1). Given the value of Î¾(x) and Î¶(x)  the reï¬‚ection layer and the
transmission layer can be computed by

Ir(x) = 2 Â· (2 âˆ’ Î¾(x)) Â· Ipol(x) âˆ’ (1 âˆ’ Î¶(x)) Â· Iunpol(x)

2Î¶(x) âˆ’ Î¾(x)

 

(7)

It(x) = 2 Â· Î¶(x) Â· Iunpol(x) âˆ’ Î¾(x) Â· Ipol(x)

2Î¶(x) âˆ’ Î¾(x)

(8)
except for 2Î¶(x) = Î¾(x) where Ï† âˆ’ Ï†âŠ¥(x) = Â±45â—¦ or Â±135â—¦. The angle of a polarizer Ï† can be
measured by calibration. Associated with surface geometry of semireï¬‚ector  Ï†âŠ¥(x) is not constant
but spatially varying over the whole image plane. There may exist trivial Ï† âˆ’ Ï†âŠ¥(x) corresponding
to a few pixels  which have negligible effect on the separation.
In short  the reï¬‚ection layer Ir(x) and the transmission layer It(x) are determined by Î¾(x) and Î¶(x)
when a pair of unpolarized and polarized images are given.

 

3.2 Semireï¬‚ector Surface Geometry

In order to recover the reï¬‚ection layer Ir and the transmission layer It  we ï¬rst have to solve Î¾(x)
and Î¶(x) according to Equations (5) and (6)  which can be further computed by Î¸(x) and Ï† âˆ’ Ï†âŠ¥(x)
according to Equations (3) and (4). In this section  we will describe how we compute Î¸(x) and
Ï† âˆ’ Ï†âŠ¥(x) for each pixel given the surface normal of the semireï¬‚ector and camera parameters.
We assume the semireï¬‚ector has a planar surface  and the camera coordinate is the same as the world
coordinate. Then the semireï¬‚ector plane can be expressed as

sin Î± Â· x âˆ’ cos Î± sin Î² Â· y + cos Î± cos Î²(z âˆ’ z0) = 0 

(9)
where Î± represents the rotation angle around y-axis and Î² represents the angle around x-axis. The
plane normal is thus given by

(cid:35)(cid:34) cos Î±

(cid:35)(cid:34) 0

(cid:35)

(cid:34)

nglass =

0

0

0 cos Î² âˆ’ sin Î²
cos Î²
0

sin Î²

0
1
âˆ’ sin Î± 0

0

sin Î±

0

cos Î±

=

0
1

sin Î±

âˆ’ cos Î± sin Î²
cos Î± cos Î²

(cid:35)

.

(10)

(cid:34) 1

Let f be the focal length of the camera  and (px  py) be the coordinate of the principal point. For the
pixel x located at (u  v) on the image plane  we can easily compute its corresponding 3D point X on
the medium plane as

(cid:35)

(cid:34) u âˆ’ px

v âˆ’ py

f

X =

f cos Î± cos Î² + (u âˆ’ px) sin Î± âˆ’ (v âˆ’ py) cos Î± sin Î²

z0 cos Î± cos Î²

.

(11)

Let X = X/(cid:107)X(cid:107)  then the AoI corresponding to pixel x can be calculated as

Î¸(x) = arccos(cid:12)(cid:12)nglass Â· X(cid:12)(cid:12).

(12)
We calculate the absolute value for the above term since Î¸(x) âˆˆ [0  90â—¦]. The normal of PoI
nP oI = (xP oI   yP oI   zP oI )

(cid:62) is then calculated as

nP oI = nglass Ã— X 
and the projection of nP oI on imaging plane is (xP oI   yP oI )
Ï†âŠ¥(x) âˆˆ [0  360â—¦)  we have

(13)
(cid:62) denoting orientation of Ï†âŠ¥(x). For

Ï†âŠ¥(x) = arctan

.

(14)

yP oI
xP oI

We combine the reï¬‚ection and transmission image formation and semireï¬‚ector surface geometry
to compute Ï†âŠ¥(x) and Î¸(x) for each pixel. Note they are not affected by z0  because physically
the transparent plane can be projected to parallel plane with arbitrary intercept about z-axis and
mathematically before computing arctan and arccos  z0 has been eliminated according to Equations
(12) and (14).
In short  it is the normal of glass that matters  and we only need to estimate coefï¬cients Î± and Î² to
determine the semireï¬‚ector plane.

4

Figure 2: Our method takes a cascaded architecture with three modules: semireï¬‚ector orientation
estimation  polarization-guided separation  and separated layers reï¬nement.

4 Reï¬‚ection Separation Network

In this section  we introduce the proposed reï¬‚ection separation network which makes use of physical
model discussed in Section 3  and details about loss function and network training.

4.1 Network Architecture

As shown in Figure 2  our network takes a cascaded architecture which consists of three modules:
semireï¬‚ector orientation estimation  polarization-guided separation  and separated layers reï¬nement.
Taking a pair of unpolarized and polarized images  the semireï¬‚ector orientation module aims to
predict coefï¬cients of the glass plane  i.e.  Î± and Î². As we only need to estimate two parameters  the
pose estimation module is pretty light  and consists of seven convolutional layers followed by two fully
connected layers. The polarization-guided separation module takes Î± and Î² as inputs  and computes
the reï¬‚ection layer Ë†Ir and transmission layer Ë†It. This module only relies on the physical image
formation model in Section 3 using analytic equations  so we do not have any parameters to learn
here. The separated layers using equations may not be satisfactory due to the gap between physical
model and real data. The numerical problem also occurs when the denominators in Equation (7) and
Equation (8) approach zero  and the computed results are degenerated. Fortunately  this happens only
for a few pixels and the remaining non-degenerated calculations can guide a reï¬nement network to
produce compelling separation results. We therefore further feed Ë†Ir and Ë†It with original input images
and Î¾  Î¶ into the separated layers reï¬nement module to improve the initial estimation. The reï¬nement
module has a widely adopted encoder-decoder architecture. In detail  the encoder consists of eight
convolutional layers and the decoder consists of ï¬ve deconvolutional layers.
We hope the network to reconstruct details in the original image as many as possible  so we deï¬ne
the loss on both the estimated image and its gradient as:

L = Î»1Lr(Ir) + Î»2Lt(It) + Î»3Lr(gr) + Î»4Lt(gt) 

(15)
where Lr(Ir) and Lt(It) deï¬ne the loss on the estimated reï¬‚ection and transmission layers  Lr(gr)
and Lt(gt) deï¬ne the loss on the gradients of the estimated reï¬‚ection and transmission layers  and
Î»1 2 3 4 are the weighting parameters. The mean square error (MSE) is used for all the loss. We
implement our model using PyTorch deep learning framework [17]. Adam [11] is used as the
optimizer with a starting learning rate of 0.0004  Î²1 = 0.9 and Î²2 = 0.999. The learning rate is
descended to 0.0002 and 0.00008 after 12th and 18th epochs respectively. Î»1 2 3 4 are set to be 1.2 
1.5  1.0  and 1.5 respectively for our training.

4.2 Training Data Generation

The deep-learning method tends to be data-hungry  but it is difï¬cult to obtain pairwise reï¬‚ection and
transmission images with both polarized and unpolarized observations at a large scale. It is possible
to directly use Equation (1) and Equation (2) to generate the synthetic data  but it is expected that the

5

Encoder InputUnpolarized imagePolarized imageğœ‰ğœƒOutputReflection layerTransmission layerEncoderDecoderğœ·ğœ¶Fully connected layerğœ™âŠ¥ğœáˆ˜ğ¼ğ‘Ÿáˆ˜ğ¼ğ‘¡Semireflectororientation estimationPolarization-guided separationSeparated layers refinementTable 1: Quantitative evaluation results on synthetic data.

Ours

SSIM 0.9708
28.23
PSNR
SSIM 0.8953
20.92
PSNR

Ours-
Initial
0.8324
21.61

0.6253
13.90

Reï¬‚ectNet-
Finetuned

0.9627
27.52

0.8303
18.50

Ours-

2% noise
0.9691
28.08

0.8785
20.53

Ours-

8% noise
0.9668
27.31

0.8418
19.18

Ours-

16% noise

0.9619
27.17

0.8022
18.26

Transmission

Reï¬‚ection

network trained with such data may not generalize well on real scenarios. Therefore  we propose an
effective data generation pipeline to better match images of real-world scenes.
At the ï¬rst step  we randomly pick two images from PLACE2 dataset [26] as original reï¬‚ection and
transmission layers. Based on a commonly adopted assumption that people take photos focusing on
the background scene (transmission layer) so the reï¬‚ection layer is likely to be blurry [6]  a Gaussian
smoothing kernel with a random kernel size in the range of 3 to 7 pixels is applied to a portion of
reï¬‚ection images. We also need to simulate coefï¬cients Î± and Î² of the semireï¬‚ector plane. We
assume people rarely take photos in front of the glass that inclines by a weird angle  e.g.  glass nearly
orthogonal to the image plane  so we set Î± âˆˆ (âˆ’65â—¦  65â—¦) and Î² âˆˆ (âˆ’35â—¦  35â—¦). For the virtual
camera  we set the focal length as 1.4 times as long as the image width  and the image resolution as
256 Ã— 256. By ï¬xing these factors  the normal of glass is speciï¬ed  Î¸(x) and Ï†âŠ¥(x) can be derived
from Equation (12) and Equation (14)  respectively. Ï† can be an arbitrary value in the range of [0  2Ï€) 
as long as the polarization images are captured under the same polarizer angle. In our experiment  we
set Ï† to be 0. Additionally  real-world scenes are generally high-dynamic-range (HDR)  so we apply
dynamic range manipulation as conducted in [22] to simulate appearance of reï¬‚ections in a more
realistic manner. Finally  the synthetic unpolarized image Iunpol and the polarized image Ipol can be
obtained by Equation (5) and Equation (6).

5 Experimental Results

We evaluate our method on both synthetic and real data with extensive experiments including the
comparison with related work and ablation study. For all quantitative evaluations  both the peak-
signal-to-noise ratio (PSNR) and the structural similarity index measure (SSIM) are used to evaluate
the quality of separated images.

5.1 Evaluation on Synthetic Data

We use 5000 pairs of images from our synthetic validation dataset with ground truth reï¬‚ection
and transmission layers to quantitatively compare our method with state-of-the-art approaches.
Reï¬‚ectNet [22] is a learning based method using three polarized images; Zhang et al. [25]  CRRN
[20]  CEILNet [6] are deep learning based solutions using a single image; and LB14 [16] is a non-
learning method using a single image. To test the performance of Reï¬‚ectNet [22]  we generated two
additional polarization images for each pair of (un)polarized images in our dataset  and ï¬netuned
Reï¬‚ectNet using Adam solver with a learning rate of 0.005 for 5 epochs. The experimental results
are shown in Figure 3 and Table 1. We can see that  compared to all single-image based methods 
our method has much better performance  which shows the advantage with the additional polarized
image. We can also see that all single-image based methods have bad performance for reï¬‚ection
layers1 due to their weak signal in the input images. Our method also outperforms Reï¬‚ectNet [22]
which requires three polarized images as input  especially in terms of the quality of the reï¬‚ection
layer  although our method only needs one polarized image in addition to an unpolarized image.
Moreover  our method performs the best in suppressing undesired reï¬‚ection in transmission layer
and recovers high-quality reï¬‚ection layer as well  as indicated by corresponding SSIM and PSNR
values under images in Figure 3. We also evaluate our initial polarization-guided separation Ë†Ir and Ë†It
("Ours-Initial") in Table 1  and we can see that the initial separation is effective  and our reï¬nement
network helps eliminate the artifact and noise caused by rough estimation of Î¾ and Î¶. At last  we test

1Brightness is upgraded for visualization purpose.

6

Figure 3: Quantitative and qualitive evaluation on synthetic data  compared with Reï¬‚ectNet [22] 
Zhang et al. [25]  CRRN [20]  CEILNet [6]  and LB14 [16].

7

OursGTInput imagesSSIM:0.8049PSNR:17.99SSIM:0.04383PSNR:4.680SSIM:0.4811PSNR:13.07SSIM:0.1249PSNR:4.511SSIM:0.8990PSNR:18.44SSIM:0.1758PSNR:7.826Zhang et al.SSIM:0.7352PSNR:15.64SSIM:0.8049PSNR:17.99SSIM:0.6100PSNR:13.67SSIM:0.00047PSNR:3.153SSIM:0.7464PSNR:16.64SSIM:0.00324PSNR:6.686LB14SSIM:0.7438PSNR:4.321SSIM:0.00152PSNR:4.321SSIM:0.5368PSNR:13.33SSIM:0.0117PSNR:3.337SSIM:0.7876PSNR:16.72SSIM:0.06656PSNR:7.223CRRNSSIM:0.7664PSNR:18.17SSIM:0.00015PSNR:3.565SSIM:0.5682PSNR:12.39SSIM:0.00035PSNR:2.590SSIM:0.8454PSNR:18.55SSIM:0.00049PSNR:5.887CEILNetReflectNetSSIM:0.9655PSNR:25.86SSIM:0.9721PSNR:22.10SSIM:0.8466PSNR:19.26SSIM:0.9655PSNR:20.24SSIM:0.9708PSNR:27.58SSIM:0.9695PSNR:24.01SSIM:0.4797PSNR:13.89SSIM:0.8130PSNR:14.64SSIM:0.5289PSNR:14.86SSIM:0.6890PSNR:10.69SSIM:0.7744PSNR:18.52SSIM:0.7209PSNR:16.29ReflectionReflectionReflectionTransmission TransmissionTransmissionUnpolarized imgPolarized imgUnpolarized imgPolarized imgUnpolarized imgPolarized imgSSIM:0.8858PSNR:21.31SSIM:0.7956PSNR:14.44SSIM:0.7106PSNR:16.97SSIM:0.9416PSNR:18.15SSIM:0.9016PSNR:20.56SSIM:0.9271PSNR:15.57InitialFigure 4: Qualitative evaluation on real data  compared with Reï¬‚ectNet [22]  Zhang et al. [25]  CRRN
[20]  CEILNet [6]  and LB14 [16].

our method against Gaussian noise added to images with different standard deviations. The results are
shown in Table 1. We can see that our method performs consistently well and is robust to Gaussian
noise.

5.2 Evaluation on Real Data

We use the Lucid Vision Phoenix polarization camera1 to capture the real dataset. The polarization
camera can take four images with different polarizer angles at a single shot. We use three of them
as input images to Reï¬‚ectNet [22] and one of them as polarized input image to our method. The
unpolarized input image is calculated by summing two polarized images captured with orthogonal
polarizer angles [9]. Note the polarization camera has no color ï¬lter  so we can only provide results in
gray scale  as displayed in Figure 42. These scenes contain strong reï¬‚ections with complex textures 
and all single-image based methods fail to recover the transmissions while removing the reï¬‚ections.
Thanks to the polarimetric cues  both Reï¬‚ectNet [22] and our method show obvious advantage over

1https://thinklucid.com/product/phoenix-5-0-mp-polarized-model/
2For better visualization  the minimum and maximum intensity values of different algorithms are stretched in

a consistent range.

8

OursInput imagesZhang et al.LB14CRRNCEILNetReflectNetReflection Reflection Reflection Transmission Transmission Transmission Polarized imgUnpolarized imgPolarized imgUnpolarized imgPolarized imgUnpolarized imgTable 2: Quantitative evaluation results in ablation study.

Ours

SSIM 0.9708
28.23
PSNR
SSIM 0.8953
PSNR
20.92

W/o
Î¾ & Î¶
0.9632
27.38

0.8721
20.02

Reï¬‚ectNet-
reï¬nement

0.9594
27.20

0.8084
18.30

W/o

ori. est.
0.9647
27.47

0.8015
18.84

W/o

grad. loss
0.9674
27.82
0.9131
21.94

Ours-

Parabola
0.8846
24.40

0.4833
13.69

Transmission

Reï¬‚ection

single-image based methods. Compared to Reï¬‚ectNet [22]  our method shows stronger capability in
suppressing the ghost in transmission and clear extraction of the reï¬‚ection layer.

5.3 Ablation Study

We ï¬rst verify the contribution of semireï¬‚ector orientation estimation by directly estimating Î¾(x)
and Î¶(x) from the network (without inferring Î± and Î² ï¬rst). In other words  we also use an encoder-
decoder architecture to estimate Î¾(x) and Î¶(x) directly from a given pair of (un)polarized images.
SSIM and PSNR averaged over 5000 validation images are shown in "W/o ori. est." column of Table 2.
From Table 2  we can see that  with more prior knowledge encoded in the network  the orientation
estimation with only two parameters is easier to learn and also better than directly estimating Î¾(x)
and Î¶(x) for each pixel.
We further evaluate different loss functions  and train our network without the gradient loss. The
results are listed in "W/o grad. loss" column of Table 2. We ï¬nd the gradient loss is particularly useful
in improving the quality of transmission layer estimation (background scene with more interests) 
though it may hurt the accuracy of reï¬‚ection layer (usually treated as noise to be removed [16]).
In order to compare our method with Reï¬‚ectNet [22] thoroughly  we remove Î¾ and Î¶ from the input
of our reï¬nement network  and feed the results of Reï¬‚ectNet and our polarization-guided separation
into this reï¬nement network. Under this setup  the quantitative results of reï¬‚ection and transmission
are listed in "Reï¬‚ectNet-reï¬nement" and "W/o Î¾ & Î¶" columns of Table 2. We can see that even with
this reï¬nement Reï¬‚ectNet still performs worse than our full pipeline. It also shows the importance of
feeding Î¾ and Î¶ into the reï¬nement network.
Our model assumes the semireï¬‚ector approximately has a planar shape. When it becomes a curved
shape such as the windshield in a car  our semireï¬‚ector orientation estimation module will fail  and
thus the performance of our method will deteriorate. We generate the test data using the parabola
surface simulation as Reï¬‚ectNet  and directly test using our current model. The result is listed
in Table 2. We can see that the performance becomes much worse especially for the reï¬‚ection.
The performance might be improved if we modify the semireï¬‚ector orientation estimation module
accordingly  and we will consider this as our future work.

6 Conclusion

We solved the problem of integrating polarimetric constraints from a pair of unpolarized and polarized
images to separate reï¬‚ection and transmission layers. To deal with the ill-posedness introduced
by using fewer polarized images  we derived a semireï¬‚ector orientation constraint to make the
physical image formation for layer separation valid given our setup  and trained a neural network
to successfully separate two layers  showing state-of-the-art performance. Our simple yet unique
capturing setup not only explored polarimetric constraints for separating reï¬‚ection and transmission
layers as reliably as existing approaches using three or more polarized images  but also could be
potentially integrated into smart phones without affecting the original photography quality by not
making all images polarized.

Acknowledgments

This work was supported by the National Natural Science Foundation of China under Grant 61872012
and 61702047.

9

References
[1] N. Arvanitopoulos  R. Achanta  and S. SÃ¼sstrunk. Single image reï¬‚ection suppression. In Proc. CVPR 

2017.

[2] E. Beâ€™Ery and A. Yeredor. Blind separation of superimposed shifted images using parameterized joint

diagonalization. IEEE TIP  17(3):340â€“353  2008.

[3] A. M. Bronstein  M. M. Bronstein  M. Zibulevsky  and Y. Y. Zeevi. Sparse ICA for blind separation of
transmitted and reï¬‚ected images. International Journal of Imaging Systems and Technology  15(1):84â€“91 
2005.

[4] P. Chandramouli  M. Noroozi  and P. Favaro. Convnet-based depth estimation  reï¬‚ection separation and

deblurring of plenoptic images. In Proc. ACCV  2016.

[5] Y. Diamant and Y. Y. Schechner. Overcoming visual reverberations. In Proc. CVPR  2008.

[6] Q. Fan  J. Yang  G. Hua  B. Chen  and D. Wipf. A generic deep architecture for single image reï¬‚ection

removal and image smoothing. In Proc. ICCV  2017.

[7] H. Farid and E. H. Adelson. Separating reï¬‚ections and lighting using independent components analysis. In

Proc. CVPR  1999.

[8] K. Gai  Z. Shi  and C. Zhang. Blind separation of superimposed moving images using image statistics.

IEEE TPAMI  34(1):19â€“32  2012.

[9] E. Hecht. Optics. Pearson education. Addison-Wesley  2002.

[10] Hermanto  A. K. D. B. Filho  T. Yamamura  and N. Ohnishi. Separating virtual and real objects using
independent component analysis. IEICE Transactions on Information and Systems  84(9):1241â€“1248 
2001.

[11] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 

2014.

[12] N. Kong  Y. Tai  and J. S. Shin. A physically-based approach to reï¬‚ection separation: from physical

modeling to constrained optimization. IEEE TPAMI  36(2):209â€“221  2014.

[13] A. Levin and Y. Weiss. User assisted separation of reï¬‚ections from a single image using a sparsity prior.

In Proc. ECCV  2004.

[14] A. Levin and Y. Weiss. User assisted separation of reï¬‚ections from a single image using a sparsity prior.

IEEE TPAMI  29(9):1647â€“1654  2007.

[15] Y. Li and M. S. Brown. Exploiting reï¬‚ection change for automatic reï¬‚ection removal. In Proc. ICCV 

2013.

[16] Y. Li and M. S. Brown. Single image layer separation using relative smoothness. In Proc. CVPR  2014.

[17] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison  L. Antiga  and

A. Lerer. Automatic differentiation in PyTorch. In Proc. NIPS Autodiff Workshop  2017.

[18] Y. Y. Schechner  J. Shamir  and N. Kiryati. Polarization and statistical analysis of scenes containing a

semireï¬‚ector. Journal of the Optical Society of America  17(2):276â€“284  2000.

[19] Y. Shih  D. Krishnan  F. Durand  and W. T. Freeman. Reï¬‚ection removal using ghosting cues. In Proc.

CVPR  2015.

[20] R. Wan  B. Shi  L.-Y. Duan  A.-H. Tan  and A. C. Kot. Crrn: Multi-scale guided concurrent reï¬‚ection

removal network. In Proc. CVPR  2018.

[21] R. Wan  B. Shi  A. H. Tan  and A. C. Kot. Depth of ï¬eld guided reï¬‚ection removal. In Proc. ICIP  2016.

[22] P. Wieschollek  O. Gallo  J. Gu  and J. Kautz. Separating reï¬‚ection and transmission images in the wild. In

Proc. ECCV  2018.

[23] T. Xue  M. Rubinstein  C. Liu  and W. T. Freeman. A computational approach for obstruction-free

photography. ACM TOG  34(4):79:1â€“79:11  2015.

[24] J. Yang  D. Gong  L. Liu  and Q. Shi. Seeing deeply and bidirectionally: A deep learning approach for

single image reï¬‚ection removal. In Proc. ECCV  2018.

10

[25] X. Zhang  R. Ng  and Q. Chen. Single image reï¬‚ection separation with perceptual losses. In Proc. CVPR 

2018.

[26] B. Zhou  A. Lapedriza  A. Khosla  A. Oliva  and A. Torralba. Places: A 10 million image database for

scene recognition. IEEE TPAMI  40(6):1452â€“1464  2017.

11

,Youwei Lyu
Zhaopeng Cui
Si Li
Marc Pollefeys
Boxin Shi