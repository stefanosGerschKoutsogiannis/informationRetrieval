2008,Rademacher Complexity Bounds for Non-I.I.D. Processes,This paper presents the first data-dependent generalization bounds for non-i.i.d. settings based on the notion of Rademacher complexity. Our bounds extend to the non-i.i.d. case existing Rademacher complexity bounds derived for the i.i.d. setting. These bounds provide a strict generalization of the ones found in the i.i.d. case  and can also be used within the standard i.i.d. scenario. They apply to the standard scenario of beta-mixing stationary sequences examined in many previous studies of non-i.i.d. settings and benefit form the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular  they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from finite samples and lead to tighter bounds.,Rademacher Complexity Bounds

for Non-I.I.D. Processes

Mehryar Mohri

Courant Institute of Mathematical Sciences

and Google Research

251 Mercer Street

New York  NY 10012
mohri@cims.nyu.edu

Afshin Rostamizadeh

Department of Computer Science

Courant Institute of Mathematical Sciences

251 Mercer Street

New York  NY 10012
rostami@cs.nyu.edu

Abstract

This paper presents the ﬁrst Rademacher complexity-based error bounds for non-
i.i.d. settings  a generalization of similar existing bounds derived for the i.i.d. case.
Our bounds hold in the scenario of dependent samples generated by a stationary
β-mixing process  which is commonly adopted in many previous studies of non-
i.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity
over other measures of the complexity of hypothesis classes. In particular  they are
data-dependent and measure the complexity of a class of hypotheses based on the
training sample. The empirical Rademacher complexity can be estimated from
such ﬁnite samples and lead to tighter generalization bounds. We also present
the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and
brieﬂy study their convergence.

1 Introduction

Most learning theory models such as the standard PAC learning framework [13] are based on the as-
sumption that sample points are independently and identically distributed (i.i.d.). The design of most
learning algorithms also relies on this key assumption. In practice  however  the i.i.d. assumption
often does not hold. Sample points have some temporal dependence that can affect the learning pro-
cess. This dependence may appear more clearly in times series prediction or when the samples are
drawn from a Markov chain  but various degrees of time-dependence can also affect other learning
problems.

A natural scenario for the analysis of non-i.i.d. processes in machine learning is that of observations
drawn from a stationary mixing sequence  a standard assumption adopted in most previous studies 
which implies a dependence between observations that diminishes with time [7 9 10 14 15]. The pi-
oneering work of Yu [15] led to VC-dimension bounds for stationary β-mixing sequences. Similarly 
Meir [9] gave bounds based on covering numbers for time series prediction [9]. Vidyasagar [14]
studied the extension of PAC learning algorithms to these non-i.i.d. scenarios and proved that under
some sub-additivity conditions  a PAC learning algorithm continues to be PAC for these settings.
Lozano et al. studied the convergence and consistency of regularized boosting under the same as-
sumptions [7]. Generalization bounds have also been derived for stable algorithms with weakly
dependent observations [10]. The consistency of learning under the more general scenario of α-
mixing with non-stationary sequences has also been studied by Irle [3] and Steinwart et al. [12].

This paper gives data-dependent generalization bounds for stationary β-mixing sequences. Our
bounds are based on the notion of Rademacher complexity. They extend to the non-i.i.d. case the
Rademacher complexity bounds derived in the i.i.d. setting [2  4  5]. To the best of our knowledge 
these are the ﬁrst Rademacher complexity bounds derived for non-i.i.d. processes. Our proofs make

1

use of the so-called independent block technique due to Yu [15] and Bernstein and extend the appli-
cability of the notion of Rademacher complexity to non-i.i.d. cases.

Our generalization bounds beneﬁt from all the advantageous properties of Rademacher complexity
as in the i.i.d. case. In particular  since the Rademacher complexity can be bounded in terms of
other complexity measures such as covering numbers and VC-dimension [1]  it allows us to derive
generalization bounds in terms of these other complexity measures  and in fact improve on existing
bounds in terms of these other measures  e.g.  VC-dimension. But  perhaps the most crucial advan-
tage of bounds based on the empirical Rademacher complexity is that they are data-dependent: they
measure the complexity of a class of hypotheses based on the training sample and thus better capture
the properties of the distribution that has generated the data. The empirical Rademacher complex-
ity can be estimated from ﬁnite samples and lead to tighter bounds. Furthermore  the Rademacher
complexity of large hypothesis sets such as kernel-based hypotheses  decision trees  convex neu-
ral networks  can sometimes be bounded in some speciﬁc ways [2]. For example  the Rademacher
complexity of kernel-based hypotheses can be bounded in terms of the trace of the kernel matrix.

In Section 2  we present the essential notion of a mixing process for the discussion of learning in
non-i.i.d. cases and deﬁne the learning scenario. Section 3 introduces the idea of independent blocks
and proves a bound on the expected deviation of the error from its empirical estimate. In Section 4 
we present our main Rademacher generalization bounds and discuss their properties.

2 Preliminaries

This section introduces the concepts needed to deﬁne the non-i.i.d. scenario we will consider  which
coincides with the assumptions made in previous studies [7  9  10  14  15].

2.1 Non-I.I.D. Distributions

The non-i.i.d. scenario we will consider is based on stationary β-mixing processes.
Deﬁnition 1 (Stationarity). A sequence of random variables Z = {Zt}∞
t=−∞ is said to be sta-
tionary if for any t and non-negative integers m and k  the random vectors (Zt  . . .   Zt+m) and
(Zt+k  . . .   Zt+m+k) have the same distribution.

Thus  the index t or time  does not affect the distribution of a variable Zt in a stationary sequence
(note that this does not imply independence).
Deﬁnition 2 (β-mixing). Let Z = {Zt}∞
t=−∞ be a stationary sequence of random variables. For
any i  j ∈ Z ∪ {−∞  +∞}  let σj
i denote the σ-algebra generated by the random variables Zk 
i ≤ k ≤ j. Then  for any positive integer k  the β-mixing coefﬁcient of the stochastic process Z is
deﬁned as
(1)

β(k) = sup

E
B∈σn

n

−∞h sup

A∈σ∞

n+k(cid:12)(cid:12)(cid:12)Pr[A | B] − Pr[A](cid:12)(cid:12)(cid:12)i.

Z is said to be β-mixing if β(k) → 0. It is said to be algebraically β-mixing if there exist real
numbers β0 > 0 and r > 0 such that β(k) ≤ β0/kr for all k  and exponentially mixing if there
exist real numbers β0 and β1 such that β(k) ≤ β0 exp(−β1kr) for all k.
Thus  a sequence of random variables is mixing when the dependence of an event on those occurring
k units of time in the past weakens as a function of k.

2.2 Rademacher Complexity

Our generalization bounds will be based on the following measure of the complexity of a class of
functions.
Deﬁnition 3 (Rademacher Complexity). Given a sample S ∈ X m  the empirical Rademacher
complexity of a set of real-valued functions H deﬁned over a set X is deﬁned as follows:

bRS(H) =

2
m

E

σ(cid:20) sup
h∈H(cid:12)(cid:12)(cid:12)

mXi=1

σih(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)S = (x1  . . .   xm)(cid:21).

2

(2)

The expectation is taken over σ = (σ1  . . .   σn) where σis are independent uniform random vari-
ables taking values in {−1  +1} called Rademacher random variables. The Rademacher complexity
of a hypothesis set H is deﬁned as the expectation of bRS(H) over all samples of size m:

Rm(H) = E

(3)

S(cid:2)bRS(H)(cid:12)(cid:12)|S| = m(cid:3).

The deﬁnition of the Rademacher complexity depends on the distribution according to which sam-
ples S of size m are drawn  which in general is a dependent β-mixing distribution D. In the rare

indicate that distribution as a superscript: R

instances where a different distribution eD is considered  typically for an i.i.d. setting  we explicitly

The Rademacher complexity measures the ability of a class of functions to ﬁt noise. The empirical
Rademacher complexity has the added advantage that it is data-dependent and can be measured from
ﬁnite samples. This can lead to tighter bounds than those based on other measures of complexity
such as the VC-dimension [2  4  5].

eD
m(H).

tation over a sample S drawn according to a stationary β-mixing distribution:

We will denote by bRS(h) the empirical average of a hypothesis h : X → R and by R(h) its expec-

1
m

mXi=1

bRS(h) =

h(zi)

R(h) = E
S

[bRS(h)].

The following proposition shows that this expectation is independent of the size of the sample S  as
in the i.i.d. case.
Proposition 1. For any sample S of size m drawn from a stationary distribution D  the following
holds: ES∼Dm[bRS(h)] = Ez∼D[h(z)].
Proof. Let S = (x1  . . .   xm). By stationarity  Ezi∼D[h(zi)] = Ezj ∼D[h(zj)] for all 1 ≤ i  j ≤ m 
thus  we can write:
mXi=1

[h(zi)] = E
z

mXi=1

[h(zi)] =

[h(z)].

1
m

1
m

E
zi

E
S

E
S

[bRS(h)] =

3 Proof Components

Our proof makes use of McDiarmid’s inequality [8] to show that the empirical average closely
estimates its expectation. To derive a Rademacher generalization bound  we apply McDiarmid’s
inequality to the following random variable  which is the quantity we wish to bound:

(4)

(5)

Φ(S) = sup
h∈H

R(h) − bRS(h).

McDiarmid’s inequality bounds the deviation of Φ from its mean  thus  we must also bound the
expectation E[Φ]. However  we immediately face two obstacles: both McDiarmid’s inequality and
the standard bound on E[Φ] hold only for samples drawn in an i.i.d. fashion. The main idea behind
our proof is to analyze the non-i.i.d. setting and transfer it to a close independent setting. The
following sections will describe in detail our solution to these problems.

3.1 Independent Blocks

We derive Rademacher generalization bounds for the case where training and test points are drawn
from a stationary β-mixing sequence. As in previous non-i.i.d. analyses [7  9  10  15]  we use a
technique transferring the original problem based on dependent points to one based on a sequence
of independent blocks. The method consists of ﬁrst splitting a sequence S into two subsequences S0
and S1  each made of µ blocks of a consecutive points. Given a sequence S = (z1  . . .   zm) with
m = 2aµ  S0 and S1 are deﬁned as follows:

S0 = (Z1  Z2  . . .   Zµ) 
S1 = (Z (1)

1   Z (1)

2   . . .   Z (1)

µ ) 

where Zi = (z(2i−1)+1  . . .   z(2i−1)+a) 
where Z (1)

i = (z2i+1  . . .   z2i+a).

(6)

(7)

3

same distribution as in Zk. As stated by the following result of Yu [15][Corollary 2.7]  for a sufﬁ-
ciently large spacing a between blocks and a sufﬁciently fast mixing distribution  the expectation of

Instead of the original sequence of odd blocks S0  we will be working with a sequence eS0 of
independent blocks of equal size a to which standard i.i.d. techniques can be applied: eS0 =
(eZ1 eZ2  . . .  eZµ) with mutually independent eZks  but  the points within each block eZk follow the
a bounded measurable function h is essentially unchanged if we work with eS0 instead of S0.
Corollary 1 ([15]). Let h be a measurable function bounded by M ≥ 0 deﬁned over the blocks Zk 
then the following holds:
(8)

| E

S0

[h] − E
eS0

[h]| ≤ (µ − 1)M β(a) 

where ES0 denotes the expectation with respect to S0  E eS0

the expectation with respect to the eS0.
We denote by eD the distribution corresponding to the independent blocks eZk. Also  to work with
block sequences  we extend some of our deﬁnitions: we deﬁne the extension ha : Z a → R of any
aPa
hypothesis h∈ H to a block-hypothesis by ha(B) = 1
i=1 h(Zi) for any block B = (z1  . . .   za)∈
Z a  and deﬁne Ha as the set of all block-based hypotheses ha generated from h∈ H.
It will also be useful to deﬁne the subsequence Sµ  which consists of µ singleton points separated
by a gap of 2a − 1 points. This can be thought of as the sequence constructed from S0  or S1  by
selecting only the jth point from each block  for any ﬁxed j ∈ {1  . . .   a}.
3.2 Concentration Inequality

McDiarmid’s inequality requires the sample to be i.i.d. Thus  we ﬁrst show that Pr[Φ(S)] can be
bounded in terms of independent blocks and then apply McDiarmid’s inequality to the independent
blocks.
Lemma 1. Let H be a set of hypotheses bounded by M . Let S denote a sample  of size m  drawn

according to a stationary β-mixing distribution and let eS0 denote a sequence of independent blocks.

Then  for all a  µ  ǫ > 0 with 2µa = m and ǫ > E eS0

[Φ(eS0)]  the following bound holds:
[Φ(eS0)] > ǫ′] + 2(µ − 1)β(a) 

[Φ(eS0) − E

eS0

where ǫ′ = ǫ − E eS0
Proof. We ﬁrst rewrite the left-hand side probability in terms of even and odd blocks and then apply
Corollary 1 as follows:

[Φ(eS0)].

Pr
S

[Φ(S) > ǫ] ≤ 2 Pr
eS0

Pr
S

[Φ(S) > ǫ] = Pr
S

h

[sup

(R(h) − bRS(h)) > ǫ]
Shsup
h (cid:16) R(h)− bRS0 (h)
Sh 1
2(cid:16)sup
(R(h) − bRS0(h)) + sup

[Φ(S0) + Φ(S1) > 2ǫ]

+ R(h)− bRS1 (h)

h

h

2

2

(cid:17) > ǫi
(R(h) − bRS1(h))(cid:17) > ǫi

[Φ(S0) > ǫ] + Pr
S1

[Φ(S1) > ǫ]

[Φ(S0) > ǫ]

= Pr

≤ Pr
= Pr
S

S0

≤ Pr
= 2 Pr
S0

(def. of bRS(h))

(convexity of sup)

(def. of Φ)

(union bound)

(stationarity)

= 2 Pr
S0

[Φ(S0) − E
eS0

[Φ(eS0)] > ǫ′].
The second inequality holds by the union bound and the fact that Φ(S0) or Φ(S1) must surpass ǫ
for their sum to surpass 2ǫ. To complete the proof  we apply Corollary 1 to the expectation of the
indicator variable of the event {Φ(S0) − E eS0
[Φ(eS0)] > ǫ′] ≤ 2 Pr

[Φ(eS0)] > ǫ′}  which yields
[Φ(eS0) − E

[Φ(eS0)] > ǫ′] + 2(µ − 1)β(a).

We can now apply McDiarmid’s inequality to the independent blocks of Lemma 1.

[Φ(S0) − E
eS0

(def. of ǫ′)

2 Pr
S0

eS0

eS0

4

Pr
S

[Φ(eS0)].

Proposition 2. For the same assumptions as in Lemma 1  the following bound holds for all ǫ >
E eS0

[Φ(eS0)]:
where ǫ′ = ǫ − E eS0
Proof. To apply McDiarmid’s inequality  we view each block as an i.i.d. point with respect to ha.

[Φ(S) > ǫ] ≤ 2 exp(cid:18)−2µǫ′2

M 2 (cid:19) + 2(µ − 1)β(a) 

µPµ
k=1 ha(eZk).
Φ(eS0) can be written in terms of ha as: Φ(eS0) = R(ha) − bR eS0
µ|h(eZk)| ≤ M/µ. By
Thus  changing a block eZk of the sample eS0 can change Φ(eS0) by at most 1
McDiarmid’s inequality  the following holds for any ǫ > 2(µ − 1)M β(a):
i=1(M/µ)2(cid:19) = exp(cid:18)−2µǫ′2
M 2 (cid:19) .
−2ǫ′2
Pµ

[Φ(eS0)] > ǫ′] ≤ exp(cid:18)

Plugging in the right-hand side in the statement of Lemma 1 proves the proposition.

[Φ(eS0) − E

(ha) = R(ha) − 1

Pr
eS0

eS0

3.3 Bound on the Expectation

[Φ(S0)] based on the Rademacher complexity  as in the i.i.d. case [2].

Here  we give a bound on E eS0
But  unlike the standard case  the proof requires an analysis in terms of independent blocks.
Lemma 2. The following inequality holds for the expectation E eS0
independent block sequence:E eS0

eD
µ (H).

[Φ(eS0)] ≤ R

[Φ(eS0)] deﬁned in terms of an
[Φ(eS0)] can be

(h)].

Proof. By the convexity of the supremum function and Jensen’s inequality  E eS0
bounded in terms of empirical averages over two samples:

E
eS0

E
eS0

0

1
µ

ha∈Ha

ha∈Ha

(h)]

eS0

0

[ sup

eS0  eS ′

0

[ sup

0

1
µ

1
µ

= E

eS0  eS ′

≤ E
eS0  eS ′

= E
eS0  eS ′

[ sup
h∈H

E
eS ′

0

(h)] ≤ E
eS0  eS ′

0

(def. of bR)

h∈H bR eS ′

(h) − bR eS0

(h)] − bR eS0

[Φ(eS0)] ≤ E

We now proceed with a standard symmetrization argument with the independent blocks thought of
as i.i.d. points:

[Φ(eS0)] = E
h∈H bR eS ′
0(cid:20) sup
0 σ(cid:20) sup
0 σ(cid:20) sup
eS0 σ(cid:20) sup

[bR eS ′
(h) − bR eS0
i)(cid:21)
µXi=1
ha(Zi) − ha(Z ′
i))(cid:21)
µXi=1
σi(ha(Zi) − ha(Z ′
0 σ(cid:20) sup
σiha(Zi)(cid:21) + E
µXi=1
σiha(Zi)(cid:21).
µXi=1
In the second equality  we introduced the Rademacher random variables σis. With probability 1/2 
σi = 1 and the difference ha(Zi) − ha(Z ′
i) is left unchanged; and  with probability 1/2  σi = −1
and Zi and Z ′
i are independent  taking the expectation over
σ leaves the expectation unchanged. The inequality follows from the sub-additivity of the supremum
function and the linearity of expectation. The ﬁnal equality holds becauseeS0 and eS′
0 are identically
j )(cid:21) 

We now relate the Rademacher block sequence to a sequence over independent points. The right-
hand side of the inequality just presented can be rewritten as

σiha(Zi)(cid:21) = E

i are permuted. Since the blocks Zi  or Z ′

distributed due to the assumption of stationarity.

eS0 σ(cid:20) sup

eS0 σ(cid:20)sup

i)(cid:21) (sub-add. of sup)

(Rad. var.’s)

µXi=1

eS0  eS ′

ha∈Ha

σiha(Z ′

h(z(i)

2
µ

µXi=1

σi

1
a

aXj=1

1
µ

µXi=1

= 2 E

ha∈Ha

1
µ

ha∈Ha

2 E

ha∈Ha

1
µ

h∈H

5

where z(i)
0 denote the i.i.d. sample
j
constructed from the jth point of each independent block Zi  i ∈ [1  µ]. By reversing the order of
summations and using the convexity of the supremum function  we obtain the following:

E
eS0

1
a

2
µ

h∈H

E

σih(z(i)

denotes the jth point of the ith block. For j ∈ [1  a]  let eSj
eS0 σ(cid:20) sup
[Φ(eS0)] ≤ E
aXj=1
aXj=1
eSµ σ(cid:20) sup

j )(cid:21)
µXi=1
j )(cid:21)
µXi=1
j )(cid:21)
µXi=1
σih(zi)(cid:21) ≤ R

aXj=1
eS0 σ(cid:20)sup
0  σ(cid:20)sup
µXi=1

σih(z(i)

σih(z(i)

eD
µ (H).

1
a

1
a

2
µ

2
µ

≤

=

= E

2
µ

E
eSj

h∈H

h∈H

h∈H

zi∈ eSµ

(reversing order of sums)

(convexity of sup)

(marginalization)

The ﬁrst equality in this derivation is obtained by marginalizing over the variables that do not appear
within the inner sum. Then  the second equality holds since  by stationarity  the choice of j does
not change the value of the expectation. The remaining quantity  modulo absolute values  is the
Rademacher complexity over µ independent points.

4 Non-i.i.d. Rademacher Generalization Bounds

4.1 General Bounds

This section presents and analyzes our main Rademacher complexity generalization bounds for sta-
tionary β-mixing sequences.
Theorem 1 (Rademacher complexity bound). Let H be a set of hypotheses bounded by M ≥ 0.
Then  for any sample S of size m drawn from a stationary β-mixing distribution  and for any µ  a >
0 with 2µa = m and δ > 2(µ − 1)β(a)  with probability at least 1 − δ  the following inequality
holds for all hypotheses h ∈ H:

where δ′ = δ − 2(µ − 1)β(a).

R(h) ≤ bRS(h) + R

µ (H) + Ms log 2

δ′
2µ

eD

 

Proof. Setting the right-hand side of Proposition 2 to δ and using Lemma 2 to bound E eS0
with the Rademacher complexity R

eD
µ (H) shows the result.

[Φ(eS0)]

As pointed out earlier  a key advantage of the Rademacher complexity is that it can be measured
from data  assuming that the computation of the minimal empirical error can be done effectively and

drawn from a β-mixing distribution  by considering random samples of σ. The following theorem

efﬁciently. In particular we can closely estimate bRSµ(H)  where Sµ is a subsample of the sample S
gives a bound precisely with respect to the empirical Rademacher complexitybRSµ.
Theorem 2 (Empirical Rademacher complexity bound). Under the same assumptions as in Theo-
rem 1  for any µ  a > 0 with 2µa = m and δ > 4(µ − 1)β(a)  with probability at least 1 − δ  the
following inequality holds for all hypotheses h ∈ H:

where δ′ = δ − 4(µ − 1)β(a).

R(h) ≤ bRS(h) +bRSµ(H) + 3Ms log 4

δ′
2µ

 

6

Proof. To derive this result from Theorem 1  it sufﬁces to bound R
eD
application of Corollary 1 to the indicator variable of the event {R

eD

µ (H) in terms of bRSµ(H). The
µ (H) −bRSµ(H) > ǫ} yields

(H) > ǫ(cid:1) + (µ − 1)β(2a − 1).

(9)

eD

eD

eD

Pr(cid:0)R

armid’s inequality gives

Now  we can apply McDiarmid’s inequality to R

µ (H) −bRSµ(H) > ǫ(cid:1) ≤ Pr(cid:0)R

µ (H) −bR eSµ
µ (H) − bR eSµ
drawn in an i.i.d. fashion. Changing a point of Sµ can affect bR eSµ
2M 2(cid:17) + (µ − 1)β(2a − 1).
µ (H) −bRSµ(H) > ǫ(cid:1) ≤ exp(cid:16)−µǫ2

(10)
Note β is a decreasing function  which implies β(2a − 1) ≤ β(a). Thus  with probability at least
  with δ′ = δ/2 − (µ − 1)β(a)  a fortiori with δ′ =
δ/4 − (µ − 1)β(a). The result follows this inequality combined with the statement of Theorem 1
for a conﬁdence parameter δ/2.

1 − δ/2  Rµ(H) ≤ bRSµ(H) + Mq 2 log 1

(H) which is deﬁned over points
by at most (2M/µ)  thus  McDi-

Pr(cid:0)R

eD

δ′

µ

This theorem can be used to derive generalization bounds for a variety of hypothesis sets and learning
settings. In the next section  we present margin bounds for kernel-based classiﬁcation.

4.2 Classiﬁcation

S(h) = 1

Let X denote the input space  Y ={−1  +1} the target values in classiﬁcation  and Z = X × Y . For
any hypothesis h and margin ρ > 0  let bRρ
S(h) denote the average amount by which yh(x) deviates
mPm
from ρ over a sample S: bRρ
i=1(ρ − yih(xi))+. Given a positive deﬁnite symmetric
kernel K : X ×X → R  let K denote its Gram matrix for the sample S and HK the kernel-based
hypothesis set {x 7→Pm
i=1 αiK(xi  x) : αKαT ≤ 1}  where α ∈ Rm×1 denotes the column-vector
with components αi  i = 1  . . .   m.
Theorem 3 (Margin bound). Let ρ > 0 and K be a positive deﬁnite symmetric kernel. Then  for any
µ  a > 0 with 2µa = m and δ > 4(µ − 1)β(a)  with probability at least 1 − δ over samples S of size
m drawn from a stationary β-mixing distribution  the following inequality holds for all hypotheses
h∈ HK :

Pr[yh(x) ≤ 0] ≤

S(h) +

1

ρbRρ

4

µρpTr[K] + 3s log 4

δ′
2µ

 

where δ′ = δ − 4(µ − 1)β(a).
Proof. For any h∈ H  let h denote the corresponding hypothesis deﬁned over Z by: ∀z∈ Z  h(z) =
−yh(x); and H K the hypothesis set {z ∈ Z 7→ h(z) : h ∈ HK}. Let L denote the loss function
associated to the margin loss bRρ
S(h). Then  Pr[yh(x) ≤ 0] ≤ Pr[(L ◦ h)(z) ≤ 0] = R(L ◦ h).
Since L − 1 is 1/ρ-Lipschitz and (L − 1)(0) = 0  by Talagrand’s lemma [6]  bRS((L − 1) ◦ H K)≤
2bRS(H K)/ρ. The result is then obtained by applying Theorem 2 to R((L− 1)◦ h) = R(L◦ h)− 1
with bR((L − 1) ◦ h) = bR(L ◦ h) − 1  and using the known bound for the empirical Rademacher
complexity of kernel-based classiﬁers [2  11]: bRS(H K)≤ 2
In order to show that this bound converges  we must appropriately choose the parameter µ  or equiv-
alently a  which will depend on the mixing parameter β. In the case of algebraic mixing and using
the straightforward bound Tr[K] ≤ mR2 for the kernel trace  where R is the radius of the ball that
contains the data  the following corollary holds.
Corollary 2. With the same assumptions as in Theorem 3  if β is further algebraically β-mixing 
β(a) = β0a−r  then  with probability at least 1 − δ  the following bound holds for all hypotheses
h∈ HK :

|S|pTr[K].

where γ1 = 1

Pr[yh(x) ≤ 0] ≤

S(h) +

r+2 − 1(cid:1)  γ2 = 1
2r+4 − 1(cid:1) and δ′ = δ − 2β0mγ1.
2(cid:0) 3
2(cid:0) 3

1

ρbRρ

8Rmγ1

ρ

+ 3mγ2rlog

4
δ′  

7

2r+1

2 m

This bound is obtained by choosing µ = 1
2r+4   which  modulo a multiplicative constant  is the
minimizer of (√m/µ + µβ(a)). Note that for r > 1 we have γ1  γ2 < 0 and thus  it is clear that
the bound converges  while the actual rate will depend on the distribution parameter r. A tighter
estimate of the trace of the kernel matrix  possibly derived from data  would provide a better bound 
as would stronger mixing assumptions  e.g.  exponential mixing. Finally  we note that as r → ∞
and β0 → 0  that is as the dependence between points vanishes  the right-hand side of the bound
S + 1/√m)  which coincides with the asymptotic behavior in the i.i.d. case [2 4 5].
approaches O(bRρ

5 Conclusion

We presented the ﬁrst Rademacher complexity error bounds for dependent samples generated by a
stationary β-mixing process  a generalization of similar existing bounds derived for the i.i.d. case.
We also gave the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting  includ-
ing explicit bounds for algebraic β-mixing processes. Similar margin bounds can be obtained for
the regression setting by using Theorem 2 and the properties of the empirical Rademacher com-
plexity  as in the i.i.d. case. Many non-i.i.d. bounds based on other complexity measures such as
the VC-dimension or covering numbers can be retrieved from our framework. Our framework and
the bounds presented could serve as the basis for the design of regularization-based algorithms for
dependent samples generated by a stationary β-mixing process.

Acknowledgements

This work was partially funded by the New York State Ofﬁce of Science Technology and Academic Research
(NYSTAR).

References

[1] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University

Press  Cambridge  UK  1999.

[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3:2002  2002.

[3] A. Irle. On the consistency in nonparametric estimation under mixing assumptions. Journal of Multivari-

ate Analysis  60:123–147  1997.

[4] V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the risk of function learning. In

High Dimensional Probability II  pages 443–459. preprint  2000.

[5] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error

of combined classiﬁers. Annals of Statistics  30  2002.

[6] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer 

1991.

[7] A. Lozano  S. Kulkarni  and R. Schapire. Convergence and consistency of regularized boosting algorithms

with stationary β-mixing observations. Advances in Neural Information Processing Systems  18  2006.

[8] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics  pages 148–188.

Cambridge University Press  1989.

[9] R. Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning 

39(1):5–34  2000.

[10] M. Mohri and A. Rostamizadeh. Stability bounds for non-iid processes. Advances in Neural Information

Processing Systems  2007.

[11] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press 

2004.

[12] I. Steinwart  D. Hush  and C. Scovel. Learning from dependent observations. Technical Report LA-UR-

06-3507  Los Alamos National Laboratory  2007.

[13] L. G. Valiant. A theory of the learnable. ACM Press New York  NY  USA  1984.
[14] M. Vidyasagar. Learning and Generalization: with Applications to Neural Networks. Springer  2003.
[15] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. Annals Probability 

22(1):94–116  1994.

8

,Balázs Szörényi
Róbert Busa-Fekete
Adil Paul
Eyke Hüllermeier
Zhao Song
Ruosong Wang
Lin Yang
Hongyang Zhang
Peilin Zhong