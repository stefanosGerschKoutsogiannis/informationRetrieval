2013,The Power of Asymmetry in Binary Hashing,When approximating binary similarity using the hamming distance between short binary hashes  we shown that even if the similarity is symmetric  we can have shorter and more accurate hashes by using two distinct code maps. I.e.~by approximating the similarity between $x$ and $x'$ as the hamming distance between $f(x)$ and $g(x')$  for two distinct binary codes $f g$  rather than as the hamming distance between $f(x)$ and $f(x')$.,The Power of Asymmetry in Binary Hashing

Behnam Neyshabur

Payman Yadollahpour

Yury Makarychev

Toyota Technological Institute at Chicago

[btavakoli pyadolla yury]@ttic.edu

Ruslan Salakhutdinov

Nathan Srebro

Departments of Statistics and Computer Science

Toyota Technological Institute at Chicago

University of Toronto

rsalakhu@cs.toronto.edu

and Technion  Haifa  Israel

nati@ttic.edu

Abstract

When approximating binary similarity using the hamming distance between short
binary hashes  we show that even if the similarity is symmetric  we can have
shorter and more accurate hashes by using two distinct code maps. I.e. by approx-
imating the similarity between x and x(cid:48) as the hamming distance between f (x)
and g(x(cid:48))  for two distinct binary codes f  g  rather than as the hamming distance
between f (x) and f (x(cid:48)).

1

Introduction

Encoding high-dimensional objects using short binary hashes can be useful for fast approximate
similarity computations and nearest neighbor searches. Calculating the hamming distance between
two short binary strings is an extremely cheap computational operation  and the communication cost
of sending such hash strings for lookup on a server (e.g. sending hashes of all features or patches in
an image taken on a mobile device) is low. Furthermore  it is also possible to quickly look up nearby
hash strings in populated hash tables. Indeed  it only takes a fraction of a second to retrieve a shortlist
of similar items from a corpus containing billions of data points  which is important in image  video 
audio  and document retrieval tasks [11  9  10  13]. Moreover  compact binary codes are remarkably
storage efﬁcient  and allow one to store massive datasets in memory. It is therefore desirable to ﬁnd
short binary hashes that correspond well to some target notion of similarity. Pioneering work on
Locality Sensitive Hashing used random linear thresholds for obtaining bits of the hash [1]. Later
work suggested learning hash functions attuned to the distribution of the data [15  11  5  7  3].
More recent work focuses on learning hash functions so as to optimize agreement with the target
similarity measure on speciﬁc datasets [14  8  9  6] . It is important to obtain accurate and short
hashes—the computational and communication costs scale linearly with the length of the hash  and
more importantly  the memory cost of the hash table can scale exponentially with the length.
In all the above-mentioned approaches  similarity S(x  x(cid:48)) between two objects is approximated by
the hamming distance between the outputs of the same hash function  i.e. between f (x) and f (x(cid:48)) 
for some f ∈ {±1}k. The emphasis here is that the same hash function is applied to both x and x(cid:48)
(in methods like LSH multiple hashes might be used to boost accuracy  but the comparison is still
between outputs of the same function).
The only exception we are aware of is where a single mapping of objects to fractional vectors
˜f (x) ∈ [−1  1]k is used  its thresholding f (x) = sign ˜f (x) ∈ {±1}k is used in the database 
and similarity between x and x(cid:48) is approximated using
. This has become known
as “asymmetric hashing” [2  4]  but even with such a-symmetry  both mappings are based on the

f (x)  ˜f (x(cid:48))

(cid:68)

(cid:69)

1

same fractional mapping ˜f (·). That is  the asymmetry is in that one side of the comparison gets
thresholded while the other is fractional  but not in the actual mapping.
In this paper  we propose using two distinct mappings f (x)  g(x) ∈ {±1}k and approximating the
similarity S(x  x(cid:48)) by the hamming distance between f (x) and g(x(cid:48)). We refer to such hashing
schemes as “asymmetric”. Our main result is that even if the target similarity function is sym-
metric and “well behaved” (e.g.  even if it is based on Euclidean distances between objects)  using
asymmetric binary hashes can be much more powerful  and allow better approximation of the tar-
get similarity with shorter code lengths. In particular  we show extreme examples of collections
of points in Euclidean space  where the neighborhood similarity S(x  x(cid:48)) can be realized using an
asymmetric binary hash (based on a pair of distinct functions) of length O(r) bits  but where a sym-
metric hash (based on a single function) would require at least Ω(2r) bits. Although actual data is
not as extreme  our experimental results on real data sets demonstrate signiﬁcant beneﬁts from using
asymmetric binary hashes.
Asymmetric hashes can be used in almost all places where symmetric hashes are typically used 
usually without any additional storage or computational cost. Consider the typical application of
storing hash vectors for all objects in a database  and then calculating similarities to queries by
computing the hash of the query and its hamming distance to the stored database hashes. Using
an asymmetric hash means using different hash functions for the database and for the query. This
neither increases the size of the database representation  nor the computational or communication
cost of populating the database or performing a query  as the exact same operations are required.
In fact  when hashing the entire database  asymmetric hashes provide even more opportunity for
improvement. We argue that using two different hash functions to encode database objects and
queries allows for much more ﬂexibility in choosing the database hash. Unlike the query hash 
which has to be stored compactly and efﬁciently evaluated on queries as they appear  if the database
is ﬁxed  an arbitrary mapping of database objects to bit strings may be used. We demonstrate that
this can indeed increase similarity accuracy while reducing the bit length required.

2 Minimum Code Lengths and the Power of Asymmetry
Let S : X × X → {±1} be a binary similarity function over a set of objects X   where we can
interpret S(x  x(cid:48)) to mean that x and x(cid:48) are “similar” or “dissimilar”  or to indicate whether they are
“neighbors”. A symmetric binary coding of X is a mapping f : X → {±1}k  where k is the bit-
length of the code. We are interested in constructing codes such that the hamming distance between
f (x) and f (x(cid:48)) corresponds to the similarity S(x  x(cid:48)). That is  for some threshold θ ∈ R  S(x  x(cid:48)) ≈
sign((cid:104)f (x)  f (x(cid:48))(cid:105) − θ). Although discussing the hamming distance  it is more convenient for us
to work with the inner product (cid:104)u  v(cid:105)  which is equivalent to the hamming distance dh(u  v) since
(cid:104)u  v(cid:105) = (k − 2dh(u  v)) for u  v ∈ {±1}k.
In this section  we will consider the problem of capturing a given similarity using an arbitrary binary
code. That is  we are given the entire similarity mapping S  e.g. as a matrix S ∈ {±1}n×n over
a ﬁnite domain X = {x1  . . .   xn} of n objects  with Sij = S(xi  xj). We ask for an encoding
ui = f (xi) ∈ {±1}k of each object xi ∈ X   and a threshold θ  such that Sij = sign((cid:104)ui  uj(cid:105) − θ) 
or at least such that equality holds for as many pairs (i  j) as possible. It is important to emphasize
that the goal here is purely to approximate the given matrix S using a short binary code—there is no
out-of-sample generalization (yet).
We now ask: Can allowing an asymmetric coding enable approximating a symmetric similarity
matrix S with a shorter code length?
Denoting U ∈ {±1}n×k for the matrix whose columns contain the codewords ui  the minimal
binary code length that allows exactly representing S is then given by the following matrix factor-
ization problem:

k s.t U ∈ {±1}k×n

θ ∈ R

(1)

ks(S) = min
k U θ

where 1n is an n × n matrix of ones.

Y (cid:44) U(cid:62)U − θ1n
∀ij SijYij > 0

2

We begin demonstrating the power of asymmetry by considering an asymmetric variant of the above
problem. That is  even if S is symmetric  we allow associating with each object xi two distinct
binary codewords  ui ∈ {±1}k and vi ∈ {±1}k (we can think of this as having two arbitrary
mappings ui = f (xi) and vi = g(xi))  such that Sij = sign((cid:104)ui  vj(cid:105)− θ). The minimal asymmetric
binary code length is then given by:

θ ∈ R

(2)

ka(S) = min
k U V θ

k s.t U  V ∈ {±1}k×n
Y (cid:44) U(cid:62)V − θ1n
∀ij SijYij > 0

Writing the binary coding problems as matrix factorization problems is useful for understanding
the power we can get by asymmetry: even if S is symmetric  and even if we seek a symmetric Y  
insisting on writing Y as a square of a binary matrix might be a tough constraint. This is captured
in the following Theorem  which establishes that there could be an exponential gap between the
minimal asymmetry binary code length and the minimal symmetric code length  even if the matrix
S is symmetric and very well behaved:
Theorem 1. For any r  there exists a set of n = 2r points in Euclidean space  with similarity matrix

Sij =

−1

if (cid:107)xi − xj(cid:107) ≤ 1
if (cid:107)xi − xj(cid:107) > 1

  such that ka(S) ≤ 2r but ks(S) ≥ 2r/2

(cid:26)1

Proof. Let I1 = {1  . . .   n/2} and I2 = {n/2 + 1  . . .   n}. Consider the matrix G deﬁned by
Gii = 1/2  Gij = −1/(2n) if i  j ∈ I1 or i  j ∈ I2  and Gij = 1/(2n) otherwise. Matrix G is
diagonally dominant. By the Gershgorin circle theorem  G is positive deﬁnite. Therefore  there exist
vectors x1  . . .   xn such that (cid:104)xi  xj(cid:105) = Gij (for every i and j). Deﬁne

(cid:26)1

Sij =

−1

if (cid:107)xi − xj(cid:107) ≤ 1
if (cid:107)xi − xj(cid:107) > 1

.

(cid:21)

(cid:21)

(cid:20)B

(cid:20) B−C

Note that if i = j then Sij = 1; if i (cid:54)= j and (i  j) ∈ I1 × I1 ∪ I2 × I2 then (cid:107)xi − xj(cid:107)2 =
Gii+Gjj−2Gij = 1+1/n > 1 and therefore Sij = −1. Finally  if i (cid:54)= j and (i  j) ∈ I1×I2∪I2×I1
then (cid:107)xi − xj(cid:107)2 = Gii + Gjj − 2Gij = 1 + 1/n < 1 and therefore Sij = 1. We show that
ka(S) ≤ 2r. Let B be an r × n matrix whose column vectors are the vertices of the cube {±1}r
(in any order); let C be an r × n matrix deﬁned by Cij = 1 if j ∈ I1 and Cij = −1 if j ∈ I2. Let
. For Y = U(cid:62)V − θ1n where threshold θ = −1   we have that Yij ≥ 1
U =
if Sij = 1 and Yij ≤ −1 if Sij = −1. Therefore  ka(S) ≤ 2r.
Now we show that ks = ks(S) ≥ n/2. Consider Y   U and θ as in (1). Let Y (cid:48) = (U(cid:62)U ). Note
ij ∈ [−ks  ks] and thus θ ∈ [−ks + 1  ks − 1]. Let q = [1  . . .   1 −1  . . .  −1](cid:62) (n/2 ones
that Y (cid:48)
followed by n/2 minus ones). We have 
Y (cid:48)
ii +

0 ≤ q(cid:62)Y (cid:48)q =

and V =

C

Y (cid:48)

ij

n(cid:88)
≤ n(cid:88)

i=1

(cid:88)
(cid:88)

Y (cid:48)

ij − (cid:88)
(θ − 1) − (cid:88)

i j:Sij =1 i(cid:54)=j

i j:Sij =−1

ks +

(θ + 1)

i=1

i j:Sij =−1

i j:Sij =1 i(cid:54)=j
= nks + (0.5n2 − n)(θ − 1) − 0.5n2(θ + 1)
= nks − n2 − n(θ − 1)
≤ 2nks − n2.

We conclude that ks ≥ n/2.

The construction of Theorem 1 shows that there exists data sets for which an asymmetric binary hash
might be much shorter then a symmetric hash. This is an important observation as it demonstrates
that asymmetric hashes could be much more powerful  and should prompt us to consider them
instead of symmetric hashes. The precise construction of Theorem 1 is of course rather extreme (in
fact  the most extreme construction possible) and we would not expect actual data sets to have this
exact structure  but we will show later signiﬁcant gaps also on real data sets.

3

10-D Uniform

LabelMe

Figure 1: Number of bits required for approximating two similarity matrices (as a function of average pre-
cision). Left: uniform data in the 10-dimensional hypercube  similarity represents a thresholded Euclidean
distance  set such that 30% of the similarities are positive. Right: Semantic similarity of a subset of LabelMe
images  thresholded such that 5% of the similarities are positive.
3 Approximate Binary Codes

As we turn to real data sets  we also need to depart from seeking a binary coding that exactly
captures the similarity matrix. Rather  we are usually satisﬁed with merely approximating S  and
for any ﬁxed code length k seek the (symmetric or asymmetric) k-bit code that “best captures” the
similarity matrix S. This is captured by the following optimization problem:

(cid:88)

(cid:88)

L(Y ; S) (cid:44) β

min
U V θ

(cid:96)(Yij) + (1 − β)

(cid:96)(−Yij)

i j:Sij =1

i j:Sij =−1

s.t. U  V ∈ {±1}k×n
Y (cid:44) U(cid:62)V − θ1n

θ ∈ R

(3)

where (cid:96)(z) = 1z≤0 is the zero-one-error and β is a parameter that allows us to weight positive
and negative errors differently. Such weighting can compensate for Sij being imbalanced (typically
many more pairs of points are non-similar rather then similar)  and allows us to obtain different
balances between precision and recall.
The optimization problem (3) is a discrete  discontinuous and highly non-convex problem. In our
experiments  we replace the zero-one loss (cid:96)(·) with a continuous loss and perform local search
by greedily updating single bits so as to improve this objective. Although the resulting objective
(let alone the discrete optimization problem) is still not convex even if (cid:96)(z) is convex  we found it
beneﬁcial to use a loss function that is not ﬂat on z < 0  so as to encourage moving towards the
correct sign. In our experiments  we used the square root of the logistic loss  (cid:96)(z) = log1/2(1+e−z).
Before moving on to out-of-sample generalizations  we brieﬂy report on the number of bits needed
empirically to ﬁnd good approximations of actual similarity matrices with symmetric and asymmet-
ric codes. We experimented with several data sets  attempting to ﬁt them with both symmetric and
asymmetric codes  and then calculating average precision by varying the threshold θ (while keeping
U and V ﬁxed). Results for two similarity matrices  one based on Euclidean distances between
points uniformly distributed in a hypoercube  and the other based on semantic similarity between
images  are shown in Figure 1.

4 Out of Sample Generalization: Learning a Mapping

So far we focused on learning binary codes over a ﬁxed set of objects by associating an arbitrary
code word with each object and completely ignoring the input representation of the objects xi.
We discussed only how well binary hashing can approximate the similarity  but did not consider
generalizing to additional new objects. However  in most applications  we would like to be able to
have such an out-of-sample generalization. That is  we would like to learn a mapping f : X →
{±1}k over an inﬁnite domain X using only a ﬁnite training set of objects  and then apply the
mapping to obtain binary codes f (x) for future objects to be encountered  such that S(x  x(cid:48)) ≈
sign((cid:104)f (x)  f (x(cid:48))(cid:105) − θ). Thus  the mapping f : X → {±1}k is usually limited to some constrained
parametric class  both so we could represent and evaluate it efﬁciently on new objects  and to ensure
good generalization. For example  when X = Rd  we can consider linear threshold mappings
fW (x) = sign(W x)  where W ∈ Rk×d and sign(·) operates elementwise  as in Minimal Loss
Hashing [8]. Or  we could also consider more complex classes  such as multilayer networks [11  9].
We already saw that asymmetric binary codes can allow for better approximations using shorter
codes  so it is natural to seek asymmetric codes here as well. That is  instead of learning a single

4

0.70.750.80.850.90.9505101520253035Average Precisionbits  SymmetricAsymmetric0.80.820.840.860.880.90.920.940.96010203040506070Average Precisionbits  SymmetricAsymmetricparametric map f (x) we can learn a pair of maps f : X → {±1}k and g : X → {±1}k  both
constrained to some parametric class  and a threshold θ  such that S(x  x(cid:48)) ≈ sign((cid:104)f (x)  g(x(cid:48))(cid:105) −
θ). This has the potential of allowing for better approximating the similarity  and thus better overall
accuracy with shorter codes (despite possibly slightly harder generalization due to the increase in
the number of parameters).
In fact  in a typical application where a database of objects is hashed for similarity search over
future queries  asymmetry allows us to go even further. Consider the following setup: We are given
n objects x1  . . .   xn ∈ X from some inﬁnite domain X and the similarities S(xi  xj) between
these objects. Our goal is to hash these objects using short binary codes which would allow us to
quickly compute approximate similarities between these objects (the “database”) and future objects
x (the “query”). That is  we would like to generate and store compact binary codes for objects in a
database. Then  given a new query object  we would like to efﬁciently compute a compact binary
code for a given query and retrieve similar items in the database very fast by ﬁnding binary codes
in the database that are within small hamming distance from the query binary code. Recall that it
is important to ensure that the bit length of the hashes are small  as short codes allow for very fast
hamming distance calculations and low communication costs if the codes need to be sent remotely.
More importantly  if we would like to store the database in a hash table allowing immediate lookup 
the size of the hash table is exponential in the code length.
The symmetric binary hashing approach (e.g. [8])  would be to ﬁnd a single parametric mapping
f : X → {±1}k such that S(x  xi) ≈ sign((cid:104)f (x)  f (xi)(cid:105) − θ) for future queries x and database
objects xi  calculate f (xi) for all database objects xi  and store these hashes (perhaps in a hash table
allowing for fast retrieval of codes within a short hamming distance). The asymmetric approach
described above would be to ﬁnd two parametric mappings f : X → {±1}k and g : X → {±1}k
such that S(x  xi) ≈ sign((cid:104)f (x)  g(xi)(cid:105) − θ)  and then calculate and store g(xi).
But if the database is ﬁxed  we can go further. There is actually no need for g(·) to be in a constrained
parametric class  as we do not need to generalize g(·) to future objects  nor do we have to efﬁciently
calculate it on-the-ﬂy nor communicate g(x) to the database. Hence  we can consider allowing the
database hash function g(·) to be an arbitrary mapping. That is  we aim to ﬁnd a simple parametric
mapping f : X → {±1}k and n arbitrary codewords v1  . . .   vn ∈ {±1}k for each x1  . . .   xn
in the database  such that S(x  xi) ≈ sign((cid:104)f (x)  vi(cid:105) − θ) for future queries x and for the objects
xi  . . .   xn in the database. This form of asymmetry can allow us for greater approximation power 
and thus better accuracy with shorter codes  at no additional computational or storage cost.
In Section 6 we evaluate empirically both of the above asymmetric strategies and demonstrate their
beneﬁts. But before doing so  in the next Section  we discuss a local-search approach for ﬁnding the
mappings f  g  or the mapping f and the codes v1  . . .   vn.

5 Optimization

We focus on x ∈ X ⊂ Rd and linear threshold hash maps of the form f (x) = sign(W x)  where
W ∈ Rk×d. Given training points x1  . . .   xn  we consider the two models discussed above:
LIN:LIN We learn two linear threshold functions f (x) = sign(Wqx) and g(x) = sign(Wdx).

I.e. we need to ﬁnd the parameters Wq  Wd ∈ Rk×d.

LIN:V We learn a single linear threshold function f (x) = sign(Wqx) and n codewords
I.e. we need to ﬁnd Wq ∈ Rk×d  as well as V ∈ Rk×n (where vi

v1  . . .   vn ∈ Rk.
are the columns of V ).

In either case we denote ui = f (xi)  and in LIN:LIN also vi = g(xi)  and learn by attempting to
minimizing the objective in (3)  where (cid:96)(·) is again a continuous loss function such as the square
root of the logistic. That is  we learn by optimizing the problem (3) with the additional constraint
U = sign(WqX)  and possibly also V = sign(WdX) (for LIN:LIN)  where X = [x1 . . . xn] ∈
Rd×n.
We optimize these problems by alternatively updating rows of Wq and either rows of Wd (for
LIN:LIN ) or of V (for LIN:V ). To understand these updates  let us ﬁrst return to (3) (with un-

5

Mij =

βij
2

(cid:96)(Sij(Y (t)

ij − 1)) − (cid:96)(Sij(Y (t)

ij + 1))

 

(cid:17)

(cid:16)

constrained U  V )  and consider updating a row u(t) ∈ Rn of U. Denote

Y (t) = U(cid:62)V − θ1n − u(t)(cid:62)

v(t) 

the prediction matrix with component t subtracted away. It is easy to verify that we can write:

L(U(cid:62)V − θ1n; S) = C − u(t)M v(t)(cid:62)

(4)
2 (L(Y (t) +1n; S)+L(Y (t)−1n; S)) does not depend on u(t) and v(t)  and M ∈ Rn×n

where C = 1
also does not depend on u(t)  v(t) and is given by:

with βij = β or βij = (1 − β) depending on Sij. This implies that we can optimize over the entire
row u(t) concurrently by maximizing u(t)M v(t)(cid:62)
  and so the optimum (conditioned on θ  V and all
other rows of U) is given by

(5)
Symmetrically  we can optimize over the row v(t) conditioned on θ  U and the rest of V   or in the
case of LIN:V   conditioned on θ  Wq and the rest of V .
Similarly  optimizing over a row w(t) of Wq amount to optimizing:

u(t) = sign(M v(t)).

sign(w(t)X)M v(t)(cid:62)

i

w(t)  xi

).

sign(

arg max
w(t)∈Rd

= arg max
w(t)∈Rd

This is a weighted zero-one-loss binary classiﬁcation problem  with targets sign((cid:10)Mi  v(t)(cid:11)) and
weights(cid:12)(cid:12)(cid:10)Mi  v(t)(cid:11)(cid:12)(cid:12). We approximate it as a weighted logistic regression problem  and at each

update iteration attempt to improve the objective using a small number (e.g. 10) epochs of stochastic
gradient descent on the logistic loss. For LIN:LIN   we also symmetrically update rows of Wd.
When optimizing the model for some bit-length k  we initialize to the optimal k − 1-length model.
We initialize the new bit either randomly  or by thresholding the rank-one projection of M (for
unconstrained U  V ) or the rank-one projection after projecting the columns of M (for LIN:V ) or
both rows and columns of M (for LIN:LIN ) to the column space of X. We take the initialization
(random  or rank-one based) that yields a lower objective value.

(6)

(cid:88)

(cid:68)
Mi  v(t)(cid:69)

(cid:68)

(cid:69)

6 Empirical Evaluation

In order to empirically evaluate the beneﬁts of asymmetry in hashing  we replicate the experiments
of [8]  which were in turn based on [5]  on six datasets using learned (symmetric) linear threshold
codes. These datasets include: LabelMe and Peekaboom are collections of images  represented as
512D GIST features [13]  Photo-tourism is a database of image patches  represented as 128 SIFT
features [12]  MNIST is a collection of 785D greyscale handwritten images  and Nursery contains
8D features. Similar to [8  5]  we also constructed a synthetic 10D Uniform dataset  containing
uniformly sampled 4000 points for a 10D hypercube. We used 1000 points for training and 3000 for
testing.
For each dataset  we ﬁnd the Euclidean distance at which each point has  on average  50 neighbours.
This deﬁnes our ground-truth similarity in terms of neighbours and non-neighbours. So for each
dataset  we are given a set of n points x1  . . .   xn  represented as vectors in X = Rd  and the binary
similarities S(xi  xj) between the points  with +1 corresponding to xi and xj being neighbors and
-1 otherwise. Based on these n training points  [8] present a sophisticated optimization approach
for learning a thresholded linear hash function of the form f (x) = sign(W x)  where W ∈ Rk×d.
This hash function is then applied and f (x1)  . . .   f (xn) are stored in the database. [8] evaluate
the quality of the hash by considering an independent set of test points and comparing S(x  xi) to
sign((cid:104)f (x)  f (xi)(cid:105) − θ) on the test points x and the database objects (i.e. training points) xi.
In our experiments  we followed the same protocol  but with the two asymmetric variations LIN:LIN
and LIN:V  using the optimization method discussed in Sec. 5. In order to obtain different balances
between precision and recall  we should vary β in (3)  obtaining different codes for each value of

6

10-D Uniform

LabelMe

MNIST

Peekaboom

Photo-tourism

Nursery

Figure 2: Average Precision (AP) of points retrieved using Hamming distance as a function of code length
for six datasets. Five curves represent: LSH  BRE  KSH  MLH  and two variants of our method: Asymmetric
LIN-LIN and Asymmetric LIN-V. (Best viewed in color.)

LabelMe

MNIST

Peekaboom

Figure 3: Code length required as a function of Average Precision (AP) for three datasets.

β. However  as in the experiments of [8]  we actually learn a code (i.e. mappings f (·) and g(·)  or
a mapping f (·) and matrix V ) using a ﬁxed value of β = 0.7  and then only vary the threshold θ to
obtain the precision-recall curve.
In all of our experiments  in addition to Minimal Loss Hashing (MLH)  we also compare our ap-
proach to three other widely used methods: Kernel-Based Supervised Hashing (KSH) of [6]  Binary
Reconstructive Embedding (BRE) of [5]  and Locality-Sensitive Hashing (LSH) of [1]. 1
In our ﬁrst set of experiments  we test performance of the asymmetric hash codes as a function of
the bit length. Figure 2 displays Average Precision (AP) of data points retrieved using Hamming
distance as a function of code length. These results are similar to ones reported by [8]  where MLH
yields higher precision compared to BRE and LSH. Observe that for all six datasets both variants
of our method  asymmetric LIN:LIN and asymmetric LIN:V   consistently outperform all other
methods for different binary code length. The gap is particularly large for short codes. For example 
for the LabelMe dataset  MLH and KSH with 16 bits achieve AP of 0.52 and 0.54 respectively 
whereas LIN:V already achieves AP of 0.54 with only 8 bits. Figure 3 shows similar performance
gains appear for a number of other datasets. We also note across all datasets LIN:V improves upon
LIN:LIN for short-sized codes. These results clearly show that an asymmetric binary hash can be
much more compact than a symmetric hash.

1We used the BRE  KSH and MLH implementations available from the original authors. For each method 
we followed the instructions provided by the authors. More speciﬁcally  we set the number of points for each
hash function in BRE to 50 and the number of anchors in KSH to 300 (the default values). For MLH  we learn
the threshold and shrinkage parameters by cross-validation and other parameters are initialized to the suggested
values in the package.

7

812162024283236404448525660640.20.40.60.81Number of BitsAverage Precision  LIN:VLIN:LINMLHKSHBRELSH812162024283236404448525660640.20.40.60.81Number of BitsAverage Precision  LIN:VLIN:LINMLHKSHBRELSH812162024283236404448525660640.20.40.60.81Number of BitsAverage Precision  LIN:VLIN:LINMLHKSHBRELSH812162024283236404448525660640.20.40.60.81Number of BitsAverage Precision  LIN:VLIN:LINMLHKSHBRELSH812162024283236404448525660640.20.40.60.81Number of BitsAverage Precision  LIN:VLIN:LINMLHKSHBRELSH812162024283236404448525660640.20.40.60.81Number of BitsAverage Precision  LIN:VLIN:LINMLHKSHBRELSH0.550.60.650.70.750.805101520253035404550Average PrecisionBits Required  LIN:VLIN:LINMLHKSH0.550.60.650.70.750.805101520253035404550Average PrecisionBits Required  LIN:VLIN:LINMLHKSH0.550.60.650.70.750.805101520253035404550Average PrecisionBits Required  LIN:VLIN:LINMLHKSH16 bits

LabelMe

64 bits

16 bits

MNIST

64bits

Figure 4: Precision-Recall curves for LabelMe and MNIST datasets using 16 and 64 binary codes. (Best
viewed in color.)

Figure 5: Left: Precision-Recall curves for the Semantic 22K LabelMe dataset Right: Percentage of 50
ground-truth neighbours as a function of retrieved images. (Best viewed in color.)

Next  we show  in Figure 4  the full Precision-Recall curves for two datasets  LabelMe and MNIST 
and for two speciﬁc code lengths: 16 and 64 bits. The performance of LIN:LIN and LIN:V is almost
uniformly superior to that of MLH  KSH and BRE methods. We observed similar behavior also for
the four other datasets across various different code lengths.
Results on previous 6 datasets show that asymmetric binary codes can signiﬁcantly outperform
other state-of-the-art methods on relatively small scale datasets. We now consider a much larger
LabelMe dataset [13]  called Semantic 22K LabelMe. It contains 20 019 training images and 2 000
test images  where each image is represented by a 512D GIST descriptor. The dataset also provides a
semantic similarity S(x  x(cid:48)) between two images based on semantic content (object labels overlap in
two images). As argued by [8]  hash functions learned using semantic labels should be more useful
for content-based image retrieval compared to Euclidean distances. Figure 5 shows that LIN:V with
64 bits substantially outperforms MLH and KSH with 64 bits.

7 Summary

The main point we would like to make is that when considering binary hashes in order to approxi-
mate similarity  even if the similarity measure is entirely symmetric and “well behaved”  much power
can be gained by considering asymmetric codes. We substantiate this claim by both a theoretical
analysis of the possible power of asymmetric codes  and by showing  in a fairly direct experimental
replication  that asymmetric codes outperform state-of-the-art results obtained for symmetric codes.
The optimization approach we use is very crude. However  even using this crude approach  we could
ﬁnd asymmetric codes that outperformed well-optimized symmetric codes. It should certainly be
possible to develop much better  and more well-founded  training and optimization procedures.
Although we demonstrated our results in a speciﬁc setting using linear threshold codes  we believe
the power of asymmetry is far more widely applicable in binary hashing  and view the experiments
here as merely a demonstration of this power. Using asymmetric codes instead of symmetric codes
could be much more powerful  and allow for shorter and more accurate codes  and is usually straight-
forward and does not require any additional computational  communication or signiﬁcant additional
memory resources when using the code. We would therefore encourage the use of such asymmetric
codes (with two distinct hash mappings) wherever binary hashing is used to approximate similarity.

Acknowledgments

This research was partially supported by NSF CAREER award CCF-1150062 and NSF grant IIS-
1302662.

8

0.20.40.60.81RecallPrecision  LIN:VLIN:LINMLHKSHBRELSH0.20.40.60.81RecallPrecision  LIN:VLIN:LINMLHKSHBRELSH0.20.40.60.81RecallPrecision  LIN:VLIN:LINMLHKSHBRELSH0.20.40.60.81RecallPrecision  LIN:VLIN:LINMLHKSHBRELSH0.20.40.60.810.10.20.3RecallPrecision  LIN:VMLHKSH5001000150020002500300035004000450050000.20.40.60.8Number RetrievedRecall  LIN:VMLHKSHReferences

[1] M. Datar  N. Immorlica  P. Indyk  and V.S. Mirrokni. Locality-sensitive hashing scheme based
on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational
geometry  pages 253–262. ACM  2004.

[2] W. Dong and M. Charikar. Asymmetric distance estimation with sketches for similarity search

in high-dimensional spaces. SIGIR  2008.

[3] Y. Gong  S. Lazebnik  A. Gordo  and F. Perronnin.

Iterative quantization: A procrustean

approach to learning binary codes for large-scale image retrieval. TPAMI  2012.

[4] A. Gordo and F. Perronnin. Asymmetric distances for binary embeddings. CVPR  2011.
[5] B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. NIPS  2009.
[6] W. Liu  R. Ji J. Wang  Y.-G. Jiang  and S.-F. Chang. Supervised hashing with kernels. CVPR 

2012.

[7] W. Liu  J. Wang  S. Kumar  and S.-F. Chang. Hashing with graphs. ICML  2011.
[8] M. Norouzi and D. J. Fleet. Minimal loss hashing for compact binary codes. ICML  2011.
[9] M. Norouzi  D. J. Fleet  and R. Salakhutdinov. Hamming distance metric learning. NIPS 

2012.

[10] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from shift-invariant kernels.

NIPS  2009.

[11] R. Salakhutdinov and G. Hinton. Semantic hashing.

Reasoning  2009.

International Journal of Approximate

[12] N. Snavely  S. M. Seitz  and R.Szeliski. Photo tourism: Exploring photo collections in 3d. In

Proc. SIGGRAPH  2006.

[13] A. Torralba  R. Fergus  and Y. Weiss. Small codes and large image databases for recognition.

CVPR  2008.

[14] J. Wang  S. Kumar  and S. Chang. Sequential projection learning for hashing with compact

codes. ICML  2010.

[15] Y. Weiss  A. Torralba  and R. Fergus. Spectral hashing. NIPS  2008.

9

,Behnam Neyshabur
Nati Srebro
Russ Salakhutdinov
Yury Makarychev
Payman Yadollahpour
Kaustubh Patil
Jerry Zhu
Łukasz Kopeć
Bradley Love
Xiao Li
Kannan Ramchandran
Alejandro Newell
Jia Deng
Alexander Liu
Yen-Cheng Liu
Yu-Ying Yeh
Yu-Chiang Frank Wang
Soeren Laue
Matthias Mitterreiter
Joachim Giesen