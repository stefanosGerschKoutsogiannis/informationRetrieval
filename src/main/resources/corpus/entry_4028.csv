2019,Online Convex Matrix Factorization with Representative Regions,Matrix factorization (MF) is a versatile learning method that has found wide applications in various data-driven disciplines. Still  many MF algorithms do not adequately scale with the size of available datasets and/or lack interpretability. To improve the computational efficiency of the method  an online (streaming) MF algorithm was proposed in Mairal et al.  2010. To enable data interpretability  a constrained version of MF  termed convex MF  was introduced in Ding et al.  2010. In the latter work  the basis vectors are required to lie in the convex hull of the data samples  thereby ensuring that every basis can be interpreted as a weighted combination of data samples. No current algorithmic solutions for online convex MF are known as it is challenging to find adequate convex bases without having access to the complete dataset. We address both problems by proposing the first online convex MF algorithm that maintains a collection of constant-size sets of representative data samples needed for interpreting each of the basis (Ding et al.  2010) and has the same almost sure convergence guarantees as the online learning algorithm of Mairal et al.  2010. Our proof techniques combine random coordinate descent algorithms with specialized quasi-martingale convergence analysis. Experiments on synthetic and real world datasets show significant computational savings of the proposed online convex MF method compared to classical convex MF. Since the proposed method maintains small representative sets of data samples needed for convex interpretations  it is related to a body of work in theoretical computer science  pertaining to generating point sets (Blum et al.  2016)  and in computer vision  pertaining to archetypal analysis (Mei et al.  2018). Nevertheless  it differs from these lines of work both in terms of the objective and algorithmic implementations.,Online Convex Matrix Factorization with

Representative Regions

Abhishek Agarwal ࢩ

Electrical and Computer Engineering
University of Illinois Urbana-Champaign

abhiag@illinois.edu

Jianhao Peng ࢩ

Electrical and Computer Engineering
University of Illinois Urbana-Champaign

jianhao2@illinois.edu

Olgica Milenkovic

Electrical and Computer Engineering
University of Illinois Urbana-Champaign

milenkov@illinois.edu

Abstract

Matrix factorization (MF) is a versatile learning method that has found wide ap-
plications in various data-driven disciplines. Still  many MF algorithms do not
adequately scale with the size of available datasets and/or lack interpretability. To
improve the computational eﬃciency of the method  an online (streaming) MF
algorithm was proposed in [1]. To enable data interpretability  a constrained ver-
sion of MF  termed convex MF  was introduced in [2]. In the latter work  the basis
vectors are required to lie in the convex hull of the data samples  thereby ensuring
that every basis can be interpreted as a weighted combination of data samples. No
current algorithmic solutions for online convex MF are known as it is challenging
to ﬁnd adequate convex bases without having access to the complete dataset. We
address both problems by proposing the ﬁrst online convex MF algorithm that
maintains a collection of constant-size sets of representative data samples needed
for interpreting each of the basis [2] and has the same almost sure convergence
guarantees as the online learning algorithm of [1]. Our proof techniques combine
random coordinate descent algorithms with specialized quasi-martingale conver-
gence analysis. Experiments on synthetic and real world datasets show signiﬁcant
computational savings of the proposed online convex MF method compared to
classical convex MF. Since the proposed method maintains small representative
sets of data samples needed for convex interpretations  it is related to a body of
work in theoretical computer science  pertaining to generating point sets [3]  and in
computer vision  pertaining to archetypal analysis [4]. Nevertheless  it diﬀers from
these lines of work both in terms of the objective and algorithmic implementations.

Introduction

1
Matrix Factorization (MF) is a widely used dimensionality reduction technique [5  6] whose goal
is to ﬁnd a basis that allows for a sparse representation of the underlying data [7  8]. Compared to
other dimensionality reduction techniques based on eigendecompositions [9]  MF enforces fewer
restrictions on the choice of the basis and hence ensures larger representation ﬂexibility for complex
datasets. At the same time  it provides a natural  application-speciﬁc interpretation for the bases.

ࢩ Abhishek Agarwal and Jianhao Peng contribute equally to this work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

MF methods have been studied under various modeling constraints [2  10  11  12  13  14  15  16].
The most frequently used constraints are non-negativity  constraints that accelerate convergence
rates  semi-non-negativity  orthogonality and convexity [11  2  17]. Convex MF (cvxMF) [2] is
of special interest as it requires the basis vectors to be convex combinations of the observed data
samples [18  19]. This constraint allows one to interpret the basis vectors as probabilistic sums of a
(small) representative subsets of data samples.
Unfortunately  most of the aforementioned constrained MF problems are non-convex and NP-hard [20 
21  22]  but can often be suboptimally solved using alternating optimization approaches for ﬁnding
local optima [13]. Alternating optimization approaches have scalability issues since the number
of matrix multiplications and convex optimization steps in each iteration depends both on the data
set size and its dimensionality. To address the scalability issue [23  24  25]  Mairal  Bach  Ponce
and Sapiro [1] introduced an online MF algorithm that minimizes a surrogate function amenable to
sequential optimization. The online algorithm comes with strong performance guarantees  asserting
that its solution converges almost surely to a local optima of the generalization loss.
Currently  no online/streaming solutions for convex MF are known as it appears hard to satisfy
the convexity constraint without having access to the whole dataset. We propose the ﬁrst online
MF method accounting for convexity constraints on multi-cluster data sets  termed online convex
Matrix Factorization (online cvxMF). The proposed method solves the cvxMF problem of Ding 
Li and Jordan [2] in an online/streaming fashion  and allows for selecting a collection of “typical”
representative sets of individual clusters (see Figure 1). The method sequentially processes single data
sample and updates a running version of a collection of constant-size sets of representative samples
of the clusters  needed for convex interpretations of each basis element. In this case  the basis also
plays the role of the cluster centroid  and further increases interpretability. The method also allows
for both sparse data and sparse basis representations. In the latter context  sparsity refers to restricting
each basis to be a convex combination of data samples in a small representative region. The online
cvxMF algorithm has the same theoretical convergence guarantees as [1].
We also consider a more restricted version of the cvxMF problem  in which the representative samples
are required to be strictly contained within their corresponding clusters. The algorithm is semi-
heuristic as it has provable convergence guarantees only when sample classiﬁcation is error-free  as
is the case for non-trivial supervised MF [26] (note that applying [1] to each cluster individually is
clearly suboptimal  as one needs to jointly optimize both the basis and the embedding). The restricted
cvxMF method nevertheless oﬀers excellent empirical performance when properly initialized.
It is worth pointing out that our results complement a large body of work that generalize the method
of [1] for diﬀerent loss functions [27  28  29] but do not impose convexity constraints. Furthermore 
the proposed online cvxMF exhibits certain similarities with online generating point set methods [3]
and online archetypal analysis [4]. The goal of these two lines of work is to ﬁnd a small set of
representative samples whose convex hull contains the majority of observed samples. In contrast  we
only seek a small set of representative samples needed for accurately describing a basis of the data.
The paper is organized as follows. Section 2 introduces the problem  relevant notation and introduces
our approach towards an online algorithm for the cvxMF problem. Section 3 describes the proposed
online algorithm and Section 4 establishes that the learned basis almost surely converge to a stationary
point of the approximation-error function. The theoretical guarantees hold under mild assumptions
on the data distribution reminiscent of those used in [1]  while the proof techniques combine ran-
dom coordinate descent algorithms with specialized quasi-martingale convergence analysis. The
performance of the algorithm is tested on both synthetic and real world datasets  as outlined in Sec-
tion 5. The real world datasets include are taken from the UCI Machine Learning [30] and the 10X
Genomics repository [31]. The experiments reveal that our online cvxMF runs four times faster than
its non-online counterpart on datasets with 104 samples  while for larger sample sets cvxMF becomes
exponentially harder to execute. The online cvxMF also produces high-accuracy clustering results.

2 Notation and Problem Formulation
We denote sets by [] = {1  …   }. Capital letters are reserved for matrices (bold font) and random
variables (RVs) (regular font). Random vectors are described by capital underlined letters  while
deterministic vectors are denoted by lower-case underlined letters. We use [] to denote the th
column of the matrix   [  ] to denote the element in row  and column   and [] to denote the

2

Figure 1: A multi-cluster dataset: Stars represent the learned bases  while circles denote representative
samples for the basis of the same color. Left: The representative sets for the individual basis elements
are unrestricted. Right: The representative sets are restricted to lie within their corresponding clusters.

min

2 + ߠߠ1.

whereߠߠ2 á Ý

  andߠߠ1 á ࢣ

th coordinate of a vector . Furthermore  col() stands for the set of columns of   while cvx()
stands for the convex hull of col().
Let  ࢠ Ó× denote a matrix of  data samples of constant dimension  arranged (summarized)
column-wise  let  ࢠ Ó× denote the  basis vectors used to represent the data and let  ࢠ Ó×
stand for the low-dimension embedding matrix. The classical MF problem reads as:

 ߠ − ߠ2
[] denote the 2-norm and 1-norm of the vector  
ࢠÓߠX − ߠ2

respectively.
In practice   is inherently random and in the stochastic setting it is more adequate to minimize the
above objective in expectation. In this case  the data approximation-error () for a ﬁxed  equals:
(2)
where X is a random vector of dimension  and the parameter  controls the sparsity of the coeﬃcient
vector . For analytical tractability  we assume that X is drawn from the union of  disjoint  convex
compact regions (clusters)  () ࢠ Ó   ࢠ []. Each cluster is independently selected based on
a given distribution  and the vector X is sampled from the chosen cluster. Both the cluster and
intra-cluster sample distributions are mildly constrained  as described in the next section.
The approximation-error of a single data sample  ࢠ Ó with respect to  equals

() á X[ min

2 + ߠߠ1] 

(1)

(  ) á min

2ߠ − ߠ2

1

2 + ߠߠ1.

ࢠÓ

(3)
(X  ). The function () is non-convex and optimizing it is NP-hard and requires prior
Consequently  the approximation error-function () in Equation (2) may be written as () =
X
knowledge of the distribution. To mitigate the latter problem  one can revert to an empirical estimate
of () involving the data samples    ࢠ [] 
() = 1


(  ).

ࢣ

Maintaining a running estimate of  of an optimizer of () involves updating the coeﬃcient vectors
for all the data samples observed up to time . Hence  it is desirable to use surrogate functions to
simplify the updates. The surrogate function Į() proposed in [1] reads as

=1

ࢣ

Į() á 1

2ߠ − ߠ2

2 + ߠߠ1 

1

(4)
where  is an approximation of the optimal value of  at step   computed by solving Equation (3)
with  ﬁxed to −1  an optimizer of Į−1().
The above approach lends itself to an implementation of an online MF algorithm  as the sum in Equa-
tion (4) may be eﬃciently optimized whenever adding a new sample. However  in order to satisfy the

=1



3



 ). The values of  are kept constant  and we require [] ࢠ cvx( Į()

convexity constraint of [2]  all previous values of  are needed to update . To mitigate this problem 
we introduce for each cluster() a representative set Į()
 ࢠ Ó× and its convex hull (representative
region) cvx( Į()
 ). As illustrated
 ࣪ cvx(
in Figure 1  we may further restrict the representative regions as follows.
 ())   ࢠ []. This unrestricted case leads
 (Figure 1  Left): We only require that Į()
to an online solution for the cvxMF problem [2] as one may use
 as a single representative
Į()
region. The underlying online algorithms has provable performance guarantees.
 ࣪ ()  which is a new cvxMF constraint for both the
 (Figure 1  Right): We require that Į()
classical and online setting. Theoretical guarantees for the underlying algorithm follow from small and
fairly-obvious modiﬁcations in the proof for the case  assuming error-free sample classiﬁcation.
3 Online Algorithm
The proposed online cvxMF method for solving consists of two procedures  described in Algo-
rithms 1 and 2. Algorithm 1 describes the initialization of the main procedure in Algorithm 2. Algo-
rithm 1 generates an initial estimate for the basis 0 and for the representative regions {cvx( Į()
0 )}ࢠ[].
A similar initialization was used in classical cvxMF  with the bases vectors obtained either through
clustering (on a potentially subsampled dataset) or through random selection and additional pro-
cessing [2]. During initialization  one ﬁrst collects a ﬁxed prescribed number of  data samples 
sample lies in cluster . The sizes of the generated clusters
summarized in Į. Subsequently  one runs the K-means algorithm on the collected samples to obtain
a clustering  described by the cluster indicator matrix  ࢠ {0  1}×  in which [  ] = 1 if the -th
ࢠ[] are used as ﬁxed cardinalities
of the representative sets of the online methods. The initial estimate of the basis 0[] equals the
average of the samples inside the cluster  i.e. 0 á Į  diag(1ࢧ1  …   1ࢧ).
Note again that initialization is performed using only a constant number of  samples. Hence  K-
means clustering does not signiﬁcantly contribute to the complexity of the online algorithm. Second 
to ensure that the restricted online cvxMF algorithm instantiates each cluster with at least one data
sample  one needs to take into account the size of the smallest cluster (discussed in the Supplement).
Algorithm 1 Initialization
1: Input: i.i.d samples 1  2  …    of a random vector X ࢠ Ó summarized in Į.
2: Run K-means on Į to generate the cluster indicator matrix  ࢠ {0  1}× and determine the
3: Compute 0 and Į()

initial cluster sizes (subsequent representative set sizes)    ࢠ [].

0 ࢠ Ó×  ࢘ ࢠ []  according to:



0 = Į  diag(1ࢧ1  …   1ࢧ)

and summarize the initial representative sets of the clusters into matrices Į()

0    = [].

4: Return: 0  { Į()

0 }ࢠ[].

Figure 2: Illustration of one step of the online cvxMF algorithm with multiple-representative regions.
Following initialization  Algorithm 2 sequentially selects one sample  at a time and then updates the
current representative sets Į()
   ࢠ []  and bases . More precisely  after computing the coeﬃcient
vector  inStep5  oneplacesthesample  intotheappropriatecluster  indexedby . The -subsets



4

Algorithm 2 Online cvxMF
1: Input: Data samples   a parameter  ࢠ Ó  and the maximum number of iterations .
2: Initialization: Compute 0  { Į()
3: for  = 1 to  do
4:
5:

0 }ࢠ[] using Algorithm 1. Set 0 =   0 = .

Sample  from X.
Update  according to:

1
2

 = arg min
ࢠÓ

 and  = 1

( − 1)−1 + 

Set  = 1
Choose the index of the basis  to be updated according to  = Uniform([]).

ߠߠߠ − −1ߠߠߠ2
+ ߠߠ1.
( − 1)−1 +  

Generate the augmented representative regions Į{}
 Į()

ࢠ[]Þ{0}

.



:

2











 = Į()
Į{0}
−1
−1[] 
 

[] =

ࢼ Į{}


ࢠ[]

 Į{}



Update { Į()

 }ࢠ[] and  by executing the following two steps:
Compute   Į by solving the optimization problems:

a.

6:
7:
8:

9:

  Į =

=

b.

Set

[]ࢠcvx

[]ࢠcvx

[]ࢠcvx

[]ࢠcvx

arg min
   s.t.
−1

1





 Į()
ࣔ 
 Į{}


 Į()
ࣔ 
 Į{}

 Į{}

1
2



arg min
   s.t.
−1


Į()
−1 

Į()
 =
 = Į.

10: end for
11: return   the learned convex dictionary.

if  ࢠ []  
if  = .

1

ࢣ

=1

ߠߠߠ − 

2



 

ߠߠߠ2

2

+ ߠߠ1

Tr( ) − Tr( ).

 

if  = 
if  ࢠ []   

(5)

(6)

(7)





) Þ } (referred to as the augmented representative sets Į{}

of {col( Į()
   ࢠ [] Þ {0})  are used in
+1 for cluster . To ﬁnd the optimal index
Steps 8 and 9 to determine the new representative region Į()
 ࢠ [] Þ {0} and the corresponding updated basis []  in Step 9 we solve  convex problems.
The minimum of the optimal solutions of these optimization problems determines the new bases 
and the representative regions Į()
(see Figure 2 for clariﬁcations). Note that the combinatorial search
step is executed on a constant-sized set of samples and is hence computationally eﬃcient.
In Step 7  the new sample may be assigned to a cluster in two diﬀerent ways. For the case  we
use a random assignment. For the case   we need to perform the correct sample assignment in
order to establish theoretical guarantees for the algorithm. Extensive simulations show that using
 = arg max  works very well in practice. Note that in either case  in order to minimize ()  one
does not necessarily require an error-free classiﬁcation process.



5



(  )


ࢧvol() 

4 Convergence Analysis
Inwhatfollows weshowthatthesequenceofdictionaries {} convergesalmostsurelytoastationary
point of () under assumptions similar to those used in [1]  listed below.
(A.1) The data distribution on a compact support set has bounded “skewness”. The com-
pact support assumption naturally arises in many practical applications. The bounded
skewness assumption for the distribution of X reads as

ࡇ(ߠX − ߠ2 ࣘ  X ࢠ ) ࣙ  vol
where á cvx(
(8)
())   is a positive constant and (  ) = { ࢼ ߠ − ߠ2 ࣘ } stands
for the ball of radius  around  ࢠ . This assumption is satisﬁed for appropriate values of 
and distributions of X that are “close” to uniform.
(A.2) The quadratic surrogate functions Į are strictly convex  and have Hessians that are
lower-bounded by a positive constant 1 > 0. It is straightforward to enforce this assump-
2 to the surrogate or original objective function; this leads to
tion by adding a term 1
replacing the positive semi-deﬁnite matrix 1
(A.3) The approximation-error function (  ) is “well-behaved”. We assume that the func-
tion (  ) deﬁned in Equation (3) is continuously diﬀerentiable  and that its expectation
() = X[(X  )] is continuously diﬀerentiable and Lipschitz on the compact set. This
assumption parallels the one made in [1  Proposition 2]  and it holds if the solution to Equa-
ߠߠ2
tion (3) is unique. The uniqueness condition can be enforced by adding a regularization term
2 ( > 0) to () in Equation (3). This term makes the (LARS) optimization problem
in Equation (5) strictly convex and hence ensures that it has a unique solution.

  in Equation (7) by 1

2ߠߠ2

  + 1.

In addition  recall the deﬁnition of  and deﬁne 
arg min
[]ࢠcvx( Į()

 = arg min
[]ࢠ  ࢠ[]

 =

 )  ࢠ[]
Į().

Į() 

 as the global optima of the surrogate Į() 

4.1 Main Results
Theorem 1. Under assumptions (A.1) to (A.3)  the sequence {} converges almost surely to a
stationary point of ().
Lemma 2 bounds the diﬀerence of the surrogates for two diﬀerent dictionary arguments. Lemma 3
establishes that restricting the optima of the surrogate function Į() to the representative region
Ý. Lemma 4 establishes
 ) does not aﬀect convergence to the asymptotic global optima 
cvx( Į()
that Algorithm 2 converges almost surely and that the limit is an optima 
Ý. Based on the results
in Lemma 4  Theorem 1 establishes that the generated sequence of dictionaries  converges to a
 ) denote the diﬀerence between the surrogate functions for an unrestricted
Let Ɗ á Į() − Į(
stationary point of (). The proofs are relegated to the Supplement  but sketched below.

basis and a basis for which one requires [] ࢠ cvx
Ɗ ࣘ min

. Then  one can show that
 Į()
−1)
Ɗ−1  Į−1() − Į−1(
 1
 −  Ɗ−1

1

 for [Ɗ].
 +2

Based on an upper bound on the error of random coordinate descent used for minimizing the surrogate
function and assumption (A.1)  one can derive a recurrence relation for [Ɗ]  described in the lemma
below. This recurrence establishes a rate of decrease of 
Lemma 2. Let Ɗ á Į() − Į(
 ࣘ 
Ɗ

+ Ɗ−1

 ). Then

1

2ࢧ(+2)



2  

+ 

.







6

 2

ࢧ2

 


2

Ɖ( 
 Į(+2) vol()
2 +1)

where  á 8 min 
  and  is the same constant used in Equation (8) of assump-
tion (A.1). Also   = max [  ]  ࢘   while  Į denotes a bound on the condition number of   ࢘ 
and  denotes the probability of choosing  =  in Step 7 of Algorithm 2.
Lemma 3 establishes that the optima  conﬁned to the representative region and the global optima
show that Ɗ =ߠ − 
ߠ Į
ߠ2. Lemma 3 then follows from Lemma 2 by applying the
 are close. From the Lipschitz continuity of Į() asserted in assumptions (A.1) and (A.2)  we can

Lemma 3. ࢣ
quasi-martingale convergence theorem stated in the Supplement.
ߠ−
ߠ2

ࣘߠ − 

converges almost surely.



+1

Lemma 4. The following claims hold true:

P1)

P2)

 ) converge almost surely;

Į() and Į(
Į() − Į(
Į(
 ) − (
P3)
P4) (
 ) converges almost surely .

 ) converges almost surely to 0;
 ) converges almost surely to 0;

The proofs of P1) and P2) involve completely new analytic approaches described in the Supplement.
5 Experimental Validation
We compare the approximation error and running time of our proposed online cvxMF algorithm with
non-negative MF (NMF)  cvxMF [2] and online MF [1]. For datasets with a ground truth  we also
report the clustering accuracy. The datasets used include a) clusters of synthetic data samples; b)
MNIST handwritten digits [32]; c) single-cell RNA sequencing datasets [31] and d) four other real
world datasets from the UCI Machine Learning repository [30]. The largest sample size scales as 106.

(a) Well-separated clusters.

(b) Overlapping clusters.

Figure 3: Results for Gaussian mixtures with color-coded clusters. Here  tSNE stands for the t-
distributed stochastic neighbor embedding [33]  in which the -axis represents the ﬁrst and the -axis
the second element of the embedding. Color-coded circles represent samples  diamonds represent
basis vectors learned by the diﬀerent algorithms  while crosses describe samples in the representative
regions. The “interpretability property” can be easily observed visually.
Synthetic Datasets. The synthetic datasets were generated by sampling from a 3-truncated Gaussian
mixture model with 5 components  and with samples-sizes in [103  106]. Each component Gaussian
has an expected value drawn uniformly at random from [0  20] while the mixture covariance matrix
equals the identity matrix  (“well-separated clusters”) or 2.5  ("overlapping clusters"). We ran the
online cvxMF algorithm with both unconstrained and restricted representative regions  and

7

used the normalization factor  = ࢧÝ suggested in [34]. After performing cross validation on an

evaluation set of size 1000  we selected  = 0.2. Figure 3 shows the results for two synthetic datasets
each of size  = 2  500 and with  = 150. The sample size was restricted for ease of visualization
and to accommodate the cvxMF method which cannot run on larger sets. The number of iterations
was limited to 1  200. Both the cvxMF and online cvxMF algorithms generate bases that provide
excellent representations of the data clusters. The MF and online MF method produce bases that
are hard to interpret and fail to cover all clusters. Note that for the unrestricted version of cvxMF 
samples of one representative set may belong to multiple clusters.
For the same Gaussian mixture model but larger datasets  we present running times and times to
convergence (or  if convergence is slow  the maximum number of iterations) in Figure 4 (a) and (b) 
respectively. For well-separated synthetic datasets  we let  increase from 102 to 106 and plot the
results in (a). The non-online cvxMF algorithm becomes intractable after 104 sample  while the
cvxMF and MF easily scale for 106 and more samples. To illustrate the convergence  we used a
synthetic dataset with  = 5  000 in order to ensure that all four algorithms converge within 100s.
Figure 4 (b) plots the approximation error 2 = 1
chose a small value of  so as to be able to run all algorithms  and for this case the online algorithms
may have larger errors. But as already pointed out  as  increases  non-online algorithms become
intractable while the online counterparts operate eﬃciently (and with provable guarantees).

ߠ − ߠ2 with respect to the running time. We

(a) time complexity of diﬀerent methods

(b) convergence of objective

Figure 4: (a): Running times (s) vs. the log of the dataset sizes; (b) Running times (s) vs. the 2 error.

(b) cvxMF

(c) online MF

(d) online cvxMF ()

(a) MF

Figure 5: MNIST results (as the eigenimage set is overcomplete  clustering accuracy is omitted).
The MNIST Dataset. The MNIST dataset was subsampled to a smaller set of 10  000 images of
resolution 28×28toillustratetheperformanceofboththecvxMFandonlinecvxMFmethodsonimage
datasets. All algorithms ran 3  000 iterations with  = 150 and  = 0.1 to generate “eigenimages ”
capturing the characteristic features used as bases [35]. Figure 5 plots the ﬁrst 9 eigenimages. The
results for the  algorithm are similar to that of the non-online cvxMF algorithm and omitted.
CvxMF produces blurry images since one averages all samples. The results are signiﬁcantly better
for the case  as one only averages a small subset of representative samples.
Single-Cell (sc) RNA Data. scRNAdatatsetscontainexpressions(activities)ofallgenesinindividual
cells  and each cell represents one data sample. Cells from the same tissue under same cellular
condition tend to cluster  and due to the fact that that the sampled tissue are known  the cell labels are

8

known a priori. This setting allows us to investigate the version of the online cvxMF algorithm
to identify “typical” samples. For our dataset  described in more detail in the Supplement  the two
non-online method failed to converge and required signiﬁcantly larger memory. Hence  we only
present results for the online methods. Results pertaining to real world datasets from the UCI Machine

Figure 6: Results for the online methods executed on a blood-cell scRNA dataset.
Learning repository [30]  also used for testing cvxMF [2]  are presented in the Supplement.
6 Acknowledgement
The authors are grateful to Prof. Bruce Hajek for valuable discussions. This work was funded by the
DB2K NIH 3U01CA198943-02S1  NSF/IUCR CCBGM Center  and the SVCF CZI 2018-182799
2018-182797.
References
[1] Julien Mairal  Francis Bach  Jean Ponce  and Guillermo Sapiro. Online learning for matrix
factorization and sparse coding. Journal of Machine Learning Research  11(Jan):19–60  2010.
[2] Chris HQ Ding  Tao Li  and Michael I Jordan. Convex and semi-nonnegative matrix fac-
torizations. IEEE transactions on pattern analysis and machine intelligence  32(1):45–55 
2010.
[3] Avrim Blum  Sariel Har-Peled  and Benjamin Raichel. Sparse approximation via generating
point sets. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete
algorithms  pages 548–557. Society for Industrial and Applied Mathematics  2016.
[4] Jieru Mei  Chunyu Wang  and Wenjun Zeng. Online dictionary learning for approximate
archetypal analysis. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 486–501  2018.
[5] Ivana Tosic and Pascal Frossard. Dictionary learning. IEEE Signal Processing Magazine 
28(2):27–38  2011.
[6] JulienMairal JeanPonce GuillermoSapiro AndrewZisserman andFrancisRBach. Supervised
dictionary learning. In Advances in neural information processing systems  pages 1033–1040 
2009.
[7] Ron Rubinstein  Michael Zibulevsky  and Michael Elad. Double sparsity: Learning sparse dic-
tionaries for sparse signal approximation. IEEE Transactions on signal processing  58(3):1553–
1564  2010.
[8] WeiDai  TaoXu  andWenwuWang. Simultaneouscodewordoptimization(simco)fordictionary
update and learning. IEEE Transactions on Signal Processing  60(12):6340–6353  2012.
[9] Laurens Van Der Maaten  Eric Postma  and Jaap Van den Herik. Dimensionality reduction: a
comparative. J Mach Learn Res  10:66–71  2009.
[10] Nathan Srebro  Jason Rennie  and Tommi S Jaakkola. Maximum-margin matrix factorization.
In Advances in neural information processing systems  pages 1329–1336  2005.

9

[11] HaifengLiu ZhaohuiWu XuelongLi DengCai andThomasSHuang. Constrainednonnegative
matrix factorization for image representation. IEEE Transactions on Pattern Analysis and
Machine Intelligence  34(7):1299–1311  2012.

[12] Francis Bach  Julien Mairal  and Jean Ponce. Convex sparse matrix factorizations. arXiv preprint

arXiv:0812.1869  2008.

[13] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In

Advances in neural information processing systems  pages 556–562  2001.

[14] Pentti Paatero and Unto Tapper. Positive matrix factorization: A non-negative factor model
with optimal utilization of error estimates of data values. Environmetrics  5(2):111–126  1994.
[15] Xiao Wang  Peng Cui  Jing Wang  Jian Pei  Wenwu Zhu  and Shiqiang Yang. Community
preserving network embedding. In Thirty-First AAAI Conference on Artiﬁcial Intelligence  2017.
[16] Cédric Févotte and Nicolas Dobigeon. Nonlinear hyperspectral unmixing with robust non-
negative matrix factorization. IEEE Transactions on Image Processing  24(12):4810–4819 
2015.

[17] Nicolas Gillis and François Glineur. Accelerated multiplicative updates and hierarchical als
algorithms for nonnegative matrix factorization. Neural computation  24(4):1085–1105  2012.
[18] Ernie Esser  Michael Moller  Stanley Osher  Guillermo Sapiro  and Jack Xin. A convex model
for nonnegative matrix factorization and dimensionality reduction on physical space. IEEE
Transactions on Image Processing  21(7):3239–3252  2012.

[19] Guillaume Bouchard  Dawei Yin  and Shengbo Guo. Convex collective matrix factorization. In

Artiﬁcial Intelligence and Statistics  pages 144–152  2013.

[20] Xingguo Li  Zhaoran Wang  Junwei Lu  Raman Arora  Jarvis Haupt  Han Liu  and Tuo Zhao.
Symmetry  saddle points  and global geometry of nonconvex matrix factorization. arXiv preprint
arXiv:1612.09296  2016.

[21] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural compu-

tation  19(10):2756–2779  2007.

[22] Stephen A Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal on

Optimization  20(3):1364–1377  2009.

[23] Rémi Gribonval  Rodolphe Jenatton  Francis Bach  Martin Kleinsteuber  and Matthias Seibert.
Sample complexity of dictionary learning and other matrix factorizations. IEEE Transactions
on Information Theory  61(6):3469–3486  2015.

[24] Honglak Lee  Alexis Battle  Rajat Raina  and Andrew Y Ng. Eﬃcient sparse coding algorithms.

In Advances in neural information processing systems  pages 801–808  2007.

[25] Shiva P Kasiviswanathan  Huahua Wang  Arindam Banerjee  and Prem Melville. Online
l1-dictionary learning with application to novel document detection. In Advances in Neural
Information Processing Systems  pages 2258–2266  2012.

[26] Jun Tang  Ke Wang  and Ling Shao. Supervised matrix factorization hashing for cross-modal

retrieval. IEEE Transactions on Image Processing  25(7):3157–3166  2016.

[27] Renbo Zhao  Vincent YF Tan  and Huan Xu. Online nonnegative matrix factorization with

general divergences. arXiv preprint arXiv:1608.00075  2016.

[28] Renbo Zhao and Vincent YF Tan. Online nonnegative matrix factorization with outliers. In
2016 IEEE International Conference on Acoustics  Speech and Signal Processing (ICASSP) 
pages 2662–2666. IEEE  2016.

[29] Rui Xia  Vincent YF Tan  Louis Filstroﬀ  and Cédric Févotte. A ranking model motivated
by nonnegative matrix factorization with applications to tennis tournaments. arXiv preprint
arXiv:1903.06500  2019.

10

[30] Dheeru Dua and Casey Graﬀ. UCI machine learning repository  2017.
[31] Grace XY Zheng  Jessica M Terry  Phillip Belgrader  Paul Ryvkin  Zachary W Bent  Ryan
Wilson  Solongo B Ziraldo  Tobias D Wheeler  Geoﬀ P McDermott  Junjie Zhu  et al. Massively
parallel digital transcriptional proﬁling of single cells. Nature communications  8:14049  2017.
[32] Yann LeCun  Léon Bottou  Yoshua Bengio  Patrick Haﬀner  et al. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[33] Laurens van der Maaten and Geoﬀrey Hinton. Visualizing data using t-sne. Journal of machine

learning research  9(Nov):2579–2605  2008.

[34] Peter J Bickel  Ya’acov Ritov  Alexandre B Tsybakov  et al. Simultaneous analysis of lasso and

dantzig selector. The Annals of Statistics  37(4):1705–1732  2009.

[35] Aleš Leonardis and Horst Bischof. Robust recognition using eigenimages. Computer Vision

and Image Understanding  78(1):99–118  2000.

11

,Jianhao Peng
Olgica Milenkovic
Abhishek Agarwal