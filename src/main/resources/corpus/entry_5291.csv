2011,Structural equations and divisive normalization for energy-dependent component analysis,Components estimated by independent component analysis and related methods  are typically not independent in real data. A very common form of nonlinear  dependency between the components is correlations in their variances or ener-  gies. Here  we propose a principled probabilistic model to model the energy-  correlations between the latent variables. Our two-stage model includes a linear  mixing of latent signals into the observed ones like in ICA. The main new fea-  ture is a model of the energy-correlations based on the structural equation model  (SEM)  in particular  a Linear Non-Gaussian SEM. The SEM is closely related to  divisive normalization which effectively reduces energy correlation. Our new two-  stage model enables estimation of both the linear mixing and the interactions re-  lated to energy-correlations  without resorting to approximations of the likelihood  function or other non-principled approaches. We demonstrate the applicability of  our method with synthetic dataset  natural images and brain signals.,Structural equations and divisive normalization for

energy-dependent component analysis

Jun-ichiro Hirayama
Dept. of Systems Science

Graduate School of of Informatics

Kyoto University

611-0011 Uji  Kyoto  Japan

Aapo Hyv¨arinen

Dept. of Mathematics and Statistics
Dept. of Computer Science and HIIT

University of Helsinki
00560 Helsinki  Finland

Abstract

Components estimated by independent component analysis and related methods
are typically not independent in real data. A very common form of nonlinear
dependency between the components is correlations in their variances or ener-
gies. Here  we propose a principled probabilistic model to model the energy-
correlations between the latent variables. Our two-stage model includes a linear
mixing of latent signals into the observed ones like in ICA. The main new fea-
ture is a model of the energy-correlations based on the structural equation model
(SEM)  in particular  a Linear Non-Gaussian SEM. The SEM is closely related to
divisive normalization which effectively reduces energy correlation. Our new two-
stage model enables estimation of both the linear mixing and the interactions re-
lated to energy-correlations  without resorting to approximations of the likelihood
function or other non-principled approaches. We demonstrate the applicability of
our method with synthetic dataset  natural images and brain signals.

1 Introduction

Statistical models of natural signals have provided a rich framework to describe how sensory neurons
process and adapt to ecologically-valid stimuli [28  12]. In early studies  independent component
analysis (ICA) [2  31  13] and sparse coding [22] have successfully shown that V1 simple cell-like
edge ﬁlters  or receptive ﬁelds  emerge as optimal inference on latent quantities under linear genera-
tive models trained on natural image patches. In the subsequent developments over the last decade 
many studies (e.g. [10  32  11  14  23  17]) have focused explicitly or implicitly on modeling a par-
ticular type of nonlinear dependency between the responses of the linear ﬁlters  namely correlations
in their variances or energies. Some of them showed that models on energy-correlation could ac-
count for  e.g.  response properties of V1 complex cells [10  15]  cortical topography [11  23]  and
contrast gain control [26].

Interestingly  such energy correlations are also prominent in other kinds of data  including brain
signals [33] and presumably even ﬁnancial time series which have strong heteroscedasticity. Thus 
developing a general model for energy-correlations of linear latent variables is an important problem
in the theory of machine learning  and such models are likely to have a wide domain of applicability.

Here  we propose a new statistical model incorporating energy-correlations within the latent vari-
ables. Our two-stage model includes a linear mixing of latent signals into the observed ones like
in ICA  and a model of the energy-correlations based on the structural equation model (SEM) [3] 
in particular the Linear Non-Gaussian (LiNG) SEM [27  18] developed recently. As a model of
natural signals  an important feature of our model is its connection to “divisive normalization”
(DN) [7  4  26]  which effectively reduces energy-correlations of linearly-transformed natural sig-
nals [32  26  29  19  21] and is now part of a well-accepted model of V1 single cell responses [12].

1

We provide a new generative interpretation of DN based on the SEM  which is an important con-
tribution of this work. Also  from machine learning perspective  causal analysis by using SEM has
recently become very popular; our model could extend the applicability of LiNG-SEM for blindly
mixed signals.

As a two-stage extension of ICA  our model is also closely related to both the scale-mixture-based
models  e.g. [11  30  14] (see also [32]) and the energy-based models  e.g. [23  17]. An advantage of
our new model is its tractability: our model requires neither an approximation of likelihood function
nor non-canonical principles for modeling and estimation as previous models.

2 Structural equation model and divisive normalization

A structural equation model (SEM) [3] of a random vector y = (y1  y2  . . .   yd)
simultaneous equations of random variables  such that

⊤

is formulated as

yi = κi(yi  y−i  ri) 

i = 1  2  . . .   d 

(1)

.

′

⊤

or y = κ(y  r)  where the function κi describes how each single variable yi is related to other
variables y−i  possibly including itself  and a corresponding stochastic disturbance or external input
ri which is independent of y. These equations  called structural equations  specify the distribution
of y  as y is an implicit function (assuming the system is invertible) of the random vector r =
(r1  r2  . . .   rd)
If there exists a permutation (cid:5) : y 7→ y
′
i only depends on the preceding ones
|j < i}  an SEM is called recursive or acyclic  associated with a directed acyclic graph (DAG);
{y
the model is then a cascade of (possibly) nonlinear regressions of yi’s on the preceding variables
on the graph  and is also seen as a Bayesian network. Otherwise  the SEM is called non-recursive
or cyclic  where the structural equations cannot be simply decomposed into regressive models. In
a standard interpretation  a cyclic SEM rather describes the distribution of equilibrium points of a
dynamical system  y(t) = κ(y(t − 1)  r) (t = 0  1  . . .)  where every realized input r is ﬁxed until
y(t) converges to y [24  18]; some conditions are usually needed to make the interpretation valid.

such that each y

′
j

2.1 Divisive normalization as non-linear SEM

⊤

Now  we brieﬂy point out the connection of SEM to DN  which strongly motivated us to explore the
application of SEM to natural signal statistics.
Let s1  s2  . . .   sd be scalar-valued outputs of d linear ﬁlters applied to a multivariate input  collec-
tively written as s = (s1  s2  . . .   sd)
. The linear ﬁlters may either be derived/designed with some
mathematical principles (e.g. Wavelets) or be learned from data (e.g. ICA). The outputs of linear
ﬁlters often have the property that their energies ϕ(|si|) (i = 1  2  . . .   d) have non-negligible depen-
dencies or correlations to each other  even when the outputs themselves are linearly uncorrelated.
The nonlinear function ϕ is any appropriate measure of energy  typically given by the squaring func-
tion  i.e. ϕ(|s|) = s2 [26  12]  while other choices will not be excluded; we assume ϕ is continuously
differentiable and strictly increasing over [0 ∞)  and ϕ(0) = 0.
Divisive Normalization (DN) [26] is an effective nonlinear transformation for eliminating the
energy-dependencies remained in the ﬁltered outputs. Although several variants have been pro-
posed  a basic form can be formulated as follows: Given the d outputs  their energies are normalized
(divided) by a linear combination of the energies of other signals  such that

 

i = 1  2  . . .   d 

(2)

∑

zi =

ϕ(|si|)
j hijϕ(|sj|) + hi0
∑

where hij and hi0 are real-valued parameters of this transform. Now  it is straightforward to see that
the following structural equations in the log-energy domain 

yi := ln ϕ(|si|) = ln(

hij exp(yj) + hi0) + ri 

i = 1  2  . . .   d 

(3)

j

correspond to Eq. (2) where zi = exp(ri) is another representation of the disturbance. The SEM will
typically be cyclic  since the coefﬁcients hij in Eq. (2) are seldom constrained to satisfy acyclicity;

2

Eq. (3) thus implies a nonlinear dynamical system  and this can be interpreted as the data-generating
processes underlying DN. Interestingly  Eq. (3) also implies a linear system with multiplicative

∑
j hijeyj + hi0)zi  in the energy domain  i.e. eyi := ϕ(|si|). The DN transform of

input  eyi = (
Note also that the SEM above implies ey = (I − diag(z)H)

Eq. (2) gives the optimal mapping under the SEM to infer the disturbance from given si’s; if the true
disturbances are independent  it optimally reduces the energy-dependencies. This is consistent with
the redundancy reduction view of DN [29  19].

−1diag(h0)z with H = (hij) and
h0 = (hi0)  as shown in [20] in the context of DN 1. Although mathematically equivalent  such a
complicated dependence [20] on the disturbance z does not provide an elegant model of the under-
lying data-generating process  compared to relatively the simple form of Eq. (3).

3 Energy-dependent ICA using structural equation model

Now  we deﬁne a new generative model which models energy-dependencies of linear latent compo-
nents using an SEM.

3.1 Scale-mixture model

⊤
Let s now be a random vector of d source signals underlying an observation x = (x1  x2  . . .   xd)
which has the same dimensionality for simplicity. They follow a standard linear generative model:

x = As 

(4)

where A is a square mixing matrix. We assume here E[x] = E[s] = 0 without loss of generality  by
always subtracting the sample mean from every observation. Then  assuming A is invertible  each
−1 gives the optimal ﬁlter to recover
transposed row wi of the demixing (ﬁltering) matrix W = A
si from x  which is constrained to have unit norm  ∥wi∥2
To introduce energy-correlations into the sources  a classic approach is to use a scale-mixture rep-
resentation of sources  such that si = uiσi  where ui represents a normalized signal having zero
mean and constant variance  and σi is a positive factor that is independent of ui and modulates the
variance (energy) of si [32  11  30  14  16]. Also  in vector notation  we write

2 = 1 to ﬁx the scaling ambiguity.

s = u ⊙ (cid:27) 

(5)
where ⊙ denotes component-wise multiplication. Here  u and (cid:27) are mutually independent  and ui’s
are also independent of each other. Then E[s|(cid:27)] = 0 and E[ss
d) for any
given (cid:27)  where σi’s may be dependent of each other and introduce energy-correlations. A drawback
of this approach is that to learn effectively the model based on the likelihood  we usually need some
approximation to deal with the marginalization over u.

⊤|(cid:27)] = diag(σ2

2  . . .   σ2

1  σ2

3.2 Linear Non-Gaussian SEM
Here  we simplify the above scale-mixture model by restricting ui to be binary  i.e. ui ∈ {−1  1} 
and uniformly distributed. Although the simpliﬁcation reduces the ﬂexibility of source distribution 
the resultant model is tractable  i.e. no approximation is needed for likelihood computation  as will
be shown below. Also  this implies that ui = sign(si) and σi = |si|  and hence the log-energy
above now has a simple deterministic relation to σi  i.e. yi = ln ϕ(σi)  which can be inverted to
σi = ϕ
We particularly assume the log-energies yi follow the Linear Non-Gaussian (LiNG) [27  18] SEM:

−1(exp(yi)).

∑

yi =

hijyj + hi0 + ri 

i = 1  2  . . .   d 

(6)

j

where the disturbances are zero-mean and in particular assumed to be non-Gaussian and independent
of each other  which has been shown to greatly improve the identiﬁability of linear SEMs [27];
the interaction structure in Eq. (6) can be represented by a directed graph for which the matrix
1To be precise  [20] showed the invertibility of the entire mapping s 7! z in the case of a “signed” DN

transform that keeps the signs of zi and si to be the same.

3

(∏

eyi =

)

eyhij

j

j

DN transform  given by

H = (hij) serves as the weighted adjacency matrix. In the energy domain  Eq. (6) is equivalent to
ehi0 zi (i = 1  2  . . .   d)  and interestingly  these SEMs further imply a novel form of

∏
ϕ(|si|)
j ϕ(|sj|)hij

zi =

ehi0

 

i = 1  2  . . .   d 

(7)

where the denominator is now not additive but multiplicative. It provides an interesting alternative
to the original DN.

To recapitulate the new generative model proposed here: 1) The log-energies y are generated accord-
−1(exp(yi))
ing to the SEM in Eq. (6); 2) the sources are generated according to Eq. (5) with σi = ϕ
and random signs  ui; and 3) the observation x is obtained by linearly mixing the sources as in
Eq. (4). In our model  the optimal mapping to infer zi = exp(ri) from x under this model is the
linear ﬁltering W followed by the new DN transform  Eq. (7). On the other hand  it would also be
possible to deﬁne the energy-dependent ICA by using the nonlinear SEM in Eq. (3) instead. Then 
the optimal inference would be given by the divisive normalization in Eq. (2). However  estimation
and other theoretical issues (e.g. identiﬁability) related to nonlinear SEMs  particularly in the case
of non-Gaussianity of the disturbances  are quite involved  and are still under development  e.g. [8].

3.3

Identiﬁability issues

Both the theory and algorithms related to LiNG coincide largely with those of ICA  since Eq. (6)
with non-Gaussian r implies the generative model of ICA  y = Br + b0  where B = (I − H)
−1
and b0 = Bh0 with h0 = (hi0). Like ICA [13]  Eq. (6) is not completely identiﬁable due to
the ambiguities related to scaling (with signs) and permutation [27  18]. To ﬁx the scaling  we set
] = I here. The permutation ambiguity is more serious than in the case of ICA  because
E[rr
the row-permutation of H completely changes the structure of corresponding directed graph  and is
typically addressed by constraining the graph structure  as will be discussed next.

⊤

Two classes of LiNG-SEM have been proposed  corresponding to different constraints on the graph
structure. One is LiNGAM [27]  which ensures the full identiﬁability by the DAG constraint. The
other is generally referred to as LiNG [18] which allows general cyclic graphs; the “LiNG discovery”
algorithm in [18] dealt with the non-identiﬁability of cyclic SEMs by ﬁnding out multiple solutions
that give the same distribution.

Here we deﬁne two variants of our model: One is the acyclic model  using LiNGAM. In contrast
to original LiNGAM  our target is (linear) latent variables  but not observed ones. The ordering of
latent variables is not meaningful  because the rows of ﬁlter matrix W can be arbitrarily permuted.
The acyclic constraint thus can be simpliﬁed into a lower-triangular constraint on H. Another one is
the symmetric model  which uses a special case of cyclic SEM  i.e. those with a symmetric constraint
on H. Such constraint would be relatively new to the context of SEM  although it is a well-known
setting in the ICA literature (e.g. [5]). The SEM is then identiﬁable using only the ﬁrst- and second-
order statistics  based on the relations h0 = VE[y] and V := I − H = Cov[y]
2 [5]  provided
that V is positive deﬁnite 2. This implies the non-Gaussianity is not essential for identiﬁability  in
contrast that the acyclic model is not identiﬁable without non-Gaussianity [27]. The above relations
also suggest moment-based estimators of h0 and V  which can be used either as the ﬁnal estimates
or as the initial conditions in the maximum likelihood algorithm below.

− 1

3.4 Maximum likelihood
∏
(|s|) as a con-
Let ψ(s) := ln ϕ(|s|) for notational simplicity  and denote ψ
vention  e.g. (ln|s|)
′
:= 1/s. Also  following the basic theory of ICA  we assume the disturbances
have a joint probability density function (pdf) pr(r) =
i ρ(ri) with a common ﬁxed marginal pdf
ρ. Then  we have the following pdf of s without any approximation (see Appendix for derivation):

′
(s) := sign(s)(ln ϕ)

′

i ψ(s) − hi0)|ψ
⊤

′

(si)|.

ρ(v

(8)

| det V| d∏

i=1

ps(s) =

1
2d

2Under the dynamical system interpretation  the matrix H should have absolute eigenvalues smaller than
one for stability [18]  where V = I (cid:0) H is naturally positive deﬁnite because the eigenvalues are all positive.

4

Figure 1: Estimation performance of mixing matrix measured by the “Amari Index” [1] (non-
negative  and zero denotes perfect estimation up to unavoidable indeterminacies) versus sample
size  shown in log-log scales. Each panel corresponds to a particular value of α  which determined
the relative connection strength between sources. The solid lines denotes the median of ten runs.

∑

where vi is i-th transposed row vector of V (= I − H). The pdf of x is given by px(x) =
| det W|ps(Wx)  and the corresponding loss function  l = − ln px(x) + const.  is given by

∑
l(x  W  V  h0) = (cid:22)f (Vψ(Wx) − h0) + (cid:22)g(Wx) − ln| det W| − ln| det V| 
(si)|.
i f (ri)  f (ri) = − ln ρ(ri)  (cid:22)g(s) =

i g(si)  and g(si) = − ln|ψ

where (cid:22)f (r) =
Note that the loss function above is closely related to the ones in previous studies  such as of energy-
based models [23  17]. Our model is less ﬂexible to these models  since it is limited to the case that
A is square  but the exact likelihood is available. It is also interesting to see that the loss function
above includes an additional second term that has not appeared in previous models  due to the formal
derivation of pdf by the argument of transformation of random variables.
To obtain the maximum likelihood estimates of W  V  and h0  we minimize the negative log-
likelihood (i.e. empirical average of the losses) by the projected gradient method (for the unit-norm
constraints  ∥wi∥2

(9)

′

2 = 1). The required ﬁrst derivatives are given by
−⊤
∂l
∂h0
∂l
∂W

(Vy − h0)y
⊤ − V
(Vy − h0) + g
′
′

{
= −f

(Wx))V

∂l
∂V

diag(ψ

(r) 

= f

=

′

⊤

f

 

}

(Wx)

′

′

⊤ − W
x

−⊤

.

(10a)

(10b)

In both acyclic and symmetric cases  only the lower-triangular elements in V are free parameters.
If acyclic  the upper-triangular elements are ﬁxed at zero; if symmetric  they are dependent of the
lower-triangular elements  and thus ∂l/∂vij (i > j) should be replaced with ∂l/∂vij + ∂l/∂vji.

4 Simulations

To demonstrate the applicability of our method  we conducted the following simulation experiments.
In all experiments below  we set ϕ(|s|) = |s|  and ρ(r) = (1/2)sech(πr/2) corresponding to
the standard tanh nonlinearity in ICA: f
In our projected gradient
algorithm  the matrix W was ﬁrst initialized by FastICA [9]; the SEM parameters  H and h0  were
initialized by the moment-based estimator described above (symmetric model) or by the LiNGAM
algorithm [27] (acyclic model). The algorithm was terminated when the decrease of objective value
−6; the learning rate was adjusted in each step by simply multiplying it by the
was smaller than 10
factor 0.9 until the new point did not increase the objective value.

(r) = (π/2) tanh((π/2)r).

′

4.1 Synthetic dataset

First  we examined how the energy-dependence learned in the SEM affects the estimation of linear
ﬁlters. We artiﬁcially sampled the dataset with d = 10 from our generative model by setting the
matrix V to be tridiagonal  where all the main and the ﬁrst diagonals were set at 10 and 10α 
respectively. Figure 1 shows the “Amari Index” [1] of estimated W by three methods  at several

5

102103100101a=−0.4Amari Index102103a=−0.3102103a=−0.2102103a=0Sample Size102103a=0.2  FastICA102103a=0.3  No Dep.102103a=0.4  ProposedFigure 2: Connection weights versus pairwise differences of four properties of linear basis functions 
estimated by ﬁtting 2D Gabor functions. The curves were ﬁt by local Gaussian smoothing.

factors α and sample sizes  with ten runs for every condition. In each run  the true mixing matrix
was given by inverting W randomly generated from standard Gaussian and then row-normalized to
have unit norms. The three methods were: 1) FastICA 3 with the tanh nonlinearity  2) Our method
(symmetric model) without energy-dependence (NoDep) initialized by FastICA  and 3) Our full
method (symmetric model) initialized by NoDep. NoDep was the same as the full method except
that the off-diagonal elements of H was kept zero. Note that our two algorithms used exactly the
same criterion for termination of algorithm  while FastICA used a different one. This could cause
the relatively poor performance of FastICA in this ﬁgure. The comparison between the full method
and NoDep showed that energy-dependence learned in the SEM could improve the estimation of
ﬁlter matrix  especially when the dependence was relatively strong.

4.2 Natural images
The dataset consisted of 50  000 image patches of 16 × 16 pixels randomly taken from the original
gray-scale pictures of natural scenes 4. As a preprocessing  the sample mean was subtracted and
the dimensionality was reduced to 160 by the principal component analysis (PCA) where 99% of
the variance was retained. We constrained the SEM to be symmetric. Both of the obtained basis
functions and ﬁlters were qualitatively very similar to those reported in many previous studies  and
given in the Supplementary Material.
Figure 2 shows the values of connection weights hij (after a row-wise re-scaling of V to set any
hii = 1 − vii to be zero  as a standard convention in SEM [18]) for every d(d − 1) pairs  compared
with the pairwise difference of four properties of learned features (i.e. basis functions)  estimated by
ﬁtting 2D Gabor functions: spatial positions  frequencies  orientations and phases. As is clearly seen 
the connection weights tended to be large if the features were similar to each other  except for their
phases; the phases were not strongly correlated with the weights as suggested by the ﬁtted curve 
while they exhibited a weak tendency to be the same or the opposite (shifted ±π) to each other. We
can also see a weak tendency for the negative weights to have large magnitudes when the pairs have
near-orthogonal directions or different frequencies. Figure 3 illustrates how the learned features are
associated with the other ones  using iconiﬁed representations. We can see: 1) associations with
positive weights between features were quite spatially-localized and occur particularly with similar
orientations  and 2) those with negative weights especially occur from cross-oriented features to a
target  which were sometimes non-localized and overlapped to the target feature. Notice that in the
DN transform (7)  these positive weights learned in the SEM perform as inhibitory and will suppress
the energies of the ﬁlters having similar properties.

4.3 Magnetoencephalography (MEG)

Brain activity was recorded in a single healthy subject who received alternating visual  auditory  and
tactile stimulation interspersed with rest periods [25]. The original signals were measured in 204
channels (sensors) for several minutes with sampling rate (75Hz); the total number of measurements 
i.e. sample size  was N = 73  760. As a preprocessing  we applied a band-pass ﬁlter (8-30Hz) and
remove some outliers. Also  we subtracted the sample mean and then reduced the dimensionality by
PCA to d = 24  with 90% of variance still retained.

3Matlab package is available at http://research.ics.tkk.ﬁ/ica/fastica/. We used the following options: g=tanh 

approach=symm  epsilon=10

(cid:0)6  MaxNumIterations=104  ﬁnetune=tanh.

4Available in Imageica Toolbox by Patrik Hoyer  at http://www.cs.helsinki.ﬁ/u/phoyer/software.html

6

00.5100.020.040.06PositionPairwise DistanceConnection Weight−10100.020.040.06OrientationPairwise Difference (mod –p/2)Connection Weight−0.200.200.020.040.06FrequencyPairwise DifferenceConnection Weight−20200.020.040.06PhasePairwise Difference (mod – p)Connection WeightFigure 3: Depiction of connection properties between learned basis functions in a similar manner
to that has used in e.g. [6]. In each small panel  the black bar depicts the position  orientation and
length of a single Gabor-like basis function obtained by our method; the red (resp. blue) pattern
of superimposed bars is a linear combination of the bars of the other basis functions according to
the absolute values of positive (resp. negative) connection weights to the target one. The intensities
of red and blue colors were adjusted separately from each other in each panel; the ratio of the
maximum positive and negative connection strengths is depicted at the bottom of each small panel
by the relative length of horizontal color bars.

Figure 4: Estimated interaction graph (DAG) for MEG data. The red and blue edges respec-
tively denotes the positive and negative connections. Only the edges with strong connections are
drawn  where the absolute threshold value was the same for positive and negative weights. The two
manually-inserted contours denote possible clusters of sources (see text).

7

Figure 4 shows an interaction graph under the DAG constraint. One cluster of components  high-
lighted in the ﬁgure by the manually inserted yellow contour  seems to consist of components related
to auditory processing. The components are located in the temporal cortex  and all but one in the
left hemisphere. The direction of inﬂuence  which we can estimate in the acyclic model  seems to
be from the anterior areas to posterior ones. This may be related to top-down inﬂuence  since the
primary auditory cortex seems to be included in the posterior areas on the left hemisphere; at the
end of the chain  the signal goes to the right hemisphere. Such temporal components are typically
quite difﬁcult to ﬁnd because the modulation of their energies is quite weak. Our method may help
in grouping such components together by analyzing the energy correlations.

Another cluster of components consists of low-level visual areas  highlighted by the green contour.
It is more difﬁcult to interpret these interactions because the areas corresponding to the components
are very close to each other. It seems  however  that here the inﬂuences are mainly from the primary
visual areas to the higher-order visual areas.

5 Conclusion

We proposed a new statistical model that uses SEM to model energy-dependencies of latent variables
in a standard linear generative model. In particular  with a simpliﬁed form of scale-mixture model 
the likelihood function was derived without any approximation. The SEM has both acyclic and
cyclic variants. In the acyclic case  non-Gaussianity is essential for identiﬁability  while in the cyclic
case we introduces the constraint of symmetricity which also guarantees identiﬁability. We also
provided a new generative interpretation of DN transform based on a nonlinear SEM. Our method
exhibited a high applicability in three simulations each with synthetic dataset  natural images  and
brain signals.

∫

∫

S1

Appendix: Derivation of Eq. (8)
From the uniformity of signs  we have ps(s) = ps(Ds) for any D = diag(±1  . . .  ±1); par-
ticularly  let Dk correspond to the signs of k-th orthant Sk of Rd  and S1 = (0 ∞)d. Then  the
d(cid:27) ps((cid:27)) im-
relation
plies ps(s) = (1/2d)p(cid:27)(s) for any s ∈ S1; thus ps(s) = (1/2d)p(cid:27)(|s|) for any s ∈ Rd. Now 
(σi)|  where we assume
|(ln ϕ)
′
y = ln ϕ(σ) (for every component) and thus p(cid:27)((cid:27)) = py(y)
ϕ is differentiable. Let ψ(s) := ln ϕ(|s|) and ψ
(|s|). Then it follows that
′
(s) := sign(s)(ln ϕ)
(si)|  where ψ(s) performs component-wise. Since y maps lin-
ps(s) = (1/2d)py(ψ(s))
i ρ(ri); combining it

early to r with the absolute Jacobian | det V|  we have py(y) = | det V|∏

d(cid:27) ps(Dk(cid:27)) = 2d

∑
∏

d(cid:27) p(cid:27)((cid:27)) =

ds ps(s) =

K
k=1

Sk

∏

i

∫

S1

′

|ψ

i

∑

∫

K
k=1

S1

′

with ps above  we obtain Eq. (8).

Acknowledgements

We would like to thank Jes´us Malo and Valero Laparra for inspiring this work  Michael Gutmann
and Patrik Hoyer for helpful discussions and providing codes for ﬁtting Gabor functions and visual-
ization. The MEG data was kindly provided by Pavan Ramkumar and Riitta Hari. J.H. was partially
supported by JSPS Research Fellowships for Young Scientists.

References

[1] S. Amari  A. Cichoki  and H. H. Yang. A new learning algorithm for blind signal separation. In Advances

in Neural Information Processing Systems  volume 8  1996.

[2] A. J. Bell and T. J. Sejnowski. The ‘independent components’ of natural scenes are edge ﬁlters. Vision

Res.  37:3327–3338  1997.

[3] K. A. Bollen. Structural Equations with Latent Variables. Wiley  New York  1989.
[4] M. Carandini  D. J. Heeger  and J. A. Movshon. Linearity and normalization in simple cells of the

macaque primary visual cortex. Journal of Neuroscience  17:8621–8644  1997.

[5] A. Cichocki and P. Georgiev. Blind source separation algorithms with matrix constraints. IEICE Trans.

Fundamentals  E86-A(3):522–531  2003.

8

[6] P. Garrigues and B. A. Olshausen. Learning horizontal connections in a sparse coding model of natural

images. In Advances in Neural Information Processing Systems  volume 20  pages 505–512  2008.

[7] D. J. Heeger. Normalization of cell responses in cat striate cortex. Visual Neuroscience  9:181–197  1992.
[8] P. O. Hoyer  D. Janzing  J. Mooij  J. Peters  and B. Sch¨olkopf. Nonlinear causal discovery with additive
noise models. In Advances in Neural Information Processing Systems  volume 21  pages 689–696  2009.
[9] A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE Trans-

actions on Neural Networks  10(3):626–634  1999.

[10] A. Hyv¨arinen and P.O. Hoyer. Emergence of phase and shift invariant features by decomposition of

natural images into independent feature subspaces. Neural Comput.  12(7):1705–1720  2000.

[11] A. Hyv¨arinen  P.O. Hoyer  and M. Inki. Topographic independent component analysis. Neural Comput. 

13(7):1527–1558  2001.

[12] A Hyv¨arinen  J. Hurri  and P. O. Hoyer. Natural Image Statistics – A probabilistic approach to early

computational vision. Springer-Verlag  2009.

[13] A. Hyv¨arinen  J. Karhunen  and E. Oja. Independent Component Analysis. John Wiley & Sons  2001.
[14] Y. Karklin and M. S. Lewicki. A hierarchical Bayesian model for learning nonlinear statistical regularities

in nonstationary natural signals. Neural Comput.  17:397–423  2005.

[15] Y. Karklin and M. S. Lewicki. Emergence of complex cell properties by learning to generalize in natural

scenes. Nature  457:83–86  January 2009.

[16] M. Kawanabe and K.-R. M¨uller. Estimating functions for blind separation when sources have variance

dependencies. Journal of Machine Learning Research  6:453–482  2005.

[17] U. K¨oster and A. Hyv¨arinen. A two-layer model of natural stimuli estimated with score matching. Neural

Comput.  22:2308–2333  2010.

[18] G. Lacerda  P. Spirtes  J. Ramsey  and P. Hoyer. Discovering cyclic causal models by independent com-
ponents analysis. In Proceedings of the Twenty-Fourth Conference Annual Conference on Uncertainty in
Artiﬁcial Intelligence (UAI’08)  pages 366–374  2008.

[19] S. Lyu. Divisive normalization: Justiﬁcation and effectiveness as efﬁcient coding transform. In Advances

in Neural Information Processing Systems 23  pages 1522–1530  2010.

[20] J. Malo  I. Epifanio  R. Navarro  and E. P. Simoncelli. Nonlinear image representation for efﬁcient per-

ceptual coding. IEEE Trans Image Process  15(1):68–80  2006.

[21] J. Malo and V. Laparra. Psychophysically tuned divisive normalization approximately factorizes the PDF

of natural images. Neural Comput.  22(12):3179–3206  2010.

[22] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381:607–609  1996.

[23] S. Osindero  M. Welling  and G. E. Hinton. Topographic product models applied to natural scene statistics.

Neural Comput.  18:381–414  2006.

[24] J. Pearl. On the statistical interpretation of structural equations. Technical Report R-200  UCLA Cognitive

Systems Laboratory  1993.

[25] P. Ramkumar  L. Parkkonen  R. Hari  and A. Hyv¨arinen. Characterization of neuromagnetic brain rhythms
over time scales of minutes using spatial independent component analysis. Human Brain Mapping  2011.
In press.

[26] O. Schwartz and E. P. Simoncelli. Natural signal statistics and sensory gain control. Nature Neuroscience 

4(8)  2001.

[27] S. Shimizu  P.O. Hoyer  A. Hyv¨arinen  and A. Kerminen. A linear non-Gaussian acyclic model for causal

discovery. Journal of Machine Learning Research  7:2003–2030  2006.

[28] E. P. Simoncelli and B. A. Olshausen. Natural image statistics and neural representation. Annu. Rev.

Neurosci.  24:1193–1216  2001.

[29] R. Valerio and R. Navarro. Optimal coding through divisive normalization models of V1 neurons. Net-

work: Computation in Neural Systems  14:579–593  2003.

[30] H. Valpola  M. Harva  and J. Karhunen. Hierarchical models of variance sources. Signal Processing 

84(2):267–282  2004.

[31] J. H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with

simple cells in primary visual cortex. Proc. R. Soc. Lond. B  265(359–366)  1998.

[32] M. J. Wainwright and E. P. Simoncelli. Scale mixtures of gaussians and the statistics of natural images.

In Advances in Neural Information Processing Systems  volume 12  pages 855–861  2000.

[33] K. Zhang and A. Hyv¨arinen. Source separation and higher-order causal analysis of MEG and EEG. In

Proceedings of the Twenty-Sixth Conference (UAI 2010)  pages 709–716  2010.

9

,Aron Yu
Kristen Grauman
Matthew Chalk
Olivier Marre
Gasper Tkacik