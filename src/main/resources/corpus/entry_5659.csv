2019,Transfer Anomaly Detection by Inferring Latent Domain Representations,We propose a method to improve the anomaly detection performance on target
domains by transferring knowledge on related domains. Although anomaly labels
are valuable to learn anomaly detectors  they are difficult to obtain due to their rarity.
To alleviate this problem  existing methods use anomalous and normal instances
in the related domains as well as target normal instances. These methods require
training on each target domain. However  this requirement can be problematic
in some situations due to the high computational cost of training. The proposed
method can infer the anomaly detectors for target domains without re-training by
introducing the concept of latent domain vectors  which are latent representations
of the domains and are used for inferring the anomaly detectors. The latent
domain vector for each domain is inferred from the set of normal instances in the
domain. The anomaly score function for each domain is modeled on the basis of
autoencoders  and its domain-specific property is controlled by the latent domain
vector. The anomaly score function for each domain is trained so that the scores of
normal instances become low and the scores of anomalies become higher than those
of the normal instances  while considering the uncertainty of the latent domain
vectors. When target normal instances can be used during training  the proposed
method can also use them for training in a unified framework. The effectiveness
of the proposed method is demonstrated through experiments using one synthetic
and four real-world datasets. Especially  the proposed method without re-training
outperforms existing methods with target specific training.,Transfer Anomaly Detection by

Inferring Latent Domain Representations

Atsutoshi Kumagai

NTT Software Innovation Center
NTT Secure Platform Laboratories
atsutoshi.kumagai.ht@hco.ntt.co.jp

Tomoharu Iwata

NTT Communication Science Laboratories

tomoharu.iwata.gy@hco.ntt.co.jp

Yasuhiro Fujiwara

NTT Communication Science Laboratories

yasuhiro.fujiwara.kh@hco.ntt.co.jp

Abstract

We propose a method to improve the anomaly detection performance on target
domains by transferring knowledge on related domains. Although anomaly labels
are valuable to learn anomaly detectors  they are difﬁcult to obtain due to their rarity.
To alleviate this problem  existing methods use anomalous and normal instances
in the related domains as well as target normal instances. These methods require
training on each target domain. However  this requirement can be problematic
in some situations due to the high computational cost of training. The proposed
method can infer the anomaly detectors for target domains without re-training by
introducing the concept of latent domain vectors  which are latent representations
of the domains and are used for inferring the anomaly detectors. The latent
domain vector for each domain is inferred from the set of normal instances in the
domain. The anomaly score function for each domain is modeled on the basis of
autoencoders  and its domain-speciﬁc property is controlled by the latent domain
vector. The anomaly score function for each domain is trained so that the scores of
normal instances become low and the scores of anomalies become higher than those
of the normal instances  while considering the uncertainty of the latent domain
vectors. When target normal instances can be used during training  the proposed
method can also use them for training in a uniﬁed framework. The effectiveness
of the proposed method is demonstrated through experiments using one synthetic
and four real-world datasets. Especially  the proposed method without re-training
outperforms existing methods with target speciﬁc training.

1

Introduction

Anomaly detection is an important task in artiﬁcial intelligence [8  6]. The goal of anomaly detection
is to detect anomalous instances  called anomalies or outliers  that do not conform to the expected
normal pattern. Anomaly detection methods have been used in a wide variety of applications such as
intrusion detection [16]  fraud detection [33]  medical care [30]  and industrial asset monitoring [28].
Many semi-supervised anomaly detection methods have been proposed such as autoencoders (AEs)
[43]  one-class support vector machines (OSVMs) [44]  and isolation forests [35]. Since they require
only normal instances  which are relatively easy to prepare  to obtain anomaly detectors  they are
particularly used in practice. In some situations  anomaly labels that indicate anomalous instances
can be used. By using anomaly labels  supervised anomaly detection methods can detect anomalies

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

much better than semi-supervised ones [29  50  23  10]. Although anomaly labels are valuable  they
are typically difﬁcult to obtain since anomalies rarely occur.
Even if anomaly labels are difﬁcult to obtain in a domain of interest  called a target domain  they may
be obtainable in related domains  called source domains. For example  in cyber-security  security
companies monitor customers’ networks to prevent cyber-attacks [14]. Although anomalies are
difﬁcult to obtain from a new customer’s network (target domain)  they may be obtained from existing
customers’ networks (source domains) that have long been monitored. Similarly  in condition-based
monitoring of industrial assets such as coal mining drilling machines using sensor data [27]  although
anomalies are difﬁcult to obtain from a new asset  anomalies may be obtained from related assets that
have long been working.
Several transfer anomaly detection methods have been proposed to learn the anomaly detector by
using both anomalous and normal instances in the source domains [3  7  28  17  48]. These methods
also use target normal instances for training. However  training after obtaining target instances can be
problematic in some applications. For example  consider anomaly detection on Internet-of-Things
(IoT) devices such as sensors  cameras  and cars. Since each device does not have sufﬁcient computa-
tional resources  training is difﬁcult to perform on these devices even if target domains appear that
contain normal instances. As another example  in cyber-security  widely various IoT devices them-
selves need to be protected from cyber attacks [4]. However  it is difﬁcult to protect all devices quickly
with time-consuming training since many new devices (target domains) appear one after another.

In this paper  we propose a novel method to im-
prove the anomaly detection performance on target
domains by using anomalous and normal instances 
or only normal instances in source domains. The
proposed method can infer the anomaly detectors
for any domains given the sets of normal instances
in the domains without re-training. In addition  the
proposed method can use target normal instances for
training in a uniﬁed framework when they can be
used during training. With the proposed method  an
anomaly score function  which outputs an anomaly
score given an instance  is deﬁned on the basis of the
AE  which is widely used in recent anomaly detection
methods [43  56  12  1  50]. Note that the proposed
method can use other semi-supervised anomaly de-
tection methods with learnable parameters instead of
AEs such as variational AEs [32] and energy-based
models [55]. The parameters of the AE are shared
across all domains to learn the common property of
domains. However  each domain has a speciﬁc prop-
erty that cannot be explained only by the common
property. To reﬂect these domain-speciﬁc properties
to the anomaly score function efﬁciently  we intro-
duce latent domain vectors  which are latent representations of domains. The latent domain vectors
are used to condition the anomaly score function and control the property of the anomaly score
function for each domain.
The latent domain vector of each domain is estimated from the set of normal instances in the domain
by a neural network. This model enables us to infer the anomaly detectors for any domains given the
sets of normal instances in the domains without re-training or anomalies. To infer the latent domain
vectors of different domains  the neural networks need to take sets with different sizes of normal
instances as inputs. We realize this by using the deep sets [54]  which are permutation invariant
neural networks to the order of instances in the sets. Speciﬁcally  we model the parameters for the
posterior of the latent domain vector with the deep sets.
The anomaly score function for each domain is trained so that the anomaly scores of normal instances
become low  which is achieved by minimizing the reconstruction error of the normal instances.
Anomaly labels are used so that the anomaly scores of anomalous instances become higher than those

Figure 1: The latent domain vector zd is in-
ferred from the set of normal instances in
the domain (blue dotted line) so as to detect
anomalies. The decision boundary for each
domain (red line) is induced by the domain-
speciﬁc anomaly score function  which is de-
termined by zd. The green dotted line de-
notes the decision boundary of AE  which
uses only target normal instances. The pro-
posed method can detect test anomalies that
AE cannot via data on the related domains.

2

……Source domain 1Source domain 2Latent spaceSource domain 1Source domain 2Target domain DS+DTz1: normal: anomalous: anomalous (test)Targetdomain DS+DTz2zDs+DTof normal instances  which is realized by adding a differentiable area under the curve (AUC) loss
as the regularizer. This regularizer enables us to improve the performance even if the number of
anomalies is small [29]. Since the latent domain vectors are estimated from data  the estimation is
often uncertain. To handle this uncertainty appropriately  we take the expectation of the loss function
w.r.t. the latent domain vector and regularize the posterior of the latent domain vector by a prior
using Kullback Leibler (KL) divergence  which is used as our objective function for the domain.
The parameters for the anomaly score function and the posteriors of the latent domain vectors are
estimated simultaneously by minimizing the sum of the objective function for each domain. Figure 1
illustrates the proposed method.

2 Related Work

Anomaly detection  which is also called outlier detection or novelty detection  has been widely
studied [8  6]. Although many unsupervised or semi-supervised anomaly detection methods have
been proposed such as AE based methods [43  56  12  1]  OSVM based methods [44  7  40]  and
density based methods [57  51  55]  they cannot use anomaly labels. Supervised anomaly detection
uses anomaly labels to improve the performance  which is also referred to as imbalanced classiﬁcation
[9  21]. They assume that all instances are obtained from one domain and thus cannot perform well
when there is a domain difference which is focused on in this paper.
Transfer learning or domain adaptation aims to solve a problem in a target domain using data in
source domains [39]. Unsupervised approaches aim to adapt to the target domain by using labeled
source data and target unlabeled data [26  37  5  42  36  34]. Semi-supervised approaches use labeled
target data as well as these data [38  13  41]. Although these methods performed impressively  they
usually do not assume the class-imbalance and thus are not appropriate for anomaly detection [47].
Although several transfer learning methods are designed to deal with the class-imbalance [2  53  18] 
they assume anomalous and normal instances in the target domains for training. The proposed method
does not assume target anomalous instances  which is more practical since anomalies rarely occur.
Some transfer anomaly detection methods do not use target anomalies to obtain anomaly detectors
as with the proposed method. The two-step approach is widely used in transfer anomaly detection
[3  7]. This approach ﬁrst extracts discriminative features from data in the source domain with neural
networks. After feature extraction  any semi-supervised algorithms like OSVM are applied to the
target normal instances. Although this approach is effective to some extent  dividing anomaly detector
and feature (transfer) learning risks losing information to be transferred [7  40]. In contrast  the
proposed method learns them simultaneously in an end-to-end learning manner. A few methods
use only normal instances in both the source and target domains [28  17  48]. One method uses
an auxiliary dataset of outliers [22]. Another assumes target unlabeled data for training [47]. All
these methods require training on each target domain  which can be problematic in some situations
as described in Section 1. The proposed method can instantly infer the anomaly detector for each
domain given the set of normal instances in the domain without re-training.
One-class data transfer learning (OTL) [11] can predict classiﬁers for new domains without re-training
although this ability is not mentioned by Chen and Liu [11]. OTL trains a regression model that
predicts parameters of anomaly density from those of a normal density using labeled source data.
OTL predicts a target classiﬁer by predicting densities given target normal instances. Although OTL
requires the anomaly density to be estimated for each source domain  this is quite difﬁcult since the
number of anomalies is small in anomaly detection tasks. In addition  OTL cannot use domains that
contain only normal instances for training although the proposed method can.

3 Preliminary

AEs are neural networks originally proposed for non-linear dimensionality reduction [25]. Due
to their simplicity and effectiveness  AEs have become a fundamental component of recent semi-
(cid:80)N
supervised anomaly detection [6  43  56  12  1]. Thus  we use AEs as a building block of the proposed
method. Given instances X := {x1  . . .   xN}  the AE is trained by minimizing the following loss
n=1 (cid:107)xn − GθG (FθF (xn))(cid:107)2  where FθF is a neural network with the
function  L(θF   θG) := 1
N
parameter θF   called the encoder; GθG is a neural network with the parameter θG  called the decoder;
(cid:107) · (cid:107) is a Euclidean norm; and (cid:107)x − GθG (FθF (x))(cid:107)2 is a reconstruction error of x. When the AE is

3

trained with normal instances  the reconstruction errors of normal instances become low. In contrast 
the reconstruction errors of instances dissimilar to normal instances  i.e.  anomalies  can be expected
to become high since they are not learned. Thus  AEs can be used for anomaly detection with the
reconstruction error as the anomaly score.

4 Proposed Method

4.1 Task

d

dn}N +

d := {x+

n=1 be a set of anomalous instances in the d-th domain  where x+
dn}N

dn ∈ RM is the
Let X+
M-dimensional feature vector of the n-th anomalous instance of the d-th domain and N +
d is the
number of the anomalous instances in the d-th domain. Similarly  let X−
n=1 be a set of
normal instances in the d-th domain. We assume that N +
d in each domain since anomalies
rarely occur  and the feature vector size M is the same in all domains as in many existing studies
[38  20  28  42]. Suppose that we have both anomalous and normal instances in DS source domains 
{X+
d=DS+1. Note that the proposed
method can also treat source domains that have only normal instances although we assumed that all
source domains have anomalies for simplicity. Our goal is to obtain an appropriate domain-speciﬁc
anomaly score function  which outputs its anomaly score given an instance  for each target domain.

d=1  and normal instances in DT target domains  {X−

d := {x−

d }DS+DT

d (cid:28) N−

d ∪ X−

d }DS

−
d

4.2 Domain-speciﬁc Anomaly Score Function

We deﬁne the domain-speciﬁc anomaly score function based on the reconstruction error of the AE. To
represent the property of each domain efﬁciently  we assume that each domain has a K-dimensional
latent continuous variable zd ∈ RK  which is called a latent domain vector in this paper. For the d-th
domain  we deﬁne the anomaly score function conditioned on the latent domain vector zd as follows 
(1)
where the parameters θ := (θF   θG) are shared among all domains. Unlike the original reconstruction
error  this score function depends on the latent domain vector zd. By changing the value of zd  the
proposed method can ﬂexibly control the property of the anomaly score function. Although we use
the reconstruction error for simplicity  we can use other anomaly score functions with learnable
parameters such as autoregressive density models [19  46] and ﬂow-based density models [15  31].

sθ(xdn|zd) := (cid:107)xdn − GθG (FθF (xdn  zd))(cid:107)2 

4.3 Models for Latent Domain Vectors

Since the latent domain vectors are unobserved  we have to estimate them from data. For a ﬁrst step 
we model the conditional probability of the latent domain vector given the set of normal instances as
a multivariate Gaussian distribution with a diagonal covariance matrix:
φ(X−

d ) := N (zd|µφ(X−

qφ(zd|X−

d )  diag(σ2

d ))) 

(2)

φ(X−

d ) ∈ RK

d ) ∈ RK and variance σ2

where mean µφ(X−
+ are modeled by neural networks with
parameters φ that are shared among all domains  and diag(x) returns a diagonal matrix whose
diagonal elements are x. In this model  the latent domain vector zd depends on the set of normal
instances X−
d . By this modeling  we can infer the latent domain vectors of any domains when the sets
of normal instances in the domains are given. Accordingly  we can obtain domain-speciﬁc anomaly
score functions for the domains without re-training or anomalies.
Since the qφ deals with the set of normal instances X−
d as an input  the neural networks for the
d ) ∈ RK must be permutation invariant to the order of instances
parameters µφ(X−
in the set. To achieve this  we use the recently proposed deep sets architecture [54]  τ (X−
d ) =
d )  ρ and η are any
ρ
neural networks  respectively. This architecture is permutation invariant due to the summation.
Although this architecture is quite simple  it can express any permutation invariant function and
preserve all the properties of the set with suitable ρ and η [54]. Thus  we can capture the property of
each domain well with this architecture.

d ) represents one of the µφ(X−

  where τ (X−

(cid:16)(cid:80)N

n=1 η(x−
dn)

d ) and ln σ2

d ) and ln σ2

φ(X−

φ(X−

(cid:17)

−
d

4

4.4 Objective Function

We deﬁne the objective function of the proposed method using the domain-speciﬁc anomaly score
functions and latent domain vectors. First  the objective function for the d-th source domain condi-
tioned on the latent domain vector zd to be minimized is deﬁned by

Ld(θ|zd) :=

1
N−

d

sθ(x−

dn|zd) −

λ
N−
d N +

d

f (sθ(x+

dm|zd) − sθ(x−

dn|zd)) 

(3)

d(cid:88)

−

N

n=1

d(cid:88)

−
d  N +

N

n m=1

1

where λ ≥ 0 is the hyperparameter and f is the sigmoid function  f (x) =
1+exp(−x). This form of
the objective function has recently been proposed by Iwata and Yamanaka [29] and showed better
performance than existing methods although they do not consider domain differences. The ﬁrst
term of Eq. (3) represents the anomaly scores of normal instances in the d-th domain. Since the
anomaly scores of normal instances should be low  we minimize this term. The second term of Eq.
(3) is a differentiable approximation of the AUC [52]  which is effective for class-imbalanced data
[23]. The anomaly scores of anomalous instances should be higher than those of normal instances 
d   x−
sθ(x+
d . The AUC term encourages this since
the f (·) takes the maximal value one when sθ(x+
dn|zd) and the minimal value zero
dn|zd). When there are no anomalies or λ = 0  the second term of Eq.
when sθ(x+
(3) becomes zero and the ﬁrst term of Eq. (3) remains. Thus  it is a supervised extension of the AE
described in Section 3.
Since the latent domain vector zd has uncertainty with the variance σ2
φ  we want to appropriately take
this into account for the objective function. To achieve this  we deﬁne the objective function for the
d-th source domain to be minimized as follows

dn ∈ X−
dm|zd) (cid:29) sθ(x−

dn|zd) for any x+

dm|zd) (cid:28) sθ(x−

dm|zd) > sθ(x−

dm ∈ X+

(4)

d )||p(zd)) 

Ld(θ  φ) := E

qφ(zd|X

d ) [Ld(θ|zd)] + βDKL(qφ(zd|X−
d )||p(zd)) is the KL divergence between qφ(zd|X−

−

d = µφ(X−

where DKL(qφ(zd|X−
d ) and a standard Gaussian
distribution p(zd) := N (0  I)  and β > 0 is a hyperparameter. The ﬁrst term of Eq. (4) is the
expectation of the objective function (3) w.r.t. the qφ(zd|X−
(cid:80)L
d ). Since the expectation considers all the
probabilities of the zd  it can lead to robust training. This expectation term can be effectively approx-
(cid:96)=1 Ld(θ|z((cid:96))
imated by the reparametrization trick [32]. That is  E
d ) 
d ∼ N (0  I)  and (cid:12) is an element-wise product. The
where z((cid:96))
second term of Eq. (4) is a regularization term that prevents over-ﬁtting of the latent domain vectors 
where its strength is controlled by β. This type of regularization is common in variational AEs
[32  24] and can be analytically calculated [32]. The qφ(zd|X−
d ) is trained so as to minimize the
loss term (the ﬁrst term of Eq. (4)) while being constrained to the prior p(zd). The effectiveness of
considering the uncertainty will be demonstrated in our experiments.
The objective function for the d-th target domain to be minimized is obtained by omitting the AUC
loss term from Eq. (4) since the target domain does not have anomalous instances. That is 

d ) [Ld(θ|zd)] ≈ 1

d (cid:12) σφ(X−

d ) + ((cid:96))

d )  ((cid:96))

qφ(zd|X

−

L

 + βDKL(qφ(zd|X−

sθ(x−

dn|zd)

d )||p(zd)) 

(5)

Ld(θ  φ) := E

 1
each domain  L(θ  φ) :=(cid:80)DS+DT

qφ(zd|X

−
d )

N−

d

d(cid:88)

−

N

n=1

where the ﬁrst term can also be approximated using the reparametrization trick. As a result  the
objective function for the proposed method is the following weighted sum of the objective function for
αdLd(θ  φ)  where αd ≥ 0 is the hyperparameter. This objective
function can be minimized w.r.t. θ and φ by gradient-based optimization methods. This formulation
includes various settings. For example  when no target instances can be used in the training phase 
we set αd = 0 for d = DS + 1  . . .   DS + DT and αd = 1 for d = 1  . . .   DS. The proposed method
can infer the anomaly score functions for the domains that are not used for training when the sets of
normal instances in the domains are given without re-training as described below.

d=1

5

Inference

4.5
By using the learned parameters (θ∗  φ∗) and the normal instances X−
the domain-speciﬁc anomaly score function for the d(cid:48)-th domain as follows:

(cid:90)

s(xd(cid:48)) :=

sθ∗ (xd(cid:48)|zd(cid:48))qφ∗ (zd(cid:48)|X−

d(cid:48))dzd(cid:48) ≈ 1
L

d(cid:48)  the proposed method infers

sθ∗ (xd(cid:48)|z((cid:96))
d(cid:48) )

(6)

L(cid:88)

(cid:96)=1

d(cid:48) = µφ∗ (X−

d(cid:48)) + ((cid:96))(cid:12)σφ∗ (X−

d(cid:48))  ((cid:96)) ∼ N (0  I)  and xd(cid:48) is any instance in the d(cid:48)-th domain.
where z((cid:96))
The proposed method can infer the s(·) considering the uncertainty of the latent domain vectors by
sampling zd(cid:48) from the qφ∗ (zd(cid:48)|X−
d(cid:48))  which enables robust anomaly detection. This can be inferred
even if X−
d(cid:48) + L).

d(cid:48) are not used for training. The computational complexity for the inference is O(N−

5 Experiments

We demonstrate the effectiveness of the proposed method using one synthetic and four real-world
class-imbalanced datasets. To measure anomaly detection ability on target domains  we evaluated the
AUC  which is a well used measure for anomaly detection tasks  on one domain while training the
rest. We used the following setup: CPU was Intel Xeon E5-2660v3 2.6 GHz  the memory size was
128 GB  and GPU was NVIDIA Tesla k80.

5.1 Data

We created the simple two-dimensional dataset shown in Figure 2a. This dataset consists of eight
double circles (domains) around the (0  0). Each double circle has outer and inner circles that consist
of normal and anomalous instances  respectively. We used the ‘7’-th domain as the target domain
and the rest as the source domains. We used four real-world public datasets: MNIST-r  Anuran
Calls  Landmine  and IoT. The MNIST-r is derived from the MNIST by rotating images  which
was introduced by Ghifary et al. [20]. The MNIST-r has six domains. We selected the ‘4’ digit as
the anomalous class and the rest as the normal class since it was the most difﬁcult setting in our
preliminarily experiment. The Anuran Calls is a real-world dataset collected from frog croaking
sounds  which is used in the multi-task anomaly detection study [28]. We regarded each specie as a
domain referring to the study [28]  and thus  the Anuran Calls has ﬁve domains. The Landmine is a
real-world dataset that is well-used in multi-task learning [49]. We used ten domains that consist of
the ﬁrst ﬁve (1-5) and last ﬁve (25-29) domains. The IoT contains real network trafﬁc data  which are
gathered from nine IoT devices infected by BASHLITE malware. We did not use the device that had
d + N−
no normal data  and thus  the IoT has eight domains. The average anomaly rate N +
d )
of Synthetic  MNIST-r  Anuran Calls  Landmine  and IoT are 0.048  0.1  0.024  0.062  and 0.05 
respectively. Due to the length limit of the paper  the details of the datasets including download links
are provided in the supplemental material.

d /(N +

5.2 Comparison Methods

We evaluated two variants of the proposed method: ProT and ProS. ProT uses normal instances in
the target domain as well as anomalous and normal source instances for training. ProS does not use
target normal instances for training. After training with source domains  ProS infers the anomaly
score function for the target domain using the set of target normal instances without re-training. The
proposed method was implemented by Chainer [45].
We compared the proposed method with eight methods: the feed-forward neural network classiﬁer
(NN)  the NN for class-imbalanced data (NNAUC)  the autoencoder based classiﬁer for class-
imbalanced data (AEAUC) [29]  the autoencoder (AE) [43]  the one-class support vector machine
(OSVM) [44]  the contrastive semantic alignment (CCSA) [38]  the transfer one-class support vector
machine (TOSVM) [3]  and the one-class data transfer learning (OTL) [11]. AE and OSVM are
semi-supervised anomaly detection methods  which use only normal instances in the target domain
for training. NN  NNAUC  and AEAUC are supervised anomaly detection methods  which use both
anomalous and normal instances in the source domains for training. AEAUC is obtained from ProS
by omitting the latent domain vectors. CCSA  TOSVM  and OTL are transfer learning or transfer
anomaly detection methods  which use both anomalous and normal instances in the source domains

6

(a) Synthetic Data

(b) Latent Domain Vectors

(c) Anomaly Score

(d) AUC

Figure 2: (a) Synthetic dataset consists of eight double circles (domains). Anomalous and normal
source instances are represented by orange and blue points  respectively. Anomalous and normal
target instances are represented by red and green points  respectively. (b) Posteriors of the latent
domain vectors estimated by ProS. Orange points denote the mean of each posterior. (c) Heatmap of
anomaly scores on the target domain obtained by ProS. Darker color indicates higher anomaly score.
(d) Average and standard error of AUCs when λ was changed.

Table 1: Average and standard deviation of AUCs [%] on each target domain on MNIST-r.

Target
0
15
30
45
60
75
Avg

ProT

93.9(0.7)
99.2(0.3)
98.8(0.4)
97.1(0.8)
99.2(0.4)
91.2(2.0)
96.6(3.2)

ProS

92.5(1.2)
99.2(0.3)
98.8(0.4)
95.3(0.8)
99.2(0.2)
91.1(2.9)
96.0(3.6)

NN

88.7(1.2)
98.2(0.4)
98.2(0.3)
96.4(0.8)
98.9(0.5)
88.7(2.2)
94.8(4.6)

NNAUC AEAUC
87.2(1.3)
88.5(2.8)
98.7(0.5)
97.7(0.3)
98.1(0.6)
97.6(0.4)
94.6(1.6)
94.7(0.9)
98.2(0.9)
98.7(0.3)
87.7(2.3)
88.5(1.4)
94.1(4.7)
94.3(4.9)

AE

72.6(2.5)
72.6(2.5)
71.8(3.9)
73.6(3.6)
72.8(2.3)
72.9(2.9)
72.7(3.0)

OSVM
56.7(1.2)
63.9(1.2)
63.5(1.3)
63.9(0.9)
63.6(1.0)
71.4(2.2)
63.8(4.5)

CCSA
87.3(1.3)
97.7(0.4)
97.5(0.4)
96.5(1.0)
98.2(0.5)
92.3(0.8)
94.9(4.1)

TOSVM
89.4(4.8)
95.5(1.8)
94.6(2.4)
90.6(4.8)
96.2(1.8)
93.5(2.8)
93.3(4.1)

OTL

86.5(1.3)
95.2(0.8)
95.3(1.0)
91.9(1.4)
94.8(1.4)
78.4(3.0)
90.3(6.4)

and normal instances in the target domain to obtain the anomaly detector. Although CCSA and
TOSVM use target normal instances for training like ProT  OTL does not like ProS.
We selected hyperparameters using average validation AUC on the source domains for all methods
except for AE. AE used the validation reconstruction error on the target domains. We evaluated the
test AUC when the method obtained the best validation AUC after 15 epochs to avoid over-ﬁtting.
We conducted experiments on ten different datasets for each target domain and reported the mean test
AUC. The details of the experimental setup such as network architectures for the proposed method
and comparison methods and hyperparameter candidates are described in the supplemental material.

5.3 Results

d ) are inferred from an input X−
d ) are continuous w.r.t. the input X−

First  we show how the proposed method works by using the synthetic dataset. Each domain of
the synthetic dataset is located on a circle as shown Figure 2a. Figure 2b represents an example
of the posterior of the latent domain vectors estimated by ProS with K = 2. We found that the
estimated latent domain vectors were located on a circle to preserve the relationship between the
domains. Especially  the posterior of the target domain (‘7’-th domain) was predicted between the
posteriors of the ‘0’-th and ‘6’-th domains even if the target domain was not used for training (i.e. 
the objective function). The parameters of qφ(zd|X−
d so as to ﬁnd
anomalies. Since the parameters of qφ(zd|X−
d   the posteriors
of zd’s become similar if the X−
d ’s are similar. Thus  similar domains (e.g.  the ‘3’-th and ‘4’-th
domains) were located near each other in the latent space. Figure 2c represents an example of the
heatmap of anomaly scores on the target domain obtained by ProS. We found that ProS was able to
give high (low) anomaly scores to the anomalous (normal) region on the target domain  i.e.  AUC is
one even if the target domain was not used for training. Figure 2d shows average and standard error
of AUCs with different λ on the target domain. We used AEAUC as a baseline since it is obtained
from ProS by omitting the latent domain vectors. ProT and ProS outperformed the others by large
margins when the values of λ were relatively large. This result indicates the importance of both
anomalies and modeling domain differences. Overall  these results demonstrate that the proposed
method can capture the property of domains as the latent domain vectors and perform well.
Second  we evaluated anomaly detection performance on the target domain using the real-world
datasets. Tables 1 – 4 show the average and standard deviation of AUCs with different target domains

7

00.10.20.30.40.50.60.70.80.910.1110100100010000AUCProTProSAEAUCAETable 2: Average and standard deviation of AUCs [%] on each target domain on Anuran Calls.

Target
Ade
Ame
Hyl
HCi
HCo
Avg

ProT

99.9(0.1)
99.4(1.3)
99.7(0.3)
99.9(0.2)
99.9(0.1)
99.8(0.6)

ProS

95.5(4.8)
93.5(3.6)
98.6(2.0)
98.9(1.6)
97.6(3.5)
96.8(3.8)

NN

84.8(19.)
87.1(4.5)
99.6(0.9)
98.7(2.1)
97.1(4.7)
93.4(11.)

NNAUC AEAUC
95.6(4.0)
64.0(17.)
94.1(5.5)
85.7(5.2)
98.9(2.0)
99.4(1.0)
99.4(0.8)
98.2(3.1)
96.2(3.6)
96.5(3.4)
96.9(4.0)
88.7(16.)

AE

84.5(12.)
81.6(8.1)
93.8(3.7)
89.5(1.2)
94.2(6.3)
88.7(8.5)

OSVM
96.6(0.9)
89.1(3.0)
89.9(1.7)
93.6(0.6)
92.7(1.7)
92.4(3.2)

CCSA
95.2(4.4)
96.2(2.6)
96.1(6.5)
96.7(2.8)
99.5(1.2)
96.7(4.1)

TOSVM
77.9(20.)
93.6(5.7)
99.6(0.4)
99.3(1.3)
97.4(1.7)
93.6(12.)

OTL

90.9(4.7)
93.8(2.9)
95.4(6.3)
90.4(4.5)
97.2(2.4)
93.5(4.9)

Table 3: Average and standard deviation of AUCs [%] on each target domain on Landmine.

Target
1
2
3
4
5
25
26
27
28
29
Avg

ProT

83.9(2.5)
78.4(2.2)
80.8(1.7)
84.4(2.3)
84.1(2.8)
61.8(2.4)
63.6(1.7)
60.2(2.4)
71.6(4.7)
55.5(3.1)
72.4(11.)

ProS

83.8(2.0)
78.5(2.3)
80.9(1.7)
84.1(1.7)
84.3(2.9)
62.8(3.6)
62.4(2.7)
60.9(5.2)
70.8(3.9)
55.3(3.4)
72.4(11.)

NN

77.8(2.0)
73.8(2.0)
79.4(1.3)
78.2(3.0)
77.2(2.9)
55.6(4.4)
63.6(2.0)
59.7(3.2)
63.7(5.1)
54.9(2.4)
68.4(9.9)

NNAUC AEAUC
78.8(3.3)
80.9(2.8)
72.6(3.0)
76.7(2.0)
80.5(1.2)
79.3(2.2)
78.0(4.5)
80.0(2.2)
81.4(2.5)
77.6(4.9)
60.5(6.0)
54.2(3.0)
62.6(1.7)
61.9(2.3)
59.5(3.1)
59.3(2.7)
66.5(5.7)
67.4(3.7)
56.7(2.5)
55.9(1.9)
69.9(11.)
69.1(9.4)

AE

50.0(5.7)
61.6(3.8)
58.6(3.8)
47.0(7.6)
51.4(7.6)
67.3(1.9)
48.3(3.2)
57.4(6.0)
67.5(3.1)
50.6(5.3)
56.0(8.8)

OSVM
38.0(7.2)
45.9(5.7)
49.2(9.7)
64.0(13.)
55.6(11.)
56.7(1.0)
63.0(2.4)
68.3(2.6)
72.5(1.8)
59.6(2.9)
57.3(12.)

CCSA
78.2(3.4)
72.9(2.0)
76.1(2.6)
79.7(2.9)
74.5(7.9)
51.1(1.4)
63.7(1.4)
64.9(1.9)
66.3(3.2)
57.9(2.2)
68.5(9.5)

TOSVM
55.9(6.2)
61.6(5.2)
60.2(4.9)
49.5(12.)
44.3(6.4)
57.9(2.2)
61.9(4.2)
61.1(2.3)
72.1(3.3)
55.8(4.2)
58.0(9.0)

OTL

61.0(6.2)
64.5(2.2)
61.0(2.9)
59.2(3.3)
55.5(2.3)
60.2(2.4)
59.9(3.2)
62.2(2.4)
69.0(2.3)
57.5(1.9)
61.0(4.6)

on all datasets. In Tables 1 – 6  the boldface denotes the best and comparable methods according
to the paired t-test at the signiﬁcance level of 5%. ProT showed the best/comparable AUCs in
almost all target domains (25 of 29 cases) and ProS also showed the best/comparable AUCs in
many target domains (21 of 29 cases). Supervised anomaly detection methods (NN  NNAUC  and
AEAUC) tended to perform better than semi-supervised methods (AE and OSVM) for all datasets 
which suggests the effectiveness of using anomaly labels on the related domains. ProT and ProS
outperformed these supervised methods. Moreover  ProT and ProS outperformed the transfer learning
methods (CCSA  TOSVM  and OTL) by modeling the domain difference via the latent domain
vectors. As for ProT and ProS  ProT showed better results than ProS in MNST-r and Anuran Calls
since ProT uses target normal instances for training. Although ProS does not use target normal
instances for training  it performed almost the same as ProT in Landmine and IoT  which indicates
the effectiveness of inferring domain-speciﬁc anomaly detectors without re-training. Overall  these
results showed that the proposed method detects anomalies superiorly on the target domains.
Third  we investigated the effect of considering the uncertainty of the latent domain vectors in the
proposed method. To asses this  we consider the deterministic variants of ProT and ProS  called
D-ProT and D-ProS  respectively. The objective function for both methods is obtained by replacing
Eq. (2) with the delta distribution qφ(zd|X−
d )) and omitting the KL divergence
terms in Eqs. (4) and (5). Note that these methods also do not exist in previous studies  and thus  we
can regard them as our proposal. Table 5 shows the average AUCs over all target domains of each
dataset. ProT and ProS performed better than D-ProT and D-ProS in all datasets. These results show
the effectiveness of considering the uncertainty in the proposed method.
Fourth  we investigated the effect of the number of anomalous training instances. Table 6 shows the
average AUCs over target domains when the anomaly rate of each source domain ra := N +
d +
N−
d ) was equally changed on MNIST-r. As expected  as the number of anomalous training instances
decreased  ProT  ProS  and AEAUC came to perform worse. However  even if the anomaly rate ra
is small  ProT and ProS showed better results than AE. In addition  ProT and ProS outperformed
AEAUC for all the values of the ra. This result suggests that the proposed method is relatively robust
against the anomaly ratio ra.
Last  we evaluated the computation time of the proposed method. We evaluated the training time
of 100 epochs for ProT  ProS  and AEAUC on MNIST-r. We set the hyperparameters as follows:
the regularization parameter of the AUC loss λ was 104  the dimension of the latent domain vector
K was 20  the regularization parameter of the latent domain vector β was one  and the sample size
of the reparametrization trick L was one. The computation time of 100 epoch training of ProT 
ProS  and AEAUC were 10.58  9.94 and 5.56 seconds  respectively. Although ProT and ProS took
more computation time than AEAUC due to the additional network for the latent domain vector 
the computation costs of ProT and ProS were not so large. Since ProT uses target normal instances

d ) = δ(zd − µφ(X−

d /(N +

8

Table 4: Average and standard deviation of AUCs [%] on each target domain on IoT.

ProT

Target
99.6(0.1)
Dbell
Therm 99.6(0.1)
99.6(0.2)
Ebell
94.3(1.3)
Baby
98.5(0.4)
737
99.1(0.2)
838
99.1(0.2)
Web
97.8(0.3)
1002
98.4(1.7)
Avg

ProS

99.6(0.1)
99.6(0.1)
99.6(0.1)
94.7(1.0)
98.3(0.6)
99.2(0.2)
99.0(0.2)
97.8(0.3)
98.5(1.6)

NN

99.4(0.4)
99.6(0.1)
99.5(0.1)
93.1(1.4)
97.9(0.6)
99.0(0.3)
99.2(0.1)
97.5(0.5)
98.2(2.1)

NNAUC AEAUC
99.5(0.1)
99.5(0.2)
99.5(0.2)
99.6(0.1)
99.6(0.2)
99.6(0.1)
90.4(2.7)
93.5(1.3)
98.1(1.0)
98.1(0.5)
99.1(0.2)
99.1(0.3)
99.1(0.2)
99.2(0.1)
97.6(0.6)
97.7(0.2)
98.3(2.1)
97.9(3.1)

AE

82.1(13.)
90.8(11.)
92.8(6.6)
46.8(3.3)
68.9(11.)
68.4(12.)
85.6(12.)
56.5(15.)
74.0(19.)

OSVM
99.1(0.2)
98.6(0.2)
97.4(0.3)
69.3(0.8)
89.0(1.9)
87.4(2.7)
97.7(0.3)
72.6(2.1)
88.9(11.)

CCSA
99.3(0.4)
99.6(0.1)
99.5(0.2)
94.1(1.4)
97.5(0.5)
99.1(0.3)
99.1(0.1)
97.7(0.2)
98.2(1.8)

TOSVM
99.5(0.2)
99.6(0.1)
98.4(2.0)
80.8(9.1)
97.4(2.0)
96.8(2.9)
98.8(0.6)
97.3(0.4)
96.1(6.8)

OTL

99.2(0.2)
97.8(0.4)
97.5(0.4)
92.3(2.3)
97.3(0.5)
97.9(0.4)
97.7(0.2)
94.8(2.8)
96.8(2.4)

Table 5: The effect of considering the uncertainty.
Average and standard deviation of AUCs [%] over
all target domains of each dataset.

Table 6: Average and standard deviation of AUCs
[%] over all target domains when changing the
anomaly rate ra on MNIST-r.

Data
MNIST-r
Anuran Calls
Landmine
IoT

ProT

96.6(3.2)
99.8(0.6)
72.4(11.)
98.4(1.7)

ProS

96.0(3.6)
96.8(3.8)
72.4(11.)
98.5(1.6)

D-ProT
95.9(4.0)
86.5(2.2)
71.1(11.)
98.1(2.6)

D-ProS
95.5(4.0)
84.8(2.2)
71.1(11.)
97.9(2.8)

ra
0.1
0.05
0.01
0.005

ProT

96.6(3.2)
94.5(5.0)
89.7(7.1)
87.0(8.4)

ProS

96.0(3.6)
94.1(5.6)
88.7(8.5)
85.0(9.8)

AEAUC
94.3(4.9)
91.3(7.3)
86.4(8.2)
81.9(10.)

AE

72.7(3.0)
72.7(3.0)
72.7(3.0)
72.7(3.0)

as well as source instances to learn the target-speciﬁc anomaly score function  ProT took a little
more computation time than ProS. ProS infer the target-speciﬁc anomaly score function using the
set of target normal instances without re-training. In this experiment  ProS inferred it with 0.0027
seconds when the sample size of the reparametrization trick L was ten. This inference time was 3918
times faster than the training time of ProT. Additional experimental results such as the λ  K  and β’s
dependency are described in the supplemental material.

6 Conclusion

In this paper  we proposed a method to improve the anomaly detection performance on target domains
by inferring their latent domain vectors. The proposed method can infer the anomaly detectors for
any domains given the sets of normal instances in the domains without re-training or anomalies. In
addition  the proposed method can also use target normal instances for training. The most attractive
point of the proposed method is that it can infer domain-speciﬁc anomaly detectors in two situations 
i.e.  target normal instances can or cannot be used for training  in a uniﬁed framework. In experiments
using one synthetic and four real-world datasets  the proposed method outperformed various existing
anomaly detection methods. For future work  we will apply sophisticated density models such as
autoregressive models and ﬂow-based models as the anomaly score function.

References
[1] S. Akcay  A. Atapour-Abarghouei  and T. P. Breckon. Ganomaly: Semi-supervised anomaly detection via

adversarial training. arXiv  2018.

[2] S. Al-Stouhi and C. K. Reddy. Transfer learning for class imbalance problems with inadequate data.

Knowledge and information systems  48(1):201–228  2016.

[3] J. T. Andrews  T. Tanay  E. J. Morton  and L. D. Grifﬁn. Transfer representation-learning for anomaly

detection. In Anomaly Detection Workshop in ICML  2016.

[4] S. Babar  P. Mahalle  A. Stango  N. Prasad  and R. Prasad. Proposed security model and threat taxonomy

for the internet of things (iot). In ICNSA  2010.

[5] K. Bousmalis  N. Silberman  D. Dohan  D. Erhan  and D. Krishnan. Unsupervised pixel-level domain

adaptation with generative adversarial networks. In CVPR  2017.

[6] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv  2019.

[7] R. Chalapathy  A. K. Menon  and S. Chawla. Anomaly detection using one-class neural networks. arXiv 

2018.

9

[8] V. Chandola  A. Banerjee  and V. Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR) 

41(3):15  2009.

[9] N. V. Chawla. Data mining for imbalanced datasets: An overview. In Data mining and knowledge discovery

handbook  pages 875–886. Springer  2009.

[10] N. V. Chawla  K. W. Bowyer  L. O. Hall  and W. P. Kegelmeyer. Smote: synthetic minority over-sampling

technique. Journal of artiﬁcial intelligence research  16:321–357  2002.

[11] J. Chen and X. Liu. Transfer learning with one-class data. Pattern Recognition Letters  37:32–40  2014.

[12] J. Chen  S. Sathe  C. Aggarwal  and D. Turaga. Outlier detection with autoencoder ensembles. In SDM 

2017.

[13] H. Daumé III. Frustratingly easy domain adaptation. ACL  2007.

[14] D. Deshpande. Managed security services: an emerging solution to security. In InfoSecCD  2005.

[15] L. Dinh  D. Krueger  and Y. Bengio. Nice: non-linear independent components estimation. arXiv  2014.

[16] P. Dokas  L. Ertoz  V. Kumar  A. Lazarevic  J. Srivastava  and P.-N. Tan. Data mining for network intrusion

detection. In Proc. NSF Workshop on Next Generation Data Mining  pages 21–30  2002.

[17] H. Fujita  T. Matsukawa  and E. Suzuki. One-class selective transfer machine for personalized anomalous

facial expression detection. In VISIGRAPP (5: VISAPP)  pages 274–283  2018.

[18] L. Ge  J. Gao  H. Ngo  K. Li  and A. Zhang. On handling negative transfer and imbalanced distributions in
multiple source transfer learning. Statistical Analysis and Data Mining: The ASA Data Science Journal 
7(4):254–271  2014.

[19] M. Germain  K. Gregor  I. Murray  and H. Larochelle. Made: masked autoencoder for distribution

estimation. In ICML  2015.

[20] M. Ghifary  W. Bastiaan Kleijn  M. Zhang  and D. Balduzzi. Domain generalization for object recognition

with multi-task autoencoders. In ICCV  2015.

[21] G. Haixiang  L. Yijing  J. Shang  G. Mingyun  H. Yuanyue  and G. Bing. Learning from class-imbalanced

data: Review of methods and applications. Expert Systems with Applications  73:220–239  2017.

[22] D. Hendrycks  M. Mazeika  and T. G. Dietterich. Deep anomaly detection with outlier exposure. In ICLR 

2019.

[23] A. Herschtal and B. Raskutti. Optimising area under the roc curve using gradient descent. In ICML  2004.

[24] I. Higgins  L. Matthey  A. Pal  C. Burgess  X. Glorot  M. Botvinick  S. Mohamed  and A. Lerchner.

beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR  2017.

[25] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science 

313(5786):504–507  2006.

[26] J. Hoffman  E. Tzeng  T. Park  J.-Y. Zhu  P. Isola  K. Saenko  A. A. Efros  and T. Darrell. Cycada:

cycle-consistent adversarial domain adaptation. In ICML  2018.

[27] T. Idé. Collaborative anomaly detection on blockchain from noisy sensor data. In 2018 IEEE International

Conference on Data Mining Workshops (ICDMW)  pages 120–127. IEEE  2018.

[28] T. Idé  D. T. Phan  and J. Kalagnanam. Multi-task multi-modal models for collective anomaly detection. In

ICDM  2017.

[29] T. Iwata and Y. Yamanaka. Supervised anomaly detection based on deep autoregressive density estimators.

arXiv  2019.

[30] F. Keller  E. Muller  and K. Bohm. Hics: high contrast subspaces for density-based outlier ranking. In

ICDE  2012.

[31] D. P. Kingma and P. Dhariwal. Glow: generative ﬂow with invertible 1x1 convolutions. In NeurIPS  2018.

[32] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR  2014.

[33] Y. Kou  C.-T. Lu  S. Sirwongwattana  and Y.-P. Huang. Survey of fraud detection techniques. In ICNSC 

2004.

10

[34] A. Kumagai and T. Iwata. Zero-shot domain adaptation without domain semantic descriptors. arXiv  2018.

[35] F. T. Liu  K. M. Ting  and Z.-H. Zhou. Isolation forest. In ICDM  2008.

[36] M. Long  Z. CAO  J. Wang  and M. I. Jordan. Conditional adversarial domain adaptation. In NeurIPS.

2018.

[37] M. Long  H. Zhu  J. Wang  and M. I. Jordan. Unsupervised domain adaptation with residual transfer

networks. In NeurIPS  2016.

[38] S. Motiian  M. Piccirilli  D. A. Adjeroh  and G. Doretto. Uniﬁed deep supervised domain adaptation and

generalization. In ICCV  2017.

[39] S. J. Pan and Q. Yang. A survey on transfer learning.

engineering  22(10):1345–1359  2010.

IEEE Transactions on knowledge and data

[40] L. Ruff  N. Görnitz  L. Deecke  S. A. Siddiqui  R. Vandermeulen  A. Binder  E. Müller  and M. Kloft. Deep

one-class classiﬁcation. In ICML  2018.

[41] K. Saenko  B. Kulis  M. Fritz  and T. Darrell. Adapting visual category models to new domains. In ECCV 

2010.

[42] K. Saito  K. Watanabe  Y. Ushiku  and T. Harada. Maximum classiﬁer discrepancy for unsupervised

domain adaptation. In CVPR  2018.

[43] M. Sakurada and T. Yairi. Anomaly detection using autoencoders with nonlinear dimensionality reduction.
In Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis  page 4.
ACM  2014.

[44] B. Schölkopf  J. C. Platt  J. Shawe-Taylor  A. J. Smola  and R. C. Williamson. Estimating the support of a

high-dimensional distribution. Neural computation  13(7):1443–1471  2001.

[45] S. Tokui  K. Oono  S. Hido  and J. Clayton. Chainer: a next-generation open source framework for deep

learning. In Proceedings of workshop on machine learning systems (LearningSys) in NeurIPS  2015.

[46] B. Uria  M.-A. Côté  K. Gregor  I. Murray  and H. Larochelle. Neural autoregressive distribution estimation.

JMLR  17(1):7184–7220  2016.

[47] J. Wang  Y. Chen  S. Hao  W. Feng  and Z. Shen. Balanced distribution adaptation for transfer learning. In

ICDM  2017.

[48] Y. Xiao  B. Liu  S. Y. Philip  and Z. Hao. A robust one-class transfer learning method with uncertain data.

Knowledge and Information Systems  44(2):407–438  2015.

[49] Y. Xue  X. Liao  L. Carin  and B. Krishnapuram. Multi-task learning for classiﬁcation with dirichlet

process priors. Journal of Machine Learning Research  8(Jan):35–63  2007.

[50] Y. Yamanaka  T. Iwata  H. Takahashi  M. Yamada  and S. Kanai. Autoencoding binary classiﬁers for

supervised anomaly detection. arXiv  2019.

[51] K. Yamanishi  J.-I. Takeuchi  G. Williams  and P. Milne. On-line unsupervised outlier detection using ﬁnite
mixtures with discounting learning algorithms. Data Mining and Knowledge Discovery  8(3):275–300 
2004.

[52] L. Yan  R. H. Dodier  M. Mozer  and R. H. Wolniewicz. Optimizing classiﬁer performance via an

approximation to the wilcoxon-mann-whitney statistic. In ICML  2003.

[53] Z. Yuan  D. Bao  Z. Chen  and M. Liu. Integrated transfer learning algorithm using multi-source tradaboost

for unbalanced samples classiﬁcation. In CIIS  2017.

[54] M. Zaheer  S. Kottur  S. Ravanbakhsh  B. Poczos  R. R. Salakhutdinov  and A. J. Smola. Deep sets. In

NeurIPS  2017.

[55] S. Zhai  Y. Cheng  W. Lu  and Z. Zhang. Deep structured energy based models for anomaly detection. In

ICML  2016.

[56] C. Zhou and R. C. Paffenroth. Anomaly detection with robust deep autoencoders. In KDD  2017.

[57] B. Zong  Q. Song  M. R. Min  W. Cheng  C. Lumezanu  D. Cho  and H. Chen. Deep autoencoding gaussian

mixture model for unsupervised anomaly detection. ICLR  2018.

11

,Marius Pachitariu
Biljana Petreska
Maneesh Sahani
Atsutoshi Kumagai
Tomoharu Iwata
Yasuhiro Fujiwara