2017,Bayesian Dyadic Trees and Histograms for  Regression,Many machine learning  tools for regression are based on recursive partitioning of the covariate space into smaller regions  where the regression function can be estimated locally. Among these  regression trees and their ensembles have demonstrated impressive empirical performance.    In this work   we shed light on the machinery behind Bayesian variants of these methods.  In particular  we study Bayesian regression histograms  such as Bayesian dyadic trees  in the simple regression case with just one predictor.   We focus on the reconstruction of regression surfaces that are piecewise constant  where the number of jumps is unknown. We show that with suitably designed priors  posterior distributions concentrate around the true step regression function at a near-minimax rate. These results {\sl do not} require the knowledge of the true number of steps  nor the width of the true partitioning cells. Thus  Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location  of jumps. Our results constitute the first step towards  understanding why Bayesian trees and their ensembles have worked so well in practice.  As an aside  we discuss prior distributions  on balanced interval partitions and how they relate to an old  problem in geometric probability. Namely  we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid  a new variant of the original problem.,Bayesian Dyadic Trees and Histograms for Regression

Stéphanie van der Pas
Mathematical Institute

Leiden University

Leiden  The Netherlands

svdpas@math.leidenuniv.nl

Veronika Roˇcková

Booth School of Business

University of Chicago
Chicago  IL  60637

Veronika.Rockova@ChicagoBooth.edu

Abstract

Many machine learning tools for regression are based on recursive partitioning
of the covariate space into smaller regions  where the regression function can
be estimated locally. Among these  regression trees and their ensembles have
demonstrated impressive empirical performance.
In this work  we shed light
on the machinery behind Bayesian variants of these methods. In particular  we
study Bayesian regression histograms  such as Bayesian dyadic trees  in the simple
regression case with just one predictor. We focus on the reconstruction of regression
surfaces that are piecewise constant  where the number of jumps is unknown. We
show that with suitably designed priors  posterior distributions concentrate around
the true step regression function at a near-minimax rate. These results do not require
the knowledge of the true number of steps  nor the width of the true partitioning
cells. Thus  Bayesian dyadic regression trees are fully adaptive and can recover the
true piecewise regression function nearly as well as if we knew the exact number
and location of jumps. Our results constitute the ﬁrst step towards understanding
why Bayesian trees and their ensembles have worked so well in practice. As an
aside  we discuss prior distributions on balanced interval partitions and how they
relate to an old problem in geometric probability. Namely  we relate the probability
of covering the circumference of a circle with random arcs whose endpoints are
conﬁned to a grid  a new variant of the original problem.

1

Introduction

Histogram regression methods  such as regression trees [1] and their ensembles [2]  have an impressive
record of empirical success in many areas of application [3  4  5  6  7]. Tree-based machine learning
(ML) methods build a piecewise constant reconstruction of the regression surface based on ideas
of recursive partitioning. Perhaps the most popular partitioning schemes are the ones based on
parallel-axis splits. One recent example is the Mondrian process [8]  which was introduced to the
ML community as a prior over tree data structures with interesting self-consistency properties. Many
efﬁcient algorithms exist that can be deployed to ﬁt regression histograms underpinned by some
partitioning scheme. Among these  Bayesian variants  such as Bayesian CART [9  10] and BART
[11]  have appealed to umpteen practitioners. There are several reasons why. Bayesian tree-based
regression tools (a) can adapt to regression surfaces without any need for pruning  (b) are reluctant to
overﬁt  (c) provide an avenue for uncertainty statements via posterior distributions. While practical
success stories abound [3  4  5  6  7]  the theoretical understanding of Bayesian regression tree
methods has been lacking. In this work  we study the quality of posterior distributions with regard
to the three properties mentioned above. We provide ﬁrst theoretical results that contribute to the
understanding of Bayesian Gaussian regression methods based on recursive partitioning.
Our performance metric will be the speed of posterior concentration/contraction around the true
regression function. This is ultimately a frequentist assessment  describing the typical behavior of the
posterior under the true generative model [12]. Posterior concentration rate results are now slowly

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

entering the machine learning community as a tool for obtaining more insights into Bayesian methods
[13  14  15  16  17]. Such results quantify not only the typical distance between a point estimator
(posterior mean/median) and the truth  but also the typical spread of the posterior around the truth.
Ideally  most of the posterior mass should be concentrated in a ball centered around the true value
with a radius proportional to the minimax rate [12  18]. Being inherently a performance measure of
both location and spread  optimal posterior concentration provides a necessary certiﬁcate for further
uncertainty quantiﬁcation [19  20  21]. Beyond uncertainty assessment  theoretical guarantees that
describe the average posterior shrinkage behavior have also been a valuable instrument for assessing
the suitability of priors. As such  these results can often provide useful guidelines for the choice of
tuning parameters  e.g. the latent Dirichlet allocation model [14].
Despite the rapid growth of this frequentist-Bayesian theory ﬁeld  posterior concentration results
for Bayesian regression histograms/trees/forests have  so far  been unavailable. Here  we adopt this
theoretical framework to get new insights into why these methods work so well.
Related Work

Bayesian density estimation with step functions is a relatively well-studied problem [22  23  24]. The
literature on Bayesian histogram regression is a bit less crowded. Perhaps the closest to our conceptual
framework is the work by Coram and Lalley [25]  who studied Bayesian non-parametric binary
regression with uniform mixture priors on step functions. The authors focused on L1 consistency.
Here  we focus on posterior concentration rather than consistency. We are not aware of any other
related theoretical study of Bayesian histogram methods for Gaussian regression.
Our Contributions

In this work we focus on a canonical regression setting with merely one predictor. We study
hierarchical priors on step functions and provide conditions under which the posteriors concentrate
optimally around the true regression function. We consider the case when the true regression function
itself is a step function  i.e. a tree or a tree ensemble  where the number and location of jumps is
unknown.
We start with a very simple space of approximating step functions  supported on equally sized intervals
where the number of splits is equipped with a prior. These partitions include dyadic regression trees.
We show that for a suitable complexity prior  all relevant information about the true regression
function (jump sizes and the number of jumps) is learned from the data automatically. During the
course of the proof  we develop a notion of the complexity of a piecewise constant function relative
to its approximating class.
Next  we take a larger approximating space consisting of functions supported on balanced partitions
that do not necessarily have to be of equal size. These correspond to more general trees with splits at
observed values. With a uniform prior over all balanced partitions  we are able to achieve a nearly
ideal performance (as if we knew the number and the location of jumps). As an aside  we describe
the distribution of interval lengths obtained when the splits are sampled uniformly from a grid. We
relate this distribution to the probability of covering the circumference of a circle with random arcs  a
problem in geometric probability that dates back to [26  27]. Our version of this problem assumes
that the splits are chosen from a discrete grid rather than from a unit interval.
Notation
With ∝ and (cid:46) we will denote an equality and inequality  up to a constant. The ε-covering number
of a set Ω for a semimetric d  denoted by N (ε  Ω  d)  is the minimal number of d-balls of radius ε
needed to cover the set Ω. We denote by φ(·) the standard normal density and by P n
(cid:80)n
n-fold product measure of the n independent observations under (1) with a regression function f (·).
i=1 δxi we denote the empirical distribution of the observed covariates  by || · ||n the
By Px
n) and by || · ||2 the standard Euclidean norm.
norm on L2(Px
2 Bayesian Histogram Regression
We consider a classical nonparametric regression model  where response variables Y (n) =
(Y1  . . .   Yn)(cid:48) are related to input variables x(n) = (x1  . . .   xn)(cid:48) through the function f0 as fol-
lows
(1)

f =(cid:78) Pf i the

Yi = f0(xi) + εi 

n = 1
n

εi ∼ N (0  1) 

i = 1  . . .   n.

2

We assume that the covariate values xi are one-dimensional  ﬁxed and have been rescaled so that
xi ∈ [0  1]. Partitioning-based regression methods are often invariant to monotone transformations
of observations. In particular  when f0 is a step function  standardizing the distance between the
observations  and thereby the split points  has no effect on the nature of the estimation problem.
Without loss of generality  we will thereby assume that the observations are aligned on an equispaced
grid.
Assumption 1. (Equispaced Grid) We assume that the scaled predictor values satisfy xi = i
each i = 1  . . .   n.

n for

This assumption implies that partitions that are balanced in terms of the Lebesque measure will be
balanced also in terms of the number of observations. A similar assumption was imposed by Donoho
[28] in his study of Dyadic CART.
The underlying regression function f0 : [0  1] → R is assumed to be a step function  i.e.

K0(cid:88)

k=1

f0(x) =

β0
k

I
Ω0
k

(x) 

k}K0

k is associated with a step size β0

k. The entire vector of K0 step sizes will be denoted by β0 = (β0

k=1 is a partition of [0  1] into K0 non-overlapping intervals. We assume that {Ω0

k}K0
where {Ω0
k=1
is minimal  meaning that f0 cannot be represented with a smaller partition (with less than K0 pieces).
Each partitioning cell Ω0
k  determining the level of the function f0
on Ω0
One might like to think of f0 as a regression tree with K0 bottom leaves. Indeed  every step function
can be associated with an equivalence class of trees that live on the same partition but differ in
their tree topology. The number of bottom leaves K0 will be treated as unknown throughout this
paper. Our goal will be designing a suitable class of priors on step functions so that the posterior
concentrates tightly around f0. Our analysis with a single predictor has served as a precursor to a
full-blown analysis for high-dimensional regression trees [29].
We consider an approximating space of all step functions (with K = 1  2  . . . bottom leaves)

1   . . .   β0

K)(cid:48).

which consists of smaller spaces (or shells) of all K-step functions

F = ∪∞

K=1FK 

(cid:40)

FK =

fβ : [0  1] → R; fβ(x) =

βkIΩk (x)

(cid:41)

(2)

 

K(cid:88)

k=1

k=1 of size K  and (c) a prior on step sizes β = (β1  . . .   βK)(cid:48).

each indexed by a partition {Ωk}K
k=1 and a vector of K step heights β. The fundamental building
block of our theoretical analysis will be the prior on F. This prior distribution has three main
ingredients  described in detail below  (a) a prior on the number of steps K  (b) a prior on the
partitions {Ωk}K
2.1 Prior πK(·) on the Number of Steps K
To avoid overﬁtting  we assign an exponentially decaying prior distribution that penalizes partitions
with too many jumps.
Deﬁnition 2.1. (Prior on K) The prior on the number of partitioning cells K satisﬁes

πK(k) ≡ Π(K = k) ∝ exp(−cK k log k)

for k = 1  2  . . . .

(3)

This prior is no stranger to non-parametric problems. It was deployed for stepwise reconstructions of
densities [24  23] and regression surfaces [25]. When cK is large  this prior is concentrated on models
with small complexity where overﬁtting should not occur. Decreasing cK leads to the smearing of
the prior mass over partitions with more jumps. This is illustrated in Figure 1  which depicts the prior
for various choices of cK. We provide recommendations for the choice of cK in Section 3.1.
2.2 Prior πΩ(·| K) on Interval Partitions {Ωk}K
After selecting the number of steps K from πK(k)  we assign a prior over interval partitions πΩ(·|K).
We will consider two important special cases.

k=1

3

Figure 1: (Left) Prior on the tree size for several values of cK  (Right) Best approximations of f0 (in
the (cid:96)2 sense) by step functions supported on equispaced blocks of size K ∈ {2  5  10}.

2.2.1 Equivalent Blocks

Perhaps the simplest partition is based on statistically equivalent blocks [30]  where all the cells are
required to have the same number of points. This is also known as the K-spacing rule that partitions
the unit interval using order statistics of the observations.
Deﬁnition 2.2. (Equivalent Blocks) Let x(i) denote the ith order statistic of x = (x1  . . .   xn)(cid:48) 
where x(n) ≡ 1 and n = Kc for some c ∈ N\{0}. Denote by x(0) ≡ 0. A partition {Ωk}K
consists of K equivalent blocks  when Ωk = (x(jk)  x(jk+1)]  where jk = (k − 1)c.
A variant of this deﬁnition can be obtained in terms of interval lengths rather than numbers of
observations.
Deﬁnition 2.3. (Equispaced Blocks) A partition {Ωk}K

k=1 consists of K equispaced blocks Ωk  when

k=1

Ωk =(cid:0) k−1

K   k

K

(cid:3)

for k = 1  . . .   K.

When K = 2s for some s ∈ N\{0}  the equispaced partition corresponds to a full complete binary
tree with splits at dyadic rationals. If the observations xi lie on a regular grid (Assumption 1)  then
Deﬁnition 2.2 and 2.3 are essentially equivalent. We will thereby focus on equivalent blocks (EB)
and denote such a partition (for a given K > 0) with ΩEB
K . Because there is only one such partition
for each K  the prior πΩ(·|K) has a single point mass mass at ΩEB
K we
denote the set of all EB partitions for K = 1  2  . . . . We will use these partitioning schemes as a
jump-off point.

K . With ΩEB = ∪∞

K=1ΩEB

2.2.2 Balanced Intervals

Equivalent (equispaced) blocks are deterministic and  as such  do not provide much room for learning
about the actual location of jumps in f0. Balanced intervals  introduced below  are a richer class of
partitions that tolerate a bit more imbalance. First  we introduce the notion of cell counts µ(Ωk). For
each interval Ωk  we write

µ(Ωk) =

I(xi ∈ Ωk) 

(4)

the proportion of observations falling inside Ωk. Note that for equivalent blocks  we can write
µ(Ω1) = ··· = µ(ΩK) = c/n = 1/K.
Deﬁnition 2.4. (Balanced Intervals) A partition {Ωk}K

k=1 is balanced if

n(cid:88)

i=1

1
n

4

C 2
min
K

≤ µ(Ωk) ≤ C 2
max
K

for all k = 1  . . .   K

(5)

for some universal constants Cmin ≤ 1 ≤ Cmax not depending on K.

2468100.00.10.20.30.40.5klllllllllll11/21/51/10pK(k)0.00.20.40.60.81.00123456xllllllllllllllllllllllllllllllllllllllllTrueK=2K=5K=10f0(x)(a) K = 2

(b) K = 3

Figure 2: Two sets EK of possible stick lengths that satisfy the minimal cell-size condition |Ωk| ≥ C
with n = 10  C = 2/n and K = 2  3.

(cid:101)C 2
min/K ≤ |Ωk| ≤ (cid:101)C 2

K=1ΩBI

The following variant of the balancing condition uses interval widths rather than cell counts:
max/K. Again  under Assumption 1  these two deﬁnitions are equiva-
lent. In the sequel  we will denote by ΩBI
K the set of all balanced partitions consisting of K intervals
and by ΩBI = ∪∞
K the set of all balanced intervals of sizes K = 1  2  . . . . It is worth pointing
out that the balance assumption on the interval partitions can be relaxed  at the expense of a log factor
in the concentration rate [29].
With balanced partitions  the K th shell FK of the approximating space F in (2) consists of all step
K and have K−1 points of discontinuity uk ∈ In ≡ {xi :
functions that are supported on partitions ΩBI
i = 1  . . .   n − 1} for k = 1  . . . K − 1. For equispaced blocks in Deﬁnition 2.3  we assumed that
the points of subdivision were deterministic  i.e. uk = k/K. For balanced partitions  we assume that
uk are random and chosen amongst the observed values xi. The order statistics of the vector of splits
u = (u1  . . .   uK−1)(cid:48) uniquely deﬁne a segmentation of [0  1] into K intervals Ωk = (u(k−1)  u(k)] 
where u(k) designates the kth smallest value in u and u(0) ≡ 0  u(K) = x(n) ≡ 1.
Our prior over balanced intervals πΩ(·| K) will be deﬁned implicitly through a uniform prior over
the split vectors u. Namely  the prior over balanced partitions ΩBI

I(cid:16){Ωk}K

K satisﬁes
k=1 ∈ ΩBI

K

(cid:17)

.

(6)

πΩ({Ωk}K

k=1 | K) =

1

card(ΩBI
K )

In the following Lemma  we obtain upper bounds on card(ΩBI
K ) and discuss how they relate to an
old problem in geometric probability. In the sequel  we denote with |Ωk| the lengths of the segments
deﬁned through the split points u.
Lemma 2.1. Assume that u = (u1  . . .   uK−1)(cid:48) is a vector of independent random variables
obtained by uniform sampling (without replacement) from In. Then under Assumption 1  we have for
1/n < C < 1/K

(cid:18)

Π

min
1≤k≤K

|Ωk| ≥ C

=

(cid:19)

|Ωk| ≤ C

= 1 −

(−1)k

(cid:19)
(cid:101)n(cid:88)

k=1

(cid:1)

(cid:0)(cid:98)n(1−K C)(cid:99)+K−1
(cid:0) n−1
(cid:1)
(cid:19)(cid:0)(cid:98)n(1−k C)(cid:99)+K−1
(cid:1)
(cid:18)n − 1
(cid:1)

(cid:0) n−1

K−1
K−1

k

K−1
K−1

(7)

(8)

 

and

(cid:18)

Π

max
1≤k≤K

where(cid:101)n = min{n − 1 (cid:98)1/C(cid:99)}.

Proof. The denominator of (7) follows from the fact that there are n − 1 possible splits for the
K − 1 points of discontinuity uk. The numerator is obtained after adapting the proof of Lemma

5

W2W1llllllllllllll00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91c1-cc1-cK=200.20.40.60.8100.20.40.60.8100.20.40.60.81W2W1W3lllllllllllllllllllllllllllllllllllllllllK=3Konheim [31] that the set EK = {|Ωk| : (cid:80)K
in the interior of a convex hull of K points vr = (1 − KC)er + C(cid:80)K
points) and a = 2. With K = 2 (Figure 2(a))  there are only 7 =(cid:0)n(1−KC)+K−1

2 of Flatto and Konheim [31]. Without lost of generality  we will assume that C = a/n for some
a = 1  . . .  (cid:98)n/K(cid:99) so that n(1 − KC) is an integer. Because the jumps uk can only occur on the
grid In  we have |Ωk| = j/n for some j = 1  . . .   n − 1. It follows from Lemma 1 of Flatto and
k=1 |Ωk| = 1 and |Ωk| ≥ C for k = 1  . . .   K} lies
k=1 ek for r = 1  . . .   K 
where er = (er1  . . .   erK)(cid:48) are unit base vectors  i.e. erj = I(r = j). Two examples of the set EK
(for K = 2 and K = 3) are depicted in Figure 2. In both ﬁgures  n = 10 (i.e. 9 candidate split
lengths (|Ω1| |Ω2|)(cid:48) that satisfy the minimal cell condition. These points lie on a grid between
the two vertices v1 = (1 − C  C) and v2 = (C  1 − C). With K = 3  the convex hull of points
v1 = (1 − 2C  C  C)(cid:48)  v2 = (C  1 − 2C  C)(cid:48) and v1 = (C  C  1 − 2C)(cid:48) corresponds to a diagonal
dissection of a cube of a side length (1 − 3C) (Figure 2(b)  again with a = 2 and n = 10). The
number of lattice points in the interior (and on the boundary) of such tetrahedron corresponds to
an arithmetic sum 1
K = 3. To complete the induction argument  suppose that the formula holds for some arbitrary
K > 0. Then the size of the lattice inside (and on the boundary) of a (K + 1)-tetrahedron of a side
length [1 − (K + 1)C] can be obtained by summing lattice sizes inside K-tetrahedrons of increasing
side lengths 0 

2 (n − 3a + 2)(n − 3a + 1) =(cid:0)n−3a+2

(cid:1). So far  we showed (7) for K = 2 and

(cid:1) pairs of interval

2/n  . . .   [1 − (K + 1)C]

2/n  2

K−1

√

√

√

2

n[1−(K+1)C]+K−1(cid:88)
(cid:0) j

(cid:18) j
(cid:1) =(cid:0)N +1

j=K−1

2/n  i.e.

(cid:18)n[1 − (K + 1)C] + K

(cid:19)
(cid:1). The second statement (8) is obtained by writing the

(cid:19)

K

=

 

K − 1

where we used the fact(cid:80)N

event as a complement of the union of events and applying the method of inclusion-exclusion.
Remark 2.1. Flatto and Konheim [31] showed that the probability of covering a circle with random
arcs of length C is equal to the probability that all segments of the unit interval  obtained with iid
random uniform splits  are smaller than C. Similarly  the probability (8) could be related to the
probability of covering the circle with random arcs whose endpoints are chosen from a grid of n − 1
equidistant points on the circumference.

There are(cid:0) n−1
balancing condition (where (cid:101)C 2

(cid:1) partitions of size K  of which(cid:0)(cid:98)n(1−(cid:101)C2

(cid:1) satisfy the minimal cell width

min > K/n). This number gives an upper bound on the combinatorial

min)(cid:99)+K−1
K−1

complexity of balanced partitions card(ΩBI
2.3 Prior π(β | K) on Step Heights β
To complete the prior on F K  we take independent normal priors on each of the coefﬁcients. Namely

K ).

K−1

j=K

K

K+1

K(cid:89)

π(β | K) =

φ(βk) 

(9)

where φ(·) is the standard normal density.

k=1

3 Main Results

k}K0

k=1 and the approximating partitions {Ωk}K

A crucial ingredient of our proof will be understanding how well one can approximate f0 with other
step functions (supported on partitions Ω  which are either equivalent blocks ΩEB or balanced
partitions ΩBI). We will describe the approximation error in terms of the overlap between the true
partition {Ω0
k=1 ∈ Ω. More formally  we deﬁne the
restricted cell count (according to Nobel [32]) as

(cid:16)
m
the number of cells in {Ω0
k=1 that overlap with an interval V ⊂ [0  1]. Next  we deﬁne the
k}K0
complexity of f0 as the smallest size of a partition in Ω needed to completely cover f0 without any
overlap.

k ∩ V (cid:54)= ∅| 

V ;{Ω0

= |Ω0

k}K0

k : Ω0

(cid:17)

k=1

6

Deﬁnition 3.1. (Complexity of f0 w.r.t. Ω) We deﬁne K(f0  Ω) as the smallest K such that there
exists a K-partition {Ωk}K
m

k=1 in the class of partitions Ω for which

= 1 for all k = 1  . . .   K.

Ωk;{Ω0

(cid:16)

(cid:17)

k}K0

k=1

The number K(f0  Ω) will be referred to as the complexity of f0 w.r.t. Ω.

k}K0

k}K0

k|. If the minimal partition {Ω0

The complexity number K(f0  Ω) indicates the optimal number of steps needed to approximate f0
with a step function (supported on partitions in Ω) without any error. It depends on the true number
of jumps K0 as well as the true interval lengths |Ω0
k=1 resided in the
k=1 ∈ Ω  then we would obtain K(f0  Ω) = K0  the true number of
approximating class  i.e. {Ω0
k}K0
k=1 /∈ Ω  the complexity number K(f0  Ω) can be much larger.
steps. On the other hand  when {Ω0
This is illustrated in Figure 1 (right)  where the true partition {Ω0
k=1 consists of K0 = 4 unequal
pieces and we approximate it with equispaced blocks with K = 2  5  10 steps. Because the intervals
k are not equal and the smallest one has a length 1/10  we need K(f0  ΩEB) = 10 equispaced
Ω0
blocks to perfectly approximate f0. For our analysis  we do not need to assume that {Ω0
k=1 ∈ Ω
(i.e. f0 does not need to be inside the approximating class) or that K(f0  Ω) is ﬁnite. The complexity
number can increase with n  where sharper performance is obtained when f0 can be approximated
error-free with some f ∈ Ω  where f has a small number of discontinuities relative to n.
Another way to view K(f0  Ω) is as the ideal partition size on which the posterior should con-
centrate. If this number were known  we could achieve a near-minimax posterior concentration

rate n−1/2(cid:112)K(f0  Ω) log[n/K(f0  Ω)] (Remark 3.3). The actual minimax rate for estimating a
piece-wise constant f0 (consisting of K0 > 2 pieces) is n−1/2(cid:112)K0 log(n/K0) [33]. In our main

k}K0

k}K0

results  we will target the nearly optimal rate expressed in terms of K(f0  Ω).

3.1 Posterior Concentration for Equivalent Blocks

Our ﬁrst result shows that the minimax rate is nearly achieved  without any assumptions on the
number of pieces of f0 or the sizes of the pieces.
Theorem 3.1. (Equivalent Blocks) Let f0 : [0  1] → R be a step function with K0 steps  where K0
is unknown. Denote by F the set of all step functions supported on equivalent blocks  equipped
with priors πK(·) and π(β | K) as in (3) and (9). Denote with Kf0 ≡ K(f0  ΩEB) and assume
(cid:107)β0(cid:107)2∞ (cid:46) log n and Kf0

n. Then  under Assumption 1  we have

(cid:46) √

(cid:18)
f ∈ F : (cid:107)f − f0(cid:107)n ≥ Mnn−1/2(cid:113)

Π

(cid:19)

Kf0 log (n/Kf0 ) | Y (n)

→ 0

(10)

in P n
f0

-probability  for every Mn → ∞ as n → ∞.

.

Before we proceed with the proof  a few remarks ought to be made. First  it is worthwhile to
emphasize that the statement in Theorem 3.1 is a frequentist one as it relates to an aggregated
behavior of the posterior distributions obtained under the true generative model P n
f0
Second  the theorem shows that the Bayesian procedure performs an automatic adaptation to
K(f0  ΩEB). The posterior will concentrate on EB partitions that are ﬁne enough to approximate f0
well. Thus  we are able to recover the true function as well as if we knew K(f0  ΩEB).
Third  it is worth mentioning that  under Assumption 1  Theorem 3.1 holds for equivalent as well as
equisized blocks. In this vein  it describes the speed of posterior concentration for dyadic regression
trees. Indeed  as mentioned previously  with K = 2s for some s ∈ N\{0}  the equisized partition
corresponds to a full binary tree with splits at dyadic rationals.
Another interesting insight is that the Gaussian prior (9)  while selected for mathematical convenience 
turns out to be sufﬁcient for optimal recovery. In other words  despite the relatively large amount of
mass near zero  the Gaussian prior does not rule out optimal posterior concentration. Our standard
normal prior is a simpler version of the Bayesian CART prior  which determines the variance from
the data [9].
Let Kf0 ≡ K(f0  ΩEB) be as in Deﬁnition 3.1. Theorem 3.1 is proved by verifying the three
K=0 FK  with

conditions of Theorem 4 of [18]  for εn = n−1/2(cid:112)Kf0 log(n/Kf0) and Fn = (cid:83)kn

7

kn of the order Kf0 log(n/Kf0). The approximating subspace Fn ⊂ F should be rich enough to
approximate f0 well and it should receive most of the prior mass. The conditions for posterior
contraction at the rate εn are:

log N(cid:0) ε

(C1) sup
ε>εn

36  {f ∈ Fn : (cid:107)f − f0(cid:107)n < ε} (cid:107).(cid:107)n

(C2)

Π(F\Fn)
Π(f ∈ F : (cid:107)f − f0(cid:107)2

= o(e−2nε2
(C3) Π(f ∈ Fn : jεn < (cid:107)f − f0(cid:107)n ≤ 2jεn)

n ≤ ε2
n)

Π(f ∈ F : (cid:107)f − f0(cid:107)2

n ≤ ε2
n)

n ) 

≤ e

(cid:1) ≤ nε2

n 

j2
4 nε2

n for all sufﬁciently large j.

The entropy condition (C1) restricts attention to EB partitions with small K. As will be seen from the
proof  the largest allowed partitions have at most (a constant multiple of) Kf0 log (n/Kf0 ) pieces..
Condition (C2) requires that the prior does not promote partitions with more than Kf0 log (n/Kf0)
pieces. This property is guaranteed by the exponentially decaying prior πK(·)  which penalizes large
partitions.
The ﬁnal condition  (C3)  requires that the prior charges a (cid:107).(cid:107)n neighborhood of the true function. In
our proof  we verify this condition by showing that the prior mass on step functions of the optimal
size Kf0 is sufﬁciently large.

N(cid:0) ε

Proof. We verify the three conditions (C1)  (C2) and (C3).
(C1) Let ε > εn and K ∈ N. For fα  fβ ∈ FK  we have K−1(cid:107)α − β(cid:107)2
n because
µ(Ωk) = 1/K for each k. We now argue as in the proof of Theorem 12 of [18] to show that
Kε/36-balls required
Kε-ball in RK. This number is bounded above by 108K. Summing over K  we
to cover a
recognize a geometric series. Taking the logarithm of the result  we ﬁnd that (C1) is satisﬁed if
log(108)(kn + 1) ≤ nε2
n.

(cid:1) can be covered by the number of

36  {f ∈ FK : (cid:107)f − f0(cid:107)n < ε} (cid:107).(cid:107)n

2 = (cid:107)fα − fβ(cid:107)2
√

√

(cid:16)

(cid:17)

(C2) We bound the denominator by:

Π(f ∈ F : (cid:107)f − f0(cid:107)2
0 ∈ RKf0 is an extended version of β0 ∈ RK0  containing the coefﬁcients for f0 expressed

β ∈ RK(f0) : (cid:107)β − βext
0 (cid:107)2

n ≤ ε2) ≥ πK(Kf0)Π

2 ≤ ε2Kf0

 

where βext
as a step function on the partition {Ω0

πK(Kf0)
e(cid:107)βext
2/2

0 (cid:107)2

Π

β ∈ RK(f0) : (cid:107)β(cid:107)2

(cid:16)

k=1. This can be bounded from below by

(cid:90) ε2Kf0 /2

k}Kf0
(cid:17)
2 ≤ ε2Kf0/2

>

πK(Kf0)
e(cid:107)βext
2/2

0 (cid:107)2

0

xKf0 /2−1e−x/2
2Kf0 /2Γ(Kf0 /2)

dx.

We bound this from below by bounding the exponential at the upper integration limit  yielding:

e−ε2Kf0 /4

πK(Kf0)
e(cid:107)βext
2/2

0 (cid:107)2

(11)
For ε = εn → 0  we thus ﬁnd that the denominator in (C2) can be lower bounded with
eKf0 log εn−cK Kf0 log Kf0−(cid:107)βext
0 (cid:107)2
Π(F\Fn) = Π

n/2]. We bound the numerator:
e−cK k log k ≤ e−cK (kn+1) log(kn+1) +

(cid:32) ∞(cid:91)

2/2−Kf0 /2[log 2+ε2

2Kf0 Γ(Kf0/2 + 1)

e−cK x log x 

(cid:90) ∞

∞(cid:88)

εKf0 K

(cid:33)

Fk

∝

Kf0 /2
f0

.

k=kn+1

k=kn+1

kn+1

which is of order e−cK (kn+1) log(kn+1). Combining this bound with (11)  we ﬁnd that (C2) is met if:

e−Kf0 log εn+(cK +1) Kf0 log Kf0 +Kf0(cid:107)β0(cid:107)2∞−cK (kn+1) log(kn+1)+2nε2

n → 0 as n → ∞.

(C3) We bound the numerator by one  and use the bound (11) for the denominator. As εn → 0  we
obtain the condition −Kf0 log εn + (cK + 1)Kf0 log Kf0 + Kf0(cid:107)β0(cid:107)2∞ ≤ j2
n for all sufﬁciently
large j.

4 nε2

8

(cid:46) √

n. Finally  the condition (C3) is met for Kf0

Conclusion With εn = n−1/2(cid:112)Kf0 log(n/Kf0)  letting kn ∝ nε2
(cid:46) √

n = Kf0 log(n/Kf0)  the
condition (C1) is met. With this choice of kn  the condition (C2) holds as well as long as (cid:107)β0(cid:107)2∞ (cid:46)
log n and Kf0
Remark 3.1. It is worth pointing out that the proof will hold for a larger class of priors on K 
as long as the prior shrinks at least exponentially fast (meaning that it is bounded from above by
ae−bK for constants a  b > 0). However  a prior at this exponential limit will require tuning  because
the optimal a and b will depend on K(f0  ΩEB). We recommend using the prior (2.1) that prunes
somewhat more aggressively  because it does not require tuning by the user. Indeed  Theorem 3.1
holds regardless of the choice of cK > 0. We conjecture  however  that values cK ≥ 1/K(f0  ΩEB)
lead to a faster concentration speed and we suggest cK = 1 as a default option.
Remark 3.2. When Kf0 is known  there is no need for assigning a prior πK(·) and the conditions
(C1) and (C3) are veriﬁed similarly as before  ﬁxing the number of steps at Kf0.

n.

3.2 Posterior Concentration for Balanced Intervals

An analogue of Theorem 3.1 can be obtained for balanced partitions from Section 2.2.2 that correspond
to regression trees with splits at actual observations. Now  we assume that f0 is ΩBI-valid and carry
out the proof with K(f0  ΩBI ) instead of K(f0  ΩEB). The posterior concentration rate is only
slightly worse.
Theorem 3.2. (Balanced Intervals) Let f0 : [0  1] → R be a step function with K0 steps  where K0
is unknown. Denote by F the set of all step functions supported on balanced intervals equipped with
priors πK(·)  πΩ(·|K) and π(β | K) as in (3)  (6) and (9). Denote with Kf0 ≡ K(f0  ΩBI ) and
assume (cid:107)β0(cid:107)2∞ (cid:46) log2β n and K(f0  ΩBI ) (cid:46) √
f ∈ F : (cid:107)f − f0(cid:107)n ≥ Mnn−1/2

n. Then  under Assumption 1  we have
→ 0

Kf0 log2β(n/Kf0) | Y (n)

(cid:113)

(cid:18)

(cid:19)

(12)

Π

(cid:16)(cid:80)kn

k−1

kn−1

) (cid:46)

(cid:113)

2

k=1 C kcard(ΩBI
k )

-probability  for every Mn → ∞ as n → ∞  where β > 1/2.

Kf0 log2β(n/Kf0). Using the upper bound card(ΩBI

in P n
f0
Proof. All three conditions (C1)  (C2) and (C3) hold if we choose kn ∝ Kf0 [log(n/Kf0 )]2β−1. The
entropy condition will be satisﬁed when log
n for some C > 0  where
εn = n−1/2
kn < n−1
Kf0 log(n/Kf0)  the condition (C2) will be satisﬁed when  for some D > 0  we have
e−Kf0 log εn+(cK +1) Kf0 log Kf0 +D Kf0 log(n/Kf0 )+Kf0(cid:107)β0(cid:107)2∞−cK (kn+1) log(kn+1)+2nε2
This holds for our choice of kn under the assumption (cid:107)β0(cid:107)2∞ (cid:46) log2β n and Kf0
choices also yield (C3).
Remark 3.3. When Kf0

for large enough n)  the condition (C1) is veriﬁed. Using the fact that card(ΩKf0

n  Theorem 3.1 and Theorem 3.2 still hold  only with the bit slower

(cid:17) (cid:46) n ε2
k ) <(cid:0)n−1

slower concentration rate n−1/2(cid:112)Kf0 log n.

(cid:38) √

(cid:1) (because

(cid:1) <(cid:0) n−1

(13)
n. These

n → 0.
(cid:46) √

4 Discussion

We provided the ﬁrst posterior concentration rate results for Bayesian non-parametric regression with
step functions. We showed that under suitable complexity priors  the Bayesian procedure adapts to
the unknown aspects of the target step function. Our approach can be extended in three ways: (a)
to smooth f0 functions  (b) to dimension reduction with high-dimensional predictors  (c) to more
general partitioning schemes that correspond to methods like Bayesian CART and BART. These three
extensions are developed in our followup manuscript [29].

5 Acknowledgment

This work was supported by the James S. Kemper Foundation Faculty Research Fund at the University
of Chicago Booth School of Business.

9

References
[1] L. Breiman  J. H. Friedman  R. A. Olshen  and C. J. Stone. Classiﬁcation and Regression Trees. Statis-

tics/Probability Series. Wadsworth Publishing Company  Belmont  California  U.S.A.  1984.

[2] L. Breiman. Random forests. Mach. Learn.  45:5–32  2001.

[3] A. Berchuck  E. S. Iversen  J. M. Lancaster  J. Pittman  J. Luo  P. Lee  S. Murphy  H. K. Dressman  P. G.
Febbo  M. West  J. R. Nevins  and J. R. Marks. Patterns of gene expression that characterize long-term
survival in advanced stage serous ovarian cancers. Clin. Cancer Res.  11(10):3686–3696  2005.

[4] S. Abu-Nimeh  D. Nappa  X. Wang  and S. Nair. A comparison of machine learning techniques for phishing
detection. In Proceedings of the Anti-phishing Working Groups 2nd Annual eCrime Researchers Summit 
eCrime ’07  pages 60–69  New York  NY  USA  2007. ACM.

[5] M. A. Razi and K. Athappilly. A comparative predictive analysis of neural networks (NNs)  nonlinear
regression and classiﬁcation and regression tree (CART) models. Expert Syst. Appl.  29(1):65 – 74  2005.

[6] D. P. Green and J. L. Kern. Modeling heterogeneous treatment effects in survey experiments with Bayesian

Additive Regression Trees. Public Opin. Q.  76(3):491  2012.

[7] E. C. Polly and M.

Super
http://works.bepress.com/mark_van_der_laan/200/  2010.

J. van der Laan.

learner

in prediction.

Available at:

[8] D. M. Roy and Y. W. Teh. The Mondrian process. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou 
editors  Advances in Neural Information Processing Systems 21  pages 1377–1384. Curran Associates 
Inc.  2009.

[9] H. A. Chipman  E. I. George  and R. E. McCulloch. Bayesian CART model search. JASA  93(443):935–948 

1998.

[10] D. Denison  B. Mallick  and A. Smith. A Bayesian CART algorithm. Biometrika  95(2):363–377  1998.

[11] H. A. Chipman  E. I. George  and R. E. McCulloch. BART: Bayesian Additive Regression Trees. Ann.

Appl. Stat.  4(1):266–298  03 2010.

[12] S. Ghosal  J. K. Ghosh  and A. W. van der Vaart. Convergence rates of posterior distributions. Ann. Statist. 

28(2):500–531  04 2000.

[13] T. Zhang. Learning bounds for a generalized family of Bayesian posterior distributions. In S. Thrun 
L. K. Saul  and P. B. Schölkopf  editors  Advances in Neural Information Processing Systems 16  pages
1149–1156. MIT Press  2004.

[14] J. Tang  Z. Meng  X. Nguyen  Q. Mei  and M. Zhang. Understanding the limiting factors of topic
modeling via posterior contraction analysis. In T. Jebara and E. P. Xing  editors  Proceedings of the
31st International Conference on Machine Learning (ICML-14)  pages 190–198. JMLR Workshop and
Conference Proceedings  2014.

[15] N. Korda  E. Kaufmann  and R. Munos. Thompson sampling for 1-dimensional exponential family bandits.
In C. J. C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Q. Weinberger  editors  Advances in
Neural Information Processing Systems 26  pages 1448–1456. Curran Associates  Inc.  2013.

[16] F.-X. Briol  C. Oates  M. Girolami  and M. A. Osborne. Frank-Wolfe Bayesian quadrature: Probabilistic
integration with theoretical guarantees. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and
R. Garnett  editors  Advances in Neural Information Processing Systems 28  pages 1162–1170. Curran
Associates  Inc.  2015.

[17] M. Chen  C. Gao  and H. Zhao. Posterior contraction rates of the phylogenetic indian buffet processes.

Bayesian Anal.  11(2):477–497  06 2016.

[18] S. Ghosal and A. van der Vaart. Convergence rates of posterior distributions for noniid observations. Ann.

Statist.  35(1):192–223  02 2007.

[19] B. Szabó  A. W. van der Vaart  and J. H. van Zanten. Frequentist coverage of adaptive nonparametric

Bayesian credible sets. Ann. Statist.  43(4):1391–1428  08 2015.

[20] I. Castillo and R. Nickl. On the Bernstein von Mises phenomenon for nonparametric Bayes procedures.

Ann. Statist.  42(5):1941–1969  2014.

10

[21] J. Rousseau and B. Szabo. Asymptotic frequentist coverage properties of Bayesian credible sets for sieve

priors in general settings. ArXiv e-prints  September 2016.

[22] I. Castillo. Polya tree posterior distributions on densities.
lpma-paris. fr/ pageperso/ castillo/ polya. pdf   2016.

preprint available at http: // www.

[23] L. Liu and W. H. Wong. Multivariate density estimation via adaptive partitioning (ii): posterior concentra-

tion. arXiv:1508.04812v1  2015.

[24] C. Scricciolo. On rates of convergence for Bayesian density estimation. Scand. J. Stat.  34(3):626–642 

2007.

[25] M. Coram and S. Lalley. Consistency of Bayes estimators of a binary regression function. Ann. Statist. 

34(3):1233–1269  2006.

[26] L. Shepp. Covering the circle with random arcs. Israel J. Math.  34(11):328–345  1972.

[27] W. Feller. An Introduction to Probability Theory and Its Applications  Vol. 2  3rd Edition. Wiley  3rd

edition  January 1968.

[28] D. L. Donoho. CART and best-ortho-basis: a connection. Ann. Statist.  25(5):1870–1911  10 1997.

[29] V. Rockova and S. L. van der Pas. Posterior concentration for Bayesian regression trees and their ensembles.

arXiv:1708.08734  2017.

[30] T. Anderson. Some nonparametric multivariate procedures based on statistically equivalent blocks. In P.R.

Krishnaiah  editor  Multivariate Analysis  pages 5–27. Academic Press  New York  1966.

[31] L. Flatto and A. Konheim. The random division of an interval and the random covering of a circle. SIAM

Rev.  4:211–222  1962.

[32] A. Nobel. Histogram regression estimation using data-dependent partitions. Ann. Statist.  24(3):1084–1105 

1996.

[33] C. Gao  F. Han  and C.H. Zhang. Minimax risk bounds for piecewise constant models. Manuscript  pages

1–36  2017.

11

,Brendan McMahan
Jacob Abernethy
Stephan Mandt
David Blei
Shakir Mohamed
Danilo Jimenez Rezende
Stéphanie van der Pas
Veronika Ročková
Faidra Georgia Monachou
Itai Ashlagi