2015,Efficient Non-greedy Optimization of Decision Trees,Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper  we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters  based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables  and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d^2)  where d is the tree depth  and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.,Efﬁcient Non-greedy Optimization of Decision Trees

Mohammad Norouzi1∗

Maxwell D. Collins2 ∗

David J. Fleet4

Pushmeet Kohli5

Matthew Johnson3

1 4 Department of Computer Science  University of Toronto

2 Department of Computer Science  University of Wisconsin-Madison

3 5 Microsoft Research

Abstract

Decision trees and randomized forests are widely used in computer vision and ma-
chine learning. Standard algorithms for decision tree induction optimize the split
functions one node at a time according to some splitting criteria. This greedy pro-
cedure often leads to suboptimal trees. In this paper  we present an algorithm for
optimizing the split functions at all levels of the tree jointly with the leaf param-
eters  based on a global objective. We show that the problem of ﬁnding optimal
linear-combination (oblique) splits for decision trees is related to structured pre-
diction with latent variables  and we formulate a convex-concave upper bound on
the tree’s empirical loss. Computing the gradient of the proposed surrogate ob-
jective with respect to each training exemplar is O(d2)  where d is the tree depth 
and thus training deep trees is feasible. The use of stochastic gradient descent for
optimization enables effective training with large datasets. Experiments on sev-
eral classiﬁcation benchmarks demonstrate that the resulting non-greedy decision
trees outperform greedy decision tree baselines.

1

Introduction

Decision trees and forests [5  21  4] have a long and rich history in machine learning [10  7]. Recent
years have seen an increase in their popularity  owing to their computational efﬁciency and applica-
bility to large-scale classiﬁcation and regression tasks. A case in point is Microsoft Kinect where
decision trees are trained on millions of exemplars to enable real-time human pose estimation from
depth images [22].
Conventional algorithms for decision tree induction are greedy. They grow a tree one node at a
time following procedures laid out decades ago by frameworks such as ID3 [21] and CART [5].
While recent work has proposed new objective functions to guide greedy algorithms [20  12]  it
continues to be the case that decision tree applications (e.g.  [9  14]) utilize the same dated methods
of tree induction. Greedy decision tree induction builds a binary tree via a recursive procedure as
follows: beginning with a single node  indexed by i  a split function si is optimized based on a
corresponding subset of the training data Di such that Di is split into two subsets  which in turn
deﬁne the training data for the two children of the node i. The intrinsic limitation of this procedure
is that the optimization of si is solely conditioned on Di  i.e.  there is no ability to ﬁne-tune the split
function si based on the results of training at lower levels of the tree. This paper proposes a general
framework for non-greedy learning of the split parameters for tree-based methods that addresses this
limitation. We focus on binary trees  while extension to N-ary trees is possible. We show that our
joint optimization of the split functions at different levels of the tree under a global objective not
only promotes cooperation between the split nodes to create more compact trees  but also leads to
better generalization performance.

∗Part of this work was done while M. Norouzi and M. D. Collins were at Microsoft Research  Cambridge.

1

One of the key contributions of this work is establishing a link between the decision tree optimiza-
tion problem and the problem of structured prediction with latent variables [25]. We present a novel
formulation of the decision tree learning that associates a binary latent decision variable with each
split node in the tree and uses such latent variables to formulate the tree’s empirical loss. Inspired
by advances in structured prediction [23  24  25]  we propose a convex-concave upper bound on the
empirical loss. This bound acts as a surrogate objective that is optimized using stochastic gradi-
ent descent (SGD) to ﬁnd a locally optimal conﬁguration of the split functions. One complication
introduced by this particular formulation is that the number of latent decision variables grows expo-
nentially with the tree depth d. As a consequence  each gradient update will have a complexity of
O(2dp) for p-dimensional inputs. One of our technical contributions is showing how this complexity
can be reduced to O(d2p) by modifying the surrogate objective  thereby enabling efﬁcient learning
of deep trees.

2 Related work

Finding optimal split functions at different levels of a decision tree according to some global ob-
jective  such as a regularized empirical risk  is NP-complete [11] due to the discrete and sequential
nature of the decisions in a tree. Thus  ﬁnding an efﬁcient alternative to the greedy approach has
remained a difﬁcult objective despite many prior attempts.
Bennett [1] proposes a non-greedy multi-linear programming based approach for global tree op-
timization and shows that the method produces trees that have higher classiﬁcation accuracy than
standard greedy trees. However  their method is limited to binary classiﬁcation with 0-1 loss and
has a high computation complexity  making it only applicable to trees with few nodes.
The work in [15] proposes a means for training decision forests in an online setting by incrementally
extending the trees as new data points are added. As opposed to a naive incremental growing of the
trees  this work models the decision trees with Mondrian Processes.
The Hierarchical Mixture of Experts model [13] uses soft splits rather than hard binary decisions to
capture situations where the transition from low to high response is gradual. The use of soft splits at
internal nodes of the tree yields a probabilistic model in which the log-likelihood is a smooth func-
tion of the unknown parameters. Hence  training based on log-likelihood is amenable to numerical
optimization via methods such as expectation maximization (EM). That said  the soft splits neces-
sitate the evaluation of all or most of the experts for each data point  so much of the computational
advantage of the decision tree are lost.
Murthy and Salzburg [17] argue that non-greedy tree learning methods that work by looking ahead
are unnecessary and sometimes harmful. This is understandable since their methods work by min-
imizing the empirical loss without any regularization  which is prone to overﬁtting. To avoid this
problem  it is a common practice (see Breiman [4] or Criminisi and Shotton [7] for an overview)
to limit the tree depth and introduce limits on the number of training instances below which a tree
branch is not extended  or to force a diverse ensemble of trees (i.e.  a decision forest) through the
use of bagging. Bennett and Blue [2] describe a different way to overcome overﬁtting by using
max-margin framework and the Support Vector Machines (SVM) at the split nodes of the tree. Sub-
sequently  Bennett et al. [3] show how enlarging the margin of decision tree classiﬁers results in
better generalization performance.
Our formulation for decision tree induction improves on prior art in a number of ways. Not only
does our latent variable formulation of decision trees enable efﬁcient learning  it can handle any
general loss function while not sacriﬁcing the O(dp) complexity of inference imparted by the tree
structure. Further  our surrogate objective provides a natural way to regularize the joint optimization
of tree parameters to discourage overﬁtting.

3 Problem formulation

For ease of exposition  this paper focuses on binary classiﬁcation trees  with m internal (split) nodes 
and m + 1 leaf (terminal) nodes. Note that in a binary tree the number of leaves is always one more
than the number of internal (non-leaf) nodes. An input  x ∈ Rp  is directed from the root of the
tree down through internal nodes to a leaf node. Each leaf node speciﬁes a distribution over k class
labels. Each internal node  indexed by i ∈ {1  . . .   m}  performs a binary test by evaluating a node-

2

h1
+1

h3
+1

h2
-1

h1
-1

h2
+1

h3
+1

θ1

θ2

θ3

θ4

θ1

θ2

θ3

θ4

f ([+1 −1  +1]T) = [0  0  0  1]T = 14

f ([−1  +1  +1]T) = [0  1  0  0]T = 12

θ = ΘTf (h) = θ4

θ = ΘTf (h) = θ2

Figure 1: The binary split decisions in a decision tree with m = 3 internal nodes can be thought as
a binary vector h = [h1  h2  h3]T. Tree navigation to reach a leaf can be expressed in terms of a
function f (h). The selected leaf parameters can be expressed by θ = ΘTf (h).
speciﬁc split function si(x) : Rp → {−1  +1}. If si(x) evaluates to −1  then x is directed to the
left child of node i. Otherwise  x is directed to the right child. And so on down the tree. Each split
function si(·)  by parameterized a weight vector wi  is assumed to be a linear threshold function 
i.e.  si(x) = sgn(wi
Tx). We incorporate an offset parameter to obtain split functions of the form
sgn(wi
Each leaf node  indexed by j ∈ {1  . . .   m + 1}  speciﬁes a conditional probability distribution over
class labels  l ∈ {1  . . .   k}  denoted p(y = l | j). Leaf distributions are parametrized with a vector
of unnormalized predictive log-probabilities  denoted θj ∈ Rk  and a softmax function; i.e. 

Tx − bi) by appending a constant “−1” to the input feature vector.

p(y = l | j) =

(cid:9)
exp(cid:8)θj[l]
α=1 exp(cid:8)θj[α]
(cid:80)k

(cid:9)  

(1)

where θj[α] denotes the αth element of vector θj.
The parameters of the tree comprise the m internal weight vectors  {wi}m
i=1  and the m + 1 vectors
of unnormalized log-probabilities  one for each leaf node  {θj}m+1
j=1 . We pack these parameters
into two matrices W ∈ Rm×p and Θ ∈ R(m+1)×k whose rows comprise weight vectors and leaf
parameters  i.e.  W ≡ [w1  . . .   wm]T and Θ ≡ [θ1  . . .   θm+1]T. Given a dataset of input-output
pairs  D ≡ {xz  yz}n
z=1  where yz ∈ {1  . . .   k} is the ground truth class label associated with
input xz ∈ Rp  we wish to ﬁnd a joint conﬁguration of oblique splits W and leaf parameters Θ
that minimize some measure of misclassiﬁcation loss on the training dataset. Joint optimization of
the split functions and leaf parameters according to a global objective is known to be extremely
challenging [11] due to the discrete and sequential nature of the splitting decisions within the tree.
One can evaluate all of the split functions  for every internal node of the tree  on input x by com-
puting sgn(W x)  where sgn(·) is the element-wise sign function. One key idea that helps linking
decision tree learning to latent structured prediction is to think of an m-bit vector of potential split
decisions  e.g.  h = sgn(W x) ∈ {−1  +1}m  as a latent variable. Such a latent variable determines
the leaf to which a data point is directed  and then classiﬁed using the leaf parameters. To formulate
the loss for (x  y)  we introduce a tree navigation function f : Hm → Im+1 that maps an m-bit
sequence of split decisions (Hm ≡ {−1  +1}m) to an indicator vector that speciﬁes a 1-of-(m + 1)
encoding. Such an indicator vector is only non-zero at the index of the selected leaf. Fig. 1 illustrates
the tree navigation function for a tree with 3 internal nodes.
Using the notation developed above  θ = ΘTf (sgn(W x)) represents the parameters corresponding
to the leaf to which x is directed by the split functions in W . A generic loss function of the form
(cid:96)(θ  y) measures the discrepancy between the model prediction based on θ and an output y. For
the softmax model given by (1)  a natural loss is the negative log probability of the correct label 
referred to as log loss 

(cid:96)(θ  y) = (cid:96)log(θ  y) = − θ[y] + log

exp(θ[β])

.

(2)

(cid:18) k(cid:88)

(cid:19)

β=1

3

For regression tasks  when y ∈ Rq  and the value of θ ∈ Rq is directly emitted as the model
prediction  a natural choice of (cid:96) is squared loss 

(3)
One can adopt other forms of loss within our decision tree learning framework as well. The goal of
learning is to ﬁnd W and Θ that minimize empirical loss  for a given training set D  that is 

(cid:96)(θ  y) = (cid:96)sqr(θ  y) = (cid:107)θ − y(cid:107)2 .

(cid:88)

(cid:96)(cid:0)ΘTf (sgn(W x))  y(cid:1) .

(4)

L(W  Θ;D) =

(x y)∈D

Direct global optimization of empirical loss L(W  Θ;D) with respect to W is challenging. It is a
discontinuous and piecewise-constant function of W . Furthermore  given an input x  the navigation
function f (·) yields a leaf parameter vector based on a sequence of binary tests  where the results of
the initial tests determine which subsequent tests are performed. It is not clear how this dependence
of binary tests should be formulated.

4 Decision trees and structured prediction
To overcome the intractability in the optimization of L  we develop a piecewise smooth upper bound
on empirical loss. Our upper bound is inspired by the formulation of structured prediction with latent
variables [25]. A key observation that links decision tree learning to structured prediction  is that
one can re-express sgn(W x) in terms of a latent variable h. That is 

sgn(W x) = argmax
h∈Hm

(hTW x) .

(5)

In this form  decision tree’s split functions implicitly map an input x to a binary vector h by max-
imizing a score function hTW x  the inner product of h and W x. One can re-express the score
function in terms of a more familiar form of a joint feature space on h and x  as wTφ(h  x)  where
φ(h  x) = vec (hxT)  and w = vec (W ). Previously  Norouzi and Fleet [19] used the same re-
formulation (5) of linear threshold functions to learn binary similarity preserving hash functions.
Given (5)  we re-express empirical loss as 
L(W  Θ;D) =

(cid:96)(ΘTf ((cid:98)h(x))  y)  

(cid:88)

This objective resembles the objective functions used in structured prediction  and since we do not

have a priori access to the ground truth split decisions (cid:98)h(x)  this problem is a form of structured

prediction with latent variables.

5 Upper bound on empirical loss

(cid:16)

(cid:17) − max

We develop an upper bound on loss for an input-output pair (x  y)  which takes the form 

(7)

h∈Hm

(hTW x) .

(cid:96)(ΘTf (sgn(W x))  y) ≤ max
g∈Hm

To validate the bound  ﬁrst note that the second term on the RHS is maximized by h = (cid:98)h(x) =
sgn(W x). Second  when g = (cid:98)h(x)  it is clear that the LHS equals the RHS. Finally  for all other
values of g  the RHS can only get larger than when g =(cid:98)h(x) because of the max operator. Hence 

gTW x + (cid:96)(ΘTf (g)  y)

the inequality holds. An algebraic proof of (7) is presented in the supplementary material.
In the context of structured prediction  the ﬁrst term of the upper bound  i.e.  the maximization over
g  is called loss-augmented inference  as it augments the inference problem  i.e.  the maximization
over h  with a loss term. Fortunately  the loss-augmented inference for our decision tree learning
formulation can be solved exactly  as discussed below.
It is also notable that the loss term on the LHS of (7) is invariant to the scale of W   but the upper
bound on the right side of (7) is not. As a consequence  as with binary SVM and margin-rescaling
formulations of structural SVM [24]  we introduce a regularizer on the norm of W when optimizing
the bound. To justify the regularizer  we discuss the effect of the scale of W on the bound.

4

(x y)∈D

(cid:98)h(x) = argmax

h∈Hm

where

(hTW x) .

(6)

Proposition 1. The upper bound on the loss becomes tighter as a constant multiple of W increases 
i.e.  for a > b > 0:

(cid:16)

max
g∈Hm

agTW x + (cid:96)(ΘTf (g)  y)

(cid:17) − max

h∈Hm

(cid:16)

(ahTW x) ≤

(cid:17) − max

h∈Hm

max
g∈Hm

bgTW x + (cid:96)(ΘTf (g)  y)

(bhTW x).

(8)

Proof. Please refer to the supplementary material for the proof.
In the limit  as the scale of W approach +∞  the loss term (cid:96)(ΘTf (g)  y) becomes negligible com-
pared to the score term gTW x. Thus  the solutions to loss-augmented inference and inference
problems become almost identical  except when an element of W x is very close to 0. Thus  even
though a larger (cid:107)W(cid:107) yields a tighter bound  it makes the bound approach the loss itself  and there-
fore becomes nearly piecewise-constant  which is hard to optimize. Based on Proposition 1  one
easy way to decrease the upper bound is to increase the norm of W   which does not affect the loss.
Our experiments indicate that a lower value of the loss can be achieved when the norm of W is
regularized. We therefore constrain the norm of W to obtain an objective with better generalization.
Since each row of W acts independently in a decision tree in the split functions  it is reasonable to
constrain the norm of each row independently. Summing over the bounds for different training pairs
and constraining the norm of rows of W   we obtain the following optimization problem  called the
surrogate objective:

minimize L(cid:48)(W  Θ;D) =

gTW x + (cid:96)(ΘTf (g)  y)

(hTW x)

(cid:17) − max

h∈Hm

(cid:19)

(9)

(cid:18)

(cid:88)

(cid:16)

(x y)∈D
s.t.

max
g∈Hm
(cid:107)wi(cid:107)2 ≤ ν

for all i ∈ {1  . . .   m}

where ν ∈ R+ is a regularization parameter and wi is the ith row of W . For all values of ν  we
have L(W  Θ;D) ≤ L(cid:48)(W  Θ;D). Instead of using the typical Lagrange form for regularization  we
employ hard constraints to enable sparse gradient updates of the rows of W   since the gradients for
most rows of W are zero at each step in training.

6 Optimizing the surrogate objective

Even though minimizing the surrogate objective of (9) entails a non-convex optimization 
L(cid:48)(W  Θ;D) is much better behaved than empirical loss in (4). L(cid:48)(W  Θ;D) is piecewise linear
and convex-concave in W   and the constraints on W deﬁne a convex set.
Loss-augmented inference. To evaluate and use the surrogate objective in (9) for optimization  we
must solve a loss-augmented inference problem to ﬁnd the binary code that maximizes the sum of

the score and loss terms: (cid:98)g(x) = argmax

g∈Hm

(cid:0) gTW x + (cid:96)(ΘTf (g)  y)(cid:1) .

(10)

An observation that makes this optimization tractable is that f (g) can only take on m+1 distinct
values  which correspond to terminating at one of the m+1 leaves of the tree and selecting a leaf
parameter from {θj}m+1

j=1 . Fortunately  for any leaf index j ∈ {1  . . .   m+1}  we can solve

(cid:0) gTW x + (cid:96)(θj  y)(cid:1)

argmax
g∈Hm

s. t.

f (g) = 1j  

(11)

efﬁciently. Note that if f (g) = 1j  then ΘTf (g) equals the jth row of Θ  i.e.  θj. To solve (11)
we need to set all of the binary bits in g corresponding to the path from the root to the leaf j to be
consistent with the path direction toward the leaf j. However  bits of g that do not appear on this path
have no effect on the output of f (g)  and all such bits should be set based on g[i] = sgn(wi
Tx) to
obtain maximum gTW x. Accordingly  we can essentially ignore the off-the-path bits by subtracting
sgn(W x)TW x from (11) to obtain 

(cid:0)gTW x + (cid:96)(θj  y)(cid:1) = argmax

(cid:16)(cid:0)g − sgn(W x)(cid:1)T

W x + (cid:96)(θj  y)

.

(12)

(cid:17)

argmax
g∈Hm

g∈Hm

5

Algorithm 1 Stochastic gradient descent (SGD) algorithm for non-greedy decision tree learning.
1: Initialize W (0) and Θ(0) using greedy procedure
2: for t = 0 to τ do
3:
4:
5:

(cid:8)gTW (t)x + (cid:96)(ΘTf (g)  y)(cid:9)

Sample a pair (x  y) uniformly at random from D

6: W (tmp) ← W (t) − η(cid:98)gxT + η(cid:98)hxT
(cid:46)(cid:13)(cid:13)W (tmp)

(cid:98)h ← sgn(W (t)x)
(cid:98)g ← argmaxg∈Hm
(cid:110)
(cid:111)
(cid:13)(cid:13)2
∂Θ (cid:96)(ΘTf ((cid:98)g)  y)(cid:12)(cid:12)Θ=Θ(t)

7:
8:
1 
9:
10: Θ(t+1) ← Θ(t) − η ∂
11: end for

for i = 1 to m do
i  . ← min

end for

W (t+1)

i  .

√

ν

W (tmp)

i  .

Note that sgn(W x)TW x is constant in g  and this subtraction zeros out all bits in g that are not on
the path to the leaf j. So  to solve (12)  we only need to consider the bits on the path to the leaf j for
which sgn(wi
Tx) is not consistent with the path direction. Using a single depth-ﬁrst search on the
decision tree  we can solve (11) for every j  and among those  we pick the one that maximizes (11).
The algorithm described above is O(mp) ⊆ O(2dp)  where d is the tree depth  and we require
a multiple of p for computing the inner product wix at each internal node i. This algorithm is
not efﬁcient for deep trees  especially as we need to perform loss-augmented inference once for
every stochastic gradient computation. In what follows  we develop an alternative more efﬁcient
formulation and algorithm with time complexity of O(d2p).
Fast loss-augmented inference. To motivate the fast loss-augmented inference algorithm  we for-
mulate a slightly different upper bound on the loss  i.e. 

(cid:0)gTW x + (cid:96)(ΘTf (g)  y)(cid:1) − max

(cid:0)hTW x(cid:1)  

h∈Hm

(13)

(cid:96)(ΘTf (sgn(W x))  y) ≤

max

g∈B1(sgn(W x))

where B1(sgn(W x)) denotes the Hamming ball of radius 1 around sgn(W x)  i.e.  B1(sgn(W x)) ≡
{g ∈ Hm | (cid:107)g − sgn(W x)(cid:107)H ≤ 1}  hence g ∈ B1(sgn(W x)) implies that g and sgn(W x) differ
in at most one bit. The proof of (13) is identical to the proof of (7). The key beneﬁt of this new
formulation is that loss-augmented inference with the new bound is computationally efﬁcient. Since

(cid:98)g and sgn(W x) differ in at most one bit  then f ((cid:98)g) can only take d + 1 distinct values. Thus we

need to evaluate (12) for at most d + 1 values of j  requiring a running time of O(d2p).
Stochastic gradient descent (SGD). One reasonable approach to minimizing (9) uses stochastic
gradient descent (SGD)  the steps of which are outlined in Alg 1. Here  η denotes the learning rate 
and τ is the number of optimization steps. Line 6 corresponds to a gradient update in W   which is
∂W hTW x = hxT. Line 8 performs projection back to the feasible region
supported by the fact that ∂
of W   and Line 10 updates Θ based on the gradient of loss. Our implementation modiﬁes Alg 1 by
adopting common SGD tricks  including the use of momentum and mini-batches.
Stable SGD (SSGD). Even though Alg 1 achieves good training and test accuracy relatively quickly 
we observe that after several gradient updates some of the leaves may end up not being assigned to
any data points and hence the full tree capacity is not exploited. We call such leaves inactive as
opposed to active leaves that are assigned to at least one training data point. An inactive leaf may
become active again  but this rarely happens given the form of gradient updates. To discourage
abrupt changes in the number of inactive leaves  we introduce a variant of SGD  in which the as-
signments of data points to leaves are ﬁxed for a number of gradient updates. Thus  the bound is
optimized with respect to a set of data point leaf assignment constraints. When the improvement in
the bound becomes negligible the leaf assignment variables are updated  followed by another round
of optimization of the bound. We call this algorithm Stable SGD (SSGD) because it changes the as-
signment of data points to leaves more conservatively than SGD. Let a(x) denote the 1-of-(m + 1)
encoding of the leaf to which a data point x should be assigned to. Then  each iteration of SSGD

6

SensIT

Connect4

Protein

MNIST

Figure 2: Test and training accuracy of a single tree as a function of tree depth for different methods.
Non-greedy trees achieve better test accuracy throughout different depths. Non-greedy exhibit less
vulnerability to overﬁtting.

with fast loss-augmented inference relies on the following upper bound on loss 
(cid:96)(ΘTf (sgn(W x))  y) ≤

(cid:0)gTW x + (cid:96)(ΘTf (g)  y)(cid:1) −

max

g∈B1(sgn(W x))

max

h∈Hm|f (h)=a(x)

(cid:0)hTW x(cid:1) .

(14)

One can easily verify that the RHS of (14) is larger than the RHS of (13)  hence the inequality.
Computational complexity. To analyze the computational complexity of each SGD step  we note

that Hamming distance between(cid:98)g (deﬁned in (10)) and (cid:98)h = sgn(W x) is bounded above by the
depth of the tree d. This is because only those elements of(cid:98)g corresponding to the path to a selected
leaf can differ from sgn(W x). Thus  for SGD the expression ((cid:98)g −(cid:98)h) xT needed for Line 6 of Alg 1
can be computed in O(dp)  if we know which bits of(cid:98)h and(cid:98)g differ. Accordingly  Lines 6 and 7

can be performed in O(dp). The computational bottleneck is the loss augmented inference in Line
5. When fast loss-augmented inference is performed in O(d2p) time  the total time complexity of
gradient update for both SGD and SSGD becomes O(d2p + k)  where k is the number of labels.

7 Experiments

Experiments are conducted on several benchmark datasets from LibSVM [6] for multi-class classi-
ﬁcation  namely SensIT  Connect4  Protein  and MNIST. We use the provided train; validation; test
sets when available. If such splits are not provided  we use a random 80%/20% split of the training
data for train; validation and a random 64%/16%/20% split for train; validation; test sets.
We compare our method for non-greedy learning of oblique trees with several greedy baselines 
including conventional axis-aligned trees based on information gain  OC1 oblique trees [17] that
use coordinate descent for optimization of the splits  and random oblique trees that select the best
split function from a set of randomly generated hyperplanes based on information gain. We also
compare with the results of CO2 [18]  which is a special case of our upper bound approach applied
greedily to trees of depth 1  one node at a time. Any base algorithm for learning decision trees can
be augmented by post-training pruning [16]  or building ensembles with bagging [4] or boosting [8].
However  the key differences between non-greedy trees and baseline greedy trees become most
apparent when analyzing individual trees. For a single tree the major determinant of accuracy is the
size of the tree  which we control by changing the maximum tree depth.
Fig. 2 depicts test and training accuracy for non-greedy trees and four other baselines as function of
tree depth. We evaluate trees of depth 6 up to 18 at depth intervals of 2. The hyper-parameters for
each method are tuned for each depth independently. While the absolute accuracy of our non-greedy
trees varies between datasets  a few key observations hold for all cases. First  we observe that non-

7

6101418Depth0.60.70.8Test accuracy6101418Depth0.50.60.70.86101418Depth0.40.50.60.76101418Depth0.50.60.70.80.96101418Depth0.50.60.70.80.91.0Training accuracy6101418Depth0.50.60.70.80.91.06101418Depth0.50.60.70.80.91.06101418Depth0.60.70.80.91.0Axis-alignedCO2Non-greedyRandomOC1s
e
v
a
e
l

e
v
i
t
c
a

.

m
u
N

4 000

3 000

2 000

1 000

4 000

3 000

2 000

1 000

4 000

3 000

2 000

1 000

0
100
103
Regularization parameter ν (log)

101

102

0
100
103
Regularization parameter ν (log)

102

101

0
100
103
Regularization parameter ν (log)

101

102

Tree depth d =10

Tree depth d =13

Tree depth d =16

Figure 3: The effect of ν on the structure of the trees trained by MNIST. A small value of ν prunes
the tree to use far fewer leaves than an axis-aligned baseline used for initialization (dotted line).

greedy trees achieve the best test performance across tree depths across multiple datasets. Secondly 
trees trained using our non-greedy approach seem to be less susceptible to overﬁtting and achieve
better generalization performance at various tree depths. As described below  we think that the norm
regularization provides a principled way to tune the tightness of the tree’s ﬁt to the training data.
Finally  the comparison between non-greedy and CO2 [18] trees concentrates on the non-greediness
of the algorithm  as it compares our method with its simpler variant  which is applied greedily one
node at a time. We ﬁnd that in most cases  the non-greedy optimization helps by improving upon
the results of CO2.
A key hyper-parameter of our method is the regularization
constant ν in (9)  which controls the tightness of the up-
per bound. With a small ν  the norm constraints force the
method to choose a W with a large margin at each inter-
nal node. The choice of ν is therefore closely related to the
generalization of the learned trees. As shown in Fig. 3  ν
also implicitly controls the degree of pruning of the leaves
of the tree during training. We train multiple trees for dif-
ferent values of ν ∈ {0.1  1  4  10  43  100}  and we pick
the value of ν that produces the tree with minimum valida-
tion error. We also tune the choice of the SGD learning rate 
η  in this step. This ν and η are used to build a tree using
the union of both the training and validation sets  which is
evaluated on the test set.
To build non-greedy trees  we initially build an axis-aligned tree with split functions that threshold
a single feature optimized using conventional procedures that maximize information gain. The axis-
aligned split is used to initialize a greedy variant of the tree training procedure called CO2 [18]. This
provides initial values for W and Θ for the non-greedy procedure.
Fig. 4 shows an empirical comparison of training time for SGD with loss-augmented inference
and fast loss-augmented inference. As expected  run-time of loss-augmented inference exhibits
exponential growth with deep trees whereas its fast variant is much more scalable. We expect to see
much larger speedup factors for larger datasets. Connect4 only has 55  000 training points.

time to execute
Figure 4:
1000 epochs of SGD on the Connect4
dataset using loss-agumented infer-
ence and its fast varient.

Total

8 Conclusion

We present a non-greedy method for learning decision trees  using stochastic gradient descent to
optimize an upper bound on the empirical loss of the tree’s predictions on the training set. Our model
poses the global training of decision trees in a well-characterized optimization framework. This
makes it simpler to pose extensions that could be considered in future work. Efﬁciency gains could
be achieved by learning sparse split functions via sparsity-inducing regularization on W . Further 
the core optimization problem permits applying the kernel trick to the linear split parameters W  
making our overall model applicable to learning higher-order split functions or training decision
trees on examples in arbitrary Reproducing Kernel Hilbert Spaces.
Acknowledgment. MN was ﬁnancially supported in part by a Google fellowship. DF was ﬁnan-
cially supported in part by NSERC Canada and the NCAP program of the CIFAR.

8

6101418Depth0300600900120015001800Training time (sec)Loss-aug infFast loss-aug infReferences
[1] K. P. Bennett. Global tree optimization: A non-greedy decision tree algorithm. Computing Science and

Statistics  pages 156–156  1994.

[2] K. P. Bennett and J.A. Blue. A support vector machine approach to decision trees.

In Department
of Mathematical Sciences Math Report No. 97-100  Rensselaer Polytechnic Institute  pages 2396–2401 
1997.

[3] K. P. Bennett  N. Cristianini  J. Shawe-Taylor  and D. Wu. Enlarging the margins in perceptron decision

trees. Machine Learning  41(3):295–313  2000.

[4] L. Breiman. Random forests. Machine Learning  45(1):5–32  2001.
[5] L. Breiman  J. Friedman  R. A. Olshen  and C. J. Stone. Classiﬁcation and regression trees. Chapman &

Hall/CRC  1984.

[6] C. C. Chang and C. J. Lin. LIBSVM: a library for support vector machines  2001.
[7] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer 

2013.

[8] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics 

pages 1189–1232  2001.

[9] J. Gall  A. Yao  N. Razavi  L. Van Gool  and V. Lempitsky. Hough forests for object detection  tracking 

and action recognition. IEEE Trans. PAMI  33(11):2188–2202  2011.

[10] T. Hastie  R. Tibshirani  and J. Friedman. The elements of statistical learning (Ed. 2). Springer  2009.
[11] L. Hyaﬁl and R. L. Rivest. Constructing optimal binary decision trees is NP-complete.

Information

Processing Letters  5(1):15–17  1976.

[12] J. Jancsary  S. Nowozin  and C. Rother. Loss-speciﬁc training of non-parametric image restoration mod-

els: A new state of the art. ECCV  2012.

[13] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural Comput. 

6(2):181–214  1994.

[14] E. Konukoglu  B. Glocker  D. Zikic  and A. Criminisi. Neighbourhood approximation forests. In Medical

Image Computing and Computer-Assisted Intervention–MICCAI 2012  pages 75–82. Springer  2012.

[15] B. Lakshminarayanan  D. M. Roy  and Y. H. Teh. Mondrian forests: Efﬁcient online random forests. In

Advances in Neural Information Processing Systems  pages 3140–3148  2014.

[16] J. Mingers. An empirical comparison of pruning methods for decision tree induction. Machine Learning 

4(2):227–243  1989.

[17] S. K. Murthy and S. L. Salzberg. On growing better decision trees from data. PhD thesis  John Hopkins

University  1995.

[18] M. Norouzi  M. D. Collins  D. J. Fleet  and P. Kohli. Co2 forest: Improved random forest by continuous

optimization of oblique splits. arXiv:1506.06155  2015.

[19] M. Norouzi and D. J. Fleet. Minimal Loss Hashing for Compact Binary Codes. ICML  2011.
[20] S. Nowozin. Improved information gain estimates for decision tree induction. ICML  2012.
[21] J. R. Quinlan. Induction of decision trees. Machine learning  1(1):81–106  1986.
[22] J. Shotton  R. Girshick  A. Fitzgibbon  T. Sharp  M. Cook  M. Finocchio  R. Moore  P. Kohli  A. Criminisi 
A. Kipman  et al. Efﬁcient human pose estimation from single depth images. IEEE Trans. PAMI  2013.

[23] B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. NIPS  2003.
[24] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning for interde-

pendent and structured output spaces. ICML  2004.

[25] C. N. J. Yu and T. Joachims. Learning structural SVMs with latent variables. ICML  2009.

9

,Viliam Lisy
Vojta Kovarik
Marc Lanctot
Branislav Bosansky
Mohammad Norouzi
Maxwell Collins
Matthew Johnson
David Fleet
Pushmeet Kohli
Rodrigo Nogueira
Kyunghyun Cho