2019,Sequential Experimental Design for Transductive Linear Bandits,In this paper we introduce the pure exploration transductive linear bandit problem: given a set of measurement vectors $\mathcal{X}\subset \mathbb{R}^d$  a set of items $\mathcal{Z}\subset \mathbb{R}^d$  a fixed confidence $\delta$  and an unknown vector $\theta^{\ast}\in \mathbb{R}^d$  the goal is to infer $\arg\max_{z\in \mathcal{Z}} z^\top\theta^\ast$ with probability $1-\delta$ by making as few sequentially chosen noisy measurements of the form $x^\top\theta^{\ast}$ as possible. When $\mathcal{X}=\mathcal{Z}$  this setting generalizes linear bandits  and when $\mathcal{X}$ is the standard basis vectors and $\mathcal{Z}\subset \{0 1\}^d$  combinatorial bandits. The transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example  in drug discovery the compounds and dosages $\mathcal{X}$ a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages $\mathcal{Z}$ that can be safely administered to patients in vivo. Alternatively  in recommender systems for books  the set of books $\mathcal{X}$ a user is queried about may be restricted to known best-sellers even though the goal might be to recommend more esoteric titles $\mathcal{Z}$. In this paper  we provide instance-dependent lower bounds for the transductive setting  an algorithm that matches these up to logarithmic factors  and an evaluation. In particular  we present the first non-asymptotic algorithm for linear bandits that nearly achieves the information-theoretic lower bound.,Sequential Experimental Design for Transductive

Linear Bandits

Tanner Fiez

Electrical & Computer Engineering

University of Washington

ﬁezt@uw.edu

Allen School of Computer Science & Engineering

Lalit Jain⇤

University of Washington
lalitj@cs.washington.edu

Allen School of Computer Science & Engineering

Kevin Jamieson

University of Washington

jamieson@cs.washington.edu

Lillian Ratliff

Electrical & Computer Engineering

University of Washington

ratlifﬂ@uw.edu

Abstract

In this paper we introduce the pure exploration transductive linear bandit problem:
given a set of measurement vectors X⇢ Rd  a set of items Z⇢ Rd  a ﬁxed
conﬁdence   and an unknown vector ✓⇤ 2 Rd  the goal is to infer argmaxz2Z z>✓⇤
with probability 1   by making as few sequentially chosen noisy measurements
of the form x>✓⇤ as possible. When X = Z  this setting generalizes linear bandits 
and when X is the standard basis vectors and Z⇢{ 0  1}d  combinatorial bandits.
The transductive setting naturally arises when the set of measurement vectors is
limited due to factors such as availability or cost. As an example  in drug discovery
the compounds and dosages X a practitioner may be willing to evaluate in the lab
in vitro due to cost or safety reasons may differ vastly from those compounds and
dosages Z that can be safely administered to patients in vivo. Alternatively  in
recommender systems for books  the set of books X a user is queried about may be
restricted to known best-sellers even though the goal might be to recommend more
esoteric titles Z. In this paper  we provide instance-dependent lower bounds for
the transductive setting  an algorithm that matches these up to logarithmic factors 
and an evaluation. In particular  we present the ﬁrst non-asymptotic algorithm for
linear bandits that nearly achieves the information-theoretic lower bound.

1

Introduction

In content recommendation or property optimization in the physical sciences  often there is a set of
items (e.g.  products to purchase  drugs) described by a set of feature vectors Z⇢ Rd  and the goal is
to ﬁnd the z 2Z that maximizes some response or property (e.g.  afﬁnity of user to the product  drug
combating disease). A natural model for these settings is to assume that there is an unknown vector
✓⇤ 2 Rd and the expected response to any item z 2Z   if evaluated  is equal to z>✓⇤. However 
we often cannot measure z>✓⇤ directly  but we may infer it transductively through some potentially
noisy probes. That is  given a ﬁnite set of probes X⇢ Rd we observe x>✓⇤ + ⌘ for any x 2X
where ⌘ is independent mean-zero  sub-Gaussian noise. Given a set of measurements {(xi  ri)}N
i=1(ri  x>i ✓)2 and then useb✓ as a
plug-in estimate for ✓⇤ to estimate the optimal z⇤ := argmaxz2Z z>✓⇤. However  the accuracy of
such a plug-in estimator depends critically on the number and choice of probes used to construct

one can construct the least squares estimatorb✓ = arg min✓PN

i=1

⇤Contribution shared equally among T. Fiez and L. Jain.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

b✓. Unfortunately  the optimal allocation of probes cannot be decided a priori: it must be chosen
sequentially and adapt to the observations in real-time to optimize the accuracy of the prediction.
If the probing vectors (arms) X are equal to the item vectors Z  this problem is known as pure
exploration for linear bandits which is considered in [21  30  31  33]. This naturally arises in content
recommendation  for example  if X = Z is a feature representation of songs  and ✓⇤ represents a
user’s music preferences  a music recommendation system can elicit the preference for a particular
song z 2Z directly by enqueuing it in the user’s playlist. However  often times there are constraints
on which items in Z can be shown to the user.
1. X⇢Z . Consider a whiskey bar with hundreds of whiskies ranging in price from dollars a shot to
hundreds of dollars. The bar tender may have an implicit feature representation of each whiskey 
the patron has an implicit preference vector ✓⇤  and the bar tender wants to select the affordable
whiskeys X⇢Z in a taste test to get an idea of the patron’s preferences before recommending
the expensive whiskies that optimize the patron’s preferences in Z.
2. Z⇢X . In drug discovery  thousands of compounds are evaluated in order to determine which
ones are effective at combating a disease. However  it may be that while Z is the set of compounds
and doses that are approved for medical use (e.g.  safe)  it may be advantageous to test even unsafe
compounds or dosages X such that XZ . Such unsafe X may aid in predicting the optimal
z⇤ 2Z because they provide more information about ✓⇤.
3. Z\X = ;. Consider a user shopping for a home among a set Z where each is parameterized by
a number of factors like distance to work  school quality  crime rate  etc. so that each z 2Z can
be described as a linear combination of the relevant factors described by X : z =Px2X ↵z xx 
where we may take each x 2X to simply be one-hot-encoded. The response x>✓⇤ + ⌘ reﬂects
the user’s preferences for the query x  a speciﬁc attribute of the house. Indeed  if all ↵z x 2{ 0  1}
this is known as pure exploration for combinatorial bandits [10  8]. That is  a house either has the
attribute  or not.

Given items Z  measurement probes X   a conﬁdence   and an unknown ✓⇤  this paper develops
algorithms to sequentially decide which measurements in X to take in order to minimize the number
of measurements necessary in order to determine z⇤ with high probability.
1.1 Contributions

Our goals are broadly to ﬁrst deﬁne the transductive bandit problem and then characterize the
instance-optimal sample complexity for this problem. Our contributions include the following.
1. In Section 2 we provide instance dependent lower bounds for the transductive bandit problem
that simultaneously generalize previous known lower bounds for linear bandits and combinatorial
bandits using standard arguments.

2. In Section 3 we give an algorithm (Algorithm 1) for transductive linear bandits and prove an
associated sample complexity result (Theorem 2). We show that the sample complexity we
obtain matches the lower bound up to logarithmic factors. This is the primary contribution of
the paper. Along the way  we discuss how rounding procedures can be used to improve upon the
computational complexity of this algorithm.

3. In Sections 4 and 5 we contrast our algorithm with previous work from a theoretical and empirical
perspective  respectively. Our experiments show that our theoretically superior algorithm is
empirically competitive with previous algorithms on a range of problem scenarios.

0 is a positive semideﬁnite matrix  and y 2 Rd is a vector  let kyk2

1.2 Notation
For each z 2Z deﬁne the gap of z  (z) = (z⇤  z)>✓⇤ and furthermore  min = minz6=z⇤
If A 2 Rd⇥d
induced semi-norm. Let X := { 2 R|X| :   0 Px2X x = 1} denote the set of probability
distributions on X . Taking S⇢Z to a subset of the arm set  we deﬁne two operators we deﬁne
Y(S) = {z  z0 : 8 z  z0 2S   z 6= z0} as the directions obtained from the differences between
each pair of arms and Y⇤(S) = {z⇤  z : 8 z 2S \ z⇤} as the directions obtained from the
differences between the optimal arm and each suboptimal arm. Finally  for an arbitrary set of vectors
V⇢ Rd  deﬁne ⇢(V) = min2 X
(Px2X xxx>)1. This quantity will be crucial in the
discussion of our sample complexity and it is motivated in Section 2.2

(z).
A := y>Ay denote the

maxv2V kvk2

2

2 Transductive Linear Bandits Problem
Consider known ﬁnite collections of d-dimensional vectors X⇢ Rd and Z⇢ Rd   known conﬁdence
 2 (0  1)  and unknown ✓⇤ 2 Rd. The objective is to identify z⇤ = argmaxz2Z z>✓⇤ with
probability at least 1  while taking as few measurements in X as possible. Formally  a transductive
linear bandits algorithm is described by a selection rule Xt 2X at each time t given the history
(Xs  Rs)s<t  stopping time ⌧ with respect to the ﬁltration Ft = (Xs  Rs)st  and recommendation
rulebz 2Z invoked at time ⌧ which is F⌧ -measurable. We assume that Xt is Ft1-measurable and
may use additional sources of randomness; in addition at each time t that Rt = X>t ✓⇤ + ⌘t where ⌘t
is independent  zero-mean  and 1-sub-Gaussian. Let P✓⇤  E✓⇤ denote the probability law of Rt|Ft1
for all t.
Deﬁnition 1. We say that an algorithm for a transductive bandit problem is -PAC for X  Z⇢ Rd if
for all ✓⇤ 2 Rd we have P✓⇤(bz = z⇤)  1  .

2.1 Optimal allocations
In this section we discuss a number of ways we can allocate a measurement budget to the different
arms. The following establishes a lower bound on the expected number of samples any -PAC
algorithm must take.
Theorem 1. Assume ⌘t
satisfy

iid⇠N (0  1) for all t. Then for any  2 (0  1)  any -PAC algorithm must

E✓⇤[⌧ ]  log(1/2.4) min
2 X

max

z2Z\{z⇤}

kz⇤  zk2

(Px2X xxx>)1

((z⇤  z)>✓⇤)2

.

kz⇤  zk2

((z⇤  z)>✓⇤)2

⇤ := argmin
2 X

max

z2Z\{z⇤}

((z⇤  z)>✓⇤)2

and ⇤ = max
Z\{z⇤}

(Px2X ⇤xxx>)1

(Px2X xxx>)1

This lower bound is proved in Appendix C using standard techniques and employs the transportation
inequality of [22]. It generalizes a previous lower bound in the setting of linear bandits [29] and
lower bounds in the combinatorial bandit literature [10].
Optimal static allocation. To demonstrate that this lower bound is tight  deﬁne
kz⇤  zk2

 
(1)
where ⇤ is the value of the lower bound and ⇤ is the allocation that achieves it. Suppose we
sample arm x 2X exactly 2b⇤xNc times where we assume2 N 2 N is sufﬁciently large so that
minx:x>0bxNc > 0. If N = d2 ⇤ log(|Z|/)e then as we will show shortly (Section 2.2)  the
least squares estimatorb✓ satisﬁes (z⇤  z)>b✓> 0 for all z 2Z \ z⇤ with probability at least 1  .
Thus  with probability at least 1    z⇤ is equal tobz = arg maxz2Z z>b✓ and the total number of
samples is bounded by 2N which is within 4 log(|Z|) of the lower bound. Unfortunately  of course 
the allocation ⇤ relies on knowledge of ✓⇤ (which determines z⇤) which is unknown a priori  and
thus this is not a realizable strategy.
Other static allocations. Short of ⇤ it is natural to consider allocations that arise from optimal
linear experimental design [27]. For the special case of X = Z it has been argued ad nauseam that a
G-optimal design  argmin2 X
(Px2X xxx>)1  is woefully loose since it does
not utilize the differences x  x0  x  x0 2X [25  30  33]. Also for the X = Z case  [34  30] have
proposed the static XY-allocation given as argmin2 X
(Px2X xxx>)1. In
[30] it is shown that no more than O( d
log(|X| log(1/min)/)) samples from each of these
2
allocations sufﬁce to identify the best arm. While the above discussion demonstrates that for every ✓⇤
there exists an optimal static allocation (that explicitly uses ✓⇤) nearly achieving the lower bound  any
static allocation with no prior knowledge of ✓⇤ can require a factor of d more samples than necessary.
Proposition 1. Let c  c0 be universal constants. For any > 0  d even  there exists sets X = Z⇢ Rd
and a set ⇥ ⇢ Rd  such that infA max✓2⇥ E✓[⌧ ]  cd log(1/)
where A is the set of all algorithms
that are -PAC for X  Z and take a static allocation of samples. On the other hand ⇤/c0  d + 1
for every choice of ✓⇤ 2 ⇥.

maxx x02X kx  x0k2

maxx2X  x6=x⇤ kxk2

min

2Such an assumption is avoided by a sophisticated rounding procedure that we will describe shortly.



3



This proposition indicates that it is necessary to devise an adaptive algorithm to obtain a instance-
optimal sample complexity. The proof of this proposition can be found in Appendix D.
Adaptive allocations. As suggested by the problem deﬁnition  our strategy is to adapt the allocation
over time  informed by the observations up to the current time. Speciﬁcally  our algorithm will
proceed in rounds where at round t  we perform an XY-allocation that is sufﬁcient to remove all arms
z 2Z that have gaps of at least 2(t1). We show that the total number of measurements accumulates
to ⇤ log(|Z|2/) times some additional logarithmic factors  nearly achieving the optimal allocation
as well as the lower bound. In Section 4  we review related procedures for the speciﬁc case of X = Z.
2.2 Review of Least Squares
Given a ﬁxed design xT = (xt)T

t=1  a natural
approach is to construct the ordinary-least squares (OLS) estimateb✓ = (PT
t=1 rtxt).
One can showb✓ is unbiased with covariance  (PT
t=1 xtx>t )1. Moreover  for any y 2 Rd  we
P⇣y>(✓⇤ b✓) qkyk2
t=1 xtx>t )12 log(1/)⌘  .
(2)
(PT

t=1 with each xt 2X and associated rewards (rt)T
t=1 xtx>t )1(PT

In particular  if we want this to hold for all y 2Y ⇤(Z)  we need to union bound over Z replacing 
with /|Z|. Let us now use this to analyze the procedure discussed above (in the discussion on the
optimal static allocation after Theorem 1) that gives an allocation matching the lower bound. With
the choice of N = d2 ⇤ log(|Z|/)e and the allocation 2b⇤xNc for each x 2X   we have for each
z 2Z \ z⇤ that with probability at least 1   

have3

(Px 2bN⇤xxxT c)12 log(|Z|/)  0

since for each y = z⇤  z 2Y ⇤(Z) we have

(z⇤  z)>b✓  (z⇤  z)>✓⇤ qkz⇤  zk2
⇤xxx>⌘1
2bN⇤xcxx>⌘1

y  y>⇣Xx2X

where the last inequality plugs in the value of N and the deﬁnition of ⇤. The fact that at most

y>⇣Xx2X
one z0 2Z can satisfy (z0  z)>b✓> 0 for all z 6= z0 2Z   and that z0 = z⇤ does  certiﬁes that
bz = arg maxz2Z z>b✓ is indeed the best arm with probability at least 1  . Note that equation (3)

provides the motivation for how the form of ⇤ is obtained. Rearranging  it is equivalent to 

y/N  ((z⇤  z)>✓⇤)2/(2 log(|Z|/)) 

(3)

N  2 log(|Z|/) max
Z\{z⇤}

kz⇤  zk2

(Px2X ⇤xxx>)1

((z⇤  z)>✓⇤)2

for all z 2Z \ { z⇤}

Thinking of the right hand side of the inequality as a function of   ⇤ is precisely chosen to minimize
this quantity and hence the sample complexity.

2.3 Rounding Procedures
We brieﬂy digress to address a technical issue. Given an allocation  and an arbitrary subset of
vectors Y  in general  drawing N samples xN := {x1  . . .   xN} at random from X according to the
t=1 xtx>t )1 (which appears in the width
distribution x may result in a design where maxy2Y kyk2
(PN
of the conﬁdence interval (2)) differs signiﬁcantly from maxy2Y kyk2
(Px2X xxx>)1/N. Naive
strategies for choosing xN will fail. We can not simply use an allocation of N x samples for any
speciﬁc x since this may not be an integer. Furthermore  greedily rounding N x to an allocation
bN xc or dN xe may result in fewer than necessary  or far more than N total samples if the support
of  is large. However  given ✏> 0  there are efﬁcient rounding procedures that produce (1 + ✏)
approximations as long as N is greater than some minimum number of samples r(✏). In short 
given  and a choice of N they return an allocation xN satisfying maxy2Y kyk2
i=1 xix>i )1 
(PN
(Px2X xxx>)1/N. Such a procedure with r(✏)  O(d/✏2) is described in
(1 + ✏) maxy2Y kyk2
3There is a technical issue of whether the set Z lies in the span of X which in general is necessary to obtain

unbiased estimates of (z⇤  z)>✓⇤. Throughout the following we assume that span(X ) = Rd.

4

Section B in the supplementary. In our experiments we use a rounding procedure from [27] that is
easier to implement with r(✏) = 2kk0/✏  (d(d + 1) + 2)/✏. In general  ✏ should be thought of as
a constant. The number of samples N we need to take in our algorithm will be signiﬁcantly larger
than r(✏)  so the impact of the rounding procedure is minimal. We provide details on this rounding
procedure in Section B of the supplementary (also see [30  Appendix C]).

3 Sequential Experimental Design for Transductive Linear Bandits

Our algorithm for the pure exploration transductive bandit is presented in Algorithm 1. The algorithm

maxz2bZt kz⇤zk2

t  the algorithm samples in such a way to remove all arms with gaps greater than 2(t1). Thus

proceeds in rounds  keeping track of the active arms bZt ✓Z in each round t. At the start of round
denoting St := {z 2Z :( z)  4 · 2t}  in round t we expect bZt ⇢S t.
As described above  if we knew ✓⇤  we would sample according to the optimal allocation
argmin2 X
(Px2X xxx>)1/((z⇤z)>✓⇤)2. However  if at the start of the round
we only have an upper bound on the gaps (z)  4 · 2t and do not know z⇤  we can use the tri-
angle inequality to obtain 4 maxz2bZt kz⇤  zk2
(Px2X xxxT )1
(Px2X xxxT )1.4 This mo-
and lower-bound the objective by (2t3)2 min2 X
tivates our choice of t and ⇢(Y(bZt)). Thus by the same logic used in Section 2.2  Nt =
d2(2t)2(1 + ✏)⇢(Y(bZt)) log(|Z|2/t)e samples should sufﬁce to guarantee that we can construct a
conﬁdence interval on each (z  z0)>✓⇤ for (z  z0) 2Y (bZt) of size at most 2t (with the |Z|2 in
rounding principle. Finally  this conﬁdence interval allows us to provably remove any arm z 2 bZt

the logarithm accounting for a union bound over arms). The (1 + ✏) accounts for slack from the

(Px2X xxx>)1  maxy2Y(bZt) kyk2

maxy2Y(bZt) kyk2

such that (z) > 2(t1) in round t.

t2

Algorithm 1: RAGE(X  Z ✏  r (·)  ): Randomized Adaptive Gap Elimination
Input: Arms X⇢ Rd  items Z⇢ Rd  rounding approximation factor ✏ with default value 1/10  function r(·)
giving minimum number of samples to obtain rounding approximation ✏  and conﬁdence level  2 (0  1).
Initialize: Let bZ1 Z   t 1
while |bZt| > 1 do
t 
⇤t arg min2 X
(Px2X xxx>)1
⇢(Y(bZt)) min2 X
(Px2X xxx>)1
Nt max⌃2(2t)2⇢(Y(bZt))(1 + ") log(|Z|2/t)⌥  r(✏) 
xNt ROUND(⇤t   Nt)
Pull arms x1  . . .   xNt and obtain rewards r1  . . .   rNt
j=1 xjx>j and bt :=PNt
Computeb✓t = A1
bZt+1 bZt \z 2 bZ|9 z0 2 bZ : kz0  zkA1
Output: bZt
Theorem 2. Assume that maxz2Z (z)  2. Then with probability at least 1  using an ✏-efﬁcient
rounding procedure  Algorithm 1 returns z⇤ and requires a worst-case sample complexity of

maxy2Y(bZt) kyk2
maxy2Y(bZt) kyk2
t bt using At :=PNt

t p2 log(|Z|2/t) < (z0  z)>b✓t 

t t + 1

j=1 xjrj

N 

blog2(4/min)cXt=1

max⌃2(2t)2⇢(Y(St))(1 + ✏) log(t2|Z|2/)⌥  r(✏) 

where St = {z 2Z :( z)  4· 2t}. In particular  ROUND can be chosen so that r(✏) = O(d/✏2).
Furthermore  N  c ⇤ log2(4/min) log(|Z|2 log2(4/min)2/) + r(✏) log2(4/min) for some
absolute constant c  in other words Algorithm 1 is instance optimal up to logarithmic factors.
We provide a proof of the sample complexity bound in Section A. The primary novelty in our analysis
is in quantifying the relationship between the algorithm sample complexity and the lower bound.

4 Where we recall for any subset S⇢Z  Y(S) := {z  z0 : z  z0 2S} and for an arbitrary subset V⇢ Rd

we have ⇢(V ) = min2 X

maxv2V kvk2

(Px2X xxx>)1.
5

3.1

Interpreting the sample complexity.

Up to logarithmic factors  Algorithm 1 matches the lower bound obtained in Theorem 1. However 
the term ⇢(Y(St)) may seem a bit mysterious. In this section we try to interpret this quantity in terms
of the geometry of X and Z.
Let conv(X[ X ) denote the convex hull of X[ X   and for any set Y⇢ Rd deﬁne the gauge of
Y 
In the case where Y is a singleton Y = {y}  (y) := Y is the gauge norm of y with respect to
conv(X[X )  a familiar quantity from convex analysis [28]. We can provide a natural upper bound
for ⇢(Y) in terms of the gauge.
Lemma 1. Let Y⇢ Rd. Then

Y = max{c > 0 : cY⇢ conv(X[ X )}.

(4)

max

y2Y kyk2

2/(max

x2X kxk2)  ⇢(Y)  d/2
Y .

In the case of a singleton Y = {y}  we can improve the upper bound to ⇢(Y)  1/(y)2.
The proof of this Lemma is in Appendix E. To see the potential for adaptive gains we focus on
the case of linear bandits where X = Z. Consider an example with X = {ei}d
i=1 [{ z0} for z0 =
(cos(↵)  sin(↵)  0 ···   0) where ↵ 2 [0  .1)  and ✓⇤ = e1. Note that min ⇡ 1  cos(↵) ⇡ ↵2/2.
Then S1 = X   and an easy computation shows Y(X ) is a constant bounded from zero. After
the ﬁrst round  all arms except e1 and z0 will be removed  so Y(St) = {e1  z0} for t  2  and
Y(St) ⇡ 1/ sin(↵) ⇡ 1/↵. Summing over all rounds  we see that this implies a sample complexity
of eO(d + 1/↵2) up to log factors  which is a signiﬁcant improvement over the static XY-allocation
sample complexity of eO(d/↵2).
4 Related Work
When X = Z = {e1 ···   ed}⇢ Rd is the set of standard basis vectors  the problem reduces
to that of the best-arm identiﬁcation problem for multi-armed bandits which has been extensively
studied [14  19  20  22  11]. In addition  pure exploration for combinatorial bandits where X =
{e1 ···   ed}⇢ Rd and Z⇢{ 0  1}d has also received a great deal of attention [10  8  12  9].
In the setting of linear bandits when X = Z  despite a great deal of work in the regret and contextual
settings [1  26  25  13]  there has been far less work on linear bandits for pure exploration. This
problem was ﬁrst introduced in [30] and since then  there have been a few other works on this topic 
[31  21  33] that we now discuss.
• Soare et al. [30] made the initial connections to G-optimal experimental design. That work provides
the ﬁrst passive algorithm with a sample complexity of O( d
log(|X|/) + d2). Note that the d2
2
comes from the minimum number of samples needed for an efﬁcient rounding procedure and thus
could be reduced to d using improved rounding procedures (see [2]). They also provide an adaptive
algorithm  XY-adaptive algorithm for linear bandits. Their algorithm is very similar to ours  with
two notable differences. Firstly  instead of using an efﬁcient rounding procedure  they use a greedy
iterative scheme to compute an optimal allocation. Secondly  their algorithm does not discard items
that are provably sub-optimal. As a result  their sample complexity (up to logarithmic factors)
scales as max{M⇤  ⇤} log(|X|/(min)) + d2 where M⇤ is deﬁned (informally) as the amount
of samples needed using a static allocation to remove all sub-optimal directions in Y(X ) \ Y⇤(X ).
• In Tao et al. [31]  the focus is on developing different estimators with the goal of removing the
constant term d2 in Soare et al.’s passive sample complexity. Instead of using a rounding procedure 
they use a different estimator than the OLS estimator ✓⇤. Note that the rounding procedure in [2]
and described in the supplementary could have been applied directly to Soare’s static allocation
algorithm giving the same sample complexity as the one obtained in [31]. They also provide an
adaptive algorithm ALBA  that achieves a sample complexity of O(Pd
i ) where i is the
i-th smallest gap of the vectors in X . It is easy to see that this sample complexity is not optimal:
imagine a situation in which the vectors of X with the (d  1)-smallest gaps are identical to
the vector x0 6= x?. Then we only need to pay once for the samples needed to remove x0  not

i=1 1/2

min

6

(d  1)-times. Finally  their algorithms do not compute the optimal allocation over differences of
vectors in X   but instead on X directly à la G-optimal design. We will see the inefﬁciency of this
strategy in the experiments.
• Karnin [21] provides an algorithm that uses repeated rounds (for probability ampliﬁcation) of ex-
ploration phases combined with veriﬁcation phases to provide an asymptotically optimal algorithm 
meaning when  ! 0 the sample complexity divided by log(1/) approaches ⇤. Though this
is a nice theoretical result  the algorithm is not practical; the exploration phase is simply a naïve
passive G-optimal design.

• In Xu et al. [33]  a fully adaptive algorithm called LinGapE inspired by the UGapE algorithm [15]
is proposed. Since LinGapE is fully adaptive  a conﬁdence bound allowing for dependence in the
samples is necessary and the authors employ the self-normalized bound of [1]. The algorithm
requires each arm to be pulled once - an undesirable characteristic of a linear bandit algorithm
since the structure of the problem allows for information to be obtained about arms that are not
pulled. A recent work [23]  extends this algorithm to generalized linear models where the expected
reward of pulling arm z reward is given by a non-linear link function of z>✓⇤.

Finally  we mention [34]  which considers transductive experimental design from a computational
and optimization perspective  and explores XY-allocation for arbitrary kernels.
5 Experiments

In this section  we present simulations for the linear bandit pure exploration problem and the general
transductive bandit problem. We compare our proposed algorithm with both adaptive and non-
adaptive strategies. The adaptive strategies are XY-Adaptive allocation from [30]  LinGapE from
[33]  and ALBA from [31]. The non-adaptive strategies are static XY-allocation  as described in
Section 2  and an oracle strategy that knows ✓⇤ and samples according to ⇤. We do not compare to
the algorithm given in [21] since it is primarily a theoretical contribution and in moderate-conﬁdence
regimes obtains only the non-adaptive sample complexity. We run each algorithm at a conﬁdence
level of  = 0.05. The empirical failure probability of each of the algorithms in the simulations is
zero. To compute the samples for RAGE  we ﬁrst used the Frank-Wolfe algorithm (with a precise
stopping condition in the supplementary) to ﬁnd t  and then the rounding procedure from [27] with
✏ = 1/10. Further implementation details of RAGE and discussion pertaining to the implementation
of the other algorithms can be found in the supplementary material in Section F. We remark here that
in our implementation of the XY-Adaptive allocation  we follow the experiments in [30] and allow
for provably suboptimal arms to be discarded (though this is not how the algorithm is written in their
paper). The resulting algorithm is then similar to our algorithm. Unless explicitly mentioned  noise
in the observations was generated from a standard normal distribution.
Linear bandits: benchmark example. The ﬁrst experiment we present has become a benchmark in
the linear bandit pure exploration literature since it was introduced in [30]. In this problem  X =
Z = {e1  . . .   ed  x0}⇢ Rd where ei is the i-th standard basis vector  x0 = cos(.01)e1 + sin(.01)e2 
and ✓⇤ = 2e1 so that x⇤ = x1. An efﬁcient sampling strategy for this problem needs to focus on
reducing uncertainty in the direction (x1  xd+1)  which can be achieved by focusing pulls on arm
x2 = e2 since it is most aligned with this direction.
The results for this experiment are shown in Fig. 1a. The RAGE algorithm performs competitively
with existing algorithms and the oracle allocation. The XY-Adaptive algorithm is similar to RAGE 
but with weaker theoretical guarantees  so naturally it performs nearly equivalently. We omit it from
the remaining experiments for this reason. The LinGapE algorithm performs well when the number
of dimensions and arms is small. However  as the number of arms grows  LinGapE suffers from a
worse dimension dependency in the conﬁdence interval. ALBA performs the worst of the recently
proposed algorithms and this is to be expected since it computes an allocation on the X set instead of
on the Y(X ) set. This example clearly highlights the gains of adaptive sampling over non-adaptive
allocations such as the static XY-allocation. However  since X is relatively small in this case  it
fails to tease out important differences between the algorithms that can greatly increase the sample
complexity. We construct examples to demonstrate these claims now.
Many arms with moderate gaps. In this example  for a given value of n  3  we construct a set of
arms X⇢ R2  where X = Z = {e1  cos(3⇡/4)e1 + sin(3⇡/4)e2}[{ cos(⇡/4 + i)e1 + sin(⇡/4 +
i=3 with i ⇠N (0  .09) for each i 2{ 3  . . .   n}. The parameter vector is ﬁxed to be ✓⇤ = e1
i)e2}n

7

(a) Benchmark

(b) Duplicate arms

(c) Uniform sphere

(d) Transductive

Figure 1

so that x1 is the optimal arm  x2 gives the most information to identify the optimal arm  and the
remaining arms roughly point in the same direction with an expected gap of  ⇡ 0.3.
In Fig. 1b  we show the results of the experiment as we increase the number of arms. The LinGapE
algorithm suffers from a linear scaling in the number of arms since it must sample each arm as an
initialization. An efﬁcient sampling strategy should focus energy on x2  and as it does so  it will gain
information about the arms that are nearly duplicates of each other  which is how RAGE performs.
Uniform distribution on a sphere. In this example  X = Z is sampled from a unit sphere of
dimension d = 9 centered at the origin. Following [31]  we select the two closest arms x  x0 2X
and let ✓⇤ = x. In Fig. 1c  we show the sample complexity of the algorithms as the number of arms
grows. The RAGE algorithm signiﬁcantly outperforms ALBA and this is primarily due to the fact that
ALBA computes a G-optimal design on the active vectors in each round instead of on the differences
between these vectors. Thus the ALBA sampling distribution can be focused on a very different set
of arms from the optimal one.
Transductive example. We now present a general transductive bandit example. Since the existing
algorithms in the linear bandit literature do not generalize to this problem  we compare with a static
XY-allocation on X  Y(Z) and an oracle XY-allocation on X  Y⇤(Z) that knows the optimal arm
and the gaps. We construct an example in Rd with d even where X = {e1  . . .   ed}. The set Z is
also chosen so |Z| = d  the ﬁrst d/2 vectors are given by z1  . . .   zd/2 = (e1  . . .   ed/2) and then
zd/2+j = cos(.1)ej + sin(.1)ej+d/2 for each j 2{ 1  . . .   d/2}. Take ✓⇤ = e1 so z1 is the optimal
arm. The results of this simulation are depicted in Fig. 1d. The RAGE algorithm signiﬁcantly
outperforms the static allocation and nearly matches the oracle allocation.
We now present examples motivated by real-world applications.
Multivariate testing example. In many experimental design settings  there are a series of D factors
that can be either in a set of N states  and the goal is to determine the treatment conﬁguration that has
the highest outcome for a given metric. As a concrete example in web page optimization  it is common
that the composition of an advertisement layout selection may consist of several choices such as an
image  background color  and keyword to display (e.g. [16])  and we seek to ﬁnd the combination
with the highest clickthrough rate. To formalize the problem  consider a webpage consisting of D
distinct slots and suppose that there are 2 content choices that can be presented in each slot. Let

8

(a) Experimental Design

(b) Yahoo Example

Figure 2

x>✓⇤ = ✓⇤0 + ↵1PD

j=1 ✓⇤j wj + ↵2PD

k=1PD

the set W = {1  1}D satisfying |W| = 2D encode each layout. We model the problem using a
factorial design (see  e.g.  [6]) including pairwise interaction features to generate a linear bandit
problem. Each layout is represented by an arm x 2X where X = Z ⇢ {1  1}1+D+D(D1)/2 and
|X| = 2D. The expected reward of any x 2X corresponding to a layout w 2W is given by

`=k+1 ✓⇤k `wkw` 

where ✓⇤0 is a common bias weight  ✓⇤j is a weight for the j–th slot  and ✓⇤j k is a weight for the
interaction between the content in the k–th and `–th slots. We also include known parameters ↵1 = 1
and ↵2 = 0.5 that control the strength of the ﬁrst and second order interactions respectively. The
weights of the parameter vector are drawn from a discrete uniform distribution with a range of
[0.3  0.3] and a granularity of 0.01. The results of this example are shown in Fig. 2a. The RAGE
algorithm performs close to the oracle on this example  while the sample complexity of the rest of the
algorithms grows as the number of arms and dimension of the problem goes up.
Click-through example. To conduct an experiment based on real data  we build a problem using
the Yahoo! Webscope Dataset R6A.5 The dataset contains user click log records for news articles
displayed uniformly at random on the Yahoo! front page between May 1st  2009 and May 10th  2009.
Each click log record consists of a binary outcome along with 6 features identifying the user and 6
features identifying the article.
To build a linear bandit problem from the dataset  we construct an arm set X = Z⇢ R36 by taking
the outer product of the user and article features for each click log record on May 1st  2009. We
then ﬁt a regularized least squares estimate using a regularization parameter of 0.01 to obtain ✓⇤. To
model binary rewards  we let the observed reward be generated by a draw of a Bernoulli random
variable with parameter x>✓⇤ for any arm selection x 2X . Since x>✓⇤ 2 (0  0.11) 8 x 2X   the
noise is bounded between [1  1]  which causes it to be 1-sub-Gaussian. We simulate the problem
with 40 arms including the arm with the maximum reward in the dataset and the remaining arms were
selected at random from the set of arms with gap at least 0.01 from the optimal arm so the problem
is not too hard. The experiment setup is similar to that from [33] for this dataset. The results are
presented in Fig. 2b. We see that the RAGE algorithm has good performance on this example based
on real world data.

6 Conclusion

In this paper we have proposed the problem of best-arm identiﬁcation for transductive linear bandits 
provided an algorithm  and matching upper and lower bounds. As a remark it is straightforward
to exit our algorithm early with an "-good arm. It still remains to develop anytime algorithms
for this problem  as has been done in pure exploration for multi-armed bandits [19] that do not
throw out samples. In addition  we suspect our algorithm actually matches the lower-bound and the
log(1/min) factor is unnecessary. Finally  it is possible that some of the ideas developed here extend
to the setting of regret and could be used to give instance based regret bounds for linear bandits [25].
We hope to explore connections to both the regret and ﬁxed budget settings in further works.

5https://webscope.sandbox.yahoo.com/

9

References
[1] Yasin Abbasi-Yadkori  Dávid Pál  and Csaba Szepesvári.

Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems  pages 2312–2320 
2011.

[2] Zeyuan Allen-Zhu  Yuanzhi Li  Aarti Singh  and Yining Wang. Near-optimal discrete optimiza-
tion for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174 
2017.

[3] Jean-Yves Audibert and Sébastien Bubeck. Best arm identiﬁcation in multi-armed bandits. In

Conference on Learning Theory  pages 41–53  2010.

[4] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3:397–422  2002.

[5] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31(3):167–175  2003.

[6] George EP Box  J Stuart Hunter  and William G Hunter. Statistics for experimenters. In Wiley

Series in Probability and Statistics. Wiley Hoboken  NJ  USA  2005.

[7] Sébastien Bubeck  Rémi Munos  and Gilles Stoltz. Pure exploration in multi-armed bandits
problems. In International Conference on Algorithmic Learning Theory  pages 23–37  2009.

[8] Tongyi Cao and Akshay Krishnamurthy. Disagreement-based combinatorial pure exploration:
Efﬁcient algorithms and an analysis with localization. arXiv preprint arXiv:1711.08018  2017.

[9] Lijie Chen  Anupam Gupta  and Jian Li. Pure exploration of multi-armed bandit under matroid

constraints. In Conference on Learning Theory  pages 647–669  2016.

[10] Lijie Chen  Anupam Gupta  Jian Li  Mingda Qiao  and Ruosong Wang. Nearly optimal sampling

algorithms for combinatorial pure exploration. arXiv preprint arXiv:1706.01081  2017.

[11] Lijie Chen and Jian Li. On the optimal sample complexity for best arm identiﬁcation. arXiv

preprint arXiv:1511.03774  2015.

[12] Shouyuan Chen  Tian Lin  Irwin King  Michael R Lyu  and Wei Chen. Combinatorial pure
exploration of multi-armed bandits. In Advances in Neural Information Processing Systems 
pages 379–387  2014.

[13] Varsha Dani  Thomas P Hayes  and Sham M Kakade. Stochastic linear optimization under

bandit feedback. 2008.

[14] Eyal Even-Dar  Shie Mannor  and Yishay Mansour. Action elimination and stopping conditions
for the multi-armed bandit and reinforcement learning problems. Journal of machine learning
research  7(Jun):1079–1105  2006.

[15] Victor Gabillon  Mohammad Ghavamzadeh  and Alessandro Lazaric. Best arm identiﬁcation:
A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence. In Advances in Neural Information
Processing Systems  pages 3212–3220  2012.

[16] Daniel N Hill  Houssam Nassif  Yi Liu  Anand Iyer  and SVN Vishwanathan. An efﬁcient bandit
algorithm for realtime multivariate optimization. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 1813–1821. ACM 
2017.

[17] Matthew Hoffman  Bobak Shahriari  and Nando Freitas. On correlation and budget constraints
in model-based bandit optimization with application to automatic machine learning. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics  pages 365–374  2014.

[18] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1) 

pages 427–435  2013.

10

[19] Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sébastien Bubeck. lil’ucb: An optimal
exploration algorithm for multi-armed bandits. In Conference on Learning Theory  pages
423–439  2014.

[20] Zohar Karnin  Tomer Koren  and Oren Somekh. Almost optimal exploration in multi-armed

bandits. In International Conference on Machine Learning  pages 1238–1246  2013.

[21] Zohar S Karnin. Veriﬁcation based solution for structured mab problems. In Advances in Neural

Information Processing Systems  pages 145–153  2016.

[22] Emilie Kaufmann  Olivier Cappé  and Aurélien Garivier. On the complexity of best-arm
identiﬁcation in multi-armed bandit models. Journal of Machine Learning Research  17:1–42 
2016.

[23] Abbas Kazerouni and Lawrence M Wein. Best arm identiﬁcation in generalized linear bandits.

arXiv preprint arXiv:1905.08224  2019.

[24] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian

Journal of Mathematics  12:363–366  1960.

[25] Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of ﬁnite-

armed linear bandits. arXiv preprint arXiv:1610.04491  2016.

[26] Lihong Li  Wei Chu  John Langford  and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web  pages 661–670. ACM  2010.

[27] Friedrich Pukelsheim. Optimal design of experiments. SIAM  2006.
[28] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press  2015.
[29] Marta Soare. Sequential resource allocation in linear stochastic bandits. PhD thesis  Université

Lille 1-Sciences et Technologies  2015.

[30] Marta Soare  Alessandro Lazaric  and Rémi Munos. Best-arm identiﬁcation in linear bandits.

In Advances in Neural Information Processing Systems  pages 828–836  2014.

[31] Chao Tao  Saúl Blanco  and Yuan Zhou. Best arm identiﬁcation in linear bandits with linear
dimension dependency. In International Conference on Machine Learning  pages 4884–4893 
2018.

[32] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics 

pages 135–166  2004.

[33] Liyuan Xu  Junya Honda  and Masashi Sugiyama. A fully adaptive algorithm for pure explo-
ration in linear bandits. In International Conference on Artiﬁcial Intelligence and Statistics 
pages 843–851  2018.

[34] Kai Yu  Jinbo Bi  and Volker Tresp. Active learning via transductive experimental design. In
Proceedings of the 23rd international conference on Machine learning  pages 1081–1088. ACM 
2006.

11

,Chen Dan
Liu Leqi
Bryon Aragam
Pradeep Ravikumar
Eric Xing
Tanner Fiez
Lalit Jain
Kevin Jamieson
Lillian Ratliff