2009,Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing,We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices  by performing convex variational inference on a large scale non-Gaussian linear dynamical system  tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a first study  designs are found that improve significantly on others chosen independently for each slice or drawn at random.,Speeding up Magnetic Resonance Image Acquisition

by Bayesian Multi-Slice Adaptive Compressed

Sensing

Matthias W. Seeger

Saarland University and Max Planck Institute for Informatics

Campus E1.4  66123 Saarbr¨ucken  Germany
mseeger@mmci.uni-saarland.de

Abstract

We show how to sequentially optimize magnetic resonance imaging measurement
designs over stacks of neighbouring image slices  by performing convex varia-
tional inference on a large scale non-Gaussian linear dynamical system  tracking
dominating directions of posterior covariance without imposing any factorization
constraints. Our approach can be scaled up to high-resolution images by reduc-
tions to numerical mathematics primitives and parallelization on several levels. In
a ﬁrst study  designs are found that improve signiﬁcantly on others chosen inde-
pendently for each slice or drawn at random.

1

Introduction

Magnetic resonance imaging (MRI) [10  6] is a very ﬂexible imaging modality. Inﬂicting no harm
on patients  it is used for an ever-growing number of diagnoses in health-care. Its most serious
limitation is acquisition speed  being based on a serial idea (gradient encoding) with limited scope
for parallelization. Fourier (aka. k-space) coefﬁcients are sampled along smooth trajectories (phase
encodes)  many of which are needed for reconstructions of sufﬁcient quality [17  1]. Long scan
times lead to patient annoyance  grave errors due to movement  and high running costs. The Nyquist
sampling theorem [2] fundamentally limits traditional linear image reconstruction  but with modern
3D MRI scenarios  dense sampling is not practical anymore. Acquisition is accelerated to some
extent in parallel MRI1  by using receive coil arrays [19  9]:
the sensitivity proﬁles of different
coils provide part of the localization normally done by more phase steps. A different idea is to use
(nonlinear) sparse image reconstruction  with which the Nyquist limit can be undercut robustly for
images  emphasized recently as compressed sensing [5  3]. While sparse reconstruction has been
used for MRI [28  12]  we address the more fundamental question of how to optimize the sampling
design for sparse reconstruction over a speciﬁc real-world signal class (MR images) in an adaptive
manner  avoiding strong assumptions such as exact  randomly distributed sparsity that do not hold
for real images [23]. Our approach is in line with recent endeavours to extend MRI capabilities
and reduce its cost  by complementing expensive  serial hardware with easily parallelizable digital
computations.
We extend the framework of [24]  the ﬁrst approximate Bayesian method for MRI sampling opti-
mization applicable at resolutions of clinical interest. Their approach falls short of real MRI practice
on a number of points. They considered single image slices only  while stacks2 of neighbouring

1While parallel MRI is becoming the standard  its use is not straightforward. The sensitivity maps are

unknown up front  depend partly on what is scanned  and their reliable estimation can be difﬁcult.

2“Stack-of-slices” acquisition along the z axis works by transmitting a narrow-band excitation pulse while
applying a magnetic ﬁeld gradient linear in z. If the echo time (between excitation and readout) is shorter than

1

slices are typically acquired. Reconstruction can be improved signiﬁcantly by taking the strong
statistical dependence between pixels of nearby slices into account [14  26  18]. Design optimiza-
tion is a joint problem as well: using the same acquisition pattern for neighbouring slices is clearly
redundant. Second  the latent image was modelled as real-valued in [24]  while in reality it is a
complex-valued signal. To our knowledge  the few directly comparable approaches rely on “trial-
and-error” exploration [12  16  27]  requiring substantially more human expert interventions and real
MRI measurements  whose high costs our goal-directed method aims to minimize.
Our extension to stacks of slices requires new technology. Global Gaussian covariances have to
be approximated  a straightforward extension of which to many slices is out of the question. We
show how to use approximate Kalman smoothing  implementing message passing by the Lanczos
algorithm  which has not been done in machine learning before (see [20  25] for similar proposals
to oceanography problems). Our technique is complementary to mean ﬁeld variational inference
approximations (“variational Bayes”)  where most correlations are ruled out a priori. We track
the dominating posterior covariance directions inside our method  allowing them to change during
optimization. While our double loop approach may be technically more demanding to implement 
relaxation as well as algorithm are characterized much better (convex problem; algorithm reducing to
standard computational primitives)  running orders of magnitude faster. Beyond MRI  applications
could be to Bayesian inference over video streams  or to computational photography [11]. Our
approach is parallelizable on several levels. This property is essential to even start projecting such
applications: on the scale demanded by modern MRI applications  with practitioners being used to
view images directly after acquisition  little else but highly parallelizable approaches are viable.
Large scale variational inference is reviewed and extended to complex-valued data in Section 2 
lifted to non-Gaussian linear dynamical systems in Section 3  and the experimental design extension
is given in Section 4. Results of a preliminary study on data from a Siemens 3T scanner are provided
in Section 5  using a serial implementation.

2 Large Scale Sparse Inference

Our motivation is to improve MR image reconstruction  not by ﬁnding a better estimation technique 
but by sampling data more economically. A latent MR image slice u ∈ Cn (n pixels) is measured
by a design matrix X ∈ Cm×n: y = Xu + ε (ε ∼ N(0  σ2I) models noise). For Cartesian
MRI  X = IS ·Fn  Fn the 2D fast Fourier transform  S ⊂ {1  . . .   n} the sampling pattern (which
partitions into complete columns or rows: phase encodes  the atomic units of the design). Sparse
reconstruction works by encoding super-Gaussian image statistics in a non-Gaussian prior  then
ﬁnding the posterior mode (MAP estimation): a convex quadratic program for the model employed
here. To improve the measurement design X itself  posterior information beyond (and independent
of) its mode is required  chieﬂy posterior covariances.
We brieﬂy review [24]  extending it to complex-valued u. The super-Gaussian image prior P (u) is
adapted by placing potentials on absolute values |sj|  the posterior has the form

P (u|y) ∝ N(y|Xu  σ2I)(cid:89)q

e−τj|sj /σ| 

s = Bu ∈ Cq.

j=1

Here  B is a sparsity transform [24]. We use the C → R2 embedding  s = (sj)  sj ∈ R2 
and norm potentials e−τj(cid:107)sj /σ(cid:107). Two main ideas lead to [24]. First  inference is relaxed to an
optimization problem by lower-bounding the log partition function [7] (intuitively  each Laplace
potential e−τj(cid:107)sj /σ(cid:107) is lower-bounded by a Gaussian-form potential of variance γj > 0)  leading to

φ(γ) = log |A| + h(γ) + minu R(u  γ)  R := σ−2(cid:0)(cid:107)y − Xu(cid:107)2 + sT Γ−1s(cid:1)   γ = (γj)  (1)

h(γ) = (τ 2)T γ. This procedure implies a Gaussian approximation Q(u|y) = N(u|u∗  σ2A−1)
to P (u|y)  with A = X H X + BT Γ−1B and u∗ = u∗(γ). The complex extension is formally
similar to [24] (π there is γ−1 here): Γ := (diag γ)⊗I2 = diag(γ1  γ1  γ2  . . . )T   B := Borig⊗I2 
Borig the real-valued sparsity transform. Q(u|y) is ﬁtted to P (u|y) by minγ(cid:31)0 φ: a convex problem
[24]. Used within an automatic decision architecture  convexity and robustness of inference become
assets that are more important than smaller bias after a lot of human expert attention.

the repeat time (between phase encodes)  several slices are acquired in an interleaved fashion  separated by
slice gaps to avoid crosstalk [17].

2

Second  φ(γ) can be minimized very efﬁciently by a double loop algorithm [24]. The compu-
tationally intensive log |A| term is concave in γ−1. Upper-bounding it tangentially by the afﬁne
zT (γ−1) − g∗(z) at outer loop (OL) update points  the resulting φz ≥ φ decouples and is mini-
mized much more efﬁciently in inner loops (ILs). minγ(cid:31)0 φz leaves us with

(cid:110)
φz(u) = σ−2(cid:107)y − Xu(cid:107)2 + 2(cid:88)

(cid:111)
j (|sj|)
h∗

j

 

min
u

j (|sj|) := τj(zj + (|sj|/σ)2)1/2 
h∗

additional Laplace coupling potentials(cid:81)n

(2)
a penalized least squares problem. At convergence  u∗ = EQ[u|y]  γj ← (zj + |s∗ j/σ|2)1/2/τj.
We can use iteratively reweighted least squares (IRLS)  each step of which needs a linear sys-
tem to be solved of the structure of A. Reﬁtting z (OL updates) is much harder: z ← (I ⊗
1T ) diag−1(BA−1BT ) = (I ⊗ 1T )(σ−2VarQ[sj|y]). In terms of Gaussian (Markov) random
ﬁelds  the inner optimization needs posterior mean computations only  while OL updates require
bulk Gaussian variances [21  15]. The reason why the double loop algorithm is much faster than
previous approaches is that only few variance computations are required. The extension to complex-
valued u is non-trivial only when it comes to IRLS search direction computations (see Appendix).
Given multi-slice data (Xt  yt)  t = 1  . . .   T   we can use an undirected hidden Markov model
over image slices u = (ut) ∈ CnT . By the stack-of-slices methodology  the likelihood poten-
tials P (yt|ut) are independent  and P (ut) from above serves as single-node potential  based on
st = But. If st→ := ut − ut+1  the dependence between neighbouring slices is captured by
i=1 e−τc i|(st→)i/σ|. The variational parameters γt at each
node are complemented by coupling parameters γt→ ∈ Rn
+. The Gaussian Q(u|y)  y = (yt) 
has the same form as above with a huge A ∈ CnT×nT . Inheriting the Markov structure  it is a
Gaussian linear dynamical system (LDS) with very high-dimensional states. How will an efﬁcient
extension of the double loop algorithm look like? The IL criterion φz should be coupled between
neighbouring slices  by way of potentials on st→. OL updates are more difﬁcult to lift: we have to
approximate marginal variances in a Gaussian LDS. We will do this by Kalman smoothing  approx-
imating inversion in message computations (conversion from natural to moment parameters) by the
Lanczos algorithm.
The central role of Gaussian covariance for approximating non-Gaussian posteriors has not been
emphasized much in machine learning  where if Bayesian computations are intractable  simpler
“variational Bayesian” concepts are routinely used  imposing factorization constraints on the poste-
rior up front. While such constraints can be adjusted in light of the data  this is difﬁcult and typically
not done. Factorization assumptions are a double-edged sword: they radically simplify implemen-
tations  but result in non-convex algorithms  and half of the problem is left undone. Our approach
offers an alternative: by using Lanczos on Q(u|y)  we retain precisely the maximum-covariance
directions of intermediate ﬁts to the posterior  without running into combinatorial or non-convex
problems. Finally  we place more varied sparsity penalties on the in-plane dimensions [24] than on
the third one. This is justiﬁed by voxels typically being larger and spaced with a gap in the third
dimension  with partial volume effects reducing sparsity. Moreover  a non-local sparsity transform
along the third dimension would destroy the Markovian structure essential for efﬁcient computation.

3 Approximate Inference over Multiple Slices

We aim to extend the single slice method of [24] to the hidden Markov extension  thereby reusing
code whenever possible. The variational criterion is (1) with

ht(γt) + I{t<T}ht→(γt→)  R =(cid:88)

h(γ) =(cid:88)
Rt = σ−2(cid:0)(cid:107)yt − Xtut(cid:107)2 + sT

(cid:1)   Rt→ = σ−2sT

t Γ−1
t st

t

t

Rt + I{t<T}Rt→  Γt→ := (diag γt→) ⊗ I2 

t→Γ−1

t→st→.

The coupling term log |A| is upper-bounded (φ ≤ φz)  so that the IL criterion φz(u) is the sum
of terms φt zt(ut)  φt→ zt→(st→). Problems of the form minu φz  jointly convex with couplings
between neighbours  are routinely addressed in parallel convex optimization. In order to update ut 
we consider its neighbours ut−1  ut+1 ﬁxed  massaging φt zt(ut) + φ(t−1)→ z(t−1)→(s(t−1)→) +
t   (ut − ut−1)T   (ut − ut+1)T )T  
φt→ zt→(st→) into the form of [24]: ˜B = (BT   I  I)T   ˜s = (sT
˜u = ut. These updates can be run asynchronously in parallel  sending ut to neighbours after every
few IRLS steps.

3

For OL updates  we have to compute zt = σ−2(I ⊗ 1T )VarQ[st|y] and zt→ = σ−2(I ⊗
1T )VarQ[st→|y]  where Q(u|y) is a Gaussian LDS (ﬁxed γ). To output a global criterion
value  an estimate of log |A| is required as well. We use the two-ﬁlter Kalman information
smoother  which entails passing Gaussian-form messages along the chain in both directions. Once
all messages are available  marginal (co)variances are computed at each node in parallel. Shift
Q(u|y) to zero mean (EQ[u|y] = u∗ is found in the IL). Denoting N U (A) = N U (u|A) :=
e−(1/2)σ−2uT Au  Q(u|y) consists of single node potentials Φt(ut) = N U (At) and pair po-
tentials Φt→(st→) = N U (Γ−1
t B. Deﬁning messages
Mt→(ut) = N U ( ˜At→)  M←t(ut) = N U ( ˜A←t)  the usual message propagation equation is

Mt→(ut) ∝(cid:82) M(t−1)→(ut−1)Φ(t−1)→(s(t−1)→)dut−1Φt(ut)  so that

t Xt + BT Γ−1

t→)  where At

:= X H

˜At→ = At + M( ˜A(t−1)→  Γ(t−1)→)  M( ˜A  Γ) := Γ−1 − Γ−1( ˜A + Γ−1)−1Γ−1.

(3)
In the same way  ˜A←t = At + M( ˜A←(t+1)  Γt→). Denote Mt→ := M( ˜At→  Γt→)  M←t :=
M( ˜A←t  Γ(t−1)→). Once all messages have been computed  the node marginal Q(ut|y) has
If Ψ := (δ1 − δ2) ⊗ I  the precision
precision matrix ˜At := At + M(t−1)→ + M←(t+1).
matrix of Q(ut  ut+1|y) is diag( ˜At→  ˜A←(t+1)) + ΨΓ−1
t→ΨT   and st→ = ΨT (uT
t+1)T .
−1
t+1 and Mt→. Finally  by tracking normalization con-
CovQ[st→|y] can be written in terms of ˜A
t<˜t log | ˜At→ + Γ−1
(t−1)→| + log | ˜A˜t| for any ˜t. In

t>˜t log | ˜A←t + Γ−1
practice  we average over ˜t. The algorithm is sketched in Algorithm 1.

stants: log |A| =(cid:80)

t→| +(cid:80)

t   uT

Algorithm 1 Double loop variational inference algorithm

repeat

if ﬁrst iteration then

else

Default-initialize z ∝ 1  u = 0.
Run Kalman smoothing to determine Mt→  and (in parallel) M←t.
Determine node variances zt  pair variances zt→  and log |A| from messages. Reﬁt upper
bound φz to φ (tangent at γ). Initialize u = u∗ (previous solution).

end if
repeat

Distributed IRLS to minimize minγ φz w.r.t. u.
Each local update of ut entails solving a linear system (conjugate gradients).

until u∗ = argminu φz converged
Update γj = (zj + |s∗ j/σ|2)1/2/τj.

until outer loop converged

For reconstruction  we run parallel MAP estimation. Following [12]  we smooth out the nondiffer-
entiable l1 penalty by |sj/σ| ≈ (ε + |sj/σ|2)1/2 for very small ε > 0  then use nonlinear conjugate
gradients with Armijo line search. Nodes return with ∇utφz at the line minimum ut  the next search
direction is centrally determined and distributed (just a scalar has to be transferred). This is not the
same as centralized CG: line searches are distributed and not done on the global criterion.
We brieﬂy comment on how to approximate Kalman message passing by way of the Lanczos algo-
rithm [8]  full details are given in [22]. Gaussian (Markov) random ﬁeld practitioners will appre-
ciate the difﬁculties: there is no locally connected MRF structure  and the Q(u|y) are highly non-
stationary  being ﬁtted to a posterior with non-Gaussian statistics (edges in the image  etc). Message
passing requires the inversion of a precision matrix A. The idea behind Lanczos approximations is
PCA: if A ≈ UΛU T   Λ the l (cid:28) n smallest eigenvalues  UΛ−1U T is the PCA approximation of
A−1. With matrices A of certain spectral decay  this representation can be approximated by Lanc-
zos (see [24  22] for details). For a low rank PCA approximation of ˜At→  Mt→ has the same rank
(see Appendix)  which allows to run Gaussian message passing tractably. In a parallel implementa-
tion  the forward and backward ﬁlter passes run in parallel  passing low rank messages (the rank km
of these should be smaller than the rank kc for subsequent marginal covariance computations). On
a lower level  both matrix-vector multiplications with Xt (FFT) and reorthogonalizations required
during the Lanczos algorithm can easily be parallelized on commodity graphics hardware.

4

4 Sampling Optimization by Bayesian Experimental Design

With our multi-slice variational inference algorithm in place  we address sampling optimization
by Bayesian sequential experimental design  following [24]. At slice t  the information gain score
∆(X∗) := log |I +X∗CovQ[ut|y]X T∗ | is computed for a ﬁxed number of phase encode candidates
X∗ ∈ Cd×n not yet in Xt  the score maximizer is appended  and a novel measurement is acquired
(for the maximizer only). ∆(X∗) depends primarily on the marginal posterior covariance matrix
CovQ[ut|y]  computed by Gaussian message passing just as variances in OL updates above (while a
single value ∆(X∗) can be estimated more efﬁciently  the dominating eigendirections of the global
covariance matrix seem necessary to approximate many score values for different candidates X∗).
Once messages have been passed  scores can be computed in parallel at different nodes. A purely
sequential approach  extending one design Xt by one encode in each round  is not tractable. In
practice  we extend several node designs Xt in each round (a ﬁxed subset Cit ⊂ {1  . . .   T}; “it” the
round number). Typically  Cit repeats cyclically. This is approximate  since candidates are scored
independently at each node. Certainly  Cit should not contain neighbouring nodes. In the interleaved
stack-of-slices methodology  scan time is determined by the largest factor Xt (number of rows)  so
we strive for balanced designs here.
To sum up  our adaptive design optimization algorithm starts with an initial variational inference
phase for a start-up design (low frequencies only)  then runs through a ﬁxed number of design
rounds. Each round starts with Gaussian message passing  based on which scores are computed at
nodes t ∈ Cit  new measurements are acquired  and designs Xt are extended. Finally  variational
inference is run for the extended model  using a small number of OL iterations (only one in our
experiments). Time can be saved by basing the ﬁrst OL update on the same messages and node
marginal covariances than the design score computations (neglecting their change through new phase
encodes).

5 Experiments

a further term(cid:81)

We present experimental results  comparing designs found by our Bayesian joint design optimization
method against alternative choices on real MRI data. We use the model of Section 2  with the
prior previously used in [24] (potentials of strength τa on wavelet coefﬁcients  of strength τr on
Cartesian ﬁnite differences). While the MRI signal u is complex-valued  phase contributions are
mostly erroneous  and reconstruction as well as design optimization are improved by multiplying
i e−(τi/σ)|(cid:61)(ui)| into each single node prior potential  easily incorporated into the
generic setup by appending I ⊗ δT
2 to B. We focus on Cartesian MRI (phase encodes are complete
columns3 in k-space): a more clinically relevant setting than spiral sampling treated in [24].
We use data of resolution 64×64 (in-plane) to test our approach with a serial implementation. While
this is not a resolution of clinical relevance  a truly parallel implementation is required in order to
run our method at resolutions 256 × 256 or beyond: an important point for future work.

5.1 Quality of Lanczos Variance Approximations

We begin with experiments to analyze the errors in Lanczos variance approximations. Recall from
[24] that variances are underestimated. We work with a single slice of resolution 64 × 64  using
a design X of 30 phase encodes  running a single common OL iteration (default-initialized z) 
comparing different ways of continuing from there: exact z computations (Cholesky decomposition
of A) versus Lanczos approximations with different numbers of steps k. Results are in Figure 1.
While the relative approximation errors are rather large uniformly  there is a clear structure to them:
the largest (and also the very smallest) true values zj are approximated signiﬁcantly more accurately
than smaller true values. This structure can be used to motivate why  in the presence of large errors
over all coefﬁcients  our inference still works well for sparse linear models  indeed in some cases bet-
ter than if exact computations are used (Figure 1  upper right). The spectrum of A shows a roughly
linear decay  so that the largest and smallest eigenvalues (and eigenvectors) are well-approximated

3Our data are sagittal head scans  where the frequency encode direction (along which oversampling is

possible at no extra cost) is typically chosen vertically (the longer anatomic axis).

5

Figure 1: Lanczos approximations of Gaussian variances  at beginning of second OL iteration  64 × 64
data (upper left). Spectral decay of inverse covariance matrix A roughly linear (upper middle).
l2 recon-
struction error of posterior mean estimate after subsequent OL iterations  for exact variance computation vs.
k = 250  500  750  1500 Lanczos steps (upper right). Lower panel: Relative accuracy zj (cid:55)→ zk j/zj at be-
ginning of second OL iteration  separately for “a” sites (on wavelet coefﬁcients; red)  “r” sites (on derivatives;
blue)  and “i” sites (on (cid:61)(u); green).

by Lanczos  while the middle part of the spectrum is not penetrated. Contributions to the largest
values zj come dominatingly from small eigenvalues (large eigenvalues of A−1)  explaining their
smaller relative error. On the other hand  smaller values zj are strongly underestimated (zk j (cid:28) zj) 
which means that the selective shrinkage effect underlying sparse linear models (shrink most co-
efﬁcients strongly  but some not at all) is strengthened by these systematic errors. Finally  the IL
penalties are τj(zj + |sj/σ|2)1/2  enforcing sparsity more strongly for smaller zj. Therefore  Lanc-
zos approximation errors lead to strengthened sparsity in subsequent ILs  but least so for sites with
largest true zj. As an educated guess  this effect might even compensate for the fact that Laplace
potentials may not be sparse enough for natural images.

Joint Design Optimization

5.2
We use sagittal head scan data of resolution 64 × 64 in-plane  32 slices  acquired on a Siemens
3T scanner (phase direction anterior-posterior)  see [22] for further details. We consider joint and
independent MAP reconstruction (for the latter  we run nonlinear CG separately for each slice)  for
a number of different design choices: {Xt} optimized jointly by our method here [op-jt]; each
Xt optimized separately  by running the complex variant of [24] on slice ut [op-sp]; Xt = X for
all t  with X optimized on the most detailed slice (number 16  Figure 2  row 2 middle) [op-eq];
and encodes of each Xt drawn at random  from the density proposed in [12] [rd]  respecting the
typical spectral decay of images [4] (all designs contain the 8 lowest-frequency encodes). Results
for rd are averaged over ten repetitions. For all setups but op-eq  Xt are different across t.
Hyperparameters are adjusted based on MAP reconstruction results for a ﬁxed design picked ad hoc
(τa = τr = 0.01  τi = 0.1 in-plane; τc = 0.08 between slices)  then used for all design optimization
and MAP reconstruction runs. We run the op-jt optimization with an odd-even schedule {Cit} (all
odd (even) t ∈ 0  . . .   T − 1 for odd (even) “it”); results for two other schedules of period four come
out very similar  but require more running time. For variational inference  we run 6 OL iterations
in the initial  1 OL iteration in each design round  with up to 30 IL steps (ILs in design rounds
typically converged in 2–3 steps). The rank parameters (number of Lanczos steps)4 were km = 100 
kc = 250 (here  ut has ˜n = 8192 real coefﬁcients). Results are given in Figure 2.
First  across all designs  joint MAP reconstruction improves signiﬁcantly upon independent MAP
reconstruction. This improvement is strongest by far for op-jt (see Figure 2  rows 3 4)  which
for joint reconstruction improves on all other variants signiﬁcantly  especially with 16–30 phase

4We repeated op-jt partly with km = 250  with very similar MAP reconstruction errors for the ﬁnal

designs  but signiﬁcantly longer run time.

6

1000200030004000500060007000800000.511.522.53Spectrum of A12345671.881.91.921.941.961.9822.022.04Outer loop iteration  exactk=1500k=750k=500k=100Figure 2: Top row: l2 reconstruction errors (cid:107)| ˆuMAP| − |utrue|(cid:107) of MAP reconstruction for different measure-
ment designs. Left: joint MAP reconstruction; right: independent MAP reconstruction of each slice. op-jt:
{Xt} optimized jointly; op-sp: Xt optimized separately for each slice; op-eq: Xt = X  optimized on
slice 16; rd: Xt variable density drawn at random (averaged over 10 repetitions).
Rows 2–4: Images for op-jt (25 encodes)  slices 15–17. Row 2: true images (range 0–0.35). Row 3: errors
joint MAP. Row 4: errors indep. MAP (range 0–0.08).

encodes  where scan time is reduced by a factor 2–4 (Nyquist sampling requires 64 phase encodes).
op-eq does worst in this domain: with a model of dependencies between slices in place  it pays

7

10152025303540451234567Number phase encodesL2 reconstruction error  op−jtop−spop−eqrd(avg)10152025303540451234567Number phase encodesL2 reconstruction error  op−jtop−spop−eqrd(avg)off to choose different Xt for each slice. rnd does best from about 35 phase encodes on. While
this suboptimal behaviour of our optimization will be analyzed more closely in future work  it is our
experience so far that the gain in using greedy sequential Bayesian design optimization over simpler
choices is generally largest below 1/2 Nyquist.

6 Conclusions

We showed how to implement MRI sampling optimization by Bayesian sequential experimental
design  jointly over a stack of neighbouring slices  extending the single slice technique of [24].
Restricting ourselves to undersampling of Cartesian encodes  our method can be applied in prac-
tice whenever dense Cartesian sampling is well under control (sequence modiﬁcation is limited to
skipping encodes). We exploit the hidden Markov structure of the model by way of a Lanczos
approximation of Kalman smoothing. While the latter has been proposed for spatial statistics ap-
plications [20  25]  it has not been used for non-Gaussian approximate inference before  nor in the
context of sparsity-favouring image models or non-linear experimental design. Our method is a gen-
eral alternative to structured variational mean ﬁeld approximations typically used for non-Gaussian
dynamical systems  in that dominating covariances are tracked a posteriori  rather than eliminating
most of them a priori through factorization assumptions. In a ﬁrst study  we obtain encouraging
results in the range below 1/2 Nyquist. In future work  we will develop a truly parallel implementa-
tion  with which higher resolutions can be processed. We are considering extensions of our design
optimization technology to 3D MRI5 and to parallel MRI with receiver coil arrays [19  9]  whose
combination with k-space undersampling can be substantially more powerful than each acceleration
technique on its own [13].

j (sj) = h∗

t→ + QT

j = ρjI2 + κ2

j = νsj(νsj)T   ν := δ2δT

Appendix
j ((cid:107)sj(cid:107))  and the Hessians to solve for IRLS Newton directions
For norm potentials  h∗
do not have the form of A anymore. In order to understand this  note that we do not use complex
calculus here: s (cid:55)→ |s| is not complex differentiable at any s ∈ C. Rather  we use the C → R2
embedding  then standard real-valued optimization for variables twice the size. If θj := (h∗
j )(cid:48)  ρj :=
j((cid:107)sj(cid:107)2I2 −
j )(cid:48)(cid:48) at (cid:107)sj(cid:107) (cid:54)= 0  then using ∇sj(cid:107)sj(cid:107) = sj/(cid:107)sj(cid:107)  we have ∇∇sj h∗
(h∗
1 − δ1δT
j )  κj := (θj/(cid:107)sj(cid:107) − ρj)1/2/(cid:107)sj(cid:107). Since (cid:107)sj(cid:107)2I2 − sjsT
2  
sjsT
the Hessian is X H X + BH (s)BT . If ˆs := ((diag κ) ⊗ ν)s  then for any v ∈ R2q: H (s)v =
((diag ρ)⊗I2)v+((diag w)⊗I2)ˆs  where wj := vT
j ˆsj  j = 1  . . .   q  which shows how to compute
Hessian matrix-vector multiplications  thus to implement IRLS steps in the complex-valued case.
Recall that messages are passed  alternating between ˜At→ and Mt→ matrices. For a PCA approxi-
t→  Qt→ ∈ R˜n×km orthonormal  Tt→ tridiagonal (obtained by running
mation ˜At→ ≈ Qt→Tt→QT
km Lanczos steps for ˜At→)  low rank algebra gives
t→  Vt→ ∈ R˜n×km 
Mt→ = M( ˜At→  Γ−1
m) by way of a Cholesky decomposition. Now  ˜A(t+1)→ = At+1 + Vt→V T
computed in O(n k2
t→
becomes the precision matrix for the next Lanczos run: MVMs have additional complexity of
O(n km). Given all messages  node covariances are PCA-approximated by running Lanczos on
(t−1)→ + V←(t+1)V T←(t+1) for kc iterations. Pair variances VarQ[st→|y] are es-
At + V(t−1)→V T
timated by running Lanczos on vectors of size 2˜n (say for kc/2 iterations; the precision matrix is
given in Section 3). More details are given in [22].

t→) = Qt→(cid:0)T −1

t→Γt→Qt→(cid:1)−1

QT

t→ = Vt→V T

Acknowledgments

This work is partly funded by the Excellence Initiative of the German research foundation (DFG). It is part of
an ongoing collaboration with Rolf Pohmann  Hannes Nickisch and Bernhard Sch¨olkopf  MPI for Biological
Cybernetics  T¨ubingen  where data for this study has been acquired.

5In 3D MRI  image volumes are acquired without slice selection  using phase encoding along two dimen-

sions. There are no unmeasured slice gaps and voxels are isotropic  but scan time is much longer.

8

1st edition  2004.

References
[1] M.A. Bernstein  K.F. King  and X.J. Zhou. Handbook of MRI Pulse Sequences. Elsevier Academic Press 

[2] R. Bracewell. The Fourier Transform and Its Applications. McGraw-Hill  3rd edition  1999.
[3] E. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from

highly incomplete frequency information. IEEE Trans. Inf. Theo.  52(2):489–509  2006.

2009.

[4] H. Chang  Y. Weiss  and W. Freeman. Informative sensing. Technical Report 0901.4275v1 [cs.IT]  ArXiv 
[5] D. Donoho. Compressed sensing. IEEE Trans. Inf. Theo.  52(4):1289–1306  2006.
[6] A. Garroway  P. Grannell  and P. Mansﬁeld. Image formation in NMR by a selective irradiative pulse. J.

Phys. C: Solid State Phys.  7:L457–L462  1974.

[7] M. Girolami. A variational method for learning sparse and overcomplete representations. N. Comp. 

13:2517–2532  2001.

[8] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins University Press  3rd edition  1996.
[9] M. A. Griswold  P. M. Jakob  R. M. Heidemann  M. Nittka  V. Jellus  J. Wang  B. Kiefer  and A. Haase.
Generalized autocalibrating partially parallel acquisitions (GRAPPA). Magn. Reson. Med.  47(6):1202–
10  2002.

Image formation by induced local interactions: Examples employing nuclear magnetic

[10] P. Lauterbur.

resonance. Nature  242:190–191  1973.

[11] A. Levin  W. Freeman  and F. Durand. Understanding camera trade-offs through a Bayesian analysis of
light ﬁeld projections. In European Conference on Computer Vision  LNCS 5305  pages 88–101. Springer 
2008.
[12] M. Lustig  D. Donoho  and J. Pauly. Sparse MRI: The application of compressed sensing for rapid MR
imaging. Magn. Reson. Med.  85(6):1182–1195  2007.

[13] M. Lustig and J. Pauly. SPIR-iT: Iterative self consistent parallel imaging reconstruction from arbitrary

k-space. Magn. Reson. Med.  2009. In print.

[14] B. Madore  G. Glover  and N. Pelc. Unalising by Fourier-encoding the overlaps using the temporal

dimension (UNFOLD)  applied to cardiac imaging and fMRI. Magn. Reson. Med.  42:813–828  1999.

[15] D. Malioutov  J. Johnson  and A. Willsky. Low-rank variance estimation in large-scale GMRF models. In

ICASSP  2006.

[16] G. Marseille  R. de Beer  M. Fuderer  A. Mehlkopf  and D. van Ormondt. Nonuniform phase-encode

distributions for MRI scan time reduction. J. Magn. Reson. B  111(1):70–75  1996.

[17] D. McRobbie  E. Moore  M. Graves  and M. Prince. MRI: From Picture to Proton. Cambridge University

Press  2nd edition  2007.

[18] C. Mistretta  O. Wieben  J. Velikina  W. Block  J. Perry  Y. Wu  K. Johnson  and Y. Wu. Highly constrained

backprojection for time-resolved MRI. Magn. Reson. Med.  55:30–40  2006.

[19] K. Pruessmann  M. Weiger  M. Scheidegger  and P. Boesiger. SENSE: Sensitivity encoding for fast MRI.

Magn. Reson. Med.  42:952–962  1999.

[20] M. Schneider and A. Willsky. Krylov subspace algorithms for space-time oceanography data assimilation.

In IEEE International Geoscience and Remote Sensing Symposium  2000.

[21] M. Schneider and A. Willsky. Krylov subspace estimation. SIAM J. Comp.  22(5):1840–1864  2001.
[22] M. Seeger. Speeding up magnetic resonance image acquisition by Bayesian multi-slice adaptive com-

pressed sensing. Supplemental Appendix  2010.

[23] M. Seeger and H. Nickisch. Compressed sensing and Bayesian experimental design. In ICML 25  2008.
[24] M. Seeger  H. Nickisch  R. Pohmann  and B. Sch¨olkopf. Bayesian experimental design of magnetic

resonance imaging sequences. In NIPS 21  pages 1441–1448  2009.

[25] D. Treebushny and H. Madsen. On the construction of a reduced rank square-root Kalman ﬁlter for

efﬁcient uncertainty propagation. Future Gener. Comput. Syst.  21(7):1047–1055  2005.

[26] J. Tsao  P. Boesinger  and K. Pruessmann. k-t BLAST and k-t SENSE: Dynamic MRI with high frame

rate exploting spatiotemporal correlations. Magn. Reson. Med.  50:1031–1042  2003.

[27] F. Wajer. Non-Cartesian MRI Scan Time Reduction through Sparse Sampling. PhD thesis  Delft University

[28] J. Weaver  Y. Xu  D. Healy  and L. Cromwell. Filtering noise from images with wavelet transforms. Magn.

of Technology  2001.

Reson. Med.  21(2):288–295  1991.

9

,Stefano Ermon
Carla Gomes
Ashish Sabharwal
Bart Selman
Xuezhi Wang
Jeff Schneider
Christopher Dance
Tomi Silander