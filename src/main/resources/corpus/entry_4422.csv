2016,Probabilistic Inference with Generating Functions for Poisson Latent Variable Models,Graphical models with latent count variables arise in a number of fields. Standard exact inference techniques such as variable elimination and belief propagation do not apply to these models because the latent variables have countably infinite support. As a result  approximations such as truncation or MCMC are employed. We present the first exact inference algorithms for a class of models with latent count variables by developing a novel representation of countably infinite factors as probability generating functions  and then performing variable elimination with generating functions. Our approach is exact  runs in pseudo-polynomial time  and is much faster than existing approximate techniques. It leads to better parameter estimates for problems in population ecology by avoiding error introduced by approximate likelihood computations.,Probabilistic Inference with Generating Functions for

Poisson Latent Variable Models

Kevin Winner1 and Daniel Sheldon1 2
{kwinner sheldon}@cs.umass.edu

1 College of Information and Computer Sciences  University of Massachusetts Amherst

2 Department of Computer Science  Mount Holyoke College

Abstract

Graphical models with latent count variables arise in a number of ﬁelds. Standard
exact inference techniques such as variable elimination and belief propagation
do not apply to these models because the latent variables have countably inﬁnite
support. As a result  approximations such as truncation or MCMC are employed.
We present the ﬁrst exact inference algorithms for a class of models with latent
count variables by developing a novel representation of countably inﬁnite factors
as probability generating functions  and then performing variable elimination with
generating functions. Our approach is exact  runs in pseudo-polynomial time  and
is much faster than existing approximate techniques. It leads to better parameter
estimates for problems in population ecology by avoiding error introduced by
approximate likelihood computations.

1

Introduction

A key reason for the success of graphical models is the existence of fast algorithms that exploit the
graph structure to perform inference  such as Pearl’s belief propagation [19] and related propagation
algorithms [13  16  23] (which we refer to collectively as “message passing” algorithms)  and variable
elimination [27]. For models with a simple enough graph structure  these algorithms can compute
marginal probabilities exponentially faster than direct summation.
However  these fast exact inference methods apply only to a relatively small class of models—those
for which the basic operations of marginalization  conditioning  and multiplication of constituent
factors can be done efﬁciently. In most cases  this means that the user is limited to models where the
variables are either discrete (and ﬁnite) or Gaussian  or they must resort to some approximate form of
inference. Why are Gaussian and discrete models tractable while others are not? The key issue is one
of representation. If we start with factors that are all discrete or all Gaussian  then: (1) factors can be
represented exactly and compactly  (2) conditioning  marginalization  and multiplication can be done
efﬁciently in the compact representation  and (3) each operation produces new factors of the same
type  so they can also be represented exactly and compactly.
Many models fail the restriction of being discrete or Gaussian even though they are qualitatively
“easy”. The goal of this paper is to expand the class of models amenable to fast exact inference
by developing and exploiting a novel representation for factors with properties similar to the three
above. In particular  we investigate models with latent count variables  and we develop techniques to
represent and manipulate factors using probability generating functions.
Figure 1 provides a simple example to illustrate the main ideas. It shows a model that is commonly
used to interpret ﬁeld surveys in ecology  where it is known as an N-mixture model [22]. The latent
variable n ⇠ Poisson() represents the unknown number of individual animals at a given site.
Repeated surveys are conducted at the site during which the observer detects each individual with

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

p(n)

prior
posterior

n

yk
k=1:K

0.15

0.1

0.05

0

0

10

30

40

Generating Function

F (s) =

p(n  y1 = 2  y2 = 5  y3 = 3)sn

1Xn=0
=0.0061s5 +0.1034s6 +0.5126s7
+1.0000s8 +0.8023s9 +0.2184s10

⇥ exp(8.4375s  15.4101)

n ⇠ Poisson()

20
n
(b)

(a)

yk|n ⇠ Binomial(n  ⇢)
Figure 1: The N-mixture model [22] is a simple model with a Poisson latent variable for which
no exact inference algorithm is known: (a) the model  (b) the prior and posterior for  = 20 
⇢ = 0.25  y1 = 2  y2 = 5  y3 = 3  (c) a closed form representation of the generating function of the
unnormalized posterior  which is a compact and exact description of the posterior.

(c)

probability ⇢  so each observation yk is Binomial(n  ⇢). From these observations (usually across
many sites with shared )  the scientist wishes to infer n and ﬁt  and ⇢.
This model is very simple: all variables are marginally Poisson  and the unnormalized posterior has a
simple form (e.g.  see Figure 1b). However  until recently  there was no known algorithm to exactly
compute the likelihood p(y1:K). The naive way is to sum the unnormalized posterior p(n  y1  . . .   yK)
over all possible values of n. However  n has a countably inﬁnite support  so this is not possible. In
practice  users of this and related models truncate the inﬁnite sum at a ﬁnite value [22]. A recent paper
developed an exact algorithm for the N-mixture model  but one with running time that is exponential
in K [8]. For a much broader class of models with Poisson latent variables [5  7  11  15  28]  there
are no known exact inference algorithms. Current methods either truncate the support [5  7  11]  which
is slow (e.g.  see [4]) and interacts poorly with parameter estimation [6  8]  or use MCMC [15  28] 
which is slow and for which convergence is hard to assess. The key difﬁculty with these models
is that we lack ﬁnite and computationally tractable representations of factors over variables with a
countably inﬁnite support  such as the posterior distribution in the N-mixture model  or intermediate
factors in exact inference algorithms.
The main contribution of this paper is to develop compact and exact representations of countably
inﬁnite factors using probability generating functions (PGFs) and to show how to perform variable
elimination in the domain of generating functions. We provide the ﬁrst exact pseudo-polynomial
time inference algorithms (i.e.  polynomial in the magnitude of the observed variables) for a class of
Poisson latent variable models  including the N-mixture model and a more general class of Poisson
HMMs. For example  the generating function of the unnormalized N-mixture posterior is shown
in Figure 1c  from which we can efﬁciently recover the likelihood p(y1 = 2  y2 = 5  y3 = 3) =
F (1) = 0.0025. For Poisson HMMs  we ﬁrst develop a PGF-based forward algorithm to compute
the likelihood  which enables efﬁcient parameter estimatation. We then develop a “tail elimination”
approach to compute posterior marginals. Experiments show that our exact algorithms are much
faster than existing approximate approaches  and lead to better parameter estimation.
Related work. Several previous works have used factor transformations for inference. Bickson and
Guestrin [2] show how to perform inference in the space of characteristic functions (see also [17])
for a certain class of factor graphs. Xue et al. [26] perform variable elimination in discrete models
using Walsh-Hadamard transforms. Jha et al. [14] use generating functions (over ﬁnite domains) to
compute the partition function of Markov logic networks. McKenzie [18] describes the use of PGFs
in discrete time series models  which are related to our models except they are fully observed  and
thus require no inference.

2 The Poisson Hidden Markov Model

Although our PGF-based approaches will apply more broadly  the primary focus of our work is a
Poisson hidden Markov model (HMM) that captures a number of models from different disciplines.
To describe the model  we ﬁrst introduce notation for an operation called binomial thinning [24].

2

Write z = ⇢n to mean that z|n ⇠ Binomial(n  ⇢)  i.e.  z is the result
of “thinning” the n individuals so that each remains with probability
⇢. The Poisson HMM model is given by:
nk = Poisson(k) + k1  nk1 

yk = ⇢k  nk.

n1

y1

n2

y2

...

nK

yK

Figure 2: Poisson HMM

for k  1  with the initialization condition n0 = 0. The variables
n1  . . .   nK describe the size of a population at sampling times t1 <
t2 < . . . < tK. At time tk  the population consists of a Poisson(k) number of new arrivals  plus
k1  nk1 survivors from the previous time step (each individual survives with probability k). A
noisy count yk = ⇢k  nk is made of the population at time tk  where ⇢k is the detection probability
of each individual. This model is broadly applicable. It models situations where individuals arrive in
an iid fashion  and the time they remain is “memoryless”. Versions of this model are used in ecology
to model surveys of “open populations” (individuals arrive and depart over time) [7] and the timing
and abundance of insect populations [12  25  29]  and it also capture models from queueing theory [9]
and generic time series models for count data [1  18].
Existing approaches. Two classes of methods have been applied for inference in Poisson HMMs
and related models. The ﬁrst is to truncate the support of the Poisson variables at a large but ﬁnite
value Nmax [5  7  11  22]. Then  for example  the Poisson HMM reduces to a standard discrete
HMM. This is unsatisfactory because it is slow (a smart implementation that uses the fast Fourier
transform takes time O(KN 2
max log Nmax))  and the choice of Nmax is intertwined with the unknown
Poisson parameters k  so the approximation interacts poorly with parameter estimation [6  8]. The
second class of approximate methods that has been applied to these problems is MCMC [28]. This is
undesirable because it is also slow  and because the problem has a simple structure that should admit
fast algorithms.

3 Variable Elimination with Generating Functions

Our approach to inference in Poisson HMMs will be to implement the same abstract set of operations
as variable elimination  but using a representation based on probability generating functions. Because
variable elimination will produce intermediate factors on larger sets of variables  and to highlight
the ability of our methods to generalize to a larger class of models  we ﬁrst abstract from the
Poisson HMM to introduce notation general for graphical models with multivariate factors  and their
corresponding multivariate generating functions.
Factors. Let x = (x1  . . .   xd) be a vector of nonnegative integer-valued random variables where
xi 2X i ✓ Z0. The set Xi may be ﬁnite (e.g.  to model binary or ﬁnite discrete variables)  but
we assume without loss of generality that Xi = Z0 for all i by deﬁning factors to take value zero
for integers outside of Xi. For any set ↵ ✓{ 1  . . .   d}  deﬁne the subvector x↵ := (xi  i 2 ↵).
ZQ↵2A ↵(x↵)  where Z is a normalization
We consider probability models of the form p(x) = 1
constant and { ↵} is a set of factors ↵ : Z0 ! R+ indexed by subsets ↵ ✓{ 1  . . .   d} in a
collection A.
Generating Functions. A general factor ↵ on integer-valued variables cannot be ﬁnitely rep-
resented. We instead use the formalization of probability generating functions (PGFs). Let
s = (s1  . . .   sd) be a vector of indeterminates corresponding to the random variables x. The
joint PGF of a factor ↵ is

F↵(s↵) =Xx↵

 ↵(x↵) ·Yi2↵

sxi

i =Xx↵

 ↵(x↵) · sx↵
↵ .

over all vectors x↵ of non-negative integers.

Here  for two vectors a and b with the same index set I  we have deﬁned ab =Qi2I abi
Univariate PGFs of the form F (s) = P1x=0 Pr(X = x)sx = E[sX]  where X is a nonnegative
integer-valued random variable  are widely used in probability and statistics [3  21]  and have a
number of nice properties. A PGF uniquely encodes the distribution of X  and there are formulas
to recover moments and entries of the the probability mass function from the PGF. Most common
distributions have closed-form PGFs  e.g.  F (s) = exp{(s 1)} when X ⇠ Poisson(). Similarly 
the joint PGF F↵ uniquely encodes the factor ↵  and we will develop a set of useful operations on
joint PGFs. Note that we abuse terminology slightly by referring to the generating function of the

i . The sum is

3

factor ↵ as a probability generating function; however  it is consistent with the view of ↵ as an
unnormalized probability distribution.

@a
@sa
i

a!

3.1 Operations on Generating Functions
Our goal is to perform variable elimination using factors represented as PGFs. To do this  the basic
operations we need to support are are multiplication  marginalization  and “entering evidence” into
factors (reducing the factor by ﬁxing the value of one variable). In this section we state a number of
results about PGFs that show how to perform such operations. For the most part  these are either well
known or variations on well known facts about PGFs (e.g.  see [10]  Chapters 11  12). All proofs can
be found in the supplementary material.
First  we see that marginalization of factors is very easy in the PGF domain:

 ↵(x↵\i  xi) be the factor obtained
from marginalizing i out of ↵. The joint PGF of ↵\i is F↵\i(s↵\i) = F↵(s↵\i  1). The normaliza-

Proposition 1 (Marginalization). Let ↵\i(x↵\i) :=Pxi2Xi
tion constantPx↵

 ↵(x↵) is equal to F↵(1  . . .   1).

Addition of

two variables). Let (x↵  x  xk)

F↵(s↵)si=0.

Entering evidence is also straightforward:
Proposition 2 (Evidence). Let ↵\i(x↵\i) := ↵(x↵\i  a) be the factor resulting from observing
the value xi = a in ↵. The joint PGF of ↵\i is F↵\i(s↵\i) = 1
Multiplication in the PGF domain—i.e.  computing the PGF of the product ↵(x↵) (x) of
two factors ↵ and —is not straightforward in general. However  for certain types of factors 
multiplication is possible. We give two cases.
Proposition 3 (Multiplication: Binomial thinning). Let ↵[j(x↵  xj) = ↵(x↵)·Binomial(xj|xi ⇢ )
be the factor resulting from expanding ↵ to introduce a thinned variable xj := ⇢  xi  where i 2 ↵
and j /2 ↵. The joint PGF of ↵[j is F↵[j(s↵  sj) = F↵(s↵\i  si(⇢sj + 1  ⇢)).
Proposition 4 (Multiplication:
:=
 ↵(x↵) (x)I{xk = xi + xj} be the joint factor resulting from the introduction of a new variable
xk = xi + xj  where i 2 ↵  j 2   k /2 ↵ [    := ↵ [  [{ k}. The joint PGF of  is
F(s↵  s  sk) = F↵(s↵\i  sksi)F(s\j  sksj).
The four basic operations above are enough to perform variable elimination on a large set of models.
In practice  it is useful to introduce additional operations that combine two of the above operations.
Proposition 5 (Thin then observe). Let 0↵(x↵) := ↵(x↵)·Binomial(a|xi ⇢ ) be the factor resulting
from observing the thinned variable ⇢  xi = a for i 2 ↵. The joint PGF of 0↵ is F 0↵(s↵) =
a! (si⇢)a @a
@ta
i
Proposition 6 (Thin then marginalize). Let (↵\i)[j(x↵\i  xj) :=Pxi
 ↵(x↵)· Binomial(xj|xi ⇢ )
be the factor resulting from introducing xj := ⇢  xi and then marginalizing xi for i 2 ↵  j /2 ↵. The
joint PGF of (↵\i)[j is F(↵\i)[j(s↵\i  sj) = F↵(s↵\i  ⇢sj + 1  ⇢).
Proposition 7 (Add then marginalize). Let (x↵\i  x\j  xk) :=Pxi xj
 ↵(x↵) (x)I{xk =
xi + xj} be the factor resulting from the deterministic addition xi + xj = xk followed by marginal-
ization of xi and xj  where i 2 ↵  j 2   k /2 ↵ [    := (↵ \ i) [ ( \ j) [{ k}. The joint PGF of
  is F(s↵\i  s\j  sk) = F↵(s↵\i  sk)F(s\j  sk).
3.2 The PGF-Forward Algorithm for Poisson HMMs
We now use the operations from the previous section to implement the forward algorithm for Poisson
HMMs in the domain of PGFs. The forward algorithm is an instance of variable elimination  but in
HMMs is more easily described using the following recurrence for the joint probability p(nk  y1:k):

F↵(s↵\i  ti)ti=si(1⇢)

.

1

We can compute the “forward messages” ↵k(nk) := p(nk  y1:k) in a sequential forward pass 
assuming it is possible to enumerate all possible values of nk to store the messages and compute the
recurrence. In our case  nk can take on an inﬁnite number of values  so this is not possible.

p(nk  y1:k)

|

↵k(nk)

{z

}

= Xnk1

|

p(nk1  y1:k1)

p(nk|nk1)p(yk|nk)

↵k1(nk1)

}

{z

4

Algorithm 1 FORWARD
1: 1(z1) := I{z1 = 0}
2: for k = 1 to K do
k(nk) := Pzk mk
3:
 k+1(zk+1) :=Pnk

4:
5:
6:
7:
8: end for

↵k(nk) := k(nk)p(yk | nk)
if k < K then

end if

 k(zk)p(mk)I{nk = zk +mk}

↵k(nk)p(zk+1 | nk)

We proceed instead using generating functions. To apply the oper-
ations from the previous section  it is useful to instantiate explicit
random variables mk and zk for the number of new arrivals in step
k and survivors from step k  1  respectively  to get the model (see
Figure 3):

mk ⇠ Poisson(k) 
nk = mk + zk 

zk = k1  nk1 
yk = ⇢k  nk.

We can now expand the recurrence for ↵k(nk) as:

yk! (s⇢k)yk (yk)

Algorithm 2 PGF-FORWARD
1: 1(s) := 1
2: for k = 1 to K do
3:
4:
5:
6:
7:
8: end for

k(s) := k(s) · exp{k(s  1)}
Ak(s) := 1
if k < K then

s(1  ⇢k)
 k+1(s) := Akks + 1  k
"k

!k

end if

mk

k

Figure 3: Expanded model.

!k–1

nk–1

yk–1

 k(zk)

}|

zk

!k

nk

yk

{
}

↵k(nk) = p(yk|nk)

p(mk)p(nk|zk  mk)

↵k1(nk1)p(zk|nk1)

(1)

1Xzk=0

1Xmk=0
|

z
1Xnk1=0
{z

k(nk)

We have introduced the intermediate factors k(zk) and k(nk) to clarify the implementation.
FORWARD (Algorithm 1) is a dynamic programming algorithm based on this recurrence to compute
the ↵k messages for all k. However  it cannot be implemented due to the inﬁnite sums. PGF-FORWARD
(Algorithm 2) instead performs the same operations in the domain of generating functions— k  k 
and Ak are the PGFs of k  k  and ↵k  respectively. Each line in PGF-FORWARD implements the
operation in the corresponding line of FORWARD using the operations given in Section 3.1. In Line 1 
 1(s) =Pz1
 1(z1)sz1 = 1 is the PGF of 1. Line 3 uses “Add then marginalize” (Proposition 7)
combined with the fact that the Poisson PGF for mk is exp{k(s  1)}. Line 4 uses “Thin then
observe” (Proposition 5)  and Line 6 uses “Thin then marginalize” (Proposition 6).
Implementation and Complexity. The PGF-FORWARD algorithm as stated is symbolic. It remains
to see how it can be implemented efﬁciently. For this  we need to respresent and manipulate the PGFs
in the algorithm efﬁciently. We do so based on the following result:
Theorem 1. All PGFs in the PGF-FORWARD algorithm have the form f (s) exp{as + b} where f is
a polynomial with degree at most Y =Pk yk.
Proof. We verify the invariant inductively. It is clearly satisﬁed in Line 1 of PGF-FORWARD (f (s) =
1  a = b = 0). We check that it is preserved for each operation within the loop. In Line 3  suppose
 k(s) = f (s) exp{as + b}. Then k(s) = f (s) exp{(a + k)s + (b  k)} has the desired form.
In Line 4  assume that k(s) = f (s) exp{as + b}. Then one can verify by taking the ykth derivative
of k(s) that Ak(s) is given by:

Ak(s) = (a⇢k)yk · syk

f (`)(s(1  ⇢k))

a``!(yk  `)! ! · exp{a(1  ⇢k)s + b}

ykX`=0

The scalar (a⇢)yk can be combined with the polynomial coefﬁcients or the scalar exp(b) in the
exponential. The second term is a polynomial of degree yk + deg(f ). The third term has the form
exp{a0s + b0}. Therefore  in Line 4  Ak(s) has the desired form  and the degree of the polynomial
part of the representation increases by yk.

5

In Line 6  suppose Ak(s) = f (s) exp{as + b}. Then k+1(s) = g(s) expaks +b + a(1 k)  
where g(s) is the composition of f with the afﬁne function ks + 1  k  so g is a polynomial of the
same degree as f. Therefore  k+1(s) has the desired form.
We have shown that each PGF retains the desired form  and the degree of the polynomial is initially
zero and increases by yk each time through the loop  so it is always bounded by Y =Pk yk.

The important consequence of Theorem 1 is that we can represent and manipulate PGFs in PGF-
FORWARD by storing at most Y coefﬁcients for the polynomial f plus the scalars a and b. An
efﬁcient implementation based on this principle and the proof of the previous theorem is given in the
supplementary material.
Theorem 2. The running time of PGF-FORWARD for Poisson HMMs is O(KY 2).
3.3 Computing Marginals by Tail Elimination
PGF-FORWARD allows us to efﬁciently compute
the likelihood in a Poisson HMM. We would also
like to compute posterior marginals  the standard
approach for which is the forward-backward al-
gorithm [20]. A natural question is whether there
is an efﬁcient PGF implementation of the back-
ward algorithm for Poisson HMMs. While we
were able to derive this algorithm symbolically 
the functional form of the PGFs is more complex
and we do not know of a polynomial-time im-
plementation. Instead  we adopt a variable elim-
ination approach that is less efﬁcient in terms of
the number of operations performed on factors
(O(K2) instead of O(K) to compute all poste-
rior marginals) but with the signiﬁcant advantage
that those operations are efﬁcient. The key principle is to always eliminate predecessors before suc-
cessors in the Poisson HMM. This allows us to apply operations similar to those in PGF-FORWARD.
Deﬁne ✓ij(ni  nj) := p(ni  nj  y1:j) for j > i. We can write a recurrence for ✓ij similar to Equation
(1). For j > i + 1:

Algorithm 3 PGF-TAIL-ELIMINATE
Output: PGF of unnormalized marginal p(ni  y1:K )
1: i i+1(s  t) := Ai(s(it + 1  i))
2: for j = i + 1 to K do
3:
4:
5:
6:
7:
8: end for
9: return ⇥iK (s  1)

i j+1(s  t) :=⇥ ij (s  j t + 1  j )

u=t(1⇢j )

Hij (s  t) := ij (s  t) exp{k(t  1)}
⇥ij (s  t) := 1
if j < K then

yj ! (t⇢j )yj

@yj Hij (s u)

@uyj

end if

✓ij(ni  nj) = p(yj|nj) Xmj  zj
|

We have again introduced intermediate factors  with probabilistic meanings ij(ni  zj) =
p(ni  zj  y1:j1) and ⌘ij(ni  nj) = p(ni  nj  y1:j1).
PGF-TAIL-ELIMINATE (Algorithm 3) is a PGF-domain dynamic programming algorithm based on
this recurrence to compute the PGFs of the ✓ij factors for all j 2{ i + 1  . . .   K}. The non-PGF
version of the algorithm appears in the supplementary material for comparison. We use ⇥ij  ij 
and Hij to represent the joint PGFs of ✓ij  ij  and ⌘ij  respectively. The algorithm can also be
interpreted as variable elimination using the order zi+1  ni+1  . . .   zK  nK  after having already
eliminated variables n1:i1 and z1:i1 in the forward algorithm  and therefore starting with the PGF
of ↵i(ni). PGF-TAIL-ELIMINATE concludes by marginalizing nK from ⇥iK to obtain the PGF of
the unnormalized posterior marginal p(ni  y1:K). Each line of PGF-TAIL-ELIMINATE uses the same
operations given in Section 3.1. Line 1 uses “Binomial thinning” (Proposition 3)  Line 3 uses “Add
then marginalize” (Proposition 7)  Line 4 uses “Thin then observe” (Proposition 5) and Line 6 uses
“Thin then marginalize” (Proposition 6).
Implementation and Complexity. The considerations for implementating PGF-TAIL-ELIMINATE
are similar to those of PGF-FORWARD  with the details being slightly more complex due to the
larger factors. We state the main results here and include proofs and implementation details in the
supplementary material.
Theorem 3. All PGFs in the PGF-TAIL-ELIMINATE algorithm have the form f (s  t) exp{ast + bs +
ct + d} where f is a bivariate polynomial with maximum exponent most Y =Pk yk.

6

p(mj)p(nj|zj  mj)

✓i j1(ni  nj1)p(zj|nj1)

.

ij (ni zj )

}|

z
Xnj1
{z

⌘ij (ni nj )

{
}

$; vs Runtime

FA - Poiss
FA - Oracle
PGFFA

100

10-1

10-2

10-3

)
s
(
 

e
m

i
t

n
u
r
 

n
a
e
M

10-4

10-1

100

101
$;

102

103

)
s
(
 

e
m

i
t

n
u
r
 

n
a
e
M

9
8
7
6
5
4
3
2
1
0

#10-3 $; vs Runtime of PGFFA

PGFFA

0

100

200

300

400

500

$;

^6

200
180
160
140
120
100
80
60
40
20
0

6 Recovery

Trunc
PGFFA
True 6

10

30

50

70

90

110

130

150

6

Figure 4: Runtime of PGF-FORWARD and truncated algorithm vs.
⇤⇢. Left: log-log scale. Right: PGF-FORWARD only  linear scale.

Figure 5: Parameter estimation
w/ PGF-FORWARD

Theorem 4. PGF-TAIL-ELIMINATE can be implemented to run in time O(Y 3(log Y + K))  and the
PGFs for all marginals can be computed in time O(KY 3(log Y + K)).
3.4 Extracting Posterior Marginals and Moments

After computing the PGF of the posterior marginals  we wish to compute the actual probabilities and
other quantities  such as the moments  of the posterior distribution. This can be done efﬁciently:
Theorem 5. The PGF of the unnormalized posterior marginal p(ni  y1:K) has the form F (s) =
f (s) exp{as + b} where f (s) =Pm
j=0 cjsj is a polynomial of degree m  Y . Given the parameters
of the PGF  the posterior mean  the posterior variance  and an arbitrary entry of the posterior
probability mass function can each be computed in O(m) = O(Y ) time as follows  where Z =
f (1) exp{a + b}:
(i) µ := E[ni | y1:k] = ea+blog ZPm
(ii) 2 := Var(ni | y1:k) = µ  µ2 + ea+blog ZPm
(iii) Pr(ni = ` | y1:k) = eblog ZPmin{m `}

j=0((a + m)2  m)cj

j=0(a + m)cj

j=0

cj

a`i
(`i)!

4 Experiments

We conducted experiments to demonstrate that our method is faster than standard approximate
approaches for computing the likelihood in Poisson HMMs  that it leads to better parameter estimates 
and to demonstrate the computation of posterior marginals on an ecological data set.
Running time. We compared the runtimes of PGF-FORWARD and the truncated forward algorithm  a
standard method for Poisson HMMs in the ecology domain [7]. The runtime of our algorithm depends
on the magnitude of the observed counts. The runtime of the truncated forward is very sensitive to
the setting of the trunctation parameter Nmax: smaller values are faster  but may underestimate the
likelihood. Selecting Nmax large enough to yield correct likelihoods but small enough to be fast is
difﬁcult [4  6  8]. We evaluated two strategies to select Nmax. The ﬁrst is an oracle strategy  where
we ﬁrst searched for the smallest value of Nmax for which the error in the likelihood is at most 0.001 
and then compared vs. the runtime for that value (excluding the search time). The second strategy 
adapted from [8]  is to set Nmax such that the maximum discarded tail probability of the Poisson
prior over any nk is less than 105.
To explore these issues we generated data from models with arrival rates  =⇤ ⇥
[0.0257  0.1163  0.2104  0.1504  0.0428] and survival rates  = [0.2636  0.2636  0.2636  0.2636]
based on a model for insect populations [29]. We varied the overall population size parameter
⇤ 2{ 10  20  . . .   100  125  150  . . .   500}  and detection probability ⇢ 2{ 0.05  0.10  . . .   1.00}. For
each parameter setting  we generated 25 data sets and recorded the runtime of both methods.
Figure 4 shows that PGF-FORWARD is 2–3 orders of magnitude faster than even the oracle truncated
algorithm. The runtime is plotted against ⇤⇢ / E[Y ]  the primary parameter controlling the runtime
of PGF-FORWARD. Empirically  the runtime depends linearly instead of quadratically  as predicted 

7

on the magnitude of observed counts—this is likely due to the implementation  which is dominated
by loops that execute O(Y ) times  with much faster vectorized O(Y ) operations within the loops.
Parameter Estimation. We now examine the impact of exact vs. truncated likelihood computations
on parameter estimation in the N-mixture model [22]. A well-known feature of this and related
models is that it is usually easy to estimate the product of the population size parameter  and
detection probability ⇢  which determines the mean of the observed counts  but  without enough
data  it is difﬁcult to estimate both parameters accurately  especially as ⇢ ! 0 (e.g.  see [8]). It was
previously shown that truncating the likelihood can artiﬁcially suppress instances where the true
maximum-likelihood estimates are inﬁnite [8]  a phenomenon that we also observed. We designed
a different  simple  experiment to reveal another failure case of the truncated likelihood  which is
avoided by our exact methods. In this case  the modeler is given observed counts over 50 time steps
(K = 50) at 20 iid locations. She selects a heuristic ﬁxed value of Nmax approximately 5 times the
average observed count based on her belief that the detection probability is not too small and this will
capture most of the probability mass.
To evaluate the accuracy of parameter estimates obtained by numerically maximizing the truncated
and exact likelihoods using this heuristic for Nmax we generated true data from different values of 
and ⇢ with ⇢ = E[y] ﬁxed to be equal to 10—the modeler does not know the true parameters  and in
each cases chooses Nmax = 5E[y] = 50. Figure 5 shows the results. As the true  increases close
to and beyond Nmax  the truncated method cuts off signiﬁcant portions of the probability mass and
severely underestimates . Estimation with the exact likelihood is noisier as  increases and ⇢ ! 0 
but not biased by truncation. While this result is not surprising  it reﬂects a realistic situation faced by
the practitioner who must select this trunctation parameter.
Marginals. We demonstrate the computation of posterior
marginals and parameter estimation on an end-to-end case
study to model the abundance of Northern Dusky Salamanders
at 15 sites in the mid-Atlantic US using data from [28]. The
data consists of 14 counts at each site  conducted in June and
July over 7 years. We ﬁrst ﬁt a Poisson HMM by numerically
maximizing the likelihood as computed by PGF-FORWARD.
The model has three parameters total  which are shared across
sites and time: arrival rate  survival rate  and detection proba-
bility. Arrivals are modeled as a homogenous Poisson process 
and survival is modeled by assuming indvidual lifetimes are
exponentially distributed. The ﬁtted parameters indicated an
arrival rate of 0.32 individuals per month  a mean lifetime of
14.25 months  and detection probability of 0.58.
Figure 6 shows the posterior marginals as computed by PGF-TAIL-ELIMINATE with the ﬁtted pa-
rameters  which are useful both for model diagnostics and for population status assessments. The
crosses show the posterior mean  and color intensity indicates the actual PMF. Overall  computing
maximum likelihood estimates required 189 likelihood evaluations and thus 189 ⇥ 15 = 2835 calls
to PGF-FORWARD  which took 24s total. Extracting posterior marginals at each site required 14
executions of the full PGF-TAIL-ELIMINATE routine (at all 14 latent variables)  and took 1.6s per site.
Extracting the marginal probabilities and posterior mean took 0.0012s per latent variable.

Figure 6: Posterior marginals for
abundance of Northern Dusky Sala-
manders at 1 site. See text.

5 Conclusion

We have presented techniques for exact inference in countably inﬁnite latent variable models using
probability generating functions. Although many aspects of the methodology are general  the current
method is limited to HMMs with Poisson latent variables  for which we can represent and manipulate
PGFs efﬁciently (cf. Theorems 1 and 3). Future work will focus on extending the methods to
graphical models with more complex structures and to support a larger set of distributions  for
example  including the negative binomial  geometric  and others. One path toward these goals is to
ﬁnd a broader parametric representation for PGFs that can be manipulated efﬁciently.

Acknowledgments. This material is based upon work supported by the National Science Founda-
tion under Grant No. 1617533.

8

References
[1] M. A. Al-Osh and A. A. Alzaid. First-order integer-valued autoregressive (INAR(1)) process. Journal of

Time Series Analysis  8(3):261–275  1987.

[2] D. Bickson and C. Guestrin. Inference with Multivariate Heavy-Tails in Linear Models. In Advances in

Neural Information Processing Systems (NIPS)  2010.

[3] G. Casella and R. Berger. Statistical Inference. Duxbury advanced series in statistics and decision sciences.

Thomson Learning  2002. ISBN 9780534243128.

[4] R. Chandler. URL http://www.inside-r.org/packages/cran/unmarked/docs/pcountOpen.
[5] R. B. Chandler  J. A. Royle  and D. I. King. Inference about density and temporary emigration in unmarked

populations. Ecology  92(7):1429–1435  2011.

[6] T. Couturier  M. Cheylan  A. Bertolero  G. Astruc  and A. Besnard. Estimating abundance and population
trends when detection is low and highly variable: A comparison of three methods for the Hermann’s
tortoise. Journal of Wildlife Management  77(3):454–462  2013.

[7] D. Dail and L. Madsen. Models for estimating abundance from repeated counts of an open metapopulation.

Biometrics  67(2):577–87  2011.

(1):237–246  2015.

[8] E. B. Dennis  B. J. Morgan  and M. S. Ridout. Computational aspects of n-mixture models. Biometrics  71

(4):731–742  1993.

[9] S. G. Eick  W. A. Massey  and W. Whitt. The physics of the Mt/G/1 queue. Operations Research  41
[10] W. Feller. An Introduction to Probability Theory and Its Applications. Wiley  1968.
[11] I. J. Fiske and R. B. Chandler. unmarked: An R package for ﬁtting hierarchical models of wildlife

occurrence and abundance. Journal of Statistical Software  43:1–23  2011.

[12] K. Gross  E. J. Kalendra  B. R. Hudgens  and N. M. Haddad. Robustness and uncertainty in estimates of

butterﬂy abundance from transect counts. Population Ecology  49(3):191–200  2007.

[13] F. V. Jensen  S. L. Lauritzen  and K. G. Olesen. Bayesian updating in causal probabilistic networks by

local computations. Computational statistics quarterly  1990.

[14] A. Jha  V. Gogate  A. Meliou  and D. Suciu. Lifted inference seen from the other side: The tractable

features. In Advances in Neural Information Processing Systems (NIPS)  pages 973–981  2010.

[15] M. Kéry  R. M. Dorazio  L. Soldaat  A. Van Strien  A. Zuiderwijk  and J. A. Royle. Trend estimation in

populations with imperfect detection. Journal of Applied Ecology  46:1163–1172  2009.

[16] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and
their application to expert systems. Journal of the Royal Statistical Society. Series B (Methodological) 
pages 157–224  1988.

[17] Y. Mao and F. R. Kschischang. On factor graphs and the Fourier transform. IEEE Transactions on

Information Theory  51(5):1635–1649  2005.

[18] E. McKenzie. Ch. 16. discrete variate time series. In Stochastic Processes: Modelling and Simulation 

volume 21 of Handbook of Statistics  pages 573 – 606. Elsevier  2003.

[19] J. Pearl. Fusion  propagation  and structuring in belief networks. Artiﬁcial intelligence  29(3):241–288 

1986.

[20] L. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceed-

ings of the IEEE  77(2):257–286  Feb 1989.

[21] S. I. Resnick. Adventures in stochastic processes. Springer Science & Business Media  2013.
[22] J. A. Royle. N-Mixture Models for Estimating Population Size from Spatially Replicated Counts. Biomet-

[23] P. P. Shenoy and G. Shafer. Axioms for probability and belief-function propagation. In Uncertainty in

[24] C. H. Weiß. Thinning operations for modeling time series of counts—a survey. AStA Advances in Statistical

rics  60(1):108–115  2004.

Artiﬁcial Intelligence  1990.

Analysis  92(3):319–341  2008.

[25] K. Winner  G. Bernstein  and D. Sheldon. Inference in a partially observed queueing model with appli-
cations in ecology. In Proceedings of the 32nd International Conference on Machine Learning (ICML) 
volume 37  pages 2512–2520  2015.

[26] Y. Xue  S. Ermon  R. Lebras  C. P. Gomes  and B. Selman. Variable Elimination in Fourier Domain. In

Proceedings of the 33rd International Conference on Machine Learning (ICML)  pages 1–10  2016.

[27] N. L. Zhang and D. Poole. A simple approach to bayesian network computations. In Proc. of the Tenth

Canadian Conference on Artiﬁcial Intelligence  1994.

[28] E. F. Zipkin  J. T. Thorson  K. See  H. J. Lynch  E. H. C. Grant  Y. Kanno  R. B. Chandler  B. H. Letcher 
and J. A. Royle. Modeling structured population dynamics using data from unmarked individuals. Ecology 
95(1):22–29  2014.

[29] C. Zonneveld. Estimating death rates from transect counts. Ecological Entomology  16(1):115–121  1991.

9

,Yu Xin
Tommi Jaakkola
Kevin Winner
Daniel Sheldon
George Papamakarios
Theo Pavlakou
Iain Murray