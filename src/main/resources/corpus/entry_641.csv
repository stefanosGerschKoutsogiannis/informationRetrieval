2012,Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics,Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning  the motions are always directed towards a single target. In this work  we focus on combining several such DS with distinct attractors  resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations  it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show  via implementations on a simulated 10 degrees of freedom mobile robotic platform  that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations.,Augmented-SVM: Automatic space partitioning for

combining multiple non-linear dynamics

Ashwini Shukla

Aude Billard

ashwini.shukla@epfl.ch

aude.billard@epfl.ch

Learning Algorithms and Systems Laboratory (LASA)

´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)

Lausanne  Switzerland - 1015

Abstract

Non-linear dynamical systems (DS) have been used extensively for building gen-
erative models of human behavior. Their applications range from modeling brain
dynamics to encoding motor commands. Many schemes have been proposed for
encoding robot motions using dynamical systems with a single attractor placed
at a predeﬁned target in state space. Although these enable the robots to react
against sudden perturbations without any re-planning  the motions are always di-
rected towards a single target. In this work  we focus on combining several such
DS with distinct attractors  resulting in a multi-stable DS. We show its applicabil-
ity in reach-to-grasp tasks where the attractors represent several grasping points
on the target object. While exploiting multiple attractors provides more ﬂexibil-
ity in recovering from unseen perturbations  it also increases the complexity of
the underlying learning problem. Here we present the Augmented-SVM (A-SVM)
model which inherits region partitioning ability of the well known SVM classiﬁer
and is augmented with novel constraints derived from the individual DS. The new
constraints modify the original SVM dual whose optimal solution then results in a
new class of support vectors (SV). These new SV ensure that the resulting multi-
stable DS incurs minimum deviation from the original dynamics and is stable at
each of the attractors within a ﬁnite region of attraction. We show  via implemen-
tations on a simulated 10 degrees of freedom mobile robotic platform  that the
model is capable of real-time motion generation and is able to adapt on-the-ﬂy to
perturbations.

1 Introduction

Dynamical systems (DS) have proved to be a promising framework for encoding and generating
complex motions. A major advantage of representing motion using DS based models [1  2  3  4] is
the ability to counter perturbations by virtue of the fact that re-planning of trajectories is instanta-
neous. These are generative schemes that deﬁne the ﬂow of trajectories in state space x ∈ RN by
means of a non-linear dynamical function ˙x = f (x). DS with single stable attractors have been used
in pick and place tasks to control for both the motion of the end-effector [5  6  7] and the placement
of the ﬁngers on an object [8]. Assuming a single attractor  and hence a single grasping location on
the object  constrains considerably the applicability of these methods to realistic grasping problems.
A DS composed of multiple stable attractors provides an opportunity to encode different ways to
reach and grasp an object. Recent neuro-physiological results [9] have shown that a DS based mod-
eling best explains the trajectories followed by humans while switching between several reaching
targets. From a robotics viewpoint  a robot controlled using a DS with multiple attractors would

1

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

(a) Motion 1

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

(b) Motion 2

Training data
Streamlines
Attractors

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

(c) Crossing over

(d) Fast switching

Figure 2: Combining motions using naive SVM classiﬁcation based switching.

be able to switch online across grasping strategies. This may be useful  e.g.  when one grasping
point becomes no longer accessible due to a sudden change in the orientation of the object or the
appearance of an obstacle along the current trajectory. This paper presents a method by which
one can learn multiple dynamics directed toward different attractors in a single dynamical system.
The dynamical function f (x) is usually estimated us-
ing non-linear regression functions such as Gaussian Pro-
cess Regression (GPR) [10]  Gaussian Mixture Regres-
sion (GMR) [7]  Locally Weighted Projection Regression
(LWPR) [11] or Dynamical Movement Primitives (DMP)
[1]. However  all of these works modeled DS with a sin-
gle attractor. While [7  12] ensure global stability at the
attractor  other approaches result in unstable DS with spu-
rious attractors.

−0.5

1.5

0.5

Y

2

0

1

−1

0

1

−1

−1.5

−0.5

−1.5

Stability at multiple targets has been addressed to date
largely through neural networks approaches. The Hop-
ﬁeld network and variants offered a powerful means to
encode several stable attractors in the same system to pro-
vide a form of content-addressable memory [13  14]. The
dynamics to reach these attractors was however not controlled for  nor was the partitioning of the
state space that would send the trajectories to each attractor. Echo-state networks provide alternative
ways to encode various complex dynamics [15]. Although they have proved to be universal estima-
tors  their ability to generalize in untrained regions of state space remains unveriﬁed. Also  the key
issue of global stability of the learned dynamics is achieved using heuristic rules. To our knowledge 
this is the ﬁrst attempt at learning simultaneously a partitioning of the state space and an embedding
of multiple dynamical systems with separate regions of attractions and distinct attractors.

Figure 1: 8 attractor DS

0.5

1.5

X

2 Preliminaries

A naive approach to building a multi-attractor DS would be to ﬁrst partition the space and then learn
a DS in each partition separately. This would unfortunately rarely result in the desired compound
system. Consider  for instance  two DS with distinct attractors  as shown in Fig. 2(a)-(b). First  we
build a SVM classiﬁer to separate data points of the ﬁrst DS  labeled +1  from data points of the
other DS  labeled −1. We then estimate each DS separately using any of the techniques reviewed in
the previous section. Let h : RN 7→ R denote the classiﬁer function that separates the state space
x ∈ RN into two regions with labels yi ∈ {+1  −1}. Also  let the two DS be ˙x = fyi(x) with
. The combined DS is then given by ˙x = fsgn(h(x))(x). Figure 2(c) shows
stable attractors at x∗yi
the trajectories resulting from this approach. Due to the non-linearity of the dynamics  trajectories
initialized in one region cross the boundary and converge to the attractor located in the opposite
region. In other words  each region partitioned by the SVM hyperplane is not a region of attraction
for its attractor. In a real-world scenario where the attractors represent grasping points on an object
and the trajectories are to be followed by robots  crossing over may take the trajectories towards
kinematically unreachable regions. Also  as shown in Fig. 2(d)  trajectories that encounter the
boundary may switch rapidly between different dynamics leading to jittery motion.

To ensure that the trajectories do not cross the boundary and remain within the region of attrac-
tion of their respective attractors  one could adopt a more informed approach in which each of the

2

original DS is modulated such that the generated trajectories always move away from the classiﬁer
boundary. Recall that by construction  the absolute value of the classiﬁer function h(x) increases as
one moves away from the classiﬁcation hyperplane. The gradient ∇h(x) is hence positive  respec-
tively negative  as one moves inside the region of the positive  respectively negative  class. We can
exploit this observation to deﬂect selective components of the velocity signal from the original DS
along  respectively opposite to  the direction ∇h(x). Concretely  if ˙xO = fsgn(h(x))(x) denotes the
velocity obtained from the original DS and

the modulated dynamical system is given by

λ(x) = (cid:26) max(cid:0)ǫ  ∇h(x)T ˙xO(cid:1)
min(cid:0)−ǫ  ∇h(x)T ˙xO(cid:1)
˙x = ˜f (x) = λ(x)∇h(x) + ˙x⊥.

if h(x) > 0
if h(x) < 0

 

(1)

(2)

Here  ǫ is a small positive scalar and ˙x⊥ = ˙xO − (cid:16) ∇h(x)T ˙xO

k∇h(xk2 (cid:17) ∇h(x) is the compo-

nent of the original velocity perpendicular to ∇h. This results in a vector ﬁeld that ﬂows
along increasing values of the classiﬁer function in the regions of space where h(x) > 0
and along decreasing values for h(x) < 0. As a result 
the trajectories move away from
the classiﬁcation hyperplane and converge to a point located in the region where they were
initialized.
Such modulated systems have been used extensively for estimating stability re-
gions of interconnected power networks [16] and are known as quasi gradient systems [17].
If h(x) is upper bounded1  all
trajectories converge to one of the stationary points {x :
∇h(x) = 0} and h(x) is a Lyapunov function of the overall system (refer [17]  proposition 1).
Figure 3 shows the result of applying the above modulation to our
pair of DS. As expected  it forces the trajectories to ﬂow along the
gradient of the function h(x). Although this solves the problem of
“crossing-over” the boundary  the trajectories obtained are deﬁcient
in two major ways. They depart heavily from the original dynamics
and do not terminate at the desired attractors. This is due to the fact
that the function h(x) used to modulate the DS was designed solely
for classiﬁcation and contained no information about the dynamics
of the two original DS. In other words  the vector ﬁeld given by
∇h(x) was not aligned with the ﬂow of the training trajectories and
the stationary points of the modulation function did not coincide
with the desired attractors.

0.5

1.5

2.5

1

2

0

2

1

1.5

0.5

In subsequent sections  we show how we can learn a new modu-
lation function which takes into account the three issues we high-
lighted in this preliminary discussion. We will seek a system that
a) ensures strict classiﬁcation across regions of attraction (ROA)
for each DS  b) follows closely the dynamics of each DS in each ROA and c) ensures that all trajec-
tories in each ROA reach the desired attractor. Satisfying requirements a) and b) above is equivalent
to performing classiﬁcation and regression simultaneously. We take advantage of the fact that the
optimization in support vector classiﬁcation and support vector regression have the same form to
phrase our problem in a single constrained optimization framework. In next sections  we show that
in addition to the usual SVM support vectors (SVs)  the resulting modulation function is composed
of an additional class of SVs. We geometrically analyze the effect of these new support vectors on
the resulting dynamics. While this preliminary discussion considered solely binary classiﬁcation 
we will now extend the problem to multi-class classiﬁcation.

Figure 3: Modulated trajs.

3 Problem Formulation

The N -dimensional state space of the system represented by x ∈ RN is partitioned into M dif-
ferent classes  one for each of the M motions to be combined. We collect trajectories in the state
space  yielding a set of P data points {xi; ˙xi; li}i=1...P where li ∈ {1  2  · · ·   M } refers to the
class label of each point2. To learn the set of modulation functions {hm(x)}m=1...M   we pro-
ceed recursively. We learn each modulation function in a one-vs-all classiﬁer scheme and then

1SVM classiﬁer function is bounded if the Radial Basis Function (rbf) is used as kernel.
2Bold faced fonts represent vectors. xi denotes the i-th vector and xi denotes the i-th element of vector x.

3

compute the ﬁnal modulation function ˜h(x) = max
hm(x). In the multi-class setting  the be-
m=1···M
havior of avoiding boundaries is obtained if the trajectories move along increasing values of the
function ˜h(x). To this effect  the deﬂection term λ(x) presented in the binary case 1 becomes
λ(x) = max(cid:16)ǫ  ∇˜h(x)T ˙xO(cid:17) ; ∀x ∈ RN . Next  we describe the procedure for learning a single
hm(x) function.
We follow classical SVM formulation and lift the data into a higher dimensional feature space
through the mapping φ : RN 7→ RF where F denotes the dimension of the feature space. We
also assume that each function hm(x) is linear in feature space  i.e.  hm(x) = wT φ(x) + b where
w ∈ RF   b ∈ R. We label the current (m − th) motion class as positive and all others negative such
that the set of labels for the current sub-problem is given by

yi = (cid:26) +1 if li = m
−1 if li 6= m

;

i = 1 · · · P.

Also  the set indexing the positive class is then deﬁned as I+ = {i : i ∈ [1  P ]; li = m}. With this 
we formalize the three constraints explained in Section 2 as:
Region separation: Each point must be classiﬁed correctly yields P constraints:

yi(cid:0)wT φ(xi) + b(cid:1) ≥ 1 ∀i = 1...P.

(3)

Lyapunov constraint: To ensure that the modulated ﬂow is aligned with the training trajectories 
the gradient of the modulation function must have a positive component along the velocities at the
data points. That is 

∇hm(xi)T ˆ˙xi = wT J(xi)ˆ˙xi ≥ 0 ∀i ∈ I+

(4)
where J ∈ RF×N is the Jacobian matrix given by J = [ ∇φ1(x)∇φ2(x) · · · ∇φF (x) ]T and
ˆ˙xi = ˙xi/k ˙xik is the normalized velocity at the i − th data point.
Stability: Lastly  the gradient of the modulation function must vanish at the attractor of the positive
class x∗. This constraint can be expressed as

∇hm(x∗)T ei = wT J(x∗)ei = 0 ∀i = 1...N

(5)

where the set of vectors {ei}i=1···N is the canonical basis of RN .
3.1 Primal & Dual forms

As in the standard SVM [18]  we optimize for maximal margin between the positive and negative
class  subject to constraints 3-5 above. This can be formulated as:

min
w ξi

1
2

kwk2 + C Xi∈I+

ξi

subject to

yi(cid:0)wT φ(xi) + b(cid:1) ≥ 1
wT J(xi)ˆ˙xi + ξi > 0
ξi > 0
wT J(x∗)ei = 0

∀i = 1 · · · P
∀i ∈ I+
∀i ∈ I+
∀i = 1 · · · N

.

(6)




Here ξi ∈ R are slack variables that relax the Lyapunov constraint in Eq. 4. We retain these in
our formulation to accommodate noise in the data representing the dynamics. C ∈ R+ is a penalty
parameter for the slack variables. The Lagrangian for the above problem can be written as

L(w  b  α  β  γ) =

1
2

kwk2 + C Xi∈I+

P

ξi − Xi∈I+
− Xi∈I+

µiξi −

Xi=1

αi(cid:0)yi(wT φ(xi) + b) − 1(cid:1)

βi(cid:16)wT J(xi)ˆ˙xi + ξi(cid:17) +

N

Xi=1

γiwT J(x∗)ei

(7)

where αi  βi  µi  γi are the Lagrange multipliers with αi  βi  µi ∈ R+ and γi ∈ R. Employing a
similar analysis as in the standard SVM  it can be shown that the corresponding dual is given by the
constrained quadratic program:

min
α β γ

1

2 hαT βT γ Ti


K
GT
−GT

G
H

∗ −HT

∗


−αT 1 subject to


0 ≤ αi

0 ≤ βi ≤ C
PP
i=1 αiyi = 0

∀i = 1...P
∀i ∈ I+

−G∗
−H∗
H∗∗







α
β
γ

4

where 1 ∈ RP is a vector with all entries equal to one. Let k : RN × RN 7→ R represents the kernel
function such that k(x1  x2) = φT (x1)φ(x2). The matrices K ∈ RP×P   G ∈ RP×|I+|  G∗ ∈
RP×N   H ∈ R|I+|×|I+|  H∗ ∈ R|I+|×N   H∗∗ ∈ RN×N can be expressed in terms of the kernel
function and its ﬁrst and second order derivatives:

(K)ij = yiyjk(xi  xj )

(G)ij = yi(cid:16) ∂k(xi xj)
(G∗)ij = yi(cid:16) ∂k(xi x

∂x∗

∂xj

∗)

(cid:17)T
(cid:17)T

;

;

;

ˆ˙xj

ej

(H)ij = ˆ˙xT
i
(H∗)ij = ˆ˙xT
(H∗∗)ij = eT

i

i

∂ 2k(xi xj)

∂xi∂xj
∂ 2k(xi x
∂xi∂x∗

∗)

∂ 2k(x

∗ x
∂x∗∂x∗

∗)

ˆ˙xj

ej

ej




(8)

where (.)ij denotes the i  j −th entry of the corresponding matrix. Due to space constraints  detailed
development of the dual and proof of the above relations are given in appendices A and B of the
supplement material.
Note that since the matrices K  H and H∗∗ are symmetric  the overall Hessian matrix for the resulting
quadratic program is also symmetric. However  unlike the standard SVM dual  it may not be positive
deﬁnite resulting in multiple solutions to the above problem. In our implementation  we use the
interior point solver IPOPT [19] to ﬁnd a local optimum. We initialize the iterations using the α
found by running ﬁrst a standard SVM classiﬁcation problem. All entries of β and γ are set to 03.
The solution to the above problem yields a modulation function (see Eq. A.11 for proof) given by

hm(x) =

P

Xi=1

αiyik(x  xi) + Xi∈I+

βiˆ˙xT
i

∂k(x  xi)

∂xi

−

N

Xi=1

γieT
i

∂k(x  x∗)

∂x∗

+ b

(9)

which
sions

can
for

further

be
the Radial Basis

expanded

depending on

Function

(rbf)

the
kernel

choice
are

of
given

kernel.
Expan-
in Appendix C.

7

−

−1

0

8

1.5

0.5

1.5

0.5

0

1

0

1

1

7

7

1

0

1

0

−

−1

−

−0.5

2

2

7

1

−

0.1

0.1

7

7

5

5

5

5

7

2

2

7

0

8

5

5

7

7

5

5

7

5

5

7

−0.5

−1.5

−1.5

−2
−2

−2
−2

0.0
1

0.0
0

0.0
0

0.0
1

0.1
7

0.1
7

−1
(a) σ = 1

−1
(b) σ = 0.5

The modulation function 9 learned using the A-
SVM has noticeable similarities with the stan-
dard SVM classiﬁer function. The ﬁrst sum-
mation term is composed of the α support vec-
tors (α-SV) which act as support to the classi-
ﬁcation hyperplane. The second term entails a
new class of support vectors that perform a lin-
ear combination of the normalized velocity ˆ˙xi
at the training data points xi. These β sup-
port vectors (β-SVs) collectively contribute to
the fulﬁllment of the Lyapunov constraint by
introducing a positive slope in the modulation
function value along the directions ˆ˙xi. Figure 4
shows the inﬂuence of a β-SV for the rbf kernel
k(xi  xj) = e1/2σ2kxi−xjk2 with xi at the origin
and ˆ˙xi = [ 1√2
]T . It can be seen that the smaller the kernel width σ  the steeper the slope. The
third summation term is a non-linear bias  which does not depend on the chosen support vectors  and
performs a local modiﬁcation around the desired attractor x∗ to ensure that the modulation function
has a local maximum at that point. b is the constant bias which normalizes the classiﬁcation margins
as −1 and +1. We calculate its value by making use of the fact that for all the α-SV xi  we must
have yihm(xi) = 1. We use average of the values obtained from the different support vectors.
Figure 5 illustrates the effects of the support vectors in a 2D example by progressively adding them
and overlaying the resulting DS ﬂow in each case. The value of the modulation function hm(x) is
shown by the color plot (white indicates high values). As the β-SVs are added in Figs. 5(b)-(d) 
they push the ﬂow of trajectories along their associated directions. In Figs. 5(e)-(f)  adding the two
γ terms shifts the location of the maximum of the modulation function to coincide with the desired
attractor. Once all the SVs have been taken into account  the streamlines of the resulting DS achieve
the desired criteria  i.e.  they follow the training trajectories and terminate at the desired attractor.

Figure 4: Isocurves of f (x) = ˆ˙xT
i
xi = [0 0]T   ˆ˙xi = [ 1√2

]T for the rbf kernel.

∂k(x xi)

1√2

1√2

∂xi

at

3Source code for learning is available at http://asvm.epfl.ch

5

0.4

Y

−0.2

0.4

Y

−0.2

0.4

Y

−0.2

−0.8

−0.5

0
X

(a) α only

0.5

−0.8

−0.5

0
X

(b) α and β

0.5

−0.8

−0.5

0
X

0.5

(c) α and β

0.4

Y

−0.2

0.4

Y

−0.2

0.4

Y

−0.2

Modulated Streamlines
Training Data

 

α SV
β SV

Obtained attractor
Desired attractor

−0.8

−0.5

0
X

0.5

−0.8

−0.5

0
X

0.5

−0.8

 

−0.5

0
X

0.5

(d) α and β

(e) α  β and γ1

(f) α  β  γ1 and γ2

Figure 5: Progressively adding support vectors to highlight their effect on shaping the dynamics
of the motion. (a) α-SVs largely affect classiﬁcation. (b)-(d) β-SVs guide the ﬂow of trajectories
along their respective associated directions ˆ˙xi shown by arrows. (e)-(f) The 2 γ terms force the local
maximum of the modulation function to coincide with the desired attractor along the X and Y axes
respectively.

4 Results

In this section  we validate the presented A-SVM model on 2D (synthetic) data and on a robotic
simulated experiment using a 7 degrees of freedom (DOF) KUKA-LWR arm mounted on a 3-DOF
Omnirob base to catch falling objects. A video of the robotic experiment - simulated and real -
is provided in Annexes. Next  we present a cross-validation analysis of the error introduced by
the modulation in the original dynamics. A sensitivity analysis of the region of attraction of the
resulting dynamical system with respect to the model parameters is also presented. We used the
rbf kernel for all the results presented in this section. As discussed in Section 2  the RBF kernel is
advantageous as it ensures that the function hm(x) is bounded. To generate an initial estimate of
each individual dynamical system  we used the technique proposed in [7].

2D Example Figure 6(a) shows a synthetic example with 4 motion classes  each generated
from a different closed form dynamics and containing 160 data points. The color plot indicates the
value of the combined modulation function ˜h(x) = max
hm(x) where each of the functions
m=1···M
hm(x) are learned using the presented A-SVM technique. A total of 9 support vectors were
obtained which is < 10% of the number of training data points. The trajectories obtained after
modulating the original dynamical systems ﬂow along increasing values of the modulation function 
thereby bifurcating towards different attractors at the region boundaries. Unlike the dynamical
system in Fig. 3  the ﬂow here is aligned with the training trajectories and terminates at the desired
attractors. To recall  this is made possible thanks to the additional constraints (Eq. 4 and 5) in our
formulation.

In a second example  we tested the ability of our model to accommodate a higher density of
attractors. We created 8 synthetic dynamics by capturing motion data using a screen mouse. Figure
1 shows the resulting 8 attractor system.

Error Analysis As formulated in Eq. 6  the Lyapunov constraints admit some slack  which
allows the modulation to introduce slight deviations from the original dynamics. Here we
statistically analyze this error via 5-fold cross validation.
In the 4 attractor problem presented

6

Training Data

Modulated Trajs.

Attractors

 

1

0.5

0

2

1

0

−1

−2

y

 

−3

−2

−1

0

x

1

2

(a) Combined ﬂow

40

30

20

10

r
o
r
r
E
g
n

i
t
s
e
T
%

 

0
0

Class 1
Class 2
Class 3
Class 4

 

1

0.8

 

Training
Testing

r
o
r
r
E
%

0.6

0.4

5

10
σ

15

20

0.2

0

 

Class 1 Class 2 Class 3 Class 4

(b) Cross validation error

(c) Best case errors

Figure 6: Synthetic 2D case with 4-attractors.

k ˙xik

× 100Ei:li=m

em = D k ˙xi− ˜f (xi)k

above  we generate a total of 10 trajectories per motion class and use 2:3 training to testing ratio
for cross validation. We calculate the average percentage error between the original velocity
(read off from the data) and the modulated velocity (calculated using 2) for the m − th class as
where < . > denotes average over the indicated range. Figure
6(b) shows the cross validation error (mean and standard deviation over the 5 folds) for a range
of values of kernel width. The general trend revealed here is that for each class of motion  there
exists a band of optimum values of the kernel width for which the testing error is the smallest. The
region covered by this band of optimal values may vary depending on the relative location of the
attractors and other data points. In Fig. 6(a)  motion classes 2 (upper left) and 4 (upper right) are
better ﬁtted and show less sensitivity to the choice of kernel width than classes 1 (lower left) and
3 (lower right). We will show later in this section that this is correlated to the distance between
the attractors. A comparison of testing and training errors for the least error case is shown in Fig.
6(c). We see that the testing errors for all the classes in the best case scenario are less than 1%.

0.5

0

 

 

−0.5

h(x) = const

ROA boundary
−1
Meshed contour
Actual attractor
Spurious attractor

Sensitivity analysis The partitioning of space created by our
method results in M regions of attraction (ROA) for each of our
M attractors. To assess the size of these regions and the existence
of spurious attractors  we adopt an empirical approach. For each
class  we compute the isosurfaces of the corresponding modulation
function hm(x) in the range [0  hm(x∗)]. These hypersurfaces
incrementally span the volume of the m − th region around its
attractor. We mesh each of these test surfaces and compute trajec-
tories starting from the obtained mesh-points  looking for spurious
attractors. hROA is the isosurface of maximal value that encloses
no spurious attractor and marks the ROA of the corresponding
motion dynamics. We use the example in Fig. 5 to illustrate this
process. Figure 7 shows a case where one spurious attractor is
detected using a larger test surface (dotted line) whereas the actual
ROA (solid line) is smaller. Once hROA is calculated  we deﬁne
the size of ROA as rROA = (h(x∗) − hROA)/h(x∗). rROA = 0
when no trajectory except those originating at the attractor itself  lead to the attractor. rROA = 1
when the ROA is bounded by the isosurface h(x) = 0. The size of the rROA is affected by both
the choice of kernel width and the distance across nearby attractors. This is illustrated in Fig. 9
using data points from class 1 of Fig. 6(a) and translating the attractors so that they are either very
far apart (left  distance datt = 1.0) or very close to one another (right  datt = 0.2). As expected 
rROA increases as we reach the optimal range of parameters. Furthermore  when the attractors are
farther apart  high values of rROA are obtained for a larger range of values of the kernel width  i.e. 
the model is less sensitive to the chosen kernel width. With smaller distance between the attractors
(Fig. 9(b))  only a small deviation from the optimum kernel width results in a considerable loss in
rROA  exhibiting high sensitivity to the model parameter.

Figure 7: Test
trajectories
generated from several points
on an isocurve (dotted line) to
determine spurious attractors.

−0.5

0

0.5

3D Example We validated our method on a real world 3D problem. The attractors here rep-
resent manually labeled grasping points on a pitcher. The 3D model of the object was taken
from the ROS IKEA object library. We use the 7-DOF KUKA-LWR arm mounted on the 3-DOF

7

0.4

0.2

0

−0.2

−0.2

0

0.2

0.1

0

−0.1

0.4 −0.2

0.4

0.2

0

−0.2

−0.2

0

0.2

0.4

0.1

0
−0.2

−0.1

(a) Training data

(b) hm(x) = 0

(c) Trajectory 1

(d) Trajectory 2

(e) Combined ﬂow

Figure 8: 3D Experiment. (a) shows training trajectories for three manually chosen grasping points.
(b) shows the isosurfaces hm(x) = 0; m = 1  2  3 along with the locations of the corresponding at-
tractors. In (c) and (d)  the robot executes the generated trajectories starting from different positions
and hence converging to different grasping points. (e) shows the complete ﬂow of motion.

KUKA-Omnirob base for executing the modulated Cartesian trajectories in simulation. We control
all 10 DOF of the robot using the damped least square inverse kinematics. Training data for this
implementation was obtained by recording the end-effector positions xi ∈ R3 from kinesthetic
demonstrations of reach-to-grasp motions directed towards these grasping points  yielding a 3-class
problem (see Fig. 8(a)). Each class was represented by 75 data points. Figure 8(b) shows the
isosurfaces hm(x) = 0; m ∈ {1  2  3} learned using the presented method. Figures 8(c)-(d) show
the robot executing two trajectories when started from two different locations and converging to a
different attractor (grasping point). Figure 8(e) shows the ﬂow of motion around the object. Note
that the time required to generate each trajectory point is O(S) where S denotes the total number of
support vectors in the model. In this particular example with a total of 18 SVs  the trajectory points
were generated at 1000 Hz which is well suited for real-time control. Such a fast generative model
allows the robot to switch on-the-ﬂy between the attractors and adapt to real-time perturbations
in the object or the end-effector pose  without any re-planning or re-learning. Results for another
object (champagne glass) are included in Appendix D (Fig. D.1). A video illustrating how the robot
exploits multiple attractors to catch one of the grasping points on the object as it falls down is also
provided in the supplementary material.

5 Conclusions

1400

0.6

C

0.8

0.6

0.8

1200

1000

800

C

 

1.0

 

1.0

1400

1200

600

400

1000

In this work  we presented the A-
SVM model
for combining non-
linear dynamical systems through a
partitioning of the space. We refor-
mulated the optimization framework
of SVM to encapsulate constraints
that ensure accurate reproduction of
the dynamics of motion. The new
set of constraints result in a new class
of support vectors that exploit partial
derivatives of the kernel function to
align the ﬂow of trajectories with the
training data. The resulting model
behaves as a multi-stable DS with attractors at the desired locations. Each of the classiﬁed re-
gions are forward invariant w.r.t the learned DS. This ensures that the trajectories do not cross over
region boundaries. We validated the presented method on synthetic motions in 2D and 3D grasping
motions on real objects. Results show that even though spurious attractors may occur  in practice
they can be avoided by a careful choice of model parameters through grid search. The applicability
of the method for real-time control of a 10-DOF robot was also demonstrated.

Figure 9: Variation of rROA with varying model parameters.

(b) datt = 0.2

(a) datt = 1.0

1

σ

1

σ

800

600

400

200

0.4

0.2

0.4

0.2

0

2

0

2

200

 

 

0.5

0.5

1.5

1.5

Acknowledgments

This work was supported by EU Project First-MM (FP7/2007-2013) under grant agreement number
248258. The authors would also like thank Prof. Franc¸ois Margot for his insightful comments on
the technical material.

8

References

[1] Peter Pastor  Heiko Hoffmann  Tamim Asfour  and Stefan Schaal. Learning and generalization of motor
skills by learning from demonstration. In Robotics and Automation  2009. ICRA ’09. IEEE International
Conference on  pages 763 –768  may 2009.

[2] G. Sch¨oner and M. Dose. A dynamical systems approach to task-level system integration used to plan

and control autonomous vehicle motion. Robotics and Autonomous Systems  10(4):253–267  1992.

[3] G. Sch¨oner  M. Dose  and C. Engels. Dynamics of behavior: Theory and applications for autonomous

robot architectures. Robotics and Autonomous Systems  16(2):213–245  1995.

[4] L.P. Ellekilde and H.I. Christensen. Control of mobile manipulator using the dynamical systems approach.
In Robotics and Automation  2009. ICRA’09. IEEE International Conference on  pages 1370–1376. IEEE 
2009.

[5] H. Reimann  I. Iossiﬁdis  and G. Sch¨oner. Autonomous movement generation for manipulators with
In Robotics and Automation

multiple simultaneous constraints using the attractor dynamics approach.
(ICRA)  2011 IEEE International Conference on  pages 5470–5477. IEEE  2011.

[6] K.R. Dixon and P.K. Khosla. Trajectory representation using sequenced linear dynamical systems. In
Robotics and Automation  2004. Proceedings. ICRA’04. 2004 IEEE International Conference on  vol-
ume 4  pages 3925–3930. IEEE  2004.

[7] S. M. Khansari-Zadeh and Aude Billard. Learning Stable Non-Linear Dynamical Systems with Gaussian

Mixture Models. IEEE Transaction on Robotics  2011.

[8] A. Shukla and A. Billard. Coupled dynamical system based armhand grasping model for learning fast

adaptation strategies. Robotics and Autonomous Systems  60(3):424 – 440  2012.

[9] H. Hoffmann. Target switching in curved human arm movements is predicted by changing a single control

parameter. Experimental brain research  208(1):73–87  2011.

[10] C. Rasmussen. Gaussian processes in machine learning. Advanced Lectures on Machine Learning  pages

63–71  2004.

[11] S. Schaal  C.G. Atkeson  and S. Vijayakumar. Scalable techniques from nonparametric statistics for real

time robot learning. Applied Intelligence  17(1):49–60  2002.

[12] Auke Jan Ijspeert  Jun Nakanishi  and Stefan Schaal. Movement imitation with nonlinear dynamical sys-
tems in humanoid robots. In In IEEE International Conference on Robotics and Automation (ICRA2002 
pages 1398–1403  2002.

[13] A. Fuchs and H. Haken. Pattern recognition and associative memory as dynamical processes in a syner-
getic system. i. translational invariance  selective attention  and decomposition ofscenes. Biol. Cybern. 
60:17–22  November 1988.

[14] A.N. Michel and J.A. Farrell. Associative memories via artiﬁcial neural networks. Control Systems

Magazine  IEEE  10(3):6 –17  apr 1990.

[15] H. Jaeger  M. Lukosevicius  D. Popovici  and U. Siewert. Optimization and applications of echo state

networks with leaky-integrator neurons. Neural Networks  20(3):335–352  2007.

[16] J. Lee. Dynamic gradient approaches to compute the closest unstable equilibrium point for stability region
estimate and their computational limitations. Automatic Control  IEEE Transactions on  48(2):321–324 
2003.

[17] H.D. Chiang and C.C. Chu. A systematic search method for obtaining multiple local optimal solutions of
nonlinear programming problems. Circuits and Systems I: Fundamental Theory and Applications  IEEE
Transactions on  43(2):99–109  1996.

[18] B. Sch¨olkopf and A.J. Smola. Learning with kernels: Support vector machines  regularization  optimiza-

tion  and beyond. MIT press  2001.

[19] Andreas Wchter and Lorenz T. Biegler. On the implementation of an interior-point ﬁlter line-search

algorithm for large-scale nonlinear programming. Mathematical Programming  106:25–57  2006.

9

,Kush Bhatia
Prateek Jain
Purushottam Kar