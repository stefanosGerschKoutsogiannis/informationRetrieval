2008,An Extended Level Method for Efficient Multiple Kernel Learning,We consider the problem of multiple kernel learning (MKL)  which can be formulated as a convex-concave problem. In the past  two efficient methods  i.e.  Semi-Infinite Linear Programming (SILP) and Subgradient Descent (SD)  have been proposed for large-scale multiple kernel learning. Despite their success  both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution  and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work  we extend the level method  which was originally designed for optimizing non-smooth objective functions  to convex-concave optimization  and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method.,An Extended Level Method for

Efﬁcient Multiple Kernel Learning

Zenglin Xu†

Rong Jin‡

Irwin King†

Michael R. Lyu†

† Dept. of Computer Science & Engineering

The Chinese University of Hong Kong

Shatin  N.T.  Hong Kong

{zlxu  king  lyu}@cse.cuhk.edu.hk

‡Dept. of Computer Science & Engineering

Michigan State University
East Lansing  MI  48824

rongjin@cse.msu.edu

Abstract

We consider the problem of multiple kernel learning (MKL)  which can be for-
mulated as a convex-concave problem. In the past  two efﬁcient methods  i.e. 
Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD)  have
been proposed for large-scale multiple kernel learning. Despite their success  both
methods have their own shortcomings: (a) the SD method utilizes the gradient of
only the current solution  and (b) the SILP method does not regularize the approx-
imate solution obtained from the cutting plane model. In this work  we extend
the level method  which was originally designed for optimizing non-smooth ob-
jective functions  to convex-concave optimization  and apply it to multiple kernel
learning. The extended level method overcomes the drawbacks of SILP and SD
by exploiting all the gradients computed in past iterations and by regularizing the
solution via a projection to a level set. Empirical study with eight UCI datasets
shows that the extended level method can signiﬁcantly improve efﬁciency by sav-
ing on average 91.9% of computational time over the SILP method and 70.3%
over the SD method.

1 Introduction

Kernel learning [5  9  7] has received a lot of attention in recent studies of machine learning. This
is due to the importance of kernel methods in that kernel functions deﬁne a generalized similarity
measure among data. A generic approach to learning a kernel function is known as multiple kernel
learning (MKL) [5]: given a list of base kernel functions/matrices  MKL searches for the linear com-
bination of base kernel functions which maximizes a generalized performance measure. Previous
studies [5  14  13  4  1] have shown that MKL is usually able to identify appropriate combination of
kernel functions  and as a result to improve the performance.

A variety of methods have been used to create base kernels. For instance  base kernels can be created
by using different kernel functions; they can also be created by using a single kernel function but
with different subsets of features. As for the performance measures needed to ﬁnd the optimal ker-
nel function  several measures have been studied for multiple kernel learning  including maximum
margin classiﬁcation errors [5]  kernel-target alignment [4]  and Fisher discriminative analysis [13].

The multiple kernel learning problem was ﬁrst formulated as a semi-deﬁnite programming (SDP)
problem by [5]. An SMO-like algorithm was proposed in [2] in order to solve medium-scale prob-
lems. More recently  a Semi-Inﬁnite Linear Programming (SILP) approach was developed for
MKL [12]. SILP is an iterative algorithm that alternates between the optimization of kernel weights
and the optimization of the SVM classiﬁer. In each step  given the current solution of kernel weights 
it solves a classical SVM with the combined kernel; it then constructs a cutting plane model for the
objective function and updates the kernel weights by solving a corresponding linear programming

problem. Although the SILP approach can be employed for large scale MKL problems  it often suf-
fers from slow convergence. One shortcoming of the SILP method is that it updates kernel weights
solely based on the cutting plane model. Given that a cutting plane model usually differs signif-
icantly from the original objective function when the solution is far away from the points where
the cutting plane model is constructed  the optimal solution to the cutting plane model could be
signiﬁcantly off target. In [10]  the authors addressed the MKL problems by a simple Subgradient
Descent (SD) method. However  since the SD method is memoryless  it does not utilize the gradi-
ents computed in previous iterations  which could be very useful in boosting the efﬁciency of the
search.

To further improve the computational efﬁciency of MKL  we extended the level method [6]  which
was originally designed for optimizing non-smooth functions  to the optimization of convex-concave
problems. In particular  we regard the MKL problem as a saddle point problem. In the present work 
similar to the SILP method  we construct in each iteration a cutting plane model for the target
objective function using the solutions to the intermediate SVM problems. A new solution for kernel
weights is obtained by solving the cutting plane model. We furthermore adjust the new solution via a
projection to a level set. This adjustment is critical in that it ensures on one hand the new solution is
sufﬁciently close to the current solution  and on the other hand the new solution signiﬁcantly reduces
the objective function. We show that the extended level method has a convergence rate of O(1/ε2)
for a ε-accurate solution. Although this is similar to that of the SD method  the extended level
method is advantageous in that it utilizes all the gradients that have been computed so far. Empirical
results with eight UCI datasets show that the extended level method is able to greatly improve the
efﬁciency of multiple kernel learning in comparison with the SILP method and the SD method.

The rest of this paper is organized as follows.
In section 2  we review the efﬁcient algorithms
that have been designed for multiple kernel learning. In section 3  we describe the details of the
extended level method for MKL  including a study of its convergence rate. In section 4  we present
experimental results by comparing both the effectiveness and the efﬁciency of the extended level
method with the corresponding measures of SILP and SD. We conclude this work in section 5.

2 Related Work

Let X = (x1  . . .   xn) ∈ Rn×d denote the collection of n training samples that are in a d-
dimensional space. We further denote by y = (y1  y2  . . .   yn) ∈ {−1  +1}n the binary class labels
for the data points in X. We employ the maximum margin classiﬁcation error  an objective used
in SVM  as the generalized performance measure. Following [5]  the problem of multiple kernel
learning for classiﬁcation in the primal form is deﬁned as follows:

min
p∈P

max
α∈Q

f (p  α) = α⊤e −

1
2

(α ◦ y)⊤ m
Xi=1

piKi! (α ◦ y) 

(1)

where P = {p ∈ Rm : p⊤e = 1  0 ≤ p ≤ 1} and Q = {α ∈ Rn : α⊤y = 0  0 ≤ α ≤ C}
are two solid convex regions  denoting the set of kernel weights and the set of SVM dual variables 
i=1 is a group
respectively. Here  e is a vector of all ones  C is the trade-off parameter in SVM  {Ki}m
of base kernel matrices  and ◦ deﬁnes the element-wise product between two vectors. It is easy to
verify that f (p  α) is convex on p and concave on α. Thus the above optimization problem is indeed
a convex-concave problem. It is important to note that the block-minimization formulation of MKL
presented in [10  2] is equivalent to (1).

A straightforward approach toward solving the convex-concave problem in (1) is to transform it
into a Semi-deﬁnite Programming (SDP) or a Quadratically Constrained Quadratic Programming
(QCQP) [5  2]. However  given their computational complexity  they cannot be applied to large-
scale MKL problems. Recently  Semi-inﬁnite Linear Programming (SILP) [12] and Subgradient
Descent (SD) [10] have been applied to handle large-scale MKL problems. We summarize them
into a uniﬁed framework in Algorithm 1. Note that a superscript is used to indicate the index of
iteration  a convention that is used throughout this paper. We use [x]t to denote x to the power of t
in the case of ambiguity.

As indicated in Algorithm 1  both methods divide the MKL problem into two cycles: the inner cycle
solves a standard SVM problem to update α  and the outer cycle updates the kernel weight vector

Algorithm 1 A general framework for solving MKL
1: Initialize p0 = e/m and i = 0
2: repeat
3:
4:
5:
6: until ∆i ≤ ε

Solve the dual of SVM with kernel K =Pm
Update kernel weights by pi+1 = arg min{ϕi(p; α) : p ∈ P}
Update i = i + 1 and calculate stopping criterion ∆i

j=1 pi

jKj and obtain optimal solution αi

p. They differ in the 4th step in Algorithm 1: the SILP method updates p by solving a cutting
plane model  while the SD method updates p using the subgradient of the current solution. More
speciﬁcally  ϕi(p; α) for SILP and SD are deﬁned as follows:

ϕi

SILP (p; α) = min

ϕi

SD(p; α) =

ν {ν : ν ≥ f (p  αj)  j = 0  . . .   i} 
1
2kp − pik2

2 + γi(p − pi)⊤∇pf (pi  αi) 

(2)

(3)

where γi is the step size that needs to be decided dynamically (e.g.  by a line search). ∇pf (pi  αi) =
− 1
2 [(αi◦ y)⊤K1(αi◦ y)  . . .   (αi◦ y)⊤Km(αi◦ y)]⊤ denotes the subgradient of f (· ·) with respect
to p at (pi  αi). Comparing the two methods  we observe

• In SILP  the cutting plane model ϕSILP (p) utilizes all the {αj}i
tions. In contrast  SD only utilizes αi of the current solution pi.
• SILP updates the solution for p based on the cutting plane model ϕSILP (p). Since the
cutting plane model is usually inaccurate when p is far away from {pj}i
j=1  the updated
solution p could be signiﬁcantly off target [3]. In contrast  a regularization term kp −
2/2 is introduced in SD to prevent the new solution being far from the current one  pi.
pik2
The proposed level method combines the strengths of both methods. Similar to SILP  it utilizes the
gradient information of all the iterations; similar to SD  a regularization scheme is introduced to
prevent the updated solution from being too far from the current solution.

j=1 obtained in past itera-

3 Extended Level Method for MKL

We ﬁrst introduce the basic steps of the level method  followed by the extension of the level method
to convex-concave problems and its application to MKL.

3.1

Introduction to the Level Method

i

The level method [6] is from the family of bundle methods  which have recently been employed to
efﬁciently solve regularized risk minimization problems [11]. It is an iterative approach designed
for optimizing a non-smooth objective function. Let f (x) denote the convex objective function
to be minimized over a convex domain G. In the ith iteration  the level method ﬁrst constructs a
lower bound for f (x) by a cutting plane model  denoted by gi(x). The optimal solution  denoted
i and a
by ˆxi  that minimizes the cutting plane model gi(x) is then computed. An upper bound f
lower bound f
are computed for the optimal value of the target optimization problem based on
ˆxi. Next  a level set for the cutting plane model gi(x) is constructed  denoted by Li = {x ∈ G :
+ (1 − λ)f i} where λ ∈ (0  1) is a tradeoff constant. Finally  a new solution xi+1
gi(x) ≤ λf
is computed by projecting xi onto the level set Li. It is important to note that the projection step 
serving a similar purpose to the regularization term in SD  prevents the new solution xi+1 from
being too far away from the old one xi. To demonstrate this point  consider a simple example
minx{f (x) = [x]2 : x ∈ [−4  4]}. Assume x0 = −3 is the initial solution. The cutting plane
model at x0 is g0(x) = 9 − 6(x + 3). The optimal solution minimizing g0(x) is ˆx1 = 4. If we
directly take ˆx1 as the new solution  as SILP does  we found it is signiﬁcantly worse than x0 in
terms of [x]2. The level method alleviates this problem by projecting x0 = −3 to the level set
L0 = {x : g0(x) ≤ 0.9[x0]2 + 0.1g0(ˆx1) −4 ≤ x ≤ 4} where λ = 0.9. It is easy to verify that

i

the projection of x0 to L0 is x1 = −2.3  which signiﬁcantly reduces the objective function f (x)
compared with x0.

3.2 Extension of the Level Method to MKL

We now extend the level method  which was originally designed for optimizing non-smooth func-
tions  to convex-concave optimization. First  since f (p  α) is convex in p and concave in α  accord-
ing to van Neuman Lemma  for any optimal solution (p∗  α∗) we have
f (p  α) ≥ f (p∗  α∗) ≥ f (p∗  α) = min

f (p  α∗) = max
α∈Q

f (p  α).

(4)

p∈P

This observation motivates us to design an MKL algorithm which iteratively updates both the lower
and the upper bounds for f (p  α) in order to ﬁnd the saddle point. To apply the level method  we
ﬁrst construct the cutting plane model. Let {pj}i
j=1 denote the solutions for p obtained in the last
i iterations. Let αj = arg maxα∈Q f (pj  α) denote the optimal solution that maximizes f (pj  α).
We construct a cutting plane model gi(p) as follows:

gi(p) = max
1≤j≤i

f (p  αj).

(5)

We have the following proposition for the cutting plane model gi(x)
Proposition 1. For any p ∈ P  we have (a) gi+1(p) ≥ gi(p)  and (b) gi(p) ≤ maxα∈Q f (p  α).
Next  we construct both the lower and the upper bounds for the optimal value f (p∗  α∗). We deﬁne
two quantities f i and f

i as follows:

f i = min
p∈P

gi(p) and f

i

= min
1≤j≤i

f (pj  αj).

(6)

The following theorem shows that {f j}i
bounds for f (p∗  α∗).
Theorem 1. We have the following properties for {f j}i
i  and (c) f 1 ≤ f 2 ≤ . . . ≤ f i.
(b) f

j=1 and {f

≥ . . . ≥ f

≥ f

2

1

j

j=1 provide a series of increasingly tight
}i

j=1 and {f

j

j=1: (a) f i ≤ f (p∗  α∗) ≤ f
}i

i 

Proof. First  since gi(p) ≤ maxα∈Q f (p  α) for any p ∈ P  we have
f (p  α).

f i = min
p∈P

gi(p) ≤ min

p∈P

max
α∈Q

Second  since f (pj  αj) = max
α∈Q

f (pj  α)  we have

i

f

= min
1≤j≤i

f (pj  αj) =

min

p∈{p1 ... pi}

max
α∈Q

f (p  α) ≥ min

p∈P

max
α∈Q

f (p  α) = f (p∗  α∗).

Combining the above results  we have (a) in the theorem. It is easy to verify (b) and (c).

We furthermore deﬁne the gap ∆i as

∆i = f

i

− f i.

The following corollary indicates that the gap ∆i can be used to measure the sub-optimality for
solution pi and αi.
Corollary 2. (a) ∆j ≥ 0  j = 1  . . .   i  (b) ∆1 ≥ ∆2 ≥ . . . ≥ ∆i  (c) |f (pj  αj)−f (p∗  α∗)| ≤ ∆i
It is easy to verify these three properties of ∆i in the above corollary using the results of Theorem 1.
In the third step  we construct the level set Li using the estimated bounds f

i and f i as follows:

Li = {p ∈ P : gi(p) ≤ ℓi = λf

i

+ (1 − λ)f i} 

(7)

where λ ∈ (0  1) is a predeﬁned constant. The new solution  denoted by pi+1  is computed as
the projection of pi onto the level set Li  which is equivalent to solving the following optimization
problem:

pi+1 = arg min

p

(cid:8)kp − pik2

2 : p ∈ P  f (p  αj) ≤ ℓi  j = 1  . . .   i(cid:9) .

(8)

Although the projection is regarded as a quadratic programming problem  it can often be solved ef-
ﬁciently because its solution is likely to be the projection onto one of the hyperplanes of polyhedron
Li. In other words  only very few linear constraints of L are active; most of them are inactive. This
sparse nature usually leads to signiﬁcant speedup of QP  similar to the solver of SVM. As we argue
in the last subsection  by means of the projection  we on the one hand ensure pi+1 is not very far
away from pi  and on the other hand ensure signiﬁcant progress is made in terms of gi(p) when the
solution is updated from pi to pi+1. Note that the projection step in the level method saves the effort
of searching for the optimal step size in SD  which is computationally expensive as will be revealed
later. We summarize the steps of the extended level method in Algorithm 2.

Algorithm 2 The Level Method for Multiple Kernel Learning
1: Initialize p0 = e/m and i = 0
2: repeat
3:
4:
5:
6:
7:
8: until ∆i ≤ ε

Solve the dual problem of SVM with K =Pm
Construct the cutting plane model gi(p) in (5)
Calculate the lower bound f i and the upper bound f
Compute the projection of pi onto the level set Li by solving the optimization problem in (8)
Update i = i + 1

j Kj to obtain the optimal solution αi

i in (6)  and the gap ∆i in (3.2)

j=1 pi

Finally  we discuss the convergence behavior of the level method. In general  convergence is guar-
anteed because the gap ∆i  which bounds the absolute difference between f (p∗  α∗) and f (pi  αi) 
monotonically decreases through iterations. The following theorem shows the convergence rate of
the level method when applied to multiple kernel learning.
Theorem 3. To obtain a solution p that satisﬁes the stopping criterion  i.e.  | maxα∈Q f (p  α) −
f (p∗  α∗)| ≤ ε  the maximum number of iterations N that the level method requires is bounded
as follows N ≤ 2c(λ)L2
(1−λ)2λ(2−λ) and L = 1
Λmax(Ki). The
operator Λmax(M ) computes the maximum eigenvalue of matrix M.

2√mnC 2 max

  where c(λ) =

1≤i≤m

ε2

1

Due to space limitation  the proof of Theorem 3 can be found in the long version of this paper.
Theorem 3 tells us that the convergence rate of the level method is O(1/ε2). It is important to note
that according to Information Based Complexity (IBC) theory  given a function family F(L) with a
ﬁxed Lipschitz constant L  O(1/ε2) is almost the optimal convergence rate that can be achieved for
any optimization method based on the black box ﬁrst order oracle. In other words  no matter which
optimization method is used  there always exists an function f (·) ∈ F(L) such that the convergence
rate is O(1/ε2) as long as the optimization method is based on a black box ﬁrst order oracle. More
details can be found in [8  6].

4 Experiments

We conduct experiments to evaluate the efﬁciency of the proposed algorithm for MKL in constrast
with SILP and SD  the two state-of-the-art algorithms for MKL.

4.1 Experimental Setup

We follow the settings in [10] to construct the base kernel matrices  i.e. 

single feature

• Gaussian kernels with 10 different widths ({2−3  2−2  . . .   26}) on all features and on each
• Polynomial kernels of degree 1 to 3 on all features and on each single feature.

Table 1: The performance comparison of three MKL algorithms. Here n and m denote the size of
training samples and the number of kernels  respectively.

SD

SILP

Level

Time(s)
Accuracy (%)
#Kernel

Time(s)
Accuracy (%)
#Kernel

Time(s)
Accuracy (%)
#Kernel

Time(s)
Accuracy (%)
#Kernel

Iono
33.5 ±11.6
92.1 ±2.0
26.9 ±4.0
Pima
39.4 ±8.8
76.9 ±1.9
16.6 ±2.2
Wpbc

7.8 ±2.4
77.0 ±2.9
19.5 ±2.8
Vote
23.7 ±9.7
95.7 ±1.0
14.0 ±3.6

n = 175 m = 442
1161.0 ±344.2

92.0 ±1.9
24.4 ±3.4

7.1 ±4.3
92.1±1.9
25.4±3.9

n = 384 m = 117

62.0 ±15.2
76.9 ±2.1
12.0 ±1.8

9.1 ±1.6
76.9±2.1
17.6±2.6

n = 198 m = 442
142.0 ±122.3

76.9 ±2.8
17.2 ±2.2

5.3 ±1.3
76.9±2.9
20.3±2.6

n = 218 m = 205

26.3 ±12.4
95.7 ±1.0
10.6 ±2.6

4.1 ±1.3
95.7±1.0
13.8±2.6

SD
Breast
47.4 ±8.9
96.6 ±0.9
13.1 ±1.7
Sonar
60.1 ±29.6
79.1 ±4.5
39.8 ±3.9
Heart
4.7 ±2.8
82.2 ±2.2
17.5 ±1.8
Wdbc
122.9±38.2
96.7 ±0.8
16.6 ±3.2

SILP

Level

n = 342 m = 117

54.2 ±9.4
96.6 ±0.8
10.6 ±1.1

4.6 ±1.0
96.6±0.8
13.3±1.5

n = 104 m = 793
1964.3±68.4

79.3 ±4.2
34.2 ±2.6

24.9±10.6
79.0±4.7
38.6±4.1

n = 135 m = 182
79.2 ±38.1
82.2 ±2.0
15.2 ±1.5

2.1 ±0.4
82.2±2.1
18.6±1.9

n = 285 m = 403
146.3 ±48.3
96.5 ±0.9
12.9 ±2.3

15.5±7.5
96.7±0.8
15.6±3.0

Each base kernel matrix is normalized to unit trace. The experiments are conducted on a PC with
3.2GHz CPU and 2GB memory. According to the above scheme of constructing base kernel matri-
ces  we select a batch of UCI data sets  with the cardinality and dimension allowed by the memory
limit of the PC  from the UCI repository for evaluation. We repeat all the algorithms 20 times for
each data set. In each run  50% of the examples are randomly selected as the training data and the
remaining data are used for testing. The training data are normalized to have zero mean and unit
variance  and the test data are then normalized using the mean and variance of the training data. The
regularization parameter C in SVM is set to 100 as our focus is to evaluate the computational time  as
justiﬁed in [10]. For a fair comparison among the MKL algorithms  we adopt the same stopping cri-
terion for all three algorithms under comparison: we adopt the duality gap criterion used in [10]  i.e. 
max
1≤i≤m
is less than 0.01 or the number of iterations larger than 500. We empirically initialize the parameter λ
to 0.9 and increase it to 0.99 when the ratio ∆i/ℓi is less than 0.01 for all experiments  since a larger
λ accelerates the projection when the solution is close to the optimal one. We use the SimpleMKL
toolbox [10] to implement the SILP and SD methods. The linear programming in the SILP method
and the auxiliary subproblems in the level method are solved using a general optimization toolbox
MOSEK (http://www.mosek.com). The toolbox for the level method can be downloaded from
http://www.cse.cuhk.edu.hk/˜zlxu/toolbox/level_mkl.html.

j=1 pjKj(cid:17) (α◦y)  and stop the algorithm when the criterion

(α◦y)⊤Ki(α◦y)−(α◦y)⊤(cid:16)Pm

4.2 Experimental Results

We report the following performance measures: prediction accuracy  training time  and the averaged
number of kernels selected. From Table 1  we observe that all algorithms achieve almost the same
prediction accuracy under the same stopping criterion. This is not surprising because all algorithms
are essentially trying to solve the same optimization problem. Regarding the computational efﬁ-
ciency  we observe that the time cost of the SILP approach is the highest among all the three MKL
algorithms. For datasets “Iono” and “Sonar”  the SILP method consumes more than 30 times the
computational cycles of the other two methods for MKL. We also observe that the level method is the
most efﬁcient among three methods in comparison. To obtain a better picture of the computational
efﬁciency of the proposed level method  we compute the time-saving ratio  as shown in Table 2. We
observe that the level method saves 91.9% of computational time on average when compared with
the SILP method  and 70.3% of computational time when compared with the SD method.
In order to see more details of each optimization algorithm  we plot the logarithm values of the
MKL objective function to base 10 against time in Figure 1. Due to space limitation  we randomly
choose only three datasets  “Iono”  “Breast”  and “Pima”  as examples. It is interesting to ﬁnd that
the level method converges overwhelmingly faster than the other two methods. The efﬁciency of the
level method arises from two aspects: (a) the cutting plane model utilizes the computational results
of all iterations and therefore boosts the search efﬁciency  and (b) the projection to the level sets
ensures the stability of the new solution. A detailed analysis of the SD method reveals that a large

number of function evaluations are consumed in order to compute the optimal stepsize via a line
search. Note that in convex-concave optimization  every function evaluation in the line search of SD
requires solving an SVM problem. As an example  we found that for dataset “Iono”  although SD
and the level method require similar numbers of iterations  SD calls the SVM solver 1231 times on
average  while the level method only calls it 47 times. For the SILP method  the high computational
cost is mainly due to the oscillation of solutions. This instability leads to very slow convergence
when the solution is close to the optimal one  as indicated by the long tail of SILP in Figure 1. The
instability of SILP is further conﬁrmed by the examination of kernel weights  as shown below.

To understand the evolution of kernel weights (i.e.  p)  we plot the evolution curves of the ﬁve largest
kernel weights for datasets “Iono”  “Breast”  and “Pima” in Figure 2. We observe that the values
of p computed by the SILP method are the most unstable due to oscillation of the solutions to the
cutting plane models. Although the unstable-solution problem is to some degree improved by the
SD method  we still clearly observe that p ﬂuctuates signiﬁcantly through iterations. In contrast 
for the proposed level method  the values of p change smoothly through iterations. We believe that
the stability of the level method is mainly due to the accurate estimation of bounds as well as the
regularization of the projection to the level sets. This observation also sheds light on why the level
method can be more efﬁcient than the SILP and the SD methods.

Table 2: Time-saving ratio of the level method over the SILP and the SD method

Level/SD (%)
Level/SILP (%)

Iono
78.9
99.4

Breast
90.4
91.6

Pima
77.0
85.4

Sonar Wpbc Heart Vote Wdbc Average
58.7
98.7

54.7
97.3

32.5
88.7

82.8
84.5

87.4
89.4

70.3
91.9

Evolution of objective values with time

SD
SILP
Level

e
v
i
t
c
e
j
b
o
 
f
o
 
g
o
l

3.8

3.75

3.7

3.65

3.6

3.55

3.5

3.45

Evolution of objective values with time

SD
SILP
Level

3.75

3.7

3.65

3.6

3.55

e
v
i
t
c
e
j
b
o
 
f
o
 
g
o
l

4.4

4.38

4.36

4.34

4.32

4.3

4.28

4.26

4.24

4.22

e
v
i
t
c
e
j
b
o
 
f
o
 
g
o
l

Evolution of objective values with time

SD
SILP
Level

3.4

0

20

40
60
time (s)
(a) Iono

80

100

3.5

0

10

20

30

time (s)

40

50

60

4.2

0

10

20

40

30
time (s)

50

60

70

(b) Breast

(c) Pima

Figure 1: Evolution of objective values over time (seconds) for datasets “Iono”  “Breast”  and
“Pima”. The objective values are plotted on a logarithm scale (base 10) for better comparison.
Only parts of the evolution curves are plotted for SILP due to their long tails.

5 Conclusion and Future Work
In this paper  we propose an extended level method to efﬁciently solve the multiple kernel learning
problem. In particular  the level method overcomes the drawbacks of both the SILP method and
the SD method for MKL. Unlike the SD method that only utilizes the gradient information of the
current solution  the level method utilizes the gradients of all the solutions that are obtained in past
iterations; meanwhile  unlike the SILP method that updates the solution only based on the cutting
plane model  the level method introduces a projection step to regularize the updated solution. It
is the employment of the projection step that guarantees ﬁnding an updated solution that  on the
one hand  is close to the existing one  and one the other hand  signiﬁcantly reduces the objective
function. Our experimental results have shown that the level method is able to greatly reduce the
computational time of MKL over both the SD method and the SILP method. For future work  we
plan to ﬁnd a scheme to adaptively set the value of λ in the level method and apply the level method
to other tasks  such as one-class classiﬁcation  multi-class classiﬁcation  and regression.

Acknowledgement
The work was supported by the National Science Foundation (IIS-0643494)  National Institute of Health
(1R01GM079688-01) and Research Grants Council of Hong Kong (CUHK4150/07E and CUHK4125/07).
References

[1] F. R. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine Learning

Research  9:1179–1225  2008.

Evolution of the kernel weight values in SD

Evolution of the kernel weight values in SILP

Evolution of the kernel weight values in Level method

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v
p

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v
p

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v
p

 

0

0

20

40

60

iteration

80

100

0

0

100

200
300
iteration

400

500

0

0

5

10

15
20
iteration

25

30

35

(a) Iono/SD

Evolution of the kernel weight values in SD

(b) Iono/SILP

Evolution of the kernel weight values in SILP

(c) Iono/Level

Evolution of the kernel weight values in Level method

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v

 

p

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v

 

p

0

0

5

10
15
iteration

20

25

0

0

20

40

60
80
iteration

100

120

140

(d) Breast/SD

Evolution of the kernel weight values in SD

(e) Breast/SILP

Evolution of the kernel weight values in SILP

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v
 
p

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v
 
p

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
e
u
l
a
v

 

p

s
e
u
l
a
v
 
p

5

10

iteration

15

20

(f) Breast/Level

Evolution of the kernel weight values in Level method

0

0

5

10

15

20
iteration

25

30

0

0

20

40

60

iteration

80

100

0

0

5

10

15

20
iteration

25

30

(g) Pima/SD

(h) Pima/SILP

(i) Pima/Level

Figure 2: The evolution curves of the ﬁve largest kernel weights for datasets “Iono”  “Breast” and
“Pima” computed by the three MKL algorithms

[2] F. R. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the SMO

algorithm. In ICML  2004.

[3] J. Bonnans  J. Gilbert  C. Lemar´echal  and C. Sagastiz´abal. Numerical Optimization  Theoretical and

Practical Aspects. Springer-Verlag  Berlin  2nd ed.  2006.

[4] N. Cristianini  J. Shawe-Taylor  A. Elisseeff  and J. S. Kandola. On kernel-target alignment. In NIPS 13 

pages 367–373  2001.

[5] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. E. Ghaoui  and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. Journal of Machine Learning Research  5  2004.

[6] C. Lemar´echal  A. Nemirovski  and Y. Nesterov. New variants of bundle methods. Mathematical Pro-

gramming  69(1)  1995.

[7] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine

Learning Research  6  2005.

[8] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. John Wiley

and Sons Ltd  1983.

[9] C. S. Ong  A. J. Smola  and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine

Learning Research  6  2005.

[10] A. Rakotomamonjy  F. R. Bach  S. Canu  and Y. Grandvalet. SimpleMKL. Technical Report HAL-

00218338  INRIA  2008.

[11] A. Smola  S. V. N. Vishwanathan  and Q. Le. Bundle methods for machine learning. In NIPS 20  pages

1377–1384  2007.

[12] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. Journal of

Machine Learning Research  7  2006.

[13] J. Ye  J. Chen  and S. Ji. Discriminant kernel and regularization parameter learning via semideﬁnite

programming. In ICML  2007.

[14] A. Zien and C. S. Ong. Multiclass multiple kernel learning. In ICML  2007.

,Amit Daniely
Nati Linial
Shai Shalev-Shwartz
Omar Besbes
Yonatan Gur
Assaf Zeevi