2019,On the number of variables to use in principal component regression,We study least squares linear regression over $N$ uncorrelated Gaussian features that are selected in order of decreasing variance. When the number of selected features $p$ is at most the sample size $n$  the estimator under consideration coincides with the principal component regression estimator; when $p>n$  the estimator is the least $\ell_2$ norm solution over the selected features. We give an average-case analysis of the out-of-sample prediction error as $p n N \to \infty$ with $p/N \to \alpha$ and $n/N \to \beta$  for some constants $\alpha \in [0 1]$ and $\beta \in (0 1)$. In this average-case setting  the prediction error exhibits a ``double descent'' shape as a function of $p$. We also establish conditions under which the minimum risk is achieved in the interpolating ($p>n$) regime.,On the number of variables to use in principal

component regression

Ji Xu

Columbia University

jixu@cs.columbia.edu

Daniel Hsu

Columbia University

djhsu@cs.columbia.edu

Abstract

We study least squares linear regression over N uncorrelated Gaussian features that
are selected in order of decreasing variance. When the number of selected features
p is at most the sample size n  the estimator under consideration coincides with the
principal component regression estimator; when p > n  the estimator is the least
`2 norm solution over the selected features. We give an average-case analysis of
the out-of-sample prediction error as p  n  N ! 1 with p/N ! ↵ and n/N !  
for some constants ↵ 2 [0  1] and  2 (0  1). In this average-case setting  the
prediction error exhibits a “double descent” shape as a function of p. We also
establish conditions under which the minimum risk is achieved in the interpolating
(p > n) regime.

1

Introduction

In principal component regression (PCR)  a linear model is ﬁt to variables obtained using principal
component analysis on the original covariates. Suppose the data consists of n i.i.d. observations
(x1  y1)  . . .   (xn  yn) from RN ⇥ R. Let X := [x1|···|xn]> be the n ⇥ N design matrix  y :=
(y1  . . .   yn)> be the n-dimensional vector of responses  and ⌃ := E[x1x>1 ] 2 RN⇥N. Assuming
⌃ is known (as we do in this paper)  the PCR ﬁt is given by V (XV )+y  where V 2 RN⇥p is the
matrix of top p (orthonormal) eigenvectors of ⌃  and A+ denotes the Moore-Penrose pseudo-inverse
of A. PCR notably addresses issues of multi-collinearity in under-determined (n < N) settings  while
avoiding saturation effects suffered by other regression methods such as ridge regression [1  7  12].
The critical parameter in PCR is the number of components p to include in the regression. Nearly
all previous analyses of variable selection have restricted attention to the p < n regime [e.g.  4].
This restriction may seem benign  as conventional wisdom suggests that choosing p > n leads to
over-ﬁtting. This paper aims to challenge this conventional wisdom in a particular setting for PCR.
We study the prediction error of the PCR ﬁt for all values of p in the under-determined regime. We
assume the xi are Gaussian and conduct an “average-case” analysis  where the “true” coefﬁcient
vector is randomly chosen from an isotropic prior distribution. Thus  all of the original variables
in xi are relevant but weak in terms of predicting the response. When the eigenvalues of ⌃ exhibit
some decay  one expects diminishing returns as p increases. It is often suggested to ﬁnd a value of p
that balances bias and variance  and such a value of p can be found in the p < n regime.
However  we show that when p > n  the prediction error can again be decreasing with p. This
phenomenon—the second descent of the so-called “double descent” risk curve [2]—has been observed
in a number of scenarios and for many different machine learning models (where p is regarded as a
nominal number of model parameters) [2  3  8  13  17]. In these previous studies  the limiting risk
as p ! 1 was often (but not always) observed to be lower than the best risk achieved in the p < n
regime. We prove that this phenomenon occurs with PCR in our data model: the lowest prediction
error is achieved at some p > n  rather than any p < n.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Our data model. Our data (x1  y1)  . . .   (xn  yn) are assumed to be i.i.d. with xi ⇠N (0  ⌃)  and

yi = x>i ✓ + wi.

Here  w1  . . .   wn are i.i.d. N (0  2) noise variables  and ✓ 2 RN is the true coefﬁcient vector. We
assume  without loss of generality  that ⌃ is diagonal. In fact  we shall take ⌃ := diag(1  . . .   N )
with distinct positive eigenvalues 1 > ··· > N > 0. The prediction (squared) error of ✓0 2 RN is
Ex y[(y  x>✓0)2]  where (x  y) is an independent copy of (x1  y1).
Some notation. For a vector v 2 RN  let vP 2 Rp denote the sub-vector of the ﬁrst p entries of v  and
let vP c 2 RNp denote the sub-vector of the last N  p entries. Similarly  for a matrix M 2 Rn⇥N 
let M P 2 Rn⇥p denote the sub-matrix of the ﬁrst p columns of M  and let M P c 2 Rn⇥(Np)
denote the sub-matrix of the last N  p columns.
Recall that PCR selects components in order of decreasing j. So  using the notation from above  the
PCR estimator ˆ✓ for ✓ is deﬁned by

ˆ✓P c := 0.

(1)

ˆ✓P :=((X>

X>

P X P )1X>
P (X P X>

P y if p  n 
P )1y if p > n;

(Recall that X := [x1|···|xn]> and y := (y1  . . .   yn)>; also  the matrices being inverted above
are  indeed  invertible with probability 1.) The prediction error of the PCR estimate ˆ✓ is denoted by

Error := Ex y[(y  x>ˆ✓)2].

Observe that the (squared) correlation between the response and the jth variable is proportional to
j   but PCR selects variables only on the basis of the j. So  for a worst-case ✓  PCR may be
j✓2
unlucky and end up selecting the p least correlated variables. To avoid this worst-case scenario  we
consider an “average-case” analysis  where the true coefﬁcient vector ✓ is independently drawn from
an isotropic prior distribution:

E✓[✓] = 0  E✓[✓✓>] = I.

(2)
We will study the random quantity Ew ✓[Error]  where the expectation is conditional on the design
matrix X  but averages over the observation noise w = (w1  . . .   wn) and random choice of ✓.
Our analysis uses high-dimensional asymptotic considerations to study the under-determined (n < N)
regression problem  letting p  n  N ! 1 with p/N ! ↵ and n/N !  for some ﬁxed constants
↵ 2 [0  1] and  2 (0  1). We are primarily interested in the limiting value of Ew ✓[Error]  which is
the asymptotic risk.

Our results.
In Section 2  we give an exact expression for the asymptotic risk in the case where
the eigenvalues of ⌃ exhibit polynomial decay  namely j = j for a ﬁxed constant > 0. Our
expression covers both the p < n and p > n regimes  and we ﬁnd that the smallest asymptotic risk
can be achieved with p > n (or equivalently  ↵> ) in noiseless settings. In noisy settings  the
comparison of the p < n and p > n regimes depends crucially on the exponent .
In Section 3  we relax the condition on the eigenvalues  and instead just assume that the empirical
distribution of the cN j  for some suitable sequence (cN )N1  has a “nice” limiting distribution. We
obtain results similar to those in Section 2 using a slightly different variable selection rule.
Our analyses permit a 1 o(1) fraction of j’s to converge to zero as p  n  N ! 1. (In particular  the
cN may go to inﬁnity.) This makes our analysis technically non-trivial and more generally applicable.
The proofs of the results are detailed in the full version of the paper [19].

Related works. Strategies for choosing the optimal value of p in PCR (e.g.  cross validation 
variance inﬂation factors) are typically only studied in the p < n regime [9]. For instance  the exact
risk of PCR as a function of p for Gaussian designs can be extracted from the analysis of Breiman
and Freedman [4]  but only for the p < n regime.
The high-dimensional analyses of ridge regression by Dicker [5]  Dobriban and Wager [6]  Hastie
et al. [8] are closely related to our work. Indeed  for ﬁxed p  the PCR estimator (or “ridgeless”
estimator) is obtained by taking the ridge regularization parameter to zero. These analyses extend

2

beyond the Gaussian design setting that we consider  but are restricted to cases where either all
eigenvalues of ⌃ remain bounded below by an absolute constant as N ! 1  or where the ridge
regularization parameter is held at some positive constant.
The “double descent” phenomenon was observed by several researchers [e.g.  2  8  13  17] for a
variety of machine learning models such as neural networks and ensemble methods. Belkin et al.
[3]  Hastie et al. [8]  Muthukumar et al. [13] provide statistical explanations for this phenomenon by
studying the behavior of the minimum `2 norm linear ﬁt with p > n. The analysis of Muthukumar
et al. [13] restricts attention to correctly-speciﬁed linear models (i.e.  p = N in our notation) and
shows some potential beneﬁts of the p > n regime. A related analysis of estimation variance was
carried out by Neal et al. [14]. The analysis of Belkin et al. [3] studies an isotropic Gaussian design
that is otherwise similar to our setup  as well as a Fourier design that is related to the random
Fourier features of Rahimi and Recht [15]. The analyses of Hastie et al. [8] look at more general
and non-isotropic designs (and  in fact  certain non-linear models related to neural networks!)  but
as mentioned before  they assume the eigenvalues of ⌃ are bounded away from zero. While their
“misspeciﬁed” setting appears to be similar to our setup  we note that varying their p/n parameter
(which they call ) changes the statistical problem under consideration. In contrast  our analysis
looks at the effect of choosing different p on the same statistical problem  and thus is able to shed
light on the number of variables one should use in principal component regression.

Notations for asymptotics. For any two random quantities X and Y   we use the notation X p! Y
to mean that X = Y + op(Y ) as n  p  N ! 1. Similarly  for any two non-random quantities X and
Y   we use the notation X ! Y to mean that X = Y + o(Y ) as n  p  N ! 1. Finally  we say that
X > Y holds in probability if Pr(X > Y ) ! 1 as n  p  N ! 1.
2 Analysis under polynomial eigenvalue decay

In this section  we analyze the asymptotic risk of PCR under the following assumptions:

A.1 There exists a constant > 0 such that j = j for all j = 1  . . .   N.
A.2 There exist constants ↵ 2 [0  1] and  2 (0  1) such that p/N ! ↵ and n/N !  as

p  n  N ! 1.

Assumption A.1 implies that the eigenvalues of ⌃ decay to zero at a polynomial rate  while Assump-
tion A.2 is a standard scaling for high-dimensional asymptotic analysis.
We also assume in this section that there is no observation noise  i.e.  var(wi) = 2 = 0. In the
noiseless setting  the asymptotic risk is the limiting value of E✓[Error]. Results for the noisy setting
are stated in Appendix C.

2.1 Main results

Our ﬁrst theorem provides characterizes the asymptotic risk when ↵< . Deﬁne the functions h
and R on (0  ):

Theorem 1. Assume A.1 with constant ; A.2 with constants ↵ and ; 2 = 0; and ↵< . Then

E✓[Error] p!R (↵).

Furthermore  the equation h(↵) = 0 has a unique solution ↵⇤ over the interval (0  )  and R(↵)
is decreasing on ↵ 2 [0 ↵ ⇤) and increasing on ↵ 2 (↵⇤  ). Finally 
0↵< R(↵) = N 1 
(↵⇤) .

R(↵⇤) = min

(5)

3



h(↵) :=

↵ Z 1
R(↵) := N 1Z 1

↵

↵

t2 dt  1 

t dt ·

  ↵

for all ↵< ;

 

for all ↵<.

(3)

(4)

5
1

0
1

8
0
1

k
s
i
r

k
s
i
r

6

4
5

2

0
0

 = 1

<latexit sha1_base64="A0YchN0cVRV7/kKcxtDLjLttdgA=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O2tu3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMi5Tgxvr+t1dYWV1b3yhulra2d3b3yvsHTZOkmrIGTUSiWxEaJrhkDcutYC2lGcaRYA/R6GbqPzwxbXgi7+1YsTDGgeR9TtE6qdUZoVJ4FXTLFb/qz0CWSZCTCuSod8tfnV5C05hJSwUa0w58ZcMMteVUsEmpkxqmkI5wwNqOSoyZCbPZvRNy4pQe6SfalbRkpv6eyDA2ZhxHrjNGOzSL3lT8z2untn8ZZlyq1DJJ54v6qSA2IdPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8Ai+CPoQ==</latexit>
<latexit sha1_base64="A0YchN0cVRV7/kKcxtDLjLttdgA=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O2tu3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMi5Tgxvr+t1dYWV1b3yhulra2d3b3yvsHTZOkmrIGTUSiWxEaJrhkDcutYC2lGcaRYA/R6GbqPzwxbXgi7+1YsTDGgeR9TtE6qdUZoVJ4FXTLFb/qz0CWSZCTCuSod8tfnV5C05hJSwUa0w58ZcMMteVUsEmpkxqmkI5wwNqOSoyZCbPZvRNy4pQe6SfalbRkpv6eyDA2ZhxHrjNGOzSL3lT8z2untn8ZZlyq1DJJ54v6qSA2IdPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8Ai+CPoQ==</latexit>
<latexit sha1_base64="A0YchN0cVRV7/kKcxtDLjLttdgA=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O2tu3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMi5Tgxvr+t1dYWV1b3yhulra2d3b3yvsHTZOkmrIGTUSiWxEaJrhkDcutYC2lGcaRYA/R6GbqPzwxbXgi7+1YsTDGgeR9TtE6qdUZoVJ4FXTLFb/qz0CWSZCTCuSod8tfnV5C05hJSwUa0w58ZcMMteVUsEmpkxqmkI5wwNqOSoyZCbPZvRNy4pQe6SfalbRkpv6eyDA2ZhxHrjNGOzSL3lT8z2untn8ZZlyq1DJJ54v6qSA2IdPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8Ai+CPoQ==</latexit>
<latexit sha1_base64="A0YchN0cVRV7/kKcxtDLjLttdgA=">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O2tu3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMi5Tgxvr+t1dYWV1b3yhulra2d3b3yvsHTZOkmrIGTUSiWxEaJrhkDcutYC2lGcaRYA/R6GbqPzwxbXgi7+1YsTDGgeR9TtE6qdUZoVJ4FXTLFb/qz0CWSZCTCuSod8tfnV5C05hJSwUa0w58ZcMMteVUsEmpkxqmkI5wwNqOSoyZCbPZvRNy4pQe6SfalbRkpv6eyDA2ZhxHrjNGOzSL3lT8z2untn8ZZlyq1DJJ54v6qSA2IdPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8Ai+CPoQ==</latexit>

<latexit sha1_base64="b+Ytr3XWSnr8N38ZMyOL/kC6yrU=">AAACD3icbVDLSsNAFJ34rPUVdekmWBQRKYkIuixKwWUF+4Amhsl02g6dPJi5UUqIX+DGX3HjQhG3bt35N07SLLT1wIXDOfdy7z1exJkE0/zW5uYXFpeWSyvl1bX1jU19a7slw1gQ2iQhD0XHw5JyFtAmMOC0EwmKfY/Ttje6zPz2HRWShcENjCPq+HgQsD4jGJTk6ge2j2HoeUk9dZP7YxuGFPDtUfqQ68JP6kKEInX1ilk1cxizxCpIBRVouPqX3QtJ7NMACMdSdi0zAifBAhjhNC3bsaQRJiM8oF1FA+xT6ST5P6mxr5Se0Q+FqgCMXP09kWBfyrHvqc7sSjntZeJ/XjeG/rmTsCCKgQZksqgfcwNCIwvH6DFBCfCxIpgIpm41yBALTEBFWFYhWNMvz5LWSdUyq9b1aaV2UcRRQrtoDx0iC52hGrpCDdREBD2iZ/SK3rQn7UV71z4mrXNaMbOD/kD7/AFrvZ2H</latexit>
<latexit sha1_base64="b+Ytr3XWSnr8N38ZMyOL/kC6yrU=">AAACD3icbVDLSsNAFJ34rPUVdekmWBQRKYkIuixKwWUF+4Amhsl02g6dPJi5UUqIX+DGX3HjQhG3bt35N07SLLT1wIXDOfdy7z1exJkE0/zW5uYXFpeWSyvl1bX1jU19a7slw1gQ2iQhD0XHw5JyFtAmMOC0EwmKfY/Ttje6zPz2HRWShcENjCPq+HgQsD4jGJTk6ge2j2HoeUk9dZP7YxuGFPDtUfqQ68JP6kKEInX1ilk1cxizxCpIBRVouPqX3QtJ7NMACMdSdi0zAifBAhjhNC3bsaQRJiM8oF1FA+xT6ST5P6mxr5Se0Q+FqgCMXP09kWBfyrHvqc7sSjntZeJ/XjeG/rmTsCCKgQZksqgfcwNCIwvH6DFBCfCxIpgIpm41yBALTEBFWFYhWNMvz5LWSdUyq9b1aaV2UcRRQrtoDx0iC52hGrpCDdREBD2iZ/SK3rQn7UV71z4mrXNaMbOD/kD7/AFrvZ2H</latexit>
<latexit sha1_base64="b+Ytr3XWSnr8N38ZMyOL/kC6yrU=">AAACD3icbVDLSsNAFJ34rPUVdekmWBQRKYkIuixKwWUF+4Amhsl02g6dPJi5UUqIX+DGX3HjQhG3bt35N07SLLT1wIXDOfdy7z1exJkE0/zW5uYXFpeWSyvl1bX1jU19a7slw1gQ2iQhD0XHw5JyFtAmMOC0EwmKfY/Ttje6zPz2HRWShcENjCPq+HgQsD4jGJTk6ge2j2HoeUk9dZP7YxuGFPDtUfqQ68JP6kKEInX1ilk1cxizxCpIBRVouPqX3QtJ7NMACMdSdi0zAifBAhjhNC3bsaQRJiM8oF1FA+xT6ST5P6mxr5Se0Q+FqgCMXP09kWBfyrHvqc7sSjntZeJ/XjeG/rmTsCCKgQZksqgfcwNCIwvH6DFBCfCxIpgIpm41yBALTEBFWFYhWNMvz5LWSdUyq9b1aaV2UcRRQrtoDx0iC52hGrpCDdREBD2iZ/SK3rQn7UV71z4mrXNaMbOD/kD7/AFrvZ2H</latexit>
<latexit sha1_base64="b+Ytr3XWSnr8N38ZMyOL/kC6yrU=">AAACD3icbVDLSsNAFJ34rPUVdekmWBQRKYkIuixKwWUF+4Amhsl02g6dPJi5UUqIX+DGX3HjQhG3bt35N07SLLT1wIXDOfdy7z1exJkE0/zW5uYXFpeWSyvl1bX1jU19a7slw1gQ2iQhD0XHw5JyFtAmMOC0EwmKfY/Ttje6zPz2HRWShcENjCPq+HgQsD4jGJTk6ge2j2HoeUk9dZP7YxuGFPDtUfqQ68JP6kKEInX1ilk1cxizxCpIBRVouPqX3QtJ7NMACMdSdi0zAifBAhjhNC3bsaQRJiM8oF1FA+xT6ST5P6mxr5Se0Q+FqgCMXP09kWBfyrHvqc7sSjntZeJ/XjeG/rmTsCCKgQZksqgfcwNCIwvH6DFBCfCxIpgIpm41yBALTEBFWFYhWNMvz5LWSdUyq9b1aaV2UcRRQrtoDx0iC52hGrpCDdREBD2iZ/SK3rQn7UV71z4mrXNaMbOD/kD7/AFrvZ2H</latexit>

E Error
Ew ⇤ Error
R(alpha)
R(↵)

<latexit sha1_base64="G3+rf6RqL6ugJ3aBEV4KtdSPiRA=">AAACBXicbVBNS8NAEN34WetX1KMegkWol5KIoMeiF49V7Ac0IUy2m3bpJll2N0IJuXjxr3jxoIhX/4M3/42bNgdtfTDweG+GmXkBZ1Qq2/42lpZXVtfWKxvVza3tnV1zb78jk1Rg0sYJS0QvAEkYjUlbUcVIjwsCUcBINxhfF373gQhJk/heTTjxIhjGNKQYlJZ888iNQI0wsOwu9zN3DJxDXneB8RGc+mbNbthTWIvEKUkNlWj55pc7SHAakVhhBlL2HZsrLwOhKGYkr7qpJBzwGIakr2kMEZFeNv0it060MrDCROiKlTVVf09kEEk5iQLdWdws571C/M/rpyq89DIa81SRGM8WhSmzVGIVkVgDKghWbKIJYEH1rRYegQCsdHBVHYIz//Ii6Zw1HLvh3J7XmldlHBV0iI5RHTnoAjXRDWqhNsLoET2jV/RmPBkvxrvxMWtdMsqZA/QHxucPwoGYtg==</latexit>
<latexit sha1_base64="G3+rf6RqL6ugJ3aBEV4KtdSPiRA=">AAACBXicbVBNS8NAEN34WetX1KMegkWol5KIoMeiF49V7Ac0IUy2m3bpJll2N0IJuXjxr3jxoIhX/4M3/42bNgdtfTDweG+GmXkBZ1Qq2/42lpZXVtfWKxvVza3tnV1zb78jk1Rg0sYJS0QvAEkYjUlbUcVIjwsCUcBINxhfF373gQhJk/heTTjxIhjGNKQYlJZ888iNQI0wsOwu9zN3DJxDXneB8RGc+mbNbthTWIvEKUkNlWj55pc7SHAakVhhBlL2HZsrLwOhKGYkr7qpJBzwGIakr2kMEZFeNv0it060MrDCROiKlTVVf09kEEk5iQLdWdws571C/M/rpyq89DIa81SRGM8WhSmzVGIVkVgDKghWbKIJYEH1rRYegQCsdHBVHYIz//Ii6Zw1HLvh3J7XmldlHBV0iI5RHTnoAjXRDWqhNsLoET2jV/RmPBkvxrvxMWtdMsqZA/QHxucPwoGYtg==</latexit>
<latexit sha1_base64="G3+rf6RqL6ugJ3aBEV4KtdSPiRA=">AAACBXicbVBNS8NAEN34WetX1KMegkWol5KIoMeiF49V7Ac0IUy2m3bpJll2N0IJuXjxr3jxoIhX/4M3/42bNgdtfTDweG+GmXkBZ1Qq2/42lpZXVtfWKxvVza3tnV1zb78jk1Rg0sYJS0QvAEkYjUlbUcVIjwsCUcBINxhfF373gQhJk/heTTjxIhjGNKQYlJZ888iNQI0wsOwu9zN3DJxDXneB8RGc+mbNbthTWIvEKUkNlWj55pc7SHAakVhhBlL2HZsrLwOhKGYkr7qpJBzwGIakr2kMEZFeNv0it060MrDCROiKlTVVf09kEEk5iQLdWdws571C/M/rpyq89DIa81SRGM8WhSmzVGIVkVgDKghWbKIJYEH1rRYegQCsdHBVHYIz//Ii6Zw1HLvh3J7XmldlHBV0iI5RHTnoAjXRDWqhNsLoET2jV/RmPBkvxrvxMWtdMsqZA/QHxucPwoGYtg==</latexit>
<latexit sha1_base64="G3+rf6RqL6ugJ3aBEV4KtdSPiRA=">AAACBXicbVBNS8NAEN34WetX1KMegkWol5KIoMeiF49V7Ac0IUy2m3bpJll2N0IJuXjxr3jxoIhX/4M3/42bNgdtfTDweG+GmXkBZ1Qq2/42lpZXVtfWKxvVza3tnV1zb78jk1Rg0sYJS0QvAEkYjUlbUcVIjwsCUcBINxhfF373gQhJk/heTTjxIhjGNKQYlJZ888iNQI0wsOwu9zN3DJxDXneB8RGc+mbNbthTWIvEKUkNlWj55pc7SHAakVhhBlL2HZsrLwOhKGYkr7qpJBzwGIakr2kMEZFeNv0it060MrDCROiKlTVVf09kEEk5iQLdWdws571C/M/rpyq89DIa81SRGM8WhSmzVGIVkVgDKghWbKIJYEH1rRYegQCsdHBVHYIz//Ii6Zw1HLvh3J7XmldlHBV0iI5RHTnoAjXRDWqhNsLoET2jV/RmPBkvxrvxMWtdMsqZA/QHxucPwoGYtg==</latexit>

2
2
1
1
0
0
0
0

.
.

 = 2

<latexit sha1_base64="YL1ePw28IevvBzwsoV26KDEGt2I=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoBeh6MVjBfuBbSiT7aZdutmE3Y1QQv+FFw+KePXfePPfuG1z0NYHA4/3ZpiZFySCa+O6305hbX1jc6u4XdrZ3ds/KB8etXScKsqaNBax6gSomeCSNQ03gnUSxTAKBGsH49uZ335iSvNYPphJwvwIh5KHnKKx0mNvjEmC5JrU+uWKW3XnIKvEy0kFcjT65a/eIKZpxKShArXuem5i/AyV4VSwaamXapYgHeOQdS2VGDHtZ/OLp+TMKgMSxsqWNGSu/p7IMNJ6EgW2M0Iz0sveTPzP66YmvPIzLpPUMEkXi8JUEBOT2ftkwBWjRkwsQaq4vZXQESqkxoZUsiF4yy+vklat6rlV7/6iUr/J4yjCCZzCOXhwCXW4gwY0gYKEZ3iFN0c7L86787FoLTj5zDH8gfP5Az3oj/Y=</latexit>
<latexit sha1_base64="YL1ePw28IevvBzwsoV26KDEGt2I=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoBeh6MVjBfuBbSiT7aZdutmE3Y1QQv+FFw+KePXfePPfuG1z0NYHA4/3ZpiZFySCa+O6305hbX1jc6u4XdrZ3ds/KB8etXScKsqaNBax6gSomeCSNQ03gnUSxTAKBGsH49uZ335iSvNYPphJwvwIh5KHnKKx0mNvjEmC5JrU+uWKW3XnIKvEy0kFcjT65a/eIKZpxKShArXuem5i/AyV4VSwaamXapYgHeOQdS2VGDHtZ/OLp+TMKgMSxsqWNGSu/p7IMNJ6EgW2M0Iz0sveTPzP66YmvPIzLpPUMEkXi8JUEBOT2ftkwBWjRkwsQaq4vZXQESqkxoZUsiF4yy+vklat6rlV7/6iUr/J4yjCCZzCOXhwCXW4gwY0gYKEZ3iFN0c7L86787FoLTj5zDH8gfP5Az3oj/Y=</latexit>
<latexit sha1_base64="YL1ePw28IevvBzwsoV26KDEGt2I=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoBeh6MVjBfuBbSiT7aZdutmE3Y1QQv+FFw+KePXfePPfuG1z0NYHA4/3ZpiZFySCa+O6305hbX1jc6u4XdrZ3ds/KB8etXScKsqaNBax6gSomeCSNQ03gnUSxTAKBGsH49uZ335iSvNYPphJwvwIh5KHnKKx0mNvjEmC5JrU+uWKW3XnIKvEy0kFcjT65a/eIKZpxKShArXuem5i/AyV4VSwaamXapYgHeOQdS2VGDHtZ/OLp+TMKgMSxsqWNGSu/p7IMNJ6EgW2M0Iz0sveTPzP66YmvPIzLpPUMEkXi8JUEBOT2ftkwBWjRkwsQaq4vZXQESqkxoZUsiF4yy+vklat6rlV7/6iUr/J4yjCCZzCOXhwCXW4gwY0gYKEZ3iFN0c7L86787FoLTj5zDH8gfP5Az3oj/Y=</latexit>
<latexit sha1_base64="YL1ePw28IevvBzwsoV26KDEGt2I=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoBeh6MVjBfuBbSiT7aZdutmE3Y1QQv+FFw+KePXfePPfuG1z0NYHA4/3ZpiZFySCa+O6305hbX1jc6u4XdrZ3ds/KB8etXScKsqaNBax6gSomeCSNQ03gnUSxTAKBGsH49uZ335iSvNYPphJwvwIh5KHnKKx0mNvjEmC5JrU+uWKW3XnIKvEy0kFcjT65a/eIKZpxKShArXuem5i/AyV4VSwaamXapYgHeOQdS2VGDHtZ/OLp+TMKgMSxsqWNGSu/p7IMNJ6EgW2M0Iz0sveTPzP66YmvPIzLpPUMEkXi8JUEBOT2ftkwBWjRkwsQaq4vZXQESqkxoZUsiF4yy+vklat6rlV7/6iUr/J4yjCCZzCOXhwCXW4gwY0gYKEZ3iFN0c7L86787FoLTj5zDH8gfP5Az3oj/Y=</latexit>

E Error
R(alpha)

k
s
i
r

0
0
1
1
0
0
0
0

.
.

8
8
0
0
0
0
0
0

.
.

6
6
0
0
0
0
0
0

.
.

0.0
0.0

0.2
0.2

0.6
0.6

0.4
0.4
↵ = p/N
p/N

<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>
<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>
<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>
<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>

0.8
0.8

1.0
1.0

0.0
0.0

0.2
0.2

0.6
0.6

0.4
0.4
↵ = p/N

<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>
<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>
<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>
<latexit sha1_base64="dadLJSVMX+K2Z5x9P6fR+QzKsK0=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSYi6EUoevEkFewHtqFMtpt26Waz7G6EEvovvHhQxKv/xpv/xm2bg1YfDDzem2FmXig508bzvpzC0vLK6lpxvbSxubW9U97da+okVYQ2SMIT1Q5RU84EbRhmOG1LRTEOOW2Fo+up33qkSrNE3JuxpEGMA8EiRtBY6aGLXA7xUp7c9soVr+rN4P4lfk4qkKPeK392+wlJYyoM4ah1x/ekCTJUhhFOJ6VuqqlEMsIB7VgqMKY6yGYXT9wjq/TdKFG2hHFn6s+JDGOtx3FoO2M0Q73oTcX/vE5qoosgY0KmhgoyXxSl3DWJO33f7TNFieFjS5AoZm91yRAVEmNDKtkQ/MWX/5LmadX3qv7dWaV2lcdRhAM4hGPw4RxqcAN1aAABAU/wAq+Odp6dN+d93lpw8pl9+AXn4xvuWZBq</latexit>

0.8
0.8

1.0
1.0

Figure 1: The asymptotic risk function R as a function of ↵ (with n = 300  N = 1000   =
n/N = 0.3 and  = 1  2 respectively). The location of ↵⇤ from Theorem 1 is marked with a black
circle. In both cases  the asymptotic risk at ↵ = 1 is lower than the asymptotic risk at ↵⇤.

The proof of Theorem 1 is sketched in Section 2.2  with some details left to Appendix A. Theorem 1
supports the well-known intuition that the risk curve is “U-shaped” in the p < n regime. Our next
theorem  however  shows a very different behavior when ↵> .
Formally deﬁne m(z) for z  0 to be the smallest positive solution to the equation

 z =

1

m(z) 

1

Z 1

↵

1

t1/(1 + t · m(z))

dt 

(6)

(7)

and let m0(·) denote the derivative of m(·). Also deﬁne the function R on (  1]:

R(↵) := N 1 

m(0)

+Z 1

↵

m0(0)

m(0)2!  

t dt ·

for all ↵>.

Theorem 2. Assume A.1 with constant ; A.2 with constants ↵ and ; 2 = 0; and ↵> . The
function m and its derivative m0 are well-deﬁned and positive at z = 0 (and hence R(↵) is
well-deﬁned for all ↵> ). Moreover 

E✓[Error] p!R (↵).

The proof of Theorem 2 is sketched in Section 2.3  with some details left to Appendix B.
We plot the asymptotic risk function R in Figure 1 for two different values of   both with  = 0.3.
(In simulations  we ﬁnd that E✓[Error] matches these curves very closely for sample sizes as small
as n = 300.) For both values of  2{ 1  2}  we observe the striking “double descent” behavior as
found in previous studies [e.g.  2]. Moreover  we see that the asymptotic risk at ↵ = 1 is smaller than
the minimum asymptotic risk achieved at any ↵< . This  in fact  happens for all values of > 0 
as we claim in the next theorem.
Theorem 3. Assume A.1 with constant   A.2 with constants ↵ and   2 = 0. Let ↵⇤ be
the minimizer of R over the interval [0  ). Then lim supN R(1)/R(↵⇤) < 1. Moreover 
R(↵)/R(1) ! 1 as ↵ ! .
The proof of Theorem 3 is given in Section 2.4. Theorem 3 shows that the asymptotic risk exhibits a
second decrease somewhere in the p > n regime when N is sufﬁciently large  and moreover  that it is
possible to ﬁnd a value of p in this p > n regime to achieve a lower asymptotic risk than any p < n.
In the noisy setting (see Appendix C)  it is possible for the asymptotic risk to be dominated by the
noise  in which case the minimum asymptotic risk is in fact achieved by ↵ = 0 (i.e.  p = o(n)).
However  there exists a regime with 2 > 0 in which we have the same conclusion as in Theorem 3.

4

2.2 Proof sketch for Theorem 1
We ﬁrst show that h(↵) = 0 has a unique solution on (0  ). Deﬁne ˜h(↵) := ↵1h(↵). We
shall show that ˜h(↵) = 0 has a unique solution on (0  )  which in turn immediately implies that
h(↵) = 0 also has a unique solution on the same interval. Observe that

d˜h(↵)

d↵

=  + ↵

↵1+

< 0.

Hence  the function ˜h(↵) is strictly decreasing on ↵ 2 (0  ]. Furthermore  we have

˜h(↵) > 0 as ↵ ! 0+ 

and

˜h(↵) < 0 at ↵ = .

(8)

(9)

Because ˜h is continuous  it follows that the equation ˜h(↵) = 0 has a unique solution on (0  ).
We now prove E✓[Error] p!R (↵). Since the proof only requires standard techniques  we just
sketch the main ideas in this section  and leave the full proof to Appendix A. First  since ↵<   for
large enough N  we have p < n. Then the prediction error is given by

P c ✓P ck2 

E✓[Error] = tr(X>

P X P c✓P ck2 + k⌃1/2

P X P1 X>

Error = Ex y[(y  x>ˆ✓)2] = k⌃1/2

where ⌃P 2 Rp⇥p and ⌃P c 2 R(Np)⇥(Np) are two diagonal matrices whose diagonal elements
are the ﬁrst p and last N  p diagonal elements of ⌃  respectively. By (2)  we have

P X>
P X P1 ⌃PX>
Note that X P c is independent of X P   thus  given X P   the trace that includes X P c is a sum of
N  p independent random variables. Therefore  we have
p! tr(⌃P c) · (tr((X>
= tr(⌃P c) · (tr(( ¯X>
p! tr(⌃P c)

P X P )1⌃P ) + 1)
¯X P )1) + 1)

P X P1 X>

P cX PX>

P X P c) + tr(⌃P c).

E✓[Error]



P

 

  ↵

P

(4)  we just need to compute tr(⌃P c). Note thatR s+1

where ¯X P := X P ⌃1/2
is a standard Gaussian matrix. The ﬁrst line above uses Markov’s
inequality to show that E✓[Error] converges in probability to E✓ X P c [Error]. The third line above
¯X P is a standard Wishart matrix Wp(I  n). So  to prove
uses Assumption A.2 and the fact that ¯X>
P
t dt < s <R s
s1 t dt. Hence  we have
i = N 1 tr(⌃P c) < Z N
NXi=p+1
N 
t dt ·
Therefore  we have tr(⌃P c) ! N 1R 1
↵ t dt as p ! 1  and thus we have E✓[Error] p!R (↵).
Finally  to prove (5)  we analyze the shape of R(↵) to ﬁnd its minimum value over ↵< . We take
the derivative of g(↵) := N 1R(↵):

N 
t dt ·

Z N

N 
N

(10)

1
N

1
N

p+1

<

1

p

.

s

dg(↵)

d↵

=  ·

↵1  ↵ +R 1

(  ↵)2

↵ t dt

= ↵1 · h(↵)

(  ↵)2

.

(11)

Using (8) and (9)  we deduce that R(↵) ﬁrst decreases and then increases as a function of ↵ in the
interval (0  ). Therefore  the minimum risk is achieved at the unique solution ↵⇤ of the equation
h(↵) = 0 over the interval (0  ). Equation (11) also impliesR 1
↵⇤ t dt = (↵⇤)(↵⇤). Hence 

the minimum risk is given by

↵< R(↵) = N 1
min

t dt = N 1 

(↵⇤) .



  ↵⇤Z 1

↵⇤

5

2.3 Proof sketch for Theorem 2
We ﬁrst show that m(0) is well-deﬁned. Consider the RHS expression from Equation (6) evaluated
at z = 0; by a change-of-variable in the integral  we have

1
m 

1

Z 1

↵

1t1/
1 + t · m

dt =

1

↵m11/ 

m1//↵  ↵Z 1

m1//↵

t2

1 + t dt!  

(12)

where m = m(0). So  we just need to show that q(s  ↵) = 0 has a unique solution s⇤ for s over
the positive real line  where q(s  ↵) is deﬁned by

q(s  ↵) :=



s  ↵Z 1

s

t2
1 + t dt.

(13)

(This makes m(0) well-deﬁned  via the equation s⇤ = m(0)1//↵  and also veriﬁes its positivity.)
The derivative of q(s  ↵) with respect to s is

@q(s  ↵)

@s

=

(↵  )s  
s2(1 + s)

.

(14)

Hence  since ↵>   we know the function q(s  ↵) is strictly decreasing on s 2 (0  ( 
↵ )1/] and
strictly increasing on s 2 [( 
↵ )1/ 1). Furthermore  q(s  ↵) ! 1 as s ! 0 and q(s  ↵) ! 0
as s ! 1. Hence  by the continuity of s 7! q(s  ↵)  we conclude that q(s  ↵) = 0 has a unique
solution s⇤.
Using the chain rule  we can also show that m0(0) is well-deﬁned  and that its value is given by

m0(0) = m2
We leave the details to Appendix B.1.
Our next goal is to prove E✓[Error] p!R (↵). Since ↵>   we have p > n for large enough N. In
this case 

(0) · (1 + (s⇤))/ + (  ↵)(s⇤) > 0.

Error = Ex y[(y  x>ˆ✓)2] = Ex y[(x>P (ˆ✓P  ✓P )  x>P c✓P c)2]

P ((⇧X P  I)✓P + X>

P (X P X>

P )1X P c✓P c)k2 + k⌃1/2

P c ✓P ck2 

where ⇧X P := X>
Section 2.2. Hence  E✓[Error] is equal to

P1 X P   and the diagonal matrices ⌃P and ⌃P c are as deﬁned in

tr(⌃P (I  ⇧X P ))

+ tr(X>

P c(X P X>

P (X P X>

P )1X P c) + tr(⌃P c)

.

(15)

= k⌃1/2
PX P X>
|
}

part 1

{z

|

We claim that

P )1X P ⌃P X>
part 2

{z

}

part 1

p!

N 1
m(0)

 

and part 2 p! N 1 ·

m0(0)
m2

(0) ·Z 1

↵

t2 dt + op(N 1);

(16)

together  they complete the proof that E✓[Error] p!R (↵). Rigorous proofs of the claims in (16)
are presented in Appendix B.2 and Appendix B.3; here  we give a heuristic argument that conveys
the main idea. For part 1  let ˜⌃P = N ⌃P and ˜X P = N /2X P . This scaling ensures that the
empirical eigenvalue distribution of ˜⌃P has a limiting distribution with probability density

f(s) =

1
↵

s11/ · 1

{s2[↵ 1)}

(Lemma 2 in Appendix B.2). Also  under this scaling  we have

tr(⌃PI  ⇧X P) = lim
tr ˜⌃P✓ 1

= lim
µ!0

n
N  ·

µ!0

µ
n

n

n

N ✓ 1

n

˜X>
P

tr( ˜⌃P ) 

1
n

˜X P + µI◆1! = lim

µ!0

tr( ˜⌃P ( ˜X>
P

˜X P + µnI)1 ˜X>
P

˜X P )◆

n
N  ·

µ
n

tr( ˜⌃P ˜Sn) 

(17)

6

where ˜Sn := (n1 ˜X>
P
limiting distribution with bounded support  we have

˜X P + µI)1. As long as the empirical eigenvalue distribution of ˜⌃P has a

8µ > 0  µ ·

1
n

tr ˜⌃P✓ 1

n

˜X>
P

˜X P + µI◆1! p!

1

m(µ)

 

(18)

where m(z) is  in fact  the Stieltjes transform of the limiting empirical eigenvalue distribution of
n1 ˜X P ˜X>
P (Lemma 1 in Appendix B.2); this follows from results of Dobriban and Wager [6] 
which in turn are derived from the results of Ledoit and Péché [11]. Assume we can exchange the two
limits µ ! 0+ and N ! 1  and also that (18) still holds for f(s) which has unbounded support.
Then  from (17)  we conclude

.

µ!0

part 2

N 1
m(0)

PX P X>

For part 2  note that X P c is independent of X P . Thus  conditional on X P   part 2 is a sum of N  p
independent random variables. Therefore  using Markov inequality  we can show that

part 1 = tr(⌃PI  ⇧X P) p!
p! EX P c [part 2] = tr (⌃P c) ·✓tr⇣⌃P X>
= tr (⌃P c) · lim
= tr (⌃P c) ·✓ lim
(19)
Again  if we ignore the fact that the support of f(s) is unbounded and assume the limits of µ ! 0
and N ! 1 can be exchanged  then by Lemma 7.4 of Dobriban and Wager [6]  we have
part 2 p! tr (⌃P c) ·✓ lim
m0(0)
m2
(0)
A straightforward analysis of tr(⌃P c) (as in (10)) completes the analysis of part 2 of (16).
Remark 1. Although Theorem 2 should intuitively hold given the results of Dobriban and Wager
[6]  a careful and more involved argument is needed to deal with the facts that k ˜⌃Pk2 ! 1 (since
n tr( ˜⌃P ˜Sn) = Op(N ).
k⌃1
However  we need the stronger bound µ

P2 X P⌘ + 1◆
˜X P⇣ ˜X>
n⌘ + 1◆ .
n⌘ + 1◆ p! tr (⌃P c) ·

tr✓ ˜⌃P⇣ ˜X>
tr⇣ ˜⌃P ˜Sn⌘ 
tr⇣ ˜⌃P ˜Sn⌘ 

˜X P + µnI⌘1
tr⇣ ˜⌃P ˜S
tr⇣ ˜⌃P ˜S

P k2 ! 1) and µ ! 0. For example  standard techniques only imply µ

˜X P + µnI⌘1◆ + 1!

n tr( ˜⌃P ˜Sn) = Op(1) (e.g.  Appendix B.2.2).

˜X>
P

µ!0

µ!0

(20)

µ
n

µ
n

1
n

1
n

P

2

2

P

.

2.4 Proof of Theorem 3
Comparing the expression for R(↵) in (7) at ↵ = 1 to the expression for R(↵⇤) in (5)  we see that
it sufﬁces to prove m(0)1/ >↵ ⇤. Recall that in Section 2.3  we have proved s⇤ := m(0)1/ is
the unique solution of the equation q(s  1) = 0. Furthermore  using the expression for the derivative
of q(s  1) with respect to s in (14)  we know that q(s  1) > 0 ) s < s⇤. Thus  we only need to
show q(↵⇤  1) > 0 = h(↵⇤)  where the equality is due to the deﬁnition of ↵⇤ in Theorem 1. Note
that by the deﬁnitions of the functions q and h in (3) and (13)  we have

h(s) =



s Z 1

s

t2 dt  1 = q(s  1) +Z 1

s

t2

(1 + t)

dt Z 1

s

t2 dt  1.

Furthermore  h(s)  q(s  1) is increasing in s:
= 

d (h(s)  q(s  1))
Hence  for all for all s 2 (0  1]  we have

ds

s2

(1 + s)

+ s2 =

s22
1 + s > 0.

h(s)  q(s  1)  h(1)  q(1  1) = Z 1
dt Z 1

= Z 1

t2

1

1

t2

dt  1

(1 + t)

1

t2 dt = Z 1

(1 + t)

t2(1 + t)
Since ↵⇤ << 1  we have 0 = h(↵⇤) < q(↵⇤  1)  and thus we have s⇤ >↵ ⇤.
By inspection of the expression for R(↵) in (4)  it is also clear that R(↵)/R(1) ! 1 as
↵ ! .

1

1

1

dt < 0.

7

3 Analysis under general eigenvalue decay

1

j=1

{j⌫N}.

In this section  we extend the results from Section 2 (with noise) to hold under a more general
assumption on the eigenvalues of ⌃. To simplify calculations  we use a slightly different feature

Instead of Assumptions A.1 and A.2  we assume the following:

selection procedure that includes all components j such that j  ⌫N  so p =PN
B.1 k⌃k2  C for some constant C > 0. Also  there exists a positive sequence (cN )N1
such that the empirical eigenvalue distribution of cN ⌃ converges as N ! 1 to F =
(1  )F0 + F1  where  2 (0  1]  F0 is a point mass of 0  and F1 has a continuous
probability density f supported on either [⌘1 ⌘ 2] or [⌘1 1) for some constants ⌘1 ⌘ 2 > 0.
B.2 There exist constants ⌫> 0 and  2 (0  ) s.t. ⌫N cN ! ⌫ and n/N !  as n  N ! 1.
The cN in Assumption B.1 generalizes the N  scaling introduced in the proof of Theorem 2. In
fact  Assumption B.1 is more general than the eigenvalue assumptions made by Dobriban and Wager
[6] and Hastie et al. [8]: the eigenvalues of ⌃ could decrease smoothly ( = 1)  or there could be a
sudden drop between (say) j and j+1 (< 1). Since p is now determined by ⌫  whether p < n
or p > n is now determined by whether ⌫>⌫ b or ⌫<⌫ b  where ⌫b >⌘ 1 is given by the equation

f (t) dt = . Finally  by Assumption B.1 

p
N

=

1
N

NXj=1

1

{cN j⌫}

a.s.! Es⇠f [1

{s⌫}] = Z 1

⌫

f (t) dt =: ↵(⌫) 

8⌫> 0.

(21)

For ⌫ = 0  i.e.  ⌫N = o(1/cN )  we choose ⌫N be the N largest eigenvalues of ⌃  then ↵(⌫) = .
Hence  combined with Assumption B.2  we have the same asymptotics considered in Section 2 
except that  is now restricted in (0  ). This restriction on  is required  otherwise both cN X>X
and cN XX> are asymptotically singular.
The following theorem generalizes the results in Section 2 to hold under Assumptions B.1 and B.2.
Theorem 4. Assume B.1 with sequence (cN )N1 and constants C    ⌘1  and ⌘2; and B.2 with
constants ⌫ and .

R 1

⌫b

=: Rf (⌫  ).

(22)

tf (t) dt. If the equation hf (⌫) = 0 has a

⌘1

⌘1


⌫ f (t) dt

tf (t) dt + 2!
⌫ f (t) dt  R ⌫
Rf (⌫⇤  0) = min

(i) Assume ⌫ 2 (⌫b 1). Then
Ew ✓[Error] p! N
cN · Z ⌫
Deﬁne hf (⌫) := ⌫  ⌫R 1
solution on (⌫b 1)T supp(f )  then the solution ⌫⇤ is unique  and
N
cN · ⌫⇤.
Z 1

⌫2(⌫b 1)Rf (⌫  0) = lim

⌫2(⌫b 1)Rf (⌫  0) =

  R 1

⌫!1Rf (⌫  0) =
(ii) Assume ⌫ 2 [0 ⌫ b). Deﬁne qf (s  ⌫) := s  sR 1
R ⌫
s⇤fR 1

Ew ✓[Error]

where s⇤f is the unique solution of the equation qf (s  ⌫) = 0.

s+t dt. Then
tf (t) dt + 2
(s⇤f +t)2 dt

tf (t)

s⇤f +  ·

Otherwise 

N
cN

⌫

⌘1

⌫

N
cN

p!

N
cN

tf (t)

inf

(23)

(24)

tf (t) dt.

⌘1

=: Rf (⌫  ) 

(25)

(iii) Suppose  = 0. Let ⌫⇤ be the minimizer of Rf (⌫  0) over the interval (⌫b 1] (including 1).
Let Rf (⌘1  0) be the risk achieved at ⌫ = ⌘1. Then lim supN Rf (⌘1  0)/Rf (⌫⇤  0) < 1.

The proof of this theorem is presented in Appendix D.

8

4 Discussion

Our results conﬁrm the emergence of the “double descent” risk curve in a natural setting with
Gaussian design. As in previous works [e.g.  3  8  13]  the shape emerges when there is a spike at the
interpolation threshold (p = n)  which is typically caused by a near-zero minimum eigenvalue of the
empirical covariance matrix.
More importantly  however  our results shed light on when the minimum risk is achieved before
or after the interpolation threshold in terms of the noise level and eigenvalues of the (population)
covariance matrix. For instance  when the eigenvalues decay very slowly or not at all (< 1)  a
smaller risk is achieved after the interpolation threshold (p > n) than any point before (p < n). On
the other hand  when the eigenvalues decay more quickly (> 1)  a smaller risk is achieved in the
p > n regime only in the noiseless setting. In general  the p < n regime yields a smaller risk when
the noise dominates the error due to model misspeciﬁcation. Providing a full characterization is an
important direction for future research.
Finally  we point out that the PCR estimator we study is a non-standard “oracle” estimator because it
generally requires knowledge of ⌃. Although it can be plausibly implemented in a semi-supervised
setting (by estimating ⌃ very accurately using unlabeled data)  a full analysis that accounts for
estimation errors in ⌃  or of a more standard PCR estimator  remains open. However  we note that
the PCR estimator with p = N can be implemented  and in our analysis  the dominance of the p > n
regime is always established at p = N. We believe that this should be true for the standard PCR
estimator as well.

Acknowledgments

This research was supported by NSF CCF-1740833  a Sloan Research Fellowship  a Google Faculty
Award  and a Cheung-Kong Graduate School of Business Fellowship.

References
[1] Frank Bauer  Sergei Pereverzev  and Lorenzo Rosasco. On regularization algorithms in learning

theory. Journal of complexity  23(1):52–72  2007.

[2] Mikhail Belkin  Daniel Hsu  Siyuan Ma  and Soumik Mandal. Reconciling modern machine

learning and the bias-variance trade-off. arXiv preprint arXiv:1812.11118  2018.

[3] Mikhail Belkin  Daniel Hsu  and Ji Xu. Two models of double descent for weak features. arXiv

preprint arXiv:1903.07571  2019.

[4] Leo Breiman and David Freedman. How many variables should be entered in a regression

equation? Journal of the American Statistical Association  78(381):131–136  1983.

[5] Lee H Dicker. Ridge regression and asymptotic minimax estimation over spheres of growing

dimension. Bernoulli  22(1):1–37  2016.

[6] Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regres-

sion and classiﬁcation. The Annals of Statistics  46(1):247–279  2018.

[7] L Lo Gerfo  Lorenzo Rosasco  Francesca Odone  Ernesto De Vito  and Alessandro Verri.

Spectral algorithms for supervised learning. Neural Computation  20(7):1873–1897  2008.

[8] Trevor Hastie  Andrea Montanari  Saharon Rosset  and Ryan J Tibshirani. Surprises in high-

dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560  2019.

[9] Ian Jolliffe. Principal Component Analysis. Springer  2011.

[10] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model

selection. Annals of Statistics  pages 1302–1338  2000.

[11] Olivier Ledoit and Sandrine Péché. Eigenvectors of some large sample covariance matrix

ensembles. Probability Theory and Related Fields  151(1-2):233–264  2011.

9

[12] Peter Mathé. Saturation of regularization methods for linear ill-posed problems in hilbert spaces.

SIAM journal on numerical analysis  42(3):968–973  2004.

[13] Vidya Muthukumar  Kailas Vodrahalli  and Anant Sahai. Harmless interpolation of noisy data

in regression. arXiv preprint arXiv:1903.09139  2019.

[14] Brady Neal  Sarthak Mittal  Aristide Baratin  Vinayak Tantia  Matthew Scicluna  Simon Lacoste-
Julien  and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv preprint arXiv:1810.08591  2018.

[15] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in Neural Information Processing Systems  pages 1177–1184  2008.

[16] Jack W Silverstein and Sang-Il Choi. Analysis of the limiting spectral distribution of large

dimensional random matrices. Journal of Multivariate Analysis  54(2):295–309  1995.

[17] Stefano Spigler  Mario Geiger  Stéphane d’Ascoli  Levent Sagun  Giulio Biroli  and Matthieu
Wyart. A jamming transition from under-to over-parametrization affects loss landscape and
generalization. arXiv preprint arXiv:1810.09665  2018.

[18] Antonia M Tulino and Sergio Verdú. Random matrix theory and wireless communications.

Foundations and Trends in Communications and Information Theory  1(1):1–182  2004.

[19] Ji Xu and Daniel Hsu. On the number of variables to use in principal component regression.

arXiv preprint arXiv:1906.01139  2019.

[20] Ji Xu  Arian Maleki  and Kamiar Rahnama Rad. Consistent risk estimation in high-dimensional

linear regression. arXiv preprint arXiv:1902.01753  2019.

10

,Ji Xu
Daniel Hsu