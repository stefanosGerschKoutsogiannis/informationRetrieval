2014,Finding a sparse vector in a subspace: Linear sparsity using alternating directions,We consider the problem of recovering the sparsest vector in a subspace $ \mathcal{S} \in \mathbb{R}^p $ with $ \text{dim}(\mathcal{S})=n$. This problem can be considered a homogeneous variant of the sparse recovery problem  and finds applications in sparse dictionary learning  sparse PCA  and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/ \sqrt{n}$. In contrast  we exhibit a relatively simple nonconvex approach based on alternating directions  which provably succeeds even when the fraction of nonzero entries is $\Omega(1)$. To our knowledge  this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model  in which the target sparse vector is embedded in an otherwise random subspace. Empirically  our proposed algorithm also succeeds in more challenging data models arising  e.g.  from sparse dictionary learning.,Finding a sparse vector in a subspace:

Linear sparsity using alternating directions

Qing Qu  Ju Sun  and John Wright

{qq2105  js4038  jw2966}@columbia.edu

Dept. of Electrical Engineering  Columbia University  New York City  NY  USA  10027

Abstract

We consider the problem of recovering the sparsest vector in a subspace S ⊆ Rp
with dim (S) = n. This problem can be considered a homogeneous variant of
the sparse recovery problem  and ﬁnds applications in sparse dictionary learning 
sparse PCA  and other problems in signal processing and machine learning. Simple
√
convex heuristics for this problem provably break down when the fraction of
nonzero entries in the target sparse vector substantially exceeds 1/
n. In contrast 
we exhibit a relatively simple nonconvex approach based on alternating directions 
which provably succeeds even when the fraction of nonzero entries is Ω(1). To
our knowledge  this is the ﬁrst practical algorithm to achieve this linear scaling.
This result assumes a planted sparse model  in which the target sparse vector is
embedded in an otherwise random subspace. Empirically  our proposed algorithm
also succeeds in more challenging data models arising  e.g.  from sparse dictionary
learning. Full version is online: http://arxiv.org/abs/1412.4659.

Introduction

1
Suppose we are given a linear subspace S of a high-dimensional space Rp  which contains a sparse
vector x0 (cid:54)= 0. Given arbitrary basis of S  can we efﬁciently recover x0? Equivalently  provided a
matrix A ∈ R(p−n)×p  can we efﬁciently ﬁnd a nonzero sparse vector x such that Ax = 0? In the
language of sparse approximation  can we solve

(cid:107)x(cid:107)0

min

x

s.t. Ax = 0  x (cid:54)= 0

?

(1)

Variants of this problem have been studied in the context of applications to numerical linear algebra
[13]  graphical model learning [27]  nonrigid structure from motion [14]  spectral estimation and
Prony’s problem [9]  sparse PCA [29]  blind source separation [28]  dictionary learning [23]  graphical
model learning [3]  and sparse coding on manifolds [19].
However  in contrast to the standard sparse regression problem (Ax = b  b (cid:54)= 0)  for which convex
relaxations perform nearly optimally for broad classes of designs A [12  16]  the computational
properties of problem (1) are not nearly as well understood. It has been known for several decades
that the basic formulation

(cid:107)x(cid:107)0  

min

x

s.t. x ∈ S \ {0} 

(2)

is NP-hard [13]. However  it is only recently that efﬁcient computational surrogates with nontrivial
recovery guarantees have been discovered. In the context of sparse dictionary learning  Spielman et
al. [23] introduced a relaxation which replaces the nonconvex problem (2) with a sequence of linear
programs:

(3)
and proved that when S is generated as a span of n random sparse vectors  with high probability
the relaxation recovers these vectors  provided the probability of an entry being nonzero is at most

s.t. xi = 1  x ∈ S  1 ≤ i ≤ p 

(cid:107)x(cid:107)1  

min

x

1

√

(4)

(cid:107)X(cid:107)1  

min
X

s.t.

θ ∈ O (1/
n). In a planted sparse model  in which S consists of a single sparse vector x0 embedded
√
in a “generic” subspace  Hand et al. proved that (3) also correctly recovers x0  provided the fraction
of nonzeros in x0 scales as θ ∈ O (1/
√
n) [17]. Unfortunately  the results of [23  17] are essentially
sharp: when θ substantially exceeds 1/
n  in both models the relaxation (3) provably breaks down.
Moreover  the most natural semideﬁnite programming relaxation of (1) 

(cid:10)A(cid:62)A  X(cid:11) = 0  trace[X] = 1  X (cid:23) 0.

n.1

√
also breaks down at exactly the same threshold of θ ∼ 1/
√
One might naturally conjecture that this 1/
n threshold is simply an intrinsic price we must pay for
having an efﬁcient algorithm  even in these random models. Some evidence towards this conjecture
might be borrowed from the surface similarity of (2)-(4) and sparse PCA [29]. In sparse PCA  there is
√
a substantial gap between what can be achieved with efﬁcient algorithms and the information theoretic
optimum [8]. Is this also the case for recovering a sparse vector in a subspace? Is θ ∈ O (1/
n)
simply the best we can do with efﬁcient  guaranteed algorithms?
Remarkably  this is not the case. Recently  Barak et al. introduced a new rounding technique for
sum-of-squares relaxations  and showed that the sparse vector x0 in the planted sparse model can be

recovered when p ≥ Ω(cid:0)n2(cid:1) and θ ≥ Ω(1) [6]. It is perhaps surprising that this is possible at all with

a polynomial time algorithm. Unfortunately  the runtime of this approach is a high-degree polynomial
in p  and so for machine learning problems in which p is either a feature dimension or sample size 
this algorithm is of theoretical interest only. However  it raises an interesting algorithmic question: Is
there a practical algorithm that provably recovers a sparse vector with θ (cid:29) 1/
n nonzeros from a
generic subspace S?
In this paper  we address this problem  under the following hypotheses: we assume the planted
sparse model  in which a target sparse vector x0 is embedded in an otherwise random n-dimensional
subspace of Rp. We allow x0 to have up to θ0p nonzero entries  where θ0 is a constant. We provide
a relatively simple algorithm which  with very high probability  exactly recovers x0  provided that

√

p ≥ Ω(cid:0)n4 log2 n(cid:1).

Our algorithm is based on alternating directions  with two special twists. First  we introduce a
special data driven initialization  which seems to be important for achieving θ = Ω(1). Second  our
theoretical results require a second  linear programming based rounding phase  which is similar to
[23]. Our core algorithm has very simple iterations  of linear complexity in the size of the data  and
hence should be scalable to moderate-to-large scale problems.
In addition to enjoying theoretical guarantees in a regime (θ = Ω(1)) that is out of the reach
of previous practical algorithms  it performs well in simulations – succeeding empirically with
p ≥ Ω (n log n). It also performs well empirically on more challenging data models  such as the
√
dictionary learning model  in which the subspace of interest contains not one  but n target sparse
vectors. Breaking the O(1/
n) sparsity barrier with a practical algorithm is an important open
problem in the nascent literature on algorithmic guarantees for dictionary learning [5  4  2  1]. We are
optimistic that the techniques introduced here will be applicable in this direction. 2

2 Problem Formulation and Global Optimality
We study the problem of recovering a sparse vector x0 (cid:54)= 0 (up to scale)  which is an element of a
known subspace S ⊂ Rp of dimension n  provided an arbitrary orthonormal basis Y ∈ Rp×n for S.
Our starting point is the nonconvex formulation (2). Both the objective and constraint are nonconvex 
and hence not easy to optimize over. We relax (2) by replacing the (cid:96)0 norm with the (cid:96)1 norm. For the
constraint x (cid:54)= 0  which is necessary to avoid a trivial solution  we force x to live on the unit sphere
(cid:107)x(cid:107)2 = 1  giving

(cid:107)x(cid:107)1  

s.t. x ∈ S  (cid:107)x(cid:107)2 = 1.

min

x

(5)

1This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (with
b (cid:54)= 0)  in which it is possible to handle very large fractions of nonzeros (say  θ = Ω(1/ log n)  or even
θ = Ω(1)) using a very simple (cid:96)1 relaxation [12  16]

2In work currently in preparation [24]  we show that in the dictionary learning problem  efﬁcient algorithms

based on nonconvex optimization also produce global solutions  even when θ = Ω (1).

2

This formulation is still nonconvex  and so we should not expect to obtain an efﬁcient algorithm
that can solve it globally for general inputs S. Nevertheless  the geometry of the sphere is benign
enough that for well-structured inputs it actually will be possible to give algorithms that ﬁnd the
global optimum of this problem.
The formulation (5) can be contrasted with (3)  in which we optimize the (cid:96)1 norm subject to the
constraint (cid:107)x(cid:107)∞ = 1. Because (cid:107)·(cid:107)∞ is polyhedral  that formulation immediately yields a sequence
of linear programs. This is very convenient for computation and analysis  but suffers from the
aforementioned breakdown behavior around (cid:107)x0(cid:107)0 ∼ p/
In contrast  the sphere (cid:107)x(cid:107)2 = 1 is a more complicated geometric constraint  but will allow much
larger numbers of nonzeros in x0. For example  if we consider the global optimizer of a variant of
(5):

√

n.

(cid:107)Yq(cid:107)1  

s.t.

(cid:107)q(cid:107)2 = 1 

min
q∈Rn

(6)

√

under the planted sparse model (detailed below)  e1 is the unique to (6) with very high probability:
Theorem 2.1 ((cid:96)1/(cid:96)2 recovery  planted sparse model). There exists a constant θ0 > 0 such that if the
subspace S follows the planted sparse model

S = span (x0  g1  . . .   gn−1) ⊂ Rp 

(7)
1√
θp Ber(θ)  with x0  g1  . . .   gn−1 mutually independent and
n < θ < θ0  then ±e0 are the only global minimizers to (6) if Y = [x0  g1  . . .   gn−1]  provided

with gi ∼i.i.d. N (0  1/p)  and x0 ∼i.i.d.
1/
p ≥ Ω (n log n). 3
Hence  if we could ﬁnd the global optimizer of (6)  we would be able to recover x0 whose number of
nonzero entries is quite large – even linear in the dimension p (θ = Ω(1)). On the other hand  it is
not obvious that this should be possible: (6) is nonconvex. In the next section  we will describe a
simple heuristic algorithm for (a near approximation of) the (cid:96)1/(cid:96)2 problem (6)  which guarantees
to ﬁnd a stationary point. More surprisingly  we will then prove that for a class of random problem
instances  this algorithm  plus an auxiliary rounding technique  actually recovers the global optimum
– the target sparse vector x0. The proof requires a detailed probabilistic analysis  which is sketched in
Section 4.2.
Before continuing  it is worth noting that the formulation (5) is in no way novel – see  e.g.  the work
of [28] in blind source separation for precedent. However  the novelty originates from our algorithms
and subsequent analysis.

3 Algorithm based on Alternating Direction Method (ADM)
To develop an algorithm for solving (6)  we work with the orthonormal basis Y ∈ Rp×n for S. For
numerical purposes  and also for coping with noise in practical application  it is useful to consider a
slight relaxation of (6)  in which we introduce an auxiliary variable x ≈ Yq:

min
q x

1
2

(cid:107)Yq − x(cid:107)2

2 + λ(cid:107)x(cid:107)1  

s.t.

(cid:107)q(cid:107)2 = 1 

(8)

Here  λ > 0 is a penalty parameter. It is not difﬁcult to see that this problem is equivalent to
minimizing the Huber m-estimator over Yq. This relaxation makes it possible to apply alternating
direction method to this problem  which  starting from some initial point q(0)  alternates between
optimizing with respect to x and optimizing with respect to q:

x(k+1) = arg min

x

q(k+1) = arg min

q

1
2
1
2

(cid:13)(cid:13)(cid:13)Yq(k) − x
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Yq − x(k+1)(cid:13)(cid:13)(cid:13)2

2

2

+ λ(cid:107)x(cid:107)1  

s.t. (cid:107)q(cid:107)2 = 1.

Both (9) and (10) have simple closed form solutions:

x(k+1) = Sλ[Yq(k)] 

q(k+1) =

Y(cid:62)x(k+1)

(cid:13)(cid:13)Y(cid:62)x(k+1)(cid:13)(cid:13)2

 

3In the full version [22]  this theorem has been strengthened and become more practical.

3

(9)

(10)

(11)

where Sλ [x] = sign(x) max{|x| − λ  0} is the soft-thresholding operator. The proposed ADM
algorithm is summarized in Algorithm 1.

The recovered sparse vector ˆx0 = Yq(k)

Algorithm 1 Nonconvex ADM
Input: A matrix Y ∈ Rp×n with Y(cid:62)Y = I  initialization q(0)  threshold λ > 0.
Output:
1: Set k = 0 
2: while not converged do
x(k+1) = Sλ[Yq(k)] 
3:
q(k+1) = Y(cid:62)x(k+1)
(cid:107)Y(cid:62)x(k+1)(cid:107)2
4:
Set k = k + 1.

 

5:
6: end while

The algorithm is simple to state and easy to implement. However  if our goal is to recover the sparsest
vector x0  some additional tricks are needed.

in which x0(i) is nonzero  then x0(i) = Θ(cid:0)1/

Initialization. Because the problem (6) is nonconvex  an arbitrary or random initialization is
unlikely to produce a global minimizer.4 Therefore  good initializations are critical for the proposed
ADM algorithm to succeed. For this purpose  we suggest to use every normalized row of Y as
initializations for q  and solve a sequence of p nonconvex programs (6) by the ADM algorithm.
To get an intuition of why our initialization works  recall the planted sparse model: S =
span(x0  g1  . . .   gn−1). Write Z = [x0 | g1 | ··· | gn−1] ∈ Rp×n.s Suppose we take a row zi of Z 
√
√
are all N (0  1/p)  and so have size about 1/
p. Hence  when θ is not too large  x0(i) will be
somewhat bigger than most of the other entries in zi. Put another way  zi is biased towards the ﬁrst
standard basis vector e1.
Now  under our probabilistic assumptions  Z is very well conditioned: Z(cid:62)Z ≈ I.5 Using  e.g. 
Gram-Schmidt  we can ﬁnd a basis ¯Y for S of the form
¯Y = ZR 

θp(cid:1). Meanwhile  the entries of g1(i)  . . . gn−1(i)

(12)
where R is upper triangular  and R is itself well-conditioned: R ≈ I. Since the i-th row of Z is
biased in the direction of e1 and R is well-conditioned  the i-th row ¯yi is also biased in the direction
of e1. Moreover  we know that the global optimizer q(cid:63) should satisfy ¯Yq(cid:63) = x0. Since Ze1 = x0 
we have q(cid:63) = R−1e1 ≈ e1. Here  the approximation comes from R ≈ I. Hence  for this particular
choice of Y  described in (12)  the i-th row is biased in the direction of the global optimizer.
What if we are handed some other basis Y = ¯YU  where U is an orthogonal matrix? Suppose
q(cid:63) is a global optimizer to (6) with input matrix ¯Y  then it is easy to check that  with input matrix
Y  U(cid:62)q(cid:63) is also a global optimizer to (6)  which implies that our initialization is invariant to any
rotation of the basis. Hence  even if we are handed an arbitrary basis for S  the i-th row is still biased
in the direction of the global optimizer.

Rounding. Let ¯q denote the output of Algorithm 1. We will prove that with our particular initializa-
tion and an appropriate choice of λ  the solution of our ADM algorithm falls within a certain radius
of the globally optimal solution q(cid:63) to (6). To recover q(cid:63)  or equivalently to recover the sparse vector
x0 = Yq(cid:63)  we solve the linear program

(cid:107)Yq(cid:107)1

min

q

s.t.

(cid:104)r  q(cid:105) = 1 

(13)

with r = ¯q. We will prove that if r is close enough to q(cid:63)  then this relaxation exactly recovers q(cid:63) 
and hence x0.

4More precisely  in our models  random initialization does work  but only when the subspace dimension n is

extremely low compared to the ambient dimension p.

5This is the common heuristic that “tall random matrices are well conditioned” [25].

4

4 Analysis

4.1 Main Results

In this section  we describe our main theoretical result  which shows that with high probability  the
algorithm described in the previous section succeeds.
Theorem 4.1. Suppose that S satisﬁes the planted sparse model  and let Y be an arbitrary basis for
√
S. Let y1 . . . yp ∈ Rn denote the (transposes of) the rows of Y. Apply Algorithm 1 with λ = 1/
p 
using initializations q(0) = y1  . . .   yp  to produce outputs ¯q1  . . .   ¯qp. Solve the linear program
(13) with r = ¯q1  . . .   ¯qp  to produce ˆq1  . . .   ˆqp. Set i(cid:63) ∈ arg mini (cid:107)Yˆqi(cid:107)0. Then

Yˆqi(cid:63) = γx0 
for some γ (cid:54)= 0  with overwhelming probability  provided

p > Cn4 log2 n 

and

1√
n

≤ θ ≤ θ0.

(14)

(15)

Here  C and θ0 > 0 are universal constants.

We can see that the result in Theorem 4.1 is suboptimal compared to the global optimality condition

and Barak et al.’s result in sampling complexity: we require p ≥ Ω(cid:0)n4 log2 n(cid:1)  while the global opti-
mality condition and Barak et al demand p ≥ Ω (n log n) and p ≥ Ω(cid:0)n2(cid:1)  respectively. Nonetheless 

compared to Barak et al.  we believe this is the ﬁrst practical and efﬁcient method that is guaranteed
to achieve θ ∼ O(1) rate. The lower bound on θ in Theorem 4.1 is mostly for convenience in the
proof; in fact  the LP rounding stage of our algorithm already succeeds with high probability when
θ ∈ O (1/

√

n).

4.2 A Sketch of Analysis

The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration
properties of Algorithm 1. In this subsection  we brieﬂy sketch the main ideas. For detailed proofs 
please see the technical supplement to this paper.
As noted in Section 3  the ADM algorithm is invariant to change of basis. So  we can assume without
loss of generality that we are working with the particular basis ¯Y = ZR deﬁned in that section. In
order to further streamline the presentation  we are going to sketch the proof under the assumption
that

(16)
rather than the orthogonalized version ¯Y. This may seem plausible  but when p is large Y is already
nearly orthogonal  and hence Y is very close to ¯Y. In fact  in our proof  we simply carry through the
argument for Y  and then note that Y and ¯Y are close enough that all steps of the proof still hold
with Y replaced by ¯Y. With that noted  let y1  . . .   yp ∈ Rn denote the transposes of the rows of Y 
and note that these are independent random vectors. From (11)  we can see one step of the ADM
algorithm takes the form:

Y = [x0 | g1 | ··· | gn−1] 

q(k+1) =

.

(17)

i=1 yiSλ[(cid:0)yi(cid:1)(cid:62)
(cid:80)p
(cid:80)p
(cid:62)
i=1 yiSλ[(yi)

1
p

(cid:13)(cid:13)(cid:13) 1

p

(cid:13)(cid:13)(cid:13)2

q(k)]

q(k)]

yiSλ[(cid:0)yi(cid:1)(cid:62)

p(cid:88)

i=1

Q(q) =

1
p

This is a very favorable form for analysis: if q is viewed as ﬁxed  the term in the numerator is a sum
of p independent random vectors. To this end  we deﬁne a vector valued random process Q(q) on
q ∈ Sn−1  via

q].

(18)

We study the behavior of the iteration (17) through the random process Q(q). We wish to show
that w.h.p. in our choice of Y  q(k) converges to (±e1)  so that the algorithm successfully retrieves
the sparse vector x0 = Ye1. Thus  we hope that in general  Q(q) is more concentrated on the ﬁrst

5

(cid:20) q1

q2

(cid:21)

  with q1 ∈ R and q2 ∈ Rn−1  and

coordinate than q. Let us partition the vector q as q =

correspondingly partition Q(q) =

  where

p(cid:88)

i=1

Q1(q) =

1
p

x0iSλ

p(cid:88)

giSλ

(cid:104)(cid:0)yi(cid:1)(cid:62)

(cid:105)

.

q

(19)

and

Q2(q) =

1
p

i=1

(cid:21)

(cid:20) Q1(q)
(cid:105)

Q2(q)

q

(cid:104)(cid:0)yi(cid:1)(cid:62)

The inner product of Q(q)/(cid:107)Q(q)(cid:107)2 and e1 is strictly larger than the inner product of q and e1 if
and only if

|Q1(q)|
|q1| >

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

.

(20)

In the appendix  we show that with high probability  this inequality holds uniformly over a signiﬁcant
portion of the sphere  so the algorithm moves in the correct direction. To complete the proof of
Theorem 4.1  we combine the following observations:
1. Good initializers. With high probability  at least one of the initializers q(0) satisﬁes |q(0)
2. Uniform progress away from the equator. With high probability  for every q such that
|q1| ≤ C(cid:63)

1 | > 1
√
4

θ  the bound

√
1
θn
2

.
≤

√

θn

|q1| − (cid:107)Q2(q)(cid:107)2
|Q1(q)|
(cid:107)q(cid:107)2

>

c
np

(21)

holds. This implies that if at any iteration k of the algorithm  |q(k)
eventually obtain a point q(k(cid:48))  k(cid:48) > k  for which |q(k(cid:48))
makes bounding the iteration complexity possible.
3. No jumps away from the caps. With high probability  for all q such that |q|1 > C(cid:63)

1 | > 1
√
  the algorithm will
2
θ. Moreover the steady progress

| > C(cid:63)

√

θ 

√

θn

1

(cid:113)|Q1(q)2| + (cid:107)Q2(q)(cid:107)2

|Q1(q)|

2

√
≥ 2

θ.

(22)

θn

√

1 | > 1
√
4

  it will converge to a point ¯q with ¯q1 > C(cid:63)

4. Location of stationary points. Steps 1  2 and 3 above imply that if Algorithm 1 ever obtains a
point q(k) with |q(k)
√
5. Rounding succeeds when |r1| > 2
θ. With high probability  the linear programming based
rounding (13) will produce ±x0  up to scale  whenever it is provided with an input r whose ﬁrst
√
coordinate has magnitude at least 2
Taken together  these claims imply that from at least one of the initializers q(0)  the ADM algorithm
will produce an output ¯q which is accurate enough for LP rounding to exactly return x0  up to scale.
As x0 is the sparsest nonzero vector in the subspace S with overwhelming probability  it will be
selected as Yqi(cid:63)  and hence produced by the algorithm.

θ.

θ.

5 Experimental Results

In this section  we show the performance of the proposed ADM algorithm on both synthetic and real
datasets. On the synthetic dataset  we show the phase transition of our algorithm on both the planted
sparse vector and dictionary learning models; for the real dataset  we demonstrate how seeking sparse
vectors can help discover interesting patterns.

5.1 Phase Transition on Synthetic Data

For the planted sparse model  for each pair of (k  p)  we generate the n dimensional subspace
S ∈ Rp by a k sparse vector x0 with nonzero entries equal to 1 and a random Gaussian matrix
G ∈ Rp×(n−1) with Gij
i.i.d.∼ N (0  1/p)  so that the basis Yof the subspace S can be constructed

6

by Y = GS ([x0  G]) U  where GS (·) denotes the Gram-Schmidt orthonormalization operator and
U ∈ Rn×n is an arbitrary orthogonal matrix. We ﬁx the relationship between n and p as p = 5n log n 
√
and set the regularization parameter in (8) as λ = 1/
p. We use all the normalized rows of Y as
initializations of q for the proposed ADM algorithm  and run every program for 5000 iterations. We
assume the proposed method to be success whenever (cid:107)x0/(cid:107)x0(cid:107)2 − Yq(cid:107)2 ≤  for at least one of the
p programs  for some error tolerance  = 10−3. For each pair of (k  p)  we repeat the simulation for
5 times.

Figure 1: Phase transition for the planted sparse model (left) and dictionary learning (right) using the ADM
algorithm  with ﬁxed relationship between p and n: p = 5n log n. White indicates success and black indicates
failure.

Second  we consider the same dictionary learning model as in [23]. Speciﬁcally  the observation is
assumed to be Y = A0X0where A0 is a square  invertible matrix  and X0 a n × p sparse matrix.
Since A0 is invertible  the row space of Y is the same as that of X0. For each pair of (k  n)  we
(cid:62)  where each vector xi ∈ Rp is k-sparse with every nonzero entry
generate X0 = [x1 ···   xn]

following i.i.d. Gaussian distribution  and construct the observation by Y(cid:62) = GS(cid:0)X(cid:62)

repeat the same experiment as for the planted sparse model presented above. The only difference is
that we assume the proposed method to be success as long as one sparse row of X0 is recovered by
those p programs.
Fig. 1 shows the phase transition between the sparsity level k = θp and p for both models. It seems
clear for both problems our algorithm can work well into (beyond) the linear regime in sparsity level.
Hence for the planted sparse model  to close the gap between our algorithm and practice is one future
direction. Also  how to extend our analysis for dictionary learning is another interesting direction.

(cid:1) U(cid:62). We

0

5.2 Exploratory Experiments on Faces

It is well known in computer vision convex objects only subject to illumination changes produce
image collection that can be well approximated by low-dimensional space in raw-pixel space [7].
We will play with face subspaces here. First  we extract face images of one person (65 images)
under different illumination conditions. Then we apply robust principal component analysis [10]
to the data and get a low dimensional subspace of dimension 10  i.e.  the basis Y ∈ R32256×10. We
apply the ADM algorithm to ﬁnd the sparsest element in such a subspace  by randomly selecting
10% rows as initializations for q. We judge the sparsity in a (cid:96)1/(cid:96)2 sense  that is  the sparsest vector
ˆx0 = Yq∗ should produce the smallest (cid:107)Yq(cid:107)1 /(cid:107)Yq(cid:107)2 among all results. Once some sparse vectors
are found  we project the subspace onto orthogonal complement of the sparse vectors already found 
and continue the seeking process in the projected subspace. Fig. 2 shows the ﬁrst four sparse vectors
we get from the data. We can see they correspond well to different extreme illumination conditions.
Second  we manually select ten different persons’ faces under the normal lighting condition. Again 
the dimension of the subspace is 10 and Y ∈ R32256×10. We repeat the same experiment as stated
above. Fig. 3 shows four sparse vectors we get from the data. Interestingly  the sparse vectors roughly
correspond to differences of face images concentrated around facial parts that different people tend to
differ from each other.
In sum  our algorithm seems to ﬁnd useful sparse vectors for potential applications  like peculiar
discovery in ﬁrst setting  and locating differences in second setting. Netherless  the main goal of this
experiment is to invite readers to think about similar pattern discovery problems that might be cast as
searching for a sparse vector in a subspace. The experiment also demonstrates in a concrete way the

7

Figure 2: Four sparse vectors extracted by the ADM algorithm for one person in the Yale B database under
different illuminations.

Figure 3: Four sparse vectors extracted by the ADM algorithm for 10 persons in the Yale B database under
normal illuminations.

practicality of our algorithm  both in handling data sets of realistic size and in producing attractive
results even outside of the (idealized) planted sparse model that we adopt for analysis.

6 Discussion

The random models we assume for the subspace can be easily extended to other random models 
particularly for dictionary learning. Moreover we believe the algorithm paradigm works far beyond
the idealized models  as our preliminary experiments on face data have clearly shown. For the
particular planted sparse model  the performance gap in terms of (p  n  θ) between the empirical
simulation and our result is likely due to analysis itself. Advanced techniques to bound the empirical
process  such as decoupling [15] techniques  can be deployed in place of our crude union bound
to cover all iterates. Our algorithmic paradigm as a whole sits well in the recent surge of research
endeavors in provable and practical nonconvex approaches towards many problems of interest  often
in large-scale setting [11  20  18  21  26]. We believe this line of research will become increasingly
important in theory and practice. On the application side  the potential of seeking sparse/structured
element in a subspace seems largely unexplored  despite the cases we mentioned at the start. We
hope this work can invite more application ideas.

Acknowledgements.
JS thanks the Wei Family Private Foundation for their generous support. We thank
Cun Mu  IEOR Department of Columbia University  for helpful discussion and input regarding this work. This
work was partially supported by grants ONR N00014-13-1-0492  NSF 1343282  and funding from the Moore
and Sloan Foundations.

References
[1] AGARWAL  A.  ANANDKUMAR  A.  JAIN  P.  NETRAPALLI  P.  AND TANDON  R. Learning sparsely

used overcomplete dictionaries via alternating minimization. arXiv preprint arXiv:1310.7991 (2013).

[2] AGARWAL  A.  ANANDKUMAR  A.  AND NETRAPALLI  P. Exact recovery of sparsely used overcomplete

dictionaries. arXiv preprint arXiv:1309.1952 (2013).

[3] ANANDKUMAR  A.  HSU  D.  JANZAMIN  M.  AND KAKADE  S. M. When are overcomplete topic
models identiﬁable? uniqueness of tensor tucker decompositions with structured sparsity. In Advances in
Neural Information Processing Systems (2013)  pp. 1986–1994.

[4] ARORA  S.  BHASKARA  A.  GE  R.  AND MA  T. More algorithms for provable dictionary learning.

arXiv preprint arXiv:1401.0579 (2014).

8

[5] ARORA  S.  GE  R.  AND MOITRA  A. New algorithms for learning incoherent and overcomplete

dictionaries. arXiv preprint arXiv:1308.6273 (2013).

[6] BARAK  B.  KELNER  J.  AND STEURER  D. Rounding sum-of-squares relaxations. arXiv preprint

arXiv:1312.6652 (2013).

[7] BASRI  R.  AND JACOBS  D. W. Lambertian reﬂectance and linear subspaces. Pattern Analysis and

Machine Intelligence  IEEE Transactions on 25  2 (2003)  218–233.

[8] BERTHET  Q.  AND RIGOLLET  P. Complexity theoretic lower bounds for sparse principal component

detection. In Conference on Learning Theory (2013)  pp. 1046–1066.

[9] BEYLKIN  G.  AND MONZ ´ON  L. On approximation of functions by exponential sums. Applied and

Computational Harmonic Analysis 19  1 (2005)  17–48.

[10] CAND `ES  E.  LI  X.  MA  Y.  AND WRIGHT  J. Robust principal component analysis? Journal of the

ACM 58  3 (May 2011).

[11] CAND `ES  E. J.  LI  X.  AND SOLTANOLKOTABI  M. Phase retrieval via wirtinger ﬂow: Theory and

algorithms. arXiv preprint arXiv:1407.1065 (2014).

[12] CANDES  E. J.  AND TAO  T. Decoding by linear programming. Information Theory  IEEE Transactions

on 51  12 (2005)  4203–4215.

[13] COLEMAN  T. F.  AND POTHEN  A. The null space problem i. complexity. SIAM Journal on Algebraic

Discrete Methods 7  4 (1986)  527–537.

[14] DAI  Y.  LI  H.  AND HE  M. A simple prior-free method for non-rigid structure-from-motion factorization.
In Computer Vision and Pattern Recognition (CVPR)  2012 IEEE Conference on (2012)  IEEE  pp. 2018–
2025.

[15] DE LA PENA  V.  AND GIN ´E  E. Decoupling: from dependence to independence. Springer  1999.
[16] DONOHO  D. L. For most large underdetermined systems of linear equations the minimal (cid:96)1-norm solution

is also the sparsest solution. Communications on pure and applied mathematics 59  6 (2006)  797–829.

[17] HAND  P.  AND DEMANET  L. Recovering the sparsest element in a subspace. arXiv preprint

arXiv:1310.1654 (2013).

[18] HARDT  M. On the provable convergence of alternating minimization for matrix completion. arXiv

preprint arXiv:1312.0925 (2013).

[19] HO  J.  XIE  Y.  AND VEMURI  B. On a nonlinear generalization of sparse coding and dictionary learning.

In Proceedings of The 30th International Conference on Machine Learning (2013)  pp. 1480–1488.

[20] JAIN  P.  NETRAPALLI  P.  AND SANGHAVI  S. Low-rank matrix completion using alternating minimiza-
tion. In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing (2013) 
ACM  pp. 665–674.

[21] NETRAPALLI  P.  JAIN  P.  AND SANGHAVI  S. Phase retrieval using alternating minimization. In

Advances in Neural Information Processing Systems (2013)  pp. 2796–2804.

[22] QU  Q.  SUN  J.  AND WRIGHT  J. Finding a sparse vector in a subspace: Linear sparsity using alternating

directions. arXiv preprint arXiv:1412.4659 (2014).

[23] SPIELMAN  D. A.  WANG  H.  AND WRIGHT  J. Exact recovery of sparsely-used dictionaries. In

Proceedings of the 25th Annual Conference on Learning Theory (2012).

[24] SUN  J.  QU  Q.  AND WRIGHT  J. Complete dictionary recovery over the sphere. In preparation (2014).
Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
[25] VERSHYNIN  R.

arXiv:1011.3027 (2010).

[26] YI  X.  CARAMANIS  C.  AND SANGHAVI  S. Alternating minimization for mixed linear regression.

arXiv preprint arXiv:1310.3745 (2013).

[27] ZHAO  Y.-B.  AND FUKUSHIMA  M. Rank-one solutions for homogeneous linear matrix equations over

the positive semideﬁnite cone. Applied Mathematics and Computation 219  10 (2013)  5569–5583.

[28] ZIBULEVSKY  M.  AND PEARLMUTTER  B. A. Blind source separation by sparse decomposition in a

signal dictionary. Neural computation 13  4 (2001)  863–882.

[29] ZOU  H.  HASTIE  T.  AND TIBSHIRANI  R. Sparse principal component analysis. Journal of computa-

tional and graphical statistics 15  2 (2006)  265–286.

9

,Qing Qu
Ju Sun
John Wright
Brian Bullins
Elad Hazan
Tomer Koren