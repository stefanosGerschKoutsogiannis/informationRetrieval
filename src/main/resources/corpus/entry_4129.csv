2015,Optimal Testing for Properties of Distributions,Given samples from an unknown  distribution  p  is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has receivedtremendous attention in Statistics  albeit focusing onasymptotic analysis  as well as in Computer Science  wherethe emphasis has been on small sample size and computationalcomplexity. Nevertheless  even for basic classes ofdistributions such as monotone  log-concave  unimodal  and monotone hazard rate  the optimal sample complexity is unknown.We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem:Given samplesfrom an unknown distribution p  and a known distribution q  are p and q close in Chi^2-distance  or far in total variation distance?The optimality of all testers is established by providing matching lower bounds. Finally  a necessary building block for our tester and important byproduct of our work are the first known computationally efficient proper learners for discretelog-concave and monotone hazard rate distributions. We exhibit the efficacy of our testers via experimental analysis.,Optimal Testing for Properties of Distributions

Jayadev Acharya  Constantinos Daskalakis  Gautam Kamath

EECS  MIT

{jayadev  costis  g}@mit.edu

Abstract

Given samples from an unknown discrete distribution p  is it possible to distin-
guish whether p belongs to some class of distributions C versus p being far from
every distribution in C? This fundamental question has received tremendous at-
tention in statistics  focusing primarily on asymptotic analysis  as well as in in-
formation theory and theoretical computer science  where the emphasis has been
on small sample size and computational complexity. Nevertheless  even for ba-
sic properties of discrete distributions such as monotonicity  independence  log-
concavity  unimodality  and monotone-hazard rate  the optimal sample complexity
is unknown.
We provide a general approach via which we obtain sample-optimal and compu-
tationally efﬁcient testers for all these distribution families. At the core of our
approach is an algorithm which solves the following problem: Given samples
from an unknown distribution p  and a known distribution q  are p and q close in
2-distance  or far in total variation distance?
The optimality of our testers is established by providing matching lower bounds 
up to constant factors. Finally  a necessary building block for our testers and
an important byproduct of our work are the ﬁrst known computationally efﬁcient
proper learners for discrete log-concave  monotone hazard rate distributions.

1

Introduction

The quintessential scientiﬁc question is whether an unknown object has some property  i.e. whether a
model from a speciﬁc class ﬁts the object’s observed behavior. If the unknown object is a probability
distribution  p  to which we have sample access  we are typically asked to distinguish whether p
belongs to some class C or whether it is sufﬁciently far from it.
This question has received tremendous attention in the ﬁeld of statistics (see  e.g.  [1  2])  where test
statistics for important properties such as the ones we consider here have been proposed. Neverthe-
less  the emphasis has been on asymptotic analysis  characterizing the rates of convergence of test
statistics under null hypotheses  as the number of samples tends to inﬁnity. In contrast  we wish to
study the following problem in the small sample regime:

⇧(C " ): Given a family of distributions C  some "> 0  and sample access to an unknown
distribution p over a discrete support  how many samples are required to distinguish between
p 2C versus dTV(p C) >" ?1
The problem has been studied intensely in the literature on property testing and sublinear algorithms
[3  4  5]  where the emphasis has been on characterizing the optimal tradeoff between p’s support
size and the accuracy " in the number of samples. Several results have been obtained  roughly
1We want success probability at least 2/3  which can be boosted to 1  by repeating the test O(log(1/))

times and taking the majority.

1

clustering into three groups  where (i) C is the class of monotone distributions over [n]  or more
generally a poset [6  7]; (ii) C is the class of independent  or k-wise independent distributions over a
hypergrid [8  9]; and (iii) C contains a single-distribution q  and the problem becomes that of testing
whether p equals q or is far from it [8  10  11  13].
With respect to (iii)  [13] exactly characterizes the number of samples required to test identity to each
distribution q  providing a single tester matching this bound simultaneously for all q. Nevertheless 
this tester and its precursors are not applicable to the composite identity testing problem that we
consider.
If our class C were ﬁnite  we could test against each element in the class  albeit this
would not necessarily be sample optimal. If our class C were a continuum  we would need tolerant
identity testers  which tend to be more expensive in terms of sample complexity [12]  and result
in substantially suboptimal testers for the classes we consider. Or we could use approaches related
to generalized likelihood ratio test  but their behavior is not well-understood in our regime  and
optimizing likelihood over our classes becomes computationally intense.

Our Contributions We obtain sample-optimal and computationally efﬁcient testers for ⇧(C " )
for the most fundamental shape restrictions to a distribution. Our contributions are the following:

1. For a known distribution q over [n]  and sample access to p  we show that distinguishing the
cases: (a) whether the 2-distance between p and q is at most "2/2  versus (b) the `1 distance
between p and q is at least 2"  requires ⇥(pn/"2) samples. As a corollary  we obtain an alternate
argument that shows that identity testing requires ⇥(pn/"2) samples (previously shown in [13]).

n of monotone distributions over [n]d we require an optimal ⇥nd/2/"2
2. For the class C = Md
number of samples  where prior work requires ⌦pn log n/"6 samples for d = 1 and
˜⌦nd1/2poly (1/") for d > 1 [6  7]. Our results improve the exponent of n with respect

to d  shave all logarithmic factors in n  and improve the exponent of " by at least a factor of 2.
(a) A useful building block and interesting byproduct of our analysis is extending Birg´e’s obliv-
ious decomposition for single-dimensional monotone distributions [14] to monotone distri-
butions in d  1  and to the stronger notion of 2-distance. See Section C.1.
[n]d in 2-distance. See Lemma 3 for the precise statement.

(b) Moreover  we show that O(logd n) samples sufﬁce to learn a monotone distribution over

3. For the class C =⇧ d of product distributions over [n1] ⇥···⇥ [nd]  our algorithm requires
O(Q` n`)1/2 +P` n` /"2 samples. We note that a product distribution is one where all
marginals are independent  so this is equivalent to testing if a collection of random variables are
all independent. In the case where n`’s are large  then the ﬁrst term dominates  and the sample
complexity is O((Q` n`)1/2 /"2). In particular  when d is a constant and all n`’s are equal to n 
we achieve the optimal sample complexity of ⇥(nd/2/"2). To the best of our knowledge  this
is the ﬁrst result for d  3  and when d = 2  this improves the previously known complexity
"6 polylog(n/") [8  15]  signiﬁcantly improving the dependence on " and shaving all
from O n
logarithmic factors.
4. For the classes C = LCDn  C = MHRn and C = Un of log-concave  monotone-hazard-rate
and unimodal distributions over [n]  we require an optimal ⇥pn/"2 number of samples. Our
testers for LCDn and C = MHRn are to our knowledge the ﬁrst for these classes for the
low sample regime we are studying—see [16] and its references for statistics literature on the
asymptotic regime. Our tester for Un improves the dependence of the sample complexity on " by
at least a factor of 2 in the exponent  and shaves all logarithmic factors in n  compared to testers
based on testing monotonicity.
(a) A useful building block and important byproduct of our analysis are the ﬁrst computation-
ally efﬁcient algorithms for properly learning log-concave and monotone-hazard-rate dis-
tributions  to within " in total variation distance  from poly(1/") samples  independent of
the domain size n. See Corollaries 4 and 6. Again  these are the ﬁrst computationally ef-
ﬁcient algorithms to our knowledge in the low sample regime. [17] provide algorithms for
density estimation  which are non-proper  i.e. will approximate an unknown distribution
from these classes with a distribution that does not belong to these classes. On the other
hand  the statistics literature focuses on maximum-likelihood estimation in the asymptotic
regime—see e.g. [18] and its references.

2

5. For all the above classes we obtain matching lower bounds  showing that the sample complexity
of our testers is optimal with respect to n  " and when applicable d. See Section 8. Our lower
bounds are based on extending Paninski’s lower bound for testing uniformity [10].

Our Techniques At the heart of our tester lies a novel use of the 2 statistic. Naturally  the 2
and its related `2 statistic have been used in several of the afore-cited results. We propose a new
use of the 2 statistic enabling our optimal sample complexity. The essence of our approach is to
ﬁrst draw a small number of samples (independent of n for log-concave and monotone-hazard-rate
distributions and only logarithmic in n for monotone and unimodal distributions) to approximate the
unknown distribution p in 2 distance. If p 2C   our learner is required to output a distribution q
that is O(")-close to C in total variation and O("2)-close to p in 2 distance. Then some analysis
reduces our testing problem to distinguishing the following cases:

• p and q are O("2)-close in 2 distance; this case corresponds to p 2C .
• p and q are ⌦(")-far in total variation distance; this case corresponds to dTV(p C) >" .

We draw a comparison with robust identity testing  in which one must distinguish whether p and q are
c1"-close or c2"-far in total variation distance  for constants c2 > c1 > 0. In [12]  Valiant and Valiant
show that ⌦(n/ log n) samples are required for this problem – a nearly-linear sample complexity 
which may be prohibitively large in many settings. In comparison  the problem we study tests for
2 closeness rather than total variation closeness: a relaxation of the previous problem. However 
our tester demonstrates that this relaxation allows us to achieve a substantially sublinear complexity
of O(pn/"2). On the other hand  this relaxation is still tight enough to be useful  demonstrated by
our application in obtaining sample-optimal testers.
We note that while the 2 statistic for testing hypothesis is prevalent in statistics providing opti-
mal error exponents in the large-sample regime  to the best of our knowledge  in the small-sample
regime  modiﬁed-versions of the 2 statistic have only been recently used for closeness-testing
in [19  20  21] and for testing uniformity of monotone distributions in [22].
In particular  [19]
design an unbiased statistic for estimating the 2 distance between two unknown distributions.

Organization In Section 4  we show that a version of the 2 statistic  appropriately excluding
certain elements of the support  is sufﬁciently well-concentrated to distinguish between the above
cases. Moreover  the sample complexity of our algorithm is optimal for most classes. Our base tester
is combined with the afore-mentioned extension of Birg´e’s decomposition theorem to test monotone
distributions in Section 5 (see Theorem 2 and Corollary 1)  and is also used to test independence of
distributions in Section 6 (see Theorem 3).
In Section 7  we give our results on testing unimodal  log-concave and monotone hazard rate dis-
tributions. Naturally  there are several bells and whistles that we need to add to the above skeleton
to accommodate all classes of distributions that we are considering. In Remark 1 we mention the
additional modiﬁcations for these classes.

Related Work. For the problems that we study in this paper  we have provided the related works
in the previous section along with our contributions. We cannot do justice to the role of shape re-
strictions of probability distributions in probabilistic modeling and testing. It sufﬁces to say that
the classes of distributions that we study are fundamental  motivating extensive literature on their
learning and testing [23]. In the recent times  there has been work on shape restricted statistics  pio-
neered by Jon Wellner  and others. [24  25] study estimation of monotone and k-monotone densities 
and [26  27] study estimation of log-concave distributions. Due to the sheer volume of literature in
statistics in this ﬁeld  we will restrict ourselves to those already referenced.
As we have mentioned  statistics has focused on the asymptotic regime as the number of samples
tends to inﬁnity. Instead we are considering the low sample regime and are more stringent about the
behavior of our testers  requiring 2-sided guarantees. We want to accept if the unknown distribution
is in our class of interest  and also reject if it is far from the class. For this problem  as discussed
above  there are few results when C is a whole class of distributions. Closer related to our paper is
the line of papers [6  7  28] for monotonicity testing  albeit these papers have sub-optimal sample
complexity as discussed above. Testing independence of random variables has a long history in
statisics [29  30]. The theoretical computer science community has also considered the problem of

3

testing independence of two random variables [8  15]. While our results sharpen the case where the
variables are over domains of equal size  they demonstrate an interesting asymmetric upper bound
when this is not the case. More recently  Acharya and Daskalakis provide optimal testers for the
family of Poisson Binomial Distributions [31].
Finally  contemporaneous work of Canonne et al [32] provides a generic algorithm and lower bounds
for the single-dimensional families of distributions considered here. We note that their algorithm
has a sample complexity which is suboptimal in both n and "  while our algorithms are optimal.
Their algorithm also extends to mixtures of these classes  though some of these extensions are not
computationally efﬁcient. They also provide a framework for proving lower bounds  giving the
optimal bounds for many classes when " is sufﬁciently large with respect to 1/n. In comparison  we
provide these lower bounds unconditionally by modifying Paninski’s construction [10] to suit the
classes we consider.

2 Preliminaries

We use the following probability distances in our paper.
The total variation distance between distributions p and q is dTV(p  q) def= supA |p(A)  q(A)| =
2kp qk1. The 2-distance between p and q over [n] is deﬁned as 2(p  q) def= Pi2[n]
. The
1
Kolmogorov distance between two probability measures p and q over an ordered set (e.g.  R) with
cumulative density functions Fp and Fq is dK(p  q) def= supx2R |Fp(x)  Fq(x)|.
Our paper is primarily concerned with testing against classes of distributions  deﬁned formally as:
Deﬁnition 1. Given " 2 (0  1] and sample access to a distribution p  an algorithm is said to test a
class C if it has the following guarantees:

(piqi)2

qi

• If p 2C   the algorithm outputs ACCEPT with probability at least 2/3;
• If dTV(p C)  "  the algorithm outputs REJECT with probability at least 2/3.

We note the following useful relationships between these distances [33]:
Proposition 1. dK(p  q)2  dTV(p  q)2  1
Deﬁnition 2. An ⌘-effective support of a distribution p is any set S such that p(S)  1  ⌘.
The ﬂattening of a function f over a subset S is the function ¯f such that ¯fi = p(S)/|S|.
Deﬁnition 3. Let p be a distribution  and support I1  . . . is a partition of the domain. The ﬂattening
of p with respect to I1  . . . is the distribution ¯p which is the ﬂattening of p over the intervals I1  . . ..

4 2(p  q).

Poisson Sampling Throughout this paper  we use the standard Poissonization approach. Instead
of drawing exactly m samples from a distribution p  we ﬁrst draw m0 ⇠ Poisson(m)  and then
draw m0 samples from p. As a result  the number of times different elements in the support of p
occur in the sample become independent  giving much simpler analyses. In particular  the number
of times we will observe domain element i will be distributed as Poisson(mpi)  independently for
each i. Since Poisson(m) is tightly concentrated around m  this additional ﬂexibility comes only at
a sub-constant cost in the sample complexity with an inversely exponential in m  additive increase
in the error probability.

3 The Testing Algorithm – An Overview
Our algorithm for testing a class C can be decomposed into three steps.
Near-proper learning in 2-distance. Our ﬁrst step requires learning with very speciﬁc guarantees.
Given sample access to p 2C   we wish to output q such that (i) q is close to C in total variation
distance  and (ii) p and q are O("2)-close in 2-distance on an "effective support2 of p. When
2We also require the algorithm to output a description of an effective support for which this property holds.

This requirement can be slightly relaxed  as we show in our results for testing unimodality.

4

p is not in C  we do not guarantee anything about q. From an information theoretic standpoint 
this problem is harder than learning the distribution in total variation  since 2-distance is more
restrictive than total variation distance. Nonetheless  for the structured classes we consider  we are
able to learn in 2 by modifying the approaches to learn in total variation.
Computation of distance to class. The next step is to see if the hypothesis q is close to the class C
or not. Since we have an explicit description of q  this step requires no further samples from p  i.e.
it is purely computational. If we ﬁnd that q is far from the class C  then it must be that p 62 C  as
otherwise the guarantees from the previous step would imply that q is close to C. Thus  if it is not 
we can terminate the algorithm at this point.
2-testing. At this point  the previous two steps guarantee that our distribution q is such that:
- If p 2C   then p and q are close in 2 distance on a (known) effective support of p;
- If dTV(p C)  "  then p and q are far in total variation distance.
We can distinguish between these two cases using O(pn/"2) samples with a simple statistical 2-
test  that we describe in Section 4.

Using the above three-step approach  our tester  as described in the next section  can directly test
monotonicity  log-concavity  and monotone hazard rate. With an extra trick  using Kolmogorov’s
max inequality  it can also test unimodality.

4 A Robust 2-`1 Identity Test

Our main result in this section is Theorem 1.
Theorem 1. Given " 2 (0  1]  a class of probability distributions C  sample access to a distribution
p  and an explicit description of a distribution q  both over [n] with the following properties:

Property 1. dTV(q C)  "
2.
Property 2. If p 2C   then 2(p  q)  "2
500.

Then there exists an algorithm such that: If p 2C   it outputs ACCEPT with probability at least 2/3;
If dTV(p C)  "  it outputs REJECT with probability at least 2/3. The time and sample complexity
of this algorithm are Opn/"2.

Proof. Algorithm 1 describes a 2 testing procedure that gives the guarantee of the theorem.

the number of occurrences of the ith domain element.

Algorithm 1 Chi-squared testing algorithm
1: Input: "; an explicit distribution q; (Poisson) m samples from a distribution p  where Ni denotes
2: A { i : qi  "2/50n}
3: Z Pi2A
4: if Z  m"2/10 return close
5: else return far

(Nimqi)2Ni

mqi

qi

p2
i
q2
i

+ 4m ·

(pi  qi)2

= m · 2(pA  qA)  Var [Z] =Xi2A2

In Section A we compute the mean and variance of the statistic Z (deﬁned in Algorithm 1) as:
pi · (pi  qi)2

 (1)
E [Z] = m ·Xi2A
where by pA and qA we denote respectively the vectors p and q restricted to the coordinates in
A  and we slightly abuse notation when we write 2(pA  qA)  as these do not then correspond to
probability distributions.
Lemma 1 demonstrates the separation in the means of the statistic Z in the two cases of interest 
i.e.  p 2C versus dTV(p C)  "  and Lemma 2 shows the separation in the variances in the two
cases. These two results are proved in Section B.

q2
i

5

Lemma 1. If p 2C   then E [Z]  m"2/500. If dTV(p C)  "  then E [Z]  m"2/5.
Lemma 2. Let m  20000pn/"2. If p 2C then Var [Z]  1
Var [Z]  1
Assuming Lemmas 1 and 2  Theorem 1 is now a simple application of Chebyshev’s inequality.

500000 m2"4. If dTV(p C)  "  then

100 E[Z]2.

When p 2C   we have that E [Z] +p3 Var [Z] ⇣1/500 +p3/500000⌘ m"2  m"2/200. Thus 

Chebyshev’s inequality gives

Pr⇥Z  m"2/10⇤  Pr⇥Z  m"2/200⇤  PrhZ  E [Z]  p3 Var [Z]1/2i  1/3.
When dTV(p C)  "  E [Z] p3 Var [Z] ⇣1 p3/100⌘ E[Z]  3m"2/20. Therefore 
Pr⇥Z  m"2/10⇤  Pr⇥Z  3m"2/20⇤  PrhZ  E [Z]  p3 Var [Z]1/2i  1/3.

This proves the correctness of Algorithm 1. For the running time  we divide the summation in Z
into the elements for which Ni > 0 and Ni = 0. When Ni = 0  the contribution of the term to
the summation is mqi  and we can sum them up by subtracting the total probability of all elements
appearing at least once from 1.
Remark 1. To apply Theorem 1  we need to learn distribution in C and ﬁnd a q that is O("2)-close
in 2-distance to p. For the class of monotone distributions  we are able to efﬁciently obtain such
a q  which immediately implies sample-optimal learning algorithms for this class. However  for
some classes  we may not be able to learn a q with such strong guarantees  and we must consider
modiﬁcations to our base testing algorithm.
For example  for log-concave and monotone hazard rate distributions  we can obtain a distribution
q and a set S with the following guarantees:
- If p 2C   then 2(pS  qS)  O("2) and p(S)  1  O(");
- If dTV(p C)  "  then dTV(p  q)  "/2. In this scenario  the tester will simply pretend that the
support of p and q is S  ignoring any samples and support elements in [n] \ S. Analysis of this
tester is extremely similar to Theorem 1. In particular  we can still show that the statistic Z will be
separated in the two cases. When p 2C   excluding [n] \ S will only reduce Z. On the other hand 
when dTV(p C)  "  since p(S)  1 O(")  p and q must still be far on the remaining support  and
we can show that Z is still sufﬁciently large. Therefore  a small modiﬁcation allows us to handle
this case with the same sample complexity of O(pn/"2).
For unimodal distributions  we are even unable to identify a large enough subset of the support
where the 2 approximation is guaranteed to be tight. But we can show that there exists a light
enough piece of the support (in terms of probability mass under p) that we can exclude to make the
2 approximation tight. Given that we only use Chebyshev’s inequality to prove the concentration
of the test statistic  it would seem that our lack of knowledge of the piece to exclude would involve a
union bound and a corresponding increase in the required number of samples. We avoid this through
a careful application of Kolmogorov’s max inequality in our setting. See Theorem 7 of Section 7.

5 Testing Monotonicity

As the ﬁrst application of our testing framework  we will demonstrate how to test for monotonicity.
Let d  1  and i = (i1  . . .   id)  j = (j1  . . .   jd) 2 [n]d. We say i < j if il  jl for l = 1  . . .   d. A
distribution p over [n]d is monotone (decreasing) if for all i < j  pi  pj
We follow the steps in the overview. The learning result we show is as follows (proved in Section C).

3.

3This deﬁnition describes monotone non-increasing distributions. By symmetry  identical results hold for

monotone non-decreasing distributions.

6

Lemma 3. Let d  1. There is an algorithm that takes m = O((d log(n)/"2)d/"2) samples from a
distribution p over [n]d  and outputs a distribution q such that if p is monotone  then with probability
at least 5/6  2(p  q)  "2
500. Furthermore  the distance of q to monotone distributions can be
computed in time poly(m).

·

1

"2!

O nd/2

This accomplishes the ﬁrst two steps in the overview. In particular  if the distance of q from mono-
tone distributions is more than "/2  we declare that p is not monotone. Therefore  Property 1 in
Theorem 1 is satisﬁed  and the lemma states that Property 2 holds with probability at least 5/6. We
then proceed to the 2  `1 test. At this point  we have precisely the guarantees needed to apply
Theorem 1 over [n]d  directly implying our main result of this section:
Theorem 2. For any d  1  there exists an algorithm for testing monotonicity over [n]d with sample
complexity

"2 +✓ d log n
"2 ◆d
and time complexity Ond/2/"2 + poly(log n  1/")d.
In particular  this implies the following optimal algorithms for monotonicity testing for all d  1:
Corollary 1. Fix any d  1  and suppose "> pd log n/n1/4. Then there exists an algorithm for
testing monotonicity over [n]d with sample complexity Ond/2/"2.

We note that the class of monotone distributions is the simplest of the classes we consider. We
now consider testing for log-concavity  monotone hazard rate  and unimodality  all of which are
much more challenging to test. In particular  these classes require a more sophisticated structural
understanding  more complex proper 2-learning algorithms  and non-trivial modiﬁcations to our
2-tester. We have already given some details on the required adaptations to the tester in Remark 1.
Our algorithms for learning these classes use convex programming. One of the main challenges is
to enforce log-concavity of the PDF when learning LCDn (respectively  of the CDF when learning
MHRn)  while simultaneously enforcing closeness in total variation distance. This involves a
careful choice of our variables  and we exploit structural properties of the classes to ensure the
soundness of particular Taylor approximations. We encourage the reader to refer to the proofs of
Theorems 7  8  and 9 for more details.

6 Testing Independence of Random Variables

Let X def= [n1] ⇥ . . . ⇥ [nd]  and let ⇧d be the class of all product distributions over X . Similar to
learning monotone distributions in 2 distance we prove the following result in Section E.
`=1 n`)/"2⌘ samples from a distribution p and
Lemma 4. There is an algorithm that takes O⇣(Pd

outputs a q 2 ⇧d such that if p 2 ⇧d  then with probability at least 5/6  2(p  q)  O("2).
The distribution q always satisﬁes Property 1 since it is in ⇧d  and by this lemma  with probability
at least 5/6 satisﬁes Property 2 in Theorem 1. Therefore  we obtain the following result.
Theorem 3. For any d  1  there exists an algorithm for testing independence of random variables

over [n1] ⇥ . . . [nd] with sample and time complexity O⇣⇣(Qd

`=1 n`)1/2 +Pd

When d = 2 and n1 = n2 = n this improves the result of [8] for testing independence of two
random variables.
Corollary 2. Testing if two distributions over [n] are independent has sample complexity ⇥(n/"2).

`=1 n`⌘ /"2⌘ .

7 Testing Unimodality  Log-Concavity and Monotone Hazard Rate

Unimodal distributions over [n] (denoted by Un) are all distributions p for which there exists an i⇤
such that pi is non-decreasing for i  i⇤ and non-increasing for i  i⇤. Log-concave distributions
over [n] (denoted by LCDn)  is the sub-class of unimodal distributions for which pi1pi+1  p2
i .

7

Monotone hazard rate (MHR) distributions over [n] (denoted by MHRn)  are distributions p with
CDF F for which i < j implies

.

fi

1Fi  fj
1Fj

tonicity to give an algorithm with Opn log n/"2 samples. However  this is unsatisfactory  since

The following theorem bounds the complexity of testing these classes (for moderate ").
Theorem 4. Suppose "> n 1/5. For each of the classes  unimodal  log-concave  and MHR  there
exists an algorithm for testing the class over [n] with sample complexity O(pn/"2).
This result is a corollary of the speciﬁc results for each class  which is proved in the appendix.
In particular  a more complete statement for unimodality  log-concavity and monotone-hazard rate 
with precise dependence on both n and " is given in Theorems 7  8 and 9 respectively. We mention
some key points about each class  and refer the reader to the respective appendix for further details.
Testing Unimodality Using a union bound argument  one can use the results on testing mono-
our lower bound (and as we will demonstrate  the true complexity of this problem) is pn/"2. We
overcome the logarithmic barrier introduced by the union bound  by employing a non-oblivious
decomposition of the domain  and using Kolmogorov’s max-inequality.
Testing Log-Concavity The key step is to design an algorithm to learn a log-concave distribution
in 2 distance. We formulate the problem as a linear program in the logarithms of the distribution
and show that using O(1/"5) samples  it is possible to output a log-concave distribution that has a
2 distance at most O("2) from the underlying log-concave distribution.
Testing Monotone Hazard Rate For learning MHR distributions in 2 distance  we formulate a
linear program in the logarithms of the CDF and show that using O(log(n/")/"5) samples  it is
possible to output a MHR distribution that has a 2 distance at most O("2) from the underlying
MHR distribution.

8 Lower Bounds

We now prove sharp lower bounds for the classes of distributions we consider. We show that the
example studied by Paninski [10] to prove lower bounds on testing uniformity can be used to prove
lower bounds for the classes we consider. They consider a class Q consisting of 2n/2 distributions
deﬁned as follows. Without loss of generality assume that n is even. For each of the 2n/2 vectors
z0z1 . . . zn/21 2 {1  1}n/2  deﬁne a distribution q 2Q over [n] as follows.

qi =( (1+z`c")

(1z`c")

n

n

for i = 2` + 1
for i = 2`.

(2)

Each distribution in Q has a total variation distance c"/2 from Un  the uniform distribution over
[n]. By choosing c to be an appropriate constant  Paninski [10] showed that a distribution picked
uniformly at random from Q cannot be distinguished from Un with fewer than pn/"2 samples with
probability at least 2/3.
Suppose C is a class of distributions such that (i) The uniform distribution Un is in C  (ii) For
appropriately chosen c  dTV(C Q)  "  then testing C is not easier than distinguishing Un from Q.
Invoking [10] immediately implies that testing the class C requires ⌦(pn/"2) samples.
The lower bounds for all the one dimensional distributions will follow directly from this construc-
tion  and for testing monotonicity in higher dimensions  we extend this construction to d  1 
appropriately. These arguments are proved in Section H  leading to the following lower bounds for
testing these classes:
Theorem 5.
• For any d  1  any algorithm for testing monotonicity over [n]d requires ⌦(nd/2/"2) samples.
• For d  1  testing independence over [n1]⇥···⇥ [nd] requires ⌦(n1n2 . . . nd)1/2/"2 samples.
• Testing unimodality  log-concavity  or monotone hazard rate over [n] needs ⌦(pn/"2) samples.

8

References
[1] R. A. Fisher  Statistical Methods for Research Workers. Edinburgh: Oliver and Boyd  1925.
[2] E. Lehmann and J. Romano  Testing statistical hypotheses. Springer Science & Business Media  2006.
[3] E. Fischer  “The art of uninformed decisions: A primer to property testing ” Science  2001.
[4] R. Rubinfeld  “Sublinear-time algorithms ” in International Congress of Mathematicians  2006.
[5] C. L. Canonne  “A survey on distribution testing: your data is big  but is it blue ” ECCC  2015.
[6] T. Batu  R. Kumar  and R. Rubinfeld  “Sublinear algorithms for testing monotone and unimodal distribu-

tions ” in Proceedings of STOC  2004.

[7] A. Bhattacharyya  E. Fischer  R. Rubinfeld  and P. Valiant  “Testing monotonicity of distributions over

general partial orders ” in ICS  2011  pp. 239–252.

[8] T. Batu  E. Fischer  L. Fortnow  R. Kumar  R. Rubinfeld  and P. White  “Testing random variables for

independence and identity ” in Proceedings of FOCS  2001.

[9] N. Alon  A. Andoni  T. Kaufman  K. Matulef  R. Rubinfeld  and N. Xie  “Testing k-wise and almost

k-wise independence ” in Proceedings of STOC  2007.

[10] L. Paninski  “A coincidence-based test for uniformity given very sparsely sampled discrete data.” IEEE

Transactions on Information Theory  vol. 54  no. 10  2008.

[11] D. Huang and S. Meyn  “Generalized error exponents for small sample universal hypothesis testing ”

IEEE Transactions on Information Theory  vol. 59  no. 12  pp. 8157–8181  Dec 2013.

[12] G. Valiant and P. Valiant  “Estimating the unseen: An n/ log n-sample estimator for entropy and support

size  shown optimal via new CLTs ” in Proceedings of STOC  2011.

[13] ——  “An automatic inequality prover and instance optimal identity testing ” in FOCS  2014.
[14] L. Birg´e  “Estimating a density under order restrictions: Nonasymptotic minimax risk ” The Annals of

Statistics  vol. 15  no. 3  pp. 995–1012  September 1987.

[15] R. Levi  D. Ron  and R. Rubinfeld  “Testing properties of collections of distributions ” Theory of Com-

[16] P. Hall and I. Van Keilegom  “Testing for monotone increasing hazard rate ” Annals of Statistics  pp.

puting  vol. 9  no. 8  pp. 295–347  2013.

1109–1137  2005.

[17] S. O. Chan  I. Diakonikolas  R. A. Servedio  and X. Sun  “Learning mixtures of structured distributions

over discrete domains ” in Proceedings of SODA  2013.

[18] M. Cule and R. Samworth  “Theoretical properties of the log-concave maximum likelihood estimator of

a multidimensional density ” Electronic Journal of Statistics  vol. 4  pp. 254–270  2010.

[19] J. Acharya  H. Das  A. Jafarpour  A. Orlitsky  S. Pan  and A. T. Suresh  “Competitive classiﬁcation and

[20] S. Chan  I. Diakonikolas  G. Valiant  and P. Valiant  “Optimal algorithms for testing closeness of discrete

closeness testing ” in COLT  2012  pp. 22.1–22.18.

distributions ” in SODA  2014  pp. 1193–1203.

[21] B. B. Bhattacharya and G. Valiant  “Testing closeness with unequal sized samples ” in NIPS  2015.
[22] J. Acharya  A. Jafarpour  A. Orlitsky  and A. Theertha Suresh  “A competitive test for uniformity of

monotone distributions ” in Proceedings of AISTATS  2013  pp. 57–65.

[23] R. E. Barlow  D. J. Bartholomew  J. M. Bremner  and H. D. Brunk  Statistical Inference under Order

[24] H. K. Jankowski and J. A. Wellner  “Estimation of a discrete monotone density ” Electronic Journal of

Restrictions. New York: Wiley  1972.

Statistics  vol. 3  pp. 1567–1605  2009.

[25] F. Balabdaoui and J. A. Wellner  “Estimation of a k-monotone density: characterizations  consistency and

minimax lower bounds ” Statistica Neerlandica  vol. 64  no. 1  pp. 45–70  2010.

[26] F. Balabdaoui  H. Jankowski  and K. Ruﬁbach  “Maximum likelihood estimation and conﬁdence bands

for a discrete log-concave distribution ” 2011. [Online]. Available: http://arxiv.org/abs/1107.3904v1

[27] A. Saumard and J. A. Wellner  “Log-concavity and strong log-concavity: a review ” Statistics Surveys 

vol. 8  pp. 45–114  2014.

[28] M. Adamaszek  A. Czumaj  and C. Sohler  “Testing monotone continuous distributions on high-

dimensional real cubes ” in SODA  2010  pp. 56–65.

[29] J. N. Rao and A. J. Scott  “The analysis of categorical data from complex sample surveys ” Journal of the

American Statistical Association  vol. 76  no. 374  pp. 221–230  1981.
[30] A. Agresti and M. Kateri  Categorical data analysis. Springer  2011.
[31] J. Acharya and C. Daskalakis  “Testing Poisson Binomial Distributions ” in SODA  2015  pp. 1829–1840.
[32] C. Canonne  I. Diakonikolas  T. Gouleakis  and R. Rubinfeld  “Testing shape restrictions of discrete

[33] A. L. Gibbs and F. E. Su  “On choosing and bounding probability metrics ” International Statistical

distributions ” in STACS  2016.

Review  vol. 70  no. 3  pp. 419–435  dec 2002.

[34] J. Acharya  A. Jafarpour  A. Orlitsky  and A. T. Suresh  “Efﬁcient compression of monotone and m-modal

[35] S. Kamath  A. Orlitsky  D. Pichapati  and A. T. Suresh  “On learning distributions from their samples ” in

distributions ” in ISIT  2014.

COLT  2015.

[36] P. Massart  “The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality ” The Annals of Probability 

vol. 18  no. 3  pp. 1269–1283  07 1990.

9

,Jayadev Acharya
Constantinos Daskalakis
Gautam Kamath
Tomas Jakab
Ankush Gupta
Hakan Bilen
Andrea Vedaldi
Edgar Dobriban
Sifan Liu