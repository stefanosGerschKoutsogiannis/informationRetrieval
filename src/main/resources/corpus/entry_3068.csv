2009,Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization,We address the problem of learning classifiers when observations have multiple views  some of which may not be observed for all examples.  We assume the existence of view generating functions which may complete the missing views in an approximate way.  This situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages.  In that case  Machine Translation (MT) systems may be used to translate each document in the missing languages.  We derive a generalization error bound for classifiers learned on examples with multiple artificially created views. Our result uncovers a trade-off between the size of the training set  the number of views  and the quality of the view generating functions. As a consequence  we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning.  An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning.  Experimental results on a subset of the Reuters RCV1/RCV2 collections support our findings by showing that additional views obtained from MT may significantly improve the classification performance in the cases identified by our trade-off.,Learning from Multiple Partially Observed Views –
an Application to Multilingual Text Categorization

Massih R. Amini

Interactive Language Technologies Group

National Research Council Canada

Nicolas Usunier

Laboratoire d’Informatique de Paris 6
Universit´e Pierre et Marie Curie  France

Massih-Reza.Amini@cnrc-nrc.gc.ca

Nicolas.Usunier@lip6.fr

Cyril Goutte

Interactive Language Technologies Group

National Research Council Canada
Cyril.Goutte@cnrc-nrc.gc.ca

Abstract

We address the problem of learning classiﬁers when observations have multiple
views  some of which may not be observed for all examples. We assume the
existence of view generating functions which may complete the missing views
in an approximate way. This situation corresponds for example to learning text
classiﬁers from multilingual collections where documents are not available in all
languages. In that case  Machine Translation (MT) systems may be used to trans-
late each document in the missing languages. We derive a generalization error
bound for classiﬁers learned on examples with multiple artiﬁcially created views.
Our result uncovers a trade-off between the size of the training set  the number
of views  and the quality of the view generating functions. As a consequence 
we identify situations where it is more interesting to use multiple views for learn-
ing instead of classical single view learning. An extension of this framework is
a natural way to leverage unlabeled multi-view data in semi-supervised learning.
Experimental results on a subset of the Reuters RCV1/RCV2 collections support
our ﬁndings by showing that additional views obtained from MT may signiﬁcantly
improve the classiﬁcation performance in the cases identiﬁed by our trade-off.

1

Introduction

We study the learning ability of classiﬁers trained on examples generated from different sources 
but where some observations are partially missing. This problem occurs for example in non-parallel
multilingual document collections  where documents may be available in different languages  but
each document in a given language may not be translated in all (or any) of the other languages.

Our framework assumes the existence of view generating functions which may approximate miss-
ing examples using the observed ones. In the case of multilingual corpora these view generating
functions may be Machine Translation systems which for each document in one language produce
its translations in all other languages. Compared to other multi-source learning techniques [6] 
we address a different problem here by transforming our initial problem of learning from partially
observed examples obtained from multiple sources into the classical multi-view learning. The con-
tributions of this paper are twofold. We ﬁrst introduce a supervised learning framework in which
we deﬁne different multi-view learning tasks. Our main result is a generalization error bound for
classiﬁers trained over multi-view observations. From this result we induce a trade-off between the
number of training examples  the number of views and the ability of view generating functions to

produce accurate additional views. This trade-off helps us identify situations in which artiﬁcially
generated views may lead to substantial performance gains. We then show how the agreement of
classiﬁers over their class predictions on unlabeled training data may lead to a much tighter trade-off.
Experiments are carried out on a large part of the Reuters RCV1/RCV2 collections  freely available
from Reuters  using 5 well-represented languages for text classiﬁcation. Our results show that our
approach yields improved classiﬁcation performance in both the supervised and semi-supervised
settings.

In the following two sections  we ﬁrst deﬁne our framework  then the learning tasks we address.
Section 4 describes our trade-off bound in the Empirical Risk Minimization (ERM) setting  and
shows how and when the additional  artiﬁcially generated views may yield a better generalization
performance in a supervised setting. Section 5 shows how to exploit these results when additional
unlabeled training data are available  in order to obtain a more accurate trade-off. Finally  section 6
describes experimental results that support this approach.

2 Framework and Deﬁnitions

In this section  we introduce basic deﬁnitions and the learning objectives that we address in our
setting of artiﬁcially generated representations.

2.1 Observed and Generated Views

def= (x1  ...  xV )  where different views xv provide a rep-
A multi-view observation is a sequence x
resentation of the same object in different sets Xv. A typical example is given in [3] where each
Web-page is represented either by its textual content (ﬁrst view) or by the anchor texts which point
to it (second view). In the setting of multilingual classiﬁcation  each view is the textual representa-
tion of a document written in a given language (e.g. English  German  French).

We consider binary classiﬁcation problems where  given a multi-view observation  some of the
views are not observed (we obviously require that at least one view is observed). This hap-
pens  for instance  when documents may be available in different languages  yet a given document
may only be available in a single language. Formally  our observations x belong to the input set
X def= (X1 ∪ {⊥}) × ... × (XV ∪ {⊥})  where xv =⊥ means that the v-th view is not observed.
In binary classiﬁcation  we assume that examples are pairs (x  y)  with y ∈ Y def= {0  1}  drawn
according to a ﬁxed  but unknown distribution D over X ×Y  such that P(x y)∼D (∀v : xv =⊥) = 0
(at least one view is available). In multilingual text classiﬁcation  a parallel corpus is a dataset where
all views are always observed (i.e. P(x y)∼D (∃v : xv =⊥) = 0)  while a comparable corpus is a
dataset where only one view is available for each example (i.e. P(x y)∼D (|{v : xv 6=⊥}| 6= 1) = 0).
For a given observation x  the views v such that xv 6=⊥ will be called the observed views. The
originality of our setting is that we assume that we have view generating functions Ψv→v′ : Xv →
Xv′ which take as input a given view xv and output an element of Xv′  that we assume is close
to what xv′ would be if it was observed. In our multilingual text classiﬁcation example  the view
generating functions are Machine Translation systems. These generating functions can then be used
to create surrogate observations  such that all views are available. For a given partially observed x 
the completed observation x is obtained as:

∀v  xv =(cid:26) xv

Ψv′→v(xv′

)

otherwise  where v′ is such that xv′

if xv 6=⊥

6=⊥

(1)

In this paper  we focus on the case where only one view is observed for each example. This setting
corresponds to the problem of learning from comparable corpora  which will be the focus of our
experiments. Our study extends to the situation where two or more views may be observed in a
straightforward manner. Our setting differs from previous multi-view learning studies [5] mainly on
the straightforward generalization to more than two views and the use of view generating functions
to induce the missing views from the observed ones.

2.2 Learning objective

The learning task we address is to ﬁnd  in some predeﬁned classiﬁer set C  the stochastic classiﬁer c
that minimizes the classiﬁcation error on multi-view examples (with  potentially  unobserved views)
drawn according to some distribution D as described above. Following the standard multi-view
framework  in which all views are observed [3  13]  we assume that we are given V deterministic
v=1  each working on one speciﬁc view1. That is  for each view v  Hv is a set
classiﬁer sets (Hv)V
of functions hv : Xv → {0  1}. The ﬁnal set of classiﬁers C contains stochastic classiﬁers  whose
output only depends on the outputs of the view-speciﬁc classiﬁers. That is  associated to a set of
classiﬁers C  there is a function ΦC : (Hv)V

v=1 × X → [0  1] such that:
C = {x 7→ ΦC(h1  ...  hV   x)|∀v  hv ∈ Hv }

For simplicity  in the rest of the paper  when the context is clear  the function x 7→ ΦC(h1  ...  hV   x)
will be denoted by ch1 ... hV . The overall objective of learning is therefore to ﬁnd c ∈ C with low
generalization error  deﬁned as:
(2)

ǫ(c) = E

e (c  (x  y))

(x y)∼D

where e is a pointwise error  for instance the 0/1 loss: e(c  (x  y)) = c(x)(1 − y) + (1 − c(x))y.
In the following sections  we address this learning task in our framework in terms of supervised and
semi-supervised learning.

3 Supervised Learning Tasks

We ﬁrst focus on the supervised learning case. We assume that we have a training set S of m
examples drawn i.i.d. according to a distribution D  as presented in the previous section. Depending
on how the generated views are used at both training and test stages  we consider the following
learning scenarios:

- Baseline: This setting corresponds to the case where each view-speciﬁc classiﬁer is trained using
the corresponding observed view on the training set  and prediction for a test example is
done using the view-speciﬁc classiﬁer corresponding to the observed view:

∀v  hv ∈ arg min

h∈Hv X(x y)∈S:xv6=⊥

e(h  (xv  y))

(3)

In this case we pose ∀x  cb
that this is the most basic way of learning a text classiﬁer from a comparable corpus.

h1 ... hV (x) = hv(xv)  where v is the observed view for x. Notice

- Generated Views as Additional Training Data: The most natural way to use the generated
views for learning is to use them as additional training material for the view-speciﬁc clas-
siﬁers:

∀v  hv ∈ arg min

h∈Hv X(x y)∈S

e(h  (xv  y))

(4)

with x deﬁned by eq. (1). Prediction is still done using the view-speciﬁc classiﬁers cor-
responding to the observed view  i.e. ∀x  cb
h1 ... hV (x) = hv(xv). Although the test set
distribution is a subdomain of the training set distribution [2]  this mismatch is (hopefully)
compensated by the addition of new examples.

- Multi-view Gibbs Classiﬁer: In order to avoid the potential bias introduced by the use of gener-
ated views only during training  we consider them also during testing. This becomes a stan-
dard multi-view setting  where generated views are used exactly as if they were observed.
The view-speciﬁc classiﬁers are trained exactly as above (eq. 4)  but the prediction is car-
ried out with respect to the probability distribution of classes  by estimating the probability
of class membership in class 1 from the mean prediction of each view-speciﬁc classiﬁer:

∀x  cmg

h1 ... hV (x) =

1
V

V

Xv=1

hv(xv)

(5)

1We assume deterministic view-speciﬁc classiﬁers for simplicity and with no loss of generality.

- Multi-view Majority Voting: With view generating functions involved in training and test  a nat-
ural way to obtain a (generally) deterministic classiﬁer with improved performance is to
take the majority vote associated with the Gibbs classiﬁer. The view-speciﬁc classiﬁers are
again trained as in eq. 4  but the ﬁnal prediction is done using a majority vote:

∀x  cmv

h1 ... hV (x) =( 1

2

I(cid:16)PV

v=1 hv(xv) > V

2(cid:17)

if PV

v=1 hv(xv) = V
2
otherwise

(6)

Where I(.) is the indicator function. The classiﬁer outputs either the majority voted class 
or either one of the classes with probability 1/2 in case of a tie.

4 The trade-offs with the ERM principle

We now analyze how the generated views can improve generalization performance. Essentially 
the trade-off is that generated views offer additional training material  therefore potentially helping
learning  but can also be of lower quality  which may degrade learning.

The following theorem sheds light on this trade-off by providing bounds on the baseline vs. multi-
view strategies. Note that such trade-offs have already been studied in the literature  although in
different settings (see e.g. [2  4]). Our ﬁrst result is the following theorem. The notion of func-
tion class capacity used here is the empirical Rademacher complexity [1]. Proof is given in the
supplementary material.

Theorem 1 Let D be a distribution over X × Y  satisfying P(x y)∼D (|{v : xv 6=⊥}| 6= 1) = 0.
Let S = ((xi  yi))m
i=1 be a dataset of m examples drawn i.i.d. according to D. Let e be the 0/1
v=1 be the view-speciﬁc deterministic classiﬁer sets. For each view v  denote
loss  and let (Hv)V
def= {(xv  y) 7→ e(h  (xv  y))|h ∈ Hv}  and denote   for any sequence S v ∈ (Xv × Y)mv of
e◦Hv
size mv  ˆRmv (e ◦ Hv S v) the empirical Rademacher complexity of e ◦ Hv on S v. Then  we have:
Baseline setting: for all 1 > δ > 0  with probability at least 1 − δ over S:

h′

ǫ(cb

h1 ... hV ) ≤ inf

)i + 2
v∈Hvhǫ(cb
Xv=1
where  for all v  S v def= {(xv
i   yi)|i = 1..m and xv
classiﬁer minimizing the empirical risk on S v.

1 ... h′

h′

V

V

mv
m

ˆRmv (e ◦ Hv S v) + 6r ln(2/δ)
i 6=⊥}  mv = |S v| and hv ∈ Hv is the

2m

Multi-view Gibbs classiﬁcation setting: for all 1 > δ > 0  with probability at least 1 − δ over S:

h′

ǫ(cmg

h1 ... hV ) ≤ inf

v∈Hvhǫ(cb
where  for all v  S v def= {(xv
empirical risk on S v  and

h′

1 ... h′

V

2
V

)i +

V

Xv=1

ˆRm(e ◦ Hv S v) + 6r ln(2/δ)

2m

+ η

i   yi)|i = 1..m}  hv ∈ Hv is the classiﬁer minimizing the

η = inf

v∈Hvhǫ(cmg

h′

h′

1 ... h′

V

)i − inf

v∈Hvhǫ(cb

h′

h′

1 ... h′

V

)i

(7)

This theorem gives us a rule for whether it is preferable to learn only with the observed views
(the baseline setting) or preferable to use the view-generating functions in the multi-view Gibbs
ˆRm(e ◦
classiﬁcation setting: we should use the former when 2Pv
Hv S v) + η  and the latter otherwise.
Let us ﬁrst explain the role of η (Eq. 7). The difference between the two settings is in the train
and test distributions for the view-speciﬁc classiﬁers. η compares the best achievable error for each
of the distribution. inf h′
without generated views)  with the automatically generated views  the best achievable error becomes
inf h′

)i is the best achievable error in the baseline setting (i.e.

ˆRmv (e ◦ Hv S v) < 2

V Pv

1 ... h′

mv
m

h′

V

v∈Hvhǫ(cb
)i.

v∈Hvhǫ(cmg

h′

1 ... h′

V

Therefore η measures the loss incurred by using the view generating functions.
situation  the quality of the generating functions will be sufﬁcient to make η small.
The terms depending on the complexity of the class of functions may be better explained using
orders of magnitude. Typically  the Rademacher complexity for a sample of size n is usually of
order O( 1√n ) [1].
Assuming  for simplicity  that all empirical Rademacher complexities in Theorem 1 are approxi-
mately equal to d/√n  where n is the size of the sample on which they are computed  and assuming
that mv = m/V for all v. The trade-off becomes:

In a favorable

Choose the Multi-view Gibbs classiﬁcation setting when: d(cid:16)q V

m − 1√m(cid:17) > η

This means that we expect important performance gains when the number of examples is small  the
generated views of sufﬁciently high quality for the given classiﬁcation task  and/or there are many
views available. Note that our theoretical framework does not take the quality of the MT system in a
standard way: in our setup  a good translation system is (roughly) one which generates bag-of-words
representations that allow to correctly discriminate between classes.

Majority voting One advantage of the multi-view setting at prediction time is that we can use a
majority voting scheme  as described in Section 2. In such a case  we expect that ǫ(cmv
) ≤
h′
ǫ(cmg
) if the view-speciﬁc classiﬁers are not correlated in their errors. It can not be guaranteed
h′
in general  though  since  in general  we can not prove any better than ǫ(cmv
)
h′
(see e.g. [9]).

) ≤ 2ǫ(cmg

1 ... h′

1 ... h′

1 ... h′

V

V

V

h′

1 ... h′

V

5 Agreement-Based Semi-Supervised Learning

One advantage of the multi-view settings described in the previous section is that unlabeled training
examples may naturally be taken into account in a semi–supervised learning scheme  using existing
approaches for multi-view learning (e.g. [3]).

In this section  we describe how  under the framework of [11]  the supervised learning trade-off
presented above can be improved using extra unlabeled examples. This framework is based on
the notion of disagreement between the various view-speciﬁc classiﬁers  deﬁned as the expected
variance of their outputs:

E

(x y)∼D


1

V Xv

hv(xv)2 − 1

V Xv

hv(xv)!2


(8)

V (h1  ...  hV )

def=

The overall idea is that a set of good view-speciﬁc classiﬁers should agree on their predictions 
making the expected variance small. This notion of disagreement has two key advantages. First  it
does not depend on the true class labels  making its estimation easy over a large  unlabeled training
set. The second advantage is that if  during training  it turns out that the view-speciﬁc classiﬁers
have a disagreement of at most µ on the unlabeled set  the set of possible view-speciﬁc classiﬁers
that needs be considered in the supervised learning stage is reduced to:

H∗v(µ) def= {h′v ∈ Hv |∀v′ 6= v ∃h′v′ ∈ Hv′   V(h′1  ...  h′V ) ≤ µ}

Thus  the more the various view-speciﬁc classiﬁers tend to agree  the smaller the possible set of
functions will be. This suggests a simple way to do semi-supervised learning: the unlabeled data
can be used to choose  among the classiﬁers minimizing the empirical risk on the labeled training
set  those with best generalization performance (by choosing the classiﬁers with highest agreement
on the unlabeled set). This is particularly interesting when the number of labeled examples is small 
as the train error is usually close to 0.
Theorem 3 of [11] provides a theoretical value B(ǫ  δ) for the minimum number of unlabeled ex-
amples required to estimate Eq. 8 with precision ǫ and probability 1 − δ (this bound depends on
{Hv}v=1..V ). The following result gives a tighter bound of the generalization error of the multi-view
Gibbs classiﬁer when unlabeled data are available. The proof is similar to Theorem 4 in [11].

Proposition 2 Let 0 ≤ µ ≤ 1 and 0 < δ < 1. Under the conditions and notations of Theorem
1  assume furthermore that we have access to u ≥ B(µ/2  δ/2) unlabeled examples drawn i.i.d.
according to the marginal distribution of D on X .
Then  with probability at
least 1 − δ 
arg minh∈HvP(xv  y)∈S v e(h  (xv  y)) have a disagreement

∈
less than µ/2 on the unlabeled

risk minimizers hv

set  we have:

empirical

the

if

ǫ(cmg

h1 ... hV ) ≤ inf

h′

v∈Hvhǫ(cb

h′

1 ... h′

V

2
V

)i +

V

Xv=1

ˆRm(e ◦ H∗v(µ) S v) + 6r ln(4/δ)

2m

+ η

becomes:

We can now rewrite the trade-off between the baseline setting and the multi-view Gibbs classiﬁer 
taking semi-supervised learning into account. Using orders of magnitude  and assuming that for

each view  ˆRm(e ◦ H∗v(µ) S v) is O(du/√m)  with the proportional factor du ≪ d  the trade-off
Choose the mutli-view Gibbs classiﬁcation setting when: dpV /m − du/√m > η.

Thus  the improvement is even more important than in the supervised setting. Also note that the
more views we have  the greater the reduction in classiﬁer set complexity should be.

Notice that this semi-supervised learning principle enforces agreement between the view speciﬁc
classiﬁers. In the extreme case where they almost always give the same output  majority voting is
then nearly equivalent to the Gibbs classiﬁer (when all voters agree  any vote is equal to the majority
vote). We therefore expect the majority vote and the Gibbs classiﬁer to yield similar performance in
the semi-supervised setting.

6 Experimental Results

In our experiments  we address the problem of learning document classiﬁers from a comparable
corpus. We build the comparable corpus by sampling parts of the Reuters RCV1 and RCV2 collec-
tions [12  14]. We used newswire articles written in 5 languages  English  French  German 
Italian and Spanish. We focused on 6 relatively populous classes: C15  CCAT  E21  ECAT 
GCAT  M11.

For each language and each class  we sampled up to 5000 documents from the RCV1 (for English)
or RCV2 (for other languages). Documents belonging to more than one of our 6 classes were as-
signed the label of their smallest class. This resulted in 12-30K documents per language  and 11-34K
documents per class (see Table 1). In addition  we reserved a test split containing 20% of the doc-
uments (respecting class and language proportions) for testing. For each document  we indexed
the text appearing in the title (headline tag)  and the body (body tags) of each article. As prepro-
cessing  we lowercased  mapped digits to a single digit token  and removed non alphanumeric
tokens. We also ﬁltered out function words using a stop-list  as well as tokens occurring in less than
5 documents.
Documents were then represented as a bag of words  using a TFIDF-based weighting scheme. The
ﬁnal vocabulary size for each language is given in table 1. The artiﬁcial views were produced using

Table 1: Distribution of documents over languages and classes in the comparable corpus.

Language
English
French
German
Italian
Spanish

Total

# docs
18  758
26  648
29  953
24  039
12  342
111  740

(%)
16.78
23.45
26.80
21.51
11.46

# tokens
21  531
24  893
34  279
15  506
11  547

Class
C15
CCAT
E21
ECAT
GCAT
M11

Size (all lang.)

18  816
21  426
13  701
19  198
19  178
19  421

(%)
16.84
19.17
12.26
17.18
17.16
17.39

PORTAGE  a statistical machine translation system developed at NRC [15]. Each document from
the comparable corpus was thus translated to the other 4 languages.2
For each class  we set up a binary classiﬁcation task by using all documents from that class as
positive examples  and all others as negative. We ﬁrst present experimental results obtained in
supervised learning  using various amounts of labeled examples. We rely on linear SVM models as
base classiﬁers  using the SVM-Perf package [8]. For comparisons  we employed the four learning
strategies described in section 3: 1− the single-view baseline svb (Eq. 3)  2− generated views as
additional training data gvb (Eq. 4)  3− multi-view Gibbs mvg (Eq. 5)  and 4− multi-view majority
voting mvm (Eq. 6). Recall that the second setting  gvb  is the most straightforward way to train and
test classiﬁers when additional examples are available (or generated) from different sources. It can
thus be seen as a baseline approach  as opposed to the last two strategies (mvg and mvm)  where
view-speciﬁc classiﬁers are both trained and tested over both original and translated documents.
Note also that in our case (V = 5 views)  additional training examples obtained from machine
translation represent 4 times as many labeled examples as the original texts used to train the baseline
svb. All test results were averaged over 10 randomly sampled training sets.

Table 2: Test classiﬁcation accuracy and F1 in the supervised setting  for both baselines (svb  gvb) 
Gibbs (mvg) and majority voting (mvw) strategies  averaged over 10 random sets of 10 labeled
examples per view. ↓ indicates statistically signiﬁcantly worse performance that the best result 
according to a Wilcoxon rank sum test (p < 0.01) [10].
Strategy

GCAT

M11

C15

E21

CCAT
F1

Acc.

Acc.
.559↓
.705
.693↓
.716

Acc.

F1

F1
.388↓ .639↓ .403↓ .557↓ .294↓
.474↓ .691↓ .464↓ .665↓ .351↓
.494↓ .681↓ .445↓ .665↓ .375↓
.521
.405

.708

.478

.693

svb
gvb
mvg
mvm

ECAT
F1

Acc.
.579↓ .374↓
.623↓ .424↓
.620↓ .420↓
.636
.441

Acc.
F1
.800↓ .501↓
.835↓ .595↓
.834↓ .594↓
.860
.642

F1

Acc.
.651↓ .483↓
.786↓ .589↓
.787↓ .600↓
.820
.644

Results obtained in a supervised setting with only 10 labeled documents per language for training are
summarized in table 2. All learning strategies using the generated views during training outperform
the single-view baseline. This shows that  although imperfect  artiﬁcial views do bring additional
information that compensates the lack of labeled data. Although the multi-view Gibbs classiﬁer
predicts based on a translation rather than the original in 80% of cases  it produces almost identical
performance to the gvb run (which only predicts using the original text). These results indicate that
the translation produced by our MT system is of sufﬁcient quality for indexing and classiﬁcation
purposes. Multi-view majority voting reaches the best performance  yielding a 6 − 17% improve-
ment in accuracy over the baseline. A similar increase in performance is observed using F1  which
suggests that the multi-view SVM appropriately handles unbalanced classes.

Figure 1 shows the learning curves obtained on 3 classes  C15  ECAT and M11. These ﬁgures show
that when there are enough labeled examples (around 500 for these 3 classes)  the artiﬁcial views do
not provide any additional useful information over the original-language examples. These empirical
results illustrate the trade-off discussed at the previous section. When there are sufﬁcient original
labeled examples  additional generated views do not provide more useful information for learning
than what view-speciﬁc classiﬁers have available already.

We now investigate the use of unlabeled training examples for learning the view-speciﬁc classiﬁers.
Our overall aim is to illustrate our ﬁndings from section 5. Recall that in the case where view-speciﬁc
classiﬁers are in agreement over the class labels of a large number of unlabeled examples  the multi-
view Gibbs and majority vote strategies should have the same performance. In order to enforce
agreement between classiﬁers on the unlabeled set  we use a variant of the iterative co-training
algorithm [3]. Given the view-speciﬁc classiﬁers trained on an initial set of labeled examples  we
iteratively assign pseudo-labels to the unlabeled examples for which all classiﬁer predictions agree.
We then train new view-speciﬁc classiﬁers on the joint set of the original labeled examples  and those
unanimously (pseudo-)labeled ones. Key differences between this algorithm and co-training are the
number of views used for learning (5 instead of 2)  and the use of unanimous and simultaneous
labeling.

2The dataset is available from http://multilingreuters.iit.nrc.ca/ReutersMultiLingualMultiView.htm

C15

ECAT

M11

1
F

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

 
 
 

10

20

50

100

200

Labeled training size

mvm
mvg
svb

500

1
F

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

 
 
 

10

20

50

100

200

Labeled training size

mvm
mvg
svb

500

1
F

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

 
 
 

10

20

50

100

200

Labeled training size

mvm
mvg
svb

500

Figure 1: F1 vs. size of the labeled training set for classes C15  ECAT and M11.

We call this iterative process self-learning multiple-view algorithm  as it also bears a similarity with
the self-training paradigm [16]. Prediction from the multi-view SVM models obtained from this
m).
self-learning multiple-view algorithm is done either using Gibbs (mvs
These results are shown in table 3. For comparison we also trained a TSVM model [7] on each view
separately  a semi-supervised equivalent to the single-view baseline strategy. Note that the TSVM
model mostly out-performs the supervised baseline svb  although the F1 suffers on some classes.
This suggests that the TSVM has trouble handling unbalanced classes in this setting.

g) or majority voting (mvs

Table 3: Test classiﬁcation accuracy and F1 in the semi-supervised setting  for single-view TSVM
m)  averaged over 10
and multi-view self-learning using either Gibbs (mvs
random sets using 10 labeled examples per view to start. For comparison we provide the single-view
baseline and multi-view majority voting performance for supervised learning.

g) or majority voting (mvs

Strategy

svb
mvm
TSVM
mvs
g
mvs
m

C15

CCAT

E21

Acc.
.559↓
.716↓
.721↓
.772
.773

F1

.388↓
.521↓
.482↓
.586
.589

Acc.
.639↓
.708↓
.721↓
.762
.766

F1

.403↓
.478↓
.405↓
.538
.545

Acc.
.557↓
.693↓
.746↓
.765
.767

F1

.294↓
.405↓
.269↓
.470
.473

ECAT
F1

Acc.
.579↓ .374↓
.636↓ .441↓
.665↓ .263↓
.691 .504
.701
.508

GCAT

F1

Acc.
.800↓ .501↓
.860↓ .642↓
.876↓ .606↓
.903 .729
.905
.734

M11

Acc.
F1
.651↓ .483↓
.820↓ .644↓
.834↓ .706↓
.900 .764
.901
.766

The multi-view self-learning algorithm achieves the best classiﬁcation performance in both accuracy
and F1  and signiﬁcantly outperforms both the TSVM and the supervised multi-view strategy in all
classes. As expected  the performance of both mvs

m strategies are similar.

g and mvs

7 Conclusion

The contributions of this paper are twofold. First  we proposed a bound on the risk of the Gibbs
classiﬁer trained over artiﬁcially completed multi-view observations  which directly corresponds to
our target application of learning text classiﬁers from a comparable corpus. We showed that our
bound may lead to a trade-off between the size of the training set  the number of views  and the
quality of the view generating functions. Our result identiﬁes in which case it is advantageous to
learn with additional artiﬁcial views  as opposed to sticking with the baseline setting in which a clas-
siﬁer is trained over single view observations. This result leads to our second contribution  which is
a natural way of using unlabeled data in semi-supervised multi-view learning. We showed that in the
case where view-speciﬁc classiﬁers agree over the class labels of additional unlabeled training data 
the previous trade-off becomes even much tighter. Empirical results on a comparable multilingual
corpus support our ﬁndings by showing that additional views obtained using a Machine Translation
system may signiﬁcantly increase classiﬁcation performance in the most interesting situation  when
there are few labeled data available for training.

Acknowlegdements This work was supported in part by the IST Program of the European Com-
munity  under the PASCAL2 Network of Excellence  IST-2002-506778.

References

[1] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: risk bounds and

structural results. Journal of Machine Learning Research  3:463–482  2003.

[2] J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman. Learning bounds for domain

adaptation. In NIPS  2007.

[3] A. Blum and T. M. Mitchell. Combining labeled and unlabeled sata with co-training. In COLT 

pages 92–100  1998.

[4] K. Crammer  M. Kearns  and J. Wortman. Learning from multiple sources. Journal of Machine

Learning Research  9:1757–1774  2008.

[5] J. D. R. Farquhar  D. Hardoon  H. Meng  J. Shawe-Taylor  and S. Szedmak. Two view learning:
Svm-2k  theory and practice. In Advances in Neural Information Processing Systems 18  pages
355–362. 2006.

[6] D. R. Hardoon  G. Leen  S. Kaski  and J. S.-T. (eds). Nips workshop on learning from multiple

sources. 2008.

[7] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In

ICML  pages 200–209  1999.

[8] T. Joachims. Training linear svms in linear time. In Proceedings of the ACM Conference on

Knowledge Discovery and Data Mining (KDD)  pages 217–226  2006.

[9] J. Langford and J. Shawe-taylor. Pac-bayes & margins. In NIPS 15  pages 439–446  2002.
[10] E. Lehmann. Nonparametric Statistical Methods Based on Ranks. McGraw-Hill  New York 

1975.

[11] B. Leskes. The value of agreement  a new boosting algorithm. In COLT  pages 95–110  2005.
[12] D. D. Lewis  Y. Yang  T. Rose  and F. Li. RCV1: A new benchmark collection for text catego-

rization research. Journal of Machine Learning Research  5:361–397  2004.

[13] I. Muslea. Active learning with multiple views. PhD thesis  USC  2002.
[14] Reuters. Corpus  volume 2  multilingual corpus  1996-08-20 to 1997-08-19. 2005.
[15] N. Uefﬁng  M. Simard  S. Larkin  and J. H. Johnson. NRC’s PORTAGE system for WMT. In

In ACL-2007 Second Workshop on SMT  pages 185–188  2007.

[16] X. Zhu. Semi-supervised learning literature survey. Technical report  Univ. Wisconsis  2007.

,Simon Du
Chi Jin
Jason Lee
Michael Jordan
Aarti Singh
Barnabas Poczos