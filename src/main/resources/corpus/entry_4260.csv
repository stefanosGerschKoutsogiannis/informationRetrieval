2016,PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions,We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks  a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization  we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally  we show that perforation is complementary to the recently proposed acceleration method of Zhang et al.,PerforatedCNNs:AccelerationthroughEliminationofRedundantConvolutionsMichaelFigurnov1 2 AijanIbraimova4 DmitryVetrov1 3 andPushmeetKohli51NationalResearchUniversityHigherSchoolofEconomics2LomonosovMoscowStateUniversity3Yandex4SkolkovoInstituteofScienceandTechnology5MicrosoftResearchmichael@figurnov.ru aijan.ibraimova@gmail.com vetrovd@yandex.ru pkohli@microsoft.comAbstractWeproposeanovelapproachtoreducethecomputationalcostofevaluationofconvolutionalneuralnetworks afactorthathashinderedtheirdeploymentinlow-powerdevicessuchasmobilephones.Inspiredbytheloopperforationtechniquefromsourcecodeoptimization wespeedupthebottleneckconvolutionallayersbyskippingtheirevaluationinsomeofthespatialpositions.Weproposeandanalyzeseveralstrategiesofchoosingthesepositions.WedemonstratethatperforationcanacceleratemodernconvolutionalnetworkssuchasAlexNetandVGG-16byafactorof2×-4×.Additionally weshowthatperforationiscomplementarytotherecentlyproposedaccelerationmethodofZhangetal.[28].1IntroductionThelastfewyearshaveseenconvolutionalneuralnetworks(CNNs)emergeasanindispensabletoolforcomputervision.However modernCNNshaveahighcomputationalcostofevaluation withconvolutionallayersusuallytakingupover80%ofthetime.Forinstance VGG-16network[25]fortheproblemofobjectrecognitionrequires1.5·1010ﬂoatingpointmultiplicationsperimage.ThesecomputationalrequirementshinderthedeploymentofsuchnetworksonsystemswithoutGPUsandinscenarioswherepowerconsumptionisamajorconcern suchasmobiledevices.Theproblemoftradingaccuracyofcomputationsforspeediswell-knownwithinthesoftwareengineeringcommunity.Oneofthemostprominentmethodsforthisproblemisloopperforation[18 19 24].Inanutshell thistechniqueisolatesloopsinthecodethatarenotcriticalfortheexecution andthenreducestheircomputationalcostbyskippingsomeiterations.Morerecently researchershaveconsideredproblem-dependentperforationstrategiesthatexploitthestructureoftheproblem[23].Inspiredbythegeneralprincipleofperforation weproposetoreducethecomputationalcostofCNNevaluationbyexploitingthespatialredundancyofthenetwork.ModernCNNs suchasAlexNet exploitthisredundancythroughtheuseofstridesintheconvolutionallayers.However usingtheconvolutionalstrideschangesthearchitectureofthenetwork(intermediaterepresentationssizeandthenumberofweightsintheﬁrstfully-connectedlayer) whichmightbeundesirable.Insteadofusingstrides wearguefortheuseofinterpolation(perforation)ofresponsesintheconvolutionallayer.Akeyelementofthisapproachisthechoiceoftheperforationmask whichdeﬁnestheoutputpositionstoevaluateexactly.Weproposeseveralapproachestoselecttheperforationmasksandamethodofchoosingacombinationofperforationmasksfordifferentlayers.Torestorethenetworkaccuracy weperformﬁne-tuningoftheperforatednetwork.OurexperimentsshowthatthismethodcanreducetheevaluationtimeofmodernCNNarchitecturesproposedintheliteraturebyafactorof2×-4×withasmalldecreaseinaccuracy.2RelatedWorkReducingthecomputationalcostofCNNevaluationisanactiveareaofresearch withbothhighlyoptimizedimplementationsandapproximatemethodsinvestigated.30thConferenceonNeuralInformationProcessingSystems(NIPS2016) Barcelona Spain.𝑑"𝑆𝑋%𝑌′tensorUdata matrix M𝑌𝑋𝑑𝑑𝑆𝑆im2rowkernel KtensorV×=𝑇𝑌′X′𝑇𝑑"𝑆𝑇11Figure1:Reductionofconvolutionallayerevaluationtomatrixmultiplication.Ourideaistoleaveonlyasubsetofrows(deﬁnedbyaperforationmask)inthedatamatrixMandtointerpolatethemissingoutputvalues.ImplementationsthatexploittheparallelismavailableincomputationalarchitectureslikeGPUs(cuda-convnet2[13] CuDNN[3])haveallowedtosigniﬁcantlyreducetheevaluationtimeofCNNs.SinceCuDNNinternallyreducesthecomputationofconvolutionallayerstothematrix-by-matrixmultiplication(withoutexplicitlymaterializingthedatamatrix) ourapproachcanpotentiallybeincorporatedintothislibrary.Inasimilarvein theuseofFPFGAs[22]leadstobettertrade-offsbetweenspeedandpowerconsumption.Severalpapers[5 9]showedthatCNNsmaybeefﬁcientlyevaluatedusinglowprecisionarithmetic whichisimportantforFPFGAimplementations.MostapproximatemethodsofdecreasingtheCNNcomputationalcostexploittheredundanciesoftheconvolutionalkernelusinglow-ranktensordecompositions[6 10 16 28].Inmostcases aconvolutionallayerisreplacedbyseveralconvolutionallayersappliedsequentially whichhaveamuchlowertotalcomputationalcost.WeshowthatthecombinationofperforationwiththemethodofZhangetal.[28]improvesuponbothapproaches.Forspatiallysparseinputs itispossibletoexploitthissparsitytospeedupevaluationandtraining[8].Whilethisapproachissimilartooursinthespirit wedonotrelyonspatiallysparseinputs.Instead wesparselysampletheoutputsofaconvolutionallayerandinterpolatetheremainingvalues.Inarecentwork LebedevandLempitsky[15]alsodecreasetheCNNcomputationalcostbyreducingthesizeofthedatamatrix.Thedifferenceisthattheirapproachreducestheconvolutionalkernel’ssupportwhileourapproachdecreasesthenumberofspatialpositionsinwhichtheconvolutionsareevaluated.Thetwomethodsarecomplementary.Severalpapershavedemonstratedthatitispossibletocompresstheparametersofthefully-connectedlayers(wheremostCNNparametersreside)withamarginalerrorincrease[4 21 27].Sinceourmethoddoesnotdirectlymodifythefully-connectedlayers itispossibletocombinethesemethodswithourapproachandobtainafastandsmallCNN.3PerforatedCNNsThesectionprovidesadetaileddescriptionofourapproach.Beforeproceedingfurther weintroducethenotationthatwillbeusedintherestofthepaper.Notation.AconvolutionallayertakesasinputatensorUofsizeX×Y×SandoutputsatensorVofsizeX0×Y0×T X0=X−d+1 Y0=Y−d+1.Theﬁrsttwodimensionsarespatial(heightandwidth) andthethirddimensionisthenumberofchannels(forexample foranRGBinputimageS=3).ThesetofTconvolutionkernelsKisgivenbyatensorofsized×d×S×T.Forsimplicityofnotation weassumeunitstride nozero-paddingandskipthebiases.Theconvolutionallayeroutputmaybedeﬁnedasfollows:V(x y t)=dXi=1dXj=1SXs=1K(i j s t)U(x+i−1 y+j−1 s)(1)Additionally wedeﬁnethesetofallspatialindices(positions)oftheoutputΩ={1 ... X0}×{1 ... Y0}.PerforationmaskI⊆Ωisthesetofindicesinwhichtheoutputsarecalculatedexactly.DenoteN=|I|thenumberofpositionstobecalculatedexactly andr=1−N|Ω|theperforationrate.Reductiontomatrixmultiplication.Toachievehighcomputationalperformance manydeeplearn-ingframeworks includingCaffe[12]andMatConvNet[26] reducethecomputationofconvolutional2layerstotheheavily-optimizedmatrix-by-matrixmultiplicationroutineofbasiclinearalgebrapack-ages.Thisprocess sometimesreferredtoaslowering isillustratedinﬁg.1.First adatamatrixMofsizeX0Y0×d2Sisconstructedusingim2rowfunction.TherowsofMareelementsofpatchesofinputtensorUofsized×d×S.Then MismultipliedbythekerneltensorKreshapedintosized2S×T.TheresultingmatrixofsizeX0Y0×TistheoutputtensorV uptoareshape.Foramoredetailedexposition see[26].3.1PerforatedconvolutionallayerInthissectionwepresenttheperforatedconvolutionallayer.Inasmallfractionofspatialpositions theoutputsoftheproposedlayerareequaltotheoutputsofausualconvolutionallayer.Theremainingvaluesareinterpolatedusingthenearestneighborfromthissetofpositions.WeevaluateotherinterpolationstrategiesinappendixA.Theperforatedconvolutionallayerisageneralizationofthestandardconvolutionallayer.Whentheperforationmaskisequaltoalltheoutputspatialpositions theperforatedconvolutionallayer’soutputequalstheconventionalconvolutionallayer’soutput.Formally letI⊆Ωbetheperforationmaskofspatialoutputtobecalculatedexactly(theconstraintthatthemasksaresharedforallchannelsoftheoutputisrequiredforthereductiontomatrixmultiplication).Thefunction‘(x y):Ω→IreturnstheindexofthenearestneighborinIaccordingtoEuclideandistance(withtiesbrokenrandomly):‘(x y)=(‘1(x y) ‘2(x y))=argmin(x0 y0)∈Ip(x−x0)2+(y−y0)2.(2)Notethatthefunction‘(x y)maybecalculatedinadvanceandcached.TheperforatedconvolutionallayeroutputˆVisdeﬁnedasfollows:ˆV(x y t)=V(‘1(x y) ‘2(x y) t) (3)whereV(x y t)istheoutputoftheusualconvolutionallayer deﬁnedby(1).Since‘(x y)=(x y)for(x y)∈I theoutputsinthespatialpositionsIarecalculatedexactly.Thevaluesinotherpositionsareinterpolatedusingthevalueofthenearestneighbor.Toevaluateaperforatedconvolutionallayer weonlyneedtocalculatethevaluesV(x y t)for(x y)∈I whichcanbedoneefﬁcientlybyreductiontomatrixmultiplication.Inthiscase thedatamatrixMcontainsjustN=|I|rows insteadoftheoriginalX0Y0=|Ω|rows.Perforationisnotlimitedtothisimplementationofaconvolutionallayer andcanbecombinedwithotherimplementationsthatsupportstridedconvolutions suchasthedirectconvolutionapproachofcuda-convnet2[13].Inourimplementation weonlystoretheoutputvaluesV(x y t)for(x y)∈I.Theinterpolationisperformedimplicitlybymaskingthereadsofthefollowingpoolingorconvolutionallayer.Forexample whenacceleratingconv3layerofAlexNet theinterpolationcostistransferredtoconv4layer.Weobservenoslowdownoftheconv4layerwhenusingGPU anda0-3%slowdownwhenusingCPU.Thisdesignchoicehasseveraladvantages.Firstly thememorysizerequiredtostoretheactivationsisreducedbyafactorof11−r.Secondly thefollowingnon-linearitylayersand1×1convolutionallayersarealsospedupsincetheyareappliedtoasmallernumberofelements.3.2PerforationmasksWeproposeseveralwaysofgeneratingtheperforationmasks orchoosingNpointsfromΩ.WevisualizetheperforationmasksIasbinarymatriceswithblacksquaresinthepositionsofthesetI.Weonlyconsidertheperforationmasksthatareindependentoftheinputobjectandleaveexplorationofinput-dependentperforationmaskstothefuturework.UniformperforationmaskisjustNpointschosenrandomlywithoutreplacementfromthesetΩ.However ascanbeseenfromﬁg.2a forN(cid:28)|Ω| thepointstendtocluster.ThisisundesirablebecauseamorescatteredsetIwouldreducetheaveragedistancetothesetI.GridperforationmaskisasetofpointsI={a(1) ... a(Kx)}×{b(1) ... b(Ky)} seeﬁg.2b.Wechoosethevaluesofa(i) b(i)usingthepseudorandomintegersequencegenerationschemeof[7].Poolingstructuremaskexploitsthestructureoftheoverlapsofpoolingoperators.DenotebyA(x y)thenumberoftimesanoutputoftheconvolutionallayerisusedinthepoolingoperators.Thegrid-likepatternasinﬁg.2discausedbyapoolingofsize3×3withstride2(suchparametersareusede.g.inNetworkinNetworkandAlexNet).Thepoolingstructuremaskisobtainedbypickingtop-NpositionswiththehighestvaluesofA(x y) withtiesbrokenrandomly seeﬁg.2c.3(a)Uniform(b)Grid(c)Poolingstruc-ture1234(d)WeightsA(x y)Figure2:Perforationmasks AlexNetconv2 r=80.25%.Bestviewedincolor.00.010.020.030.040.050.060.070.080.050.10.150.20.250.300.050.10.150.20.250.30.010.020.030.040.050.060.070.080.09(a)B(x y) origi-nalnetwork00.050.10.150.20.250.30.350.4(b)B(x y) perfo-ratednetwork(c)Impactmask r=90%Figure3:Top:ImageNetimagesandcorrespondingvaluesofimpactG(x y;V)forAlexNetconv2.Bottom:averageimpactsandimpactperforationmaskforAlexNetconv2.Bestviewedincolor.ImpactmaskestimatestheimpactofperforationofeachpositionontheCNNlossfunction andthenremovestheleastimportantpositions.DenotebyL(V)thelossfunctionoftheCNN(suchasnegativelog-likelihood)asafunctionoftheconsideredconvolutionallayeroutputsV.Next supposeV0isobtainedfromVbyreplacingoneelement(x0 y0 t0)withaneutralvaluezero.Weestimatetheimpactofapositionasaﬁrst-orderTaylorapproximationofthemagnitudeofchangeofL(V):|L(V0)−L(V)|≈(cid:12)(cid:12)(cid:12)XXx=1YXy=1TXt=1∂L(V)∂V(x y t)(V0(x y t)−V(x y t))(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)∂L(V)∂V(x0 y0 t0)V(x0 y0 t0)(cid:12)(cid:12)(cid:12).(4)Thevalue∂L(V)∂V(x0 y0 t0)maybeobtainedusingbackpropagation.Inthecaseofaperforatedconvolu-tionallayer wecalculatethederivativeswithrespecttotheconvolutionallayeroutputV(nottheinterpolatedoutputˆV).Thismakestheimpactofthepreviouslyperforatedpositionszeroandsumstheimpactofthenon-perforatedpositionsoveralltheoutputswhichsharethevalue.Sinceweareinterestedinthetotalimpactofaspatialposition(x y)∈Ω wetakeasumoverallthechannelsandaveragethisestimateofimpactsoverthetrainingdataset:G(x y;V)=TXt=1(cid:12)(cid:12)(cid:12)∂L(V)∂V(x y t)V(x y t)(cid:12)(cid:12)(cid:12)(5)B(x y)=EV∼trainingsetG(x y;V)(6)Finally theimpactmaskisformedbytakingthetop-NpositionswiththehighestvaluesofB(x y).ExamplesofthevaluesofG(x y;V) B(x y)andimpactmaskareshownonﬁg.3.NotethattheregionsofthehighvalueofG(x y;V)usuallycontainthemostsalientfeaturesoftheimage.TheaveragedweightsB(x y)tendtobehigherinthecentersinceImageNet’simagesusuallycontainacenteredobject.Additionally agrid-likestructureofpoolingstructuremaskisautomaticallyinferred.4NetworkDatasetErrorCPUtimeGPUtimeMem.Mult.#convNINCIFAR-10top-110.4%4.6ms0.8ms5.1MB2.2·1083AlexNetImageNettop-519.6%16.7ms2.0ms6.6MB0.5·1095VGG-16top-510.1%300ms29ms110MB1.5·101013Table1:DetailsoftheCNNsusedfortheexperimentalevaluation.Timings memoryconsumptionandnumberofmultiplicationsarenormalizedbythebatchsize.Memoryconsumptionisthememoryrequiredtostoreactivations(intermediateresults)ofthenetworkduringtheforwardpass.CPU speedup (times)123456Top-5 error increase (%)0246810UniformGridPooling structureImpact(a)conv2 CPUGPU speedup (times)123456Top-5 error increase (%)0246810(b)conv2 GPUCPU speedup (times)123456Top-5 error increase (%)0246810UniformGridImpact(c)conv3 CPUGPU speedup (times)123456Top-5 error increase (%)0246810(d)conv3 GPUFigure4:AccelerationofasinglelayerofAlexNetfordifferentmasktypeswithoutﬁne-tuning.Valuesareaveragedover5runs.Sinceperforationofalayerchangestheimpactsofallthelayers intheexperimentsweiteratebetweenincreasingtheperforationrateofalayerandrecalculationofimpacts.Weﬁndthatthisimprovesresultsbyco-adaptingtheperforationmasksofdifferentconvolutionallayers.3.3ChoosingtheperforationconﬁgurationsForwholenetworkacceleration itisimportanttoﬁndacombinationofper-layerperforationratesthatwouldachievehighspeedupwithlowerrorincrease.Todothis weemployasimplegreedystrategy.Weuseasingleperforationmasktypeandaﬁxedrangeofincreasingperforationrates.Denotebyttheevaluationtimeoftheacceleratednetworkandbyetheobjective(weusenegativelog-likelihoodforasubsetoftrainingimages).Lett0ande0betherespectivevaluesforthenon-acceleratednetwork.Ateachiteration wetrytoincreasetheperforationrateforeachlayerandchoosethelayerforwhichthisresultsintheminimalvalueofthecostfunctione−e0t0−t.4ExperimentsWeusethreeconvolutionalneuralnetworksofincreasingsizeandcomputationalcomplexity:Net-workinNetwork[17] AlexNet[14]andVGG-16[25] seetable1.Inallnetworks weattempttoperforatealltheconvolutionallayers exceptforthe1×1convolutionallayersofNIN.Weperformtimingsonacomputerwithaquad-coreIntelCorei5-4460CPU 16GBRAMandanVidiaGeforceGTX980GPU.Thebatchsizeusedfortimingsis128forNIN 256forAlexNetand16forVGG-16.ThenetworksareobtainedfromCaffeModelZoo.ForAlexNet theCaffereimplementationisusedwhichisslightlydifferentfromtheoriginalarchitecture(poolingandnormalizationlayersareswapped).WeuseaforkofMatConvNetframeworkforallexperi-ments exceptforﬁne-tuningofAlexNetandVGG-16 forwhichweuseaforkofCaffe.Thesourcecodeisavailableathttps://github.com/mfigurnov/perforated-cnn-matconvnet https://github.com/mfigurnov/perforated-cnn-caffe.Webeginourexperimentsbycomparingtheproposedperforationmasksinacommonbenchmarksetting:accelerationofasingleAlexNetlayer.Then wecomparewhole-networkaccelerationwiththebest-performingmaskstobaselinessuchasdecreaseofinputimagessizeandanincreaseofstrides.Weproceedtoshowthatperforationscalestolargenetworksbypresentingthewhole-networkaccelerationresultsforAlexNetandVGG-16.Finally wedemonstratethatperforationiscomplementarytotherecentlyproposedaccelerationmethodofZhangetal.[28].5MethodCPUtime↓Error↑(%)Impact r=34 3×3ﬁlters9.1×+1Impact r=565.3×+1.4Impact r=454.2×+0.9LebedevandLempitsky[15]20×top-1+1.1LebedevandLempitsky[15]9×top-1+0.3Jaderbergetal.[10]6.6×+1Lebedevetal.[16]4.5×+1Dentonetal.[6]2.7×+1Table2:AccelerationofAlexNet’sconv2.Top:ourresultsafterﬁne-tuning bottom:previouslypublishedresults.Resultof[10]providedby[16].Theexperimentwithreducedspatialsizeofthekernel(3×3 insteadof5×5)suggeststhatperforationiscomplementarytothe“braindamage”methodof[15]whichalsoreducesthespatialsupportofthekernel.4.1SinglelayerresultsWeexplorethespeedup-errortrade-offoftheproposedperforationmasksonthetwobottleneckconvolutionallayersofAlexNet conv2andconv3 seeﬁg.4.Thepoolingstructureperforationmaskisonlyapplicabletotheconv2becauseitisdirectlyfollowedbyamax-pooling whereastheconv3isfollowedbyanotherconvolutionallayer.Weseethatimpactperforationmaskworksbestfortheconv2layerwhilegridmaskperformsverywellforconv3.Thestandarddeviationofresultsissmallforalltheperforationmasks excepttheuniformmaskforhighspeedups(wherethegridmaskoutperformsit).TheresultsaresimilarforbothCPUandGPU showingtheapplicabilityofourmethodforbothplatforms.Notethatifweconsiderthebestperforationmaskforeachspeedupvalue thenweseethattheconv2layeriseasiertoacceleratethantheconv3layer.Weobservethispatterninotherexperiments:layersimmediatelyfollowedbyamax-poolingareeasiertoacceleratethanthelayersfollowedbyaconvolutionallayer.AdditionalresultsforNINnetworkarepresentedinappendixB.Wecompareourresultsafterﬁne-tuningtothepreviouslypublishedresultsontheaccelerationofAlexNet’sconv2intable2.Motivatedbytheresultsof[15]thatthespatialsupportofconv2convolutionalkernelmaybereducedwithasmallerrorincrease wereducethekernel’sspatialsizefrom5×5to3×3andapplytheimpactperforationmask.Thisleadstothe9.1×accelerationfor1%top-5errorincrease.Usingthemoresophisticatedmethodof[15]toreducethespatialsupportmayleadtofurtherimprovements.4.2BaselinesWecomparePerforatedCNNswiththebaselinemethodsofdecreasingthecomputationalcostofCNNsbyexploitingthespatialredundancy.Unlikeperforation thesemethodsdecreasethesizeoftheactivations(intermediateoutputs)oftheCNN.Foranetworkwithfully-connected(FC)layers thiswouldchangethenumberofCNNparametersintheﬁrstFClayer effectivelymodifyingthearchitecture.Toavoidthis weuseCIFAR-10NINnetwork whichreplacesFClayerswithglobalaveragepooling(mean-poolingoverallspatialpositionsinthelastlayer).Weconsiderthefollowingbaselinemethods.Resize.Theinputimageisdownscaledwiththeaspectratiopreserved.Stride.Thestridesoftheconvolutionallayersareincreased makingtheactivationsspatiallysmaller.Fractionalstride.Motivatedbyfractionalmax-pooling[7] weintroduceamoreﬂexiblemodiﬁcationofstrideswhichevaluatesconvolutionsonanon-regulargrid(withavaryingstepsize) providingamoreﬁne-grainedcontrolovertheactivationssizeandspeedup.Weusegridperforationmaskgenerationschemetochoosetheoutputpositionstoevaluate.Wecomparethesestrategiestoperforationofallthelayerswiththetwotypesofmaskswhichperformedbestintheprevioussection:gridandimpact.Notethat“grid”is infact equivalenttofractionalstrides butwithmissingvaluesbeinginterpolated.Allthemethods exceptresize requireaparametervalueperconvolutionallayer leadingtoalargenumberofpossibleconﬁgurations.Weusetheoriginalnetworktoexplorethisspaceofconﬁgurations.Forimpact weusethegreedyalgorithm.Forstride weevaluateallpossiblecombinationsofparameters.Forgridandfractionalstrides foreachlayerweconsiderthesetofrates13 12 ... 89 910(forfractionalstridesthisisthefractionofconvolutionscalculated) andevaluateallcombinationsofsuchrates.Then foreachmethod webuildaPareto-optimalfrontofparameters6CPU speedup (times)1234Top-1 error (%)102030405060ResizeStrideFrac. strideGridImpact(a)OriginalnetworkCPU speedup (times)1234Top-1 error (%)10.51111.51212.51313.51414.515(b)AfterretrainingFigure5:Comparisonofwholenetworkperforation(gridandimpactmask)withbaselinestrategies(resizingtheinputimages increasingthestridesofconvolutionallayers)foraccelerationofCIFAR-10NINnetwork.whichproducedsmallesterrorincreaseforagivenCPUspeedup.Finally wetrainthenetworkweights“fromscratch”(startingfromarandominitialization)forthePareto-optimalconﬁgurationswithaccelerationscloseto2× 3× 4×.Forfractionalstrides weuseﬁne-tuning sinceitperformssigniﬁcantlybetterthantrainingfromscratch.Theresultsaredisplayedonﬁg.5.Impactperforationisthebeststrategybothfortheoriginalnetworkandaftertrainingthenetworkfromscratch.Gridperforationisslightlyworse.ConvolutionalstridesareusedinmanyCNNs suchasAlexNet todecreasethecomputationalcostoftrainingandevaluation.Ourresultsshowthatifchangingtheintermediaterepresentationssizeandtrainingthenetworkfromscratchisanoption thenitisindeedagoodstrategy.Althoughmoregeneral fractionalstridesperformpoorlycomparedtostrides mostlikelybecausethey“downsample”theoutputsofaconvolutionallayernon-uniformly makingthemhardtoprocessbythenextconvolutionallayer.4.3WholenetworkresultsWeevaluatetheeffectofperforationofalltheconvolutionallayersofthreeCNNmodels.Totunetheperforationrates weemploythegreedymethoddescribedinsection3.3.Weusetwentyperforationrates:13 12 23 ... 1819 1920.ForNINandAlexNetweusetheimpactperforationmask.ForVGG-16weusethegridperforationmaskasweﬁndthatitconsiderablysimpliﬁesﬁne-tuning.Usingmorethanonetypeofperforationmasksdoesnotimprovetheresults.Obtainingtheperforationratesconﬁgurationtakesaboutonedayforthelargestnetworkweconsidered VGG-16.Inordertodecreasetheerroroftheacceleratednetwork wetunethenetwork’sweights.Wedonotobserveanyproblemswithbackpropagation suchasexploding/vanishinggradients.Theresultsarepresentedintable3.Perforationdamagesthenetworkperformancesigniﬁcantly butnetworkweightstuningrestoresmostoftheaccuracy.AlltheconsiderednetworksmaybeacceleratedbyafactoroftwoonbothCPUandGPU withunder2.6%increaseoferror.Theoreticalspeedups(reductionofthenumberofmultiplications)areusuallyclosetotheempiricalones.Additionally thememoryrequiredtostorenetworkactivationsissigniﬁcantlyreducedbystoringonlythenon-perforatedoutputvalues.4.4CombiningaccelerationmethodsApromisingwaytoachievehighspeedupswithlowerrorincreaseistocombinemultipleaccelerationmethods.Forthistosucceed themethodsshouldexploitdifferenttypesofredundancyinthenetwork.Inthissection weverifythatperforationcanbecombinedwiththeinter-channelredundancyeliminationapproachof[28]toachieveimprovedspeedup-errorratios.Wereimplementthelinearasymmetricmethodof[28].Itdecomposesaconvolutionallayerwitha(d×d×S×T)kernel(height-width-inputchannels-outputchannels)intoasequenceoftwolayers (d×d×S×T0)→(1×1×T0×T) T0<T.Thesecondlayeristypicallyveryfast sotheoverallspeedupisroughlyTT0.Whendecomposingaperforatedconvolutionallayer wetransfertheperforationmasktotheﬁrstobtainedlayer.Weﬁrstapplyperforationtothenetworkandﬁne-tuneit asintheprevioussection.Then weapplytheinter-channelredundancyeliminationmethodtothisnetwork.Finally weperformthesecondroundofﬁne-tuningwithamuchlowerlearningrateof1e-9 duetoexplodinggradients.Allthemethodsaretestedatthetheoreticalspeeduplevelof4×.Whenthetwomethodsarecombined theaccelerationrateforeachmethodistakentoberoughlyequal.Theresultsarepresentedinthetable7NetworkDeviceSpeedupMult.↓Mem.↓Error↑(%)Tunederror↑(%)NINCPU2.2×2.5×2.0×+1.5+0.43.1×4.4×3.5×+5.5+1.94.2×6.6×4.4×+8.3+2.9GPU2.1×3.6×3.3×+4.5+1.63.0×10.1×5.7×+18.2+5.63.5×19.1×9.2×+37.4+12.4AlexNetCPU2.0×2.1×1.8×+10.7+2.33.0×3.5×2.6×+28.0+6.13.6×4.4×2.9×+60.7+9.9GPU2.0×2.0×1.7×+8.5+2.03.0×2.6×2.0×+16.4+3.24.1×3.4×2.4×+28.1+6.2VGG-16CPU2.0×1.8×1.5×+15.6+1.13.0×2.9×1.8×+54.3+3.74.0×4.0×2.5×+71.6+5.5GPU2.0×1.9×1.7×+23.1+2.53.0×2.8×2.4×+65.0+6.84.0×4.7×3.4×+76.5+7.3Table3:Fullnetworkaccelerationresults.Arrowsindicateincreaseordecreaseinthemetric.Speedupisthewall-clockacceleration.Mult.isareductionofthenumberofmultiplicationsinconvolutionallayers(theoreticalspeedup).Mem.isareductionofmemoryrequiredtostorethenetworkactivations.Tunederroristheerroraftertrainingfromscratch(NIN)orﬁne-tuning(AlexNet VGG16)oftheacceleratednetwork’sweights.PerforationAsymm.[28]Mult.↓Mem.↓Error↑(%)Tunederror↑(%)4.0×-4.0×2.5×+71.6+5.5-3.9×3.9×0.93×+6.7+2.01.8×2.2×4.0×1.4×+2.9+1.6Table4:AccelerationofVGG-16 4×theoreticalspeedup.Firstrowistheproposedmethod thesecondrowisourreimplementationoflinearasymmetricmethodofZhangetal.[28] thethirdrowisthecombinedmethod.PerforationiscomplementarytotheaccelerationmethodofZhangetal.4.Whilethedecompositionmethodoutperformsperforation thecombinedmethodisbetterthanbothofthecomponents.5ConclusionWehavepresentedPerforatedCNNswhichexploitredundancyofintermediaterepresentationsofmodernCNNstoreducetheevaluationtimeandmemoryconsumption.PerforationrequiresonlyaminormodiﬁcationoftheconvolutionlayerandobtainsspeedupsclosetotheoreticalonesonbothCPUandGPU.Comparedtothebaselines PerforatedCNNsachievelowererror aremoreﬂexibleanddonotchangethearchitectureofaCNN(numberofparametersinthefully-connectedlayersandthesizeoftheintermediaterepresentations).RetainingthearchitectureallowstoeasilypluginPerforatedCNNsintotheexistingcomputervisionpipelinesandonlyperformﬁne-tuningofthenetwork insteadofcompleteretraining.Additionally perforationcanbecombinedwithaccelerationmethodswhichexploitothertypesofnetworkredundancytoachievefurtherspeedups.Infuture weplantoexploretheconnectionbetweenPerforatedCNNsandvisualattentionbyconsideringinput-dependentperforationmasksthatcanfocusonthesalientpartsoftheinput.Unlikerecentworksonvisualattention[1 11 20]whichconsiderrectangularcropsofanimage PerforatedCNNscanprocessnon-rectangularandevendisjointsalientpartsoftheimagebychoosingappropriateperforationmasksintheconvolutionallayers.Acknowledgments.WewouldliketothankAlexanderKirillovandDmitryKropotovforhelpfuldiscussions andYandexforprovidingcomputationalresourcesforthisproject.ThisworkwassupportedbyRFBRprojectNo.15-31-20596(mol-a-ved)andbyMicrosoft:MoscowStateUniversityJointResearchCenter(RPD1053945).8References[1]J.Ba R.Salakhutdinov R.Grosse andB.Frey “Learningwake-sleeprecurrentattentionmodels ”NIPS 2015.[2]T.Chen “Matrixshadowlibrary ”https://github.com/dmlc/mshadow 2015.[3]S.Chetlur C.Woolley P.Vandermersch J.Cohen J.Tran B.Catanzaro andE.Shelhamer “cuDNN:Efﬁcientprimitivesfordeeplearning ”arXiv 2014.[4]M.D.CollinsandP.Kohli “Memoryboundeddeepconvolutionalnetworks ”arXiv 2014.[5]M.Courbariaux Y.Bengio andJ.David “Lowprecisionarithmeticfordeeplearning ”ICLR 2015.[6]E.L.Denton W.Zaremba J.Bruna Y.LeCun andR.Fergus “Exploitinglinearstructurewithinconvolutionalnetworksforefﬁcientevaluation ”NIPS 2014.[7]B.Graham “Fractionalmax-pooling ”arXiv 2014.[8]—— “Spatially-sparseconvolutionalneuralnetworks ”arXiv 2014.[9]S.Gupta A.Agrawal K.Gopalakrishnan andP.Narayanan “Deeplearningwithlimitednumericalprecision ”ICML 2015.[10]M.Jaderberg A.Vedaldi andA.Zisserman “Speedingupconvolutionalneuralnetworkswithlowrankexpansions ”BMVC 2014.[11]M.Jaderberg K.Simonyan A.Zissermanetal. “Spatialtransformernetworks ”NIPS 2015.[12]Y.Jia E.Shelhamer J.Donahue S.Karayev J.Long R.Girshick S.Guadarrama andT.Darrell “Caffe:Convolutionalarchitectureforfastfeatureembedding ”ACMICM 2014.[13]A.Krizhevsky “cuda–convnet2 ”https://github.com/akrizhevsky/cuda-convnet2/ 2014.[14]A.Krizhevsky I.Sutskever andG.E.Hinton “Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks ”NIPS 2012.[15]V.LebedevandV.Lempitsky “Fastconvnetsusinggroup-wisebraindamage ”CVPR 2016.[16]V.Lebedev Y.Ganin M.Rakhuba I.Oseledets andV.Lempitsky “Speeding-upconvolutionalneuralnetworksusingﬁne-tunedCP-decomposition ”ICLR 2015.[17]M.Lin Q.Chen andS.Yan “Networkinnetwork ”ICLR 2014.[18]S.Misailovic S.Sidiroglou H.Hoffmann andM.Rinard “Qualityofserviceproﬁling ”ICSE 2010.[19]S.Misailovic D.M.Roy andM.C.Rinard “Probabilisticallyaccurateprogramtransformations ”StaticAnalysis 2011.[20]V.Mnih N.Heess A.Gravesetal. “Recurrentmodelsofvisualattention ”NIPS 2014.[21]A.Novikov D.Podoprikhin A.Osokin andD.Vetrov “Tensorizingneuralnetworks ”NIPS 2015.[22]K.Ovtcharov O.Ruwase J.-Y.Kim J.Fowers K.Strauss andE.S.Chung “Acceleratingdeepconvolu-tionalneuralnetworksusingspecializedhardware ”MicrosoftResearchWhitepaper 2015.[23]M.Samadi D.A.Jamshidi J.Lee andS.Mahlke “Paraprox:Pattern-basedapproximationfordataparallelapplications ”ASPLOS 2014.[24]S.Sidiroglou-Douskos S.Misailovic H.Hoffmann andM.Rinard “Managingperformancevs.accuracytrade-offswithloopperforation ”ACMSIGSOFT 2011.[25]K.SimonyanandA.Zisserman “Verydeepconvolutionalnetworksforlarge-scaleimagerecognition ”ICLR 2015.[26]A.VedaldiandK.Lenc “MatConvNet–convolutionalneuralnetworksforMATLAB ”arXiv 2014.[27]Z.Yang M.Moczulski M.Denil N.deFreitas A.J.Smola L.Song andZ.Wang “Deepfriedconvnets ”ICCV 2015.[28]X.Zhang J.Zou K.He andJ.Sun “Acceleratingverydeepconvolutionalnetworksforclassiﬁcationanddetection ”arXiv 2015.9,Fajwel Fogel
Alexandre d'Aspremont
Milan Vojnovic
Mikhail Figurnov
Aizhan Ibraimova
Dmitry Vetrov
Pushmeet Kohli
Tobias Plötz
Stefan Roth