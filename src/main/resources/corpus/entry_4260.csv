2016,PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions,We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks  a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization  we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally  we show that perforation is complementary to the recently proposed acceleration method of Zhang et al.,PerforatedCNNs:AccelerationthroughEliminationofRedundantConvolutionsMichaelFigurnov1 2 AijanIbraimova4 DmitryVetrov1 3 andPushmeetKohli51NationalResearchUniversityHigherSchoolofEconomics2LomonosovMoscowStateUniversity3Yandex4SkolkovoInstituteofScienceandTechnology5MicrosoftResearchmichael@figurnov.ru aijan.ibraimova@gmail.com vetrovd@yandex.ru pkohli@microsoft.comAbstractWeproposeanovelapproachtoreducethecomputationalcostofevaluationofconvolutionalneuralnetworks afactorthathashinderedtheirdeploymentinlow-powerdevicessuchasmobilephones.Inspiredbytheloopperforationtechniquefromsourcecodeoptimization wespeedupthebottleneckconvolutionallayersbyskippingtheirevaluationinsomeofthespatialpositions.Weproposeandanalyzeseveralstrategiesofchoosingthesepositions.WedemonstratethatperforationcanacceleratemodernconvolutionalnetworkssuchasAlexNetandVGG-16byafactorof2√ó-4√ó.Additionally weshowthatperforationiscomplementarytotherecentlyproposedaccelerationmethodofZhangetal.[28].1IntroductionThelastfewyearshaveseenconvolutionalneuralnetworks(CNNs)emergeasanindispensabletoolforcomputervision.However modernCNNshaveahighcomputationalcostofevaluation withconvolutionallayersusuallytakingupover80%ofthetime.Forinstance VGG-16network[25]fortheproblemofobjectrecognitionrequires1.5¬∑1010Ô¨Çoatingpointmultiplicationsperimage.ThesecomputationalrequirementshinderthedeploymentofsuchnetworksonsystemswithoutGPUsandinscenarioswherepowerconsumptionisamajorconcern suchasmobiledevices.Theproblemoftradingaccuracyofcomputationsforspeediswell-knownwithinthesoftwareengineeringcommunity.Oneofthemostprominentmethodsforthisproblemisloopperforation[18 19 24].Inanutshell thistechniqueisolatesloopsinthecodethatarenotcriticalfortheexecution andthenreducestheircomputationalcostbyskippingsomeiterations.Morerecently researchershaveconsideredproblem-dependentperforationstrategiesthatexploitthestructureoftheproblem[23].Inspiredbythegeneralprincipleofperforation weproposetoreducethecomputationalcostofCNNevaluationbyexploitingthespatialredundancyofthenetwork.ModernCNNs suchasAlexNet exploitthisredundancythroughtheuseofstridesintheconvolutionallayers.However usingtheconvolutionalstrideschangesthearchitectureofthenetwork(intermediaterepresentationssizeandthenumberofweightsintheÔ¨Årstfully-connectedlayer) whichmightbeundesirable.Insteadofusingstrides wearguefortheuseofinterpolation(perforation)ofresponsesintheconvolutionallayer.Akeyelementofthisapproachisthechoiceoftheperforationmask whichdeÔ¨Ånestheoutputpositionstoevaluateexactly.Weproposeseveralapproachestoselecttheperforationmasksandamethodofchoosingacombinationofperforationmasksfordifferentlayers.Torestorethenetworkaccuracy weperformÔ¨Åne-tuningoftheperforatednetwork.OurexperimentsshowthatthismethodcanreducetheevaluationtimeofmodernCNNarchitecturesproposedintheliteraturebyafactorof2√ó-4√ówithasmalldecreaseinaccuracy.2RelatedWorkReducingthecomputationalcostofCNNevaluationisanactiveareaofresearch withbothhighlyoptimizedimplementationsandapproximatemethodsinvestigated.30thConferenceonNeuralInformationProcessingSystems(NIPS2016) Barcelona Spain.ùëë"ùëÜùëã%ùëå‚Ä≤tensorUdata matrix MùëåùëãùëëùëëùëÜùëÜim2rowkernel KtensorV√ó=ùëáùëå‚Ä≤X‚Ä≤ùëáùëë"ùëÜùëá11Figure1:Reductionofconvolutionallayerevaluationtomatrixmultiplication.Ourideaistoleaveonlyasubsetofrows(deÔ¨Ånedbyaperforationmask)inthedatamatrixMandtointerpolatethemissingoutputvalues.ImplementationsthatexploittheparallelismavailableincomputationalarchitectureslikeGPUs(cuda-convnet2[13] CuDNN[3])haveallowedtosigniÔ¨ÅcantlyreducetheevaluationtimeofCNNs.SinceCuDNNinternallyreducesthecomputationofconvolutionallayerstothematrix-by-matrixmultiplication(withoutexplicitlymaterializingthedatamatrix) ourapproachcanpotentiallybeincorporatedintothislibrary.Inasimilarvein theuseofFPFGAs[22]leadstobettertrade-offsbetweenspeedandpowerconsumption.Severalpapers[5 9]showedthatCNNsmaybeefÔ¨Åcientlyevaluatedusinglowprecisionarithmetic whichisimportantforFPFGAimplementations.MostapproximatemethodsofdecreasingtheCNNcomputationalcostexploittheredundanciesoftheconvolutionalkernelusinglow-ranktensordecompositions[6 10 16 28].Inmostcases aconvolutionallayerisreplacedbyseveralconvolutionallayersappliedsequentially whichhaveamuchlowertotalcomputationalcost.WeshowthatthecombinationofperforationwiththemethodofZhangetal.[28]improvesuponbothapproaches.Forspatiallysparseinputs itispossibletoexploitthissparsitytospeedupevaluationandtraining[8].Whilethisapproachissimilartooursinthespirit wedonotrelyonspatiallysparseinputs.Instead wesparselysampletheoutputsofaconvolutionallayerandinterpolatetheremainingvalues.Inarecentwork LebedevandLempitsky[15]alsodecreasetheCNNcomputationalcostbyreducingthesizeofthedatamatrix.Thedifferenceisthattheirapproachreducestheconvolutionalkernel‚Äôssupportwhileourapproachdecreasesthenumberofspatialpositionsinwhichtheconvolutionsareevaluated.Thetwomethodsarecomplementary.Severalpapershavedemonstratedthatitispossibletocompresstheparametersofthefully-connectedlayers(wheremostCNNparametersreside)withamarginalerrorincrease[4 21 27].Sinceourmethoddoesnotdirectlymodifythefully-connectedlayers itispossibletocombinethesemethodswithourapproachandobtainafastandsmallCNN.3PerforatedCNNsThesectionprovidesadetaileddescriptionofourapproach.Beforeproceedingfurther weintroducethenotationthatwillbeusedintherestofthepaper.Notation.AconvolutionallayertakesasinputatensorUofsizeX√óY√óSandoutputsatensorVofsizeX0√óY0√óT X0=X‚àíd+1 Y0=Y‚àíd+1.TheÔ¨Årsttwodimensionsarespatial(heightandwidth) andthethirddimensionisthenumberofchannels(forexample foranRGBinputimageS=3).ThesetofTconvolutionkernelsKisgivenbyatensorofsized√ód√óS√óT.Forsimplicityofnotation weassumeunitstride nozero-paddingandskipthebiases.TheconvolutionallayeroutputmaybedeÔ¨Ånedasfollows:V(x y t)=dXi=1dXj=1SXs=1K(i j s t)U(x+i‚àí1 y+j‚àí1 s)(1)Additionally wedeÔ¨Ånethesetofallspatialindices(positions)oftheoutput‚Ñ¶={1 ... X0}√ó{1 ... Y0}.PerforationmaskI‚äÜ‚Ñ¶isthesetofindicesinwhichtheoutputsarecalculatedexactly.DenoteN=|I|thenumberofpositionstobecalculatedexactly andr=1‚àíN|‚Ñ¶|theperforationrate.Reductiontomatrixmultiplication.Toachievehighcomputationalperformance manydeeplearn-ingframeworks includingCaffe[12]andMatConvNet[26] reducethecomputationofconvolutional2layerstotheheavily-optimizedmatrix-by-matrixmultiplicationroutineofbasiclinearalgebrapack-ages.Thisprocess sometimesreferredtoaslowering isillustratedinÔ¨Åg.1.First adatamatrixMofsizeX0Y0√ód2Sisconstructedusingim2rowfunction.TherowsofMareelementsofpatchesofinputtensorUofsized√ód√óS.Then MismultipliedbythekerneltensorKreshapedintosized2S√óT.TheresultingmatrixofsizeX0Y0√óTistheoutputtensorV uptoareshape.Foramoredetailedexposition see[26].3.1PerforatedconvolutionallayerInthissectionwepresenttheperforatedconvolutionallayer.Inasmallfractionofspatialpositions theoutputsoftheproposedlayerareequaltotheoutputsofausualconvolutionallayer.Theremainingvaluesareinterpolatedusingthenearestneighborfromthissetofpositions.WeevaluateotherinterpolationstrategiesinappendixA.Theperforatedconvolutionallayerisageneralizationofthestandardconvolutionallayer.Whentheperforationmaskisequaltoalltheoutputspatialpositions theperforatedconvolutionallayer‚Äôsoutputequalstheconventionalconvolutionallayer‚Äôsoutput.Formally letI‚äÜ‚Ñ¶betheperforationmaskofspatialoutputtobecalculatedexactly(theconstraintthatthemasksaresharedforallchannelsoftheoutputisrequiredforthereductiontomatrixmultiplication).Thefunction‚Äò(x y):‚Ñ¶‚ÜíIreturnstheindexofthenearestneighborinIaccordingtoEuclideandistance(withtiesbrokenrandomly):‚Äò(x y)=(‚Äò1(x y) ‚Äò2(x y))=argmin(x0 y0)‚ààIp(x‚àíx0)2+(y‚àíy0)2.(2)Notethatthefunction‚Äò(x y)maybecalculatedinadvanceandcached.TheperforatedconvolutionallayeroutputÀÜVisdeÔ¨Ånedasfollows:ÀÜV(x y t)=V(‚Äò1(x y) ‚Äò2(x y) t) (3)whereV(x y t)istheoutputoftheusualconvolutionallayer deÔ¨Ånedby(1).Since‚Äò(x y)=(x y)for(x y)‚ààI theoutputsinthespatialpositionsIarecalculatedexactly.Thevaluesinotherpositionsareinterpolatedusingthevalueofthenearestneighbor.Toevaluateaperforatedconvolutionallayer weonlyneedtocalculatethevaluesV(x y t)for(x y)‚ààI whichcanbedoneefÔ¨Åcientlybyreductiontomatrixmultiplication.Inthiscase thedatamatrixMcontainsjustN=|I|rows insteadoftheoriginalX0Y0=|‚Ñ¶|rows.Perforationisnotlimitedtothisimplementationofaconvolutionallayer andcanbecombinedwithotherimplementationsthatsupportstridedconvolutions suchasthedirectconvolutionapproachofcuda-convnet2[13].Inourimplementation weonlystoretheoutputvaluesV(x y t)for(x y)‚ààI.Theinterpolationisperformedimplicitlybymaskingthereadsofthefollowingpoolingorconvolutionallayer.Forexample whenacceleratingconv3layerofAlexNet theinterpolationcostistransferredtoconv4layer.Weobservenoslowdownoftheconv4layerwhenusingGPU anda0-3%slowdownwhenusingCPU.Thisdesignchoicehasseveraladvantages.Firstly thememorysizerequiredtostoretheactivationsisreducedbyafactorof11‚àír.Secondly thefollowingnon-linearitylayersand1√ó1convolutionallayersarealsospedupsincetheyareappliedtoasmallernumberofelements.3.2PerforationmasksWeproposeseveralwaysofgeneratingtheperforationmasks orchoosingNpointsfrom‚Ñ¶.WevisualizetheperforationmasksIasbinarymatriceswithblacksquaresinthepositionsofthesetI.Weonlyconsidertheperforationmasksthatareindependentoftheinputobjectandleaveexplorationofinput-dependentperforationmaskstothefuturework.UniformperforationmaskisjustNpointschosenrandomlywithoutreplacementfromtheset‚Ñ¶.However ascanbeseenfromÔ¨Åg.2a forN(cid:28)|‚Ñ¶| thepointstendtocluster.ThisisundesirablebecauseamorescatteredsetIwouldreducetheaveragedistancetothesetI.GridperforationmaskisasetofpointsI={a(1) ... a(Kx)}√ó{b(1) ... b(Ky)} seeÔ¨Åg.2b.Wechoosethevaluesofa(i) b(i)usingthepseudorandomintegersequencegenerationschemeof[7].Poolingstructuremaskexploitsthestructureoftheoverlapsofpoolingoperators.DenotebyA(x y)thenumberoftimesanoutputoftheconvolutionallayerisusedinthepoolingoperators.Thegrid-likepatternasinÔ¨Åg.2discausedbyapoolingofsize3√ó3withstride2(suchparametersareusede.g.inNetworkinNetworkandAlexNet).Thepoolingstructuremaskisobtainedbypickingtop-NpositionswiththehighestvaluesofA(x y) withtiesbrokenrandomly seeÔ¨Åg.2c.3(a)Uniform(b)Grid(c)Poolingstruc-ture1234(d)WeightsA(x y)Figure2:Perforationmasks AlexNetconv2 r=80.25%.Bestviewedincolor.00.010.020.030.040.050.060.070.080.050.10.150.20.250.300.050.10.150.20.250.30.010.020.030.040.050.060.070.080.09(a)B(x y) origi-nalnetwork00.050.10.150.20.250.30.350.4(b)B(x y) perfo-ratednetwork(c)Impactmask r=90%Figure3:Top:ImageNetimagesandcorrespondingvaluesofimpactG(x y;V)forAlexNetconv2.Bottom:averageimpactsandimpactperforationmaskforAlexNetconv2.Bestviewedincolor.ImpactmaskestimatestheimpactofperforationofeachpositionontheCNNlossfunction andthenremovestheleastimportantpositions.DenotebyL(V)thelossfunctionoftheCNN(suchasnegativelog-likelihood)asafunctionoftheconsideredconvolutionallayeroutputsV.Next supposeV0isobtainedfromVbyreplacingoneelement(x0 y0 t0)withaneutralvaluezero.WeestimatetheimpactofapositionasaÔ¨Årst-orderTaylorapproximationofthemagnitudeofchangeofL(V):|L(V0)‚àíL(V)|‚âà(cid:12)(cid:12)(cid:12)XXx=1YXy=1TXt=1‚àÇL(V)‚àÇV(x y t)(V0(x y t)‚àíV(x y t))(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)‚àÇL(V)‚àÇV(x0 y0 t0)V(x0 y0 t0)(cid:12)(cid:12)(cid:12).(4)Thevalue‚àÇL(V)‚àÇV(x0 y0 t0)maybeobtainedusingbackpropagation.Inthecaseofaperforatedconvolu-tionallayer wecalculatethederivativeswithrespecttotheconvolutionallayeroutputV(nottheinterpolatedoutputÀÜV).Thismakestheimpactofthepreviouslyperforatedpositionszeroandsumstheimpactofthenon-perforatedpositionsoveralltheoutputswhichsharethevalue.Sinceweareinterestedinthetotalimpactofaspatialposition(x y)‚àà‚Ñ¶ wetakeasumoverallthechannelsandaveragethisestimateofimpactsoverthetrainingdataset:G(x y;V)=TXt=1(cid:12)(cid:12)(cid:12)‚àÇL(V)‚àÇV(x y t)V(x y t)(cid:12)(cid:12)(cid:12)(5)B(x y)=EV‚àºtrainingsetG(x y;V)(6)Finally theimpactmaskisformedbytakingthetop-NpositionswiththehighestvaluesofB(x y).ExamplesofthevaluesofG(x y;V) B(x y)andimpactmaskareshownonÔ¨Åg.3.NotethattheregionsofthehighvalueofG(x y;V)usuallycontainthemostsalientfeaturesoftheimage.TheaveragedweightsB(x y)tendtobehigherinthecentersinceImageNet‚Äôsimagesusuallycontainacenteredobject.Additionally agrid-likestructureofpoolingstructuremaskisautomaticallyinferred.4NetworkDatasetErrorCPUtimeGPUtimeMem.Mult.#convNINCIFAR-10top-110.4%4.6ms0.8ms5.1MB2.2¬∑1083AlexNetImageNettop-519.6%16.7ms2.0ms6.6MB0.5¬∑1095VGG-16top-510.1%300ms29ms110MB1.5¬∑101013Table1:DetailsoftheCNNsusedfortheexperimentalevaluation.Timings memoryconsumptionandnumberofmultiplicationsarenormalizedbythebatchsize.Memoryconsumptionisthememoryrequiredtostoreactivations(intermediateresults)ofthenetworkduringtheforwardpass.CPU speedup (times)123456Top-5 error increase (%)0246810UniformGridPooling structureImpact(a)conv2 CPUGPU speedup (times)123456Top-5 error increase (%)0246810(b)conv2 GPUCPU speedup (times)123456Top-5 error increase (%)0246810UniformGridImpact(c)conv3 CPUGPU speedup (times)123456Top-5 error increase (%)0246810(d)conv3 GPUFigure4:AccelerationofasinglelayerofAlexNetfordifferentmasktypeswithoutÔ¨Åne-tuning.Valuesareaveragedover5runs.Sinceperforationofalayerchangestheimpactsofallthelayers intheexperimentsweiteratebetweenincreasingtheperforationrateofalayerandrecalculationofimpacts.WeÔ¨Åndthatthisimprovesresultsbyco-adaptingtheperforationmasksofdifferentconvolutionallayers.3.3ChoosingtheperforationconÔ¨ÅgurationsForwholenetworkacceleration itisimportanttoÔ¨Åndacombinationofper-layerperforationratesthatwouldachievehighspeedupwithlowerrorincrease.Todothis weemployasimplegreedystrategy.WeuseasingleperforationmasktypeandaÔ¨Åxedrangeofincreasingperforationrates.Denotebyttheevaluationtimeoftheacceleratednetworkandbyetheobjective(weusenegativelog-likelihoodforasubsetoftrainingimages).Lett0ande0betherespectivevaluesforthenon-acceleratednetwork.Ateachiteration wetrytoincreasetheperforationrateforeachlayerandchoosethelayerforwhichthisresultsintheminimalvalueofthecostfunctione‚àíe0t0‚àít.4ExperimentsWeusethreeconvolutionalneuralnetworksofincreasingsizeandcomputationalcomplexity:Net-workinNetwork[17] AlexNet[14]andVGG-16[25] seetable1.Inallnetworks weattempttoperforatealltheconvolutionallayers exceptforthe1√ó1convolutionallayersofNIN.Weperformtimingsonacomputerwithaquad-coreIntelCorei5-4460CPU 16GBRAMandanVidiaGeforceGTX980GPU.Thebatchsizeusedfortimingsis128forNIN 256forAlexNetand16forVGG-16.ThenetworksareobtainedfromCaffeModelZoo.ForAlexNet theCaffereimplementationisusedwhichisslightlydifferentfromtheoriginalarchitecture(poolingandnormalizationlayersareswapped).WeuseaforkofMatConvNetframeworkforallexperi-ments exceptforÔ¨Åne-tuningofAlexNetandVGG-16 forwhichweuseaforkofCaffe.Thesourcecodeisavailableathttps://github.com/mfigurnov/perforated-cnn-matconvnet https://github.com/mfigurnov/perforated-cnn-caffe.Webeginourexperimentsbycomparingtheproposedperforationmasksinacommonbenchmarksetting:accelerationofasingleAlexNetlayer.Then wecomparewhole-networkaccelerationwiththebest-performingmaskstobaselinessuchasdecreaseofinputimagessizeandanincreaseofstrides.Weproceedtoshowthatperforationscalestolargenetworksbypresentingthewhole-networkaccelerationresultsforAlexNetandVGG-16.Finally wedemonstratethatperforationiscomplementarytotherecentlyproposedaccelerationmethodofZhangetal.[28].5MethodCPUtime‚ÜìError‚Üë(%)Impact r=34 3√ó3Ô¨Ålters9.1√ó+1Impact r=565.3√ó+1.4Impact r=454.2√ó+0.9LebedevandLempitsky[15]20√ótop-1+1.1LebedevandLempitsky[15]9√ótop-1+0.3Jaderbergetal.[10]6.6√ó+1Lebedevetal.[16]4.5√ó+1Dentonetal.[6]2.7√ó+1Table2:AccelerationofAlexNet‚Äôsconv2.Top:ourresultsafterÔ¨Åne-tuning bottom:previouslypublishedresults.Resultof[10]providedby[16].Theexperimentwithreducedspatialsizeofthekernel(3√ó3 insteadof5√ó5)suggeststhatperforationiscomplementarytothe‚Äúbraindamage‚Äùmethodof[15]whichalsoreducesthespatialsupportofthekernel.4.1SinglelayerresultsWeexplorethespeedup-errortrade-offoftheproposedperforationmasksonthetwobottleneckconvolutionallayersofAlexNet conv2andconv3 seeÔ¨Åg.4.Thepoolingstructureperforationmaskisonlyapplicabletotheconv2becauseitisdirectlyfollowedbyamax-pooling whereastheconv3isfollowedbyanotherconvolutionallayer.Weseethatimpactperforationmaskworksbestfortheconv2layerwhilegridmaskperformsverywellforconv3.Thestandarddeviationofresultsissmallforalltheperforationmasks excepttheuniformmaskforhighspeedups(wherethegridmaskoutperformsit).TheresultsaresimilarforbothCPUandGPU showingtheapplicabilityofourmethodforbothplatforms.Notethatifweconsiderthebestperforationmaskforeachspeedupvalue thenweseethattheconv2layeriseasiertoacceleratethantheconv3layer.Weobservethispatterninotherexperiments:layersimmediatelyfollowedbyamax-poolingareeasiertoacceleratethanthelayersfollowedbyaconvolutionallayer.AdditionalresultsforNINnetworkarepresentedinappendixB.WecompareourresultsafterÔ¨Åne-tuningtothepreviouslypublishedresultsontheaccelerationofAlexNet‚Äôsconv2intable2.Motivatedbytheresultsof[15]thatthespatialsupportofconv2convolutionalkernelmaybereducedwithasmallerrorincrease wereducethekernel‚Äôsspatialsizefrom5√ó5to3√ó3andapplytheimpactperforationmask.Thisleadstothe9.1√óaccelerationfor1%top-5errorincrease.Usingthemoresophisticatedmethodof[15]toreducethespatialsupportmayleadtofurtherimprovements.4.2BaselinesWecomparePerforatedCNNswiththebaselinemethodsofdecreasingthecomputationalcostofCNNsbyexploitingthespatialredundancy.Unlikeperforation thesemethodsdecreasethesizeoftheactivations(intermediateoutputs)oftheCNN.Foranetworkwithfully-connected(FC)layers thiswouldchangethenumberofCNNparametersintheÔ¨ÅrstFClayer effectivelymodifyingthearchitecture.Toavoidthis weuseCIFAR-10NINnetwork whichreplacesFClayerswithglobalaveragepooling(mean-poolingoverallspatialpositionsinthelastlayer).Weconsiderthefollowingbaselinemethods.Resize.Theinputimageisdownscaledwiththeaspectratiopreserved.Stride.Thestridesoftheconvolutionallayersareincreased makingtheactivationsspatiallysmaller.Fractionalstride.Motivatedbyfractionalmax-pooling[7] weintroduceamoreÔ¨ÇexiblemodiÔ¨Åcationofstrideswhichevaluatesconvolutionsonanon-regulargrid(withavaryingstepsize) providingamoreÔ¨Åne-grainedcontrolovertheactivationssizeandspeedup.Weusegridperforationmaskgenerationschemetochoosetheoutputpositionstoevaluate.Wecomparethesestrategiestoperforationofallthelayerswiththetwotypesofmaskswhichperformedbestintheprevioussection:gridandimpact.Notethat‚Äúgrid‚Äùis infact equivalenttofractionalstrides butwithmissingvaluesbeinginterpolated.Allthemethods exceptresize requireaparametervalueperconvolutionallayer leadingtoalargenumberofpossibleconÔ¨Ågurations.WeusetheoriginalnetworktoexplorethisspaceofconÔ¨Ågurations.Forimpact weusethegreedyalgorithm.Forstride weevaluateallpossiblecombinationsofparameters.Forgridandfractionalstrides foreachlayerweconsiderthesetofrates13 12 ... 89 910(forfractionalstridesthisisthefractionofconvolutionscalculated) andevaluateallcombinationsofsuchrates.Then foreachmethod webuildaPareto-optimalfrontofparameters6CPU speedup (times)1234Top-1 error (%)102030405060ResizeStrideFrac. strideGridImpact(a)OriginalnetworkCPU speedup (times)1234Top-1 error (%)10.51111.51212.51313.51414.515(b)AfterretrainingFigure5:Comparisonofwholenetworkperforation(gridandimpactmask)withbaselinestrategies(resizingtheinputimages increasingthestridesofconvolutionallayers)foraccelerationofCIFAR-10NINnetwork.whichproducedsmallesterrorincreaseforagivenCPUspeedup.Finally wetrainthenetworkweights‚Äúfromscratch‚Äù(startingfromarandominitialization)forthePareto-optimalconÔ¨Ågurationswithaccelerationscloseto2√ó 3√ó 4√ó.Forfractionalstrides weuseÔ¨Åne-tuning sinceitperformssigniÔ¨Åcantlybetterthantrainingfromscratch.TheresultsaredisplayedonÔ¨Åg.5.Impactperforationisthebeststrategybothfortheoriginalnetworkandaftertrainingthenetworkfromscratch.Gridperforationisslightlyworse.ConvolutionalstridesareusedinmanyCNNs suchasAlexNet todecreasethecomputationalcostoftrainingandevaluation.Ourresultsshowthatifchangingtheintermediaterepresentationssizeandtrainingthenetworkfromscratchisanoption thenitisindeedagoodstrategy.Althoughmoregeneral fractionalstridesperformpoorlycomparedtostrides mostlikelybecausethey‚Äúdownsample‚Äùtheoutputsofaconvolutionallayernon-uniformly makingthemhardtoprocessbythenextconvolutionallayer.4.3WholenetworkresultsWeevaluatetheeffectofperforationofalltheconvolutionallayersofthreeCNNmodels.Totunetheperforationrates weemploythegreedymethoddescribedinsection3.3.Weusetwentyperforationrates:13 12 23 ... 1819 1920.ForNINandAlexNetweusetheimpactperforationmask.ForVGG-16weusethegridperforationmaskasweÔ¨ÅndthatitconsiderablysimpliÔ¨ÅesÔ¨Åne-tuning.Usingmorethanonetypeofperforationmasksdoesnotimprovetheresults.ObtainingtheperforationratesconÔ¨Ågurationtakesaboutonedayforthelargestnetworkweconsidered VGG-16.Inordertodecreasetheerroroftheacceleratednetwork wetunethenetwork‚Äôsweights.Wedonotobserveanyproblemswithbackpropagation suchasexploding/vanishinggradients.Theresultsarepresentedintable3.PerforationdamagesthenetworkperformancesigniÔ¨Åcantly butnetworkweightstuningrestoresmostoftheaccuracy.AlltheconsiderednetworksmaybeacceleratedbyafactoroftwoonbothCPUandGPU withunder2.6%increaseoferror.Theoreticalspeedups(reductionofthenumberofmultiplications)areusuallyclosetotheempiricalones.Additionally thememoryrequiredtostorenetworkactivationsissigniÔ¨Åcantlyreducedbystoringonlythenon-perforatedoutputvalues.4.4CombiningaccelerationmethodsApromisingwaytoachievehighspeedupswithlowerrorincreaseistocombinemultipleaccelerationmethods.Forthistosucceed themethodsshouldexploitdifferenttypesofredundancyinthenetwork.Inthissection weverifythatperforationcanbecombinedwiththeinter-channelredundancyeliminationapproachof[28]toachieveimprovedspeedup-errorratios.Wereimplementthelinearasymmetricmethodof[28].Itdecomposesaconvolutionallayerwitha(d√ód√óS√óT)kernel(height-width-inputchannels-outputchannels)intoasequenceoftwolayers (d√ód√óS√óT0)‚Üí(1√ó1√óT0√óT) T0<T.Thesecondlayeristypicallyveryfast sotheoverallspeedupisroughlyTT0.Whendecomposingaperforatedconvolutionallayer wetransfertheperforationmasktotheÔ¨Årstobtainedlayer.WeÔ¨ÅrstapplyperforationtothenetworkandÔ¨Åne-tuneit asintheprevioussection.Then weapplytheinter-channelredundancyeliminationmethodtothisnetwork.Finally weperformthesecondroundofÔ¨Åne-tuningwithamuchlowerlearningrateof1e-9 duetoexplodinggradients.Allthemethodsaretestedatthetheoreticalspeeduplevelof4√ó.Whenthetwomethodsarecombined theaccelerationrateforeachmethodistakentoberoughlyequal.Theresultsarepresentedinthetable7NetworkDeviceSpeedupMult.‚ÜìMem.‚ÜìError‚Üë(%)Tunederror‚Üë(%)NINCPU2.2√ó2.5√ó2.0√ó+1.5+0.43.1√ó4.4√ó3.5√ó+5.5+1.94.2√ó6.6√ó4.4√ó+8.3+2.9GPU2.1√ó3.6√ó3.3√ó+4.5+1.63.0√ó10.1√ó5.7√ó+18.2+5.63.5√ó19.1√ó9.2√ó+37.4+12.4AlexNetCPU2.0√ó2.1√ó1.8√ó+10.7+2.33.0√ó3.5√ó2.6√ó+28.0+6.13.6√ó4.4√ó2.9√ó+60.7+9.9GPU2.0√ó2.0√ó1.7√ó+8.5+2.03.0√ó2.6√ó2.0√ó+16.4+3.24.1√ó3.4√ó2.4√ó+28.1+6.2VGG-16CPU2.0√ó1.8√ó1.5√ó+15.6+1.13.0√ó2.9√ó1.8√ó+54.3+3.74.0√ó4.0√ó2.5√ó+71.6+5.5GPU2.0√ó1.9√ó1.7√ó+23.1+2.53.0√ó2.8√ó2.4√ó+65.0+6.84.0√ó4.7√ó3.4√ó+76.5+7.3Table3:Fullnetworkaccelerationresults.Arrowsindicateincreaseordecreaseinthemetric.Speedupisthewall-clockacceleration.Mult.isareductionofthenumberofmultiplicationsinconvolutionallayers(theoreticalspeedup).Mem.isareductionofmemoryrequiredtostorethenetworkactivations.Tunederroristheerroraftertrainingfromscratch(NIN)orÔ¨Åne-tuning(AlexNet VGG16)oftheacceleratednetwork‚Äôsweights.PerforationAsymm.[28]Mult.‚ÜìMem.‚ÜìError‚Üë(%)Tunederror‚Üë(%)4.0√ó-4.0√ó2.5√ó+71.6+5.5-3.9√ó3.9√ó0.93√ó+6.7+2.01.8√ó2.2√ó4.0√ó1.4√ó+2.9+1.6Table4:AccelerationofVGG-16 4√ótheoreticalspeedup.Firstrowistheproposedmethod thesecondrowisourreimplementationoflinearasymmetricmethodofZhangetal.[28] thethirdrowisthecombinedmethod.PerforationiscomplementarytotheaccelerationmethodofZhangetal.4.Whilethedecompositionmethodoutperformsperforation thecombinedmethodisbetterthanbothofthecomponents.5ConclusionWehavepresentedPerforatedCNNswhichexploitredundancyofintermediaterepresentationsofmodernCNNstoreducetheevaluationtimeandmemoryconsumption.PerforationrequiresonlyaminormodiÔ¨ÅcationoftheconvolutionlayerandobtainsspeedupsclosetotheoreticalonesonbothCPUandGPU.Comparedtothebaselines PerforatedCNNsachievelowererror aremoreÔ¨ÇexibleanddonotchangethearchitectureofaCNN(numberofparametersinthefully-connectedlayersandthesizeoftheintermediaterepresentations).RetainingthearchitectureallowstoeasilypluginPerforatedCNNsintotheexistingcomputervisionpipelinesandonlyperformÔ¨Åne-tuningofthenetwork insteadofcompleteretraining.Additionally perforationcanbecombinedwithaccelerationmethodswhichexploitothertypesofnetworkredundancytoachievefurtherspeedups.Infuture weplantoexploretheconnectionbetweenPerforatedCNNsandvisualattentionbyconsideringinput-dependentperforationmasksthatcanfocusonthesalientpartsoftheinput.Unlikerecentworksonvisualattention[1 11 20]whichconsiderrectangularcropsofanimage PerforatedCNNscanprocessnon-rectangularandevendisjointsalientpartsoftheimagebychoosingappropriateperforationmasksintheconvolutionallayers.Acknowledgments.WewouldliketothankAlexanderKirillovandDmitryKropotovforhelpfuldiscussions andYandexforprovidingcomputationalresourcesforthisproject.ThisworkwassupportedbyRFBRprojectNo.15-31-20596(mol-a-ved)andbyMicrosoft:MoscowStateUniversityJointResearchCenter(RPD1053945).8References[1]J.Ba R.Salakhutdinov R.Grosse andB.Frey ‚ÄúLearningwake-sleeprecurrentattentionmodels ‚ÄùNIPS 2015.[2]T.Chen ‚ÄúMatrixshadowlibrary ‚Äùhttps://github.com/dmlc/mshadow 2015.[3]S.Chetlur C.Woolley P.Vandermersch J.Cohen J.Tran B.Catanzaro andE.Shelhamer ‚ÄúcuDNN:EfÔ¨Åcientprimitivesfordeeplearning ‚ÄùarXiv 2014.[4]M.D.CollinsandP.Kohli ‚ÄúMemoryboundeddeepconvolutionalnetworks ‚ÄùarXiv 2014.[5]M.Courbariaux Y.Bengio andJ.David ‚ÄúLowprecisionarithmeticfordeeplearning ‚ÄùICLR 2015.[6]E.L.Denton W.Zaremba J.Bruna Y.LeCun andR.Fergus ‚ÄúExploitinglinearstructurewithinconvolutionalnetworksforefÔ¨Åcientevaluation ‚ÄùNIPS 2014.[7]B.Graham ‚ÄúFractionalmax-pooling ‚ÄùarXiv 2014.[8]‚Äî‚Äî ‚ÄúSpatially-sparseconvolutionalneuralnetworks ‚ÄùarXiv 2014.[9]S.Gupta A.Agrawal K.Gopalakrishnan andP.Narayanan ‚ÄúDeeplearningwithlimitednumericalprecision ‚ÄùICML 2015.[10]M.Jaderberg A.Vedaldi andA.Zisserman ‚ÄúSpeedingupconvolutionalneuralnetworkswithlowrankexpansions ‚ÄùBMVC 2014.[11]M.Jaderberg K.Simonyan A.Zissermanetal. ‚ÄúSpatialtransformernetworks ‚ÄùNIPS 2015.[12]Y.Jia E.Shelhamer J.Donahue S.Karayev J.Long R.Girshick S.Guadarrama andT.Darrell ‚ÄúCaffe:Convolutionalarchitectureforfastfeatureembedding ‚ÄùACMICM 2014.[13]A.Krizhevsky ‚Äúcuda‚Äìconvnet2 ‚Äùhttps://github.com/akrizhevsky/cuda-convnet2/ 2014.[14]A.Krizhevsky I.Sutskever andG.E.Hinton ‚ÄúImagenetclassiÔ¨Åcationwithdeepconvolutionalneuralnetworks ‚ÄùNIPS 2012.[15]V.LebedevandV.Lempitsky ‚ÄúFastconvnetsusinggroup-wisebraindamage ‚ÄùCVPR 2016.[16]V.Lebedev Y.Ganin M.Rakhuba I.Oseledets andV.Lempitsky ‚ÄúSpeeding-upconvolutionalneuralnetworksusingÔ¨Åne-tunedCP-decomposition ‚ÄùICLR 2015.[17]M.Lin Q.Chen andS.Yan ‚ÄúNetworkinnetwork ‚ÄùICLR 2014.[18]S.Misailovic S.Sidiroglou H.Hoffmann andM.Rinard ‚ÄúQualityofserviceproÔ¨Åling ‚ÄùICSE 2010.[19]S.Misailovic D.M.Roy andM.C.Rinard ‚ÄúProbabilisticallyaccurateprogramtransformations ‚ÄùStaticAnalysis 2011.[20]V.Mnih N.Heess A.Gravesetal. ‚ÄúRecurrentmodelsofvisualattention ‚ÄùNIPS 2014.[21]A.Novikov D.Podoprikhin A.Osokin andD.Vetrov ‚ÄúTensorizingneuralnetworks ‚ÄùNIPS 2015.[22]K.Ovtcharov O.Ruwase J.-Y.Kim J.Fowers K.Strauss andE.S.Chung ‚ÄúAcceleratingdeepconvolu-tionalneuralnetworksusingspecializedhardware ‚ÄùMicrosoftResearchWhitepaper 2015.[23]M.Samadi D.A.Jamshidi J.Lee andS.Mahlke ‚ÄúParaprox:Pattern-basedapproximationfordataparallelapplications ‚ÄùASPLOS 2014.[24]S.Sidiroglou-Douskos S.Misailovic H.Hoffmann andM.Rinard ‚ÄúManagingperformancevs.accuracytrade-offswithloopperforation ‚ÄùACMSIGSOFT 2011.[25]K.SimonyanandA.Zisserman ‚ÄúVerydeepconvolutionalnetworksforlarge-scaleimagerecognition ‚ÄùICLR 2015.[26]A.VedaldiandK.Lenc ‚ÄúMatConvNet‚ÄìconvolutionalneuralnetworksforMATLAB ‚ÄùarXiv 2014.[27]Z.Yang M.Moczulski M.Denil N.deFreitas A.J.Smola L.Song andZ.Wang ‚ÄúDeepfriedconvnets ‚ÄùICCV 2015.[28]X.Zhang J.Zou K.He andJ.Sun ‚ÄúAcceleratingverydeepconvolutionalnetworksforclassiÔ¨Åcationanddetection ‚ÄùarXiv 2015.9,Fajwel Fogel
Alexandre d'Aspremont
Milan Vojnovic
Mikhail Figurnov
Aizhan Ibraimova
Dmitry Vetrov
Pushmeet Kohli
Tobias Pl√∂tz
Stefan Roth