2016,Sorting out typicality with the inverse moment matrix SOS polynomial,We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that  given a collection (a cloud) of data points  the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact  this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications  we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.,Sorting out typicality with the inverse moment matrix

SOS polynomial

Jean-Bernard Lasserre
LAAS-CNRS & IMT
Université de Toulouse
31400 Toulouse  France

lasserre@laas.fr

Edouard Pauwels

IRIT & IMT

Université Toulouse 3 Paul Sabatier

31400 Toulouse  France

edouard.pauwels@irit.fr

Abstract

We study a surprising phenomenon related to the representation of a cloud of data
points using polynomials. We start with the previously unnoticed empirical obser-
vation that  given a collection (a cloud) of data points  the sublevel sets of a certain
distinguished polynomial capture the shape of the cloud very accurately. This
distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner
from the inverse of the empirical moment matrix. In fact  this SOS polynomial is
directly related to orthogonal polynomials and the Christoffel function. This allows
to generalize and interpret extremality properties of orthogonal polynomials and to
provide a mathematical rationale for the observed phenomenon. Among diverse
potential applications  we illustrate the relevance of our results on a network intru-
sion detection task for which we obtain performances similar to existing dedicated
methods reported in the literature.

1

Introduction

Capturing and summarizing the global shape of a cloud of points is at the heart of many data
processing applications such as novelty detection  outlier detection as well as related unsupervised
learning tasks such as clustering and density estimation. One of the main difﬁculties is to account
for potentially complicated shapes in multidimensional spaces  or equivalently to account for non
standard dependence relations between variables. Such relations become critical in applications  for
example in fraud detection where a fraudulent action may be the dishonest combination of several
actions  each of them being reasonable when considered on their own.
Accounting for complicated shapes is also related to computational geometry and nonlinear algebra
applications  for example integral computation [11] and reconstruction of sets from moments data
[6  7  12]. Some of these problems have connections and potential applications in machine learning.
The work presented in this paper brings together ideas from both disciplines  leading to a method
which allows to encode in a simple manner the global shape and spatial concentration of points within
a cloud.
We start with a surprising (and apparently unnoticed) empirical observation. Given a collection of
points  one may build up a distinguished sum-of-squares (SOS) polynomial whose coefﬁcients (or
Gram matrix) is the inverse of the empirical moment matrix (see Section 3). Its degree depends on
how many moments are considered  a choice left to the user. Remarkably its sublevel sets capture
much of the global shape of the cloud as illustrated in Figure 3. This phenomenon is not incidental as
illustrated in many additional examples in Appendix A. To the best of our knowledge  this observation
has remained unnoticed and the purpose of this paper is to report this empirical ﬁnding to the machine
learning community and provide ﬁrst elements toward a mathematical understanding as well as
potential machine learning applications.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

polynomial Qµ d (d = 4). The level set(cid:0)p+d

(cid:1)  which corresponds to the average value of Qµ d  is

Figure 1: Left: 1000 points in R2 and the level sets of the corresponding inverse moment matrix SOS

d

represented in red. Right: 1040 points in R2 with size and color proportional to the value of inverse
moment matrix SOS polynomial Qµ d (d = 8).
The proposed method is based on the computation of the coefﬁcients of a very speciﬁc polynomial
which depends solely on the empirical moments associated with the data points. From a practical
perspective  this can be done via a single pass through the data  or even in an online fashion via
a sequence of efﬁcient Woodbury updates. Furthermore the computational cost of evaluating the
polynomial does not depend on the number of data points which is a crucial difference with existing
nonparametric methods such as nearest neighbors or kernel based methods [3]. On the other hand 
this computation requires the inversion of a matrix whose size depends on the dimension of the
problem (see Section 3). Therefore  the proposed framework is suited for moderate dimensions and
potentially very large number of observations.
In Section 4 we ﬁrst describe an afﬁne invariance result which suggests that the distinguished SOS
polynomial captures very intrinsic properties of clouds of points. In a second step  we provide a
mathematical interpretation that supports our empirical ﬁndings based on connections with orthogonal
polynomials [5]. We propose a generalization of a well known extremality result for orthogonal
univariate polynomials on the real line (or the complex plane) [16  Theorem 3.1.2]. As a consequence 
the distinguished SOS polynomial of interest in this paper is understood as the unique optimal
solution of a convex optimization problem: minimizing an average value over a structured set of
positive polynomials. In addition  we revisit [16  Theorem 3.5.6] about the Christoffel function.
The mathematics behind provide a simple and intuitive explanation for the phenomenon that we
empirically observed.
Finally  in Section 5 we perform numerical experiments on KDD cup network intrusion dataset
[13]. Evaluation of the distinguished SOS polynomial provides a score that we use as a measure of
outlyingness to detect network intrusions (assuming that they correspond to outlier observations).
We refer the reader to [3] for a discussion of available methods for this task. For the sake of a
fair comparison we have reproduced the experiments performed in [18] for the same dataset. We
report results similar to (and sometimes better than) those described in [18] which suggests that the
method is comparable to other dedicated approaches for network intrusion detection  including robust
estimation and Mahalanobis distance [8  10]  mixture models [14] and recurrent neural networks
[18].

2 Multivariate polynomials  moments and sums of squares

Notations: We ﬁx the ambient dimension to be p throughout the text. For example  we will
manipulate vectors in Rp as well as p-variate polynomials with real coefﬁcients. We denote by X a
set of p variables X1  . . .   Xp which we will use in mathematical expressions deﬁning polynomials.
We identify monomials from the canonical basis of p-variate polynomials with their exponents in
Np: we associate to α = (αi)i=1...p ∈ Np the monomial X α := X α1
p which degree is
i=1 αi. We use the expressions <gl and ≤gl to denote the graded lexicographic order 
a well ordering over p-variate monomials. This amounts to  ﬁrst  use the canonical order on the

deg(α) :=(cid:80)p

. . . X αp

1 X α2

2

2

 15 10 30 80 210 460 950 950 950 1850 1850 1850 3570 3570 3570 6860 6860 6860 13810 13810 13810 30670 30670 67380 67380 146370 146370 333340 333340 degree and  second  break ties in monomials with the same degree using the lexicographic order with
X1 = a  X2 = b . . . For example  the monomials in two variables X1  X2  of degree less or equal to
3 listed in this order are given by: 1  X1  X2  X 2
d  the set {α ∈ Np; deg(α) ≤ d} ordered by ≤gl. R[X] denotes the set of p-variate
We denote by Np
polynomials: linear combinations of monomials with real coefﬁcients. The degree of a polynomial is
the highest of the degrees of its monomials with nonzero coefﬁcients1. We use the same notation 
deg(·)  to denote the degree of a polynomial or of an element of Np. For d ∈ N  Rd[X] denotes

the set of p-variate polynomials of degree less or equal to d. We set s(d) =(cid:0)p+d

(cid:1)  the number of

1   X1X2  X 2

1 X2  X1X 2

2 .
2   X 3

2   X 3

1   X 2

d

d

monomials of degree less or equal to d. We will denote by vd(X) the vector of monomials of degree
less or equal to d sorted by ≤gl. We let vd(X) := (X α)α∈Np
∈ Rd[X]s(d). With this notation 
we can write a polynomial P ∈ Rd[X] as follows P (X) = (cid:104)p  vd(X)(cid:105) for some real vector of
∈ Rs(d) ordered using ≤gl. Given x = (xi)i=1...p ∈ Rp  P (x) denotes
coefﬁcients p = (pα)α∈Np
the evaluation of P with the assignments X1 = x1  X2 = x2  . . . Xp = xp. Given a Borel probability
Rp xαdµ(x). Throughout the

measure µ and α ∈ Np  yα(µ) denotes the moment α of µ: yα(µ) =(cid:82)

paper  we will only consider measures of which all moments are ﬁnite.
Moment matrix: Given a Borel probability measure µ on Rp  the moment matrix of µ  Md(µ)  is a
matrix indexed by monomials of degree at most d ordered by ≤gl. For α  β ∈ Np
d  the corresponding
entry in Md(µ) is deﬁned by Md(µ)α β := yα+β(µ)  the moment α + β of µ. When p = 2  letting
yα = yα(µ) for α ∈ N2

4  we have

d

1 X1 X2 X 2

1 X1X2 X 2
2

M2(µ) :

1
X1
X2
X 2
1
X1X2
X 2
2

1
y10
y01
y20
y11
y02

y10
y20
y11
y30
y21
y12

y01
y11
y02
y21
y12
y03

y20
y30
y21
y40
y31
y22

y11
y21
y12
y31
y22
y13

.

y02
y12
y03
y22
y13
y04

polynomial with vector of coefﬁcients p  we have pT Md(µ)p =(cid:82)
we have the identity Md(µ) =(cid:82)
a family of polynomials Pj ∈ Rm[X]  j ∈ J  such that P = (cid:80)

Md(µ) is positive semideﬁnite for all d ∈ N. Indeed  for any p ∈ Rs(d)  let P ∈ Rd[X] be the
Rp P 2(x)dµ(x) ≥ 0. Furthermore 
Rp vd(x)vd(x)T dµ(x) where the integral is understood elementwise.
Sum of squares (SOS): We denote by Σ[X] ⊂ R[X] (resp. Σd[X] ⊂ Rd[X])  the set of polynomi-
als (resp. polynomials of degree at most d) which can be written as a sum of squares of polynomials.
Let P ∈ R2m[X] for some m ∈ N  then P belongs to Σ2m[X] if there exists a ﬁnite J ⊂ N and
j . It is obvious that sum
of squares polynomials are always nonnegative. A further interesting property is that this class of
polynomials is connected with positive semideﬁniteness. Indeed  P belongs to Σ2m[X] if and only if
(1)
As a consequence  every positive semideﬁnite matrix Q ∈ Rs(m)×s(m) deﬁnes a polynomial in
Σ2m[X] by using the representation in (1).

∃Q ∈ Rs(m)×s(m)  Q (cid:23) 0  P (x) = vd(x)T Qvd(x)  ∀x ∈ Rp.

j∈J P 2

3 Empirical observations on the inverse moment matrix SOS polynomial

The inverse moment-matrix SOS polynomial is associated to a measure µ which satisﬁes the following.
Assumption 1 µ is a Borel probability measure on Rp with all its moments ﬁnite and Md(µ) is
positive deﬁnite for a given d ∈ N.
Deﬁnition 1 Let µ  d satisfy Assumption 1. We call the SOS polynomial Qµ d ∈ Σ2d[X] deﬁned by
the application:

x (cid:55)→ Qµ d(x) := vd(x)T Md(µ)−1vd(x) 

(2)
1For the null polynomial  we use the convention that its degree is 0 and it is ≤gl smaller than all other

x ∈ Rp 

monomials.

3

(cid:80)n

the inverse moment-matrix SOS polynomial of degree 2d associated to µ.
Actually  connection to orthogonal polynomials will show that the inverse function x (cid:55)→ Qµ d(x)−1
is called the Christoffel function in the literature [16  5] (see also Section 4).
In the remainder of this section  we focus on the situation when µ corresponds to an empirical
measure over n points in Rp which are ﬁxed. So let x1  . . .   xn ∈ Rp be a ﬁxed set of points and let
i=1 δxi where δx corresponds to the Dirac measure at x. In such a case the polynomial
µ := 1
n
Qµ d in (2) is determined only by the empirical moments up to degree 2d of our collection of points.
Note that we also require that Md(µ) (cid:31) 0. In other words  the points x1  . . .   xn do not belong to
an algebraic set deﬁned by a polynomial of degree less or equal to d. We ﬁrst describe empirical
properties of inverse moment matrix SOS polynomial in this context of empirical measures. A
mathematical intuition and further properties behind these observations are developped in Section 4.

3.1 Sublevel sets

The starting point of our investigations is the following phenomenon which to the best of our
knowledge has remained unnoticed in the literature. For the sake of clarity and simplicity we provide
an illustration in the plane. Consider the following experiment in R2 for a ﬁxed d ∈ N: represent on
the same graphic  the cloud of points {xi}i=1...n and the sublevel sets of SOS polynomial Qµ d in
R2 (equivalently  the superlevel sets of the Christoffel function). This is illustrated in the left panel
of Figure 3. The collection of points consists of 500 simulations of two different Gaussians and the
value of d is 4. The striking feature of this plot is that the level sets capture the global shape of the

cloud of points quite accurately. In particular  the level set {x : Qµ d(x) ≤(cid:0)p+d

(cid:1)} captures most of

the points. We could reproduce very similar observations on different shapes with various number of
points in R2 and degree d (see Appendix A).

d

3.2 Measuring outlyingness

An additional remark in a similar line is that Qµ d tends to take higher values on points which are
isolated from other points. Indeed in the left panel of Figure 3  the value of the polynomial tends to
be smaller on the boundary of the cloud. This extends to situations where the collection of points
correspond to shape with a high density of points with a few additional outliers. We reproduce a
similar experiment on the right panel of Figure 3. In this example  1000 points are sampled close to a
ring shape and 40 additional points are sampled uniformly on a larger square. We do not represent
the sublevel sets of Qµ d here. Instead  the color and shape of the points are taken proportionally to
the value of Qµ d  with d = 8.
First  the results conﬁrm the observation of the previous paragraph  points that fall close to the ring
shape tend to be smaller and points on the boundary of the ring shape are larger. Second  there is a
clear increase in the size of the points that are relatively far away from the ring shape. This highlight
the fact that Qµ d tends to take higher value in less populated areas of the space.

3.3 Relation to maximum likelihood estimation

(cid:80)n

(cid:19)

(cid:18) 1 µT

µ

S

Md(µ) =

If we ﬁx d = 1  we recover the maximum likelihood estimation for the Gaussian  up to a constant
additive factor. To see this  set µ = 1
i . With this notation  we have
i=1 xi and S = 1
n
n
the following block representation of the moment matrix 

i=1 xixT

(cid:80)n
(cid:18) 1 + µT V −1µ −µT V −1

(cid:19)

 

Md(µ)−1 =

−V −1µ

V −1

where V = S − µµT is the empirical covariance matrix and the expression for the inverse is given by
Schur complement. In this case  we have Qµ 1(x) = 1 + (x − µ)T V −1(x − µ) for all x ∈ Rp. We
recognize the quadratic form that appears in the density function of the multivariate Gaussian with
parameters estimated by maximum likelihood. This suggests a connection between the inverse SOS
moment polynomial and maximum likelihood estimation. Unfortunately  this connection is difﬁcult
to generalize for higher values of d and we do not pursue the idea of interpreting the empirical
observations of this section through the prism of maximum likelihood estimation and leave it for
further research. Instead  we propose an alternative view in Section 4.

4

d

3.4 Computational aspects

Recall that s(d) =(cid:0)p+d

(cid:1) is the number of p-variate monomials of degree up to d. The computation

of Qµ d requires O(ns(d)2) operations for the computation of the moment matrix and O(s(d)3)
operations for the matrix inversion. The evaluation of Qµ d requires O(s(d)2) operations.
Estimating the coefﬁcients of Qµ d has a computational cost that depends only linearly in the number
of points n. The cost of evaluating Qµ d is constant with respect to the number of points n. This is
an important contrast with kernel based or distance based methods (such as nearest neighbors and
one class SVM) for density estimation or outlier detection since they usually require at least O(n2)
operations for the evaluation of the model [3]. Moreover  this is well suited for online settings where
inverse moment matrix computation can be done using rank one Woodbury updates [15  Section
2.7.1].
The dependence in the dimension p is of the order of pd for a ﬁxed d. Similarly  the dependence in d
is of the order of dp for a ﬁxed dimension p and the joint dependence is exponential. Furthermore 
Md(µ) has a Hankel structure which is known to produce ill conditioned matrices. This suggests
that the direct computation and evaluation of Qµ d will mostly make sense for moderate dimensions
and degree d. In our experiments  for large d  the evaluation of Qµ d remains quite stable  but the
inversion leads to numerical error for higher values (around 20).

4

Invariance and interpretation through orthogonal polynomials

The purpose of this section is to provide a mathematical rationale that explains the empirical obser-
vations made in Section 3. All the proofs are postponed to Appendix B. We ﬁx a Borel probability
measure µ on Rp which satisﬁes Assumption 1. Note that Md(µ) is always positive deﬁnite if µ
is not supported on the zero set of a polynomial of degree at most d. Under Assumption 1  Md(µ)
induces an inner product on Rs(d) and by extension on Rd[X] (see Section 2). This inner product is
denoted by (cid:104)· ·(cid:105)µ and satisﬁes for any polynomials P  Q ∈ Rd[X] with coefﬁcients p  q ∈ Rs(d) 

(cid:104)P  Q(cid:105)µ := (cid:104)p  Md(µ)q(cid:105)Rs(d) =

P (x)Q(x)dµ(x).

(cid:90)

Rp

We will also use the canonical inner product over Rd[X] which we write (cid:104)P  Q(cid:105)Rd[X] := (cid:104)p  q(cid:105)Rs(d)
for any polynomials P  Q ∈ Rd[X] with coefﬁcients p  q ∈ Rs(d). We will omit the subscripts for
this canonical inner product and use (cid:104)· ·(cid:105) for both products.

4.1 Afﬁne invariance
It is worth noticing that the mapping x (cid:55)→ Qµ d(x) does not depend on the particular choice of vd(X)
as a basis of Rd[X]  any other basis would lead to the same mapping. This leads to the result that
Qµ d captures afﬁne invariant properties of µ.
Lemma 1 Let µ satisfy Assumption 1 and A ∈ Rp×p  b ∈ Rp deﬁne an invertible afﬁne mapping on
Rp  A : x → Ax+b. Then  the push foward measure  deﬁned by ˜µ(S) = µ(A−1(S)) for all Borel sets
S ⊂ Rp  satisﬁes Assumption 1 (with the same d as µ) and for all x ∈ Rp  Qµ d(x) = Q˜µ d(Ax + b).
i=1 δxi as in Section 3. In this case  we
i=1 δAxi+b and Lemma 1 asserts that the level sets of Q˜µ d are simply the images of

Lemma 1 is probably better understood when µ = 1/n(cid:80)n
have ˜µ = 1/n(cid:80)n

those of Qµ d under the afﬁne transformation x (cid:55)→ Ax + b. This is illustrated in Appendix D.

4.2 Connection with orthogonal polynomials
We deﬁne a classical [16  5] family of orthonormal polynomials  {Pα}α∈Np
which satisﬁes for all α ∈ Np

d

d

ordered according to ≤gl

(cid:104)Pα  X β(cid:105) = 0 if α <gl β  (cid:104)Pα  Pα(cid:105)µ = 1  (cid:104)Pα  X β(cid:105)µ = 0 if β <gl α  (cid:104)Pα  X α(cid:105)µ > 0.

(3)
It follows from (3) that (cid:104)Pα  Pβ(cid:105)µ = 0 if α (cid:54)= β. Existence and uniqueness of such a family is
guaranteed by the Gram-Schmidt orthonormalization process following the ≤gl order  and by the

5

positivity of the moment matrix  see for instance [5  Theorem 3.1.11]. There exist determinantal
formulae [9] and more precise description can be made for measures which have additional geometric
properties  see [5] for many examples.
Let Dd(µ) be the lower triangular matrix whose rows are the coefﬁcients of the polynomials Pα
deﬁned in (3) ordered by ≤gl. It can be shown that Dd(µ) = Ld(µ)−T   where Ld(µ) is the Cholesky
factorization of Md(µ). Furthermore  there is a direct relation with the inverse moment matrix as
Md(µ)−1 = Dd(µ)T Dd(µ) [9  Proof of Theorem 3.1]. This has the following consequence.

Lemma 2 Let µ satisfy Assumption 1  then Qµ d = (cid:80)
deﬁned by (3) and(cid:82)

Rp Qµ d(x)dµ(x) = s(d).

α  where the family {Pα}α∈Np
P 2

d

is

α∈Np

d

That is  Qµ d is a very speciﬁc and distinguished SOS polynomial  the sum of squares of the
orthonormal basis elements {Pα}α∈Np
of Rd(X) (w.r.t. µ). Furthermore  the average value of Qµ d
with respect to µ is s(d) which corresponds to the red level set in left panel of Figure 3.

d

4.3 A variational formulation for the inverse moment matrix SOS polynomial
In this section  we show that the family of polynomials {Pα}α∈Np
deﬁned in (3) is the unique
solution (up to a multiplicative constant) of a convex optimization problem over polynomials. This
fact combined with Lemma 2 provides a mathematical rationale for the empirical observations
outlined in Section 3. Consider the following optimization problem.

d

(cid:90)

(cid:88)

Rp

α∈Np

d

min

Qα θα α∈Np

d

1
2

Qα(x)2dµ(x)

(4)

(cid:88)

α∈Np

d

θα = 0 

s.t. qαα ≥ exp(θα)  qαβ = 0  α  β ∈ Np

d  α <gl β 

where Qα(x) =(cid:80)
comment on problem (4). Let P =(cid:80)

β∈Np

d

1
2

Q2

α∈Np

d

(cid:82) P dµ.

qαβxβ is a polynomial and θα is a real variable for each α ∈ Np

d. We ﬁrst
α be the SOS polynomial appearing in the objective
function of (4). The objective of (4) simply involves the average value of P with respect to µ. Let
Sd ⊂ Σd[X] be the set of such SOS polynomials P which have a sum of square decomposition
satisfying the constraints of (4) (for some arbitrary value of the real variables {θα}α∈Np
). With this
notation  problem (4) has the simple formulation minP∈Sd
Based on this formulation  problem (4) can be interpreted as balancing two antagonist targets. On one
hand the minimization of the average value of the SOS polynomial P with respect to µ  on the other
hand the avoidance of the trivial polynomial  enforced by the constraint that P ∈ Sd. The constraint
P ∈ Sd is simple and natural. It ensures that P is a sum of squares of polynomials {Qα}α∈Np
  where
the leading term of each Qα (according to the ordering ≤gl) is qααxα with qαα > 0 (and hence
does not vanish). Inversely  using Cholesky factorization  for any SOS polynomial Q of degree 2d
which coefﬁcient matrix (see equation (1)) is positive deﬁnite  there exists a > 0 such that aQ ∈ Sd.
This suggests that Sd is a quite general class of nonvanishing SOS polynomials. The following result 
which gives a relation between Qµ d and solutions of (4)  uses a generalization of [16  Theorem 3.1.2]
to several orthogonal polynomials of several variables.
√

Theorem 1 : Under Assumption 1  problem (4) is a convex optimization problem with a unique
optimal solution (Q∗
d  for some λ > 0. In particular 
α)2  is (part of) the unique
α∈Np

the distinguished SOS polynomial Qµ d = (cid:80)

α)  which satisﬁes Q∗

λPα  α ∈ Np
α = 1
P 2
λ

α =
α∈Np

(cid:80)

α  θ∗

(Q∗

d

d

d

d

optimal solution of (4).

Theorem 1 states that up to the scaling factor λ  the distinguished SOS polynomial Qµ d is the
unique optimal solution of problem (4). A detailed proof is provided in the Appendix B and
we only sketch the main ideas here. First  it is remarkable that for each ﬁxed α ∈ Np
d (and
again up to a scaling factor) the polynomial Pα is the unique optimal solution of the problem:
. This fact is well-known in the
minQ
univariate case [16  Theorem 3.1.2] and does not seem to have been exploited in the literature  at

(cid:110)(cid:82) Q2dµ : Q ∈ Rd[X]  Q(x) = xα +(cid:80)

β<glα qβ xβ(cid:111)

6

(cid:26)(cid:90)

(cid:27)

that  at an optimal solution  the contribution of(cid:82) (Q∗

least for purposes similar to ours. So intuitively  P 2
α should be as close to 0 as possible on the support
of µ. Problem (4) has similar properties and the constraint on the vector of weights θ enforces
α)2 dµ to the overall sum in the criterion is the
same for all α. Using Lemma 2 yields (up to a multiplicative constant) the polynomial Qµ d. Other
constraints on θ would yield different weighted sum of the squares P 2
α. This will be a subject of
further investigations.
To sum up  Theorem 1 provides a rationale for our observations. Indeed when solving (4)  intuitively 
Qµ d should be close to 0 on average while remaining in a class of nonvanishing SOS polynomials.

4.4 Christoffel function and outlier detection

The following result from [5  Theorem 3.5.6] draws a direct connection between Qµ d and the
Chritoffel function (the right hand side of (5)).
Theorem 2 ([5]) Let Assumption 1 hold and let z ∈ Rp be ﬁxed  arbitrary. Then

Qµ d(z)−1 = min
P∈Rd[X]

Rp

P (x)2 dµ(x) : P (z) = 1

.

(5)

Theorem 2 provides a mathematical rationale for the use of Qµ d for outlier or novelty detection
purposes. Indeed  from Lemma 2 and equation (3)  we have Qµ d ≥ 1 on Rp. Furthermore  the
1 − Qµ d(z)−1 (by Markov’s inequality). Hence  for high values of Qµ d(z)  the sublevel set

solution of the minimization problem in (5) satisﬁes P (z)2 = 1 and µ(cid:0)(cid:8)x ∈ Rp : P (x)2 ≤ 1(cid:9)(cid:1) ≥
(cid:8)x ∈ Rp : P (x)2 ≤ 1(cid:9) contains most of the mass of µ while P (z)2 = 1. An illustration of this

discussion is given in appendix E. Again the result of Theorem 2 does not seem to have been
interpreted for purposes similar to ours.

5 Experiments on network intrusion datasets

For instance  the sub-level sets of Qµ d  and in particular {x ∈ Rp : Qµ d(x) ≤(cid:0)p+d

In addition to having its own mathematical interest  Theorem 1 can be exploited for various purposes.

(cid:1)}  can be used

to encode a cloud of points in a simple and compact form. However in this section we focus on
another potential application in anomaly detection.
Empirical ﬁndings described in Section 3 suggest that the polynomial Qµ d can be used to detect
outliers in a collection of real vectors (with µ the empirical average). This is backed up by the results
presented in Section 4. We illustrate these properties on a real world example. We choose the KDD
cup 99 network intrusion dataset [13] consisting of network connection data  labeled as normal trafﬁc
or network intrusions. We follow [19] and [18] and construct ﬁve datasets consisting of labeled
vectors in R3 with the following properties

d

Dataset

Number of examples
Proportions of attacks

http

567498
0.004

smtp
95156
0.0003

ftp-data
30464
0.023

ftp
4091
0.077

others
5858
0.016

The details on the datasets construction are available in [19  18] and reproduced in Appendix C.
The main idea is to compute an outlyingness score (independant of the label) and compare outliers
predicted by the score and network intrusion labels. The underlying assumption is that network
intrusions correspond to infrequent abnormal behaviors and could be considered as outliers.
We reproduce the same experiment as in [18  Section 5.4] using the value of Qµ d from Deﬁnition 1
as an outlyingness score (with d = 3). The authors of [18] have compared different methods in the
same experimental setting: robust estimation and Mahalanobis distance [8  10]  mixture models [14]
and recurrent neural networks. The results are gathered in [18  Figure 7]. In the left panel of Figure 2
we represent the same performance measure for our approach: we ﬁrst compute the value of Qµ d
for each datapoint and use it as an outlyingness score. We then display the proportion of correctly
identiﬁed outliers  with score above a given threshold  as a function of the proportion of examples
with score above the threshold (for different values of the threshold). The main comments are as
follows.

7

Figure 2: Left: reproduction of the results described in [18] with the evaluation of Qµ d as an
outlyingness score (d = 3). Right: precision-recall curves for different values of d (dataset “others”).
• The inverse moment matrix SOS polynomial does detect network intrusions with varying perfor-
mances on the ﬁve datasets.
• Except for the “ftp-data dataset”  the global shape of these curves are very similar to results reported
in [18  Figure 7] indicating that the proposed approach is comparable to other dedicated methods for
intrusion detection in these four datasets.
In a second experiment  we investigate the effect of changing the value of d on the performances.
We focus on the “others” dataset because it is the most heterogeneous. We adopt a slightly different
measure of performance and use precision recall (see for example [4]) to measure performances
in identifying network intrusions (the higher the curve  the better). We call the area under such
curves the AUPR. The right panel of Figure 2 represents these results. First  the case d = 1  which
corresponds to vanilla Mahalanobis distance as outlined in Section 3.3  gives poor performances.
Second  the global performances rapidly increase with d and then decrease and stabilize.
This suggests that d can be used as a tuning parameter to control the “complexity” of Qµ d. Indeed 
2d is the degree of the polynomial Qµ d and it is expected that more complex models will identify
more diverse classes of examples as outliers. In our case  this means identifying regular trafﬁc as
outliers while it actually does not correspond to intrusions. In general  a good heuristic regarding the
tuning of d is to investigate performances on a well speciﬁed task in a preliminary experiment.

6 Future work
An important question is the asymptotic regime when d → ∞. Current state of knowledge suggests
that  up to a correct scaling  the limit of the Christoffel functions (when known to exist) involves an
edge effect term  related to the support of the measure  and the density of µ with respect to Lebesgue
measure  see for example [2] for the Euclidean ball. It also suggests connections with the notion of
equilibrium measure in potential theory [17  1  7]. Generalization and interpretation of these results
in our context will be investigated in future work.
Even though good approximations are obtained with low degree (at least in dimension 2 or 3)  the
approach involves the inversion of large ill conditioned Hankel matrices which reduces considerably
the applicability for higher degrees and dimensions. A promising research line is to develop approx-
imation procedures and advanced optimization and algebra tools so that the approach could scale
computationally to higher dimensions and degrees.
Finally  we did not touch the question of statistical accuracy. In the context of empirical processes  this
will be very relevant to understand further potential applications in machine learning and reduce the
gap between the abstract orthogonal polynomial theory and practical machine learning applications.

Acknowledgments

This work was partly supported by project ERC-ADG TAMING 666981  ERC-Advanced Grant of
the European Research Council and grant number FA9550-15-1-0500 from the Air Force Ofﬁce of
Scientiﬁc Research  Air Force Material Command.

8

0.00.20.40.60.81.00.00.20.40.60.81.0% top outlyingness score% correctly identified outliersdatasethttpsmtpftp_dataftpothers0.000.250.500.751.000.00.20.40.60.81.0RecallPrecisiond (AUPR)1 (0.08)2 (0.18)3 (0.18)4 (0.16)5 (0.15)6 (0.13)References
[1] R. J. Berman (2009). Bergman kernels for weighted polynomials and weighted equilibrium

measures of Cn . Indiana University Mathematics Journal  58(4):1921–1946.

[2] L. Bos  B. Della Vecchia and G. Mastroianni (1998). On the asymptotics of Christoffel functions
for centrally symmetric weights functions on the ball in Rn. Rendiconti del Circolo Matematico
di Palermo  52:277–290.

[3] V. Chandola  A. Banerjee and V. Kumar (2009). Anomaly detection: A survey. ACM computing

surveys (CSUR) 41(3):15.

[4] J. Davis and M. Goadrich (2006). The relationship between Precision-Recall and ROC curves.

Proceedings of the 23rd international conference on Machine learning (pp. 233-240). ACM.

[5] C.F. Dunkl and Y. Xu (2001). Orthogonal polynomials of several variables. Cambridge

University Press. MR1827871.

[6] G.H Golub  P. Milanfar and J. Varah (1999). A stable numerical method for inverting shape

from moments. SIAM Journal on Scientiﬁc Computating 21(4):1222–1243 (1999).

[7] B. Gustafsson  M. Putinar  E. Saff and N. Stylianopoulos (2009). Bergman polynomials on an
archipelago: estimates  zeros and shape reconstruction. Advances in Mathematics 222(4):1405–
1460.

[8] A.S. Hadi (1994). A modiﬁcation of a method for the detection of outliers in multivariate

samples. Journal of the Royal Statistical Society. Series B (Methodological)  56(2):393-396.

[9] J.W. Helton  J.B. Lasserre and M. Putinar (2008). Measures with zeros in the inverse of their

moment matrix. The Annals of Probability  36(4):1453-1471.

[10] E.M. Knorr  R.T. Ng and R.H.Zamar (2001). Robust space transformations for distance-based
operations. Proceedings of the international conference on Knowledge discovery and data
mining (pp. 126-135). ACM.

[11] J.B. Lasserre (2015). Level Sets and NonGaussian Integrals of Positively Homogeneous

Functions. International Game Theory Review  17(01):1540001.

[12] J.B. Lasserre and M.Putinar (2015). Algebraic-exponential Data Recovery from Moments.

Discrete & Computational Geometry  54(4):993-1012.

[13] M. Lichman (2013). UCI Machine Learning Repository  http://archive.ics.uci.edu/ml

University of California  Irvine  School of Information and Computer Sciences.

[14] J.J. Oliver  R.A.Baxter and C.S. Wallace (1996). Unsupervised learning using MML. Proceed-

ings of the International Conference on Machine Learning (pp. 364-372).

[15] W. H. Press  S. A. Teukolsky  W. T. Vetterling and B. P. Flannery (2007). Numerical Recipes:

The Art of Scientiﬁc. Computing (3rd Edition). Cambridge University Press.

[16] G. Szegö (1974). Orthogonal polynomials. In Colloquium publications  AMS  (23)  fourth

edition.

[17] V. Totik (2000). Asymptotics for Christoffel functions for general measures on the real line.

Journal d’Analyse Mathématique  81(1):283-303.

[18] G. Williams  R. Baxter  H. He  S. Hawkins and L. Gu (2002). A Comparative Study of RNN
for Outlier Detection in Data Mining. IEEE International Conference on Data Mining (p. 709).
IEEE Computer Society.

[19] K. Yamanishi  J.I. Takeuchi  G. Williams and P. Milne (2004). On-line unsupervised outlier de-
tection using ﬁnite mixtures with discounting learning algorithms. Data Mining and Knowledge
Discovery  8(3):275-300.

9

,Edouard Pauwels
Jean Lasserre
Di He
Hanqing Lu
Yingce Xia
Tao Qin
Liwei Wang
Tie-Yan Liu
Karl Ridgeway
Michael Mozer