2014,Generalized Dantzig Selector: Application to the k-support norm,We propose a Generalized Dantzig Selector (GDS) for linear models  in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator  a flexible inexact ADMM framework is designed for solving GDS. Thereafter  non-asymptotic high-probability bounds are established on the estimation error  which rely on Gaussian widths of the unit norm ball and the error set. Further  we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis  we provide upper bounds for the Gaussian widths needed in the GDS analysis  yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.,Generalized Dantzig Selector:

Application to the k-support norm

Soumyadeep Chatterjee∗

Sheng Chen∗

Arindam Banerjee

Dept. of Computer Science & Engg.
University of Minnesota  Twin Cities

{chatter shengc banerjee}@cs.umn.edu

Abstract

We propose a Generalized Dantzig Selector (GDS) for linear models  in which any
norm encoding the parameter structure can be leveraged for estimation. We inves-
tigate both computational and statistical aspects of the GDS. Based on conjugate
proximal operator  a ﬂexible inexact ADMM framework is designed for solving
GDS. Thereafter  non-asymptotic high-probability bounds are established on the
estimation error  which rely on Gaussian widths of the unit norm ball and the error
set. Further  we consider a non-trivial example of the GDS using k-support norm.
We derive an efﬁcient method to compute the proximal operator for k-support
norm since existing methods are inapplicable in this setting. For statistical analy-
sis  we provide upper bounds for the Gaussian widths needed in the GDS analysis 
yielding the ﬁrst statistical recovery guarantee for estimation with the k-support
norm. The experimental results conﬁrm our theoretical analysis.

1

Introduction

The Dantzig Selector (DS) [3  5] provides an alternative to regularized regression approaches such as
Lasso [19  22] for sparse estimation. While DS does not consider a regularized maximum likelihood
approach  [3] has established clear similarities between the estimates from DS and Lasso. While
norm regularized regression approaches have been generalized to more general norms [14  2]  the
literature on DS has primarily focused on the sparse L1 norm case  with a few notable exceptions
which have considered extensions to sparse group-structured norms [11].
In this paper  we consider linear models of the form y = Xθ∗ + w  where y ∈ Rn is a set of
observations  X ∈ Rn×p is a design matrix with i.i.d. standard Gaussian entries  and w ∈ Rn
is i.i.d. standard Gaussian noise. For any given norm R(·)  the parameter θ∗ is assumed to be
structured in terms of having a low value of R(θ∗). For this setting  we propose the following
Generalized Dantzig Selector (GDS) for parameter estimation:

s.t. R∗(cid:0)XT (y − Xθ)(cid:1)

ˆθ = argmin

θ∈Rp R(θ)

≤ λp  

(1)

where R∗(·) is the dual norm of R(·)  and λp is a suitable constant. If R(·) is the L1 norm  (1)
reduces to standard DS [5]. A key novel aspect of GDS is that the constraint is in terms of the dual
norm R∗(·) of the original structure inducing norm R(·). It is instructive to contrast GDS with
the recently proposed atomic norm based estimation framework [6] which  unlike GDS  considers
constraints based on the L2 norm of the error (cid:107)y − Xθ(cid:107)2.
In this paper  we consider both computational and statistical aspects of the GDS. For the L1-norm
Dantzig selector  [5] proposed a primal-dual interior point method since the optimization is a linear
program. DASSO and its generalization proposed in [10  9] focused on homotopy methods  which

∗Both authors contributed equally.

1

provide a piecewise linear solution path through a sequential simplex-like algorithm. However  none
of the algorithms above can be immediately extended to our general formulation. In recent work 
the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1-norm Dantzig
selection problem [12  21]  and the linearized version in [21] proved to be efﬁcient. Motivated
by such results for DS  we propose a general inexact ADMM [20] framework for GDS where the
primal update steps  interestingly  turn out respectively to be proximal updates involving R(θ) and
its convex conjugate  the indicator of R∗(x) ≤ λp. As a result  by Moreau decomposition  it sufﬁces
to develop efﬁcient proximal update for either R(θ) or its conjugate. On the statistical side  we
establish non-asymptotic high-probability bounds on the estimation error (cid:107) ˆθ − θ∗(cid:107)2. Interestingly 
the bound depends on the Gaussian width of the unit norm ball of R(·) as well as the Gaussian width
of intersection of error cone and unit sphere [6  16].
As a non-trivial example of the GDS framework  we consider estimation using the recently proposed
k-support norm [1  13]. We show that proximal operators for k-support norm can be efﬁciently
computed in O(p log p + log k log(p − k))  and hence the estimation can be done efﬁciently. Note
that existing work [1  13] on k-support norm has focused on the proximal operator for the square of
the k-support norm  which is not directly applicable in our setting. On the statistical side  we provide
upper bounds for the Gaussian widths of the unit norm ball and the error cone as needed in the GDS
framework  yielding the ﬁrst statistical recovery guarantee for estimation with the k-support norm.
The rest of the paper is organized as follows: We establish general optimization and statistical
recovery results for GDS for any norm in Section 2. In Section 3  we present efﬁcient algorithms
and estimation error bounds for the k-support norm. We present experimental results in Section 4
and conclude in Section 5. All technical analysis and proofs can be found in [7].

2 General Optimization and Statistical Recovery Guarantees

The problem in (1) is a convex program  and a suitable choice of λp ensures that the feasible set
is not empty. We start the section with an inexact ADMM framework for solving problems of the
form (1)  and then present bounds on the estimation error establishing statistical consistency of GDS.

2.1 General Optimization Framework using Inexact ADMM

For convenience  we temporarily drop the subscript p of λp. We let A = XT X  b = XT y  and
deﬁne the set Cλ = {v : R∗(v) ≤ λ}. The optimization problem is equivalent to

θ v R(θ)
min

s.t. b − Aθ = v  v ∈ Cλ .

(2)

Due to the nonsmoothness of both R and R∗  solving (2) can be quite challenging and a generally
applicable algorithm is Alternating Direction Method of Multipliers (ADMM) [4]. The augmented
Lagrangian function for (2) is given as

LR(θ  v  z) = R(θ) + (cid:104)z  Aθ + v − b(cid:105) +

ρ
2||Aθ + v − b||2
2  

(3)

where z is the Lagrange multiplier and ρ controls the penalty introduced by the quadratic term. The
iterative updates of the variables (θ  v  z) in standard ADMM are given by

θ

LR(θ  vk  zk)  
v∈Cλ LR(θk+1  v  zk)  

θk+1 ← argmin
vk+1 ← argmin
zk+1 ← zk + ρ(Aθk+1 + vk+1 − b) .

(6)
Note that update (4) amounts to a norm regularized least squares problem for θ  which can be
computationally expensive. Thus we use an inexact update for θ instead  which can alleviate the
computational cost and lead to a quite simple algorithm. Inspired by [21  20]  we consider a simpler
subproblem for the θ-update which minimizes

(4)

(5)

(7)

(cid:101)Lk

R(θ  vk  zk) = R(θ) + (cid:104)zk  Aθ + vk − b(cid:105) +

2(cid:10)θ − θk  AT (Aθk + vk − b)(cid:11) +

ρ
2

(cid:16)(cid:13)(cid:13)Aθk + vk − b(cid:13)(cid:13)2
(cid:17)
(cid:13)(cid:13)θ − θk(cid:13)(cid:13)2

µ
2

2

 

2+

2

Algorithm 1 ADMM for Generalized Dantzig Selector
Input: A = XT X  b = XT y  ρ  µ
Output: Optimal ˆθ of (1)
1: Initialize (θ  v  z)
2: while not converged do
3:

(cid:1)
θk+1 ← prox 2R
vk+1 ← proxICλ
zk+1 ← zk + ρ(Aθk+1 + vk+1 − b)

4:
5:
6: end while

µ AT (Aθk + vk − b + zk

ρµ

ρ )(cid:1)

ρ

(cid:0)θk − 2
(cid:0)b − Aθk+1 − zk
(cid:101)Lk
(cid:13)(cid:13)(cid:13)θ −

R(θ  vk  zk)
1
2

ρµ

+

R(θ  vk  zk) can be viewed as an approximation of

θk+1 ← argmin

(cid:101)Lk
(cid:26) 2R(θ)

where µ is a user-deﬁned parameter.
LR(θ  vk  zk) with the quadratic term linearized at θk. Then the update (4) is replaced by
(cid:27)
)(cid:1)(cid:13)(cid:13)(cid:13)2
)(cid:13)(cid:13)2

Similarly the update of v in (5) can be recast as

(cid:13)(cid:13)v − (b − Aθk+1 −

AT (Aθk + vk − b +

v∈Cλ LR(θk+1  v  zk) = argmin

vk+1 ← argmin

(cid:0)θk −

= argmin

zk
ρ

zk
ρ

v∈Cλ

2
µ

2 .

θ

θ

.

2

1
2

(8)

(9)

In fact  the updates of both θ and v are to compute certain proximal operators [15]. In general  the
proximal operator proxh(·) of a closed proper convex function h : Rp → R ∪ {+∞} is deﬁned as

(cid:110) 1

(cid:111)

2 + h(w)

.

2(cid:107)w − x(cid:107)2
Hence it is easy to see that (8) and (9) correspond to prox 2R
ICλ(·) is the indicator function of set Cλ given by

proxh(x) = argmin
w∈Rp

ρµ

(cid:26) 0

ICλ(x) =

if x ∈ Cλ

+∞ if otherwise

.

(·) and proxICλ

(·)  respectively  where

In Algorithm 1  we provide our general ADMM for the GDS. For the ADMM to work  we need two
subroutines that can efﬁciently compute the proximal operators for the functions in Line 3 and 4
respectively. The simplicity of the proposed approach stems from the fact that we in fact need only
one subroutine  for any one of the functions  since the functions are conjugates of each other.

Proposition 1 Given β > 0 and a norm R(·)  the two functions  f (x) = βR(x) and g(x) = ICβ (x)
are convex conjugate to each other  thus giving the following identity 

x = proxf (x) + proxg(x) .

(10)

Proof: The Proposition 1 simply follows from the deﬁnition of convex conjugate and dual norm 
and (10) is just Moreau decomposition provided in [15].

The decomposition enables conversion of the two types of proximal operator to each other at neg-
ligible cost (i.e.  vector subtraction). Thus we have the ﬂexibility in Algorithm 1 to focus on the
proximal operator that is efﬁciently computable  and the other can be simply obtained through (10).
Remark on convergence: Note that Algorithm 1 is a special case of inexact Bregman ADMM pro-
posed in [20]  which matches the case of linearizing quadratic penalty term by using Bϕ(cid:48)
(θ  θk) =
2 to be larger than
1
2(cid:107)θ − θk(cid:107)2
the spectral radius of AT A  and the convergence rate is O(1/T ) according to Theorem 2 in [20].

2 as Bregman divergence. In order to converge  the algorithm requires µ

θ

3

2.2 Statistical Recovery for Generalized Dantzig Selector

Our goal is to provide non-asymptotic bounds on (cid:107) ˆθ − θ∗(cid:107)2 between the true parameter θ∗ and
the minimizer ˆθ of (1). Let the error vector be deﬁned as ˆ∆ = ˆθ − θ∗. For any set Ω ⊆ Rp  we
would measure the size of this set using its Gaussian width [17  6]  which is deﬁned as ω(Ω) =
Eg [supz∈Ω(cid:104)g  z(cid:105)]   where g is a vector of i.i.d. standard Gaussian entries. We also consider the
error cone TR(θ∗)  generated by the set of possible error vectors ∆ and containing ˆ∆  deﬁned as

(11)
Note that this set contains a restricted set of directions and does not in general span the entire space
of Rp. With these deﬁnitions  we obtain our main result.

TR(θ∗) := cone{∆ ∈ Rp : R(θ∗ + ∆) ≤ R(θ∗)} .

Theorem 1 Suppose the design matrix X consists of i.i.d. Gaussian entries with zero mean variance
1  and we solve the optimization problem (1) with

λp ≥ cE(cid:2)

R∗(XT w)(cid:3) .

Then  with probability at least (1 − η1 exp(−η2n))  we have
4cΨRω(ΩR)
κL√n

(13)
where ω(TR(θ∗)∩ Sp−1) is the Gaussian width of the intersection of TR(θ∗) and the unit spherical
shell Sp−1  ω(ΩR) is the Gaussian width of the unit norm ball  κL > 0 is the gain given by

(cid:107) ˆθ − θ∗(cid:107)2 ≤

 

(14)
ΨR = sup∆∈TR R(∆)/(cid:107)∆(cid:107)2 is a norm compatibility factor  (cid:96)n is the expected length of a length n
i.i.d. standard Gaussian vector with

n√n+1 < (cid:96)n < √n  and c > 1  η1  η2 > 0 are constants.

κL =

 

(cid:0)(cid:96)n − ω(TR(θ∗) ∩ Sp−1)(cid:1)2

1
n

Remark: The choice of λp is also intimately connected to the notion of Gaussian width. Note that
= z is an i.i.d. standard Gaussian vector for any w. Therefore the right hand side of (12)
XT w
(cid:107)w(cid:107)2
can be written as

E(cid:2)
R∗(XT w)(cid:3) = E

(cid:20)
R∗(XT w
(cid:107)w(cid:107)2

(cid:21)
≤ √n · ω ({u : R(u) ≤ 1})  

)(cid:107)w(cid:107)2

= E

(cid:34)
(cid:35)
u: R(u)≤1(cid:104)u  z(cid:105)

sup

E [(cid:107)w(cid:107)2]

which is the Gaussian width of the unit ball of the norm R(·) scaled up by a factor of √n.
Example: L1-norm Dantzig Selector When R(·) is chosen to be L1 norm  the dual norm is the
L∞ norm  and (1) is reduced to the standard DS  given by

(12)

(15)

(16)

s.t. (cid:107)XT (y − Xθ)(cid:107)∞ ≤ λ .

ˆθ = argmin

θ∈Rp (cid:107)θ(cid:107)1

(cid:2)proxβ(cid:107)·(cid:107)1(x)(cid:3)
(cid:0)θk −

We know that proxβ(cid:107)·(cid:107)1(·) is given by the elementwise soft-thresholding operation

Based on Proposition 1  the ADMM updates in Algorithm 1 can be instantiated as

i = sign(xi) · max(0 |xi| − β) .

AT (Aθk + vk − u +

ρµ

θk+1 ← prox 2(cid:107)·(cid:107)1
vk+1 ← (u − Aθk+1 −
zk+1 ← zk + ρ(Aθk+1 + vk+1 − u)  

) − proxλ(cid:107)·(cid:107)1

2
µ
zk
ρ

)(cid:1)  
(cid:0)u − Aθk+1 −

zk
ρ

(cid:1)  

zk
ρ

where the update of v leverages the decomposition (10). Similar updates were used in [21] for
L1-norm Dantzig selector.

4

Sp−1) is upper bounded as ω(TL1(θ∗)∩Sp−1)2 ≤ 2s log(cid:0) p
R∗(XT w)(cid:3) =
(cid:1)+ 5
4 s. Also note that E(cid:2)
For statistical recovery  we assume that θ∗ is s-sparse  i.e.  contains s non-zero entries  and that
(cid:107)θ∗(cid:107)2 = 1  so that (cid:107)θ∗(cid:107)1 ≤ s. It was shown in [6] that the Gaussian width of the set (TL1 (θ∗) ∩
E[(cid:107)w(cid:107)2]E[(cid:107)g(cid:107)∞] ≤ √n log p  where g is a vector of i.i.d. standard Gaussian entries [5]. Further 
(cid:32)(cid:114)
[14] has shown that ΨR = √s. Therefore  if we solve (15) with λp = c√n log p  then

(cid:33)

s

n
with high probability  which agrees with known results for DS [3  5].

(cid:107) ˆθ − θ∗(cid:107)2 ≤ 4c

√s log p
κL√n

= O

s log p

(17)

3 Dantzig Selection with k-support norm
We ﬁrst introduce some notations. Given any θ ∈ Rp  let |θ| denote its absolute-valued counterpart
and θ↓ denote the permutation of θ with its elements arranged in decreasing order. In previous
work [1  13]  the k-support norm has been deﬁned as

(cid:107)θ(cid:107)sp

k = min

vI = θ

(18)

where G(k) denotes the set of subsets of {1  . . .   p} of cardinality at most k. The unit ball of this
norm is the set Ck = conv{θ ∈ Rp : (cid:107)θ(cid:107)0 ≤ k (cid:107)θ(cid:107)2 ≤ 1} . The dual norm of the k-support norm
is given by

  

(cid:88)

 (cid:88)
I∈G(k) (cid:107)vI(cid:107)2 : supp(vI ) ⊆ I 
(cid:32) k(cid:88)
(cid:110)
(cid:107)θG(cid:107)2 : G ∈ G(k)(cid:111)

=

I∈G(k)

|θ|↓2

i

i=1

(cid:33) 1

2

(cid:107)θ(cid:107)sp∗

k = max

.

(19)

(20)

Note that k = 1 gives the L1 norm and its dual norm is L∞ norm. The k-support norm was
proposed in order to overcome some of the empirical shortcomings of the elastic net [23] and the
(group)-sparse regularizers. It was shown in [1] to behave similarly as the elastic net in the sense
that the unit norm ball of the k-support norm is within a constant factor of √2 of the unit elastic net
ball. Although multiple papers have reported good empirical performance of the k-support norm on
selecting correlated features  where L1 regularization fails  there exists no statistical analysis of the
k-support norm. Besides  current computational methods consider square of k-support norm in their
formulation  which might fail to work out in certain cases.
In the rest of this section  we focus on GDS with R(θ) = (cid:107)θ(cid:107)sp

ˆθ = argmin

θ∈Rp (cid:107)θ(cid:107)sp

k

s.t.

k given as
(cid:107)XT (y − Xθ)(cid:107)sp∗

k ≤ λp .

For the indicator function ICλ(·) of the dual norm  we present a fast algorithm for computing its
proximal operator by exploiting the structure of its solution  which can be directly plugged in Al-
gorithm 1 to solve (20). Further  we prove statistical recovery bounds for k-support norm Dantzig
selection  which hold even for a high-dimensional scenario  where n < p.

3.1 Computation of Proximal Operator

k

(·) for (cid:107) · (cid:107)sp∗
In order to solve (20)  either proxλ(cid:107)·(cid:107)sp
should be efﬁciently com-
(·) or proxICλ
putable. Existing methods [1  13] are inapplicable to our scenario since they compute the proximal
(·) cannot be directly obtained. In Theo-
operator for squared k-support norm  from which proxICλ
rem 2  we show that proxICλ
(·) can be efﬁciently computed  and thus Algorithm 1 is applicable.
deﬁne Asr =(cid:80)r
Theorem 2 Given λ > 0 and x ∈ Rp  if (cid:107)x(cid:107)sp∗
k > λ 
i=1(|x|↓i )2  in which 0 ≤ s < k and k ≤ r ≤ p  and construct

k ≤ λ  then w∗ = proxICλ

(x) = x. If (cid:107)x(cid:107)sp∗

k

the nonlinear equation of β 

i=s+1 |x|↓i   Bs =(cid:80)s
(cid:20)

(cid:21)2

(k − s)A2

sr

− λ2(1 + β)2 + Bs = 0 .

(21)

1 + β

r − s + (k − s)β
5

Let βsr be given by

βsr =

Then the proximal operator w∗ = proxICλ
1+βs∗ r∗ |x|↓i

1

(x) is given by

0

(cid:26) nonnegative root of (21)
(cid:113) λ2−Bs∗

k−s∗

As∗ r∗

r∗−s∗+(k−s∗)βs∗ r∗
|x|↓i



if s > 0 and the root exists
otherwise

if 1 ≤ i ≤ s∗
if s∗ < i ≤ r∗ and βs∗r∗ = 0
if s∗ < i ≤ r∗ and βs∗r∗ > 0
if r∗ < i ≤ p

|w∗|↓i =

.

 

(22)

(23)

where the indices s∗ and r∗ with computed |w∗|↓ satisfy the following two inequalities:

(24)
(25)
There might be multiple pairs of (s  r) satisfying the inequalities (24)-(25)  and we choose the pair
with the smallest (cid:107)|x|↓ − |w|↓(cid:107)2. Finally  w∗ is obtained by sign-changing and reordering |w∗|↓ to
conform to x.

|x|↓r∗+1 ≤ |w∗|↓k < |x|↓r∗ .

|w∗|↓s∗ > |w∗|↓k  

Remark: The nonlinear equation (21) is quartic  for which we can use general formula to get all the
roots [18]. In addition  if it exists  the nonnegative root is unique  as shown in the proof [7].
Theorem 2 indicates that computing proxICλ
(·) requires sorting of entries in |x| and a two-
dimensional search of s∗ and r∗. Hence the total time complexity is O(p log p + k(p − k)). How-
ever  a more careful observation can particularly reduce the search complexity from O(k(p − k)) to
O(log k log(p − k))  which is motivated by Theorem 3.
Theorem 3 In search of (s∗  r∗) deﬁned in Theorem 2  there can be only one ˜r for a given candidate
˜s of s∗  such that the inequality (25) is satisﬁed. Moreover if such ˜r exists  then for any r < ˜r  the
associated | ˜w|↓k violates the ﬁrst part of (25)  and for r > ˜r  | ˜w|↓k violates the second part of (25).
On the other hand  based on the ˜r  we have following assertion of s∗ 

if ˜r does not exist
if ˜r exists and corresponding | ˜w|↓k satisﬁes (24)
if ˜r exists but corresponding | ˜w|↓k violates (24)

.

(26)

 > ˜s

≥ ˜s
< ˜s

s∗

Based on Theorem 3  the accelerated search procedure for ﬁnding (s∗  r∗) is to execute a two-
dimensional binary search  and Algorithm 2 gives the details. Therefore the total time complexity
becomes O(p log p + log k log(p − k)). Compared with previous proximal operators for squared
k-support norm  this complexity is better than that in [1]  and roughly the same as the one in [13].

3.2 Statistical Recovery Guarantees for k-support norm

The analysis of the generalized Dantzig Selector for k-support norm consists of addressing two key
challenges. First  note that Theorem 1 requires an appropriate choice of λp. Second  one needs
to compute the Gaussian width of the subset of the error set TR(θ∗) ∩ Sp−1. For the k-support
norm  we can get upper bounds to both of these quantities. We start by deﬁning some notation. Let
G∗ ⊆ G(k) be the set of groups intersecting with the support of θ∗  and let S be the union of groups
in G∗  such that s = |S|. Then  we have the following bounds which are used for choosing λp  and
bounding the Gaussian width.
Theorem 4 For the k-support norm Generalized Dantzig Selection problem (20)  we obtain For the
k-support norm Generalized Dantzig Selection problem (20)  we obtain

R∗(XT w)(cid:3)
E(cid:2)
ω(TA(θ∗) ∩ Sp−1)2 ≤

(cid:18)(cid:114)
≤ √n
(cid:18)(cid:114)

2k log

(cid:16)

2k log

(cid:16) pe

(cid:17)

k

(cid:19)

+ √k
(cid:109)
(cid:108) s

k

+ 2

(cid:17)

+ √k

(cid:19)2

(cid:108) s

(cid:109)

k

·

+ s .

(27)

(28)

p − k −

6

(·) of (cid:107) · (cid:107)sp∗

k

(x)

k ≤ λ then

Algorithm 2 Algorithm for computing proxICλ
Input: x  k  λ
Output: w∗ = proxICλ
1: if (cid:107)x(cid:107)sp∗
2: w∗ := x
3: else
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end if

else if ˜r exists and (24) is satisﬁed then

w∗ := ˜w  l := ˜s + 1

u := ˜s − 1

end while

l := ˜s + 1

end if

else if ˜r exists but (24) is not satisﬁed then

l := 0  u := k − 1  and sort |x| to get |x|↓
while l ≤ u do
˜s := (cid:98)(l + u)/2(cid:99)  and binary search for ˜r that satisﬁes (25) and compute ˜w based on (23)
if ˜r does not exist then

Our analysis technique for these bounds follows [16]. Similar results were obtained in [8] in the
context of calculating norms of Gaussian vectors  and our bounds are of the same order as those
of [8]. Choosing λp = √n
  and under the assumptions of Theorem 1  we
obtain the following result on the error bound for the minimizer of (20).

2k log(cid:0) pe

(cid:1) + √k

k

(cid:16)(cid:113)

(cid:17)

Corollary 1 Suppose that we obtain i.i.d. Gaussian measurements X  and the noise w is i.i.d.
N (0  1). If we solve (20) with λp chosen as above. Then  with high probability  we obtain

(cid:17)

(cid:1) + √k

(cid:16)(cid:113)
2k log(cid:0) pe
κL√n

k

4cΨR

(cid:107) ˆθ − θ∗(cid:107)2 ≤

(29)

Remark The error bound provides a natural interpretation for the two special cases of the k-support
norm  viz. k = 1 and k = p. First  for k = 1 the k-support norm is exactly the same as the L1 norm 
  the same as known results of DS  and shown in
and the error bound obtained will be O
Section 2.2. Second  for k = p  the k-support norm is equal to the L2 norm  and the error cone (11)

is then simply a half space (there is no structural constraint) and the error bound scales as O(cid:0)(cid:112) p

(cid:1).

n

(cid:18)(cid:113) s log p

(cid:19)

n

4 Experimental Results
On the optimization side  we focus on the efﬁciency of different proximal operators related to k-
support norm. On the statistical side  we concentrate on the behavior and performance of GDS with
k-support norm. All experiments are implemented in MATLAB.

4.1 Efﬁciency of Proximal Operator
(·) in The-
We tested four proximal operators related to k-support norm  which are normal proxICλ
k )2(·) in [1]  and prox λ
orem 2 and the accelerated one in Theorem 3  prox 1
(·) in [13].
2β ((cid:107)·(cid:107)sp
2 (cid:107)·(cid:107)2
The dimension p of vector varied from 1000 to 10000  and the ratio p/k = {200  100  50  20}. As
(·) is considerable compared with the
illustrated in Figure 1  the speedup of accelerated proxICλ
normal one and prox 1
(cid:125)
(cid:124)
(cid:125)

k )2 (·). It is also slightly better than the prox λ
(cid:125)
(cid:123)(cid:122)

4.2 Statistical Recovery on Synthetic Data
Data generation We ﬁxed p = 600  and θ∗ = (10  . . .   10

(·).
(cid:123)(cid:122)

  10  . . .   10

  10  . . .   10

  0  0  . . .   0

2β ((cid:107)·(cid:107)sp

(cid:123)(cid:122)

(cid:123)(cid:122)

2 (cid:107)·(cid:107)2

(cid:124)

(cid:125)

(cid:124)

(cid:124)

)

Θ

Θ

throughout the experiment  in which nonzero entries were divided equally into three groups. The
design matrix X were generated from a normal distribution such that the entries in the same group

10

10

10

570

7

(·) in Theorem 2  Square: prox 1

k )2 (·)
Figure 1: Efﬁciency of proximal operators. Diamond: proxICλ
2β ((cid:107)·(cid:107)sp
(·)
(·) in [13]  Upward-pointing triangle: accelerated proxICλ
in [1]  Downward-pointing triangle: prox λ
in Theorem 3. For each (p  k)  200 vectors are randomly generated for testing. Time is measured in seconds.
have the same mean sampled from N (0  1). X was normalized afterwards. The response vector y
was given by y = Xθ∗ + 0.01 × N (0  1). The number of samples n is speciﬁed later.
ROC curves with different k We ﬁxed n = 400 to obtain the ROC plot for k = {1  10  50} as
shown in Figure 2(a). λp ranged from 10−2 to 103. Small k gets better ROC curve.

2 (cid:107)·(cid:107)2

Θ

(a) ROC curves

(b) L2 error vs. n

(c) L2 error vs. k

Figure 2: (a) The true positive rate reaches 1 quite early for k = 1  10. When k = 50  the ROC gets worse
due to the strong smoothing effect introduced by large k. (b) For each k  the L2 error is large when the sample
is inadequate. As n increases  the error decreases dramatically for k = 1  10 and becomes stable afterwards 
while the decrease is not that signiﬁcant for k = 50 and the error remains relatively large. (c) Both mean and
standard deviation of L2 error are decreasing as k increases until it exceeds the number of nonzero entries in
θ∗  and then the error goes up for larger k.
L2 error vs. n We investigated how the L2 error (cid:107) ˆθ − θ∗(cid:107)2 of Dantzig selector changes as the
number of samples increases  where k = {1  10  50} and n = {30  60  90  . . .   300}. k = 1  10 give
small errors when n is sufﬁciently large.
L2 error vs. k We also looked at the L2 error with different k. We again ﬁxed n = 400 and
varied k from 1 to 39. For each k  we repeated the experiment 100 times  and obtained the mean and
standard deviation plot in Figure 2(c). The result shows that the k-support-norm GDS with suitable
k outperforms the L1-norm DS (i.e. k = 1) when correlated variables present in data.

5 Conclusions
In this paper  we introduced the GDS  which generalizes the standard L1-norm Dantzig selector to
estimation with any norm  such that structural information encoded in the norm can be efﬁciently
exploited. A ﬂexible framework based on inexact ADMM is proposed for solving the GDS  which
only requires one of conjugate proximal operators to be efﬁciently solved. Further  we provide a
uniﬁed statistical analysis framework for the GDS  which utilizes Gaussian widths of certain re-
stricted sets for proving consistency. In the non-trivial example of k-support norm  we showed that
the proximal operators used in the inexact ADMM can be computed more efﬁciently compared to
previously proposed variants. Our statistical analysis for the k-support norm provides the ﬁrst result
of consistency of this structured norm. Further  experimental results provided sound support to the
theoretical development in the paper.
Acknowledgements
The research was supported by NSF grants IIS-1447566  IIS-1422557  CCF-1451986  CNS-
1314560  IIS-0953274  IIS-1029711  and by NASA grant NNX12AQ39A.

8

500010000−3−2−101plog(time)p / k = 200500010000−3−2−101plog(time)p / k = 100500010000−3−2−101plog(time)p / k = 50500010000−3−2−101plog(time)p / k = 2000.20.40.60.8100.10.20.30.40.50.60.70.80.91FPRTPR k = 1k = 10k = 500501001502002503000102030405060n(cid:2)2error:(cid:2)ˆθ−θ∗(cid:2)2 k = 1k = 10k = 50051015202530354000.511.522.5k(cid:2)2error:(cid:2)ˆθ−θ∗(cid:2)2 Mean of errorReferences

[1] Andreas Argyriou  Rina Foygel  and Nathan Srebro. Sparse prediction with the k-support

norm. In NIPS  pages 1466–1474  2012.

[2] Arindam Banerjee  Sheng Chen  Farideh Fazayeli  and Vidyashankar Sivakumar. Estimation

with norm regularization. In NIPS  2014.

[3] Peter J Bickel  Ya’acov Ritov  and Alexandre B Tsybakov. Simultaneous analysis of lasso and

dantzig selector. The Annals of Statistics  37(4):1705–1732  2009.

[4] Stephen P. Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed
optimization and statistical learning via the alternating direction method of multipliers. Foun-
dations and Trends in Machine Learning  3(1):1–122  2011.

[5] Emmanuel Candes and Terence Tao. The Dantzig selector: Statistical estimation when p is

much larger than n. The Annals of Statistics  35(6):2313–2351  December 2007.

[6] Venkat Chandrasekaran  Benjamin Recht  Pablo A Parrilo  and Alan S Willsky. The convex
geometry of linear inverse problems. Foundations of Computational Mathematics  12(6):805–
849  2012.

[7] Soumyadeep Chatterjee  Sheng Chen  and Arindam Banerjee. Generalized dantzig selector:

Application to the k-support norm. ArXiv e-prints  June 2014.

[8] Yehoram Gordon  Alexander E. Litvak  Shahar Mendelson  and Alain Pajor. Gaussian averages
of interpolated bodies and applications to approximate reconstruction. Journal of Approxima-
tion Theory  149(1):59–73  2007.

[9] Gareth M. James and Peter Radchenko. A generalized Dantzig selector with shrinkage tuning.

Biometrika  96(2):323–337  2009.

[10] Gareth M. James  Peter Radchenko  and Jinchi Lv. DASSO: connections between the Dantzig

selector and lasso. Journal of the Royal Statistical Society Series B  71(1):127–142  2009.

[11] Han Liu  Jian Zhang  Xiaoye Jiang  and Jun Liu. The group Dantzig selector. In AISTATS 

pages 461–468  2010.

[12] Zhaosong Lu  Ting Kei Pong  and Yong Zhang. An alternating direction method for ﬁnding

dantzig selectors. Computational Statistics & Data Analysis  56(12):4037 – 4046  2012.

[13] Andrew M. McDonald  Massimiliano Pontil  and Dimitris Stamos. New perspectives on k-

support and cluster norms. ArXiv e-prints  March 2014.

[14] Sahand N Negahban  Pradeep Ravikumar  Martin J Wainwright  Bin Yu  et al. A uniﬁed frame-
work for high-dimensional analysis of m-estimators with decomposable regularizers. Statisti-
cal Science  27(4):538–557  2012.

[15] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization 

1(3):127–239  2014.

[16] Nikhil S Rao  Ben Recht  and Robert D Nowak. Universal measurement bounds for structured

sparse signal recovery. In AISTATS  pages 942–950  2012.

[17] Mark Rudelson and Roman Vershynin. On sparse reconstruction from Fourier and Gaussian
measurements. Communications on Pure and Applied Mathematics  61(8):1025–1045  2008.
[18] Ian Stewart. Galois Theory  Third Edition. Chapman Hall/CRC Mathematics Series. Taylor &

Francis  2003.

[19] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 267–288  1996.

[20] Huahua Wang and Arindam Banerjee. Bregman alternating direction method of multipliers. In

NIPS  2014.

[21] Xiangfeng Wang and Xiaoming Yuan. The linearized alternating direction method of multipli-

ers for Dantzig selector. SIAM Journal on Scientiﬁc Computing  34(5)  2012.

[22] Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine

Learning Research  7:2541–2563  2006.

[23] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal

of the Royal Statistical Society: Series B (Statistical Methodology)  67(2):301–320  2005.

9

,Soumyadeep Chatterjee
Sheng Chen
Arindam Banerjee
Ariel Procaccia
Nisarg Shah