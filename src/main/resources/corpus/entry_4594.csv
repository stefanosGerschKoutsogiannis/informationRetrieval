2016,Recovery Guarantee of Non-negative Matrix Factorization  via Alternating Updates,Non-negative matrix factorization is a popular tool for  decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features  and shows that assuming a generative model of the data  it provably recovers the ground-truth under fairly mild conditions. In particular  its only essential requirement on features is linear independence. Furthermore  the algorithm uses ReLU to exploit the non-negativity for decoding the weights  and thus can tolerate adversarial noise that can potentially be as large as the signal  and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions  which we believe is of independent interest.,Recovery Guarantee of Non-negative Matrix

Factorization via Alternating Updates

Yuanzhi Li  Yingyu Liang  Andrej Risteski

Computer Science Department at Princeton University

35 Olden St  Princeton  NJ 08540

{yuanzhil  yingyul  risteski}@cs.princeton.edu

Abstract

Non-negative matrix factorization is a popular tool for decomposing data into
feature and weight matrices under non-negativity constraints. It enjoys practical
success but is poorly understood theoretically. This paper proposes an algorithm
that alternates between decoding the weights and updating the features  and shows
that assuming a generative model of the data  it provably recovers the ground-
truth under fairly mild conditions. In particular  its only essential requirement on
features is linear independence. Furthermore  the algorithm uses ReLU to exploit
the non-negativity for decoding the weights  and thus can tolerate adversarial noise
that can potentially be as large as the signal  and can tolerate unbiased noise much
larger than the signal. The analysis relies on a carefully designed coupling between
two potential functions  which we believe is of independent interest.

1

Introduction

In this paper  we study the problem of non-negative matrix factorization (NMF)  where given a matrix
Y ∈ Rm×N   the goal to ﬁnd a matrix A ∈ Rm×n and a non-negative matrix X ∈ Rn×N such
that Y ≈ AX.1 A is often referred to as feature matrix and X referred as weights. NMF has been
extensively used in extracting a parts representation of the data (e.g.  [LS97  LS99  LS01]). It has
been shown that the non-negativity constraint on the coefﬁcients forcing features to combine  but not
cancel out  can lead to much more interpretable features and improved downstream performance of
the learned features.
Despite all the practical success  however  this problem is poorly understood theoretically  with only
few provable guarantees known. Moreover  many of the theoretical algorithms are based on heavy
tools from algebraic geometry (e.g.  [AGKM12]) or tensors (e.g. [AKF+12])  which are still not
as widely used in practice primarily because of computational feasibility issues or sensitivity to
assumptions on A and X. Some others depend on speciﬁc structure of the feature matrix  such as
separability [AGKM12] or similar properties [BGKP16].
A natural family of algorithms for NMF alternate between decoding the weights and updating the
features. More precisely  in the decoding step  the algorithm represents the data as a non-negative
combination of the current set of features; in the updating step  it updates the features using the
decoded representations. This meta-algorithm is popular in practice due to ease of implementation 
computational efﬁciency  and empirical quality of the recovered features. However  even less
theoretical analysis exists for such algorithms.

1In the usual formulation of the problem  A is also assumed to be non-negative  which we will not require in

this paper.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

√

This paper proposes an algorithm in the above framework with provable recovery guarantees. To
be speciﬁc  the data is assumed to come from a generative model y = A∗x∗ + ν. Here  A∗ is the
ground-truth feature matrix  x∗ are the non-negative ground-truth weights generated from an unknown
distribution  and ν is the noise. Our algorithm can provably recover A∗ under mild conditions  even
in the presence of large adversarial noise.
Overview of main results. The existing theoretical results on NMF can be roughly split into two
categories. In the ﬁrst category  they make heavy structural assumptions on the feature matrix A∗
such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]). In the
second one  they impose strict distributional assumptions on x∗ ([AKF+12])  where the methods are
usually based on the method of moments and tensor decompositions and have poor tolerance to noise 
which is very important in practice.
In this paper  we present a very simple and natural alternating update algorithm that achieves the best
of both worlds. First  we have minimal assumptions on the feature matrix A∗: the only essential
condition is linear independence of the features. Second  it is robust to adversarial noise ν which
in some parameter regimes be potentially be on the same order as the signal A∗x∗  and is robust to
unbiased noise potentially even higher than the signal by a factor of O(
n). The algorithm does not
require knowing the distribution of x∗  and allows a fairly wide family of interesting distributions.
We get this at a rather small cost of a mild “warm start”. Namely  we initialize each of the features to
be “correlated” with the ground-truth features. This type of initialization is often used in practice as
well  for example in LDA-c  the most popular software for topic modeling ([lda16]).
A major feature of our algorithm is the signiﬁcant robustness to noise. In the presence of adversarial
noise on each entry of y up to level Cν  the noise level (cid:107)ν(cid:107)1 can be in the same order as the signal
A∗x∗. Still  our algorithm is able to output a matrix A such that the ﬁnal (cid:107)A∗ − A(cid:107)1 ≤ O((cid:107)ν(cid:107)1) in
the order of the noise in one data point. If the noise is unbiased (i.e.  E[ν|x∗] = 0)  the noise level
(cid:107)ν(cid:107)1 can be Ω(
n) times larger than the signal A∗x∗  while we can still guarantee (cid:107)A∗ − A(cid:107)1 ≤
O ((cid:107)ν(cid:107)1
n) – so our algorithm is not only tolerant to noise  but also has very strong denoising effect.
Note that even for the unbiased case the noise can potentially be correlated with the ground-truth in
very complicated manner  and also  all our results are obtained only requiring the columns of A∗ are
independent.
Technical contribution. The success of our algorithm crucially relies on exploiting the non-negativity
of x∗ by a ReLU thresholding step during the decoding procedure. Similar techniques have been
considered in prior works on matrix factorization  however to the best of our knowledge  the analysis
(e.g.  [AGMM15]) requires that the decodings are correct in all the intermediate iterations  in the
sense that the supports of x∗ are recovered with no error. Indeed  we cannot hope for a similar
guarantee in our setting  since we consider adversarial noise that could potentially be the same order
as the signal. Our major technical contribution is a way to deal with the erroneous decoding through
out all the intermediate iterations. We achieve this by a coupling between two potential functions
that capture different aspects of the working matrix A. While analyzing iterative algorithms like
alternating minimization or gradient descent in non-convex settings is a popular topic in recent
years  the proof usually proceeds by showing that the updates are approximately performing gradient
descent on an objective with some local or hidden convex structure. Our technique diverges from the
common proof strategy  and we believe is interesting in its own right.
Organization. After reviewing related work  we deﬁne the problem in Section 3 and describe our
main algorithm in Section 4. To emphasize the key ideas  we ﬁrst present the results and the proof
sketch for a simpliﬁed yet still interesting case in Section 5  and then present the results under much
more general assumptions in Section 6. The complete proof is provided in the appendix.

√

√

2 Related work

Non-negative matrix factorization relates to several different topics in machine learning.
Non-negative matrix factorization. The area of non-negative matrix factorization (NMF) has a rich
empirical history  starting with the practical algorithm of [LS97].On the theoretical side  [AGKM12]
provides a ﬁxed-parameter tractable algorithm for NMF  which solves algebraic equations and thus has
poor noise tolerance. [AGKM12] also studies NMF under separability assumptions about the features.

2

[BGKP16] studies NMF under heavy noise  but also needs assumptions related to separability  such
as the existence of dominant features. Also  their noise model is different from ours.
Topic modeling. A closely related problem to NMF is topic modeling  a common generative model
for textual data [BNJ03  Ble12]. Usually  (cid:107)x∗(cid:107)1 = 1 while there also exist work that assume
i ∈ [0  1] and are independent [ZX12]. A popular heuristic in practice for learning A∗ is variational
x∗
inference  which can be interpreted as alternating minimization in KL divergence norm. On the
theory front  there is a sequence of works by based on either spectral or combinatorial approaches 
which need certain “non-overlapping” assumptions on the topics. For example  [AGH+13] assume
the topic-word matrix contains “anchor words”: words which appear in a single topic. Most related
is the work of [AR15] who analyze a version of the variational inference updates when documents
are long. However  they require strong assumptions on both the warm start  and the amount of
“non-overlapping” of the topics in the topic-word matrix.
ICA. Our generative model for x∗ will assume the coordinates are independent  therefore our problem
can be viewed as a non-negative variant of ICA with high levels of noise. Results here typically are
not robust to noise  with the exception of [AGMS12] that tolerates Gaussian noise. However  to best
of our knowledge  no result in this setting is provably robust to adversarial noise.
Non-convex optimization. The framework of having a “decoding” for the samples  along with
performing an update for the model parameters has proven successful for dictionary learning as
well. The original empirical work proposing such an algorithm (in fact  it suggested that the V1
layer processes visual signals in the same manner) was due to [OF97]. Even more  similar families
of algorithms based on “decoding” and gradient-descent are believed to be neurally plausible as
mechanisms for a variety of tasks like clustering  dimension-reduction  NMF  etc ([PC15  PC14]). A
theoretical analysis came latter for dictionary learning due to [AGMM15] under the assumption that
the columns of A∗ are incoherent. The technique is not directly applicable to our case  as we don’t
wish to have any assumptions on the matrix A∗. For instance  if A∗ is non-negative and columns
with l1 norm 1  incoherence effectively means the the columns of A∗ have very small overlap.

3 Problem deﬁnition and assumptions
Given a matrix Y ∈ Rm×N   the goal of non-negative matrix factorization (NMF) is to ﬁnd a matrix
A ∈ Rm×n and a non-negative matrix X ∈ Rn×N   so that Y ≈ AX. The columns of Y are
called data points  those of A are features  and those of X are weights. We note that in the original
NMF  A is also assumed to be non-negative  which is not required here. We also note that typically
m (cid:29) n  i.e.  the features are a few representative components in the data space. This is different
from dictionary learning where overcompleteness is often assumed.
The problem in the worst case is NP-hard [AGKM12]  so some assumptions are needed to design
provable efﬁcient algorithms. In this paper  we consider a generative model for the data point

y = A∗x∗ + ν

(1)
where A∗ is the ground-truth feature matrix  x∗ is the ground-truth non-negative weight from some
unknown distribution  and ν is the noise. Our focus is to recover A∗ given access to the data
(cid:80)
distribution  assuming some properties of A∗  x∗  and ν. To describe our assumptions  we let [M]i
denote the i-th row of a matrix M  [M]j its i-th column  Mi j its (i  j)-th entry. Denote its column
j |Mi j| 
norm  row norm  and symmetrized norm as (cid:107)M(cid:107)1 = maxj
and (cid:107)M(cid:107)s = max{(cid:107)M(cid:107)1 (cid:107)M(cid:107)∞}   respectively.
We assume the following hold for parameters C1  c2  C2  (cid:96)  Cν to be determined in our theorems.

(cid:80)
i |Mi j| (cid:107)M(cid:107)∞ = maxi

(A1) The columns of A∗ are linearly independent.
(A2) For all i ∈ [n]  x∗
i ’s are independent.
n and c2
(A3) The initialization A(0) = A∗(Σ(0) + E(0)) + N(0)  where Σ(0) is diagonal  E(0) is off-

i ∈ [0  1]  E[x∗

n ≤ E[(x∗

i )2] ≤ C2

n   and x∗

diagonal  and

We consider two noise models.

(cid:13)(cid:13)(cid:13)E(0)(cid:13)(cid:13)(cid:13)s

≤ (cid:96).

i ] ≤ C1

Σ(0) (cid:23) (1 − (cid:96))I 

3

(N1) Adversarial noise: only assume that maxi |νi| ≤ Cν almost surely.
(N2) Unbiased noise: maxi |νi| ≤ Cν almost surely  and E[ν|x∗] = 0.

3 = 1 and the case when x∗

Remarks. We make several remarks about each of the assumptions.
(A1) is the assumption about A∗. It only requires the columns of A∗ to be linear independent  which
is very mild and needed to ensure identiﬁability. Otherwise  for instance  if (A∗)3 = λ1(A∗)1 +
λ2(A∗)2  it is impossible to distinguish between the case when x∗
2 = λ1
and x∗
1 = λ2. In particular  we do not restrict the feature matrix to be non-negative  which is more
general than the traditional NMF and is potentially useful for many applications. We also do not
make incoherence or anchor word assumptions that are typical in related work.
(A2) is the assumption on x∗. First  the coordinates are non-negative and bounded by 1; this is simply
a matter of scaling. Second  the assumption on the moments requires that  roughly speaking  each
feature should appear with reasonable probability. This is expected: if the occurrences of the features
are extremely unbalanced  then it will be difﬁcult to recover the rare ones. The third requirement
on independence is motivated by that the features should be different so that their occurrences are
not correlated. Here we do not stick to a speciﬁc distribution  since the moment conditions are more
general  and highlight the essential properties our algorithm needs. Example distributions satisfying
our assumptions will be discussed later.
The warm start required by (A3) means that each feature A(0)
has a large fraction of the ground-
truth feature A∗
i and a small fraction of the other features  plus some noise outside the span of the
ground-truth features. We emphasize that N(0) is the component of A(0) outside the column space
of A∗  and is not the difference between A(0) and A∗. This requirement is typically achieved in
practice by setting the columns of A(0) to reasonable “pure” data points that contains one major
feature and a small fraction of some other features (e.g. [lda16  AR15]); in this initialization  it is
generally believed that N(0) = 0. But we state our theorems to allow some noise N(0) for robustness
in the initialization.
The adversarial noise model (N1) is very general  only imposing an upper bound on the entry-wise
noise level. Thus  ν can be correlated with x∗ in some complicated unknown way. (N2) additionally
requires it to be zero mean  which is commonly assumed and will be exploited by our algorithm to
tolerate larger noise.

i

4 Main algorithm

Algorithm 1 Puriﬁcation
Input: initialization A(0)  threshold α  step size η  scaling factor r  sample size N  iterations T
1: for t = 0  1  2  ...  T − 1 do
2:
3:

Draw examples y1  . . .   yN .
(Decode) Compute A†  the pseudo-inverse of A(t) with minimum (cid:107)(A)†(cid:107)∞.

Set x = φα(A†y) for each example y.

// φα is ReLU activation; see (2) for the

4:

deﬁnition
(Update) Update the feature matrix

A(t+1) = (1 − η) A(t) + rη ˆE(cid:2)(y − y(cid:48))(x − x(cid:48))(cid:62)(cid:3)

where ˆE is over independent uniform y  y(cid:48) from {y1  . . .   yN}  and x  x(cid:48) are their decodings.

Output: A = A(T )

Our main algorithm is presented in Algorithm 1. It keeps a working feature matrix and operates in
iterations. In each iteration  it ﬁrst compute the weights for a batch of N examples (decoding)  and
then uses the computed weights to update the feature matrix (updating).
The decoding is simply multiplying the example by the pseudo-inverse of the current feature matrix
and then passing it through the rectiﬁed linear unit (ReLU) φα with offset α. The pseudo-inverse
with minimum inﬁnity norm is used so as to maximize the robustness to noise (see the theorems).
The ReLU function φα operates element-wisely on the input vector v  and for an element vi  it is

4

deﬁned as

φα(vi) = max{vi − α  0} .

(2)

To get an intuition why the decoding makes sense  suppose the current feature matrix is the ground-
truth. Then A†y = A†A∗x∗ + A†ν = x∗ + A†ν. So we would like to use a small A† and use
threshold to remove the noise term.

In the encoding step  the algorithm move the feature matrix along the direction E(cid:2)(y − y(cid:48))(x − x(cid:48))(cid:62)(cid:3).
noise  E(cid:2)(y − y(cid:48))(x − x(cid:48))(cid:62)(cid:3) = A∗  and thus it is moving towards the ground-truth. Without those

To see intuitively why this is a good direction  note that when the decoding is perfect and there is no

ideal conditions  we need to choose a proper step size  which is tuned by the parameters η and r.

5 Results for a simpliﬁed case

Our intuitions can be demonstrated in a simpliﬁed setting with (A1)  (A2’)  (A3)  and (N1)  where

(A2’) x∗

i ’s are independent  and x∗

i = 1 with probability s/n and 0 otherwise for a constant s > 0.

Furthermore  let N(0) = 0. This is a special case of our general assumptions  with C1 = c2 = C2 = s
where s is the parameter in (A2’). It is still an interesting setting; as far as we know  there is no
existing guarantee of alternating type algorithms for it.
To present our results  we let (A∗)† denote the matrix satisfying (A∗)†A∗ = I; if there are multiple
such matrices we let it denote the one with minimum (cid:107)(A∗)†(cid:107)∞.
Theorem 1 (Simpliﬁed case  adversarial noise). There exists a absolute constant G such that when
Assumption (A1)(A2’)(A3) and (N1) are satisﬁed with l = 1/10  Cν ≤
max{m n(cid:107)(A∗)†(cid:107)∞} for
some 0 ≤ c ≤ 1  and N(0) = 0  then there exist α  η  r such that for every 0 <   δ < 1 and
N = poly(n  m  1/  1/δ) the following holds with probability at least 1 − δ.

(cid:1) iterations  Algorithm 1 outputs a solution A = A∗(Σ + E) + N where

After T = O(cid:0)ln 1

Σ (cid:23) (1 − (cid:96))I is diagonal  (cid:107)E(cid:107)1 ≤  + c is off-diagonal  and (cid:107)N(cid:107)1 ≤ c.
Remarks. Consequently  when (cid:107)A∗(cid:107)1 = 1  we can do normalization ˆAi = Ai/(cid:107)Ai(cid:107)1  and the
normalized output ˆA satisﬁes

Gc



(cid:107) ˆA − A∗(cid:107)1 ≤  + 2c.

So under mild conditions and with proper parameters  our algorithm recovers the ground-truth in a
geometric rate. It can achieve arbitrary small recovery error in the noiseless setting  and achieve error
up to the noise limit even with adversarial noise whose level is comparable to the signal.
The condition on (cid:96) means that a constant warm start is sufﬁcient for our algorithm to converge  which
is much better than previous work such as [AR15]. Indeed  in that work  the (cid:96) needs to even depend
on the dynamic range of the entries of A∗ which is problematic in practice.
It is shown that with large adversarial noise  the algorithm can still recover the features up to the
† (cid:107)∞  each data point has adversarial noise with (cid:96)1 norm as large
noise limit. When m ≥ n(cid:107) (A∗)
as (cid:107)ν(cid:107)1 = Cνm = Ω(c)  which is in the same order as the signal (cid:107)A∗x∗(cid:107)1 = O(1). Our algorithm
still works in this regime. Furthermore  the ﬁnal error (cid:107)A − A∗(cid:107)1 is O(c)  in the same order as the
adversarial noise in one data point.
† (cid:107)∞ is not surprising. The case when the columns are the canonical
Note the appearance of (cid:107) (A∗)
† (cid:107)∞ = 1  is expected to be easier than the
unit vectors for instance  which corresponds to (cid:107) (A∗)
case when the columns are nearly the same  which corresponds to large (cid:107) (A∗)
A similar theorem holds for the unbiased noise model.
Theorem 2 (Simpliﬁed case  unbiased noise). If Assumption (A1)(A2’)(A3) and (N2) are satisﬁed
with Cν =
max{m n(cid:107)(A∗)†(cid:107)∞} and the other parameters set as in Theorem 1  then the same
guarantee in holds.

† (cid:107)∞.

Gc

√

n

5

√

n larger than the adversarial case. When m ≥ n(cid:107) (A∗)

Remarks. With unbiased noise which is commonly assumed in many applications  the algorithm can
† (cid:107)∞  each data point
√
tolerate noise level
has adversarial noise with (cid:96)1 norm as large as (cid:107)ν(cid:107)1 = Cνm = Ω(c
n)  which can be Ω(
n) times
larger than the signal (cid:107)A∗x∗(cid:107)1 = O(1). The algorithm can recover the ground-truth in this heavy
√
√
noise regime. Furthermore  the ﬁnal error (cid:107)A − A∗(cid:107)1 is O ((cid:107)ν(cid:107)1/
n)  which is only O(1/
n)
fraction of the noise in one data point. This is very strong denoising effect and a bit counter-intuitive.
It is possible since we exploit the average of the noise for cancellation  and also use thresholding to
remove noise spread out in the coordinates.

√

5.1 Analysis: intuition

A natural approach typically employed to analyze algorithms for non-convex problems is to deﬁne a
function on the intermediate solution A and the ground-truth A∗ measuring their distance and then
show that the function decreases at each step. However  a single potential function will not be enough
in our case  as we argue below  so we introduce a novel framework of maintaining two potential
functions which capture different aspects of the intermediate solutions.
Let us denote the intermediate solution and the update as (omitting the superscript (t))

ˆE[(y − y(cid:48))(x − x(cid:48))(cid:62)] = A∗((cid:101)Σ +(cid:101)E) + (cid:101)N 

(cid:62)

] 

(3)

i +(cid:80)

A = A∗(Σ + E) + N 

where Σ and (cid:101)Σ are diagonal  E and(cid:101)E are off-diagonal  and N and (cid:101)N are the terms outside the span

j   if the ratio between (cid:107)Ei(cid:107)1 =(cid:80)

of A∗ which is caused by the noise. To cleanly illustrate the intuition behind ReLU and the coupled
potential functions  we focus on the noiseless case and assume that we have inﬁnite samples.
j(cid:54)=i |Ej i| and Σi i gets smaller 
Since Ai = Σi iA∗
j(cid:54)=i Ej iA∗
then the algorithm is making progress; if the ratio is large at the end  a normalization of Ai gives a
i . So it sufﬁces to show that Σi i is always about a constant while (cid:107)Ei(cid:107)1
good approximation of A∗
decreases at each iteration. We will focus on E and consider the update rule in more detail to argue
this. After some calculation  we have

(cid:101)E = E[(x∗ − (x(cid:48))∗) (x − x(cid:48))
(cid:0)(Σ + E)−1(x(cid:48))∗(cid:1) .

E ← (1 − η)E + rη(cid:101)E 
(cid:0)(Σ + E)−1x∗(cid:1)  
(cid:101)E = E(x∗ − (x(cid:48))∗)(cid:2)A†A∗(x∗ − (x(cid:48))∗)(cid:3)(cid:62)
= E(cid:2)(x∗ − (x(cid:48))∗)(x∗ − (x(cid:48))∗)(cid:62)(cid:3)(cid:2)(Σ + E)−1(cid:3)(cid:62)
∝(cid:2)(Σ + E)−1(cid:3)(cid:62) ≈ Σ−1 − Σ−1EΣ−1.
where we used Taylor expansion and the fact that E(cid:2)(x∗ − (x(cid:48))∗)(x∗ − (x(cid:48))∗)(cid:62)(cid:3) is a scaling of

To see why the ReLU function matters  consider the case when we do not use it.

where x  x(cid:48) are the decoding for x∗  (x(cid:48))∗ respectively:

x(cid:48) = φα

x = φα

(4)

identity. Hence  if we think of Σ as approximately I and take an appropriate r  the update to the
matrix E is approximately E ← E − ηE(cid:62). Since we do not have control over the signs of E
throughout the iterations  the problematic case is when the entries of E(cid:62) and E roughly match in
signs  which would lead to the entries of E increasing.
Now we consider the decoding to see why ReLU is important. Ignoring the higher order terms and
regarding Σ = I  we have

(cid:0)Σ−1x∗ − Σ−1EΣ−1x∗(cid:1) ≈ φα (x∗ − Ex∗) .

(5)
The problematic term is Ex∗. These errors when summed up will be comparable or even larger
than the signals  and the algorithm will fail. However  since the signals are non-negative and most
coordinates with errors only have small values  thresholding with ReLU properly can remove those

errors while keeping a large fraction of the signals. This leads to large (cid:101)Σi i and small(cid:101)Ej i’s  and then

(cid:0)(Σ + E)−1x∗(cid:1) ≈ φα

x = φα

we can choose an r such that Ej i’s keep decreasing while Σi i’s stay in a certain range.
To get a quantitative bound  we divide E into its positive part E+ and its negative part E−:

[E+]i j = max{Ei j  0}  

[E−]i j = max{−Ei j  0} .

(6)

6

tion (cid:2)(Σ + E)−1x∗(cid:3)
(cid:2)(Σ + E)−1x∗(cid:3)

The reason to do so is the following: when Ei j is negative  by the Taylor expansion approxima-
i will tend to be more positive and will not be thresholded most of the time.
Therefore  Ej i will turn more positive at next iteration. On the other hand  when Ei j is positive 
i will tend to be more negative and zeroed out by the threshold function. Therefore 
Ej i will not be more negative at next iteration. We will show for positive and negative parts of E:
postive(t+1) ← (1−η)positive(t)+(η)negative(t)  negative(t+1) ← (1−η)negative(t)+(εη)positive(t)
for a small ε (cid:28) 1. Due to   we can couple the two parts so that a weighted average of them will
decrease  which implies that (cid:107)E(cid:107)s is small at the end. This leads to our coupled potential function.2

5.2 Analysis: proof sketch

Here we describe a proof sketch for the simpliﬁed case while the complete proof for the general case
is presented in the appendix. The lemmas here are direct corollaries of those in the appendix.
One iteration. We focus on one update and omit the superscript (t). Recall the deﬁnitions of E  Σ

and N in (5.1)  and (cid:101)E  (cid:101)Σ and (cid:101)N in (5.1). Our goal is to derive lower and upper bounds for (cid:101)E  (cid:101)Σ
and (cid:101)N  assuming that Σi i falls into some range around 1  while E and N are small. This will allow

doing induction on them.
First  begin with the decoding. Some calculation shows that  the decoding for y = A∗x∗ + ν is

(7)

n2 + c

−1   ξ = −A†NZx∗ + A†ν.

x = φα (Zx∗ + ξ)   where Z = (Σ + E)

Now  we can present our key lemmas bounding(cid:101)E  (cid:101)Σ  and (cid:101)N.
(cid:12)(cid:12)(cid:12)(cid:101)Ej i
(cid:12)(cid:12)(cid:12) ≤ O(cid:0) 1
n2 (|Zi j| + c)(cid:1)  
Lemma 3 (Simpliﬁed bound on(cid:101)E  informal). (1) if Zi j < 0  then
n2|Zi j|(cid:1) ≤(cid:12)(cid:12)(cid:12)(cid:101)Ej i
(cid:12)(cid:12)(cid:12) ≤ O(cid:0) 1
(2) if Zi j ≥ 0  then −O(cid:0) c
n(cid:107)Zi j(cid:107)(cid:1) .
the upper bound on |(cid:101)Ej i| is very small and thus |Ej i| decreases  as described in the intuition.

Note that Z ≈ Σ−1 − Σ−1EΣ−1  so Zi j < 0 corresponds roughly to Ei j > 0. In this case 
What is most interesting is the case when Zi j ≥ 0 (roughly Ei j < 0). The upper bound is much
larger  corresponding to the intuition that negative Ei j can contribute a large positive value to Ej i.
Fortunately  the lower bounds are of much smaller absolute value  which allows us to show that a
potential function that couples Case (1) and Case (2) in Lemma 3 actually decreases; see the induction
below.

n|Zi j| + 1

Lemma 4 (Simpliﬁed bound on (cid:101)Σ  informal). (cid:101)Σi i ≥ Ω(Σ−1
Lemma 5 (Simpliﬁed bound on (cid:101)N  adversarial noise  informal).

(cid:12)(cid:12)(cid:12) ≤ O(Cν/n).

(cid:12)(cid:12)(cid:12)(cid:101)Ni j

i i − α)/n.

Induction by iterations. We now show how to use the three lemmas to prove the theorem for the
adversarial noise  and that for the unbiased noise is similar.

  and choose η = (cid:96)/6. We begin with proving the following

+

(cid:13)(cid:13)(cid:13)s

(cid:13)(cid:13)(cid:13)s

and bt :=

(cid:13)(cid:13)(cid:13)E(t)−

(1) (1 − (cid:96))I (cid:22) Σ(t)

Let at :=
three claims by induction on t: at the beginning of iteration t 

(cid:13)(cid:13)(cid:13)E(t)
(2) (cid:13)(cid:13)E(t)(cid:13)(cid:13)s ≤ 1/8  and if t > 0  then at + βbt ≤(cid:0)1 − 1
(3) (cid:13)(cid:13)N(t)(cid:13)(cid:13)s ≤ c/10.
(cid:18)
(cid:19)

β ∈ (1  8)  and some small value h 

(cid:18)

at+1 ≤

1 − 3
25

η

at + 7ηbt + ηh 

25 η(cid:1) (at−1 + βbt−1) + ηh  for some

The most interesting part is the second claim. At a high level  by Lemma 3  we can show that

(cid:19)

bt+1 ≤

1 − 24
25

η

bt +

1
100

ηat + ηh.

2Note that since intuitively  Ei j gets affected by Ej i after an update  if we have a row which contains
i (cid:107)1 as a

i (cid:107)1 increases. So we cannot simply use maxi (cid:107)Ai − A∗

negative entries  it is possible that (cid:107)Ai − A∗
potential function.

7

Notice that the contribution of bt to at+1 is quite large (due to the larger upper bound in Case (2)
in Lemma 3)  but the other terms are much nicer  such as the small contribution of at to bt+1. This
allows to choose a β ∈ (1  8) so that at+1 + βbt+1 leads to the desired recurrence in the second
claim. In other words  at+1 + βbt+1 is our potential function which decreases at each iteration up to
the level h. The other claims can also be proved by the corresponding lemmas. Then the theorem
follows from the induction claims.

6 More general results
More general weight distributions. Our argument holds under more general assumptions on x∗.
Theorem 6 (Adversarial noise). There exists an absolute constant G such that when Assumption (A0)-
(A3) and (N1) are satisﬁed with l = 1/10  C2 ≤ 2c2  C 3

2n  Cν ≤

1 ≤ Gc2

(cid:26)

(cid:27)

2Gc
c4

2Gc
c2
1 m  
C2

1 n(cid:107)(A∗)†(cid:107)∞

C5

for 0 ≤ c ≤ 1  and(cid:13)(cid:13)N(0)(cid:13)(cid:13)∞ ≤
After T = O(cid:0)ln 1

2Gc
c2

1(cid:107)(A∗)†(cid:107)∞
C3

  then there exist α  η  r such that for every 0 <   δ < 1

and N = poly(n  m  1/  1/δ)  with probability at least 1 − δ the following holds.

(cid:1) iterations  Algorithm 1 outputs a solution A = A∗(Σ + E) + N where

Σ (cid:23) (1 − (cid:96))I is diagonal  (cid:107)E(cid:107)1 ≤  + c/2 is off-diagonal  and (cid:107)N(cid:107)1 ≤ c/2.
Theorem 7 (Unbiased noise).
If Assumption (A0)-(A3) and (N2) are satisﬁed with Cν =
C1 max{m n(cid:107)(A∗)†(cid:107)∞} and the other parameters set as in Theorem 6  then the same guarantee
holds.

c2G√

cn



The conditions on C1  c2  C2 intuitively mean that each feature needs to appear with reasonable
probability. C2 ≤ 2c2 means that their proportions are reasonably balanced. This may be a mild
restriction for some applications  and additionally we propose a pre-processing step that can relax
this in the next subsection. The conditions allow a rather general family of distributions  so we point
out an important special case to provide a more concrete sense of the parameters. For example  for
the uniform independent distribution considered in the simpliﬁed case  we can actually allow s to be
much larger than a constant; our algorithm just requires s ≤ Gn for a ﬁxed constant G. So it works
for uniform sparse distributions even when the sparsity is linear  which is an order of magnitude larger
than in the dictionary learning regime. Furthermore  the distributions of x∗
i can be very different 
2n). Moreover  all these can be handled without speciﬁc structural
since we only require C 3
assumptions on A∗.
More general proportions. A mild restriction in Theorem 6 and 7 is that C2 ≤ 2c2  that is 
i )2] ≤ 2 mini∈[n] E[(x∗
maxi∈[n] E[(x∗
i )2]. To satisfy this  we propose a preprocessing algorithm for
i )2]. The idea is quite simple: instead of solving Y ≈ A∗X  we could also solve
balancing E[(x∗
Y ≈ [A∗D][(D)−1X] for a positive diagonal matrix D  where E[(x∗
i i is with in a factor of
2 from each other. We show in the appendix that this can be done under assumptions as the above
theorems  and additionally Σ (cid:22) (1 + (cid:96))I and E(0) ≥ entry-wise. After balancing  one can use
Algorithm 1 on the new ground-truth matrix [A∗D] to get the ﬁnal result.

1 = O(c2

i )2]/D2

7 Conclusion

A simple and natural algorithm that alternates between decoding and updating is proposed for
non-negative matrix factorization and theoretical guarantees are provided. The algorithm provably
recovers a feature matrix close to the ground-truth and is robust to noise. Our analysis provides
insights on the effect of the ReLU units in the presence of the non-negativity constraints  and the
resulting interesting dynamics of the convergence.

Acknowledgements

This work was supported in part by NSF grants CCF-1527371  DMS-1317308  Simons Investigator
Award  Simons Collaboration Grant  and ONR-N00014-16-1-2329.

8

References
[AGH+13] S. Arora  R. Ge  Y. Halpern  D. Mimno  A. Moitra  D. Sontag  Y. Wu  and M. Zhu. A

practical algorithm for topic modeling with provable guarantees. In ICML  2013.

[AGKM12] Sanjeev Arora  Rong Ge  Ravindran Kannan  and Ankur Moitra. Computing a nonnega-

tive matrix factorization–provably. In STOC  pages 145–162. ACM  2012.

[AGM12] S. Arora  R. Ge  and A. Moitra. Learning topic models – going beyond svd. In FOCS 

2012.

[AGMM15] S. Arora  R. Ge  T. Ma  and A. Moitra. Simple  efﬁcient  and neural algorithms for

sparse coding. In COLT  2015.

[AGMS12] Sanjeev Arora  Rong Ge  Ankur Moitra  and Sushant Sachdeva. Provable ica with
unknown gaussian noise  with implications for gaussian mixtures and autoencoders. In
NIPS  pages 2375–2383  2012.

[AKF+12] A. Anandkumar  S. Kakade  D. Foster  Y. Liu  and D. Hsu. Two svds sufﬁce: Spectral
decompositions for probabilistic topic modeling and latent dirichlet allocation. Technical
report  2012.

[AR15] Pranjal Awasthi and Andrej Risteski. On some provably correct cases of variational

inference for topic models. In NIPS  pages 2089–2097  2015.

[BGKP16] Chiranjib Bhattacharyya  Navin Goyal  Ravindran Kannan  and Jagdeep Pani. Non-
negative matrix factorization under heavy noise. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning  2016.

[Ble12] David M Blei. Probabilistic topic models. Communications of the ACM  2012.

[BNJ03] David M Blei  Andrew Y Ng  and Michael I Jordan. Latent dirichlet allocation. JMLR 

3:993–1022  2003.

[lda16] Lda-c software. https://github.com/blei-lab/lda-c/blob/master/readme.

txt  2016. Accessed: 2016-05-19.

[LS97] Daniel D Lee and H Sebastian Seung. Unsupervised learning by convex and conic

coding. NIPS  pages 515–521  1997.

[LS99] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative

matrix factorization. Nature  401(6755):788–791  1999.

[LS01] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization.

In NIPS  pages 556–562  2001.

[OF97] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set:

A strategy employed by v1? Vision research  37(23):3311–3325  1997.

[PC14] Cengiz Pehlevan and Dmitri B Chklovskii. A hebbian/anti-hebbian network derived
from online non-negative matrix factorization can cluster and discover sparse features.
In Asilomar Conference on Signals  Systems and Computers  pages 769–775. IEEE 
2014.

[PC15] Cengiz Pehlevan and Dmitri Chklovskii. A normative theory of adaptive dimensionality

reduction in neural networks. In NIPS  pages 2260–2268  2015.

[ZX12] Jun Zhu and Eric P Xing. Sparse topical coding. arXiv preprint arXiv:1202.3778  2012.

9

,Yuanzhi Li
Yingyu Liang
Andrej Risteski