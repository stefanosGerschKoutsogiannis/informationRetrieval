2019,Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms,The data deluge comes with high demands for data labeling. Crowdsourcing (or  more generally  ensemble learning) techniques aim to produce accurate labels via integrating noisy  non-expert labeling from annotators. The classic Dawid-Skene estimator and its accompanying expectation maximization (EM) algorithm have been widely used  but the theoretical properties are not fully understood. Tensor methods were proposed to guarantee identification of the Dawid-Skene model  but the sample complexity is a hurdle for applying such approaches---since the tensor methods hinge on the availability of third-order statistics that are hard to reliably estimate given limited data. In this paper  we propose a framework using pairwise co-occurrences of the annotator responses  which naturally admits lower sample complexity. We show that the approach can identify the Dawid-Skene model under realistic conditions. We propose an algebraic algorithm reminiscent of convex geometry-based structured matrix factorization to solve the model identification problem efficiently  and an identifiability-enhanced algorithm for handling more challenging and critical scenarios. Experiments show that the proposed algorithms outperform the state-of-art algorithms under a variety of scenarios.,Crowdsourcing via Pairwise Co-occurrences:

Identiﬁability and Algorithms

School of Elect. Eng. & Computer Sci.

School of Elect. Eng. & Computer Sci.

Shahana Ibrahim

Oregon State University

Corvallis  OR 97331

ibrahish@oregonstate.edu

Nikos Kargas

University of Minnesota
Minneapolis  MN 55455

kaga005@umn.edu

Xiao Fu∗

Oregon State University

Corvallis  OR 97331

xiao.fu@oregonstate.edu

Kejun Huang

University of Florida
Gainesville  FL 32611
kejun.huang@ufl.edu

Department of Elect. & Computer Eng.

Department of Computing & Info. Sci. & Eng.

Abstract

The data deluge comes with high demands for data labeling. Crowdsourcing (or 
more generally  ensemble learning) techniques aim to produce accurate labels via
integrating noisy  non-expert labeling from annotators. The classic Dawid-Skene
estimator and its accompanying expectation maximization (EM) algorithm have
been widely used  but the theoretical properties are not fully understood. Tensor
methods were proposed to guarantee identiﬁcation of the Dawid-Skene model  but
the sample complexity is a hurdle for applying such approaches—since the tensor
methods hinge on the availability of third-order statistics that are hard to reliably
estimate given limited data. In this paper  we propose a framework using pairwise
co-occurrences of the annotator responses  which naturally admits lower sample
complexity. We show that the approach can identify the Dawid-Skene model under
realistic conditions. We propose an algebraic algorithm reminiscent of convex
geometry-based structured matrix factorization to solve the model identiﬁcation
problem efﬁciently  and an identiﬁability-enhanced algorithm for handling more
challenging and critical scenarios. Experiments show that the proposed algorithms
outperform the state-of-art algorithms under a variety of scenarios.

1

Introduction

Background. The drastically increasing availability of data has successfully enabled many timely
applications in machine learning and artiﬁcial intelligence. At the same time  most supervised
learning tasks  e.g.  the core tasks in computer vision  natural language processing  and speech
processing  heavily rely on labeled data. However  labeling data is not a trivial task—it requires
educated and knowledgeable annotators (which could be human workers or machine classiﬁers) 
to work under a reliable way. More importantly  it needs an effective mechanism to integrate the
possibly different labeling from multiple annotators. Techniques addressing this problem in machine
learning are called crowdsourcing [24] or more generally  ensemble learning [8].

∗The work is supported in part by the National Science Foundation under projects ECCS 1808159 and NSF
ECCS 1608961  and by the Army Research Ofﬁce (ARO) under projects ARO W911NF-19-1-0247 and ARO
W911NF-19-1-0407.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Crowdsourcing has a long history in machine learning  which can be traced back to the 1970s [6].
Many models and methods have appeared since then [22  23  21  34  38  28  37]. Intuitively  if a
number of reliable annotators label the same data samples  then majority voting among the annotators
is expected to work well. However  in practice  not all the annotators are equally reliable—e.g. 
different annotators could be specialized for recognizing different classes. In addition  not all the
annotators are labeling all the data samples  since data samples are often dispatched to different groups
of annotators in a certain way. Under such circumstances  majority voting is not very promising.
A more sophisticate way is to treat the crowdsourcing problem as a model identiﬁcation problem.
The arguably most popular generative model in crowdsourcing is the Dawid-Skene model [6]  where
every annotator is assigned with a ‘confusion matrix’ that decides the probability of an annotator
giving class label (cid:96) when the ground-truth label is g. If such confusion matrices and the probability
mass function (PMF) of the ground-truth label can be identiﬁed  then a maximum likelihood (ML) or
a maximum a posteriori (MAP) estimator for the true label of any given sample can be constructed.
The Dawid-Skene model is quite simple and succinct  and some of the model assumptions (e.g.  the
conditional independence of the annotator responses) are actually debatable. Nonetheless  this model
has been proven very useful in practice [31  37  14  23  28  39].
Theoretical aspects for the Dawid-Skene model  however  are less well understood. In particular  it
had been unclear if the model could be identiﬁed via the accompanying expectation maximization
(EM) algorithm proposed in the same paper [6]  until some recent works addressing certain special
cases [23]. The works in [37  39] put forth tensor methods for learning the Dawid-Skene model.
These methods admit model identiﬁability  and also can be used to effectively initialize the classic
EM algorithm provably [39]. The challenge is that tensor methods utilize third-order statistics of the
data samples  which are rather hard to estimate reliably in practice given limited data [19].
Contributions. In this work  we propose an alternative for identifying the Dawid-Skene model 
without using third-order statistics. Our approach is based on utilizing the pairwise co-occurrences
of annotators’ responses to data samples—which are second-order statistics and thus are naturally
much easier to estimate compared to the third-order ones. We show that  by judiciously combining
the co-occurrences between different annotator pairs  the confusion matrices and the ground-truth
label’s prior PMF can be provably identiﬁed  under realistic conditions (e.g.  when there exists a
relatively well-trained annotator among all annotators). This is reminiscent of nonnegative matrix
theory and convex geometry [13  15]. Our approach is also naturally robust to spammers as well as
scenarios where every annotator only labels partial data. We offer two algorithms under the same
framework. The ﬁrst algorithm is algebraic  and thus is efﬁcient and suitable for handling very
large-scale crowdsourcing problems. The second algorithm offers enhanced identiﬁability guarantees 
and is able to deal with more critical cases (e.g.  when no highly reliable annotators exist)  with the
price of using a computationally more involved iterative optimization algorithm. Experiments show
that both approaches outperform a number of competitive baselines.
2 Background
The Dawid-Skene Model. Let us consider a dataset {fn}N
n=1  where fn ∈ Rd is a data sample (or 
feature vector) and N is the number of samples. Each fn belongs to one of K classes. Let yn be
the ground-truth label of the data sample fn. Suppose that there are M annotators who work on
the dataset {fn}N
n=1 and provide labels. Let Xm(fn) represent the response of the annotator m to
fn. Hence  Xm can be understood as a discrete random variable whose alphabet is {1  . . .   K}. In
crowdsourcing or ensemble learning  our goal is to estimate the true label corresponding to each item
fn from the M annotator responses. Note that in a realistic scenario  an annotator will likely to only
work on part of the dataset  since having all annotators work on all the samples is much more costly.
In 1979  Dawid and Skene proposed an intuitively pleasing model for estimating the ‘true response’
of the patients from recorded answers [6]  which is essentially a crowdsourcing/ensemble learning
problem. This model has sparked a lot of interest in the machine learning community [31  37  14 
23  28  39]. The Dawid-Skene model in essence is a naive Bayesian model [29]. In this model  the
ground-truth label of a data sample is a latent discrete random variable  Y   whose values are different
class indices. The ambient variables are the responses given by different annotators  denoted as
X1  . . .   XM   where M is the number of annotators. The key assumption in the Dawid-Skene model
is that given the ground-truth label  the responses of the annotators are conditionally independent.
Of course  the Dawid-Skene model is a simpliﬁed version of reality  but has been proven very
useful—and it has been a workhorse for crowdsourcing since its proposal.

2

Under the Dawid-Skene model  one can see that

Pr(X1 = k1  . . .   XM = kM ) =

k=1

m=1

K(cid:88)

M(cid:89)

Pr(Xm = km|Y = k)Pr(Y = k) 

(1)

where k ∈ {1  . . .   K} denotes the index of a given class  and km denotes the response of the m-th
annotator. If one deﬁnes a series of matrices Am ∈ RK×K and let
A(km  k) := Pr(Xm = km|Y = k) 

(2)
then Am ∈ RK×K can be understood as the ‘confusion matrix’ of annotator m: It contains all the
conditional probabilities of annotator m labeling a given data sample as from class km while the
ground-truth label is k. Also deﬁne a vector d ∈ RK such that d(k) := Pr(Y = k); i.e.  the prior
PMF of the ground-truth label Y . Then the crowdsourcing problem boils down to estimating Am for
m = 1  . . .   M and d.
Prior Art. In the seminal paper [6]  Dawid and Skene proposed an EM-based algorithm to estimate
Pr(Xm = km|Y = k) and Pr(Y = k). Their formulation is well-motivated from an ML viewpoint 
but also has some challenges. First  it is unknown if the model is identiﬁable  especially when there
is a large number of unrecorded responses (i.e.  missing values)—but model identiﬁcation plays an
essential role in such estimation problems [13]. Second  since the ML estimator is a nonconvex
optimization criterion  the solution quality of the EM algorithm is not easy to characterize in general.
More recently  tensor methods were proposed to identify the Dawid-Skene model [39  37]. Take the
most recent work in [37] as an example. The approach considers estimating the joint probability
Pr(Xi = ki  Xj = kj  X(cid:96) = k(cid:96)) for different triples i  j  (cid:96). Such joint PMFs can be regarded as third-
order tensors  and the confusion matrices and the prior d are latent factors of these tensors. The upshot
is that identiﬁability of Am and d can be elegantly established leveraging tensor algebra [33  25]. The
challenge  however  is that reliably estimating Pr(Xi = ki  Xj = kj  X(cid:96) = k(cid:96)) is quite hard  since it
normally needs a large number of annotator responses. Another tensor method in [39] judiciously
partitions the data and works with group statistics between three groups  which is reminiscent of the
graph statistics proposed in [1]. The method is computationally more tractable  leveraging orthogonal
tensor decomposition. Nevertheless  the challenge again lies in sample complexity: the group/graph
statistics are still third-order statistics.
3 Proposed Approach
In this section  we propose a model identiﬁcation approach that only uses second-order statistics  in
particular  pairwise co-occurrences Pr(Xi = ki  Xj = kj).
Problem Formulation. Let us consider the following pairwise joint PMF: Pr(Xm = km  X(cid:96) =
k=1 Pr(Y = k)Pr(Xm = km|Y = k)Pr(X(cid:96) = k(cid:96)|Y = k). Letting Rm (cid:96)(km  k(cid:96)) =
Pr(Xm = km  X(cid:96) = k(cid:96))  and using the matrix notations that we deﬁned  we have Rm (cid:96)(km  k(cid:96)) =

k(cid:96)) = (cid:80)K
(cid:80)K
k=1 Pr(Y = k)Pr(Xm = km|Y = k)Pr(X(cid:96) = k(cid:96)|Y = k)—or  in a more compact form:

Rm (cid:96)(km  k(cid:96)) =

d(k)Am(km  k)A(cid:96)(k(cid:96)  k) ⇐⇒ Rm (cid:96) := AmDA(cid:62)
(cid:96)  

where we have D = Diag(d)  which is a diagonal matrix. Note that Am is a confusion matrix  i.e. 
its columns are respectable probability measures. In addition  d is a prior PMF. Hence  we have

k=1

1(cid:62)Am = 1(cid:62)  Am ≥ 0  ∀ m 

1(cid:62)d = 1  d ≥ 0.

(3)

(cid:80)

In practice  Rm (cid:96)’s are not available but can be estimated via sample averaging.
Speciﬁcally 
if we are given the annotator
I [Xm(fn) = km  X(cid:96)(fn) = k(cid:96)]   where Sm (cid:96) is the index set of samples which
1|Sm (cid:96)|
both annotators m and (cid:96) have worked on. Here  I[·] is an indicator function: If the event E happens 
then I[E] = 1  and I[Ec] = 0 otherwise. It is readily seen that

responses Xm(fn) 

n∈Sm (cid:96)

then (cid:98)Rm (cid:96)(km  k(cid:96)) =

E [I(Xm(fn) = km  X(cid:96)(fn) = k(cid:96))] = Rm (cid:96)(km  k(cid:96)) 

(4)
where the expectation is taken over data samples. Note that the sample complexity for reliably
estimating Rm (cid:96) is much lower relative to that of estimating Rm n (cid:96) [39  1]  and the latter is needed

3

K(cid:88)

in tensor based methods  e.g.  [37]. To be speciﬁc  to achieve |Rm (cid:96)(km  k(cid:96)) − (cid:98)Rm (cid:96)(km  k(cid:96))| ≤ 
needed. However  in order to attain the same accuracy for (cid:98)Rm n (cid:96)(km  kn  k(cid:96))  the number of joint

with a probability greater than 1 − δ  O(−2(log 1

δ )) joint responses from annotators m and (cid:96) are

responses from annotators m n and (cid:96) is required to be atleast O(K−2(log K
number of classes (also see supplementary materials Sec. J for a short discussion).
An Algebraic Algorithm. Assume that we have obtained Rm (cid:96)’s for different pairs of m  (cid:96). We now
show how to identify Am’s and d from such second-order statistics. Let us take the estimation of
Am as an illustrative example. First  we construct a matrix Zm as follows:

δ ))  where K is the

(5)
where mt (cid:54)= m for t = 1  . . .   T (m) denote the indices of annotators who have co-labeled data
samples with annotator m  and the integer T (m) denotes the number of such annotators. Due
to the underlying model of Rm (cid:96) in (3)  we have Zm =
=

(cid:3)  

Zm =(cid:2)Rm m1  Rm m2   . . .   Rm mT (m)
(cid:104)
(cid:105) ∈ RK×KT (m). Let us deﬁne H(cid:62)

(cid:104)

AmDA(cid:62)
m =

m1

  . . .   AmDA(cid:62)
  . . .   DA(cid:62)

DA(cid:62)

T (m)

DA(cid:62)

  . . .   DA(cid:62)

m1

T (m)

Am
RK×KT (m). This leads to the model Zm = AmH(cid:62)
m. We propose to identify Am from Zm. The
key enabling postulate is that  among all annotators  some A(cid:96)’s should be diagonally dominant—if
there exist annotators who are reasonably trained. In other words  for a reasonable annotator (cid:96) 
Pr(X(cid:96) = j|Y = j) should be greater than Pr(X(cid:96) = j|Y = k) and Pr(X(cid:96) = j|Y = i) for k  i (cid:54)= j.
To see the intuition of the algorithm  consider an ideal case where for each class k  there exists an
annotator mt(k) ∈ {m1  . . .   mT (m)} such that

T (m)

m1

Pr(Xmt(k) = k|Y = k) = 1  Pr(Xmt(k) = k|Y = j) = 0 

j (cid:54)= k.

(6)

(cid:105)
(cid:105) ∈

(cid:104)

This physically means that annotator mt(k) is very good at recognizing class k and never confuses
other classes with class k. Under such circumstances  one can use the following procedure to
identify Am. First  let us normalize the columns of Zm via Zm(:  q) = Zm(:  q)/(cid:107)Zm(:  q)(cid:107)1 for
q = {1  . . .   KT (m)}. This way  we have a normalized model Zm = AmH

(cid:62)
m  where
Hm(q  :)(cid:107)Am(:  k)(cid:107)1

Am(:  k)
(cid:107)Am(:  k)(cid:107)1

Am(:  k) =

(7)
where the second equality above is because (cid:107)Am(:  k)(cid:107)1 = 1 [cf. Eq. (3)]. After normalization  it
can be veriﬁed that

= Am(:  k)  H m(q  :) =

(cid:107)Zm(:  q)(cid:107)1

.

(8)
i.e.  all the rows of H m reside in the (K − 1)-probability simplex. In addition  by the assumption
in (6)  it is readily seen that there exists Λq = {q1  . . .   qK} ⊂ {1  . . .   Lm} where Lm = KT (m)
such that

H m1 = 1  H m ≥ 0 

H m(Λq  :) = IK 

(9)

i.e.  an identity matrix is a submatrix of H m (after proper row permutations). Consequently  we have
Am = Zm(:  Λq)—i.e.  Am can be identiﬁed from Zm up to column permutations. The task also
boils down to identifying Λq. This turns out to be a well-studied task in the context of separable
nonnegative matrix factorization [16  15  13]  and an algebraic algorithm exists:

⊥(cid:98)Am(: 1:k−1)Zm(:  q)
where (cid:98)Am(:  1 : k − 1) = [Zm(: (cid:98)q1)  . . .   Zm(: (cid:98)qk−1)] and P ⊥(cid:98)Am(: 1:k−1) is a projector onto the orthogonal
complement of range((cid:98)Am(:  1 : k − 1)) and we let P ⊥(cid:98)Am(: 1:0) := I.

q∈{1 ... Lm}

  ∀k.

(10)

2

(cid:98)qk = arg max

(cid:13)(cid:13)(cid:13)P

(cid:13)(cid:13)(cid:13)2

It has been shown in [16  2] that the so-called successive projection algorithm (SPA) in Eq. (10)
identiﬁes Λq in K steps. This is a very plausible result  since the procedure admits Gram-Schmitt-like
lightweight steps and thus is quite scalable. See more details in Sec. F.1.
Each of the Am’s can be estimated from the corresponding Zm by repeatedly applying SPA  and we
call this simple procedure multiple SPA (MultiSPA) as we elaborate in Algorithm 1.

4

Of course  assuming that (6) or (9) holds per-
fectly may be too ideal. It is more likely that
there exist some annotators who are good at
recognizing certain classes  but still have some
possibilities of being confused. It is of interest
to analyze how SPA can do under such condi-
tions. Another challenge is that one may not
have Rm (cid:96) perfectly estimated  since only lim-
ited number of samples are available. It is desir-
able to understand the sample complexity of ap-
plying SPA to Dawid-Skene identiﬁcation. We
answer these two key technical questions in the
following theorem:
Theorem 1. Assume that annotators m
least S samples ∀t ∈
and t co-label at

Algorithm 1 MultiSPA

(cid:96)1 norm;

for m = 1 to M do

Input: Annotator Responses {Xm(fn)}.

Output: (cid:98)Am for m = 1  . . .   M  (cid:98)d.
estimate second order statistics (cid:98)Rm (cid:96);
construct (cid:98)Zm and normalize columns to unit
estimate (cid:98)Am using Eq. (10);
ﬁx permutation mismatch between (cid:98)Am and (cid:98)A(cid:96)
estimate (cid:98)D = (cid:98)A−1
m Rm (cid:96)((cid:98)A(cid:62)
extract the prior (cid:98)d = diag((cid:98)D).

end for
for all m (cid:54)= (cid:96);

(cid:96) )−1 (and take av-

{m1  . . .   mT (m)}  and that (cid:98)Zm is constructed
using (cid:98)Rm mT (m)’s according to Eq. (5). Also assume that the constructed (cid:98)Zm satisﬁes (cid:107)(cid:98)Zm(:

  l)(cid:107)1 ≥ η ∀l ∈ {1  . . . KT (m)}  where η ∈ (0  1]. Suppose that rank(Am) = rank(D) = K
for m = 1  . . .   M  and that for every class index k ∈ {1  . . .   K}  there exists an annotator
mt(k) ∈ {m1  . . .   mT (m)} such that

erage over all pairs (m  (cid:96)) if needed).;

K(cid:88)

j=1

(11)

(cid:16)

max

√

√

Pr(Xmt(k) = k|Y = k) ≥ (1 − )

Pr(Xmt(k) = k|Y = j) 

(cid:16)

min
Π

  with

Kκ2(Am) max

(cid:107)(cid:98)AmΠ − Am(cid:107)2 ∞

K−1κ−3(Am) (cid:112)ln(1/δ)(σmax(Am)
σmax(Am) (cid:112)ln(1/δ)(

Sη)−1(cid:17)(cid:17)
where  ∈ [0  1]. Then  if  ≤ O(cid:16)
probability greater than 1 − δ  the SPA algorithm in (10) can estimate an (cid:98)Am such that
Sη)−1(cid:17)(cid:19)
(cid:18)√
(cid:16)
(cid:17) ≤ O
In the above Theorem  the assumption (cid:107)(cid:98)Zm(:  l)(cid:107)1 ≥ η means that the proposed algorithm favors
cases where more co-occurrences are observed  since (cid:98)Zm’s elements are averaged number of co-

(12)
where Π ∈ RK×K is a permutation matrix  (cid:107)Y (cid:107)2 ∞ = max(cid:96) (cid:107)Y (:  (cid:96))(cid:107)2  σmax(Am) is the largest
singular value of Am  and κ(Am) is the condition number of Am.

occurrences—which makes a lot of sense. In addition  Eq. (11) relaxes the ideal assumption in (6) 
allowing the ‘good annotator’ mt(k) to confuse class j (cid:54)= k with class k up to a certain probability 
thereby being more realistic. The proof of Theorem 1 is reminiscent of the noise robustness of the
SPA algorithm [16  2]; see the supplementary materials (Sec. F.1). A direct corollary is as follows:

Corollary 1. Assume that the conditions in Theorem 1 hold for (cid:98)Zm and Am  ∀m ∈ {1  . . .   M}.
Then  the estimation error bound in (12) holds for every MultiSPA-output (cid:98)Am  ∀m ∈ {1  . . .   M}.

Theorem 1 and Corollary 1 are not entirely surprising due to the extensive research on SPA-like
algorithms [2  16  10  30  4]. The implication for crowdsourcing  however  is quite intriguing. First 
one can see that if an annotator m does not label all the data samples  it does not necessarily hurt
the model identiﬁability—as long as annotator m has co-labeled some samples with a number of
other annotators  identiﬁcation of Am is possible. Second  assume that there exists a well-trained
annotator m(cid:63) whose confusion matrix is diagonally dominant  then for every annotator m who has
co-labeled samples with annotator m(cid:63)  the matrix H m can easily satisfy (11) by letting mt(k) = m(cid:63)
for all k. In practice  one would not know who is m(cid:63)—otherwise the crowdsourcing problem would
be trivial. However  one can design a dispatch strategy such that every pair of annotators m and (cid:96)
co-label a certain amount of data. This way  it guarantees that Am(cid:63) appears in everyone else’s Hm
and thus ensures identiﬁability of all Am’s for m (cid:54)= m(cid:63). This insight may shed some light on how to
effectively dispatch data to annotators.
Another interesting question to ask is does having more annotators help? Intuitively  having more
annotators should help: If one has more rows in H m  then it is more likely that some rows approach

5

ρ

K

log

(cid:17)(cid:17)

(cid:16) K

(cid:16) ε−2(K−1)

the vertices of the probability simplex—which can then enable SPA. We use the following simpliﬁed
generative model and theorem to formalize the intuition:
Theorem 2. Let ρ > 0  ε > 0  and assume that the rows of H m are generated within the
(K − 1)-probability simplex uniformly at random. If the number of annotators satisﬁes M ≥
  then  with probability greater than or equal to 1 − ρ  there exist rows of
Ω
H m indexed by q1  . . . qK such that (cid:107)H m(qk  :) − e(cid:62)
Note that Theorem 2 implies (11) under proper ε and —and thus having more annotators indeed
helps identify the model. The above can be shown by utilizing the Chernoff-Hoeffding inequality 
and the detailed proof can be found in the supplementary materials (Sec. G).

After obtaining (cid:98)Am’s  d can be estimated via various ways—see the supplementary materials in Sec.
D. Using (cid:98)d and (cid:98)Am’s together  ML and MAP estimators for the true labels can be built up [37].

k(cid:107)2 ≤ ε  k = 1  . . .   K.

4

Identiﬁability-enhanced Algorithm

The MultiSPA algorithm is intuitive and lightweight  and is effective as we will show in the experi-
ments. One concern is that perhaps the assumption in (11) may be violated in some cases. In this
section  we propose another model identiﬁcation algorithm that is potentially more robust to critical
scenarios. Speciﬁcally  we consider the following feasibility problem:

ﬁnd {Am}M

m=1  D

subject to Rm (cid:96) = AmDA(cid:62)

(cid:96)   ∀m  (cid:96) ∈ {1  . . .   M}

1(cid:62)Am = 1(cid:62)  Am ≥ 0  ∀m  1(cid:62)d = 1  d ≥ 0.

(13a)
(13b)
(13c)

The criterion in (13) seeks confusion matrices and a prior PMF that ﬁt the available second-order
statistics. The constraints in (13c) reﬂect the fact that the columns of Am’s are conditional PMFs and
the prior d is also a PMF.
To proceed  let us ﬁrst introduce the following notion from convex geometry [13  27]:
Deﬁnition 1. (Sufﬁciently Scattered) A nonnegative matrix H ∈ RL×K is sufﬁciently scattered if 1)
cone{H(cid:62)} ⊇ C  and 2) cone{H(cid:62)}∗ ∩ bdC∗ = {λek | λ ≥ 0  k = 1  ...  K}. Here  C = {x|x(cid:62)1 ≥
√
K − 1(cid:107)x(cid:107)2}  C∗ = {x|x(cid:62)1 ≥ (cid:107)x(cid:107)2}. In addition  cone{H(cid:62)} = {x|x = H(cid:62)θ  ∀θ ≥ 0}
and cone{H(cid:62)}∗ = {y|x(cid:62)y ≥ 0  ∀x ∈ cone{H(cid:62)}} are the conic hull of H(cid:62) and its dual cone 
respectively  and bd is the boundary of a closed set.

The sufﬁciently scattered condition has recently emerged in convex geometry-based matrix factoriza-
tion [27  12]. This condition models how the rows of H are spread in the nonnegative orthant. In
principle  the sufﬁciently scattered condition is much easier to be satisﬁed relative to the condition as
in (9)  or  the so-called separability condition under the context of nonnegative matrix factorization
[9  16]. H satisfying the separability condition is the extreme case  meaning that cone{H(cid:62)} = RK
+ .
However  the sufﬁciently scattered condition only requires C ⊆ cone{H(cid:62)}—which is naturally much
more relaxed; also see [13] and the supplementary materials for detailed illustrations (Sec. E).
Regarding identiﬁability of A1  . . .   AM and d  we have the following result:
Theorem 3. Assume that rank(D) = rank(Am) = K for all m = 1  . . .   M  and that there
exist two subsets of the annotators  indexed by P1 and P2  where P1 ∩ P2 = ∅ and P1 ∪ P2 ⊆
{1  . . .   M}. Suppose that from P1 and P2 the following two matrices can be constructed: H (1) =
(cid:96)|P2|](cid:62)  where mt ∈ P1 and (cid:96)j ∈ P2. Furthermore 
[A(cid:62)
assume that i) both H (1) and H (2) are sufﬁciently scattered; ii) all Rmt (cid:96)j ’s for mt ∈ P1 and
(cid:96)j ∈ P2 are available; and iii) for every m /∈ P1 ∪ P2 there exists a Rm r available  where
r ∈ P1 ∪ P2. Then  solving Problem (13) recovers Am for m = 1  . . .   M and D = Diag(d) up to
identical column permutation.

m|P1|](cid:62)  H (2) = [A(cid:62)

  . . .   A(cid:62)

  . . .   A(cid:62)

m1

(cid:96)1

The proof of Theorem 3 is relegated to the supplementary results (Sec. H). Note that the theorem
holds under the the existence of P1 and P2  but there is no need to know the sets a priori. Generally

6

(cid:16) K(K−1)

(cid:17)(cid:17)

ρ

speaking  a ‘taller’ matrix H (i) would have a better chance to have its rows sufﬁciently spread in the
nonnegative orthant under the same intuition of Theorem 2. Thus  having more annotators also helps
to attain the sufﬁciently scattered condition. Nevertheless  formally showing the relationship between
the number of annotators and H (i) for i = 1  2 being sufﬁciently scattered is more challenging than
the case in Theorem 2  since the sufﬁciently scattered condition is a bit more abstract relative to the
separability condition—the latter speciﬁcally assumes ek’s exist as rows of H (i) while the former
depends on the ‘shape’ of the conic hull of (H (i))(cid:62)  which contains an inﬁnite number of cases.
Towards this end  let us ﬁrst deﬁne the following notion:

Deﬁnition 2. Assume that there exist(cid:102)H ∈ RL×K such that(cid:102)H is sufﬁciently scattered. Also assume
V is the row index set of(cid:102)H such that(cid:102)H(V  :) collects the extreme rays of cone{(cid:102)H (cid:62)}. If there exist
row indices (cid:96)v ∈ {1  . . .   L} for all v ∈ V  such that (cid:107)(cid:102)H(v  :) − H((cid:96)v  :)(cid:107)2 ≤ ε  then H ∈ RL×K

Kα2(K−2)ε2 log

(cid:16) (K−1)2

is called ε-sufﬁciently scattered.
One can see that an ε-sufﬁciently scattered matrix is sufﬁciently scattered when ε → 0. With this
deﬁnition  we show the following theorem:
Theorem 4. Let ρ > 0  α
2 > ε > 0   and assume that the rows of H (1) and H (2) are generated from
RK uniformly at random. If the number of annotators satisﬁes M ≥ Ω
 
where α = 1 for K = 2  α = 2/3 for K = 3 and α = 1/2 for K > 3  then with probability greater
than or equal to 1 − ρ  H (1) and H (2) are ε-sufﬁciently scattered.
The proof of Theorem 4 is relegated to the supplementary materials (Sec. I). One can see that to
satisfy ε-sufﬁciently scattered condition  M is smaller than that in Theorem 2. Conditions i)-iii)
in Theorem 3 and Theorem 4 together imply that if we have enough annotators  and if many pairs
co-label a certain number of data  then it is quite possible that one can identify the Dawid-Skene
model via simply ﬁnding a feasible solution to (13). This feasibility problem is nonconvex  but can
be effectively approximated; see the supplementary materials (Sec. C). In a nutshell  we reformulate
the problem as a Kullback-Leibler (KL) divergence-based constrained ﬁtting problem and handle it
using alternating optimization. Since nonconvex optimization relies on initialization heavily  we use
MultiSPA to initialize the ﬁtting stage—which we will refer to as the MultiSPA-KL algorithm.
5 Experiments
Baselines. The performance of the proposed approach is compared with a number of competitive
baselines  namely  Spectral-D&S [39]  TensorADMM [37]  and KOS [22]  EigRatio [5]  GhoshSVD
[14] and MinmaxEntropy [40]. The performance of the Majority Voting scheme and the Majority
Voting initialized Dawid-Skene (MV-D&S) estimator [6] are also presented. We also use MultiSPA to
initialize EM algorithm (named as MultiSPA-D&S). Note that KOS  EigRatio and MinmaxEntropy
work with more complex models relative to the Dawid-Skene model  but are considered as good
baselines for the crowdsourcing/ensemble learning tasks. After identifying the model parameters  we
construct a MAP predictor following [37] and observe the result. The algorithms are coded in Matlab.
Synthetic-data Simulations. Due to page limitations  synthetic data experiments demonstrating
model identiﬁability of the proposed algorithms are presented in the supplementary materials (Sec. A).
Integrating Machine Classiﬁers. We employ different UCI datasets (https://archive.ics.
uci.edu/ml/datasets.html; details in Sec. B). For each of the datasets under test  we use a
collection of different classiﬁcation algorithms to annotate the data samples. Different classiﬁca-
tion algorithms from the MATLAB machine learning toolbox (https://www.mathworks.com/
products/statistics.html) such as various k-nearest neighbour classiﬁers  support vector ma-
chine classiﬁers  and decision tree classiﬁers are employed to serve as our machine annotators. In
order to train the annotators  we use 20% of the samples to act as training data. After the data samples
are trained  we use the annotators to label the unseen data samples. In practice  not all samples
are labeled by an annotator due to several factors such as annotator capacity  difﬁculty of the task 
economical issues and so on. To simulate such a scenario  each of the trained algorithms is allowed
to label a data sample with probability p ∈ (0  1]. We test the performance of all the algorithms under
different p’s—and a smaller p means a more challenging scenario. All the results are averaged from
10 random trials.
Table 1 shows the classiﬁcation error of the algorithms under test. Since GhoshSVD and EigenRatio
works only on binary tasks  they are not evaluated for the Nursery dataset where K = 4. The ‘single

7

Table 1: Classiﬁcation Error (%) on UCI Datasets; see runtime tabulated in Sec. B.

Nursery
Algorithms
p = 0.5
p = 1
4.54
2.83
MultiSPA
4.26
2.72
MultiSPA-KL
4.44
2.82
MultiSPA-D&S
37.2
3.14
Spectral-D&S
7.26
17.97
TensorADMM
66.48
2.92
MV-D&S
26.31
3.63
Minmax-entropy
N/A
N/A
EigenRatio
6.07
4.21
KOS
N/A
N/A
Ghosh-SVD
4.83
Majority Voting 2.94
3.94
Single Best
N/A
15.65 N/A
Single Worst

p = 0.2
17.96
13.06
13.39
44.29
19.78
66.61
11.09
N/A
13.48
N/A
19.75
N/A
N/A

Mushroom
p = 0.5
0.293
0.152
0.194
0.198
0.237
47.99
0.163
0.329
0.576
0.329
0.566
N/A
N/A

p = 0.2
6.35
5.89
6.17
6.17
6.18
48.63
8.14
5.97
6.42
5.97
6.57
N/A
N/A

p = 1
0.02
0.00
0.00
0.00
0.06
0.00
0.00
0.06
0.06
0.06
0.14
0.00
7.22

Adult
p = 0.5
p = 1
16.05
15.71
15.98
15.66
16.29
15.74
16.31
15.72
16.05
15.72
75.21
15.76
16.92
16.11
16.28
15.84
24.97
17.19
16.28
15.84
15.75
16.21
16.23 N/A
19.27 N/A

p = 0.2
17.66
17.63
23.88
23.97
25.08
75.13
15.64
17.69
38.29
17.71
20.57
N/A
N/A

Table 2: Classiﬁcation Error (%) and Run-time (sec) : AMT Datasets

Algorithms

TREC

Bluebird

RTE

Web

Dog

(%) Error (sec) Time (%) Error (sec) Time (%) Error (sec) Time (%) Error (sec) Time (%) Error (sec) Time
31.47
MultiSPA
29.23
MultiSPA-KL
29.84
MultiSPA-D&S
29.58
Spectral-D&S
N/A
TensorADMM
30.02
MV-D&S
Minmax-entropy 91.61
43.95
EigenRatio
51.95
KOS
43.03
GhoshSVD
Majority Voting 34.85

50.68
536.89
53.14
919.98
N/A
3.20
352.36
1.48
9.98
11.62
N/A

0.07
15.88
0.12
51.16
603.93
0.04
7.22
N/A
0.13
N/A
N/A

0.54
12.34
0.84
179.92
N/A
0.28
26.61
N/A
0.31
N/A
N/A

17.09
15.48
16.11
17.84
17.96
15.86
16.23
N/A
31.84
N/A
17.91

13.88
11.11
12.03
12.03
12.03
12.03
8.33
27.77
11.11
27.77
21.29

0.07
1.94
0.09
1.97
2.74
0.02
3.43
0.02
0.01
0.01
N/A

8.75
7.12
7.12
7.12
N/A
7.25
7.50
9.01
39.75
49.12
10.31

0.28
17.06
0.32
6.40
N/A
0.07
9.10
0.03
0.03
0.03
N/A

15.22
14.58
15.11
16.88
N/A
16.02
11.51
N/A
42.93
N/A
26.93

best’ and ‘single worst’ rows correspond to the results of using the classiﬁers individually when
p = 1  as references. The best and second-best performing algorithms are highlighted in the table.
One can see that the proposed methods are quite promising for this experiment. Both algorithms
largely outperform the tensor based methods TensorADMM and Spectral-D&S in this case  perhaps
because the limited number of available samples makes the third-order statistics hard to estimate. It
is also observed that the proposed algorithms enjoy favorable runtime;s ee supplementary materials
(cf. Table 8 in Sec. B). Using the MultiSPA to initialize EM (i.e. MultiSPA-D&S) also works well 
which offers another viable option that strikes a good balance between runtime and accuracy.
Amazon Mechanical Turk Crowdsourcing Data. In this section  the performance of the proposed
algorithms are evaluated using the Amazon Mechanical Turk (AMT) data (https://www.mturk.
com) in which human annotators label various classiﬁcation tasks. Data description is given in the
supplementary materials Sec. B. Table 2 shows the classiﬁcation error and the runtime performance
of the algorithms under test. One can see that MultiSPA has a very favorable execution time 
because it is a Gram-Schmitt-like algorithm. MultiSPA-KL uses more time  because it is an iterative
optimization method—with better accuracy paid off. Since TensorADMM algorithm does not scale
well  the results are not reported for very large datasets (i.e.  TREC and RTE). Similar as before 
since Web and Dog are multi-class datasets  EigenRatio and GhoshSVD are not applicable. From
the results  it can be seen that the proposed algorithms outperform many existing crowdsourcing
algorithms in both classiﬁcation accuracy and runtime. In particular  one can see that the algebraic
algorithm MultiSPA gives very similar results compared to the computationally much more involved
algorithms. This shows the potential for its application in big data crowdsourcing.
6 Conclusion
In this work  we have revisited the classic Dawid-Skene model for multi-class crowdsourcing. We
have proposed a second-order statistics-based approach that guarantees identiﬁability of the model
parameters  i.e.  the confusion matrices of the annotators and the label prior. The proposed method
naturally admits lower sample complexity relative to existing methods that utilize tensor algebra
to ensure model identiﬁability. The proposed approach also has an array of favorable features. In
particular  our framework enables a lightweight algebraic algorithm  which is reminiscent of the
Gram-Schmitt-like SPA algorithm for nonnegative matrix factorization. We have also proposed a
coupled and constrained matrix factorization criterion that enjoys enhanced-identiﬁability  as well as
an alternating optimization algorithm for handling the identiﬁcation problem. Real-data experiments
show that our proposed algorithms are quite promising for integrating crowdsourced labeling.

8

References
[1] Anandkumar  A.  Ge  R.  Hsu  D.  and Kakade  S. M. A tensor approach to learning mixed
membership community models. The Journal of Machine Learning Research  15(1):2239–2312 
2014.

[2] Arora  S.  Ge  R.  Halpern  Y.  Mimno  D.  Moitra  A.  Sontag  D.  Wu  Y.  and Zhu  M. A
practical algorithm for topic modeling with provable guarantees. In Proceedings of ICML  2013.

[3] Bertsekas  D. P. Nonlinear programming. Athena Scientiﬁc  1999.

[4] Chan  T.-H.  Ma  W.-K.  Ambikapathi  A.  and Chi  C.-Y. A simplex volume maximization
framework for hyperspectral endmember extraction. IEEE Trans. Geosci. Remote Sens.  49(11):
4177 –4193  Nov. 2011.

[5] Dalvi  N.  Dasgupta  A.  Kumar  R.  and Rastogi  V. Aggregating crowdsourced binary ratings.
In Proceedings of the 22nd International Conference on World Wide Web  pp. 285–294  New
York  NY  USA  2013. ACM.

[6] Dawid  A. P. and Skene  A. M. Maximum likelihood estimation of observer error-rates using

the em algorithm. Applied statistics  pp. 20–28  1979.

[7] Deng  J.  Dong  W.  Socher  R.  Li  L.  and and. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition  pp. 248–255 
June 2009.

[8] Dietterich  T. G. Ensemble methods in machine learning. In International Workshop on Multiple

Classiﬁer Systems  pp. 1–15. Springer  2000.

[9] Donoho  D. and Stodden  V. When does non-negative matrix factorization give a correct
decomposition into parts? In Advances in Neural Information Processing Systems  volume 16 
2003.

[10] Fu  X.  Ma  W.-K.  Chan  T.-H.  and Bioucas-Dias  J. M. Self-dictionary sparse regression for
hyperspectral unmixing: Greedy pursuit and pure pixel search are related. IEEE J. Sel. Topics
Signal Process.  9(6):1128–1141  2015.

[11] Fu  X.  Huang  K.  Yang  B.  Ma  W.-K.  and Sidiropoulos  N. D. Robust volume minimization-
based matrix factorization for remote sensing and document clustering. IEEE Trans. Signal
Process.  64(23):6254–6268  2016.

[12] Fu  X.  Huang  K.  and Sidiropoulos  N. D. On identiﬁability of nonnegative matrix factorization.

IEEE Signal Process. Lett.  25(3):328–332  2018.

[13] Fu  X.  Huang  K.  Sidiropoulos  N. D.  and Ma  W.-K. Nonnegative matrix factorization for
signal and data analytics: Identiﬁability  algorithms  and applications. IEEE Signal Process.
Mag.  36(2):59–80  March 2019.

[14] Ghosh  A.  Kale  S.  and McAfee  P. Who moderates the moderators?: crowdsourcing abuse
detection in user-generated content. In Proceedings of the 12th ACM conference on Electronic
commerce  pp. 167–176. ACM  2011.

[15] Gillis  N. The why and how of nonnegative matrix factorization. Regularization  Optimization 

Kernels  and Support Vector Machines  12:257  2014.

[16] Gillis  N. and Vavasis  S. Fast and robust recursive algorithms for separable nonnegative matrix

factorization. IEEE Trans. Pattern Anal. Mach. Intell.  36(4):698–714  April 2014.

[17] Huang  K.  Sidiropoulos  N.  and Swami  A. Non-negative matrix factorization revisited:
Uniqueness and algorithm for symmetric decomposition. IEEE Trans. Signal Process.  62(1):
211–224  2014.

[18] Huang  K.  Sidiropoulos  N. D.  and Liavas  A. P. A ﬂexible and efﬁcient algorithmic framework
for constrained matrix and tensor factorization. IEEE Trans. Signal Process.  64(19):5052–5065 
2016.

9

[19] Huang  K.  Fu  X.  and Sidiropoulos  N. D. Learning hidden markov models from pairwise

co-occurrences with applications to topic modeling. In Proceedings of ICML 2018  2018.

[20] Jonker  R. and Volgenant  T. Improving the hungarian assignment algorithm. Operations

Research Letters  5(4):171–175  1986.

[21] Karger  D. R.  Oh  S.  and Shah  D. Budget-optimal crowdsourcing using low-rank matrix
approximations. In 2011 49th Annual Allerton Conference on Communication  Control  and
Computing (Allerton)  pp. 284–291  Sep. 2011.

[22] Karger  D. R.  Oh  S.  and Shah  D. Efﬁcient crowdsourcing for multi-class labeling. ACM

SIGMETRICS Performance Evaluation Review  41(1):81–92  2013.

[23] Karger  D. R.  Oh  S.  and Shah  D. Budget-optimal task allocation for reliable crowdsourcing

systems. Operations Research  62(1):1–24  2014.

[24] Kittur  A.  Chi  E. H.  and Suh  B. Crowdsourcing user studies with mechanical turk. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems  pp. 453–456.
ACM  2008.

[25] Kolda  T. G. and Bader  B. W. Tensor decompositions and applications. SIAM review  51(3):

455–500  2009.

[26] Lease  M. and Kazai.  G. Overview of the trec 2011 crowdsourcing track. 2011.

[27] Lin  C.-H.  Ma  W.-K.  Li  W.-C.  Chi  C.-Y.  and Ambikapathi  A. Identiﬁability of the simplex
volume minimization criterion for blind hyperspectral unmixing: The no-pure-pixel case. IEEE
Trans. Geosci. Remote Sens.  53(10):5530–5546  Oct 2015.

[28] Liu  Q.  Peng  J.  and Ihler  A. T. Variational inference for crowdsourcing. In Advances in

Neural Information Processing Systems  pp. 692–700  2012.

[29] Murphy  K. P. Machine Learning: A Probabilistic Perspective. The MIT Press  2012.

[30] Nascimento  J. and Bioucas-Dias  J. Vertex component analysis: A fast algorithm to unmix

hyperspectral data. IEEE Trans. Geosci. Remote Sens.  43(4):898–910  2005.

[31] Raykar  V. C.  Yu  S.  Zhao  L. H.  Valadez  G. H.  Florin  C.  Bogoni  L.  and Moy  L. Learning

from crowds. Journal of Machine Learning Research  11(Apr):1297–1322  2010.

[32] Razaviyayn  M.  Hong  M.  and Luo  Z.-Q. A uniﬁed convergence analysis of block successive
minimization methods for nonsmooth optimization. SIAM Journal on Optimization  23(2):
1126–1153  2013.

[33] Sidiropoulos  N. D.  De Lathauwer  L.  Fu  X.  Huang  K.  Papalexakis  E. E.  and Faloutsos  C.
Tensor decomposition for signal processing and machine learning. IEEE Trans. Signal Process. 
65(13):3551–3582  2017.

[34] Snow  R.  O’Connor  B.  Jurafsky  D.  and Ng  A. Y. Cheap and fast—but is it good?: evaluating
non-expert annotations for natural language tasks. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing  pp. 254–263. Association for Computational
Linguistics  2008.

[35] Stein  P. A Note on the Volume of a Simplex. The American Mathematical Monthly  73(3) 

1966. doi: 10.2307/2315353.

[36] Stephane Boucheron  Gabor Lugosi  O. B. Concentration Inequalities  2004. URL: http:

//www.econ.upf.edu/~lugosi/mlss_conc.pdf.

[37] Traganitis  P. A.  Pages-Zamora  A.  and Giannakis  G. B. Blind multiclass ensemble classiﬁca-

tion. IEEE Trans. Signal Process.  66(18):4737–4752  2018.

[38] Welinder  P.  Branson  S.  Perona  P.  and Belongie  S. J. The multidimensional wisdom of

crowds. In Advances in Neural Information Processing Systems  pp. 2424–2432  2010.

10

[39] Zhang  Y.  Chen  X.  Zhou  D.  and Jordan  M. I. Spectral methods meet em: A provably
optimal algorithm for crowdsourcing. In Advances in Neural Information Processing Systems 
pp. 1260–1268  2014.

[40] Zhou  D.  Liu  Q.  Platt  J.  and Meek  C. Aggregating ordinal labels from crowds by minimax

conditional entropy. In Proceedings of ICML  volume 32  pp. 262–270  2014.

11

,Shahana Ibrahim
Xiao Fu
Nikolaos Kargas
Kejun Huang