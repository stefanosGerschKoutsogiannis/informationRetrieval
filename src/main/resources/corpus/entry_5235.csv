2012,Stochastic Gradient Descent with Only One Projection,Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization  most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g.  positive semidefinite cone)  the projection step can be computationally expensive  making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead  only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability  the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization  and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function.,Stochastic Gradient Descent with

Only One Projection

Mehrdad Mahdavi†  Tianbao Yang‡  Rong Jin†  Shenghuo Zhu(cid:63)  and Jinfeng Yi†
†Dept. of Computer Science and Engineering  Michigan State University  MI  USA

‡Machine Learning Lab  GE Global Research  CA  USA

†{mahdavim rongjin yijinfen}@msu.edu ‡tyang@ge.com (cid:63)zsh@nec-labs.com

(cid:63)NEC Laboratories America  CA  USA

Abstract

Although many variants of stochastic gradient descent have been proposed for
large-scale convex optimization  most of them require projecting the solution at
each iteration to ensure that the obtained solution stays within the feasible domain.
For complex domains (e.g.  positive semideﬁnite cone)  the projection step can
be computationally expensive  making stochastic gradient descent unattractive for
large-scale optimization problems. We address this limitation by developing novel
stochastic optimization algorithms that do not need intermediate projections. In-
stead  only one projection at the last iteration is needed to obtain a feasible solution
√
in the given domain. Our theoretical analysis shows that with a high probability 
the proposed algorithms achieve an O(1/
T ) convergence rate for general con-
vex optimization  and an O(ln T /T ) rate for strongly convex optimization under
mild conditions about the domain and the objective function.

1

Introduction

With the increasing amount of data that is available for training  it becomes an urgent task to devise
efﬁcient algorithms for optimization/learning problems with unprecedented sizes. Online learning
algorithms  such as celebrated Stochastic Gradient Descent (SGD) [16  2] and its online counterpart
Online Gradient Descent (OGD) [22]  despite of their slow rate of convergence compared with the
batch methods  have shown to be very effective for large scale and online learning problems  both
theoretically [16  13] and empirically [19]. Although a large number of iterations is usually needed
to obtain a solution of desirable accuracy  the lightweight computation per iteration makes SGD
attractive for many large-scale learning problems.
To ﬁnd a solution within the domain K that optimizes the given objective function f (x)  SGD
computes an unbiased estimate of the gradient of f (x)  and updates the solution by moving it in
the opposite direction of the estimated gradient. To ensure that the solution stays within the domain
K  SGD has to project the updated solution back into the K at every iteration. Although efﬁcient
algorithms have been developed for projecting solutions into special domains (e.g.  simplex and (cid:96)1
ball [6  14]); for complex domains  such as a positive semideﬁnite (PSD) cone in metric learning
and bounded trace norm matrices in matrix completion (more examples of complex domains can
be found in [10] and [11])  the projection step requires solving an expensive convex optimization 
leading to a high computational cost per iteration and consequently making SGD unappealing for
large-scale optimization problems over such domains. For instance  projecting a matrix into a PSD
cone requires computing the full eigen-decomposition of the matrix  whose complexity is cubic in
the size of the matrix.
The central theme of this paper is to develop a SGD based method that does not require projection
at each iteration. This problem was ﬁrst addressed in a very recent work [10]  where the authors
extended Frank-Wolfe algorithm [7] for online learning. But  one main shortcoming of the algo-

1

rithm proposed in [10] is that it has a slower convergence rate (i.e.  O(T −1/3)) than a standard
SGD algorithm (i.e.  O(T −1/2)). In this work  we demonstrate that a properly modiﬁed SGD algo-
rithm can achieve the optimal convergence rate of O(T −1/2) using only ONE projection for general
stochastic convex optimization problem. We further develop an SGD based algorithm for strongly
convex optimization that achieves a convergence rate of O(ln T /T )  which is only a logarithmic
factor worse than the optimal rate [9]. The key idea of both algorithms is to appropriately penalize
the intermediate solutions when they are outside the domain. With an appropriate design of penal-
close to the domain K  even without intermediate projections. As a result  the ﬁnal feasible solution

ization mechanism  the average solution (cid:98)xT obtained by the SGD after T iterations will be very
(cid:101)xT can be obtained by projecting(cid:98)xT into the domain K  the only projection that is needed for the

entire algorithm. We note that our approach is very different from the previous efforts in developing
projection free convex optimization algorithms (see [8  12  11] and references therein)  where the
key idea is to develop appropriate updating procedures to restore the feasibility of solutions at every
iteration.
We close this section with a statement of contributions and main results made by the present work:
• We propose a stochastic gradient descent algorithm for general convex optimization that in-
troduces a Lagrangian multiplier to penalize the solutions outside the domain and performs
primal-dual updating. The proposed algorithm achieves the optimal convergence rate of
O(1/
• We propose a stochastic gradient descent algorithm for strongly convex optimization that
constructs the penalty function using a smoothing technique. This algorithm attains an
O(ln T /T ) convergence rate with only one projection.

T ) with only one projection;

√

2 Related Works

Generally  the computational complexity of the projection step in SGD has seldom been taken into
account in the literature. Here  we brieﬂy review the previous works on projection free convex op-
timization  which is closely related to the theme of this study. For some speciﬁc domains  efﬁcient
algorithms have been developed to circumvent the high computational cost caused by projection
step at each iteration of gradient descent methods. The main idea is to select an appropriate direc-
tion to take from the current solution such that the next solution is guaranteed to stay within the
domain. Clarkson [5] proposed a sparse greedy approximation algorithm for convex optimization
over a simplex domain  which is a generalization of an old algorithm by Frank and Wolfe [7] (a.k.a
conditional gradient descent [3]). Zhang [21] introduced a similar sequential greedy approximation
algorithm for certain convex optimization problems over a domain given by a convex hull. Hazan [8]
devised an algorithm for approximately maximizing a concave function over a trace norm bounded
PSD cone  which only needs to compute the maximum eigenvalue and the corresponding eigenvec-
tor of a symmetric matrix. Ying et al. [20] formulated the distance metric learning problems into
eigenvalue maximization and proposed an algorithm similar to [8].
Recently  Jaggi [11] put these ideas into a general framework for convex optimization with a gen-
eral convex domain. Instead of projecting the intermediate solution into a complex convex domain 
Jaggi’s algorithm solves a linearized problem over the same domain. He showed that Clark’s algo-
rithm   Zhang’s algorithm and Hazan’s algorithm discussed above are special cases of his general
algorithm for special domains. It is important to note that all these algorithms are designed for batch
optimization  not for stochastic optimization  which is the focus of this work.
Our work is closely related to the online Frank-Wolfe (OFW) algorithm proposed in [10]. It is a
projection free online learning algorithm  built on the the assumption that it is possible to efﬁciently
minimize a linear function over the complex domain. One main shortcoming of the OFW algorithm
is that its convergence rate for general stochastic optimization is O(T −1/3)  signiﬁcantly slower than
that of a standard stochastic gradient descent algorithm (i.e.  O(T −1/2)). It achieves a convergence
rate of O(T −1/2) only when the objective function is smooth  which unfortunately does not hold
for many machine learning problems where either a non-smooth regularizer or a non-smooth loss
function is used. Another limitation of OFW is that it assumes a linear optimization problem over
the domain K can be solved efﬁciently. Although this assumption holds for some speciﬁc domains
as discussed in [10]  but in many settings of practical interest  this may not be true. The proposed
algorithms address the two limitations explicitly. In particular  we show that how two seemingly
different modiﬁcations of the SGD can be used to avoid performing expensive projections with
similar convergency rates as the original SGD method.

2

3 Preliminaries

Throughout this paper  we consider the following convex optimization problem:

min
x∈K f (x) 

(1)
where K is a bounded convex domain. We assume that K can be characterized by an inequality
constraint and without loss of generality is bounded by the unit ball  i.e. 

K = {x ∈ Rd : g(x) ≤ 0} ⊆ B = {x ∈ Rd : (cid:107)x(cid:107)2 ≤ 1} 

(2)
where g(x) is a convex constraint function. We assume that K has a non-empty interior  i.e.  there
exists x such that g(x) < 0 and the optimal solution x∗ to (1) is in the interior of the unit ball B  i.e. 
(cid:107)x∗(cid:107)2 < 1. Note that when a domain is characterized by multiple convex constraint functions  say
gi(x) ≤ 0  i = 1  . . .   m  we can summarize them into one constraint g(x) ≤ 0  by deﬁning g(x) as
g(x) = max1≤i≤m gi(x).
To solve the optimization problem in (1)  we assume that the only information available to the al-
gorithm is through a stochastic oracle that provides unbiased estimates of the gradient of f (x).
More precisely  let ξ1  . . .   ξT be a sequence of independently and identically distributed (i.i.d)
random variables sampled from an unknown distribution P . At each iteration t  given solu-

tion xt  the oracle returns (cid:101)∇f (xt; ξt)  an unbiased estimate of the true gradient ∇f (xt)  i.e. 
Eξt[(cid:101)∇f (xt  ξt)] = ∇f (xt). The goal of the learner is to ﬁnd an approximate optimal solution

by making T calls to this oracle.
Before proceeding  we recall a few deﬁnitions from convex analysis [17].
Deﬁnition 1. A function f (x) is a G-Lipschitz continuous function w.r.t a norm (cid:107) · (cid:107)  if

|f (x1) − f (x2)| ≤ G(cid:107)x1 − x2(cid:107) ∀x1  x2 ∈ B.

(3)
In particular  a convex function f (x) with a bounded (sub)gradient (cid:107)∂f (x)(cid:107)∗ ≤ G is G-Lipschitz
continuous  where (cid:107) · (cid:107)∗ is the dual norm to (cid:107) · (cid:107).
Deﬁnition 2. A convex function f (x) is β-strongly convex w.r.t a norm (cid:107)·(cid:107) if there exists a constant
β > 0 (often called the modulus of strong convexity) such that  for any α ∈ [0  1]  it holds:

f (αx1 + (1 − α)x2) ≤ αf (x1) + (1 − α)f (x2) − 1
2

α(1 − α)β(cid:107)x1 − x2(cid:107)2 ∀x1  x2 ∈ B.

When f (x) is differentiable  the strong convexity is equivalent to f (x1) ≥ f (x2) + (cid:104)∇f (x2)  x1 −
x2(cid:105) + β
2(cid:107)x1 − x2(cid:107)2 ∀x1  x2 ∈ B. In the sequel  we use the standard Euclidean norm to deﬁne
Lipschitz and strongly convex functions. Stochastic gradient descent method is an iterative algorithm
and produces a sequence of solutions xt  t = 1  . . .   T   by

xt+1 = ΠK(xt − ηt(cid:101)∇f (xt  ξt)) 

Eξt[exp((cid:107)(cid:101)∇f (x  ξt) − ∇f (x)(cid:107)2

where {ηt}T

(4)
t=1 is a sequence of step sizes  ΠK(x) is a projection operator that projects x into the

domain K  and (cid:101)∇f (x  ξt) is an unbiased stochastic gradient of f (x)  for which we further assume

bounded gradient variance as

(5)
√
T ) con-
For general convex optimization  stochastic gradient descent methods can obtain an O(1/
vergence rate in expectation or in a high probability provided (5) [16]. As we mentioned in the
Introduction section  SGD methods are computationally efﬁcient only when the projection ΠK(x)
can be carried out efﬁciently. The objective of this work is to develop computationally efﬁcient
stochastic optimization algorithms that are able to yield the same performance guarantee as the
standard SGD algorithm but with only ONE projection when applied to the problem in (1).

2/σ2)] ≤ exp(1).

4 Algorithms and Main Results

We now turn to extending the SGD method to the setting where only one projection is allowed to
perform for the entire sequence of updating. The main idea is to incorporate the constraint function
g(x) into the objective function to penalize the intermediate solutions that are outside the domain.
The result of the penalization is that  although the average solution obtained by SGD may not be
feasible  it should be very close to the boundary of the domain. A projection is performed at the end
of the iterations to restore the feasibility of the average solution.

3

Algorithm 1 (SGDP-PD): SGD with ONE Projection by Primal Dual Updating
1: Input: a sequence of step sizes {ηt}  and a parameter γ > 0
2: Initialization: x1 = 0 and λ1 = 0
3: for t = 1  2  . . .   T do
Compute x(cid:48)
4:
t+1(cid:107)2  1) 
Update xt+1 = x(cid:48)
5:
Update λt+1 = [(1 − γηt)λt + ηtg(xt)]+
6:
7: end for

t+1 = xt − ηt((cid:101)∇f (xt  ξt) + λt∇g(xt))

8: Output:(cid:101)xT = ΠK ((cid:98)xT )  where(cid:98)xT =(cid:80)T

t+1/ max ((cid:107)x(cid:48)

t=1 xt/T .

(6)
(7)

(8)

The key ingredient of proposed algorithms is to replace the projection step with the gradient com-
putation of the constraint function deﬁning the domain K  which is signiﬁcantly cheaper than pro-
jection step. As an example  when a solution is restricted to a PSD cone  i.e.  X (cid:23) 0 where X
is a symmetric matrix  the corresponding inequality constraint is g(X) = λmax(−X) ≤ 0  where
λmax(X) computes the largest eigenvalue of X and is a convex function. In this case  ∇g(X) only
requires computing the minimum eigenvector of a matrix  which is cheaper than a full eigenspectrum
computation required at each iteration of the standard SGD algorithm to restore feasibility.
Below  we state a few assumptions about f (x) and g(x) often made in stochastic optimization as:

A1
A2

(cid:107)∇f (x)(cid:107)2 ≤ G1 

Eξt[exp((cid:107)(cid:101)∇f (x  ξt) − ∇f (x)(cid:107)2

(cid:107)∇g(x)(cid:107)2 ≤ G2 

|g(x)| ≤ C2 

∀x ∈ B 

2/σ2)] ≤ exp(1) 

∀x ∈ B.

We also make the following mild assumption about the boundary of the convex domain K as:

A3

there exists a constant ρ > 0 such that min
g(x)=0

(cid:107)∇g(x)(cid:107)2 ≥ ρ.

Remark 1. The purpose of introducing assumption A3 is to ensure that the optimal dual variable
for the constrained optimization problem in (1) is well bounded from the above  a key factor for our
analysis. To see this  we write the problem in (1) into a convex-concave optimization problem:

x∈B max
min
λ≥0

f (x) + λg(x).

Let (x∗  λ∗) be the optimal solution to the above convex-concave optimization problem. Since we
assume g(x) is strictly feasible  x∗ is also an optimal solution to (1) due to the strong duality
theorem [4]. Using the ﬁrst order optimality condition  we have ∇f (x∗) = −λ∗∇g(x∗). Hence 
λ∗ = 0 when g(x∗) < 0  and λ∗ = (cid:107)∇f (x∗)(cid:107)2/(cid:107)∇g(x∗)(cid:107)2 when g(x∗) = 0. Under assumption
A3  we have λ∗ ∈ [0  G1/ρ].
We note that  from a practical point of view  it is straightforward to verify that for many domains
including PSD cone and Polytope  the gradient of the constraint function is lower bounded on the
boundary and therefore assumption A3 does not limit the applicability of the proposed algorithms
for stochastic optimization. For the example of g(X) = λmax(−X)  the assumption A3 implies
ming(X)=0 (cid:107)∇g(X)(cid:107)F = (cid:107)uu(cid:62)(cid:107)F = 1  where u is an orthonomal vector representing the corre-
sponding eigenvector of the matrix X whose minimum eigenvalue is zero.
We propose two different ways of incorporating the constraint function into the objective function 
which result in two algorithms  one for general convex and the other for strongly convex functions.
4.1 SGD with One Projection for General Convex Optimization

To incorporate the constraint function g(x)  we introduce a regularized Lagrangian function 

L(x  λ) = f (x) + λg(x) − γ
2

λ2 

λ ≥ 0.

The summation of the ﬁrst two terms in L(x  λ) corresponds to the Lagrangian function in dual anal-
ysis and λ corresponds to a Lagrangian multiplier. A regularization term −(γ/2)λ2 is introduced in
L(x  λ) to prevent λ from being too large. Instead of solving the constrained optimization problem
in (1)  we try to solve the following convex-concave optimization problem

x∈B max
min
λ≥0

L(x  λ).

(9)

The proposed algorithm for stochastically optimizing the problem in (9) is summarized in Algo-
rithm 1. It differs from the existing stochastic gradient descent methods in that it updates both the
primal variable x (steps 4 and 5) and the dual variable λ (step 6)  which shares the same step sizes.

4

We note that the parameter ρ is not employed in the implementation of Algorithm 1 and is only
required for the theoretical analysis. It is noticeable that a similar primal-dual updating is explored
in [15] to avoid projection in online learning. Our work differs from [15] in that their algorithm
and analysis only lead to a bound for the regret and the violation of the constraints in a long run 
which does not necessarily guarantee the feasibility of ﬁnal solution. Also our proof techniques
differ from [16]  where the convergence rate is obtained for the saddle point; however our goal is to
attain bound on the convergence of the primal feasible solution.
Remark 2. The convex-concave optimization problem in (9) is equivalent to the following mini-
mization problem:

min
x∈B f (x) +

[g(x)]2
+

2γ

 

(10)

where [z]+ outputs z if z > 0 and zero otherwise. It thus may seem attractive to directly optimize
√
the penalized function f (x) + [g(x)]2
+/(2γ) using the standard SGD method  which unfortunately
does not yield a regret of O(
T )  we need
T )  which unfortunately will lead to a blowup of the gradients and consequently a
to set γ = Ω(
√
poor regret bound. Using a primal-dual updating schema allows us to adjust the penalization term
more carefully to obtain an O(1/

√
T ). This is because  in order to obtain a regret of O(

T ) convergence rate.

√

2/(cid:112)(G2

Theorem 1. For any general convex function f (x)  if we set ηt = γ/(2G2
γ = G2
with a probability at least 1 − δ 

2)  t = 1 ···   T   and
2 + (1 + ln(2/δ))σ2)T in Algorithm 1  under assumptions A1-A3  we have 

1 + C 2

f ((cid:101)xT ) ≤ min

x∈K f (x) + O

(cid:18) 1√

(cid:19)

 

T

where O(·) suppresses polynomial factors that depend on ln(2/δ)  G1  G2  C2  ρ  and σ.

4.2 SGD with One Projection for Strongly Convex Optimization

We ﬁrst emphasize that it is difﬁcult to extend Algorithm 1 to achieve an O(ln T /T ) convergence
rate for strongly convex optimization. This is because although −L(x  λ) is strongly convex in λ 
its modulus for strong convexity is γ  which is too small to obtain an O(ln T ) regret bound.
To achieve a faster convergence rate for strongly convex optimization  we change assumptions A1
and A2 to

(cid:107)(cid:101)∇f (x  ξt)(cid:107)2 ≤ G1 

A4

(cid:107)∇g(x)(cid:107)2 ≤ G2 

∀x ∈ B 

where we slightly abuse the same notation G1. Note that A1 only requires that (cid:107)∇f (x)(cid:107)2 is
bounded and A2 assumes a mild condition on the stochastic gradient.
In contrast  for strongly

convex optimization we need to assume a bound on the stochastic gradient (cid:107)(cid:101)∇f (x  ξt)(cid:107)2. Al-
sampling over the training examples. Given the bound on (cid:107)(cid:101)∇f (x  ξt)(cid:107)2  we can easily have
(cid:107)∇f (x)(cid:107)2 = (cid:107)E(cid:101)∇f (x  ξt)(cid:107)2 ≤ E(cid:107)(cid:101)∇f (x  ξt)(cid:107)2 ≤ G1  which is used to set an input parameter

though assumption A4 is stronger than assumptions A1 and A2  however  it is always possible
to bound the stochastic gradient for machine learning problems where f (x) usually consists of
a summation of loss functions on training examples  and the stochastic gradient is computed by

λ0 > G1/ρ to the algorithm. According to the discussion in the last subsection  we know that the
optimal dual variable λ∗ is upper bounded by G1/ρ  and consequently is upper bounded by λ0.
Similar to the last approach  we write the optimization problem (1) into an equivalent convex-
concave optimization problem:

min
g(x)≤0

f (x) = min

x∈B max
0≤λ≤λ0

f (x) + λg(x) = min

x∈B f (x) + λ0[g(x)]+.

To avoid unnecessary complication due to the subgradient of [·]+  following [18]  we introduce a
smoothing term H(λ/λ0)  where H(p) = −p ln p − (1 − p) ln(1 − p) is the entropy function  into
the Lagrangian function  leading to the optimization problem min
x∈B F (x)  where F (x) is deﬁned as

(cid:18)

(cid:18) λ0g(x)

(cid:19)(cid:19)

 

F (x) = f (x) + max
0≤λ≤λ0

λg(x) + γH(λ/λ0) = f (x) + γ ln

1 + exp

γ

where γ > 0 is a parameter whose value will be determined later. Given the smoothed objective
function F (x)  we ﬁnd the optimal solution by applying SGD to minimize F (x)  where the gradient

5

Algorithm 2 (SGDP-ST): SGD with ONE Projection by a Smoothing Technique
1: Input: a sequence of step sizes {ηt}  λ0  and γ
2: Initialization: x1 = 0.
3: for t = 1  . . .   T do
4:

(cid:18)(cid:101)∇f (xt  ξt) +
7: Output:(cid:101)xT = ΠK ((cid:98)xT )  where(cid:98)xT =(cid:80)T

Compute x(cid:48)
Update xt+1 = x(cid:48)

t+1 = xt − ηt

t+1/ max((cid:107)x(cid:48)

1 + exp(λ0g(xt)/γ)

5:
6: end for

λ0∇g(xt)

exp (λ0g(xt)/γ)

t+1(cid:107)2  1)

t=1 xt/T .

(cid:19)

of F (x) is computed by

∇F (x) = ∇f (x) +

exp (λ0g(x)/γ)

1 + exp (λ0g(x)/γ)

λ0∇g(x).

(11)

Algorithm 2 gives the detailed steps. Unlike Algorithm 1  only the primal variable x is updated in
each iteration using the stochastic gradient computed in (11).
The following theorem shows that Algorithm 2 achieves an O(ln T /T ) convergence rate if the cost
functions are strongly convex.
Theorem 2. For any β-strongly convex function f (x)  if we set ηt = 1/(2βt)  t = 1  . . .   T   γ =
ln T /T   and λ0 > G1/ρ in Algorithm 2  under assumptions A3 and A4  we have with a probability
at least 1 − δ 

f ((cid:101)xT ) ≤ min

x∈K f (x) + O

(cid:18) ln T

(cid:19)

 

T

where O(·) suppresses polynomial factors that depend on ln(1/δ)  1/β  G1  G2  ρ  and λ0.
It is well known that the optimal convergence rate of SGD for strongly convex optimization is
O(1/T ) [9] which has been proven to be tight in stochastic optimization setting [1]. According to
Theorem 2  Algorithm 2 achieves an almost optimal convergence rate except for the factor of ln T .
It is worth mentioning that although it is not explicitly given in Theorem 2  the detailed expression
for the convergence rate of Algorithm 2 exhibits a tradeoff in setting λ0 (more can be found in the
proof of Theorem 2). Finally  under assumptions A1-A3  Algorithm 2 can achieve an O(1/
T )
convergence rate for general convex functions  similar to Algorithm 1.

√

5 Convergence Rate Analysis

We here present the proofs of main theorems. The omitted proofs are provided in the Appendix. We
use O(·) notation in a few inequalities to absorb constants independent from T for ease of exposition.

5.1 Proof of Theorem 1

To pave the path for the proof  we present a series of lemmas. The lemma below states two key
inequalities  which follows the standard analysis of gradient descent.
Lemma 1. Under the bounded assumptions in (6) and (7)  for any x ∈ B and λ > 0  we have
(xt − x)(cid:62)∇xL(xt  λt) ≤ 1
2ηt

(cid:1) + 2ηtG2
(cid:0)(cid:107)x − xt(cid:107)2
2 − (cid:107)x − xt+1(cid:107)2
+ (x − xt)(cid:62)((cid:101)∇f (xt  ξt) − ∇f (xt))
+ 2ηt (cid:107)(cid:101)∇f (xt  ξt) − ∇f (xt)(cid:107)2
(cid:124)
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:0)|λ − λt|2 − |λ − λt+1|2(cid:1) + 2ηtC 2

1 + ηtG2

(cid:123)(cid:122)

≡ζt(x)

≡∆t

2λ2
t

(λ − λt)∇λL(xt  λt) ≤ 1
2ηt

2 .

T(cid:88)

An immediate result of Lemma 1 is the following which states a regret-type bound.
T(cid:88)
2)  t = 1 ···   T   we have
Lemma 2. For any general convex function f (x)  if we set ηt = γ/(2G2
ζt(x∗) 
γ
G2
2

(f (xt) − f (x∗)) +

t=1 g(xt)]2
+
2/γ)

1 + C 2
2 )
G2
2

[(cid:80)T

≤ G2
2
γ

2(γT + 2G2

T(cid:88)

∆t +

γT +

(G2

+

2

2

 

t=1

t=1

t=1

where x∗ = arg minx∈K f (x).

6

t=1

t=1

T ) 

√

√

√
1

C

T

T
C

1 + C 2

+ ≤ O

(cid:2) T(cid:88)

√
+ ≤ O(

(f (xt) − f (x∗)) +

plugging the stated value of γ  we have with a probability 1 − δ

1 + C 2
suppresses polynomial factors that depend on ln(2/δ)  G1  G2  C2  σ.

g(xt)(cid:3)2
2 + (1 + ln(2/δ))σ2 + 2(cid:112)G2
[g((cid:98)xT )]2

t=1 ζt(x∗) ≤ 2σ(cid:112)3 ln(2/δ)
1 − δ/2  we have(cid:80)T
1 − δ/2  we have(cid:80)T
Proof of Therorem 1. First  by martingale inequality (e.g.  Lemma 4 in [13])  with a probability
T . By Markov’s inequality  with a probability
t=1 ∆t ≤ (1 + ln(2/δ))σ2T . Substituting these inequalities into Lemma 2 
T(cid:88)
where C = 2G2(1/(cid:112)G2
Recalling the deﬁnition of(cid:98)xT =(cid:80)T
(cid:18) 1√
Assume g((cid:98)xT ) > 0  otherwise(cid:101)xT =(cid:98)xT and we easily have f ((cid:101)xT ) ≤ minx∈K f (x) + O(1/
Since(cid:101)xT is the projection of(cid:98)xT into K  i.e. (cid:101)xT = arg ming(x)≤0 (cid:107)x −(cid:98)xT(cid:107)2
g((cid:101)xT ) = 0  and(cid:98)xT −(cid:101)xT = s∇g((cid:101)xT )
which indicates that(cid:98)xT −(cid:101)xT is in the same direction to ∇g(˜xT ). Hence 
g((cid:98)xT ) = g((cid:98)xT ) − g((cid:101)xT ) ≥ ((cid:98)xT −(cid:101)xT )(cid:62)∇g((cid:101)xT ) = (cid:107)(cid:98)xT −(cid:101)xT(cid:107)2(cid:107)∇g((cid:101)xT )(cid:107)2 ≥ ρ(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 
due to f (x∗) ≤ f ((cid:101)xT ) and Lipschitz continuity of f (x). Combining inequalities (12)  (13)  and (14)

2 + (1 + ln(2/δ))σ2) and O(·)
(cid:19)

(13)
where the last inequality follows the deﬁnition of ming(x)=0 (cid:107)∇g(x)(cid:107)2 ≥ ρ. Additionally  we have
(14)

f (x∗) − f ((cid:98)xT ) ≤ f (x∗) − f ((cid:101)xT ) + f ((cid:101)xT ) − f ((cid:98)xT ) ≤ G1(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 

t=1 xt/T and using the convexity of f (x) and g(x)  we have

optimality condition  there exists a positive constant s > 0 such that

f ((cid:98)xT ) − f (x∗) +

yields

T(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2

T ) + G1(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2.
√
(cid:16)(cid:113) C
(cid:17)
2 ≤ O(1/
By simple algebra  we have (cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 ≤ G1C
(cid:18) 1√
(cid:19)
(cid:19)
(cid:18) 1√
f ((cid:101)xT ) ≤ f ((cid:101)xT )− f ((cid:98)xT ) + f ((cid:98)xT ) ≤ G1(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 + f (x∗) + O
where we use the inequality in (12) to bound f ((cid:98)xT ) by f (x∗) and absorb the dependence on ρ  G1  C

≤ f (x∗) + O

. Therefore

T ).
2  then by ﬁrst order

(12)
√

√

ρ2

T

ρ2
C

+ O

.

T

 

T

√

ρ2T

T

into the O(·) notation.

Remark 3. From the proof of Theorem 1  we can see that the key inequalities are (12)  (13)  and (14).
In particular  the regret-type bound in (12) depends on the algorithm. If we only update the primal
variable using the penalized objective in (10)  whose gradient depends on 1/γ  it will cause a blowup
in the regret bound with (1/γ + γT + T /γ)  which leads to a non-convergent bound.

5.2 Proof of Theorem 2

T(cid:88)

t=1

Our proof of Theorem 2 for the convergence rate of Algorithm 2 when applied to strongly convex
functions starts with the following lemma by analogy of Lemma 2.
Lemma 3. For any β-strongly convex function f (x)  if we set ηt = 1/(2βt)  we have

(F (x) − F (x∗)) ≤ (G2

1 + λ2

0G2
2)(1 + ln T )
2β

+

ζt(x∗) − β
4

(cid:107)x∗ − xt(cid:107)2

2

where x∗ = arg minx∈K f (x).
In order to prove Theorem 2  we need the following result for an improved martingale inequality.

T(cid:88)

t=1

T(cid:88)

t=1

Lemma 4. For any ﬁxed x ∈ B  deﬁne DT = (cid:80)T
(cid:114)

(cid:18)
(cid:100)log2 T(cid:101). We have
Pr

ΛT ≤ 4G1

(cid:18)

(cid:19)

+ Pr

DT ≤ 4
T

2  ΛT = (cid:80)T
(cid:19)

m
δ

+ 4G1 ln

≥ 1 − δ.

m
δ

t=1 (cid:107)xt − x(cid:107)2

t=1 ζt(x)  and m =

DT ln

7

Proof of Theorem 2. We substitute the bound in Lemma 4 into the inequality in Lemma 3 with
x = x∗. We consider two cases. In the ﬁrst case  we assume DT ≤ 4/T . As a result  we have

(∇f (xt) −(cid:101)∇f (xt  ξt))(cid:62)(x∗ − xt) ≤ 2G1

(cid:112)

T DT ≤ 4G1 

T(cid:88)

t=1

ζt(x∗) =

which together with the inequality in Lemma 3 leads to the bound
0G2
2)(1 + ln T )
2β

(F (xt) − F (x∗)) ≤ 4G1 +

1 + λ2

(G2

T(cid:88)
T(cid:88)

t=1

t=1

.

(cid:19)

(cid:18) 16G2

1

β

In the second case  we assume
ζt(x∗) ≤ 4G1

(cid:114)

T(cid:88)
T(cid:88)

t=1

t=1

m
δ

 

.

m
δ

β

≤ β
4

m
δ

(cid:19)

.

t=1

m
δ

1

β

(cid:19)

1 + λ2

m
δ

+

+ 4G1

ln

DT ln
√

+ 4G1 ln

DT +
ab ≤ a2 + b2. We thus have

1

+ 4G1

ln

+ 4G1 +

+ 4G1

ln

(G2

1 + λ2

T(cid:88)

0G2
2)(1 + ln T )
2β

where the last step uses the fact 2
(F (xt) − F (x∗)) ≤

(cid:18) 16G2
(cid:18) 16G2
(cid:124)
(cid:18)
F ((cid:98)xT ) = f ((cid:98)xT ) + γ ln

(F (xt) − F (x∗)) ≤

1 + exp

Combing the results of the two cases  we have  with a probability 1 − δ 
(G2

0G2
2)(1 + ln T )
2β

we have F (x∗) ≤ f (x∗) + γ ln 2. On the other hand 

γ
Therefore  with the value of γ = ln T /T   we have

By convexity of F (x)  we have F ((cid:98)xT ) ≤ F (x∗) + O (ln T /T ). Noting that x∗ ∈ K  g(x∗) ≤ 0 

(cid:123)(cid:122)
(cid:125)
(cid:19)(cid:19)
(cid:18) λ0g((cid:98)xT )
≥ f ((cid:98)xT ) + max (0  λ0g((cid:98)xT )) .
(cid:19)
(cid:18) ln T
f ((cid:98)xT ) ≤ f (x∗) + O
(cid:18) ln T
(cid:19)
f ((cid:98)xT ) + λ0g((cid:98)xT ) ≤ f (x∗) + O
(cid:18) ln T
(cid:19)
λ0ρ(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 ≤ G1(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 + O
For λ0 > G1/ρ  we have (cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 ≤ (1/(λ0ρ − G1))O(ln T /T ). Therefore
(cid:19)
(cid:18) ln T
f ((cid:101)xT ) ≤ f ((cid:101)xT )− f ((cid:98)xT ) + f ((cid:98)xT ) ≤ G1(cid:107)(cid:98)xT −(cid:101)xT(cid:107)2 + f (x∗) + O

Applying the inequalities (13) and (14) to (16)  and noting that γ = ln T /T   we have

(cid:18) ln T

≤ f (x∗) + O

(15)

(16)

(cid:19)

 

.

T

 

T

.

T

O(ln T )

T

T

where in the second inequality we use inequality (15).

6 Conclusions

In the present paper  we made a progress towards making the SGD method efﬁcient by proposing a
framework in which it is possible to exclude the projection steps from the SGD algorithm. We have
proposed two novel algorithms to overcome the computational bottleneck of the projection step in
applying SGD to optimization problems with complex domains. We showed using novel theoretical
analysis that the proposed algorithms can achieve an O(1/
T ) convergence rate for general convex
functions and an O(ln T /T ) rate for strongly convex functions with a overwhelming probability
which are known to be optimal (up to a logarithmic factor) for stochastic optimization.

√

Acknowledgments

The authors would like to thank the anonymous reviewers for their helpful suggestions. This work
was supported in part by National Science Foundation (IIS-0643494) and Ofﬁce of Navy Research
(Award N000141210431 and Award N00014-09-1-0663).

8

References
[1] A. Agarwal  P. L. Bartlett  P. D. Ravikumar  and M. J. Wainwright.

Information-theoretic
lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions
on Information Theory  58(5):3235–3249  2012.

[2] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for

machine learning. In NIPS  pages 451–459  2011.

[3] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  2nd edition  1999.
[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[5] K. L. Clarkson. Coresets  sparse greedy approximation  and the frank-wolfe algorithm. ACM

Transactions on Algorithms  6(4)  2010.

[6] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the l1-ball

for learning in high dimensions. In ICML  pages 272–279  2008.

[7] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics 

3  1956.

[8] E. Hazan. Sparse approximate solutions to semideﬁnite programs. In LATIN  pages 306–316 

2008.

[9] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. Journal of Machine Learning Research - Proceedings
Track  19:421–436  2011.

[10] E. Hazan and S. Kale. Projection-free online learning. In ICML  2012.
[11] M. Jaggi. Sparse Convex Optimization Methods for Machine Learning. PhD thesis  ETH

Zurich  Oct. 2011.

[12] M. Jaggi and M. Sulovsk´y. A simple algorithm for nuclear norm regularized problems. In

ICML  pages 471–478  2010.

[13] G. Lan. An optimal method for stochastic composite optimization. Math. Program.  133(1-

2):365–397  2012.

[14] J. Liu and J. Ye. Efﬁcient euclidean projections in linear time. In ICML  page 83  2009.
[15] M. Mahdavi  R. Jin  and T. Yang. Trading regret for efﬁciency: online convex optimization

with long term constraints. JMLR  13:2465–2490  2012.

[16] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM J. on Optimization  19:1574–1609  2009.

[17] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Aca-

demic Publishers  2004.

[18] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program.  103(1):127–

152  2005.

[19] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver

for svm. In ICML  pages 807–814  2007.

[20] Y. Ying and P. Li. Distance metric learning with eigenvalue optimization. JMLR.  13:1–26 

2012.

[21] T. Zhang. Sequential greedy approximation for certain convex optimization problems. Infor-

mation Theory  IEEE Transactions on  49:682–691  2003.

[22] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In

ICML  pages 928–936  2003.

9

,Yiming Ying
Longyin Wen
Siwei Lyu