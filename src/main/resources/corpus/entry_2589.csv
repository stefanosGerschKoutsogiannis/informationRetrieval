2016,Improved Variational Inference with Inverse Autoregressive Flow,The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow  inverse autoregressive flow (IAF)  that  in contrast to earlier published flows  scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations  where each transformation is based on an autoregressive neural network. In experiments  we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition  we demonstrate that a novel type of variational autoencoder  coupled with IAF  is competitive with neural autoregressive models in terms of attained log-likelihood on natural images  while allowing significantly faster synthesis.,Improved Variational Inference
with Inverse Autoregressive Flow

Diederik P. Kingma

dpkingma@openai.com

Tim Salimans

tim@openai.com

Rafal Jozefowicz
rafal@openai.com

Xi Chen

peter@openai.com

Ilya Sutskever

ilya@openai.com

Max Welling⇤

M.Welling@uva.nl

Abstract

The framework of normalizing ﬂows provides a general strategy for ﬂexible vari-
ational inference of posteriors over latent variables. We propose a new type of
normalizing ﬂow  inverse autoregressive ﬂow (IAF)  that  in contrast to earlier
published ﬂows  scales well to high-dimensional latent spaces. The proposed ﬂow
consists of a chain of invertible transformations  where each transformation is
based on an autoregressive neural network. In experiments  we show that IAF
signiﬁcantly improves upon diagonal Gaussian approximate posteriors. In addition 
we demonstrate that a novel type of variational autoencoder  coupled with IAF  is
competitive with neural autoregressive models in terms of attained log-likelihood
on natural images  while allowing signiﬁcantly faster synthesis.

1

Introduction

Stochastic variational inference (Blei et al.  2012; Hoffman et al.  2013) is a method for scalable
posterior inference with large datasets using stochastic gradient ascent. It can be made especially
efﬁcient for continuous latent variables through latent-variable reparameterization and inference
networks  amortizing the cost  resulting in a highly scalable learning procedure (Kingma and Welling 
2013; Rezende et al.  2014; Salimans et al.  2014). When using neural networks for both the
inference network and generative model  this results in class of models called variational auto-
encoders (Kingma and Welling  2013) (VAEs). A general strategy for building ﬂexible inference
networks  is the framework of normalizing ﬂows (Rezende and Mohamed  2015). In this paper we
propose a new type of ﬂow  inverse autoregressive ﬂow (IAF)  which scales well to high-dimensional
latent space.
At the core of our proposed method lie Gaussian autoregressive functions that are normally used
for density estimation: functions that take as input a variable with some speciﬁed ordering such
as multidimensional tensors  and output a mean and standard deviation for each element of the
input variable conditioned on the previous elements. Examples of such functions are autoregressive
neural density estimators such as RNNs  MADE (Germain et al.  2015)  PixelCNN (van den Oord
et al.  2016b) or WaveNet (van den Oord et al.  2016a) models. We show that such functions
can often be turned into invertible nonlinear transformations of the input  with a simple Jacobian
determinant. Since the transformation is ﬂexible and the determinant known  it can be used as a
normalizing ﬂow  transforming a tensor with relatively simple known density  into a new tensor with
more complicated density that is still cheaply computable. In contrast with most previous work on

⇤University of Amsterdam  University of California Irvine  and the Canadian Institute for Advanced Research

(CIFAR).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

(a) Prior distribution

(b) Posteriors in standard VAE (c) Posteriors in VAE with IAF

Figure 1: Best viewed in color. We ﬁtted a variational auto-encoder (VAE) with a spherical Gaussian
prior  and with factorized Gaussian posteriors (b) or inverse autoregressive ﬂow (IAF) posteriors (c)
to a toy dataset with four datapoints. Each colored cluster corresponds to the posterior distribution of
one datapoint. IAF greatly improves the ﬂexibility of the posterior distributions  and allows for a
much better ﬁt between the posteriors and the prior.

improving inference models including previously used normalizing ﬂows  this transformation is well
suited to high-dimensional tensor variables  such as spatio-temporally organized variables.
We demonstrate this method by improving inference networks of deep variational auto-encoders.
In particular  we train deep variational auto-encoders with latent variables at multiple levels of the
hierarchy  where each stochastic variable is a three-dimensional tensor (a stack of featuremaps)  and
demonstrate improved performance.

2 Variational Inference and Learning

Let x be a (set of) observed variable(s)  z a (set of) latent variable(s) and let p(x  z) be the parametric
model of their joint distribution  called the generative model deﬁned over the variables. Given a
dataset X = {x1  ...  xN} we typically wish to perform maximum marginal likelihood learning of its
parameters  i.e. to maximize

log p(X) =

log p(x(i)) 

(1)

NXi=1

but in general this marginal likelihood is intractable to compute or differentiate directly for ﬂexible
generative models  e.g. when components of the generative model are parameterized by neural
networks. A solution is to introduce q(z|x)  a parametric inference model deﬁned over the latent
variables  and optimize the variational lower bound on the marginal log-likelihood of each observation
x:

L(x; ✓) = log p(x)  DKL(q(z|x)||p(z|x))

log p(x)  Eq(z|x) [log p(x  z)  log q(z|x)] = L(x; ✓)

(2)
where ✓ indicates the parameters of p and q models. Keeping in mind that Kullback-Leibler diver-
gences DKL(.) are non-negative  it’s clear that L(x; ✓) is a lower bound on log p(x) since it can be
written as follows ):
(3)
There are various ways to optimize the lower bound L(x; ✓); for continuous z it can be done efﬁciently
through a re-parameterization of q(z|x)  see e.g. (Kingma and Welling  2013; Rezende et al.  2014).
As can be seen from equation (3)  maximizing L(x; ✓) w.r.t. ✓ will concurrently maximize log p(x)
and minimize DKL(q(z|x)||p(z|x)). The closer DKL(q(z|x)||p(z|x)) is to 0  the closer L(x; ✓) will
be to log p(x)  and the better an approximation our optimization objective L(x; ✓) is to our true objec-
tive log p(x). Also  minimization of DKL(q(z|x)||p(z|x)) can be a goal in itself  if we’re interested
in using q(z|x) for inference after optimization. In any case  the divergence DKL(q(z|x)||p(z|x))
is a function of our parameters through both the inference model and the generative model  and
increasing the ﬂexibility of either is generally helpful towards our objective.

2

Note that in models with multiple latent variables  the inference model is typically factorized into
partial inference models with some ordering; e.g. q(za  zb|x) = q(za|x)q(zb|za  x). We’ll write
q(z|x  c) to denote such partial inference models  conditioned on both the data x and a further context
c which includes the previous latent variables according to the ordering.

2.1 Requirements for Computational Tractability
Requirements for the inference model  in order to be able to efﬁciently optimize the bound  are that it
is (1) computationally efﬁcient to compute and differentiate its probability density q(z|x)  and (2)
computationally efﬁcient to sample from  since both these operations need to be performed for each
datapoint in a minibatch at every iteration of optimization. If z is high-dimensional and we want
to make efﬁcient use of parallel computational resources like GPUs  then parallelizability of these
operations across dimensions of z is a large factor towards efﬁciency. This requirement restrict the
class of approximate posteriors q(z|x) that are practical to use. In practice this often leads to the use
of diagonal posteriors  e.g. q(z|x) ⇠N (µ(x)  2(x))  where µ(x) and (x) are often nonlinear
functions parameterized by neural networks. However  as explained above  we also need the density
q(z|x) to be sufﬁciently ﬂexible to match the true posterior p(z|x).
2.2 Normalizing Flow
Normalizing Flow (NF)  introduced by (Rezende and Mohamed  2015) in the context of stochastic
gradient variational inference  is a powerful framework for building ﬂexible posterior distributions
through an iterative procedure. The general idea is to start off with an initial random variable with a
relatively simple distribution with known (and computationally cheap) probability density function 
and then apply a chain of invertible parameterized transformations ft  such that the last iterate zT has
a more ﬂexible distribution2:

(4)
As long as the Jacobian determinant of each of the transformations ft can be computed  we can still
compute the probability density function of the last iterate:

zt = ft(zt1  x) 8t = 1...T

z0 ⇠ q(z0|x) 

TXt=1

log det

dzt

dzt1

log q(zT|x) = log q(z0|x) 

(5)

However  (Rezende and Mohamed  2015) experiment with only a very limited family of such
invertible transformation with known Jacobian determinant  namely:
ft(zt1) = zt1 + uh(wT zt1 + b)

(6)
where u and w are vectors  wT is w transposed  b is a scalar and h(.) is a nonlinearity  such that
uh(wT zt1 + b) can be interpreted as a MLP with a bottleneck hidden layer with a single unit. Since
information goes through the single bottleneck  a long chain of transformations is required to capture
high-dimensional dependencies.

3

Inverse Autoregressive Transformations

In order to ﬁnd a type of normalizing ﬂow that scales well to high-dimensional space  we consider
Gaussian versions of autoregressive autoencoders such as MADE (Germain et al.  2015) and the
PixelCNN (van den Oord et al.  2016b). Let y be a variable modeled by such a model  with some
i=1. We will use [µ(y)  (y)] to denote the function of the
chosen ordering on its elements y = {yi}D
vector y  to the vectors µ and . Due to the autoregressive structure  the Jacobian is lower triangular
with zeros on the diagonal: @[µi  i]/@yj = [0  0] for j  i. The elements [µi(y1:i1)  i(y1:i1)]
are the predicted mean and standard deviation of the i-th element of y  which are functions of only
the previous elements in y.
Sampling from such a model is a sequential transformation from a noise vector ✏ ⇠N (0  I) to the
corresponding vector y: y0 = µ0 + 0  ✏0  and for i > 0  yi = µi(y1:i1) + i(y1:i1) · ✏i. The
2where x is the context  such as the value of the datapoint. In case of models with multiple levels of latent

variables  the context also includes the value of the previously sampled latent variables.

3

Algorithm 1: Pseudo-code of an approximate posterior with Inverse Autoregressive Flow (IAF)
Data:

x: a datapoint  and optionally other conditioning information
✓: neural network parameters
EncoderNN(x; ✓): encoder neural network  with additional output h
AutoregressiveNN[⇤](z  h; ✓): autoregressive neural networks  with additional input h
sum(.): sum over vector elements
sigmoid(.): element-wise sigmoid function
z: a random sample from q(z|x)  the approximate posterior distribution
l: the scalar value of log q(z|x)  evaluated at sample ’z’

Result:

[µ    h] EncoderNN(x; ✓)
✏ ⇠N (0  I)
z   ✏ + µ
l sum(log  + 1
for t 1 to T do

2 ✏2 + 1

2 log(2⇡))

[m  s] AutoregressiveNN[t](z  h; ✓)
 sigmoid(s)
z   z + (1  )  m
l l  sum(log )

end

computation involved in this transformation is clearly proportional to the dimensionality D. Since
variational inference requires sampling from the posterior  such models are not interesting for direct
use in such applications. However  the inverse transformation is interesting for normalizing ﬂows  as
we will show. As long as we have i > 0 for all i  the sampling transformation above is a one-to-one
transformation  and can be inverted: ✏i = yiµi(y1:i1)
i(y1:i1)
We make two key observations  important for normalizing ﬂows. The ﬁrst is that this inverse
transformation can be parallelized  since (in case of autoregressive autoencoders) computations of
the individual elements ✏i do not depend on eachother. The vectorized transformation is:

.

✏ = (y  µ(y))/(y)

(7)

where the subtraction and division are elementwise.
The second key observation  is that this inverse autoregressive operation has a simple Jacobian
determinant. Note that due to the autoregressive structure  @[µi  i]/@yj = [0  0] for j  i. As a
result  the transformation has a lower triangular Jacobian (@✏i/@yj = 0 for j > i)  with a simple
diagonal: @✏i/@yi = i. The determinant of a lower triangular matrix equals the product of the
diagonal terms. As a result  the log-determinant of the Jacobian of the transformation is remarkably
simple and straightforward to compute:

log det

d✏

dy =

DXi=1

 log i(y)

(8)

The combination of model ﬂexibility  parallelizability across dimensions  and simple log-determinant 
make this transformation interesting for use as a normalizing ﬂow over high-dimensional latent space.

4

Inverse Autoregressive Flow (IAF)

We propose a new type normalizing ﬂow (eq. (5))  based on transformations that are equivalent to
the inverse autoregressive transformation of eq. (7) up to reparameterization. See algorithm 1 for
pseudo-code of an appproximate posterior with the proposed ﬂow. We let an initial encoder neural
network output µ0 and 0  in addition to an extra output h  which serves as an additional input to
each subsequent step in the ﬂow. We draw a random sample ✏ ⇠N (0  I)  and initialize the chain
with:
(9)

z0 = µ0 + 0  ✏

4

Approximate Posterior with Inverse Autoregressive Flow (IAF)

x

Encoder NN

σ

×

ε

μ

+

h

z

IAF 
step

IAF 
step

z

···

···

···

z

IAF Step

Autoregressive NN

σ

×

μ

+

z

h

z

Figure 2: Like other normalizing ﬂows  drawing samples from an approximate posterior with Inverse
Autoregressive Flow (IAF) consists of an initial sample z drawn from a simple distribution  such as a
Gaussian with diagonal covariance  followed by a chain of nonlinear invertible transformations of z 
each with a simple Jacobian determinants.

Approximate Posterior with Inverse Autoregressive Flow (IAF)

Encoder NN

Autoregressive NN

Autoregressive NN

x

ε

σ

×

μ

+

h

z

σ

μ

σ

μ

The ﬂow consists of a chain of T of the following transformations:

zT

Flow

z0

Generative 

Normalizing 

zt = µt + t  zt1

+
(10)
where at the t-th step of the ﬂow  we use a different autoregressive neural network with inputs zt1
and h  and outputs µt and t. The neural network is structured to be autoregressive w.r.t. zt1  such
that for any choice of its parameters  the Jacobians dµt
are triangular with zeros on the
dzt1
diagonal. As a result  dzt
i=1 t i. (Note
dzt1
that the Jacobian w.r.t. h does not have constraints.) Following eq. (5)  the density under the ﬁnal
iterate is:

is triangular with t on the diagonal  with determinantQD

and dt
dzt1

Posterior with Inverse Autoregressive Flow (IAF)

Inference 
model

Initial Network

IAF Step

IAF step

z

x

model

×

+

x

z

×

···

z

IAF Step

Context 

(e.g. Encoder NN)

Autoregressive NN

log q(zT|x) = 

Encoder 
x
NN
i + 1
2 log(2⇡) +

2 ✏2

DXi=1 1

z0

IAF 
step

log t i!

TXt=0

z1

IAF 
step

···

zT

(11)

μt

-

zt

The ﬂexibility of the distribution of the ﬁnal iterate zT   and its ability to closely ﬁt to the true posterior 
increases with the expressivity of the autoregressive models and the depth of the chain. See ﬁgure 2
for an illustration.
A numerically stable version  inspired by the LSTM-type update  is where we let the autoregressive
network output [mt  st]  two unconstrained real-valued vectors:

and compute zt as:

[mt  st] AutoregressiveNN[t](zt  h; ✓)

(12)

t = sigmoid(st)
zt = t  zt1 + (1  t)  mt

(13)
(14)
This version is shown in algorithm 1. Note that this is just a particular version of the update of
eq. (10)  so the simple computation of the ﬁnal log-density of eq. (11) still applies.
We found it beneﬁcial for results to parameterize or initialize the parameters of each
AutoregressiveNN[t] such that its outputs st are  before optimization  sufﬁciently positive  such as
close to +1 or +2. This leads to an initial behaviour that updates z only slightly with each step of IAF.
Such a parameterization is known as a ’forget gate bias’ in LSTMs  as investigated by Jozefowicz
et al. (2015).
Perhaps the simplest special version of IAF is one with a simple step  and a linear autoregressive
model. This transforms a Gaussian variable with diagonal covariance  to one with linear dependencies 
i.e. a Gaussian distribution with full covariance. See appendix A for an explanation.
Autoregressive neural networks form a rich family of nonlinear transformations for IAF. For non-
convolutional models  we use the family of masked autoregressive networks introduced in (Germain
et al.  2015) for the autoregressive neural networks. For CIFAR-10 experiments  which beneﬁts more
from scaling to high dimensional latent space  we use the family of convolutional autoregressive
autoencoders introduced by (van den Oord et al.  2016b c).
We found that results improved when reversing the ordering of the variables after each step in the IAF
chain. This is a volume-preserving transformation  so the simple form of eq. (11) remains unchanged.

5

5 Related work

Inverse autoregressive ﬂow (IAF) is a member of the family of normalizing ﬂows  ﬁrst discussed
in (Rezende and Mohamed  2015) in the context of stochastic variational inference. In (Rezende and
Mohamed  2015) two speciﬁc types of ﬂows are introduced: planar ﬂows and radial ﬂows. These
ﬂows are shown to be effective to problems relatively low-dimensional latent space (at most a few
hundred dimensions). It is not clear  however  how to scale such ﬂows to much higher-dimensional
latent spaces  such as latent spaces of generative models of /larger images  and how planar and radial
ﬂows can leverage the topology of latent space  as is possible with IAF. Volume-conserving neural
architectures were ﬁrst presented in in (Deco and Brauer  1995)  as a form of nonlinear independent
component analysis.
Another type of normalizing ﬂow  introduced by (Dinh et al.  2014) (NICE)  uses similar transforma-
tions as IAF. In contrast with IAF  this type of transformations updates only half of the latent variables
z1:D/2 per step  adding a vector f (zD/2+1:D) which is a neural network based function of the
remaining latent variables zD/2+1:D. Such large blocks have the advantage of computationally cheap
inverse transformation  and the disadvantage of typically requiring longer chains. In experiments 
(Rezende and Mohamed  2015) found that this type of transformation is generally less powerful than
other types of normalizing ﬂow  in experiments with a low-dimensional latent space. Concurrently
to our work  NICE was extended to high-dimensional spaces in (Dinh et al.  2016) (Real NVP). An
empirical comparison would be an interesting subject of future research.
A potentially powerful transformation is the Hamiltonian ﬂow used in Hamiltonian Variational
Inference (Salimans et al.  2014). Here  a transformation is generated by simulating the ﬂow
of a Hamiltonian system consisting of the latent variables z  and a set of auxiliary momentum
variables. This type of transformation has the additional beneﬁt that it is guided by the exact
posterior distribution  and that it leaves this distribution invariant for small step sizes. Such as
transformation could thus take us arbitrarily close to the exact posterior distribution if we can apply
it for a sufﬁcient number of times. In practice  however  Hamiltonian Variational Inference is very
demanding computationally. Also  it requires an auxiliary variational bound to account for the
auxiliary variables  which can impede progress if the bound is not sufﬁciently tight.
An alternative method for increasing the ﬂexiblity of the variational inference  is the introduction
of auxiliary latent variables (Salimans et al.  2014; Ranganath et al.  2015; Tran et al.  2015) and
corresponding auxiliary inference models. Latent variable models with multiple layers of stochastic
variables  such as the one used in our experiments  are often equivalent to such auxiliary-variable
methods. We combine deep latent variable models with IAF in our experiments  beneﬁting from both
techniques.

6 Experiments

We empirically evaluate IAF by applying the idea to improve variational autoencoders. Please see
appendix C for details on the architectures of the generative model and inference models. Code for
reproducing key empirical results is available online3.

6.1 MNIST
In this expermiment we follow a similar implementation of the convolutional VAE as in (Salimans
et al.  2014) with ResNet (He et al.  2015) blocks. A single layer of Gaussian stochastic units
of dimension 32 is used. To investigate how the expressiveness of approximate posterior affects
performance  we report results of different IAF posteriors with varying degrees of expressiveness.
We use a 2-layer MADE (Germain et al.  2015) to implement one IAF transformation  and we stack
multiple IAF transformations with ordering reversed between every other transformation.
Results: Table 1 shows results on MNIST for these types of posteriors. Results indicate that as
approximate posterior becomes more expressive  generative modeling performance becomes better.
Also worth noting is that an expressive approximate posterior also tightens variational lower bounds
as expected  making the gap between variational lower bounds and marginal likelihoods smaller. By
making IAF deep and wide enough  we can achieve best published log-likelihood on dynamically

3https://github.com/openai/iaf

6

Table 1: Generative modeling results on the dynamically sampled binarized MNIST version used
in previous publications (Burda et al.  2015). Shown are averages; the number between brackets
are standard deviations across 5 optimization runs. The right column shows an importance sampled
estimate of the marginal likelihood for each model with 128 samples. Best previous results are repro-
duced in the ﬁrst segment: [1]: (Salimans et al.  2014) [2]: (Burda et al.  2015) [3]: (Kaae Sønderby
et al.  2016) [4]: (Tran et al.  2015)

Model

Convolutional VAE + HVI [1]
DLGM 2hl + IWAE [2]
LVAE [3]
DRAW + VGP [4]

VLB

-83.49

-79.88

log p(x) ⇡
-81.94
-82.90
-81.74

Diagonal covariance
IAF (Depth = 2  Width = 320)
IAF (Depth = 2  Width = 1920)
IAF (Depth = 4  Width = 1920)
IAF (Depth = 8  Width = 1920)

-84.08 (± 0.10)
-82.02 (± 0.08)
-81.17 (± 0.08)
-80.93 (± 0.09)
-80.80 (± 0.07)

-81.08 (± 0.08)
-79.77 (± 0.06)
-79.30 (± 0.08)
-79.17 (± 0.08)
-79.10 (± 0.07)

…

…

…

…

…

z3

z2

z1

+

z3

z2

z1

=

z3

z2

z1

x

x

Deep 

generative model

Bidirectional 
inference model

x

x

VAE with 

bidirectional inference

+

Bottom-Up 
ResNet Block

Top-Down 
ResNet Block

ELU

ELU

Layer Prior: 
z ~ p(zi|z>i)

+

Layer Posterior: 
z ~ q(zi|z>i x)

ELU

ELU

+

= Identity
Identity

= Convolution

ELU

= Nonlinearity

Figure 3: Overview of our ResNet VAE with bidirectional inference. The posterior of each layer is
parameterized by its own IAF.

binarized MNIST: -79.10. On Hugo Larochelle’s statically binarized MNIST  our VAE with deep
IAF achieves a log-likelihood of -79.88  which is slightly worse than the best reported result  -79.2 
using the PixelCNN (van den Oord et al.  2016b).

6.2 CIFAR-10

We also evaluated IAF on the CIFAR-10 dataset of natural images. Natural images contain a much
greater variety of patterns and structure than MNIST images; in order to capture this structure well 
we experiment with a novel architecture  ResNet VAE  with many layers of stochastic variables  and
based on residual convolutional networks (ResNets) (He et al.  2015  2016). Please see our appendix
for details.

Log-likelihood. See table 2 for a comparison to previously reported results. Our architecture with
IAF achieves 3.11 bits per dimension  which is better than other published latent-variable models 
and almost on par with the best reported result using the PixelCNN. See the appendix for more
experimental results. We suspect that the results can be further improved with more steps of ﬂow 
which we leave to future work.

Synthesis speed. Sampling took about 0.05 seconds/image with the ResNet VAE model  versus
52.0 seconds/image with the PixelCNN model  on a NVIDIA Titan X GPU. We sampled from the
PixelCNN naïvely by sequentially generating a pixel at a time  using the full generative model at each
iteration. With custom code that only evaluates the relevant part of the network  PixelCNN sampling
could be sped up signiﬁcantly; however the speedup will be limited on parallel hardware due to the

7

Table 2: Our results with ResNet VAEs on CIFAR-10 images  compared to earlier results  in average
number of bits per data dimension on the test set. The number for convolutional DRAW is an upper
bound  while the ResNet VAE log-likelihood was estimated using importance sampling.
bits/dim 

Method

Results with tractable likelihood models:
Uniform distribution (van den Oord et al.  2016b)
Multivariate Gaussian (van den Oord et al.  2016b)
NICE (Dinh et al.  2014)
Deep GMMs (van den Oord and Schrauwen  2014)
Real NVP (Dinh et al.  2016)
PixelRNN (van den Oord et al.  2016b)
Gated PixelCNN (van den Oord et al.  2016c)

Results with variationally trained latent-variable models:
Deep Diffusion (Sohl-Dickstein et al.  2015)
Convolutional DRAW (Gregor et al.  2016)
ResNet VAE with IAF (Ours)

8.00
4.70
4.48
4.00
3.49
3.00
3.03

5.40
3.58
3.11

sequential nature of the sampling operation. Efﬁcient sampling from the ResNet VAE is a parallel
computation that does not require custom code.

7 Conclusion

We presented inverse autoregressive ﬂow (IAF)  a new type of normalizing ﬂow that scales well to
high-dimensional latent space. In experiments we demonstrated that autoregressive ﬂow leads to
signiﬁcant performance gains compared to similar models with factorized Gaussian approximate
posteriors  and we report close to state-of-the-art log-likelihood results on CIFAR-10  for a model
that allows much faster sampling.

Acknowledgements

We thank Jascha Sohl-Dickstein  Karol Gregor  and many others at Google Deepmind for interesting
discussions. We thank Harri Valpola for referring us to Gustavo Deco’s relevant pioneering work on
a form of inverse autoregressive ﬂow applied to nonlinear independent component analysis.

References
Blei  D. M.  Jordan  M. I.  and Paisley  J. W. (2012). Variational Bayesian inference with Stochastic Search. In

Proceedings of the 29th International Conference on Machine Learning (ICML-12)  pages 1367–1374.

Bowman  S. R.  Vilnis  L.  Vinyals  O.  Dai  A. M.  Jozefowicz  R.  and Bengio  S. (2015). Generating sentences

from a continuous space. arXiv preprint arXiv:1511.06349.

Burda  Y.  Grosse  R.  and Salakhutdinov  R. (2015). Importance weighted autoencoders. arXiv preprint

arXiv:1509.00519.

Clevert  D.-A.  Unterthiner  T.  and Hochreiter  S. (2015). Fast and accurate deep network learning by Exponential

Linear Units (ELUs). arXiv preprint arXiv:1511.07289.

Deco  G. and Brauer  W. (1995). Higher order statistical decorrelation without information loss. Advances in

Neural Information Processing Systems  pages 247–254.

Dinh  L.  Krueger  D.  and Bengio  Y. (2014). Nice: non-linear independent components estimation. arXiv

preprint arXiv:1410.8516.

Dinh  L.  Sohl-Dickstein  J.  and Bengio  S. (2016). Density estimation using Real NVP. arXiv preprint

arXiv:1605.08803.

8

Germain  M.  Gregor  K.  Murray  I.  and Larochelle  H. (2015). Made: Masked autoencoder for distribution

estimation. arXiv preprint arXiv:1502.03509.

Gregor  K.  Besse  F.  Rezende  D. J.  Danihelka  I.  and Wierstra  D. (2016). Towards conceptual compression.

arXiv preprint arXiv:1604.08772.

Gregor  K.  Mnih  A.  and Wierstra  D. (2013). Deep AutoRegressive Networks. arXiv preprint arXiv:1310.8499.

He  K.  Zhang  X.  Ren  S.  and Sun  J. (2015). Deep residual learning for image recognition. arXiv preprint

arXiv:1512.03385.

He  K.  Zhang  X.  Ren  S.  and Sun  J. (2016). Identity mappings in deep residual networks. arXiv preprint

arXiv:1603.05027.

Hoffman  M. D.  Blei  D. M.  Wang  C.  and Paisley  J. (2013). Stochastic variational inference. The Journal of

Machine Learning Research  14(1):1303–1347.

Jozefowicz  R.  Zaremba  W.  and Sutskever  I. (2015). An empirical exploration of recurrent network archi-
tectures. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15)  pages
2342–2350.

Kaae Sønderby  C.  Raiko  T.  Maaløe  L.  Kaae Sønderby  S.  and Winther  O. (2016). How to train deep

variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282.

Kingma  D. P. and Welling  M. (2013). Auto-encoding variational Bayes. Proceedings of the 2nd International

Conference on Learning Representations.

Ranganath  R.  Tran  D.  and Blei  D. M. (2015). Hierarchical variational models.

arXiv:1511.02386.

arXiv preprint

Rezende  D. and Mohamed  S. (2015). Variational inference with normalizing ﬂows. In Proceedings of The

32nd International Conference on Machine Learning  pages 1530–1538.

Rezende  D. J.  Mohamed  S.  and Wierstra  D. (2014). Stochastic backpropagation and approximate inference
In Proceedings of the 31st International Conference on Machine Learning

in deep generative models.
(ICML-14)  pages 1278–1286.

Salimans  T. (2016). A structured variational auto-encoder for learning deep hierarchies of sparse features. arXiv

preprint arXiv:1602.08734.

Salimans  T. and Kingma  D. P. (2016). Weight normalization: A simple reparameterization to accelerate training

of deep neural networks. arXiv preprint arXiv:1602.07868.

Salimans  T.  Kingma  D. P.  and Welling  M. (2014). Markov chain Monte Carlo and variational inference:

Bridging the gap. arXiv preprint arXiv:1410.6460.

Sohl-Dickstein  J.  Weiss  E. A.  Maheswaranathan  N.  and Ganguli  S. (2015). Deep unsupervised learning

using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585.

Tran  D.  Ranganath  R.  and Blei  D. M. (2015). Variational gaussian process. arXiv preprint arXiv:1511.06499.

van den Oord  A.  Dieleman  S.  Zen  H.  Simonyan  K.  Vinyals  O.  Graves  A.  Kalchbrenner  N.  Senior  A. 
and Kavukcuoglu  K. (2016a). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.

van den Oord  A.  Kalchbrenner  N.  and Kavukcuoglu  K. (2016b). Pixel recurrent neural networks. arXiv

preprint arXiv:1601.06759.

van den Oord  A.  Kalchbrenner  N.  Vinyals  O.  Espeholt  L.  Graves  A.  and Kavukcuoglu  K. (2016c).

Conditional image generation with pixelcnn decoders. arXiv preprint arXiv:1606.05328.

van den Oord  A. and Schrauwen  B. (2014). Factoring variations in natural images with deep gaussian mixture

models. In Advances in Neural Information Processing Systems  pages 3518–3526.

Zagoruyko  S. and Komodakis  N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146.

Zeiler  M. D.  Krishnan  D.  Taylor  G. W.  and Fergus  R. (2010). Deconvolutional networks. In Computer

Vision and Pattern Recognition (CVPR)  2010 IEEE Conference on  pages 2528–2535. IEEE.

9

,Durk Kingma
Tim Salimans
Rafal Jozefowicz
Xi Chen
Ilya Sutskever
Max Welling