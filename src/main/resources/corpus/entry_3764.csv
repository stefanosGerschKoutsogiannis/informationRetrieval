2013,New Subsampling Algorithms for Fast Least Squares Regression,We address the problem of fast estimation of ordinary least squares (OLS) from large amounts of data ($n \gg p$). We propose three methods which solve the big data problem by subsampling the covariance matrix using either a single or two stage estimation. All three run in the order of size of input i.e. O($np$) and our best method  {\it Uluru}  gives an error bound of $O(\sqrt{p/n})$ which is independent of the amount of subsampling as long as it is above a threshold.  We provide theoretical bounds for our algorithms in the fixed design (with Randomized Hadamard preconditioning) as well as sub-Gaussian random design setting. We also compare the performance of our methods on synthetic and real-world datasets and show that if observations are i.i.d.  sub-Gaussian then one can directly subsample without the expensive Randomized Hadamard preconditioning without loss of accuracy.,New Subsampling Algorithms for Fast Least Squares

Regression

Paramveer S. Dhillon1 Yichao Lu2 Dean Foster2

Lyle Ungar1

1Computer & Information Science  2Statistics (Wharton School)

University of Pennsylvania  Philadelphia  PA  U.S.A

{dhillon|ungar}@cis.upenn.edu

foster@wharton.upenn.edu  yichaolu@sas.upenn.edu

Abstract

We address the problem of fast estimation of ordinary least squares (OLS) from
large amounts of data (n (cid:29) p). We propose three methods which solve the big
data problem by subsampling the covariance matrix using either a single or two
stage estimation. All three run in the order of size of input i.e. O(np) and our best

method  Uluru  gives an error bound of O((cid:112)p/n) which is independent of the

amount of subsampling as long as it is above a threshold. We provide theoretical
bounds for our algorithms in the ﬁxed design (with Randomized Hadamard pre-
conditioning) as well as sub-Gaussian random design setting. We also compare the
performance of our methods on synthetic and real-world datasets and show that if
observations are i.i.d.  sub-Gaussian then one can directly subsample without the
expensive Randomized Hadamard preconditioning without loss of accuracy.

1

Introduction

Ordinary Least Squares (OLS) is one of the oldest and most widely studied statistical estimation
methods with its origins tracing back over two centuries. It is the workhorse of ﬁelds as diverse
as Machine Learning  Statistics  Econometrics  Computational Biology and Physics. To keep pace
with the growing amounts of data ever faster ways of estimating OLS are sought. This paper focuses
on the setting (n (cid:29) p)  where n is the number of observations and p is the number of covariates or
features  a common one for web scale data.
Numerous approaches to this problem have been proposed [1  2  3  4  5]. The predominant approach
to solving big data OLS estimation involves using some kind of random projections  for instance 
transforming the data with a randomized Hadamard transform [6] or Fourier transform and then
uniformly sampling observations from the resulting transformed matrix and estimating OLS on this
smaller data set. The intuition behind this approach is that these frequency domain transformations
uniformize the data and smear the signal across all the observations so that there are no longer
any high leverage points whose omission could unduly inﬂuence the parameter estimates. Hence 
a uniform sampling in this transformed space sufﬁces. Another way of looking at this approach is
as preconditioning the design matrix with a carefully constructed data-independent random matrix
before subsampling. This approach has been used by a variety of papers proposing methods such as
the Subsampled Randomized Hadamard Transform (SRHT) [1  4] and the Subsampled Randomized
Fourier Transform (SRFT) [2  3]. There is also publicly available software which implements these
ideas [7]. It is worth noting that these approaches assume a ﬁxed design setting.
Following this line of work  in this paper we provide two main contributions:

1

1. Novel Subsampling Algorithms for OLS: We propose three novel1 algorithms for fast
estimation of OLS which work by subsampling the covariance matrix. Some recent re-

sults in [8] allow us to bound the difference between the parameter vector ((cid:98)w) we esti-

mate from the subsampled data and the true underlying parameter (w0) which generates
the data. We provide theoretical analysis of our algorithms in the ﬁxed design (with Ran-
domized Hadamard preconditioning) as well as sub-Gaussian random design setting. The
error bound of our best algorithm  Uluru  is independent of the fraction of data subsampled
(above a minimum threshold of sub-sampling) and depends only on the characteristics of
the data/design matrix X.

2. Randomized Hadamard preconditioning not always needed: We show that the error
bounds for all the three algorithms are similar for both the ﬁxed design and the sub-
Gaussian random design setting. In other words  one can either transform the data/design
matrix via Randomized Hadamard transform (ﬁxed design setting) and then use any of our
three algorithms or  if the observations are i.i.d. and sub-Gaussian  one can directly use
any of our three algorithms. Thus  another contribution of this paper is to show that if
the observations are i.i.d. and sub-Gaussian then one does not need the slow Randomized
Hadamard preconditioning step and one can get similar accuracies much faster.

The remainder of the paper is organized as follows: In the next section  we formally deﬁne nota-
tion for the regression problem  then in Sections 3 and 4  we describe our algorithms and provide
theorems characterizing their performance. Finally  we compare the empirical performance of our
methods on synthetic and real world data.

2 Notation and Preliminaries
Let X be the n × p design matrix. For the random design case we assume the rows of X are n i.i.d
samples from the 1 × p independent variable (a.k.a. “covariates” or “predictors”) X. Y is the real
valued n × 1 response vector which contains n corresponding values of the dependent variable Y
(in general we use bold letter for samples and normal letter for random variables or vectors).  is
the n × 1 homoskedastic noise vector with common variance σ2. We want to infer w0 i.e. the p × 1
population parameter vector that generated the data.
More formally  we can write the true model as:

Y = Xw0 + 
 ∼iid N (0  σ2)
equation
above

to

the

is

given

by

The

sample

(cid:98)wsample = (X(cid:62)X)−1X(cid:62)Y and by consistency of the OLS estimator we know that (cid:98)wsample →d w0
as n → ∞. Classical algorithms to estimate (cid:98)wsample use QR decomposition or bidiagonalization [9]

(in matrix

notation)

solution

subs ; Y(cid:62)

and they require O(np2) ﬂoating point operations.
Since our algorithms are based on subsampling the covariance matrix  we need some extra notation.
Let r = nsubs/n (< 1) be the subsampling ratio  giving the ratio of the number of observations
(nsubs) in the subsampled matrix Xsubs fraction to the number of observations (n) in the original
X matrix. I.e.  r is the fraction of the observations sampled. Let Xrem  Yrem denote the data and
response vector for the remaining n − nsubs observations. In other words X(cid:62) = [X(cid:62)
rem] and
Y(cid:62) = [Y(cid:62)
Also  let ΣXXbe the covariance of X and ΣXY be the covariance between X and Y . Then  for
the ﬁxed design setting ΣXX = X(cid:62)X/n and ΣXY = X(cid:62)Y/n and for the random design setting
ΣXX = E(X(cid:62)X) and ΣXY = E(X(cid:62)Y).
The bounds presented in this paper are expressed in terms of the Mean Squared Error (or Risk) for
the (cid:96)2 loss. For the ﬁxed design setting 

M SE = (w0 − (cid:98)w)(cid:62)X(cid:62)X(w0 − (cid:98)w)/n = (w0 − (cid:98)w)(cid:62)ΣXX (w0 − (cid:98)w)

subs ; X(cid:62)

rem].

For the random design setting

M SE = EX(cid:107)Xw0 − X(cid:98)w(cid:107)2 = (w0 − (cid:98)w)(cid:62)ΣXX (w0 − (cid:98)w)

1One of our algorithms (FS) is similar to [4] as we describe in Related Work. However  even for that

algorithm  our theoretical analysis is novel.

2

2.1 Design Matrix and Preconditioning

Thus far  we have not made any assumptions about the design matrix X. In fact  our algorithms and
analysis work for both ﬁxed design and random design settings.
As mentioned earlier  our algorithms involve subsampling the observations  so we have to ensure
that we do not leave behind any observations which are outliers/high leverage points; This is done
differently for ﬁxed and random designs. For the ﬁxed design setting the design matrix X is arbitrary
and may contain high leverage points. Therefore before subsampling we precondition the matrix by
a Randomized Hadamard/Fourier Transform [1  4] and after conditioning  the probability of having
high leverage points in the new design matrix becomes very small. On the other hand  if we assume
X to be random design and its rows are i.i.d. draws from some nice distribution like sub-Gaussian 
then the probability of having high leverage points is very small and we can happily subsample X
without preconditioning.
In this paper we analyze both the ﬁxed as well as sub-Gaussian random design settings. Since the
ﬁxed design analysis would involve transforming the design matrix with a preconditioner before
subsampling  some background on SRHT is warranted.

Subsampled Randomized Hadamard Transform (SRHT):
In the ﬁxed design setting we pre-
condition and subsample the data with a nsubs × n randomized hadamard transform matrix Θ(=

(cid:113) n

nsubs

RHD) as Θ · X.

The matrices R  H  and D are deﬁned as:

• R ∈ Rnsubs×n is a set of nsubs rows from the n × n identity matrix  where the rows are
• D ∈ Rn×n is a random diagonal matrix whose entries are independent random signs  i.e.

chosen uniformly at random without replacement.

random variables uniformly distributed on {±1}.

• H ∈ Rn×n is a normalized Walsh-Hadamard matrix  deﬁned as: Hn =

(cid:20)Hn/2 Hn/2

Hn/2 −Hn/2

(cid:21)

(cid:20)+1 +1
(cid:21)

+1 −1

with  H2 =

. H = 1√

n Hn is a rescaled version of Hn.

It is worth noting that HD is the preconditioning matrix and R is the subsampling matrix.
The running time of SRHT is n p log(p) ﬂoating point operations (FLOPS) [4]. [4] mention ﬁx-
ing nsubs = O(p). However  in our experiments we vary the amount of subsampling  which is
not something recommended by their theory. With varying subsampling  the run time becomes
O(n p log(nsubs)).

3 Three subsampling algorithms for fast linear regression

All our algorithms subsample the X matrix followed by a single or two stage ﬁtting and are described
below. The algorithms given below are for the random design setting. The algorithms for the ﬁxed
design are exactly the same as below  except that Xsubs  Ysubs are replaced by Θ · X  Θ · Y and
Xrem  Yrem with Θrem · X  Θrem · Y  where Θ is the SRHT matrix deﬁned in the previous section
and Θrem is the same as Θ  except that R is of size nrem × n. Still  for the sake of completeness 
the algorithms are described in detail in the Supplementary material.

Full Subsampling (FS): Full subsampling provides a baseline for comparison; In it we simply
r-subsample (X  Y) as (Xsubs  Ysubs) and use the subsampled data to estimate both the ΣXX and
ΣXY covariance matrices.

Covariance Subsampling (CovS):
In Covariance Subsampling we r-subsample X as Xsubs only to
estimate the ΣXX covariance matrix; we use all the n observations to compute the ΣXY covariance
matrix.

3

Uluru : Uluru2 is a two stage ﬁtting algorithm. In the ﬁrst stage it uses the r-subsampled (X  Y)
In the sec-
ond stage it uses the remaining data (Xrem  Yrem) to estimate the bias of the ﬁrst stage estimator

to get an initial estimate of (cid:98)w (i.e.  (cid:98)wF S) via the Full Subsampling (FS) algorithm.
wcorrect = w0 − (cid:98)wF S. The ﬁnal estimate (wU luru) is taken to be a weighted combination (gener-
ally just the sum) of the FS estimator and the second stage estimator ((cid:98)wcorrect). Uluru is described
In the second stage  since (cid:98)wF S is known  on the remaining data we have Yrem = Xremw0 + rem 

in Algorithm 1.

hence

Rrem = Yrem − Xrem · (cid:98)wF S

= Xrem(w0 − (cid:98)wF S) + rem

X(cid:62)

remXrem)−1X(cid:62)

remXrem takes too many FLOPS  we use

remRrem. Since computing X(cid:62)

so we correct almost all the error  hence reducing the bias.

The above formula shows we can estimate wcorrect = w0 − (cid:98)wF S with another regression  i.e.
(cid:98)wcorrect = (X(cid:62)
subXsub instead (which has already been computed). Finally we correct (cid:98)wF S and (cid:98)wcorrect to get
(cid:98)wU luru. The estimate wcorrect can be seen as an almost unbiased estimation of the error w0 − wsubs 
Output: (cid:98)w
(cid:98)wF S = (X(cid:62)
Rrem = Yrem − Xrem · (cid:98)wF S;
subsXsubs)−1X(cid:62)
(cid:98)wcorrect = nsubs
(cid:98)wU luru = (cid:98)wF S + (cid:98)wcorrect;
nrem · (X(cid:62)
return (cid:98)w = (cid:98)wU luru;

subsXsubs)−1X(cid:62)

remRrem;

Input: X  Y  r

subsYsubs;

Algorithm 1: Uluru Algorithm

4 Theory

In this section we provide the theoretical guarantees of the three algorithms we discussed in the
previous sections in the ﬁxed as well as random design setting. All the theorems assume OLS
setting as mentioned in Section 2. Without loss of generality we assume that X is whitened  i.e.
ΣX X = Ip (see Supplementary Material for justiﬁcation). For both the cases we bound the square

root of Mean Squared Error which becomes (cid:107)w0 − (cid:98)w(cid:107)  as described in Section 2.

4.1 Fixed Design Setting

Here we assume preconditioning and subsampling with SRHT as described in previous sections.
(Please see the Supplementary Material for all the Proofs)
Theorem 1 Assume X ∈ n × p and X(cid:62)X = n · Ip. Let Y = Xw0 +  where  ∈ n × 1 is i.i.d.
gaussian noise with standard deviation σ.
If we use algorithm FS  then with failure probability at most 2 n

ep + 2δ 

(cid:107)w0 − ˆwF S(cid:107) ≤ Cσ

ln(nr + 1/δ)

p
nr

(1)

(cid:114)

(cid:115)

Theorem 2 Assuming our data comes from the same model as Theorem 1 and we use CovS  then
with failure probability at most 3δ + 3 n
ep  

(cid:32)

(cid:114)

(cid:107)w0− ˆwCovS(cid:107) ≤ (1−r)

C1

ln(

+ C2

ln(

2p
δ

)

p
nr

2p
δ

)

p

n(1 − r)

(cid:107)w0(cid:107)+C3σ

(cid:33)

(cid:114)

log(n + 1/δ)

p
n
(2)

2Uluru is a rock that is shaped like a quadratic and is solid. So  if your estimate of the quadratic term is as

solid as Uluru  you do not need use more data to make it more accurate.

4

Theorem 3 Assuming our data comes from the same model as Theorem 1 and we use Uluru  then
with failure probability at most 5δ + 5 n
ep  

(cid:115)

(cid:33)

(cid:107)w0 − ˆwU luru(cid:107) ≤ σ

ln(nr + 1/δ)

C1

ln(

+ C2

ln(

2p
δ

)

p

n(1 − r)

(cid:114)

(cid:114)

(cid:32)

(cid:114)

p
nr

+σC3

ln(n(1 − r) + 1/δ) ·

2p
δ

)

p
nr

p

n(1 − r)

Remark 1 The probability n
ep becomes really small for large p  hence it can be ignored and the
ln terms can be viewed as constants. Lets consider the case nsubs (cid:28) nrem  since only in this
situation subsampling reduces computational cost signiﬁcantly. Then  keeping only the dominating
terms  the result of the above three theorems can be summarized as: With some failure probability
nr )  the error of CovS algorithm is

less than some ﬁxed number  the error of FS algorithm is O(σ(cid:112) p
O((cid:112) p

nr(cid:107)w(cid:107) + σ(cid:112) p

n ) and the error of Uluru algorithm is O(σ p

nr + σ(cid:112) p

n )

4.2 Sub-gaussian Random Design Setting

4.2.1 Deﬁnitions

The following two deﬁnitions from [10] characterize what it means to be sub-gaussian.

Deﬁnition 1 A random variable X is sub-gaussian with sub-gaussian norm (cid:107)X(cid:107)ψ2 if and only if
(3)

for all p ≥ 1
Here (cid:107)X(cid:107)ψ2 is the minimal constant for which the above condition holds.
Deﬁnition 2 A random vector X ∈ Rn is sub-gaussian if the one dimensional marginals x(cid:62)X are
sub-gaussian for all x ∈ Rn. The sub-gaussian norm of random vector X is deﬁned as

(E|X|p)1/p ≤ (cid:107)X(cid:107)ψ2

√

p

(cid:107)X(cid:107)ψ2 = sup
(cid:107)x(cid:107)2=1

(cid:107)x(cid:62)X(cid:107)ψ2

(4)

Remark 2 Since the sum of two sub-gaussian variables is sub-gaussian  it is easy to conclude that
a random vector X = (X1  ..Xp)(cid:62) is a sub-gaussian random vector when the components X1  ..Xp
are sub-gaussian variables.

4.2.2 Sub-gaussian Bounds

Under the assumption that the rows of the design matrix X are i.i.d draws for a p dimensional
sub-Gaussian random vector X with ΣXX = Ip  we have the following bounds (Please see the
Supplementary Material for all the Proofs):

Theorem 4 If we use the FS algorithm  then with failure probability at most δ 

Theorem 5 If we use the CovS algorithm  then with failure probability at most δ 

(cid:114)

p · ln(2p/δ)

nr

(cid:107)w0 − (cid:98)wF S(cid:107) ≤ Cσ
(cid:32)
(cid:114) p
(cid:107)w0 − (cid:98)wCovS(cid:107) ≤ (1 − r)
(cid:114)

C1

+ C3σ

n · r

+ C2
p · ln(2(p + 2)/δ)

(cid:114) p

(cid:33)

n(1 − r)

(cid:107)w0(cid:107)

(5)

(6)

n

5

Theorem 6 If we use Uluru  then with failure probability at most δ 

(cid:34)

(cid:114) p

C2

n · r

(cid:114)

+ C3

p

(1 − r) · n

(cid:35)

(cid:107)w0 − (cid:98)wU luru(cid:107) ≤ C1σ

(cid:114)
(cid:115)

p · ln(2(2p + 2)/δ)

n · r

p · ln(2(2p + 2)/δ)

(1 − r) · n

+ C4σ

Remark 3 Here also  the ln terms can be viewed as constants. Consider the case r (cid:28) 1  since this
is the only case where subsampling reduces computational cost signiﬁcantly. Keeping only dominat-
ing terms  the result of the above three theorems can be summarized as: With failure probability less
rn )  the error of the CovS algorithm
n ). These errors are

than some ﬁxed number  the error of the FS algorithm is O(σ(cid:112) p
is O((cid:112) p

n ) and the error of the Uluru algorithm is O(σ p

rn(cid:107)w(cid:107) + σ(cid:112) p

rn + σ(cid:112) p

exactly the same as in the ﬁxed design case.

4.3 Discussion

We can make a few salient observations from the error expressions for the algorithms presented in
Remarks 1 & 3.
The second term for the error of the Uluru algorithm does not contain r at all. If it is the dominating
term  which is the case if

then the error of Uluru is approximately O(σ(cid:112) p

(7)
n )  which is completely independent of r. Thus  if
r is not too small (i.e.  when Eq. 7 holds)  the error bound for Uluru is not a function of r. In other
words  when Eq. 7 holds  we do not increase the error by using less data in estimating the covariance
matrix in Uluru. FS Algorithm does not have this property since its error is proportional to 1√
r .

r > O((cid:112)p/n)

Similarly  for the CovS algorithm  when

r > O(

(cid:107)w0(cid:107)2
σ2

)

(8)

the second term dominates and we can conclude that the error does not change with r. However 
Eq. 8 depends on how large the standard deviation σ of the noise is. We can assume (cid:107)w0(cid:107)2 = O(p)
since it is p dimensional. Hence if σ ≤ O(
p)  Eq. 8 fails since it implies r > O(1) and the error
bound of CovS algorithm increases with r.

√

To sum this up  Uluru has the nice property that its error bound does not increase as r gets smaller
as long as r is greater than a threshold. This threshold is completely independent of how noisy the
data is and only depends on the characteristics of the design/data matrix (n  p).

4.4 Run Time complexity

Table 1 summarizes the run time complexity and theoretically predicted error bounds for all the
methods. We use these theoretical run times (FLOPS) in our plots.

5 Experiments

In this section we elucidate the relative merits of our methods by comparing their empirical perfor-
mance on both synthetic and real world datasets.

5.1 Methodology

We can compare our algorithms by allowing them each to have about O(np) CPU time (ignoring
the log factors). This is of order the same time as it takes to read the data. Our target accuracy is

(cid:112)p/n  namely what a full least squares algorithm would generate. We will assume n (cid:29) p. The

6

Error
bound

Methods
Methods
OLS
FS
CovS
Uluru
SRHT-FS
SRHT-CovS O(max(n p log(p)  nsubs p2 + n p))

O((cid:112)p/n)
O((cid:112)p/nsubs)
O((cid:112)p/n)
O((cid:112)p2/n)
SRHT-Uluru O(max(n p log(p)  nsubs p2 + n p)) O((cid:112)p/n)

Running Time
O(FLOPS)
O(n p2)
O(nsubs p2)
O(nsubs p2 + n p)
O(nsubs p2 + n p)
O(max(n p log(p)  nsubs p2))

*

*

Table 1: Runtime complexity. nsubs is the number of observations in the subsample  n is the number of
observations  and p is the number of predictors. * indicates that no uniform error bounds are known.

subsample size  nsubs  for FS should be O(n/p) to keep the CPU time O(np)  which leads to an

accuracy of(cid:112)p2/n. For the CovS method  the accuracy depends on how noisy our data is (i.e. how
big σ is). When σ is large  it performs as well as(cid:112)p/n  which is the same as full least squares.
When σ is small  it performs as poorly as(cid:112)p2/n. For Uluru  to keep the CPU time O(np)  nsubs
when r ≥ O((cid:112)p/n) (in this set up we want r = O(1/p)  which implies n ≥ O(p3))  Uluru has
error bound O((cid:112)p/n) no matter what signal noise ratio the problem has.

should be O(n/p) or equivalently r = O(1/p). As stated in the discussions after the theorems 

5.2 Synthetic Datasets

We generated synthetic data by distributing the signal uniformly across all the p singular values 
picking the p singular values to be λi = 1/i2  i = 1 : p  and further varying the amount of signal.

5.3 Real World Datasets

We also compared the performance of the algorithms on two UCI datasets 3: CPUSMALL (n=8192 
p=12) and CADATA (n=20640  p=8) and the PERMA sentiment analysis dataset described in [11]
(n=1505  p=30)  which uses LR-MVL word embeddings [12] as features. 4

5.4 Results

The results for synthetic data are shown in Figure 1 (top row) and for real world datasets are also
shown in Figure 1 (bottom row).
To generate the plots  we vary the amount of data used in the subsampling  nsubs  from 1.1p to n.
For FS  this simply means using a fraction of the data; for CovS and Uluru  only the data for the
covariance matrix is subsampled. We report the Mean Squared Error (MSE)  which in the case of
squared loss is the same as the risk  as was described in Section 2. For the real datasets we do not
know the true population parameter  w0  so we replace it with its consistent estimator wM LE  which
is computed using standard OLS on the entire dataset.
The horizontal gray line in the ﬁgures is the overﬁtting point; it is the error generated by ˆw vector of
all zeros. The vertical gray line is the n · p point; thus anything which is faster than that must look
at only some of the data.
Looking at the results  we can see two trends for the synthetic data. Firstly  our algorithms with
no preconditioning are much faster than their counterparts with preconditioning and give similar
accuracies. Secondly  as we had expected  CovS performs best for high noise setting being slightly
better than Uluru  and Uluru is signiﬁcantly better for low noise setting.
For real world datasets also  Uluru is almost always better than the other algorithms  both with and
without preconditioning. As earlier  the preconditioned alternatives are slower.

3http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/regression.html
4We also compared our approaches against coordinate ascent methods from [13] and our algorithms outper-

form them. Due to paucity of space we relegated that comparison to the supplementary material.

7

Figure 1: Results for synthetic datasets (n=4096  p=8) in top row and for (PERMA  CPUSMALL  CADATA 
left-right) bottom row. The three columns in the top row have different amounts of signal  2 
p and n
p
respectively. In all settings  we varied the amount of subsampling from 1.1 p to n in multiples of 2. Color
scheme: + (Green)-FS  + (Blue)-CovS  + (Red)-Uluru. The solid lines indicate no preconditioning (i.e.
random design) and dashed lines indicate ﬁxed design with Randomized Hadamard preconditioning. The
FLOPS reported are the theoretical values (see Supp. material)  the actual values were noisy due to varying
load settings on CPUs.

(cid:113) n

6 Related Work

The work that comes closest to our work is the set of approaches which precondition the matrix by
either Subsampled Randomized Hadamard Transform (SRHT) [1  4]  or Subsampled Randomized
Fourier Transform (SRFT) [2  3]  before subsampling uniformly from the resulting transformed
matrix.
However  this line of work is different our work in several ways. They are doing their analysis in a
mathematical set up  i.e. solving an overdetermined linear system ( ˆw = arg minw∈Rp (cid:107)Xw − Y (cid:107)2) 
while we are working in a statistical set up (a regression problem Y = Xβ + ) which leads to
different error analysis.
Our FS algorithm is essentially the same as the subsampling algorithm proposed by [4]. However 
our theoretical analysis of it is novel  and furtheremore they only consider it in ﬁxed design setting
with Hadamard preconditioning.
The CovS and Uluru are entirely new algorithms and as we have seen differ from FS in a key sense 
namely that CovS and Uluru make use of all the data but FS uses only a small proportion of the data.

7 Conclusion
In this paper we proposed three subsampling methods for faster least squares regression. All three
run in O(size of input) = np. Our best method  Uluru  gave an error bound which is independent of
the amount of subsampling as long as it is above a threshold.
Furthermore  we argued that for problems arising from linear regression  the Randomized Hadamard
transformation is often not needed. In linear regression  observations are generally i.i.d. If one fur-
ther assumes that they are sub-Gaussian (perhaps as a result of a preprocessing step  or simply
because they are 0/1 or Gaussian)  then subsampling methods without a Randomized Hadamard
transformation sufﬁce. As shown in our experiments  dropping the Randomized Hadamard trans-
formation signiﬁcantly speeds up the algorithms and in i.i.d sub-Gaussian settings  does so without
loss of accuracy.

8

0.020.050.200.502.005.001e+001e+021e+041e+06# FLOPS/n*pMSE/Risk0.020.050.200.502.005.001e−011e+011e+031e+05# FLOPS/n*pMSE/Risk0.020.050.200.502.005.001e−031e+001e+031e+06# FLOPS/n*pMSE/Risk125101e−011e+011e+031e+05# FLOPS/n*pMSE/Risk0.020.050.200.502.005.001e−051e−021e+011e+041e+07# FLOPS/n*pMSE/Risk5e−035e−025e−015e+001e−051e−021e+011e+04# FLOPS/n*pMSE/RiskReferences
[1] Boutsidis  C.  Gittens  A.:

hadamard transform. CoRR abs/1204.0062 (2012)

Improved matrix algorithms via the subsampled randomized

[2] Tygert  M.: A fast algorithm for computing minimal-norm solutions to underdetermined sys-

tems of linear equations. CoRR abs/0905.4745 (2009)

[3] Rokhlin  V.  Tygert  M.: A fast randomized algorithm for overdetermined linear least-squares
regression. Proceedings of the National Academy of Sciences 105(36) (September 2008)
13212–13217

[4] Drineas  P.  Mahoney  M.W.  Muthukrishnan  S.  Sarl´os  T.: Faster least squares approxima-

tion. CoRR abs/0710.1435 (2007)

[5] Mahoney  M.W.: Randomized algorithms for matrices and data. (April 2011)
[6] Ailon  N.  Chazelle  B.: Approximate nearest neighbors and the fast johnson-lindenstrauss

transform. In: STOC. (2006) 557–563

[7] Avron  H.  Maymounkov  P.  Toledo  S.: Blendenpik: Supercharging lapack’s least-squares

solver. SIAM J. Sci. Comput. 32(3) (April 2010) 1217–1236

[8] Vershynin  R.: How Close is the Sample Covariance Matrix to the Actual Covariance Matrix?

Journal of Theoretical Probability 25(3) (September 2012) 655–686

[9] Golub  G.H.  Van Loan  C.F.: Matrix Computations (Johns Hopkins Studies in Mathematical

Sciences)(3rd Edition). 3rd edn. The Johns Hopkins University Press (October 1996)

[10] Vershynin  R.:

Introduction to the non-asymptotic analysis of random matrices. CoRR

abs/1011.3027 (2010)

[11] Dhillon  P.S.  Rodu  J.  Foster  D.  Ungar  L.: Two step cca: A new spectral method for
estimating vector models of words. In: Proceedings of the 29th International Conference on
Machine learning. ICML’12 (2012)

[12] Dhillon  P.S.  Foster  D.  Ungar  L.: Multi-view learning of word embeddings via cca. In:

Advances in Neural Information Processing Systems (NIPS). Volume 24. (2011)

[13] Shalev-Shwartz  S.  Zhang  T.: Stochastic dual coordinate ascent methods for regularized loss

minimization. CoRR abs/1209.1873 (2012)

9

,Paramveer Dhillon
Yichao Lu
Dean Foster
Lyle Ungar