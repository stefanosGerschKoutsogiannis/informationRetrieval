2019,Knowledge Extraction with No Observable Data,Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge  it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However  the data are not accessible in many cases due to the privacy or confidentiality issues in medical  industrial  and military domains. To the best of our knowledge  there has been no approach that distills the knowledge of a neural network when no data are observable. In this work  we propose KegNet (Knowledge Extraction with Generative Networks)  a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KegNet outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.,Knowledge Extraction with No Observable Data

Jaemin Yoo

Seoul National University
jaeminyoo@snu.ac.kr

Taebum Kim

Seoul National University
k.taebum@snu.ac.kr

Minyong Cho

Seoul National University
chominyong@gmail.com

U Kang∗

Seoul National University

ukang@snu.ac.kr

Abstract

Knowledge distillation is to transfer the knowledge of a large neural network into
a smaller one and has been shown to be effective especially when the amount of
training data is limited or the size of the student model is very small. To transfer
the knowledge  it is essential to observe the data that have been used to train the
network since its knowledge is concentrated on a narrow manifold rather than the
whole input space. However  the data are not accessible in many cases due to the
privacy or conﬁdentiality issues in medical  industrial  and military domains. To the
best of our knowledge  there has been no approach that distills the knowledge of a
neural network when no data are observable. In this work  we propose KEGNET
(Knowledge Extraction with Generative Networks)  a novel approach to extract the
knowledge of a trained deep neural network and to generate artiﬁcial data points
that replace the missing training data in knowledge distillation. Experiments show
that KEGNET outperforms all baselines for data-free knowledge distillation. We
provide the source code of our paper in https://github.com/snudatalab/KegNet.

1

Introduction

How can we distill the knowledge of a deep neural network without any observable data?
Knowledge distillation [9] is to transfer the knowledge of a large neural network or an ensemble of
neural networks into a smaller network. Given a set of trained teacher models  one feeds training
data to them and uses their predictions instead of the true labels to train the small student model. It
has been effective especially when the amount of training data is limited or the size of the student
model is very small [14  28]  because the teacher’s knowledge helps the student to learn efﬁciently
the hidden relationships between the target labels even with a small dataset.
However  it is essential for knowledge distillation that at least a few training examples are observable 
since the knowledge of a deep neural network does not cover the whole input space; it is focused
on a manifold px of data that the network has actually observed. The network is likely to produce
unpredictable outputs if given random inputs that are not described by px  misguiding the student
network. There are recent works for distilling a network’s knowledge by a small dataset [22] or only
metadata at each layer [23]  but no approach has successfully distilled the knowledge without any
observable data. It is desirable in this case to generate artiﬁcial data by generative networks [7  31] 
but they also require a large amount of training data to estimate the true manifold px.
We propose KEGNET (Knowledge Extraction with Generative Networks)  a novel architecture that
extracts the knowledge of a trained neural network for knowledge distillation without observable data.

∗Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: An overall structure of KEGNET. The generator creates artiﬁcial data and feed them into
the classiﬁer and decoder. The ﬁxed classiﬁer produces the label distribution of each data point  and
the decoder ﬁnds its hidden representation as a low-dimensional vector.

KEGNET estimates the data manifold px by generating artiﬁcial data points  based on the conditional
distribution p(y|x) of label y which has been learned by the given neural network. The generated
examples replace the missing training data when distilling the knowledge of the given network. As a
result  the knowledge is transferred well to other networks even with no observable data.
The overall structure of KEGNET is depicted in Figure 1  which consists of two learnable networks G
and D. A trained network M is given as a teacher  and we aim to distill its knowledge to a student
network which is not included in this ﬁgure. The ﬁrst learnable network is a generator G that takes a
pair of sampled variables ˆy and ˆz to create a fake data point ˆx. The second network is a decoder D
which aims to extract the variable ˆz from ˆx  given as an input for ˆx. The variable ˆz is interpreted as a
low-dimensional representation of ˆx  which contains the implicit meaning of ˆx independently from
the label ˆy. The networks G and D are updated to minimize the reconstruction errors between the
input and output variables: ˆy and ¯y  and ˆz and ¯z. After the training  G is used to generate fake data
which replace the missing training data in the distillation; D is not used in this step.
Our extensive experiments in Section 5 show that KEGNET accurately extracts the knowledge of a
trained deep neural network for various types of datasets. KEGNET outperforms baseline methods
for distilling the knowledge without observable data  showing a large improvement of accuracy up to
39.6 percent points compared with the best competitors. Especially  KEGNET generates artiﬁcial
data whose patterns are clearly recognizable by extracting the knowledge of well-known classiﬁers
such as a residual network [8] trained with image datasets such as MNIST [21] and SVHN [25].

2 Related Work

2.1 Knowledge Distillation

Knowledge distillation [9] is a technique to transfer the knowledge of a large neural network or an
ensemble of neural networks into a small one. Given a teacher network M and a student network S 
we feed training data to M and use its predictions to train S instead of the true labels. As a result  S
is trained by soft distributions rather than one-hot vectors  learning latent relationships between the
labels that M has already learned. Knowledge distillation has been used for reducing the size of a
model or training a model with insufﬁcient data [1  2  12  14  29].
Recent works focused on the distillation with insufﬁcient data. Papernot et al. [28] and Kimura et al.
[15] used knowledge distillation to effectively use unlabeled data for semi-supervised learning when
a trained teacher network is given. Li et al. [22] added a 1 × 1 convolutional layer at the end of each
block of a student network and aligned the teacher and student networks by updating those layers
when only a few samples are given. Lopes et al. [23] considered the case where the data were not
observable  but metadata were given for each activation layer of the teacher network. From the given
metadata  they reconstructed the missing data and used them to train a student network.

2

Noise 𝑧̂Label 𝑦$Generated data𝑥$Generator 𝐺𝑝𝑥𝑦 𝑧Classifier 𝑀(fixed)Decoder 𝐷𝑝𝑧𝑥Label 𝑦+Noise 𝑧̅Classifier lossDecoder loss𝑝̂-𝑦𝑝.𝑧concatsamplingsamplingHowever  these approaches assume that at least a few labeled examples or metadata are given so that
it is able to estimate the distribution of missing data. They are not applicable to situations where no
data are accessible due to strict privacy or conﬁdentiality. To the best of our knowledge  there has
been no approach that works well without training data in knowledge distillation  despite its large
importance in various domains that impose strict limitations for distributing the data.

2.2 Tucker Decomposition

A tensor decomposition is to represent an n-dimensional tensor as a sequence of small tensors. Tucker
decomposition [13  30] is one of the most successful algorithms for a tensor decomposition  which
decomposes an n-dimensional tensor X ∈ RI1×I2×···×In into the following form:

ˆX = G ×1 A(1) ×2 A(2) ×3 ··· ×N A(N ) 

(1)
where ×i is the i-mode product [16] between a tensor and a matrix  G ∈ RR1×R2×···×Rn is a core
tensor  and A(i) ∈ RIi×Ri is the i-th factor matrix.
Tucker decomposition has been used to compress various types of deep neural networks. Kim et al.
[13] and Kholiavchenko [11] compressed convolutional neural networks using Tucker-2 decomposi-
tion which decomposes convolution kernels along the ﬁrst two axes (the numbers of ﬁlters and input
channels). They used the global analytic variational Bayesian matrix factorization (VBMF) [24] for
selecting the rank R  which is important to the performance of compression. Kossaiﬁ et al. [17] used
Tucker decomposition to compress fully connected layers as well as convolutional layers.
Unlike most compression algorithms [3  4]  Tucker decomposition itself is a data-free algorithm that
requires no training data in the execution. However  a ﬁne-tuning of the compressed networks has
been essential [11  13] since the compression is done layerwise and the compressed layers are not
aligned with respect to the target problem. In this work  we use Tucker decomposition to initialize a
student network that requires the teacher’s knowledge to improve its performance. Our work can be
seen as using Tucker decomposition as a general compression algorithm when the target network is
given but no data are observable  and can be extended to other compression algorithms.

3 Knowledge Extraction

We are given a trained network M that predicts the label of a feature vector x as a probability vector.
However  we have no information about the data distribution px(x) that was used to train M  which
is essential to understand its functionality and to use its learned knowledge in further tasks. It is thus
desirable to estimate px(x) from M  which is the opposite of a traditional learning problem that aims
to train M based on observable px. We call this knowledge extraction.
However  it is impracticable to estimate px(x) directly since the data space R|x| is exponential with
the dimensionality of data  while we have no single observation except the trained classiﬁer M. We
thus revert to sampling data points and modeling an empirical distribution: for a set D of sampled
points  the probability of each sampled point is 1/|D|  and the probability at any other point is zero.
We generate the set D of sampled data points by modeling a conditional probability of x given two
random vectors y and z  where y is a probability vector that represents a label  and z is our proposed
variable that represents the implicit meaning of a data point as a low-dimensional vector:

D =

p(ˆx|ˆy  ˆz) | ˆy ∼ ˆpy(y) and ˆz ∼ pz(z)

arg max

ˆx

 

(2)

where ˆpy(y) is an estimation of the true label distribution py(y) which we cannot observe  and pz(z)
is our proposed distribution that is assumed to describe the property of z.
In this way  we reformulate the problem as to estimate the conditional distribution p(x|y  z) instead
of the data distribution px(x). Recall that z is a low-dimensional representation of a data point x.
The variables y and z are conditionally independent of each other given x  since they both depend on
x but have no direct interactions. Thus  the argmax in Equation (2) is rewritten as follows:

p(ˆx|ˆy  ˆz) = arg max

arg max

ˆx

ˆx

= arg max

ˆx

(log p(ˆy|ˆx  ˆz) + log p(ˆx|ˆz) − log p(ˆy|ˆz))
(log p(ˆy|ˆx) + log p(ˆx|ˆz)) 

(3)

(4)

(cid:26)

(cid:27)

3

where the ﬁrst probability p(ˆy|ˆx) is the direct output of M when ˆx is given as an input  which we do
not need to estimate since M is already trained. The second probability p(ˆx|ˆz) represents how well ˆz
represents the property of ˆx as its low-dimensional representation.
However  estimating the distribution p(x|z) requires knowing px(x) in advance  which we cannot
observe due to the absence of accessible data. Thus  we rewrite Equation (4) as Equation (5) and then
approximate it as Equation (6) ignoring the data probability px(x):

p(ˆx|ˆy  ˆz) = arg max
≈ arg max

ˆx

(log p(ˆy|ˆx) + log p(ˆz|ˆx) + log p(ˆx) − log p(ˆz))
(log p(ˆy|ˆx) + log p(ˆz|ˆx)).

(6)
The difference is that now we estimate the likelihood p(ˆz|ˆx) of the variable ˆz given ˆx instead of the
posterior p(ˆx|ˆz). Equation (6) is our ﬁnal target of estimation for extracting the knowledge of the
given model M. We introduce in the next section how to model these conditional distributions by
deep neural networks and how to design an objective function which we aim to minimize.

ˆx

arg max

ˆx

(5)

4 Proposed Method

KEGNET (Knowledge Extraction with Generative Networks) is our novel architecture to distill the
knowledge of a neural network without using training data  by extracting its knowledge as a set of
artiﬁcial data points of Equation (2). KEGNET uses two kinds of deep neural networks to model the
conditional distributions in Equation (6). The ﬁrst is a generator G which takes ˆy and ˆz as inputs and
returns a data point with the maximum conditional likelihood p(ˆx|ˆy  ˆz). The second is a decoder D
which takes a data point ˆx as an input and returns its low-dimensional representation ¯z.
The overall structure of KEGNET is depicted in Figure 1. The generator G is our main component
that estimates the empirical distribution by sampling a data point ˆx several times. Given a sampled
class vector ˆy as an input  G is trained to produce data points that M is likely to classify as ˆy. This
makes G learn different properties of different classes based on M  but leads it to generate similar
data points for each class. To address this problem  we train G also to minimize the reconstruction
error between ˆz and ¯z  forcing G to embed the information of ˆz in the generated data ˆx so that D can
successfully recover it. Thus  data points of the same class should be different from each other when
given different input variables ˆz. The reconstruction errors are computed for ˆy and ˆz  respectively 
and then added to the ﬁnal objective function. We also introduce a diversity loss to further increase
the data diversity in each batch so that the generated data cover a larger region in the data space.

4.1 Objective Function

We formulate the conditional probabilities of Equation (6) as loss terms to train both the generator G
and decoder D  and combine them as a single objective function:

l(B) =

lcls(ˆy  ˆz) + αldec(ˆy  ˆz)

+ βldiv(B) 

(7)

(cid:17)

(cid:88)

(cid:16)

(ˆy ˆz)∈B

which consists of three different loss functions lcls  ldec  and ldiv. B is a batch of sampled variables
{(ˆy  ˆz) | ˆy ∼ ˆpy(y)  ˆz ∼ pz(z)}  and α and β are two nonnegative hyperparameters that adjust the
balance between the loss terms. Each batch is created by sampling ˆy and ˆz randomly several times
from the distributions ˆpy and pz which are determined also as hyperparameters. In our experiments 
we set ˆpy to the categorical distribution that produces one-hot vectors as ˆy  and pz to the multivariate
Gaussian distribution that produces standard normal vectors.
The classiﬁer loss lcls in Equation (8) represents the distance between the input label ˆy given to G and
the output M (G(ˆy  ˆz)) returned from M as the cross-entropy between two probability distributions.
Note that ˆy is not a scalar label but a probability vector of length |S| where S is the set of classes.
Minimizing lcls forces the generated data to follow a manifold that M is able to classify well. The
learned manifold may be different from px  but is suited for extracting the knowledge of M.

lcls(ˆy  ˆz) = −(cid:80)

i∈S ˆyi log M (G(ˆy  ˆz))i

(8)

The decoder loss ldec in Equation (9) represents the distance between the input variable ˆz given to
G and the output D(G(ˆy  ˆz)) returned from D. We use the Euclidean distance instead of the cross

4

entropy since z is not a probability distribution. If we optimize G only for lcls  it is likely to produce
similar data points for each class with little diversity. ldec prevents such a problem by forcing G to
include the information of ˆz along with ˆy in the generated data.
ldec(ˆy  ˆz) = (cid:107)ˆz − D(G(ˆy  ˆz))(cid:107)2
2.

(9)

However  the diversity of generated data points may still be insufﬁcient even though D forces G to
include the information of ˆz in ˆx. In such a case  the empirical distribution estimated by G covers
only a small manifold in the large data space  extracting only partial knowledge of M. The diversity
loss ldiv is introduced to address the problem and further increase the diversity of generated data.
Given a distance function d between two data points  the diversity loss ldiv is deﬁned as follows:

ldiv(B) = exp

(ˆy2 ˆz2)∈B (cid:107)ˆz1 − ˆz2(cid:107)2

2 · d(G(ˆy1  ˆz1)  G(ˆy2  ˆz2))

(10)
It increases the pairwise distance between sampled data points in each batch B  multiplied with the
distance between ˆz1 and ˆz2. This gives more weights to the pairs of data points whose input variables
are more distant by multiplying the distance of noise variables as a scalar weight. The exponential
function makes ldiv produce a positive value as a loss to be minimized. We set d to the Manhattan
distance function d(x1  x2) = (cid:107)x1 − x2(cid:107)1 in our experiments.

.

(cid:16)−(cid:80)

(ˆy1 ˆz1)∈B(cid:80)

(cid:17)

4.2 Relations to Other Structures

Autoencoders The overall structure of KEGNET can be understood as an autoencoder that tries to
reconstruct two variables y and z at the same time. It is speciﬁcally an overcomplete autoencoder
which learns a larger embedding vector than the target variables  since x is a learned representation
and y and z are target variables by this interpretation. It is typically difﬁcult to train an overcomplete
autoencoder because it can recover the target variable in the representation and make a zero recon-
struction error. However in our case  the trained classiﬁer M prevents such a problem because it acts
as a strong regularizer over the generated representations by classifying their labels based on its ﬁxed
knowledge. Thus  G needs to be trained carefully so that the generated representations ﬁt as correct
inputs to M  while containing the information of both y and z.

Generative adversarial networks KEGNET is similar to generated adversarial networks (GAN)
[7] in that a generator creates fake data to estimate the true distribution  and the generated data are
fed into another network to be evaluated. The structure of G is also motivated by DCGAN [31] and
ACGAN [26] for generating image datasets. However  the main difference from GAN-based models
is that we have no observable data and thus we cannot train a discriminator which separates fake data
from the real ones. We instead rely on the trained classiﬁer M and guide G indirectly toward the true
distribution. The decoder D in KEGNET can be understood as an adversarial model that hinders G
from converging to a naive solution  but it is not a direct counterpart of G. Thus  KEGNET can be
understood as a novel architecture designed for the case where no observable data are available.

4.3 Knowledge Distillation

To distill the knowledge of M  we use the trained generator G to create artiﬁcial data and feed them
into both the teacher M and student S. We apply the following two ideas to make S explore a large
space and to maximize its generalization performance. First  we use a set G of multiple generators
instead of a single network. Since each generator is initialized randomly  each of the generators learns
a data manifold that is different from those of the others. The number of generators is not limited
because they do not require observable training data. Second  we set ˆpy to the elementwise uniform
distribution which generates unnormalized probability vectors: ˆyi ∼ U(0  1) for each i. This gives an
uncertain evidence to G and forces it to generate data points which are not classiﬁed easily by M 
making M produce soft distributions in which its knowledge is embedded well.
As a result  our loss function ldis to train S by knowledge distillation is given as follows:

ldis(ˆy  ˆz) =

CE(M (G(ˆy  ˆz))  S(G(ˆy  ˆz))) 

(11)

(cid:88)

G∈G

where CE denotes the cross entropy. Previous works for knowledge distillation use a temperature [9]
to increase the entropy of predictions from M so that S can learn hidden relationships between the
classes more easily. We do not use the temperature since the predictions of M are soft already due to
our second idea of using the elementwise uniform distribution as ˆpy.

5

Table 1: Detailed information of datasets.

Dataset
8
Shuttle
16
PenDigits
16
Letter
1 × 28 × 28
MNIST
Fashion MNIST 1 × 28 × 28
3 × 32 × 32
SVHN

Features Labels Training Valid.
5 438
937
2 000
5 000
5 000
5 000

7
10
26
10
10
10

38 062
6 557
14 000
55 000
55 000
68 257

Test Properties

14 500 Unstructured
3 498 Unstructured
4 000 Unstructured
10 000 Grayscale images
10 000 Grayscale images
26 032 RGB images

Table 2: Classiﬁcation accuracy of KEGNET and the baseline methods on the unstructured datasets.
We report the compression ratios of student models along with the accuracy of Tucker.

Model Approach
MLP
MLP
MLP
MLP
MLP

Original
Tucker (T)
T+Uniform
T+Gaussian
T+KEGNET

Pendigits
96.56%
26.44% (8.07×)

Letter
Shuttle
95.63%
99.83%
75.49% (8.17×)
31.40% (4.13×)
93.83 ± 0.13% 80.21 ± 0.98% 62.50 ± 0.90%
94.00 ± 0.06% 78.22 ± 1.74% 76.80 ± 1.84%
94.21 ± 0.03% 82.62 ± 1.05% 77.73 ± 0.33%

5 Experiments

We evaluate KEGNET on two kinds of networks and datasets: multilayer perceptrons on unstructured
datasets from the UCI Machine Learning Repository2  and convolutional neural networks on MNIST
[21]  Fashion MNIST [33]  and SVHN [25]. The datasets are summarized as Table 1.
We compare KEGNET with baseline approaches for distilling the knowledge of a neural network
without using observable data. The simplest approach is to use Tucker decomposition alone  but the
resulting student is not optimized for the target problem because its objective is only to minimize
the reconstruction error. The second approach is to ﬁne-tune the student after Tucker decomposition
using artiﬁcial data derived from a sampling distribution. If the distribution is largely different from
the true distribution  this approach may even decrease the performance of the student from the ﬁrst
approach. We use the Gaussian distribution N (0  1) and uniform distribution U(−1  1).
In each setting  we train ﬁve generators with different random seeds as G and combine the generated
data from all generators. We also train ﬁve student networks and report the average and standard
deviation of classiﬁcation accuracy for quantitative evaluation. We initialize the compressed weights
of student networks by running the singular value decomposition on the original weights and update
them by Tucker decomposition to minimize the reconstruction errors [18]. We also use the hidden
variable ˆz of length 10 in all settings  which is much smaller than the data vectors. We use a decoder
network of the same structure in all settings: a multilayer perceptron of n hidden layers with the ELU
activation [5] and batch normalization. n is chosen by the data complexity: n = 1 in MNIST  n = 2
in the unstructured datasets  and n = 3 in Fashion MNIST and SVHN.

5.1 Unstructured Datasets

We use unstructured datasets in the UCI Machine Learning Repository  for which previous works
[6  27] have established reliable standards of performances. We select three datasets which have at
least three classes and ten thousand instances. We divide each dataset into training  validation  and
test sets with the 7:1:2 ratios if the explicit training and test sets are not given. Otherwise  we divide
the given training data into new training and validation sets.
We use a multilayer perceptron (MLP) as a classiﬁer M  which has been used in [27] and contains
ten hidden layers with the ELU activation function and dropout [32] of probability 0.15. We create
student networks by applying Tucker decomposition to all dense layers: the target rank is 5 in Shuttle
and 10 in the others. We use an MLP as a generator G of two hidden layers with the ELU activation

2http://persoal.citius.usc.es/manuel.fernandez.delgado/papers/jmlr/

6

Table 3: Classiﬁcation accuracy of KEGNET and the baselines on the image datasets. We report the
compression ratios of student models along with the accuracy of Tucker. We use three variants of
students for each dataset with different compression ratios.

Dataset Model
LeNet5
MNIST
LeNet5
MNIST
LeNet5
MNIST
MNIST
LeNet5
LeNet5
MNIST

Approach
Original
Tucker (T)
T+Uniform
T+Gaussian
T+KEGNET

SVHN
SVHN
SVHN
SVHN
SVHN

ResNet14 Original
ResNet14 Tucker (T)
ResNet14 T+Uniform
ResNet14 T+Gaussian
ResNet14 T+KEGNET

Fashion ResNet14 Original
Fashion ResNet14 Tucker (T)
Fashion ResNet14 T+Uniform
Fashion ResNet14 T+Gaussian
Fashion ResNet14 T+KEGNET

Student 2
98.90%
67.35% (4.10×)

93.23%
11.02% (1.65×)

Student 3
Student 1
98.90%
98.90%
85.18% (3.62×)
50.01% (4.49×)
95.48 ± 0.11% 88.27 ± 0.07% 69.89 ± 0.28%
95.45 ± 0.15% 87.70 ± 0.12% 71.76 ± 0.18%
96.32 ± 0.05% 90.89 ± 0.11% 89.94 ± 0.08%
93.23%
93.23%
19.31% (1.44×)
11.07% (3.36×)
33.08 ± 1.47% 63.08 ± 1.77% 23.83 ± 1.86%
26.58 ± 1.61% 60.22 ± 4.17% 21.49 ± 2.96%
69.89 ± 1.24% 87.26 ± 0.46% 63.40 ± 1.80%
92.50%
65.09% (1.40×)
< 65.09%
< 65.09%
85.23 ± 1.36% 87.80 ± 0.31% 79.95 ± 1.36%

92.50%
75.80% (1.58×)
< 75.80%
< 75.80%

92.50%
46.55% (2.90×)
< 46.55%
< 46.55%

and batch normalization. We also apply the non-learnable batch normalization after the output layer
to restrict the output space to the standard normal distribution: the parameters γ and β [10] are ﬁxed
as 0 and 1  respectively. This is natural since most neural networks take standardized inputs.
Table 2 compares the classiﬁcation accuracy of student networks trained by KEGNET and the baseline
approaches on the unstructured datasets. All three approaches show large improvements of accuracy
over Tucker  which applies Tucker decomposition without ﬁne-tuning. This implies that even simple
distributions are helpful to improve the performance of student networks when no training data are
observable. Nevertheless  KEGNET shows the highest accuracy in all datasets.

5.2

Image Datasets

We use two well-known classiﬁers on the image datasets: LeNet5 [20] for MNIST and ResNet with
14 layers (referred to as ResNet14) [8] for Fashion MNIST and SVHN. We initialize the student
networks by compressing the weight tensors using Tucker-2 decomposition [13] with VBMF [24];
we compress only the convolutional layers except the dense layers as [13]. Since the classiﬁers are
convolutional neural networks that are optimized for image datasets  we use a generator that is similar
to that of ACGAN [26]  which consists of two fully connected layers followed by three transposed
convolutional layers with the batch normalization after each layer.

5.2.1 Quantitative Analysis

We evaluate KEGNET by training three different student networks for each classiﬁer. For LeNet5  we
compress the last convolutional layer in Student 1 and the last two convolutional layers in Student 2.
We then increase the compression ratio of Student 2 by decreasing the projection rank in Student 3.
For ResNet14  we compress the last residual block which consists of two convolutional layers. We
compress each of the convolutional layers in Students 1 and 2 and the both layers in Student 3.
Table 3 shows the classiﬁcation accuracy of student networks trained by KEGNET and the baseline
approaches. In MNIST where the dataset and classiﬁer are both simple  the Uniform and Gaussian
baselines also achieve high accuracy which is up to 21.8%p higher than that of Tucker. However 
their accuracy gain becomes much lower in SVHN  and the accuracy becomes even lower than that
of Tucker in Fashion MNIST  meaning that ﬁne-tuning after Tucker decomposition is not helpful at
all. This shows that simple random distributions fail with complex datasets whose manifolds are
far from trivial distributions. We do not report their exact accuracy in Fashion because they keep

7

(a) MNIST (z = 0).

(b) SVHN (z = 0).

(c) SVHN (averaged by z).

(d) Latent space walking from 0 to 5 in SVHN.

Figure 2: Artiﬁcial images generated by ﬁve generators of KEGNET for MNIST and SVHN. We ﬁx
the noise variable z to a zero vector in (a) and (b)  while we average multiple images with random z
in (c) and (d). The digits are blurry but recognizable especially when averaged by z.

decreasing as we continue the training. On the other hand  KEGNET outperforms all baselines by
learning successfully the data distributions from the given classiﬁers in all datasets.

5.2.2 Qualitative Analysis

We also analyze qualitatively the extracted data for the image datasets. Figure 2 visualizes artiﬁcial
images for MNIST and SVHN  which were generated by the ﬁve generators in G. The images seem
noisy but contain digits which are clearly recognizable  even though the generators do not have any
information about the true datasets. KEGNET generates more clear images in SVHN than in MNIST 
because the digits in SVHN have more distinct patterns than in the hand-written digits. The digits
are more clear when averaged from multiple hidden variables  implying that images with different
hidden variables are diverse but share a common feature that the classiﬁer is able to capture. We also
visualize images with soft evidence in Figure 2d by changing smoothly the input label from 0 to 5. It
is shown that the generators create digits following the strength of evidence for each class.

6 Conclusion

We propose KEGNET (Knowledge Extraction with Generative Networks)  a novel architecture that
extracts the knowledge of a trained neural network without any observable data. KEGNET learns the
conditional distribution of data points by training the generator and decoder networks  and estimates
the manifold of missing data as a set of artiﬁcial data points. Our experiments show that KEGNET is
able to reconstruct unobservable data that were used to train a deep neural network  especially for
image datasets that have distinct and complex manifolds  and improves the performance of data-free
knowledge distillation. Future works include extending KEGNET to knowledge distillation between
neural networks of different structures  such as LeNet5 and ResNet14  or to more complex datasets
such as CIFAR-10/100 [19] that may require a careful design of new generator networks.

Acknowledgments

This work was supported by the ICT R&D program of MSIT/IITP (No.2017-0-01772  Development
of QA systems for Video Story Understanding to pass the Video Turing Test).

8

0123456789𝐺"𝐺#𝐺$𝐺%𝐺&0123456789𝐺"𝐺#𝐺$𝐺%𝐺&0123456789𝐺"𝐺#𝐺$𝐺%𝐺&05𝐺"𝐺#𝐺$𝐺%𝐺&References
[1] Anoop Korattikara Balan  Vivek Rathod  Kevin P. Murphy  and Max Welling. Bayesian dark

knowledge. In NIPS  2015.

[2] Tianqi Chen  Ian J. Goodfellow  and Jonathon Shlens. Net2net: Accelerating learning via

knowledge transfer. In ICLR  2016.

[3] Jian Cheng  Peisong Wang  Gang Li  Qinghao Hu  and Hanqing Lu. Recent advances in efﬁcient
computation of deep convolutional neural networks. Frontiers of IT & EE  19(1):64–77  2018.
[4] Yu Cheng  Duo Wang  Pan Zhou  and Tao Zhang. A survey of model compression and

acceleration for deep neural networks. arXiv  2017.

[5] Djork-Arné Clevert  Thomas Unterthiner  and Sepp Hochreiter. Fast and accurate deep network

learning by exponential linear units (elus). In ICLR  2016.

[6] Manuel Fernández Delgado  Eva Cernadas  Senén Barro  and Dinani Gomes Amorim. Do we
need hundreds of classiﬁers to solve real world classiﬁcation problems? Journal of Machine
Learning Research  15(1):3133–3181  2014.

[7] Ian J. Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil

Ozair  Aaron C. Courville  and Yoshua Bengio. Generative adversarial nets. In NIPS  2014.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. In CVPR  pages 770–778  2016.

[9] Geoffrey E. Hinton  Oriol Vinyals  and Jeffrey Dean. Distilling the knowledge in a neural

network. arXiv  2015.

[10] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. In ICML  pages 448–456  2015.

[11] Maksym Kholiavchenko. Iterative low-rank approximation for CNN compression. arXiv  2018.

URL http://arxiv.org/abs/1803.08995.

[12] Jangho Kim  Seonguk Park  and Nojun Kwak. Paraphrasing complex network: Network

compression via factor transfer. In NeurIPS  2018.

[13] Yong-Deok Kim  Eunhyeok Park  Sungjoo Yoo  Taelim Choi  Lu Yang  and Dongjun Shin.
Compression of deep convolutional neural networks for fast and low power mobile applications.
ICLR  2016.

[14] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In EMNLP  pages

1317–1327  2016.

[15] Akisato Kimura  Zoubin Ghahramani  Koh Takeuchi  Tomoharu Iwata  and Naonori Ueda.
Few-shot learning of neural networks from scratch by pseudo example optimization. In BMVC 
page 105  2018.

[16] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review 

51(3):455–500  2009.

[17] Jean Kossaiﬁ  Zachary C. Lipton  Aran Khanna  Tommaso Furlanello  and Anima Anandkumar.

Tensor regression networks. arXiv  2017.

[18] Jean Kossaiﬁ  Yannis Panagakis  Anima Anandkumar  and Maja Pantic. Tensorly: Tensor

learning in python. Journal of Machine Learning Research  20:26:1–26:6  2019.

[19] Alex Krizhevsky  Geoffrey Hinton  et al. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[20] Yann LeCun  LD Jackel  Léon Bottou  Corinna Cortes  John S Denker  Harris Drucker  Isabelle
Guyon  Urs A Muller  Eduard Sackinger  Patrice Simard  et al. Learning algorithms for
classiﬁcation: A comparison on handwritten digit recognition. Neural networks: the statistical
mechanics perspective  261:276  1995.

[21] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[22] Tianhong Li  Jianguo Li  Zhuang Liu  and Changshui Zhang. Knowledge distillation from few

samples. arXiv  2018.

9

[23] Raphael Gontijo Lopes  Stefano Fenu  and Thad Starner. Data-free knowledge distillation for

deep neural networks. NIPS Workshop  2017.

[24] Shinichi Nakajima  Masashi Sugiyama  S. Derin Babacan  and Ryota Tomioka. Global ana-
lytic solution of fully-observed variational bayesian matrix factorization. Journal of Machine
Learning Research  14(1):1–37  2013.

[25] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y Ng.

Reading digits in natural images with unsupervised feature learning. 2011.

[26] Augustus Odena  Christopher Olah  and Jonathon Shlens. Conditional image synthesis with

auxiliary classiﬁer gans. In ICML  2017.

[27] Matthew Olson  Abraham J. Wyner  and Richard Berk. Modern neural networks generalize on

small data sets. In NeurIPS  2018.

[28] Nicolas Papernot  Martín Abadi  Úlfar Erlingsson  Ian J. Goodfellow  and Kunal Talwar. Semi-

supervised knowledge transfer for deep learning from private training data. ICLR  2017.

[29] Antonio Polino  Razvan Pascanu  and Dan Alistarh. Model compression via distillation and

quantization. In ICLR  2018.

[30] Stephan Rabanser  Oleksandr Shchur  and Stephan Günnemann. Introduction to tensor decom-

positions and their applications in machine learning. arXiv  2017.

[31] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR  2016.

[32] Nitish Srivastava  Geoffrey E. Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdi-
nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine
Learning Research  15(1):1929–1958  2014.

[33] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms  2017.

10

,Dan Rosenbaum
Yair Weiss
Jaemin Yoo
Minyong Cho
Taebum Kim
U Kang