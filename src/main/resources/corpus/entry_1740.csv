2013,Efficient Optimization for Sparse Gaussian Process Regression,We propose an efficient discrete optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates this inducing set and the hyperparameters using a single objective  either the marginal likelihood or a variational free energy. The space and time complexity are linear in the training set size  and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in the discrete case and competitive results in the continuous case.,Efﬁcient Optimization for

Sparse Gaussian Process Regression

Yanshuai Cao1 Marcus A. Brubaker2 David J. Fleet1 Aaron Hertzmann1 3

1Department of Computer Science

University of Toronto

2TTI-Chicago

3Adobe Research

Abstract

We propose an efﬁcient optimization algorithm for selecting a subset of train-
ing data to induce sparsity for Gaussian process regression. The algorithm esti-
mates an inducing set and the hyperparameters using a single objective  either the
marginal likelihood or a variational free energy. The space and time complexity
are linear in training set size  and the algorithm can be applied to large regression
problems on discrete or continuous domains. Empirical evaluation shows state-of-
art performance in discrete cases and competitive results in the continuous case.

Introduction

1
Gaussian Process (GP) learning and inference are computationally prohibitive with large datasets 
having time complexities O(n3) and O(n2)  where n is the number of training points. Sparsiﬁcation
algorithms exist that scale linearly in the training set size (see [10] for a review). They construct a
low-rank approximation to the GP covariance matrix over the full dataset using a small set of induc-
ing points. Some approaches select inducing points from training points [7  8  12  13]. But these
methods select the inducing points using ad hoc criteria; i.e.  they use different objective functions to
select inducing points and to optimize GP hyperparameters. More powerful sparsiﬁcation methods
[14  15  16] use a single objective function and allow inducing points to move freely over the input
domain which are learned via gradient descent. This continuous relaxation is not feasible  however 
if the input domain is discrete  or if the kernel function is not differentiable in the input variables.
As a result  there are problems in myraid domains  like bio-informatics  linguistics and computer
vision where current sparse GP regression methods are inapplicable or ineffective.
We introduce an efﬁcient sparsiﬁcation algorithm for GP regression. The method optimizes a single
objective for joint selection of inducing points and GP hyperparameters. Notably  it optimizes either
the marginal likelihood  or a variational free energy [15]  exploiting the QR factorization of a par-
tial Cholesky decomposition to efﬁciently approximate the covariance matrix. Because it chooses
inducing points from the training data  it is applicable to problems on discrete or continuous input
domains. To our knowledge  it is the ﬁrst method for selecting discrete inducing points and hy-
perparameters that optimizes a single objective  with linear space and time complexity. It is shown
to outperform other methods on discrete datasets from bio-informatics and computer vision. On
continuous domains it is competitive with the Pseudo-point GP [14] (SPGP).
1.1 Previous Work
Efﬁcient state-of-the-art sparsiﬁcation methods are O(m2n) in time and O(mn) in space for learn-
ing. They compute the predictive mean and variance in time O(m) and O(m2). Methods based on
continuous relaxation  when applicable  entail learning O(md) continuous parameters  where d is
the input dimension. In the discrete case  combinatorial optimization is required to select the induc-
ing points  and this is  in general  intractable. Existing discrete sparsiﬁcation methods therefore use
other criteria to greedily select inducing points [7  8  12  13]. Although their criteria are justiﬁed 
each in their own way (e.g.  [8  12] take an information theoretic perspective)  they are greedy and
do not use the same objective to select inducing points and to estimate GP hyperparameters.

1

The variational formulation of Titsias [15] treats inducing points as variational parameters  and gives
a uniﬁed objective for discrete and continuous inducing point models. In the continuous case  it uses
gradient-based optimization to ﬁnd inducing points and hyperparameters. In the discrete case  our
method optimizes the same variational objective of Titsias [15]  but is a signiﬁcant improvement over
greedy forward selection using the variational objective as selection criteria  or some other criteria.
In particular  given the cost of evaluating the variational objective on all training points  Titsias [15]
evaluates the objective function on a small random subset of candidates at each iteration  and then
select the best element from the subset. This approximation is often slow to achieve good results 
as we explain and demonstrate below in Section 4.1. The approach in [15] also uses greedy forward
selection  which provides no way to reﬁne the inducing set after hyperparameter optimization  except
to discard all previous inducing points and restart selection. Hence  the objective is not guaranteed
to decrease after each restart. By comparison  our formulation considers all candidates at each step 
and revisiting previous selections is efﬁcient  and guaranteed to decrease the objective or terminate.
Our low-rank decomposition is inspired by the Cholesky with Side Information (CSI) algorithm for
kernel machines [1]. We extend that approach to GP regression. First  we alter the form of the low-
rank matrix factorization in CSI to be suitable for GP regression with full-rank diagonal term in the
covariance. Second  the CSI algorithm selects inducing points in a single greedy pass using an ap-
proximate objective. We propose an iterative optimization algorithm that swaps previously selected
points with new candidates that are guaranteed to lower the objective. Finally  we perform induc-
ing set selection jointly with gradient-based hyperparameter estimation instead of the grid search in
CSI. Our algorithm selects inducing points in a principled fashion  optimizing the variational free
energy or the log likelihood. It does so with time complexity O(m2n)  and in practice provides an
improved quality-speed trade-off over other discrete selection methods.
2 Sparse GP Regression
Let y ∈ R be the noisy output of a function  f  of input x. Let X = {xi}n
i=1 denote n training
inputs  each belonging to input space D  which is not necessarily Euclidean. Let y ∈ Rn denote the
corresponding vector of training outputs. Under a full zero-mean GP  with the covariance function
(1)
where κ is the kernel function  1[·] is the usual indicator function  and σ2 is the variance of the obser-
vation noise  the predictive distribution over the output f(cid:63) at a test point x(cid:63) is normally distributed.
The mean and variance of the predictive distribution can be expressed as

E[yiyj] = κ(xi  xj) + σ21[i = j]  

µ(cid:63) = κ(x(cid:63))T(cid:0)K + σ2In
(cid:63) = κ(x(cid:63)  x(cid:63)) − κ(x(cid:63))T(cid:0)K + σ2In

(cid:1)−1

v2

y

(cid:1)−1

κ(x(cid:63))

Efull (θ) = ( y(cid:62)(cid:0)K +σ2In

(cid:1)−1

where In is the n × n identity matrix  K is the kernel matrix whose ijth element is κ(xi  xj)  and
κ(x(cid:63)) is the column vector whose ith element is κ(x(cid:63)  xi).
The hyperparameters of a GP  denoted θ  comprise the parameters of the kernel function  and the
noise variance σ2. The natural objective for learning θ is the negative marginal log likelihood
(NMLL) of the training data  − log (P (y|X  θ))  given up to a constant by

y + log |K +σ2In| ) / 2 .

(2)
The computational bottleneck lies in the O(n2) storage and O(n3) inversion of the full covariance
matrix  K + σ2In. To lower this cost with a sparse approximation  Csat´o and Opper [5] and Seeger
et al. [12] proposed the Projected Process (PP) model  wherein a set of m inducing points are used
to construct a low-rank approximation of the kernel matrix. In the discrete case  where the inducing
points are a subset of the training data  with indices I ⊂ {1  2  ...  n}  this approach amounts to
replacing the kernel matrix K with the following Nystr¨om approximation [11]:

K (cid:39) ˆK = K[: I] K[I I]−1 K[I  :]

(3)
where K[: I] denotes the sub-matrix of K comprising columns indexed by I  and K[I I] is the
sub-matrix of K comprising rows and columns indexed by I. We assume the rank of K is m or
higher so we can always ﬁnd such rank-m approximations. The PP NMLL is then algebraically
equivalent to replacing K with ˆK in Eq. (2)  i.e. 

E(θ I) = (cid:0) ED(θ I) + EC(θ I)(cid:1) /2  

(4)

2

with data term ED(θ I) = y(cid:62)(ˆK +σ2In)−1y  and model complexity EC(θ I) = log | ˆK +σ2In|.
The computational cost reduction from O(n3) to O(m2n) associated with the new likelihood is
achieved by applying the Woodbury inversion identity to ED(θ I) and EC(θ I). The objective
in (4) can be viewed as an approximate log likelihood for the full GP model  or as the exact log
likelihood for an approximate model  called the Deterministically Trained Conditional [10].
The same PP model can also be obtained by a variational argument  as in [15]  for which the varia-
tional free energy objective can be shown to be Eq. (4) plus one extra term; i.e. 

F (θ I) = (cid:0) ED(θ I) + EC(θ I) + EV(θ I)(cid:1) / 2  

(5)
where EV (θ I) = σ−2 tr(K− ˆK) arises from the variational formulation. It effectively regularizes
the trace norm of the approximation residual of the covariance matrix. The kernel machine of [1]
also uses a regularizer of the form λ tr(K− ˆK)  however λ is a free parameter that is set manually.

3 Efﬁcient optimization
We now outline our algorithm for optimizing the variational free energy (5) to select the inducing set
I and the hyperparameters θ. (The negative log-likelihood (4) is similarly minimized by simply dis-
carding the EV term.) The algorithm is a form of hybrid coordinate descent that alternates between
discrete optimization of inducing points  and continuous optimization of the hyperparameters. We
ﬁrst describe the algorithm to select inducing points  and then discuss continuous hyperparameter
optimization and termination criteria in Sec. 3.4.
Finding the optimal inducing set is a combinatorial problem; global optimization is intractable.
Instead  the inducing set is initialized to a random subset of the training data  which is then reﬁned
by a ﬁxed number of swap updates at each iteration.1 In a single swap update  a randomly chosen
inducing point is considered for replacement. If swapping does not improve the objective  then the
original point is retained. There are n − m potential replacements for each each swap update; the
key is to efﬁciently determine which will maximally improve the objective. With the techniques
described below  the computation time required to approximately evaluate all possible candidates
and swap an inducing point is O(mn). Swapping all inducing points once takes O(m2n) time.

3.1 Factored representation
To support efﬁcient evaluation of the objective and swapping  we use a factored representation of the
kernel matrix. Given an inducing set I of k points  for any k ≤ m  the low-rank Nystr¨om approx-
imation to the kernel matrix (Eq. 3) can be expressed in terms of a partial Cholesky factorization:

ˆK = K[: I] K[I I]−1 K[I  :] = L(I)L(I)(cid:62)  

(6)
where L(I) ∈ Rn×k is  up to permutation of rows  lower trapezoidal matrix (i.e.  has a k × k
top lower triangular block  again up to row permutation). The derivation of Eq. 6 follows from
Proposition 1 in [1]  and the fact that  given the ordered sequence of pivots I  the partial Cholesky
factorization is unique.
Using this factorization and the Woodbury identities (dropping the dependence on θ and I for clar-
ity)  the terms of the negative marginal log-likelihood (4) and variational free energy (5) become

ED = σ−2(cid:16)
EC = log(cid:0)(σ2)n−k|L(cid:62)L + σ2I|(cid:1)

y(cid:62)y − y(cid:62)L(cid:0)L(cid:62)L + σ2I(cid:1)−1

(cid:17)

L(cid:62)y

(7)

(8)
(9)

EV = σ−2(tr(K) − tr(L(cid:62)L))

We can further simplify the data term by augmenting the factor matrix as(cid:101)L = [L(cid:62)  σIk](cid:62)  where
Ik is the k×k identity matrix  and(cid:101)y = [yT  0T
ED = σ−2(cid:16)

y(cid:62)y −(cid:101)y(cid:62)(cid:101)L ((cid:101)L(cid:62)(cid:101)L)−1(cid:101)L(cid:62)(cid:101)y

T is the y vector with k zeroes appended:

(cid:17)

(10)

k ]

1The inducing set can be incrementally constructed  as in [1]  however we found no beneﬁt to this.

3

Now  let (cid:101)L = QR be a QR factorization of (cid:101)L  where Q ∈ R(n+k)×k has orthogonal columns and

R ∈ Rk×k is invertible. The ﬁrst two terms in the objective simplify further to

ED = σ−2(cid:0)(cid:107)y(cid:107)2 − (cid:107)Q(cid:62)(cid:101)y(cid:107)2(cid:1)

EC = (n − k) log(σ2) + 2 log |R| .

(11)
(12)

3.2 Factorization update
Here we present the mechanics of the swap update algorithm  see [3] for pseudo-code. Suppose we
wish to swap inducing point i with candidate point j in Im  the inducing set of size m. We ﬁrst
modify the factor matrices in order to remove point i from Im  i.e. to downdate the factors. Then
we update all the key terms using one step of Cholesky and QR factorization with the new point j.
Downdating to remove inducing point i requires that we shift the corresponding columns/rows in

the factorization to the right-most columns of(cid:101)L  Q  R and to the last row of R. We can then simply

and removing i take O(mn) time  as does the updating with point j.

and update the factors to rank m  one step of Cholesky factorization is performed with point j  for

discard these last columns and rows  and modify related quantities. When permuting the order of the
inducing points  the underlying GP model is invariant  but the matrices in the factored representation
are not. If needed  any two points in Im  can be permuted  and the Cholesky or QR factors can be
updated in time O(mn). This is done with the efﬁcient pivot permutation presented in the Appendix

of [1]  with minor modiﬁcations to account for the augmented form of (cid:101)L. In this way  downdating
After downdating  we have factors(cid:101)Lm−1 Qm−1  Rm−1  and inducing set Im−1. To add j to Im−1 
which  ideally  the new column to append to(cid:101)L is formed as
(cid:46)(cid:113)
T. Then  we set (cid:101)Lm = [(cid:101)Lm−1
˜(cid:96)m]  where ˜(cid:96)m is just (cid:96)m augmented
where ˆKm−1 = Lm−1Lm−1
with σem = [0  0  ...  σ  ...  0  0](cid:62). The ﬁnal updates are Qm = [Qm−1 qm]  where qm is given
Rm is updated from Rm−1 so that(cid:101)Lm = QmRm.
by Gram-Schmidt orthogonalization  qm = ((I − Qm−1Q(cid:62)
m−1)˜(cid:96)m(cid:107)  and

m−1)˜(cid:96)m) /(cid:107)(I − Qm−1Q(cid:62)

(cid:96)m = (K− ˆKm−1)[:  j]

(K− ˆKm−1)[j  j]

(13)

3.3 Evaluating candidates
Next we show how to select candidates for inclusion in the inducing set. We ﬁrst derive the exact
change in the objective due to adding an element to Im−1. Later we will provide an approximation
to this objective change that can be computed efﬁciently.

Given an inducing set Im−1  and matrices(cid:101)Lm−1  Qm−1  and Rm−1  we wish to evaluate the change

in Eq. 5 for Im =Im−1 ∪ j. That is  ∆F ≡ F (θ Im−1)−F (θ Im) = (∆ED + ∆EC + ∆EV )/2 
where  based on the mechanics of the incremental updates above  one can show that

∆ED = σ−2((cid:101)y(cid:62)(cid:0)I − Qm−1Q(cid:62)
∆EC = log(cid:0)σ2(cid:1) − log (cid:107)(I − Qm−1Q(cid:62)

m−1

(cid:1) ˜(cid:96)m)2(cid:46) (cid:107)(cid:0)I − Qm−1Q(cid:62)

m−1

(cid:1) ˜(cid:96)m(cid:107)2

(14)

m−1)˜(cid:96)m(cid:107)2

∆EV = σ−2(cid:107)(cid:96)m(cid:107)2

(15)
(16)
This gives the exact decrease in the objective function after adding point j. For a single point this
evaluation is O(mn)  so to evaluate all n − m points would be O(mn2).
3.3.1 Fast approximate cost reduction
While O(mn2) is prohibitive  computing the exact change is not required. Rather  we only need a
ranking of the best few candidates. Thus  instead of evaluating the change in the objective exactly 
we use an efﬁcient approximation based on a small number  z  of training points which provide
information about the residual between the current low-rank covariance matrix (based on inducing
points) and the full covariance matrix. After this approximation proposes a candidate  we use the
actual objective to decide whether to include it. The techniques below reduce the complexity of
evaluating all n − m candidates to O(zn).
To compute the change in objective for one candidate  we need the new column of the updated
Cholesky factorization  (cid:96)m.
In Eq. (13) this vector is a (normalized) column of the residual

4

z )[:  j]

(cid:46)(cid:112)(LzL(cid:62)

K − ˆKm−1 between the full kernel matrix and the Nystr¨om approximation. Now consider the
full Cholesky decomposition of K = L∗L∗(cid:62) where L∗ = [Lm−1  L(Jm−1)] is constructed with
Im−1 as the ﬁrst pivots and Jm−1 = {1  ...  n}\Im−1 as the remaining pivots  so the resid-
ual becomes K − ˆKm−1 = L(Jm−1)L(Jm−1)(cid:62). We approximate L(Jm−1) by a rank z (cid:28) n
matrix  Lz  by taking z points from Jm−1 and performing a partial Cholesky factorization of
K − ˆKm−1 using these pivots. The residual approximation becomes K − ˆKm−1 ≈ LzL(cid:62)
z   and
thus (cid:96)m ≈ (LzL(cid:62)
z )[j  j]. The pivots used to construct Lz are called information
pivots; their selection is discussed in Sec. 3.3.2.
k and ∆EV

k   Eqs. (14)-(16)  for all candidate points  involve
The approximations to ∆ED
the following terms: diag(LzL(cid:62)
z . The ﬁrst term
can be computed in time O(z2n)  and the other two in O(zmn) with careful ordering of matrix
multiplications.2 Computing Lz costs O(z2n)  but can be avoided since information pivots change
by at most one when an information pivots is added to the inducing set and needs to be replaced.
The techniques in Sec. 3.2 bring the associated update cost to O(zn) by updating Lz rather than
recomputing it. These z information pivots are equivalent to the “look-ahead” steps of Bach and
Jordan’s CSI algorithm  but as described in Sec. 3.3.2  there is a more effective way to select them.

k   ∆EC

z LzL(cid:62)

z )  y(cid:62)LzL(cid:62)

(cid:62)
z   and (Qk−1[1 : n  :])

LzL(cid:62)

3.3.2 Ensuring a good approximation
Selection of the information pivots determines the approximate objective  and hence the candidate
proposal. To ensure a good approximation  the CSI algorithm [1] greedily selects points to ﬁnd
an approximation of the residual K − ˆKm−1 in Eq. (13) that is optimal in terms of a bound of
the trace norm. The goal  however  is to approximate Eqs. (14)-(16) . By analyzing the role of
the residual matrix  we see that the information pivots provide a low-rank approximation to the
orthogonal complement of the space spanned by current inducing set. With a ﬁxed set of information
pivots  parts of that subspace may never be captured. This suggests that we might occasionally
update the entire set of information pivots. Although information pivots are changed when one is
moved into the inducing set  we ﬁnd empirically that this is not insufﬁcient. Instead  at regular
intervals we replace the entire set of information pivots by random selection. We ﬁnd this works
better than optimizing the information pivots as in [1].

Figure 1 compares the exact and approximate
cost reduction for candidate inducing points
(left)  and their respective rankings (right). The
approximation is shown to work well. It is also
robust to changes in the number of information
pivots and the frequency of updates. When bad
candidates are proposed  they are rejected after
evaluating the change in the true objective. We
ﬁnd that rejection rates are typically low during
early iterations (< 20%)  but increase as opti-
mization nears convergence (to 30% or 40%). Rejection rates also increase for sparser models 
where each inducing point plays a more critical role and is harder to replace.

Figure 1: Exact vs approximate costs  based on
the 1D example of Sec. 4  with z = 10  n = 200.

3.4 Hybrid optimization
The overall hybrid optimization procedure performs block coordinate descent in the inducing points
and the continuous hyperparameters.
It alternates between discrete and continuous phases until
improvement in the objective is below a threshold or the computational time budget is exhausted.

In the discrete phase  inducing points are considered for swapping with the hyper-parameters ﬁxed.
With the factorization and efﬁcient candidate evaluation above  swapping an inducing point i ∈ Im
proceeds as follows: (I) down-date the factorization matrices as in Sec. 3.2 to remove i; (II) compute
the true objective function value Fm−1 over the down-dated model with Im\{i}  using (11)  (12)
and (9); (III) select a replacement candidate using the fast approximate cost change from Sec. 3.3.1;
(IV) evaluate the exact objective change  using (14)  (15)  and (16); (V) add the exact change to the
true objective Fm−1 to get the objective value with the new candidate. If this improves  we include

2Both can be further reduced to O(zn) by appropriate caching during the updates of Q R and(cid:101)L  and Lz

5

00.0050.010.0150.020.0250.030.03500.0050.010.0150.020.0250.030.035exact total reductionapprox total reduction050100150050100150ranking exact total reductionranking approx total reductionFigure 2: Test performance on discrete datasets. (top row) BindingDB  values at each marker is the
average of 150 runs (50-fold random train/test splits times 3 random initialization); (bottom row)
HoG dataset  each marker is the average of 10 randomly initialized runs.
the candidate in I and update the matrices as in Sec. 3.2. Otherwise it is rejected and we revert to
the factorization with i; (VI) if needed  update the information pivots as in Secs. 3.3.1 and 3.3.2.
After each discrete optimization step we ﬁx the inducing set I and optimize the hyperparameters
using non-linear conjugate gradients (CG). The equivalence in (6) allows us to compute the gradient
with respect to the hyperparameters analytically using the Nystr¨om form. In practice  because we
alternate each phase for many training epochs  attempting to swap every inducing point in each
epoch is unnecessary  just as there is no need to run hyperparameter optimization until convergence.
As long as all inducing set points are eventually considered we ﬁnd that optimized models can
achieve similar performance with shorter learning times.

N ΣN

4 Experiments and analysis
For the experiments that follow we jointly learn inducing points and hyperparameters  a more chal-
lenging task than learning inducing points with known hyperparameters [12  14]. For all but the 1D
example  the number of inducing points swapped per epoch is min(60  m). The maximum num-
ber of function evaluations per epoch in CG hyperparameter optimization is min(20  max(15  2d)) 
where d is the number of continuous hyperparameters. Empirically we ﬁnd the algorithm is robust
to changes in these limits. We use two performance measures  (a) standardized mean square er-
t=1(ˆyt − yt)2/ˆσ2∗  where ˆσ2∗ is the sample variance of test outputs {yt}  and (2)
ror (SMSE)  1
standardized negative log probability (SNLP) deﬁned in [11].
4.1 Discrete input domain
We ﬁrst show results on two discrete datasets with kernels that are not differentiable in the input
variable x. Because continuous relaxation methods are not applicable  we compare to discrete se-
lection methods  namely  random selection as baseline (Random)  greedy subset-optimal selection
of Titsias [15] with either 16 or 512 candidates (Titsias-16 and Titsias-512)  and Informative Vec-
tor Machine [8] (IVM). For learning continuous hyperparameters  each method optimizes the same
objective using non-linear CG. Care is taken to ensure consist initialization and termination criteria
[3]. For our algorithm we use z = 16 information pivots with random selection (CholQR-z16).
Later  we show how variants of our algorithm trade-off speed and performance. Additionally  we
also compare to least-square kernel regression using CSI (in Fig. 3(c)).
The ﬁrst discrete dataset  from bindingdb.org  concerns the prediction of binding afﬁnity for a
target (Thrombin)  from the 2D chemical structure of small molecules (represented as graphs). We
do 50-fold random splits to 3660 training points and 192 test points for repeated runs. We use a
compound kernel  comprising 14 different graph kernels  and 15 continuous hyperparameters (one

6

163264128256512−0.7−0.6−0.5−0.4−0.3−0.2number of inducing points (m)Testing SNLP  CholQR−z16IVMRandomTitsias−16Titsias−5121632641282565120.30.40.50.60.7number of inducing points (m)Testing SMSE163264128256512−1.2−1−0.8−0.6−0.4number of inducing points (m)Testing SNLP  CholQR−z16IVMRandomTitsias−16Titsias−5121632641282565120.10.150.20.250.30.350.4number of inducing points (m)Testing SMSE(a)

(d)

(b)

(e)

(c)

(f)

Figure 3: Training time versus test performance on discrete datasets. (a) the average BindingDB
training time; (b) the average BindingDB objective function value at convergence; (d) and (e) show
test scores versus training time with m = 32 for a single run; (c) shows the trade-off between training
time and testing SMSE on the HoG dataset with m = 32  for various methods including multiple
variants of CholQR and CSI; (f) a zoomed-in version of (c) comparing the variants of CholQR.

noise variance and 14 data variances). In the second task  from [2]  the task is to predict 3D human
joint position from histograms of HoG image features [6]. Training and test sets have 4819 and
4811 data points. Because our goal is the general purpose sparsiﬁcation method for GP regression 
we make no attempt at the more difﬁcult problem of modelling the multivariate output structure in
the regression as in [2]. Instead  we predict the vertical position of joints independently  using a
histogram intersection kernel [9]  having four hyperparameters: one noise variance  and three data
variances corresponding to the kernel evaluated over the HoG from each of three cameras. We select
and show result on the representative left wrist here (see [3] for others joints  and more details about
the datasets and kernels used).
The results in Fig. 2 and 3 show that CholQR-z16 outperforms the baseline methods in terms of
test-time predictive power with signiﬁcantly lower training time. Titsias-16 and Titsias-512 shows
similar test performance  but they are two to four orders of magnitude slower than CholQR-z16 (see
Figs. 3(d) and 3(e)). Indeed  Fig. 3(a) shows that the training time for CholQR-z16 is comparable to
IVM and Random selection  but with much better performance. The poor performance of Random
selection highlights the importance of selecting good inducing points  as no amount of hyperparam-
eter optimization can correct for poor inducing points. Fig. 3(a) also shows IVM to be somewhat
slower due to the increased number of iterations needed  even though per epoch  IVM is faster than
CholQR. When stopped earlier  IVM test performance further degrades.
Finally  Fig. 3(c) and 3(f) show the trade-off between the test SMSE and training time for variants of
CholQR  with baselines and CSI kernel regression [1]. For CholQR we consider different numbers
of information pivots (denoted z8  z16  z64 and z128)  and different strategies for their selection in-
cluding random selection  optimization as in [1] (denote OI) and adaptively growing the information
pivot set (denoted AA  see [3] for details). These variants of CholQR trade-off speed and perfor-
mance (3(f))  all signiﬁcantly outperform the other methods (3(c)); CSI  which uses grid search to
select hyper-parameters  is slow and exhibits higher SMSE.
4.2 Continuous input domain
Although CholQR was developed for discrete input domains  it can be competitive on continuous
domains. To that end  we compare to SPGP [14] and IVM [8]  using RBF kernels with one length-
j )2). We show

scale parameter per input dimension; κ(xi  xj) = c exp(−0.5(cid:80)d

results from both the PP log likelihood and variational objectives  sufﬁxed by MLE and VAR.

t=1 bt(x(t)

i − x(t)

7

163264128256512102103104number of inducing points (m)Total training time (secs)16326412825651205001000number of inducing points (m)Training VAR  CholQR−z16IVMRandomTitsias−16Titsias−5121011021031040.10.20.3Testing SMSETime in secs (log scaled)  CholQR−z8CholQR−z16CholQR−OI−z16CholQR−z64CholQR−OI−z64CholQR−AA−z128IVMRandomTitsias−16Titsias−512CSI100101102103104−0.3−0.2−0.10Cumulative training time in secs (log scale)Testing SNLP  CholQR−z16IVMRandomTitsias−16Titsias−5121001011021031040.550.60.650.70.75Cumulative training time in secs (log scale)Testing SMSE  CholQR−z16IVMRandomTitsias−16Titsias−5121011020.1380.140.1420.144Testing SMSETime in secs (log scaled)(a) CholQR-MLE (b) CholQR-MLE

(c) SPGP

(d) CholQR-VAR

(e) CholQR-VAR

(f) SPGP

Figure 4: Snelson’s 1D example: prediction mean (red curves); one standard deviation in prediction
uncertainty (green curves); inducing point initialization (black points at top of each ﬁgure); learned
inducing point locations (the cyan points at the bottom  also overlaid on data for CholQR).

Figure 5: Test scores on KIN40K as function of number of inducing points: for each number of
inducing points the value plotted is averaged over 10 runs from 10 different (shared) initializations.

We use the 1D toy dataset of [14] to show how the PP likelihood with gradient-based optimization
of inducing points is easily trapped in local minima. Fig. 4(a) and 4(d) show that for this dataset
our algorithm does not get trapped when initialization is poor (as in Fig. 1c of [14]). To simulate
the sparsity of data in high-dimensional problems we also down-sample the dataset to 20 points
(every 10th point). Here CholQR out-performs SPGP (see Fig. 4(b)  4(e)  and 4(c)). By comparison 
Fig. 4(f) shows SPGP learned with a more uniform initial distribution of inducing points avoids this
local optima and achieves a better negative log likelihood of 11.34 compared to 14.54 in Fig. 4(c).
Finally  we compare CholQR to SPGP [14] and IVM [8] on a large dataset. KIN40K concerns
nonlinear forward kinematic prediction. It has 8D real-valued inputs and scalar outputs  with 10K
training and 30K test points. We perform linear de-trending and re-scaling as pre-processing. For
SPGP we use the implementation of [14]. Fig. 5 shows that CholQR-VAR outperforms IVM in terms
of SMSE and SNLP. Both CholQR-VAR and CholQR-MLE outperform SPGP in terms of SMSE on
KIN40K with large m  but SPGP exhibits better SNLP. This disparity between the SMSE and SNLP
measures for CholQR-MLE is consistent with ﬁndings about the PP likelihood in [15]. Recently 
Chalupka et al. [4] introduced an empirical evaluation framework for approximate GP methods 
and showed that subset of data (SoD) often compares favorably to more sophisticated sparse GP
methods. Our preliminary experiments using this framework suggest that CholQR outperforms
SPGP in speed and predictive scores; and compared to SoD  CholQR is slower during training  but
proportionally faster during testing since CholQR ﬁnds a much sparser model to achieve the same
predictive scores. In future work  we will report results on the complete suit of benchmark tests.

5 Conclusion
We describe an algorithm for selecting inducing points for Gaussian Process sparsiﬁcation. It op-
timizes principled objective functions  and is applicable to discrete domains and non-differentiable
kernels. On such problems it is shown to be as good as or better than competing methods and  for
methods whose predictive behavior is similar  our method is several orders of magnitude faster. On
continuous domains the method is competitive. Extension to the SPGP form of covariance approxi-
mation would be interesting future research.

8

128256512102420480.050.10.150.20.25testing SMSE  CholQR−MLECholQR−VARSPGPIVM−MLEIVM−VAR12825651210242048−2.5−2−1.5−1−0.5testing SNLPReferences
[1] F. R. Bach and M. I. Jordan. Predictive low-rank decomposition for kernel methods. ICML 

pp. 33–40  2005..

[2] L. Bo and C. Sminchisescu. Twin gaussian processes for structured prediction. IJCV  87:28–

52  2010.

[3] Y. Cao  M. A. Brubaker  D. J. Fleet  and A. Hertzmann.

supplemen-
tary material and software for efﬁcient optimization for sparse gaussian process regression.
www.cs.toronto.edu/˜caoy/opt_sgpr  2013.

Project page:

[4] K. Chalupka  C. K. I. Williams  and I. Murray. A framework for evaluating approximation

methods for gaussian process regression. JMLR  14(1):333–350  February 2013.

[5] L. Csat´o and M. Opper. Sparse on-line gaussian processes. Neural Comput.  14:641–668 

2002.

[6] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. IEEE CVPR 

pp. 886–893  2005.

[7] S. S. Keerthi and W. Chu. A matching pursuit approach to sparse gaussian process regression.

NIPS 18  pp. 643–650. 2006.

[8] N. D. Lawrence  M. Seeger  and R. Herbrich  Fast sparse gaussian process methods: The

informative vector machine. NIPS 15  pp. 609–616. 2003.

[9] J. J. Lee. Libpmk: A pyramid match toolkit. TR: MIT-CSAIL-TR-2008-17  MIT CSAIL 

2008. URL http://hdl.handle.net/1721.1/41070.

[10] J. Qui˜nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate gaussian

process regression. JMLR  6:1939–1959  2005.

[11] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. Adaptive

computation and machine learning. MIT Press  2006.

[12] M. Seeger  C. K. I. Williams  and N. D. Lawrence. Fast forward selection to speed up sparse

gaussian process regression. AI & Stats. 9  2003.

[13] A. J. Smola and P. Bartlett. Sparse greedy gaussian process regression. In Advances in Neural

Information Processing Systems 13  pp. 619–625. 2001.

[14] E. Snelson and Z. Ghahramani. Sparse gaussian processes using pseudo-inputs. NIPS 18  pp.

1257–1264. 2006.

[15] M. K. Titsias. Variational learning of inducing variables in sparse gaussian processes. JMLR 

5:567–574  2009.

[16] C. Walder  K. I. Kwang  and B. Sch¨olkopf. Sparse multiscale gaussian process regression.

ICML  pp. 1112–1119  2008.

9

,Yanshuai Cao
Marcus Brubaker
David Fleet
Aaron Hertzmann
Eran Treister
Javier Turek
Xinan Wang
Sanjoy Dasgupta
Bo Dai
Dahua Lin
Tao Sun
Yuejiao Sun
Dongsheng Li
Qing Liao