2015,On some provably correct cases of variational inference for topic models,Variational inference is an efficient  popular heuristic used in the context of latent variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum  in the setting of topic models. Our initializations are natural  one of them being used in LDA-c  the mostpopular implementation of variational inference.In addition to providing intuition into why this heuristic might work in practice  the multiplicative  rather than additive nature of the variational inference updates forces us to usenon-standard proof arguments  which we believe might be of general theoretical interest.,On some provably correct cases of variational

inference for topic models

Pranjal Awasthi

Department of Computer Science

Rutgers University

New Brunswick  NJ 08901

pranjal.awasthi@rutgers.edu

risteski@cs.princeton.edu

Andrej Risteski

Department of Computer Science

Princeton University
Princeton  NJ 08540

Abstract

Variational inference is an efﬁcient  popular heuristic used in the context of latent
variable models. We provide the ﬁrst analysis of instances where variational in-
ference algorithms converge to the global optimum  in the setting of topic models.
Our initializations are natural  one of them being used in LDA-c  the most popular
implementation of variational inference. In addition to providing intuition into
why this heuristic might work in practice  the multiplicative  rather than additive
nature of the variational inference updates forces us to use non-standard proof
arguments  which we believe might be of general theoretical interest.

1

Introduction

Over the last few years  heuristics for non-convex optimization have emerged as one of the most
fascinating phenomena for theoretical study in machine learning. Methods like alternating mini-
mization  EM  variational inference and the like enjoy immense popularity among ML practitioners 
and with good reason: they’re vastly more efﬁcient than alternate available methods like convex
relaxations  and are usually easily modiﬁed to suite different applications.
Theoretical understanding however is sparse and we know of very few instances where these meth-
ods come with formal guarantees. Among more classical results in this direction are the analyses of
Lloyd’s algorithm for K-means  which is very closely related to the EM algorithm for mixtures of
Gaussians [20]  [13]  [14]. The recent work of [9] also characterizes global convergence properties
of the EM algorithm for more general settings. Another line of recent work has focused on a differ-
ent heuristic called alternating minimization in the context of dictionary learning. [1]  [6] prove that
with appropriate initialization  alternating minimization can provably recover the ground truth. [22]
have proven similar results in the context of phase retreival.
Another popular heuristic which has so far eluded such attempts is known as variational infer-
ence [19]. We provide the ﬁrst characterization of global convergence of variational inference based
algorithms for topic models [12]. We show that under natural assumptions on the topic-word matrix
and the topic priors  along with natural initialization  variational inference converges to the param-
eters of the underlying ground truth model. To prove our result we need to overcome a number
of technical hurdles which are unique to the nature of variational inference. Firstly  the difﬁculty
in analyzing alternating minimization methods for dictionary learning is alleviated by the fact that
one can come up with closed form expressions for the updates of the dictionary matrix. We do
not have this luxury. Second  the “norm” in which variational inference naturally operates is KL
divergence  which can be difﬁcult to work with. We stress that the focus of this work is not to iden-
tify new instances of topic modeling that were previously not known to be efﬁciently solvable  but
rather providing understanding about the behaviour of variational inference  the defacto method for
learning and inference in the context of topic models.

1

2 Latent variable models and EM

(cid:88)

i

In the E-step  we compute the distribution ˜P t(Z) = P (Z|X  θt).

We brieﬂy review EM and variational methods. The setting is latent variable models  where
the observations Xi are generated according to a distribution P (Xi|θ) = P (Zi|θ)P (Xi|Zi  θ)
where θ are parameters of the models  and Zi is a latent variable. Given the observations Xi  a
log(P (Xi|θ)).
common task is to ﬁnd the max likelihood value of the parameter θ: argmaxθ
The EM algorithm is an iterative method to achieve this  dating all the way back to [15] and
[24] in the 70s. In the above framework it can be formulated as the following procedure  main-
taining estimates θt  ˜P t(Z) of the model parameters and the posterior distribution over the hid-
den variables:
In the M-
E ˜P t[log P (Xi  Zi|θ)]. Sometimes even the above two steps
step  we set θt+1 = argmaxθ
may not be computationally feasible  in which case they can be relaxed by choosing a fam-
In the variational E-step 
ily of simple distributions F   and performing the following updates.
we compute the distribution ˜P t(Z) = min
In the M-step  we set
P t∈F
E ˜P t[log P (Xi  Zi|θ)]. By picking the family F appropriately  it’s often possi-
θt+1 = argmaxθ
ble to make both steps above run in polynomial time. As expected  none of the above two families
of approximations  come with any provable global convergence guarantees. With EM  the problem
is ensuring that one does not get stuck in a local optimum. With variational EM  additionally  we
are faced with the issue of in principle not even exploring the entire space of solutions.

KL(P t(Z)||P (Z|X  θt)).

(cid:88)

i

(cid:88)

i

3 Topic models and prior work

We focus on a particular  popular latent variable model - topic models [12]. The generative model
over word documents is the following. For each document in the corpus  a proportion of topics
γ1  γ2  . . .   γk is sampled according to a prior distribution α. Then  for each position p in the doc-
ument  we pick a topic Zp according to a multinomial with parameters γ1  . . .   γk. Conditioned on
Zp = i  we pick a word j from a multinomial with parameters (βi 1  βi 2  . . .   βi k) to put in position
p. The matrix of values {βi j} is known as the topic-word matrix.
The body of work on topic models is vast [11]. Prior theoretical work relevant in the context of
this paper includes the sequence of works by [7] [4]  as well as [2]  [16]  [17] and [10]. [7] and
[4] assume that the topic-word matrix contains “anchor words”. This means that each topic has a
word which appears in that topic  and no other. [2] on the other hand work with a certain expansion
assumption on the word-topic graph  which says that if one takes a subset S of topics  the number
of words in the support of these topics should be at least |S| + smax  where smax is the maximum
support size of any topic. Neither paper needs any assumption on the topic priors  and can handle
(almost) arbitrarily short documents.
The assumptions we make on the word-topic matrix will be related to the ones in the above works 
but our documents will need to be long  so that the empirical counts of the words are close to their
expected counts. Our priors will also be more structured. This is expected since we are trying to
analyze an existing heuristic rather than develop a new algorithmic strategy. The case where the
documents are short seems signiﬁcantly more difﬁcult. Namely  in that case there are two issues to
consider. One is proving the variational approximation to the posterior distribution over topics is not
too bad. The second is proving that the updates do actually reach the global optimum. Assuming
long documents allows us to focus on the second issue alone  which is already challenging. On a
high level  the instances we consider will have the following structure:
• The topics will satisfy a weighted expansion property: for any set S of topics of constant size 
for any topic i in this set  the probability mass on words which belong to i  and no other topic in
S will be large. (Similar to the expansion in [2]  but only over constant sized subsets.)
• The number of topics per document will be small. Further  the probability of including a given
topic in a document is almost independent of any other topics that might be included in the
document already. Similar properties are satisﬁed by the Dirichlet prior  one of the most popular

2

priors in topic modeling.
“dominating topic”  similarly as in [10].

(Originally introduced by [12].) The documents will also have a
• For each word j  and a topic i it appears in  there will be a decent proportion of documents that
contain topic i and no other topic containing j. These can be viewed as “local anchor documents”
for that word-pair topic.

We state below  informally  our main result. See Sections 6 and 7 for more details.
Theorem. Under the above mentioned assumptions  popular variants of variational inference for
topic models  with suitable initializations  provably recover the ground truth model in polynomial
time.

4 Variational relaxation for learning topic models

In this section we brieﬂy review the variational relaxation for topic models  following closely [12].
Throughout the paper  we will denote by N the total number of words and K the number of topics.
We will assume that we are working with a sample set of D documents. We will also denote by
˜fd j the fractional count of word j in document d (i.e. ˜fd j = Count(j)/Nd  where Count(j) is the
number of times word j appears in the document  and Nd is the number of words in the document).
For topic models variational updates are a way to approximate the computationally intractable
E-step [23] as described in Section 2. Recall the model parameters for topic models are the
topic prior parameters α and the topic-word matrix β. The observable X is the list of words
in the document. The latent variables are the topic assignments Zj at each position j in the
document and the topic proportions γ. The variational E-step hence becomes ˜P t(Z  γ) =
minP t∈F KL(P t(Z  γ)||P (Z  γ|X  αt  βt) for some family F of distributions. The family F one
usually considered is P t(γ  Z) = q(γ)ΠNd
j(Zj)  i.e. a mean ﬁeld family. In [12] it’s shown
that for Dirichlet priors α the optimal distributions q  q(cid:48)
j are a Dirichlet distribution for q  with some
parameter ˜γ  and multinomials for q(cid:48)
j  with some parameters φj. The variational EM updates are
shown to have the following form. In the E-step  one runs to convergence the following updates on
the φ and ˜γ parameters: φd j i ∝ βt
j=1 φd j i. In the M-step  one

eEq[log(γd)|˜γd]  ˜γd i = αt

d i +(cid:80)Nd

j=1q(cid:48)

i wd j

updates the β and parameters by setting βt+1

d j iwd j j(cid:48) where φt
φt

d j i is the converged

value of φd j i; wd j is the word in document d  position j; wd j j(cid:48) is an indicator variable which is 1
if the word in position j(cid:48) in document d is word j. The α Dirichlet parameters do not have a closed
form expression and are updated via gradient descent.

i j ∝ D(cid:88)

Nd(cid:88)

d=1

j(cid:48)=1

4.1 Simpliﬁed updates in the long document limit

From the above updates it is difﬁcult to give assign an intuitive meaning to the ˜γ and φ parameters.
(Indeed  it’s not even clear what one would like them to be ideally at the global optimum.) We will
be however working in the large document limit - and this will simplify the updates. In particular 
in the E-step  in the large document limit  the ﬁrst term in the update equation for ˜γ has a vanishing
contribution. In this case  we can simplify the E-update as: φd j i ∝ βt
j=1 φd j i.
Notice  importantly  in the second update we now use variables γd i instead of ˜γd i  which are nor-

i jγd i  γd i ∝(cid:80)Nd

malized such that

γd i = 1. These correspond to the max-likelihood topic proportions  given

K(cid:88)

i=1

i j ∝ D(cid:88)

βt+1

our current estimates βt
i j for the model parameters. The M-step will remain as is - but we will
focus on the β only  and ignore the α updates - as the α estimates disappeared from the E updates:

˜fd jγt

d i  where γt

d i is the converged value of γd i. In this case  the intuitive mean-

d=1

ing of the βt and γt variables is clear: they are estimates of the the model parameters  and the
max-likelihood topic proportions  given an estimate of the model parameters  respectively.
The way we derived them  these updates appear to be an approximate form of the variational updates
in [12]. However it is possible to also view them in a more principled manner. These updates

3

approximate the posterior distribution P (Z  γ|X  αt  βt) by ﬁrst approximating this posterior by
P (Z|X  γ∗  αt  βt)  where γ∗ is the max-likelihood value for γ  given our current estimates of
α  β  and then setting P (Z|X  γ∗  αt  βt) to be a product distribution. It is intuitively clear that
in the large document limit  this approximation should not be much worse than the one in [12] 
as the posterior concentrates around the maximum likelihood value. (And in fact  our proofs will
work for ﬁnite  but long documents.) Finally  we will rewrite the above equations in a slightly

more convenient form. Denoting fd j =

γd iβt

i j  the E-step can be written as: iterate until

K(cid:88)

N(cid:88)

j=1

i=1

˜fd j
fd j

i j. The M-step becomes: βt+1
βt

i j = βt
i j

(cid:80)D
(cid:80)D

d=1

˜fd j
f t
d j

γt
d i

d=1 γt
d i

where

convergence γd i = γd i

K(cid:88)

f t
d j =

γt
d iβt

i j and γt

d i is the converged value of γd i.

i=1

4.2 Alternating KL minimization and thresholded updates

d = minγt

d=1 KL(f t

There the authors proved that under these updates(cid:80)D

We will further modify the E and M-step update equations we derived above. In a slightly modiﬁed
form  these updates were used in a paper by [21] in the context of non-negative matrix factorization.
d j|| ˜fd j) is non-decreasing. One can
easily modify their arguments to show that the same property is preserved if the E-step is replaced
d∈∆K KL( ˜fd||fd)  where ∆K is the K-dimensional simplex - i.e. minimizing
by a step γt
the KL divergence between the counts and the ”predicted counts” with respect to the γ variables. (In
fact  iterating the γ updates above is a way to solve this convex minimization problem via a version
of gradient descent which makes multiplicative updates  rather than additive updates.)
Thus the updates are performing alternating minimization using the KL divergence as the distance
measure (with the difference that for the β variables one essentially just performs a single gradient
step). In this paper  we will make a modiﬁcation of the M-step which is very natural. Intuitively  the
update for βt
i j goes over all appearances of the word j and adds the “fractional assignment” of the
word j to topic i under our current estimates of the variables β  γ. In the modiﬁed version we will
d i(cid:48) ∀i(cid:48) (cid:54)= i. The intuitive reason behind this
only average over those documents d  where γt
modiﬁcation is the following. The EM updates we are studying work with the KL divergence  which
d i should
puts more weight on the larger entries. Thus  for the documents in Di  the estimates for γt
be better than they might be in the documents D \ Di. (Of course  since the terms f t
d j involve all
the variables γt
d i  it is not a priori clear that this modiﬁcation will gain us much  but we will prove
that it in fact does.) Formally  we discuss the three modiﬁcations of variational inference speciﬁed
as Algorithm 1  2 and 3 (we call them tEM  for thresholded EM):

d i > γt

Algorithm 1 KL-tEM

d i ≥ 0 (cid:80)

(E-step) Solve the following convex program for each document d: minγt
d i = 0 if i is not in the support of document d
γt
(M-step) Let Di to be the set of documents d  s.t. γt

d i(cid:48) ∀i(cid:48) (cid:54)= i.

d i > γt

i γt

d i

d i = 1 and γt
(cid:80)
(cid:80)

˜fd j
f t
d∈Di

d∈Di

d j
γt

d i

γt
d i

Set βt+1

i j = βt
i j

(cid:80)

j

˜fd j log(

˜fd j
f t
d j

)  s.t.

5

Initializations

We will consider two different strategies for initialization. First  we will consider the case where
we initialize with the topic-word matrix  and the document priors having the correct support. The
analysis of tEM in this case will be the cleanest. While the main focus of the paper is tEM  we’ll
show that this initialization can actually be done for our case efﬁciently. Second  we will consider
an initialization that is inspired by what the current LDA-c implementation uses. Concretely  we’ll

4

Algorithm 2 Iterative tEM

(E-step) Initialize γd i uniformly among the topics in the support of document d.
Repeat

N(cid:88)

γd i = γd i

˜fd j
fd j

βt
i j

(4.1)

until convergence.
(M-step) Same as above.

Algorithm 3 Incomplete tEM

j=1

(E-step) Initialize γd i with the values gotten in the previous iteration  then perform just one step
of 4.1.
(M-step) Same as before.

assume that the user has some way of ﬁnding  for each topic i  a seed document in which the
proportion of topic i is at least Cl. Then  when initializing  one treats this document as if it were
pure: namely one sets β0
i j to be the fractional count of word j in this document. We do not attempt
to design an algorithm to ﬁnd these documents.

6 Case study 1: Sparse topic priors  support initialization

d iβ∗

i=1 γ∗

We start with a simple case. As mentioned  all of our results only hold in the long documents
regime: we will assume for each document d  the number of sampled words is large enough  so that
one can approximate the expected frequencies of the words  i.e.  one can ﬁnd values γ∗
d i  such that
i j. We’ll split the rest of the assumptions into those that apply to the topic-
word matrix  and the topic priors. Let’s ﬁrst consider the assumptions on the topic-word matrix. We
will impose conditions that ensure the topics don’t overlap too much. Namely  we assume:
• Words are discriminative: Each word appears in o(K) topics.

˜fd j = (1±)(cid:80)K
• Almost disjoint supports: ∀i  i(cid:48)  if the intersection of the supports of i and i(cid:48) is S (cid:80)
o(1) ·(cid:80)

j∈S β∗

i j ≤

j β∗
i j.

We also need assumptions on the topic priors. The documents will be sparse  and all topics will
be roughly equally likely to appear. There will be virtually no dependence between the topics:
conditioning on the size or presence of a certain topic will not inﬂuence much the probability of
another topic being included. These are analogues of distributions that have been analyzed for
dictionary learning [6]. Formally:
• Sparse and gapped documents: Each of the documents in our samples has at most T = O(1)
topics. Furthermore  for each document d  the largest topic i0 = argmaxiγ∗
d i is such that for any
other topic i(cid:48)  γ∗
d i(cid:48) ∀i(cid:48) (cid:54)= i is
• Dominant topic equidistribution: The probability that topic i is such that γ∗
d i > γ∗
• Weak topic correlations and independent topic distribution: For all sets S with o(K) topics  it
d i(cid:48) =
d i(cid:48)∀i(cid:48) ∈ S] =

Θ(1/K).
d i|γ∗
must be the case that: E[γ∗
0  i(cid:48) ∈ S]. Furthermore  for any set S of topics  s.t. |S| ≤ T − 1  Pr[γ∗
Θ( 1

d i is dominating] = (1 ± o(1))E[γ∗

d i is dominating  γ∗
d i > 0|γ∗

> ρ for some (arbitrarily small) constant ρ.

d i(cid:48) − γ∗

d i|γ∗

d i0

K )

These assumptions are a less smooth version of properties of the Dirichlet prior. Namely  it’s a
folklore result that Dirichlet draws are sparse with high probability  for a certain reasonable range of
parameters. This was formally proven by [25] - though sparsity there means a small number of large
coordinates. It’s also well known that Dirichlet essentially cannot enforce any correlation between
different topics. 1

1We show analogues of the weak topic correlations property and equidistribution in the supplementary

material for completeness sake.

5

The above assumptions can be viewed as a local notion of separability of the model  in the following
sense. First  consider a particular document d. For each topic i that participates in that document 
consider the words j  which only appear in the support of topic i in the document. In some sense 
these words are local anchor words for that document: these words appear only in one topic of that
document. Because of the ”almost disjoint supports” property  there will be a decent mass on these
words in each document. Similarly  consider a particular non-zero element β∗
i j of the topic-word
i(cid:48) j = 0 for all other topics i(cid:48) (cid:54)= i appearing in
matrix. Let’s call Dl the set of documents where β∗
that document. These documents are like local anchor documents for that word-topic pair: in those
documents  the word appears as part of only topic i. It turns out the above properties imply there is
a decent number of these for any word-topic pair.
Finally  a technical condition: we will also assume that all nonzero γ∗
poly(N ).
Intuitively  this means if a topic is present  it needs to be reasonably large  and similarly for words
in topics. Such assumptions also appear in the context of dictionary learning [6].
We will prove the following
Theorem 1. Given an instance of topic modelling satisfying the properties speciﬁed above  where
the number of documents is Ω( K log2 N
d i variables
correctly  after O (log(1/(cid:48)) + log N ) KL-tEM  iterative-tEM updates or incomplete-tEM updates 
we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + (cid:48)  for any (cid:48)
s.t. 1 + (cid:48) ≤ 1
Theorem 2. If the number of documents is Ω(K 4 log2 K)  there is a polynomial-time procedure
which with probability 1 − Ω( 1

K ) correctly identiﬁes the supports of the β∗

)  if we initialize the supports of the βt

i j are at least

i j and γt

i j and γ∗

d i variables.

d i  β∗

1

2

(1−)7 .

document  topic i is actually the largest topic.

Provable convergence of tEM: The correctness of the tEM updates is proven in 3 steps:
• Identifying dominating topic: First  we prove that if γt
d i is the largest one among all topics in the
• Phase I: Getting constant multiplicative factor estimates: After initialization  after O(log N )
d i which are within a constant multiplicative factor from
• Phase II (Alternating minimization - lower and upper bound evolution): Once the β and γ es-
timates are within a constant factor of their true values  we show that the lone words and docu-
ments have a boosting effect: they cause the multiplicative upper and lower bounds to improve
at each round.

rounds  we will get to variables βt
β∗
i j  γ∗
d i.

i j  γt

i j ≈ αβ∗

i j + (1 − α)C tβ∗

The updates we are studying are multiplicative  not additive in nature  and the objective they are
optimizing is non-convex  so the standard techniques do not work. The intuition behind our proof in
Phase II can be described as follows. Consider one update for one of the variables  say βt
i j. We show
that βt+1
i j for some constant C t at time step t. α is something fairly large
(one should think of it as 1 − o(1))  and comes from the existence of the local anchor documents.
A similar equation holds for the γ variables  in which case the “good” term comes from the local
anchor words. Furthermore  we show that the error in the ≈ decreases over time  as does the value
of C t  so that eventually we can reach β∗
i j. The analysis bears a resemblance to the state evolution
and density evolution methods in error decoding algorithm analysis - in the sense that we maintain
a quantity about the evolving system  and analyze how it evolves under the speciﬁed iterations. The
quantities we maintain are quite simple - upper and lower multiplicative bounds on our estimates at
any round t.
Initialization: Recall the goal of this phase is to recover the supports - i.e. to ﬁnd out which topics
are present in a document  and identify the support of each topic. We will ﬁnd the topic supports
ﬁrst. This uses an idea inspired by [8] in the setting of dictionary learning. Roughly  we devise a
test  which will take as input two documents d  d(cid:48)  and will try to determine if the two documents
have a topic in common or not. The test will have no false positives  i.e.  will never say YES  if the
documents don’t have a topic in common  but might say NO even if they do. We then ensure that
with high probability  for each topic we ﬁnd a pair of documents intersecting in that topic  such that
the test says YES. 2

2The detailed initialization algorithm is included in the supplementary material.

6

7 Case study 2: Dominating topics  seeded initialization

d i ≥ Cl.

i j = f∗
d j.

Next  we’ll consider an initialization which is essentially what the current implementation of LDA-c
uses. Namely  we will call the following initialization a seeded initialization:
• For each topic i  the user supplies a document d  in which γ∗
• We treat the document as if it only contains topic i and initialize with β0
We show how to modify the previous analysis to show that with a few more assumptions  this
strategy works as well. Firstly  we will have to assume anchor words  that make up a decent fraction
of the mass of each topic. Second  we also assume that the words have a bounded dynamic range  i.e.
the values of a word in two different topics are within a constant B from each other. The documents
are still gapped  but the gap now must be larger. Finally  in roughly 1/B fraction of the documents
where topic i is dominant  that topic has proportion 1 − δ  for some small (but still constant) δ. A
similar assumption (a small fraction of almost pure documents) appeared in a recent paper by [10].
Formally  we have:
• Small dynamic range and large fraction of anchors: For each discriminative words  if β∗

i j (cid:54)= 0
i(cid:48) j. Furthermore  each topic i has anchor words  such that their total
• Gapped documents: In each document  the largest topic has proportion at least Cl  and all the

i(cid:48) j (cid:54)= 0  β∗
and β∗
weight is at least p.

other topics are at most Cs  s.t.

i j ≤ Bβ∗
(cid:32)(cid:115)

(cid:18)

• Small fraction of 1 − δ dominant documents: Among all the documents where topic i is domi-

Cl − Cs ≥ 1
p

2

p log(

) + (1 − p) log(BCl)

1
Cl

(cid:32)

(cid:32)(cid:115)

(cid:18)

nating  in a 8/B fraction of them  γ∗

δ := min

2

p log(

C 2
l

2B3 − 1

p

d i ≥ 1 − δ  where
1
Cl

) + (1 − p) log(BCl)

(cid:19)

(cid:19)

(cid:33)
+(cid:112)log(1 + )
(cid:33)
+(cid:112)log(1 + )

+ 

(cid:33)

−   1 −(cid:112)

Cl

log B   since log( 1
Cl

) ≈ 1+η  roughly we want that Cl−Cs (cid:29) 2

√
The dependency between the parameters B  p  Cl is a little difﬁcult to parse  but if one thinks of Cl
as 1−η for η small  and p ≥ 1− η
η.
(In other words  the weight we require to have on the anchors depends only logarithmically on the
range B.) In the documents where the dominant topic has proportion 1 − δ  a similar reasoning as
above gives that we want is approximately γ∗
η. The precise statement is as
follows:
Theorem 3. Given an instance of topic modelling satisfying the properties speciﬁed above 
where the number of documents is Ω( K log2 N
)  if we initialize with seeded initialization  after
O (log(1/(cid:48)) + log N ) of KL-tEM updates  we recover the topic-word matrix and topic proportions
to multiplicative accuracy 1 + (cid:48)  if 1 + (cid:48) ≥ 1

d i ≥ 1 − 1 − 2η
2B3 +

√

2
p

2

p

(1−)7 .

The proof is carried out in a few phases:
• Phase I: Anchor identiﬁcation: We show that as long as we can identify the dominating topic in
each of the documents  anchor words will make progress: after O(log N ) number of rounds  the
values for the topic-word estimates will be almost zero for the topics for which word w is not an
anchor. For topic for which a word is an anchor we’ll have a good estimate.
• Phase II: Discriminative word identiﬁcation: After the anchor words are properly identiﬁed in
the previous phase  if β∗
i j will keep dropping and quickly reach almost zero. The
values corresponding to β∗
• Phase III: Alternating minimization: After Phase I and II above  we are back to the scenario of

i j = 0  βt
i j (cid:54)= 0 will be decently estimated.

the previous section: namely  there is improvement in each next round.

During Phase I and II the intuition is the following: due to our initialization  even in the beginning 
each topic is ”correlated” with the correct values. In a γ update  we are minimizing KL( ˜fd||fd)
with respect to the γd variables  so we need a way to argue that whenever the β estimates are not too
bad  minimizing this quantity provides an estimate about how far the optimal γd variables are from
γ∗
d. We show the following useful claim:

7

2

i ||βt

(cid:113) 1

2 Rβ + 1

(cid:112)Rf ) + .

i ) ≤ Rβ  and minγd∈∆K KL( ˜fd j||fd j) ≤ Rf   after
d−γd||1 ≤

Lemma 4. If  for all topics i  KL(β∗
running a KL divergence minimization step with respect to the γd variables  we get that ||γ∗
1
p (
This lemma critically uses the existence of anchor words - namely we show ||β∗v||1 ≥ p||v||1.
Intuitively  if one thinks of v as γ∗ − γt  ||β∗v||1 will be large if ||v||1 is large. Hence  if ||β∗ − βt||1
is not too large  whenever ||f∗ − f t||1 is small  so is ||γ∗ − γt||1. We will be able to maintain Rβ
and Rf small enough throughout the iterations  so that we can identify the largest topic in each of
the documents.

i j ≤ κβ∗

8 On common words
i(cid:48) j ∀i  i(cid:48)  κ ≤ B. In this case  the
We brieﬂy remark on common words: words such that β∗
proofs above  as they are  will not work  3 since common words do not have any lone documents.
However  if 1 − 1
κ100 fraction of the documents where topic i is dominant contains topic i with
proportion 1 − 1
κ100 and furthermore  in each topic  the weight on these words is no more than
κ100   then our proofs still work with either initialization4 The idea for the argument is simple: when
the dominating topic is very large  we show that f∗
  so these
documents behave like anchor documents. Namely  one can show:
Theorem 5. If we additionally have common words satisfying the properties speciﬁed above  after
O(log(1/(cid:48)) + log N ) KL-tEM updates in Case Study 2  or any of the tEM variants in Case Study 1 
and we use the same initializations as before  we recover the topic-word matrix and topic proportions
to multiplicative accuracy 1 + (cid:48)  if 1 + (cid:48) ≥ 1

is very highly correlated with β∗

βt

f t

d j

d j

i j

i j

1

(1−)7 .

9 Discussion and open problems

In this work we provide the ﬁrst characterization of sufﬁcient conditions when variational inference
leads to optimal parameter estimates for topic models. Our proofs also suggest possible hard cases
for variational inference  namely instances with large dynamic range compared to the proportion of
anchor words and/or correlated topic priors. It’s not hard to hand-craft such instances where support
initialization performs very badly  even with only anchor and common words. We made no effort to
explore the optimal relationship between the dynamic range and the proportion of anchor words  as
it’s not clear what are the “worst case” instances for this trade-off.
Seeded initialization  on the other hand  empirically works much better. We found that when Cl ≥
0.6  and when the proportion of anchor words is as low as 0.2  variational inference recovers the
ground truth  even on instances with fairly large dynamic range. Our current proof methods are too
weak to capture this observation. (In fact  even the largest topic is sometimes misidentiﬁed in the
initial stages  so one cannot even run tEM  only the vanilla variational inference updates.) Analyzing
the dynamics of variational inference in this regime seems like a challenging problem which would
require signiﬁcantly new ideas.

References
[1] A. Agarwal  A. Anandkumar  P. Jain  and P. Netrapalli. Learning sparsely used overcomplete
dictionaries via alternating minimization. In Proceedings of The 27th Conference on Learning
Theory (COLT)  2013.

[2] A. Anandkumar  D. Hsu  A. Javanmard  and S. Kakade. Learning latent bayesian networks and
topic models under expansion constraints. In Proceedings of the 30th International Conference
on Machine Learning (ICML)  2013.

3We stress we want to analyze whether variational inference will work or not. Handling common words
algorithmically is easy: they can be detected and ”ﬁltered out” initially. Then we can perform the variational
inference updates over the rest of the words only. This is in fact often done in practice.

4See supplementary material.

8

[3] A. Anandkumar  S. Kakade  D. Foster  Y. Liu  and D. Hsu. Two svds sufﬁce: Spectral de-
compositions for probabilistic topic modeling and latent dirichlet allocation. Technical report 
2012.

[4] S. Arora  R. Ge  Y. Halpern  D. Mimno  A. Moitra  D. Sontag  Y. Wu  and M. Zhu. A practical
algorithm for topic modeling with provable guarantees. In Proceedings of the 30th Interna-
tional Conference on Machine Learning (ICML)  2013.

[5] S. Arora  R. Ge  R. Kanna  and A. Moitra. Computing a nonnegative matrix factorization–
provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of Computing 
pages 145–162. ACM  2012.

[6] S. Arora  R. Ge  T. Ma  and A. Moitra. Simple  efﬁcient  and neural algorithms for sparse

coding. In Proceedings of The 28th Conference on Learning Theory (COLT)  2015.

[7] S. Arora  R. Ge  and A. Moitra. Learning topic models – going beyond svd. In Proceedings of

the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS)  2012.

[8] S. Arora  R. Ge  and A. Moitra. New algorithms for learning incoherent and overcomplete

dictionaries. In Proceedings of The 27th Conference on Learning Theory (COLT)  2014.

[9] S. Balakrishnan  M.J. Wainwright  and B. Yu. Statistical guarantees for the em algorithm:

From population to sample-based analysis. arXiv preprint arXiv:1408.2156  2014.

[10] T. Bansal  C. Bhattacharyya  and R. Kannan. A provable svd-based algorithm for learning
topics in dominant admixture corpus. In Advances in Neural Information Processing Systems
(NIPS)  2014.

[11] D. Blei and J.D. Lafferty. Topic models. Text mining: classiﬁcation  clustering  and applica-

tions  10:71  2009.

[12] D. Blei  A. Ng    and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[13] S. Dasgupta and L. Schulman. A two-round variant of em for gaussian mixtures. In Proceed-

ings of Uncertainty in Artiﬁcial Intelligence (UAI)  2000.

[14] S. Dasgupta and L. Schulman. A probabilistic analysis of em for mixtures of separated  spher-

ical gaussians. Journal of Machine Learning Research  8:203–226  2007.

[15] A. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the em

algorithm. Journal of the Royal Statistical Society  Series B  39:1–38  1977.

[16] W. Ding  M.H. Rohban  P. Ishwar  and V. Saligrama. Topic discovery through data dependent

and random projections. arXiv preprint arXiv:1303.3664  2013.

[17] W. Ding  M.H. Rohban  P. Ishwar  and V. Saligrama. Efﬁcient distributed topic modeling
with provable guarantees. In Proceedings ot the 17th International Conference on Artiﬁcial
Intelligence and Statistics  pages 167–175  2014.

[18] M. Hoffman  D. Blei  J. Paisley  and C. Wan. Stochastic variational inference. Journal of

Machine Learning Research  14:1303–1347  2013.

[19] M. Jordan  Z. Ghahramani  T. Jaakkola  and L. Saul. An introduction to variational methods

for graphical models. Machine learning  37(2):183–233  1999.

[20] A. Kumar and R. Kannan. Clustering with spectral norm and the k-means algorithm.

Proceedings of Foundations of Computer Science (FOCS)  2010.

In

[21] D. Lee and S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural

Information Processing Systems (NIPS)  2000.

[22] P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization.

Advances in Neural Information Processing Systems (NIPS)  2013.

In

[23] D. Sontag and D. Roy. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems (NIPS)  2000.

[24] R. Sundberg. Maximum likelihood from incomplete data via the em algorithm. Scandinavian

Journal of Statistics  1:49–58  1974.

[25] M. Telgarsky. Dirichlet draws are sparse with high probability. Manuscript  2013.

9

,Mijung Park
Jonathan Pillow
Pranjal Awasthi
Andrej Risteski