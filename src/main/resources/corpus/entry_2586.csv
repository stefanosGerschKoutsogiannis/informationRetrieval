2016,Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits,We propose a new oracle-based algorithm  BISTRO+  for the adversarial contextual bandit problem  where either contexts are drawn i.i.d. or the sequence of contexts is known a priori  but where the losses are picked adversarially. Our algorithm is computationally efficient  assuming access to an offline optimization oracle  and enjoys a regret of order $O((KT)^{\frac{2}{3}}(\log N)^{\frac{1}{3}})$  where $K$ is the number of actions  $T$ is the number of iterations  and $N$ is the number of baseline policies. Our result is the first to break the $O(T^{\frac{3}{4}})$ barrier achieved by recent algorithms  which was left as a major open problem. Our analysis employs the recent relaxation framework of (Rakhlin and Sridharan  ICML'16).,Improved Regret Bounds for Oracle-Based

Adversarial Contextual Bandits

Vasilis Syrgkanis
Microsoft Research
vasy@microsoft.com

Haipeng Luo

Microsoft Research

haipeng@microsoft.com

Akshay Krishnamurthy

University of Massachusetts  Amherst

akshay@cs.umass.edu

Robert E. Schapire
Microsoft Research

schapire@microsoft.com

Abstract

We propose a new oracle-based algorithm  BISTRO+  for the adversarial con-
textual bandit problem  where either contexts are drawn i.i.d. or the sequence
of contexts is known a priori  but where the losses are picked adversarially. Our
algorithm is computationally efﬁcient  assuming access to an ofﬂine optimization
oracle  and enjoys a regret of order O((KT ) 2
3 )  where K is the number
of actions  T is the number of iterations  and N is the number of baseline policies.
Our result is the ﬁrst to break the O(T 3
4 ) barrier achieved by recent algorithms 
which was left as a major open problem. Our analysis employs the recent relaxation
framework of Rakhlin and Sridharan [7].

3 (log N ) 1

1

Introduction

We study online decision making problems where a learner chooses an action based on some side
information (context) and incurs some cost for that action with a goal of incurring minimal cost over
a sequence of rounds. These contextual online learning settings form a powerful framework for
modeling many important decision-making scenarios with applications ranging from personalized
health care to content recommendation and targeted advertising. Many of these applications also
involve a partial feedback component  wherein costs for alternative actions are unobserved  and are
typically modeled as contextual bandits.
The contextual information present in these problems enables learning of a much richer policy
for choosing actions based on context. In the literature  the typical goal for the learner is to have
cumulative cost that is not much higher than the best policy π in a large policy class Π. This is
formalized by the notion of regret  which is the learner’s cumulative cost minus the cumulative cost
of the best ﬁxed policy π in hindsight.
Naively  one can view the contextual problem as a standard online learning problem where the set
of possible “actions” available at each iteration is the set of policies. This perspective is fruitful  as
classical algorithms  such as Hedge [5  3] and Exp4 [2]  give information theoretically optimal regret

bounds of O((cid:112)T log(N )) in full-information and O((cid:112)T K log(N ) in the bandit setting  where T is

the number of rounds  K is the number of actions  and N is number of policies. However  naively
lifting standard online learning algorithms to the contextual setting leads to a running time that is
linear in the number of policies. Given that the optimal regret is only logarithmic in N and that our
high-level goal is to learn a very rich policy  we want to capture policy classes that are exponentially
large. When we use a large policy class  existing algorithms are no longer computationally tractable.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

To study this computational question  a number of recent papers have developed oracle-based
algorithms that only access the policy class through an optimization oracle for the ofﬂine full-
information problem. Oracle-based approaches harness the research in supervised learning that
focuses on designing efﬁcient algorithms for full-information problems and uses it for online and
partial-feedback problems. Optimization oracles have been used in designing contextual bandit

algorithms [1  4] that achieve the optimal O((cid:112)KT log(N )) regret while also being computationally

4 K 1

2 (log(N )) 1

3 (log(N )) 1

4 ) barrier can be broken.

efﬁcient (i.e. requiring poly(K  log(N )  T ) oracle calls and computation). However  these results
only apply when the contexts and costs are independently and identically distributed at each iteration 
contrasting with the computationally inefﬁcient approaches that can handle adversarial inputs.
Two very recent works provide the ﬁrst oracle efﬁcient algorithms for the contextual bandit problem
in adversarial settings [7  8]. Rakhlin and Sridharan [7] considers a setting where the contexts are
drawn i.i.d. from a known distribution with adversarial costs and they provide an oracle efﬁcient
algorithm called BISTRO with O(T 3
4 ) regret. Their algorithm also applies in the
transductive setting where the sequence of contexts is known a priori. Srygkanis et. al [8] also obtain
a T 3
4 -style bound with a different oracle-efﬁcient algorithm  but in a setting where the learner knows
√
only the set of contexts that will arrive. Both of these results achieve very suboptimal regret bounds 
as the dependence on the number of iterations is far from the optimal O(
T )-bound. A major open
question posed by both works is whether the O(T 3
In this paper  we provide an oracle-based contextual bandit algorithm  BISTRO+  that achieves regret
O((KT ) 2
3 ) in both the i.i.d. context and the transductive settings considered by Rakhlin
and Sridharan [7]. This bound matches the T -dependence of the epoch-greedy algorithm of Langford
and Zhang [6] that only applies to the fully stochastic setting. As in Rakhlin and Sridharan [7] 
our algorithm only requires access to a value oracle  which is weaker than the standard argmax
oracle  and it makes K + 1 oracle calls per iteration. To our knowledge  this is the best regret bound
achievable by an oracle-efﬁcient algorithm for any adversarial contextual bandit problem.
Our algorithm and regret bound are based on a novel and improved analysis of the minimax prob-
lem that arises in the relaxation-based framework of Rakhlin and Sridharan [7] (hence the name
BISTRO+). Our proof requires analyzing the value of a sequential game where the learner chooses a
distribution over actions and then the adversary chooses a distribution over costs in some bounded
ﬁnite domain  with - importantly - a bounded variance. This is unlike the simpler minimax problem
analyzed in [7]  where the adversary is only constrained by the range of the costs.
Apart from showing that this more structured minimax problem has a small value  we also need to
derive an oracle-based strategy for the learner that achieves the improved regret bound. The additional
constraints on the game require a much more intricate argument to derive this strategy which is an
algorithm for solving a structured two-player minimax game (see Section 4).

2 Model and Preliminaries
Basic notation. Throughout the paper we denote with x1:t a sequence of quantities {x1  . . .   xt}
and with (x  y  z)1:t a sequence of tuples {(x1  y1  z1)  . . .}. ∅ denotes an empty sequence. The
vector of ones is denoted by 1 and the vector of zeroes is denoted by 0. Denote with [K] the set
{1  . . .   K}  e1  . . .   eK the standard basis vectors in RK  and ∆U the set of distributions over a set
U. We also use ∆K as a shorthand for ∆[K].

Contextual online learning. We consider the following version of the contextual online learning
problem. On each round t = 1  . . .   T   the learner observes a context xt and then chooses a probability
distribution qt over a set of K actions. The adversary then chooses a cost vector ct ∈ [0  1]K. The
learner picks an action ˆyt drawn from distribution qt  incurs a cost ct(ˆyt) and observes only ct(ˆyt)
and not the cost of the other actions.
Throughout the paper we will assume that the context xt at each iteration t is drawn i.i.d. from a
distribution D. This is referred to as the hybrid i.i.d.-adversarial setting [7]. As in prior work [7]  we
assume that the learner can sample contexts from this distribution as needed. It is easy to adapt the
arguments in the paper to apply for the transductive setting where the learner knows the sequence of
contexts that will arrive. The cost vectors ct are chosen by a non-adaptive adversary.

2

The goal of the learner is to compete with a set of policies Π of size N  where each policy π ∈ Π is a
function mapping contexts to actions. The cumulative expected regret with respect to the best ﬁxed
policy in hindsight is

T(cid:88)

t=1

T(cid:88)

t=1

REG =

(cid:104)qt  ct(cid:105) − min
π∈Π

ct(π(xt)).

Optimization value oracle. We will assume that we are given access to an optimization oracle that
when given as input a sequence of contexts and cost vectors (x  c)1:t  it outputs the cumulative cost
of the best ﬁxed policy  which is

ct(π(xt)).

(1)

t(cid:88)

τ =1

min
π∈Π

(cid:20)

This can be viewed as an ofﬂine batch optimization or ERM oracle.

2.1 Relaxation based algorithms

We brieﬂy review the relaxation based framework proposed in [7]. The reader is directed to [7]
for a more extensive exposition. We will also slightly augment the framework with some internal
randomness that the algorithm can generate and use  which does not affect the cost of the algorithm.
A crucial concept in the relaxation based framework is the information obtained by the learner at the
end of each round t ∈ [T ]  which is the following tuple:

It(xt  qt  ˆyt  ct  St) = (xt  qt  ˆyt  ct(ˆyt)  St) 

where ˆyt is the realized chosen action drawn from the distribution qt and St is some random string
drawn from some distribution that can depend on qt  ˆyt and ct(ˆyt) and which can be used by the
algorithm in subsequent rounds.
Deﬁnition 1 A partial-information relaxation REL(·) is a function that maps (I1  . . .   It) to a real
value for any t ∈ [T ]. A partial-information relaxation is admissible if for any t ∈ [T ]  and for all
I1  . . .   It−1

Ext

min
qt

max

ct

Eˆyt∼qt St [ct(ˆyt) + REL(I1:t−1  It(xt  qt  ˆyt  ct  St))]

≤ REL(I1:t−1) 

(2)

and for all x1:T   c1:T and q1:T

Eˆy1:T ∼q1:T  S1:T [REL(I1:T )] ≥ − min
π∈Π

T(cid:88)

t=1

ct(π(xt)).

(3)

Deﬁnition 2 Any randomized strategy q1:T that certiﬁes inequalities (2) and (3) is called an admissi-
ble strategy.

A basic lemma proven in [7] is that if one constructs a relaxation and a corresponding admissible
strategy  then the expected regret of the admissible strategy is upper bounded by the value of the
relaxation at the beginning of time.

Lemma 1 ([7]) Let REL be an admissible relaxation and q1:T be an admissible strategy. Then for
any c1:T   we have

E [REG] ≤ REL(∅).

We will utilize this framework and construct a novel relaxation with an admissible strategy. We
will show that the value of the relaxation at the beginning of time is upper bounded by the desired
improved regret bound and that the admissible strategy can be efﬁciently computed assuming access
to an optimization value oracle.

3

(cid:21)

3 A Faster Contextual Bandit Algorithm

(cid:40)

First we deﬁne an unbiased estimator the cost vectors ct. In addition to doing the usual importance
weighting  we also discretize the estimated cost to either 0 or L for some constant L ≥ K to be
speciﬁed later. Speciﬁcally  suppose that at iteration t an action ˆyt is picked based on some distribution
ht ∈ ∆K. Now  consider the random variable Xt  which is deﬁned conditionally on ˆyt and ht  as

Xt =

1 with probability ct(ˆyt)
0 with the remaining probability.

Lht(ˆyt)  

(4)

This is a valid random variable whenever miny ht(y) ≥ 1
L  which will be ensured by the algorithm.
This is the only randomness in the random string St that we used in the general relaxation framework.
Our construction of an unbiased estimate for each ct based on the information It collected at the end
of each round is then: ˆct = LXteˆyt. Observe that for any y ∈ [K] 

Eˆyt Xt [ˆct(y)] = L · Pr[ˆyt = y] · Pr[Xt = 1|ˆyt = y] = L · ht(y) · ct(y)
Lht(y)

= ct(y).

Hence  ˆct is an unbiased estimate of ct.
We are now ready to deﬁne our relaxation. Let t ∈ {−1  1}K be a Rademacher random vector
(i.e. each coordinate is an independent Rademacher random variable  which is −1 or 1 with equal
probability)  and let Zt ∈ {0  L} be a random variable which is L with probability K/L and 0
otherwise. We denote with ρt = (x    Z)t+1:T and with Gt the distribution of ρt which is described
above. Our relaxation is deﬁned as follows:

REL(I1:t) = Eρt∼Gt [R((x  ˆc)1:t  ρt)] 

(5)

where

(cid:32) t(cid:88)

T(cid:88)

(cid:34)

T(cid:88)

(cid:33)

(cid:35)

R((x  ˆc)1:t  ρt) = − min
π∈Π

+ (T − t)K/L.
Note that REL(∅) is the following quantity  whose ﬁrst part resembles a Rademacher average:

2τ (π(xτ ))Zτ

ˆcτ (π(xτ )) +

τ =t+1

τ =1

REL(∅) = 2E(x  Z)1:T

max
π∈Π

τ (π(xτ ))Zτ

+ T K/L.

Using the following lemma (whose proof is deferred to the supplementary material) and the fact

(cid:3) ≤ KL  we can upper bound REL(∅) by O((cid:112)T KL log(N ) + T K/L)  which after tuning L

τ =1

will give the claimed O(T 2/3) bound.
Lemma 2 Let t be Rademacher random vectors  and Zt be non-negative real-valued random

(cid:3) ≤ M for some constant M > 0. Then

E(cid:2)Z 2
variables such that E(cid:2)Z 2

t

t

(cid:34)

T(cid:88)

t=1

EZ1:T  1:T

max
π∈Π

t(π(xt)) · Zt

≤(cid:112)2T M log(N ).

(cid:19)

To show an admissible strategy for our relaxation  we let D = {L · ei : i ∈ [K]} ∪ {0}. For a
distribution p ∈ ∆(D)  we denote with p(i)  for i ∈ {0  . . .   K}  the probability assigned to vector
ei  with the convention that e0 = 0. Also let ∆(cid:48)
Based on this notation our admissible strategy is deﬁned as

D = {p ∈ ∆D : p(i) ≤ 1/L ∀i ∈ [K]}.

and

qt = Eρt [qt(ρt)] where

q∗
t (ρt) = argmin
q∈∆K

max
pt∈∆(cid:48)

D

qt(ρt) =

1 − K
L

q∗
t (ρt) +

1
L
Eˆct∼pt [(cid:104)q  ˆct(cid:105) + R((x  ˆc)1:t  ρt)].

1 

(6)

(7)

Algorithm 1 implements this admissible strategy. Note that it sufﬁces to use qt(ρt) for a single
random draw ρt instead of qt to ensure the exact same guarantee in expectation. In Section 4 we
show that qt(ρt) can be computed efﬁciently using an optimization value oracle.
We state the main theorem of our relaxation construction and defer the proof to Section 5.

4

(cid:35)

(cid:18)

Algorithm 1 BISTRO+

Input: parameter L ≥ K
for each time step t ∈ [T ] do

Observe xt. Draw ρt = (x    Z)t+1:T where each xτ is drawn from the distribution of contexts 
τ is a Rademacher random vectors and Zτ ∈ {0  L} is L with probability K/L and 0 otherwise.
Compute qt(ρt) based on Eq. (6) (using Algorithm 2).
Predict ˆyt ∼ qt(ρt) and observe ct(ˆyt).
Create an estimate ˆct = LXteˆyt  where Xt is deﬁned in Eq. (4) using qt(ρt) as ht.

end for

Algorithm 2 Computing q∗

t (ρt)

Input: a value optimization oracle  (x  ˆc)1:t−1  xt and ρt.
Output: q ∈ ∆K  a solution to Eq. (7).
Compute ψi as in Eq. (9) for all i = 0  . . .   K using the optimization oracle.
for all i ∈ [K].
Compute φi = ψi−ψ0
Let m = 1 and q = 0.
for each coordinate i ∈ [K] do

L

Set q(i) = min{(φi)+  m}. ((x)+ = max{x  0})
Update m ← m − q(i).

end for
Distribute m arbitrarily on the coordinates of q if m > 0.

Theorem 3 The relaxation deﬁned in Equation (5) is admissible. An admissible randomized strategy
for this relaxation is given by (6). The expected regret of BISTRO+ is upper bounded by

2(cid:112)2T KL log(N ) + T K/L 

(8)
3 when T ≥ K 2 log(N )  the regret is of

1

for any L ≥ K. Speciﬁcally  setting L = (KT / log(N ))
order O((KT ) 2

3 (log(N )) 1

3 ).

4 Computational Efﬁciency

In this section we will argue that if one is given access to a value optimization oracle (1)  then one
can run BISTRO+ efﬁciently. Speciﬁcally  we will show that the minimizer of Equation (7) can be
computed efﬁciently via Algorithm 2.

Lemma 4 Computing the quantity deﬁned in equation (7) for any given ρt can be done in time O(K)
and with only K + 1 accesses to a value optimization oracle.
Proof: For i ∈ {0  . . .   K}  let

(cid:32)t−1(cid:88)

τ =1

(cid:33)

T(cid:88)

τ =t+1

K(cid:88)

i=1

5

ψi = min
π∈Π

ˆcτ (π(xτ )) + Lei(π(xt)) +

2τ (π(xτ ))Zt

(9)

with the convention e0 = 0. Then observe that we can re-write the deﬁnition of q∗
pt(i)(L · q(i) − ψi) − pt(0) · ψ0.

K(cid:88)

q∗
t (ρt) = argmin
q∈∆K

max
pt∈∆(cid:48)
D

i=1

t (ρt) as

Observe that each ψi can be computed with a single oracle access. Thus we can assume that all K + 1
ψ’s are computed efﬁciently and are given. We now argue how to compute the minimizer.
For any q  the maximum over pt can be characterized as follows. With the notation zi = L· q(i)− ψi
and z0 = −ψ0 we re-write the minimax quantity as

q∗
t (ρt) = argmin
q∈∆K

max
pt∈∆(cid:48)

D

pt(i) · zi + pt(0) · z0.

Observe that without the constraint that pt(i) ≤ 1/L for i > 0 we would put all the probability mass
on the maximum of the zi. However  with the constraint the maximizer put as much probability
mass as allowed on the maximum coordinate argmaxi∈{0 ... K} zi and continues to the next highest
quantity. We repeat this until reaching the quantity z0  which is unconstrained. Thus we can put all
the remaining probability mass on this coordinate.
Let z(1)  z(2)  . . .   z(K) denote the ordered zi quantities for i > 0 (from largest to smallest). Moreover 
let µ ∈ [K] be the largest index such that z(µ) ≥ z0. By the above reasoning we get that for a given
q  the maximum over pt is equal to (recall that we assume L ≥ K)
z(t) − z0

z(t)
L

+

1 − µ
L

z0 =

+ z0.

L

Now since for any t > µ  z(t) < z0  we can write the latter as

z(t)
L

+

1 − µ
L

z0 =

(zi − z0)+

L

+ z0

with the convention (x)+ = max{x  0}. We thus further re-write the minimax expression as

(cid:17)
(cid:17)

µ(cid:88)
K(cid:88)

t=1

i=1

µ(cid:88)
µ(cid:88)

t=1

t=1

(cid:16)
(cid:16)

K(cid:88)
K(cid:88)

i=1

i=1

q∗
t (ρt) = argmin
q∈∆K

= argmin
q∈∆K

+ z0 = argmin
q∈∆K

(zi − z0)+

L

(cid:18)
q(i) − ψi − ψ0

L

(cid:19)+

.

K(cid:88)

(zi − z0)+

L

i=1

(cid:80)K
t=1(q(i) − φi)+.

t (ρt) = argminq∈∆K

L . The expression becomes: q∗

Let φi = ψi−ψ0
This quantity is minimized as follows: consider any i ∈ [K] such that φi ≤ 0. Then putting positive
mass ξ on such a coordinate i is going to lead to a marginal increase of ξ in the objective. On the other
hand if we put some mass on an index φi > 0  then that will not increase the objective until we reach
i:φi>0 φi  1} 
on the coordinates for which φi > 0. The remaining mass  if any  can be distributed arbitrarily. See
Algorithm 2 for details.

the point where q(i) = φi. Thus a minimizer will distribute probability mass of min{(cid:80)

5 Proof of Theorem 3

We verify the two conditions for admissibility.

Final condition.

It is clear that inequality (3) is satisﬁed since ˆct are unbiased estimates of ct:

Eˆy1:T  X1:T [REL(I1:T )] = Eˆy1:T  X1:T

max
π∈Π

ˆcτ (π(xτ ))

(cid:34)

− T(cid:88)
(cid:34) T(cid:88)

τ =1

τ =1

(cid:35)
(cid:35)

≥ max
π∈Π

−Eˆy1:T  X1:T

ˆcτ (π(xτ ))

= max
π∈Π

− T(cid:88)

τ =1

cτ (π(xτ )).

t-th Step condition. We now check that inequality (2) is also satisﬁed at some time step t ∈ [T ].
We reason conditionally on the observed context xt and show that qt deﬁnes an admissible strategy
for the relaxation. For convenience let Ft denote the joint distribution of the pair (ˆyt  Xt). Observe
that the marginal of Ft on the ﬁrst coordinate is equal to qt. Let q∗
t (ρt)]. First observe that:
E(ˆyt Xt)∼Ft [ct(ˆyt)] = Eˆyt∼qt [ct(ˆyt)] = (cid:104)qt  ct(cid:105) ≤ (cid:104)q∗
K
L

t = Eρt [q∗
(cid:104)1  ct(cid:105) ≤ E(ˆyt Xt)∼Ft [(cid:104)q∗

t   ˆct(cid:105)]+

t   ct(cid:105)+

1
L

.

Hence 

max

ct∈[0 1]K

E(ˆyt Xt)∼Ft [ct(ˆyt) + REL(I1:t)] ≤ max
ct∈[0 1]K

E(ˆyt Xt)∼Ft [(cid:104)q∗

t   ˆct(cid:105) + REL(I1:t)] +

K
L

.

6

We now work with the ﬁrst term of the right hand side.

max

ct∈[0 1]K

= max

ct∈[0 1]K

t   ˆct(cid:105) + REL(I1:t)]

E(ˆyt Xt)∼Ft [(cid:104)q∗
E(ˆyt Xt)∼Ft [Eρt∼Gt [(cid:104)q∗

t (ρt)  ˆct(cid:105) + R((x  ˆc)1:t  ρt)]]

(cid:20)

(cid:21)

Observe that ˆct is a random variable taking values in D and such that the probability that it is equal
to Ley (for y ∈ {0  . . .   K}) can be upper bounded as

Pr[ˆct = Ley] = Eρt∼Gt [Pr[ˆct = Ley|ρt]] = Eρt∼Gt

qt(ρt)(y)

ct(y)

L · qt(ρt)(y)

≤ 1/L.

max

Thus we can upper bound the latter quantity by the supremum over all distributions in ∆(cid:48)

E(ˆyt Xt) [(cid:104)q∗

t   ˆct(cid:105) + REL(I1:t)] ≤ max
pt∈∆(cid:48)

ct∈[0 1]K
Now we can continue by pushing the expectation over ρt outside of the supremum  i.e. 

t (ρt)  ˆct(cid:105) + R((x  ˆc)1:t  ρt)]].
(cid:21)
t (ρt)  ˆct(cid:105) + R((x  ˆc)1:t  ρt)]
ct∈[0 1]K
and working conditionally on ρt. Since the expression is linear in pt  the supremum is realized  and
by the deﬁnition of q∗

Eˆct∼pt [Eρt∼Gt [(cid:104)q∗
(cid:20)

t (ρt)  the quantity inside the expectation Eρt∼Gt is equal to

t   ˆct(cid:105) + REL(I1:t)] ≤ Eρt∼Gt

E(ˆyt Xt) [(cid:104)q∗

Eˆct∼pt [(cid:104)q∗

max
pt∈∆(cid:48)

D  i.e. 

max

D

D

Eˆct∼pt [(cid:104)q  ˆct(cid:105) + R((x  ˆc)1:t  ρt)].
We can now apply the minimax theorem and upper bound the above by
Eˆct∼pt [(cid:104)q  ˆct(cid:105) + R((x  ˆc)1:t  ρt)].

max
pt∈∆(cid:48)
D

min
q∈∆K

max
pt∈∆(cid:48)
D

min
q∈∆K

Since the inner objective is linear in q  we continue with

max
pt∈∆(cid:48)
D

min

y

Eˆct∼pt [ˆct(y) + R((x  ˆc)1:t  ρt)].

(cid:34)

(cid:32) t(cid:88)

We can now expand the deﬁnition of R(·)

y

min

Eˆct∼pt

max
pt∈∆(cid:48)
D

With the notation Aπ = −(cid:80)t−1

ˆct(y) + max
π∈Π

−

τ =1

τ =1 ˆcτ (π(xτ )) −(cid:80)T
(cid:20)

max
pt∈∆(cid:48)
D

min

y

Eˆct∼pt

ˆct(y) + max
π∈Π

T(cid:88)

τ =t+1

(cid:33)(cid:35)

+ (T − t)K/L.

ˆcτ (π(xτ )) +

2τ (π(xτ ))Zτ

τ =t+1 2τ (π(xτ ))Zτ   we re-write the above as
(Aπ − ˆct(π(xt)))

+ (T − t)K/L.

(cid:21)

We now upper bound the ﬁrst term. The extra term (T − t)K/L will be combined with the extra
K/L that we have abandoned to give the correct term (T − (t − 1))K/L needed for REL(I1:t−1).
Observe that we can re-write the ﬁrst term by using symmetrization as
(Aπ − ˆct(π(xt)))

(cid:21)

(cid:20)

min

Eˆct∼pt

ˆct(y) + max
π∈Π

Eˆc(cid:48)

(cid:21)
(cid:21)
t(y)] − ˆct(π(xt)))
t∼pt [ˆc(cid:48)
t(π(xt))] − ˆct(π(xt)))
(cid:21)
t(π(xt)) − ˆct(π(xt))))

(cid:21)
t∼pt [ˆc(cid:48)
t(π(xt)) − ˆct(π(xt)))

(cid:21)

max
π∈Π

(Aπ + min

y

max
π∈Π

(Aπ + Eˆc(cid:48)

(Aπ + ˆc(cid:48)

max
π∈Π

(cid:20)

(cid:20)

(cid:20)

Eˆct ˆc(cid:48)

t∼pt δ

max
π∈Π

(Aπ + δ (ˆc(cid:48)

(cid:20)
(cid:20)

Eˆct∼pt

Eˆct∼pt

Eˆct ˆc(cid:48)

t∼pt

Eˆct∼pt δ

max
pt∈∆(cid:48)
D

y

= max
pt∈∆(cid:48)
D
≤ max
pt∈∆(cid:48)
≤ max
pt∈∆(cid:48)

D

D

D

= max
pt∈∆(cid:48)
≤ max
pt∈∆(cid:48)

D

max
π∈Π

(Aπ + 2δˆct(π(xt)))

7

(cid:20)

where δ is a random variable which is −1 and 1 with equal probability. The last inequality follows by
splitting the maximum into two equal parts.
Conditioning on ˆct  consider the random variable Mt which is − maxy ˆct(y) or maxy ˆct(y) on the
coordinates where ˆct is equal to zero and equal to ˆct on the coordinate that achieves the maximum.
This is clearly an unbiased estimate of ˆct. Thus we can upper bound the last quantity by

≤ max
pt∈∆(cid:48)

D

(Aπ + 2δE [Mt(π(xt))|ˆct])

Eˆct∼pt δ

D

max
π∈Π

Eˆct δ Mt

max
(Aπ + 2δMt(π(xt)))
pt∈∆(cid:48)
The random vector δMt  conditioning on ˆct  is equal to − maxy ˆct(y) or maxy ˆct(y) with equal
probability independently on each coordinate. Moreover  observe that for any distribution pt ∈ ∆(cid:48)
D 
the distribution of the maximum coordinate of ˆct has support on {0  L} and is equal to L with
probability at most K/L. Since the objective only depends on the distribution of the maximum
coordinate of ˆct  we can continue the upper bound with a maximum over any distribution of random
vectors whose coordinates are 0 with probability at least 1 − K/L and otherwise are −L or L with
equal probability. Speciﬁcally  let t be a Rademacher random vector  we continue with

max
π∈Π

(cid:21)

.

Zt∈∆{0 L}:P r[Zt=L]≤K/L

max

Et Zt

max
π∈Π

(Aπ + 2t(π(xt))Zt)

.

Now observe that if we denote with a = Pr[Zt = L]  the above is equal to

max

a:0≤a≤K/L

(1 − a) max
π∈Π

(Aπ) + aEt

max
π∈Π

(Aπ + 2t(π(xt))L)

.

We now argue that this maximum is achieved by setting a = K/L. For that it sufﬁces to show that

(cid:21)

(cid:20)

(cid:20)

(cid:20)

(cid:21)

(cid:21)

(cid:21)(cid:19)

(cid:18)

(cid:20)

max
π∈Π

(Aπ) ≤ Et
(cid:21)

which is true by observing that with π∗ = argmaxπ∈Π(Aπ) one has

max
π∈Π

(Aπ + 2t(π(xt))L)

 

(cid:20)

Et

max
π∈Π

(Aπ + 2t(π(xt))L)

≥ Et [Aπ∗ + 2t(π∗(xt))L)] = Aπ∗ +Et [2t(π∗(xt))L)] = Aπ∗ .

Thus we can upper bound the quantity we want by

(cid:20)

Et Zt

max
π∈Π

(Aπ + 2t(π(xt))Zt

(cid:21)

 

where t is a Rademacher random vector and Zt is now a random variable which is equal to L with
probability K/L and is equal to 0 with the remaining probability.
Taking expectation over ρt and xt and adding the (T − (t − 1))K/L term that we abandoned  we
arrive at the desired upper bound of REL(I1:t−1). This concludes the proof of admissibility.

Regret bound. By applying Lemma 2 (See Appendix A) with E[Z 2
and invoking Lemma 1  we get the regret bound in Equation (8).

t ] = L2 Pr[Zt = L] = KL

6 Discussion

In this paper  we present a new oracle-based algorithm for adversarial contextual bandits and we prove
that it achieves O((KT )2/3 log(N )1/3) regret in the settings studied by Rakhlin and Sridharan [7].
This is the best regret bound that we are aware of among oracle-based algorithms.
While our bound improves on the O(T 3/4) bounds in prior work [7  8]  achieving the optimal

O((cid:112)T K log(N )) regret bound with an oracle based approach still remains an important open

question. Another interesting avenue for future work involves removing the stochastic assumption on
the contexts.

8

References
[1] Alekh Agarwal  Daniel Hsu  Satyen Kale  John Langford  Lihong Li  and Robert E. Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning (ICML)  2014.

[2] Peter Auer  Nicolo Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. Gambling in a rigged casino: The

adversarial multi-armed bandit pproblem. In Foundations of Computer Science (FOCS)  1995.

[3] Nicolo Cesa-Bianchi  Yoav Freund  David Haussler  David P Helmbold  Robert E Schapire  and Manfred K

Warmuth. How to use expert advice. Journal of the ACM (JACM)  1997.

[4] Miroslav Dudík  Daniel Hsu  Satyen Kale  Nikos Karampatziakis  John Langford  Lev Reyzin  and Tong
Zhang. Efﬁcient optimal learning for contextual bandits. In Uncertainty and Artiﬁcial Intelligence (UAI) 
2011.

[5] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  1997.

[6] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information.

In Advances in Neural Information Processing Systems (NIPS)  2008.

[7] Alexander Rakhlin and Karthik Sridharan. BISTRO: an efﬁcient relaxation-based method for contextual

bandits. In International Conference on Machine Learning (ICML)  2016.

[8] Vasilis Syrgkanis  Akshay Krishnamurthy  and Robert E. Schapire. Efﬁcient algorithms for adversarial

contextual learning. In International Conference on Machine Learning (ICML)  2016.

9

,Hong Wang
Wei Xing
Kaiser Asif
Brian Ziebart
Vasilis Syrgkanis
Haipeng Luo
Akshay Krishnamurthy
Robert Schapire
Debarghya Ghoshdastidar
Ulrike von Luxburg