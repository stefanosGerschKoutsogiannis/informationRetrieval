2019,Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator,We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR)  building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation  and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically  we show that to obtain a controller that is within $\varepsilon$ of the optimal LQR controller  each step of policy evaluation requires at most $(n+d)^3/\varepsilon^2$ samples  where $n$ is the dimension of the state vector and $d$ is the dimension of the input vector. On the other hand  only $\log(1/\varepsilon)$ policy improvement steps suffice  resulting in an overall sample complexity of $(n+d)^3 \varepsilon^{-2} \log(1/\varepsilon)$. We furthermore build on our analysis and construct a simple adaptive procedure based on $\varepsilon$-greedy exploration which relies on approximate PI as a sub-routine and obtains $T^{2/3}$ regret  improving upon a recent result of Abbasi-Yadkori et al. 2019.,Finite-time Analysis of Approximate Policy Iteration

for the Linear Quadratic Regulator

Karl Krauth

University of California  Berkeley

karlk@berkeley.edu

Stephen Tu

University of California  Berkeley

stephentu@berkeley.edu

Benjamin Recht

University of California  Berkeley

brecht@berkeley.edu

Abstract

We study the sample complexity of approximate policy iteration (PI) for the
Linear Quadratic Regulator (LQR)  building on a recent line of work using LQR
as a testbed to understand the limits of reinforcement learning (RL) algorithms
on continuous control tasks. Our analysis quantiﬁes the tension between policy
improvement and policy evaluation  and suggests that policy evaluation is the
dominant factor in terms of sample complexity. Speciﬁcally  we show that to obtain
a controller that is within ε of the optimal LQR controller  each step of policy
evaluation requires at most (n + d)3/ε2 samples  where n is the dimension of
the state vector and d is the dimension of the input vector. On the other hand 
only log(1/ε) policy improvement steps sufﬁce  resulting in an overall sample
complexity of (n + d)3ε−2 log(1/ε). We furthermore build on our analysis and
construct a simple adaptive procedure based on ε-greedy exploration which relies
on approximate PI as a sub-routine and obtains T 2/3 regret  improving upon a
recent result of Abbasi-Yadkori et al. [3].

1

Introduction

With the recent successes of reinforcement learning (RL) on continuous control tasks  there has been
a renewed interest in understanding the sample complexity of RL methods. A recent line of work
has focused on the Linear Quadratic Regulator (LQR) as a testbed to understand the behavior and
trade-offs of various RL algorithms in the continuous state and action space setting. These results
can be broadly grouped into two categories: (1) the study of model-based methods which use data to
build an estimate of the transition dynamics  and (2) model-free methods which directly estimate the
optimal feedback controller from data without building a dynamics model as an intermediate step.
Much of the recent progress in LQR has focused on the model-based side  with an analysis of robust
control from Dean et al. [12] and certainty equivalence control by Fiechter [17] and Mania et al.
[26]. These techniques have also been extended to the online  adaptive setting [1  4  11  13  31]. On
the other hand  for classic model-free RL algorithms such as Q-learning  SARSA  and approximate
policy iteration (PI)  our understanding is much less complete within the context ofd LQR. This is
despite the fact that these algorithms are well understood in the tabular (ﬁnite state and action space)
setting. Indeed  most of the model-free analysis for LQR [16  24  35] has focused exclusively on
derivative-free random search methods.
In this paper  we extend our understanding of model-free algorithms for LQR by studying the
performance of approximate PI on LQR  which is a classic approximate dynamic programming
algorithm. Approximate PI is a model-free algorithm which iteratively uses trajectory data to estimate

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the state-value function associated to the current policy (via e.g. temporal difference learning)  and
then uses this estimate to greedily improve the policy. A key issue in analyzing approximate PI is
to understand the trade-off between the number of policy improvement iterations  and the amount
of data to collect for each policy evaluation phase. Our analysis quantiﬁes this trade-off  showing
that if least-squares temporal difference learning (LSTD-Q) [9  20] is used for policy evaluation 

then a trajectory of length (cid:101)O((n + d)3/ε2) for each inner step of policy evaluation combined with
O(log(1/ε)) outer steps of policy improvement sufﬁces to learn a controller that has ε-error from
the optimal controller. This yields an overall sample complexity of O((n + d)3ε−2 log(1/ε)). Prior
to our work  the only known guarantee for approximate PI on LQR was the asymptotic consistency
result of Bradtke [10] in the setting of no process noise.
We also extend our analysis of approximate PI to the online  adaptive LQR setting popularized by
Abbasi-Yadkori and Szepesvári [1]. By using a greedy exploration scheme similar to Dean et al. [13]

and Mania et al. [26]  we prove a (cid:101)O(T 2/3) regret bound for a simple adaptive policy improvement
[1  11  26]  our analysis improves the (cid:101)O(T 2/3+ε) regret (for T ≥ C 1/ε) from the model-free Follow

algorithm. While the T 2/3 rate is sub-optimal compared to the T 1/2 regret from model-based methods

the Leader (FTL) algorithm of Abbasi-Yadkori et al. [3]. To the best of our knowledge  we give the
best regret guarantee known for a model-free algorithm. We leave open the question of whether or
not a model-free algorithm can achieve optimal T 1/2 regret.

2 Main Results

In this paper  we consider the following linear dynamical system:

xt+1 = Axt + But + wt   wt ∼ N (0  σ2

(2.1)
We let n denote the dimension of the state xt and d denote the dimension of the input ut. For
simplicity we assume that d ≤ n  e.g. the system is under-actuated. We ﬁx two positive deﬁnite cost
matrices (S  R)  and consider the inﬁnite horizon average-cost Linear Quadratic Regulator (LQR):

wI)   x0 ∼ N (0  Σ0) .

(cid:34)

T(cid:88)

t=1

(cid:35)

J(cid:63) := min

{ut(·)} lim
T→∞

E

1
T

xT
t Sxt + uT

t Rut

subject to (2.1) .

(2.2)

We assume the dynamics matrices (A  B) are unknown to us  and our method of interaction with
(2.1) is to choose an input sequence {ut} and observe the resulting states {xt}.
We study the solution to (2.2) using least-squares policy iteration (LSPI)  a well-known approximate
dynamic programming method in RL introduced by Lagoudakis and Parr [20]. The study of approxi-
mate PI on LQR dates back to the Ph.D. thesis of Bradtke [10]  where he showed that for noiseless
LQR (when wt = 0 for all t)  the approximate PI algorithm is asymptotically consistent. In this paper
we expand on this result and quantify non-asymptotic rates for approximate PI on LQR. Proofs of all
results can be found in the extended version of this paper [19].
Notation. For a positive scalar x > 0  we let x+ = max{1  x}. A square matrix L is called stable
if ρ(L) < 1 where ρ(·) denotes the spectral radius of L. For a symmetric matrix M ∈ Rn×n  we
let dlyap(L  M ) denote the unique solution to the discrete Lyapunov equation P = LTP L + M.
We also let svec(M ) ∈ Rn(n+1)/2 denote the vectorized version of the upper triangular part of
M so that (cid:107)M(cid:107)2
F = (cid:104)svec(M )  svec(M )(cid:105). Finally  smat(·) denotes the inverse of svec(·)  so that
smat(svec(M )) = M.

2.1 Least-Squares Temporal Difference Learning (LSTD-Q)

The ﬁrst component towards an understanding of approximate PI is to understand least-squares
temporal difference learning (LSTD-Q) for Q-functions  which is the fundamental building block of
LSPI. Given a deterministic policy Keval which stabilizes (A  B)  the goal of LSTD-Q is to estimate
the parameters of the Q-function associated to Keval. Bellman’s equation for inﬁnite-horizon average
cost MDPs (c.f. Bertsekas [6]) states that the (relative) Q-function associated to a policy π satisﬁes
the following ﬁxed-point equation:

λ + Q(x  u) = c(x  u) + Ex(cid:48)∼p(·|x u)[Q(x(cid:48)  π(x(cid:48)))] .

(2.3)

2

Here  λ ∈ R is a free parameter chosen so that the ﬁxed-point equation holds. LSTD-Q operates
under the linear architecture assumption  which states that the Q-function can be described as
Q(x  u) = qTφ(x  u)  for a known (possibly non-linear) feature map φ(x  u). It is well known that
LQR satisﬁes the linear architecture assumption  since we have:

Q(x  u) = svec(Q)Tsvec

  Q =

(cid:32)(cid:20)x

(cid:21)(cid:20)x

(cid:21)T(cid:33)

u

u

(cid:20)S 0

(cid:21)

0 R

(cid:20)AT

(cid:21)

BT

+

(cid:42)

V [A B]  

(cid:20) I

(cid:21)(cid:20) I

Keval

Keval

(cid:21)T(cid:43)

.

V = dlyap(A + BKeval  S + K T

evalRKeval)   λ =

Q  σ2
w

Here  we slightly abuse notation and let Q denote the Q-function and also the matrix parameterizing
the Q-function. Now suppose that a trajectory {(xt  ut  xt+1)}T
t=1 is collected. Note that LSTD-Q is
an off-policy method (unlike the closely related LSTD estimator for value functions)  and therefore the
inputs ut can come from any sequence that provides sufﬁcient excitation for learning. In particular  it
does not have to come from the policy Keval. In this paper  we will consider inputs of the form:

ut = Kplayxt + ηt   ηt ∼ N (0  σ2

(2.4)
where Kplay is a stabilizing controller for (A  B). Once again we emphasize that Kplay (cid:54)= Keval in
general. Furthermore  the policy under Keval is stochastic while the policy under Kplay is stochastic 
where the injected noise ηt is needed in order to provide sufﬁcient excitation for learning. In order to
describe the LSTD-Q estimator  we deﬁne the following quantities which play a key role throughout
the paper:

ηI)  

φt := φ(xt  ut)   ψt := φ(xt  Kevalxt)  

(cid:21)(cid:20) I

(cid:21)T(cid:33)

f := svec

σ2
w

Keval

Keval

  ct := xT

t Sxt + uT

t Rut .

The LSTD-Q estimator estimates q via:

(cid:33)† T(cid:88)

(cid:32)

(cid:98)q :=

(cid:20) I
(cid:32) T(cid:88)

φt(φt − ψt+1 + f )T

(2.5)
bound on the quality of the estimator(cid:98)q  measured in terms of (cid:107)(cid:98)q − q(cid:107). Before we state our result  we
Here  (·)† denotes the Moore-Penrose pseudo-inverse. Our ﬁrst result establishes a non-asymptotic
introduce a key deﬁnition that we will use extensively.
Deﬁnition 1. Let L be a square matrix. Let τ ≥ 1 and ρ ∈ (0  1). We say that L is (τ  ρ)-stable if

φtct .

t=1

t=1

(cid:107)Lk(cid:107) ≤ τ ρk   k = 0  1  2  ... .

While stability of a matrix is an asymptotic notion  Deﬁnition 1 quantiﬁes the degree of stability
by characterizing the transient response of the powers of a matrix by the parameter τ. It is closely
related to the notion of strong stability from Cohen et al. [11].
With Deﬁnition 1 in place  we state our ﬁrst result for LSTD-Q.
Theorem 2.1. Fix a δ ∈ (0  1). Let policies Kplay and Keval stabilize (A  B)  and assume that both
A + BKplay and A + BKeval are (τ  ρ)-stable. Let the initial state x0 ∼ N (0  Σ0) and consider
the inputs ut = Kplayxt + ηt with ηt ∼ N (0  σ2
ηI). For simplicity  assume that ση ≤ σw. Let P∞
denote the steady-state covariance of the trajectory {xt}:
P∞ = dlyap((A + BKplay)T  σ2

(2.6)

ηBBT) .

wI + σ2

Deﬁne the proxy variance σ2 by:

σ2 := τ 2ρ4(cid:107)Σ0(cid:107) + (cid:107)P∞(cid:107) + σ2

η(cid:107)B(cid:107)2 .

(cid:26)

Suppose that T satisﬁes:

T ≥ (cid:101)O(1) max

(n + d)2 

τ 4

(n + d)4

ρ4(1 − ρ2)2

σ4
η

3

σ2
wσ2(cid:107)Kplay(cid:107)4

+(cid:107)Keval(cid:107)8

+((cid:107)A(cid:107)4 + (cid:107)B(cid:107)4)+

.

(2.7)

(cid:27)

(2.8)

Then we have with probability at least 1 − δ 

(cid:107)(cid:98)q − q(cid:107) ≤ (cid:101)O(1)
Here the (cid:101)O(1) hides polylog(n  τ (cid:107)Σ0(cid:107) (cid:107)P∞(cid:107) (cid:107)Kplay(cid:107)  T /δ  1/ση) factors.

(n + d)
η√T
σ2

+(cid:107)Keval(cid:107)4

ρ2(1 − ρ2)

τ 2

Theorem 2.1 states that:

(2.9)

+((cid:107)A(cid:107)2 + (cid:107)B(cid:107)2)+(cid:107)QKeval(cid:107)F .
(cid:19)

σwσ(cid:107)Kplay(cid:107)2
(cid:18)

(n + d)4 

T ≤ (cid:101)O

(n + d)3

1
σ4
η

ε2

timesteps are sufﬁcient to achieve error (cid:107)(cid:98)q − q(cid:107) ≤ ε w.h.p. Several remarks are in order. First  while

the (n + d)4 burn-in is likely sub-optimal  the (n + d)3/ε2 dependence is sharp as shown by the
asymptotic results of Tu and Recht [35]. Second  the 1/σ4
η dependence on the injected excitation
noise will be important when we study the online  adaptive setting in Section 2.3. We leave improving
the polynomial dependence of the burn-in period to future work.
The proof of Theorem 2.1 rests on top of several recent advances. First  we build off the work of
Abbasi-Yadkori et al. [3] to derive a new basic inequality for LSTD-Q which serves as a starting
point for the analysis. Next  we combine the small-ball techniques of Simchowitz et al. [33] with
the self-normalized martingale inequalities of Abbasi-Yadkori et al. [2]. While an analysis of LSTD-
Q is presented in Abbasi-Yadkori et al. [3] (which builds on the analysis for LSTD from Tu and
Recht [34])  a direct application of their result yields a 1/σ8
η dependence; the use of self-normalized
inequalities is necessary in order to reduce this dependence to 1/σ4
η.

2.2 Least-Squares Policy Iteration (LSPI)

With Theorem 2.1 in place  we are ready to present the main results for LSPI. We describe two
versions of LSPI in Algorithm 1 and Algorithm 2.

Algorithm 1 LSPIv1 for LQR
Input: K0: initial stabilizing controller 
N: number of policy iterations 
T : length of rollout 
σ2
η: exploration variance 
µ: lower eigenvalue bound.
1: Collect D = {(xk  uk  xk+1)}T
(cid:98)Qt = Projµ(LSTDQ(D  Kt)).
put uk = K0xk + ηk  ηk ∼ N (0  σ2
2: for t = 0  ...  N − 1 do
Kt+1 = G((cid:98)Qt). [See (2.10).]
3:
4:
5: end for
6: return KN .

k=1 with in-

ηI).

Algorithm 2 LSPIv2 for LQR
Input: K0: initial stabilizing controller 
N: number of policy iterations 
T : length of rollout 
σ2
η: exploration variance 
µ: lower eigenvalue bound.

1: for t = 0  ...  N − 1 do
Collect Dt = {(x(t)
2:
(cid:98)Qt = Projµ(LSTDQ(D  Kt)).
u(t)
k + η(t)
k = K0x(t)
Kt+1 = G((cid:98)Qt).

k   u(t)
k  η(t)

k+1)}T

k   x(t)
k=1 
k ∼ N (0  σ2

ηI).

3:
4:
5: end for
6: return KN .

In Algorithms 1 and 2  Projµ(·) = arg minX=X T:X(cid:23)µ·I(cid:107)X − ·(cid:107)F is the Euclidean projection
onto the set of symmetric matrices lower bounded by µ · I. Furthermore  the map G(·) takes an
(n + d) × (n + d) positive deﬁnite matrix and returns a d × n matrix:

(cid:18)(cid:20)Q11 Q12

(cid:21)(cid:19)

QT

12 Q22

G

= −Q−1

22 QT

12 .

(2.10)

Algorithm 1 corresponds to the version presented in Lagoudakis and Parr [20]  where all the data
D is collected up front and is re-used in every iteration of LSTD-Q. Algorithm 2 is the one we will
analyze in this paper  where new data is collected for every iteration of LSTD-Q. The modiﬁcation
made in Algorithm 2 simpliﬁes the analysis by allowing the controller Kt to be independent of the
data Dt in LSTD-Q. We remark that this does not require the system to be reset after every iteration
of LSTD-Q. We leave analyzing Algorithm 1 to future work.

4

Before we state our main ﬁnite-sample guarantee for Algorithm 2  we review the notion of a (relative)
value-function. Similarly to (relative) Q-functions  the inﬁnite horizon average-cost Bellman equation
states that the (relative) value function V associated to a policy π satisﬁes the ﬁxed-point equation:

λ + V (x) = c(x  π(x)) + Ex(cid:48)∼p(·|x π(x))[V (x(cid:48))] .

(2.11)

For a stabilizing policy K  it is well known that for LQR the value function V (x) = xTV x with

V = dlyap(A + BK  S + K TRK)   λ = (cid:104)σ2

wI  V (cid:105) .

Once again as we did for Q-functions  we slightly abuse notation and let V denote the value function
and the matrix that parameterizes the value function. Our main result for Algorithm 2 appears in the
following theorem. For simplicity  we will assume that (cid:107)S(cid:107) ≥ 1 and (cid:107)R(cid:107) ≥ 1.
Theorem 2.2. Fix a δ ∈ (0  1). Let the initial policy K0 input to Algorithm 2 stabilize (A  B).
Suppose the initial state x0 ∼ N (0  Σ0) and that the excitation noise satisﬁes ση ≤ σw. Recall that
the steady-state covariance of the trajectory {xt} is

P∞ = dlyap((A + BK0)T  σ2

wI + σ2

ηBBT) .

Let V0 denote the value function associated to the initial policy K0  and V(cid:63) denote the value function
associated to the optimal policy K(cid:63) for the LQR problem (2.2). Deﬁne the variables µ  L as:

µ := min{λmin(S)  λmin(R)}  
L := max{(cid:107)S(cid:107) (cid:107)R(cid:107)} + 2((cid:107)A(cid:107)2 + (cid:107)B(cid:107)2 + 1)(cid:107)V0(cid:107)+ .

Fix an ε > 0 that satisﬁes:

(cid:18) L

(cid:19)2

(cid:26)

ε ≤ 5

µ

min

1 

2 log((cid:107)V0(cid:107)/λmin(V(cid:63)))

 

e

(cid:107)V(cid:63)(cid:107)2

Suppose we run Algorithm 2 for N := N0 + 1 policy improvement iterations where

(cid:24)

8µ2 log((cid:107)V0(cid:107)/λmin(V(cid:63)))

(cid:18) 2 log((cid:107)V0(cid:107)/λmin(V(cid:63)))

(cid:19)(cid:25)

ε

N0 :=

(1 + L/µ) log

 

(2.13)

(cid:27)

.

(2.12)

and we set the rollout length T to satisfy:

(cid:26)

T ≥ (cid:101)O(1) max

(n + d)2 

L2

(1 − µ/L)2
L4
1
ε2

(1 − µ/L)2

(cid:19)17 (n + d)4
(cid:18) L
(cid:19)42 (n + d)3
(cid:18) L

σ4
η

µ

µ

σ4
η

σ2
w((cid:107)Σ0(cid:107) + (cid:107)P∞(cid:107) + σ2

η(cid:107)B(cid:107)2) 

(cid:27)
η(cid:107)B(cid:107)2)

σ2
w((cid:107)Σ0(cid:107) + (cid:107)P∞(cid:107) + σ2

.

(2.14)

Then with probability 1 − δ  we have that each policy Kt for t = 1  ...  N stabilizes (A  B) and
furthermore:

(cid:107)KN − K(cid:63)(cid:107) ≤ ε .

Here the (cid:101)O(1) hides polylog(n  τ (cid:107)Σ0(cid:107) (cid:107)P∞(cid:107)  L/µ  T /δ  N0  1/ση) factors.
Theorem 2.2 states roughly that T · N ≤ (cid:101)O( (n+d)3

log(1/ε)) samples are sufﬁcient for LSPI to
recover a controller K that is within ε of the optimal K(cid:63). That is  only log(1/ε) iterations of policy
improvement are necessary  and furthermore more iterations of policy improvement do not necessary
help due to the inherent statistical noise of estimating the Q-function for every policy Kt. We
note that the polynomial factor in L/µ is by no means optimal and was deliberately made quite
conservative in order to simplify the presentation of the bound. A sharper bound can be recovered
from our analysis techniques at the expense of a less concise expression.
It is worth taking a moment to compare Theorem 2.2 to classical results in the RL literature regarding
approximate policy iteration. For example  a well known result (c.f. Theorem 7.1 of Lagoudakis and

ε2

5

Parr [20]) states that if LSTD-Q is able to return Q-function estimates with error L∞ bounded by ε

at every iteration  then letting (cid:98)Qt denote the approximate Q-function at the t-th iteration of LSPI:

t→∞ (cid:107)(cid:98)Qt − Q(cid:63)(cid:107)∞ ≤

lim sup

2γε

(1 − γ)2 .

(2.15)

Here  γ is the discount factor of the MDP. Theorem 2.2 is qualitatively similar to this result in that
we show roughly that ε error in the Q-function estimate translates to ε error in the estimated policy.
However  there are several fundamental differences. First  our analysis does not rely on discounting
to show contraction of the Bellman operator. Instead  we use the (τ  ρ)-stability of closed loop system
to achieve this effect. Second  our analysis does not rely on L∞ bounds on the estimated Q-function 
although we remark that similar type of results to (2.15) exist also in Lp (see e.g. [27  29]). Finally 
our analysis is non-asymptotic.
The proof of Theorem 2.2 combines the estimation guarantee of Theorem 2.1 with a new analysis
of policy iteration for LQR  which we believe is of independent interest. Our new policy iteration
analysis combines the work of Bertsekas [7] on policy iteration in inﬁnite horizon average cost MDPs
with the contraction theory of Lee and Lim [22] for non-linear matrix equations.

2.3 LSPI for Adaptive LQR

T(cid:88)

t=1

We now turn our attention to the online  adaptive LQR problem as studied in Abbasi-Yadkori and
Szepesvári [1]. In the adaptive LQR problem  the quantity of interest is the regret  deﬁned as:

Regret(T ) :=

xT
t Sxt + uT

t Rut − T · J(cid:63) .

(2.16)

Here  the algorithm is penalized for the cost incurred from learning the optimal policy K(cid:63)  and
must balance exploration (to better learn the optimal policy) versus exploitation (to reduce cost). As

mentioned previously  there are several known algorithms which achieve (cid:101)O(√T ) regret [1  4  11 
section to give an adaptive model-free algorithm for LQR which achieves (cid:101)O(T 2/3) regret  which
improves upon the (cid:101)O(T 2/3+ε) regret (for T ≥ C 1/ε) achieved by the adaptive model-free algorithm

26  31]. However  these algorithms operate in a model-based manner  using the collected data to
build a conﬁdence interval around the true dynamics (A  B). On the other hand  the performance of
adaptive algorithms which are model-free is less well understood. We use the results of the previous

of Abbasi-Yadkori et al. [3]. Our adaptive algorithm based on LSPI is shown in Algorithm 3.

2i

η i = σ2
w

value bound µ.

(cid:1)1/3.

Algorithm 3 Online Adaptive Model-free LQR Algorithm
Input: Initial stabilizing controller K (0)  number of epochs E  epoch multiplier Tmult  lower eigen-
(cid:0) 1
1: for i = 0  ...  E − 1 do
Set Ti = Tmult2i.
2:
Set σ2
3:
4:
5: end for

Set K (i+1) = LSPIv2(K0=K (i)  N =(cid:101)O((i + 1)Γ(cid:63)/µ)  T =Ti  σ2

Using an analysis technique similar to that in Dean et al. [13]  we prove the following (cid:101)O(T 2/3) regret
bound for Algorithm 3.
Theorem 2.3. Fix a δ ∈ (0  1). Let the initial feedback K (0) stabilize (A  B) and let V (0) denote
its associated value function. Also let K(cid:63) denote the optimal LQR controller and let V(cid:63) denote
the optimal value function. Let Γ(cid:63) = 1 + max{(cid:107)A(cid:107) (cid:107)B(cid:107) (cid:107)V (0)(cid:107) (cid:107)V(cid:63)(cid:107) (cid:107)K (0)(cid:107) (cid:107)K(cid:63)(cid:107) (cid:107)Q(cid:107) (cid:107)R(cid:107)}.
Suppose that Tmult is set to:

η=σ2

η i).

and suppose µ is set to µ = min{λmin(S)  λmin(R)}. With probability at least 1 − δ  we have that
the regret of Algorithm 3 satisﬁes:

Tmult ≥ (cid:101)O(1)poly(Γ(cid:63)  n  d  1/λmin(S)) .
Regret(T ) ≤ (cid:101)O(1)poly(Γ(cid:63)  n  d  1/λmin(S))T 2/3 .

6

We note that the regret scaling as T 2/3 in Theorem 2.3 is due to the 1/σ4
η dependence from LSTD-Q
(c.f. (2.9)). As mentioned previously  the existing LSTD-Q analysis from Abbasi-Yadkori et al. [3]
yields a 1/σ8
η dependence in the analysis of Algorithm 3
would translate into T 4/5 regret.

η dependence in LSTD-Q; using this 1/σ8

3 Related Work

For model-based methods  in the ofﬂine setting Fiechter [17] provided the ﬁrst PAC-learning bound
for inﬁnite horizon discounted LQR using certainty equivalence (nominal) control. Later  Dean et al.
[12] use tools from robust control to analyze a robust synthesis method for inﬁnite horizon average
cost LQR  which is applicable in regimes of moderate uncertainty when nominal control fails. Mania
the online adaptive setting  [1  4  11  18  26] give (cid:101)O(√T ) regret algorithms. A key component of
et al. [26] show that certainty equivalence control actually provides a fast O(ε2) rate of sub-optimality
where ε is the size of the parameter error  unlike the O(ε) sub-optimality guarantee of [12  17]. For
model-based algorithms is being able to quantify a conﬁdence interval for the parameter estimate  for
which several recent works [14  32  33] provide non-asymptotic results.
Turning to model-free methods  Tu and Recht [34] study the behavior of least-squares temporal
difference (LSTD) for learning the discounted value function associated to a stabilizing policy. They
evaluate the LSPI algorithm studied in this paper empirically  but do not provide any analysis. In
terms of policy optimization  most of the work has focused on derivative-free random search methods
[16  24]. Tu and Recht [35] study a special family of LQR instances and characterize the asymptotic
behavior of both model-based certainty equivalent control versus policy gradients (REINFORCE) 
showing that policy gradients has polynomially worse sample complexity. Most related to our work
is Abbasi-Yadkori et al. [3]  who analyze a model-free algorithm for adaptive LQR based on ideas
from online convex optimization. LSTD-Q is a sub-routine of their algorithm  and their analysis
incurs a sub-optimal 1/σ8
η dependence on the injected exploration noise  which we improve to 1/σ4
η
using self-normalized martingale inequalities [2]. This improvement allows us to use a simple greedy
exploration strategy to obtain T 2/3 regret. Finally  as mentioned earlier  the Ph.D. thesis of Bradtke
[10] presents an asymptotic consistency argument for approximate PI for discounted LQR in the
noiseless setting (i.e. wt = 0 for all t).
For the general function approximation setting in RL  Antos et al. [5] and Lazaric et al. [21] analyze
variants of LSPI for discounted MDPs where the state space is compact and the action space ﬁnite. In
Lazaric et al. [21]  the policy is greedily updated via an update operator that requires access to the
underlying dynamics (and is therefore not implementable). Farahmand et al. [15] extend the results
of Lazaric et al. [21] to when the function spaces considered are reproducing kernel Hilbert spaces.
Zou et al. [37] give a ﬁnite-time analysis of both Q-learning and SARSA  combining the asymptotic
analysis of Melo et al. [28] with the ﬁnite-time analysis of TD-learning from Bhandari et al. [8].
We note that checking the required assumptions to apply the results of Zou et al. [37] is non-trivial
(c.f. Section 3.1  [28]). We are un-aware of any non-asymptotic analysis of LSPI in the average cost
setting  which is more difﬁcult as the Bellman operator is no longer a contraction.
Finally  we remark that our LSPI analysis relies on understanding exact policy iteration for LQR 
which is closely related to the ﬁxed-point Riccati recurrence (value iteration). An elegant analysis for
value iteration is given by Lincoln and Rantzer [23]. Recently  Fazel et al. [16] show that exact policy
iteration is a special case of Gauss-Newton and prove linear convergence results. Our analysis  on the
other hand  is based on combining the ﬁxed-point theory from Lee and Lim [22] with recent work on
policy iteration for average cost problems from Bertsekas [7].

4 Experiments

We ﬁrst look at the performance of LSPI in the non-adaptive setting (Section 2.2). Here  we compare
LSPI to other popular model-free methods  and the model-based certainty equivalence (nominal)
controller (c.f. [26]). For model-free  we look at policy gradients (REINFORCE) (c.f. [36]) and
derivative-free optimization (c.f. [24  25  30]). A full description of the methods we compare to is
given in the full paper [19].

7

(cid:34)0.95

0.01

0

0.01
0.95
0.01

(cid:35)

0

(cid:34)1

  B =

0.01
0.95

0
0

(cid:35)

  S = I3 

0.1
0.1
0.1

We consider the LQR instance (A  B  S  R) with A =

and R = I2. We choose an LQR problem where the A matrix is stable  since the model-free methods
we consider need to be seeded with an initial stabilizing controller; using a stable A allows us to start
at K0 = 02×3. We ﬁx the process noise σw = 1. The model-based nominal method learns (A  B)
using least-squares  exciting the system with Gaussian inputs ut with variance σu = 1.

(a) Ofﬂine Evaluation

(b) Adaptive Evaluation

Figure 1: The performance of various model-free methods compared with the nominal controller. (a) Plot of
non-adaptive performance. The shaded regions represent the lower 10th and upper 90th percentile over 100
trials  and the solid line represents the median performance. Here  PG (simple) is policy gradients with the
simple baseline  PG (vf) is policy gradients with the value function baseline  LSPIv2 is Algorithm 2  LSPIv1 is
Algorithm 1  and DFO is derivative-free optimization. (b) Plot of adaptive performance. The shaded regions
represent the median to upper 90th percentile over 100 trials. Here  LSPI is Algorithm 3 using LSPIv1  MFLQ
is from Abbasi-Yadkori et al. [3]  nominal is the ε-greedy adaptive certainty equivalent controller (c.f. [13])  and
optimal has access to the true dynamics.

For policy gradients and derivative-free optimization  we use the projected stochastic gradient
descent (SGD) method with a constant step size µ as the optimization procedure. For policy
iteration  we evaluate both LSPIv1 (Algorithm 1) and LSPIv2 (Algorithm 2). For every iteration of
LSTD-Q  we project the resulting Q-function parameter matrix onto the set {Q : Q (cid:23) γI} with
γ = min{λmin(S)  λmin(R)}. For LSPIv1  we choose N = 15 by picking the N ∈ [5  10  15] which
results in the best performance after T = 106 timesteps. For LSPIv2  we set (N  T ) = (3  333333)
which yields the lowest cost over the grid N ∈ [1  2  3  4  5  6  7] and T such that N T = 106.
Next  we compare the performance of LSPI in the adaptive setting (Section 2.3). We compare
LSPI against the model-free linear quadratic control (MFLQ) algorithm of Abbasi-Yadkori et al. [3] 
nominal certainty equivalence controller (c.f. [13])  and the optimal controller. We use the example

of Dean et al. [12]  with A =

  B = I  S = 10I3  R = I3  and σw = 1.

Figure 1 shows the results of these experiments. In Figure 1a  we plot the relative error (J((cid:98)K) −

J(cid:63))/J(cid:63) versus the number of timesteps. We see that the model-based certainty equivalence (nominal)
method is more sample efﬁcient than the other model-free methods considered. We also see that the
value function baseline is able to dramatically reduce the variance of the policy gradient estimator
compared to the simple baseline. The DFO method performs the best out of all the model-free
methods considered on this example after 106 timesteps  although the performance of policy iteration
is comparable. In Figure 1b  we plot the regret (c.f. Equation 2.16). We see that LSPI and MFLQ
both perform similarly with MFLQ slightly outperforming LSPI. We also note that the model-based
nominal methods performs signiﬁcantly better than both LSPI and MFLQ.

(cid:34)1.01 0.01

0.01 1.01
0.01

0

(cid:35)

0

0.01
1.01

8

0200 000400 000600 000800 0001 000 000timesteps10−810−610−410−2100relativeerrorPG(simple)PG(vf)LSPIv2LSPIv1DFOnominal0200040006000800010000timesteps025005000750010000125001500017500regretoptimalnominalLSPIMFLQ5 Conclusion

We studied the sample complexity of approximate PI on LQR  showing that roughly (n +
d)3ε−2 log(1/ε) samples are sufﬁcient to estimate a controller that is within ε of the optimal. We
also show how to turn this ofﬂine method into an adaptive LQR method with T 2/3 regret. Several
questions remain open with our work. The ﬁrst is if policy iteration is able to achieve T 1/2 regret 
which is possible with other model-based methods. The second is whether or not model-free methods
provide advantages in situations of partial observability for LQ control. Finally  an asymptotic
analysis of LSPI  in the spirit of Tu and Recht [35]  is of interest in order to clarify which parts of our
analysis are sub-optimal due to the techniques we use versus are inherent in the algorithm.

Acknowledgments

We thank the anonymous reviewers for their valuable feedback. We also thank the authors of Abbasi-
Yadkori et al. [3] for providing us with an implementation of their model-free LQ algorithm. ST
is supported by a Google PhD fellowship. This work is generously supported in part by ONR
awards N00014-17-1-2191  N00014-17-1-2401  and N00014-18-1-2833  the DARPA Assured Auton-
omy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs  a Siemens Futuremakers
Fellowship  and an Amazon AWS AI Research Award.

References
[1] Y. Abbasi-Yadkori and C. Szepesvári. Regret Bounds for the Adaptive Control of Linear

Quadratic Systems. In Conference on Learning Theory  2011.

[2] Y. Abbasi-Yadkori  D. Pál  and C. Szepesvári. Online Least Squares Estimation with Self-
Normalized Processes: An Application to Bandit Problems. In Conference on Learning Theory 
2011.

[3] Y. Abbasi-Yadkori  N. Lazi´c  and C. Szepesvári. Model-Free Linear Quadratic Control via

Reduction to Expert Prediction. In AISTATS  2019.

[4] M. Abeille and A. Lazaric.

Improved Regret Bounds for Thompson Sampling in Linear

Quadratic Control Problems. In International Conference on Machine Learning  2018.

[5] A. Antos  C. Szepesvári  and R. Munos. Learning near-optimal policies with Bellman-residual
minimization based ﬁtted policy iteration and a single sample path. Machine Learning  71(1):
89–129  2008.

[6] D. P. Bertsekas. Dynamic Programming and Optimal Control  Vol. II. 2007.
[7] D. P. Bertsekas. Value and Policy Iterations in Optimal Control and Adaptive Dynamic Pro-
gramming. IEEE Transactions on Neural Networks and Learning Systems  28(3):500–509 
2017.

[8] J. Bhandari  D. Russo  and R. Singal. A Finite Time Analysis of Temporal Difference Learning

With Linear Function Approximation. In Conference on Learning Theory  2018.

[9] J. Boyan. Least-Squares Temporal Difference Learning.

Machine Learning  1999.

In International Conference on

[10] S. J. Bradtke. Incremental Dynamic Programming for On-Line Adaptive Optimal Control. PhD

thesis  University of Massachusetts Amherst  1994.

[11] A. Cohen  T. Koren  and Y. Mansour. Learning Linear-Quadratic Regulators Efﬁciently with

only √T Regret. arXiv:1902.06223  2019.

[12] S. Dean  H. Mania  N. Matni  B. Recht  and S. Tu. On the Sample Complexity of the Linear

Quadratic Regulator. arXiv:1710.01688  2017.

[13] S. Dean  H. Mania  N. Matni  B. Recht  and S. Tu. Regret Bounds for Robust Adaptive Control

of the Linear Quadratic Regulator. In Neural Information Processing Systems  2018.

9

[14] M. K. S. Faradonbeh  A. Tewari  and G. Michailidis. Finite Time Identiﬁcation in Unstable

Linear Systems. Automatica  96:342–353  2018.

[15] A. Farahmand  M. Ghavamzadeh  C. Szepesvári  and S. Mannor. Regularized Policy Iteration
with Nonparametric Function Spaces. Journal of Machine Learning Research  17(139):1–66 
2016.

[16] M. Fazel  R. Ge  S. Kakade  and M. Mesbahi. Global Convergence of Policy Gradient Methods
for the Linear Quadratic Regulator. In International Conference on Machine Learning  2018.
[17] C.-N. Fiechter. PAC Adaptive Control of Linear Systems. In Conference on Learning Theory 

1997.

[18] M. Ibrahimi  A. Javanmard  and B. V. Roy. Efﬁcient Reinforcement Learning for High Dimen-

sional Linear Quadratic Systems. In Neural Information Processing Systems  2012.

[19] K. Krauth  S. Tu  and B. Recht. Finite-time Analysis of Approximate Policy Iteration for the

Linear Quadratic Regulator. arXiv:1905.12842  2019.

[20] M. G. Lagoudakis and R. Parr. Least-Squares Policy Iteration. Journal of Machine Learning

Research  4:1107–1149  2003.

[21] A. Lazaric  M. Ghavamzadeh  and R. Munos. Finite-Sample Analysis of Least-Squares Policy

Iteration. Journal of Machine Learning Research  13:3041–3074  2012.

[22] H. Lee and Y. Lim. Invariant metrics  contractions and nonlinear matrix equations. Nonlinearity 

21(4):857–878  2008.

[23] B. Lincoln and A. Rantzer. Relaxed Dynamic Programming. IEEE Transactions on Automatic

Control  51(8):1249–1260  2006.

[24] D. Malik  K. Bhatia  K. Khamaru  P. L. Bartlett    and M. J. Wainwright. Derivative-Free
Methods for Policy Optimization: Guarantees for Linear Quadratic Systems. In AISTATS  2019.

[25] H. Mania  A. Guy  and B. Recht. Simple random search provides a competitive approach to

reinforcement learning. In Neural Information Processing Systems  2018.

[26] H. Mania  S. Tu  and B. Recht. Certainty Equivalent Control of LQR is Efﬁcient.

arXiv:1902.07826  2019.

[27] A. massoud Farahmand  R. Munos  and C. Szepesvári. Error Propagation for Approximate

Policy and Value Iteration. In Neural Information Processing Systems  2010.

[28] F. S. Melo  S. P. Meyn  and M. I. Ribeiro. An Analysis of Reinforcement Learning with Function

Approximation. In International Conference on Machine Learning  2008.

[29] R. Munos. Error Bounds for Approximate Policy Iteration. In International Conference on

Machine Learning  2003.

[30] Y. Nesterov and V. Spokoiny. Random Gradient-Free Minimization of Convex Functions.

Foundations of Computational Mathematics  17(2):527–566  2017.

[31] Y. Ouyang  M. Gagrani  and R. Jain. Control of unknown linear systems with Thompson
sampling. In 55th Annual Allerton Conference on Communication  Control  and Computing 
2017.

[32] T. Sarkar and A. Rakhlin. Near optimal ﬁnite time identiﬁcation of arbitrary linear dynamical

systems. In International Conference on Machine Learning  2019.

[33] M. Simchowitz  H. Mania  S. Tu  M. I. Jordan  and B. Recht. Learning Without Mixing:
Towards A Sharp Analysis of Linear System Identiﬁcation. In Conference on Learning Theory 
2018.

[34] S. Tu and B. Recht. Least-Squares Temporal Difference Learning for the Linear Quadratic

Regulator. In International Conference on Machine Learning  2018.

10

[35] S. Tu and B. Recht. The Gap Between Model-Based and Model-Free Methods on the Linear

Quadratic Regulator: An Asymptotic Viewpoint. In Conference on Learning Theory  2019.

[36] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning  8(3–4):229–246  1992.

[37] S. Zou  T. Xu  and Y. Liang. Finite-Sample Analysis for SARSA and Q-Learning with Linear

Function Approximation. arXiv:1902.02234  2019.

11

,Karl Krauth
Stephen Tu
Benjamin Recht