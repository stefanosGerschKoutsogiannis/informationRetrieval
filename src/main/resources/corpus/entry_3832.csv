2012,Nonparametric Reduced Rank Regression,We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models.  An additive model is estimated for each dimension of a $q$-dimensional response  with a shared $p$-dimensional predictor variable.  To control the complexity of the model  we employ a functional form of the Ky-Fan or nuclear norm  resulting in a set of function estimates that have low rank.  Backfitting algorithms are derived and justified using a nonparametric form of the nuclear norm subdifferential.  Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting.  The methods are illustrated on gene expression data.,Nonparametric Reduced Rank Regression

Rina Foygel† ∗  Michael Horrell†  Mathias Drton† ‡  John Lafferty†

∗Department of Statistics

Stanford University

†Department of Statistics
University of Chicago

‡Department of Statistics
University of Washington

Abstract

We propose an approach to multivariate nonparametric regression that generalizes
reduced rank regression for linear models. An additive model is estimated for each
dimension of a q-dimensional response  with a shared p-dimensional predictor
variable. To control the complexity of the model  we employ a functional form of
the Ky-Fan or nuclear norm  resulting in a set of function estimates that have low
rank. Backﬁtting algorithms are derived and justiﬁed using a nonparametric form
of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived
that exhibit the scaling behavior of the procedure in the high dimensional setting.
The methods are illustrated on gene expression data.

1

Introduction

dimensional covariate vector. This is also referred to as multi-task learning in the machine learning

In the multivariate regression problem the objective is to estimate the conditional mean E(Y∣ X)=
m(X) = (m1(X)  . . .   mq(X))⊺ where Y is a q-dimensional response vector and X is a p-
literature. We are given a sample of n iid pairs{(Xi  Yi)} from the joint distribution of X and Y .
Under a linear model  the mean is estimated as m(X)= BX where B ∈ R
q×p is a q× p matrix
In reduced rank regression the matrix B is estimated under a rank constraint r= rank(B)≤ C  so

of regression coefﬁcients. When the dimensions p and q are large relative to the sample size n  the
coefﬁcients of B cannot be reliably estimated  without further assumptions.

p. Intuitively  this implies
that the rows or columns of B lie in an r-dimensional subspace of R
that the model is based on a smaller number of features than the ambient dimensionality p would
suggest  or that the tasks representing the components Y k of the response are closely related. In low
dimensions  the constrained rank model can be computed as an orthogonal projection of the least
squares solution; but in high dimensions this is not well deﬁned.

The nuclear norm∥B∥∗  also known as the trace or Ky-Fan norm  is the sum of the singular vectors

Recent research has studied the use of the nuclear norm as a convex surrogate for the rank constraint.

q or R

In this paper we study nonparametric parallels of reduced rank linear models. We focus our attention

of B. A rank constraint can be thought of as imposing sparsity  but in an unknown basis; the nuclear
norm plays the role of the (cid:1)1 norm in sparse estimation. Its use for low rank estimation problems
was proposed by Fazel in [2]. More recently  nuclear norm regularization in multivariate linear
regression has been studied by Yuan et al. [10]  and by Neghaban and Wainwright [4]  who analyzed
the scaling properties of the procedure in high dimensions.

on additive models  so that the regression function m(X)=(m1(X)  . . .   mq(X))⊺ has each com-
ponent mk(X)=∑p
j(Xj) equal to a sum of p functions  one for each covariate. The objective
is then to estimate the q× p matrix of functions M(X)=[mk
penalty∥B∥∗ in the linear model. Because we must estimate a matrix of functions  the analogue of

The ﬁrst problem we address  in Section 2  is to determine a replacement for the regularization

j(Xj)].

j=1 mk

the nuclear norm is not immediately apparent. We propose two related regularization penalties for

1

nonparametric low rank regression  and show how they specialize to the linear case. We then study 
in Section 4  the (inﬁnite dimensional) subdifferential of these penalties. In the population setting 
this leads to stationary conditions for the minimizer of the regularized mean squared error. This
subdifferential calculus then justiﬁes penalized backﬁtting algorithms for carrying out the optimiza-
tion for a ﬁnite sample. Constrained rank additive models (CRAM) for multivariate regression are
analogous to sparse additive models (SPAM) for the case where the response is 1-dimensional [6]
(studied also in the reproducing kernel Hilbert space setting by [5])  but with the goal of recovering
a low-rank matrix rather than an entry-wise sparse vector. The backﬁtting algorithms we derive in
Section 5 are analogous to the iterative smoothing and soft thresholding backﬁtting algorithms for
SPAM proposed in [6]. A uniform bound on the excess risk of the estimator relative to an oracle
is given Section 6. This shows the statistical scaling behavior of the methods for prediction. The
analysis requires a concentration result for nonparametric covariance matrices in the spectral norm.
Experiments with gene data are given in Section 7  which are used to illustrate different facets of the
proposed nonparametric reduced rank regression techniques.

2 Nonparametric Nuclear Norm Penalization

s=1

a linear combination of r of the other functions.

s=1 σs(M)=∑q

does it mean for this collection of functions to be low rank? Let x1  x2  . . .   xn be a collection

In the multivariate regression setting  but still assuming the domain is one-dimensional for simplicity

We begin by presenting the penalty that we will use to induce nonparametric regression esti-
mates to be low rank. To motivate our choice of penalty and provide some intuition  suppose

that f 1(x)  . . .   f q(x) are q smooth one-dimensional functions with a common domain. What
of points in the common domain of the functions. We require that the n× q matrix of function values
F(x1∶n)=[f k(xi)] is low rank. This matrix is of rank at most r< q for every set{xi} of arbitrary
size n if and only if the functions{f k} are r-linearly independent—each function can be written as
(q > 1 and p = 1)  we have a random sample X1  . . .   Xn. Consider the n× q sample matrix
M=[mk(Xi)] associated with a vector M =(m1  . . .   mq) of q smooth (regression) functions 
and suppose that n> q. We would like for this to be a low rank matrix. This suggests the penalty
√
∥M∥∗=∑q
λs(M⊺M)  where{λs(A)} denotes the eigenvalues of a symmetric
matrix A and{σs(B)} denotes the singular values of a matrix B. Now  assuming the columns of M
M as the sample covariancêΣ(M)
are centered  and E[mk(X)]= 0 for each k  we recognize 1
of the population covariance Σ(M)∶= Cov(M(X))=[E(mk(X)ml(X))]. This motivates the
∥Σ(M)1/2∥∗=∥ Cov(M(X))1/2∥∗
∥̂Σ(M)1/2∥∗= 1√
∥M∥∗.
E∥Y − M(X)∥2
2+ λ∥Σ(M)1/2∥∗
F+ λ√
∥Y− M∥2
∥M∥∗.
We recall that if A⪰ 0 has spectral decomposition A= U DU ⊺ then A1/2= U D1/2U ⊺.

With Y denoting the n× q matrix of response values for the sample(Xi  Yi)  this leads to the fol-

following sample and population penalties  where A1/2 denotes the matrix square root:

lowing population and empirical regularized risk functionals for low rank nonparametric regression:

population penalized risk:

population penalty:

sample penalty:

(2.1)

(2.2)

(2.3)

(2.4)

⊺

M

n

n

empirical penalized risk:

n

1
2
1
2n

3 Constrained Rank Additive Models (CRAM)

We now consider the case where X is p-dimensional. Throughout the paper we use superscripts to
denote indices of the q-dimensional response  and subscripts to denote indices of the p-dimensional

covariate. We consider the family of additive models  with regression functions of the form m(X)=
(m1(X)  . . .   mq(X))⊺=∑p
j(Xj))⊺ is

j=1 Mj(Xj)  where each term Mj(Xj)=(m1

j(Xj)  . . .   m

q

a q-vector of functions evaluated at Xj.

2

q

∗

∗

∗

∗

n

∗

∗

n

j=1

(3.1)

(3.2)

1  mk

p∑

p)⊺

= 1√

∥Mj∥∗.

to be low rank. This penalty is given by

In this setting we propose two different penalties. The ﬁrst penalty  intuitively  encourages the

all have mean zero; this is required for identiﬁability in the additive model. As a shorthand  let

The second penalty  intuitively  encourages the set of q vector-valued functions(mk

vector(m1
j(Xj)
j(Xj)) to be low rank  for each j. Assume that the functions mk
j(Xj)  . . .   m
Σj= Σ(Mj)= Cov(Mj(Xj)) denote the covariance matrix of the j-th component functions  with
sample version̂Σj. The population and sample versions of the ﬁrst penalty are then given by
∥Σ1/2
1 ∥
∥̂Σ1/2
1 ∥

+⋯+∥Σ1/2
+∥Σ1/2
2 ∥
p ∥
+∥̂Σ1/2
+⋯+∥̂Σ1/2
2 ∥
p ∥
∥(Σ1/2
p )∥
1 ⋯Σ1/2
∥(̂Σ1/2
1 ⋯̂Σ1/2
p )∥
= 1√
∥M1∶p∥∗
1⋯M
where  for convenience of notation  M1∶p =(M
p)⊺
is an np× q matrix. The corresponding
∥Σ1/2
j ∥
E∥Y − p∑
+ λ
Mj(X)∥2
p∑
+ λ√
∥Y− p∑
∥Mj∥∗
Mj∥2
p∑
j)= 1. In the linear case we have Mj(Xj)=
Now suppose that each Xj is normalized so that E(X 2
q. Let B =(B1⋯Bp)∈ R
XjBj where Bj ∈ R
p ∥∗=∥B∥∗
j=1∥Σ1/2
the penalties reduce to∑p
the group lasso [11]. The second penalty reduces to the nuclear norm regularization∥B∥∗ used for

for the second. Thus  in the linear case the ﬁrst penalty is encouraging B to be column-wise sparse 
so that many of the Bjs are zero  meaning that Xj doesn’t appear in the ﬁt. This is a version of

j=1∥Bj∥2 for the ﬁrst penalty  and∥Σ1/2

population and empirical risk functionals  for the ﬁrst penalty  are then

q×p. Some straightforward calculation shows that

j ∥∗=∑p

1 ⋯Σ1/2

and similarly for the second penalty.

1
2
1
2n

2  . . .   mk

(3.3)

(3.5)

n

j=1

(3.6)

(3.4)

∗

∗

⊺

⊺

2

j=1

j=1

F

j=1

∗

high-dimensional reduced-rank regression.

4 Subdifferentials for Functional Matrix Norms

tials of the penalties. We are interested in(q× p)-dimensional matrices of functions F =[f k

A key to deriving algorithms for functional low-rank regression is computation of the subdifferen-

j]. For

⊺

j=1

j=1

k=1

j gk

⊺))  

E(F
E(F F ⊺)∥

j Gj)= tr(E(F G

each column index j and row index k  f k
j is a function of a random variable Xj  and we will take
expectations with respect to Xj implicitly. We write Fj to mean the jth column of F   which is a
q-vector of functions of Xj. We deﬁne the inner product between two matrices of functions as

⟪F  G⟫∶= p∑
j)= p∑
E(f k
q∑
and write∥F∥2=√⟪F  F⟫. Note that∥F∥2=∥√
is a positive semideﬁnite q× q matrix.
∣∣∣F∣∣∣sp∶=√∥E(F F ⊺)∥sp=∥√
E(F F ⊺)∥
where∥A∥sp is the spectral norm (operator norm)  the largest singular value of A  and it is convenient
√
A = A1/2. Each of the norms depends on F only through
E(F F ⊺). In fact  these two norms are dual—for any F  
∣∣∣F∣∣∣∗= sup

where E(F F ⊺)=∑j E(FjF ⊺
j)⪰ 0
∣∣∣F∣∣∣∗∶=∥√

We deﬁne two further norms on a matrix of functions F   namely 

E(F F ⊺)∥∗ 

to write the matrix square root as

⟪G  F⟫  

(4.1)

(4.2)

and

sp

F

∣∣∣G∣∣∣sp≤1

3

to show

(4.3)

(4.4)

pseudo-inverse.

F   with A−1 denoting the matrix

E(F F ⊺))−1

F+ H ∶

E(F F ⊺))−1

Expanding the right-hand side of (4.4)  we have

setting (matrices of functions) to the ordinary matrix case; see [9  7]. Here  we show the reverse

∣∣∣F+ G∣∣∣∗≥∣∣∣F∣∣∣∗+⟪G  D⟫  

where the supremum is attained by setting G=(√
Proposition 4.1. The subdifferential of∣∣∣F∣∣∣∗ is the set
S(F)∶={(√
∣∣∣H∣∣∣sp≤ 1  E(F H ⊺)= 0q×q  E(F F ⊺)H= 0q×p a.e.} .
Proof. The fact thatS(F) contains the subdifferential ∂∣∣∣F∣∣∣∗ can be proved by comparing our
inclusion S(F)⊆ ∂∣∣∣F∣∣∣∗. Let D∈S(F) and let G be any element of the function space. We need
where D=(√
F+ H =∶ ̃F+ H for some H satisfying the conditions in (4.3) above.
E(F F ⊺))−1
∣∣∣F∣∣∣∗+⟪G  D⟫=∣∣∣F∣∣∣∗+⟪G ̃F+ H⟫=⟪F+ G ̃F+ H⟫≤∣∣∣F+ G∣∣∣∗∣∣∣D∣∣∣sp  
where the second equality follows from∣∣∣F∣∣∣∗=⟪F ̃F⟫  and the fact that⟪F  H⟫= tr(E(F H ⊺))=
Finally  we show that∣∣∣D∣∣∣sp≤ 1. We have
⊺)= E(̃F̃F ⊺)+ E(̃F H ⊺)+ E(H̃F ⊺)+ E(HH ⊺)= E(̃F̃F ⊺)+ E(HH ⊺)  
E(DD
where we use the fact that E(F H ⊺)= 0q×q  implying E(̃F H ⊺)= 0q×q. Next  let E(F F ⊺)= V DV ⊺
be a reduced singular value decomposition  where D is a positive diagonal matrix of size q′ ≤ q.
Then E(̃F̃F ⊺)= V V ⊺  and we have
E(F F ⊺)⋅ H= 0q×p a.e. ⇔ V ⊺H= 0q′×p a.e. ⇔ E(̃F̃F ⊺)H= 0q×p a.e. .
This implies that E(̃F̃F ⊺)⋅ E(HH ⊺)= 0q×q and so these two symmetric matrices have orthogonal
∥E(DD
⊺)∥
⊺)∥
}≤ 1  
where the last bound comes from the fact that∣∣∣̃F∣∣∣sp ∣∣∣H∣∣∣sp≤ 1. Therefore∣∣∣D∣∣∣sp≤ 1.

= max{∥E(̃F̃F

0. The inequality follows from the duality of the norms.

⊺)∥
sp  ∥E(HH

row spans and orthogonal column spans. Therefore 

⊺)+ E(HH

=∥E(̃F̃F

⊺)∥

sp

sp

sp

This gives the subdifferential of penalty 2  deﬁned in (3.3). We can view the ﬁrst penalty update as
just a special case of the second penalty update. For penalty 1 in (3.1)  if we are updating Fj and ﬁx
all the other functions  we are now penalizing the norm

 

(4.5)

Fj+ Hj ∶

j))−1
E(FjF ⊺

which is clearly just a special case of penalty 2 with a single q-vector of functions instead of p
different q-vectors of functions. So  we have

∂∣∣∣Fj∣∣∣∗={(√
j)Hj= 0 a.e.} . (4.6)
Returning to the base case of p= 1 covariate  consider the population regularized risk optimization

5 Stationary Conditions and Backﬁtting Algorithms

j)= 0  E(FjF

⊺

⊺

{1

E∥Y − M(X)∥2

2+ λ∣∣∣M∣∣∣∗} 
a.e. for some V ∈ ∂∣∣∣M∣∣∣∗.

min
M

2

E(Y∣ X)= M(X)+ λV(X)

(5.1)

(5.2)

∣∣∣Fj∣∣∣∗=∥√
E(FjF ⊺
j)∥
∣∣∣Hj∣∣∣sp≤ 1  E(FjH

∗

where M is a vector of q univariate functions. The stationary condition for this optimization is

Deﬁne P(X)∶= E(Y∣ X).

4

CRAM BACKFITTING ALGORITHM — FIRST PENALTY

Iterate until convergence:

Input: Data(Xi  Yi)  regularization parameter λ.
InitializêMj= 0  for j= 1  . . .   p.
For each j= 1  . . .   p:
̂Mk(Xk);
(1) Compute the residual: Zj= Y −∑k≠j
(2) Estimate Pj= E[Zj∣ Xj] by smoothing: ̂Pj=SjZj;
̂P ⊺
̂Pj
j = U diag(τ)U ⊺
(4) Soft threshold: ̂Mj= U diag([1− λ/√
)U ⊺̂Pj;
τ]
(5) Center: ̂Mj←̂Mj− mean(̂Mj).
Output: Component functionŝMj and estimator̂M(Xi)=∑j

(3) Compute SVD: 1
n

+

̂Mj(Xij).

⊺

P

⊺

that

(5.3)

τ]+)U

Figure 1: The CRAM backﬁtting algorithm  using the ﬁrst penalty  which penalizes each component.

M= U diag([1− λ/√

Proposition 5.1. Let E(P P ⊺)= U diag(τ)U ⊺ be the singular value decomposition and deﬁne
where[x]+= max(x  0). Then M satisﬁes stationary condition (5.2)  and is a minimizer of (5.1).
Proof. Assume the singular values are sorted as τ1≥ τ2≥⋯≥ τq  and let r be the largest index such
√
E(M M ⊺)= U diag([√
√
τ− λ]+)U ⊺  and therefore
τr> λ. Thus  M has rank r. Note that
λ(√
M= U diag(λ/√
E(M M ⊺))−1
τ1∶r  0q−r)U
where x1∶k=(x1  . . .   xk) and ck=(c  . . .   c). It follows that
M+ λ(√
E(M M ⊺))−1
M= U diag(1r  0q−r)U ⊺P.
U diag(0r  1q−r)U ⊺P
H= 1
and take V =(√
E(M M ⊺))−1
M+ H. Then we have M+ λV = P .
√
E(HH ⊺)=
√
√
U diag(0r 
τr+1/λ  . . .  
τq/λ)U ⊺ we have∣∣∣H∣∣∣sp≤ 1. Also  E(M H ⊺)= 0q×q since
diag(1− λ/√
τ1∶r  0q−r) diag(0r  1q−r/λ)= 0q×q.
Similarly  E(M M ⊺)H= 0q×q since
diag((√
τ1∶r− λ)2  0q−r) diag(0r  1q−r/λ)= 0q×q.
It follows that V ∈ ∂∣∣∣M∣∣∣sp.

It remains to show that H satisﬁes the conditions of the subdifferential in (4.3). Since

Now deﬁne

(5.8)

(5.7)

(5.5)

(5.6)

(5.4)

P

λ

The analysis above justiﬁes a backﬁtting algorithm for estimating a constrained rank additive model
with the ﬁrst penalty  where the objective is

E∥Y − p∑
{1
Mj(Xj)∥2
+ λ
∣∣∣Mj∣∣∣∗}.
p∑
For a given coordinate j  we form the residual Zj= Y −∑k≠j Mk  and then compute the projection
Pj= E(Zj∣ Xj)  with singular value decomposition E(PjP ⊺
j)= U diag(τ)U ⊺. We then update
Mj= U diag([1− λ/√
τ]+)U

min
Mj

(5.10)

(5.9)

Pj

j=1

j=1

2

⊺

2

5

+

∗

sp

sp

2 

6 Excess Risk Bounds

Note that to predict at a point x not included in the training set  the smoother matrices are constructed

and proceed to the next variable. This is a Gauss-Seidel procedure that parallels the population
backﬁtting algorithm for SPAM [6]. In the sample version we replace the conditional expectation

The algorithm for penalty 2 is similar. In step (3) of the algorithm in Figure 1 we compute the SVD
of 1
n
Both algorithms can be viewed as functional projected gradient descent procedures.

Pj= E(Zj∣ Xj) by a nonparametric linear smoother ̂Pj=SjZj. The algorithm is given in Figure 1.
using that point; that is  ̂Pj(xj)= Sj(xj)⊺Zj.
)U ⊺̂P1∶p.
̂P1∶p

1∶p. Then  in step (4) we soft threshold according tôM1∶p= U diag([1− λ/√
̂P ⊺
τ]
The population risk of a q× p regression matrix M(X)=[M1(X1)⋯Mp(Xp)] is
R(M)= E∥Y − M(X)1p∥2
with sample version denoted ̂R(M). Consider all models that can be written as
M(X)= U⋅ D⋅ V(X)⊺
where U is an orthogonal q× r matrix  D is a positive diagonal matrix  and V(X)=[vjs(Xj)]
satisﬁes E(V ⊺V)= Ir. The population risk can be reexpressed as
R(M)= tr{(−Iq
V(X)⊺ )⊺](−Iq
V(X)⊺ )(
DU ⊺)⊺
E[(
DU ⊺)}
= tr{(−Iq
)(−Iq
DU ⊺)⊺(ΣY Y ΣY V
DU ⊺)}
and similarly for the sample risk  witĥΣn(V) replacing Σ(V)∶= Cov((Y  V(X)⊺)) above. The
“uncontrollable” contribution to the risk  which does not depend on M  is Ru= tr{ΣY Y}. We can
Rc(M)= R(M)− Ru= tr{(−2Iq
Σ(V)( 0q
DU ⊺)} .
DU ⊺)⊺
Using the von Neumann trace inequality  tr(AB)≤∥A∥p∥B∥p′ where 1/p+ 1/p′= 1 
Rc(M)−̂Rc(M)≤∥(−2Iq
DU ⊺)⊺(Σ(V)−̂Σn(V))∥
∥( 0q
DU ⊺)∥
≤∥(−2Iq
∥Σ(V)−̂Σn(V)∥
DU ⊺)⊺∥
∥D∥∗
≤ C max(2 ∥D∥sp)∥Σ(V)−̂Σn(V)∥
∥D∥∗
∗}∥Σ(V)−̂Σn(V)∥
≤ C max{2 ∥D∥2
⊺(Σ(V)−̂Σn(V)) w
≤ C sup

express the remaining “controllable” risk as

Y V ΣV V

(6.1)

Σ⊺

w

Y

Y

sp

sp

sp

sup
V

sup
w∈N

where here and in the following C is a generic constant. For the last factor in (6.1)  it holds that

∥Σ(V)−̂Σn(V)∥
whereN is a 1/2-covering of the unit(q+ r)-sphere  which has size∣N∣≤ 6q+r≤ 36q; see [8]. We
now assume that the functions vsj(xj) are uniformly bounded from a Sobolev space of order two.
Speciﬁcally  let{ψjk∶ k= 0  1  . . .} denote a uniformly bounded  orthonormal basis with respect to
L2[0  1]  and assume that vsj∈Hj where
Hj={fj∶ fj(xj)= ∞∑
for some 0< K<∞. The L∞-covering number ofHj satisﬁes logN(Hj  )≤ K/√

ajkψjk(xj) 

jkk4≤ K 2}

∞∑

a2

k=0

k=0

.

V

sp

6

sup
V

n

0

d



n

(6.2)

E(sup

V

sup
w∈N

sp

⊺

  with E(V

for some constant B. Thus  by Markov’s inequality we conclude that

sample continuous. It follows from a result of Cesa-Bianchi and Lugosi [1] that

Suppose that Y − E(Y∣ X)= W is Gaussian and the true regression function E(Y∣ X) is bounded.
Then the family of random variables Z(V w) ∶=√
n⋅ w⊺(Σ(V)−̂Σn(V))w is sub-Gaussian and
√
w⊺(Σ(V)−̂Σn(V))w)≤ C√
q log(36)+ log(pq)+ K√
∫ B
√
⎞⎠ .
⎛⎝
q+ log(pq)
∥Σ(V)−̂Σn(V)∥
= OP
If∣∣∣M∣∣∣∗=∥D∥∗= o(n/(q+ log(pq)))1/4  then returning to (6.1)  this gives us a bound on Rc(M)−
̂Rc(M) that is oP(1). More precisely  we deﬁne a class of matrices of functions:
q+ log(pq))1/4⎫⎪⎪⎬⎪⎪⎭ .
Mn=⎧⎪⎪⎨⎪⎪⎩M ∶ M(X)= U DV(X)⊺
V)= I  vsj∈Hj ∥D∥∗= o(
Then  for a ﬁtted matrix̂M chosen fromMn  writing M∗= arg minM ∈Mn R(M)  we have
R(̂M)− inf
R(M)= R(̂M)−̂R(̂M)−(R(M∗)−̂R(M∗))+(̂R(̂M)−̂R(M∗))
≤[R(̂M)−̂R(̂M)]−[R(M∗)−̂R(M∗)].
Subtracting Ru−̂Ru from each of the bracketed differences  we obtain that
R(M)≤[Rc(̂M)−̂Rc(̂M)]−[Rc(M∗)−̂Rc(M∗)]
{Rc(M)−̂Rc(M)}
∗∥Σ(V)−̂Σn(V)∥
) by (6.2)= oP(1).
n∑i∥Yi−∑j Mj(Xij)∥2
R(M) PD→ 0 .

Proposition 6.1. Let̂M minimize the empirical risk 1

≤ 2 sup
by (6.1)≤ O(∥D∥2
R(̂M)− inf

2 over the classMn.

R(̂M)− inf

This proves the following result.

M ∈Mn

M ∈Mn

M ∈Mn

sp

n

Then

M ∈Mn

7 Application to Gene Expression Data

To illustrate the proposed nonparametric reduced rank regression techniques  we consider data on
gene expression in E. coli from the “DREAM 5 Network Inference Challenge”1 [3]. In this challenge
genes were classiﬁed as transcription factors (TFs) or target genes (TGs). Transcription factors
regulate the target genes  as well as other TFs.

We focus on predicting the expression levels Y for a particular set of q= 27 TGs  using the expres-
sion levels X for p= 6 TFs. Our motivation for analyzing these 33 genes is that  according to the
functional relationship between X and Y is given by the composition of a map g∶ R
map h∶ R
27. If g and h are both linear  their composition h○ g is a linear map of rank no more
is linear  then h○ g has rank at most 2 in the sense of penalty 2. Higher rank can in principle occur

gold standard gene regulatory network used for the DREAM 5 challenge  the 6 TFs form the parent
set common to two additional TFs  which have the 27 TGs as their child nodes. In fact  the two
intermediate nodes d-separate the 6 TFs and the 27 TGs in a Bayesian network interpretation of this
gold standard. This means that if we treat the gold standard as a causal network  then up to noise  the
2 and a

than 2. As observed in Section 2  such a reduced rank linear model is a special case of an additive
model with reduced rank in the sense of penalty 2. More generally  if g is an additive function and h

6→ R

2→ R

1http://wiki.c2b2.columbia.edu/dream/index.php/D5c4

7

Penalty 1  λ = 20

Penalty 2  λ = 5

Figure 2: Left: Penalty 1 with large tuning parameter. Right: Penalty 2 with tuning parameter ob-
tained through 10-fold cross-validation. Plotted points are residuals holding out the given predictor.

under functional composition  since even a univariate additive map h∶ R→ R

q under our penalties (recall that penalty 1 and 2 coincide for univariate maps).

q may have rank up to

The backﬁtting algorithm of Figure 1 with penalty 1 and a rather aggressive choice of the tuning
parameter λ produces the estimates shown in Figure 2  for which we have selected three of the 27
TGs. Under such strong regularization  the 5th column of functions is rank zero and  thus  identically
zero. The remaining columns have rank one; the estimated ﬁtted values are scalar multiples of one
another. We also see that scalings can be different for different columns. The third plot in the third
row shows a slightly negative slope  indicating a negative scaling for this particular estimate. The
remaining functions in this row are oriented similarly to the other rows  indicating the same  positive
scaling. This property characterizes the difference between penalties 1 and 2; in an application of
penalty 2  the scalings would have been the same across all functions in a given row.

Next  we illustrate a higher-rank solution for penalty 2. Choosing the regularization parameter λ by
ten-fold cross-validation gives a ﬁt of rank 5  considerably lower than 27  the maximum possible
rank. Figure 2 shows a selection of three coordinates of the ﬁtted functions. Under rank ﬁve  each
row of functions is a linear combination of up to ﬁve other  linearly independent rows. We remark
that the use of cross-validation generally produces somewhat more complex models than is necessary
to capture an underlying low-rank data-generating mechanism. Hence  if the causal relationships for
these data were indeed additive and low rank  then the true low rank might well be smaller than ﬁve.

8 Summary

This paper introduced two penalties that induce reduced rank ﬁts in multivariate additive nonpara-
metric regression. Under linearity  the penalties specialize to group lasso and nuclear norm penalties
for classical reduced rank regression. Examining the subdifferentials of each of these penalties  we
developed backﬁtting algorithms for the two resulting optimization problems that are based on soft-
thresholding of singular values of smoothed residual matrices. The algorithms were demonstrated
on a gene expression data set constructed to have a naturally low-rank structure. We also provided a
persistence analysis that shows error tending to zero under a scaling assumption on the sample size
n and the dimensions q and p of the regression problem.

Acknowledgements

Research supported in part by NSF grants IIS-1116730  DMS-0746265  and DMS-1203762 
AFOSR grant FA9550-09-1-0373  ONR grant N000141210762  and an Alfred P. Sloan Fellowship.

8

References
[1] Nicol`o Cesa-Bianchi and G´abor Lugosi. On prediction of individual sequences. The Annals of

Statistics  27(6):1865–1894  1999.

[2] Maryam Fazel. Matrix rank minimization with applications. Technical report  Stanford Uni-

versity  2002. Doctoral Dissertation  Electrical Engineering Department.

[3] D. Marbach  J. C. Costello  R. K¨uffner  N. Vega  R. J. Prill  D. M. Camacho  K. R. Allison 
the DREAM5 Consortium  M. Kellis  J. J. Collins  and G. Stolovitzky. Wisdom of crowds for
robust gene network inference. Nature Methods  9(8):796–804  2012.

[4] Sahan Negahban and Martin J. Wainwright. Estimation of (near) low-rank matrices with noise

and high-dimensional scaling. Annals of Statistics  39:1069–1097  2011.

[5] Garvesh Raskutti  Martin J. Wainwright  and Bin Yu. Minimax-optimal rates for sparse addi-

tive models over kernel classes via convex programming. arxiv:1008.3654  2010.

[6] Pradeep Ravikumar  John Lafferty  Han Liu  and Larry Wasserman. Sparse additive models.

Journal of the Royal Statistical Society  Series B  Methodological  71(5):1009–1030  2009.

[7] Benjamin Recht  Maryam Fazel  and Pablo A. Parrilo. Guaranteed minimum rank solutions to

linear matrix equations via nuclear norm minimization. SIAM Review  52(3):471–501  2010.

[8] Roman Vershynin. How close is the sample covariance matrix to the actual covariance matrix?

arxiv:1004.3484  2010.

[9] G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra

and Applications  170:1039–1053  1992.

[10] Ming Yuan  Ali Ekici  Zhaosong Lu  and Renato Monteiro. Dimension reduction and coeff-

cient estimation in multivariate linear regression. J. R. Statist. Soc. B  69(3):329–346  2007.

[11] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2006.

9

,David Barrett
Sophie Denève
Christian Machens
Tae-Hyun Oh
Yasuyuki Matsushita
In Kweon
David Wipf
Brandon Yang
Gabriel Bender
Quoc Le
Jiquan Ngiam