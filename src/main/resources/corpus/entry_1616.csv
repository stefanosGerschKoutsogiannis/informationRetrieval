2019,Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction,Existing state-of-the-art estimation systems can detect 2d poses of multiple people in images quite reliably. In contrast  3d pose estimation from a single image is ill-posed due to occlusion and depth ambiguities. Assuming access to multiple cameras  or given an active system able to position itself to observe the scene from multiple viewpoints  reconstructing 3d pose from 2d measurements becomes well-posed within the framework of standard multi-view geometry. Less clear is what is an informative set of viewpoints for accurate 3d reconstruction  particularly in complex scenes  where people are occluded by others or by scene objects. In order to address the view selection problem in a principled way  we here introduce ACTOR  an active triangulation agent for 3d human pose reconstruction. Our fully trainable agent consists of a 2d pose estimation network (any of which would work) and a deep reinforcement learning-based policy for camera viewpoint selection. The policy predicts observation viewpoints  the number of which varies adaptively depending on scene content  and the associated images are fed to an underlying pose estimator. Importantly  training the policy requires no annotations - given a 2d pose estimator  ACTOR is trained in a self-supervised manner. In extensive evaluations on complex multi-people scenes filmed in a Panoptic dome  under multiple viewpoints  we compare our active triangulation agent to strong multi-view baselines  and show that ACTOR produces significantly more accurate 3d pose reconstructions. We also provide a proof-of-concept experiment indicating the potential of connecting our view selection policy to a physical drone observer.,Domes to Drones: Self-Supervised Active

Triangulation for 3D Human Pose Reconstruction

Supplementary Material

Aleksis Pirinen1∗  Erik Gärtner1∗ and Cristian Sminchisescu1 2
1Department of Mathematics  Faculty of Engineering  Lund University

2Google Research

{aleksis.pirinen  erik.gartner  cristian.sminchisescu}@math.lth.se

This supplementary provides more insight into our ACTOR model and experimental setup. Section §1
describes the details of the network architecture  implementation  and hyperparameters. §2 elaborates
on how we match 2d pose estimates in space and time using instance features. In §3 we provide 2d
reprojection errors onto 2d OpenPose [2] estimates on the Panoptic test splits. Finally  §4 describes
further dataset details.

1 Model Architecture

See Fig. 1 for a full description of the ACTOR network architecture. ACTOR was implemented in
Caffe [5] and MATLAB. We used an open-source TensorFlow implementation of OpenPose2. All
code and pre-trained weights have been made publicly available.3

Figure 1: ACTOR policy architecture. A multi-people 2d pose estimation system (here OpenPose 
but any similar deep system would would work) processes an input image. The deep feature maps Bt
produced by OpenPose (conv4_4_CPM) is fed into the ACTOR policy network and is processed by
two convolutional layers with ReLU-activations. The ﬁrst and second convolutional layers both have
3 × 3 kernels with stride 1. Their output dimensions are 8 × 39 × 21 and 4 × 18 × 9  respectively.
The max pooling layer has a 2 × 2 kernel with stride 2. The output from the second convolutional
layer is then concatenated with agent-centric camera rig information about previously visited cameras
relative to current position (Rig)  and auxiliary information such as the number of joints triangulated
and number of people detected in the view (Aux). The ﬂattened and concatenated data is then fed into
three fully-connected layers with tanh-activations with 1024  512 and 2 neurons respectively. The
ﬁnal output is scaled by two constants to produce radial angles on the viewing sphere.

1.1 Hyperparamters

Hyperparameter search was performed using two powerful workstations equipped with several
NVIDIA Titan V100 GPU:s. Training a single model for 40k episodes took about 32 hours using one
GPU and to speed up results while searching for optimal hyperparameters we trained several model
conﬁgurations in parallell using Hyperdock [3]. The most important parameters for training ACTOR

∗Denotes equal contribution  order determined by coin ﬂip.
2https://gist.github.com/alesolano/b073d8ec9603246f766f9f15d002f4f4
3https://github.com/ErikGartner/actor

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

!"CNNRig + AuxFCFCFCOpenPose('( '*)MaxPoolCNNPolicy Headwere learning rate  precision of the the von Mises (ma  me) and the annealing rate of the precision.
See Table 1 for a summary of the values tested for these hyperparameters. In total we trained around
10 different versions of the ﬁnal model with varying hyperparameters and evaluated each of them on
the validation set. Finally  the best model was evaluated on the test dataset and retrained with four
additional random seeds to measure the model’s sensitivity to the random seed (the model is not very
sensitive as indicated in Fig. 2  main paper).

Hyperparameter
Learning Rate

von Mises precision

Attempted values

{(1  10) → (25  50)  (10  50) → (20  100)  (10 50) → (100  500)}

{1e-7  5e-7  1e-6  5e-6}

Table 1: The values tested for the most important hyperparameters when training ACTOR. The ﬁnal
and best values are highlighted in bold. For the von Mises precisions  the arrow indicates linear
annealing performed during training (e.g. from (ma  me) = (1  10) to (ma  me) = (25  50) for the
best conﬁguration).

2 Matching Multiple People

ACTOR reconstructs multiple people in both space and time from 2d pose estimates. In order to
track and match these estimates we compute instance sensitive features. These deep features can then
be stably matched to each other using the Hungarian algorithm and the L2 distance to compute the
matching cost.
We trained an instance classiﬁer structured as a siamese network [1] using a contrastive loss [4] that
aims to produce 50-dimensional features for each person that can be used to distinguish individuals.
As input the instance classiﬁer takes VGG-19 [7] features from the bounding box of the 2d pose
estimate. The instance classiﬁer is trained for 40k iterations on the training set with a mini-batch
size of 16 where half contains positive pairs and the other half contains negative pairs. The training
examples are sampled randomly in both space and time yielding a robust classiﬁer. Lastly  the
instance classiﬁer is ﬁne-tuned for 2k iterations on each scene  creating scene-speciﬁc versions of
the classiﬁer that are slightly adapted to the environment of those scenes. This tuning is performed
outside the range of the active-sequence in which the agent operates.
At the start of an active-sequence the agents is given an appearance model for each target it should
reconstruct. These appearance models are averages of K different instance features computed for
each target in the scene but from time-freezes that are not part of the current active-sequence. We
denote the i:th instance feature for the l:th person by ul
i  with i = 1  . . .   K. In practice we use
K = 10. Then we set as appearance model ml to:

ml = median(ul

1  . . .   ul

K)

(1)

For each camera location we compute the distance between the instance features of each detected
person to all appearance models in that scene. This gives us a cost matrix whose elements are
cj l = (cid:107)uj − ml(cid:107)2
2  i.e.  the cost to match detection j to person l. Given this matrix we assign
detections according to the Hungarian algorithm. Since there might be false detections by the 2d pose
estimator and not all people are visible from every camera location we ﬁlter out matches with a cost
larger than a threshold C  such that all matches cj l ≤ C = 0.5.
If a person is never detected in an active-view  and if it does not have a previous temporal backup to
use as 3d pose reconstruction (cf. §3 and the implementation details in §4 of the main paper)  we set
each joint estimate to the ground-truth center hip location. Obviously  this estimate is implausible and
highly inaccurate – it is used only to compute average errors (not including such an estimate when
computing average errors would be another option  but this would not penalize missed persons).

3 Reprojection Errors onto OpenPose 2d Estimates

The 3d ground-truth in Panoptic is generated from exhaustive triangulation of 2d pose estimates [6] 
but those 2d pose estimates are not from OpenPose. Thus it is relevant to also look at reprojection

2

Figure 2: Mean 2d reprojection errors per joint relative to OpenPose 2d estimates vs number of
cameras on the test sets. Left: Multi-people data. Right: Single-people data. ACTOR reduces the 2d
reprojection error faster than the heuristic baselines  particularly for multi-people data. Single-person
scenes are easier to reconstruct  especially when using many cameras – also note that all models
converge close to the error of the oracle in this case.

errors onto the OpenPose 2d estimates  since these errors are not affected by any potential incorrect
bias in the 3d ground-truth. Such reprojection errors are shown in Fig. 2. We note that ACTOR is
more accurate relative to the oracle in this metric. For single-people data the agent even converges
close to the oracle  while the oracle is still slightly better for multi-people data due to its more difﬁcult
nature with occlusions. ACTOR yields lower reprojection errors than the heuristic baselines  with an
exception at 2 cameras for multi-people data where Max-Azim is more accurate. Note however that
ACTOR was not trained to produce accurate estimates at any ﬁxed number of cameras  but rather to
quickly triangulate all joints. Despite this  we outperform the baselines in the vast majority of cases.

4 Additional Dataset Insights

Table 2 shows the size and split of the Panoptic dataset [6] into train  validation and test sets. The data
was created using scripts that downsampled from 30 FPS to 2 FPS to increase movement between
frames.

Train

Val

Test

All

Maﬁa

53 100

27 900

33 728

114 728

Ultimatum 27 960

4 340

55 825

88 125

Pose

All

51 079

29 672

59 288

140 039

132 139

61 912

148 841

342 892

Table 2: The number of images in our dataset categorized by scene type and subset type (training 
validation  and testing). Note that Maﬁa and Ultimatum are complex multi-people scenes and that
they account for more than half of the dataset.

3

2345678910# cameras80110140170200230260290mean pixel errorACTORRandomMax-AzimOracle2345678910# cameras3036424854606672mean pixel errorACTORRandomMax-AzimOracleReferences
[1] J. Bromley  I. Guyon  Y. LeCun  E. Säckinger  and R. Shah. Signature veriﬁcation using a" siamese" time

delay neural network. In NIPS  pages 737–744  1994.

[2] Z. Cao  G. Hidalgo  T. Simon  S.-E. Wei  and Y. Sheikh. OpenPose: realtime multi-person 2D pose

estimation using Part Afﬁnity Fields. In CVPR  2017.

[3] E. Gärtner. Hyperdock. https://github.com/ErikGartner/Hyperdock  2019.
[4] R. Hadsell  S. Chopra  and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR 

pages 1735–1742. IEEE  2006.

[5] Y. Jia  E. Shelhamer  J. Donahue  S. Karayev  J. Long  R. Girshick  S. Guadarrama  and T. Darrell. Caffe:
Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international
conference on Multimedia  pages 675–678. ACM  2014.

[6] H. Joo  H. Liu  L. Tan  L. Gui  B. Nabbe  I. Matthews  T. Kanade  S. Nobuhara  and Y. Sheikh. Panoptic

studio: A massively multiview system for social motion capture. In ICCV  2015.

[7] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

ICLR  2015.

4

,Xiang Li
Tao Qin
Jian Yang
Tie-Yan Liu
Aleksis Pirinen
Erik Gärtner
Cristian Sminchisescu