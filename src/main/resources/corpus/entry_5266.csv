2016,Stochastic Variance Reduction Methods for Saddle-Point Problems,We consider convex-concave saddle-point problems where the objective functions may be split in many components  and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first  large-scale linearly convergent algorithms for this class of problems which are common in machine learning. While the algorithmic extension is straightforward  it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence  showing in particular that the same algorithm applies to a larger class of problems  such as variational inequalities   (b) there are two notions of splits  in terms of functions  or in terms of partial derivatives  (c) the split does need to be done with convex-concave terms  (d) non-uniform sampling is key to an efficient algorithm  both in theory and practice  and (e)  these incremental algorithms can be easily accelerated using a simple extension of the "catalyst" framework   leading to an algorithm which is always superior to accelerated batch algorithms.,Stochastic Variance Reduction Methods

for Saddle-Point Problems

P. Balamurugan

INRIA - Ecole Normale Supérieure  Paris
balamurugan.palaniappan@inria.fr

Francis Bach

INRIA - Ecole Normale Supérieure  Paris

francis.bach@ens.fr

Abstract

We consider convex-concave saddle-point problems where the objective functions
may be split in many components  and extend recent stochastic variance reduction
methods (such as SVRG or SAGA) to provide the ﬁrst large-scale linearly conver-
gent algorithms for this class of problems which are common in machine learning.
While the algorithmic extension is straightforward  it comes with challenges and
opportunities: (a) the convex minimization analysis does not apply and we use
the notion of monotone operators to prove convergence  showing in particular
that the same algorithm applies to a larger class of problems  such as variational
inequalities  (b) there are two notions of splits  in terms of functions  or in terms of
partial derivatives  (c) the split does need to be done with convex-concave terms 
(d) non-uniform sampling is key to an efﬁcient algorithm  both in theory and prac-
tice  and (e) these incremental algorithms can be easily accelerated using a simple
extension of the “catalyst” framework  leading to an algorithm which is always
superior to accelerated batch algorithms.

Introduction

1
When using optimization in machine learning  leveraging the natural separability of the objective
functions has led to many algorithmic advances; the most common example is the separability as a sum
of individual loss terms corresponding to individual observations  which leads to stochastic gradient
descent techniques. Several lines of work have shown that the plain Robbins-Monro algorithm could
be accelerated for strongly-convex ﬁnite sums  e.g.  SAG [1]  SVRG [2]  SAGA [3]. However  these
only apply to separable objective functions.
In order to tackle non-separable losses or regularizers  we consider the saddle-point problem:

min
x∈Rd

max
y∈Rn

K(x  y) + M (x  y) 

(1)

where the functions K and M are “convex-concave”  that is  convex with respect to the ﬁrst variable 
and concave with respect to the second variable  with M potentially non-smooth but “simple” (e.g. 
for which the proximal operator is easy to compute)  and K smooth. These problems occur naturally
within convex optimization through Lagrange or Fenchel duality [4]; for example the bilinear saddle-
point problem minx∈Rd maxy∈Rn f (x)+y(cid:62)Kx−g(y) corresponds to a supervised learning problem
with design matrix K  a loss function g∗ (the Fenchel conjugate of g) and a regularizer f.
We assume that the function K may be split into a potentially large number of components. Many
problems in machine learning exhibit that structure in the saddle-point formulation  but not in the
associated convex minimization and concave maximization problems (see examples in Section 2.1).
Like for convex minimization  gradient-based techniques that are blind to this separable structure
need to access all the components at every iteration. We show that algorithms such as SVRG [2] and
SAGA [3] may be readily extended to the saddle-point problem. While the algorithmic extension is
straightforward  it comes with challenges and opportunities. We make the following contributions:

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

– We provide the ﬁrst convergence analysis for these algorithms for saddle-point problems  which
differs signiﬁcantly from the associated convex minimization set-up. In particular  we use in
Section 6 the interpretation of saddle-point problems as ﬁnding the zeros of a monotone operator 
and only use the monotonicity properties to show linear convergence of our algorithms  thus
showing that they extend beyond saddle-point problems  e.g.  to variational inequalities [5  6].

– We show that the saddle-point formulation (a) allows two different notions of splits  in terms
of functions  or in terms of partial derivatives  (b) does need splits into convex-concave terms
(as opposed to convex minimization)  and (c) that non-uniform sampling is key to an efﬁcient
algorithm  both in theory and practice (see experiments in Section 7).

– We show in Section 5 that these incremental algorithms can be easily accelerated using a simple
extension of the “catalyst” framework of [7]  thus leading to an algorithm which is always superior
to accelerated batch algorithms.

proxσ

M (x(cid:48)  y(cid:48)) = arg min
x∈Rd

max
y∈Rn

2 Composite Decomposable Saddle-Point Problems
We now present our new algorithms on saddle-point problems and show a natural extension to
monotone operators later in Section 6. We thus consider the saddle-point problem deﬁned in Eq. (1) 
with the following assumptions:
2(cid:107)y(cid:107)2 is
(A) M is strongly (λ  γ)-convex-concave  that is  the function (x  y) (cid:55)→ M (x  y) − λ
2(cid:107)x(cid:107)2 + γ
convex-concave. Moreover  we assume that we may compute the proximal operator of M  i.e.  for
any (x(cid:48)  y(cid:48)) ∈ Rn+d (σ is the step-length parameter associated with the prox operator):
2(cid:107)y − y(cid:48)(cid:107)2.

2(cid:107)x − x(cid:48)(cid:107)2 − γ

σM (x  y) + λ

(2)
The values of λ and γ lead to the deﬁnition of a weighted Euclidean norm on Rn+d deﬁned as
Ω(x  y)2 = λ(cid:107)x(cid:107)2 + γ(cid:107)y(cid:107)2  with dual norm deﬁned through Ω∗(x  y)2 = λ−1(cid:107)x(cid:107)2 + γ−1(cid:107)y(cid:107)2.
Dealing with the two different scaling factors λ and γ is crucial for good performance  as these
may be very different  depending on the many arbitrary ways to set-up a saddle-point problem.
(B) K is convex-concave and has Lipschitz-continuous gradients; it is natural to consider the gradient
operator B : Rn+d → Rn+d deﬁned as B(x  y) = (∂xK(x  y) −∂yK(x  y)) ∈ Rn+d and
to consider L = supΩ(x−x(cid:48) y−y(cid:48))=1 Ω∗(B(x  y) − B(x(cid:48)  y(cid:48))). The quantity L represents the
condition number of the problem.
(C) The vector-valued function B(x  y) = (∂xK(x  y) −∂yK(x  y)) ∈ Rn+d may be split into a
i∈I Bi  where the only constraint is that each Bi is
Lipschitz-continuous (with constant Li). There is no need to assume the existence of a function
Ki : Rn+d → R such that Bi = (∂xKi −∂yKi).
We will also consider splits which are adapted to the saddle-point nature of the problem  that is 
j∈J By
I = J × K  Bjk(x  y) = (pjBx
j (x  y))  for p and q sequences that sum to one. This
substructure  which we refer to as “factored”  will only make a difference when storing the values
of these operators in Section 4 for our SAGA algorithm.

family of vector-valued functions as B =(cid:80)
of the form B(x  y) =(cid:0)(cid:80)
k (x  y) (cid:80)

j (x  y)(cid:1)  which is a subcase of the above with

k (x  y)  qkBy

k∈K Bx

Given assumptions (A)-(B)  the saddle-point problem in Eq. (1) has a unique solution (x∗  y∗) such
that K(x∗  y)+M (x∗  y) (cid:54) K(x∗  y∗)+M (x∗  y∗) (cid:54) K(x  y∗)+M (x  y∗)  for all (x  y); moreover
minx∈Rd maxy∈Rn K(x  y) + M (x  y) = maxy∈Rn minx∈Rd K(x  y) + M (x  y) (see  e.g.  [8  4]).
The main generic examples for the functions K(x  y) and M (x  y) are:
– Bilinear saddle-point problems: K(x  y) = y(cid:62)Kx for a matrix K ∈ Rn×d (we identify here a
√
matrix with the associated bilinear function)  for which the vector-valued function B(x  y) is linear 
i.e.  B(x  y) = (K(cid:62)y −Kx). Then L = (cid:107)K(cid:107)op/
γλ  where (cid:107)K(cid:107)op is the largest singular value
(cid:80)d
of K.
There are two natural potential splits with I = {1  . . .   n} × {1  . . .   d}  with B =
k=1 Bjk: (a) the split into individual elements Bjk(x  y) = Kjk(yj −xk)  where ev-
ery element is the gradient operator of a bi-linear function  and (b) the “factored” split into
j·  −pjxkK·k)  where Kj· and K·k are the j-th row and k-th
rows/columns Bjk(x  y) = (qkyjK(cid:62)
column of K  p and q are any set of vectors summing to one  and every element is not the gradient
operator of any function. These splits correspond to several “sketches” of the matrix K [9]  adapted
to subsampling of K  but other sketches could be considered.

(cid:80)n

j=1

2

– Separable functions: M (x  y) = f (x) − g(y) where f is any λ-strongly-convex and g is γ-
2(cid:107)x−x(cid:48)(cid:107)2
2(cid:107)y − y(cid:48)(cid:107)2 are easy to compute. In this situation 
g (y(cid:48))). Following the usual set-up of composite optimiza-

strongly convex  for which the proximal operators proxσ
and proxσ
proxσ
tion [10]  no smoothness assumption is made on M and hence on f or g.

g (y(cid:48)) = arg maxy∈Rd −σg(y) − γ

f (x(cid:48)) = arg minx∈Rd σf (x)+ λ

M (x(cid:48)  y(cid:48)) = (proxσ

f (x(cid:48))  proxσ

2.1 Examples in machine learning

Many learning problems are formulated as convex optimization problems  and hence by duality as
saddle-point problems. We now give examples where our new algorithms are particularly adapted.
Supervised learning with non-separable losses or regularizers. For regularized linear supervised
learning  with n d-dimensional observations put in a design matrix K ∈ Rn×d  the predictions
are parameterized by a vector x ∈ Rd and lead to a vector of predictions Kx ∈ Rn. Given a loss
function deﬁned through its Fenchel conjugate g∗ from Rn to R  and a regularizer f (x)  we obtain
exactly a bi-linear saddle-point problem. When the loss g∗ or the regularizer f is separable  i.e.  a
sum of functions of individual variables  we may apply existing fast gradient-techniques [1  2  3] to
the primal problem minx∈Rd g∗(Kx) + f (x) or the dual problem maxy∈Rn −g(y) − f∗(K(cid:62)y)  as
well as methods dedicated to separable saddle-point problems [11  12]. When the loss g∗ and the
regularizer f are not separable (but have a simple proximal operator)  our new fast algorithms are the
only ones that can be applied from the class of large-scale linearly convergent algorithms.
Non-separable losses may occur when (a) predicting by afﬁne functions of the inputs and not
penalizing the constant terms (in this case deﬁning the loss functions as the minimum over the
constant term  which becomes non-separable) or (b) using structured output prediction methods
that lead to convex surrogates to the area under the ROC curve (AUC) or other precision/recall
quantities [13  14]. These come often with efﬁcient proximal operators (see Section 7 for an
example).
Non-separable regularizers with available efﬁcient proximal operators are numerous  such as grouped-
norms with potentially overlapping groups  norms based on submodular functions  or total variation
(see [15] and references therein  and an example in Section 7).
Robust optimization. The framework of robust optimization [16] aims at optimizing an objective
function with uncertain data. Given that the aim is then to minimize the maximal value of the
objective function given the uncertainty  this leads naturally to saddle-point problems.
Convex relaxation of unsupervised learning. Unsupervised learning leads to convex relaxations
which often exhibit structures naturally amenable to saddle-point problems  e.g  for discriminative
clustering [17] or matrix factorization [18].

2.2 Existing batch algorithms

In this section  we review existing algorithms aimed at solving the composite saddle-point problem in
Eq. (1)  without using the sum-structure. Note that it is often possible to apply batch algorithms for
the associated primal or dual problems (which are not separable in general).
Forward-backward (FB) algorithm. The main iteration is

(cid:2)(xt−1  yt−1) − σ(cid:0)1/λ 0
(cid:0)xt−1 − σλ−1∂xK(xt−1  yt−1) + σγ−1∂yK(xt−1  yt−1)).

(cid:1)B(xt−1  yt−1)(cid:3)

(xt  yt) = proxσ
M
= proxσ
M

0 1/γ

The algorithm aims at simultaneously minimizing with respect to x while maximizing with re-
spect to y (when M (x  y) is the sum of isotropic quadratic terms and indicator functions  we get
simultaneous projected gradient descents). This algorithm is known not to converge in general [8] 
but is linearly convergent for strongly-convex-concave problems  when σ = 1/L2  with the rate
(1 − 1/(1 + L2))t [19] (see simple proof in Appendix B.1). This is the one we are going to adapt to
stochastic variance reduction.
When M (x  y) = f (x) − g(y)  we obtain the two parallel updates xt = proxσ
λ−1σ∂xK(xt−1  yt−1
ally by replacing the second one by yt = proxσ
g
to as the Arrow-Hurwicz method (see [20] and references therein).

(cid:0)xt−1 −
(cid:1)(cid:1)  which can de done seri-
(cid:1)(cid:1). This is often referred

(cid:0)yt−1 + γ−1σ∂yK(xt−1  yt−1
(cid:0)yt−1 + γ−1σ∂yK(xt  yt−1

(cid:1)(cid:1) and yt = proxσ

g

f

3

(cid:2)(xt−1  yt−1) − σ(cid:0)1/λ 0

Accelerated forward-backward algorithm. The forward-backward algorithm may be accelerated
by a simple extrapolation step  similar to Nesterov’s acceleration for convex minimization [21]. The
algorithm from [20]  which only applies to bilinear functions K  and which we extend from separable
M to our more general set-up in Appendix B.2  has the following iteration:

(cid:1)B(xt−1 + θ(xt−1 − xt−2)  yt−1 + θ(yt−1 − yt−2))(cid:3).

(xt  yt) = proxσ
M
With σ = 1/(2L) and θ = L/(L + 1)  we get an improved convergence rate  where (1 −
1/(1 + L2))t is replaced by (1 − 1/(1 + 2L))t. This is always a strong improvement when L
is large (ill-conditioned problems)  as illustrated in Section 7. Note that our acceleration technique in
Section 5 may be extended to get a similar rate for the batch set-up for non-linear K.

0 1/γ

2.3 Existing stochastic algorithms

Forward-backward algorithms have been studied with added noise [22]  leading to a convergence
rate in O(1/t) after t iterations for strongly-convex-concave problems. In our setting  we replace
Bi(x  y)  where i ∈ I is sampled from the probability vector (πi)i
B(x  y) in our algorithm with 1
πi
(good probability vectors will depend on the application  see below for bilinear problems). We have
EBi(x  y) = B(x  y); the main iteration is then

(cid:2)(xt−1  yt−1) − σt

(cid:0)1/λ 0

0 1/γ

πit

(cid:1) 1

Bit(xt−1  yt−1)(cid:3) 

(xt  yt) = proxσt
M

with it selected independently at random in I with probability vector π. In Appendix C  we show that
using σt = 2/(t + 1 + 8 ¯L(π)2) leads to a convergence rate in O(1/t)  where ¯L(π) is a smoothness
constant explicited below. For saddle-point problems  it leads to the complexities shown in Table 1.
Like for convex minimization  it is fast early on but the performance levels off. Such schemes are
typically used in sublinear algorithms [23].

2.4 Sampling probabilities  convergence rates and running-time complexities

(cid:80)

In order to characterize running-times  we denote by T (A) the complexity of computing A(x  y)
for any operator A and (x  y) ∈ Rn+d  while we denote by Tprox(M ) the complexity of computing
M (x  y). In our motivating example of bilinear functions K(x  y)  we assume that Tprox(M )
proxσ
takes times proportional to n + d and getting a single element of K is O(1).
In order to characterize the convergence rate  we need the Lipschitz-constant L (which happens to
be the condition number with our normalization) deﬁned earlier as well as a smoothness constant
adapted to our sampling schemes:
Ω∗(Bi(x  y) − Bi(x(cid:48)  y(cid:48)))2 such that Ω(x − x(cid:48)  y − y(cid:48))2 (cid:54) 1.
¯L(π)2 = sup(x y x(cid:48) y(cid:48))
i∈I
We always have the bounds L2 (cid:54) ¯L(π)2 (cid:54) maxi∈I L2
. However  in structured situations
i∈I
(like in bilinear saddle-point problems)  we get much improved bounds  as described below.
√
Bi-linear saddle-point. The constant L is equal to (cid:107)K(cid:107)op/
λγ  and we will consider as well
the Frobenius norm (cid:107)K(cid:107)F deﬁned through (cid:107)K(cid:107)2
jk  and the norm (cid:107)K(cid:107)max deﬁned as
j k K 2
(cid:107)K(cid:107)max = max{supj(KK(cid:62))1/2
jj   supk(K(cid:62)K)1/2

(cid:107)K(cid:107)max (cid:54) (cid:107)K(cid:107)op (cid:54) (cid:107)K(cid:107)F (cid:54)(cid:112)max{n  d}(cid:107)K(cid:107)max (cid:54)(cid:112)max{n  d}(cid:107)K(cid:107)op 

kk }. Among the norms above  we always have:

i ×(cid:80)
F =(cid:80)

(3)

1
πi

1
πi

√

which allows to show below that some algorithms have better bounds than others.
There are several schemes to choose the probabilities πjk (individual splits) and πjk = pjqk (fac-
tored splits). For the factored formulation where we select random rows and columns  we con-
sider the non-uniform schemes pj = (KK(cid:62))jj/(cid:107)K(cid:107)2
F and qk = (K(cid:62)K)kk/(cid:107)K(cid:107)2
F   leading to
√
¯L(π) (cid:54) (cid:107)K(cid:107)F /
λγ. For the indi-
jk/(cid:107)K(cid:107)2
F   leading to
vidual formulation where we select random elements  we consider πjk = K 2
√
λγ (in these

λγ  or uniform  leading to ¯L(π) (cid:54) (cid:112)max{n  d}(cid:107)K(cid:107)max/

¯L(π) (cid:54) (cid:112)max{n  d}(cid:107)K(cid:107)F /

λγ  or uniform  leading to ¯L(π) (cid:54) √

nd(cid:107)K(cid:107)max/

situations  it is important to select several elements simultaneously  which our analysis supports).
We characterize convergence with the quantity ε = Ω(x− x∗  y − y∗)2/Ω(x0 − x∗  y0 − y∗)2  where
(x0  y0) is the initialization of our algorithms (typically (0  0) for bilinear saddle-points). In Table 1
we give a summary of the complexity of all algorithms discussed in this paper: we recover the same
type of speed-ups as for convex minimization. A few points are worth mentioning:

√

4

Complexity

Algorithms
Batch FB
Batch FB-accelerated

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
SVRG-non-uniform-accelerated log(1/ε) ×(cid:0) nd +(cid:112)nd max{n  d}(cid:107)K(cid:107)F /

log(1/ε) ×(cid:0) nd + nd(cid:107)K(cid:107)2
log(1/ε) ×(cid:0) nd + nd(cid:107)K(cid:107)op/
(1/ε) ×(cid:0) max{n  d}(cid:107)K(cid:107)2
(cid:12)(cid:12)(cid:12)
(1/ε) ×(cid:0) nd(cid:107)K(cid:107)2
log(1/ε) ×(cid:0) nd + nd(cid:107)K(cid:107)2
log(1/ε) ×(cid:0) nd + max{n  d}(cid:107)K(cid:107)2

SAGA/SVRG-uniform
SAGA/SVRG-non-uniform

Stochastic FB-non-uniform
Stochastic FB-uniform

op/(λγ)
λγ)

max/(λγ)

max/(λγ)

F /(λγ)

√

(cid:12)(cid:12)(cid:12)

√

F /(λγ)

(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:12)(cid:12)(cid:12) (cid:1)

λγ

Table 1: Summary of convergence results for the strongly (λ  γ)-convex-concave bilinear saddle-point
problem with matrix K and individual splits (and n + d updates per iteration). For factored splits
(little difference)  see Appendix D.4. For accelerated SVRG  we omitted the logarithmic term (see
Section 5).

– Given the bounds between the various norms on K in Eq. (3)  SAGA/SVRG with non-uniform
sampling always has convergence bounds superior to SAGA/SVRG with uniform sampling  which
is always superior to batch forward-backward. Note however  that in practice  SAGA/SVRG with
uniform sampling may be inferior to accelerated batch method (see Section 7).

– Accelerated SVRG with non-uniform sampling is the most efﬁcient method  which is conﬁrmed
in our experiments. Note that if n = d  our bound is better than or equal to accelerated forward-
backward  in exactly the same way than for regular convex minimization. There is thus a formal
advantage for variance reduction.

(cid:80)

3 SVRG: Stochastic Variance Reduction for Saddle Points
Following [2  24]  we consider a stochastic-variance reduced estimation of the ﬁnite sum B(x  y) =
i∈I Bi(x  y). This is achieved by assuming that we have an iterate (˜x  ˜y) with a known value of

B(˜x  ˜y)  and we consider the estimate of B(x  y):

B(˜x  ˜y) + 1
πi

Bi(x  y) − 1

πi

Bi(˜x  ˜y) 

which has the correct expectation when i is sampled from I with probability π  but with a reduced
variance. Since we need to refresh (˜x  ˜y) regularly  the algorithm works in epochs (we allow to
sample m elements per updates  i.e.  a mini-batch of size m)  with an algorithm that shares the same
structure than SVRG for convex minimization; see Algorithm 1. Note that we provide an explicit
number of iterations per epoch  proportional to (L2 + 3 ¯L2/m). We have the following theorem 
shown in Appendix D.1 (see also a discussion of the proof in Section 6).

Theorem 1 Assume (A)-(B)-(C). After v epochs of Algorithm 1  we have:

E(cid:2)Ω(xv − x∗  yv − y∗)2(cid:3) (cid:54) (3/4)vΩ(x0 − x∗  y0 − y∗)2.

¯L2) maxi∈I T (Bi) + (1 + L2 + ¯L2/m)Tprox(M )(cid:3) log 1

The computational complexity to reach precision ε is proportional

ε . Note that by taking the mini-batch size m
large  we can alleviate the complexity of the proximal operator proxM if too large. Moreover  if L2
is too expensive to compute  we may replace it by ¯L2 but with a worse complexity bound.
Bilinear saddle-point problems. When using a mini-batch size m = 1 with the factored updates 
or m = n + d for the individual updates  we get the same complexities proportional to [nd +
max{n  d}(cid:107)K(cid:107)2
F /(λγ)] log(1/ε) for non-uniform sampling  which improve signiﬁcantly over (non-
accelerated) batch methods (see Table 1).

to (cid:2)T (B) + (mL2 +

4 SAGA: Online Stochastic Variance Reduction for Saddle Points

Following [3]  we consider a stochastic-variance reduced estimation of B(x  y) =(cid:80)

i∈I Bi(x  y).
This is achieved by assuming that we store values gi = Bi(xold(i)  yold(i)) for an old iterate

5

Algorithm 1 SVRG: Stochastic Variance Reduction for Saddle Points
Input: Functions (Ki)i  M  probabilities (πi)i  smoothness ¯L(π) and L  iterate (x  y)

number of epochs v  number of updates per iteration (mini-batch size) m

Set σ =(cid:2)L2 + 3 ¯L2/m(cid:3)−1

for u = 1 to v do

Initialize (˜x  ˜y) = (x  y) and compute B(˜x  ˜y)
for k = 1 to log 4 × (L2 + 3 ¯L2/m) do

Sample i1  . . .   im ∈ I from the probability vector (πi)i with replacement
(x  y) ← proxσ

(cid:2)(x  y)−σ(cid:0)1/λ 0

(cid:1)(cid:0)B(˜x  ˜y)+ 1

(cid:80)m

(cid:8) 1

Bik (x  y)− 1

M

m

k=1

0 1/γ

πik

πik

Bik (˜x  ˜y)(cid:9)(cid:1)(cid:3)

end for

end for

Output: Approximate solution (x  y)

(xold(i)  yold(i))  and we consider the estimate of B(x  y):

(cid:80)

j∈I gj + 1
πi

Bi(x  y) − 1

πi

gi 

which has the correct expectation when i is sampled from I with probability π. At every iteration 
we also refresh the operator values gi ∈ Rn+d  for the same index i or with a new index i sampled
uniformly at random. This leads to Algorithm 2  and we have the following theorem showing linear
convergence  proved in Appendix D.2. Note that for bi-linear saddle-points  the initialization at (0  0)
has zero cost (which is not possible for convex minimization).

Theorem 2 Assume (A)-(B)-(C). After t iterations of Algorithm 2 (with the option of resampling
when using non-uniform sampling)  we have:

E(cid:2)Ω(xt − x∗  yt − y∗)2(cid:3) (cid:54) 2(cid:0)1 − (max{ 3|I|

Ω(x0 − x∗  y0 − y∗)2.

mµ2})−1(cid:1)t

2m   1 + L2

µ2 + 3 ¯L2

Resampling or re-using the same gradients. For the bound above to be valid for non-uniform
sampling  like for convex minimization [25]  we need to resample m operators after we make
the iterate update. In our experiments  following [25]  we considered a mixture of uniform and
non-uniform sampling  without the resampling step.
SAGA vs. SVRG. The difference between the two algorithms is the same as for convex minimization
(see  e.g.  [26] and references therein)  that is SVRG has no storage  but works in epochs and requires
slightly more accesses to the oracles  while SAGA is a pure online method with fewer parameters but
requires some storage (for bi-linear saddle-point problems  we only need to store O(n+d) elements
for the factored splits  while we need O(dn) for the individual splits). Overall they have the same
running-time complexity for individual splits; for factored splits  see Appendix D.4.
Factored splits. When using factored splits  we need to store the two parts of the operator values
separately and update them independently  leading in Theorem 2 to replacing |I| by max{|J| |K|}.

5 Acceleration
Following the “catalyst” framework of [7]  we consider a sequence of saddle-point problems with
added regularization; namely  given (¯x  ¯y)  we use SVRG to solve approximately
2 (cid:107)y − ¯y(cid:107)2 

2 (cid:107)x − ¯x(cid:107)2 − γτ

K(x  y) + M (x  y) + λτ

(4)

min
x∈Rd

max
y∈Rn

for well-chosen τ and (¯x  ¯y). The main iteration of the algorithm differs from the original SVRG by
the presence of the iterate (¯x  ¯y)  which is updated regularly (after a precise number of epochs)  and
different step-sizes (see details in Appendix D.3). The complexity to get an approximate solution of
Eq. (4) (forgetting the complexity of the proximal operator and for a single update)  up to logarithmic
terms  is proportional  to T (B) + ¯L2(1 + τ )−2 maxi∈I T (Bi).
The key difference with the convex optimization set-up is that the analysis is simpler  without
the need for Nesterov acceleration machinery [21] to deﬁne a good value of (¯x  ¯y); indeed  the
solution of Eq. (4) is one iteration of the proximal-point algorithm  which is known to converge

6

Algorithm 2 SAGA: Online Stochastic Variance Reduction for Saddle Points
Input: Functions (Ki)i  M  probabilities (πi)i  smoothness ¯L(π) and L  iterate (x  y)

number of iterations t  number of updates per iteration (mini-batch size) m

2m − 1  L2 + 3 ¯L2

Set σ =(cid:2) max{ 3|I|
Initialize gi = Bi(x  y) for all i ∈ I and G =(cid:80)
(cid:1)(cid:0)G + 1

m }(cid:3)−1
(cid:2)(x  y) − σ(cid:0)1/λ 0

for u = 1 to t do

Sample i1  . . .   im ∈ I from the probability vector (πi)i with replacement
(cid:8) 1
(cid:80)m
Compute hk = Bik (x  y) for k ∈ {1  . . .   m}
(x  y) ← proxσ
(optional) Sample i1  . . .   im ∈ I uniformly with replacement
(optional) Compute hk = Bik (x  y) for k ∈ {1  . . .   m}

Replace G ← G −(cid:80)m

k=1{gik − hk} and gik ← hk for k ∈ {1  . . .   m}

gik(cid:9)(cid:1)(cid:3)

hk − 1
πik

m

k=1

πik

i∈I gi

M

0 1/γ

end for

Output: Approximate solution (x  y)

linearly [27] with rate (1 + τ−1)−1 = (1 − 1
1+τ ). Thus the overall complexity is up to loga-
rithmic terms equal to T (B)(1 + τ ) + ¯L2(1 + τ )−1 maxi∈I T (Bi). The trade-off in τ is opti-

mal for 1 + τ = ¯L(cid:112)maxi∈I T (Bi)/T (B)  showing that there is a potential acceleration when
¯L(cid:112)maxi∈I T (Bi)/T (B) (cid:62) 1  leading to a complexity ¯L(cid:112)T (B) maxi∈I T (Bi).

Since the SVRG algorithm already works in epochs  this leads to a simple modiﬁcation where every
log(1 + τ ) epochs  we change the values of (¯x  ¯y). See Algorithm 3 in Appendix D.3. Moreover  we
can adaptively update (¯x  ¯y) more aggressively to speed-up the algorithm.
The following theorem gives the convergence rate of the method (see proof in Appendix D.3).

(cid:112)max{n−1  d−1} − 1(cid:9)
With the value of τ deﬁned above (corresponding to τ = max(cid:8)0 
for bilinear problems)  we get the complexity ¯L(cid:112)T (B) maxi∈I T (Bi)  up to the logarithmic term

log(1 + τ ). For bilinear problems  this provides a signiﬁcant acceleration  as shown in Table 1.

(cid:107)K(cid:107)F√

λγ

Theorem 3 Assume (A)-(B)-(C). After v epochs of Algorithm 3  we have  for any positive v:

E(cid:2)Ω(xv − x∗  yv − y∗)2(cid:3) (cid:54)(cid:0)1 − 1

(cid:1)v

τ +1

Ω(x0 − x∗  y0 − y∗)2.

While we provide a proof only for SVRG  the same scheme should work for SAGA. Moreover  the
same idea also applies to the batch setting (by simply considering |I| = 1  i.e.  a single function) 
leading to an acceleration  but now valid for all functions K (not only bilinear).

6 Extension to Monotone Operators

monotone) operators Bi  i ∈ I  with B =(cid:80)
A +(cid:80)

In this paper  we have chosen to focus on saddle-point problems because of their ubiquity in machine
learning. However  it turns out that our algorithm and  more importantly  our analysis extend
to all set-valued monotone operators [8  28]. We thus consider a maximal strongly-monotone
operator A on a Euclidean space E  as well as a ﬁnite family of Lipschitz-continuous (not necessarily
i∈I Bi monotone. Our algorithm then ﬁnds the zeros of
i∈I Bi = A + B  from the knowledge of the resolvent (“backward”) operator (I + σA)−1
(for a well chosen σ > 0) and the forward operators Bi  i ∈ I. Note the difference with [29]  which
requires each Bi to be monotone with a known resolvent and A to be monotone and single-valued.
There several interesting examples (on which our algorithms apply):
– Saddle-point problems: We assume for simplicity that λ = γ = µ (this can be achieved by a
simple change of variable). If we denote B(x  y) = (∂xK(x  y) −∂yK(x  y)) and the multi-
valued operator A(x  y) = (∂xM (x  y) −∂yM (x  y))  then the proximal operator proxσ
M may be
written as (µI + σA)−1(µx  µy)  and we recover exactly our framework from Section 2.

tions fi: we recover proximal-SVRG [24] and SAGA [3]  to minimize minz∈E g(z) +(cid:80)

– Convex minimization: A = ∂g and Bi = ∂fi for a strongly-convex function g and smooth func-
i∈I fi(z).
However  this is a situation where the operators Bi have an extra property called co-coercivity [6] 

7

which we are not using because it is not satisﬁed for saddle-point problems. The extension of
SAGA and SVRG to monotone operators was proposed earlier by [30]  but only co-coercive opera-
tors are considered  and thus only convex minimization is considered (with important extensions
beyond plain SAGA and SVRG)  while our analysis covers a much broader set of problems. In
particular  the step-sizes obtained with co-coercivity lead to divergence in the general setting.
Because we do not use co-coercivity  applying our results directly to convex minimization  we
would get slower rates  while  as shown in Section 2.1  they can be easily cast as a saddle-point
problem if the proximal operators of the functions fi are known  and we then get the same rates
than existing fast techniques which are dedicated to this problem [1  2  3].

– Variational inequality problems  which are notably common in game theory (see  e.g.  [5]).

norm λ(cid:107)x(cid:107)2/2 and the clustering-inducing term(cid:80)
surrogate to the area under the ROC curve  deﬁned as proportional to(cid:80)

7 Experiments
We consider binary classiﬁcation problems with design matrix K and label vector in {−1  1}n  a
non-separable strongly-convex regularizer with an efﬁcient proximal operator (the sum of the squared
i(cid:54)=j |xi − xj|  for which the proximal operator
(cid:80)
may be computed in O(n log n) by isotonic regression [31]) and a non-separable smooth loss (a
i−∈I− (1− yi + yj)2 
where I+/I− are sets with positive/negative labels  for a vector of prediction y  for which an efﬁcient
proximal operator may be computed as well  see Appendix E).
F /(λγ) where λ is the regularization strength and γ ≈ n
Our upper-bounds depend on the ratio (cid:107)K(cid:107)2
in our setting where we minimize an average risk. Setting λ = λ0 = (cid:107)K(cid:107)2
F /n2 corresponds to a
regularization proportional to the average squared radius of the data divided by 1/n which is standard
in this setting [1]. We also experiment with smaller regularization (i.e.  λ/λ0 = 10−1)  to make
the problem more ill-conditioned (it turns out that the corresponding testing losses are sometimes
slightly better). We consider two datasets  sido (n = 10142  d = 4932  non-separable losses and
regularizers presented above) and rcv1 (n = 20242  d = 47236  separable losses and regularizer
described in Appendix F  so that we can compare with SAGA run in the primal). We report below the
squared distance to optimizers which appears in our bounds  as a function of the number of passes on
the data (for more details and experiments with primal-dual gaps and testing losses  see Appendix F).
Unless otherwise speciﬁed  we always use non-uniform sampling.

i+∈I+

We see that uniform sampling for SAGA does not improve on batch methods  SAGA and accelerated
SVRG (with non-uniform sampling) improve signiﬁcantly over the existing methods  with a stronger
gain for the accelerated version for ill-conditioned problems (middle vs. left plot). On the right plot 
we compare to primal methods on a separable loss  showing that primal methods (here “fba-primal” 
which is Nesterov acceleration) that do not use separability (and can thus be applied in all cases)
are inferior  while SAGA run on the primal remains faster (but cannot be applied for non-separable
losses).

8 Conclusion
We proposed the ﬁrst linearly convergent incremental gradient algorithms for saddle-point problems 
which improve both in theory and practice over existing batch or stochastic algorithms. While we
currently need to know the strong convexity-concavity constants  we plan to explore in future work
adaptivity to these constants like already obtained for convex minimization [3]  paving the way to an
analysis without strong convexity-concavity.

8

010020030040050010−5100sido − distance to optimizers − λ/λ0=1.00 fb−accfb−stosagasaga (unif)svrgsvrg−accfba−primal010020030040050010−5100sido − distance to optimizers − λ/λ0=0.10 fb−accfb−stosagasaga (unif)svrgsvrg−accfba−primal010020030040050010−1510−1010−5100rcv1 − distance to optimizers − λ/λ0=1.00 fb−accfb−stosagasaga (unif)svrgsvrg−accfba−primalsaga−primalReferences
[1] N. Le Roux  M. Schmidt  and F. Bach. A stochastic gradient method with an exponential convergence rate

for ﬁnite training sets. In Adv. NIPS  2012.

[2] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Adv. NIPS  2013.

[3] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for

non-strongly convex composite objectives. In Adv. NIPS  2014.

[4] R. T. Rockafellar. Monotone operators associated with saddle-functions and minimax problems. Nonlinear

Functional Analysis  18(part 1):397–407  1970.

[5] P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity

problems: a survey of theory  algorithms and applications. Math. Prog.  48(1-3):161–220  1990.

[6] D. L. Zhu and P. Marcotte. Co-coercivity and its role in the convergence of iterative schemes for solving

variational inequalities. SIAM Journal on Optimization  6(3):714–726  1996.

[7] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Adv. NIPS  2015.
[8] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer Science & Business Media  2011.

[9] D. Woodruff. Sketching as a tool for numerical linear algebra. Technical Report 1411.4357  arXiv  2014.
[10] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[11] X. Zhu and A. J. Storkey. Adaptive stochastic primal-dual coordinate descent for separable saddle point
problems. In Machine Learning and Knowledge Discovery in Databases  pages 645–658. Springer  2015.
[12] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization.

In Proc. ICML  2015.

[13] T. Joachims. A support vector method for multivariate performance measures. In Proc. ICML  2005.
[14] R. Herbrich  T. Graepel  and K. Obermayer. Large margin rank boundaries for ordinal regression. In Adv.

NIPS  1999.

[15] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Optimization with sparsity-inducing penalties. Founda-

tions and Trends in Machine Learning  4(1):1–106  2012.

[16] A. Ben-Tal  L. El Ghaoui  and A. Nemirovski. Robust Optimization. Princeton University Press  2009.
[17] L. Xu  J. Neufeld  B. Larson  and D. Schuurmans. Maximum margin clustering. In Adv. NIPS  2004.
[18] F. Bach  J. Mairal  and J. Ponce. Convex sparse matrix factorizations. Technical Report 0812.1869  arXiv 

2008.

[19] G. H. G. Chen and R. T. Rockafellar. Convergence rates in forward-backward splitting. SIAM Journal on

Optimization  7(2):421–444  1997.

[20] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to

imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  2011.

[21] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer  2004.
[22] L. Rosasco  S. Villa  and B. C. V˜u. A stochastic forward-backward splitting method for solving monotone

inclusions in hilbert spaces. Technical Report 1403.7999  arXiv  2014.

[23] K. L. Clarkson  E. Hazan  and D. P. Woodruff. Sublinear optimization for machine learning. Journal of the

ACM (JACM)  59(5):23  2012.

[24] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization  24(4):2057–2075  2014.

[25] M. Schmidt  R. Babanezhad  M.O. Ahmed  A. Defazio  A. Clifton  and A. Sarkar. Non-uniform stochastic

average gradient method for training conditional random ﬁelds. In Proc. AISTATS  2015.

[26] R. Harikandeh  M. O. Ahmed  A. Virani  M. Schmidt  J. Koneˇcn`y  and S. Sallinen. Stop wasting my

gradients: Practical SVRG. In Adv. NIPS  2015.

[27] R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and

Optimization  14(5):877–898  1976.

[28] E. Ryu and S. Boyd. A primer on monotone operator methods. Appl. Comput. Math.  15(1):3–43  2016.
[29] H. Raguet  J. Fadili  and G. Peyré. A generalized forward-backward splitting. SIAM Journal on Imaging

Sciences  6(3):1199–1226  2013.

[30] D. Davis. Smart: The stochastic monotone aggregated root-ﬁnding algorithm. Technical Report 1601.00698 

arXiv  2016.

[31] X. Zeng and M. Figueiredo. Solving OSCAR regularization problems by fast approximate proximal

splitting algorithms. Digital Signal Processing  31:124–135  2014.

9

,Xiaoqin Zhang
Di Wang
Zhengyuan Zhou
Yi Ma
Balamurugan Palaniappan
Francis Bach