2012,No-Regret Algorithms for Unconstrained Online Convex Optimization,Some of the most compelling applications of online convex optimization  including online prediction and classification  are unconstrained: the natural feasible set is R^n.  Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x* are known in advance.  We present an algorithm that  without such prior knowledge  offers near-optimal regret bounds with respect to _any_ choice of x*.  In particular  regret with respect to x* = 0 is _constant_.  We then prove lower bounds showing that our algorithm's guarantees are optimal in this setting up to constant factors.,No-Regret Algorithms for Unconstrained

Online Convex Optimization

Matthew Streeter
Duolingo  Inc.∗

Pittsburgh  PA 15232

matt@duolingo.com

H. Brendan McMahan

Google  Inc.

Seattle  WA 98103

mcmahan@google.com

Abstract

Some of the most compelling applications of online convex optimization  includ-
ing online prediction and classiﬁcation  are unconstrained: the natural feasible set
is Rn. Existing algorithms fail to achieve sub-linear regret in this setting unless
constraints on the comparator point ˚x are known in advance. We present algo-
rithms that  without such prior knowledge  offer near-optimal regret bounds with
respect to any choice of ˚x. In particular  regret with respect to ˚x = 0 is constant.
We then prove lower bounds showing that our guarantees are near-optimal in this
setting.

1

Introduction

Over the past several years  online convex optimization has emerged as a fundamental tool for solv-
ing problems in machine learning (see  e.g.  [3  12] for an introduction). The reduction from general
online convex optimization to online linear optimization means that simple and efﬁcient (in memory
and time) algorithms can be used to tackle large-scale machine learning problems. The key theoret-
ical techniques behind essentially all the algorithms in this ﬁeld are the use of a ﬁxed or increasing
strongly convex regularizer (for gradient descent algorithms  this is equivalent to a ﬁxed or decreas-
ing learning rate sequence). In this paper  we show that a fundamentally different type of algorithm
can offer signiﬁcant advantages over these approaches. Our algorithms adjust their learning rates
based not just on the number of rounds  but also based on the sum of gradients seen so far. This
allows us to start with small learning rates  but effectively increase the learning rate if the problem
instance warrants it.

This approach produces regret bounds of the form O(cid:0)R

T log((1 + R)T )(cid:1)  where R = (cid:107)˚x(cid:107)2 is the

L2 norm of an arbitrary comparator. Critically  our algorithms provide this guarantee simultaneously
for all ˚x ∈ Rn  without any need to know R in advance. A consequence of this is that we can
guarantee at most constant regret with respect to the origin  ˚x = 0. This technique can be applied to
any online convex optimization problem where a ﬁxed feasible set is not an essential component of
the problem. We discuss two applications of particular interest below:

√

Online Prediction Perhaps the single most important application of online convex optimization
is the following prediction setting: the world presents an attribute vector at ∈ Rn; the prediction
algorithm produces a prediction σ(at · xt)  where xt ∈ Rn represents the model parameters  and
σ : R → Y maps the linear prediction into the appropriate label space. Then  the adversary reveals
the label yt ∈ Y   and the prediction is penalized according to a loss function (cid:96) : Y × Y → R.
For appropriately chosen σ and (cid:96)  this becomes a problem of online convex optimization against
functions ft(x) = (cid:96)(σ(at·x)  yt). In this formulation  there are no inherent restrictions on the model
coefﬁcients x ∈ Rn. The practitioner may have prior knowledge that “small” model vectors are more

∗This work was performed while the author was at Google.

1

likely than large ones  but this is rarely best encoded as a feasible set F  which says: “all xt ∈ F are
equally likely  and all other xt are ruled out.” A more general strategy is to introduce a ﬁxed convex
regularizer: L1 and L2
2 penalties are common  but domain-speciﬁc choices are also possible. While
algorithms of this form have proved very effective at solving these problems  theoretical guarantees
usually require ﬁxing a feasible set of radius R  or at least an intelligent guess of the norm of an
optimal comparator ˚x.

xt i on each expert i  and then receives reward(cid:80)

The Unconstrained Experts Problem and Portfolio Management
In the classic problem of
predicting with expert advice (e.g.  [3])  there are n experts  and on each round t the player selects
an expert (say i)  and obtains reward gt i from a bounded interval (say [−1  1]). Typically  one uses
an algorithm that proposes a probability distribution pt on experts  so the expected reward is pt · gt.
Our algorithms apply to an unconstrained version of this problem: there are still n experts with
payouts in [−1  1]  but rather than selecting an individual expert  the player can place a “bet” of
i xt igt i = xt · gt. The bets are unconstrained
(betting a negative value corresponds to betting against the expert). In this setting  a natural goal is
the following: place bets so as to achieve as much reward as possible  subject to the constraint that
total losses are bounded by a constant (which can be set equal to some starting budget which is to be
invested). Our algorithms can satisfy constraints of this form because regret with respect to ˚x = 0
(which equals total loss) is bounded by a constant.
It is useful to contrast our results in this setting to previous applications of online convex optimiza-
tion to portfolio management  for example [6] and [2]. By applying algorithms for exp-concave
loss functions  they obtain log-wealth within O(log(T )) of the best constant rebalanced portfolio.
However  this approach requires a “no-junk-bond” assumption: on each round  for each investment 
you always retain at least an α > 0 fraction of your initial investment. While this may be realistic
(though not guaranteed!) for blue-chip stocks  it certainly is not for bets on derivatives that can
lose all their value unless a particular event occurs (e.g.  a stock price crosses some threshold). Our
model allows us to handle such investments: if we play xi > 0  an outcome of gi = −1 corresponds
exactly to losing 100% of that investment. Our results imply that if even one investment (out of
exponentially many choices) has signiﬁcant returns  we will increase our wealth exponentially.

Notation and Problem Statement For the algorithms considered in this paper  it will be more
natural to consider reward-maximization rather than loss-minimization. Therefore  we consider
online linear optimization where the goal is to maximize cumulative reward given adversarially
selected linear reward functions ft(x) = gt · x. On each round t = 1 . . . T   the algorithm selects a
point xt ∈ Rn  receives reward ft(xt) = gt · xt  and observes gt. For simplicity  we assume gt i ∈
[−1  1]  that is  (cid:107)gt(cid:107)∞ ≤ 1. If the real problem is against convex loss functions (cid:96)t(x)  they can be
converted to our framework by taking gt = −(cid:79)(cid:96)t(xt) (see pseudo-code for REWARD-DOUBLING) 
using the standard reduction from online convex optimization to online linear optimization [13].

We use the compressed summation notation g1:t =(cid:80)t

s=1 gs for both vectors and scalars. We study

the reward of our algorithms  and their regret against a ﬁxed comparator ˚x:

Reward ≡ T(cid:88)

gt · xt

and

Regret(˚x) ≡ g1:T · ˚x − T(cid:88)

gt · xt.

t=1

t=1

rithm that  for any ˚x ∈ Rn  guarantees Regret(˚x) ≤ O(cid:0)(cid:107)˚x(cid:107)2

Comparison of Regret Bounds The primary contribution of this paper is to establish matching
upper and lower bounds for unconstrained online convex optimization problems  using algorithms
that require no prior information about the comparator point ˚x. Speciﬁcally  we present an algo-
√
this guarantee  we show that it is sufﬁcient (and necessary) that reward is Ω(exp(|g1:T|/
T )) (see
Theorem 1). This shift of emphasis from regret-minimization to reward-maximization eliminates
the quantiﬁcation on ˚x  and may be useful in other contexts.
Table 1 compares the bounds for REWARD-DOUBLING (this paper) to those of two previous algo-
rithms: online gradient descent [13] and projected exponentiated gradient descent [8  12]. For each
Our bounds are not directly comparable to the bounds cited above: a O(log(T )) regret bound on log-

wealth implies wealth at least O(cid:0)OPT/T(cid:1)  whereas we guarantee wealth like O(cid:0)OPT’ − √

T )(cid:1). To obtain

√
T log((1 + (cid:107)˚x(cid:107)2)

T(cid:1). But more

√

importantly  the comparison classes are different.

2

Assuming (cid:107)gt(cid:107)2 ≤ 1:

Gradient Descent  η = R√
REWARD-DOUBLING
Assuming (cid:107)gt(cid:107)∞ ≤ 1:

T

√
˚x = 0
R
T



√

R

√

(cid:107)˚x(cid:107)2 ≤ R
R

(cid:16) n(1+R)T

T

T log



Exponentiated G.D.
REWARD-DOUBLING

R

√
˚x = 0

T log n


√

R

(cid:107)˚x(cid:107)1 ≤ R
√
T log n
R
T log

(cid:16) n(1+R)T



(cid:17)

(cid:17)

Arbitrary ˚x
(cid:107)˚x(cid:107)2T

(cid:16) n(1+(cid:107)˚x(cid:107)2)T

T log



√

(cid:107)˚x(cid:107)2

Arbitrary ˚x
(cid:107)˚x(cid:107)1T

(cid:16) n(1+(cid:107)˚x(cid:107)1)

√



√

(cid:107)˚x(cid:107)1

T log

(cid:17)

(cid:17)

T

Table 1: Worst-case regret bounds for various algorithms (up to constant factors). Exponentiated
G.D. uses feasible set {x : (cid:107)x(cid:107)1 ≤ R}  and REWARD-DOUBLING uses i = 

n in both cases.

algorithm  we consider a ﬁxed choice of parameter settings and then look at how regret changes as
we vary the comparator point ˚x.
Gradient descent is minimax-optimal [1] when the comparator point is contained in a hypershere
whose radius is known in advance ((cid:107)˚x(cid:107)2 ≤ R) and gradients are sparse ((cid:107)gt(cid:107)2 ≤ 1  top table).
Exponentiated gradient descent excels when gradients are dense ((cid:107)gt(cid:107)∞ ≤ 1  bottom table) but the
comparator point is sparse ((cid:107)˚x(cid:107)1 ≤ R for R known in advance). In both these cases  the bounds for
REWARD-DOUBLING match those of the previous algorithms up to logarithmic factors  even when
they are tuned optimally with knowledge of R.
The advantage of REWARD-DOUBLING shows up when the guess of R used to tune the compet-
√
ing algorithms turns out to be wrong. When ˚x = 0  REWARD-DOUBLING offers constant regret
T ) for the other algorithms. When ˚x can be arbitrary  only REWARD-DOUBLING
compared to Ω(
offers sub-linear regret (and in fact its regret bound is optimal  as shown in Theorem 8).
In order to guarantee constant origin-regret  REWARD-DOUBLING frequently “jumps” back to
playing the origin  which may be undesirable in some applications.
In Section 4 we introduce
SMOOTH-REWARD-DOUBLING  which achieves similar guarantees without resetting to the origin.

Related Work Our work is related  at least in spirit  to the use of a momentum term in stochastic
gradient descent for back propagation in neural networks [7  11  9]. These results are similar in
motivation in that they effectively yield a larger learning rate when many recent gradients point in
the same direction.
In Follow-The-Regularized-Leader terms  the exponentiated gradient descent algorithm with unnor-
η (x log x − x) 
malized weights of Kivinen and Warmuth [8] plays xt+1 = arg minx∈Rn
which has closed-form solution xt+1 = exp(−ηg1:t). Like our algorithm  this algorithm moves
away from the origin exponentially fast  but unlike our algorithm it can incur arbitrarily large regret
with respect to ˚x = 0. Theorem 9 shows that no algorithm of this form can provide bounds like the
ones proved in this paper.
Hazan and Kale [5] give regret bounds in terms of the variance of the gt. Letting G = |g1:t| and
V ) where V = H − G2/T . This result
√
H − V   and so if we hold H constant  then
has some similarity to our work in that G/
when V is low  the critical ratio G/
T that appears in our bounds is large. However  they consider
the case of a known feasible set  and their algorithm (gradient descent with a constant learning rate)
cannot obtain bounds of the form we prove.

√
t   they prove regret bounds of the form O(

H =(cid:80)T

g1:t · x + 1

t=1 g2

T =

√

√

+

2 Reward and Regret

In this section we present a general result that converts lower bounds on reward into upper bounds
on regret  for one-dimensional online linear optimization. In the unconstrained setting  this result
will be sufﬁcient to provide guarantees for general n-dimensional online convex optimization.

3

Theorem 1. Consider an algorithm for one-dimensional online linear optimization that  when run
on a sequence of gradients g1  g2  . . .   gT   with gt ∈ [−1  1] for all t  guarantees

where γ  κ > 0 and  ≥ 0 are constants. Then  against any comparator ˚x ∈ [−R  R]  we have

Reward ≥ κ exp (γ|g1:T|) −  

(cid:18)

(cid:18) R

(cid:19)

κγ

(cid:19)

− 1

+  

Regret(˚x) ≤ R
γ

log

(1)

(2)

letting 0 log 0 = 0 when R = 0. Further  any algorithm with the regret guarantee of Eq. (2) must
guarantee the reward of Eq. (1).

We give a proof of this theorem in the appendix. The duality between reward and regret can also be
seen as a consequence of the fact that exp(x) and y log y − y are convex conjugates. The γ term
typically contains a dependence on T like 1/
T . This bound holds for all R  and so for some small
R the log term becomes negative; however  for real algorithms the  term will ensure the regret
bound remains positive. The minus one can of course be dropped to simplify the bound further.

√

3 Gradient Descent with Increasing Learning Rates

In this section we show that allowing the learning rate of gradient descent to sometimes increase
leads to novel theoretical guarantees.
To build intuition  consider online linear optimization in one dimension  with gradients
g1  g2  . . .   gT   all in [−1  1]. In this setting  the reward of unconstrained gradient descent has a
simple closed form:
Lemma 2. Consider unconstrained gradient descent in one dimension  with learning rate η. On
t   the

round t  this algorithm plays the point xt = ηg1:t−1. Letting G = |g1:t| and H = (cid:80)T

t=1 g2

cumulative reward of the algorithm is exactly

(cid:0)G2 − H(cid:1) .

Reward =

η
2

We give a simple direct proof in Appendix A. Perhaps surprisingly  this result implies that the reward
is totally independent of the order of the linear functions selected by the adversary. Examining the
expression in Lemma 2  we see that the optimal choice of learning rate η depends fundamentally on
two quantities: the absolute value of the sum of gradients (G)  and the sum of the squared gradients
(H). If G2 > H  we would like to use as large a learning rate as possible in order to maximize
reward. In contrast  if G2 < H  the algorithm will obtain negative reward  and the best it can do is
to cut its losses by setting η as small as possible.
One of the motivations for this work is the observation that the state-of-the-art online gradient de-
scent algorithms adjust their learning rates based only on the observed value of H (or its upper bound
T ); for example [4  10]. We would like to increase reward by also accounting for G. But unlike H 
which is monotonically increasing with time  G can both increase and decrease. This makes simple
guess-and-doubling tricks fail when applied to G  and necessitates a more careful approach.

3.1 Analysis in One Dimension

series of epochs. We suppose for the moment that an upper bound ¯H on H =(cid:80)T
Lemma 3. Applied to a sequence of gradients g1  g2  . . .   gT   all in [−1  1]  where H =(cid:80)T

In this section we analyze algorithm REWARD-DOUBLING-1D (Algorithm 1)  which consists of a
t is known
in advance. In the ﬁrst epoch  we run gradient descent with a small initial learning rate η = η1.
Whenever the total reward accumulated in the current epoch reaches η ¯H  we double η and start a
new epoch (returning to the origin and forgetting all previous gradients except the most recent one).
t ≤

t=1 g2

t=1 g2

¯H  REWARD-DOUBLING-1D obtains reward satisfying

− η1 ¯H 

(3)

Reward =

√
for a = log(2)/

3.

t=1

(cid:19)

(cid:18)

|g1:T|√
¯H

a

T(cid:88)

xtgt ≥ 1
4

η1 ¯H exp

4

Algorithm 1 REWARD-DOUBLING-1D

bound ¯H ≥(cid:80)T

Parameters: initial learning rate η1  upper
t .
t=1 g2
Initialize x1 ← 0  i ← 1  and Q1 ← 0.
for t = 1  2  . . .   T do

Play xt  and receive reward xtgt.
Qi ← Qi + xtgt.
if Qi < ηi ¯H then

else

xt+1 ← xt + ηigt.
i ← i + 1.
ηi ← 2ηi−1; Qi ← 0.
xt+1 ← 0 + ηigt.

Algorithm 2 REWARD-DOUBLING

Parameters: maximum origin-regret i
for 1 ≤ i ≤ n.
for i = 1  2  . . .   n do

a

Let Ai be
REWARD-DOUBLING-1D-GUESS
(see Theorem 4)  with parameter i.

copy of

algorithm

for t = 1  2  . . .   T do

Play xt  with xt i selected by Ai.
Receive gradient vector gt = −(cid:79)ft(xt).
for i = 1  2  . . .   n do
Feed back gt i to Ai.

Proof. Suppose round T occurs during the k’th epoch. Because epoch i can only come to an end if
Qi ≥ ηi ¯H  where ηi = 2i−1η1  we have

k(cid:88)

(cid:32)k−1(cid:88)

Reward =

Qi ≥

2i−1η1 ¯H

i=1

i=1

(cid:33)

+ Qk =(cid:0)2k−1 − 1(cid:1) η1 ¯H + Qk .

(4)

We now lower bound Qk. For i = 1  . . .   k let ti denote the round on which Qi is initialized to 0 
with t1 ≡ 1  and deﬁne tk+1 ≡ T . By construction  Qi is the total reward of a gradient descent
algorithm that is active on rounds ti through ti+1 inclusive  and that uses learning rate ηi (note that
on round ti  this algorithm gets 0 reward and we initialize Qi to 0 on that round). Thus  by Lemma
2  we have that for any i 

ηi
2

Qi =

(cid:33)

(cid:32)
(gti:ti+1)2 −

ti+1(cid:88)
2 ηk ¯H = −2k−2η1 ¯H. Substituting into (4) gives
(5)
. At the end of round ti+1 − 1  we must have had Qi < ηi ¯H (otherwise

≥ − ηi
2

Reward ≥ η1 ¯H(2k−1 − 1 − 2k−2) = η1 ¯H(2k−2 − 1) .

¯H .

g2
s

s=ti

Applying this bound to epoch k  we have Qk ≥ − 1

We now show that k ≥ |g1:T |√
epoch i + 1 would have begun earlier). Thus  again using Lemma 2 

3 ¯H

so |gti:ti+1−1| ≤

√

3 ¯H. Thus 

ηi
2

(cid:0)(gti:ti+1−1)2 − ¯H(cid:1) ≤ ηi ¯H
(cid:112)

|gti:ti+1−1| ≤ k

|g1:T| ≤ k(cid:88)

3 ¯H .

(cid:32)

(cid:112) ¯H

√

(cid:32)

√
4Rb
η1

¯H

(cid:33)

(cid:33)

− 1

5

Rearranging gives k ≥ |g1:T |√

3 ¯H

i=1

  and combining with Eq. (5) proves the lemma.

We can now apply Theorem 1 to the reward (given by Eq. (3)) of REWARD-DOUBLING-1D to show

Regret(˚x) ≤ bR

log

+ η1 ¯H

(6)

for any ˚x ∈ [−R  R]  where b = a−1 =
3/ log(2) < 2.5. When the feasible set is also ﬁxed in
advance  online gradient descent with a ﬁxed learning obtains a regret bound of O(R
T ). Suppose
we use the estimate ¯H = T . By choosing η1 = 1
T   we guarantee constant regret against the origin 
˚x = 0 (equivalently  constant total loss). Further  for any feasible set of radius R  we still have

√

√

T log((1 + R)T ))  which is only modestly worse than that of

worst-case regret of at most O(R
gradient descent with the optimal R known in advance.
The need for an upper bound ¯H can be removed using a standard guess-and-doubling approach  at
the cost of a constant factor increase in regret (see appendix for proof).
Theorem 4. Consider algorithm REWARD-DOUBLING-1D-GUESS  which behaves as follows. On
each era i  the algorithm runs REWARD-DOUBLING-1D with an upper bound of ¯Hi = 2i−1  and
1 = 2−2i. An era ends when ¯Hi is no longer an upper bound on the sum of
initial learning rate ηi
squared gradients seen during that era. Letting c =

  this algorithm has regret at most

(cid:18)

√
2√
2−1

(cid:18) R



(cid:19)

(cid:19)

H + 1

log

(2H + 2)5/2

− 1

+ .

Regret ≤ cR

√

3.2 Extension to n dimensions

To extend our results to general online convex optimization  it is sufﬁcient to run a separate copy of
REWARD-DOUBLING-1D-GUESS for each coordinate  as is done in REWARD-DOUBLING (Algo-
rithm 2). The key to the analysis of this algorithm is that overall regret is simply the sum of regret
on n one-dimensional subproblems which can be analyzed independently.
Theorem 5. Given a sequence of convex loss functions f1  f2  . . .   fT from Rn to R 
REWARD-DOUBLING with i = 
Regret(˚x) ≤  + c

n has regret bounded by

log

Hi + 1

(cid:16) n

(cid:16)
n(cid:88)
|˚xi|(cid:112)
(cid:16)
(cid:16) n

(cid:107)˚x(cid:107)2
t i and H =(cid:80)T
log
t=1 (cid:107)gt(cid:107)2
2.

H + n

√

i=1



|˚xi|(2Hi + 2)5/2(cid:17) − 1
(cid:17)
2(2H + 2)5/2(cid:17) − 1
(cid:17)

≤  + c(cid:107)˚x(cid:107)2

  where Hi =(cid:80)T

t=1 g2

for c =

√
2√
2−1

Proof. Fix a comparator ˚x. For any coordinate i  deﬁne

Observe that

Regreti =

n(cid:88)

i=1

Regreti =

T(cid:88)

t=1

xt igt i .

˚xigt i − T(cid:88)
T(cid:88)
˚x · gt − T(cid:88)

t=1

t=1

t=1

xt · gt = Regret(˚x) .

Furthermore  Regreti is simply the regret of REWARD-DOUBLING-1D-GUESS on the gradient se-
quence g1 i  g2 i  . . .   gT i. Applying the bound of Theorem 4 to each Regreti term completes the
√
proof of the ﬁrst inequality. For the second inequality  let (cid:126)H be a vector whose ith component is
Hi + 1  and let (cid:126)x ∈ Rn where (cid:126)xi = |˚xi|. Using the Cauchy-Schwarz inequality  we have

n(cid:88)

|˚xi|(cid:112)

Hi + 1 = (cid:126)x · (cid:126)H ≤ (cid:107)˚x(cid:107)2 (cid:107) (cid:126)H(cid:107)2 = (cid:107)˚x(cid:107)2

H + n .

√

i=1

This  together with the fact that log(|˚xi|(2Hi + 2)5/2) ≤ log((cid:107)˚x(cid:107)2
second inequality.

2(2H + 2)5/2)  sufﬁces to prove

In some applications  n is not known in advance.
coordinate we encounter  and get the same bound up to constant factors.

In this case  we can set i = 

i2 for the ith

4 An Epoch-Free Algorithm

In this section we analyze SMOOTH-REWARD-DOUBLING  a simple algorithm that achieves bounds
comparable to those of Theorem 4  without guessing-and-doubling. We consider only the 1-d prob-
lem  as the technique of Theorem 5 can be applied to extend to n dimensions. Given a parameter

6

η > 0  we achieve

√

Regret ≤ R

(cid:18)

(cid:18) RT 3/2

(cid:19)

(cid:19)

− 1

T

log

(7)
for all T and R  which is better (by constant factors) than Theorem 4 when gt ∈ {−1  1} (which
implies T = H). The bound can be worse on a problems where H < T .
The idea of the algorithm is to maintain the invariant that our cumulative reward  as a function of
g1:t and t  satisﬁes Reward ≥ N (g1:t  t)  for some ﬁxed function N. Because reward changes by
gtxt on round t  it sufﬁces to guarantee that for any g ∈ [−1  1] 

+ 1.76η 

η

N (g1:t  t) + gxt+1 ≥ N (g1:t + g  t + 1)

where xt+1 is the point the algorithm plays on round t + 1  and we assume N (0  1) = 0.
This inequality is approximately satisﬁed (for small g) if we choose

xt+1 =

∂N (g1:t + g  t)

∂g

≈ N (g1:t + g  t) − N (g1:t  t)

≈ N (g1:t + g  t + 1) − N (g1:t  t)

This suggests that if we want to maintain reward at least N (g1:t  t) = 1
should set xt+1 ≈ sign(g1:t)t−3/2 exp
provides an inductive analysis of an algorithm of this form.
Theorem 6. Fix a sequence of reward functions ft(x) = gtx with gt ∈ [−1  1]  and let Gt = |g1:t|.
We consider SMOOTH-REWARD-DOUBLING  which plays 0 on round 1 and whenever Gt = 0;
otherwise  it plays

t) − 1)   we
. The following theorem (proved in the appendix)

t

g

√
t (exp(|g1:t|/

g

(cid:16)|g1:t|√

(cid:17)

xt+1 = η sign(g1:t)B(Gt  t + 5)

(8)

.

(9)

with η > 0 a learning-rate parameter and

1
t3/2
Then  at the end of each round t  this algorithm has

B(G  t) =

Reward(t) ≥ η

1

exp

t + 5

exp

(cid:18) G√
(cid:18) Gt√

t

(cid:19)
(cid:19)

.

t + 5

(10)

− 1.76η.

√

Two main technical challenges arise in the proof: ﬁrst  we prove a result like Eq. (8) for N (g1:t  t) =

t(cid:1). However  this Lemma only holds for t ≥ 6 and when the sign of g1:t doesn’t

(1/t) exp(cid:0)|g1:t|/

change. We account for this by showing that a small modiﬁcation to N (costing only a constant over
all rounds) sufﬁces.
By running this algorithm independently for each coordinate using an appropriate choice of η  one
can obtain a guarantee similar to that of Theorem 5.

5 Lower Bounds

As with our previous results  it is sufﬁcient to show a lower bound in one dimension  as it can then
lower bound contains the factor log(|˚x|√
be replicated independently in each coordinate to obtain an n dimensional bound. Note that our
T )  which can be negative when ˚x is small relative to T  
hence it is important to hold ˚x ﬁxed and consider the behavior as T → ∞. Here we give only a
proof sketch; see Appendix A for the full proof.
Theorem 7. Consider the problem of unconstrained online linear optimization in one dimension 
and an online algorithm that guarantees origin-regret at most . Then  for any ﬁxed comparator ˚x 
and any integer T0  there exists a gradient sequence {gt} ∈ [−1  1]T of length T ≥ T0 for which
the algorithm’s regret satisﬁes

Regret(˚x) ≥ 0.336|˚x|

(cid:118)(cid:117)(cid:117)(cid:116)T log

(cid:32)|˚x|√

T

(cid:33)

.



7

Proof. (Sketch) Assume without loss of generality that ˚x > 0. Let Q be the algorithm’s reward
when each gt is drawn independently uniformly from {−1  1}. We have E[Q] = 0  and because the
algorithm guarantees origin-regret at most   we have Q ≥ − with probability 1. Letting G = g1:T  
it follows that for any threshold Z = Z(T ) 

0 = E[Q]
= E[Q|G < Z] · Pr[G < Z] + E[Q|G ≥ Z] · Pr[G ≥ Z]
≥ − Pr[G < Z] + E[Q|G ≥ Z] · Pr[G ≥ Z]
> − + E[Q|G ≥ Z] · Pr[G ≥ Z] .

Equivalently 

√

E[Q|G ≥ Z] <
√

(cid:106)



.
Pr[G ≥ Z]
)/ log(p−1)

(cid:107)

. Here R = |˚x| and p > 0 is a

We choose Z(T ) =
constant chosen using binomial distribution lower bounds so that Pr[G ≥ Z] ≥ pk. This implies

E[Q|G ≥ Z] < p−k =  exp(cid:0)k log p−1(cid:1) ≤ R

kT   where k =

log( R

√

T .

T



√

√

√

kT − R

This implies there exists a sequence with G ≥ Z and Q < R
G˚x − Q ≥ R
Theorem 8. Consider the problem of unconstrained online linear optimization in Rn  and consider
an online algorithm that guarantees origin-regret at most . For any radius R  and any T0  there ex-
ists a gradient sequence gradient sequence {gt} ∈ ([−1  1]n)T of length T ≥ T0  and a comparator
˚x with (cid:107)˚x(cid:107)1 = R  for which the algorithm’s regret satisﬁes

T . On this sequence  regret is at least

T = Ω(R

kT ).

√

Regret(˚x) ≥ 0.336

|˚xi|

n(cid:88)

i=1

T(cid:88)

˚xigt i − T(cid:88)

t=1

t=1

xt igt i ≥ 0.336|˚xi|

(cid:33)

T

.

(cid:118)(cid:117)(cid:117)(cid:116)T log
(cid:32)|˚xi|√
(cid:118)(cid:117)(cid:117)(cid:116)T log



(cid:32)|˚xi|√

T

(cid:33)

.



Proof. For each coordinate i  Theorem 7 implies that there exists a T ≥ T0 and a sequence of
gradients gt i such that

(The proof of Theorem 7 makes it clear that we can use the same T for all i.) Summing this
inequality across all n coordinates then gives the regret bound stated in the theorem.

The following theorem presents a stronger negative result for Follow-the-Regularized-Leader algo-
rithms with a ﬁxed regularizer: for any such algorithm that guarantees origin-regret at most T after
T rounds  worst-case regret with respect to any point outside [−T   T ] grows linearly with T .
Theorem 9. Consider a Follow-The-Regularized-Leader algorithm that sets

xt = arg min

x

(g1:t−1x + ψT (x))

where ψT is a convex  non-negative function with ψT (0) = 0. Let T be the maximum origin-regret
incurred by the algorithm on a sequence of T gradients. Then  for any ˚x with |˚x| > T   there exists a
2 (|˚x| − T ).
sequence of T gradients such that the algorithm’s regret with respect to ˚x is at least T−1
In fact  it is clear from the proof that the above result holds for any algorithm that selects xt+1 purely
as a function of g1:t (in particular  with no dependence on t).

6 Future Work

This work leaves open many interesting questions. It should be possible to apply our techniques
to problems that do have constrained feasible sets; for example  it is natural to consider the uncon-
strained experts problem on the positive orthant. While we believe this extension is straightforward 
handling arbitrary non-axis-aligned constraints will be more difﬁcult. Another possibility is to de-
velop an algorithm with bounds in terms of H rather than T that doesn’t use a guess and double
approach.

8

References
[1] Jacob Abernethy  Peter L. Bartlett  Alexander Rakhlin  and Ambuj Tewari. Optimal strategies

and minimax lower bounds for online convex games. In COLT  2008.

[2] Amit Agarwal  Elad Hazan  Satyen Kale  and Robert E. Schapire. Algorithms for portfolio

management based on the Newton method. In ICML  2006.

[3] Nicol`o Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge Uni-

versity Press  New York  NY  USA  2006. ISBN 0521841089.

[4] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning

and stochastic optimization. In COLT  2010.

[5] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by varia-

tion in costs. In COLT  2008.

[6] Elad Hazan and Satyen Kale. On stochastic and worst-case models for investing. In Advances

in Neural Information Processing Systems 22. 2009.

[7] Robert A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural

Networks  1987.

[8] Jyrki Kivinen and Manfred Warmuth. Exponentiated Gradient Versus Gradient Descent for

Linear Predictors. Journal of Information and Computation  132  1997.

[9] Todd K. Leen and Genevieve B. Orr. Optimal stochastic search and adaptive momentum. In

NIPS  1993.

[10] H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex

optimization. In COLT  2010.

[11] Barak Pearlmutter. Gradient descent: Second order momentum and saturating error. In NIPS 

1991.

[12] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning  4(2):107–194  2012.

[13] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In ICML  2003.

9

,Juho Lee
Seungjin Choi