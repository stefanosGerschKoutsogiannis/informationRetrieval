2018,Efficient Formal Safety Analysis of Neural Networks,Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving  aircraft collision avoidance  and malware detection. However  these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crash. Thus  there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample  i.e.  an input for which the network will violate the property. Unfortunately  most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper  we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.,Efﬁcient Formal Safety Analysis of Neural Networks

Shiqi Wang  Kexin Pei  Justin Whitehouse  Junfeng Yang  Suman Jana

Columbia University  NYC  NY 10027  USA

{tcwangshiqi  kpei  jaw2228  junfeng  suman}@cs.columbia.edu

Abstract

Neural networks are increasingly deployed in real-world safety-critical domains
such as autonomous driving  aircraft collision avoidance  and malware detection.
However  these networks have been shown to often mispredict on inputs with minor
adversarial or even accidental perturbations. Consequences of such errors can be
disastrous and even potentially fatal as shown by the recent Tesla autopilot crashes.
Thus  there is an urgent need for formal analysis systems that can rigorously check
neural networks for violations of different safety properties such as robustness
against adversarial perturbations within a certain L-norm of a given image. An
effective safety analysis system for a neural network must be able to either ensure
that a safety property is satisﬁed by the network or ﬁnd a counterexample  i.e. 
an input for which the network will violate the property. Unfortunately  most
existing techniques for performing such analysis struggle to scale beyond very
small networks and the ones that can scale to larger networks suffer from high
false positives and cannot produce concrete counterexamples in case of a property
violation. In this paper  we present a new efﬁcient approach for rigorously checking
different safety properties of neural networks that signiﬁcantly outperforms existing
approaches by multiple orders of magnitude. Our approach can check different
safety properties and ﬁnd concrete counterexamples for networks that are 10×
larger than the ones supported by existing analysis techniques. We believe that our
approach to estimating tight output bounds of a network for a given input range
can also help improve the explainability of neural networks and guide the training
process of more robust neural networks.

1

Introduction

Over the last few years  signiﬁcant advances in neural networks have resulted in their increasing
deployments in critical domains including healthcare  autonomous vehicles  and security. However 
recent work has shown that neural networks  despite their tremendous success  often make dangerous
mistakes  especially for rare corner case inputs. For example  most state-of-the-art neural networks
have been shown to produce incorrect outputs for adversarial inputs speciﬁcally crafted by adding
minor human-imperceptible perturbations to regular inputs [36  14]. Similarly  seemingly minor
changes in lighting or orientation of an input image have been shown to cause drastic mispredictions
by the state-of-the-art neural networks [29  30  37]. Such mistakes can have disastrous and even
potentially fatal consequences. For example  a Tesla car in autopilot mode recently caused a fatal
crash as it failed to detect a white truck against a bright sky with white clouds [3].
A principled way of minimizing such mistakes is to ensure that neural networks satisfy simple
safety/security properties such as the absence of adversarial inputs within a certain L-norm of a given
image or the invariance of the network’s predictions on the images of the same object under different
lighting conditions. Ideally  given a neural network and a safety property  an automated checker
should either guarantee that the property is satisﬁed by the network or ﬁnd concrete counterexamples
demonstrating violations of the safety property. The effectiveness of such automated checkers hinges
on how accurately they can estimate the decision boundary of the network.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

However  strict estimation of the decision boundary of a neural network with piecewise linear
activation functions such as ReLU is a hard problem. While the linear pieces of each ReLU node can
be partitioned into two linear constraints and efﬁciently check separately  the total number of linear
pieces grow exponentially with the number of nodes in the network [25  27]. Therefore  exhaustive
enumeration of all combinations of these pieces for any modern network is prohibitively expensive.
Similarly  sampling-based inference techniques like blackbox Monte Carlo sampling may need an
enormous amount of data to generate tight accurate bounds on the decision boundary [11].
In this paper  we propose a new efﬁcient approach for rigorously checking different safety properties
of neural networks that signiﬁcantly outperform existing approaches by multiple orders of magnitude.
Speciﬁcally  we introduce two key techniques. First  we use symbolic linear relaxation that combines
symbolic interval analysis and linear relaxation to compute tighter bounds on the network outputs
by keeping track of relaxed dependencies across inputs during interval propagation when the actual
dependencies become too complex to track. Second  we introduce a novel technique called directed
constraint reﬁnement to iteratively minimize the errors introduced during the relaxation process
until either a safety property is satisﬁed or a counterexample is found. To make the reﬁnement
process efﬁcient  we identify the potentially overestimated nodes  i.e.  the nodes where inaccuracies
introduced during relaxation can potentially affect the checking of a given safety property  and use
off-the-shelf solvers to focus only on those nodes to further tighten their output ranges.
We implement our techniques as part of Neurify  a system for rigorously checking a diverse set
of safety properties of neural networks 10× larger than the ones that can be handled by existing
techniques. We used Neurify to check six different types of safety properties of nine different
networks trained on ﬁve different datasets. Our experimental results show that on average Neurify is
5  000× faster than Reluplex [17] and 20× than ReluVal [39].
Besides formal analysis of safety properties  we believe our method for efﬁciently estimating tight
and rigorous output ranges of a network will also be useful for guiding the training process of robust
networks [42  32] and improving explainability of the decisions made by neural networks [34  20  23].
Related work. Several researchers have tried to extend and customize Satisﬁability Modulo Theory
(SMT) solvers for estimating decision boundaries with strong guarantees [17  18  15  10  31]. Another
line of research has used Mixed Integer Linear Programming (MILP) solvers for such analysis [38 
12  7]. Unfortunately  the efﬁciency of both of these approaches is severely limited by the high
nonlinearity of the resulting formulas.
Different convex or linear relaxation techniques have also been used to strictly approximate the
decision boundary of neural networks. While these techniques tend to scale signiﬁcantly better than
solver-based approaches  they suffer from high false positive rates and struggle to ﬁnd concrete
counterexamples demonstrating violations of safety properties [42  32  13  9]. Similarly  existing
works on ﬁnding lower bounds of adversarial perturbations to fool a neural network also suffer
from the same limitations [28  41]. Note that concurrent work of Weng et al. [40] uses similar
linear relaxation method as ours but it alone struggles to solve such problems as shown in Table
6. Also  their follow-up work [44] that provides a generic relaxation method for general activation
functions does not address this issue either. In contrast  we mainly use our relaxation technique to
identify crucial nodes and iteratively reﬁne output approximations over these nodes with the help of
linear solver. Another line of research has focused on strengthening network robustness either by
incorporating these relaxation methods into training process [43  8  24] or by leveraging techniques
like differential privacy [22]. Our method  essentially providing a more accurate formal analysis of a
network  can potentially be incorporated into training process to further improve network robustness.
Recently  ReluVal  by Wang et al. [39]  has used interval arithmetic [33] for rigorously estimating
a neural network’s decision boundary by computing tight bounds on the outputs of a network for
a given input range. While ReluVal achieved signiﬁcant performance gain over the state-of-the-art
solver-based methods [17] on networks with a small number of inputs  it struggled to scale to larger
networks (see detailed discussions in Section 2).

2 Background

We build upon two prior works [10  39] on using interval analysis and linear relaxations for analyzing
neural networks. We brieﬂy describe them and refer interested readers to [10  39] for more details.

2

Symbolic interval analysis. Interval arithmetic [33] is a ﬂexible and efﬁcient way of rigorously
estimating the output ranges of a function given an input range by computing and propagating the
output intervals for each operation in the function. However  naive interval analysis suffers from
large overestimation errors as it ignores the input dependencies during interval propagation. To
minimize such errors  Wang et al. [39] used symbolic intervals to keep track of dependencies by
maintaining linear equations for upper and lower bounds for each ReLU and concretizing only for
those ReLUs that demonstrate non-linear behavior for the given input intervals. Speciﬁcally  consider
an intermediate ReLU node z = Relu(Eq)  (l  u) = (Eq  Eq)  where Eq denotes the symbolic
representation (i.e.  a closed-form equation) of the ReLU’s input in terms of network inputs X and
(l  u) denote the concrete lower and upper bounds of Eq  respectively. There are three possible output
intervals that the ReLU node can produce depending on the bounds of Eq: (1) z = [Eq  Eq] when
l ≥ 0  (2) z = [0  0] when u ≤ 0  or (3) z = [l  u] when l < 0 < u. Wang et al. will concretize the
output intervals for this node only if the third case is feasible as the output in this case cannot be
represented using a single linear equation.
Bisection of input features. To further minimize overestimation  [39] also proposed an iterative
reﬁnement strategy involving repeated input bisection and output reunion. Consider a network
F taking d-dimensional input  and the i-th input feature interval is Xi and network output in-
terval is F (X) where X = {X1  ...  Xd}. A single bisection on Xi will create two children:
X′ = {X1  ...  [Xi  Xi+Xi
2   Xi]  ...  Xd}. The reunion of the
corresponding output intervals F (X′)! F (X′′)  will be tighter than the original output interval  i.e. 
F (X′)! F (X′′) ⊆ F (X)  as the Lipschitz continuity of the network ensures that the overestimation

error decreases as the width of input interval becomes smaller. However  the efﬁciency of input
bisection decreases drastically as the number of input dimensions increases.
Linear relaxation. Ehlers et al. [10] used lin-
ear relaxation of ReLU nodes to strictly over-
approximate the non-linear constraints intro-
duced by each ReLU. The generated linear con-
straints can then be efﬁciently solved using a
linear solver to get bounds on the output of a
neural network for a given input range. Consider
the simple ReLU node taking input z′ with an
upper and lower bound u and l respectively and
producing output z as shown in Figure 1. Linear relaxation of such a node will use the following
three linear constraints: (1) z ≥ 0  (2) z ≥ z′  and (3) z ≤ u(z′−l)
to expand the feasible region to the
u−l
green triangle from the two original piecewise linear components. The effectiveness of this approach
heavily depends on how accurately u and l can be estimated. Unfortunately  Ehlers et al. [10] used
naive interval propagation to estimate u and l leading to large overestimation errors. Furthermore 
their approach cannot efﬁciently reﬁne the estimated bounds and thus cannot beneﬁt from increasing
computing power.

2 ]  ...  Xd} and X′′ = {X1  ...  [ Xi+Xi

Figure 1: Linear relaxation of a ReLU node.

3 Approach

In this paper  we make two major contributions to scale formal safety analysis to networks signiﬁcantly
larger than those evaluated in prior works [17  10  42  39]. First  we combine symbolic interval
analysis and linear relaxation (described in Section 2) in a novel way to create a signiﬁcantly
more efﬁcient propagation method–symbolic linear relaxation–that can achieve substantially tighter
estimations (evaluated in Section 4). Second  we present a technique for identifying the overestimated
intermediate nodes  i.e.  the nodes whose outputs are overestimated  during symbolic linear relaxation
and propose directed constraint reﬁnement to iteratively reﬁne the output ranges of these nodes. In
Section 4  we also show that this method mitigates the limitations of input bisection [39] and scales
to larger networks.
Figure 2 illustrates the high-level workﬂow of Neurify. Neurify takes in a range of inputs X and
then determines using linear solver whether the output estimation generated by symbolic linear
relaxation satisﬁes the safety proprieties. A property is proven to be safe if the solver ﬁnd the
relaxed constraints unsatisﬁable. Otherwise  the solver returns potential counterexamples. Note that
the returned counterexamples found by the solver might be false positives due to the inaccuracies

3

introduced by the relaxation process. Thus Neurify will check whether a counterexample is a false
positive. If so  Neurify will use directed constraint reﬁnement guided by symbolic linear relaxation
to obtain a tighter output bound and recheck the property with the solver.

3.1 Symbolic Linear Relaxation
The symbolic linear relaxation of the output of each
ReLU z = Relu(z′) leverages the bounds on z′ 
Eqlow and Equp (Eqlow ≤ Eq∗(x) ≤ Equp). Here
Eq∗ denotes the closed-form representation of z′.
Speciﬁcally  Equation 1 shows the symbolic linear
relaxation where %→ denotes “relax to”. In addition 
[llow  ulow] and [lup  uup] denote the concrete lower
and upper bounds for Eqlow and Equp  respectively.
In supplementary material Section 1.2  we give a de-
tailed proof showing that this relaxation is the tightest
achievable due to its least maximum distance from
Eq∗. In the following discussion  we simplify Eqlow
and Equp as Eq and the corresponding lower and
upper bounds as [l  u]. Figure 3 shows the differ-
ence between our symbolic relaxation process and
the naive concretizations used by Wang et al. [39].
More detailed discussions can be found in supple-
mentary material Section 2.

Relu(Eqlow) %→

ulow

(Eqlow)

Input intervals

Symbolic linear 

relaxation
Split
target
node

Safe

Unsat

Constraints

Linear solver

DNN

Safety
property

Concrete
sample

Refine 

overest. node

False positive

Timeout

Check for 
violation

Violated

Unsafe

Figure 2: Workﬂow of Neurify to formally
analyze safety properties of neural networks.

uup

z ≤ u 

l 

Relu(Equp) %→
z 

(Equp −lup)
z 

(1)

z ≤

z′

z ≥ 0 

0

u

z ≥

u
Eq 
u - l
z′ 

uup −lup

u
u - l (Eq - l) 

ulow −llow
In practice  symbolic linear relaxation
can cut (on average) 59.64% more
overestimation error than symbolic in-
terval analysis (cf. Section 2) and
saves the time needed to prove a prop-
erty by several orders of magnitude
(cf. Section 4). There are three key
reasons behind such signiﬁcant per-
formance improvement. First  the
maximum possible error after intro-
ducing relaxations is −lup∗uup
for up-
uup−lup
per bound and −llow∗ulow
for lower
ulow−llow
bound in Figure 3(b) (the proof is in
supplementary material Section 1.2).
These relaxations are considerably
tighter than naive concretizations shown in Figure 3(a)  which introduces a larger error uup. Second 
symbolic linear relaxation  unlike naive concretization  partially keeps the input dependencies during
interval propagation ([ u
u−l (Eq −l)] by maintaining symbolic equations. Third  as the ﬁnal
output error is exponential to the error introduced at each node (proved in supplementary 1.2)  tighter
bounds on earlier nodes produced by symbolic relaxation signiﬁcantly reduce the ﬁnal output error.

Figure 3: An illustration of symbolic linear relaxation for
an intermediate node. (a) Original symbolic interval anal-
ysis [39] used naive concretization.
(b) Symbolic linear
relaxation leverages the knowledge of concrete bounds for z′
and computes relaxed symbolic interval. Eq is the symbolic
representation of z′.

(b) Symbolic linear relaxation

u−l Eq  u

(a) Naive concretizaion

l

0

u 

3.2 Directed Constraint Reﬁnement
Besides symbolic linear relaxation  we also develop another generic approach  directed constraint
reﬁnement  to further improve the overall performance of property checking. Our empirical results
in Section 4 shows the substantial improvement from using this approach combined with symbolic
linear relaxation. In the following  we ﬁrst deﬁne overestimated nodes before describing the directed
constraint reﬁnement process in detail.
Overestimated nodes. We note that  for most networks  only a small proportion of intermediate
ReLU nodes operate in the non-linear region for a given input range X. These are the only nodes that

4

need to be relaxed (cf. Section 2). We call these nodes overestimated as they introduce overestimation
error during relaxation. We include other useful properties and proofs regarding overestimated nodes
in supplementary material Section 1.1.
Based on the deﬁnition of overestimated nodes  we deﬁne one step of directed constraint reﬁnement
as computing the reﬁned output range F ′(X):

F ′(X) = F (x ∈ X|Eq(x) ≤ 0) ∪ F (x ∈ X|Eq(x) > 0)

(2)
where X denotes the input intervals to the network  F is the corresponding network  and Eq is the
input equation of an overestimated node. Note that here we are showing the input of a node as a
single equation for simplicity instead of the upper and lower bounds shown in Section 3.1.
We iteratively reﬁne the bounds by invoking a linear solver  allowing us to make Neurify more
scalable for difﬁcult safety properties. The convergence analysis is given in supplementary material
Section 1.3.
The reﬁnement includes the following three steps:
Locating overestimated nodes. From symbolic linear relaxations  we can get the set of overestimated
nodes within the network. We then prioritize the overestimated nodes with larger output gradient
and reﬁne these inﬂuential overestimated nodes ﬁrst. We borrow the idea from [39] of computing
the gradient of network output with respect to the input interval of the overestimated node. A larger
gradient value of a node signiﬁes that the input of that node has a greater inﬂuence towards changing
the output than than the inputs of other nodes.
Splitting. After locating the target overestimated node  we split its input ranges into two independent
cases  Eqt > 0 and Eqt ≤ 0 where Eqt denotes the input of the target overestimated node. Now 
unlike symbolic linear relaxation where Relu([Eqt  Eqt]) %→ [ u
u−l (Eqt −l)]  neither of
the two split cases requires any relaxation (Section 2) as the input interval no longer includes 0.
Therefore  splitting creates two tighter approximations of the output F (x ∈ X|Eqt(x) > 0) and
F (x ∈ X|Eqt(x) ≤ 0).
Solving. We solve the resulting linear constraints  along with the constraints deﬁned in safety
properties  by instantiating an underlying linear solver. In particular  we deﬁne safety properties that
check that the conﬁdence value of a target output class F t is always greater than the outputs of other
classes F o (e.g.  outputs other than 7 for an image of a hand-written 7). We thus deﬁne the constraints
for safety properties as Eqt
up are the lower bound equations
for F t and the upper bound equations for F o derived using symbolic linear relaxation. Each step
of directed constraint reﬁnement of an overestimated node results in two independent problems as
shown in Equation 3 that can be checked with a linear solver.

u−l Eqt  u

up < 0. Here  Eqt

low and Eqo

low −Eqo

Check Satiﬁability: Eqt
Check Satiﬁability: Eqt

low1−Eqo
low2−Eqo

up1 < 0; Eqt ≤ 0; xi −ϵ ≤ xi ≤ xi + ϵ (i = 1 . . . d)
up2 < 0; Eqt > 0; xi −ϵ ≤ xi ≤ xi + ϵ (i = 1 . . . d)

(3)

In this process  we invoke the solver in two ways. (1) If the solver tells that both cases are unsatisﬁable 
then the property is formally proved to be safe. Otherwise  further iterative reﬁnement steps can be
applied. (2) If either case is satisﬁable  we treat the solutions returned by the linear solver as potential
counterexamples violating the safety properties. Note that these solutions might be false positives
due to the inaccuracies introduced during the relaxation process. We thus resort to directly executing
the target network with the solutions returned from the solver as input. If the solution does not violate
the property  we repeat the above process for another overestimated node (cf. Figure 2).

3.3 Safety Properties
In this work  we support checking diverse safety properties of networks including ﬁve different
classes of properties based on the input constraints. Particularly  we specify the safety properties of
neural network based on deﬁning constraints on its input-output. For example  as brieﬂy mentioned
in Section 3.1  we specify that the output of the network on input x should not change (i.e.  remain
invariant) when x is allowed to vary within a certain range X. For output constraints  taking an
arbitrary classiﬁer as an example  we deﬁne the output invariance by specifying the difference
greater than 0 between lower and upper bound of conﬁdence value of the original class of the input
and other classes. For specifying input constraints  we consider three popular bounds  i.e.  L∞ 

5

L1  and L2  which are widely used in the literature of adversarial machine learning [14]. These
three bounds allow for arbitrary perturbations of the input features as long as the corresponding
norms of the overall perturbation are within a certain threshold.
In addition to these arbitrary
perturbations  we consider two speciﬁc perturbations that change brightness and contrast of the
input images as discussed in [30]. Properties speciﬁed using L∞ naturally ﬁt into our symbolic
linear relaxation process where each input features are bounded by an interval. For properties
i=1|xi|≤ ϵ for L1  or
2≤ ϵ for L2  which are no longer linear. We handle such cases by using solvers that support
quadratic constraints (see details in Section 4). The safety properties involving changes in brightness
and contrast can be efﬁciently checked by iteratively bisecting the input nodes simultaneously as
minx∈[x−ϵ x+ϵ](F (x)) = min(minx∈[x x+ϵ](F (x))  minx∈[x−ϵ x](F (x))) where F represents the
computation performed by the target network .

speciﬁed in L1 ≤ ϵ or L2 ≤ ϵ  we need to add more constraints  i.e. "d
"d

i=1xi

4 Experiments

Implementation. We implement Neurify with about 26 000 lines of C code. We use the highly
optimized OpenBLAS1 library for matrix multiplications and lp_solve 5.52 for solving the linear
constraints generated during the directed constraint reﬁnement process. We further use Gurobi 8.0.0
solver for L2-bounded safety properties. All our evaluations were performed on a Linux server
running Ubuntu 16.04 with 8 CPU cores and 256GB memory. Besides  Neurify uses optimization
like thread rebalancing for parallelization and outward rounding to avoid incorrect results due to
ﬂoating point imprecision. Details of such techniques can be found in Section 3 of the supplementary
material.
Table 1: Details of the evaluated networks and corresponding safety properties. The last three columns
summarize the number of safety properties that are satisﬁed  violated  and timed out  respectively as
found by Neurify with a timeout threshold of 1 hour.
Architecture
<5  50  50  50 
50  50  50  5>#

Safe Violated Timeout

Dataset
ACAS
Xu [16]

# of
ReLUs

Models

300
ACAS Xu
48
MNIST_FC1
100
MNIST_FC2
MNIST_FC3
1024
MNIST_CN 4804
Drebin_FC1
100
210
Drebin_FC2
Drebin_FC3
400

MNIST [21]

Drebin [5]

Safety
Property

C.P.∗
in [39]
L∞
L∞
L∞
L∞
C.P.∗
in [29]
L∞ L1 
Brightness 
Contrast

<784  24  24  10>#
<784  50  50  10>#
<784  512  512  10>#
<784  k:16*4*4 s:2 

k:32*4*4 s:2  100  10>+

<545334  50  50  2>#
<545334  200  10  2>#
<545334  200  200  2>#
<30000  k:24*5*5 s:5 
k:36*5*5 s:5  100  10>+

141
267
271
322
91
458
437
297

37
233
194
41
476
21
22
27

0
0
35
137
233
21
41
176

Car [2]

DAVE

10276

80

82

58

* Custom properties.
# <x  y  ...> denotes hidden layers with x neurons in ﬁrst layer  y neurons in second layer  etc.
+ k:c*w*h s:stride denotes the output channel (c)  kernel width (w)  height (h) and stride (stride).

4.1 Properties Checked by Neurify for Each Model

Summary. To evaluate the performance of Neurify  we test it on nine models trained over ﬁve
datasets for different tasks where each type of model includes multiple architectures. Speciﬁcally  we
evaluate on fully connected ACAS Xu models [16]  three fully connected Drebin models [5]  three
fully connected MNIST models [21]  one convolutional MNIST model [42]  and one convolutional
self-driving car model [2]. Table 1 summarizes the detailed structures of these models. We include
more detailed descriptions in supplementary material Section 4. All the networks closely follow
the publicly-known settings and are either pre-trained or trained ofﬂine to achieve comparable
performance to the real-world models on these datasets.

1https://www.openblas.net/
2http://lpsolve.sourceforge.net/5.5/

6

We also summarize the safety properties checked by Neurify in Table 1 with timeout threshold set to
3 600 seconds. Here we report the result of the self-driving care model (DAVE) to illustrate how we
deﬁne the safety properties and the numbers of safe and violated properties found by Neurify. We
report the other results in supplementary material Section 5.

Table 2: Different safety properties checked by Neurify out of 10 random images on Dave within
3600 seconds.

(a) ||X′ − X||∞ ≤ ϵ

ϵ
Safe(%)
Violated(%)
Timeout(%)

1
50
0
50

2
10
20
70

5
0
70
30

8
0
100
0

10
0
100
0

ϵ
Safe(%)
Violated(%)
Timeout(%)

(b) ||X′ − X||1 ≤ ϵ
300
100
100
10
40
0
0
50

200
100
0
0

500
10
50
40

700
0
60
40

(c) Brightness: X − ϵ ≤ X′ ≤ X + ϵ

ϵ
Safe(%)
Violated(%)
Timeout(%)

10
100
0
0

70
30
30
40

80
20
50
30

90
10
60
30

100
10
70
20

(d) Contrast: ϵX ≤ X′ ≤ X or X ≤ X′ ≤ ϵX
2.5
ϵ
0
Safe(%)
50
Violated(%)
Timeout(%)
50

1.01
100
0
0

0.99
100
0
0

0.5
10
20
70

0.2
0
70
30

Dave. We show that Neurify is the ﬁrst formal analysis tool that can systematically check different
safety properties for a large (over 10 000 ReLUs) convolutional self-driving car network  Dave [2  6].
We use the dataset from Udacity self-driving car challenge containing 101 396 training and 5 614
testing samples [4]. Our model’s architecture is similar to the DAVE-2 self-driving car architecture
from NVIDIA [6  2] and it achieves similar 1-MSE as models used in [29]. We formally analyze the
network with inputs bounded by L∞  L1  brightness  and contrast as described in Section 3.3. We
deﬁne the safe range of deviation of the output steering direction from the original steering angle to
be less than 30 degrees. The total number of cases Neurify can verify are shown in Table 2.

Table 3: Total cases that can be veriﬁed by Neurify on three Drebin models out of 100 random
malware apps. The timeout setting here is 3600 seconds.

Models

Drebin_FC1

Drebin_FC2

Drebin_FC3

Cases(%)

Safe

Violated

Violated

Total
Safe

Total
Safe

Violated

Total

10
0
100
100
0
100
100
0
100
100

50
1
98
99
4
96
100
4
89
93

100
3
97
100
4
90
94
4
74
78

150
5
86
91
6
81
87
4
23
33

200
12
77
89
8
70
78
15
11
26

DREBIN. We also evaluate Neurify on three different Drebin models containing 545 334 input
features. The safety property we check is that simply adding app permissions without changing any
functionality will not cause the models to misclassify malware apps as benign. Here we show in
Table 3 that Neurify can formally verify safe and unsafe cases for most of the apps within 3 600
seconds.

4.2 Comparisons with Other Formal Checkers

ACAS Xu. Unmanned aircraft alert systems (ACAS Xu) [19] are networks advising steering decisions
for aircrafts  which is on schedule to be installed in over 30 000 passengers and cargo aircraft
worldwide [26] and US Navy’s ﬂeets [1]. It is comparably small and only has ﬁve input features so
that ReluVal [39] can efﬁciently check different safety properties. However  its performance still
suffers from the over-approximation of output ranges due to the concretizations introduced during
symbolic interval analysis. Neurify leverages symbolic linear relaxation and achieves on average
20× better performance than ReluVal [39] and up to 5 000× better performance than Reluplex [17].
In Table 4  we summarize the time and speedup of Neurify compared to ReluVal and Reluplex for all
the properties tested in [17  39].

7

6 Acknowledgements

We thank the anonymous reviewers for their constructive and valuable feedback. This work is
sponsored in part by NSF grants CNS-16-17670  CNS-15-63843  and CNS-15-64055; ONR grants
N00014-17-1-2010  N00014-16-1- 2263  and N00014-17-1-2788; and a Google Faculty Fellowship.
Any opinions  ﬁndings  conclusions  or recommendations expressed herein are those of the authors 
and do not necessarily reﬂect those of the US Government  ONR  or NSF.

References
[1] NAVAIR plans to install ACAS Xu on MQ-4C ﬂeet.

articles/navair-plans-to-install-acas-xu-on-mq-4c-fleet-444989/.

https://www.flightglobal.com/news/

[2] Nvidia-Autopilot-Keras. https://github.com/0bserver07/Nvidia-Autopilot-Keras.
[3] Tesla’s autopilot was involved in another deadly car crash. https://www.wired.com/story/tesla-

autopilot-self-driving-crash-california/.

[4] Using Deep Learning to Predict Steering Angles. https://github.com/udacity/self-driving-car.
[5] D. Arp  M. Spreitzenbarth  M. Hubner  H. Gascon  K. Rieck  and C. Siemens. Drebin: Effective and
explainable detection of android malware in your pocket. In Proceedings of the Network and Distributed
System Security Symposium  volume 14  pages 23–26  2014.

[6] M. Bojarski  D. Del Testa  D. Dworakowski  B. Firner  B. Flepp  P. Goyal  L. D. Jackel  M. Monfort 
U. Muller  J. Zhang  et al. End to end learning for self-driving cars. IEEE Intelligent Vehicles Symposium 
2017.

[7] S. Dutta  S. Jha  S. Sankaranarayanan  and A. Tiwari. Output range analysis for deep feedforward neural

networks. In NASA Formal Methods Symposium  pages 121–138. Springer  2018.

[8] K. Dvijotham  S. Gowal  R. Stanforth  R. Arandjelovic  B. O’Donoghue  J. Uesato  and P. Kohli. Training

veriﬁed learners with learned veriﬁers. arXiv preprint arXiv:1805.10265  2018.

[9] K. Dvijotham  R. Stanforth  S. Gowal  T. Mann  and P. Kohli. A dual approach to scalable veriﬁcation of

deep networks. The Conference on Uncertainty in Artiﬁcial Intelligence  2018.

[10] R. Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. 15th International

Symposium on Automated Technology for Veriﬁcation and Analysis  2017.

[11] R. Eldan. A polynomial number of random points does not determine the volume of a convex body.

Discrete & Computational Geometry  46(1):29–47  2011.

[12] M. Fischetti and J. Jo. Deep neural networks as 0-1 mixed integer linear programs: A feasibility study.

arXiv preprint arXiv:1712.06174  2017.

[13] T. Gehr  M. Mirman  D. Drachsler-Cohen  P. Tsankov  S. Chaudhuri  and M. Vechev. Ai 2: Safety and
robustness certiﬁcation of neural networks with abstract interpretation. In IEEE Symposium on Security
and Privacy  2018.

[14] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. International

Conference on Learning Representations  2015.

[15] X. Huang  M. Kwiatkowska  S. Wang  and M. Wu. Safety veriﬁcation of deep neural networks. In

International Conference on Computer Aided Veriﬁcation  pages 3–29. Springer  2017.

[16] K. D. Julian  J. Lopez  J. S. Brush  M. P. Owen  and M. J. Kochenderfer. Policy compression for aircraft

collision avoidance systems. In 35th Digital Avionics Systems Conference  pages 1–10. IEEE  2016.

[17] G. Katz  C. Barrett  D. Dill  K. Julian  and M. Kochenderfer. Reluplex: An efﬁcient smt solver for verifying

deep neural networks. International Conference on Computer Aided Veriﬁcation  2017.

[18] G. Katz  C. Barrett  D. L. Dill  K. Julian  and M. J. Kochenderfer. Towards proving the adversarial
robustness of deep neural networks. 1st Workshop on Formal Veriﬁcation of Autonomous Vehicles  2017.

[19] M. J. Kochenderfer  J. E. Holland  and J. P. Chryssanthacopoulos. Next-generation airborne collision
avoidance system. Technical report  Massachusetts Institute of Technology-Lincoln Laboratory Lexington
United States  2012.

10

[20] P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence functions. International

Conference on Machine Learning  2017.

[21] Y. LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/  1998.
[22] M. Lecuyer  V. Atlidakis  R. Geambasu  H. Daniel  and S. Jana. Certiﬁed robustness to adversarial

examples with differential privacy. arXiv preprint arXiv:1802.03471  2018.

[23] J. Li  W. Monroe  and D. Jurafsky. Understanding neural networks through representation erasure. arXiv

preprint arXiv:1612.08220  2016.

[24] M. Mirman  T. Gehr  and M. Vechev. Differentiable abstract interpretation for provably robust neural

networks. In International Conference on Machine Learning  pages 3575–3583  2018.

[25] G. F. Montufar  R. Pascanu  K. Cho  and Y. Bengio. On the number of linear regions of deep neural

networks. In Advances in neural information processing systems  pages 2924–2932  2014.

[26] M. T. Notes. Airborne collision avoidance system x. MIT Lincoln Laboratory  2015.
[27] R. Pascanu  G. Montufar  and Y. Bengio. On the number of response regions of deep feed forward networks

with piece-wise linear activations. Advances in neural information processing systems  2013.

[28] J. Peck  J. Roels  B. Goossens  and Y. Saeys. Lower bounds on the robustness to adversarial perturbations.

In Advances in Neural Information Processing Systems  pages 804–813  2017.

[29] K. Pei  Y. Cao  J. Yang  and S. Jana. Deepxplore: Automated whitebox testing of deep learning systems.

In 26th Symposium on Operating Systems Principles  pages 1–18. ACM  2017.

[30] K. Pei  Y. Cao  J. Yang  and S. Jana. Towards practical veriﬁcation of machine learning: The case of

computer vision systems. arXiv preprint arXiv:1712.01785  2017.

[31] L. Pulina and A. Tacchella. An abstraction-reﬁnement approach to veriﬁcation of artiﬁcial neural networks.

In International Conference on Computer Aided Veriﬁcation  pages 243–257. Springer  2010.

[32] A. Raghunathan  J. Steinhardt  and P. Liang. Certiﬁed defenses against adversarial examples. International

Conference on Learning Representations  2018.

[33] M. J. C. Ramon E. Moore  R. Baker Kearfott. Introduction to Interval Analysis. SIAM  2009.
[34] A. Shrikumar  P. Greenside  and A. Kundaje. Learning important features through propagating activation

differences. International Conference on Machine Learning  2017.

[35] M. Spreitzenbarth  F. Freiling  F. Echtler  T. Schreck  and J. Hoffmann. Mobile-sandbox: having a deeper
look into android applications. In 28th Annual ACM Symposium on Applied Computing  pages 1808–1815.
ACM  2013.

[36] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus. Intriguing

properties of neural networks. International Conference on Learning Representations  2013.

[37] Y. Tian  K. Pei  S. Jana  and B. Ray. DeepTest: Automated testing of deep-neural-network-driven

autonomous cars. In 40th International Conference on Software Engineering  2018.

[38] V. Tjeng  K. Xiao  and R. Tedrake. Evaluating robustness of neural networks with mixed integer program-

ming. arXiv preprint arXiv:1711.07356  2017.

[39] S. Wang  K. Pei  W. Justin  J. Yang  and S. Jana. Formal security analysis of neural networks using

symbolic intervals. 27th USENIX Security Symposium  2018.

[40] T.-W. Weng  H. Zhang  H. Chen  Z. Song  C.-J. Hsieh  D. Boning  I. S. Dhillon  and L. Daniel. Towards

fast computation of certiﬁed robustness for relu networks. arXiv preprint arXiv:1804.09699  2018.

[41] T.-W. Weng  H. Zhang  P.-Y. Chen  J. Yi  D. Su  Y. Gao  C.-J. Hsieh  and L. Daniel. Evaluating the
robustness of neural networks: An extreme value theory approach. International Conference on Learning
Representations  2018.

[42] E. Wong and J. Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial

polytope. International Conference on Machine Learning  2018.

[43] E. Wong  F. Schmidt  J. H. Metzen  and J. Z. Kolter. Scaling provable adversarial defenses. Advances in

Neural Information Processing Systems  2018.

[44] H. Zhang  T.-W. Weng  P.-Y. Chen  C.-J. Hsieh  and L. Daniel. Efﬁcient neural network robustness
certiﬁcation with general activation functions. Advances in Neural Information Processing Systems  2018.

11

,Shiqi Wang
Kexin Pei
Justin Whitehouse
Junfeng Yang
Suman Jana
Zhiyong Yang
Qianqian Xu
Yangbangyan Jiang
Xiaochun Cao
Qingming Huang