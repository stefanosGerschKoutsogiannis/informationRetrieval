2018,Distributed Weight Consolidation: A Brain Segmentation Case Study,Collecting the large datasets needed to train deep neural networks can be very difficult  particularly for the many applications for which sharing and pooling data is complicated by practical  ethical  or legal concerns. However  it may be the case that derivative datasets or predictive models developed within individual sites can be shared and combined with fewer restrictions. Training on distributed data and combining the resulting networks is often viewed as continual learning  but these methods require networks to be trained sequentially. In this paper  we introduce distributed weight consolidation (DWC)  a continual learning method to consolidate the weights of separate neural networks  each trained on an independent dataset. We evaluated DWC with a brain segmentation case study  where we consolidated dilated convolutional neural networks trained on independent structural magnetic resonance imaging (sMRI) datasets from different sites. We found that DWC led to increased performance on test sets from the different sites  while maintaining generalization performance for a very large and completely independent multi-site dataset  compared to an ensemble baseline.,Distributed Weight Consolidation:
A Brain Segmentation Case Study

Patrick McClure

National Institute of Mental Health

patrick.mcclure@nih.gov

Charles Y. Zheng

National Institute of Mental Health

charles.zheng@nih.gov

Jakub R. Kaczmarzyk

Massachusetts Institute of Technology

jakubk@mit.edu

Massachusetts Institute of Technology

Satrajit S. Ghosh

satra@mit.edu

Peter Bandettini

National Institute of Mental Health

bandettini@nih.gov

John A. Lee

National Institute of Mental Health

john.rodgers-lee@nih.gov

Dylan Nielson

National Institute of Mental Health

dylann.nielson@nih.gov

Francisco Pereira

National Institute of Mental Health
francisco.pereira@nih.gov

Abstract

Collecting the large datasets needed to train deep neural networks can be very
difﬁcult  particularly for the many applications for which sharing and pooling data
is complicated by practical  ethical  or legal concerns. However  it may be the case
that derivative datasets or predictive models developed within individual sites can
be shared and combined with fewer restrictions. Training on distributed data and
combining the resulting networks is often viewed as continual learning  but these
methods require networks to be trained sequentially. In this paper  we introduce
distributed weight consolidation (DWC)  a continual learning method to consolidate
the weights of separate neural networks  each trained on an independent dataset.
We evaluated DWC with a brain segmentation case study  where we consolidated
dilated convolutional neural networks trained on independent structural magnetic
resonance imaging (sMRI) datasets from different sites. We found that DWC led
to increased performance on test sets from the different sites  while maintaining
generalization performance for a very large and completely independent multi-site
dataset  compared to an ensemble baseline.

1

Introduction

Deep learning methods require large datasets to perform well. Collecting such datasets can be very
difﬁcult  particularly for the many applications for which sharing and pooling data is complicated
by practical  ethical  or legal concerns. One prominent application is human subjects research  in
which researchers may be prevented from sharing data due to privacy concerns or other ethical
considerations. These concerns can signiﬁcantly limit the purposes for which the collected data can
be used  even within a particular collection site. If the datasets are collected in a clinical setting 
they may be subject to many additional constraints. However  it may be the case that derivative

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

datasets or predictive models developed within individual sites can be shared and combined with
fewer restrictions.
In the neuroimaging literature  several platforms have been introduced for combining models trained
on different datasets  such as ENIGMA ([29]  for meta-analyses) and COINSTAC ([23]  for distributed
training of models). Both platforms support combining separately trained models by averaging the
learned parameters. This works for convex methods (e.g. linear regression)  but does not generally
work for non-convex methods (e.g. deep neural networks  DNNs).
[23] also discussed using
synchronous stochastic gradient descent training using server-client communication; this assumes
that all of the training data is simultaneously available. Also  for large models such as DNNs  the
bandwidth required could be problematic  given the need to transmit gradients at every update.
Learning from non-centralized datasets using DNNs is often viewed as continual learning  a sequential
process where a given predictive model is updated to perform well on new datasets  while retaining
the ability to predict on those previously used for training [32  4  21  16  18]. Continual learning is
particularly applicable to problems with shifting input distributions  where the data collected in the
past may not represent data collected now or in the future. This is true for neuroimaging  since the
statistics of MRIs may change due to scanner upgrades  new reconstruction algorithms  different
sequences  etc. The scenario we envisage is a more complex situation where multiple continual
learning processes may take place non-sequentially. For instance  a given organization produces a
starting DNN  which different  independent sites will then use with their own data. The sites will
then contribute back updated DNNs  which the organization will use to improve the main DNN being
shared  with the goal of continuing the sharing and consolidation cycle.
Our application is segmentation of structural magnetic resonance imaging (sMRI) volumes. These
segmentations are often generated using the Freesurfer package [8]  a process that can take close to a
day for each subject. The computational resources for doing this at a scale of hundreds to thousands
of subjects are beyond the capabilities of most sites. We use deep neural networks to predict the
Freesurfer segmentation directly from the structural volumes  as done previously by other groups
[26  27  7  6]. We train several of those networks – each using data from a different site – and
then consolidate their weights. We show that this results in a model with improved generalization
performance in test data from these sites  as well as a very large  completely independent multi-site
dataset.

2 Data and Methods

2.1 Datasets

We use several sMRI datasets collected at different sites. We train networks using 956 sMRI volumes
collected by the Human Connectomme Project (HCP) [30]  1 136 sMRI volumes collected by the
Nathan Kline Institute (NKI) [22]  183 sMRI volumes collected by the Buckner Laboratory [2]  and
120 sMRI volumes from the Washington University 120 (WU120) dataset [24]. In order to provide an
independent estimate of how well a given network generalizes to any new site  we also test networks
on a completely held-out dataset consisting of 893 sMRI volumes collected across several institutions
by the ABIDE project [5].

2.2 Architecture

Several deep neural network architectures have been proposed for brain segmentation  such as U-net
[26]  QuickNAT [27]  HighResNet [18] and MeshNet [7  6]. We chose MeshNet because of its
relatively simple structure  its lower number of learned parameters  and its competitive performance.
MeshNet uses dilated convolutional layers [31] due to the 3D structural nature of sMRI data. The
output of these discrete volumetric dilated convolutional layers can be expressed as:

(wf ∗l h)i j k =

a(cid:88)

b(cid:88)

c(cid:88)

˜i=−a

˜j=−b

˜k=−c

wf ˜i ˜j ˜khi−l˜i j−l˜j k−l˜k = (wf ∗l h)v =

(cid:88)

t∈Wabc

wf thv−lt.

(1)

2

Layer

1
2
3
4

Filter
96x33
96x33
96x33
96x33

Pad Dilation (l)
1
1
1
2

1
1
1
2

Function
ReLU
ReLU
ReLU
ReLU

Layer

5
6
7
8

Filter
96x33
96x33
96x33
50x13

Pad Dilation (l)
4
8
1
0

4
8
1
1

Function
ReLU
ReLU
ReLU
Softmax

Table 1: The Meshnet-like dilated convolutional neural network architecture for brain segmentation.

where h is the input to the layer  a  b  and c are the bounds for the i  j  and k axes of the ﬁlter with
weights wf   (i  j  k) is the voxel  v  where the convolution is computed. The set of indices for the
elements of wf can be deﬁned as Wabc = {−a  ...  a}×{−b  ...  b}×{−c  ...  c}. The dilation factor
l allows the convolution kernel to operate on every l-th voxel  since adjacent voxels are expected to
be highly correlated. The dilation factor  number of ﬁlters  and other details of the MeshNet-like
architecture that we used for all experiments is shown in Table 1.

2.3 Bayesian Inference in Neural Networks

2.3.1 Maximum a Posteriori Estimate

N(cid:88)

When training a neural network  the weights of the network  w are learned by optimizing
argmaxwp(w|D) where D = {(x1  y1)  ...  (xN   yN )} and (xn  yn) is the nth input-output ex-
ample  per maximum likelihood estimation (MLE). However  this often overﬁts  so we used a prior
on the network weights  p(w)  to obtain a maximum a posteriori (MAP) estimate  by maximizing:

LM AP (w) =

log p(yn|xn  w) + log p(w).

(2)

2.3.2 Approximate Bayesian Inference

n=1

In Bayesian inference for neural networks  a distribution of possible weights is learned instead of just
a MAP point estimate. Using Bayes’ rule  p(w|D) = p(D|w)p(w)/p(D)  where p(w) is the prior
over weights. However  directly computing the posterior  p(w|D)  is often intractable  particularly
for DNNs. As a result  an approximate inference method must be used.
One of the most popular approximate inference methods for neural networks is variational inference 
since it scales well to large DNNs. In variational inference  the posterior distribution p(w|D) is
approximated by a learned variational distribution of weights qθ(w)  with learnable parameters θ.
This approximation is enforced by minimizing the Kullback-Leibler divergence (KL) between qθ(w) 
and the true posterior  p(w|D)  KL[qθ(w)||p(w|D)]. This is equivalent to maximizing the variational
lower bound [11  10  3  14  9  20  19]  also known as the evidence lower bound (ELBO) 

where LD(θ) is

LELBO(θ) = LD(θ) − LKL(θ) 

N(cid:88)

n=1

LD(θ) =

Eqθ(w)[log p(yn|xn  w)]

and LKL(θ) is the KL divergence between the variational distribution of weights and the prior 

LKL(θ) = KL[qθ(w)||p(w)].

(3)

(4)

(5)

Maximizing LD seeks to learn a qθ(w) that explains the training data  while minimizing LKL (i.e.
keeping qθ(w) close to p(w)) prevents learning a qθ(w) that overﬁts to the training data.

3

2.3.3 Stochastic Variational Bayes

Optimizing Eq. 3 for deep neural networks is usually impractical to compute  due to both: (1) being
a full-batch approach and (2) integrating over qθ(w). (1) is often dealt with by using stochastic
mini-batch optimization [25] and (2) is often approximated using Monte Carlo sampling. [14] applied
these methods to variational inference in deep neural networks. They used the "reparameterization
trick" [15]  which formulates qθ(w) as a deterministic differentiable function w = f (θ  ) where
 ∼ N (0  I)  to calculate an unbiased estimate of ∇θLD for a mini-batch  {(x1  y1)  ...  (xM   yM )} 
and one weight noise sample  m  for each mini-batch example:

where

LELBO(θ) ≈ LSGV BD
M(cid:88)

(θ) − LKL(θ) 

log p(ym|xm  f (θ  m)).

(6)

(7)

LD(θ) ≈ LSGV BD

(θ) =

N
M

m=1

2.3.4 Variational Continual Learning

In Bayesian neural networks  p(w) is often set to a multivariate Gaussian with diagonal covariance
N (000  σ2
priorI). (A variational distribution of the same form is called a fully factorized Gaussian
(FFG).) However  instead of using a naïve prior  the parameters of a previously trained DNN can be
used. Several methods  such elastic weight consolidation [16] and synaptic intelligence [32]  have
explored this approach. Recently  these methods have been reinterpreted from a Bayesian perspective
[21  17]. In variational continual learning (VCL) [21] and Bayesian incremental learning [17]  the
DNNs trained on previously obtained data  D1-DT−1  are used to regularize the training of a new
neural network trained on DT per:

p(w|D1:T ) =

p(D1:T|w)p(w)

p(D1:T )

p(D1:T−1|w)p(DT|w)p(w)

p(D1:T−1)p(DT )

=

=

p(w|D1:T−1)p(DT|w)

p(DT )

 

(8)

where p(w|D1:T−1) is the network resulting from training on a sequence of datasets D1-DT−1.
For DNNs  computing p(w|D1:T ) directly can be intractable  so variational inference is iteratively
θ (w)||p(w|D1:τ )] for each sequential
used to learn an approximation  qT
dataset Dτ   with τ ranging over integers from 1 to T .
The sequential nature of this approach is a limitation in our setting. In many cases it is not feasible
for one site to wait for another site to complete training  which can take days  in order to begin their
own training.

θ (w)  by minimizing KL[qτ

2.4 Distributed Weight Consolidation

The main motivation of our method – distributed weight consolidation (DWC) – is to make it possible
to train neural networks on different  distributed datasets  independently  and consolidate their weights
into a single network.

2.4.1 Bayesian Continual Learning for Distributed Data

T   ... DS

In DWC  we seek to consolidate several distributed DNNs trained on S separate  distributed datasets 
DT = {D1
T}  so that the resulting DNN can then be used to inform the training of a DNN on
DT +1. The training on each dataset starts from an existing network p(w|D1:T−1).
Assuming that the S datasets are independent allows Eq. 8 to be rewritten as:

.

(9)

p(w|D1:T−1)(cid:81)S

(cid:81)S
s=1 p(Ds
T )

s=1 p(Ds

T|w)

p(w|D1:T ) =

4

However  training one of the S networks using VCL produces an approximation for p(w|D1:T−1 Ds
T ).
Eq.
T ) =
p(w|D1:T−1)p(Ds

9 can be written in terms of the learned distributions  since p(w|D1:T−1 Ds

T|w)/p(Ds

T ) per Eq. 8:

p(w|D1:T ) =

1

p(w|D1:T−1)S−1

p(w|D1:T−1  Ds
T ).

(10)

p(w|D1:T−1) and each p(w|D1:T−1 Ds
distribution can then be used to learn p(w|D1:T +1) per Eq. 8.

T ) can be learned and then used to compute p(w|D1:T ). This

2.4.2 Variational Approximation

In DNNs  however  directly calculating these probability distributions can be intractable  so varia-
tional inference is used to learn an approximation  qT s
T ) by minimizing
KL[qT

(w)  for p(w|D1:T−1 Ds

T )]. This results in approximating Eq. 10 using:

θ (w)||p(w|D1:T−1 Ds

θ

S(cid:89)

s=1

S(cid:89)

s=1

p(w|D1:T ) ≈

1
(w)S−1

qT−1

θ

qT s
θ

(w).

(11)

2.4.3 Dilated Convolutions with Fully Factorized Gaussian Filters

each of the F ﬁlters are independent (i.e. p(w) =(cid:81)F
also independent (i.e. p(wf ) =(cid:81)

Although more complicated variational families have recently been explored in DNNs  the relatively
simple FFG variational distribution can do as well as  or better than  more complex methods for
continual learning [17]. In this paper  we use dilated convolutions with FFG ﬁlters. This assumes
f =1 p(wf ))  that each weight within a ﬁlter is
p(wf t))  and that each weight is Gaussian (i.e. wf t ∼
N (µf t  σ2
f t)) with learnable parameters µf t and σf t. However  as discussed in [14  20]  randomly
sampling each weight for each mini-batch example can be computationally expensive  so the fact
that the sum of independent Gaussian variables is also Gaussian is used to move the noise from the
weights to the convolution operation. For  dilated convolutions  this is described by

t∈Wabc

(wf ∗l h)v ∼ N (µ∗

f v  (σ∗

f v)2) 

where

and

(cid:88)

t∈Wabc

µ∗
f v =

µf thv−lt

(cid:88)

t∈Wabc

(σ∗

f v)2 =

σ2
f th2

v−lt.

Eq. 12 can be rewritten using the Gaussian "reparameterization trick":

(wf ∗l h)v = µ∗

f v + σ∗

f vf v where f v ∼ N (0  1).

2.4.4 Consolidating an Ensemble of Fully Factorized Gaussian Networks

(12)

(13)

(14)

(15)

Eq. 11 can be used to consolidate an ensemble of distributed networks in order to allow for
training on new datasets. Eq. 11 can be directly calculated if qT−1
f t)2)
and qT s

f t  (σ0
f t)2) are known  resulting in p(w|D1:T ) also being an FFG per

(wf t) = N (µ0

(wf t) = N (µs

f t  (σs

θ

θ

5

p(wf t|D1:T ) ∝∼ e

(S−1)

(wf 0 t−µ0
f t )2
)2

2(σ0

f t

S(cid:89)

s=1

e

−(wf s t−µs
f t )2
)2

2(σs

f t

(16)

(17)

(cid:33)

.

1
(σ0
f t)2

s=1

1

1
(σs

f t)2 −(cid:80)S−1
f t)2 −(cid:80)S−1

s=1

s=1

and

(cid:32)(cid:80)S
(cid:80)S

f t)2 −(cid:80)S−1
f t)2 −(cid:80)S−1

s=1

s=1

f t

s=1

µs
(σs

p(wf t|D1:T ) ≈ N

(cid:80)S
to the multivariate Gaussian density; it is deﬁned when(cid:80)S

µ0
f t
(σ0
f t)2
1
(σ0
f t)2

1
(σs

s=1

 

Eq. 17 follows from Eq. 16 by completing the square inside the exponent and matching the parameters
f t)2 > 0. To ensure
f t)2. This should be the case if the loss is optimized  since LD
f t)2. p(wf t|D1:T ) can then be used as a

this  we constrained (σ0
f t)2 to 0 and LKL pulls (σs
should pull (σs
prior for training another variational DNN.

f t)2 towards (σ0

f t)2 ≥ (σs

1
(σ0

1
(σs

s=1

3 Experiments

3.1 Experimental Setup

3.1.1 Data Preprocessing

The only pre-processing that we performed was conforming the input sMRIs with Freesurfer’s
mri_convert  which resized all of the volumes used to 256x256x256 1 mm voxels . We computed
50-class Freesurfer [8] segmentations  as in [6]  for all subjects in each of the datasets described
earlier. These were used as the labels for prediction. A 90-10 training-test split was used for the HCP 
NKI  Buckner  and WU120 datasets. During training and testing  input volumes were individually
z-scored across voxels. We split each input volume into 512 non-overlapping 32x32x32 sub-volumes 
as in [7  6].

3.1.2 Training Procedure

All networks were trained with Adam [13] and an initial learning rate of 0.001. The MAP networks
were trained until convergence. The subsequent networks were trained until the training loss started
to oscillate around a stable loss value. These networks trained much faster than the MAP networks 
since they were initialized with previously trained networks. Speciﬁcally  we found that using VCL

DWC errors

DWC

Ground Truth

MAP

MAP errors

DWC errors

DWC

Ground Truth

MAP

MAP errors

Figure 1: The axial and sagittal segmentations produced by DWC and the HN BWM AP baseline on
a HCP subject. The subject was selected by matching the subject speciﬁc Dice with the average Dice
across HCP. Segmentations errors for all classes are shown in red in the respective plot.

6

DWC errors

DWC

Ground Truth

MAP

MAP errors

DWC errors

DWC

Ground Truth

MAP

MAP errors

Figure 2: The axial and sagittal segmentations produced by DWC and the HN BWM AP baseline on
a NKI subject. The subject was selected by matching the subject speciﬁc Dice with the average Dice
across NKI. Segmentations errors for all classes are shown in red in the respective plot.

DWC errors

DWC

Ground Truth

MAP

MAP errors

DWC errors

DWC

Ground Truth

MAP

MAP errors

Figure 3: The axial and sagittal segmentations produced by DWC and the HN BWM AP baseline on
a Buckner subject. The subject was selected by matching the subject speciﬁc Dice with the average
Dice across Buckner. Segmentations errors for all classes are shown in red in the respective plot.

DWC errors

DWC

Ground Truth

MAP

MAP errors

DWC errors

DWC

Ground Truth

MAP

MAP errors

Figure 4: The axial and sagittal segmentations produced by DWC and the HN BWM AP baseline on
a WU120 subject. The subject was selected by matching the subject speciﬁc Dice with the average
Dice across WU120. Segmentations errors for all classes are shown in red in the respective plot.

7

led to ∼3x  ∼2x  and ∼4x convergence speedups for HCP to NKI  HCP to Buckner and HCP to
WU120  respectively. The batch-size was set to 10. Weight normalization [28] was used for the
weight means for all networks and the weight standard deviations were initialized to 0.001 as in [19]
for the variational network trained on HCP. For MAP networks and the variational network trained
on HCP  p(w) = N (0  1).

3.1.3 Performance Metric

To measure the quality of the produced segmentations  we calculated the Dice coefﬁcient  which is
deﬁned by

Dicec =

2|ˆyc · yc|

||ˆyc||2 + ||yc||2 =

2T Pc

2T Pc + F Nc + F Pc

 

(18)

where ˆyc is the binary segmentation for class c produced by a network  yc is the ground truth
produced by Freesurfer  T Pc is the true positive rate for class c  F Nc is the false negative rate for
class c  and F Pc is the false positive rate for class c. We calculate the Dice coefﬁcient for each class
c and average across classes to compute the overall performance of a network.

3.1.4 Baselines

We trained MAP networks on the HCP (HM AP )  NKI (NM AP )  Buckner (BM AP ) and WU120
(WM AP ) datasets. We averaged the output probabilities of the HM AP   NM AP   BM AP   and WM AP
networks to create an ensemble baseline. We also trained a MAP model on the aggregated HCP  NKI 
Buckner  and WU120 training data (HN BWM AP ) to estimate the performance ceiling of having the
training data from all sites available together.

3.1.5 Variational Continual Learning

We trained an initial FFG variational network on HCP (H) using HM AP to initialize the network. We
then used used VCL with HCP as the prior for distributed training of the FFG variational networks
on the NKI (H → N)  Buckner (H → B) and WU120 (H → W ) datasets. Additionally  we trained
networks using VCL to transfer from HCP to NKI to Buckner to WU120 (H → N → B → W )
and from HCP to WU120 to Buckner to NKI (H → W → B → N). These options test training on
NKI  Buckner  and WU120 in increasing and decreasing order of dataset size  since dataset order
may matter and may be difﬁcult to control in practice.

3.1.6 Distributed Weight Consolidation

For DWC  our goal was to take distributed networks trained using VCL with an initial network
as a prior  consolidate them per Eq. 17  and then use this consolidated model as a prior for ﬁne-
tuning on the original dataset. We used DWC to consolidate H → N  H → B  and H → W into
H → N + B + W per Eq. 17. VCL [21] performance was found to be improved by using coresets
[1  12]  a small sample of data from the different training sets. However  if examples cannot be
collected from the different datasets  as may be the case when examples from the separate datasets
cannot be shared  coresets are not applicable. For this reason  we used H → N + B + W as a prior
for ﬁne-tuning (FT) by training the network on H (H → N + B + W → H) and giving LD the
weight of one example volume.

3.2 Experimental Results

In Table 2 we show the average Dice scores across classes and sMRI volumes for the differently
trained networks. The weighted average Dice scores were computed across H  N  B  and W by
weighing each of the Dice scores according to the number of volumes in each test set. For the
variational networks  10 MC samples were used during test time to approximate the expected network
output. The weighted average Dice scores of DWC were better than the scores of the ensemble  the
baseline method for combining methods across sites  (p = 1.66e-15  per a two tailed paired t-test
across volumes). The ABIDE Dice scores of DWC were not signiﬁcantly different from the scores of
the ensemble (p = 0.733  per a two tailed paired t-test across volumes)  showing that DWC does not
reduce generalization performance for a very large and completely independent multi-site dataset.

8

N etwork
HM AP
NM AP
BM AP
WM AP
H → N
H → B
H → W

H → N + B + W (DWC w/o FT)
H → N + B + W → H (DWC)

Ensemble

H → N → B → W
H → W → B → N

HN BWM AP

H

82.25
71.20
65.69
70.18
75.40
73.85
77.07
77.42
78.04
78.28
79.13
80.34
81.38

N

65.88
72.19
50.17
66.27
73.24
56.79
67.63
71.46
78.15
73.52
72.32
73.64
77.99

B

67.94
70.73
82.02
72.20
71.77
79.49
76.15
79.70
75.79
78.02
80.02
77.46
80.64

W
70.88
73.06
68.87
76.38
73.17
68.53
77.26
79.82
79.50
77.37
78.84
78.10
79.54

Avg.
72.92
71.66
59.25
68.76
74.03
65.78
72.51
74.86
77.99
75.95
75.94
76.82
79.62

A

55.25
66.67
50.23
62.83
64.62
49.27
62.31
63.3
70.79
65.56
66.27
66.21
70.76

Table 2: The average Dice scores across test volumes for the trained networks on HCP (H)  NKI (N) 
Buckner (B)  and WU120 (W)  along with the weighted average Dice scores across H  N  B  and W
and the average Dice scores across volumes on the independent ABIDE (A) dataset.

Training on different datasets sequentially using VCL was very sensitive to dataset order  as seen by
the difference in Dice scores when training on NKI  Buckner  and WU120 in order of decreasing
and increasing dataset size (H → N → B → W and H → W → B → N  respectively). The
performance of DWC was within the range of VCL performance. The weighted average and ABIDE
dice scores of DWC were better than the H → N → B → W Dice scores  but not better than the
H → W → B → N Dice scores.
In Figures 1  2  3  and 4  we show selected example segmentations for DWC and HN BWM AP  
for volumes that have Dice scores similar to the average Dice score across the respective dataset.
Visually  the DWC segmentation appears very similar to the ground truth. The errors made appear to
occur mainly at region boundaries. Additionally  the DWC errors appear to be similar to the errors
made by HN BWM AP .

4 Discussion

There are many problems for which accumulating data into one accessible dataset for training can
be difﬁcult or impossible  such as for clinical data. It may  however  be feasible to share models
derived from such data. A method often proposed for dealing with these independent datasets is
continual learning  which trains on each of these datasets sequentially [4]. Several recent continual
learning methods use previously trained networks as priors for networks trained on the next dataset
[32  21  16]  albeit with the requirement that training happens sequentially. We developed DWC
by modifying these methods to allow for training networks on several new datasets in a distributed
way. Using DWC  we consolidated the weights of the distributed neural networks to perform brain
segmentation on data from different sites. The resulting weight distributions can then be used as a
prior distribution for further training  either for the original site or for novel sites. Compared to an
ensemble made from models trained on different sites  DWC increased performance on the held-out
test sets from the sites used in training and led to similar ABIDE performance. This demonstrates the
feasibility of DWC for combining the knowledge learned by networks trained on different datasets 
without either training on the sites sequentially or ensembling many trained models. One important
direction for future research is scaling DWC up to allow for consolidating many more separate 
distributed networks and repeating this training and consolidation cycle several times. Another area
of research is to investigate the use of alternative families of variational distributions within the
framework of DWC. Our method has the potential to be applied to many other applications where
it is necessary to train specialized networks for speciﬁc sites  informed by data from other sites 
and where constraints on data sharing necessitate a distributed learning approach  such as disease
diagnosis with clinical data.

9

Acknowledgments

This work was supported by the National Institute of Mental Health Intramural Research Program
(ZIC-MH002968  ZIC-MH002960). JK’s and SG’s work was supported by NIH R01 EB020740.

References
[1] Olivier Bachem  Mario Lucic  and Andreas Krause. Coresets for nonparametric estimation-the case of

DP-means. In International Conference on Machine Learning  pages 209–217  2015.

[2] Bharat B Biswal  Maarten Mennes  Xi-Nian Zuo  Suril Gohel  Clare Kelly  Steve M Smith  Christian F
Beckmann  Jonathan S Adelstein  Randy L Buckner  Stan Colcombe  et al. Toward discovery science of
human brain function. Proceedings of the National Academy of Sciences  107(10):4734–4739  2010.

[3] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty in neural

networks. In International Conference on Machine Learning  pages 1613–1622  2015.

[4] Ken Chang  Niranjan Balachandar  Carson Lam  Darvin Yi  James Brown  Andrew Beers  Bruce Rosen 
Daniel L Rubin  and Jayashree Kalpathy-Cramer. Distributed deep learning networks among institutions
for medical imaging. Journal of the American Medical Informatics Association.

[5] Adriana Di Martino  Chao-Gan Yan  Qingyang Li  Erin Denio  Francisco X Castellanos  Kaat Alaerts 
Jeffrey S Anderson  Michal Assaf  Susan Y Bookheimer  Mirella Dapretto  et al. The autism brain imaging
data exchange: Towards a large-scale evaluation of the intrinsic brain architecture in autism. Molecular
Psychiatry  19(6):659  2014.

[6] Alex Fedorov  Eswar Damaraju  Vince Calhoun  and Sergey Plis. Almost instant brain atlas segmentation

for large-scale studies. arXiv preprint arXiv:1711.00457  2017.

[7] Alex Fedorov  Jeremy Johnson  Eswar Damaraju  Alexei Ozerin  Vince Calhoun  and Sergey Plis. End-to-
end learning of brain tissue segmentation from imperfect labeling. In International Joint Conference on
Neural Networks  pages 3785–3792. IEEE  2017.

[8] Bruce Fischl. Freesurfer. Neuroimage  62(2):774–781  2012.

[9] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty

in deep learning. In International Conference on Machine Learning  pages 1050–1059  2016.

[10] Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information

Processing Systems  pages 2348–2356  2011.

[11] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning theory 
pages 5–13. ACM  1993.

[12] Jonathan Huggins  Trevor Campbell  and Tamara Broderick. Coresets for scalable Bayesian logistic

regression. In Advances in Neural Information Processing Systems  pages 4080–4088  2016.

[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

Conference on Learning Representations  2015.

In International

[14] Diederik P Kingma  Tim Salimans  and Max Welling. Variational dropout and the local reparameterization

trick. In Advances in Neural Information Processing Systems  pages 2575–2583  2015.

[15] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 

2013.

[16] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins  Andrei A Rusu 
Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences  114(13):3521–3526 
2017.

[17] Max Kochurov  Timur Garipov  Dmitry Podoprikhin  Dmitry Molchanov  Arsenii Ashukha  and Dmitry

Vetrov. Bayesian incremental learning for deep neural networks. ICLR Workshop  2018.

[18] Wenqi Li  Guotai Wang  Lucas Fidon  Sebastien Ourselin  M Jorge Cardoso  and Tom Vercauteren. On the
compactness  efﬁciency  and representation of 3D convolutional networks: Brain parcellation as a pretext
task. In International Conference on Information Processing in Medical Imaging  pages 348–360. Springer 
2017.

10

[19] Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational Bayesian neural

networks. In International Conference on Machine Learning  pages 2218–2227  2017.

[20] Dmitry Molchanov  Arsenii Ashukha  and Dmitry Vetrov. Variational dropout sparsiﬁes deep neural

networks. In International Conference on Machine Learning  pages 2498–2507  2017.

[21] Cuong V Nguyen  Yingzhen Li  Thang D Bui  and Richard E Turner. Variational continual learning. In

International Conference on Learning Representations  2018.

[22] Kate Brody Nooner  Stanley Colcombe  Russell Tobe  Maarten Mennes  Melissa Benedict  Alexis Moreno 
Laura Panek  Shaquanna Brown  Stephen Zavitz  Qingyang Li  et al. The NKI-Rockland sample: A model
for accelerating the pace of discovery science in psychiatry. Frontiers in Neuroscience  6:152  2012.

[23] Sergey M Plis  Anand D Sarwate  Dylan Wood  Christopher Dieringer  Drew Landis  Cory Reed  Sandeep R
Panta  Jessica A Turner  Jody M Shoemaker  Kim W Carter  et al. COINSTAC: a privacy enabled model
and prototype for leveraging and processing decentralized brain imaging data. Frontiers in Neuroscience 
10:365  2016.

[24] Jonathan D Power  Mark Plitt  Prantik Kundu  Peter A Bandettini  and Alex Martin. Temporal interpolation
alters motion in fMRI scans: Magnitudes and consequences for artifact detection. PloS one  12(9):e0182939 
2017.

[25] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  pages 400–407  1951.

[26] Olaf Ronneberger  Philipp Fischer  and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical Image Computing and Computer-Assisted
Intervention  pages 234–241. Springer  2015.

[27] Abhijit Guha Roy  Sailesh Conjeti  Nassir Navab  and Christian Wachinger. QuickNAT: Segmenting MRI

neuroanatomy in 20 seconds. arXiv preprint arXiv:1801.04161  2018.

[28] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems  pages 901–909 
2016.

[29] Paul M Thompson  Jason L Stein  Sarah E Medland  Derrek P Hibar  Alejandro Arias Vasquez  Miguel E
Renteria  Roberto Toro  Neda Jahanshad  Gunter Schumann  Barbara Franke  et al. The ENIGMA
consortium: Large-scale collaborative analyses of neuroimaging and genetic data. Brain imaging and
behavior  8(2):153–182  2014.

[30] David C Van Essen  Stephen M Smith  Deanna M Barch  Timothy EJ Behrens  Essa Yacoub  Kamil
Ugurbil  Wu-Minn HCP Consortium  et al. The WU-Minn human connectome project: An overview.
NeuroImage  80:62–79  2013.

[31] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In International

Conference on Learning Representations  2015.

[32] Friedemann Zenke  Ben Poole  and Surya Ganguli. Continual learning through synaptic intelligence. In

International Conference on Machine Learning  pages 3987–3995  2017.

11

,Jin Lu
Guannan Liang
Jiangwen Sun
Jinbo Bi
Patrick McClure
Charles Zheng
Jakub Kaczmarzyk
John Rogers-Lee
Satra Ghosh
Dylan Nielson
Peter Bandettini
Francisco Pereira
Cheng Fu
Huili Chen
Haolan Liu
Xinyun Chen
Yuandong Tian
Farinaz Koushanfar
Jishen Zhao