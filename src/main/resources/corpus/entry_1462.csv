2014,Difference of Convex Functions Programming for Reinforcement Learning,Large Markov Decision Processes (MDPs) are usually solved using Approximate Dynamic Programming (ADP) methods such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API). The main contribution of this paper is to show that  alternatively  the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so  we study the minimization of a norm of the Optimal Bellman Residual (OBR) $T^*Q-Q$  where $T^*$ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function  and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally  we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning (RL) problem.,Diﬀerence of Convex Functions Programming

for Reinforcement Learning

Bilal Piot1 2  Matthieu Geist1  Olivier Pietquin2 3

1MaLIS research group (SUPELEC) - UMI 2958 (GeorgiaTech-CNRS)  France

2LIFL (UMR 8022 CNRS/Lille 1) - SequeL team  Lille  France

3 University Lille 1 - IUF (Institut Universitaire de France)  France

bilal.piot@lifl.fr  matthieu.geist@supelec.fr  olivier.pietquin@univ-lille1.fr

Abstract

Large Markov Decision Processes are usually solved using Approximate Dy-
namic Programming methods such as Approximate Value Iteration or Ap-
proximate Policy Iteration. The main contribution of this paper is to show
that  alternatively  the optimal state-action value function can be estimated
using Diﬀerence of Convex functions (DC) Programming. To do so  we
study the minimization of a norm of the Optimal Bellman Residual (OBR)
T ∗Q − Q  where T ∗ is the so-called optimal Bellman operator. Control-
ling this residual allows controlling the distance to the optimal action-value
function  and we show that minimizing an empirical norm of the OBR is
consistant in the Vapnik sense. Finally  we frame this optimization problem
as a DC program. That allows envisioning using the large related literature
on DC Programming to address the Reinforcement Leaning problem.

1

Introduction

This paper addresses the problem of solving large state-space Markov Decision Processes
(MDPs)[16] in an inﬁnite time horizon and discounted reward setting. The classical methods
to tackle this problem  such as Approximate Value Iteration (AVI) or Approximate Policy
Iteration (API) [6  16]1  are derived from Dynamic Programming (DP). Here  we propose
an alternative path. The idea is to search directly a function Q for which T ∗Q ≈ Q 
where T ∗ is the optimal Bellman operator  by minimizing a norm of the Optimal Bellman
Residual (OBR) T ∗Q− Q. First  in Sec. 2.2  we show that the OBR Minimization (OBRM)
is interesting  as it can serve as a proxy for the optimal action-value function estimation.
Then  in Sec. 3  we prove that minimizing an empirical norm of the OBR is consistant in
the Vapnick sense (this justiﬁes working with sampled transitions). However  this empirical
norm of the OBR is not convex. We hypothesize that this is why this approach is not
studied in the literature (as far as we know)  a notable exception being the work of Baird [5].
Therefore  our main contribution  presented in Sec. 4  is to show that this minimization can
be framed as a minimization of a Diﬀerence of Convex functions (DC) [11]. Thus  a large
literature on Diﬀerence of Convex functions Algorithms (DCA) [19  20](a rather standard
approach to non-convex programming) is available to solve our problem. Finally in Sec. 5 
we conduct a generic experiment that compares a naive implementation of our approach to
API and AVI methods  showing that it is competitive.

1Others methods such as Approximate Linear Programming (ALP) [7  8] or Dynamic Policy

Programming (DPP) [4] address the same problem. Yet  they also rely on DP.

1

2 Background

x∈X ν(x)|α(x)|p) 1

kαkp ν = (P

2.1 MDP and ADP
Before describing the framework of MDPs in the inﬁnite-time horizon and discounted reward
setting  we give some general notations. Let (R |.|) be the real space with its canonical
norm and X a ﬁnite set  RX is the set of functions from X to R. The set of probability
distributions over X is noted ∆X. Let Y be a ﬁnite set  ∆Y
X is the set of functions from Y to
∆X. Let α ∈ RX  p ≥ 1 and ν ∈ ∆X  we deﬁne the Lp ν-semi-norm of α  noted kαkp ν  by:
p . In addition  the inﬁnite norm is noted kαk∞ and deﬁned
as kαk∞ = maxx∈X |α(x)|. Let v be a random variable which takes its values in X  v ∼ ν
means that the probability that v = x is ν(x).
Now  we provide a brief summary of some of the concepts from the theory of MDP and
ADP [16]. Here  the agent is supposed to act in a ﬁnite MDP 2 represented by a tuple
M = {S  A  R  P  γ} where S = {si}1≤i≤NS is the state space  A = {ai}1≤i≤NA is the action
space  R ∈ RS×A is the reward function  γ ∈]0  1[ is a discount factor and P ∈ ∆S×A
is the Markovian dynamics which gives the probability  P(s0|s  a)  to reach s0 by choosing
action a in state s. A policy π is an element of AS and deﬁnes the behavior of an agent.
The quality of a policy π is deﬁned by the action-value function. For a given policy π  the
t=0 γtR(st  at)]  where Eπ
is the expectation over the distribution of the admissible trajectories (s0  a0  s1  π(s1)  . . . )
obtained by executing the policy π starting from s0 = s and a0 = a. Moreover  the function
Q∗ ∈ RS×A deﬁned as Q∗ = maxπ∈AS Qπ is called the optimal action-value function. A
policy π is optimal if ∀s ∈ S  Qπ(s  π(s)) = Q∗(s  π(s)). A policy π is said greedy with
respect to a function Q if ∀s ∈ S  π(s) ∈ argmaxa∈A Q(s  a). Greedy policies are important
because a policy π greedy with respect to Q∗ is optimal. In addition  as we work in the
ﬁnite MDP setting  we deﬁne  for each policy π  the matrix Pπ of size NSNA × NSNA with
elements Pπ((s  a)  (s0  a0)) = P(s0|s  a)1{π(s0)=a0}. Let ν ∈ ∆S×A  we note νPπ ∈ ∆S×A
(s0 a0)∈S×A ν(s0  a0)Pπ((s0  a0)  (s  a)). Finally  Qπ
and Q∗ are known to be ﬁxed points of the contracting operators T π and T ∗ respectively:

action-value function Qπ ∈ RS×A is deﬁned as Qπ(s  a) = Eπ[P+∞

the distribution such that (νPπ)(s  a) =P

S

∀Q ∈ RS×A ∀(s  a) ∈ S × A  T πQ(s  a) = R(s  a) + γ

∀Q ∈ RS×A ∀(s  a) ∈ S × A  T ∗Q(s  a) = R(s  a) + γ

P(s0|s  a)Q(s  π(s0)) 

P(s0|s  a) max
b∈A

Q(s  b).

X
X

s0∈S

s0∈S

When the state space becomes large  two important problems arise to solve large MDPs.
The ﬁrst one  called the representation problem  is that an exact representation of the values
of the action-value functions is impossible  so these functions need to be represented with
a moderate number of coeﬃcients. The second problem  called the sample problem  is that
there is no direct access to the Bellman operators but only samples from them. One solution
for the representation problem is to linearly parameterize the action-value functions thanks
i=1 where φi ∈ RS×A. In addition  we deﬁne for each
to a basis of d ∈ N∗ functions φ = (φi)d
state-action couple (s  a) the vector φ(s  a) ∈ Rd such that φ(s  a) = (φi(s  a))d
i=1. Thus  the
action-value functions are characterized by a vector θ ∈ Rd and noted Qθ :

∀θ ∈ Rd ∀(s  a) ∈ S × A  Qθ(s  a) =

θiφi(s  a) = hθ  φ(s  a)i 

dX

i=1

where h.  .i is the canonical dot product of Rd.
The usual frameworks to solve large MDPs are for instance AVI and API. AVI consists in
. API consists
processing a sequence (QAVI
θn
≈
in processing two sequences (QAPI
θn
2This work could be easily extended to measurable state spaces as in [9]; we choose the ﬁnite

)n∈N where θ0 ∈ Rd and ∀n ∈ N  QAVI
)n∈N where πAPI
0

∈ AS  ∀n ∈ N  QAPI

θn+1 ≈ T ∗QAVI

)n∈N and (πAPI

θn

θn

n

case for the ease and clarity of exposition.

2

and πAPI

n+1 is greedy with respect to QAPI
n = T ∗QAVI

. The approximation steps in AVI and
T πn QAPI
θn
−
API generate the sequences of errors (AVI
)n∈N respectively. Those approximation errors are due to both the representation
QAPI
θn
and the sample problems and can be made explicit for speciﬁc implementations of those
methods [14  1]. These ADP methods are legitimated by the following bound [15  9]:

θn+1)n∈N and (API

n = T πn QAPI

− QAVI

θn

θn

θn

lim sup
n→∞

kQ∗ − QπAPI\AVI

n

kp ν ≤

2γ

(1 − γ)2 C2(ν  µ) 1

p API\AVI 

where π

API\AVI
n

is greedy with respect to Q

C2(ν  µ) is a second order concentrability coeﬃcient  C2(ν  µ) = (1 − γ)P

API\AVI
kp µ and
n
m≥1 mγm−1c(m) 
In the next section  we compare
where c(m) = maxπ1 ... πm (s a)∈S×A
the bound Eq. (1) with a similar bound derived from the OBR minimization approach in
order to justify it.

  API\AVI = supn∈N k

(νPπ1 Pπ2 ...Pπm )(s a)

API\AVI
θn

µ(s a)

.

(1)

2.2 Why minimizing the OBR?
The aim of Dynamic Programming (DP) is  given an MDP M  to ﬁnd Q∗ which is equivalent
to minimizing a certain norm of the OBR Jp µ(Q) = kT ∗Q− Qkp µ where µ ∈ ∆S×A is such
that ∀(s  a) ∈ S × A  µ(s  a) > 0 and p ≥ 1. Indeed  it is trivial to verify that the only
minimizer of Jp µ is Q∗. Moreover  we have the following bound given by Th. 1.
Theorem 1. Let ν ∈ ∆S×A  µ ∈ ∆S×A  ˆπ ∈ AS and C1(ν  µ  ˆπ) ∈ [1  +∞[∪{+∞} the

t≥0 γtP t

(cid:18) C1(ν  µ  π) + C1(ν  µ  π∗)

ˆπ ≤ C1(ν  µ  ˆπ)µ  then:
(cid:19) 1

p kT ∗Q − Qkp µ 

(2)

smallest constant verifying (1 − γ)νP
∀Q ∈ RS×A kQ∗ − Qπkp ν ≤ 2
1 − γ

2

2

where π is greedy with respect to Q and π∗ is any optimal policy.

Proof. A proof is given in the supplementary ﬁle. Similar results exist [15].

In Reinforcement Leaning (RL)  because of the representation and the sample problems 
minimizing kT ∗Q − Qkp µ over RS×A is not possible (see Sec. 3 for details)  but we can
consider that our approach provides us a function Q such that T ∗Q ≈ Q and deﬁne the
error OBRM = kT ∗Q − Qkp µ. Thus  via Eq. (2)  we have:

(cid:18) C1(ν  µ  π) + C1(ν  µ  π∗)

(cid:19) 1

p

OBRM 

(3)

kQ∗ − Qπkp ν ≤ 2
1 − γ

where π is greedy with respect to Q. This bound has the same form as the one of API
and AVI described in Eq. (1) and the Tab. 1 allows comparing them. This bound has two

Algorithms Horizon term Concentrability term Error term
API\AVI
API\AVI
OBRM
OBRM

C1(ν µ π)+C1(ν µ π∗)

C2(ν  µ)

2γ

(1−γ)2
2
1−γ
Table 1: Bounds comparison.

2

advantages over API\AVI. First  the horizon term 2
1−γ is better than the horizon term
2γ
(1−γ)2 as long as γ > 0.5  which is the usual case. Second  the concentrability term
C1(ν µ π)+C1(ν µ π∗)
is considered better that C2(ν  µ)  mainly because if C2(ν  µ) < +∞
then C1(ν µ π)+C1(ν µ π∗)
< +∞  the contrary being not true (see [17] for a discussion about
the comparison of these concentrability coeﬃcients). Thus  the bound Eq. (3) justiﬁes the
minimization of a norm of the OBR  as long as we are able to control the error term OBRM.

2

2

3

N

N

PN

PN
i=1 |T ∗Q(Si  Ai) − Q(Si  Ai)|p) 1

3 Vapnik-Consistency of the empirical norm of the OBR
When the state space is too large  it is not possible to minimize directly kT ∗Q − Qkp µ 
as we need to compute T ∗Q(s  a) for each couple (s  a) (sample problem). However  we
can consider the case where we choose N samples represented by N independent and iden-
tically distributed random variables (Si  Ai)1≤i≤N such that (Si  Ai) ∼ µ and minimize
kT ∗Q − Qkp µN where µN is the empirical distribution µN(s  a) = 1
i=1 1{(Si Ai)=(s a)}.
An important question (answered below) is to know if controlling the empirical norm allows
controlling the true norm of interest (consistency in the Vapnik sense [22])  and at what
rate convergence occurs.
Computing kT ∗Q − Qkp µN = ( 1
p is tractable if we con-
sider that we can compute T ∗Q(Si  Ai) which means that we have a perfect knowledge of
the dynamics P and that the number of next states for the state-action couple (Si  Ai)
is not too large. In Sec. 4.3  we propose diﬀerent solutions to evaluate T ∗Q(Si  Ai) when
the number of next states is too large or when the dynamics is not provided. Now  the
natural question is to what extent minimizing kT ∗Q − Qkp µN corresponds to minimizing
kT ∗Q − Qkp µ. In addition  we cannot minimize kT ∗Q − Qkp µN over RS×A as this space
is too large (representation problem) but over the space {Qθ ∈ RS×A  θ ∈ Rd}. Moreover 
as we are looking for a function such that Qθ = Q∗  we can limit our search to the func-
tions satisfying kQθk∞ ≤ kRk∞
1−γ . Thus  we search for a function Q in the hypothesis space
Q = {Qθ ∈ RS×A  θ ∈ Rd kQθk∞ ≤ kRk∞
1−γ }  in order to minimize kT ∗Q − Qkp µN . Let
QN ∈ argminQ∈Q kT ∗Q − Qkp µN be a minimizer of the empirical norm of the OBR  we
want to know to what extent the empirical error kT ∗QN − QNkp µN is related to the real
error OBRM = kT ∗QN − QNkp µ. The answer for deterministic-ﬁnite MPDs relies in Th. 2
(the continuous-stochastic MDP case being discussed shortly after).
Theorem 2. Let η ∈]0  1[ and M be a ﬁnite deterministic MDP  with probability at least
1 − η  we have:

∀Q ∈ Q kT ∗Q − Qkp

OBRM = kT ∗QN − QNkp µ ≤

where ε(N) = h(ln( 2N

h )+1)+ln( 4
η )

N

and h = 2NA(d + 1). With probability at least 1 − 2η:

p µ ≤ kT ∗Q − Qkp
 
B + 2kRk∞
1 − γ

p µN

pε(N) 
+ 2kRk∞
1 − γ
r

 pε(N) +

ln(1/η)

2N

!! 1

p

 

where B = minQ∈Q kT ∗Q − Qkp
Proof. The complete proof is provided in the supplementary ﬁle.
computing the Vapnik-Chervonenkis dimension of the residual.

p µ is the error due to the choice of features.

It mainly consists in

Thus  if we were able to compute a function such as QN  we would have  thanks to Eq .(2)
and Th. 2:

kQ∗−QπNkp ν ≤

(cid:18) C1(ν  µ  πN) + C1(ν  µ  π∗)

1 − γ

r

 pε(N) +
p 
(cid:19) 1
B + 2kRk∞
1 − γ
(cid:19)
(cid:18)pε(N) +
q ln(1/η)

!! 1

p

.

ln(1/η)

2N

where πN is greedy with respect to QN. The error term OBRM is explicitly controlled by
two terms B  a term of bias  and 2kRk∞
a term of variance. The
1−γ
term B = minQ∈Q kT ∗Q − Qkp
p µ is relative to the representation problem and is ﬁxed by
the choice of features. The term of variance is decreasing at the speed
A similar bound can be obtained for non-deterministic continuous-state MDPs with ﬁnite
number of actions where the state space is a compact set in a metric space  the features

q 1

N .

2N

4

i=1 are Lipschitz and for each state-action couple the next states belongs to a ball of
(φi)d
ﬁxed radius. The proof is a simple extension of the one given in the supplementary material.
Those continuous MDPs are representative of real dynamical systems. Now that we know
allows controlling kQ∗ − QπNkp ν  the question is how do
that minimizing kT ∗Q − Qkp
we frame this optimization problem. Indeed kT ∗Q − Qkp
is a non-convex and a non-
diﬀerentiable function with respect to Q  thus a direct minimization could lead us to bad
solutions. In the next section  we propose a method to alleviate those diﬃculties.

p µN

p µN

4 Reduction to a DC problem
Here  we frame the minimization of the empirical norm of the OBR as a DC problem and
instantiate a general algorithm  DCA [20]  that tries to solve it. First  we provide a short
introduction to diﬀerence of convex functions.

4.1 DC background
Let E be a ﬁnite dimensional Hilbert space and h.  .iE  k.kE its dot product and norm
respectively. We say that a function f ∈ RE is DC if there exists g  h ∈ RE which are
convex and lower semi-continuous such that f = g − h. The set of DC functions is noted
DC(E) and is stable to most of the operations that can be encountered in optimization 
i=1 be a sequence of K ∈ N∗ DC
contrary to the set of convex functions. Indeed  let (fi)K
i=1 fi  min1≤i≤K fi  max1≤i≤K fi and |fi|
functions and (αi)K
are DC functions [11]. In order to minimize a DC function f = g − h  we need to deﬁne a
notion of diﬀerentiability for convex and lower semi-continuous functions. Let g be such a
function and e ∈ E  we deﬁne the sub-gradient ∂eg of g in e as:

i=1 ∈ RK then PK

i=1 αifi  QK

∂eg = {δ ∈ E ∀e0 ∈ E  g(e0) ≥ g(e) + he0 − e  δiE}.

For a convex and lower semi-continuous g ∈ RE  the sub-gradient ∂eg is non empty for all
e ∈ E [11]. This observation leads to a minimization method of a function f ∈ DC(E)
called Diﬀerence of Convex functions Algorithm (DCA). Indeed  as f is DC  we have:

∀(e  e0) ∈ E2  f(e0) = g(e0) − h(e0) ≤
(a)

g(e0) − h(e) − he0 − e  δiE 

where δ ∈ ∂eh and inequality (a) is true by deﬁnition of the sub-gradient. Thus  for all
e ∈ E  the function f is upper bounded by a function fe ∈ RE deﬁned for all e0 ∈ E by
fe(e0) = g(e0) − h(e) − he0 − e  δiE. The function fe is a convex and lower semi-continuous
function (as it is the sum of two convex and lower semi-continuous functions which are g
and the linear function ∀e0 ∈ E he − e0  δiE − h(e)). In addition  those functions have the
particular property that ∀e ∈ E  f(e) = fe(e). The set of convex functions (fe)e∈E that
upper-bound the function f plays a key role in DCA.
The algorithm DCA [20] consists in constructing a sequence (en)n∈N such that the sequence
(f(en))n∈N decreases. The ﬁrst step is to choose a starting point e0 ∈ E  then we minimize
the convex function fe0 that upper-bounds the function f. We note e1 a minimizer of fe0 
e1 ∈ argmine∈E fe0. This minimization can be realized by any convex optimization solver.
As f(e0) = fe0(e0) ≥ fe0(e1) and fe0(e1) ≥ f(e1)  then f(e0) ≥ f(e1). Thus  if we construct
the sequence (en)n∈N such that ∀n ∈ N  en+1 ∈ argmine∈E fen and e0 ∈ E  then we obtain a
decreasing sequence (f(en))n∈N. Therefore  the algorithm DCA solves a sequence of convex
optimization problems in order to solve a DC optimization problem. Three important
choices can radically change the DCA performance: the ﬁrst one is the explicit choice of
the decomposition of f  the second one is the choice of the starting point e0 and ﬁnally the
choice of the intermediate convex solver. The DCA algorithm hardly guarantee convergence
to the global optima  but it usually provides good solutions. Moreover  it has some nice
properties when one of the functions g or h is polyhedral. A function g is said polyhedral
when ∀e ∈ E  g(e) = max1≤i≤K[hαi  eiH + βi]  where (αi)K
i=1 ∈ RK. If
one of the function g  h is polyhedral  f is under bounded and the DCA sequence (en)n∈N
is bounded  the DCA algorithm converges in ﬁnite time to a local minima. The ﬁnite time
aspect is quite interesting in term of implementation. More details about DC programming
and DCA are given in [20] and even conditions for convergence to the global optima.

i=1 ∈ EK and (βi)K

5

4.2 The OBR minimization framed as a DC problem
A ﬁrst important result is that for any choice of p ≥ 1  the OBRM is actually a DC problem.
Theorem 3. Let J p
(θ) is
a DC functions when p ∈ N∗.

be a function from Rd to reals  J p

(θ) = kT ∗Qθ − Qθkp

p µN

p µN

p µN

X

s0∈S

Proof. Let us write J p

as:

p µN

J p
p µN

(θ) = 1

N

NX

i=1

|hφ(Si  Ai)  θi − R(Si  Ai) − γ

P(s0|Si  Ai) max
a∈A

hφ(s0  a)  θi|p.

tinuous functions. In addition  the function hi = γP

First  as for each (Si  Ai) the linear function hφ(Si  Ai)  .i is convex and continuous  the aﬃne
function gi = hφ(Si  Ai)  .i + R(Si  Ai) is convex and continuous. Therefore  the function
maxa∈Ahφ(s0  a)  .i is also convex and continuous as a ﬁnite maximum of convex and con-
s0∈S P(s0|Si  Ai) maxa∈Ahφ(s0  a)  .i| is
convex and continuous as a positively weighted ﬁnite sum of convex and continuous func-
tions. Thus  the function fi = gi − hi is a DC function. As an absolute value of a DC
function is DC  a ﬁnite product of DC functions is DC and a weighted sum of DC functions
is DC  then J p

PN
i=1 |fi|p is a DC function.

= 1

p µN

N

p µN

p µN

p µN

is DC is not suﬃcient in order to use the DCA algorithm.
as a diﬀerence of two convex functions.

However  knowing that J p
Indeed  we need an explicit decomposition of J p
We present two polyhedral explicit decompositions of J p
Theorem 4. There exists explicit polyhedral decompositions of J p
For p = 1:
and H1 µN = 1

when p = 1 and p = 2.
PN
J1 µN = G1 µN − H1 µN   where G1 µN = 1
i=1 2 max(gi  hi)
i=1(gi + hi)  with gi = hφ(Si  Ai)  .i + R(Si  Ai) and hi =
= G2 µN − H2 µN   where G2 µN = 1

when p = 1 and when p = 2.

2
i + h
i ] and H2 µN =

γP
s0∈S P(s0|Si  Ai) maxa∈Ahφ(s0  a)  .i.
PN
i=1[gi + hi]2 with:
gi = max(gi  hi) + gi −

For p = 2: J2
1
N

hφ(Si  Ai) + γ

PN

PN

i=1[g2

2 µN

p µN

N

N

N

 
 

P(s0|Si  Ai)φ(s0  a1)  .i − R(Si  Ai)

P(s0|Si  Ai)φ(s0  a1)  .i − R(Si  Ai)

!
!

 

.

X
X

s0∈S

s0∈S

hi = max(gi  hi) + hi −

hφ(Si  Ai) + γ

Proof. The proof is provided in the supplementary material.

= Gp µN − Hp µN
Unfortunately  there is currently no guarantee that DCA applied to J p
outputs QN ∈ argminQ∈Q kT ∗Q − Qkp µN . The error between the output ˆQN of DCA and
QN is not studied here but it is a nice theoretical perspective for future works.

p µN

that

γP
N0PN0

4.3 The batch scenario
it was possible to calculate T ∗Q(s  a) = R(s  a) +
Previously  we admit
s0∈S P(s0|s  a) maxb∈A Q(s0  b). However  if the number of next states s0 for a given
couple (s  a) is too large or if T ∗ is unknown  this can be intractable. A solution 
when we have a simulator  is to generate for each couple (Si  Ai) a set of N0 samples
j=1 and provide a non-biased estimation of T ∗Q(Si  Ai): ˆT ∗Q(Si  Ai) = R(Si  Ai) +
(S0
i j  a). Even if | ˆT ∗Q(Si  ai) − Q(Si  Ai)|p is a biased estimator of
j=1 maxa∈A Q(S0
γ 1
|T ∗Q(Si  Ai) − Q(Si  Ai)|p  this biais can be controlled by the number of samples N0.
In the case where we do not have such a simulator  but only sampled transitions
(Si  Ai  S0
i)N
i=1 (the batch scenario)  it is possible to provide a non-biased estimation of

i j)N0

6

ˆT ∗Q(Si  Ai) = R(Si  Ai) + γ maxb∈A Q(S0

T ∗Q(Si  Ai) via:
i  b). However in that case 
| ˆT ∗Q(Si  Ai) − Q(Si  Ai)|p is a biased estimator of |T ∗Q(Si  Ai) − Q(Si  Ai)|p and the
biais is uncontrolled [2].
In order to alleviate this typical problem from the batch sce-
nario  several techniques have been proposed in the literature to provide a better es-
timator | ˆT ∗Q(Si  Ai) − Q(Si  Ai)|p  such as embeddings in Reproducing Kernel Hilbert
Spaces (RKHS)[13] or locally weighted averager such as Nadaraya-Watson estimators[21].
In both cases  the non-biased estimation of T ∗Q(Si  Ai) takes the form ˆT ∗Q(Si  Ai) =
R(Si  Ai) + γ 1
j) represents the weight of the
PN
j in the estimation of T ∗Q(Si  Ai). To obtain an explicit DC decomposition 
samples S0
i=1 | ˆT ∗Qθ(Si  Ai) − Qθ(Si  Ai)|p it is suﬃ-
when p = 1 or p = 2  of ˆJ p
j  a)
.

PN
j=1 βi(S0
cient to replace P
N0PN0

j) maxa∈A Q(S0
i j  a) if we have a simulator) in the DC decomposition of J p

s0∈S P(s0|Si  Ai) maxa∈Ahφ(s0  a)  θi by 1

PN
j=1 βi(S0

j  a)  where βi(S0

j=1 maxa∈A Q(S0

j) maxa∈A Q(S0

(or 1

p µN

N

(θ) = 1

N

p µN

N

5 Illustration

ks− s0k2 =Pi=d

i=1(si − s0i)2. Thus  we obtain MDPs with a state space size ofQd

This experiment focuses on stationary Garnet problems  which are a class of randomly
constructed ﬁnite MDPs representative of the kind of ﬁnite MDPs that might be en-
countered in practice [3]. A stationary Garnet problem is characterized by 3 parameters:
Garnet(NS  NA  NB). The parameters NS and NA are the number of states and actions
respectively  and NB is a branching factor specifying the number of next states for each
state-action pair. Here  we choose a particular type of Garnets which presents a topolog-
ical structure relative to real dynamical systems and aims at simulating the behavior of
a smooth continuous-state MDPs (as described in Sec. 3). Those systems are generally
multi-dimensional state spaces MDPs where an action leads to diﬀerent next states close
to each other. The fact that an action leads to close next states can model the noise in
a real system for instance. Thus  problems such as the highway simulator [12]  the moun-
tain car or the inverted pendulum (possibly discretized) are particular cases of this type
of Garnets. For those particular Garnets  the state space is composed of d dimensions
(d = 2 in this particular experiment) and each dimension i has a ﬁnite number of elements
xi (xi = 10). So  a state s = [s1  s2  ..  si  ..  sd] is a d-uple where each composent si can
take a ﬁnite value between 1 and xi. In addition  the distance between two states s  s0 is
i=1 xi. The
number of actions is NA = 5. For each state action couple (s  a)  we choose randomly NB
next states (NB = 5) via a Gaussian distribution of d dimensions centered in s where the
covariance matrix is the identity matrix of size d  Id  multiply by a term σ (here σ = 1).
This allows handling the smoothness of the MDP: if σ is small the next states s0 are close
to s and if σ is large  the next states s0 can be very far form each other and also from s.
The probability of going to each next state s0 is generated by partitioning the unit interval
at NB − 1 cut points selected randomly. For each couple (s  a)  the reward R(s  a) is drawn
uniformly between −1 and 1. For each Garnet problem  it is possible to compute an optimal
policy π∗ thanks to the policy iteration algorithm.
In this experiment  we construct 50 Garnets {Gp}1≤p≤50 as explained before. For each Gar-
net Gp  we build 10 data sets {Dp q}1≤q≤10 composed of N sampled transitions (si  ai  s0
i)N
i=1
drawn uniformly and independently. Thus  we are in the batch scenario. The minimiza-
tion of J1 N and J2 N via the DCA algorithms  where the estimation of T ∗Q(si  ai) is done
via R(si  ai) + γ maxb∈A Q(s0
i  b) (so uncontrolled biais)  are called DCA1 and DCA2 re-
spectively. The initialisation of DCA is θ0 = 0 and the intermediary optimization convex
problems are solved by a sub-gradient descent [18]. Those two algorithms are compared
with state-of the art Reinforcement Learning algorithms which are LSPI (API implemen-
tation) and Fitted-Q (AVI implementation). The four algorithms uses the tabular ba-
sis. Each algorithm outputs a function Qp q
A is
A (s  a). In order to quantify the performance of a given algorithm 
A (s) = argmaxa∈A Qp q
πp q
A = Eρ[V π∗−V π
A is computed via the policy
  where V πp q
we calculate the criterion T p q
Eρ[|V π∗|]
evaluation algorithm. The mean performance criterion TA is
A . We also

A ∈ RS×A and the policy associated to Qp q

P50

P10

q=1 T p q

p q
A ]

p=1

1
500

7

q=1(T p q
A )2
calculate  for each algorithm  the variance criterion stdp
and the resulting mean variance criterion is stdA = 1
A. In Fig. 1(a)  we plot the
50
performance versus the number of samples. We observe that the 4 algorithms have similar
performances  which shows that our alternative approach is competitive. In Fig. 1(b)  we

A = 1
10
p=1 stdp

A − 1
10

q=1 T p q

P10

P50

P10

(a) Performance

(b) Standard deviation

Figure 1: Garnet Experiment

plot the standard deviation versus the number of samples. Here  we observe that DCA
algorithms have less variance which is an advantage. This experiment shows us that DC
programming is relevant for RL but still has to prove its eﬃciency on real problems.

6 Conclusion and Perspectives

In this paper  we presented an alternative approach to tackle the problem of solving large
MDPs by estimating the optimal action-value function via DC Programming. To do so  we
ﬁrst showed that minimizing a norm of the OBR is interesting. Then  we proved that the
empirical norm of the OBR is consistant in the Vapnick sense (strict consistency). Finally 
we framed the minimization of the empirical norm as DC minimization which allows us
to rely on the literature on DCA. We conduct a generic experiment with a basic setting
for DCA as we choose a canonical explicit decomposition of our DC functions criterion
and a sub-gradient descent in order to minimize the intermediary convex minimization
problems. We obtain similar results to AVI and API. Thus  an interesting perspective would
be to have a less naive setting for DCA by choosing diﬀerent explicit decompositions and
ﬁnd a better convex solver for the intermediary convex minimization problems. Another
interesting perspective is that our approach can be non-parametric.
Indeed  as pointed
in [10] a convex minimization problem can be solved via boosting techniques which avoids
the choice of features. Therefore  each intermediary convex problem of DCA could be
solved via a boosting technique and hence make DCA non-parametric. Thus  seeing the RL
problem as a DC problem provides some interesting perspectives for future works.

Acknowledgements

The research leading to these results has received partial funding from the European Union
Seventh Framework Program (FP7/2007-2013) under grant agreement number 270780 and
the ANR ContInt program (MaRDi project  number ANR- 12-CORD-021 01). We also
would like to thank professors Le Thi Hoai An and Pham Dinh Tao for helpful discussions
about DC programming.

8

020040060080010000.40.50.60.70.80.911.1Number of samplesPerformance LSPIDCA1DCA2randFitted−Q0200400600800100000.020.040.060.080.10.120.14Number of samplesStandard deviation LSPIDCA1DCA2Fitted−QReferences
[1] A. Antos  R. Munos  and C. Szepesv´ari. Fitted-Q iteration in continuous action-space

MDPs. In Proc. of NIPS  2007.

[2] A. Antos  C. Szepesv´ari  and R. Munos. Learning near-optimal policies with Bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine
Learning  2008.

[3] T. Archibald  K. McKinnon  and L. Thomas. On the generation of Markov decision

processes. Journal of the Operational Research Society  1995.

[4] M.G. Azar  V. G´omez  and H.J Kappen. Dynamic policy programming. The Journal

of Machine Learning Research  13(1)  2012.

[5] L. Baird. Residual algorithms: reinforcement learning with function approximation. In

Proc. of ICML  1995.

[6] D.P. Bertsekas. Dynamic programming and optimal control  volume 1. Athena Scien-

tiﬁc  Belmont  MA  1995.

[7] D.P. de Farias and B. Van Roy. The linear programming approach to approximate

dynamic programming. Operations Research  51  2003.

[8] Vijay Desai  Vivek Farias  and Ciamac C Moallemi. A smoothed approximate linear

program. In Proc. of NIPS  pages 459–467  2009.

[9] A. Farahmand  R. Munos  and Csaba. Szepesv´ari. Error propagation for approximate

policy and value iteration. Proc. of NIPS  2010.

[10] A. Grubb and J.A. Bagnell. Generalized boosting algorithms for convex optimization.

In Proc. of ICML  2011.

[11] J.B Hiriart-Urruty. Generalized diﬀerentiability  duality and optimization for problems
dealing with diﬀerences of convex functions. In Convexity and duality in optimization.
Springer  1985.

[12] E. Klein  M. Geist  B. Piot  and O. Pietquin. Inverse reinforcement learning through

structured classiﬁcation. In Proc. of NIPS  2012.

[13] G. Lever  L. Baldassarre  A. Gretton  M. Pontil  and S. Gr¨unew¨alder. Modelling tran-

sition dynamics in MDPs with RKHS embeddings. In Proc. of ICML  2012.

[14] O. Maillard  R. Munos  A. Lazaric  and M. Ghavamzadeh. Finite-sample analysis of

Bellman residual minimization. In Proc. of ACML  2010.

[15] R. Munos. Performance bounds in Lp-norm for approximate value iteration. SIAM

journal on control and optimization  2007.

[16] M.L. Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons  1994.

[17] B. Scherrer. Approximate policy iteration schemes: a comparison. In Proc. of ICML 

2014.

[18] N.Z. Shor  K.C. Kiwiel  and A. Ruszcaynski. Minimization methods for non-

diﬀerentiable functions. Springer-Verlag  1985.

[19] P.D. Tao and L.T.H. An. Convex analysis approach to DC programming: theory 

algorithms and applications. Acta Mathematica Vietnamica  22:289–355  1997.

[20] P.D. Tao and L.T.H. An. The DC programming and DCA revisited with DC models of
real world nonconvex optimization problems. Annals of Operations Research  133:23–
46  2005.

[21] G. Taylor and R. Parr. Value function approximation in noisy environments using

locally smoothed regularized approximate linear programs. In Proc. of UAI  2012.

[22] V. Vapnik. Statistical learning theory. Wiley  1998.

9

,Bilal Piot
Matthieu Geist
Olivier Pietquin
Surbhi Goel
Adam Klivans
Stephen Gillen
Christopher Jung
Michael Kearns
Aaron Roth