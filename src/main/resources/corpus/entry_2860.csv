2011,Learning Anchor Planes for Classification,Local Coordinate Coding (LCC) [18] is a method for modeling functions of data lying on non-linear manifolds. It provides a set of anchor points which form a local coordinate system  such that each data point on the manifold can be approximated by a linear combination of its anchor points  and the linear weights become the local coordinate coding. In this paper we propose encoding data using orthogonal  anchor planes  rather than anchor points. Our method needs only a few orthogonal anchor planes for coding  and it can linearize any (\alpha \beta p)-Lipschitz smooth nonlinear  function with a fixed expected value of the upper-bound approximation error on any high dimensional data. In practice  the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (SVD). We apply our method to model the coordinates locally in linear SVMs for classification tasks  and our experiment on MNIST shows that using only 50 anchor planes our method achieves 1.72% error rate  while LCC achieves 1.90% error rate using 4096 anchor points.,Learning Anchor Planes for Classiﬁcation

Ziming Zhang† L’ubor Ladický‡

Philip H.S. Torr† Amir Saffari†§

† Department of Computing  Oxford Brookes University  Wheatley  Oxford  OX33 1HX  U.K.
‡ Department of Engineering Science  University of Oxford  Parks Road  Oxford  OX1 3PJ  U.K.

§ Sony Computer Entertainment Europe  London  UK

{ziming.zhang  philiptorr}@brookes.ac.uk

lubor@robots.ox.ac.uk

amir@ymer.org

Abstract

Local Coordinate Coding (LCC) [18] is a method for modeling functions of data
lying on non-linear manifolds. It provides a set of anchor points which form a local
coordinate system  such that each data point on the manifold can be approximated
by a linear combination of its anchor points  and the linear weights become the
local coordinate coding. In this paper we propose encoding data using orthogonal
anchor planes  rather than anchor points. Our method needs only a few orthogonal
anchor planes for coding  and it can linearize any (α  β  p)-Lipschitz smooth non-
linear function with a ﬁxed expected value of the upper-bound approximation error
on any high dimensional data. In practice  the orthogonal coordinate system can be
easily learned by minimizing this upper bound using singular value decomposition
(SVD). We apply our method to model the coordinates locally in linear SVMs
for classiﬁcation tasks  and our experiment on MNIST shows that using only 50
anchor planes our method achieves 1.72% error rate  while LCC achieves 1.90%
error rate using 4096 anchor points.

1

Introduction

Local Coordinate Coding (LCC) [18] is a coding scheme that encodes the data locally so that any
non-linear (α  β  p)-Lipschitz smooth function (see Deﬁnition 1 in Section 2 for details) over the data
manifold can be approximated using linear functions. There are two components in this method: (1)
a set of anchor points which decide the local coordinates  and (2) the coding for each data based
on the local coordinates given the anchor points. Theoretically [18] suggests that under certain
assumptions  locality is more essential than sparsity for non-linear function approximation. LCC has
been successfully applied to many applications such like object recognition (e.g. locality-constraint
linear coding (LLC) [16]) in VOC 2009 challenge [7].
One big issue in LCC is that its classiﬁcation performance is highly dependent on the number of
anchor points  as observed in Yu and Zhang [19]  because these points should be “local enough”
to encode surrounding data on the data manifold accurately  which sometimes means that in real
applications the number of anchor points explodes to a surprisingly huge number. This has been
demonstrated in [18] where LCC has been tested on MNIST dataset  using from 512 to 4096 anchor
points learned from sparse coding  the error rate decreased from 2.64% to 1.90%. This situation
could become a serious problem when the distribution of the data points is sparse in the feature
space  i.e. there are many “holes” between data points (e.g. regions of feature space that are sparsely
populated by data). As a result of this  many redundant anchor points will be distributed in the holes
with little information. By using many anchor points  the computational complexity of the classiﬁer
at both training and test time increases signiﬁcantly  defeating the original purpose of using LCC.

1

So far several approaches have been proposed for problems closely related to anchor point learning
such as dictionary learning or codebook learning. For instance  Lee et. al. [12] proposed learning
the anchor points for sparse coding using the Lagrange dual. Mairal et. al. [13] proposed an online
dictionary learning algorithm using stochastic approximations. Wang et. al. [16] proposed locality-
constraint linear coding (LLC)  which is a fast implementation of LCC  and an online incremental
codebook learning algorithm using coordinate descent method  whose performance is very close to
that using K-Means. However  none of these algorithms can deal with holes of sparse data as they
need many anchor points.
In this paper  we propose a method to approximate any non-linear (α  β  p)-Lipschitz smooth func-
tion using an orthogonal coordinate coding (OCC) scheme on a set of orthogonal basis vectors. Each
basis vector v ∈ Rd deﬁnes a family of anchor planes  each of which can be considered as consist-
ing of inﬁnite number of anchor points  and the nearest point on each anchor plane to a data point
x ∈ Rd is used for coding  as illustrated in Figure 1. The data point x will be encoded based on
the margin  xT v where (·)T denotes the matrix transpose operator  between x and an anchor plane
deﬁned by v. The beneﬁts of using anchor planes are:

• A few anchor planes can replace many anchor points while preserving similar locality of
anchor points. This sparsity may lead to a better generalization since many anchor points
will overﬁt the data easily. Therefore  it can deal with the hole problem in LCC.

• The learned orthogonal basis vectors can ﬁt naturally into locally linear SVM’s (such as

[9 10 11 19 21]) which we describe below.

Theoretically we show that using OCC any (α  β  p)-Lipschitz smooth non-linear function can be
linearized with a ﬁxed upper-bound approximation error.
In practice by minimizing this upper
bound  the orthogonal basis vectors can be learned using singular value decomposition (SVD). In
our experiments  We integrate OCC into LL-SVM for classiﬁcation.
Linear support vector machines have become popular for solving classiﬁcation tasks due to their
fast and simple online application to large scale data sets. However  many problems are not linearly
separable. For these problems kernel-based SVMs are often used  but unlike their linear variant they
suffer from various drawbacks in terms of computational and memory efﬁciency. Their response
can be represented only as a function of the set of support vectors  which has been experimentally
shown to grow linearly with the size of the training set. A recent trend has grown to create a classiﬁer
locally based on a set of linear SVMs [9 10 11 19 21]. For instance  in [20] SVMs are trained only
based on the N nearest neighbors of each data  and in [9] multiple kernel learning was applied
locally. In [10] Kecman and Brooks proved that the stability bounds for local SVMs are tighter than
the ones for traditional  global  SVMs. Ladicky and Torr [11] proposed a novel locally linear SVM
classiﬁer (LL-SVM) with smooth decision boundary and bounded curvature. They show how the
functions deﬁning the classiﬁer can be approximated using local codings and show how this model
can be optimized in an online fashion by performing stochastic gradient descent with the same
convergence guarantees as standard gradient descent method for linear SVMs. Mathematically LL-
SVM is formulated as follows:

(cid:88)

1
|S|
ξk
ξk ≥ 1 − yk

k∈S

arg min

W b

(cid:107)W(cid:107)2 +

λ
2

s.t. ∀k ∈ S :

(cid:2)γT

xk

Wxk + γT
xk

b(cid:3)   ξk ≥ 0

(1)

∈ RN is its local coding 
where ∀k  xk ∈ Rd is a training vector  yk ∈ {−1  1} is its label  γxk
λ ≥ 0 is a pre-deﬁned scalar  and W ∈ RN×d and b ∈ RN are the model parameters. As
demonstrated in our experiments  the choices of the local coding methods are very important for
LL-SVM  and an improper choice will hurt its performance.
The rest of the paper is organized as follows. In Section 2 we ﬁrst recall some deﬁnitions and lemmas
in LCC  then introduce OCC for non-linear function approximation and its property on the upper
bound of localization error as well as comparing OCC with LCC in terms of geometric interpretation
and optimization. In Section 3  we explain how to ﬁt OCC into LL-SVM to model the coordinates
for classiﬁcation. We show our experimental results and comparison in Section 4  and conclude the
paper in Section 5.

2

2 Anchor Plane Learning

In this section  we introduce our Orthogonal Coordinate Coding (OCC) based on some orthogonal
basis vectors. For clariﬁcation  we summarize some notations in Table 1 which are used in LCC and
OCC.

Notation
v ∈ Rd
C ⊂ Rd

C ∈ Rd×|C|
γv(x) ∈ R
γ(x) ∈ Rd
γx ∈ R|C|
(γ C)

γ

Table 1: Some notations used in LCC and OCC.

Deﬁnition

A d-dimensional anchor point in LCC; a d-dimensional basis vector which deﬁnes a
family of anchor planes in OCC.
A subset in d-dimensional space containing all the anchor points (∀v  v ∈ C) in LCC;
a subset in d-dimensional space containing all the basis vectors in OCC.
The anchor point (or basis vector) matrix with v ∈ C as columns.
The local coding of a data point x ∈ Rd using the anchor point (or basis vector) v.
The physical approximation vector of a data point x.
The coding vector of data point x containing all γv(x) in order γx = [γv(x)]v∈C.
A map of x ∈ Rd to γx.
A coordinate coding.

2.1 Preliminary

We ﬁrst recall some deﬁnitions and lemmas in LCC based on which we develop our method. Notice
that in the following sections  (cid:107) · (cid:107) denotes the (cid:96)2-norm without explicit explanation.
Deﬁnition 1 (Lipschitz Smoothness [18]). A function f (x) on Rd is (α  β  p)-Lipschitz smooth with
respect to a norm (cid:107)·(cid:107) if |f (x(cid:48))−f (x)| ≤ α(cid:107)x−x(cid:48)(cid:107) and |f (x(cid:48))−f (x)−∇f (x)T (x(cid:48)−x)| ≤ β(cid:107)x−x(cid:48)(cid:107)1+p 
where we assume α  β > 0 and p ∈ (0  1].
Deﬁnition 2 (Coordinate Coding [18]). A coordinate coding is a pair (γ C)  where C ⊂ Rd is a set
v γv(x) = 1. It induces the
v∈C γv(x)v. Moreover  for all x ∈ Rd  we deﬁne the

of anchor points  and γ is a map of x ∈ Rd to [γv(x)]v∈C ∈ R|C| such that(cid:80)
following physical approximation of x in Rd: γ(x) =(cid:80)
corresponding coding norm as (cid:107)x(cid:107)γ = ((cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ α(cid:107)x − γ(x)(cid:107) + β
should be localized (i.e. smaller localization error(cid:80)

Lemma 1 (Linearization [18]). Let (γ C) be an arbitrary coordinate coding on Rd. Let f be an (α  β  p)-
Lipschitz smooth function. We have for all x ∈ Rd:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)f (x) −(cid:88)

As explained in [18]  a good coding scheme for non-linear function approximation should make x
close to its physical approximation γ(x) (i.e. smaller data reconstruction error (cid:107)x − γ(x)(cid:107)) and
v∈C |γv(x)|(cid:107)v − γ(x)(cid:107)1+p). This is the basic
(cid:88)

idea of LCC.
Deﬁnition 3 (Localization Measure [18]). Given α  β  p  and coding (γ C)  we deﬁne

(cid:34)

(cid:35)

|γv(x)|(cid:107)v − γ(x)(cid:107)1+p

v∈C γv(x)2)1/2.

Qα β p(γ C) = Ex

α(cid:107)x − γ(x)(cid:107) + β

|γv(x)|(cid:107)v − γ(x)(cid:107)1+p

γv(x)f (v)

v∈C

(2)

(3)

(cid:88)

v∈C

v∈C

Localization measure is equivalent to the expectation of the upper bound of the approximate error.

2.2 Orthogonal Coordinate Coding

In the following sections  we will follow the notations in Table 1  and deﬁne our orthogonal coordi-
nate coding (OCC) as below.
Deﬁnition 4 (Orthogonal Coordinate Coding). An orthogonal coordinate coding is a pair (γ C) 
where C ⊂ Rd contains |C| orthogonal basis vectors  that is  ∀u  v ∈ C  if u (cid:54)= v  then uT v = 0 
v∈C |γv(x)| =

and coding γ is a map of x ∈ Rd to [γv(x)]v∈C ∈ R|C| such that γv(x) ∝ xT v(cid:107)v(cid:107)2 and(cid:80)

1.

3

Figure 1: Comparison of the geometric views on (a) LCC and (b) OCC  where the white and red dots denote
the data and anchor points  respectively. In LCC  the anchor points are distributed among the data space and
several nearest neighbors around the data are selected for data reconstruction  while in OCC the anchor points
are located on the anchor plane deﬁned by the normal vector (i.e. coordinate  basis vector) v and only the
closest point to each data point on the anchor plane is selected for coding. The ﬁgures are borrowed from the
slides of [17]  and best viewed in color.

Compared to Deﬁnition 2  there are two changes in OCC: (1) instead of anchor points we use a set
of orthogonal basis vectors  which deﬁnes a set of anchor planes  and (2) the coding for each data
point is deﬁned on the (cid:96)1-norm unit ball  which removes the scaling factors in both x and v. Notice
that since given the data matrix  the maximum number of orthogonal basis vectors which can be
used to represent all the data precisely is equal to the rank of the data matrix  the maximum value of
|C| is equal to the rank of the data matrix as well.
Figure 1 illustrates the geometric views on LCC and OCC respectively. Intuitively  in both methods
anchor points try to encode data locally. However  the ways of their arrangement are quite different.
In LCC anchor points are distributed among the whole data space such that each data can be covered
by certain anchor points in a local region  and their distribution cannot be described using regular
shapes. On the contrary  the anchor points in OCC are located on the anchor plane deﬁned by
a basis vector. In fact  each anchor plane can be considered as inﬁnite number of anchor points 
and for each data point only its closest point on each anchor plane is utilized for reconstruction.
Therefore  intuitively the number of anchor planes in OCC should be much fewer than the number
of anchor points in LCC.
Theorem 1 (Localization Error of OCC). Let (γ C) be an orthogonal coordinate coding on Rd
where C ⊂ Rd with size |C| = M. Let f be an (α  β  p)-Lipschitz smooth function. Without
losing generalization  assuming ∀x ∈ Rd (cid:107)x(cid:107) ≤ 1 and ∀v ∈ C  1 ≤ (cid:107)v(cid:107) ≤ h(h ≥ 1)  then the
localization error in Lemma 1 is bounded by:

(cid:107)x(cid:107)(cid:107)v(cid:107)
(cid:107)v(cid:107)2 ≤ M

v∈C

v∈C

4

(cid:88)

v∈C

v∈C

v∈C

(1 + M )h

(cid:105)1+p

|xT v|
(cid:107)v(cid:107)2   then

Proof. Let γv(x) = xT v

|γv(x)|(cid:107)v − γ(x)(cid:107)1+p =

|γv(x)|(cid:107)v − γ(x)(cid:107)1+p ≤(cid:104)

(cid:88)
sx(cid:107)v(cid:107)2   where sx =(cid:80)
|γv(x)|(cid:104)(cid:107)v(cid:107)2 − 2sxγv(x)(cid:107)v(cid:107)2 +
|γv(x)|(cid:104)(cid:107)v(cid:107)2 + 2sx(cid:107)v(cid:107)2|γv(x)| +
xγv(x)2(cid:107)v(cid:107)2(cid:105) 1+p
(cid:88)
|γv(x)|(cid:104)(cid:107)v(cid:107)2 + 2sx(cid:107)v(cid:107)2|γv(x)| +
(cid:16)(cid:88)
γv(x)2(cid:17)(cid:16)
∵ ∀x ∈ Rd (cid:107)x(cid:107) ≤ 1 and ∀v ∈ C  1 ≤ (cid:107)v(cid:107) ≤ h(h ≥ 1) (cid:80)
v∈C γv(x)2 ≤ 1  sx =(cid:80)
∴ ∀v ∈ C |γv(x)| ≤ 1 (cid:80)
(cid:107)v(cid:107)2 ≤(cid:80)

v∈C
v∈C |γv(x)| = 1 

≤(cid:88)
≤(cid:88)

v∈C

(cid:88)

v∈C

(cid:88)

v∈C

|xT v|

s2

v∈C

v∈C

.

s2

2

xγv(x)2(cid:107)v(cid:107)2(cid:105) 1+p
x(cid:107)v(cid:107)2(cid:17)(cid:105) 1+p

v∈C s2
max

2

2

(4)

(5)

|γv(x)|(cid:107)v − γ(x)(cid:107)1+p ≤ (cid:88)

∴(cid:88)

v∈C

2

2

v∈C

h2 + 2M h2|γv(x)| + M 2h2(cid:105) 1+p
|γv(x)|(cid:104)
1 + 2M|γv(x)| + M 2(cid:105) 1+p
|γv(x)|(cid:104)
= h1+p(cid:88)
≤ h1+p(cid:16)(cid:88)
|γv(x)|(cid:17)(cid:16)
2 (cid:27)
(cid:26)(cid:104)
1 + 2M|γv(x)| + M 2(cid:105) 1+p
1 + 2M + M 2(cid:105) 1+p
≤ h1+p(cid:104)
(cid:105)1+p

2 (cid:27)(cid:17)
(cid:26)(cid:104)
1 + 2M|γv(x)| + M 2(cid:105) 1+p
(cid:104)

v∈C
= h1+p · max
v∈C

(1 + M )h

max
v∈C

v∈C

.

2

=

(6)

(7)

(8)

2.3 Learning Orthogonal Basis Vectors

(cid:110) 1
(cid:88)
s.t. ∀x  (cid:80)

x∈X

2

min
(γ C)

(cid:88)

v∈C

Instead of optimizing Deﬁnition 3  LCC simpliﬁes the localization error term by assuming γ(x) = x
and p = 1. Mathematically LCC solves the following optimization problem:

(cid:107)x − γ(x)(cid:107)2 + µ

|γv(x)|(cid:107)v − x(cid:107)2 + λ

(cid:107)v(cid:107)2(cid:111)

(cid:88)

v∈C

v∈C γv(x) = 1.

They update C and γ via alternating optimization. The step of updating γ can be transformed into a
canonical LASSO problem  and the step of updating C is a least squares problem.
For OCC  given an (α  β  p)-Lipschitz smooth function f and a set of data X ⊂ Rd  whose corre-
sponding data matrix and its rank are denoted as X and D  respectively  we would like to learn an
orthogonal coordinate coding (γ C) where the number of basis vectors |C| = M ≤ D such that the
localization measure of this coding is minimized. Since Theorem 1 proves that the localization error
per data point is bounded by a constant given an OCC  in practice we only need to minimize the data
reconstruction error in order to minimize the upper bound of the localization measure. That is  we
need to solve the following problem:

(cid:88)

(cid:107)x − Cγx(cid:107)2

min
(γ C)
s.t. ∀u  v ∈ C  u (cid:54)= v ⇒ uT v = 0 

x∈X
|C| = M 
∀x 

(cid:107)γx(cid:107)1 = 1.

This optimization problem is quite similar to sparse coding [12]  except that there exists the orthog-
onal constraint on the basis vectors. In practice we relax this problem by removing the constraint
∀x (cid:107)γx(cid:107)1 = 1.
(I) Solving for C. Eqn. 8 can be solved ﬁrst using singular value decomposition (SVD). Let the
SVD of X = VΣU where the singular values are positive and in descending order with respect to
Σ. Then we set C = V{d×M}Σ{M×M}  where V{d×M} denotes a sub-matrix of V containing the
elements within rows from 1 to d and columns from 1 to M  similarly for Σ{M×M}. We need only
to use a few top eigenvectors as our orthogonal basis vectors for coding  and the search space is far
smaller than generating anchor points.
(II) Solving for γx. Since we have the orthogonal basis vectors in C  we can easily derive the for-
mulation for calculating ˜γx  the value of γx before normalization  that is  ˜γx = (CT C)−1CT x.
Letting {¯v} and {σv} be the corresponding singular vectors and singular values  based on the or-
thogonality of basis vectors we have ˜γv(x) = ¯vT x
  which is a variant of the coding deﬁnition in
σv
Deﬁnition 4. Finally  we can calculate γx by normalizing ˜γx as follows: γx = ˜γx(cid:107)˜γx(cid:107)1

.

5

3 Modeling Classiﬁcation Decision Boundary in SVM
Given a set of data {(xi  yi)} where yi ∈ {−1  1} is the label of xi  the decision boundary for binary
classiﬁcation of a linear SVM is f (x) = wT x + b where w is the normal vector of the decision
hyperplane (i.e. coefﬁcients) of the SVM and b is a bias term. Here  we assume that the decision
boundary is an (α  β  p)-Lipschitz smooth function. Since in LCC each data is encoded by some
anchor points on the data manifold  it can model the decision boundary of an SVM directly using
v∈C γv(x)f (v). Then by taking γx as the input data of a linear SVM  f (v)’s can be

f (x) ≈ (cid:80)

learned to approximate the decision boundary f.
However  OCC learns a set of orthogonal basis vectors  rather than anchor points  and corresponding
coding for data. This makes OCC suitable to model the normal vectors of decision hyperplanes in
SVMs locally with LL-SVM. Given data x and an orthogonal coordinate coding (γ C)  the decision
boundary in LL-SVM can be formulated as follows 1.

f (x) = w(x)T x + b =

γv(x)w(v)T x + b = γT

x Wx + b

(9)

(cid:88)

v∈C

where W ∈ RM×d is a matrix which needs to be learned for SVMs. In the view of kernel SVMs 
we actually deﬁne another kernel K based on x and γx as shown below.

(10)
where < · · > denotes the Frobenius inner product. Notice that in our kernel  latent semantic kernel
[6] has been involved which is deﬁned based on a set of orthogonal basis vectors.

i   γxj xT

j >

∀i  j  K(xi  xj) =< γxixT

4 Experiments

In our experiments  we test OCC with LL-SVM for classiﬁcation on the benchmark datasets:
MNIST  USPS and LETTER. The features we used are the raw features such that we can compare
our results fairly with others.
MNIST contains 40000 training and 10000 test gray-scale images with resolution 28×28  which are
normalized directly into 784 dimensional vectors. The label of each image is one of the 10 digits
from 0 to 9. USPS contains 7291 training and 2007 test gray-scale images with resolution 16 x 16 
directly stored as 256 dimensional vectors  and the label of each image still corresponds to one of
the 10 digits from 0 to 9. LETTER contains 16000 training and 4000 testing images  each of which
is represented as a relatively short 16 dimensional vector  and the label of each image corresponds
to one of the 26 letters from A to Z.
We re-implemented LL-SVM based on the C++ code of LIBLINEAR [8] 2 and PEGASOS [14] 3 
respectively  and performed multi-class classiﬁcation using the one-vs-all strategy. This aims to
test the effect of either quadratic programming or stochastic gradient based SVM solver on both
accuracy and computational time. We denote these two ways of LL-SVM as LIB-LLSVM and PEG-
LLSVM for short. We tried to learn our basis vectors in two ways: (1) SVD is applied directly to the
entire training data matrix  or (2) SVD is applied separately to the data matrix consisting of all the
positive training data. We denote these two types of OCC as G-OCC (i.e. Generic OCC) and C-OCC
(i.e. Class-speciﬁc OCC)  respectively. Then the coding for each data is calculated as explained in
Section 2.3. Next  all the training raw features and their coding vectors are taken as the input to train
the model (W  b) of LL-SVM. For each test data x  we calculate its coding in the same way and
classify it based on its decision values  that is  y(x) = arg maxy γT
Figure 2 shows the comparison of classiﬁcation error rates among G-OCC + LIB-LLSVM  G-OCC +
PEG-LLSVM  C-OCC + LIB-LLSVM  and C-OCC + PEG-LLSVM on MNIST (left)  USPS (middle) 
and LETTER (right)  respectively  using different numbers of orthogonal basis vectors. With the
same OCC  LIB-LLSVM performs slightly better than PEG-LLSVM in terms of accuracy  and both

x yWyx + by.

1Notice that Eqn. 9 is slightly different from the original formulation in [11] by ignoring the different bias

term for each orthogonal basis vector.

2Using LIBLINEAR  we implemented LL-SVM based on Eqn. 9.
3Using PEGASOS  we implemented LL-SVM based on the original formulation in [11].

6

Figure 2: Performance comparison among the 4 different combinations of OCC + LL-SVM on MNIST (left) 
USPS (middle)  and LETTER (right) using different numbers of orthogonal basis vectors. This ﬁgure is best
viewed in color.

It seems that in

behaves similarly with the increase of the number of orthogonal basis vectors.
general C-OCC is better than G-OCC.
Table 2 summarizes our comparison results between our methods and some other SVM based ap-
proaches. The parameters of the RBF kernel used in the kernel SVMs are the same as [2]. Since
there are no results of LCC on USPS and LETTER or its code  we tested the published code of LLC
[16] on these two datasets so that we can have a rough idea of how well LCC works. The anchor
points are found using K-Means. From Table 2  we can see that applying linear SVM directly on
OCC works slightly better than on the raw features  and when OCC is working with LL-SVM  the
performance is boosted signiﬁcantly while the numbers of anchor points that are needed in LL-SVM
are reduced. On MNIST we can see that our non-linear function approximation is better than LCC 
improved LCC  LLC  and LL-SVM  on USPS ours is better than both LLC and LL-SVM  but on
LETTER ours is worse than LLC (4096 anchor points) and LL-SVM (100 anchor points). The
reason for this is that strictly speaking LETTER is not a high dimensional dataset (only 16 dimen-
sions per data)  which limits the power of OCC. Compared with kernel based SVMs  our method
can achieve comparable or even better results (e.g. on USPS). All of these results demonstrate that
OCC is quite suitable to model the non-linear normal vectors using linear SVMs for classiﬁcation on
high dimensional data. In summary  our encoding scheme uses much less number of basis vectors
compared to anchor points in LCC while achieving better test accuracy  which translates to higher
performance both in terms of generalization and efﬁciency in computation.
We show our training and test time on these three datasets as well in Table 3 based on unoptimized
MATLAB code on a single thread of a 2.67 GHz CPU. For training  the time includes calculating
OCC and training LL-SVM. From this table  we can see that our methods are a little slower than the
original LL-SVM  but still much faster than kernel SVMs. The main reason for this is that OCC is
non-sparse while in [11] the coefﬁcients are sparse. However  for calculating coefﬁcients  OCC is
faster than [11]  because there is no distance calculation or K nearest neighbor search involved in
OCC  just simple multiplication and normalization.

5 Conclusion

In this paper  we propose orthogonal coordinate coding (OCC) to encode high dimensional data
based on a set of anchor planes deﬁned by a set of orthogonal basis vectors. Theoretically we prove
that our OCC can guarantee a ﬁxed upper bound of approximation error for any (α  β  p)-Lipschitz
smooth function  and we can easily learn the orthogonal basis vectors using SVD to minimize
the localization measure. Meanwhile  OCC can help locally linear SVM (LL-SVM) approximate
the kernel-based SVMs  and our experiments demonstrate that with a few orthogonal anchor
planes  LL-SVM can achieve comparable or better results than LCC and its variants improved
LCC and LLC with linear SVMs  and on USPS even better than kernel-based SVMs. In future  we
would like to learn the orthogonal basis vectors using semi-deﬁnite programming to guarantee the
orthogonality.

Acknowledgements. We thank J. Valentin  P. Sturgess and S. Sengupta for useful discussion
in this paper. This work was supported by the IST Programme of the European Community  under

7

Table 2: Classiﬁcation error rate comparison (%) between our methods and others on MNIST  USPS  and
LETTER. The numbers of anchor planes in the brackets are the ones which returns the best result on each
dataset. All kernel methods [13  14  15  16  17] use the RBF kernel.
In general  LIB-LLSVM + C-OCC
performs best.

USPS

7.82 (95)
5.98 (95)
4.14 (20)
4.38 (50)
3.94 (80)
4.09 (80)

9.57

-
-
-
-

5.78
4.38

-
-
-

4.24
4.38
4.25
5.78

LETTER
30.52 (15)
14.95 (16)
6.85 (15)
9.83 (14)
7.35 (16)
8.30 (16)

41.77

-
-
-
-

9.02
4.12

-
-
-

2.42
2.40
2.80
5.32

Methods

Linear SVM + G-OCC (# basis vectors)
Linear SVM + C-OCC (# anchor planes)
LIB-LLSVM + G-OCC (# basis vectors)
PEG-LLSVM + G-OCC (# basis vectors)
LIB-LLSVM + C-OCC (# basis vectors)
PEG-LLSVM + C-OCC (# basis vectors)

Linear SVM (10 passes) [1]

Linear SVM + LCC (512 anchor points) [18]
Linear SVM + LCC (4096 anchor points) [18]

Linear SVM + improved LCC (512 anchor points) [19]
Linear SVM + improved LCC (4096 anchor points) [19]

Linear SVM + LLC (512 anchor points) [16]
Linear SVM + LLC (4096 anchor points) [16]

LibSVM [4]

LA-SVM (1 pass) [3]
LA-SVM (2 passes) [3]

MCSVM [5]

SV Mstruct[15]

LA-RANK (1 pass) [2]

LL-SVM (100 anchor points  10 passes) [11]

MNIST
9.25 (100)
7.42 (100)
1.72 (50)
1.81 (40)
1.61 (90)
1.74 (90)

12.00
2.64
1.90
1.95
1.64
3.69
2.28
1.36
1.42
1.36
1.44
1.40
1.41
1.85

Table 3: Computational time comparison between our methods and others on MNIST  USPS  and LETTER.
The numbers in Row 7-14 are copied from [11]. The training times of our methods include the calculation of
OCC and training LL-SVM. All the numbers are corresponding to the methods shown in Table 2 with the same
parameters. Notice that for PEG-LLSVM  106 random data points is used for training.

Training Time (s)

Test Time (ms)

Methods

LIB-LLSVM + G-OCC
PEG-LLSVM + G-OCC
LIB-LLSVM + C-OCC
PEG-LLSVM + C-OCC

Linear SVM (10 passes) [1]

LibSVM [4]

LA-SVM (1 pass) [3]
LA-SVM (2 passes) [3]

MCSVM [5]

SV Mstruct[15]

LA-RANK (1 pass) [2]

LL-SVM (100  10 passes) [11]

MNIST
113.38
125.03
224.09
273.70

1.5

1.75×104
4.9×103
1.22×104
2.5×104
2.65×105
3×104
81.7

USPS
5.78
14.50
25.61
23.31
0.26

-
-
-
60

6.3×103

LETTER

4.14
2.02
1.66
0.85
0.18

-
-
-

1.2×103
2.4×104

85
6.2

940
4.2

MNIST
5.51×103
302.28
9.57×103
503.18

8.75×10−3

46
40.6
42.8

-
-
-

0.47

USPS
19.23
23.25
547.60
50.63

LETTER

4.09
3.33
63.13
28.94

-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-

the PASCAL2 Network of Excellence  IST-2007-216886. P. H. S. Torr is in receipt of Royal Society
Wolfson Research Merit Award.

8

References

[1] Bordes  A.  Bottou  L. & Gallinari  P. (2009) Sgd-qn: Careful quasi-newton stochastic gradient
descent. Journal of Machine Learning Research (JMLR).
[2] Bordes  A.  Bottou  L.  Gallinari  P.  & Weston  J. (2007) Solving multiclass support vector
machines with larank. In Proceeding of International Conference on Machine Learning (ICML).
[3] Bordes  A.  Ertekin  S.  Weston  J.  & Bottou  L. (2005) Fast kernel classiﬁers with online and
active learning. Journal of Machine Learning Research (JMLR).
[4] Chang  C. & Lin  C. (2011) LIBSVM: A Library for Support Vector Machines. ACM Transac-
tions on Intelligent Systems and Technology  vol. 2  issue 3  pp. 27:1-27:27.
[5] Crammer  K. & Singer  Y. (2002) On the algorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Research (JMLR).
[6] Cristianini  N.  Shawe-Taylor  J. & Lodhi  H. (2002) Latent Semantic Kernels. Journal of Intel-
ligent Information Systems  Vol. 18  No. 2-3  127-152.
[7] Everingham  M.  Van Gool  L.  Williams  C.K.I.  Winn  J. & Zisserman  A. The PASCAL
Visual Object Classes Challenge 2009 (VOC2009). http://www.pascal-network.org/
challenges/VOC/voc2009/workshop/index.html
[8] Fan  R.  Chang  K.  Hsieh  C.  Wang  X. & Lin  C. (2008) LIBLINEAR: A Library for Large
Linear Classiﬁcation. Journal of Machine Learning Research (JMLR)  vol. 9  pp. 1871-1874.
[9] Gönen  M. & Alpaydin  E. (2008) Localized Multiple Kernel Learning. In Proceeding of Inter-
national Conference on Machine Learning (ICML).
[10] Kecman  V. & Brooks  J.P. (2010) Locally Linear Support Vector Machines and Other Local
Models. In Proceeding of IEEE World Congress on Computational Intelligence (WCCI)  pp. 2615-
2620.
[11] Ladicky  L. & Torr  P.H.S. (2011) Locally Linear Support Vector Machines. In Proceeding of
International Conference on Machine Learning (ICML).
[12] Lee  H.  Battle  A.  Raina  R.  & Ng  A.Y. (2007) Efﬁcient Sparse Coding Algorithms.
Advances in Neural Information Processing Systems (NIPS).
[13] Mairal  J.  Bach  F.  Ponce  J. & Sapiro  G. (2009) Online Dictionary Learning for Sparse
Coding. In Proceeding of International Conference on Machine Learning (ICML).
[14] Shalev-Shwartz  S.  Singer  Y.  & Srebro  N. (2007) Pegasos: Primal Estimated sub-GrAdient
SOlver for SVM. In Proceeding of International Conference on Machine Learning (ICML).
[15] Tsochantaridis  I.  Joachims  T.  Hofmann  T.  & Altun  Y. (2005) Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research (JMLR).
[16] Wang  J.  Yang  J.  Yu  K.  Lv  F.  Huang  T.  & Gong  Y. (2010) Locality-constrained Linear
In Proceedings of IEEE Conference on Computer Vision and
Coding for Image Classiﬁcation.
Pattern Recognition (CVPR).
[17] Yu  K. & Ng  A. (2010) ECCV-2010 Tutorial: Feature Learning for Image Classiﬁcation.
http://ufldl.stanford.edu/eccv10-tutorial/.
[18] Yu  K.  Zhang  T.  & Gong  Y. (2009) Nonlinear Learning using Local Coordinate Coding. In
Advances in Neural Information Processing Systems (NIPS).
[19] Yu  K. & Zhang  T. (2010) Improved Local Coordinate Coding using Local Tangents. In Pro-
ceeding of International Conference on Machine Learning (ICML).
[20] Zhang  H.  Berg  A.  Maure  M. & Malik  J. (2006) SVM-KNN: Discriminative nearest neigh-
bor classiﬁcation for visual category recognition. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  pp. 2126-2136.

In

9

,Yizhe Zhang
Michel Galley
Jianfeng Gao
Zhe Gan
Xiujun Li
Chris Brockett
Bill Dolan