2010,Direct Loss Minimization for Structured Prediction,In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance  or loss. In binary classification one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction  structural SVMs and CRFs  do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptron-like learning rule  involving features vectors derived from loss-adjusted inference  directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus  which surpasses all previously reported results on this problem.,Direct Loss Minimization for Structured Prediction

David McAllester

TTI-Chicago

mcallester@ttic.edu

Tamir Hazan
TTI-Chicago

tamir@ttic.edu

Joseph Keshet
TTI-Chicago

jkeshet@ttic.edu

Abstract

In discriminative machine learning one is interested in training a system to opti-
mize a certain desired measure of performance  or loss. In binary classiﬁcation
one typically tries to minimizes the error rate. But in structured prediction each
task often has its own measure of performance such as the BLEU score in machine
translation or the intersection-over-union score in PASCAL segmentation. The
most common approaches to structured prediction  structural SVMs and CRFs  do
not minimize the task loss: the former minimizes a surrogate loss with no guar-
antees for task loss and the latter minimizes log loss independent of task loss.
The main contribution of this paper is a theorem stating that a certain perceptron-
like learning rule  involving features vectors derived from loss-adjusted inference 
directly corresponds to the gradient of task loss. We give empirical results on pho-
netic alignment of a standard test set from the TIMIT corpus  which surpasses all
previously reported results on this problem.

1

Introduction

Many modern software systems compute a result as the solution  or approximate solution  to an op-
timization problem. For example  modern machine translation systems convert an input word string
into an output word string in a different language by approximately optimizing a score deﬁned on the
input-output pair. Optimization underlies the leading approaches in a wide variety of computational
problems including problems in computational linguistics  computer vision  genome annotation  ad-
vertisement placement  and speech recognition. In many optimization-based software systems one
must design the objective function as well as the optimization algorithm. Here we consider a param-
eterized objective function and the problem of setting the parameters of the objective in such a way
that the resulting optimization-driven software system performs well.
We can formulate an abstract problem by letting X be an abstract set of possible inputs and Y an
abstract set of possible outputs. We assume an objective function sw : X × Y → R parameterized
by a vector w ∈ Rd such that for x ∈ X and y ∈ Y we have a score sw(x  y). The parameter setting
w determines a mapping from input x to output yw(x) is deﬁned as follows:
(1)

yw(x) = argmax

sw(x  y)

y∈Y

Our goal is to set the parameters w of the scoring function such that the mapping from input to
output deﬁned by (1) performs well. More formally  we assume that there exists some unknown
probability distribution ρ over pairs (x  y) where y is the desired output (or reference output) for
input x. We assume a loss function L  such as the BLEU score  which gives a cost L(y  ˆy) ≥ 0 for
producing output ˆy when the desired output (reference output) is y. We then want to set w so as to
minimize the expected loss.

w∗ = argmin

E [L(y  yw(x))]

(2)

w

In (2) the expectation is taken over a random draw of the pair (x  y) form the source data distribution
ρ. Throughout this paper all expectations will be over a random draw of a fresh pair (x  y). In
machine learning terminology we refer to (1) as inference and (2) as training.

1

Unfortunately the training objective function (2) is typically non-convex and we are not aware of any
polynomial algorithms (in time and sample complexity) with reasonable approximation guarantees
to (2) for typical loss functions  say 0-1 loss  and an arbitrary distribution ρ. In spite of the lack of
approximation guarantees  it is common to replace the objective in (2) with a convex relaxation such
as structural hinge loss [8  10]. It should be noted that replacing the objective in (2) with structural
hinge loss leads to inconsistency — the optimum of the relaxation is different from the optimum of
(2).
An alternative to a convex relaxation is to perform gradient descent directly on the objective in (2).
In some applications it seems possible that the local minima problem of non-convex optimization is
less serious than the inconsistencies introduced by a convex relaxation.
Unfortunately  direct gradient descent on (2) is conceptually puzzling in the case where the output
space Y is discrete. In this case the output yw(x) is not a differentiable function of w. As one
smoothly changes w the output yw(x) jumps discontinuously between discrete output values. So
one cannot write ∇wE [L(y  yw(x))] as E [∇wL(y  yw(x))]. However  when the input space X is
continuous the gradient ∇wE [L(y  yw(x))] can exist even when the output space Y is discrete. The
main results of this paper is a perceptron-like method of performing direct gradient descent on (2)
in the case where the output space is discrete but the input space is continuous.
After formulating our method we discovered that closely related methods have recently become
popular for training machine translation systems [7  2]. Although machine translation has discrete
inputs as well as discrete outputs  the training method we propose can still be used  although without
theoretical guarantees. We also present empirical results on the use of this method in phoneme
alignment on the TIMIT corpus  where it achieves the best known results on this problem.

2 Perceptron-Like Training Methods

Perceptron-like training methods are generally formulated for the case where the scoring function
is linear in w. In other words  we assume that the scoring function can be written as follows where
φ : X × Y → Rd is called a feature map.
Because the feature map φ can itself be nonlinear  and the feature vector φ(x  y) can be very high
dimensional  objective functions of the this form are highly expressive.
Here we will formulate perceptron-like training in the data-rich regime where we have access to
an unbounded sequence (x1  y1)  (x2  y2)  (x3  y3)  . . . where each (xt  yt) is drawn IID from the
distribution ρ. In the basic structured prediction perceptron algorithm [3] one constructs a sequence
of parameter settings w0  w1  w2  . . . where w0 = 0 and wt+1 is deﬁned as follows.

sw(x  y) = w(cid:62)φ(x  y)

wt+1 = wt + φ(xt  yt) − φ(xt  ywt(xt))

(3)
Note that if ywt(xt) = yt then no update is made and we have wt+1 = wt. If ywt(xt) (cid:54)= yt then the
update changes the parameter vector in a way that favors yt over ywt(xt). If the source distribution ρ
is γ-separable  i.e.  there exists a weight vector w with the property that yw(x) = y with probability 1
and yw(x) is always γ-separated from all distractors  then the perceptron update rule will eventually
lead to a parameter setting with zero loss. Note  however  that the basic perceptron update does not
involve the loss function L. Hence it cannot be expected to optimize the training objective (2) in
cases where zero loss is unachievable.
A loss-sensitive perceptron-like algorithm can be derived from the structured hinge loss of a margin-
scaled structural SVM [10]. The optimization problem for margin-scaled structured hinge loss can
be deﬁned as follows.

(cid:20)

(cid:0)L(y  ˜y) − w(cid:62) (φ(x  y) − φ(x  ˜y))(cid:1)(cid:21)

w∗ = argmin

E

w

max
˜y∈Y

yt
hinge = argmax

˜y∈Y

= argmax

˜y∈Y

It can be shown that this is a convex relaxation of (2). We can optimize this convex relaxation
with stochastic sub-gradient descent. To do this we compute a sub-gradient of the objective by ﬁrst
computing the value of ˜y which achieves the maximum.
L(yt  ˜y) − (wt)
(cid:62)

(φ(xt  yt) − φ(xt  ˜y))

(cid:62)

(wt)

φ(xt  ˜y) + L(yt  ˜y)

(4)

2

This yields the following perceptron-like update rule where the update direction is the negative of
the sub-gradient of the loss and ηt is a learning rate.

(5)
Equation (4) is often referred to as loss-adjusted inference. The use of loss-adjusted inference causes
the rule update (5) to be at least inﬂuenced by the loss function.
Here we consider the following perceptron-like update rule where ηt is a time-varying learning rate
and t is a time-varying loss-adjustment weight.

hinge)(cid:1)
wt+1 = wt + ηt(cid:0)φ(xt  yt) − φ(xt  yt
wt+1 = wt + ηt(cid:0)φ(xt  ywt(xt)) − φ(xt  yt

direct)(cid:1)

(6)

(7)

yt
direct = argmax

(wt)(cid:62)φ(xt  ˜y) + tL(y  ˜y)

˜y∈Y

In the update (6) we view yt
direct as being worse than ywt(xt). The update direction moves away
from feature vectors of larger-loss labels. Note that the reference label yt in (5) has been replaced
by the inferred label ywt(x) in (6). The main result of this paper is that under mild conditions the
expected update direction of (6) approaches the negative direction of ∇wE [L(y  yw(x))] in the limit
as the update weight t goes to zero. In practice we use a different version of the update rule which
moves toward better labels rather than away from worse labels. The toward-better version is given
in Section 5. Our main theorem applies equally to the toward-better and away-from-worse versions
of the rule.

3 The Loss Gradient Theorem

The main result of this paper is the following theorem.
Theorem 1. For a ﬁnite set Y of possible output values  and for w in general position as deﬁned
below  we have the following where ydirect is a function of w  x  y and .

where

∇wE [L(y  yw(x))] = lim
→0

1


E [φ(x  ydirect) − φ(x  yw(x)))]

ydirect = argmax

˜y∈Y

w(cid:62)φ(x  ˜y) + L(y  ˜y)

We prove this theorem in the case of only two labels where we have y ∈ {−1  1}. Although the
proof is extended to the general case in a straight forward manner  we omit the general case to
maintain the clarity of the presentation. We assume an input set X and a probability distribution or
a measure ρ on X × {−1  1} and a loss function L(y  y(cid:48)) for y  y(cid:48) ∈ {−1  1}. Typically the loss
L(y  y(cid:48)) is zero if y = y(cid:48) but the loss of a false positive  namely L(−1  1)  may be different from the
loss of a false negative  L(1 −1).
By deﬁnition the gradient of expected loss satisﬁes the following condition for any vector ∆w ∈ Rd.

∆w(cid:62)∇wE [L(y  yw(x))] = lim
→0

E [L(y  yw+∆w(x))] − E [L(y  yw(x))]



Using this observation  the direct loss theorem is equivalent to the following

E [L(y  yw+∆w(x)) − L(y  yw(x))]



lim
→0

= lim
→0

(∆w)(cid:62)E [φ(x  ydirect) − φ(x  yw(x))]



(8)

For the binary case we deﬁne ∆φ(x) = φ(x  1)−φ(x −1). Under this convention we have yw(x) =
sign(w(cid:62)∆φ(x)). We ﬁrst focus on the left hand side of (8). If the two labels yw+∆w(x) and yw(x)
are the same then the quantity inside the expectation is zero. We now deﬁne the following two sets
which correspond to the set of inputs x for which these two labels are different.

S+
 = {x : yw(x) = −1  yw+∆w(x) = 1}

= (cid:8)x : w(cid:62)∆φ(x) < 0  (w + ∆w)(cid:62)∆φ(x) ≥ 0(cid:9)
= (cid:8)x : w(cid:62)∆φ(x) ∈ [−(∆w)(cid:62)∆φ(x)  0)(cid:9)

3

E(cid:2)∆L(y) · 1{wt∆φ(x)∈[−(∆w)(cid:62)∆φ(x) 0)}(cid:3)
−E(cid:2)∆L(y) · 1{wt∆φ(x)∈(0 −(∆w)(cid:62)∆φ(x)]}(cid:3)

(a)

E(cid:2)∆L(y) · (∆w)T ∆φ(x) · 1{wt∆φ(x)∈[0 ]}(cid:3)

(b)

Figure 1: Geometrical interpretation of the loss gradient. In (a) we illustrate the integration of the
(the green
 and the constant value −∆L(y) over the set S−
constant value ∆L(y) over the set S+
In (b) we
area). The lines represent the decision boundaries deﬁned by the associated vectors.
 = {x : wt∆φ(x) ∈ [0  ]} and
show the integration of ∆L(y)(∆w)(cid:62)∆φ(x) over the sets U +
U− = {x : wt∆φ(x) ∈ [−(∆w)(cid:62)∆φ(x)  0)}. The key observation of the proof is that under very
general conditions these integrals are asymptotically equivalent in the limit as  goes to zero.

and

= (cid:8)x : w(cid:62)∆φ(x) ≥ 0  (w + ∆w)(cid:62)∆φ(x) < 0(cid:9)
S− = {x : yw(x) = 1  yw+∆w(x) = −1}
= (cid:8)x : w(cid:62)∆φ(x) ∈ [0 −(∆w)(cid:62)∆φ(x))(cid:9)
− E(cid:104)
(cid:105)

(cid:105)
We deﬁne ∆L(y) = L(y  1) − L(y −1) and then write the left hand side of (8) as follows.
−
 }

E [L(y  yw+∆w(x)) − L(y  yw(x))] = E(cid:104)

(9)
These expectations are shown as integrals in Figure 1 (a) where the lines in the ﬁgure represent the
decision boundaries deﬁned by w and w + ∆w.
To analyze this further we use the following lemma.
Lemma 1. Let Z(z)  U (u) and V (v) be three real-valued random variables whose joint measure
ρ(S) =(cid:82)
ρ can be expressed as a measure µ on U and V and a bounded continuous conditional density
function f (z|u  v). More rigorously  we require that for any ρ-measurable set S ⊆ R3 we have
following.

{z u v∈S}dz(cid:3) dµ(u  v). For any three such random variables we have the
(cid:3)

(cid:2)(cid:82)
(cid:0)Eρ

(cid:2)U · 1

(cid:2)U · 1

z f (z|u  v)1

− Eρ

{z∈[0 V ]}

{z∈[V 0]}

∆L(y)1

∆L(y)1

lim
→+0

{x∈S+
 }

{x∈S

(cid:3)

1


u v

Proof. First we note the following where V + denotes max(0  V ).

(cid:3)(cid:1) = Eµ [U V · f (0|u  v)]
(cid:2)U V · 1
(cid:34)
(cid:35)

= lim
→+0

Eρ

1


f (z|u  v)dz

{z∈[0 ]}

0

U

1


Eµ

→+0

= Eµ

(cid:3) = lim
(cid:3) = lim
→+0
= −Eµ

(cid:90) V
(cid:2)U V + · f (0|U  V )(cid:3)
(cid:20)
(cid:90) 0
(cid:2)U V − · f (0|U  V )(cid:3)

Eµ

1


U

V

4

(cid:21)

f (z|u  v)dz

(cid:2)U · 1
(cid:2)U · 1

Eρ

1


lim
→+0

{z∈[0 V )}

1


lim
→+0

Eρ

{z∈(V 0]}

Similarly we have the following where V − denotes min(0  V ).

w+∆ww−∆L(y)S−￿={0<w￿∆φ(x)<−￿∆w￿∆φ(x)}∆L(y)S+￿∆φ1(x)∆φ2(x)w+∆ww￿−slice∆L(y)·∆w￿∆φ(x)∆L(y)·∆w￿∆φ(x)Subtracting these two expressions gives the following.

(cid:2)U V + · f (0|U  V )(cid:3) + Eµ

(cid:2)U V − · f (0|U  V )(cid:3) = Eµ

Eµ

(cid:2)U (V + + V −) · f (0|U  V )(cid:3)

= Eµ [U V · f (0|U  V )]

Applying Lemma 1 to (9) with Z being the random variable wT ∆φ(x)  U being the random variable
−∆L(y) and V being −(∆w)T ∆φ(x) yields the following.
∆L(y) · 1

(cid:105)

(cid:105)

−
 }

E(cid:104)
− E(cid:104)
E(cid:2)∆L(y) · (∆w)(cid:62)∆φ(x) · 1

{x∈S+
 }

1

1


(∆w)(cid:62)∇wE [L(y  yw(x))] = lim
→+0
= lim
→+0

∆L(y) · 1
{x∈S
{w(cid:62)∆φ∈[0 ]}

(cid:3)

(10)

Of course we need to check that the conditions of Lemma 1 hold. This is where we need a general
position assumption for w. We discuss this issue brieﬂy in Section 3.1.
Next we consider the right hand side of (8). If the two labels ydirect and yw(x) are the same then the
quantity inside the expectation is zero. We note that we can write ydirect as follows.

ydirect = sign(cid:0)w(cid:62)∆φ(x) + ∆L(y)(cid:1)

We now deﬁne the following two sets which correspond to the set of pairs (x  y) for which yw(x)
and ydirect are different.

B+
 = {(x  y) : yw(x) = −1  ydirect = 1}

B− = {(x  y) : yw(x) = 1  ydirect = −1}

= (cid:8)(x  y) : w(cid:62)∆φ(x) < 0  w(cid:62)∆φ(x) + ∆L(y) ≥ 0(cid:9)
= (cid:8)(x  y) : w(cid:62)∆φ(x) ∈ [−∆L(y)(x)  0)(cid:9)
= (cid:8)(x  y) : w(cid:62)∆φ(x) ≥ 0  w(cid:62)∆φ(x) + ∆L(y) < 0(cid:9)
= (cid:8)(x  y) : w(cid:62)∆φ(x) ∈ [0 −∆L(y))(cid:9)
E(cid:2)(∆w)(cid:62) (φ(x  ydirect) − φ(x  yw(x)))(cid:3)
= E(cid:104)
− E(cid:104)

(∆w)(cid:62)∆φ(x) · 1

(∆w)(cid:62)∆φ(x) · 1

{(x y)∈B+
 }

(cid:105)

(cid:105)

(11)

{(x y)∈B

−
 }

We now have the following.

These expectations are shown as integrals in Figure 1 (b). Applying Lemma 1 to (11) with Z set to
w(cid:62)∆φ(x)  U set to −(∆w)(cid:62)∆φ(x) and V set to −∆L(y) gives the following.
(cid:3)

(∆w)(cid:62)E [φ(x  ydirect) − φ(x  yw(x))]

E(cid:2)(∆w)(cid:62)∆φ(x) · ∆L(y) · 1

{w(cid:62)∆φ(x)∈[0 ]}

1

1


(12)

lim
→+0
= lim
→+0

Theorem 1 now follows from (10) and (12).

3.1 The General Position Assumption

The general position assumption is needed to ensure that Lemma 1 can be applied in the proof of
Theorem 1. As a general position assumption  it is sufﬁcient  but not necessary  that w (cid:54)= 0 and
φ(x  y) has a bounded density on Rd for each ﬁxed value of y. It is also sufﬁcient that the range of
the feature map is a submanifold of Rd and φ(x  y) has a bounded density relative to the surface of
that submanifold  for each ﬁxed value of y. More complex distributions and feature maps are also
possible.

5

4 Extensions: Approximate Inference and Latent Structure

In many applications the inference problem (1) is intractable. Most commonly we have some form
of graphical model. In this case the score w(cid:62)φ(x  y) is deﬁned as the negative energy of a Markov
random ﬁeld (MRF) where x and y are assignments of values to nodes of the ﬁeld. Finding a lowest
energy value for y in (1) in a general graphical model is NP-hard.
A common approach to an intractable optimization problem is to deﬁne a convex relaxation of the
objective function. In the case of graphical models this can be done by deﬁning a relaxation of a
marginal polytope [11]. The details of the relaxation are not important here. At a very abstract
level the resulting approximate inference problem can be deﬁned as follows where the set R is a
relaxation of the set Y  and corresponds to the extreme points of the relaxed polytope.

rw(x) = argmax

r∈R

w(cid:62)φ(x  r)

(13)

We assume that for y ∈ Y and r ∈ R we can assign a loss L(y  r). In the case of a relaxation of
the marginal polytope of a graphical model we can take L(y  r) to be the expectation over a random
rounding of r to ˜y of L(y  ˜y). For many loss functions  such as weighted Hamming loss  one can
compute L(y  r) efﬁciently. The training problem is then deﬁned by the following equation.

w∗ = argmin

w

E [L(y  rw(x))]

(14)

Note that (14) directly optimizes the performance of the approximate inference algorithm. The pa-
rameter setting optimizing approximate inference might be signiﬁcantly different from the parameter
setting optimizing the loss under exact inference.
The proof of Theorem 1 generalizes to (14) provided that R is a ﬁnite set  such as the set of vertices
of a relaxation of the marginal polytope. So we immediately get the following generalization of
Theorem 1.

where

∇wE

(x y)∼ρ [L(y  rw(x))] = lim
→0

1


E [φ(x  rdirect) − φ(x  rw(x))]

rdirect = argmax

˜r∈R

w(cid:62)φ(x  ˜r) + L(y  ˜r)

Another possible extension involves hidden structure. In many applications it is useful to introduce
hidden information into the inference optimization problem. For example  in machine translation
we might want to construct parse trees for the both the input and output sentence. In this case the
inference equation can be written as follows where h is the hidden information.

yw(x) = argmax

y∈Y

max
h∈H

w(cid:62)φ(x  y  h)

(15)

In this case we can take the training problem to again be deﬁned by (2) but where yw(x) is deﬁned
by (15).
Latent information can be handled by the equations of approximate inference but where R is reinter-
preted as the set of pairs (y  h) with y ∈ Y and h ∈ H. In this case L(y  r) has the form L(y  (y(cid:48)  h))
which we can take to be equal to L(y  y(cid:48)).

5 Experiments

In this section we present empirical results on the task of phoneme-to-speech alignment. Phoneme-
to-speech alignment is used as a tool in developing speech recognition and text-to-speech systems.
In the phoneme alignment problem each input x represents a speech utterance  and consists of a pair
(s  p) of a sequence of acoustic feature vectors  s = (s1  . . .   sT )  where st ∈ Rd  1 ≤ t ≤ T ; and a
sequence of phonemes p = (p1  . . .   pK)  where pk ∈ P  1 ≤ k ≤ K is a phoneme symbol and P is
a ﬁnite set of phoneme symbols. The lengths K and T can be different for different inputs although
typically we have T signiﬁcantly larger than K. The goal is to generate an alignment between the
two sequences in the input. Sometimes this task is called forced-alignment because one is forced

6

Table 1: Percentage of correctly positioned phoneme boundaries  given a predeﬁned tolerance on
the TIMIT corpus. Results are reported on the whole TIMIT test-set (1344 utterances).

t ≤ 10ms

τ-alignment accuracy [%]
t ≤ 20ms
t ≤ 30ms

t ≤ 40ms

τ-insensitive

loss

Brugnara et al. (1993)
Keshet (2007)
Hosom (2009)
Direct loss min. (trained τ-alignment)
Direct loss min. (trained τ-insensitive)

74.6
80.0
79.30
86.01
85.72

88.8
92.3
93.36
94.08
94.21

94.1
96.4
96.74
97.08
97.21

96.8
98.2
98.22
98.44
98.60

-
-

0.278
0.277

to interpret the given acoustic signal as the given phoneme sequence. The output y is a sequence
(y1  . . .   yK)  where 1 ≤ yk ≤ T is an integer giving the start frame in the acoustic sequence of
the k-th phoneme in the phoneme sequence. Hence the k-th phoneme starts at frame yk and ends at
frame yk+1−1.
Two types of loss functions are used to quantitatively assess alignments. The ﬁrst loss is called the
τ-alignment loss and it is deﬁned as

Lτ-alignment(¯y  ¯y(cid:48)) =

1

|¯y| |{k : |yk − y(cid:48)k| > τ}| .

(16)

In words  this loss measures the average number of times the absolute difference between the pre-
dicted alignment sequence and the manual alignment sequence is greater than τ. This loss with
different values of τ was used to measure the performance of the learned alignment function in
[1  9  4]. The second loss  called τ-insensitive loss was proposed in [5] as is deﬁned as follows.

Lτ-insensitive(¯y  ¯y(cid:48)) =

max{|yk − y(cid:48)k| − τ  0}

(17)

1
|¯y|

This loss measures the average disagreement between all the boundaries of the desired alignment
sequence and the boundaries of predicted alignment sequence where a disagreement of less than τ
is ignored. Note that τ-insensitive loss is continuous and convex while τ-alignment is discontinuous
and non-convex. Rather than use the “away-from-worse” update given by (6) we use the “toward-
better” update deﬁned as follows. Both updates give the gradient direction in the limit of small  but
the toward-better version seems to perform better for ﬁnite .

wt+1 = wt + ηt(cid:0)φ(¯xt  ¯yt

direct) − φ(¯xt  ¯ywt(¯xt))(cid:1)

¯yt
direct = argmax

˜y∈Y

(wt)(cid:62)φ(¯xt  ˜y) − tL(¯y  ˜y)

Our experiments are on the TIMIT speech corpus for which there are published benchmark results
[1  5  4]. The corpus contains aligned utterances each of which is a pair (x  y) where x is a pair of
a phonetic sequence and an acoustic sequence and y is a desired alignment. We divided the training
portion of TIMIT (excluding the SA1 and SA2 utterances) into three disjoint parts containing 1500 
1796  and 100 utterances  respectively. The ﬁrst part of the training set was used to train a phoneme
frame-based classiﬁer  which given a speech frame and a phoneme  outputs the conﬁdent that the
phoneme was uttered in that frame. The phoneme frame-based classiﬁer is then used as part of a
seven dimensional feature map φ(x  y) = φ((¯s  ¯p)  ¯y) as described in [5]. The feature set used to
train the phoneme classiﬁer consisted of the Mel-Frequency Cepstral Coefﬁcient (MFCC) and the
log-energy along with their ﬁrst and second derivatives (∆+∆∆) as described in [5]. The classiﬁer
used a Gaussian kernel with σ2 = 19 and a trade-off parameter C = 5.0. The complete set of 61
TIMIT phoneme symbols were mapped into 39 phoneme symbols as proposed by [6]  and was used
throughout the training process.
The seven dimensional weight vector w was trained on the second set of 1796 aligned utterances.
We trained twice  once for τ-alignment loss and once for τ-insensitive loss  with τ = 10 ms in both
cases. Training was done by ﬁrst setting w0 = 0 and then repeatedly selecting one of the 1796
training pairs at random and performing the update (6) with ηt = 1 and t set to a ﬁxed value . It
should be noted that if w0 = 0 and t and ηt are both held constant at  and η respectively  then the

7

direction of wt is independent of the choice of η. These updates are repeated until the performance
of wt on the third data set (the hold-out set) begins to degrade. This gives a form of regularization
known as early stopping. This was repeated for various values of  and a value of  was selected
based on the resulting performance on the 100 hold-out pairs. We selected  = 1.1 for both loss
functions.
We scored the performance of our system on the whole TIMIT test set of 1344 utterances using
τ-alignment accuracy (one minus the loss) with τ set to each of 10  20  30 and 40 ms and with τ-
insensitive loss with τ set to 10 ms. As should be expected  for τ equal to 10 ms the best performance
is achieved when the loss used in training matches the loss used in test. Larger values of τ correspond
to a loss function that was not used in training. The results are given in Table 1. We compared our
results with [4]  which is an HMM/ANN-based system  and with [5]  which is based on structural
SVM training for τ-insensitive loss. Both systems are considered to be state-of-the-art results on
this corpus. As can be seen  our algorithm outperforms the current state-of-the-art results in every
tolerance value. Also  as might be expected  the τ-insensitive loss seems more robust to the use of a
τ value at test time that is larger than the τ value used in training.

6 Open Problems and Discussion

provided that both ηt and t go to zero while(cid:80)

The main result of this paper is the loss gradient theorem of Section 3. This theorem provides a
theoretical foundation for perceptron-like training methods with updates computed as a difference
between the feature vectors of two different inferred outputs where at least one of those outputs
is inferred with loss-adjusted inference. Perceptron-like training methods using feature differences
between two inferred outputs have already been shown to be successful for machine translation but
theoretical justiﬁcation has been lacking. We also show the value of these training methods in a
phonetic alignment problem.
Although we did not give an asymptotic convergence results it should be straightforward to show
that under the update given by (6) we have that wt converges to a local optimum of the objective
t ηtt goes to inﬁnity. For example one could take
ηt = t = 1/√t.
An open problem is how to properly incorporate regularization in the case where only a ﬁnite cor-
pus of training data is available. In our phoneme alignment experiments we trained only a seven
dimensional weight vector and early stopping was used as regularization. It should be noted that
naive regularization with a norm of w  such as regularizing with λ||w||2  is nonsensical as the loss
E [L(y  yw(x))] is insensitive to the norm of w. Regularization is typically done with a surrogate
loss function such as hinge loss. Regularization remains an open theoretical issue for direct gradi-
ent descent on a desired loss function on a ﬁnite training sample. Early stopping may be a viable
approach in practice.
Many practical computational problems in areas such as computational linguistics  computer vision 
speech recognition  robotics  genomics  and marketing seem best handled by some form of score op-
timization. In all such applications we have two optimization problems. Inference is an optimization
problem (approximately) solved during the operation of the ﬁelded software system. Training in-
volves optimizing the parameters of the scoring function to achieve good performance of the ﬁelded
system. We have provided a theoretical foundation for a certain perceptron-like training algorithm
by showing that it can be viewed as direct stochastic gradient descent on the loss of the inference
system. The main point of this training method is to incorporate domain-speciﬁc loss functions  such
as the BLEU score in machine translation  directly into the training process with a clear theoretical
foundation. Hopefully the theoretical framework provided here will prove helpful in the continued
development of improved training methods.

References

[1] F. Brugnara  D. Falavigna  and M. Omologo. Automatic segmentation and labeling of speech

based on hidden markov models. Speech Communication  12:357–370  1993.

[2] D. Chiang  K. Knight  and W. Wang. 11 001 new features for statistical machine translation.

In Proc. NAACL  2009  2009.

8

[3] M. Collins. Discriminative training methods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Conference on Empirical Methods in Natural Language
Processing  2002.

[4] J.-P. Hosom. Speaker-independent phoneme alignment using transition-dependent states.

Speech Communication  51:352–368  2009.

[5] J. Keshet  S. Shalev-Shwartz  Y. Singer  and D. Chazan. A large margin algorithm for speech
and audio segmentation. IEEE Trans. on Audio  Speech and Language Processing  Nov. 2007.
[6] K.-F. Lee and H.-W. Hon. Speaker independent phone recognition using hidden markov mod-

els. IEEE Trans. Acoustic  Speech and Signal Proc.  37(2):1641–1648  1989.

[7] P. Liang  A. Bouchard-Ct  D. Klein  and B. Taskar. An end-to-end discriminative approach to
machine translation. In International Conference on Computational Linguistics and Associa-
tion for Computational Linguistics (COLING/ACL)  2006.

[8] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In Advances in Neural

Information Processing Systems 17  2003.

[9] D.T. Toledano  L.A.H. Gomez  and L.V. Grande. Automatic phoneme segmentation. IEEE

Trans. Speech and Audio Proc.  11(6):617–625  2003.

[10] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured
and interdependent output variables. Journal of Machine Learning Research  6:1453–1484 
2005.

[11] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends in Machine Learning  1(1-2):1–305  December 2008.

9

,Shahin Jabbari
Ryan Rogers
Aaron Roth
Steven Wu