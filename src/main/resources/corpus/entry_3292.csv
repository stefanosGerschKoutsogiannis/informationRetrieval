2013,Low-rank matrix reconstruction and clustering via approximate message passing,We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework  which allows us to exploit structural properties of matrices in addition to low-rankedness  such as sparsity. We propose an efficient approximate message passing algorithm  derived from the belief propagation algorithm  to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem  by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.,Low-rank matrix reconstruction and clustering via

approximate message passing

Ryosuke Matsushita

NTT DATA Mathematical Systems Inc.

1F Shinanomachi Rengakan  35 

Shinanomachi  Shinjuku-ku  Tokyo 

160-0016  Japan

Toshiyuki Tanaka

Department of Systems Science 

Graduate School of Informatics  Kyoto University

Yoshida Hon-machi  Sakyo-ku  Kyoto-shi 

606-8501 Japan

matsur8@gmail.com

tt@i.kyoto-u.ac.jp

Abstract

We study the problem of reconstructing low-rank matrices from their noisy ob-
servations. We formulate the problem in the Bayesian framework  which allows
us to exploit structural properties of matrices in addition to low-rankedness  such
as sparsity. We propose an efﬁcient approximate message passing algorithm  de-
rived from the belief propagation algorithm  to perform the Bayesian inference for
matrix reconstruction. We have also successfully applied the proposed algorithm
to a clustering problem  by reformulating it as a low-rank matrix reconstruction
problem with an additional structural property. Numerical experiments show that
the proposed algorithm outperforms Lloyd’s K-means algorithm.

1 Introduction

Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its
noisy observations. In such problems  there are often demands to incorporate additional structural
properties of matrices in addition to the low-rankedness. In this paper  we consider the case where
0   U0 2 Rm(cid:2)r  V0 2 RN(cid:2)r
a matrix A0 2 Rm(cid:2)N to be reconstructed is factored as A0 = U0V
⊤
(r ≪ m; N)  and where one knows structural properties of the factors U0 and V0 a priori. Sparseness
and non-negativity of the factors are popular examples of such structural properties [1  2].
Since the properties of the factors to be exploited vary according to the problem  it is desirable
that a reconstruction method has enough ﬂexibility to incorporate a wide variety of properties. The
Bayesian approach achieves such ﬂexibility by allowing us to select prior distributions of U0 and V0
reﬂecting a priori knowledge on the structural properties. The Bayesian approach  however  often
involves computationally expensive processes such as high-dimensional integrations  thereby requir-
ing approximate inference methods in practical implementations. Monte Carlo sampling methods
and variational Bayes methods have been proposed for low-rank matrix reconstruction to meet this
requirement [3–5].
We present in this paper an approximate message passing (AMP) based algorithm for Bayesian low-
rank matrix reconstruction. Developed in the context of compressed sensing  the AMP algorithm re-
constructs sparse vectors from their linear measurements with low computational cost  and achieves
a certain theoretical limit [6]. AMP algorithms can also be used for approximating Bayesian in-
ference with a large class of prior distributions of signal vectors and noise distributions [7]. These
successes of AMP algorithms motivate the use of the same idea for low-rank matrix reconstruction.
The IterFac algorithm for the rank-one case [8] has been derived as an AMP algorithm. An AMP
algorithm for the general-rank case is proposed in [9]  which  however  can only treat estimation of
posterior means. We extend their algorithm so that one can deal with other estimations such as the
maximum a posteriori (MAP) estimation. It is the ﬁrst contribution of this paper.

1

As the second contribution  we apply the derived AMP algorithm to K-means type clustering to
obtain a novel efﬁcient clustering algorithm. It is based on the observation that our formulation
of the low-rank matrix reconstruction problem includes the clustering problem as a special case.
Although the idea of applying low-rank matrix reconstruction to clustering is not new [10  11]  our
proposed algorithm is  to our knowledge  the ﬁrst that directly deals with the constraint that each
datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction.
We present results of numerical experiments  which show that the proposed algorithm outperforms
Lloyd’s K-means algorithm [12] when data are high-dimensional.
Recently  AMP algorithms for dictionary learning and blind calibration [13] and for matrix recon-
struction with a generalized observation model [14] were proposed. Although our work has some
similarities to these studies  it differs in that we ﬁx the rank r rather than the ratio r=m when taking
the limit m; N ! 1 in the derivation of the algorithm. Another difference is that our formulation 
explained in the next section  does not assume statistical independence among the components of
each row of U0 and V0. A detailed comparison among these algorithms remains to be made.

2 Problem setting

⊤ 2 Rm(cid:2)r and V0 := (v0;1; : : : ; v0;N )

2.1 Low-rank matrix reconstruction
We consider the following problem setting. A matrix A0 2 Rm(cid:2)N to be estimated is deﬁned
⊤ 2 RN(cid:2)r as
by two matrices U0 := (u0;1; : : : ; u0;m)
0   where u0;i; v0;j 2 Rr. We consider the case where r ≪ m; N. Observations of A0
⊤
A0 := U0V
are corrupted by additive noise W 2 Rm(cid:2)N   whose components Wi;j are i.i.d. Gaussian random
variables following N (0; m(cid:28) ). Here (cid:28) > 0 is a noise variance parameter and N (a; (cid:27)2) denotes the
Gaussian distribution with mean a and variance (cid:27)2. The factor m in the noise variance is introduced
to allow a proper scaling in the limit where m and N go to inﬁnity in the same order  which is
employed in deriving the algorithm. An observed matrix A 2 Rm(cid:2)N is given by A := A0 + W .
Reconstructing A0 and (U0; V0) from A is the problem considered in this paper.
We take the Bayesian approach to address this problem  in which one requires prior distributions
of variables to be estimated  as well as conditional distributions relating observations with variables
to be estimated. These distributions need not be the true ones because in some cases they are not
available so that one has to assume them arbitrarily  and in some other cases one expects advantages
by assuming them in some speciﬁc manner in view of computational efﬁciencies. In this paper  we
suppose that one uses the true conditional distribution

p(AjU0; V0) =

1

exp

(2(cid:25)m(cid:28) ) mN

(1)
where ∥ (cid:1) ∥F denotes the Frobenius norm. Meanwhile  we suppose that the assumed prior distribu-
tions of U0 and V0  denoted by ^pU and ^pV  respectively  may be different from the true distributions
pU and pV  respectively. We restrict ^pU and ^pV to distributions of the form ^pU(U0) =
i ^pu(u0;i)
j ^pv(v0;j)  respectively  which allows us to construct computationally efﬁcient
and ^pV(V0) =
algorithms. When U (cid:24) ^pU(U ) and V (cid:24) ^pV(V )  the posterior distribution of (U; V ) given A is

∏

∏

F

;

2

(cid:0) 1
2m(cid:28)

∥A (cid:0) U0V

⊤
0

∥2

(

)

)

(
^p(U; V jA) / exp

(cid:0) 1
2m(cid:28)

∥A (cid:0) U V

⊤∥2

F

^pU(U )^pV(V ):

(2)

Prior probability density functions (p.d.f.s) ^pu and ^pv can be improper  that is  they can integrate to
inﬁnity  as long as the posterior p.d.f. (2) is proper. We also consider cases where the assumed rank
^r may be different from the true rank r. We thus suppose that estimates U and V are of size m (cid:2) ^r
and N (cid:2) ^r  respectively.
We consider two problems appearing in the Bayesian approach. The ﬁrst problem  which we call
the marginalization problem  is to calculate the marginal posterior distributions given A 

∫

∏

∏

^pi;j(ui; vjjA) :=

^p(U; V jA)

∫

These are used to calculate the posterior mean E[U V
^pi;j(u; vjA)dv and vMMAP
uMMAP

:= arg maxu

i

l̸=j

duk

dvl:

(3)
k̸=i
⊤jA] and the marginal MAP estimates
^pi;j(u; vjA)du. Because
:= arg maxv

∫

j

2

calculation of ^pi;j(ui; vjjA) typically involves high-dimensional integrations requiring high com-
putational cost  approximation methods are needed.
The second problem  which we call
arg maxU;V ^p(U; V jA). It is formulated as the following optimization problem:

is to calculate the MAP estimate

the MAP problem 

where CMAP(U; V ) is the negative logarithm of (2):

CMAP(U; V ) :=

∥A (cid:0) U V

⊤∥2

F

1

2m(cid:28)

CMAP(U; V );

min
U;V

(cid:0) m∑

log ^pu(ui) (cid:0) N∑

(4)

(5)

log ^pv(vj):

Because ∥A (cid:0) U V
optimal solutions of (4) and therefore approximation methods are needed in this problem as well.

F is a non-convex function of (U; V )  it is generally hard to ﬁnd the global

⊤∥2

i=1

j=1

2.2 Clustering as low-rank matrix reconstruction

A clustering problem can be formulated as a problem of low-rank matrix reconstruction [11]. Sup-
pose that v0;j 2 fe1; : : : ; erg; j = 1; : : : ; N  where el 2 f0; 1gr is the vector whose lth component
is 1 and the others are 0. When V0 and U0 are ﬁxed  aj follows one of the r Gaussian distributions
N ( ~u0;l; m(cid:28) I); l = 1; : : : ; r  where ~u0;l is the lth column of U0. We regard that each Gaussian
distribution deﬁnes a cluster  ~u0;l being the center of cluster l and v0;j representing the cluster
assignment of the datum aj. One can then perform clustering on the dataset fa1; : : : ; aNg by re-
constructing U0 and V0 from A = (a1; : : : ; aN ) under the structural constraint that every row of V0
should belong to fe1; : : : ; e^rg  where ^r is an assumed number of clusters.
∑
Let us consider maximum likelihood estimation arg maxU;V p(AjU; V )  or equivalently  MAP esti-
l=1 (cid:14)(v(cid:0)el).
mation with the (improper) uniform prior distributions ^pu(u) = 1 and ^pv(v) = ^r
The corresponding MAP problem is
∑
∑
subject to vj 2 fe1; : : : ; e^rg:

(6)
∥aj (cid:0)
When V satisﬁes the constraints  the objective function ∥A (cid:0) U V
~ul∥2
2I(vj = el) is the sum of squared distances  each of which is between a datum and the center of
the cluster that the datum is assigned to. The optimization problem (6)  its objective function  and
clustering based on it are called in this paper the K-means problem  the K-means loss function  and
the K-means clustering  respectively.
One can also use the marginal MAP estimation for clustering. If U0 and V0 follow ^pU and ^pV  re-
spectively  the marginal MAP estimation is optimal in the sense that it maximizes the expectation of
accuracy with respect to ^p(V0jA). Here  accuracy is deﬁned as the fraction of correctly assigned data
among all data. We call the clustering using approximate marginal MAP estimation the maximum
accuracy clustering  even when incorrect prior distributions are used.

U2Rm(cid:2)^r;V 2f0;1gN(cid:2)^r

∥A (cid:0) U V

⊤∥2

⊤∥2

F

F =

N
j=1

^r
l=1

(cid:0)1

^r

min

3 Previous work

U (U ) and pVB
U (U )pVB

Existing methods for approximately solving the marginalization problem and the MAP problem
are divided into stochastic methods such as Markov-Chain Monte-Carlo methods and deterministic
ones. A popular deterministic method is to use the variational Bayesian formalism. The variational
Bayes matrix factorization [4  5] approximates the posterior distribution p(U; V jA) as the product
of two functions pVB
V (V )  which are determined so that the Kullback-Leibler (KL)
V (V ) to p(U; V jA) is minimized. Global minimization of the KL di-
divergence from pVB
vergence is difﬁcult except for some special cases [15]  so that an iterative method to obtain a local
minimum is usually adopted. Applying the variational Bayes matrix factorization to the MAP prob-
lem  one obtains the iterated conditional modes (ICM) algorithm  which alternates minimization of
CMAP(U; V ) over U for ﬁxed V and minimization over V for ﬁxed U.
The representative algorithm to solve the K-means problem approximately is Lloyd’s K-means algo-
rithm [12]. Lloyd’s K-means algorithm is regarded as the ICM algorithm: It alternates minimization
of the K-means loss function over U for ﬁxed V and minimization over V for ﬁxed U iteratively.

3

Algorithm 1 (Lloyd’s K-means algorithm).

N∑

nt

l =

I(vt

j = el);

j=1

lt+1
j = arg min

l2f1;:::;^rg

~ut
l =
∥2
∥aj (cid:0) ~ut
2;

l

N∑

ajI(vt

j = el);

j=1
vt+1
j = elt+1

j

:

1
nt
l

(7a)

(7b)

Throughout this paper  we represent an algorithm by a set of equations as in the above. This repre-
sentation means that the algorithm begins with a set of initial values and repeats the update of the
variables using the equations presented until it satisﬁes some stopping criteria. Lloyd’s K-means
algorithm begins with a set of initial assignments V 0 2 fe1; : : : ; e^rgN . This algorithm easily gets
stuck in local minima and its performance heavily depends on the initial values of the algorithm.
Some methods for initialization to obtain a better local minimum are proposed [16].
Maximum accuracy clustering can be solved approximately by using the variational Bayes matrix
factorization  since it gives an approximation to the marginal posterior distribution of vj given A.

4 Proposed algorithm

4.1 Approximate message passing algorithm for low-rank matrix reconstruction

We ﬁrst discuss the general idea of the AMP algorithm and advantages of the AMP algorithm com-
pared with the variational Bayes matrix factorization. The AMP algorithm is derived by approximat-
ing the belief propagation message passing algorithm in a way thought to be asymptotically exact for
large-scale problems with appropriate randomness. Fixed points of the belief propagation message
passing algorithm correspond to local minima of the KL divergence between a kind of trial function
and the posterior distribution [17]. Therefore  the belief propagation message passing algorithm can
be regarded as an iterative algorithm based on an approximation of the posterior distribution  which
is called the Bethe approximation. The Bethe approximation can reﬂect dependence of random vari-
ables (dependence between U and V in ^p(U; V jA) in our problem) to some extent. Therefore  one
can intuitively expect that performance of the AMP algorithm is better than that of the variational
Bayes matrix factorization  which treats U and V as if they were independent in ^p(U; V jA).
An important property of the AMP algorithm  aside from its efﬁciency and effectiveness  is that
one can predict performance of the algorithm accurately for large-scale problems by using a set of
equations  called the state evolution [6]. Analysis with the state evolution also shows that required
iteration numbers are O(1) even when the problem size is large. Although we can present the state
evolution for the algorithm proposed in this paper and give a proof of its validity like [8  18]  we do
not discuss the state evolution here due to the limited space available.
We introduce a one-parameter extension of the posterior distribution ^p(U; V jA) to treat the marginal-
ization problem and the MAP problem in a uniﬁed manner. It is deﬁned as follows:

^p(U; V jA; (cid:12)) / exp

∥A (cid:0) U V

⊤∥2

F

2m(cid:28)

^pU(U )^pV(V )

(8)
which is proportional to ^p(U; V jA)(cid:12)  where (cid:12) > 0 is the parameter. When (cid:12) = 1  ^p(U; V jA; (cid:12))
is reduced to ^p(U; V jA). In the limit (cid:12) ! 1  the distribution ^p(U; V jA; (cid:12)) concentrates on the
maxima of ^p(U; V jA). An algorithm for the marginalization problem on ^p(U; V jA; (cid:12)) is particu-
larized to the algorithms for the marginalization problem and for the MAP problem for the original
posterior distribution ^p(U; V jA) by letting (cid:12) = 1 and (cid:12) ! 1  respectively. The AMP algorithm
for the marginalization problem on ^p(U; V jA; (cid:12)) is derived in a way similar to that described in [9] 
as detailed in the Supplementary Material.
In the derived algorithm  the values of variables Bt
u;m)
v =
⊤ 2 Rm(cid:2)^r 
(bt
v;1; : : : ; bt
1; : : : ; ut
2 R^r(cid:2)^r are calculated it-
V t = (vt
eratively  where the superscript t 2 N [ f0g represents iteration numbers. Variables with a negative
iteration number are deﬁned as 0. The algorithm is as follows:

⊤ 2 RN(cid:2)^r  (cid:3)t
⊤ 2 RN(cid:2)^r  St
N )

2 R^r(cid:2)^r  (cid:3)t
1; : : : ; St
m

⊤ 2 Rm(cid:2)^r  Bt

u;1; : : : ; bt
2 R^r(cid:2)^r  U t = (ut

u = (bt
2 R^r(cid:2)^r  and T t

1; : : : ; T t
N

v

1; : : : ; vt

v;N )

m)

u

((cid:0) (cid:12)

)(

)(cid:12)

;

4

N∑
m∑

j=1

N∑
m∑

j=1

U t(cid:0)1

T t
j ; (cid:3)t

u =

⊤

(V t)

V t +

1
m(cid:28)

1

(cid:12)m(cid:28)

T t
j

(cid:0) 1
m(cid:28)

T t
j ;

(9a)

j=1
St
i = G(bt

u;i; (cid:3)t

u; ^pu);

V t

St
i ; (cid:3)t

v =

i=1
T t+1
j = G(bt

v;j; (cid:3)t

v; ^pv):

⊤

(U t)

U t +

1
m(cid:28)

1

(cid:12)m(cid:28)

St
i

(cid:0) 1
m(cid:28)

i=1

i=1

St
i ;

(9b)

(9c)

(9d)

Algorithm 2.

Bt

u =

1
m(cid:28)

AV t (cid:0) 1
m(cid:28)

ut

i = f (bt

u;i; (cid:3)t

Bt

v =

⊤

A

1
m(cid:28)

u; ^pu);
U t (cid:0) 1
m(cid:28)

vt+1
j = f (bt

v;j; (cid:3)t

v; ^pv);

N∑
m∑

∫

Algorithm 2 is almost symmetric in U and V . Equations (9a)–(9b) and (9c)–(9d) update quantities
related to the estimates of U0 and V0  respectively. The algorithm requires an initial value V 0 and
j = O. The functions f ((cid:1);(cid:1); ^p) : R^r(cid:2)R^r(cid:2)^r ! R^r and G((cid:1);(cid:1); ^p) : R^r(cid:2)R^r(cid:2)^r ! R^r(cid:2)^r 
begins with T 0
which have a p.d.f. ^p : R^r ! R as a parameter  are deﬁned by

f (b; (cid:3); ^p) :=

u^q(u; b; (cid:3); ^p)du;

G(b; (cid:3); ^p) :=

where ^q(u; b; (cid:3); ^p) is the normalized p.d.f. of u deﬁned by
(cid:3)u (cid:0) b

(
^q(u; b; (cid:3); ^p) / exp

(cid:0)(cid:12)

(

⊤

u

⊤

@b

))
u (cid:0) log ^p(u)

1
2

@f (b; (cid:3); ^p)

;

(10)

:

(11)

One can see that f (b; (cid:3); ^p) is the mean of the distribution ^q(u; b; (cid:3); ^p) and that G(b; (cid:3); ^p) is its
covariance matrix scaled by (cid:12). The function f (b; (cid:3); ^p) need not be differentiable everywhere;
Algorithm 2 works if f (b; (cid:3); ^p) is differentiable at b for which one needs to calculate G(b; (cid:3); ^p) in
running the algorithm.
We assume in the rest of this section the convergence of Algorithm 2  although the convergence is
1 be the converged values
not guaranteed in general. Let B
of the respective variables. First  consider running Algorithm 2 with (cid:12) = 1. The marginal posterior
distribution is then approximated as

1  and V

1
j   U

1
u   B

1
v   (cid:3)

1
u   (cid:3)

1
v   S

1
i

  T

1
Since u
i
posterior mean E[U V

and v

1
j

∫

^pi;j(ui; vjjA) (cid:25) ^q(ui; b
1
1
u ; ^pu)^q(vj; b
u;i; (cid:3)
1
1
are the means of ^q(u; b
u ; ^pu) and ^q(v; b
u;i; (cid:3)
⊤jA] =

1
v;j; (cid:3)
1
1
v ; ^pv)  respectively  the
v;j; (cid:3)
^p(U; V jA)dU dV is approximated as
E[U V

1
v ; ^pv):

(12)

U V

(13)

(V

1

1

⊤

⊤

)

:

i

i

j

The marginal MAP estimates uMMAP
1
u;i; (cid:3)

uMMAP

1
u ; ^pu);

^q(u; b

(14)
Taking the limit (cid:12) ! 1 in Algorithm 2 yields an algorithm for the MAP problem (4). In this case 
the functions f and G are replaced with
(cid:3)u (cid:0) b

u (cid:0) log ^p(u)

; G1(b; (cid:3); ^p) :=

@f1(b; (cid:3); ^p)

vMMAP
j

^q(v; b

]

(cid:25) arg max
[

1
v ; ^pv):

1
v;j; (cid:3)

are approximated as
(cid:25) arg max

(15)

⊤

⊤

u

u

v

:

f1(b; (cid:3); ^p) := arg min
u

1
2

@b

One may calculate G1(b; (cid:3); ^p) from the Hessian of log ^p(u) at u = f1(b; (cid:3); ^p)  denoted by H 
via the identity G1(b; (cid:3); ^p) =
: This identity follows from the implicit function theorem
under some additional assumptions and helps in the case where the explicit form of f1(b; (cid:3); ^p) is
not available. The MAP estimate is approximated by (U

; V

1

1

).

(
(cid:3)(cid:0)H

)(cid:0)1

⊤jA] (cid:25) U
and vMMAP

4.2 Properties of the algorithm

Algorithm 2 has several plausible properties. First  it has a low computational cost. The compu-
tational cost per iteration is O(mN )  which is linear in the number of components of the matrix
A. Calculation of f ((cid:1);(cid:1); ^p) and G((cid:1);(cid:1); ^p) is performed O(N + m) times per iteration. The constant

5

factor depends on ^p and (cid:12). Calculation of f for (cid:12) < 1 generally involves an ^r-dimensional numer-
ical integration  although they are not needed in cases where an analytic expression of the integral
is available and cases where the variables take only discrete values. Calculation of f1 involves
minimization over an ^r-dimensional vector. When (cid:0) log ^p is a convex function and (cid:3) is positive
semideﬁnite  this minimization problem is convex and can be solved at relatively low cost.
Second  Algorithm 2 has a form similar to that of an algorithm based on the variational Bayesian
matrix factorization. In fact  if the last terms on the right-hand sides of the four equations in (9a)
and (9c) are removed  the resulting algorithm is the same as an algorithm based on the variational
Bayesian matrix factorization proposed in [4] and  in particular  the same as the ICM algorithm when
(cid:12) ! 1. (Note  however  that [4] only treats the case where the priors ^pu and ^pv are multivariate
Gaussian distributions.) Note that additional computational cost for these extra terms is O(m + N ) 
which is insigniﬁcant compared with the cost of the whole algorithm  which is O(mN ).
Third  when one deals with the MAP problem  the value of C MAP(U; V ) may increase in itera-
tions of Algorithm 2. The following proposition  however  guarantees optimality of the output of
Algorithm 2 in a certain sense  if it has converged.
1
1
Proposition 1. Let (U
1 ; : : : ; T
1 ; : : : ; S
1
for the MAP problem and suppose that
i and
1 is a global minimum of CMAP(U; V
U

1
N ) be a ﬁxed point of the AMP algorithm
1
j are positive semideﬁnite. Then

1 is a global minimum of CMAP(U

1
m ; T
m
i=1 S
) and V

∑

∑

N
j=1 T

; V ).

; S

; V

1

1

1

1

The proof is in the Supplementary Material. The key to the proof is the following reformulation:

( 1

N∑

)

2m(cid:28)

j=1

)]

(U (cid:0) U t(cid:0)1)

(U (cid:0) U t(cid:0)1)

⊤

T t
j

(16)

U t = arg min

U

∑

[
(
C MAP(U; V t) (cid:0) tr
∑
∑

N
j=1 T t

j is positive semideﬁnite  the second term of the minimand is the negative squared pseudo-
If
metric between U and U t(cid:0)1  which is interpreted as a penalty on nearness to the temporal estimate.
Positive semideﬁniteness of
j holds in almost all cases. In fact  we only have
to assume lim(cid:12)!1 G(b; (cid:3); ^p) = G1(b; (cid:3); ^p)  since G(b; (cid:3); ^p) is a scaled covariance matrix of
^q(u; b; (cid:3); ^p)  which is positive semideﬁnite. It follows from Proposition 1 that any ﬁxed point of the
AMP algorithm is also a ﬁxed point of the ICM algorithm. It has two implications: (i) Execution
of the ICM algorithm initialized with the converged values of the AMP algorithm does not improve
CMAP(U t; V t). (ii) The AMP algorithm has not more ﬁxed points than the ICM algorithm. The
second implication may help the AMP algorithm avoid getting stuck in bad local minima.

N
j=1 T t

m
i=1 St

i and

4.3 Clustering via AMP algorithm

∑

One can use the AMP algorithm for the MAP problem to perform the K-means clustering by letting
l=1 (cid:14)(v (cid:0) el): Noting that f1(b; (cid:3); ^pv) is piecewise constant with
^pu(u) = 1 and ^pv(v) = ^r
respect to b and hence G1(b; (cid:3); ^pv) is O almost everywhere  we obtain the following algorithm:
Algorithm 3 (AMP algorithm for the K-means clustering).

(cid:0)1

^r

Bt

u =

Bt

v =

1
m(cid:28)
1
m(cid:28)

u =

⊤

(V t)

1
m(cid:28)

V tSt; (cid:3)t

AV t; (cid:3)t
U t (cid:0) 1
(cid:28)

⊤

A

[

vt+1
j = arg

min

v2fe1;:::;e^rg

⊤

v

1
2

u((cid:3)t
u)
U t (cid:0) 1
(cid:28)

St;

V t; U t = Bt

⊤

(U t)

]

:

1
m(cid:28)
⊤
bt
v;j

(cid:0)1;
(cid:0)1; St = ((cid:3)t
u)

(17a)

(17b)

(17c)

(18a)

(18b)

N∑

j=1

It is initialized with an assignment V 0 2 fe1; : : : ; e^rgN . Algorithm 3 is rewritten as follows:

nt

l =

I(vt

j = el);

lt+1
j = arg min

l2f1;:::;^rg

1
m(cid:28)

[

~ut

l =

1
nt
l
∥aj (cid:0) ~ut

l

ajI(vt

j = el);
j = el) (cid:0) m
nt
l

I(vt

]

;

vt+1
j = elt+1

j

:

v =
vv (cid:0) v
(cid:3)t
N∑

j=1

∥2
2 +

2m
nt
l

6

∑

∑

(cid:0)1

(cid:0)2

ijSt

⊤∥2

(cid:0)2N

m
i=1 St

m
i=1 A2

i was estimated by (cid:28) m

In practice  we propose using m

The parameter (cid:28) appearing in the algorithm does not exist in the K-means clustering problem. In
fact  (cid:28) appears because m
i in deriving Algorithm 2 
(cid:0)1∥A (cid:0)
which can be justiﬁed for large-sized problems.
F as a temporary estimate of (cid:28) at tth iteration. While the AMP algorithm for the K-
U t(V t)
means clustering updates the value of U in the same way as Lloyd’s K-means algorithm  it performs
assignments of data to clusters in a different way. In the AMP algorithm  in addition to distances
from data to centers of clusters  the assignment at present is taken into consideration in two ways:
(i) A datum is less likely to be assigned to the cluster that it is assigned to at present. (ii) Data are
more likely to be assigned to a cluster whose size at present is smaller. The former can intuitively be
j = el  one should take account of the fact that the cluster center
understood by observing that if vt
(cid:0)1I(vt
l is biased toward aj. The term 2m(nt
j = el) in (18b) corrects this bias  which  as it
~ut
l)
should be  is inversely proportional to the cluster size.
The AMP algorithm for maximum accuracy clustering is obtained by letting (cid:12) = 1 and ^pv(v) be
a discrete distribution on fe1; : : : ; e^rg. After the algorithm converges  arg maxv ^q(v; v
1
v ; ^pv)
1 gives the estimate of the cluster centers.
gives the ﬁnal cluster assignment of the jth datum and U

1
j ; (cid:3)

5 Numerical experiments

We conducted numerical experiments on both artiﬁcial and real data sets to evaluate performance
of the proposed algorithms for clustering. In the experiment on artiﬁcial data sets  we set m = 800
and N = 1600 and let ^r = r. Cluster centers ~u0;l; l = 1; : : : ; r; were generated according to the
multivariate Gaussian distribution N (0; I). Cluster assignments v0;j; j = 1; : : : ; N; were generated
according to the uniform distribution on fe1; : : : ; erg. For ﬁxed (cid:28) = 0:1 and r  we generated 500
problem instances and solved them with ﬁve algorithms: Lloyd’s K-means algorithm (K-means) 
the AMP algorithm for the K-means clustering (AMP-KM)  the variational Bayes matrix factoriza-
tion [4] for maximum accuracy clustering (VBMF-MA)  the AMP algorithm for maximum accuracy
clustering (AMP-MA)  and the K-means++ [16]. The K-means++ updates the variables in the same
way as Lloyd’s K-means algorithm with an initial value chosen in a sophisticated manner. For the
other algorithms  initial values v0
j ; j = 1; : : : ; N; were randomly generated from the same distribu-
tion as v0;j. We used the true prior distributions of U and V for maximum accuracy clustering.
We ran Lloyd’s K-means algorithm and the K-means++ until no change was observed. We ran the
AMP algorithm for the K-means clustering until either V t = V t(cid:0)1 or V t = V t(cid:0)2 is satisﬁed.
This is because we observed oscillations of assignments of a small number of data. For the other
F and ∥V t (cid:0)
two algorithms  we terminated the iteration when ∥U t (cid:0) U t(cid:0)1∥2
F < 10
V t(cid:0)1∥2
F were met or the number of iterations exceeded 3000. We then evaluated
∑
(cid:3)
the following performance measures for the obtained solution (U

(cid:15) Normalized K-means loss ∥A(cid:0)U
(V
(cid:15) Accuracy maxP N
(cid:3)
j = v0;j)  where the maximization is taken over all
r-by-r permutation matrices. We used the Hungarian algorithm [19] to solve this maxi-
mization problem efﬁciently.

):
∥aj(cid:0) (cid:22)a∥2

2)  where (cid:22)a := 1
N

(cid:0)15∥V t(cid:0)1∥2

(cid:0)15∥U t(cid:0)1∥2

N
j=1 I(P v

j=1 aj.

∑

∑

F < 10

⊤∥2

N
j=1

F =(

(cid:3)

; V

N

(cid:3)

(cid:3)

)

(cid:0)1

(cid:15) Number of iterations needed to converge.

We calculated the averages and the standard deviations of these performance measures over 500
instances. We conducted the above experiments for various values of r.
Figure 1 shows the results. The AMP algorithm for the K-means clustering achieves the smallest K-
means loss among the ﬁve algorithms  while the Lloyd’s K-means algorithm and K-means++ show
large K-means losses for r (cid:21) 5. We emphasize that all the three algorithms are aimed to minimize
the same K-means loss and the differences lie in the algorithms for minimization. The AMP algo-
rithm for maximum accuracy clustering achieves the highest accuracy among the ﬁve algorithms. It
also shows fast convergence. In particular  the convergence speed of the AMP algorithm for max-
imum accuracy clustering is comparable to that of the AMP algorithm for the K-means clustering
when the two algorithms show similar accuracy (r < 9). This is in contrast to the common observa-
tion that the variational Bayes method often shows slower convergence than the ICM algorithm.

7

(a)

(c)

(b)

(d)

Figure 1: (a)–(c) Performance for different r: (a) Normalized K-means loss. (b) Accuracy. (c)
Number of iterations needed to converge.
(d) Dynamics for r = 5. Average accuracy at each
iteration is shown. Error bars represent standard deviations.

(a)

(b)

Figure 2: Performance measures in real-data experiments. (a) Normalized K-means loss. (b) Accu-
racy. The results for the 50 trials are shown in the descending order of performance for AMP-KM.
The worst two results for AMP-KM are out of the range.

In the experiment on real data  we used the ORL Database of Faces [20]  which contains 400 images
of human faces  ten different images of each of 40 distinct subjects. Each image consists of 112 (cid:2)
92 = 10304 pixels whose value ranges from 0 to 255. We divided N = 400 images into ^r = 40
clusters with the K-means++ and the AMP algorithm for the K-means clustering. We adopted the
initialization method of the K-means++ also for the AMP algorithm  because random initialization
often yielded empty clusters and almost all data were assigned to only one cluster. The parameter (cid:28)
was estimated in the way proposed in Subsection 4.3. We ran 50 trials with different initial values 
and Figure 2 summarizes the results.
The AMP algorithm for the K-means clustering outperformed the standard K-means++ algorithm
in 48 out of the 50 trials in terms of the K-means loss and in 47 trials in terms of the accuracy.
The AMP algorithm yielded just one cluster with all data assigned to it in two trials. The attained
minimum value of K-means loss is 0.412 with the K-means++ and 0.400 with the AMP algorithm.
The accuracies at these trials are 0.635 with the K-means++ and 0.690 with the AMP algorithm. The
average number of iterations was 6.6 with the K-means++ and 8.8 with the AMP algorithm. These
results demonstrate efﬁciency of the proposed algorithm on real data.

8

 0.97 0.975 0.98 0.985 0.99 0.995 1 2 4 6 8 10 12 14 16 18rNormalized K-means lossK-meansAMP-KMVBMF-MAAMP-MAK-means++ 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10 12 14 16 18rAccuracyK-meansAMP-KMVBMF-MAAMP-MAK-means++ 0 500 1000 1500 2000 2500 2 4 6 8 10 12 14 16 18rNumber of iterationsK-meansAMP-KMVBMF-MAAMP-MAK-means++ 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50Iteration numberAccuracyAMP-KMVBMF-MAAMP-MA 0.39 0.4 0.41 0.42 0.43 0.44 0.45 0.46 0 10 20 30 40 50Number of trialsNormalized K-means lossK-means++AMP-KM 0.5 0.55 0.6 0.65 0.7 0.75 0 10 20 30 40 50Number of trialsAccuracyK-means++AMP-KMReferences
[1] P. Paatero  “Least squares formulation of robust non-negative factor analysis ” Chemometrics and Intelli-

gent Laboratory Systems  vol. 37  no. 1  pp. 23–35  May 1997.

[2] P. O. Hoyer  “Non-negative matrix factorization with sparseness constraints ” The Journal of Machine

Learning Research  vol. 5  pp. 1457–1469  Dec. 2004.

[3] R. Salakhutdinov and A. Mnih  “Bayesian probabilistic matrix factorization using Markov chain Monte
Carlo ” in Proceedings of the 25th International Conference on Machine Learning  New York  NY  Jul. 5–
Aug. 9  2008  pp. 880–887.

[4] Y. J. Lim and Y. W. Teh  “Variational Bayesian approach to movie rating prediction ” in Proceedings of

KDD Cup and Workshop  San Jose  CA  Aug. 12  2007.

[5] T. Raiko  A. Ilin  and J. Karhunen  “Principal component analysis for large scale problems with lots
of missing values ” in Machine Learning: ECML 2007  ser. Lecture Notes in Computer Science  J. N.
Kok  J. Koronacki  R. L. de Mantaras  S. Matwin  D. Mladeniˇc  and A. Skowron  Eds. Springer Berlin
Heidelberg  2007  vol. 4701  pp. 691–698.

[6] D. L. Donoho  A. Maleki  and A. Montanari  “Message-passing algorithms for compressed sensing ”
Proceedings of the National Academy of Sciences USA  vol. 106  no. 45  pp. 18 914–18 919  Nov. 2009.
[7] S. Rangan  “Generalized approximate message passing for estimation with random linear mixing ” in Pro-
ceedings of 2011 IEEE International Symposium on Information Theory  St. Petersburg  Russia  Jul. 31–
Aug. 5  2011  pp. 2168–2172.

[8] S. Rangan and A. K. Fletcher  “Iterative estimation of constrained rank-one matrices in noise ” in Pro-
ceedings of 2012 IEEE International Symposium on Information Theory  Cambridge  MA  Jul. 1–6  2012 
pp. 1246–1250.

[9] R. Matsushita and T. Tanaka  “Approximate message passing algorithm for low-rank matrix reconstruc-
tion ” in Proceedings of the 35th Symposium on Information Theory and its Applications  Oita  Japan 
Dec. 11–14  2012  pp. 314–319.

[10] W. Xu  X. Liu  and Y. Gong  “Document clustering based on non-negative matrix factorization ” in Pro-
ceedings of the 26th annual international ACM SIGIR conference on Research and development in infor-
maion retrieval  Toronto  Canada  Jul. 28–Aug. 1  2003  pp. 267–273.

[11] C. Ding  T. Li  and M. Jordan  “Convex and semi-nonnegative matrix factorizations ” IEEE Transactions

on Pattern Analysis and Machine Intelligence  vol. 32  no. 1  pp. 45–55  Jan. 2010.

[12] S. P. Lloyd  “Least squares quantization in PCM ” IEEE Transactions on Information Theory  vol. IT-28 

no. 2  pp. 129–137  Mar. 1982.

[13] F. Krzakala  M. M´ezard  and L. Zdeborov´a  “Phase diagram and approximate message passing for blind

calibration and dictionary learning ” preprint  Jan. 2013  arXiv:1301.5898v1 [cs.IT].

[14] J. T. Parker  P. Schniter  and V. Cevher  “Bilinear generalized approximate message passing ” preprint 

Oct. 2013  arXiv:1310.2632v1 [cs.IT].

[15] S. Nakajima and M. Sugiyama  “Theoretical analysis of Bayesian matrix factorization ” Journal of Ma-

chine Learning Research  vol. 12  pp. 2583–2648  Sep. 2011.

[16] D. Arthur and S. Vassilvitskii  “k-means++: the advantages of careful seeding ” in SODA ’07 Proceedings
of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms  New Orleans  Louisiana  Jan. 7–9 
2007  pp. 1027–1035.

[17] J. S. Yedidia  W. T. Freeman  and Y. Weiss  “Constructing free-energy approximations and generalized
belief propagation algorithms ” IEEE Transactions on Information Theory  vol. 51  no. 7  pp. 2282–2312 
Jul. 2005.

[18] M. Bayati and A. Montanari  “The dynamics of message passing on dense graphs  with applications to
compressed sensing ” IEEE Transactions on Information Theory  vol. 57  no. 2  pp. 764–785  Feb. 2011.
[19] H. W. Kuhn  “The Hungarian method for the assignment problem ” Naval Research Logistics Quarterly 

vol. 2  no. 1–2  pp. 83–97  Mar. 1955.

[20] F. S. Samaria and A. C. Harter  “Parameterisation of a stochastic model for human face identiﬁcation ” in
Proceedings of 2nd IEEE Workshop on Applications of Computer Vision  Sarasota FL  Dec. 1994  pp.
138–142. [Online]. Available: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

9

,Ryosuke Matsushita
Toshiyuki Tanaka
Di He
Yingce Xia
Tao Qin
Liwei Wang
Nenghai Yu
Tie-Yan Liu
Wei-Ying Ma