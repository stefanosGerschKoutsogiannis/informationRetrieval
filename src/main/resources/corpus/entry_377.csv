2018,Adaptation to Easy Data in Prediction with Limited Advice,We derive an online learning algorithm with improved regret guarantees for ``easy'' loss sequences. We consider two types of ``easiness'': (a) stochastic loss sequences and (b) adversarial loss sequences with small effective range of the losses. While a number of algorithms have been proposed for exploiting small effective range in the full information setting  Gerchinovitz and Lattimore [2016] have shown the impossibility of regret scaling with the effective range of the losses in the bandit setting. We show that just one additional observation per round is sufficient to circumvent the impossibility result. The proposed Second Order Difference Adjustments (SODA) algorithm requires no prior knowledge of the effective range of the losses  $\varepsilon$  and achieves an $O(\varepsilon \sqrt{KT \ln K}) + \tilde{O}(\varepsilon K \sqrt[4]{T})$ expected regret guarantee  where $T$ is the time horizon and $K$ is the number of actions. The scaling with the effective loss range is achieved under significantly weaker assumptions than those made by Cesa-Bianchi and Shamir [2018] in an earlier attempt to circumvent the impossibility result. We also provide a regret lower bound of $\Omega(\varepsilon\sqrt{T K})$  which almost matches the upper bound. In addition  we show that in the stochastic setting SODA achieves an $O\left(\sum_{a:\Delta_a>0} \frac{K\varepsilon^2}{\Delta_a}\right)$ pseudo-regret bound that holds simultaneously with the adversarial regret guarantee. In other words  SODA is safe against an unrestricted oblivious adversary and provides improved regret guarantees for at least two different types of ``easiness'' simultaneously.,Adaptation to Easy Data in Prediction with Limited

Advice

Tobias Sommer Thune

Department of Computer Science

University of Copenhagen
tobias.thune@di.ku.dk

Abstract

Yevgeny Seldin

Department of Computer Science

University of Copenhagen

seldin@di.ku.dk

We derive an online learning algorithm with improved regret guarantees for “easy”
loss sequences. We consider two types of “easiness”: (a) stochastic loss sequences
and (b) adversarial loss sequences with small effective range of the losses. While
a number of algorithms have been proposed for exploiting small effective range
in the full information setting  Gerchinovitz and Lattimore [2016] have shown
the impossibility of regret scaling with the effective range of the losses in the
bandit setting. We show that just one additional observation per round is sufﬁcient
to circumvent the impossibility result. The proposed Second Order Difference
Adjustments (SODA) algorithm requires no prior knowledge of the effective range
of the losses  ε  and achieves an O(ε
T ) expected regret
guarantee  where T is the time horizon and K is the number of actions. The scaling
with the effective loss range is achieved under signiﬁcantly weaker assumptions
than those made by Cesa-Bianchi and Shamir [2018] in an earlier attempt to
circumvent the impossibility result. We also provide a regret lower bound of
T K)  which almost matches the upper bound. In addition  we show that in
Ω(ε
the stochastic setting SODA achieves an O
pseudo-regret bound
that holds simultaneously with the adversarial regret guarantee. In other words 
SODA is safe against an unrestricted oblivious adversary and provides improved
regret guarantees for at least two different types of “easiness” simultaneously.

√
KT ln K) + ˜O(εK 4

(cid:16)(cid:80)

(cid:17)

a:∆a>0

Kε2
∆a

√

√

1

Introduction

Online learning algorithms with both worst-case regret guarantees and reﬁned guarantees for “easy”
loss sequences have come into research focus in recent years. In our work we consider prediction
with limited advice games [Seldin et al.  2014]  which are an interpolation between full information
games [Vovk  1990  Littlestone and Warmuth  1994  Cesa-Bianchi and Lugosi  2006] and games with
limited (a.k.a. bandit) feedback [Auer et al.  2002b  Bubeck and Cesa-Bianchi  2012].1 In prediction
with limited advice the learner faces K unobserved sequences of losses {(cid:96)a
t }t a  where a indexes
the sequence number and t indexes the elements within the a-th sequence. At each round t of the
game the learner picks a sequence At ∈ {1  . . .   K} and suffers the loss (cid:96)At
  which is then observed.
After that  the learner is allowed to observe the losses of M additional sequences in the same round t 
where 0 ≤ M ≤ K − 1. For M = K − 1 the setting is equivalent to a full information game and for
M = 0 it becomes a bandit game.

t

1There exists an orthogonal interpolation between full information and bandit games through the use of
feedback graphs Alon et al. [2017]  which is different and incomparable with prediction with limited advice  see
Seldin et al. [2014] for a discussion.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

For a practical motivation behind prediction with limited advice imagine that the loss sequences
correspond to losses of K different algorithms for solving some problem  or K different parametriza-
tions of one algorithm  or K different experts. If we had the opportunity we would have executed all
the algorithms or queried all the experts before making a prediction. This would correspond to a full
information game. But in reality we may be constrained by time  computational power  or monetary
budget. In such case we are forced to select algorithms or experts to query. Being able to query just
one expert or algorithm per prediction round corresponds to a bandit game  but we may have time
or money to get a bit more  even though not all of it. This is the setting modeled by prediction with
limited advice.
Our goal is to derive an algorithm for prediction with limited advice that is robust in the worst case and
provides improved regret guarantees in “easy” cases. There are multiple ways to deﬁne “easiness” of
loss sequences. Among them  loss sequences generated by i.i.d. sources  like the classical stochastic
bandit model [Robbins  1952  Lai and Robbins  1985  Auer et al.  2002a]  and adversarial sequences
with bounded effective range of the losses within each round [Cesa-Bianchi et al.  2007]. For the
former a simple calculation shows that in the full information setting the basic Hedge algorithm
[Vovk  1990  Littlestone and Warmuth  1994] achieves an improved “constant” (independent of time
horizon) pseudo-regret guarantee without sacriﬁcing the worst-case guarantee. Much more work is
required to achieve adaptation to this form of easiness in the bandit setting if we want to keep the
adversarial regret guarantee simultaneously [Bubeck and Slivkins  2012  Seldin and Slivkins  2014 
Auer and Chiang  2016  Seldin and Lugosi  2017  Wei and Luo  2018  Zimmert and Seldin  2018].
An algorithm that adapts to the second form of easiness in the full information setting was ﬁrst
proposed by Cesa-Bianchi et al. [2007] and a number of variations have followed [Gaillard et al. 
2014  Koolen and van Erven  2015  Luo and Schapire  2015  Wintenberger  2017]. However  a recent
result by Gerchinovitz and Lattimore [2016] have shown that such adaptation is impossible in the
bandit setting. Cesa-Bianchi and Shamir [2018] proposed a way to circumvent the impossibility result
by either assuming that the ranges of the individual losses are provided to the algorithm in advance or
assuming that the losses are smooth and an “anchor” loss of one additional arm is provided to the
algorithm. The latter assumption has so far only lead to a substantial improvement when the “anchor”
loss is always the smallest loss in the corresponding round.
We consider adaptation to both types of easiness in prediction with limited advice. We show that
M = 1 (just one additional observation per round) is sufﬁcient to circumvent the impossibility result
of Gerchinovitz and Lattimore [2016]. This assumption is weaker than the assumptions in Cesa-
Bianchi and Shamir [2018]. We propose an algorithm  which achieves improved regret guarantees
both when the effective loss range is small and when the losses are stochastic (generated i.i.d.). The
algorithm is inspired by the BOA algorithm of Wintenberger [2017]  but instead of working with
exponential weights of the cumulative losses and their second moment corrections it uses estimates
of the loss differences. The algorithm achieves an O(ε
T ) expected regret
guarantee with no prior knowledge of the effective loss range ε or time horizon T . We also provide
regret lower bound of Ω(ε
KT )  which matches the upper bound up to logarithmic terms and
smaller order factors. Furthermore  we show that in the stochastic setting the algorithm achieves an
pseudo-regret guarantee. The improvement in the stochastic setting is achieved
O
without compromising the adversarial regret guarantee.
The paper is structured in the following way. In Section 2 we lay out the problem setting. In Section 3
we present the algorithm and in Section 4 the main results about the algorithm. Proofs of the main
results are presented in Section 5.

√
KT ln K) + ˜O(εK 4

(cid:16)(cid:80)

(cid:17)

√

√

a:∆a>0

Kε2
∆a

2 Problem Setting

2  . . .}a∈{1 ... K} 
We consider sequential games deﬁned by K inﬁnite sequences of losses {(cid:96)a
t ∈ [0  1] for all a and t. At each round t ∈ {1  2  . . .} of the game the learner selects an
where (cid:96)a
action (a.k.a. “arm”) At ∈ [K] := {1  . . .   K} and then suffers and observes the corresponding loss
(cid:96)At
. Additionally  the learner is allowed to choose a second arm  Bt  and observe (cid:96)Bt
. The loss of
t
t
the second arm  (cid:96)Bt
  is not suffered by the learner. (This is analogous to the full information setting 
t
where the losses of all arms a (cid:54)= At are observed  but not suffered). It is assumed that (cid:96)Bt
is observed

1  (cid:96)a

t

2

after At has been selected  but other relative timing of events within a round is unimportant for our
analysis.
The performance of the learner up to round T is measured by expected regret deﬁned as

(cid:35)

(cid:34) T(cid:88)

t=1

(cid:35)

(cid:34) T(cid:88)

t=1

RT := E

(cid:96)At
t

− min
a∈[K]

E

(cid:96)a
t

 

(1)

where the expectation is taken with respect to potential randomization of the loss generation process
and potential randomization of the algorithm. We note that in the adversarial setting the losses
are considered deterministic and the second expectation can be omitted  whereas in the stochastic
setting the deﬁnition coincides with the deﬁnition of pseudo-regret [Bubeck and Cesa-Bianchi  2012 
Seldin and Lugosi  2017]. In some literature RT is termed excess of cumulative predictive risk
[Wintenberger  2017].
Below we deﬁne adversarial and stochastic loss generation models and effective range of loss
sequences.

Adversarial losses

In the adversarial setting the loss sequences are selected arbitrarily by an adversary. We restrict
ourselves to the oblivious model  where the losses are ﬁxed before the start of the game and do not
depend on the actions of the learner.

Stochastic losses
In the stochastic setting the losses are drawn i.i.d.  so that E[(cid:96)a
t ] = µa independently of t. Since we
have a ﬁnite number of arms  there exists a best arm a(cid:63) (not necessarily unique) such that µa(cid:63) ≤ µa
for all a. We further deﬁne the suboptimality gaps by

∆a := µa − µa(cid:63) ≥ 0.
In the stochastic setting the expected regret can be rewritten as

(cid:88)

a∈[K]:∆a>0

(cid:34) T(cid:88)

t=1

(cid:35)

RT =

where 1 is the indicator function.

Effective loss range

∆a E

1(At = a)

 

(2)

For both the adversarial and stochastic losses  we deﬁne the effective loss range as the smallest
number ε  such that for all t ∈ [T ] and a  a(cid:48) ∈ [K]:
t | ≤ ε

(3)
t ∈ [0  1]  we have ε ≤ 1  where ε = 1 corresponds to an unrestricted

almost surely.

|(cid:96)a
t − (cid:96)a(cid:48)

Since we have assumed that (cid:96)a
setting.

3 Algorithm

We introduce the Second Order Difference Adjustments (SODA) algorithm  summarized in Algo-
rithm 1. SODA belongs to the general class of exponential weights algorithms. The algorithm
has two important distinctions from the common members of this class. First  it uses cumulative
loss difference estimators instead of cumulative loss estimators for the exponential weights updates.
Instantaneous loss difference estimators at round t are deﬁned by

(cid:102)∆(cid:96)
t = (K − 1)1(Bt = a)

a

t − (cid:96)At
(cid:96)Bt

t

(cid:16)

(cid:17)

(4)
SODA samples the “secondary” action Bt (the additional observation) uniformly from K − 1 arms 
all except At  and the (K − 1) term above corresponds to importance weighting with respect to the

.

3

sampling of Bt. The loss difference estimators scale with the effective range of the losses and they
can be positive and negative. Both of these properties are distinct from the traditional loss estimators.
The second difference is that we are using a second order adjustment in the weighting inspired by
Wintenberger [2017]. We deﬁne the cumulative loss difference estimator and its second moment by

We then have the distribution pt for selecting the primary action At deﬁned by

Dt(a) :=

pa
t =

a
s

s=1

(cid:102)∆(cid:96)

a
s   St(a) :=

(cid:17)2
(cid:16)(cid:102)∆(cid:96)
t(cid:88)
t(cid:88)
t St−1(a)(cid:1)
exp(cid:0)−ηtDt−1(a) − η2
(cid:80)K
a=1 exp (−ηtDt−1(a) − η2
(cid:40)(cid:115)

t St−1(a))

s=1

.

ln K

1

 

(cid:41)

(5)

(6)

where ηt is a learning rate scheme  deﬁned as

ηt = min

(7)
The learning rate satisﬁes ηt ≤ 1/(2ε(K − 1)) for all t  which is required for the subsequent analysis.
The algorithm is summarized below:

maxa St−1(a) + (K − 1)2  

2(K − 1)

.

Initialize p1 ← (1/K  . . .   1/K).
for t = 1  2  . . . do

Draw At according to pt;
Draw Bt uniformly at random from the remaining actions [K] \ {At};
Observe (cid:96)At
t

;

Construct (cid:102)∆(cid:96)

and suffer (cid:96)At
  (cid:96)Bt
t
t
a
t by equation (4);

Update Dt(a)  St(a) by (5);
Deﬁne pt+1 by (6);

end

Algorithm 1: Second Order Difference Adjustments (SODA)

4 Main Results

We are now ready to present the regret bounds for SODA. We start with regret upper and lower
bounds in the adversarial regime and then show that the algorithm simultaneously achieves improved
regret guarantee in the stochastic regime.

4.1 Regret Upper Bound in the Adversarial Regime

First we provide an upper bound for the expected regret of SODA against oblivious adversaries that
produce loss sequences with effective loss range bounded by ε. Note that this result does not depend
on prior knowledge of the effective loss range ε or time horizon T .
Theorem 1. The expected regret of SODA against an oblivious adversary satisﬁes

RT ≤ 4ε(cid:112)(K − 1) ln K

(cid:118)(cid:117)(cid:117)(cid:116)T + (K − 1)

√

(cid:32)

T

2 +

ln

T (K − 1)

/2

+ 4(K − 1) ln K.

(cid:114)

(cid:16)√

(cid:33)

(cid:17)

√
√
A proof of this theorem is provided in Section 5.1.2 The upper bound scales as O(ε
˜O(εK 4

T )  which nearly matches the lower bound provided below.

KT ln K) +

2It is straightforward to extended the analysis to time-varying ranges  εt : |(cid:96)a

(cid:19)

(cid:18)

K 4(cid:113)(cid:80)T

+ ˜O

t=1 ε2
t

(cid:19)

t − (cid:96)a(cid:48)

t | ≤ εt for all a  a(cid:48) a.s. 
regret bound . For the sake of clarity we

(cid:18)(cid:113)(cid:80)T

which leads to an O
restrict the presentation to a constant ε.

t=1(ε2

t )K ln K

4

4.2 Regret Lower Bound in the Adversarial Regime

We show that in the worst case the regret must scale linearly with the effective loss range ε.
Theorem 2. In prediction with limited advice with M = 1 (one additional observation per round or 
equivalently  two observations per round in total)  for loss sequences with effective loss range ε  we
have for T ≥ 3K/32:

inf supRT ≥ 0.02ε

KT  

√

where the inﬁmum is with respect to the choices of the algorithm and the supremum is over all
oblivious loss sequences with effective loss range bounded by ε.

The theorem is proven by adaptation of the Ω(
KT ) lower bound by Seldin et al. [2014] for
prediction with limited advice with unrestricted losses in [0  1] and one extra observation. We provide
it in Appendix A. Note that the upper bound in Theorem 1 matches the lower bound up to logarithmic
terms and lower order additive factors. In particular  changing the selection strategy for the second
arm  Bt  from uniform to anything more sophisticated is not expected to yield signiﬁcant beneﬁts in
the adversarial regime.

√

4.3 Regret Upper Bound in the Stochastic Regime

Finally  we show that SODA enjoys constant expected regret in the stochastic regime. This is achieved
without sacriﬁcing the adversarial regret guarantee.
Theorem 3. The expected regret of SODA applied to stochastic loss sequences with gaps ∆a satisﬁes

(cid:20)(cid:18) 8K

RT ≤ (cid:88)

a:∆a>0

+ 16

ln K

(cid:19) ε2

∆a

(cid:21)

+ 4K +

∆a
K

.

(8)

A brief sketch of a proof of this theorem is given in Section 5.2  with the complete proof provided in
Appendix C.
Note that ε is the effective range of realizations of the losses  whereas the gaps ∆a are based on
the expected losses. Naturally  ∆a ≤ ε. For example  if the losses are Bernoulli then the range is
ε = 1  but the gaps are based on the distances between the biases of the Bernoulli variables. When
the losses are not {0  1}  but conﬁned to a smaller range ε  Theorem 3 yields a tighter regret bound.
The scaling of the regret bound in K is suboptimal and it is currently unknown whether it could be
improved without compromising the worst-case guarantee. Perhaps changing the selection strategy
for Bt could help here. We leave this improvement for future work.
To summarize  SODA achieves adversarial regret guarantee that scales with the effective loss range
and almost matches the lower bound and simultaneously has improved regret guarantee in the
stochastic regime.

5 Proofs

This section contains the proof of Theorem 1 and a proof sketch for Theorem 3. The proof of
Theorem 2 is provided in Appendix A.

5.1 Proof of Theorem 1

The proof of the theorem is prefaced by two lemmas  but ﬁrst we show some properties of the loss
difference estimators. We use EBt to denote expectation with respect to selection of Bt conditioned
on all random outcomes prior to this selection. For oblivious adversaries  the expected cumulative
loss difference estimators are equal to the negative expect regret against the corresponding arm a:

(cid:34) T(cid:88)

(cid:35)

(cid:102)∆(cid:96)

a
t

E

= E

(cid:34) T(cid:88)

(cid:105)(cid:35)

(cid:104)(cid:102)∆(cid:96)

a
t

E
Bt

t=1

t=1

(cid:34) T(cid:88)

(cid:16)

(cid:17)(cid:35)

T(cid:88)

(cid:34) T(cid:88)

(cid:35)

= E

t − (cid:96)At
(cid:96)a

t

=

t − E
(cid:96)a

(cid:96)At
t

=: −Ra
T  

t=1

t=1

t=1

5

where we have used the fact that(cid:102)∆(cid:96)
(cid:34) T(cid:88)
(cid:16)(cid:102)∆(cid:96)

E

a
t

(cid:17)2(cid:35)

with respect to the choice of Bt. Similarly  we have

(cid:34) T(cid:88)

(cid:16)

(cid:17)2(cid:35)

a
t is an unbiased estimate of (cid:96)a

t − (cid:96)At

t due to importance weighting

= (K − 1) E

t − (cid:96)At
(cid:96)a

t

.

(9)

t=1

t=1

Similar to the analysis of the anytime version of EXP3 in Bubeck and Cesa-Bianchi [2012]  which
builds on Auer et al. [2002b]  we consider upper and lower bounds on the expectation of the
incremental update. This is captured by the following lemma:
Lemma 1. With a learning rate scheme ηt for t = 1  2  . . .   where ηt ≤ 1/2ε(K − 1)  SODA fulﬁlls:

− T(cid:88)

t=1

(cid:102)∆(cid:96)
t ≤ ln K
ηT

a

+ ηT

T(cid:88)

t=1

a
t

(cid:16)(cid:102)∆(cid:96)
(cid:32)

ln

(cid:105)

E
a∼pt

(cid:88)

(cid:104)(cid:102)∆(cid:96)

(cid:17)2 − T(cid:88)
exp(cid:0)−ηDt(a) − η2St(a)(cid:1)(cid:33)
K(cid:88)

t=1

+

a
t

t

.

Φt(η) :=

1
η

1
K

a=1

for all a  where we deﬁne the potential

(Φt(ηt+1) − Φt(ηt))

(10)

(11)

Note that unlike in the analysis of EXP3  here the learning rates ηt do not have to be non-increasing.
A proof of this lemma is based on modiﬁcation of standard arguments and is found in Appendix B.1.
The second lemma is a technical one and is proven in Appendix B.2.
Lemma 2. Let σt with t ∈ N be an increasing positive sequence with bounded differences such that
σt − σt−1 ≤ c for a ﬁnite constant c. Let further σ0 = 0. Then

(cid:18)

T(cid:88)

t=1

σt

1√
σt−1 + c

−

1√
σt + c

(cid:19)

≤ 2(cid:112)σT−1 + c.

Proof of Theorem 1 We apply Lemma 1  which leads to the following inequality for any learning
rate scheme ηt for t = 1  2  . . .   where ηt ≤ 1/2ε(K − 1):

− T(cid:88)

t=1

a

(cid:102)∆(cid:96)
t ≤ ln K
ηT(cid:124)(cid:123)(cid:122)(cid:125)

1st

+ ηT

(cid:124)

(cid:16)(cid:102)∆(cid:96)
T(cid:88)
(cid:123)(cid:122)

t=1

2nd

a
t

(cid:17)2
(cid:125)

− T(cid:88)
(cid:124)

t=1

E
a∼pt

(cid:123)(cid:122)

3rd

(cid:104)(cid:102)∆(cid:96)

a
t

(cid:105)
(cid:125)

T(cid:88)
(cid:124)

t=1

+

(Φt(ηt+1) − Φt(ηt))

.

(12)

(cid:123)(cid:122)

4th

(cid:125)

Note that in expectation  the left hand side of (12) is the regret against arm a. We are thus interested
in bounding the expectation of the terms on the right hand side  where we note that the third term

vanishes in expectation. We ﬁrst consider the case where ηt =(cid:112)ln K/(maxa St(a) + (K − 1)2) 

postponing the initial value for now.
The ﬁrst term becomes:

√

(cid:113)

max

a

ln K

ST−1(a) + (K − 1)2.

(13)

ln K
ηT

=

The second term becomes:

√

(cid:112)maxa ST−1(a) + (K − 1)2

ST (a)

ηT ST (a) =

ln K

(cid:113)

√

≤

ln K

ST−1(a) + (K − 1)2 

(14)

max

a

where we use that St(a) ≤ St−1(a) + (K − 1)2 for all t by design.
Finally  for the fourth term in equation (12)  we need to consider the potential differences. Unlike in
the anytime analysis of EXP3  where this term is negative [Bubeck and Cesa-Bianchi  2012]  in our
case it turns to be related to the second moment of the loss difference estimators. We let

exp(cid:0)−ηDt(a) − η2St(a)(cid:1)
(cid:80)K
a=1 exp (−ηDt(a) − η2St(a))

qη
t =

(15)

6

1
K

η2 ln

(cid:88)

(cid:1)(cid:33)

exp(cid:0)−ηDa − η2Sa

denote the exponential update using the loss estimators up to t  but with a free learning rate η. We
further suppress some indices for readability  such that Da = Dt(a) and Sa = St(a) in the following.
We have
t(η) = − 1
Φ(cid:48)
(cid:88)

exp(cid:0)−ηDa − η2Sa
By using −ηDa − 2η2Sa = ln(cid:0)exp(−ηDa − η2Sa) exp(−η2Sa)(cid:1) the above becomes

(cid:32)
(cid:32)
exp(cid:0)−ηDa − η2Sa
(cid:18) qη

(cid:1) · (−Da − 2ηSa)
(cid:1)(cid:33)(cid:33)(cid:33)

a exp(cid:0)−ηDa − η2Sa
(cid:80)
(cid:80)
a exp (−ηDa − η2Sa)
(cid:32)
(cid:88)
1
K
a exp (−ηDa − η2Sa)
(cid:19)(cid:21)

(cid:32)
η2(cid:80)

−ηDa − 2η2Sa − ln

(cid:1) ·

(cid:20)

1
η

+

=

a

a

a

Φ(cid:48)
t(η) =

1
η2

E
a∼qη

t

ln

exp(−η2Sa)

=

1
η2 KL (qη

.

(16)

[St(a)]  

where we have used that 1/K is the pmf. of the uniform distribution over K arms. Since the
KL-divergence is always positive  we can rewrite the potential differences as

t

t (cid:107)1/K) − E
a∼qη
(cid:90) ηt

(cid:90) ηt

Φt(ηt+1) − Φt(ηt) = −

t(η)dη ≤
Φ(cid:48)

E
a∼qη

t

ηt+1

[St(a)] dη ≤

max

a

St(a)dη

ηt+1

√

=

ln K max

a

St(a)

1

St−1(a) + (K − 1)2

−

max

a

1

St(a) + (K − 1)2

(cid:113)

max

a

 .

t (a)
1/K

(cid:90) ηt

(cid:113)

ηt+1

By Lemma 2 we then have

√
Φt(ηt+1) − Φt(ηt) ≤ 2

ln K

(cid:113)

ST−1(a) + (K − 1)2.

max

a

(17)

T(cid:88)

t=1

Collecting the terms (13)  (14) and (17) and noting that these bounds hold for all a  by taking
expectations and using Jensen’s inequality we get

(cid:21)

ST−1(a) + (K − 1)2

(cid:105)

ST−1(a)

+ (K − 1)2.

(18)

(cid:20)

√
4

a

ln K

max

(cid:113)
(cid:114)
E(cid:104)
(cid:105) ≤ (K − 1)2ε2 E

max

a

ln K

(cid:34)

RT ≤ E
√
≤ 4

E(cid:104)

max

a

ST−1(a)

The remainder of the proof is to bound this inner expectation:

(cid:35)

1[Bt = a]

.

T−1(cid:88)

t=1

max

a

(cid:111)

max

a

Z a

T−1 > α

t = (cid:80)t

E[max

a

Z a

Let Z a
probability for a cutoff α > 0:

s=1

1[Bs = a] and note that Z a

T−1 ≤ T − 1. We now consider a partioning of the

a

max

T−1 ≤ α
Z a

+ (T − 1) P(cid:110)
(cid:111)
T−1] ≤ α P(cid:110)
≤ α + (T − 1)K P(cid:8)Z a
T−1 > α(cid:9)  
T =(cid:80)T
T−1 > α(cid:9) .
T−1 > α(cid:9) ≤ P(cid:8)X a
P(cid:8)Z a

t=1 xa

using a union bound for the ﬁnal inequality. To continue we need to address the fact that the Bt’s are
not independent. We can however note that P{Bt = a} ≤ (K − 1)−1 for all t and a. By letting xa
t
be Bernoulli with parameter (K − 1)−1 and X a

t we then get

In the upper bound we can thus substitute X a
independent by construction. Note further that E[X a

T−1 for Z a

T−1 and exploit the fact that the xa

K−1  so by choosing α = T−1

T−1] = T−1

(19)
t ’s are
K−1 + δ for

7

δ > 0  we obtain by Hoeffding’s inequality:

(cid:114)

We now choose δ =

E[max

a

Z a

T−1] ≤ T − 1
K − 1
≤ T − 1
(cid:17)
(cid:16)√
K − 1
T (K − 1)
T−1] ≤ T − 1
K − 1

Z a

a

T
2 ln

E[max

+ δ + (T − 1)K P

+ δ + (T − 1)K exp

(cid:26)

(cid:27)

> δ

X a

T−1 − T − 1
(cid:18)
(cid:19)
K − 1
− 2δ2
T − 1

.

  which gives us

(cid:114)

(cid:16)√

+

T
2

ln

T (K − 1)

(cid:17)

√

T .

+ 2

Inserting this in (18) gives us the desired bound.
For the case where the learning rate at T is instead given by 1/2(K − 1) implying 4(K − 1)2 ln K ≥
maxa ST−1(a) + (K − 1)2  the ﬁrst term is ln K

= 2(K − 1) ln K  and the second term is

ηT

ηT ST (a) =

1

2(K − 1)

ST (a) ≤ ST−1(a) + (K − 1)2

2(K − 1)

≤ 4(K − 1)2 ln K
2(K − 1)

≤ 2(K − 1) ln K.

Since the learning rate is constant the potential differences vanish  completing the proof.

(cid:3)

5.2 Proof sketch of Theorem 3

E [pa

t ] ≤ σ + P{pa

Here we present the key ideas used to prove Theorem 3. The complete proof is provided in Ap-
pendix C.
Recall that the expected regret in the stochastic setting is given by (2)  where E[1(At = a)] = E[pa
t ].

Thus  we need to bound E[(cid:80)

t pa

(cid:111)
t ]. The ﬁrst step is to bound this as
(cid:80)t−1
Ke−ηt
i=1 Xi for Xi := (cid:102)∆(cid:96)
(cid:80)t−1

t > σ} ≤ σ + P(cid:110)
i −(cid:102)∆(cid:96)
t ≤ Ke−ηt

i=1 Xi > σ

for a positive threshold σ  where we show that pa

approach is motivated by the fact that EBi [(cid:102)∆(cid:96)
The next step is to tune σ ∝ exp((cid:80) Ei[Xi])  which allows us to bound the second term using

. This
] ∝ ∆a  where the expectation is with respect

to selection of Bi and the loss generation  conditioned on all prior randomness.

Azuma’s inequality and balance the two terms. Finally  this bound is summed over t using a technical
lemma for the limit of this sum.

a(cid:63)
i

a(cid:63)
i

a

(20)

i −(cid:102)∆(cid:96)

a

6 Discussion

√

(cid:17)

(cid:16)(cid:80)

√
KT ln K) + ˜O(εK 4

We have presented the SODA algorithm for prediction with limited advice with two observations
per round (the “primary” observation of the loss of the action that was played and one additional
observation). We have shown that the algorithm adapts to two types of simplicity of loss sequences
simultaneously: (a) it provides improved regret guarantees for adversarial sequences with bounded
effective range of the losses and (b) for stochastic loss sequences. In both cases the regret scales
linearly with the effective range and the knowledge of the range is not required. In the adversarial
case we achieve O(ε
T ) regret guarantee and in the stochastic case we achieve
regret guarantee. Our result demonstrates that just one extra observation per
O
round is sufﬁcient to circumvent the impossibility result of Gerchinovitz and Lattimore [2016] and
signiﬁcantly relaxes the assumptions made by Cesa-Bianchi and Shamir [2018] to achieve the same
goal.
There are a number of open questions and interesting directions for future research. One is to improve
the regret guarantee in the stochastic regime. Another is to extend the results to bandits with limited
advice in the spirit of Seldin et al. [2013]  Kale [2014].

Kε2
∆a

a:∆a>0

8

Acknowledgments

The authors thank Julian Zimmert for valuable input and discussions.

References
Noga Alon  Nicolo Cesa-Bianchi  Claudio Gentile  Shie Mannor  Yishay Mansour  and Ohad Shamir.
Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal on Computing 
46(6):1785–1826  2017.

Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic
and adversarial bandits. In Proceedings of the International Conference on Computational Learning
Theory (COLT)  2016.

Peter Auer  Nicolò Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine Learning  47  2002a.

Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal of Computing  32(1)  2002b.

Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-

armed bandit problems. Foundations and Trends in Machine Learning  5  2012.

Sébastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial
bandits. In Proceedings of the International Conference on Computational Learning Theory
(COLT)  2012.

Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  Learning  and Games. Cambridge University

Press  2006.

Nicolò Cesa-Bianchi and Ohad Shamir. Bandit regret scaling with the effective loss range. In

Proceedings of the International Conference on Algorithmic Learning Theory (ALT)  2018.

Nicolò Cesa-Bianchi  Yishay Mansour  and Gilles Stoltz. Improved second-order bounds for predic-

tion with expert advice. Machine Learning  66  2007.

Pierre Gaillard  Gilles Stoltz  and Tim van Erven. A second-order bound with excess losses. In
Proceedings of the International Conference on Computational Learning Theory (COLT)  2014.

Sébastien Gerchinovitz and Tor Lattimore. Reﬁned lower bounds for adversarial bandits. In Advances

in Neural Information Processing Systems (NIPS)  2016.

Satyen Kale. Multiarmed bandits with limited expert advice. In Proceedings of the International

Conference on Computational Learning Theory (COLT)  2014.

Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinatorial
games. In Proceedings of the International Conference on Computational Learning Theory (COLT) 
2015.

Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6  1985.

Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and

Computation  108  1994.

Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adanormalhedge. In
Proceedings of the International Conference on Computational Learning Theory (COLT)  2015.

Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematical Society  1952.

Yevgeny Seldin and Gábor Lugosi. An improved parametrization and analysis of the EXP3++
algorithm for stochastic and adversarial bandits. In Proceedings of the International Conference
on Computational Learning Theory (COLT)  2017.

9

Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial

bandits. In Proceedings of the International Conference on Machine Learning (ICML)  2014.

Yevgeny Seldin  Koby Crammer  and Peter L. Bartlett. Open problem: Adversarial multiarmed
bandits with limited advice. In Proceedings of the International Conference on Computational
Learning Theory (COLT)  2013.

Yevgeny Seldin  Peter L. Bartlett  Koby Crammer  and Yasin Abbasi-Yadkori. Prediction with
limited advice and multiarmed bandits with paid observations. In Proceedings of the International
Conference on Machine Learning (ICML)  2014.

Vladimir Vovk. Aggregating strategies. In Proceedings of the International Conference on Computa-

tional Learning Theory (COLT)  1990.

Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Proceedings of

the International Conference on Computational Learning Theory (COLT)  2018.

Olivier Wintenberger. Optimal learning with Bernstein online aggregation. Machine Learning  106 

2017.

Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits.

Technical report  https://arxiv.org/abs/1807.07623  2018.

10

,Matthew Staib
Sebastian Claici
Justin Solomon
Stefanie Jegelka
Tobias Sommer Thune
Yevgeny Seldin
Lili Su
Pengkun Yang