2017,Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications,We study combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of 1/p*  where p* is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing a triggering probability modulated (TPM) bounded smoothness condition into the influence maximization bandit and combinatorial cascading bandit satisfy this TPM condition. As a result  we completely remove the factor of 1/p* from the regret bounds  achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally  we provide lower bound results showing that the factor 1/p* is unavoidable for general CMAB-T problems  suggesting that the TPM condition is crucial in removing this factor.,Improving Regret Bounds for Combinatorial

Semi-Bandits with Probabilistically Triggered Arms

and Its Applications

Qinshi Wang

Princeton University
Princeton  NJ 08544

qinshiw@princeton.edu

Wei Chen

Microsoft Research

Beijing  China

weic@microsoft.com

Abstract

We study combinatorial multi-armed bandit with probabilistically triggered arms
and semi-bandit feedback (CMAB-T). We resolve a serious issue in the prior
CMAB-T studies where the regret bounds contain a possibly exponentially large
factor of 1/p∗  where p∗ is the minimum positive probability that an arm is trig-
gered by any action. We address this issue by introducing a triggering probability
modulated (TPM) bounded smoothness condition into the general CMAB-T fra-
mework  and show that many applications such as inﬂuence maximization bandit
and combinatorial cascading bandit satisfy this TPM condition. As a result  we
completely remove the factor of 1/p∗ from the regret bounds  achieving signiﬁ-
cantly better regret bounds for inﬂuence maximization and cascading bandits than
before. Finally  we provide lower bound results showing that the factor 1/p∗ is
unavoidable for general CMAB-T problems  suggesting that the TPM condition is
crucial in removing this factor.

1

Introduction

Stochastic multi-armed bandit (MAB) is a classical online learning framework modeled as a game
between a player and the environment with m arms. In each round  the player selects one arm and
the environment generates a reward of the arm from a distribution unknown to the player. The player
observes the reward  and use it as the feedback to the player’s algorithm (or policy) to select arms in
future rounds. The goal of the player is to cumulate as much reward as possible over time. MAB
models the classical dilemma between exploration and exploitation: whether the player should keep
exploring arms in search for a better arm  or should stick to the best arm observed so far to collect
rewards. The standard performance measure of the player’s algorithm is the (expected) regret  which
is the difference in expected cumulative reward between always playing the best arm in expectation
and playing according to the player’s algorithm.
In recent years  stochastic combinatorial multi-armed bandit (CMAB) receives many attention (e.g.
[9  7  6  10  13  15  14  16  8])  because it has wide applications in wireless networking  online
advertising and recommendation  viral marketing in social networks  etc. In the typical setting of
CMAB  the player selects a combinatorial action to play in each round  which would trigger the
play of a set of arms  and the outcomes of these triggered arms are observed as the feedback (called
semi-bandit feedback). Besides the exploration and exploitation tradeoff  CMAB also needs to deal
with the exponential explosion of the possible actions that makes exploring all actions infeasible.
One class of the above CMAB problems involves probabilistically triggered arms [7  14  16]  in
which actions may trigger arms probabilistically. We denote it as CMAB-T in this paper. Chen et al.
[7] provide such a general model and apply it to the inﬂuence maximization bandit  which models

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

stochastic inﬂuence diffusion in social networks and sequentially selecting seed sets to maximize
the cumulative inﬂuence spread over time. Kveton et al. [14  16] study cascading bandits  in which
arms are probabilistically triggered following a sequential order selected by the player as the action.
However  in both studies  the regret bounds contain an undesirable factor of 1/p∗  where p∗ is the
minimum positive probability that any arm can be triggered by any action 1 and this factor could be
exponentially large for both inﬂuence maximization and cascading bandits.
In this paper  we adapt the general CMAB framework of [7] in a systematic way to completely remove
the factor of 1/p∗ for a large class of CMAB-T problems including both inﬂuence maximization and
combinatorial cascading bandits. The key observation is that for these problems  a harder-to-trigger
arm has less impact to the expected reward and thus we do not need to observe it as often. We turn
this key observation into a triggering probability modulated (TPM) bounded smoothness condition 
adapted from the original bounded smoothness condition in [7]. We eliminates the 1/p∗ factor
in the regret bounds for all CMAB-T problems with the TPM condition  and show that inﬂuence
maximization bandit and the conjunctive/disjunctive cascading bandits all satisfy the TPM condition.
Moreover  for general CMAB-T without the TPM condition  we show a lower bound result that 1/p∗
is unavoidable  because the hard-to-trigger arms are crucial in determining the best arm and have to
be observed enough times.
Besides removing the exponential factor  our analysis is also tighter in other regret factors or constants
comparing to the existing inﬂuence maximization bandit results [7  25]  combinatorial cascading
bandit [16]  and linear bandits without probabilistically triggered arms [15]. Both the regret analysis
based on the TPM condition and the proof that inﬂuence maximization bandit satisﬁes the TPM
condition are technically involved and nontrivial  but due to the space constraint  we have to move
the complete proofs to the supplementary material. Instead we introduce the key techniques used in
the main text.

Related Work. Multi-armed bandit problem is originally formated by Robbins [20]  and has been
extensively studied in the literature [cf. 3  21  4]. Our study belongs to the stochastic bandit research 
while there is another line of research on adversarial bandits [2]  for which we refer to a survey
like [4] for further information. For stochastic MABs  an important approach is Upper Conﬁdence
Bound (UCB) approach [1]  on which most CMAB studies are based upon.
As already mentioned in the introduction  stochastic CMAB has received many attention in recent
years. Among the studies  we improve (a) the general framework with probabilistically triggered arms
of [7]  (b) the inﬂuence maximization bandit results in [7] and [25]  (c) the combinatorial cascading
bandit results in [16]  and (d) the linear bandit results in [15]. We defer the technical comparison
with these studies to Section 4.3. Other CMAB studies do not deal with probabilistically triggered
arms. Among them  [9] is the ﬁrst study on linear stochastic bandit  but its regret bound has since
been improved by Chen et al. [7]  Kveton et al. [15]. Combes et al. [8] improve the regret bound of
[15] for linear bandits in a special case where arms are mutually independent. Most studies above
are based on the UCB-style CUCB algorithm or its minor variant  and differ on the assumptions and
regret analysis. Gopalan et al. [10] study Thompson sampling for complex actions  which is based on
the Thompson sample approach [22] and can be applied to CMAB  but their regret bound has a large
exponential constant term.
Inﬂuence maximization is ﬁrst formulated as a discrete optimization problem by Kempe et al. [12] 
and has been extensively studied since (cf. [5]). Variants of inﬂuence maximization bandit have also
been studied [18  23  24]. Lei et al. [18] use a different objective of maximizing the expected size of
the union of the inﬂuenced nodes over time. Vaswani et al. [23] discuss how to transfer node level
feedback to the edge level feedback  and then apply the result of [7]. Vaswani et al. [24] replace
the original maximization objective of inﬂuence spread with a heuristic surrogate function  avoiding
the issue of probabilistically triggered arms. But their regret is deﬁned against a weaker benchmark
relaxed by the approximation ratio of the surrogate function  and thus their theoretical result is weaker
than ours.

1The factor of 1/f∗ used for the combinatorial disjunctive cascading bandits in [16] is essentially 1/p∗.

2

2 General Framework

i

1   . . .   X (t)

In this section we present the general framework of combinatorial multi-armed bandit with probabi-
listically triggered arms originally proposed in [7] with a slight adaptation  and denote it as CMAB-T.
We illustrate that the inﬂuence maximization bandit [7] and combinatorial cascading bandits [14  16]
are example instances of CMAB-T.
CMAB-T is described as a learning game between a learning agent (or player) and the environment.
The environment consists of m random variables X1  . . .   Xm called base arms (or arms) following
a joint distribution D over [0  1]m. Distribution D is picked by the environment from a class of
distributions D before the game starts. The player knows D but not the actual distribution D.
The learning process proceeds in discrete rounds. In round t ≥ 1  the player selects an action St from
an action space S based on the feedback history from the previous rounds  and the environment draws
from the joint distribution D an independent sample X (t) = (X (t)
m ). When action St is
played on the environment outcome X (t)  a random subset of arms τt ⊆ [m] are triggered  and the
for all i ∈ τt are observed as the feedback to the player. The player also obtains a
outcomes of X (t)
nonnegative reward R(St  X (t)  τt) fully determined by St  X (t)  and τt. A learning algorithm aims
at properly selecting actions St’s over time based on the past feedback to cumulate as much reward
as possible. Different from [7]  we allow the action space S to be inﬁnite. In the supplementary
material  we discuss an example of continuous inﬂuence maximization [26] that uses continuous and
inﬁnite action space while the number of base arms is still ﬁnite.
We now describe the triggered set τt in more detail  which is not explicit in [7]. In general  τt may
have additional randomness beyond the randomness of X (t). Let Dtrig(S  X) denote a distribution
of the triggered subset of [m] for a given action S and an environment outcome X. We assume that τt
is drawn independently from Dtrig(St  X (t)). We refer Dtrig as the probabilistic triggering function.
To summarize  a CMAB-T problem instance is a tuple ([m] S D  Dtrig  R)  with elements already
described above. These elements are known to the player  and hence establishing the problem input
to the player. In contrast  the environment instance is the actual distribution D ∈ D picked by the
environment  and is unknown to the player. The problem instance and the environment instance
together form the (learning) game instance  in which the learning process would unfold. In this paper 
we ﬁx the environment instance D  unless we need to refer to more than one environment instances.
For each arm i  let µi = EX∼D[Xi]. Let vector µ = (µ1  . . .   µm) denote the expectation vector of
arms. Note that vector µ is determined by D. Same as in [7]  we assume that the expected reward
E[R(S  X  τ )]  where the expectation is taken over X ∼ D and τ ∼ Dtrig(S  X)  is a function of
action S and the expectation vector µ of the arms. Henceforth  we denote rS(µ) (cid:44) E[R(S  X  τ )].
We remark that Chen et al. [6] relax the above assumption and consider the case where the entire
distribution D  not just the mean of D  is needed to determine the expected reward. However  they
need to assume that arm outcomes are mutually independent  and they do not consider probabilistically
triggered arms. It might be interesting to incorporate probabilistically triggered arms into their setting 
but this is out of the scope of the current paper. To allow algorithm to estimate µi directly from
samples  we assume the outcome of an arm does not depend on whether itself is triggered  i.e.
EX∼D τ∼Dtrig(S X)[Xi | i ∈ τ ] = EX∼D[Xi].
The performance of a learning algorithm A is measured by its (expected) regret  which is the difference
in expected cumulative reward between always playing the best action and playing actions selected
by algorithm A. Formally  let optµ = supS∈S rS(µ)  where µ = EX∼D[X]  and we assume
that optµ is ﬁnite. Same as in [7]  we assume that the learning algorithm has access to an ofﬂine
(α  β)-approximation oracle O  which takes µ = (µ1  . . .   µm) as input and outputs an action SO
such that Pr{rµ(SO) ≥ α · optµ} ≥ β  where α is the approximation ratio and β is the success
probability. Under the (α  β)-approximation oracle  the benchmark cumulative reward should be the
αβ fraction of the optimal reward  and thus we use the following (α  β)-approximation regret:

Deﬁnition 1 ((α  β)-approximation Regret). The T -round (α  β)-approximation regret of a le-
arning algorithm A (using an (α  β)-approximation oracle) for a CMAB-T game instance

3

([m] S D  Dtrig  R  D) with µ = EX∼D[X] is

RegA

µ α β(T ) = T · α· β · optµ − E

R(SA

t   X (t)  τt)

(cid:34) T(cid:88)

i=1

(cid:35)

= T · α· β · optµ − E

(cid:35)

rSA

t

(µ)

 

(cid:34) T(cid:88)

i=1

where SA
is the action A selects in round t  and the expectation is taken over the randomness of
t
the environment outcomes X (1)  . . .   X (T )  the triggered sets τ1  . . .   τT   as well as the possible
randomness of algorithm A itself.
We remark that because probabilistically triggered arms may strongly impact the determination of
the best action  but they may be hard to trigger and observe  the regret could be worse and the regret
analysis is in general harder than CMAB without probabilistically triggered arms.
The above framework essentially follows [7]  but we decouple actions from subsets of arms  allow
action space to be inﬁnite  and explicitly model triggered set distribution  which makes the framework
more powerful in modeling certain applications (see supplementary material for more discussions).

2.1 Examples of CMAB-T: Inﬂuence Maximization and Cascading Bandits

In social inﬂuence maximization [12]  we are given a weighted directed graph G = (V  E  p)  where
V and E are sets of vertices and edges respectively  and each edge (u  v) is associated with a
probability p(u  v). Starting from a seed set S ⊆ V   inﬂuence propagates in G as follows: nodes
in S are activated at time 0  and at time t ≥ 1  a node u activated in step t − 1 has one chance to
activate its inactive out-neighbor v with an independent probability p(u  v). The inﬂuence spread of
seed set S  σ(S)  is the expected number of activated nodes after the propagation ends. The ofﬂine
problem of inﬂuence maximization is to ﬁnd at most k seed nodes in G such that the inﬂuence spread
is maximized. Kempe et al. [12] provide a greedy algorithm with approximation ratio 1 − 1/e − ε
and success probability 1 − 1/|V |  for any ε > 0.
For the online inﬂuence maximization bandit [7]  the edge probabilities p(u  v)’s are unknown and
need to be learned over time through repeated inﬂuence maximization tasks: in each round t  k seed
nodes St are selected  the inﬂuence propagation from St is observed  the reward is the number of
nodes activated in this round  and one wants to repeat this process to cumulate as much reward as
possible. Putting it into the CMAB-T framework  the set of edges E is the set of arms [m]  and their
outcome distribution D is the joint distribution of m independent Bernoulli distributions with means
p(u  v) for all (u  v) ∈ E. Any seed set S ⊆ V with at most k nodes is an action. The triggered
arm set τt is the set of edges (u  v) reached by the propagation  that is  u can be reached from St
by passing through only edges e ∈ E with X (t)
e = 1. In this case  the distribution Dtrig(St  X (t))
degenerates to a deterministic triggered set. The reward R(St  X (t)  τt) equals to the number of nodes
in V that is reached from S through only edges e ∈ E with X (t)
e = 1  and the expected reward is
exactly the inﬂuence spread σ(St). The ofﬂine oracle is a (1− 1/e− ε  1/|V |)-approximation greedy
algorithm. We remark that the general triggered set distribution Dtrig(St  X (t)) (together with inﬁnite
action space) can be used to model extended versions of inﬂuence maximization  such as randomly
selected seed sets in general marketing actions [12] and continuous inﬂuence maximization [26] (see
supplementary material).
Now let us consider combinatorial cascading bandits [14  16]. In this case  we have m independent
Bernoulli random variables X1  . . .   Xm as base arms. An action is to select an ordered sequence
from a subset of these arms satisfying certain constraint. Playing this action means that the player
reveals the outcomes of the arms one by one following the sequence order until certain stopping
condition is satisﬁed. The feedback is the outcomes of revealed arms and the reward is a function
form of these arms. In particular  in the disjunctive form the player stops when the ﬁrst 1 is revealed
and she gains reward of 1  or she reaches the end and gains reward 0. In the conjunctive form  the
player stops when the ﬁrst 0 is revealed (and receives reward 0) or she reaches the end with all 1
outcomes (and receives reward 1). Cascading bandits can be used to model online recommendation
and advertising (in the disjunctive form with outcome 1 as a click) or network routing reliability (in
the conjunctive form with outcome 0 as the routing edge being broken). It is straightforward to see
that cascading bandits ﬁt into the CMAB-T framework: m variables are base arms  ordered sequences
are actions  and the triggered set is the preﬁx set of arms until the stopping condition holds.

4

For each arm i ∈ [m]  ρi ←(cid:113) 3 ln t

Algorithm 1 CUCB with computation oracle.
Input: m  Oracle
1: For each arm i  Ti ← 0 {maintain the total number of times arm i is played so far}
2: For each arm i  ˆµi ← 1 {maintain the empirical mean of Xi}
3: for t = 1  2  3  . . . do
4:
5:
6:
7:
8:
9: end for

For each arm i ∈ [m]  ¯µi = min{ˆµi + ρi  1} {the upper conﬁdence bound}
S ← Oracle(¯µ1  . . .   ¯µm)
Play action S  which triggers a set τ ⊆ [m] of base arms with feedback X (t)
i − ˆµi)/Ti
For every i ∈ τ  update Ti and ˆµi: Ti = Ti + 1  ˆµi = ˆµi + (X (t)

{the conﬁdence radius  ρi = +∞ if Ti = 0}

2Ti

i

’s  i ∈ τ

3 Triggering Probability Modulated Condition

Chen et al. [7] use two conditions to guarantee the theoretical regret bounds. The ﬁrst one is
monotonicity  which we also use in this paper  and is restated below.
Condition 1 (Monotonicity). We say that a CMAB-T problem instance satisﬁes monotonicity  if for
any action S ∈ S  for any two distributions D  D(cid:48) ∈ D with expectation vectors µ = (µ1  . . .   µm)
and µ(cid:48) = (µ(cid:48)

m)  we have rS(µ) ≤ rS(µ(cid:48)) if µi ≤ µ(cid:48)

i for all i ∈ [m].

1  . . .   µ(cid:48)

The second condition is bounded smoothness. One key contribution of our paper is to properly
strengthen the original bounded smoothness condition in [7] so that we can both get rid of the
undesired 1/p∗ term in the regret bound and guarantee that many CMAB problems still satisfy the
conditions. Our important change is to use triggering probabilities to modulate the condition  and
thus we call such conditions triggering probability modulated (TPM) conditions. The key point of
TPM conditions is including the triggering probability in the condition. We use pD S
to denote the
probability that action S triggers arm i when the environment instance is D. With this deﬁnition 
we can also technically deﬁne p∗ as p∗ = inf i∈[m] S∈S pD S
i >0 pD S
. In this section  we further use
1-norm based conditions instead of the inﬁnity-norm based condition in [7]  since they lead to better
regret bounds for the inﬂuence maximization and cascading bandits.
Condition 2 (1-Norm TPM Bounded Smoothness). We say that a CMAB-T problem instance sa-
tisﬁes 1-norm TPM bounded smoothness  if there exists B ∈ R+ (referred as the bounded smoothness
constant) such that  for any two distributions D  D(cid:48) ∈ D with expectation vectors µ and µ(cid:48)  and any

action S  we have |rS(µ) − rS(µ(cid:48))| ≤ B(cid:80)

|µi − µ(cid:48)
i|.

i∈[m] pD S

i

i

i

Note that the corresponding non-TPM version of the above condition would remove pD S
in the
above condition  which is a generalization of the linear condition used in linear bandits [15]. Thus  the
TPM version is clearly stronger than the non-TPM version (when the bounded smoothness constants
are the same). The intuition of incorporating the triggering probability pD S
to modulate the 1-norm
condition is that  when an arm i is unlikely triggered by action S (small pD S
)  the importance of
arm i also diminishes in that a large change in µi only causes a small change in the expected reward
rS(µ). This property sounds natural in many applications  and it is important for bandit learning —
although an arm i may be difﬁcult to observe when playing S  it is also not important to the expected
reward of S and thus does not need to be learned as accurately as others more easily triggered by S.

i

i

i

4 CUCB Algorithm and Regret Bound with TPM Bounded Smoothness

We use the same CUCB algorithm as in [7] (Algorithm 1). The algorithm maintains the empirical
estimate ˆµi for the true mean µi  and feed the upper conﬁdence bound ¯µi to the ofﬂine oracle to
obtain the next action S to play. The upper conﬁdence bound ¯µi is large if arm i is not triggered often
(Ti is small)  providing optimistic estimates for less observed arms. We next provide its regret bound.

5

Deﬁnition 2 (Gap). Fix a distribution D and its expectation vector µ. For each action S  we deﬁne
the gap ∆S = max(0  α · optµ − rS(µ)). For each arm i  we deﬁne

∆i

min =

inf
S∈S:pD S

i >0 ∆S >0

∆S 

∆i

max =

S∈S:pD S

sup
i >0 ∆S >0

∆S.

min  and ∆max = maxi∈[m] ∆i

i > 0 and ∆S > 0  we deﬁne ∆i

As a convention  if there is no action S such that pD S
max = 0. We deﬁne ∆min = mini∈[m] ∆i
∆i
Let ˜S = {i ∈ [m] | pµ S
For convenience  we use (cid:100)x(cid:101)0 to denote max{(cid:100)x(cid:101)  0} for any real number x.
Theorem 1. For the CUCB algorithm on a CMAB-T problem instance that satisﬁes monotonicity
(Condition 1) and 1-norm TPM bounded smoothness (Condition 2) with bounded smoothness constant
B  (1) if ∆min > 0  we have distribution-dependent bound

i > 0} be the set of arms that could be triggered by S. Let K = maxS∈S | ˜S|.

max.

min = +∞ 

Regµ α β(T ) ≤ (cid:88)

576B2K ln T

∆i

min

+

i∈[m]

(2) we have distribution-independent bound

Regµ α β(T ) ≤ 12B

√

mKT ln T +

log2

(cid:19)

(cid:18)(cid:24)

(cid:88)
(cid:18)(cid:24)

i∈[m]

2BK
∆i

min

(cid:25)

log2

T

18 ln T

0

(cid:25)

0

(cid:19)

+ 2

· π2
6

· ∆max + 4Bm;

(1)

+ 2

· m · π2
6

· ∆max + 2Bm.
√

(2)

∆

∆

√

min = ∆ for all i ∈ [m] and ∆i

log T ) since for that instance B = 1 and there are K arms with ∆i

For the above theorem  we remark that the regret bounds are tight (up to a O(
log T ) factor in the
case of distribution-independent bound) base on a lower bound result in [15]. More speciﬁcally 
Kveton et al. [15] show that for linear bandits (a special class of CMAB-T without probabilis-
tic triggering)  the distribution-dependent regret is lower bounded by Ω( (m−K)K
log T )  and the
mKT ) when T ≥ m/K  for some in-
distribution-independent regret is lower bounded by Ω(
min < ∞. Comparing with our regret upper
stance where ∆i
bound in the above theorem  (a) for distribution-dependent bound  we have the regret upper bound
O( (m−K)K
min = ∞  so tight with
√
the lower bound in [15]; and (b) for distribution-independent bound  we have the regret upper bound
log T ) factor  same as the upper bound for
O(
the linear bandits in [15]. This indicates that parameters m and K appeared in the above regret
bounds are all needed. As for parameter B  we can view it simply as a scaling parameter. If we
scale the reward of an instance to B times larger than before  certainly  the regret is B times larger.
Looking at the distribution-dependent regret bound (Eq. (1))  ∆i
min would also be scaled by a factor
of B  canceling one B factor from B2  and ∆max is also scaled by a factor of B  and thus the regret
bound in Eq. (1) is also scaled by a factor of B. In the distribution-independent regret bound (Eq. (2)) 
the scaling of B is more direct. Therefore  we can see that all parameters m  K  and B appearing in
the above regret bounds are needed. Finally  we remark that the TPM Condition 2 can be reﬁned such
that B is replaced by arm-dependent Bi that is moved inside the summation  and B in Theorem 1 is
replaced with Bi accordingly. See the supplementary material for details.

mKT log T )  tight to the lower bound up to a O(

√

4.1 Novel Ideas in the Regret Analysis

Due to the space limit  the full proof of Theorem 1 is moved to the supplementary material. Here
we brieﬂy explain the novel aspects of our analysis that allow us to achieve new regret bounds and
differentiate us from previous analyses such as the ones in [7] and [16  15].
We ﬁrst give an intuitive explanation on how to incorporate the TPM bounded smoothness condition
to remove the factor 1/p∗ in the regret bound. Consider a simple illustrative example of two actions
S0 and S  where S0 has a ﬁxed reward r0 as a reference action  and S has a stochastic reward
depending on the outcomes of its triggered base arms. Let ˜S be the set of arms that can be triggered
by S. For i ∈ ˜S  suppose i can be triggered by action S with probability pS
i   and its true mean is µi
and its empirical mean at the end of round t is ˆµi t. The analysis in [7] would need a property that  if
for all i ∈ ˜S |ˆµi t − µi| ≤ δi for some properly deﬁned δi  then S no longer generates regrets. The
analysis would conclude that arm i needs to be triggered Θ(log T /δ2
i ) times for the above condition

6

i δ2

i∈ ˜St

i >0 pS

i log T /δ2

S:pS

i (δi/pS

i )2)) = O(pS

i log T /δ2

to happen. Since arm i is only triggered with probability pS
i   it means action S may need to be played
i )) times. This is the essential reason why the factor 1/p∗ appears in the regret bound.
Θ(log T /(pS
Now with the TPM bounded smoothness  we know that the impact of |ˆµi t − µi| ≤ δi to the
difference in the expected reward is only pS
i δi  or equivalently  we could relax the requirement to
|ˆµi t − µi| ≤ δi/pS
i to achieve the same effect as in the previous analysis. This translates to the result
that action S would generate regret in at most O(log T /(pS
i ) rounds.
We then need to handle the case when we have multiple actions that could trigger arm i. The
i is not feasible since we may have exponentially or even
inﬁnitely many such actions. Instead  we introduce the key idea of triggering probability groups 
such that the above actions are divided into groups by putting their triggering probabilities pS
into geometrically separated bins: (1/2  1]  (1/4  1/2] . . .   (2−j  2−j+1]  . . . The actions in the same
i
group would generate regret in at most O(2−j+1 log T /δ2
i ) rounds with a similar argument  and
i ) = O(log T /δ2
i )

simple addition of(cid:80)
summing up together  they could generate regret in at most O((cid:80)

j 2−j+1 log T /δ2

B(cid:80)

i or 1/p∗ is completely removed from the regret bound.

rounds. Therefore  the factor of 1/pS
Next  we brieﬂy explain our idea to achieve the improved bound over the linear bandit result
in [15]. The key step is to bound regret ∆St generated in round t. By a derivation similar to
[15  7] together with the 1-norm TPM bounded smoothness condition  we would obtain that ∆St ≤
(¯µi t − µi) with high probability. The analysis in [15] would analyze the errors
|¯µi t − µi| by a cascade of inﬁnitely many sub-cases of whether there are xj arms with errors larger
than yj with decreasing yj  but it may still be loose. Instead we directly work on the above summation.
Naive bounding the about error summation would not give a O(log T ) bound because there could
be too many arms with small errors. Our trick is to use a reverse amortization: we cumulate small
errors on many sufﬁciently sampled arms and treat them as errors of insufﬁciently sample arms  such
that an arm sampled O(log T ) times would not contribute toward the regret. This trick tightens our
analysis and leads to signiﬁcantly improved constant factors.

pD St
i

4.2 Applications to Inﬂuence Maximization and Combinatorial Cascading Bandits

The following two lemmas show that both the cascading bandits and the inﬂuence maximization
bandit satisfy the TPM condition.
Lemma 1. For both disjunctive and conjunctive cascading bandit problem instances  1-norm TPM
bounded smoothness (Condition 2) holds with bounded smoothness constant B = 1.
Lemma 2. For the inﬂuence maximization bandit problem instances  1-norm TPM bounded
smoothness (Condition 2) holds with bounded smoothness constant B = ˜C  where ˜C is the largest
number of nodes any node can reach in the directed graph G = (V  E).

The proof of Lemma 1 involves a technique called bottom-up modiﬁcation. Each action in cascading
bandits can be viewed as a chain from top to bottom. When changing the means of arms below  the
triggering probability of arms above is not changed. Thus  if we change µ to µ(cid:48) backwards  the
triggering probability of each arm is unaffected before its expectation is changed  and when changing
the mean of an arm i  the expected reward of the action is at most changed by pD S
The proof of Lemma 2 is more complex  since the bottom-up modiﬁcation does not work directly
on graphs with cycles. To circumvent this problem  we develop an inﬂuence tree decomposition
technique as follows. First  we order all inﬂuence paths from the seed set S to a target v. Second 
each edge is independently sampled based on its edge probability to form a random live-edge graph.
Third  we divide the reward portion of activating v among all paths from S to v: for each live-edge
graph L in which v is reachable from S  assign the probability of L to the ﬁrst path from S to v in L
according to the path total order. Finally  we compose all the paths from S to v into a tree with S
as the root and copies of v as the leaves  so that we can do bottom-up modiﬁcation on this tree and
properly trace the reward changes based on the reward division we made among the paths.

|µ(cid:48)
i − µi|.

i

4.3 Discussions and Comparisons
We now discuss the implications of Theorem 1 together with Lemmas 1 and 2 by comparing them
with several existing results.

7

(cid:112)|E| in the dominant terms of distribution-dependent and -independent bounds  respectively  due

Comparison with [7] and CMAB with ∞-norm bounded smoothness conditions. Our work is
a direct adaption of the study in [7]. Comparing with [7]  we see that the regret bounds in Theorem 1
are not dependent on the inverse of triggering probabilities  which is the main issue in [7]. When
applied to inﬂuence maximization bandit  our result is strictly stronger than that of [7] in two aspects:
(a) we remove the factor of 1/p∗ by using the TPM condition; (b) we reduce a factor of |E| and
to our use of 1-norm instead of ∞-norm conditions used in Chen et al. [7]. In the supplementary
material  we further provide the corresponding ∞-norm TPM bounded smoothness conditions and
the regret bound results  since in general the two sets of results do not imply each other.

Comparison with [25] on inﬂuence maximization bandits. Conceptually  our work deals with
the general CMAB-T framework with inﬂuence maximization and combinatorial cascading bandits
as applications  while Wen et al. [25] only work on inﬂuence maximization bandit. Wen et al. [25]
further study a generalization of linear transformation of edge probabilities  which is orthogonal
to our current study  and could be potentially incorporated into the general CMAB-T framework.
Technically  both studies eliminate the exponential factor 1/p∗ in the regret bound. Comparing the
rest terms in the regret bounds  our regret bound depends on a topology dependent term ˜C (Lemma 2) 
while their bound depends on a complicated term C∗  which is related to both topology and edge
probabilities. Although in general it is hard to compare the regret bounds  for the several graph
families for which Wen et al. [25] provide concrete topology-dependent regret bounds  our bounds
k) to O(|V |)  where k is the number of seeds selected in each
are always better by a factor from O(
round and V is the node set in the graph. This indicates that  in terms of characterizing the topology
effect on the regret bound  our simple complexity term ˜C is more effective than their complicated
term C∗. See the supplementary material for the detailed table of comparison.

√

constant B = 1  achieving O((cid:80) 1
an extra factor of 1/f∗  where f∗ =(cid:81)

∆i

min

√
K log T ) distribution-dependent  and O(

Comparison with [16] on combinatorial cascading bandits By Lemma 1  we can apply The-
orem 1 to combinatorial conjunctive and disjunctive cascading bandits with bounded smoothness
mKT log T )
distribution-independent regret. In contrast  besides having exactly these terms  the results in [16] have
i∈S∗ (1− p(i))
for disjunctive cascades  with S∗ being the optimal solution and p(i) being the probability of success
for item (arm) i. For conjunctive cascades  f∗ could be reasonably close to 1 in practice as argued in
[16]  but for disjunctive cascades  f∗ could be exponentially small since items in optimal solutions
typically have large p(i) values. Therefore  our result completely removes the dependency on 1/f∗
and is better than their result. Moreover  we also have much smaller constant factors owing to the
new reverse amortization method described in Section 4.1.

i∈S∗ p(i) for conjunctive cascades  and f∗ =(cid:81)

Comparison with [15] on linear bandits. When there is no probabilistically triggered arms
(i.e. p∗ = 1)  Theorem 1 would have tighter bounds since some analysis dealing with probabilistic
triggering is not needed. In particular  in Eq. (1) the leading constant 624 would be reduced to 48  the
(cid:100)log2 x(cid:101)0 term is gone  and 6Bm becomes 2Bm; in Eq. (2) the leading constant 50 is reduced to 14 
and the other changes are the same as above (see the supplementary material). The result itself is also
a new contribution  since it generalizes the linear bandit of [15] to general 1-norm conditions with
matching regret bounds  while signiﬁcantly reducing the leading constants (their constants are 534
and 47 for distribution-dependent and independent bounds  respectively). This improvement comes
from the new reversed amortization method described in Section 4.1.
5 Lower Bound of the General CMAB-T Model
In this section  we show that there exists some CMAB-T problem instance such that the regret
distribution-independent bound are unavoidable  where p∗ is the minimum positive probability that
any base arm i is triggered by any action S. It also implies that the TPM bounded smoothness may
not be applied to all CMAB-T instances.
For our purpose  we only need a simpliﬁed version of the bounded smoothness condition of [7] as
below: There exists a bounded smoothness constant B such that  for every action S and every pair of
mean outcome vectors µ and µ(cid:48)  we have |rS(µ) − rS(µ(cid:48))| ≤ B maxi∈ ˜S |µi − µ(cid:48)
i|  where ˜S is the
set of arms that could possibly be triggered by S.

bound in [7] is tight  i.e. the factor 1/p∗ in the distribution-dependent bound and(cid:112)1/p∗ in the

8

RegA

µ (T ) ≥ 1
170

mT
p

.

problems. Comparing to the upper bound O((cid:112)p−1mT log T ). obtained from [7]  Theorem 2 implies

The proof of the above and the next theorem are all based on the results for the classical MAB

that the regret upper bound of CUCB in [7] is tight up to a O(
log T ) factor. This means that the
1/p∗ factor in the regret bound of [7] cannot be avoided in the general class of CMAB-T problems.
Next we give the distribution-dependent lower bound. For a learning algorithm  we say that it is
consistent if  for every µ  every non-optimal arm is played o(T a) times in expectation  for any real
number a > 0. Then we have the following distribution-dependent lower bound.
Theorem 3. For any consistent algorithm A running on instance FTP(p) and µi < 1 for every arm
i  we have

√

lim inf
T→+∞

µ (T )

RegA
ln T

p−1∆i
kl(µi  µ∗)

 

≥ (cid:88)

i:µi<µ∗

i

is observed  and the reward of playing Si is p−1X (t)

We prove the lower bounds using the following CMAB-T problem instance ([m] S D  Dtrig  R). For
each base arm i ∈ [m]  we deﬁne an action Si  with the set of actions S = {S1  . . .   Sm}. The family
of distributions D consists of distributions generated by every µ ∈ [0  1]m such that the arms are
independent Bernoulli variables. When playing action Si in round t  with a ﬁxed probability p  arm i
is triggered and its outcome X (t)
; otherwise
with probability 1− p no arm is triggered  no feedback is observed and the reward is 0. Following the
CMAB-T framework  this means that Dtrig(Si  X)  as a distribution on the subsets of [m]  is either
{i} with probability p or ∅ with probability 1− p  and the reward R(Si  X  τ ) = p−1Xi·I{τ = {i}}.
The expected reward rSi(µ) = µi. So this instance satisﬁes the above bounded smoothness with
constant B = 1. We denote the above instance as FTP(p)  standing for ﬁxed triggering probability
instance. This instance is similar with position-based model [17] with only one position  while the
feedback is different. For the FTP(p) instance  we have p∗ = p and rSi (µ) = p · p−1µi = µi.
log T ) and

Then applying the result in [7]  we have distributed-dependent upper bound O((cid:80)
distribution-independent upper bound O((cid:112)p−1mT log T ).

1
p∆i

min

i

i

We ﬁrst provide the distribution-independent lower bound result.
Theorem 2. Let p be a real number with 0 < p < 1. Then for any CMAB-T algorithm A  if
T ≥ 6p−1  there exists a CMAB-T environment instance D with mean µ such that on instance
FTP(p) 

(cid:115)

where µ∗ = maxi µi  ∆i = µ∗ − µi  and kl(· ·) is the Kullback-Leibler divergence function.
Again we see that the distribution-dependent upper bound obtained from [7] asymptotically match the
lower bound above. Finally  we remark that even if we rescale the reward from [1  1/p] back to [0  1] 
the corresponding scaling factor B would become p  and thus we would still obtain the conclusion
log T ) factor)  and thus 1/p∗ is in general needed in
that the regret bounds in [7] is tight (up to a O(
those bounds.

√

6 Conclusion and Future Work

In this paper  we propose the TPM bounded smoothness condition  which conveys the intuition that
an arm difﬁcult to trigger is also less important in determining the optimal solution. We show that this
condition is essential to guarantee low regret  and prove that important applications  such as inﬂuence
maximization bandits and combinatorial cascading bandits all satisfy this condition.
There are several directions one may further pursue. One is to improve the regret bound for some
speciﬁc problems. For example  for the inﬂuence maximization bandit  can we give a better algorithm
or analysis to achieve a better regret bound than the one provided by the general TPM condition?
Another direction is to look into other applications with probabilistically triggered arms that may not
satisfy the TPM condition or need other conditions to guarantee low regret. Combining the current
CMAB-T framework with the linear generalization as in [25] to achieve scalable learning result is
also an interesting direction.

9

Acknowledgment

Wei Chen is partially supported by the National Natural Science Foundation of China (Grant No.
61433014).

References
[1] Peter Auer  Nicolò Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002.

[2] Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM J. Comput.  32(1):48–77  2002.

[3] Donald A. Berry and Bert Fristedt. Bandit problems: Sequential Allocation of Experiments. Chapman and

Hall  1985.

[4] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122  2012.

[5] Wei Chen  Laks V. S. Lakshmanan  and Carlos Castillo. Information and Inﬂuence Propagation in Social

Networks. Morgan & Claypool Publishers  2013.

[6] Wei Chen  Wei Hu  Fu Li  Jian Li  Yu Liu  and Pinyan Lu. Combinatorial multi-armed bandit with general

reward functions. In NIPS  2016.

[7] Wei Chen  Yajun Wang  Yang Yuan  and Qinshi Wang. Combinatorial multi-armed bandit and its extension
to probabilistically triggered arms. Journal of Machine Learning Research  17(50):1–33  2016. A
preliminary version appeared as Chen  Wang  and Yuan  “combinatorial multi-armed bandit: General
framework  results and applications”  ICML’2013.

[8] Richard Combes  M. Sadegh Talebi  Alexandre Proutiere  and Marc Lelarge. Combinatorial bandits

revisited. In NIPS  2015.

[9] Yi Gai  Bhaskar Krishnamachari  and Rahul Jain. Combinatorial network optimization with unknown
variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions
on Networking  20  2012.

[10] Aditya Gopalan  Shie Mannor  and Yishay Mansour. Thompson sampling for complex online problems. In

Proceedings of the 31st International Conference on Machine Learning (ICML)  2014.

[11] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

Statistical Association  58(301):13–30  1963.

[12] David Kempe  Jon M. Kleinberg  and Éva Tardos. Maximizing the spread of inﬂuence through a social
network. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD)  pages 137–146  2003.

[13] Branislav Kveton  Zheng Wen  Azin Ashkan  Hoda Eydgahi  and Brian Eriksson. Matroid bandits: Fast
In Proceedings of the 30th Conference on Uncertainty in

combinatorial optimization with learning.
Artiﬁcial Intelligence (UAI)  2014.

[14] Branislav Kveton  Csaba Szepesvári  Zheng Wen  and Azin Ashkan. Cascading bandits: learning to rank
in the cascade model. In Proceedings of the 32th International Conference on Machine Learning  2015.

[15] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvári. Tight regret bounds for stochastic
combinatorial semi-bandits. In Proceedings of the 18th International Conference on Artiﬁcial Intelligence
and Statistics  2015.

[16] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvari. Combinatorial cascading bandits.

Advances in Neural Information Processing Systems  2015.

[17] Paul Lagrée  Claire Vernade  and Olivier Cappé. Multiple-play bandits in the position-based model. In

Advances in Neural Information Processing Systems  pages 1597–1605  2016.

[18] Siyu Lei  Silviu Maniu  Luyi Mo  Reynold Cheng  and Pierre Senellart. Online inﬂuence maximization. In

KDD  2015.

[19] Michael Mitzenmacher and Eli Upfal. Probability and Computing. Cambridge University Press  2005.

10

[20] Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin American Mathematical

Society  55:527–535  1952.

[21] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press  1998.

[22] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the

evidence of two samples. Biometrika  25(3/4):285–294  1933.

[23] Sharan Vaswani  Laks V. S. Lakshmanan  and Mark Schmidt. Inﬂuence maximization with bandits. In

NIPS Workshop on Networks in the Social and Information Sciences  2015.

[24] Sharan Vaswani  Branislav Kveton  Zheng Wen  Mohammad Ghavamzadeh  Laks V.S. Lakshmanan  and
Mark Schmidt. Diffusion independent semi-bandit inﬂuence maximization. In Proceedings of the 34th
International Conference on Machine Learning (ICML)  2017. to appear.

[25] Zheng Wen  Branislav Kveton  and Michal Valko. Inﬂuence maximization with semi-bandit feedback.

CoRR  abs/1605.06593v1  2016.

[26] Yu Yang  Xiangbo Mao  Jian Pei  and Xiaofei He. Continuous inﬂuence maximization: What discounts
In Proceedings of the 2016 International Conference on

should we offer to social network users?
Management of Data (SIGMOD)  2016.

11

,Sebastian Tschiatschek
Rishabh Iyer
Jeff Bilmes
Alaa Saade
Florent Krzakala
Lenka Zdeborová
Qinshi Wang
Wei Chen