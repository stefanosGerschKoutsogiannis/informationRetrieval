2016,Minimax Optimal Alternating Minimization for Kernel Nonparametric Tensor Learning,We investigate the statistical performance and computational efficiency of the  alternating minimization procedure for nonparametric tensor learning. Tensor modeling has been widely used for capturing the higher order relations between  multimodal data sources. In addition to a linear model   a nonlinear tensor model has been received much attention recently because of its high flexibility. We consider  an alternating minimization procedure for  a general nonlinear model where the true function  consists of components in a reproducing kernel Hilbert space (RKHS). In this paper  we show that the alternating minimization method achieves linear convergence as an optimization algorithm  and that the generalization error of the resultant estimator yields the minimax optimality. We apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.,Minimax Optimal Alternating Minimization
for Kernel Nonparametric Tensor Learning

Taiji Suzuki∗  Heishiro Kanagawa†

∗ †Department of Mathematical and Computing Science  Tokyo Institute of Technology

∗PRESTO  Japan Science and Technology Agency

∗Center for Advanced Integrated Intelligence Research  RIKEN

s-taiji@is.titech.ac.jp  kanagawa.h.ab@m.titech.ac.jp

Hayato Kobayash  Nobuyuki Shimizu  Yukihiro Tagami

Yahoo Japan Corporation

{ hakobaya  nobushim  yutagami } @yahoo-corp.jp

Abstract

We investigate the statistical performance and computational efﬁciency of the
alternating minimization procedure for nonparametric tensor learning. Tensor
modeling has been widely used for capturing the higher order relations between
multimodal data sources. In addition to a linear model  a nonlinear tensor model has
been received much attention recently because of its high ﬂexibility. We consider
an alternating minimization procedure for a general nonlinear model where the true
function consists of components in a reproducing kernel Hilbert space (RKHS).
In this paper  we show that the alternating minimization method achieves linear
convergence as an optimization algorithm and that the generalization error of the
resultant estimator yields the minimax optimality. We apply our algorithm to some
multitask learning problems and show that the method actually shows favorable
performances.

1

Introduction

Tensor modeling is widely used for capturing the higher order relations between several data sources.
For example  it has been applied to spatiotemporal data analysis [19]  multitask learning [20  2  14]
and collaborative ﬁltering [15]. The success of tensor modeling is usually based on the low-rank
property of the target parameter. As in the matrix  the low-rank decomposition of tensors  e.g. 
canonical polyadic (CP) decomposition [10  11] and Tucker decomposition [31]  reduces the effective
dimension of the statistical model  improves the generalization error  and gives a better understanding
of the model based on an condensed representation of the target system.
Among several tensor models  linear models have been extensively studied from both theoretical
and practical points of views [16]. A difﬁculty of the tensor model analysis is that typical tensor
analysis problems usually fall under a non-convex problem and it is difﬁcult to solve the problem.
To overcome the computational difﬁculty  several authors have proposed convex relaxation methods
[18  23  9  30  29]. Unfortunately  however  convex relaxation methods lose statistical optimality in
favor of computational efﬁciency [28].
Another promising approach is the alternating minimization procedure which alternately updates
each component of the tensor with the other ﬁxed components. The method has shown a nice
performance in practice. Moreover  its theoretical analysis has also been given by several authors
[1  13  6  3  21  36  27  37]. These theoretical analyses indicate that the estimator given by the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

alternating minimization procedure has a good generalization error  with a mild dependency on the
size of the tensor if the initial solution is properly set. In addition to the alternating minimization
procedure  it has been shown that the Bayes estimator achieves the minimax optimality under quite
weak assumptions [28].
Nonparametric models have also been proposed for capturing nonlinear relations [35  24  22]. In
particular  [24] extended the linear tensor learning to the nonparametric learning problem using
a kernel method and proposed a convex regularization method and an alternating minimization
method. Recently  [14  12] showed that the Bayesian approach has good theoretical properties for the
nonparametric problem. In particular  it achieves the minimax optimality under weak assumptions.
However  from a practical point of view  the Bayesian approach is computationally expensive
compared with the alternating minimization approach. An interesting observation is that the practical
performance of the alternating minimization procedure is quite good [24] and is comparable to the
Bayesian one [14]  although its computational efﬁciency is much better than that of the Bayesian
one. Despite the practical usefulness of the alternating minimization  its statistical properties have
not been investigated yet in the general nonparametric model.
In this paper  we theoretically analyze the alternating minimization procedure in the nonparametric
model. We investigate its computational efﬁciency and analyze its statistical performance. It is shown
that  if the true function is included in a reproducing kernel Hilbert space (RKHS)  then the algorithm
converges to an (a possibly local) optimal solution in linear rate  and the generalization error of the
estimator achieves the minimax optimality if the initial point of the algorithm is in the O(1) distance
from the true function. Roughly speaking  the theoretical analysis shows that

where (cid:98)f (t) is the estimated nonlinear tensor at the tth iteration of the alternating minimization

procedure  n is the sample size  d is the rank of the true tensor  K is the number of modes  and s is
the complexity of the RKHS. This indicates that the alternating minimization procedure can produce
a minimax optimal estimator after O(log(n)) iterations.

= Op

L2

(cid:107)(cid:98)f (t) − f∗(cid:107)2

(cid:0)dKn− 1

1+s log(dK) + dK (3/4)t(cid:1)

2 Problem setting: nonlinear tensor model

Here  we describe the model to be analyzed. Suppose that we are given n input-output pairs
{(xi  yi)}n
i=1 that are generated from the following system. The input xi is a concatenation of K
variables  i.e.  xi = (x(1)
is an element of a set
Xk and is generated from a distribution Pk. We consider the regression problem where the outputs
{yi}n

i=1 are observed according to the nonparametric tensor model [24]:

) ∈ X1 × ··· × XK = X   where each x(k)

 ···   x(K)

i

i

i

d(cid:88)

K(cid:89)

yi =

r=1

k=1

f∗
(r k)(x(k)

i

) + i 

(1)

r=1

(cid:81)K
k=1 f∗

i=1 represents an i.i.d. zero-mean noise and each f∗

f∗(x) = f∗(x(1)  . . .   x(K)) = (cid:80)d

where {i}n
(r k) is a component of the true function
included in some RKHS Hr k. In this regression problem  our objective is to estimate the true function
(r k)(x(k)) based on the observations {(xi  yi)}n
i=1.
This model has been applied to several problems such as multitask learning  recommendation system
and spatiotemporal data analysis. Although we focus on the squared loss regression problem  the
discussion in this paper can be easily generalized to Lipschitz continuous and strongly convex losses
as in [4].
Example 1: multitask learning Suppose that we have several tasks indexed by a two-dimensional
index (s  t) ∈ [M1] × [M2]1  and each task (s  t) is a regression problem for which there is a true
[s t](x) that takes an input feature w ∈ X3. The ith input sample is given as xi = (si  ti  wi) 
function g∗
which is a combination of task index (si  ti) and input feature wi. By assuming that the true function
g∗
[s t] is a linear combination of a few latent factors hr as

r=1 αs rβt rhr(w) (x = (s  t  w)) 

(2)

[s t](x) =(cid:80)d

g∗

1 We denote by [k] = {1  . . .   k}.

2

Algorithm 1 Alternating minimization procedure for nonlinear tensor estimation
Require: Training data Dn = {(xi  yi)}n

i=1  the regularization parameter λ(n)  iteration number T .

Ensure: (cid:98)f =(cid:80)d
Set ˜f(r k) = (cid:98)f (t−1)

for t = 1  . . .   T do

r

(cid:81)K
k=1 (cid:98)f (T )

(r k) as the estimator

r=1 ˆv(T )
(r k) (∀(r  k))  and ˜vr = ˆv(t−1)

r

(cid:104)

yi−(cid:16)

for (r  k) ∈ {1  . . .   d} × {1  . . .   K} do
(cid:89)
The (r  k)-element of ˜f is updated as

n(cid:88)

(cid:40)

(cid:48)
˜f
(r k) = argmin
f(r k)∈Hr k
(r k)(cid:107)n  ˜f(r k) ← ˜f(cid:48)

˜vr ← (cid:107) ˜f(cid:48)

end for

Set (cid:98)f (t)

(r k)/˜vr.
(r k) = ˜f(r k) (∀(r  k)) and ˆv(t)

f(r k)

1
n

i=1

k(cid:48)(cid:54)=k

r = ˜vr (∀r).

end for

(∀r).
(cid:88)

˜f(r k(cid:48)) +

˜vr(cid:48)

r(cid:48)(cid:54)=r

k(cid:48)=1

K(cid:89)

(cid:17)

(xi)

(cid:105)2

(cid:41)
+ Cn(cid:107)f(cid:107)2Hr k

.

(4)

˜f(r(cid:48) k(cid:48))

and the output is given as yi = g∗
learning problem for estimating {g∗
αs r  f(r 2)(t) = βt r  f(r 3)(w) = hr(w).

[si ti](xi) + i [20  2  14]  then we can reduce the multitask
[s t]}s t to the tensor estimation problem  where f(r 1)(s) =

3 Alternating regularized least squares algorithm

(cid:107)f(r k)(cid:107)2Hr k  

(3)

To learn the nonlinear tensor factorization model (1)  we propose to optimize the regularized empirical
risk in an alternating way. That is  we optimize each component f(r k) with the other ﬁxed components
{f(r(cid:48) k(cid:48))}(r(cid:48) k(cid:48))(cid:54)=(r k). Basically  we want to execute the following optimization problem:

d(cid:88)
where the ﬁrst term is the loss function for measuring how our guess(cid:80)d

{f(r k)}(r k):f(r k)∈Hr k

f(r k)(x(k)

n(cid:88)

K(cid:89)

+ Cn

min

1
n

k=1

r=1

r=1

i=1

)

i

d(cid:88)
(cid:81)K

k=1

(cid:32)
yi − d(cid:88)

(cid:33)2

r=1

k=1 f(r k) ﬁts the data
and the second term is a regularization term for controlling the complexity of the learning function.
However  this optimization problem is not convex and is difﬁcult to exactly solve.
We found that this computational difﬁculty could be overcome if we assume some additional assump-
tions and aim to achieve a better generalization error instead of exactly minimizing the training error.
The optimization procedure we discuss to obtain such an estimator is the alternating minimization
procedure  which minimizes the objective function (3) alternately with respect to each component
f(r k). For each component f(r k)  the objective function (3) is a convex function  and thus  it is easy
to obtain the optimal solution. Actually  the subproblem is reduced to a variant of the kernel ridge
regression  and the solution can be analytically obtained.
The algorithm we call alternating minimization procedure (AMP) is summarized in Algorithm 1.
After minimizing the objective (Eq. (4))  the obtained solution is normalized so that its empirical L2-
norm becomes 1 to adjust the scaling factor freedom. The parameter Cn in Eq. (4) is a regularization
parameter that is appropriately chosen.
For theoretical simplicity  we consider the following equivalent constraint formula instead of the
penalization one (4):

(cid:32)

(cid:40)

n(cid:88)

i=1

1
n

(r k) ∈
(cid:48)
˜f

argmin
f(r k)∈Hr k
(cid:107)f(r k)(cid:107)Hr k

≤ ˜R

yi − f(r k)(x(k)

i

)

˜f(r k(cid:48))(x(k(cid:48))

i

˜f(r(cid:48) k(cid:48))(x(k(cid:48))

i

)

) −(cid:88)

K(cid:89)

˜vr(cid:48)

r(cid:48)(cid:54)=r

k(cid:48)=1

(cid:33)2(cid:41)

(5)
where the parameter ˜R is a regularization parameter for controlling the complexity of the estimated
function.

(cid:89)

k(cid:48)(cid:54)=k

3

4 Assumptions and problem settings for the convergence analysis
Here  we prepare some assumptions for our theoretical analysis. First  we assume that the distribution
P (X) of the input feature x ∈ X is a product measure of Pk(X) on each Xk. That is  PX (dX) =
P1(dX1) × ··· × PK(dXK) for X = (X1  . . .   XK) ∈ X = X1 × ··· × XK. This is typically
assumed in the analysis of linear tensor estimation methods [13  6  3  21  1  36  27  37]. Thus  the

L2-norm of a “rank-1” function f (x) =(cid:81)K
Xk → R. The inner product in the space L2 is denoted by (cid:104)f  g(cid:105)L2 := (cid:82) f (X)g(X)dPX (X).
Note that because of the construction of PX   it holds that (cid:104)f  g(cid:105)L2 =(cid:81)K
f (x) =(cid:81)K
Let the magnitude of the rth component of the true function be vr := (cid:107)(cid:81)K

Hereafter  with a slight abuse of notations  we denote by (cid:107)f(cid:107)L2 = (cid:107)f(cid:107)L2(Pk) for a function f :
k=1(cid:104)fk  gk(cid:105)L2 for functions

Next  we assume that the norm of the true function is bounded away from zero and from above.
(r k)(cid:107)L2 and the

k=1 fk(x(k)) and g(x) =(cid:81)K

k=1 fk(x(k)) can be decomposed into
L2(P1) × ··· × (cid:107)fK(cid:107)2

k=1 gk(x(k)) where x = (x(1)  . . .   x(K)) ∈ X .

L2(PX ) = (cid:107)f1(cid:107)2

k=1 f∗

L2(PK ).

(cid:107)f(cid:107)2

(r k)/(cid:107)f∗

(r k) := f∗

normalized components be f∗∗
Assumption 1 (Boundedness Assumption).
(A1-1) There exist 0 < vmin ≤ vmax such that vmin ≤ vr ≤ vmax (∀r = 1  . . .   d).
(A1-2) The true function f∗

(r k) is included in the RKHS Hr k  i.e.  f∗

(r k)(cid:107)L2 (∀(r  k)).

there exists R > 0 such that max{vr  1}(cid:107)f∗∗

(r k)(cid:107)Hr k ≤ R (∀(r  k)).
function k(r k) associated with the RKHS Hr k

(A1-3) The kernel

supx∈Xk

k(r k)(x  x) ≤ 1 (∀(r  k)).

(A1-4) There exists L > 0 such that the noise is bounded as |i| ≤ L (a.s.).

(r k) ∈ Hr k (∀(r  k))  and

is bounded as

Assumption 1 is a standard one for the analysis of the tensor model and the kernel regression
model. Note that the boundedness condition of the kernel gives that (cid:107)f(cid:107)∞ = supx(k) |f (x(k))| ≤
(cid:107)f(cid:107)Hr k for all f ∈ Hr k because the Cauchy-Schwarz inequality gives |(cid:104)f  k(r k)(·  x(k))(cid:105)Hr k| ≤
(r k)(cid:107)∞ ≤ R.
k(r k)(x(k)  x(k))(cid:107)f(cid:107)Hr k for all x(k). Thus  combining with (A1-2)  we also have (cid:107)f∗∗
The last assumption (A1-4) is a bit restrictive. However  this assumption can be replaced with a
Gaussian assumption. In that situation  we may use the Gaussian concentration inequality [17] instead
of Talagrand’s concentration inequality in the proof.
Next  we characterize the complexity of each RKHS Hr k by using the entropy number [33  25].
The -covering number N ( G  L2(PX )) with respect to L2(PX ) is the minimal number of balls
with radius  measured by L2(PX ) needed to cover a set G ⊂ L2(PX ). The ith entropy number
ei(G  L2(PX )) is deﬁned as the inﬁmum of  > 0 such that N ( G  L2) ≤ 2i−1 [25]. Intuitively  if
the entropy number is small  the space G is “simple”; otherwise  it is “complicated.”
Assumption 2 (Complexity Assumption). Let BHr k be the unit ball of an RKHS Hr k. There exist
0 < s < 1 and c such that

ei(BHr k   L2(PX )) ≤ ci− 1
2s  

(6)

for all 1 ≤ r ≤ d and 1 ≤ k ≤ K.
The optimal rate of the ordinary kernel ridge regression on the RKHS with Assumption 2 is given as
n− 1
Assumption 3 (Inﬁnity Norm Assumption). There exist 0 < s2 ≤ 1 and c2 such that

1+s [26]. Next  we give a technical assumption about the L∞-norm.

(cid:107)f(cid:107)∞ ≤ c2(cid:107)f(cid:107)1−s2

L2

(cid:107)f(cid:107)s2Hr k

(∀f ∈ Hr k)

(7)

for all 1 ≤ r ≤ d and 1 ≤ k ≤ K.
By Assumption 1  this assumption is always satisﬁed for c2 = 1 and s2 = 1. s2 < 1 is a nontrivial
situation and gives a tighter bound. We would like to note that this condition with s2 < 1 is satisﬁed

4

by many practically used kernels such as the Gaussian kernel. In particular  it is satisﬁed if the kernel
is smooth so that Hr k is included in a Sobolev space W 2 s2[0  1]. More formal characterization of
this condition using the notion of a real interpolation space can be found in [26] and Proposition
2.10 of [5].
Finally  we assume an incoherence condition on {f∗
(r k)}r k. Roughly speaking  the incoherence
property of a set of functions {f(r k)}r k means that components {f(r k)}r are linearly independent
across different 1 ≤ r ≤ d on the same mode k. This is required to distinguish each component. An
analogous assumption has been assumed also in the literature of linear models [13  6  3  21  36  27].
Deﬁnition 1 (Incoherence). A set of functions {f(r k)}r k  where f(r k) ∈ L2(Pk)  is µ-incoherent if 
for all k = 1  . . .   K  it holds that

|(cid:104)f(r k)  f(r(cid:48) k)(cid:105)L2| ≤ µ(cid:107)f(r k)(cid:107)L2(cid:107)f(r(cid:48) k)(cid:107)L2 (∀r (cid:54)= r(cid:48)).

Assumption 4 (Incoherence Assumption). There exists 1 > µ∗ ≥ 0 such that the true function
{f∗
(r k)}r k is µ∗-incoherent.
5 Linear convergence of alternating minimization procedure
In this section  we give the convergence analysis of the AMP algorithm. Under the assumptions
presented in the previous section  it will be shown that the AMP algorithm shows linear convergence
in the sense of optimization algorithm and achieves the minimax optimal rate in the sense of statistical
performance. Roughly speaking  if the initial solution is sufﬁciently close to the true function (namely 
in a distance of O(1))  then the solution generated by AMP linearly converges to the optimal solution
and the estimation accuracy of the ﬁnal solution is given as O(dKn− 1
We analyze how close the updated estimator is to the true one when the (r  k)th component is
(r k). The tensor decomposition {f(r k)}r k of a nonlinear tensor model has a
updated from ˜f(r k) to ˜f(cid:48)
˜f(r(cid:48) k(cid:48))/(cid:107) ˜f(r(cid:48) k(cid:48))(cid:107)L2 (∀(r(cid:48)  k(cid:48)) ∈ [d] × [K]) and ¯vr(cid:48) = ˜vr(cid:48)(cid:81)K
freedom of scaling. Thus  we need to measure the accuracy based on a normalized representation to
avoid the scaling factor uncertainty. Let the normalized components of the estimator be ¯f(r(cid:48) k(cid:48)) =
k(cid:48)=1 (cid:107) ˜f(r(cid:48) k(cid:48))(cid:107)L2 (∀r(cid:48) ∈ [d]). On the
(cid:81)
(r k) (see Eq. (4)) and we denote by ¯v(cid:48)
(r k)(cid:107)L2
k(cid:48)(cid:54)=k (cid:107) ˜f(r k(cid:48))(cid:107)L2. The normalized newly
(r k)(cid:107)L2.

other hand  the newly updated (r  k)th element is denoted by ˜f(cid:48)
the updated value of ¯vr correspondingly: ¯v(cid:48)
updated element is denoted by ¯f(cid:48)
(r k) = ˜f(cid:48)
For an estimator ( ¯f   ¯v) = ({ ¯f(r(cid:48) k(cid:48))}r(cid:48) k(cid:48) {¯vr(cid:48)}r(cid:48)) which is a couple of the normalized component
and the scaling factor  deﬁne

1+s ) up to log(dK) factor.

r = (cid:107) ˜f(cid:48)
(r k)/(cid:107) ˜f(cid:48)

r

d∞( ¯f   ¯v) := max
(r(cid:48) k(cid:48))

{vr(cid:48)(cid:107) ¯f(r(cid:48) k(cid:48)) − f∗∗

(r(cid:48) k(cid:48))(cid:107)L2 + |vr(cid:48) − ¯vr(cid:48)|}.

For any λ1 n > 0 and λ2 n > 0 and τ > 0  we let aτ := max{1  L} max{1  τ} log(dK) and deﬁne
ξn = ξn(λ1 n  τ ) and ξ(cid:48)

(cid:18) K

n = ξ(cid:48)
n(λ2 n  τ ) as 2
− s
2 λ
1 n√
n

∨

1+2s

K

2

1+2s
1+s

2s+(1−s)s2

2(1+s)

λ

1 n

1

1+s

n

(cid:19)

ξn := aτ

(cid:18) λ

2

− s
2 n√
n

  ξ(cid:48)

n := aτ

∨

1

1
2

λ
2 nn

1

1+s

(cid:19)

.

Theorem 2. Suppose that Assumptions 1–4 are satisﬁed  and the regularization parameter ˜R in Eq.
(5) is set as ˜R = 2R. Let ˆR = 8 ˜R/ min{vmin  1} and suppose that we have already obtained an
estimator ˜f satisfying the following conditions:
• The RKHS-norms of { ¯f(r(cid:48) k(cid:48))}r(cid:48) k(cid:48) are bounded as (cid:107) ¯f(r(cid:48) k(cid:48))(cid:107)Hr(cid:48)  k(cid:48) ≤ ˆR/2 (∀(r(cid:48)  k(cid:48)) (cid:54)= (r  k)).
• The distance from the true one is bounded as d∞( ¯f   ¯v) ≤ γ.
Then  for a sufﬁciently small µ∗ and γ (independent of n)  there exists an event with probability
greater than 1 − 3 exp(−τ ) where any ( ¯f   ¯v) satisfying the above conditions gives
d∞( ¯f   ¯v)2 + Sn ˆR2K

r − vr|(cid:17)2 ≤ 1

(r k)(cid:107)L2 + |¯v(cid:48)

(r k) − f∗∗

vr(cid:107) ¯f(cid:48)

(cid:16)

(8)

2

2The symbol ∨ indicates the max operation  that is  a ∨ b := max{a  b}.

5

for any sufﬁciently large n  where Sn is deﬁned for a constant C(cid:48) depending on s  s2  c  c2 as

−1)(dξn)2/s2(1 + vmax)2(cid:105)

.

Sn := C(cid:48)(cid:104)

ξ(cid:48)
nλ1/2

2 n + ξ(cid:48)2

n + dξnλ1/2

1 n + ˆR2(K−1)( 1

s2

Moreover  if we denote by ηn the right hand side of Eq. (8)  then it holds that

(cid:107) ¯f(cid:48)

(r k)(cid:107)Hr k ≤

2

vr − √

ηn

˜R.

The proof and its detailed statement are given in the supplementary material (Theorem A.1). It
is proven by using such techniques as the so-called peeling device [32] or  equivalently  the local
Rademacher complexity [4]  and by combining these techniques with the coordinate descent opti-
mization argument. Theorem 2 states that  if the initial solution is sufﬁciently close to the true one 
then the following updated estimator gets closer to the true one and its RKHS-norm is still bounded
above by a constant. Importantly  it can be shown that the updated one still satisﬁes the conditions
of Theorem 2 for large n. Since the bound given in Theorem 2 is uniform  the inequality (8) can be

recursively applied to the sequence of (cid:98)f (t) (t = 1  2  . . . ).

By substituting λ1 n = K− 1+s

(cid:16)

1+s ∨(cid:16)

n

1−s n− 1
1−s d− 2
1+s−(1−s2) min{ 1−s
− 1

1+s and λ2 n = n− 1
s2(1+s)}

4(1+s)  

1

n− 1

Sn = O

1+s   we have that

(cid:17)(cid:17)

poly(d  K)

log(dK) 

where poly(d  K) means a polynomial of d  K. Thus  if s2 < 1 and n is sufﬁciently large compared
with d and K  then the second term is smaller than the ﬁrst term and we have Sn ≤ Cn− 1
1+s with a
constant C. Furthermore  we can bound the L2-norm from the true one as in the following theorem.

Theorem 3. Let ((cid:98)f (t)  ˆv(t)) be the estimator at the tth iteration. In addition to the assumptions of
Theorem 2  suppose that ((cid:98)f (1)  ˆv(1)) satisﬁes d∞((cid:98)f (1)  ˆv(1))2 ≤ v2
and n (cid:29) d  K  then ˇf (t)(x) =(cid:80)d

8 and Sn ˆR2K ≤ v2

8   s2 < 1

min

min

(cid:81)K
k=1 (cid:98)f (t)
(cid:16)
1+s log(dK) + dK (3/4)t(cid:17)

(r k)(x(k)) satisﬁes

dKn− 1

.

(cid:107) ˇf (t) − f∗(cid:107)2

L2

= O

r=1 ˆv(t)

r

for all t ≥ 2 uniformly with probability 1 − 3 exp(−τ ).
More detailed argument is given in Theorem A.3 in the supplementary material. This means that
after T = O(log(n)) iterations  we obtain the estimation accuracy of O(dKn− 1
1+s log(dK)). The
estimation accuracy bound O(dKn− 1
1+s log(dK)) is intuitively natural because we are estimating
d × K functions {f∗
(r k) is
known as n− 1
1+s [26]. Indeed  recently  it has been shown that this accuracy bound is minimax
optimal up to log(dK) factor [14]  that is 

(r k)}r k and the optimal sample complexity to estimate one function f∗

E[(cid:107)(cid:98)f − f∗(cid:107)2] (cid:38) dKn− 1

1+s

inf
ˆf

sup
f∗

(r k)(cid:107)Hr k ≤ R.
where inf is taken over all estimators and sup runs over all low rank tensors f∗ with (cid:107)f∗
The Bayes estimator also achieves this minimax lower bound [14]. Hence  a rough Bayes estimator
would be a good initial solution satisfying the assumptions.

6 Relation to existing works
In this section  we describe the relation of our work to existing works. First  our work can be
seen as a nonparametric extension of the linear parametric tensor model. The AMP algorithm
and related methods for the linear model has been extensively studied in the recent years  e.g.
[1  13  6  3  21  36  27  37]. Overall  the tensor completion problem has been mainly studied instead
of a general regression problem. Among the existing works  [37] analyzed the AMP algorithm for
a low-rank matrix estimation problem. It is shown that  under an incoherence condition  the AMP
algorithm converges to the optimal in a linear rate. However  their analysis is limited to a matrix
case. [1] analyzed an alternating minimization approach to estimate a low-rank tensor with positive
entries in a noisy observation setting. [13  6] considered an AMP algorithm for a tensor completion.

6

Their estimation method is close to our AMP algorithm. However  the analysis is for a linear tensor
completion with/without noise and is a different direction from our general nonparametric regression
setting. [3  36] proposed estimation methods other than an alternating minimization one  which were
specialized to a linear tensor completion problem.
As for the theoretical analysis for the nonparametric tensor regression model  some Bayes estimators
have been analyzed very recently by [14  12]. They analyzed Bayes methods with Gaussian process
priors and showed that the Gaussian process methods possess a good statistical performance. In
particular  [14] showed that the Gaussian process method for the nonlinear tensor estimation yields
the mini-max optimality as an extension of the linear model analysis [28]. However  the Bayes
estimators require posterior sampling such as Gibbs sampling  which is rather computationally
expensive. On the other hand  the AMP algorithm yields a linear convergence rate and satisﬁes
the minimax optimality. An interesting observation is that the AMP algorithm requires a stronger
assumption than the Bayesian one. There would be a trade-off between computational efﬁciency and
statistical property.
7 Numerical experiments
We numerically compare the following methods in multitask learning problems (Eq. (2)):
• Gaussian process method (GP-MTL) [14]: The nonparametric Bayesian method with Gaussian
process priors. It was shown that the generalization error of GP-MTL achieves the minimax
optimal rate [14].
• Our AMP method with different kernels for the latent factors hr (see Eq. (2)); the Gaussian RBF
kernel and the linear kernel. We also examined their mixture like 2 RBF kernels and 1 linear
kernel among d = 3 components. They are indicated as Lin(1)+RBF(2).

The tensor rank for AMP and GP-MTL was ﬁxed d = 3 in the following two data sets. The kernel
width and the regularization parameter were tuned by cross validation. We also examined the scaled
latent convex regularization method [34]. However  it did not perform well and was omitted.

7.1 Restaurant data

Here  we compared the methods in the Restaurant & Consumer Dataset [7]. The task was to
predict consumer ratings about several aspects of different restaurants  which is a typical task of a
recommendation system. The number of consumers was M1 = 138  and each consumer gave scores
of about M2 = 3 different aspects (food quality  service quality  and overall quality). Each restaurant
was described by M3 = 44 features as in [20]  and the task was to predict the score of an aspect
by a certain consumer based on the restaurant feature vector. This is a multitask learning problem
consisting of M1 × M2 = 414 (nonlinear) regression tasks where the input feature vector is M3 = 44
dimensional. The kernel function representing the task similarities among Task 1 (restaurant) and
Task 2 (aspect) are set as k(p  p(cid:48)) = δp p(cid:48) + 0.8 · (1 − δp p(cid:48)) (where the pair p  p(cid:48) are restaurants or
aspects) 3.
Fig. 1 shows the relative MSE (the discrepancy of MSE from the best one) for different training
sample sizes n computed on the validation data against the number of iterations t averaged over 10
repetitions. It can be seen that the validation error dropped rapidly to the optimal one. The best
achievable validation error depended on the sample size. An interesting observation was that  until
the algorithm converged to the best possible error  it dropped at a linear rate. After it reached the
bottom  the error was no longer improved.
Fig. 2 shows the performance comparison between the AMP method with different kernels and
the Gaussian process method (GP-MTL). The performances of AMP and GP-MTL were almost
identical. Although AMP is computationally quite efﬁcient  as shown in Fig. 1  it did not deteriorate
the statistical performance. This is a remarkable property of the AMP algorithm.

7.2 Online shopping data

Here  we compared our AMP method with the existing method using data of Yahoo! Japan shopping.
Yahoo! Japan shopping contains various types of shops. The dataset is built on the purchase history
3We also tested the delta kernel k(p  p(cid:48)) = δp p(cid:48)  but its performance was worse that the presented kernel.

7

Figure 1: Convergence property
of the AMP method:
relative
MSE against the number of it-
erations.

Figure 2: Comparison between
AMP method with different ker-
nels and GP-MTL on the restau-
rant data.

Figure 3: Comparison between
AMP and GP-MTL on the on-
line shopping data with different
kernels.

that describes how many times each consumer bought each product in each shop. Our objective was
to predict the quantity of a product purchased by a consumer at a speciﬁc shop. Each consumer was
described by 65 features based on his/her properties such as age  gender  and industry type of his/her
occupation. We executed the experiments on 100 items and 508 different shops. Hence  the problem
was reduced to a multitask learning problem consisting of 100 × 508 regression tasks.
Similarly to [14]  we put a commute-time kernel K = L† [8] on the shops based on a Laplacian matrix
L on a weighted graph constructed by two similarity measures between shops (where † denotes

psuedoinverse). Here  the Lapalacian on the graph is given by Li j = (cid:0)(cid:80)

(cid:1)δi j − wi j

j∈V wi j

where wi j is the similarity between shops (i  j). We employed the cosine similarity with different
parameters as the similarity measures (indicated by “cossim” and “cosdis”).
Based on the above settings  we performed a comparison between AMP and GP-MTL with different
similarity parameters. We used the Gaussian kernel for the latent factor hr. The result is shown in
Fig. 3  which presents the validation error (MSE) against the size of the training data. We can see that 
for both “cossim” and “cosdis ” AMP performed comparably well to the GP-MTL method and even
better than the GP-MTL method in some situations. Here it should be noted that AMP is much more
computationally efﬁcient than GP-MTL despite its high predictive performance. This experimental
result justiﬁes our theoretical analysis.
8 Conclusion
We have developed a convergence theory of the AMP method for the nonparametric tensor learning.
The AMP method has been used by several authors in the literature  but its theoretical analysis has
not been addressed in the nonparametric setting. We showed that the AMP algorithm converges in a
linear rate as an optimization algorithm and achieves the minimax optimal statistical error if the initial
point is in the O(1)-neighborhood of the true function. We may use the Bayes estimator as a rough
initial solution  but it would be an important future work to explore more sophisticated determination
of the initial solution.
Acknowledgment This work was partially supported by MEXT kakenhi (25730013  25120012 
26280009  15H01678 and 15H05707)  JST-PRESTO and JST-CREST.
References
[1] A. Aswani. Low-rank approximation and completion of positive tensors. arXiv:1412.0620  2014.
[2] M. T. Bahadori  Q. R. Yu  and Y. Liu. Fast multivariate spatio-temporal analysis via low rank tensor

learning. In Advances in Neural Information Processing Systems 27.

[3] B. Barak and A. Moitra. Tensor prediction  Rademacher complexity and random 3-xor. arXiv:1501.06521 

2015.

[4] P. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. The Annals of Statistics  33:

1487–1537  2005.

[5] C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press  Boston  1988.
[6] S. Bhojanapalli and S. Sanghavi. A new sampling technique for tensors. arXiv:1502.05023  2015.
[7] V.-G. Blanca  G.-S. Gabriel  and P.-M. Rafael. Effects of relevant contextual features in the performance
of a restaurant recommender system. In Proceedings of 3rd Workshop on Context-Aware Recommender
Systems  2011.

8

 8 9 10 11 12 13 4000 6000 8000 10000 12000 14000MSESample sizeGP-MTL(cosdis)GP-MTL(cossim)AMP(cosdis)AMP(cossim)[8] F. Fouss  A. Pirotte  J.-M. Renders  and M. Saerens. Random-walk computation of similarities between
nodes of a graph with application to collaborative recommendation. IEEE Trans. on Knowl. and Data Eng. 
19(3):355–369  Mar. 2007.

[9] S. Gandy  B. Recht  and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex

optimization. Inverse Problems  27:025010  2011.

[10] F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of Mathematics

and Physics  6:164–189  1927.

[11] F. L. Hitchcock. Multilple invariants and generalized rank of a p-way matrix or tensor. Journal of

Mathematics and Physics  7:39–79  1927.

[12] M. Imaizumi and K. Hayashi. Doubly decomposing nonparametric tensor regression. In International

Conference on Machine Learning (ICML2016)  page to appear  2016.

[13] P. Jain and S. Oh. Provable tensor factorization with missing data. In Advances in Neural Information

Processing Systems 27  pages 1431–1439. Curran Associates  Inc.  2014.

[14] H. Kanagawa  T. Suzuki  H. Kobayashi  N. Shimizu  and Y. Tagami. Gaussian process nonparametric tensor
estimator and its minimax optimality. In International Conference on Machine Learning (ICML2016) 
pages 1632–1641  2016.

[15] A. Karatzoglou  X. Amatriain  L. Baltrunas  and N. Oliver. Multiverse recommendation: N-dimensional
tensor factorization for context-aware collaborative ﬁltering. In Proceedings of the 4th ACM Conference
on Recommender Systems 2010  pages 79–86  2010.

[16] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Rev.  51(3):455–500  Aug.

2009.

[17] M. Ledoux. The concentration of measure phenomenon. Number 89 in Mathematical Surveys and

Monographs. American Mathematical Soc.  2005.

[18] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual data.
In Proceedings of the 12th International Conference on Computer Vision (ICCV)  pages 2114–2121  2009.
[19] M. Mørup. Applications of tensor (multiway array) factorizations and decompositions in data mining.

Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery  1(1):24–40  2011.

[20] B. Romera-Paredes  H. Aung  N. Bianchi-Berthouze  and M. Pontil. Multilinear multitask learning. In
Proceedings of the 30th International Conference on Machine Learning (ICML2013)  volume 28 of JMLR
Workshop and Conference Proceedings  pages 1444–1452  2013.

[21] P. Shah  N. Rao  and G. Tang. Optimal low-rank tensor recovery from separable measurements: Four

contractions sufﬁce. arXiv:1505.04085  2015.

[22] W. Shen and S. Ghosal. Adaptive Bayesian density regression for high-dimensional data. Bernoulli  22(1):

396–420  02 2016.

[23] M. Signoretto  L. D. Lathauwer  and J. Suykens. Nuclear norms for tensors and their use for convex

multilinear estimation. Technical Report 10-186  ESAT-SISTA  K.U.Leuven  2010.

[24] M. Signoretto  L. D. Lathauwer  and J. A. K. Suykens. Learning tensors in reproducing kernel Hilbert

spaces with multilinear spectral penalties. CoRR  abs/1310.4977  2013.

[25] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.
[26] I. Steinwart  D. Hush  and C. Scovel. Optimal rates for regularized least squares regression. In Proceedings

of the Annual Conference on Learning Theory  pages 79–93  2009.

[27] W. Sun  Z. Wang  H. Liu  and G. Cheng. Non-convex statistical optimization for sparse tensor graphical

model. In Advances in Neural Information Processing Systems  pages 1081–1089  2015.

[28] T. Suzuki. Convergence rate of Bayesian tensor estimator and its minimax optimality. In Proceedings of

the 32nd International Conference on Machine Learning (ICML2015)  pages 1273–1282  2015.

[29] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured schatten norm regularization. In

Advances in Neural Information Processing Systems 26  pages 1331–1339  2013. NIPS2013.

[30] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor decomposi-

tion. In Advances in Neural Information Processing Systems 24  pages 972–980  2011. NIPS2011.

[31] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika  31(3):279–311 

1966.

[32] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press  2000.
[33] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to

Statistics. Springer  New York  1996.

[34] K. Wimalawarne  M. Sugiyama  and R. Tomioka. Multitask learning meets tensor factorization: task
imputation via convex optimization. In Advances in Neural Information Processing Systems 27  pages
2825–2833. 2014. NIPS2014.

[35] Z. Xu  F. Yan  and Y. A. Qi. Inftucker: t-process based inﬁnite tensor decomposition. CoRR  abs/1108.6296 

2011.

[36] Z. Zhang and S. Aeron. Exact tensor completion using t-svd. arXiv:1502.04689  2015.
[37] T. Zhao  Z. Wang  and H. Liu. A nonconvex optimization framework for low rank matrix estimation. In
Advances in Neural Information Processing Systems 28  pages 559–567. Curran Associates  Inc.  2015.
NIPS2015.

9

,Niao He
Zaid Harchaoui
Taiji Suzuki
Heishiro Kanagawa
Hayato Kobayashi
Nobuyuki Shimizu
Yukihiro Tagami
Yi Zhou
Zhe Wang
Yingbin Liang