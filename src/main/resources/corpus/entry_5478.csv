2011,The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning,We derive an upper bound on the local Rademacher complexity of Lp-norm multiple kernel learning  which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p=1 only while our analysis covers all cases $1\leq p\leq\infty$  assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight  and derive consequences regarding excess loss  namely fast convergence rates of the order $O(n^{-\frac{\alpha}{1+\alpha}})$  where $\alpha$ is the minimum eigenvalue decay rate of the individual kernels.,The Local Rademacher Complexity of (cid:96)p-Norm

Multiple Kernel Learning

Marius Kloft∗

Machine Learning Laboratory

TU Berlin  Germany

kloft@tu-berlin.de

Gilles Blanchard

Department of Mathematics

University of Potsdam  Germany

gilles.blanchard@math.uni-potsdam.de

Abstract

We derive an upper bound on the local Rademacher complexity of (cid:96)p-norm mul-
tiple kernel learning  which yields a tighter excess risk bound than global ap-
proaches. Previous local approaches analyzed the case p = 1 only while our
analysis covers all cases 1 ≤ p ≤ ∞  assuming the different feature mappings
corresponding to the different kernels to be uncorrelated. We also show a lower
bound that shows that the bound is tight  and derive consequences regarding ex-
cess loss  namely fast convergence rates of the order O(n− α
1+α )  where α is the
minimum eigenvalue decay rate of the individual kernels.

1

Introduction

Kernel methods [24  21] allow to obtain nonlinear learning machines from simpler  linear ones;
nowadays they can almost completely be applied out-of-the-box [3]. Nevertheless  after more than
a decade of research it still remains an unsolved problem to ﬁnd the best abstraction or kernel for
a problem at hand. Most frequently  the kernel is selected from a candidate set according to its
generalization performance on a validation set. Clearly  the performance of such an algorithm is
limited by the best kernel in the set. Unfortunately  in the current state of research  there is little
hope that in the near future a machine will be able to automatically ﬁnd—or even engineer—the
best kernel for a particular problem at hand [25]. However  by restricting to a less general problem 
can we hope to achieve the automatic kernel selection?
In the seminal work of Lanckriet et al. [18] it was shown that learning a support vector machine
(SVM) [9] and a convex kernel combination at the same time is computationally feasible. This ap-
proach was entitled multiple kernel learning (MKL). Research in the subsequent years focused on
speeding up the initially demanding optimization algorithms [22  26]—ignoring the fact that empir-
ical evidence for the superiority of MKL over trivial baseline approaches (not optimizing the kernel)
was missing. In 2008  negative results concerning the accuracy of MKL in practical applications ac-
cumulated: at the NIPS 2008 MKL workshop [6] several researchers presented empirical evidence
showing that traditional MKL rarely helps in practice and frequently is outperformed by a reg-
ular SVM using a uniform kernel combination  see http://videolectures.net/lkasok08_
whistler/. Subsequent research (e.g.  [10]) revealed further negative evidence and peaked in the
provocative question “Can learning kernels help performance?” posed by Corinna Cortes in an
invited talk at ICML 2009 [5].
Consequently  despite all the substantial progress in the ﬁeld of MKL  there remained an unsatisﬁed
need for an approach that is really useful for practical applications: a model that has a good chance
of improving the accuracy (over a plain sum kernel). A ﬁrst step towards a model of kernel learning
∗Marius Kloft is also with Friedrich Miescher Laboratory  Max Planck Society  T¨ubingen. A part of this
work was done while Marius Kloft was with UC Berkeley  USA  and Gilles Blanchard was with Weierstraß In-
stitute for Applied Analysis and Stochastics  Berlin.

1

Figure 1: Result of a typical (cid:96)p-norm MKL experiment in terms of accuracy (LEFT) and kernel weights output
by MKL (RIGHT).

m=1 θmkm

that is useful for practical applications was made in [7  13  14]: by imposing an (cid:96)q-norm penalty
(q > 1) rather than an (cid:96)1-norm one on the kernel combination coefﬁcients. This (cid:96)q-norm MKL is
an empirical minimization algorithm that operates on the multi-kernel class consisting of functions
f : x (cid:55)→ (cid:104)w  φk(x)(cid:105) with (cid:107)w(cid:107)k ≤ D  where φk is the kernel mapping into the reproducing kernel
Hilbert space (RKHS) Hk with kernel k and norm (cid:107).(cid:107)k  while the kernel k itself ranges over the

(cid:12)(cid:12)(cid:12) (cid:107)θ(cid:107)q ≤ 1  θ ≥ 0(cid:9). A conceptual milestone going

set of possible kernels(cid:8)k = (cid:80)M

back to the work of [1] and [20] is that this multi-kernel class can equivalently be represented as a
block-norm regularized linear class in the product RKHS:

Hp D M =(cid:8)fw : x (cid:55)→ (cid:104)w  φ(x)(cid:105) (cid:12)(cid:12) w = (w(1)  . . .   w(M )) (cid:107)w(cid:107)2 p ≤ D(cid:9) 

where there is a one-to-one mapping of q ∈ [1 ∞] to p ∈ [1  2] given by p = 2q
q+1.
In Figure 1  we show exemplary results of an (cid:96)p-norm MKL experiment  achieved on the protein
fold prediction dataset used in [4] (see supplementary material A for experimental details). We ﬁrst
observe that  as expected  (cid:96)p-norm MKL enforces strong sparsity in the coefﬁcients θm when p = 1
and no sparsity at all otherwise (but various degrees of soft sparsity for intermediate p). Crucially 
the performance (as measured by the test error) is not monotonic as a function of p; p = 1 (sparse
MKL) yields the same performance as the regular SVM using a uniform kernel combination  but
optimal performance is attained for some intermediate value of p—namely  p = 1.14. This is a
strong empirical motivation to study theoretically the performance of (cid:96)p-MKL beyond the limiting
cases p = 1 and p = ∞.
Clearly  the complexity of (1) will be greater than one that is based on a single kernel only. However 
it is unclear whether the increase is decent or considerably high and—since there is a free parameter
p—how this relates to the choice of p. To this end  the main aim of this paper is to analyze the sample
complexity of the hypothesis class (1). An analysis of this model  based on global Rademacher
complexities  was developed by [8] for special cases of p. In the present work  we base our main
local Rademacher complexities  which allows to derive improved and
analysis on the theory of
more precise rates of convergence that cover the whole range of p ∈ [1 ∞].
Outline of the contributions. This paper makes the following contributions:

(1)

(cid:0) 1
p∗ −1(cid:1)

• An upper bound on the local Rademacher complexity of (cid:96)p-norm MKL is shown  from
which we derive an excess risk bound that achieves a fast convergence rate of the order
O(M 1+ 2
1+α )  where α is the minimum eigenvalue decay rate of the individ-
ual kernels (previous bounds for (cid:96)p-norm MKL only achieved O(M

• A lower bound is shown that beside absolute constants matches the upper bounds  showing

p∗ n− 1
2 ).

1

1+α

n− α

that our results are tight.

• The generalization performance of (cid:96)p-norm MKL as guaranteed by the excess risk bound
is studied for varying values of p  shedding light on the appropriateness of a small/large p
in various learning scenarios.

Furthermore  we also present a simpler  more general proof of the global Rademacher bound shown
in [8] (at the expense of a slightly worse constant). A comparison of the rates obtained with local
and global Rademacher analysis is carried out in Section 3.

2

CHPZSVL1L4L14L30SW1SW200.10.20.30.40.50.60.70.8Test Set Accuracy SVM (single)1−norm MKL1.07−norm MKL1.14−norm MKL1.33−norm MKLSVM (all)CHPZSVL1L4L14L30SW1SW200.20.40.60.811.21.4Kernel Weights θi 1−norm MKL1.07−norm MKL1.14−norm MKL1.33−norm MKLSVMn k(xi  xj). Also  we denote u = (u(m))M

Notation. We abbreviate Hp = Hp D = Hp D M if clear from the context. We denote the (normal-
ized) kernel matrices corresponding to k and km by K and Km  respectively  i.e.  the ijth entry of
m=1 = (u(1)  . . .   u(M )) ∈ H = H1 × . . . × HM .
K is 1
Furthermore  let P be a probability measure on X i.i.d. generating the data x1  . . .   xn and denote
by E the corresponding expectation operator. We work with operators in Hilbert spaces and will use
instead of the usual vector/matrix notation φ(x)φ(x)(cid:62) the tensor notation φ(x) ⊗ φ(x) ∈ HS(H) 
which is a Hilbert-Schmidt operator H (cid:55)→ H deﬁned as (φ(x) ⊗ φ(x))u = (cid:104)φ(x)  u(cid:105) φ(x). The
space HS(H) of Hilbert-Schmidt operators on H is itself a Hilbert space  and the expectation
Eφ(x) ⊗ φ(x) is well-deﬁned and belongs to HS(H) as soon as E(cid:107)φ(x)(cid:107)2 is ﬁnite  which will
always be assumed. We denote by J = Eφ(x) ⊗ φ(x) and Jm = Eφm(x) ⊗ φm(x) the uncen-
tered covariance operators corresponding to variables φ(x) and φm(x)  respectively; it holds that
tr(J) = E(cid:107)φ(x)(cid:107)2

2 and tr(Jm) = E(cid:107)φm(x)(cid:107)2
2.

n

(cid:80)n
Global Rademacher Complexities We ﬁrst review global Rademacher complexities (GRC) in
multiple kernel learning. Let x1  . . .   xn be an i.i.d. sample drawn from P . The global Rademacher
complexity is deﬁned as R(Hp) = E supfw∈Hp(cid:104)w  1
i=1 σiφ(xi)(cid:105)  where (σi)1≤i≤n is an i.i.d.
denoted by (cid:98)R(Hp) = E(cid:2)R(Hp)(cid:12)(cid:12)x1  . . .   xn
(cid:80)n
family (independent of φ(xi)) of Rademacher variables (random signs). Its empirical counterpart is
(cid:0)cp∗(cid:13)(cid:13)(cid:0) tr(Km)(cid:1)M
i=1 σiφ(xi)(cid:105).
In the recent paper of [8] it was shown (cid:98)R(Hp) ≤ D

(cid:3) = Eσ supfw∈Hp(cid:104)w  1

(cid:1)1/2 for p ∈ [1  2]

and p∗ being an integer (where c = 23/44 and p∗ := p
p−1 is the conjugated exponent). This bound
is tight and improves a series of loose results that were given for p = 1 in the past (see [8] and
references therein). In fact  the above result can be extended to the whole range of p ∈ [1 ∞] (in
the supplementary material we present a quite simple proof using c = 1):
Proposition 1 (GLOBAL RADEMACHER COMPLEXITY BOUND). For any p ≥ 1 the empirical
version of global Rademacher complexity of the multi-kernel class Hp can be bounded as

(cid:13)(cid:13) p∗

m=1

n

n

2

(cid:98)R(Hp) ≤ min

t∈[p ∞]

D

(cid:115)

(cid:13)(cid:13)(cid:13)(cid:16) 1

n

t∗
n

(cid:17)M

(cid:13)(cid:13)(cid:13) t∗

2

.

tr(Km)

m=1

Interestingly  the above GRC bound is not monotonic in p and thus the minimum is not always
attained for t := p.

2 The Local Rademacher Complexity of Multiple Kernel Learning

(cid:80)n
i=1 σiφ(xi)(cid:105)  where P f 2

n

sample drawn from P . We deﬁne the local Rademacher com-
Let x1  . . .   xn be an i.i.d.
w≤r(cid:104)w  1
plexity (LRC) of Hp as Rr(Hp) = E supfw∈Hp:P f 2
w :=
E(fw(φ(x)))2. Note that it subsumes the global RC as a special case for r = ∞. We will also
use the following assumption in the bounds for the case p ∈ [1  2]:
Assumption (U) (no-correlation). Let x ∼ P . The Hilbert space valued random variables
φ1(x)  . . .   φM (x) are said to be (pairwise) uncorrelated if for any m (cid:54)= m(cid:48) and w ∈ Hm   w(cid:48) ∈
Hm(cid:48)   the real variables (cid:104)w  φm(x)(cid:105) and (cid:104)w(cid:48)  φm(cid:48)(x)(cid:105) are uncorrelated.
For example  if X = RM   the above means that the input variable x ∈ X has independent co-
ordinates  and the kernels k1  . . .   kM each act on a different coordinate. Such a setting was also
considered by [23] (for sparse MKL). To state the bounds  note that covariance operators enjoy
j=1 λjuj ⊗ uj and
)j≥1 form orthonormal
bases of H and Hm  respectively. We are now equipped to state our main results:
Theorem 2 (LOCAL RADEMACHER COMPLEXITY BOUND  p ∈ [1  2] ). Assume that the kernels
are uniformly bounded ((cid:107)k(cid:107)∞ ≤ B < ∞) and that Assumption (U) holds. The local Rademacher
complexity of the multi-kernel class Hp can be bounded for any p ∈ [1  2] as

discrete eigenvalue-eigenvector decompositions J = Eφ(x) ⊗ φ(x) = (cid:80)∞
Jm = Ex(m) ⊗ x(m) =(cid:80)∞
(cid:118)(cid:117)(cid:117)(cid:116) 16
(cid:13)(cid:13)(cid:13)(cid:13)(cid:18) ∞(cid:88)

Rr(Hp) ≤ min
t∈[p 2]

BeDM 1

t∗ t∗

n

.

  where (uj)j≥1 and (u(m)

t∗   ceD2t∗2λ(m)

j ⊗ u(m)

j=1 λ(m)

j u(m)

(cid:17)(cid:19)M

(cid:13)(cid:13)(cid:13)(cid:13) t∗

rM 1− 2

(cid:16)

min

√

+

n

j

j

j

j=1

m=1

2

3

Rr(Hp) ≤ min
t∈[p ∞]

Theorem 3 (LOCAL RADEMACHER COMPLEXITY BOUND  p ∈ [2 ∞] ). For any p ∈ [2 ∞] 

It is interesting to compare the above bounds for the special case p = 2 with the ones of Bartlett et al.
[2]. The main term of the bound of Theorem 3 (taking t = p = 2) is then essentially determined by

j=1 min(r  λ(m)

j

n

m=1

(cid:80)M

O(cid:0)(cid:0) 1
(cid:80)∞
is equivalently of order O(cid:0)(cid:0) 1
(cid:8)λ(m)
{λi  i ≥ 1} =(cid:83)M
(cid:13)(cid:13)(cid:0)(cid:80)∞
bound Rr(H1) ≤ (cid:0) 16

m=1

i

n

j=1

∞(cid:88)

t∗ −1λj).

min(r  D2M 2

(cid:118)(cid:117)(cid:117)(cid:116) 2
)(cid:1)1/2(cid:1). If the variables (φm(x)) are centered and uncorrelated  this
j=1 min(r  λj)(cid:1)1/2(cid:1) because spec(J) =(cid:83)M
(cid:80)∞
(cid:13)(cid:13)∞
j=1 min(cid:0)rM  e3D2(log M )2λ(m)
(cid:1)1/2

m=1 spec(Jm); that is 
  i ≥ 1}; this rate is also what we would obtain through Theorem 3  so

(cid:1)(cid:1)M

3
2 D log(M )

m=1

√

+

Be

n

n

n

both bounds on the LRC recover the rate shown in [2] for the special case p = 2.
It is also interesting to study the case p = 1: by using t = (log(M ))∗ in Theorem 2  we obtain the
  for
all M ≥ e2. We now turn to proving Theorem 2. the proof of Theorem 3 is straightforward and
shown in the supplementary material C.
Proof of Theorem 2. . Note that it sufﬁces to prove the result for t = p as trivially (cid:107)w(cid:107)2 t ≤ (cid:107)w(cid:107)2 p
holds for all t ≥ p so that Hp ⊆ Ht and therefore Rr(Hp) ≤ Rr(Ht).
the no-correlation assumption  we will work in large parts of the proof with the centered class (cid:102)Hp =
(cid:12)(cid:12) (cid:107)w(cid:107)2 p ≤ D(cid:9)  wherein (cid:101)fw : x (cid:55)→ (cid:104)w (cid:101)φ(x)(cid:105)  and(cid:101)φ(x) := φ(x) − Eφ(x). We start the proof
(cid:8)(cid:101)fw
STEP 1: RELATING THE ORIGINAL CLASS WITH THE CENTERED CLASS.
by noting that (cid:101)fw(x) = fw(x) − (cid:104)w  Eφ(x)(cid:105) = fw(x) − E(cid:104)w  φ(x)(cid:105) = fw(φ(x)) − Efw(φ(x)) 

In order to exploit

j

2

p∗

2

p∗

=

m=1

P f 2

Jensen≤

m=1
p∗

Furthermore we note that by Jensen’s inequality

so that  by the bias-variance decomposition  it holds that

(cid:1)2
w + (cid:0)P fw
w = Efw(x)2 = E (fw(x) − Efw(x))2 + (Efw(φ(x)))2 = P(cid:101)f 2
(cid:19) 1
(cid:13)(cid:13)Eφ(x)(cid:13)(cid:13)2 p∗ =
(cid:10)Eφm(x)  Eφm(x)(cid:11) p∗
(cid:115)(cid:13)(cid:13)(cid:13)(cid:16)
(cid:13)(cid:13)(cid:13) p∗
(cid:17)M
n(cid:88)
σiEφ(x)(cid:11) .
(cid:10)w 
Concerning the ﬁrst term of the above upper bound  using (2) we have P(cid:101)f 2
σi(cid:101)φ(xi)(cid:11) = Rr((cid:101)Hp).

(cid:19) 1
(cid:18) M(cid:88)
(cid:18) M(cid:88)
(cid:13)(cid:13)Eφm(x)(cid:13)(cid:13)p∗
(cid:19) 1
(cid:18) M(cid:88)
E(cid:10)φm(x)  φm(x)(cid:11) p∗
n(cid:88)
(cid:10)w 
σi(cid:101)φ(xi)(cid:11) + E sup
n(cid:88)
n(cid:88)
(cid:10)w 
σi(cid:101)φ(xi)(cid:11) ≤ E sup
P(cid:101)f 2
fw∈Hp 
w≤r
n(cid:88)
σiEφ(x)(cid:11) ≤ √
(cid:10)w 

Rr(Hp) ≤ E sup
fw∈Hp 
w≤r
P f 2

Now to bound the second term  we write

E sup
fw∈Hp 
w≤r
P f 2

(cid:104)w  Eφ(x)(cid:105) .

w ≤ P f 2

(cid:10)w 

fw∈Hp 
w≤r
P f 2

tr(Jm)

1
n

1
n

1
n

1
n

m=1

m=1

i=1

i=1

i=1

i=1

=

2

2

E sup
fw∈Hp 
w≤r
P f 2
Now observe that we have

1
n

i=1

w  and thus

n sup
fw∈Hp 
w≤r
P f 2

(cid:114)(cid:13)(cid:13)(cid:0) tr(Jm)(cid:1)M
(cid:114)(cid:13)(cid:13)(cid:0) tr(Jm)(cid:1)M
(cid:13)(cid:13) p∗

m=1

2

m=1

(cid:17)

(cid:13)(cid:13) p∗

2

as well as (cid:104)w  Eφ(x)(cid:105) = Efw(x) ≤(cid:112)P f 2
(3)≤ (cid:107)w(cid:107)2 p
(cid:104)w  Eφ(x)(cid:105) H¨older≤ (cid:107)w(cid:107)2 p (cid:107)Eφ(x)(cid:107)2 p∗
(cid:16)√
Rr(Hp) ≤ Rr((cid:101)Hp) + n− 1

2 min

r  D

w . We ﬁnally obtain  putting together the steps above 

.

(4)

This shows that there is no loss in working with the centered class instead of the uncentered one.

4

so that we can express the complexity of the centered class in terms of the uncentered one as follows:

.

(2)

(3)

j

STEP 2: BOUNDING THE COMPLEXITY OF THE CENTERED CLASS.
In this step of the proof
we generalize the technique of [19] to multi-kernel classes. First we note that  since the (centered)

covariance operator E(cid:101)φm(x) ⊗(cid:101)φm(x) is also a self-adjoint Hilbert-Schmidt operator on Hm  there
exists an eigendecomposition E(cid:101)φm(x) ⊗(cid:101)φm(x) =(cid:80)∞
j=1(cid:101)λ(m)
(cid:101)u(m)
j ⊗(cid:101)u(m)
  wherein ((cid:101)u(m)
is an orthogonal basis of Hm. Furthermore  the no-correlation assumption (U) entails E(cid:101)φl(x) ⊗
(cid:101)φm(x) = 0 for all l (cid:54)= m. As a consequence  for all j and m 
(cid:10)wm (cid:101)φm(x)(cid:11)(cid:17)2
w = E((cid:101)fw(x))2 = E(cid:16) M(cid:88)
(cid:68)
(cid:69)2
M(cid:88)
∞(cid:88)
P(cid:101)f 2
(cid:101)λ(m)
wm (cid:101)u(m)
(cid:16) 1
(cid:68)(cid:101)u(m)
(cid:69)
(cid:17)(cid:101)u(m)
E(cid:68) 1
(cid:101)λ(m)
n(cid:88)
n(cid:88)
E(cid:101)φm(xi) ⊗(cid:101)φm(xi)
σi(cid:101)φm(xi) (cid:101)u(m)
Rr((cid:101)Hp)

Let now h1  . . .   hM be arbitrary nonnegative integers. We can express the LRC in terms of the
eigendecompositon as follows

m=1
1
n

(cid:69)2

)j≥1

(6)

(5)

j
n

m=1

j=1

i=1

i=1

=

=

=

n

n

.

 

j

j

j

j

j

j

j

w 

(cid:68)
(cid:34)(cid:118)(cid:117)(cid:117)(cid:116) M(cid:88)
hm(cid:88)
(cid:28)

sup

= E

fw∈(cid:101)Hp:P(cid:101)f 2
w≤r
C.-S.  Jensen≤
P(cid:101)f 2
sup
w≤r
+ E sup
fw∈(cid:101)Hp

m=1

j=1

w 

1
n

i=1

(cid:69)
n(cid:88)
σi(cid:101)φ(xi)
(cid:101)λ(m)
(cid:104)w(m) (cid:101)u(m)
(cid:16) ∞(cid:88)
n(cid:88)

j

j

(cid:104) 1
n

j=hm+1

i=1

so that (5) and (6) yield

Rr((cid:101)Hp)

(5)  (6) H¨older≤

(cid:115)

r(cid:80)M

m=1 hm
n

+ D E

= E

j

m=1

(cid:105)2

sup

(cid:68)(cid:0)w(m)(cid:1)M
fw∈(cid:101)Hp:P(cid:101)f 2
(cid:118)(cid:117)(cid:117)(cid:116) M(cid:88)
(cid:16)(cid:101)λ(m)
(cid:17)−1 E(cid:10) 1
hm(cid:88)
w≤r
(cid:29)
(cid:17)M
σi(cid:101)φm(xi) (cid:101)u(m)
(cid:105)(cid:101)u(m)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) ∞(cid:88)

n(cid:88)

(cid:104) 1
n

m=1

j=1

n

j

j

j=hm+1

i=1

(cid:69)

m=1 (cid:0) 1
n(cid:88)

n(cid:88)
σi(cid:101)φm(xi)(cid:1)M
(cid:35)
(cid:11)2
σi(cid:101)φm(xi) (cid:101)u(m)

i=1

n

j

i=1

m=1

σi(cid:101)φm(xi) (cid:101)u(m)

j

(cid:17)M

m=1

(cid:13)(cid:13)(cid:13)(cid:13)2 p∗

.

(cid:105)(cid:101)u(m)

j

E(cid:16) ∞(cid:88)

n

≤ (cid:113) p∗
(cid:18) M(cid:88)

(cid:16)(cid:80)M

E(cid:16)(cid:80)

STEP 3: KHINTCHINE-KAHANE’S AND ROSENTHAL’S INEQUALITIES. We use the Khintchine-
Kahane (K.-K.) inequality (see Lemma B.2 in the supplementary material) to further bound the right
term in the above expression as E

(cid:104) 1

(cid:17)M

(cid:105)(cid:101)u(m)

j

m=1

(cid:13)(cid:13)(cid:13)2 p∗

m=1

. Note that for p ≥ 2 it holds that
p∗/2 ≤ 1  and thus it sufﬁces to employ Jensen’s inequality once again to move the expectation
operator inside the inner term. In the general case we need a handle on the p∗
2 -th moments and to
this end employ Lemma C.1 (Rosenthal + Young; see supplementary material)  which yields

j>hm

1
n

j

j

n

p∗

j>hm

(cid:13)(cid:13)(cid:13)(cid:16)(cid:80)
(cid:80)n
i=1 σi(cid:101)φm(xi) (cid:101)u(m)
(cid:105)2(cid:17) p∗
2 (cid:17) 1
(cid:80)n
i=1(cid:104)(cid:101)φm(xi) (cid:101)u(m)
2 (cid:19) 1
(cid:105)2(cid:17) p∗
(cid:104)(cid:101)φm(xi) (cid:101)u(m)
(cid:18)(cid:16) B
(cid:16) ∞(cid:88)
(cid:17) p∗
(cid:18) M(cid:88)
(cid:16) ∞(cid:88)
(cid:101)λ(m)

(ep∗)

+

+

p∗

j

2

2

2

i=1

1
n

n(cid:88)
(cid:32) M(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)ep∗
(cid:32)

m=1

p∗

m=1

R+Y≤

j=hm+1

n(cid:88)
E(cid:104)(cid:101)φm(xi) (cid:101)u(m)
p∗(cid:33)
2 (cid:19) 2
(cid:17) p∗
where for (∗) we used the subadditivity of p∗√·. Note that ∀j  m : (cid:101)λ(m)
Mirsky-Wielandt theorem since Eφm(x)⊗ φm(x) = E(cid:101)φm(x)⊗(cid:101)φm(x) +Eφm(x)⊗Eφm(x). Thus

j ≤ λ(m)

by the Lidskii-

BM
n

j=hm+1

j=hm+1

(∗)≤

1
n

m=1

i=1

p∗

p∗

n

j

j

j

2 (cid:19)(cid:33) 1
(cid:105)2(cid:17) p∗

5

n

≤

j=hm+1

j=hm+1

+

n

+ D

n

√

+

.

(7)

2

p∗

+

m=1

2

m=1

2

λ(m)
j

λ(m)
j

(cid:33)

1

p∗ p∗

BeDM

BM
n

(cid:19)M

m=1 hm
n

m=1 hm
n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) p∗

by the subadditivity of the root function

Rr((cid:101)Hp) ≤
(cid:115)

(cid:115)
r(cid:80)M
r(cid:80)M

(cid:118)(cid:117)(cid:117)(cid:116) ep∗2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:32)
(cid:18) ∞(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) ep∗2D2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) p∗
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:18) ∞(cid:88)
(cid:19)M
(cid:1)1/2(cid:17) ≤
r  D(cid:0)(cid:13)(cid:13)(cid:0) tr(Jm)(cid:1)M
2 min(cid:0)√
(cid:13)(cid:13)(cid:0)(cid:80)∞
(cid:13)(cid:13) p∗
(cid:1)M
(cid:1)1/2
(cid:0) ep∗ 2D2
r  D(cid:0)(cid:13)(cid:13)(cid:0) tr(Jm)(cid:1)M
(cid:13)(cid:13) p∗
m=1 hm/n(cid:1)1/2 (in case that at least one
2 min(cid:0)√
(cid:1)1/2(cid:1) ≤ (cid:0)r(cid:80)M
r  D(cid:0)(cid:13)(cid:13)(cid:0) tr(Jm)(cid:1)M
(cid:1)1/2(cid:1) ≤
2 min(cid:0)√
(cid:13)(cid:13)(cid:0)(cid:80)∞
(cid:13)(cid:13) p∗
(cid:0) r(cid:80)M
+(cid:0) ep∗ 2D2
(cid:1)M
(cid:1)1/2
(cid:1)1/2
(cid:115)
(cid:118)(cid:117)(cid:117)(cid:116) 4ep∗2D2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) p∗
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
4r(cid:80)M
(cid:19)M
(cid:18) ∞(cid:88)

STEP 4: BOUNDING THE COMPLEXITY OF THE ORIGINAL CLASS.
all nonnegative integers hm we either have n− 1
case

(8)
for all nonnegative integers hm ≥ 0. Later  we will use the above bound (8) for the computation of
the excess loss; however  to gain more insight in the bounds’ properties  we express it in terms of
the truncated spectra of the kernels at the scale r as follows:

n− 1
hm is nonzero) so that

. Thus the following preliminary bound

in any case we get n− 1

n
follows from (4) by (7):

(cid:13)(cid:13) p∗
(cid:13)(cid:13) p∗

Now note that for

j=hm+1 λ(m)

j=hm+1 λ(m)

Rr(Hp) ≤

m=1 hm
n

m=1
or

hm are

BeDM

1

p∗ p∗

λ(m)
j

m=1

2

m=1

2

m=1

2

m=1

2

it

holds

m=1 hm

n

m=1

2

j

n

zero)

√

+

(in

all

 

n

n

j

j=hm+1

+

2

STEP 5: RELATING THE BOUND TO THE TRUNCATION OF THE SPECTRA OF THE KERNELS.
Next  we notice that for all nonnegative real numbers A1  A2 and any a1  a2 ∈ Rm
+ it holds for all
q ≥ 1

A1 +

(9)
(10)
(the ﬁrst statement follows from the concavity of the square root function and the second one is
readily proved; see Lemma C.3 in the supplementary material) and thus

(cid:112)
A2 ≤ (cid:112)2(A1 + A2)

(cid:112)
(cid:107)a1(cid:107)q + (cid:107)a2(cid:107)q ≤ 21− 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:18)

p∗ hm + ep∗2D2

rM 1− 2

(cid:118)(cid:117)(cid:117)(cid:116) 16

n

1

p∗ p∗

 

BeDM

n

Rr(Hp)≤

q (cid:107)a1 + a2(cid:107)q ≤ 2(cid:107)a1 + a2(cid:107)q
∞(cid:88)

(cid:19)M
H¨older≤ (cid:16)(cid:107)1(cid:107)(p/q)∗ (cid:107)aq(cid:107)p/q

(cid:13)(cid:13)(cid:13)(cid:13) p∗
(cid:17)1/q

λ(m)
j

j=hm+1

m=1

+

2

√

where we used that for all non-negative a ∈ RM and 0 < q < p ≤ ∞ it holds
((cid:96)q-to-(cid:96)p conversion)
Since the above holds for all nonnegative integers hm  the result follows  completing the proof.

(cid:107)a(cid:107)q = (cid:104)1  aq(cid:105) 1

p (cid:107)a(cid:107)p .

q − 1

= M

1

q

(11)

2.1 Lower and Excess Risk Bounds

To investigate the tightness of the presented upper bounds on the LRC of Hp  we consider the case
where φ1(x)  . . .   φM (x) are i.i.d; for example  this happens if the original input space X is RM  
the original input variable x ∈ X has i.i.d. coordinates  and the kernels k1  . . .   kM are identical and
each act on a different coordinate of x.
Theorem 4 (LOWER BOUND). Assume that the kernels are centered and i.i.d.. Then  there is an
absolute constant c such that if λ(1) ≥ 1

nD2 then for all r ≥ 1

n and p ≥ 1 

(cid:118)(cid:117)(cid:117)(cid:116) c

n

∞(cid:88)

j=1

Rr(Hp D M ) ≥

min(rM  D2M 2/p∗

λ(1)
j ).

(12)

Comparing the above lower bound with the upper bounds  we observe that
per bound of Theorem 2 for centered identical

independent kernels

is of

the up-
the order

6

O(cid:0)(cid:113)(cid:80)∞

j=1 min(cid:0)rM  D2M

2

p∗ λ(1)

j

(cid:1)(cid:1)  thus matching the rate of the lower bound (the same holds

for the bound of Theorem 3). This shows that the upper bounds of the previous section are tight.

(cid:80)n

n− α

1+α

2(cid:1) (if α ≈ 1

1

p∗ n− 1

1
n

P (l ˆf − lf∗ ) := E l( ˆf (x)  y) − E l(f∗(x)  y)  where f∗ := argminf

As an application of our results to prediction problems such as classiﬁcation or regression  we also
bound the excess loss of empirical minimization  ˆf := argminf
i=1 l(f (xi)  yi)  w.r.t. to a loss
E l(f (x)  y) .
function l:
We use the analysis of Bartlett et al. [2] to show the following excess risk bound under the assump-
tion of algebraically decreasing eigenvalues of the kernel matrices  i.e. ∃d > 0  α > 1 ∀m : λ(m)
j ≤
d j−α (proof shown in the supplementary material E):
Theorem 5. Assume that (cid:107)k(cid:107)∞ ≤ B and ∃d > 0  α > 1 ∀m : λ(m)
≤ d j−α. Let l be a
Lipschitz continuous loss with constant L and assume there is a positive constant F such that ∀f ∈
F : P (f − f∗)2 ≤ F P (lf − lf∗ ). Then for all z > 0 with probability at least 1 − e−z the excess
loss of the multi-kernel class Hp can be bounded for p ∈ [1  2] as

j

1+α

+

47

√

186

1+α F

t∗ t∗

(cid:114) 3 − α

α−1
α+1 M 1+ 2

1 − α
BDLM 1

P (l ˆf − lf∗ ) ≤ min
t∈[p 2]

(cid:0)dD2L2t∗2(cid:1) 1

(cid:0) 1
t∗ −1(cid:1)
We see from the above bound that convergence can be almost as slow as O(cid:0)p∗M
is small ) and almost as fast as O(cid:0)n−1(cid:1) (if α is large).
(cid:16)(cid:0)t∗D(cid:1) 2
(cid:16)

∀t ∈ [p  2] : P (l ˆf − lf∗ ) = O

(22BDLM 1
t∗ + 27F )z
n

Interpretation of Bounds

t∗ −1(cid:1)
(cid:0) 1
(cid:17)

1+α M 1+ 2

n− α

3

+

1+α

1+α

n

(cid:17)

.

1

In this section  we discuss the rates of Theorem 5 obtained by local analysis bounds  that is

2

.

(cid:16)

(cid:16)

On the other hand  the global Rademacher complexity directly leads to a bound of the form [8]

∀t ∈ [p  2] : P (l ˆf − lf∗ ) = O

(14)
To compare the above rates  we ﬁrst assume p ≥ (log M )∗ so that the best choice is t = p. Clearly 
the rate obtained through local analysis is better in n since α > 1. Regarding the rate in the number
of kernels M and the radius D  a straightforward calculation shows that the local analysis improves
over the global one whenever M
n) . Interestingly  this “phase transition” does not
depend on α (i.e. the “complexity” of the kernels)  but only on p.
Second  if p ≤ (log M )∗  the best choice in (13) and (14) is t = (log M )∗ so that

√
1
p /D = O(

t∗DM

t∗ n− 1

t∗ n− 1

t∗DM

(cid:16) 1

(cid:16) M

D log M = O(

P (l ˆf − lf∗ ) ≤ O
M n−1  min
(15)
t∈[p 2]
√
n). Note  that when letting α → ∞ the
and the phase transition occurs for
classical case of aggregation of M basis functions is recovered. This situation is to be com-
pared to the sharp analysis of the optimal convergence rate of convex aggregation of M func-
tions obtained by [27] in the framework of squared error loss regression  which is shown to be
. This corresponds to the setting studied here with D = 1  p = 1
O
and α → ∞  and we see that our bound recovers (up to log factors) in this case this sharp bound and
the related phase transition phenomenon.
Please note that  by introducing an inequality in Eq. (5)  Assumption (U)—a similar assumption was
also used in [23]—can be relaxed to a more general  RIP-like assumption as used in [16]; this comes
at the expense of an additional factor in the bounds (details omitted here).

(cid:17)1/2(cid:17)(cid:17)(cid:17)

(cid:16) M√

n log

(cid:17)(cid:17)

(cid:16)

min

min

n  

M

n

1

2

.

(13)

When Can Learning Kernels Help Performance? As a practical application of the presented
bounds  we analyze the impact of the norm-parameter p on the accuracy of (cid:96)p-norm MKL in var-
ious learning scenarios  showing why an intermediate p often turns out to be optimal in practical
applications. As indicated in the introduction  there is empirical evidence that the performance of
(cid:96)p-norm MKL crucially depends on the choice of the norm parameter p (for example  cf. Figure 1

7

(a) β = 2

(b) β = 1

(c) β = 0.5

Figure 2: Illustration of the three analyzed learning scenarios (TOP) differing in their soft sparsity of the
Bayes hypothesis w∗ (parametrized by β) and corresponding values of the bound factor νt as a function of p
(BOTTOM). A soft sparse (LEFT)  a intermediate non-sparse (CENTER)  and an almost uniform w∗ (RIGHT).
in the introduction). The aim of this section is to relate the theoretical analysis presented here to this
empirically observed phenomenon.
To start with  ﬁrst note that the choice of p only affects the excess risk bound in the factor (cf.
Theorem 5 and Equation (13))

(cid:0)Dpt∗(cid:1) 2

1+α M 1+ 2

1+α

(cid:0) 1
t∗ −1(cid:1)

.

νt := min
t∈[p 2]

Let us assume that the Bayes hypothesis can be represented by w∗ ∈ H such that the block com-
ponents satisfy (cid:107)w∗
m(cid:107)2 = m−β  m = 1  . . .   M  where β ≥ 0 is a parameter parameterizing the
“soft sparsity” of the components. For example  the cases β ∈ {0.5  1  2} are shown in Figure 2
for M = 2 and rank-1 kernels. If n is large  the best bias-complexity trade-off for a ﬁxed p will
correspond to a vanishing bias  so that the best choice of D will be close to the minimal value such
that w∗ ∈ Hp D  that is  Dp = ||w∗||p. Plugging in this value for Dp  the bound factor νp becomes

νp := (cid:107)w∗(cid:107) 2

1+α
p

min
t∈[p 2]

t∗ 2

1+α M 1+ 2

1+α

(cid:0) 1
t∗ −1(cid:1)

.

We can now plot the value νp as a function of p ﬁxing α  M  and β. We realized this simulation for
α = 2  M = 1000  and β ∈ {0.5  1  2}.The results are shown in Figure 2. Note that the soft sparsity
of w∗ is increased from the left hand to the right hand side. We observe that in the “soft sparsest”
scenario (LEFT) the minimum is attained for a quite small p = 1.2  while for the intermediate case
(CENTER) p = 1.4 is optimal  and ﬁnally in the uniformly non-sparse scenario (RIGHT) the choice
of p = 2 is optimal  i.e. SVM. This means that if the true Bayes hypothesis has an intermediately
dense representation (which is frequently encountered in practical applications)  our bound gives the
strongest generalization guarantees to (cid:96)p-norm MKL using an intermediate choice of p.
4 Conclusion
We derived a sharp upper bound on the local Rademacher complexity of (cid:96)p-norm multiple kernel
learning. We also proved a lower bound that matches the upper one and shows that our result is
tight. Using the local Rademacher complexity bound  we derived an excess risk bound that attains
the fast rate of O(n− α
1+α )  where α is the minimum eigenvalue decay rate of the individual kernels.
In a practical case study  we found that the optimal value of that bound depends on the true Bayes-
optimal kernel weights. If the true weights exhibit soft sparsity but are not strongly sparse  then
the generalization bound is minimized for an intermediate p. This is not only intuitive but also
supports empirical studies showing that sparse MKL (p = 1) rarely works in practice  while some
intermediate choice of p can improve performance.

Acknowledgments
We thank Peter L. Bartlett and K.-R. M¨uller for valuable comments. This work was supported by the
German Science Foundation (DFG MU 987/6-1  RA 1894/1-1) and by the European Community’s
7th Framework Programme under the PASCAL2 Network of Excellence (ICT-216886) and under
the E.U. grant agreement 247022 (MASH Project).

8

w*w*w*1.01.21.41.61.82.060708090110pbound1.01.21.41.61.82.0404550556065pbound1.01.21.41.61.82.02030405060pboundReferences
[1] F. R. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the SMO

algorithm. In Proc. 21st ICML. ACM  2004.

[2] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. Annals of Statistics 

33(4):1497–1537  2005.

[3] R. R. Bouckaert  E. Frank  M. A. Hall  G. Holmes  B. Pfahringer  P. Reutemann  and I. H. Witten. WEKA–
experiences with a java open-source project. Journal of Machine Learning Research  11:2533–2541 
2010.

[4] C. Campbell and Y. Ying. Learning with Support Vector Machines. Synthesis Lectures on Artiﬁcial

Intelligence and Machine Learning. Morgan & Claypool Publishers  2011.

[5] C. Cortes.

Invited talk: Can learning kernels help performance?

In Proceedings of the 26th Annual
International Conference on Machine Learning  ICML ’09  pages 1:1–1:1  New York  NY  USA  2009.
ACM. Video http://videolectures.net/icml09_cortes_clkh/.

[6] C. Cortes  A. Gretton  G. Lanckriet  M. Mohri  and A. Rostamizadeh. Proceedings of the NIPS Workshop
on Kernel Learning: Automatic Selection of Optimal Kernels  2008. URL http://videolectures.
net/lkasok08_whistler/  Video http://www.cs.nyu.edu/learning_kernels.

[7] C. Cortes  M. Mohri  and A. Rostamizadeh. L2 regularization for learning kernels. In Proceedings of the

International Conference on Uncertainty in Artiﬁcial Intelligence  2009.

[8] C. Cortes  M. Mohri  and A. Rostamizadeh. Generalization bounds for learning kernels. In Proceedings 

27th ICML  2010.

[9] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning  20(3):273–297  1995.
[10] P. V. Gehler and S. Nowozin. Let the kernel ﬁgure it out: Principled learning of pre-processing for kernel
classiﬁers. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition  06 2009.
[11] R. Ibragimov and S. Sharakhmetov. The best constant in the rosenthal inequality for nonnegative random

variables. Statistics & Probability Letters  55(4):367 – 376  2001.

[12] J.-P. Kahane. Some random series of functions. Cambridge University Press  2nd edition  1985.
[13] M. Kloft  U. Brefeld  S. Sonnenburg  P. Laskov  K.-R. M¨uller  and A. Zien. Efﬁcient and accurate lp-norm
multiple kernel learning. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta 
editors  Advances in Neural Information Processing Systems 22  pages 997–1005. MIT Press  2009.

[14] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien. Lp-norm multiple kernel learning. Journal of Machine

Learning Research  12:953–997  Mar 2011.

[15] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Annals of

Statistics  34(6):2593–2656  2006.

[16] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. Annals of Statistics  38(6):3660–3695 

2010.

[17] S. Kwapi´en and W. A. Woyczy´nski. Random Series and Stochastic Integrals: Single and Multiple.

Birkh¨auser  Basel and Boston  M.A.  1992.

[18] G. Lanckriet  N. Cristianini  L. E. Ghaoui  P. Bartlett  and M. I. Jordan. Learning the kernel matrix with

semi-deﬁnite programming. JMLR  5:27–72  2004.

[19] S. Mendelson. On the performance of kernel classes. J. Mach. Learn. Res.  4:759–771  December 2003.
[20] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine

Learning Research  6:1099–1125  2005.

[21] K.-R. M¨uller  S. Mika  G. R¨atsch  K. Tsuda  and B. Sch¨olkopf. An introduction to kernel-based learning

algorithms. IEEE Neural Networks  12(2):181–201  May 2001.

[22] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet. SimpleMKL. Journal of Machine Learning

Research  9:2491–2521  2008.

[23] G. Raskutti  M. J. Wainwright  and B. Yu. Minimax-optimal rates for sparse additive models over kernel

classes via convex programming. CoRR  abs/1008.3654  2010.

[24] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue problem.

Neural Computation  10:1299–1319  1998.

[25] J. R. Searle. Minds  brains  and programs. Behavioral and Brain Sciences  3(03):417–424  1980.
[26] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. Journal of

Machine Learning Research  7:1531–1565  July 2006.

[27] A. Tsybakov. Optimal rates of aggregation. In B. Sch¨olkopf and M. Warmuth  editors  Computational
Learning Theory and Kernel Machines (COLT-2003)  volume 2777 of Lecture Notes in Artiﬁcial Intelli-
gence  pages 303–313. Springer  2003.

9

,Mehrdad Farajtabar
Nan Du
Manuel Gomez Rodriguez
Isabel Valera
Hongyuan Zha
Le Song
Hao Zhou
Vamsi Ithapu
Sathya Narayanan Ravi
Vikas Singh
Grace Wahba
Sterling Johnson