2019,DTWNet: a Dynamic Time Warping Network,Dynamic Time Warping (DTW) is widely used as a similarity measure in various domains. Due to its invariance against warping in the time axis  DTW provides more meaningful discrepancy measurements between two signals than other dis- tance measures. In this paper  we propose a novel component in an artificial neural network. In contrast to the previous successful usage of DTW as a loss function  the proposed framework leverages DTW to obtain a better feature extraction. For the first time  the DTW loss is theoretically analyzed  and a stochastic backpropogation scheme is proposed to improve the accuracy and efficiency of the DTW learning. We also demonstrate that the proposed framework can be used as a data analysis tool to perform data decomposition.,DTWNet: a Dynamic Time Warping Network

Xingyu Cai

University of Connecticut

Tingyang Xu
Tencent AI Lab

Jinfeng Yi

JD.com AI Lab

Junzhou Huang
Tencent AI Lab

Sanguthevar Rajasekaran
University of Connecticut

Abstract

Dynamic Time Warping (DTW) is widely used as a similarity measure in various
domains. Due to its invariance against warping in the time axis  DTW provides
more meaningful discrepancy measurements between two signals than other dis-
tance measures. In this paper  we propose a novel component in an artiÔ¨Åcial neural
network. In contrast to the previous successful usage of DTW as a loss function  the
proposed framework leverages DTW to obtain a better feature extraction. For the
Ô¨Årst time  the DTW loss is theoretically analyzed  and a stochastic backpropogation
scheme is proposed to improve the accuracy and efÔ¨Åciency of the DTW learning.
We also demonstrate that the proposed framework can be used as a data analysis
tool to perform data decomposition.

((cid:80)d

Introduction

1
In many data mining and machine learning problems  a proper metric of similarity or distance could
play a signiÔ¨Åcant role in the model performance. Minkowski distance  deÔ¨Åned as dist(x  y) =
k=1 |xk ‚àí yk|p)1/p for input x  y ‚àà Rd  is one of the most popular metrics. In particular  when
p = 1  it is called Manhattan distance; when p = 2  it is the Euclidean distance. Another popular
measure  known as Mahalanobis distance  can be viewed as the distorted Euclidean distance. It is
deÔ¨Åned as dist(x  y) = ((x ‚àí y)T Œ£‚àí1(x ‚àí y))1/2  where Œ£ ‚àà Rd√ód is the covariance matrix. With
geometry in mind  these distance (or similarity) measures  are straightforward and easy to represent.
However  in the domain of sequence data analysis  both Minkowski and Mahalanobis distances fail to
reveal the true similarity between two targets. Dynamic Time Warping (DTW) [1] has been proposed
as an attractive alternative. The most signiÔ¨Åcant advantage of DTW is its invariance against signal
warping (shifting and scaling in the time axis  or Doppler effect). Therefore  DTW has become one
of the most preferable measures in pattern matching tasks. For instance  two different sampling
frequencies could generate two pieces of signals  while one is just a compressed version of the other.
In this case  it will be very dissimilar and deviant from the truth to use the point-wise Euclidean
distance. On the contrary  DTW would capture such scaling nicely and output a very small distance
between them. DTW not only outputs the distance value  but also reveals how two sequences are
aligned against each other. Sometimes  the alignment could be more interesting. Furthermore  DTW
could be leveraged as a feature extracting tool  and hence it becomes much more useful than a
similarity measure itself. For example  predeÔ¨Åned patterns can be identiÔ¨Åed in the data via DTW
computing. Subsequently these patterns could be used to classify the temporal data into categories 
e.g.  [8]. Some interesting applications can be found in  e.g.  [6  14].
The standard algorithm for computing Dynamic Time Warping involves a Dynamic Programming
(DP) process. With the help of O(n2) space  a cost matrix C would be built sequentially  where

Ci j = ||xi ‚àí yj|| + min{Ci‚àí1 j  Ci j‚àí1  Ci‚àí1 j‚àí1}

(1)
Here ||xi ‚àí yj|| denotes the norm of (xi ‚àí yj)  e.g.  p-norm  p = 1  2 or ‚àû. After performing the
DP  we can trace back and identify the warping path from the cost matrix. This is illustrated in

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) DTW aligns x and y
Figure 1: Illustration of DTW Computation  Dynamic Programming and Warping Path

(c) The path is Ô¨Åxed after DP

(b) DTW path of x and y

Figure 1a 1b  where two sequences of different lengths are aligned. There are speedup techniques
to reduce DTW‚Äôs time complexity  e.g.  [15]  which is beyond the scope of this paper. In general  a
standard DP requires O(n2) time.
Although DTW is already one of the most important similarity measures and feature extracting
tools in temporal data mining  it has not contributed much to the recent deep learning Ô¨Åeld. As we
know  a powerful feature extractor is the key to the success of an artiÔ¨Åcial neural network (ANN).
The best example could be the CNNs that utilize convolutional kernels to capture local and global
features [10]. Unlike the convolution  DTW has the non-linear transformation property (warping) 
providing a summary of the target against Doppler effects. This makes DTW a good candidate as
a feature extractor in general ANNs. With this motivation  we propose DTWNet  a neural network
with learnable DTW kernels.
Key Contributions: We apply the learnable DTW kernels in neural networks to represent Doppler
invariance in the data. To learn the DTW kernel  a stochastic backpropogation method based on the
warping path is proposed  to compute the gradient of a DP process. A convergence analysis of our
backpropogation method is offered. To the best of the authors‚Äô knowledge  for the Ô¨Årst time  DTW
loss function is theoretically analyzed. A differentiable streaming DTW learning is also proposed to
overcome the problem of missing local features  caused by global alignment of the standard DTW.
Empirical study shows the effectiveness of the proposed backpropogation and the success of capturing
features using DTW kernels. We also demonstrate a data decomposition application.
2 Related Work
2.1

Introduction of Dynamic Time Warping

Dynamic Time Warping is a very popular tool in temporal data mining. For instance  DTW is
invariant of Doppler effects thus it is very useful in acoustic data analysis [14]. Another example is
that biological signals such as ECG or EEG  could use DTW to characterize potential diseases [24].
DTW is also a powerful feature extractor in conjunction with predeÔ¨Åned patterns (features)  in the
time series classiÔ¨Åcation problem [8]. Using Hamming distance  the DTW alignment in this setting is
called the Edit distance and also well studied [7].
Due to the Dynamic Programming involved in DTW computation  the complexity of DTW can be
high. More critically  DP is a sequential process which makes DTW not parallelizable. To speedup
the computation  some famous lower bounds based techniques [9  12  20] have been proposed. There
are also attempts on parallelization of DP [25] or GPU acceleration [22].
Two dimensional DTW has also drawn research interests. In [11]  the author showed that the DTW
could be extended to the 2-D case for image matching. Note that this is different from another
technique called multi-variate DTW [23  13  21]  sometimes also referred to as multi-dimensional
DTW. In multi-variate DTW  the input is a set of 1-D sequences  e.g.  of dimension k √ó n where n is
the sequence length. However  in 2-D or k-D DTW  the input is no longer a stack of 1-D sequences
but images (n2) or higher dimensional volumes (nk). As a result  the cost of computing 2-D DTW
can be as high as O(n6) and thus making it not applicable for large datasets.

2.2 SPRING Algorithm  the Streaming Version of DTW

To process the streaming data under DTW measure  [18] proposed a modiÔ¨Åed version of DTW
computation called SPRING. The original DTW aims to Ô¨Ånd the best alignment between two input
sequences  and the alignment is from the beginning of both sequences to the end. On the contrary 

2

024681012140.00.51.01.52.02.53.0xy02468101214X0246810Y0510152025c[i-1  j-1]c[i  j]c[i-1  j]c[i+1  j-1]c[i  j-1]c[i+1  j]c[i  j+1]c[i-1  j+1]c[i+1  j+1]the streaming version tries to identify all the subsequences from a given sequence  that are close to a
given pattern under the DTW measure. The naive approach computes DTW between all possible
subsequences and the pattern. Let the input sequence and the pattern be of lengths n and l  respectively.
The naive method takes (nl + (n ‚àí 1)l + . . .) = O(n2l) time. However  SPRING only takes O(nl)
time  which is consistent with the standard DTW.
SPRING modiÔ¨Åes the original DTW computation with two key factors. First  it prepends one wild-
card to the pattern. When matching the pattern with the input  since the wild-card can represent any
value  the start of the pattern could match any position in the input sequence at no cost. The second
modiÔ¨Åcation is that SPRING makes use of an auxiliary matrix to store the source of each entry in the
original dynamic programming matrix. This source matrix will keep records of each candidate path
and hence we can trace back from the end. Interested readers could refer to [18] for more details.

2.3 DTW as a Loss Function

Recently  in order to apply the DTW distance for optimization problems  the differentiability of DTW
has been discussed in the literature. As we know  computing DTW is a sequential process in general.
During the Ô¨Ålling of the DP matrix  each step takes a min operation on the neighbors. Since the min
operator is not continuous  the gradient or subgradient is not very well deÔ¨Åned. The Ô¨Årst attempt to use
soft-min function to replace min is reported in [19]. In their paper  the authors provide the gradient of
soft-min DTW  and perform shapelet learning to boost the performance of time series classiÔ¨Åcation in
limited test datasets. Using the same soft-min idea  in [4]  the authors empirically show that applying
DTW as a loss function leads to a better performance than conventional Euclidean distance loss  in a
number of applications. Another very recent paper [2] also uses continuous relaxation of the min
operator in DTW to solve video alignment and segmentation problems.

3 Proposed DTW Layer and its Backpropogation

In this paper  we propose to use DTW layers in a deep neural network. A DTW layer consists of
multiple DTW kernels that extract meaningful features from the input. Each DTW kernel generates a
single channel by performing DTW computation between the kernel and the input sequences. For
regular DTW  one distance value will be generated for each kernel. For the streaming DTW  multiple
values would be output (details will be given in ¬ß 5). If using a sliding window  the DTW kernel
would generate a sequence of distances  just as a convolutional kernel. After the DTW layer  linear
layers could be appended  to obtain classiÔ¨Åcation or regression results. A complete example of
DTWNet on a classiÔ¨Åcation task is illustrated in Algorithm 1.

denoted as Gx w : Rn ‚Üí Z.

Algorithm 1 DTWNet training for a classiÔ¨Åcation task. Network parameters are: number of DTW
kernels Nkernel; kernels xi ‚àà Rl; linear layers with weights w.
INPUT: Dataset Y = {(yi  zi)|yi ‚àà Rn  zi ‚àà Z = [1  Nclass]}. The DTWNet dataÔ¨Çow can be
OUTPUT: The trained DTWNet Gx w
1: Init w; For i = 1 to Nkernel: randomly init xi; Set total # of iteration be T   stopping condition 
2: for t = 0 to T do
3:
4:
5:
6:
7:
8:

Sample a mini-batch (y  z) ‚àà Y . Compute DTWNet output: ÀÜz ‚Üê Gx w(y)
Record warping path P and obtain determined form ft(x  y)  as in Equation 2
Let Lt ‚Üê LCrossEntropy(ÀÜz  z). Compute ‚àáwLt through regular BP.
For i = 1 to Nkernel: compute ‚àáxiLt ‚Üê ‚àáxift(xi  y) ‚àÇLt
SGD Update: let w ‚Üê w ‚àí Œ±‚àáwLt and for i = 1 to Nkernel do xi ‚Üê xi ‚àí Œ≤‚àáxiLt
If ‚àÜL = |Lt ‚àí Lt‚àí1| < : return Gx w

based on P  as in Equation 3

‚àÇft

Gradient Calculation and Backpropogation

To achieve learning of the DTW kernels  we propose a novel gradient calculation and backpropogation
(BP) approach. One simple but important observation is that: after performing DP and obtaining the
warping path  the path itself is settled down for this iteration. If the input sequences and the kernel
are of lengths n and l  respectively  the length of the warping path cannot be larger than O(n + l).

3

This means that the Ô¨Ånal DTW distance could be represented using O(n + l) terms  and each term is
||yi ‚àí xj|| where i  j ‚àà S  and S is the set containing the indices of elements along the warping path.
For example  if we use 2-norm  the Ô¨Ånal squared DTW distance could be of the following form:

dtw2(x  y) = ft(x  y) = ||y0 ‚àí x0||2

2 + ||y1 ‚àí x0||2

2 + ||y2 ‚àí x1||2

2 + . . .

(2)

This is illustrated in Figure 1c  where the solid bold lines and the highlighted nodes represent the
warping path after Dynamic Programming. Since the warping path is determined  other entries in the
cost matrix no longer affect the DTW distance  thus the differentiation can be done only along the
path. Since the DTW distance obtains its determined form  e.g.  Equation 2  taking derivative with
respect to either x or y becomes trivial  e.g. 

‚àáxdtw2(x  y) = ‚àáxft(x  y) = [2(y0 + y1 ‚àí 2x0)   2(y2 ‚àí x1)   . . .]T

(3)

Since the min operator does not have a gradient  directly applying auto-diff will result in a very high
variance. Soft-min could somewhat mitigate this problem  however  as shown above  since the Ô¨Ånal
DTW distance is only dependent on the elements along the warping path  differentiation on all the
entries in the cost matrix becomes redundant. Other than this  additional attention needs to be paid
to the temperature hyperparameter in the soft-min approach  which controls the trade-off between
accuracy and numerical stability.
In contrast  taking derivative using the determined form along the warping path  we can avoid the
computation redundancy. As the warping path length cannot exceed O(n + l)  the differentiation part
only takes O(n + l) time instead of O(nl) as in the soft-min approaches. Note that there is still a
variance which arises from the difference in DP‚Äôs warping paths from iteration to iteration  so the BP
can be viewed as a stochastic process.
Time Complexity: The computation of DTW loss requires building a Dynamic Programming matrix.
The standard DP needs O(nl) time. There are speeding-up/approximating techniques for DP such as
banded constraint (limit the warping path within a band)  which is beyond the scope of this paper.
The gradient is evaluated in O(n + l) time as shown above. Although the DP part is not parallelizable
in general  parallelization can still be achieved for independent evaluation for different kernels.

4 DTW Loss and Convergence
To simplify the analysis  we consider that for one input sequence y ‚àà Rn. The goal is to obtain
a target kernel x ‚àà Rl that has the best alignment with y  i.e.  minx dtw2(x  y). Without loss of
generality  we assume l ‚â§ n. The kernel x is randomly initialized and we perform learning through
standard gradient descent. DeÔ¨Åne the DTW distance function as d = Hy(x)  where d ‚àà R is the
DTW distance evaluated by performing the Dynamic Programming operator  i.e.  d = DP(x  y).
DeÔ¨Ånition 1. Since DP provides a deterministic warping path for arbitrary x  we deÔ¨Åne the space of
all the functions of x representing all possible warping paths as
Iij||(xi ‚àí yj)||2
2}

Fy = {fy(x)|fy(x) =

(cid:88)

s.t.

i j

i ‚àà [0  l ‚àí 1]; j ‚àà [0  n ‚àí 1]; Iij ‚àà {0  1}; n ‚â§ |I| ‚â§ n + l;
i  j satisfy temporal order constraints.

Here the cardinality of I is within the range of n and n + l  because the warping path length can only
be between n and n + l. The temporal order constraints make sure that the combination of i  j must
be valid. For example  if xi is aligned with yj  then xi+1 cannot be aligned with yj‚àí1  otherwise the
alignment will be against the DTW deÔ¨Ånition.
With DeÔ¨Ånition 1  when we perform Dynamic Programming at an arbitrary point x to evaluate
Hy(x)  we know that it must be equal to some function sampled from the functional space Fy  i.e. 
Hy(x)|x=ÀÜx = f (u)
y ‚àà Fy. So we can approximate Hy(x) as a collection of functions
in Fy  where each x could correspond to its own sample function. In the proposed backpropogation
step we compute the gradient of f (u)
(x) and perform the gradient descent using this gradient. The
Ô¨Årst question is whether ‚àáxf (u)

(x)|x=ÀÜx = ‚àáxHy(x)|x=ÀÜx.

(x)|x=ÀÜx   f (u)

y

y

y

4

(a) Quadratic

(b) Linear

(c) Analysis case 1

(d) Analysis case 2

Figure 2: Loss function d = Hy(x) and analysis. (a): Hy(x) approximated by quadratic fy(x);
(b): by linear fy(x); The curves on the wall are projections of Hy(x) for better illustration. (c):
Illustration of transitions from u to v  here f (v)
y = 0) is outside of
v; (d): both u and v have bowl-shapes.

y ‚Äôs stationary point (where ‚àáxk f (v)

We notice the fact that Hy(x) is not smooth in the space of x. More speciÔ¨Åcally  there exist positions
x such that

u (cid:54)= v; f (u)

y

y ‚àà Fy
  f (v)

(4)

(cid:40)

Hy(x) =

(x)|x=x+
f (u)
y
y (x)|x=x‚àí
f (v)

where x+ and x‚àí represent inÔ¨Ånitesimal amounts of perturbation applied on x  in the opposite
directions. However  note that the cardinality of Fy is Ô¨Ånite. In fact  in the Dynamic Programming
matrix  for any position  the warping path can only evolve in at most three directions  due to the
temporal order constraints. In boundary positions  only one direction can the warping path evolve
along. So we have:
Lemma 1. Warping paths number |Fy| < 3n+l  where (n + l) is the largest possible path length.

y

This means that the space of x is divided into regions such that Hy(x) is perfectly approximated
by f (u)
(x) in the particular region u. In other words  the loss function Hy(x) is a piece-wise (or
region-wise) quadratic function of x  if we compute the DTW loss as a summation of squared 2-norms 
e.g.  dtw2(x  y) = ||x0 ‚àí y0||2
2 + . . .. Similarly  if we use the absolute value as the
element distance for the functions in the set Fy  then we obtain piece-wise linear function as Hy(x).
This is shown in Figure 2a  2b. We perform Monte-Carlo simulations to generate the points and
compute their corresponding DTW loss. The length of x is 6  but we only vary the middle two
elements after a random initialization and hence can generate the 3-D plots. The length of y is 10.
The elements in both x and y are randomly initialized within [0  1]. Figure 2a veriÔ¨Åes that Hy(x) is
piece-wise quadratic using 2-norms  where Figure 2b corresponds to the piece-wise linear function.

2 + ||x1 ‚àí y0||2

Escaping Local Minima

Some recent theoretical work provides proofs for global convergence in non-convex neural network
loss functions  e.g.  see [5]. In this paper  we offer a different perspective for the analysis by exploiting
the fact that the global loss function is piece-wise quadratic or linear obtained by a DP process 
and the number of regions is bounded by O(3n+l) (Lemma 1). Without loss of generality  we only
consider HY (x) being piece-wise quadratic. Treating the regions as a collection of discrete states
V   where |V | < 3n+l  we Ô¨Årst analyze the behavior of escaping u and jumping to its neighbor v  for
u  v ‚àà V   using the standard gradient descent. Without loss of generality  we only look at coordinate
k (xk is of interest). Assume that after DP  a fraction yp:p+q is aligned with xk. Taking out the items
related to xk  we can write the local quadratic function in u  and its partial derivative with respect to
xk  as

p+q(cid:88)

j=p

(cid:88)

i j‚ààU

f (u)
y =

(yj ‚àí xk)2 +

Iij(xi ‚àí yj)2 and ‚àáxk f (u)

y =

2(xk ‚àí yj)

(5)

where U = {i  j|i (cid:54)= k  j /‚àà [p  p + q]}  Iij ‚àà {0  1}  which is obtained through DP  and i  j satisfy
temporal order. Setting ‚àáxk f (u)

y = 0 we get the stationary point at x(u)‚àó

k = 1
q+1

j=p yj.

p+q(cid:88)

j=p

(cid:80)p+q

5

X10.00.20.40.60.81.0X20.00.20.40.60.81.0Z0.750.800.850.900.951.00X10.00.20.40.60.81.0X20.00.20.40.60.81.0Z0.80.91.01.11.21.31.4ùë§ùë¢ùë£ùë•%&‚àóùë•%(‚àóùë•%)‚àóùë•%ùë§ùë¢ùë£ùë•%&‚àóùë•%(‚àóùë•%)‚àóùë•%ùë•*y   the same as f (u)

Without loss of generality  consider the immediate neighbor f (v)
the alignment of yp+q+1  i.e. 

p+q+1(cid:88)

(cid:88)
for the other immediate neighbor w that aligns(cid:80)p+q‚àí1
(cid:80)p+q+1

(yj ‚àí xk)2 +

(cid:80)p+q

f (v)
y =

where V = {i  j|i (cid:54)= k  j /‚àà [p  p + q + 1]}. The corresponding stationary point is at x(v)‚àó
yj  the stationary point is at x(w)‚àó

(cid:80)p+q‚àí1

i j‚ààV

j=p

j=p

k

k

y

x(u)‚àó
k =

j=p yj
q + 1

  x(v)‚àó

k =

yj

  x(w)‚àó

k =

j=p
q + 2

j=p
q

Iij(xi ‚àí yj)2

(6)

except for only

. Similarly 
.We have

yj

(7)

k

k

k

k

k

k

k

k

y

y

k

.

k < x2

k < x3

  x(u)‚àó

k  for x1

= x(v)‚àó

and x(v)‚àó

k ‚àà u  x3

k ‚àí x(u)‚àó

k ‚àà w  x2
  f (u)
  f (v)

y   and their local minima (or stationary points) x(w)‚àó

Without loss of generality  assume that the three neighbor regions w  u  v are from left to right 
k ‚àà v. The three regions corresponding to three local
i.e.  x1
  x(v)‚àó
quadratic functions f (w)
 
are illustrated in Figure 2c  2d. Note that we are interested in transition u ‚Üí v  when u‚Äôs local
minimum is not at the boundary (u has a bowl-shape and we want to jump out).
There could be 3 possibilities for the destination (region v). The Ô¨Årst one is illustrated in Figure 2c 
where x(v)‚àó
is not inside region v  but somewhere to the left. In this case  it is easy to see the
global minimum will not be in v since some part in u is lower (u has the bowl-shape due to its local
minimum). If jumping to v  the gradient in v would point back to u  which is not the case of interest.
In the second case  both u and v have the bowl-shapes. As shown in Figure 2d  the distance between
the bottom of two bowls is d(u v)
. The boundary must be somewhere in between
x(u)‚àó
. Since we need to travel from u to v  the starting point xk = Àúx ‚àà u must be to the
k
left of x(u)‚àó
(as shown in the red double-arrows region  in Figure 2d). Otherwise the gradient at Àúx
will point to region w instead of v. To ensure one step crossing the boundary and arrives at v  it needs
to travel a distance of at most (x(v)‚àó
k ‚àí Àúx)  because the boundary between u and v could never reach
x(v)‚àó
For the third case  v does not have the bowl-shape  but x(v)‚àó
is to the right of v. We can still travel
(x(v)‚àó
k ‚àí Àúx) to jump beyond v. Similar to case 1  the right neighbor of v (denoted as v+) would have a
lower minimum if v+ has bowl-shape. Even if v+ does not have a bowl-shape  the combined region
[v  v+] can be viewed as either a quasi-bowl or an extended v  thus jumping here is still valid.
Next  we need to consider the relationship between feasible starting point Àúx and f (w)
‚Äôs stationary
point x(w)‚àó
. However  there could
be cases in which w does not hold f (w)
is to the left
of region w  then the inequality Àúx > x(w)‚àó
becomes looser  but still valid. Another case is that when
x(w)‚àó
is to the right side of w. This means w is monotonically decreasing  so we can combine [w  u]
as a whole quasi-bowl region u(cid:48)  and let w(cid:48) be the left neighbor of the combined u(cid:48). Therefore  the
above analysis on w(cid:48)  u(cid:48) and v still holds  and we want to jump out u(cid:48) to v. Hence we arrive at the
following theorem.
Theorem 1. Assume that the starting point at coordinate k  i.e. xk = Àúx  is in some region u where
f (u)
is deÔ¨Åned in Equation 5. Let x and y have lengths n and l  respectively  and assume that l < n.
y
To ensure escaping from u to its immediate right-side neighbor region  the expected step size E[Œ∑]
needs to satisfy: E[Œ∑] > l
2n .
(cid:80)m
i=0 Hyi(x) and ‚àáxHY (x) =(cid:80)m
The proof can be found in the supplementary A. In other cases  we consider a dataset Y = {yi|yi ‚àà
(cid:80)
Rn  i = 1  . . .   m}. The DTW loss and its full gradient have the summation form  i.e.  HY (x) =
i=0 ‚àáxHyi(x). The updating of x is done via stochastic gradient
(cid:80)
descent (SGD) over mini-batches  i.e.  x ‚Üê x + Œ∑ m
b ‚àáxHyi(x)  where b < m is the mini-
batch size  and Œ∑ is the step size. Though the stochastic gradient is an unbiased estimator  i.e.
b ‚àáxHyi(x)] = ‚àáxHY (x)  the variance offers the capability to jump out of local minima.
E[ m
b

is within region w  since Àúx ‚àà u  we know that Àúx > x(w)‚àó

‚Äôs stationary point. If the stationary point x(w)‚àó

. If x(w)‚àó

k

k

k

k

y

k

k

k

y

b

6

(a) Data samples

(b) No reg

(c) Week reg

(d) Strong reg

Figure 3: Illustration of the effect of the streaming DTW‚Äôs regularizer: from left to right  Œ± = 0 and
1 √ó 10‚àí4 and 0.1  respectively.

5 Streaming DTW Learning

The typical length of a DTW kernel is much shorter than the input data. Aligning the short kernel
with a long input sequence  could lead to misleading results. For example  consider the ECG data
sequence which consists of several periods of heartbeat pulses  and we would like to let the kernel
learn the heartbeat pulse pattern. However  applying an end-to-end DTW  the kernel will align the
entire sequence rather than a single pulse period. If the kernel is very short  it does not even have
enough resolution and thus Ô¨Ånally outputs a useless abstract.
To address this problem  we bring the SPRING [18] algorithm to output the patterns aligning
subsequences of the original input:

x‚àó = arg min

i ‚àÜ x

dtw2(x  yi:i+‚àÜ)

(8)

where yi:i+‚àÜ denotes the subsequence of y that starts at position i and ends at i + ‚àÜ  and x is the
pattern (the DTW kernel) we would like to learn. Note that i and ‚àÜ are parameters to be optimized.
In fact  SPRING not only Ô¨Ånds the best matching among all subsequences  but also reports a number
of candidate warping paths that have small DTW distances. As a result  we propose two schemes that
exploit this property. In the Ô¨Årst scheme  we pre-specify a constant k (e.g. 3 or 5) and let SPRING
provide the top k best warping paths (k different non-overlapping subsequences that have least DTW
distances to the pattern x). In the second scheme  rather than specifying the number of paths  we set
a value of  such that all the paths that have distances smaller than (1 + )d‚àó are reported  where d‚àó
is the best warping path‚Äôs DTW distance. After obtaining multiple warping paths  we can do either
an averaging  or random sampling as our DTW computing result. In our experiments  we choose
 = 0.1 and randomly sample one path for simplicity.

Regularizer in Streaming DTW

Since SPRING encourages the kernel x to learn some repeated pattern in the input sequence  there is
no constraint of such patterns‚Äô shapes  which could cause problematic learning results. As a matter of
fact  some common shapes that do not carry much useful information always occur in the input data.
For example  an up-sweep or down-sweep always exists  even the Gaussian noise is a combination of
such sweeps. The kernel without any regularization would easily capture such useless patterns and
fall into such local minima. To solve this issue  we propose a simple solution that adds a regularizer
on the shape of the pattern. Assuming x is of length l  we change the objective to

(1 ‚àí Œ±)dtw2(x  yi:i+‚àÜ) + Œ±||x0 ‚àí xl||

min
i ‚àÜ x

(9)

where Œ± is the hyper parameter that controls the regularizer. This essentially forces the pattern to be
a "complete" one  in the sense that the beginning and the ending of the pattern should be close. It
is a general assumption that we want to capture such "complete" signal patterns  rather than parts
of them. As shown in Figure 3a  the input sequences contain either upper or lower half circles as
the target to be learned. Without regulation  Figure 3b shows that the kernel only learns a part of
that signal. Figure 3c corresponds to a weak regularizer  where the kernel tries to escape from the
tempting local minima (these up-sweeps are so widely spread in the input and lead to small SPRING
DTW distances). A full shape is well learned with a proper Œ±  as shown in Figure 3d. Other shape
regularizers could be also used  if they contain prior knowledge from human experts.

7

01020304050607080‚àí1.0‚àí0.50.00.51.0Data samples from 2 categoriestype 1type 1type 2type 20.02.55.07.50.000.250.500.751.001.25filter shape0.02.55.07.5‚àí1.0‚àí0.50.0filter shape0.02.55.07.5‚àí1.5‚àí1.0‚àí0.50.0filter shape(a) Data samples

(b) Learned kernels

(c) Test acc

(d) Test loss

Figure 4: Performance comparison on synthetic data sequences (400 iterations)

6 Experiments and Applications

In this experimental section  we compare the proposed scheme with existing approaches. We refer
to the end-to-end DTW kernel as Full DTW  and the streaming version as SPRING DTW. We
implement our approach in PyTorch [16].

6.1 Comparison with Convolution Kernel

In this very simple classiÔ¨Åcation task  two types of synthetic data sequences are generated. Category
1 only consists of half square signal patterns. Category 2 only has upper triangle signal patterns.
Each data sequence was planted with two such signals  but in random locations with random pattern
lengths. The patterns do not overlap in each sequence. Also  Gaussian noise is injected into the
sequences. Figure 4a provides some sample sequences from both categories.
The length of the input sequences is 100 points  where the planted pattern length varies from 10
to 30. There are a total of 100 sequences in the training set  50 in each category. Another 100
sequences form the testing set  50 for each type as well. We added Gaussian noise with œÉ = 0.1. For
comparison  we tested one full DTW kernel  one SPRING DTW kernel  and one convolution kernel.
The kernel lengths are set to 10. Œ± = 0.1 for SPRING DTW. We append 3 linear layers to generate
the prediction.
In Figure 4b  we show the learned DTW kernel after convergence. As expected  the full DTW kernel
tries to capture the whole sequence. Since the whole sequence consists of two planted patterns 
the full DTW also has two peaks. On the contrary  SPRING DTW only matches partial signal 
thus resulting in a sweep shape. Figure 4c and Figure 4d show the test accuracy and test loss for
400 iterations. Since both full DTW and SPRING DTW achieve 100% accuracy  and their curves
are almost identical  we only show the curve from the full DTW. Surprisingly  the network with
the convolution kernel fails to achieve 100% accuracy after convergence on this simple task. The
"MLP" represents a network consisting of only 3 linear layers  and performs the worst among all the
candidates as expected.
Note that we can easily extend the method to multi-variate time series data (MDTW [21])  without
any signiÔ¨Åcant modiÔ¨Åcations. Details can be found in the supplementary B.

6.2 Evaluation of Gradient Calculation

To evaluate the effectiveness and accuracy of the proposed BP scheme  we follow the experimental
setup in [4] and perform barycenter computations. The UCR repository [3] is used in this experiment.
We evaluate our method against SoftDTW [4]  DBA [17] and SSG [4]. We report the average of 5
runs for each experiment. A random initialization is done for all the methods. Due to space limit  we
only provide a summary in this section but details can be found in supplementary C (Table 2  3).
The barycenter experiment aims to Ô¨Ånd the barycenter for the given input sequences. We use the
entire training set to train the model to obtain the barycenter bi for each category  and then calculate
the DTW loss as:

dtw(si j  bi)

(10)

Nclass(cid:88)

Ni(cid:88)

j=0

1
Ni

Ldtw =

1

Nclass

i=0

where Nclass is the number of categories  Ni is the number of sequences in class i  and si j is sequence
j in class i. The DTW distance is computed using (cid:96)2 norm. Clearly  the less the loss  the better is the

8

01020304050607080‚àí0.20.00.20.40.60.81.01.2Data samples from 2 categoriestype 1: squaretype 2: triangular02468‚àí0.50.00.51.01.5Kernel ShapeFull DTWSPRING DTW01002003004000.50.60.70.80.91.0test accdtwconvMLP01002003004000.30.40.50.60.7test lossdtwconvMLPTable 1: Barycenter Experiment Summary

Training Set

Testing Set

Alg
Win

Avg-rank
Avg-loss

4

SoftDTW SSG
23
2.14
26.19

3.39
27.75

DBA Ours
37
21
2.2
2.27
24.79
26.42

SoftDTW SSG
21
2.31
33.84

11
3.12
33.08

DBA Ours
31
22
2.21
2.36
31.99
33.62

performance. We also evaluate on the testing set by using si j from the testing set. Note that we Ô¨Årst
run SoftDTW with 4 different hyperparameter settings Œ≥ = 1  0.1  0.01 and 0.001 as in [4]. In the
training set  Œ≥ = 0.1 outperforms others  while in the testing set  Œ≥ = 0.001 gives the best results 
thus we select Œ≥ accordingly.
The experimental results are summarized in Table 1. "Win" denotes the number of times the smallest
loss was achieved  among all the 85 datasets. We also report the average rank and average loss (sum
all the losses and divide by number of datasets) in the table. From the results we can clearly see
that our proposed approach achieves the best performance among these methods. The details of this
experiment can be found in supplementary C.

6.3 Application of DTW Decomposition

In this subsection  we propose an application of DTWNet as a time series data decomposition tool.
Without loss of generality  we design 5 DTW layers and each layer has one DTW kernel  i.e.  xi.
The key idea is to forward the residual of layer i to the next layer in this network. Note that DTW
computation dtw(y  xi) will generate the warping path like Equation 2  from which we obtain the
residual by subtracting the corresponding aligned xi j from yj  where j is the index of elements.

(a) Samples of Haptics dataset

(b) Kernel 0 (c) Kernel 1 (d) Kernel 2 (e) Kernel 3 (f) Kernel 4

Figure 5: Illustration of DTW Decomposition

Figure 5 illustrates the effect of the decomposition. Kernel 0 to kernel 4 correspond to the Ô¨Årst
layer (input side) till the last layer (output side). The training goal is to minimize the residual of the
network‚Äôs output  and we randomly initialize the kernels before training. We use the Haptics dataset
from the UCR repository to demonstrate the decomposition.
After a certain amount of epochs  we can clearly see that the kernels from different layers form
different shapes. The kernel 0 from the Ô¨Årst layer  has a large curve that describes the overall shape
of the data. This can be seen as the low-frequency part of the signal. In contrast  kernel 4 has those
zig-zag shapes that describe the high-frequency parts. Generally  in deeper layers  the kernels tend to
learn "higher frequency" parts. This can be utilized as a good decomposition tool given a dataset.
More meaningfully  the shapes of the kernels are very interpretable for human beings.

7 Conclusions and Future Work

In this paper  we have applied DTW kernel as a feature extractor and proposed the DTWNet
framework. To achieve backpropogation  after evaluating DTW distance via Dynamic Programming 
we compute the gradient along the determined warping path. A theoretical study of the DTW as a
loss function is provided. We identify DTW loss as region-wise quadratic or linear  and describe
the conditions for the step size of the proposed method in order to jump out of local minima. In
the experiments  we show that the DTW kernel could outperform standard convolutional kernels in
certain tasks. We have also evaluated the effectiveness of the proposed gradient computation and
backpropogation  and offered an application to perform data decomposition.

9

020406080100‚àí4‚àí202Data Samplessample 1sample 2sample 305‚àí1.0‚àí0.50.005‚àí0.250.000.250.5005‚àí1.0‚àí0.50.00.505‚àí0.4‚àí0.20.005‚àí1.0‚àí0.50.00.5References
[1] Donald J Berndt and James Clifford. Using dynamic time warping to Ô¨Ånd patterns in time series.

In KDD workshop  volume 10  pages 359‚Äì370. Seattle  WA  1994.

[2] Chien-Yi Chang  De-An Huang  Yanan Sui  Li Fei-Fei  and Juan Carlos Niebles. D3tw:
Discriminative differentiable dynamic time warping for weakly supervised action alignment
and segmentation. arXiv preprint arXiv:1901.02598  2019.

[3] Yanping Chen  Eamonn Keogh  Bing Hu  Nurjahan Begum  Anthony Bagnall  Abdullah Mueen 
and Gustavo Batista. The ucr time series classiÔ¨Åcation archive  July 2015. www.cs.ucr.edu/
~eamonn/time_series_data/.

[4] Marco Cuturi and Mathieu Blondel. Soft-dtw: a differentiable loss function for time-series.

arXiv preprint arXiv:1703.01541  2017.

[5] Simon S. Du  Jason D. Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent Ô¨Ånds

global minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018.

[6] Sergio Giraldo  Ariadna Ortega  Alfonso Perez  Rafael Ramirez  George Waddell  and Aaron
Williamon. Automatic assessment of violin performance using dynamic time warping classiÔ¨Å-
cation. In 2018 26th Signal Processing and Communications Applications Conference (SIU) 
pages 1‚Äì3. IEEE  2018.

[7] Omer Gold and Micha Sharir. Dynamic time warping and geometric edit distance: Breaking

the quadratic barrier. ACM Transactions on Algorithms (TALG)  14(4):50  2018.

[8] Rohit J Kate. Using dynamic time warping distances as features for improved time series

classiÔ¨Åcation. Data Mining and Knowledge Discovery  30(2)  2016.

[9] Eamonn Keogh and Chotirat Ann Ratanamahatana. Exact indexing of dynamic time warping.

Knowledge and information systems  7(3):358‚Äì386  2005.

[10] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097‚Äì1105  2012.

[11] Hansheng Lei and Venu Govindaraju. Direct image matching by dynamic warping. In Computer
Vision and Pattern Recognition Workshop  2004. CVPRW‚Äô04. Conference on  pages 76‚Äì76.
IEEE  2004.

[12] Daniel Lemire. Faster retrieval with a two-pass dynamic-time-warping lower bound. Pattern

recognition  42(9):2169‚Äì2180  2009.

[13] Jiangyuan Mei  Meizhu Liu  Yuan-Fang Wang  and Huijun Gao. Learning a mahalanobis
distance-based dynamic time warping measure for multivariate time series classiÔ¨Åcation. IEEE
transactions on Cybernetics  46(6):1363‚Äì1374  2016.

[14] Lindasalwa Muda  Mumtaj Begam  and Irraivan Elamvazuthi. Voice recognition algorithms
using mel frequency cepstral coefÔ¨Åcient (mfcc) and dynamic time warping (dtw) techniques.
arXiv preprint arXiv:1003.4083  2010.

[15] Abdullah Mueen  Nikan Chavoshi  Noor Abu-El-Rub  Hossein Hamooni  Amanda Minnich 
and Jonathan MacCarthy. Speeding up dynamic time warping distance for sparse time series
data. Knowledge and Information Systems  54(1):237‚Äì263  2018.

[16] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[17] Fran√ßois Petitjean and Pierre Gan√ßarski. Summarizing a set of time series by averaging: From
steiner sequence to compact multiple alignment. Theoretical Computer Science  414(1):76‚Äì91 
2012.

10

[18] Yasushi Sakurai  Christos Faloutsos  and Masashi Yamamuro. Stream monitoring under the time
warping distance. In Data Engineering  2007. ICDE 2007. IEEE 23rd International Conference
on  pages 1046‚Äì1055. IEEE  2007.

[19] Mit Shah  Josif Grabocka  Nicolas Schilling  Martin Wistuba  and Lars Schmidt-Thieme. Learn-
ing dtw-shapelets for time-series classiÔ¨Åcation. In Proceedings of the 3rd IKDD Conference on
Data Science  2016  page 3. ACM  2016.

[20] Yilin Shen  Yanping Chen  Eamonn Keogh  and Hongxia Jin. Accelerating time series searching
with large uniform scaling. In Proceedings of the 2018 SIAM International Conference on Data
Mining  pages 234‚Äì242. SIAM  2018.

[21] Mohammad Shokoohi-Yekta  Bing Hu  Hongxia Jin  Jun Wang  and Eamonn Keogh. Gen-
eralizing dtw to the multi-dimensional case requires an adaptive approach. Data mining and
knowledge discovery  31(1):1‚Äì31  2017.

[22] Peter Steffen  Robert Giegerich  and Mathieu Giraud. Gpu parallelization of algebraic dynamic
programming. In International Conference on Parallel Processing and Applied Mathematics 
pages 290‚Äì299. Springer  2009.

[23] Gineke A ten Holt  Marcel JT Reinders  and EA Hendriks. Multi-dimensional dynamic time
warping for gesture recognition. In Thirteenth annual conference of the Advanced School for
Computing and Imaging  volume 300  page 1  2007.

[24] R Varatharajan  Gunasekaran Manogaran  MK Priyan  and Revathi Sundarasekar. Wearable
sensor devices for early detection of alzheimer disease using dynamic time warping algorithm.
Cluster Computing  pages 1‚Äì10  2017.

[25] Fei-Yue Wang  Jie Zhang  Qinglai Wei  Xinhu Zheng  and Li Li. Pdp: parallel dynamic

programming. IEEE/CAA Journal of Automatica Sinica  4(1):1‚Äì5  2017.

11

,Xingyu Cai
Tingyang Xu
Jinfeng Yi
Junzhou Huang
Sanguthevar Rajasekaran