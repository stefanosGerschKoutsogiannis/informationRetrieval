2019,Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation,Existing Rademacher complexity bounds for neural networks rely only on norm control of the weight matrices and depend exponentially on depth via a product of the matrix norms. Lower bounds show that this exponential dependence on depth is unavoidable when no additional properties of the training data are considered. We suspect that this conundrum comes from the fact that these bounds depend on the training data only through the margin. In practice  many data-dependent techniques such as Batchnorm improve the generalization performance. For feedforward neural nets as well as RNNs  we obtain tighter Rademacher complexity bounds by considering additional data-dependent properties of the network: the norms of the hidden layers of the network  and the norms of the Jacobians of each layer with respect to all previous layers. Our bounds scale polynomially in depth when these empirical quantities are small  as is usually the case in practice. To obtain these bounds  we develop general tools for augmenting a sequence of functions to make their composition Lipschitz and then covering the augmented functions. Inspired by our theory  we directly regularize the network’s Jacobians during training and empirically demonstrate that this improves test performance.,Data-dependentSampleComplexityofDeepNeuralNetworksviaLipschitzAugmentationColinWeiComputerScienceDepartmentStanfordUniversitycolinwei@stanford.eduTengyuMaComputerScienceDepartmentStanfordUniversitytengyuma@stanford.eduAbstractExistingRademachercomplexityboundsforneuralnetworksrelyonlyonnormcontroloftheweightmatricesanddependexponentiallyondepthviaaproductofthematrixnorms.Lowerboundsshowthatthisexponentialdependenceondepthisunavoidablewhennoadditionalpropertiesofthetrainingdataareconsidered.Wesuspectthatthisconundrumcomesfromthefactthattheseboundsdependonthetrainingdataonlythroughthemargin.Inpractice manydata-dependenttechniquessuchasBatchnormimprovethegeneralizationperformance.ForfeedforwardneuralnetsaswellasRNNs weobtaintighterRademachercomplexityboundsbyconsideringadditionaldata-dependentpropertiesofthenetwork:thenormsofthehiddenlayersofthenetwork andthenormsoftheJacobiansofeachlayerwithrespecttoallpreviouslayers.Ourboundsscalepolynomiallyindepthwhentheseempiricalquantitiesaresmall asisusuallythecaseinpractice.Toobtainthesebounds wedevelopgeneraltoolsforaugmentingasequenceoffunctionstomaketheircompositionLipschitzandthencoveringtheaugmentedfunctions.Inspiredbyourtheory wedirectlyregularizethenetwork’sJacobiansduringtrainingandempiricallydemonstratethatthisimprovestestperformance.1IntroductionDeepnetworkstrainedinpracticetypicallyusemanymoreparametersthantrainingexamples andthereforehavethecapacitytooverﬁttothetrainingset[Zhangetal. 2016].Fortunately therearealsomanyknown(andunknown)sourcesofregularizationduringtraining:modelcapacityregularizationsuchassimpleweightdecay implicitoralgorithmicregularization[Gunasekaretal. 2017 2018b Soudryetal. 2018 Lietal. 2018] andﬁnallyregularizationthatdependsonthetrainingdatasuchasBatchnorm[IoffeandSzegedy 2015] layernormalization[Baetal. 2016] groupnormalization[WuandHe 2018] pathnormalization[Neyshaburetal. 2015a] dropout[Srivastavaetal. 2014 Wageretal. 2013] andregularizingthevarianceofactivations[LittwinandWolf 2018].Inmanycases itremainsunclearwhydata-dependentregularizationcanimprovetheﬁnaltesterror—forexample whyBatchnormempiricallyimprovesthegeneralizationperformanceinpractice[IoffeandSzegedy 2015 Zhangetal. 2019].Wedonothavemanytoolsforanalyzingdata-dependentregularizationintheliterature;withtheexceptionofDziugaiteandRoy[2018] [Aroraetal. 2018]and[NagarajanandKolter 2019](withwhichwecomparelaterinmoredetail) existingboundstypicallyconsiderpropertiesoftheweightsofthelearnedmodelbutlittleabouttheirinteractionswiththetrainingset.Formally deﬁneadata-dependentpropertyasanyfunctionofthelearnedmodelandthetrainingdata.Inthiswork weprovetightergeneralizationboundsbyconsideringadditionaldata-dependentpropertiesofthenetwork.Optimizingtheseboundsleadstodata-dependentregularizationtechniquesthatempiricallyimproveperformance.33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.Onewell-understoodandimportantdata-dependentpropertyisthetrainingmargin:Bartlettetal.[2017]showthatnetworkswithlargernormalizedmarginshavebettergeneralizationguarantees.However neuralnetsarecomplex sothereremainmanyotherdata-dependentpropertieswhichcouldpotentiallyleadtobettergeneralization.WeextendtheboundsandtechniquesofBartlettetal.[2017]byconsideringadditionalproperties:thehiddenlayernormsandinterlayerJacobiannorms.Ourﬁnalgeneralizationbound(Theorem5.1)isapolynomialinthehiddenlayernormsandLipschitzconstantsonthetrainingdata.Wegiveasimpliﬁedversionbelowforexpositionalpurposes.LetFdenoteaneuralnetworkwithsmoothactivationφparameterizedbyweightmatrices{W(i)}ri=1thatperfectlyclassiﬁesthetrainingdatawithmarginγ>0.Lettdenotethemaximum‘2normofanyhiddenlayerortrainingdatapoint andσthemaximumoperatornormofanyinterlayerJacobian wherebothquantitiesareevaluatedonlyonthetrainingdata.Theorem1.1(SimpliﬁedversionofTheorem5.1).Supposeσ t≥1.Withprobability1−δoverthetrainingdata wecanboundthetesterrorofFbyL0-1(F)≤eO(σγ+r3σ2)t(cid:16)1+PikW(i)>k2/32 1(cid:17)3/2+r2σ(cid:16)1+PikW(i)k2/31 1(cid:17)3/2√n+rslog(1δ)nThenotation˜Ohideslogarithmicfactorsind r σ tandthematrixnorms.Thek·k2 1normisformallydeﬁnedinSection3.Thedegreeofthedependenciesonσmaylookunconventional—thisismostlyduetothedramaticsimpliﬁcationfromourfullTheorem5.1 whichobtainsamorenaturalboundthatconsidersallinterlayerJacobiannormsinsteadofonlythemaximum.Ourboundispolynomialint σ andnetworkdepth butindependentofwidth.Inpractice tandσhavebeenobservedtobemuchsmallerthantheproductofmatrixnorms[Aroraetal. 2018 NagarajanandKolter 2019].Weremarkthatourboundisnothomogeneousbecausethesmoothactivationsarenothomogeneousandcancauseasecondordereffectonthenetworkoutputs.Incontrast theboundsofNeyshaburetal.[2015b] Bartlettetal.[2017] Neyshaburetal.[2017a] Golowichetal.[2017]alldependonaproductofnormsofweightmatriceswhichscalesexponentiallyinthenetworkdepth andwhichcanbethoughtofasaworstcaseLipschitzconstantofthenetwork.Infact lowerboundsshowthatwithonlynorm-basedconstraintsonthehypothesisclass thisproductofnormsisunavoidableforRademachercomplexity-basedapproaches(seeforexampleTheorem3.4of[Bartlettetal. 2017]andTheorem7of[Golowichetal. 2017]).Wecircumventtheselowerboundsbyadditionallyconsideringthemodel’sJacobiannorms–empiricalLipschitzconstantswhicharemuchsmallerthantheproductofnormsbecausetheyareonlycomputedonthetrainingdata.TheboundofAroraetal.[2018]dependsonsimilarquantitiesrelatedtonoisestabilitybutonlyholdsforacompressednetworkandnottheoriginal.TheboundofNagarajanandKolter[2019]alsodependspolynomiallyontheJacobiannormsratherthanexponentiallyindepth;howevertheseboundsalsorequirethattheinputstotheactivationlayersareboundedawayfrom0 anassumptionthatdoesnotholdinpractice[NagarajanandKolter 2019].Wedonotrequirethisassumptionbecauseweconsidernetworkswithsmoothactivations whereastheboundofNagarajanandKolter[2019]appliestorelunets.InSectionG weadditionallypresentageneralizationboundforrecurrentneuralnetsthatscalespolynomiallyinthesamequantitiesasourboundforstandardneuralnets.PriorgeneralizationboundsforRNNseitherrequireparametercounting[KoiranandSontag 1997]ordependexponentiallyondepth[Zhangetal. 2018 Chenetal. 2019].InFigure1 weplotthedistributionoverthesumofproductsofJacobianandhiddenlayernorms(whichistheleadingtermoftheboundinourfullTheorem5.1)foraWideResNet[ZagoruykoandKomodakis 2016]trainedwithandwithoutBatchnorm.Figure1showsthatthissumblowsupfornetworkstrainedwithoutBatchnorm indicatingthatthetermsinourboundareempiricallyrelevantforexplainingdata-dependentregularization.AnimmediatebottleneckinprovingTheorem1.1isthatstandardtoolsrequireﬁxingthehypothesisclassbeforelookingattrainingdata whereasconditioningondata-dependentpropertiesmakesthehypothesisclassarandomobjectdependingonthedata.Anaturalattemptistoaugmenttheloss2Figure1:Leth1 h2 h3denotethe1st 2nd and3rdblocksofa16-layerWideResNetandJitheJacobianoftheoutputw.r.tlayeri.Inlog-scaleweplotahistogramofthe100largestvaluesonthetrainingsetofP3i=1khikkJik/γforaWideRes-NettrainedwithandwithoutBatchnormonCI-FAR10 whereγistheexample’smargin.withindicatorsontheintendeddata-dependentquantities{γi} withdesiredbounds{κi}asfollows:laug=(lold−1)Ypropertiesγi1(γi≤κi)+1Thisaugmentedlossupperboundstheoriginallosslold∈[0 1] withequalitywhenallpropertiesholdforthetrainingdata.Theaugmentationletsusreasonaboutahypothesisclassthatisindependentofthedatabydirectlyconditioningondata-dependentpropertiesintheloss.Themainchallengeswiththisapproacharetwofold:1)designingthecorrectsetofpropertiesand2)provinggeneralizationoftheﬁnallosslaug acomplicatedfunctionofthenetwork.Ourmaintooliscoveringnumbers:Lemma4.1showsthatacompositionoffunctions(i.e aneuralnetwork)haslowcoveringnumberiftheoutputisworst-caseLipschitzateachlevelofthecompositionandinternallayersareboundedinnorm.Unfortunately thestandardneuralnetlosssatisﬁesneitheroftheseproperties(withoutexponentialdependenciesondepth).However byaugmentingwithpropertiesγ wecanguaranteetheyhold.Onetechnicalchallengeisthataugmentingthelossmakesithardertoreasonaboutcovering astheindicatorscanintroducecomplicateddependenciesbetweenlayers.Ourmaintechnicalcontributionsare:1)WedemonstratehowtoaugmentacompositionoffunctionstomakeitLipschitzatalllayers andthuseasytocover.Beforethisaugmentation theLipschitzconstantcouldscaleexponentiallyindepth(Theorem4.4).2)Wereducecoveringacomplicatedsequenceofoperationstocoveringtheindividualoperations(Theorem4.3).3)Bycombining1and2 itfollowscleanlythatouraugmentedlossonneuralnetworkshaslowcoveringnumberandthereforehasgoodgeneralization.Ourboundscalespolynomially notexponentially inthedepthofthenetworkwhenthenetworkhasgoodLipschitzconstantsonthetrainingdata(Theorem5.1).Asacomplementtothemaintheoreticalresultsinthispaper weshowempiricallyinSection6thatdirectlyregularizingourcomplexitymeasurecanresultinimprovedtestperformance.2RelatedWorkZhangetal.[2016]andNeyshaburetal.[2017b]showthatgeneralizatonindeeplearningoftendisobeysconventionalstatisticalwisdom.Oneoftheapproachesadoptedtorwardsexplaininggeneralizationisimplicitregularization;numerousrecentworkshaveshownthatthetrainingmethodprefersminimumnormormaximummarginsolutions[Soudryetal. 2018 Lietal. 2018 JiandTelgarsky 2018 Gunasekaretal. 2017 2018a b Weietal. 2018].Withtheexceptionof[Weietal. 2018] thesepapersanalyzesimpliﬁedsettingsanddonotapplytolargerneuralnetworks.ThispapermorecloselyfollowsalineofworkrelatedtoRademachercomplexityboundsforneuralnetworks[Neyshaburetal. 2015b 2018 Bartlettetal. 2017 Golowichetal. 2017].Foracomparison seetheintroduction.TherehasalsobeenworkonderivingPAC-Bayesianboundsforgeneralization[Neyshaburetal. 2017b a NagarajanandKolter 2019].DziugaiteandRoy[2017a]optimizeaboundtocomputenon-vacuousboundsforgeneralizationerror.Anotherlineofworkanalyzesneuralnetsviatheirbehavioronnoisyinputs.Neyshaburetal.[2017b]provePAC-Bayesiangeneralizationboundsforrandomnetworksunderassumptionsonthenetwork’sempiricalnoisestability.Aroraetal.[2018]developanotionofnoisestabilitythatallowsforcompressionofanetworkunderanappropriatenoisedistribution.Theyadditionallyprovethatthecompressednetworkgeneralizeswell.Incomparison ourLipschitznessconstructionalsorelatestonoisestability butourboundsholdfortheoriginalnetworkanddonotrelyontheparticularnoisedistribution.3NagarajanandKolter[2019]usePAC-BayesboundstoproveasimilarresultasoursforgeneralizationofanetworkwithboundedhiddenlayerandJacobiannorms.Themaindifferenceisthattheirboundsdependontheinverserelupreactivations whicharefoundtobelargeinpractice[NagarajanandKolter 2019];ourboundsapplytosmoothactivationsandavoidthisdependenceatthecostofanadditionalfactorintheJacobiannorm(showntobeempiricallysmall).Wenotethatthechoiceofsmoothactivationsisempiricallyjustiﬁed[Clevertetal. 2015 Klambaueretal. 2017].WealsoworkwithRademachercomplexityandcoveringnumbersinsteadofthePAC-Bayesframework.ItisrelativelysimpletoadaptourtechniquestorelunetworkstoproduceasimilarresulttothatofNagarajanandKolter[2019] byconditioningonlargepre-activationvaluesinourLipschitzaugmentationstep(seeSection4.2).InSectionH weprovideasketchofthisargumentandobtainaboundforrelunetworksthatispolynomialinhiddenlayerandJacobiannormsandinversepreactivations.However itisnotobvioushowtoadapttheargumentofNagarajanandKolter[2019]toactivationfunctionswhosederivativesarenotpiecewise-constant.DziugaiteandRoy[2018 2017b]developPAC-Bayesboundsfordata-dependentpriorsobtainedviasomedifferentiallyprivatemechanism.Theirboundsareforarandomizedclassiﬁersampledfromtheprior whereasweanalyzeadeterministic ﬁxedmodel.Novaketal.[2018]empiricallydemonstratethatthesensitivityofaneuralnettoinputnoisecorrelateswithgeneralization.Sokoli´cetal.[2017] KruegerandMemisevic[2015]proposestability-basedregularizersforneuralnets.Hardtetal.[2015]showthatmodelswhichtrainfastertendtogeneralizebetter.Keskaretal.[2016] Hofferetal.[2017]studytheeffectofbatchsizeongeneralization.Brutzkusetal.[2017]analyzeaneuralnetworktrainedonhingelossandlinearlyseparabledataandshowthatgradientdescentrecoverstheexactseparatinghyperplane.3NotationLet1(E)betheindicatorfunctionofeventE.Letl0-1denotethestandard0-1loss.Forκ≥0 Let1≤κ(·)bethesoftenedindicatorfunctiondeﬁnedas1≤κ(t)=(1ift≤κ2−t/κifκ≤t≤2κ0if2κ≤tNotethat1≤κisκ−1-Lipschitz.Deﬁnethenormk·kp qbykAkp q (cid:16)Pj(cid:0)PiApi j(cid:1)q/p(cid:17)1/q.LetPnbeauniformdistributionovernpoints{x1 ... xn}⊂Dx.LetfbeafunctionthatmapsDxtosomeoutputspaceDf andassumebothspacesareequippedwithsomenorms|||·|||(thesenormscanbedifferentbutweusethesamenotationsforthem).ThentheL2(Pn |||·|||)normofthefunctionfisdeﬁnedaskfkL2(Pn |||·|||) (cid:16)1nPi|||f(xi)|||2(cid:17)1/2.WeuseDtodenotetotalderivativeoperator andthusDf(x)representstheJacobianoffatx.SupposeFisafamilyoffunctionsfromDxtoDf.LetC( F ρ)bethecoveringnumberofthefunctionclassFw.r.t.metricρwithcoversize.Inmanycases thecoveringnumberdependsontheexamplesthroughthenormsoftheexamples andinthispaperweonlyworkwiththesecases.Thus weletN( F s)bethemaximumcoveringnumberforanypossiblendatapointswithnormnotlargerthans.Precisely ifwedeﬁnePn stobethesetofallpossibleuniformdistributionssupportedonndatapointswithnormsnotlargerthans thenN( F s) supPn∈Pn sC( F L2(Pn |||·|||)).SupposeFcontainsfunctionswithminputsthatmapfromatensorproductmEuclideanspacetoEuclideanspace thenwedeﬁneN( F (s1 ... sm)) supP:∀(x1 ... xm)∈supp(P)kxik≤siC( F L2(P)).4OverviewofMainResultsandProofTechniquesInthissection wegiveageneraloverviewofthemaintechnicalresultsandoutlinehowtoprovethemwithminimalnotation.Wewillpointtolatersectionswheremanystatementsareformalized.Tosimplifythecoremathematicalreasoning weabstractfeed-forwardneuralnetworks(includingresidualnetworks)ascompositionsofoperations.LetF1 ... Fkbeasequenceoffamiliesoffunctions(correspondingtofamiliesofsinglelayerneuralnetsinthedeeplearningsetting)and‘be4aLipschitzlossfunctiontakingvaluesin[0 1].Westudythecompositionsof‘andfunctionsinFi’s:L ‘◦Fk◦Fk−1···◦F1={‘◦fk◦fk−1◦···◦f1:∀i fi∈Fi}(1)Textbookresults[BartlettandMendelson 2002]boundthegeneralizationerrorbytheRademachercomplexity(formallydeﬁnedinSectionC)ofthefamilyoflossesL whichinturnisboundedbythecoveringnumberofLthroughDudley’sentropyintegraltheorem[Dudley 1967].Modulominornuances thekeyremainingquestionistogiveatightcoveringnumberboundforthefamilyLforeverytargetcoversizeinacertainrange(often considering∈[1/nO(1) 1]sufﬁces).Asalludedtointheintroduction generalizationerrorboundsobtainedthroughthismachineryonlydependonthe(training)datathroughthemargininthelossfunction andouraimistoutilizemoredata-dependentproperties.Towardsunderstandingwhichdata-dependentpropertiesareusefultoregularize itishelpfultorevisitthedata-independentcoveringtechniqueof[Bartlettetal. 2017] theskeletonofwhichissummarizedbelow.RecallthatN( F s)denotesthecoveringnumberforarbitraryndatapointswithnormlessthans.Thefollowinglemmasaysthatiftheintermediatevariable(orthehiddenlayer)fi◦···◦f1(x)isbounded andthecompositionoftherestofthefunctionsl◦fk◦···◦fi+1(x)isLipschitz thensmallcoveringnumberoflocalfunctionsimplysmallcoveringnumberforthecompositionoffunctions.Lemma4.1.[abstractionoftechniquesin[Bartlettetal. 2017]]Inthecontextabove assume:1.foranyx∈supp(Pn) |||fi◦···◦f1(x)|||≤si.2.‘◦fk◦···◦fi+1isκi-Lipschitzforalli.Then wehavethefollowingcoveringnumberboundforL(foranychoiceof1 ... k>0):logN(Pki=1κii L s0)≤Pki=1logN(i Fi si−1).ThelemmasaysthatthelogcoveringnumberandthecoversizescalelinearlyiftheLipschitznessparametersandnormsremainconstant.However thesetwoquantities intheworstcase caneasilyscaleexponentiallyinthenumberoflayers andtheyarethemainsourcesofthedependencyofproductofspectral/Frobeniusnormsoflayersin[Golowichetal. 2017 Bartlettetal. 2017 Neyshaburetal. 2017a 2015b]Moreprecisely theworst-caseLipschitznessoverallpossibledatapointscanbeexponentiallybiggerthantheaverage/typicalLipschitznessforexamplesrandomlydrawnfromthetrainingortestdistribution.WeaimtobridgethisgapbyderivingageneralizationerrorboundthatonlydependsontheLipschitznessandboundednessonthetrainingexamples.Ourgeneralapproach partiallyinspiredbymargintheory istoaugmentthelossfunctionbysoftindicatorsofLipschitznessandboundedness.Lethibeshorthandnotationforfi◦···◦f1 thei-thintermediatevalue andletz(x) ‘(hk(x))betheoriginalloss.Ourﬁrstattemptconsidered:˜z0(x) 1+(z(x)−1)·kYi=11≤si(khi(x)k)·kYi=11≤κi(k∂z/∂hikop)(2)Sinceztakesvaluesin[0 1] theaugmentedloss˜z0isanupperboundontheoriginallosszwithequalitywhenalltheindicatorsaresatisﬁedwithvalue1.ThehopewasthattheindicatorswouldﬂattenthoseregionswherehiisnotboundedandwherezisnotLipschitzinhi.However therearetwoimmediateissues.First thesoftindicatorsfunctionsarethemselvesfunctionsofhi.It’sunclearwhethertheaugmentedfunctioncanbeLipschitzwithasmallconstantw.r.thi andthuswecannotapplyLemma4.1.1Second theaugmentedlossfunctionbecomescomplicatedanddoesn’tfallintothesequentialcomputationformofLemma4.1 andthereforeevenifLipschitznessisnotanissue weneednewcoveringtechniquesbeyondLemma4.1.WeaddresstheﬁrstissuebyrecursivelyaugmentingthelossfunctionbymultiplyingmoresoftindicatorsthatboundtheJacobianofthecurrentfunction.Theﬁnalloss˜zreads:2˜z(x) 1+(z(x)−1)·kYi=11≤si(khi(x)k)·Y1≤i≤j≤k1≤κj←i(kDfj◦···◦fi[hi−1]kop)(3)1Apriori it’salsounclearwhat“Lipschitzinhi”meanssincethe¯z0doesnotonlydependonxthroughhi.Wewillformalizethisinlatersectionafterdeﬁningproperlanguageaboutdependenciesbetweenvariables.2Unlikeinequation(2) wedon’taugmenttheJacobianofthelossw.r.tthelayers.Thisallowsustodealwithnon-differentiablelossfunctionssuchasramploss.5whereκj←i’sareuser-deﬁnedparameters.Forourapplicationtoneuralnets weinstantiatesiasthemaximumnormoflayeriandκj←iasthemaximumnormoftheJacobianbetweenlayerjandiacrossthetrainingdataset.Apolynomialinκ scanbeshowntoboundtheworst-caseLipschitznessofthefunctionw.r.t.theintermediatevariablesintheformulaabove.3Byourchoiceofκ s a)thetraininglossisunaffectedbytheaugmentationandb)theworst-caseLipschitznessofthelossiscontrolledbyapolynomialoftheLipschitznessonthetrainingexamples.WeprovideaninformaloverviewofouraugmentationprocedureinSection4.2andformallystatedeﬁnitionsandguaranteesinSectionB.ThedownsideoftheLipschitzaugmentationisthatitfurthercomplicatesthelossfunction.Towardscoveringthelossfunction(assumingLipschitzproperties)efﬁciently weextendLemma4.1 whichworksforsequentialcompositionsoffunctions togeneralfamiliesofformulas orcomputationalgraphs.WeinformallyoverviewthisextensioninSection4.1usingaminimalsetofnotations andinSectionA wegiveaformalpresentationoftheseresults.CombiningtheLipschitzaugmentationandgraphscoveringresults weobtainacoveringnumberboundofaugmentedloss.ThetheorembelowisformallystatedinTheoremB.3ofSectionB.Theorem4.2.Let˜Lbethefamilyofaugmentedlossesdeﬁnedin(3).Forcoverresolutionsiandvalues˜κithatarepolynomialintheparameterssi κj←i weobtainthefollowingcoveringnumberboundfor˜L:logN(Xii˜κi ˜L s0)≤XilogN(i Fi si−1)+XilogN(i DFi si−1)whereDFidenotesthefunctionclassobtainedfromapplyingthetotalderivativeoperatortoallfunctionsinFi.Now followingthestandardtechniqueofboundingRademachercomplexityviacoveringnumbers wecanobtaingeneralizationerrorboundsforaugmentedloss.Forthedemonstrationofourtechnique supposethatthefollowingsimpliﬁcationholds:logN(i DFi si−1)=logN(i Fi si−1)=s2i−1/2i.Thenafterminimizingthecoveringnumberboundiniviastandardtechniques weobtainthebelowgeneralizationerrorboundontheoriginallossforparameters˜κialludedtoinTheorem4.2andformallydeﬁnedinTheoremB.2.Whenthetrainingexamplessatisfytheaugmentedindicators Etrain[˜z]=Etrain[z] andbecause˜zboundszfromabove wehaveEtest[z]−Etrain[z]≤Etest[˜z]−Etrain[˜z]≤eO (cid:16)Pi˜κ2/3is2/3i−1(cid:17)3/2√n+rlog(1/δ)n!(4)4.1OverviewofComputationalGraphCoveringToobtaintheaugmented˜zdeﬁnedin(3) weneededtoconditionondata-dependentpropertieswhichintroduceddependenciesbetweenthevariouslayers.Becauseofthis Lemma4.1isnolongersufﬁcienttocover˜z.Inthissection weinformallyoverviewhowtoextendLemma4.1tocovermoregeneralfunctionsviathenotionofcomputationalgraphs.Forspaceconstraints thissectionisadramaticallyabbreviatedandinformalversionofSectionA.AcomputationalgraphG(V E {RV})isanacyclicdirectedgraphwiththreecomponents:thesetofnodesVcorrespondstovariables thesetofedgesEdescribesdependenciesbetweenthesevariables and{RV}containsalistofcompositionrulesindexedbythevariablesV’s representingtheprocessofcomputingVfromitsdirectpredecessors.Forsimplicity weassumethegraphcontainsauniquesink denotedbyOG andwecallitthe“outputnode”.WealsooverloadthenotationOGtodenotethefunctionthatthecomputationalgraphGﬁnallycomputes.LetIG={I1 ... Ip}bethesubsetofnodeswithnopredecessors whichwecallthe“inputnodes”ofthegraph.Thenotionofafamilyofcomputationalgraphsgeneralizesthesequentialfamilyoffunctioncom-positionsin(1).LetG={G(V E {RV})}beafamilyofcomputationalgraphswithsharednodes edges outputnode andinputnodes(denotedbyI).LetRVbethecollectionofallpossiblecompo-sitionrulesusedfornodeVbythegraphsinthefamilyG.ThisfamilyGdeﬁnesasetoffunctionsOG {OG:G∈G}.3Asmentionedinfootnote1 wewillformalizetheprecisemeaningofLipschitznesslater.6ThetheorembelowextendsLemma4.1.Inthecomputationalgraphinterpretation Lemma4.1appliestoasequentialfamilyofcomputationalgraphswithkinternalnodesV1 ... Vk whereeachVicomputesthefunctionfi andtheoutputcomputesthecompositionOG=‘◦fk···◦f1=z.However theaugmentedloss˜znolongerhasthissequentialstructure requiringthebelowtheoremforcoveringgenericfamiliesofcomputationalgraphs.Weshowthatcoveringageneralfamilyofcomputationalgraphscanbereducedtocoveringallthelocalcompositionrules.Theorem4.3(InformalandweakerversionofTheoremA.3).Supposethatthereisanordering(V1 ... Vm)ofthenodes sothataftercuttingoutnodesV1 ... Vi−1 thenodeVibecomesaleafnodeandtheoutputOGisκVi-Lipschitzw.r.ttoViforallG∈G.Inaddition assumethatforallG∈G thenodeV’svaluehasnormatmostsV.Letpr(V)beallthepredecessorsofVandspr(V)bethelistofnormupperboundsofthepredecessorsofV.Then smallcoveringnumbersforallofthelocalcompositionrulesofVwithresolutionVwouldimplysmallcoveringnumberforthefamilyofcomputationalgraphswithresolutionPVVκV:logN(XV∈V\I∪{O}κVV+O OG sI)≤XV∈V\IlogN(V RV spr(V))(5)InSectionAweformalizethenotionof“cutting”nodesfromthegraph.TheconditionthatnodeV’svaluehasnormatmostsVisasimpliﬁcationmadeforexpositionalpurposes;ourfullTheo-remA.3alsoappliesifOGcollapsestoaconstantwhenevernodeV’svaluehasnormgreaterthansV.Thisallowsforthesoftenedindicators1≤si(khi(x)k)usedin(3).4.2LipschitzAugmentationofComputationalGraphsThecoveringnumberboundofTheorem4.3reliesonLipschitznessw.r.tinternalnodesofthegraphunderaworst-casechoiceofinputs.Fordeepnetworks thiscanscaleexponentiallyindepthviatheproductofweightnormsandeasilybelargerthantheaverageLipschitz-nessovertypicalinputs.Inthissection weexplainageneraloperationtoaugmentsequentialgraphs(suchasneuralnets)intographswithbetterworst-caseLipschitzconstants sotoolssuchasTheorem4.3canbeapplied.Thissectionisheavilysimpliﬁedforspaceconstraints.FormaldeﬁnitionsandtheoremstatementsareinSectionB.Theaugmentationreliesonintroducingtermssuchasthesoftindicatorsinequation(2)and(3)whichconditionondata-dependentproperties.AsoutlinedinSection4 theywilltranslatetothedata-dependentpropertiesinthegeneralizationbounds.Wealsorequiretheaugmentedfunctiontoupperboundtheoriginal.Wewillpresentagenericapproachtoaugmentfunctioncompositionssuchasz ‘◦fk◦...◦f1 whoseLipschitzconstantsarepotentiallyexponentialindepth withonlypropertiesinvolvingthenormsoftheinter-layerJacobians.Wewillproduce˜z whoseworst-caseLipschitznessw.r.t.internalnodescanbepolynomialindepth.InformalexplanationofLipschitzaugmentation:InthesamesettingofSection4 recallthatin(2) ourﬁrstunsuccessfulattempttosmoothoutthefunctionwasbymultiplyingindicatorsonthenormsofthederivativesoftheoutput:Qki=11≤κi(k∂z/∂hikop).ThedifﬁcultyliesincontrollingtheLipschitznessofthenewtermsk∂z/∂hikopthatweintroduce:bythechainrule wehavetheexpansion∂z∂hi=∂z∂hk∂hk∂hk−1···∂hi+1∂hi whereeachhj0isitselfafunctionofhjforj0>j.Thismeans∂z∂hiisacomplicatedfunctionintheintermediatevariableshjfor1≤j≤k.BoundingtheLipschitznessof∂z∂hirequiresaccountingfortheLipschitznessofeveryterminitsexpansion whichischallengingandcreatescomplicateddependenciesbetweenvariables.Ourkeyinsightisthatbyconsideringamorecomplicatedaugmentationwhichconditionsonthederivativesbetweenallintermediatevariables wecanstillcontrolLipschitznessofthesystem leadingtothemoreinvolvedaugmentationpresentedin(3).OurmaintechnicalcontributionisTheorem4.4 whichweinformallystatebelow.Theorem4.4(InformalversionofTheoremB.2).Thefunctions˜z(deﬁnedin(3))canbecomputedbyafamilyofcomputationalgraphseGillustratedinFigure2.ThisfamilyhasinternalnodesViandJicomputinghiandDfi[hi−1] respectively andcomputesamodiﬁedoutputrulethataugmentsthe7originalwithsoftindicators.ThesesoftindicatorsconditionthatthenormsoftheJacobiansandhiareboundedbyparametersκj←i si.Importantly theoutputO˜Gis˜κVi ˜κJi-Lipschitzw.r.t.Vi Ji respectively aftercuttingnodesV1 J1 ... Vi−1 Ji−1 forparameters˜κVi ˜κJithatarepolynomialsinκj←i si.Inaddition theaugmentedfunction˜zwillupperboundtheoriginalwithequalitywhenalltheindicatorsaresatisﬁed.Thecruxoftheproofisleveragingthechainruletodecompose∂z∂hiintoaproductandthenapplyingatelescopingargumenttoboundthedifferenceintheproductbydifferencesinindividualterms.InSectionBwepresentaformalversionofthisresultandalsoapplyTheorem4.3toproduceacoveringnumberboundforeG.5ApplicationtoNeuralNetworksFigure2:Lipschitzaugmentation(informallydeﬁned).Inthissectionweprovideourgeneralizationboundforneuralnets whichwasobtainedusingmachineryfromSection4.1.DeﬁneaneuralnetworkFparameterizedbyrweightmatri-ces{W(i)}byF(x)=W(r)φ(···φ(W(1)(x))···).Weusetheconventionthatactivationsandmatrixmultiplicationsaretreatedasdistinctlayersindexedwithasubscript withoddlay-ersapplyingamatrixmultiplicationandevenlayersapplyingφ(seeExampleA.1foravisualization).AdditionalnotationdetailsandtheproofareinSectionC.ThebelowresultfollowsfrommodelingtheneuralnetlossasasequentialcomputationalgraphandusingouraugmentationproceduretomakeitLipschitzinitsnodeswithparametersκhidden (i) κjacobian (i).ThenwecovertheaugmentedlosstobounditsRademachercomplexity.Theorem5.1.Assumethattheactivationφis1-Lipschitzwitha¯σφ-Lipschitzderivative.Fixreferencematrices{A(i)} {B(i)}.Withprobability1−δovertherandomdrawsofthedataPn allneuralnetworksFwithparameters{W(i)}andpositivemarginγsatisfy:E(x y)∼P[l0-1(F(x) y)]≤˜O(cid:16)Pi(κhidden (i)a(i)t(i−1))2/3+(κjacobian (i)b(i))2/3(cid:17)3/2√n+rrlog(1/δ)nwhereκjacobian (i) P1≤j≤2i−1≤j0≤2r−1σj0←2iσ2i−2←jσj0←j andκhidden (i) ξ+σ2r−1←2iγ+Pi≤i0<rσ2i0←2it(i0)+P1≤j≤j0≤2r−1Pj0j00=max{2i j} j00even¯σφσj0←j00+1σj00−1←2iσj00−1←jσj0←j.Intheseexpressions wedeﬁneσj−1←j=1 ξ=poly(r)−1 and:a(i) kW(i)>−A(i)>k2 1+ξ b(i) kW(i)−B(i)k1 1+ξt(0) maxx∈Pnkxk+ξ t(i) maxx∈PnkF2i←1(x)k+ξσj0←j maxx∈PnkQj0←j(x)kop+ξ andγ min(x y)∈Pn[F(x)]y−maxy06=y[F(x)]y0>0whereQj0←jcomputestheJacobianoflayerj0w.r.t.layerj.Notethatthetrainingerrorhereis0becauseoftheexistenceofpositivemarginγ.Wenotethatourboundhasnoexplicitdependenceonwidthandinsteaddependsonthek·k2 1 k·k1 1normsoftheweightsoffsetbyreferencematrices{A(i)} {B(i)}.Thesenormscanavoidscalingwiththewidthofthenetworkifthedifferencebetweentheweightsandreferencematricesissparse.Thereferencematrices{A(i)} {B(i)}areusefulifthereissomepriorbeliefbeforetrainingaboutwhatweightmatricesarelearned andtheyalsoappearintheboundsofBartlettetal.[2017].InSectionG wealsoshowthatourtechniquescaneasilybeextendedtoprovidegeneralizationboundsforRNNsscalingpolynomiallyindepthviathesamequantitiest(i) σj0←j.8Table1:TesterrorforamodeltrainedonCIFAR10invarioussettings.SettingNormalizationJacobianRegTestErrorBaselineBatchNorm×4.43%Lowlearningrate(0.01)BatchNorm×5.98%X5.46%NodataaugmentationBatchNorm×10.44%X8.25%NoBatchNormNone×6.65%LayerNorm[Baetal. 2016]×6.20%X5.57%6ExperimentsThoughthemainpurposeofthepaperistostudythedata-dependentgeneralizationboundsfromatheoreticalperspective weprovidepreliminaryexperimentsdemonstratingthattheproposedcomplexitymeasureandgeneralizationboundsareempiricallyrelevant.Weshowthatregularizingthecomplexitymeasureleadstobettertestaccuracy.InspiredbyTheorem5.1 wedirectlyregularizetheJacobianoftheclassiﬁcationmarginw.r.toutputsofnormalizationlayersandafterresidualblocks.Ourreasoningisthatnormalizationlayerscontrolthehiddenlayernorms soadditionallyregularizingtheJacobiansresultsinregularizationoftheproduct whichappearsinourbound.Weﬁndthatthisiseffectiveforimprovingtestaccuracyinavarietyofsettings.WenotethatSokoli´cetal.[2017]showpositiveexperimentalresultsforasimilarregularizationtechniqueindata-limitedsettings.Supposethatm(F(x) y)=[F(x)]y−maxj6=y[t]jdenotesthemarginofthenetworkforex-ample(x y).Lettingh(i)denotesomehiddenlayerofthenetwork wedeﬁnethenotationJ(i) ∂∂h(i)m(F(x) y)andusetrainingobjectiveˆLreg[F] E(x y)∼Pn"l(x y)+λ Xi1(kJ(i)(x)k2F≥σ)kJ(i)(x)k2F!#whereldenotesthestandardcrossentropyloss andλ σarehyperparameters.NotetheJacobianistakenwithrespecttoascalaroutputandthereforeisavector soitiseasytocompute.ForaWideResNet16[ZagoruykoandKomodakis 2016]architecture wetrainusingtheaboveobjective.ThethresholdontheFrobeniusnormintheregularizationisinspiredbythetruncationsinouraugmentedloss(inallourexperiments wechooseσ=0.1).Wetunethecoefﬁcientλasahyperparameter.Inourexperiments wetooktheregularizedindicesitobelastlayersineachresidualblockaswellaslayersinresidualblocksfollowingaBatchNorminthestandardWideResNet16architecture.IntheLayerNormsetting wesimplyreplacedBatchNormlayerswithLayerNorm.TheremaininghyperparametersettingsarestandardforWideResNet;foradditionaldetailsseeSectionI.1.Figure1showstheresultsformodelstrainedandtestedonCIFAR10inlowlearningrateandnodataaugmentationsettings whicharesettingswheregeneralizationtypicallysuffers.WealsoexperimentwithreplacingBatchNormlayerswithLayerNormandadditionallyregularizingtheJacobian.Weobserveimprovementsintesterrorforallthesesettings.InSectionI.2 weempiricallydemonstratethatourcomplexitymeasureindeedavoidstheexponentialscalingindepthforaWideResNetmodeltrainedonCIFAR10.7ConclusionInthispaper wetacklethequestionofhowdata-dependentpropertiesaffectgeneralization.WeprovetightergeneralizationboundsthatdependpolynomiallyonthehiddenlayernormsandnormsoftheinterlayerJacobians.Toprovethesebounds weworkwiththeabstractionofcomputationalgraphsanddevelopgeneraltoolstoaugmentanysequentialfamilyofcomputationalgraphsintoaLipschitzfamilyandthencoverthisLipschitzfamily.Thisaugmentationandcoveringprocedureappliestoanysequenceoffunctioncompositions.Aninterestingdirectionforfutureworkistogeneralizeourtechniquestoarbitrarycomputationalgraphstructures.Additionally encouragedbyourpromisingpreliminaryresults webelievethereistheexcitingempiricaldirectionofapplyingtheseboundstodevelopbetterdata-dependentregularization.9AcknowledgmentsCWwassupportedbyaNSFGraduateResearchFellowship.ToyotaResearchInstitute(TRI)providedfundstoassisttheauthorswiththeirresearchbutthisarticlesolelyreﬂectstheopinionsandconclusionsofitsauthorsandnotTRIoranyotherToyotaentity.ReferencesSanjeevArora RongGe BehnamNeyshabur andYiZhang.Strongergeneralizationboundsfordeepnetsviaacompressionapproach.arXivpreprintarXiv:1802.05296 2018.JimmyLeiBa JamieRyanKiros andGeoffreyEHinton.Layernormalization.arXivpreprintarXiv:1607.06450 2016.PeterLBartlettandShaharMendelson.Rademacherandgaussiancomplexities:Riskboundsandstructuralresults.JournalofMachineLearningResearch 3(Nov):463–482 2002.PeterLBartlett DylanJFoster andMatusJTelgarsky.Spectrally-normalizedmarginboundsforneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages6240–6249 2017.FriedrichLBauer.Computationalgraphsandroundingerror.SIAMJournalonNumericalAnalysis 11(1):87–96 1974.AlonBrutzkus AmirGloberson EranMalach andShaiShalev-Shwartz.Sgdlearnsover-parameterizednetworksthatprovablygeneralizeonlinearlyseparabledata.arXivpreprintarXiv:1710.10174 2017.MinshuoChen XingguoLi andTuoZhao.Ongeneralizationboundsofafamilyofrecurrentneuralnetworks 2019.URLhttps://openreview.net/forum?id=Skf-oo0qt7.Djork-ArnéClevert ThomasUnterthiner andSeppHochreiter.Fastandaccuratedeepnetworklearningbyexponentiallinearunits(elus).arXivpreprintarXiv:1511.07289 2015.RMDudley.Thesizesofcompactsubsetsofhilbertspaceandcontinuityofgaussianprocesses.JournalofFunctionalAnalysis 1(3):290–330 1967.GintareKarolinaDziugaiteandDanielMRoy.Computingnonvacuousgeneralizationboundsfordeep(stochastic)neuralnetworkswithmanymoreparametersthantrainingdata.arXivpreprintarXiv:1703.11008 2017a.GintareKarolinaDziugaiteandDanielMRoy.Entropy-sgdoptimizesthepriorofapac-bayesbound:Generalizationpropertiesofentropy-sgdanddata-dependentpriors.arXivpreprintarXiv:1712.09376 2017b.GintareKarolinaDziugaiteandDanielMRoy.Data-dependentpac-bayespriorsviadifferentialprivacy.InAdvancesinNeuralInformationProcessingSystems pages8430–8441 2018.NoahGolowich AlexanderRakhlin andOhadShamir.Size-independentsamplecomplexityofneuralnetworks.arXivpreprintarXiv:1712.06541 2017.SuriyaGunasekar BlakeEWoodworth SrinadhBhojanapalli BehnamNeyshabur andNatiSrebro.Implicitregularizationinmatrixfactorization.InAdvancesinNeuralInformationProcessingSystems pages6151–6159 2017.SuriyaGunasekar JasonLee DanielSoudry andNathanSrebro.Characterizingimplicitbiasintermsofoptimizationgeometry.arXivpreprintarXiv:1802.08246 2018a.SuriyaGunasekar JasonLee DanielSoudry andNathanSrebro.Implicitbiasofgradientdescentonlinearconvolutionalnetworks.arXivpreprintarXiv:1806.00468 2018b.MoritzHardt BenjaminRecht andYoramSinger.Trainfaster generalizebetter:Stabilityofstochasticgradientdescent.arXivpreprintarXiv:1509.01240 2015.10EladHoffer ItayHubara andDanielSoudry.Trainlonger generalizebetter:closingthegeneraliza-tiongapinlargebatchtrainingofneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages1731–1741 2017.SergeyIoffeandChristianSzegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.arXivpreprintarXiv:1502.03167 2015.ZiweiJiandMatusTelgarsky.Riskandparameterconvergenceoflogisticregression.arXivpreprintarXiv:1803.07300 2018.NitishShirishKeskar DheevatsaMudigere JorgeNocedal MikhailSmelyanskiy andPingTakPeterTang.Onlarge-batchtrainingfordeeplearning:Generalizationgapandsharpminima.arXivpreprintarXiv:1609.04836 2016.GünterKlambauer ThomasUnterthiner AndreasMayr andSeppHochreiter.Self-normalizingneuralnetworks.InAdvancesinneuralinformationprocessingsystems pages971–980 2017.PascalKoiranandEduardoDSontag.Vapnik-chervonenkisdimensionofrecurrentneuralnetworks.InEuropeanConferenceonComputationalLearningTheory pages223–237.Springer 1997.DavidKruegerandRolandMemisevic.Regularizingrnnsbystabilizingactivations.arXivpreprintarXiv:1511.08400 2015.YuanzhiLi TengyuMa andHongyangZhang.Algorithmicregularizationinover-parameterizedmatrixsensingandneuralnetworkswithquadraticactivations.InConferenceOnLearningTheory pages2–47 2018.EtaiLittwinandLiorWolf.Regularizingbythevarianceoftheactivations’sample-variances.InAdvancesinNeuralInformationProcessingSystems pages2115–2125 2018.VaishnavhNagarajanandZicoKolter.DeterministicPAC-bayesiangeneralizationboundsfordeepnetworksviageneralizingnoise-resilience.InInternationalConferenceonLearningRepresenta-tions 2019.URLhttps://openreview.net/forum?id=Hygn2o0qKX.BehnamNeyshabur RyotaTomioka RuslanSalakhutdinov andNathanSrebro.Data-dependentpathnormalizationinneuralnetworks.arXivpreprintarXiv:1511.06747 2015a.BehnamNeyshabur RyotaTomioka andNathanSrebro.Norm-basedcapacitycontrolinneuralnetworks.InConferenceonLearningTheory pages1376–1401 2015b.BehnamNeyshabur SrinadhBhojanapalli DavidMcAllester andNathanSrebro.Apac-bayesianapproachtospectrally-normalizedmarginboundsforneuralnetworks.arXivpreprintarXiv:1707.09564 2017a.BehnamNeyshabur SrinadhBhojanapalli DavidMcAllester andNatiSrebro.Exploringgeneraliza-tionindeeplearning.InAdvancesinNeuralInformationProcessingSystems pages5947–5956 2017b.BehnamNeyshabur ZhiyuanLi SrinadhBhojanapalli YannLeCun andNathanSrebro.Towardsunderstandingtheroleofover-parametrizationingeneralizationofneuralnetworks.arXivpreprintarXiv:1805.12076 2018.RomanNovak YasamanBahri DanielAAbolaﬁa JeffreyPennington andJaschaSohl-Dickstein.Sensitivityandgeneralizationinneuralnetworks:anempiricalstudy.arXivpreprintarXiv:1802.08760 2018.JureSokoli´c RajaGiryes GuillermoSapiro andMiguelRDRodrigues.Robustlargemargindeepneuralnetworks.IEEETransactionsonSignalProcessing 65(16):4265–4280 2017.DanielSoudry EladHoffer MorShpigelNacson SuriyaGunasekar andNathanSrebro.Theimplicitbiasofgradientdescentonseparabledata.TheJournalofMachineLearningResearch 19(1):2822–2878 2018.11NitishSrivastava GeoffreyHinton AlexKrizhevsky IlyaSutskever andRuslanSalakhutdinov.Dropout:asimplewaytopreventneuralnetworksfromoverﬁtting.TheJournalofMachineLearningResearch 15(1):1929–1958 2014.StefanWager SidaWang andPercySLiang.Dropouttrainingasadaptiveregularization.InAdvancesinneuralinformationprocessingsystems pages351–359 2013.ColinWei JasonDLee QiangLiu andTengyuMa.Onthemargintheoryoffeedforwardneuralnetworks.arXivpreprintarXiv:1810.05369 2018.Wikipediacontributors.Chainrule—Wikipedia thefreeencyclopedia 2019.YuxinWuandKaimingHe.Groupnormalization.arXivpreprintarXiv:1803.08494 2018.SergeyZagoruykoandNikosKomodakis.Wideresidualnetworks.arXivpreprintarXiv:1605.07146 2016.ChiyuanZhang SamyBengio MoritzHardt BenjaminRecht andOriolVinyals.Understandingdeeplearningrequiresrethinkinggeneralization.arXivpreprintarXiv:1611.03530 2016.HongyiZhang YannN.Dauphin andTengyuMa.Residuallearningwithoutnormalizationviabetterinitialization.InInternationalConferenceonLearningRepresentations 2019.URLhttps://openreview.net/forum?id=H1gsz30cKX.JiongZhang QiLei andInderjitSDhillon.Stabilizinggradientsfordeepneuralnetworksviaefﬁcientsvdparameterization.arXivpreprintarXiv:1803.09327 2018.12,Colin Wei
Tengyu Ma