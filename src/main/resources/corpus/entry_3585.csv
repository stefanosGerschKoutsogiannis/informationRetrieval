2011,Multi-armed bandits on implicit metric spaces,The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation. In this setting  an online algorithm has a fixed set of alternatives ("arms")  and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood  a lot of recent work has focused on multi-armed bandits with (infinitely) many arms  where one needs to assume extra structure in order to make the problem tractable. In particular  in the Lipschitz MAB problem there is an underlying similarity metric space  known to the algorithm  such that any two arms that are close in this metric space have similar payoffs.  In this paper we consider the more realistic scenario in which the metric space is *implicit* -- it is defined by the available structure but not revealed to the algorithm directly. Specifically  we assume that an algorithm is given a tree-based classification of arms. For any given problem instance such a classification implicitly defines a similarity metric space  but the numerical similarity information is not available to the algorithm. We provide an algorithm for this setting  whose performance guarantees (almost) match the best known guarantees for the corresponding instance of the Lipschitz MAB problem.,Multi-armed bandits on implicit metric spaces

Aleksandrs Slivkins

slivkins at microsoft.com

Microsoft Research Silicon Valley

Mountain View  CA 94043

Abstract

The multi-armed bandit (MAB) setting is a useful abstraction of many online
learning tasks which focuses on the trade-off between exploration and exploita-
tion. In this setting  an online algorithm has a ﬁxed set of alternatives (“arms”) 
and in each round it selects one arm and then observes the corresponding reward.
While the case of small number of arms is by now well-understood  a lot of re-
cent work has focused on multi-armed bandits with (inﬁnitely) many arms  where
one needs to assume extra structure in order to make the problem tractable. In
particular  in the Lipschitz MAB problem there is an underlying similarity metric
space  known to the algorithm  such that any two arms that are close in this metric
space have similar payoffs. In this paper we consider the more realistic scenario
in which the metric space is implicit – it is deﬁned by the available structure but
not revealed to the algorithm directly. Speciﬁcally  we assume that an algorithm
is given a tree-based classiﬁcation of arms. For any given problem instance such
a classiﬁcation implicitly deﬁnes a similarity metric space  but the numerical sim-
ilarity information is not available to the algorithm. We provide an algorithm for
this setting  whose performance guarantees (almost) match the best known guar-
antees for the corresponding instance of the Lipschitz MAB problem.

1

Introduction

In a multi-armed bandit (MAB) problem  a player is presented with a sequence of trials. In each
round  the player chooses one alternative from a set of alternatives (“arms”) based on the past history 
and receives the payoff associated with this alternative. The goal is to maximize the total payoff of
the chosen arms. The multi-armed bandit setting was introduced in 1950s and has since been studied
intensively since then in Operations Research  Economics and Computer Science  e.g. see [8] for
background. This setting is often used to model the tradeoffs between exploration and exploitation 
which is the principal issue in sequential decision-making under uncertainty.
One standard way to evaluate the performance of a multi-armed bandit algorithm is regret  deﬁned
as the difference between the expected payoff of an optimal arm and that of the algorithm. By
now the multi-armed bandit problem with a small ﬁnite number of arms is quite well understood
(e.g. see [22  3  2]). However  if the set of arms is exponentially or inﬁnitely large  the problem
becomes intractable  unless we make further assumptions about the problem instance. Essentially 
an MAB algorithm needs to ﬁnd a needle in a haystack; for each algorithm there are inputs on which
it performs as badly as random guessing.
The bandit problems with large sets of arms have received a considerable attention  e.g. [1  5  23 
12  21  10  24  25  11  4  16  20  7  19]. The common theme in these works is to assume a certain
structure on payoff functions. Assumptions of this type are natural in many applications  and often
lead to efﬁcient learning algorithms  e.g. see [18  8] for a background.

1

In particular  the line of work [1  17  4  20  7  19] considers the Lipschitz MAB problem  a broad
and natural bandit setting in which the structure is induced by a metric on the set of arms.1 In this
setting an algorithm is given a metric space (X D)  where X is the set of arms  which represents the
available similarity information (information on similarity between arms). Payoffs are stochastic:
the payoff from choosing arm x is an independent random sample with expectation µ(x). The metric
space is related to payoffs via the following Lipschitz condition:2

|µ(x) − µ(y)| ≤ D(x  y)

for all x  y ∈ X.

(1)

Performance guarantees consider regret R(t) as a function of time t  and focus on the asymptoti-
cal dependence of R(·) on a suitably deﬁned “dimensionality” of the problem instance (X D  µ).
Various upper and lower bounds of the form R(t) = ˜Θ(tγ)  γ < 1 have been proved.
We relax an important assumption in Lipschitz MAB that the available similarity information pro-
vides numerical values in the sense of (1).3 Speciﬁcally  following [21  24  25] we assume that an
algorithm is (only) given a taxonomy on arms: a tree-based classiﬁcation modeled by a rooted tree
T whose leaf set is X. The idea is that any two arms in the same subtree are likely to have sim-
ilar payoffs. Motivations include contextual advertising and web search with topical taxonomies 
e.g. [25  6  29  27]  Monte-Carlo planning [21  24]  and Computer Go [13  14].
We call the above formulation the Taxonomy MAB problem; a problem instance is a triple (X T   µ).
Crucially  in Taxonomy MAB no numerical similarity information is explicitly revealed. All prior
algorithms for Lipschitz MAB (and in particular  all algorithms in [20  7]) are parameterized by
some numerical similarity information  and therefore do not directly apply to Taxonomy MAB.
One natural way to quantify the extent of similarity between arms in a given subtree is via the
maximum difference in expected payoffs. Speciﬁcally  for each internal node v we deﬁne the width
of the corresponding subtree T (v) to be W(v) = supx y∈X(v) |µ(x) − µ(y)|  where X(v) is the set
of leaves in T (v). Note that the subtree widths are non-increasing from root to leaves. A standard
notion of distance induced by subtree widths  henceforth called implicit distance  is as follows:
Dimp(x  y) is the width of the least common ancestor of leaves x  y. It is immediate that this is
indeed a metric  and moreover that it satisﬁes (1). In fact  Dimp(x  y) is the smallest “width-based”
distance that satisﬁes (1). If the widths are strictly decreasing  T can be reconstructed from Dimp.
Thus  an instance (X T   µ) of Taxonomy MAB naturally induces an instance (X Dimp  µ) of Lip-
schitz MAB which (assuming the widths are strictly decreasing) encodes all relevant information.
The crucial distinction is that in Taxonomy MAB the metric space (X Dimp) is implicit: the subtree
widths are not revealed to the algorithm. In particular  the algorithms in [20  7] do not apply.
We view Lipschitz MAB as a performance benchmark for Taxonomy MAB. We are concerned with
the following question: can an algorithm for Taxonomy MAB perform as if it was given the implicit
metric space (X Dimp)? More formally  we ask whether it is possible to obtain guarantees for
Taxonomy MAB that (almost) match the state-of-art for Lipschitz MAB.
We answer this question in the afﬁrmative as long as the implicit metric space (X Dimp) has a small
doubling constant (see Section 2 for a milder condition). We provide an algorithm with guarantees
that are almost identical to those for the zooming algorithm in [20].4
Our algorithm proceeds by estimating subtree widths of near-optimal subtrees. Thus  we encounter
a two-pronged exploration-exploitation trade-off: samples from a given subtree reveal information
not only about payoffs but also about the width  whereas in Lipschitz MAB we only need to worry
about the payoffs. Dealing with this more complicated trade-off is the main new conceptual hurdle
(which leads to some technical complications such as the proof of Lemma 4.4). These complications
aside  our algorithm is similar to those in [17  20] in that it maintains a partition of the space of arms
into regions (in this case  subtrees) so that each region is treated as a “meta-arm”  and this partition
is adapted to the high-payoff regions.

1This problem has been explicitly deﬁned in [20]. Preceding work [1  17  9  4] considered a few special
cases such as a one-dimensional real interval with a metric deﬁned by D(x  y) = |x − y|α  α ∈ (0  1].
2Lipschitz constant is clip = 1 without loss of generality: else  one could take a metric clip × D.
3In the full version of [20] the setting is relaxed so that (1) needs to hold only if x is optimal  and the

distances between non-optimal points do not need to be explicitly known; [7] provides a similar result.

4The guarantees in [7] are similar but slightly different technically.

2

R(T ) (cid:44) µ∗T − E(cid:104)(cid:80)T

(cid:105)

= E(cid:104)(cid:80)T

(cid:105)

1.1 Preliminaries
The Taxonomy MAB problem and the implicit metric space (X Dimp) are deﬁned as in Section 1.
We assume stochastic payoffs [2]: in each round t the algorithm chooses a point x = xt ∈ X and
observes an independent random sample from a payoff distribution Ppayoff(x) with support [0  1]
and expectation µ(x).5 The payoff function µ : X → [0  1] is not revealed to an algorithm. The
goal is to minimize regret with respect to the best expected arm:

 

t=1 µ(xt)

t=1 ∆(xt)

(2)
where µ∗ (cid:44) supx∈X µ(x) is the maximal expected payoff  and ∆(x) (cid:44) µ∗ − µ(x)  is the “badness”
of arm x. An arm x ∈ X is called optimal if µ(x) = µ∗.
We will assume that the number of arms is ﬁnite (but possibly very large). Extension to inﬁnitely
many arms (which does not require new algorithmic ideas) is not included to simplify presentation.
Also  we will assume a known time horizon (total number of rounds)  denoted Thor.
Our guarantees are in terms of the zooming dimension [20] of (X Dimp  µ)  a concept that takes
into account both the dimensionality of the metric space and the “goodness” of the payoff function.
Below we specialize the deﬁnition from [20] to Taxonomy MAB.
Deﬁnition 1.1 (zooming dimension). For X(cid:48) ⊂ X  deﬁne the covering number N cov
(X(cid:48)) as the
smallest number of subtrees of width at most δ that cover X(cid:48). Let Xδ (cid:44) {x ∈ X : 0 < ∆(x) ≤ δ}.
The zooming dimension of a problem instance I = (X T   µ)  with multiplier c  is
δ/8 (Xδ) ≤ c δ−d ∀δ > 0}.
(3)
δ/8 (Xδ) ≤ c δ−d  and deﬁne the zooming dimen-
In other words  we consider a covering property N cov
sion as the smallest d such that this covering property holds for all δ > 0. The zooming dimension
essentially coincides with the covering dimension of (X D) 6 for the worst-case payoff function µ 
but can be (much) smaller when µ is “benign”. In particular  zooming dimension would “ignore” a
subtree with high covering dimension but signiﬁcantly sub-optimal payoffs.
The doubling constant cDBL of a metric space is the smallest k such that any ball can be covered by
k balls of half the radius. (In our case  any subtree can be covered by k subtrees of half the width.)
Doubling constant has been a standard notion in theoretical computer science literature since [15];
since then  it was used to characterize tractable problem instances for a variety of problems. It is
known that cDBL = O(2d) for any bounded subset S ⊂ Rd(cid:48)
of linear dimension d  under any metric
(cid:96)p  p ≥ 1. Moreover  cDBL ≥ c 2d if d is the covering dimension with multiplier c.

ZoomDim(I  c) (cid:44) inf{d ≥ 0 : N cov

δ

2 Statement of results

We will prove that our algorithm (TaxonomyZoom) satisﬁes the following regret bound:

For each instance I of Taxonomy MAB  each c > 0 and each T ≤ Thor 

d = ZoomDim(I  c).

R(T ) ≤ O(c KI log Thor)1/(2+d) × T 1−1/(2+d) 

(4)
We will bound the factor KI below. For KI = 1 this is the guarantee for the zooming algorithm
in [20] for the corresponding instance (X Dimp  µ) of Lipschitz MAB. Note that the deﬁnition of
zooming dimension allows a trade-off between c and d  and we obtain the optimal trade-off since (4)
holds for all values of c at once. Following the prior work on Lipschitz MAB  we identify the
exponent in (4) as the crucial parameter  as long as the multiplier c is sufﬁciently small.7
Our ﬁrst (and crude) bound for KI is in terms of the doubling constant of (X Dimp).
Theorem 2.1 (Crude bound). Given an upper bound c(cid:48)
TaxonomyZoom achieves (4) with KI = f (c(cid:48)

DBL on the doubling constant of (X Dimp) 

DBL) log |X|  where f (n) = n 2n.

5Other than support and expectation  the “shape” of Ppayoff(x) is not essential for this paper.
6Covering dimension is deﬁned as in (3)  replacing N cov
7One can reduce ZoomDim by making c huge  e.g. ZoomDim = 0 for c = |X|. However  this is not likely to

δ/8 (Xδ) with N cov

(X)..

δ

lead to useful regret bounds. Similar trade-off (dimension vs multiplier) is implicit in [7].

3

Our main result (which implies Theorem 2.1) uses a more efﬁcient bound for KI.
Recall that in Taxonomy MAB subtree widths are not revealed  and the algorithm has to use sam-
pling to estimate them. Informally  the taxonomy is useful for our purposes if and only if subtree
widths can be efﬁciently estimated using random sampling. We quantify this as a parameter called
quality  and bound KI in terms of this parameter.
We use simple random sampling: start at a tree node v and choose a branch uniformly at random at
each junction. Let P(u|v) be the probability that node u is reached starting from v. The probabilities
P(·|v) induce a distribution on X(v)  the leaf set of subtree T (v). A sample from this distribution

is called a random sample from T (v)  with expected payoff µ(v) (cid:44)(cid:80)

x∈X(v) µ(x) P(x|v).

Deﬁnition 2.2. The quality of the taxonomy for a given problem instance is the largest number
q ∈ (0  1) with the following property: for each subtree T (v) containing an optimal arm there exist
tree nodes u  u(cid:48) ∈ T (v) such that P(u|v) and P(u(cid:48)|v) are at least q and

|µ(u) − µ(u(cid:48))| ≥ 1

2 W(v).

(5)

2 in (5) is arbitrary; we ﬁx it for convenience.

One could use the pair u  u(cid:48) in Deﬁnition 2.2 to obtain reliable estimates for W(v). The deﬁnition
focuses on the difﬁculty of obtaining such pair via random sampling from T (v). The deﬁnition is
ﬂexible: it allows u and u(cid:48) to be at different depth (which is useful if node degrees are large and
non-uniform) and the widths of other internal nodes in T (v) cannot adversely impact quality. The
constant 1
For a particularly simple example  consider a binary taxonomy such that for each subtree T (v)
containing an optimal arm there exist grandchildren u  u(cid:48) of v that satisfy (5). For instance  such
u  u(cid:48) exist if the width of each grandchild of v is at most 1
Theorem 2.3 (Main result). Assume an lower bound q ≤ quality(I) is known.
TaxonomyZoom achieves (4) with KI = deg
q
Theorem 2.1 follows because  letting cDBL be the doubling constant of (X Dimp)  all node degrees
are at most cDBL and moreover quality ≥ 2−cDBL (we omit the proof from this version).

log |X|  where deg is the degree of the taxonomy.

4 W(v). Then quality ≥ 1
4.

Then

Discussion. The guarantee in Theorem 2.3 is instance-dependent: it depends on deg/quality
and ZoomDim  and is meaningful only if these quantities are small compared to the number of arms
(informally  we will call such problem instances “benign”). Also  the algorithm needs to know
a non-trivial lower bound on quality; very conservative estimates would not sufﬁce. However 
underestimating quality (and likewise overestimating Thor) is relatively inexpensive as long as the
“inﬂuence” of these parameters on regret is eventually dominated by the T 1−1/(2+d) term.
For benign problem instances  the beneﬁt of using the taxonomy is the vastly improved dependence
on the number of arms. Without a taxonomy or any other structure  regret of any algorithm for
stochastic MAB scales linearly in the number of (near-optimal) arms  for a sufﬁciently large t.
2 < ∆(x) ≤ δ. Then the worst-case regret
Speciﬁcally  let Nδ be the number of arms x such that δ
(over all problem instances) cannot be better than R(t) = min(δt  Ω( 1
An alternative approach to MAB problems on trees (without knowing the “widths”) are the “tree
bandit algorithms” explored in [21  24]. Here for each tree node v there is a separate  independent
copy of UCB1 [2] or a UCB1-style index algorithm (call it Av)  so that the “arms” for Av corre-
spond to children u of v  and selecting a child u in a given round corresponds to playing Au in
this round. [21  24] report successful empirical performance of such algorithms on some examples.
However  regret bounds for these algorithms do not scale as well with the number of arms: even if
the tree widths are given  then letting ∆min (cid:44) minx∈X: ∆(x)>0 ∆(x)  the regret bound is propor-
tional to |Xδ|/∆min (where Xδ is as in Deﬁnition 1.1)  whereas the regret bound in Theorem 2.3 is
(essentially) in terms of the covering numbers N cov

δ Nδ)). 8

δ/8 (Xδ).

8This is implicit from the lower-bounding analysis in [22] and [3].

4

3 Main algorithm

Our algorithm TaxonomyZoom(Thor  q) is parameterized by the time horizon Thor and the quality
parameter q ≤ quality. In each round the algorithm selects one of the tree nodes  say v  and plays
a randomly sampled arm x from T (v). We say that a subtree T (u) is hit in this round if u ∈ T (v)
and x ∈ T (u). For each tree node v and time t  let nt(v) be the number of times the subtree T (v)
has been hit by the algorithm before time t  and let µt(v) be the corresponding average reward. Note
that E[µt(v)| nt(v) > 0] = µ(v). Deﬁne the conﬁdence radius of v at time t as

radt(v) (cid:44)(cid:112)8 log(Thor|X|) / (2 + nt(v)).

(6)

(7)

(8)

The meaning of the conﬁdence radius is that |µt(v) − µ(v)| ≤ radt(v) with high probability.
For each tree node v and time t  let us deﬁne the index of v at time t as

It(v) (cid:44) µt(v) + (1 + 2 kA) radt(v)  where kA (cid:44) 4(cid:112)2/q.

Here we posit µt(v) = 0 if nt(v) = 0. Let us deﬁne the width estimate9

Wt(v) (cid:44) max(0  Ut(v) − Lt(v))  where

(cid:26) Ut(v) (cid:44) maxu∈T (v)  s≤t µs(u) − rads(u) 

Lt(v) (cid:44) minu∈T (v)  s≤t µs(u) + rads(u).

Here Ut(v) is the best available lower conﬁdence bound on maxx∈X(v) µ(x)  and Lt(v) is the best
available upper conﬁdence bound on minx∈X(v) µ(x). If both bounds hold then Wt(v) ≤ W(v).
Throughout the phase  some tree nodes are designated active. We maintain the following invariant:

Wt(v) < kA radt(v) for each active internal node v.

(9)

TaxonomyZoom(Thor  q ) operates as follows. Initially the only active tree node is the root. In each
round  the algorithm performs the following three steps:

(S1) While Invariant (9) is violated by some v  de-activate v and activate all its children.
(S2) Select an active tree node v with the maximal index (7)  breaking ties arbitrarily.
(S3) Play a randomly sampled arm from T (v).

Note that each arm is activated and deactivated at most once.

Implementation details.
If an explicit representation of the taxonomy can be stored in memory 
then the following simple implementation is possible. For each tree node v  we store several statis-
tics: nt  µt  Ut and Lt. Further  we maintain a linked list of active nodes  sorted by the index.
Suppose in a given round t  a subtree v is chosen  and an arm x is played. We update the statistics
by going up the x → v path in the tree (note that only the statistics on this path need to be updated).
This update can be done in time O(depth(x)). Then one can check whether Invariant (9) holds for
a given node in time O(1). So step (S1) of the algorithm can be implemented in time O(1 + N ) 
where N is the number of nodes activated during this step. Finally  the linked list of active nodes
can be updated in time O(1 + N ). Then the selections in steps (S2) and (S3) are done in time O(1).
Lemma 3.1. TaxonomyZoom can be implemented with O(1) storage per each tree node  so that in
each round the time complexity is O(N + depth(x))  where N is the number of arms activated in
step (S1)  and x is the arm chosen in step (S3).

Sometimes it may be feasible (and more space-efﬁcient) to represent the taxonomy implicitly  so that
a tree node is expanded only if needed. Speciﬁcally  suppose the following interface is provided:
given a tree node v  return all its children and an arbitrary arm x ∈ T (v). Then TaxonomyZoom can
be implemented so that it only stores the statistics for each node u such that P(u|v) ≥ q for some
active node v (rather than for all tree nodes).10 The running times are as in Lemma 3.1.

9Deﬁning Ut  Lt in (8) via s ≤ t (rather than s = t) improves performance  but is not essential for analysis.
10The algorithm needs to be modiﬁed slightly; we leave the details to the full version.

5

4 Analysis: proof of Theorem 2.3
First  let us ﬁx some notation. We will focus on regret up to a ﬁxed time T ≤ Thor. In what follows 
let d = ZoomDim(I  c) for some ﬁxed c > 0. Recall the notation Xδ (cid:44) {x ∈ X : ∆(x) ≤ δ} from
Deﬁnition 1.1. Here δ is the “ distance scale”; we will be interested in δ ≥ δ0  for

δ0 (cid:44) ( K

T )1/(d+2)  where K (cid:44) O(c deg k2A log Thor).

(10)

We identify a certain high-probability behavior of the algorithm  and argue deterministically condi-
tional on the event that this behavior actually holds.
Deﬁnition 4.1. An execution of TaxonomyZoom is called clean if for each time t ≤ T and all tree
nodes v the following two properties hold:

(P1) |µt(v) − µ(v)| ≤ radt(v) as long as nt(v) > 0.
(P2) If u ∈ T (v) then

nt(v)P(u|v) ≥ 8 log T ⇒ nt(u) ≥ 1

2 nt(v)P(u|v).

Note that in a clean execution the quantities in (8) satisfy the desired high-conﬁdence bounds:
Ut(v) ≤ maxx∈X(v) µ(x) and Lt(v) ≥ minx∈X(v) µ(x)  which implies W(v) ≥ Wt(v).
Lemma 4.2. An execution of TaxonomyZoom is clean with probability at least 1 − 2 T −2
hor.

Proof. For part (P1)  ﬁx a tree node v and let ζj to be the payoff in the j-th round that v has been
j=1(ζj − µ(v))}n=1..T is a martingale.11 Let ¯ζn (cid:44) 1
j=1 ζj be the n-th average.
Then by the Azuma-Hoeffding inequality  for any n ≤ Thor we have:

hit. Then {(cid:80)n
Pr[|¯ζn − µ(v)| > r(n)] ≤ (Thor |X|)−2  where r(n) (cid:44)(cid:112)8 log(Thor|X|) / (2 + n).

(11)
Note that radt(v) = r(nt(v)). We obtain (P1) by taking the Union Bound for (11) over all nodes v
and all n ≤ T . (This is the only place where we use the log |X| term in (6).)
Part (P2) is proved via a similar application of martingales and Azuma-Hoeffding inequality.
From now on we will argue about a clean execution. Recall that by deﬁnition of W(·) 

(cid:80)n

n

µ(v) ≤ µ(u) + W(v)

for any tree node u ∈ T (v).

(12)

The crux of the proof of Theorem 2.3 is that at all times the maximal index is at least µ∗.
Lemma 4.3. Consider a clean execution of TaxonomyZoom(Thor  q). Then the following holds: in
any round t ≤ Thor  at any point in the execution such that the invariant (9) holds  there exists an
active tree node v∗ such that It(v∗) ≥ µ∗.
Proof. Fix an optimal arm x∗ ∈ X. Note that in each round t  there exist an active tree node v∗
such that x∗ ∈ T (v∗). (One can prove it by induction on t  using the (de)activation rule (S1) in
TaxonomyZoom.) Fix round t and the corresponding tree node v∗ = v∗
t .
By Deﬁnition 2.2  there exist v0  v1 ∈ Tq(v∗) such that |µ(v1) − µ(v0)| ≥ W(v∗)/2.
Assume that ∆ (cid:44) W(v∗) > 0  and deﬁne f (∆) = 83 log(Thor) ∆−2. Then for each tree node v

radt(v) ≤ ∆/8 ⇐⇒ nt(v) ≥ f (∆).
(13)
Now  for the sake of contradiction let us suppose that nt(v∗) ≥ ( 1
4 kA)2 f (∆). By (13)  this is
equivalent to ∆ ≥ 2 kA radt(v∗). Note that nt(v∗) ≥ (2/q) f (∆) by our assumption on kA  so
by property (P2) in the deﬁnition of the clean execution  for each node vj  j ∈ {0  1} we have
nt(vj) ≥ f (∆)  which implies radt(vj) ≤ ∆/8. Therefore (8) gives a good estimate of W(v∗) 
namely Wt(v∗) ≥ ∆/4. It follows that Wt(v∗) ≥ kA radt(v∗)  which violates Invariant (9).
We proved that W(v∗) ≤ 2 kA radt(v∗). Using (12)  we have ∆(v∗) ≤ W(v∗) < 2 kA radt(v∗) and
(14)

It(v∗) ≥ µ(v∗) + 2 kA radt(v∗) ≥ µ∗ 

t

where the ﬁrst inequality in (14) holds by deﬁnition (7) and property (P1) of a clean execution.

11To make ζn well-deﬁned for any n ≤ Thor  consider a hypothetical algorithm which coincides with

TaxonomyZoom for the ﬁrst Thor rounds and then proceeds so that each tree node is selected Thor times.

6

We use Lemma 4.3 to show that the algorithm does not activate too many tree nodes with large
badness ∆(·)  and each such node is not played too often. For each tree node v  let N (v) be the
number of times node v was selected in step (S2) of the algorithm. Call v positive if N (v) > 0. We
partition all positive tree nodes and all deactivated tree nodes into sets

Si = {positive tree nodes v : 2−i < ∆(v) ≤ 2−i+1} 
i = {deactivated tree nodes v : 2−i < 4 W(v) ≤ 2−i+1}.
S∗
Lemma 4.4. Consider a clean execution of algorithm TaxonomyZoom(Thor  q ).

(a) for each tree node v we have N (v) ≤ O(k2A log Thor) ∆−2(v).
(b) if node v is de-activated at some point in the execution  then ∆(v) ≤ 4 W(v).
(c) For each i  |S∗
(d) For each i  |Si| ≤ O(deg Ki+1).

i | ≤ 2Ki  where Ki (cid:44) c 2(i+1) d.

Proof. For part (a)  ﬁx an arbitrary tree node v and let t be the last time v was selected in step (S2)
of the algorithm. By Lemma 4.3  at that point in the execution there was a tree node v∗ such that
It(v∗) ≥ µ∗. Then using the selection rule (step (S2)) and the deﬁnition of index (7)  we have

µ∗ ≤ It(v∗) ≤ It(v) ≤ µ(v) + (2 + 2 kA) radt(v) 

∆(v) ≤ (2 + 2 kA) radt(v).
N (v) ≤ nt(v) ≤ O(k2A log Thor) ∆−2(v).

(15)

For part (b)  suppose tree node v was de-activated at time s. Let t be the last round in which v was
selected. Then

3 (2 + 2 kA) radt(v) ≥ 1

W(v) ≥ Ws(v) ≥ kA rs(v) ≥ 1

(16)
Indeed  the ﬁrst inequality in (16) holds since we are in a clean execution  the second inequality
in (16) holds because v was de-activated  the third inequality holds because ns(v) = nt(v) + 1  and
the last inequality in (16) holds by (15).
For part (c)  let us ﬁx i and deﬁne Yi = {x ∈ X : ∆(x) ≤ 2−i+1}. By Deﬁnition 1.1  this set can
be covered by Ki subtrees T (v1)  . . .  T (vKi)  each of width < 2−i/4. Fix a deactivated tree node
v ∈ S∗

i . For each arm x ∈ X in subtree T (v) we have  by part (b) 

3 ∆(v).

∆(x) ≤ ∆(v) + W(v) ≤ 4 W(v) ≤ 2−i+1 

i | ≤ 2Ki  proving part (c).

so x ∈ Yi and therefore is contained in some T (vj). Note that vj ∈ T (v) since W(v) > W(vj). It
follows that the subtrees T (v1)  . . .  T (vK) cover the leaf set of T (v).
i ∪ {v1  . . .   vK}  where two nodes u  v are connected by a
Consider the graph G on the node set S∗
directed edge (u  v) if there is a path from u to v in the tree T . This is a directed forest of out-degree
at least 2  whose leaf set is a subset of {v1  . . .   vKi}. Since in any directed tree of out-degree ≥ 2
the number of nodes is at most twice the number of leaves  G contains at most Ki internal nodes.
Thus |S∗
For part (d)  let us ﬁx i and consider a positive tree node u ∈ Si. Since N (u) > 0  either u is active
at time Thor  or it was deactivated in some round before Thor. In the former case  let v be the parent
of u. In the latter case  let v = u. Then by part (b) we have 2−i ≤ ∆(u) ≤ ∆(v) + W(v) ≤ 4 W(v) 
so v ∈ S∗
For each tree node v  deﬁne its family as the set which consists of u itself and all its children.
We have proved that each positive node u ∈ Si belongs to the family of some deactivated node
v ∈ ∪i+1

j . Since each family consists of at most 1 + deg nodes  it follows that

j for some j ≤ i + 1.

j=1S∗

j=1Kj) ≤ O(deg Ki+1).

|Si| ≤ (1 + deg) ((cid:80)i+1
N (v) ∆(v) ≤ O(k2A log Thor)(cid:80)

v∈Si

(cid:80)

v∈Si

Proof of Theorem 2.3: The theorem follows Lemma 4.4(ad). Let us assume a clean execution.
(Recall that by Lemma 4.2 the failure probability is sufﬁciently small to be neglected.) Then:

∆(v) ≤ O(k2A log Thor)|Si| 2i ≤ K 2(i+2)(1+d) 

1

7

where K is deﬁned in (10). For any δ0 = 2−i0 we have

R(T ) ≤(cid:80)
(cid:16)(cid:80)

tree nodes v N (v) ∆(v)

(cid:16)(cid:80)

v: ∆(v)<δ0

=
≤ δ0T +
i≤i0
≤ δ0T + O(K) ( 8

δ0

(cid:80)

v∈Si
)(1+d).

(cid:17)

(cid:16)(cid:80)
(cid:17) ≤ δ0T +(cid:80)

v: ∆(v)≥δ0

N (v) ∆(v)

(cid:17)

N (v) ∆(v)

+

N (v) ∆(v)

i≤i0

K 2(i+2)(1+d)

We obtain the desired regret bound (4) by setting δ0 as in (10).

5

(De)parameterizing the algorithm

Recall that TaxonomyZoom needs to be parameterized by Thor and q. dependence on the param-
eters can be removed using a suitable version of the standard doubling trick: consider a “meta-
algorithm” that proceeds in phases so that in each phase i = 1  2  3  . . . a fresh instance of
TaxonomyZoom(2i  qi) is run for 2i rounds  where qi slowly decreases with i. For instance  if we
take qi = 2−αi for some α ∈ (0  1) then this meta-algorithm has regret

R(T ) ≤ O(c deg log T )1/(2+d) × T 1−(1−α)/(2+d)

∀T ≥ quality

−1/α

where d = ZoomDim(I  c)  for any given c > 0.
While the doubling trick is very useful in theory of online decision problems  its practical importance
is questionable  as running a fresh algorithm instance in each phase seems unnecessarily wasteful.
We conjecture that in practice one could run a single instance of the algorithm while gradually
increasing Thor and decreasing q. However  providing provable guarantees for this modiﬁed algo-
rithm seems beyond the current techniques. In particular  extending a much simpler analysis of the
zooming algorithm [20] to arbitrary time horizon remains a challenge.12
Further  we conjecture that TaxonomyZoom will typically work in practice even if the parameters are
misspeciﬁed  i.e. even if Thor is too low and q is too optimistic. Indeed  recall that our algorithm
is index-based  in the style of UCB1 [2]. The only place where the parameters are invoked is in
the deﬁnition of the index (7)  namely in the constant in front of the exploration term. It has been
observed in [28  29] that in a related MAB setting  reducing this constant to 1 from the theoretically
mandated Θ(log T )-type term actually improves algorithms’ performance in simulations.

(17)

6 Conclusions

In this paper  we have extended previous multi-armed bandit learning algorithms with large numbers
of available strategies. Whereas the most effective previous approaches rely on explicitly knowing
the distance between available strategies  we consider the case where the distances are implicit in a
hierarchy of available strategies. We have provided a learning algorithm for this setting  and show
that its performance almost matches the best known guarantees for the Lipschitz MAB problem.
Further  we have shown how our approach results in stronger provable guarantees than alternative
algorithms such as tree bandit algorithms [21  24].
We conjecture that the dependence on quality (or some version thereof) is necessary for the worst-
case regret bounds  even if ZoomDim is low. It is an open question whether there are non-trivial
families of problem instances with low quality for which one could achieve low regret.
Our results suggest some natural extensions. Most interestingly  a number of applications recently
posed as MAB problems over large sets of arms – including learning to rank online advertisements
or web documents (e.g. [26  29]) – naturally involve choosing among arms (e.g. ads) that can be clas-
siﬁed according to any of a number of hierarchies (e.g. by class of product sold  geographic location 
etc). In particular  such different hierarchies may be of different usefulness. Selecting among  or
combining from  a set of available hierarchical representations of arms poses interesting challenges.
More generally  we would like to generalize Theorem 2.3 to other structures that implicitly deﬁne
a metric space on arms (in the sense of (1)). One speciﬁc target would be directed acyclic graphs.
While our algorithm is well-deﬁned for this setting  the theoretical analysis does not apply.

12However  [7] obtains similar guarantees for arbitrary time horizon  with a different algorithm.

8

References
[1] Rajeev Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization  33(6):1926–

1951  1995.

[2] Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002. Preliminary version in 15th ICML  1998.

[3] Peter Auer  Nicol`o Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM J. Comput.  32(1):48–77  2002. Preliminary version in 36th IEEE FOCS  1995.

[4] Peter Auer  Ronald Ortner  and Csaba Szepesv´ari. Improved Rates for the Stochastic Continuum-Armed

Bandit Problem. In 20th COLT  pages 454–468  2007.

[5] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. of Computer

and System Sciences  74(1):97–114  February 2008. Preliminary version in 36th ACM STOC  2004.

[6] Andrei Broder  Marcus Fontoura  Vanja Josifovski  and Lance Riedel. A semantic approach to contextual

advertising. In 30th SIGIR  pages 559–566  2007.

[7] S´ebastien Bubeck  R´emi Munos  Gilles Stoltz  and Csaba Szepesvari. Online Optimization in X-Armed
Bandits. J. of Machine Learning Research (JMLR)  12:1587–1627  2011. Preliminary version in NIPS
2008.

[8] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  learning  and games. Cambridge Univ. Press  2006.
[9] Eric Cope. Regret and convergence bounds for immediate-reward reinforcement learning with continuous

action spaces. IEEE Trans. on Automatic Control  54(6):1243–1253  2009. A manuscript from 2004.

[10] Varsha Dani and Thomas P. Hayes. Robbing the bandit: less regret in online geometric optimization

against an adaptive adversary. In 17th ACM-SIAM SODA  pages 937–943  2006.

[11] Varsha Dani  Thomas P. Hayes  and Sham Kakade. The Price of Bandit Information for Online Optimiza-

tion. In 20th NIPS  2007.

[12] Abraham Flaxman  Adam Kalai  and H. Brendan McMahan. Online Convex Optimization in the Bandit

Setting: Gradient Descent without a Gradient. In 16th ACM-SIAM SODA  pages 385–394  2005.

[13] Sylvain Gelly and David Silver. Combining online and ofﬂine knowledge in UCT. In 24th ICML  2007.
[14] Sylvain Gelly and David Silver. Achieving master level play in 9x9 computer go. In 23rd AAAI  2008.
[15] Anupam Gupta  Robert Krauthgamer  and James R. Lee. Bounded geometries  fractals  and low–

distortion embeddings. In 44th IEEE FOCS  pages 534–543  2003.

[16] Sham M. Kakade  Adam T. Kalai  and Katrina Ligett. Playing Games with Approximation Algorithms.

In 39th ACM STOC  2007.

[17] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In 18th NIPS  2004.
[18] Robert Kleinberg. Online Decision Problems with Large Strategy Sets. PhD thesis  MIT  2005.
[19] Robert Kleinberg and Aleksandrs Slivkins. Sharp Dichotomies for Regret Minimization in Metric Spaces.

In 21st ACM-SIAM SODA  2010.

[20] Robert Kleinberg  Aleksandrs Slivkins  and Eli Upfal. Multi-Armed Bandits in Metric Spaces. In 40th

ACM STOC  pages 681–690  2008.

[21] Levente Kocsis and Csaba Szepesvari. Bandit Based Monte-Carlo Planning. In 17th ECML  pages 282–

293  2006.

[22] T.L. Lai and Herbert Robbins. Asymptotically efﬁcient Adaptive Allocation Rules. Advances in Applied

Mathematics  6:4–22  1985.

[23] H. Brendan McMahan and Avrim Blum. Online Geometric Optimization in the Bandit Setting Against

an Adaptive Adversary. In 17th COLT  pages 109–123  2004.

[24] R´emi Munos and Pierre-Arnaud Coquelin. Bandit algorithms for tree search. In 23rd UAI  2007.
[25] Sandeep Pandey  Deepak Agarwal  Deepayan Chakrabarti  and Vanja Josifovski. Bandits for Taxonomies:

A Model-based Approach. In SDM  2007.

[26] Sandeep Pandey  Deepayan Chakrabarti  and Deepak Agarwal. Multi-armed Bandit Problems with De-

pendent Arms. In 24th ICML  2007.

[27] Susan T. Dumais Paul N. Bennett  Krysta Marie Svore. Classiﬁcation-enhanced ranking. In 19th WWW 

pages 111–120  2010.

[28] Filip Radlinski  Robert Kleinberg  and Thorsten Joachims. Learning diverse rankings with multi-armed

bandits. In 25th ICML  pages 784–791  2008.

[29] Aleksandrs Slivkins  Filip Radlinski  and Sreenivas Gollapudi. Learning optimally diverse rankings over

large document collections. In 27th ICML  pages 983–990  2010.

9

,Francesco Orabona
Michael Andersen
Ole Winther
Lars Hansen
Corinna Cortes
Giulia DeSalvo
Mehryar Mohri
Bo-Jian Hou
Lijun Zhang
Zhi-Hua Zhou
Horia Mania
Aurelia Guy
Benjamin Recht
Rui Li