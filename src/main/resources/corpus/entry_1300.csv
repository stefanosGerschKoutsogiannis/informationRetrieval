2009,Data-driven calibration of linear estimators with minimal penalties,This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression  the choice of a regularization parameter in kernel ridge regression or spline smoothing  and the choice of a kernel in multiple kernel learning. We propose a new algorithm which first estimates consistently the variance of the noise  based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then  plugging our variance estimate in Mallows $C_L$ penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.,Data-driven calibration of linear estimators

with minimal penalties

Francis Bach †

INRIA ; Willow Project-Team
Laboratoire d’Informatique de
l’Ecole Normale Superieure

(CNRS/ENS/INRIA UMR 8548)

Sylvain Arlot ∗

CNRS ; Willow Project-Team
Laboratoire d’Informatique de
l’Ecole Normale Superieure

(CNRS/ENS/INRIA UMR 8548)

23  avenue d’Italie  F-75013 Paris  France

23  avenue d’Italie  F-75013 Paris  France

sylvain.arlot@ens.fr

francis.bach@ens.fr

Abstract

This paper tackles the problem of selecting among several linear estimators in
non-parametric regression; this includes model selection for linear regression  the
choice of a regularization parameter in kernel ridge regression or spline smooth-
ing  and the choice of a kernel in multiple kernel learning. We propose a new
algorithm which ﬁrst estimates consistently the variance of the noise  based upon
the concept of minimal penalty which was previously introduced in the context of
model selection. Then  plugging our variance estimate in Mallows’ CL penalty
is proved to lead to an algorithm satisfying an oracle inequality. Simulation ex-
periments with kernel ridge regression and multiple kernel learning show that the
proposed algorithm often improves signiﬁcantly existing calibration procedures
such as 10-fold cross-validation or generalized cross-validation.

1 Introduction

Kernel-based methods are now well-established tools for supervised learning  allowing to perform
various tasks  such as regression or binary classiﬁcation  with linear and non-linear predictors [1  2].
A central issue common to all regularization frameworks is the choice of the regularization parame-
ter: while most practitioners use cross-validation procedures to select such a parameter  data-driven
procedures not based on cross-validation are rarely used. The choice of the kernel  a seemingly
unrelated issue  is also important for good predictive performance: several techniques exist  either
based on cross-validation  Gaussian processes or multiple kernel learning [3  4  5].

In this paper  we consider least-squares regression and cast these two problems as the problem of
selecting among several linear estimators  where the goal is to choose an estimator with a quadratic
risk which is as small as possible. This problem includes for instance model selection for linear
regression  the choice of a regularization parameter in kernel ridge regression or spline smoothing 
and the choice of a kernel in multiple kernel learning (see Section 2).

The main contribution of the paper is to extend the notion of minimal penalty [6  7] to all discrete
classes of linear operators  and to use it for deﬁning a fully data-driven selection algorithm satisfying
a non-asymptotic oracle inequality. Our new theoretical results presented in Section 4 extend simi-
lar results which were limited to unregularized least-squares regression (i.e.  projection operators).
Finally  in Section 5  we show that our algorithm improves the performances of classical selection
procedures  such as GCV [8] and 10-fold cross-validation  for kernel ridge regression or multiple
kernel learning  for moderate values of the sample size.

∗http://www.di.ens.fr/∼arlot/
†http://www.di.ens.fr/∼fbach/

1

2 Linear estimators

In this section  we deﬁne the problem we aim to solve and give several examples of linear estimators.

2.1 Framework and notation
Let us assume that one observes

Yi = f (xi) + εi ∈ R

for

i = 1 . . . n  

2   where ∀t ∈ Rn   we denote by ktk2 the ℓ2-norm of t   deﬁned as ktk2

i ] = σ2 unknown  f is an unknown
where ε1  . . .   εn are i.i.d. centered random variables with E[ε2
measurable function X 7→ R and x1  . . .   xn ∈ X are deterministic design points. No assumption
is made on the set X . The goal is to reconstruct the signal F = (f (xi))1≤i≤n ∈ Rn   with some
estimator bF ∈ Rn   depending only on (x1  Y1)  . . .   (xn  Yn)   and having a small quadratic risk
n−1kbF − Fk2
i .
i=1 t2
In this paper  we focus on linear estimators bF that can be written as a linear function of Y =
(Y1  . . .   Yn) ∈ Rn   that is  bF = AY   for some (deterministic) n × n matrix A . Here and in

the rest of the paper  vectors such as Y or F are assumed to be column-vectors. We present in
Section 2.2 several important families of estimators of this form. The matrix A may depend on
x1  . . .   xn (which are known and deterministic)  but not on Y   and may be parameterized by certain
quantities—usually regularization parameter or kernel combination weights.

2 :=Pn

2.2 Examples of linear estimators
In this paper  our theoretical results apply to matrices A which are symmetric positive semi-deﬁnite 
such as the ones deﬁned below.
Ordinary least-squares regression / model selection.

If we consider linear predictors from a

design matrix X ∈ Rn×p   then bF = AY with A = X(X ⊤X)−1X ⊤   which is a projection matrix
(i.e.  A⊤A = A); bF = AY is often called a projection estimator. In the variable selection setting 
one wants to select a subset J ⊂ {1  . . .   p}   and matrices A are parameterized by J .
Kernel ridge regression / spline smoothing. We assume that a positive deﬁnite kernel k : X ×
X → R is given  and we are looking for a function f : X → R in the associated reproducing kernel
Hilbert space (RKHS) F   with norm k · kF . If K denotes the n × n kernel matrix  deﬁned by
Kab = k(xa  xb)   then the ridge regression estimator—a.k.a. spline smoothing estimator for spline
kernels [9]—is obtained by minimizing with respect to f ∈ F [2]:
F .

(Yi − f (xi))2 + λkfk2

nXi=1
The unique solution is equal to bf =Pn

1
n

i=1 αik(·  xi)   where α = (K + nλI)−1Y . This leads to the
smoothing matrix Aλ = K(K + nλIn)−1   parameterized by the regularization parameter λ ∈ R+ .
Multiple kernel learning / Group Lasso / Lasso. We now assume that we have p different
kernels kj   feature spaces Fj and feature maps Φj : X → Fj   j = 1  . . .   p . The group Lasso [10]
and multiple kernel learning [11  5] frameworks consider the following objective function

J(f1  . . .   fp) = 1
n

nXi=1(cid:0)yi−Pp

j=1hfj  Φj(xi)i(cid:1)2

pXj=1

+2λ

kfjkFj = L(f1  . . .   fp)+2λ

kfjkFj

.

pXj=1

1

Using a1/2 = minb>0
j=1 kfjk = minη∈Rp

Note that when Φj(x) is simply the j-th coordinate of x ∈ Rp   we get back the penalization by the
ℓ1-norm and thus the regular Lasso [12].
2{ a
b + b}   we obtain a variational formulation of the sum of norms
+ ηjo . Thus  minimizing J(f1  . . .   fp) with respect to
j=1n kfj k2
+Pp
pXj=1
pXj=1

2Pp
(f1  . . .   fp) is equivalent to minimizing with respect to η ∈ Rp
y⊤(cid:0)Pp

j=1 ηjKj + nλIn(cid:1)−1

+ (see [5] for more details):

L(f1  . . .   fp) + λ

kfjk2
ηj

pXj=1

min

f1 ... fp

+ λ

ηj =

y + λ

ηj  

1
n

ηj

2

Aη λ = (Pp

j=1 ηjKj)(Pp

where In is the n × n identity matrix. Moreover  given η   this leads to a smoothing matrix of the
form
(1)

j=1 ηjKj + nλIn)−1  

parameterized by the regularization parameter λ ∈ R+ and the kernel combinations in Rp
that it depends only on λ−1η   which can be grouped in a single parameter in Rp
Thus  the Lasso/group lasso can be seen as particular (convex) ways of optimizing over η .
In
this paper  we propose a non-convex alternative with better statistical properties (oracle inequality
in Theorem 1). Note that in our setting  ﬁnding the solution of the problem is hard in general
since the optimization is not convex. However  while the model selection problem is by nature
combinatorial  our optimization problems for multiple kernels are all differentiable and are thus
amenable to gradient descent procedures—which only ﬁnd local optima.
Non symmetric linear estimators. Other linear estimators are commonly used  such as nearest-
neighbor regression or the Nadaraya-Watson estimator [13]; those however lead to non symmetric
matrices A   and are not entirely covered by our theoretical results.

+—note

+ .

3 Linear estimator selection

In this section  we ﬁrst describe the statistical framework of linear estimator selection and introduce
the notion of minimal penalty.

3.1 Unbiased risk estimation heuristics

The best choice would be the oracle:

this paper is then to select one of them  that is  to choose a matrix A . Let us assume that a family
of matrices (Aλ)λ∈Λ is given (examples are shown in Section 2.2)  hence a family of estimators

Usually  several estimators of the form bF = AY can be used. The problem that we consider in
(bFλ)λ∈Λ can be used  with bFλ := AλY . The goal is to choose from data somebλ ∈ Λ   so that the
quadratic risk of bFbλ is as small as possible.
λ⋆ ∈ arg min
data-drivenbλ satisfying an oracle inequality

which cannot be used since it depends on the unknown signal F . Therefore  the goal is to deﬁne a

λ∈Λn n−1kbFλ − Fk2
2o  
λ∈Λn n−1kbFλ − Fk2

with large probability  where the leading constant Cn should be close to 1 (at least for large n) and
the remainder term Rn should be negligible compared to the risk of the oracle.

n−1kbFbλ − Fk2

2o + Rn  

2 ≤ Cn inf

(2)

minimizes a criterion crit(λ) such that

Many classical selection methods are built upon the “unbiased risk estimation” heuristics: If bλ

∀λ ∈ Λ 

E [ crit(λ) ] ≈ Eh n−1kbFλ − Fk2
2i  

validation [14  15] and generalized cross-validation (GCV) [8] are built upon this heuristics.

thenbλ satisﬁes an oracle inequality such as in Eq. (2) with large probability. For instance  cross-

One way of implementing this heuristics is penalization  which consists in minimizing the sum of
the empirical risk and a penalty term  i.e.  using a criterion of the form:

2 + pen(λ) .

The unbiased risk estimation heuristics  also called Mallows’ heuristics  then leads to the ideal
(deterministic) penalty

crit(λ) = n−1kbFλ − Y k2
penid(λ) := Eh n−1kbFλ − Fk2

3

2i − Eh n−1kbFλ − Y k2
2i .

When bFλ = AλY   we have:
2 + kAλεk2
kbFλ − Fk2
kbFλ − Y k2
where ε = Y − F ∈ Rn and ∀t  u ∈ Rn   ht  ui =Pn

2 = k(Aλ − In)Fk2
2 = kbFλ − Fk2

matrix σ2In   Eq. (3) and Eq. (4) imply that

2 + kεk2

2 + 2hAλε  (Aλ − In)Fi  

(3)

2 − 2hε  Aλεi + 2hε  (In − Aλ)Fi  

(4)
i=1 tiui . Since ε is centered with covariance

penid(λ) =

2σ2 tr(Aλ)

n

 

(5)

2] = −σ2   which can be dropped off since it does not vary with λ .

up to the term −E[n−1kεk2
Note that df(λ) = tr(Aλ) is called the effective dimensionality or degrees of freedom [16]  so that
the ideal penalty in Eq. (5) is proportional to the dimensionality associated with the matrix Aλ—
for projection matrices  we get back the dimension of the subspace  which is classical in model
selection.

The expression of the ideal penalty in Eq. (5) led to several selection procedures  in particular Mal-
lows’ CL (called Cp in the case of projection estimators) [17]  where σ2 is replaced by some esti-

matorcσ2 . The estimator of σ2 usually used with CL is based upon the value of the empirical risk at

some λ0 with df(λ0) large; it has the drawback of overestimating the risk  in a way which depends
on λ0 and F [18]. GCV  which implicitly estimates σ2   has the drawback of overﬁtting if the family
(Aλ)λ∈Λ contains a matrix too close to In [19]; GCV also overestimates the risk even more than CL
for most Aλ (see (7.9) and Table 4 in [18]).
In this paper  we deﬁne an estimator of σ2 directly related to the selection task which does not have
similar drawbacks. Our estimator relies on the concept of minimal penalty  introduced by Birg´e and
Massart [6] and further studied in [7].

3.2 Minimal and optimal penalties
We deduce from Eq. (3) the bias-variance decomposition of the risk:
tr(A⊤
λ Aλ)σ2
n

2 +

Eh n−1kbFλ − Fk2
Eh n−1kbFλ − Y k2

2i = n−1 k(Aλ − In)Fk2
2 − kεk2

2i = n−1 k(Aλ − In)Fk2

and from Eq. (4) the expectation of the empirical risk:

= bias + variance  

(6)

2 −(cid:0) 2 tr(Aλ) − tr(A⊤

λ Aλ)(cid:1) σ2

n

.

(7)

Note that the variance term in Eq. (6) is not proportional to the effective dimensionality df(λ) =
tr(Aλ) but to tr(A⊤
λ Aλ) . Although several papers argue these terms are of the same order (for
instance  they are equal when Aλ is a projection matrix)  this may not hold in general. If Aλ is
symmetric with a spectrum Sp(Aλ) ⊂ [0  1]   as in all the examples of Section 2.2  we only have

0 ≤ tr(A⊤

λ Aλ) ≤ tr(Aλ) ≤ 2 tr(Aλ) − tr(A⊤

λ Aλ) ≤ 2 tr(Aλ) .

(8)

In order to give a ﬁrst intuitive interpretation of Eq. (6) and Eq. (7)  let us consider the kernel ridge
regression example and assume that the risk and the empirical risk behave as their expectations
in Eq. (6) and Eq. (7); see also Fig. 1. Completely rigorous arguments based upon concentration
inequalities are developed in [20] and summarized in Section 4  leading to the same conclusion as
the present informal reasoning.
First  as proved in [20]  the bias n−1 k(Aλ − In)Fk2
2 is a decreasing function of the dimensionality
λ Aλ)σ2n−1 is an increasing function of df(λ)   as well
df(λ) = tr(Aλ)   and the variance tr(A⊤
λ Aλ) . Therefore  Eq. (6) shows that the optimal λ realizes the best trade-off
as 2 tr(Aλ) − tr(A⊤
between bias (which decreases with df(λ)) and variance (which increases with df(λ))  which is a
classical fact in model selection.

Second  the expectation of the empirical risk in Eq. (7) can be decomposed into the bias and a
negative variance term which is the opposite of

penmin(λ) := n−1(cid:0) 2 tr(Aλ) − tr(A⊤

λ Aλ)(cid:1) σ2 .

4

(9)

s
r
o
r
r
e

 

n
o

i
t

a
z

i
l

a
r
e
n
e
g

0.5

0

−0.5

 
0

 

σ2trA

σ2trA2 − 2σ2trA

bias
variance ∼  σ2tr A2
generalization error ∼ bias +  σ2 tr A2
empirical error−σ2 ∼ bias+σ2trA2−2σ2 tr A

200
600
degrees of freedom ( tr A )

400

800

Figure 1: Bias-variance decomposition of the generalization error  and minimal/optimal penalties.

As suggested by the notation penmin   we will show it is a minimal penalty in the following sense.
If

λ∈Λn n−1kbFλ − Y k2

2 + C penmin(λ)o  

mizer of

∀C ≥ 0 

then  up to concentration inequalities that are detailed in Section 4.2 bλmin(C) behaves like a mini-
gC(λ) = Eh n−1kbFλ − Y k2

bλmin(C) ∈ arg min
2 + C penmin(λ)i−n−1σ2 = n−1 k(Aλ − In)Fk2

Therefore  two main cases can be distinguished:

2+(C−1) penmin(λ) .

then gC(λ) increases with df(λ) when df(λ) is large enough  so that

• if C < 1   then gC(λ) decreases with df(λ) so that df(bλmin(C)) is huge:bλmin(C) overﬁts.
• if C > 1  
df(bλmin(C)) is much smaller than when C < 1 .
As a conclusion  penmin(λ) is the minimal amount of penalization needed so that a minimizerbλ of

Following an idea ﬁrst proposed in [6] and further analyzed or used in several other papers such as
[21  7  22]  we now propose to use that penmin(λ) is a minimal penalty for estimating σ2 and plug
this estimator into Eq. (5). This leads to the algorithm described in Section 4.1.

a penalized criterion is not clearly overﬁtting.

Note that the minimal penalty given by Eq. (9) is new; it generalizes previous results [6  7] where
penmin(Aλ) = n−1 tr(Aλ)σ2 because all Aλ were assumed to be projection matrices  i.e.  A⊤
λ Aλ =
Aλ . Furthermore  our results generalize the slope heuristics penid ≈ 2 penmin (only valid for
projection estimators [6  7]) to general linear estimators for which penid / penmin ∈ (1  2] .
4 Main results

In this section  we ﬁrst describe our algorithm and then present our theoretical results.

4.1 Algorithm

The following algorithm ﬁrst computes an estimator of bC of σ2 using the minimal penalty in Eq. (9) 
then considers the ideal penalty in Eq. (5) for selecting λ .
Input: Λ a ﬁnite set with Card(Λ) ≤ Knα for some K  α ≥ 0   and matrices Aλ .
λ Aλ)(cid:1)} .
2 + C(cid:0) 2 tr(Aλ) − tr(A⊤

• ∀C > 0   computebλ0(C) ∈ arg minλ∈Λ{kbFλ − Y k2
• Find bC such that df(bλ0(bC)) ∈(cid:2) n3/4  n/10(cid:3) .
• Selectbλ ∈ arg minλ∈Λ{kbFλ − Y k2
2 + 2bC tr(Aλ)} .

In the steps 1 and 2 of the above algorithm  in practice  a grid in log-scale is used  and our theoretical
results from the next section suggest to use a step-size of order n−1/4 . Note that it may not be

5

the presence of a jump around σ2   but do not show the absence of similar jumps elsewhere.

possible in all cases to ﬁnd a C such that df(bλ0(C)) ∈ [n3/4  n/10] ; therefore  our condition in
step 2  could be relaxed to ﬁnding a bC such that for all C > bC + δ   df(bλ0(C)) < n3/4 and for all
C < bC − δ   df(bλ0(C)) > n/10   with δ = n−1/4+ξ   where ξ > 0 is a small constant.
Alternatively  using the same grid in log-scale  we can select bC with maximal jump between succes-
sive values of df(bλ0(C))—note that our theoretical result then does not entirely hold  as we show
Theorem 1 Let bC andbλ be deﬁned as in the algorithm of Section 4.1  with Card(Λ) ≤ Knα for
some K  α ≥ 0 . Assume that ∀λ ∈ Λ   Aλ is symmetric with Sp(Aλ) ⊂ [0  1]   that εi are i.i.d.
Gaussian with variance σ2 > 0   and that ∃λ1  λ2 ∈ Λ with

4.2 Oracle inequality

df(λ1) ≥

n
2

  df(λ2) ≤ √n  and ∀i ∈ { 1  2}   n−1 k(Aλi − In)Fk2

2 ≤ σ2r ln(n)

n

.

(A1−2)

Furthermore  if

Then  a numerical constant Ca and an event of probability at least 1 − 8Kn−2 exist on which  for
every n ≥ Ca  

n ! σ2 ≤ bC ≤  1 +

! σ2 .
  1 − 91(α + 2)r ln(n)
44(α + 2)pln(n)
∃κ ≥ 1  ∀λ ∈ Λ  n−1 tr(Aλ)σ2 ≤ κEh n−1kbFλ − Fk2
2i  
2 ≤(cid:18) 1 +

then  a constant Cb depending only on κ exists such that for every n ≥ Cb   on the same event 
n−1kbFbλ − Fk2
Theorem 1 is proved in [20]. The proof mainly follows from the informal arguments developed in
Section 3.2  completed with the following two concentration inequalities: If ξ ∈ Rn is a standard
Gaussian random vector  α ∈ Rn and M is a real-valued n × n matrix  then for every x ≥ 0  

2o +
λ∈Λn n−1kbFλ − Fk2

ln(n)(cid:19) inf

36(κ + α + 2) ln(n)σ2

(10)

(A3)

n1/4

.

(11)

40κ

n

(12)

(13)

P(cid:16)|hα  ξi| ≤

√2xkαk2(cid:17) ≥ 1 − 2e−x

P(cid:16)∀θ > 0  (cid:12)(cid:12)(cid:12)kM ξk2

2 − tr(M ⊤M )(cid:12)(cid:12)(cid:12) ≤ θ tr(M ⊤M ) + 2(1 + θ−1)kMk2 x(cid:17) ≥ 1 − 2e−x  

where kMk is the operator norm of M . A proof of Eq. (12) and (13) can be found in [20].
4.3 Discussion of the assumptions of Theorem 1
Gaussian noise. When ε is sub-Gaussian  Eq. (12) and Eq. (13) can be proved for ξ = σ−1ε at the
price of additional technicalities  which implies that Theorem 1 is still valid.
Symmetry. The assumption that matrices Aλ must be symmetric can certainly be relaxed  since it
is only used for deriving from Eq. (13) a concentration inequality for hAλξ  ξi . Note that Sp(Aλ) ⊂
[0  1] barely is an assumption since it means that Aλ actually shrinks Y .
Assumptions (A1−2).
(A1−2) holds if maxλ∈Λ { df(λ)} ≥ n/2 and the bias is smaller than
c df(λ)−d for some c  d > 0   a quite classical assumption in the context of model selection. Besides 
(A1−2) is much less restrictive and can even be relaxed  see [20].
Assumption (A3).
The upper bound (A3) on tr(Aλ) is certainly the strongest assumption of
Theorem 1  but it is only needed for Eq. (11). According to Eq. (6)  (A3) holds with κ = 1 when
λ Aλ) = tr(Aλ) . In the kernel ridge regression framework 
Aλ is a projection matrix since tr(A⊤
(A3) holds as soon as the eigenvalues of the kernel matrix K decrease like j−α—see [20].
In

general  (A3) means that bFλ should not have a risk smaller than the parametric convergence rate

associated with a model of dimension df(λ) = tr(Aλ) .
When (A3) does not hold  selecting among estimators whose risks are below the parametric rate
is a rather difﬁcult problem and it may not be possible to attain the risk of the oracle in general.

6

400

300

200

100

m
o
d
e
e
r
f
 
f

o
 
s
e
e
r
g
e
d
d
e
t
c
e
e
s

 

l

0

 
−2

0

log(C/σ2)

2

 

minimal penalty
optimal penalty / 2

m
o
d
e
e
r
f
 
f

 

optimal/2
minimal (discrete)
minimal (continuous)

200

150

100

50

0

 
−2

0

log(C/σ2)

2

o
 
s
e
e
r
g
e
d
d
e
t
c
e
e
s

 

l

Figure 2: Selected degrees of freedom vs. penalty strength log(C/σ2) : note that when penalizing
by the minimal penalty  there is a strong jump at C = σ2   while when using half the optimal penalty 
this is not the case. Left: single kernel case  Right: multiple kernel case.

Nevertheless  an oracle inequality can still be proved without (A3)  at the price of enlarging bC
slightly and adding a small fraction of σ2n−1 tr(Aλ) in the right-hand side of Eq. (11)  see [20].
Enlarging bC is necessary in general: If tr(A⊤
λ Aλ) ≪ tr(Aλ) for most λ ∈ Λ   the minimal penalty
is very close to 2σ2n−1 tr(Aλ)   so that according to Eq. (10)  overﬁtting is likely as soon as bC

underestimates σ2   even by a very small amount.

4.4 Main consequences of Theorem 1 and comparison with previous results

of σ2 in a general framework and under mild assumptions. Compared to classical estimators of σ2  

assumed to have almost no bias  which can lead to overestimating σ2 by an unknown amount [18].
Oracle inequality. Our algorithm satisﬁes an oracle inequality with high probability  as shown by

Consistent estimation of σ2 . The ﬁrst part of Theorem 1 shows that bC is a consistent estimator
such as the one usually used with Mallows’ CL  bC does not depend on the choice of some model
Eq. (11): The risk of the selected estimator bFbλ is close to the risk of the oracle  up to a remainder

term which is negligible when the dimensionality df(λ⋆) grows with n faster than ln(n)   a typical
situation when the bias is never equal to zero  for instance in kernel ridge regression.
Several oracle inequalities have been proved in the statistical literature for Mallows’ CL with a con-
sistent estimator of σ2   for instance in [23]. Nevertheless  except for the model selection problem
(see [6] and references therein)  all previous results were asymptotic  meaning that n is implicitly
assumed to be larged compared to each parameter of the problem. This assumption can be prob-
lematic for several learning problems  for instance in multiple kernel learning when the number p
of kernels may grow with n . On the contrary  Eq. (11) is non-asymptotic  meaning that it holds for
every ﬁxed n as soon as the assumptions explicitly made in Theorem 1 are satisﬁed.
Comparison with other procedures. According to Theorem 1 and previous theoretical results
[23  19]  CL  GCV  cross-validation and our algorithm satisfy similar oracle inequalities in various
frameworks. This should not lead to the conclusion that these procedures are completely equivalent.
Indeed  second-order terms can be large for a given n   while they are hidden in asymptotic results
and not tightly estimated by non-asymptotic results. As showed by the simulations in Section 5  our
algorithm yields statistical performances as good as existing methods  and often quite better.

Furthermore  our algorithm never overﬁts too much because df(bλ) is by construction smaller than
the effective dimensionality ofbλ0(bC) at which the jump occurs. This is a quite interesting property

compared for instance to GCV  which is likely to overﬁt if it is not corrected because GCV minimizes
a criterion proportional to the empirical risk.

5 Simulations

i=1 e−|xi−yi|   with the
x’s sampled i.i.d. from a standard multivariate Gaussian. The functions f are then selected randomly
i=1 αik(·  zi)   where both α and z are i.i.d. standard Gaussian (i.e.  f belongs to the RKHS).

Throughout this section  we consider exponential kernels on Rd   k(x  y) =Qd
asPm

7

)
 

l

e
c
a
r
o

r
o
r
r
e

 
/
 
r
o
r
r
e

 
(
n
a
e
m

3

2.5

2

1.5

1

0.5

 

 

10−fold CV
GCV
min. penalty

4

5
6
log(n)

7

 

MKL+CV
GCV
kernel sum
min. penalty

)
 

s
w
o

l
l

a
M

r
o
r
r
e

 
/
 
r
o
r
r
e

 
(
n
a
e
m

3.5

3

2.5

2

1.5

1

 

3.5

4

5

4.5
log(n)

5.5

Figure 3: Comparison of various smoothing parameter selection (minikernel  GCV  10-fold cross
validation) for various values of numbers of observations  averaged over 20 replications. Left: single
kernel  right: multiple kernels.

Jump.
In Figure 2 (left)  we consider data xi ∈ R6   n = 1000  and study the size of the jump
in Figure 2 for kernel ridge regression. With half the optimal penalty (which is used in traditional
variable selection for linear regression)  we do not get any jump  while with the minimal penalty we
always do. In Figure 2 (right)  we plot the same curves for the multiple kernel learning problem with
two kernels on two different 4-dimensional variables  with similar results. In addition  we show two
+   by discrete optimization with n different kernel matrices—a
ways of optimizing over λ ∈ Λ = R2
situation covered by Theorem 1—or with continuous optimization with respect to η in Eq. (1)  by
gradient descent—a situation not covered by Theorem 1.
Comparison of estimator selection methods.
In Figure 3  we plot model selection results for 20
replications of data (d = 4  n = 500)  comparing GCV [8]  our minimal penalty algorithm  and
cross-validation methods. In the left part (single kernel)  we compare to the oracle (which can be
computed because we can enumerate Λ)  and use for cross-validation all possible values of λ . In the
right part (multiple kernel)  we compare to the performance of Mallows’ CL when σ2 is known (i.e. 
penalty in Eq. (5))  and since we cannot enumerate all λ’s  we use the solution obtained by MKL
with CV [5]. We also compare to using our minimal penalty algorithm with the sum of kernels.

6 Conclusion

A new light on the slope heuristics. Theorem 1 generalizes some results ﬁrst proved in [6] where
all Aλ are assumed to be projection matrices  a framework where assumption (A3) is automatically
satisﬁed. To this extent  Birg´e and Massart’s slope heuristics has been modiﬁed in a way that sheds
a new light on the “magical” factor 2 between the minimal and the optimal penalty  as proved in
[6  7]. Indeed  Theorem 1 shows that for general linear estimators 

penid(λ)
penmin(λ)

=

 

(14)

2 tr(Aλ)

2 tr(Aλ) − tr(A⊤

λ Aλ)

λ Aλ)  

which can take any value in (1  2] in general; this ratio is only equal to 2 when tr(Aλ) ≈ tr(A⊤
hence mostly when Aλ is a projection matrix.
Future directions.
In the case of projection estimators  the slope heuristics still holds when the de-
sign is random and data are heteroscedastic [7]; we would like to know whether Eq. (14) is still valid
for heteroscedastic data with general linear estimators. In addition  the good empirical performances
of elbow heuristics based algorithms (i.e.  based on the sharp variation of a certain quantity around
good hyperparameter values) suggest that Theorem 1 can be generalized to many learning frame-
works (and potentially to non-linear estimators)  probably with small modiﬁcations in the algorithm 
but always relying on the concept of minimal penalty.
Another interesting open problem would be to extend the results of Section 4  where Card(Λ) ≤
Knα is assumed  to continuous sets Λ such as the ones appearing naturally in kernel ridge regression
and multiple kernel learning. We conjecture that Theorem 1 is valid without modiﬁcation for a
“small” continuous Λ   such as in kernel ridge regression where taking a grid of size n in log-scale is
almost equivalent to taking Λ = R+ . On the contrary  in applications such as the Lasso with p ≫ n
variables  the natural set Λ cannot be well covered by a grid of cardinality nα with α small  and our
minimal penalty algorithm and Theorem 1 certainly have to be modiﬁed.

8

References

[1] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-

sity Press  2004.

[2] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  2001.
[3] O. Chapelle and V. Vapnik. Model selection for support vector machines.

Neural Information Processing Systems (NIPS)  1999.

In Advances in

[4] C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[5] F. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine

Learning Research  9:1179–1225  2008.

[6] L. Birg´e and P. Massart. Minimal penalties for Gaussian model selection. Probab. Theory

Related Fields  138(1-2):33–73  2007.

[7] S. Arlot and P. Massart. Data-driven calibration of penalties for least-squares regression. J.

Mach. Learn. Res.  10:245–279  2009.

[8] P. Craven and G. Wahba. Smoothing noisy data with spline functions. Estimating the correct
degree of smoothing by the method of generalized cross-validation. Numer. Math.  31(4):377–
403  1978/79.

[9] G. Wahba. Spline Models for Observational Data. SIAM  1990.
[10] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of The Royal Statistical Society Series B  68(1):49–67  2006.

[11] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M. I. Jordan. Learning the

kernel matrix with semideﬁnite programming. J. Mach. Learn. Res.  5:27–72  2003/04.

[12] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of The Royal Statis-

tical Society Series B  58(1):267–288  1996.

[13] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer-

Verlag  2001.

[14] D. M. Allen. The relationship between variable selection and data augmentation and a method

for prediction. Technometrics  16:125–127  1974.

[15] M. Stone. Cross-validatory choice and assessment of statistical predictions. J. Roy. Statist.

Soc. Ser. B  36:111–147  1974.

[16] T. Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural

Comput.  17(9):2077–2098  2005.

[17] C. L. Mallows. Some comments on Cp. Technometrics  15:661–675  1973.
[18] B. Efron. How biased is the apparent error rate of a prediction rule? J. Amer. Statist. Assoc. 

81(394):461–470  1986.

[19] Y. Cao and Y. Golubev. On oracle inequalities related to smoothing splines. Math. Methods

Statist.  15(4):398–414 (2007)  2006.

[20] S. Arlot and F. Bach. Data-driven calibration of linear estimators with minimal penalties 

September 2009. Long version. arXiv:0909.1884v1.

[21] ´E. Lebarbier. Detecting multiple change-points in the mean of a gaussian process by model

selection. Signal Proces.  85:717–736  2005.

[22] C. Maugis and B. Michel. Slope heuristics for variable selection and clustering via gaussian

mixtures. Technical Report 6550  INRIA  2008.

[23] K.-C. Li. Asymptotic optimality for Cp  CL  cross-validation and generalized cross-validation:

discrete index set. Ann. Statist.  15(3):958–975  1987.

9

,Sahar Akram
Jonathan Simon
Shihab Shamma
Behtash Babadi
Guiguan Lin
xinyang gong
Kang-Jun Liu
Shan-Hung (Brandon) Wu
Abhishek Kar
Christian Häne