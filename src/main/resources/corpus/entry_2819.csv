2017,Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences,We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem  they are mostly not scalable. In this paper  we develop a method that transforms the quadratic constraint into a linear form by a sampling a set of low-discrepancy points. The transformed problem can then be solved by applying any state-of-the-art large-scale solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability in practice.,Large-Scale Quadratically Constrained Quadratic

Program via Low-Discrepancy Sequences

Kinjal Basu  Ankan Saha  Shaunak Chatterjee

LinkedIn Corporation

Mountain View  CA 94043

{kbasu  asaha  shchatte}@linkedin.com

Abstract

We consider the problem of solving a large-scale Quadratically Constrained
Quadratic Program. Such problems occur naturally in many scientiﬁc and web
applications. Although there are efﬁcient methods which tackle this problem  they
are mostly not scalable. In this paper  we develop a method that transforms the
quadratic constraint into a linear form by sampling a set of low-discrepancy points
[16]. The transformed problem can then be solved by applying any state-of-the-art
large-scale quadratic programming solvers. We show the convergence of our ap-
proximate solution to the true solution as well as some ﬁnite sample error bounds.
Experimental results are also shown to prove scalability as well as improved quality
of approximation in practice.

Introduction

1
In this paper we consider the class of problems called quadratically constrained quadratic program-
ming (QCQP) which take the following form:

Minimize

x

subject to

xT P0x + qT

0 x + r0

xT Pix + qT

i x + ri ≤ 0 

1
2
1
2
Ax = b 

i = 1  . . .   m

(1)

where P0  . . .   Pm are n × n matrices. If each of these matrices are positive deﬁnite  then the
optimization problem is convex. In general  however  solving QCQP is NP-hard  which can be
veriﬁed by easily reducing a 0 − 1 integer programming problem (known to be NP-hard) to a QCQP
[4]. In spite of that challenge  they form an important class of optimization problems  since they
arise naturally in many engineering  scientiﬁc and web applications. Two famous examples of QCQP
include the max-cut and boolean optimization [11]. Other examples include alignment of kernels
in semi-supervised learning [29]  learning the kernel matrix in discriminant analysis [28] as well as
more general learning of kernel matrices [21]  steering direction estimation for radar detection [15] 
several applications in signal processing [20]  the triangulation in computer vision [3] among others.
Internet applications handling large scale of data  often model trade-offs between key utilities using
constrained optimization formulations [1  2]. When there is independence among the expected utilities
(e.g.  click  time spent  revenue obtained) of items  the objective or the constraints corresponding to
those utilities are linear. However  in most real life scenarios  there is dependence among expected
utilities of items presented together on a web page or mobile app. Examples of such dependence are
abundant in newsfeeds  search result pages and most lists of recommendations on the internet. If
this dependence is expressed through a linear model  it makes the corresponding objective and/or
constraint quadratic. This makes the constrained optimization problem a very large scale QCQP  if

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the dependence matrix (often enumerated by a very large number of members or updates) is positive
deﬁnite with co-dependent utilities [6].
Although there are a plethora of such applications  solving this problem on a large scale is still
extremely challenging. There are two main relaxation techniques that are used to solve a QCQP 
namely  semi-deﬁnite programming (SDP) and reformulation-linearization technique (RLT) [11].
However  both of them introduce a new variable X = xxT so that the problem becomes linear in X.
Then they relax the condition X = xxT by different means. Doing so unfortunately increases the
number of variables from n to O(n2). This makes these methods prohibitively expensive for most
large scale applications. There is literature comparing these methods which also provides certain
combinations and generalizations[4  5  22]. However  they all suffer from the same curse of dealing
with O(n2) variables. Even when the problem is convex  there are techniques such as second order
cone programming [23]  which can be efﬁcient  but scalability still remains an important issue with
prior QCQP solvers.
The focus of this paper is to introduce a novel approximate solution to the convex QCQP problem
which can tackle such large-scale situations. We devise an algorithm which approximates the
quadratic constraints by a set of linear constraints  thus converting the problem into a quadratic
program (QP) [11]. In doing so  we remain with a problem having n variables instead of O(n2). We
then apply efﬁcient QP solvers such as Operator Splitting or ADMM [10  26] which are well adapted
for distributed computing  to get the ﬁnal solution for problems of much larger scale. We theoretically
prove the convergence of our technique to the true solution in the limit. We also provide experiments
comparing our algorithm to existing state-of-the-art QCQP solvers to show comparative solutions for
smaller data size as well as signiﬁcant scalability in practice  particularly in the large data regime
where existing methods fail to converge. To the best of our knowledge  this technique is new and has
not been previously explored in the optimization literature.
Notation: Throughout the paper  bold small case letters refer to vectors while bold large-case letters
refer to matrices.
The rest of the paper is structured as follows. In Section 2  we describe the approximate problem 
important concepts to understand the sampling scheme as well as the approximation algorithm
to convert the problem into a QP. Section 3 contains the proof of convergence  followed by the
experimental results in Section 4. Finally  we conclude with some discussion in Section 5.

2 QCQP to QP Approximation
For sake of simplicity throughout the paper  we deal with a QCQP having a single quadratic constraint.
The procedure detailed in this paper can be easily generalized to multiple constraints. Thus  for the
rest of the paper  without loss of generality we consider the problem of the form 

Minimize

x

subject to

(x − a)T A(x − a)
(x − b)T B(x − b) ≤ ˜b 

Cx = c.

(2)

This is a special case of the general formulation in (1). For this paper  we restrict our case to A 
B ∈ Rn×n being positive deﬁnite matrices so that the objective function is strongly convex.
In this section  we describe the linearization technique to convert the quadratic constraint into a set of
N linear constraints. The main idea behind this approximation  is the fact that given any convex set
in the Euclidean plane  there exists a convex polytope that covers the set. Let us begin by introducing
a few notations. Let P denote the optimization problem (2). Deﬁne 

S := {x ∈ Rn : (x − b)T B(x − b) ≤ ˜b}.

(3)
Let ∂S denote the boundary of the ellipsoid S. To generate the N linear constraints for this one
quadratic constraint  we generate a set of N points  XN = {x1  . . .   xN} such that each xj ∈ ∂S for
j = 1  . . .   N . The sampling technique to select the point set is given in Section 2.1. Corresponding
to these N points we get the following set of N linear constraints 

(4)
Looking at it geometrically  it is not hard to see that each of these linear constraints are just tangent
planes to S at xj for j = 1  . . .   N. Figure 1 shows a set of six linear constraints for a ellipsoidal

(x − b)T B(xj − b) ≤ ˜b

for j = 1  . . .   N.

2

feasible set in two dimensions. Thus  using these N linear constraints we can write the approximate
optimization problem  P(XN )  as follows.

Minimize

x

subject to

(x − a)T A(x − a)
(x − b)T B(xj − b) ≤ ˜b

Cx = c.

for j = 1  . . .   N

(5)

Now instead of solving P  we solve P(XN ) for a large enough value of N. Note that as we sample
more points (N → ∞)  our approximation keeps getting better.

Figure 1: Converting a quadratic constraint into linear constraints. The tangent planes through the 6
points x1  . . .   x6 create the approximation to S.
2.1 Sampling Scheme
The accuracy of the solution of P(XN ) solely depends on the choice of XN . The tangent planes to
S at those N points create a cover of S. We use the notion of a bounded cover  which we deﬁne as
follows.
Deﬁnition 1. Let T be the convex polytope generated by the tangent planes to S at the points
x1  . . .   xN ∈ ∂S. T is said to be a bounded cover of S if 

d(T  S) := sup
t∈T

d(t S) < ∞ 

where d(t S) := inf x∈S (cid:107)t − x(cid:107) and (cid:107) · (cid:107) denotes the Euclidean distance.
The ﬁrst result shows that there exists a bounded cover with only n + 1 points.
Lemma 1. Let S be a n dimensional ellipsoid as deﬁned in (3). Then there exists a bounded cover
with n + 1 points.
Proof. Note that since S is a compact convex body in Rn  there exists a location translated version of
an n-dimensional simplex T = {x ∈ Rn
i=1 xi = K} such that S is contained in the interior of
T . We can always shrink T such that each edge touches S tangentially. Since there are n + 1 faces 
we will get n + 1 points whose tangent surface creates a bounded cover.

+ :(cid:80)n

Although Lemma 1 gives a simple constructive proof of a bounded cover  it is not what we are truly
interested in. What we want is to construct a bounded cover T which is as close as possible to S  thus
leading to a better approximation. However note that  choosing the points via a naive sampling can
lead to arbitrarily bad enlargements of the feasible set and in the worst case might even create a cover
which is not bounded. Hence we need an optimal set of points which creates an optimal bounded
cover. Formally 
Deﬁnition 2. T

N ) is said to be an optimal bounded cover  if

∗ = T (x∗

1  . . .   x∗

sup
t∈T ∗

d(t S) ≤ sup
t∈T

d(t S)

for any bounded cover T generated by any other N-point sets. Moreover  {x∗
N} are deﬁned
to be the optimal N-point set.
Note that we can think of the optimal N-point set as that set of N points which minimize the
maximum distance between T and S  i.e.
T

∗ = argmin

1  . . .   x∗

d(T  S).

T

3

x1x2x3x4x5x6STIt is not hard to see that the optimal N-point set on the unit circle in two dimensions are the N-th
roots of unity  unique up to rotation. This point set also has a very good property. It has been shown
that the N-th roots of unity minimize the discrete Riesz energy for the unit circle [14  17]. The
concept of Reisz energy also exists in higher dimensions. Thus  generalizing this result  we choose
our optimal N-point set on ∂S which tries to minimize the Reisz energy. We brieﬂy describe it below.
2.1.1 Riesz Energy

Riesz energy of a point set AN = {x1  . . .   xN} is deﬁned as Es(AN ) :=(cid:80)N

−s for
a positive real parameter s. There is a vast literature on Riesz energy and its association with “good”
conﬁguration of points. It is well known that the measures associated to the optimal point set that
minimizes the Riesz energy on ∂S converge to the normalized surface measure of ∂S [17]. Thus
using this fact  we can associate the optimal N-point set to the set of N points that minimize the
Riesz energy on ∂S. For more details see [18  19] and the references therein. To describe these good
conﬁgurations of points  we introduce the concept of equidistribution. We begin with a “good” or
equidistributed point set in the unit hypercube (described in Section 2.1.2) and map it to ∂S such that
the equidistribution property still holds (described in Section 2.1.3).

i(cid:54)=j=1 (cid:107)xi − xj(cid:107)

2.1.2 Equidistribution

Informally  a set of points in the unit hypercube is said to be equidistributed  if the expected number
of points inside any axis-parallel subregion  matches the true number of points. One such point set
in [0  1]n is called the (t  m  n)-net in base η  which is deﬁned as a set of N = ηm points in [0  1]n
such that any axis parallel η-adic box with volume ηt−m would contain exactly ηt points. Formally 
it is a point set that can attain the optimal integration error of O((log(N ))n−1/N ) [16] and is usually
referred to as a low-discrepancy point set. There is vast literature on easy construction of these point
sets. For more details on nets we refer to [16  24].
2.1.3 Area preserving map to ∂S
Now once we have a point set on [0  1]n we try to map it to ∂S using a measure preserving transfor-
mation so that the equidistribution property remains intact. We describe the mapping in two steps.
First we map the point set from [0  1]n to the hyper-sphere Sn = {x ∈ Rn+1 : xT x = 1}. Then we
map it to ∂S. The mapping from [0  1]n to Sn is based on [12].
The cylindrical coordinates of the n-sphere  can be written as

(cid:113)

x = xn = ((cid:112)1 − t2

nxn−1  tn) 

. . .

  x2 = (

2x1  t2)  x1 = (cos φ  sin φ)

1 − t2

where 0 ≤ φ ≤ 2π −1 ≤ td ≤ 1  xd ∈ Sd and d = (1  . . .   n). Thus  an arbitrary point x ∈ Sn can
be represented through angle φ and heights t2  . . .   tn as 

x = x(φ  t2  . . .   tn) 

0 ≤ φ ≤ 2π −1 ≤ t2  . . .   tn ≤ 1.

(d = 2  . . .   n)

ϕ1(y1) = 2πy1 

We map a point y = (y1  . . .   yn) ∈ [0  1)n to x ∈ Sn using
ϕd(yd) = 1 − 2yd
and cylindrical coordinates x = Φn(y) = x(ϕ1(y1)  ϕ2(y2)  . . .   ϕn(yn)). The fact that Φn :
[0  1)n → Sn is an area preserving map has been proved in [12].
Remark. Instead of using (t  m  n)-nets and mapping to Sn  we could have also used spherical
t-designs  the existence of which was proved in [9]. However  construction of such sets is still a tough
problem in high dimensions. We refer to [13] for more details.
(cid:112)˜bB−1/2x + b.
Finally  we consider the map ψ to translate the point set from Sn−1 to ∂S. Speciﬁcally we deﬁne 
(6)
From the deﬁnition of S in (3)  it is easy to see that ψ(x) ∈ ∂S. The next result shows that this is
also an area-preserving map  in the sense of normalized surface measures.
Lemma 2. Let ψ be a mapping from Sn−1 → ∂S as deﬁned in (6). Then for any set A ⊆ ∂S 
where  σn  λn are the normalized surface measure of ∂S and Sn−1 respectively.

σn(A) = λn(ψ−1(A))

ψ(x) =

4

(cid:40)
Proof. Pick any A ⊆ ∂S. Then we can write 

(cid:41)

1(cid:112)˜b

ψ−1(A) =

B1/2(x − b) : x ∈ A
(cid:41)(cid:33)
(cid:32)(cid:40)
Now since the linear shift does not change the surface area  we have 
λn(ψ−1(A)) = λn

(cid:32)(cid:40)

= λn

1(cid:112)˜b

.

(cid:41)(cid:33)

B1/2x : x ∈ A

= σn(A) 

1(cid:112)˜b

B1/2(x − b) : x ∈ A

(cid:112)˜b ∈ Sn−1. This completes the proof.

where the last equality follows from the deﬁnition of normalized surface measures and noting that
B1/2x/

Using Lemma 2 we see that the map ψ ◦ Φn−1 : [0  1)n−1 → ∂S  is a measure preserving map.
Using this map and the (t  m  n − 1) net in base η  we derive the optimal ηm-point set on ∂S. Figure
2 shows how we transform a (0  7  2)-net in base 2 to a sphere and then to an ellipsoid. For more
general geometric constructions we refer to [7  8].

Figure 2: The left panel shows a (0  7  2)-net in base 2 which is mapped to a sphere in 3 dimensions
(middle panel) and then mapped to the ellipsoid as seen in the right panel.

2.2 Algorithm and Efﬁcient Solution

From the description in the previous section we are now at a stage to describe the approximation
algorithm. We approximate the problem P by P(XN ) using a set of points x1  . . .   xN as described
in Algorithm 1. Once we formulate the problem P as P(XN )  we solve the large scale QP via
Algorithm 1 Point Simulation on ∂S
1: Input : B  b  ˜b to specify S and N = ηm points
2: Output : x1  . . .   xN ∈ ∂S
3: Generate y1  . . .   yN as a (t  m  n − 1)-net in base η.
4: for i ∈ 1  . . .   N do
5:
6: end for
7: return x1  . . .   xN

xi = ψ ◦ Φn−1(yi)

state-of-the-art solvers such as Operator Splitting or Block Splitting approaches [10  25  26].

3 Convergence of P(XN ) to P
In this section  we shall show that if we follow Algorithm 1 to generate the approximate problem
P(XN )  then we converge to the original problem P as N → ∞. We shall also prove some ﬁnite
sample results to give error bounds on the solution to P(XN ). We start by introducing some notation.

5

Let x∗  x∗(N ) denote the solution to P and P(XN ) respectively and f (·) denote the strongly convex
objective function in (2)  i.e.  for ease of notation

f (x) = (x − a)T A(x − a).

We begin with our main result.
Theorem 1. Let P be the QCQP deﬁned in (2) and P(XN ) be the approximate QP problem deﬁned in
(5) via Algorithm 1. Then  P(XN ) → P as N → ∞ in the sense that limN→∞ (cid:107)x∗(N ) − x∗
(cid:107) = 0.
Proof. Fix any N. Let TN denote the optimal bounded cover constructed with N points on ∂S. Note
that to prove the result  it is enough to show that TN → S as N → ∞. This guarantees that linear
constraints of P(XN ) converge to the quadratic constraint of P  and hence the two problems match.
Now since S ⊆ TN for all N  it is easy to see that S ⊆ limN→∞ TN .
To prove the converse  let t0 ∈ limN→∞ TN but t0 (cid:54)∈ S. Thus  d(t0 S) > 0. Let t1 denote the
projection of t0 onto S. Thus  t0 (cid:54)= t1 ∈ ∂S. Choose  to be arbitrarily small and consider any
region A around t1 on ∂S such that d(x  t1) ≤  for all x ∈ A. Here d denotes the surface distance
function. Now  by the equidistribution property of Algorithm 1 as N → ∞  there exists a point
t∗
∈ A  the tangent plane through which cuts the plane joining t0 and t1. Thus  t0 (cid:54)∈ limN→∞ TN .
Hence  we get a contradiction and the result is proved.
As a simple Corollary to Theorem 1 it is easy to see that as limN→∞ |f (x∗(N )) − f (x∗)| = 0. We
now move to some ﬁnite sample results.
Theorem 2. Let g : N → R such that limn→∞ g(n) = 0. Further assume that (cid:107)x∗(N ) − x∗
(cid:107) ≤
C1g(N ) for some constant C1 > 0. Then  |f (x∗(N )) − f (x∗)| ≤ C2g(N ) where C2 > 0 is a
(cid:107). Note that since x∗ satisﬁes the constraint of the optimization
Proof. We begin by bounding the (cid:107)x∗
problem  we have  ˜b ≥ (x∗
− b(cid:107)2  where σmin(B) denotes the
smallest singular value of B. Thus 

− b) ≥ σmin(B)(cid:107)x∗

constant.

− b)T B(x∗
(cid:107)x∗

(cid:115)

˜b

σmin(B)

(cid:107) ≤ (cid:107)b(cid:107) +

(cid:90) 1
0 (cid:104)∇f (x∗ + t(x − x∗))  x − x∗

Now  since f (x) = (x − a)T A(x − a) and ∇f (x) = 2A(x − a)  we can write

f (x) = f (x∗) +

(cid:105)dt

(cid:90) 1
0 (cid:104)∇f (x∗ + t(x − x∗)) − ∇f (x∗)  x − x∗

(cid:105)dt

.

(7)

= f (x∗) + (cid:104)∇f (x∗)  x − x∗
= I1 + I2 + I3 (say) .

(cid:105) +

Now  we can bound the last term as follows. Observe that using Cauchy-Schwarz inequality 

(cid:90) 1
(cid:90) 1
0 |(cid:104)∇f (x∗ + t(x − x∗)) − ∇f (x∗)  x − x∗
0 (cid:107)∇f (x∗ + t(x − x∗)) − ∇f (x∗)(cid:107)(cid:107)x − x∗

(cid:105)| dt

|I3| ≤

≤

(cid:107)dt = σmax(A)(cid:107)x − x∗
where σmax(A) denotes the largest singular value of A. Thus  we have
(cid:105) + ˜C(cid:107)x − x∗

(cid:90) 1
0 (cid:107)t(x − x∗)(cid:107)(cid:107)x − x∗
f (x) = f (x∗) + (cid:104)∇f (x∗)  x − x∗

≤ 2σmax(A)

(cid:107)dt

(cid:107)2

(cid:107)2 

where | ˜C| ≤ σmax(A). Furthermore 

|(cid:104)∇f (x∗)  x∗(N ) − x∗

− a)  x∗(N ) − x∗

(cid:105)|

(cid:107) + (cid:107)a(cid:107))(cid:107)x∗(N ) − x∗

(cid:107)

(cid:105)| = |(cid:104)2A(x∗
≤ 2σmax(A)((cid:107)x∗
≤ 2C1σmax(A)

(cid:115)

˜b

σmin(B)

+ (cid:107)b(cid:107) + (cid:107)a(cid:107)

 g(N ) 

(8)

(9)

6

where the last line inequality follows from (7). Combining (8) and (9) the result follows.

Note that the function g gives us an idea about how fast x∗(N ) converges x∗. To help  identify the
function g we state the following results.
Lemma 3. If f (x∗) = f (x∗(N ))  then x∗ = x∗(N ). Furthermore  if f (x∗) ≥ f (x∗(N ))  then
x∗

∈ ∂U and x∗(N ) (cid:54)∈ U  where U = S ∩ {x : Cx = c} is the feasible set for (2).

Proof. Let V = TN ∩ {x : Cx = c}. It is easy to see that U ⊆ V. Assume f (x∗) = f (x∗(N ))  but
(cid:54)= x∗(N ). Note that x∗  x∗(N ) ∈ V. Since V is convex  consider a line joining x∗ and x∗(N ).
x∗
For any point λt = tx∗ + (1 − t)x∗(N ) 

f (λt) ≤ tf (x∗) + (1 − t)f (x∗(N )) = f (x∗(N )).

◦

∈

Thus  f is constant on the line joining x∗ and x∗(N ). But  it is known that f is strongly convex
since A is positive deﬁnite [27]. Thus  there exists only one unique minimum. Hence  we have
a contradiction  which proves x∗ = x∗(N ). Now let us assume that f (x∗) ≥ f (x∗(N )). Clearly 
x∗(N ) (cid:54)∈ U. Suppose x∗
U  the interior of U. Let ˜x ∈ ∂U denote the point on the line joining
x∗ and x∗(N ). Clearly  ˜x = tx∗ + (1 − t)x∗(N ) for some t > 0. Thus  f (˜x) < tf (x∗) + (1 −
t)f (x∗(N )) ≤ f (x∗). But x∗ is the minimizer over U. Thus  we have a contradiction  which gives
x∗
Lemma 4. Following the notation of Lemma 3  if x∗(N ) (cid:54)∈ U  then x∗ lies on ∂U and no point on
the line joining x∗ and x∗(N ) lies in S.
Proof. Since the gradient of f is linear  the result follows from a similar argument to Lemma 3.

∈ ∂U. This completes the proof.

Based on the above two results we can identify the function g by considering the maximum distance
of the points lying on the conic cap to the hyperplanes forming it. That is g(N ) is the maximum
distance between a point x ∈ ∂S and a point in t ∈ T such the line joining x and t do not intersect
S and hence  lie completely within the conic section. This is highly dependent on the shape of S and
on the cover TN . For example  if S is the unit circle in two dimensions  then the optimal N-point set
are the N-th roots of unity. In which case  there are N equivalent conic sections C1  . . .  CN which
are created by the intersections of ∂S with TN . Figure 3 elaborates these regions.

Figure 3: The shaded region shows the 6 equivalent conic regions  C1  . . .  C6.

To formally deﬁne g(N ) in this situation  let us deﬁne A(t  x) to be the set of all points in the line
joining t ∈ T and x ∈ ∂S. Now  it is easy to see that 

g(N ) := max

i=1 ... N

sup

t x:A(t x)∈Ci (cid:107)t − x(cid:107) = tan

N

= O

 

(10)

(cid:16) π

(cid:17)

(cid:18) 1

(cid:19)

N

where the bound follows from using the Taylor series expansion of tan(x). Combining this observa-
tion with Theorem 2 shows that in order to get an objective value within  of the true optimal  we
would need N to be a constant multiplier of −1. More such results can be achieved by such explicit
calculations over various different domains S.

7

C1C2C3C4C5C6π/64 Experimental Results
We compare our proposed technique to the current state-of-the-art solvers of QCQP. Speciﬁcally 
we compare it to the SDP and RLT relaxation procedures as described in [4]. For small enough
problems  we also compare our method to the exact solution by interior point methods. Furthermore 
we provide empirical evidence to show that our sampling technique is better than other simpler
sampling procedures such as uniform sampling on the unit square or on the unit sphere and then
mapping it subsequently to our domain as in Algorithm 1. We begin by considering a very simple
QCQP for the form

Minimize

x

subject to

xT Ax
(x − x0)T B(x − x0) ≤ ˜b 
l ≤ x ≤ u.

(11)

We randomly sample A  B  x0 and ˜b keeping the problem convex. The lower bound  l and upper
bounds u are chosen in a way such that they intersect the ellipsoid. We vary the dimension n of the
problem and tabulate the ﬁnal objective value as well as the time taken for the different procedures
to converge in Table 1. The stopping criteria throughout our simulation is same as that of Operator
Splitting algorithm as presented in [26].

Table 1: The Optimal Objective Value and Convergence Time

n

5

10

20

50

100

1000

105

106

Our Method

3.00
(4.61s)
206.85
(5.04s)
6291.4
(6.56s)
99668
(15.55s)
1.40 × 106
(58.41s)
2.24 × 107
(14.87m)
3.10 × 108
(25.82m)
3.91 × 109
(38.30m)

Sampling
on [0  1]n

2.99
(4.74s)
205.21
(5.65s)
4507.8
(6.28s)
15122
(18.98s)
69746
(1.03m)
8.34 × 106
(15.63m)
7.12 × 107
(24.59m)
2.69 × 108
(39.15m)

Sampling
on Sn
2.95

(6. 11s)
206.5
(5.26s)
5052.2
(6.69s)
26239
(17.32s)
1.24 × 106
(54.69s)
9.02 × 106
(15.32m)
8.39 × 107
(27.23m)
7.53 × 108
(37.21m)

SDP

RLT

3.07
(0.52s)
252.88
(0.53s)
6841.6
(2.05s)
1.11 × 105
(4.31s)
1.62 × 106
(30.41s)

3.08
(0.51s)
252.88
(0.51s)
6841.6
(1.86s)
1.08 × 105
(2.96s)
1.52 × 106
(15.36s)

Exact

3.07
(0.49)
252.88
(0.51)
6841.6
(0.54)

(0.64)

1.11 × 105
1.62 × 106
(2.30s)

NA

NA

NA

NA

NA

NA

NA

NA

NA

Throughout our simulations  we have chosen η = 2 and the number of optimal points as N =
max(1024  2m)  where m is the smallest integer such that 2m ≥ 10n. Note that even though the
SDP and the interior point methods converge very efﬁciently for small values of n  they cannot
scale to values of n ≥ 1000  which is where the strength of our method becomes evident. From
Table 1 we observe that the relaxation procedures SDP and RLT fail to converge within an hour of
computation time for n ≥ 1000  whereas all the approximation procedures can easily scale up to
n = 106 variables. Moreover  since the A  B were randomly sampled  we have seen that the true
optimal solution occurred at the boundary. Therefore  relaxing the constraint to linear forced the
solution to occur outside of the feasible set  as seen from the results in Table 1 as well as from Lemma
3. However  that is not a concern  since increasing N will deﬁnitely bring us closer to the feasible
set. The exact choice of N differs from problem to problem but can be computed as we did with the
small example in (10). Finally  the last column in Table 1 is obtained by solving the problem using
cvx in MATLAB using via SeDuMi and SDPT3  which gives the true x∗.
Furthermore  our procedure gives the best approximation result when compared to the remaining
two sampling schemes. Lemma 3 shows that if the both the objective values are the same then we
indeed get the exact solution. To see how much the approximation deviates from the truth  we also
plot the log of the relative squared error  i.e. log((cid:107)x∗(N ) − x∗
(cid:107)2) for each of the sampling

(cid:107)2/(cid:107)x∗

8

procedures in Figure 4. Throughout this simulation  we keep N ﬁxed at 1024. This is why we see
that the error level increases with the increase in dimension. We omit SDP and RLT results in Figure

Figure 4: The log of the relative squared error log((cid:107)x∗(N ) − x∗
and varying dimension n.

(cid:107)2/(cid:107)x∗

(cid:107)2) with N ﬁxed at 1024

4 since both of them produce a solution very close to the exact minimum for small n. If we grow this
N with the dimension  then we see that the increasing trend vanishes and we get much more accurate
results as seen in Figure 5. We plot both the log of relative squared error as well as the log of the
feasibility error  where the feasibility error is deﬁned as

Feasibility Error =

where (x)+ denotes the positive part of x.

(cid:16)

(cid:17)
(x∗(N ) − x0)T B(x∗(N ) − x0) − ˜b

+

Figure 5: The plot on the left panel and the right panel shows the decay in the relative squared error
and the feasibility error respectively  for our method  as we increase N for various dimensions.

From these results  it is clear that our procedure gets the smallest relative error compared to the other
sampling schemes  and increasing N brings us closer to the feasible set  with better accurate results.

5 Discussion and Future Work
In this paper  we look at the problem of solving a large scale QCQP problem by relaxing the quadratic
constraint by a near-optimal sampling scheme. This approximate method can scale up to very large
problem sizes  while generating solutions which have good theoretical properties of convergence.
Theorem 2 gives us an upper bound as a function of g(N )  which can be explicitly calculated for
different problems. To get the rate as a function of the dimension n  we need to understand how the
maximum and minimum eigenvalues of the two matrices A and B grow with n. One idea is to use
random matrix theory to come up with a probabilistic bound. Because of the nature of complexity of
these problems  we believe they deserve special attention and hence we leave them to future work.
We also believe that this technique can be immensely important in several applications. Our next step
is to do a detailed study where we apply this technique on some of these applications and empirically
compare it with other existing large-scale commercial solvers such as CPLEX and ADMM based
techniques for SDP.

9

−3−2−10125102050100Dimension of the Problemlog (Relative Square Error)MethodLow Discrepancy SamplingUniform Sampling on SquareUniform Sampling on Sphere−16−14−12−10−8−6−4−2234567log(Number of Constraints)log(Relative Squared Error)Dimension51020−6−4−20234567log(Number of Constraints)log(FeasibilityError)Dimension51020Acknowledgment
We would sincerely like to thank the anonymous referees for their helpful comments which has
tremendously improved the paper. We would also like to thank Art Owen  Souvik Ghosh  Ya Xu and
Bee-Chung Chen for the helpful discussions.
References
[1] D. Agarwal  S. Chatterjee  Y. Yang  and L. Zhang. Constrained optimization for homepage
relevance. In Proceedings of the 24th International Conference on World Wide Web Companion 
pages 375–384. International World Wide Web Conferences Steering Committee  2015.

[2] D. Agarwal  B.-C. Chen  P. Elango  and X. Wang. Personalized click shaping through lagrangian
duality for online recommendation. In Proceedings of the 35th international ACM SIGIR
conference on Research and development in information retrieval  pages 485–494. ACM  2012.
[3] C. Aholt  S. Agarwal  and R. Thomas. A QCQP Approach to Triangulation. In Proceedings of
the 12th European Conference on Computer Vision - Volume Part I  ECCV’12  pages 654–667 
Berlin  Heidelberg  2012. Springer-Verlag.

[4] K. M. Anstreicher. Semideﬁnite programming versus the reformulation-linearization tech-
nique for nonconvex quadratically constrained quadratic programming. Journal of Global
Optimization  43(2):471–484  2009.

[5] X. Bao  N. V. Sahinidis  and M. Tawarmalani. Semideﬁnite relaxations for quadratically
constrained quadratic programming: A review and comparisons. Mathematical Programming 
129(1):129–157  2011.

[6] K. Basu  S. Chatterjee  and A. Saha. Constrained Multi-Slot Optimization for Ranking Recom-

mendations. arXiv:1602.04391  2016.

[7] K. Basu and A. B. Owen. Low discrepancy constructions in the triangle. SIAM Journal on

Numerical Analysis  53(2):743–761  2015.

[8] K. Basu and A. B. Owen. Scrambled Geometric Net Integration Over General Product Spaces.

Foundations of Computational Mathematics  17(2):467–496  Apr. 2017.

[9] A. V. Bondarenko  D. Radchenko  and M. S. Viazovska. Optimal asymptotic bounds for

spherical designs. Annals of Mathematics  178(2):443–452  2013.

[10] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning  3(1):1–122  2011.

[11] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[12] J. S. Brauchart and J. Dick. Quasi–Monte Carlo rules for numerical integration over the unit

sphere S2 . Numerische Mathematik  121(3):473–502  2012.

[13] J. S. Brauchart and P. J. Grabner. Distributing many points on spheres: minimal energy and

designs. Journal of Complexity  31(3):293–326  2015.

[14] J. S. Brauchart  D. P. Hardin  and E. B. Saff. The riesz energy of the nth roots of unity: an
asymptotic expansion for large n. Bulletin of the London Mathematical Society  41(4):621–633 
2009.

[15] A. De Maio  Y. Huang  D. P. Palomar  S. Zhang  and A. Farina. Fractional QCQP with
applications in ML steering direction estimation for radar detection. IEEE Transactions on
Signal Processing  59(1):172–185  2011.

[16] J. Dick and F. Pillichshammer. Digital sequences  discrepancy and quasi-Monte Carlo integra-

tion. Cambridge University Press  Cambridge  2010.

[17] M. Götz. On the Riesz energy of measures. Journal of Approximation Theory  122(1):62–78 

2003.

[18] P. J. Grabner. Point sets of minimal energy. In Applications of Algebra and Number Theory
(Lectures on the Occasion of Harald Niederreiter’s 70th Birthday) (edited by G. Larcher  F.
Pillichshammer  A. Winterhof  and C. Xing)  pages 109–125  2014.

[19] D. Hardin and E. Saff. Minimal Riesz energy point conﬁgurations for rectiﬁable d-dimensional

manifolds. Advances in Mathematics  193(1):174–204  2005.

10

[20] Y. Huang and D. P. Palomar. Randomized algorithms for optimal solutions of double-sided
IEEE Transactions on Signal Processing 

QCQP with applications in signal processing.
62(5):1093–1108  2014.

[21] G. R. G. Lanckriet  N. Cristianini  P. L. Bartlett  L. E. Ghaoui  and M. I. Jordan. Learning the
kernel matrix with semi-deﬁnite programming. In Machine Learning  Proceedings of (ICML
2002)  pages 323–330  2002.

[22] J. B. Lasserre. Semideﬁnite programming vs. LP relaxations for polynomial programming.

Mathematics of Operations Research  27(2):347–360  2002.

[23] Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming.

SIAM  1994.

[24] H. Niederreiter. Random Number Generation and Quasi-Monte Carlo Methods. SIAM 

Philadelphia  PA  1992.

[25] B. O’Donoghue  E. Chu  N. Parikh  and S. Boyd. Conic optimization via operator splitting
and homogeneous self-dual embedding. Journal of Optimization Theory and Applications 
169(3):1042–1068  2016.

[26] N. Parikh and S. Boyd. Block splitting for distributed optimization. Mathematical Programming

Computation  6(1):77–102  2014.

[27] R. T. Rockafellar. Convex analysis  1970.
[28] J. Ye  S. Ji  and J. Chen. Learning the kernel matrix in discriminant analysis via quadratically
constrained quadratic programming. In Proceedings of the 13th ACM SIGKDD 2007  pages
854–863  2007.

[29] X. Zhu  J. Kandola  J. Lafferty  and Z. Ghahramani. Graph kernels by spectral transforms.

Semi-supervised learning  pages 277–291  2006.

11

,Arash Amini
XuanLong Nguyen
Kinjal Basu
Ankan Saha