2018,On Coresets for Logistic Regression,Coresets are one of the central methods to facilitate the analysis of large data. We continue a recent line of research applying the theory of coresets to logistic regression. First  we show the negative result that no strongly sublinear sized coresets exist for logistic regression. To deal with intractable worst-case instances   we introduce a complexity measure $\mu(X)$  which quantifies the hardness of compressing a data set for logistic regression. $\mu(X)$ has an intuitive statistical interpretation that may be of independent interest. For data sets with bounded $\mu(X)$-complexity  we show that a novel sensitivity sampling scheme produces the first provably sublinear $(1\pm\eps)$-coreset. We illustrate the performance of our method by comparing to uniform sampling as well as to state of the art methods in the area. The experiments are conducted on real world benchmark data for logistic regression.,On Coresets for Logistic Regression

Alexander Munteanu

Department of Computer Science

TU Dortmund University
44227 Dortmund  Germany

Chris Schwiegelshohn

Department of Computer Science

Sapienza University of Rome

00185 Rome  Italy

alexander.munteanu@tu-dortmund.de

schwiegelshohn@diag.uniroma1.it

Christian Sohler

Department of Computer Science

TU Dortmund University
44227 Dortmund  Germany

christian.sohler@tu-dortmund.de

David P. Woodruff

Department of Computer Science

Carnegie Mellon University
Pittsburgh  PA 15213  USA
dwoodruf@cs.cmu.edu

Abstract

Coresets are one of the central methods to facilitate the analysis of large data. We
continue a recent line of research applying the theory of coresets to logistic regres-
sion. First  we show the negative result that no strongly sublinear sized coresets
exist for logistic regression. To deal with intractable worst-case instances we intro-
duce a complexity measure µ(X)  which quantiﬁes the hardness of compressing a
data set for logistic regression. µ(X) has an intuitive statistical interpretation that
may be of independent interest. For data sets with bounded µ(X)-complexity  we
show that a novel sensitivity sampling scheme produces the ﬁrst provably sublinear
(1 ± ε)-coreset. We illustrate the performance of our method by comparing to
uniform sampling as well as to state of the art methods in the area. The experiments
are conducted on real world benchmark data for logistic regression.

1

Introduction

Scalability is one of the central challenges of modern data analysis and machine learning. Algorithms
with polynomial running time might be regarded as efﬁcient in a conventional sense  but nevertheless
become intractable when facing massive data sets. As a result  performing data reduction techniques
in a preprocessing step to speed up a subsequent optimization problem has received considerable
attention. A natural approach is to sub-sample the data according to a certain probability distribution.
This approach has been successfully applied to a variety of problems including clustering [31  22  6  4] 
mixture models [21  33]  low rank approximation [16]  spectral approximation [2  32]  and Nyström
methods [2  37].
The unifying feature of these works is that the probability distribution is based on the sensitivity
score of each point. Informally  the sensitivity of a point corresponds to the importance of the point
with respect to the objective function we wish to minimize. If the total sensitivity  i.e.  the sum of
all sensitivity scores S  is bounded by a reasonably small value S  there exists a collection of input
points known as a coreset with very strong aggregation properties. Given any candidate solution (e.g. 
a set of k centers for k-means  or a hyperplane for linear regression)  the objective function computed
on the coreset evaluates to the objective function of the original data up to a small multiplicative error.
See Sections 2 and 4 for formal deﬁnitions of sensitivity and coresets.
Our Contribution We investigate coresets for logistic regression within the sensitivity framework.
Logistic regression is an instance of a generalized linear model where we are given data Z ∈ Rn×d 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

and labels Y ∈ {−1  1}n. The optimization task consists of minimizing the negative log-likelihood

(cid:80)n
i=1 ln(1 + exp(−YiZiβ)) with respect to the parameter β ∈ Rd [34].

• Our ﬁrst contribution is an impossibility result: logistic regression has no sublinear streaming
algorithm. Due to a standard reduction between coresets and streaming algorithms  this also implies
that logistic regression admits no coresets or bounded sensitivity scores in general.
• Our second contribution is an investigation of available sensitivity sampling distributions for logistic
regression. For points with large contribution  where −YiZiβ (cid:29) 0  the objective function increases
by a term almost linear in −YiZiβ. This questions the use of sensitivity scores designed for problems
with squared cost functions such as (cid:96)2-regression  k-means  and (cid:96)2-based low-rank approximation.
Instead  we propose sampling from a mixture distribution with one component proportional to the
square root of the (cid:96)2
2 leverage scores. Though seemingly similar to the sampling distributions of e.g.
[6  4] at ﬁrst glance  it is important to note that sampling according to (cid:96)2
2 scores is different from
sampling according to their square roots. The former is good for (cid:96)2-related loss functions  while
the latter preserves (cid:96)1-related functions such as the linear part of the original logistic regression loss
function studied here. The other mixture component is uniform sampling to deal with the remaining
domain  where the cost function consists of an exponential decay towards zero. Our experiments
show that this distribution outperforms uniform and k-means based sensitivity sampling by a wide
margin on real data sets. The algorithm is space efﬁcient  and can be implemented in a variety of
models used to handle large data sets such as 2-pass streaming  and massively parallel frameworks
such as Hadoop and MapReduce  and can be implemented in input sparsity time  ˜O(nnz(Z))  the
number of non-zero entries of the data [12].
• Our third contribution is an analysis of our sampling distribution for a parametrized class of instances
we call µ-complex  placing our work in the framework of beyond worst-case analysis [5  39]. The
parameter µ roughly corresponds to the ratio between the log of correctly estimated odds and the
log of incorrectly estimated odds. The condition of small µ is justiﬁed by the fact that for instances
with large µ  logistic regression exhibits methodological problems like imbalance and separability 
cf. [35  26]. We show that the total sensitivity of logistic regression can be bounded in terms of µ 
and that our sampling scheme produces the ﬁrst coreset of provably sublinear size  provided that µ is
small.
Related Work There is more than a decade of extensive work on sampling based methods relying on
the sensitivity framework for (cid:96)2-regression [19  20  32  15] and (cid:96)1-regression [10  40  11]. These were
generalized to (cid:96)p-regression for all p ∈ [1 ∞) [17  44]. More recent works study sampling methods
for M-estimators [14  13] and extensions to generalized linear models [27  36]. The contemporary
theory behind coresets has been applied to logistic regression  ﬁrst by [38] using ﬁrst order gradient
methods  and subsequently via sensitivity sampling by [27]. In the latter work  the authors recovered
the result that bounded sensitivity scores for logistic regression imply coresets. Explicit sublinear
bounds on the sensitivity scores  as well as an algorithm for computing them  were left as an open
question. Instead  they proposed using sensitivity scores derived from any k-means clustering for
logistic regression. While high sensitivity scores of an input point for k-means provably do not imply
a high sensitivity score of the same point for logistic regression  the authors observed that they can
outperform uniform random sampling on a number of instances with a clustering structure. Recently
and independently of our work  [41] gave a coreset construction for logistic regression in a more
general framework. Our construction is without regularization and therefore can be also applied for
any regularized version of logistic regression  but we have constraints regarding the µ-complexity
of the input. Their result is for (cid:96)2
2-regularization  which signiﬁcantly changes the objective and
does not carry over to the unconstrained version. They do not constrain the input but the domain of
optimization is bounded. This indicates that both results differ in many important points and are of
independent interest.
All proofs and additional plots from the experiments are in the appendices A and B  respectively.

2 Preliminaries and Problem Setting
In logistic regression we are given a data matrix Z ∈ Rn×d  and labels Y ∈ {−1  1}n. Logistic
regression has a negative log-likelihood [34]

(cid:88)n

i=1

L(β|Z  Y ) =

ln(1 + exp(−YiZiβ))

2

which from a learning and optimization perspective  is the objective function that we would like to
minimize over β ∈ Rd. For brevity we fold for all i ∈ [n] the labels Yi as well as the factor −1 in
the exponent into X ∈ Rn×d comprising row vectors xi = −YiZi. Let g(z) = ln(1 + exp(z)). For
technical reasons we deal with a weighted version for weights w ∈ Rn
>0  where each weight satisﬁes
wi > 0. Any positive scaling of the all ones vector 1 = {1}n corresponds to the unweighted case.
We denote by Dw a diagonal matrix carrying the entries of w  i.e.  (Dw)ii = wi  so that multiplying
Dw to a vector or matrix has the effect of scaling row i by a factor of wi. The objective function
becomes

(cid:88)n

(cid:88)n

fw(Xβ) =

wig(xiβ) =

wi ln(1 + exp(xiβ)).

i=1

i=1

In this paper we assume we have a very large number of observations in a moderate number of
dimensions  that is  n (cid:29) d. In order to speed up the computation and to lower memory and storage
requirements we would like to signiﬁcantly reduce the number of observations without losing much
information in the original data. A suitable data compression reduces the size to a sublinear number
of o(n) data points while the dependence on d and the approximation parameters may be polynomials
of low degree. To achieve this  we design a so-called coreset construction for the objective function.
A coreset is a possibly (re)weighted and signiﬁcantly smaller subset of the data that approximates the
objective value for any possible query points. More formally  we deﬁne coresets for the weighted
logistic regression function.
Deﬁnition 1 ((1 ± ε)-coreset for logistic regression). Let X ∈ Rn×d be a set of points weighted
by w ∈ Rn
>0  is a (1 ± ε)-coreset of X for fw  if
k (cid:28) n and

>0. Then a set C ∈ Rk×d  (re)weighted by u ∈ Rk

∀β ∈ Rd : |fw(Xβ) − fu(Cβ)| ≤ ε · fw(Xβ).

µ-Complex Data Sets We will see in Section 3 that in general  there is no sublinear one-pass
streaming algorithm approximating the objective function up to any ﬁnite constant factor. More
speciﬁcally there exists no sublinear summary or coreset construction that works for all data sets.
For the sake of developing coreset constructions that work reasonably well  as well as conducting a
formal analysis beyond worst-case instances  we introduce a measure µ that quantiﬁes the complexity
of compressing a given data set.
Deﬁnition 2. Given a data set X ∈ Rn×d weighted by w ∈ Rn
>0 and a vector β ∈ Rd let (DwXβ)−
denote the vector comprising only the negative entries of DwXβ. Similarly let (DwXβ)+ denote
the vector of positive entries. We deﬁne for X weighted by w

(cid:107)(DwXβ)+(cid:107)1
(cid:107)(DwXβ)−(cid:107)1

.

µw(X) = sup

β∈Rd\{0}
X weighted by w is called µ-complex if µw(X) ≤ µ.
The size of our (1 ± ε)-coreset constructions for logistic regression for a given µ-complex data set X
will have low polynomial dependency on µ  d  1/ε but only sublinear dependency on its original size
parameter n. So for µ-complex data sets having small µ(X) ≤ µ we have the ﬁrst (1 ± ε)-coreset of
provably sublinear size. The above deﬁnition implies  for µ(X) ≤ µ  the following inequalities. The
reader should keep in mind that for all β ∈ Rd

−1(cid:107)(DwXβ)
µ

−(cid:107)1 ≤ (cid:107)(DwXβ)+(cid:107)1 ≤ µ(cid:107)(DwXβ)

−(cid:107)1 .

We conjecture that computing the value of µ(X) is hard. However  it can be approximated in
polynomial time. It is not necessary to do so in practical applications  but we include this result for
those who wish to evaluate whether their data has nice µ-complexity.
Theorem 3. Let X ∈ Rn×d be weighted by w ∈ Rn
of µw(X) can be computed in O(poly(nd)) time.

>0. Then a poly(d)-approximation to the value

The parameter µ(X) has an intuitive interpretation and might be of independent interest. The odds of
P[V =1]
P[V =0] . The model assumption of logistic regression is
a binary random variable V are deﬁned as
that for every sample Xi  the logarithm of the odds is a linear function of Xiβ. For a candidate β 
multiplying all odds and taking the logarithm is then exactly (cid:107)Xβ(cid:107)1. Our deﬁnition now relates the
probability mass due to the incorrectly predicted odds and the probability mass due to the correctly

3

predicted odds. We say that the ratio between these two is upper bounded by µ. For logistic regression 
assuming they are within some order of magnitude is not uncommon. One extreme is the (degenerate)
case where the data set is exactly separable. Choosing β to parameterize a separating hyperplane for
which Xβ is all positive  implies that µ(X) = ∞. Another case is when we have a large ratio between
the number of positively and negatively labeled points which is a lower bound to µ. Under either
of these conditions  logistic regression exhibits methodological weaknesses due to the separation or
imbalance between the given classes  cf. [35  26].

3 Lower Bounds

At ﬁrst glance  one might think of taking a uniform sample as a coreset. We demonstrate and discuss
on worst-case instances in Appendix C that this won’t work in theory or in practice. In the following
we will show a much stronger result  namely that no efﬁcient streaming algorithms or coresets
for logistic regression can exist in general  even if we assume that the points lie in 2-dimensional
Euclidean space. To this end we will reduce from the INDEX communication game. In its basic
variant  there exist two players Alice and Bob. Alice is given a binary bit string x ∈ {0  1}n and Bob
is given an index i ∈ [n]. The goal is to determine the value of xi with constant probability while
using as little communication as possible. Clearly  the difﬁculty of the problem is inherently one-way;
otherwise Bob could simply send his index to Alice. If the entire communication consists of only a
single message sent by Alice to Bob  the message must contain Ω(n) bits [30].
Theorem 4. Let Z ∈ Rn×2  Y ∈ {−1  1}n be an instance of logistic regression in 2-dimensional
Euclidean space. Any one-pass streaming algorithm that approximates the optimal solution of logistic
regression up to any ﬁnite multiplicative approximation factor requires Ω(n/ log n) bits of space.

A similar reduction also holds if Alice’s message consists of points forming a coreset. Hence  the
following corollary holds.
Corollary 5. Let Z ∈ Rn×2  Y ∈ {−1  1}n be an instance of logistic regression in 2-dimensional
Euclidean space. Any coreset of Z  Y for logistic regression consists of at least Ω(n/ log n) points.

We note that the proof can be slightly modiﬁed to rule out any ﬁnite additive error as well. This
indicates that the notion of lightweight coresets with multiplicative and additive error [4] is not a
sufﬁcient relaxation. Independently of our work [41] gave a linear lower bound in a more general
context based on a worst case instance to the sensitivity approach due to [27]. Our lower bounds and
theirs are incomparable; they show that if a coreset can only consist of input points it comprises the
entire data set in the worst-case. We show that no coreset with o(n/ log n) can exist  irrespective
of whether input points are used. While the distinction may seem minor  a number of coreset
constructions in literature necessitate the use of non-input points  see [1] and [23].

4 Sampling via Sensitivity Scores

1. Recall the function under study is fw(Xβ) =(cid:80)n

Our sampling based coreset constructions are obtained with the following approach  called sensitivity
sampling. Suppose we are given a data set X ∈ Rn×d together with weights w ∈ Rn
>0 as in Deﬁnition
i=1 wi · g(xiβ). Associate with each point xi the
function gi(β) = g(xiβ). Then we have the following deﬁnition.
Deﬁnition 6. [31] Consider a family of functions F = {g1  . . .   gn} mapping from Rd to [0 ∞) and
weighted by w ∈ Rn

>0. The sensitivity of gi for fw(β) =(cid:80)n

i=1 wigi(β) is

(1)
where the sup is over all β ∈ Rd with fw(β) > 0. If this set is empty then ςi = 0. The total sensitivity

ςi = sup

wigi(β)
fw(β)

is S =(cid:80)n

i=1 ςi.

The sensitivity of a point measures its worst-case importance for approximating the objective function
on the entire input data set. Performing importance sampling proportional to the sensitivities of the
input points thus yields a good approximation. Computing the sensitivities is often intractable and
involves solving the original optimization problem to near-optimality  which is the problem we want
to solve in the ﬁrst place  as pointed out in [8]. To get around this  it was shown that any upper bound

4

i=1 si ≥(cid:80)n

depends on the total sensitivity  that is  the sum of their estimates S =(cid:80)n

on the sensitivities si ≥ ςi also has provable guarantees. However  the number of samples needed
i=1 ςi = S  so
we need to carefully control this quantity. Another complexity measure that plays a crucial role in the
sampling complexity is the VC dimension of the range space induced by the set of functions under
study.
Deﬁnition 7. A range space is a pair R = (F  ranges) where F is a set and ranges is a family of
subsets of F. The VC dimension ∆(R) of R is the size |G| of the largest subset G ⊆ F such that G
is shattered by ranges  i.e.  |{G ∩ R | R ∈ ranges}| = 2|G|.
Deﬁnition 8. Let F be a ﬁnite set of functions mapping from Rd to R≥0. For every β ∈ Rd and
r ∈ R≥0  let rangeF (β  r) = {f ∈ F | f (β) ≥ r}  and ranges(F) = {rangeF (β  r) | β ∈ Rd  r ∈
R≥0}  and RF = (F  ranges(F)) be the range space induced by F.
Recently a framework combining the sensitivity scores with a theory on the VC dimension of range
spaces was developed in [8]. For technical reasons we use a slightly modiﬁed version.
Theorem 9. Consider a family of functions F = {f1  . . .   fn} mapping from Rd to [0 ∞) and a
vector of weights w ∈ Rn
i=1 si ≥ S. Given si one
can compute in time O(|F|) a set R ⊂ F of

(cid:19)(cid:19)(cid:19)

>0. Let ε  δ ∈ (0  1/2). Let si ≥ ςi. Let S =(cid:80)n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:18) S
(cid:18)
wifi(β) −(cid:88)

(cid:18) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ε

∆ log S + log

(cid:88)

(cid:88)

wifi(β).

uifi(β)

O

ε2

δ

weighted functions such that with probability 1 − δ we have for all β ∈ Rd simultaneously

f∈F

f∈R

f∈F

S from F  ui = Swj

where each element of R is sampled i.i.d. with probability pj = sj
sj|R| denotes the
weight of a function fi ∈ R that corresponds to fj ∈ F  and where ∆ is an upper bound on the VC
dimension of the range space RF∗ induced by F∗ that can be obtained by deﬁning F∗ to be the set
of functions fj ∈ F where each function is scaled by Swj
sj|R| .
Now we show that the VC dimension of the range space induced by the set of functions studied in
logistic regression can be related to the VC dimension of the set of linear classiﬁers. We ﬁrst start
with a ﬁxed common weight and generalize the result to a more general ﬁnite set of distinct weights.
Lemma 10. Let X ∈ Rn×d  c ∈ R>0. The range space induced by F c
log = {c · g(xiβ)| i ∈ [n]}
satisﬁes ∆(RF c
Lemma 11. Let X ∈ Rn×d be weighted by w ∈ Rn where wi ∈ {v1  . . .   vt} for all i ∈ [n]. The
range space induced by Flog = {wi · g(xiβ) | i ∈ [n]} satisﬁes ∆(RFlog ) ≤ t · (d + 1).
We will see later how to bound the number of distinct weights t by a logarithmic term in the range of
the involved weights. It remains for us to derive tight and efﬁciently computable upper bounds on the
sensitivities.
Base Algorithm We show that sampling proportional to the square root of the (cid:96)2-leverage scores
j∈[n] wj yields a coreset whose size is roughly linear in µ and the dependence

augmented by wi/(cid:80)

log ) ≤ d + 1.

n. In what follows  let W =(cid:80)

on the input size is roughly
We make a case distinction covered by lemmas 12 and 13. The intuition in the ﬁrst case is that for
a sufﬁciently large positive entry z  we have that |z| ≤ g(z) ≤ 2|z|. The lower bound holds even
for all non-negative entries. Moreover  for µ-complex inputs we are able to relate the (cid:96)1 norm of all
entries to the positive ones  which will yield the desired bound  arguing similarly to the techniques of
[13] though adapted here for logistic regression.
Lemma 12. Let X ∈ Rn×d weighted by w ∈ Rn
>0 be µ-complex. Let U be an orthonormal
basis for the columnspace of DwX. If for index i  the supreme β in (1) satisﬁes 0.5 ≤ xiβ then
wig(xiβ) ≤ 2(1 + µ)(cid:107)Ui(cid:107)2fw(Xβ).
In the second case  the element under study is bounded by a constant. We consider two sub cases. If
there are a lot of contributions  which are not too small  and thus cost at least a constant each  then

i∈[n] wi.

√

5

nd.

>0 be µ-complex. If for index i  the supreme β in (1)

we can lower bound the total cost by a constant times their total weight. If on the other hand there are
many very small negative values  then this implies again that the cost is within a µ fraction of the
total weight.
Lemma 13. Let X ∈ Rn×d weighted by w ∈ Rn
satisﬁes 0.5 ≥ xiβ then wig(xiβ) ≤ (20+µ)wiW fw(Xβ).
Combining both lemmas yields general upper bounds on the sensitivities that we can use as an
importance sampling distribution. We also derive an upper bound on the total sensitivity that will be
used to bound the sampling complexity.
Lemma 14. Let X ∈ Rn×d weighted by w ∈ Rn
>0 be µ-complex. Let U be an orthonormal basis
for the columnspace of DwX. For each i ∈ [n]  the sensitivity of gi(β) = g(xiβ) for the weighted
logistic regression function is bounded by ςi ≤ si = (20+2µ)·((cid:107)Ui(cid:107)2 +wi/W). The total sensitivity
√
is bounded by S ≤ S ≤ 44µ
We combine the above results into the following theorem.
Theorem 15. Let X ∈ Rn×d weighted by w ∈ Rn be µ-complex. Let ω = wmax
be the ratio between
the maximum and minimum weight in w. Let ε ∈ (0  1/2). There exists a (1 ± ε)-coreset of X  w for
√
logistic regression of size k ∈ O( µ
ε2 d3/2 log(µnd) log(ωn)). Such a coreset can be constructed
in two passes over the data  in O(nnz(X) log n + poly(d) log n) time  and with success probability
1 − 1/nc for any absolute constant c > 1.
Recursive Algorithm Here we develop a recursive algorithm  inspired by the recursive sampling
technique of [14] for the Huber M-estimator  though adapted here for logistic regression. This yields
a better dependence on the input size. More speciﬁcally  we can diminish the leading
n factor
to only logc(n) for an absolute constant c. One complication is that the parameter µ grows in the
recursion  which we need to control  while another complication is having to deal with the separate
(cid:96)1 and uniform parts of our sampling distribution.
We apply the Algorithm of Theorem 15 recursively. To do so  we need to ensure that after one stage
of subsampling and reweighting  the resulting data set remains µ(cid:48)-complex for a value µ(cid:48) that is not
too much larger than µ. To this end  we ﬁrst bound the VC dimension of a range space induced by an
(cid:96)1 related family of functions.
Lemma 16. The range space induced by F(cid:96)1 = {hi(β) = wi|xiβ|| i ∈ [n]} satisﬁes ∆(RF(cid:96)1
) ≤
10(d + 1).
Applying Theorem 9 to F(cid:96)1 implies that the subsample of Theorem 15 satisﬁes a so called ε-subspace
embedding property for (cid:96)1. Note that  by linearity of the (cid:96)1-norm  we can fold the weights into DwX.
Lemma 17. Let T be a sampling and reweighting matrix according to Theorem 15. I.e.  T DwX is
the resulting reweighted sample when Theorem 15 is applied to µ-complex input X  w. Then with
probability 1 − 1/nc  for all β ∈ Rd simultaneously

√

wmin

n

(1 − ε
(cid:48)
√

)(cid:107)DwXβ(cid:107)1 ≤ (cid:107)T DwXβ(cid:107)1 ≤ (1 + ε

(cid:48)

)(cid:107)DwXβ(cid:107)1

µ + 1.

holds  where ε(cid:48) = ε/
Using this  we can show that the µ-complexity is not violated too much after one stage of sampling.
Lemma 18. Let T be a sampling and reweighting matrix according to Theorem 15 where parameter
ε is replaced by ε/
µ + 1. That is T DwX is the resulting reweighted sample when Theorem 15
succeeds on µ-complex input X  w. Suppose that simultaneously Lemma 17 holds. Let

√

(cid:107)(T DwXβ)+(cid:107)1
(cid:107)(T DwXβ)−(cid:107)1

.

= µT w(X) = sup
β∈Rd

(cid:48)
µ
Then we have µ(cid:48) ≤ (1 + ε)µ.
Now we are ready to prove our theorem regarding the recursive subsampling algorithm.
Theorem 19. Let X ∈ Rn×d be µ-complex. Let ε ∈ (0  1/2). There exists a (1 ± ε)-coreset of
X for logistic regression of size k ∈ O( µ3
ε4 d3 log2(µnd) log2 n (log log n)4). Such a coreset can be
constructed in time O((nnz(X) + poly(d)) log n log log n) in 2 log( 1
η ) passes over the data for a
small η > 0  assuming the machine has access to sufﬁcient memory to store and process ˜O(nη)
weighted points. The success probability is 1 − 1/nc for any absolute constant c > 1.

6

5 Experiments

We ran a series of experiments to illustrate the performance of our coreset method. All experiments
were run on a Linux machine using an Intel i7-6700  4 core CPU at 3.4 GHz  and 32GB of RAM. We
implemented our algorithms in Python. Now  we compare our basic algorithm to simple uniform
sampling and to sampling proportional to the sensitivity upper bounds given by [27].
Implementation Details The approach of [27] is based on a k-means++ clustering [3] on a small
uniform sample of the data and was performed using standard parameters taken from the publication.
For this purpose we used parts of their original Python code. However  we removed the restriction
of the domain of optimization to a region of small radius around the origin. This way  we enabled
unconstrained regression in the domain Rd. The exact QR-decomposition is rather slow on large
data matrices. We thus optimized the running time of our approach in the following way. We used
a fast approximation algorithm based on the sketching techniques of [12]  cf. [43]. That leads to a
provable constant approximation of the square root of the leverage scores with constant probability 
cf. [18]  which means that the total sensitivity bounds given in our theory will grow by only a
small constant factor. A detailed description of the algorithm is in the proof of Theorem 15. The
subsequent optimization was done for all approaches with the standard gradient based optimizer from
the scipy.optimize package  see http://www.scipy.org/.
Data Sets We brieﬂy introduce the data sets that we used. The WEBB SPAM1 data consists of
350  000 unigrams with 127 features from web pages which have to be classiﬁed as spam or normal
pages (61% positive). The COVERTYPE2 data consists of 581  012 cartographic observations of
different forests with 54 features. The task is to predict the type of trees at each location (49%
positive). The KDD CUP ’993 data comprises 494  021 network connections with 41 features and
the task is to detect network intrusions (20% positive).
Experimental Assessment For each data set we assessed the total running times for computing the
sampling probabilities  sampling and optimizing on the sample. In order to assess the approximation
accuracy we examined the relative error |L(β∗|X)−L( ˜β|X)|/L(β∗|X) of the negative log-likelihood
for the maximum likelihood estimators obtained from the full data set β∗ and the subsamples ˜β. For
each data set  we ran all three subsampling algorithms for a number of thirty regular subsampling
steps in the range k ∈ [(cid:98)2
n(cid:99) (cid:100)n/16(cid:101)]. For each step  we present the mean relative error as well as
the trade-off between mean relative error and running time  taken over twenty independent repetitions 
in Figure 1. Relative running times  standard deviations and absolute values are presented in Figure 2
respectively in Table 1 in Appendix B.
Evaluation The accuracy of the QR-sampling distribution outperforms uniform sampling and the
distribution derived from k-means on all instances. This is especially true for small sampling
sizes. Here  the relative error especially for uniform sampling tends to deteriorate. While k-means
sampling occasionally improved over uniform sampling for small sample sizes  the behavior of
both distributions was similar for larger sampling sizes. The standard deviations had a similarly
low magnitude as the mean values  where the QR method usually showed the lowest values. The
trade-off between the running time and relative errors shows a common picture for WEBB SPAM and
COVERTYPE. QR is nearly always more accurate than the other algorithms for a similar time budget 
except for regions where the relative error is large  say above 5-10% while for larger time budgets 
QR is better by a factor between 1.5-3 and drops more quickly towards 0. The conclusion so far
could be that for a quick guess  say a 1.1-approximation  the competitors are faster  but to provably
obtain a reasonably small relative error below 5%  QR outperforms its competitors. However  for
KDD CUP ’99  QR always has a lower error than its competitors. Their relative errors remain above
15% or much worse  while QR never exceeds 22% and drops quickly below 4%. Our estimates for µ
support that KDD CUP ’99 seems more difﬁcult to approximate than the others. The estimates were
4.39 for WEBB SPAM  1.86 for COVERTYPE  and 35.18 for KDD CUP ’99.
The relative running time for the QR-distribution was comparable to k-means and only slightly higher
than uniform sampling. However  it never exceeded a factor of two compared to its competitors
and remained negligible compared to the full optimization task  see Figure 2 in Appendix B. The
standard deviations were negligible except for the k-means algorithm and the KDD CUP ’99 data

√

1https://www.cc.gatech.edu/projects/doi/WebbSpamCorpus.html
2https://archive.ics.uci.edu/ml/datasets/covertype
3http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html

7

WEBB SPAM

COVERTYPE

KDD CUP ’99

Figure 1: Each column shows the results for one data set comprising thirty different coreset sizes
(depending on the individual size of the data sets). The plotted values are means taken over twenty
independent repetitions of each experiment. The plots in the upper row show the mean relative
log-likelihood errors of the three subsampling distributions  uniform sampling (blue)  our QR
derived distribution (red)  and the k-means based distribution (green). All values are relative to the
corresponding optimal log-likelihood values of the optimization task on the full data set. The plots in
the lower row show the trade-off between running time and relative errors (lower is better).

set  where the uniform and k-means based algorithms showed larger values. The QR method had
much lower standard deviations. This indicates that the resulting coresets are more stable for the
subsequent numerical optimization. We note that the savings of all presented data reduction methods
become even more signiﬁcant when performing more time consuming data analysis tasks like MCMC
sampling in a Bayesian setting  see e.g.  [27  24].

6 Conclusions

We ﬁrst showed that (sublinear) coresets for logistic regression do not exist in general. It is thus
necessary to make further assumptions on the nature of the data. To this end we introduced a new
complexity measure µ(X)  which quantiﬁes the amount of overlap of positive and negative classes
and the balance in their cardinalities. We developed the ﬁrst rigorously sublinear (1 ± ε)-coresets
√
for logistic regression  given that the original data has small µ-complexity. The leading factor is
O(ε−2µ
n). We have further developed a recursive coreset construction that reduces the dependence
on the input size to only O(logc n) for absolute constant c. This comes at the cost of an increased
dependence on µ. However  it is beneﬁcial for very large and well-behaved data. Our algorithms are
space efﬁcient  and can be implemented in a variety of models  used to tackle the challenges of large
data sets  such as 2-pass streaming  and massively parallel frameworks like Hadoop and MapReduce 
and can be implemented to run in input sparsity time ˜O(nnz(X))  which is especially beneﬁcial for
sparsely encoded input data. Our experimental evaluation shows that our implementation of the basic
algorithm outperforms uniform sampling as well as state of the art methods in the area of coresets for
logistic regression while being competitive to both regarding its running time.

Acknowledgments

We thank the anonymous reviewers for their valuable comments. We also thank our assistant Moritz Paweletz.
This work was supported by the German Science Foundation (DFG) Collaborative Research Center SFB 876 
projects A2 and C4. Chris Schwiegelshohn is supported in part by an ERC Advanced Grant 788893 AMDROMA.
David P. Woodruff is supported in part by an Ofﬁce of Naval Research (ONR) grant N00014-18-1-2562  and
part of this work was done while he was visiting the Simons Institute for the Theory of Computing.

8

25005000750010000125001500017500samplesize0.050.100.150.200.250.300.350.40meanrelativelog-likelihooderrorQRUniformk-Means50001000015000200002500030000samplesize0.000.050.100.150.200.250.300.35meanrelativelog-likelihooderrorQRUniformk-Means500010000150002000025000samplesize0.00.10.20.30.40.50.60.70.8meanrelativelog-likelihooderrorQRUniformk-Means51015202530absoluterunningtime(s)0.0250.0500.0750.1000.1250.1500.1750.200meanrelativelog-likelihooderrorQRUniformk-Means5101520absoluterunningtime(s)0.0000.0250.0500.0750.1000.1250.1500.175meanrelativelog-likelihooderrorQRUniformk-Means2468101214absoluterunningtime(s)0.00.10.20.30.40.50.60.70.8meanrelativelog-likelihooderrorQRUniformk-MeansReferences
[1] P. K. Agarwal  S. Har-Peled  and K. R. Varadarajan. Approximating extent measures of points. Journal of

the ACM  51(4):606–635  2004.

[2] A. E. Alaoui and M. W. Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In

Advances in Neural Information Processing Systems 28 (NIPS)  pages 775–783  2015.

[3] D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings of the 18th

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 1027–1035  2007.

[4] O. Bachem  M. Lucic  and A. Krause. Scalable k-means clustering via lightweight coresets. In Proceedings
of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) 
pages 1119–1127  2018.

[5] M.-F. Balcan  B. Manthey  H. Röglin  and T. Roughgarden. Analysis of algorithms beyond the worst case

(Dagstuhl seminar 14372). Dagstuhl Reports  4(9):30–49  2015.

[6] A. Barger and D. Feldman. k-means for streaming and distributed big sparse data. In Proceedings of the

SIAM International Conference on Data Mining (SDM)  pages 342–350  2016.

[7] A. Blumer  A. Ehrenfeucht  D. Haussler  and M. K. Warmuth. Learnability and the Vapnik-Chervonenkis

dimension. Journal of the ACM  36(4):929–965  1989.

[8] V. Braverman  D. Feldman  and H. Lang. New frameworks for ofﬂine and streaming coreset constructions.

arXiv preprint CoRR  abs/1612.00889  2016.

[9] M. T. Chao. A general purpose unequal probability sampling plan. Biometrika  69(3):653–656  1982.

[10] K. L. Clarkson. Subgradient and sampling algorithms for (cid:96)1 regression. In Proceedings of the 16th annual

ACM-SIAM symposium on Discrete algorithms (SODA)  pages 257–266  2005.

[11] K. L. Clarkson  P. Drineas  M. Magdon-Ismail  M. W. Mahoney  X. Meng  and D. P. Woodruff. The fast

Cauchy transform and faster robust linear regression. SIAM J. Comput.  45(3):763–810  2016.

[12] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In

Symposium on Theory of Computing (STOC)  pages 81–90  2013.

[13] K. L. Clarkson and D. P. Woodruff. Input sparsity and hardness for robust subspace approximation. In

IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS)  pages 310–329  2015.

[14] K. L. Clarkson and D. P. Woodruff. Sketching for M-estimators: A uniﬁed approach to robust regression. In
Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 921–939 
2015.

[15] M. B. Cohen  Y. T. Lee  C. Musco  C. Musco  R. Peng  and A. Sidford. Uniform sampling for matrix
approximation. In Proceedings of the Conference on Innovations in Theoretical Computer Science (ITCS) 
pages 181–190  2015.

[16] M. B. Cohen  C. Musco  and C. Musco. Input sparsity time low-rank approximation via ridge leverage score
sampling. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) 
pages 1758–1777  2017.

[17] A. Dasgupta  P. Drineas  B. Harb  R. Kumar  and M. W. Mahoney. Sampling algorithms and coresets for

(cid:96)p regression. SIAM Journal on Computing  38(5):2060–2078  2009.

[18] P. Drineas  M. Magdon-Ismail  M. W. Mahoney  and D. P. Woodruff. Fast approximation of matrix

coherence and statistical leverage. Journal of Machine Learning Research  13:3475–3506  2012.

[19] P. Drineas  M. W. Mahoney  and S. Muthukrishnan. Sampling algorithms for (cid:96)2 regression and applications.
In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 1127–
1136  2006.

[20] P. Drineas  M. W. Mahoney  and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM

Journal on Matrix Analysis and Applications  30(2):844–881  2008.

[21] D. Feldman  M. Faulkner  and A. Krause. Scalable training of mixture models via coresets. In Advances in

Neural Information Processing Systems 24 (NIPS)  pages 2142–2150  2011.

9

[22] D. Feldman and M. Langberg. A uniﬁed framework for approximating and clustering data. In Proceedings

of the 43rd ACM Symposium on Theory of Computing (STOC)  pages 569–578  2011.

[23] D. Feldman  M. Schmidt  and C. Sohler. Turning big data into tiny data: Constant-size coresets for k-means 
PCA and projective clustering. In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA)  pages 1434–1453  2013.

[24] L. N. Geppert  K. Ickstadt  A. Munteanu  J. Quedenfeld  and C. Sohler. Random projections for Bayesian

regression. Statistics and Computing  27(1):79–101  2017.

[25] G. H. Golub and C. F. van Loan. Matrix computations (4. ed.). J. Hopkins Univ. Press  2013.

[26] G. Heinze and M. Schemper. A solution to the problem of separation in logistic regression. Statistics in

Medicine  21(16):2409–2419  2002.

[27] J. H. Huggins  T. Campbell  and T. Broderick. Coresets for scalable Bayesian logistic regression. In

Advances in Neural Information Processing Systems 29 (NIPS)  pages 4080–4088  2016.

[28] W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary

Mathematics  26(1):189–206  1984.

[29] M. J. Kearns and U. V. Vazirani. An Introduction to Computational Learning Theory. MIT Press  1994.

[30] I. Kremer  N. Nisan  and D. Ron. On randomized one-round communication complexity. Computational

Complexity  8(1):21–49  1999.

[31] M. Langberg and L. J. Schulman. Universal ε-approximators for integrals. In Proceedings of the 21st

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 598–607  2010.

[32] M. Li  G. L. Miller  and R. Peng. Iterative row sampling. In 54th Annual IEEE Symposium on Foundations

of Computer Science (FOCS)  pages 127–136  2013.

[33] M. Lucic  O. Bachem  and A. Krause. Strong coresets for hard and soft Bregman clustering with
applications to exponential family mixtures. In Proceedings of the 19th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1–9  2016.

[34] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall  London  1989.

[35] C. R. Mehta and N. R. Patel. Exact logistic regression: Theory and examples. Statistics in Medicine 

14(19):2143–2160  1995.

[36] A. Molina  A. Munteanu  and K. Kersting. Core dependency networks. In Proceedings of the 32nd AAAI

Conference on Artiﬁcial Intelligence (AAAI)  2018.

[37] C. Musco and C. Musco. Recursive sampling for the Nyström method. In Advances in Neural Information

Processing Systems 30 (NIPS)  pages 3836–3848  2017.

[38] S. J. Reddi  B. Póczos  and A. J. Smola. Communication efﬁcient coresets for empirical loss minimization.
In Proceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages
752–761  2015.

[39] T. Roughgarden. Beyond worst-case analysis  2017. Invited talk held at the Highlights of Algorithms

conference (HALG)  2017.

[40] C. Sohler and D. P. Woodruff. Subspace embeddings for the L1-norm with applications. In Proceedings of

the 43rd ACM Symposium on Theory of Computing (STOC)  pages 755–764  2011.

[41] E. Tolochinsky and D. Feldman. Coresets for monotonic functions with applications to deep learning.

CoRR  abs/1802.07382  2018.

[42] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer  New York  USA  1995.

[43] D. P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical

Computer Science  10(1-2):1–157  2014.

[44] D. P. Woodruff and Q. Zhang. Subspace embeddings and (cid:96)p-regression using exponential random variables.

In The 26th Conference on Learning Theory (COLT)  pages 546–567  2013.

10

,Alexander Munteanu
Chris Schwiegelshohn
Christian Sohler
David Woodruff