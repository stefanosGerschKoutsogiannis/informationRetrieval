2019,Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD,We study the convergence of Stochastic Gradient Descent (SGD) for strongly convex  objective functions. We prove for all $t$ a lower bound on the expected convergence rate after the $t$-th SGD iteration; the lower bound is over all possible sequences of diminishing step sizes. It implies that recently proposed sequences of step sizes at ICML 2018 and ICML 2019 are {\em universally} close to optimal in that the expected convergence rate after {\em each} iteration is within a factor $32$ of our lower bound. This factor is independent of dimension $d$. We offer a framework for comparing with lower bounds in state-of-the-art literature and when applied to SGD for strongly convex objective functions our lower bound is a significant factor $775\cdot d$ larger compared to existing work.,Tight Dimension Independent Lower Bound on the
Expected Convergence Rate for Diminishing Step

Sizes in SGD

Phuong Ha Nguyen

Lam M. Nguyen

Electrical and Computer Engineering

IBM Research  Thomas J. Watson Research Center

University of Connecticut  USA

phuongha.ntu@gmail.com

Yorktown Heights  USA

LamNguyen.MLTD@ibm.com

Marten van Dijk

Electrical and Computer Engineering

University of Connecticut  USA
marten.van_dijk@uconn.edu

Abstract

We study the convergence of Stochastic Gradient Descent (SGD) for strongly
convex objective functions. We prove for all t a lower bound on the expected
convergence rate after the t-th SGD iteration; the lower bound is over all possible
sequences of diminishing step sizes. It implies that recently proposed sequences
of step sizes at ICML 2018 and ICML 2019 are universally close to optimal in
that the expected convergence rate after each iteration is within a factor 32 of our
lower bound. This factor is independent of dimension d. We offer a framework
for comparing with lower bounds in state-of-the-art literature and when applied to
SGD for strongly convex objective functions our lower bound is a signiﬁcant factor
775 · d larger compared to existing work.

1

Introduction

We are interested in solving the following stochastic optimization problem

{F (w) = E[f (w; ξ)]}  

min
w∈Rd
where ξ is a random variable obeying some distribution g(ξ). In the case of empirical risk mini-
mization with a training set {(xi  yi)}n
i=1  ξi is a random variable that is deﬁned by a single random
sample (x  y) pulled uniformly from the training set. Then  by deﬁning fi(w) := f (w; ξi)  empirical
risk minimization reduces to

(1)

(cid:40)

min
w∈Rd

F (w) =

fi(w)

.

(2)

(cid:41)

n(cid:88)

i=1

1
n

n(cid:88)

i=1

Problems of this type arise frequently in supervised learning applications [8]. The classic ﬁrst-order
methods to solve problem (2) are gradient descent (GD) [19] and stochastic gradient descent (SGD)1
[21] algorithms. GD is a standard deterministic gradient method  which updates iterates along the
negative full gradient with learning rate ηt as follows

wt+1 = wt − ηt∇F (wt) = wt − ηt
n

∇fi(wt)   t ≥ 0.

1We notice that even though stochastic gradient is referred to as SG in literature  the term stochastic gradient

descent (SGD) has been widely used in many important works of large-scale learning.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We can choose ηt = η = O(1/L) and achieve a linear convergence rate for the strongly convex case
[15]. The upper bound of the convergence rate of GD and SGD has been studied in [2  4  15  22  17 
16  7].
The disadvantage of GD is that it requires evaluation of n derivatives at each step  which is very
expensive and therefore avoided in large-scale optimization. To reduce the computational cost for
solving (2)  a class of variance reduction methods [11  5  9  18] has been proposed. The difference
between GD and variance reduction methods is that GD needs to compute the full gradient at each
step  while the variance reduction methods will compute the full gradient after a certain number of
steps. In this way  variance reduction methods have less computational cost compared to GD. To
avoid evaluating the full gradient at all  SGD generates an unbiased random variable ξt satisfying

Eξt [∇f (wt; ξt)] = ∇F (wt) 

and then evaluates gradient ∇f (wt; ξt) for ξt drawn from a distribution g(ξ). After this  wt is updated
as follows

wt+1 = wt − ηt∇f (wt; ξt).

(3)

We focus on the general problem (1) where F is strongly convex. Since F is strongly convex  a
unique optimal solution of (1) exists and throughout the paper we denote this optimal solution by w∗
and are interested in studying the expected convergence rate
Yt = E[(cid:107)wt − w∗(cid:107)2].

Algorithm 1 provides a detailed description of SGD. Obviously  the computational cost of a single
iteration in SGD is n times cheaper than that of a single iteration in GD. However  as has been shown
in literature we need to choose ηt = O(1/t) and the expected convergence rate of SGD is slowed
down to O(1/t) [3]  which is a sublinear convergence rate.

Algorithm 1 Stochastic Gradient Descent (SGD) Method

Initialize: w0
Iterate:
for t = 0  1  . . . do

Choose a step size (i.e.  learning rate) ηt > 0.
Generate a random variable ξt with probability density g(ξt).
Compute a stochastic gradient ∇f (wt; ξt).
Update the new iterate wt+1 = wt − ηt∇f (wt; ξt).

end for

Problem Statement and Contributions: We seek to ﬁnd a tight lower bound on the expected
convergence rate Yt with the purpose of showing that the stepsize sequences of [17] and [7] for
classical SGD is optimal for µ-strongly convex and L-smooth respectively expected L-smooth
objective functions within a small dimension independent constant factor. This is important because
of the following reasons:

1. The lower bound tells us that a sequence of stepsizes as a function of only µ and L cannot
beat an expected convergence rate of O(1/t) – this is known general knowledge and was
already proven in [1]  where a dimension dependent lower bound for a larger class of
algorithms that includes SGD was proven. For the class of SGD with diminishing stepsizes
as a function of only global parameters µ and L we show a dimension independent lower
bound which is a factor 775 · d larger.

2. We now understand into what extent the sequence of stepsizes of [17] and [7] are optimal
in that it leads to minimal expected convergence rates Yt for all t: For each t we will show
a dimension independent lower bound on Yt over all possible stepsize sequences. This
includes the best possible stepsize sequence which minimizes Yt for a given t. Our lower
bound achieves the upper bound on Yt for the stepsize sequences of [17] and [7] within a
factor 32 for all t. This implies that these stepsize sequences universally minimizes each Yt
within factor 32.

2

3. As a consequence  in order to attain a better expected convergence rate  we need to either
assume more speciﬁc knowledge about the objective function F so that we can construct a
better stepsize sequence for SGD based on this additional knowledge or we need to step
away from SGD and use a different kind of algorithm. For example  the larger class of
algorithms in [1] may contain a non-SGD algorithm which may get close to the lower bound
proved in [1] which is a factor 775 · d smaller. Since the larger class of algorithms in [1]
contains algorithms such as Adam [10]  AdaGrad [6]  SGD-Momentum [23]  RMSProp
[24] we now know that these practical algorithms will at most improve a factor 32 · 775 · d
over SGD for strongly convex optimization – this can be signiﬁcant as this can lead to orders
of magnitude less gradient computations. We are the ﬁrst to make such quantiﬁcation.

Outline: Section 2 discusses background: First  we discuss the recurrence on Yt used in [17] for
proving their upper bound on Yt – this recurrence plays a central role in proving our lower bound.
We discuss the upper bounds of both [17] and [7] – the latter holding for a larger class of algorithms.
Second  we explain the lower bound of [1] in detail in order to be able to properly compare with our
lower bound. Section 3 introduces a framework for comparing bounds and explains the consequences
of our lower bound in detail. Section 4 describes a class of strongly convex and smooth objective
functions which is used to derive our lower bound. We also verify our theory by experiments in the
supplementary material. Section 5 concludes the paper.

2 Background

We explain the upper bound of [17  7]  and lower bound of [1] respectively.

2.1 Upper Bound for Strongly Convex and Smooth Objective Functions

The starting point for analysis is the recurrence ﬁrst introduced in [17  12]
E[(cid:107)wt+1 − w∗(cid:107)2] ≤ (1 − µηt)E[(cid:107)wt − w∗(cid:107)2] + η2

t N 

where

N = 2E[(cid:107)∇f (w∗; ξ)(cid:107)2]

(4)

and ηt is upper bounded by 1

2L; the recurrence has been shown to hold  see [17  12]  if we assume

1. F (.) is µ-strongly convex 
2. f (w; ξ) is L-smooth 
3. f (w; ξ) is convex  and
4. N is ﬁnite;

we detail these assumptions below:
Assumption 1 (µ-strongly convex). The objective function F : Rd → R is µ-strongly convex  i.e. 
there exists a constant µ > 0 such that ∀w  w(cid:48) ∈ Rd 

F (w) − F (w(cid:48)) ≥ (cid:104)∇F (w(cid:48))  (w − w(cid:48))(cid:105) +

(cid:107)w − w(cid:48)(cid:107)2.

µ
2

(5)

Assumption 2 (L-smooth). f (w; ξ) is L-smooth for every realization of ξ  i.e.  there exists a constant
L > 0 such that  ∀w  w(cid:48) ∈ Rd 

(cid:107)∇f (w; ξ) − ∇f (w(cid:48); ξ)(cid:107) ≤ L(cid:107)w − w(cid:48)(cid:107).

(6)

Assumption 2 implies that F is also L-smooth.
Assumption 3. f (w; ξ) is convex for every realization of ξ  i.e.  ∀w  w(cid:48) ∈ Rd 

f (w; ξ) − f (w(cid:48); ξ) ≥ (cid:104)∇f (w(cid:48); ξ)  (w − w(cid:48))(cid:105).

Assumption 4. N = 2E[(cid:107)∇f (w∗; ξ)(cid:107)2] is ﬁnite.

3

We denote the set of strongly convex objective functions by Fstr and denote the subset of Fstr
satisfying Assumptions 1  2  3  and 4 by Fsm.
We notice that the earlier established recurrence in [13] under the same set of assumptions

E[(cid:107)wt+1 − w∗(cid:107)2] ≤ (1 − 2µηt + 2L2η2

t )E[(cid:107)wt − w∗(cid:107)2] + η2
t N
L2 where (4) holds for ηt ≤ 1

2L2 the above recurrence provides a better bound than (4)  i.e.  1−2µηt+2L2η2

is similar  but worse than (4) as it only holds for ηt < µ
2L. Only for step
t ≤ 1−µηt.
sizes ηt < µ
In practical settings such as logistic regression µ = O(1/n)  L = O(1)  and t = O(n) (i.e. t is
at most a relatively small constant number of epochs  where a single epoch represents n iterations
resembling the complexity of a single GD computation). See (8) below  for this parameter setting
the optimally chosen step sizes are (cid:29) µ
L2 . This is the reason we focus in this paper on analyzing
recurrence (4) in order to prove our lower bound: For ηt ≤ 1
2L 
Yt+1 ≤ (1 − µηt)Yt + η2
t N 

where Yt = E[(cid:107)wt − w∗(cid:107)2].
Based on the above assumptions (without the so-called bounded gradient assumption) and knowledge
of only µ and L a sequence of step sizes ηt can be constructed such that Yt is smaller than O(1/t) [17];
more explicitly  for the sequence of step sizes

(7)

ηt =

2

µt + 4L

we have for all objective functions in Fsm the upper bound
16N
µ2t

µ(t − T (cid:48)) + 4L

Yt ≤ 16N
µ

=

1

(1 + O(1/t)) 

(8)

(9)

where

t ≥ T (cid:48) =

4L
µ

max{ LµY0
N

  1} − 4L
µ

.

We notice that [7] studies the larger class  which we denote Fesm  which is deﬁned as Fsm where
expected smoothness is assumed in stead of smoothness and convexity of component functions. We
rephrase their assumption for classical SGD as studied in this paper.2
Assumption 5. (L-smooth in expectation) The objective function F : Rd → R is L-smooth in
expectation if there exists a constant L > 0 such that  ∀w ∈ Rd 

E[(cid:107)∇f (w; ξ) − ∇f (w∗; ξ)(cid:107)2] ≤ 2L(cid:107)F (w) − F (w∗)(cid:107).

(10)

The results in [7] assume the above assumption for empirical risk minimization (2). L-smoothness 
see [15]  implies Lipschitz continuity (i.e.  ∀w  w(cid:48) ∈ Rd 

(cid:107)w − w(cid:48)(cid:107)2

f (w  ξ) ≤ f (w(cid:48)  ξ) + (cid:104)∇f (w(cid:48)  ξ)  (w − w(cid:48))(cid:105) +

L
2
) and together with Proposition A.1 in [7] this implies L-smooth in expectation. This shows that
Fesm deﬁned by Assumptions 1  4  and 5 is indeed a superset of Fsm.
The step sizes (8) from [17] for Fsm ⊆ Fesm and
4L
µ

for t ≤ 4L
µ
developed for Fesm in [7] and [17] are equivalent in that they are both ≈ 2
µt for t large enough. Both
step size sequences give exactly the same asymptotic upper bound (9) on Yt (in our notation).
In [21]  the authors proved the convergence of SGD for the step size sequence {ηt} satisfying
t < ∞. In [13]  the authors studied the expected convergence
rates for another class of step sizes of O(1/tp) where 0 < p ≤ 1. However  the authors of both [21]
and [13] do not discuss about the optimal step sizes among all proposed step sizes which is what is
done in this paper.

t=0 ηt = ∞ and (cid:80)∞

conditions(cid:80)∞

and ηt =

(t + 1)2µ

for t >

t=0 η2

1
2L

2t + 1

ηt =

(11)

2This means that distribution D in [7] must be over unit vectors v ∈ [0 ∞)n  where n is the number
of component functions  i.e.  n possible values for ξ. Arbitrary distributions D correspond to SGD with
mini-batches where each component function indexed by ξ is weighted with vξ.

4

2.2 Lower Bound for First Order Stochastic Oracles

The authors of [14] proposed the ﬁrst formal study on lower bounding the expected convergence
rate for a large class of algorithms which includes SGD. The authors of [1] and [20] independently
studied this lower bound using information theory and were able to improve it.
The derivation in [1] is for algorithms including SGD where the sequence of stepsizes is a-priori
ﬁxed based on global information regarding assumed stochastic parameters concerning the objective
function F . Their proof uses the following set of assumptions: First  The assumption of a strongly
convex objective function  i.e.  Assumption 1 (see Deﬁnition 3 in [1]). Second  the objective function
is convex Lipschitz:
Assumption 6. (convex Lipschitz) The objective function F is a convex Lipschitz function  i.e.  there
exists a bounded convex set S ⊂ Rd and a positive number K such that ∀w  w(cid:48) ∈ S ⊂ Rd

(cid:107)F (w) − F (w(cid:48))(cid:107) ≤ K(cid:107)w − w(cid:48)(cid:107).

E[(cid:107)∇f (w; ξ)(cid:107)2] ≤ σ2

We notice that this assumption implies the assumption on bounded gradients as stated here (and
explicitly mentioned in Deﬁnition 1 in [1]): There exists a bounded convex set S ⊂ Rd and a positive
number σ such that

(12)
for all w ∈ S ⊂ Rd. This is not the same as the bounded gradient assumption where S = Rd is
unbounded.3 Clearly  for w∗  (12) implies a ﬁnite N ≤ 2σ2.
We deﬁne Flip as the set of strongly convex objective functions that satisfy Assumption 6. Classes
Fesm and Flip are both subsets of Fstr and differ (are not subclasses of each other) in that they
assume expected smoothness and convex Lipschitz respectively.
To prove a lower bound of Yt for Flip  the authors constructed a class of objective functions ⊆ Flip
and showed a lower bound of Yt for this class; in terms of the notation used in this paper 

√
log(2/
e)
432 · d

N
µ2t

.

(13)

The authors of [1] prove lower bound (13) for the class Astoch of stochastic ﬁrst order algorithms
that can be understood as operating based on information provided by a stochastic ﬁrst-order oracle 
i.e.  any algorithm which bases its computation in the t-th iteration on µ  K or L  d  and access to
an oracle that provides f (wt; ξt) and ∇f (wt; ξt). This class includes ASGD deﬁned as SGD with
some sequence of diminishing step sizes as a function of global parameters such as µ and L or µ and
K  see Algorithm 1. We notice that Astoch also includes practical algorithms such as Adam [10] 
etc. We revisit their derivation in the supplementary material where we show4 how their lower bound
transforms into (13). Notice that their lower bound depends on dimension d.

3 Framework for Upper and Lower Bounds

Let par(F ) denote the concrete values of the global parameters of an objective function F such
as the values for µ and L corresponding to objective functions F in Fsm and Fesm or µ and K
corresponding to objective functions F in Flip. When deﬁning a class F of objective functions 
we also need to explain how F deﬁnes a corresponding par(.) function. We will use the notation
F[p] to stand for the subclass {F ∈ F : p = par(F )} ⊆ F  i.e.  the subclass of objective
functions of F with the same parameters p. We assume that parameters of a class are included in
the parameters of a smaller subclass: For example  Fsm is a subset of the class of strongly convex
objective functions Fstr with only global parameter µ. This means that for concrete values µ and L
we have Fsm[µ  L] ⊆ Fstr[µ].
For a given objective function F   we are interested in the best possible expected convergence rate
after the t-th iteration among all possible algorithms A in a larger class of algorithms A. Here  we
3The bounded gradient assumption  where S is unbounded  is in conﬂict with assuming strong convexity as

explained in [17].

4We also discuss the underlying assumption of convex Lipschitz and show that in order for the analysis in [1]

to follow through one – likely tedious but believable – statement still needs a formal proof.

5

assume that A is a subclass of the larger class Astoch U of stochastic ﬁrst order algorithms where
the computation in the t-th iteration not only has access to par(F ) and access to an oracle that
provides f (wt; ξt) and ∇f (wt; ξt) but also access to possibly another oracle U providing even more
information. Notice that A ⊆ Astoch ⊆ Astoch U for any oracle U. With respect to the expected
convergence rate  we want to know which algorithm A in A minimizes Yt the most. Notice that for
different t this may be a different algorithm A. We deﬁne for F ∈ F (with associated par(.))

t (A) = inf
γF

A∈A Yt(F  A) 

where Yt is explicitly shown as a function of the objective function F and choice of algorithm A.
Among the objective functions F ∈ F with same global parameters p = par(F ) (i.e.  F ∈ F[p])  we
consider the objective function F which has the worst expected convergence rate at the t-th iteration.
This is of interest to us because algorithms A only have access to p = par(F ) as the sole information
about objective function F   hence  if we prove an upper bound on the expected convergence rate for
algorithm A  then this upper bound must hold for all F ∈ F with the same parameters p = par(F ).
In other words such an upper bound must be at least

γt(F[p] A) = sup
F∈F [p]

t (A) = sup
γF
F∈F [p]

inf
A∈A Yt(F  A).

So  any lower bound on γt(F[p] A) gives us a lower bound on the best possible upper bound on Yt
that can be achieved. Such a lower bound tells us into what extent the expected convergence rate Yt
cannot be improved.
The lower bound (13) and upper bound (9) are not only a function of µ in p = par(F ) but also a
function of N which is outside p = par(F ) for F ∈ Flip or F ∈ Fesm. We are really interested
in such more ﬁne-grained bounds that are a function of N. For this reason we need to consider the
subclass of objective functions F in F[p] that all have the same N. We implicitly understand that N
is an auxiliary parameter of an objective function F and we denote this as a function of F as N (F ).
We deﬁne F a[p] = {F ∈ F[p]
: a = aux(F )} where aux(.) represents for example N (.). This
leads to notation like F N
lip[µ  K  d]. Notice that p = par(F ) can be used by an algorithm A ∈ A
while a = aux(F ) is not available to A through p = par(F ) (but may be available through access to
an oracle).
If we ﬁnd a tight lower bound with upper bound up to a constant factor  as in this paper  then we know
that the algorithm that achieves the upper bound is close to optimal in that the expected convergence
rate cannot be further minimized/improved in a signiﬁcant way. In practice we are only interested
in upper bounds on Yt that can be realized by the same algorithm A (if not  then we need to know
a-priori the exact number of iterations t we want to run an algorithm and then choose the best one
for that t). In this paper we consider the algorithm A for F in Fsm resp. Fesm deﬁned as SGD with
diminishing step sizes (8) resp. (11) as a function of par(F ) = (µ  L) giving upper bound (9) on
expected convergence rate Yt(F  A). We show that A is close to optimal.
Given the above deﬁnitions we have

γt(F[p] A) ≤ γt(F(cid:48)[p(cid:48)] A(cid:48))

(14)
for F[p] ⊆ F(cid:48)[p(cid:48)] and A(cid:48) ⊆ A  i.e.  the worst objective function in a larger class of objective
functions is worse than the worst objective function in a smaller class of objective functions (see
the supremum used in deﬁning γt) and the best algorithm from a larger class of algorithms is better
than the best algorithm from a smaller class of algorithms (see the inﬁnum used in deﬁning γt). This
implies

γt(F N
γt(F N

(15)
(16)

lip[µ  K  d] Astoch) ≤ γt(F N
sm[µ  L] AExtSGD) ≤ γt(F N
where ASGD ⊆ AExtSGD is deﬁned as follows:
In our framework we introduce extended SGD as the class AExtSGD of SGD algorithms where the
stepsize in the t-th iteration can be computed based on global parameters µ  L  and access to an
oracle U that provides additional information N  ∇F (wt)  and Yt. This class also includes SGD with

str[µ] ASGD) 
esm[µ  L] ASGD) ≤ γt(F N

str[µ] ASGD) 

6

1
2

N
µ2t

diminishing stepsizes as deﬁned in Algorithm 1  i.e.  ASGD ⊆ AExtSGD. The reason for introducing
the larger class AExtSGD is not because it contains practical algorithms different than SGD  on the
contrary. The only reason is that it allows us to deﬁne one single algorithm A ∈ AExtSGD which
t (AExtSGD) for all t for all F in a to be constructed subclass F ⊆ Fsm – the topic of the
realizes γF
next section. This property allows a rather straightforward calculus based proof without needing to
use more advanced concepts from information and probability theory as required in the proof of [1].
Looking ahead  we will prove in Theorem 1

(1 − O((ln t)/t)) ≤ γt(F N

sm[µ  L] AExtSGD).

(17)
Notice that the construction of ηt for algorithms in AExtSGD does not depend on knowledge of the
stochastic gradient ∇f (wt; ξt). So  we do not consider step sizes that are adaptively computed based
on ∇f (wt; ξt).
As a disclaimer we notice that for some objective functions F ∈ F N
sm[µ  L] the expected convergence
rate can be much better than what is stated in (17); this is because γt({F} AExtSGD) can be much
sm[µ  L] AExtSGD)  see (14). This is due to the speciﬁc nature of the objective
smaller than γt(F N
function F itself. However  without knowledge about this nature  one can only prove a general upper
bound on the expected convergence rate Yt and any such upper bound must be at least the lower
bound (17).
Results (13) and (9) of the previous section combined with (15)  (16)  and (17) yield

√
e)
log(2/
432 · d

N
µ2t

≤ γt(F N
(1 − O((ln t)/t)) ≤ γt(F N
(1 − O((ln t)/t)) ≤ γt(F N

N
1
µ2t
2
N
1
µ2t
2

lip[µ  K  d] Astoch) ≤ γt(F N
esm[µ  L] AExtSGD) ≤ γt(F N
sm[µ  L] AExtSGD) ≤ γt(F N
≤ 16N
µ2t

str[µ] ASGD) 
str[µ] ASGD) 
esm[µ  L] ASGD)
(1 + O(1/t)).

(18)

(19)

(20)

We conclude the following observations (our contributions):

1. The ﬁrst inequality (18) is from [1]. Comparing (19) to (18) shows that as a lower bound
str[µ] ASGD) (SGD for the class of strongly convex objective functions) our lower
for γt(F N
bound (17) is dimension independent and improves the lower bound (13) of [1] by a factor
775 · d. This is a signiﬁcant improvement.
2. However  our lower bound does not hold for the larger class Astoch. This teaches us that if
we wish to reach smaller (better) expected convergence rates  then one approach is to step
beyond SGD where our lower bound does not hold implying that within Astoch there may be
an opportunity to ﬁnd an algorithm leading to at most a factor 32 · 775 · d smaller expected
convergence rate compared to upper bound (20). This is the ﬁrst exact quantiﬁcation into
what extent a better (practical) algorithm when compared to classical SGD can be found.
E.g.  Adam [10]  AdaGrad [6]  SGD-Momentum [23]  RMSProp [24] are all in Astoch and
can beat classical SGD by at most a factor 32 · 775 · d.
3. When searching for a better algorithm in Astoch which signiﬁcantly improves over SGD 
it does not help to take an SGD-like algorithm which uses step sizes that are a function of
iteratively computed estimates of ∇F (wt) and Yt as this would keep such an algorithm in
AExtSGD for which our lower bound is tight.

4. Another approach to reach smaller expected convergence rates is to stick with SGD but
consider a smaller restricted class of objective functions for which more/other information
in the form of extra global parameters is available for adaptively computing ηt.
5. For strongly convex and smooth  respectively expected smooth  objective functions the
algorithm A ∈ ASGD with stepsizes ηt = 2
(t+1)2µ for t > 4L
and ηt = 1
µ   realizes the upper bound in (20) for all t. Inequalities (20) show
that this algorithm is close to optimal: For each t  the best sequence of diminishing step
sizes which minimizes Yt can at most achieve a constant (dimension independent) factor 32
smaller expected convergence rate.

µt+4L  respectively ηt = 2t+1

2L for t ≤ 4L

µ

7

4 Lower Bound for Extended SGD

In order to prove a lower bound we propose a speciﬁc subclass of strongly convex and smooth
objective functions F and we show in the extended SGD setting how  based on recurrence (7)  to
compute the optimal step size ηt as a function of µ and L and an oracle U with access to N  ∇F (wt) 
and Yt  i.e.  this step size achieves the smallest Yt+1 at the t-th iteration.
We consider the following class of objective functions F : We consider a multivariate normal
distribution of a d-dimensional random vector ξ  i.e.  ξ ∼ N (m  Σ)  where m = E[ξ] and Σ =
E[(ξ−m)(ξ−m)T] is the (symmetric positive semi-deﬁnite) covariance matrix. The density function
of ξ is chosen as

exp(

g(ξ) =

−(ξ−m)TΣ−1(ξ−m)

)

.

(cid:112)(2π)d|Σ|

2

We select component functions f (w; ξ) = s(ξ)
according to the following random process:

(cid:107)w−ξ(cid:107)2

2

  where function s(ξ) is constructed a-priori

• With probability 1−µ/L  we draw s(ξ) from the uniform distribution over interval [0  µ/(1−
• With probability µ/L  we draw s(ξ) from the uniform distribution over interval [0  L].

µ/L)].

The following theorem analyses the sequence of optimal step sizes for our class of objective functions
and gives a lower bound on the corresponding expected convergence rates. The theorem states that we
cannot ﬁnd a better sequence of step sizes. In other words without any more additional information
about the objective function (beyond µ  L  N  Y0  . . .   Yt for computing ηt)  we can at best prove a
general upper bound which is at least the lower bound as stated in the theorem. The proof of the
lower bound is presented in the supplementary material:
Theorem 1. We assume that component functions f (w; ξ) are constructed according to the recipe
described above with µ < L/18. Then  the corresponding objective function is µ-strongly convex
and the component functions are L-smooth and convex.
If we run Algorithm 1 and assume that access to an oracle U with access to N  ∇F (wt)  and Yt
is given at the t-th iteration (our extended SGD problem setting)  then an exact expression for the
optimal sequence of stepsizes ηt based on µ  L  N  Y0  . . .   Yt can be given  i.e.  this sequence of
stepsizes achieves the smallest possible Yt+1 at the t-th iteration for all t. For this sequence of
stepsizes 

Yt ≥ N
2µ

1

µt + 2µ ln(t + 1) + W

 

(21)

where

W =

L2

12(L − µ)

.

In the supplementary material we show numerical experiments in agreement with the presented
theorem.

5 Conclusion

We have studied the convergence of SGD by introducing a framework for comparing upper bounds
and lower bounds and by proving a new lower bound based on straightforward calculus. The new
lower bound is dimension independent and improves a factor 775· d over previous work [1] applied to
SGD  shows the optimality of step sizes in [17  7]  and shows that practical algorithms like Adam [10] 
AdaGrad [6]  SGD-Momentum [23]  RMSProp [24] for strongly convex objective functions can at
most achieve a factor 32 · 775 · d smaller expected convergence rate compared to classical SGD.

Acknowledgement

We thank the reviewers for useful suggestions to improve the paper. Phuong Ha Nguyen and Marten
van Dijk were supported in part by AFOSR MURI under award number FA9550-14-1-0351.

8

References
[1] Alekh Agarwal  Peter L Bartlett  Pradeep Ravikumar  and Martin J Wainwright. Information-

theoretic lower bounds on the oracle complexity of stochastic convex optimization. 2010.

[2] D.P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  1999.

[3] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. arXiv:1606.04838  2016.

[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press 

2004.

[5] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In NIPS  pages 1646–1654 
2014.

[6] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research  12(Jul):2121–2159  2011.

[7] Robert Mansel Gower  Nicolas Loizou  Xun Qian  Alibek Sailanbayev  Egor Shulgin  and Peter
Richtarik. Sgd: General analysis and improved rates. arXiv preprint arXiv:1901.09401  2019.

[8] Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The Elements of Statistical Learning:

Data Mining  Inference  and Prediction. Springer Series in Statistics  2nd edition  2009.

[9] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS  pages 315–323  2013.

[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[11] Nicolas Le Roux  Mark Schmidt  and Francis Bach. A stochastic gradient method with an

exponential convergence rate for ﬁnite training sets. In NIPS  pages 2663–2671  2012.

[12] Rémi Leblond  Fabian Pederegosa  and Simon Lacoste-Julien. Improved asynchronous parallel
optimization analysis for stochastic incremental methods. arXiv preprint arXiv:1801.03749 
2018.

[13] Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation
algorithms for machine learning. In Advances in Neural Information Processing Systems  pages
451–459  2011.

[14] Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method

efﬁciency in optimization. 1983.

[15] Yurii Nesterov.

Introductory lectures on convex optimization : a basic course. Applied

optimization. Kluwer Academic Publ.  Boston  Dordrecht  London  2004.

[16] Lam Nguyen  Phuong Ha Nguyen  Peter Richtarik  Katya Scheinberg  Martin Takac  and
Marten van Dijk. New convergence aspects of stochastic gradient algorithms. arXiv preprint
arXiv:1811.12403  2018.

[17] Lam Nguyen  Phuong Ha Nguyen  Marten van Dijk  Peter Richtarik  Katya Scheinberg  and
Martin Takac. SGD and hogwild! Convergence without the bounded gradients assumption. In
ICML  2018.

[18] Lam M. Nguyen  Jie Liu  Katya Scheinberg  and Martin Takáˇc. SARAH: A novel method for

machine learning problems using stochastic recursive gradient. In ICML  2017.

[19] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer  New York  2nd

edition  2006.

[20] Maxim Raginsky and Alexander Rakhlin.

Information-Based Complexity  Feedback and
Dynamics in Convex Programming. IEEE Trans. Information Theory  57(10):7036–7056  2011.

9

[21] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of

Mathematical Statistics  22(3):400–407  1951.

[22] Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a

strong growth condition. arXiv preprint arXiv:1308.6370  2013.

[23] Ilya Sutskever  James Martens  George Dahl  and Geoffrey Hinton. On the importance of
initialization and momentum in deep learning. In International conference on machine learning 
pages 1139–1147  2013.

[24] Fangyu Zou  Li Shen  Zequn Jie  Weizhong Zhang  and Wei Liu. A Sufﬁcient Condition for

Convergences of Adam and RMSProp. arXiv preprint arXiv:1811.09358  2018.

10

,Arthur Choi
Yujia Shen
Adnan Darwiche
Hao Wu
Luca Pasquali
Frank Noe
PHUONG_HA NGUYEN
Lam Nguyen
Marten van Dijk