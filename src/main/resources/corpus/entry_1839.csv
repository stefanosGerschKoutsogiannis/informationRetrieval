2019,Equal Opportunity in Online Classification with Partial Feedback,We study an online classification problem with partial feedback in which individuals arrive one at a time from a fixed but unknown distribution  and must be classified as positive or negative. Our algorithm only observes the true label of an individual if they are given a positive classification. This setting captures many classification problems for which fairness is a concern: for example  in criminal recidivism prediction  recidivism is only observed if the inmate is released; in lending applications  loan repayment is only observed if the loan is granted. We require that our algorithms satisfy common statistical fairness constraints (such as equalizing false positive or negative rates --- introduced as "equal opportunity" in Hardt et al. (2016)) at every round  with respect to the underlying distribution. We give upper and lower bounds characterizing the cost of this constraint in terms of the regret rate (and show that it is mild)  and give an oracle efficient algorithm that achieves the upper bound.,Equal Opportunity in Online Classiﬁcation with

Partial Feedback

Yahav Bechavod
Hebrew University

yahav.bechavod@cs.huji.ac.il

Katrina Ligett

Hebrew University

katrina@cs.huji.ac.il

Aaron Roth

University of Pennsylvania
aaroth@cis.upenn.edu

Bo Waggoner

University of Colorado
bwag@colorado.edu

Zhiwei Steven Wu

University of Minnesota

zsw@umn.edu

Abstract

We study an online classiﬁcation problem with partial feedback in which indi-
viduals arrive one at a time from a ﬁxed but unknown distribution  and must be
classiﬁed as positive or negative. Our algorithm only observes the true label of an
individual if they are given a positive classiﬁcation. This setting captures many
classiﬁcation problems for which fairness is a concern: for example  in criminal
recidivism prediction  recidivism is only observed if the inmate is released; in
lending applications  loan repayment is only observed if the loan is granted. We
require that our algorithms satisfy common statistical fairness constraints (such as
equalizing false positive or negative rates — introduced as “equal opportunity” in
[18]) at every round  with respect to the underlying distribution. We give upper
and lower bounds characterizing the cost of this constraint in terms of the regret
rate (and show that it is mild)  and give an oracle efﬁcient algorithm that achieves
the upper bound.1

1

Introduction

Many real-world prediction tasks in which fairness concerns arise — such as online advertising 
short-term hiring  lending micro-loans  and predictive policing — are naturally modeled as online
binary classiﬁcation problems  but with an important twist: feedback is only received for one of the
two classiﬁcation outcomes. Clickthrough is only observed if the advertisement is shown; worker
performance is only observed for candidates who were actually hired; those who are denied a loan
never have an opportunity to demonstrate that they would have repaid; only if police troops were
dispatched to a precinct they able to detect unreported crimes. Applying standard techniques for
enforcing statistical fairness constraints on the gathered data can thus lead to pernicious feedback
loops that can lead to classiﬁers that badly violate these constraints on the underlying distribution.
This kind of failure to “explore” has been highlighted as an important source of algorithmic unfairness
— for example  in predictive policing settings [26  13  14].
To avoid this problem  it is important to explicitly manage the exploration/exploitation tradeoff that
characterizes learning in partial feedback settings  which is what we study in this paper. We ask for
algorithms that enforce well-studied statistical fairness constraints across two protected populations
(we focus on the “equal opportunity” constraint of [18]  which enforces equalized false positive
rates or false negative rates  but our techniques also apply to other statistical fairness constraints
like “statistical parity” [12]). In particular  we ask for algorithms that satisfy these constraints (with

1The full version of this paper is available at https://arxiv.org/abs/1902.02242.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

respect to the unknown underlying distribution) at every round of the learning procedure. The result
is that the fairness constraints restrict how our algorithms can explore  not just how they can exploit 
which makes the problem of fairness-constrained online learning substantially different from in the
batch setting. The main question that we explore in this paper is: “how much does the constraint of
fairness impact the regret bound of learning algorithms?”

1.1 Our Model and Results
In our setting  there is an unknown distribution D over examples  which are triples (ˆx  a  y) 2
X ⇥ {1  1} ⇥ {1  1}. Here ˆx 2X represents a vector of features in some arbitrary feature space 
a 2 A = {±1} is the group to which this example belongs (which we also call the sensitive feature) 
and y 2Y = {±1} is a binary label. We write x to denote a pair (ˆx  a) – the set of all features
(including the sensitive one) that the learner has access to.
In each round t 2 [T ]  our learner selects hypotheses from a hypothesis class H consisting of
functions h : X⇥ A !Y recommending an action (or label) as a function of the features (potentially
including the sensitive feature). We take the positive label to be the one that corresponds to observing
feedback (hiring a worker  admitting a student  approving a loan  releasing an inmate  etc.) We allow
algorithms that randomize over H. Let (H) be the set of probability distributions over H. We refer
to a ⇡ 2 (H) as a convex combination of classiﬁers.
Deﬁnition 1.1 (False positive rate). For a ﬁxed distribution D on examples  we deﬁne the false
positive rate (FPR) of a convex combination of classiﬁers ⇡ 2 (H) on group j 2 {±1} to be
(h(x) = +1|a = j  y = 1) .

F P Rj(⇡) = P(⇡(x) = +1|a = j  y = 1) = E

h⇠⇡ P

(x y)⇠D

We denote the difference between false positive rates between populations as

F P R(⇡) := F P R1(⇡)  F P R1(⇡).

The fairness constraint we impose on our classiﬁers in this paper asks that false positive rates be
approximately equalized across populations at every round t. Throughout  analogous results hold for
false negative rates. These constraints were called equal opportunity constraints in [18].
Deﬁnition 1.2 (-equalized rates [18]). Fix a distribution D. A convex combination ⇡ 2 (H)
satisﬁes the -equalized false positive rate (-EFP) constraint if |F P R(⇡)| . We informally use
the term -fair to refer to such a classiﬁer or combination of classiﬁers.

As we will see in Deﬁnition 2.1  we will actually allow our algorithm to have a tiny probability of
ever breaking the fairness constraint.
Remark 1.3. The sources of unfairness we deal with here are the differential abilities of models
in H to predict on different populations (which we inherit from the batch setting)  and the biased
data collection inherent in online partial information settings. We use equal opportunity constraints
only as a canonical example of a statistical fairness constraint and do not take the position that it is
always the right one. Our techniques also apply to other constraints like statistical parity.
Note that the fairness constraint is deﬁned with respect to the true underlying distribution D. One of
the primary difﬁculties we face is that in early rounds  the learner has very little information about D 
and yet is required to satisfy the fairness constraint with respect to D.
It is straightforward to see (and a consequence of a more general lower bound that we prove) that a
-fair algorithm cannot in general achieve non-trivial regret to the set of -fair convex combinations
of classiﬁers  because of ongoing statistical uncertainty about the fairness level for all non-trivial
classiﬁers. Thus our goal is to minimize our regret to the -fair convex combination of classiﬁers that
has the lowest classiﬁcation error on D  while guaranteeing that our algorithm only deploys convex
combinations of classiﬁers that guarantee fairness level 0 for some 0 > . Clearly  the optimal
regret bound will be a function of the gap (0  )  and one of our aims is to characterize this tradeoff.
Our results. We show that the tradeoff achieved by the inefﬁcient algorithm is tight by proving
a lower bound in Section 4. In some sense  the computational inefﬁciency of the simple bandits
reduction above is unavoidable  because we measure the regret of our learner with respect to 0/1

2

classiﬁcation error  which is computationally hard to minimize  even for very simple classes H
(see  e.g.  [22  16  11]). However  we can still hope to give an oracle efﬁcient algorithm for our
problem. This approach  which is common in the contextual bandits literature  assumes access to
an “oracle” which can in polynomial time solve the empirical risk minimization problem over H
(absent fairness constraints)  and is an attractive way to isolate the “hard part” of the problem that is
often tractable in practice. Our main result  to which we devote the body of the paper  is to show that
access to such an oracle is sufﬁcient to give a polynomial-time algorithm for the fairness-constrained
learning problem  matching the simple information theoretically optimal bounds described above.
To do this  we use two tools. Our high-level strategy is to apply the oracle efﬁcient stochastic
contextual bandit algorithm from [2]. In order to do this  we need to supply it with an ofﬂine learning
oracle for the set of classiﬁers that can with high probability be certiﬁed to satisfy our fairness
constraints given the data so far. We construct an approximate oracle for this problem (given a
learning oracle for H) using the oracle-efﬁcient reduction for ofﬂine fair classiﬁcation from [1]. We
need to overcome a number of technical difﬁculties stemming from the fact that the fair oracle that we
can construct is only an approximate empirical risk minimizer  whereas the oracle assumed in [2] is
exact. Moreover  the algorithm from [2] assumes a ﬁnite hypothesis class  whereas we need to obtain
no regret to a continuous family of distributions over hypotheses. The ﬁnal result is an oracle-efﬁcient
algorithm trading off between regret and fairness  allowing for a regret bound of O(T 2↵) to the best
-fair classiﬁer while satisfying 0-fairness at every round  with a gap of 0   = O(T ↵) for
↵ 2 [0.25  0.5].
1.2 Additional Related Work
We build on two lines of work in the fair machine learning literature. First  batch (non-online)
classiﬁcation under a variety of statistical fairness constraints: raw classiﬁcation rates [8  23  15]
(statistical parity [12])  positive predictive value [24  9]  and false positive and false negative rates [24 
9  18] ; see [5] for more examples. Second  fair online classiﬁcation and regression in the contextual
bandit setting [20  21  25]. Unlike some of this prior work that demands stringent individual fairness
constraints at every round  which requires strong realizability assumptions to avoid lower bounds
[20]  this paper interpolates by requiring statistical fairness constraints to be enforced  but they still
must hold for every round. This allows much stronger positive results. Recently  [7] considered the
problem of enforcing statistical fairness in online learning  but from a very different perspective. That
work studies the full information and adversarial setting with false positive and error rates averaged
across rounds. Here  we have partial and bandit feedback  distributional assumptions  and require
statistical fairness guarantees at every round.

2 Additional Preliminaries
Throughout the paper  we assume +1 1 2H   where +1 and 1 are the two constant classiﬁers
(that is  +1(x) = 1 and 1(x) = 1 for all x). In some cases  we will additionally assume
+a a 2H   where +a and a are the identity function (and its negation) on the sensitive feature
(that is  +a(ˆx  a) = a and a(ˆx  a) = a for all ˆx  a).
The Online Setting: The learner interacts with the environment as follows. For each round
t = 1  . . .   T   the learner chooses some convex combination ⇡t 2 (H). The environment draws
(xt  yt) ⇠D independently; the learner observes xt. The learner labels the point ˆyt = ht(xt)  where
ht ⇠ ⇡t. If ˆyt = +1  the learner observes yt; otherwise  there is no feedback for this round.
We measure a learner’s performance using 0-1 loss  `(ˆyt  yt) = [ˆyt 6= yt]. Given a class of
distributions P over H ✓H and a sequence of T examples  the optimal convex combination of
hypotheses from H in hindsight is deﬁned as ⇡⇤(P) = argmin⇡2PPT
A learner’s (pseudo)-regret with respect to P is

t=1 Eh⇠P [`(h(xt)  yt)].

TXt=1

3

Regret =

TXt=1

E

(xt yt)⇠D

[`(h(xt)  yt)] 

E

(xt yt)⇠D h⇠⇡⇤(P)

[`(h(xt)  yt)].

In particular  when P = {⇡ 2 (H) : ⇡ satisﬁes -EFP}  we call this the learner’s -EFP regret.
Finally  we ask for online learning algorithms that satisfy the following notion of fairness:

Deﬁnition 2.1 (A -EFP() online learning algorithm). An online learning algorithm is said to satisfy
-EFP() fairness (for  2 [0  1]) if  with probability 1   over the draw of {(xt  at  yt)}T
t=1 ⇠D T  
simultaneously for all rounds t 2 [T ]: ⇡t satisﬁes -EFP.
Cost Sensitive Classiﬁcation Algorithms: We aim to give oracle-efﬁcient online learning algo-
rithms — that is  algorithms that run in polynomial time per round  assuming access to an oracle
which can solve the corresponding ofﬂine empirical risk minimization problem. Concretely  we
assume oracles for solving cost sensitive classiﬁcation (CSC) problems over H  which are deﬁned by
a set of examples xj and a set of weights c1
  c+1
j 2 R corresponding to the cost of a negative and
positive classiﬁcation respectively.
Deﬁnition 2.2. Given an instance of a CSC problem S = {xj  c1
j=1  a CSC oracle O for H
returns O(S) 2 arg minh2H Pn
. From these oracles  we will construct ⌫-approximate
CSC oracles that may have restricted ranges ⇧ ✓ (H). Such oracles return O⌫(S) = ⇡ 2 ⇧ such
that Eh⇠⇡[Pn

]  arg min⇡2⇧ Eh⇠⇡[Pn

From “Apple Tasting” to Contextual Bandits: Online classiﬁcation problems under the feedback
model we study were ﬁrst described as “Apple Tasting” problems [19]. The algorithm’s loss at each
round accumulates according to the following loss matrix:

j=1 c(h(xj ))

j

j=1 c(h(xj ))

j

  c+1
j }n

j

j

j=1 c(h(xj ))

j

] + ⌫.

L =

y = +1

0
1

ˆy = +1
ˆy = 1

⇣

y = 1

1
0

 

⌘

but feedback is only observed for positive classiﬁcations (when ˆy = +1). This is a different feedback
model than the more commonly studied contextual bandits setting. In that setting  the learner always
get to observe the loss of the selected action (regardless of a positive or a negative classiﬁcation). We
will defer the formal description of contextual bandits to the appendix.
It is nevertheless straightforward to transform the apple tasting setting into the contextual bandits
setting (similar observations have been previously made [4]).
Proposition 2.3. Let A be an contextual bandits algorithm that guarantees a regret bound R(T ) with
probability 1 . There exists a transformation that maps feedbacks for apple tasting to feedbacks for
contextual bandits such that A gurantees regret bound 2R(T ) with probability 1   when running
on the transformed feedbacks on any apple tasting instance.

Baseline approaches. Given the reduction above  we can draw on standard methods from contex-
tual bandits to solve our fair online learning problem. A simple baseline approach that is oracle-
efﬁcient is to perform “exploration-then-exploitation”: the learner ﬁrst “explores” by predicting
+1 for roughly T 2/3 rounds  then “exploit” what we have learned by deploying the (empirically)
best performing fair policy. This approach would guarantee a sub-optimal regret bound of ˜O(T 2
3 )
to the best -fair classiﬁer  while satisfying a 0-fairness constraint at every round with a gap of
(0  ) = O(T  1
3 ).
A more sophisticated approach starts with the observation (Lemma B.1) that although the set of “fair
distributions over classiﬁers” is continuously large  the “fair empirical risk minimization” problem
only has a single constraint  and so we may without loss of generality consider distributions over
hypotheses H that have support of size 2. By an appropriate discretization  this allows us to restrict
attention to a ﬁnite net of classiﬁers whenever H itself is ﬁnite. From this observation  one could
employ a simple strategy to obtain an information theoretic result: Fix any parameter ↵ 2 [1/4  1/2].
The learner ﬁrst predicts +1 for roughly T 2↵ rounds  then uses the collected data to deﬁne a set of fair
policies according to the observed empirical distribution  and lastly runs the EXP4 algorithm [3  6]
over the set of fair policies. Such algorithm obtains a regret bound of O(T 2↵) to the best -fair
classiﬁer  while satisfying a 0-fairness constraint at every round with a gap of (0  ) = O(T ↵).
However  this algorithm needs to maintain a distribution of exponential size  and our goal is to match
its regret rate with an oracle-efﬁcient algorithm.

4

3 An Oracle-Efﬁcient Algorithm

Our algorithm proceeds in two phases. First  during the ﬁrst T0 rounds  the algorithm performs pure
exploration and always predicts +1 to collect labelled data. Because constant classiﬁers exactly
equalize the false positive rates across populations  each exploration round satisﬁes our fairness
constraint. The algorithm then use the collected data to form empirical fairness constraints  which we
use to deﬁne our construction of a fair CSC oracle  given a CSC oracle unconstrained by fairness.
Then  in the remaining rounds  we will run an adaptive contextual bandit algorithm that minimizes
cumulative regret  while satisfying the empirical fairness constraint at every round.
We make two mild assumptions to simplify our analysis and the statement of our ﬁnal bounds. First 
we assume that negative examples from each of the two protected groups have constant probability
mass: Pr[a = 1  y = 1]  Pr[a = 1  y = 1] 2 ⌦(1). Second  we assume that the hypothesis
class H contains the two constant classiﬁers and the identity function and its negation on the protected
attribute: {+1 1  +a a}✓H .
Our main theorem is as follows:
Theorem 3.1. For any H and data distribution satisfying the two mild assumptions above  there
exists an oracle-efﬁcient algorithm that takes parameters  2 [0  1pT
] and   0 as input and satisﬁes
( + )-EFP() fairness and has an expected regret at most ˜O(pT ln(|H|/)) with respect to the
class of -EFP fair policies  where  = O(pln(|H|/)/T 1/4).
Remark 3.2. More generally  we can extend Theorem 3.1 to give an algorithm that satisﬁes ( + )-
2 + pT ln(|H|/)◆ with
EFP() for any > 0  and achieves an expected regret at most ˜O✓ ln( |H| )
respect to the class of -EFP fair policies.
Remark 3.3. We state our theorem in what we believe is the most attractive parametric regime:
when it can obtain a regret bound of O(pT ). But it is straightforward  by modifying the length of the
exploration round  to obtain a more general tradeoff—a regret bound of O(T 2↵) with respect to the
set of -EFP fair policies  while satisfying ( + O(T ↵))-EFP() fairness  for any ↵ 2 [1/4  1/2].
This tradeoff is tight  as we show in Section 4.

Algorithm. The outline of our algorithm is as follows.
1. Label the ﬁrst T0 arrivals as ˆyt = 1; observe their true labels.
2. Based on this data  construct an efﬁcient FairCSC oracle. The oracle will be given a cost-sensitive
classiﬁcation objective. It returns an approximately-optimal convex combination ⇡ of hypotheses
subject to the linear constraint of ( + T 1/4)-EFP on the empirical distribution of data. We show the
algorithm can be implemented to always return a member of ⇧  deﬁned to be the set of mixtures on
H with support size two whose empirical fairness on the exploration data is at most  + ˜O(T 1/4).
3. Instantiate a bandit algorithm with policy class ⇧. The bandit algorithm  a modiﬁcation of [2]  is
described in detail in the next sections. In order to select its hypotheses  the bandit algorithm makes
calls to the FairCSC oracle we implemented above.
4. For the remaining rounds t > T0  choose labels ˆyt selected by the bandit algorithm and provide
feedback to the bandit algorithm via the reduction given by Proposition 2.3.

Analysis.
In the remainder of this section  we present our analysis in three main steps. First  we
study the empirical fairness constraint given by the data collected during the exploration phase and
give a reduction from a cost-sensitive classiﬁcation problem subject to such fairness constraint to a
standard cost-sensitive classiﬁcation problem absent the constraint  based on [1]. We need to perform
two modiﬁcations on the reduction method in [1]. First  we allow our algorithm to handle fairness
constraints deﬁned by a separate data set that is different from the one deﬁning the cost objective.
Secondly  we also provide a fair approximate CSC oracle that returns a sparse solution  a distribution
over H with support size of at most 2. This will be useful for establishing uniform convergence.
Next  we present the algorithm run in the second phase: at each round t > T0  the algorithm makes a
prediction based on a randomized policy ⇡t 2 (H)  which is a solution to a feasibility program
given by [2]. We show how to rely on an approximate fair CSC oracle to solve this program efﬁciently.
Consequently  we generalize the results of [2] to the setting in which the given oracle may only
optimize the cost sensitive objective approximately. This may be of independent interest.

5

Finally  we bound the deviation between the algorithm’s empirical regret and true expected regret.
This in particular requires uniform convergence over the entire class of fair randomized policies 
which we show by leveraging the sparsity of the fair distributions.
We now give the proof of Theorem 3.1  with forward references to needed theorems and lemmas.

Proof of Theorem 3.1. We set T0 =⇥( pT ln(|H|/)). First  Lemma 3.4 shows that given our
empirical EFP constraint  there exists an optimal policy of support size at most 2. Next  Lemma B.2
shows that  with probability 1   over arrivals 1  . . .   T0  all convex combinations ⇡ 2 ⇧ satisfy
ˆ-EFP for ˆ =  +    = O⇣pln(|H|/)/T 1/4⌘. It also implies that the optimal -fair policy
is in the class. Theorem 3.5 shows that  given a CSC oracle for H  we can implement an efﬁcient
approximate CSC oracle for this class ⇧. Theorem 3.11 shows that  given an approximate CSC oracle
for any class  there is an efﬁcient bandit algorithm that plays from this class and achieves expected
regret O⇣ln (|H|T /)pT⌘.
Fairness: In the ﬁrst T0 rounds we play +1 which is 0-fair  and in the remaining rounds we play only
policies from ⇧. With probability 1   over the exploration data  every member of ⇧ is ( + )-fair.
Regret: The algorithm’s regret is at most T0 plus its regret  on rounds T0 + 1  . . .   T   to the optimal
policy in ⇧. By Proposition 2.3  this is at most twice the bandit algorithm’s regret on those rounds.
So our expected regret totals at most O⇣ln (|H|T /)pT⌘ to the best policy in ⇧. With probability
1    ⇧ contains the optimal -fair classiﬁer; with the remaining probability  the algorithm’s regret
to the best -fair classiﬁer can be bounded by T . Choosing   1pT gives the result.
3.1 Step 1: Constructing a Fair CSC Oracle From Exploration Data
Let SE denote the set of T0 labeled examples {zi = (xi  ai  yi)}T0
i=1 collected from the initial
exploration phase  and let DE denote the empirical distribution over SE. We will use DE as a proxy
for the true distribution to form an empirical fairness constraint. To support the learning algorithm in
the second phase  we need to construct an oracle that solves CSC problems subject to the empirical
fairness constraint. Formally  an instance of the FairCSC problem for the class H is given by a set of
n tuples {(xj  c(1)
j=1 as before  along with a fairness parameter  and an approximation
)}n
parameter ⌫. We wish to solve the following fair CSC problem:

  c(+1)

j

j

where F P R(⇡  DE) = F P R1(⇡  DE)  F P R1(⇡  DE) and each F P Rj(⇡  DE) denotes the
false positive rate of ⇡ on distribution DE. We show a useful structural property that there always
exists a small-support optimal solution; the proof appears in Appendix B.1.
Lemma 3.4. There exists an optimal solution for the FairCSC that is a distribution over H with
support size no greater than 2.

We therefore consider the set of sparse convex combinations:

⇧= {⇡ 2 (H) | Supp(⇡)  2 

|F P R(⇡  DE)|  + }

and focus on algorithms that only play policies from ⇧ and measure their performance with respect to
⇧. For any ⇡ 2 ⇧  we will write ⇡(h) to denote the probability ⇡ places on h. Applying a standard
concentration inequality  we can show (Lemma B.2) that each policy in ⇧ is also approximately fair
with respect to the underlying distribution.
We provide a reduction from FairCSC problems to standard CSC problems as follows: 1) We
ﬁrst apply a standard transformation on the input CSC objective to derive an equivalent weighted
classiﬁcation problem  in which each example j has importance weight |c(1)
|. 2) We
then run the fair classiﬁcation algorithm due to [1] that solves the weighted classiﬁcation problem
approximately using a polynomal number of CSC oracle calls. 3) Finally  we follow an approach
similar to that of [10] to shrink the support size of the solution returned by the fair classiﬁcation
algorithm down to at most 2  which can be done in polynomial time.

 c(+1)

j

j

6

min

⇡2(H)

E

h⇠⇡24

nXj=1

35

c(h(xj ))
j

such that

|F P R(⇡  DE)| 

(1)

Theorem 3.5 (Reduction from FairCSC to CSC). For any 0 <⌫</
2  there exists an oracle-
efﬁcient algorithm that calls a CSC oracle for H at most O(1/⌫2) times and computes a solution
ˆ⇡ 2 (H) that has a support size of at most 2  satisﬁes -EFP  and has total cost

E

h⇠ˆ⇡24

nXj=1
 c(+1)

j

ch(xj  aj )
j

|.

35  min

⇡2⇧

E

h⇠⇡24

nXj=1

ch(xj  aj )
j

35 + ✏

with ✏ = 4⌫Pn

j=1 |c(1)

j

3.2 Step 2: The Adaptive Learning Phase
Overview of bandit algorithm.
In the second phase  rounds t > T0  we utilize a bandit algorithm
to make predictions. We now describe the algorithm  which closely follows the ILOVETOCONBAN-
DITS algorithm by [2] but with important modiﬁcations that are necessary to handle approximation
error in the FairCSC oracle.
At each round t > T0  the bandit algorithm produces a distribution Qt over policies ⇡. Each policy ⇡
is a convex combination of two classiﬁers in H and satisﬁes approximate fairness. The algorithm
then draws ⇡ from Qt  draws h from ⇡  and labels ˆyt = h(xt). To choose Qt  the algorithm places
some constraints on Q and runs a short coordinate descent algorithm to ﬁnd a Q satisfying those
constraints. Finally  it mixes in a small amount of the uniform distribution over labels (which can
be realized by mixing between +1 and 1). We will see that the constraints  called the feasibility
program  correspond to roughly bounding the expected regret of the algorithm along with bounding
the variance in regret of each possible ⇡.

Feasibility program. To describe the feasibility program  we ﬁrst introduce some notation. For
each t  we will write pt to denote the probability that prediction ˆyt is selected by the learner  and `t
be the incurred (contextual bandit) loss given by the transformation in Proposition 2.3.
for each policy ⇡ 2 ⇧  let
tXs=1

denote the estimated average loss given by the inverse propensity score (IPS) estimator and true
expected loss for ⇡  respectively. Similarly  let

[1[⇡(x) 6= y]]i

(x a y)⇠DhE⇡

Pr[⇡(xs) = ˆys]

ˆLt(⇡) =

L(⇡) =

1
t

ps

E

`s

 

Reg(⇡) = L(⇡)  min
⇡02⇧

L(⇡0) 

dRegt(⇡) = ˆLt(⇡)  min

⇡02⇧

ˆLt(⇡0) 

Q(⇡) Pr[⇡(x) = ˆy]d⇡

denote the estimated average regret and the true expected regret. In order to bound the variance of
the IPS estimators  we will ensure that the learner predicts each label with minimum probability
µt at each round t. In particular  given a solution Q for the program and a minimum probability
parameter µt  the learner will predict according to the mixture distribution Qµt(· | x) (a distribution
that predicts +1 w.p. µt  and predicts according to Q w.p. 1  µt):

We describe the feasibility problem solved at each step. The approach and analysis directly follow
and extend that of [2]. In that work  the ﬁrst step at each round is to compute the best policy so far 

Qµt(ˆy | x) = µt + (1  2µt)Z⇡2⇧
Note that this can be represented as a convex combination of classiﬁers from H since we assume that
+1 2H . We deﬁne for each ⇡ 2 ⇧  bt(⇡) = dRegt(⇡)
which lets us computedRegt(⇡) and bt(⇡) for any policy ⇡. Here  our FairCSC oracle only computes
so far  which leads to corresponding approximationsgRegt(⇡) and ˜bt(⇡). Then  our algorithm solves

the same feasibility program (although a few more technicalities must be handled): given history Ht
(in the second phase) and minimum probability µt  ﬁnd a probability distribution Q over ⇧ such that
(Low regret)

approximate solutions  and so we can only compute regret relative to the approximately best policy

4(e2)µt ln(T )  and also initialize b0(⇡) = 0.

Z⇡2⇧
x⇠Ht

E

Q(⇡)˜bt1(⇡)d⇡  4
Qµt(⇡(x) | x)  4 + ˜bt1(⇡)

1

8⇡ 2 ⇧:

(Low variance)

7

Intuitively  the ﬁrst constraint ensures that the estimated regret (based on historical data) of the
solution is at most ˜O(1/pt). The second constraint bounds the variance of the resulting IPS loss
estimator for policies in ⇧  which in turn allows us to bound the deviation between the empirical
regret and the true regret for each policy over time. Importantly  we impose a tighter variance
constraint on policies that have lower empirical regret so far  which prioritizes their regret estimation.
To solve the feasibility program using our FairCSC oracle  we will run a coordinate descent algorithm 
similar to [2] (full description in Section B.3 as Algorithm 1). The FairCSC oracle is used to identify
and ﬁx violated constraints. Via a potential argument similar to the one of [2]  we can show that the
algorithm halts in a small number of iterations. We will also bound the additional error in the output
solution due to the approximation in the FairCSC oracle. In the following  let ⇤0 = 0 and for any
t  1 

t ln(T )
where ⌫ is the approximation parameter of the FairCSC oracle.
Lemma 3.6. Algorithm 1 halts in a number of iterations (and oracle calls) that is polynomial in 1
µt
and outputs a weight vector Q that is a probability distribution with the following guarantee:

 

⇤t :=

⌫
4(e  2)µ2

.

Z⇡2⇧

8⇡ 2 ⇧:

Q(⇡)(4 + bt1(⇡))d⇡  4 +⇤ t
E

Qµt(⇡(x) | x)  4 + bt1(⇡) +⇤ t.

x⇠Ht

1

3.3 Step 3: Regret Analysis
The key step in our regret analysis is to establish a tight relationship between the estimated regret and

The ﬁnal regret guarantee then essentially follows from the guarantee of Lemma 3.6 that the estimated
regret of our policy is bounded by ˜O (1/t) with proper setting of µt.

the true expected regret and show that for any ⇡ 2 ⇧  Reg(⇡)  2dReg(⇡) + ✏t  with ✏t = ˜O(1/pt).
To bound the deviation between Reg(⇡) and dRegt(⇡)  we need to bound the variance of our IPS
estimators. Let us deﬁne the following for any probability distribution P over ⇧  ⇡ 2 ⇧ 
P µ(⇡(x) | x)

P µ(⇡(x) | x)

ˆVt(P  ⇡  µ) := E

x⇠Ht

V (P  ⇡  µ) := E

x⇠D

1

1

Recall that through the feasibility program  we can directly bound ˆVt(Qt ⇡  µ t) for each round.
However  to apply a concentration inequality on the IPS estimator  we need to bound the population
variance V (Qt ⇡  µ t). We do that through a deviation bound between ˆVt(Qt ⇡  µ t) and V (Qt ⇡  µ t)
for all ⇡ 2 ⇧. In particular  we rely on the sparsity on ⇧ and apply a covering argument. Let ⇧⌘ ⇢ ⇧
denote an ⌘-cover such that for every ⇡ in ⇧  min⇡02⇧⌘ k⇡(h)  ⇡0(h)k1  ⌘ for any h 2H .
Since ⇧ consists of distributions with support size at most 2  we can take the cardinality of ⇧⌘ to be
bounded by d|H|2/⌘e.
Claim 3.7. Let P be any distribution over the policy set ⇧  and let ⇡ be any policy in ⇧. Then there
exists ⇡0 2 ⇧⌘ such that |V (P  ⇡  µ)  V (P  ⇡0  µ)|1 | ˆVt(P  ⇡  µ)  ˆVt(P  ⇡0  µ)|1  ⌘
Lemma 3.8. Suppose that µt q ln(2|⇧⌘|t2/)

  t  8 ln(2|⇧⌘|t2/). Then with probability 1   

µ(µ+⌘) .

2t

V (P  ⇡  µt)  6.4 ˆVt(P  ⇡  µt) + 162.6 +

2⌘

µt(µt + ⌘)

Next we bound the deviation between the estimated loss and true expected loss for every ⇡ 2 ⇧.
Lemma 3.9. Assume that the algorithm solves the per-round feasibility program with accuracy
guarantee of Lemma 3.6. With probability at least 1    we have for all t 2 [T ] all policies ⇡ 2 ⇧ 
 2 [0  µt]  and t  8 ln(2|⇧⌘|t2/) 
ln⇣|⇧⌘|T
 ⌘
|L(⇡) ˆLt(⇡)| (e2) 188.2 +

tXs=1✓6.4bs1(⇡) + 6.4⇤s1 +

µs(µs + ⌘)◆!+

1
t

2⌘

t

8

To bound the difference between Reg(⇡) and dRegt(⇡)  we will set ⌘ = 1/T 2  µt = 3.2 ln(|⇧⌘|T / )
the approximation parameter ⌫ of FairCSC to be 1/T .
Lemma 3.10. Assume that the algorithm solves the per-round feasibility program with the accuracy
guarantee of Lemma 3.6. With probability at least 1    we have for all t 2 [T ] all policies ⇡ 2 ⇧ 
and for all t  8 ln(2|H|2T 3/) 

pt

Reg(⇡)  2dRegt(⇡) + ✏t 

pt

and dRegt(⇡)  2Reg(⇡) + ✏t

with ✏t = 1000 ln(|H|2T 2/)
Theorem 3.11. The bandit algorithm  given access to an approximate-CSC oracle  runs in time

.

polynomial in T and achieves expected regret at most O⇣ln(|H|T /) pT⌘.

4 Lower Bound

In this section we show that the tradeoff that our algorithm exhibits between its regret bound and
the “fairness gap” 0   (i.e. our algorithm is 0-fair  but competes with the best -fair classiﬁer
when measuring regret) is optimal. We do this by constructing a lower bound instance consisting of
two very similar distributions  D1 and D2 deﬁned as a function of our algorithm’s fairness target .
Roughly  there are not enough samples to distinguish the distributions until at least ⇥( 1
2 ) rounds
elapse  but in order to equalize false positive rates on both distributions  an algorithm must “play it
safe” and incur constant regret per round during this time.

Theorem 4.1. Fix any ↵ 2 (0  0.5) and let T  ↵p16. Fix any   0.24. There exists a hypothesis
class H containing {±1} such that any algorithm satisfying a T ↵-EFP() fairness constraint has
expected regret with respect to the set of 0-EFP fair policies of ⌦T 2↵.

Acknowledgments
We thank Nati Srebro for a conversation leading to the question we study here. We thank Michael
Kearns for helpful discussions at an early stage of this work. YB and KL were funded in part by Israel
Science Foundation (ISF) grant 1044/16  the United States Air Force and DARPA under contract
FA8750-16-C-0022  and the Federmann Cyber Security Center in conjunction with the Israel national
cyber directorate. AR was funded in part by NSF grant CCF-1763307 and the United States Air Force
and DARPA under contract FA8750-16-C-0022. ZSW was supported in part by a Google Faculty
Research Award  a J.P. Morgan Faculty Award  a Mozilla research grant  and a Facebook Research
Award. Part of this work was done while KL and ZSW were visiting the Simons Institute for the
Theory of Computing  and BW was a postdoc at the University of Pennsylvania’s Warren Center and
at Microsoft Research  New York City. Any opinions  ﬁndings and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views of JP
Morgan  the United States Air Force and DARPA.

References
[1] Alekh Agarwal  Alina Beygelzimer  Miroslav Dudík  John Langford  and Hanna M. Wallach. A
reductions approach to fair classiﬁcation. In Proceedings of the 35th International Conference
on Machine Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018 
pages 60–69  2018.

[2] Alekh Agarwal  Daniel J. Hsu  Satyen Kale  John Langford  Lihong Li  and Robert E. Schapire.
Taming the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the
31th International Conference on Machine Learning  ICML 2014  Beijing  China  21-26 June
2014  pages 1638–1646  2014.

[3] Peter Auer  Nicolo Cesa-Bianchi  Yoav Freund  and Robert E Schapire. The nonstochastic

multiarmed bandit problem. SIAM journal on computing  32(1):48–77  2002.

[4] Gábor Bartók  Dávid Pál  and Csaba Szepesvári. Toward a classiﬁcation of ﬁnite partial-
monitoring games. In International Conference on Algorithmic Learning Theory  pages 224–
238. Springer  2010.

9

[5] Richard Berk  Hoda Heidari  Shahin Jabbari  Michael Kearns  and Aaron Roth. Fairness in
criminal justice risk assessments: The state of the art. Sociological Methods & Research 
0(0):0049124118782533  2018.

[6] Alina Beygelzimer  John Langford  Lihong Li  Lev Reyzin  and Robert Schapire. Contextual
bandit algorithms with supervised learning guarantees. In Geoffrey Gordon  David Dunson  and
Miroslav Dudík  editors  Proceedings of the Fourteenth International Conference on Artiﬁcial
Intelligence and Statistics  volume 15 of Proceedings of Machine Learning Research  pages
19–26  Fort Lauderdale  FL  USA  11–13 Apr 2011. PMLR.

[7] Avrim Blum  Suriya Gunasekar  Thodoris Lykouris  and Nati Srebro. On preserving non-
discrimination when combining expert advice. In Advances in Neural Information Processing
Systems  pages 8386–8397  2018.

[8] Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classiﬁ-

cation. Data Mining and Knowledge Discovery  21(2):277–292  2010.

[9] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big data  5(2):153–163  2017.

[10] Andrew Cotter  Heinrich Jiang  and Karthik Sridharan. Two-player games for efﬁcient non-

convex constrained optimization. CoRR  abs/1804.06500  2018.

[11] Amit Daniely  Nati Linial  and Shai Shalev-Shwartz. From average case complexity to improper

learning complexity. arXiv preprint arXiv:1311.2272  2013.

[12] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science
conference  pages 214–226. ACM  2012.

[13] Danielle Ensign  Sorelle A Friedler  Scott Neville  Carlos Scheidegger  and Suresh Venkata-
subramanian. Runaway feedback loops in predictive policing. In Conference on Fairness 
Accountability and Transparency  pages 160–171  2018.

[14] Danielle Ensign  Frielder Sorelle  Neville Scott  Scheidegger Carlos  and Venkatasubramanian
Suresh. Decision making with limited feedback. In Firdaus Janoos  Mehryar Mohri  and Karthik
Sridharan  editors  Proceedings of Algorithmic Learning Theory  volume 83 of Proceedings of
Machine Learning Research  pages 359–367. PMLR  07–09 Apr 2018.

[15] Michael Feldman  Sorelle A Friedler  John Moeller  Carlos Scheidegger  and Suresh Venkata-

subramanian. Certifying and removing disparate impact. In KDD  2015.

[16] Vitaly Feldman  Venkatesan Guruswami  Prasad Raghavendra  and Yi Wu. Agnostic learning

of monomials by halfspaces is hard. SIAM Journal on Computing  41(6):1558–1590  2012.

[17] M. Grötschel  L. Lovász  and A. Schrijver. The ellipsoid method and its consequences in

combinatorial optimization. Combinatorica  1(2):169–197  Jun 1981.

[18] Moritz Hardt  Eric Price  Nati Srebro  et al. Equality of opportunity in supervised learning. In

Advances in neural information processing systems  pages 3315–3323  2016.

[19] David P Helmbold  Nicholas Littlestone  and Philip M Long. Apple tasting. Information and

Computation  161(2):85–139  2000.

[20] Matthew Joseph  Michael Kearns  Jamie H Morgenstern  and Aaron Roth. Fairness in learning:
Classic and contextual bandits. In Advances in Neural Information Processing Systems  pages
325–333  2016.

[21] Matthew Joseph  Michael J. Kearns  Jamie Morgenstern  Seth Neel  and Aaron Roth. Merito-
cratic fairness for inﬁnite and contextual bandits. In Jason Furman  Gary E. Marchant  Huw
Price  and Francesca Rossi  editors  Proceedings of the 2018 AAAI/ACM Conference on AI 
Ethics  and Society  AIES 2018  New Orleans  LA  USA  February 02-03  2018  pages 158–163.
ACM  2018.

10

[22] Adam Tauman Kalai  Adam R Klivans  Yishay Mansour  and Rocco A Servedio. Agnostically

learning halfspaces. SIAM Journal on Computing  37(6):1777–1805  2008.

[23] Toshihiro Kamishima  Shotaro Akaho  and Jun Sakuma. Fairness-aware learning through
regularization approach. In Data Mining Workshops (ICDMW)  2011 IEEE 11th International
Conference on  pages 643–650. IEEE  2011.

[24] Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan. Inherent trade-offs in the fair

determination of risk scores. arXiv preprint arXiv:1609.05807  2016.

[25] Yang Liu  Goran Radanovic  Christos Dimitrakakis  Debmalya Mandal  and David C Parkes.

Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875  2017.

[26] Kristian Lum and William Isaac. To predict and serve? Signiﬁcance  13(5):14–19  2016.

11

,Yahav Bechavod
Katrina Ligett
Aaron Roth
Bo Waggoner
Steven Wu