2014,Efficient learning by implicit exploration in bandit problems with side observations,We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant  we assume that in addition to its own loss  the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting  we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines  we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting  we propose another algorithm with similar properties and with the benefit of always being computationally efficient  at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration  which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.,Efﬁcient learning by implicit exploration in bandit

problems with side observations

Tom´aˇs Koc´ak

R´emi Munos∗
{tomas.kocak gergely.neu michal.valko remi.munos}@inria.fr

SequeL team  INRIA Lille – Nord Europe  France

Gergely Neu

Michal Valko

Abstract

We consider online learning problems under a a partial observability model cap-
turing situations where the information conveyed to the learner is between full
information and bandit feedback. In the simplest variant  we assume that in addi-
tion to its own loss  the learner also gets to observe losses of some other actions.
The revealed losses depend on the learner’s action and a directed observation sys-
tem chosen by the environment. For this setting  we propose the ﬁrst algorithm
that enjoys near-optimal regret guarantees without having to know the observa-
tion system before selecting its actions. Along similar lines  we also deﬁne a new
partial information setting that models online combinatorial optimization prob-
lems where the feedback received by the learner is between semi-bandit and full
feedback. As the predictions of our ﬁrst algorithm cannot be always computed
efﬁciently in this setting  we propose another algorithm with similar properties
and with the beneﬁt of always being computationally efﬁcient  at the price of a
slightly more complicated tuning mechanism. Both algorithms rely on a novel
exploration strategy called implicit exploration  which is shown to be more efﬁ-
cient both computationally and information-theoretically than previously studied
exploration strategies for the problem.

1

Introduction

Consider the problem of sequentially recommending content for a set of users. In each period of
this online decision problem  we have to assign content from a news feed to each of our subscribers
so as to maximize clickthrough. We assume that this assignment needs to be done well in advance 
so that we only observe the actual content after the assignment was made and the user had the
opportunity to click. While we can easily formalize the above problem in the classical multi-armed
bandit framework [3]  notice that we will be throwing out important information if we do so! The
additional information in this problem comes from the fact that several news feeds can refer to the
same content  giving us the opportunity to infer clickthroughs for a number of assignments that
we did not actually make. For example  consider the situation shown on Figure 1a. In this simple
example  we want to suggest one out of three news feeds to each user  that is  we want to choose a
matching on the graph shown on Figure 1a which covers the users. Assume that news feeds 2 and 3
refer to the same content  so whenever we assign news feed 2 or 3 to any of the users  we learn
the value of both of these assignments. The relations between these assignments can be described
by a graph structure (shown on Figure 1b)  where nodes represent user-news feed assignments  and
edges mean that the corresponding assignments reveal the clickthroughs of each other. For a more
compact representation  we can group the nodes by the users  and rephrase our task as having to
choose one node from each group. Besides its own reward  each selected node reveals the rewards
assigned to all their neighbors.

∗Current afﬁliation: Google DeepMind

1

Figure 1a: Users and news feeds. The thick edges represent one
potential matching of users to feeds  grouped news feeds show the
same content.

Figure 1b: Users and news
feeds. Connected feeds mutually
reveal each others clickthroughs.

The problem described above ﬁts into the framework of online combinatorial optimization where in
each round  a learner selects one of a very large number of available actions so as to minimize the
losses associated with its sequence of decisions. Various instances of this problem have been widely
studied in recent years under different feedback assumptions [7  2  8]  notably including the so-called
full-information [13] and semi-bandit [2  16] settings. Using the example in Figure 1a  assuming full
information means that clickthroughs are observable for all assignments  whereas assuming semi-
bandit feedback  clickthroughs are only observable on the actually realized assignments. While
it is unrealistic to assume full feedback in this setting  assuming semi-bandit feedback is far too
restrictive in our example. Similar situations arise in other practical problems such as packet routing
in computer networks where we may have additional information on the delays in the network
besides the delays of our own packets.
In this paper  we generalize the partial observability model ﬁrst proposed by Mannor and Shamir
[15] and later revisited by Alon et al. [1] to accommodate the feedback settings situated between the
full-information and the semi-bandit schemes. Formally  we consider a sequential decision making
problem where in each time step t the (potentially adversarial) environment assigns a loss value to
each out of d components  and generates an observation system whose role will be clariﬁed soon.
Obliviously of the environment’s choices  the learner chooses an action Vt from a ﬁxed action
set S ⊂ {0  1}d represented by a binary vector with at most m nonzero components  and incurs
the sum of losses associated with the nonzero components of Vt. At the end of the round  the
learner observes the individual losses along the chosen components and some additional feedback
based on its action and the observation system. We represent this observation system by a directed
observability graph with d nodes  with an edge connecting i → j if and only if the loss associated
with j is revealed to the learner whenever Vt i = 1. The goal of the learner is to minimize its total
loss obtained over T repetitions of the above procedure. The two most well-studied variants of this
general framework are the multi-armed bandit problem [3] where each action consists of a single
component and the observability graph is a graph without edges  and the problem of prediction with
expert advice [17  14  5] where each action consists of exactly one component and the observability
graph is complete. In the true combinatorial setting where m > 1  the empty and complete graphs
correspond to the semi-bandit and full-information settings respectively.
Our model directly extends the model of Alon et al. [1]  whose setup coincides with m = 1 in our
framework. Alon et al. themselves were motivated by the work of Mannor and Shamir [15]  who
considered undirected observability systems where actions mutually uncover each other’s losses.
Mannor and Shamir proposed an algorithm based on linear programming that achieves a regret of
˜O(
√
cT )  where c is the number of cliques into which the graph can be split. Later  Alon et al. [1]
proposed an algorithm called EXP3-SET that guarantees a regret of O(
αT log d)  where α is an
upper bound on the independence numbers of the observability graphs assigned by the environment.
In particular  this bound is tighter than the bound of Mannor and Shamir since α ≤ c for any graph.
Furthermore  EXP3-SET is much more efﬁcient than the algorithm of Mannor and Shamir as it only
requires running the EXP3 algorithm of Auer et al. [3] on the decision set  which runs in time linear
in d. Alon et al. [1] also extend the model of Mannor and Shamir in allowing the observability
graph to be directed. For this setting  they offer another algorithm called EXP3-DOM with similar
guarantees  although with the serious drawback that it requires access to the observation system
before choosing its actions. This assumption poses severe limitations to the practical applicability
of EXP3-DOM  which also needs to solve a sequence of set cover problems as a subroutine.

√

2

content1content2e1 1e1 2e1 3e2 1e2 2e2 3user1user2newsfeed1newsfeed2newsfeed3user1user2content2content2e1 1e1 2e1 3e2 1e2 2e2 3In the present paper  we offer two computationally and information-theoretically efﬁcient algorithms
for bandit problems with directed observation systems. Both of our algorithms circumvent the costly
exploration phase required by EXP3-DOM by a trick that we will refer to IX as in Implicit eXplo-
ration. Accordingly  we name our algorithms EXP3-IX and FPL-IX  which are variants of the
well-known EXP3 [3] and FPL [12] algorithms enhanced with implicit exploration. Our ﬁrst algo-
rithm EXP3-IX is speciﬁcally designed1 to work in the setting of Alon et al. [1] with m = 1 and
does not need to solve any set cover problems or have any sort of prior knowledge concerning the
observation systems chosen by the adversary.2 FPL-IX  on the other hand  does need either to solve
set cover problems or have a prior upper bound on the independence numbers of the observability
graphs  but can be computed efﬁciently for a wide range of true combinatorial problems with m > 1.
We note that our algorithms do not even need to know the number of rounds T and our regret bounds
scale with the average independence number ¯α of the graphs played by the adversary rather than the
largest of these numbers. They both employ adaptive learning rates and unlike EXP3-DOM  they
do not need to use a doubling trick to be anytime or to aggregate outputs of multiple algorithms to
optimally set their learning rates. Both algorithms achieve regret guarantees of ˜O(m3/2
¯αT ) in the
combinatorial setting  which becomes ˜O(
Before diving into the main content  we give an important graph-theoretic statement that we will
rely on when analyzing both of our algorithms. The lemma is a generalized version of Lemma 13 of
Alon et al. [1] and its proof is given in Appendix A.
Lemma 1. Let G be a directed graph with vertex set V = {1  . . .   d}. Let N−
i be the in-
neighborhood of node i  i.e.  the set of nodes j such that (j → i) ∈ G. Let α be the independence

¯αT ) in the simple setting.

√

√

number of G and p1 . . .  pd are numbers from [0  1] such that(cid:80)d
where Pi =(cid:80)

pi
m pi + 1
m Pi + c
pj and c is a positive constant.

≤ 2mα log

d(cid:88)

(cid:18)

1 +

i=1

1

α

j∈N

−
i

i=1 pi ≤ m. Then

(cid:19)

m(cid:100)d2/c(cid:101) + d

+ 2m 

2 Multi-armed bandit problems with side information
In this section  we start by the simplest setting ﬁtting into our framework  namely the multi-armed
bandit problem with side observations. We provide intuition about the implicit exploration procedure
behind our algorithms and describe EXP3-IX  the most natural algorithm based on the IX trick.
The problem we consider is deﬁned as follows. In each round t = 1  2  . . .   T   the environment as-
signs a loss vector (cid:96)t ∈ [0  1]d for d actions and also selects an observation system described by the
directed graph Gt. Then  based on its previous observations (and likely some external source of ran-
domness) the learner selects action It and subsequently incurs and observes loss (cid:96)t It. Furthermore 
the learner also observes the losses (cid:96)t j for all j such that (It → j) ∈ Gt  denoted by the indicator
Ot i. Let Ft−1 = σ(It−1  . . .   I1) capture the interaction history up to time t. As usual in online
settings [6]  the performance is measured in terms of (total expected) regret  which is the difference
between a total loss received and the total loss of the best single action chosen in hindsight 

(cid:34) T(cid:88)

(cid:35)
((cid:96)t It − (cid:96)t i)

 

t=1

RT = max
i∈[d]

E

where the expectation integrates over the random choices made by the learning algorithm. Alon
et al. [1] adapted the well-known EXP3 algorithm of Auer et al. [3] for this precise problem. Their
algorithm  EXP3-DOM  works by maintaining a weight wt i for each individual arm i ∈ [d] in each
round  and selecting It according to the distribution

P [It = i|Ft−1 ] = (1 − γ)pt i + γµt i = (1 − γ)

+ γµt i 

wt i(cid:80)d

j=1 wt j

1EXP3-IX can also be efﬁciently implemented for some speciﬁc combinatorial decision sets even with

m > 1  see  e.g.  Cesa-Bianchi and Lugosi [7] for some examples.

2However  it is still necessary to have access to the observability graph to construct low bias estimates of

losses  but only after the action is selected.

3

where γ ∈ (0  1) is parameter of the algorithm and µt is an exploration distribution whose role we
will shortly clarify. After each round  EXP3-DOM deﬁnes the loss estimates

ˆ(cid:96)t i =

(cid:96)t i
ot i

1{(It→i)∈Gt} where

ot i = E [Ot i |Ft−1 ] = P [(It → i) ∈ Gt |Ft−1 ]

for each i ∈ [d]. These loss estimates are then used to update the weights for all i as

wt+1 i = wt ie−γ ˆ(cid:96)t i.

It is easy to see that the these loss estimates ˆ(cid:96)t i are unbiased estimates of the true losses whenever
pt i > 0 holds for all i. This requirement along with another important technical issue justify
the presence of the exploration distribution µt. The key idea behind EXP3-DOM is to compute a
dominating set Dt ⊆ [d] of the observability graph Gt in each round  and deﬁne µt as the uniform
distribution over Dt. This choice ensures that ot i ≥ pt i + γ/|Dt|  a crucial requirement for the
analysis of [1]. In what follows  we propose an exploration scheme that does not need any fancy
computations but  more importantly  works without any prior knowledge of the observability graphs.

2.1 Efﬁcient learning by implicit exploration
In this section  we propose the simplest exploration scheme imaginable  which consists of merely
pretending to explore. Precisely  we simply sample our action It from the distribution deﬁned as

P [It = i|Ft−1 ] = pt i =

 

(1)

wt i(cid:80)d

j=1 wt j

without explicitly mixing with any exploration distribution. Our key trick is to deﬁne the loss esti-
mates for all arms i as

ˆ(cid:96)t i =

(cid:96)t i

ot i + γt

1{(It→i)∈Gt} 

where γt > 0 is a parameter of our algorithm. It is easy to check that ˆ(cid:96)t i is a biased estimate of (cid:96)t i.
The nature of this bias  however  is very special. First  observe that ˆ(cid:96)t i is an optimistic estimate of

(cid:96)t i in the sense that E(cid:104)ˆ(cid:96)t i |Ft−1
(cid:105) ≤ (cid:96)t i. That is  our bias always ensures that  on expectation  we
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Ft−1
(cid:35)

underestimate the loss of any ﬁxed arm i. Even more importantly  our loss estimates also satisfy

(cid:18) ot i

(cid:34) d(cid:88)

pt i(cid:96)t i +

ot i + γt

pt i

ˆ(cid:96)t i

pt i(cid:96)t i

(cid:19)

− 1

E

i=1

(2)

d(cid:88)
d(cid:88)

i=1

d(cid:88)
d(cid:88)

i=1

=

=

pt i(cid:96)t i − γt

i=1

i=1

pt i(cid:96)t i
ot i + γt

 

that is  the bias of the estimated losses suffered by our algorithm is directly controlled by γt. As we
will see in the analysis  it is sufﬁcient to control the bias of our own estimated performance as long
as we can guarantee that the loss estimates associated with any ﬁxed arm are optimistic—which is
precisely what we have. Note that this slight modiﬁcation ensures that the denominator of ˆ(cid:96)t i is
lower bounded by pt i + γt  which is a very similar property as the one achieved by the exploration
scheme used by EXP3-DOM. We call the above loss estimation method implicit exploration or IX 
as it gives rise to the same effect as explicit exploration without actually having to implement any
exploration policy. In fact  explicit and implicit explorations can both be regarded as two different
approaches for bias-variance tradeoff: while explicit exploration biases the sampling distribution
of It to reduce the variance of the loss estimates  implicit exploration achieves the same result by
biasing the loss estimates themselves.
From this point on  we take a somewhat more predictable course and deﬁne our algorithm EXP3-IX
as a variant of EXP3 using the IX loss estimates. One of the twists is that EXP3-IX is actually based
on the adaptive learning-rate variant of EXP3 proposed by Auer et al. [4]  which avoids the necessity
of prior knowledge of the observability graphs in order to set a proper learning rate. This algorithm

is deﬁned by setting(cid:98)Lt−1 i =(cid:80)t−1

ˆ(cid:96)s i and for all i ∈ [d] computing the weights as
wt i = (1/d)e−ηt(cid:98)Lt−1 i .

s=1

These weights are then used to construct the sampling distribution of It as deﬁned in (1). The
resulting EXP3-IX algorithm is shown as Algorithm 1.

4

2.2 Performance guarantees for EXP3-IX

Our analysis follows the footsteps of Auer et al.
[3] and Gy¨orﬁ and Ottucs´ak [9]  who provide
an improved analysis of the adaptive learning-
rate rule proposed by Auer et al. [4]. However 
a technical subtlety will force us to proceed a
little differently than these standard proofs: for
achieving the tightest possible bounds and the
most efﬁcient algorithm  we need to tune our
learning rates according to some random quan-
tities that depend on the performance of EXP3-
IX. In fact  the key quantities in our analysis are
the terms

d(cid:88)

i=1

Qt =

pt i

ot i + γt

 

Algorithm 1 EXP3-IX
1: Input: Set of actions S = [d] 
2:
3: for t = 1 to T do
4:
5:

parameters γt ∈ (0  1)  ηt > 0 for t ∈ [T ].

wt i ← (1/d) exp (−ηt(cid:98)Lt−1 i) for i ∈ [d]

6: Wt ←(cid:80)d
ot i ←(cid:80)

An adversary privately chooses losses (cid:96)t i
for i ∈ [d] and generates a graph Gt
i=1 wt i
pt i ← wt i/Wt
Choose It ∼ pt = (pt 1  . . .   pt d)
Observe graph Gt
Observe pairs {i  (cid:96)t i} for (It → i) ∈ Gt
1{(It→i)∈Gt} for i ∈ [d]
ˆ(cid:96)t i ← (cid:96)t i

7:
8:
9:
10:
11:
12:
13: end for

pt j for i ∈ [d]

(j→i)∈Gt

ot i+γt

which depend on the interaction history Ft−1 for all t. Our theorem below gives the performance
guarantee for EXP3-IX using a parameter setting adaptive to the values of Qt. A full proof of the
theorem is given in the supplementary material.
Theorem 1. Setting ηt = γt =

s=1 Qs)   the regret of EXP3-IX satisﬁes

(cid:113)

RT ≤ 4E

(cid:34)(cid:114)(cid:16)

(log d)/(d +(cid:80)t−1
d +(cid:80)T
(cid:17)2
(cid:16)ˆ(cid:96)t i
T(cid:88)

d(cid:88)

(cid:17)

pt i

i=1

+

+ γt

Qt +

t=1

ˆ(cid:96)t i ≤ ηt
2

i=1

pt i

d(cid:88)
pt i(cid:96)t i ≤ T(cid:88)
(cid:114)(cid:16)

t=1

(cid:16) ηt
d +(cid:80)T

2

d(cid:88)

i=1

T(cid:88)
d(cid:88)

t=1

T(cid:88)

t=1

i=1

(cid:17)

(cid:20)

pt i(cid:96)t i ≤ 3

t=1 Qt

log d +

E

≤ E
for any j ∈ [d]  giving the desired result as

− log WT +1

ηT +1

(cid:20) log W1
T(cid:88)

η1

d(cid:88)

(cid:21)
pt i(cid:96)t i ≤ T(cid:88)

− log wT +1 j

ηT +1

(cid:34)(cid:114)(cid:16)

(cid:96)t j + 4E

(3)

(4)

(cid:35)

.

ηt

E

log d

t=1 Qt

(cid:17)
(cid:18) log Wt
(cid:20)(cid:18) log Wt
(cid:20)(cid:18) log Wt
T(cid:88)
(cid:20) log d
(cid:21)
(cid:17)

d +(cid:80)T

= E

ηT +1

t=1 Qt

t=1

E

ηt

ηt

(cid:19)

.

− log Wt+1

ηt+1

− log Wt+1

ηt+1

(cid:21)

.

− log Wt+1

.

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)Ft−1
(cid:21)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)Ft−1
(cid:105)
+ E(cid:104) ˆLT j
(cid:35)

ηt+1

(cid:21)

log d

 

Proof sketch. Following the proof of Lemma 1 in Gy¨orﬁ and Ottucs´ak [9]  we can prove that

Taking conditional expectations  using Equation (2) and summing up both sides  we get

Using Lemma 3.5 of Auer et al. [4] and plugging in ηt and γt  this becomes

Taking expectations on both sides  the second term on the right hand side telescopes into

t=1

i=1

t=1

where we used the deﬁnition of ηT and the optimistic property of the loss estimates.

Setting m = 1 and c = γt in Lemma 1  gives the following deterministic upper bound on each Qt.
Lemma 2. For all t ∈ [T ] 

d(cid:88)

Qt =

pt i

ot i + γt

i=1

(cid:18)

≤ 2αt log

5

(cid:19)

(cid:100)d2/γt(cid:101) + d

αt

1 +

+ 2.

Combining Lemma 2 with Theorem 1 we prove our main result concerning the regret of EXP3-IX.
Corollary 1. The regret of EXP3-IX satisﬁes

(cid:17)

(cid:114)(cid:16)
d + 2(cid:80)T
(cid:100)d2(cid:112)td/ log d(cid:101) + d

(cid:33)

t=1 (Htαt + 1)

log d 

= O(log(dT )).

where

RT ≤ 4

(cid:32)

Ht = log

1 +

αt

3 Combinatorial semi-bandit problems with side observations
We now turn our attention to the setting of online combinatorial optimization (see [13  7  2]). In
this variant of the online learning problem  the learner has access to a possibly huge action set
S ⊆ {0  1}d where each action is represented by a binary vector v of dimensionality d. In what
follows  we assume that (cid:107)v(cid:107)1 ≤ m holds for all v ∈ S and some 1 ≤ m (cid:28) d  with the case m = 1
corresponding to the multi-armed bandit setting considered in the previous section. In each round
t = 1  2  . . .   T of the decision process  the learner picks an action Vt ∈ S and incurs a loss of V T
t (cid:96)t.
At the end of the round  the learner receives some feedback based on its decision Vt and the loss
vector (cid:96)t. The regret of the learner is deﬁned as

(cid:35)

(cid:34) T(cid:88)

t=1

RT = max
v∈S

E

(Vt − v)T (cid:96)t

.

Previous work has considered the following feedback schemes in the combinatorial setting:

√

• The full information scheme where the learner gets to observe (cid:96)t regardless of the chosen
action. The minimax optimal regret of order m
T log d here is achieved by COMPONENT-
shown to enjoy a regret of order m3/2√
HEDGE algorithm of [13]  while the Follow-the-Perturbed-Leader (FPL) [12  10] was
• The semi-bandit scheme where the learner gets to observe the components (cid:96)t i of the loss
√
vector where Vt i = 1  that is  the losses along the components chosen by the learner at
time t. As shown by [2]  COMPONENTHEDGE achieves a near-optimal O(
√
mdT log d)
regret guarantee  while [16] show that FPL enjoys a bound of O(m
dT log d).
• The bandit scheme where the learner only observes its own loss V T

t (cid:96)t. There are currently
no known efﬁcient algorithms that get close to the minimax regret in this setting—the
reader is referred to Audibert et al. [2] for an overview of recent results.

T log d by [16].

In this section  we deﬁne a new feedback scheme situated between the semi-bandit and the full-
information schemes. In particular  we assume that the learner gets to observe the losses of some
other components not included in its own decision vector Vt. Similarly to the model of Alon et al.
[1]  the relation between the chosen action and the side observations are given by a directed observ-
ability Gt (see example in Figure 1). We refer to this feedback scheme as semi-bandit with side
observations. While our theoretical results stated in the previous section continue to hold in this set-
ting  combinatorial EXP3-IX could rarely be implemented efﬁciently—we refer to [7  13] for some
positive examples. As one of the main concerns in this paper is computational efﬁciency  we take
a different approach: we propose a variant of FPL that efﬁciently implements the idea of implicit
exploration in combinatorial semi-bandit problems with side observations.

3.1

Implicit exploration by geometric resampling

In each round t  FPL bases its decision on some estimate (cid:98)Lt−1 = (cid:80)t−1
Lt−1 =(cid:80)t−1

s=1 (cid:96)s as follows:

s=1

vT(cid:16)

ηt(cid:98)Lt−1 − Zt

(cid:17)

Vt = arg min

v∈S

ˆ(cid:96)s of the total losses

.

(5)

Here  ηt > 0 is a parameter of the algorithm and Zt is a perturbation vector with components drawn
independently from an exponential distribution with unit expectation. The power of FPL lies in
that it only requires an oracle that solves the (ofﬂine) optimization problem minv∈S vT(cid:96) and thus

6

can be used to turn any efﬁcient ofﬂine solver into an online optimization algorithm with strong
guarantees. To deﬁne our algorithm precisely  we need to some further notation. We redeﬁne Ft−1
to be σ(Vt−1  . . .   V1)  Ot i to be the indicator of the observed component and let

qt i = E [Vt i |Ft−1 ]

and

ot i = E [Ot i |Ft−1 ] .

The most crucial point of our algorithm is the construction of our loss estimates. To implement
the idea of implicit exploration by optimistic biasing  we apply a modiﬁed version of the geometric
resampling method of Neu and Bart´ok [16] constructed as follows: Let O(cid:48)
t(2)  . . . be inde-
pendent copies3 of Ot and let Ut i be geometrically distributed random variables for all i = [d] with
parameter γt. We let

t(1)  O(cid:48)

Kt i = min(cid:0)(cid:8)k : O(cid:48)

t i(k) = 1(cid:9) ∪ {Ut i}(cid:1)

(6)

and deﬁne our loss-estimate vector ˆ(cid:96)t ∈ Rd with its i-th element as

(7)
By deﬁnition  we have E [Kt i |Ft−1 ] = 1/(ot i + (1− ot i)γt)  implying that our loss estimates are
optimistic in the sense that they lower bound the losses in expectation:

ˆ(cid:96)t i = Kt iOt i(cid:96)t i.

E(cid:104) ˆ(cid:96)t i

(cid:105)

(cid:12)(cid:12)(cid:12)Ft−1

ot i

=

ot i + (1 − ot i)γt

(cid:96)t i ≤ (cid:96)t i.

Here we used the fact that Ot i is independent of Kt i and has expectation ot i given Ft−1. We call
this algorithm Follow-the-Perturbed-Leader with Implicit eXploration (FPL-IX  Algorithm 2).
Note that the geometric resampling procedure can be terminated as soon as Kt i becomes well-
deﬁned for all i with Ot i = 1. As noted by Neu and Bart´ok [16]  this requires generating at most d
copies of Ot on expectation. As each of these copies requires one access to the linear optimization
oracle over S  we conclude that the expected running time of FPL-IX is at most d times that of
the expected running time of the oracle. A high-probability guarantee of the running time can be

(cid:1) /γt holds with probability at least 1 − δ and thus we can
(cid:1) /γt steps with probability at least 1 − δ.

obtained by observing that Ut i ≤ log(cid:0) 1
stop sampling after at most d log(cid:0) d

δ

δ

3.2 Performance guarantees for FPL-IX
The analysis presented in this section com-
bines some techniques used by Kalai and Vem-
pala [12]  Hutter and Poland [11]  and Neu
and Bart´ok [16] for analyzing FPL-style learn-
ers. Our proofs also heavily rely on some spe-
ciﬁc properties of the IX loss estimate deﬁned
in Equation 7. The most important difference
from the analysis presented in Section 2.2 is
that now we are not able to use random learn-
ing rates as we cannot compute the values cor-
responding to Qt efﬁciently. In fact  these val-
ues are observable in the information-theoretic
sense  so we could prove bounds similar to The-
orem 1 had we had access to inﬁnite compu-
tational resources. As our focus in this paper
is on computationally efﬁcient algorithms  we
choose to pursue a different path. In particular 

Algorithm 2 FPL-IX
1: Input: Set of actions S 
2:
3: for t = 1 to T do
4:

parameters γt ∈ (0  1)  ηt > 0 for t ∈ [T ].

An adversary privately chooses losses (cid:96)t i
for all i ∈ [d] and generates a graph Gt
Draw Zt i ∼ Exp(1) for all i ∈ [d]

Vt ← arg minv∈S vT(cid:16)

ηt(cid:98)Lt−1 − Zt

(cid:17)

Receive loss V T
t (cid:96)t
Observe graph Gt
Observe pairs {i  (cid:96)t i} for all i  such that
(j → i) ∈ Gt and v(It)j = 1
Compute Kt i for all i ∈ [d] using Eq. (6)
ˆ(cid:96)t i ← Kt iOt i(cid:96)t i

5:
6:
7:
8:
9:

10:
11:
12: end for

our learning rates will be tuned according to efﬁciently computable approximations (cid:101)αt of the re-
spective independence numbers αt that satisfy αt/C ≤(cid:101)αt ≤ αt ≤ d for some C ≥ 1. For the sake

of simplicity  we analyze the algorithm in the oblivious adversary model. The following theorem
states the performance guarantee for FPL-IX in terms of the learning rates and random variables of
the form

d(cid:88)

(cid:101)Qt(c) =

qt i

.

ot i + c

i=1

3Such independent copies can be simply generated by sampling independent copies of Vt using the FPL rule
t(k) using the observability Gt. Notice that this procedure requires no interaction

(5) and then computing O(cid:48)
between the learner and the environment  although each sample requires an oracle access.

7

Theorem 2. Assume γt ≤ 1/2 for all t and η1 ≥ η2 ≥ ··· ≥ ηT . The regret of FPL-IX satisﬁes

RT ≤ m (log d + 1)

ηT

+ 4m

ηtE

T(cid:88)

t=1

(cid:19)(cid:21)

(cid:18) γt

1 − γt

T(cid:88)

t=1

+

γtE(cid:104)(cid:101)Qt(γt)
(cid:105)

.

Proof sketch. As usual for analyzing FPL methods [12  11  16]  we ﬁrst deﬁne a hypothetical learner

that uses a time-independent perturbation vector (cid:101)Z ∼ Z1 and has access to ˆ(cid:96)t on top of (cid:98)Lt−1

ηt(cid:98)Lt − (cid:101)Z

(cid:17)

.

.

E

t=1

ηT

E(cid:104)

≤ m (log d + 1)

which we can further upper bounded after a long and tedious calculation as

Clearly  this learner is infeasible as it uses observations from the future. Also  observe that this
learner does not actually interact with the environment and depends on the predictions made by the
actual learner only through the loss estimates. By standard arguments  we can prove

(cid:16)(cid:101)Vt − v
(cid:17)T ˆ(cid:96)t
Using the techniques of Neu and Bart´ok [16]  we can relate the performance of Vt to that of (cid:101)Vt 
(cid:17)2(cid:12)(cid:12)(cid:12)(cid:12)Ft−1
(cid:20)(cid:16)(cid:101)V T
(cid:20)(cid:101)Qt
(cid:105) ≤ ηt E
(cid:12)(cid:12)(cid:12)Ft−1
The result follows by observing that E(cid:104)
(cid:105) ≤ vT(cid:96)t for any ﬁxed v ∈ S by the optimistic
(cid:12)(cid:12)(cid:12)Ft−1
(cid:105)
t (cid:96)t|Ft−1] − γtE(cid:104)(cid:101)Qt(γt)
(cid:105) ≥ E [ V T
follows from observing that ot i ≥ (1/m)(cid:80)

property of the IX estimate and also from the fact that by the deﬁnition of the estimates we infer that

The next lemma shows a suitable upper bound for the last two terms in the bound of Theorem 2. It

(cid:12)(cid:12)(cid:12)Ft−1
E(cid:104)(cid:101)V T

(Vt − (cid:101)Vt)T ˆ(cid:96)t

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)Ft−1

t i∪{i}} qt j and applying Lemma 1.
−

(cid:18) γ

≤ 4mηtE

1 − γ

t−1

ˆ(cid:96)t

t−1

ˆ(cid:96)t

vT ˆ(cid:96)t

j∈{N

(cid:21)

(cid:21)

.

.

Lemma 3. For all t ∈ [T ] and any c ∈ (0  1) 

d(cid:88)

(cid:101)Qt(c) =

(cid:18)

(cid:19)

qt i

≤ 2mαt log

1 +

ot i + c

i=1

m(cid:100)d2/c(cid:101) + d

αt

+ 2m.

We are now ready to state the main result of this section  which is obtained by combining Theorem 2 
Lemma 3  and Lemma 3.5 of Auer et al. [4] applied to the following upper bound

t=1

Corollary 2. Assume that for all t ∈ [T ]  αt/C ≤ (cid:101)αt ≤ αt ≤ d for some C > 1  and assume

s=1 αs/C

md > 4. Setting ηt = γt =

(log d + 1) /

m

  the regret of FPL-IX satisﬁes

d + C(cid:80)T

t=1 αt.

≤ 2

(cid:113)
(cid:113)
C(cid:80)T
t=1 αt ≤ 2
(cid:17)(cid:17)
d +(cid:80)t−1
s=1(cid:101)αs

(cid:16)

T(cid:88)

αt

(cid:113)(cid:80)t

≤ T(cid:88)
(cid:113)
d +(cid:80)t−1
s=1(cid:101)αs
(cid:114)
(cid:114)(cid:16)
d + C(cid:80)T

RT ≤ Hm3/2

t=1

αt

(cid:16)
(cid:17)

(cid:20)(cid:101)Qt
vT(cid:16)
(cid:35)

v∈S

(cid:101)Vt = arg min
(cid:34) T(cid:88)

t=1 αt

(log d + 1)  where H = O(log(mdT )).

Conclusion We presented an efﬁcient algorithm for learning with side observations based on im-
plicit exploration. This technique gave rise to multitude of improvements. Remarkably  our algo-
rithms no longer need to know the observation system before choosing the action unlike the method
of [1]. Moreover  we extended the partial observability model of [15  1] to accommodate problems
with large and structured action sets and also gave an efﬁcient algorithm for this setting.

Acknowledgements The research presented in this paper was supported by French Ministry
of Higher Education and Research  by European Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no270327 (CompLACS)  and by FUI project Herm`es.

8

References
[1] Alon  N.  Cesa-Bianchi  N.  Gentile  C.  and Mansour  Y. (2013). From Bandits to Experts: A

Tale of Domination and Independence. In Neural Information Processing Systems.

[2] Audibert  J. Y.  Bubeck  S.  and Lugosi  G. (2014). Regret in Online Combinatorial Optimiza-

tion. Mathematics of Operations Research  39:31–45.

[3] Auer  P.  Cesa-Bianchi  N.  Freund  Y.  and Schapire  R. E. (2002a). The nonstochastic multi-

armed bandit problem. SIAM J. Comput.  32(1):48–77.

[4] Auer  P.  Cesa-Bianchi  N.  and Gentile  C. (2002b). Adaptive and self-conﬁdent on-line learning

algorithms. Journal of Computer and System Sciences  64:48–75.

[5] Cesa-Bianchi  N.  Freund  Y.  Haussler  D.  Helmbold  D.  Schapire  R.  and Warmuth  M.

(1997). How to use expert advice. Journal of the ACM  44:427–485.

[6] Cesa-Bianchi  N. and Lugosi  G. (2006). Prediction  Learning  and Games. Cambridge Univer-

sity Press  New York  NY  USA.

[7] Cesa-Bianchi  N. and Lugosi  G. (2012). Combinatorial bandits. Journal of Computer and

System Sciences  78:1404–1422.

[8] Chen  W.  Wang  Y.  and Yuan  Y. (2013). Combinatorial Multi-Armed Bandit: General Frame-

work and Applications. In International Conference on Machine Learning  pages 151–159.

[9] Gy¨orﬁ  L. and Ottucs´ak  b. (2007). Sequential prediction of unbounded stationary time series.

IEEE Transactions on Information Theory  53(5):866–1872.

[10] Hannan  J. (1957). Approximation to Bayes Risk in Repeated Play. Contributions to the theory

of games  3:97–139.

[11] Hutter  M. and Poland  J. (2004). Prediction with Expert Advice by Following the Perturbed

Leader for General Weights. In Algorithmic Learning Theory  pages 279–293.

[12] Kalai  A. and Vempala  S. (2005). Efﬁcient algorithms for online decision problems. Journal

of Computer and System Sciences  71:291–307.

[13] Koolen  W. M.  Warmuth  M. K.  and Kivinen  J. (2010). Hedging structured concepts. In

Proceedings of the 23rd Annual Conference on Learning Theory (COLT)  pages 93–105.

[14] Littlestone  N. and Warmuth  M. (1994). The weighted majority algorithm. Information and

Computation  108:212–261.

[15] Mannor  S. and Shamir  O. (2011).

From Bandits to Experts: On the Value of Side-

Observations. In Neural Information Processing Systems.

[16] Neu  G. and Bart´ok  G. (2013). An Efﬁcient Algorithm for Learning with Semi-bandit Feed-
In Jain  S.  Munos  R.  Stephan  F.  and Zeugmann  T.  editors  Algorithmic Learning
back.
Theory  volume 8139 of Lecture Notes in Computer Science  pages 234–248. Springer Berlin
Heidelberg.

[17] Vovk  V. (1990). Aggregating strategies.

In Proceedings of the third annual workshop on

Computational learning theory (COLT)  pages 371–386.

9

,Tomáš Kocák
Gergely Neu
Michal Valko
Remi Munos