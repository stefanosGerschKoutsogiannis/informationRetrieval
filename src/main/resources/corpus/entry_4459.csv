2014,Projective dictionary pair learning for pattern classification,Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However  the $\ell_0$ or $\ell_1$-norm sparsity constraint on the representation coefficients adopted in many DL methods makes the training and testing phases time consuming. We propose a new discriminative DL framework  namely projective dictionary pair learning (DPL)  which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods  the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases  but also lead to very competitive accuracies in a variety of visual classification tasks.,Projective dictionary pair learning for pattern

classiﬁcation

Shuhang Gu1  Lei Zhang1  Wangmeng Zuo2  Xiangchu Feng3

1Dept. of Computing  The Hong Kong Polytechnic University  Hong Kong  China

2School of Computer Science and Technology  Harbin Institute of Technology  Harbin  China

3Dept. of Applied Mathematics  Xidian University  Xi(cid:48)an  China

{cssgu  cslzhang}@comp.polyu.edu.hk

cswmzuo@gmail.com  xcfeng@mail.xidian.edu.cn

Abstract

Discriminative dictionary learning (DL) has been widely studied in various pattern
classiﬁcation problems. Most of the existing DL methods aim to learn a synthesis
dictionary to represent the input signal while enforcing the representation coef-
ﬁcients and/or representation residual to be discriminative. However  the (cid:96)0 or
(cid:96)1-norm sparsity constraint on the representation coefﬁcients adopted in most DL
methods makes the training and testing phases time consuming. We propose a new
discriminative DL framework  namely projective dictionary pair learning (DPL) 
which learns a synthesis dictionary and an analysis dictionary jointly to achieve
the goal of signal representation and discrimination. Compared with convention-
al DL methods  the proposed DPL method can not only greatly reduce the time
complexity in the training and testing phases  but also lead to very competitive
accuracies in a variety of visual classiﬁcation tasks.

1

Introduction

Sparse representation represents a signal as the linear combination of a small number of atoms cho-
sen out of a dictionary  and it has achieved a big success in various image processing and computer
vision applications [1  2]. The dictionary plays an important role in the signal representation process
[3]. By using a predeﬁned analytical dictionary (e.g.  wavelet dictionary  Gabor dictionary) to rep-
resent a signal  the representation coefﬁcients can be produced by simple inner product operations.
Such a fast and explicit coding makes analytical dictionary very attractive in image representation;
however  it is less effective to model the complex local structures of natural images.
Sparse representation with a synthesis dictionary has been widely studied in recent years [2  4  5].
With synthesis dictionary  the representation coefﬁcients of a signal are usually obtained via an
(cid:96)p-norm (p ≤1) sparse coding process  which is computationally more expensive than analytical
dictionary based representation. However  synthesis based sparse representation can better model
the complex image local structures and it has led to many state-of-the-art results in image restoration
[6]. Another important advantage lies in that the synthesis based sparse representation model allows
us to easily learn a desired dictionary from the training data. The seminal work of KSVD [1] tells
us that an over-complete dictionary can be learned from example natural images  and it can lead
to much better image reconstruction results than the analytically designed off-the-shelf dictionaries.
Inspired by KSVD  many dictionary learning (DL) methods have been proposed and achieved state-
of-the-art performance in image restoration tasks.
The success of DL in image restoration problems triggers its applications in image classiﬁcation
tasks. Different from image restoration  assigning the correct class label to the test sample is the
goal of classiﬁcation problems; therefore  the discrimination capability of the learned dictionary is

1

of the major concern. To this end  supervised dictionary learning methods have been proposed to
promote the discriminative power of the learned dictionary [4  5  7  8  9]. By encoding the query
sample over the learned dictionary  both the coding coefﬁcients and the coding residual can be used
for classiﬁcation  depending on the employed DL model. Discriminative DL has led to many state-
of-the-art results in pattern recognition problems.
One popular strategy of discriminative DL is to learn a shared dictionary for all classes while en-
forcing the coding coefﬁcients to be discriminative [4  5  7]. A classiﬁer on the coding coefﬁcients
can be trained simultaneously to perform classiﬁcation. Mairal et al. [7] proposed to learn a dictio-
nary and a corresponding linear classiﬁer in the coding vector space. In the label consistent KSVD
(LC-KSVD) method  Jiang et al. [5] introduced a binary class label sparse code matrix to encourage
samples from the same class to have similar sparse codes. In [4]  Mairal et al. proposed a task driv-
en dictionary learning (TDDL) framework  which minimizes different risk functions of the coding
coefﬁcients for different tasks.
Another popular line of research in DL attempts to learn a structured dictionary to promote dis-
crimination between classes [2  8  9  10]. The atoms in the structured dictionary have class labels 
and the class-speciﬁc representation residual can be computed for classiﬁcation. Ramirez et al. [8]
introduced an incoherence promotion term to encourage the sub-dictionaries of different classes
to be independent. Yang et al. [9] proposed a Fisher discrimination dictionary learning (FDDL)
method which applies the Fisher criterion to both representation residual and representation coef-
ﬁcient. Wang et al. [10] proposed a max-margin dictionary learning (MMDL) algorithm from the
large margin perspective.
In most of the existing DL methods  (cid:96)0-norm or (cid:96)1-norm is used to regularize the representation
coefﬁcients since sparser coefﬁcients are more likely to produce better classiﬁcation results. Hence
a sparse coding step is generally involved in the iterative DL process. Although numerous algorithms
have been proposed to improve the efﬁciency of sparse coding [11  12]  the use of (cid:96)0-norm or (cid:96)1-
norm sparsity regularization is still a big computation burden and makes the training and testing
inefﬁcient.
It is interesting to investigate whether we can learn discriminative dictionaries but without the costly
(cid:96)0-norm or (cid:96)1-norm sparsity regularization. In particular  it would be very attractive if the represen-
tation coefﬁcients can be obtained by linear projection instead of nonlinear sparse coding. To this
end  in this paper we propose a projective dictionary pair learning (DPL) framework to learn a syn-
thesis dictionary and an analysis dictionary jointly for pattern classiﬁcation. The analysis dictionary
is trained to generate discriminative codes by efﬁcient linear projection  while the synthesis dictio-
nary is trained to achieve class-speciﬁc discriminative reconstruction. The idea of using functions to
predict the representation coefﬁcients is not new  and fast approximate sparse coding methods have
been proposed to train nonlinear functions to generate sparse codes [13  14]. However  there are
clear difference between our DPL model and these methods. First  in DPL the synthesis dictionary
and analysis dictionary are trained jointly  which ensures that the representation coefﬁcients can be
approximated by a simple linear projection function. Second  DPL utilizes class label information
and promotes discriminative power of the representation codes.
One related work to this paper is the analysis-based sparse representation prior learning [15  16] 
which represents a signal from a dual viewpoint of the commonly used synthesis model. Analy-
sis prior learning tries to learn a group of analysis operators which have sparse responses to the
latent clean signal. Sprechmann et al. [17] proposed to train a group of analysis operators for clas-
siﬁcation; however  in the testing phase a costly sparsity-constrained optimization problem is still
required. Feng et al.
[18] jointly trained a dimensionality reduction transform and a dictionary
for face recognition. The discriminative dictionary is trained in the transformed space  and sparse
coding is needed in both the training and testing phases.
The contribution of our work is two-fold. First  we introduce a new DL framework  which extends
the conventional discriminative synthesis dictionary learning to discriminative synthesis and analysis
dictionary pair learning (DPL). Second  the DPL utilizes an analytical coding mechanism and it
largely improves the efﬁciency in both the training and testing phases. Our experiments in various
visual classiﬁcation datasets show that DPL achieves very competitive accuracy with state-of-the-art
DL algorithms  while it is signiﬁcantly faster in both training and testing.

2

2 Projective Dictionary Pair Learning

2.1 Discriminative dictionary learning

Denote by X = [X1  . . .   Xk  . . .   XK] a set of p-dimensional training samples from K classes  where
Xk ∈ Rp×n is the training sample set of class k  and n is the number of samples of each class. Dis-
criminative DL methods aim to learn an effective data representation model from X for classiﬁcation
tasks by exploiting the class label information of training data. Most of the state-of-the-art discrim-
inative DL methods [5  7  9] can be formulated under the following framework:

minD A (cid:107) X − DA (cid:107)2

F +λ (cid:107) A (cid:107)p +Ψ(D  A  Y) 

(1)
where λ ≥ 0 is a scalar constant  Y represents the class label matrix of samples in X  D is the
synthesis dictionary to be learned  and A is the coding coefﬁcient matrix of X over D. In the training
model (1)  the data ﬁdelity term (cid:107) X − DA (cid:107)2
F ensures the representation ability of D; (cid:107) A (cid:107)p is
the (cid:96)p-norm regularizer on A; and Ψ(D  A  Y) stands for some discrimination promotion function 
which ensures the discrimination power of D and A.
As we introduced in Section 1  some DL methods [4  5  7] learn a shared dictionary for all classes
and a classiﬁer on the coding coefﬁcients simultaneously  while some DL methods [8  9  10] learn
a structured dictionary to promote discrimination between classes. However  they all employ (cid:96)0 or
(cid:96)1-norm sparsity regularizer on the coding coefﬁcients  making the training stage and the consequent
testing stage inefﬁcient.
In this work  we extend the conventional DL model in (1)  which learns a discriminative synthesis
dictionary  to a novel DPL model  which learns a pair of synthesis and analysis dictionaries. No
costly (cid:96)0 or (cid:96)1-norm sparsity regularizer is required in the proposed DPL model  and the coding
coefﬁcients can be explicitly obtained by linear projection. Fortunately  DPL does not sacriﬁce the
classiﬁcation accuracy while achieving signiﬁcant improvement in the efﬁciency  as demonstrated
by our extensive experiments in Section 3.

2.2 The dictionary pair learning model

The conventional discriminative DL model in (1) aims to learn a synthesis dictionary D to sparsely
represent the signal X  and a costly (cid:96)1-norm sparse coding process is needed to resolve the code A.
Suppose that if we can ﬁnd an analysis dictionary  denoted by P ∈ RmK×p   such that the code A
can be analytically obtained as A = PX  then the representation of X would become very efﬁcient.
Based on this idea  we propose to learn such an analysis dictionary P together with the synthesis
dictionary D  leading to the following DPL model:

(cid:107)X − DPX(cid:107)2

{P∗ D∗} = arg min
P D

F +Ψ(D  P  X  Y) 

(2)
where Ψ(D  P  X  Y) is some discrimination function. D and P form a dictionary pair: the analysis
dictionary P is used to analytically code X  and the synthesis dictionary D is used to reconstruct X.
The discrimination power of the DPL model depends on the suitable design of Ψ(D  P  X  Y) .
We propose to learn a structured synthesis dictionary D = [D1  . . .   Dk  . . .   DK] and a structured
analysis dictionary P = [P1; . . . ; Pk; . . . ; PK]  where {Dk ∈ Rp×m  Pk ∈ Rm×p} forms a sub-
dictionary pair corresponding to class k. Recent studies on sparse subspace clustering [19] have
proved that a sample can be represented by its corresponding dictionary if the signals satisfy certain
incoherence condition. With the structured analysis dictionary P  we want that the sub-dictionary
Pk can project the samples from class i  i (cid:54)= k  to a nearly null space  i.e. 

(3)
Clearly  with (3) the coefﬁcient matrix PX will be nearly block diagonal. On the other hand  with
the structured synthesis dictionary D  we want that the sub-dictionary Dk can well reconstruct the
data matrix Xk from its projective code matrix PkXk; that is  the dictionary pair should minimize
the reconstruction error:

PkXi ≈ 0 ∀k (cid:54)= i.

(cid:88)K

min
P D

k=1

(cid:107) Xk − DkPkXk (cid:107)2
F .

(4)

(5)

Based on the above analysis  we can readily have the following DPL model:

{P∗  D∗} = arg min
P D

(cid:88)K

k=1

(cid:107) Xk − DkPkXk (cid:107)2

F +λ (cid:107) Pk ¯Xk (cid:107)2
F 

s.t. (cid:107) di (cid:107)2

2≤ 1.

3

Algorithm 1 Discriminative synthesis&analysis dictionary pair learning (DPL)
Input: Training samples for K classes X = [X1  X2  . . .   XK ]  parameter λ  τ  m;
1: Initialize D(0) and P(0) as random matrixes with unit Frobenious norm  t = 0;
2: while not converge do
3:
4:
5:
6:
7:
end for
8:
9: end while
Output: Analysis dictionary P  synthesis dictionary D.

t ← t + 1;
for i=1:K do
Update A(t)
Update P(t)
Update D(t)

k by (8);
k by (10);
k by (12);

where ¯Xk denotes the complementary data matrix of Xk in the whole training set X  λ > 0 is a scalar
constant  and di denotes the ith atom of synthesis dictionary D. We constrain the energy of each
atom di in order to avoid the trivial solution of Pk = 0 and make the DPL more stable.
The DPL model in (5) is not a sparse representation model  while it enforces group sparsity on the
code matrix PX (i.e.  PX is nearly block diagonal). Actually  the role of sparse coding in classiﬁ-
cation is still an open problem  and some researchers argued that sparse coding may not be crucial
to classiﬁcation tasks [20  21]. Our ﬁndings in this work are supportive to this argument. The D-
PL model leads to very competitive classiﬁcation performance with those sparse coding based DL
models  but it is much faster.

2.3 Optimization

The objective function in (5) is generally non-convex. We introduce a variable matrix A and relax
(5) to the following problem:
{P∗  A∗  D∗} = arg min
P A D

F +τ (cid:107)PkXk−Ak(cid:107)2

F +λ(cid:107)Pk ¯Xk(cid:107)2
F 

(cid:107)Xi−DkAk(cid:107)2

s.t. (cid:107) di (cid:107)2

2≤ 1. (6)

K(cid:88)

k=1

where τ is a scalar constant. All terms in the above objective function are characterized by Frobenius
norm  and (6) can be easily solved. We initialize the analysis dictionary P and synthesis dictionary
D as random matrices with unit Frobenius norm  and then alternatively update A and {D  P}. The
minimization can be alternated between the following two steps.
(1) Fix D and P  update A

(cid:88)K

F +τ (cid:107) PkXk − Ak (cid:107)2
F .
This is a standard least squares problem and we have the closed-form solution:

A∗ = arg min
A

(cid:107) Xk − DkAk (cid:107)2

k=1

(2) Fix A  update D and P:

A∗
k = (DT

(cid:40)
P∗ = arg minP(cid:80)K
D∗ = arg minD(cid:80)K

k Dk + τI)−1(τPkXk + DT

k Xk).

k=1 τ (cid:107) PkXk − Ak (cid:107)2
k=1 (cid:107) Xk − DkAk (cid:107)2

F +λ (cid:107) Pk ¯Xk (cid:107)2
F;
2≤ 1.
F  s.t. (cid:107) di (cid:107)2

(7)

(8)

(9)

The closed-form solutions of P can be obtained as:
k (τXkXT

P∗
k = τAkXT

(10)
where γ = 10e−4 is a small number. The D problem can be optimized by introducing a variable S:
(11)

s.t. D = S  (cid:107) si (cid:107)2

(cid:107) Xk − DkAk (cid:107)2
F 

2≤ 1.

k + λ ¯Xk

k + γI)−1 
¯XT

The optimal solution of (11) can be obtained by the ADMM algorithm:
F +ρ (cid:107) Dk − S(r)

k=1 (cid:107) Xk − DkAk (cid:107)2
k=1 ρ (cid:107) D(r+1)
− S(r+1)

k

k

− Sk + T(r)

k

(cid:107)2
F 

  update ρ if appropriate.

(cid:107)2
k + T(r)
F 
k
s.t. (cid:107) si (cid:107)2
2≤ 1 

(12)

min
D S

(cid:88)K
D(r+1) = arg minD(cid:80)K
S(r+1) = arg minS(cid:80)K

T(r+1) =T(r) + D(r+1)

k=1

k

4

(a)(cid:107) P∗

k y (cid:107)2

2

(b)(cid:107) y − D∗

k P∗

k y (cid:107)2

2

Figure 1: (a) The representation codes and (b) reconstruction error on the Extended YaleB dataset.

In each step of optimization  we have closed form solutions for variables A and P  and the ADMM
based optimization of D converges rapidly. The training of the proposed DPL model is much faster
than most of previous discriminative DL methods. The proposed DPL algorithm is summarized in
Algorithm 1. When the difference between the energy in two adjacent iterations is less than 0.01 
the iteration stops. The analysis dictionary P and the synthesis dictionary D are then output for
classiﬁcation.
One can see that the ﬁrst sub-objective function in (9) is a discriminative analysis dictionary learner 
focusing on promoting the discriminative power of P; the second sub-objective function in (9) is a
representative synthesis dictionary learner  aiming to minimize the reconstruction error of the input
signal with the coding coefﬁcients generated by the analysis dictionary P. When the minimization
process converges  a balance between the discrimination and representation power of the model can
be achieved.

k P∗

k Xi (cid:107)2

k P∗

k Xk (cid:107)2

k Xk; that is  the residual (cid:107) Xk − D∗

k Xi  i (cid:54)= k  will be small and D∗
F will be much larger than (cid:107) Xk − D∗

2.4 Classiﬁcation scheme
In the DPL model  the analysis sub-dictionary P∗
k is trained to produce small coefﬁcients for samples
from classes other than k  and it can only generate signiﬁcant coding coefﬁcients for samples from
class k. Meanwhile  the synthesis sub-dictionary D∗
k is trained to reconstruct the samples of class k
from their projective coefﬁcients P∗
F will be small. On
the other hand  since P∗
k is not trained to reconstruct Xi  the residual
(cid:107) Xi − D∗
k P∗
In the testing phase  if the query sample y is from class k  its projective coding vector by P∗
k (i.e. 
i   i (cid:54)= k  tend to
P∗
ky ) will be more likely to be signiﬁcant  while its projective coding vectors by P∗
be small. Consequently  the reconstruction residual (cid:107) y − D∗
2 tends to be much smaller than
the residuals (cid:107) y − D∗
2  i (cid:54)= k. Let us use the Extended YaleB face dataset [22] to illustrate
i P∗
this. (The detailed experimental setting can be found in Section 3.) Fig. 1(a) shows the (cid:96)2-norm
of the coefﬁcients P∗
ky  where the horizontal axis refers to the index of y and the vertical axis refers
k . One can clearly see that (cid:107) P∗
to the index of P∗
2 has a nearly block diagonal structure  and
the diagonal blocks are produced by the query samples which have the same class labels as P∗
k .
Fig. 1(b) shows the reconstruction residual (cid:107) y − D∗
k y (cid:107)2
2 also
has a block diagonal structure  and only the diagonal blocks have small residuals. Clearly  the class-
speciﬁc reconstruction residual can be used to identify the class label of y  and we can naturally have
the following classiﬁer associated with the DPL model:

2. One can see that (cid:107) y − D∗

k y (cid:107)2
k P∗

k Xk (cid:107)2
F .

k P∗

k y (cid:107)2

i y (cid:107)2

k P∗

k y (cid:107)2

identity(y) = arg mini (cid:107) y − DiPiy (cid:107)2 .

(13)

2.5 Complexity and Convergence

Complexity In the training phase of DPL  Ak  Pk and Dk are updated alternatively. In each iteration 
the time complexities of updating Ak  Pk and Dk are O(mpn + m3 + m2n)  O(mnp + p3 + mp2) and
O(W(pmn + m3 + m2p + p2m))  respectively  where W is the iteration number in ADMM algorithm
for updating D. We experimentally found that in most cases W is less than 20. In many applications 
the number of training samples and the number of dictionary atoms for each class are much smaller
than the dimension p. Thus the major computational burden in the training phase of DPL is on
updating Pk  which involves an inverse of a p× p matrix {τXkXT
k + γI}. Fortunately  this

k + λ¯Xk ¯XT

5

 (a) *22kPy (b) **22kkyDPy (a) *22kPy (b) **22kkyDPy Figure 2: The convergence curve of DPL on the AR database.

k P∗

matrix will not change in the iteration  and thus the inverse of it can be pre-computed. This greatly
accelerates the training process.
In the testing phase  our classiﬁcation scheme is very efﬁcient. The computation of class-speciﬁc
reconstruction error (cid:107) y − D∗
k y (cid:107)2 only has a complexity of O(mp). Thus  the total complexity of
our model to classify a query sample is O(Kmp).
Convergence The objective function in (6) is a bi-convex problem for {(D  P)  (A)}  e.g.  by ﬁxing
A the function is convex for D and P  and by ﬁxing D and P the function is convex for A. The con-
vergence of such a problem has already been intensively studied [23]  and the proposed optimization
algorithm is actually an alternate convex search (ACS) algorithm. Since we have the optimal solu-
tions of updating A  P and D  and our objective function has a general lower bound 0  our algorithm
is guaranteed to converge to a stationary point. A detailed convergence analysis can be found in our
supplementary ﬁle.
It is empirically found that the proposed DPL algorithm converges rapidly. Fig. 2 shows the conver-
gence curve of our algorithm on the AR face dataset [24]. One can see that the energy drops quickly
and becomes very small after 10 iterations. In most of our experiments  our algorithm will converge
in less than 20 iterations.

3 Experimental Results

We evaluate the proposed DPL method on various visual classiﬁcation datasets  including two face
databases (Extended YaleB [22] and AR [24])  one object categorization database (Caltech101)
[25]  and one action recognition database (UCF 50 action [26]). These datasets are widely used in
previous works [5  9] to evaluate the DL algorithms.
Besides the classiﬁcation accuracy  we also report the training and testing time of competing algo-
rithms in the experiments. All the competing algorithms are implemented in Matlab except for SVM
which is implemented in C. All experiments are run on a desktop PC with 3.5GHz Intel CPU and
8 GB memory. The testing time is calculated in terms of the average processing time to classify a
single query sample.

3.1 Parameter setting
There are three parameters  m  λ and τ in the proposed DPL model. To achieve the best performance 
in face recognition and object recognition experiments  we set the number of dictionary atoms as
its maximum (i.e.  the number of training samples) for all competing DL algorithms  including the
proposed DPL. In the action recognition experiment  since the samples per class is relatively big  we
set the number of dictionary atoms of each class as 50 for all the DL algorithms. Parameter τ is an
algorithm parameter  and the regularization parameter λ is to control the discriminative property of
P. In all the experiments  we choose λ and τ by 10-fold cross validation on each dataset. For all the
competing methods  we tune their parameters for the best performance.

3.2 Competing methods
We compare the proposed DPL method with the following methods: the base-line nearest subspace
classiﬁer (NSC) and linear support vector machine (SVM)  sparse representation based classiﬁcation
(SRC) [2] and collaborative representation based classiﬁcation (CRC) [21]  and the state-of-the-art
DL algorithms DLSI [8]  FDDL [9] and LC-KSVD [5]. The original DLSI represents the test sample
by each class-speciﬁc sub-dictionary. The results in [9] have shown that by coding the test sample
collaboratively over the whole dictionary  the classiﬁcation performance can be greatly improved.

6

01020304050456Iteration NumberEnergyFigure 3: Sample images in the (a) Extended YaleB and (b) AR databases.

Therefore  we follow the use of DLDI in [9] and denote this method as DLSI(C). For the two
variants of LC-KSVD proposed in [5]  we adopt the LC-KSVD2 since it can always produce better
classiﬁcation accuracy.
3.3 Face recognition
We ﬁrst evaluate our algorithm on two widely used face datasets: Extended YaleB [22] and AR [24].
The Extended YaleB database has large variations in illumination and expressions  as illustrated in
Fig. 3(a). The AR database involves many variations such as illumination  expressions and sunglass
and scarf occlusion  as illustrated in Fig. 3(b).
We follow the experimental settings in [5] for fair comparison with state-of-the-arts. A set of 2 414
face images of 38 persons are extracted from the Extended YaleB database. We randomly select half
of the images per subject for training and the other half for testing. For the AR database  a set of
2 600 images of 50 female and 50 male subjects are extracted. 20 images of each subject are used
for training and the remain 6 images are used for testing. We use the features provided by Jiang
et al. [5] to represent the face image. The feature dimension is 504 for Extended YaleB and 540
for AR. The parameter τ is set to 0.05 on both the datasets and λ is set to 3e-3 and 5e-3 on the
Extended YaleB and AR datasets  respectively. In these two experiments  we also compare with the
max-margin dictionary learning (MMDL) [10] algorithm  whose recognition accuracy is cropped
from the original paper but the training/testing time is not available.

Table 1: Results on the Extended YaleB database.

Table 2: Results on the AR database.

Accuracy

Accuracy

(%)
94.7
95.6
97.0
96.5
97.0
96.7
96.7
97.3
97.5

Training
time (s)
no need

0.70

no need
no need
567.47
6 574.6
412.58

-

4.38

Testing
time (s)
1.41e-3
3.49e-5
1.92e-3
2.16e-2
4.30e-2

1.43

4.22e-4

-

1.71e-4

NSC
SVM
CRC
SRC

DLSI(C)
FDDL

LC-KSVD

MMDL

DPL

(%)
92.0
96.5
98.0
97.5
97.5
97.5
97.8
97.3
98.3

Training
time (s)
no need

3.42

no need
no need
2 470.5
61 709
1 806.3

-

11.30

Testing
time (s)
3.29e-3
6.16e-5
5.08e-3
3.42e-2

0.16
2.50

7.72e-4

-

3.93e-4

NSC
SVM
CRC
SRC

DLSI(C)
FDDL

LC-KSVD

MMDL

DPL

Extended YaleB database The recognition accuracies and training/testing time by different algo-
rithms on the Extended YaleB database are summarized in Table 1. The proposed DPL algorithm
achieves the best accuracy  which is slightly higher than MMDL  DLSI(C)  LC-KSVD and FDDL.
However  DPL has obvious advantage in efﬁciency over the other DL algorithms.
AR database The recognition accuracies and running time on the AR database are shown in Table 2.
DPL achieves the best results among all the competing algorithms. Compared with the experiment
on Extended YaleB  in this experiment there are more training samples and the feature dimension is
higher  and DPL(cid:48)s advantage of efﬁciency is much more obvious. In training  it is more than 159
times faster than DLSI and LC-KSVD  and 5 460 times faster than FDDL.

3.4 Object recognition
In this section we test DPL on object categorization by using the Caltech101 database [25]. The
Caltech101 database [25] includes 9 144 images from 102 classes (101 common object classes and
a background class). The number of samples in each category varies from 31 to 800. Following
the experimental settings in [5  27]  30 samples per category are used for training and the rest are

7

 (a) (b) (c) (d) (a) (b) (a) Table 3: Recognition accuracy(%) & running time(s) on the Caltech101 database.

Accuracy

Training time

Testing time

NSC
SVM
CRC
SRC

DLSI(C)
FDDL

LC-KSVD

DPL

70.1
64.6
68.2
70.7
73.1
73.2
73.6
73.9

no need

14.6

no need
no need
97 200
104 000
12 700
134.6

1.79e-2
1.81e-4
1.38e-2

1.09
1.46
12.86
4.17e-3
1.29e-3

used for testing. We use the standard bag-of-words (BOW) + spatial pyramid matching (SPM)
framework [27] for feature extraction. Dense SIFT descriptors are extracted on three grids of sizes
1×1  2×2  and 4×4 to calculate the SPM features. For a fair comparison with [5]  we use the vector
quantization based coding method to extract the mid-level features and use the standard max pooling
approach to build up the high dimension pooled features. Finally  the original 21 504 dimensional
data is reduced to 3 000 dimension by PCA. The parameters τ and λ used in our algorithm are 0.05
and 1e-4  respectively.
The experimental results are listed in Table 3. Again  DPL achieves the best performance. Though
its classiﬁcation accuracy is only slightly better than the DL methods  its advantage in terms of
training/testing time is huge.

3.5 Action recognition

Action recognition is an important yet very challenging task and it has been attracting great research
interests in recent years. We test our algorithm on the UCF 50 action database [26]  which includes
50 categories of 6 680 human action videos from YouTube. We use the action bank features [28]
and ﬁve-fold data splitting to evaluate our algorithm. For all the comparison methods  the feature
dimension is reduced to 5 000 by PCA. The parameters τ and λ used in our algorithm are both 0.01.
The results by different methods are reported in Table 4. Our DPL algorithm achieves much higher
accuracy than its competitors. FDDL has the second highest accuracy; however  it is 1 666 times
slower than DPL in training and 83 317 times slower than DPL in testing.

Table 4: Recognition accuracy(%) & running time(s) on the UCF50 action database

Methods

Accuracy

Training time

Testing time

NSC
SVM
CRC
SRC

DLSI(C)
FDDL

LC-KSVD

DPL

51.8
57.9
60.3
59.6
60.0
61.1
53.6
62.9

no need

59.8

no need
no need
397 000
415 000
9 272.4
249.0

6.11e-2
5.02e-4
6.76e-2

8.92
10.11
89.15
0.12

1.07e-3

4 Conclusion

We proposed a novel projective dictionary pair learning (DPL) model for pattern classiﬁcation tasks.
Different from conventional dictionary learning (DL) methods  which learn a single synthesis dictio-
nary  DPL learns jointly a synthesis dictionary and an analysis dictionary. Such a pair of dictionaries
work together to perform representation and discrimination simultaneously. Compared with previ-
ous DL methods  DPL employs projective coding  which largely reduces the computational burden
in learning and testing. Performance evaluation was conducted on publically accessible visual clas-
siﬁcation datasets. DPL exhibits highly competitive classiﬁcation accuracy with state-of-the-art DL
methods  while it shows signiﬁcantly higher efﬁciency  e.g.  hundreds to thousands times faster than
LC-KSVD and FDDL in training and testing.

8

References
[1] Aharon  M.  Elad  M.  Bruckstein  A.: K-svd: An algorithm for designing overcomplete dictionaries for

sparse representation. IEEE Trans. on Signal Processing  54(11) (2006) 4311–4322

[2] Wright  J.  Yang  A.Y.  Ganesh  A.  Sastry  S.S.  Ma  Y.: Robust face recognition via sparse representation.

IEEE Transactions on Pattern Analysis and Machine Intelligence 31(2) (2009) 210–227

[3] Rubinstein  R.  Bruckstein  A.M.  Elad  M.: Dictionaries for sparse representation modeling. Proceedings

of the IEEE 98(6) (2010) 1045–1057

[4] Mairal  J.  Bach  F.  Ponce  J.: Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intelli-

gence 34(4) (2012) 791–804

[5] Jiang  Z.  Lin  Z.  Davis  L.: Label consistent k-svd: learning a discriminative dictionary for recognition.

IEEE Trans. on Pattern Anal. Mach. Intelligence 35(11) (2013) 2651–2664

[6] Elad  M.  Aharon  M.: Image denoising via sparse and redundant representations over learned dictionar-

ies. IEEE Transactions on Image Processing 15(12) (2006) 3736–3745

[7] Mairal  J.  Bach  F.  Ponce  J.  Sapiro  G.  Zisserman  A.  et al.: Supervised dictionary learning. In: NIPS.

(2008)

[8] Ramirez  I.  Sprechmann  P.  Sapiro  G.: Classiﬁcation and clustering via dictionary learning with struc-

tured incoherence and shared features. In: CVPR. (2010)

[9] Yang  M.  Zhang  L.    Feng  X.  Zhang  D.: Fisher discrimination dictionary learning for sparse repre-

sentation. In: ICCV. (2011)

[10] Wang  Z.  Yang  J.  Nasrabadi  N.  Huang  T.: A max-margin perspective on sparse representation-based

classiﬁcation. In: ICCV. (2013)

[11] Lee  H.  Battle  A.  Raina  R.  Ng  A.Y.: Efﬁcient sparse coding algorithms. In: NIPS. (2007)
[12] Hale  E.T.  Yin  W.  Zhang  Y.: Fixed-point continuation for (cid:96)1-minimization: Methodology and conver-

gence. SIAM Journal on Optimization 19(3) (2008) 1107–1130

[13] Gregor  K.  LeCun  Y.: Learning fast approximations of sparse coding. In: ICML. (2010)
[14] Ranzato  M.  Poultney  C.  Chopra  S.  Cun  Y.L.: Efﬁcient learning of sparse representations with an

energy-based model. In: NIPS. (2006)

[15] Yunjin  C.  Thomas  P.  Bischof  H.: Learning l1-based analysis and synthesis sparsity priors using bi-

level optimization. NIPS workshop (2012)

[16] Elad  M.  Milanfar  P.  Rubinstein  R.: Analysis versus synthesis in signal priors. Inverse problems 23(3)

(2007) 947

[17] Sprechmann  P.  Litman  R.  Yakar  T.B.  Bronstein  A.  Sapiro  G.: Efﬁcient supervised sparse analysis

and synthesis operators. In: NIPS. (2013)

[18] Feng  Z.  Yang  M.  Zhang  L.  Liu  Y.  Zhang  D.:

Joint discriminative dimensionality reduction and

dictionary learning for face recognition. Pattern Recognition 46(8) (2013) 2134–2143

[19] Soltanolkotabi  M.  Elhamifar  E.  Candes  E.:

iv:1301.2603 (2013)

Robust subspace clustering.

arXiv preprint arX-

[20] Coates  A.  Ng  A.Y.: The importance of encoding versus training with sparse coding and vector quanti-

zation. In: ICML. (2011)

[21] Zhang  L.  Yang  M.  Feng  X.: Sparse representation or collaborative representation: Which helps face

recognition? In: ICCV. (2011)

[22] Georghiades  A.  Belhumeur  P.  Kriegman  D.: From few to many: Illumination cone models for face
recognition under variable lighting and pose. IEEE Trans. Patt. Anal. Mach. Intel. 23(6) (2001) 643–660
[23] Gorski  J.  Pfeuffer  F.  Klamroth  K.: Biconvex sets and optimization with biconvex functions: a survey

and extensions. Mathematical Methods of Operations Research 66(3) (2007) 373–407

[24] Martinez  A.  Benavente.  R.: The ar face database. CVC Technical Report (1998)
[25] Fei-Fei  L.  Fergus  R.  Perona  P.: Learning generative visual models from few training examples: An in-
cremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding
106(1) (2007) 59–70

[26] Reddy  K.K.  Shah  M.: Recognizing 50 human action categories of web videos. Machine Vision and

Applications 24(5) (2013) 971–981

[27] Lazebnik  S.  Schmid  C.  Ponce  J.: Beyond bags of features: Spatial pyramid matching for recognizing

natural scene categories. In: CVPR. (2006)

[28] Sadanand  S.  Corso  J.J.: Action bank: A high-level representation of activity in video.

(2012)

In: CVPR.

9

,Raif Rustamov
Leonidas Guibas
Shuhang Gu
Lei Zhang
Xiangchu Feng
Peter Flach
Meelis Kull
Arsenii Vanunts
Alexey Drutsa