2016,Measuring Neural Net Robustness with Constraints,Despite having high accuracy  neural nets have been shown to be susceptible to adversarial examples  where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore  we show how existing approaches to improving robustness “overfit” to adversarial examples generated using a specific algorithm. Finally  we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose  but also according to previously proposed metrics.,Measuring Neural Net Robustness with Constraints

Osbert Bastani
Stanford University

obastani@cs.stanford.edu

Dimitrios Vytiniotis
Microsoft Research

dimitris@microsoft.com

Yani Ioannou

University of Cambridge

yai20@cam.ac.uk

Aditya V. Nori

Microsoft Research

adityan@microsoft.com

Leonidas Lampropoulos
University of Pennsylvania
llamp@seas.upenn.edu

Antonio Criminisi
Microsoft Research

antcrim@microsoft.com

Abstract

Despite having high accuracy  neural nets have been shown to be susceptible to
adversarial examples  where a small perturbation to an input can cause it to become
mislabeled. We propose metrics for measuring the robustness of a neural net and
devise a novel algorithm for approximating these metrics based on an encoding of
robustness as a linear program. We show how our metrics can be used to evaluate
the robustness of deep neural nets with experiments on the MNIST and CIFAR-10
datasets. Our algorithm generates more informative estimates of robustness metrics
compared to estimates based on existing algorithms. Furthermore  we show how
existing approaches to improving robustness “overﬁt” to adversarial examples
generated using a speciﬁc algorithm. Finally  we show that our techniques can be
used to additionally improve neural net robustness both according to the metrics
that we propose  but also according to previously proposed metrics.

1

Introduction

Recent work [21] shows that it is often possible to construct an input mislabeled by a neural net
by perturbing a correctly labeled input by a tiny amount in a carefully chosen direction. Lack of
robustness can be problematic in a variety of settings  such as changing camera lens or lighting
conditions  successive frames in a video  or adversarial attacks in security-critical applications [18].
A number of approaches have since been proposed to improve robustness [6  5  1  7  20]. However 
work in this direction has been handicapped by the lack of objective measures of robustness. A typical
approach to improving the robustness of a neural net f is to use an algorithm A to ﬁnd adversarial
examples  augment the training set with these examples  and train a new neural net f(cid:48) [5]. Robustness
is then evaluated by using the same algorithm A to ﬁnd adversarial examples for f(cid:48)—if A discovers
fewer adversarial examples for f(cid:48) than for f  then f(cid:48) is concluded to be more robust than f. However 
f(cid:48) may have overﬁt to adversarial examples generated by A—in particular  a different algorithm A(cid:48)
may ﬁnd as many adversarial examples for f(cid:48) as for f. Having an objective robustness measure is
vital not only to reliably compare different algorithms  but also to understand robustness of production
neural nets—e.g.  when deploying a login system based on face recognition  a security team may
need to evaluate the risk of an attack using adversarial examples.
In this paper  we study the problem of measuring robustness. We propose to use two statistics of
the robustness ρ(f  x∗) of f at point x∗ (i.e.  the L∞ distance from x∗ to the nearest adversarial
example) [21]. The ﬁrst one measures the frequency with which adversarial examples occur; the
other measures the severity of such adversarial examples. Both statistics depend on a parameter  
which intuitively speciﬁes the threshold below which adversarial examples should not exist (i.e. 
points x with L∞ distance to x∗ less than  should be assigned the same label as x∗).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

The key challenge is efﬁciently computing ρ(f  x∗). We give an exact formulation of this problem
as an intractable optimization problem. To recover tractability  we approximate this optimization
problem by constraining the search to a convex region Z(x∗) around x∗. Furthermore  we devise
an iterative approach to solving the resulting linear program that produces an order of magnitude
speed-up. Common neural nets (speciﬁcally  those using rectiﬁed linear units as activation functions)
are in fact piecewise linear functions [15]; we choose Z(x∗) to be the region around x∗ on which
f is linear. Since the linear nature of neural nets is often the cause of adversarial examples [5]  our
choice of Z(x∗) focuses the search where adversarial examples are most likely to exist.
We evaluate our approach on a deep convolutional neural network f for MNIST. We estimate ρ(f  x∗)
using both our algorithm ALP and (as a baseline) the algorithm AL-BFGS introduced by [21]. We show
that ALP produces a substantially more accurate estimate of ρ(f  x∗) than AL-BFGS. We then use data
augmentation with each algorithm to improve the robustness of f  resulting in ﬁne-tuned neural nets
fLP and fL-BFGS. According to AL-BFGS  fL-BFGS is more robust than f  but not according to ALP. In
other words  fL-BFGS overﬁts to adversarial examples computed using AL-BFGS. In contrast  fLP is
more robust according to both AL-BFGS and ALP. Furthermore  to demonstrate scalability  we apply
our approach to evaluate the robustness of the 23-layer network-in-network (NiN) neural net [13] for
CIFAR-10  and reveal a surprising lack of robustness. We ﬁne-tune NiN and show that robustness
improves  albeit only by a small amount. In summary  our contributions are:

propose two statistics for measuring robustness based on this notion (§2).

• We formalize the notion of pointwise robustness studied in previous work [5  21  6] and
• We show how computing pointwise robustness can be encoded as a constraint system
(§3). We approximate this constraint system with a tractable linear program and devise an
optimization for solving this linear program an order of magnitude faster (§4).
• We demonstrate experimentally that our algorithm produces substantially more accurate
measures of robustness compared to algorithms based on previous work  and show evidence
that neural nets ﬁne-tuned to improve robustness (§5) can overﬁt to adversarial examples
identiﬁed by a speciﬁc algorithm (§6).

1.1 Related work

The susceptibility of neural nets to adversarial examples was discovered by [21]. Given a test point
x∗ with predicted label (cid:96)∗  an adversarial example is an input x∗ + r with predicted label (cid:96) (cid:54)= (cid:96)∗
where the adversarial perturbation r is small (in L∞ norm). Then  [21] devises an approximate
algorithm for ﬁnding the smallest possible adversarial perturbation r. Their approach is to minimize
the combined objective loss(f (x∗ + r)  (cid:96)) + c(cid:107)r(cid:107)∞  which is an instance of box-constrained convex
optimization that can be solved using L-BFGS-B. The constant c is optimized using line search.
Our formalization of the robustness ρ(f  x∗) of f at x∗ corresponds to the notion in [21] of ﬁnding the
minimal (cid:107)r(cid:107)∞. We propose an exact algorithm for computing ρ(f  x∗) as well as a tractable approxi-
mation. The algorithm in [21] can also be used to approximate ρ(f  x∗); we show experimentally
that our algorithm is substantially more accurate than [21].
There has been a range of subsequent work studying robustness;
[17] devises an algorithm for
ﬁnding purely synthetic adversarial examples (i.e.  no initial image x∗)  [22] searches for adversarial
examples using random perturbations  showing that adversarial examples in fact exist in large regions
of the pixel space  [19] shows that even intermediate layers of neural nets are not robust to adversarial
noise  and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.
Starting with [5]  a major focus has been on devising faster algorithms for ﬁnding adversarial
examples. Their idea is that adversarial examples can then be computed on-the-ﬂy and used as
training examples  analogous to data augmentation approaches typically used to train neural nets [10].
To ﬁnd adversarial examples quickly  [5] chooses the adversarial perturbation r to be in the direction
of the signed gradient of loss(f (x∗ + r)  (cid:96)) with ﬁxed magnitude. Intuitively  given only the gradient
of the loss function  this choice of r is most likely to produce an adversarial example with (cid:107)r(cid:107)∞ ≤ .
In this direction  [16] improves upon [5] by taking multiple gradient steps  [7] extends this idea to
norms beyond the L∞ norm  [6] takes the approach of [21] but ﬁxes c  and [20] formalizes [5] as
robust optimization.
A key shortcoming of these lines of work is that robustness is typically measured using the same
algorithm used to ﬁnd adversarial examples  in which case the resulting neural net may have overﬁt

2

to adversarial examples generating using that algorithm. For example  [5] shows improved accuracy
to adversarial examples generated using their own signed gradient method  but do not consider
whether robustness increases for adversarial examples generated using more precise approaches such
as [21]. Similarly  [7] compares accuracy to adversarial examples generated using both itself and [5]
(but not [21])  and [20] only considers accuracy on adversarial examples generated using their own
approach on the baseline network. The aim of our paper is to provide metrics for evaluating robustness 
and to demonstrate the importance of using such impartial measures to compare robustness.
Additionally  there has been work on designing neural network architectures [6] and learning proce-
dures [1] that improve robustness to adversarial perturbations  though they do not obtain state-of-the-
art accuracy on the unperturbed test sets. There has also been work using smoothness regularization
related to [5] to train neural nets  focusing on improving accuracy rather than robustness [14].
Robustness has also been studied in more general contexts; [23] studies the connection between
robustness and generalization  [2] establishes theoretical lower bounds on the robustness of linear and
quadratic classiﬁers  and [4] seeks to improve robustness by promoting resiliance to deleting features
during training. More broadly  robustness has been identiﬁed as a desirable property of classiﬁers
beyond prediction accuracy. Traditional metrics such as (out-of-sample) accuracy  precision  and
recall help users assess prediction accuracy of trained models; our work aims to develop analogous
metrics for assessing robustness.

2 Robustness Metrics
Consider a classiﬁer f : X → L  where X ⊆ Rn is the input space and L = {1  ...  L} are the labels.
We assume that training and test points x ∈ X have distribution D. We ﬁrst formalize the notion
of robustness at a point  and then describe two statistics to measure robustness. Our two statistics
depend on a parameter   which captures the idea that we only care about robustness below a certain
threshold—we disregard adversarial examples x whose L∞ distance to x∗ is greater than . We use
 = 20 in our experiments on MNIST and CIFAR-10 (on the pixel scale 0-255).
Pointwise robustness. Intuitively  f is robust at x∗ ∈ X if a “small” perturbation to x∗ does not
affect the assigned label. We are interested in perturbations sufﬁciently small that they do not affect
human classiﬁcation; an established condition is (cid:107)x − x∗(cid:107)∞ ≤  for some parameter . Formally  we
say f is (x∗  )-robust if for every x such that (cid:107)x − x∗(cid:107)∞ ≤   f (x) = f (x∗). Finally  the pointwise
robustness ρ(f  x∗) of f at x∗ is the minimum  for which f fails to be (x∗  )-robust:

ρ(f  x∗)

def

= inf{ ≥ 0 | f is not (x∗  )-robust}.

(1)

This deﬁnition formalizes the notion of robustness in [5  6  21].

Adversarial frequency. Given a parameter   the adversarial frequency

φ(f  )

def

= Prx∗∼D[ρ(f  x∗) ≤ ]

measures how often f fails to be (x∗  )-robust. In other words  if f has high adversarial frequency 
then it fails to be (x∗  )-robust for many inputs x∗.

Adversarial severity. Given a parameter   the adversarial severity
= Ex∗∼D[ρ(f  x∗) | ρ(f  x∗) ≤ ]

µ(f  )

def

measures the severity with which f fails to be robust at x∗ conditioned on f not being (x∗  )-robust.
We condition on pointwise robustness since once f is (x∗  )-robust at x∗  then the degree to which f
is robust at x∗ does not matter. Smaller µ(f  ) corresponds to worse adversarial severity  since f is
more susceptible to adversarial examples if the distances to the nearest adversarial example are small.
The frequency and severity capture different robustness behaviors. A neural net may have high
adversarial frequency but low adversarial severity  indicating that most adversarial examples are
about  distance away from the original point x∗. Conversely  a neural net may have low adversarial
frequency but high adversarial severity  indicating that it is typically robust  but occasionally severely
fails to be robust. Frequency is typically the more important metric  since a neural net with low
adversarial frequency is robust most of the time. Indeed  adversarial frequency corresponds to the

3

(a)

(b)

Figure 1: Neural net with a single hidden layer and
ReLU activations trained on dataset with binary labels.
(a) The training data and loss surface. (b) The linear
region corresponding to the red training point.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 2: For MNIST  (a) an im-
age classiﬁed 1  (b) its adversar-
ial example classifed 3  and (c)
the (scaled) adversarial perturba-
tion. For CIFAR-10  (d) an im-
age classiﬁed as “automobile”  (e)
its adversarial example classiﬁed as
“truck”  and (f) the (scaled) adver-
sarial perturbation.

accuracy on adversarial examples used to measure robustness in [5  20]. Severity can be used to
differentiate between neural nets with similar adversarial frequency.
Given a set of samples X ⊆ X drawn i.i.d. from D  we can estimate φ(f  ) and µ(f  ) using the
following standard estimators  assuming we can compute ρ:

|{x∗ ∈ X | ρ(f  x∗) ≤ }|

|X|

ˆφ(f    X)

def
=

ˆµ(f    X)

def
=

(cid:80)

x∗∈X ρ(f  x∗)I[ρ(f  x∗) ≤ ]
|{x∗ ∈ X | ρ(f  x∗) ≤ }|

.

An approximation ˆρ(f  x∗) ≈ ρ(f  x∗) of ρ  such as the one we describe in Section 4  can be used in
place of ρ. In practice  X is taken to be the test set Xtest.

3 Computing Pointwise Robustness

3.1 Overview

Consider the training points in Figure 1 (a) colored based on the ground truth label. To classify this
data  we train a two-layer neural net f (x) = arg max(cid:96){(W2g(W1x))(cid:96)}  where the ReLU function g
is applied pointwise. Figure 1 (a) includes contours of the per-point loss function of this neural net.
Exhaustively searching the input space to determine the distance ρ(f  x∗) to the nearest adversarial
example for input x∗ (labeled (cid:96)∗) is intractable. Recall that neural nets with rectiﬁed-linear (ReLU)
units as activations are piecewise linear [15]. Since adversarial examples exist because of this
linearity in the neural net [5]  we restrict our search to the region Z(x∗) around x∗ on which the
neural net is linear. This region around x∗ is deﬁned by the activation of the ReLU function: for
each i  if (W1x∗)i ≥ 0 (resp.  (W1x∗) ≤ 0)  we constrain to the half-space {x | (W1x)i ≥ 0} (resp. 
{x | (W1x)i ≤ 0}). The intersection of these half-spaces is convex  so it admits efﬁcient search.
Figure 1 (b) shows one such convex region 1.
Additionally  x is labeled (cid:96) exactly when f (x)(cid:96) ≥ f (x)(cid:96)(cid:48) for each (cid:96)(cid:48) (cid:54)= (cid:96). These constraints are linear
since f is linear on Z(x∗). Therefore  we can ﬁnd the distance to the nearest input with label (cid:96) (cid:54)= (cid:96)∗
by minimizing (cid:107)x − x∗(cid:107)∞ on Z(x∗). Finally  we can perform this search for each label (cid:96) (cid:54)= (cid:96)∗ 
though for efﬁciency we take (cid:96) to be the label assigned the second-highest score by f. Figure 1 (b)
shows the adversarial example found by our algorithm in our running example. In Figure 1 note that
the direction of the nearest adversarial example is not necessary aligned with the signed gradient of
the loss function  as observed by others [7].

1Our neural net has 8 hidden units  but for this x∗  6 of the half-spaces entirely contain the convex region.

4

3.2 Formulation as Optimization
We compute ρ(f  ) by expressing (1) as constraints C  which consist of

b = 0)  where x ∈ Rm (for some m) are variables and w ∈ Rm  b ∈ R are constants.

• Linear relations; speciﬁcally  inequalities C ≡ (wT x + b ≥ 0) and equalities C ≡ (wT x +
• Conjunctions C ≡ C1 ∧ C2  where C1 and C2 are themselves constraints. Both constraints
• Disjunctions C ≡ C1∨C2  where C1 and C2 are themselves constraints. One of the constraints

must be satisﬁed for the conjunction to be satisﬁed.

must be satisﬁed for the disjunction to be satisﬁed.

The feasible set F(C) of C is the set of x ∈ Rm that satisfy C; C is satisﬁable if F(C) is nonempty.
In the next section  we show that the condition f (x) = (cid:96) can be expressed as constraints Cf (x  (cid:96));
i.e.  f (x) = (cid:96) if and only if Cf (x  (cid:96)) is satisﬁable. Then  ρ(f  ) can be computed as follows:

ρ(f  x∗) = min
(cid:96)(cid:54)=(cid:96)∗

ρ(f  x∗  (cid:96))

ρ(f  x∗  (cid:96))

def

= inf{ ≥ 0 | Cf (x  (cid:96)) ∧ (cid:107)x − x∗(cid:107)∞ ≤  satisﬁable}.

(2)

(3)

The optimization problem is typically intractable; we describe a tractable approximation in §4.

(cid:9)  where the ith layer of

assume f has form f (x) = arg max(cid:96)∈L(cid:8)(cid:2)f (k)(f (k−1)(...(f (1)(x))...))(cid:3)

3.3 Encoding a Neural Network
We show how to encode the constraint f (x) = (cid:96) as constraints Cf (x  (cid:96)) when f is a neural net. We
the network is a function f (i) : Rni−1 → Rni  with n0 = n and nk = |L|. We describe the encoding
of fully-connected and ReLU layers; convolutional layers are encoded similarly to fully-connected
layers and max-pooling layers are encoded similarly to ReLU layers. We introduce the variables
x(0)  . . .   x(k) into our constraints  with the interpretation that x(i) represents the output vector of
layer i of the network; i.e.  x(i) = f (i)(x(i−1)). The constraint Cin(x) ≡ (x(0) = x) encodes the
input layer. For each layer f (i)  we encode the computation of x(i) given x(i−1) as a constraint Ci.
In this case  x(i) = f (i)(x(i−1)) = W (i)x(i−1) + b(i)  which we encode
is the j-th row of W (i).

Fully-connected layer.

  where W (i)

j x(i−1) + b(i)

x(i)
j = W (i)

(cid:111)

(cid:110)

(cid:96)

j

j

j=1

using the constraints Ci ≡(cid:86)ni
the constraints Ci ≡(cid:86)ni
Finally  the constraints Cout((cid:96)) ≡(cid:86)
the constraints Cf (x  (cid:96)) ≡ Cin(x) ∧(cid:16)(cid:86)k

In this case  x(i)

ReLU layer.

(cid:96)(cid:48)(cid:54)=(cid:96)

j = max {x(i−1)

j

j=1 Cij  where Cij = (x(i−1)

j

<0 ∧ x(i)

  0} (for each 1 ≤ j ≤ ni)  which we encode using
).

j =0) ∨ (x(i−1)
ensure that the output label is (cid:96). Together 

(cid:17) ∧ Cout((cid:96)) encodes the computation of f:

(cid:110)
(cid:96) ≥ x(k)
x(k)
(cid:96)(cid:48)
i=1 Ci

j =x(i−1)

≥ 0 ∧ x(i)

(cid:111)

j

j

Theorem 1 For any x ∈ X and (cid:96) ∈ L  we have f (x) = (cid:96) if and only if Cf (x  (cid:96)) is satisﬁable.

4 Approximate Computation of Pointwise Robustness
Convex restriction. The challenge to solving (3) is the non-convexity of the feasible set of Cf (x  (cid:96)).
To recover tractability  we approximate (3) by constraining the feasible set to x ∈ Z(x∗)  where
Z(x∗) ⊆ X is carefully chosen so that the constraints ˆCf (x  (cid:96)) ≡ Cf (x  (cid:96)) ∧ (x ∈ Z(x∗)) have
convex feasible set. We call ˆCf (x  (cid:96)) the convex restriction of Cf (x  (cid:96)). In some sense  convex
restriction is the opposite of convex relaxation. Then  we can approximately compute robustness:

ˆρ(f  x∗  (cid:96))

def

= inf{ ≥ 0 | ˆCf (x  (cid:96)) ∧ (cid:107)x − x∗(cid:107)∞ ≤  satisﬁable}.

(4)

The objective is optimized over x ∈ Z(x∗)  which approximates the optimum over x ∈ X .

5

Choice of Z(x∗). We construct Z(x∗) as the feasible set of constraints D(x∗); i.e.  Z(x∗) =
F(D(x∗)). We now describe how to construct D(x∗).
Note that F(wT x + b = 0) and F(wT x + b ≥ 0) are convex sets. Furthermore  if F(C1) and F(C2)
are convex  then so is their conjunction F(C1 ∧ C2). However  their disjunction F(C1 ∨ C2) may not
be convex; for example  F((x ≥ 0) ∨ (y ≥ 0)). The potential non-convexity of disjunctions makes
(3) difﬁcult to optimize.
We can eliminate disjunction operations by choosing one of the two disjuncts to hold. For example 
note that for C1 ≡ C2 ∨ C3  we have both F(C2) ⊆ F(C1) and F(C3) ⊆ F(C1). In other words  if we
replace C1 with either C2 or C3  the feasible set of the resulting constraints can only become smaller.
Taking D(x∗) ≡ C2 (resp.  D(x∗) ≡ C3) effectively replaces C1 with C2 (resp.  C3).
To restrict (3)  for every disjunction C1 ≡ C2 ∨C3  we systematically choose either C2 or C3 to replace
the constraint C1. In particular  we choose C2 if x∗ satisﬁes C2 (i.e.  x∗ ∈ F(C2)) and choose C3
otherwise. In our constraints  disjunctions are always mutually exclusive  so x∗ never simultaneously
satisﬁes both C2 and C3. We then take D(x∗) to be the conjunction of all our choices. The resulting
constraints ˆCf (x  (cid:96)) contains only conjunctions of linear relations  so its feasible set is convex. In
fact  it can be expressed as a linear program (LP) and can be solved using any standard LP solver.
For example  consider a rectiﬁed linear layer (as before  max pooling layers are similar). The original
constraint added for unit j of rectiﬁed linear layer f (i) is
x(i−1)

(cid:17) ∨(cid:16)

≤ 0 ∧ x(i)

≥ 0 ∧ x(i)

x(i−1)

(cid:16)

(cid:17)

j = x(i−1)

j

j = 0

j

j

To restrict this constraint  we evaluate the neural network on the seed input x∗ and look at the input
to f (i)  which equals x(i−1)

∗

(cid:40)

= f (i−1)(...(f (1)(x∗))...). Then  for each 1 ≤ j ≤ ni:
)j ≤ 0
)j > 0.

j = x(i−1)
j = 0

≤ 0 ∧ x(i)
≥ 0 ∧ x(i)

if (x(i−1)
if (x(i−1)

x(i−1)
x(i−1)

∗
∗

j

j

j

D(x∗) ← D(x∗) ∧

Iterative constraint solving. We implement an optimization for solving LPs by lazily adding
constraints as necessary. Given all constraints C  we start off solving the LP with the subset of
equality constraints ˆC ⊆ C  which yields a (possibly infeasible) solution z. If z is feasible  then z is
also an optimal solution to the original LP; otherwise  we add to ˆC the constraints in C that are not
satisﬁed by z and repeat the process. This process always yields the correct solution  since in the
worst case ˆC becomes equal to C. In practice  this optimization is an order of magnitude faster than
directly solving the LP with constraints C.
Single target label. For simplicity  rather than minimize over ρ(f  x∗  (cid:96)) for each (cid:96) (cid:54)= (cid:96)∗  we ﬁx (cid:96)
to be the second most probable label ˜f (x∗); i.e. 

ˆρ(f  x∗)

def

= inf{ ≥ 0 | ˆCf (x  ˜f (x∗)) ∧ (cid:107)x − x∗(cid:107)∞ ≤  satisﬁable}.

(5)

Approximate robustness statistics. We can use ˆρ in our statistics ˆφ and ˆµ deﬁned in §2. Because
ˆρ is an overapproximation of ρ (i.e.  ˆρ(f  x∗) ≥ ρ(f  x∗))  the estimates ˆφ and ˆµ may not be unbiased
(in particular  ˆφ(f  ) ≤ φ(f  )). In §6  we show empirically that our algorithm produces substantially
less biased estimates than existing algorithms for ﬁnding adversarial examples.

5

Improving Neural Net Robustness

Finding adversarial examples. We can use our algorithm for estimating ˆρ(f  x∗) to compute
adversarial examples. Given x∗  the value of x computed by the optimization procedure used to solve
(5) is an adversarial example for x∗ with (cid:107)x − x∗(cid:107)∞ = ˆρ(f  x∗).
Finetuning. We use ﬁne-tuning to reduce a neural net’s susceptability to adversarial examples.
First  we use an algorithm A to compute adversarial examples for each x∗ ∈ Xtrain and add them to
the training set. Then  we continue training the network on a the augmented training set at a reduced
training rate. We can repeat this process multiple rounds (denoted T ); at each round  we only consider
x∗ in the original training set (rather than the augmented training set).

6

Neural Net

LeNet (Original)
Baseline (T = 1)
Baseline (T = 2)
Our Algo. (T = 1)
Our Algo. (T = 2)

Accuracy (%) Adversarial Frequency (%) Adversarial Severity (pixels)
Our Algo.
12.4
12.3
12.4
12.2
11.7

Our Algo.
7.15
6.89
6.97
5.40
5.03

Baseline
11.9
11.0
10.9
12.8
12.2

Baseline
1.32
1.02
0.99
1.18
1.12

99.08
99.14
99.15
99.17
99.23

Table 1: Evaluation of ﬁne-tuned networks. Our method discovers more adversarial examples than
the baseline [21] for each neural net  hence producing better estimates. LeNet ﬁne-tuned for T = 1  2
rounds (bottom four rows) exhibit a notable increase in robustness compared to the original LeNet.

(a)

(b)

(c)

Figure 3: The cumulative number of test points x∗ such that ρ(f  x∗) ≤  as a function of . In (a)
and (b)  the neural nets are the original LeNet (black)  LeNet ﬁne-tuned with the baseline and T = 2
(red)  and LeNet ﬁne-tuned with our algorithm and T = 2 (blue); in (a)  ˆρ is measured using the
baseline  and in (b)  ˆρ is measured using our algorithm. In (c)  the neural nets are the original NiN
(black) and NiN ﬁnetuned with our algorithm  and ˆρ is estimated using our algorithm.

(cid:96) ≥ x(k)

Rounding errors. MNIST images are represented as integers  so we must round the perturbation
to obtain an image  which oftentimes results in non-adversarial examples. When ﬁne-tuning  we add
(cid:96)(cid:48) + α for all (cid:96)(cid:48) (cid:54)= (cid:96)  which eliminates this problem by ensuring that the neural
a constraint x(k)
net has high conﬁdence on its adversarial examples. In our experiments  we ﬁx α = 3.0.
Similarly  we modiﬁed the L-BFGS-B baseline so that during the line search over c  we only count
(cid:96)(cid:48) + α for all (cid:96)(cid:48) (cid:54)= (cid:96). We choose α = 0.15  since larger α causes the
x∗ + r as adversarial if x(k)
baseline to ﬁnd signiﬁcantly fewer adversarial examples  and small α results in smaller improvement
in robustness. With this choice  rounding errors occur on 8.3% of the adversarial examples we ﬁnd
on the MNIST training set.

(cid:96) ≥ x(k)

6 Experiments

6.1 Adversarial Images for CIFAR-10 and MNIST

We ﬁnd adversarial examples for the neural net LeNet [12] (modiﬁed to use ReLUs instead of
sigmoids) trained to classify MNIST [11]  and for the network-in-network (NiN) neural net [13]
trained to classify CIFAR-10 [9]. Both neural nets are trained using Caffe [8]. For MNIST  Figure 2
(b) shows an adversarial example (labeled 1) we ﬁnd for the image in Figure 2 (a) labeled 3  and
Figure 2 (c) shows the corresponding adversarial perturbation scaled so the difference is visible (it
has L∞ norm 17). For CIFAR-10  Figure 2 (e) shows an adversarial example labeled “truck” for
the image in Figure 2 (d) labeled “automobile”  and Figure 2 (f) shows the corresponding scaled
adversarial perturbation (which has L∞ norm 3).

6.2 Comparison to Other Algorithms on MNIST

We compare our algorithm for estimating ρ to the baseline L-BFGS-B algorithm proposed by [21].
We use the tool provided by [22] to compute this baseline. For both algorithms  we use adversarial
target label (cid:96) = ˜f (x∗). We use LeNet in our comparisons  since we ﬁnd that it is substantially more
robust than the neural nets considered in most previous work (including [21]). We also use versions

7

of LeNet ﬁne-tuned using both our algorithm and the baseline with T = 1  2. To focus on the most
severe adversarial examples  we use a stricter threshold for robustness of  = 20 pixels.
We performed a similar comparison to the signed gradient algorithm proposed by [5] (with the signed
gradient multiplied by  = 20 pixels). For LeNet  this algorithm found only one adversarial example
on the MNIST test set (out of 10 000) and four adversarial examples on the MNIST training set (out
of 60 000)  so we omit results 2.

In Figure 3  we plot the number of test points x∗ for which ˆρ(f  x∗) ≤   as a function
Results.
of   where ˆρ(f  x∗) is estimated using (a) the baseline and (b) our algorithm. These plots compare
the robustness of each neural network as a function of . In Table 1  we show results evaluating the
robustness of each neural net  including the adversarial frequency and the adversarial severity. The
running time of our algorithm and the baseline algorithm are very similar; in both cases  computing
ˆρ(f  x∗) for a single input x∗ takes about 1.5 seconds. For comparison  without our iterative constraint
solving optimization  our algorithm took more than two minutes to run.

Discussion. For every neural net  our algorithm produces substantially higher estimates of the
adversarial frequency. In other words  our algorithm estimates ˆρ(f  x∗) with substantially better
accuracy compared to the baseline.
According to the baseline metrics shown in Figure 3 (a)  the baseline neural net (red) is similarly
robust to our neural net (blue)  and both are more robust than the original LeNet (black). Our neural
net is actually more robust than the baseline neural net for smaller values of   whereas the baseline
neural net eventually becomes slightly more robust (i.e.  where the red line dips below the blue line).
This behavior is captured by our robustness statistics—the baseline neural net has lower adversarial
frequency (so it has fewer adversarial examples with ˆρ(f  x∗) ≤ ) but also has worse adversarial
severity (since its adversarial examples are on average closer to the original points x∗).
However  according to our metrics shown in Figure 3 (b)  our neural net is substantially more robust
than the baseline neural net. Again  this is reﬂected by our statistics—our neural net has substantially
lower adversarial frequency compared to the baseline neural net  while maintaining similar adversarial
severity. Taken together  our results suggest that the baseline neural net is overﬁtting to the adversarial
examples found by the baseline algorithm. In particular  the baseline neural net does not learn the
adversarial examples found by our algorithm. On the other hand  our neural net learns both the
adversarial examples found by our algorithm and those found by the baseline algorithm.

6.3 Scaling to CIFAR-10

We also implemented our approach for the for the CIFAR-10 network-in-network (NiN) neural
net [13]  which obtains 91.31% test set accuracy. Computing ˆρ(f  x∗) for a single input on NiN
takes about 10-15 seconds on an 8-core CPU. Unlike LeNet  NiN suffers severely from adversarial
examples—we measure a 61.5% adversarial frequency and an adversarial severity of 2.82 pixels. Our
neural net (NiN ﬁne-tuned using our algorithm and T = 1) has test set accuracy 90.35%  which is
similar to the test set accuracy of the original NiN. As can be seen in Figure 3 (c)  our neural net
improves slightly in terms of robustness  especially for smaller . As before  these improvements are
reﬂected in our metrics—the adversarial frequency of our neural net drops slightly to 59.6%  and
the adversarial severity improves to 3.88. Nevertheless  unlike LeNet  our ﬁne-tuned version of NiN
remains very prone to adversarial examples. In this case  we believe that new techniques are required
to signiﬁcantly improve robustness.

7 Conclusion

We have shown how to formulate  efﬁciently estimate  and improve the robustness of neural nets
using an encoding of the robustness property as a constraint system. Future work includes devising
better approaches to improving robustness on large neural nets such as NiN and studying properties
beyond robustness.

2Futhermore  the signed gradient algorithm cannot be used to estimate adversarial severity since all the

adversarial examples it ﬁnds have L∞ norm .

8

References
[1] K. Chalupka  P. Perona  and F. Eberhardt. Visual causal feature learning. 2015.

[2] A. Fawzi  O. Fawzi  and P. Frossard. Analysis of classifers’ robustness to adversarial perturbations. ArXiv

e-prints  2015.

[3] Jiashi Feng  Tom Zahavy  Bingyi Kang  Huan Xu  and Shie Mannor. Ensemble robustness of deep learning

algorithms. arXiv preprint arXiv:1602.02389  2016.

[4] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion.
Proceedings of the 23rd international conference on Machine learning  pages 353–360. ACM  2006.

In

[5] Ian J. Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. 2015.

[6] S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples. 2014.

[7] Ruitong Huang  Bing Xu  Dale Schuurmans  and Csaba Szepesvári. Learning with a strong adversary.

CoRR  abs/1511.03034  2015.

[8] Yangqing Jia  Evan Shelhamer  Jeff Donahue  Sergey Karayev  Jonathan Long  Ross Girshick  Sergio
Guadarrama  and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv
preprint arXiv:1408.5093  2014.

[9] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images  2009.

[10] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. 2012.

[11] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[12] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to
document recognition. In S. Haykin and B. Kosko  editors  Intelligent Signal Processing  pages 306–351.
IEEE Press  2001.

[13] Min Lin  Qiang Chen  and Shuicheng Yan. Network In Network. CoRR  abs/1312.4400  2013.

[14] Takeru Miyato  Shin-ichi Maeda  Masanori Koyama  Ken Nakae  and Shin Ishii. Distributional smoothing

with virtual adversarial training. stat  1050:25  2015.

[15] Guido F. Montúfar  Razvan Pascanu  KyungHyun Cho  and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in Neural Information Processing Systems 27: Annual
Conference on Neural Information Processing Systems 2014  December 8-13 2014  Montreal  Quebec 
Canada  pages 2924–2932  2014.

[16] Seyed Mohsen Moosavi Dezfooli  Alhussein Fawzi  and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  number EPFL-CONF-218057  2016.

[17] Anh Nguyen  Jason Yosinski  and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In Computer Vision and Pattern Recognition (CVPR)  2015 IEEE
Conference on  pages 427–436. IEEE  2015.

[18] Nicolas Papernot  Patrick McDaniel  Ian Goodfellow  Somesh Jha  Z Berkay Celik  and Ananthram Swami.
Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint
arXiv:1602.02697  2016.

[19] Sara Sabour  Yanshuai Cao  Fartash Faghri  and David J Fleet. Adversarial manipulation of deep represen-

tations. arXiv preprint arXiv:1511.05122  2015.

[20] Uri Shaham  Yutaro Yamada  and Sahand Negahban. Understanding adversarial training: Increasing local

stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432  2015.

[21] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfellow  and

Rob Fergus. Intriguing properties of neural networks. 2014.

[22] Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. CoRR  abs/1510.05328 

2015.

[23] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning  86(3):391–423  2012.

9

,Osbert Bastani
Yani Ioannou
Leonidas Lampropoulos
Dimitrios Vytiniotis
Aditya Nori
Antonio Criminisi
Yao Liu
Omer Gottesman
Aniruddh Raghu
Matthieu Komorowski
Aldo Faisal
Finale Doshi-Velez
Emma Brunskill