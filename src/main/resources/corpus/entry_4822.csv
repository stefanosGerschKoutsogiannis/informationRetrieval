2019,Saccader: Improving Accuracy of Hard Attention Models for Vision,Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks  their decisions are difficult to interpret. One approach that offers some level of interpretability by design is \textit{hard attention}  which uses only relevant portions of the image. However  training hard attention models with only class label supervision is challenging  and hard attention has proved difficult to scale to complex datasets. Here  we propose a novel hard attention model  which we term Saccader. 
Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines  achieving $75\%$  top-1 and $91\%$ top-5 while attending to less than one-third of the image.,Saccader: Improving Accuracy of Hard Attention

Models for Vision

Gamaleldin F. Elsayed

Google Research  Brain Team
gamaleldin@google.com

Simon Kornblith

Google Research  Brain Team

Quoc V. Le

Google Research  Brain Team

Abstract

Although deep convolutional neural networks achieve state-of-the-art performance
across nearly all image classiﬁcation tasks  their decisions are difﬁcult to interpret.
One approach that offers some level of interpretability by design is hard attention 
which uses only relevant portions of the image. However  training hard attention
models with only class label supervision is challenging  and hard attention has
proved difﬁcult to scale to complex datasets. Here  we propose a novel hard
attention model  which we term Saccader. Key to Saccader is a pretraining step that
requires only class labels and provides initial attention locations for policy gradient
optimization. Our best models narrow the gap to common ImageNet baselines 
achieving 75% top-1 and 91% top-5 while attending to less than one-third of the
image.

1

Introduction

Despite the success of convolutional neural networks (CNNs) across many computer vision tasks 
their predictions are difﬁcult to interpret. Because CNNs compute complex nonlinear functions of
their inputs  it is often unclear what aspects of the input contributed to the prediction. Although
many researchers have attempted to design methods to interpret predictions of off-the-shelf CNNs
[Zhou et al.  2016  Baehrens et al.  2010  Simonyan and Zisserman  2014  Zeiler and Fergus  2014 
Springenberg et al.  2014  Bach et al.  2015  Yosinski et al.  2015  Nguyen et al.  2016  Montavon
et al.  2017  Zintgraf et al.  2017]  it is unclear whether these explanations faithfully describe the
underlying model [Kindermans et al.  2017  Adebayo et al.  2018  Hooker et al.  2018  Rudin  2019].
Additionally  adversarial machine learning research [Szegedy et al.  2013  Goodfellow et al.  2017 
2014] has demonstrated that imperceptible modiﬁcations to inputs can change classiﬁer predictions 
underscoring the unintuitive nature of CNN-based image classiﬁers.
One interesting class of models that offers more interpretable decisions are “hard” visual attention
models. These models rely on a controller that selects relevant parts of the input to contribute to
the decision  which provides interpretability by design. These models are inspired by human vision 
where the fovea and visual system process only a limited portion of the visual scene at high resolution
[Wandell  1995]  and top-down pathways control eye movements to sequentially sample salient parts
of visual scenes [Schütz et al.  2011]. Although models with hard attention perform well on simple
datasets [Larochelle and Hinton  2010  Mnih et al.  2014  Ba et al.  2014  Gregor et al.  2015]  it has
been challenging to scale these models from small tasks to real world images [Sermanet et al.  2015].
Here  we propose a novel hard visual attention model that we name Saccader  as well as an effective
procedure to train this model. The Saccader model learns features for different patches in the image
that reﬂect the degree of relevance to the classiﬁcation task  then proposes a sequence of image
patches for classiﬁcation. Our pretraining procedure overcomes the sparse-reward problem that makes
hard attention models difﬁcult to optimize. It requires access to only class labels and provides initial
attention locations. These initial locations provide better rewards for the policy gradient learning. Our

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a)

(b)

Figure 1: Examples of visual attention policies. (a) Glimpses predicted by the Saccader model
(more examples are shown in Figure Supp.1). (b) Glimpses predicted by the DRAM model [Ba et al. 
2014  Sermanet et al.  2015].

results show that the Saccader model is highly accurate compared to other visual attention models
while remaining interpretable (Figure 1). Our best models narrow the gap to common ImageNet
baselines  achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.
We further demonstrate that occluding the image patches proposed by the Saccader model highly
impairs classiﬁcation  thus conﬁrming these patches strong relevance to the classiﬁcation task.

2 Related Work

Hard attention: Models that employ hard attention make decisions based on only a subset of
pixels in the input image  typically in the form of a series of glimpses. These models typically utilize
some sort of artiﬁcial fovea controlled by an adaptive controller that selects parts of the input to
be processed [Burt  1988  Ballard et al.  1988  Ballard  1989  Seibert and Waxman  1989  Ahmad 
1992  Olshausen et al.  1993]. Early work using backpropagation to train the controller computed the
gradient with respect to its weights by backpropagating through a separate neural network model of
the environmental dynamics [Schmidhuber and Huber  1990  1991b a]. Butko and Movellan [2008]
proposed instead to use policy gradient  learning a convolutional logistic policy to maximize long-
term information gain. Later work extended the policy gradient framework to policies parameterized
by neural networks to perform image classiﬁcation [Mnih et al.  2014  Ba et al.  2014] and image
captioning [Xu et al.  2015]. Other non-policy-gradient-based approaches include direct estimation
of the probability of correct classiﬁcation for each glimpse location [Larochelle and Hinton  2010 
Zheng et al.  2015] or differentiable attention based on adaptive downsampling [Gregor et al.  2015 
Jaderberg et al.  2015  Eslami et al.  2016].
Work employing hard attention for vision tasks has generally examined performance only on relatively
simple datasets such as MNIST and SVHN [Mnih et al.  2014  Ba et al.  2014]. Sermanet et al. [2015]
adapted the model proposed by Ba et al. [2014] to classify the more challenging Stanford Dogs
dataset  but the model achieved only a modest accuracy improvement over a non-attentive baseline 
and did not appear to derive signiﬁcant beneﬁt from glimpses beyond the ﬁrst.
Soft attention: Models with hard attention are difﬁcult to train with gradient-based optimization.
To make training more tractable  other models have resorted to soft attention [Bahdanau et al.  2014 

2

Xuetal. 2015].Typicalsoftattentionmechanismsrescalefeaturesatoneormorestagesofthenetwork.Thesoftmasksusedforrescalingoftenappeartoprovidesomeinsightintothemodel’sdecision-makingprocess[Xuetal. 2015 Dasetal. 2017] butthemodel’sﬁnaldecisionmaynonethelessrelyoninformationprovidedbyfeatureswithsmallweights[JainandWallace 2019].Softattentionispopularinmodelsusedfornaturallanguagetasks[Bahdanauetal. 2014 Luongetal. 2015 Vaswanietal. 2017] imagecaptioning[Xuetal. 2015 Youetal. 2016 Rennieetal. 2017 Luetal. 2017 Chenetal. 2017] andvisualquestionanswering[Andreasetal. 2016] butlesscommoninimageclassiﬁcation.Althoughseveralspatialsoftattentionmechanismsforimageclassiﬁcationhavebeenproposed[Wangetal. 2017 Jetleyetal. 2018 Wooetal. 2018 Linsleyetal. 2019 Fukuietal. 2019] currentstate-of-the-artmodelsdonotusethesemechanisms[Zophetal. 2018 Liuetal. 2018a Realetal. 2018 Huangetal. 2018].Thesqueeze-and-excitationblock whichwasacriticalcomponentofthemodelthatwonthe2017ImageNetChallenge[Huetal. 2018] canbeviewedasaformofsoftattention butoperatesoverfeaturechannelsratherthanspatialdimensions.Supervisedapproaches:Althoughouraiminthisworkistoperformclassiﬁcationwithonlyimage-levelclasslabels ourapproachbearssomeresemblancetotwo-stageobjectdetectionmodels.Thesemodelsoperatebygeneratingmanyregionproposalsandthenapplyingaclassiﬁcationmodeltoeachproposal[Uijlingsetal. 2013 Girshicketal. 2014 Girshick 2015 Renetal. 2015].Unlikeourwork theseapproachesuseground-truthboundingboxestotraintheclassiﬁcationmodel andmodernarchitecturesalsouseboundingboxestosupervisetheproposalgenerator[Renetal. 2015].3Methods3.1Architecturecatballclasses2048where featurescoordinate at time t512logitsatten.netwhat featuresconcat.1x1 convmixed features (F)logits1512512...avg.logits per locationrep.net1x1 conv1x1 convlogits2Saccadercellargmax(x  y)tattention policy at time tsliceFigure2:Modelarchitecture.(a)IllustrationoftheSaccadermodel.Theproposedlocationnetworkpretrainingisperformedoncomponentswithinthedashedredbox.ComponentsinyellowaredescribedindetailinFiguresSupp.4 Supp.5 andSupp.6TounderstandtheintuitionbehindtheSaccadermodelarchitecture(Figure2) imagineoneusesatrainedImageNetmodelandappliesitatdifferentlocationsofanimagetoobtainlogitsvectorsattheselocations.Toﬁndthecorrectlabel onecouldcomputeaglobalaverageofthesevectors andtoﬁndasalientlocationontheimage onereasonablechoiceisthelocationofthepatchtheelicitsthelargestresponse.Withthatintuitioninmind wedesigntheSaccadermodeltocomputesetsof2Dfeaturesfordifferentpatchesintheimage andselectoneofthesesetsoffeaturesateachtime(FigureSupp.6).Theattentionmechanismlearnstoselectthemostsalientlocationatthecurrenttime.Foreachpredictedlocation onemayextractthelogitsvectorandperformaveragingacrossdifferenttimestoperformclassprediction.Inparticular ourarchitectureconsistsofthefollowingthreecomponents1:1.Representationnetwork:ThisisaCNNthatprocessesglimpsesfromdifferentlocationsofanimage(Figure2).Torestrictthesizeofthereceptiveﬁeld(RF) onecoulddividetheimageinto1Modelcodeavailableatgithub.com/google-research/google-research/tree/master/saccader.3patches and process each separately with an ordinary CNN  but this is computationally expensive.
Here  we used the “BagNet" architecture from Brendel and Bethge [2019]  which enables us to
compute representations with restricted RFs efﬁciently in a single pass  without scanning. Similar
to Brendel and Bethge [2019]  we use a ResNet architecture where most 3 ⇥ 3 convolutions are
replaced with 1 ⇥ 1 convolutions to limit the RF of the model and strides are adjusted to obtain
a higher resolution output (Figure Supp.4). Our model has a 77 ⇥ 77 pixel RF and computes
2048-dimensional feature vectors at different locations in the image separated by only 8 pixels.
For 224 ⇥ 224 pixel images  this maps to 361 possible attention locations. Before computing the
logits  we apply a 1 ⇥ 1 convolution with ReLU activation to encode the representations into a
512-dimensional feature space (“what" features; name motivated by the visual system ventral
pathway [Goodale and Milner  1992])  and then apply another 1 ⇥ 1 convolution to produce the
1000-dimensional logits tensor for classiﬁcation. We ﬁnd that introducing this bottleneck provides
a small performance improvement over the original BagNet model; we term the modiﬁed network
BagNet-lowD.

2. Attention network: This is a CNN that operates on the 2048-dimensional feature vectors (Figure
2)  and includes 4 convolutional layers alternating between 1 ⇥ 1 convolution and 3 ⇥ 3 dilated
convolution with rate 2  each followed by batch normalization and ReLU activation (see Fig-
ure Supp.5). The 1 ⇥ 1 convolution layers reduce the dimensionality from 2048 to 1024 to 512
location features while the 3 ⇥ 3 convolutional layers widen the RF (“where" features; name
motivated by the visual system dorsal pathway [Goodale and Milner  1992]). The what and where
features are then concatenated and mixed using a linear 1 ⇥ 1 convolution to produce a compact
tensor with 512 features (F ).
3. Saccader cell: This cell takes the mixed what and where features F and produces a sequence of
location predictions. Elements in the sequence correspond to target locations (Figure Supp.6). The
cell includes a 2D state (Ct) that keeps memory of the visited locations until time t by placing
1 in the corresponding location in the cell state. We use this state to prevent the network from
returning to previously seen locations. The cell ﬁrst selects relevant spatial locations from F and
then selects feature channels based on the relevant locations:

Gt

ij =

ht
k =

dXp=1
hXi=1

ij

Fijpappd  105Ct1
wXj=1

Fijk ˜Gt
ij

exp(Gt

ij)

n=1 exp(Gt

mn)

˜Gt

ij =

˜ht
k =

Ph
m=1Pw
Pd

exp(ht
k)
p=1 exp(ht
p)

(1)

(2)

(3)

where h and w are the height and width of the output features from the representation network 
d is the dimensionality of the mixed features  and a 2 Rd is a trainable vector. We use a large
negative number multiplied by the state (i.e.  105Ct1
) to mask out previously used locations.
Next  the cell computes a weighted sum of the feature channels and performs a spatial softmax to
compute the policy:

ij

Rt

ij =

dXk=1

Fijk˜ht

k  105Ct1

ij

˜Rt

ij =

exp(Rt

ij)

n=1 exp(Rt

mn)

Ph
m=1Pw

˜R reﬂects the model’s policy over glimpse locations. At test time  the model extracts the logits at
time t from the representation network at location arg max
ij). The ﬁnal prediction is obtained
by averaging the extracted logits across all times.
In terms of complexity  the Saccader model has 35 583 913 parameters  which is 20% fewer than
the 45 610 219 parameters in the DRAM model (Table Supp.1).

( ˜Rt

i j

3.2 Training Procedure

In all our training  we divide the standard ImageNet ILSVRC 2012 training set into training and
development subsets. We trained our model on the training subset and chose our hyperparameters
based on the development subset. We follow common practice and report results on the separate
ILSVRC 2012 validation set  which we do not use for training or hyperparameter selection. The goal

4

of our training procedure is to learn a policy that predicts a sequence of visual attention locations that
is useful to the downstream task (here image classiﬁcation) in absence of location labels.
We performed a three step training procedure using only the training class labels as supervision. First 
we pretrained the representation network by optimizing the cross entropy loss computed based on
the average logits across all possible locations plus `2-regularization on the model weights. More
formally  we optimize:

J (✓) =  log0@ Qh
i=1Qw
Pc
k=1Qh

j=1 P✓(ytarget|X ij)
j=1 P✓(yk|X ij)

i=1Qw

1

hw

hw1A +

1


2

NXi=1

✓2
i

(4)

where X ij 2 R77⇥77⇥3 is the image patch at location (i  j)  ytarget is the target class  c = 1000 is the
number of classes  ✓ are the representation network parameters  and  is a hyperparameter.
Second  we use self-supervision to pretrained the location network (i.e.  attention network  1 ⇥ 1
mixing convolution and Saccader cell) to emit glimpse locations ordered by descending value of the
logits. Just as SGD biases neural network training toward solutions that generalize well  the purpose
of this pretraining is to alter the training trajectory in a way that produces a better-performing model.
Namely  we optimized the following objective:

J (⌘) =  log TYt=1

⇡✓ ⌘ (lt

target|X  Ct1)! +

⌫
2

NXi=1

⌘2
i

(5)

target is the tth sorted target location  i.e.  l1

target is the location with the smallest maximum logit. ⇡✓ ⌘ (lt

target is the location with largest maximum logit 
target|X  Ct1) is the probability
target at time t given the input image X 2 R224⇥224⇥3
target. The parameters ⌘ are the weights of the attention

where lt
and l361
the model gives for attending to location lt
and cell state Ct1  i.e. ˜Rt
ij where (i  j) = lt
network and Saccader cell. For this step  we ﬁxed T = 12.
Finally  we trained the whole model to maximize the expected reward  where the reward (r 2{ 0  1})
represents whether the model ﬁnal prediction after 6 glimpses (T = 6) is correct. In particular 
we used the REINFORCE loss [Williams  1992] for discrete policies  cross entropy loss and `2-
regularization. The parameter update is given by the gradient of the objective:

s

⌫
2

J (✓  ⌘) = 

SXs=1 log TYt=1
⇡✓ ⌘lt
s|X  Ct1
t=1 P✓(yk|X t)1/T! +
 log QT
t=1 P✓(ytarget|X t)1/T
Pc
k=1QT
  b is the average accuracy of the model computed on each
probabilities given by ⇡✓ ⌘l|X  Ct1

minibatch  and X t denotes the image patch sampled at time t. The role of adding b and the S Monte
Carlo samples is to reduce variance in our gradient estimates [Sutton et al.  2000  Mnih et al.  2014].
In each of the above steps  we trained our model for 120 epochs using Nesterov momentum of 0.9.
(See Appendix for training hyperparameters).

!! (rs  b) +
NXi=1

where we sampled S = 2 trajectories ls at each time from a categorical distribution with location

NXi=1

(6)


2

⌘2
i

✓2
i

s

4 Results

In this section  we use the Saccader model to classify the ImageNet (ILSVRC 2012) dataset. ImageNet
is an extremely large and diverse dataset that contains both coarse- and ﬁne-grained class distinction.
To achieve high accuracy  a model must not only distinguish among superclasses  but also e.g. among
the > 100 ﬁne-grained classes of dogs. We show that the Saccader model learns a policy that yields
high accuracy on ImageNet classiﬁcation task compared to other learned and engineered policies.
Moreover  we show that our pretraining for the location network helps achieve that high accuracy.
Finally  we demonstrate that the attention locations proposed by our model are highly relevant to the
classiﬁcation task.

5

(a)

(b)

Figure 3: Saccader makes accurate predictions with few glimpses.
(a) Top-1 and top-5 test
accuracy on 224 ⇥ 224 images from ImageNet as a function of the number of attention glimpses
used. Traces show different models and visual attention policies; vertical dotted line indicates the
number of glimpses used in training. Black markers show base networks with no visual attention. (b)
Fraction of the image area covered by the model glimpses. Error bars indicate ± SD computed from
training models from 5 different random initialization.

4.1 Saccader Makes Accurate Predictions on ImageNet

We trained the Saccader model on the ImageNet dataset. Our results show that  with only a few
glimpses covering a fraction of the image  our model achieves accuracy close to CNN models that
make predictions using the whole image (see Figure 3a b and 4a).
We compared the policy learned by the Saccader model to alternative policies/models: a random
policy  where visual attention locations are picked uniformly from the image; an ordered logits policy
that uses the BagNet model to pick the top K locations based on the largest class logits; policies
based on simple edge detection algorithms (Sobel mean  Sobel variance)  which pick the top K
locations based on strength of edge features computed using the per-patch mean or variance of the
Sobel operator [Kanopoulos et al.  1988] applied to the input image; and the deep recurrent attention
model (DRAM) from Sermanet et al. [2015]  Ba et al. [2014].
With small numbers of glimpses  the random policy achieves relatively poor accuracy on ImageNet
(Figure 3a  4d). With more glimpses  the accuracy slowly increases  as the policy samples more
locations and covers larger parts of the image. The random policy is able to collect features from
different parts from the image (Figure 4d)  but many of these features are not very relevant. Edge
detector-based policies (Figure 3a) also perform poorly.
The ordered logits policy starts off with accuracy much higher than a random policy  suggesting that
the patches it initially picks are meaningful to classiﬁcation. However  accuracy is still lower than the
learned Saccader model (Figure 3a and 4b)  and performance improves only slowly with additional
glimpses. The ordered logits policy is able to capture some of the features relevant to classiﬁcation 
but it is a greedy policy that produces glimpses that cluster around a few top features (i.e.  with low
image coverage; Figure 4c). The learned Saccader policy on the other hand captures more diverse
features (Figure 4a)  leading to high accuracy with only a few glimpses.
The DRAM model also performs worse than the learned Saccader policy (Figure 3a). One major
difference between the Saccader and DRAM policies is that the Saccader policy generalizes to
different times  whereas accuracy of the DRAM model does not improve when allowed more
glimpses than it was trained on. Sermanet et al. [2015] also reported this issue when using this model
to perform ﬁne-grained classiﬁcation. In fact  increasing the number of glimpses beyond the number
used for DRAM policy training leads to a drop in performance (Figure 3a) unlike the Saccader model
that generalizes to greater numbers of glimpses.

6

(b)

(c)

(d)

Norwich terrier

Norwich terrier

Norwich terrier

Norwich terrier

Great white shark

Great white shark

Great white shark

Tiger shark

Cardigan Welsh Corgi

Cardigan Welsh Corgi

Cardigan Welsh Corgi

Cardigan Welsh Corgi

(a)

r
e
i
r
r
e

t
 

i

h
c
w
r
o
N

k
r
a
h
s
 

e

t
i

h
w

 
t

a
e
r
G

i

 

l

g
r
o
C
h
s
e
W
n
a
g
d
r
a
C

 

i

Figure 4: Comparison of attention policies from different models. Attention patches for different
models. Green text indicates correct predictions; red text indicates incorrect predictions. (a) Saccader
model. (b) DRAM model. (c) Policy based on the logits order from high to low across a space
obtained from BagNet-77-lowD. (d) Random policy using BagNet-77-lowD model.

4.2 Saccader Attends to Locations Relevant to Classiﬁcation

Glimpse locations identiﬁed by the Saccader model contain features that are highly relevant to
classiﬁcation. Figure 5a shows the accuracy of the model as a function of fraction of area covered
by the glimpses. For the same image coverage  the Saccader model achieves the highest accuracy
compared to other models. This demonstrates that the superiority of the Saccader model is not due to
simple phenomena such as having larger image coverage. Our results also suggest that the pretraining
procedure is necessary to achieve this performance (see Figure Supp.3 for a comparison of Saccader
models with and without location pretraining). Furthermore  we show that attention network and
Saccader cell are crucial components of our system. Removing the Saccader cell (i.e. using the
BagNet-77-lowD ordered logits policy) yields poor results compared to the Saccader model (Figure
5a)  and ablating the attention network greatly degrades performance (Figure Supp.3). The wide
receptive ﬁeld (RF) of the attention network allows the Saccader model to better select locations
to attend to. Note that this wide RF does not impact the interpretability of the model  since the
classiﬁcation path RF is still limited to 77 ⇥ 77.
We further investigated the importance of the attended regions for classiﬁcation using an analysis
similar to that proposed by Zeiler and Fergus [2014]. We occluded the patches the model attends to
(i.e.  set the pixels to 0) and classiﬁed the resulting image using a pretrained ResNet-v2-50 model
(Figures 5b and Supp.2b). Our results show that occluding patches selected by the Saccader model
produces a larger drop in accuracy than occluding areas selected by other policies.

7

(a)

(b)

Figure 5: Using (Occluding) Saccader glimpses gives high (poor) classiﬁcation accuracy. (a)
Classiﬁcation accuracy of models as a function of area covered by glimpses. The learned Saccader
model achieves signiﬁcantly higher accuracy while attending to a relatively small area of the image.
(b) Classiﬁcation accuracy of ResNet-v2-50 model on images where attention patches are occluded.
Occluding attention patches predicted by the Saccader model signiﬁcantly impairs classiﬁcation. See
Figure Supp.2 for top-5 accuracy. Error bars indicate ± SD computed from training models from 5
different random initializations.

Figure 6: Higher capacity classiﬁcation network and high-resolution images further improve
classiﬁcation accuracy. Graphs show top-1 and top-5 accuracy on ImageNet classiﬁcation task when
NASNet is used as a classiﬁcation network (Saccader-NASNet) for ImageNet 224 and the higher
resolution ImageNet 331. Error bars indicate ± SD computed from training models from 5 different
random initializations.

4.3 Higher Classiﬁcation Network Capacity and Better Data Quality Improve Accuracy

Further

In previous sections  we used a single network to learn useful representations for both the visual
attention and classiﬁcation. This approach is efﬁcient and gives reasonably good classiﬁcation

8

accuracy. Here  we investigate if further improvements in accuracy can be attained by expanding the
capacity of the classiﬁcation network and using high-resolution images. In this section  we add a
powerful NASNet classiﬁcation network [Zoph et al.  2018] to the base Saccader model. The use
of separate models for classiﬁcation and localization is reminiscent of approaches used in object
detection [Girshick et al.  2014  Uijlings et al.  2013  Girshick  2015]  yet here we do not have access
to location labels.
We ﬁrst applied the Saccader model to 224⇥224 pixel images to determine relevant glimpse locations.
Then  we extracted the corresponding patches and used the NASNet  ﬁne-tuned to operate on these
patches  to make class predictions. Our results show that the Saccader-NASNet model is able to
increase the accuracy even more while still retaining the interpretability of the predictions (Figure 6).
We investigated whether accuracy can be improved even further by training on higher resolution
images. We applied the Saccader-NASNet model to patches extracted from 331 ⇥ 331 pixel high-
resolution images from ImageNet. We down-sized these images to 224 ⇥ 224 and fed them to the
Saccader model to identify visual attention locations. Then  we extracted the corresponding patches
from the high resolution images and fed them to the NASNet model for classiﬁcation (NASNet model
was ﬁne tuned on these patches). The accuracy was even higher than obtained with Saccader-NASNet
model on ImageNet 224; with 6 glimpses  the top-1 and top-5 accuracy were 75.03 ± 0.08% and
91.19 ± 0.22%  respectively  while processing only 29.47 ± 0.26% of the image with the NASNet.
5 Conclusion
In this work  we propose the Saccader model  a novel approach to image classiﬁcation with hard
visual attention. We design an optimization procedure that uses pretraining on an auxiliary task
with only class labels and no visual attention guidance. The Saccader model is able to achieve
good accuracy on ImageNet while only covering fraction of the image. The locations to which the
Saccader model attends are highly relevant to the downstream classiﬁcation task  and occluding them
substantially reduces classiﬁcation performance. Since ImageNet is a representative benchmark for
natural image classiﬁcation (e.g.  Kornblith et al. [2019] showed that accuracy on ImageNet predicts
accuracy on other natural image classiﬁcation datasets)  we expect the Saccader model to perform
well in practical applications involving natural images. Future work is necessary to determine whether
the Saccader model is applicable to non-natural image domains (e.g.  in the medical ﬁeld).
Although Saccader outperforms other hard attention models  it still lags behind state-of-the-art
feedforward models in terms of accuracy. Our results suggest that  it may be possible to improve upon
the performance achieved here by exploring larger classiﬁcation model capacity and/or training on
higher-quality images. Additionally  although it was previously suggested that foveation mechanisms
might provide natural robustness against adversarial examples [Luo et al.  2015]  the hard attention-
based models that we explored here are not substantially more robust than traditional CNNs (see
Appendix D). We ensure that the Saccader classiﬁcation network is interpretable by limiting its input 
but the attention network has access to the entire image  and thus the patch selection process remains
difﬁcult to interpret.
Here  we consider only classiﬁcation task; future work can potentially extend the Saccader to many
other vision tasks. The high accuracy obtained by our hard attention model and the quality of the
learned visual attention policy open the door to the use of this interesting class of models in practice 
particularly in applications that require understanding of classiﬁcation predictions.

Acknowledgements

We are grateful to Pieter-Jan Kindermans  Jonathon Shlens  and Jascha Sohl-Dickstein for useful dis-
cussions and helpful feedback on the manuscript. We thank Jaehoon Lee for help with computational
resources.

References
J. Adebayo  J. Gilmer  M. Muelly  I. Goodfellow  M. Hardt  and B. Kim. Sanity checks for saliency

maps. In Advances in Neural Information Processing Systems  pages 9505–9515  2018.

9

S. Ahmad. Visit: a neural model of covert visual attention. In Advances in neural information

processing systems  pages 420–427  1992.

J. Andreas  M. Rohrbach  T. Darrell  and D. Klein. Neural module networks. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition  pages 39–48  2016.

A. Athalye  L. Engstrom  A. Ilyas  and K. Kwok. Synthesizing robust adversarial examples. In J. Dy
and A. Krause  editors  Proceedings of the 35th International Conference on Machine Learning 
volume 80 of Proceedings of Machine Learning Research  pages 284–293  Stockholmsmässan 
Stockholm Sweden  10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/
athalye18b.html.

J. Ba  V. Mnih  and K. Kavukcuoglu. Multiple object recognition with visual attention. arXiv preprint

arXiv:1412.7755  2014.

S. Bach  A. Binder  G. Montavon  F. Klauschen  K.-R. Müller  and W. Samek. On pixel-wise
explanations for non-linear classiﬁer decisions by layer-wise relevance propagation. PloS one  10
(7):e0130140  2015.

D. Baehrens  T. Schroeter  S. Harmeling  M. Kawanabe  K. Hansen  and K.-R. MÃžller. How
to explain individual classiﬁcation decisions. Journal of Machine Learning Research  11(Jun):
1803–1831  2010.

D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align and

translate. arXiv preprint arXiv:1409.0473  2014.

D. H. Ballard. Reference frames for animate vision. In IJCAI  volume 89  pages 1635–1641  1989.
D. H. Ballard  T. G. Becker  C. M. Brown  R. F. Gans  N. G. Martin  T. J. Olson  R. D. Potter  R. D.
Rimey  D. G. Tilley  and S. D. Whitehead. The rochester robot. Technical report  University of
Rochester Department of Computer Science  1988.

W. Brendel and M. Bethge. Approximating CNNs with bag-of-local-features models works surpris-
ingly well on ImageNet. In International Conference on Learning Representations  2019. URL
https://openreview.net/forum?id=SkfMWhAqYQ.

P. J. Burt. Attention mechanisms for vision in a dynamic world.

In [1988 Proceedings] 9th

International Conference on Pattern Recognition  pages 977–987. IEEE  1988.

N. J. Butko and J. R. Movellan. I-pomdp: An infomax model of eye movement. In 2008 7th IEEE

International Conference on Development and Learning  pages 139–144. IEEE  2008.

L. Chen  H. Zhang  J. Xiao  L. Nie  J. Shao  W. Liu  and T.-S. Chua. SCA-CNN: Spatial and
channel-wise attention in convolutional networks for image captioning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition  pages 5659–5667  2017.

A. Das  H. Agrawal  L. Zitnick  D. Parikh  and D. Batra. Human attention in visual question
answering: Do humans and deep networks look at the same regions? Computer Vision and Image
Understanding  163:90–100  2017.

S. A. Eslami  N. Heess  T. Weber  Y. Tassa  D. Szepesvari  G. E. Hinton  et al. Attend  infer  repeat:
Fast scene understanding with generative models. In Advances in Neural Information Processing
Systems  pages 3225–3233  2016.

H. Fukui  T. Hirakawa  T. Yamashita  and H. Fujiyoshi. Attention branch network: Learning of
attention mechanism for visual explanation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  2019.

R. Girshick. Fast R-CNN. In The IEEE International Conference on Computer Vision (ICCV) 

December 2015.

R. Girshick  J. Donahue  T. Darrell  and J. Malik. Rich feature hierarchies for accurate object
detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition  pages 580–587  2014.

10

M. A. Goodale and A. D. Milner. Separate visual pathways for perception and action. Trends in

neurosciences  15(1):20–25  1992.

I. Goodfellow  N. Papernot  S. Huang  Y. Duan  P. Abbeel  and J. Clark. Attacking machine learning
with adversarial examples. OpenAI. https://blog. openai. com/adversarial-example-research  2017.

I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. arXiv

preprint arXiv:1412.6572  2014.

K. Gregor  I. Danihelka  A. Graves  D. J. Rezende  and D. Wierstra. Draw: A recurrent neural

network for image generation. arXiv preprint arXiv:1502.04623  2015.

S. Hooker  D. Erhan  P. Kindermans  and B. Kim. Evaluating feature importance estimates. CoRR 

abs/1806.10758  2018. URL http://arxiv.org/abs/1806.10758.

J. Hu  L. Shen  and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition  pages 7132–7141  2018.

Y. Huang  Y. Cheng  D. Chen  H. Lee  J. Ngiam  Q. V. Le  and Z. Chen. Gpipe: Efﬁcient training of

giant neural networks using pipeline parallelism. arXiv preprint arXiv:1811.06965  2018.

M. Jaderberg  K. Simonyan  A. Zisserman  et al. Spatial transformer networks. In Advances in neural

information processing systems  pages 2017–2025  2015.

S. Jain and B. C. Wallace. Attention is not explanation. In NAACL  2019.

S. Jetley  N. A. Lord  N. Lee  and P. Torr. Learn to pay attention. In International Conference on

Learning Representations  2018. URL https://openreview.net/forum?id=HyzbhfWRW.

N. Kanopoulos  N. Vasanthavada  and R. L. Baker. Design of an image edge detection ﬁlter using the

sobel operator. IEEE Journal of solid-state circuits  23(2):358–367  1988.

P.-J. Kindermans  S. Hooker  J. Adebayo  M. Alber  K. T. Schütt  S. Dähne  D. Erhan  and B. Kim.

The (un) reliability of saliency methods. arXiv preprint arXiv:1711.00867  2017.

S. Kornblith  J. Shlens  and Q. V. Le. Do better imagenet models transfer better? In The IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  June 2019.

H. Larochelle and G. E. Hinton. Learning to combine foveal glimpses with a third-order boltzmann

machine. In Advances in neural information processing systems  pages 1243–1251  2010.

D. Linsley  D. Shiebler  S. Eberhardt  and T. Serre. Learning what and where to attend with
In International Conference on Learning Representations  2019. URL

humans in the loop.
https://openreview.net/forum?id=BJgLg3R9KQ.

C. Liu  B. Zoph  M. Neumann  J. Shlens  W. Hua  L.-J. Li  L. Fei-Fei  A. Yuille  J. Huang  and
K. Murphy. Progressive neural architecture search. In Proceedings of the European Conference on
Computer Vision (ECCV)  pages 19–34  2018a.

R. Liu  J. Lehman  P. Molino  F. P. Such  E. Frank  A. Sergeev  and J. Yosinski. An intriguing failing
of convolutional neural networks and the coordconv solution. In Advances in Neural Information
Processing Systems  pages 9605–9616  2018b.

J. Lu  C. Xiong  D. Parikh  and R. Socher. Knowing when to look: Adaptive attention via a visual
sentinel for image captioning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 375–383  2017.

Y. Luo  X. Boix  G. Roig  T. Poggio  and Q. Zhao. Foveation-based mechanisms alleviate adversarial

examples. arXiv preprint arXiv:1511.06292  2015.

T. Luong  H. Pham  and C. D. Manning. Effective approaches to attention-based neural machine
translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing  pages 1412–1421  2015.

11

A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Representations  2018. URL
https://openreview.net/forum?id=rJzIBfZAb.

V. Mnih  N. Heess  A. Graves  et al. Recurrent models of visual attention. In Advances in neural

information processing systems  pages 2204–2212  2014.

G. Montavon  S. Lapuschkin  A. Binder  W. Samek  and K.-R. Müller. Explaining nonlinear
classiﬁcation decisions with deep taylor decomposition. Pattern Recognition  65:211–222  2017.

A. Nguyen  A. Dosovitskiy  J. Yosinski  T. Brox  and J. Clune. Synthesizing the preferred inputs
for neurons in neural networks via deep generator networks. In Advances in Neural Information
Processing Systems  pages 3387–3395  2016.

B. A. Olshausen  C. H. Anderson  and D. C. Van Essen. A neurobiological model of visual attention
and invariant pattern recognition based on dynamic routing of information. Journal of Neuroscience 
13(11):4700–4719  1993.

N. Papernot  F. Faghri  N. Carlini  I. Goodfellow  R. Feinman  A. Kurakin  C. Xie  Y. Sharma 
T. Brown  A. Roy  A. Matyasko  V. Behzadan  K. Hambardzumyan  Z. Zhang  Y.-L. Juang  Z. Li 
R. Sheatsley  A. Garg  J. Uesato  W. Gierke  Y. Dong  D. Berthelot  P. Hendricks  J. Rauber  and
R. Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv preprint
arXiv:1610.00768  2018.

E. Real  A. Aggarwal  Y. Huang  and Q. V. Le. Regularized evolution for image classiﬁer architecture

search. arXiv preprint arXiv:1802.01548  2018.

S. Ren  K. He  R. Girshick  and J. Sun. Faster R-CNN: Towards real-time object detection with region
proposal networks. In Advances in Neural Information Processing Systems  pages 91–99  2015.

S. J. Rennie  E. Marcheret  Y. Mroueh  J. Ross  and V. Goel. Self-critical sequence training for image
captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 7008–7024  2017.

C. Rudin. Stop explaining black box machine learning models for high stakes decisions and use

interpretable models instead. Nature Machine Intelligence  1(5):206  2019.

J. Schmidhuber and R. Huber. Learning to generate focus trajectories for attentive vision. Institut für

Informatik  1990.

J. Schmidhuber and R. Huber. Learning to generate artiﬁcial fovea trajectories for target detection.

International Journal of Neural Systems  2(01n02):125–134  1991a.

J. Schmidhuber and R. Huber. Using sequential adaptive neuro-control for efﬁcient learning of

rotation and translation invariance. In Artiﬁcial Neural Networks  pages 315–320  1991b.

A. C. Schütz  D. I. Braun  and K. R. Gegenfurtner. Eye movements and perception: A selective

review. Journal of vision  11(5):9–9  2011.

M. Seibert and A. M. Waxman. Spreading activation layers  visual saccades  and invariant representa-

tions for neural pattern recognition systems. Neural Networks  2(1):9–27  1989.

P. Sermanet  A. Frome  and E. Real. Attention for ﬁne-grained categorization. In International
Conference on Learning Representations (ICLR 2015) workshop  2015. URL http://arxiv.
org/abs/1412.7054.

K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

J. C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approx-

imation. IEEE Transactions on Automatic Control  37(3):332–341  1992.

J. T. Springenberg  A. Dosovitskiy  T. Brox  and M. Riedmiller. Striving for simplicity: The all

convolutional net. arXiv preprint arXiv:1412.6806  2014.

12

R. S. Sutton  D. A. McAllester  S. P. Singh  and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in neural information processing systems 
pages 1057–1063  2000.

C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus. Intriguing

properties of neural networks. arXiv preprint arXiv:1312.6199  2013.

J. Uesato  B. O’Donoghue  P. Kohli  and A. van den Oord. Adversarial risk and the dangers
of evaluating against weak attacks. In J. Dy and A. Krause  editors  Proceedings of the 35th
International Conference on Machine Learning  volume 80 of Proceedings of Machine Learning
Research  pages 5025–5034  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.
URL http://proceedings.mlr.press/v80/uesato18a.html.

J. R. Uijlings  K. E. Van De Sande  T. Gevers  and A. W. Smeulders. Selective search for object

recognition. International Journal of Computer Vision  104(2):154–171  2013.

A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  Ł. Kaiser  and I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems  pages 5998–6008 
2017.

B. A. Wandell. Foundations of vision. Sinauer Associates  1995.
F. Wang  M. Jiang  C. Qian  S. Yang  C. Li  H. Zhang  X. Wang  and X. Tang. Residual attention
network for image classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 3156–3164  2017.

R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning  8(3-4):229–256  1992.

S. Woo  J. Park  J.-Y. Lee  and I. So Kweon. CBAM: Convolutional block attention module. In

Proceedings of the European Conference on Computer Vision (ECCV)  pages 3–19  2018.

C. Xie  Y. Wu  L. v. d. Maaten  A. L. Yuille  and K. He. Feature denoising for improving adversarial
robustness. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 501–509  2019.

K. Xu  J. Ba  R. Kiros  K. Cho  A. Courville  R. Salakhudinov  R. Zemel  and Y. Bengio. Show 
attend and tell: Neural image caption generation with visual attention. In International Conference
on Machine Learning  pages 2048–2057  2015.

J. Yosinski  J. Clune  A. Nguyen  T. Fuchs  and H. Lipson. Understanding neural networks through

deep visualization. arXiv preprint arXiv:1506.06579  2015.

Q. You  H. Jin  Z. Wang  C. Fang  and J. Luo.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 4651–
4659  2016.

Image captioning with semantic attention.

M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European

conference on computer vision  pages 818–833. Springer  2014.

Y. Zheng  R. S. Zemel  Y.-J. Zhang  and H. Larochelle. A neural autoregressive approach to attention-

based recognition. International Journal of Computer Vision  113(1):67–79  2015.

B. Zhou  A. Khosla  A. Lapedriza  A. Oliva  and A. Torralba. Learning deep features for discriminative
localization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June
2016.

L. M. Zintgraf  T. S. Cohen  T. Adel  and M. Welling. Visualizing deep neural network decisions:

Prediction difference analysis. arXiv preprint arXiv:1702.04595  2017.

B. Zoph  V. Vasudevan  J. Shlens  and Q. V. Le. Learning transferable architectures for scalable image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 8697–8710  2018.

13

,Vibhav Vineet
Carsten Rother
Philip Torr
Changyou Chen
Jun Zhu
Xinhua Zhang
Sida Wang
Arun Tejasvi Chaganty
Percy Liang
Gamaleldin Elsayed
Simon Kornblith
Quoc Le