2019,McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds,A crucial assumption in most statistical learning theory is that samples are independently and identically distributed (i.i.d.). However  for many real applications  the i.i.d. assumption does not hold. We consider learning problems in which examples are dependent and their dependency relation is characterized by a graph. To establish algorithm-dependent generalization theory for learning with non-i.i.d. data  we first prove novel McDiarmid-type concentration inequalities for Lipschitz functions of graph-dependent random variables. We show that concentration relies on the forest complexity of the graph  which characterizes the strength of the dependency. We demonstrate that for many types of dependent data  the forest complexity is small and thus implies good concentration. Based on our new inequalities we are able to build stability bounds for learning from graph-dependent data.,McDiarmid-Type Inequalities for Graph-Dependent

Variables and Stability Bounds

Rui (Ray) Zhang (cid:3)
School of Mathematics

Monash University

rui.zhang@monash.edu

Yuyi Wang

ETH Zurich  Switzerland

X-Order Lab  China

yuyiwang920@gmail.com

Xingwu Liu y

Institute of Computing Technology 

Chinese Academy of Sciences.

University of Chinese Academy of Sciences

liuxingwu@ict.ac.cn

Liwei Wang

Key Laboratory of Machine Perception  MOE 

School of EECS  Peking University

Center for Data Science  Peking University

wanglw@cis.pku.edu.cn

Abstract

A crucial assumption in most statistical learning theory is that samples are inde-
pendently and identically distributed (i.i.d.). However  for many real applications 
the i.i.d. assumption does not hold. We consider learning problems in which ex-
amples are dependent and their dependency relation is characterized by a graph.
To establish algorithm-dependent generalization theory for learning with non-i.i.d.
data  we ﬁrst prove novel McDiarmid-type concentration inequalities for Lipschitz
functions of graph-dependent random variables. We show that concentration re-
lies on the forest complexity of the graph  which characterizes the strength of the
dependency. We demonstrate that for many types of dependent data  the forest
complexity is small and thus implies good concentration. Based on our new in-
equalities  we establish stability bounds for learning graph-dependent data.

1 Introduction

Generalization theory is at the foundation of machine learning. It quantiﬁes how accurate a model
would predict on the test data which the learning algorithm is not able to access during training.
It usually relies on a crucial assumption: The data are independently and identically distributed
(i.i.d.). The i.i.d. assumption allows one to use many powerful tools from probability to prove
strong generalization error bounds. However  in real applications  the data are often non-i.i.d. i.e. 
the data collected can be dependent. There have been extensive discussions on why and how the
data are dependent. We refer the readers to [1  2].
Establishing generalization theory for dependent data has received a lot of attention [3  4  5  6  7].
A major line of research in this direction models the data dependency by various types of mixing
such as (cid:11)-mixing [8]  (cid:12)-mixing [9]  ϕ-mixing [10]  (cid:17)-mixing [11]  etc. Mixing models have been
used in statistical learning theory to establish generalization error bounds based on Rademacher
complexity [4  6  12] or algorithmic stability [3  12  13] via concentration results [14] or independent

This work was done when this author was a master student at the Institute of Computing Technology 
Chinese Academy of Sciences and University of Chinese Academy of Sciences. This research forms part of
Rui (Ray) Zhang’s master thesis submitted to the University of Chinese Academy of Sciences in May 2019.

(cid:3)

y

Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In these models  the mixing coefﬁcients measure the extent to which
blocking technique [15].
the data are dependent to each other. Similar to the mixing models  learning under Dobrushin’s
condition [16] is also investigated via concentration results [17  18  19] using Dobrushin’s interaction
matrix [20]. Although the results under the various mixing conditions and Dobrushin’s condition
are fruitful  they are faced with difﬁculties in application: It is sometimes difﬁcult to determine the
quantitative dependency among data points. On the other hand  determining whether two data are
dependent or not is often much easier. In this paper  we focus on such qualitative dependency of
data. We use simple graphs as a natural tool to describe the dependency among data  and establish
generalization theory for such graph-dependent data.
A basic building block of generalization theory is concentration inequality. Different settings and
different assumptions require different concentration tools. The less we assume  the more powerful
tools we need. In order to establish generalization theory for dependent data  standard concentration
for i.i.d. data no longer applies. One must develop concentration inequalities for dependent data 
which is a very challenging task.
In his seminal work [21]  Janson proved an elegant concentration inequality for graph-dependent
data. The inequality is a beautiful extension of Hoeffding inequality. It bounds the probability that
the summation of graph-dependent random variables deviates from its expected value  in terms of
the fractional coloring number of the dependency graph. Janson’s inquality has been extended to
any functions that can be decomposed into the summation of some functions of independent random
variables [22]. This extension enables to establish generalization error bounds for graph-dependent
data via fractional Rademacher complexity.
In [5]  PAC-Bayes bounds for classiﬁcation with non-i.i.d. data are obtained based on fractional col-
orings of graphs. The results also hold for speciﬁc learning settings such as ranking and learning
from stationary (cid:12)-mixing distributions. In [23]  Ralaivola and Amini established new concentra-
tion inequalities for fractionally sub-additive and fractionally self-bounding functions of dependent
variables. Their results are based on the fractional chromatic numbes and the entropy method. In
[24  25]  Wang et al. used hypergraphs to model dependent random variables that are generated by
independent ones. Leveraging the notion of fractional matching  they also establish concentration
inequalities of Hoeffding- or Bernstein-type.
Though fundamental and elegant  the above generalization bounds are algorithm-independent. They
considered the complexity of the hypothesis space and data distribution  but does not involve the
learning algorithm. To derive better generalization bounds  there are growing interests in developing
algorithm-dependent generalization theories. This line of research heavily relies on the algorithmic
stability. A key advantage of stability bounds is that they are tailored to speciﬁc learning algorithms 
exploiting their particular properties.
How can we establish algorithmic stability theory for graph-dependent data? Note that under the
assumption of i.i.d. data  Hoeffding-type concentration inequality  which bounds the deviation of
sample average from expectation  is not strong enough to prove stability-based generalization. On
the contrary  McDiarmids inequality characterizes the concentration of general Lipschitz functions
of i.i.d. random variables  hence serving as the key tool for proving the stability theory. Therefore 
to build algorithmic stability theory for non-i.i.d. samples  one has to develop McDiarmid-type
concentration for graph-dependent random variables.
In this paper  we prove the ﬁrst McDiarmid-type concentration inequality for graph-dependent ran-
dom variables in terms of a new notion called forest complexity  which measures the strength of the
dependency. It turns out that for various dependency graphs  it is easy to estimate the forest com-
plexity. The proposed concentration inequality enables us to prove stability-based generalization
bounds for graph-dependent data. Our results provide basic tools for understanding learning with
overparameterized models.
The rest of the paper is organized as follows. In section 2  we brieﬂy introduce the notations and
related results.
In section 3  we establish McDiarmid-type inequalities for acyclic dependency
graphs  and extend the concentration results to the general dependency graphs. In section 4  we
apply our concentration results to the learning theory and establish generalization error bounds for
learning graph-dependent data via algorithmic stability  we also provide an application of learning
m-dependent data. Section 5 concludes the paper and points out the future research directions.
The supplementary materials can be found in [26].

2

2 Preliminaries

∏

In this section  we present the notations and the basic McDiarmid’s inequality for i.i.d. random
variables.
Throughout this paper  let n be a positive integer with [n] standing for the set f1; 2; : : : ; ng. Let Ωi
be a Polish space for any i 2 [n]  Ω =
i2[n] Ωi be the product space  R be the set of real numbers 
R+ be the set of non-negative real numbers  N+ be the set of non-negative integers.
Concentration inequalities are fundamental tools in statistical learning theory. They are essentially
tail probability bounds indicating how much a function of random variables deviates from some
value that is usually the expectation. Among the most powerful ones is the McDiarmid’s inequality
which establishes a sharp  even tight in some cases  bound on the concentration  when the function
satisﬁes c-Lipschitz condition (bounded differences condition)  namely  does not depend too much
on any individual variable.
+  a function f : Ω ! R is said
Deﬁnition 2.1 (c-Lipschitz). Given a vector c = (c1; : : : ; cn) 2 Rn
to be c-Lipschitz if for any x = (x1; : : : ; xn); x
jf (x) (cid:0) f (x
′

)j (cid:20) n∑

n) 2 Ω  it satisﬁes
′

′
= (x
1; : : : ; x

g;

′

ci1fxi̸=x

′
i

i=1

where ci is called the i-th Lipschitz coefﬁcient of f.
Theorem 2.2 (McDiarmid’s inequality [27]). Suppose f : Ω ! R is c-Lipschitz  and X =
(X1; : : : ; Xn) is a vector of independent random variables with each Xi taking values in Ωi. Then
for any t > 0  the tail probability satisﬁes

(

)

Pr (f (X) (cid:0) E[f (X)] (cid:21) t) (cid:20) exp

(cid:0) 2t2
∥c∥2

2

:

(1)

∑

the McDiarmid’s inequality works for independent random variables.

Notice that
Janson’s
Hoeffding-type inequality [21] for graph-dependent random variables is a special case of
n
McDiarmid-type inequality when the function is a summation. Speciﬁcally  when f (X) =
i=1 Xi
with each Xi ranging over an interval of length ci 
(cid:21) t

[
n∑

n∑

(cid:20) exp

(

)

(

)

]

2t2

(cid:0)

(2)

Xi

(cid:31)(cid:3)(G)∥c∥2

2

;

Xi (cid:0) E
(cid:3)
where c = (c1; : : : ; cn) and (cid:31)
random variables X.

Pr

i=1

i=1

(G) is the fractional coloring number of a dependency graph G of

3 McDiarmid Concentration for Graph-dependent Random Variables

In this section we present our ﬁrst set of main results  the McDiarmid-type concentration inequalities
(i.e.  concentration of Lipschitz functions) for graph-dependent random variables. The results in this
section will serve as the tools for developing learning theory for dependent data.
We start from the simplest case that the dependency graph is acyclic  i.e.  trees or forests. We
prove McDiarmid-type concentration bounds for trees and forests with very simple forms. These
inequalities are then extended to general graphs. To this end  we introduce the notion of forest
complexity  which characterizes to what extent a general graph can be best approximated by a forest.
We prove McDiarmid-type concentration inequality for general graph-dependent random variables
in terms of the forest complexity. Finally we demonstrate that for many important classes of graphs 
forest complexity is easy to estimate.
Below we ﬁrst deﬁne the notion of dependency graphs  which is a widely used model in probability 
statistics  and combinatorics  see [28  29  30  31  32] for examples.
Deﬁnition 3.1 (Dependency Graphs). An undirected graph G is called a dependency graph of a
random vector X = (X1; : : : ; Xn) if

1. V (G) = [n]
2. if I; J (cid:26) [n] are non-adjacent in G  then fXigi2I and fXjgj2J are independent.

3

3.1 McDiarmid Concentration for Acyclic Graph-dependent Variables

Our ﬁrst result is for the case that the dependency graph is a tree.
Theorem 3.2. Suppose that f : Ω ! R is a c-Lipschitz function and G is a dependency graph of a
random vector X that takes values in Ω. If G is a tree  then for any t > 0  the following inequality
holds:

(

)

Pr(f (X) (cid:0) E[f (X)] (cid:21) t) (cid:20) exp

(cid:0)

2t2

⟨i;j⟩2E(G)(ci + cj)2 + c2

min

(3)

∑

n

∑

where cmin is the minimum entry in c.
The proof of this theorem relies on decomposing f (X)(cid:0)E[f (X)] into the summation
i=1 Vi with
Vi := E[f (X)jX1; : : : Xi] (cid:0) E[f (X)jX1; : : : Xi(cid:0)1]. We show that each Vi ranges in an interval of
length at most ci + cj  where j is the parent of i in the tree (in the proof  we make the tree rooted by
choosing the vertex with the minimum Lipschitz coefﬁcient as the root). The theorem is then proved
by applying the Chernoff-Cramér technique to
i=1 Vi. For details  please refer to Subsection A.1
in the supplementary materials.
Like McDiarmid’s inequality  Theorem 3.2 also claims a deviation probability bound that decays
exponentially. The decay rate is determined by two interplaying factors. One is the Lipschitz co-
efﬁcient that is inherent to the function. The other is the pattern of the dependency  namely  which
random variables are dependent and connected by an edge.
We then generalize the above result to the case where dependency graph G is a forest.
Theorem 3.3. Suppose that f : Ω ! R is a c-Lipschitz function and G is a dependency graph of a
random vector X that takes values in Ω. If G is a forest consisting of trees fTigi2[k]  then for any
t > 0  the following inequality holds:

n

;

∑

)

Pr(f (X) (cid:0) E[f (X)] (cid:21) t) (cid:20) exp

(

∑

(cid:0)

∑

2t2

⟨i;j⟩2E(G)(ci + cj)2 +

k
i=1 c2

min;i

;

(4)

where cmin;i = minfcj : j 2 V (Ti)g.
Theorem 3.3 can be proved in a similar way as Theorem 3.2. The detailed proof is presented in
Subsection A.2 of the supplementary materials.
We point out that Theorem 3.3 is a strict generalization of the McDiarmid’s inequality for i.i.d.
random variables. If all the random variables are independent  i.e.  there is no edge in the dependency
graph  then it is clear that Eq. (4) degenerates exactly to Eq. (1).
Theorem 3.3 also clearly demonstrates how dependency between random variables affects concen-
tration. The decay rate of the probability that f (X) deviates from its expectation is approximately
reversely proportional to the number of edges in the dependency graph.

3.2 McDiarmid Concentration for General Graphs

In this subsection  we consider general graphs. Our basic idea for handling general graphs is to use
a forest to approximate the graph. Speciﬁcally  we partition the variables into groups so that the
dependency graph of these groups is a forest. We try to ﬁnd the optimal forest approximation  which
leads to the notion of forest complexity. We then prove McDiarmid-type concentration inequality
for general graph-dependent random variables in terms of its forest complexity  which yields a very
simple form.
We ﬁrst deﬁne the concept of forest approximation.
Deﬁnition 3.4 (Forest Approximation). Given a graph G  a forest F   and a mapping ϕ : V (G) !
V (F )  if ϕ(u) = ϕ(v) or ⟨ϕ(u); ϕ(v)⟩ 2 E(F ) for any ⟨u; v⟩ 2 E(G)  we say that (ϕ; F ) is a forest
approximation of G. Let (cid:8)(G) denote the set of forest approximations of G.

Intuitively  a forest approximation is transforming a graph into a forest by merging vertices and
removing the incurred self-loops and multi-edges. In this way  we rule out the redundant variables
that heavily depend on others and thus contribute little to concentration.

4

Based on forest approximation  we deﬁne the notion of forest complexity of a graph  which intu-
itively measures how much the graph looks like a forest.
Deﬁnition 3.5 (Forest Complexity). Given a graph G and any forest approximation (ϕ; F ) 2 (cid:8)(G)
with F consisting of trees fTigi2[k]  let

∑

(jϕ

(cid:0)1(u)j + jϕ

(cid:0)1(v)j)2

k∑

jϕ

(cid:0)1(u)j2:

+

i=1

min
u2V (Ti)

(cid:21)(ϕ;F ) =

⟨u;v⟩2E(F )

We call

the forest complexity of the graph G.

(cid:3)(G) = min

(ϕ;F )2(cid:8)(G)

(cid:21)(ϕ;F )

Now we are ready to state our McDiarmid-type concentration inequality for general graph-dependent
random variables.
Theorem 3.6. Suppose that f : Ω ! R is a c-Lipschitz function and G is a dependency graph of a
random vector X that takes values in Ω. For any t > 0  the following inequality holds:

Pr(f (X) (cid:0) E[f (X)] (cid:21) t) (cid:20) exp

(cid:0)

2t2

(cid:3)(G)∥c∥21

(

)

:

With the tool of forest approximation  we reduce the concentration problem deﬁned on graphs to that
deﬁned on forests. Basically  we use a new variable to represent each set of the original variables that
are merged together by the forest approximation. The function can be equivalently transformed into
a function of the new variables whose dependency graph is the forest. The proof is done by applying
Theorem 3.3 to the new function. For details  please refer to Subsection A.3 in the supplementary
materials.
Like the above theorems  Theorem 3.6 also establishes an exponentially decaying probability of
deviation. The decay rate is totally determined by the Lipschitz coefﬁcient of the function and the
forest complexity of the variables’ dependency graph. Intuitively  the more the dependency graph
looks like a forest  the faster the deviation probability decays. This uncovers how the dependencies
among random variables inﬂuence concentration.

3.3

Illustrations and Examples

This subsection consists of two parts. In the ﬁrst part we review a widely-studied random process
that generates dependent data whose dependency graph can be naturally constructed. In the second
part  we deal with some dependency graphs to show that in many cases  the forest complexity is
small and easy to estimate.
Consider a data generating procedure modeled by the spatial Poisson point process  which is a Pois-
son point process on R2 (See [33  34] for discussions of using this process to model data collection
in various machine learning applications.) The number of points in each ﬁnite region follows a
Poisson distribution  and the number of points in disjoint regions are independent. Given a ﬁnite set
I = fIign
i=1 of regions in R2  let Xi be the number of points in region Ii  1 (cid:20) i (cid:20) n. Then the
graph G ([n];f⟨i; j⟩ : Ii \ Ij ̸= ∅g) is a dependency graph of the random variables fXign
We present three examples to demonstrate that estimating the forest complexity (cid:3)(G) is usually
easy. All the examples can naturally appear in the above process.
Example 3.7 (G is a tree). In this case  the identity map between G and itself is a forest approxima-
tion of G. Then (cid:3)(G) (cid:20) jE(G)j(1 + 1)2 + 1 = 4n (cid:0) 3 = O(n). We get an upper bound of (cid:3)(G)
that is linear in the number of variables  which is almost tight compared with Hoeffding’s inequality
or Janson’s result (see (2) with (cid:31)
Example 3.8 (G is a cycle Cn). If n is even  a forest approximation is illustrated in Figure 1 
where the cycle is approximated by a path F of length n
2 . The approximation ϕ maps any vertex
of G to the vertex of F having the same shape  so each gray belt stands for a preimage set of ϕ.
We will keep this convention in the rest of this section. By the illustrated forest approximation 

(G) = 2).

i=1.

(cid:3)

5

(cid:3)(G) (cid:20) 2 (cid:2) (1 + 2)2 + ( n
forest approximation shown in Figure 2  (cid:3)(G) (cid:20) (1+2)2+( n(cid:0)1
(cid:3)
Since (cid:31)

(cid:0) 2)(2 + 2)2 + 1 = 8n (cid:0) 13 = O(n). When n is odd  according to the
(cid:0)1)(2+2)2+1 = 8n(cid:0)14 = O(n).

(G) is 2 or 3  our bound is again very tight compared with Jansons result.

2

2

.......

G

ϕ(cid:0)(cid:0)!

.....
F

ϕ(cid:0)(cid:0)!

....
F

......

G

Figure 1: A forest approximation of C6

Figure 2: A forest approximation of C5

Example 3.9 (G is a grid). Suppose G is a two-dimensional (m (cid:2) m)-grid. Then n = m2. Consid-
ering the forest approximation illustrated in Figure 3  (cid:3)(G) (cid:20) 2[32 + 52 + : : : + (2m (cid:0) 1)2] + 1 =
2m(2m+1)(2m(cid:0)1)(cid:0)3

3

= O(m3) = O(n 3
2 )

G

F

..................

....
.

ϕ

..
.......

Figure 3: A forest approximation of the (4 (cid:2) 4)-gird

4 Generalization Theory for Learning from Graph-dependent Data

This section establishes stability generalization error bounds for learning from graph-dependent
data  using the concentration inequalities derived in the last section.
Consider the supervised learning setting: Let S = ((x1; y1); : : : ; (xn; yn)) 2 (X (cid:2)Y)n be a training
sample of size n  where X is the input space and Y is the output space. Let D be the underlying
distribution of data on X (cid:2) Y. Assume that all the training data points (xi; yi)’s have the same
marginal distribution D and that G is a dependency graph of S.
Throughout this section  ﬁx a non-negative loss function ℓ : Y (cid:2) Y ! R. For any hypothesis
f : X ! Y  the empirical error on sample S is
1
n

bR(f ) =

ℓ(yi; f (xi)):

n∑

i=1

For learning from dependent data  the generalization error can be deﬁned in various ways. We adopt
the following widely-used one [35  36  37  38]

R(f ) = E(x;y)(cid:24)D[ℓ(y; f (x))];
which assumes that the test set is independent of the training set.

4.1 Bounding Generalization Error via Algorithmic Stability

(5)

Algorithmic stability has been used in the study of classiﬁcation and regression to derive generaliza-
tion bounds [39  40  41  42  43  44]. A key advantage of stability bounds is that they are designed for

6

speciﬁc learning algorithms  exploiting particular properties of the algorithms. Introduced 17 years
ago  uniform stability [45] is now among the most widely used notions of algorithmic stability.
Given a training sample S of size n and i 2 [n]  remove the i-th element from S  resulting in a sample
of size n (cid:0) 1  which is denoted by S
ni = ((x1; y1); : : : ; (xi(cid:0)1; yi(cid:0)1); (xi+1; yi+1) : : : ; (xn; yn)).
For a learning algorithm A  deﬁne f
S : X ! Y to be the the hypothesis that A has learned from
A
the sample S.
Deﬁnition 4.1 (Uniform Stability [45]). Given integer n > 0  the learning algorithm A is called
(cid:12)n-uniformly stable with respect to the loss function ℓ  if for any i 2 [n]  S 2 (X (cid:2) Y)n  and
(x; y) 2 X (cid:2) Y  it holds that

jℓ(y; f

S (x)) (cid:0) ℓ(y; f
A

Sni (x))j (cid:20) (cid:12)n:
A

Intuitively  the stability of a leaning algorithm means that any small perturbation of training samples
has little effect on the result of learning.
A
Now  we begin our analysis with studying the distribution of (cid:8)A(S) = R(f
S )  namely  the
difference between the empirical and the generalization errors. The mapping (cid:8)A : (X (cid:2) Y)n ! R
A
S ) via stability. We ﬁrst show that the deviation of (cid:8)A(S)
will play a critical role in estimating R(f
from its expectation can be bounded with high probability (Lemma 4.2)  and then upper bound the
expected value of (cid:8)A(S) in Lemma 4.3.
Lemma 4.2. Given a sample S of size n with dependency graph G  assume that the learning algo-
rithm A is (cid:12)n-uniformly stable. Suppose the loss function ℓ is bounded by M. Then for any t > 0 
it holds that

S )(cid:0)bR(f

(

)

A

Pr((cid:8)A(S) (cid:0) E[(cid:8)A(S)] (cid:21) t) (cid:20) exp

(cid:0)

2n2t2

(cid:3)(G)(4n(cid:12)n + M )2

:

Lemma 4.2 is proved in two steps. First  we treat (cid:8)A((cid:1)) as an n-ary function and show that its
Lipschitz coefﬁcients are all bounded by 4(cid:12)n + M=n. Second  regarding S as a random vector  we
apply Theorem 3.6 to (cid:8)A(S). For detail  see Subsection B.1 of the supplementary materials.
Lemma 4.3. Given a sample S of size n with dependency graph G  assume that the learning al-
gorithm A is (cid:12)i-uniformly stable for any i (cid:20) n. Suppose the maximum degree of G is ∆. Let
(cid:12)n;∆ = maxi2[0;∆] (cid:12)n(cid:0)i. It holds that

E[(cid:8)A(S)] (cid:20) 2(cid:12)n;∆(∆ + 1):

The proof of the lemma is based on iterative perturbations on the training sample S. A perturbation
is essentially removing a data point from or adding a data point to S. The property of uniform
stability of the algorithm guarantees that each perturbation causes a discrepancy up to (cid:12)n;∆  and in
total 2(∆ + 1) perturbations have to be made in order to eliminate the dependency between a data
point and the others. For detail  please refer to Subsection B.2 of the supplementary materials.
Combining Lemma 4.2 and Lemma 4.3  we immediately have
Theorem 4.4. Given a sample S of size n with dependency graph G  assume that the learning
algorithm A is (cid:12)i-uniformly stable for any i (cid:20) n. Suppose the maximum degree G is ∆  and the
loss function ℓ is bounded by M. Let (cid:12)n;∆ = maxi2[0;∆] (cid:12)n(cid:0)i. For any (cid:14) 2 (0; 1)  with probability
at least 1 (cid:0) (cid:14)  it holds that

√

A

S ) (cid:20) bR(f
)

R(f

(√

A
S ) + 2(cid:12)n;∆(∆ + 1) +

4n(cid:12)n + M

(cid:3)(G) ln(1=(cid:14))

n

2

:

Remark 4.5. It is well known that for many learning algorithms (cid:12)n = O(1=n) [45]. Thus  we
often have (cid:12)n;∆(∆ + 1) (cid:20) (cid:12)n(cid:0)∆(∆ + 1) = O( ∆
n(cid:0)∆ )  which vanishes asymptotically if ∆ = o(n).
also vanishes asymptotically if (cid:3)(G) = o(n2). As a result  in case of
The term O
weak dependence such as the examples in Subsection 3.3  the generalization error is almost upper-
bounded by the empirical error. We also observe that if the training data are i.i.d.  Theorem 4.4
degenerates to the standard stability bound in [45]  by applying ∆ = 0  (cid:12)n;∆ = (cid:12)n  (cid:3)(G) = n.

(cid:3)(G)=n

7

4.2 Application: Learning from m-dependent Data

We present a practical application in machine learning. Suppose there are linearly aligned locations 
for example  real estates along a street. Let yi be the observation at location i  e.g.  the house price 
and xi stand for the random variable modeling geographical effect at location i. Suppose that x’s are
mutually independent and each yi is geographically inﬂuenced by a neighborhood of size at most
2q +1. One hope to learn the model of y from a sample f((xi(cid:0)q; : : : ; xi; : : : ; xi+q); yi)gi2[n]  where
n is the size of the sample. This model accounts for the impact of local locations on house prices.
Similar scenarios are frequently considered in spatial econometrics  see [46] for more examples.
This application is a special case of m-dependence  which is an important statistical model intro-
duced by Hoeffding in [47]. m-dependence has been studied extensively in probability  statistics 
and combinatorics [48  49  50].
Deﬁnition 4.6 (m-dependence [47]). For some m; n 2 N+  a sequence of random variables
fXign
j=i+m+1.

i=1 is called m-dependent if for any i 2 [n(cid:0)m(cid:0)1]  fXjgi

j=1 is independent of fXjgn

The upper part of Figure 4 illustrates a dependency graph of 2-dependent sequence fXign
As illustrated in Figure 4  we divide an m-dependent sequence into blocks of size m  and sequen-
tially map the blocks to vertices of a path of length

. This forest approximation leads to

i=1.

⌈

⌉

n
m

(cid:3)(G) (cid:20)

(m + m)2 + m2 (cid:20) 4mn = O(mn)

(⌈

⌉

)
(cid:0) 1

n
m

G

..
......

....
.

ϕ

F

..
...

Figure 4: A forest approximation of a 2-dependent sequence. The approximation ϕ maps any vertex
of G to the vertex of F having the same shape  so each gray belt stands for a pre-image set of ϕ.

Combining Theorem 4.4 and the estimated forest complexity  we have
Corollary 4.7. Given an m-dependent sequence S of length n as training sample  assume that the
learning algorithm A is (cid:12)i-uniformly stable for any i (cid:20) n. Suppose the loss function ℓ is bounded
√
by M. For any (cid:14) 2 (0; 1)  with probability at least 1 (cid:0) (cid:14)  it holds that

A
S ) + 2(cid:12)n;2m(2m + 1) + (4n(cid:12)n + M )

2m ln(1=(cid:14))

n

:

S ) (cid:20) bR(f

A

R(f

)
Choose any uniformly stable learning algorithm A in [45] with (cid:12)n = O(1=n)  such as regularization
algorithms in RKHS. Apply it to the above mentioned house price prediction problem. Then for
any ﬁxed q  with high probability  Corollary 4.7 leads to R(f
for
sufﬁciently large n  matching the stability bound of the i.i.d. case in [45].

S ) (cid:20) bR(f

(√

A
S ) + O

ln(1=(cid:14))

A

n

5 Conclusion and Future Work

In this paper  we establish McDiarmid-type concentration inequalities for general functions of graph-
dependent random variables. We apply our concentration results to obtain a stability-based gener-
alization error bound for learning from graph-dependent samples. There are several possible exten-
sions of this work.

8

(cid:15) We provide upper bounds of the forest complexity for several classes of graphs. It is an
interesting algorithmic problem to efﬁciently estimate the forest complexity. One heuristic
method to do this on a connected graph is via graph diameter  by merging vertices of the
same distances to a peripheral vertex  resulting in a path as long as the diameter.
(cid:15) If more information of the dependency structure is known  e.g.  dependency hyper-
graphs [24  25]  can we obtain better concentration inequalities and generalization bounds?
(cid:15) In [3  12  6]  generalization error is deﬁned different from that in this paper. The differences
between these two deﬁnitions has been discussed in [3  12]. It is a natural question whether
our results can be adapted to that deﬁnition.
(cid:15) There are some newly introduced dependency graph models such as thresholded depen-
dency graphs [51] and weighted dependency graphs [52  53]. Can the problem in this
paper be solved under these new models?

Acknowledgments

Rui (Ray) Zhang would like to thank Nick Wormald for valuable comments on an early version
of this paper. Yuyi Wang would like to thank Ondˇrej Kuželka for very helpful discussions. Liwei
Wang would like to thank Yunchang Yang for very helpful discussions. Xingwu Liu’s work is
partially supported by the National Key Research and Development Program of China (Grant No.
2016YFB1000201)  the National Natural Science Foundation of China (61420106013)  State Key
Laboratory of Computer Architecture Open Fund (CARCH3410)  and Youth Innovation Promotion
Association of Chinese Academy of Sciences.

References
[1] Herold Dehling and Walter Philipp. Empirical process techniques for dependent data.

Empirical process techniques for dependent data  pages 3–113. Springer  2002.

In

[2] Massih-Reza Amini and Nicolas Usunier. Learning with Partially Labeled and Interdependent

Data. Springer  2015.

[3] Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for non-iid processes. In Advances

in Neural Information Processing Systems  pages 1025–1032  2008.

[4] Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid pro-

cesses. In Advances in Neural Information Processing Systems  pages 1097–1104  2009.

[5] Liva Ralaivola  Marie Szafranski  and Guillaume Stempfel. Chromatic PAC-Bayes bounds for
non-iid data: Applications to ranking and stationary (cid:12)-mixing processes. Journal of Machine
Learning Research  11(Jul):1927–1956  2010.

[6] Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for non-stationary mixing pro-

cesses. Machine Learning  106(1):93–117  2017.

[7] Hao Yi  Alon Orlitsky  and Venkatadheeraj Pichapati. On learning markov chains. In Advances

in Neural Information Processing Systems  pages 646–655  2018.

[8] Murray Rosenblatt. A central limit theorem and a strong mixing condition. Proceedings of the

National Academy of Sciences of the United States of America  42(1):43  1956.

[9] VA Volkonskii and Yu A Rozanov. Some limit theorems for random functions. i. Theory of

Probability & Its Applications  4(2):178–197  1959.

[10] Ildar A Ibragimov. Some limit theorems for stationary processes. Theory of Probability & Its

Applications  7(4):349–382  1962.

[11] Leonid Kontorovich. Measure concentration of strongly mixing processes with applications.

Carnegie Mellon University  2007.

[12] Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary φ-mixing and (cid:12)-

mixing processes. Journal of Machine Learning Research  11(Feb):789–814  2010.

9

[13] Fangchao He  Ling Zuo  and Hong Chen. Stability analysis for ranking with stationary φ-

mixing samples. Neurocomputing  171:1556–1562  2016.

[14] Leonid Aryeh Kontorovich  Kavita Ramanan  et al. Concentration inequalities for dependent
random variables via the martingale method. The Annals of Probability  36(6):2126–2158 
2008.

[15] Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The

Annals of Probability  pages 94–116  1994.

[16] Yuval Dagan  Constantinos Daskalakis  Nishanth Dikkala  and Siddhartha Jayanti. Learning
from weakly dependent data under dobrushins condition. In Proceedings of the Thirty-Second
Conference on Learning Theory  volume 99 of Proceedings of Machine Learning Research 
pages 914–928  Phoenix  USA  25–28 Jun 2019. PMLR.

[17] Christof Külske. Concentration inequalities for functions of gibbs ﬁelds with application to
diffraction and random gibbs measures. Communications in mathematical physics  239(1-
2):29–51  2003.

[18] Sourav Chatterjee. Concentration inequalities with exchangeable pairs (ph. d. thesis). arXiv

preprint math/0507526  2005.

[19] Aryeh Kontorovich and Maxim Raginsky. Concentration of measure without independence: a
uniﬁed approach via the martingale method. In Convexity and Concentration  pages 183–210.
Springer  2017.

[20] PL Dobruschin. The description of a random ﬁeld by means of conditional probabilities and

conditions of its regularity. Theory of Probability & Its Applications  13(2):197–224  1968.

[21] Svante Janson. Large deviations for sums of partly dependent random variables. Random

Structures & Algorithms  24(3):234–248  2004.

[22] Nicolas Usunier  Massih-Reza Amini  and Patrick Gallinari. Generalization error bounds for
In Advances in neural information processing

classiﬁers trained with interdependent data.
systems  pages 1369–1376  2006.

[23] Liva Ralaivola and Massih-Reza Amini. Entropy-based concentration inequalities for depen-

dent variables. In International Conference on Machine Learning  pages 2436–2444  2015.

[24] Yuyi Wang  Zheng-Chu Guo  and Jan Ramon. Learning from networked examples. In Inter-
national Conference on Algorithmic Learning Theory  ALT 2017  15-17 October 2017  Kyoto
University  Kyoto  Japan  pages 641–666  2017.

[25] Yuanhong Wang  Yuyi Wang  Xingwu Liu  and Juhua Pu. On the ERM principle with net-

worked data. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[26] Rui (Ray) Zhang  Xingwu Liu  Yuyi Wang  and Liwei Wang. McDiarmid-type inequalities for

graph-dependent variables and stability bounds. arXiv preprint arXiv:1909.02330  2019.

[27] Colin McDiarmid. On the method of bounded differences.

141(1):148–188  1989.

Surveys in combinatorics 

[28] Paul Erdos and László Lovász. Problems and results on 3-chromatic hypergraphs and some

related questions. Inﬁnite and ﬁnite sets  10(2):609–627  1975.

[29] Svante Janson  Tomasz Luczak  and Andrzej Rucinski. An exponential bound for the probabil-
ity of nonexistence of a speciﬁed subgraph in a random graph. Institute for Mathematics and
its Applications (USA)  1988.

[30] Louis HY Chen. Two central limit problems for dependent random variables. Probability

Theory and Related Fields  43(3):223–243  1978.

[31] Pierre Baldi  Yosef Rinott  et al. On normal approximations of distributions in terms of depen-

dency graphs. The Annals of Probability  17(4):1646–1650  1989.

10

[32] Svante Janson  Tomasz Luczak  and Andrzej Rucinski. Random graphs  volume 45. John

Wiley & Sons  2011.

[33] Scott Linderman and Ryan Adams. Discovering latent network structure in point process data.

In International Conference on Machine Learning  pages 1413–1421  2014.

[34] Alisa Kirichenko and Harry Van Zanten. Optimality of poisson processes intensity learning
with gaussian processes. The Journal of Machine Learning Research  16(1):2909–2919  2015.

[35] Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine

learning  39(1):5–34  2000.

[36] Aurélie C Lozano  Sanjeev R Kulkarni  and Robert E Schapire. Convergence and consistency
In Advances in

of regularized boosting algorithms with stationary b-mixing observations.
neural information processing systems  pages 819–826  2006.

[37] Ingo Steinwart and Andreas Christmann. Fast learning from non-iid observations. In Advances

in neural information processing systems  pages 1768–1776  2009.

[38] Hanyuan Hang and Ingo Steinwart. Fast learning from (cid:11)-mixing observations. Journal of

Multivariate Analysis  127:184–199  2014.

[39] William H Rogers and Terry J Wagner. A ﬁnite sample distribution-free performance bound

for local discrimination rules. The Annals of Statistics  pages 506–514  1978.

[40] Luc Devroye and Terry Wagner. Distribution-free performance bounds for potential function

rules. IEEE Transactions on Information Theory  25(5):601–604  1979.

[41] Michael Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-

out cross-validation. Neural computation  11(6):1427–1453  1999.

[42] Samuel Kutin and Partha Niyogi. Almost-everywhere algorithmic stability and generalization
error. In Proceedings of the Eighteenth conference on Uncertainty in artiﬁcial intelligence 
pages 275–282. Morgan Kaufmann Publishers Inc.  2002.

[43] Wenlong Mou  Yuchen Zhou  Jun Gao  and Liwei Wang. Dropout training  data-dependent
regularization  and generalization bounds. In International Conference on Machine Learning 
pages 3642–3650  2018.

[44] Wenlong Mou  Liwei Wang  Xiyu Zhai  and Kai Zheng. Generalization bounds of SGLD for

non-convex learning: Two theoretical viewpoints. arXiv preprint arXiv:1707.05947  2017.

[45] Olivier Bousquet and André Elisseeff. Stability and generalization. Journal of machine learn-

ing research  2(Mar):499–526  2002.

[46] Luc Anselin. Spatial econometrics: methods and models  volume 4. Springer Science &

Business Media  2013.

[47] Wassily Hoeffding  Herbert Robbins  et al. The central limit theorem for dependent random

variables. Duke Mathematical Journal  15(3):773–780  1948.

[48] PH Diananda and MS Bartlett. Some probability limit theorems with statistical applications.
In Mathematical Proceedings of the Cambridge Philosophical Society  volume 49  pages 239–
246. Cambridge University Press  1953.

[49] Pranab Kumar Sen. Asymptotic normality of sample quantiles for m-dependent processes. The

annals of mathematical statistics  pages 1724–1730  1968.

[50] Louis HY Chen and Qi-Man Shao. Stein’s method for normal approximation. An introduction

to Steins method  4:1–59  2005.

[51] Christoph H Lampert  Liva Ralaivola  and Alexander Zimin. Dependency-dependent bounds

for sums of dependent random variables. arXiv preprint arXiv:1811.01404  2018.

11

[52] Jehanne Dousse and Valentin Féray. Weighted dependency graphs and the ising model. arXiv

preprint arXiv:1610.05082  2016.

[53] Valentin Féray et al. Weighted dependency graphs. Electronic Journal of Probability  23 

2018.

12

,Rui (Ray) Zhang
Xingwu Liu
Yuyi Wang
Liwei Wang