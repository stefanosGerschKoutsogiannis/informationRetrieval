2014,Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing,The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However  since the estimator maximizes a non-convex log-likelihood function  it is hard to theoretically justify its performance. In this paper  we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach  while outperforming several other recently proposed methods.,Spectral Methods Meet EM: A Provably Optimal

Algorithm for Crowdsourcing

Yuchen Zhang†

Xi Chen(cid:93)

Dengyong Zhou∗ Michael I. Jordan†

†University of California  Berkeley  Berkeley  CA 94720

{yuczhang jordan}@berkeley.edu
(cid:93)New York University  New York  NY 10012

xichen@nyu.edu

∗Microsoft Research  1 Microsoft Way  Redmond  WA 98052

dengyong.zhou@microsoft.com

Abstract

The Dawid-Skene estimator has been widely used for inferring the true labels
from the noisy labels provided by non-expert crowdsourcing workers. However 
since the estimator maximizes a non-convex log-likelihood function  it is hard to
theoretically justify its performance. In this paper  we propose a two-stage efﬁ-
cient algorithm for multi-class crowd labeling problems. The ﬁrst stage uses the
spectral method to obtain an initial estimate of parameters. Then the second stage
reﬁnes the estimation by optimizing the objective function of the Dawid-Skene
estimator via the EM algorithm. We show that our algorithm achieves the optimal
convergence rate up to a logarithmic factor. We conduct extensive experiments on
synthetic and real datasets. Experimental results demonstrate that the proposed
algorithm is comparable to the most accurate empirical approach  while outper-
forming several other recently proposed methods.

1

Introduction

With the advent of online crowdsourcing services such as Amazon Mechanical Turk  crowdsourcing
has become an appealing way to collect labels for large-scale data. Although this approach has
virtues in terms of scalability and immediate availability  labels collected from the crowd can be of
low quality since crowdsourcing workers are often non-experts and can be unreliable. As a remedy 
most crowdsourcing services resort to labeling redundancy  collecting multiple labels from different
workers for each item. Such a strategy raises a fundamental problem in crowdsourcing: how to infer
true labels from noisy but redundant worker labels?
For labeling tasks with k different categories  Dawid and Skene [8] propose a maximum likelihood
approach based on the Expectation-Maximization (EM) algorithm. They assume that each worker
is associated with a k × k confusion matrix  where the (l  c)-th entry represents the probability that
a randomly chosen item in class l is labeled as class c by the worker. The true labels and work-
er confusion matrices are jointly estimated by maximizing the likelihood of the observed worker
labels  where the unobserved true labels are treated as latent variables. Although this EM-based
approach has had empirical success [21  20  19  26  6  25]  there is as yet no theoretical guarantee
for its performance. A recent theoretical study [10] shows that the global optimal solutions of the
Dawid-Skene estimator can achieve minimax rates of convergence in a simpliﬁed scenario  where
the labeling task is binary and each worker has a single parameter to represent her labeling accura-
cy (referred to as a “one-coin model” in what follows). However  since the likelihood function is
non-convex  this guarantee is not operational because the EM algorithm may get trapped in a local
optimum. Several alternative approaches have been developed that aim to circumvent the theoretical
deﬁciencies of the EM algorithm  still in the context of the one-coin model [14  15  11  7]. Unfor-

1

tunately  they either fail to achieve the optimal rates or depend on restrictive assumptions which are
hard to justify in practice.
We propose a computationally efﬁcient and provably optimal algorithm to simultaneously estimate
true labels and worker confusion matrices for multi-class labeling problems. Our approach is a
two-stage procedure  in which we ﬁrst compute an initial estimate of worker confusion matrices
using the spectral method  and then in the second stage we turn to the EM algorithm. Under some
mild conditions  we show that this two-stage procedure achieves minimax rates of convergence up
to a logarithmic factor  even after only one iteration of EM. In particular  given any δ ∈ (0  1) 
we provide the bounds on the number of workers and the number of items so that our method can
correctly estimate labels for all items with probability at least 1− δ. We also establish a lower bound
to demonstrate the optimality of this approach. Further  we provide both upper and lower bounds for
estimating the confusion matrix of each worker and show that our algorithm achieves the optimal
accuracy.
This work not only provides an optimal algorithm for crowdsourcing but sheds light on understand-
ing the general method of moments. Empirical studies show that when the spectral method is used
as an initialization for the EM algorithm  it outperforms EM with random initialization [18  5]. This
work provides a concrete way to theoretically justify such observations. It is also known that starting
from a root-n consistent estimator obtained by the spectral method  one Newton-Raphson step leads
to an asymptotically optimal estimator [17]. However  obtaining a root-n consistent estimator and
performing a Newton-Raphson step can be demanding computationally. In contrast  our initializa-
tion doesn’t need to be root-n consistent  thus a small portion of data sufﬁces to initialize. Moreover 
performing one iteration of EM is computationally more attractive and numerically more robust than
a Newton-Raphson step especially for high-dimensional problems.
2 Related Work
Many methods have been proposed to address the problem of estimating true labels in crowdsourcing
[23  20  22  11  19  26  7  15  14  25]. The methods in [20  11  15  19  14  7] are based on the
generative model proposed by Dawid and Skene [8].
In particular  Ghosh et al. [11] propose a
method based on Singular Value Decomposition (SVD) which addresses binary labeling problems
under the one-coin model. The analysis in [11] assumes that the labeling matrix is full  that is 
each worker labels all items. To relax this assumption  Dalvi et al. [7] propose another SVD-based
algorithm which explicitly considers the sparsity of the labeling matrix in both algorithm design
and theoretical analysis. Karger et al. propose an iterative algorithm for binary labeling problems
under the one-coin model [15] and extend it to multi-class labeling tasks by converting a k-class
problem into k − 1 binary problems [14]. This line of work assumes that tasks are assigned to
workers according to a random regular graph  thus imposing speciﬁc constraints on the number
of workers and the number of items. In Section 5  we compare our theoretical results with that
of existing approaches [11  7  15  14]. The methods in [20  19  6] incorporate Bayesian inference
into the Dawid-Skene estimator by assuming a prior over confusion matrices. Zhou et al.
[26 
25] propose a minimax entropy principle for crowdsourcing which leads to an exponential family
model parameterized with worker ability and item difﬁculty. When all items have zero difﬁculty  the
exponential family model reduces to the generative model suggested by Dawid and Skene [8].
Our method for initializing the EM algorithm in crowdsourcing is inspired by recent work using
spectral methods to estimate latent variable models [3  1  4  2  5  27  12  13]. The basic idea in this
line of work is to compute third-order empirical moments from the data and then to estimate param-
eters by computing a certain orthogonal decomposition of a tensor derived from the moments. Given
the special symmetric structure of the moments  the tensor factorization can be computed efﬁcient-
ly using the robust tensor power method [3]. A problem with this approach is that the estimation
error can have a poor dependence on the condition number of the second-order moment matrix and
thus empirically it sometimes performs worse than EM with multiple random initializations. Our
method  by contrast  requires only a rough initialization from the moment of moments; we show that
the estimation error does not depend on the condition number (see Theorem 2 (b)).

3 Problem Setup
Throughout this paper  [a] denotes the integer set {1  2  . . .   a} and σb(A) denotes the b-th largest
singular value of the matrix A. Suppose that there are m workers  n items and k classes. The true

2

h and

(1) Partition the workers into three disjoint and non-empty group G1  G2 and G3. Compute the
(2) For (a  b  c) ∈ {(2  3  1)  (3  1  2)  (1  2  3)}  compute the second and the third order moments

group aggregated labels Zgj by Eq. (1).

tensor decomposition:

Algorithm 1: Estimating confusion matrices
Input: integer k  observed labels zij ∈ Rk for i ∈ [m] and j ∈ [n].

Output: confusion matrix estimates (cid:98)Ci ∈ Rk×k for i ∈ [m].
(cid:99)M2 ∈ Rk×k (cid:99)M3 ∈ Rk×k×k by Eq. (2a)-(2d)  then compute (cid:98)C(cid:5)
(a) Compute whitening matrix (cid:98)Q ∈ Rk×k (such that (cid:98)QT(cid:99)M2(cid:98)Q = I) using SVD.
(b) Compute eigenvalue-eigenvector pairs {((cid:98)αh (cid:98)vh)}k
by using the robust tensor power method [3]. Then compute (cid:98)wh =(cid:98)α−2
h = ((cid:98)QT )−1((cid:98)αh(cid:98)vh).
(cid:98)µ(cid:5)
(c) For l = 1  . . .   k  set the l-th column of (cid:98)C(cid:5)
c by some(cid:98)µ(cid:5)
greatest component  then set the l-th diagonal entry of(cid:99)W by (cid:98)wh.
(3) Compute (cid:98)Ci by Eq. (3).
where {wl : l ∈ [k]} are positive values satisfying(cid:80)k

c ∈ Rk×k and(cid:99)W ∈ Rk×k by
h=1 of the whitened tensor(cid:99)M3((cid:98)Q (cid:98)Q (cid:98)Q)

label yj of item j ∈ [n] is assumed to be sampled from a probability distribution P[yj = l] = wl
l=1 wl = 1. Denote by a vector zij ∈ Rk
the label that worker i assigns to item j. When the assigned label is c  we write zij = ec  where ec
represents the c-th canonical basis vector in Rk in which the c-th entry is 1 and all other entries are
0. A worker may not label every item. Let πi indicate the probability that worker i labels a randomly
chosen item. If item j is not labeled by worker i  we write zij = 0. Our goal is to estimate the true
labels {yj : j ∈ [n]} from the observed labels {zij : i ∈ [m]  j ∈ [n]}.
In order to obtain an estimator  we need to make assumptions on the process of generating observed
labels. Following the work of Dawid and Skene [8]  we assume that the probability that worker i
labels an item in class l as class c is independent of any particular chosen item  that is  it is a constant
over j ∈ [n]. Let us denote the constant probability by µilc. Let µil = [µil1 µil2 ··· µilk]T . The
matrix Ci = [µi1 µi2 . . . µik] ∈ Rk×k is called the confusion matrix of worker i. Besides estimating
the true labels  we also want to estimate the confusion matrix for each worker.

h whose l-th coordinate has the

4 Our Algorithm

In this section  we present an algorithm to estimate confusion matrices and true labels. Our algorithm
consists of two stages. In the ﬁrst stage  we compute an initial estimate of confusion matrices via
the method of moments. In the second stage  we perform the standard EM algorithm by taking the
result of the Stage 1 as an initialization.

4.1 Stage 1: Estimating Confusion Matrices

Partitioning the workers into three disjoint and non-empty groups G1  G2 and G3  the outline of
this stage is the following: we use the spectral method to estimate the averaged confusion matrices
for the three groups  then utilize this intermediate estimate to obtain the confusion matrix of each
individual worker. In particular  for g ∈ {1  2  3} and j ∈ [n]  we calculate the averaged labeling
within each group by

(cid:88)

Zgj :=

1
|Gg|

3

i∈Gg
Denoting the aggregated confusion matrix columns by µ(cid:5)
our ﬁrst step is to estimate C(cid:5)

g2  . . .   µ(cid:5)

g := [µ(cid:5)

g1  µ(cid:5)

zij.
gl := E(Zgj|yj = l) = 1|Gg|

(cid:80)

πiµil 
gk] and to estimate the distribution of true labels

i∈Gg

(1)

W := diag(w1  w2  . . .   wk). The following proposition shows that we can solve for C(cid:5)
from the moments of {Zgj}.
Proposition 1 (Anandkumar et al. [3]). Assume that the vectors {µ(cid:5)
g2  . . .   µ(cid:5)
independent for each g ∈ {1  2  3}. Let (a  b  c) be a permutation of {1  2  3}. Deﬁne

g and W
gk} are linearly

g1  µ(cid:5)

then we have M2 =(cid:80)k

bj] and M3 := E[Z(cid:48)
l=1 wl µ(cid:5)

aj ⊗ Z(cid:48)
cl ⊗ µ(cid:5)

bj ⊗ Zcj];
cl ⊗ µ(cid:5)
cl.

Since we only have ﬁnite samples  the expectations in Proposition 1 have to be approximated by
empirical moments. In particular  they are computed by averaging over indices j = 1  2  . . .   n. For
(cid:17)−1
each permutation (a  b  c) ∈ {(2  3  1)  (3  1  2)  (1  2  3)}  we compute
(cid:17)−1

Zaj ⊗ Zbj

Zcj ⊗ Zbj

Zaj 

(2a)

j=1

n

n

Zbj ⊗ Zaj

Zbj 

−1 Zaj 
−1 Zbj 

aj ⊗ Z(cid:48)
cl ⊗ µ(cid:5)

aj := E[Zcj ⊗ Zbj] (E[Zaj ⊗ Zbj])
Z(cid:48)
bj := E[Zcj ⊗ Zaj] (E[Zbj ⊗ Zaj])
Z(cid:48)
cl and M3 =(cid:80)k
M2 := E[Z(cid:48)
l=1 wl µ(cid:5)
(cid:17)(cid:16) 1
(cid:16) 1
n(cid:88)
n(cid:88)
(cid:17)(cid:16) 1
(cid:16) 1
n(cid:88)
n(cid:88)
n(cid:88)
(cid:98)Z
aj ⊗ (cid:98)Z
n(cid:88)
(cid:98)Z
aj ⊗ (cid:98)Z

(cid:98)Z
(cid:98)Z
(cid:99)M2 :=
(cid:99)M3 :=

Zcj ⊗ Zaj

bj ⊗ Zcj.
(cid:48)

(cid:48)
aj :=

(cid:48)
bj :=

(cid:48)
bj 

1
n

j=1

j=1

j=1

j=1

n

(cid:48)

(cid:48)

n

1
n

j=1

(2b)

(2c)

(2d)

c and the diagonal

c . Thus  (cid:98)µ(cid:5)

The statement of Proposition 1 suggests that we can recover the columns of C(cid:5)

entries of W by operating on the moments (cid:99)M2 and (cid:99)M3. This is implemented by the tensor fac-
vectors {((cid:98)µ(cid:5)
h (cid:98)wh) estimates a particular column of C(cid:5)

torization method in Algorithm 1. In particular  the tensor factorization algorithm returns a set of
c (for
some µ(cid:5)
cl) and a particular diagonal entry of W (for some wl). It is important to note that the tensor
factorization algorithm doesn’t provide a one-to-one correspondence between the recovered colum-
n and the true columns of C(cid:5)
k represents an arbitrary permutation of the true
columns.

h (cid:98)wh) : h = 1  . . .   k}  where each ((cid:98)µ(cid:5)
1  . . .  (cid:98)µ(cid:5)

To discover the index correspondence  we take each (cid:98)µ(cid:5)
in the next section. As a consequence  if (cid:98)µ(cid:5)
coordinate is expected to be greater than other coordinates. Thus  we set the l-th column of (cid:98)C(cid:5)
some vector(cid:98)µ(cid:5)
then randomly select one of them; if there is no such vector  then randomly select a(cid:98)µ(cid:5)
set the l-th diagonal entry of (cid:99)W to the scalar (cid:98)wh associated with (cid:98)µ(cid:5)
(a  b  c) ∈ {(2  3  1)  (3  1  2)  (1  2  3)}  we obtain (cid:98)C(cid:5)
three copies of(cid:99)W estimating the same matrix W —we average them for the best accuracy.

h and examine its greatest component. We
assume that within each group  the probability of assigning a correct label is always greater than
the probability of assigning any speciﬁc incorrect label. This assumption will be made precise
c   then its l-th
c to
h whose l-th coordinate has the greatest component (if there are multiple such vectors 
h). Then  we
h. Note that by iterating over
c for c = 1  2  3 respectively. There will be

h corresponds to the l-th column of C(cid:5)

In the second step  we estimate each individual confusion matrix Ci. The following proposition
shows that we can recover Ci from the moments of {zij}. See [24] for the proof.
Proposition 2. For any g ∈ {1  2  3} and any i ∈ Gg  let a ∈ {1  2  3}\{g} be one of the remaining
group index. Then

a)T = E[zijZ T

of E[zijZ T

Proposition 2 suggests a plug-in estimator for Ci. We compute (cid:98)Ci using the empirical approximation
a  (cid:98)C(cid:5)
aj] and using the matrices (cid:98)C(cid:5)
b  (cid:99)W obtained in the ﬁrst step. Concretely  we calculate
(cid:16) 1
n(cid:88)
(cid:98)Ci := normalize

(cid:17)(cid:16)(cid:99)W ((cid:98)C(cid:5)

a)T(cid:17)−1

  

zijZ T
aj

πiCiW (C(cid:5)

aj].

(3)

n

j=1

4

where the normalization operator rescales the matrix columns  making sure that each column sums
to one. The overall procedure for Stage 1 is summarized in Algorithm 1.

4.2 Stage 2: EM algorithm

n(cid:89)

m(cid:89)

k(cid:89)

The second stage is devoted to reﬁning the initial estimate provided by Stage 1. The joint likelihood
of true label yj and observed labels zij  as a function of confusion matrices µi  can be written as

L(µ; y  z) :=

(µiyj c)

I(zij =ec).

log((cid:80)
takes the values {(cid:98)µilc} provided as output by Stage 1 as initialization  then executes the following

By assuming a uniform prior over y  we maximize the marginal log-likelihood function (cid:96)(µ) :=
y∈[k]n L(µ; y  z)). We reﬁne the initial estimate of Stage 1 by maximizing the objective func-
tion  which is implemented by the Expectation Maximization (EM) algorithm. The EM algorithm

j=1

c=1

i=1

E-step and M-step for at least one round.

E-step Calculate the expected value of the log-likelihood function  with respect to the conditional

(cid:98)qjl log

distribution of y given z under the current estimate of µ:

(cid:32) m(cid:89)
(cid:40) k(cid:88)
k(cid:89)
n(cid:88)
Q(µ) := Ey|zf (cid:98)µ [log(L(µ; y  z))] =
I(zij = ec) log((cid:98)µilc)(cid:1)
where (cid:98)qjl ← exp(cid:0)(cid:80)m
(cid:80)k
l(cid:48)=1 exp(cid:0)(cid:80)m
I(zij = ec) log((cid:98)µil(cid:48)c)(cid:1)
(cid:80)k
(cid:80)k
(cid:80)n
j=1(cid:98)qjlI(zij = ec)
(cid:80)k
(cid:80)n
j=1(cid:98)qjlI(zij = ec(cid:48))

M-step Find the estimate(cid:98)µ that maximizes the function Q(µ):

(cid:98)µilc ←

c(cid:48)=1

j=1

c=1

c=1

c=1

i=1

i=1

i=1

l=1

(cid:33)(cid:41)

I(zij =ec)

(µilc)

 

for j ∈ [n]  l ∈ [k].

for i ∈ [m]  l ∈ [k]  c ∈ [k].

(4)

(5)

In practice  we alternatively execute the updates (4) and (5)  for one iteration or until convergence.
Each update increases the objective function (cid:96)(µ). Since (cid:96)(µ) is not concave  the EM update doesn’t
guarantee converging to the global maximum. It may converge to distinct local stationary points for
different initializations. Nevertheless  as we prove in the next section  it is guaranteed that the EM
algorithm will output statistically optimal estimates of true labels and worker confusion matrices if
it is initialized by Algorithm 1.

5 Convergence Analysis

To state our main theoretical results  we ﬁrst need to introduce some notation and assumptions. Let

wmin := min{wl}k

l=1

and πmin := min{πi}m

i=1

be the smallest portion of true labels and the most extreme sparsity level of workers. Our ﬁrst
assumption assumes that both wmin and πmin are strictly positive  that is  every class and every
worker contributes to the dataset.
Our second assumption assumes that the confusion matrices for each of the three groups  namely
1   C(cid:5)
C(cid:5)
3   are nonsingular. As a consequence  if we deﬁne matrices Sab and tensors Tabc for
any a  b  c ∈ {1  2  3} as

2 and C(cid:5)

wl µ(cid:5)

al ⊗ µ(cid:5)

bl = C(cid:5)

aW (C(cid:5)
b )T

l=1

Sab :=

and Tabc :=
then there will be a positive scalar σL such that σk(Sab) ≥ σL > 0.
Our third assumption assumes that within each group  the average probability of assigning a correct
label is always higher than the average probability of assigning any incorrect label. To make this

l=1

wl µ(cid:5)

al ⊗ µ(cid:5)

bl ⊗ µ(cid:5)
cl 

k(cid:88)

k(cid:88)

5

statement rigorous  we deﬁne a quantity
κ := min

g∈{1 2 3} min
l∈[k]

min

c∈[k]\{l}

{µ(cid:5)

gll − µ(cid:5)

glc}

labels. For two discrete distributions P and Q  let DKL (P  Q) :=(cid:80)

indicating the smallest gap between diagonal entries and non-diagonal entries in the same confusion
matrix column. The assumption requires κ being strictly positive. Note that this assumption is
group-based  thus does not assume the accuracy of any individual worker.
Finally  we introduce a quantity that measures the average ability of workers in identifying distinct
i P (i) log(P (i)/Q(i)) repre-
sent the KL-divergence between P and Q. Since each column of the confusion matrix represents a
discrete distribution  we can deﬁne the following quantity:

m(cid:88)

i=1

D = min
l(cid:54)=l(cid:48)

1
m

πiDKL (µil  µil(cid:48)) .

(6)

The quantity D lower bounds the averaged KL-divergence between two columns. If D is strictly
positive  it means that every pair of labels can be distinguished by at least one subset of workers. As
the last assumption  we assume that D is strictly positive.
The following two theorems characterize the performance of our algorithm. We split the conver-
gence analysis into two parts. Theorem 1 characterizes the performance of Algorithm 1  providing
sufﬁcient conditions for achieving an arbitrarily accurate initialization. We provide the proof of
Theorem 1 in the long version of this paper [24].
Theorem 1. For any scalar δ > 0 and any scalar  satisfying  ≤ min
number of items n satisﬁes

πminwminσL

  if the

(cid:110)

(cid:111)

36κk

  2

(cid:18) k5 log((k + m)/δ)

(cid:19)

 

n = Ω

then the confusion matrices returned by Algorithm 1 are bounded as
for all i ∈ [m] 

(cid:107)(cid:98)Ci − Ci(cid:107)∞ ≤ 

2π2

minw2

minσ13
L

with probability at least 1 − δ. Here  (cid:107) · (cid:107)∞ denotes the element-wise (cid:96)∞-norm of a matrix.

It states that when a sufﬁciently accurate

See the long version of this paper [24] for the proof.

Theorem 2 characterizes the error rate in Stage 2.

Theorem 2. Assume that there is a positive scalar ρ such that µilc ≥ ρ for all (i  l  c) ∈ [m] × [k]2.

initialization is taken  the updates (4) and (5) reﬁne the estimates(cid:98)µ and(cid:98)y to the optimal accuracy.
For any scalar δ > 0  if confusion matrices (cid:98)Ci are initialized in a manner such that
(cid:18) log(mk/δ)

(cid:26) ρ
(cid:107)(cid:98)Ci − Ci(cid:107)∞ ≤ α := min
(cid:18) log(1/ρ) log(kn/δ) + log(mn)

and the number of workers m and the number of items n satisfy

1 − δ 

and n = Ω

then  for(cid:98)µ and(cid:98)q obtained by iterating (4) and (5) (for at least one round)  with probability at least
(a) Letting(cid:98)yj = arg maxl∈[k](cid:98)qjl  we have that(cid:98)yj = yj holds for all j ∈ [n].
(b) (cid:107)(cid:98)µil − µil(cid:107)2

holds for all (i  l) ∈ [m] × [k].

2 ≤ 48 log(2mk/δ)

πminwminα2

for all i ∈ [m] 

ρD
16

m = Ω

(cid:27)

(cid:19)

(cid:19)

πiwln

(7)

D

2

 

 

In Theorem 2  the assumption that all confusion matrix entries are lower bounded by ρ > 0 is
somewhat restrictive. For datasets violating this assumption  we enforce positive confusion matrix
entries by adding random noise: Given any observed label zij  we replace it by a random label in
{1  ...  k} with probability kρ. In this modiﬁed model  every entry of the confusion matrix is lower
bounded by ρ  so that Theorem 2 holds. The random noise makes the constant D smaller than its
original value  but the change is minor for small ρ.

6

Dataset name

Bird
RTE
TREC
Dog
Web

# classes

2
2
2
4
5

# items

108
800

19 033

807
2 665

# workers

39
164
762
52
177

# worker labels

4 212
8 000
88 385
7 354
15 567

Table 1: Summary of datasets used in the real data experiment.

(cid:19)

To see the consequence of the convergence analysis  we take error rate  in Theorem 1 equal to the
constant α deﬁned in Theorem 2. Then we combine the statements of the two theorems. This shows
that if we choose the number of workers m and the number of items n such that
L min{ρ2  (ρD)2}

(cid:18) 1
m =(cid:101)Ω
then with high probability  the predictor (cid:98)y will be perfectly accurate  and the estimator (cid:98)µ will be
2 ≤ (cid:101)O(1/(πiwln)). To show the optimality of this convergence rate  we
bounded as (cid:107)(cid:98)µil − µil(cid:107)2

that is  if both m and n are lower bounded by a problem-speciﬁc constant and logarithmic terms 

and n =(cid:101)Ω

present the following minimax lower bounds. Again  see [24] for the proof.

π2
minw2

minσ13

(cid:19)

(cid:18)

(8)

k5

D

;

Theorem 3. There are universal constants c1 > 0 and c2 > 0 such that:
(a) For any {µilc}  {πi} and any number of items n  if the number of workers m ≤ 1/(4D)  then

(b) For any {wl}  {πi}  any worker-item pair (m  n) and any pair of indices (i  l) ∈ [m] × [k]  we

inf(cid:98)y

sup
v∈[k]n

have

inf(cid:98)µ

sup

µ∈Rm×k×k

E(cid:104) n(cid:88)
I((cid:98)yj (cid:54)= yj)
E(cid:104)(cid:107)(cid:98)µil − µil(cid:107)2

(cid:12)(cid:12)(cid:12){µilc} {πi}  y = v
(cid:12)(cid:12)(cid:12){wl} {πi}(cid:105) ≥ c2 min

j=1

2

(cid:105) ≥ c1n.
(cid:26)

1

1 

(cid:27)

.

πiwln

In part (a) of Theorem 3  we see that the number of workers should be at least 1/(4D)  otherwise
any predictor will make many mistakes. This lower bound matches our sufﬁcient condition on the
number of workers m (see Eq. (8)). In part (b)  we see that the best possible estimate for µil has

Ω(1/(πiwln)) mean-squared error. It veriﬁes the optimality of our estimator(cid:98)µil. It is worth noting

that the constraint on the number of items n (see Eq. (8)) might be improvable. In real datasets we
usually have n (cid:29) m so that the optimality for m is more important than for n.
It is worth contrasting our convergence rate with existing algorithms. Ghosh et al. [11] and Dalvi et
al. [7] proposed consistent estimators for the binary one-coin model. To attain an error rate δ  their
algorithms require m and n scaling with 1/δ2  while our algorithm only requires m and n scaling
with log(1/δ). Karger et al. [15  14] proposed algorithms for both binary and multi-class problems.
Their algorithm assumes that workers are assigned by a random regular graph. Moreover  their
analysis assumes that the limit of number of items goes to inﬁnity  or that the number of workers is
many times the number of items. Our algorithm no longer requires these assumptions.
We also compare our algorithm with the majority voting estimator  where the true label is simply
estimated by a majority vote among workers. Gao and Zhou [10] showed that if there are many
spammers and few experts  the majority voting estimator gives almost a random guess. In con-

trast  our algorithm only requires mD = (cid:101)Ω(1) to guarantee good performance. Since mD is the

aggregated KL-divergence  a small number of experts are sufﬁcient to ensure it is large enough.

6 Experiments

In this section  we report the results of empirical studies comparing the algorithm we propose in
Section 4 (referred to as Opt-D&S) with a variety of existing methods which are also based on the
generative model of Dawid and Skene. Speciﬁcally  we compare to the Dawid & Skene estimator

7

(a) RTE

(b) Dog

(c) Web

Figure 1: Comparing MV-D&S and Opt-D&S with different thresholding parameter ∆. The label
prediction error is plotted after the 1st EM update and after convergence.

Opt-D&S MV-D&S Majority Voting KOS Ghosh-SVD EigenRatio

Bird
RTE
TREC
Dog
Web

10.09
7.12
29.80
16.89
15.86

11.11
7.12
30.02
16.66
15.74

24.07
10.31
34.86
19.58
26.93

11.11
39.75
51.96
31.72
42.93

27.78
49.13
42.99

–
–

27.78
9.00
43.96

–
–

Table 2: Error rate (%) in predicting true labels on real data.

initialized by majority voting (referred to as MV-D&S)  the pure majority voting estimator  the
multi-class labeling algorithm proposed by Karger et al. [14] (referred to as KOS)  the SVD-based
algorithm proposed by Ghosh et al. [11] (referred to as Ghost-SVD) and the “Eigenvalues of Ratio”
algorithm proposed by Dalvi et al. [7] (referred to as EigenRatio). The evaluation is made on ﬁve
real datasets.
We compare the crowdsourcing algorithms on three binary tasks and two multi-class tasks. Binary
tasks include labeling bird species [22] (Bird dataset)  recognizing textual entailment [21] (RTE
dataset) and assessing the quality of documents in the TREC 2011 crowdsourcing track [16] (TREC
dataset). Multi-class tasks include labeling the breed of dogs from ImageNet [9] (Dog dataset) and
judging the relevance of web search results [26] (Web dataset). The statistics for the ﬁve datasets
are summarized in Table 1. Since the Ghost-SVD algorithm and the EigenRatio algorithm work on
binary tasks  they are evaluated only on the Bird  RTE and TREC datasets. For the MV-D&S and
the Opt-D&S methods  we iterate their EM steps until convergence.
Since entries of the confusion matrix are positive  we ﬁnd it helpful to incorporate this prior knowl-
edge into the initialization stage of the Opt-D&S algorithm. In particular  when estimating the con-
fusion matrix entries by Eq. (3)  we add an extra checking step before the normalization  examining
if the matrix components are greater than or equal to a small threshold ∆. For components that are
smaller than ∆  they are reset to ∆. The default choice of the thresholding parameter is ∆ = 10−6.
Later  we will compare the Opt-D&S algorithm with respect to different choices of ∆. It is impor-
tant to note that this modiﬁcation doesn’t change our theoretical result  since the thresholding is not
needed in case that the initialization error is bounded by Theorem 1.
Table 2 summarizes the performance of each method. The MV-D&S and the Opt-D&S algorithms
consistently outperform the other methods in predicting the true label of items. The KOS algorithm 
the Ghost-SVD algorithm and the EigenRatio algorithm yield poorer performance  presumably due
to the fact that they rely on idealized assumptions that are not met by the real data. In Figure 1  we
compare the Opt-D&S algorithm with respect to different thresholding parameters ∆ ∈ {10−i}6
i=1.
We plot results for three datasets (RET  Dog  Web)  where the performance of MV-D&S is equal to or
slightly better than that of Opt-D&S. The plot shows that the performance of the Opt-D&S algorithm
is stable after convergence. But at the ﬁrst EM iterate  the error rates are more sensitive to the choice
of ∆. A proper choice of ∆ makes Opt-D&S outperform MV-D&S. The result suggests that a
proper initialization combined with one EM iterate is good enough for the purposes of prediction.
In practice  the best choice of ∆ can be obtained by cross validation.

8

10−610−510−410−310−210−10.080.10.120.140.160.180.20.22ThresholdLabel prediction error Opt−D&S: 1st iterationOpt−D&S: 50th iterationMV−D&S: 1st iterationMV−D&S: 50th iteration10−610−510−410−310−210−10.150.160.170.180.190.20.21ThresholdLabel prediction error Opt−D&S: 1st iterationOpt−D&S: 50th iterationMV−D&S: 1st iterationMV−D&S: 50th iteration10−610−510−410−310−210−10.150.20.250.30.35ThresholdLabel prediction error Opt−D&S: 1st iterationOpt−D&S: 50th iterationMV−D&S: 1st iterationMV−D&S: 50th iterationReferences
[1] A. Anandkumar  D. P. Foster  D. Hsu  S. M. Kakade  and Y.-K. Liu. A spectral algorithm for latent

Dirichlet allocation. arXiv preprint: 1204.6703  2012.

[2] A. Anandkumar  R. Ge  D. Hsu  and S. M. Kakade. A tensor spectral approach to learning mixed mem-

bership community models. In Annual Conference on Learning Theory  2013.

[3] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor decompositions for learning

latent variable models. arXiv preprint:1210.7559  2012.

[4] A. Anandkumar  D. Hsu  and S. M. Kakade. A method of moments for mixture models and hidden

Markov models. In Annual Conference on Learning Theory  2012.

[5] A. T. Chaganty and P. Liang. Spectral experts for estimating mixtures of linear regressions. arXiv preprint:

1306.3729  2013.

[6] X. Chen  Q. Lin  and D. Zhou. Optimistic knowledge gradient policy for optimal budget allocation in

crowdsourcing. In Proceedings of ICML  2013.

[7] N. Dalvi  A. Dasgupta  R. Kumar  and V. Rastogi. Aggregating crowdsourced binary ratings. In Proceed-

ings of World Wide Web Conference  2013.

[8] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the EM

algorithm. Journal of the Royal Statistical Society  Series C  pages 20–28  1979.

[9] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image

database. In IEEE CVPR  2009.

[10] C. Gao and D. Zhou. Minimax optimal convergence rates for estimating ground truth from crowdsourced

labels. arXiv preprint arXiv:1310.5764  2014.

[11] A. Ghosh  S. Kale  and P. McAfee. Who moderates the moderators? crowdsourcing abuse detection in

user-generated content. In Proceedings of the ACM Conference on Electronic Commerce  2011.

[12] D. Hsu  S. M. Kakade  and T. Zhang. A spectral algorithm for learning hidden Markov models. Journal

of Computer and System Sciences  78(5):1460–1480  2012.

[13] P. Jain and S. Oh. Learning mixtures of discrete product distributions using spectral decompositions.

arXiv preprint:1311.2972  2013.

[14] D. R. Karger  S. Oh  and D. Shah. Efﬁcient crowdsourcing for multi-class labeling. In ACM SIGMET-

RICS  2013.

[15] D. R. Karger  S. Oh  and D. Shah. Budget-optimal task allocation for reliable crowdsourcing systems.

Operations Research  62(1):1–24  2014.

[16] M. Lease and G. Kazai. Overview of the TREC 2011 crowdsourcing track. In Proceedings of TREC 2011 

2011.

[17] E. Lehmann and G. Casella. Theory of Point Estimation. Springer  2nd edition  2003.
[18] P. Liang. Partial information from spectral methods. NIPS Spectral Learning Workshop  2013.
[19] Q. Liu  J. Peng  and A. T. Ihler. Variational inference for crowdsourcing. In NIPS  2012.
[20] V. C. Raykar  S. Yu  L. H. Zhao  G. H. Valadez  C. Florin  L. Bogoni  and L. Moy. Learning from crowds.

Journal of Machine Learning Research  11:1297–1322  2010.

[21] R. Snow  B. O’Connor  D. Jurafsky  and A. Y. Ng. Cheap and fast—but is it good? evaluating non-expert

annotations for natural language tasks. In Proceedings of EMNLP  2008.

[22] P. Welinder  S. Branson  S. Belongie  and P. Perona. The multidimensional wisdom of crowds. In NIPS 

2010.

[23] J. Whitehill  P. Ruvolo  T. Wu  J. Bergsma  and J. R. Movellan. Whose vote should count more: Optimal

integration of labels from labelers of unknown expertise. In NIPS  2009.

[24] Y. Zhang  X. Chen  D. Zhou  and M. I. Jordan. Spectral methods meet EM: A provably optimal algorithm

for crowdsourcing. arXiv preprint arXiv:1406.3824  2014.

[25] D. Zhou  Q. Liu  J. C. Platt  and C. Meek. Aggregating ordinal labels from crowds by minimax conditional

entropy. In Proceedings of ICML  2014.

[26] D. Zhou  J. C. Platt  S. Basu  and Y. Mao. Learning from the wisdom of crowds by minimax entropy. In

NIPS  2012.

[27] J. Zou  D. Hsu  D. Parkes  and R. Adams. Contrastive learning using spectral methods. In NIPS  2013.

9

,Yuchen Zhang
Xi Chen
Dengyong Zhou
Michael Jordan
Kristofer Bouchard
Alejandro Bujan
Fred Roosta
Shashanka Ubaru
Mr. Prabhat
Michael Mahoney
Sharmodeep Bhattacharya