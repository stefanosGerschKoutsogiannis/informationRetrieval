2017,Differentially Private Empirical Risk Minimization Revisited: Faster and More General,In this paper we study differentially private Empirical Risk Minimization(ERM) in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization  we give algorithms which achieve either optimal or near optimal utility bound with less gradient complexity compared with previous work.  For ERM with smooth convex loss function in high-dimension($p\gg n$) setting  we give an algorithm which achieves the upper bound with less gradient complexity than previous ones. At last  we generalize the expected excess empirical risk from convex to Polyak-Lojasiewicz condition and give a tighter upper bound of the utility comparing with the result in \cite{DBLP:journals/corr/ZhangZMW17}.,Differentially Private Empirical Risk Minimization

Revisited: Faster and More General∗

Di Wang

Minwei Ye

Dept. of Computer Science and Engineering

State University of New York at Buffalo

Dept. of Computer Science and Engineering

State University of New York at Buffalo

Buffalo  NY 14260

dwang45@buffalo.edu

Buffalo  NY 14260

minweiye@buffalo.edu

Jinhui Xu

Dept. of Computer Science and Engineering

State University of New York at Buffalo

Buffalo  NY 14260

jinhui@buffalo.edu

Abstract

In this paper we study the differentially private Empirical Risk Minimization
(ERM) problem in different settings. For smooth (strongly) convex loss function
with or without (non)-smooth regularization  we give algorithms that achieve either
optimal or near optimal utility bounds with less gradient complexity compared with
previous work. For ERM with smooth convex loss function in high-dimensional
(p (cid:29) n) setting  we give an algorithm which achieves the upper bound with less
gradient complexity than previous ones. At last  we generalize the expected excess
empirical risk from convex loss functions to non-convex ones satisfying the Polyak-
Lojasiewicz condition and give a tighter upper bound on the utility than the one in
[34].

1

Introduction

Privacy preserving is an important issue in learning. Nowadays  learning algorithms are often required
to deal with sensitive data. This means that the algorithm needs to not only learn effectively from
the data but also provide a certain level of guarantee on privacy preserving. Differential privacy is a
rigorous notion for statistical data privacy and has received a great deal of attentions in recent years
[11  10]. As a commonly used supervised learning method  Empirical Risk Minimization (ERM)
also faces the challenge of achieving simultaneously privacy preserving and learning. Differentially
Private (DP) ERM with convex loss function has been extensively studied in the last decade  starting
from [7]. In this paper  we revisit this problem and present several improved results.
Problem Setting Given a dataset D = {z1  z2 ···   zn} from a data universe X   and a closed
convex set C ⊆ Rp  DP-ERM is to ﬁnd

n(cid:88)

i=1

x∗ ∈ arg min

x∈C F r(x  D) = F (x  D) + r(x) =

1
n

f (x  zi) + r(x)

with the guarantee of being differentially private. We refer to f as loss function. r(·) is some simple
(non)-smooth convex function called regularizer. If the loss function is convex  the utility of the
∗This research was supported in part by NSF through grants IIS-1422591  CCF-1422324  and CCF-1716400.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

[8][7]

[21]

[6]

[34]

Method

Objective Perturbation

Objective Perturbation O( p

Gradient Perturbation

Output Perturbation

n22 )

O( p
n22 + λ||x∗||2
O( p log2(n)

n

)

n22
O( p

n22 )

N/A

)

N/A

O(n2)

O(nκ log( n

κ ))

Utility Upper Bd

Gradient Complexity Non smooth Regularizer?

No

Yes

Yes

No

Yes

This Paper Gradient Perturbation

O( p log(n)
n22 )

O((n + κ) log( nµ

p ))

p

n22})[6].

Table 1: Comparison with previous (  δ)-DP algorithms. We assume that the loss function f is
convex  1-smooth  differentiable (twice differentiable for objective perturbation)  and 1-Lipschitz. F r
is µ-strongly convex. Bound and complexity ignore multiplicative dependence on log(1/δ). κ = L
µ
is the condition number. The lower bound is Ω(min{1 
algorithm is measured by the expected excess empirical risk  i.e. E[F r(xprivate  D)]− F r(x∗  D). The
expectation is over the coins of the algorithm.
A number of approaches exist for this problem with convex loss function  which can be roughly
classiﬁed into three categories. The ﬁrst type of approaches is to perturb the output of a non-DP
algorithm. [7] ﬁrst proposed output perturbation approach which is extended by [34]. The second
type of approaches is to perturb the objective function [7]. We referred to it as objective perturbation
approach. The third type of approaches is to perturb gradients in ﬁrst order optimization algorithms.
[6] proposed gradient perturbation approach and gave the lower bound of the utility for both general
convex and strongly convex loss functions. Later  [28] showed that this bound can actually be broken
by adding more restrictions on the convex domain C of the problem.
As shown in the following tables2   the output perturbation approach can achieve the optimal bound of
utility for strongly convex case. But it cannot be generalized to the case with non-smooth regularizer.
The objective perturbation approach needs to obtain the optimal solution to ensure both differential
privacy and utility  which is often intractable in practice  and cannot achieve the optimal bound. The
gradient perturbation approach can overcome all the issues and thus is preferred in practice. However 
its existing results are all based on Gradient Descent (GD) or Stochastic Gradient Descent (SGD).
For large datasets  they are slow in general. In the ﬁrst part of this paper  we present algorithms
with tighter utility upper bound and less running time. Almost all the aforementioned results did
not consider the case where the loss function is non-convex. Recently  [34] studied this case and
measured the utility by gradient norm. In the second part of this paper  we generalize the expected
excess empirical risk from convex to Polyak-Lojasiewicz condition  and give a tighter upper bound of
the utility given in [34]. Due to space limit  we leave many details  proofs  and experimental studies
in the supplement.

2 Related Work

There is a long list of works on differentially private ERM in the last decade which attack the problem
from different perspectives. [17][30] and [2] investigated regret bound in online settings. [20] studied
regression in incremental settings. [32] and [31] explored the problem from the perspective of
learnability and stability. We will compare to the works that are most related to ours from the utility
and gradient complexity (i.e.  the number (complexity) of ﬁrst order oracle (f (x  zi) ∇f (x  zi))
being called) points of view. Table 1 is the comparison for the case that loss function is strongly
convex and 1-smooth. Our algorithm achieves near optimal bound with less gradient complexity
compared with previous ones. It is also robust to non-smooth regularizers.
Tables 2 and 3 show that for non-strongly convex and high-dimension cases  our algorithms outper-
form other peer methods. Particularly  we improve the gradient complexity from O(n2) to O(n log n)
while preserving the optimal bound for non-strongly convex case. For high-dimension case  gradient
complexity is reduced from O(n3) to O(n1.5). Note that [19] also considered high-dimension case

2 Bound and complexity ignore multiplicative dependence on log(1/δ).

2

[21]

[6]

[34]

Method

Objective Perturbation

Gradient Perturbation

O(

Output Perturbation

O([

This paper Gradient Perturbation

Utility Upper Bd Gradient Complexity Non smooth Regularizer?

√
p
n )

O(
√

)

N/A

O(n2)

O(n[ n

d ] 2
3 )

O( n√

p + n log( n

p ))

Yes

Yes

No

Yes

p log3/2(n)

p

n
√
n ] 2
3 )
√
p
n )

O(

Table 2: Comparison with previous (  δ)-DP algorithms  where F r is not necessarily strongly convex.
We assume that the loss function f is convex  1-smooth  differentiable( twice differentiable for
objective perturbation)  and 1-Lipschitz. Bound and complexity ignore multiplicative dependence on
log(1/δ). The lower bound in this case is Ω(min{1 

p

√
n })[6].

via dimension reduction. But their method requires the optimal value in the dimension-reduced space 
in addition they considered loss functions under the condition rather than (cid:96)2- norm Lipschitz.
For non-convex problem under differential privacy  [15][9][13] studied private SVD. [14] investigated
k-median clustering. [34] studied ERM with non-convex smooth loss functions. In [34]  the authors
deﬁned the utility using gradient norm as E[||∇F (xprivate)||2]. They achieved a qualiﬁed utility in
O(n2) gradient complexity via DP-SGD. In this paper  we use DP-GD and show that it has a tighter
utility upper bound.

Utility Upper Bd

Method
Gradient Perturbation
Objective Perturbation O( GC+λ||C||2

O(

n

G2C+||C||2 log(n)

)

√

n

Gradient Complexity Non smooth Regularizer?

) O(

n32

(G2C+||C||2) log2(n) ) Yes
No

N/A

[28]

[28]

[29]

Gradient Perturbation

This paper Gradient Perturbation

O( (G
√

O(

2

3C log2(n))
(n)

2
3

G2C+||C||2

n

)

O( (n)
2
3C
G

(cid:18)

2
3

)

O

)
n1.5√



(G2C+||C||2)

(cid:19)

1
4

Yes

No

Table 3: Comparison with previous (  δ)-DP algorithms. We assume that the loss function f is
convex  1-smooth  differentiable( twice differentiable for objective perturbation)  and 1-Lipschitz.
The utility bound depends on GC  which is the Gaussian width of C. Bound and complexity ignore
multiplicative dependence on log(1/δ).

3 Preliminaries
Notations: We let [n] denote {1  2  . . .   n}. Vectors are in column form. For a vector v  we use
||v||2 to denote its (cid:96)2-norm. For the gradient complexity notation  G  δ   are omitted unless speciﬁed.
D = {z1 ···   zn} is a dataset of n individuals.
Deﬁnition 3.1 (Lipschitz Function over θ). A loss function f : C × X → R is G-Lipschitz (under
(cid:96)2-norm) over θ  if for any z ∈ X and θ1  θ2 ∈ C  we have |f (θ1  z) − f (θ2  z)| ≤ G||θ1 − θ2||2.
Deﬁnition 3.2 (L-smooth Function over θ). A loss function f : C × X → R is L-smooth over θ with
respect to the norm || · || if for any z ∈ X and θ1  θ2 ∈ C  we have

||∇f (θ1  z) − ∇f (θ2  z)||∗ ≤ L||θ1 − θ2|| 
where || · ||∗ is the dual norm of || · ||. If f is differentiable  this yields
L
2

f (θ1  z) ≤ f (θ2  z) + (cid:104)∇f (θ2  z)  θ1 − θ2(cid:105) +

||θ1 − θ2||2.

We say that two datasets D  D(cid:48) are neighbors if they differ by only one entry  denoted as D ∼ D(cid:48).
Deﬁnition 3.3 (Differentially Private[11]). A randomized algorithm A is (  δ)-differentially private
if for all neighboring datasets D  D(cid:48) and for all events S in the output space of A  we have

P r(A(D) ∈ S) ≤ eP r(A(D(cid:48)) ∈ S) + δ 

3

when δ = 0 and A is -differentially private.

We will use Gaussian Mechanism [11] and moments accountant [1] to guarantee (  δ)-DP.
Deﬁnition 3.4 (Gaussian Mechanism). Given any function q : X n → Rp  the Gaussian Mechanism
is deﬁned as:

MG(D  q  ) = q(D) + Y 

√

where Y is drawn from Gaussian Distribution N (0  σ2Ip) with σ ≥
. Here ∆2(q)
is the (cid:96)2-sensitivity of the function q  i.e. ∆2(q) = supD∼D(cid:48) ||q(D)−q(D(cid:48))||2. Gaussian Mechanism
preservers (  δ)-differentially private.

2 ln(1.25/δ)∆2(q)



The moments accountant proposed in [1] is a method to accumulate the privacy cost which has tighter
bound for  and δ. Roughly speaking  when we use the Gaussian Mechanism on the (stochastic)

gradient descent  we can save a factor of(cid:112)ln(T /δ) in the asymptotic bound of standard deviation of

noise compared with the advanced composition theorem in [12].
Theorem 3.1 ([1]). For G-Lipschitz loss function  there exist constants c1 and c2 so that given the
sampling probability q = l/n and the number of steps T  for any  < c1q2T   a DP stochastic gradient
algorithm with batch size l that injects Gaussian Noise with standard deviation G
n σ to the gradients
(Algorithm 1 in [1])  is (  δ)-differentially private for any δ > 0 if

q(cid:112)T ln(1/δ)

.



σ ≥ c2

4 Differentially Private ERM with Convex Loss Function

In this section we will consider ERM with (non)-smooth regularizer3  i.e.

n(cid:88)

i=1

F r(x  D) = F (x  D) + r(x) =

min
x∈Rp

1
n

f (x  zi) + r(x).

(1)

The loss function f is convex for every z. We deﬁne the proximal operator as

proxr(y) = arg min
x∈Rp

||x − y||2

2 + r(x)} 

{ 1
2

and denote x∗ = arg minx∈Rp F r(x  D).

˜x = ˜xs−1
˜v = ∇F (˜x)
xs
0 = ˜x
for t = 1  2 ···   m do

Algorithm 1 DP-SVRG(F r  ˜x0  T  m  η  σ)
Input: f (x  z) is G-Lipschitz and L-smooth. F r(x  D) is µ-strongly convex w.r.t (cid:96)2-norm. ˜x0 is the
initial point  η is the step size  T  m are the iteration numbers.
1: for s = 1  2 ···   T do
2:
3:
4:
5:
t ∈ [n]
Pick is
6:
t = ∇f (xs
vs
7:
(cid:80)m
t = proxηr(xs
xs
8:
end for
9:
˜xs = 1
10:
m
11: end for
12: return ˜xT

t ∼ N (0  σ2Ip)

) − ∇f (˜x  zis

t−1 − ηvs
t )

t   where us

) + ˜v + us

t−1  zis

t

k=1 xs
k

t

3 All of the algorithms and theorems in this section are applicable to closed convex set C rather than Rp.

4

4.1 Strongly convex case

We ﬁrst consider the case that F r(x  D) is µ-strongly convex  Algorithm 1 is based on the Prox-
SVRG [33]  which is much faster than SGD or GD. We will show that DP-SVRG is also faster than
DP-SGD or DP-GD in terms of the time needed to achieve the near optimal excess empirical risk
bound.
Deﬁnition 4.1 (Strongly Convex). The function f (x) is µ-strongly convex with respect to norm || · ||
if for any x  y ∈ dom(f )  there exist µ > 0 such that

f (y) ≥ f (x) + (cid:104)∂f  y − x(cid:105) +

||y − x||2 

µ
2

where ∂f is any subgradient on x of f.
Theorem 4.1. In DP-SVRG(Algorithm 1)  for  ≤ c1
(  δ)-differentially private if

T m

n2 with some constant c1 and δ > 0  it is

(2)

(3)

σ2 = c

G2T m ln( 1
δ )

n22

L ) ≤ 1
(cid:16)

1

η(1 − 8ηL)µm

+

for some constant c.
Remark 4.1. The constraint on  in Theorems 4.1 and 4.3 comes from Theorem 3.1. This constraint
can be removed if the noise σ is ampliﬁed by a factor of O(ln(T /δ)) in (3) and (6). But accordingly
there will be a factor of ˜O(log(T m/δ)) in the utility bound in (5) and (7). In this case the guarantee
of differential privacy is by advanced composition theorem and privacy ampliﬁcation via sampling[6].
Theorem 4.2 (Utility guarantee). Suppose that the loss function f (x  z) is convex  G-Lipschitz and
L-smooth over x. F r(x  D) is µ-strongly convex w.r.t (cid:96)2-norm. In DP-SVRG(Algorithm 1)  let σ
be as in (3). If one chooses η = Θ( 1
µ ) so that they satisfy
inequality

12L and sufﬁciently large m = Θ( L

 

<

1
2

8Lη(m + 1)
m(1 − 8Lη)

(cid:17)
(cid:18) p log(n)G2 log(1/δ)

 

n22µ

(cid:19)

 

(4)

(5)

then the following holds for T = O

n22µ

log(

pG2 ln(1/δ) )
E[F r(˜xT   D)] − F r(x∗  D) ≤ ˜O

(cid:16)

(cid:17)

(n + L

where some insigniﬁcant logarithm terms are hiding in the ˜O-notation. The total gradient complexity
is O
Remark 4.2. We can further use some acceleration methods to reduce the gradient complexity  see
[25][3].

µ ) log nµ

.

p

4.2 Non-strongly convex case

In some cases  F r(x  D) may not be strongly convex. For such cases  [5] has recently showed that
SVRG++ has less gradient complexity than Accelerated Gradient Descent. Following the idea of
DP-SVRG  we present the algorithm DP-SVRG++ for the non-strongly convex case. Unlike the
previous one  this algorithm can achieve the optimal utility bound.
Theorem 4.3. In DP-SVRG++(Algorithm 2)  for  ≤ c1
(  δ)-differentially private if

2T m
n2 with some constant c1 and δ > 0  it is

σ2 = c

G22T m ln( 2
δ )

n22

(6)

for some constant c.
Theorem 4.4 (Utility guarantee). Suppose that the loss function f (x  z) is convex  G-Lipschitz and
L-smooth. In DP-SVRG++(Algorithm 2)  if σ is chosen as in (6)  η = 1
13L  and m = Θ(L) is
 

sufﬁciently large  then the following holds for T = O

(cid:18)

log(

E[F r(˜xT   D)] − F r(x∗  D) ≤ O

.

(7)

(cid:19)
(cid:33)

)

(cid:32)

√

√

n
log(1/δ)

G(cid:112)p ln(1/δ))

G

p

n

The gradient complexity is O

p + n log( n
p )

.

(cid:16) nL√

(cid:17)

5

Algorithm 2 DP-SVRG++(F r  ˜x0  T  m  η  σ)
Input:f (x  z) is G-Lipschitz  and L-smooth over x ∈ C. ˜x0 is the initial point  η is the step size  and
T  m are the iteration numbers.

x1
0 = ˜x0
for s = 1  2 ···   T do

˜v = ∇F (˜xs−1)
ms = 2sm
for t = 1  2 ···   ms do

t ∈ [n]
Pick is
t = ∇f (xs
vs
(cid:80)ms
t = proxηr(xs
xs
end for
˜xs = 1
ms
xs+1
0 = xs
end for
return ˜xT

k=1 xs
k

ms

) − ∇f (˜xs−1  zis

t

) + ˜v + ut

s  where ut

s ∼ N (0  σ2Ip)

t−1  zis

t

t−1 − ηvs
t )

5 Differentially Private ERM for Convex Loss Function in High Dimensions

The utility bounds and gradient complexities in Section 4 depend on dimensionality p. In high-
dimensional (i.e.  p (cid:29) n) case  such a dependence is not very desirable. To alleviate this issue  we
can usually get rid of the dependence on dimensionality by reformulating the problem so that the
goal is to ﬁnd the parameter in some closed centrally symmetric convex set C ⊆ Rp (such as l1-norm
ball)  i.e. 

min
x∈C F (x  D) =

1
n

f (x  zi) 

(8)

n(cid:88)

i=1

√

where the loss function is convex.
p term in (5) (7) can be replaced by the Gaussian Width of C  which is no
[28] [29] showed that the
p) and can be signiﬁcantly smaller in practice (for more detail and examples one
larger than O(
may refer to [28]). In this section  we propose a faster algorithm to achieve the upper utility bound.
We ﬁrst give some deﬁnitions.

√

Algorithm 3 DP-AccMD(F  x0  T  σ  w)
Input:f (x  z) is G-Lipschitz   and L-smooth over x ∈ C . ||C||2 is the (cid:96)2 norm diameter of the
convex set C. w is a function that is 1-strongly convex w.r.t || · ||C. x0 is the initial point  and T is the
iteration number.

Deﬁne Bw(y  x) = w(y) − (cid:104)∇w(x)  y − x(cid:105) − w(x)
y0  z0 = x0
for k = 0 ···   T − 1 do
4L and rk =

1

2αk+1L

αk+1 = k+2
xk+1 = rkzk + (1 − rk)yk
yk+1 = arg miny∈C{ L||C||2
zk+1 = arg minz∈C{Bw(z  zk) + αk+1(cid:104)∇F (xk+1) + bk+1  z − zk(cid:105)}  where bk+1 ∼

||y − xk+1||2C + (cid:104)∇F (xk+1)  y − xk+1(cid:105)}

2

2

N (0  σ2Ip)
end for
return yT

Deﬁnition 5.1 (Minkowski Norm). The Minkowski norm (denoted by || · ||C) with respect to a
centrally symmetric convex set C ⊆ Rp is deﬁned as follows. For any vector v ∈ Rp 

The dual norm of || · ||C is denoted as || · ||C∗  for any vector v ∈ Rp  ||v||C∗ = maxw∈C |(cid:104)w  v(cid:105)|.

|| · ||C = min{r ∈ R+ : v ∈ rC}.

6

2-smooth with respect to || · ||C norm.

The following lemma implies that for every smooth convex function f (x  z) which is L-smooth with
respect to (cid:96)2 norm  it is L||C||2
Lemma 5.1. For any vector v  we have ||v||2 ≤ ||C||2||v||C  where ||C||2 is the (cid:96)2-diameter and
||C||2 = supx y∈C ||x − y||2.
Deﬁnition 5.2 (Gaussian Width). Let b ∼ N (0  Ip) be a Gaussian random vector in Rp. The Gaussian
width for a set C is deﬁned as GC = Eb[supw∈C(cid:104)b  w(cid:105)].
Lemma 5.2 ([28]). For W = (maxw∈C(cid:104)w  v(cid:105))2 where v ∼ N (0  Ip)  we have Ev[W ] = O(G2C +
||C||2
2).
Our algorithm DP-AccMD is based on the Accelerated Mirror Descent method  which was studied
in [4] [23].
Theorem 5.3. In DP-AccMD( Algorithm 3)  for   δ > 0  it is (  δ)-differentially private if

σ2 = c

G2T ln(1/δ)

n22

(9)

for some constant c.
Theorem 5.4 (Utility Guarantee). Suppose the loss function f (x  z) is G-Lipschitz   and L-smooth
over x ∈ C . In DP-AccMD  let σ be as in (9) and w be a function that is 1-strongly convex with
respect to || · ||C. Then if

(cid:32)

2

L||C||2

(cid:33)
(cid:112)Bw(x∗  x0)n
G(cid:112)ln(1/δ)(cid:112)G2C + ||C||2
(cid:32)(cid:112)Bw(x∗  x0)(cid:112)G2C + ||C||2

2

 

(cid:18)

√

n1.5

L
(G2C+||C||2
2)

1
4

(cid:19)

n

.

2G(cid:112)ln(1/δ)

(cid:33)

.

T 2 = O

we have

E[F (yT   D)] − F (x∗  D) ≤ O

The total gradient complexity is O

6 ERM for General Functions

In this section  we consider non-convex functions with similar objective function as before 

min
x∈Rp

F (x  D) =

1
n

f (x  zi).

(10)

n(cid:88)

i=1

Algorithm 4 DP-GD(x0  F  η  T  σ  D)
Input:f (x  z) is G-Lipschitz   and L-smooth over x ∈ C . F is under the assumptions. 0 < η ≤ 1
is the step size. T is the iteration number.

L

for t = 1  2 ···   T do

xt = xt−1 − η (∇F (xt−1  D) + zt−1)  where zt−1 ∼ N (0  σ2Ip)

end for
return xT (For section 6.1)
return xm where m is uniform sampled from {0  1 ···   m − 1}(For section 6.2)

Theorem 6.1. In DP-GD( Algorithm 4)  for   δ > 0  it is (  δ)-differentially private if

for some constant c.

σ2 = c

G2T ln(1/δ)

n22

7

(11)

6.1 Excess empirical risk for functions under Polyak-Lojasiewicz condition

In this section  we consider excess empirical risk in the case where the objective function F (x  D)
satisﬁes Polyak-Lojasiewicz condition. This topic has been studied in [18][27][26][24][22].
Deﬁnition 6.1 ( Polyak-Lojasiewicz condition). For function F (·)  denote X ∗ = arg minx∈Rp F (x)
and F ∗ = minx∈Rp F (x). Then there exists µ > 0 and for every x 
||∇F (x)||2 ≥ 2µ(F (x) − F ∗).

(12)

(12) guarantees that every critical point (i.e.  the point where the gradient vanish) is the global
minimum. [18] shows that if F is differentiable and L-smooth w.r.t (cid:96)2 norm  then we have the
following chain of implications:
Strong Convex ⇒ Essential Strong Convexity⇒ Weak Strongly Convexity ⇒ Restricted Secant
Inequality ⇒ Polyak-Lojasiewicz Inequality ⇔ Error Bound
Theorem 6.2. Suppose that f (x  z) is G-Lipschitz  and L-smooth over xC  and F (x  D) satisﬁes
the Polyak-Lojasiewicz condition. In DP-GD( Algorithm 4)  let σ be as in (11) with η = 1
L. Then if
T = ˜O

  the following holds

(cid:17)

(cid:16)

log(

n22

pG2 log(1/δ) )

E[F (xT   D)] − F (x∗  D) ≤ O(

G2p log2(n) log(1/δ)

n22

) 

(13)

where ˜O hides other log  L  µ terms.

DP-GD achieves near optimal bound since strongly convex functions can be seen as a special case in
the class of functions satisfying Polyak-Lojasiewicz condition. The lower bound for strongly convex
functions is Ω(min{1 
n22})[6]. Our result has only a logarithmic multiplicative term comparing to
that. Thus we achieve near optimal bound in this sense.

p

6.2 Tight upper bound for (non)-convex case

In [34]  the authors considered (non)-convex smooth loss functions and measured the utility as
||F (xprivate  D)||2. They proposed an algorithm with gradient complexity O(n2). For this algorithm 
they showed that E[||F (xprivate  D)||2] ≤ O(
). By using DP-GD( Algorithm 4)  we
can eliminate the log(n) term.
Theorem 6.3. Suppose that f (x  z) is G-Lipschitz  and L-smooth. In DP-GD( Algorithm 4)  let σ
be as in (11) with η = 1

p log(1/δ)
n

L. Then when T = O(

log(n)

√

√

Ln

√

p log(1/δ)G

)  we have

LG(cid:112)p log(1/δ)

√

E[||∇F (xm  D)||2] ≤ O(

).

(14)

n

Remark 6.1. Although we can obtain the optimal bound by Theorem 3.1 using DP-SGD  there will
be a constraint on . Also  we still do not know the lower bound of the utility using this measure. We
leave it as an open problem.

7 Discussions

From the discussion in previous sections  we know that when gradient perturbation is combined
with linearly converge ﬁrst order methods  near optimal bound with less gradient complexity can be
achieved. The remaining issue is whether the optimal bound can be obtained in this way. In Section
6.1  we considered functions satisfying the Polyak-Lojasiewicz condition  and achieved near optimal
bound on the utility. It will be interesting to know the bound for functions satisfying other conditions
(such as general Gradient-dominated functions [24]  quasi-convex and locally-Lipschitz in [16])
under the differential privacy model. For general non-smooth convex loss function (such as SVM
)  we do not know whether the optimal bound is achievable with less time complexity. Finally  for
non-convex loss function  proposing an easier interpretable measure for the utility is another direction
for future work.

8

References
[1] M. Abadi  A. Chu  I. Goodfellow  H. B. McMahan  I. Mironov  K. Talwar  and L. Zhang. Deep
learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on
Computer and Communications Security  pages 308–318. ACM  2016.

[2] N. Agarwal and K. Singh. The price of differential privacy for online learning. In Proceedings
of the 34th International Conference on Machine Learning  ICML 2017  Sydney  NSW  Australia 
6-11 August 2017  pages 32–40  2017.

[3] Z. Allen-Zhu. Katyusha: the ﬁrst direct acceleration of stochastic gradient methods.

In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
1200–1205. ACM  2017.

[4] Z. Allen-Zhu and L. Orecchia. Linear Coupling: An Ultimate Uniﬁcation of Gradient and Mirror
Descent. In Proceedings of the 8th Innovations in Theoretical Computer Science  ITCS ’17 
2017.

[5] Z. Allen-Zhu and Y. Yuan. Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex
In Proceedings of the 33rd International Conference on Machine Learning 

Objectives.
ICML ’16  2016.

[6] R. Bassily  A. Smith  and A. Thakurta. Private empirical risk minimization: Efﬁcient algorithms
and tight error bounds. In Foundations of Computer Science (FOCS)  2014 IEEE 55th Annual
Symposium on  pages 464–473. IEEE  2014.

[7] K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression. In Advances in Neural

Information Processing Systems  pages 289–296  2009.

[8] K. Chaudhuri  C. Monteleoni  and A. D. Sarwate. Differentially private empirical risk mini-

mization. Journal of Machine Learning Research  12(Mar):1069–1109  2011.

[9] K. Chaudhuri  A. Sarwate  and K. Sinha. Near-optimal differentially private principal compo-

nents. In Advances in Neural Information Processing Systems  pages 989–997  2012.

[10] C. Dwork. Differential privacy: A survey of results. In International Conference on Theory and

Applications of Models of Computation  pages 1–19. Springer  2008.

[11] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private

data analysis. In Theory of Cryptography Conference  pages 265–284. Springer  2006.

[12] C. Dwork  G. N. Rothblum  and S. Vadhan. Boosting and differential privacy. In Foundations of
Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pages 51–60. IEEE  2010.

[13] C. Dwork  K. Talwar  A. Thakurta  and L. Zhang. Analyze gauss: optimal bounds for privacy-
preserving principal component analysis. In Proceedings of the 46th Annual ACM Symposium
on Theory of Computing  pages 11–20. ACM  2014.

[14] D. Feldman  A. Fiat  H. Kaplan  and K. Nissim. Private coresets. In Proceedings of the forty-ﬁrst

annual ACM symposium on Theory of computing  pages 361–370. ACM  2009.

[15] M. Hardt and A. Roth. Beyond worst-case analysis in private singular vector computation. In
Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing  pages 331–340.
ACM  2013.

[16] E. Hazan  K. Levy  and S. Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-

mization. In Advances in Neural Information Processing Systems  pages 1594–1602  2015.

[17] P. Jain  P. Kothari  and A. Thakurta. Differentially private online learning. In COLT  volume 23 

pages 24–1  2012.

[18] H. Karimi  J. Nutini  and M. Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases  pages 795–811. Springer  2016.

9

[19] S. P. Kasiviswanathan and H. Jin. Efﬁcient private empirical risk minimization for high-
In Proceedings of The 33rd International Conference on Machine

dimensional learning.
Learning  pages 488–497  2016.

[20] S. P. Kasiviswanathan  K. Nissim  and H. Jin. Private incremental regression. In Proceedings of
the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems  PODS
2017  Chicago  IL  USA  May 14-19  2017  pages 167–182  2017.

[21] D. Kifer  A. Smith  and A. Thakurta. Private convex empirical risk minimization and high-

dimensional regression. Journal of Machine Learning Research  1(41):3–1  2012.

[22] G. Li and T. K. Pong. Calculus of the exponent of kurdyka-{\ L} ojasiewicz inequality and
its applications to linear convergence of ﬁrst-order methods. arXiv preprint arXiv:1602.02915 
2016.

[23] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming 

103(1):127–152  2005.

[24] Y. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance.

Mathematical Programming  108(1):177–205  2006.

[25] A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Advances in

Neural Information Processing Systems  pages 1574–1582  2014.

[26] B. T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational

Mathematics and Mathematical Physics  3(4):864–878  1963.

[27] S. J. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for
nonconvex optimization. In International conference on machine learning  pages 314–323 
2016.

[28] K. Talwar  A. Thakurta  and L. Zhang. Private empirical risk minimization beyond the worst

case: The effect of the constraint set geometry. arXiv preprint arXiv:1411.5417  2014.

[29] K. Talwar  A. Thakurta  and L. Zhang. Nearly optimal private lasso. In Advances in Neural

Information Processing Systems  pages 3025–3033  2015.

[30] A. G. Thakurta and A. Smith. (nearly) optimal algorithms for private online learning in full-
information and bandit settings. In Advances in Neural Information Processing Systems  pages
2733–2741  2013.

[31] Y.-X. Wang  J. Lei  and S. E. Fienberg. Learning with differential privacy: Stability  learnability
and the sufﬁciency and necessity of erm principle. Journal of Machine Learning Research 
17(183):1–40  2016.

[32] X. Wu  M. Fredrikson  W. Wu  S. Jha  and J. F. Naughton. Revisiting differentially private regres-
sion: Lessons from learning theory and their consequences. arXiv preprint arXiv:1512.06388 
2015.

[33] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[34] J. Zhang  K. Zheng  W. Mou  and L. Wang. Efﬁcient private ERM for smooth objectives. In
Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence  IJCAI
2017  Melbourne  Australia  August 19-25  2017  pages 3922–3928  2017.

10

,Kameron Harris
Stefan Mihalas
Eric Shea-Brown
Di Wang
Minwei Ye
Jinhui Xu
Dimitris Bertsimas
Christopher McCord