2016,A Bandit Framework for Strategic Regression,We consider a learner's problem of acquiring data dynamically for training a regression model  where the training data are collected from strategic data sources. A fundamental challenge is to incentivize data holders to exert effort to improve the quality of their reported data  despite that the quality is not directly verifiable by the learner. In this work  we study a dynamic data acquisition process where data holders can contribute multiple times. Using a bandit framework  we leverage on the long-term incentive of future job opportunities to incentivize high-quality contributions. We propose a Strategic Regression-Upper Confidence Bound (SR-UCB) framework  an UCB-style index combined with a simple payment rule  where the index of a worker approximates the quality of his past contributions and is used by the learner to determine whether the worker receives future work. For linear regression and certain family of non-linear regression problems  we show that SR-UCB enables a $O(\sqrt{\log T/T})$-Bayesian Nash Equilibrium (BNE) where each worker exerting a target effort level that the learner has chosen  with $T$ being the number of data acquisition stages. The SR-UCB framework also has some other desirable properties: (1) The indexes can be updated in an online fashion (hence computationally light). (2) A slight variant  namely Private SR-UCB (PSR-UCB)  is able to preserve $(O(\log^{-1} T)  O(\log^{-1} T))$-differential privacy for workers' data  with only a small compromise on incentives (achieving $O(\log^{6} T/\sqrt{T})$-BNE).,A Bandit Framework for Strategic Regression

Yang Liu and Yiling Chen

School of Engineering and Applied Science  Harvard University

{yangl yiling}@seas.harvard.edu

Abstract

We consider a learner’s problem of acquiring data dynamically for training a re-
gression model  where the training data are collected from strategic data sources.
A fundamental challenge is to incentivize data holders to exert effort to improve
the quality of their reported data  despite that the quality is not directly veriﬁable
by the learner. In this work  we study a dynamic data acquisition process where
data holders can contribute multiple times. Using a bandit framework  we lever-
age the long-term incentive of future job opportunities to incentivize high-quality
contributions. We propose a Strategic Regression-Upper Conﬁdence Bound (SR-
UCB) framework  a UCB-style index combined with a simple payment rule 
where the index of a worker approximates the quality of his past contributions
and is used by the learner to determine whether the worker receives future work.
For linear regression and a certain family of non-linear regression problems  we

show that SR-UCB enables an O(cid:0)(cid:112)logT /T(cid:1)-Bayesian Nash Equilibrium (BNE)
(PSR-UCB)  is able to preserve (O(cid:0)log−1 T(cid:1) O(cid:0)log−1 T(cid:1))-differential privacy for
a target effort level is an O(cid:0)log6 T /

where each worker exerts a target effort level that the learner has chosen  with T
being the number of data acquisition stages. The SR-UCB framework also has
some other desirable properties: (1) The indexes can be updated in an online fash-
ion (hence computation is light). (2) A slight variant  namely Private SR-UCB

workers’ data  with only a small compromise on incentives (each worker exerting

T(cid:1)-BNE).

√

1

Introduction

More and more data for machine learning nowadays are acquired from distributed  unmonitored
and strategic data sources and the quality of these collected data is often unveriﬁable. For example 
in a crowdsourcing market  a data requester can pay crowd workers to label samples. While this
approach has been widely adopted  crowdsourced labels have been shown to degrade the learning
performance signiﬁcantly  see e.g.  [19]  due to the low quality of the data. How to incentivize
workers to contribute high-quality data is hence a fundamental question that is crucial to the long-
term viability of this approach.
Recent works [2 4 10] have considered incentivizing data contributions for the purpose of estimating
a regression model. For example Cai et al. [2] design payment rules so that workers are incentivized
to exert effort to improve the quality of their contributed data  while Cummings et al. [4] design
mechanisms to compensate privacy-sensitive workers for their privacy loss when contributing their
data. These studies focus on a static data acquisition process  only considering one-time data acqui-
sition from each worker. Hence  the incentives completely rely on the payment rule. However  in
stable crowdsourcing markets  workers return to receive additional work. Future job opportunities
are thus another dimension of incentives that can be leveraged to motive high-quality data contribu-
tions. In this paper  we study dynamic data acquisition from strategic agents for regression problems
and explore the use of future job opportunities to incentivize effort exertion.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In our setting  a learner has access to a pool of workers and in each round decides on which workers
to ask for data. We propose a Multi-armed Bandit (MAB) framework  called Strategic Regression-
Upper Conﬁdence Bound (SR-UCB)  that combines a UCB-style index rule with a simple per-round
payment rule to align the incentives of data acquisition with the learning objective. Intuitively  each
worker is an arm and has an index associated with him that measures the quality of his past con-
tributions. The indexes are used by the learner to select workers in the next round. While MAB
framework is natural for modeling selection problem with data contributors of potentially varying
qualities  our setting has two challenges that are distinct from classical bandit settings. First  after
a worker contributes his data  there is no ground-truth observation to evaluate how well the worker
performs (or reward as commonly referred to in a MAB setting). Second  a worker’s performance
is a result of his strategic decision (e.g. how much effort he exerts)  instead of being purely exoge-
nously determined. Our SR-UCB framework overcomes the ﬁrst challenge by evaluating the quality
of an agent’s contributed data against an estimator trained on data provided by all other agents to
obtain an unbiased estimate of the quality  an idea inspired by the peer prediction literature [11  16].
To address the second challenge  our SR-UCB framework enables a game-theoretic equilibrium
with workers exerting target effort levels chosen by the learner. More speciﬁcally  in addition to
proposing the SR-UCB framework  our contributions include:
• We show that SR-UCB helps simplify the design of payment  and successfully incentivizes effort
exertion for acquiring data for linear regression. Every worker exerting a targeted effort level

(for labeling and reporting the data) is an O(cid:0)(cid:112)logT /T(cid:1)-Bayesian Nash Equilibrium (BNE). We
small compromise on incentives. PSR-UCB is (O(cid:0)log−1 T(cid:1) O(cid:0)log−1 T(cid:1))-differentially private
and every worker exerting the targeted effort level is an O(cid:0)log6 T /

• SR-UCB indexes can be maintained in an online fashion  hence are computationally light.
• We extend SR-UCB to Private SR-UCB (PSR-UCB) to further provide privacy guarantees  with

can also extend the above results to a certain family of non-linear regression problems.

T(cid:1)-BNE.

√

2 Related work

Recent works have formulated various strategic learning settings under different objectives [2 4 10 
20]. Among these  payment based solutions are proposed for regression problems when data come
from workers who are either effort sensitive [2] or privacy sensitive [4]. These solutions induce
game-theoretic equilibria where high-quality data are contributed. The basic idea of designing the
payment rules is inspired by the much mature literature of proper scoring rules [8] and peer predic-
tion [16]. Both [2] and [4] consider a static data acquisition procedure  while our work focuses on
a dynamic data acquisition process. Leveraging the long-term incentive of future job opportunities 
our work has a much simpler payment rule than those of [2] and [4] and relaxes some of the re-
strictions on the learning objectives (e.g.  well behaved in [2])  at the cost of a weaker equilibrium
concept (approximate BNE in this work vs. dominate strategy in [2]).
Multi-armed Bandit (MAB) is a sequential decision making and learning framework which has
been extensively studied. It is nearly impossible to survey the entire bandit literature. The seminal
work by Lai et al [13] derived lower and upper bounds on asymptotic regret on bandit selection.
More recently  ﬁnite-time algorithms have been developed for i.i.d. bandits [1] . Different from
the classical settings  this work needs to deal with challenges such as no ground-truth observations
for bandits and bandits’ rewards being strategically determined. A few recent works [7  15] also
considered bandit settings with strategic arms. Our work differs from these in that we consider
a regression learning setting without ground-truth observations  as well as we consider long-term
workers whose decisions on reporting data can change over time.
Our work and motivations have some resemblance to online contract design problems for a principal-
agent model [9]. But unlike the online contract design problems  our learner cannot verify the quality
of ﬁnished work after each task assignment. In addition  instead of focusing on learning the optimal
contract  we use bandits mainly to maintain a long-term incentive for inducing high-quality data.

3 Formulation

The learner observes a set of feature data X for training. To make our analysis tractable  we assume
each x ∈ X is sampled uniformly from a unit ball with dimension d: x ∈ Rd s.t. ||x||2 ≤ 1. Each

2

x associates with a ground-truth response (or label) y(x)  which cannot be observed directly by the
learner. Suppose x and y(x) are related through a function f : Rd → R that y(x) = f (x) + z  where
z is a zero-mean noise with variance σz  and is independent of x. For example  for linear regression
f (x) = θT x for some θ ∈ Rd. The learner would like to learn a good estimate ˜f of f . For the purpose
of training  the learner needs to ﬁgure out y(x) for different x ∈ X. To obtain an estimate ˜y(x) of
y(x)  the learner assigns each x to a selected worker to obtain a label.
Agent model: Suppose we have a set of workers U = {1 2  ... N} with N ≥ 2. After receiving
the labeling task  each worker will decide on the effort level e he wants to exert to generate an
outcome – higher effort leads to a better outcome  but is also associated with a higher cost. We
assume e has bounded support [0  ¯e] for all worker i ∈ U. When deciding on an effort level  a
worker wants to maximize his expected payment minus cost for effort exertion. The resulted label
˜y(x) will be given back to the learner. Denote by ˜yi(x e) the label returned by worker i for data
instance x (if assigned) with chosen effort level e. We consider the following effort-sensitive agent
model: ˜yi(x e) = f (x) + z + zi(e)  where zi(e) is a zero-mean noise with variance σi(e). σi(e) can
be different for different workers  and σi(e) decreases in e ∀i. The z and zi’s have bounded support
such that |z| |zi| ≤ Z  ∀i. Without loss of generality  we assume that the cost for exerting effort e is
simply e for every worker.

Learner’s objective Suppose the learner wants to learn f with the set of samples X. Then the
learner ﬁnds effort levels e∗ for data points in X such that

e∗ ∈ argmin{e(x)}x∈X ERROR( ˜f ({x  ˜y(x e(x))}x∈X )) + λ· PAYMENT({e(x)}x∈X )  

where e(x) is the effort level for sample x  and { ˜y(x e(x))}x∈X is the set of labeled responses for
training data X. ˜f (·) is the regression model trained over this data. The learner assigns the data and
pay appropriately to induce the corresponding effort level e∗. This formulation resembles the one
presented in [2]. The ERROR term captures the expected error of the trained model using collected
data (e.g.  measure in squared loss)  while the PAYMENT term captures the total expected budget that
the learner spends to receive the labels. This payment quantity depends on the mechanism that the
learner chooses to use and is the expected payment of the mechanism to induce selected effort level
for each data point {e(x)}x∈X. λ > 0 is a weighting factor  which is a constant. It is clear that the
objective function depends on σi’s. We assume for now that the learner knows σi(·)’s 1 and the
optimal e∗ can be computed.

4 StrategicRegression-UCB (SR-UCB): A general template

We propose SR-UCB for solving the dynamic data acquisition problem. SR-UCB enjoys a bandit
setting  where we borrow the idea from the classical UCB algorithm [1]  which maintains an index
for each arm (worker in our setting)  balancing exploration and exploitation. While a bandit frame-
work is not necessarily the best solution for our dynamic data acquisition problem  it is a promising
option for the following reasons. First  as utility maximizers  workers would like to be assigned
tasks as long as the marginal gain for taking a task is positive. A bandit algorithm can help execute
the assignment process. Second  carefully designed indexes can potentially reﬂect the amount of
effort exerted by the agents. Third  because the arm selection (of bandit algorithms) is based on the
indexes of workers  it introduces competition among workers for improving their indexes.
SR-UCB contains the following two critical components:
Per-round payment For each worker i  once selected to label a sample x  we will assign a base
payment pi = ei + γ  2 after reporting the labeling outcome  where ei is the desired effort level that
we would like to induce from worker i (for simplicity we have assumed the cost for exerting effort ei
equals to the effort level)  and γ > 0 is a small quantity. The design of this base payment is to ensure
once selected  a worker’s base cost will be covered. Note the above payment depends on neither the
assigned data instance x nor the reported outcome ˜y. Therefore such a payment procedure can be
pre-deﬁned after the learner sets a target effort level.

1This assumption can be relaxed. See our supplementary materials for the case with homogeneous σ.
2We assume workers have knowledge of how the mechanism sets up this γ.

3

Assignment The learner assigns multiple task {xi(t)}i∈d(t) at time t  with d(t) denoting the set of
workers selected at t. Denote by ei(t) the effort level worker i exerted for xi(t)  if i ∈ d(t). Note all
{xi(t)}i∈d(t) are different tasks  and each of them is assigned to exactly one worker. The selection of
workers will depend on the notion of indexes. Details are given in Algorithm 1.

Algorithm 1 SR-UCB: Worker index & selection

Step 1. For each worker i  ﬁrst train estimator ˜f−i t using data {x j(n) : 1 ≤ n ≤ t −1  j ∈ d(n)  j (cid:54)=
i}  that is using the data collected from workers j (cid:54)= i up to time t − 1. When t = 1  we will
initialize by sampling each worker at least once such that ˜f−i t can be computed.
Step 2. Then compute the following index for worker i at time t

(cid:20)

(cid:18)

(cid:19)2(cid:21)

(cid:115)

logt
ni(t)

 

Ii(t) =

1

ni(t)

t

∑

n=1

1(i ∈ d(n))

a− b

˜f−i t (xi(n))− ˜yi(n ei(n))

+ c

where ni(t) is the number of times worker i has been selected up to time t. a b are two positive
constants for “scoring”  and c is a normalization constant. ˜yi(n ei(n)) is the corresponding label
for task xi(n) with effort level ei(n)  if i ∈ d(n).
Step 3. Based on the above index  we select d(t) at time t such that d(t) := { j : Ij(t) ≥ maxi Ii(t)−
τ(t)}  where τ(t) is a perturbation term decreasing in t.

Some remarks on SR-UCB: (1) Different from the classical bandit setting  when calculating the
indexes  there is no ground-truth observation for evaluating the performance of each worker. There-
fore we adopt the notion of scoring rule [8]. Particularly the one we used above is the well-known
Brier scoring rule: B(p q) = a− b(p− q)2 . (2) The scoring rule based index looks similar to the
payment rules studied in [2  4]. But as we will show later  under our framework the selection of a b
is much less sensitive to different problem settings  as with an index policy  only the relative values
matter (ranking). This is another beneﬁt of separating payment from selection. (3) Instead of only
selecting the best worker with the highest index  we select workers whose index is within a certain
range of the maximum one (a conﬁdence region). This is because workers may have competing
expertise level and hence selecting only one of them would de-incentivize workers’ effort exertion.

4.1 Solution concept
Denote by e(n) := {e1(n)  ... eN(n)}  and e−i(n) = {e j(n)} j(cid:54)=i. We deﬁne approximate Bayesian
Nash Equilibrium as our solution concept:
Deﬁnition 1. Suppose SR-UCB runs for T stages. {ei(t)}N T

i=1 t=1 is a π-BNE if ∀i { ˜ei(t)}T

(pi − ei(t))1(i ∈ d(t))(cid:12)(cid:12){e(n)}n≤t ] ≥ 1

T

(pi − ˜ei(t))1(i ∈ d(t))(cid:12)(cid:12){ ˜ei(n) e−i(n)}n≤t ]− π.

t=1:

t=1

E[

E[

∑

1
T
This is to say by deviating  each worker will gain no more than π net-payment per around. We
will establish our main results in terms of π-BNE. The reason we adopt such a notion is that in a
sequential setting it is generally hard to achieve strict BNE or other stronger notion as any one-step
deviation may not affect a long-term evaluation by much.3 Approximate BNE is likely the best
solution concept we can hope for.

T

T

∑

t=1

5 Linear regression

5.1 Settings and a warm-up scenario

In this section we present our results for a simple linear regression task where the feature x and ob-
servation y are linearly related via an unknown θ: y(x) = θT x + z  ∀x ∈ X. Let’s start with assuming
all workers are statistically identical such that σ1 = σ2 = ... = σN. This is an easier case that serves
as a warm-up. It is known that given training data  we can ﬁnd an estimation ˜θ that minimizes a

3Certainly  we can run mechanisms that induce BNE or dominant-strategy equilibrium for one-shot setting 

e.g. [2]  for every time step. But such solution does not incorporate long-term incentives.

4

non-regularized empirical risk function: ˜θ = argminˆθ∈Rd ∑x∈X (y(x)− ˆθT x)2 (linear least square). To
put this model into SR-UCB  denote ˜θ−i(t) as the linear least square estimator trained using data

from workers j (cid:54)= i up to time t − 1. And Ii(t) := Si(t) + c(cid:112)logt/ni(t)  with

(cid:20)

(cid:18)
(cid:19)2(cid:21)
˜θT−i(t)xi(n)− ˜yi(n ei(n))

1(i ∈ d(n))

a− b

.

(5.1)

Si(t) :=

1

ni(t)

t−1
∑

n=1

Suppose ||θ||2 ≤ M. Given ||x||2 ≤ 1 and |z|  |zi| ≤ Z  we then prove that ∀t n i  (˜θT−i(t)xi(n) −
˜yi(n ei(n)))2 ≤ 8M2 + 2Z2. Choose a b such that a− (8M2 + 2Z2)b ≥ 0  then we have 0 ≤ Si(t) ≤

a  ∀i t. For the perturbation term  we set τ(t) := O(cid:0)(cid:112)logt/t(cid:1). The intuition is that with t samples 
upper bounded at the order of O(cid:0)(cid:112)logt/t(cid:1). Thus  to not miss a competitive worker  we set the

the uncertainties in the indexes  coming from both the score calculation and the bias term  can be

tolerance to be at the same order.
We now develop the formal equilibrium result of SR-UCB for linear least square. Our analysis
requires the following assumption on the smoothness of σ.
Assumption 1. We assume σ(e) is convex on e ∈ [0  ¯e]  with gradient σ(cid:48)(e) being both upper
bounded  and lower bounded away from 0  i.e.  L ≥ |σ(cid:48)(e)| ≥ L > 0  ∀e.
The learner wants to learn f with a total of NT (= |X| or (cid:100)NT(cid:101) = |X|) samples. Since workers are
statistically equivalent  ideally the learner would like to run SR-UCB for T steps and collect a label
for a unique sample from each worker at each step. Hence  the learner would like to elicit a single
target effort level e∗ from all workers and for all samples:

(cid:20)

(cid:21)2

e∗ ∈ argmineEx y  ˜y

θT ({xi(n)  ˜yi(n e)}N T

i=1 n=1)· x− y

+ λ· (e + γ)NT.

(5.2)

The net payment (payment minus the cost of effort) per task can be made arbitrarily small by setting

Due to the uncertainty in worker selection  it is highly likely that after step T   there will be tasks
left unlabelled. We can let the mechanism go for extra steps to complete labelling of these tasks.
But due to the bounded number of missed selections as we will show later  stopping at step T won’t
affect the accuracy in the model trained.
Theorem 1. Under SR-UCB for linear least square  set ﬁxed payment pi = e∗ + γ for all i  where

Our solution heavily relies on forming a race among workers. By establishing the convergence of
bandit indexes to a function of effort (via σ(·))  we show that when other workers j (cid:54)= i follow
the equilibrium strategy  worker i will be selected w.h.p. at each round  if he also puts in the same

γ = Ω((cid:112)logT /T )  choose c to be a large enough constant  c ≥ Const.(M Z N b)  and let τ(t) :=
O(cid:0)(cid:112)logt/t(cid:1). Workers have full knowledge of the mechanism and the values of the parameters.
Then at an O(cid:0)(cid:112)logT /T(cid:1)-BNE  workers  whenever selected  exert effort ei(t) ≡ e∗ for all i and t.
γ exactly on the order of O(cid:0)(cid:112)logT /T(cid:1)  and pi − e∗ = γ = O(cid:0)(cid:112)logT /T(cid:1) → 0  as T → ∞.
amount of effort. On the other hand  if worker i shirks from doing so by as much as (O(cid:0)(cid:112)logT /T(cid:1)) 
in the next section  all workers shirking from exerting effort is also an O(cid:0)(cid:112)logT /T(cid:1)-BNE. This
signed a task w.p. 1. Set ps := 1− O(cid:0)(cid:112)logT /T /γ(cid:1). So with probability 1− ps = O(cid:0)(cid:112)logT /T /γ(cid:1) 
O(cid:0)(cid:112)logT /T(cid:1)-BNE  while every worker exerting any effort level that is ∆e > O(cid:0)γ(cid:1) lower than the
target effort level is not a π-BNE with π ≤ O(cid:0)(cid:112)logT /T(cid:1).

equilibrium can be removed by adding some uncertainty on top of the bandit selection procedure.
When there are ≥ 2 workers being selected in SR-UCB  each of them will be assigned a task with
certain probability 0 < ps < 1. While when there is a single selected worker  the worker is as-
even the “winning”workers will miss the selection. With this change  exerting e∗ still forms an

his number of selection will go down in order. This establishes the π-BNE. As long as there exists
one competitive worker  all others will be incentivized to exert effort. Though as will be shown

5

5.2 Linear regression with different σ

(cid:20)

(cid:21)2
i=1 n=1)x− y

+ λ· (e2 + γ)2T.

Now we consider the more realistic case that different workers have different noise-effort function
σ’s. W.l.o.g.  we assume σ1(e) < σ2(e) < ... < σN(e) ∀e.4 In such a setting  ideally we would
always like to collect data from worker 1 since he has the best expertise level (lowest variance in
labeling noise). Suppose we are targeting an effort level e∗
1 from data source 1 (the best data source).
We ﬁrst argue that we also need to incentivize worker 2 to exert competitive effort level e∗
2 such that
σ1(e∗
2 exists.5 This also naturally implies that e∗
2 > e∗
1 as
worker 1 contributes data with less variance in noise at the same effort level. The reason is similar
to the homogeneous setting—over time workers form a competition on σi(ei). Having a competitive
peer will motivate workers to exert as much effort as he can (up to the payment). Therefore the goal
for such a learner (with 2T samples to assign) is to ﬁnd an effort level e∗ such that 6

2)  and we assume such an e∗

1) = σ2(e∗

e∗ ∈ argmine2:σ1(e1)=σ2(e2)

Ex y  ˜y

θT ({xi(n)  ˜yi(n ei))}2 T

i )− σ1(e∗

1 be the solution to σ1(e∗

Set the one-step payment to be pi = e∗ + γ ∀i. Let e∗
1) = σ2(e∗) and let
i = e∗ for i ≥ 2. Note for i > 2 we have σi(e∗
e∗
1) > 0. While we have argued about the
necessity for choosing the top two most competitive workers  we have not mentioned the optimality
of doing so.
In fact selecting the top two is the best we can do. Suppose on the contrary  the
optimal solution is by selecting top k > 2 workers  at effort level ek. According to our solution  we
targeted the effort level that leads to variance of noise σk(ek) (so the least competitive worker will
be incentivized). Then we can simply target the same effort level ek  but migrating the task loads to
only the top two workers – this keeps the payment the same  but the variance of noise now becomes
σ2(ek) < σk(ek)  which leads to better performance. Denote ∆1 := σ3(e∗)− σ1(e∗
1) > 0 and assume
Assumption 1 applies to all σi’s. We prove:

Theorem 2. Under SR-UCB for linear least square  set c ≥ Const.(M Z b ∆1)  Ω((cid:112)logT /T ) =
2L   τ(t) := O(cid:0)(cid:112)logt/t(cid:1). Then  each worker i exerting effort e∗
O(cid:0)(cid:112)logT /T(cid:1)-BNE.
O(cid:0)σ1(e∗
O(cid:0)σ1(e∗
O(cid:0)logT(cid:1)) is bounded by E[σ1(e∗

1)/(∑i=1 2 ni(T ))2(cid:1). Ideally we want to have ∑i=1 2 ni(T ) = 2T   such that an upper bound of
1)/(2T )2(cid:1) can be achieved. Compared to the bound O(cid:0)σ1(e∗
1)/(2T )2(cid:1)  SR-UCB’s expected
1)logT /T 3(cid:1) w.h.p. .

Performance with acquired data If workers follow the π-BNE  the contributed data from the
top two workers (who have been selected the most number of times) will have the same variance
σ1(e∗
1). Then following results in [4]  w.h.p. the performance of the trained classiﬁer is bounded by

performance loss (due to missed sampling & wrong selection  which is bounded at the order of

1)/(2T )2] ≤ O(cid:0)σ1(e∗

1)/(∑i=1 2 ni(T ))2 − σ1(e∗

i once selected forms an

γ ≤ ∆1

Regularized linear regression Ridge estimator has been widely adopted for solving linear regres-
sion. The objective is to ﬁnd a linear model ˜θ that minimizes the following regularized empirical
risk: ˜θ = argminˆθ∈Rd ∑x∈X (y(x)− ˆθT x)2 + ρ||ˆθ||2
2   with ρ > 0 being the regularization parameter.
We claim that simply changing the ˜f−i t (·) in SR-UCB to the output from the above ridge regression 

the O(cid:0)(cid:112)logT /T(cid:1)-BNE for inducing an effort level e∗ will hold. Different from the non-regularized

case  the introduction of the regularization term will add bias in ˜θT−i(t)  which gives a biased evalu-
ation of indexes. However  we prove the convergence of ˜θT−i(t) (so again the indexes will converge
properly) in the following lemma  which enables an easy adaption of our previous results for non-
regularized case to ridge regression:
Lemma 1. With n i.i.d. samples  w.p. ≥ 1− e−Kn (K > 0 is a constant)  ||˜θ−i(t)− θ||2
Non-linear regression The basic idea for extending the results to non-linear regression is inspired
by the consistency results on M-estimator [14]  when the error of training data satisﬁes zero mean.
Similar to the reasoning for Lemma 1  if ( ˜f−i t (x)− f (x))2 → 0  we can hope for an easy adaptation
4Combing with the results for homogeneous workers  we can again easily extend our results to the case
5It exists when the supports for σ1(·) σ2(·) overlap for a large support range.
6Since we only target the top two workers  we can limit the number of acquisitions on each stage to be no

where there are a mixture of homogeneous and heterogenous workers.

2 ≤ O(cid:0) 1

(cid:1).

n2

more than two  so the number of query does not go beyond 2T .

6

of our previous results. Suppose the non-linear regression model can be characterized by a parameter
family Θ  where f is characterized by parameter θ  and ˜f−i t by ˜θi(t). Due to the consistency
of M-estimator we will have ||˜θi(t) − θ||2 → 0. More speciﬁcally  according to the results from

n(cid:1) convergence rate with n
[18]  for the non-linear regression model we can establish an O(cid:0)1/
( ˜f−i t (x)− f (x))2 → 0  and ( ˜f−i t (x)− f (x))2 ≤ O(cid:0)1/t(cid:1). The rest of the proof can then follow.

√
there exists a constant LN > 0
training samples. When f is Lipschitz in parameter space  i.e.
such that | ˜f−i t (x) − f (x)| ≤ LN||˜θi(t) − θ||2  by dominated convergence theorem we also have

Example 1. Logistic function f (x) =

1

1+e−θT x satisﬁes Lipschitz condition with LN = 1/4.

6 Computational issues
In order to update the indexes and select workers adaptively  we face a few computational challenges.
First  in order to update the index for each worker at any time t  a new estimator ˜θ−i(t) (using data
from all other workers j (cid:54)= i up to time t − 1) needs to be re-computed. Second  we need to re-apply
˜θ−i(t) to every collected sample from worker i {(xi(n)  ˜yi(n ei(n)) : i ∈ d(n) n = 1 2  ...t − 1} from
previous rounds. We propose online variants of SR-UCB to address these challenges.
Online update of ˜θ−i(·)
Inspired by the online learning literature  instead of re-computing ˜θ−i(t)
at each step  which involves re-calculating the inverse of a covariance matrix (e.g.  (ρI + X T X)−1
for ridge regression) whenever there is a new sample point arriving  we can update ˜θ−i(t) in an
online fashion  which is computationally much more efﬁcient. We demonstrate our results with
ridge linear regression. Start with an initial model ˜θonline−i
(1). Denote by (x−i(t)  ˜y−i(t)) any newly
arrived sample at time t from worker j (cid:54)= i. Update ˜θonline−i
(t + 1) (for computing Ii(t + 1)) as [17]:

˜θonline−i

(t + 1) := ˜θonline−i

(t)− ηt · ∇˜θonline−i

(t)[(θT x−i(t)− ˜y−i(t))2 + ρ||θ||2
2]  

Notice there could be multiple such data points arriving at each time – in which case we will up-
date sequentially in an arbitrarily order. It is also possible that there is no sample point arriving from
workers other than i at a time t  in which case we simply do not perform an update. Name this online
updating SR-UCB as OSR1-UCB. With online updating  the accuracy of trained model ˜θonline−i
(t +1)
converges slower  so is the accuracy in the index for characterizing worker’s performance. Never-

theless we prove exerting targeted effort exertion e∗ is O(cid:0)(cid:112)logT /T(cid:1)-BNE under OSR1-UCB for

ridge regression  using convergence results for ˜θonline−i
Online score update Online updating can also help compute Si(t) (in Ii(t)) efﬁciently. Instead of
repeatedly re-calculating the score for each data point (in Si(t))  we only update the newly assigned
samples which has not been evaluated yet  by replacing ˜θonline−i

(t) with ˜θonline−i

(t) proved in [17].

(n) in Si(t):

Sonline
i

(t) :=

1

ni(t)

t

∑

n=1

1(i ∈ d(n))[a− b((˜θonline−i

(n))T xi(n)− ˜yi(n ei(n)))2].

(6.1)

With less aggressive update  again the index term’s accuracy converges slower than before  which is
due to the fact the older data is scored using an older (and less accurate) version of ˜θonline−i without
being further updated. We propose OSR2-UCB where we change the index SR-UCB to: Sonline
(t) +

c(cid:112)(logt)2/ni(t)  to accommondate the slower convergence. We establish an O(cid:0)logT /

T(cid:1)-BNE

√
i

for workers exerting target effort—the change is due to the change of the bias term.

7 Privacy preserving SR-UCB
With a repeated data acquisition setting  workers’ privacy in data may leak repeatedly. In this section
we study an extension of SR-UCB to preserve privacy of each individual worker’s contributed data.
Denote the training data collected as D := { ˜yi(t ei(t))}i∈d(t) t. We quantify privacy using differential
privacy [5]  and we adopt (ε δ)-differential privacy (DP) [6]  which for our setting is deﬁned below:
Deﬁnition 2. A mechanism M : (X ×R)|D| → O is (ε δ)-differentially private if for any i ∈ d(t) t 
i(t))  and for every subset of possible outputs S ⊆ O  Pr[M (D) ∈
any two distinct ˜yi(t ei(t))  ˜y(cid:48)
S ] ≤ exp(ε)Pr[M (D\{ ˜yi(t ei(t))}  ˜y(cid:48)

i(t))) ∈ S ] + δ.

i(t e(cid:48)

i(t e(cid:48)

7

√
T  

the output

An outcome o ∈ O of a mechanism contains two parts  both of which can contribute to privacy
leakage: (1) The learned regression model ˜θ(T )  which is trained using all data collected after T
rounds. Suppose after learning the regression model ˜θ(T )  this information will be released for
public usage or monitoring. This information contains each individual worker’s private information.
Note this is a one-shot leak of privacy (published at the end of the training (step T )).
(2) The
indexes can reveal private information. Each worker i’s data will be utilized towards calculating
other workers’ indexes Ij(t)  j (cid:54)= i  as well as his own Ii(t)  which will be published.7 Note this type
of leakage occurs at each step. The lemma below allows us to focus on the privacy losses in S j(t) 
instead of Ij(t)  as both Ij(t) and ni(t) are functions of {S j(n)}n≤t.
Lemma 2. At any time t  ∀i  ni(t) can be written as a function of {S j(n) n < t} j.
Preserving privacy in ˜θ(T ) To protect privacy in ˜θ(T )  following standard method [6]  we add a
Laplacian noise vector vθ to it: ˜θp(T ) = ˜θ(T ) + vθ  where Pr(vθ) ∝ exp(−εθ||vθ||2). εθ > 0 is a
parameter controlling the noise level.
Lemma 3. Set εθ = 2

(O(cid:0)T−1/2(cid:1) exp(−O(cid:0)T(cid:1)))-DP. Further w.p. ≥ 1− 1/T 2  ||˜θp(T )− ˜θ(T )||2 = ||vθ||2 ≤ logT /

˜θp(T ) of SR-UCB for linear regression preserves
√
T .
Preserving privacy in {Ii(t)}i t: a continual privacy preserving model For indexes {Ii(t)}i  it
is also tempting to add vi(t) to each index  i.e. Ii(t) := Ii(t) + vi(t)  where again vi(t) is a zero-
mean Laplacian noise. However releasing {Ii(t)}i at each step will release a noisy version of each
˜yi(n ei(n)) i ∈ d(n) ∀n < t. The composition theory in differential privacy [12] implies that the
preserved privacy level will grow in time t  unless we add signiﬁcant noise on each stage  which
will completely destroy the informativeness of our index policy. We borrow the partial sum idea for
continual observations [3]. The idea is when releasing continual data  instead of inserting noise at
every step  the current to-be-released data will be decoupled into sum of partial sums  and we only
add noise to each partial sum and this noisy version of the partial sums can be re-used repeatedly.
We consider adding noise to a modiﬁed version of the online indexes {Sonline
(t)}i t as deﬁned in
˜θ−i(n)/t  where ˜θ−i(n) is the regression model we
Eqn. (6.1)  with ˜θonline−i
estimated using all data from worker j (cid:54)= i up to time n. For each worker i  his contributed data
appear in both {Sonline
(t)  j (cid:54)= i  we want to preserve privacy
in ∑t
˜θ− j(n)/t. Write down t as a binary string and ﬁnd the
We ﬁrst apply the partial sums idea to ∑t
rightmost digit that is a 1  then ﬂip that digit to 0: convert is back to decimal gives q(t). Take the
˜θ− j(n) as one partial sum. Repeat above for q(t)  to get q(q(t)) 
sum from q(t) + 1 to t: ∑t
n=q(t)+1
and the second partial sum ∑q(t)
n=q(q(t))+1
1
t (

˜θ− j(n)  until we reach q(·) = 0. So
∑

(t)}t   j (cid:54)= i. For Sonline
˜θ− j(n)/t  which contains information of ˜yi(n ei(n)).

(t) replaced by ∑t
(t)}t and{Sonline

˜θ− j(n) + ... +

˜θ− j(n)/t =

˜θ− j(n) +

˜θ− j(n)) .

(7.1)

∑

∑

∑

n=1

n=1

n=1

q(t)

i

j

t

i

j

n=q(t)+1

n=q(q(t))+1

i

i

n=1

(n). Each Sonline

˜θ− j(n)/t as ˜˜θonline−i

(t) is computed using ˜˜θonline−i

(t)  we also want to preserve privacy in ˜yi(n ei(n)). Clearly Sonline

Add noise v˜θ with Pr(v˜θ) ∝ e−ε||v˜θ||2 to each partial sum. The number of noise terms is bounded
by ≤ (cid:100)logt(cid:101) at time t. So is the number of appearance of each private data in the partial sums [3].
Denote the noisy version of ∑t
(n).
For Sonline
sum of partial sums of terms involving ˜yi(n ei(n)): write Sonline
(short-handing dS(n) := a− b((˜˜θonline−i
time of worker i being sampled the n-th time.). Decouple Sonline
technique. For each partial sum  add a noise vS with distribution Pr(vS) ∝ e−ε|vS|.
We then show that with the above two noise exertion procedures  our index policy SR-UCB will
not lose its value in incentivizing effort. In order to prove similar convergence results  we need to
modify SR-UCB by changing the index to the following format:

(t) can be written as
n=1 dS(n)/ni(t)
(t(n)))T xi(t(n))− ˜yi(t(n) ei(t(n))))2  where t(n) denotes the
(t) into partial sums using the same

(t) + c(log3 t log3 T )/(cid:112)ni(t)  τ(t) = O(cid:0)(log3 t log3 T )/

(t) as a summation: ∑ni(t)

t(cid:1)  

Ii(t) = ˆSonline

i

√

i

i

i

7It is debatable whether the indexes should be published or not. But revealing decisions on worker selection

will also reveal information on the indexes. We consider the more direct scenario – indexes are published.

8

t

n=1

0

n=0

i

i

(t) denotes the noisy version of Sonline

where ˆSonline
(t) with added noises ( vS v˜θ etc). The change of
bias is mainly to incorporate the increased uncertainty level (due to added privacy preserving noise).
Denote this mechanism as PSR-UCB  we have:
Theorem 3. Set ε := 1/log3 T for added noises (both vS and v˜θ)  PSR-UCB preserves
With homogeneous workers  we similarly can prove exerting effort {e∗
i }i (optimal effort level) is

T(cid:1)-BNE. We can see that  in order to protect privacy in the bandit setting  the approxi-

(O(cid:0)log−1 T(cid:1) O(cid:0)log−1 T(cid:1))-DP for linear regression.
O(cid:0)log6 T /

√

mation term of BNE is worse than before.
Acknowledgement: We acknowledge the support of NSF grant CCF-1301976.

References
[1] Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine learning  47(2-3):235–256  2002.

[2] Yang Cai  Constantinos Daskalakis  and Christos H Papadimitriou. Optimum statistical esti-

mation with strategic data sources. arXiv preprint arXiv:1408.2539  2014.

[3] T-H Hubert Chan  Elaine Shi  and Dawn Song. Private and continual release of statistics. ACM

Transactions on Information and System Security (TISSEC)  14(3):26  2011.

[4] Rachel Cummings  Stratis Ioannidis  and Katrina Ligett. Truthful linear regression. In Pro-

ceedings of The 28th Conference on Learning Theory  COLT 2015  pages 448–483  2015.

[5] Cynthia Dwork. Differential privacy. In Automata  languages and programming. 2006.
[6] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy.
[7] Arpita Ghosh and Patrick Hummel. Learning and incentives in user-generated content: Multi-
armed bandits with endogenous arms. In Proceedings of the 4th conference on Innovations in
Theoretical Computer Science  pages 233–246. ACM  2013.

[8] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules  prediction  and estima-

tion. Journal of the American Statistical Association  102(477):359–378  2007.

[9] Chien-Ju Ho  Aleksandrs Slivkins  and Jennifer Wortman Vaughan. Adaptive contract design
for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In Pro-
ceedings of the ﬁfteenth ACM EC  pages 359–376. ACM  2014.

[10] Stratis Ioannidis and Patrick Loiseau. Linear regression as a non-cooperative game. In Web

and Internet Economics  pages 277–290. Springer  2013.

[11] Radu Jurca and Boi Faltings. Collusion-resistant  incentive-compatible feedback payments. In
Proceedings of the 8th ACM conference on Electronic commerce  pages 200–209. ACM  2007.
[12] Peter Kairouz  Sewoong Oh  and Pramod Viswanath. The composition theorem for differential

privacy. arXiv preprint arXiv:1311.0776  2013.

[13] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Ad-

vances in applied mathematics  6(1):4–22  1985.

[14] Guy Lebanon. m-estimators and z-estimators.
[15] Yishay Mansour  Aleksandrs Slivkins  and Vasilis Syrgkanis. Bayesian incentive-compatible

bandit exploration. In Proceedings of the Sixteenth ACM EC  pages 565–582. ACM  2015.

[16] Nolan Miller  Paul Resnick  and Richard Zeckhauser. Eliciting informative feedback: The

peer-prediction method. Management Science  51(9):1359–1373  2005.

[17] Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Making gradient descent optimal for

strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647  2011.

[18] BLS Prakasa Rao. The rate of convergence of the least squares estimator in a non-linear

regression model with dependent errors. Journal of Multivariate Analysis  1984.

[19] Victor S Sheng  Foster Provost  and Panagiotis G Ipeirotis. Get another label? improving
data quality and data mining using multiple  noisy labelers. In Proceedings of the 14th ACM
SIGKDD international conference on Knowledge discovery and data mining  2008.

[20] Panos Toulis  David C. Parkes  Elery Pfeffer  and James Zou. Incentive-Compatible Experi-

mental Design. Proceedings 16th ACM EC’15  pages 285–302  2015.

9

,Yang Liu
Yiling Chen
Binghui Chen
Weihong Deng
Haifeng Shen