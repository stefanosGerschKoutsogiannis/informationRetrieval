2015,Spectral Learning of Large Structured HMMs for Comparative Epigenomics,We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure. The main challenge with learning parameters of such models is that iterative methods such as EM are very slow  while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally  we show that beyond our specific model  some of our algorithmic ideas can be applied to other graphical models.,Spectral Learning of Large Structured HMMs for

Comparative Epigenomics

Chicheng Zhang
UC San Diego

Jimin Song

Rutgers University

chz038@eng.ucsd.edu

song@dls.rutgers.edu

Kevin C Chen

Rutgers University

Kamalika Chaudhuri

UC San Diego

kcchen@dls.rutgers.edu

kamalika@eng.ucsd.edu

Abstract

We develop a latent variable model and an efﬁcient spectral algorithm motivated
by the recent emergence of very large data sets of chromatin marks from multiple
human cell types. A natural model for chromatin data in one cell type is a Hidden
Markov Model (HMM); we model the relationship between multiple cell types by
connecting their hidden states by a ﬁxed tree of known structure.
The main challenge with learning parameters of such models is that iterative meth-
ods such as EM are very slow  while naive spectral methods result in time and
space complexity exponential in the number of cell types. We exploit properties
of the tree structure of the hidden states to provide spectral algorithms that are
more computationally efﬁcient for current biological datasets. We provide sample
complexity bounds for our algorithm and evaluate it experimentally on biological
data from nine human cell types. Finally  we show that beyond our speciﬁc model 
some of our algorithmic ideas can be applied to other graphical models.

1

Introduction

In this paper  we develop a latent variable model and efﬁcient spectral algorithm motivated by the
recent emergence of very large data sets of chromatin marks from multiple human cell types [7  9].
Chromatin marks are chemical modiﬁcations on the genome which are important in many basic
biological processes. After standard preprocessing steps  the data consists of a binary vector (one
bit for each chromatin mark) for each position in the genome and for each cell type.
A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM) [8  13] 
for which efﬁcient spectral algorithms are known. On biological data sets  spectral algorithms have
been shown to have several practical advantages over maximum likelihood-based methods  including
speed  prediction accuracy and biological interpretability [24]. Here we extend the approach by
modeling multiple cell types together. We model the relationships between cell types by connecting
their hidden states by a ﬁxed tree  the standard model in biology for relationships between cell
types. This comparative approach leverages the information shared between the different data sets
in a statistically uniﬁed and biologically motivated manner.
Formally  our model is an HMM where the hidden state zt at time t has a structure represented by
a tree graphical model of known structure. For each tree node u we can associate an individual
hidden state zu
t−1 for the same tree node u but
also on the individual hidden state of its parent node. Additionally  there is an observation variable
t is independent of other state and observation variables
t for each node u  and the observation xu
xu

t that depends not only on the previous hidden state zu

1

conditioned on the hidden state variable zu
t . In the bioinformatics literature  [5] studied this model
with the additional constraint that all tree nodes share the same emission parameters. In biological
applications  the main outputs of interest are the learned observation matrices of the HMM and a
segmentation of the genome into regions which can be used for further studies.
A standard approach to unsupervised learning of HMMs is the Expectation-Maximization (EM) al-
gorithm. When applied to HMMs with very large state spaces  EM is very slow. A recent line of
work on spectral learning [18  1  23  6] has produced much more computationally efﬁcient algo-
rithms for learning many graphical models under certain mild conditions  including HMMs. How-
ever  a naive application of these algorithms to HMMs with large state spaces results in computa-
tional complexity exponential in the size of the underlying tree.
Here we exploit properties of the tree structure of the hidden states to provide spectral algorithms
that are more computationally efﬁcient for current biological datasets. This is achieved by three
novel key ideas. Our ﬁrst key idea is to show that we can treat each root-to-leaf path in the tree
separately and learn its parameters using tensor decomposition methods. This step improves the
running time because our trees typically have very low depth. Our second key idea is a novel tensor
symmetrization technique that we call Skeletensor construction where we avoid constructing the full
tensor over the entire root-to-leaf path. Instead we use carefully designed symmetrization matrices
to reveal its range in a Skeletensor which has dimension equal to that of a single tree node. The third
and ﬁnal key idea is called Product Projections  where we exploit the independence of the emission
matrices along the root-to-leaf path conditioned on the hidden states to avoid constructing the full
tensors and instead construct compressed versions of the tensors of dimension equal to the number
of hidden states  not the number of observations. Beyond our speciﬁc model  we also show that
Product Projections can be applied to other graphical models and thus we contribute a general tool
for developing efﬁcient spectral algorithms.
Finally we implement our algorithm and evaluate it on biological data from nine human cell types
[7]. We compare our results with the results of [5] who used a variational EM approach. We also
compare with spectral algorithms for learning HMMs for each cell type individually to assess the
value of the tree model.

1.1 Related Work

The ﬁrst efﬁcient spectral algorithm for learning HMM parameters was due to [18]. There has been
an explosion of follow-up work on spectral algorithms for learning the parameters and structure of
latent variable models [23  6  4]. [18] gives a spectral algorithm for learning an observable operator
representation of an HMM under certain rank conditions.
[23] and [3] extend this algorithm to
the case when the transition matrix and the observation matrix respectively are rank-deﬁcient. [19]
extends [18] to Hidden Semi-Markov Models.
[2] gives a general spectral algorithm for learning parameters of latent variable models that have a
multi-view structure – there is a hidden node and three or more observable nodes that are not con-
nected to any other nodes and are independent conditioned on the hidden node. Many latent variable
models have this structure  including HMMs  tree graphical models  topic models and mixture mod-
els. [1] provides a simpler  more robust algorithm that involves decomposing a third order tensor.
[21  22  25] provide algorithms for learning latent trees and of latent junction trees.
Several algorithms have been designed for learning HMM parameters for chromatin modeling  in-
cluding stochastic variational inference [16] and contrastive learning of two HMMs [26]. However 
none of these methods extend directly to modeling multiple chromatin sequences simultaneously.

2 The Model

Probabilistic Model. The natural probabilistic model for a single epigenomic sequence is a hidden
Markov model (HMM)  where time corresponds to position in the sequence. The observation at
time t is the sequence value at position t  and the hidden state at t is the regulatory function in this
position.

2

Figure 1: Left: A tree T with 3 nodes V = {r  u  v}. Right: A HMM whose hidden state has
structure T .

t is a parent of zu

In comparative epigenomics  the goal is to jointly model epigenomic sequences from multiple
species or cell-types. This is done by an HMM with a tree-structured hidden state [5](THS-HMM) 1
where each node in the tree representing the hidden state has a corresponding observation node.
Formally  we represent the model by a tuple H = (G O T  W); Figure 1 shows a pictorial repre-
sentation.
G = (V  E) is a directed tree with known structure whose nodes represent individual cell-types
or species. The hidden state zt and the observation xt are represented by vectors {zu
t } and {xu
t }
indexed by nodes u ∈ V . If (v  u) ∈ E  then v is the parent of u  denoted by π(u); if v is a parent
t . In addition  the observations have the following product
of u  then for all t  zv
structure: if u� �= u  then conditioned on zu
t as
well as any zu�
O is a set of observation matrices Ou = P (xu
tensors T u = P (zu
1|zπ(u)
W u = P (zu
Given a tree structure and a number of iid observation sequences corresponding to each node of
the tree  our goal is to determine the parameters of the underlying THS-HMM and then use these
parameters to infer the most likely regulatory function at each position in the sequences.
Below we use the notation D to denote the number of nodes in the tree and d to denote its depth.
For typical epigenomic datasets  D is small to moderate (5-50) while d is very small (2 or 3) as
it is difﬁcult to obtain data with large d experimentally. Typically m  the number of possible val-
ues assumed by the hidden state at a single node  is about 6-25  while n  the number of possible
observation values assumed by a single node is much larger (e.g. 256 in our dataset).

t ) for each u ∈ V and T is a set of transition
t+1 ) for each u ∈ V . Finally  W is the set of initial distributions where

t   zπ(u)
t+1|zu
) for each zu
1 .

t   the observation xu

t is independent of zu�

t and xu�

t |zu

t� and xu�

t� for t �= t�.

1

Tensors. An order-3 tensor M ∈ Rn1 ⊗ Rn2 ⊗ Rn3 is a 3-dimensional array with n1n2n3 entries 
with its (i1  i2  i3)-th entry denoted as Mi1 i2 i3.
Given ni × 1 vectors vi  i = 1  2  3  their tensor product  denoted by v1 ⊗ v2 ⊗ v3 is the n1 × n2 × n3
tensor whose (i1  i2  i3)-th entry is (v1)i1 (v2)i2 (v3)i3. A tensor that can be expressed as the tensor
product of a set of vectors is called a rank 1 tensor. A tensor M is symmetric if and only if for any
permutation π : [3] → [3]  Mi1 i2 i3 = Mπ(i1) π(i2) π(i3).
Let M ∈ Rn1 ⊗ Rn2 ⊗ Rn3. If Vi ∈ Rni×mi  then M (V1  V2  V3) is a tensor of size m1 × m2 × m3 
whose (i1  i2  i3)-th entry is: M (V1  V2  V3)i1 i2 i3 =�j1 j2 j3
Mj1 j2 j3 (V1)j1 i1 (V2)j2 i2 (V3)j3 i3.
Since a matrix is a order-2 tensor  we also use the following shorthand to denote matrix multipli-
cation. Let M ∈ Rn1 ⊗ Rn2. If Vi ∈ Rmi×ni  then M (V1  V2) is a matrix of size m1 × m2 
whose (i1  i2)-th entry is: M (V1  V2)i1 i2 =�j1 j2
Mj1 j2 (V1)j1 i1 (V2)j2 i2. This is equivalent to
V �1 M V2.

1In the bioinformatics literature  this model is also known as a tree HMM.

3

t ⊗ xu
t� ⊗ xu

t� ]  and ˆP u u
t t�
t�� ] and its empirical version ˆP u u u

t� at a single node u  we use the notation P u
t t� = E[xu
t ⊗ xu

Meta-States and Observations  Co-occurrence Matrices and Tensors. Given observations xu
t
and xu
t t� to denote their expected co-occurence frequen-
cies: P u u
to denote their corresponding empirical version. The tensor
P u u u
t t� t�� = E[xu
Occasionally  we will consider the states or observations corresponding to a subset of nodes in G
coalesced into a single meta-state or meta-observation. Given a connected subset S ⊆ V of nodes in
t to denote the meta-state represented
the tree G that includes the root  we use the notation zS
by (zu
t   u ∈ S) respectively. We deﬁne the
t ) ∈ Rn|S|×m|S| and the transition matrix for S as
observation matrix for S as OS = P (xS
T S = P (zS

t and xS
t   u ∈ S) and the meta-observation represented by (xu

t t� t�� are deﬁned similarly.

t |zS
t ) ∈ Rm|S|×m|S|  respectively.

t+1|zS

For sets of nodes V1 and V2  we use the notation P V1 V2
frequencies of the meta-observations xV1
t
Similarly  we can deﬁne the notation P V1 V2 V3

and xV2
t� .

t t� t��

and its empirical version ˆP V1 V2 V3

.

t t� t��

to denote the expected co-occurrence
t t�
Its empirical version is denoted by ˆP V1 V2
.

t t�

Background on Spectral Learning for Latent Variable Models. Recent work by [1] has provided
a novel elegant tensor decomposition method for learning latent variable models. Applied to HMMs 
the main idea is to decompose a transformed version of the third order co-occurrence tensor of the
ﬁrst three observations to recover the parameters; [1] shows that given enough samples and under
fairly mild conditions on the model  this provides an approximation to the globally optimal solution.
The algorithm has three main steps. First  the third order tensor of the co-occurrences is symmetrized
using the second order co-occurrence matrices to yield a symmetric tensor; this symmetric tensor
is then orthogonalized by a whitening transformation. Finally  the resultant symmetric orthogonal
tensor is decomposed via the tensor power method.
In biological applications  instead of multiple independent sequences  we have a single long se-
quence in the steady state. In this case  following ideas from [23]  we use the average over t of the
third order co-occurence tensors of three consecutive observations starting at time t. The second
order co-occurence tensor is also modiﬁed similarly.

3 Algorithm

A naive approach for learning parameters of HMMs with tree-structured hidden states is to directly
apply the spectral method of [1]. Since this method ignores the structure of the hidden state  its
running time is very high  Ω(nDmD)  even with optimized implementations. This motivates the
design of more computationally efﬁcient approaches.
A plausible approach is to observe that at t = 1  the observations are generated by a tree graphical
model; thus in principle one could learn the parameters of the underlying tree using existing algo-
rithms [22  21  25]. However  this approach does not directly produce the HMM parameters; it also
does not work for biological sequences because we do not have multiple independent samples at
t = 1; instead we have a single long sequence at the steady state  and the steady state distribution
of observations is not generated by a latent tree. Another plausible approach is to use the spectral
junction tree algorithm of [25]; however  this algorithm does not provide the actual transition and
observation matrix parameters which hold important biological information  and instead provides
an observable operator representation.
Our main contribution is to show that we can achieve a much better running time by exploiting the
structure of the hidden state. Our algorithm is based on three key ideas – Partitioning  Skeletensor
Construction and Product Projections. We explain these ideas next.

Partitioning. Our ﬁrst observation is that to learn the parameters at a node u  we can focus only on
the unique path from the root to u. Thus we partition the learning problem on the tree into separate
learning problems on these paths. This maintains correctness as proved in the Appendix.
The Partitioning step reduces the computational complexity since we now need to learn an HMM
with md states and nd observations  instead of the naive method where we learn an HMM with mD
states and nD observations. As d � D in biological data  this gives us signiﬁcant savings.

4

Constructing the Skeletensor. A naive way to learn the parameters of the HMM corresponding to
each root-to-node path is to work directly on the O(nd × nd × nd) co-occurrence tensor. Instead 
we show that for each node u on a root-to-node path  a novel symmetrization method can be used
to construct a much smaller skeleton tensor T u of size n × n × n  which nevertheless captures the
effect of the entire root-to-node path and projects it into the skeleton tensor  thus revealing the range
of Ou. We call this the skeletensor.
Let Hu be the path from the root to a node u  and let ˆP Hu u Hu
be the empirical n|Hu| × n × n|Hu|
tensor of co-occurrences of the meta-observations Hu  u and Hu at times 1  2 and 3 respectively.
Based on the data we construct the following symmetrization matrices:

1 2 3

S1 ∼ ˆP u Hu

2 3

( ˆP Hu Hu

1 3

)†  S3 ∼ ˆP u Hu

2 1

( ˆP Hu Hu

3 1

)†

1 2 3

  xu1
t

  . . .   xud−1

Note that S1 and S3 are n × n|Hu| matrices. Symmetrizing ˆP Hu u Hu
with S1 and S3 gives us an
n× n× n skeletensor  which can in turn be decomposed to give an estimate of Ou (see Lemma 3 in
the Appendix).
Even though naively constructing the symmetrization matrices and skeletensor takes O(N n2d+1 +
n3d) time  this procedure improves computational efﬁciency because tensor construction is a one-
time operation  while the power method which takes many iterations is carried out on a much smaller
tensor.
Product Projections. We further reduce the computational complexity by using a novel algo-
rithmic technique that we call Product Projections. The key observation is as follows. Let
Hu = {u0  u1  . . .   ud−1} be any root-to-node path in the tree and consider the HMM that generates
the observations (xu0
) for t = 1  2  . . .. Even though the individual observations
t
xuj
t   j = 0  1  . . .   d − 1 are highly dependent  the range of OHu  the emission matrix of the HMM
describing the path Hu  is contained in the product of the ranges of Ouj   where Ouj is the emission
matrix at node uj (Lemma 4 in the Appendix). Furthermore  even though the Ouj matrices are difﬁ-
cult to ﬁnd  their ranges can be determined by computing the SVDs of the observation co-occurrence
matrices at uj.
Thus we can implicitly construct and store (an estimate of) the range of OHu. This also gives us
estimates of the range of ˆP Hu Hu
  and the range of the
ﬁrst and third modes of the tensor ˆP Hu u Hu
. Therefore during skeletensor construction we can
avoid explicitly constructing S1  S3 and ˆP Hu u Hu
  and instead construct their projections onto their
ranges. This reduces the time complexity of the skeletensor construction step to O(N m2d+1 +
m3d + dmn2) (recall that the range has dimension m.) While the number of hidden states m could
be as high as n  this is a signiﬁcant gain in practice  as n � m in biological datasets (e.g. 256
observations vs. 6 hidden states).
Product projections are more efﬁcient than random projections [17] on the co-occurrence matrix of
meta-observations: the co-occurrence matrices are nd × nd matrices  and random projections would
take Ω(nd) time. Also  product projections differ from the suggestion of [15] since we exploit
properties of the model to efﬁciently ﬁnd good projections.
The Product Projections technique is a general technique with applications beyond our model. Some
examples are provided in Appendix C.3.

  the column spaces of ˆP u Hu

2 1

and ˆP u Hu

2 3

t

1 3

1 2 3

1 2 3

3.1 The Full Algorithm

Our ﬁnal algorithm follows from combining the three key ideas above. Algorithm 1 shows how
to recover the observation matrices Ou at each node u. Once the Ous are recovered  one can use
standard techniques to recover T and W ; details are described in Algorithm 2 in the Appendix.

3.2 Performance Guarantees

We now provide performance guarantees on our algorithm. Since learning parameters of HMMs
and many other graphical models is NP-Hard  spectral algorithms make simplifying assumptions on
the properties of the model generating the data. Typically these assumptions take the form of some

5

with tree structured hidden state with known tree structure.

Algorithm 1 Algorithm for Observation Matrix Recovery
1: Input: N samples of the three consecutive observations (x1  x2  x3)N
2: for u ∈ V do
3:
4: end for
5: for u ∈ V do
6:
7:

1 2 to get the ﬁrst m left singular vectors ˆU u.

Perform SVD on ˆP u u

Let Hu denote the set of nodes on the unique path from root r to u. Let ˆU Hu = ⊗v∈Hu
Construct Projected Skeletensor. First  compute symmetrization matrices:

ˆU v.

i=1 generated by an HMM

1 = (( ˆU u)� ˆP u Hu
ˆSu

2 3

ˆU Hu)(( ˆU Hu )� ˆP Hu Hu

1 3

ˆU Hu)−1

3 = (( ˆU u)� ˆP u Hu
ˆSu

2 1

ˆU Hu)(( ˆU Hu )� ˆP Hu Hu

3 1

ˆU Hu)−1

8:

Compute symmetrized second and third co-occurrences for u:

ˆM u
ˆM u

2 = ( ˆP Hu u
3 = ˆP Hu u Hu

1 2 3

1 2

( ˆU Hu ( ˆSu

1 )�  ˆU u) + ˆP Hu u
1 )�  ˆU u  ˆU Hu( ˆSu

( ˆU Hu ( ˆSu
3 )�)

1 2

( ˆU Hu ( ˆSu

1 )�  ˆU u)�)/2

9: Orthogonalization and Tensor Decomposition. Orthogonalize ˆM u

3 using ˆM u

2 and decom-

pose to recover (ˆθu
Undo Projection onto Range. Estimate Ou as: ˆOu = ˆU u ˆΘu  where ˆΘu = (ˆθu

m) as in [1] (See Algorithm 3 in the Appendix for details).

1   . . .   ˆθu

1   . . .   ˆθu

m).

10:
11: end for

conditions on the rank of certain parameter matrices. We state below the conditions needed for our
algorithm to successfully learn parameters of a HMM with tree structured hidden states. Observe
that we need two kinds of rank conditions – node-wise and path-wise – to ensure that we can recover
the full set of parameters on a root-to-node path.
Assumption 1 (Node-wise Rank Condition). For all u ∈ V   the matrix Ou has rank m  and the
joint probability matrix P u u
Assumption 2 (Path-wise Rank Condition). For any u ∈ V   let Hu denote the path from root to u.
Then  the joint probability matrix P Hu Hu

2 1 has rank m.

has rank m|Hu|.

1 2

Assumption 1 is required to ensure that the skeletensor can be decomposed  and that ˆU u indeed
captures the range of Ou. Assumption 2 ensures that the symmetrization operation succeeds. This
kind of assumption is very standard in spectral learning [18  1].
[3] has provided a spectral algorithm for learning HMMs involving fourth and higher order moments
when Assumption 1 does not hold. We believe similar approaches will apply to our problem as well 
and we leave this as an avenue for future work.
If Assumptions 1 and 2 hold  we can show that Algorithm 1 is consistent – provided enough samples
are available  the model parameters learnt by the algorithms are close to the true model parameters.
A ﬁnite sample guarantee is provided in the Appendix.

Theorem 1 (Consistency). Suppose we run Algorithm 1 on the ﬁrst three observation vectors
{xi 1  xi 2  xi 3} from N iid sequences generated by an HMM with tree-structured hidden states.
Then  for all nodes u ∈ V   the recovered estimates ˆOu satisfy the following property: with high
probability over the iid samples  there exists a permutation Πu of the columns of ˆOu such that as
�Ou − Πu ˆOu� ≤ ε(N ) where ε(N ) → 0 as N → ∞.
Observe that the observation matrices (as well as the transition and initial probabilities) are recovered
upto permutations of hidden states in a globally consistent manner.

6

4 Experiments

Data and experimental settings. We ran our algorithm  which we call “Spectacle-Tree”  on a
chromatin dataset on human chromosome 1 from nine cell types (H1-hESC  GM12878  HepG2 
HMEC  HSMM  HUVEC  K562  NHEK  NHLF) from the ENCODE project [7]. Following [5]  we
used a biologically motivated tree structure of a star tree with H1-hESC  the embryonic stem cell
type  as the root. There are data for eight chromatin marks for each cell type which we preprocessed
into binary vectors using a standard Poisson background assumption [11]. The chromosome is
divided into 1 246 253 segments of length 200  following [11]. The observed data consists of a
binary vector of length eight for each segment  so the number of possible observations is the number
of all combinations of presence or absence of the chromatin marks (i.e. n = 28 = 256). We set
the number of hidden states  which we interpret as chromatin states  to m = 6  similar to the choice
of ENCODE. Our goals are to discover chromatin states corresponding to biologically important
functional elements such as promoters and enhancers  and to label each chromosome segment with
the most probable chromatin state.
Observe that instead of the ﬁrst few observations from N iid sequences  we have a single long
sequence in the steady state per cell type; thus  similar to [23]  we calculate the empirical co-
occurrence matrices and tensors used in the algorithm based on two and three successive obser-
vations respectively (so  more formally  instead of ˆP1 2  we use the average over t of ˆPt t+1 and
so on). Additionally  we use a projection procedure similar to [4] for rounding negative entries in
the recovered observation matrices. Our experiments reveal that the rank conditions appear to be
satisﬁed for our dataset.

Run time and memory usage comparisons. First  we ﬂattened the HMM with tree-structured
hidden states into an ordinary HMM with an exponentially larger state space. Our Python imple-
mentation of the spectral algorithm for HMMs of [18] ran out of memory while performing singular
value decomposition on the co-occurence matrix  even using sparse matrix libraries. This suggests
that naive application of spectral HMM is not practical for biological data.
Next we compared the performance of Spectacle-Tree to a similar model which additionally con-
strained all transition and observation parameters to be the same on each branch [5]. That work used
several variational approximations to the EM algorithm and reported that SMF (structured mean
ﬁeld) performed the best in their tests. Although we implemented Spectacle-Tree in Matlab and did
not optimize it for run-time efﬁciency  Spectacle-Tree took ∼2 hr  whereas the SMF algorithm took
∼13 hr for 13 iterations to convergence. This suggests that spectral algorithms may be much faster
than variational EM for our model.

Biological interpretation of the observation matrices. Having examined the efﬁciency of
Spectacle-Tree  we next studied the accuracy of the learned parameters. We focused on the observa-
tion matrices which hold most of the interesting biological information. Since the full observation
matrix is very large (28 × 6 where each row is a combination of chromatin marks)  Figure 2 shows
the 8× 6 marginal distribution of each chromatin mark conditioned on each hidden state. Spectacle-
Tree identiﬁed most of the major types of functional elements typically discovered from chromatin
data: repressive  strong enhancer  weak enhancer  promoter  transcribed region and background state
(states 1-6  respectively  in Figure 2b). In contrast  the SMF algorithm used three out of the six states
to model the large background state (i.e. the state with no chromatin marks). It identiﬁed repressive 
transcribed and promoter states (states 2  4  5  respectively  in Figure 2a) but did not identify any
enhancer states  which are one of the most interesting classes for further biological studies.
We believe these results are due to that fact that the background state in the data set is large: ∼62%
of the segments do not have chromatin marks for any cell type. The background state has lower
biological interest but is modeled well by the maximum likelihood approach. In contast  biologi-
cally interesting states such as promoters and enhancers comprise a relatively small fraction of the
genome. We cannot simply remove background segments to make the classes balanced because it
would change the length distribution of the hidden states. Finally  we observed that our model esti-
mated signiﬁcantly different parameters for each cell type which captures different chromatin states
(Appendix Figure 3). For example  we found enhancer states with strong H3K27ac in all cell types
except for H1-hESC  where both enhancer states (3 and 6) had low signal for this mark. This mark
is known to be biologically important in these cells for distinguishing active from poised enhancers

7

(a) SMF

(b) Spectacle-Tree

Figure 2: The compressed observation matrices for the GM12878 cell type estimated by the SMF
and Spectacle-Tree algorithms. The hidden states are on the X axis.

[10]. This suggests that modeling the additional branch-speciﬁc parameters can yield interesting
biological insights.

Comparison of the chromosome segments labels. We computed the most probable state for each
chromosome segment using a posterior decoding algorithm. We tested the accuracy of the predic-
tions using an experimentally deﬁned data set and compared it to SMF and the spectral algorithm
for HMMs run for individual cell types without the tree (Spectral-HMM). Speciﬁcally we assessed
promoter prediction accuracy (state 5 for SMF and state 4 for Spectacle-Tree in Figure 2) using
CAGE data from [14] which was available for six of the nine cell types. We used the F1 score
(harmonic mean of precision and recall) for comparison and found that Spectacle-Tree was much
more accurate than SMF for all six cell types (Table 1). This was because the promoter predictions
of SMF were biased towards the background state so those predictions had slightly higher recall but
much lower speciﬁcity.
Finally  we compared our predictions to Spectral-HMM to assess the value of the tree model. H1-
hESC is the root node so Spectral-HMM and Spectacle-Tree have the same model and obtain the
same accuracy (Table 1). Spectacle-Tree predicts promoters more accurately than Spectral-HMM
for all other cell types except HepG2. However  HepG2 is the most diverged from the root among
the cell types based on the Hamming distance between the chromatin marks. We hypothesize that
for HepG2  the tree is not a good model which slightly reduces the prediction accuracy.

Cell type
SMF
H1-hESC .0273
.0220
GM12878
.0274
.0275
.0255
.0287

HepG2
HUVEC

K562
NHEK

Spectral-HMM Spectacle-Tree

.1930
.1230
.1022
.1221
.0964
.1528

.1930
.1703
.0993
.1621
.1966
.1719

Table 1: F1 score for predicting promoters for six cell types. The highest F1 score for each cell type
is emphasized in bold. Ground-truth labels for the other 3 cell-types are currently unavailable.

Our experiments show that Spectacle-Tree has improved computational efﬁciency  biological inter-
pretability and prediction accuracy on an experimentally-deﬁned feature compared to variational
EM for a similar tree HMM model and a spectral algorithm for single HMMs. A previous study
showed improvements for spectral learning of single HMMs over the EM algorithm [24]. Thus our
algorithms may be useful to the bioinformatics community in analyzing the large-scale chromatin
data sets currently being produced.

Acknowledgements. KC and CZ thank NSF under IIS 1162581 for research support.

8

References
[1] Anima Anandkumar  Rong Ge  Daniel Hsu  Sham M. Kakade  and Matus Telgarsky. Tensor

decompositions for learning latent variable models. CoRR  abs/1210.7559  2012.

[2] Animashree Anandkumar  Daniel Hsu  and Sham M. Kakade. A method of moments for

mixture models and hidden Markov models. CoRR  abs/1203.0683  2012.

[3] B. Balle  X. Carreras  F. Luque  and A. Quattoni. Spectral learning of weighted automata - A

forward-backward perspective. Machine Learning  96(1-2)  2014.

[4] B. Balle  W. L. Hamilton  and J. Pineau. Methods of moments for learning stochastic lan-

guages: Uniﬁed presentation and empirical comparison. In ICML  pages 1386–1394  2014.

[5] Jacob Biesinger  Yuanfeng Wang  and Xiaohui Xie. Discovering and mapping chromatin states

using a tree hidden Markov model. BMC Bioinformatics  14(Suppl 5):S4  2013.

[6] A. Chaganty and P. Liang. Estimating latent-variable graphical models using moments and

likelihoods. In ICML  2014.

[7] ENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human

genome. Nature  489:57–74  2012.

[8] Jason Ernst and Manolis Kellis. Discovery and characterization of chromatin states for sys-

tematic annotation of the human genome. Nature Biotechnology  28(8):817–825  2010.

[9] Bernstein et. al. The NIH Roadmap Epigenomics Mapping Consortium. Nature Biotechnology 

28:1045–1048  2010.

[10] Creyghton et. al. Histone H3K27ac separates active from poised enhancers and predicts devel-

opmental state. Proc Natl Acad Sci  107(50):21931–21936  2010.

[11] Ernst et. al. Mapping and analysis of chromatin state dynamics in nine human cell types.

Nature  473:43–49  2011.

[12] Jun Zhu et al. Characterizing dynamic changes in the human blood transcriptional network.

PLoS Comput Biol  6:e1000671  2010.

[13] M. Hoffman et al. Unsupervised pattern discovery in human chromatin structure through ge-

nomic segmentation. Nature Methods  9(5):473–476  2012.

[14] S. Djebali et al. Landscape of transcription in human cells. Nature  2012.
[15] D. Foster  J. Rodu  and L. Ungar. Spectral dimensionality reduction for HMMs. In CoRR 

2012.

[16] N. Foti  J. Xu  D. Laird  and E. Fox. Stochastic variational inference for hidden markov models.

In NIPS  2014.

[17] N. Halko  P. Martinsson  and J. Tropp. Finding structure with randomness: Probabilistic algo-

rithms for constructing approximate matrix decompositions. SIAM Review  53  2011.

[18] D. Hsu  S. Kakade  and T. Zhang. A spectral algorithm for learning hidden Markov models.

In COLT  2009.

[19] I. Melnyk and A. Banerjee. A spectral algorithm for inference in hidden semi-Markov models.

In AISTATS  2015.

[20] E. Mossel and S Roch. Learning non-singular phylogenies and hidden Markov models. Ann.

Appl. Probab.  16(2)  05 2006.

[21] A. Parikh  L. Song  and E. P. Xing. A spectral algorithm for latent tree graphical models. In

ICML  pages 1065–1072  2011.

[22] A. P. Parikh  L. Song  M. Ishteva  G. Teodoru  and E. P. Xing. A spectral algorithm for latent

junction trees. In UAI  2012.

[23] S. Siddiqi  B. Boots  and G. Gordon. Reduced-rank hidden Markov models. In AISTATS  2010.
[24] J. Song and K. C. Chen. Spectacle: fast chromatin state annotation using spectral learning.

Genome Biology  16:33  2015.

[25] L. Song  M. Ishteva  A. P. Parikh  E. P. Xing  and H. Park. Hierarchical tensor decomposition

of latent tree graphical models. In ICML  2013.

[26] J. Zou  D. Hsu  D. Parkes  and R. Adams. Contrastive learning using spectral methods. In

NIPS  2013.

9

,Charlie Tang
Russ Salakhutdinov
Xiaolong Wang
Liliang Zhang
Liang Lin
Zhujin Liang
Chicheng Zhang
Jimin Song
Kamalika Chaudhuri
Kevin Chen
Boris Hanin
David Rolnick