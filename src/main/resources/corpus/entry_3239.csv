2012,Link Prediction in Graphs with Autoregressive Features,In the paper  we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features  such as the node degree  follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm.,Link Prediction in Graphs with Autoregressive

Features

Emile Richard

CMLA UMR CNRS 8536 

ENS Cachan  France

Stéphane Gaïffas

CMAP - Ecole Polytechnique
& LSTA - Université Paris 6

Abstract

Nicolas Vayatis

CMLA UMR CNRS 8536 

ENS Cachan  France

In the paper  we consider the problem of link prediction in time-evolving graphs.
We assume that certain graph features  such as the node degree  follow a vector
autoregressive (VAR) model and we propose to use this information to improve
the accuracy of prediction. Our strategy involves a joint optimization procedure
over the space of adjacency matrices and VAR matrices which takes into account
both sparsity and low rank properties of the matrices. Oracle inequalities are de-
rived and illustrate the trade-offs in the choice of smoothing parameters when
modeling the joint effect of sparsity and low rank property. The estimate is com-
puted efﬁciently using proximal methods through a generalized forward-backward
agorithm.

1

Introduction

Forecasting systems behavior with multiple responses has been a challenging issue in many contexts
of applications such as collaborative ﬁltering  ﬁnancial markets  or bioinformatics  where responses
can be  respectively  movie ratings  stock prices  or activity of genes within a cell. Statistical model-
ing techniques have been widely investigated in the context of multivariate time series either in the
multiple linear regression setup [4] or with autoregressive models [23]. More recently  kernel-based
regularized methods have been developed for multitask learning [7  2]. These approaches share the
use of the correlation structure among input variables to enrich the prediction on every single output.
Often  the correlation structure is assumed to be given or it is estimated separately. A discrete en-
coding of correlations between variables can be modeled as a graph so that learning the dependence
structure amounts to performing graph inference through the discovery of uncovered edges on the
graph. The latter problem is interesting per se and it is known as the problem of link prediction
where it is assumed that only a part of the graph is actually observed [15  9]. This situation occurs
in various applications such as recommender systems  social networks  or proteomics  and the ap-
propriate tools can be found among matrix completion techniques [21  5  1]. In the realistic setup
of a time-evolving graph  matrix completion was also used and adapted to take into account the
dynamics of the features of the graph [18]. In this paper  we study the prediction problem where the
observation is a sequence of graphs adjacency matrices (At)0≤t≤T and the goal is to predict AT +1.
This type of problem arises in applications such as recommender systems where  given informa-
tion on purchases made by some users  one would like to predict future purchases. In this context 
users and products can be modeled as the nodes of a bipartite graph  while purchases or clicks are
modeled as edges. In functional genomics and systems biology  estimating regulatory networks in
gene expression can be performed by modeling the data as graphs and ﬁtting predictive models is
a natural way for estimating evolving networks in these contexts. A large variety of methods for
link prediction only consider predicting from a single static snapshot of the graph - this includes
heuristics [15  20]  matrix factorization [13]  diffusion [16]  or probabilistic methods [22]. More
recently  some works have investigated using sequences of observations of the graph to improve the
prediction  such as using regression on features extracted from the graphs [18]  using matrix factor-
ization [14]  continuous-time regression [25]. Our main assumption is that the network effect is a

1

cause and a symptom at the same time  and therefore  the edges and the graph features should be
estimated simultaneously. We propose a regularized approach to predict the uncovered links and the
evolution of the graph features simultaneously. We provide oracle bounds under the assumption that
the noise sequence has subgaussian tails and we prove that our procedure achieves a trade-off in the
calibration of smoothing parameters which adjust with the sparsity and the rank of the unknown ad-
jacency matrix. The rest of this paper is organized as follows. In Section 2  we describe the general
setup of our work with the main assumptions and we formulate a regularized optimization problem
which aims at jointly estimating the autoregression parameters and predicting the graph. In Section
3  we provide technical results with oracle inequalities and other theoretical guarantees on the joint
estimation-prediction. Section 4 is devoted to the description of the numerical simulations which
illustrate our approach. We also provide an efﬁcient algorithm for solving the optimization prob-
lem and show empirical results. The proof of the theoretical results are provided as supplementary
material in a separate document.

2 Estimation of low-rank graphs with autoregressive features

Our approach is based on the asumption that features can explain most of the information contained
in the graph  and that these features are evolving with time. We make the following assumptions
about the sequence (At)t≥0 of adjacency matrices of the graphs sequence.

Low-Rank. We assume that the matrices At have low-rank. This reﬂects the presence of highly
connected groups of nodes such as communities in social networks  or product categories and groups
of loyal/fan users in a market place data  and is sometimes motivated by the small number of factors
that explain nodes interactions.
Autoregressive linear features. We assume to be given a linear map ω : Rn×n → Rd deﬁned by

(cid:16)(cid:104)Ω1  A(cid:105) ···  (cid:104)Ωd  A(cid:105)(cid:17)(cid:62)

 

ω(A) =

(1)
where (Ωi)1≤i≤d is a set of n× n matrices. These matrices can be either deterministic or random in
our theoretical analysis  but we take them deterministic for the sake of simplicity. The vector time
series (ω(At))t≥0 has autoregressive dynamics  given by a VAR (Vector Auto-Regressive) model:
(2)
where W0 ∈ Rd×d is a unknown sparse matrix and (Nt)t≥0 is a sequence of noise vectors in Rd.
An example of linear features is the degree (i.e. number of edges connected to each node  or the sum
of their weights if the edges are weighted)  which is a measure of popularity in social and commerce
networks. Introducing

ω(At+1) = W (cid:62)

0 ω(At) + Nt+1 

XT−1 = (ω(A0)  . . .   ω(AT−1))(cid:62) and XT = (ω(A1)  . . .   ω(AT ))(cid:62) 

which are both T × d matrices  we can write this model in a matrix form:

XT = XT−1W0 + NT  

(3)

where NT = (N1  . . .   NT )(cid:62).
This assumes that the noise is driven by time-series dynamics (a martingale increment)  where each
coordinates are independent (meaning that features are independently corrupted by noise)  with a
sub-gaussian tail and variance uniformly bounded by a constant σ2. In particular  no independence
assumption between the Nt is required here.
Notations. The notations (cid:107)·(cid:107)F   (cid:107)·(cid:107)p  (cid:107)·(cid:107)∞  (cid:107)·(cid:107)∗ and (cid:107)·(cid:107)op stand  respectively  for the Frobenius
norm  entry-wise (cid:96)p norm  entry-wise (cid:96)∞ norm  trace-norm (or nuclear norm  given by the sum of the
singular values) and operator norm (the largest singular value). We denote by (cid:104)A  B(cid:105) = tr(A(cid:62)B)
the Euclidean matrix product. A vector in Rd is always understood as a d × 1 matrix. We denote
by (cid:107)A(cid:107)0 the number of non-zero elements of A. The product A ◦ B between two matrices with
matching dimensions stands for the Hadamard or entry-wise product between A and B. The matrix
|A| contains the absolute values of entries of A. The matrix (M )+ is the componentwise positive part
of the matrix M  and sign(M ) is the sign matrix associated to M with the convention sign(0) = 0

2

If A is a n × n matrix with rank r  we write its SVD as A = U ΣV (cid:62) = (cid:80)r

j where
Σ = diag(σ1  . . .   σr) is a r × r diagonal matrix containing the non-zero singular values of A in
decreasing order  and U = [u1  . . .   ur]  V = [v1  . . .   vr] are n× r matrices with columns given by
the left and right singular vectors of A. The projection matrix onto the space spanned by the columns
(resp. rows) of A is given by PU = U U(cid:62) (resp. PV = V V (cid:62)). The operator PA : Rn×n → Rn×n
given by PA(B) = PU B + BPV − PU BPV is the projector onto the linear space spanned by the
k for 1 ≤ j  k ≤ r and x  y ∈ Rn. The projector onto the orthogonal space is
matrices ukx(cid:62) and yv(cid:62)
given by P⊥

A (B) = (I − PU )B(I − PV ). We also use the notation a ∨ b = max(a  b).

j=1 σjujv(cid:62)

2.1

Joint prediction-estimation through penalized optimization

In order to reﬂect the autoregressive dynamics of the features  we use a least-squares goodness-of-
ﬁt criterion that encourages the similarity between two feature vectors at successive time steps. In
order to induce sparsity in the estimator of W0  we penalize this criterion using the (cid:96)1 norm. This
leads to the following penalized objective function:

J1(W ) =

F + κ(cid:107)W(cid:107)1 

(cid:107)XT − XT−1W(cid:107)2

1
T
where κ > 0 is a smoothing parameter.
Now  for the prediction of AT +1  we propose to minimize a least-squares criterion penalized by the
combination of an (cid:96)1 norm and a trace-norm. This mixture of norms induces sparsity and a low-rank
of the adjacency matrix. Such a combination of (cid:96)1 and trace-norm was already studied in [8] for the
matrix regression model  and in [19] for the prediction of an adjacency matrix.
The objective function deﬁned below exploits the fact that if W is close to W0  then the features of
the next graph ω(AT +1) should be close to W (cid:62)ω(AT ). Therefore  we consider
F + τ(cid:107)A(cid:107)∗ + γ(cid:107)A(cid:107)1 

(cid:107)ω(A) − W (cid:62)ω(AT )(cid:107)2

J2(A  W ) =

1
d

.
=

1
T

1
d

where τ  γ > 0 are smoothing parameters. The overall objective function is the sum of the two
partial objectives J1 and J2  which is jointly convex with respect to A and W :
L(A  W )
2 + τ(cid:107)A(cid:107)∗ + γ(cid:107)A(cid:107)1  (4)
If we choose convex cones A ⊂ Rn×n and W ⊂ Rd×d  our joint estimation-prediction procedure is
deﬁned by

(cid:107)ω(A) − W (cid:62)ω(AT )(cid:107)2

(cid:107)XT − XT−1W(cid:107)2

F + κ(cid:107)W(cid:107)1 +

( ˆA  ˆW ) ∈ arg min
(A W )∈A×W

(5)
It is natural to take W = Rd×d and A = (R+)n×n since there is no a priori on the values of the
feature matrix W0  while the entries of the matrix AT +1 must be positive.
In the next section we propose oracle inequalities which prove that this procedure can estimate W0
and predict AT +1 at the same time.

L(A  W ).

2.2 Main result

The central contribution of our work is to bound the prediction error with high probability under the
following natural hypothesis on the noise process.
Assumption 1. We assume that (Nt)t≥0 satisﬁes E[Nt|Ft−1] = 0 for any t ≥ 1 and that there is
σ > 0 such that for any λ ∈ R and j = 1  . . .   d and t ≥ 0:

E[eλ(Nt)j|Ft−1] ≤ eσ2λ2/2.

Moreover  we assume that for each t ≥ 0  the coordinates (Nt)1  . . .   (Nt)d are independent.
The main result can be summarized as follows. The prediction error and the estimation error can be
simultaneously bounded by the sum of three terms that involve homogeneously (a) the sparsity  (b)
the rank of the adjacency matrix AT +1  and (c) the sparsity of the VAR model matrix W0. The tight
bounds we obtain are similar to the bounds of the Lasso and are upper bounded by:

3

log d

(cid:107)W0(cid:107)0 + C2

C1

log n

(cid:107)AT +1(cid:107)0 + C3

log n

rank AT +1 .

T

d

d

The positive constants C1  C2  C3 are proportional to the noise level σ. The interplay between the
rank and sparsity constraints on AT +1 are reﬂected in the observation that the values of C2 and C3
can be changed as long as their sum remains constant.

3 Oracle inequalities

In this section we give oracle inequalities for the mixed prediction-estimation error which is given 
for any A ∈ Rn×n and W ∈ Rd×d  by

1
d

1
T

2 +

E(A  W )2 .
=

(cid:107)(W − W0)(cid:62)ω(AT ) − ω(A − AT +1)(cid:107)2

(6)
It is important to have in mind that an upper-bound on E implies upper-bounds on each of
its two components.
It entails in particular an upper-bound on the feature estimation error

(cid:107)XT−1((cid:99)W − W0)(cid:107)F that makes (cid:107)((cid:99)W − W0)(cid:62)ω(AT )(cid:107)2 smaller and consequently controls the
prediction error over the graph edges through (cid:107)ω((cid:98)A − AT +1)(cid:107)2.

(cid:107)XT−1(W − W0)(cid:107)2
F .

The upper bounds on E given below exhibit the dependence of the accuracy of estimation and pre-
diction on the number of features d  the number of edges n and the number T of observed graphs in
the sequence.
Let us recall NT = (N1  . . .   NT )(cid:62) and introduce the noise processes

M = − 1
d

(NT +1)jΩj

and Ξ =

1
T

ω(At−1)N(cid:62)

t +

ω(AT )N(cid:62)

T +1 

1
d

which are  respectively  n × n and d × d random matrices. The source of randomness comes from
the noise sequence (Nt)t≥0  see Assumption 1. If these noise processes are controlled correctly  we
can prove the following oracle inequalities for procedure (5). The next result is an oracle inequality
of slow type (see for instance [3])  that holds in full generality.
Theorem 1. Under Assumption 2  let ( ˆA  ˆW ) be given by (5) and suppose that
γ ≥ 2(1 − α)(cid:107)M(cid:107)∞ and κ ≥ 2(cid:107)Ξ(cid:107)∞

(7)

τ ≥ 2α(cid:107)M(cid:107)op 
for some α ∈ (0  1). Then  we have

E((cid:98)A (cid:99)W )2 ≤

inf

(A W )∈A×W

(cid:110)E(A  W )2 + 2τ(cid:107)A(cid:107)∗ + 2γ(cid:107)A(cid:107)1 + 2κ(cid:107)W(cid:107)1

(cid:111)

.

d(cid:88)

j=1

T(cid:88)

t=1

For the proof of oracle inequalities of fast type  the restricted eigenvalue (RE) condition introduced
in [3] and [10  11] is of importance. Restricted eigenvalue conditions are implied by  and in gen-
eral weaker than  the so-called incoherence or RIP (Restricted isometry property  [6]) assumptions 
which excludes  for instance  strong correlations between covariates in a linear regression model.
This condition is acknowledged to be one of the weakest to derive fast rates for the Lasso (see [24]
for a comparison of conditions).
Matrix version of these assumptions are introduced in [12]. Below is a version of the RE assumption
that ﬁts in our context. First  we need to introduce the two restriction cones.
The ﬁrst cone is related to the (cid:107)W(cid:107)1 term used in procedure (5). If W ∈ Rd×d  we denote by
ΘW = sign(W ) ∈ {0 ±1}d×d the signed sparsity pattern of W and by Θ⊥
W ∈ {0  1}d×d the
orthogonal sparsity pattern. For a ﬁxed matrix W ∈ Rd×d and c > 0  we introduce the cone

C1(W  c)

.
=

W (cid:48) ∈ W : (cid:107)Θ⊥

W ◦ W (cid:48)(cid:107)1 ≤ c(cid:107)ΘW ◦ W (cid:48)(cid:107)1

This cone contains the matrices W (cid:48) that have their largest entries in the sparsity pattern of W .
The second cone is related to mixture of the terms (cid:107)A(cid:107)∗ and (cid:107)A(cid:107)1 in procedure (5). Before deﬁning
it  we need further notations and deﬁnitions.

4

(cid:111)

.

(cid:110)

For a ﬁxed A ∈ Rn×n and c  β > 0  we introduce the cone

C2(A  c  β)

.
=

A(cid:48) ∈ A : (cid:107)P⊥

A (A(cid:48))(cid:107)∗ + β(cid:107)Θ⊥

A ◦ A(cid:48)(cid:107)1 ≤ c

(cid:110)

(cid:16)(cid:107)PA(A(cid:48))(cid:107)∗ + β(cid:107)ΘA ◦ A(cid:48)(cid:107)1

(cid:17)(cid:111)

.

This cone consist of the matrices A(cid:48) with large entries close to that of A and that are “almost aligned”
with the row and column spaces of A. The parameter β quantiﬁes the interplay between these too
notions.
Assumption 2 (Restricted Eigenvalue (RE)). For W ∈ W and c > 0  we have

µ1(W  c) = inf

µ > 0 : (cid:107)ΘW ◦ W (cid:48)(cid:107)F ≤ µ√
T

For A ∈ A and c  β > 0  we introduce

(cid:107)XT−1W (cid:48)(cid:107)F   ∀W (cid:48) ∈ C1(W  c)

.

µ2(A  W  c  β) = inf

µ > 0 : (cid:107)PA(A(cid:48))(cid:107)F ∨ (cid:107)ΘA ◦ A(cid:48)(cid:107)F ≤ µ√
d

(cid:107)W (cid:48)(cid:62)ω(AT ) − ω(A(cid:48))(cid:107)2

(cid:110)
(cid:110)

(cid:111)

(cid:111)

∀W (cid:48) ∈ C1(W  c) ∀A(cid:48) ∈ C2(A  c  β)

.

(8)

The RE assumption consists of assuming that the constants µ1 and µ2 are ﬁnite. Now we can state
the following Theorem that gives a fast oracle inequality for our procedure using RE.
Theorem 2. Under Assumption 2 and Assumption 2  let ( ˆA  ˆW ) be given by (5) and suppose that

E((cid:98)A (cid:99)W )2 ≤

τ ≥ 3α(cid:107)M(cid:107)op 
for some α ∈ (0  1). Then  we have

(cid:110)E(A  W )2+

γ ≥ 3(1 − α)(cid:107)M(cid:107)∞ and κ ≥ 3(cid:107)Ξ(cid:107)∞

µ2(A  W )2(cid:0)τ 2 rank(A)+γ2(cid:107)A(cid:107)0)+

25
36
where µ1(W ) = µ1(W  5) and µ2(A  W ) = µ2(A  W  5  γ/τ ) (see Assumption 2).

(A W )∈A×W

25
18

inf

(9)

κ2µ1(W )2(cid:107)W(cid:107)0

(cid:111)

 

The proofs of Theorems 1 and 2 use tools introduced in [12] and [3].
Note that the residual term from this oracle inequality mixes the notions of sparsity of A and W
via the terms rank(A)  (cid:107)A(cid:107)0 and (cid:107)W(cid:107)0. It says that our mixed penalization procedure provides an
optimal trade-off between ﬁtting the data and complexity  measured by both sparsity and low-rank.
This is the ﬁrst result of this nature to be found in literature.
In the next Theorem 3  we obtain convergence rates for the procedure (5) by combining Theorem 2
with controls on the noise processes. We introduce

(cid:13)(cid:13)(cid:13) 1

d

d(cid:88)

j=1

(cid:13)(cid:13)(cid:13)op

∨(cid:13)(cid:13)(cid:13) 1

d

d(cid:88)

j=1

ΩjΩ(cid:62)

v2
Ω op =

Ω(cid:62)
j Ωj

σ2
ω = max
j=1 ... d

ω j  where σ2
σ2

ω j =

d(cid:88)

(cid:13)(cid:13)(cid:13) 1

Ωj ◦ Ωj

d

v2
Ω ∞ =

ωj(At−1)2 + ωj(AT )2(cid:17)

j=1

 

(cid:13)(cid:13)(cid:13)∞ 

which are the (observable) variance terms that naturally appear in the controls of the noise processes.
We introduce also

j

 

(cid:13)(cid:13)(cid:13)op
(cid:16) 1
T(cid:88)
(cid:18)

t=1

T

(cid:19)

(cid:96)T = 2 max

j=1 ... d

log log

ω j ∨ 1
σ2
σ2

ω j

∨ e

 

which is a small (observable) technical term that comes out of our analysis of the noise process Ξ.
This term is a small price to pay for the fact that no independence assumption is required on the
noise sequence (Nt)t≥0  but only a martingale increment structure with sub-gaussian tails.
Theorem 3. Consider the procedure ( ˆA  ˆW ) given by (5) with smoothing parameters given by

(cid:114)
(cid:18)(cid:114)

τ = 3ασvΩ op
γ = 3(1 − α)σvΩ ∞

(cid:114)

2(x + log(2n))

 

d
2(x + 2 log n)

d

2e(x + 2 log d + (cid:96)T )

κ = 6σσω

T

5

 

(cid:112)2e(x + 2 log d + (cid:96)T )

(cid:19)

d

.

+

for some α ∈ (0  1) and ﬁx a conﬁdence level x > 0. Then  we have

(cid:110)E(A  W )2 + C1(cid:107)W(cid:107)0(x + 2 log d + (cid:96)T )
(cid:16) 1

E((cid:98)A (cid:99)W )2 ≤

inf

(A W )∈A×W

+

T

+ C2(cid:107)A(cid:107)0

2(x + 2 log n)

+ C3 rank(A)

d

(cid:17)

1
d2
2(x + log(2n))

(cid:111)

d

ω  C2 = 25µ2(A  W )2(1−α)2σ2v2

where
C1 = 100eµ1(W )2σ2σ2
with a probability larger than 1 − 17e−x  where µ1 and µ2 are the same as in Theorem 2.
The proof of Theorem 3 follows directly from Theorem 2 basic noise control results. In the next
Theorem  we propose more explicit upper bounds for both the indivivual estimation of W0 and the
prediction of AT +1.
Theorem 4. Under the same assumptions as in Theorem 3 and the same choice of smoothing pa-
rameters  for any x > 0 the following inequalities hold with probability larger than 1 − 17e−x:

Ω ∞  C3 = 25µ2(A  W )2α2σ2v2

Ω op 

• Feature prediction error:
(cid:107)XT ( ˆW − W0)(cid:107)2

1
T

F ≤ 25
36

κ2µ1(W0)2(cid:107)W0(cid:107)0

µ2(A  W0)2(cid:0)τ 2 rank(A) + γ2(cid:107)A(cid:107)0)

(cid:111)

(10)

+ inf
A∈A

(cid:107)ω(A) − ω(AT +1)(cid:107)2

2 +

25
18

• VAR parameter estimation error:

(cid:114) 1

(cid:107) ˆW − W0(cid:107)1 ≤ 5κµ1(W0)2(cid:107)W0(cid:107)0

+6(cid:112)(cid:107)W0(cid:107)0µ1(W0) inf
(cid:107) ˆA−AT +1(cid:107)∗ ≤ 5κµ1(W0)2(cid:107)W0(cid:107)0+µ2(AT +1  W0)(6(cid:112)rank AT +1+5

µ2(A  W0)2(cid:0)τ 2 rank(A) + γ2(cid:107)A(cid:107)0)
(cid:112)(cid:107)AT +1(cid:107)0)

(cid:107)ω(A) − ω(AT +1)(cid:107)2

• Link prediction error:

25
18

A∈A

2 +

d

(11)

µ2(A  W0)2(cid:0)τ 2 rank(A) + γ2(cid:107)A(cid:107)0) .

γ
τ

(12)

× inf
A∈A

(cid:107)ω(A) − ω(AT +1)(cid:107)2

2 +

25
18

d

(cid:110) 1

d

(cid:114) 1

4 Algorithms and Numerical Experiments
4.1 Generalized forward-backward algorithm for minimizing L
We use the algorithm designed in [17] for minimizing our objective function. Note that this algo-
rithm is preferable to the method introduced in [18] as it directly minimizes L jointly in (S  W )
rather than alternately minimizing in W and S.
Moreover we use the novel
graphs. The proximal operator for the trace norm is given by the shrinkage operation 
Z = U diag(σ1 ···   σn)V T is the singular value decomposition of Z 
proxτ||.||∗ (Z) = U diag((σi − τ )+)iV T .

is more suited for estimating
if

joint penalty from [19]

that

Similarly  the proximal operator for the (cid:96)1-norm is the soft thresholding operator deﬁned by using
the entry-wise product of matrices denoted by ◦:

proxγ||.||1(Z) = sgn(Z) ◦ (|Z| − γ)+ .

The algorithm converges under very mild conditions when the step size θ is smaller than 2
L is the operator norm of the joint quadratic loss:

L  where

Φ : (A  W ) (cid:55)→ 1
T

(cid:107)XT − XT−1W(cid:107)2

F +

1
d

(cid:107)ω(A) − W (cid:62)ω(AT )(cid:107)2

F .

6

Algorithm 1 Generalized Forward-Backward to Minimize L

Initialize A  Z1  Z2  W
repeat

Compute (GA  GW ) = ∇A W Φ(A  W ).
Compute Z1 = prox2θτ||.||∗ (2A − Z1 − θGA)
Compute Z2 = prox2θγ||.||1(2A − Z2 − θGA)
Set A = 1
Set W = proxθκ||.||1(W − θGW )

2 (Z1 + Z2)

until convergence
return (A  W ) minimizing L

(cid:18)

(cid:19)

(cid:18)

=

t

(cid:19)

t

4.2 A generative model for graphs having linearly autoregressive features
(cid:62)†
Let V0 ∈ Rn×r be a sparse matrix  V
0 = Ir. Fix two
sparse matrices W0 ∈ Rr×r and U0 ∈ Rn×r . Now deﬁne the sequence of matrices (At)t≥0 for
t = 1  2 ··· by

†
0 its pseudo-inverse such  that V

†
0 V0 = V (cid:62)
0 V

and

Ut = Ut−1W0 + Nt

At = UtV (cid:62)

0 + Mt

for i.i.d sparse noise matrices Nt and Mt  which means that for any pair of indices (i  j)  with high
(cid:62)†
probability (Nt)i j = 0 and (Mt)i j = 0. We deﬁne the linear feature map ω(A) = AV
0   and
point out that

1. The sequence

ω(At)(cid:62)

(cid:62)†
Ut + MtV
0

follows the linear autoregressive relation

ω(At)(cid:62) = ω(At−1)(cid:62)W0 + Nt + MtV

(cid:62)†
0

.

2. For any time index t  the matrix At is close to UtV0 that has rank at most r
3. The matrices At and Ut are both sparse by construction.

4.3 Empirical evaluation

We tested the presented methods on synthetic data generated as in section (4.2). In our experiments
the noise matrices Mt and Nt where built by soft-thresholding i.i.d. noise N (0  σ2). We took as
input T = 10 successive graph snapshots on n = 50 nodes graphs of rank r = 5. We used d = 10
linear features  and ﬁnally the noise level was set to σ = .5. We compare our methods to standard
baselines in link prediction. We use the area under the ROC curve as the measure of performance
and report empirical results averaged over 50 runs with the corresponding conﬁdence intervals in
ﬁgure 4.3. The competitor methods are the nearest neighbors (NN) and static sparse and low-rank
cumulative graph adjacency matrix (cid:102)AT = (cid:80)T
estimation  that is the link prediction algorithm suggested in [19]. The algorithm NN scores pairs
of nodes with the number of common friends between them  which is given by A2 when A is the
is obtained by minimizing the objective (cid:107)X − (cid:102)AT(cid:107)2
t=0 At and the static sparse and low-rank estimation
F + τ(cid:107)X(cid:107)∗ + γ(cid:107)X(cid:107)1  and can be seen as the
closest static version of our method. The two methods autoregressive low-rank and static low-rank
are regularized using only the trace-norm  (corresponding to forcing γ = 0) and are slightly inferior
consider the feature map ω(A) = AV where (cid:102)AT = U ΣV (cid:62) is the SVD of (cid:102)AT . The parameters τ
to their sparse and low-rank rivals. Since the matrix V0 deﬁning the linear map ω is unknown we

and γ are chosen by 10-fold cross validation for each of the methods separately.

4.4 Discussion

1. Comparison with the baselines. This experiment sharply shows the beneﬁt of using a tem-
poral approach when one can handle the feature extraction task. The left-hand plot shows
that if few snapshots are available (T ≤ 4 in these experiments)  then static approaches are

7

Figure 1: Left: performance of algorithms in terms of Area Under the ROC Curve  average and
conﬁdence intervals over 50 runs. Right: Phase transition diagram.

to be preferred  whereas feature autoregressive approaches outperform as soon as sufﬁcient
number T graph snapshots are available (see phase transition). The decreasing performance
of static algorithms can be explained by the fact that they use as input a mixture of graphs
observed at different time steps. Knowing that at each time step the nodes have speciﬁc
latent factors  despite the slow evolution of the factors  adding the resulting graphs leads to
confuse the factors.

2. Phase transition. The right-hand ﬁgure is a phase transition diagram showing in which part
of rank and time domain the estimation is accurate and illustrates the interplay between
these two domain parameters.

3. Choice of the feature map ω. In the current work we used the projection onto the vector
space of the top-r singular vectors of the cumulative adjacency matrix as the linear map ω 
and this choice has shown empirical superiority to other choices. The question of choosing
the best measurement to summarize graph information as in compress sensing seems to
have both theoretical and application potential. Moreover  a deeper understanding of the
connections of our problem with compressed sensing  for the construction and theoretical
validation of the features mapping  is an important point that needs several developments.
One possible approach is based on multi-kernel learning  that should be considered in a
future work.

4. Generalization of the method. In this paper we consider only an autoregressive process of
order 1. For better prediction accuracy  one could consider mode general models  such as
vector ARMA models  and use model-selection techniques for the choice of the orders of
the model. A general modelling based on state-space model could be developed as well.
We presented a procedure for predicting graphs having linear autoregressive features. Our
approach can easily be generalized to non-linear prediction through kernel-based methods.

References

[1] J. Abernethy  F. Bach  Th. Evgeniou  and J.-Ph. Vert. A new approach to collaborative ﬁltering:

operator estimation with spectral regularization. JMLR  10:803–826  2009.

[2] A. Argyriou  M. Pontil  Ch. Micchelli  and Y. Ying. A spectral regularization framework for
multi-task structure learning. Proceedings of Neural Information Processing Systems (NIPS) 
2007.

[3] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig selector.

Annals of Statistics  37  2009.

[4] L. Breiman and J. H. Friedman. Predicting multivariate responses in multiple linear regression.
Journal of the Royal Statistical Society (JRSS): Series B (Statistical Methodology)  59:3–54 
1997.

8

23456789100.750.80.850.90.95TAUCLink prediction performance  Autoregressive Sparse and Low−rankAutoregressive Low−rankStatic Sparse and Low−rankStatic  Low−rankNearest−Neighborsrank AT+1 TAUC  010203040506070246810120.90.910.920.930.940.950.960.970.980.99[5] E.J. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

IEEE Transactions on Information Theory  56(5)  2009.

[6] Candès E. and Tao T. Decoding by linear programming. In Proceedings of the 46th Annual

IEEE Symposium on Foundations of Computer Science (FOCS)  2005.

[7] Th. Evgeniou  Ch. A. Micchelli  and M. Pontil. Learning multiple tasks with kernel methods.

Journal of Machine Learning Research  6:615–637  2005.

[8] S. Gaiffas and G. Lecue. Sharp oracle inequalities for high-dimensional matrix prediction.

Information Theory  IEEE Transactions on  57(10):6942 –6957  oct. 2011.

[9] M. Kolar and E. P. Xing. On time varying undirected graphs.

in Proceedings of the 14th

International Conference on Artiﬁcal Intelligence and Statistics AISTATS  2011.

[10] V. Koltchinskii. The Dantzig selector and sparsity oracle inequalities. Bernoulli  15(3):799–

828  2009.

[11] V. Koltchinskii. Sparsity in penalized empirical risk minimization. Ann. Inst. Henri Poincaré

Probab. Stat.  45(1):7–57  2009.

[12] V. Koltchinskii  K. Lounici  and A. Tsybakov. Nuclear norm penalization and optimal rates for

noisy matrix completion. Annals of Statistics  2011.

[13] Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model.
In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery
and data mining  pages 426–434. ACM  2008.

[14] Y. Koren. Collaborative ﬁltering with temporal dynamics. Communications of the ACM 

53(4):89–97  2010.

[15] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal

of the American society for information science and technology  58(7):1019–1031  2007.

[16] S.A. Myers and Jure Leskovec. On the convexity of latent social network inference. In NIPS 

2010.

[17] H. Raguet  J. Fadili  and G. Peyré. Generalized forward-backward splitting. Arxiv preprint

arXiv:1108.4404  2011.

[18] E. Richard  N. Baskiotis  Th. Evgeniou  and N. Vayatis. Link discovery using graph feature

tracking. Proceedings of Neural Information Processing Systems (NIPS)  2010.

[19] E. Richard  P.-A. Savalle  and N. Vayatis. Estimation of simultaneously sparse and low-rank
matrices. In Proceeding of 29th Annual International Conference on Machine Learning  2012.
[20] P. Sarkar  D. Chakrabarti  and A.W. Moore. Theoretical justiﬁcation of popular link prediction

heuristics. In International Conference on Learning Theory (COLT)  pages 295–307  2010.

[21] N. Srebro  J. D. M. Rennie  and T. S. Jaakkola. Maximum-margin matrix factorization. In
Lawrence K. Saul  Yair Weiss  and Léon Bottou  editors  in Proceedings of Neural Information
Processing Systems 17  pages 1329–1336. MIT Press  Cambridge  MA  2005.

[22] B. Taskar  M.F. Wong  P. Abbeel  and D. Koller. Link prediction in relational data. In Neural

Information Processing Systems  volume 15  2003.

[23] R. S. Tsay. Analysis of Financial Time Series. Wiley-Interscience; 3rd edition  2005.
[24] S. A. van de Geer and P. Bühlmann. On the conditions used to prove oracle results for the

Lasso. Electron. J. Stat.  3:1360–1392  2009.

[25] D.Q. Vu  A. Asuncion  D. Hunter  and P. Smyth. Continuous-time regression models for
In Advances in Neural Information Processing Systems. MIT Press 

longitudinal networks.
2011.

9

,Pierre Baldi
Peter Sadowski
Jie Hu
Rongrong Ji
ShengChuan Zhang
Xiaoshuai Sun
Qixiang Ye
Chia-Wen Lin
Qi Tian