2018,Provably Correct Automatic Sub-Differentiation for Qualified Programs,The \emph{Cheap Gradient Principle}~\citep{Griewank:2008:EDP:1455489} --- the computational cost of computing a $d$-dimensional vector of  partial derivatives of a scalar function is nearly the same (often within a factor of $5$)  as that of simply computing the scalar function itself --- is of central importance in optimization; it allows us to quickly obtain (high-dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing sub-derivatives: widely used ML libraries  including TensorFlow and PyTorch  do \emph{not} correctly compute (generalized) sub-derivatives even on simple differentiable examples. This work considers the question: is there a \emph{Cheap Sub-gradient Principle}?  Our main result shows that  under certain restrictions on our library of non-smooth functions (standard in non-linear programming)  provably correct generalized sub-derivatives can be computed at a computational cost that is within a (dimension-free) factor of $6$ of the cost of computing the scalar function itself.,Provably Correct Automatic Subdifferentiation

for Qualiﬁed Programs

Sham M. Kakade

University of Washington
sham@cs.washington.edu

Jason D. Lee

University of Southern California
jasonlee@marshall.usc.edu

Abstract

The Cheap Gradient Principle [Griewank and Walther  2008] — the computa-
tional cost of computing the gradient of a scalar-valued function is nearly the
same (often within a factor of 5) as that of simply computing the function itself
— is of central importance in optimization; it allows us to quickly obtain (high
dimensional) gradients of scalar loss functions which are subsequently used in
black box gradient-based optimization procedures. The current state of affairs is
markedly different with regards to computing subderivatives: widely used ML
libraries  including TensorFlow and PyTorch  do not correctly compute (general-
ized) subderivatives even on simple examples. This work considers the question:
is there a Cheap Subgradient Principle? Our main result shows that  under certain
restrictions on our library of nonsmooth functions (standard in nonlinear program-
ming)  provably correct generalized subderivatives can be computed at a compu-
tational cost that is within a (dimension-free) factor of 6 of the cost of computing
the scalar function itself.

1

Introduction

The widespread implementation of Automatic Differentiation (AD) methods [Baydin et al.  2015]
has had a transformative effect on applied machine learning; these methods have eased the difﬁ-
culty for practitioners  across a range of disciplines  to learn sophisticated machine learning models
(including deep neural architectures and richer inferential models). The paradigm is: one simply
writes a program to compute the function of interest  say a scalar (loss) function f pxq : Rd Ñ R 
and then a correctly implemented AD method will return both f pxq and all d of its partial derivatives
when provided with x as an input. These partial derivatives are often used in conjunction with some
(stochastic) gradient-based optimization approach.
Underlying the effectiveness of this general black-box approach is the Cheap Gradient Principle
[Griewank and Walther  2008]: the computational cost of computing the vector of partial deriva-
tives pBf {Bx1  Bf {Bx2  . . . Bf {Bxdq is often nearly the same as that of simply computing the scalar
function f pxq itself. In fact  for all rational functions  the striking Baur-Strassen theorem [Baur and
Strassen  1983  Griewank  1989] shows that this increase in computational complexity is a (dimen-
sion free) factor of 5.
In many settings  our underlying function f pxq is a nonsmooth function  and we resort to subgradient
methods. This work considers the question: is there a Cheap Subgradient Principle? Speciﬁcally 
given a program that computes a (locally Lipschitz) function f and given a point x  can we automat-
ically compute an element of the (Clarke) subdifferential Bf pxq [Clarke  1975]  and can we do this
at a cost which is comparable to computing the function f pxq itself? Informally  the set Bf pxq is the
convex hull of limits of gradients at nearby differentiable points. It can be thought of as generalizing
the gradient (for smooth functions) and the subgradient (for convex functions).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

Let us brieﬂy consider how current approaches handle nonsmooth functions  which are available
to the user as functions in some library. Consider the following three equivalent ways to write the
identity function  where x P R 

f1pxq  x 

f2pxq  ReLU pxq  ReLU pxq  

f3pxq  10f1pxq  9f2pxq  

1p0q  f 1

2p0q  f 1

where ReLU pxq  maxtx  0u  and so f1pxq  f2pxq  f3pxq. As these functions are differentiable
3p0q  1. However  both TensorFlow [Abadi et al. 
at 0  the unique derivative is f 1
2015] and PyTorch [Paszke et al.  2017]  claim that f 1
3p0q  10. This
particular answer is due to using a subgradient of 0 at x  0. One may ask if a more judicious
choice ﬁxes such issues; unfortunately  it is not difﬁcult to see that no such universal choice exists1.
This example should be concerning for a number of reasons. The use of nonsmooth functions in AD
go well beyond simple one dimensional nonsmooth functions (such as ReLU pq or the |  |); current
methods permit utilizing eigenvalues  SVDs  QR decompositions (there are AD procedures on these
nonsmooth linear algebra functions [Maclaurin et al.  2015  Seeger et al.  2017]).

2p0q  0  f 1

1p0q  1  f 1

Is correctness important? One option is to disregard these issues — which is the current state
of affairs — based on the observation that in most cases these issues are unlikely to harm our opti-
mization method. In numerical linear algebra  one could make the same argument: we never truly
encounter degenerate linear systems (or degenerate eigenspaces); nonetheless  in retrospect  numer-
ical issues have made evident the importance of carefully addressing these “corner cases”. The
situation may be analogous here: numerical issues in these approaches can easily lead to unstable
outputs. Note that some numerical instability is certainly to be expected due to nonsmoothness (a
point we return to in the Discussion under the notion of mixed stability); yet we would still hope to
have nontrivial stability guarantees in our widely used AD libraries  much in the manner we have
for our established numerical linear algebra libraries [Trefethen and Bau III  1997  Demmel  1997].
Ultimately  the importance of correctness in these methods is a decision that must be made by the
broader ML community. Here  it is worthwhile to consider that AD software has a range of ap-
plications: from physical simulators to health care/social science applications to deployed online
learning systems to differentiable programming. For example  when using physical simulators (say
in robotics or in the sciences)  a strong notion of stability may be critical when doing AD through
nonsmooth system dynamics. In safety-critical settings  we may seek to have deployed online learn-
ing methods which are not susceptible to errors due to misspeciﬁed input-output behavior in our
programs. Perhaps the most compelling reason for provably correct software implementations is to
avoid costly failure modes due to the utilization of the methods in novel and unforeseen manners.

Related Work: These issues are in fact known in the mathematical AD literature (see Griewank
and Walther [2008  Chapter 14]). Once we include either nonsmooth primitive functions or permit
branching in a program  the usual chain rule fails to hold and incorrect input-out behavior is easy to
observe. Due to that established calculus properties of nonsmooth functions [Klatte and Kummer 
2002  Mordukhovich  2006] do not seem amenable to AD approaches  the current provable methods
do not have general purpose  computationally efﬁcient AD methods for subdifferentials.
One inﬂuential and powerful idea is that of lexicographic differentiation [Nesterov  2005]; it is a
property of a subclass of nonsmooth functions which allow these function to inherit a generalized
notion of a chain rule. This idea has been utilized for obtaining correct generalized derivatives in
Khan and Barton [2013]  Griewank [2013]. The difﬁculty is that lexicographic approach often is
expensive in that it involves a dimensional factor in the computational cost increase.
The other relatively few works that do focus on automatic generalized differentiation go through
some notion of algorithmic linearization [A.Griewank  1995  Nesterov  2005  Khan and Barton 
2013  2015  Fiege et al.  2017]  where often piecewise smooth functions are considered  and the ap-
proach attempts at correct AD through probing the pieces through some linearization (see Griewank
[2014] for review). The difﬁculties are due to understanding what information we can extract
through linear “probes” into the function.

1By deﬁning ReLU1p0q  1{2  the reader may note we obtain the correct derivative on f2  f3; how-
ever  consider f4pxq  ReLU pReLU pxqq  ReLU pxq  which also equals f1pxq. Here  we would need
ReLU1p0q 

to obtain the correct answer.

?51

2

2

Algorithm 1: Straight Line Program for f pxq
Input: x  px1  . . . xdq
1: for k  d  1  d  2  . . . T do
2:

Compute:

where parentspkq is the index set of the “parent” variables of k.

3: end for
Return: xT .

xk  gkpxparentspkqq

Algorithm 2: The Reverse Mode of AD
Input: variables px1  . . . xT q; a computational graph tchildrenptqutPt1 ...T u; the associated

derivatives

1: Initialize: BxT
BxT
2: for t  T  T  1  . . . 1 do
3:

Compute:

 1

BxT
Bxt

 ¸iPchildrenptq

BxT
Bxi

Bxi
Bxt

  BxT
Bx2

  . . . BxT

Bxd	.

4: end for
Return: BxT

Bx   BxT

Bx1

One of the ﬁrst ideas along this line of thought is due to [A.Griewank  1995]  which shows how to
compute directional derivatives of nonsmooth functions through following a “branch” the program
would take on an input (where the branch corresponds to the approach direction in the directional
derivative). In fact  our work uses this basic idea  as does the “branch locking” approach in Khan
[2017]  Griewank [2013]. The difﬁculty in these approaches is in ﬁnding a means to relate this
linearization to properties of the (nonsmooth) functions  which will allow the algorithm to succeed;
naively  we can tell when a method might have failed though it is difﬁcult to guarantee if it will
succeed.
As such  the extant body of work does not contain methods which contain only a constant fac-
tor blow up in the computational cost. Notable differences in this work is that our assumptions
make strong connections to nonlinear programming [Abadie  1967  Peterson  1973  Gould and Tolle 
1971]  which help in characterizing when the linearization approach is informative  and we provide
a key technical result showing a certain chain rule holds for randomized algorithms. Furthermore 
our focus is on generalizing the reverse mode for scalar functions (as opposed to focusing on multi-
variate functions where there is no known Cheap Gradient Principle).

Our contributions: Our main result provides — under a natural set of assumptions widely
used in nonlinear programming — a provably correct Automatic Subdifferentiation procedure 
which given some x  computes both the functional value f pxq and a d dimensional subdifferential
pu1  . . . udq P Bf pxq  with a computational cost that is a factor of at most 6 times that of computing
the scalar function f pxq itself. Our assumption is that our library of functions be implemented in
a manner consistent with the standard constraint qualiﬁcation assumptions in nonlinear program-
ming [Abadie  1967]. In short  this work shows that in fact there is a Cheap Subgradient Principle.

2 Preliminaries

Assume f : Rd Ñ R is a locally Lipschitz function  and recall  that by Rademacher’s theorem  this
implies that f is differentiable almost everywhere. The Clarke subdifferential of f at any point x is
the set [Clarke et al.  2008  Theorem 8.1]

Bf pxq : conv! lim

iÑ8

∇f pxiq : xi

Ω

ÝÑ x)  

(1)

3

where Ω is any full-measure subset of Rd such that f is differentiable at each of its points. Here  the
limit is taken to be the set of all limit points. In classical circumstances  the subdifferential reduces
to more familiar objects. Namely  when f is C 1-smooth at x  the subdifferential Bf pxq consists only
of the gradient ∇f pxq  while for convex functions  it reduces to the subdifferential in the sense of
convex analysis.

2.1 AD Review and The Baur-Strassen Theorem
A straight line program for computing f pxq : Rd Ñ R is speciﬁed by a program of the form shown
in Algorithm 1. Here the functions g1  g2  . . . are assumed to be some function from a library of
functions. In the algebraic circuit complexity model  these functions are either monomials or afﬁne
functions of its inputs.
More generally  we will be interested in utilizing a richer class of functions where g P L  a library of
functions  e.g. we may desire functions like the |  |  ReLU pxq  or ever richer nonsmooth functions
like eigenvalues.
Deﬁne Runtimepf ; xq to be the time it takes to compute f pxq under a given program for f.
Theorem 2.1. [Baur and Strassen  1983  Griewank  1989] Assume all multiplications and additions
have unit runtime cost. If we restrict to the algebraic circuit complexity model (where the functions
gk are either monomials or afﬁne functions)  then it is possible to compute both f pxq and all its
partial derivatives ∇f pxq in time that is at most 5  Runtimepf ; xq.
An algorithm achieving this guarantee is to ﬁrst compute f pxq and then use the reverse mode of AD 
in Algorithm 2. To see the speciﬁc counting argument  see [Morgenstern  1985]. This theorem is
often more general: the reverse mode also correctly returns the derivatives even with a richer family
of smooth functions in our library L  often with a constant factor cost increase as well [Griewank 
1989]. The reverse mode itself has been rediscovered many times [Griewank  2012]; the well known
back-propagation algorithm [Rumelhart et al.  1986] is one example of the reverse mode of AD. The
reverse mode (and the back-propagation algorithm) is not a direct application of the chain rule; the
direct application of the chain rule is referred to as the forward mode of AD (see Griewank and
Walther [2008])  which is d times more expensive procedure to compute the gradient. The reverse
mode can be viewed as a form of dynamic programming. To compare the two  in the reverse mode
  referred to as the adjoints2  while in the forward mode of
of AD  we compute the derivatives BxT
Bxt
AD we would compute (d-dimensional) derivatives of the form Bxt
Bx (referred to as dual numbers).

2.2 Nonsmooth functions and our computational model

To specify how our nonsmooth functions are implemented  we extend the computational model
to allow for branching  using (a restricted version3 of) the Blum-Shub-Smale model of computa-
tion [Blum et al.  1988].
Deﬁnition 2.1 (Computation Model). The computational model for computing any gpxq : Rd Ñ R
in our library (d may be different for each function) is speciﬁed by a program of the form shown in
Algorithm 3. We assume that the function gk z is either a monomial or an afﬁne function of its inputs.
Furthermore  for every g  we assume that there exists a time T   where the program terminates in at
most this amount of time.

Throughout  we make the following assumption:
Assumption 2.1. (Computational Cost) Assume all multiplications and additions have unit runtime
cost and that an execution of an “If” statement is also unit cost. For example  the cost of computing
a monomial is the number of multiplications.

The program implicitly encodes a function that has the following representation:

f pxq  ¸zPt1 1uT

ISz pxqpzpxq 

(2)

2For a variable xT  gpxparentsq  the notation BxT
Bxt

refers to the derivative with respect to xt  but holding all

parent variables of xt as ﬁxed. If xt is an input variable  then this is the usual partial derivative.

3We avoid halting concerns by assuming our programs halt in a bounded amount of time. We also explicitly

avoid discussing tapes and registers in our computational cost model.

4

Algorithm 3: Program for a Nonsmooth function gpxq
Input: x  px1  . . . xdq
1: Initialize a vector z to be all 1’s. z is for notational convenience to keep track of the branch.
2: for k  d  1  d  2  . . . T do
3:

Compute:

xk  gk zpxparentspk zqq

4:

If the program branches at pk  zq  then

•
•

If: xk ¥ 0  zk  1.
Else: zk  1.

If the program halts at pk  zq  then terminate the for loop.

5:
6: end for
Return: xk.

Algorithm 4: ReLU pxq
Input: x  x1
1: Branch:

Algorithm 5: ReLU pxq
Input: x  x1
1: Branch:

• If: x1 ¥ 0  set x2  x1.
• Else: set x2  0.

• If: x3
• Else: set x2  0.

1 ¥ 0  set x2  x1.

Return: x2.

Return: x2.

Figure 1: Two programs that implement ReLU pxq: Both programs are correct and return the same
value. However  the program on the right violates Assumption 3.1 since the gradient of the constraint
function at x  0  ∇px3

1q  3x2

1  0.

where each pz is a polynomial; ISz is the indicator function on the set Sz; and Sz consists of all x
where the program executes branch z when given x as input. The set Sz can be explicitly deﬁned as
follows: for steps k where the programs branches on z  deﬁne hk zpxq  xk; on non-branching k 
deﬁne hk zpxq  1; deﬁne the vector valued function hzpxq  ph1pzq  . . . hT pxqq;

Sz : tx| Ipsignphzpxqq  zqu

(3)

where the signpq is the usual sign function (applied componentwise) taking values in t1  1u (where
we take signp0q  1). Note that Sz is speciﬁed by a set of polynomial inequalities as deﬁned by the
functions hk zpxq.

3 Provable Automatic Subdifferentiation

In the algebraic circuit complexity model  where AD is provably correct  branching is not permitted.
The inclusion of branching into our program leads to a number of subtle issues. Branching allows
us to implement the same nonsmooth function in different manners  which have important conse-
quences in linearization approaches. Consider two different programs (with the same input-output
behavior) for the ReLU pxq function in Figure 1. The left program returns x on the constraint set that
is encoded as S1  tx|x ¥ 0u  while the right program returns x on the constraint set that is encoded
as S1  tx|x3 ¥ 0u. In nonlinear programming  the importance of avoiding encoding constraints
in the latter manner is well-known [Abadie  1967  Peterson  1973  Gould and Tolle  1971].
This example motivates our restriction to only consider library functions that are encoded like the
former set. We will make the standard constraint qualiﬁcation assumption4. Roughly speaking  the
assumption states that ﬁrst order information characterizes the set of feasible perturbations. We state
this assumption in a manner more directly applicable to our setting (see [Abadie  1967  Peterson 
1973  Gould and Tolle  1971]).

4The standard constraint qualiﬁcation assumption on a constraint set is that the tangent cone of the constraint

set equals the linearized cone (of the functions which deﬁne the constraints).

5

Assumption 3.1. (Constraint Qualiﬁcation on our Library) Assume for all g P L that g is locally
Lipschitz and our program for g (in our computational model) satisﬁes the constraint qualiﬁcation
condition on all sets Sz in the following sense: suppose thzu (for binary z) are the corresponding
constraint functions in our program. For any x  v (of the same input dimensionality of g)  assume
that for all z:

lim
δÓ0

psignphzpx  δvqqq  lim
δÓ0

psignphzpxq  δ∇hzpxq  vqq .

Roughly  this states that the set approached along the limiting direction x  δv  when δ Ó 0  can be
determined with ﬁrst order information.

Before we state our main theorem  one more deﬁnition is in order  due to that Runtimepf ; xq may
not be continuous. Deﬁne the limiting runtime Runtimepf ; xq of f at x as the (supremum) runtime
to compute f pxq  as x is approached from nearby points. Precisely 

Runtimepf ; xq : sup! lim

iÑ8

Runtimepf ; xiq : xi Ñ x)  

(where the limit is taken to be the set of all limit points).
Theorem 3.1. (A Cheap Subgradient Principle) Assume that our program for f pxq  in Algorithm 1 
is allowed to use nonsmooth functions from our library L (in addition to afﬁne functions and mono-
mials). Suppose assumptions 2.1 and 3.1 hold. There exists a (randomized) algorithm  which upon
input x  terminates in time that is at most 6  Runtimepf ; xq  and  almost surely  returns both f pxq
and an element u P Bf pxq.

The following example shows one subtle issue with regards to constraint qualiﬁcation.
Example 3.1. (Constraint qualiﬁcation on programs do not compose) Consider the function f pxq 
ReLUx2 (which is equivalent to the smooth function x2). It is straight forward to see that the in-
duced program for f pxq  ReLUx2 (when we unravel it) does not satisfy the constraint qualiﬁca-

tion assumption  even if we do use an implementation of ReLU pq that does satisfy this assumption.
Regardless  in Example 3.4  we show that our algorithm does indeed correctly compute the gradient
on this (continuous) function.

Before we present the construction  we ﬁrst provide a chain rule for nonsmooth functions.

3.1 A Chain Rule for Nonsmooth Functions

Let Drg; vspxq denote the one-sided (Dini) directional derivative:

Drg; vspxq : lim
δÓ0

gpx  δvq  gpxq

δ

.

(note that we are not assuming that v is a unit vector). This derivative exists for all piecewise
polynomials and semialgebraic functions [Coste  2000  Lemma 6.2].
Assumption 3.2. (Overloading the library with ASD subroutines) Assume we have a library of (lo-
cally Lipschitz) functions L computable in our computational model. For any g P L  with the rep-
resentation gpxq  °zPt1 1uT ISz pxqpzpxq  assume we have the following associated automatic

subdifferentiation subroutine ASDrgs with the following behavior: upon input px; vq  the output
ra  d  us  ASDrgspx; vq satisﬁes

a  gpxq  d  Drg; vspxq  u  ∇pzpxq

where z is such that:

pISz px  δvqq  1 .

lim
δÓ0

Roughly speaking  u is the derivative determined by the set Sz which is approached along the limit-
ing direction x  δv  when δ Ó 0.

For any locally Lipschitz function h  deﬁne the limiting total derivate as:

Brh; vspxq : lim
δÓ0

∇hpx  δvq

if the limit exists. For almost all v  the limit exists  and Brh; vspxq is a subdifferential of h.

6

Algorithm 6: Automatic Subdifferentiation
Input: x  px1  . . . xdq  v P Rd.
Initialize: Set 9x1  v1  9x2  v2  . . . 9xd  vd.
1: for k  d  1  d  2  . . . T do
2:

Compute ra  d  us  ASDrgkspxparentspkq; 9xparentspkqq and set:

xk  a  9xk  d 

Bxk

Bxparentspkq

 u

3: end for
4: Compute BxT
Return: xT   and BxT
Bx .

Bx using the Reverse Mode on these precomputed variables.

Theorem 3.2. (A Chain Rule for Nonsmooth Functions) Assume h : Rm Ñ R and g1  . . . gm (where
gi : Rd Ñ R) are locally Lipschitz functions computable in our computational model and that the
function h is overloaded with an ASD subroutine as speciﬁed in Assumption 3.2. Deﬁne:

f pxq : hpg1pxq  . . . gmpxqq  hpgpxqq  

where gpxq is the vector valued function pg1pxq  . . . gmpxqqJ. Denote the m1 vector of (one-sided)
directional derivatives as Drg; vspxq. If it exists  let Brg; vspxq denote md limiting Jacobian matrix
(whose rows are given by the vectors Brgi; vspxq’s). Set:

ra  d  us  ASDrhspgpxq; Drg; vspxqq

For all but a measure 0 set of v  we have that Brf ; vspxq and Brg; vspxq exist and that:

Brf ; vspxq  Brg; vspxqJu .

(4)
Example 3.2. Consider the example x  f2pxq  ReLU pxq  ReLU pxq. We deﬁne hpy1  y2q 
y1 y2  g1pxq  ReLU pxq  and g2pxq  ReLU pxq  so that f2  hpg1pxq  g2pxqq. By applying the
ASD subroutine to h  starting at x  0 with v  1 which leads to running ASDrhspp0  0q; p1  0qq 
ra  d  us (where it is straightforward to verify that u  r1  1sJ)  we obtain

Brf2; vsp0q  Brg; vsp0qT u
0J 1
1

 1

 1 

which is correct. Furthermore  note a correct answer is obtained for any v  0.

Example 3.3. We return to f pxq  ReLUx2 from Example 3.1. Deﬁne hpyq  ReLU pyq 

gpxq  x2  and so f pxq  hpgpxqq. By applying the chain rule lemma at x  0 with v  1 

Brf ; vsp0q  Brg; vsp0qu  0  u  0

Subtly  note that ra  d  us  ASDrhsp0; 0q  so we are feeding a degenerate direction d  0 into our
subroutine. Regardless  the chain rule lemma still applies (for any v in this case).

3.2 The algorithm

We ﬁrst present the algorithm that utilizes an overloaded library. We then provide a provably correct
construction of this overloaded library. All proofs are provided in the appendix.

Subdifferentiation with the overloaded library

Algorithm 6 is the Automatic Subdifferentiation procedure. Correctness follows from Lemma 3.1.
Lemma 3.1. Suppose Assumptions 2.1 and 3.2 hold. Upon input of an arbitrary x  and if v is
sampled uniformly at random from the unit sphere  then  almost surely  Algorithm 6 returns both
f pxq and an element u P Bf pxq.

7

Algorithm 7: Overloading the function gpxq
Input: x  px1  . . . xdq  v P Rd.
Initialize: Set 9x1  v1  9x2  v2  . . . 9xd  vd.
1: for k  d  1  d  2  . . . T do
2:

Compute xk  its partial derivatives  and the directional derivative:

xk  gk zpxparentspk zqq   " Bxk
Bxj 
j P parentspk  zq*  
9xk  ¸jPparentspk zq
If the program branches at pk  zq  then:

Bxk
Bxj

9xj

3:

•
•
•

If: xk ¡ 0  then zk  1.
Elseif: xk  0 and 9xk ¥ 0  then zk  1.
Else: zk  1

If the program halts at pk  zq  then terminate the for loop.

4:
5: end for
6: Compute Bxk
Return: ra  d  us  rxk 

9xk  Bxk

Bx s.

Bx using the Reverse Mode on these pre-computed variables.

Proof. Fix k P rd  1  . . .   T s. Every parent variable j P parentpkq can be expressed as xj  ˜gjpxq 
where gj is a piecewise polynomial on the d dimensional input x. Thus

xk  gkp˜g1pxq  . . .   ˜gk1pxqq.

Now the usual chain rule holds for directional derivatives [Shapiro  1990]. As the forward mode of
AD implements the usual chain rule of directional derivatives  then we have 9xj  Dr˜gj; vs.
By Assumption 3.2 and Theorem 3.2  ASDrgkspxparentspkq  9xparentspkqq returns u 

Brgk; 9xparentspkqs and this limiting total derivate satisﬁes the chain rule Brxk; vspxq  Br˜g; vspxqJu.
Since the limiting total derivates satisﬁes the chain rule and the validity of reverse mode AD algo-
rithm relies only on the chain rule  Algorithm 6 correctly computes Brf pxq; vs.
By Rademacher’s theorem and the deﬁnition of Clarke subgradient in Equation (1)  Brf pxq; vs P
Bf pxq  for almost all v.

Bxparentspkq

Bxk

Overloading the Library Functions

The following lemma shows that we can provide a method to correctly overload the library  which
we use in Algorithm 6.
Lemma 3.2.
(Correct Library Overloading) Assume g satisﬁes the constraint qualiﬁca-
tion conditions in Assumption 3.1  Suppose the corresponding representation is gpxq 
°zPt1 1uT ISz pxqpzpxq  On an arbitrary input x and v  Algorithm 7 returns gpxq  Drg; vspxq 
and an element u  ∇pzpxq where z is such that: limδÓ0pISz px  δvqq  1 .
Example 3.4. We again return to ReLUx2 from Example 3.1. Here we examine how h is
which leads to running hp2v2q. However  the gradient is correctly computed  BReLUx2  0  re-
4 Discussion and Open Questions

overloaded based on the implementation in Algorithm 7. When px  vq  p0  1q  we are running
ASDrhsp0; 0q and this may not follow the same branch had we run on the (inﬁnitesimal) input x  v

gardless of the branch taken.

Overloading the Library Functions: It is not difﬁcult to see that piecewise univariate functions
can be implemented in our library.

8

Algorithm 8: σpxq
Input: x  x1
1: Branch:

• If: x1 ¤ b1  set x2  p1pxq.
• Elseif: x1 ¤ b2  set x2  p2pxq.

...

• Elseif: x1 ¤ bk1  set x2  pk1pxq.
• Else: set x2  pkpxq.

Return: x2.

Example 4.1. Univariate Piecewise Polynomial (Algorithm 8). Let σ : R Ñ R be a uni-
variate piecewise polynomial  meaning that the domain R is partitioned into a set of k inter-
vals p8  b1q  pb1  b2q  . . .   pbk1  8q. On each interval  the function is equal to a polynomial
p1  . . .   pk.
Algorithm 8 provides a constraint qualiﬁed program for the function σpq  which can be used as a
library function.

An important step would be in extending our computational model to allow the incorpora-
tion of provably correct automatic subdifferentiation libraries for linear algebra libraries. Auto-
Grad [Maclaurin et al.  2015] does do AD through linear algebra methods though it can not be used
to obtain correct subdifferentials in programs (at nondifferentiable points); obtaining correct gener-
alized derivatives may be particularly important in cases where we deal with low rank methods. We
conjecture our results can be extended  by extending the computational model  to handle these cases
(there is already much known about the ﬁrst order structure of these methods [Seeger et al.  2017]);
technically  SVDs are not exactly computable in either the algebraic circuit complexity model or the
Blum-Shub-Smale model.
Numerical Analysis: The most important open question is how to obtain numerically stable and
accurate solutions [Trefethen and Bau III  1997  Demmel  1997]. We conjecture the techniques
developed here will help in characterizing these issues. In particular  the most natural question is
how to develop algorithms that satisfy the mixed stability criterion: the algorithm should give “nearly
the right answer to nearly the right problem” (as in [Trefethen and Bau III  1997]). For example  for
the abspq function  it should be acceptable for an AD method to provide a subgradient near to 1 for
a small input  ¡ 0 due to roundoff error; however  it would undesirable for numerical error to lead
vastly different gradients than those that arise from any nearby problem. This may be particularly
important when doing AD in physical simulators.

Acknowledgments: We thank Dima Drusvyatskiy for many helpful discussions. Sham Kakade
acknowledges funding from Washington Research Foundation Fund for Innovation in Data-Intensive
Discovery  the NSF through award CCF-1740551  and ONR award N00014-18-1-2247. Jason D.
Lee acknowledges support of the ARO under MURI Award W911NF-11-1-0303. This is part of
the collaboration between US DOD  UK MOD and UK Engineering and Physical Research Council
(EPSRC) under the Multidisciplinary University Research Initiative.

References
M. Abadi  A. Agarwal  P. Barham  E. Brevdo  et al. TensorFlow: Large-scale machine learning on
heterogeneous systems  2015. URL https://www.tensorflow.org/. Software available from
tensorﬂow.org.

J. Abadie. On the kuhn tucker theorem. Nonlinear Programming  pages 19–36  1967.
A.Griewank. Automatic directional differentiation of nonsmooth composite functions. In R.Durier 
editor  Recent developments in Optimization / Seventh French-German Conference on Optimiza-
tion  Dijon 1994  pages 155–169. Springer Verlag  1995.

Walter Baur and Volker Strassen. The complexity of partial derivatives. Theoretical Computer

Science  22:317–330  1983.

9

Atilim Gunes Baydin  Barak A. Pearlmutter  and Alexey Radul. Automatic differentiation in ma-

chine learning: a survey. CoRR  abs/1502.05767  2015.

Lenore Blum  Mike Shub  and Steve Smale. On a theory of computation over the real numbers; NP
completeness  recursive functions and universal machines (extended abstract). In FOCS  pages
387–397. IEEE Computer Society  1988.

F.H. Clarke. Generalized gradients and applications. Trans. Amer. Math. Soc.  205:247–262  Apr.

1975.

F.H. Clarke  Y.S. Ledyaev  R.J. Stern  and P.R. Wolenski. Nonsmooth analysis and control theory 

volume 178. Springer Science & Business Media  2008.

Michel Coste. An introduction to o-minimal geometry. Istituti editoriali e poligraﬁci internazionali

Pisa  2000.

J. W. Demmel. Applied numerical linear algebra. Society for Industrial Mathematics  1997.
Sabrina Fiege  Andrea Walther  Kshitij Kulshreshtha  and Andreas Griewank. Algorithmic differ-
entiation for piecewise smooth functions: a case study for robust optimization. pages 1–16  06
2017.

F.J. Gould and J.W. Tolle. A necessary and sufﬁcient qualiﬁcation for constrained optimization. 20 

03 1971.

Andreas Griewank. On automatic differentiation. In Mathematical Programming: Recent Develop-

ments and Applications  pages 83–108. Kluwer Academic Publishers  1989.

Andreas Griewank. Who invented the reverse mode of differentiation? Optimization Stories  Docu-

menta Matematica  Extra Volume ISMP (2012):389–400  2012.

Andreas Griewank. On stable picewise linearization and generalized differentiation. Optimization

Methods and Software  28(6):1139–1178  2013.

Andreas Griewank. On Automatic Differentiation and Algorithmic Linearization. Pesquisa Opera-

cional  34:621 – 645  12 2014. ISSN 0101-7438.

Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of
Algorithmic Differentiation. Society for Industrial and Applied Mathematics  Philadelphia  PA 
USA  second edition  2008.

K. A. Khan and P.I. Barton. Evaluating an element of the clarke generalized jacobian of a composite

piecewise differentiable function. ACM Trans. Math. Softw.  39:23:1  2013.

K. A. Khan and P.I. Barton. A vector forward mode of automatic differentiation for generalized

derivative evaluation. Optim. Method Softw.  30:1185  2015.

Kamil A. Khan. Branch-locking ad techniques for nonsmooth composite functions and nonsmooth

implicit functions. Optimization Methods and Software  0(0):1–29  2017.

D. Klatte and B. Kummer. Nonsmooth equations in optimization  volume 60 of Nonconvex Optimiza-
tion and its Applications. Kluwer Academic Publishers  Dordrecht  2002. ISBN 1-4020-0550-4.
Regularity  calculus  methods and applications.

Dougal Maclaurin  David Duvenaud  Matthew Johnson  and Ryan P. Adams. Autograd: Reverse-

mode differentiation of native Python  2015. URL http://github.com/HIPS/autograd.

B. S. Mordukhovich.

Variational Analysis and Generalized Differentiation II: Applications.
Springer Berlin Heidelberg  2006. ISBN 9783540312468. URL https://books.google.com/
books?id=lmdmY75lrokC.

Jacques Morgenstern. How to compute fast a function and all its derivatives: a variation on the

theorem of baur-strassen. In SIGA  1985.

Yurii Nesterov. Lexicographic differentiation of nonsmooth functions. Math. Program.  104(2-3):

669–700  2005.

A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison  L. Antiga 

and A. Lerer. Automatic differentiation in pytorch. In NIPS-W  2017.

David W. Peterson. A review of constraint qualiﬁcations in ﬁnite-dimensional spaces. SIAM Review 

15(3):639–654  1973.

10

D. E. Rumelhart  G. E. Hinton  and R. J. Williams. Parallel distributed processing: Explorations
in the microstructure of cognition  vol. 1. chapter Learning Internal Representations by Error
Propagation  pages 318–362. MIT Press  Cambridge  MA  USA  1986.

Matthias W. Seeger  Asmus Hetzel  Zhenwen Dai  and Neil D. Lawrence. Auto-differentiating linear

algebra. CoRR  abs/1710.08717  2017. URL http://arxiv.org/abs/1710.08717.

A. Shapiro. On concepts of directional differentiability. Journal of Optimization Theory and Appli-

cations  66(3):477–487  Sep 1990.

Lloyd N Trefethen and David Bau III. Numerical linear algebra  volume 50. SIAM  1997.

11

,Sham Kakade
Jason Lee