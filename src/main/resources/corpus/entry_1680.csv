2017,Early stopping for kernel boosting algorithms: A general analysis with localized complexities,Early stopping of iterative algorithms is a widely-used form of regularization in statistical learning  commonly used in conjunction with boosting and related gradient-type algorithms. Although consistency results have been established in some settings  such estimators are less well-understood than their analogues based on penalized regularization.  In this paper  for a relatively broad   class of loss functions and boosting algorithms (including   $L^2$-boost  LogitBoost and AdaBoost  among others)  we connect the performance of a stopped iterate to the localized  Rademacher/Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis  now standard in the analysis of penalized estimators  can be used to derive optimal stopping rules.  We derive such stopping rules in detail for various kernel classes  and illustrate the correspondence of our theory with practice for Sobolev kernel classes.,Early stopping for kernel boosting algorithms: A

general analysis with localized complexities

Yuting Wei1

Fanny Yang2∗ Martin J. Wainwright1 2
Department of Statistics1

Department of Electrical Engineering and Computer Sciences2

UC Berkeley

Berkeley  CA 94720

{ytwei  fanny-yang  wainwrig}@berkeley.edu

Abstract

Early stopping of iterative algorithms is a widely-used form of regularization
in statistics  commonly used in conjunction with boosting and related gradient-
type algorithms. Although consistency results have been established in some
settings  such estimators are less well-understood than their analogues based on
penalized regularization. In this paper  for a relatively broad class of loss functions
and boosting algorithms (including L2-boost  LogitBoost and AdaBoost  among
others)  we exhibit a direct connection between the performance of a stopped
iterate and the localized Gaussian complexity of the associated function class.
This connection allows us to show that local ﬁxed point analysis of Gaussian or
Rademacher complexities  now standard in the analysis of penalized estimators 
can be used to derive optimal stopping rules. We derive such stopping rules in
detail for various kernel classes  and illustrate the correspondence of our theory
with practice for Sobolev kernel classes.

1

Introduction

While non-parametric models offer great ﬂexibility  they can also lead to overﬁtting  and thus poor
generalization performance. For this reason  procedures for ﬁtting non-parametric models must
involve some form of regularization  most commonly done by adding some type of penalty to the
objective function. An alternative form of regularization is based on the principle of early stopping  in
which an iterative algorithm is terminated after a pre-speciﬁed number of steps prior to convergence.

While the idea of early stopping is fairly old (e.g.  [31  1  35])  recent years have witnessed renewed
interests in its properties  especially in the context of boosting algorithms and neural network training
(e.g.  [25  12]). Over the past decade  a line of work has yielded some theoretical insight into early
stopping  including works on classiﬁcation error for boosting algorithms [3  13  18  23  39  40] 
L2-boosting algorithms for regression [8  7]  and similar gradient algorithms in reproducing kernel
Hilbert spaces (e.g. [11  10  34  39  26]). A number of these papers establish consistency results for
particular forms of early stopping  guaranteeing that the procedure outputs a function with statistical
error that converges to zero as the sample size increases. On the other hand  there are relatively
few results that actually establish rate optimality of an early stopping procedure  meaning that the
achieved error matches known statistical minimax lower bounds. To the best of our knowledge 
Bühlmann and Yu [8] were the ﬁrst to prove optimality for early stopping of L2-boosting as applied

∗Yuting Wei and Fanny Yang contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

to spline classes  albeit with a rule that was not computable from the data. Subsequent work by
Raskutti et al. [26] reﬁned this analysis of L2-boosting for kernel classes and ﬁrst established an
important connection to the localized Rademacher complexity; see also the related work [39  27  9]
with rates for particular kernel classes.

More broadly  relative to our rich and detailed understanding of regularization via penalization
(e.g.  see the books [17  33  32  37] and papers [2  20] for details)  the theory for early stopping
regularization is still not as well developed. In particular  for penalized estimators  it is now well-
understood that complexity measures such as the localized Gaussian width  or its Rademacher
analogue  can be used to characterize their achievable rates [2  20  32  37]. Is such a general and sharp
characterization also possible in the context of early stopping? The main contribution of this paper
is to answer this question in the afﬁrmative for boosting algorithms in regression and classiﬁcation
problems involving functions in reproducing kernel Hilbert spaces (RKHS).

The remainder of this paper is organized as follows. In Section 2  we provide background on
boosting methods and reproducing kernel Hilbert spaces  and then introduce the updates studied in
this paper. Section 3 is devoted to statements of our main results  followed by a discussion of their
consequences for particular function classes in Section 4. We provide simulations that conﬁrm the
practical effectiveness of our stopping rules and show close agreement with our theoretical predictions.
The proofs for all of our results can be found in the supplemental material.
2 Background and problem formulation
The goal of prediction is to learn a function that maps covariates x ∈ X to responses y ∈ Y. In a
regression problem  the responses are typically real-valued  whereas in a classiﬁcation problem  the
responses take values in a ﬁnite set. In this paper  we study both regression (Y = R) and classiﬁcation
problems (e.g.  Y = {−1  +1} in the binary case) where we observe a collection of n pairs of the
form {(xi  Yi)}n
i=1  with ﬁxed covariates xi ∈ X and corresponding random responses Yi ∈ Y drawn
independently from a distribution PY |xi. In this section  we provide some necessary background on a
gradient-type algorithm which is often referred to as boosting algorithm.
2.1 Boosting and early stopping
Consider a cost function φ : R × R → [0 ∞)  where the non-negative scalar φ(y  θ) denotes the cost
associated with predicting θ when the true response is y. Some common examples of loss functions
φ that we consider in later sections include:
• the least-squares loss φ(y  θ) : = 1
• the logistic regression loss φ(y  θ) = ln(1 + e−yθ) that underlies the LogitBoost algo-
• the exponential loss φ(y  θ) = exp(−yθ) that underlies the AdaBoost algorithm [13].

2 (y − θ)2 that underlies L2-boosting [8] 

rithm [14  15]  and

The least-squares loss is typically used for regression problems (e.g.  [8  11  10  34  39  26])  whereas
the latter two losses are frequently used in the setting of binary classiﬁcation (e.g.  [13  23  15]).
Given some loss function φ and function space F   we deﬁne the population cost functional f (cid:55)→ L(f )
and the corresponding optimal (minimizing) function† via

(cid:104) 1
n(cid:88)
(1)
n
Note that with the covariates {xi}n
i=1 ﬁxed  the functional L is a non-random object. As a standard
2 (y − θ)2  the population minimizer
example  when we adopt the least-squares loss φ(y  θ) = 1
f∗ corresponds to the conditional expectation x (cid:55)→ E[Y |x]. Since we do not have access to the
population distribution of the responses however  the computation of f∗ is impossible. Given our
samples {Yi}n

i=1  we consider instead some procedure applied to the empirical loss

φ(cid:0)Yi  f (xi)(cid:1)(cid:105)

f∗ : = arg min
f∈F

L(f ).

L(f ) : = EY n

1

i=1

 

n(cid:88)

i=1

Ln(f ) : =

1
n

†As clariﬁed in the sequel  our assumptions guarantee uniqueness of f∗.

φ(Yi  f (xi)) 

(2)

where the population expectation has been replaced by an empirical expectation. For example  when
Ln corresponds to the log likelihood of the samples with φ(Yi  f (xi)) = log[P(Yi; f (xi))]  direct
unconstrained minimization of Ln would yield the maximum likelihood estimator.
It is well-known that direct minimization of Ln over a rich function class F may lead to overﬁtting.
A classical method to mitigate this phenomenon is to minimize the sum of the empirical loss with a
penalty term. Adjusting the weight on the regularization term allows for trade-off between ﬁt to the
data  and some form of regularity or smoothness of the ﬁt. The behavior of such penalized estimation
methods is quite well understood (see e.g. the books [17  33  32  37] and papers [2  20] for details).
In this paper  we study a form of algorithmic regularization  based on applying a gradient-type
algorithm to Ln. In particular  we consider boosting algorithms (see survey paper [7]) which involve
“boosting” or improve the ﬁt of a function via a sequence of additive updates (see e.g. [28  13  6  5  29])
and can be understood as forms of functional gradient methods [23  15]. Instead of running until
convergence  we then stop it “early”—that is  after some ﬁxed number of steps. The way in which
the number of steps is chosen is referred to as a stopping rule  and the overall procedure is referred to
as early stopping of a boosting algorithm.

(a)

(cid:80)n
Figure 1: Plots of the squared error (cid:107)f t − f∗(cid:107)2
i=1(f t(xi) − f∗(xi))2 versus the iteration
number t for (a) LogitBoost using a ﬁrst-order Sobolev kernel (b) AdaBoost using the same ﬁrst-order
Sobolev kernel K(x  x(cid:48)) = 1 + min(x  x(cid:48)) which generates a class of Lipschitz functions (splines of
order one). Both plots correspond to a sample size n = 100.

n = 1
n

(b)

In more detail  a broad class of boosting algorithms [23] generate a sequence {f t}∞
the form

t=0 via updates of

gt ∝ arg max
(cid:107)d(cid:107)F≤1

(cid:104)∇Ln(f t)  d(xn

1 )(cid:105) 

f t+1 = f t − αtgt with

(3)
where the scalar {αt}∞
t=0 is a sequence of step sizes chosen by the user  the constraint (cid:107)d(cid:107)F ≤ 1
deﬁnes the unit ball in a given function class F   ∇Ln(f ) ∈ Rn denotes the gradient taken at
For non-decaying step sizes and a convex objective Ln  running this procedure for an inﬁnite
number of iterations will lead to a minimizer of the empirical loss  thus causing overﬁtting. In
order to illustrate this phenomenon  Figure 1 provides plots of the squared error (cid:107)f t − f∗(cid:107)2
n : =

the vector(cid:0)f (x1)  . . .   f (xn))  and (cid:104)h  g(cid:105) is the usual inner product between vectors h  g ∈ Rn.
(cid:0)f t(xi) − f∗(xi)(cid:1)2 versus the iteration number  for LogitBoost in panel (a) and AdaBoost
(cid:80)n

1
n
in panel (b). (See Section 4.2 for more details on how these experiments were set up.)

i=1

In these plots  the dotted line indicates the minimum mean-squared error ρ2
n over all iterates of that
particular run of the algorithm. Both plots are qualitatively similar  illustrating the existence of a
“good” number of iterations to take  after which the MSE greatly increases. Hence a natural problem
is to decide at what iteration T to stop such that the iterate f T satisﬁes bounds of the form

L(f T ) − L(f∗) (cid:45) ρ2

(4)
with high probability. The main results of this paper provide a stopping rule T for which bounds of
the form (4) do in fact hold with high probability over the randomness in the observed responses.

and

n

n

(cid:107)f T − f∗(cid:107)2

n

(cid:45) ρ2

050100150200250Iteration0.040.060.080.100.12Squared error |ftf*|2nEarly stopping for LogitBoost: MSE vs iteration050100150200250Iteration0.10.20.30.40.5Squared error |ftf*|2nMinimum errorEarly stopping for AdaBoost: MSE vs iterationMoreover  as shown by our later results  under suitable regularity conditions 
tation of the minimum squared error ρ2

inf(cid:98)f supf∈F E[L((cid:98)f ) − L(f )]  where the inﬁmum is taken over all possible estimators (cid:98)f. Cou-

the expec-
to the statistical minimax risk

pled with our stopping time guarantee (4) this implies that our estimate achieves the minimax risk up
to constant factors. As a result  our bounds are unimprovable in general (see Corollary 1).

n is proportional

2.2 Reproducing Kernel Hilbert Spaces

j=1 is some collection of points in X   and {ωj}∞

The analysis of this paper focuses on algorithms with the update (3) when the function class F is
a reproducing kernel Hilbert space H (RKHS  see standard sources [36  16  30  4])  consisting of
functions mapping a domain X to the real line R. Any RKHS is deﬁned by a bivariate symmetric
kernel function K : X × X → R which is required to be positive semideﬁnite  i.e. for any integer
semideﬁnite. The associated RKHS is the closure of linear span of the form f (·) =(cid:80)
N ≥ 1 and a collection of points {xj}N
j=1 in X   the matrix [K(xi  xj)]ij ∈ RN×N is positive
j≥1 ωjK(·  xj) 
two functions f1  f2 ∈ H which can be expressed as a ﬁnite sum f1(·) = (cid:80)(cid:96)1
where {xj}∞
(cid:80)(cid:96)2
f2(·) =(cid:80)(cid:96)2
j=1 is a real-valued sequence. For
i=1 αiK(·  xi) and
j=1 αiβjK(xi  xj)
K(xi  xi). For each x ∈ X   the function K(·  x) belongs to

j=1 βjK(·  xj)  the inner product is deﬁned as (cid:104)f1  f2(cid:105)H =(cid:80)(cid:96)1

with induced norm (cid:107)f1(cid:107)2
H   and satisﬁes the reproducing relation (cid:104)f  K(·  x)(cid:105)H = f (x) for all f ∈ H .
Throughout this paper  we assume that the kernel function is uniformly bounded  meaning that there
is a constant L such that supx∈X K(x  x) ≤ L. Such a boundedness condition holds for many kernels
used in practice  including the Gaussian  Laplacian  Sobolev  other types of spline kernels  as well
as any trace class kernel with trignometric eigenfunctions. By rescaling the kernel as necessary  we
may assume without loss of generality that L = 1. As a consequence  for any function f such that
(cid:107)f(cid:107)H ≤ r  we have by the reproducing relation that

H =(cid:80)(cid:96)1

i=1 α2
i

i=1

(cid:107)f(cid:107)∞ = sup

(cid:104)f  K(·  x)(cid:105)H ≤ (cid:107)f(cid:107)H sup

(cid:107)K(·  x)(cid:107)H ≤ r.

x

x

Given samples {(xi  yi)}n
the linear subspace Hn = span{K(·  xi)}n

i=1  by the representer theorem [19]  it is sufﬁcient to restrict ourselves to

n(cid:88)

i=1  for which all f ∈ Hn can be expressed as
1√
n

ωiK(·  xi)

f =

i=1

(5)

for some coefﬁcient vector ω ∈ Rn. Among those functions which achieve the inﬁmum in expression
(1)  let us deﬁne f∗ as the one with the minimum Hilbert norm. This deﬁnition is equivalent to
restricting f∗ to be in the linear subspace Hn.

2.3 Boosting in kernel spaces

For a ﬁnite number of covariates xi from i = 1 . . . n  let us deﬁne the normalized kernel matrix
K ∈ Rn×n with entries Kij = K(xi  xj)/n. Since we can restrict the minimization of Ln and L
from H to the subspace Hn w.l.o.g.  using expression (5) we can then write the function value
vectors f (xn
nKω. As there is a one-to-one correspondence
1 ) ∈ Rn and the corresponding function f ∈ Hn in H by
between the n-dimensional vectors f (xn
the representer theorem  minimization of an empirical loss in the subspace Hn essentially becomes
the n-dimensional problem of ﬁtting a response vector y over the set range(K). In the sequel  all
updates will thus be performed on the function value vectors f (xn

1 ) : = (f (x1)  . . .   f (xn)) as f (xn

1 ) =

√

1 ).

nK∇Ln(f t)

√
∇Ln(f t)K∇Ln(f t)

With a change of variable d(xn
√
study gt = (cid:104)∇Ln(f t)  dt(xn
the form

n

1 ) =

Kz we then have dt(xn

1 )(cid:105) =
  where the maximum is taken over vectors d ∈ range(K). In this paper we
1 )(cid:105)dt in the boosting update (3)  so that the function value iterates take

(cid:104)∇Ln(f t)  d(xn

1 ) : = arg max
(cid:107)d(cid:107)H ≤1

√

√

f t+1(xn

1 ) = f t(xn

1 ) − αnK∇Ln(f t) 

(6)

function (cid:98)f = 1

1 ) = 0 ensures that all iterates f t(xn
where α > 0 is a constant stepsize choice. Choosing f 0(xn
1 )
remain in the range space of K. Our goal is to propose a stopping time T such that the averaged
t=1 f t satisﬁes bounds of the type (4). Importantly  we exhibit such bounds with a

statistical error term δn that is speciﬁed by the localized Gaussian complexity of the kernel class.

T

(cid:80)T

3 Main results

We now turn to the statement of our main results  beginning with the introduction of some regularity
assumptions.

3.1 Assumptions
Recall from our earlier set-up that we differentiate between the empirical loss function Ln in
expression (2)  and the population loss L in expression (1). Apart from assuming differentiability of
both functions  all of our remaining conditions are imposed on the population loss. Such conditions
at the population level are weaker than their analogues at the empirical level.
For a given radius r > 0  let us deﬁne the Hilbert ball around the optimal function f∗ as

BH (f∗  r) : = {f ∈ H | (cid:107)f − f∗(cid:107)H ≤ r}.

Our analysis makes particular use of this ball deﬁned for the radius C 2
where σ is the effective noise level deﬁned as

E[e((Yi−f∗(xi))2/t2)] < ∞(cid:111)

(cid:40)

(cid:110)

min

t | max

i=1 ... n

σ : =

4 (2M + 1)(1 + 2CH )

H : = 2 max{(cid:107)f∗(cid:107)2

(7)
H   32  σ2} 

for least squares
for φ(cid:48)-bounded losses.

(8)

(cid:107)f − g(cid:107)2

(cid:107)f − g(cid:107)2

n

1 ) − g(xn

1 )(cid:105) ≤ M
2

n ≤ L(f ) − L(g) − (cid:104)∇L(g)  f (xn

We assume that the population loss is m-strongly convex and M-smooth over BH (f∗  2CH ) 
meaning that the sandwich inequality
m-M-condition m
2
holds for all f  g ∈ BH (f∗  2CH ). On top of that we assume φ to be M-Lipschitz in the second
argument. To be clear  here ∇L(g) denotes the vector in Rn obtained by taking the gradient of L
1 ). It can be veriﬁed by a straightforward computation that when L is
with respect to the vector g(xn
2 (y − θ)2  the m-M-condition holds for m = M = 1.
induced by the least-squares cost φ(y  θ) = 1
The logistic and exponential loss satisfy this condition (see supp. material)  where it is key that we
have imposed the condition only locally on the ball BH (f∗  2CH ).
In addition to the least-squares cost  our theory also applies to losses L induced by scalar functions φ
that satisfy the following condition:
φ(cid:48)-boundedness max
This condition holds with B = 1 for the logistic loss for all Y  and B = exp(2.5CH ) for the
exponential loss for binary classiﬁcation with Y = {−1  1}  using our kernel boundedness condition.
Note that whenever this condition holds with some ﬁnite B  we can always rescale the scalar loss φ
by 1/B so that it holds with B = 1  and we do so in order to simplify the statement of our results.

for all f ∈ BH (f∗  2CH ) and y ∈ Y.

(cid:12)(cid:12)(cid:12)(cid:12) ∂φ(y  θ)

(cid:12)(cid:12)(cid:12)(cid:12)θ=f (xi)

≤ B

i=1 ... n

∂θ

3.2 Upper bound in terms of localized Gaussian width

Our upper bounds involve a complexity measure known as the localized Gaussian width. In general 
Gaussian widths are widely used to obtain risk bounds for least-squares and other types of M-
estimators. In our case  we consider Gaussian complexities for “localized” sets of the form

En(δ  1) : =

f − g | f  g ∈ H  

(cid:107)f − g(cid:107)H ≤ 1 

(cid:107)f − g(cid:107)n ≤ δ

(cid:111)

.

(cid:110)

The Gaussian complexity localized at scale δ is given by

(cid:0)En(δ  1)(cid:1) : = E(cid:104)

Gn

(cid:105)

n(cid:88)

i=1

sup

g∈En(δ 1)

1
n

wig(xi)

 

(9)

(10)

where (w1  . . .   wn) denotes an i.i.d. sequence of standard Gaussian variables.
An essential quantity in our theory is speciﬁed by a certain ﬁxed point equation that is now standard
in empirical process theory [32  2  20  26]. The critical radius δn is the smallest positive scalar such
that

Gn(En(δ  1))

δ

≤ δ
σ

.

(11)

We note that past work on localized Rademacher and Gaussian complexity [24  2] guarantee that
there exists a unique δn > 0 that satisﬁes this condition  so that our deﬁnition is sensible.

3.2.1 Upper bounds on excess risk and empirical L2(Pn)-error

With this set-up  we are now equipped to state our main theorem. It provides high-probability bounds
on the excess risk and L2(Pn)-error of the estimator ¯f T : = 1
t=1 f t deﬁned by averaging the T
iterates of the algorithm.
Theorem 1. Consider any loss function satisfying the m-M-condition and the φ(cid:48)-boundedness
condition (if not least squares)  for which we generate function iterates {f t}∞
t=0 of the form (6) with
M   M}]  initialized at f 0 = 0. Then  if n is large enough such that δn ≤ M
step size α ∈ (0  min{ 1
m  
for all iterations T = 0  1  . . .(cid:98) m

(cid:99)  the averaged function estimate ¯f T satisﬁes the bounds

T

(cid:80)T

(cid:17)

8M δ2
n

L( ¯f T ) − L(f∗) ≤ CM

(cid:107) ¯f T − f∗(cid:107)2

n ≤ C

(cid:16) 1
(cid:16) 1

αmT

+

αmT

+

δ2
n
m2

δ2
n
m2

(cid:17)

 

 

and

(12a)

(12b)

where both inequalities hold with probability at least 1 − c1 exp(−C2

m2nδ2
n

σ2

).

H : = 2 max{(cid:107)f∗(cid:107)2

In our statements  constants of the form cj are universal  whereas capital Cj may depend on parameters
of the joint distribution and population loss L. In the previous theorem  C2 = { m2
σ2   1} and C depends
H   32  σ2}. In order to gain intuition for the claims in
on the squared radius C 2
the theorem  note that (disregarding factors depending on (m  M ))  for all iterations T (cid:46) 1/δ2
n  the
ﬁrst term 1
m2   so that taking further iterations reduces the upper
bound on the error until T ∼ 1/δ2
Furthermore  note that similar bounds as in Theorem 1 can be obtained for the expected loss (over the
response yi  with the design ﬁxed) by a simple integration argument. Hence if we perform updates
with step size α = 1

n  at which point the upper bound on the error is of the order δ2
n.

αmT dominates the second term δ2

n

M   after τ : =

δ2

m

n max{8 M} iterations  the mean squared error is bounded as
E(cid:107) ¯f τ − f∗(cid:107)2

n ≤ C(cid:48) δ2
m2  

n

(13)
where we use M ≥ m and where C(cid:48) is another constant depending on CH . It is worth noting that
guarantee (13) matches the best known upper bounds for kernel ridge regression (KRR)—indeed  this
must be the case  since a sharp analysis of KRR is based on the same notion of localized Gaussian
complexity. Thus  our results establish a strong parallel between the algorithmic regularization of
early stopping  and the penalized regularization of kernel ridge regression. Moreover  as discussed in
Section 3.3  under suitable regularity conditions on the RKHS  the critical squared radius δ2
n also acts
as a lower bound for the expected risk  i.e. our upper bounds are not improvable in general.

Compared with the work of Raskutti et al. [26]  which also analyzes the kernel boosting iterates of
the form (6)  our theory more directly analyzes the effective function class that is explored in the
boosting process by taking T steps  with the localized Gaussian width (10) appearing more naturally.
In addition  our analysis applies to a broader class of loss functions beyond least-squares.

In the case of reproducing kernel Hilbert spaces  it is possible to sandwich the localized Gaussian
complexity by a function of the eigenvalues of the kernel matrix. Mendelson [24] provides this
argument in the case of the localized Rademacher complexity  but similar arguments apply to the

localized Gaussian complexity. Letting µ1 ≥ µ2 ≥ ··· ≥ µn ≥ 0 denote the ordered eigenvalues of
the normalized kernel matrix K  deﬁne the function

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

j=1

R(δ) =

1√
n

min{δ2  µj}.

(14)

(cid:0)En(δ  1)(cid:1) for

(15)

(16)

Up to a universal constant  this function is an upper bound on the Gaussian width Gn
all δ ≥ 0  and up to another universal constant  it is also a lower bound for all δ ≥ 1√
n.

i=1 through the solution
Note that the critical radius δ2
of inequality (11). In many cases  with examples given in Section 4  it is possible to compute or
upper bound this critical radius  so that a concrete stopping rule can indeed by calculated in advance.

n only depends on our observations {(xi  yi)}n

3.3 Achieving minimax lower bounds
We claim that when the noise Y − f (x) is Gaussian  for a broad class of kernels  upper bound (13)
matches the known minimax lower bound  thus is unimprovable in general. In particular  Yang et
al. [38] deﬁne the class of regular kernels  which includes the Gaussian and Sobolev kernels as
particular cases. For such kernels  the authors provide a minimax lower bound over the unit ball of

the Hilbert space involving δn  which implies that any estimator (cid:98)f has prediction risk lower bounded

as

E(cid:107)(cid:98)f − f∗(cid:107)2

n ≥ c(cid:96)δ2
n.

sup

(cid:107)f∗(cid:107)H ≤1

Comparing the lower bound (15) with upper bound (13) for our estimator ¯f T stopped after O(1/δ2
n)
many steps  it follows that the bounds proven in Theorem 1 are unimprovable apart from constant
factors. We summarize our ﬁndings in the following corollary:
Corollary 1. For the class of regular kernels and any function f∗ with (cid:107)f∗(cid:107)H ≤ 1  running
T : = (cid:98)
M and f 0 = 0 yields an estimate ¯f T such that

n max{8 M}(cid:99) iterations with step size α = m

δ2

1

E(cid:107) ¯f T − f∗(cid:107)2

n (cid:16) inf(cid:98)f

sup

(cid:107)f∗(cid:107)H ≤1

E(cid:107)(cid:98)f − f∗(cid:107)2

n 

where the inﬁmum is taken over all measurable functions of the input data and the expectation is
taken over the randomness of the response variables {Yi}n

i=1.

On a high level  the statement in Corollary 1 implies that stopping early essentially prevents us from
overﬁtting to the data and automatically ﬁnds the optimal balance between low training error (i.e.
ﬁtting the data well) and low model complexity (i.e. generalizing well).
4 Consequences for various kernel classes
In this section  we apply Theorem 1 to derive some concrete rates for different kernel spaces and
then illustrate them with some numerical experiments. It is known that the complexity of a RKHS in
association with ﬁxed covariates {xi}n
i=1 can be characterized by the decay rate of the eigenvalues
{µj}n
j=1 of the normalized kernel matrix K. The representation power of a kernel class is directly
correlated with the eigen-decay: the faster the decay  the smaller the function class.

4.1 Theoretical predictions as a function of decay

In this section  let us consider two broad types of eigen-decay:
• γ-exponential decay: For some γ > 0  the kernel matrix eigenvalues satisfy a decay condition
of the form µj ≤ c1 exp(−c2jγ)  where c1  c2 are universal constants. Examples of kernels in
this class include the Gaussian kernel  which for the Lebesgue measure satisﬁes such a bound
with γ = 2 (real line) or γ = 1 (compact domain).

• β-polynomial decay: For some β > 1/2  the kernel matrix eigenvalues satisfy a decay condition
of the form µj ≤ c1j−2β  where c1 is a universal constant. Examples of kernels in this class

include the kth-order Sobolev spaces for some ﬁxed integer k ≥ 1 with Lebesgue measure on
a bounded domain. We consider Sobolev spaces that consist of functions that have kth-order
weak derivatives f (k) being Lebesgue integrable and f (0) = f (1)(0) = ··· = f (k−1)(0) = 0.
For such classes  the β-polynomial decay condition holds with β = k.

n

(cid:45) (log n)1/γ

(cid:0)E(δ  1)(cid:1)  we can show that for γ-exponentially decaying kernels  we have

Given eigendecay conditions of these types  it is possible to compute an upper bound on the critical
radius δn. In particular  using the fact that the function R from equation (14) is an upper bound
on the function Gn
  whereas for β-polynomial kernels  we have δ2
δ2
n
n
Combining with our Theorem 1  we obtain the following result:
Corollary 2 (Bounds based on eigendecay). Suppose we apply boosting with stepsize α = m
M and
initialization f 0 = 0 on the empirical loss function Ln which satisﬁes the m-M-condition and
φ(cid:48)-boundedness conditions  and is deﬁned on covariate-response pairs {(xi  Yi)}n
i=1 with Yi drawn
from the distribution PY |xi. Then  the error of the averaged iterate ¯f T satisﬁes the following upper
bounds with high probability  “(cid:46)” neglecting dependence on problem parameters such as (m  M ):

2β+1 up to universal constants.

(cid:45) n

− 2β

(cid:107) ¯f T − f∗(cid:107)2

(a) For kernels with γ-exponential eigen-decay with respect to {xi}n
steps.
(b) For kernels with β-polynomial eigen-decay with respect to {xi}n

when stopped after T (cid:16)

(cid:46) log1/γ n

log1/γ n

n

n

n

i=1:

i=1:

(cid:107) ¯f T − f∗(cid:107)2

n

(cid:46) n−2β/(2β+1)  when stopped after T (cid:16) n2β/(2β+1) steps.

In particular  these bounds hold for LogitBoost and AdaBoost.

To the best of our knowledge  this result is the ﬁrst to show non-asymptotic and optimal statistical rates
for the (cid:107) · (cid:107)2
n-error when using early stopping LogitBoost or AdaBoost with an explicit dependence
of the stopping rule on n. Our results also yield similar guarantees for L2-boosting  as has been
established in past work [26]. Note that we can observe a similar trade-off between computational
efﬁciency and statistical accuracy as in the case of kernel least-squares regression [39  26]: although
larger kernel classes (e.g. Sobolev classes) yield higher estimation errors  boosting updates reach the
optimum faster than for a smaller kernel class (e.g. Gaussian kernels).

4.2 Numerical experiments

2| − 1

We now describe some numerical experiments that provide illustrative conﬁrmations of our theoretical
predictions using the ﬁrst-order Sobolev kernel as a typical example for kernel classes with polynomial
eigen-decay. In particular  we consider the ﬁrst-order Sobolev space of Lipschitz functions on the
unit interval [0  1]  deﬁned by the kernel K(x  x(cid:48)) = 1 + min(x  x(cid:48))  and with the design points
{xi}n
i=1 set equidistantly over [0  1]. Note that the equidistant design yields β-polynomial decay
n (cid:16) n−2/3. Accordingly  our theory predicts that the
of the eigenvalues of K with β = 1 so that δ2
stopping time T = (cn)2/3 should lead to an estimate ¯f T such that (cid:107) ¯f T − f∗(cid:107)2
In our experiments for L2-Boost  we sampled Yi according to Yi = f∗(xi)+wi with wi ∼ N (0  0.5) 
which corresponds to the probability distribution P(Y | xi) = N (f∗(xi); 0.5)  where f∗(x) =
|x − 1
4 is deﬁned on the unit interval [0  1]. By construction  the function f∗ belongs to the
ﬁrst-order Sobolev space with (cid:107)f∗(cid:107)H = 1. For LogitBoost  we sampled Yi according to Bern(p(xi))
where p(x) = exp(f∗(x))
1+exp(f∗(x)) with the same f∗. We chose f 0 = 0 in all cases  and ran the updates (6)
for L2-Boost and LogitBoost with the constant step size α = 0.75. We compared various stopping
rules to the oracle gold standard G  which chooses the stopping time G = arg mint≥1 (cid:107)f t − f∗(cid:107)2
that yields the minimum prediction error among all iterates {f t}. Although this procedure is
n
unimplementable in practice  but it serves as a convenient lower bound with which to compare.
Figure 2 shows plots of the mean-squared error (cid:107) ¯f T − f∗(cid:107)2
n over the sample size n averaged over 40
trials  for the gold standard T = G and stopping rules based on T = (7n)κ for different choices of
κ. Error bars correspond to the standard errors computed from our simulations. Panel (a) shows the
behavior for L2-boosting  whereas panel (b) shows the behavior for LogitBoost.

(cid:45) n−2/3.

n

Note that both plots are qualitatively similar and that the theoretically derived stopping rule T = (7n)κ
with κ∗ = 2/3 = 0.67  while slightly worse than the Gold standard  tracks its performance closely.

(a)

(b)

Figure 2: The mean-squared errors for the stopped iterates ¯f T at the Gold standard  i.e. iterate with
the minimum error among all unstopped updates (blue) and at T = (7n)κ (with the theoretically
optimal κ = 0.67 in red  κ = 0.33 in black and κ = 1 in green) for (a) L2-Boost and (b) LogitBoost.

We also performed simulations for some “bad” stopping rules  in particular for an exponent κ not
equal to κ∗ = 2/3  indicated by the green and black curves. In the log scale plots in Figure 3 we
can clearly see that for κ ∈ {0.33  1} the performance is indeed much worse  with the difference in
slope even suggesting a different scaling of the error with the number of observations n. Recalling
our discussion for Figure 1  this phenomenon likely occurs due to underﬁtting and overﬁtting effects.

(a)

(b)

Figure 3: Logarithmic plots of the mean-squared errors at the Gold standard in blue and at T = (7n)κ
(with the theoretically optimal rule for κ = 0.67 in red  κ = 0.33 in black and κ = 1 in green) for (a)
L2-Boost and (b) LogitBoost.

5 Discussion

In this paper  we have proven non-asymptotic bounds for early stopping of kernel boosting for a
relatively broad class of loss functions. These bounds allowed us to propose simple stopping rules
which  for the class of regular kernel functions [38]  yield minimax optimal rates of estimation.
Although the connection between early stopping and regularization has long been studied and
explored in the literature  to the best of our knowledge  this paper is the ﬁrst one to establish a
general relationship between the statistical optimality of stopped iterates and the localized Gaussian
complexity  a quantity well-understood to play a central role in controlling the behavior of regularized
estimators based on penalization [32  2  20  37].

There are various open questions suggested by our results. Can fast approximation techniques for
kernels be used to approximately compute optimal stopping rules without having to calculate all
eigenvalues of the kernel matrix? Furthermore  we suspect that similar guarantees can be shown for
the stopped estimator f T which we observed to behave similarly to the averaged estimator ¯f T in our
simulations. It would be of interest to establish results on f T directly.

2004006008001000Sample size n0.0000.0050.0100.0150.0200.025Mean squared error |fTf*|2nGood versus bad rules: L2-BoostOracleStop at = 1.00Stop at = 0.67Stop at = 0.3326272829210Sample size n102Mean squared error |fTf*|2nGood versus bad rules: L2-BoostOracleStop at = 1.00Stop at = 0.67Stop at = 0.332004006008001000Sample size n0.0050.0100.0150.0200.0250.030Mean squared error |fTf*|2nGood versus bad rules: LogitBoostOracleStop at = 1.00Stop at = 0.67Stop at = 0.3326272829210Sample size n102Mean squared error |fTf*|2nGood versus bad rules: LogitBoostOracleStop at = 1.00Stop at = 0.67Stop at = 0.33Acknowledgements

This work was partially supported by DOD Advanced Research Projects Agency W911NF-16-1-
0552  National Science Foundation grant NSF-DMS-1612948  and Ofﬁce of Naval Research Grant
DOD-ONR-N00014.

References
[1] R. S. Anderssen and P. M. Prenter. A formal comparison of methods proposed for the numerical solution

of ﬁrst kind integral equations. Jour. Australian Math. Soc. (Ser. B)  22:488–500  1981.

[2] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. Annals of Statistics 

33(4):1497–1537  2005.

[3] P. L. Bartlett and M. Traskin. Adaboost is consistent. Journal of Machine Learning Research  8(Oct):2347–

2368  2007.

[4] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Kluwer

Academic  Norwell  MA  2004.

[5] L. Breiman. Prediction games and arcing algorithms. Neural computation  11(7):1493–1517  1999.

[6] L. Breiman et al. Arcing classiﬁer (with discussion and a rejoinder by the author). Annals of Statistics 

26(3):801–849  1998.

[7] P. Bühlmann and T. Hothorn. Boosting algorithms: Regularization  prediction and model ﬁtting. Statistical

Science  pages 477–505  2007.

[8] P. Bühlmann and B. Yu. Boosting with L2 loss: Regression and classiﬁcation. Journal of American

Statistical Association  98:324–340  2003.

[9] R. Camoriano  T. Angles  A. Rudi  and L. Rosasco. Nytro: When subsampling meets early stopping. In
Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics  pages 1403–1411 
2016.

[10] A. Caponetto and Y. Yao. Adaptation for regularization operators in learning theory. Technical Report

CBCL Paper #265/AI Technical Report #063  Massachusetts Institute of Technology  September 2006.

[11] A. Caponneto. Optimal rates for regularization operators in learning theory. Technical Report CBCL Paper

#264/AI Technical Report #062  Massachusetts Institute of Technology  September 2006.

[12] R. Caruana  S. Lawrence  and C. L. Giles. Overﬁtting in neural nets: Backpropagation  conjugate gradient 

and early stopping. In Advances in Neural Information Processing Systems  pages 402–408  2001.

[13] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to

boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[14] J. Friedman  T. Hastie  R. Tibshirani  et al. Additive logistic regression: a statistical view of boosting (with

discussion and a rejoinder by the authors). Annals of statistics  28(2):337–407  2000.

[15] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics 

29:1189–1232  2001.

[16] C. Gu. Smoothing spline ANOVA models. Springer Series in Statistics. Springer  New York  NY  2002.

[17] L. Gyorﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution-Free Theory of Nonparametric Regression.

Springer Series in Statistics. Springer  2002.

[18] W. Jiang. Process consistency for adaboost. Annals of Statistics  21:13–29  2004.

[19] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Jour. Math. Anal. Appl. 

33:82–95  1971.

[20] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Annals of

Statistics  34(6):2593–2656  2006.

[21] M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs.

American Mathematical Society  Providence  RI  2001.

[22] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer-Verlag 

New York  NY  1991.

[23] L. Mason  J. Baxter  P. L. Bartlett  and M. R. Frean. Boosting algorithms as gradient descent. In Advances

in Neural Information Processing Systems 12  pages 512–518  1999.

[24] S. Mendelson. Geometric parameters of kernel machines. In Proceedings of the Conference on Learning

Theory (COLT)  pages 29–43  2002.

[25] L. Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade  pages 55–69. Springer 

1998.

[26] G. Raskutti  M. J. Wainwright  and B. Yu. Early stopping and non-parametric regression: An optimal

data-dependent stopping rule. Journal of Machine Learning Research  15:335–366  2014.

[27] L. Rosasco and S. Villa. Learning with incremental iterative regularization.

Information Processing Systems  pages 1630–1638  2015.

In Advances in Neural

[28] R. E. Schapire. The strength of weak learnability. Machine learning  5(2):197–227  1990.

[29] R. E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear estimation and

classiﬁcation  pages 149–171. Springer  2003.

[30] B. Schölkopf and A. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.

[31] O. N. Strand. Theory and methods related to the singular value expansion and Landweber’s iteration for

integral equations of the ﬁrst kind. SIAM J. Numer. Anal.  11:798–825  1974.

[32] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press  2000.

[33] A. W. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag  New

York  NY  1996.

[34] E. D. Vito  S. Pereverzyev  and L. Rosasco. Adaptive kernel methods using the balancing principle.

Foundations of Computational Mathematics  10(4):455–479  2010.

[35] G. Wahba. Three topics in ill-posed problems. In M. Engl and G. Groetsch  editors  Inverse and ill-posed

problems  pages 37–50. Academic Press  1987.

[36] G. Wahba. Spline models for observational data. CBMS-NSF Regional Conference Series in Applied

Mathematics. SIAM  Philadelphia  PN  1990.

[37] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press 

2017.

[38] Y. Yang  M. Pilanci  and M. J. Wainwright. Randomized sketches for kernels: Fast and optimal non-

parametric regression. Annals of Statistics  2017. To appear.

[39] Y. Yao  L. Rosasco  and A. Caponnetto. On early stopping in gradient descent learning. Constructive

Approximation  26(2):289–315  2007.

[40] T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics 

33(4):1538–1579  2005.

,Yuting Wei
Fanny Yang
Martin Wainwright