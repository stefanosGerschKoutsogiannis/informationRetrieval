2017,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting,We present an algorithm to identify sparse dependence structure in continuous and non-Gaussian probability distributions  given a corresponding set of data. The conditional independence structure of an arbitrary distribution can be represented as an undirected graph (or Markov random field)  but most algorithms for learning this structure are restricted to the discrete or Gaussian cases. Our new approach allows for more realistic and accurate descriptions of the distribution in question  and in turn better estimates of its sparse Markov structure. Sparsity in the graph is of interest as it can accelerate inference  improve sampling methods  and reveal important dependencies between variables. The algorithm relies on exploiting the connection between the sparsity of the graph and the sparsity of transport maps  which deterministically couple one probability measure to another.,Beyond normality: Learning sparse probabilistic

graphical models in the non-Gaussian setting

Rebecca E. Morrison

MIT

Ricardo Baptista

MIT

Youssef Marzouk

MIT

rmorriso@mit.edu

rsb@mit.edu

ymarz@mit.edu

Abstract

We present an algorithm to identify sparse dependence structure in continuous
and non-Gaussian probability distributions  given a corresponding set of data. The
conditional independence structure of an arbitrary distribution can be represented
as an undirected graph (or Markov random ﬁeld)  but most algorithms for learning
this structure are restricted to the discrete or Gaussian cases. Our new approach
allows for more realistic and accurate descriptions of the distribution in question 
and in turn better estimates of its sparse Markov structure. Sparsity in the graph is
of interest as it can accelerate inference  improve sampling methods  and reveal
important dependencies between variables. The algorithm relies on exploiting the
connection between the sparsity of the graph and the sparsity of transport maps 
which deterministically couple one probability measure to another.

1 Undirected probabilistic graphical models

Given n samples from the joint probability distribution of some random variables X1  . . .   Xp  a
common goal is to discover the underlying Markov random ﬁeld. This ﬁeld is speciﬁed by an
undirected graph G  comprising the set of vertices V = {1  . . .   p} and the set of edges E. The edge
set E encodes the conditional independence structure of the distribution  i.e.  ejk /∈ E ⇐⇒ Xj ⊥⊥
Xk | XV \{jk}. Finding the edges of the graph is useful for several reasons: knowledge of conditional
independence relations can accelerate inference and improve sampling methods  as well as illuminate
structure underlying the process that generated the data samples. This problem—identifying an
undirected graph given samples—is quite well studied for Gaussian or discrete distributions. In the
Gaussian setting  the inverse covariance  or precision  matrix precisely encodes the sparsity of the
graph. That is  a zero appears in the jk-th entry of the precision if and only if variables Xj and Xk
are conditionally independent given the rest. Estimation of the support of the precision matrix is often
achieved using a maximum likelihood estimate with (cid:96)1 penalties. Coordinate descent (glasso) [4] and
neighborhood selection [14] algorithms can be consistent for sparse recovery with few samples  i.e. 
p > n. In the discrete setting  [12] showed that for some particular graph structure  the support of a
generalized covariance matrix encodes the conditional independence structure of the graph  while
[21] employed sparse (cid:96)1-penalized logistic regression to identify Ising Markov random ﬁelds.
Many physical processes  however  generate data that are continuous but non-Gaussian. One example
is satellite images of cloud cover formation  which may greatly impact weather conditions and
climate [25  20]. Other examples include biological processes such as bacteria growth [5]  heartbeat
behavior [19]  and transport in biological tissues [9]. Normality assumptions about the data may
prevent the detection of important conditional dependencies. Some approaches have allowed for
non-Gaussianity  such as the nonparanormal approach of [11  10]  which uses copula functions to
estimate a joint non-Gaussian density while preserving conditional independence. However  this
approach is still restricted by the choice of copula function  and as far as we know  no fully general
approach is yet available. Our goal in this work is to consistently estimate graph structure when

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the underlying data-generating process is non-Gaussian. To do so  we propose the algorithm SING
(Sparsity Identiﬁcation in Non-Gaussian distributions). SING uses the framework of transport maps
to characterize arbitrary continuous distributions  as described in §3. Our representations of transport
maps employ polynomial expansions of degree β. Setting β = 1 (i.e.  linear maps) is equivalent to
assuming that the data are well approximated by a multivariate Gaussian. With β > 1  SING acts as
a generalization of Gaussian-based algorithms by allowing for arbitrarily rich parameterizations of
the underlying data-generating distribution  without additional assumptions on its structure or class.

2 Learning conditional independence
Let X1  . . .   Xp have a smooth and strictly positive density π on Rp. A recent result in [26] shows
that conditional independence of the random variables Xj and Xk can be determined as follows:

Xj ⊥⊥ Xk | XV \{jk} ⇐⇒ ∂jk log π(x) = 0  ∀ x ∈ Rp 

(1)
where ∂jk(·) denotes the jk-th mixed partial derivative. Here  we deﬁne the generalized precision
as the matrix Ω  where Ωjk = Eπ [|∂jk log π(x)|]. Note that Ωjk = 0 is a sufﬁcient condition
that variables Xj and Xk be conditionally independent. Thus  ﬁnding the zeros in the matrix Ω is
equivalent to ﬁnding the graphical structure underlying the density π. Note that the zeros of the
precision matrix in a Gaussian setting encode the same information—the lack of an edge—as the
zeros in the generalized precision introduced here.
Our approach identiﬁes the zeros of Ω and thus the edge set E in an iterative fashion via the
algorithm SING  outlined in §4. Note that computation of an entry of the generalized precision
relies on an expression for the density π. We represent π and also estimate Ω using the notion of a
transport map between probability distributions. This map is estimated from independent samples
x(i) ∼ π  i = 1  . . .   n  as described in §3. Using a map  of the particular form described below 
offers several advantages: (1) computing the generalized precision given the map is efﬁcient (e.g. 
the result of a convex optimization problem); (2) the transport map itself enjoys a notion of sparsity
that directly relates to the Markov structure of the data-generating distribution; (3) a coarse map may
capture these Markov properties without perfectly estimating the high-dimensional density π.
Let us ﬁrst summarize our objective and proposed approach. We aim to solve the following graph
recovery problem:

Given samples {x(i)}n
graph of this distribution and the associated Markov properties.

i=1 from some unknown distribution  ﬁnd the dependence

Our proposed approach loosely follows these steps:

• Estimate a transport map from samples
• Given an estimate of the map  compute the generalized precision Ω
• Threshold Ω to identify a (sparse) graph
• Given a candidate graphical structure  re-estimate the map. Iterate. . .

The ﬁnal step—re-estimating the map  given a candidate graphical structure—makes use of a strong
connection between the sparsity of the graph and the sparsity of the transport map (as shown by [26]
and described in §3.2). Sparsity in the graph allows for sparsity in the map  and a sparser map allows
for improved estimates of Ω. This is the motivation behind an iterative algorithm.

3 Transport maps

The ﬁrst step of SING is to estimate a transport map from samples [13]. A transport map induces
a deterministic coupling of two probability distributions [22  15  18  26]. Here  we build a map
between the distribution generating the samples (i.e.  X ∼ π) and a standard normal distribution
η = N (0  Ip). As described in [28  2]  given two distributions with smooth and strictly positive
densities (π  η) 1 there exists a monotone map S : Rp → Rp such that S(cid:93)π = η and S(cid:93)η = π  where
(2)
(3)

−1(y) det(cid:0)

−1(y)(cid:1)

1Regularity assumptions on π  η can be substantially relaxed  though (2) and (3) may need modiﬁcation [2].

S(cid:93)π(y) = π ◦ S
S(cid:93)η(x) = η ◦ S(x) det (∇S(x)) .

∇S

2

We say η is the pushforward density of π by the map S  and similarly  π is the pullback of η by S.
Many possible transport maps satisfy the measure transformation conditions above. In this work  we
restrict our attention to lower triangular monotone increasing maps. [22  7  2] show that  under the
conditions above  there exists a unique lower triangular map S of the form



S1(x1)
S2(x1  x2)
S3(x1  x2  x3)
...
Sp(x1  . . . . . .   xp)

  

S(x) =

with ∂kSk > 0. The qualiﬁer “lower triangular” refers to the property that each component of the
map Sk only depends on variables x1  . . .   xk. The space of such maps is denoted S∆.
As an example  consider a normal random variable: X ∼ N (0  Σ). Taking the Cholesky decomposi-
tion of the covariance KK T = Σ  then K−1 is a linear operator that maps (in distribution) X to a
random variable Y ∼ N (0  Ip)  and similarly  K maps Y to X. In this example  we associate the
map K−1 with S  since it maps the more exotic distribution to the standard normal:

S(X) = K

−1X d= Y  

−1(Y ) = KY d= X.

S

In general  however  the map S may be nonlinear. This is exactly what allows us to represent and
capture arbitrary non-Gaussianity in the samples. The monotonicity of each component of the map
(that is  ∂kSk > 0) can be enforced by using the following parameterization:

Sk(x1  . . .   xk) = ck(x1  . . .   xk−1) +

exp{hk (x1  . . .   xk−1  t)}dt 

with functions ck : Rk−1 → R and hk : Rk → R. Next  a particular form for ck and hk is
speciﬁed; in this work  we use a linear expansion with Hermite polynomials for ck and Hermite
functions for hk. An important choice is the maximum degree β of the polynomials. With higher
degree  the computational difﬁculty of the algorithm increases by requiring the estimation of more
coefﬁcients in the expansion. This trade-off between higher degree (which captures more possible
nonlinearity) and computational expense is a topic of current research [1]. The space of lower
triangular maps  parameterized in this way  is denoted S β
∆. Computations in the transport map
framework are performed using the software TransportMaps [27].

(cid:90) xk

0

3.1 Optimization of map coefﬁcients is an MLE problem
Let α ∈ Rnα be the vector of coefﬁcients that parameterize the functions ck and hk  which in turn
deﬁne a particular instantiation of the transport map Sα ∈ S β
∆. (We include the subscript in this
subsection to emphasize that the map depends on its particular parameterization  but later drop it for
notational efﬁciency.) To complete the estimation of Sα  it remains to optimize for the coefﬁcients α.
This optimization is achieved by minimizing the Kullback-Leibler divergence between the density in
question  π  and the pullback of the standard normal η by the map Sα [27]:

(cid:0)π||S(cid:93)
αη(cid:1)
αη(cid:1)
(cid:0)log π − log S(cid:93)
(cid:16)
(cid:16)
x(i)(cid:17)(cid:17)
n(cid:88)

log

S(cid:93)

αη

α∗

= argmin

α DKL

= argmin

α

Eπ

≈ argmax

α

1
n

i=1

(4)

(5)

(6)

= ˆα.

As shown in [13  17]  for standard Gaussian η and lower triangular S  this optimization problem
is convex and separable across dimensions 1  . . .   p. Moreover  by line (6)  the solution to the
optimization problem is a maximum likelihood estimate ˆα. Given that the n samples are random  ˆα
converges in distribution as n → ∞ to a normal random variable whose mean is the exact minimizer
α∗  and whose variance is I−1(α∗)/n  where I(α) is the Fisher information matrix. That is:

  as n → ∞.

(7)

(cid:18)

ˆα ∼ N

(cid:19)

α∗

 

1
n

−1(α∗

)

I

3

(a)

(b)

Figure 1: (a) A sparse graph with an optimal ordering; (b) Suboptimal ordering induces extra edges.

Optimizing for the map coefﬁcients yields a representation of the density π as S(cid:93)
possible to compute the conditional independence scores with the generalized precision:

αη. Thus  it is now

(cid:2)(cid:12)(cid:12)∂jk log S(cid:93)
(cid:12)(cid:12)(cid:12)∂jk log S(cid:93)
n(cid:88)
Ωjk = Eπ [|∂jk log π(x)|] = Eπ

1
n

≈

i=1

(cid:12)(cid:12)(cid:3)
x(i)(cid:17)(cid:12)(cid:12)(cid:12) = ˆΩjk.
(cid:16)

αη(x)

αη

(8)

(9)

The next step is to threshold ˆΩ. First  however  we explain the connection between the two notions of
sparsity—one of the graph and the other of the map.

3.2 Sparsity and ordering of the transport map

Because the transport maps are lower triangular  they are in some sense already sparse. However 
it may be possible to prescribe more sparsity in the form of the map. [26] showed that the Markov
structure associated with the density π yields tight lower bounds on the sparsity pattern IS  where
the latter is deﬁned as the set of all pairs (j  k)  j < k  such that the kth component of the map does
not depend on the jth variable: IS := {(j  k) : j < k  ∂jSk = 0}. The variables associated with
the complement of this set are called active. Moreover  these sparsity bounds can be identiﬁed by
simple graph operations; see §5 in [26] for details. Essentially these operations amount to identifying
the intermediate graphs produced by the variable elimination algorithm  but they do not involve
actually performing variable elimination or marginalization. The process starts with node p  creates a
clique between all its neighbors  and then “removes” it. The process continues in the same way with
nodes p − 1  p − 2  and so on until node 1. The edges in the resulting (induced) graph determine the
sparsity pattern of the map IS. In general  the induced graph will be more highly connected unless
the original graph is chordal. Since the set of added edges  or ﬁll-in  depends on the ordering of the
nodes  it is beneﬁcial to identify an ordering that minimizes it. For example  consider the graph in
Figure 1a. The corresponding map has a nontrivial sparsity pattern  and is thus more sparse than a
dense lower triangular map:



S(x) =

S1(x1)
S2(x1  x2)
S3(x1  x2  x3)
S4(
S5(

x3  x4)

x4  x5)

  

IS = {(1  4)  (2  4)  (1  5)  (2  5)  (3  5)}.

(10)

Now consider Figure 1b. Because of the suboptimal ordering  edges must be added to the induced
graph  shown in dashed lines. The resulting map is then less sparse than in 1a: IS = {(1  5)  (2  5)}.
An ordering of the variables is equivalent to a permutation ϕ  but the problem of ﬁnding an optimal
permutation is NP-hard  and so we turn to heuristics. Possible schemes include so-called min-degree
and min-ﬁll [8]. Another that we have found to be successful in practice is reverse Cholesky  i.e.  the
reverse of a good ordering for sparse Cholesky factorization [24]. We use this in the examples below.
The critical point here is that sparsity in the graph implies sparsity in the map. The space of maps
that respect this sparsity pattern is denoted S βI . A sparser map can be described by fewer coefﬁcients
α  which in turn decreases their total variance when found via MLE. This improves the subsequent
estimate of Ω. Numerical results supporting this claim are shown in Figure 2 for a Gaussian grid
graph  p = 16. The plots show three levels of sparsity: “under ” corresponding to a dense lower

4

2134521453triangular map; “exact ” in which the map includes only the necessary active variables; and “over ”
corresponding to a diagonal map. In each case  the variance decreases with increasing sample size 
and the sparser the map  the lower the variance. However  non-negligible bias is incurred when the
map is over-sparsiﬁed; see Figure 2b. Ideally  the algorithm would move from the under-sparsiﬁed
level to the exact level.

(a)

(b)

Figure 2: (a) Variance of ˆΩjk decreases with fewer coefﬁcients and/or more samples; (b) Bias in ˆΩjk
occurs with oversparsiﬁcation. The bias and variance of ˆΩ are computed using the Frobenius norm.

4 Algorithm: SING

We now present the full algorithm. Note that the ending condition is controlled by a variable
DECREASING  which is set to true until the size of the recovered edge set is no longer decreasing. The
ﬁnal ingredient is the thresholding step  explained in §4.1. Subscripts l in the algorithm refer to the
given quantity at that iteration.

:n i.i.d. samples {x(i)}n

Algorithm 1: Sparsity Identiﬁcation in Non-Gaussian distributions (SING)
input
i=1 ∼ π  maximum polynomial degree β
output : sparse edge set ˆE
deﬁne : IS1 = {∅}  l = 1  | ˆE0| = p(p − 1)/2  DECREASING = TRUE
1 while DECREASING = TRUE do
2

  where Sl(cid:93) π = η

Estimate transport map Sl ∈ S βIl
Compute ( ˆΩl)jk = 1
n
Threshold ˆΩl
Compute | ˆEl| (the number of edges in the thresholded graph)
if | ˆEl| < | ˆEl−1| then

i=1

(cid:80)n

(cid:12)(cid:12)∂jk log S(cid:93)

αη(cid:0)x(i)(cid:1)(cid:12)(cid:12)

3

4

5

6
7
8
9

10
11

Find appropriate permutation of variables ϕl (for example  reverse Cholesky ordering)
Identify sparsity pattern of subsequent map ISl+1
l ← l + 1

else

DECREASING = FALSE

SING is not a strictly greedy algorithm—neither for the sparsity pattern of the map nor for the edge
removal of the graph. First  the process of identifying the induced graph may involve ﬁll-in  and the
extent of this ﬁll-in might be larger than optimal due to the ordering heuristics. Second  the estimate
of the generalized precision is noisy due to ﬁnite sample size  and this noise can add randomness to a
thresholding decision. As a result  a variable that is set as inactive may be reactivated in subsequent
iterations. However  we have found that oscillation in the set of active variables is a rare occurence.
Thus  checking that the total number of edges is nondecreasing (as a global measure of sparsity)
works well as a practical stopping criterion.

5

102103Numberofsamples10−1100AveragevarianceinˆΩGridgraph p=16SparsitylevelUnderExactOver102103Numberofsamples10−310−210−1100BiassquaredinˆΩGridgraph p=16SparsitylevelUnderExactOver4.1 Thresholding the generalized precision

An important component of this algorithm is a thresholding of the generalized precision. Based
on literature [3] and numerical results  we model the threshold as τjk = δρjk  where δ is a tuning
parameter and ρjk = [V( ˆΩjk)]1/2 (where V denotes variance). Note that a threshold τjk is computed
at each iteration and for every off-diagonal entry of Ω. More motivation for this choice is given in
the scaling analysis of the following section. The expression (7) yields an estimate of the variances
of the map coefﬁcients ˆα  but this uncertainty must still be propagated to the entries of Ω in order
to compute ρjk. This is possible using the delta method [16]  which states that if a sequence of
one-dimensional random variables satisﬁes

d−→ N

d−→ N

(cid:12)(cid:12)(cid:12)

√n

(cid:12)(cid:12)(cid:12)X (n) − θ
(cid:12)(cid:12)(cid:12)
X (n)(cid:17)
(cid:18) 1

− g (θ)

(cid:0)µ  σ2(cid:1)  
(cid:0)g(µ)  σ2|g
(cid:19)

(∇αΩjk)

(cid:48)

(θ)|2(cid:1) .
(cid:12)(cid:12)(cid:12)α∗ .

The MLE result also states that the coefﬁcients are normally distributed as n → ∞. Thus  generalizing
this method to vector-valued random variables gives an estimate for the variance in the entries of Ω 
as a function of α  evaluated at the true minimizer α∗:
−1(α)

(11)

I

jk ≈ (∇αΩjk)T
ρ2

n

5 Scaling analysis

In this section  we derive an estimate for the number of samples needed to recover the exact graph
with some given probability. We consider a one-step version of the algorithm  or in other words: what
is the probability that the correct graph will be returned after a single step of SING? We also assume
a particular instantiation of the transport map  and that κ  the minimum non-zero edge weight in the
true generalized precision  is given. That is  κ = minj(cid:54)=k Ωjk(cid:54)=0 (Ωjk).
There are two possibilities for each pair (j  k)  j < k: the edge ejk does exist in the true edge set E
(case 1)  or it does not (case 2). In case 1  the estimated value should be greater than its variance  up
to some level of conﬁdence  reﬂected in the choice of δ: Ωjk > δρjk. In the worst case  Ωjk = κ 
so it must be that κ > δρjk. On the other hand  in case 2  in which the edge does not exist  then
similarly κ − δρjk > 0.
If ρjk < κ/δ  then by equation (11)  we have

then for a function g(θ) 

√n

(cid:16)

(cid:12)(cid:12)(cid:12)g

(∇αΩjk)T I
and so it must be that the number of samples

1
n

n > (∇αΩjk)T I

δ

(cid:16) κ
(cid:17)2
−1(α) (∇αΩjk) <
(cid:18) δ
(cid:19)2
(cid:17)

−1(α) (∇αΩjk)
(cid:16)
n∗

κ

(12)

(13)

.

.

jk

jk and set n∗ = maxj(cid:54)=k

Let us deﬁne the RHS above as n∗
Recall that the estimate in line (9) contains the absolute value of a normally distributed quantity 
known as a folded normal distribution. In case 1  the mean is bounded away from zero  and with
small enough variance  the folded part of this distribution is negligible. In case 2  the mean (before
taking the absolute value) is zero  and so this estimate takes the form of a half-normal distribution.
Let us now relate the level of conﬁdence as reﬂected in δ to the probability z that an edge is correctly
estimated. We deﬁne a function for the standard normal (in case 1) φ1 : R+ → (0  1) such that
−1
1 (z1)  and similarly for the half-normal with φ2  δ2  and z2.
φ1(δ1) = z1 and its inverse δ1 = φ
Consider the event Bjk as the event that edge ejk is estimated incorrectly:

(cid:110)(cid:16)

(cid:17)
(ejk ∈ E) ∩ (ˆejk /∈ ˆE)

(cid:16)

(cid:17)(cid:111)
(ejk /∈ E) ∩ (ˆejk ∈ ˆE)

.

∪

Bjk =

6

In case 1 

δ1ρjk < κ =⇒ P (Bjk) <

(1 − z1)

1
2

where the factor of 1/2 appears because this event only occurs when the estimate is below κ (and not
when the estimate is high). In case 2  we have

δ2ρjk < κ =⇒ P (Bjk) < (1 − z2).

To unify these two cases  let us deﬁne z where 1 − z = (1 − z1)/2  and set z = z2. Finally  we have
(Bjk) < (1 − z)  j < k.
Now we bound the probability that at least one edge is incorrect with a union bound:

P

Bjk

P (Bjk)

(14)

(cid:91)

j<k

 ≤

(cid:88)

j<k
1
2

(15)
Note p(p − 1)/2 is the number of possible edges. The probability that an edge is incorrect increases
as p increases  and decreases as z approaches 1. Next  we bound this probability of recovering an
incorrect graph by m. Then p(p − 1)(1 − z) < 2m which yields z > 1 − 2m/ (p(p − 1)). Let

p(p − 1)(1 − z).

=

(cid:20)

(cid:18)

(cid:19)(cid:21)

(cid:19)

∗

δ

= max [δ1  δ2] = max

−1
1

φ

1 −

2m

p(p − 1)

−1
  φ
2

Therefore  to recover the correct graph with probability m we need at least n∗ samples  where

.

(16)

(cid:40)
(∇αΩjk)T I

−1(α) (∇αΩjk)

(cid:18)
(cid:18) δ∗

κ

2m

p(p − 1)

1 −

(cid:19)2(cid:41)

.

∗

n

= max
j(cid:54)=k

6 Examples

6.1 Modiﬁed Rademacher

Consider r pairs of random variables (X  Y )  where:

X ∼ N (0  1)
Y = W X  with W ∼ N (0  1).

(17)
(18)
(A common example illustrating that two random variables can be uncorrelated but not independent
uses draws for W from a Rademacher distribution  which are −1 and 1 with equal probability.) When
r = 5  the corresponding graphical model and support of the generalized precision are shown in
Figure 3. The same ﬁgure also shows the one- and two-dimensional marginal distributions for one
pair (X  Y ). Each 1-dimensional marginal is symmetric and unimodal  but the two-dimensional
marginal is quite non-Gaussian.
Figures 4a–4c show the progression of the identiﬁed graph over the iterations of the algorithm  with
n = 2000  δ = 2  and maximum degree β = 2. The variables are initially permuted to demonstrate
that the algorithm is able to ﬁnd a good ordering. After the ﬁrst iteration  one extra edge remains.
After the second  the erroneous edge is removed and the graph is correct. After the third  the sparsity
of the graph has not changed and the recovered graph is returned as is. Importantly  an assumption of
normality on the data returns the incorrect graph  displayed in Figure 4d. (This assumption can be
enforced by using a linear transport map  or β = 1.) In fact  not only is the graph incorrect  the use of
a linear map fails to detect any edges at all and deems the ten variables to be independent.

6.2 Stochastic volatility

As a second example  we consider data generated from a stochastic volatility model of a ﬁnancial asset
[23  6]. The log-volatility of the asset is modeled as an autoregressive process at times t = 1  . . .   T .
In particular  the state at time t + 1 is given as

Zt+1 = µ + φ(Zt − µ) + t 

t ∼ N (0  1)

(19)

7

(a)

(c)

(b)

Figure 3: (a) The undirected graphical model; (b) One- and two-dimensional marginal distributions
for one pair (X  Y ); (c) Adjacency matrix of true graph (dark blue corresponds to an edge  off-white
to no edge).

(a)

(b)

(c)

(d)

Figure 4: (a) Adjacency matrix of original graph under random variable permutation; (b) Iteration 1;
(c) Iterations 2 and 3 are identical: correct graph recovered via SING with β = 2; (d) Recovered
graph  using SING with β = 1.

where

(cid:18)

(cid:19)

1

Z0|µ  φ ∼ N

µ 

1 − φ2

  µ ∼ N (0  1)
∗
eφ
∗
1 + eφ∗ − 1  φ

∼ N (3  1).

φ = 2

(20)

(21)

The corresponding graph is depicted in Figure 6. With T = 6  samples were generated from the
posterior distribution of the state Z1:6 and hyperparameters µ and φ  given noisy measurements of the
state. Using a relatively large number of samples n = 15000  δ = 1.5  and β = 2  the correct graph
is recovered  shown in Figure 6a. With the same amount of data  a linear map returns the incorrect
graph—having both missing and spurious additional edges. The large number of samples is required

(a)

(b)

Figure 5: (a) The graph of the stochastic volatility model; (b) Adjacency matrix of true graph.

8

1357924680−3−2−10123x−3−2−10123y510246810510246810510246810510246810510246810µφZ1Z2Z3Z4ZTµφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6(a)

(b)

(c)

Figure 6: Recovered graphs using: (a) SING  β = 2  n = 15000; (b) SING  β = 1; (c) GLASSO.

because the edges between hyperparameters and state variables are quite weak. Magnitudes of the
entries of the generalized precision (scaled to have maximum value 1) are displayed in Figure 7a.
The stronger edges may be recovered with a much smaller number of samples (n = 2000)  however;
see Figure 7b. This example illustrates the interplay between the minimum edge weight κ and the
number of samples needed  as seen in the previous section. In some cases  it may be more reasonable
to expect that  given a ﬁxed number of samples  SING could recover edges with edge weight above
some κmin  but would not reliably discover edges below that cutoff. Strong edges could also be
discovered using fewer samples and a modiﬁed SING algorithm with (cid:96)1 penalties (a modiﬁcation to
the algorithm currently under development).
For comparison  Figure 6c shows the graph produced by assuming that the data are Gaussian and
using the GLASSO algorithm [4]. Results were generated for 40 different values of the tuning
parameter λ ∈ (10−6  1). The result shown here was chosen such that the sparsity level is locally
constant with respect to λ  speciﬁcally at λ = .15. Here we see that using a Gaussian assumption
with non-Gaussian data overestimates edges among state variables and underestimates edges between
state variables and the hyperparameter φ.

(a)

(b)

Figure 7: (a) The scaled generalized precision matrix ˆΩ; (b) Strong edges recovered via SING 
n = 2000.

7 Discussion

The scaling analysis presented here depends on a particular representation of the transport map. An
interesting open question is: What is the information-theoretic (representation-independent) lower
bound on the number of samples needed to identify edges in the non-Gaussian setting? This question
relates to the notion of an information gap: any undirected graph satisﬁes the Markov properties of an
inﬁnite number of distributions  and thus identiﬁcation of the graph should require less information
than that of the distribution. Formalizing these notions is an important topic of future work.

Acknowledgments

This work has been supported in part by the AFOSR MURI on “Managing multiple information
sources of multi-physics systems ” program ofﬁcer Jean-Luc Cambier  award FA9550-15-1-0038. We
would also like to thank Daniele Bigoni for generous help with code implementation and execution.

9

µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z60.00.20.40.60.81.0µφZ1Z2Z3Z4Z5Z6µφZ1Z2Z3Z4Z5Z6References
[1] D. Bigoni  A. Spantini  and Y. Marzouk. On the computation of monotone transports. In preparation.

[2] V. I. Bogachev  A. V. Kolesnikov  and K. V. Medvedev. Triangular transformations of measures. Sbornik:

Mathematics  196(3):309  2005.

[3] T. Cai and W. Liu. Adaptive thresholding for sparse covariance matrix estimation. Journal of the American

Statistical Association  106(494):672–684  2011.

[4] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

Biostatistics  9(3):432–441  2008.

[5] S. K. Ghosh  A. G. Cherstvy  D. S. Grebenkov  and R. Metzler. Anomalous  non-Gaussian tracer diffusion

in crowded two-dimensional environments. New Journal of Physics  18(1):013027  2016.

[6] S. Kim  N. Shephard  and S. Chib. Stochastic volatility: likelihood inference and comparison with ARCH

models. The Review of Economic Studies  65(3):361–393  1998.

[7] H. Knothe. Contributions to the theory of convex bodies. The Michigan Mathematical Journal 

1957(1028990175)  1957.

[8] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press  2009.

[9] C. Liu  R. Bammer  B. Acar  and M. E. Moseley. Characterizing non-Gaussian diffusion by using

generalized diffusion tensors. Magnetic Resonance in Medicine  51(5):924–937  2004.

[10] H. Liu  F. Han  M. Yuan  J. Lafferty  and L. Wasserman. High-dimensional semiparametric Gaussian

copula graphical models. The Annals of Statistics  40(4):2293–2326  2012.

[11] H. Liu  J. Lafferty  and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimensional

undirected graphs. Journal of Machine Learning Research  10:2295–2328  2009.

[12] P.-L. Loh and M. J. Wainwright. Structure estimation for discrete graphical models: Generalized covariance

matrices and their inverses. In NIPS  pages 2096–2104  2012.

[13] Y. Marzouk  T. Moselhy  M. Parno  and A. Spantini. Sampling via measure transport: An introduction. In
R. Ghanem  D. Higdon  and H. Owhadi  editors  Handbook of Uncertainty Quantiﬁcation. Springer  2016.

[14] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the lasso. The

Annals of Statistics  pages 1436–1462  2006.

[15] T. A. Moselhy and Y. M. Marzouk. Bayesian inference with optimal maps. Journal of Computational

Physics  231(23):7815–7850  2012.

[16] G. W. Oehlert. A note on the delta method. The American Statistician  46(1):27–29  1992.

[17] M. Parno and Y. Marzouk. Transport map accelerated Markov chain Monte Carlo. arXiv preprint

arXiv:1412.5492  2014.

[18] M. Parno  T. Moselhy  and Y. M. Marzouk. A multiscale strategy for Bayesian inference using transport

maps. SIAM/ASA Journal on Uncertainty Quantiﬁcation  4(1):1160–1190  2016.

[19] C.-K. Peng  J. Mietus  J. Hausdorff  S. Havlin  H. E. Stanley  and A. L. Goldberger. Long-range anticorre-

lations and non-Gaussian behavior of the heartbeat. Physical Review Letters  70(9):1343  1993.

[20] M. Perron and P. Sura. Climatology of non-Gaussian atmospheric statistics. Journal of Climate  26(3):1063–

1083  2013.

[21] P. Ravikumar  M. J. Wainwright  and J. D. Lafferty. High-dimensional Ising model selection using

l1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319  2010.

[22] M. Rosenblatt. Remarks on a multivariate transformation. The Annals of Mathematical Statistics  23(3):470–

472  1952.

[23] H. Rue and L. Held. Gaussian Markov Random Fields: Theory and Applications. Chapman & Hall/CRC

Monographs on Statistics & Applied Probability. CRC Press  2005.

[24] Y. Saad. Iterative Methods for Sparse Linear Systems. SIAM  2003.

10

[25] A. Sengupta  N. Cressie  B. H. Kahn  and R. Frey. Predictive inference for big  spatial  non-Gaussian
data: MODIS cloud data and its change-of-support. Australian & New Zealand Journal of Statistics 
58(1):15–45  2016.

[26] A. Spantini  D. Bigoni  and Y. Marzouk.

arXiv:1703.06131  2017.

Inference via low-dimensional couplings. arXiv preprint

[27] T. M. Team. TransportMaps v1.0. http://transportmaps.mit.edu.

[28] C. Villani. Optimal Transport: Old and New  volume 338. Springer  2008.

11

,Jimmy Ba
Rich Caruana
Rebecca Morrison
Ricardo Baptista
Youssef Marzouk
Jaeho Lee
Maxim Raginsky