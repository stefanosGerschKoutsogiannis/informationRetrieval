2017,EX2: Exploration with Exemplar Models for Deep Reinforcement Learning,Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However  sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations  which can be difficult to train when the observations are very high-dimensional and complex  as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models  where classifiers are trained to discriminate each visited state against all others. Intuitively  novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation  and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks  including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.,EX2: Exploration with Exemplar Models for Deep

Reinforcement Learning

Justin Fu∗

John D. Co-Reyes∗

Sergey Levine

University of California Berkeley

{justinfu jcoreyes svlevine}@eecs.berkeley.edu

Abstract

Deep reinforcement learning algorithms have been shown to learn complex tasks
using highly general policy classes. However  sparse reward problems remain a
signiﬁcant challenge. Exploration methods based on novelty detection have been
particularly successful in such settings but typically require generative or predictive
models of the observations  which can be difﬁcult to train when the observations
are very high-dimensional and complex  as in the case of raw images. We propose a
novelty detection algorithm for exploration that is based entirely on discriminatively
trained exemplar models  where classiﬁers are trained to discriminate each visited
state against all others. Intuitively  novel states are easier to distinguish against
other states seen during training. We show that this kind of discriminative modeling
corresponds to implicit density estimation  and that it can be combined with count-
based exploration to produce competitive results on a range of popular benchmark
tasks  including state-of-the-art results on challenging egocentric observations in
the vizDoom benchmark.

1

Introduction

Recent work has shown that methods that combine reinforcement learning with rich function ap-
proximators  such as deep neural networks  can solve a range of complex tasks  from playing Atari
games (Mnih et al.  2015) to controlling simulated robots (Schulman et al.  2015). Although deep
reinforcement learning methods allow for complex policy representations  they do not by themselves
solve the exploration problem: when the reward signals are rare and sparse  such methods can struggle
to acquire meaningful policies. Standard exploration strategies  such as -greedy strategies (Mnih
et al.  2015) or Gaussian noise (Lillicrap et al.  2015)  are undirected and do not explicitly seek out
interesting states. A promising avenue for more directed exploration is to explicitly estimate the
novelty of a state  using predictive models that generate future states (Schmidhuber  1990; Stadie
et al.  2015; Achiam & Sastry  2017) or model state densities (Bellemare et al.  2016; Tang et al.  2017;
Abel et al.  2016). Related concepts such as count-based bonuses have been shown to provide sub-
stantial speedups in classic reinforcement learning (Strehl & Littman  2009; Kolter & Ng  2009)  and
several recent works have proposed information-theoretic or probabilistic approaches to exploration
based on this idea (Houthooft et al.  2016; Chentanez et al.  2005) by drawing on formal results in
simpler discrete or linear systems (Bubeck & Cesa-Bianchi  2012). However  most novelty estimation
methods rely on building generative or predictive models that explicitly model the distribution over
the current or next observation. When the observations are complex and high-dimensional  such as in
the case of raw images  these models can be difﬁcult to train  since generating and predicting images
and other high-dimensional objects is still an open problem  despite recent progress (Salimans et al. 
2016). Though successful results with generative novelty models have been reported with simple
synthetic images  such as in Atari games (Bellemare et al.  2016; Tang et al.  2017)  we show in our

∗equal contribution.

experiments that such generative methods struggle with more complex and naturalistic observations 
such as the ego-centric image observations in the vizDoom benchmark.
How can we estimate the novelty of visited states  and thereby provide an intrinsic motivation signal
for reinforcement learning  without explicitly building generative or predictive models of the state or
observation? The key idea in our EX2 algorithm is to estimate novelty by considering how easy it is
for a discriminatively trained classiﬁer to distinguish a given state from other states seen previously.
The intuition is that  if a state is easy to distinguish from other states  it is likely to be novel. To
this end  we propose to train exemplar models for each state that distinguish that state from all other
observed states. We present two key technical contributions that make this into a practical exploration
method. First  we describe how discriminatively trained exemplar models can be used for implicit
density estimation  allowing us to unify this intuition with the theoretically rigorous framework of
count-based exploration. Our experiments illustrate that  in simple domains  the implicitly estimated
densities provide good estimates of the underlying state densities without any explicit generative
training. Second  we show how to amortize the training of exemplar models to prevent the total
number of classiﬁers from growing with the number of states  making the approach practical and
scalable. Since our method does not require any explicit generative modeling  we can use it on a
range of complex image-based tasks  including Atari games and the vizDoom benchmark  which
has complex 3D visuals and extensive camera motion due to the egocentric viewpoint. Our results
show that EX2 matches the performance of generative novelty-based exploration methods on simpler
tasks  such as continuous control benchmarks and Atari  and greatly exceeds their performance on the
complex vizDoom domain  indicating the value of implicit density estimation over explicit generative
modeling for intrinsic motivation.

2 Related Work

In ﬁnite MDPs  exploration algorithms such as E3 (Kearns & Singh  2002) and R-max (Brafman &
Tennenholtz  2002) offer theoretical optimality guarantees. However  these methods typically require
maintaining state-action visitation counts  which can make extending them to high dimensional and/or
continuous states very challenging. Exploring in such state spaces has typically involved strategies
such as introducing distance metrics over the state space (Pazis & Parr  2013; Kakade et al.  2003) 
and approximating the quantities used in classical exploration methods. Prior works have employed
approximations for the state-visitation count (Tang et al.  2017; Bellemare et al.  2016; Abel et al. 
2016)  information gain  or prediction error based on a learned dynamics model (Houthooft et al. 
2016; Stadie et al.  2015; Achiam & Sastry  2017). Bellemare et al. (2016) show that count-based
methods in some sense bound the bonuses produced by exploration incentives based on intrinsic
motivation  such as model uncertainty or information gain  making count-based or density-based
bonuses an appealing and simple option.
Other methods avoid tackling the exploration problem directly and use randomness over model
parameters to encourage novel behavior (Chapelle & Li  2011). For example  bootstrapped DQN
(Osband et al.  2016) avoids the need to construct a generative model of the state by instead training
multiple  randomized value functions and performs exploration by sampling a value function  and
executing the greedy policy with respect to the value function. While such methods scale to complex
state spaces as well as standard deep RL algorithms  they do not provide explicit novelty-seeking
behavior  but rather a more structured random exploration behavior.
Another direction explored in prior work is to examine exploration in the context of hierarchical
models. An agent that can take temporally extended actions represented as action primitives or skills
can more easily explore the environment (Stolle & Precup  2002). Hierarchical reinforcement learning
has traditionally tried to exploit temporal abstraction (Barto & Mahadevan  2003) and relied on semi-
Markov decision processes. A few recent works in deep RL have used hierarchies to explore in sparse
reward environments (Florensa et al.  2017; Heess et al.  2016). However  learning a hierarchy is
difﬁcult and has generally required curriculum learning or manually designed subgoals (Kulkarni
et al.  2016). In this work  we discuss a general exploration strategy that is independent of the design
of the policy and applicable to any architecture  though our experiments focus speciﬁcally on deep
reinforcement learning scenarios  including image-based navigation  where the state representation is
not conducive to simple count-based metrics or generative models.

2

Concurrently with this work  Pathak et al. (2017) proposed to use discriminatively trained exploration
bonuses by learning state features which are trained to predict the action from state transition pairs.
Then given a state and action  their model predicts the features of the next state and the bonus is
calculated from the prediction error. In contrast to our method  this concurrent work does not attempt
to provide a probabilistic model of novelty and does not perform any sort of implicit density estimation.
Since their method learns an inverse dynamics model  it does not provide for any mechanism to
handle novel events that do not correlate with the agent’s actions  though it does succeed in avoiding
the need for generative modeling.

rewards  π∗ = arg maxπ Eτ∼π[(cid:80)T
and π(τ ) = ρ0(s0)(cid:81)T

3 Preliminaries
In this paper  we consider a Markov decision process (MDP)  deﬁned by the tuple (S A T   R  γ  ρ0).
S A are the state and action spaces  respectively. The transition distribution T (s(cid:48)|a  s)  initial
state distribution ρ0(s)  and reward function R(s  a) are unknown in the reinforcement learning
(RL) setting and can only be queried through interaction with the MDP. The goal of reinforce-
ment learning is to ﬁnd the optimal policy π∗ that maximizes the expected sum of discounted
t=0 γtR(st  at)]   where  τ denotes a trajectory (s0  a0  ...sT   aT )
t=0 π(at|st)T (st+1|st  at). Our experiments evaluate episodic tasks with a
policy gradient RL algorithm  though extensions to inﬁnite horizon settings or other algorithms  such
as Q-learning and actor-critic  are straightforward.
Count-based exploration algorithms maintain a state-action visitation count N (s  a)  and encourage
the agent to visit rarely seen states  operating on the principle of optimism under uncertainty. This is
typically achieved by adding a reward bonus for visiting rare states. For example  MBIE-EB (Strehl
uses a β/(N (s  a) + |S|). In the ﬁnite state and action spaces  these methods are PAC-MDP (for
MBIE-EB) or PAC-BAMDP (for BEB)  roughly meaning that the agent acts suboptimally for only a
polynomial number of steps. In domains where explicit counting is impractical  pseudo-counts can
be used based on a density estimate p(s  a)  which typically is done using some sort of generatively
trained density estimation model (Bellemare et al.  2016). We will describe how we can estimate
densities using only discriminatively trained classiﬁers  followed by a discussion of how this implicit
estimator can be incorporated into a pseudo-count novelty bonus method.

& Littman  2009) uses a bonus of β/(cid:112)N (s  a)  where β is a constant  and BEB (Kolter & Ng  2009)

4 Exemplar Models and Density Estimation

We begin by describing our discriminative model used to predict novelty of states visited during
training. We highlight a connection between this particular form of discriminative model and density
estimation  and in Section 5 describe how to use this model to generate reward bonuses.

4.1 Exemplar Models

To avoid the need for explicit generative models  our novelty estimation method uses exemplar
models. Given a dataset X = {x1  ...xn}  an exemplar model consists of a set of n classiﬁers or
discriminators {Dx1   ....Dxn}  one for each data point. Each individual discriminator Dxi is trained
to distinguish a single positive data point xi  the “exemplar ” from the other points in the dataset
X. We borrow the term “exemplar model” from Malisiewicz et al. (2011)  which coined the term
“exemplar SVM” to refer to a particular linear model trained to classify each instance against all others.
However  to our knowledge  our work is the ﬁrst to apply this idea to exploration for reinforcement
learning. In practice  we avoid the need to train n distinct classiﬁers by amortizing through a single
exemplar-conditioned network  as discussed in Section 6.
Let PX (x) denote the data distribution over X   and let Dx∗ (x) : X → [0  1] denote the discriminator
associated with exemplar x∗. In order to obtain correct density estimates  as discussed in the next
section  we present each discriminator with a balanced dataset  where half of the data consists of the
exemplar x∗ and half comes from the background distribution PX (x). Each discriminator is then
trained to model a Bernoulli distribution Dx∗ (x) = P (x = x∗|x) via maximum likelihood. Note
that the label x = x∗ is noisy because data that is extremely similar or identical to x∗ may also
occur in the background distribution PX (x)  so the classiﬁer does not always output 1. To obtain the

3

maximum likelihood solution  the discriminator is trained to optimize the following cross-entropy
objective

(Eδx∗ [log D(x)] + EPX [log 1 − D(x)]) .

(1)

Dx∗ = arg max

D∈D

We discuss practical amortized methods that avoid the need to train n discriminators in Section 6  but
to keep the derivation in this section simple  we consider independent discriminators for now.

4.2 Exemplar Models as Implicit Density Estimation

To show how the exemplar model can be used for implicit density estimation  we begin by considering
an inﬁnitely powerful  optimal discriminator  for which we can make an explicit connection between
the discriminator and the underlying data distribution PX (x):
Proposition 1. (Optimal Discriminator) For a discrete distribution PX (x)  the optimal discriminator
Dx∗ for exemplar x∗ satisﬁes

Dx∗ (x) =

δx∗ (x)

δx∗ (x) + PX (x)

and

Dx∗ (x∗) =

1

1 + PX (x∗)

.

Proof. The proof is obtained by taking the derivative of the loss in Eq. (1) with respect to D(x) 
setting it to zero  and solving for D(x).
It follows that  if the discriminator is optimal  we can recover the probability of a data point PX (x∗)
by evaluating the discriminator at its own exemplar x∗  according to

PX (x∗) =

1 − Dx∗ (x∗)
Dx∗ (x∗)

.

(2)

For continuous domains  δx∗ (x∗) → ∞  so D(x) → 1. This means we are unable to recover
PX (x) via Eq. (2). However  we can smooth the delta by adding noise  ∼ q() to the exemplar
x∗ during training  which allows us to recover exact density estimates by solving for PX (x). For
example  if we let q = N (0  σ2I)  then the optimal discriminator evaluated at x∗ satisﬁes Dx∗ (x∗) =
√
1/

. Even if we do not know the noise variance  we have

√
1/

d(cid:105)

2πσ2

2πσ2

/

d

+ PX (x)

(cid:104)

(cid:104)

(cid:105)
PX (x∗) ∝ 1 − Dx∗ (x∗)
Dx∗ (x∗)

(3)
This proportionality holds for any noise q as long as (δx∗ ∗ q)(x∗) (where ∗ denotes convolution) is
the same for every x∗. The reward bonus we describe in Section 5 is invariant to the normalization
factor  so proportional estimates are sufﬁcient.
In practice  we can get density estimates that are better suited for exploration by introducing smooth-
ing  which involves adding noise to the background distribution PX   to produce the estimator

.

(δx∗ ∗ q)(x)

Dx∗ (x) =

(δx∗ ∗ q)(x) + (PX ∗ q)(x∗)

.

We then recover our density estimate as (PX ∗ q)(x∗). In the case when PX is a collection of delta
functions around data points  this is equivalent to kernel density estimation using the noise distribution
as a kernel. With Gaussian noise q = N (0  σ2I)  this is equivalent to using an RBF kernel.

4.3 Latent Space Smoothing with Noisy Discriminators

In the previous section  we discussed how adding noise can provide for smoothed density estimates 
which is especially important in complex or continuous spaces  where all states might be distin-
guishable with a powerful enough discriminator. Unfortunately  for high-dimensional states  such as
images  adding noise directly to the state often does not produce meaningful new states  since the
distribution of states lies on a thin manifold  and any added noise will lift the noisy state off of this
manifold. In this section  we discuss how we can learn a smoothing distribution by injecting the noise
into a learned latent space  rather than adding it to the original states.

4

Formally  we introduce a latent variable z. We wish to train an encoder distribution q(z|x)  and a
latent space classiﬁer p(y|z) = D(z)y(1 − D(z))1−y  where y = 1 when x = x∗ and y = 0 when
x (cid:54)= x∗. We additionally regularize the noise distribution against a prior distribution p(z)  which
2 pX (x) denote the balanced training

in our case is a unit Gaussian. Letting (cid:101)p(x) = 1

distribution from before  we can learn the latent space by maximizing the objective

2 δx∗ (x) + 1

(4)

E(cid:101)p[Eqz|x [log p(y|z)] − DKL(q(z|x)||p(z))] .

max

py|z qz|x

Intuitively  this objective optimizes the noise distribution so as to maximize classiﬁcation accuracy
while transmitting as little information through the latent space as possible. This causes z to only
capture the factors of variation in x that are most informative for distinguish points from the exemplar 
resulting in noise that stays on the state manifold. For example  in the Atari domain  latent space
noise might correspond to smoothing over the location of the player and moving objects on the screen 
in contrast to performing pixel-wise Gaussian smoothing.
x pX (x)q(z|x)dx denote the marginal-
ized positive and negative densities over the latent space  we can characterize the optimal discriminator
and encoder distributions as follows. For any encoder q(z|x)  the optimal discriminator D(z) satisﬁes:

x δx∗ (x)q(z|x)dx and q(z|y = 0) =(cid:82)

Letting q(z|y = 1) =(cid:82)

p(y = 1|z) = D(z) =

q(z|y = 1) + q(z|y = 0)
and for any discriminator D(z)  the optimal encoder distribution satisﬁes:
q(z|x) ∝ D(z)ysoft(x)(1 − D(z))1−ysoft(x)p(z)  

 

q(z|y = 1)

δx∗ (x)

where ysoft(x) = p(y = 1|x) =
δx∗ (x)+pX (x) is the average label of x. These can be obtained by
differentiating the objective  and the full derivation is included in Appendix A.1. Intuitively  q(z|x)
is equal to the prior p(z) by default  which carries no information about x. It then scales up the
probability on latent codes z where the discriminator is conﬁdent and correct. To recover a density
estimate  we estimate D(x) = Eq[D(z)] and apply Eq. (3) to obtain the density.

4.4 Smoothing from Suboptimal Discriminators

In our previous derivations  we assume an optimal  inﬁnitely powerful discriminator which can
emit a different value D(x) for every input x. However  this is typically not possible except for
small  countable domains. A secondary but important source of density smoothing occurs when the
discriminator has difﬁculty distinguishing two states x and x(cid:48). In this case  the discriminator will
average over the outputs of the inﬁnitely powerful discriminator. This form of smoothing comes from
the inductive bias of the discriminator  which is difﬁcult to quantify. In practice  we typically found
this effect to be beneﬁcial for our model rather than harmful. An example of such smoothed density
estimates is shown in Figure 2. Due to this effect  adding noise is not strictly necessary to beneﬁt
from smoothing  though it provides for signiﬁcantly better control over the degree of smoothing.

5 EX2: Exploration with Exemplar Models

We can now describe our exploration algorithm based on implicit density models. Pseudocode for a
batch policy search variant using the single exemplar model is shown in Algorithm 1. Online variants
for other RL algorithms  such as Q-learning  are also possible. In order to apply the ideas from
count-based exploration described in Section 3  we must approximate the state visitation counts
N (s) = nP (s)  where P (s) is the distribution over states visited during training. Note that we can
easily use state-action counts N (s  a)  but we omit the action for simplicity of notation. To generate
approximate samples from P (s)  we use a replay buffer B  which is a ﬁrst-in ﬁrst-out (FIFO) queue
that holds previously visited states. Our exemplars are the states we wish to score  which are the states
in the current batch of trajectories. In an online algorithm  we would instead train a discriminator
after receiving every new observation one at a time  and compute the bonus in the same manner.
Given the output from discriminators trained to optimize Eq (1)  we augment the reward with a
function of the “novelty” of the state (where β is a hyperparameter that can be tuned to the magnitude
of the task reward): R(cid:48)(s  a) = R(s  a) + βf (Ds(s)).

5

Sample trajectories {τj} from policy πi
for state s in {τ} do

Algorithm 1 EX2 for batch policy optimization
1: Initialize replay buffer B
2: for iteration i in {1  . . .   N} do
3:
4:
5:
6:
7:
8:
9:
10: B ← B ∪ {τi}
11: end for

Sample a batch of negatives {s(cid:48)
Train discriminator Ds to minimize Eq. (1) with positive s  and negatives {s(cid:48)
Compute reward R(cid:48)(s  a) = R(s  a) + βf (Ds(s))

k} from B.

end for
Improve πi with respect to R(cid:48)(s  a) using any policy optimization method.

k}.

In our experiments  we use the heuristic bonus − log p(s)  due to the fact that normalization constants
become absorbed by baselines used in typical RL algorithms. For discrete domains  we can also use a

count-based 1/(cid:112)N (s) (Tang et al.  2017)  where N (s) = nP (s)  and n being the size of the replay

buffer B. A summary of EX2 for a generic batch reinforcement learner is shown in Algorithm 1.

6 Model Architecture

To process complex observations such as images  we implement our exemplar model using neural
networks  with convolutional models used for image-based domains. To reduce the computational
cost of training such large per-exemplar classiﬁers  we explore two methods for amortizing the
computation across multiple exemplars.

6.1 Amortized Multi-Exemplar Model

Instead of training a separate classiﬁer for each exemplar  we can instead train a single model that is
conditioned on the exemplar x∗. When using the latent space formulation  we condition the latent
space discriminator p(y|z) on an encoded version of x∗ given by q(z∗|x∗)  resulting in a classiﬁer
for the form p(y|z  z∗) = D(z  z∗)y(1 − D(z  z∗))1−y. The advantage of this amortized model is
that it does not require us to train new discriminators from scratch at each iteration  and provides
some degree of generalization for density estimation at new states. A diagram of this architecture is
shown in Figure 1. The amortized architecture has the appearance of a comparison operator: it is
trained to output 0 when x∗ (cid:54)= x  and the optimal discriminator values covered in Section 4 when
x∗ = x  subject to the smoothing imposed by the latent space noise.

6.2 K-Exemplar Model

As long as the distribution of positive examples is known  we can recover density estimates via Eq. (3).
Thus  we can also consider a batch of exemplars x1  ...  xK  and sample from this batch uniformly
during training. We refer to this model as the "K-Exemplar" model  which allows us to interpolate
smoothly between a more powerful model with one discriminator per state (K = 1) with a weaker
model that uses a single discriminator for all states (K = # states). A more detailed discussion of
this method is included in Appendix A.2. In our experiments  we batch adjacent states in a trajectory
into the same discriminator which corresponds to a form of temporal regularization that assumes that
adjacent states in time are similar. We also share the majority of layers between discriminators in the
neural networks similar to (Osband et al.  2016)  and only allow the ﬁnal linear layer to vary amongst
discriminators  which forces the shared layers to learn a joint feature representation  similarly to the
amortized model. An example architecture is shown in Figure 1.

6.3 Relationship to Generative Adverserial Networks (GANs)

Our exploration algorithm has an interesting interpretation related to GANs (Goodfellow et al. 
2014). The policy can be viewed as the generator of a GAN  and the exemplar model serves as the
discriminator  which is trying to classify states from the current batch of trajectories against previous

6

a) Amortized Architecture
Figure 1: A diagram of our a) amortized model architecture and b) the K-exemplar model architecture.
Noise is injected after the encoder module (a) or after the shared layers (b). Although possible  we do
not tie the encoders of (a) in our experiments.

b) K-Exemplar Architecture

states. Using the K-exemplar version of our algorithm  we can train a single discriminator for all
states in the current batch (rather than one for each state)  which mirrors the GAN setup.
In GANs  the generator plays an adverserial game with the discriminator by attempting to produce
indistinguishable samples in order to fool the discriminator. However  in our algorithm  the generator
is rewarded for helping the discriminator rather than fooling it  so our algorithm plays a cooperative
game instead of an adverserial one. Instead  they are competing with the progression of time: as a
novel state becomes visited frequently  the replay buffer will become saturated with that state and it
will lose its novelty. This property is desirable in that it forces the policy to continually seek new
states from which to receive exploration bonuses.

7 Experimental Evaluation

The goal of our experimental evaluation is to compare the EX2 method to both a naïve exploration
strategy and to recently proposed exploration schemes for deep reinforcement learning based on
explicit density modeling. We present results on both low-dimensional benchmark tasks used in
prior work  and on more complex vision-based tasks  where prior density-based exploration bonus
methods are difﬁcult to apply. We use TRPO (Schulman et al.  2015) for policy optimization  because
it operates on both continuous and discrete action spaces  and due to its relative robustness to hyper-
parameter choices (Duan et al.  2016). Our code and additional supplementary material including
videos will be available at https://sites.google.com/view/ex2exploration.

Experimental Tasks Our experiments include three low-dimensional tasks intended to assess
whether EX2 can successfully perform implicit density estimation and computer exploration bonuses 
and four high-dimensional image-based tasks of varying difﬁculty intended to evaluate whether
implicit density estimation provides improvement in domains where generative modeling is difﬁcult.
The ﬁrst low-dimensional task is a continuous 2D maze with a sparse reward function that only
provides a reward when the agent is within a small radius of the goal. Because this task is 2D  we can
use it to directly visualize the state visitation densities and compare to an upper bound histogram
method for density estimation. The other two low-dimensional tasks are benchmark tasks from
the OpenAI gym benchmark suite  SparseHalfCheetah and SwimmerGather  which provide for a
comparison against prior work on generative exploration bonuses in the presence of sparse rewards.
For the vision-based tasks  we include three Atari games  as well as a much more difﬁcult ego-centric
navigation task based on vizDoom (DoomMyWayHome+). The Atari games are included for easy
comparison with prior methods based on generative models  but do not provide especially challenging
visual observations  since the clean 2D visuals and relatively low visual diversity of these tasks makes
generative modeling easy. In fact  prior work on video prediction for Atari games easily achieves
accurate predictions hundreds of frames into the future (Oh et al.  2015)  while video prediction
on natural images is challenging even a couple of frames into the future (Mathieu et al.  2015).
The vizDoom maze navigation task is intended to provide a comparison against prior methods with
substantially more challenging observations: the game features a ﬁrst-person viewpoint  3D visuals 
and partial observability  as well as the usual challenges associated with sparse rewards. We make
the task particularly difﬁcult by initializing the agent in the furthest room from the goal location 

7

b) Empirical

a) Exemplar
c) Varying Smoothing
Figure 2: a  b) Illustration of estimated densities on the 2D
maze task produced by our model (a)  compared to the empiri-
cal discretized distribution (b). Our method provides reasonable 
somewhat smoothed density estimates. c) Density estimates pro-
duced with our implicit density estimator on a toy dataset (top
left)  with increasing amounts of noise regularization.

Figure 3: Example task images.
From top to bottom  left to right:
Doom  map of the MyWayHome
task (goal is green  start is blue) 
Venture  HalfCheetah.

requiring it to navigate through 8 rooms before reaching the goal. Sample images taken from several
of these tasks are shown in Figure 3 and detailed task descriptions are given in Appendix A.3.
We compare the two variants of our method (K-exemplar and amortized) to standard random ex-
ploration  kernel density estimation (KDE) with RBF kernels  a method based on Bayesian neural
network generative models called VIME (Houthooft et al.  2016)  and exploration bonuses based on
hashing of latent spaces learned via an autoencoder (Tang et al.  2017).

2D Maze On the 2D maze task  we can visually compare the estimated state density from our
exemplar model and the empirical state-visitation distribution sampled from the replay buffer  as
shown in Figure 2. Our model generates sensible density estimates that smooth out the true empirical
distribution. For exploration performance  shown in Table 1 TRPO with Gaussian exploration cannot
ﬁnd the sparse reward goal  while both variants of our method perform similarly to VIME and KDE.
Since the dimensionality of the task is low  we also use a histogram-based method to estimate the
density  which provides an upper bound on the performance of count-based exploration on this task.

Continuous Control: SwimmerGather and SparseHalfCheetah SwimmerGather and Sparse-
HalfCheetah are two challenging continuous control tasks proposed by Houthooft et al. (2016). Both
environments feature sparse reward and medium-dimensional observations (33 and 20 dimensions
respectively). SwimmerGather is a hierarchical task in which no previous algorithms using naïve
exploration have made any progress. Our results demonstrate that  even on medium-dimensional
tasks where explicit generative models should perform well  our implicit density estimation approach
achieves competitive results. EX2  VIME  and Hashing signiﬁcantly outperform the naïve TRPO
algorithm and KDE on SwimmerGather  and amortized EX2outperforms all other methods on Sparse-
HalfCheetah by a signiﬁcant margin. This indicates that the implicit density estimates obtained by
our method provide for exploration bonuses that are competitive with a variety of explicit density
estimation techniques.

Image-Based Control: Atari and Doom In our ﬁnal set of experiments  we test the ability of
our algorithm to scale to rich sensory inputs and high dimensional image-based state spaces. We
chose several Atari games that have sparse rewards and present an exploration challenge  as well as a
maze navigation benchmark based on vizDoom. Each domain presents a unique set of challenges.
The vizDoom domain contains the most realistic images  and the environment is viewed from an
egocentric perspective which makes building dynamics models difﬁcult and increases the importance
of intelligent smoothing and generalization. The Atari games (Freeway  Frostbite  Venture) contain
simpler images from a third-person viewpoint  but often contain many moving  distractor objects
that a density model must generalize to. Freeway and Venture contain sparse reward  and Frostbite
contains a small amount of dense reward but attaining higher scores typically requires exploration.
Our results demonstrate that EX2 is able to generate coherent exploration behavior even high-
dimensional visual environments  matching the best-performing prior methods on the Atari games.
On the most challenging task  DoomMyWayHome+  our method greatly exceeds all of the prior

8

Task
2D Maze
SparseHalfCheetah
SwimmerGather
Freeway (Atari)
Frostbite (Atari)
Venture (Atari)
DoomMyWayHome
1 Houthooft et al. (2016)

-104.2
3.56
0.228

-
-
-

-132.2
173.2
0.240
33.3
4901
900
0.788

0.740
2 Schulman et al. (2015)

0.443
3 Tang et al. (2017)

K-Ex.(ours) Amor.(ours) VIME1 TRPO2 Hashing3 KDE
-117.5

-175.6

-135.5
98.0
0.196

-
-
-

0
0

16.5
2869
121
0.250

-
0.5
0.258
33.5
5214
445
0.331

0.098

0

-
-
-

0.195

Histogram

-69.6

-
-
-
-
-
-

Table 1: Mean scores (higher is better) of our algorithm (both K-exemplar and amortized) versus
VIME (Houthooft et al.  2016)  baseline TRPO  Hashing  and kernel density estimation (KDE). Our
approach generally matches the performance of previous explicit density estimation methods  and
greatly exceeds their performance on the challenging DoomMyWayHome+ task  which features
camera motion  partial observability  and extremely sparse rewards. We did not run VIME or K-
Exemplar on Atari games due to computational cost. Atari games are trained for 50 M time steps.
Learning curves are included in Appendix A.5

exploration techniques  and is able to guide the agent through multiple rooms to the goal. This result
indicates the beneﬁt of implicit density estimation: while explicit density estimators can achieve good
results on simple  clean images in the Atari games  they begin to struggle with the more complex
egocentric observations in vizDoom  while our EX2 is able to provide reasonable density estimates
and achieves good results.

8 Conclusion and Future Work

We presented EX2  a scalable exploration strategy based on training discriminative exemplar models
to assign novelty bonuses. We also demonstrate a novel connection between exemplar models and
density estimation  which motivates our algorithm as approximating pseudo-count exploration. This
density estimation technique also does not require reconstructing samples to train  unlike most
methods for training generative or energy-based models. Our empirical results show that EX2 tends
to achieve comparable results to the previous state-of-the-art for continuous control tasks on low-
dimensional environments  and can scale gracefully to handle rich sensory inputs such as images.
Since our method avoids the need for generative modeling of complex image-based observations  it
exceeds the performance of prior generative methods on domains with more complex observation
functions  such as the egocentric Doom navigation task.
To understand the tradeoffs between discriminatively trained exemplar models and generative mod-
eling  it helps to consider the behavior of the two methods when overﬁtting or underﬁtting. Both
methods will assign ﬂat bonuses when underﬁtting and high bonuses to all new states when overﬁtting.
However  in the case of exemplar models  overﬁtting is easy with high dimensional observations 
especially in the amortized model where the network simply acts as a comparator. Underﬁtting is
also easy to achieve  simply by increasing the magnitude of the noise injected into the latent space.
Therefore  although both approach can suffer from overﬁtting and underﬁtting  the exemplar method
provides a single hyperparameter that interpolates between these extremes without changing the
model. An exciting avenue for future work would be to adjust this smoothing factor automatically 
based on the amount of available data. More generally  implicit density estimation with exemplar
models is likely to be of use in other density estimation applications  and exploring such applications
would another exciting direction for future work.

Acknowledgement We would like to thank Adam Stooke  Sandy Huang  and Haoran Tang for
providing efﬁcient and parallelizable policy search code. We thank Joshua Achiam for help with
setting up benchmark tasks. This research was supported by NSF IIS-1614653  NSF IIS-1700696  an
ONR Young Investigator Program award  and Berkeley DeepDrive.

9

References
Abel  David  Agarwal  Alekh  Diaz  Fernando  Krishnamurthy  Akshay  and Schapire  Robert E.
Exploratory gradient boosting for reinforcement learning in complex domains. In Advances in
Neural Information Processing Systems (NIPS)  2016.

Achiam  Joshua and Sastry  Shankar. Surprise-based intrinsic motivation for deep reinforcement

learning. CoRR  abs/1703.01732  2017.

Barto  Andrew G. and Mahadevan  Sridhar. Recent advances in hierarchical reinforcement learning.

Discrete Event Dynamic Systems  13(1-2)  2003.

Bellemare  Marc G.  Srinivasan  Sriram  Ostrovski  Georg  Schaul  Tom  Saxton  David  and Munos 
Remi. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Informa-
tion Processing Systems (NIPS)  2016.

Brafman  Ronen I. and Tennenholtz  Moshe. R-max – a general polynomial time algorithm for

near-optimal reinforcement learning. Journal of Machine Learning Research (JMLR)  2002.

Bubeck  Sébastien and Cesa-Bianchi  Nicolò. Regret analysis of stochastic and nonstochastic

multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5  2012.

Chapelle  O. and Li  Lihong. An empirical evaluation of thompson sampling. In Advances in Neural

Information Processing Systems (NIPS)  2011.

Chentanez  Nuttapong  Barto  Andrew G  and Singh  Satinder P.

Intrinsically Motivated Rein-
forcement Learning. In Advances in Neural Information Processing Systems (NIPS). MIT Press 
2005.

Duan  Yan  Chen  Xi  Houthooft  Rein  Schulman  John  and Abbeel  Pieter. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning
(ICML)  2016.

Florensa  Carlos Campo  Duan  Yan  and Abbeel  Pieter. Stochastic neural networks for hierarchical
reinforcement learning. In International Conference on Learning Representations (ICLR)  2017.

Goodfellow  Ian  Pouget-Abadie  Jean  Mirza  Mehdi  Xu  Bing  Warde-Farley  David  Ozair  Sherjil 
In Advances in Neural

Courville  Aaron  and Bengio  Yoshua. Generative adversarial nets.
Information Processing Systems (NIPS). 2014.

Heess  Nicolas  Wayne  Gregory  Tassa  Yuval  Lillicrap  Timothy P.  Riedmiller  Martin A.  and
Silver  David. Learning and transfer of modulated locomotor controllers. CoRR  abs/1610.05182 
2016.

Houthooft  Rein  Chen  Xi  Duan  Yan  Schulman  John  Turck  Filip De  and Abbeel  Pieter. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems (NIPS)  2016.

Kakade  Sham  Kearns  Michael  and Langford  John. Exploration in metric state spaces.

International Conference on Machine Learning (ICML)  2003.

In

Kearns  Michael and Singh  Satinder. Near-optimal reinforcement learning in polynomial time.

Machine Learning  2002.

Kolter  J. Zico and Ng  Andrew Y. Near-bayesian exploration in polynomial time. In International

Conference on Machine Learning (ICML)  2009.

Kulkarni  Tejas D  Narasimhan  Karthik  Saeedi  Ardavan  and Tenenbaum  Josh. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in
Neural Information Processing Systems (NIPS). 2016.

Lillicrap  Timothy P.  Hunt  Jonathan J.  Pritzel  Alexander  Heess  Nicolas  Erez  Tom  Tassa  Yuval 
Silver  David  and Wierstra  Daan. Continuous control with deep reinforcement learning. In
International Conference on Learning Representations (ICLR)  2015.

10

Malisiewicz  Tomasz  Gupta  Abhinav  and Efros  Alexei A. Ensemble of exemplar-svms for object

detection and beyond. In International Conference on Computer Vision (ICCV)  2011.

Mathieu  Michaël  Couprie  Camille  and LeCun  Yann. Deep multi-scale video prediction beyond
mean square error. CoRR  abs/1511.05440  2015. URL http://arxiv.org/abs/1511.05440.

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David  Rusu  Andrei A.  Veness  Joel  Bellemare 
Marc G.  Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas K.  Ostrovski  Georg  Petersen  Stig 
Beattie  Charles  Sadik  Amir  Antonoglou  Ioannis  King  Helen  Kumaran  Dharshan  Wierstra 
Daan  Legg  Shane  and Hassabis  Demis. Human-level control through deep reinforcement
learning. Nature  518(7540):529–533  02 2015.

Oh  Junhyuk  Guo  Xiaoxiao  Lee  Honglak  Lewis  Richard  and Singh  Satinder. Action-conditional
video prediction using deep networks in atari games. In Advances in Neural Information Processing
Systems (NIPS)  2015.

Osband  Ian  Blundell  Charles  and Alexander Pritzel  Benjamin Van Roy. Deep exploration via

bootstrapped DQN. In Advances in Neural Information Processing Systems (NIPS)  2016.

Pathak  Deepak  Agrawal  Pulkit  Efros  Alexei A.  and Darrell  Trevor. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning (ICML)  2017.

Pazis  Jason and Parr  Ronald. Pac optimal exploration in continuous space markov decision processes.

In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2013.

Salimans  Tim  Goodfellow  Ian J.  Zaremba  Wojciech  Cheung  Vicki  Radford  Alec  and Chen  Xi.
Improved techniques for training gans. In Advances in Neural Information Processing Systems
(NIPS)  2016.

Schmidhuber  Jürgen. A possibility for implementing curiosity and boredom in model-building
neural controllers. In Proceedings of the First International Conference on Simulation of Adaptive
Behavior on From Animals to Animats  Cambridge  MA  USA  1990. MIT Press. ISBN 0-262-
63138-5.

Schulman  John  Levine  Sergey  Moritz  Philipp  Jordan  Michael I.  and Abbeel  Pieter. Trust region

policy optimization. In International Conference on Machine Learning (ICML)  2015.

Stadie  Bradly C.  Levine  Sergey  and Abbeel  Pieter. Incentivizing exploration in reinforcement

learning with deep predictive models. CoRR  abs/1507.00814  2015.

Stolle  Martin and Precup  Doina. Learning Options in Reinforcement Learning. Springer Berlin
Heidelberg  Berlin  Heidelberg  2002. ISBN 978-3-540-45622-3. doi: 10.1007/3-540-45622-8_16.

Strehl  Alexander L. and Littman  Michael L. An analysis of model-based interval estimation for

markov decision processes. Journal of Computer and System Sciences  2009.

Tang  Haoran  Houthooft  Rein  Foote  Davis  Stooke  Adam  Chen  Xi  Duan  Yan  Schulman  John 
Turck  Filip De  and Abbeel  Pieter. #exploration: A study of count-based exploration for deep
reinforcement learning. In Advances in Neural Information Processing Systems (NIPS)  2017.

11

,Matjaz Jogan
Alan Stocker
Justin Fu
John Co-Reyes
Sergey Levine