2019,A Tensorized Transformer for Language Modeling,Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular  Transformer  which is solely based on self-attention  has led to breakthroughs in Natural Language Processing (NLP) tasks. However  the multi-head attention mechanism  as a key component of Transformer  limits the effective deployment of the model to a resource-limited setting. In this paper  based on the ideas of tensor decomposition and parameters sharing  we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e.  PTB  WikiText-103 and One-billion) and a neural machine translation task (i.e.  WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements  compared with a number of language modeling approaches  such as Transformer  Transformer-XL  and Transformer with tensor train decomposition.,A Tensorized Transformer for Language Modeling

1College of Intelligence and Computing  Tianjin University  Tianjin  China

2Microsoft Research Asia  Beijing  China

3School of Computer Science and Technology  Beijing Institute of Technology  Beijing  China

Xindian Ma1  Peng Zhang1∗  Shuai Zhang1 

Nan Duan2  Yuexian Hou1  Dawei Song3  Ming Zhou2

{xindianma  pzhang  szhang96  yxhou}@tju.edu.cn

{nanduan  mingzhou}@microsoft.com

{dwsong}@bit.edu.cn

Abstract

Latest development of neural models has connected the encoder and decoder
through a self-attention mechanism. In particular  Transformer  which is solely
based on self-attention  has led to breakthroughs in Natural Language Processing
(NLP) tasks. However  the multi-head attention mechanism  as a key component
of Transformer  limits the effective deployment of the model to a resource-limited
setting. In this paper  based on the ideas of tensor decomposition and parameters
sharing  we propose a novel self-attention model (namely Multi-linear attention)
with Block-Term Tensor Decomposition (BTD). We test and verify the proposed at-
tention method on three language modeling tasks (i.e.  PTB  WikiText-103 and One-
billion) and a neural machine translation task (i.e.  WMT-2016 English-German).
Multi-linear attention can not only largely compress the model parameters but also
obtain performance improvements  compared with a number of language modeling
approaches  such as Transformer  Transformer-XL  and Transformer with tensor
train decomposition.

1

Introduction

In NLP  Neural language model pre-training has shown to be effective for improving many
tasks [12  26]. Transformer [35] is based solely on the attention mechanism  and dispensing with
recurrent and convolutional networks entirely. At present  this model has received extensive attentions
and plays an key role in many neural language models  such as BERT [12]  GPT [27] and Universal
Transformer [10]. However  in Transformer based model  a lot of model parameters may cause prob-
lems in training and deploying these parameters in a resource-limited setting. Thus  the compression
of large neural pre-training language models has been an essential problem in NLP research.
In literature  there are some compression methods [18  38  14] proposed. When the vocabulary is
large  the corresponding weight matrices can be enormous. Tensorized embedding (TE) [18] uses the
tensor-train [25] to compress the embedding layers in Transformer-XL [7]  but has not compressed
the attention layer. Recently  Block-Term Tensor Decomposition(BTD) [9] is used to compress
recurrent neural networks (RNNs) [38]. Ye et al. [38] propose a compact ﬂexible structure to deal
with the large number of model parameters instead by high dimensional inputs in training recurrent
neural networks (RNNs). This method greatly reduces the parameters of RNNs and improves their
training efﬁciency. Still  the model only considers the input layer compression by the idea of low-rank
approximation. On the other hand  some methods [14  2] aim to develop a speciﬁc structure on its

∗Corresponding Author: Peng Zhang

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

weight matrices and can reduce the parameters of the models. However  the new structure after
compressing can not be integrated into the model [35].
In Transformer  the multi-head attention is a key part and it is constructed by a large number
of parameters. Speciﬁcally  Ashish et.al [35] compute the attention function on a set of queries
simultaneously  packed together into a matrix Q  while the keys and values are also packed together
into matrices K and V   respectively. The attention function then adopts a no-linear function sof tmax
over two matrices Q and K. There are two challenges to ﬁnd a high-quality compression method to
compress the multi-head attention in Transformer.
First  the self-attention function in Transformer is a non-linear function  which makes it difﬁcult
to compress. In order to address this challenge  we ﬁrst prove that the output of the attention
function of the self-attention model [35] can be linearly represented by a group of orthonormal
base vectors. Then  by initializing a low rank core tensor  we use Tucker-decomposition [33  20] to
reconstruct a new attention representation  where Q  K and V can be considered as factor matrices.
In order to construct the multi-head mechanism and compress the model  we use the method of
Block-Term Tensor Decomposition (BTD)  which is a combination of CP decomposition [3] and
Tucker decomposition [33]. The difference is that three factor matrices Q  K and V are shared in
constructing each 3-order block tensor. This process can reduce many parameters.
The second challenge is that the attention model after compressing can not be directly integrated
into the encoder and decoder framework of Transformer [35  7]. In order to address this challenge 
there are three steps as follows. First  the average of each block tensor can be computed; Second 
multiple matrices can be given by tensor split. Third  the concatenation of these matrices can serve as
the input to the next layer network in Transformer. After that  it can be integrated into the encoder
and decoder framework of Transformer [35  7] and trained end-to-end. Moreover  we also prove
that the 3-order tensor can reconstruct the scaled dot-product attention in Transformer by a sum on a
particular dimension.
Our method combines two ideas which are the low-rank approximation and parameters sharing at
the same time. Therefore  it achieves the higher compression ratios. Although the self-attention (i.e. 
scaled dot-product attention) in Transformer can be reconstructed  we do not consider reconstructing
it and choose to split the 3-order tensor (the output of Multi-linear attention) which is helpful for
improving the accuracy in experiments.
Our major contributions of this paper are as follows:

1) It is proved that the output of scaled dot-product attention (considering as a function) can be

linearly represented by a group of orthonormal base vectors.

2) A novel self-attention method  namely Multi-linear attention  is provided  which combines

two compression ideas  parameters sharing and low-rank approximation  together.

3) Multi-linear attention builds the strong connection between three factor matrices (pack a
set of queries  keys and values  respectively )  enhancing the ability of capturing sufﬁcient
attention information. We also prove our model can reconstruct the scaled dot-product
attention in the original Transformer.

In order to validate the beneﬁts of our model  we test it on two NLP tasks  namely language modeling
and neural machine translation. In our experiments  the multi-head attention can be replaced by
the proposed model  namely multi-linear attention. We have observed that the standard Multi-head
attention can be compressed with higher compression ratios on One-Billion dataset. As a result  we
show that multi-linear attention not only considerably reduces the number of parameters  but also
achieve promising experiments results  especially in language modeling tasks.

2 Preliminaries

Multi-linear attention is carried out in this paper. The analysis of Multi-linear attention relies on
these concepts and results from the ﬁeld of tensor decomositon and multi-head attention. We cover
below in Section 2.1 basic background on Block-Term tensor decomposition [9]. Then  we describe
in Section 2.2 multi-head attention [35].

2

Figure 1: The representation of Block-Term tensor decomposition for a 3-order tensor. A ∈
Rd1×d2×d3 is a 3-order tensor  and can be approximated by P Tucker decomposition. P is the CP
rank  and R1  R2  R3 are the Tucker rank  respectively. In this paper  we assume that R=R1=R2=R3.

2.1 Tensor and Block-Term Tensor Decomposition
Tensor We use the Euler script letter A to denote a tensor which can be thought of as a multi-array.
Thereby a vector and a matrix are a 1-order tensor and 2-order tensor  respectively. The element in a
n-order tensor is denoted as Ad1 ... dn. In the geometric representation of a tensor  3-order tensor can
be represented by a cube. After that  there is a related concept named tensor slice that will be used
in this paper. Tensor and some other related concepts are showed in Supplementary Materials A.
Block-Term Tensor Decomposition (BTD) Block-Term tensor decomposition is a combination of
CP decomposition [3] and Tucker decomposition [33]. Given a n-order tensor A ∈ Rd1×...×dn. A
high-order tensor can be decomposed into P block terms by the method named BTD. •z is denoted as
the tenor-tensor product on the z-th order [19] and z ∈ {1  . . .   d}. Each term contains •z between a
i ∈ Rdk×Rk  where i ∈ [1  P ] and k ∈ [1  d].
core tensor Gi ∈ RR1×...×Rd and d factor matrices X (k)
The formulation of BTD decomposition is as follows:

P(cid:88)

A =

Gi•1X (1)

i •2X 2

i •3 . . .•dX (d)

i

(1)

where P is the CP rank  and d is the Core-order. In our work  the tensor is 3-order. Figure 1
demonstrates the example of how a 3-order tensor A can be decomposed into P block terms.

i=1

2.2 Multi-head Attention

In Transformer  the attention function is named as “Scaled Dot-Product Attention”. In practice 
Transformer [35] processes query  keys and values as matrices Q  K  and V respectively. The
attention function can be written as follows:

Attention(Q  K  V ) = sof tmax(

QK T√
d

)V

(2)

where d is the number of columns of Q and K. In these work [35  12  7]  they all use the multi-head
attention  as introduced in [35] 

M ultiHeadAttention(Q(cid:48)  K(cid:48)  V (cid:48)) = Concat(head1  . . .   headh)W O

where headi = Attention(Q(cid:48)W Q

i   K(cid:48)W K

i

  V (cid:48)W V
i )

(3)

where matrices W Q
dv and dk are equal to d. In this work [35]  multiple groups of parameters (W Q
used  which results in a large number of redundant parameters.

i ∈ Rdmodel×dv and W O ∈ Rhd×dmodel. In practice 
i ) are

i ∈ Rdmodel×dk  W V

i and W K

i   W K

i and W V

3 Tensorized Transformer

In this section  we ﬁrst build a Single-block attention in Figure 2 (left) based on the Tucker decompo-
sition  a low-rank decomposition method. In this process  we prove that the self-attention function in
Transformer can be represented by a linear function  i.e.  a linear combination representation of a set
of basic vectors.

3

≈++⋯𝒜𝒜𝑑𝑑1𝑑𝑑2𝑑𝑑3𝒳𝒳1(1)𝒳𝒳𝑃𝑃(1)𝒳𝒳1(3)𝒳𝒳1(2)𝒳𝒳𝑃𝑃(2)𝒳𝒳𝑃𝑃(3)𝒢𝒢1𝒢𝒢𝑃𝑃𝑑𝑑1𝑑𝑑2𝑑𝑑3𝑅𝑅1𝑅𝑅3𝑅𝑅2𝑑𝑑1𝑅𝑅1𝑑𝑑2𝑅𝑅2𝑅𝑅3𝑑𝑑3�1�2�3�1�2�3Figure 2: (left) Single-block attention using Tucker decomposition. (right) Multi-linear attention
based on Block-Term tensor decomposition.

In order to compress the multi-head mechanism  we propose a multi-linear attention constructed by a
Block-Term tensor decomposition. This attention uses the idea of parameters sharing  i.e.  sharing
factor matrices across multiple blocks  shown in Figure 2 (right). After that  the compression ratios
and relatively lower complexity have been analyzed.

3.1 Single-block Attention by Tucker Decomposition

Before building the Single-block attention  it is necessary to propose the theorem 3.1. The theorem is
closely related to attributes of Single-block attention function by Tucker decomposition [33].
Theorem 3.1. Let e1  . . .   en be basis vectors from the vector space S. Assume that these vectors
e1  . . .   en are linear independent and Q K V can be linearly represented by this set of basis vectors.
The output of the attention function in Eq. 2 can be represented by a linear combination of the set of
these basis vectors.
(4)
where M ∈ Rn×d is a coefﬁcient matrix  and d is a dimension of these matrices (i.e.  Q  K  and V ).

Attention(Q  K  V ) = (e1  . . .   en)M 

Proof. The proof can be found in Supplementary Materials B.

In Figure 2 (left)  it is a schematic diagram about the Single-block attention. First  we assume that the
query  key and value can be mapped into three factor matrices of which are composed of three groups
of orthogonal basis vectors. Three factor matrices are Q  K and V . After that  we can construct
a new attention (i.e.  Single-block attention) by initializing a 3-order diagonal tensor (trainable)
which is the G. In Figure 2 (left)  R is the rank about the tensor  N is the length of a sequence  and
d is the dimension of matrix. The function of Single-block attention can be computed based on
Tucker-decomposition as follows:

AttenT D(G; Q  K  V ) =G•1Q•2K•3V

I(cid:88)

J(cid:88)

M(cid:88)

=

i=1

j=1

m=1

GijmQi ◦ Kj ◦ Vm

(5)

where G is a core tensor. i  j and m are the indexes of the core tensor. ◦ is the outer product. •z is
the same deﬁnition in Eq. 1. Qi  Kj and Vk are column vectors from matrices Q  K and V   where
Q ∈ RN×d  K ∈ RN×d and V ∈ RN×d  and N is the length of a sequence. In practice  we set
I=J=M=R. The core tensor G can be deﬁned as follows 

Gijm =

i = j = m
otherwise

(6)

(cid:26) rand(0  1)

0

4

𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜⋯+++⋯ℎ𝑠𝑠𝑜𝑜𝑠𝑠𝑠𝑠𝑜𝑜𝑐𝑐𝑜𝑜𝑐𝑐𝑐𝑐𝑐𝑐𝑜𝑜⋯𝒢𝒢1𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁∗(1ℎ)𝑄𝑄𝐾𝐾𝑉𝑉LinearLinearLinearparameters sharing𝑄𝑄𝐾𝐾𝑉𝑉𝑑𝑑𝑑𝑑𝑅𝑅𝒢𝒢𝑅𝑅𝑅𝑅𝑅𝑅𝑇𝑇1𝑇𝑇2𝑇𝑇ℎ𝑊𝑊𝑂𝑂𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑁𝑁𝑁𝑁𝑑𝑑𝑁𝑁𝒢𝒢2𝒢𝒢ℎ𝑄𝑄′𝐾𝐾′𝑉𝑉′where the rand(0  1) is a random function  and the diagonal entries of core tensor G form the vector g.
Each entry gr ∈ (0  1)  r ∈ {1  . . .   R}. We can consider g as the trainable weight. In experiments 
we compute the weight vector by sof tmax function (i.e.  sof tmax(g)).
After that  the output of Single-block attention function is a 3-order tensor which is given by linear
computation. The Single-block attention (i.e.  a 3-order tensor with Tucker decomposition) can
reconstruct the Scaled Dot-Product attention in Eq. 2 by the summing over the tensor according to
the second index 2 (it can be seen as the coordinates in the vertical direction for a tensor)  as proved
in the following corollary. Note that in our model  we do not adopt the above reconstructing process.
Instead  to obtain a new representation  we adopt the concat method after the tensor splitting (see
Sec. 3.2). We will further show the compression ability of the Single-block attention in Sec. 3.3.
Corollary 1. Under the same conditions as in Theorem 3.1 and the value of N is equal to the value
of d  Single-block attention representation Eq. 5 can reconstruct the Scaled Dot-Product attention in
Eq. 2 by the summing over the tensor (i.e.  the output of Single-block attention function) according to
the second index. It holds that:

Attention(Q  K  V )i m =

AttenT D(G; Q  K  V )i j m

(7)

N(cid:88)

where i  j and m are the indices of the Single-block attention’s output (i.e.  a 3-order tensor).
AttenT D(·) is the function of Single-block attention based on Tucker decomposition. i and m are
the indices of outputs (i.e.  a matrix) from Eq. 2.

j=1

Proof. The proof can be found in Supplementary Materials C.

3.2 Multi-Linear Attention by Block-Term Tensor Decomposition

In order to construct the multi-head mechanism and compress the parameters of multiple groups
of mapping  we use a group of linear projections  and share the output from the linear projections.
In Figure 2(right)  the learned linear projection can map queries  keys and values to three matrices
which are composed of basis vectors. After that  we use the Block-Term tensor decomposition to
build multi-head mechanism. In our work  our model is named as Multi-linear attention  which can
be formulated as follows:

M ultiLinear(G; Q(cid:48)  K(cid:48)  V (cid:48)) = SplitConcat(

∗ (T1 + . . . + Th))W O

1
h

where Tj = AttenT D(Gj; Q(cid:48)W q  K(cid:48)W k  V (cid:48)W v)

(8)

where the core tensor Gj is a diagonal tensor  and the number of parameter in Gj is equal to the
rank of core tensor  j ∈ {1  . . .   h}. Q(cid:48)W q  K(cid:48)W k and V (cid:48)W v are equal to Q  K and V in Eq. 5 
respectively. G is the set of the core tensors. SplitConcat(·) is a function which achieves the
concatenation after splitting for a 3-order tensor. Figure 2 (right) shows the basis idea about the
multi-linear attention. The W O is the parameter matrix which is a full connection layer and correlated
to the output of Multi-linear attention. AttenT D(·) is the function of Single-block attention  which is
a part of Multi-linear attention. W q  W k and W v are the parameters matrices which are shared in
constructing Multi-linear attention.
The Multi-linear attention is a compression model. After compressing the multi-head attention in
Transformer  it is to achieve a Tensorized Transformer. The Multi-linear attention can be incorporated
into Transformer architecture. A diagram which is about the incorporating of Multi-linear attention
in partial Transformer structure is given in Supplementary Materials E.1.

3.3 Analysis of Compression and Complexity

Compression Our focus is on the compression of the multi-head mechanism in the multi-head
attention of Transformer. Previous work [35] gets the multi-head attention by multiple groups of
linear mappings. We use three linear mappings for matrices Q  K and V   respectively. For the
output of three mappings  we choose to share them which are considered as three factor matrices in

2If the coordinates of a 3-order tensor are i  j and m  j is the second index.

5

reconstructing the Multi-linear attention. This process is shown in Figure 2 (left). h is the number of
heads in [35]  and d is the dimension of factor matrices. The compression ratios can be computed
by (3 × h × d)/(3 × d + h). In practice  h is normally set to 8  d is set to 512. In this case  the
compression ratios can achieve 8. In other words  we can reduce almost 8 times parameters in the
attention layer. The details of the computing of compression ratios can be found in Supplementary
Materials D. The Transformer also contains other network layers  such as Position-wise feed forward
network and embedding layers et al. Therefore  for the compression ratios in whole Transformer  we
can compare it by the analysis of experimental results for model parameters.
Complexity The time complexity of the attention function in Eq. 2 is O(N 2d)  N is the length of
a sequence  and d is the representation dimension. In Multi-linear attention  we can reorder the
computations to receive the model complexity O(N 3)  where N is also the length of the sequence.
The minimum number of sequential operations in Multi-linear attention for different layers is
approximately equal to the self-attention in Transformer [35].

4 Related Work

The ﬁeld of language modeling has witnessed many signiﬁcant advances. Different from the archi-
tectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language
modeling  the Transformer [35] and its variants [7  12  10] achieve excellent results in language
modeling processing. Transformer networks have a potential of learning long-term dependency  but
are limited by a ﬁxed-length context in the setting of language modeling. Vaswani et al. [35] uses a
segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question.
BERT [12] is a kind of bidirectional encoder representations from transformers. It is designed to
pre-train deep bidirectional representation and obtains new SoTA on some NLP tasks. Although these
methods have achieved great results  a large number of parameters make it difﬁcult for the model to
be trained in limited resources. Transformer fails to generalize in many simple tasks  e.g. copying
string and logical inference [10]. Universal Transformers [10] propose a self-attentive recurrent
sequence model which addresses this problem. This methods can increase the training speed. In
their work  authors following weight sharing found in CNNs and RNNs  extend the Transformer
with a simple form of weight sharing that strikes an effective balance between induces and model
expressivity. This methods also uses a large number of parameters.
Therefore  it is very important to consider how to reduce the amount of memory and computing
they need. As we know  existing model compression methods are mainly divided into parameter
pruning and sharing [14]  low rank approximation [29]  knowledge transfer [2]  and transferred
convolutional ﬁlters [6]. Currently  tensor decomposition methods are used to decompose a high-
order tensor  which can get different neural network language model structures [39  40]. Besides 
tensor decomposition methods which adopts the idea of low rank approximation in most cases 
have been successfully applied to neural networks compression. For example  in literature [11  16] 
researchers approximate a tensor by minimizing the reconstruction error of the original parameters
on convolutional neural networks (CNNs). However  these approaches tend to accumulate errors
when multiple layers are compressed sequentially  and the output feature maps deviate far from
the original values with the increase of compressed layers. Our compression method uses the idea
of parameters sharing in the constructing of attention layers  and the size of output is same as the
output from self-attention in Transformer which can effectively avoid these problems. Tensorizing
Neural Networks [24] have combined the idea of reshaping weights of fully-connected layers into
high-dimensional tensors and representing them in Tensor Train format [25]. This approach was later
extended to convolutional [13] and recurrent neural networks [36]. Recently  in these work [5  34] 
researchers introduce efﬁcient compression methods for the embedding and sof tmax layers based
on structured low rank matrix approximation. TT-embedding [18] aims to compression the larger
embedding layer on Transformer-XL [7]. Sparse Transformer [28] adopts sparse techniques on the
attention matrix and reduces its parameters. This work uses a sparse attention matrix by selecting
the information on some positions in the attention matrix  but does not change the mechanism of the
attention. Our method is different from these works  and combines two compression idea (low rank
approximate and parameters sharing) to construct a tensorized Transformer.
In our work  we focus on the compression the multi-head attention in Transformer based the idea
of parameters sharing. At the same time  we also combine low-rank approximate method to reduce
parameters and computation complexity.

6

5 Experiments

Transformer is a versatile and powerful modeling tool and widely is used in various natural language
process tasks. In order to verify the effectiveness of our method (i.e.  Multi-linear attention) replacing
multi-head attention in Transformer  we carry out two NLP tasks named language modeling (LM)
and neural machine translation (NMT). Code3 for running experiments has been released  and the
key code which is about our method can be found in Supplementary Materials F.

5.1 Language Modeling

function p(s) = p(w1)(cid:81)n

Language modeling is the task of predicting the next word in a sentence. This task is to estimate
the joint probability p(s) of a sentence of tokens s=(w1  . . .   wn). The resulting models can be
used to generate text or further ﬁne-tuned to solve other NLP tasks [27]. In this paper  we employ
the standard setting of predicting next token given the sequence of preceding tokens  based on the
i=2 p(wi|w1  . . .   wi−1). We chose three datasets in the order of small (i.e. 
PTB)  medium (i.e.  WikiText-103) and large (i.e.  One-Billion). Models are evaluated based on
Perplexity (PPL)  which is the average per-word log-probability. The lower the PPL  the better the
model is.
Specially  we take Transformer  the open source state-of-the art language modeling architecture  and
replace the standard multi-head attention layers with our Multi-linear attention. Then  we test different
model conﬁgurations on the PTB [23]  WikiText-103 [22] and One-Billion Word benchmark [4]
datasets and report the results in Table 1 and Table 2.

Table 1: Results (PPL) and model parameters with state-of-the-art results on One-Billion. Tensorized
Transformer is our model. The core-1 is that the model use Single-block term tensor. Analogously 
the core-2 is that two block term tensor is used.

Model

Params Test PPL

LSTM-8192-1024+CNN Input [17]

High-Budget MoE [32]

LSTM+Mos [37]

RNN-1024+9 Gram [4]
LSTM-2018-512 [17]

GCNN-14 bottleneck [8]

Transformer+adaptive input [1]

Transformer-XL Base [7]
Transformer-XL Large [7]

Tensorized Transformer core-1
Tensorized Transformer core-2

20B
0.83B

–

1.04B

5B

113M
0.46B
0.46B
0.8B
0.16B
0.16B

51.3
43.7
31.9
30.0
28.0
37.10
23.7
23.5
21.8
20.5
19.5

5.2 Results and Details

PTB has 929k training tokens  73k validation words  and 82k test words. The results is reported in
Table 2. Similar to AWD-LSTM-MoS [37]  we apply variational dropout and weight average to our
model (i.e.  Tensorized Transformer). In addition  we need to state that  our model only replaces the
multi-head attention using Multi-linear attention structure  and the other structures remain the same.
We compare the results of our model with other models. Our model achieves the comparable results
with SoTA when the number of core tensor is equal to two. However  our model size (i.e  model
parameters) reduces by nearly half comparing with Transformer and Transformer-XL.
WikiText-103 contains 267 735 unique tokens. The dataset is available word-level language modeling
benchmark with long-term dependency. It contains 103M training tokens from 28k articles  with an
average length of 3.6k tokens per article  which allows testing the ability of long-term dependency
modeling. As shown in Table 2  our model get the perplexity of 18.9  which is a comparable
experimental result with the previous SoTA perplexity 18.3   which demonstrates the effectiveness of
the proposed attention architecture.

3https://github.com/szhangtju/The-compression-of-Transformer

7

Model

LSTM+augmented loss [15]
Variational RHN [41]
4-layer QRNN [21]
AWD-LSTM-MoS [37]
Transformer+adaptive input [1]
Transformer-XL-Base [7]
Transformer-XL-Large [7]
Transformer-XL+TT [18]
Sparse Transformer [28]
Tensorized Transformer core-1
Tensorized Transformer core-2

PTB
Params Val PPL
24M
23M
–
22M
24M
24M
–
18 M
14M
12M
12M

75.7
67.9
–
58.08
59.1
56.72
–
57.9*
74.0*
60.5
54.25

Test PPL
48.7
65.4
–
55.97
57
54.52
–
55.4*
73.1*
57.9
49.8

WikiText-103

Params Val PPL
–
–
151M
–
247M
151M
257M
130M
174M
85.3M 22.7
85.3M 19.7

–
–
–
29.0
19.8
23.1
–
23.61*
38.98*

Test PPL
48.7
45.2
33.0
29.2
20.5
24.0
18.3
25.70*
40.23*
20.9
18.9

’−’
Table 2: Results and compression with state-of-the-art results on PTB and WikiText-103.
indicates no reported results in that setting  ’∗’ indicates that the results is our own implementation.

The One-Billion Word benchmark is a large dataset derived from a news site. The dataset consists
of 829  250  940 tokens over a vocabulary of 793  471 words. In this dataset  sentences are shufﬂed
and hence the context is limited. Consequently  this dataset mainly tests the ability of modeling only
short-term dependency. The comparison between Tensorized Transformer and the other methods
are shown in Table 1. Although Tensorized Transformer is mainly designed to better compress
Transformer or Transformer-XL model  it dramatically improves the single-model SoTA from 21.8
to 19.5. Speciﬁcally  Tensorized Transformer signiﬁcantly outperforms a contemporary method
using vanilla Transformers [35]  suggesting that the advantage of the tensorized Transformer is also
generalizable to modeling short sequences.
Table 2 and Table 1 show that our model get the lower PPL than other models in three datasets.
An exciting observation is that our model has much fewer parameters. The model of Transformer-
XL+TT [18] is a recent compression model with Tensor Train to compress the input embedding layers
only. Sparse Transformer [28] uses the method of sparse attention matrix to compress Transformer
model. The results in Table 2 show that compared with Transformer-XL+TT  our method has much
fewer parameters  and better language modeling performance. These results verify that our model
(i.e.  Multi-linear attention) is effective in language modeling tasks  and has performed well for
the model compression. Other details (such as hyperparameters and Hardware) can be found in
Supplementary Materials E.

5.3 Neural Machine Translation

The goal is to map an input sequence s = (x1  x2  . . .   xn) representing a phrase in one language  to
an output sequence y = (y1  y2  . . .   ym) representing the same phrase in a different language. In
this task  we have trained the Transformer model [35] on WMT 2016 English-German dataset [31].
Sentences were tokenized using the SentencePiece 4. For our experiments  we have replaced each
of the attention layers with Multi-linear attention in Encoder. For evaluation we used beam search
with a beam size of 5 and length penalty α=0.6. In this section  we only compared the results with
Transformer [35]. Our results are summarized in Table 3. ∗ indicates that the result is our own
implementation.
In Table 3  we select two baseline models. The Base-line [31] is ﬁrst model in WMT 2016 English-
German dataset. For the other baseline  we use the basic Transformer architecture [35]. The BLEU
score is 34.5 for the basic architecture. We carry out two Tensorized Transformer structures  namely
core-1 and core-2 respectively. When Tensorized Transformer core-1 and core-2 are used  the BLEU
scores are 34.10 and 34.91  which achieves better performance over Transformer. As for the reported
model parameter size  our model uses less parameters.

4https://github.com/google/sentencepiece

8

Table 3: Results and compression with Transformer on WMT-16 English-to-German translation.

Model

Base-line [31]

Linguistic Input Featurec [30]

Attentional encoder-decoder + BPE [31]

Transformer [35]

Tensorized Transformer core-1
Tensorized Transformer core-2

–
–
–

Params BLEU
26.8
28.4
34.2
34.5*
34.10
34.91

52M
21M
21.2M

5.4 Discussion

We have shown the results on language modeling and neural machine translation tasks using the Multi-
linear attention. For the compression of the model parameters  although we report the parameters of
the whole model structure  our method mainly considers the compression of multi-head attention
but has not changed other layers in Transformer. Regarding the rationale for the improvements  in
Corollary 1  we prove that the output of the original attention can be represented by summing over the
3-order tensor. In Figure 2  we use a concat function over these matrices from tensor splitting. The
operation of concat can model all values in the 3-order tensor  and thus captures more information
than sum operator. Another reason could be the alleviation of overﬁtting by reducing parameters. The
overﬁtting will appear when the number of the core tensor is greater than 2. Besides  according to
our experiments  relatively large dimensions of the word embedding can lead to overﬁtting  resulting
in performance degradation. Therefore  our model requires a relatively small dimension of the
embedding  compared with the original Transformer. In order for a more systematic evaluation  we
report more experiments and analyses in Supplementary Materials E.4.

6 Conclusion and Further Work

We have proposed a novel self attention encoder layer  namely the Multi-linear attention  to compress
the original multi-head attention and derive a novel encoding scheme. Our main contribution lies in a
structure of Tensorized Transformer based on Block-Term tensor decomposition which is represented
by the combination of a group of 3-order tensors  with low-rank approximation and parameters
sharing ideas adopted. Compared with existing Transformer based methods  our model achieved
higher compression ratio and got better experimental results  particularly in language modeling task.
These evidences imply that our method can potentially be further applied to more NLP tasks with
limited resources.
In the future  we will continue to optimize the Tensorized Transformer framework and apply it in
other NLP tasks. As we stated earlier  our model may suffer from overﬁtting when the number of
cores is large in language modeling. In the future  we will explore the fundamental reasons that cause
the problem and tackle them within the Tensorized Transformer framework.

7 Acknowledgement

This work is supported in part by the state key development program of China (grant No.
2017YFE0111900  2018YFC0831704)  Natural Science Foundation of China (grant No. 61772363 
U1636203)  and the European Unions Horizon 2020 research and innovation programme under the
Marie SkodowskaCurie grant agreement No.721321.

References
[1] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv

preprint arXiv:1809.10853  2018.

[2] Cristian Bucilu  Rich Caruana  and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the
12th ACM SIGKDD international conference on Knowledge discovery and data mining  pages 535–541.
ACM  2006.

9

[3] J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an

n-way generalization of “eckart-young” decomposition. Psychometrika  35(3):283–319  1970.

[4] Ciprian Chelba  Tomas Mikolov  Mike Schuster  Qi Ge  Thorsten Brants  Phillipp Koehn  and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling. Computer
Science  2013.

[5] Patrick Chen  Si Si  Yang Li  Ciprian Chelba  and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank
In Advances in Neural Information Processing

approximation for neural language model shrinking.
Systems  pages 10988–10998  2018.

[6] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on

machine learning  pages 2990–2999  2016.

[7] Zihang Dai  Zhilin Yang  Yiming Yang  William W Cohen  Jaime Carbonell  Quoc V Le  and Ruslan
Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860  2019.

[8] Yann N Dauphin  Angela Fan  Michael Auli  and David Grangier. Language modeling with gated
convolutional networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70  pages 933–941. JMLR. org  2017.

[9] Lieven De Lathauwer. Decompositions of a higher-order tensor in block terms—part ii: Deﬁnitions and

uniqueness. SIAM Journal on Matrix Analysis and Applications  30(3):1033–1066  2008.

[10] Mostafa Dehghani  Stephan Gouws  Oriol Vinyals  Jakob Uszkoreit  and Łukasz Kaiser. Universal

transformers. Published at ICLR2019  2018.

[11] Emily L Denton  Wojciech Zaremba  Joan Bruna  Yann LeCun  and Rob Fergus. Exploiting linear structure
within convolutional networks for efﬁcient evaluation. In Advances in neural information processing
systems  pages 1269–1277  2014.

[12] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training of deep bidirec-

tional transformers for language understanding. 2018.

[13] Timur Garipov  Dmitry Podoprikhin  Alexander Novikov  and Dmitry Vetrov. Ultimate tensorization:

compressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214  2016.

[14] Song Han  Jeff Pool  John Tran  and William Dally. Learning both weights and connections for efﬁcient

neural network. In Advances in neural information processing systems  pages 1135–1143  2015.

[15] Hakan Inan  Khashayar Khosravi  and Richard Socher. Tying word vectors and word classiﬁers: A loss

framework for language modeling. arXiv preprint arXiv:1611.01462  2016.

[16] Max Jaderberg  Andrea Vedaldi  and Andrew Zisserman. Speeding up convolutional neural networks with

low rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press  2014.

[17] Rafal Jozefowicz  Oriol Vinyals  Mike Schuster  Noam Shazeer  and Yonghui Wu. Exploring the limits of

language modeling. arXiv preprint arXiv:1602.02410  2016.

[18] Valentin Khrulkov  Oleksii Hrinchuk  Leyla Mirvakhabova  and Ivan Oseledets. Tensorized embedding

layers for efﬁcient model compression. arXiv preprint arXiv:1901.10787  2019.

[19] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review  51(3):455–500 

2009.

[20] Guangxi Li  Jinmian Ye  Haiqin Yang  Di Chen  Shuicheng Yan  and Zenglin Xu. Bt-nets: simplifying

deep neural networks via block term decomposition. arXiv preprint arXiv:1712.05689  2017.

[21] Stephen Merity  Nitish Shirish Keskar  and Richard Socher. An analysis of neural language modeling at

multiple scales. arXiv preprint arXiv:1803.08240  2018.

[22] Stephen Merity  Caiming Xiong  James Bradbury  and Richard Socher. Pointer sentinel mixture models.

arXiv preprint arXiv:1609.07843  2016.

[23] Tomáš Mikolov  Anoop Deoras  Stefan Kombrink  Lukáš Burget  and Jan ˇCernock`y. Empirical evaluation
In Twelfth Annual Conference of the

and combination of advanced language modeling techniques.
International Speech Communication Association  2011.

10

[24] Alexander Novikov  Dmitrii Podoprikhin  Anton Osokin  and Dmitry P Vetrov. Tensorizing neural networks.

In Advances in neural information processing systems  pages 442–450  2015.

[25] Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc Computing  33(5):2295–2317 

2011.

[26] Matthew Peters  Mark Neumann  Mohit Iyyer  Matt Gardner  Christopher Clark  Kenton Lee  and Luke
Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies 
Volume 1 (Long Papers)  pages 2227–2237  2018.

[27] Alec Radford  Karthik Narasimhan  Tim Salimans  and Ilya Sutskever.

Improving language under-
standing by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language understanding paper. pdf  2018.

[28] Alec Radford Rewon Child  Scott Gray and Ilya Sutskever. Generating long sequences with sparse

transformer. arXiv preprint arXiv:1904.10509  2019.

[29] Tara N Sainath  Brian Kingsbury  Vikas Sindhwani  Ebru Arisoy  and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE
international conference on acoustics  speech and signal processing  pages 6655–6659. IEEE  2013.

[30] Rico Sennrich and Barry Haddow. Linguistic input features improve neural machine translation. arXiv

preprint arXiv:1606.02892  2016.

[31] Rico Sennrich  Barry Haddow  and Alexandra Birch. Edinburgh neural machine translation systems for

wmt 16. arXiv preprint arXiv:1606.02891  2016.

[32] Noam Shazeer  Azalia Mirhoseini  Krzysztof Maziarz  Andy Davis  Quoc Le  Geoffrey Hinton  and Jeff
Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538  2017.

[33] Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika  31(3):279–311 

1966.

[34] Ehsan Variani  Ananda Theertha Suresh  and Mitchel Weintraub. West: Word encoded sequence transducers.

arXiv preprint arXiv:1811.08417  2018.

[35] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems  pages 5998–6008  2017.

[36] Yinchong Yang  Denis Krompass  and Volker Tresp. Tensor-train recurrent neural networks for video
classiﬁcation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
3891–3900. JMLR. org  2017.

[37] Zhilin Yang  Zihang Dai  Ruslan Salakhutdinov  and William W Cohen. Breaking the softmax bottleneck:

A high-rank rnn language model. arXiv preprint arXiv:1711.03953  2017.

[38] Jinmian Ye  Linnan Wang  Guangxi Li  Di Chen  Shandian Zhe  Xinqi Chu  and Zenglin Xu. Learning
compact recurrent neural networks with block-term tensor decomposition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 9378–9387  2018.

[39] Lipeng Zhang  Peng Zhang  Xindian Ma  Shuqin Gu  Zhan Su  and Dawei Song. A generalized language

model in tensor space. arXiv preprint arXiv:1901.11167  2019.

[40] Peng Zhang  Zhan Su  Lipeng Zhang  Benyou Wang  and Dawei Song. A quantum many-body wave
function inspired language modeling approach. In Proceedings of the 27th ACM International Conference
on Information and Knowledge Management  pages 1303–1312. ACM  2018.

[41] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint

arXiv:1611.01578  2016.

11

,Rishabh Iyer
Shengjie Wang
Wenruo Bai
Jeff Bilmes
Yaniv Tenzer
Alex Schwing
Kevin Gimpel
Tamir Hazan
Quentin Berthet
Vianney Perchet
Xindian Ma
Peng Zhang
Shuai Zhang
Nan Duan
Yuexian Hou
Ming Zhou
Dawei Song