2019,Value Function in Frequency Domain and the Characteristic Value Iteration Algorithm,This paper considers the problem of estimating the distribution of returns in reinforcement learning (i.e.  distributional RL problem). It presents a new representational framework to maintain the uncertainty of returns and provides mathematical tools to compute it. 
We show that instead of representing a probability distribution function of returns  one can represent their characteristic function instead  the Fourier transform of their distribution. We call the new representation Characteristic Value Function (CVF)  which can be interpreted as the frequency domain representation of the probability distribution of returns.
We show that the CVF satisfies a Bellman-like equation  and its corresponding Bellman operator is contraction with respect to certain metrics.
The contraction property allows us to devise an iterative procedure to compute the CVF  which we call Characteristic Value Iteration (CVI). We analyze CVI and its approximate variant and show how approximation errors affect the quality of computed CVF.,Value Function in Frequency Domain

and the Characteristic Value Iteration Algorithm

Amir-massoud Farahmand∗

Vector Institute & University of Toronto

Toronto  Canada

farahmand@vectorinstitute.ai

Abstract

This paper considers the problem of estimating the distribution of returns in rein-
forcement learning  i.e.  distributional RL problem. It presents a new representa-
tional framework to maintain the uncertainty of returns and provides mathematical
tools to compute it. We show that instead of representing a probability distribution
function of returns  one can represent their characteristic function  the Fourier
transform of their distribution. We call the new representation Characteristic Value
Function (CVF). The CVF satisﬁes a Bellman-like equation  and its corresponding
Bellman operator is contraction with respect to certain metrics. The contraction
property allows us to devise an iterative procedure to compute the CVF  which
we call Characteristic Value Iteration (CVI). We analyze CVI and its approximate
variant and show how approximation errors affect the quality of the computed
CVF.

1

Introduction

The object of focus of the conventional RL is the expected return of following a policy  i.e.  the value
function [Sutton and Barto  2019]. The goal is to ﬁnd a policy that maximizes that expectation over
all states  i.e.  the optimal policy. This leads to agents that do not consider the distribution of returns
in their decision making  but only its ﬁrst moment. This might be of concern in scenarios where the
risk is of paramount importance. Estimating the distribution of the return facilitates designing agents
that consider objectives more general than maximizing the expected return  such as various notions
of risk [Tamar et al.  2012  Prashanth and Ghavamzadeh  2013  García and Fernández  2015  Chow
et al.  2018].
The Distributional RL (DistRL) literature [Engel et al.  2005  Morimura et al.  2010b  Bellemare
et al.  2017  Barth-Maron et al.  2018  Lyle et al.  2019]  on the other hand  moves away from
the conventional goal of estimating the expectation of return and attempts to estimate a richer
representation of the return  such as the distribution itself [Morimura et al.  2010b a] or some
statistical functional of it [Rowland et al.  2018  Dabney et al.  2018  Rowland et al.  2019]. It is
notable that so far the focus of the DistRL literature has mostly been on designing better performing
agents according to the expected return  and not any risk-related performance measure  but it is
conceivable that those methods can be be used for designing risk-aware agents too.
This paper develops a new framework for maintaining the information available in the distribution of
returns. Instead of estimating the distribution function itself  we maintain the Characteristic Function
(CF) of the returns. The CF of a random variable (r.v.) is the Fourier transform of its probability
distribution function (PDF). Similar to PDF  the CF of a r.v. contains all the information available
about the distribution of that r.v.  i.e.  CF and PDF have a bijection relationship. They are nonetheless

∗Homepage: http://academic.sologen.net.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

different representations of the uncertainty of a r.v.  hence they allow different types of manipulations
and processing. The beneﬁt of a new representation is that it opens up the possibility of designing
new algorithms. An example from the ﬁeld of control theory is that we have both time and frequency
domain representations of a dynamical system. Although they are equivalent in many cases  designing
a controller in the frequency domain is sometimes easier and may provide better insights. This work
brings the frequency-based representation of uncertainty to DistRL.
The estimation procedures based on CF are not novel. Methods based on the Empirical Characteristic
Function (ECF) have a long history in the statistics and econometrics literature [Feuerverger and
Mureika  1977  Feuerverger and McDunnough  1981  Feuerverger  1990  Knight and Yu  2002  Yu 
2004]. These methods are considered as alternatives to the maximum likelihood estimation (MLE) 
because as opposed to MLE  whose computation might be infeasible for some distributions  one can
always deﬁne and compute the ECF. This paper is inspired from that literature and develops similar
tools for RL and approximate dynamic programming.
The main idea of this work is that by transforming the return  which is a r.v.  to the frequency
domain through the Fourier transform  we can deﬁne Characteristic Value Function (CVF)  which
essentially captures all information about the distribution of the return. A contribution of this work
is that we prove that CVF indeed satisﬁes a Bellman-like equation ˜T π ˜V = ˜V (Section 3). The
corresponding Bellman operator  however  is different from the conventional ones or those in the
DistRL literature. Instead of having an additive form  it is multiplicative  i.e.  ( ˜T π ˜V )(ω; x) (cid:44)

˜R(ω; x)(cid:82) P π(dy|x) ˜V (γω; y) with ω being the frequency variable  x being the state variable  and

˜R being the Fourier transform of the immediate reward distribution (we will deﬁne these quantities
later). We also prove that the new Bellman operator is contraction with respect to (w.r.t.) some
speciﬁc metrics deﬁned in the frequency domain (Section 3.1). The contraction property suggests
that one might ﬁnd the CVF through an iterative procedure similar to value iteration  which we call
the Characteristic Value Iteration (CVI) algorithm (Section 4). This is the algorithmic contribution of
this work.
Any procedure that implements CVI  however  may not perform it exactly  for example because we
only have data as opposed to the actual transition probability distribution or because the state space is
very large and we need to use function approximation. In case we can only approximately perform
CVI  which we call Approximate CVI (ACVI)  we inevitably have some errors. To understand the
effect of using function approximation on these errors better  we consider a class of band-limited
(in the frequency domain) functions  and study their function approximation and covering number
properties (in the extended version of the paper). Another contribution of this work is the analysis
of how the errors caused at each iteration of ACVI propagate throughout iterations and affect
the quality of the outcome CVF (Section 5). We show that the errors in earlier iterations decay
exponentially fast  i.e.  the past errors are forgotten quickly. This is the same phenomenon observed
in the conventional approximate value iteration. Finally  we show how to convert the error of CVF in
the frequency domain to an error in distributions  measured according to the p-smooth Wasserstein
distance (Section 6).

2 Distributional Bellman equation
We consider a discounted Markov Decision Process (MDP) (X  A R P  γ) [Szepesvári  2010].
Here X is the state space  A is the action space  P : X × A → M(X ) is the transition probability
kernel  R : X × A → M(R) is the immediate reward distribution  and 0 ≤ γ < 1 is the discount
factor.2 The (Markov stationary) policy π : X → M(A) induces the transition probability kernel
P π : X → M(X ) and the immediate reward distribution for the policy Rπ : X → M(R).
An MDP together with an initial state distribution ρ ∈ M(X ) encode the laws governing the temporal
evolution of a discrete-time stochastic process controlled by an agent as follows: The controlled
process starts at time t = 0 with random initial state X0 drawn from ρ  i.e.  X0 ∼ ρ. The agent
following a policy π chooses action At ∈ A according to At ∼ π(·|Xt) (stochastic policy) or
2Here M(Ω) refers to the space of all probability distributions on an appropriately deﬁned σ-algebra of Ω 
e.g.  the Borel σ-algebra on R. We do not deal with the measure theoretic considerations in this work. Refer to
Appendix C of Bertsekas [2013] or Chapter 7 of Bertsekas and Shreve [1978]. We occasionally use ¯X to denote
the probability distribution µ of the r.v. X.

2

At = π(Xt) (deterministic policy). In response  the next state is Xt+1 ∼ P(·|Xt  At) and the agent
receives reward Rt ∼ R(·|Xt  At). This process repeats. We may occasionally use R(x  a) or Rπ(x)
to denote to the r.v. that is drawn from R(·|x  a) or Rπ(·|x). Also we may use z = (x  a) as a
shorthand. When we refer to a r.v. Z = (X  A)  this should be interpreted as a r.v. deﬁned with
A ∼ π(·|X)  where the policy should be clear from the context.
The return of the agent starting from a state x ∈ X and following a policy π is the following random
variable:

(cid:88)

i≥0

Gπ(x) =

γiRi.

The (conventional) value function V π is the ﬁrst moment of this r.v.  i.e. 

V π(x) = E [Gπ(X0)|X0 = x] .

From Gπ(x) = R0 + γ(cid:80)

Likewise  one may deﬁne the return Gπ(x  a) for starting from state x  choosing action a  and
following policy π afterwards. The corresponding ﬁrst moment of Gπ(x  a) would be the action-
value function Qπ(x  a).

i≥0 γiRi+1  we see that Gπ(x) is the addition of two r.v. R0 and γGπ(X1)
with X1 ∼ P π(·|X0 = x). Therefore  the law (probability distribution) of Gπ(x) is the same as the
law of R0 + γGπ(X1)  i.e. 

Gπ(x)

(D)
= R0 + γGπ(X1).

(1)

= to emphasize that we are comparing two probability distributions. This is

Here we use the symbol (D)
the Bellman-like distributional equation in the conventional DistRL.
We can also have a similar equation that relates ¯Gπ (the distribution of the r.v. Gπ) and ¯R(x) =
Rπ(·|x) (the distribution of the r.v. Rπ(x)) [Rowland et al.  2018]. To deﬁne it  we recall the
deﬁnition of the pushforward measure: Given a probability distribution ν ∈ M(R) and a measurable
function f : R → R  the pushforward measure f#ν ∈ M(R) is deﬁned as (f#ν)(A) = ν(f−1(A))
for all Borel sets A ⊂ R.
The Bellman operator ¯T π : M(X ) → M(X ) between distributions is deﬁned as

(cid:90)

( ¯T π ¯G)(x) (cid:44)

(r + γy)#

¯G(y)Rπ(dr|x)P π(dy|x) 

∀x ∈ X .

With this notation  the distributional Bellman equation is

¯Gπ(x) = ( ¯T π ¯Gπ)(x) 

∀x ∈ X .

(2)

The distributional Bellman equation represents the intrinsic uncertainty of the return due to the
randomness of the dynamics and policy. We may occasionally use ¯V π to refer to ¯Gπ  to show its
close relation to the conventional value function.

3 Characteristic value function

The conventional approach to representing the uncertainty of a r.v. is through its probability distribu-
tion function. This is not the only way to characterize a r.v. though. An alternative is to characterize
the r.v. through the Fourier transform of its distribution function. This is known as the Characteristic
Function (CF) of the random variable [Williams  1991].
In this section we show that the instead of representing the distribution function of the return Gπ 
we may represents its characteristic function. Interestingly  the CF of return satisﬁes a Bellman-like
equation  which is quite different from the conventional ones (1) and (2) that we have encountered so
far.
Let us brieﬂy recall the deﬁnition of a CF of a random variable. Given a real-valued r.v. X with the
probability distribution µ ∈ M(R)  its corresponding CF cX : R → C is the function deﬁned as3

cX (ω) (cid:44) E(cid:2)ejXω(cid:3) =

(cid:90)

exp(jxω)µ(dx) 

ω ∈ R

(3)

3Here X is a generic r.v. and does not refer to the state. The particular r.v. will be clear from the context.

3

√−1 is the imaginary unit. The CF of a probability distribution is closely related to

where j =
the Fourier transform of its distribution function. If the probability density function is well-deﬁned 
CF is its Fourier transform  though CF exists even if the density does not. Several properties of CF
are summarized in an appendix of the extended version of the paper. Thinking in the terms of the
spatial-frequency duality common in the Fourier analysis  the probability distribution function is the
spatial representation of a r.v. (with the magnitude of the r.v. corresponding to the space dimension) 
and the CF is its frequency representation.
Consider the recursive relation Gπ(x) = Rπ(x) + γGπ(X(cid:48))  with X(cid:48) ∼ P π(·|x)  between the
return Gπ(x) (a r.v.) and the random reward Rπ(x) and the return at the next step Gπ(X(cid:48)). By the
distributional equality of both sides (cf. (1))  we have

cGπ(x)(ω) = E [exp (jωGπ(x))] = E [exp (jω (Rπ(x) + γGπ(X(cid:48))))]  

∀ω ∈ R.

(4)

The right-hand side (RHS) of (4) is
E [exp (jω (Rπ(x) + γGπ(X(cid:48))))] = E [E [exp (jω (Rπ(x) + γGπ(X(cid:48)))) | X = x  A]]

= E [E [exp (jωRπ(x)) | X = x  A] E [exp (jωγGπ(X(cid:48))) | X = x  A]]
= cRπ(x)(ω) E [E [exp (jωγGπ(X(cid:48))) | X = x  A]]
= cRπ(x)(ω) E [exp (jωγGπ(X(cid:48))) | X = x]  

(5)
where A is a r.v. drawn from π(·|x). Here we beneﬁtted from the fact that the r.v. Rπ(x) and Gπ(X(cid:48))
are conditionally independent given X = x and A.
Let us consider the CF of Gπ(X(cid:48)) conditioned on X = x:

E [exp (jωGπ(X(cid:48))) | X = x] = E [E [exp (jωGπ(X(cid:48))) | X(cid:48)] | X = x]

(cid:90)
= E(cid:2)cGπ(X(cid:48))(ω) | X = x(cid:3)  

=

P π(dx(cid:48)|x)E [exp (jωGπ(x(cid:48)))]

(cid:90)

(cid:90)

(6)
where we conditioned the inner expectation on the next-state X(cid:48) (so its randomness comes from the
return from that point onward)  and used the deﬁnition of CF.
Plugging (6) in (5) gives the RHS of (4). So we get

cGπ(x)(ω) = cRπ(x)(ω) E [exp (jωγGπ(X(cid:48))) | X = x]

= cRπ(x)(ω)E(cid:2)cγGπ(X(cid:48))(ω) | X = x(cid:3)
= cRπ(x)(ω)E(cid:2)cGπ(X(cid:48))(γω) | X = x(cid:3) = cRπ(x)(ω)

(cid:90)

P π(dy|x)cGπ(y)(γω) 

(7)

where the penultimate equality is because of the scaling property of CF (refer to the extended version
of the paper for more information).
We denote the CF of the reward cRπ(x)(ω) by ˜R(ω; x)  and the CF of the return cGπ(x)(ω) by
˜V π(ω; x) for all x ∈ X and ω ∈ R. Here the symbol ˜· is used to remind us that we are referring to a
CF of a random variable. With these notations  we can write (7) in more compact form of

˜V π(ω; x) = ˜R(ω; x)

P π(dy|x) ˜V π(γω; y).

(8)

This is the Bellman-like equation between the CF of return and the reward. The function
˜V π : R × X → C1 (where C1 is the area within the unit circle in the complex plane  i.e. 
C1 = { z ∈ C : |z| ≤ 1}) is the CF of the Gπ(x) for all x ∈ X . We call ˜V π the Characteris-
tic Value Function (CVF).
We also deﬁne the Bellman operator between the CF functions:

P π(dy|x) ˜V (γω; y).
With this notation  the Bellman equation can be written more compactly as

( ˜T π ˜V )(ω; x) (cid:44) ˜R(ω; x)

It is worth mentioning that for any ﬁxed x ∈ X   ω (cid:55)→ ˜V π(ω; x) is a CF. A CF is continuous function
of ω and its magnitude is bounded by 1 (refer to the extended version of the paper).

˜V π = ˜T π ˜V π.

4

3.1 Bellman operator is contraction

We show that the Bellman operator ˜T π is a contraction w.r.t. certain metrics  to be speciﬁed. This
allows us to devise a value iteration-like procedure that converges to the CVF ˜V π of a policy π.
We ﬁrst deﬁne some distance metrics between CFs. Given two CF c1  c2 : R → C  and p ≥ 1  we
deﬁne

d∞ p(c1  c2) (cid:44) sup
ω∈R

d1 p(c1  c2) (cid:44)

ωp

(cid:12)(cid:12)(cid:12)(cid:12) c1(ω) − c2(ω)
(cid:12)(cid:12)(cid:12)(cid:12)  
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˜V1(ω; x) − ˜V2(ω; x)

0 = 0.4

ωp

(9)

ωp

(cid:90) (cid:12)(cid:12)(cid:12)(cid:12) c1(ω) − c2(ω)
(cid:12)(cid:12)(cid:12)(cid:12) dω.
(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˜V1(ω; x) − ˜V2(ω; x)

ωp

Here we use the convention that 0
We also deﬁne similar metrics for functions such as ˜R and ˜V π. Given ˜V1  ˜V2 : R × X → R  we
deﬁne
d∞ p( ˜V1  ˜V2) (cid:44) sup
x∈X

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   d1 p( ˜V1  ˜V2) (cid:44) sup

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dω.

sup
ω∈R

(10)
There are similar to the distances for comparing two CFs  with the difference that we take the
supremum over all states x ∈ X . To be more precise about how the distances are calculated (e.g.  sup
over X   etc.)  we could use dX (∞) ω(∞ p)( ˜V1  ˜V2) instead of d∞ p( ˜V1  ˜V2). To simplify the notations 
however  we use the overloaded symbols d∞ p and d1 p instead.
Based on these distances  we deﬁne the following norms for a function ˜V : R × X → R

x∈X

(cid:13)(cid:13)(cid:13) ˜V

(cid:13)(cid:13)(cid:13)∞ p

(cid:13)(cid:13)(cid:13) ˜V

(cid:13)(cid:13)(cid:13)1 p

= d∞ p( ˜V   0) 

= d1 p( ˜V   0) 

where 0 is a constant function (ω; x) (cid:55)→ 0. We sometimes refer to the supremum w.r.t. x ∈ X of ˜V
by (cid:107) ˜V (ω;·)(cid:107)∞ = supx∈X | ˜V (ω; x)|. This should not be confused with (cid:107) ˜V (cid:107)∞ p  whose supremum
is over both ω and x  and the ω variable is weighted by w−p.
Several properties of d∞ p and d1 p are presented in an appendix of the extended version of the
paper. Brieﬂy  we show that d1 p and d∞ p are metrics. We also show that the space of VCFs
V = { ˜V : R × X → C1 : ˜V (0; x) = 1}  which is a superset of the space of all feasible VCFs 
endowed with d∞ p is complete.
The following result shows that the Bellman operator for VCF is a contraction operator w.r.t. d1 p
and d∞ p. This is the main result of this section.
Lemma 1. Let 0 < γ < 1. The operator ˜T π is a γp-contraction in d∞ p (for p > 0) and γp−1-
contraction in d1 p (for p > 1). That is  for any ˜V1  ˜V2 : R × X → C with d∞ p( ˜V1  ˜V2) < ∞ or
d1 p( ˜V1  ˜V2) < ∞  we have

d∞ p( ˜T π ˜V1  ˜T π ˜V2) ≤ γpd∞ p( ˜V1  ˜V2) 
d1 p( ˜T π ˜V1  ˜T π ˜V2) ≤ γp−1d1 p( ˜V1  ˜V2).

For the contraction to be non-trivial  and avoid having a trivial inequality such as ∞ ≤ γp∞  we
require the boundedness of d∞ p( ˜V1  ˜V2) or d1 p( ˜V1  ˜V2). This is a condition that should be veriﬁed 
and as we shall soon see holds under certain conditions.
We brieﬂy remark that the Bellman operator ˜T π is not a contraction w.r.t.
(cid:107) ˜V (cid:107)∞ = supx∈X supω∈R | ˜V (ω; x)|. This is shown in the extended version of the paper.
The importance of showing that the Bellman operator for VCF is a contraction is that we can then
apply the Banach ﬁxed point theorem (e.g.  Theorem 3.2 of Hunter and Nachtergaele [2001]) to
show the uniqueness of the ﬁxed point ˜V π (we also require the completeness of the space  which is
shown for d∞ p). Moreover  it suggests that we can ﬁnd the ﬁxed point by iterative application of the
operator. This is the path we pursue in the next section.

the supremum norm

4The metric d∞ p has been studied under the name of Fourier-based metric Carrillo and Toscani [2007]  and

is called Toscani distance by Villani [2008].

5

4 Characteristic value iteration

The contraction property of the Bellman operator ˜T π (Lemma 1) suggests that we can ﬁnd ˜V π by an
iterative procedure  similar to the conventional value iteration. The procedure is

˜V1 ← ˜R 
˜Vk+1 ← ˜T π ˜Vk = ˜RP π ˜Vk.

(k ≥ 1)

(11)

We call this procedure Characteristic Value Iteration (CVI).
CVI converges under certain conditions. To see this  notice that ˜V π = ˜T π ˜V π  so for p ≥ 1 by
Lemma 1 we have

d∞ p( ˜T π ˜Vk  ˜V π) = d∞ p( ˜T π ˜Vk  ˜T π ˜V π) ≤ γpd∞ p( ˜Vk  ˜V π) 

under the condition that d∞ p( ˜Vk  ˜V π) < ∞.
Similarly  we have d1 p( ˜T π ˜Vk  ˜V π) ≤
γp−1d1 p( ˜Vk  ˜V π) (for p > 1). By the iterative application of this upper bound  assuming that
d∞ p( ˜R  ˜V π) < ∞  we get that

d∞ p( ˜Vk+1  ˜V π) ≤ γpd∞ p( ˜Vk  ˜V π) ≤ ··· ≤ (γp)kd∞ p( ˜V1  ˜V π) = (γp)kd∞ p( ˜R  ˜V π).

(12)

Likewise  assuming that d1 p( ˜R  ˜V π) < ∞  we obtain

d1 p( ˜Vk+1  ˜V π) ≤ (γp−1)kd1 p( ˜R  ˜V π).

(13)
As long as d∞ p( ˜R  ˜V π) (or d1 p( ˜R  ˜V π)) is ﬁnite for some p ≥ 1 (p > 1)  CVI converges geometri-
cally fast. A result in an appendix of the extended version of the paper speciﬁes the condition when
the d∞ p distance of two CF would be ﬁnite. For p = 1  it is sufﬁcient that the immediate reward
Rπ(x) ∼ R(·; x) and the return Gπ(·; x) be integrable  i.e.  E [|Rπ(x)|]   E [|Gπ(·; x)|] < ∞ for all
states x ∈ X . Since we deal with discounted MDP  the integrability of Rπ(x) (uniformly over X )
entails the integrability of Gπ(·; x). Therefore under very mild conditions  CVI is convergent w.r.t.
d∞ 1.
For integer valued p ≥ 2  the condition becomes more restrictive. The ﬁrst requirement is that
E [|Rπ(x)|p] and E [|Gπ(·; x)|p] are ﬁnite. This is not restrictive  and holds for many problems. The
restrictive condition is that the ﬁrst k = 1  . . .   p − 1 moments of the reward and the return should

match  i.e.  E(cid:2)Rπ(x)k(cid:3) = E(cid:2)Gπ(x)k(cid:3) for all x ∈ X . This does not seem realistic  perhaps except

for p = 2 when problems with zero expected immediate reward for all states but with varying variance
are imaginable.
One can show that the ﬁxed point of ˜T π is unique. The result is formally stated in the extended
version of the paper.

4.1 Approximate characteristic value iteration

Performing CVI (11) exactly may not be practical  for at least two reasons. First  for problems with
large state space  we cannot represent ˜V π exactly and we need to rely on function approximation.
Second  for learning scenario where we do not have access to the model P π  but only observe data
from interacting with the environment  we cannot apply the Bellman operator ˜T π exactly either.
We can extend CVI to Approximate CVI (ACVI) similar to how exact VI can be extended to
Approximate Value Iteration  also known as Fitted Value Iteration or Fitted Q-Iteration. Various
variants of AVI have been empirically and theoretically studied in the literature [Ernst et al.  2005 
Munos and Szepesvári  2008  Farahmand et al.  2009  Silver et al.  2016  Tosatto et al.  2017  Chen
and Jiang  2019]. We would like to build the same general framework for CVF and CVI.
Suppose that for whatever reason we perform each iteration of CVI only approximately  that is 
˜Vk+1 ≈ ˜T π ˜Vk. The resulting procedure can be described as

˜V1 ← ˜R + ˜ε1 
˜Vk+1 ← ˜T π ˜Vk + ˜εk+1.

(k ≥ 1)

(14)

6

Here ˜εk : R × X → C is the error in the frequency-state space. Recall that the value of a valid CF
at frequency ω = 0 is equal to one  i.e.  c(0) = 1. To ensure that ˜Vk(·; x) is a CF for all x ∈ X  
we must have ˜Vk(0; x) = 1. This is satisﬁed if we require that ˜εk(0; x) = 0 for all k = 1  2  . . .
and x ∈ X . We can interpret this requirement by noticing that the condition c(0) = 1 is simply a

requirement that c(0) = E(cid:2)ejX0(cid:3) = E [1] =(cid:82) µ(dx) be equal to 1. So we are essentially requiring

that we do not lose or add probability mass at each iteration of ACVI.
Performing ACVI can be quite similar to the conventional AVI. Suppose that we are given a dataset
Dn = {(Xi  Ri  X(cid:48)
i ∼ P π(·|Xi) and Ri ∼ Rπ(·|Xi). Given this dataset
and a CVF ˜V   we deﬁne the empirical Bellman operator as the following mapping:
∀ω ∈ R ∀i = 1  . . .   n.

( ˆ˜T π ˜V )(ω; Xi) (cid:44) ejωRi ˜V (γω; X(cid:48)
i) 

i)}n

i=1  with Xi ∼ µ  X(cid:48)

For any ﬁxed function ˜V and at any ﬁxed state Xi  with a r.v. Ai ∼ π(·|Xi)  we have

( ˆ˜T π ˜V )(ω; X) | X = Xi

ejωRi ˜V (γω; X(cid:48)

i) | X = Xi

E(cid:104)

(cid:105)

(cid:105)

= E(cid:104)

(cid:90)

= ˜R(ω; Xi)

P π(dy|Xi) ˜V (γω; y) = ( ˜T π ˜V )(ω; Xi).

This shows that the random process ( ˆ˜T π ˜V )(ω; Xi) is an unbiased estimate of ( ˜T π ˜V )(ω; Xi). In
other words  ( ˜T π ˜V )(ω; Xi) is the conditional mean of ( ˆ˜T π ˜V )(ω; Xi). Finding the conditional
mean of a r.v. is the regression problem (i.e.  estimating m(x) = E [Y |X = x] by ˆm(x) using a
dataset of {(Xi  Yi)}n
i=1)  which has been extensively studied in the statistics and machine learning
literature [Györﬁ et al.  2002  Wasserman  2007  Hastie et al.  2009  Goodfellow et al.  2016]. A
powerful estimator that generalizes well across states and ω allows us to approximately perform one
step of ACVI.
One approach to ﬁnding a regression estimator is to solve an empirical risk minimization problem:

(cid:90) (cid:12)(cid:12)(cid:12) ˜V (ω; Xi) − ejωRi ˜Vk(γω; X(cid:48)

i)

(cid:12)(cid:12)(cid:12)2

n(cid:88)

i=1

˜Vk+1 ← argmin
˜V ∈F

1
n

w(ω)dω 

(15)

where F ⊂ V is a space of functions from R × X to C1  which can be represented by various types
of function approximators (including decision trees  kernel-based ones  and neural networks)  and
w : R (cid:55)→ R is a weighting function that indicates the importance of different frequencies ω. This is
similar to the usual Fitted Value Iteration procedure [Ernst et al.  2005  Munos and Szepesvári  2008 
Farahmand et al.  2009  Silver et al.  2016  Tosatto et al.  2017  Chen and Jiang  2019]  which solves

Vk+1 ← argmin
V ∈F

1
n

|V (Xi) − (Ri + γVk(X(cid:48)

i))|2  

(16)

with appropriately chosen function space F (and similar for Fitted Q Iteration and the action-value
function Q). One clear difference between (15) and (16) is that we have an integral over the frequency
domain in the former. This one-dimensional integral can be numerically integrated  for example 
by discretizing the low-frequency domain [−b  +b] (with b > 0) with resolution εint. This incurs
some controlled numerical error that is a function of εint. For some function approximators  such as
a decision tree  one might be able to calculate the integral more efﬁciently by beneﬁtting from the
constancy of values within a leaf.
The quality of approximating ˜T π ˜Vk by ˜Vk+1 determines the error ˜εk. The error depends on the
regression method being used  as well as the number of data points available  capacity and express-
ibility of the function space F  etc. We do not analyze this regression problem in this paper. We are
nevertheless interested in knowing whether one can hope to have a small error with a reasonably
selected F. Two relevant questions are whether one can approximate ˜T π ˜Vk within F well enough
(function approximation error)  and whether F has enough regularity to allow reasonable convergence
rate for the estimation error. We study these questions in detail in the appendices of the extended
version of the paper. We only brieﬂy mention that if the reward distribution is smooth in a certain
sense  a band-limited function class Fb = { ˜V : R×X → C1 : ˜V (0; x) = 1  ˜V (ω; x) = 0 ∀|ω| > b}

n(cid:88)

i=1

7

provides an approximation error that goes to zero as the bandwidth b increases. More speciﬁcally 
the d∞ 1 distance-based norm of the approximation error behaves like O(b
1+β ) with β being the
smoothness parameter. Furthermore  if the ﬁrst s absolute moments of the reward distribution are ﬁ-
nite  the CVF ˜V (·; x) belongs to the smoothness class C s([−b  b])∩Fb. This leads to a well-behaving
covering number  which can be used to obtain a convergence rate for the estimation error. A side
beneﬁt of working with a band-limited function space is that the integral in (15) can be converted to a
deﬁnite integral  which is easier to integrate numerically.
Next we analyze how these errors  however generated  affect the quality of the outcome ˜VK after
performing K steps of ACVI.

− 1

5 Error propagation analysis

We analyze how the errors in the ACVI procedure (14) propagate throughout the iterations and affect
the quality of the outcome CVF ˜VK  where K is the number of times the iteration is performed.
We skip all the intermediate steps required to prove the main result of this section. They can be found
in the same section of the extended version of the paper.
Theorem 2. Consider the ACVI procedure (14) after K ≥ 1 iterations. Assume that ˜εk(0; x) = 0
for all x ∈ X and k = 1  . . .   K + 1. We have

d∞ p( ˜VK+1  ˜V π) ≤ K(cid:88)
d1 p( ˜VK+1  ˜V π) ≤ K(cid:88)

i=0

(γp)i (cid:107)˜εK+1−i(cid:107)∞ p + (γp)Kd∞ p( ˜R  ˜V π) 

(γp−1)i (cid:107)˜εK+1−i(cid:107)1 p + (γp−1)Kd1 p( ˜R  ˜V π).

(p ≥ 1)

(p > 1)

i=0

This result shows how the errors ˜εk in the ACVI procedure propagate throughout iterations and
affect the quality of the approximation of ˜V π by ˜VK+1. The error is measured according to the
distances d1 p and d∞ p. The upper bounds show that errors in the earlier iterations are geometrically
decayed. This entails that if the resources are limited  it is better to ensure the smallness of errors
in later iterations. This phenomenon is similar to what we have observed in the conventional value
iteration [Farahmand et al.  2010].
As discussed in Section 4  the condition that d∞ p( ˜R  ˜V π) is ﬁnite might be very restrictive for p > 2
and even for p = 2  it might hold only in special problems. But the ﬁniteness of d∞ 1 requires mild
conditions. For the ﬁniteness of d∞ 1( ˜R  ˜V π) in the upper bound  the ﬁniteness of the ﬁrst absolute
moment of the reward function is sufﬁcient  as discussed after (13). For the ﬁniteness of (cid:107)˜εi(cid:107)∞ 1
terms  it is sufﬁcient that ˜εi(0; x) = 0 and that its ﬁrst derivative w.r.t. ω is bounded for all states
x ∈ X   i.e.  |˜ε(1)(ω; x)| < ∞. Based on these  so from now on we focus on p = 1.

6 From error in frequency domain to error in probability distributions

Theorem 2 in the previous section relates the errors at each iteration of ACVI to the quality of the
obtained approximation of ˜V π. The error is measured according to the metrics d1 p and d∞ p. These
are metrics in the frequency domain. What does having a small error in the frequency domain imply
about the quality of approximating the distribution of returns ¯V π?
From Levy’s continuity theorem we know that the pointwise convergence of CF implies the conver-
gence in distribution of their corresponding distributions. This suggest that we could deﬁne the error
in the frequency domain

(cid:12)(cid:12)(cid:12) ˜V (ω; x) − ˜V π(ω; x)

(cid:12)(cid:12)(cid:12) .

dunif( ˜V   ˜V π) = sup
x∈X

sup
ω∈R

Nevertheless  we did not deﬁne the distance this way because the Bellman operator would not be a
contraction w.r.t. to it. So a valid question is whether  or in what sense  the smallness of d∞ p( ˜V   ˜V π)
implies anything about the closeness of their corresponding probability distribution functions ¯V

8

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)  

Ω  and Fp(Ω) = (cid:8) f ∈ Cp(Ω) : (cid:107)f (k)(cid:107)∞ ≤ 1  0 ≤ k ≤ p(cid:9). For two probability distributions

and ¯V π? In this section we show that such a relation indeed exists. We relate d∞ p and d1 p to the
p-smooth Wasserstein distance of the probability distribution functions [Arras et al.  2017].
Deﬁnition 1. Let p ≥ 1  Cp(Ω) be the space of p-times continuous differentiable functions on domain
µ1  µ2 ∈ M(Ω)  the p-smooth Wasserstein distance is deﬁned as

WCp (µ1  µ2) = sup
f∈Fp(Ω)

f (x) (dµ1(x) − dµ2(x))

Remark 1. Note that the conventional 1-Wasserstein distance is deﬁned as
f (x) (dµ1(x) − dµ2(x))

where Lip1 is the space of 1-Lipschitz functions. As(cid:13)(cid:13)f (1)(cid:13)(cid:13)∞ ≤ 1 implies 1-Lipschitz functions  but

W1(µ1  µ2) = sup

f∈Lip1(Ω)

not necessarily vice versa  WC1(µ1  µ2) ≤ W1(µ1  µ2).
Let us also deﬁne the p-smooth Wasserstein between ¯V1 and ¯V2 as follows:
2 (·; x)).

WCp ( ¯V1(·; x)  ¯V π

WCp ( ¯V1  ¯V π

2 ) (cid:44) sup
x∈X

This is the maximum over states x ∈ X of the value of the p-smooth Wasserstein between the
distribution of return according to the probability distributions ¯V1(·; x) and ¯V2(·; x).
Theorem 3. Consider the ACVI procedure (14) after K ≥ 1 iterations. Assume that ˜εk(0; x) = 0
for all x ∈ X and k = 1  . . .   K + 1. Furthermore  assume that the immediate reward distribution
Rπ(·|x) is Rmax-bounded. We then have

(cid:35)

√
2√
WC2( ¯VK+1  ¯V π) ≤ 2
π

(cid:115)

(cid:34) K(cid:88)

i=0

Rmax
1 − γ

γi (cid:107)˜εK+1−i(cid:107)∞ 1 +

2γK
1 − γ

Rmax

.

This upper bound can be simpliﬁed if we are willing to provide a uniform over iterations upper bound
on (cid:107)˜εK+1−i(cid:107)∞ 1. In that case  we have
√
√
WC2 ( ¯VK+1  ¯V π) ≤ 2
π(1 − γ)3/2

(cid:107)˜εi(cid:107)∞ 1 + 2γKRmax

i=1 ... K+1

2Rmax

(cid:21)

.

(cid:20)

max

We note that the 2-smooth Wasserstein distance WC2  which is an integral probability metric [Müller 
1997]  is only one of the many distances between probability distributions [Gibbs and Su  2002]. The
choice of the right probability distance most likely depends on the performance measure we would
like the policy to optimize. Studying this further is an interesting topic of future research.

7 Conclusion

This paper laid the groundwork for a new class of distributional RL algorithms. We have shown
that one might represent the uncertainty about the return in the frequency domain  and such a
representation (called Characteristic Value Function) enjoys properties such as satisfying a Bellman
equation and having a contractive Bellman operator. This in turn allows us to compute the CVF by
an iterative method called Characteristic Value Iteration. We also showed the effect of errors in the
iterative procedure  and provided error propagation results  in both the frequency domain and the
probability distribution space.
This paper is only the ﬁrst step towards understanding CVFs and their properties. Among remaining
questions is how to perform the regression step (15) of ACVI properly and efﬁciently. Speciﬁcally 
how should we set the weighting function w(ω) in order to achieve accurate CVF in frequencies that
are relevant for the tasks we want to solve. Studying other distances between CFs and their properties
is another interesting research directions. This work only focused on the policy evaluation problem 
so another obvious direction is designing risk-aware policy optimization algorithms based on CVF.
Finally  empirically evaluating this approach for return uncertainty representation may lead to better
understanding of its strengths and weaknesses.

9

Acknowledgments

I would like to thank the anonymous reviewers for their helpful feedback  particularly Reviewer #4. I
acknowledge the funding from the Canada CIFAR AI Chairs program.

References
Benjamin Arras  Guillaume Mijoule  Guillaume Poly  and Yvik Swan. A new approach to the Stein-
Tikhomirov method: with applications to the second Wiener chaos and Dickman convergence.
arXiv:1605.06819v2  2017. 9

Gabriel Barth-Maron  Matthew W. Hoffman  David Budden  Will Dabney  Dan Horgan  Dhruva
TB  Alistair Muldal  Nicolas Heess  and Timothy Lillicrap. Distributional policy gradients. In
International Conference on Learning Representations (ICLR)  2018. 1

Marc G. Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning (ICML)  2017.
1

Dimitri P. Bertsekas. Abstract dynamic programming. Athena Scientiﬁc Belmont  2013. 2

Dimitri P. Bertsekas and Steven E. Shreve. Stochastic Optimal Control: The Discrete-Time Case.

Academic Press  1978. 2

José Antonio Carrillo and Giuseppe Toscani. Contractive probability metrics and asymptotic behavior
of dissipative kinetic equations (notes of the Porto Ercole school  june 2006). Riv. Mat. Univ.
Parma  7(6):75–198  2007. 5

Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In

Proceedings of the 36th International Conference on Machine Learning (ICML)  2019. 6  7

Yinlam Chow  Mohammad Ghavamzadeh  Lucas Janson  and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. Journal of Machine Learning Research
(JMLR)  18(167):1–51  2018. 1

Will Dabney  Georg Ostrovski  David Silver  and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning (ICML)  2018. 1

Yaakov Engel  Shie Mannor  and Ron Meir. Reinforcement learning with Gaussian processes. In
Proceedings of the 22nd International Conference on Machine learning (ICML)  pages 201–208.
ACM  2005. 1

Damien Ernst  Pierre Geurts  and Louis Wehenkel. Tree-based batch mode reinforcement learning.

Journal of Machine Learning Research (JMLR)  6:503–556  2005. 6  7

Amir-massoud Farahmand  Mohammad Ghavamzadeh  Csaba Szepesvári  and Shie Mannor. Reg-
ularized ﬁtted Q-iteration for planning in continuous-space Markovian Decision Problems. In
Proceedings of American Control Conference (ACC)  pages 725–730  June 2009. 6  7

Amir-massoud Farahmand  Rémi Munos  and Csaba Szepesvári. Error propagation for approximate
policy and value iteration. In J. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R. S. Zemel  and
A. Culotta  editors  Advances in Neural Information Processing Systems (NIPS - 23)  pages
568–576. 2010. 8

Andrey Feuerverger. An efﬁciency result for the empirical characteristic function in stationary

time-series models. Canadian Journal of Statistics  18(2):155–161  1990. 2

Andrey Feuerverger and Philip McDunnough. On some Fourier methods for inference. Journal of

the American Statistical Association  76(374):379–387  1981. 2

Andrey Feuerverger and Roman A. Mureika. The empirical characteristic function and its applications.

Annals of Statistics  5(1):88–97  01 1977. 2

10

Javier García and Fernando Fernández. A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research (JMLR)  16:1437–1480  2015. 1

Alison L. Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International

Statistical Review  70(3):419–435  2002. 9

Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016. 7
László Györﬁ  Michael Kohler  Adam Krzy˙zak  and Harro Walk. A Distribution-Free Theory of

Nonparametric Regression. Springer Verlag  New York  2002. 7

Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The Elements of Statistical Learning: Data

Mining  Inference  and Prediction (2nd edition). Springer  2009. 7

John K. Hunter and Bruno Nachtergaele. Applied analysis. World Scientiﬁc Publishing Company 

2001. 5

John L. Knight and Jun Yu. Empirical characteristic function in time series estimation. Econometric

Theory  18(3):691–721  2002. 2

Clare Lyle  Pablo Samuel Castro  and Marc G. Bellemare. A comparative analysis of expected and
distributional reinforcement learning. In Proceedings of the 31st AAAI Conference on Artiﬁcial
Intelligence  2019. 1

Tetsuro Morimura  Masashi Sugiyama  Hisashi Kashima  Hirotaka Hachiya  and Toshiyuki Tanaka.
Nonparametric return distribution approximation for reinforcement learning. In Proceedings of the
27th International Conference on Machine Learning (ICML)  2010a. 1

Tetsuro Morimura  Masashi Sugiyama  Hisashi Kashima  Hirotaka Hachiya  and Toshiyuki Tanaka.
In Proceedings of the 26th

Parametric return density estimation for reinforcement learning.
Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 368–375  2010b. 1

Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in

Applied Probability  29(2):429–443  1997. 9

Rémi Munos and Csaba Szepesvári. Finite-time bounds for ﬁtted value iteration. Journal of Machine

Learning Research (JMLR)  9:815–857  2008. 6  7

L.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive mdps. In

Advances in Neural Information Processing Systems (NIPS - 26). 2013. 1

Mark Rowland  Marc G. Bellemare  Will Dabney  Rémi Munos  and Yee Whye Teh. An analysis
of categorical distributional reinforcement learning. In Proceedings of the 21st International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2018. 1  3

Mark Rowland  Robert Dadashi  Saurabh Kumar  Rémi Munos  Marc G. Bellemare  and Will
Dabney. Statistics and samples in distributional reinforcement learning. In Proceedings of the 36th
International Conference on Machine Learning (ICML)  2019. 1

David Silver  Aja Huang  Chris J. Maddison  Arthur Guez  Laurent Sifre  George van den Driessche 
Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  Sander Dieleman 
Dominik Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy Lillicrap  Madeleine
Leach  Koray Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mastering the game of Go with
deep neural networks and tree search. Nature  529(7587):484–489  01 2016. 6  7

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press 

second edition  2019. 1

Csaba Szepesvári. Algorithms for Reinforcement Learning. Morgan Claypool Publishers  2010. 2

Aviv Tamar  Dotan Di Castro  and Shie Mannor. Policy gradients with variance related risk criteria.

In Proceedings of the 29th International Conference on Machine Learning (ICML)  2012. 1

Samuele Tosatto  Matteo Pirotta  Carlo D’Eramo  and Marcello Restelli. Boosted ﬁtted Q-iteration.

In Proceedings of the 34th International Conference on Machine Learning (ICML)  2017. 6  7

11

Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media 

2008. 5

Larry Wasserman. All of Nonparametric Statistics. Springer  2007. 7

David Williams. Probability with Martingales. Cambridge University Press  1991. 3

Jun Yu. Empirical characteristic function estimation and its applications. Econometric reviews  23(2):

93–123  2004. 2

12

,Amir-massoud Farahmand