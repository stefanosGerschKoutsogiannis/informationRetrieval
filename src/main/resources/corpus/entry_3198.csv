2016,Leveraging Sparsity for Efficient Submodular Data Summarization,The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement  image retrieval  and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems  so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary—solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.,Leveraging Sparsity for Efﬁcient
Submodular Data Summarization

Erik M. Lindgren  Shanshan Wu  Alexandros G. Dimakis

erikml@utexas.edu  shanshan@utexas.edu  dimakis@austin.utexas.edu

The University of Texas at Austin

Department of Electrical and Computer Engineering

Abstract

The facility location problem is widely used for summarizing large datasets and
has additional applications in sensor placement  image retrieval  and clustering.
One difﬁculty of this problem is that submodular optimization algorithms require
the calculation of pairwise beneﬁts for all items in the dataset. This is infeasible for
large problems  so recent work proposed to only calculate nearest neighbor beneﬁts.
One limitation is that several strong assumptions were invoked to obtain provable
approximation guarantees. In this paper we establish that these extra assumptions
are not necessary—solving the sparsiﬁed problem will be almost optimal under
the standard assumptions of the problem. We then analyze a different method of
sparsiﬁcation that is a better model for methods such as Locality Sensitive Hashing
to accelerate the nearest neighbor computations and extend the use of the problem
to a broader family of similarities. We validate our approach by demonstrating that
it rapidly generates interpretable summaries.

1

Introduction

In this paper we study the facility location problem: we are given sets V of size n  I of size m and a
beneﬁt matrix of nonnegative numbers C 2 RI⇥V   where Civ describes the beneﬁt that element i
receives from element v. Our goal is to select a small set A of k columns in this matrix. Once we
have chosen A  element i will get a beneﬁt equal to the best choice out of the available columns 
maxv2A Civ. The total reward is the sum of the row rewards  so the optimal choice of columns is the
solution of:
(1)

Civ.

arg max

{A✓V :|A|k}Xi2I

max
v2A

A natural application of this problem is in ﬁnding a small set of representative images in a big dataset 
where Civ represents the similarity between images i and v. The problem is to select k images that
provide a good coverage of the full dataset  since each one has a close representative in the chosen
set.
Throughout this paper we follow the nomenclature common to the submodular optimization for
machine learning literature. This problem is also known as the maximization version of the k-medians
problem or the submodular facility location problem. A number of recent works have used this
problem for selecting subsets of documents or images from a larger corpus [27  39]  to identify
locations to monitor in order to quickly identify important events in sensor or blog networks [24  26] 
as well as clustering applications [23  34].
We can naturally interpret Problem 1 as a maximization of a set function F (A) which takes as an
input the selected set of columns and returns the total reward of that set. Formally  let F (;) = 0 and

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

for all other sets A ✓ V deﬁne

F (A) =Xi2I

Civ.

max
v2A

(2)

The set function F is submodular  since for all j 2 V and sets A ✓ B ✓ V \ {j}  we have
F (A [{ j})  F (A)  F (B [{ j})  F (B)  that is  the gain of an element is diminishes as we add
elements. Since the entries of C are nonnegative  F is monotone  since for all A ✓ B ✓ V   we have
F (A)  F (B). We also have F normalized  since F (;) = 0.
The facility location problem is NP-Hard  so we consider approximation algorithms. Like all mono-
tone and normalized submodular functions  the greedy algorithm guarantees a (1  1/e)-factor
approximation to the optimal solution [35]. The greedy algorithm starts with the empty set  then for
k iterations adds the element with the largest reward. This approximation is the best possible—the
maximum coverage problem is an instance of the submodular facility location problem  which was
shown to be NP-Hard to optimize within a factor of 1  1/e + " for all "> 0 [13].
The problem is that the greedy algorithm has super-quadratic running time ⇥(nmk) and in many
datasets n and m can be in the millions. For this reason  several recent papers have focused on
accelerating the greedy algorithm. In [26]  the authors point out that if the beneﬁt matrix is sparse 
this can dramatically speed up the computation time. Unfortunately  in many problems of interest 
data similarities or rewards are not sparse. Wei et al. [40] proposed to ﬁrst sparsify the beneﬁt matrix
and then run the greedy algorithm on this new sparse matrix. In particular  [40] considers t-nearest
neighbor sparsiﬁcation  i.e.  keeping for each row the t largest entries and zeroing out the rest. Using
this technique they demonstrated an impressive 80-fold speedup over the greedy algorithm with little
loss in solution quality. One limitation of their theoretical analysis was the limited setting under
which provable approximation guarantees were established.
Our Contributions: Inspired by the work of Wei et al. [40] we improve the theoretical analysis of
the approximation error induced by sparsiﬁcation. Speciﬁcally  the previous analysis assumes that
the input came from a probability distribution where the preferences of each element of i 2 I are
independently chosen uniformly at random. For this distribution  when k =⌦( n)  they establish that
the sparsity can be taken to be O(log n) and running the greedy algorithm on the sparsiﬁed problem
will guarantee a constant factor approximation with high probability. We improve the analysis in the
following ways:

on the input besides nonnegativity of the beneﬁt matrix.

low as O(1) while guaranteeing a constant factor approximation.

• We prove guarantees for all values of k and our guarantees do not require any assumptions
• In the case where k =⌦( n)  we show that it is possible to take the sparsity of each row as
• Unlike previous work  our analysis does not require the use of any particular algorithm and
• We establish a lower bound which shows that our approximation guarantees are tight up to

can be integrated to many algorithms for solving facility location problems.

log factors  for all desired approximation factors.

In addition to the above results we propose a novel algorithm that uses a threshold based sparsiﬁcation
where we keep matrix elements that are above a set value threshold. This type of sparsiﬁcation is
easier to efﬁciently implement using nearest neighbor methods. For this method of sparsiﬁcation  we
obtain worst case guarantees and a lower bound that matches up to constant factors. We also obtain a
data dependent guarantee which helps explain why our algorithm empirically performs better than
the worst case.
Further  we propose the use of Locality Sensitive Hashing (LSH) and random walk methods to
accelerate approximate nearest neighbor computations. Speciﬁcally  we use two types of similarity
metrics: inner products and personalized PageRank (PPR). We propose the use of fast approximations
for these metrics and empirically show that they dramatically improve running times. LSH functions
are well-known but  to the best of our knowledge  this is the ﬁrst time they have been used to accelerate
facility location problems. Furthermore  we utilize personalized PageRank as the similarity between
vertices on a graph. Random walks can quickly approximate this similarity and we demonstrate that
it yields highly interpretable results for real datasets.

2

2 Related Work

The use of a sparsiﬁed proxy function was shown by Wei et al. to also be useful for ﬁnding a
subset for training nearest neighbor classiﬁers [41]. Further  they also show a connection of nearest
neighbor classiﬁers to the facility location function. The facility location function was also used by
Mirzasoleiman et al. as part of a summarization objective function in [32]  where they present a
summarization algorithm that is able to handle a variety of constraints.
The stochastic greedy algorithm was shown to get a 1  1/e  " approximation with runtime
" )  which has no dependance on k [33]. It works by choosing a sample set from V of size
O(nm log 1
" each iteration and adding to the current set the element of the sample set with the largest gain.
k log 1
n
Also  there are several related algorithms for the streaming setting [5] and distributed setting [6  25 
31  34]. Since the objective function is deﬁned over the entire dataset  optimizing the submodular
facility location function becomes more complicated in these memory limited settings. Often the
function is estimated by considering a randomly chosen subset from the set I.

2.1 Beneﬁts Functions and Nearest Neighbor Methods

.

2  dot product sim(x  y) = xT y  or cosine similarity sim(x  y) = xT y
kxkkyk

For many problems  the elements V and I are vectors in some feature space where the beneﬁt
matrix is deﬁned by some similarity function sim. For example  in Rd we may use the RBF kernel
sim(x  y) = ekxyk2
There has been decades of research on nearest neighbor search in geometric spaces. If the vectors are
low dimensional  then classical techniques such as kd-trees [7] work well and are exact. However
it has been observed that as the dimensions grow that the runtime of all known exact methods does
little better than a linear scan over the dataset.
As a compromise  researchers have started to work on approximate nearest neighbor methods  one of
the most successful approaches being locality sensitive hashing [15  20]. LSH uses a hash function
that hashes together items that are close. Locality sensitive hash functions exist for a variety of metrics
and similarities such as Euclidean [11]  cosine similarity [3  9]  and dot product [36  38]. Nearest
neighbor methods other than LSH that have been shown to work for machine learning problems
include [8  10]. Additionally  see [14] for efﬁcient and exact GPU methods.
An alternative to vector functions is to use similarities and beneﬁts deﬁned from graph structures. For
instance  we can use the personalized PageRank of vertices in a graph to deﬁne the beneﬁt matrix
[37]. The personalized PageRank is similar to the classic PageRank  except the random jumps  rather
than going to anywhere in the graph  go back to the users “home” vertex. This can be used as a value
of “reputation” or “inﬂuence” between vertices in a graph [17].
There are a variety of algorithms for ﬁnding the vertices with a large PageRank personalized to some
vertex. One popular one is the random walk method. If ⇡i is the personalized PageRank vector to
some vertex i  then ⇡i(v) is the same as the probability that a random walk of geometric length
starting from i ends on a vertex v (where the parameter of the geometric distribution is deﬁned by the
probability of jumping back to i) [4]. Using this approach  we can quickly estimate all elements in
the beneﬁt matrix greater than some value ⌧.

3 Guarantees for t-Nearest Neighbor Sparsiﬁcation

We associate a bipartite support graph G = (V  I  E) by having an edge between v 2 V and i 2 I
whenever Cij > 0. If the support graph is sparse  we can use the graph to calculate the gain of an
element much more efﬁciently  since we only need to consider the neighbors of the element versus
the entire set I. If the average degree of a vertex i 2 I is t  (and we use a cache for the current best
value of an element i) then we can execute greedy in time O(mtk). See Algorithm 1 in the Appendix
for pseudocode. If the sparsity t is much smaller than the size of V   the runtime is greatly improved.
However  the instance we wish to optimize may not be sparse. One idea is to sparsify the original
matrix by only keeping the values in the beneﬁt matrix C that are t-nearest neighbors  which
was considered in [40]. That is  for every element i in I  we only keep the top t elements of
Ci1  Ci2  . . .   Cin and set the rest equal to zero. This leads to a matrix with mt nonzero elements. We

3

1

↵k log m

1+↵ from the value of the optimal solution.

then want the solution from optimizing the sparse problem to be close to the value of the optimal
solution in the original objective function F .
Our main theorem is that we can set the sparsity parameter t to be O( n
↵k )—which is a
signiﬁcant improvement for large enough k—while still having the solution to the sparsiﬁed problem
be at most a factor of
Theorem 1. Let Ot be the optimal solution to an instance of the submodular facility location problem
with a beneﬁt matrix that was sparsiﬁed with t-nearest neighbor. For any t  t⇤(↵) = O( n
↵k ) 
we have F (Ot)  1
Proof Sketch. For the value of t chosen  there exists a set  of size ↵k such that every element of
I has a neighbor in the t-nearest neighbor graph; this is proven using the probabilistic method. By
appending  to the optimal solution and using the monotonicity of F   we can move to the sparsiﬁed
function  since no element of I would prefer an element that was zeroed out in the sparsiﬁed matrix
as one of their top t most beneﬁcial elements is present in the set . The optimal solution appended
with  is a set of size (1 + ↵)k. We then bound the amount that the optimal value of a submodular
function can increase by when adding ↵k elements. See the appendix for the complete proof.

1+↵ OPT.

↵k log n

⇢

"k log m

k   then this is equivalent to taking ↵ very large and we have some guarantee of optimality.

Note that Theorem 1 is agnostic to the algorithm used to optimize the sparsiﬁed function  and so if we
use a ⇢-approximation algorithm  then we are at most a factor of
1+↵ from the optimal solution. Later
this section we will utilize this to design a subquadratic algorithm for optimizing facility location
problems as long as we can quickly compute t-nearest neighbors and k is large enough.
If m = O(n) and k =⌦( n)  we can achieve a constant factor approximation even when taking
the sparsity parameter as low as t = O(1)  which means that the beneﬁt matrix C has only O(n)
nonzero entries. Also note that the only assumption we need is that the beneﬁts between elements are
nonnegative. When k =⌦( n)  previous work was only able to take t = O(log n) and required the
beneﬁt matrix to come from a probability distribution [40].
Our guarantee has two regimes depending on the value of ↵. If we want the optimal solution to the
sparsiﬁed function to be a 1 " factor from the optimal solution to the original function  we have that
"k ) sufﬁces. Conversely  if we want to take the sparsity t to be much smaller than
t⇤(") = O( n
n
k log m
In the proof of Theorem 1  the only time we utilize the value of t is to show that there exists a small
set  that covers the entire set I in the t-nearest neighbor graph. Real datasets often contain a covering
set of size ↵k for t much smaller than O( n
↵k ). This observation yields the following corollary.
Corollary 2. If after sparsifying a problem instance there exists a covering set of size ↵k in the
t-nearest neighbor graph  then the optimal solution Ot of the sparsiﬁed problem satisﬁes F (Ot) 
1+↵ OPT.
In the datasets we consider in our experiments of roughly 7000 items  we have covering sets with
only 25 elements for t = 75  and a covering set of size 10 for t = 150. The size of covering set was
upper bounded by using the greedy set cover algorithm. In Figure 2 in the appendix  we see how the
size of the covering set changes with the choice of the number of neighbors chosen t.
It would be desirable to take the sparsity parameter t lower than the value dictated by t⇤(↵). As
demonstrated by the following lower bound  is not possible to take the sparsity signiﬁcantly lower
than 1
↵
Proposition 3. Suppose we take

1+↵ approximation in the worst case.

k and still have a

↵k log m

n

1

1

t = max⇢ 1

2↵

 

1

1 + ↵ n  1

.

k
1+↵2/k OPT.

There exists a family of inputs such that we have F (Ot) 
The example we create to show this has the property that in the t-nearest neighbor graph  the set 
needs ↵k elements to cover every element of I. We plant a much smaller covering set that is very
close in value to  but is hidden after sparsiﬁcation. We then embed a modular function within the
facility location objective. With knowledge of the small covering set  an optimal solver can take

1

4

advantage of this modular function  while the sparsiﬁed solution would prefer to ﬁrst choose the set 
before considering the modular function. See the appendix for full details.
Sparsiﬁcation integrates well with the stochastic greedy algorithm [33]. By taking t  t⇤("/2) and
running stochastic greedy with sample sets of size n
"   we get a 1  1/e  " approximation in
expectation that runs in expected time O( nm
"k ). If we can quickly sparsify the problem
and k is large enough  for example n1/3  this is subquadratic. The following proposition shows a high
probability guarantee on the runtime of this algorithm and is proven in the appendix.
Proposition 4. When m = O(n)  the stochastic greedy algorithm [33] with set sizes of
size n
"   combined with sparsiﬁcation with sparsity parameter t  will terminate in time
"k )  this algorithm
O(n log 1
has a 1  1/e  " approximation in expectation.
4 Guarantees for threshold-based Sparsiﬁcation

" max{t  log n}) with high probability. When t  t⇤("/2) = O( n

k ln 2
" log m

"k log m

"k log 1

k log 2

Rather than t-nearest neighbor sparsiﬁcation  we now consider using ⌧-threshold sparsiﬁcation  where
we zero-out all entries that have value below a threshold ⌧. Recall the deﬁnition of a locality sensitive
hash.
Deﬁnition. H is a (⌧  K⌧  p  q )-locality sensitive hash family if for x  y satisfying sim(x  y)  ⌧ we
have Ph2H(h(x) = h(y))  p and if x  y satisfy sim(x  y)  K⌧ we have Ph2H(h(x) = h(y)) 
q.

We see that ⌧-threshold sparsiﬁcation is a better model than t-nearest neighbors for LSH  as for
K = 1 it is a noisy ⌧-sparsiﬁcation and for non-adversarial datasets it is a reasonable approximation
of a ⌧-sparsiﬁcation method. Note that due to the approximation constant K  we do not have an a
priori guarantee on the runtime of arbitrary datasets. However we would expect in practice that we
would only see a few elements with threshold above the value ⌧. See [2] for a discussion on this.
One issue is that we do not know how to choose the threshold ⌧. We can sample elements of the
beneﬁt matrix C to estimate how sparse the threshold graph will be for a given threshold ⌧. Assuming
the values of C are in general position1  by using the Dvoretzky-Kiefer-Wolfowitz-Massart Inequality
[12  28] we can bound the number of samples needed to choose a threshold that achieves a desired
sparsiﬁcation level.
We establish the following data-dependent bound on the difference in the optimal solutions of the
⌧-threshold sparsiﬁed function and the original function. We denote the set of vertices adjacent to S
in the ⌧-threshold graph with N (S).
Theorem 5. Let O⌧ be the optimal solution to an instance of the facility location problem with a
beneﬁt matrix that was sparsiﬁed using a ⌧-threshold. Assume there exists a set S of size k such that
in the ⌧-threshold graph we have the neighborhood of S satisfying |N (S)| µn. Then we have

F (O⌧ ) ✓1 +

1

µ◆1

OPT.

For the datasets we consider  we see that we can keep a 0.01  0.001 fraction of the elements of C
while still having a small set S with a neighborhood N (S) that satisﬁed |N (S)| 0.3n. In Figure 3
in the appendix  we plot the relationship between the number of edges in the ⌧-threshold graph and
the number of coverable element by a a set of small size  as estimated by the greedy algorithm for
max-cover.
Additionally  we have worst case dependency on the number of edges in the ⌧-threshold graph and the
approximation factor. The guarantees follow from applying Theorem 5 with the following Lemma.
Lemma 6. For k  c
2 2n2 edges has a set S of size k such that the
1
12c2
neighborhood N (S) satisﬁes

   any graph with 1

1By this we mean that the values of C are all unique  or at least only a few elements take any particular value.
We need this to hold since otherwise a threshold based sparsiﬁcation may exclusively return an empty graph or
the complete graph.

|N (S)| cn.

5

To get a matching lower bound  consider the case where the graph has two disjoint cliques  one of
size n and one of size (1  )n. Details are in the appendix.
5 Experiments

5.1 Summarizing Movies and Music from Ratings Data

We consider the problem of summarizing a large collection of movies. We ﬁrst need to create a feature
vector for each movie. Movies can be categorized by the people who like them  and so we create
our feature vectors from the MovieLens ratings data [16]. The MovieLens database has 20 million
ratings for 27 000 movies from 138 000 users. To do this  we perform low-rank matrix completion
and factorization on the ratings matrix [21  22] to get a matrix X = U V T   where X is the completed
ratings matrix  U is a matrix of feature vectors for each user and V is a matrix of feature vectors for
each movie. For movies i and j with vectors vi and vj  we set the beneﬁt function Cij = vT
i vj. We
do not use the normalized dot product (cosine similarity) because we want our summary movies to be
movies that were highly rated  and not normalizing makes highly rated movies increase the objective
function more.
We complete the ratings matrix using the MLlib library in Apache Spark [29] after removing all but
the top seven thousand most rated movies to remove noise from the data. We use locality sensitive
hashing to perform sparsiﬁcation; in particular we use the LSH in the FALCONN library for cosine
similarity [3] and the reduction from a cosine simiarlity hash to a dot product hash [36]. As a baseline
we consider sparsiﬁcation using a scan over the entire dataset  the stochastic greedy algorithm with
lazy evaluations[33]  and the greedy algorithm with lazy evaluations [30]. The number of elements
chosen was set to 40 and for the LSH method and stochastic greedy we average over ﬁve trials.
We then do a scan over the sparsity parameter t for the sparsiﬁcation methods and a scan over the
number of samples drawn each iteration for the stochastic greedy algorithm. The sparsiﬁed methods
use the (non-stochastic) lazy greedy algorithm as the base optimization algorithm  which we found
worked best for this particular problem2. In Figure 1(a) we see that the LSH method very quickly
approaches the greedy solution—it is almost identical in value just after a few seconds even though
the value of t is much less than t⇤("). The stochastic greedy method requires much more time to get
the same function value. Lazy greedy is not plotted  since it took over 500 seconds to ﬁnish.

(a) Fraction of Greedy Set Value vs. Runtime

0.99
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.90

Exact top-t
LSH top-t

Stochastic Greedy

0

25

50

75

100

125

150

Runtime (s)

(b) Fraction of Greedy Set Contained vs. Runtime
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

75

100

125

150

0

25

50

Runtime (s)

Figure 1: Results for the MovieLens dataset [16]. Figure (a) shows the function value as the runtime increases 
normalized by the value the greedy algorithm obtained. As can be seen our algorithm is within 99.9% of greedy
in less than 5 seconds. For this experiment  the greedy algorithm had a runtime of 512 seconds  so this is a 100x
speed up for a small penalty in performance. We also compare to the stochastic greedy algorithm [33]  which
needs 125 seconds to get equivalent performance  which is 25x slower.
Figure (b) shows the fraction of the set that was returned by each method that was common with the set returned
by greedy. We see that the approximate nearest neighbor method has 90% of its elements common with the
greedy set while being 50x faster than greedy  and using exact nearest neighbors can perfectly match the greedy
set while being 4x faster than greedy.

2When experimenting on very larger datasets  we found that runtime constraints can make it necessary to use

stochastic greedy as the base optimization algorithm

6

Table 1: A subset of the summarization outputted by our algorithm on the MovieLens dataset  plus the elements
who are represented by each representative with the largest dot product. Each group has a natural interpretation:
90’s slapstick comedies  80’s horror  cult classics  etc. Note that this was obtained with only a similarity matrix
obtained from ratings.

Happy Gilmore
Tommy Boy
Billy Madison
Dumb & Dumber
Ace Ventura Pet Detective
Road Trip
American Pie 2
Black Sheep

Nightmare on Elm Street
Friday the 13th
Halloween II
Nightmare on Elm Street 3
Child’s Play
Return of the Living Dead II
Friday the 13th 2
Puppet Master

Star Wars IV
Star Wars V
Raiders of the Lost Ark
Star Wars VI
Indiana Jones  Last Crusade
Terminator 2
The Terminator
Star Trek II

Shawshank Redemption
Schindler’s List
The Usual Suspects
Life Is Beautiful
Saving Private Ryan
American History X
The Dark Knight
Good Will Hunting

Pulp Fiction
Reservoir Dogs
American Beauty
A Clockwork Orange
Trainspotting
Memento
Old Boy
No Country for Old Men

Pride and Prejudice
Anne of Green Gables
Persuasion
Emma

The Notebook
P.S. I Love You
The Holiday
Remember Me
A Walk to Remembe Mostly Martha
The Proposal
The Vow
Life as We Know It

Desk Set
The Young Victoria
Mansﬁeld Park

The Godfather
The Godfather II
One Flew Over the Cuckoo’s Nest
Goodfellas
Apocalypse Now
Chinatown
12 Angry Men
Taxi Driver

A performance metric that can be better than the objective value is the fraction of elements returned
that are common with the greedy algorithm. We treat this as a proxy for the interpretability of
the results. We believe this metric is reasonable since we found the subset returned by the greedy
algorithm to be quite interpretable. We plot this metric against runtime in Figure 1b. We see that the
LSH method quickly gets to 90% of the elements in the greedy set while stochastic greedy takes
much longer to get to just 70% of the elements. The exact sparsiﬁcation method is able to completely
match the greedy solution at this point. One interesting feature is that the LSH method does not
go much higher than 90%. This may be due to the increased inaccuracy when looking at elements
with smaller dot products. We plot this metric against the number of exact and approximate nearest
neighbors t in Figure 4 in the appendix.
We include a subset of the summarization and for each representative a few elements who are
represented by this representative with the largest dot product in Table 1 to show the interpretability
of our results.

5.2 Finding Inﬂuential Actors and Actresses

For our second experiment  we consider how to ﬁnd a diverse subset of actors and actresses in a
collaboration network. We have an edge between an actor or actress if they collaborated in a movie
together  weighted by the number of collaborations. Data was obtained from [19] and an actor or
actress was only included if he or she was one of the top six in the cast billing. As a measure of
inﬂuence  we use personalized PageRank [37]. To quickly calculate the people with the largest
inﬂuence relative to someone  we used the random walk method[4].
We ﬁrst consider a small instance where we can see how well the sparsiﬁed approach works. We
build a graph based on the cast in the top thousand most rated movies. This graph has roughly 6000
vertices and 60 000 edges. We then calculate the entire PPR matrix using the power method. Note
that this is infeasible on larger graphs in terms of time and memory. Even on this moderate sized
graph it took six hours and takes up two gigabytes of space. We then compare the value of the greedy
algorithm using the entire PPR matrix with the sparsiﬁed algorithm using the matrix approximated
by Monte Carlo sampling using the two metrics mentioned in the previous section. We omit exact
nearest neighbor and stochastic greedy because it is not clear how it would work without having to
compute the entire PPR matrix. Instead we compare to an approach where we choose a sample from
I and calculate the PPR only on these elements using the power method. As mentioned in Section 2 
several algorithms utilize random sampling from I. We take k to be 50 for this instance. In Figure 5 in
the appendix we see that sparsiﬁcation performs drastically better in both function value and percent
of the greedy set contained for a given runtime.

7

We now scale up to a larger graph by taking the actors and actresses billed in the top six for the
twenty thousand most rated movies. This graph has 57 000 vertices and 400 000 edges. We would
not be able to compute the entire PPR matrix for this graph in a reasonable amount of time and even
if we could it would be a challenge to store the entire matrix in memory. However we can run the
sparsiﬁed algorithm in three hours using only 2 GB of memory  which could be improved further by
parallelizing the Monte Carlo approximation.
We run the greedy algorithm separately on the actors and actresses. For each we take the top twenty-
ﬁve and compare to the actors and actresses with the largest (non-personalized) PageRank. In Figure 2
of the appendix  we see that the PageRank output fails to capture the diversity in nationality of the
dataset  while the submodular facility location optimization returns actors and actresses from many
of the worlds ﬁlm industries.

Acknowledgements

This material is based upon work supported by the National Science Foundation Graduate Research
Fellowship under Grant No. DGE-1110007 as well as NSF Grants CCF 1344179  1344364  1407278 
1422549 and ARO YIP W911NF-14-1-0258.

References
[1] N. Alon and J. H. Spencer. The Probabilistic Method. Wiley  3rd edition  2008.

[2] A. Andoni.
on

Exact
Theory 

dows
exact-algorithms-from-approximation-algorithms-part-2/ (version: 2016-09-06).

2012.

algorithms

from approximation

Win-
https://windowsontheory.org/2012/04/17/

algorithms?

(part

2).

[3] A. Andoni  P. Indyk  T. Laarhoven  I. Razenshteyn  and L. Schmidt. Practical and optimal LSH for angular

distance. In NIPS  2015.

[4] K. Avrachenkov  N. Litvak  D. Nemirovsky  and N. Osipova. Monte Carlo methods in PageRank computa-

tion: When one iteration is sufﬁcient. SIAM Journal on Numerical Analysis  2007.

[5] A. Badanidiyuru  B. Mirzasoleiman  A. Karbasi  and A. Krause. Streaming submodular maximization:

Massive data summarization on the ﬂy. In KDD  2014.

[6] R. Barbosa  A. Ene  H. L. Nguyen  and J. Ward. The power of randomization: Distributed submodular

maximization on massive datasets. In ICML  2015.

[7] J. L. Bentley. Multidimensional binary search trees used for associative searching. Communications of the

ACM  1975.

[8] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbor. In ICML  2006.

[9] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC  2002.

[10] J. Chen  H.-R Fang  and Y. Saad. Fast approximate k–NN graph construction for high dimensional data via

recursive Lanczos bisection. JMLR  2009.

[11] M. Datar  N. Immorlica  P. Indyk  and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable

distributions. In Symposium on Computational Geometry  2004.

[12] A. Dvoretzky  J. Kiefer  and J. Wolfowitz. Asymptotic minimax character of the sample distribution
function and of the classical multinomial estimator. The Annals of Mathematical Statistics  pages 642–669 
1956.

[13] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM  1998.

[14] V. Garcia  E. Debreuve  F. Nielsen  and M. Barlaud. K-nearest neighbor search: Fast GPU-based imple-
mentations and application to high-dimensional feature matching. In International Conference on Image
Processing  2010.

[15] A. Gionis  P. Indyk  and R. Motwani. Similarity search in high dimensions via hashing. In VLDB  1999.

[16] GroupLens. MovieLens 20M dataset  2015. http://grouplens.org/datasets/movielens/20m/.

8

[17] P. Gupta  A. Goel  J. Lin  A. Sharma  D. Wang  and R. Zadeh. Wtf: The who to follow service at twitter. In

WWW  2013.

[18] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

Statistical Association  1963.

[19] IMDb. Alternative interfaces  2016. http://www.imdb.com/interfaces.
[20] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.

In STOC  1998.

[21] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimization. In

STOC  2013.

[22] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems. Computer 

2009.

[23] A. Krause and R. G. Gomes. Budgeted nonparametric learning from data streams. In ICML  2010.
[24] A. Krause  J. Leskovec  C. Guestrin  J. VanBriesen  and C. Faloutsos. Efﬁcient sensor placement optimiza-
tion for securing large water distribution networks. Journal of Water Resources Planning and Management 
2008.

[25] R. Kumar  B. Moseley  S. Vassilvitskii  and A. Vattani. Fast greedy algorithms in MapReduce and streaming.

ACM Transactions on Parallel Computing  2015.

[26] J. Leskovec  A. Krause  C. Guestrin  C. Faloutsos  J. VanBriesen  and N. Glance. Cost-effective outbreak

detection in networks. In KDD  2007.

[27] H. Lin and J. A. Bilmes. Learning mixtures of submodular shells with application to document summariza-

tion. In UAI  2012.

[28] P. Massart. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. The Annals of Probability 

pages 1269–1283  1990.

[29] X. Meng  J. Bradley  B. Yavuz  E. Sparks  S. Venkataraman  D. Liu  J. Freeman  D. B. Tsai  M. Amde 
S. Owen  D. Xin  R. Xin  M. J. Franklin  R. Zadeh  M. Zaharia  and A. Talwalkar. MLlib: Machine learning
in Apache Spark. JMLR  2016.

[30] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization

Techniques. Springer  1978.

[31] V. Mirrokni and M. Zadimoghaddam. Randomized composable core-sets for distributed submodular

maximization. STOC  2015.

[32] B. Mirzasoleiman  A. Badanidiyuru  and A. Karbasi. Fast constrained submodular maximization: Personal-

ized data summarization. In ICML  2016.

[33] B. Mirzasoleiman  A. Badanidiyuru  A. Karbasi  J. Vondrak  and A. Krause. Lazier than lazy greedy. In

AAAI  2015.

[34] B. Mirzasoleiman  A. Karbasi  R. Sarkar  and A. Krause. Distributed submodular maximization: Identifying

representative elements in massive data. In NIPS  2013.

[35] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions—-I. Mathematical Programming  1978.

[36] B. Neyshabur and N. Srebro. On symmetric and asymmetric LSHs for inner product search. In ICML 

2015.

[37] L. Page  S. Brin  R. Motwani  and T. Winograd. The PageRank citation ranking: Bringing order to the web.

Stanford Digital Libraries  1999.

[38] A. Shrivastava and P. Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search

(MIPS). In NIPS  2014.

[39] S. Tschiatschek  R. K. Iyer  H. Wei  and J. A. Bilmes. Learning mixtures of submodular functions for

image collection summarization. In NIPS  2014.

[40] K. Wei  R. Iyer  and J. Bilmes. Fast multi-stage submodular maximization. In ICML  2014.
[41] K. Wei  R. Iyer  and J. Bilmes. Submodularity in data subset selection and active learning. In ICML  2015.

9

,Marijn Stollenga
Wonmin Byeon
Marcus Liwicki
Erik Lindgren
Shanshan Wu
Alexandros Dimakis
David Reeb
Andreas Doerr
Sebastian Gerwinn
Barbara Rakitsch