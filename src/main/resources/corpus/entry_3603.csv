2016,Multi-step learning and underlying structure in statistical models,In multi-step learning  where a final learning task is accomplished via a sequence of intermediate learning tasks  the intuition is that successive steps or levels transform the initial data into representations more and more ``suited" to the final learning task. A related principle arises in transfer-learning where Baxter (2000) proposed a theoretical framework to study how learning multiple tasks transforms the inductive bias of a learner. The most widespread multi-step learning approach is semi-supervised learning with two steps: unsupervised  then supervised. Several authors (Castelli-Cover  1996; Balcan-Blum  2005; Niyogi  2008; Ben-David et al  2008; Urner et al  2011) have analyzed SSL  with Balcan-Blum (2005) proposing a version of the PAC learning framework augmented by a ``compatibility function" to link concept class and unlabeled data distribution. We propose to analyze SSL and other multi-step learning approaches  much in the spirit of Baxter's framework  by defining a learning problem generatively as a joint statistical model on $X \times Y$. This determines in a natural way the class of conditional distributions that are possible with each marginal  and amounts to an abstract form of compatibility function. It also allows to analyze both discrete and non-discrete settings. As tool for our analysis  we define a notion of $\gamma$-uniform shattering for statistical models. We use this to give conditions on the marginal and conditional models which imply an advantage for multi-step learning approaches. In particular  we recover a more general version of a result of Poggio et al (2012): under mild hypotheses a multi-step approach which learns features invariant under successive factors of a finite group of invariances has sample complexity requirements that are additive rather than multiplicative in the size of the subgroups.,Multi-step learning and

underlying structure in statistical models

Dept. of Mathematics and Statistics
Brain and Mind Research Institute

University of Ottawa

Maia Fraser

Ottawa  ON K1N 6N5  Canada

mfrase8@uottawa.ca

Abstract

In multi-step learning  where a ﬁnal learning task is accomplished via a sequence of
intermediate learning tasks  the intuition is that successive steps or levels transform
the initial data into representations more and more “suited" to the ﬁnal learning
task. A related principle arises in transfer-learning where Baxter (2000) proposed a
theoretical framework to study how learning multiple tasks transforms the inductive
bias of a learner. The most widespread multi-step learning approach is semi-
supervised learning with two steps: unsupervised  then supervised. Several authors
(Castelli-Cover  1996; Balcan-Blum  2005; Niyogi  2008; Ben-David et al  2008;
Urner et al  2011) have analyzed SSL  with Balcan-Blum (2005) proposing a
version of the PAC learning framework augmented by a “compatibility function"
to link concept class and unlabeled data distribution. We propose to analyze SSL
and other multi-step learning approaches  much in the spirit of Baxter’s framework 
by deﬁning a learning problem generatively as a joint statistical model on X ⇥ Y .
This determines in a natural way the class of conditional distributions that are
possible with each marginal  and amounts to an abstract form of compatibility
function. It also allows to analyze both discrete and non-discrete settings. As tool
for our analysis  we deﬁne a notion of -uniform shattering for statistical models.
We use this to give conditions on the marginal and conditional models which
imply an advantage for multi-step learning approaches. In particular  we recover a
more general version of a result of Poggio et al (2012): under mild hypotheses a
multi-step approach which learns features invariant under successive factors of a
ﬁnite group of invariances has sample complexity requirements that are additive
rather than multiplicative in the size of the subgroups.

Introduction

1
The classical PAC learning framework of Valiant (1984) considers a learning problem with unknown
true distribution p on X ⇥ Y   Y = {0  1} and ﬁxed concept class C consisting of (deterministic)
functions f : X ! Y . The aim of learning is to select a hypothesis h : X ! Y   say from C itself
(realizable case)  that best recovers f. More formally  the class C is said to be PAC learnable if there
is a learning algorithm that with high probability selects h 2C having arbitrarily low generalization
error for all possible distributions D on X. The distribution D governs both the sampling of points
z = (x  y) 2 X ⇥ Y by which the algorithm obtains a training sample and also the cumulation of
error over all x 2 X which gives the generalization error. A modiﬁcation of this model  together
with the notion of learnable with a model of probability (resp. decision rule) (Haussler  1989;
Kearns and Schapire  1994)  allows to treat non-deterministic functions f : X ! Y and the case
Y = [0  1] analogously. Polynomial dependence of the algorithms on sample size and reciprocals

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

of probability bounds is further required in both frameworks for efﬁcient learning. Not only do
these frameworks consider worst case error  in the sense of requiring the generalization error to be
small for arbitrary distributions D on X  they assume the same concept class C regardless of the true
underlying distribution D. In addition  choice of the hypothesis class is taken as part of the inductive
bias of the algorithm and not addressed.
Various  by now classic  measures of complexity of a hypothesis space (e.g.  VC dimension or
Rademacher complexity  see Mohri et al. (2012) for an overview) allow to prove upper bounds on
generalization error in the above setting  and distribution-speciﬁc variants of these such as annealed
VC-entropy (see Devroye et al. (1996)) or Rademacher averages (beginning with Koltchinskii (2001))
can be used to obtain more reﬁned upper bounds.
The widespread strategy of semi-supervised learning (SSL) is known not to ﬁt well into PAC-style
frameworks (Valiant  1984; Haussler  1989; Kearns and Schapire  1994). SSL algorithms perform a
ﬁrst step using unlabeled training data drawn from a distribution on X  followed by a second step
using labeled training data from a joint distribution on X ⇥ Y . This has been studied by several
authors (Balcan and Blum  2005; Ben-David et al.  2008; Urner et al.  2011; Niyogi  2013) following
the seminal work of Castelli and Cover (1996) comparing the value of unlabeled and labeled data.
One immediate observation is that without some tie between the possible marginals D on X and the
concept class C which records possible conditionals p(y|x)  there is no beneﬁt to unlabeled data: if D
can be arbitrary then it conveys no information about the true joint distribution that generated labeled
data. Within PAC-style frameworks  however  C and D are completely independent. Balcan and
Blum therefore proposed augmenting the PAC learning framework by the addition of a compatibility
function  : C⇥D! [0  1]  which records the amount of compatibility we believe each concept
from C to have with each D 2D   the class of “all" distributions on X. This function is required to be
learnable from D and is then used to reduce the concept class from C to a sub-class which will be
used for the subsequent (supervised) learning step. If  is a good compatible function this sub-class
should have lesser complexity than C (Balcan and Blum  2005). While PAC-style frameworks in
essence allow the true joint distribution to be anything in C⇥D   the existence of a good compatibility
function in the sense of Balcan and Blum (2005) implicitly assumes the joint model that we believe
in is smaller. We return to this point in Section 2.1.
In this paper we study properties of multi-step learning strategies – those which involve multiple
training steps – by considering the advantages of breaking a single learning problem into a sequence
of two learning problems. We start by assuming a true distribution which comes from a class of
joint distributions  i.e. statistical model  P on X ⇥ Y . We prove that underlying structure of a
certain kind in P  together with differential availability of labeled vs. unlabeled data  imply a
quantiﬁable advantage to multi-step learning at ﬁnite sample size. The structure we need is the
existence of a representation t(x) of x 2 X which is a sufﬁcient statistic for the classiﬁcation or
regression of interest. Two common settings where this assumption holds are: manifold learning and
group-invariant feature learning. In these settings we have respectively

1. t = tpX is determined by the marginal pX and pX is concentrated on a submanifold of X 
2. t = tG is determined by a group action on X and p(y|x) is invariant1 under this action.

Learning t in these cases corresponds respectively to learning manifold features or group-invariant
features; various approaches exist (see (Niyogi  2013; Poggio et al.  2012) for more discussion) and
we do not assume any ﬁxed method. Our framework is also not restricted to these two settings. As a
tool for analysis we deﬁne a variant of VC dimension for statistical models which we use to prove
a useful lower bound on generalization error even2 under the assumption that the true distribution
comes from P. This allows us to establish a gap at ﬁnite sample size between the error achievable
by a single-step purely supervised learner and that achievable by a semi-supervised learner. We do
not claim an asymptotic gap. The purpose of our analysis is rather to show that differential ﬁnite
availability of data can dictate a multi-step learning approach. Our applications are respectively
a strengthening of a manifold learning example analyzed by Niyogi (2013) and a group-invariant
features example related to a result of Poggio et al. (2012). We also discuss the relevance of these to
biological learning.
Our framework has commonalities with a framework of Baxter (2000) for transfer learning. In that
work  Baxter considered learning the inductive bias (i.e.  the hypothesis space) for an algorithm for a

1 This means there is a group G of transformations of X such that p(y|x) = p(y|g·x) for all g 2 G.
2(distribution-speciﬁc lower bounds are by deﬁnition weaker than distribution-free ones)

2

“target" learning task  based on experience from previous “source" learning tasks. For this purpose he
deﬁned a learning environment E to be a class of probability distributions on X ⇥ Y together with an
unknown probability distribution Q on E  and assumed E to restrict the possible joint distributions
which may arise. We also make a generative assumption  assuming joint distributions come from P 
but we do not use a prior Q. Within his framework Baxter studied the reduction in generalization
error for an algorithm to learn a new task  deﬁned by p 2E   when given access to a sample from p
and a sample from each of m other learning tasks  p1  . . .   pm 2E   chosen randomly according to Q 
compared with an algorithm having access to only a sample from p. The analysis produced upper
bounds on generalization error in terms of covering numbers and a lower bound was also obtained in
terms of VC dimension in the speciﬁc case of shallow neural networks. In proving our lower bound
in terms of a variant of VC dimension we use a minimax analysis.

2 Setup
We assume a learning problem is speciﬁed by a joint probability distribution p on Z = X ⇥ Y and a
particular (regression  classiﬁcation or decision) function fp : X ! R determined entirely by p(y|x).
Moreover  we postulate a statistical model P on X ⇥ Y and assume p 2P . Despite the simpliﬁed
notation  fp(x) depends on the conditionals p(y|x) and not the entire joint distribution p.
There are three main types of learning problem our framework addresses (reﬂected in three types
of fp). When y is noise-free  i.e. p(y|x) is concentrated at a single y-value vp(x) 2{ 0  1} 
fp = vp : X !{ 0  1} (classiﬁcation); here fp(x) = Ep(y|x). When y is noisy  then either
fp : X !{ 0  1} (classiﬁcation/decision) or fp : X ! [0  1] (regression) and fp(x) = Ep(y|x). In
all three cases the parameters which deﬁne fp  the learning goal  depend only on p(y|x) = Ep(y|x).
We assume the learner knows the model P and the type of learning problem  i.e.  the hypothesis class
is the “concept class" C := {fp : p 2P} . To be more precise  for the ﬁrst type of fp listed above  this
is the concept class (Kearns and Vazirani  1994); for the second type  it is a class of decision rules
and for the third type  it is a class of p-concepts (Kearns and Schapire  1994). For speciﬁc choice of
loss functions  we seek worst-case bounds on learning rates  over all distributions p 2P .
Our results for all three types of learning problem are stated in Theorem 3. To keep the presentation
simple  we give a detailed proof for the ﬁrst two types  i.e.  assuming labels are binary. This shows
how classic PAC-style arguments for discrete X can be adapted to our framework where X may be
smooth. Extending these arguments to handle non-binary Y proceeds by the same modiﬁcations as
for discrete X (c.f. Kearns and Schapire (1994)). We remark that in the presence of noise  better
bounds can be obtained (see Theorem 3 for details) if a more technical version of Deﬁnition 1 is used
but we leave this for a subsequent paper.
We deﬁne the following probabilistic version of fat shattering dimension:
Deﬁnition 1. Given P  a class of probability distributions on X ⇥{ 0  1}  let  2 (0  1)  ↵ 2 (0  1/2)
and n 2 N = {0  1  . . .   ...}. Suppose there exist (disjoint) sets Si ⇢ X  i 2{ 1  . . .   n} with
S = [iSi  a reference probability measure q on X  and a sub-class Pn ⇢P of cardinality
|Pn| = 2n with the following properties:

1. q(Si)  /n for every i 2{ 1  . . .   n}
2. q lower bounds the marginals of all p 2P n on S  i.e.RB dpX RB dq for any p-measurable
subset B ⇢ S
3. 8 e 2{ 0  1}n  9 p 2P n such that Ep(y|x) > 1/2 + ↵ for x 2 Si when ei = 1 and
Ep(y|x) < 1/2  ↵ for x 2 Si when ei = 0

then we say P ↵-shatters S1  . . .   Sn -uniformly using Pn. The -uniform ↵-shattering dimen-
sion of P is the largest n such that P ↵-shatters some collection of n subsets of X -uniformly.
This provides a measure of complexity of the class P of distributions in the sense that it indicates
the variability of the expected y-values for x constrained to lie in the region S with measure at
least  under corresponding marginals. The reference measure q serves as a lower bound on the
marginals and ensures that they “uniformly" assign probabilty at least  to S. Richness (variability)
of conditionals is thus traded off against uniformity of the corresponding marginal distributions.

3

Remark 2 (Uniformity of measure). The technical requirement of a reference distribution q is
automatically satisﬁed if all marginals pX for p 2P n are uniform over S. For simplicity this is the
situation considered in all our examples. The weaker condition (in terms of q) that we postulate in
Deﬁnition 1 is however sufﬁcient for our main result  Theorem 3.
If fp is binary and y is noise-free then P shatters S1  . . .   Sn -uniformly if and only if there is
a sub-class Pn ⇢P with the speciﬁed uniformity of measure  such that each fp(·) = Ep(y|·) 
p 2P n is constant on each Si and the induced set-functions shatter {S1  . . .   Sn} in the usual
(Vapnik-Chervonenkis) sense. In that case  ↵ may be chosen arbitrarily in (0  1/2) and we omit
mention of it. If fp takes values in [0  1] or fp is binary and y noisy then -uniform shattering can be
expressed in terms of fat-shattering (both at scale ↵).
We show that the -uniform ↵-shattering dimension of P can be used to lower bound the sample
size required by even the most powerful learner of this class of problems. The proof is in the same
spirit as purely combinatorial proofs of lower bounds using VC-dimension. Essentially the added
condition on P in terms of  allows to convert the risk calculation to a combinatorial problem. As a
counterpoint to the lower bound result  we consider an alternative two step learning strategy which
makes use of underlying structure in X implied by the model P and we obtain upper bounds for the
corresponding risk.

2.1 Underlying structure
We assume a representation t : X ! Rk of the data  such that p(y|x) can be expressed in terms
of p(y|t(x))  say fp(x) = g✓(t(x)) for some parameter ✓ 2 ⇥. Such a t is generally known in
Statistics as a sufﬁcient dimension reduction for fp but here we make no assumption on the dimension
k (compared with the dimension of X). This is in keeping with the paradigm of feature extraction for
use in kernel machines  where the dimension of t(X) may even be higher than the original dimension
of X. As in that setting  what will be important is rather that the intermediate representation t(x)
reduce the complexity of the concept space. While t depends on p we will assume it does so only
via X. For example t could depend on p through the marginal pX on X or possible group action on
X; it is a manifestation in the data X  possibly over time  of underlying structure in the true joint
distribution p 2P . The representation t captures structure in X induced by p. On the other hand  the
regression function itself depends only on the conditional p(y|t(x)).
In general  the natural factorization ⇡ : P!P X  p 7! pX determines for each marginal q 2P X
a collection ⇡1(q) of possible conditionals  namely those p(y|x) arising from joint p 2P that
have marginal pX = q. More generally any sufﬁcient statistic t induces a similar factorization (c.f.
Fisher-Neyman characterization) ⇡t : P!P t  p 7! pt  where Pt is the marginal model with respect
to t  and only conditionals p(y|t) are needed for learning. As before  given a known marginal q 2P t 
this implies a collection ⇡1
Knowing q thus reduces the original problem where p(y|x) or p(y|t) can come from any p 2P to
one where it comes from p in a reduced class ⇡1(q) or ⇡1
(q) ( P. Note the similarity with the
assumption of Balcan and Blum (2005) that a good compatibility function reduce the concept class. In
our case the concept class C consists of fp deﬁned by p(y|t) in [tPY |t with PY |t :={p(y|t) : p 2P}  
and marginals come from Pt. The joint model P that we postulate  meanwhile  corresponds to
a subset of C⇥P t (pairs (fp  q) where fp uses p 2 ⇡1
(q)). The indicator function  for this
subset is an abstract (binary) version of compatibility function (recall the compatibility function of
Balcan-Blum should be a [0  1]-valued function on C⇥D   satisfying further practical conditions that
our function typically would not). Thus  in a sense  our assumption of a joint model P and sufﬁcient
statistic t amounts to a general form of compatibility function that links C and D without making
assumptions on how t might be learned. This is enough to imply the original learning problem can be
factored into ﬁrst learning the structure t and then learning the parameter ✓ for fp(x) = g✓(t(x)) in a
reduced hypothesis space. Our goal is to understand when and why one should do so.

(q) of possible conditionals p(y|t) relevant to learning.

t

t

t

2.2 Learning rates

We wish to quantify the beneﬁts achieved by using such a factorization in terms of the bounds on
the expected loss (i.e. risk) for a sample of size m 2 N drawn iid from any p 2P . We assume the
learner is provided with a sample ¯z = (z1  z2 ··· zm)  with zi = (xi  yi) 2 X ⇥ Y = Z  drawn iid
from the distribution p and uses an algorithm A : Zm !C = H to select A(¯z) to approximate fp.

4

Let `(A(¯z)  fp) denote a speciﬁc loss. It might be 0/1  absolute  squared  hinge or logistic loss. We
deﬁne L(A(¯z)  fp) to be the global expectation or L2-norm of one of those pointwise losses `:

or

L(A(¯z)  fp) := Ex`(A(¯z)(x)  fp(x)) =ZX
L(A(¯z)  fp) := ||`(A(¯z)  fp)||L2(pX ) =sZX

`(A(¯z)(x)  fp(x))dpX(x)

`(A(¯z)(x)  fp(x))2dpX.

(1)

(2)

(3)

(4)

Then the worst case expected loss (i.e. minimax risk) for the best learning algorithm with no
knowledge of tpX is

R(m) := inf
A

sup
p2P

p(y|tq)s.t.
p2P pX =q
while for the best learning algorithm with oracle knowledge of tpX it is

E¯zL(A(¯z)  fp) = inf

A

sup
q2PX

sup

E¯zL(A(¯z  fp) .

Q(m) := sup
q2PX

inf
A

sup

p(y|tq)s.t.
p2P pX =q

E¯zL(A(¯z  fp) .

Some clariﬁcation is in order regarding the classes over which the suprema are taken. In principle
the worst case expected loss for a given A is the supremum over P of the expected loss. Since fp(x)
is determined by p(y|tpX (x))  and tpX is determined by pX this is a supremum over q 2P X of
a supremum over p(y|tq(·)) such that pX = q. Finding the worst case expected error for the best
A therefore means taking the inﬁmum of the supremum just described. In the case of Q(m) since
the algorithm knows tq  the order of the supremum over t changes with respect to the inﬁmum: the
learner can select the best algorithm A using knowledge of tq.
Clearly R(m)  Q(m) by deﬁnition. In the next section  we lower bound R(m) and upper bound
Q(m) to establish a gap between R(m) and Q(m).

3 Main Result
We show that -uniform shattering dimension n or more implies a lower bound on the worst case
expected error  R(m)  when the sample size m  n. In particular - in the setup speciﬁed in the
previous section - if {g✓(·) : ✓ 2 ⇥} has much smaller VC dimension than n this results in a distinct
gap between rates for a learner with oracle access to tpX and a learner without.
Theorem 3. Consider the framework deﬁned in the previous Section with Y = {0  1}. Assume
{g✓(·) : ✓ 2 ⇥} has VC dimension d < m and P has -uniform ↵-shattering dimension n  (1+✏)m.
Then  for sample size m  Q(m)  16q d log(m+1)+log 8+1
m+1/8 where b
depends both on the type of loss and the presence of noise  while c depends on noise.
Assume the standard deﬁnition in (1). If fp are binary (in the noise-free or noisy setting) b = 1
for absolute  squared  0-1  hinge or logistic loss. In the noisy setting  if fp = E(y|x) 2 [0  1] 
b = ↵ for absolute loss and b = ↵2 for squared loss. In general  c = 1 in the noise-free setting
and c = (1/2 + ↵)m in the noisy setting. By requiring P to satisfy a stronger notion of -uniform
↵-shattering one can obtain c = 1 even in the noisy case.

while R(m) >✏bc

2m

Note that for sample size m and -uniform ↵-shattering dimension 2m  we have ✏ = 1  so the lower
bound in its simplest form becomes m+1/8. This is the bound we will use in the next Section to
derive implications of Theorem 3.
Remark 4. We have stated in the Theorem a simple upper bound  sticking to Y = {0  1} and using
VC dimension  in order to focus the presentation on the lower bound which uses the new complexity
measure. The upper bound could be improved. It could also be replaced with a corresponding upper
bound assuming instead Y = [0  1] and fat shattering dimension d.
Proof. The upper bound on Q(m) holds for an ERM algorithm (by the classic argument  see for
example Corollary 12.1 in Devroye et al. (1996)). We focus here on the lower bound for R(m).
Moreover  we stick to the simpler deﬁnition of -uniform shattering in Deﬁnition 1 and omit proof of
the ﬁnal statement of the Theorem  which is slightly more involved. We let n = 2m (i.e. ✏ = 1) and

5

we comment in a footnote on the result for general ✏. Let S1  . . .   S2m be sets which are -uniformly
↵-shattered using the family P2m ⇢P and denote their union by S. By assumption S has measure
at least  under a reference measure q which is dominated by all marginals pX for p 2P 2m (see
Deﬁnition 1). We divide our argument into three parts.
1.

If we prove a lower bound for the average over P2m 

8A 

1

22m Xp2P2m

E¯zL(A(¯z)  fp)  bcm+1/8

(5)

it will also be a lower bound for the supremum over P2m:

8A 

sup
p2P2m

E¯zL(A(¯z)  fp)  bcm+1/8 .
and hence for the supremum over P. It therefore sufﬁces to prove (5).
2. Given x 2 S  deﬁne vp(x) to be the more likely label for x under the joint distribution p 2P 2m.
This notation extends to the noisy case the deﬁnition of vp already given for the noise-free case. The
uniform shattering condition implies p(vp(x)|x) > 1/2 + ↵ in the noisy case and p(vp(x)|x) = 1
in the noise-free case. Given ¯x = (x1  . . .   xm) 2 Sm  write ¯zp(¯x) := (z1  . . .   zm) where zj =
(xj  vp(xj)). Then

E¯zL(A(¯z)  fp) = ZZm
 ZSm⇥Y m

L(A(¯z)  fp)dpm(¯z)
L(A(¯z)  fp)dpm(¯z)  cZSm

L(A(¯zp(¯x))  fp)dpm

X(¯x)

where c is as speciﬁed in the Theorem. Note the sets

Vl := {¯x 2 Sm ⇢ X m : the xj occupy exactly l of the Si}

for l = 1  . . .   m deﬁne a partition of Sm. Recall that dpX  dq on S for all p 2P 2m so
L(A(¯zp(¯x))  fp) dqm(¯x)

L(A(¯zp(¯x))  fp)dpm

X(¯x) 

1

22m Xp2P2m

mXl=1 Z¯x2Vl

ZSm

which will complete the proof of (5).
3. We now assume a ﬁxed but arbitrary ¯x 2 Vl and prove I  b/8. To simplify the discussion 
we will refer to sets Si which contain a component xj of ¯x as Si with data. We also need notation
for the elements of P2m: for each L ⇢ [2m] denote by p(L) the unique element of P2m such that
vp(L)|Si = 1 if i 2 L  and vp(L)|Si = 0 if i /2 L. Now  let L¯x := {i 2 [2m]
: ¯x \ Si 6= ;}. These
are the indices of sets Si with data. By assumption |L¯x| = l  and so |Lc
¯x| = 2m  l.
¯x. We will
Every subset L ⇢ [2m] and hence every p 2P 2m is determined by L \ L¯x and L \ Lc
collect together all p(L) having the same L \ L¯x  namely for each D ⇢ L¯x deﬁne

PD := {p(L) 2P 2m : L \ L¯x = D}.

These 2l families partition P2m and in each PD there are 22ml probability distributions. Most
importantly  ¯zp(¯x) is the same for all p 2P D (because D determines vp on the Si with data). This

6

L(A(¯zp(¯x))  fp)

dqm(¯x).

=

mXl=1 Z¯x2Vl

1

0BBBB@
22m Xp2P2m
|
mXl=1 Z¯x2Vl

{z
dqm(¯x) = Z¯x2Sm

I

1CCCCA

}

dqm(¯x)  m

We claim the integrand  I  is bounded below by b/8 (this computation is performed in part 3  and
depends on knowing ¯x 2 Vl). At the same time  S has measure at least  under q so

implies A(¯zp(¯x)) : X ! R is the same function3 of X for all p in a given PD. To simplify notation 
since we will be working within a single PD  we write f := A(¯z(¯x)).
While f is the hypothesized regression function given data ¯x  fp is the true regression function when
p is the underlying distribution. For each set Si let vi be 1 if f is above 1/2 on a majority of Si using
reference measure q (a q-majority) and 0 otherwise.
We now focus on the “unseen" Si where no data lie (i.e.  i 2 Lc
correspondence between elements p 2P D and subsets K ⇢ Lc
¯x:
Take a speciﬁc p 2P D with its associated Kp. We have |f (x)  fp(x)| >↵ on the q-majority of the
set Si for all i 2 Kp.
The condition |f (x)  fp(x)| >↵ with f (x) and fp(x) on opposite sides of 1/2 implies a lower
bound on `(f (x)  fp(x)) for each of the pointwise loss functions ` that we consider (0/1  absolute 
square  hinge  logistic). The value of b  however  differs from case to case (see Appendix).
For now we have 

p 2P D ! Kp := {i 2 Lc

¯x) and use the vi to specify a 1-1

¯x : vp 6= vi}.

1

2ZSi

dq(x) 

b
4m

.

ZSi

`(f (x)  fp(x)) dpX(x) ZSi

`(f (x)  fp(x)) dq(x)  b

b
4m

L(f (x)  fp(x))  k

Summing over all i 2 Kp  and letting k = |Kp|  we obtain (still for the same p)
(assuming L is deﬁned by equation (1))4. There are2m`
k = 0  . . .   2m  `. Therefore 
L(f (x)  fp(x)) 

2m`Xk=0 ✓2m  `
k ◆k
(using 2m  `  2m  m = m)5. Since D was an arbitrary subset of L¯x  this same lower bound
holds for each of the 2` families PD and so
22m Xp2P2m

k  possible K with cardinality k  for any
22m`(2m  `)

4m  22m` b

L(f (x)  fp(x)) 

Xp2PD

b
8

.

b
4m

=

1

I =

b

2

8

In the constructions of the next Section it is often the case that one can prove a different level of
shattering for different n  namely (n)-uniform shattering of n subsets for various n. The following
Corollary is an immediate consequence of the Theorem for such settings. We state it for binary fp
without noise.
Corollary 5. Let C 2 (0  1) and M 2 N. If P (n)-uniformly ↵-shatters n subsets of X and
(n)n+1/8 > C for all n < M then no learning algorithm can achieve worst case expected error
below ↵C  using a training sample of size less than M/2. If such uniform shattering holds for all
n 2 N then the same lower bound applies regardless of sample size.
Even when (n)-uniform shattering holds for all n 2 N and limn!1 (n) = 1  if (n) approaches
1 sufﬁciently slowly then it is possible (n)n+1 ! 0 and there is no asymptotic obstacle to learning.
By contrast  the next Section shows an extreme situation where limn!1 (n)n+1  e > 0. In that
case  learning is impossible.

4 Applications and conclusion
Manifold learning We now describe a simpler  ﬁnite dimensional version of the example in Niyogi
(2013). Let X = RD  D  2 and Y = {0  1}. Fix N 2 N and consider a very simple type of
1-dimensional manifold in X  namely the union of N linear segments  connected in circular fashion
(see Figure 1). Let PX be the collection of marginal distributions  each of which is supported on and
assigns uniform probability along a curve of this type. There is a 1-1 correspondence between the
elements of PX and curves just described.

3Warning: f need not be an element of {fp : p 2P 2n}; we only know f 2H = {fp : p 2P} .
4In the L2 version  using px  x  the reader can verify the same lower bound holds.
5In the case where we use (1 + ✏)m instead of 2m  we would have (1 + ✏)m  `  ✏m here.

7

Figure 1: An example of M with N = 12. The
dashed curve is labeled 1  the solid curve 0 (in
next Figure as well).

Figure 2: M with N = 28 = 4(n + 1) pieces 
used to prove uniform shattering of n sets
(shown for the case n = 6 with e = 010010).

n+1 (see Appendix and Figure 2). Since (1  1

On each curve M  choose two distinct points x0  x00. Removing these disconnects M. Let one
component be labeled 0 and the other 1  then label x0 and x00 oppositely. Let P be the class of joint
distributions on X ⇥ Y with conditionals as described and marginals in PX. This is a noise-free
setting and fp is binary. Given M (or circular coordinates on M)  consider the reduced class
P0 := {p 2P : support(pX) = M}. Then H0 := {fp : p 2P 0} has VC dimension 3. On the
other hand  for n < N/4  1 it can be shown that P (n)-uniformly shatters n sets with fp  where
n+1 )n+1 ! e > 0 as n ! 1  it follows
(n) = 1  1
from Corollary 5 that the worst case expected error is bounded below by e/8 for any sample of size
n  N/8  1/2. If many linear pieces are allowed (i.e. N is high) this could be an impractical
number of labeled examples. By contrast with this example  (n) in Niyogi’s example cannot be
made arbitrarily close to 1.
Group-invariant features We give a simpliﬁed  partially-discrete example (for a smooth version
and Figures  see Appendix). Let Y = {0  1} and let X = J ⇥ I where J = {0  1  . . .   n1 
1}⇥{ 0  1  . . .   n2  1} is an n1 by n2 grid (ni 2 N) and I = [0  1] is a real line segment. One
should picture X as a rectangular array of vertical sticks. Above each grid point (j1  j2) consider
two special points on the stick I  one with i = i+ := 1  ✏ and the other with i = i := 0 + ✏.
Let PX contain only the uniform distribution on X and assume the noise-free setting. For each
¯e 2{ + }n1n2  on each segment (j1  j2) ⇥ I assign  via p¯e  the label 1 above the special point
(determined by ¯e) and 0 below the point. This determines a family of n1n2 conditional distributions
and thus a family P := {p¯e : ¯e 2{ + }n1n2} of n1n2 joint distributions. The reader can verify
that P has 2✏-uniform shattering dimension n1n2. Note that when the true distribution is p¯e for some
¯e 2{ + }n1n2 the labels will be invariant under the action a¯e of Zn1 ⇥ Zn2 deﬁned as follows.
Given (z1  z2) 2 Zn1 ⇥Zn2 and (j1  j2) 2 J  let the group element (z1  z2) move the vertical stick at
(j1  j2) to the one at (z1 + j1 mod n1  z2 + j2 mod n2) without ﬂipping the stick over  just stretching
it as needed so the special point i± determined by ¯e on the ﬁrst stick goes to the one on the second
stick. The orbit space of the action can be identiﬁed with I. Let t : X ⇥ Y ! I be the projection
of X ⇥ Y to this orbit space  then there is an induced labelling of this orbit space (because labels
were invariant under the action of the group). Given access to t  the resulting concept class has VC
dimension 1. On the other hand  given instead access to a projection s for the action of the subgroup
Zn1 ⇥{ 0}  the class eP := {p(·|s) : p 2P} has 2✏-uniform shattering dimension n2. Thus we have

a general setting where the over-all complexity requirements for two-step learning are n1 + n2 while
for single-step learning they are n1n2.

Conclusion We used a notion of uniform shattering to demonstrate both manifold learning and
invariant feature learning situations where learning becomes impossible unless the learner has access
to very large amounts of labeled data or else uses a two-step semi-supervised approach in which
suitable manifold- or group-invariant features are learned ﬁrst in unsupervised fashion. Our examples
also provide a complexity manifestation of the advantages  observed by Poggio and Mallat  of forming
intermediate group-invariant features according to sub-groups of a larger transformation group.

Acknowledgements The author is deeply grateful to Partha Niyogi for the chance to have been his
student. This paper is directly inspired by discussions with him which were cut short much too soon.
The author also thanks Ankan Saha and Misha Belkin for very helpful input on preliminary drafts.

8

References
M. Ahissar and S. Hochstein. The reverse hierarchy theory of visual perceptual learning. Trends

Cogn Sci.  Oct;8(10):457–64  2004.

G. Alain and Y. Bengio. What regularized auto-encoders learn from the data generating distribution.

Technical report  2012. http://arXiv:1211.4246[cs.LG].

M.-F. Balcan and A. Blum. A pac-style model for learning from labeled and unlabeled data. In

Learning Theory  volume 3559  pages 111–126. Springer LNCS  2005.

J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research  12:149?198 

2000.

M. Belkin  P. Niyogi  and V. Sindhwani. Manifold regularization: a geometric framework for learning
from labeled and unlabeled examples. Journal of Machine Learning Research  7:2399–2434  2006.
S. Ben-David  T. Lu  and D. Pál. Does unlabeled data provably help? worst-case analysis of the

sample complexity of semi-supervised learning. In COLT  pages 33–44  2008.

J. Bourne and M. Rosa. Hierarchical development of the primate visual cortex  as revealed by
neuroﬁlament immunoreactivity: early maturation of the middle temporal area (mt). Cereb Cortex 
Mar;16(3):405–14  2006. Epub 2005 Jun 8.

V. Castelli and T. Cover. The relative value of labeled and unlabeled samples in pattern recognition.

IEEE Transactions on Information Theory  42:2102–2117  1996.

L. Devroye  L. Györﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition  volume 31 of

Applications of mathematics. Springer  New York  1996.

D. Haussler. Generalizing the pac model: Sample size bounds from metric dimension-based uniform

convergence results. pages 40–45  1989.

M. Kearns and R. Schapire. Efﬁcient distribution-free learning of probabilistic concepts. Journal of

Computer and System Sciences  48:464–497  1994.

M. J. Kearns and U. V. Vazirani. An Introduction to Computational Learning Theory. MIT Press 

Cambridge  Massachusetts  1994.

V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on

Information Theory  47(5):1902–1914  2001.

S. Mallat. Group invariant scattering. CoRR  abs/1101.2286  2011. http://arxiv.org/abs/1101.2286.
M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of Machine Learning. MIT Press  2012.
P. Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. Journal

of Machine Learning Research  14:1229–1250  2013.

T. Poggio  J. Mutch  F. Anselmi  L. Rosasco  J. Leibo  and A. Tacchetti. The computational magic of
the ventral stream: sketch of a theory (and why some deep architectures work). Technical report 
Massachussetes Institute of Technology  2012. MIT-CSAIL-TR-2012-035.

R. Urner  S. Shalev-Shwartz  and S. Ben-David. Access to unlabeled data can speed up prediction

time. In ICML  2011.

L. Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142  1984.

9

,Maia Fraser
Marc Bellemare
Will Dabney
Robert Dadashi
Adrien Ali Taiga
Pablo Samuel Castro
Nicolas Le Roux
Dale Schuurmans
Tor Lattimore
Clare Lyle