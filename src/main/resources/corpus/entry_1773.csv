2011,Efficient anomaly detection using bipartite k-NN graphs,Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature  including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector  while possessing several desirable characteristics  suffers from high computation complexity  and in [4] a simpler heuristic approximation  the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper  we propose a novel bipartite k-nearest neighbor graph (BP-kNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG  while being computationally simpler than the K-kNNG and the surrogate L1O-kNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.,Efﬁcient anomaly detection using

bipartite k-NN graphs

Kumar Sricharan
Department of EECS
University of Michigan
Ann Arbor  MI 48104

kksreddy@umich.edu

Alfred O. Hero III
Department of EECS
University of Michigan
Ann Arbor  MI 48104
hero@umich.edu

Abstract

Learning minimum volume sets of an underlying nominal distribution is a very ef-
fective approach to anomaly detection. Several approaches to learning minimum
volume sets have been proposed in the literature  including the K-point nearest
neighbor graph (K-kNNG) algorithm based on the geometric entropy minimiza-
tion (GEM) principle [4]. The K-kNNG detector  while possessing several de-
sirable characteristics  suffers from high computation complexity  and in [4] a
simpler heuristic approximation  the leave-one-out kNNG (L1O-kNNG) was pro-
posed. In this paper  we propose a novel bipartite k-nearest neighbor graph (BP-
kNNG) anomaly detection scheme for estimating minimum volume sets. Our
bipartite estimator retains all the desirable theoretical properties of the K-kNNG 
while being computationally simpler than the K-kNNG and the surrogate L1O-
kNNG detectors. We show that BP-kNNG is asymptotically consistent in recov-
ering the p-value of each test point. Experimental results are given that illustrate
the superior performance of BP-kNNG as compared to the L1O-kNNG and other
state of the art anomaly detection schemes.

1 Introduction

Given a training set of normal events  the anomaly detection problem aims to identify unknown 
anomalous events that deviate from the normal set. This novelty detection problem arises in applic-
ations where failure to detect anomalous activity could lead to catastrophic outcomes  for example 
detection of faults in mission-critical systems  quality control in manufacturing and medical dia-
gnosis.

Several approaches have been proposed for anomaly detection. One class of algorithms assumes a
family of parametrically deﬁned nominal distributions. Examples include Hotelling’s T test and the
Fisher F-test  which are both based on a Gaussian distribution assumption. The drawback of these
algorithms is model mismatch: the supposed distribution need not be a correct representation of the
nominal data  which can then lead to poor false alarm rates. More recently  several non-parametric
methods based on minimum volume (MV) set estimation have been proposed. These methods aim to
ﬁnd the minimum volume set that recovers a certain probability mass α with respect to the unknown
probability density of the nominal events. If a new event falls within the MV set  it is classiﬁed as
normal and otherwise as anomalous.

Estimation of minimum volume sets is a difﬁcult problem  especially for high dimensional data.
There are two types of approaches to this problem: (1) transform the MV estimation problem to an
equivalent density level set estimation problem  which requires estimation of the nominal density;
and (2) directly identify the minimal set using function approximation and non-parametric estima-
tion [10  6  9]. Both types of approaches involve explicit approximation of high dimensional quant-

1

ities - the multivariate density function in the ﬁrst case and the boundary of the minimum volume
set in the second and are therefore not easily applied to high dimensional problems.

The GEM principle developed by Hero [4] for determining MV sets circumvents the above difﬁ-
culties by using the asymptotic theory of random Euclidean graphs instead of function approxima-
tion. However  the GEM based K-kNNG anomaly detection scheme proposed in [4] is computation-
ally difﬁcult. To address this issue  a surrogate L1O-kNNG anomaly detection scheme was proposed
in [4]. L1O-kNNG is computationally simpler than K-kNNG  but loses some desirable properties of
the K-kNNG  including asymptotic consistency  as shown below.
In this paper  we use the GEM principle to develop a bipartite k-nearest neighbor (k-NN) graph-
based anomaly detection algorithm. BP-kNNG retains the desirable properties of the GEM principle
and as a result inherits the following features: (i) it is not restricted to linear or even convex decision
regions  (ii) it is completely non-parametric  (iii) it is optimal in that it converges to the uniformly
most powerful (UMP) test when the anomalies are drawn from a mixture of the nominal density and
the uniform density  (iv) it does not require knowledge of anomalies in the training sample  (v) it is
asymptotically consistent in recovering the p-value of the test point and (vi) it produces estimated
p-values  allowing for false positive rate control.
K-LPE [13] and RRS [7] are anomaly detection methods which are also based on k-NN graphs. BP-
kNNG differs from L1O-kNNG  K-LPE and RRS in the following respects. L1O-kNNG  K-LPE
and RRS do not use bipartite graphs. We will show that the bipartite nature of BP-kNNG results
in signiﬁcant computational savings. In addition  the K-LPE and RRS test statistics involve only
the k-th nearest neighbor distance  while the statistic in BP-kNNG  like the L1O-kNNG  involves
summation of the power weighted distance of all the edges in the k-NN graph. This will result
in increased robustness to outliers in the training sample. Finally  we will show that the mean
square rate of convergence of p-values in BP-kNNG (O(T −2/(2+d))) is faster as compared to the
convergence rate of K-LPE (O(T −2/5+T −6/5d))  where T is the size of the nominal training sample
and d is the dimension of the data.
The rest of this paper is organized as follows. In Section 2  we outline the statistical framework
for minimum volume set anomaly detection. In Section 3  we describe the GEM principle and the
K-kNNG and L1O-kNNG anomaly detection schemes proposed in [4]. Next  in Section 4  we
develop our bipartite k-NN graph (BP-kNNG) method for anomaly detection. We show consistency
of the method and compare its computational complexity with that of the K-kNNG  L1O-kNNG and
K-LPE algorithms. In Section 5  we show simulation results that illustrate the superior performance
of BP-kNNG over L1O-kNNG. We also show that our method compares favorably to other state of
the art anomaly detection schemes when applied to real world data from the UCI repository [1]. We
conclude with a short discussion in Section 6.

2 Statistical novelty detection
The problem setup is as follows. We assume that a training sample X T = {X1  . . .   XT} of d-
dimensional vectors is available. Given a new sample X  the objective is to declare X to either be
a ’nominal’ event consistent with XT or an ’anomalous’ event which deviates from X T . We seek to
ﬁnd a functional D and corresponding detection rule D(x) > 0 so that X is declared to be nominal if
D(x) > 0 holds and anomalous otherwise. The acceptance region is given by A = {x : D(x) > 0}.
We seek to further constrain the choice of D to allow as few false negatives as possible for a ﬁxed
allowance of false positives.
To formulate this problem  we adopt the standard statistical framework for testing composite hy-
potheses. We assume that the training sample XT is an i.i.d sample draw from an unknown d-
dimensional probability distribution f 0(x) on [0  1]d. Let X have density f on [0  1]d. The anomaly
detection problem can be formulated as testing the hypotheses H 0 : f = f0 versus H1 : f (cid:2)= f0.
For a given α ∈ (0  1)  we seek an acceptance region A that satisﬁes P r(X ∈ A|H 0) ≥ 1 − α.
(cid:2)
This requirement maintains the false positive rate at a level no greater than α. Let A = {A :
f0(x)dx ≥ 1 − α} denote the collection of acceptance regions of level α. The most suitable
acceptance region from the collection A would be the set which minimizes the false negative rate.
Assume that the density f is bounded above by some constant C. In this case the false negative rate
d. Consider the relaxed problem of
is bounded by Cλ(A) where λ(.) is the Lebesgue measure in R

A

2

−1

(cid:2)

A f ν

(cid:2)
A f0(x)dx ≥ α}.
(cid:2)

minimizing the upper bound Cλ(A) or equivalently the volume λ(A) of A. The optimal acceptance
region with a maximum false alarm rate α is therefore given by the minimum volume set of level α:
Λα = min{λ(A) :
f0(x)dx ≥ 1 − α} where
Deﬁne the minimum entropy set of level α to be Ω α = min{Hν(A) :
Hν(A) = (1 − ν)
0 (x)dx is the R´enyi ν-entropy of the density f 0 over the set A. It can be
d  the minimum volume set and the minimum entropy
shown that when f0 is a Lebesgue density in R
set are equivalent  i.e. Λα and Ωα are identical. Therefore  the optimal decision rule for a given level
of false alarm α is to declare an anomaly if X /∈ Ω α.
This decision rule has a strong optimality property [4]: when f 0 is Lebesgue continuous and has
no ’ﬂat’ regions over its support  this decision rule is a uniformly most powerful (UMP) test at level
1 − α for the null hypothesis that the test point has density f (x) equal to the nominal f 0(x) versus
the alternative hypothesis that f (x) = (1 − )f0(x) + U (x)  where U (x) is the uniform density
over [0  1]d and  ∈ [0  1]. Furthermore  the power function is given by β = P r(X /∈ Ω α|H1) =
(1 − )α + (1 − λ(Ωα)).

A

3 GEM principle

In this section  we brieﬂy review the geometric entropy minimization (GEM) principle method [4]
for determining minimum entropy sets Ω α of level α. The GEM method directly estimates the crit-
ical region Ωα for detecting anomalies using minimum coverings of subsets of points in a nominal
training sample. These coverings are obtained by constructing minimal graphs  e.g.  the k-minimal
spanning tree or the k-nearest neighbor graph  covering a K-point subset that is a given proportion
of the training sample. Points in the training sample that are not covered by the K-point minimal
graphs are identiﬁed as tail events.
In particular  let XK T denote one of the
K point subsets of XT . The k-nearest neighbors
(k-NN) of a point Xi ∈ XK T are the k closest points to Xi among XK T − Xi. Denote the
corresponding set of edges between X i and its k-NN by {ei(1)  . . .   ei(k)}. For any subset XK T  
deﬁne the total power weighted edge length of the k-NN graph on X K T with power weighting γ
(0 < γ < d)  as

(cid:4)

(cid:3)

T
K

LkN N (XK T ) =

|eti(l)|γ 

K(cid:5)

k(cid:5)

i=1

l=1

(cid:3)

(cid:4)

where {t1  . . .   tK} are the indices of Xi ∈ XK T . Deﬁne the K-kNNG graph to be the K-point
k-NN graph having minimal length minXT  K∈XT LkN N (XT K ) over all
subsets XK T . Denote
LkN N (XK T ).
the corresponding length minimizing subset of K points by X ∗
The K-kNNG thus speciﬁes a minimal graph covering X ∗
K T of size K. This graph can be viewed as
capturing the densest regions of XT . If XT is an i.i.d. sample from a multivariate density f 0(x) and
if limK T→∞ K/T = ρ  then the set X ∗
K T converges a.s. to the minimum ν-entropy set containing
a proportion of at least ρ of the mass of f 0(x)  where ν = 1 − γ/d [4]. This set can be used to
perform anomaly detection.

T
K
T K = argmin
XT  K∈X

3.1 K-kNNG anomaly detection
Given a test sample X  denote the pooled sample X T +1 = XT ∪ {X} and determine the K-kNNG
graph over XT +1. Declare X to be an anomaly if X /∈ X ∗
K T +1 and nominal otherwise. When the
density f0 is Lebesgue continuous  it follows from [4] that as K  T → ∞  this anomaly detection
algorithm has false alarm rate that converges to α = 1 − K/T and power that converges to that of
the minimum volume set test of level α. An identical detection scheme based on the K-minimal
spanning tree has also been developed in [4].

The K-kNNG anomaly detection scheme therefore offers a direct approach to detecting outliers
while bypassing the more difﬁcult problems of density estimation and level set estimation in high di-
mensions. However  this algorithm requires construction of k-nearest neighbor graphs (or k-minimal
spanning trees) over
different subsets. For each input test point  the runtime of this algorithm

(cid:3)

(cid:4)

T
K

3

(cid:4)

). As a result  the K-kNNG method is not well suited for anomaly detection

(cid:3)

T
is therefore O(dK 2
K
for large sample sizes.

3.2 L1O-kNNG

To address the computational problems of K-kNNG  Hero [4] proposed implementing the K-kNNG
for the simplest case K = T − 1. The runtime of this algorithm for each input test point is O(dT 2).
Clearly  the L1O-kNNG is of much lower complexity that the K-kNNG scheme. However  the L1O-
kNNG detects anomalies at a ﬁxed false alarm rate 1/(T + 1)  where T is the training sample size.
To detect anomalies at a higher false alarm rate α∗
  one would have to subsample the training set
= 1/α∗ − 1 training samples. This destroys any hope for asymptotic consistency
and only use T ∗
of the L1O-kNNG.

In the next section  we propose a different GEM based algorithm that uses bipartite graphs. The
algorithm has algorithm has a much faster runtime than the L1O-kNNG  and unlike the L1O-kNNG 
is asymptotically consistent and can operate at any speciﬁed alarm rate α. We describe our algorithm
below.

i=1

l=k−s+1

(cid:3)

4 BP-kNNG
Let {XN  XM} be a partition of XT with card{XN} = N and card{XM} = M = T − N
respectively. As above  let XK N denote one of the
subsets of K distinct points from XN .
Deﬁne the bipartite k-NN graph on {XK N  XM} to be the set of edges linking each X i ∈ XK N
to its k nearest neighbors in XM . Deﬁne the total power weighted edge length of this bipartite
k-NN graph with power weighting γ (0 < γ < d) and a ﬁxed number of edges s (1 ≤ s ≤ k)
corresponding to each vertex X i ∈ XK N to be
Ls k(XK N  XM ) =

k(cid:5)

K(cid:5)

|eti(l)|γ 

(cid:4)

N
K

N
K

(cid:3)

(cid:4)

K N = argmin
XK N∈X

where {t1  . . .   tK} are the indices of Xi ∈ XK N and {eti(1)  . . .   eti(k)} are the k-NN edges in
the bipartite graph originating from X ti ∈ XK N . Deﬁne the bipartite K-kNNG graph to be the one
subsets XK N . Deﬁne
having minimal weighted length minXN K∈XN Ls k(XN K XM ) over all
the corresponding minimizing subset of K points of X K N by X ∗
Ls k(XK N  XM ).
Using the theory of partitioned k-NN graph entropy estimators [11]  it follows that as k/M →
0  k  N → ∞ and for ﬁxed s  the set X ∗
K N converges a.s. to the minimum ν-entropy set Ω 1−ρ
containing a proportion of at least ρ of the mass of f 0(x)  where ρ = limK N→∞ K/N and ν =
1 − γ/d.
This suggests using the bipartite k-NN graph to detect anomalies in the following way. Given a
test point X  denote the pooled sample X N +1 = XN ∪ {X} and determine the optimal bipartite
K-kNNG graph X ∗
K N +1
and nominal otherwise. It is clear that by the GEM principle  this algorithm detects false alarms at
a rate that converges to α = 1 − K/T and power that converges to that of the minimum volume set
test of level α.
(cid:6)k
(cid:6)k
We can equivalently determine X ∗
K N +1 as follows. For each Xi ∈ XN   construct ds k(Xi) =
l=s−k+1 |eX(l)|γ  where
l=k−s+1 |ei(l)|γ.
{eX(1)  . . .   eX(k)} are the k-NN edges from X to XM . Now  choose the K points among X N ∪ X
with the K smallest of the N + 1 edge lengths {ds k(Xi)  Xi ∈ XN} ∪ {ds k(X)}. Because of
the bipartite nature of the construction  this is equivalent to choosing X ∗
K N +1. This leads to the
proposed BP-kNNG anomaly detection algorithm described by Algorithm 1.

K N +1 over {XK N +1 XM}. Now declare X to be an anomaly if X /∈ X ∗

For each test point X  deﬁne ds k(X) =

4.1 BP-kNNG p-value estimates

The p-value is a score between 0 and 1 that is associated with the likelihood that a given point X 0
comes from a speciﬁed nominal distribution. The BP-kNNG generates an estimate of the p-value

4

Algorithm 1 Anomaly detection scheme using bipartite k-NN graphs
1. Input: Training samples XT   test samples X  false alarm rate α
2. Training phase
a. Create partition {XN  XM}
b. Construct k-NN bipartite graph on partition
c. Compute k-NN lengths ds k(Xi) for each Xi ∈ XN : ds k(Xi) =
3. Test phase: detect anomalous points
for each input test sample X do

(cid:6)k
(cid:5)
l=k−s+1 |eX(l)|γ
1(ds k(Xi) < ds k(X)) ≥ 1 − α

(cid:6)k
l=k−s+1 |ei(l)|γ

Compute k-NN length ds k(X) =
if

(1/N )

Xi∈XN

Declare X to be anomalous

Declare X to be non-anomalous

then

else

end if
end for

that is asymptotically consistent  guaranteeing that the BP-kNNG detector is a consistent novelty
(cid:2)
detector.
Speciﬁcally  for a given test point X0  the true p-value associated with a point X 0 in a minimum
S(X0) f0(z)dz where S(X0) = {z : f0(z) ≤ f0(X0)} and
volume set test is given by ptrue(X0) =
E(X0) = {z : f0(z) = f0(X0)}. ptrue(X0) is the minimal level α at which X0 would be rejected.
(cid:6)
The empirical p-value associated with the BP-kNNG is deﬁned as

pbp(X0) =

Xi∈XN 1(ds k(Xi) ≥ ds k(X0))

N

.

(1)

4.2 Asymptotic consistency and optimal convergence rates

Here we prove that the BP-kNNG detector is asymptotically consistent by showing that for a ﬁxed
number of edges s  E[(pbp(X0) − ptrue(X0))2] → 0 as k/M → 0  k  N → ∞. In the process 
we also obtain rates of convergence of this mean-squared error. These rates depend on k  N and M
and result in the speciﬁcation of an optimal number of neighbors k and an optimal partition ratio
N/M that achieve the best trade-off between bias and variance of the p-value estimates p bp(X0).
We assume that the density f0 (i) is bounded away from 0 and ∞ and is continuous on its support
S  (ii) has no ﬂat spots over its support set and (iii) has a ﬁnite number of modes. Let E denote the
expectation w.r.t. the density f0  and B  V denote the bias and variance operators. Throughout this
section  assume without loss of generality that {X 1  . . .   XN} ∈ XN and {XN +1  . . .   XT} ∈ XM .
(cid:6)
Xi∈XN 1(f0(Xi) ≤ f0(X0))
Bias: We ﬁrst introduce the oracle p-value p orac(X0) = (1/N )
and note that E[porac(X0)] = ptrue(X0). The distance ei(l) of a point Xi ∈ XN to its l-th
nearest neighbor in XM is related to the bipartite l-nearest neighbor density estimate ˆfl(Xi) =
(l − 1)/(M cded

i(l)) (section 2.3  [11]) where c d is the unit ball volume in d dimensions. Let

(cid:7)

(cid:8)

k(cid:5)

l=k−s+1

k − 1
l − 1

ˆfl(X)

(cid:10)

(cid:9)ν−1

e(X) =

− s(f (X))ν−1

and

We then have

δ(Xi  X0) = δi = (f (Xi))ν−1 − (f (X0))ν−1.

B[pbp(X0)] = E[pbp(X0)] − ptrue(X0) = E[pbp(X0) − porac(X0)]
= E[1(ds k(X1) ≥ ds k(X0))] − E[1(f (X1) ≤ f (X0))]
= E[1(e(X1) − e(X0) + δ1 ≤ 0) − 1(δ1 ≤ 0)].

5

This bias will be non-zero when 1(e(X1) − e(X0) + δ1 ≤ 0) (cid:2)= 1(δ1 ≤ 0). First we investigate
this condition when δ1 > 0. In this case  for 1(e(X1) − e(X0) + δ1 ≤ 0) (cid:2)= 1(δ1 ≤ 0)  we need
−e(X1) + e(X0) ≥ δ1. Likewise  when δ1 ≤ 0  1(e(X1) − e(X0) + δ1 ≤ 0) (cid:2)= 1(δ1 ≤ 0) occurs
when e(X1) − e(X0) > |δ1|.
From the theory developed in [11]  for any ﬁxed s  |e(X)| = O(k/M ) 1/d + O(1/
ility greater than 1 − o(1/M ). This implies that

√
k) with probab-

B[pbp(X0)] = E[1(e(X1) − e(X0) + δ1 ≤ 0) − 1(δ1 ≤ 0)]

= P r{|δ1| = O((k/M )1/d + 1/

√
k)} + o(1/M ) = O((k/M )1/d + 1/

√
k)  (2)
where the last step follows from our assumption that the density f 0 is continuous and has a ﬁnite
number of modes.
Variance: Deﬁne bi = 1(e(Xi) − e(X0) + δi ≤ 0) − 1(δi ≤ 0). We can compute the variance
in a similar manner to the bias as follows (for additional details  please refer to the supplementary
material):

V[pbp(X0)] =

V[1(e(X1) − e(X0) + δ1 ≤ 0)] +

1
N

N − 1
N

Cov[b1  b2]

= O(1/N ) + E[b1b2] − (E[b1]E[b2]) = O(1/N + (k/M )2/d + 1/k).

(3)

Consistency of p-values: From (2) and (3)  we obtain an asymptotic representation of the estim-
ated p-value E[(pbp(X0) − ptrue(X0))2] = O((k/M )2/d) + O(1/k) + O(1/N ). This implies that
pbp converges in mean square to p true  for a ﬁxed number of edges s  as k/M → 0  k  N → ∞.

Optimal choice of parameters: The optimal choice of k to minimize the MSE is given by k =
Θ(M 2/(2+d)). For ﬁxed M + N = T   to minimize MSE  N should then be chosen to be of the
order O(M (4+d)/(4+2d))  which implies that M = Θ(T ). The mean square convergence rate for
this optimal choice of k and partition ratio N/M is given by O(T −2/(2+d)). In comparison  the
K-LPE method requires that k grows with the sample size at rate k = Θ(T 2/5). The mean square
rate of convergence of the p-values in K-LPE is then given by O(T −2/5 + T −6/5d). The rate of
convergence of the p-values is therefore faster in the case of BP-kNNG as compared to K-LPE.

4.3 Comparison of run time complexity

(cid:3)

(cid:4)

T
K

Here we compare complexity of BP-kNNG with that of K-kNNG  L1O-kNNG and K-LPE. For a
single query point X  the runtime of K-kNNG is O(dK 2
)  while the complexity of the surrogate
L1O-kNN algorithm and the K-LPE is O(dT 2). On the other hand  the complexity of the proposed
BP-kNNG algorithm is dominated by the computation of d k(Xi) for each Xi ∈ XN and dk(X) 
which is O(dN M ) = O(dT (8+3d)/(4+2d)) = o(dT 2).
For the K-kNNG  L1O-kNNG and K-LPE  a new k-NN graph has to be constructed on {X N ∪{X}}
for every new query point X. On the other hand  because of the bipartite construction of our k-NN
graph  dk(Xi) for each Xi ∈ XN needs to be computed and stored only once. For every new query
X that comes in  the cost to compute dk(X) is only O(dM ) = O(dT ). For a total of L query points 
the overall runtime complexity of our algorithm is therefore much smaller than the L1O-kNNG  K-
LPE and K-kNNG anomaly detection schemes (O(dT (T (4+d)/(4+2d) + L)) compared to O(dLT 2) 
O(dLT 2) and O(dLK 2

) respectively).

(cid:3)

(cid:4)

T
K

5 Simulation comparisons

We compare the L1O-kNNG and the bipartite K-kNNG schemes on a simulated data set. The
training set contains 1000 realizations drawn from a 2-dimensional Gaussian density f 0 with mean
0 and diagonal covariance with identical component variances of σ = 0.1. The test set contains 500
realizations drawn from 0.8f0 + 0.2U   where U is the uniform density on [0  1] 2. Samples from the
uniform distribution are classiﬁed to be anomalies. The percentage of anomalies in the test set is
therefore 20%.

6

0.98

0.97

0.96

0.95

0.94

0.93

0.92

t

e
a
r
 
e
v
i
t
i
s
o
p

 

e
u
r
T

 

0.91
0

0.02

0.04

0.06

0.1
False positive rate

0.08

 

BP−kNNG
L10−kNNG
Clairvoyant

0.12

0.14

0.16

d
e
v
r
e
s
b
O

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

 

0
0

 

0.02

0.04

0.06

0.08

Desired 

0.1

0.12

0.14

0.16

BP−kNNG
L10−kNNG

(a) ROC curves for L1O-kNNG and BP-kNNG.
The labeled ’clairvoyant’ curve is the ROC of the
UMP anomaly detector..

(b) Comparison of observed false alarm rates for
L1O-kNNG and BP-kNNG with the desired false
alarm rates.

Figure 1: Comparison of performance of L1O-kNNG and BP-kNNG.

Data set
HTTP (KDD’99)
Forest
Mulcross
SMTP (KDD’99)
Shuttle

Sample size Dimension Anomaly class
attack (0.4%)
class 4 vs class 2 (0.9%)
2 clusters (10%)
attack (0.03%)
class 2 3 5 6 7 vs class 1 (7%)

567497
286048
262144
95156
49097

3
10
4
3
9

Table 1: Description of data used in anomaly detection experiments.

√
The distribution f0 has essential support on the unit square. For this simple case the minimum
volume set of level α is a disk centered at the origin with radius
2σ2 log(1/α). The power of the
uniformly most powerful (UMP) test is 1 − 2πσ 2 log(1/α).
L1O-kNNG and BP-kNNG were implemented in Matlab 7.6 on an 2 GHz Intel processor with
3 GB of RAM. The value of k was set to 5. For the BP-kNNG  we set s = 1  N = 100 and
M = 900.
In Fig. 1(a)  we compare the detection performance of L1O-kNNG and BP-kNNG
against the ’clairvoyant’ UMP detector in terms of the ROC. We note that the proposed BP-kNNG
is closer to the optimal UMP test as compared to the L1O-kNNG. In Fig. 1(b) we note the close
agreement between desired and observed false alarm rates for BP-kNNG. Note that the L1O-kNNG
signiﬁcantly underestimates its false alarm rate for higher levels of true false alarm. In the case
of the L1O-kNNG  it took an average of 60ms to test each instance for possible anomaly. The
total run-time was therefore 60x500 = 3000ms. For the BP-kNNG  for a single instance  it took an
average of 57ms. When all the instances were processed together  the total run time was only 97ms.
This signiﬁcant savings in runtime is due to the fact that the bipartite graph does not have to be
constructed separately for each new test instance; it sufﬁces to construct it once on the entire data
set.

5.1 Experimental comparisons

In this section  we compare our algorithm to several other state of the art anomaly detection al-
gorithms  namely: MassAD [12]  isolation forest (or iForest) [5]  two distance-based methods
ORCA [2] and K-LPE [13]  a density-based method LOF [3]  and the one-class support vector
machine (or 1-SVM) [9]. All the methods are tested on the ﬁve largest data sets used in [5]. The
data characteristics are summarized in Table 1. One of the anomaly data generators is Mulcross [8]
and the other four are from the UCI repository [1]. Full details about the data can be found in [5].

The comparison performance is evaluated in terms of averaged AUC (area under ROC curve) and
processing time (a total of training and test time). Results for BP-kNNG are compared with results
for L1O-kNNG  K-LPE  MassAD  iForest and ORCA in Table 2. The results for MassAD  iForest
and ORCA are reproduced from [12]. MassAD and iForest were implemented in Matlab and tested
on an AMD Opteron machine with a 1.8 GHz processor and 4 GB memory. The results for ORCA 

7

Data sets

HTTP
Forest

Mulcross
SMTP
Shuttle

BP
0.99
0.86
1.00
0.90
0.99

L10
NA
NA
NA
NA
NA

AUC

K-LPE Mass
1.00
0.91
0.99
0.86
0.99

NA
NA
NA
NA
NA

iF
1.00
0.87
0.96
0.88
1.00

ORCA
0.36
0.83
0.33
0.87
0.60

BP
3.81
7.54
4.68
0.74
1.54

L10
.10/i
.18/i
.26/i
.11/i
.45/i

Time (secs)

K-LPE Mass
34
.19/i
18
.18/i
17
.17/i
7
.17/i
.16/i
4

iF
147
79
75
26
15

ORCA
9487
6995
2512
267
157

Table 2: Comparison of anomaly detection schemes in terms of AUC and run-time for BP-kNNG
(BP) against L1O-kNNG (L10)  K-LPE  MassAD (Mass)  iForest (iF) and ORCA. When reporting
results for L1O-kNNG and K-LPE  we report the processing time per test instance (/i). We are
unable to report the AUC for K-LPE and L1O-kNNG because of the large processing time. We note
that BP-kNNG compares favorably in terms of AUC while also requiring the least run-time.

Data sets

HTTP (KDD’99)

Forest

Mulcross

SMTP (KDD’99)

Shuttle

0.01
0.007
0.009
0.008
0.006
0.026

Desired false alarm
0.1
0.136
0.071
0.096
0.099
0.079

0.05
0.063
0.035
0.040
0.046
0.045

0.02
0.015
0.015
0.014
0.017
0.030

0.2
0.216
0.150
0.186
0.204
0.179

Table 3: Comparison of desired and observed false alarm rates for BP-kNNG. There is good agree-
ment between the desired and observed rates.

LOF and 1-SVM were conducted using the same experimental setting but on a faster 2.3 GHz
machine. We exclude the results for LOF and 1-SVM in table 2 because MassAD  iForest and
ORCA have been shown to outperform LOF and 1-SVM in [12].

We implemented BP-kNNG  L1O-kNNG and K-LPE in Matlab on an Intel 2 GHz processor with 3
GB RAM. We note that this machine is comparable to the AMD Opteron machine with a 1.8 GHz
processor. We choose T = 104 training samples and ﬁx k = 50 in all three cases. For BP-kNNG 
we ﬁx s = 5 and N = 103. When reporting results for L1O-kNNG and K-LPE  we report the
processing time per test instance (/i). We are unable to report the AUC for K-LPE because of the
large processing time and for L1O-kNNG because it cannot operate at high false alarm rates.

From the results in Table 2  we see that BP-kNNG performs comparably in terms of AUC to the
other algorithms  while having the least processing time across all algorithms (implemented on
different  but comparable machines). In addition  BP-kNNG allows the speciﬁcation of a threshold
for anomaly detection at a desired false alarm rate. This is corroborated by the results in Table 3 
where we see that the observed false alarm rates across the different data sets are close to the desired
false alarm rate.

6 Conclusions

The geometric entropy minimization (GEM) principle was introduced in [4] to extract minimal set
coverings that can be used to detect anomalies from a set of training samples. In this paper we
propose a bipartite k-nearest neighbor graph (BP-kNNG) anomaly detection algorithm based on the
GEM principle. BP-kNNG inherits the theoretical optimality properties of GEM methods including
consistency  while being an order of magnitude faster than the methods proposed in [4].

We compared BP-kNNG against state of the art anomaly detection algorithms and showed that BP-
kNNG compares favorably in terms of both ROC performance and computation time. In addition 
BP-kNNG enjoys several other advantages including the ability to detect anomalies at a desired false
alarm rate. In BP-kNNG  the p-values of each test point can also be easily computed (1)  making
BP-kNNG easily extendable to incorporating false discovery rate constraints.

8

References

[1] A. Asuncion and D.J. Newman. UCI machine learning repository  2007.
[2] S. D. Bay and M. Schwabacher. Mining distance-based outliers in near linear time with ran-
domization and a simple pruning rule. In Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining  KDD ’03  pages 29–38  New York  NY 
USA  2003. ACM.

[3] M. M. Breunig  H. Kriegel  R. T. Ng  and J. Sander. Lof: identifying density-based local
outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management
of data  SIGMOD ’00  pages 93–104  New York  NY  USA  2000. ACM.

[4] A. O. Hero. Geometric entropy minimization (gem) for anomaly detection and localization. In
Proc. Advances in Neural Information Processing Systems (NIPS  pages 585–592. MIT Press 
2006.

[5] F. T. Liu  K. M. Ting  and Z. Zhou. Isolation forest. In Proceedings of the 2008 Eighth IEEE
International Conference on Data Mining  pages 413–422  Washington  DC  USA  2008. IEEE
Computer Society.

[6] C. Park  J. Z. Huang  and Y. Ding. A computable plug-in estimator of minimum volume sets

for novelty detection. Operations Research  58(5):1469–1480  2010.

[7] S. Ramaswamy  R. Rastogi  and K. Shim. Efﬁcient algorithms for mining outliers from large

data sets. SIGMOD Rec.  29:427–438  May 2000.

[8] D. M. Rocke and D. L. Woodruff. Identiﬁcation of Outliers in Multivariate Data. Journal of

the American Statistical Association  91(435):1047–1061  1996.

[9] B. Sch¨olkopf  R. Williamson  A. Smola  J. Shawe-Taylor  and J.Platt. Support Vector Method

for Novelty Detection. volume 12  2000.

[10] C. Scott and R. Nowak. Learning minimum volume sets. J. Machine Learning Res  7:665–704 

2006.

[11] K. Sricharan  R. Raich  and A. O. Hero. Empirical estimation of entropy functionals with

conﬁdence. ArXiv e-prints  December 2010.

[12] K. M. Ting  G. Zhou  T. F. Liu  and J. S. C. Tan. Mass estimation and its applications. In
Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and
data mining  KDD ’10  pages 989–998  New York  NY  USA  2010. ACM.

[13] M. Zhao and V. Saligrama. Anomaly detection with score functions based on nearest neighbor

graphs. Computing Research Repository  abs/0910.5461  2009.

9

,Vibhav Vineet
Carsten Rother
Philip Torr
Changyou Chen
Jun Zhu
Xinhua Zhang
Sida Wang
Arun Tejasvi Chaganty
Percy Liang
Gamaleldin Elsayed
Simon Kornblith
Quoc Le