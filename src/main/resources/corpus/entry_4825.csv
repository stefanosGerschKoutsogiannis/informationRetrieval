2019,Safe Exploration for Interactive Machine Learning,In interactive machine learning (IML)  we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods  e.g.  Bayesian optimization and active learning  have been successful in applications  on real-world systems they must provably avoid unsafe decisions. To this end  safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient  as it explores decisions that are not relevant for the original IML objective. In this paper  we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result  we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP)  which have been analyzed separately before. Our method outperforms other algorithms empirically.,Safe Exploration for Interactive Machine Learning

Matteo Turchetta

Dept. of Computer Science

ETH Zurich

matteotu@inf.ethz.ch

befelix@inf.ethz.ch

Felix Berkenkamp

Dept. of Computer Science

ETH Zurich

Andreas Krause

Dept. of Computer Science

ETH Zurich

krausea@ethz.ch

Abstract

In Interactive Machine Learning (IML)  we iteratively make decisions and obtain
noisy observations of an unknown function. While IML methods  e.g.  Bayesian
optimization and active learning  have been successful in applications  on real-
world systems they must provably avoid unsafe decisions. To this end  safe IML
algorithms must carefully learn about a priori unknown constraints without making
unsafe decisions. Existing algorithms for this problem learn about the safety of all
decisions to ensure convergence. This is sample-inefﬁcient  as it explores decisions
that are not relevant for the original IML objective. In this paper  we introduce a
novel framework that renders any existing unsafe IML algorithm safe. Our method
works as an add-on that takes suggested decisions as input and exploits regularity
assumptions in terms of a Gaussian process prior in order to efﬁciently learn about
their safety. As a result  we only explore the safe set when necessary for the
IML problem. We apply our framework to safe Bayesian optimization and to safe
exploration in deterministic Markov Decision Processes (MDP)  which have been
analyzed separately before. Our method outperforms other algorithms empirically.

1

Introduction

Interactive Machine Learning (IML) problems  where an autonomous agent actively queries an
unknown function to optimize it  learn it  or otherwise act based on the observations made  are
pervasive in science and engineering. For example  Bayesian optimization (BO) (Mockus et al.  1978)
is an established paradigm to optimize unknown functions and has been applied to diverse tasks
such as optimizing robotic controllers (Marco et al.  2017) and hyperparameter tuning in machine
learning (Snoek et al.  2012). Similarly  Markov Decision Processes (MDPs) (Puterman  2014) model
sequential decision making problems with long term consequences and are applied to a wide range of
problems including ﬁnance and management of water resources (White  1993).
However  real-world applications are subject to safety constraints  which cannot be violated during
the learning process. Since the dependence of the safety constraints on the decisions is unknown a
priori  existing algorithms are not applicable. To optimize the objective without violating the safety
constraints  we must carefully explore the space and ensure that decisions are safe before evaluating
them. In this paper  we propose a data-efﬁcient algorithm for safety-constrained IML problems.
Related work One class of IML algorithms that consider safety are those for BO with Gaussian
Process (GP) (Rasmussen  2004) models of the objective. While classical BO algorithms focus
on efﬁcient optimization (Srinivas et al.  2010; Thompson  1933; Wang and Jegelka  2017)  these
methods have been extended to incorporate safety constraints. For example  Gelbart et al. (2014)
present a variant of expected improvement with unknown constraints  while Hernández-Lobato et al.
(2016) extend an information-theoretic BO criterion to handle black-box constraints. However  these
methods only consider ﬁnding a safe solution  but allow unsafe evaluations during the optimization
process. Wu et al. (2016) deﬁne safety as a constraint on the cumulative reward  while Schreiter et al.
(2015) consider the safe exploration task on its own. The algorithms SAFEOPT (Sui et al.  2015;

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Set illustration.

(b) STAGEOPT.

(c) GP-UCB + GOOSE (ours)

Figure 1: Existing algorithms for safe IML aim to expand the safe set ¯Sp (green shaded) in Fig. 1a by
evaluating decisions on the boundary of the pessimistic safe set (dark green shaded). This can be
inefﬁcient: to solve the safe BO problem in Fig. 1b  STAGEOPT evaluates decisions (green crosses 
histogram) close to the safety constraint q(·) > 0 (black dashed)  even though the maximum (black
cross) is known to be safe. In contrast  our method uses decisions x?
i from existing unsafe IML
algorithms (oracle) within the optimistic safe set ¯So ✏
(blue shaded  Fig. 1a). It can then use any
heuristic to select learning targets At (blue cross) that are informative about the safety of x?
i and
learns about them efﬁciently within Gt ✓ ¯Sp
t (blue shaded region). Since this method only learns
about the safe set when necessary  we evaluate more close-to-optimal decisions in Fig. 1c.

t

Berkenkamp et al.  2016) and STAGEOPT (Sui et al.  2018) both guarantee safety of the exploration
and near-optimality of the solution. However  they treat the exploration of the safe set as a proxy
objective  which leads to sample-inefﬁcient exploration as they explore the entire safe set  even if this
is not necessary for the optimization task  see the evaluation counts (green) in Fig. 1b for an example.
Safety has also been investigated in IML problems in directed graphs  where decisions have long-term
effects in terms of safety. Moldovan and Abbeel (2012) address this problem in the context of discrete
MDPs by optimizing over ergodic policies  i.e.  policies that are able to return to a known set of
safe states with high probability. However  they do not provide exploration guarantees. Biyik et al.
(2019) study the ergodic exploration problem in discrete and deterministic MDPs with unknown
dynamics and noiseless observations. Turchetta et al. (2016) investigate the ergodic exploration
problem subject to unknown external safety constraints under the assumption of known dynamics by
imposing additional ergodicity constraints on the SAFEOPT algorithm. Wachi et al. (2018) compute
approximately optimal policies in the same context but do not actively learn about the constraint. In
continuous domains  safety has been investigated by  for example  Akametalu et al. (2014); Koller
et al. (2018). While these methods provide safety guarantees  current exploration guarantees rely on
uncertainty sampling on a discretized domain (Berkenkamp et al.  2017). Thus  their analysis can
beneﬁt from the more efﬁcient  goal-oriented exploration introduced in this paper.
Contribution
In this paper  we introduce the Goal Oriented Safe Exploration algorithm  GOOSE;
a novel framework that works as an add-on to existing IML algorithms and renders them safe. Given
a possibly unsafe suggestion by an IML algorithm  it safely and efﬁciently learns about the safety of
this decision by exploiting continuity properties of the constraints in terms of a GP prior. Thus  unlike
previous work  GOOSE only learns about the safety of decisions relevant for the IML problem. We
analyze our algorithm and prove that  with high probability  it only takes safe actions while learning
about the safety of the suggested decisions. On safe BO problems  our algorithm leads to a bound on
a natural notion of safe cumulative regret when combined with a no-regret BO algorithm. Similarly 
we use our algorithm for the safe exploration in deterministic MDPs. Our experiments show that
GOOSE is signiﬁcantly more data-efﬁcient than existing methods in both settings.

2 Problem Statement and Background

In IML  an agent iteratively makes decisions and observes their consequences  which it can use
to make better decisions over time. Formally  at iteration i  the agent Oi uses the previous i  1
observations to make a new decision x?
i = Oi(Di) from a ﬁnite decision space Di ✓D✓ Rd. It
then observes a noisy measurement of the unknown objective function f : D! R and uses the new
information in the next iteration. This is illustrated in the top-left corner (blue shaded) of Fig. 2.
Depending on the goal of the agent  this formulation captures a broad class of problems and many
solutions to these problems have been proposed. For example  in Bayesian optimization the agent
aims to ﬁnd the global optimum maxx f (x) (Mockus et al.  1978). Similarly  in active learning

2

DomainDf(x)q(x)q(x)0#evaluationsDomainDFigure 2: Overview of GOOSE. If the oracle’s suggestion x?
i is safe  it can be evaluated. This is
equivalent to the standard unsafe IML pipeline (top-left  blue shaded) in Fig. 2. Otherwise  GOOSE
learns about the safety of x?
i by actively querying observations at decisions xt. Any provably unsafe
decision is removed from the decision space and we query a new x?
i without providing a new
observation of f (x?
i ).

(Schreiter et al.  2015)  one aims to learn about the function f. In the general case  the decision
process may be stateful  e.g.  as in dynamical systems  so that the decisions Di available to the agent
depend on those made in the past. This dependency among decisions can be modeled with a directed
graph  where nodes represent decisions and an edge connects node x to node x0 if the agent is allowed
to evaluate x0 given that it evaluated x at the previous decision step. In the BO setting  the graph is
fully-connected and any decision may be evaluated  while in a deterministic MDP decisions are states
and edges represent transitions (Turchetta et al.  2016).
In this paper  we consider IML problems with safety constraints  which frequently occur in real-world
settings. The safety constraint can be written as q(x)  0 for some function q. Any decision x?
i for
i  1 evaluated by the agent must be safe. For example  Berkenkamp et al. (2016) optimize the
control policy of a ﬂying robot and must evaluate only policies that induce trajectories satisfying
given constraints. However  it is unknown a priori which policy parameters induce safe trajectories.
Thus  we do not know which decisions are safe in advance  that is  q : D! R is a priori unknown.
However  we can learn about the safety constraint by selecting decisions xt and obtaining noisy
observations of q(xt). We denote queries to f with x?
i and queries to q with xt. As a result  we face a
two-tiered safe exploration problem: On one hand we have to safely learn about the constraint q to
determine which decisions are safe  while on the other hand we want to learn about f to solve the
IML problem. The goal is to minimize the number of queries xt required to solve the IML problem.
Regularity Without further assumptions  it is impossible to evaluate decisions without violating
the safety constraint q (Sui et al.  2015). For example  without an initial set of decisions that is
known to be safe a priori  we may fail at the ﬁrst step. Moreover  if the constraint does not exhibit
any regularity  we cannot infer the safety of decisions without evaluating them ﬁrst. We assume
that a small initial safe set of decisions  S0  is available  which may come from domain knowledge.
Additionally  we assume that D is endowed with a positive deﬁnite kernel function  k(· ·)  and that
the safety constraint q has bounded norm in the induced Reproducing Kernel Hilbert Space (RKHS)
(Schölkopf and Smola  2002))  kqkk  Bq. The RKHS norm measures the smoothness of the safety
feature with respect to the kernel  so that q is L-Lipschitz continuous with respect to the kernel metric
d(x  x0) =pk(x  x)  2k(x  x0) + k(x0  x0) with L = Bq (Steinwart and Christmann  2008  (4.21))
This assumption allows us to model the safety constraint function q with a GP (Rasmussen  2004).
A GP is a distribution over functions parameterized by a mean function µ(·) and a covariance
function k(· ·). We set µ(x) = 0 for all x 2D without loss of generality. The covariance function
encodes our assumptions about the safety constraint. Given t observations of the constraint y =
(q(x1) + ⌘1  . . .   q(xt) + ⌘t) at decisions Dt = {xn}t
n=1  where ⌘n ⇠N (0  2) is a zero-mean i.i.d.
Gaussian noise  the posterior belief is distributed as a GP with mean  covariance  and variance
µt(x) = kT
t (x)(Kt +2I)1kt(x0)  t(x) = kt(x  x)
t (x)(Kt +2I)1yt  kt(x  x0) = k(x  x0)kT
respectively. Here  kt(x) = (k(x1  x)  . . .   k(xt  x))  Kt is the positive deﬁnite kernel matrix
[k(x  x0)]x x02Dt  and I 2 Rt⇥t denotes the identity matrix.
Safe decisions
The previous regularity assumptions can be used to determine which decisions
are safe to evaluate. Our classiﬁcation of the decision space is related to the one by Turchetta
et al. (2016)  which combines non-increasing and reliable conﬁdence intervals on q with a reach-
ability analysis of the underlying graph structure for decisions. Based on a result by Chowd-
hury and Gopalan (2017)  they use the posterior GP distribution to construct conﬁdence bounds

3

lt(x) := max(lt1(x)  µt1(x)  tt1(x)) and ut(x) := min(ut1(x)  µt1(x) + tt1(x)) on
the function q. In particular  we have lt(x)  q(x)  ut(x) with high probability when the scaling
factor t is chosen as in Theorem 1. Thus  any decision x with lt(x)  0 is satisﬁes the safety
constraint q(x)  0 with high probability.
To analyze the exploration behavior of their algorithm  Turchetta et al. (2016) use the conﬁdence
intervals within the current safe set  starting from S0  and the Lipschitz continuity of q to deﬁne Sp
t  
the set of decisions that satisfy the constraint with high probability. We use a similar  albeit more
efﬁcient  deﬁnition in Sec. 3. In practice  one may use the conﬁdence intervals directly. Moreover 
in order to avoid exploring decisions that are instantaneously safe but that would force the agent to
eventually evaluate unsafe ones due to the graph structure G  Turchetta et al. (2016) deﬁne ¯Sp
t   the
subset of safe and ergodic decisions  i.e.  decisions that are safe to evaluate in the short and long term.
Previous Exploration Schemes Given that only decisions in ¯Sp
t are safe to evaluate  any safe
IML algorithm faces an extended exploration-exploitation problem: it can either optimize decisions
within ¯Sp
t by evaluating decisions on its boundary. Existing
solutions to the safe exploration problem in both discrete and continuous domains either do not
provide theoretical exploration guarantees (Wachi et al.  2018) or treat the exploration of the safe
set as a proxy objective for optimality. That is  the methods uniformly reduce uncertainty on the
boundary of the safe set in Fig. 1a until the entire safe set is learned. Since learning about the entire
safe set is often unnecessary for the IML algorithm  this procedure can be sample-inefﬁcient. For
example  in the safe BO problem in Fig. 1b with f = q  this exploration scheme leads to a large
number of unnecessary evaluations on the boundary of the safe set.

t   or expand the set of safe decisions in ¯Sp

3 Goal-oriented Safe Exploration (GOOSE)

i   GOOSE only evaluates f (x?

i ) if the decisions x?

i )  or that x?

i 2 S  see Fig. 2 (blue shaded).

i is safe and allows the oracle to evaluate f (x?

In this section  we present our algorithm  GOOSE. We do not propose a new safe algorithm for a
speciﬁc IML setting  but instead exploit that  for speciﬁc IML problems high-performance  unsafe
algorithms already exist. We treat any such unsafe algorithm as an IML oracle Oi(S)  which  given a
domain S and i  1 observations of f  suggests a new decision x?
GOOSE can extend any such unsafe IML algorithm to the safety-constrained setting. Thus  we
effectively leave the problem of querying f to the oracle and only consider safety. Given an unsafe
oracle decision x?
i is known to be safe. Otherwise 
i ) by safely and efﬁciently collecting observations q(xt). Eventually it
it safely learns about q(x?
either learns that the decision x?
i cannot be
guaranteed to be safe given an ✏-accurate knowledge of the constraint  in which case the decision set
of the oracle is restricted and a new decision is queried  see Fig. 2.
Previous approaches treat the expansion of the safe set as a proxy-objective to provide completeness
guarantees. Instead  GOOSE employs goal-directed exploration scheme with a novel theoretical
analysis that shifts the focus from greedily reducing the uncertainty inside the safe set to learning
about the safety of decisions outside of it. This scheme retains the worst-case guarantees of existing
methods  but is signiﬁcantly more sample-efﬁcient in practice. Moreover  GOOSE encompasses
existing methods for this problem. We now describe the detailed steps of GOOSE in Alg. 1 and 2.
Pessimistic and optimistic expansion.
To effectively shift the focus from inside the safe set to
outside of it  GOOSE must reason not only about the decisions that are currently known to be safe
but also about those that could eventually be classiﬁed as safe in the future. In particular  it maintains
two sets  which are an inner/outer approximation of the set of safe decisions that are reachable from
S0 and are based on a pessimistic/optimistic estimate of the constraint given the data  respectively.
The pessimistic safe set contains the decisions that are safe with high probability and is necessary to
guarantee safe exploration (Turchetta et al.  2016). It is deﬁned in two steps: discarding the decisions
that are not instantaneously safe and discarding those that we cannot reach/return from safely (see
Fig. 3b) and  thus  are not safe in the long term. To characterize it starting from a given set of safe
decisions S  we deﬁne the pessimistic constraint satisfaction operator 

pt(S) = {x 2D   |9 z 2 S : lt(z)  Ld(x  z)  0} 

(1)
which uses the lower bound on the safety constraint of the decisions in S and the Lipschitz continuity
of q to determine the decisions that instantaneously satisfy the constraint with high probability 
see Fig. 3a. However  for a general graph G  decisions in pt(S) may be unsafe in the long-term

4

(a) Expansion operators.

(b) Long term safety in graph.

Figure 3: Fig. 3a shows the pessimistic and optimistic constraint satisfaction operators that use the
conﬁdence intervals on the constraint and its Lipschitz continuity to make inference about the safety
of decisions that have not yet been evaluated. Fig. 3b illustrates the long-term safety deﬁnition. While
decisions in pt(S) are myopically safe  decisions in P 1
t (S) are safe in the long-term. This excludes
x4 and x5  as no safe path from/to them exists.

as in Fig. 3b: No safe path to the decision x5 exists  so that it can not be safely reached. Sim-
ilarly  if we were to evaluate x4  the graph structure forces us to eventually evaluate x3  which
is not contained in pt(S) and might be unsafe. That is  we cannot safely return from x4. To
exclude these decisions  we use the ergodicity operator introduced by Turchetta et al. (2016) 
which allows us to ﬁnd those decisions that are pessimistically safe in the short and long term
t (S) = pt(S) \ Rergodic(pt(S)  S) (see Appendix A or (Turchetta et al.  2016) for the deﬁnition of
P 1
Rergodic). Alternating these operations n times  we obtain the n-step pessimistic expansion operator 
t (S) = pt(P n1
(S))  S)  which  after a ﬁnite number of steps  converges
P n
to its limiting set ˜Pt(S) = limn!1 P n
The optimistic safe set excludes the decisions that are unsafe with high probability and makes the
exploration efﬁcient by restricting the decision space of the oracle. Similarly to the pessimistic one  it
is deﬁned in two steps. However  it uses the following optimistic constraint satisfaction operator 

(S)) \ Rergodic(pt(P n1
t
t (S).

t

t(S) = {x 2D   |9 z 2 S : ut(z)  Ld(x  z)  ✏  0}.
o✏

(2)

t

The IML oracle Oi(S) suggests decisions x?

t (S) are analogous to the pessimistic case by substituting pt with o✏

See Fig. 3a for a graphical intuition. The additional ✏-uncertainty term in the optimistic operator
accounts for the fact that we only have access to noisy measurements of the constraint and  therefore 
we can only learn it up to a speciﬁed statistical accuracy. The deﬁnitions of the optimistic expansion
(S) and ˜O✏
operators O✏ n
t. The
sets ˜Pt and ˜O✏
t indicate the largest set of decisions that can be classiﬁed as safe in the short and long
term assuming the constraint attains the worst/best possible value within S  given the observations
available and  for the optimistic case  despite an ✏ uncertainty.
Optimistic oracle
i 2 S to evaluate within a given
subset S of D. To make the oracle efﬁcient  we restrict its decision space to decisions that could
optimistically be safe in the long and short term. In particular  we deﬁne the optimistic safe set ¯So ✏
in Line 8 of Alg. 1 based on the optimistic expansion operator introduced above. The oracle uses this
set to suggest a potentially unsafe  candidate decision x?
Safe evaluation We determine safety of the suggestion x?
i similarly to Turchetta et al. (2016) by
constructing the set ¯Sp
t of decisions that are safe to evaluate. However  while Turchetta et al. (2016)
use the one step pessimistic expansion operator in their deﬁnition  P 1
t   we use its limit set in Line 7
of Alg. 1  ˜Pt. While both operators eventually identify the same safe set  our deﬁnition allows for a
more efﬁcient expansion. For example  consider the case where the graph over the decision space G
is a chain of length m and where  for all j = 1 ···   m  the lower bound on the safety of decision
j  1 guarantees the safety of decision j with high probability. In this case  Turchetta et al. (2016)
require m  1 iterations to fully expand the safe set  while our classiﬁcation requires only one.
If we know that x?
t   then the oracle obtains a noisy observation of
f (x?
i using a safe
expansion strategy in lines Lines 5–8 that we outline in the following. This routine is repeated until
we can either include x?
i )  or remove it from the
decision space ¯So ✏

i ) in Line 10. Otherwise GOOSE proceeds to safely learn about the safety of x?

i in ¯Sp

t   in which case we can safely evaluate f (x?

i = Oi( ¯So ✏

t ) in Line 4.

and query the oracle for a new suggestion.

t

i is safe to evaluate  i.e.  x?

i 2 ¯Sp

5

Algorithm 1 GOOSE
1: Inputs: Lipschitz constant L  Seed S0 
Graph G  Oracle O  Accuracy ✏.

2: ¯Sp

0 D   t 0 

0 S0  ¯So ✏
l0(x) 0 for x 2 S0
3: for k = 1  2  . . . do
i O ( ¯So ✏
x?
4:
t )
i 62 ¯Sp
while x?
t do
5:
  ¯Sp
SE( ¯So ✏
t  G  x?
6:
t
t ˜Pt( ¯Sp
¯Sp
7:
t1)
¯So ✏
t ˜O✏
t ( ¯Sp
8:
t1)
i 62 ¯So ✏
then go to Line 4
if x?
9:
t
Evaluate f (x?
i ) and update oracle
10:

i )  t t + 1

t

Algorithm 2 Safe Expansion (SE)
1: Inputs: ¯So ✏
2: W ✏
3: At(p) { x 2 ¯So ✏
t )| h(x) = p}
4: // Highest priority targets in At with expanders
↵⇤ max ↵ s.t.
xt argmaxx2G✏
Update GP with yt = q(xt) + ⌘t

  ¯Sp
t  G  x?
t { x 2 ¯Sp
t | ut(x)  lt(x) >✏ }
t \ p0
|G✏
t(↵)| > 0
t (↵⇤) wt(x)

5: if optimization problem feasible then
6:
7:

t ( ¯Sp

t ( ¯Sp

t

i /2 ¯Sp

t \ p0

If the oracle suggestion x?

i is not considered safe  x?

t that is informative about q(x?

excludes the decisions that are unsafe with high probability  ¯So ✏

Safe expansion
t   GOOSE employs a
goal-directed scheme to evaluate a safe decision xt 2 ¯Sp
i )  see Fig. 1a.
In practice  it is desirable to avoid learning about decisions beyond a certain accuracy ✏  as the number
of observations required to reduce the uncertainty grows exponentially with ✏ (Sui et al.  2018). Thus 
we only learn about decisions in ¯Sp
t whose safety values are not known ✏-accurately yet in Line 2 
t = {x 2 ¯Sp
t | ut(x)  lt(x) >✏ }  where ut(x)  lt(x) is the width of the conﬁdence interval at x.
W ✏
t to learn about  we ﬁrst determine a set of learning targets outside
To decide which decision in W ✏
the safe set (dark blue cross in Fig. 1a)  and then learn about them efﬁciently within ¯Sp
t . To quantify
how useful a learning target x is to learn about q(x?
i )  we use any given iteration-dependent heuristic
ht(x). We discuss particular choices later  but a large priority h(x) indicates a relevant learning target
t ) denotes the decisions that are known to satisfy the constraint with
(dashed line  Fig. 1a). Since p0
high probability and ¯So ✏
t ( ¯Sp
t )
indicates the decisions whose safety we are uncertain about. We sort them according to their priority
and let At(↵) denote the subset of decision with equal priority.
Ideally  we want to learn about the decisions with the highest priority. However  this may not be
t . Thus  we must identify the decisions with
immediately possible by evaluating decisions within W ✏
t . Therefore  similarly to the deﬁnition of
the highest priority that we can learn about starting from W ✏
the optimistic safe set  we identify decisions x in W ✏
t that have a large enough plausible value q(x) that
they could guarantee that q(z)  0 for some z in At(↵). However  in this case  we are only interested
in decisions that can be instantly classiﬁed as safe (rather than eventually). Therefore  we focus on this
t   |9 z 2 At(↵) : ut(x)  Ld(x  z)  0}.
set of potential immediate expanders  G✏
In Line 4 of Alg. 2 we select the decisions with the priority level ↵⇤ such that there exist uncertain 
t that could allow us to classify a decision in At(↵⇤) as safe and thereby expand
safe decisions in W ✏
the current safe set ¯Sp
t . Intuitively  we look for the highest priority targets that can potentially be
classiﬁed as safe by safely evaluating decisions that we have not already learned about to ✏-accuracy.
Given these learning targets At(↵⇤) (blue cross  Fig. 1a)  we evaluate the most uncertain decision in
t(↵⇤) (blue shaded  Fig. 1a) in Line 6 and update the GP model with the corresponding observation
G✏
of q(xt) in Line 7. This uncertainty sampling is restricted to a small set of decisions close to the
goal. This is different from methods without a heuristic that select the most uncertain secision on the
boundary of ¯Sp (green shaded in Fig. 1a). In fact  our method is equivalent to the one by Turchetta
et al. (2016) when an uninformative heuristic h(x) = 1 is used for all x. We iteratively select and
i is safe  in which case it is added to ¯Sp  or we
evaluate decisions xt until we either determine that x?
prove that we can not safely learn about it for given accuracy ✏  in which case is removed from ¯So ✏
and a the oracle is queried with an updated decision space for a new suggestion.
To analyze our algorithm  we deﬁne the largest set that we can learn about as ˜R✏(S0). This set
contains all the decisions that we could certify as safe if we used a full-exploration scheme that learns
the safety constraint q up to ✏ accuracy for all decisions inside the current safe set. This is a natural
exploration target for our safe exploration problem (see Appendix A for a formal deﬁnition). We
have the following main result  which holds for any heuristic:

t(↵) = {x 2 W ✏

6

t⇤

✏2

k  ¯So ✏

t ✓ ¯Sp

t ✓ ˜R0(S0).

t⇤ t⇤  C | ˜R0(S0)|

Theorem 1. Assume that q(·) is L-Lipschitz continuous w.r.t. d(· ·) with kqkk  Bq  -sub-Gaussian
noise  S0 6= ;  q(x)  0 for all x 2 S0  and that  for any two decisions x  x0 2 S0  there is a path
t = Bq + 4pt + 1 + ln(1/)  then  for any
in the graph G connecting them within S0. Let 1/2
ht : D! R  with probability at least 1    we have q(x)  0 for any x visited by GOOSE. Moreover 
let t denote the information capacity associated with the kernel k and let t⇤ be the smallest integer
such that
  with C = 8/ log(1 + 2)  then there exists a t  t⇤ such that  with
probability at least 1    ˜R✏(S0) ✓ ¯So ✏
Theorem 1 guarantees that GOOSE is safe with high probability. Moreover  for any priority function
h in Alg. 2  it upper bounds the number of measurements that Alg. 1 requires to explore the largest
safely reachable region ˜R✏(S0). Note that GOOSE only achieves this upper bound if it is required by
the IML oracle. In particular  the following is a direct consequence of Theorem 1:
Corollary 1. Under the assumptions of Theorem 1  let the IML oracle be deterministic given the
observations. Then there exists a set S with ˜R✏(S0) ✓ S ✓ ˜R0(S0) so that x?
i = O(S) for all k  1.
That is  the oracle decisions x?
i that we end up evaluating are the same as those by an oracle that was
given the safe set S in Corollary 1 from the beginning. This is true since the set ¯So ✏
converges to this
set S. Since Theorem 1 bounds the number of safety evaluations by t⇤  Corollary 1 implies that  up
to t⇤ safety evaluations  GOOSE retains the properties (e.g.  no-regret) of the IML oracle O over S.
Choice of heuristic While our worst-case guarantees hold for any heuristic  the empirical per-
formance of GOOSE depends on this choice. We propose to use the graph structure directly and
additionally deﬁne a positive cost for each edge between two nodes. For a given edge cost  we deﬁne
k within the optimistic safe set ¯So ✏
c(x  x?
 
t
k  ¯So ✏
which is equal to 1 if a path does not exist  and we consider the priority h(x) = c(x  x?
t ).
Thus  the node x with the lowest-cost path to x?
k has the highest priority. This reduces the design of a
general heuristic to a more intuitive weight assignment problem  where the edge costs determine the
planned path for learning about x?
k (dashed line in Fig. 1a). One option for the edge cost is the inverse
mutual information between x and the suggestion x?
k  so that the resulting paths contain nodes that
are informative about x?
k. Alternatively  having successive nodes in the path close to each other under
the metric d(· ·)  so that they can be easily added to the safe set and eventually lead us to x?
k  can be
desirable. Thus  increasing monotone functions of the metric d(· ·) can be effective edge costs.
4 Applications and Experiments

t ) as the cost of the minimum-cost path from x to x?

t

In this section  we introduce two safety-critical IML applications  discuss the consequences of
Theorem 1 for these problems  and empirically compare GOOSE to stae-of-the-art competing
methods. In our experiments  we set t = 3 for all t  1 as suggested by Turchetta et al. (2016). This
choice of t ensures safety in practice  but leads to more efﬁcient exploration than the theoretical
choice in Theorem 1 (Turchetta et al.  2016; Wachi et al.  2018). Moreover  since in practice it is hard
to estimate the Lipschitz constant of an unknown function  in our experiments we use the conﬁdence
intervals to deﬁne the safe set and the expanders as suggested by Berkenkamp et al. (2016).

4.1 Safe Bayesian optimization

In safe BO we want to optimize the unknown function f subject to the unknown safety constraint
q  see Sec. 2. In this setting  we aim to ﬁnd the best input over the largest set we can hope to
explore safely  ˜R✏(S0). The performance of an agent is measured in terms of the ✏-safe regret
argmaxx2 ˜R✏(S0) f (x)  f (xt) of not having evaluated the function at the optimum in ˜R✏(S0).
We combine GOOSE with the unsafe GP-UCB (Srinivas et al.  2010) algorithm as an oracle. For
computational efﬁciency  we do not use a fully connected graph  but instead connect decisions only
to their immediate neighbors as measured by the kernel and assign equal weight to each edge for the
heuristic h. We compare GOOSE to SAFEOPT (Sui et al.  2015) and STAGEOPT (Sui et al.  2018) in
terms of ✏-safe average regret. Both algorithms use safe exploration as a proxy objective  see Fig. 1.
We optimize samples from a GP with zero mean and Radial Basis Function (RBF) kernel with
variance 1.0 and lengthscale 0.1 and 0.4 for a one-dimensional and two-dimensional  respectively.

7

Table 1: Mars experiment perfor-
mance normalized to SMDP in
terms of samples to ﬁnd the ﬁrst
path  exploration cost and com-
putation time per iteration.

Sample
Cost
Time

GOOSE
SEO
30.0 % 38.4 %
12.7 %
0.7 %
37.8 % 518 %

Figure 4: Average normalized ✏-safe regret for the safe optimiza-
tion of GP samples over 40 (d=1  left) and 10 (d=2  right) samples.
GOOSE only evaluates inputs that are relevant for the BO problem
and  thereofore  it converges faster than its competitors.

(a) Cost of exploration.

(b) Samples to ﬁrst path.

(c) Computation per iteration.

Figure 5: Performance of GOOSE and SEO normalized to SMDP in terms of exploration cost 
samples to ﬁnd the ﬁrst path and computation time per iteration as a function of the world size.

The observations are perturbed by i.i.d Gaussian noise with  = 0.01. For simplicity  we set the
objective and the constraint to be the same  f = q. Fig. 4 (left) shows the average regret as a function
of the number of evaluations k + t averaged over 40 different samples from the GP described above
over a one dimensional domain (200 points evenly distributed in [1  1]). Fig. 4 (right) shows similar
results averaged over 10 samples for a two dimensional domain (25 ⇥ 25 uniform grid in [0  1]2).
These results conﬁrm the intuition from Fig. 1 that using safe exploration as a proxy objective reduces
the empirical performance of safe BO algorithms. The impact is more evident in the two dimensional
case where there are more points along the boundaries that are nor relevant to the optimization and
that are evaluated for exploration purposes.

4.2 Safe shortest path in deterministic MDPs
The graph that we introduced in Sec. 2 can model states (nodes) and state transitions (edges) in
deterministic  discrete MDPs. Hence  GOOSE naturally extends to the goal-oriented safe exploration
problem in these models. We aim to ﬁnd the minimum-cost safe path from a starting state x† to a
goal state x?  without violating the unknown safety constraint  q. At best  we can hope to ﬁnd the
path within the largest safely learnable set ˜R✏(S0) as in Theorem 1 with cost c(x†  x?  ˜R✏(S0)).
Algorithms We compare GOOSE to SEO (Wachi et al.  2018) and SMPD (Turchetta et al. 
2016) in terms of samples required to discover the ﬁrst path  total exploration cost and computation
cost on synthetic and real world data. The SMDP algorithm cannot take goals into account and
serves as a benchmark for comparison. The SEO algorithm aims to safely learn a near-optimal
policy for any given cost function and can be adapted to the safe shortest path problem by setting
the cost to c(x) = kx  x?k1. However  it cannot guarantee that a path to x? is found  if one
exists. Since the goal x? is ﬁxed  GOOSE does not need an oracle. For the heuristic we use and
optimistic estimate of the cost of the safe shortest path from x† to x? passing through x; that is
ht(x) =  minx02P red(x) c(x†  x  ¯Sp
t ). The ﬁrst term is a conservative estimate of
the safe optimal cost from x† to the best predecessor of x in G and the second term is an optimistic
estimate of the safe optimal cost from x to x? multiplied by > 1 to encourage early path discovery.
Here  we use the predecessor node because t(x) = 1 for all x not in ¯Sp
t . Notice that  if a safe path
exists  Theorem 1 guarantees that GOOSE ﬁnds the shortest one eventually.
Synthetic data
Similarly to the setting in Turchetta et al. (2016); Wachi et al. (2018) we construct
a two-dimensional grid world. At every location  the agent takes one of four actions: left  right  up
and down. We use the state augmentation in Turchetta et al. (2016) to deﬁne a constraint over state

t ) + c(x  x?  ¯So ✏

8

0102030Iterations0.00.20.40.6AveragesaferegretSafeOptStageOptGoOSE01020304050Iterations2000400060008000Worldsize0.00.51.0RatiotoSMDPGoOSESMDPSEO2000400060008000Worldsize2000400060008000Worldsize0102030RatiotoSMDPtransitions. The constraint function is a sample from a GP with mean µ = 0.6 and RBF kernel with
lengthscale l = 2 and variance 2 = 1. If the agent takes an unsafe action  it ends up in a failure state 
otherwise it moves to the desired adjacent state. We make the constraint independent of the direction
of motion  i.e.  q(x  x0) = q(x0  x). We generate 800 worlds by sampling 100 different constraints for
square maps with sides of 20  30  40 ···   90 tiles and a source-target pair for each one.
We show the geometric mean of the performance of SEO and GOOSE relative to SMDP as a
function of the world size in Fig. 5. Fig. 5b shows that GOOSE needs a factor 2.5 fewer samples
than SMDP. Fig. 5c shows that the overhead to compute the heuristic of GOOSE is negligible  while
the solution of the two MDPs 1 required by SEO is computationally intense. Fig. 5a shows that SEO
outperforms GOOSE in terms of cost of the exploration trajectory. This is expected as SEO aims
to minimize it  while GOOSE optimizes the sample-efﬁciency. However  it is easy to modify the
heuristic of GOOSE to consider the exploration cost by  for example  reducing the priority of a state
based on its distance from the current location of the agent. In conclusion  GOOSE leads to a drastic
improvement in performance with respect to the previously known safe exploration strategy with
exploration guarantees  SMDP. Moreover  it achieves similar or better performance than SEO while
providing exploration guarantees that SEO lacks.
Mars exploration We simulate the exploration of Mars with a rover. In this context  communi-
cation delays between the rover and the operator on Earth make autonomous exploration extremely
important  while the high degree of uncertainty about the environment requires the agent to consider
safety constraints. In our experiment  we consider the Mars Science Laboratory MSL (2007  Sec.
2.1.3)  a rover deployed on Mars that can climb a maximum slope of 30. We use Digital Terrain
Models of Mars available from McEwen et al. (2007).
We use a grid world similar to the one introduced above. The safety constraint is the absolute value
of the steepness of the slope between two locations: given two states x and x0  the constraint over
the state transition is deﬁned as q(x  x0) = |H(x)  H(x0)|/d(x  x0)  where H(x)  H(x0) indicate the
altitudes at x and x0 respectively and d(x  x0) is the distance between them. We set conservatively
the safety constraint to q(x  x0)   tan1(25). The step of the grid is 10m. We use square maps
from 16 different locations on Mars with sides between 100 and 150 tiles. We generate 64 scenarios
by sampling 4 source-target pairs for each map . We model the steepness with a GP with Matérn
kernel with ⌫ = 5/2. We set the hyperprior on the lengthscale and on the standard deviation to
be Lognormal(30m  0.25m2) and Lognormal(tan(10)  0.04)  respectively. These encode our
prior belief about the surface of Mars. Next  we take 1000 noisy measurements at random locations
from each map  which  in reality  could come from satellite images  to ﬁnd a maximum a posteriori
estimator of the hyperparameters to ﬁne tune our prior to each site.
In Tab. 1  we show the geometric mean of the performance of SEO and GOOSE relative to SMDP.
The results conﬁrm those of the synthetic experiments but with larger changes in performance with
respect to the benchmark due to the increased size of the world.

5 Conclusion

We presented GOOSE  an add-on module that enables existing interactive machine learning algo-
rithms to safely explore the decision space  without violating a priori unknown safety constraints.
Our method is provably safe and learns about the safety of decisions suggested by existing  unsafe
algorithms. As a result  it is more data-efﬁcient than previous safe exploration methods in practice.
Aknowlegment.
This research was partially supported by the Max Planck ETH Center for Learning
Systems and by the European Research Council (ERC) under the European Union’s Horizon 2020
research and innovation programme grantagreement No 815943.

1We use policy iteration. Policy evaluation is performed by solving a sparse linear system with SciPy (Virtanen

et al.  2019). At iteration t  we initialize policy iteration with the optimal policy from t  1.

9

References
Anayo K Akametalu  Jaime F Fisac  Jeremy H Gillula  Shahab Kaynama  Melanie N Zeilinger 
and Claire J Tomlin. Reachability-based safe learning with gaussian processes. In Decision and
Control (CDC)  2014 IEEE 53rd Annual Conference on  pages 1424–1431. IEEE  2014.

Felix Berkenkamp  Andreas Krause  and Angela P Schoellig. Bayesian optimization with safety
constraints: safe and automatic parameter tuning in robotics. arXiv preprint arXiv:1602.04450 
2016.

Felix Berkenkamp  Matteo Turchetta  Angela Schoellig  and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in Neural Information Processing
Systems  pages 908–919  2017.

Erdem Biyik  Jonathan Margoliash  Shahrouz R. Alimo  and Dorsa Sadigh. Efﬁcient and safe
In

exploration in deterministic markov decision processes with unknown transition models.
Proceedings of the American Control Conference (ACC)  July 2019.

Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Proceedings of

the 34th International Conference on Machine Learning-Volume 70  pages 844–853  2017.

Michael A Gelbart  Jasper Snoek  and Ryan P Adams. Bayesian optimization with unknown
constraints. In Proceedings of the Thirtieth Conference on Uncertainty in Artiﬁcial Intelligence 
pages 250–259. AUAI Press  2014.

José Miguel Hernández-Lobato  Michael A Gelbart  Ryan P Adams  Matthew W Hoffman  and
Zoubin Ghahramani. A general framework for constrained bayesian optimization using information-
based search. The Journal of Machine Learning Research  pages 5549–5601  2016.

Torsten Koller  Felix Berkenkamp  Matteo Turchetta  and Andreas Krause. Learning-based model
predictive control for safe exploration and reinforcement learning. In Proc. of the IEEE Conference
on Decision and Control (CDC)  December 2018.

Alonso Marco  Felix Berkenkamp  Philipp Hennig  Angela P Schoellig  Andreas Krause  Stefan
Schaal  and Sebastian Trimpe. Virtual vs. real: Trading off simulations and physical experiments
in reinforcement learning with bayesian optimization. In Robotics and Automation (ICRA)  2017
IEEE International Conference on  pages 1557–1563. IEEE  2017.

Alfred S McEwen  Eric M Eliason  James W Bergstrom  Nathan T Bridges  Candice J Hansen 
W Alan Delamere  John A Grant  Virginia C Gulick  Kenneth E Herkenhoff  Laszlo Keszthelyi 
et al. Mars reconnaissance orbiter’s high resolution imaging science experiment (hirise). Journal
of Geophysical Research: Planets  112(E5)  2007.

Jonas Mockus  Vytautas Tiesis  and Antanas Zilinskas. The application of bayesian methods for

seeking the extremum. Towards global optimization  (117-129)  1978.

Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In
Proceedings of the 29th International Coference on International Conference on Machine Learning 
ICML’12  pages 1451–1458  2012.

MSL.

MSL Landing Site Selection User’s Guide to Engineering Constraints  2007.
URL http://marsoweb.nas.nasa.gov/landingsites/msl/memoranda/MSL_Eng_User_
Guide_v4.5.1.pdf.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons  2014.

Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced lectures on machine

learning  pages 63–71. Springer  2004.

Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2002.

10

Jens Schreiter  Duy Nguyen-Tuong  Mona Eberts  Bastian Bischoff  Heiner Markert  and Marc Tous-
saint. Safe exploration for active learning with gaussian processes. In Joint European Conference
on Machine Learning and Knowledge Discovery in Databases  pages 133–149. Springer  2015.
Jasper Snoek  Hugo Larochelle  and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems  pages 2951–2959 
2012.

Niranjan Srinivas  Andreas Krause  Sham Kakade  and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. In Proceedings of the 27th
International Conference on International Conference on Machine Learning  pages 1015–1022 
2010.

Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business

Media  2008.

Yanan Sui  Alkis Gotovos  Joel Burdick  and Andreas Krause. Safe exploration for optimization with

gaussian processes. In International Conference on Machine Learning  pages 997–1005  2015.

Yanan Sui  Vincent Zhuang  Joel W Burdick  and Yisong Yue. Stagewise safe bayesian optimization

with gaussian processes. arXiv preprint arXiv:1806.07555  2018.

William R Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25:285–294  1933.

Matteo Turchetta  Felix Berkenkamp  and Andreas Krause. Safe exploration in ﬁnite markov decision
processes with gaussian processes. In Advances in Neural Information Processing Systems  pages
4312–4320  2016.

Pauli Virtanen  Ralf Gommers  Travis E. Oliphant  Matt Haberland  Tyler Reddy  David Cournapeau 
Evgeni Burovski  Pearu Peterson  Warren Weckesser  Jonathan Bright  Stéfan J. van der Walt 
Matthew Brett  Joshua Wilson  K. Jarrod Millman  Nikolay Mayorov  Andrew R. J. Nelson  Eric
Jones  Robert Kern  Eric Larson  CJ Carey  ˙Ilhan Polat  Yu Feng  Eric W. Moore  Jake Vand erPlas 
Denis Laxalde  Josef Perktold  Robert Cimrman  Ian Henriksen  E. A. Quintero  Charles R Harris 
Anne M. Archibald  Antônio H. Ribeiro  Fabian Pedregosa  Paul van Mulbregt  and SciPy 1. 0
Contributors. SciPy 1.0–Fundamental Algorithms for Scientiﬁc Computing in Python. arXiv
e-prints  art. arXiv:1907.10121  Jul 2019.

Akifumi Wachi  Yanan Sui  Yisong Yue  and Masahiro Ono. Safe exploration and optimization
of constrained mdps using gaussian processes. Association for the Advancement of Artiﬁcial
Intelligence  2018.

Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient bayesian optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages 3627–
3635. JMLR. org  2017.

Douglas J White. A survey of applications of markov decision processes. Journal of the operational

research society  pages 1073–1096  1993.

Yifan Wu  Roshan Shariff  Tor Lattimore  and Csaba Szepesvári. Conservative bandits. In Interna-

tional Conference on Machine Learning  pages 1254–1262  2016.

11

,Özlem Aslan
Xinhua Zhang
Dale Schuurmans
Ofer Dekel
Tomer Koren
Alina Beygelzimer
Daniel Hsu
John Langford
Chicheng Zhang
Matteo Turchetta
Felix Berkenkamp
Andreas Krause