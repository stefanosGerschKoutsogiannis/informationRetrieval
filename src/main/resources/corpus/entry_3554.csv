2018,Multi-Class Learning: From Theory to Algorithm,In this paper  we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate  substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantees. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.,Multi-Class Learning: From Theory to Algorithm

Jian Li1 2  Yong Liu1∗  Rong Yin1 2  Hua Zhang1  Lizhong Ding5  Weiping Wang1 3 4

1Institute of Information Engineering  Chinese Academy of Sciences
2School of Cyber Security  University of Chinese Academy of Sciences

3National Engineering Research Center for Information Security

4National Engineering Laboratory for Information Security Technology
5Inception Institute of Artiﬁcial Intelligence (IIAI)  Abu Dhabi  UAE

{lijian9026 liuyong yinrong wangweiping}@iie.ac.cn

lizhong.ding@inceptioniai.org

Abstract

In this paper  we study the generalization performance of multi-class classiﬁ-
cation and obtain a shaper data-dependent generalization error bound with fast
convergence rate  substantially improving the state-of-art bounds in the existing
data-dependent generalization analysis. The theoretical analysis motivates us to de-
vise two effective multi-class kernel learning algorithms with statistical guarantees.
Experimental results show that our proposed methods can signiﬁcantly outperform
the existing multi-class classiﬁcation methods.

1

Introduction

√

Multi-class classiﬁcation is an important problem in various applications  such as natural language
processing  information retrieval  computer vision  web advertising  etc. The statistical learning
theory of binary classiﬁcation is by now relatively well developed [19  20  21  23  27  34]  but there
are still numerous statistical challenges to its multi-class extensions [25].
To understand the existing multi-class classiﬁcation algorithms and guide the development of new
ones  people have investigated the generalization ability of multi-class classiﬁcation algorithms. In
recent years  some generalization bounds have been proposed to estimate the ability of multi-class
classiﬁcation algorithms based on different measures  such as VC-dimension [1]  Natarajan dimension
[7]  covering Number [9  11  37]  Rademacher Complexity [5  14  27]  Stability [10]  PAC-Bayesian
[26]  etc. Although there have been several recent advances in the studying of generalization bounds
of multi-class classiﬁcation algorithms  convergence rates of the existing generalization bounds are

n(cid:1)  where K and n are the number of classes and size of the sample  respectively.
usually O(cid:0)K 2/
algorithms based on the above theoretical analysis. The rate of this bound is O(cid:0)(log K)2+1/log K/n(cid:1) 

In this paper  we derive a novel data-dependent generalization bound for multi-class classiﬁcation via
the notion of local Rademacher complexity and further devise two effective multi-class kernel learning

which substantially improves on the existing data-dependent generalization bounds. Moreover  the
proposed multi-class kernel learning algorithms have statistical guarantees and fast convergence
rates. Experimental results on lots of benchmark datasets show that our proposed methods can
signiﬁcantly outperform the existing multi-class classiﬁcation methods. The major contributions
of this paper include: 1) A new local Rademacher complexity based bound with fast convergence
rate for multi-class classiﬁcation is established. Existing works [16  27] for multi-class classiﬁers
with Rademacher complexity does not take into account couplings among different classes. To
obtain sharper bound  we introduce a new structural complexity result on function classes induced
by general classes via the maximum operator  while allowing to preserve the correlations among

∗Corresponding author

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

different components meanwhile. Thus  our result in this paper is a non-trivial extension of the
binary classiﬁcation of local Rademacher complexity to multi-classiﬁcation; 2) Two novel multi-class
classiﬁcation algorithms are proposed with statistical guarantees: a) Conv-MKL. Using precomputed
kernel matrices regularized by local Rademacher complexity  this method can be implemented by any
(cid:96)p-norm multi-class MKL solvers; b) SMSD-MKL. This method puts local Rademacher complexity
in penalized ERM with (cid:96)2 p-norm regularizer  implemented by stochastic sub-gradient descent with
updating dual weights.

2 Related Work

2.1 Multi-Class Classiﬁcation Bounds

Rademacher Complexities Bounds. Koltchinskii and Panchenko [14] and Koltchinskii  Panchenko 
and Lozano [15] ﬁrst introduced a margin-based bound for multi-class classiﬁcation in terms of
Rademacher complexity. This bound was slightly improved in [27  5]. Maximov and Reshetova [25]
gave a new Rademacher complexity based bound that is linear in the number of classes. Based on the
(cid:96)p-norm regularization  Lei  Binder  and Klof [18] introduced a bound with a logarithmic dependence
on the number of class size. Instead of global Rademacher complexity  in this paper  we use local
Rademacher complexity to obtain a sharper bound  which substantially improves generalization
performance upon existing global Rademacher complexity methods.
VC-dimension Bounds. Allwein  Schapire  and Singer [1] used the notion of VC-dimension for
multi-class learning problems  and derived a VC-dimension based bound. Natarajan dimension
was introduced in [28] in order to characterize multi-class PAC learnability  which exactly matches
the notion of Vapnik-Chervonenkis dimension in the case of binary classiﬁcation. Daniely and
Shalev-Shwartz [7] derived a risk bound with Natarajan dimension for multi-class classiﬁcation. VC
dimension and Natarajan dimension are important tools to derive generalization bounds  however 
these bounds are usually dimension dependent  which makes them hardly applicable to practical
large-scale problems (such as typical computer vision problems).
Covering Number Bounds. Based on the (cid:96)∞-norm covering number bound of linear operators 
Guermeur [9] obtained a generalization bound exhibiting a linear dependence on the class size 
which was improved by [37] to a radical dependence. Hill and Doucet [11] derived a class-size
independent risk guarantee. However  their bound is based on a delicate deﬁnition of margin  which
is not commonly used in mainstream multi-class literature.
Stability Bounds and PAC-Bayesian Bounds. Stability [10] and PAC-Bayesian [26] are two
popular tools to analyze generalization performance on neural networks for multi-class setting.
Hardt  Recht and Singer [10] generated generalization bounds for models learned with stochastic
√
gradient descent. McAllester [26] proposed a dropout bound for neural networks with PAC-Bayesian.
However  the convergence rate based on stability and PAC-Bayesian is usually at most O(1/

n).

2.2 Local Rademacher Complexity

In recent years  several authors have applied local Rademacher complexity to obtain better gen-
eralization error bounds for traditional binary classiﬁcation [2  13  22  24]  similar analysis has
been explored in multi-label learning [35] and multi-task learning [36] as well. However  numerous
statistical challenges remain in the multi-class case  and it is still unclear how to use this tool to derive
a tighter bound for multi-class. In this paper  we bridge this gap by deriving a sharper generalization
bound using local Rademacher complexity.

2.3 Multi-Class Kernel Learning Algorithms

As one of the success stories in multiple kernel learning  improvements in multi-class MKL have
emerged [38]  in which a one-stage multi-class MKL algorithm was presented as a generalization
of multi-class loss function [6  33]. And Orabona designed stochastic gradient methods  named
OBSCURE [30] and UFO-MKL [29]  which optimize primal versions of equivalent problems. In
this paper  we consider the use of the local Rademacher complexity to devise the novel multi-class
classiﬁcation algorithms  which have statistical guarantees and fast convergence rates.

2

3 Notations and Preliminaries
We consider multi-class classiﬁcation problems with K ≥ 2 classes in this paper. Let X be
the input space and Y = {1  2  . . .   K} the output space. Assume that we are given a sample
S = {z1 = (x1  y1)  . . .   zn = (xn  yn)} of size n drawn i.i.d. from a ﬁxed  but unknown probability
distribution µ on Z = X × Y. Based on the training examples S  we wish to learn a scoring rule h
from a space H mapping from Z to R and use the mapping x → arg maxy∈Y h(x  y) to predict. For
any hypothesis h ∈ H  the margin of a labeled example z = (x  y) is deﬁned as

ρh(z) := h(x  y) − max
y(cid:48)(cid:54)=y

h(x  y(cid:48)).

(cid:0)1t≤0 + (1 − ts−1)10<t≤s

(cid:1)2  s > 0. In the following  we assume that: 1) (cid:96)(t) bounds the 0-1 loss:

The h misclassiﬁes the labeled example z = (x  y) if ρh(z) ≤ 0 and thus the expected risk incurred
from using h for prediction is L(h) := Eµ[1ρh(z)≤0]  where 1t≤0 is the 0-1 loss  1t≤0 = 1 if
t ≤ 0  otherwise 0. Since 0-1 loss is hard to handle in learning machines  one usually considers
the proxy loss: such as the square hinge (cid:96)(t) = (1 − t)2
+ and the square margin loss (cid:96)s(t) =
1t≤0 ≤ (cid:96)(t); 2) (cid:96) is decreasing and it has a zero point c(cid:96)  i.e.  (cid:96)(c(cid:96)) = 0; 3) (cid:96) is ζ-smooth  that
is |(cid:96)(cid:48)(t) − (cid:96)(cid:48)(s)| ≤ ζ|t − s|. Note that both square hinge loss and margin loss satisfy the above
assumptions.
Any function h : X × Y → R can be equivalently represented by the vector-valued function
(h1  . . .   hK) with hj(x) = h(x  j)  ∀j = 1  . . .   K. Let κ : X × X → R be a Mercer kernel with
φ being the associated feature map  i.e.  κ(x  x(cid:48)) = (cid:104)φ(x)  φ(x(cid:48))(cid:105). The (cid:96)p-norm hypothesis space
associated with the kernel κ is denoted by:

(cid:110)
(cid:111)
hw = ((cid:104)w1  φ(x)(cid:105)  . . .  (cid:104)wK  φ(x)(cid:105)) : (cid:107)w(cid:107)2 p ≤ 1  1 ≤ p ≤ 2

Hp κ =

(1)

 

(cid:104)(cid:80)K

(cid:105) 1

where w = (w1  . . .   wK) and (cid:107)w(cid:107)2 p =
the dual exponent of p satisfying 1/p + 1/q = 1.
The space of loss function associated with Hp κ is denoted by

i=1 (cid:107)wi(cid:107)p

2

p is the (cid:96)2 p-norm. For any p ≥ 1  let q be

L = {(cid:96)h := (cid:96)(ρh(z)) : h ∈ Hp κ} .

(2)

Let L((cid:96)h) and ˆL((cid:96)h) be expected generalization error and empirical error with respect to (cid:96)h:

L((cid:96)h) := Eµ[(cid:96)(ρh(z))] and ˆL((cid:96)h) =

1
n

(cid:96)(ρh(zi)).

Deﬁnition 1 (Rademacher complexity). Assume L is a space of loss functions as deﬁned in Equation
(2). Then the empirical Rademacher complexity of L is:

n(cid:88)

i=1

(cid:35)

(cid:34)

n(cid:88)

i=1

ˆR(L) := Eσ

1
n

sup
(cid:96)h∈L

σi(cid:96)h(zi)

 

where σ1  σ2  . . .   σn is an i.i.d. family of Rademacher variables taking values -1 and 1 with equal
probability independent of the sample S = (z1  . . .   zn). The Rademacher complexity of L is
R(L) = Eµ ˆR(L).
Generalization bounds based on the notion of Rademacher complexity for multi-class classiﬁca-
tion are standard [14  15  27]: with probability 1 − δ  L(h) ≤ inf 0<γ<1
√
log(1/δ)/
n)
for various kernel multi-class in practice  so the standard Rademacher complexity bounds converge at

(cid:0) ˆL(hγ) + O(cid:0)R(L)/γ +
(cid:3). Since R(L) is in the order of O(K 2/

(cid:2)1ρh(zi)≤γ

n(cid:1)(cid:1)  where ˆL(hγ) = 1
n(cid:1)  usually.

rate O(cid:0)K 2/

(cid:80)n

√

√

i=1

n

Although Rademacher complexity is widely used in bound generalization analysis  it does not take
into consideration the fact that  typically  the hypotheses selected by a learning algorithm have a
better performance than in the worst case and belong to a more favorable sub-family of the set of all
hypotheses [4]. Therefore  to derive sharper generalization bound  we consider the use of the local
Rademacher complexity in this paper.

3

R(Lr) := R(cid:110)
(cid:2)(cid:96)2(ρh(z))(cid:3).

Deﬁnition 2 (Local Rademacher Complexity). For any r > 0  the local Rademacher complexity of
L is deﬁned as

(cid:12)(cid:12)(cid:12)a ∈ [0  1]  (cid:96)h ∈ L  L[(a(cid:96)h)2] ≤ r

(cid:111)

 

a(cid:96)h

h) = Eµ

where L((cid:96)2
The key idea to obtain sharper generalization error bound is to choose a much smaller class Lr ⊆ L
with as small a variance as possible  while requiring that the solution is still in {h|h ∈ Hp κ  (cid:96)h ∈ Lr}.
In the following  we assume that ϑ = supx∈X κ(x  x) < ∞  and (cid:96)h : Z → [0  d]  d > 0 is a constant.
The above two assumptions are two common restrictions on kernel function and loss functions  which
are satisﬁed by the popular Gaussian kernels and the bounded hypothesis  respectively.

4 Sharper Generalization Bounds

In this section  we ﬁrst estimate the local Rademacher complexity  and further derive a sharper
generalization bound.

4.1 Local Rademacher Complexity

The estimate the local Rademacher complexity of multi-class classiﬁcation is given as follows.
Theorem 1. With probability at least 1 − δ 

√
√
R(Lr) ≤ cd ϑξ(K)
ζr log
n

3
2 (n)

+

4 log(1/δ)

n

 

where

ξ(K) =

(cid:40)√

e(4 log K)1+ 1

2 log K  

(2q)1+ 1

q K

1
q  

if q ≥ 2 log K 
otherwise 

cd ϑ is a constant depends on d and ϑ.

n(cid:1) for various
Note that the order of the (global) Rademacher complexity over L is usually O(cid:0)K 2/
n + 1/n(cid:1). Note that ξ(K) is logarithmic dependence on K when
is R(Lr) = O(cid:0)√
even reach O(cid:0)(log K)2+1/ log K/n(cid:1) (see in the next subsection)  which substantially improves the

kernel multi-classes. From Theorem 1  one can see that the order of the local Rademacher complexity
q ≥ 2 log K. For 2 ≤ q < 2 log K  ξ(K) = O(K
q ) which is also substantially milder than the
quadratic dependence for Rademacher complexity. If we choose a suitable value of r  the order can

rξ(K)/

√

√

2

Rademacher complexity bounds.

4.2 A Sharper Generalization Bound

A sharper bound for multi-class classiﬁcation based on the notion of local Rademacher complexity is
derived as follows.
Theorem 2. ∀h ∈ Hp κ and ∀k > max(1 

√
2d )  with probability at least 1 − δ  we have

2

(cid:27)

 

(cid:26) k
(cid:40)√

k − 1

L(h) ≤ max

where

ˆL((cid:96)h)  ˆL((cid:96)h) +

cd ϑ ζ kξ2(K) log3 n

n

+

cδ
n

e(4 log K)1+ 1

2 log K  

ξ(K) =

(2q)1+ 1

q K

1

q  

if q ≥ 2 log K 
otherwise 

cd ϑ is a constant depending on d  ϑ  ζ  k  and cδ is a constant depending on δ.

4

The order of the generalization bound in Theorem 2 is O(cid:0)ξ2(K)/n(cid:1). From the deﬁnition of ξ(K) 

we can obtain that

(cid:18) ξ2(K)

(cid:19)

=

n

O

O(cid:16)
O(cid:16)

(cid:17)

(log K)2+1/log K/n

(cid:17)

K 2/q/n

 

 

if q ≥ 2 log K 
if 2 ≤ q < 2 log K.

Note that our bounds is linear dependence on the reciprocal of sample size n  while for the existing
data-dependent bounds are all radical dependence. Furthermore  our bounds enjoy a mild dependence
on the number of classes. The dependence is polynomial with degree 2/q for 2 ≤ q < 2 log K and
becomes logarithmic if q ≥ 2 log K  which is substantially milder than the quadratic dependence
established in [14  15  27  5].

4.3 Comparison with the Related Work

n

√

√

√

√

√

n + log 1/δ√

n

γ

√
log(1/δ)/

(cid:1). The order is O(cid:0) K2√

Rademacher Complexity Bounds Koltchinskii and Panchenko [14] and Koltchinskii  Panchenko 
and Lozano [15] introduce a margin-based bound for multi-class classiﬁcation in terms of Rademacher

improved (by a constant factor prior to the Rademacher complexity term) by [27  5]. Maximov and
n)+

Rademacher complexity are all radical dependence on the reciprocal of sample size.
In this paper  we derive a sharper bound based on the local Rademacher complexity with order

complexities: L(h) ≤ inf 0<γ<1 ˆL(hγ) + O(cid:0) K2
(cid:1)  which is slightly
Reshetova [25] give a new Rademacher complexity bound: L(h) ≤ inf 0<γ<1 ˆL(hγ)+O(cid:0)K/(γ
n(cid:1). Based on the (cid:96)p-norm regularization  Lei  Binder 
n(cid:1)  which has the form of O(cid:0)K/
n(cid:1). The existing bounds based on
and Klof [18] derive a new bound: L(h) ≤ ˆL((cid:96)h) + O(cid:0)log2 K/
log K /n(cid:1)  substantially sharper than the existing bounds of Rademacher complexity.
O(cid:0)(log K)2+ 1
Guermeur [9] obtains a generalization of form O(cid:0)K/
n(cid:1)  which is improved by [37] to a radical
dependence: L(h) ≤ ˆL((cid:96)h) + O(cid:0)(cid:112)K/n(cid:1). Hill and Doucet [11] derive a class-size independent
risk guarantee of form O(cid:0)(cid:112)1/n(cid:1). However  their bound is based on a delicate deﬁnition of margin 
problems  and derive a VC-dimension based bounds: L(h) ≤ ˆL(hγ) +O(cid:0)√
n(cid:1)  where V
dimension: L(h) ≤ ˆL(hγ) + O(cid:0)dN at/n(cid:1)  where dN at is the Natarajan dimension. Note that VC

which is not commonly used in mainstream multi-class literature.
VC-dimension Bounds VC-dimension is an important tool to derive the generalization bound for
binary classiﬁcation. Allwein  Schapire  and Singer [1] show how to use it for multi-class learning

is the VC-dimension. Natarajan dimension is introduced in [28] in order to characterize multi-class
PAC learnability. Daniely and Shalev-Shwartz [7] derive a generalization bound with Natarajan

Covering Number Bounds Based on the (cid:96)∞-norm covering number bound of linear operators 

dimension bounds  as well as Natarajan dimension bounds  are usually dimension dependent  which
makes them hardly applicable for practical large scale problems (such as typical computer vision
problems).
Stability and PAC-Bayesian Bounds Stability [10] and PAC-Bayesian [26] are two useful tools to
analyze generalization performance on neural networks for a multi-class setting. Hardt  Recht and
Singer [10] generated generalization bounds for models learned with stochastic gradient descent

n(cid:1). McAllester [26] used the PAC-Bayesian theory to derive

using stability: L(h) ≤ ˆL(hγ) +O(cid:0)1/
generalization bound: L(h) ≤ ˆL(hγ) + O(cid:0)(cid:113)

ˆL(hγ)/n(cid:1).

√

V log K/

√

5 Multi-Class Multiple Kernel Learning

In this paper  we consider the use of multiple kernels  κµ =(cid:80)M

Motivated by the above analysis of generalization bound  we will exploit the properties of the local
Rademacher complexity to devise two algorithms for multi-class multiple kernel learning (MC-MKL).
m=1 µmκm. A common approach to
multi-class classiﬁcation is the use of joint feature maps φ(x) : X → H [33]. For multiple kernel
learning  we have M feature mappings φm  m = 1  . . .   M and κm(x  x(cid:48)) = (cid:104)φm(x)  φm(x(cid:48))(cid:105) 
where m = 1  . . .   M. Let φµ(x) = [φ1(x)  . . .   φM (x)]. Using Theorem 2  to obtain a shaper

5

generalization bound  we conﬁne q ≥ 2 log K  thus 1 < p ≤ 2 log K
multiple kernels can be written as:

(cid:110)
hw κµ = ((cid:104)w1  φµ(x)(cid:105)  . . .  (cid:104)wK  φµ(x)(cid:105))  (cid:107)w(cid:107)2 p ≤ 1  1 < p ≤ 2 log K
2 log K − 1

2 log K−1. The (cid:96)p hypothesis space of

Hmkl =

(cid:111)

.

H1 =

(cid:110)
hw κµ ∈ Hmkl :

5.1 Conv-MKL
(cid:80)M
The global Rademacher complexity of Hmkl can be bounded by the trace of kernel matrix Kµ =
m=1 Km. Existing works on [17  32] use the following constraint to Hmkl: Tr(Kµ) ≤ 1.
According to the above theoretical analysis  the local Rademacher complexity (the tail sum of
the eigenvalues of the kernel) leads to tighter generalization bounds than the global Rademacher
complexity (the trace). Thus  we add the local Rademacher complexity to restrict Hmkl:

where λj(Kµ) is the j-th eigenvalues of Kµ and ζ is free parameter removing the ζ largest eigen-
values to control the tail sum. Note that the tail sum is the difference between the trace and the ζ
j=1 λj(Kµ)  thus the tail sum can be calculated

(cid:88)
largest eigenvalues:(cid:80)
j>ζ λj(Kµ) = Tr(Kµ) −(cid:80)ζ
One can see that H1 is not convex  and we know that: (cid:80)M
(cid:0)Kµ
(cid:1). Thus  we consider the use of the con-
(cid:80)M
(cid:80)
j>ζ λj((cid:107)µ(cid:107)1Km) ≤ (cid:80)
m=1 µm/(cid:107)µ(cid:107)1
(cid:88)
M(cid:88)

in O(n2ζ) for each kernel.

λj(Kµ) ≤ 1

j>ζ λj(Km) =

vex H2:

(cid:110)
hw κµ ∈ Hmkl :

H2 =

m=1 µm

(cid:80)

j>ζ λj

(cid:111)

j>ζ

 

.

(cid:110)
According to normalized kernels ˜κm =
m=1 µm˜κm  we
(cid:111)
can simply rewrite H2 as
 (cid:107)w(cid:107)2 p ≤ 1  1 < p ≤
hw ˜κµ =
2 log K−1   µ (cid:23) 0 (cid:107)µ(cid:107)1 ≤ 1
  which is a commonly studied hypothesis class in multi-class multiple
kernel learning. A simple process with precomputed kernel matrices regularized by local Rademacher
complexity can be seen in Algorithm 1:

j>ζ λj(Km)

2 log K

(cid:111)
(cid:16)(cid:80)
(cid:16)(cid:104)w1  ˜φµ(x)(cid:105)  . . .  (cid:104)wK  ˜φµ(x)(cid:105)(cid:17)

λj(Km) ≤ 1

(cid:17)−1

µm

m=1

j>ζ

κm and ˜κµ = (cid:80)M

Algorithm 1 Conv-MKL

Input: precomputed kernel matrices K1  . . .   KM and ζ
for i = 1 to M do

Compute tail sum: rm =(cid:80)
Normalize precomputed kernel matrix: (cid:101)Km = Km/rm
Use (cid:101)Km  m = 1  . . .   M  as the basic kernels in any (cid:96)p-norm MKL solver

j>ζ λj (Km)

end for

5.2 SMSD-MKL
Considering a more challenging case  we perform penalized ERM over the class H1  aiming to solve
a convex optimization problem with an additional term representing local Rademacher complexity :

min
w µ

1
n

n(cid:88)
(cid:124)
(cid:12)(cid:12)(cid:12)(cid:12)1 −
(cid:18)

i=1

(cid:107)w(cid:107)2

2 p + β

(cid:123)(cid:122)

Ω(w)

+

α
2

(cid:125)

(cid:96)(w  φµ(xi)  yi)

(cid:124)
(cid:123)(cid:122)
(cid:104)wyi  φµ(xi)(cid:105) − max
y(cid:54)=yi

C(w)

m=1

M(cid:88)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)+

µmrm

 

(cid:125)
and rm =(cid:80)

(3)

j>ζ λj(Km)

where (cid:96)(w  φµ(xi)  yi) =
is the tail sum of the eigenvalues of the m-th kernel matrix  m = 1  . . .   M.

(cid:104)wy  φµ(xi)(cid:105)

6

Algorithm 2 SMSD-MKL

Input: α  β  r  T
Initialize: w1 = 0  θθθ1 = 0  µ1 = 1  q = 2 log K
for t = 1 to T do

Sample at random (xt  yt)
Compute the dual weight: θθθt+1 = θθθt − ∂C(wt)
m (cid:107) − tβrm  ∀m = 1  . . .   M
m = (cid:107)θt+1
νt+1
m )|νt+1
  ∀m = 1  . . .   M
m = sgn(νt+1
µt+1
m (cid:107)|νt+1
α(cid:107)θt+1
end for

m |q−1
m |q−2

q

Based on the stochastic mirror descent framework for minimization problems in [31  29]  we design
a stochastic mirror and sub-gradient descent algorithm  called SMSD-MKL  to minimize (3)  seen in
Algorithm 2.
As shown in the mirror descent algorithm  it maintains two weight vectors: the primal vector w and
the dual vector θθθ. Meanwhile  the optimization formulation can be divided into two parts: C(w) to
update θθθ and Ω(w) to update w by the gradient of the Fenchel dual of Ω. Actually  the algorithm
puts the kernel weight µµµ aside when updating θθθ  but µµµ is updated together with w according to a
tricky link function given in Theorem 3.

• For C(w)  the algorithm updates the dual vector with the gradient of C(w). Since
hinge loss used in C(w) is not differentiable  the algorithm uses sub-gradient of zt =
∂(cid:96)(wt  φµ(xt)  yt)  where ∂(cid:96)(wt  φµ(xt)  yt) is the sub-gradient w.r.t wt.
• For Ω(w)  as in the UFO-MKL [29]  the algorithm uses w = ∇Ω∗(θθθ) to update the primal

vector w  of which the calculation has been given in Theorem 3.

q = 2 log K to make the order of generalization reach O(cid:0) (log K)2+1/ log K

The algorithm starts with w1 = 0  θθθ1 = 0 and µµµ1 = 1. Especially  the algorithm initializes

(cid:1)  according to Theorem 2.

In each iteration  the algorithm randomly samples a training example from the train set.
Actually  the algorithm updates real numbers (cid:107)θt+1
m (cid:107)  νt+1
m in scalar products instead of
m . The (cid:107)θt+1
m (cid:107) can be calculated in an efﬁcient incremental
high-dimensional variables wt+1 and θt+1
way by scalar values as following:
m(cid:107)2
2 = (cid:107)θθθt
2 = (cid:107)θθθt
2 − 2θθθt
(cid:105)

(cid:104)(cid:107)θθθ1(cid:107) − βr1  . . .  (cid:107)θθθM(cid:107) − βrM

where zt = ∂(cid:96)(wt  φµ(xt)  yt).
Theorem 3. Let ν =

  then the component m-th of ∇Ω∗(θθθ) is

m and µt+1

m + (cid:107)zt

m(cid:107)2

m − zt

m(cid:107)2

(cid:107)θθθt+1
m (cid:107)2

m · zt

2

n

sgn(νm)θθθm
α(cid:107)θθθm(cid:107)

|νm|q−1
(cid:107)ν(cid:107)q−2

q

 

where sgn(x) is deﬁned as sgn(x) = 1 if x > 0  sgn(x) = −1 if x < 0 and sgn(x) ∈ [−1  +1]  if
x = 0.

6 Experiments

In this section  we compare our proposed Conv-MKL (Algorithm 1) and SMSD-MKL (Algorithm 2)
with 7 popular multi-class classiﬁcation methods: One-against-One [12]  One-against-the-Rest [3] 
(cid:96)1-norm linear multi-class SVM (LMC) [6]  generalized minimal norm problem solver (GMNP) [8] 
the Multiclass MKL (MC-MKL) with (cid:96)1-norm and (cid:96)2-norm [38] and mixed-norm MKL solved by
stochastic gradient descent (UFO-MKL) [29]. Actually  we complete comparison tests via implements
in LIBSVM (One-against-One and One-against-the-Rest)  the DOGMA library 2 (LMC  GMNP  (cid:96)1-
norm and (cid:96)2-norm MC-MKL) and the SHOGUN-6.1.3 3 (UFO-MKL). We implement our proposed
Conv-MKL and SMSD-MKL algorithms based on UFO-MKL.

2Available at http://dogma. sourceforge. net
3Available at http://www.shogun-toolbox.org/

7

Table 1: Comparison of average test accuracies of our Conv-MKL and SMSD-MKL with the others. We
bold the numbers of the best method and underline the numbers of the other methods which are not
signiﬁcantly worse than the best one.

LMC

GMNP

SMSD-MKL

One vs. One One vs. Rest

Conv-MKL
77.14±2.25 78.01±2.17 70.12±2.96 75.83±2.69 75.17±2.68 75.42±3.64 77.60±2.63
plant
74.41±3.35 76.23±3.39 63.85±3.94 73.33±4.21 71.70±4.89 73.55±4.22 71.87±4.87
psortPos
74.07±2.16 74.66±1.90 57.85±2.49 73.74±2.87 71.94±2.50 74.27±2.51 72.83±2.20
psortNeg
79.15±1.51 78.69±1.58 75.16±1.48 77.78±1.52 77.49±1.53 78.35±1.46 77.89±1.79
nonpl
92.83±2.62 93.39±0.70 93.16±0.66 90.61±0.69 91.34±0.61
sector
96.79±0.91 97.62±0.83 95.07±1.11 97.08±0.61 97.02±0.80 96.87±0.80 96.98±0.64
segment
79.35±2.27 77.28±2.78 75.61±3.56 78.72±1.92 79.11±1.94 81.57±2.24 74.96±2.93
vehicle
98.82±1.19 98.83±5.57 62.32±4.97 98.12±1.76 98.22±1.83 97.04±1.85 98.27±1.22
vowel
99.63±0.96 99.63±0.96 97.87±2.80 97.24±3.05 98.14±3.04 97.69±2.43 98.61±1.75
wine
96.08±0.83 96.30±0.79 92.02±1.50 95.89±0.56 95.61±0.73 94.60±0.94 96.27±0.68
dna
75.19±5.05 73.72±5.80 63.95±6.04 71.98±5.75 70.00±5.75 71.24±8.14 69.07±8.08
glass
96.67±2.94 97.00±2.63 88.00±7.82 95.93±3.25 95.87±3.20 95.40±7.34 95.40±6.46
iris
svmguide2 82.69±5.65 85.17±3.83 81.10±4.15 84.79±3.45 84.27±3.03 81.77±3.45 83.16±3.63
91.64±0.88 91.78±0.82 84.95±1.15 90.67±0.91 89.29±0.96 89.97±0.81 91.86±0.62
satimage

(cid:96)1 MC-MKL (cid:96)2 MC-MKL UFO-MKL
75.49±2.48 76.77±2.42
70.70±4.89 74.56±4.04
72.42±2.65 73.80±2.26
77.95±1.64 78.07±1.56
92.15±2.57 92.60±0.47
97.58±0.68 97.20±0.82
76.27±3.15 76.92±2.83
97.86±1.75 98.22±1.62
98.52±1.89 99.44±1.13
95.06±0.92 95.84±0.61
74.03±6.41 72.46±6.12
94.00±7.82 95.93±2.88
83.84±4.21 82.91±3.09
90.43±1.27 91.92±0.83

\

\

K(x  x(cid:48)) = exp(cid:0) − (cid:107)x − x(cid:48)(cid:107)2

We experiment on 14 publicly available datasets: four of them evaluated in [38] (plant  nonpl 
psortPos  and psortNeg) and others from LIBSVM Data. For each dataset  we use the Gaussian kernel

2/2τ(cid:1) as our basic kernels  where τ ∈ 2i  i = −10 −9  . . .   9  10. For

single kernel methods (One vs. One  One vs. Rest and GMNP)  we choose the kernel which have
the highest performance among basic kernels estimated by 10-folds cross-validation. Meanwhile 
we use all basic kernels in MKL methods (Conv-MKL  SMSD-MKL  (cid:96)1 MC-MKL  (cid:96)2 MC-MKL
and UFO-MKL). The regularization parameterized α ∈ 2i  i = −2  . . .   12 in all algorithms and
ζ ∈ 2i  i = 1  2  . . .   4  β ∈ 10i  i = −4  . . .   1 in SMSD-MKL are determined by 10-folds cross-
validation on training data. Other parameters in compared algorithms follow the same experimental
setting in their papers. For each dataset  we run all methods 50 times with randomly selected 80%
for training and 20% for testing  offering an estimate of the statistical signiﬁcance of differences in
performance between methods. All statement of statistical signiﬁcance in the remainder refer to a
95% level of signiﬁcance under t-test.
The average test accuracies are reported in Table 1. The results show: 1) Our methods Conv-MKL
and SMSD-MKL give best results on nearly all datasets except vehicle and satimage; 2) SMSD-MKL
is better than Conv-MKL because it wins on 2/3 datasets; 3) Compared with typical MKL methods 
our methods get better results over almost all datasets except that only UFO-MKL works slightly
better than ours on satige; 4) The MKL methods usually work better than the compared single kernel
methods (One vs. One  One vs. Rest and GMNP); 5) The kernel classiﬁcation methods have better
performance than the linear classiﬁcation machine (LMC) on all datasets.
The above results show that the use of the local Rademacher complexity can signiﬁcantly improve
the performance of multi-class multiple kernel learning algorithms  which conforms to our theoretical
analysis.

7 Conclusion

In this paper  we studied the generalization performance of multi-class classiﬁcation  and derived a
sharper data dependent generalization error bound using the local Rademacher complexity  which is
much sharper than existing data-dependent generalization bounds of multi-class classiﬁcation. Then 
we designed two algorithms with statistical guarantees and fast convergence rates: Conv-MKL and
SMSD-MKL. Based on local Rademacher complexity  our analysis can be used as a solid basis for the
design of new multi-class kernel learning algorithms.

Acknowledgments

This work is supported in part by the National Natural Science Foundation of China (No.61703396 
No.61673293  No.61602467)  the National Key Research and Development Program of China
(No.2016YFB1000604)  the Science and Technology Project of Beijing (No.Z181100002718004)
and the Excellent Talent Introduction of Institute of Information Engineering of CAS (Y7Z0111107).

8

References
[1] E. L. Allwein  R. E. Schapire  and Y. Singer. Reducing multiclass to binary: A unifying

approach for margin classiﬁers. Journal of machine learning research  1:113–141  2000.

[2] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. The Annals of

Statistics  33(4):1497–1537  2005.

[3] L. Bottou  C. Cortes  J. S. Denker  H. Drucker  I. Guyon  L. D. Jackel  Y. LeCun  U. A.
Muller  E. Sackinger  P. Simard  et al. Comparison of classiﬁer methods: a case study in
handwritten digit recognition. In Proceedings of the 12th IAPR International Conference on
Pattern Recognition  pages 77–82  1994.

[4] C. Cortes  M. Kloft  and M. Mohri. Learning kernels using local Rademacher complexity. In

Advances in Neural Information Processing Systems 25 (NIPS)  pages 2760–2768  2013.

[5] C. Cortes  M. Mohri  and A. Rostamizadeh. Multi-class classiﬁcation with maximum margin
multiple kernel. In Proceedings of the 30th International Conference on Machine Learning
(ICML)  pages 46–54  2013.

[6] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based

vector machines. Journal of Machine Learning Research  2:265–292  2002.

[7] A. Daniely and S. Shalev-Shwartz. Optimal learners for multiclass problems. In Proceedings of

the 27th Conference on Learning Theory (COLT)  pages 287–316  2014.

[8] V. Franc. Optimization algorithms for kernel methods. Prague: A PhD dissertation. Czech

Technical University  2005.

[9] Y. Guermeur. Combining discriminant models with new multi-class SVMs. Pattern Analysis &

Applications  5(2):168–179  2002.

[10] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic gradient
descent. In Proceedings of the 33rd International Conference on Machine Learning (ICML) 
pages 1225–1234  2016.

[11] S. I. Hill and A. Doucet. A framework for kernel-based multi-category classiﬁcation. Journal

of Artiﬁcial Intelligence Research  30:525–564  2007.

[12] S. Knerr  L. Personnaz  and G. Dreyfus. Single-layer learning revisited: a stepwise procedure
for building and training a neural network. In Neurocomputing  pages 41–50. Springer  1990.

[13] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization.

The Annals of Statistics  34(6):2593–2656  2006.

[14] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generaliza-

tion error of combined classiﬁers. The Annals of Statistics  30:1–50  2002.

[15] V. Koltchinskii  D. Panchenko  and F. Lozano. Some new bounds on the generalization error of
combined classiﬁers. In Advances in Neural Information Processing Systems 14 (NIPS)  pages
245–251  2001.

[16] V. Kuznetsov  M. Mohri  and U. Syed. Multi-class deep boosting. In Advances in Neural

Information Processing Systems 27 (NIPS)  pages 2501–2509  2014.

[17] G. R. G. Lanckriet  N. Cristianini  P. L. Bartlett  L. E. Ghaoui  and M. I. Jordan. Learning the
kernel matrix with semideﬁnite programming. Journal of Machine Learning Research  5:27–72 
2004.

[18] Y. Lei  U. D. A. Binder  and M. Kloft. Multi-class SVMs: From tighter data-dependent
generalization bounds to novel algorithms. In Advances in Neural Information Processing
Systems 27 (NIPS)  pages 2035–2043  2015.

[19] Y. Liu  S. Jiang  and S. Liao. Eigenvalues perturbation of integral operator for kernel selection.
In Proceedings of the 22nd ACM International Conference on Information and Knowledge
Management (CIKM)  pages 2189–2198  2013.

9

[20] Y. Liu  S. Jiang  and S. Liao. Efﬁcient approximation of cross-validation for kernel methods
using Bouligand inﬂuence function. In Proceedings of the 31st International Conference on
Machine Learning (ICML)  pages 324–332  2014.

[21] Y. Liu and S. Liao. Preventing over-ﬁtting of cross-validation with kernel stability. In Pro-
ceedings of the European Conference on Machine Learning and Principles and Practice of
Knowledge Discovery in Databases (ECML)  pages 290–305  2014.

[22] Y. Liu and S. Liao. Eigenvalues ratio for kernel selection of kernel methods. In Proceedings of

the 29th AAAI Conference on Artiﬁcial Intelligence (AAAI)  pages 2814–2820  2015.

[23] Y. Liu  S. Liao  and Y. Hou. Learning kernels with upper bounds of leave-one-out error.
In Proceedings of the 20th ACM International Conference on Information and Knowledge
Management (CIKM)  pages 2205–2208  2011.

[24] Y. Liu  S. Liao  H. Lin  Y. Yue  and W. Wang. Inﬁnite kernel learning: generalization bounds
and algorithms. In Proceedings of the 21st AAAI Conference on Artiﬁcial Intelligence (AAAI) 
pages 2280–2286  2017.

[25] Y. Maximov and D. Reshetova. Tight risk bounds for multi-class margin classiﬁers. Pattern

Recognition and Image Analysis  26(4):673–680  2016.

[26] D. McAllester. A pac-bayesian tutorial with a dropout bound. Arxiv’13  2013.

[27] M. Moh  A. Rostamizadeh  and A. Talwalkar. Foundations of machine learning. MIT press 

2012.

[28] B. K. Natarajan. On learning sets and functions. Machine Learning  4(1):67–97  1989.

[29] F. Orabona and J. Luo. Ultra-fast optimization algorithm for sparse multi kernel learning. In
Proceedings of the 28th International Conference on Machine Learning (ICML)  pages 249–256 
2011.

[30] F. Orabona  J. Luo  and B. Caputo. Online-batch strongly convex multi kernel learning. In The
23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 787–794 
2010.

[31] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1-regularized loss minimization.

Journal of Machine Learning Research  12:1865–1892  2011.

[32] S. Sonnenburg  G. Rätsch  C. Schäfer  and B. Schölkopf. Large scale multiple kernel learning.

Journal of Machine Learning Research  7:1531–1565  2006.

[33] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning
In Proceedings of the 21st International

for interdependent and structured output spaces.
Conference on Machine Learning (ICML)  page 104  2004.

[34] V. Vapnik. The nature of statistical learning theory. Springer Verlag  2000.

[35] C. Xu  T. Liu  D. Tao  and C. Xu. Local rademacher complexity for multi-label learning. IEEE

Transactions on Image Processing  25(3):1495–1507  2016.

[36] N. Youseﬁ  Y. Lei  M. Kloft  M. Mollaghasemi  and G. Anagnostopoulos. Local rademacher
complexity-based learning guarantees for multi-task learning. arXiv preprint arXiv:1602.05916 
2016.

[37] T. Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225–1251  2004.

[38] A. Zien and C. S. Ong. Multiclass multiple kernel learning.

In Proceedings of the 24th

International Conference on Machine Learning (ICML)  pages 1191–1198  2007.

10

,Ian Osband
Benjamin Van Roy
Jian Li
Yong Liu
Rong Yin
Hua Zhang
Lizhong Ding
Weiping Wang