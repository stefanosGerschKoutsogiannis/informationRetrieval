2014,Learning with Fredholm Kernels,In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels  which we call Fredholm kernels. We proceed to discuss the noise assumption" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.",Learning with Fredholm Kernels

Qichao Que Mikhail Belkin Yusu Wang

Department of Computer Science and Engineering

{que mbelkin yusu}@cse.ohio-state.edu

The Ohio State University

Columbus  OH 43210

Abstract

In this paper we propose a framework for supervised and semi-supervised learning
based on reformulating the learning problem as a regularized Fredholm integral
equation. Our approach ﬁts naturally into the kernel framework and can be inter-
preted as constructing new data-dependent kernels  which we call Fredholm ker-
nels. We proceed to discuss the “noise assumption” for semi-supervised learning
and provide both theoretical and experimental evidences that Fredholm kernels
can effectively utilize unlabeled data under the noise assumption. We demonstrate
that methods based on Fredholm learning show very competitive performance in
the standard semi-supervised learning setting.

1

Introduction

Kernel methods and methods based on integral operators have become one of the central areas of
machine learning and learning theory. These methods combine rich mathematical foundations with
strong empirical performance. In this paper we propose a framework for supervised and unsuper-
vised learning as an inverse problem based on solving the integral equation known as the Fredholm
problem of the ﬁrst kind. We develop a regularization based algorithms for solving these systems
leading to what we call Fredholm kernels.
In the basic setting of supervised learning we are given the data set (xi  yi)  where xi 2 X  yi 2 R.
We would like to construct a function f : X ! R  such that f (xi) ⇡ yi and f is “nice enough”
to generalize to new data points. This is typically done by choosing f from a class of functions (a
Reproducing Kernel Hilbert Space (RKHS) corresponding to a positive deﬁnite kernel for the kernel
methods) and optimizing a certain loss function  such as the square loss or hinge loss.
In this paper we formulate a new framework for learning based on interpreting the learning problem
as a Fredholm integral equation. This formulation shares some similarities with the usual kernel
learning framework but unlike the standard methods also allows for easy incorporation of unlabeled
data. We also show how to interpret the resulting algorithm as a standard kernel method with a
non-standard data-dependent kernel (somewhat resembling the approach taken in [14]).
We discuss reasons why incorporation of unlabeled data may be desireable  concentrating in partic-
ular on what may be termed “the noise assumption” for semi-supervised learning  which is related
but distinct from manifold and cluster assumption popular in the semi-supervised learning literature.
We provide both theoretical and empirical results showing that the Fredholm formulation allows for
efﬁcient denoising of classiﬁers.
To summarize  the main contributions of the paper are as follows:
(1) We formulate a new framework based on solving a regularized Fredholm equation. The frame-
work naturally combines labeled and unlabeled data. We show how this framework can be expressed
as a kernel method with a non-standard data-dependent kernel.

1

(2) We discuss “the noise assumption” in semi-supervised learning and provide some theoretical ev-
idence that Fredholm kernels are able to improve performance of classiﬁers under this assumption.
More speciﬁcally  we analyze the behavior of several versions of Fredholm kernels  based on com-
bining linear and Gaussian kernels. We demonstrate that for some models of the noise assumption 
Fredholm kernel provides better estimators than the traditional data-independent kernel and thus
unlabeled data provably improves inference.
(3) We show that Fredholm kernels perform well on synthetic examples designed to illustrate the
noise assumption as well as on a number of real-world datasets. We also indicate how random
feature approximations can be used to deal with large datasets.

1.1 Related work

Applications of kernel and integral methods in machine learning have a large and diverse literature
(e.g.  [13  12]). The work most directly related to our approach is [10]  where Fredholm integral
equations were introduced to address the problem of density ratio estimation and covariate shift. In
that work the problem of density ratio estimation was expressed as a Fredholm integral equation and
solved using regularization in RKHS. This setting also relates to a line of work on on kernel mean
embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral
operators with applications to density ratio estimation and other tasks [15  4  5]. A very interesting
recent work [9] explores a shrinkage estimator for estimating means in RKHS  following the Stein-
James estimator originally used for estimating the mean in an Euclidean space. The results obtained
in [9] show how such estimators can reduce variance. There is some similarity between that work
and our theoretical results presented in Section 4 which also shows variance reduction for certain
estimators of the kernel although in a different setting.
Another line of connected work is the class of semi-supervised learning techniques related to man-
ifold regularization [1]  where an additional graph Laplacian regularizer is added to take advantage
of the geometric/manifold structure of the data. Our reformulation of Fredholm learning as a ker-
nel  addressing what we called “noise assumptions”  parallels data-dependent kernels for manifold
regularization proposed in [14].

2 Fredholm Kernels

We start by formulating learning framework proposed in this paper.
Suppose we are given l labeled pairs (x1  y1)  . . .   (xl  yl) from the data distribution p(x  y) deﬁned
on X ⇥ Y and u unlabeled points xl+1  . . .   xl+u from the marginal distribution pX(x) on X. For
simplicity we will assume that the feature space X will a Euclidean space RD  and the label set Y
is either {1  1} for binary classiﬁcation the real line R for regression. Semi-supervised learning
algorithms aim to construct a (predictor) function f : X ! Y by incorporating the information of
unlabeled data distribution.
To this end  we introduce the integral operator KpX associated with a kernel function k(x  z). We
note that k(x  z) does not have to be a positive semi-deﬁnite kernel.

KpX : L2 ! L2 and KpX f (x) =Z k(x  z)f (z)pX(z)dz 

(1)

where L2 is the space of square-integrable functions. As usual  by the law of large number  the
above operator can be approximated by the unlabeled data from pX as follows 

KˆpX f (x) =

1

l + u

l+uXi=1

k(x  xi)f (xi).

(2)

This approximation provides a natural way of incorporating unlabeled data into algorithms. In our
Fredholm learning framework  we will use functions in KpXH = {KpX f : f 2H}   where H is
an appropriate Reproducing Kernel Hilbert Space (RKHS) as classiﬁcation or regression functions.
Note that unlike RKHS  this space of functions  KpXH  is density dependent.
In particular  this now allows us to formulate the following optimization problem for semi-supervised
classiﬁcation/regression in a way similar to many supervised learning algorithms:

2

The Fredholm learning framework solves the following optimization problem1:

f⇤ = arg min
f2H

1
l

lXi=1

((KˆpX f )(xi)  yi)2 + kfk2
H 

(3)

The ﬁnal classiﬁer is c(x) = (KˆpX f⇤) (x)  where KˆpX is the operator deﬁned above. Eqn 3 is a
discretized and regularized version of the Fredholm integral equation KpX f = y  thus giving the
name of Fredholm learning framework.
Even though at ﬁrst glance this setting looks similar to conventional kernel methods  the extra layer
introduced by KˆpX makes signiﬁcant difference  in particular  by allowing the integration of infor-
mation from unlabelled data distribution. In contrast  solutions to kernel method for most kernels 
e.g.  linear  polynomial or Gaussian kernels  are completely independent of the unlabeled data. We
note that our approach is closely related to [10] where a Fredholm equation is used to estimated the
density ratio for two probability distributions.
Our Fredholm learning framework is a generalization of the standard kernel framework. In fact  if
the kernel k is the -function  then our formulation above is equivalent to the standard Regularized
Least Squares equation f⇤ = arg minf2H
. We could also replace
the L2 loss in Eqn 3 by other loss functions  such as hinge loss  resulting in a SVM-like classiﬁer.
Finally  even though Eqn 3 is an optimization problem in a potentially inﬁnite dimensional function
space H  we have the following lemma that allows us to apply the Representer Theorem to get a
computationally accessible solution.
Lemma 1. Given the deﬁnition of KˆpX in Eqn 2  the solution to Eqn 3 is of the form 

i=1(f (xi)  yi)2 + kfk2
H

lPl

1

f⇤(x) =

1

l + u

kH(x  xj)vj 

l+uXj=1

l+uXi j=1

lXs=1

3

for some v 2 Rl+u.
As the proof of the above lemma is similar to that of the standard representer theorem  we put
the proof in the appendix. Using the above Representer Theorem  we could transform Eqn 3 into
quadratic optimization in a ﬁnite dimensional space. We can get have a closed form solution for
Eqn 3 as follows:

l+uXj=1

1

l + u

f⇤(x) =

kH(x  xj)vj  v =KT
where (Kl+u)ij = k(xi  xj) for 1  i  l  1  j  l + u  and (KH)ij = kH(xi  xj) for
1  i  j  l + u. Note that Kl+u is a l ⇥ (l + u) matrix.
Fredholm kernels: a convenient reformulation. Interestingly  this Fredholm learning problem
actually induces a new data-dependent kernel  which we will refer to as Fredholm kernel2. To show
this connection  ﬁrst observe the following identity  which can be easily veriﬁed:
Claim 2. Matrix Inversion Identity

l+uKl+uKH + I1

l+uy 

KT

(4)

KT
l+uKl+uKH + I1

Deﬁne KF = Kl+uKHKT

KT

l+u = KT

l+uKl+uKHKT

l+u + I1

.

l+u to be the l ⇥ l kernel matrix associated with a new kernel deﬁned by
(5)

k(x  xi)kH(xi  xj)k(z  xj) 

1

ˆkF (x  z) =

(l + u)2

and we consider the unlabeled data are ﬁxed for computing this new kernel. Using this new kernel
ˆkF   the ﬁnal classifying function c⇤ deﬁned using the solution given in Eqn 4 can be rewritten as:

k(x  xi)f⇤(xi) =

ˆkF (x  xs)↵s  ↵ = (KF + I)1 y.

c⇤(x) =

1

l + u

l+uXi=1

1We will be using the square loss to simplify the exposition. Other loss functions can also be used in Eqn 3.
2We note that the term “Fredholm Kernel” has also been used before in a different context  see page 103  [6]

and [16] in the studies of Fredholm operator. But our usage and the previous one represent different object.

Because of Eqn 5 we will sometimes refer to the kernels kH and k as the “inner” and “outer” kernels
respectively.
It can be observed that this learning algorithm can be considered as a case of the standard kernel
method  but using a new data dependent kernel ˆkF   which we will call the Fredholm kernel  since it
is induced from the Fredholm problem formulated in Eqn 3. And the following proposition shows
that this deﬁnition gives a positive semi-deﬁnite kernel.
Proposition 3. The Fredholm kernel deﬁned in Eqn 5 is positive semi-deﬁnite if kH is a positive
semi-deﬁnite kernel.

The proof is given in the appendix. The “outer” kernel k does not have to be either positive deﬁnite
or even symmetric. When using Gaussian kernel for k  discrete approximation in Eqn 5 might be
unstable when the kernel width is small  so we also introduce the normalized Fredholm kernel 

ˆkN
F (x  z) =

1

(l + u)2

l+uXi j=1

k(x  xi)

Pn k(x  xn)

kH(xi  xj)

k(z  xj)

Pn k(z  xn)

It is easy to check that the resulting Fredholm kernel ˆkN
F is still symmetric and positive semi-deﬁnite.
Using Hinge Loss Other than L2 loss we use above  hinge loss can also be used for our Fredholm
learning framework. In this section  we explain how Fredholm kernel could be derived when using
hinge loss. Plugging the hinge loss into Eqn 3  we have

.

(6)

f⇤ = arg min
f2H

1
l

lXi=1

max(0  1  yi · (KˆpX f )(xi)) + kfk2
H.

(7)

Like the Representer Theorem  we proved in Lemma 1  the solution function f is always of the form

f (x) =

vikH(x  xi).

l+uXi=1

H = vT KHv  where KH is the kernel matrix.

Thus  kfk2
And we only consider the evaluation of f at the data points  let f = [f (x1)  . . .   f (xl+u)] = KHv.
Now we can vectorize (KˆpX f )(xi) as well  by letting ki = [ 1
l+u k(xi  xl+u)].
Thus KˆpX f (xi) = 1
i KHv.
And the optimization problem using hinge loss in Eqn 7 is equivalent to the following problem with
slack variables ⇠i 

l+uPl+u

j=1 k(xi  xj)f (xj) = kT

l+u k(xi  x1)  . . .   1

i f = kT

To solve the above problem  we introduce the Lagrangian multiplier 

L(v ⇠ ↵  ) =

1
2

⇠i Xi

↵i(yi · (kiKHv)  1 + ⇠i) Xi

i⇠i

By the KKT condition in the theory of convex optimization  we have

Using this  we have the dual problem of the original problem in Eqn 7 

⇠i

vT KHv + CXi

1
min
2
f2H
yi · (kT
i KHv)  1  ⇠i
for i = 1  . . .   l
⇠i  0

s.t.

vT KHv + CXi
v =Xi
↵ Xi

s.t.

max

↵i 
0  ↵i  C.

↵iyiki ↵ i = C  i

1

2Xi j

↵i↵jyiyjkT

i KHkj

4

It is equivalent to using Fredholm kernel for regular support vector machine  because kT
kF (xi  xj) according to the deﬁnition of Fredholm kernel in Eqn 5.

i KHkj =

3 The Noise Assumption and Semi-supervised Learning

In order for unlabeled data to be useful in classiﬁcation tasks  it is necessary for the marginal distri-
bution of the features to contain information about the conditional distribution of the labels. Several
ways in which such information can be encoded have been proposed  including the “cluster assump-
tion” [2] and the “manifold assumption” [1]. The cluster assumption states that a cluster (or a high
density area) contains only (or mostly) points belonging to the same class. That is  if x1 and x2
belong to the same cluster  the corresponding labels y1  y2 should be the same. The manifold as-
sumption assumes that the regression function is smooth with respect to the underlying manifold
structure of the data  which can be interpreted as saying that the geodesic distance should be used
instead of the ambient distance for optimal classiﬁcation. The success of algorithms based on these
ideas indicates that these assumptions do capture certain characteristics of real data. Still  better
understanding of data distribution may still lead to progress in data analysis.
The noise assumption. Now we propose to formulate a new assumption  the “noise assumption” 
which is that in the neighborhood of every point  the directions with low variance (of the feature
distribution) are uninformative with respect to the class labels  and can be regarded as noise. While
being intuitive  as far as we know  it has not been explicitly formulated in the context of semi-
supervised learning algorithms  nor applied to theoretical analysis.
Note that even if the noise variance is small
along a single direction 
it could still sig-
niﬁcantly decrease the performance of su-
pervised learning algorithms if the noise are
high-dimensional.
These accumulated non-
informative variations increase the difﬁculty of
learning a good classiﬁer in particular when the
amount of labeled data is small. The Figure 1
on right illustrates the issue of noise with two
labeled points. The seemingly optimal classiﬁ-
cation boundary (the red line) differs from the correct one (in black) due to the noisy variation along
the vertical axis for the two labeled points. Intuitively unlabeled data shown in the right panel of
Figure 1 can be helpful in this setting as low variance directions can be estimated locally such that
algorithms could suppress the inﬂuences of the noisy variation when learning a classiﬁer.
Connection to cluster and manifold assumptions. The noise assumption is compatible with the
manifold assumption within the “manifold+noise” model. Speciﬁcally  we can assume that the
functions of interest vary along the manifold and are constant in the orthogonal direction. Alterna-
tively  we can think of directions with high variance as “signal/manifold” and directions with low
variance as “noise”. We note that the noise assumption does not require the data to conform to
a low-dimensional manifold in the strict mathematical sense of the word. The noise assumption
is orthogonal to the cluster assumption. For example  Figure 1 illustrates a situation where data
has no clusters but the noise assumption applies. For more examples and experimental results see
Section 5.1.

Figure 1: Left: only labelled points  and Right:
with unlabelled points.

4 Theoretical Results for Fredhom Kernels

Non-informative variation in data could degrade the performance of traditional supervised learning
algorithms. We will now show that Fredholm kernels can be used to replace traditional kernels
to inject them with “noise-suppression” power with the help of unlabelled data.
In this section
we will present two views to illustrate how such noise supression can be achieved. Speciﬁcally  in
Section 4.1) we show that under certain setup linear Fredholm kernel supresses principal components
with small variance. In Section 4.2) we prove that under certain conditions Fredholm kernels are
able to provide good approximations to the “true” kernel on the hidden underlying space.
To make our arguments more clear  in what follows  we assume that there is inﬁnite amount of
unlabelled data; that is  we know the marginal distribution of data exactly. We will then consider the
following continuous versions of the un-normalized and normalized Fredholm kernels as in Eqn 5

5

and 6:

and

kU

F (x  z) =Z Z k(x  u)kH(u  v)k(z  v)p(u)p(v)dudv
R k(z  w)p(w)dw

R k(x  w)p(w)dw

kH(u  v)

k(x  u)

k(z  v)

kN

F (x  z) =Z Z

Note  in the above equations and in what follows  we sometimes write p instead of pX for the
marginal distribution when its choice is clear from context. We will typically use kF to denote
appropriate normalized or unnormalized kernels depending on the context.

(8)

(9)

p(u)p(v)dudv.

4.1 Linear Fredholm kernels and inner products

For this section  we consider the unormalized Fredholm kernel  that is kF = kU
F . If the “outer”
kernel k(u  v) is linear  i.e. k(u  v) = uT v  the resulting Fredholm kernel can be viewed as an inner
product. Speciﬁcally  the un-normalized Fredholm kernel from Eqn 8 can be rewitten as

kF (x  z) =Z Z (xT u)(zT v)kH(u  v)p(u)p(v)dudv = xT ⌃F z  where

⌃F =Z Z uvT kH(u  v)p(u)p(v)dudv =Z Z ukH(u  v)vT p(u)p(v)dudv.

(10)

Thus kF (x  z) is simply an inner product which depends on both the data distribution p(x) and
the “inner” kernel kH. This inner product re-weights the standard norm in feature space based on
variances along the principal directions of the matrix ⌃F . We will show that for the model when
data is sampled from a normal distribution this kernel can be viewed as a “soft thresholding” PCA 
suppressing the directions with low variance.
More strictly  we have the following

Theorem 4. Let kH(x  z) = exp⇣kxzk2

a single multi-variate normal distribution  N (µ  diag(2

2t ⌘ and assume the marginal distribution pX for data is

d)). We have

1  . . .   2

d + t!✓µµT + diag✓ 4

22

1
1 + t

  . . .  

4
D
22

D + t◆◆ .

22

⌃F = DYd=1s t
the ith principal direction isq 4

i
22

Assuming that the data is mean-subtracted  i.e. µ = 0  we see that xT ⌃F z re-scales the projections
along the principal components when computing the inner product; that is  the rescaling factor for

i +t.
4
i +t ⇡ 0 when 2
i
22

4
i
22

i  t  we
Note that this rescaling factor
i +t ⇡ 2
have that
2 . Hence t can be considered as a soft threshold that eliminates the effects of
principal components with small variances. When t is small the rescaling factors are approximately
proportional to diag(2
D)  in which case ⌃F is is porportional to the covariance matrix
of the data XX T .

i ⌧ t. On the other hand when 2

2  . . .   2

1  2

i

4.2 Kernel Approximation With Noise

We have seen that one special case of Fredholm kernel could achieve the effect of principal compo-
nents re-scaling by using linear kernel as the “outer” kernel k. In this section we give a more general
interpretation of noise suppression by the Fredholm kernel.
First  we give a simple senario to provide some intuition behind the deﬁnition of Fred-
holm kernle. Consider a standard supervised learning setting which uses the solution f⇤ =
denote the ideal kernel that
arg minf2H
we intend to use on the clean data  which we call the target kernel from now on. Now suppose what
we have are two noisy labeled points xe and ze for “true” data ¯x and ¯z  i.e. xe = ¯x+"x  ze = ¯z +"z.

i=1(f (xi)  yi)2 + kfk2
H

as the classiﬁer. Let ktarget

lPl

H

1

6

The evaluation of ktarget
H (xe  ze) can be quite different from the
true signal ktarget
H (¯x  ¯z)  leading to a suboptimal ﬁnal classiﬁer
(the red line in Figure 1 (a)). On the other hand  now con-
sider the Fredholm kernel from Eqn 8 (or similarly from Eqn 9):
kF (xe  ze) = RR k(xe  u)p(u) · kH(u  v) · k(ze  v)p(v)dudv 
and set the outer kernel k to be the Gaussian kernel  and the in-
ner kernel kH to be the same as target kernel ktarget
. We can think
H
of kF (xe  ze) as an averaging of kH(u  v) over all possible pairs
of data u  v  weighted by k(xe  u)p(u) and k(ze  v)p(v) respec-
tively. Speciﬁcally  points that are close to xe (resp. ze) with
high density will receive larger weights. Hence the weighted
averages will be biased towards ¯x and ¯z respectively (which pre-
sumably lie in high density regions around xe and ze). The value of kF (xe  ze) tends to provide a
more accurate estimate of kH(¯x  ¯z). See the right ﬁgure for an illustration where the arrows indicate
points with stronger inﬂuences in the computation of kF (xe  ze) than kH(xe  ze). As a result  the
classiﬁer obtained using the Fredholm kernel will also be more resilient to noise and closer to the
optimum.
The Fredholm learning framework is rather ﬂexible in terms of the choices of kernels k and kH.
In the remainder of this section  we will consider a few speciﬁc scenarios and provide quantitative
analysis to show the noise-resilliency of the Fredholm kernel. In particular  for Section 4.2.1 and
4.2.2  we will assume the following setup for data.
Problem setup. Assume that we have a ground-truth distribution over the subspace spanned by the
ﬁrst d dimension of the Euclidean space RD. We will assume that this ground-truth distribution is
a single Gaussian N (0  2Id). Now imagine that this ground-truth distribution is corrupted with
Gaussian noise along the orthogonal subspace of dimension D  d. That is  for any observed point
xe  it could be decomposed into ¯x + "x  where the signal ¯x is drawn from N (0  2Id)  and the noise
"x is drawn from N (0  2IDd) over the orthogonal space. Thus any observed point  labelled or
unlabelled  is sampled from pX = N (0  diag(2Id  2IDd)  with the ﬁrst d dimensions as signals
and the rest corrupted by noises.
We will show that Fredholm kernel provides a better approximation to the “original” kernel given
both labeled and unlabeled data than directly computing the kernel evaluation at noisy labeled points.
We choose this simple setting so as to be able to state the theoretical results in a clean manner. Even
though this is just a Gaussian distribution over a linear subspace with noise this framework can be
generalized since local neighborhoods of a Riemannian manifold can be approximated by linear
spaces.
Note. In this section  we use the normalized Fredholm kernel given in Eqn 9 for simplicity  that
is kF = kN
F for now on. Un-normalized Fredholm kernel displays similar behavior  however  the
theoretical bounds are more complicated.

H (u  v) is the linear kernel  ktarget

4.2.1 Linear Kernel
First we consider the case where the target kernel ktarget
H (u  v) = uT v.
We will set kH in Fredholm kernel to also be linear  and k to be the Gaussian kernel k(u  v) =
e kuvk2
2t We will compare kF (xe  ze) with the target kernel on the two observed points  that is 
with ktarget
H (xe  ze). The goal is to estimate ktarget
H (¯x  ¯z). We will see that (1) both kF (xe  ze) and
(appropriately scaled) kH(xe  ze) are unbiased estimators of ktarget
H (¯x  ¯z)  however (2) the variance
of kF (xe  ze) is smaller than that of ktarget
Theorem 5. Suppose the probability distribution for the data is pX = N (0  diag(2Id  2IDd)).
For Fredholm kernel deﬁned in Eqn 9  we have

H (xe  ze)  making it a more precise estimator.

Exe ze(ktarget

H (xe  ze)) = Exe ze ✓ t + 2
2 ◆2

kF (xe  ze)! = ¯xT ¯z

Moreover  when >   Varxe ze✓⇣ t+2
2 ⌘2

kF (xe  ze)◆ < Varxe ze(ktarget

H (xe  ze)).

7

Remark: Note that we have a normalization constant for the Fredholm kernel to make it an unbiased
estimator of ¯xT ¯z. In practice  choosing normalization is subsumed in selecting the regularization
parameter for kernel methods.
We will give a sketch of the proof  complete details can be found in the appendix.
First  we have the following lemma regarding the estimator ktarget
Lemma 6. Given two samples xe ⇠ N (¯x  diag([0d  2IDd]))  ze ⇠ N (¯z  diag([0d  2IDd])) 
let kH(xe  ze) = xT

H (xe  ze).

Exe ze(ktarget

e ze. We have:
H (xe  ze)) = ¯xT ¯z and Varxe ze(ktarget

H (xe  ze)) = (D  d)4.

Now we consider the Fredholm kernel with the help of unlabelled points from the distribution p =
N (0  diag(2Id  2IDd)). Substituting kH(u  v) by the linear kernel uT v in Eqn 9  we have:

kF (xe  ze) =Z Z

k(xe  u)

k(ze  v)

uT vp(u)p(v)dudv

R k(xe  w)p(w)dw
=✓R k(xe  u)up(u)du
R k(xe  w)p(w)dw◆T✓Z

R k(ze  w)p(w)dw
R k(ze  w)p(w)dw◆
2t ⌘. Note R k(xe u)up(u)du
R k(xe w)p(w)dw (resp. R k(ze v)vp(v)dv

k(ze  v)vp(v)dv

R k(ze w)p(w)dw ) is the

(11)

where recall k(u  v) = exp⇣kuvk2

weighted mean of the unlabeled data  with the weight function being the normalized Gaussian kernel
centered at xe (resp. ze). Hence by Eqn 11  kF (xe  ze) is the linear kernel between these two means
(instead of the linear kernel for xe and ze). Thus it is not too surprising that kF (xe  ze) should
be more stable than the straightforward approximation kH(xe  ze). Indeed  we have the following
lemma (proof in appendix).
Lemma 7. Given two samples xe ⇠ N (¯x  diag([0d  2IDd]))  ze ⇠ N (¯z  diag([0d  2IDd])) 
let kH(xe  ze) = xT
e ze and p = N (0  diag(2Id  2IDd)). Let kF be as deﬁned in Eqn 11. We
have:

and

Exe ze ✓ t + 2
2 ◆2
kF (xe  ze)! = (D  d)✓ 2(t + 2)
Varxe ze ✓ t + 2
2 ◆2
2(t + 2)◆4

kF (xe  ze)! = ¯xT ¯z

4

With Lemma 6 and 7  we can now compare the variances. Since 2(t+2)
Theorem 5 follows.
Thus we can see the Fredholm kernel provides an approximation of the “true” linear kernel  but with
smaller variance than the linear kernel on noisy data.

2(t+2) < 1 when 2 > 2 

H (u  v) =

4.2.2 Gaussian Kernel
We now consider the case where the target kernel is the Gaussian kernel: ktarget

exp⇣kuvk2

2r ⌘. To approximate this kernel  we will set both k and kH to be Gaussian kernels.

To simplify the presentation of results  we assume that k and kH have the same kernel width t. The
resulting Fredholm kernel turns out to also be a Gaussian kernel  whose kernel width depends on the
choice of t.
Our main result is the following. Again  similar to the case of linear kernel  the Fredholm estimator
kF (xe  ze) and the vanilla one ktarget
H (¯x  ¯z)
upto a constant; but kF (xe  ze) has a smaller variance.
Theorem 8. Suppose
the probability distribution for
=
N (0  diag(2Id  2IDd)). Given the target kernel ktarget
nel width r > 0  we can choose t  given by the equation t(t+2)(t+32)
constants c1  c2  such that
Exe ze(c1

H (xe  ze) are both unbiased estimator for the target ktarget
the unlabeled data pX

H (u  v) = exp⇣kuvk2

2r ⌘ with ker-

2 kF (xe  ze)) = ktarget

= r  and two scaling

1 ktarget

H (xe  ze)) = Exe ze(c1

H (¯x  ¯z).

4

8

1 ktarget

H (xe  ze)) > Varxe ze(c1

and when 2 > 2  we have Varxe ze(c1
Remark.
In practice  when applying kernel methods for real world applications  optimal kernel
width r is usually unknown and chosen by cross-validation or other methods. Similarly  for our
Fredholm kernel  one can also use cross-validation to choose the optimal t for kF .
The proof of Theorem 8 is more complicated than in the linear case  and can be found in the ap-
pendix.

2 kF (xe  ze)).

5 Experiments

In this section  we will demonstrate our Fredholm kernel empirically using both synthetic examples
and data sets of text categorization and handwriting recognition. In section 5.1  we will use several
examples to illustrate the effect of reducing variances using Fredholm kernel and how noise assump-
tion is distinguished from the conventional assumptions in semi-supervised learning  such as cluster
assumption and manifold assumption. In section 5.2  we show how classiﬁers based on Fredholm
kernel perform on real world data sets like hand-written digits recognition and text categorization
problems  compared with other semi-supervised algorithms.
First recall the Fredholm kernel we deﬁned in previous section.

ˆkF (x  z) =

1

(l + u)2

l+uXi j=1

k(x  xi)kH(xi  xj)k(z  xj).

And using linear and Gaussian kernel for k or kH  we can deﬁne three instances of the Fredholm
kernel as follows.

(1) FredLin1: k(x  z) = xT z and kH(x  z) = exp⇣kxzk2
2r ⌘.
(2) FredLin2: k(x  z) = exp⇣kxzk2
2r ⌘ and kH(x  z) = xT z.
(3) FredGauss: k(x  z) = kH(x  z) = exp⇣kxzk2
2r ⌘.

For the kernels in (2) and (3) that use the Gaussian kernel as outside kernel k  intuitively we can also
deﬁne their normalized version using the following deﬁnition 

ˆkF n(x  z) =

1

(l + u)2

l+uXi j=1

k(x  xi)

Pn k(x  xn)

kH(xi  xj)

.

k(z  xj)

Pn k(z  xn)

The resulting kernels are denoted by FredLin2(N) and FredGauss(N) respectively.

5.1 Synthetic Examples

Using specially designed toy examples  we could empirically verify the behavior of Fredholm kernel
characterized by theoretical results in last section.

5.1.1 Principal Component Regression
As we have pointed out in Theorem 4  Fredholm kernel and the associated Fredholm inner prod-
uct space could stress the principal components with larger variances while suppressing the ones
with smaller variances. Instead of hard cutting-off in many PCA-based methods  it provides a soft
thresholding algorithm for feature selection. To demonstrate our methods  we consider the principal
component regression problem [8]  which assumes that the regressor X and response Y have the
following relationship:

Y = ↵Xu1 

wher u1 is the ﬁrst principal component. In this experiments  the data distribution is a Gaussian
distribution N (0  diag([10  1  . . .   1])). Note that the axes themselves are the principal components.
We will compare our method with linear regression using (1) all the dimensions; and (2) ﬁrst k
principal components  while Fredholm kernel does not need to do any hard thresholding. Figure 2

9

,Qichao Que
Mikhail Belkin
Yusu Wang
Gregory Rogez
Cordelia Schmid
Adji Bousso Dieng
Dustin Tran
Rajesh Ranganath
David Blei