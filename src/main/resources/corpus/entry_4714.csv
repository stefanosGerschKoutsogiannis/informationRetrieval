2019,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks,With substantial amount of time  resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN)  there emerges an urgent need to protect these inventions from being illegally copied  redistributed  or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line  we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks  which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole  this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that  the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words  genuine passports are not only verified by looking for the predefined signatures  but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR,Rethinking Deep Neural Network

Ownership Veriﬁcation: Embedding Passports to

Defeat Ambiguity Attacks

Lixin Fan1 Kam Woh Ng2 Chee Seng Chan2

1WeBank AI Lab  Shenzhen  China

2Center of Image and Signal Processing  Faculty of Comp. Sci. and Info.  Tech.

University of Malaya  Kuala Lumpur  Malaysia

{lixinfan@webank.com;kamwoh@siswa.um.edu.my;cs.chan@um.edu.my}

Abstract

With substantial amount of time  resources and human (team) efforts invested to
explore and develop successful deep neural networks (DNN)  there emerges an
urgent need to protect these inventions from being illegally copied  redistributed  or
abused without respecting the intellectual properties of legitimate owners. Follow-
ing recent progresses along this line  we investigate a number of watermark-based
DNN ownership veriﬁcation methods in the face of ambiguity attacks  which aim
to cast doubts on the ownership veriﬁcation by forging counterfeit watermarks. It
is shown that ambiguity attacks pose serious threats to existing DNN watermarking
methods. As remedies to the above-mentioned loophole  this paper proposes novel
passport-based DNN ownership veriﬁcation schemes which are both robust to
network modiﬁcations and resilient to ambiguity attacks. The gist of embedding
digital passports is to design and train DNN models in a way such that  the DNN
inference performance of an original task will be signiﬁcantly deteriorated due to
forged passports. In other words  genuine passports are not only veriﬁed by looking
for the predeﬁned signatures  but also reasserted by the unyielding DNN model
inference performances. Extensive experimental results justify the effectiveness
of the proposed passport-based DNN ownership veriﬁcation schemes. Code and
models are available at https://github.com/kamwoh/DeepIPR

1

Introduction

With the rapid development of deep neural networks (DNN)  Machine Learning as a Service (MLaaS)
has emerged as a viable and lucrative business model. However  building a successful DNN is
not a trivial task  which usually requires substantial investments on expertise  time and resources.
As a result of this  there is an urgent need to protect invented DNN models from being illegally
copied  redistributed or abused (i.e. intellectual property infringement). Recently  for instance  digital
watermarking techniques have been adopted to provide such a protection  by embedding watermarks
into DNN models during the training stage. Subsequently  ownerships of these inventions are veriﬁed
by the detection of the embedded watermarks  which are supposed to be robust to multiple types of
modiﬁcations such as model ﬁne-tuning  model pruning and watermark overwriting [1–4].
In terms of deep learning methods to embed watermarks  existing approaches can be broadly cat-
egorized into two schools: a) the feature-based methods that embed designated watermarks into
the DNN weights by imposing additional regularization terms [1  3  5]; and b) the trigger-set based
methods that rely on adversarial training samples with speciﬁc labels (i.e. backdoor trigger sets)
[2  4]. Watermarks embedded with either of these methods have successfully demonstrated robustness

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Application Scenario.

(b) Present Solution.

(c) Proposed Solution.

Figure 1: DNN model ownership veriﬁcation in the face of ambiguity attacks. (a): Owner Alice
uses an embedding process E to train a DNN model with watermarks (T  s) and releases the model
publicly available; Attacker Bob forges counterfeit watermarks (T(cid:48)  s(cid:48)) with an invert process I;
(b): The ownership is in doubt since both the original and forged watermarks are detected by the
veriﬁcation process V (Sect. 2.2); (c): The ambiguity is resolved when our proposed passports are
embedded and the network performances are evaluated in favour of the original passport by the
ﬁdelity evaluation process F (See Deﬁnition 1 and Sect. 3.3).

against removal attacks which involve modiﬁcations of the DNN weights such as ﬁne-tuning or
pruning. However  our studies disclose the existence and effectiveness of ambiguity attacks which
aim to cast doubt on the ownership veriﬁcation by forging additional watermarks for DNN models in
question (see Fig. 1). We also show that it is always possible to reverse-engineer forged watermarks
at minor computational cost where the original training dataset is also not needed (Sect. 2).
As remedies to the above-mentioned loophole  this paper proposes a novel passport-based approach.
There is a unique advantage of the proposed passports over traditional watermarks - i.e. the inference
performance of a pre-trained DNN model will either remain intact given the presence of valid
passports  or be signiﬁcantly deteriorated due to either the modiﬁed or forged passports. In other
words  we propose to modulate the inference performances of the DNN model depending on the
presented passports  and by doing so  one can develop ownership veriﬁcation schemes that are both
robust to removal attacks and resilient to ambiguity attacks at once (Sect. 3).
The contributions of our work are threefold: i) we put forth a general formulation of DNN own-
ership veriﬁcation schemes and  empirically  we show that existing DNN watermarking methods
are vulnerable to ambiguity attacks; ii) we propose novel passport-based veriﬁcation schemes and
demonstrate with extensive experiment results that these schemes successfully defeat ambiguity
attacks; iii) methodology-wise  the proposed modulation of DNN inference performance based on
the presented passports (Eq. 4) plays an indispensable role in bringing the DNN model behaviours
under control against adversarial attacks.

1.1 Related work

Uchida et. al [1] was probably the ﬁrst work that proposed to embed watermarks into DNN models
by imposing an additional regularization term on the weights parameters. [2  6] proposed to embed
watermarks in the classiﬁcation labels of adversarial examples in a trigger set  so that the watermarks
can be extracted remotely through a service API without the need to access the network weights
(i.e. black-box setting). Also in both black-box and white box settings  [3  5  7] demonstrated how
to embed watermarks (or ﬁngerprints) that are robust to various types of attacks. In particular  it
was shown that embedded watermarks are in general robust to removal attacks that modify network
weights via ﬁne-tuning or pruning. Watermark overwriting  on the other hand  is more problematic
since it aims to simultaneously embed a new watermark and destroy the existing one. Although [5]
demonstrated robustness against overwriting attack  it did not resolve the ambiguity resulted from the
counterfeit watermark. Adi et al. [2] also discussed how to deal with an adversary who ﬁne-tuned an
already watermarked networks with new trigger set images. Nevertheless  [2] required the new set
of images to be distinguishable from the true trigger set images. This requirement is however often
unfulﬁlled in practice  and our experiment results show that none of existing watermarking methods
are able to deal with ambiguity attacks explored in this paper (see Sect. 2).
In the context of digital image watermarking  [8  9] have studied ambiguity attacks that aim to create
an ambiguous situation in which a watermark is reverse-engineered from an already watermarked
image  by taking advantage of the invertibility of forged watermarks [10].
It was argued that
robust watermarks do not necessarily imply the ability to establish ownership  unless non-invertible
watermarking schemes are employed (see Proposition 2 for our proposed solution).

2

2 Rethinking Deep Neural Network Ownership Veriﬁcation

This section analyses and generalizes existing DNN watermarking methods in the face of ambiguity
attacks. We must emphasize that the analysis mainly focuses on three aspects i.e. ﬁdelity  robustness
and invertibility of the ownership veriﬁcation schemes  and we refer readers to representative previous
work [1–4] for formulations and other desired features of the entire watermark-based intellectual
property (IP) protection schemes  which are out of the scope of this paper.

2.1 Reformulation of DNN ownership veriﬁcation schemes

Figure 1 summarizes the application scenarios of DNN model ownership veriﬁcations provided by
the watermark based schemes. Inspired by [10]  we also illustrate an ambiguous situation in which
rightful ownerships cannot be uniquely resolved by the current watermarking schemes alone. This
loophole is largely due to an intrinsic weakness of the watermark-based methods i.e. invertibility.
Formally  the deﬁnition of DNN model ownership veriﬁcation schemes is generalized as follows.
Deﬁnition 1. A DNN model ownership veriﬁcation scheme is a tuple V = (E  F  V  I) of processes:

I) An embedding process E(cid:0)Dr  T  s  N[·]  L(cid:1) = N[W  T  s]  is a DNN learning process that

takes training data Dr = {Xr  yr} as inputs  and optionally  either trigger set data T =
{XT   yT} or signature s  and outputs the model N[W  T  s] by minimizing a given loss L.
Remark: the DNN architectures are pre-determined by N[·] and  after the DNN weights W are
learned  either the trigger set T or signatures s will be embedded and can be veriﬁed by the
veriﬁcation process deﬁned next1.

II) A ﬁdelity evaluation process F(cid:0)N[W · ·]  Dt Mt  f

(cid:1) = {True  False} is to evaluate whether

or not the discrepancy is less than a predeﬁned threshold i.e. |M(N[W · ·]  Dt) − Mt| ≤ f  
in which M(N[W · ·]  Dt) is the DNN inference performance tested against a set of test data
Dt where Mt is the target inference performance.
Remark: it is often expected that a well-behaved embedding process will not introduce a signiﬁ-
cant inference performance change that is greater than a predeﬁned threshold f . Nevertheless 
this ﬁdelity condition remains to be veriﬁed for DNN models under either removal attacks or
ambiguity attacks.

III) A veriﬁcation process V (N[W · ·]  T  s  s) = {True  False} checks whether or not the ex-
pected signature s or trigger set T is successfully veriﬁed for a given DNN model N[W · ·].
Remark: for feature-based schemes  V involves the detection of embedded signatures s =
{P  B} with a false detection rate that is lesser than a predeﬁned threshold s. Speciﬁcally  the
detection boils down to measure the distances Df (fe(W  P)  B) between target feature B and
features extracted by a transformation function fe(W  P) parameterized by P.
Remark: for trigger-set based schemes  V ﬁrst invokes a DNN inference process that takes
trigger set samples Tx as inputs  and then it checks whether the prediction f (W  XT ) produces
the designated labels Ty with a false detection rate that is lesser than a threshold s.

IV) An invert process I(N[W  T  s]) = N[W  T(cid:48)  s(cid:48)] exists and constitutes a successful ambiguity

attack  if
(a) a set of new trigger set T(cid:48) and/or signature s(cid:48) can be reverse-engineered for a given DNN
(b) the forged T(cid:48)  s(cid:48) can be successfully veriﬁed with respect to the given DNN weights W

model;
i.e. V (I(N[W  T  s])  T(cid:48)  s(cid:48)  s) = True;

(c) the ﬁdelity evaluation outcome F(cid:0)N[W · ·]  Dt Mt  f

(cid:1) deﬁned in Deﬁnition 1.II re-

mains True.
Remark: this condition plays an indispensable role in designing the non-invertible veriﬁca-
tion schemes to defeat ambiguity attacks (see Sect. 3.3).

1Learning hyper-parameters such as learning rate and the type of optimization methods are considered

irrelevant to ownership veriﬁcations  and thus they are not included in the formulation.

3

Feature based method [1]

Trigger-set based method [2]

CIFAR10

Real WM Det.

Fake WM Det.

CIFAR10

Fake WM Det.

100 (100)
100 (100)

64.25 (90.97)
74.08 (90.97)

CIFAR100
Caltech-101
Table 1: Detection of embedded watermark (in %) with two representative watermark-based DNN
methods [1  2]  before and after DNN weights ﬁne-tuning for transfer learning tasks. Top row denotes
a DNN model trained with CIFAR10 and weights ﬁne-tuned for CIFAR100; while bottom row
denotes weight ﬁne-tuned for Caltech-101. Accuracy outside bracket is the transferred task  while
in-bracket is the original task. WM Det. denotes the detection accuracies of real and fake watermarks.

65.20 (91.03)
75.06 (91.03)

27.80 (100)
46.80 (100)

100 (100)
100 (100)

Real WM Det.
25.00 (100)
43.60 (100)

V) If at least one invert process exists for a DNN veriﬁcation scheme V  then the scheme is called
an invertible scheme and denoted by V I = (E  F  V  I (cid:54)= ∅); otherwise  the scheme is called
non-invertible and denoted by V∅ = (E  F  V ∅).

The deﬁnition as such is abstract and can be instantiated by concrete implementations of processes
and functions. For instance  the following combined loss function (Eq. 1) generalizes loss functions
adopted by both the feature-based and trigger-set based watermarking methods:

(cid:0)f (W  Xr)  yr

(cid:1) + λtLc

(cid:0)f (W  XT )  yT

(cid:1) + λrR(W  s) 

L = Lc

(1)

in which λt  λr are the relative weight hyper-parameters  f (W  X−) are the network predictions
with inputs Xr or XT . Lc is the loss function like cross-entropy that penalizes discrepancies between
the predictions and the target labels yr or yT . Signature s = {P  B} consists of passports P and
signature string B. The regularization terms could be either R = Lc(σ(W  P)  B) as in [1] or
R = M SE(B − PW) as in [3].
It must be noted that  for those DNN models that will be used for classiﬁcation tasks  their inference
performance M(N[W · ·]  Dt) = Lc
independent of either the embedded signature s or trigger set T. It is this independence that induces
an invertible process for existing watermark-based methods as described next.
Proposition 1 (Invertible process). For a DNN ownership veriﬁcation scheme V as in Deﬁnition 1 
if the ﬁdelity process F () is independent of either the signature s or trigger set T  then there always
exists an invertible process I() i.e. the scheme is invertible V I = (E  F  V  I (cid:54)= ∅)).

(cid:1) tested against a dataset Dt = {Xt  yt} is

(cid:0)f (W  Xt)  yt

2.2 Watermarking in the face of ambiguity attacks

As proved by Proposition 1  one is able to construct forged watermarks for any already watermarked
networks. We tested the performances of two representative DNN watermarking methods [1  2] 
and Table 1 shows that counterfeit watermarks can be forged for the given DNN models with 100%
detection rate  and 100% fake trigger set images can be reconstructed as well in the original task.
Given that the detection accuracies for the forged trigger set is slightly better than the original trigger
set after ﬁne-tuning  the claim of the ownership is ambiguous and cannot be resolved by neither
feature-based nor trigger-set based watermarking methods. Shockingly  the computational cost to
forge counterfeit watermarks is quite minor where the forging required no more than 100 epochs to
optimize  and worst still this is achieved without the need of original training data.
In summary  the ambiguity attacks against DNN watermarking methods are effective with minor
computational and without the need of original training datasets. We ascribe this loophole to the crux
that the loss of the original task  i.e. Lc
We refer readers to our extended version [11] for an elaboration on the ambiguity attack method we
adopted and more detailed experiment results. In the next section  we shall illustrate a solution to
defeat the ambiguity attacks.

(cid:1) is independent of the forged watermarks.

(cid:0)f ( ˆW  Xr)  yr

3 Embedding passports for DNN ownership veriﬁcation

The main motivation of embedding digital passports is to design and train DNN models in a way such
that  their inference performances of the original task (i.e. classiﬁcation accuracy) will be signiﬁcantly
deteriorated due to the forged signatures. We shall illustrate next ﬁrst how to implement the desired
property by incorporating the so called passport layers  followed by different ownership protection
schemes that exploit the embedded passports to effectively defeat ambiguity attacks.

4

γ  pl

(a) An example in the ResNet layer that consists of
the proposed passporting layers. pl = {pl
β} is the
p ∗
proposed digital passports where F = Avg(Wl
γ β) is a passport function to compute the hidden
Pl
parameters (i.e. γ and β) given in Eq. (2).
Figure 2: (a) Passport layers in ResNet architecture and (b) Classiﬁcation accuracies modulated by
different passports in CIFAR10  e.g. given counterfeit passports  the DNN models performance will
be deteriorated instantaneously to fend off illegal usage.

(b) A comparison of CIFAR10 classiﬁcation accuracies
given the original DNN  proposed DNN with valid
passports  proposed DNN with randomly generated
passports (f ake1)  and proposed DNN with reverse-
engineered passports (f ake2).

3.1 Passport layers

In order to control the DNN model functionalities by the embedded digital signatures i.e. passports 
we proposed to append after a convolution layer a passport layer  whose scale factor γ and bias shift
term β are dependent on both the convolution kernels Wp and the designated passport P as follows:

p ∗ Xl
βl = Avg(Wl

γl = Avg(Wl

p + βl = γl(Wl
Ol(Xp) = γlXl
p ∗ Pl
γ) 

(2)
(3)
in which ∗ denotes the convolution operations  l is the layer number  Xp is the input to the passport
layer and Xc is the input to the convolution layer. O() is the corresponding linear transformation of
β are the passports used to derive scale factor and bias term respectively.
outputs  while Pl
Fig. 2a delineates the architecture of digital passport layers used in a ResNet layer.
Remark: for DNN models trained with passport se = {Pl
β}l  their inference performances
γ  Pl
M(N[W  se]  Dt  st) depend on the running time passports st i.e.

c) + βl 
p ∗ Pl
β) 

γ and Pl

(cid:26) Mse  

if st = se 
Mse   otherwise.

M(N[W  se]  Dt  st) =

(4)
If the genuine passport is not presented st (cid:54)= se  the running time performance Mse is signiﬁcantly
deteriorated because the corresponding scale factor γ and bias terms β are calculated based on the
wrong passports. For instance  as shown in Fig. 2b  a proposed DNN model presented with valid
passports (green) will demonstrate almost identical accuracies as to the original DNN model (red). In
contrast  the same proposed DNN model presented with counterfeit passports (blue)  the accuracy
will deteriorate to merely about 10% only.
Remark: the gist of the proposed passport layer is to enforce dependence between scale factor  bias
terms and network weights. As shown by the Proposition 2  it is this dependence that validates the
required non-invertibility to defeat ambiguity.
Proposition 2 (Non-invertible process). A DNN ownership veriﬁcation scheme V as in Deﬁnition 1
is non-invertible  if

I) the ﬁdelity process outcome F(cid:0)N[W  T  s]  Dt Mt  f

(cid:1) depends either on the presented sig-

nature s or trigger set T 

II) with forged passport st (cid:54)= se  the DNN inference performance M(N[W  se]  Dt  st) in (Eq. 4)

will deteriorate such that the discrepancy is larger than a threshold i.e. |Mse − Mse| > f .

5

3.2 Sign of scale factors as signature

During learning the DNN  to further protect the DNN models ownership from insider threat (e.g. a
former staff who establish a new start-up business with all the resources stolen from originator)  one
can enforce the scale factor γ to take either positive or negative signs (+/-) as designated  so that it
will form a unique signature string (like ﬁngerprint). This process is done by adding the following
sign loss regularization term into the combined loss (Eq. 1):

C(cid:88)

R(γ  P  B) =

max(γ0 − γibi  0)

(5)

i=1

in which B = {b1 ···   bC} ∈ {−1  1}C consists of the designated binary bits for C convolution
kernels  and γ0 is a positive control parameter (0.1 by default unless stated otherwise) to encourage
the scale factors have magnitudes greater than γ0.
It must be highlighted that the inclusion of sign loss (Eq. 5) enforces the scale factors γ to take either
positive or negative values  and the signs enforced in this way remain rather persistent against various
adversarial attacks. This feature explains the superior robustness of embedded passports against
ambiguity attacks by reverse-engineering shown in Sect. 4.2.

3.3 Ownership veriﬁcation with passports

Taking advantages of the proposed passport-based approach  we design three new ownership veriﬁca-
tion schemes V that are summarized next and refer readers to Sect. 4 for the experiment results.
V1: Passport is distributed with the trained DNN model
Hereby  the learning process aims to minimize the combined loss function (Eq. 1)  in which λt = 0
since trigger set images are not used in this scheme and the sign loss (Eq. 5) is added as the
regularization term. The trained DNN model together with the passport are then distributed to
legitimate users  who perform network inferences with the given passport fed to the passport layers
as shown in Fig. 2a. The network ownership is automatically veriﬁed by the distributed passports. As
shown in Table 2 and Fig. 3  this ownership veriﬁcation is robust to DNN model modiﬁcations. Also 
as shown in Fig. 4  ambiguity attacks are not able to forge a set of passport and signature that can
maintain the DNN inference performance.
The downside of this scheme is the requirement to use passports during inferencing  which leads
to extra computational cost by about 10% (see Sect. 4.3). Also the distribution of passports to the
end-users is intrusive and imposes additional responsibility of guarding the passports safely.
V2: Private passport is embedded but not distributed
Herein  the learning process aims to simultaneously achieve two goals  of which the ﬁrst is to
minimize the original task loss (e.g. classiﬁcation accuracy discrepancy) when no passport layers
included; and the second is to minimize the combined loss function (Eq. 1) with passports regu-
larization included. Algorithm-wise  this multi-task learning is achieved by alternating between
the minimization of these two goals. The successfully trained DNN model is then distributed to
end-users  who may perform network inference without the need of passports. Note that this is
possible since passport layers are not included in the distributed networks. The ownership veriﬁcation
will be carried out only upon requested by the law enforcement  by adding the passport layers to the
network in question and detecting the embedded sign signatures with unyielding the original network
inference performances.
Compared with scheme V1  this scheme is easy to use for end-users since no passport is needed and
no extra computational cost is incurred. In the meantime  this ownership veriﬁcation is robust to
removal attacks as well as ambiguity attacks. The downside  however  is the requirement to access the
DNN weights and to append the passport layers for ownership veriﬁcation  i.e. the disadvantages of
white-box protection mode as discussed in [2]. Therefore  we propose to combine it with trigger-set
based veriﬁcation that will be described next.
V3: Both the private passport and trigger set are embedded but not distributed
This scheme only differs from scheme V2 in that  a set of trigger images is embedded in addition to
the embedded passports. The advantage of this  as discussed in [2] is to probe and claim ownership

6

CIFAR10
CIFAR100
- (65.53)
100 (64.64)
- (62.17)
99.91 (59.31)
99.96 (59.41)

Caltech-101
- (76.33)
100 (73.03)
- (73.28)
100 (70.87)
100 (71.37)

CIFAR10
- (91.12)
100 (90.91)
- (90.88)
100 (89.44)
100 (89.15)

- (94.85)
100 (94.62)
- (93.65)
100 (93.41)
100 (93.26)

AlexNetp
Baseline (BN)
Scheme V1
Baseline (GN)
Scheme V2
Scheme V3
ResNetp-18
Baseline (BN)
- (82.88)
Scheme V1
99.99 (79.27)
Baseline (GN)
- (79.15)
Scheme V2
100 (77.34)
Scheme V3
100 (77.46)
Table 2: Removal Attack (Fine-tuning): Detection/Classiﬁcation accuracy (in %) of different passport
networks where BN = batch normalisation and GN = group normalisation. (Left: trained with
CIFAR10 and ﬁne-tune for CIFAR100/Caltech-101. Right: trained with CIFAR100 and ﬁne-tune for
CIFAR10/Caltech-101.) Accuracy outside bracket is the signature detection rate  while in-bracket is
the classiﬁcation rate.

AlexNetp
Baseline (BN)
Scheme V1
Baseline (GN)
Scheme V2
Scheme V3
ResNetp-18
Baseline (BN)
Scheme V1
Baseline (GN)
Scheme V2
Scheme V3

Caltech-101
- (79.66)
100 (78.83)
- (78.08)
100 (76.31)
100 (75.89)

- (72.62)
100 (69.63)
- (69.40)
100 (63.84)
99.98 (63.61)

- (78.98)
100 (72.13)
- (75.08)
100 (71.07)
99.99 (72.00)

CIFAR100
- (68.26)
100 (68.31)
- (65.09)
100 (64.09)
100 (63.67)

- (76.25)
100 (75.52)
- (72.06)
100 (72.15)
100 (72.10)

CIFAR100
CIFAR10
- (89.46)
100 (89.07)
- (88.30)
100 (87.47)
100 (87.46)

- (93.22)
100 (95.28)
- (91.83)
100 (90.94)
100 (91.30)

of the suspect DNN model through remote calls of service APIs. This capability allows one  ﬁrst
to claim the ownership in a black-box mode  followed by reassertion of ownership with passport
veriﬁcation in a white box mode. Algorithm-wise  the embedding of trigger set images is jointly
achieved in the same minimization process that embeds passports in scheme V2. Finally  it must be
noted that the embedding of passports in both V2 and V3 schemes are implemented through multi-task
learning tasks where we adopted group normalisation [12] instead of batch normalisation [13] that is
not applicable due to its dependency on running average of batch-wise training samples.

4 Experiment results

This section illustrates the experiment results of passport-based DNN models whereas the inference
performances of various schemes are compared in terms of robustness to both removal attacks and
ambiguity attacks. The network architectures we investigated include the well-known AlexNet
and ResNet-18  which are tested with typical CIFAR10 and CIFAR100 classiﬁcation tasks. These
medium-sized public datasets allow us to perform extensive tests of the DNN model performances.
Unless stated otherwise  all experiments are repeated 5 times and tested against 50 fake passports to
get the mean inference performance. Also  to avoid confusion to the original AlexNet and ResNet
models  we denote AlexNetp and ResNetp-18 as our proposed passport-based DNN models.

4.1 Robustness against removal attacks

Fine-tuning
Table 2 shows that the signatures are detected at near to 100% accuracy for all the ownership
veriﬁcation schemes in the original task. Even after ﬁne-tuning the proposed DNN models for a
new task (e.g. from CIFAR10 to Caltech-101)  almost 100% accuracy are still maintained. Note
that a detected signature is claimed only iff all the binary bits are exactly matched. We ascribe
this superior robustness to the unique controlling nature of the scale factors — in case that a scale
factor value is reduced near to zero  the channel output will be virtually zero  thus  its gradient will
vanish and lose momentum to move towards to the opposite value. Empirically we have not observed
counter-examples against this explanation2.
Model pruning
The aim of model pruning is to reduce redundant parameters without compromise the performance.
Here  we adopt the class-blind pruning scheme in [14]  and test our proposed DNN models with
different pruning rates. Figure 3 shows that  in general  our proposed DNN models still maintained
near to 100% accuracy even 60% parameters are pruned  while the accuracy of testing data drops
around 5%-25%. Even if we prune 90% parameters  the accuracy of our proposed DNN models are
still much higher than the accuracy of testing data. As said  we ascribe the robustness against model
pruning to the superior persistence of signatures embedded in the scale factor signs (see Sect. 3.2).

2A rigorous proof of this argument is under investigation and will be reported elsewhere.

7

(a) AlexNetp

(b) ResNetp-18

Figure 3: Removal Attack (Model Pruning): Classiﬁcation accuracy of our passport-based DNN
models on both CIFAR10/CIFAR100 and signature detection accuracy against different pruning rates.

(a) AlexNetp. (Left) CIFAR10  (Right) CIFAR100.
(b) ResNetp-18. (Left) CIFAR10  (Right) CIFAR100.
Figure 4: Ambiguity Attack: Classiﬁcation accuracy of our passport networks with valid passport 
random attack (f ake1) and reversed-engineering attack (f ake2) on CIFAR10 and CIFAR100.

Ambiguity attack

modes
f ake1
f ake2

Attackers have

access to

W

W   {Dr;Dt}

f ake3

W   {Dr;Dt} 

{P   S}

Ambiguous passport
construction methods

Invertibility
(see Def. 1.V)

- Random passport Pr
- Reverse engineer passport Pe
- Reverse engineer passport {Pe;Se}
by exploiting original passport P
& sign string S

- F (Pr) fail  by large margin
- F (Pe) fail  by moderate margin
- if Se = S:
F (Pe) pass  with negligible margin
- if Se (cid:54)= S:
F (Pe) fail  by moderate to huge margin

V1

Veriﬁcation scheme
Veriﬁcation scheme
Large accuracy ↓
Large accuracy ↓
Moderate accuracy ↓ Moderate accuracy ↓ Moderate accuracy ↓

Veriﬁcation scheme
Large accuracy ↓

V2

V3

refer to Fig. 5

refer to Fig. 5

refer to Fig. 5

Table 3: Summary of overall passport network performances in Scheme V1  V2 and V3  respectively
under three different ambiguity attack modes  f ake.

4.2 Resilience against ambiguity attacks

As shown in Fig. 4  the accuracy of our proposed DNN models trained on CIFAR10/100 classiﬁcation
task is signiﬁcantly depending on the presence of either valid or counterfeit passports — the proposed
DNN models presented with valid passports demonstrated almost identical accuracies as to the
original DNN model. Contrary  the same proposed DNN model presented with invalid passports (in
this case of f ake1 = random attack) achieved only 10% accuracy which is merely equivalent to a
random guessing. In the case of f ake2  we assume that the adversaries have access to the original
training dataset  and attempt to reverse-engineer the scale factor and bias term by freezing the trained
DNN weights. It is shown that in Fig. 4  reverse-engineering attacks are only able to achieve  for
CIFAR10  at best 84% accuracy on AlexNetp and 70% accuracy on ResNetp-18. While in CIFAR100 
for f ake1 case  attack on both our proposed DNN models achieved only 1% accuracy; for f ake2
case  this attack only able to achieve 44% accuracy for AlexNetp and 35% accuracy for ResNetp-18.
Table 3 summarizes the accuracy of the proposed methods under three ambiguity attack modes  f ake
depending on attackers’ knowledge of the protection mechanism. It shows that all the corresponding
passport-based DNN models accuracies are deteriorated to various extents. The ambiguous attacks
are therefore defeated according to the ﬁdelity evaluation process  F (). We’d like to highlight that
even under the most adversary condition  i.e. freezing weights  maximizing the distance from the
original passport P   and minimizing the accuracy loss (in layman terms  it means both the original
passports and scale signs are exploited due to insider threat  and we class this as f ake3)  attackers
are still unable to use new (modiﬁed) scale signs without compromising the network accuracies.
As shown in Fig. 5  with 10% and 50% of the original scale signs are modiﬁed  the CIFAR100
classiﬁcation accuracy drops about 5% and 50%  respectively. In case that the original scale sign
remains unchanged  the DNN model ownership can be easily veriﬁed by the pre-deﬁned string of
signs. Also  Table 3 shows that attackers are unable to exploit Dt to forge ambiguous passports.
Based on these empirical studies  we decide to set the threshold f in Deﬁnition 1 as 3% for AlexNetp
and 20% for ResNetp-18  respectively. By this ﬁdelity evaluation process  any potential ambiguity

8

020406080100Pruningrate(%)020406080100Accuracy(%)CIFAR10Signature020406080100Pruningrate(%)020406080100Accuracy(%)CIFAR100Signature020406080100Pruningrate(%)020406080100Accuracy(%)CIFAR10Signature020406080100Pruningrate(%)020406080100Accuracy(%)CIFAR100Signature0.00.20.40.60.81.00.00.20.40.60.81.0fake1fake2validorig020406080100CIFAR10classificationaccuracy(%)0.00.20.40.60.81.0Normalizedfrequency020406080100CIFAR100classificationaccuracy(%)0.00.20.40.60.81.0Normalizedfrequency020406080100CIFAR10classificationaccuracy(%)0.00.20.40.60.81.0Normalizedfrequency020406080100CIFAR100classificationaccuracy(%)0.00.20.40.60.81.0Normalizedfrequency(a) Veriﬁcation scheme V1

(b) Veriﬁcation scheme V2

(c) Veriﬁcation scheme V3

Figure 5: Ambiguity Attack: Classiﬁcation accuracy on CIFAR100 under insider threat (f ake3) on
three veriﬁcation schemes. It is shown that when a correct signature is used  the classiﬁcation accuracy
is intact  while for a partial correct signature (sign scales are modiﬁed around 10%)  the performance
will immediately drop around 5%  and a totally wrong signature will obtain a meaningless accuracy
(1%-10%). Based on the threshold ≤ f = 3% for AlexNetp and by the ﬁdelity evaluation process F  
any potential ambiguity attacks (even with partially correct signature) are effectively defeated.

Scheme V1
- Passport layers added
- Passports needed
- 15%-30% more training time
- Passport layers & Passports needed
- 10% more inferencing time
- NO separate veriﬁcation needed

Training

Inferencing

Veriﬁcation

- Passport layers added
- Passports needed
- 100%-125% more training time
- Passport layers & Passport NOT needed
- NO extra time incurred
- Passport layers & Passports needed

- Passport layers added
- Passports & Trigger set needed
- 100%-150% more training time
- Passport layers & Passport NOT needed
- NO extra time incurred
- Trigger set needed (black-box veriﬁcation)
- Passport layers & Passports needed (white-box veriﬁcation)

Scheme V2

Scheme V3

Table 4: Summary of our proposed passport networks complexity for V1  V2 and V3 schemes.

attacks are effectively defeated. In summary  extensive empirical studies have shown that it is
impossible for adversaries to maintain the original DNN model accuracies by using counterfeit
passports  regardless of they are either randomly generated or reverse-engineered with the use of
original training datasets. This passport dependent performances play an indispensable role in
designing secure ownership veriﬁcation schemes that are illustrated in Sect. 3.3.

4.3 Network Complexity

Table 4 summarizes the complexity of passport networks in various schemes. We believe that it is the
computational cost at the inference stage that is required to be minimized  since network inference is
going to be performed frequently by the end users. While extra costs at the training and veriﬁcation
stages  on the other hand  are not prohibitive since they are performed by the network owners  with
the motivation to protect the DNN model ownerships. Nonetheless  we tested a larger network (i.e.
ResNetp-50) and its training time increases 10%  182% and 191% respectively for V1  V2 and V3
schemes. This increase is consistent with those smaller models i.e. AlexNetp and ResNetp-18.

5 Discussions and conclusions

Considering billions of dollars have been invested by giant and start-up companies to explore new
DNN models virtually every second  we believe it is imperative to protect these inventions from
being stolen. While ownership of DNN models might be resolved by registering the models with
a centralized authority  it has been recognized that these regulations are inadequate and technical
solutions are urgently needed to support the law enforcement and juridical protections. It is this
motivation that highlights the unique contribution of the proposed method in unambiguous veriﬁcation
of DNN models ownerships.
Methodology-wise  our empirical studies re-asserted that over-parameterized DNN models can
successfully learn multiple tasks with arbitrarily assigned labels and/or constraints. While this
assertion has been theoretically proved [15] and empirically investigated from the perspective of
network generalization [16]  its implications to network security in general remain to be explored.
We believe the proposed modulation of DNN performance based on the presented passports will play
an indispensable role in bringing DNN behaviours under control against adversarial attacks  as it has
been demonstrated for DNN ownership veriﬁcations.

9

0255075100Dissimilaritybetweenvalidandfakesignature(%)010203040506070ClassificationAccuracy(%)FakePassportValidPassport0255075100Dissimilaritybetweenvalidandfakesignature(%)010203040506070ClassificationAccuracy(%)FakePassportValidPassport0255075100Dissimilaritybetweenvalidandfakesignature(%)010203040506070ClassificationAccuracy(%)FakePassportValidPassportAcknowledgement

This research is partly supported by the Fundamental Research Grant Scheme (FRGS) MoHE Grant
FP021-2018A  from the Ministry of Education Malaysia. Also  we gratefully acknowledge the
support of NVIDIA Corporation with the donation of the Titan V GPU used for this research.

References
[1] Yusuke Uchida  Yuki Nagai  Shigeyuki Sakazawa  and Shin’ichi Satoh. Embedding watermarks

into deep neural networks. In ICMR  pages 269–277  2017.

[2] Y Adi  C Baum  M Cisse  B Pinkas  and J Keshet. Turning your weakness into a strength:

Watermarking deep neural networks by backdooring. In USENIX  pages 1615–1631  2018.

[3] Huili Chen  Bita Darvish Rohani  and Farinaz Koushanfar. DeepMarks: A Digital Fingerprinting

Framework for Deep Neural Networks. arXiv preprint arXiv:1804.03648  2018.

[4] Jialong Zhang  Zhongshu Gu  Jiyong Jang  Hui Wu  Marc Ph Stoecklin  Heqing Huang  and
Ian Molloy. Protecting intellectual property of deep neural networks with watermarking. In
ASIACCS  pages 159–172  2018.

[5] Bita Darvish Rouhani  Huili Chen  and Farinaz Koushanfar. DeepSigns: A Generic Watermark-
ing Framework for IP Protection of Deep Learning Models. arXiv preprint arXiv:1804.00750 
2018.

[6] Erwan Le Merrer  Patrick Perez  and Gilles Trédan. Adversarial Frontier Stitching for Remote

Neural Network Watermarking. arXiv preprint arXiv:1711.01894  2017.

[7] Guo Jia and Miodrag Potkonjak. Watermarking deep neural networks for embedded systems.

In ICCAD  pages 1–8  2018.

[8] Qiming Li and Ee-Chien Chang. Zero-knowledge watermark detection resistant to ambiguity
attacks. In Proceedings of the 8th workshop on Multimedia and security  pages 158–163  2006.

[9] Husrev T. Sencar and Nasir D. Memon. Combatting ambiguity attacks via selective detection of
embedded watermarks. IEEE Trans. Information Forensics and Security  2(4):664–682  2007.

[10] S. Craver  N. Memon  B. . Yeo  and M. M. Yeung. Resolving rightful ownerships with invisible
watermarking techniques: limitations  attacks  and implications. IEEE Journal on Selected
Areas in Communications  16(4):573–586  1998.

[11] Lixin Fan  Kam Woh Ng  and Chee Seng Chan. [extended version] rethinking deep neural
network ownership veriﬁcation: Embedding passports to defeat ambiguity attacks. arXiv
preprint arXiv:1909.07830  2019.

[12] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference

on Computer Vision (ECCV)  pages 3–19  2018.

[13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. In ICML  pages 448–456  2015.

[14] Abigail See  Minh-Thang Luong  and Christopher D Manning. Compression of neural ma-
chine translation models via pruning. In Proceedings of The 20th SIGNLL Conference on
Computational Natural Language Learning  pages 291–301  2016.

[15] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via

over-parameterization. In ICML  pages 242–252  2019.

[16] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

10

,Lixin Fan
Kam Woh Ng
Chee Seng Chan