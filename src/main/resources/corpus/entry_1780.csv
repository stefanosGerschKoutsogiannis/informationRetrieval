2018,Efficient online algorithms for fast-rate regret bounds under sparsity,We consider the problem of online convex optimization in two different settings: arbitrary and  i.i.d. sequence of convex loss functions. In both settings  we provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds. 
First  the excess risks bounds depend on the sparsity of the objective rather than on the dimension of the parameters space. Second  their rates are faster than the slow-rate $1/\sqrt{T}$ under additional convexity assumptions on the loss functions. In the adversarial setting  we develop an algorithm BOA+ whose cumulative excess risks is controlled by several bounds with different trade-offs between sparsity and rate for strongly convex loss functions. In the i.i.d. setting under the Łojasiewicz's assumption  we establish new risk bounds that are sparse with a rate adaptive to the convexity of the risk (ranging from a rate $1/\sqrt{T}$ for general convex risk to $1/T$ for strongly convex risk). These results generalize previous works on sparse online learning under weak assumptions on the risk.,Efﬁcient online algorithms for fast-rate

regret bounds under sparsity

INRIA  ENS  PSL Research University

Pierre Gaillard

Paris  France

pierre.gaillard@inria.fr

olivier.wintenberger@upmc.fr

Olivier Wintenberger

Sorbonne Université  CNRS  LPSM

Paris  France

Abstract

We consider the problem of online convex optimization in two different settings:
arbitrary and i.i.d. sequence of convex loss functions. In both settings  we provide
efﬁcient algorithms whose cumulative excess risks are controlled with fast-rate
sparse bounds. First  the excess risks bounds depend on the sparsity of the objective
rather than on the dimension of the parameters space. Second  their rates are
faster than the slow-rate 1/pT under additional convexity assumptions on the
loss functions. In the adversarial setting  we develop an algorithm BOA+ whose
cumulative excess risks is controlled by several bounds with different trade-offs
between sparsity and rate for strongly convex loss functions. In the i.i.d. setting
under the Łojasiewicz’s assumption  we establish new risk bounds that are sparse
with a rate adaptive to the convexity of the risk (ranging from a rate 1/pT for general
convex risk to 1/T for strongly convex risk). These results generalize previous
works on sparse online learning under weak assumptions on the risk.

1

Introduction

We consider the following setting of online convex optimization where a sequence of random convex
loss functions (`t : Rd ! R)t>1 is sequentially observed. At each iteration t > 1  a learner

Et1 = E[·|Ft1]. For any parameter ✓ in some reference set ⇥ ⇢ Rd  the average excess risk can
be decomposed as the sum of the approximation-estimation errors:

chooses a point b✓t1 2 Rd based on past observations Ft1 = {`1  . . .  ` t1}. The learner
t=1 Et1⇥`t(b✓t1)⇤ where
aims at minimizing the average excess risk deﬁned as bLT := (1/T )PT
TXt=1
Et1⇥`t(✓)⇤
}

Et1⇥`t(b✓t1)⇤ 
{z

estimation error

Though the ﬁnal goal is to minimizebLT   a common proxy is to upper-bound the estimation term
RT (✓) (also refereed to as average excess risk1) simultaneously for all ✓ 2 ⇥. If the loss functions
are exp-concave and ⇥ is bounded  several sequential algorithms achieve the uniform bound2 on
the estimation term RT := sup✓2⇥ RT (✓) 6 ˜O(d/T ); see [13]. In this paper  we are interested
with non-uniform bounds on RT (✓) increasing with the complexity of ✓. Such non-uniform bounds
are called oracle inequalities and state that the learner achieves the best approximation-estimation

Et1⇥`t(✓)⇤
}
{z

bLT =

.

(1)

+

1
T

|

TXt=1

TXt=1

1
T

|

approximation error

1
T

:= RT (✓)

1The average excess risk RT (✓) generalizes the average regret more commonly used in the online learning
2Throughout the paper . denotes an approximate inequality which holds up to universal constants and ˜O

literature by considering the Dirac masses on {`t} as conditional distributions so that `t = Et1[`t]  t  1.
denotes an asymptotic inequality up to logarithmic terms.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

trade-off of (1). Using the `0-norm to measure the complexity of ✓  we are looking for fast-rate sparse
bounds of the form

RT (✓) 6 ˜O ✓k✓k0
T ◆ 1

2!  

for any ✓ 2 ⇥.

The parameter  2 [0  1] depends on the convexity properties of the loss functions and will be speciﬁed
later. We call fast-rate bound any bound which provides a better rate than 1/pT and sparse bounds any
bound where some dependence on d has been replaced with k✓k0. Our analysis starts from a careful
study of the ﬁnite case ⇥= {✓1  . . .  ✓ K}. We consider then online averaging algorithms on adaptive
ﬁnite discretization grids that achieve sparse oracle bounds on ⇥= B1 = {✓ 2 Rd : k✓k1 6 1}.
First contribution: fast-rate high probability quantile bound (ﬁnite ⇥  adversarial data) The
case of ﬁnite reference set ⇥= {✓1  . . .  ✓ K} corresponds to the setting of prediction with expert ad-
vice (see Section 2.2 or [5]) where a learner makes sequential predictions over a series of rounds with
the help of K experts. Hedge introduced by [19] and [26] achieves the rate RT 6 O(p(ln K)/T ).
The latter is optimal for general convex loss functions but better performance can be obtained in
favorable scenarios. The rate RT 6 O((ln K)/T ) is for instance obtained for strongly convex loss
functions in [28]. Another improvement (see [16] and references therein) is devoted to quantile
3. The latter improve
bounds  i.e. bounds on Ek⇠⇡[RT (✓k)] for any probability distribution ⇡ 2 K
the dependence on the number of experts from ln K to the Kullback divergence K(⇡ b⇡0) for any
priorb⇡0. They are smaller whenever many experts perform well or when a good prior knowledge

is available. Squint [16] achieves a fast-rate quantile bound for adversarial data. Such a bound is
obtained in high-probability by [20] but it suffers an additional gap term.
In Section 2  we extend the analysis of [16] to remove the gap term of [20]. We introduce a weak
version of exp-concavity; see Assumption (A2). It depends on a parameter  2 [0  1] which goes
from  = 0 for general convex loss functions to  = 1 for exp-concavity. We show in Theo-
rem 2.1 that BOA [28] and Squint [16] achieve a fast rate quantile bound with high probability: i.e.

E⇡[RT (✓k)] 6 ˜O(K(⇡ b⇡0)/T )1/(2).
Second contribution: efﬁcient sparse oracle bound (⇥= B1  adversarial data) The extension
from ﬁnite reference sets to convex sets is natural. The seminal paper [15] introduced the Exponen-
tiated Gradient algorithm (EG)  a version of Hedge using the sub-gradients of the loss functions.
The latter guarantees RT 6 O(p(ln d)/T ) for ⇥= B1 which is optimal for convex loss functions.
Recently  fast rate RT 6 ˜O(d/T )1/(2) are obtained by [17] under a slightly different assumption
than (A2). Here our purpose is to improve the dependence on d under the sparsity condition k✓k0
small. The literature on learning under sparsity with i.i.d. data is vast; we refer to [12] for a review.
Yet  little work was done on sparsity bounds under adversarial data; see Table 1 for a summary.
The papers [7; 18; 29] focus on providing sparse estimatorsb✓t rather than sparse guarantees. More
recent works [8; 14] consider sparse approximations of the sub-gradients. Though they also compare
themselves with sparse parameters  they incur a bound larger than O(1/pT ) which is optimal in their
setting. Fast rate sparse regret bounds involving k✓k0 were  up to our knowledge  only obtained
through non-efﬁcient (exponential time) procedures (see [10]). In Section 3.3  we provide an efﬁcient
algorithm BOA+ which satisﬁes the oracle inequality

RT (✓) 6 ˜O(pdk✓k0/T ) ^ (pk✓k0/T 3/4)  

for any ✓ 2B 1  

for strongly-convex loss functions ( = 1). The gainpk✓k0/d ^pk✓k0/T compared with the
usual rate ˜O(d/T ) is signiﬁcant for sparse parameters ✓.
A crucial step of our analysis is an intermediate result which is interesting in its own. We deﬁne an
efﬁcient algorithm with input any ﬁnite grid ⇥0 ⇢B 1. We provide in Theorem 3.2 a bound of the
form RT (✓) 6 ˜O(D(✓  ⇥0)/pT ) for a pseudo-metric D and any ✓ 2B 1. We say that this bound is
accelerable as the rate may decrease if D(✓  ⇥0) decreases with T . In particular  it yields an oracle
bound of the form RT (✓) 6 O(k✓k1/pT ).
3Here and subsequently  K := {⇡ 2 [0  1]K; k⇡k1 = 1} denotes the simplex of dimension K  1.

2

Procedure
Kale et al. [8; 14]
[7; 18; 29]
SeqSEW [11]
SABOA

Rate

Poly(d)/pT
q ln d
T or d
d0 ln d
T
pd0d

T

Polynomial

Yes
Yes
No
Yes

Assumption
Convexity

(Strong) Convexity
Strong Convexity
Strong Convexity

Sparsity setting

Sparse observed gradients
Produce sparse estimators

Sparse bound
Sparse bound

q ln d

T ^

ln d

T

Table 1: Comparison of sequential optimization procedures in sparse adversarial environment.

Third contribution: sparse regret bound under Łojasiewicz assumption (⇥= B1  i.i.d. data)
In Section 3.4 we turn to a stochastic setting where the loss functions `1  . . .  ` T are i.i.d.. This
setting extends the regression one with random design to general loss functions. The classical Lasso
procedure satisﬁes  in the regression setting for the quadratic risk ( = 1)  RT (✓) 6 ˜O(k✓k0/T )
where ✓ is a sparse approximation of ✓⇤ = arg min✓2Rd RT (✓)  see [3]. Yet  few procedures
satisfying sparse bounds are sequential; we can cite [1; 8; 9; 14; 23]. We compare in Table 2 their
results and settings.
The ﬁrst line of work [1; 9; 23] provides sparse rates of order ˜O(k✓⇤k0 ln d/T ). Their settings
are close to the one of [3] but their methods differ; the one of [23] uses a `1-penalized gradient
descent whereas the one of [1] and [9] are based on restarting a subroutine centered around the current
estimate on sessions of exponentially growing length. A common limitation of these works is that they
do not provide oracle inequality. They only compete with the global optimum over Rd only  which is
assumed to be (approximately in [1]) sparse with a known `1-bound. In other words  they assume
that the global optimum also realizes the approximation-estimation errors trade-off in (1). In order to
avoid this restriction  our ﬁrst objective is to obtain the sparse bounds RT (✓⇤(U )) 6 ˜O(k✓⇤(U )k0/T )
where ✓⇤(U ) 2 arg mink✓k16U RT (✓) for any U > 0. For U well chosen so that k✓⇤(U )k1 = U 
✓⇤(U ) is sparse and the approximation-estimation errors trade-off in (1) is achieved. We restrict to the
case U = 1 suppressing the dependence on U in ✓⇤ for the ease of notation. We leave the adaptation
in U > 0 for future research.
The second line of works [14; 8] considers sparse approximation of sub-gradients. Yet  they provide a
0 ln d/T ) where ✓⇤ is the optimum in B1 when the loss functions
sparse regret bound of order O(k✓⇤k2
are strongly convex. Our second objective is to relax the strong convexity assumption which is too
restrictive in the sequential regression setting. Indeed  the usual restricted eigenvalues conditions
on the Gram matrix cannot hold uniformly for small t’s. We work under Łojasiewicz’s Assumption
introduced by [32; 33]: There exist > 0 and µ > 0 such that for all ✓ 2B 1  there exists a minimizer
✓⇤ of the risk over B1 satisfying

µ✓  ✓⇤2

2 6 E[`t(✓)  `t(✓⇤)] .

The Łojasiewicz assumption depends on a parameter  2 [0  1] that ranges from general convex risk
function ( = 0) to generalized strongly convex risk function ( = 1). In Theorem 3.4 we show that
our new efﬁcient procedure SABOA achieves a fast rate upper-bound on the average excess risk of
order ˜O((k✓⇤k0 ln(d)/T )1/(2)) when the optimal parameters have `1-norm bounded by 1 < 1.
Then we recover the optimal rate of [1; 9; 23] in a similar setting  when the global optimum is
assumed to be sparse. When k✓⇤k1 = 1  guaranteeing a good approximation-estimation trade-off
in (1)  the bound suffers an additional factor k✓⇤k0. Notice that Łojasiewicz’s Assumption (A3)
allows multiple optima which is important when we are dealing with degenerated co-linear design
(allowing zero eigenvalues in the covariance matrix). It is an open question whether the fast rate
˜O((k✓⇤k2
0 ln(d)/T )) is optimal for efﬁcient O(dT )-complex procedures such as SABOA under
Łojasiewicz’s Assumption.

Outline of the paper To summarize our contributions  we provide
- the ﬁrst high-probability quantile bound achieving a fast rate in Theorem 2.1;
- an accelerable bound on RT (✓) that is small whenever ✓ is close to a prior grid ⇥0 (Thm. 3.2);
- two efﬁcient algorithms with sparse regret bounds in the adversarial setting with strongly convex
loss functions (BOA+  Thm. 3.3) and in the i.i.d. setting (SABOA  Thm. 3.4). In the latter setting 
the results are obtained under the Łojasiewicz’s assumption. This generalizes the usual necessary
conditions for obtaining sparse bounds that are too restrictive in our sequential setting.

3

Procedure

Setting

Rate

Assumptions / Setting

Optimum over

Lasso [3]
Kale et al. [8; 14]
[1; 9; 23]+SABOA
SABOA

B d0 ln d/T
S
d2
0 ln d/T
S
d0 ln d/T
S
d2
0 ln d/T

Mutual Coherence

Strong Convexity + Sparse Gradients

Strong convexity or Łojasiewicz ( = 1)

Łojasiewicz ( = 1)

Rd
B1
Rd
B1

Table 2: Comparison of sequential (S) and batched (B) optimization procedures in i.i.d. environment.

2 Finite reference set
In this section  we focus on ﬁnite reference set ⇥:= {✓1  . . .  ✓ K}⇢B 1  including the setting of
prediction with expert advice presented in Section 2.2. We consider the following assumptions on the
loss functions:
(A1) Convex Lipschitz4: the loss functions `t are convex on B1 and there exists G > 0 such that
(A2) Weak exp-concavity: There exist ↵> 0 and  2 [0  1] such that for all t > 1  ✓1 ✓ 2 2B 1 

r`t(✓)1 6 G for all t  1  ✓ 2B 1.
Et1⇥`t(✓1)`t(✓2)⇤ 6 Et1⇥r`t(✓1)>(✓1✓2)⇤Et1h⇣↵r`t(✓1)>(✓1  ✓2)2⌘1/i.

For convex loss functions (`t)  Assumption (A2) is satisﬁed with  = 0 and ↵< G 2. Fast rates are
obtained for > 0. It is worth pointing out that Assumption (A2) is weak even in the strongest case
 = 1. It is implied by several common assumptions such as:
– Strong convexity of the risk: under the boundedness of the gradients  assumption (A2) with

almost surely

↵ = µ/(2G2) is implied by the µ-strong convexity of the risks (Et1[`t])  t  1.
– Exp-concavity of the loss: Lemma 4.2  Hazan [13] states that (A2) with ↵ 6 1
8G  } is
implied by -exp-concavity of the loss functions `t  t  1. Our assumption is slightly weaker
since it holds in conditional expectation.

4 min{ 1

2.1 Fast-rate quantile bound with high probability

For prediction with K > 1 expert advice  [28] showed that a fast rate O(ln K)/T can be obtained

by the BOA algorithm under the LIST condition (i.e.  Lipschitz and strongly convex loss functions).
In this section  we show that Assumption (A2) is enough and we improve the dependence on the total
number of experts with a quantile bound.
Our algorithm is described in Algorithm 1 and corresponds to a particular case of two algorithms: the
Squint algorithm of [16] used with a discrete prior over a ﬁnite set of learning rates and the BOA
algorithm of [28] where each expert is replicated multiple times with different constant learning rates.
The proof (with the exact constants) is deferred to Appendix C.1.
Theorem 2.1. Let T > 1. Assume (A1) and (A2). Apply Algorithm 1  parameter E = 4G/3 and

initial weight vectorb⇡0 2 K. Then  for all ⇡ 2 K  with probability at least 1  2ex  x > 0 
where K(⇡ b⇡0) :=PK

◆ 1
Ek⇠⇡ [RT (✓k)] .✓K(⇡ b⇡0) + ln ln(GT ) + x
k=1 ⇡k ln(⇡k/b⇡k 0) is the Kullback-Leibler divergence.

A fast rate of this type (without quantiles property) can be obtained in expectation by using Hedge
for exp-concave loss functions. However  Theorem 2.1 is stronger. First  Assumption (A2) is weaker
than the exp-concavity of the loss functions `t as it holds for absolute or quantile loss functions in a
sufﬁciently regular regression setting. Second  the algorithm uses the so-called gradient trick; See
[24]. Therefore  simultaneously with the fast rate O(T 1/(2)) with respect to the experts (✓k) 
4Throughout the paper  we assume that the Lipschitz constant G in (A1) is known. It can be calibrated online

↵T

with standard tricks such as the doubling trick (see [6] for instance) under sub-Gaussian conditions.

2

 

4

Algorithm 1 Squint – BOA with multiple constant learning rates assigned to each parameter

i=1

i0=1

Initialization: For 1 6 i 6 ln(ET 2)  deﬁne ⌘i := (eiE)1.
For each iteration t = 1  . . .   T do:

Parameters: ⇥0 = {✓1  . . .  ✓ K}⇢B 1  E > 0 andb⇡0 2 K.
k=1b⇡k t1✓k and observe r`t(b✓t1) 
⌘ie⌘iPt
k s)b⇡k 0
j s)⇤  
Ej⇠b⇡0⇥⌘i0e⌘i0Pt

– Chooseb✓t1 =PK
b⇡k t = Pln(ET 2)
Pln(ET 2)

– Update component-wise for all 1 6 k 6 K
s=1(rk s⌘ir2

s=1(rj s⌘i0 r2

rk s = r`t(b✓s1)>(b✓s1  ✓k) .
the algorithm achieves the slow rate O(1/pT ) with respect to any convex combination Ek⇠⇡[✓k]
If the algorithm is run with a uniform priorb⇡0 = (1/K  . . .   1/K)  Theorem 2.1 implies that for any

(similarly to EG). Finally  high-probability regret bounds as ours are not satisﬁed by Hedge (see [2]).

subset ⇥0 ✓ ⇥

with high probability.

max✓2⇥0 RT (✓) .⇣ ln(K/ Card(⇥0))+ln ln(GT )

Thanks to the quantile bounds  we pay the proportion of good experts ln(K/ Card(⇥0)) in the regret
instead of the total number of experts ln(K). We refer to [16] for more interesting applications. Such
quantile bounds on the risk were studied by Mehta [20  Section 7] in a batch i.i.d. setting (i.e.  `t are
i.i.d.). A standard online to batch conversion shows that Theorem 2.1 yields with high probability

⌘ 1

2

↵T

ETh`T +1(¯✓T )  Ek⇠⇡⇥`T +1(✓k)⇤i .⇣K(⇡ b⇡0)+ln ln(GT )+x

2  
This improves the bound obtained by [20] who suffers the additional gap

⌘ 1

↵T

¯✓T =

1

T PT

t=1b✓t1 .

(e  1) ET⇥Ek⇠⇡[`T +1(✓k)]  min⇡⇤2K `T +1(Ej⇠⇡⇤[✓j])⇤ .

2.2 Prediction with expert advice
The framework of prediction with expert advice is widely considered in the literature (see [5] for
an overview). We recall now this setting and how it can be included in our framework. At the
beginning of each round t  a ﬁnite set of K > 1 experts predict f t = (f1 t  . . .   fK t) 2 [0  1]K
from the history Ft1. The learner then chooses a weight vector b✓t1 in the simplex K and
produces a prediction bft :=b✓>t1f t 2 R as a convex combination of the experts.
Its perfor-
mance at time t is evaluated by a loss function gt
: R ! R. The goal of the learner is to
approach the performance of the best expert on a long run. This can be done by minimizing
t=1 Et1[gt(bft)]  Et1[gt(fk t)]   with respect to all experts
the average excess risk Rk T := 1
k 2{ 1  . . .   K}. This setting reduces to our framework with dimension d = K.
Indeed  it
sufﬁces to choose the K-dimensional loss function `t : ✓ 7! gt(✓>f t) and the canonical basis
+ : k✓k1 = 1 k✓k0 = 1} in RK as the reference set. Denoting by ✓k the k-th ele-
⇥:= {✓ 2 RK
ment of the canonical basis  we see that ✓>k f t = fk t  so that `t(✓k) = gt(fk t). Therefore  Rk T
matches our deﬁnition of RT (✓k) in Equation (1) and we get under the assumptions of Theorem 2.1
a bound of order:

T PT

Ek⇠⇡⇥Rk T⇤ .⇣K(⇡ b⇡0)+ln ln(GT )+x

↵T

⌘ 1

2 .

An important point to note here is that though the parameters ✓k of the reference set are constant 
this method can be used to compare the player with arbitrary strategies fk t that may evolve over
time and depend on recent data. We do not assume in this section that there is a single ﬁxed expert
k⇤ 2{ 1  . . .   K} which is always the best  i.e.  Et1[gt(fk⇤ t)] 6 mink Et1[gt(fk t)]. Hence  we
cannot replace (A2) with the closely related Bernstein assumption (see Ass. (A2’) or [17  Cond. 1]).
Actually one can reformulate Assumption (A2) on the one dimensional loss functions gt as follows:
there exist ↵> 0 and  2 [0  1] such that for all t > 1  for all 0 6 f1  f2 6 1 

Et1[gt(f1)  gt(f2)] 6 Et1⇥g0t(f1)(f1  f2)⇤  Et1⇣↵g0t(f1)(f1  f2)2⌘1/  

It holds with ↵ = /(2G2) for -strongly convex risk Et1[gt]. For instance  the square loss
gt = (· yt )2 satisﬁes it with  = 1 and ↵ = 1/8.

a.s.

5

3 Online optimization in the unit `1-ball

The aim of this section is to extend the preceding results to the reference set ⇥= B1 instead of ﬁnite
⇥= {✓1  . . .  ✓ K}. A classical reduction from the expert advice setting to the `1-ball is the so-called
“gradient-trick”. A direct analysis on BOA applied to ⇥0 = {✓ 2 Rd : k✓k0 = 1 k✓k1 = 1} the
2d corners of the `1-ball suffers a slow rate O(1/pT ) on the average excess risk with respect to any
✓ 2B 1. The goal is to exhibit algorithms that go beyond O(1/pT ). In Section 3.1 we investigate
non-adaptive discretization grids of the space that yield optimal upper-bounds but suffer exponential
time complexity. In Section 3.2 we introduce a pseudo-metric in order to bound the regret of grids
consisting of the 2d corners and some arbitrary ﬁxed points. From this crucial step  we derive the
adaptive points to add to the 2d corners in the adversarial case (Section 3.3) and in the i.i.d. case
(Section 3.4) in order to obtain two efﬁcient procedures (BOA+ and SABOA respectively) with sparse
guarantees.

3.1 Warmup: fast rate by discretizing the space
As a warmup  we show how to use Theorem 2.1 in order to obtain fast rate on RT (✓) for any ✓ 2B 1.
Basically  if the parameter ✓ could be included into the grid ⇥0  Theorem 2.1 would turn into a bound
on the regret RT (✓) with respect to ✓. However  this is not possible as we do not know ✓ in advance.
A solution consists in approaching B1 with B1(")  a ﬁxed ﬁnite "-covering in `1-norm of minimal
cardinal so that Card(B1(")) .1/"d. We obtain a nearly optimal regret for this procedure.
Proposition 3.1. Let T > 1. Under Assumptions of Theorem 2.1  applying Algorithm 1 with grid
⇥0 = B1(T 2) and uniform priorb⇡0 over Card(B1(T 2)) satisﬁes for all ✓ 2B 1

2 +

(2)

RT (✓) .⇣ d ln T + ln ln(GT ) + x

↵T

⌘ 1

G
T 2  

with probability at least 1  ex  x > 0.
Proof. Let " = 1/T 2 and ✓ 2B 1 and ˜✓ be its "-approximation in B1("). The proof follows from
Lipschitzness of the loss: RT (✓) 6 RT (˜✓) + G" and by applying Theorem 2.1 on RT (˜✓).
One can improve d to k✓k0 ln d by carefully choosing the priorb⇡0 as in [21]; see Appendix A for

details. The obtained rate is optimal up to log-factors. However  the complexity of the discretization
is prohibitive (of order T d) and non realistic for practical purpose.

3.2 Oracle bound for arbitrary ﬁxed discretization grid
Let ⇥0 ⇢B 1 be a ﬁnite set. The aim of this Section is to study the regret of Algorithm 1 with respect
to any ✓ 2B 1. Similarly to Proposition 3.1  the average excess risk may be bounded as

RT (✓) .⇣ ln Card(⇥0)+ln ln T +x

↵T

⌘ 1
2 + Gk✓0  ✓k1  

(3)

for any ✓0 2 ⇥0. We say that a regret bound is accelerable if it provides a fast rate except a term
depending on the distance with the grid (i.e.  the term in k✓0  ✓k1 in (3)) that decreases with T .
This property will be crucial in obtaining fast rates by adapting the grid ⇥0 sequentially. The regret
bound (3) is not accelerable due to the second term that is constant. In order to ﬁnd an accelerable
regret bound  we introduce the notion of averaging accelerability  a pseudo-metric that replaces the
`1-norm in (3). We give the intuition behind this notion in the sketch of the proof of Theorem 3.2.
Deﬁnition 3.1 (Averaging accelerability). For any ✓  ✓0 2B 1  we deﬁne

D(✓  ✓0) := min0 6 ⇡ 6 1 : k✓  (1  ⇡)✓0k1 6 ⇡ .

This averaging accelerability has several nice properties. In Appendix B  we provide a few concrete
upper-bounds in terms of classical distances. For instance  Lemma B.1 provides the upper-bound
D(✓  ✓0) 6 k✓  ✓0k1/(1  k✓0k1 ^ k✓k1). We are now ready to state our regret bound  when Algo-
rithm 1 is applied with an arbitrary approximation grid ⇥0.

6

T

 ✓

+

aG
T

↵T⌘ 1

RT (✓) .⇣ a

2 + GD(✓  ⇥0)r a

Theorem 3.2. Let ⇥0 ⇢B 1 such that {✓ : k✓k1 = 1 k✓k0 = 1}✓ ⇥0. Let Assumption (A1) and
(A2) be satisﬁed. Then  Algorithm 1 applied with uniform priorb⇡0 over the elements of ⇥0 and
E = 8G/3  satisﬁes with probability 1  ex  x > 0 

2B 1  
where a = ln Card(⇥0) + ln ln(GT ) + x and D(✓  ⇥0) := min✓02⇥0 D(✓  ✓0).
Sketch of proof. The complete proof can be found in Appendix C.2. We give here the high-level
ideas. Let ✓0 2 ⇥0 be a point in the grid ⇥0 minimizing D(✓  ✓0). Then one can decompose
✓ = (1  ")✓0 + "✓00 for a unique point k✓00k1 = 1 and " := D(✓  ✓0). See Appendix C.2 for details.
The regret bound can be decomposed into two terms:
– The ﬁrst term quantiﬁes the cost of picking the correct ✓0 2 ⇥0  bounded using Theorem 2.1;
– The second one is the cost of learning ✓00 2B 1 rescaled by ". Using a classical slow-rate
bound in B1  it is of order O(1/pT ).
2 + "Gr ln Card(⇥0)
The average excess risk RT (✓) is thus of order
⌘ 1
(1  ") RT (✓0)
| {z }

.⇣ ln Card(⇥0) + ln ln(GT ) + x

+ "R T (✓00)
Gpln(Card ⇥0))/T

| {z }

Note that the bound of Theorem 3.2 is accelerable as its second term vanishes to zero on the contrary
to Inequality (3). Theorem 3.2 provides an upper-bound which may improve the rate O(1/pT ) if
the distance D(✓  ⇥0) is small enough. By using the properties of the averaging accelerability (see
Lemma B.1 in Appendix B)  Theorem 3.2 provides some interesting properties of the rate in terms of
`1 distance. By including 0 into the grid ⇥0  we get an oracle-bound of order O(k✓k1/pT ) for any
✓ 2B 1. Moreover a bound of order RT (✓) 6 Ok✓  ✓kk1/(pT ) is obtained for all ✓k 2 ⇥0
and k✓k1 6 1  < 1.
It is worth pointing out that the bound on the gradient G can be substituted with the average gradient
observed by the learner. The constant G can be improved to the level of the noise in certain situations
with vanishing gradients (see for instance Theorem 3 of [9]).

Thm 2.1

↵T

T

.

3.3 Fast-rate sparsity regret bound in the adversarial setting
In this section  we focus on the adversarial case where `t = Et1[`t] are µ-strongly convex deter-
ministic functions. In this case  Assumption (A2) is satisﬁed with  = 1 and ↵ = µ/(2G2). Our

⇥(i) = {[✓⇤i ]k  k = 0  . . .   d}[{ ✓ : k✓k1 = 2 k✓k0 = 1}  

algorithm  called BOA+  is deﬁned as follows. For each doubling session i > 0  BOA+ choosesb✓t
from time step ti = 2i to ti+1 1 by restarting Algorithm 1 with uniform prior  parameter E = 4G/3
and updated discretization grid ⇥0 indexed by i:
where ✓⇤i 2 arg min✓2B1Pti1
t=1 `t(✓) is the empirical risk minimizer (or the leader) until time
ti  1. The notation [· ]k denotes the hard-truncation with k non-zero values. Remark that ✓⇤i for
i = 1  2  . . .   ln2(T ) can be efﬁciently computed approximately as the solution of a strongly convex
optimization problem.
Theorem 3.3. Assume the loss functions are µ-strongly convex on B2 := {✓ 2 Rd : k✓k1 6 2} with
gradients bounded by G in `1-norm on B2. The average regret of BOA+ satisﬁes the oracle bound

The proof is deferred to Appendix C.6. We emphasize that the bound can be rewritten as follows:

µ Gr ln d
T ! 3
) min(Gr ln d

RT (✓) 6 ˜O0@min8<:
Gr ln d
RT (✓) 6 ˜O min(Gr ln d
It provides an intermediate rate between known optimal rates without sparsity O(pln d/T ) and
˜O(d/T ) and known optimal rates with sparsity O(pln d/T ) and (for non-efﬁcient procedures only)
˜O(k✓k0/T ). If all ✓⇤i are approximately d0-sparse it is possible to achieve the optimal rate of order
˜O(d0/T )  for any k✓k0 6 d0. We leave for future work whether it is possible to achieve it in general.

9=;
µT )!1/2

µT 1A  ✓ 2B 1 .

 pk✓k0dG2 ln d

 sk✓k0

 ✓ 2B 1\{0} .

  k✓k0G2 ln d

dG2 ln d

G2 ln d

µT

µT

+

T

T

2

T

 

7

µk✓  ✓⇤t k2

Remark 3.1. The strongly convex assumption on the loss functions can be relaxed (see Inequality (33)
in the proof of Theorem 3.3) by assuming (A2) on B2 and that there exists µ > 0 and  2 [0  1] such
that for all t > 1 and ✓ 2B 1
2 6 1
s=1(`s(✓)  `s(✓⇤t ))  where
tPt
(4)
The rates will depend on  as it is the case in Theorem 2.1. A speciﬁc interesting case is when
k✓⇤t k1 = 1. Then ✓⇤t is very likely to be sparse. Denote S⇤t its support. Assumption (4) can be
restricted in this case. Indeed any ✓ 2B 1 satisﬁes k✓k1 6 k✓⇤t k1  which from Lemma 6 of [1] yields
k✓  ✓⇤t k1 6 2k[✓  ✓⇤t ]S⇤t k1 where [✓]S = (✓i11i2S)16i6d. One can restrict Assumption (4) to
hold on S⇤t only. Such restricted conditions for  = 1 are common in the sparse learning literature
and essentially necessary for the existence of efﬁcient and optimal sparse procedures  see [31]. For
obtaining regret bounds on BOA+  the restricted condition (4) with  = 1 should hold at any time
t  1  which is unlikely in the regression setting.
3.4 Fast-rate sparse excess risk bound in the i.i.d. setting

✓⇤t 2 arg min✓2B1Pt

s=1 `s(✓) .

In this section  we assume the loss functions `t to be i.i.d. We provide an algorithm with fast-rate
sparsity risk-bound on B1 by regularly restarting Algorithm 1 with an updated discretization grid ⇥0
approaching the set of minimizers ⇥⇤ := arg min✓2B1 E[`t(✓)].
In the i.i.d. setting  a close inspection of the proof of Theorem 3.4 shows that we can replace
Assumption (A2) with the Bernstein condition: there exists ↵0 > 0 and  2 [0  1]  such that for all
✓ 2B 1  all ✓⇤ 2 ⇥⇤ and all t > 1 

↵0Ehr`t(✓)>(✓  ✓⇤)2i 6 Ehr`t(✓)>(✓  ✓⇤)i

.

(A2’)

This fast-rate type stochastic condition is equivalent to the central condition (see [25  Condition 5.2])
and was already considered to obtain faster rates of convergence for the regret (see [17  Condition 1]).

The Łojasiewicz assumption In order to obtain sparse oracle inequalities we work under Ło-
jasiewicz’s Assumption (A3) which is a relaxed version of strong convexity of the risk.

(A3) Łojasiewicz’s inequality: (`t)t>1 is an i.i.d. sequence and there exist  2 [0  1] and
0 < µ 6 1 such that  for all ✓ 2 Rd with k✓k1 6 1   there exists ✓⇤ 2 ⇥⇤ ✓B 1 satisfying

µ✓  ✓⇤2

2 6 E[`t(✓)  `t(✓⇤)] .

This assumption is fairly mild. It is indeed satisﬁed with  = 0 and µ = 1 as soon as the loss function
is convex. For  = 1  this assumption is implied by the strong convexity of the risk E[`t]. Our
framework is more general because

- multiple optima are allowed  which seems to be new when combined with sparsity bounds. An
exception is [21] that provides the optimal sparse rate under a low-rank Gram matrix setting for
the non-efﬁcient ES algorithm;
- on the contrary to [23] or [9]  our framework does not compete with the minimizer ✓⇤ over Rd
with a known upper-bound on the `1-norm k✓⇤k1. We consider the minimizer over the `1-ball B1
only. The latter is more likely to be sparse and Assumption (A3) only needs to hold over B1.
Assumption (A2) (or (A2’)) and (A3) are strongly related. Assumption (A3) is more restrictive
because it is design dependent in the regression setting; The constant µ corresponds to the smallest
non-zero eigenvalue of the covariance matrix while ↵ = 1/G2 for the square loss functions. If
⇥⇤ = {✓⇤} is a singleton than Assumption (A3) implies Assumption (A2’) with ↵0 > µ/G2.
Algorithm and excess risk bound Our new procedure called SABOA is described in Algorithm 2.
Again it starts from the accelerable bound provided in Theorem 3.2 which is small if one of the points
in ⇥0 is close to ⇥⇤. As BOA+  SABOA restarts BOA by adding current estimators of ⇥⇤ into an
updated grid ⇥0. The new points added to the grid are slightly different between the two algorithms.

They are truncated versions of the average of past iteratesb✓t1 for SABOA and of the leader for

BOA+. Remark that restart schemes under Łojasiewicz’s Assumption is natural and was already used
by [22]. We get the following upper-bound on the average excess risk. The proof that computes the
exact constants is postponed to Appendix C.7.

8

• Deﬁne ¯✓(i1) := 0 if i = 0 and ¯✓(i1) := 2i+1Pti1
• Deﬁne ⇥(i) a set of hard-truncated and dilated soft-thresholded versions of ¯✓(i1) as in (45) 
• Denote Ki := Card(⇥(i)) + 2d 6 (i + 1)(1 + ln d) + 3d  
• At time step ti  restart Algorithm 1 in Ki with parameters ⇥0 := ⇥(i) [{ ✓ : k✓k1 =
1 k✓k0 = 1} (denote by ✓1  . . .  ✓ Ki its elements)  E > 0 and uniform priorb⇡0.
– Chooseb✓t1 =PKi

– Deﬁne component-wise for all 1 6 k 6 Ki  denoting ⌘j := (ejE)1 

In other words  for time steps t = ti  . . .   ti+1  1:

t=ti1b✓t1 otherwise 

k=1b⇡k t1✓k and observe r`t(b✓t1) 
b⇡k t = Pln(ET 2)
Pln(ET 2)
where rk s = r`t(b✓s1)>(b✓s1  ✓k).

⌘je⌘jPt
Ek0⇠b⇡0⇥⌘je⌘jPt

j=1

j=1

s=ti

s=ti

(rk s⌘j r2

k s)b⇡k 0

(rk0 s⌘j r2

k0 s

)⇤  

Algorithm 2 SABOA – Sparse Acceleration of BOA
Parameters: E > 0
Initialization: ti = 2i for i > 0 
For each session i = 0  . . . do:

 

T

↵

+

2

G2

✓ 1

2⌘◆◆ 1

RT (✓⇤) .✓ ln d + ln ln(GT ) + x

Theorem 3.4. Under Assumptions (A1)  (A2) and (A3)  Algorithm 2 with E = 4/3G > 1 satisﬁes
with probability at least 1  ex  x > 0  the average excess risk bound
µ ⇣d2
d0
0 ^
where d0 = max✓⇤2⇥⇤ k✓⇤k0 and 0 6  6 1 satisﬁes ⇥⇤ ✓B 1.
We conclude with some important remarks about Theorem 3.4. First  we point out that SABOA
adapts automatically to unknown parameters     ↵  µ and d0 to fulﬁll the rate of Theorem 3.4.
On the radius of L1 ball. We provide the analysis into B1  the `1-ball of radius U = 1 only.
However  one might need to compare with points into B1(U )  the `1-ball of radius U > 0  in order to
obtain a good approximation-estimation trade-off. This can be done by rescaling the loss functions
✓ 2B 1 7! `t(U✓ ) and applying our results with U G  U 2µ and ↵ under Assumptions (A1)  (A2) and
(A3) on B1(U ). The main rate of convergence of Theorem 3.4 is unchanged. The optimal choice of
the radius  if it is not imposed by the application  is left for future research.
Support recovery. When all ✓⇤ 2 ⇥⇤ lie on the border of the `1-ball  they are likely to be sparse.
One can relax Assumption (A3) to hold in sup-norm and in a restricted version similar as done in
the end of Remark 3.1. In this interesting setting  we could not avoid a factor d2
0. The reason is that
our sequential algorithm recovers the (largest) support of ✓⇤ (see Conﬁguration 3 of Figure 1) in a
framework where the necessary (for the rate k✓⇤k0) Irreprensatibility Condition [27] does not hold.
Conclusion In this paper  we show that BOA is an optimal online algorithm for aggregating experts
under very weak conditions on the loss. Then we aggregate sparse versions of the leader (BOA+)
or of the average of BOA’s iterates (SABOA) in the adversarial or in the i.i.d. setting  respectively.
Aggregating both achieves sparse fast-rates of convergence in any case. These rates are deteriorated

compared with the ideal one ˜O(k✓k0/T )1/(2) that requires restrictive assumption for efﬁcient
algorithm. Our main condition (A3) is weaker and more realistic than the usual ones when seeking
for sequential sparse rate bounds for any t  1.
References
[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Stochastic optimization and sparse statis-
tical recovery: Optimal algorithms for high dimensions. In Advances in Neural Information
Processing Systems 25  pages 1538–1546. Curran Associates  Inc.  2012.

[2] J.-Y. Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural

Information Processing Systems  pages 41–48  2008.

9

[3] F. Bunea  A. Tsybakov  and M. Wegkamp. Sparsity oracle inequalities for the lasso. Electronic

Journal of Statistics  1:169–194  2007.

[4] O. Catoni. Universal aggregation rules with exact bias bounds. preprint  510  1999.

[5] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[6] N. Cesa-Bianchi  Y. Mansour  and G. Stoltz. Improved second-order bounds for prediction with

expert advice. Machine Learning  66(2-3):321–352  2007.

[7] J. C. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent.

In COLT  pages 14–26  2010.

[8] D. J. Foster  S. Kale  and H. Karloff. Online sparse linear regression. In Conference on Learning

Theory  pages 960–970  2016.

[9] P. Gaillard and O. Wintenberger. Sparse Accelerated Exponential Weights. In 20th International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  Apr. 2017.

[10] S. Gerchinovitz. Prediction of individual sequences and prediction in the statistical framework:
some links around sparse regression and aggregation techniques. PhD thesis  Université
Paris-Sud 11  Orsay  2011.

[11] S. Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression.

The Journal of Machine Learning Research  14(1):729–769  2013.

[12] C. Giraud. Introduction to high-dimensional statistics. Chapman and Hall/CRC  2014.
[13] E. Hazan. Introduction to online convex optimization. Foundations and Trends R in Optimiza-

tion  2(3-4):157–325  2016.

[14] S. Kale  Z. Karnin  T. Liang  and D. Pál. Adaptive feature selection: Computationally efﬁcient

online sparse linear regression under rip. arXiv preprint arXiv:1706.04690  2017.

[15] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear

predictors. Information and Computation  132(1):1–63  1997.

[16] W. M. Koolen and T. Van Erven. Second-order quantile methods for experts and combinatorial

games. In COLT  volume 40  pages 1155–1175  2015.

[17] W. M. Koolen  P. Grünwald  and T. van Erven. Combining adversarial guarantees and stochastic
fast rates in online learning. In Advances in Neural Information Processing Systems  pages
4457–4465  2016.

[18] J. Langford  L. Li  and T. Zhang. Sparse online learning via truncated gradient. Journal of

Machine Learning Research  10(Mar):777–801  2009.

[19] N. Littlestone and M. K. Warmuth. The weighted majority algorithm.

computation  108(2):212–261  1994.

Information and

[20] N. A. Mehta. Fast rates with high probability in exp-concave statistical learning. In Artiﬁcial

Intelligence and Statistics  pages 1085–1093  2017.

[21] P. Rigollet and A. Tsybakov. Exponential screening and optimal rates of sparse estimation. The

Annals of Statistics  pages 731–771  2011.

[22] V. Roulet and A. d’Aspremont. Sharpness  restart and acceleration. In Advances in Neural

Information Processing Systems  pages 1119–1129  2017.

[23] J. Steinhardt  S. Wager  and P. Liang. The statistics of streaming sparse regression. arXiv

preprint arXiv:1412.4182  2014.

[24] I. Steinwart and A. Christmann. Estimating conditional quantiles with the help of the pinball

loss. Bernoulli  17(1):211–225  2011.

10

[25] T. Van Erven  P. D. Grünwald  N. A. Mehta  M. D. Reid  and R. C. Williamson. Fast rates in
statistical and online learning. Journal of Machine Learning Research  16:1793–1861  2015.

[26] V. G. Vovk. Aggregating strategies. Proc. of Computational Learning Theory  1990  1990.
[27] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
`1-constrained quadratic programming (lasso). IEEE transactions on information theory  55(5):
2183–2202  2009.

[28] O. Wintenberger. Optimal learning with bernstein online aggregation. Machine Learning  106

(1):119–141  2017.

[29] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.

Journal of Machine Learning Research  11(Oct):2543–2596  2010.

[30] Y. Yang. Combining forecasting procedures: some theoretical results. Econometric Theory  20

(01):176–222  2004.

[31] Y. Zhang  M. J. Wainwright  and M. I. Jordan. Lower bounds on the performance of polynomial-
time algorithms for sparse linear regression. In Conference on Learning Theory  pages 921–948 
2014.

[32] S. Łojasiewicz. Une propriété topologique des sous-ensembles analytiques réels. Les équations

aux dérivées partielles  pages 87–89  1963.

[33] S. Łojasiewicz. Sur la géométrie semi-et sous-analytique. Annales de l’institut Fourier  43(5):

1575–1595  1993.

11

,Pierre Gaillard