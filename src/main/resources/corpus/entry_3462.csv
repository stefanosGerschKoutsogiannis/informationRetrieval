2016,Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria in Continuous Zero-Sum Games,We study a general adversarial online learning problem  in which we are given a decision set X' in a reflexive Banach space X and a sequence of reward vectors in the dual space of X. At each iteration  we choose an action from X'  based on the observed sequence of previous rewards. Our goal is to minimize regret  defined as the gap between the realized reward and the reward of the best fixed action in hindsight. Using results from infinite dimensional convex analysis  we generalize the method of Dual Averaging (or Follow the Regularized Leader) to our setting and obtain upper bounds on the worst-case regret that generalize many previous results. Under the assumption of uniformly continuous rewards  we obtain explicit regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space S. Importantly  we make no convexity assumptions on either the set S or the reward functions. We also prove a general lower bound on the worst-case regret for any online algorithm. We then apply these results to the problem of learning in repeated two-player zero-sum games on compact metric spaces. In doing so  we first prove that if both players play a Hannan-consistent strategy  then with probability 1 the empirical distributions of play weakly converge to the set of Nash equilibria of the game. We then show that  under mild assumptions  Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves Hannan-consistency.,Minimizing Regret on Reﬂexive Banach Spaces and
Nash Equilibria in Continuous Zero-Sum Games

Maximilian Balandat  Walid Krichene  Claire Tomlin  Alexandre Bayen

Electrical Engineering and Computer Sciences  UC Berkeley

[balandat walid tomlin]@eecs.berkeley.edu  bayen@berkeley.edu

Abstract

We study a general adversarial online learning problem  in which we are given a
decision set X in a reﬂexive Banach space X and a sequence of reward vectors
in the dual space of X. At each iteration  we choose an action from X   based on
the observed sequence of previous rewards. Our goal is to minimize regret. Using
results from inﬁnite dimensional convex analysis  we generalize the method of
Dual Averaging to our setting and obtain upper bounds on the worst-case regret that
generalize many previous results. Under the assumption of uniformly continuous
rewards  we obtain explicit regret bounds in a setting where the decision set is the
set of probability distributions on a compact metric space S. Importantly  we make
no convexity assumptions on either S or the reward functions. We also prove a
general lower bound on the worst-case regret for any online algorithm. We then
apply these results to the problem of learning in repeated two-player zero-sum
games on compact metric spaces. In doing so  we ﬁrst prove that if both players play
a Hannan-consistent strategy  then with probability 1 the empirical distributions
of play weakly converge to the set of Nash equilibria of the game. We then show
that  under mild assumptions  Dual Averaging on the (inﬁnite-dimensional) space
of probability distributions indeed achieves Hannan-consistency.

1

Introduction

Regret analysis is a general technique for designing and analyzing algorithms for sequential decision
problems in adversarial or stochastic settings (Shalev-Shwartz  2012; Bubeck and Cesa-Bianchi 
2012). Online learning algorithms have applications in machine learning (Xiao  2010)  portfolio
optimization (Cover  1991)  online convex optimization (Hazan et al.  2007) and other areas. Regret
analysis also plays an important role in the study of repeated play of ﬁnite games (Hart and Mas-
Colell  2001). It is well known  for example  that in a two-player zero-sum ﬁnite game  if both
players play according to a Hannan-consistent strategy (Hannan  1957)  their (marginal) empirical
distributions of play almost surely converge to the set of Nash equilibria of the game (Cesa-Bianchi
and Lugosi  2006). Moreover  it can be shown that playing a strategy that achieves sublinear regret
almost surely guarantees Hannan-consistency.
A natural question then is whether a similar result holds for games with inﬁnite action sets. In this
article we provide a positive answer. In particular  we prove that in a continuous two-player zero sum
game over compact (not necessarily convex) metric spaces  if both players follow a Hannan-consistent
strategy  then with probability 1 their empirical distributions of play weakly converge to the set of
Nash equilibria of the game. This in turn raises another important question: Do algorithms that
ensure Hannan-consistency exist in such a setting? More generally  can one develop algorithms that
guarantee sub-linear growth of the worst-case regret? We answer these questions afﬁrmatively as well.
To this end  we develop a general framework to study the Dual Averaging (or Follow the Regularized
Leader) method on reﬂexive Banach spaces. This framework generalizes a wide range of existing

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

results in the literature  including algorithms for online learning on ﬁnite sets (Arora et al.  2012) and
ﬁnite-dimensional online convex optimization (Hazan et al.  2007).
Given a convex subset X of a reﬂexive Banach space X  the generalized Dual Averaging (DA)
method maximizes  at each iteration  the cumulative past rewards (which are elements of X∗  the dual
space of X) minus a regularization term h. We show that under certain conditions  the maximizer in
the DA update is the Fréchet gradient Dh∗ of the regularizer’s conjugate function. In doing so  we
develop a novel characterization of the duality between essential strong convexity of h and essential
Fréchet differentiability of h∗ in reﬂexive Banach spaces  which is of independent interest.
We apply these general results to the problem of minimizing regret when the rewards are uniformly
continuous functions over a compact metric space S. Importantly  we do not assume convexity of
either S or the rewards  and show that it is possible to achieve sublinear regret under a mild geometric
condition on S (namely  the existence of a locally Q-regular Borel measure). We provide explicit
bounds for a class of regularizers  which guarantee sublinear worst-case regret. We also prove a
general lower bound on the regret for any online algorithm and show that DA asymptotically achieves
this bound up to a √log t factor.
Our results are related to work by Lehrer (2003) and Sridharan and Tewari (2010); Srebro et al.
(2011). Lehrer (2003) gives necessary geometric conditions for Blackwell approachability in inﬁnite-
dimensional spaces  but no implementable algorithm guaranteeing Hannan-consistency. Sridharan
and Tewari (2010) derive general regret bounds for Mirror Descent (MD) under the assumption that
the strategy set is uniformly bounded in the norm of the Banach space. We do not make such an
assumption here. In fact  this assumption does not hold in general for our applications in Section 3.
The paper is organized as follows: In Section 2 we introduce and provide a general analysis of Dual
Averaging in reﬂexive Banach spaces. In Section 3 we apply these results to obtain explicit regret
bounds on compact metric spaces with uniformly continuous reward functions. We use these results
in Section 4 in the context of learning Nash equilibria in continuous two-player zero sum games  and
provide a numerical example in Section 4. All proofs are given in the supplementary material.

2 Regret Minimization on Reﬂexive Banach Spaces

Consider a sequential decision problem in which we are to choose a sequence (x1  x2  . . . ) of actions
from some feasible subset X of a reﬂexive Banach space X  and seek to maximize a sequence
(u1(x1)  u2(x2)  . . . ) of rewards  where the uτ : X → R are elements of a given subset U ⊂ X∗ 
with X∗ the dual space of X. We assume that xt  the action chosen at time t  may only depend
on the sequence of previously observed reward vectors (u1  . . .   ut−1). We call any such algorithm
an online algorithm. We consider the adversarial setting  i.e.  we do not make any distributional
assumptions on the rewards. In particular  they could be picked maliciously by some adversary.
The notion of regret is a standard measure of performance for such a sequential decision problem. For
reward and the reward under x  i.e.  Rt(x) :=(cid:80)t
a sequence (u1  . . .   ut) of reward vectors  and a sequence of decisions (x1  . . .   xt) produced by an
algorithm  the regret of the algorithm w.r.t. a (ﬁxed) decision x ∈ X is the gap between the realized
τ =1 uτ (xτ ). The regret is deﬁned
as Rt := supx∈X Rt(x). An algorithm is said to have sublinear regret if for any sequence (ut)t≥1
in the set of admissible reward functions U  the regret grows sublinearly  i.e. lim supt Rt/t ≤ 0.
Example 1. Consider a ﬁnite action set S = {1  . . .   n}  let X = X∗ = Rn  and let X = ∆n−1 
the probability simplex in Rn. A reward function can be identiﬁed with a vector u ∈ Rn  such that
the i-th element ui is the reward of action i. A choice x ∈ X corresponds to a randomization over
the n actions in S. This is the classic setting of many regret-minimizing algorithms in the literature.
Example 2. Suppose S is a compact metric space with µ a ﬁnite measure on S. Consider X =
X∗ = L2(S  µ) and let X = {x ∈ X : x ≥ 0 a.e. (cid:107)x(cid:107)1 = 1}. A reward function is an L2-
integrable function on S  and each choice x ∈ X corresponds to a probability distribution (absolutely
continuous w.r.t. µ) over S. We will explore a more general variant of this problem in Section 3.

τ =1 uτ (x) −

(cid:80)t

optimization problem xt+1 = arg maxx∈X (cid:10)ηt

In this Section  we prove a general bound on the worst-case regret for DA. DA was introduced
by Nesterov (2009) for (ﬁnite dimensional) convex optimization  and has also been applied to online
learning  e.g. by Xiao (2010). In the ﬁnite dimensional case  the method solves  at each iteration  the
− h(x)  where h is a strongly convex

τ =1 uτ   x(cid:11)
(cid:80)t

2

regularizer deﬁned on X ⊂ Rn and (ηt)t≥0 is a sequence of learning rates. The regret analysis of
the method relies on the duality between strong convexity and smoothness (Nesterov  2009  Lemma
1). In order to generalize DA to our Banach space setting  we develop an analogous duality result in
Theorem 1. In particular  we show that the correct notion of strong convexity is (uniform) essential
strong convexity. Equipped with this duality result  we analyze the regret of the Dual Averaging
method and derive a general bound in Theorem 2.

2.1 Preliminaries
Let (X (cid:107) · (cid:107)) be a reﬂexive Banach space  and denote by (cid:104)·   ·(cid:105) : X × X∗
→ R the canonical
pairing between X and its dual space X∗  so that (cid:104)x  ξ(cid:105) := ξ(x) for all x ∈ X  ξ ∈ X∗. By
the effective domain of an extended real-valued function f : X → [−∞  +∞] we mean the set
dom f = {x ∈ X : f (x) < +∞}. A function f is proper if f > −∞ and dom f is non-empty.
The conjugate or Legendre-Fenchel transform of f is the function f∗ : X∗
→ [−∞  +∞] given by
(1)
for all ξ ∈ X∗. If f is proper  lower semicontinuous and convex  its subdifferential ∂f is the
dom ∂f := {x ∈ X : ∂f (x) (cid:54)= ∅}. Let Γ denote the set of all convex  lower semicontinuous
functions γ : [0 ∞) → [0 ∞] such that γ(0) = 0  and let
(2)

set-valued mapping ∂f (x) = (cid:8)ξ ∈ X∗ : f (y) ≥ f (x) + (cid:104)y − x  ξ(cid:105) for all y ∈ X(cid:9). We deﬁne

ΓL :=(cid:8)γ ∈ Γ : γ(r)/r → 0  as r → 0(cid:9)

ΓU :=(cid:8)γ ∈ Γ : ∀r > 0  γ(r) > 0(cid:9)

x∈X (cid:104)x  ξ(cid:105) − f (x)

(ξ) = sup

∗

f

We now introduce some deﬁnitions. Additional results are reviewed in the supplementary material.
Deﬁnition 1 (Strömberg  2011). A proper convex lower semicontinuous function f : X → (−∞ ∞]
is essentially strongly convex if

(i) f is strictly convex on every convex subset of dom ∂f
(ii) (∂f )−1 is locally bounded on its domain
(iii) for every x0 ∈ dom ∂f there exists ξ0 ∈ X∗ and γ ∈ ΓU such that

f (x) ≥ f (x0) + (cid:104)x − x0  ξ0(cid:105) + γ((cid:107)x − x0(cid:107)) 

∀x ∈ X.

(3)

If (3) holds with γ independent of x0  f is uniformly essentially strongly convex with modulus γ.
Deﬁnition 2 (Strömberg  2011). A proper convex lower semicontinuous function f : X → (−∞ ∞]
is essentially Fréchet differentiable if int dom f (cid:54)= ∅  f is Fréchet differentiable on int dom f with
Fréchet derivative Df  and (cid:107)Df (xj)(cid:107)∗ → ∞ for any sequence (xj)j in int dom f converging to
some boundary point of dom f.
Deﬁnition 3. A proper Fréchet differentiable function f : X → (−∞ ∞] is essentially strongly
smooth if ∀ x0 ∈ dom ∂f  ∃ ξ0 ∈ X∗  κ ∈ ΓL such that
(4)
If (4) holds with κ independent of x0  f is uniformly essentially strongly smooth with modulus κ.
With this we are now ready to give our main duality result:
Theorem 1. Let f : X → (−∞  +∞] be proper  lower semicontinuous and uniformly essentially
strongly convex with modulus γ ∈ ΓU . Then

f (x) ≤ f (x0) + (cid:104)ξ0  x − x0(cid:105) + κ((cid:107)x − x0(cid:107)) 

(i) f∗ is proper and essentially Fréchet differentiable with Fréchet derivative

∀ x ∈ X.

∗

Df

(ξ) = arg max

x∈X (cid:104)x  ξ(cid:105) − f (x).

If  in addition  ˜γ(r) := γ(r)/r is strictly increasing  then

(6)
In other words  Df∗ is uniformly continuous with modulus of continuity χ(r) = ˜γ−1(r/2).

(ξ1) − Df

(ξ2)(cid:107) ≤ ˜γ

(cid:107)Df

∗

∗

−1(cid:0)

(cid:107)ξ1 − ξ2(cid:107)∗/2(cid:1).

(5)

(ii) f∗ is uniformly essentially smooth with modulus γ∗.

Corollary 1. If γ(r) ≥ C r1+κ  ∀ r ≥ 0 then (cid:107)Df∗(ξ1) − Df∗(ξ2)(cid:107) ≤ (2C)−1/κ(cid:107)ξ1 − ξ2(cid:107)1/κ∗
.
2 r2  Deﬁnition 1 becomes the classic deﬁnition of K-strong convexity 
In particular  with γ(r) = K
and (6) yields the result familiar from the ﬁnite-dimensional case that the gradient Df∗ is 1/K
Lipschitz with respect to the dual norm (Nesterov  2009  Lemma 1).

3

2.2 Dual Averaging in Reﬂexive Banach Spaces
We call a proper convex function h : X → (−∞  +∞] a regularizer function on a set X ⊂ X if
h is essentially strongly convex and dom h = X . We emphasize that we do not assume h to be
Fréchet-differentiable. Deﬁnition 1 in conjunction with Lemma S.1 (supplemental material) implies
that for any regularizer h  the supremum of any function of the form (cid:104)·   ξ(cid:105) − h(· ) over X  where
ξ ∈ X∗  will be attained at a unique element of X   namely Dh∗(ξ)  the Fréchet gradient of h∗ at ξ.
using the simple update rule xt+1 = Dh∗(ηtUt)  where Ut =(cid:80)t
DA with regularizer h and a sequence of learning rates (ηt)t≥1 generates a sequence of decisions
Theorem 2. Let h be a uniformly essentially strongly convex regularizer on X with modulus γ and
let (ηt)t≥1 be a positive non-increasing sequence of learning rates. Then  for any sequence of payoff
functions (ut)t≥1 in X∗ for which there exists M < ∞ such that supx∈X |(cid:104)ut  x(cid:105)| ≤ M for all t  the
sequence of plays (xt)t≥0 given by
(7)

τ =1 uτ and U0 := 0.

(cid:80)t

∗(cid:0)ηt

τ =1 uτ

ensures that

Rt(x) :=

t(cid:88)
τ =1(cid:104)uτ   x(cid:105) −

xt+1 = Dh

t(cid:88)
τ =1(cid:104)uτ   xτ(cid:105) ≤

h(x) − h

ηt

+

(cid:1)
t(cid:88)
τ =1(cid:107)uτ(cid:107)∗ ˜γ

−1(cid:16) ητ−1

(cid:17)

2 (cid:107)uτ(cid:107)∗

(8)

where h = inf x∈X h(x)  ˜γ(r) := γ(r)/r and η0 := η1.
It is possible to obtain a regret bound similar to (8) also in a continuous-time setting. In fact 
following Kwon and Mertikopoulos (2014)  we derive the bound (8) by ﬁrst proving a bound on
a suitably deﬁned notion of continuous-time regret  and then bounding the difference between the
continuous-time and discrete-time regrets. This analysis is detailed in the supplementary material.
Note that the condition that supx∈X |(cid:104)ut  x(cid:105)| ≤ M in Theorem 2 is weaker than the one in Sridharan
and Tewari (2010)  as it does not imply a uniformly bounded strategy set (e.g.  if X = L2(R) and X
is the set of distributions on X  then X is unbounded in L2  but the condition may still hold).
Theorem 2 provides a regret bound for a particular choice x ∈ X . Recall that Rt := supx∈X Rt(x).
In Example 1 the set X is compact  so any continuous regularizer h will be bounded  and hence
taking the supremum over x in (8) poses no issue. However  this is not the case in our general
setting  as the regularizer may be unbounded on X . For instance  consider Example 2 with the
S x(s) log(x(s))ds  which is easily seen to be unbounded on X . As a
consequence  obtaining a worst-case bound will in general require additional assumptions on the
reward functions and the decision set X . This will be investigated in detail in Section 3.
Corollary 2. Suppose that γ(r) ≥ C r1+κ  ∀ r ≥ 0 for some C > 0 and κ > 0. Then

entropy regularizer h(x) =(cid:82)

t(cid:88)
τ−1(cid:107)uτ(cid:107)1+1/κ
η1/κ
(cid:17)1/κ
(cid:16) η
In particular  if (cid:107)ut(cid:107)∗ ≤ M for all t and ηt = η t−β  then

h(x) − h

Rt(x) ≤

+ (2C)

−1/κ

τ =1

ηt

∗

.

(9)

(10)

Rt(x) ≤

h(x) − h

tβ +

η

κ
κ − β

2C

M 1+1/κ t1−β/κ.

Assuming h is bounded  optimizing over β yields a rate of Rt(x) = O(t
2 r2  which corresponds to the classic deﬁnition of strong convexity  then Rt(x) = O(√t).
γ(r) = K
For non-vanishing uτ we will need that ηt (cid:38) 0 for the sum in (9) to converge. Thus we could get
potentially tighter control over the rate of this term for κ < 1  at the expense of larger constants.

1+κ ). In particular  if

κ

3 Online Optimization on Compact Metric Spaces

We now apply the above results to the problem minimizing regret on compact metric spaces under
the additional assumption of uniformly continuous reward functions. We make no assumptions on
convexity of either the feasible set or the rewards. Essentially  we lift the non-convex problem of
minimizing a sequence of functions over the (possibly non-convex) set S to the convex (albeit inﬁnite-
dimensional) problem of minimizing a sequence of linear functionals over a set X of probability
measures (a convex subset of the vector space of measures on S).

4

3.1 An Upper Bound on the Worst-Case Regret

p + 1

Let (S  d) be a compact metric space  and let µ be a Borel measure on S. Suppose that the reward
vectors uτ are given by elements in Lq(S  µ)  where q > 1. Let X = Lp(S  µ)  where p and q are
Hölder conjugates  i.e.  1
q = 1. Consider X = {x ∈ X : x ≥ 0 a.e. (cid:107)x(cid:107)1 = 1}  the set of
probability measures on S that are absolutely continuous w.r.t. µ with p-integrable Radon-Nikodym
derivatives. Moreover  denote by Z the class of non-decreasing χ : [0 ∞) → [0 ∞] such that
limr→0 χ(r) = χ(0) = 0. The following assumption will be made throughout this section:
Assumption 1. The reward vectors ut have modulus of continuity χ on S  uniformly in t. That is 
there exists χ ∈ Z such that |ut(s) − ut(s(cid:48))| ≤ χ(d(s  s(cid:48))) for all t and for all s  s(cid:48)
Let B(s  r) = {s(cid:48)
∈ S : d(s  s(cid:48)) < r} and denote by B(s  δ) ⊂ X the elements of X with support
contained in B(s  δ). Furthermore  let DS := sups s(cid:48)∈S d(s  s(cid:48)). Then we have the following:
Theorem 3. Let (S  d) be compact  and suppose that Assumption 1 holds. Let h be a uniformly
essentially strongly convex regularizer on X with modulus γ  and let (ηt)t≥1 be a positive non-
increasing sequence of learning rates. Then  under (7)  for any positive sequence (ϑt)t≥1 

∈ S.

−1(cid:16) ητ−1

t(cid:88)
τ =1(cid:107)uτ(cid:107)∗ ˜γ

(cid:17)

2 (cid:107)uτ(cid:107)∗

.

(11)

Rt ≤

sups∈S inf x∈B(s ϑt) h(x) − h

ηt

+ t χ(ϑt) +

Remark 1. The sequence (ϑt)t≥1 in Theorem 3 is not a parameter of the algorithm  but rather a
parameter in the regret bound. In particular  (11) holds true for any such sequence  and we will use
this fact later on to obtain explicit bounds by instantiating (11) with a particular choice of (ϑt)t≥1.
It is important to realize that the inﬁmum over B(s  ϑt) in (11) may be inﬁnite  in which case the
bound is meaningless. This happens for example if s is an isolated point of some S ⊂ Rn and µ is the
Lebesgue measure  in which case B(s  ϑt) = ∅. However  under an additional regularity assumption
on the measure µ we can avoid such degenerate situations.
Deﬁnition 4 (Heinonen. et al.  2015). A Borel measure µ on a metric space (S  d) is (Ahlfors)
Q-regular if there exist 0 < c0 ≤ C0 < ∞ such that for any open ball B(s  r)
We say that µ is r0-locally Q-regular if (12) holds for all 0 < r ≤ r0.
Intuitively  under an r0-locally Q-regular measure  the mass in the neighborhood of any point of S is
uniformly bounded from above and below. This will allow  at each iteration t  to assign sufﬁcient
probability mass around the maximizer(s) of the cumulative reward function.
Example 3. The canonical example for a Q-regular measure is the Lebesgue measure λ on Rn. If d
is the metric induced by the Euclidean norm  then Q = n and the bound (12) is tight with c0 = C0 
a dimensional constant. However  for general sets S ⊂ Rn  λ need not be locally Q-regular. A
sufﬁcient condition for local regularity of λ is that S is v-uniformly fat (Krichene et al.  2015).
Assumption 2. The measure µ is r0-locally Q-regular on (S  d).
Under Assumption 2  B(s  ϑt) (cid:54)= ∅ for all s ∈ S and ϑt > 0  hence we may hope for a bound on
inf x∈B(s ϑt) h(x) uniform in s. To obtain explicit convergence rates  we have to consider a more
speciﬁc class of regularizers.

c0rQ ≤ µ(B(s  r)) ≤ C0rQ.

(12)

3.2 Explicit Rates for f-Divergences on Lp(S)

We consider a particular class of regularizers called f-divergences or Csiszár divergences (Csiszár 
1967). Following Audibert et al. (2014)  we deﬁne ω-potentials and the associated f-divergence.
Deﬁnition 5. Let ω ≤ 0 and a ∈ (−∞  +∞]. A continuous increasing diffeomorphism
φ : (−∞  a) → (ω ∞)  is an ω-potential if limz→−∞ φ(z) = ω  limz→a φ(z) = +∞ and
1 φ−1(z) dz

φ(0) ≤ 1. Associated to φ is the convex function fφ : [0 ∞) → R deﬁned by fφ(x) =(cid:82) x
and the fφ-divergence  deﬁned by hφ(x) =(cid:82)

(cid:0)x(s)(cid:1) dµ(s) + ιX (x)  where ιX is the indicator

function of X (i.e. ιX (x) = 0 if x ∈ X and ιX (x) = +∞ if x /∈ X ).
A remarkable fact is that for regularizers based on ω potentials  the DA update (7) can be computed
efﬁciently. More precisely  it can be shown (see Proposition 3 in Krichene (2015)) that the maximizer
in this case has a simple expression in terms of the dual problem  and the problem of computing
xt+1 = Dh∗(ηt

(cid:80)t
τ =1 uτ ) reduces to computing a scalar dual variable ν∗
t .

S fφ

5

Proposition 1. Suppose that µ(S) = 1  and that Assumption 2 holds with constants r0 > 0 and
0 < c0 ≤ C0 < ∞. Under the Assumptions of Theorem 3  with h = hφ the regularizer associated to
an ω-potential φ  we have that  for any positive sequence (ϑt)t≥1 with ϑt ≤ r0 

(cid:1) + χ(ϑt) +

t(cid:88)
τ =1(cid:107)uτ(cid:107)∗ ˜γ

1
t

−1(cid:16) ητ−1

2 (cid:107)uτ(cid:107)∗

(cid:17)

.

Rt
t ≤

min(C0ϑQ
t ηt

t   µ(S))

−1
0 ϑ

−Q
t

fφ

(cid:0)c

(13)

For particular choices of the sequences (ηt)t≥1 and (ϑt)t≥1  we can derive explicit regret rates.

3.3 Analysis for Entropy Dual Averaging (The Generalized Hedge Algorithm)

Taking φ(z) = ez−1  we have that fφ(x) =(cid:82) x
hφ(x) =(cid:82)

S x(s) log x(s)dµ(s). Then Dh∗(ξ)(s) = exp ξ(s)
(cid:107) exp ξ(s)(cid:107)1

1 φ−1(z)dz = x log x  and hence the regularizer is
. This corresponds to a generalized
Hedge algorithm (Arora et al.  2012; Krichene et al.  2015) or the entropic barrier of Bubeck and
Eldan (2014) for Euclidean spaces. The regularizer hφ can be shown to be essentially strongly convex
with modulus γ(r) = 1
Corollary 3. Suppose that µ(S) = 1  that µ is r0-locally Q-regular with constants c0  C0  that
(cid:107)ut(cid:107)∗ ≤ M for all t  and that χ(r) = Cαrα for 0 < α ≤ 1 (that is  the rewards are
α-Hölder continuous). Then  under Entropy Dual Averaging  choosing ηt = η
log t/t with
η = 1
M

(cid:1)1/2 and ϑ > 0  we have that
(cid:114) 2C0
(cid:16)
(cid:17)

(cid:19)(cid:114)

(cid:112)

(cid:18)

2 r2.

log(c

−1
0 ϑ−Q/α) +

Q
2α

+ Cαϑ

log t

t

(14)

2α

log(c

−1
0 ϑ−Q/α) + Q
Rt
t ≤
log t/t < rα

2M
0 ϑ−1.

c0

(cid:0) C0Q
whenever(cid:112)

2c0

One can now further optimize over the choice of ϑ to obtain the best constant in the bound. Note also
that the case α = 1 corresponds to Lipschitz continuity.

3.4 A General Lower Bound
Theorem 4. Let (S  d) be compact  suppose that Assumption 2 holds  and let w : R → R be any
function with modulus of continuity χ ∈ Z such that (cid:107)w(d(·   s(cid:48)))(cid:107)q ≤ M for some s(cid:48)
∈ S for which
there exists s ∈ S with d(s  s(cid:48)) = DS. Then for any online algorithm  there exist a sequence (uτ )t
of reward vectors uτ ∈ X∗ with (cid:107)uτ(cid:107)∗ ≤ M and modulus of continuity χτ < χ such that

τ =1

Rt ≥

w(DS)
2√2

√t 

(15)

(16)

Maximizing the constant in (15) is of interest in order to benchmark the bound against the upper
bounds obtained in the previous sections. This problem is however quite challenging  and we will
defer this analysis to future work. For Hölder-continuous functions  we have the following result:
Proposition 2. In the setting of Theorem 4  suppose that µ(S) = 1 and that χ(r) = Cαrα for some
0 < α ≤ 1. Then

min(cid:0)C 1/α

S   M(cid:1)

Rt ≥

α Dα
2√2

√t.

Observe that  up to a √log t factor  the asymptotic rate of this general lower bound for any online
algorithm matches that of the upper bound (14) of Entropy Dual Averaging.

4 Learning in Continuous Two-Player Zero-Sum Games

Consider a two-player zero sum game G = (S1  S2  u)  in which the strategy spaces S1 and S2 of
player 1 and 2  respectively  are Hausdorff spaces  and u : S1 × S2 → R is the payoff function of
player 1 (as G is zero-sum  the payoff function of player 2 is −u). For each i  denote by Pi := P(Si)
(cid:82)
the set of Borel probability measures on Si. Denote S := S1 × S2 and P := P1 × P2. For a
(joint) mixed strategy x ∈ P  we deﬁne the natural extension ¯u : P → R by ¯u(x) := Ex[u] =
S u(s1  s2) dx(s1  s2)  which is the expected payoff of player 1 under x.

6

A continuous zero-sum game G is said to have value V if

sup
x1∈P1

inf
x2∈P2

¯u(x1  x2) = inf

x2∈P2

sup
x1∈P1

¯u(x1  x2) = V.

(17)

The elements x1 × x2 ∈ P at which (17) holds are the (mixed) Nash Equilibria of G. We denote the
set of Nash equilibria of G by N (G). In the case of ﬁnite games  it is well known that every two-player
zero-sum game has a value. This is not true in general for continuous games  and additional conditions
on strategy sets and payoffs are required  see e.g. (Glicksberg  1950).

−i
τ )

≤ 0

(18)

1
t

τ =1

τ =1

(cid:19)

(cid:18)

sup
si∈Si

ui(si

τ   s

ui(si  s

lim sup
t→∞

t(cid:88)

−i
τ ) −

t )t≥1 and (s2

i)t≥1 generated by σi has sublinear regret almost surely.

t )t≥1  we say that player i has sublinear (realized) regret if

4.1 Repeated Play
We consider repeated play of the continuous two-player zero-sum game. Given a game G and a
sequence of plays (s1

t(cid:88)
where we use −i to denote the other player.
A strategy σi for player i is  loosely speaking  a (possibly random) mapping from past observations
to its actions. Of primary interest to us are Hannan-consistent strategies:
Deﬁnition 6 (Hannan  1957). A strategy σi of player i is Hannan consistent if  for any sequence
(st−i)t≥1  the sequence of plays (st
Note that the almost sure statement in Deﬁnition 6 is with respect to the randomness in the strategy σi.
The following result is a generalization of its counterpart for discrete games (e.g. Corollary 7.1 in
(cid:80)t
(Cesa-Bianchi and Lugosi  2006)):
Proposition 3. Suppose G has value V and consider a sequence of plays (s1
assume that both players have sublinear realized regret. Then limt→∞ 1
t
As in the discrete case (Cesa-Bianchi and Lugosi  2006)  we can also say something about convergence
of the empirical distributions of play to the set of Nash Equilibria. Since these distributions have
(cid:80)t
ﬁnite support for every t  we can at best hope for convergence in the weak sense as follows:
Theorem 5. Suppose that in a repeated two-player zero sum game G that has a value both players
the marginal empirical
follow a Hannan-consistent strategy  and denote by ˆxi
distribution of play of player i at iteration t. Let ˆxt := (ˆx1
t ). Then ˆxt (cid:42) N (G) almost surely 
that is  with probability 1 the sequence (ˆxt)t≥1 weakly converges to the set of Nash equilibria of G.
Corollary 4. If G has a unique Nash equilibrium x∗  then with probability 1  ˆxt (cid:42) x∗.
4.2 Hannan-Consistent Strategies

t = 1
t
t   ˆx2

t )t≥1  (s2

t )t≥1 and

τ ) = V .

τ =1 u(s1

τ   s2

τ =1 δsi

τ

By Theorem 5  if each player follows a Hannan-consistent strategy  then the empirical distributions
of play weakly converge to the set of Nash equilibria of the game. But do such strategies exist?
Regret minimizing strategies are intuitive candidates  and the intimate connection between regret
minimization and learning in games is well studied in many cases  e.g. for ﬁnite games (Cesa-
Bianchi and Lugosi  2006) or potential games (Monderer and Shapley  1996). Using our results from
Section 3  we will show that  under the appropriate assumption on the information revealed to the
player  no-regret learning based on Dual Averaging leads to Hannan consistency in our setting.
Speciﬁcally  suppose that after each iteration t  each player i observes a partial payoff function
t : Si → R describing their payoff as a function of only their own action  si  holding the action
˜ui
played by the other player ﬁxed. That is  ˜u1
Remark 2. Note that we do not assume that the players have knowledge of the joint utility function u.
However  we do assume that the player has full information feedback  in the sense that they observe
partial reward functions u(·   s−i
τ ) on their entire action set  as opposed to only observing the reward

τ ) of the action played (the latter corresponds to the bandit setting).

t (s2) := −u(s1

t (s1) := u(s1  s2

t ) and ˜u2

t   s2).

τ   s2

u(s1

τ )t

t = (˜ui

We denote by ˜U i
t to denote the set of all possible such histories  and deﬁne U i
U i
t)∞
t=1 of (possibly random) mappings σi
collection (σi
plays si
t−1). We make the following assumption on the payoff function:
t = σi
t(U i

τ =1 the sequence of partial payoff functions observed by player i. We use
0 := ∅. A strategy σi of player i is a
t−1 → Si  such that at iteration t  player i

t : U i

7

(cid:1).

(cid:80)t−1

(cid:0)ηt−1

t is a random variable with the following distribution:

Assumption 3. The payoff function u is uniformly continuous in si with modulus of continuity
independent of s−i for i = 1  2. That is  for each i there exists χi ∈ Z such that |u(s  s−i) −
u(s(cid:48)  s−i)| ≤ χi(di(s  s(cid:48))) for all s−i ∈ S−i.
It is easy to see that Assumption 3 implies that the game has a value (see supplementary material).
It also makes our setting compatible with that of Section 3. Suppose now that each player random-
izes their play according to the sequence of probability distributions on Si generated by DA with
regularizer hi. That is  suppose that each σi
∗
σi
t ∼ Dh
i

(19)
Theorem 6. Suppose that player i uses strategy σi according to (19)  and that the DA algorithm
ensures sublinear regret (i.e. lim supt Rt/t ≤ 0). Then σi is Hannan-consistent.
Corollary 5. If both players use strategies according to (19) with the respective Dual Averaging en-
suring that lim supt Rt/t ≤ 0  then with probability 1 the sequence (ˆxt)t≥1 of empirical distributions
of play weakly converges to the set of Nash equilibria of G.
(cid:0)x1  x2(cid:1) =(cid:0) exp(s)
Example Consider a zero-sum game G1 between two players on the unit interval with payoff func-
tion u(s1  s2) = s1s2 − a1s1 − a2s2  where a1 = e−2
e−1. It is easy to verify that the pair
t (s1) =(cid:0)Σt
and (s2
U 1

(cid:1) is a mixed-strategy Nash equilibrium of G1. For sequences (s1
(cid:1)s2 − a1Σt
τ − a1t(cid:1)s1 − a2Σt

τ )t
τ =1  the cumulative payoff functions for ﬁxed action s ∈ [0  1] are given  respectively  by
τ )t

t (s2) =(cid:0)a2t − Σt

e−1   exp(1−s)
e−1

e−1 and a2 = 1

τ =1 ˜ui
τ

τ =1s2
τ

U 2

τ =1s1
τ

τ =1s1
τ

τ =1s2

τ =1

If each player i uses the Generalized Hedge Algorithm with learning rates (ητ )t
period t is to sample from the distribution xi
t = ηt(a2t− Σt
α2
statistic  in the sense that it completely determines the mixed strategy at time t.

τ =1  their strategy in
τ − a1t) and
τ ). Interestingly  in this case the sum of the opponent’s past plays is a sufﬁcient

t(s) ∝ exp(αi

ts)  where α1

t = ηt(Σt

τ =1s1

τ =1s2

Figure 1: Normalized histograms of the empirical distributions of play in G (100 bins)

Figure 1 shows normalized histograms of the empirical distributions of play at different iterations t.
As t grows the histograms approach the equilibrium densities x1 and x2  respectively. However  this
t oscillating
does not mean that the individual strategies xi
around the equilibrium parameters 1 and −1  respectively  even for very large t. We do  however 
observe that the time-averaged parameters ¯αi

t converge. Indeed  Figure 2 shows the αi
t converge to the equilibrium values 1 and −1.

Figure 2: Evolution of parameters αi

t and ¯αi

t := 1
t

τ =1 αi

τ in G1

In the supplementary material we provide additional numerical examples  including one that illustrates
how our algorithms can be utilized as a tool to compute approximate Nash equilibria in continuous
zero-sum games on non-convex domains.

(cid:80)t

8

0.00.51.01.52.02.5player1 t=5000x1(s)player1 t=50000x1(s)player1 t=500000x1(s)0.00.20.40.60.81.00.00.51.01.52.02.5player2 t=5000x2(s)0.00.20.40.60.81.0player2 t=50000x2(s)0.00.20.40.60.81.0player2 t=500000x2(s)100101102103104105106−1012α1t¯α1tα2t¯α2tReferences
Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a meta-

algorithm and applications. Theory of Computing  8(1):121–164  2012.

Jean-Yves Audibert  Sébastien Bubeck  and Gàbor Lugosi. Regret in online combinatorial optimiza-

tion. Mathematics of Operations Research  39(1):31–45  2014.

S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal universal self-concordant barrier.

ArXiv e-prints  December 2014.

Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-

armed bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge UP  2006.
Thomas M. Cover. Universal portfolios. Mathematical Finance  1(1):1–29  1991.
Imre Csiszár.

Information-type measures of difference of probability distributions and indirect

observations. Studia Scientiarum Mathematicarum Hungarica  2:299–318  1967.

Irving L. Glicksberg. Minimax theorem for upper and lower semicontinuous payoffs. Research

Memorandum RM-478  The RAND Corporation  Oct 1950.

James Hannan. Approximation to Bayes risk in repeated play. In Contributions to the Theory of

Games  vol III of Annals of Mathematics Studies 39. Princeton University Press  1957.

Sergiu Hart and Andreu Mas-Colell. A general class of adaptive strategies. Journal of Economic

Theory  98(1):26 – 54  2001.

Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning  69(2-3):169–192  2007.

Juha Heinonen.  Pekka Koskela  Nageswari Shanmugalingam  and Jeremy T. Tyson. Sobolev
Spaces on Metric Measure Spaces: An Approach Based on Upper Gradients. New Mathematical
Monographs. Cambridge University Press  2015.

Walid Krichene. Dual averaging on compactly-supported distributions and application to no-regret

learning on a continuum. CoRR  abs/1504.07720  2015.

Walid Krichene  Maximilian Balandat  Claire Tomlin  and Alexandre Bayen. The Hedge Algorithm
on a Continuum. In 32nd International Conference on Machine Learning  pages 824–832  2015.
Joon Kwon and Panayotis Mertikopoulos. A continuous-time approach to online optimization. ArXiv

e-prints  January 2014.

Ehud Lehrer. Approachability in inﬁnite dimensional spaces. International Journal of Game Theory 

31(2):253–268  2003.

Dov Monderer and Lloyd S. Shapley. Potential games. Games and Economic Behavior  14(1):124 –

143  1996.

Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming 

120(1):221–259  2009.

Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in

Machine Learning  4(2):107–194  2012.

Nati Srebro  Karthik Sridharan  and Ambuj Tewari. On the universality of online mirror descent. In

Advances in Neural Information Processing Systems 24 (NIPS)  pages 2645–2653. 2011.

Karthik Sridharan and Ambuj Tewari. Convex games in banach spaces. In COLT 2010 - The 23rd

Conference on Learning Theory   pages 1–13  Haifa  Israel  June 2010.

Thomas Strömberg. Duality between Fréchet differentiability and strong convexity. Positivity  15(3):

527–536  2011.

Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J.

Mach. Learn. Res.  11:2543–2596  December 2010.

9

,Maximilian Balandat
Walid Krichene
Claire Tomlin
Alexandre Bayen