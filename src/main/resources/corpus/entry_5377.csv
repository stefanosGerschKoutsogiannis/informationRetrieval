2010,Switching state space model for simultaneously estimating state transitions and nonstationary firing rates,We propose an algorithm for simultaneously estimating state transitions among neural states  the number of neural states  and nonstationary firing rates using a switching state space model (SSSM).  This model enables us to detect state transitions based not only on the discontinuous changes of mean firing rates but also on discontinuous changes in temporal profiles of firing rates  e.g.  temporal correlation.  We derive a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events.  Synthetic data analysis reveals the high performance of our algorithm in estimating state transitions  the number of neural states  and nonstationary firing rates compared to previous methods.  We also analyze neural data recorded from the medial temporal area.  The statistically detected neural states probably coincide with transient and sustained states  which have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition based on discontinuous change in the temporal correlation of firing rates  which transitions previous methods cannot detect.  This result suggests the advantage of our algorithm in real-data analysis.,000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053

Switching state space model for simultaneously

estimating state transitions and nonstationary ﬁring

rates

Anonymous Author(s)

Afﬁliation
Address
email

Abstract

We propose an algorithm for simultaneously estimating state transitions among
neural states  the number of neural states  and nonstationary ﬁring rates using a
switching state space model (SSSM). This algorithm enables us to detect state
transitions on the basis of not only the discontinuous changes of mean ﬁring rates
but also discontinuous changes in temporal proﬁles of ﬁring rates  e.g.  temporal
correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM
whose non-Gaussian property is caused by binary spike events. Synthetic data
analysis reveals that our algorithm has the high performance for estimating state
transitions  the number of neural states  and nonstationary ﬁring rates compared
to previous methods. We also analyze neural data that were recorded from the
medial temporal area. The statistically detected neural states probably coincide
with transient and sustained states that have been detected heuristically. Estimated
parameters suggest that our algorithm detects the state transition on the basis of
discontinuous changes in the temporal correlation of ﬁring rates  which transi-
tions previous methods cannot detect. This result suggests that our algorithm is
advantageous in real-data analysis.

1 Introduction

Elucidating neural encoding is one of the most important issues in neuroscience. Recent studies have
suggested that cortical neuron activities transit among neural states in response to applied sensory
stimuli[1-3]. Abeles et al. detected state transitions among neural states using a hidden Markov
model whose output distribution is multivariate Poisson distribution (multivariate-Poisson hidden
Markov model(mPHMM))[1]. Kemere et al.
indicated the correspondence relationship between
the time of the state transitions and the time when input properties change[2]. They also suggested
that the number of neural states corresponds to the number of input properties. Assessing neural
states and their transitions thus play a signiﬁcant role in elucidating neural encoding. Firing rates
have state-dependent properties because mean and temporal correlations are signiﬁcantly different
among all neural states[1]. We call the times of state transitions as change points. Change points
are those times when the time-series data statistics change signiﬁcantly and cause nonstationarity
in time-series data. In this study  stationarity means that time-series data have temporally uniform
statistical properties. By this deﬁnition  data that do not have stationarity have nonstationarity.

Previous studies have detected change points on the basis of discontinuous changes in mean ﬁr-
ing rates using an mPHMM. In this model  ﬁring rates in each neural state take a constant value.
However  actually in motor cortex  average ﬁring rates and preferred direction change dynamically
in motor planning and execution[4]. This makes it necessary to estimate state-dependent  instanta-
neous ﬁring rates. On the other hand  when place cells burst within their place ﬁeld[5]  the inter-burst

1

054
055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107

intervals correspond to the θ rhythm frequency. Medial temporal (MT) area neurons show oscilla-
tory ﬁring rates when the target speed is modulated in the manner of a sinusoidal function[6]. These
results indicate that change points also need to be detected when the temporal proﬁles of ﬁring rates
change discontinuously.

One solution is to simultaneously estimate both change points and instantaneous ﬁring rates. A
switching state space model(SSSM)[7] can model nonstationary time-series data that include change
points. An SSSM deﬁnes two or more system models  one of which is modeled to generate observa-
tion data through an observation model. It can model nonstationary time-series data while switching
system models at change points. Each system model estimates stationary state variables in the region
that it handles. Recent studies have been focusing on constructing algorithms for estimating ﬁring
rates using single-trial data to consider trial-by-trial variations in neural activities [8]. However 
these previous methods assume ﬁring rate stationarity within a trial. They cannot estimate nonsta-
tionary ﬁring rates that include change points. An SSSM may be used to estimate nonstationary
ﬁring rates using single-trial data.

We propose an algorithm for simultaneously estimating state transitions among neural states and
nonstationary ﬁring rates using an SSSM. We expect to be able to estimate change points when
not only mean ﬁring rates but also temporal proﬁles of ﬁring rates change discontinuously. Our
algorithm consists of a non-Gaussian SSSM  whose non-Gaussian property is caused by binary
spike events. Learning and estimation algorithms consist of variational Bayes[9 10] and local varia-
tional methods[11 12]. Automatic relevance determination (ARD) induced by the variational Bayes
method[13] enables us to estimate the number of neural states after pruning redundant ones. For
simplicity  we focus on analyzing single-neuron data. Although many studies have discussed state
transitions by analyzing multi-neuron data  some of them have suggested that single-neuron activ-
ities reﬂect state transitions in a recurrent neural network[14]. Note that we can easily extend our
algorithm to multi-neuron analysis using the often-used assumption that change points are common
among recorded neurons[1-3].

2 Deﬁnitions of Probabilistic Model

2.1 Likelihood Function

Observation time T consists of K time bins of widths ∆ (ms)  and each bin includes at most one
spike (∆ ¿ 1). The spike timings are t = ft1  ...  tSg where S is the total number of observed
spikes. We deﬁne ηk such that ηk = +1 if the kth bin includes a spike and ηk = ¡1 otherwise
(k = 1  ...  K). The likelihood function is deﬁned by the Bernoulli distribution

p(tjλ) =

K

k=1(λk∆) 1+ηk

(1 ¡ λk∆) 1¡ηk

2

2

 

(1)
where λ = fλ1  ...  λKg and λk is the ﬁring rate at the kth bin. The product of ﬁring rates and bin
width corresponds to the spike-occurrence probability and λk∆ 2 [0  1) since ∆ ¿ 1. The logit
1¡λk∆ (xk 2 (¡1  1)) lets us consider the nonnegativity of ﬁring
transformation of exp(2xk) = λk∆
rates in detail[11]. Hereinafter  we call x = fx1  ...  xKg the “ﬁring rates”.
Since K is a large because ∆ ¿ 1  the computational cost and memory accumulation do matter.
We thus use coarse graining[15]. Observation time T consists of M coarse bins of widths r = C∆
(ms). A coarse bin includes many spikes and the ﬁring rate in each bin is constant. The likelihood
function which is obtained by applying the logit transformation and the coarse graining to eq. (1) is

∏

∏

∑

p(tjx) =
u=1 η(m¡1)C+u.

C

where ˆηm =

M

m=1[exp(ˆηmxm ¡ C log 2 cosh xm)] 

(2)

2.2 Switching State Space Model

An SSSM consists of N system models; for each model  we de-
ﬁne a prior distribution. We deﬁne label variables zn
m such that
m = 1 if the nth system model generates an observation in the
zn
m = 0 otherwise (n = 1  ...  N  m = 1  ...  M).
mth bin and zn

2

Figure 1: Graphical model rep-
resentation of SSSM.

x11x21xM1x1Nx2NxMN12Mzzzη1η2ηMFiring rateLabelvariableSpiketrain^^^108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161

We call N the number of labels and the nth system model the nth
label. The joint distribution is deﬁned by

N
n=1

(2π)M exp(¡ βn
jβnΛj

2 (xn ¡ µn)T Λ(xn ¡ µn)) 

(7)

p(t  x  zjθ

0) = p(tjx  z)p(zjπ  a)p(xjµ  β) 
1   ...  xn
M

(3)
g  z = fz1
1  ..  z1

M

N
n=1

∏

1   ...  zN
M

M   ...  zN

g  and θ

p(tjx  z) =

where x = fx1  ...  xNg  xn = fxn
∏
fπ  a  µ  βg are parameters. The likelihood function  including label variables  is given by
∏

We deﬁne the prior distributions of label variables as
n=1(πn)zn

¡ C log 2 cosh xn
∑
∑
n=1 πn ¡ 1) 
k=1 ank ¡ 1) 
m+1 δ(

(6)
where πn and ank are the probabilities that the nth label is selected at the initial time and that the
nth label switches to the kth one  respectively. The prior distributions of ﬁring rates are Gaussian

0 =

(4)

(5)

m=1[exp(ˆηmxn

k=1(ank)zn

m)]zn
m.

N
n=1

1 δ(

mzk

m

N

N

N

p(zm+1jzm  a) =

∏
p(z1jπ) =
∏
correlation satisfying p(xnjβn  µn) /∏

n=1 p(xnjβn  µn) =

p(x) =

∏

N

N

∏
√

where βn  µn respectively mean the temporal correlation and the mean values of the nth-label ﬁring
rates (n = 1  ...  N). Here for simplicity  we introduced Λ  which is the structure of the temporal
2 ((xm ¡ µm) ¡ (xm¡1 ¡ µm¡1))2). Figure 1

m exp(¡ βn
depicts a graphical model representation of an SSSM.

[

∏

∏

p(πjγn) = C(γn)

Ghahramani & Hinton (2000) did not introduce a priori knowledge about the label switching fre-
quencies. However  in many cases  the time scale of state transitions is probably slower than that of
∑
the temporal variation of ﬁring rates. We deﬁne prior distributions of π and a to introduce a priori
knowledge about label switching frequencies using Dirichlet distributions
∑
∏
n=1 πn ¡ 1) 
n=1(πn)γn¡1δ(
k=1 ank ¡ 1)
p(ajγnk) =
k=1(ank)γnk¡1δ(
Γ(γ1)...Γ(γN )   C(γnk) = Γ(PN
n=1 γn)
k=1 γnk)

where C(γn) = Γ(PN
(9)
∫ 1
Γ(γn1)...Γ(γnN ) . C(γn) and C(γnk) correspond to the
normalization constants of p(πjγn) and p(ajγnk)  respectively. Γ(u) is the gamma function deﬁned
0 dttu¡1 exp(¡t). γn  γnk are hyperparameters to control the probability that the nth
by Γ(u) =
label is selected at the initial time and that the nth label switches to the kth. We deﬁne the prior
distributions of µn and βn using non-informative priors. Since we do not have a priori knowledge
about neural states  µ and β  which characterize each neural state  should be estimated from scratch.

C(γnk)

N
n=1

]

(8)

N

N

N

N

 

3 Estimation and Learning of non-Gaussian SSSM

It is generally computationally difﬁcult to calculate the marginal posterior distribution in an
SSSM[6]. We thus use the variational Bayes method to calculate approximated posterior distri-
butions q(w) and q(θ) that minimize the variational free energy

F[q] =

U[q] = ¡∫∫

where w = fz  xg are hidden variables  θ = fπ  ag are parameters 

dwdθq(w)q(θ) log p(t  w  θ) and S[q] = ¡∫∫

dwdθq(w)q(θ) log q(w)q(θ)

p(t w θ) = U[q] ¡ S[q]

(

(10)

)

dwdθq(w)q(θ) log

q(w)q(θ)

.

∫∫

We denote q(w) and q(θ) as test distributions. The variational free energy satisﬁes

log p(t) = ¡F[q] + KL(q(w)q(θ)kp(w  θjt)) 
∫

(11)
where KL(q(w)q(θ)kp(w  θjt)) is the Kullback-Leibler divergence between test distributions and
a posterior distribution p(w  θjt) deﬁned by KL(q(y)kp(yjt)) =
p(yjt) . Since the
marginal likelihood log p(t) takes a constant value  the minimization of variational free energy in-
directly minimizes Kullback-Leibler divergence. The variational Bayes method requires conjugacy
between the likelihood function (eq. (4)) and the prior distribution (eq. (7)). However  eqs. (4) and
(7) are not conjugate to each other because of the binary spike events. The local variational method
enables us to construct a variational Bayes algorithm for a non-Gaussian SSSM.

dyq(y) log q(y)

3

162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215

3.1 Local Variational Method

m) with respect to (xn

The local variational method  which was proposed by Jaakola & Jordan[11]  approximately trans-
forms a non-Gaussian distribution into a quadratic-form distribution by introducing variational pa-
rameters. Watanabe et al. have proven the effectiveness of this method in estimating stationary
ﬁring rates[12]. The exponential function in eq. (4) includes f(xn
m  which is a
m)2. The concavity can be conﬁrmed by showing the negativity of the
concave function of y = (xn
∏
second-order derivative of f(xn
m. Considering the tangent line of
f(xn
pξ(tjx  z) =
(12)
m is a variational parameter. Equation (12) satisﬁes the inequality pξ(tjx  z) • p(tjx  z).
where ξn
We use eq. (12) as the likelihood function instead of eq. (4). The conjugacy between eqs. (12)
and (7) enables us to construct the variational Bayes algorithm. Using eq. (12)  we ﬁnd that the
variational free energy

m)2 at (xn
m=1[exp(ˆηmxn

m)2  we get a lower bound for eq. (4)

m)2)) ¡ C log 2 cosh ξn

m) with respect to (xn

m) = log 2 cosh xn

m)2 ¡ (ξn

¡ C tanh ξn

m)2 for all xn

m)2 = (ξn

m)]zn
m 

∏

N
n=1

((xn

2ξn
m

M

m

m

satisﬁes the inequality Fξ[q] ‚ F[q]  where Uξ[q] = ¡∫∫

dwdθq(w)q(θ) log q(w)q(θ)

Fξ[q] =

pξ(t w θ) = Uξ[q] ¡ S[q]

dwdθqξ(w)qξ(θ) log pξ(s  w  θ).
Since the inequality log p(t  x  z) ‚ ¡F[q] ‚ ¡Fξ[q] is satisﬁed  the test distributions that mini-
mize Fξ[q] can indirectly minimize F[q] which is analytically intractable. Using the EM algorithm
to estimate variational parameters improves the approximation accuracy of Fξ[q][16].

∫∫

(13)

∏

N

3.2 Variational Bayes Method

m

N

N

1

M

2ξn
m

∫

∫

(14)

N
n=1

C tanh ξn
m

∏

∏

∑

M¡1
m=1

(h(xn

z q(z) = 1 

dπq(π) = 1 and

q(z) /∏

∏
q(xnjµn  βn) =

∫
n=1(q(xnjµn  βn))q(z)
We assume the test distributions that satisfy the constraints q(w) =
and q(θ) = q(π)q(a)  where µ = fµ1  ...  µNg  β = fβ1  ...  βNg. Under constraints
dxq(xjµ  β) = 1 
daq(a) = 1  we can obtain the test
distributions of hidden variables xn  z that minimize eq. (13) as follows:

√
∏
∏
2(xn ¡ ˆµn)T W n(xn ¡ ˆµn)) 
(2π)M exp(¡ 1
jW nj
m=1 exp(ˆbn
m )zn
n=1 exp(ˆπn)zn
m)zn
k=1 exp(ˆank
(15)
i ¡
m = ˆηmhxn
where W n = CLn + βnΛ  ˆµn = (W n)¡1(wn + βnΛµn)  ˆπn = hlog πni  ˆbn
m  ˆank = hlog anki  Ln is the diagonal matrix whose
m)2) ¡ C log 2 cosh ξn
m)2i ¡ (ξn
i tanh ξn
(m  m) component is hzn
iˆηm. h¢i means
the average obtained using a test distribution q(¢). The computational cost of calculating the inverse
of each W is O(M) because Λ is deﬁned by a tridiagonal and Ln is a diagonal matrix.
∑
i controls the effective variance of the likelihood function. A higher
In the calculation of q(xn)  hzn
i means the data are
hzn
(
i = 1  all labels estimate their ﬁring rates on the basis
unreliable. Under the constraint
m)2i that will be
of divide-and-conquer principle of data reliability. Using the equality (ξn
m = ˆηmhxn
developed in the next section  we obtain ˆbn
1 +
2 log 2 cosh
i2
(m m)/hxn
¡1
(W n)
in eq. (15). When the mth bin includes many (few) spikes  the nth label tends
m
to be selected if it estimates the highest (lowest) ﬁring rate among the labels. But the variance of the
¡1
nth label (W n)
(m m) penalizes that label’s selection probability.

i means the data are reliable for the nth label in the mth bin and lower hzn

  wn is the vector whose (1  m) component is hzn

i ¡ C log 2 coshhxn

m)2 = h(xn
i ¡ C

hzn

N
n=1

N
n=1

m+1 

)

mzk

ξn
m

m

m

m

m

m

m

m

m

m

m

∏
[

We can also obtain the test distribution of parameters π  a as

∑
∏
∑
n=1 πn ¡ 1) 
n=1(πn)ˆγn¡1δ(
k=1 ank ¡ 1)
k=1(ank)ˆγnk¡1δ(
C(ˆγnk)
∑
Γ(ˆγ1)...Γ(ˆγN )   C(ˆγnk) = Γ(PN

q(π) = C(ˆγn)
q(a) =

where C(ˆγn) = Γ(PN
k=1 ˆγnk)
normalization constants of q(π) and q(a)  and ˆγn = hzn

∏

n=1 ˆγn)

N
n=1

N

N

N

N

Γ(ˆγn1)...Γ(ˆγnN ) . C(ˆγn) and C(ˆγnk) correspond to the
i + γnk.

i + γ1  ˆγnk =

hzn
mzk

M¡1
m=1

m+1

1

]

 

(16)

(17)

4

216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269

We can see γn in ˆγn controls the probability that the nth label is selected at the initial time  and
γnk in ˆγnk biases the probability of the transition from the nth label to the kth label. A forward-
backward algorithm enables us to calculate the ﬁrst- and second-order statistics of q(z). Since an
SSSM involves many local solutions  we search for a global one using deterministic annealing 
which is proven to be effective for estimating and learning in an SSSM [7].

3.3 EM algorithm

The EM algorithm enables us to estimate variational parameters ξ and parameters µ and β. In the
EM algorithm  the calculation of the Q function is computationally difﬁcult because it requires us to
calculate averages using the true posterior distribution. We thus calculate the Q function using test
distributions instead of the true posterior distributions as follows:
˜Q(µ  β  ξkµ(t
0
)  β(t
Since ˜Q(µ  β  ξkµ(t
equivalent to minimizing the variational free energy (eq. (10) ). The update rules
m)2i 

∫
dxq(xjµ(t
))q(z)q(π)q(a) log pξ(t  x  z  π  ajµ  β). (18)
)) = ¡U[q]ξ  maximizing the Q function with respect to µ  β  ξ is

0
)  ξ(t
m = hxn
µn

Tr[Λ((Wn)¡1+(hxni¡µn)(hxni¡µn)T )]

)) =
0
)  β(t

m)2 = h(xn
(ξn

0
)  β(t

0
)  ξ(t

βn =

and

(19)

i 

M

m

0

0

0

¶

maximize the Q function. The following table summarizes our algorithm.

Summary of our algorithm
Initialize parameters of model.

t

0 ˆ 1
) ˆ ξ  µ  β

0
)  µ(t

0
)  β(t

Set γ1 and γnk.
Perform the following VB and EM algorithm until Fξ[q] converges.
0
ξ(t
Variational Bayes algorithm Perform the VB-E and VB-M step until F
VB-E step: Compute q(xjµ(t
VB-M step: Compute q(π) and q(a) using eq. (16) and eq. (17).

)) and q(z) using eq. (14) and eq. (15).

0
)  β(t

ξ(t

0

0

)[q] converges.

‡

·

0 + 1(cid:181)

EM algorithm Compute ξ  µ  β using eq. (19).
0 ˆ t

t

4 Results
The estimated ﬁring rate in the mth bin is deﬁned by ˜xm = hx˜nm
arg maxnhzn
and hzn
N ¡ (the number of pruned labels)  where we assume that the nth label is pruned out if hzn
10¡5(8

i  where ˜nm satisﬁes ˜nm =
k 6= n)
k 6= n). The estimated number of labels ˜N is given by ˜N =
i <
m). We call our algorithm “the variational Bayes switching state space model” (VB-SSSM).

i. The estimated change points ˜mr = ˜mC∆ satisﬁes hzn

i < hzk

i > hzk

i (9

i (8

˜m+1

˜m+1

m

˜m

m

˜m

m

4.1 Synthetic data analysis and Comparison with previous methods

We artiﬁcially generate spike trains from arbitrarily set ﬁring rates with an inhomogeneous gamma
process. Throughout this study  we set κ which means the spike irregularity to 2.4 in generating
spike trains. We additionally conﬁrmed that the following results are invariant if we generate spikes
using inhomogeneous Poisson or inverse Gaussian process.
In this section  we set parameters to N = 5  T = 4000  ∆ = 0.001  r = 0.04  γn = 1  γnk =
100(n = k) or 2.5(n 6= k). The hyperparameters γnk represent the a priori knowledge where the
time scale of transitions among labels is sufﬁciently slower than that of ﬁring-rate variations.

4.1.1 Accuracy of change-point detections

This section discusses the comparative results between the VB-SSSM and mPHMM regarding the
accuracy of change-point detections and number-of-labels estimation. We used the EM algorithm to

5

270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323

Figure 2: Comparative results of change-point detections for the VB-SSSM and the mPHMM. (a)
and (c): Arbitrary set ﬁring rates for validating the accuracy of change-point detections when ﬁring
rates include discontinuous changes in mean value (ﬁg. (a)) or temporal correlation (ﬁg. (c)). (b)
and (d): Comparative results that correspond to ﬁring rates in (a) ((b)) and (c) ((d)). The stronger
the white color becomes  the more dominant the label is in the bin.

estimate the label variables in the mPHMM[1-3]. Since the mPHMM is useful in analyzing multi-
(
trial data  in the estimation of mPHMM we used ten spike trains under the assumption that change
)
)
points were common among ten spike trains. On the other hand  VB-SSSM uses single-trial data.
Fig. 2(a) displays arbitrarily set ﬁring rates to verify the change point detection accuracy when
t 2
mean ﬁring rates changed discontinuously. The ﬁring rate at time t(ms) was set to λt = 0.0
t 2 [3000  4000]
[0  1000)  t 2 [2000  3000)
.
The upper graph in ﬁg. 2(b) indicates the label variables estimated with the VB-SSSM and the
lower indicates those estimated with the mPHMM. In the VB-SSSM  ARD estimated the number
of labels to be three after pruning redundant labels. As a result of ten-trial data analysis  the VB-
SSSM estimated the number of labels to be three in nine over ten spike trains. The estimated change
points were 1000§0.0  2000§0.0  and 2990§16.9ms. The true change points were 1000  2000  and
3000ms.

(
)
t 2 [1000  2000)

  and λt = 60.0

  λt = 110.0

(

(

)
t 2 [0  2000)

(

)
t 2 [2000  4000]

  λt = λt¡1 + 20.0zt

Fig. 2(c) plots the arbitrarily set ﬁring rates for verifying the change point detection accuracy when
temporal correlation changes discontinuously. The ﬁring rate at time t(ms) was set to λt = λt¡1 +
2.0zt
  where zt is a standard normal
random variable that satisﬁes hzti = 0  hztzt0i = δtt0 (δtt0 = 0(t 6= t
0)). Fig. 2(d)
shows the comparative results between the VB-SSSM and mPHMM. ARD estimates the number of
labels to be two after pruning redundant labels. As a result of ten-trial data analysis  our algorithm
estimated the number of labels to be two in nine over ten spike trains. The estimated change points
was 1933§315.1ms and the true change point was 2000ms.

0)  1(t = t

4.1.2 Accuracy of ﬁring-rate estimation

This section discusses the nonstationary ﬁring rate estimation accuracy. The comparative methods
include kernel smoothing (KS)  kernel band optimization (KBO)[17]  adaptive kernel smoothing
(KSA)[18]  Bayesian adaptive regression splines (BARS)[19]  and Bayesian binning (BB)[20]. We
used a Gaussian kernel in KS  KBO  and KSA. The kernel widths σ were set to σ = 30 (ms) (KS30) 
)
σ = 50 (ms) (KS50) and σ = 100 (ms) (KS100) in KS. In KSA  we used the bin widths estimated
(
)
using KBO. Cunningham et al. have reviewed all of these compared methods [8].
t 2 [0  480)  t 2 [3600  4000]
  λt = 90.0 £
A ﬁring rate at time t(ms) was set to λt = 5.0
exp(¡11 (t¡480)
t 2 [2400  3600)
4000 )
and we reset λt to 5.0 if λt < 5.0. We set these ﬁring rates assuming an experiment in which tran-
sient and persistent inputs are applied to an observed neuron in a series. Note that input information 
such as timings  properties  and sequences is entirely unknown.

  λt = 80.0 £ exp(¡0.5(t ¡ 2400)/4000))

)
t 2 [480  2400)

(

(

6

(a)mPHMM0< z >1VB-SSSM04080120Firing rate (Hz)True firing rate10002000300004000Time (ms)0< z >110002000300004000(c)(b)(d)True firing rate04080120Firing rate (Hz)mPHMMVB-SSSMTime (ms)324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377

Figure 3: Results of ﬁring-rate estimation. (a): Estimated ﬁring rates. Vertical bars above abscissa
axes are spikes used for estimates. (b): Averaged label variables hzn
i. (c): Estimated ﬁring rates
using each label. (d): Mean absolute error § standard deviation when applying our algorithm and
other methods to estimate ﬁring rates plotted in (a). * indicates p<0.01 and ** indicates p<0.005.

m

Fig. 3(a) plots the estimated ﬁring rates (red line). Fig. 3(b) plots the estimated label variables and
ﬁg. 3(c) plots the estimated ﬁring rates when all labels other than the pruned ones were used. ARD
estimates the number of labels to be three after pruning redundant labels. As a result of ten spike
trains analysis  the VB-SSSM estimated the number of labels to be three in eight over ten spike
trains. The change points were estimated at 420§82.8  2385§20.7  and 3605§14.1ms. The true
change points were 480  2400  and 3600ms.

∑

jλk ¡ ˆλkj  where λk and ˆλk are
The mean-absolute-error (MAE) is deﬁned by MAE = 1
K
the true and estimated ﬁring rates in the kth bin. All the methods estimate the ﬁring rates at ten
times. Fig. 3(d) shows the mean MAE values averaged across ten trials and the standard deviations.
We investigated the signiﬁcant differences in ﬁring-rate estimation among all the methods using
Wilcoxon signed rank test. Both the VB-SSSM and BB show the high performance. Note that the
VB-SSSM can estimate not only ﬁring rates but change points and the number of neural states.

K
k=1

4.2 Real Data Analysis

In area MT  neurons preferentially respond to the movement directions of visual inputs[21]. We ana-
lyzed the neural data recorded from area MT of a rhesus monkey when random dots were presented.
These neural data are available from the Neural Signal Archive (http://www.neuralsignal.org.)  and
detailed experimental setups are described by Britten et al. [22]. The input onsets correspond to
t = 0(ms)  and the end of the recording corresponds to t = 2000(ms). This section discusses our
analysis of the neural data included in nsa2004.1 j001 T2. These data were recorded from the same
neuron of the same subject. Parameters were set as follows: T = 2000  ∆ = 0.001  N = 5  r =
0.02  γn = 1(n = 1  ...  5)  γnk = 100(n = k) or 2.5(n 6= k).
Fig. 4 shows the analysis results when random dots have 3.2% coherence. Fig. 4 (a) plots the
estimated ﬁring rates (red line) and a Kolmogorov-Smirnov plot (K-S plot) (inset)[23]. Since the
true ﬁring rates for the real data are entirely unknown  we evaluated the reliability of estimated
values from the conﬁdence intervals. The black and gray lines in the inset denote the K-S plot and
95 % conﬁdence intervals. The K-S plot supported the reliability of the estimated ﬁring rates since
it ﬁts into the 95% conﬁdence intervals. Fig. 4(b) depicts the estimated label variables  and ﬁg.
4(c) shows the estimated ﬁring rates using all labels other than the pruned ones. The VB-SSSM
estimates the number of labels to be two. We call the label appearing on the right after the input
onset “the 1st neural state” and that appearing after the 1st neural state “the 2nd neural state”. The
1st and 2nd neural states in ﬁg. 4 might corresponded to transient and sustained states[6] that have
been heuristically detected  e.g. assuming the sustained state lasts for a constant time[24].

We analyzed all 105 spike trains recorded under presentations of random dots with 3.2%  6.4% 
12.8%  and 99.9% coherence  precluding the neural data in which the total spike count was less than

7

04080120Firing rate (Hz)Estimated firing rateTrue firing rate12345Label number0< z >110002000300004000Time (ms)(a)(b)10002000300004000Time (ms)04080120Firing rate (Hz)Estimated value using label 2True firing rateEstimated value using label 3Estimated value using label 110002000300004000Time (ms)(c)(d)56789101112BARSMean absolute errorKS30KS100KS50KBOKSABBVB-SSSM********(cid:638)(cid:638)(cid:638)p<0.01 **(cid:638)(cid:638)(cid:638)p<0.005*****378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431

Figure 4: Estimated results when applying the VB-SSSM to area MT neural data. (a): Estimated
ﬁring rates. Vertical bars above abscissa axes are spikes used for estimates.
Inset is result of
Kolmogorov-Smirnov goodness-of-ﬁt. Solid and gray lines correspond to K-S plot and 95% conﬁ-
dence interval. (b): Averaged label variables using test distribution. (c): Estimated ﬁring rates using
each label. (d) and (e): Estimated parameters in the 1st and the 2nd neural states.

20. The VB-SSSM estimated the number of labels to be two in 25 over 30 spike trains (3.2%)  19
over 30 spike trains (6.4%)  26 over 30 spike trains (12.8%)  and 16 over 16 spike trains (99.9%). In
summary  the number of labels is estimated to be two in 85 over 101 spike trains.

∑

Tn

Figs. 4(d) and (e) show the estimated parameters from 19 spike trains whose estimated number
of labels was two (6.4% coherence). The horizontal axis denotes the arranged number of trials
in ascending order. Figs. 4 (d) and (e) correspond to the estimated temporal correlation β and
the time average of µ  which is deﬁned by hµni = 1
t   where Tn denotes the sojourn
time in the nth label or the total observation time T . The estimated temporal correlation differed
signiﬁcantly between the 1st and 2nd neural states (Wilcoxon signed rank test  p<0.00005). On the
other hand  the estimated mean ﬁring rates did not differ signiﬁcantly between these neural states
(Wilcoxon signed rank test  p>0.1). Our algorithm thus detected the change points on the basis of
discontinuous changes in temporal correlations. We could see the similar tendencies for all random-
dot coherence conditions (data not shown). We conﬁrmed that the mPHMM could not detect these
change points (data not shown)  which we were able to deduce from the results shown in ﬁg. 2(d).
These results suggest that our algorithm is effective in real data analysis.

Tn
t=1 µn

5 Discussion

We proposed an algorithm for simultaneously estimating state transitions  the number of neural
states  and nonstationary ﬁring rates using single-trial data.

There are ways of extending our research to analyze multi-neuron data. The simplest one assumes
that the time of state transitions is common among all recorded neurons[1-3]. Since this assumption
can partially include the effect of inter-neuron interactions  we can deﬁne prior distributions that are
independent between neurons. Because there are no loops in the statistical dependencies of ﬁring
rates under these conditions  the variational Bayes method can be applied directly.
One important topic for future study is optimization of coarse bin widths r = C∆. A bin width
that is too wide obscures both the time of change points and temporal proﬁle of nonstationary ﬁring
rates. A bin width that is too narrow  on the other hand  increases computational costs and worsens
estimation accuracy. Watanabe et al. proposed an algorithm for estimating the optimal bin width by
maximization the marginal likelihood [15]  which is probably applicable to our algorithm.

8

0< z >112345Label number5001000150002000Time (ms)(a)(b)(c)50010001500020000100200Firing rate (Hz)Estimated firing rateK-S(cid:1)plotEstimated value using label 2Estimated value using label 45001000150002000Time (ms)0100200Firing rate (Hz)β(d)(e)Trial number051015200.51.52.53.5x 105The 1st neural stateThe 2nd neural state-2.2-1.8-1.4The 1st neural stateThe 2nd neural stateTrial number<d(cid:1046)>05101520p<0.0005p>0.1432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485

[1] Abeles  M. et al. (1995)  PNAS  pp. 609-616.
[2] Kemere  C. et al. (2008) J. Neurophyiol. 100(7):2441-2452.
[3] Jones  L. M. et al. (2007)  PNAS 104(47):18772-18777.
[4] Rickert  J. et al. (2009) J. Neurosci. 29(44): 13870-13882.
[5] Harvey  C. D. et al. (2009)  Nature 461(15):941-946.
[6] Lisberger  et al. (1999)  J. Neurosci. 19(6):2224-2246.
[7] Ghahramani  Z.  and Hinton  G. E. (2000) Neural Compt. 12(4):831-864.
[8] Cunningham J. P. et al. (2007)  Neural Netw. 22(9):1235-1246.

[9] Attias  H. (1999)  Proc. 15th Conf. on UAI
[10] Beal  M. (2003)  Pd. D thesis University College London.
[11] Jaakkola  T. S.  and Jordan  M. I. (2000).  Stat. and Compt. 10(1): pp. 25-37.
[12] Watanabe  K. and Okada  M. (2009) Lecture Notes in Computer Science 5506:655-662.

[13] Corduneanu  A. and Bishop  C. M. (2001) Artiﬁcial Intelligence and Statistics: 27-34.
[14] Fuzisawa  S. et al. (2005)  Cerebral Cortex 16(5):639-654.
[15] Watanabe  K. et al. (2009)  IEICE E92-D(7):1362-1368.
[16] Bishop  C. M. (2006)  Pattern Recognition and Machine Learning  Springer.

[17] Shimazaki  H.  and Shinomoto  S. (2007)  Neural Coding Abstract :120-123.
[18] Richmond  B. J. et al. (1990)  J. Neurophysiol. 64(2):351-369.
[19] Dimatteo  I.  et al. (2001)  Biometrika 88(4):1055-1071.
[20] Endres  D. et al. (2008)  Adv. in NIPS 20:393-340.
[21] Maunsell  J. H. and Van Essen  D. C. (1983) J. Neurophysiol. 49(5): 1127-1147.
[22] Britten  K. H. et al. (1992)  J. Neurosci. 12:4745-4765.
[23] Brown  E. N. et al. (2002)  Neural Compt. 14(2):325-346.
[24] Bair  W. and Koch  C. (1996) Neural Compt. 8(6): 1185-1202.

9

,Yen-Chi Chen
Christopher Genovese
Shirley Ho
Larry Wasserman
Wenqi Ren
Jiawei Zhang
Lin Ma
Jinshan Pan
Xiaochun Cao
Wangmeng Zuo
Wei Liu
Ming-Hsuan Yang