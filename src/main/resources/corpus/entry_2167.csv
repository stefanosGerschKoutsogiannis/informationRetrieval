2014,From Stochastic Mixability to Fast Rates,Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution $\mathsf{P}$ and returns a hypothesis $f$ chosen from a fixed class $\mathcal{F}$ with small loss $\ell$. In the parametric setting  depending upon $(\ell  \mathcal{F} \mathsf{P})$ ERM can have slow $(1/\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a function of the sample size $n$. There exist several results that give sufficient conditions for fast rates in terms of joint properties of $\ell$  $\mathcal{F}$  and $\mathsf{P}$  such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting  there is an analogous slow and fast rate phenomenon  and it is entirely characterized in terms of the mixability of the loss $\ell$ (there being no role there for $\mathcal{F}$ or $\mathsf{P}$). The notion of stochastic mixability builds a bridge between these two models of learning  reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of $(\ell \mathcal{F}  \mathsf{P})$  and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.,From Stochastic Mixability to Fast Rates

Nishant A. Mehta

Research School of Computer Science

Australian National University

nishant.mehta@anu.edu.au

Robert C. Williamson

Research School of Computer Science

Australian National University and NICTA

bob.williamson@anu.edu.au

Abstract

Empirical risk minimization (ERM) is a fundamental learning rule for statistical
learning problems where the data is generated according to some unknown distri-
bution P and returns a hypothesis f chosen from a ﬁxed class F with small loss (cid:96).
√
In the parametric setting  depending upon ((cid:96) F  P) ERM can have slow (1/
n)
or fast (1/n) rates of convergence of the excess risk as a function of the sample
size n. There exist several results that give sufﬁcient conditions for fast rates in
terms of joint properties of (cid:96)  F  and P  such as the margin condition and the Bern-
stein condition. In the non-statistical prediction with expert advice setting  there
is an analogous slow and fast rate phenomenon  and it is entirely characterized in
terms of the mixability of the loss (cid:96) (there being no role there for F or P). The
notion of stochastic mixability builds a bridge between these two models of learn-
ing  reducing to classical mixability in a special case. The present paper presents
a direct proof of fast rates for ERM in terms of stochastic mixability of ((cid:96) F  P) 
and in so doing provides new insight into the fast-rates phenomenon. The proof
exploits an old result of Kemperman on the solution to the general moment prob-
lem. We also show a partial converse that suggests a characterization of fast rates
for ERM in terms of stochastic mixability is possible.

1

Introduction

Recent years have unveiled central contact points between the areas of statistical and online learning.
These include Abernethy et al.’s [1] uniﬁed Bregman-divergence based analysis of online convex
optimization and statistical learning  the online-to-batch conversion of the exponentially weighted
average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the
progressive mixture rule as can be seen e.g. from the work of Audibert [2]  and most recently Van
Erven et al.’s [21] injection of the concept of mixability into the statistical learning space in the form
of stochastic mixability. It is this last connection that will be our departure point for this work.
Mixability is a fundamental property of a loss that characterizes when constant regret is possible in
the online learning game of prediction with expert advice [23]. Stochastic mixability is a natural
adaptation of mixability to the statistical learning setting; in fact  in the special case where the func-
tion class consists of all possible functions from the input space to the prediction space  stochastic
mixability is equivalent to mixability [21]. Just as Vovk and coworkers (see e.g. [24  8]) have devel-
oped a rich convex geometric understanding of mixability  stochastic mixability can be understood
as a sort of effective convexity.
In this work  we study the O(1/n)-fast rate phenomenon in statistical learning from the perspective
of stochastic mixability. Our motivation is that stochastic mixability might characterize fast rates in
statistical learning. As a ﬁrst step  Theorem 5 herein establishes via a rather direct argument that
stochastic mixability implies an exact oracle inequality (i.e. with leading constant 1) with a fast rate
for ﬁnite function classes  and Theorem 7 extends this result to VC-type classes. This result can be
understood as a new chapter in an evolving narrative that started with Lee et al.’s [13] seminal paper

1

showing fast rates for agnostic learning with squared loss over convex function classes  and that was
continued by Mendelson [18] who showed that fast rates are possible for p-losses (y  ˆy) (cid:55)→ |y − ˆy|p
over effectively convex function classes by passing through a Bernstein condition (deﬁned in (12)).
We also show that when stochastic mixability does not hold in a certain sense (described in Sec-
tion 5)  then the risk minimizer is not unique in a bad way. This is precisely the situation at the
heart of the works of Mendelson [18] and Mendelson and Williamson [19]  which show that having
non-unique minimizers is symptomatic of bad geometry of the learning problem. In such situations 
there are certain targets (i.e. output conditional distributions) close to the original target under which
empirical risk minimization learns (ERM) at a slow rate  where the guilty target depends on the sam-
ple size and the target sequence approaches the original target asymptotically. Even the best known
upper bounds have constants that blow up in the case of non-unique minimizers. Thus  whereas
stochastic mixability implies fast rates  a sort of converse is also true  where learning is hard in a
“neighborhood” of statistical learning problems for which stochastic mixability does not hold. In
addition  since a stochastically mixable problem’s function class looks convex from the perspective
of risk minimization  and since when stochastic mixability fails the function class looks non-convex
from the same perspective (it has multiple well-separated minimizers)  stochastic mixability char-
acterizes the effective convexity of the learning problem from the perspective of risk minimization.
Much of the recent work in obtaining faster learning rates in agnostic learning has taken place in set-
tings where a Bernstein condition holds  including results based on local Rademacher complexities
[3  10]. The Bernstein condition appears to have ﬁrst been used by Bartlett and Mendelson [4] in
their analysis of ERM; this condition is subtly different from the margin condition of Mammen and
Tsybakov [15  20]  which has been used to obtain fast rates for classiﬁcation. Lecu´e [12] pinpoints
that the difference between the two conditions is that the margin condition applies to the excess loss
relative to the best predictor (not necessarily in the model class) whereas the Bernstein condition
applies to the excess loss relative to the best predictor in the model class. Our approach in this work
is complementary to the approaches of previous works  coming from a different assumption that
forms a bridge to the online learning setting. Yet this assumption is related; the Bernstein condition
implies stochastic mixability under a bounded losses assumption [21]. Further understanding the
connection between the Bernstein condition and stochastic mixability is an ongoing effort.

Contributions. The core contribution of this work is to show a new path to the ˜O(1/n)-fast rate
in statistical learning. We are not aware of previous results that show fast rates from the stochastic
mixability assumption. Secondly  we establish intermediate learning rates that interpolate between
the fast and slow rate under a weaker notion of stochastic mixability. Finally  we show that in a
certain sense stochastic mixability characterizes the effective convexity of the statistical problem.
In the next section we formally deﬁne the statistical problem  review stochastic mixability  and
explain our high-level approach toward getting fast rates. This approach involves directly appealing
to the Cram´er-Chernoff method  from which nearly all known concentration inequalities arose in one
way or another. In Section 3  we frame the problem of computing a particular moment of a certain
excess loss random variable as a general moment problem. We sufﬁciently bound the optimal value
of the moment  which allows for a direct application of the Cram´er-Chernoff method. These results
easily imply a fast rates bound for ﬁnite classes that can be extended to parametric (VC-type) classes 
as shown in Section 4. We describe in Section 5 how stochastic mixability characterizes a certain
notion of convexity of the statistical learning problem. In Section 6  we extend the fast rates results to
classes that obey a notion we call weak stochastic mixability. Finally  Section 7 concludes this work
with connections to related topics in statistical learning theory and a discussion of open problems.

2 Stochastic mixability  Cram´er-Chernoff  and ERM
Let ((cid:96) F  P) be a statistical learning problem with (cid:96) : Y × R → R+ a nonnegative loss  F ⊂ RX a
compact function class  and P a probability measure over X ×Y for input space X and output/target
space Y. Let Z be a random variable deﬁned as Z = (X  Y ) ∼ P. We assume for all f ∈ F 
(cid:96)(Y  f (X)) ≤ V almost surely (a.s.) for some constant V .
A probability measure P operates on functions and loss-composed functions as:

P (cid:96)(·  f ) = E(X Y )∼P (cid:96)(cid:0)Y  f (X)(cid:1).

P f = E(X Y )∼P f (X)

2

n(cid:88)

1
n

(cid:96)(cid:0)yj  f (xj)(cid:1).

n(cid:88)

j=1

1
n

Similarly  an empirical measure Pn associated with an n-sample z  comprising n iid samples
(x1  y1)  . . .   (xn  yn)  operates on functions and loss-composed functions as:

Pn (cid:96)(·  f ) =

Pn f =

f (xj)

Let f∗ be any function for which P (cid:96)(·  f∗) = inf f∈F P (cid:96)(·  f ). For each f ∈ F deﬁne the excess

risk random variable Zf := (cid:96)(cid:0)Y  f (X)(cid:1) − (cid:96)(cid:0)Y  f∗(X)(cid:1).

j=1

We frequently work with the following two subclasses. For any ε > 0  deﬁne the subclasses

F(cid:22)ε := {f ∈ F : P Zf ≤ ε}

F(cid:23)ε := {f ∈ F : P Zf ≥ ε} .

2.1 Stochastic mixability
For η > 0  we say that ((cid:96) F  P) is η-stochastically mixable if for all f ∈ F
(1)
If η-stochastic mixability holds for some η > 0  then we say that ((cid:96) F  P) is stochastically mixable.
Throughout this paper it is assumed that the stochastic mixability condition holds  and we take η∗ to
be the largest η such that η-stochastic mixability holds. Condition (1) has a rich history  beginning
from the foundational thesis of Li [14] who studied the special case of η∗ = 1 in density estimation
with log loss from the perspective of information geometry. The connections that Li showed between
this condition and convexity were strengthened by Gr¨unwald [6  7] and Van Erven et al. [21].

log E exp(−ηZf ) ≤ 0.

2.2 Cram´er-Chernoff

The high-level strategy taken here is to show that with high probability ERM will not select a ﬁxed
n for some constant a > 0. For each hypothesis  this
hypothesis function f with excess risk above a
guarantee will ﬂow from the Cram´er-Chernoff method [5] by controlling the cumulant generating
function (CGF) of −Zf in a particular way to yield exponential concentration. This control will be
possible because the η∗-stochastic mixability condition implies that the CGF of −Zf takes the value
0 at some η ≥ η∗  a fact later exploited by our key tool Theorem 3.
Let Z be a real-valued random variable. Applying Markov’s inequality to an exponentially trans-
formed random variable yields that  for any η ≥ 0 and t ∈ R

Pr(Z ≥ t) ≤ exp(−ηt + log E exp(ηZ));

(2)

the inequality is non-trivial only if t > E Z and η > 0.

2.3 Analysis of ERM
We consider the ERM estimator ˆfz := arg minf∈F Pn (cid:96)(·  f ). That is  given an n-sample z  ERM
selects any ˆfz ∈ F minimizing the empirical risk Pn (cid:96)(·  f ). We say ERM is ε-good when ˆfz ∈ F(cid:22)ε.
In order to show that ERM is ε-good it is sufﬁcient to show that for all f ∈ F \ F(cid:22)ε we have
P Zf > 0. The goal is to show that with high probability ERM is ε-good  and we will do this by
showing that with high probability uniformly for all f ∈ F \ F(cid:22)ε we have Pn Zf > t for some
slack t > 0 that will come in handy later.
For a real-valued random variable X  recall that the cumulant generating function of X is η (cid:55)→
ΛX (η) := log E eηX; we allow ΛX (η) to be inﬁnite for some η > 0.
Theorem 1 (Cram´er-Chernoff Control on ERM). Let a > 0 and select f such that E Zf > 0.
Let t < E Zf . If there exists η > 0 such that Λ−Zf (η) ≤ − a

Proof. Let Zf 1  . . .   Zf n be iid copies of Zf   and deﬁne the sum Sf n := (cid:80)n

Pn (cid:96)(·  f ) ≤ Pn (cid:96)(·  f∗) + t

Pr

j=1 −Zf j. Since

(cid:110)

n   then

(cid:111) ≤ exp(−a + ηt).
(cid:19)

(−t) > E 1

n Sf n  then from (2) we have

(cid:18) 1

n(cid:88)

n

j=1

Pr

(cid:19)

(cid:18) 1

n

Zf j ≤ t

= Pr

≤ exp (ηt + log E exp(ηSf n))

= exp(ηt)(cid:0)E exp(−ηZf )(cid:1)n

.

Sf n ≥ −t

3

Making the replacement Λ−Zf (η) = log E exp(−ηZf ) yields

(cid:18) 1

(cid:19)

Sf n ≥ −t

≤ ηt + nΛ−Zf (η).

n

log Pr
By assumption  Λ−Zf (η) ≤ − a
This theorem will be applied by showing that for an excess loss random variable Zf taking values in
[−1  1]  if for some η > 0 we have E exp(−ηZf ) = 1 and if E Zf = a
n for some constant a (that can
and must depend on n)  then Λ−Zf (η/2) ≤ − cηa
n where c > 0 is a universal constant. This is the
nature of the next section. We then extend this result to random variables taking values in [−V  V ].

n  and so Pr{Pn Zf ≤ t} ≤ exp(−a + ηt) as desired.

3 Semi-inﬁnite linear programming and the general moment problem

n and
The key subproblem now is to ﬁnd  for each excess loss random variable Zf with mean a
Λ−Zf (η) = 0 (for some η ≥ η∗)  a pair of constants η0 > 0 and c > 0 for which Λ−Zf (η0) ≤ − ca
n .
Theorem 1 would then imply that ERM will prefer f∗ over this particular f with high probability for
ca large enough. This subproblem is in fact an instance of the general moment problem  a problem
on which Kemperman [9] has conducted a very nice geometric study. We now describe this problem.
The general moment problem. Let P(A) be the space of probability measures over a measurable
space A = (A S). For real-valued measurable functions h and (gj)j∈[m] on a measurable space
A = (A S)  the general moment problem is

inf

EX∼µ h(X)

µ∈P(A)
subject to EX∼µ gj(X) = yj 

(3)
Let the vector-valued map g : A → Rm be deﬁned in terms of coordinate functions as (g(x))j =
gj(x)  and let the vector y ∈ Rm be equal to (y1  . . .   ym).
Let D∗ ⊂ Rm+1 be the set

j ∈ {1  . . .   m}.

D∗ :=

d∗ = (d0  d1  . . .   dm) ∈ Rm+1 : h(x) ≥ d0 +

djgj(x)

for all x ∈ A

.

(4)

(cid:26)

(cid:27)

Theorem 3 of [9] states that if y ∈ int conv g(A)  the optimal value of problem (3) equals

(cid:26)

m(cid:88)

djyj : d∗ = (d0  d1  . . .   dm) ∈ D∗(cid:27)

sup

d0 +

.

(5)

Our instantiation. We choose A = [−1  1]  set m = 2 and deﬁne h  (gj)j∈{1 2}  and y ∈ R2 as:

j=1

h(x) = −e(η/2)x 

g1(x) = x 

g2(x) = eηx 

y1 = − a
n

 

y2 = 1 

for any η > 0  a > 0  and n ∈ N. This yields the following instantiation of problem (3):

m(cid:88)

j=1

inf

EX∼µ −e(η/2)X
µ∈P([−1 1])
subject to EX∼µ X = − a
n
EX∼µ eηX = 1.

(6a)

(6b)

(6c)

(7)

(cid:111)

Note that equation (5) from the general moment problem now instantiates to

(cid:110)

sup

d0 − a
n

d1 + d2 : d∗ = (d0  d1  d2) ∈ D∗(cid:111)

 

with D∗ equal to the set

(cid:110)

d∗ = (d0  d1  d2) ∈ R3 : −e(η/2)x ≥ d0 + d1x + d2eηx

(8)
Applying Theorem 3 of [9] requires the condition y ∈ int conv g([−1  1]). We ﬁrst characterize
when y ∈ conv g([−1  1]) holds and handle the int conv g([−1  1]) version after Theorem 3.

for all x ∈ [−1  1]

.

4

Lemma 2 (Feasible Moments). The point y =(cid:0)− a
≤ eη + e−η − 2
Proof. Let W denote the convex hull of g([−1  1]). We need to see if(cid:0)− a
eη − e−η =

n   1(cid:1) ∈ conv g([−1  1]) if and only if

cosh(η) − 1

sinh(η)

a
n

.

n   1(cid:1) ∈ W . Note that W

(9)

is the convex set formed by starting with the graph of x (cid:55)→ eηx on the domain [−1  1]  including the
line segment connecting this curve’s endpoints (−1  e−η) to (1  eηx)  and including all of the points
below this line segment but above the aforementioned graph. That is  W is precisely the set

(x  y) ∈ R2 : eηx ≤ y ≤ eη + e−η

+

W :=

eη − e−η

2

2

x  ∀x ∈ [−1  1]

.

It remains to check that 1 is sandwiched between the lower and upper bounds at x = − a
the lower bound holds. Simple algebra shows that the upper bound is equivalent to condition (9).

n. Clearly

(cid:26)

(cid:27)

Note that if (9) does not hold  then the semi-inﬁnite linear program (6) is infeasible; infeasibility in
turn implies that such an excess loss random variable cannot exist. Thus  we need not worry about
whether (9) holds; it holds for any excess loss random variable satisfying constraints (6b) and (6c).
The following theorem is a key technical result for using stochastic mixability to control the CGF.
The proof is long and can be found in Appendix A.
Theorem 3 (Stochastic Mixability Concentration). Let f be an element of F with Zf taking val-
ues in [−1  1]  n ∈ N  E Zf = a

n for some a > 0  and Λ−Zf (η) = 0 for some η > 0. If

eη + e−η − 2
eη − e−η

a
n

<

 

(10)

then

E e(η/2)(−Zf ) ≤ 1 − 0.18(η ∧ 1)a

n

.

.

n

Note that since log(1 − x) ≤ −x when x < 1  we have Λ−Zf (η/2) ≤ − 0.18(η ∧ 1)a
In order to apply Theorem 3  we need (10) to hold  but only (9) is guaranteed to hold. The corner
case is if (9) holds with equality. However  observe that one can always approximate the random
variable X by a perturbed version X(cid:48) which has nearly identical mean a(cid:48) ≈ a and a nearly identical
η(cid:48) ≈ η for which EX(cid:48)∼µ(cid:48) eη(cid:48)X(cid:48)
= 1  and yet the inequality in (9) is strict. Later  in the proof
of Theorem 5  for any random variable that required perturbation to satisfy the interior condition
(10)  we implicitly apply the analysis to the perturbed version  show that ERM would not pick the
(slightly different) function corresponding to the perturbed version  and use the closeness of the two
functions to show that ERM also would not pick the original function.
We now present a necessary extension for the case of losses with range [0  V ]  proved in Appendix A.
Lemma 4 (Bounded Losses). Let g1(x) = x and y2 = 1 be common settings for the following two
problems. The instantiation of problem (3) with A = [−V  V ]  h(x) = −e(η/2)x  g2(x) = eηx 
n has the same optimal value as the instantiation of problem (3) with A = [−1  1] 
and y1 = − a
h(x) = −e(V η/2)x  g2(x) = e(V η)x  and y1 = − a/V
n .

4 Fast rates

We now show how the above results can be used to obtain an exact oracle inequality with a fast rate.
We ﬁrst present a result for ﬁnite classes and then present a result for VC-type classes (classes with
logarithmic universal metric entropy).
Theorem 5 (Finite Classes Exact Oracle Inequality). Let ((cid:96) F  P) be η∗-stochastically mixable 
for all n ≥ 1  with probability at least 1 − δ

where |F| = N  (cid:96) is a nonnegative loss  and supf∈F (cid:96)(cid:0)Y  f (X)(cid:1) ≤ V a.s. for a constant V . Then

(cid:110)

(cid:111)(cid:0)log 1

δ + log N(cid:1)

.

V  1
η∗

n

P (cid:96)(·  ˆfz) ≤ P (cid:96)(·  f∗) +

6 max

5

(cid:1)∪F hyper(cid:23)γn

F(cid:23)γn =(cid:0)(cid:83)

n for a constant a to be ﬁxed later. For each η > 0  let F (η)(cid:23)γn

⊂ F(cid:23)γn correspond
Proof. Let γn = a
to those functions in F(cid:23)γn for which η is the largest constant such that E exp(−ηZf ) = 1. Let
⊂ F(cid:23)γn correspond to functions f in F(cid:23)γn for which limη→∞ E exp(−ηZf ) < 1. Clearly 
F hyper(cid:23)γn
η∈[η∗ ∞) F (η)(cid:23)γn
. The excess loss random variables corresponding to elements
f ∈ F hyper(cid:23)γn
are “hyper-concentrated” in the sense that they are inﬁnitely stochastically mixable.
However  Lemma 10 in Appendix B shows that for each hyper-concentrated Zf   there exists another
excess loss random variable Z(cid:48)
f ) = 1 for
f ≤ Zf with probability 1. The last property implies
some arbitrarily large but ﬁnite η  and with Z(cid:48)
that the empirical risk of Z(cid:48)
f is no greater than that of Zf ; hence for each hyper-concentrated Zf it is
sufﬁcient (from the perspective of ERM) to study a corresponding Z(cid:48)
f . From now on  we implicitly
η∈[η∗ ∞) F (η)(cid:23)γn

make this replacement in F(cid:23)γn itself  so that we now have F(cid:23)γn =(cid:83)

f with mean arbitrarily close to that of Zf   with E exp(−ηZ(cid:48)

.

.

Consider an arbitrary a > 0. For some ﬁxed η ∈ [η∗ ∞) for which |F (η)(cid:23)γn
| > 0  consider
the subclass F (η)(cid:23)γn
Individually for each such function  we will apply Theorem 1 as follows.
(V η/2). From Theorem 3  the latter is at most
From Lemma 4  we have Λ−Zf (η/2) = Λ− 1
− 0.18(V η ∧ 1)(a/V )
(V η ∨ 1)n . Hence  Theorem 1 with t = 0 and the η from the Theo-
rem taken to be η/2 implies that the probability of the event Pn (cid:96)(·  f ) ≤ Pn (cid:96)(·  f∗) is at most
exp

(cid:19)(cid:19)
. Applying the union bound over all of F(cid:23)γn  we conclude that

V η ∨ 1 a
Pr {∃f ∈ F(cid:23)γn : Pn (cid:96)(·  f ) ≤ Pn (cid:96)(·  f∗)} ≤ N exp

−η∗(cid:18) 0.18a

(cid:16)−0.18

= − 0.18ηa

(cid:18)

(cid:17)

V Zf

n

η

.

V η∗ ∨ 1

.

n

δ +log N)

η∗}(log 1

Since ERM selects hypotheses on their empirical risk  from inversion it holds that with probability at
least 1 − δ ERM will not select any hypothesis with excess risk at least 6 max{V  1
Before presenting the result for VC-type classes  we require some deﬁnitions. For a pseudometric
space (G  d)  for any ε > 0  let N (ε G  d) be the ε-covering number of (G  d); that is  N (ε G  d) is
the minimal number of balls of radius ε needed to cover G. We will further constrain the cover (the
set of centers of the balls) to be a subset of G (i.e. to be proper)  thus ensuring that the stochastic
mixability assumption transfers to any (proper) cover of F. Note that the “proper” requirement at
most doubles the constant K below  as shown by Vidyasagar [22  Lemma 2.1].
We now state a localization-based result that allows us to extend the result for ﬁnite classes to VC-
type classes. Although the localization result can be obtained by combining standard techniques 1
we could not ﬁnd this particular result in the literature. Below  an ε-net Fε of a set F is a subset of
F such that F is contained in the union of the balls of radius ε with centers in Fε.
Theorem 6. Let F be a separable function class whose functions have range bounded in [0  V ] and
for which  for a constant K ≥ 1  for each u ∈ (0  K] the L2(P) covering numbers are bounded as

N (u F  L2(P)) ≤

(cid:18) K

(cid:19)C

.

u

Suppose Fε is a minimal ε-net for F in the L2(P) norm  with ε = 1
L2(P)-metric projection from F to Fε. Then  provided that δ ≤ 1
there exist f ∈ F such that

(cid:115)(cid:18)

(cid:19)

(cid:32)

Pn f < Pn (π(f )) − V
n

1080C log(2Kn) + 90

log

1
δ

C log(2Kn) + log

e
δ

(11)
n . Denote by π : F → Fε an
2   with probability at most δ can

(cid:33)

.

The proof is presented in Appendix C. We now present the fast rates result for VC-type classes.
The proof (in Appendix C) uses Theorem 6 and the proof of the Theorem 5. Below  we denote the
loss-composed version of a function class F as (cid:96) ◦ F := {(cid:96)(·  f ) : f ∈ F}.

1See e.g. the techniques of Massart and N´ed´elec [16] and equation (3.17) of Koltchinskii [11].

6

ε

1
n

(cid:16)

2V

) +

max

(cid:1)C

P (cid:96)(·  ˆfz) ≤ P (cid:96)(·  f

∗

for all n ≥ 5 and δ ≤ 1

N ((cid:96) ◦ F  L2(P)  ε) ≤ (cid:0) K

(cid:110)
2   with probability at least 1 − δ
V  1
η∗

Theorem 7 (VC-Type Classes Exact Oracle Inequality). Let ((cid:96) F  P) be η∗-stochastically mix-
able with (cid:96) ◦ F separable  where 
for each ε ∈ (0  K] we have

for a constant K ≥ 1 

  and supf∈F (cid:96)(cid:0)Y  f (X)(cid:1) ≤ V a.s. for a constant V ≥ 1. Then
 +

2 (X)(cid:1) a.s. We say the excess loss class

(cid:111)(cid:0)C log(Kn) + log 2
(cid:113)(cid:0)log 2

(cid:1)C log(2Kn) + log 2e

2 of P (cid:96)(·  f ) over F satisfy (cid:96)(cid:0)Y  f∗

5 Characterizing convexity from the perspective of risk minimization
In the following  when we say ((cid:96) F  P) has a unique minimizer we mean that any two minimizers
f∗
1   f∗
{(cid:96)(·  f )− (cid:96)(·  f∗) : f ∈ F} satisﬁes a (β  B)-Bernstein condition with respect to P for some B > 0
and 0 < β ≤ 1 if  for all f ∈ F:

1 (X)(cid:1) = (cid:96)(cid:0)Y  f∗

P(cid:0)(cid:96)(·  f ) − (cid:96)(·  f∗)(cid:1)2 ≤ B(cid:0)P(cid:0)(cid:96)(·  f ) − (cid:96)(·  f∗)(cid:1)(cid:1)β

1080C log(2Kn) + 90

.

(12)

(cid:1)  

(cid:17)

δ

8 max

δ

δ

1
n

.

1 (X)(cid:1) = (cid:96)(cid:0)Y  f∗

It already is known that the stochastic mixability condition guarantees that there is a unique min-
imizer [21]; this is a simple consequence of Jensen’s inequality. This leaves open the question: if
stochastic mixability does not hold  are there necessarily non-unique minimizers? We show that in
a certain sense this is indeed the case  in bad way: the set of minimizers will be a disconnected set.

For any ε > 0  deﬁne Gε as the class Gε := {f∗} ∪(cid:8)f ∈ F : (cid:107)f − f∗(cid:107)L1(P) ≥ ε(cid:9)  where in case

not the case that (cid:96)(cid:0)Y  f∗

there are multiple minimizers in F we arbitrarily select one of them as f∗. Since we assume that F
is compact and Gε \ {f∗} is equal to F minus an open set homeomorphic to the unit L1(P) ball 
Gε \ {f∗} is also compact.
Theorem 8 (Non-Unique Minimizers). Suppose there exists some ε > 0 such that Gε is not
2 ∈ F of P (cid:96)(·  f ) over F such that it is
stochastically mixable. Then there are minimizers f∗
Proof. Select ε > 0 as in the theorem and some ﬁxed η > 0. Since Gε is not η-stochastically
mixable  there exists fη ∈ Gε such that Λ−Zfη
(η) > 0. Note that there exists η(cid:48) ∈ (0  η) with
(η)−Λ−Zfη
> 0 ⇒ Λ(cid:48)
(0) = E(−Zfη )
(0) > 0  so Λ(cid:48)
Λ−Zfη
−Zfη
implies that E Zfη < 0  a contradiction! From Lemma 2  E Zfη ≤ cosh(η(cid:48))−1
; for η(cid:48) ≥ 0 the RHS
(cid:16) η(cid:48)
sinh(η(cid:48))
has upper bound η(cid:48)
2 tanh2(η(cid:48)/2)
2 − cosh(η(cid:48))−1
and
sinh(η(cid:48))
a positive decreasing sequence (ηj)j approaching 0  corresponding to a sequence (fηj )j ⊂ Gε\{f∗}
with limit point g∗ ∈ Gε\{f∗} for which E Zg∗ = 0  and so there is a risk minimizer in Gε\{f∗}.

(cid:17)|η(cid:48)=0 = 0. Thus  E Zfη → 0 as η → 0. As Gε\{f∗} is compact  we can take

2 since the derivative of η(cid:48)

2 (X)(cid:1) a.s.

(η(cid:48)) = 0; if not  limη↓0

is the nonnegative function 1

2 − cosh(η(cid:48))−1
sinh(η(cid:48))

1   f∗

−Zfη

Λ−Zfη

(0)

η

The implications of having non-unique risk minimizers.
In the case of non-unique risk mini-
mizers  Mendelson [17] showed that for p-losses (y  ˆy) (cid:55)→ |y − ˆy|p with p ∈ [2 ∞) there is an
n-indexed sequence of probability measures (P(n))n approaching the true probability measure as
n → ∞ such that  for each n  ERM learns at a slow rate under sample size n when the true distri-
bution is P(n). This behavior is a consequence of the statistical learning problem’s poor geometry:
there are multiple minimizers and the set of minimizers is not even connected. Furthermore  in this
case  the best known fast rate upper bounds (see [18] and [19]) have a multiplicative constant that
approaches ∞ as the target probability measure approaches a probability measure for which there
are non-unique minimizers. The reason for the poor upper bounds in this case is that the constant B
in the Bernstein condition explodes  and the upper bounds rely upon the Bernstein condition.

6 Weak stochastic mixability
For some κ ∈ [0  1]  we say ((cid:96) F  P) is (κ  η0)-weakly stochastically mixable if  for every ε > 0  for
all f ∈ {f∗} ∪ F(cid:23)ε  the inequality log E exp(−ηεZf ) ≤ 0 holds with ηε := η0ε1−κ. This concept
was introduced by Van Erven et al. [21] without a name.

7

Suppose that some ﬁxed function has excess risk a = ε. Then  roughly  with high probability
ERM does not make a mistake provided that aηa = 1
n and hence when
ε = (η0n)−1/(2−κ). Modifying the proof of the ﬁnite classes result (Theorem 5) to consider all
functions in the subclass F(cid:23)γn for γn = (η0n)−1/(2−κ) yields the following corollary of Theorem 5.
Corollary 9. Let ((cid:96) F  P) be (κ  η0)-weakly stochastically mixable for some κ ∈ [0  1]  where
n ≥ 1

|F| = N  (cid:96) is a nonnegative loss  and supf∈F (cid:96)(cid:0)Y  f (X)(cid:1) ≤ V a.s. for a constant V . Then for any

n  i.e. when ε · η0ε1−κ = 1

η0

V (1−κ)/(2−κ)  with probability at least 1 − δ
P (cid:96)(·  ˆfz) ≤ P (cid:96)(·  f∗) +

6(cid:0)log 1

δ + log N(cid:1)

(η0n)1/(2−κ)

.

It is simple to show a similar result for VC-type classes; the ε-net can still be taken at the resolution
n  but we need only apply the analysis to the subclass of F with excess risk at least (η0n)−1/(2−κ).
1

7 Discussion

We have shown that stochastic mixability implies fast rates for VC-type classes  using a direct argu-
ment based on the Cram´er-Chernoff method and sufﬁcient control of the optimal value of a certain
instance of the general moment problem. The approach is amenable to localization in that the analy-
sis separately controls the probability of large deviations for individual elements of F. An important
open problem is to extend the results presented here for VC-type classes to results for nonparametric
classes with polynomial metric entropy  and moreover  to achieve rates similar to those obtained for
these classes under the Bernstein condition.
There are still some unanswered questions with regards to the connection between the Bernstein
condition and stochastic mixability. Van Erven et al. [21] showed that for bounded losses the Bern-
stein condition implies stochastic mixability. Therefore  when starting from a Bernstein condition 
Theorem 5 offers a different path to fast rates. An open problem is to settle the question of whether
the Bernstein condition and stochastic mixability are equivalent. Previous results [21] suggest that
the stochastic mixability does imply a Bernstein condition  but the proof was non-constructive  and
it relied upon a bounded losses assumption. It is well known (and easy to see) that both stochastic
mixability and the Bernstein condition hold only if there is a unique minimizer. Theorem 8 shows in
a certain sense that if stochastic mixability does not hold  then there cannot be a unique minimizer.
Is the same true when the Bernstein condition fails to hold? Regardless of whether stochastic mixa-
bility is equivalent to the Bernstein condition  the direct argument presented here and the connection
to classical mixability  which does characterize constant regret in the simpler non-stochastic setting 
motivates further study of stochastic mixability.
Finally  it would be of great interest to discard the bounded losses assumption. Ignoring the depen-
dence of the metric entropy on the maximum possible loss  the upper bound on the loss V enters the
ﬁnal bound through the difﬁculty of controlling the minimum value of uη(−1) when η is large (see
the proof of Theorem 3). From extensive experiments with a grid-approximation linear program 
we have observed that the worst (CGF-wise) random variables for ﬁxed negative mean and ﬁxed
optimal stochastic mixability constant are those which place very little probability mass at −V and
most of the probability mass at a small positive number that scales with the mean. These random
variables correspond to functions that with low probability beat f∗ by a large (loss) margin but with
high probability have slightly higher loss than f∗. It would be useful to understand if this exotic
behavior is a real concern and  if not  ﬁnd a simple  mild condition on the moments that rules it out.

Acknowledgments

RCW thanks Tim van Erven for the initial discussions around the Cram´er-Chernoff method during
his visit to Canberra in 2013 and for his gracious permission to proceed with the present paper
without him as an author  and both authors thank him for the further enormously helpful spotting
of a serious error in our original proof for fast rates for VC-type classes. This work was supported
by the Australian Research Council (NAM and RCW) and NICTA (RCW). NICTA is funded by the
Australian Government through the Department of Communications and the Australian Research
Council through the ICT Centre of Excellence program.

8

References

[1] Jacob Abernethy  Alekh Agarwal  Peter L. Bartlett  and Alexander Rakhlin. A stochastic view of optimal
In Proceedings of the 22nd Annual Conference on Learning Theory

regret through minimax duality.
(COLT 2009)  2009.

[2] Jean-Yves Audibert. Fast learning rates in statistical inference through aggregation. The Annals of Statis-

tics  37(4):1591–1646  2009.

[3] Peter L. Bartlett  Olivier Bousquet  and Shahar Mendelson. Local Rademacher complexities. The Annals

of Statistics  33(4):1497–1537  2005.

[4] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory and Related Fields 

135(3):311–334  2006.

[5] St´ephane Boucheron  G´abor Lugosi  and Pascal Massart. Concentration inequalities: A nonasymptotic

theory of independence. Oxford University Press  2013.

[6] Peter Gr¨unwald. Safe learning: bridging the gap between Bayes  MDL and statistical learning theory via
empirical convexity. In Proceedings of the 24th International Conference on Learning Theory (COLT
2011)  pages 397–419  2011.

[7] Peter Gr¨unwald. The safe Bayesian. In Proceedings of the 23rd International Conference on Algorithmic

Learning Theory (ALT 2012)  pages 169–183. Springer  2012.

[8] Yuri Kalnishkan and Michael V. Vyugin. The weak aggregating algorithm and weak mixability.

In
Proceedings of the 18th Annual Conference on Learning Theory (COLT 2005)  pages 188–203. Springer 
2005.

[9] Johannes H.B. Kemperman. The general moment problem  a geometric approach. The Annals of Mathe-

matical Statistics  39(1):93–122  1968.

[10] Vladimir Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The

Annals of Statistics  34(6):2593–2656  2006.

[11] Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Prob-

lems: Ecole dEt´e de Probabilit´es de Saint-Flour XXXVIII-2008  volume 2033. Springer  2011.

[12] Guillaume Lecu´e.

Interplay between concentration  complexity and geometry in learning theory with
applications to high dimensional data analysis. Habilitation `a diriger des recherches  Universit´e Paris-
Est  2011.

[13] Wee Sun Lee  Peter L. Bartlett  and Robert C. Williamson. The importance of convexity in learning with

squared loss. IEEE Transactions on Information Theory  44(5):1974–1980  1998.

[14] Jonathan Qiang Li. Estimation of mixture models. PhD thesis  Yale University  1999.
[15] Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. The Annals of Statistics 

27(6):1808–1829  1999.

[16] Pascal Massart and ´Elodie N´ed´elec. Risk bounds for statistical learning. The Annals of Statistics 

34(5):2326–2366  2006.

[17] Shahar Mendelson. Lower bounds for the empirical minimization algorithm.

Information Theory  54(8):3797–3803  2008.

IEEE Transactions on

[18] Shahar Mendelson. Obtaining fast error rates in nonconvex situations. Journal of Complexity  24(3):380–

397  2008.

[19] Shahar Mendelson and Robert C. Williamson. Agnostic learning nonconvex function classes. In Pro-
ceedings of the 15th Annual Conference on Computational Learning Theory (COLT 2002)  pages 1–13.
Springer  2002.

[20] Alexander B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics 

32(1):135–166  2004.

[21] Tim Van Erven  Peter D. Gr¨unwald  Mark D. Reid  and Robert C. Williamson. Mixability in statistical
In Advances in Neural Information Processing Systems 25 (NIPS 2012)  pages 1700–1708 

learning.
2012.

[22] Mathukumalli Vidyasagar. Learning and Generalization with Applications to Neural Networks. Springer 

2002.

[23] Volodya Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences 

56(2):153–173  1998.

[24] Volodya Vovk. Competitive on-line statistics. International Statistical Review  69(2):213–248  2001.

9

,Yann Dauphin
Yoshua Bengio
Nishant Mehta
Robert Williamson
Pascal Vincent
Alexandre de Brébisson
Xavier Bouthillier
Taylor Mordan
Nicolas THOME
Gilles Henaff
Matthieu Cord