2019,A Primal-Dual link between GANs and Autoencoders,Since the introduction of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAE)  the literature on generative modelling has witnessed an overwhelming resurgence. The impressive  yet elusive empirical performance of GANs has lead to the rise of many GAN-VAE hybrids  with the hopes of GAN level performance and additional benefits of VAE  such as an encoder for feature reduction  which is not offered by GANs. Recently  the Wasserstein Autoencoder (WAE) was proposed  achieving performance similar to that of GANs  yet it is still unclear whether the two are fundamentally different or can be further improved into a unified model. In this work  we study the $f$-GAN and WAE models and make two main discoveries. First  we find that the $f$-GAN and WAE objectives partake in a primal-dual relationship and are equivalent under some assumptions  which then allows us to explicate the success of WAE. Second  the equivalence result allows us to  for the first time  prove generalization bounds for Autoencoder models  which is a pertinent problem when it comes to theoretical analyses of generative models. Furthermore  we show that the WAE objective is related to other statistical quantities such as the $f$-divergence and in particular  upper bounded by the Wasserstein distance  which then allows us to tap into existing efficient (regularized) optimal transport solvers. Our findings thus present the first primal-dual relationship between GANs and Autoencoder models  comment on generalization abilities and make a step towards unifying these models.,A Primal-Dual link between GANs and Autoencoders

Hisham Husain‡ † Richard Nock† ‡ ♣ Robert C. Williamson‡ †
‡The Australian National University  †Data61  ♣The University of Sydney

firstname.lastname@{data61.csiro.au anu.edu.au}

Abstract

Since the introduction of Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAE)  the literature on generative modelling has witnessed an
overwhelming resurgence. The impressive  yet elusive empirical performance of
GANs has lead to the rise of many GAN-VAE hybrids  with the hopes of GAN
level performance and additional beneﬁts of VAE  such as an encoder for feature
reduction  which is not offered by GANs. Recently  the Wasserstein Autoencoder
(WAE) was proposed  achieving performance similar to that of GANs  yet it is still
unclear whether the two are fundamentally different or can be further improved
into a uniﬁed model. In this work  we study the f-GAN and WAE models and
make two main discoveries. First  we ﬁnd that the f-GAN and WAE objectives
partake in a primal-dual relationship and are equivalent under some assumptions 
which then allows us to explicate the success of WAE. Second  the equivalence
result allows us to  for the ﬁrst time  prove generalization bounds for Autoencoder
models  which is a pertinent problem when it comes to theoretical analyses of
generative models. Furthermore  we show that the WAE objective is related
to other statistical quantities such as the f-divergence and in particular  upper
bounded by the Wasserstein distance  which then allows us to tap into existing
efﬁcient (regularized) optimal transport solvers. Our ﬁndings thus present the ﬁrst
primal-dual relationship between GANs and Autoencoder models  comment on
generalization abilities and make a step towards unifying these models.

1

Introduction

Implicit probabilistic models [1] are deﬁned to be the pushforward of a simple distribution PZ
over a latent space Z through a map G : Z → X  where X is the space of the input data. Such
models allow easy sampling  but the computation of the corresponding probability density function is
intractable. The goal of these methods is to match G#PZ to a target distribution PX by minimizing
D(PX   G#PZ)  for some discrepancy D(· ·) between distributions. An overwhelming number
of methods have emerged after the introduction of Generative Adversarial Networks [2  3] and
Variational Autoencoders [4] (GANs and VAEs)  which have established two distinct paradigms:
Adversarial (networks) training and Autoencoders respectively. Adversarial training involves a set of
functions D  referred to as discriminators  with an objective of the form

D(PX   G#PZ) = max
d∈D

(1)
for some functions a : R → R and b : R → R. Autoencoder methods are concerned with ﬁnding a
function E : X → Z  referred to as an encoder  whose goal is to reverse G  and learn a feature space
with the objective

{Ex∼PX [a(d(x))] − Ex∼G#PZ [b(d(x))]}  

D(PX   G#PZ) = min

E

{R(G  E) + Ω(E)}  

(2)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

where R(G  E) is the reconstruction loss and acts to ensure G and E reverse each other and Ω(E) is
a regularization term. Much work on Autoencoder methods has focused upon the choice of Ω.
In practice  the two methods demonstrate contrasting abilities in their strengths and limitations  which
have resulted in differing directions of progress. Indeed  there is a lack of theoretical understanding
of how these frameworks are parametrized and it is not clear whether the methods are fundamentally
different. For example  Adversarial training based methods have empirically demonstrated high
performance when it comes to producing realistic looking samples from PX. However  GANs often
have problems in convergence and stability of training [5]. Autoencoders  on the other hand  deal
with a more well behaved objective and learn an encoder in the process  making them useful for
feature representation. However in practice  Autoencoder based methods have reported shortfalls 
such as producing blurry samples for image based datasets [6]. This has motivated researchers
to adapt Autoencoder models by borrowing elements from Adversarial networks in the hopes of
GAN level performance whilst learning an encoder. Examples include replacing Ω with Adversarial
objectives [7  8] or replacing the reconstruction loss with an adversarial objective [9  10]. Recently 
the Wasserstein Autoencoder (WAE) [6] has been shown to subsume these two methods with an
Adversarial based Ω  and has demonstrated performance similar to that of Adversarial methods.
Understanding the connection between the two paradigms is important for not only the practical
purposes outlined above but for the inheritance of theoretical analyses from one another. For example 
when it comes to directions of progress  Adversarial training methods now have theoretical guarantees
on generalization performance [11]  however no such theoretical results have been obtained to date
for autoencoders. Indeed  generalization performance is a pressing concern  since both techniques
implicitly assume the samples represent the target distribution [12] and eventually leads to memorizing
training data.
In this work  we study the two paradigms and in particular focus on the f-GANs [3] for Adversarial
training and Wasserstein Autoencoders (WAE) for Autoencoders  which generalize the original GAN
and VAE models respectively. We prove that the f-GAN objective with Lipschitz (with respect to a
metric c) discriminators is equivalent to the WAE objective with cost c. In particular  we show that
the WAE objective is an upper bound; schematically we get
f-GAN ≤ WAE

and discuss the tightness of this bound. Our result is a generalization of the Kantorovich-Rubinstein
duality and thus suggests a primal-dual relationship between Adversarial and Autoencoder methods.
Consequently we show  to the best of our knowledge  the ﬁrst generalization bounds for autoencoders.
Furthermore  using this equivalence  we show that the WAE objective is related to key statistical
quantities such as the f-divergence and Wasserstein distance  which allows us to tap into efﬁcient
(regularized) OT solvers.
The main contributions can be summarized as the following:
(cid:46) (Theorem 8) Establishes an equivalence between Adversarial training and Wasserstein Autoencoders 
showing conditions under which the f-GAN and WAE coincide. This further justiﬁes the similar
performance of WAE to GAN based methods. When the conditions are not met  we have an inequality 
which allows us to comment on the behavior of the methods.
(cid:46) (Theorem 9  10 and 14) Show that the WAE objective is related to other statistical quantities such
as f-divergence and Wasserstein distance.
(cid:46) (Theorem 13) Provide generalization bounds for WAE. In particular  this focuses on the empirical
variant of the WAE objective  which allows the use of Optimal Transport (OT) solvers as they are
concerned with discrete distributions. This allows one to employ efﬁcient (regularized) OT solvers
for the estimation of WAE  f-GANs and the generalization bounds.

2 Preliminaries

2.1 Notation

We will use X to denote the input space (a Polish space)  typically taken to be a Euclidean space. We
use Z to denote the latent space  also taken to be Euclidean. We use N∗ to denote the natural numbers
without 0: N \ {0}. We denote by P the set of probability measures over X  and elements of this set

2

will be referred to as distributions. If P ∈ P(X) happens to be absolutely continuous with respect
to the Lebesgue measure then we will use dP/dλ to refer to the density function (Radon-Nikodym
derivative with respect to the Lebesgue measure). For any T ∈ F (X  Z)  for any measure µ ∈ P(X) 
the pushforward measure of µ through T denoted T #µ ∈ P(Z) is such that T #µ(A) = µ(T −1(A))
for any measurable set A ⊂ Z. The set F (X  R) refers to all measurable functions from X into the set
R. We will use functions to represent conditional distributions over a space Z conditioned on elements
X  for example P ∈ F (X  P(Z)) so that for any x ∈ X  P (x) = P (·|x) ∈ P(Z). For any P ∈
P(X)  the support of P is supp(P ) = {x ∈ X : if x ∈ Nx open =⇒ P (Nx) > 0}. In any metric
space (X  c)  for any set S ⊆ X  we deﬁne the diameter of S to be diamc(S) = supx x(cid:48)∈S c(x  x(cid:48)).
Given a metric c over X  for any f ∈ F (X  R)  Lipc(f ) denotes the Lipschitz constant of f with
respect to c and Hc = {g ∈ F (X  R) : Lipc(g) ≤ 1}. For some set S ⊆ R  1S corresponds to the
convex indicator function  ie. 1S(x) = 0 if x ∈ S and 1S(x) = ∞ otherwise. For any x ∈ X 
δx : X → {0  1} corresponds to the characteristic function  with δx(0) = 1 if x = 0 and δx(0) = 0
if x (cid:54)= 0.

2.2 Background

2.2.1 Probability Discrepancies

Probability discrepancies are central to the objective of ﬁnding the best ﬁtting model. We introduce
some key discrepancies and their notation  which will appear later.
Deﬁnition 1 (f-Divergence) For a convex function f : R → (−∞ ∞] with f (1) = 0  for any
P  Q ∈ P(X) with P absolutely continuous with respect to Q  the f-Divergence between P and Q is

(cid:90)

(cid:18) dP

(cid:19)

dQ

with Df (P  Q) = ∞ if P is note absolutely continuous with respect to Q.

Df (P  Q) :=

f

X

dQ 

An example of a method to compute the f-divergence is to ﬁrst compute dP/dQ and estimate the
integral empirically using samples from Q.
Deﬁnition 2 (Integral Probability Metric) For a ﬁxed function class F ⊆ F (X  R)  the Integral
Probability Metric (IPM) based on F between P  Q ∈ P(X) is deﬁned as

IPMF(P  Q) := sup
f∈F

f (x)dP (x) −

X

X

f (x)dQ(x)

.

If we have that −F = F then IPMF forms a metric over P(X) [13]. A particular IPM we will make
use of is Total Variation (TV): TV(P  Q) = IPMV(P  Q) where V = {h ∈ F (X  R) : |h| ≤ 1}. We
also note that when f (x) = |x − 1| then TV = Df and thus TV is both an IPM and an f-divergence.
Deﬁnition 3 For any P  Q ∈ P(X)  deﬁne the set of couplings between P and Q to be

Π(P  Q) =

π ∈ P(X × X) :

π(x  y)dx = P 

π(x  y)dy = Q

.

For a cost c : X × X → R+  the Wasserstein distance between P and Q is

(cid:26)

(cid:90)

X

(cid:26)(cid:90)

(cid:27)

(cid:90)

X

(cid:27)

Wc(P  Q) := inf

π∈Π(P Q)

X×X

c(x  y)dπ(x  y)

.

The Wasserstein distance can be regarded as an inﬁnite linear program and thus admits a dual form 
and in the case of c being a metric  belongs to the class of IPMs. We summarize this fact the following
lemma [14].

Lemma 4 (Wasserstein Duality) Let (X  c) be a metric space  and suppose Hc is the set of all
1-Lipschitz functions with respect to c. Then for any P  Q ∈ P(X)  we have

Wc(P  Q) = sup
h∈Hc

h(x)dP (x) −

X

X

h(x)dQ(x)

(cid:90)

(cid:90)

(cid:27)

(cid:27)

(cid:26)(cid:90)

(cid:26)(cid:90)

= IPMHc(P  Q).

3

2.3 Generative Models
In both GAN and VAE models  we have a latent space Z (typically taken to be Rd  with d being
small) and a prior distribution PZ ∈ P(Z) (e.g. unit variance Gaussian). We have a function referred
to as the generator G : Z → X  which induces the generated distribution  denoted by PG ∈ P(X) 
as the pushforward of PZ through G: PG = G#PZ. The true data distribution will be referred to
as PX ∈ P(X). The common goal between the two methods is to ﬁnd a generator G such that the
samples generated by pushing forward PZ through G (G#PZ) are close to the true data distribution
(PX). More formally  one can cast this as an optimization problem by ﬁnding the best G such that
D(PG  PX ) is minimized  where D(· ·) is some discrepancy between distributions. Both methods
(as we outline below) utilize their own discrepancies between PX and PG  which offer their own
beneﬁts and weaknesses.

2.3.1 Wasserstein Autoencoder
Let E : X → P(Z) denote a probabilistic encoder 1  which maps each point x to a conditional
distribution E(x) ∈ P(Z)  denoted as the posterior distribution. The pushforward of PX through E:
E#PX  will be referred to as the aggregated posterior.
Deﬁnition 5 (Wasserstein Autoencoder [6]) Let c : X × X → R≥0  λ > 0 and Ω : P(Z) ×
P(Z) → R≥0 with Ω(P  P ) = 0 for all P ∈ P(Z). The Wasserstein Autoencoder objective is
Ez∼E(x)[c(x  G(z))]dPX (x) + λ · Ω(E#PX   PZ)

WAEc λ·Ω(PX   G) =

(cid:26)(cid:90)

(cid:27)

.

inf

E∈F (X P(Z))

X

We remark that there are various choices of c and λ · Ω. [6] select these by tuning λ and selecting
different measures of discrepancies between probability distortions for Ω.

f-Generative Adversarial Network

2.3.2
Let d : X → R denote a discriminator function.
Deﬁnition 6 (f-GAN [3]) Let f : R → (−∞ ∞] denote a convex function with property f (1) = 0
and D ⊂ F (X  R) a set of discriminators. The f-GAN model minimizes the following objective for
a generator G : Z → X

GANf (PX   G; D) := sup
d∈D

{Ex∼PX [d(x)] − Ez∼PZ [f∗(d(G(z)))]}  

(3)

where f (cid:63)(x) = supy {x · y − f (y)} is the convex conjugate of f.

There are two knobs in this method  namely D  the set of discriminators  and the convex func-
tion f. The objective in (3) is a variational approximation to Df [3]; if D = F (X  R)  then
GANf (PX   G; D) = Df (PX   PG) [15]. In the case of f (x) = x log(x)−(x+1) log(x+1)+2 log 2 
we recover the original GAN [2].

3 Related Work

Current attempts at building a taxonomy for generative models have largely been within each paradigm
or the proposal of hybrid methods that borrow elements from the two. We ﬁrst review major and
relevant advances in each paradigm  and then move on to discuss results that are close to the technical
contributions of our work.
The line of Autoencoders begin with Ω = 0  which is the original autoencoder concerned only with
reconstruction loss. VAE then introduced a non-zero Ω  along with implementing Gaussian encoders
[4]. This was then replaced by an adversarial objective [7]  which is sample based and consequently
allows arbitrary encoders. In the spirit of uniﬁcation  Adversarial Autoencoders (AAE) [8] proposed
Ω to be a discrepancy between the pushforward of the target distribution through the encoder (E#PX)

1We remark that this is not standard notation in the VAE and Variational Inference literature.

4

and the prior distribution (PZ) in the latent space  which was then showed to be equivalent to the VAE
Ω minus a mutual information term [16]. Independently  InfoVAE [17] proposed a similar objective 
which was subsequently shown to be equivalent to adding mutual information. [6] reparametrized the
Wasserstein distance into an Autoencoder objective (WAE) where the Ω term generalizes AAE  and
has reported performance comparable to that of Adversarial methods. Other attempts also include
adjusting the reconstruction loss to be adversarial as well [9  10]. Another work that focuses on WAE
is the Sinkhorn Autoencoders (SAE) [18]  which select Ω to be the Wasserstein distance and show
that the overall objective is an upper bound to the Wasserstein distance between PX and PG.
[19] discussed the two paradigms and their uniﬁcation by interpretting GANs from the perspective
of variational inference  which allowed a connection to VAE  resulting in a GAN implemented
with importance weighting techniques. While this approach is the closest to our work in forming a
link  their results apply to standard VAE (and not other AE methods such as WAE) and cannot be
extended to all f-GANs. [20] introduced the notion of an Adversarial divergence  which subsumed
mainstream adversarial based methods. This also led to the formal understanding of how the
selected discriminator set D affects the ﬁnal generator G learned. However  this approach is silent
with regard to Autoencoder based methods. [11] established the tradeoff between the Rademacher
complexity of the discriminator class D and generalization performance of G  with no results present
for Autoencoders. These theoretical advances in Adversarial training methods are inherited by
Autoencoders as a consequence of the equivalence presented in our work.
One key point in the proof of our equivalence is the use of a result that decomposes the GAN
objective into an f-divergence and an IPM for a restricted class of discriminators (which we used for
Lipschitz functions). This decomposition is used in [21] and applied to linear f-GANs  showing that
the adversarial training objective decomposes into a mixture of maximum likelihood and moment
matching. [22] used this decomposition with Lipschitz discriminators like our work  however does
not make any extension or further progress to establish the link to WAE. Indeed  GANs with Lipschitz
discriminators have been independently studied in [23]  which suggest that one should enforce
Lipschitz constraints to provide useful gradients.

4

f-Wasserstein Autoencoders

We deﬁne a new objective  that will help us in the proof of the main theorems of this paper.
Deﬁnition 7 (f-Wasserstein Autoencoder) Let c : X × X → R  λ > 0  f : R → (−∞ ∞] be a
convex function (with f (1) = 0)  the f-Wasserstein Autoencoder (f-WAE) objective is

W c λ·f (PX   G) =

inf

E∈F (X P(Z))

{Wc(PX   (G ◦ E)#PX ) + λDf (E#PX   PZ)}

(4)

In the proof of the main result  we will show that the f-WAE objective is indeed the same as the
WAE objective when using the same cost c and selecting the regularizer to be λ · Ω = Dλf = λDf .
The only difference between this and the standard WAE is the use of Wc(PX   (G ◦ E)#PX ) as
the reconstruction loss instead of the standard cost which is an upper bound (Lemma 18)  and the
regularizer is chosen to be λ · Ω = Dλf = λDf . We now present the main theorem that captures the
relationship between f-GAN and WAE.

Theorem 8 (f-GAN and WAE equivalence) Suppose (X  c) is a metric space and let Hc denote
the set of all functions from X → R that are 1-Lipschitz (with respect to c). Let f : R → (−∞ ∞]
be a convex function with f (1) = 0. Then for all λ > 0 

GANλf (PX   G; Hc) ≤ WAEc λ·Df (PX   G) 

with equality if G is invertible.

(5)

(6)

(This is a sketch  see Section A.1 for full proof). The proof begins by proving certain

Proof
properties of Hc (Lemma 16)  allowing us to use the dual form of restricted GANs (Theorem 15) 

(cid:27)
{EPX [h] − EP (cid:48)[h]}

GANf (PX   G; Hc) = inf

P (cid:48)∈P(X)

= inf

P (cid:48)∈P(X)

Df (P (cid:48)  PG) + sup
h∈Hc
{Df (P (cid:48)  PG) + Wc(P (cid:48)  PX )} .

(cid:26)

5

GANλf (PX   G; Hc)

inf

E∈F (X P(Z))
= W c λ·f (PX   G)
≤

(cid:90)
where the ﬁnal inequality follows from the fact that Wc(P  Q) ≤(cid:82)

= WAEc λ·Df (PX   G) 

λDf (E#PX   PZ) +

E∈F (X P(Z))

(cid:26)

inf

X

(cid:27)

Ez∼E(x)[c(x  G(z))]dPX (x)

The key is to reparametrize (6) by optimizing over couplings. By rewriting P (cid:48) = (G ◦ E)#PX for
some E ∈ F (X  P(Z)) and rewriting (6) as an optimization over E (Lemma 20)  we obtain

{Df (P (cid:48)  PG) + Wc(P (cid:48)  PX )}

inf

P (cid:48)∈P(X)
=

inf

E∈F (X P(Z))

{Df ((G ◦ E)#PX   PG) + Wc((G ◦ E)#PX   PX )}

(7)

We then have

Df ((G ◦ E)#PX   PG) = Df (G#(E#PX )  G#PZ)

(∗)≤ Df (E#PX   PZ) 

with equality in (∗) if G is invertible (Lemma 17). A weaker condition is required if f is differentiable 
namely if G is invertible with respect to f(cid:48) ◦ d(E#PX )/dPZ in the sense that

G(z) = G(z(cid:48)) =⇒ f(cid:48) ◦ (d(E#PX )/dPZ)(z) = f(cid:48) ◦ (d(E#PX )/dPZ)(z(cid:48)) 

(8)
noting that an invertible G trivially satisﬁes this requirement. Letting f ← λf  we have Df (· ·) ←
λDf (· ·)  and so from Equation 7  we have

(∗)≤

{λDf (E#PX   PZ) + Wc((G ◦ E)#PX   PX )}

Ez∼E(x)[c(x  G(z))]dPX (x)

X

(Lemma 18). Using the fact that W ≥ WAE (Lemma 19) completes the proof.
When G is invertible  we remark that PG can still be expressive and capable of modelling complex
distributions in WAE and GAN models. For example  if G is implemented with feedforward
neural networks  and G is invertible then PG can model deformed exponential families [24]  which
encompasses a large class appearing in statistical physics and information geometry [25  26]. There
exists many invertible activation functions under which G will be invertible. Furthermore  in the proof
of the Theorem it is clear that W and WAE are the same objective (from Lemma 18 and Lemma 19).
When using f = 1{1} (f (x) = 0 if x = 1 and f (x) = ∞ otherwise)  and noting that f (cid:63)(x) = x 
meaning that Theorem 8 (with λ = 1) reduces to

sup
h∈Hc

{Ex∼PX [h(x)] − Ex∼PG[h(x)]} = GANf (PX   G; Hc)

≤ W c f (PX   PG)
inf
=

E∈F (X P(Z)):E#PX =PZ

=

E∈F (X P(Z)):E#PX =PZ

inf

= Wc(PX   PG) 

{Wc(PX   (G ◦ E)#PX )}
{Wc(PX   G#PZ}

which is the standard primal-dual relation between Wasserstein distances as in Lemma 4. Hence 
Theorem 8 can be viewed as a generalization of this primal-dual relationship  where Autoencoder
and Adversarial objectives represent primal and dual forms respectively.
We note that the left hand of Equation (5) does not explicitly engage the prior space Z as much as
the right hand side in the sense that one can set Z = X  G = Id (which is invertible) and PZ = PG
and indeed results in the exact same f-GAN objective since G#PZ = Id#PG = PG  yet the
equivalent f-WAE objective (from Theorem 8) will be different. This makes the Theorem versatile
in reparametrizations  which we exploit in the proof of Theorem 10. We now consider weighting
the reconstruction along with the regularization term in W (which is equivalent to weighting WAE) 
which simply amounts to re-weighting the cost since for any γ > 0 

W γ·c λ·f (PX   G) =

inf

E∈F (X P(Z))

{γWc((G ◦ E)#PX   PX ) + λDf (E#PX   PZ)} .

6

The idea of weighting the regularization term by λ was introduced by [27] and furthermore studied
empirically  showing that the choice of λ inﬂuences learning disentanglement in the latent space.
[28]. We show that if λ = 1 and γ is larger than some γ∗ then W will become an f-divergence
(Theorem 9). On the other hand if we ﬁx γ = 1 and take λ is larger than some λ∗  then W becomes
the Wasserstein distance and in particular  equality holds in (5) (Theorem 10). We show explicitly
how high γ and λ need to be for such equalities to occur. This is surprising since f-divergence and
Wasserstein distance are quite different distortions.
We begin with the f-divergence case. Consider f : R → (−∞ ∞] convex  differentiable and
f (1) = 0 and assume that PX is absolutely continuous with respect to PG  so that Df (PX   PG) < ∞.
Theorem 9 Set c(x  y) = δx−y and let f : R → (−∞ ∞] be a convex function (with f (1) = 0)
and differentiable. Let γ∗ = supx x(cid:48)∈X
continuous with respect to PX and that G is invertible  then we have for all γ ≥ γ∗

(cid:12)(cid:12)(cid:12) and suppose PG is absolutely

(cid:17) − f(cid:48)( dPX

(cid:12)(cid:12)(cid:12)f(cid:48)(cid:16) dPX

dPG

)(x(cid:48))

dPG

W γ·c f (PX   G) = Df (PX   PG).

(Proof in Appendix  Section A.3). It is important to note that Wc(PX   PG) = TV(PX   PG) when
c(x  y) = δx−y and so Theorem 9 tells us that the objective with a weighted total variation reconstruc-
tion loss with a f-divergence prior regularization amounts to the f-divergence. It was shown that in
[24] that when G is an invertible feedforward neural network then Df (PX   PG) is a Bregman diver-
gence (a well regarded quantity in information geometry) between the parametrizations of the network
for a ﬁxed choice of activation function of G  which depends on f. Hence  a practioner should design
G with such activation function when using f-WAE under the above setting (c(x  y) = δx−y and
γ = γ∗) with G being invertible  so that the information theoretic divergence (Df ) between the
distributions becomes an information geometric divergence involving the network parameters.
We now show that if λ is selected higher than λ∗ := supP (cid:48)∈P(X) (Wc(P (cid:48)  PG)/Df (P (cid:48)  PG))  then
W becomes Wc and furthermore we have equality between f-GAN  f-WAE and WAE.
Theorem 10 Let c : X × X → R be a metric. For any f : R → (−∞ ∞] convex function (with
f (1) = 0)  we have for all λ ≥ λ∗

GANλf (PX   G; Hc) = W c λ·f (PX   G) = WAEc λ·Df (PX   G) = Wc(PX   PG).

(Proof in Appendix  Section A.4). Note that Theorem 10 holds for any f (satisfying properties of
the Theorem) and so one can estimate the Wasserstein distance using any f as long as λ is scaled
to λ∗. In order to understand how high λ∗ can be there are two extremes in which the supremum
may be unbounded. The ﬁrst case is when P (cid:48) is taken far from PG so that Wc(P (cid:48)  PG) increases 
however one should note that in the case when ∆ = maxx x(cid:48)∈X c(x  x(cid:48)) < ∞ then Wc ∈ [0  ∆] and
so Wc will be ﬁnite whereas Df (P (cid:48)  PG) can possibly diverge to ∞  making λ∗ → 0. The other
Df (P (cid:48) PG) → ∞ however Wc(P (cid:48)  PG) → 0
case is when P (cid:48) is made close to PG  in which case
so the quantity λ∗ can still be small in this case  depending on the rate of decrease between Wc
and Df . Now suppose that f (x) = |x − 1| and c(x  y) = δx−y  in which case Df = Wc and thus
λ∗ = 1. In this case  Theorem 10 reduces to the standard result [29] regarding the equivalence
between Wasserstein distance and f-divergence intersecting at the variational divergence under these
conditions.

1

5 Generalization bounds

We present generalization bounds using machinery developed in [30] with the following deﬁnitions.
Deﬁnition 11 (Covering Numbers) For a set S ⊆ X  we denote Nη(S) to be the η-covering number
of S  which is the smallest m ∈ N∗ such that there exists closed balls B1  . . .   Bm of radius η
  where
Nη(P  τ ) := inf {Nη(S) : P (S) ≥ 1 − τ} .

i=1 Bi. For any P ∈ P(X)  the (η  τ )-dimension is dη(P  τ ) := log Nη(P τ )
− log η

with S ⊆ (cid:83)m

7

Deﬁnition 12 (1-Upper Wasserstein Dimension) The 1-Upper Wasserstein dimension of any P ∈

P(X) is d∗(P ) := inf(cid:8)s ∈ (2 ∞) : lim supη→0 dη(P  η

s−2 ) ≤ s(cid:9) .

s

We make an assumption of PX and PG having bounded support to achieve the following bounds. For
any P ∈ P(X) in a metric space (X  c)  we use deﬁne ∆P c = diamc(supp(P )).
Theorem 13 Let (X  c) be a metric space and suppose ∆ := max{∆c PX   ∆c PG} < ∞. For any
n ∈ N∗  let ˆPX and ˆPG denote the empirical distribution with n samples drawn i.i.d from PX and
PG respectively. Let sX > d∗(PX ) and sG > d∗(PG). For all f : R → (−∞ ∞] convex functions 
f (1) = 0  λ > 0 and δ ∈ (0  1)  then with probability at least 1 − δ  we have

(cid:32)

(cid:115)

GANλf (PX   G; Hc) ≤ W c λ·f ( ˆPX   PG) + O

n−1/sX + ∆

and if f (x) = |x − 1| is chosen then

GANλf (PX   G; Hc) ≤ W c λ·f ( ˆPX   ˆPG) + O

(cid:32)

n−1/sX + n−1/sG + ∆

 

(cid:19)(cid:33)
(cid:19)(cid:33)

(cid:18) 1
(cid:18) 1

δ

1
n

ln

(cid:115)

1
n

ln

δ

(9)

.

(10)

(cid:111)

(cid:110)

(Proof in Appendix  Section A.2). First note that there is no requirement on G to be invertible and no
restriction on λ. Second  there are the quantities sX sG and ∆ that are inﬂuenced by the distributions
PX and PG. It is interesting to note that d∗ is related to fractal dimensions [31] and thus relates the
convergence of GANs to statistical geometry. If G is invertible in the above then the left hand side
of both bounds becomes W c λ·f (PX   G) by Theorem 8. In general  ˆPX and ˆPG will not share the
same support  in which case Df ( ˆPX   ˆPG) = ∞ – This would lead one to suspect the same from
W c λ·f ( ˆPX   ˆPG)  however this is not the case since

W c λ·f ( ˆPX   ˆPG) ≤

inf

E∈F (X P(X))

Wc((G ◦ E)#PX   PX ) + λDf (E# ˆPX   ˆPZ)

 

and so E ∈ F (X  P(Z)) would be selected such that E# ˆPX shares the support of ˆPZ  resulting in
a bounded value. We now show the relationship between W and Wc.
Theorem 14 For any c : X×X → R  λ > 0 and f : R → (−∞ ∞] convex function (with f (1) = 0)
we have W c λ·f (PX   G) ≤ Wc(PX   PG)

(Proof in Appendix  Section A.5). This suggests that in order to minimize W   one can minimize Wc.
Indeed  majority of the solvers are concerned with discrete distributions  which is exactly what is
present on the right hand side of the generalization bounds: W c λ·f ( ˆPX   ˆPG)

6 Discussion and Conclusion

This work is the ﬁrst to prove a generalized primal-dual betweenship between GANs and Autoen-
coders. Our result elucidated the close performance between WAE and f-GANs. Furthermore  we
explored the effect of weighting the reconstruction and regularization on the WAE objective  showing
relationships to both f-divergences and Wasserstein metrics along with the impact on the duality
relationship. This equivalence allowed us to prove generalization results  which to the best of our
knowledge  are the ﬁrst bounds given for Autoencoder models. The results imply that we can employ
efﬁcient (regularized) OT solvers to approximate upper bounds on the generalization bounds  which
involve discrete distributions and thus are natural for such solvers.
The consequences of unifying two paradigms are plentiful  generalization bounds being an example.
One line of extending and continuing the presented work is to explore the use of a general cost
c (as opposed to a metric)  invoking the generalized Wasserstein dual in the goal of forming a
generalized GAN. Our paper provides a basis to unify Adversarial Networks and Autoencoders
through a primal-dual relationship  and opens doors for the further uniﬁcation of related models.

8

Acknowledgments

We would like to acknowledge anonymous reviewers and the Australian Research Council of Data61.

References
[1] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv

preprint arXiv:1610.03483  2016.

[2] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[3] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  pages 271–279  2016.

[4] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[5] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks.

arXiv:1701.00160  2016.

arXiv preprint

[6] Ilya Tolstikhin  Olivier Bousquet  Sylvain Gelly  and Bernhard Schoelkopf. Wasserstein auto-

encoders. arXiv preprint arXiv:1711.01558  2017.

[7] Lars Mescheder  Sebastian Nowozin  and Andreas Geiger. Adversarial variational bayes:
arXiv preprint

Unifying variational autoencoders and generative adversarial networks.
arXiv:1701.04722  2017.

[8] Alireza Makhzani  Jonathon Shlens  Navdeep Jaitly  Ian Goodfellow  and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644  2015.

[9] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin Ar-
jovsky  and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704 
2016.

[10] Aibek Alanov  Max Kochurov  Daniil Yashkov  and Dmitry Vetrov. Pairwise augmented gans

with adversarial reconstruction loss. arXiv preprint arXiv:1810.04920  2018.

[11] Pengchuan Zhang  Qiang Liu  Dengyong Zhou  Tao Xu  and Xiaodong He. On the

discrimination-generalization tradeoff in gans. arXiv preprint arXiv:1711.02771  2017.

[12] Ke Li and Jitendra Malik. On the implicit assumptions of gans. arXiv preprint arXiv:1811.12402 

2018.

[13] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances

in Applied Probability  29(2):429–443  1997.

[14] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business

Media  2008.

[15] XuanLong Nguyen  Martin J Wainwright  and Michael I Jordan. Estimating divergence func-
tionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information
Theory  56(11):5847–5861  2010.

[16] Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the
variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference 
NIPS  2016.

[17] Shengjia Zhao  Jiaming Song  and Stefano Ermon. Infovae: Information maximizing variational

autoencoders. arXiv preprint arXiv:1706.02262  2017.

9

[18] Giorgio Patrini  Marcello Carioni  Patrick Forre  Samarth Bhargav  Max Welling  Rianne
van den Berg  Tim Genewein  and Frank Nielsen. Sinkhorn autoencoders. arXiv preprint
arXiv:1810.01118  2018.

[19] Zhiting Hu  Zichao Yang  Ruslan Salakhutdinov  and Eric P Xing. On unifying deep generative

models. arXiv preprint arXiv:1706.00550  2017.

[20] Shuang Liu  Olivier Bousquet  and Kamalika Chaudhuri. Approximation and convergence
properties of generative adversarial learning. In Advances in Neural Information Processing
Systems  pages 5545–5553  2017.

[21] Shuang Liu and Kamalika Chaudhuri. The inductive bias of restricted f-gans. arXiv preprint

arXiv:1809.04542  2018.

[22] Farzan Farnia and David Tse. A convex duality framework for gans. In Advances in Neural

Information Processing Systems  pages 5254–5263  2018.

[23] Zhiming Zhou  Yuxuan Song  Lantao Yu  Hongwei Wang  Weinan Zhang  Zhihua Zhang  and
Yong Yu. Understanding the effectiveness of lipschitz-continuity in generative adversarial nets.
2018.

[24] Richard Nock  Zac Cranko  Aditya K Menon  Lizhen Qu  and Robert C Williamson. f-gans
in an information geometric nutshell. In Advances in Neural Information Processing Systems 
pages 456–464  2017.

[25] Shun-ichi Amari. Information geometry and its applications. Springer  2016.

[26] Lisa Borland. Ito-langevin equations within generalized thermostatistics. Physics Letters A 

245(1-2):67–72  1998.

[27] Irina Higgins  Loic Matthey  Arka Pal  Christopher Burgess  Xavier Glorot  Matthew Botvinick 
Shakir Mohamed  and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.

[28] Alexander Alemi  Ben Poole  Ian Fischer  Joshua Dillon  Rif A Saurous  and Kevin Murphy.
Fixing a broken elbo. In International Conference on Machine Learning  pages 159–168  2018.

[29] Bharath K Sriperumbudur  Kenji Fukumizu  Arthur Gretton  Bernhard Schölkopf  and Gert RG
Lanckriet. On integral probability metrics \phi-divergences and binary classiﬁcation. arXiv
preprint arXiv:0901.2698  2009.

[30] Jonathan Weed and Francis Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of

empirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087  2017.

[31] Kenneth Falconer. Fractal geometry: mathematical foundations and applications. John Wiley

& Sons  2004.

[32] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

10

,Ulrike Von Luxburg
Morteza Alamgir
Sung Ju Hwang
Leonid Sigal
Megasthenis Asteris
Dimitris Papailiopoulos
Alexandros Dimakis
Hisham Husain
Richard Nock
Robert Williamson