2013,Sparse Additive Text Models with Low Rank Background,The sparse additive model for text modeling involves the sum-of-exp computing  with consuming costs for large scales. Moreover  the assumption of equal background across all classes/topics may be too strong. This paper extends to propose sparse additive model with low rank background (SAM-LRB)  and simple yet efficient estimation. Particularly  by employing a double majorization bound  we approximate the log-likelihood into a quadratic lower-bound with the sum-of-exp terms absent. The constraints of low rank and sparsity are then simply embodied by nuclear norm and $\ell_1$-norm regularizers. Interestingly  we find that the optimization task in this manner can be transformed into the same form as that in Robust PCA. Consequently  parameters of supervised SAM-LRB can be efficiently learned using an existing algorithm for Robust PCA based on accelerated proximal gradient. Besides the supervised case  we extend SAM-LRB to also favor unsupervised and multifaceted scenarios. Experiments on real world data demonstrate the effectiveness and efficiency of SAM-LRB  showing state-of-the-art performances.,Sparse Additive Text Models with Low Rank

Background

Lei Shi

Baidu.com  Inc.

P.R. China

shilei06@baidu.om

Abstract

The sparse additive model for text modeling involves the sum-of-exp computing 
whose cost is consuming for large scales. Moreover  the assumption of equal back-
ground across all classes/topics may be too strong. This paper extends to propose
sparse additive model with low rank background (SAM-LRB) and obtains sim-
ple yet efﬁcient estimation. Particularly  employing a double majorization bound 
we approximate log-likelihood into a quadratic lower-bound without the log-sum-
exp terms. The constraints of low rank and sparsity are then simply embodied by
nuclear norm and ℓ1-norm regularizers. Interestingly  we ﬁnd that the optimiza-
tion task of SAM-LRB can be transformed into the same form as in Robust PCA.
Consequently  parameters of supervised SAM-LRB can be efﬁciently learned us-
ing an existing algorithm for Robust PCA based on accelerated proximal gradient.
Besides the supervised case  we extend SAM-LRB to favor unsupervised and mul-
tifaceted scenarios. Experiments on three real data demonstrate the effectiveness
and efﬁciency of SAM-LRB  compared with a few state-of-the-art models.

Introduction

1
Generative models of text have gained large popularity in analyzing a large collection of documents
[3  4  17]. This type of models overwhelmingly rely on the Dirichlet-Multinomial conjugate pair 
perhaps mainly because its formulation and estimation is straightforward and efﬁcient. However 
the ease of parameter estimation may come at a cost: unnecessarily over-complicated latent struc-
tures and lack of robustness to limited training data. Several efforts emerged to seek alternative
formulations  taking the correlated topic models [13  19] for instance.

Recently in [10]  the authors listed three main problems with Dirichlet-Multinomial generative mod-
els  namely inference cost  overparameterization  and lack of sparsity. Motivated by them  a Sparse
Additive GEnerative model (SAGE) was proposed in [10] as an alternative choice of generative mod-
el. Its core idea is that the lexical distribution in log-space comes by adding the background distribu-
tion with sparse deviation vectors. Successfully applying SAGE  effort [14] discovers geographical
topics in the twitter stream  and paper [25] detects communities in computational linguistics.

However  SAGE still suffers from two problems. First  the likelihood and estimation involve the
sum-of-exponential computing due to the soft-max generative nature  and it would be time consum-
ing for large scales. Second  SAGE assumes one single background vector across all classes/topics 
or equivalently  there is one background vector for each class/topic but all background vectors are
constrained to be equal. This assumption might be too strong in some applications  e.g.  when lots
of synonyms vary their distributions across different classes/topics.

Motivated to solve the second problem  we are propose to use a low rank constrained background.
However  directly assigning the low rank assumption to the log-space is difﬁcult. We turn to ap-
proximate the data log-likelihood of sparse additive model by a quadratic lower-bound based on the

1

double majorization bound in [6]  so that the costly log-sum-exponential computation  i.e.  the ﬁrst
problem of SAGE  is avoided. We then formulate and derive learning algorithm to the proposed
SAM-LRB model. Main contributions of this paper can be summarized into four-fold as below:

• Propose to use low rank background to extend the equally constrained setting in SAGE.
• Approximate the data log-likelihood of sparse additive model by a quadratic lower-bound
based on the double majorization bound in [6]  so that the costly log-sum-exponential com-
putation is avoided.

• Formulate the constrained optimization problem into Lagrangian relaxations  leading to a
form exactly the same as in Robust PCA [28]. Consequently  SAM-LRB can be efﬁciently
learned by employing the accelerated proximal gradient algorithm for Robust PCA [20].

• Extend SAM-LRB to favor supervised classiﬁcation  unsupervised topic model and multi-

faceted model; conduct experimental comparisons on real data to validate SAM-LRB.

2 Supervised Sparse Additive Model with Low Rank Background
2.1 Supervised Sparse Additive Model
Same as in SAGE [10]  the core idea of our model is that the lexical distribution in log-space comes
from adding the background distribution with additional vectors. Particularly  we are given doc-
uments D documents over M words. For each document d ∈ [1  D]  let yd ∈ [1  K] represent
the class label in the current supervised scenario  cd ∈ RM
+ denote the vector of term counts 

and Cd = Pw cdw be the total term count. We assume each class k ∈ [1  K] has two vectors

bk  sk ∈ RM   denoting the background and additive distributions in log-space  respectively. Then
the generative distribution for each word w in a document d with label yd is a soft-max form:

p(w|yd) = p(w|yd  byd   syd ) =

.

(1)

exp(bydw + sydw)
i=1 exp(bydi + sydi)

PM

Xi=1

Given Θ = {B  S} with B = [b1  . . .   bK] and S = [s1  . . .   sK ]  the log-likelihood of data X is:

L = log p(X |Θ) =

L(d  k)  L(d  k) = c⊤

d (bk + sk) − Cd log

exp(bki + ski).

(2)

K

M

Xk=1 Xd:yd=k

Similarly  a testing document d is classiﬁed into class ˆy(d) according to ˆy(d) = arg maxk L(d  k).
In SAGE [10]  the authors further assumed that the background vectors across all classes are the
same  i.e.  bk = b for ∀k  and each additive vector sk is sparse. Although intuitive  the background
equality assumption may be too strong for real applications. For instance  to express a same/similar
meaning  different classes of documents may choose to use different terms from a tuple of synonyms.
In this case  SAGE would tend to include these terms as the sparse additive part  instead of as the
background. Taking Fig. 1 as an illustrative example  the log-space distribution (left) is the sum
of the low-rank background B (middle) and the sparse S (right). Applying SAGE to this type of
data  the equality constrained background B would fail to capture the low-rank structure  and/or the
additive part S would be not sparse  so that there may be risks of over-ﬁtting or under-ﬁtting.

Moreover  since there exists sum-of-exponential terms in Eq. (2) and thus also in its derivatives  the
computing cost becomes huge when the vocabulary size M is large. As a result  although performing
well in [10  14  25]  SAGE might still suffer from problems of over-constrain and inefﬁciency.

Figure 1: Low rank background.
Left to right illustrates the log-
space distr.  background B  and
sparse S  resp. Rows index
terms  and columns for classes.

Figure 2: Lower-bound’s optimization. Left to right
shows the trajectory of lower-bound  α  and ξ  resp.

2

2.2 Supervised Sparse Additive Model with Low Rank Background

Motivated to avoid the inefﬁcient computing due to sum-of-exp  we adopt the double majorization
lower-bound of L [6]  so that it is well approximated and quadratic w.r.t. B and S. Further based on
this lower-bound  we proceed to assume the background B across classes is low-rank  in contrast to
the equality constraint in SAGE. An optimization algorithm is proposed based on proximal gradient.

2.2.1 Double Majorization Quadratic Lower Bound
In the literature  there have been several existing efforts on efﬁcient computing the sum-of-exp ter-
m involved in soft-max [5  15  6]. For instance  based on the convexity of logarithm  one can

obtain a bound − logPi exp(xi) ≥ −φPi exp(xi) + log φ + 1 for any φ ∈ R+  namely the

lb-log-cvx bound. Moreover  via upper-bounding the Hessian matrix  one can obtain the fol-
lowing local quadratic approximation for any ∀ξi ∈ R  shortly named as lb-quad-loc:

− log

M

Xi=1

exp(xi) ≥

1
M

(Xi

xi −Xi

ξi)2 −Xi

(xi −ξi)2 −Pi(xi − ξi) exp(ξi)

Pi exp(ξi)

−logXi

exp(ξi).

In [6]  Bouchard proposed the following quadratic lower-bound by double majorization
(lb-quad-dm) and demonstrated its better approximation compared with the previous two:

− log

M

Xi=1

exp(xi) ≥ −α −

1
2

M

Xi=1(cid:8)xi − α − ξi + f (ξi)[(xi − α)2 − ξ2

i ] + 2 log[exp(ξi) + 1](cid:9)  

(3)

with α ∈ R and ξ ∈ RM
bound is closely related to the bound proposed by Jaakkola and Jordan [6].
Employing Eq. (3)  we obtain a lower-bound Llb ≤ L to the data log-likelihood in Eq. (2):

+ being auxiliary (variational) variables  and f (ξ) = 1

2ξ · exp(ξ)−1

exp(ξ)+1 . This

K

Llb =

with γk = ˜Ck(αk −

Xk=1(cid:2)−(bk + sk)⊤Ak(bk + sk) − β⊤
Xi=1(cid:2)αk + ξki + f (ξki)(α2

1
2

M

Ak = ˜Ckdiag [f (ξk)]   βk = ˜Ck(

k (bk + sk) − γk(cid:3)  

k − ξ2

ki) + 2 log(exp(ξki) + 1)(cid:3))  
˜Ck = Xd:yd=k

cd 

1
2

− αkf (ξk)) − Xd:yd=k

Cd.

(4)

For each class k  the two variational variables  αk ∈ R and ξk ∈ RM
+   can be updated iteratively as
below for a better approximated lower-bound. Therein  abs(·) denotes the absolute value operator.

αk =

1

i=1 f (ξki)" M
PM

2

− 1 +

(bki + ski)f (ξki)#  

M

Xi=1

ξk = abs(bk + sk − αk).

(5)

One example of the trajectories during optimizing this lower-bound is illustrated in Fig. 2. Partic-
ularly  the left shows the lower-bound converges quickly to ground truth  usually within 5 rounds
in our experiences. The values of the three lower-bounds with randomly sampled the variation-
al variables are also sorted and plotted. One can ﬁnd that lb-quad-dm approximates better or
comparably well even with a random initialization. Please see [6] for more comparisons.

2.2.2 Supervised SAM-LRB Model and Optimization by Proximal Gradient
Rather than optimizing the data log-likelihood in Eq. (2) like in SAGE  we turn to optimize its
lower-bound in Eq. (4)  which is convenient for further assigning the low-rank constraint on B and
the sparsity constraint on S. Concretely  our target is formulated as a constrained optimization task:

Llb 

with Llb speciﬁed in Eq. (4) 

max
B S
s.t.

(6)
Concerning the two constraints  we call the above as supervised Sparse Additive Model with Low-
Rank Background  or supervised SAM-LRB for short. Although both of the two assumptions can

B = [b1  . . .   bK ]

S = [s1  . . .   sK ]

is low rank 

is sparse.

3

be tackled via formulating a fully generative model  assigning appropriate priors  and delivering
inference in a Bayesian manner similar to [8]  we determine to choose the constrained optimization
form for not only a clearer expression but also a simpler and efﬁcient algorithm.

In the literature  there have been several efforts considering both low rank and sparse constraints
similar to Eq. (6)  most of which take the use of proximal gradient [2  7]. Papers [20  28] studied the
problems under the name of Robust Principal Component Analysis (RPCA)  aiming to decouple an
observed matrix as the sum of a low rank matrix and a sparse matrix. Closely related to RPCA 
our scenario in Eq. (6) can be regarded as a weighted RPCA formulation  and the weights are
controlled by variational variables. In [24]  the authors proposed an efﬁcient algorithm for problems
that constrain a matrix to be both low rank and sparse simultaneously.

Following these existing works  we adopt the nuclear norm to implement the low rank constraint  and
ℓ1-norm for the sparsity constraint  respectively. Letting the partial derivative w.r.t. λk = (bk + sk)
of Llb equal to zero  the maximum of Llb can be achieved at λ∗
k βk. Since Ak is
positive deﬁnite and diagonal  the optimal solution λ∗
k is well-posed and can be efﬁciently computed.
Simultaneously considering the equality λk = (bk + sk)  the low rank on B and the sparsity on S 
one can rewritten Eq. (6) into the following Lagrangian form:

k = − 1

2 A−1

min
B S

1
2

||Λ∗ − B − S||2

F + µ(||B||∗ + ν|S|1)  with Λ∗ = [λ∗

1  . . .   λ∗

K ] 

(7)

where ||·||F   ||·||∗ and | · |1 denote the Frobenius norm  nuclear norm and ℓ1-norm  respectively.
The Frobenius norm term concerns the accuracy of decoupling from Λ∗ into B and S. Lagrange
multipliers µ and ν control the strengths of low rank constraint and sparsity constraint  respectively.

Interestingly  Eq. (7) is exactly the same as the objective of RPCA [20  28]. Paper [20] proposed an
algorithm for RPCA based on accelerated proximal gradient (APG-RPCA)  showing its advantages
of efﬁciency and stability over (plain) proximal gradient. We choose it  i.e.  Algorithm 2 in [20]  for
seeking solutions to Eq. (7). The computations involved in APG-RPCA include SVD decomposition
and absolute value thresholding  and interested readers are referred to [20] for more details. The
augmented Lagrangian and alternating direction methods [9  29] could be considered as alternatives.

Data: Term counts and labels {cd  Cd  yd}D
Result: Log-space distributions: low-rank B and sparse S
Initialization: randomly initialize parameters {B  S}  and variational variables {αk  ξk}k;
while not converge do

d=1 of D docs and K classes  sparse thres. ν ≈ 0.05

if optimize variational variables then iteratively update {αk  ξk}k according to Eq. (5);
for k = 1  . . .   K do calculate Ak and βk by Eq. (4)  and λ∗
B  S ←− APG-RPCA(Λ∗  ν) by Algorithm 2 in [20]  with Λ∗ = [λ∗

k = − 1

2 A−1

k βk ;
1  . . .   λ∗

K ];

end

Algorithm 1: Supervised SAM-LRB learning algorithm

Consequently  the supervised SAM-LRB algorithm is speciﬁed in Algorithm 1. Therein  one can
choose to either ﬁx or update the variational variables {αk  ξk}k. If they are ﬁxed  Algorithm 1
has only one outer iteration with no need to check the convergence. Compared with the supervised
SAGE learning algorithm in Sec. 3 of [10]  our supervised SAM-LRB algorithm not only does not
need to compute the sum of exponentials so that computing cost is saved  but also is optimized sim-
ply and efﬁciently by proximal gradient instead of using Newton updating as in SAGE. Moreover 
adding Laplacian-Exponential prior on S for sparseness  SAGE updates the conjugate posteriors and
needs to employ a “warm start” technique to avoid being trapped in early stages with inappropriate
initializations  while in contrast SAM-LRB does not have this risk. Additionally  since the evolution
from SAGE to SAM-LRB is two folded  i.e.  the low rank background assumption and the convex
relaxation  we ﬁnd that adopting the convex relaxation also helps SAGE during optimization.

3 Extensions

Analogous to [10]  our SAM-LRB formulation can be also extended to unsupervised topic modeling
scenario with latent variables  and the scenario with multifaceted class labels.

4

3.1 Extension 1: Unsupervised Latent Variable Model

We consider how to incorporate SAM-LRB in a latent variable model of unsupervised text mod-
elling. Following topic models  there is one latent vector of topic proportions per document and
one latent discrete variable per term. That is  each document d is endowed with a vector of topic
proportions θd ∼ Dirichlet(ρ)  and each term w in this document is associated with a latent topic
label z(d)

w ∼ Multinomial(θd). Then the probability distribution for w is

p(w|z(d)

w   B  S) ∝ exp(cid:16)b

w w + s

(d)

z

z

(d)

w w(cid:17)  

which only replaces the known class label yd in Eq. (1) with the unknown topic label z(d)
w .
We can combine the mean ﬁeld variational inference for latent Dirichlet allocation (LDA) [4] with
the lower-bound treatment in Eq. (4)  leading to the following unsupervised lower-bound

(8)

(9)

K

Llb =

Xk=1(cid:2)−(bk + sk)⊤Ak(bk + sk) − β⊤
+Xd

k (bk + sk) − γk(cid:3)
[hlog p(θd|ρ)i − hlog Q(θd)i] +Xd Xw hhlog p(z(d)

M

w |θd)i − hlog Q(z(d)

w )ii  

with γk = ˜Ck(αk −

1
2

Xi=1(cid:2)αk + ξki + f (ξki)(α2

k − ξ2

ki) + 2 log(exp(ξki) + 1)(cid:3))  

Ak = ˜Ckdiag [f (ξk)]  

βk = ˜Ck(

− αkf (ξk)) − ˜ck 

1
2

where each w-th item in ˜ck is ˜ckw = Pd Q(k|d  w)cdw  i.e. the expected count of term w in topic
k  and ˜Ck =Pw ˜ckw is the topic’s expected total count throughout all words.

This unsupervised SAM-LRB model formulates a topic model with low rank background and sparse
deviation  which is learned via EM iterations. The E-step to update posteriors Q(θd) and Q(z(d)
w ) is
identical to the standard LDA. Once {Ak  βk} are computed as above  the M-step to update {B  S}
and variational variables {αk  ξk}k remains the same as the supervised case in Algorithm 1.

3.2 Extension 2: Multifaceted Modelling

We consider how SAM-LRB can be used to combine multiple facets (multi-dimensional class label-
s)  i.e  combining per-word latent topics and document labels and pursuing a structural view of labels
and topics. In the literature  multifaceted generative models have been studied in [1  21  23]  and they
incorporated latent switching variables that determine whether each term is generated from a topic
or from a document label. Topic-label interactions can also be included to capture the distributions
of words at the intersections. However in this kind of models  the number of parameters becomes
very large for large vocabulary size  many topics  many labels. In [10]  SAGE needs no switching
variables and shows advantageous of model sparsity on multifaceted modeling. More recently  paper
[14] employs SAGE and discovers meaningful geographical topics in the twitter streams.

Applying SAM-LRB to the multifaceted scenario  we still assume the multifaceted variations are
composed of low rank background and sparse deviation. Particularly  for each topic k ∈ [1  K] 
we have the topic background b(T )
k ; for each label j ∈ [1  J]  we have
label background b(L)
; for each topic-label interaction pair (k  j)  we
have only the sparse deviation s(I)
K ] and
B(L) = [b(L)

J ] are assumed of low ranks to capture single view’s distribution similarity.

kj . Again  background distributions B(T ) = [b(T )

and sparse deviation s(L)

and sparse deviation s(T )

  . . .   b(T )

  . . .   b(L)

1

k

j

j

1

Then for a single term w given the latent topic z(d)
is obtained by summing the background and sparse components together:

w and the class label yd  its generative probability

p(w|z(d)

w   yd  Θ) ∝ exp(cid:16)b(T )

z

(d)
w w

+ s(T )

(d)
w w

z

5

+ b(L)

ydw + s(L)

ydw + s(I)

z

(d)

w ydw(cid:17)  

(10)

with parameters Θ = {B(T )  S(T )  B(L)  S(L)  S(I)}. The log-likelihood’s lower-bound involves
the sum through all topic-label pairs:

K

J

Llb =

with

Xk=1
+Xd

kjAkjλkj − β⊤

Xj=1(cid:2)−λ⊤
[hlog p(θd|ρ)i − hlog Q(θd)i] +Xd Xw hhlog p(z(d)

kjλkj − γkj(cid:3)

λkj   b(T )

k + s(T )

k + b(L)

j + s(L)

j + s(I)
kj .

w |θd)i − hlog Q(z(d)

w )ii  

(11)

In the quadratic form  the values of Akj  βkj and γkj are trivial combination of Eq. (4) and Eq. (9) 
i.e.  weighted by both the observed labels and posteriors of latent topics. Details are omitted here
due to space limit. The second row remains the same as in Eq. (9) and standard LDA.

During the iterative estimation  every iteration includes the following steps:

• Estimate the posteriors Q(z(d)
• With (B(T )  S(T )  S(I)) ﬁxed  solve a quadratic program over Λ∗(L)  which approximates

w ) and Q(θd);

the sum of B(L) and S(L). Put Λ∗(L) into Algorithm 1 to update B(L) and S(L);

• With (B(L)  S(L)  S(I)) ﬁxed  solve a quadratic program over Λ∗(T )  which approximates

the sum of B(T ) and S(T ). Put Λ∗(T ) into Algorithm 1 to update B(T ) and S(T );

• With (B(T )  S(T )  B(L)  S(L)) ﬁxed  update S(I) by proximal gradient.

4 Experimental Results

In order to test SAM-LRB in different scenarios  this section considers experiments under three
tasks  namely supervised document classiﬁcation  unsupervised topic modeling  and multi-faceted
modeling and classiﬁcation  respectively.

4.1 Document Classiﬁcation

We ﬁrst test our SAM-LRB model in the supervised document modeling scenario and evaluate
the classiﬁcation accuracy. Particularly  the supervised SAM-LRB is compared with the Dirichlet-
Multinomial model and SAGE. The precision of the Dirichlet prior in Dirichlet-Multinomial model
is updated by the Newton optimization [22]. Nonparametric Jeffreys prior [12] is adopted in SAGE
as a parameter-free sparse prior. Concerning the variational variables {αi  ξi}i in the quadratic
lower-bound of SAM-LRB  both cases of ﬁxing them and updating them are considered.
We consider the benchmark 20Newsgroups data1  and aim to classify unlabelled newsgroup post-
ings into 20 newsgroups. No stopword ﬁltering is performed  and we randomly pick a vocabulary
of 55 000 terms. In order to test the robustness  we vary the proportion of training data. After 5
independent runs by each algorithm  the classiﬁcation accuracies on testing data are plotted in Fig. 3
in terms of box-plots  where the lateral axis varies the training data proportion.

Figure 3: Classiﬁcation accuracy on 20Newsgroups data. The pro-
portion of training data varies in {10%  30%  50%}.

1Following [10]  we use the training/testing sets from http://people.csail.mit.edu/jrennie/20Newsgroups/

6

One can ﬁnd that  SAGE outperforms Dirichlet-Multinomial model especially in case of limited
training data  which is consistent to the observations in [10]. Moreover  with random and ﬁxed
variational variables  the SAM-LRB model performs further better or at least comparably well. If
the variational variables are updated to tighten the lower-bound  the performance of SAM-LRB is
substantially the best  with a 10%∼20% relative improvement over SAGE. Table 1 also reports the
average computing time of SAGE and SAM-LRB. We can see that  by avoiding the log-sum-exp
calculation  SAM-LRB (ﬁxed) performs more than 7 times faster than SAGE  while SAM-LRB
(optimized) pays for updating the variational variables.

Table 1: Comparison on average time costs per iteration (in minutes).

method

SAGE SAM-LRB (ﬁxed)

SAM-LRB (optimized)

time cost (minutes)

3.8

0.6

3.3

4.2 Unsupervised Topic Modeling

We now apply our unsupervised SAM-LRB model to the benchmark NIPS data2. Following the
same preprocessing and evaluation as in [10  26]  we have a training set of 1986 documents with
237 691 terms  and a testing set of 498 documents with 57 427 terms.

For consistency  SAM-LRB is still compared with Dirichlet-Multinomial model (variational LDA
model with symmetric Dirichlet prior) and SAGE. For all these unsupervised models  the number
of latent topics is varied from 10 to 25 and then to 50. After unsupervised training  the performance
is evaluated by perplexity  the smaller the better. The performances of 5 independent runs by each
method are illustrated in Fig. 4  again in terms of box-plots.

Figure 4: Perplexity results on NIPS data.

As shown  SAGE performs worse than LDA when there are few number of topics  perhaps mainly
due to its strong equality assumption on background. Whereas  SAM-LRB performs better than
both LDA and SAGE in most cases. With one exception happens when the topic number equals 50 
SAM-LRB (ﬁxed) performs slightly worse than SAGE  mainly caused by inappropriate ﬁxed values
of variational variables. If updated instead  SAM-LRB (optimized) performs promisingly the best.

4.3 Multifaceted Modeling

We then proceed to test the multifaceted modeling by SAM-LRB. Same as [10]  we choose a
publicly-available dataset of political blogs describing the 2008 U.S. presidential election3 [11].
Out of the total 6 political blogs  three are from the right and three are from left. There are 20 827
documents and a vocabulary size of 8284. Using four blogs for training  our task is to predict the
ideological perspective of two unlabeled blogs.

On this task  Ahmed and Xing in [1] used multiview LDA model to achieve accuracy within
65.0% ∼ 69.1% depending on different topic number settings. Also  support vector machine pro-
vides a comparable accuracy of 69%  while supervised LDA [3] performs undesirably on this task.
In [10]  SAGE is repeated 5 times for each of multiple topic numbers  and achieves its best median

2http://www.cs.nyu.edu/∼roweis/data.html
3http://sailing.cs.cmu.edu/socialmedia/blog2008.html

7

result 69.6% at K = 30. Using SAM-LRB (optimized)  the median results out of 5 runs for each
topic number are shown in Table 2. Interestingly  SAM-LRB provides a similarly state-of-the-art
result  while achieving it at K = 20. The different preferences on topic numbers between SAGE and
SAM-LRB may mainly come from their different assumptions on background lexical distributions.

Table 2: Classiﬁcation accuracy on political blogs data by SAM-LRB (optimized).

# topic (K)

accuracy (%) median out of 5 runs

10
67.3

20
69.8

30
69.1

40
68.3

50
68.1

5 Concluding Remarks

This paper studies the sparse additive model for document modeling. By employing the double ma-
jorization technique  we approximate the log-sum-exponential term involved in data log-likelihood
into a quadratic lower-bound. With the help of this lower-bound  we are able to conveniently relax
the equality constraint on background log-space distribution of SAGE [10]  into a low-rank con-
straint  leading to our SAM-LRB model. Then  after the constrained optimization is transformed
into the form of RPCA’s objective function  an algorithm based on accelerated proximal gradient
is adopted during learning SAM-LRB. The model speciﬁcation and learning algorithm are some-
what simple yet effective. Besides the supervised version  extensions of SAM-LRB to unsupervised
and multifaceted scenarios are investigated. Experimental results demonstrate the effectiveness and
efﬁciency of SAM-LRB compared with Dirichlet-Multinomial and SAGE.

Several perspectives may deserve investigations in future. First  the accelerated proximal gradient
updating needs to compute SVD decompositions  which are probably consuming for very large scale
data. In this case  more efﬁcient optimization considering nuclear norm and ℓ1-norm are expected 
with the semideﬁnite relaxation technique in [16] being one possible choice. Second  this paper
uses a constrained optimization formulation  while Bayesian tackling via adding conjugate priors to
complete the generative model similar to [8] is an alternative choice. Moreover  we may also adopt
nonconjugate priors and employ nonconjugate variational inference in [27]. Last but not the least 
discriminative learning with large margins [18  30] might be also equipped for robust classiﬁcation.
Since nonzero elements of sparse S in SAM-LRB can be also regarded as selected feature  one
may design to include them into the discriminative features  rather than only topical distributions
[3]. Additionally  the augmented Lagrangian and alternating direction methods [9  29] could be also
considered as alternatives to the proximal gradient optimization.

References

[1] A. Ahmed and E. P. Xing. Staying informed: supervised and semi-supervised multi-view

topical analysis of ideological pespective. In Proc. EMNLP  pages 1140–1150  2010.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[3] D. Blei and J. McAuliffe. Supervised topic models. In Advances in NIPS  pages 121–128.

2008.

[4] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. JMLR  3:993–1022  2003.
[5] D. Bohning. Multinomial logistic regression algorithm. Annals of Inst. of Stat. Math.  44:197–

200  1992.

[6] G. Bouchard. Efﬁcient bounds for the softmax function  applications to inference in hybrid
models. In Workshop for Approximate Bayesian Inference in Continuous/Hybrid Systems at
NIPS’07  2007.

[7] X. Chen  Q. Lin  S. Kim  J. G. Carbonell  and E. P. Xing. Smoothing proximal gradient method
for general structured sparse regression. The Annals of Applied Statistics  6(2):719–752  2012.
[8] X. Ding  L. He  and L. Carin. Bayesian robust principal component analysis. IEEE Trans.

Image Processing  20(12):3419–3430  2011.

8

[9] J. Eckstein. Augmented Lagrangian and alternating direction methods for convex optimization:
A tutorial and some illustrative computational results. Technical report  RUTCOR Research
Report RRR 32-2012  2012.

[10] J. Eisenstein  A. Ahmed  and E. P. Xing. Sparse additive generative models of text. In Proc.

ICML  2011.

[11] J. Eisenstein and E. P. Xing. The CMU 2008 political blog corpus. Technical report  Carnegie

Mellon University  School of Computer Science  Machine Learning Department  2010.

[12] M. A. T. Figueiredo. Adaptive sparseness using Jeffreys prior. In Advances in NIPS  pages

679–704. 2002.

[13] M. R. Gormley  M. Dredze  B. Van Durme  and J. Eisner. Shared components topic models.

In Proc. NAACL-HLT  pages 783–792  2012.

[14] L. Hong  A. Ahmed  S. Gurumurthy  A. J. Smola  and K. Tsioutsiouliklis. Discovering geo-

graphical topics in the twitter stream. In Proc. 12th WWW  pages 769–778  2012.

[15] T. Jaakkola and M. I. Jordan. A variational approach to Bayesian logistic regression problems

and their extensions. In Proc. AISTATS  1996.

[16] M. Jaggi and M. Sulovsk`y. A simple algorithm for nuclear norm regularized problems. In

Proc. ICML  pages 471–478  2010.

[17] Y. Jiang and A. Saxena. Discovering different types of topics: Factored topics models. In Proc.

IJCAI  2013.

[18] A. Joulin  F. Bach  and J. Ponce. Efﬁcient optimization for discriminative latent class models.

In Advances in NIPS  pages 1045–1053. 2010.

[19] J. D. Lafferty and M. D. Blei. Correlated topic models. In Advances in NIPS  pages 147–155 

2006.

[20] Z. Lin  A. Ganesh  J. Wright  L. Wu  M. Chen  and Y. Ma. Fast convex optimization algorithms
for exact recovery of a corrupted low-rank matrix. Technical report  UIUC Technical Report
UILU-ENG-09-2214  August 2009.

[21] Q. Mei  X. Ling  M. Wondra  H. Su  and C. X. Zhai. Topic sentiment mixture: modeling facets

and opinions in webblogs. In Proc. WWW  2007.

[22] T. P. Minka. Estimating a dirichlet distribution. Technical report  Massachusetts Institute of

Technology  2003.

[23] M. Paul and R. Girju. A two-dimensional topic-aspect model for discovering multi-faceted

topics. In Proc. AAAI  2010.

[24] E. Richard  P.-A. Savalle  and N. Vayatis. Estimation of simultaneously sparse and low rank

matrices. In Proc. ICML  pages 1351–1358  2012.

[25] Y. S. N. A. Smith and D. A. Smith. Discovering factions in the computational linguistics

community. In ACL Workshop on Rediscovering 50 Years of Discoveries  2012.

[26] C. Wang and D. Blei. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet

process. In Advances in NIPS  pages 1982–1989. 2009.

[27] C. Wang and D. M. Blei. Variational inference in nonconjugate models. To appear in JMLR.
[28] J. Wright  A. Ganesh  S. Rao  Y. Peng  and Y. Ma. Robust principal component analysis: Exact
recovery of corrupted low-rank matrices via convex optimization. In Advances in NIPS  pages
2080–2088. 2009.

[29] J. Yang and X. Yuan. Linearized augmented Lagrangian and alternating direction methods for

nuclear norm minimization. Math. Comp.  82:301–329  2013.

[30] J. Zhu  A. Ahmed  and E. P. Xing. MedLDA: maximum margin supervised topic models.

JMLR  13:2237–2278  2012.

9

,Lei Shi
Justin Domke
Daniel Sheldon