2014,On Iterative Hard Thresholding Methods for High-dimensional M-Estimation,The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L_0 constraints. Of the known methods  the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However  the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP  CoSaMP  SP) in the high dimensional regression setting. Finally  we extend our analysis to the problem of low-rank matrix recovery.,On Iterative Hard Thresholding Methods for

High-dimensional M-Estimation

Prateek Jain∗

Ambuj Tewari†

Purushottam Kar∗

∗Microsoft Research  INDIA

†University of Michigan  Ann Arbor  USA

{prajain t-purkar}@microsoft.com  tewaria@umich.edu

Abstract

The use of M-estimators in generalized linear regression models in high dimen-
sional settings requires risk minimization with hard L0 constraints. Of the known
methods  the class of projected gradient descent (also known as iterative hard
thresholding (IHT)) methods is known to offer the fastest and most scalable solu-
tions. However  the current state-of-the-art is only able to analyze these methods
in extremely restrictive settings which do not hold in high dimensional statisti-
cal models. In this work we bridge this gap by providing the ﬁrst analysis for
IHT-style methods in the high dimensional statistical setting. Our bounds are tight
and match known minimax lower bounds. Our results rely on a general analysis
framework that enables us to analyze several popular hard thresholding style al-
gorithms (such as HTP  CoSaMP  SP) in the high dimensional regression setting.
Finally  we extend our analysis to the problem of low-rank matrix recovery.

1

Introduction

Modern statistical estimation is routinely faced with real world problems where the number of pa-
rameters p handily outnumbers the number of observations n. In general  consistent estimation of
parameters is not possible in such a situation. Consequently  a rich line of work has focused on
models that satisfy special structural assumptions such as sparsity or low-rank structure. Under
these assumptions  several works (for example  see [1  2  3  4  5]) have established that consistent
estimation is information theoretically possible in the “n (cid:28) p” regime as well.
The question of efﬁcient estimation  however  is faced with feasibility issues since consistent esti-
mation routines often end-up solving NP-hard problems. Examples include sparse regression which
requires loss minimization with sparsity constraints and low-rank regression which requires dealing
with rank constraints which are not efﬁciently solvable in general [6].
Interestingly  recent works have demonstrated that these hardness results can be avoided by assuming
certain natural conditions over the loss function being minimized such as restricted strong convexity
(RSC) and restricted strong smoothness (RSS). The estimation routines proposed in these works
typically make use of convex relaxations [5] or greedy methods [7  8  9] which do not suffer from
infeasibility issues.
Despite this  certain limitations have precluded widespread use of these techniques. Convex
relaxation-based methods typically suffer from slow rates as they solve non-smooth optimization
problems apart from being hard to analyze in terms of global guarantees. Greedy methods  on the
other hand  are slow in situations with non-negligible sparsity or relatively high rank  owing to their
incremental approach of adding/removing individual support elements.
Instead  the methods of choice for practical applications are actually projected gradient (PGD) meth-
ods  also referred to as iterative hard thresholding (IHT) methods. These methods directly project

1

1 − 
1

1 − 

(cid:21)

(cid:20) 1

the gradient descent update onto the underlying (non-convex) feasible set. This projection can be
performed efﬁciently for several interesting structures such as sparsity and low rank. However  tra-
ditional PGD analyses for convex problems viz. [10] do not apply to these techniques due to the
non-convex structure of the problem.
An exception to this is the recent work [11] that demonstrates that PGD with non-convex regulariza-
tion can offer consistent estimates for certain high-dimensional problems. However  the work in [11]
is only able to analyze penalties such as SCAD  MCP and capped L1. Moreover  their framework
cannot handle commonly used penalties such as L0 or low-rank constraints.
Insufﬁciency of RIP based Guarantees for M-estimation. As noted above  PGD/IHT-style meth-
ods have been very popular in literature for sparse recovery and several algorithms including Iterative
Hard Thresholding (IHT) [12] or GraDeS [13]  Hard Thresholding Pursuit (HTP) [14]  CoSaMP
[15]  Subspace Pursuit (SP) [16]  and OMPR((cid:96)) [17] have been proposed. However  the analysis
of these algorithms has traditionally been restricted to settings that satisfy the Restricted Isometry
property (RIP) or incoherence property. As the discussion below demonstrates  this renders these
analyses inaccessible to high-dimensional statistical estimation problems.
All existing results analyzing these methods require the condition number of the loss function  re-
stricted to sparse vectors  to be smaller than a universal constant. The best known such constant is
due to the work of [17] that requires a bound on the RIP constant δ2k ≤ 0.5 (or equivalently a bound
≤ 3 on the condition number). In contrast  real-life high dimensional statistical settings 
1+δ2k
1−δ2k
wherein pairs of variables can be arbitrarily correlated  routinely require estimation methods to per-
form under arbitrarily large condition numbers. In particular if two variates have a covariance matrix
like
  then the restricted condition number (on a support set of size just 2) of the sam-
ple matrix cannot be brought down below 1/ even with inﬁnitely many samples. In particular when
 < 1/6  none of the existing results for hard thresholding methods offer any guarantees. Moreover 
most of these analyses consider only the least squares objective. Although recent attempts have
been made to extend this to general differentiable objectives [18  19]  the results continue to require
that the restricted condition number be less than a universal constant and remain unsatisfactory in a
statistical setting.
Overview of Results. Our main contribution in this work is an analysis of PGD/IHT-style methods
in statistical settings. Our bounds are tight  achieve known minmax lower bounds [20]  and hold
for arbitrary differentiable  possibly even non-convex functions. Our results hold even when the
underlying condition number is arbitrarily large and only require the function to satisfy RSC/RSS
conditions. In particular  this reveals that these iterative methods are indeed applicable to statistical
settings  a result that escaped all previous works.
Our ﬁrst result shows that the PGD/IHT methods achieve global convergence if used with a relaxed
projection step. More formally  if the optimal parameter is s∗-sparse and the problem satisﬁes
RSC and RSS constraints α and L respectively (see Section 2)  then PGD methods offer global
convergence so long as they employ projection to an s-sparse set where s ≥ 4(L/α)2s∗. This
gives convergence rates that are identical to those of convex relaxation and greedy methods for the
Gaussian sparse linear model. We then move to a family of efﬁcient “fully corrective” methods and
show as before  that for arbitrary functions satisfying the RSC/RSS properties  these methods offer
global convergence.
Next  we show that these results allow PGD-style methods to offer global convergence in a variety
of statistical estimation problems such as sparse linear regression and low rank matrix regression.
Our results effortlessly extend to the noisy setting as a corollary and give bounds similar to those of
[21] that relies on solving an L1 regularized problem.
Our proofs are able to exploit that even though hard-thresholding is not the prox-operator for any
convex prox function  it still provides strong contraction when projection is performed onto sets of
sparsity s (cid:29) s∗. This crucial observation allows us to provide the ﬁrst uniﬁed analysis for hard
thresholding based gradient descent algorithms. Our empirical results conﬁrm our predictions with
respect to the recovery properties of IHT-style algorithms on badly-conditioned sparse recovery
problems  as well as demonstrate that these methods can be orders of magnitudes faster than their
L1 and greedy counterparts.

2

Organization. Section 2 sets the notation and the problem statement. Section 3 introduces the
PGD/IHT algorithm that we study and proves that the method guarantees recovery assuming the
RSC/RSS property. We also generalize our guarantees to the problem of low-rank matrix regression.
Section 4 then provides crisp sample complexity bounds and statistical guarantees for the PGD/IHT
estimators. Section 5 extends our analysis to a broad family of compressive sensing algorithms that
include the so-called fully-corrective hard thresholding methods and provide similar results for them
as well. We present some empirical results in Section 6 and conclude in Section 7.

2 Problem Setup and Notations
High-dimensional Sparse Estimation. Given data points X = [X1  . . .   Xn]T   where Xi ∈ Rp 
and the target Y = [Y1  . . .   Yn]T   where Yi ∈ R  the goal is to compute an s∗-sparse θ∗ s.t. 

θ∗ = arg min

θ (cid:107)θ(cid:107)0≤s∗ f (θ).

(cid:80)
(1)
i (cid:96)((cid:104)Xi  θ(cid:105)  Yi) for some
Typically  f can be thought of as an empirical risk function i.e. f (θ) = 1
n
loss function (cid:96) (see examples in Section 4). However  for our analysis of PGD and other algorithms 
we need not assume any other property of f other than differentiability and the following two RSC
and RSS properties.
Deﬁnition 1 (RSC Property). A differentiable function f : Rp → R is said to satisfy restricted
strong convexity (RSC) at sparsity level s = s1 + s2 with strong convexity constraint αs if the
following holds for all θ1  θ2 s.t. (cid:107)θ1(cid:107)0 ≤ s1 and (cid:107)θ2(cid:107)0 ≤ s2:
f (θ1) − f (θ2) ≥ (cid:104)θ1 − θ2 ∇θf (θ2)(cid:105) +

(cid:107)θ1 − θ2(cid:107)2
2.

Deﬁnition 2 (RSS Property). A differentiable function f : Rp → R is said to satisfy restricted
strong smoothness (RSS) at sparsity level s = s1 + s2 with strong convexity constraint Ls if the
following holds for all θ1  θ2 s.t. (cid:107)θ1(cid:107)0 ≤ s1 and (cid:107)θ2(cid:107)0 ≤ s2:
f (θ1) − f (θ2) ≤ (cid:104)θ1 − θ2 ∇θf (θ2)(cid:105) +

(cid:107)θ1 − θ2(cid:107)2
2.

αs
2

Ls
2

Low-rank Matrix Regression. Low-rank matrix regression is similar to sparse estimation as pre-
sented above except that each data point is now a matrix i.e. Xi ∈ Rp1×p2  the goal being to estimate
a low-rank matrix W ∈ Rp1×p2 that minimizes the empirical loss function on the given data.

W ∗ = arg

min

W rank(W )≤r

f (W ).

(2)

For this problem the RSC and RSS properties for f are deﬁned similarly as in Deﬁnition 1  2 except
that the L0 norm is replaced by the rank function.

3

Iterative Hard-thresholding Method

In this section we study the popular projected gradient descent (a.k.a iterative hard thresholding)
method for the case of the feasible set being the set of sparse vectors (see Algorithm 1 for pseu-
docode). The projection operator Ps(z)  can be implemented efﬁciently in this case by projecting
z onto the set of s-sparse vectors by selecting the s largest elements (in magnitude) of z. The stan-
2 for all (cid:107)θ(cid:48)(cid:107)0 ≤ s. However  it
dard projection property implies that (cid:107)Ps(z) − z(cid:107)2
turns out that we can prove a signiﬁcantly stronger property of hard thresholding for the case when
(cid:107)θ(cid:48)(cid:107)0 ≤ s∗ and s∗ (cid:28) s. This property is key to analysing IHT and is formalized below.
Lemma 1. For any index set I  any z ∈ RI  let θ = Ps(z). Then for any θ∗ ∈ RI such that
(cid:107)θ∗(cid:107)0 ≤ s∗  we have

2 ≤ (cid:107)θ(cid:48) − z(cid:107)2

(cid:107)θ − z(cid:107)2

2 ≤ |I| − s

|I| − s∗(cid:107)θ∗ − z(cid:107)2

2.

See Appendix A for a detailed proof.
Our analysis combines the above observation with the RSC/RSS properties of f to provide geometric
convergence rates for the IHT procedure below.

3

Algorithm 1 Iterative Hard-thresholding
1: Input: Function f with gradient oracle  sparsity level s  step-size η
2: θ1 = 0  t = 1
3: while not converged do
4:
5: end while
6: Output: θt

θt+1 = Ps(θt − η∇θf (θt))  t = t + 1

respectively. Let Algorithm 1 be invoked with f  s ≥ 32(cid:0) L

Theorem 1. Let f have RSC and RSS parameters given by L2s+s∗ (f ) = L and α2s+s∗ (f ) = α
3L . Also let θ∗ =
)) satisﬁes:

arg minθ (cid:107)θ(cid:107)0≤s∗ f (θ). Then  the τ-th iterate of Algorithm 1  for τ = O( L

s∗ and η = 2

α · log( f (θ0)

(cid:1)2

α



f (θτ ) − f (θ∗) ≤ .

Proof. (Sketch) Let St = supp(θt)  S∗ = supp(θ∗)  St+1 = supp(θt+1) and I t = S∗∪St∪St+1.
Using the RSS property and the fact that supp(θt) ⊆ I t and supp(θt+1) ⊆ I t  we have:
f (θt+1) − f (θt) ≤ (cid:104)θt+1 − θt  gt(cid:105) +

L
=
2
ζ1≤ L
2

I t +

I t − θt
(cid:107)θt+1
|I t| − s
|I t| − s∗ · (cid:107)θ∗
·

(cid:107)θt+1 − θt(cid:107)2
L
2 
2
2 − 1
· gt
(cid:107)gt
2
2L
3L
· gt
I t(cid:107)2
I t − θt
1
L

I t(cid:107)2

I t +

I t(cid:107)2
2 
2 − 1
2L

((cid:107)gt

I t\(St∪S∗)(cid:107)2

St∪S∗(cid:107)2
2) 
(3)
where ζ1 follows from an application of Lemma 1 with I = I t and the Pythagoras theorem. The
above equation has three critical terms. The ﬁrst term can be bounded using the RSS condition.
Using f (θt) − f (θ∗) ≤ (cid:104)gt
2 bounds the third term
S∗ can be arbitrarily small.
in (3). The second term is more interesting as in general elements of gt
S∗\St+1 as they are selected by
However  elements of gt
hard-thresholding. Combining this insight with bounds for gt
S∗\St+1 and with (3)  we obtain the
theorem. See Appendix A for a detailed proof.

I t\(St∪S∗) should be at least as large as gt

St∪S∗   θt − θ∗(cid:105) − α

2 (cid:107)θt − θ∗(cid:107)2

St∪S∗(cid:107)2

2 + (cid:107)gt

2 ≤ 1

2α(cid:107)gt

3.1 Low-rank Matrix Regression

We now generalize our previous analysis to a projected gradient descent (PGD) method for low-rank
matrix regression. Formally  we study the following problem:

f (W )  s.t.  rank(W ) ≤ s.

min
W

The hard-thresholding projection step for low-rank matrices can be solved using SVD i.e.

P Ms(W ) = UsΣsV T
s  

where W = U ΣV T is the singular value decomposition of W . Us  Vs are the top-s singular vectors
(left and right  respectively) of W and Σs is the diagonal matrix of the top-s singular values of W .
To proceed  we ﬁrst note a property of the above projection similar to Lemma 1.
Lemma 2. Let W ∈ Rp1×p2 be a rank-|I t| matrix and let p1 ≥ p2. Then for any rank-s∗ matrix
W ∗ ∈ Rp1×p2 we have

(cid:107)P Ms(W ) − W(cid:107)2

F ≤ |I t| − s

|I t| − s∗(cid:107)W ∗ − W(cid:107)2

F .

Proof. Let W = U ΣV T be the singular value decomposition of W . Now  (cid:107)P Ms(W ) − W(cid:107)2

i=s+1 σ2

(cid:80)|I t|
2 ≤ |I t| − s
where the last step uses the von Neumann’s trace inequality (T r(A · B) ≤(cid:80)

i = (cid:107)Ps(diag(Σ)) − diag(Σ)(cid:107)2
F ≤ |I t| − s

W . Using Lemma 1  we get:
(cid:107)P Ms(W ) − W(cid:107)2

|I t| − s∗(cid:107)Σ∗ − diag(Σ)(cid:107)2

F =
2  where σ1 ≥ ··· ≥ σ|I t| ≥ 0 are the singular values of

|I t| − s∗(cid:107)W ∗ − W(cid:107)2

(6)

F  

i σi(A)σi(B)).

(4)

(5)

4

Suppose we invoke it with f  s ≥ 32(cid:0) L

The following result for low-rank matrix regression immediately follows from Lemma 4.
Theorem 2. Let f have RSC and RSS parameters given by L2s+s∗ (f ) = L and α2s+s∗ (f ) = α.
Replace the projection operator Ps in Algorithm 1 with its matrix counterpart P Ms as deﬁned in (5).
3L . Also let W ∗ = arg minW rank(W )≤s∗ f (W ).
α · log( f (W 0)
f (W τ ) − f (W ∗) ≤ .

Then the τ-th iterate of Algorithm 1  for τ = O( L

s∗  η = 2

) satisﬁes:

(cid:1)2

α



Proof. A proof progression similar to that of Theorem 1 sufﬁces. The only changes that need to be
made are: ﬁrstly Lemma 2 has to be invoked in place of Lemma 1. Secondly  in place of consid-
ering vectors restricted to a subset of coordinates viz. θS  gt
I  we would need to consider matrices
restricted to subspaces i.e. WS = USU T
S W where US is a set of singular vectors spanning the
range-space of S.

4 High Dimensional Statistical Estimation

This section elaborates on how the results of the previous section can be used to give guarantees for
IHT-style techniques in a variety of statistical estimation problems. We will ﬁrst present a generic
convergence result and then specialize it to various settings. Suppose we have a sample of data
points Z1:n and a loss function L(θ; Z1:n) that depends on a parameter θ and the sample. Then we
can show the following result. (See Appendix B for a proof.)
Suppose L(θ; Z1:n) is differentiable and satis-
Theorem 3. Let ¯θ be any s∗-sparse vector.
ﬁes RSC and RSS at sparsity level s + s∗ with parameters αs+s∗ and Ls+s∗ respectively  for
s ≥ 32
s∗. Let θτ be the τ-th iterate of Algorithm 1 for τ chosen as in Theorem 1
and ε be the function value error incurred by Algorithm 1. Then we have

(cid:16) L2s+s∗

(cid:17)2

α2s+s∗

√
(cid:107) ¯θ − θτ(cid:107)2 ≤ 2

s + s∗(cid:107)∇L( ¯θ; Z1:n)(cid:107)∞

αs+s∗

+

2

αs+s∗

.

(cid:115)

1

Note that the result does not require the loss function to be convex. This fact will be crucially used
later. We now apply the above result to several statistical estimation scenarios.
Sparse Linear Regression. Here Zi = (Xi  Yi) ∈ Rp × R and Yi = (cid:104) ¯θ  Xi(cid:105) + ξi where
ξi ∼ N (0  σ2) is label noise. The empirical loss is the usual least squares loss i.e. L(θ; Z1:n) =
n(cid:107)Y − Xθ(cid:107)2
2. Suppose X1:n are drawn i.i.d. from a sub-Gaussian distribution with covariance
Σ with Σjj ≤ 1 for all j. Then [22  Lemma 6] immediately implies that RSC and RSS at
sparsity level k hold  with probability at least 1 − e−c0n  with αk = 1
and
(c0  c1 are universal constants). So we can set k = 2s + s∗ and if
Lk = 2σmax(Σ) + c1
4 σmin(Σ) and Lk ≤ 2.25σmax(Σ) which means that
n > 4c1k log p/σmin(Σ) then we have αk ≥ 1
Lk/9αk ≤ κ(Σ) := σmax(Σ)/σmin(Σ). Thus it is enough to choose s = 2592κ(Σ)2s∗ and ap-
ply Theorem 3. Note that (cid:107)∇L( ¯θ; Z1:n)(cid:107)∞ = (cid:107)X T ξ/n(cid:107)∞ ≤ 2σ
n with probability at least
1−c2p−c3 (c2  c3 are universal constants). Putting everything together  we have the following bound
with high probability:

2 σmin(Σ) − c1

k log p

k log p

n

n

(cid:113) log p
(cid:114) 

(cid:114)

(cid:107) ¯θ − θτ(cid:107)2 ≤ 145

κ(Σ)

σmin(Σ)

σ

s∗ log p

n

+ 2

 

σmin(Σ)

where  is the function value error incurred by Algorithm 1.
Noisy and Missing Data. We now look at cases with feature noise as well. More speciﬁcally 
assume that we only have access to ˜Xi’s that are corrupted versions of Xi’s. Two models of noise are
popular in literature [21]: a) (additive noise) ˜Xi = Xi+Wi where Wi ∼ N (0  ΣW )  and b) (missing
data) ˜X is an R∪{(cid:63)}-valued matrix obtained by independently  with probability ν ∈ [0  1)  replacing
each entry in X with (cid:63). For the case of additive noise (missing data can be handled similarly) 
2 θT ˆΓθ − ˆγT θ where ˆΓ = ˜X T ˜X/n − ΣW and ˆγ = ˜X T Y /n are
Zi = ( ˜Xi  Yi) and L(θ; Z1:n) = 1

5

Algorithm 2 Two-stage Hard-thresholding
1: Input: function f with gradient oracle  sparsity level s  sparsity expansion level (cid:96)
2: θ1 = 0  t = 1
3: while not converged do
4:
5:
6:
7:
8:
9: end while
10: Output: θt

(cid:101)θt = Ps(βt)
θt+1 = arg minθ supp(θ)⊆supp((cid:101)θt) f (θ)  t = t + 1

gt = ∇θf (θt)  St = supp(θt)
Z t = St ∪ (largest (cid:96) elements of |gt
βt = arg minβ supp(β)⊆Zt f (β)

St|)

// fully corrective step

// fully corrective step

unbiased estimators of Σ and ΣT ¯θ respectively. [21  Appendix A  Lemma 1] implies that RSC  RSS
2 σmin(Σ) −
at sparsity level k hold  with failure probability exponentially small in n  with αk = 1
op+(cid:107)ΣW (cid:107)2
  1) log p.
kτ (p)/n and Lk = 1.5σmax(Σ) + kτ (p)/n for τ (p) = c0σmin(Σ) max(
σ2
Thus for k = 2s + s∗ and n ≥ 4kτ (p)/σmin(Σ) we have Lk/αk ≤ 7κ(Σ). Note that L(·; Z1:n)
is non-convex but we can still apply Theorem 3 with s = 1568κ(Σ)2s∗ because RSC  RSS hold.
Using the high probability upper bound (see [21  Appendix A  Lemma 2]) (cid:107)∇L( ¯θ; Z1:n)(cid:107)∞ ≤
c1 ˜σ(cid:107) ¯θ(cid:107)2

((cid:107)Σ(cid:107)2

min(Σ)

op)2

(cid:114)

κ(Σ)

σmin(Σ)

˜σ(cid:107) ¯θ(cid:107)2

s∗ log p

n

+ 2

(cid:114) 

σmin(Σ)

(cid:112)log p/n gives us the following
(cid:113)(cid:107)ΣW(cid:107)2

(cid:107) ¯θ − θτ(cid:107)2 ≤ c2
op + (cid:107)Σ(cid:107)2

where ˜σ =

op((cid:107)ΣW(cid:107)op + σ) and  is the function value error in Algorithm 1.

5 Fully-corrective Methods

In this section  we study a variety of “fully-corrective” methods. These methods keep the optimiza-
tion objective fully minimized over the support of the current iterate. To this end  we ﬁrst prove a
fundamental theorem for fully-corrective methods that formalizes the intuition that for such meth-
ods  a large function value should imply a large gradient at any sparse θ as well. This result is similar
to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]) 
as well as for the general loss functions. See Appendix C for a detailed proof.
Lemma 3. Consider a function f with RSC parameter given by L2s+s∗ (f ) = L and RSS parameter
given by α2s+s∗ (f ) = α. Let θ∗ = arg minθ (cid:107)θ(cid:107)0≤s∗ f (θ) with S∗ = supp(θ∗). Let St ⊆ [p] be
any subset of co-ordinates s.t. |St| ≤ s. Let θt = arg minθ supp(θ)⊆St f (θ). Then  we have:

2α(f (θt) − f (θ∗)) ≤ (cid:107)gt

St∪S∗(cid:107)2

2 − α2(cid:107)θt

St\S∗(cid:107)2

2

Two-stage Methods. We will  for now  concentrate on a family of two-stage fully corrective meth-
ods that contains popular compressive sensing algorithms like CoSaMP and Subspace Pursuit (see
Algorithm 2 for pseudocode). These algorithms have thus far been analyzed only under RIP con-
ditions for the least squares objective. Using our analysis framework developed in the previous
sections  we present a generic RSC/RSS-based analysis for general two-stage methods for arbitrary
loss functions. Our analysis shall use the following key observation that the the hard thresholding
step in two stage methods does not increase the objective function a lot.
We defer the analysis of partial hard thresholding methods to a later version of the paper. This family
includes the OMPR((cid:96)) method [17]  which is known to provide the best known RIP guarantees in
the compressive sensing setting. Using our proof techniques  we can show that this method offers
geometric convergence rates in the statistical setting as well.

Lemma 4. Let Zt ⊆ [n] and |Zt| ≤ q. Let βt = arg minβ supp(β)⊆Zt f (β) and (cid:98)θt = Pq(βt).

Then  the following holds:

f ((cid:98)θt) − f (βt) ≤ L

α

·

(cid:96)

s + (cid:96) − s∗ · (f (βt) − f (θ∗)).

6

(a)

(b)

(c)

(d)

Figure 1: A comparison of hard thresholding techniques (HTP) and projected gradient methods
(GraDeS) with L1 and greedy methods (FoBa) on sparse noisy linear regression tasks. 1(a) gives
the number of undiscovered elements from supp(θ∗) as label noise levels are increased. 1(b) shows
the variation in running times with increasing dimensionality p. 1(c) gives the variation in running
times (in logscale) when the true sparsity level s∗ is increased keeping p ﬁxed. HTP and GraDeS are
clearly much more scalable than L1 and FoBa. 1(d) shows the recovery properties of different IHT
methods under large condition number (κ = 50) setting as the size of projected set is increased.

(cid:107)(cid:98)θt − βt(cid:107)2

2

Proof. Let vt = ∇θf (βt). Then  using the RSS property we get:

f ((cid:98)θt) − f (βt) ≤ (cid:104)(cid:98)θt − βt  vt(cid:105) +
|s + (cid:96) − s∗|(cid:107)w − βt(cid:107)2
noting that supp((cid:98)θt) ⊆ Zt. ζ2 follows by Lemma 1 and the fact that (cid:107)w(cid:107)0 ≤ s∗. Now  using the

where w is any vector such that wZt
RSC property and the fact that ∇θf (βt) = 0  we have:

= 0 and (cid:107)w(cid:107)0 ≤ s∗. ζ1 follows by observing vt

2 
(7)
= 0 and by

(cid:107)(cid:98)θt − βt(cid:107)2

ζ2≤ L
2

2

|(cid:96)|

L
2

ζ1=

L
2

Zt

(cid:107)w − βt(cid:107)2

2 ≤ f (βt) − f (w) ≤ f (βt) − f (θ∗).

α
2

(8)

The result now follows by combining (7) and (8).
Theorem 4. Let f have RSC and RSS parameters given by α2s+s∗ (f ) = α and L2s+(cid:96)(f ) =
L resp. Call Algorithm 2 with f  (cid:96) ≥ s∗ and s ≥ 4 L2
α2 s∗. Also let θ∗ =
arg minθ (cid:107)θ(cid:107)0≤s∗ f (θ). Then  the τ-th iterate of Algorithm 2  for τ = O( L
) satisﬁes:

α2 (cid:96) + s∗ − (cid:96) ≥ 4 L2

α · log( f (θ0)



f (θτ ) − f (θ∗) ≤ .

See Appendix C for a detailed proof.

6 Experiments

We conducted simulations on high dimensional sparse linear regression problems to verify our pre-
dictions. Our experiments demonstrate that hard thresholding and projected gradient techniques can
not only offer recovery in stochastic setting  but offer much more scalable routines for the same.
Data: Our problem setting is identical to the one described in the previous section. We ﬁxed a
parameter vector ¯θ by choosing s∗ random coordinates and setting them randomly to ±1 values.
Data samples were generated as Zi = (Xi  Yi) where Xi ∼ N (0  Ip) and Yi = (cid:104) ¯θ  Xi(cid:105) + ξi where
ξi ∼ N (0  σ2). We studied the effect of varying dimensionality p  sparsity s∗  sample size n and
label noise level σ on the recovery properties of the various algorithms as well as their run times.
We chose baseline values of p = 20000  s∗ = 100  σ = 0.1  n = fo · s∗ log p where fo is the
oversampling factor with default value fo = 2. Keeping all other quantities ﬁxed  we varied one of
the quantities and generated independent data samples for the experiments.
Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14] 
GraDeS [13] (or IHT [12])  CoSaMP [15]  OMPR [17] and SP [16]. We compared them with a
standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso prob-
lem and a greedy method FoBa [24] for the same.

7

00.10.20.30.4020406080Noise level (sigma)Support Recovery Error HTPGraDeSL1FoBa0.511.522.5x 104050100150200Dimensionality (p)Runtime (sec) HTPGraDeSL1FoBa010020030040050010−310−2100102104Sparsity (s*)Runtime (sec) HTPGraDeSL1FoBa80100120140160010203040Projected Sparsity (s)Support Recovery Error CoSaMPHTPGraDeSEvaluation Metrics: For the baseline noise level σ = 0.1  we found that all the algorithms were
able to recover the support set within an error of 2%. Consequently  our focus shifted to running
times for these experiments. In the experiments where noise levels were varied  we recorded  for
each method  the number of undiscovered support set elements.
Results: Figure1 describes the results of our experiments in graphical form. For sake of clarity
we included only HTP  GraDeS  L1 and FoBa results in these graphs. Graphs for the other algo-
rithms CoSaMP  SP and OMPR can be seen in the supplementary material. The graphs indicate that
whereas hard thresholding techniques are equally effective as L1 and greedy techniques for recov-
ery in noisy settings  as indicated by Figure1(a)  the former can be much more efﬁcient and scalable
than the latter. For instance  as Figure1(b)  for the base level of p = 20000  HTP was 150× faster
than the L1 method. For higher values of p  the runtime gap widened to more than 350×. We also
note that in both these cases  HTP actually offered exact support recovery whereas L1 was unable to
recover 2 and 4 support elements respectively.
Although FoBa was faster than L1 on Figure1(b) experiments  it was still slower than HTP by 50×
and 90× for p = 20000 and 25000 respectively. Moreover  due to its greedy and incremental
nature  FoBa was found to suffer badly in settings with larger true sparsity levels. As Figure 1(c)
indicates  for even moderate sparsity levels of s∗ = 300 and 500  FoBa is 60 − 75× slower than
HTP. As mentioned before  the reason for this slowdown is the greedy approach followed by FoBa:
whereas HTP took less than 5 iterations to converge for these two problems  FoBa spend 300 and
500 iterations respectively. GraDeS was found to offer much lesser run times in comparison being
slower than HTP by 30 − 40× for larger values of p and 2 − 5× slower for larger values of s∗.
Experiments on badly conditioned problems. We also ran experiments to verify the performance
of IHT algorithms in high condition number setting. Values of p  s∗ and σ were kept at baseline
levels. After selecting the optimal parameter vector ¯θ  we selected s∗/2 random coordinates from
its support and s∗/2 random coordinates outside its support and constructed a covariance matrix
with heavy correlations between these chosen coordinates. The condition number of the resulting
matrix was close to 50. Samples were drawn from this distribution and the recovery properties of
the different IHT-style algorithms was observed as the projected sparsity levels s were increased.
Our results (see Figure 1(d)) corroborate our theoretical observation that these algorithms show
a remarkable improvement in recovery properties for ill-conditioned problems with an enlarged
projection size.

7 Discussion and Conclusions

In our work we studied iterative hard thresholding algorithms and showed that these techniques
can offer global convergence guarantees for arbitrary  possibly non-convex  differentiable objective
functions  which nevertheless satisfy Restricted Strong Convexity/Smoothness (RSC/RSM) condi-
tions. Our results apply to a large family of algorithms that includes existing algorithms such as
IHT  GraDeS  CoSaMP  SP and OMPR. Previously the analyses of these algorithms required strin-
gent RIP conditions that did not allow the (restricted) condition number to be larger than universal
constants speciﬁc to these algorithms.
Our basic insight was to relax this stringent requirement by running these iterative algorithms with
an enlarged support size. We showed that guarantees for high-dimensional M-estimation follow
seamlessly from our results by invoking results on RSC/RSM conditions that have already been
established in the literature for a variety of statistical settings. Our theoretical results put hard
thresholding methods on par with those based on convex relaxation or greedy algorithms. Our
experimental results demonstrate that hard thresholding methods outperform convex relaxation and
greedy methods in terms of running time  sometime by orders of magnitude  all the while offering
competitive or better recovery properties.
Our results apply to sparsity and low rank structure  arguably two of the most commonly used
structures in high dimensional statistical learning problems. In future work  it would be interesting
to generalize our algorithms and their analyses to more general structures. A uniﬁed analysis for
general structures will probably create interesting connections with existing uniﬁed frameworks
such as those based on decomposability [5] and atomic norms [25].

8

References
[1] Peter B¨uhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods  theory and appli-

cations. Springer  2011.

[2] Sahand Negahban  Martin J Wainwright  et al. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. The Annals of Statistics  39(2):1069–1097  2011.

[3] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Minimax rates of estimation for high-dimensional

linear regression over (cid:96)q-balls. Information Theory  IEEE Transactions on  57(10):6976–6994  2011.

[4] Angelika Rohde and Alexandre B Tsybakov. Estimation of high-dimensional low-rank matrices. The

Annals of Statistics  39(2):887–930  2011.

[5] Sahand N Negahban  Pradeep Ravikumar  Martin J Wainwright  Bin Yu  et al. A uniﬁed framework
Statistical Science 

for high-dimensional analysis of M-estimators with decomposable regularizers.
27(4):538–557  2012.

[6] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing 

24(2):227–234  1995.

[7] Ji Liu  Ryohei Fujimaki  and Jieping Ye. Forward-backward greedy algorithms for general convex smooth
functions over a cardinality constraint. In Proceedings of The 31st International Conference on Machine
Learning  pages 503–511  2014.

[8] Ali Jalali  Christopher C Johnson  and Pradeep D Ravikumar. On learning discrete graphical models using

greedy methods. In NIPS  pages 1935–1943  2011.

[9] Shai Shalev-Shwartz  Nathan Srebro  and Tong Zhang. Trading accuracy for sparsity in optimization

problems with sparsity constraints. SIAM Journal on Optimization  20(6):2807–2832  2010.

[10] Yurii Nesterov.

Introductory lectures on convex optimization: A basic course  volume 87 of Applied

Optimization. Springer  2004.

[11] P. Loh and M. J. Wainwright. Regularized M-estimators with nonconvexity: Statistical and algorithmic

theory for local optima  2013. arXiv:1305.2436 [math.ST].

[12] Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Applied

and Computational Harmonic Analysis  27(3):265 – 274  2009.

[13] Rahul Garg and Rohit Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse

recovery with restricted isometry property. In ICML  2009.

[14] Simon Foucart. Hard thresholding pursuit: an algorithm for compressive sensing. SIAM J. on Num. Anal. 

49(6):2543–2563  2011.

[15] Deanna Needell and Joel A. Tropp. CoSaMP: Iterative Signal Recovery from Incomplete and Inaccurate

Samples. Appl. Comput. Harmon. Anal.  26:301–321  2008.

[16] Wei Dai and Olgica Milenkovic. Subspace pursuit for compressive sensing signal reconstruction. IEEE

Trans. Inf. Theory  55(5):22302249  2009.

[17] Prateek Jain  Ambuj Tewari  and Inderjit S. Dhillon. Orthogonal matching pursuit with replacement. In

Annual Conference on Neural Information Processing Systems  2011.

[18] Sohail Bahmani  Bhiksha Raj  and Petros T Boufounos. Greedy sparsity-constrained optimization. The

Journal of Machine Learning Research  14(1):807–841  2013.

[19] Xiaotong Yuan  Ping Li  and Tong Zhang. Gradient hard thresholding pursuit for sparsity-constrained

optimization. In Proceedings of The 31st International Conference on Machine Learning  2014.

[20] Yuchen Zhang  Martin J. Wainwright  and Michael I. Jordan. Lower bounds on the performance of

polynomial-time algorithms for sparse linear regression. arXiv:1402.1918  2014.

[21] P. Loh and M. J. Wainwright. High-dimension regression with noisy and missing data: Provable guaran-

tees with non-convexity. Annals of Statistics  40(3):1637–1664  2012.

[22] Alekh Agarwal  Sahand N. Negahban  and Martin J. Wainwright. Fast global convergence of gradient

methods for high-dimensional statistical recovery. Annals of Statistics  40(5):2452—2482  2012.

[23] Mark Schmidt. Graphical Model Structure Learning with L1-Regularization. PhD thesis  University of

British Columbia  2010.

[24] Tong Zhang. Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations. IEEE

Trans. Inf. Theory  57:4689–4708  2011.

[25] Venkat Chandrasekaran  Benjamin Recht  Pablo A Parrilo  and Alan S Willsky. The convex geometry of

linear inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

9

,Prateek Jain
Ambuj Tewari
Purushottam Kar