2018,Mean-field theory of graph neural networks in graph partitioning,A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks  the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner  whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover  whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions  a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments.,Mean-ﬁeld theory of graph neural networks

in graph partitioning

Tatsuro Kawamoto  Masashi Tsubaki
Artiﬁcial Intelligence Research Center 

National Institute of Advanced Industrial Science and Technology 

2-3-26 Aomi  Koto-ku  Tokyo  Japan

{kawamoto.tatsuro  tsubaki.masashi}@aist.go.jp

Department of Mathematical and Computing Science  Tokyo Institute of Technology 

Tomoyuki Obuchi

2-12-1 Ookayama Meguro-ku Tokyo  Japan

obuchi@c.titech.ac.jp

Abstract

A theoretical performance analysis of the graph neural network (GNN) is pre-
sented. For classiﬁcation tasks  the neural network approach has the advantage
in terms of ﬂexibility that it can be employed in a data-driven manner  whereas
Bayesian inference requires the assumption of a speciﬁc model. A fundamental
question is then whether GNN has a high accuracy in addition to this ﬂexibil-
ity. Moreover  whether the achieved performance is predominately a result of the
backpropagation or the architecture itself is a matter of considerable interest. To
gain a better insight into these questions  a mean-ﬁeld theory of a minimal GNN
architecture is developed for the graph partitioning problem. This demonstrates a
good agreement with numerical experiments.

1

Introduction

Deep neural networks have been subject to signiﬁcant attention concerning many tasks in machine
learning  and a plethora of models and algorithms have been proposed in recent years. The appli-
cation of the neural network approach to problems on graphs is no exception and is being actively
studied  with applications including social networks and chemical compounds [1  2]. A neural net-
work model on graphs is termed a graph neural network (GNN) [3]. While excellent performances
of GNNs have been reported in the literature  many of these results rely on experimental studies 
and seem to be based on the blind belief that the nonlinear nature of GNNs leads to such strong per-
formances. However  when a deep neural network outperforms other methods  the factors that are
really essential should be clariﬁed: Is this thanks to the learning of model parameters  e.g.  through
the backpropagation [4]  or rather the architecture of the model itself? Is the choice of the archi-
tecture predominantly crucial  or would even a simple choice perform sufﬁciently well? Moreover 
does the GNN generically outperform other methods?
To obtain a better understanding of these questions  not only is empirical knowledge based on bench-
mark tests required  but also theoretical insights. To this end  we develop a mean-ﬁeld theory of
GNN  focusing on a problem of graph partitioning. The problem concerns a GNN with random
model parameters  i.e.  an untrained GNN. If the architecture of the GNN itself is essential  then the
performance of the untrained GNN should already be effective. On the other hand  if the ﬁne-tuning
of the model parameters via learning is crucial  then the result for the untrained GNN is again useful
to observe the extent to which the performance is improved.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Architecture of the GNN considered in this paper.

Table 1: Comparison of various methods under the framework of Eq. (2).

algorithm
untrained GNN
trained GNN [5]
Spectral method
EM + BP

φ(x) W t
domain M ϕ(x)
I(x)
tanh
A
I(cid:0)L ReLu
I(x)
L
I(x)
I(x)
B softmax log(x)

V
V
V
⃗E

QR

learned

bt

random omitted
trained

fW t; btg update

not trained

omitted trained via backprop.
updated at each layer
learned via M-step

learned

/

For a given graph G = (V; E)  where V is the set of vertices and E is the set of (undirected)
edges  the graph partitioning problem involves assigning one out of K group labels to each vertex.
Throughout this paper  we restrict ourselves to the case of two groups (K = 2). The problem setting
for graph partitioning is relatively simple compared with other GNN applications. Thus  it is suitable
as a baseline for more complicated problems. There are two types of graph partitioning problem:
One is to ﬁnd the best partition for a given graph under a certain objective function. The other is
to assume that a graph is generated by a statistical model  and infer the planted (i.e.  preassigned)
group labels of the generative model. Herein  we consider the latter problem.
Before moving on to the mean-ﬁeld theory  we ﬁrst clarify the algorithmic relationship between
GNN and other methods of graph partitioning.

2 Graph neural network and its relationship to other methods

The goal of this paper is to examine the graph partitioning performance using a minimal GNN
architecture. To this end  we consider a GNN with the following feedforward dynamics. Each vertex
is characterized by a D-dimensional feature vector whose elements are xi(cid:22) (i 2 V   (cid:22) 2 f1; : : : ; Dg) 
and the state matrix X = [xi(cid:22)] obeys

∑

(

)

xt+1
i(cid:22) =

Aijϕ

j(cid:23)

xt
j(cid:23)

W t

(cid:23)(cid:22) + bt

i(cid:22):

(1)

Throughout this paper  the layers of the GNN are indexed by t 2 f0; : : : ; Tg. Furthermore  ϕ(x) is
a nonlinear activation function  bt = [bt
(cid:23)(cid:22)] is a linear transform
that operates on the feature space. To infer the group assignments  a D (cid:2) K matrix W out of the
readout classiﬁer is applied at the end of the last layer. Although there is no restriction on ϕ in
our mean-ﬁeld theory  we adopt ϕ = tanh as a speciﬁc choice in the experiment below. As there
is no detailed attribute on each vertex  the initial state X 0 is set to be uniformly random  and the
adjacency matrix A is the only input in the present case. For the graphs with vertex attributes  deep
neural networks that utilize such information [6  7  8] are also proposed.

i(cid:22)] is a bias term1  and W t = [W t

1The bias term is only included in this section  and will be omitted in later sections.

2

A++= (xt   xt   ...   xt )Layer tiA(cid:668)(cid:668)(cid:668)(cid:668)(cid:668)XtNϕjk1111ϕ•Wt+1ReadoutclassifiersFeature vector of iLayer t + 1•WtXt+1ii1i2iD(p   p   ...   p )i1i2iKLayer out(cid:668)ij(cid:668)(cid:668)XT(cid:668)(cid:668)(cid:668)(cid:668)(cid:668)(cid:668)(cid:668)•WoutThe set of bias terms fbtg  the linear transforms fW tg in the intermediate layers  and W out for
the readout classiﬁer are initially set to be random. These are updated through the backpropagation
using the training set. In the semi-supervised setting  (real-world) graphs in which only a portion of
the vertices are correctly labeled are employed as a training set. On the other hand  in the case of
unsupervised learning  graph instances of a statistical model can be employed as the training set.
The GNN architecture described above can be thought of as a special case of the following more
general form:

)

ϕ(xt

j(cid:23))W t
(cid:23)(cid:22)

+ bt

i(cid:22);

(2)

∑

(∑

xt+1
i(cid:22) =

Mij φ

j

(cid:23)

p

p

d1; : : : ;

(cid:0)1=2AD

(cid:0)1=2 (cid:17) diag(

(cid:0)1=2  where D

where φ(x) is another activation function. With different choices for the matrix and activation
functions shown in Table 1  various algorithm can be obtained. Equation (1) is recovered by setting
Mij = Aij and φ(x) = I(x) (where I(x) is the identity function)  while [5] employed a Laplacian-
like matrix M = I (cid:0) L = D
dN ) (di is the
degree of vertex i) and L is called the normalized Laplacian [9].
The spectral method using the power iteration is obtained in the limit where ϕ(x) and φ(x) are linear
and bt is absent.2 For the simultaneous power iteration that extracts the leading K eigenvectors  the
state matrix X 0 is set as an N (cid:2)K matrix whose column vectors are mutually orthogonal. While the
normalized Laplacian L is commonly adopted for M  there are several other choices [9  10  11].3
At each iteration  the orthogonality condition is maintained via QR decomposition [12]  i.e.  for
acts as W t. Rt is a D (cid:2) D upper triangular matrix
Zt := M X t  X t+1 = ZtR
that is obtained by the QR decomposition of Zt. Therefore  rather than training W t  it is determined
at each layer based on the current state.
The belief propagation (BP) algorithm (also called the message passing algorithm) in Bayesian
inference also falls under the framework of Eq. (2). While the domain of the state consists of the
vertices (i; j 2 V ) for GNNs  this algorithm deals with the directed edges i ! j 2 ⃗E  where ⃗E
is obtained by putting directions to every undirected edge. In this case  the state xt
(cid:27);i!j represents
the logarithm of the marginal probability that vertex i belongs to the group (cid:27) with the missing
information of vertex j at the tth iteration. With the choice of matrix and activation functions shown
in Table 1 (EM+BP)  Eq. (2) becomes exactly the update equation of the BP algorithm4 [13]. The
matrix M = B = [Bj!k;i!j] is the so-called non-backtracking matrix [14]  and the softmax
function represents the normalization of the state xt

(cid:0)1
t

  where R

(cid:0)1
t

(cid:27);i!j.

The BP algorithm requires the model parameters W t and bt as inputs. For example  when the
expectation-maximization (EM) algorithm is considered  the BP algorithm comprises half (the E-
step) of the algorithm. The parameter learning of the model is conducted in the other half (the
M-step)  which can be performed analytically using the current result of the BP algorithm. Here 
W t and bt are the estimates of the so-called density matrix (or afﬁnity matrix) and the external ﬁeld
resulting from messages from non-edges [13]  respectively  and are common for every t. Therefore 
the differences between the EM algorithm and GNN are summarized as follows. While there is an
analogy between the inference procedures  in the EM algorithm the parameter learning of the model
is conducted analytically  at the expense of the restrictions of the assumed statistical model. On
the other hand  in GNNs the learning is conducted numerically in a data-driven manner [15]  for
example by backpropagation. While we will shed light on the detailed correspondence in the case
of graph partitioning here  the relationship between GNN and BP is also mentioned in [16  17].

3 Mean-ﬁeld theory of the detectability limit

3.1 Stochastic block model and its detectability limit

We analyze the performance of an untrained GNN on the stochastic block model (SBM). This is a
random graph model with a planted group structure  and is commonly employed as the generative

2Alternatively  it can be regarded that diag(bt
3A matrix const:I is sometimes added in order to shift the eigenvalues.
4Precisely speaking  this is the BP algorithm in which the stochastic block model (SBM) is assumed as the

iD) is added to W t when bt has common rows.

i1; : : : ; bt

generative model. The SBM is explained below.

3

model of an inference-based graph clustering algorithm [13  18  19  20]. The SBM is deﬁned as
follows. We let jV j = N  and each of the vertices has a preassigned group label (cid:27) 2 f1; : : : ; Kg 
i.e.  V = [(cid:27)V(cid:27). We deﬁne V(cid:27) as the set of vertices in a group (cid:27)  (cid:13)(cid:27) (cid:17) jV(cid:27)j=N  and (cid:27)i represents
the planted group assignment of vertex i. For each pair of vertices i 2 V(cid:27) and j 2 V(cid:27)′  an edge is
generated with probability (cid:26)(cid:27)(cid:27)′  which is an element of the density matrix. Throughout this paper 
(cid:0)1)  so that the resulting graph has a constant average degree  or in
we assume that (cid:26)(cid:27)(cid:27)′ = O(N
other words the graph is sparse. We denote the average degree by c. Therefore  the adjacency matrix
A = [Aij] of the SBM is generated with probability

∏

(

)1(cid:0)Aij

p(A) =

i<j

(cid:26)Aij
(cid:27)i(cid:27)j

1 (cid:0) (cid:26)Aij

(cid:27)i(cid:27)j

:

(3)

′  the SBM is nothing but the Erd˝os-Rényi random
When (cid:26)(cid:27)(cid:27)′ is the same for all pairs of (cid:27) and (cid:27)
graph. Clearly  in this case no algorithm can infer the planted group assignments better than random
chance. Interestingly  even when (cid:26)(cid:27)(cid:27)′ is not constant there exists a limit for the strength of the group
structure  below which the planted group assignments cannot be inferred better than chance. This
is called the detectability limit. Consider the SBM consisting of two equally-sized groups that is
′  which is often referred to as the
parametrized as (cid:26)(cid:27)(cid:27)′ = (cid:26)in for (cid:27) = (cid:27)
symmetric SBM. In this case  it is rigorously known [21  22  23] that detection is impossible by any
algorithm for the SBM with a group structure weaker than

′ and (cid:26)(cid:27)(cid:27)′ = (cid:26)out for (cid:27) ̸= (cid:27)

(4)
where ϵ (cid:17) (cid:26)out=(cid:26)in. However  it is important to note that this is the information-theoretic limit 
and the achievable limit for a speciﬁc algorithm may not coincide with Eq. (4). For this reason  we
investigate the algorithmic detectability limit of the GNN here.

c + 1);

ϵ = (

p

p
c (cid:0) 1)=(

3.2 Dynamical mean-ﬁeld theory

In an untrained GNN  each element of the matrix W t is randomly determined according to the
(cid:24) N (0; 1=D). We assume that the feature dimension D is
Gaussian distribution at each t  i.e.  W t
sufﬁciently large  but D=N ≪ 1. Let us consider a state xt
(cid:23)(cid:22)
(cid:27)(cid:22) that represents the average state within
a group  i.e.  xt
(cid:27)(cid:22)] is expressed
∑
as

i(cid:22). The probability distribution that xt = [xt
xt

⟨∏

(cid:17) ((cid:13)(cid:27)N )

∑

(cid:0)1

(cid:27)(cid:22)

1A⟩

∑
0@xt+1

i2V(cid:27)

(cid:14)

(cid:27)(cid:22)

P (xt+1) =

(cid:27)(cid:22)

(cid:0) 1
(cid:13)(cid:27)N

i2V(cid:27)

j(cid:23)

Aijϕ(xt

j(cid:23))W t
(cid:23)(cid:22)

;

(5)

A;W t;X t

where ⟨: : :⟩
A;W t;X t denotes the average over the graph A  the random linear transform W t  and
∫
the state X t of the previous layer. Using the Fourier representation  the normalization condition of
Eq. (5) is expressed as
D^xt+1Dxt+1e

(cid:27)(cid:22) (cid:13)(cid:27)^xt+1

∑

∑

{

(cid:0)L0

⟨

⟩

; (6)

1 =

L1

e

(cid:27)(cid:22) xt+1
(cid:27)(cid:22)
ij AijW t

(cid:22)(cid:23)

(cid:23)(cid:22)^xt+1

(cid:27)i(cid:22)ϕ(xt

j(cid:23)):

∑
L0 =
L1 = 1

N

A;W t;X t ;

is conjugate to xt+1
(cid:27)(cid:22)  

and D^xt+1Dxt+1 (cid:17)

∏

where ^xt+1
(cid:27)(cid:22)

is an auxiliary variable that

(cid:27)(cid:22)((cid:13)(cid:27)d^xt+1

(cid:27)(cid:22) dxt+1

(cid:27)(cid:22) =2(cid:25)i).

∑

1

After taking the average of the symmetric SBM over A as well as the average over W t in the
stationary limit with respect to t  the following self-consistent equation is obtained with respect to
the covariance matrix C = [C(cid:27)(cid:27)′] of x = xt:

C(cid:27)(cid:27)′ =

(7)
where B(cid:27)(cid:27)′ (cid:17) N (cid:13)(cid:27)(cid:26)(cid:27)(cid:27)′(cid:13)(cid:27)′. The detailed derivation can be found in the supplemental material. The
reader may notice that the above expression resembles the recursive equations in [24  25]. However 
it should be noted that Eq. (7) is not obtained as an exact closed equation. The derivation relies

ϕ(x~(cid:27))ϕ(x~(cid:27)′);

B(cid:27) ~(cid:27)B(cid:27)′ ~(cid:27)′

(cid:13)(cid:27)(cid:13)(cid:27)′

det C

~(cid:27)~(cid:27)′

(cid:0) 1
2 x⊤C
p

(cid:0)1x

dx e
(2(cid:25)) N

2

∫

4

Figure 2: Performance of the untrained GNN evaluated by the behavior of the covariance matrix
C. (a) Detectability phase diagram of the SBM. The solid line represents the mean-ﬁeld estimate
by Eq. (7)  the dashed line represents the phase boundary of the spectral method (see Section 5 for
details)  and the dark shaded region represents the region above Eq. (4). The region containing points
is that where the covariance gap C11 (cid:0) C12 is signiﬁcantly larger than the gaps in the information-
theoretically undetectable region. (b) The curves of the covariances C11 and C12 of the SBMs with
c = 8: The mean-ﬁeld estimates (gray lines that have larger values) and curves obtained by the
regression of the experimental data (purple lines with smaller values).

mainly on the assumption that the macroscopic random variable xt dominates the behavior of the
state X t. It is numerically conﬁrmed that this assumption appears plausible. This type of analysis
is called dynamical mean-ﬁeld theory (or the Martin-Siggia-Rose formalism) [26  27  28  29].
When the correlation within a group C(cid:27)(cid:27) is equal to the correlation between groups C(cid:27)(cid:27)′ ((cid:27) ̸= (cid:27)
′) 
the GNN is deemed to have reached the detectability limit. Beyond the detectability limit  Eq. (7) is
no longer a two-component equation  but is reduced to an equation with respect to the variance of
one indistinguishable group.
The accuracy of our mean-ﬁeld estimate is examined in Fig. 2  using the covariance matrix C that
we obtained directly from the numerical experiment. In the detectability phase diagram in Fig. 2a 
the region that has a covariance gap C11 (cid:0) C12 > (cid:22)g + 2(cid:27)g for at least one graph instance among 30
samples is indicated by the dark purple points  and the region with C11(cid:0) C12 > (cid:22)g + (cid:27)g is indicated
by the pale purple points  where (cid:22)g and (cid:27)g are the mean and standard deviation  respectively  of the
covariance gap in the information-theoretically undetectable region. In Fig. 2b  the elements of the
covariance matrix are compared for the SBM with c = 8. The consistency of our mean-ﬁeld estimate
is examined for a speciﬁc implementation in Section 5.

4 Normalized mutual information error function

The previous section dealt with the feedforward process of an untrained GNN. By employing a clas-
siﬁer such as the k-means method [30]  W out is not required  and the inference of the SBM can be
performed without any training procedure. To investigate whether the training signiﬁcantly improves
the performance  the algorithm that updates the matrices fW tg and W out must be speciﬁed.
The cross-entropy error function is commonly employed to train W out for a classiﬁcation task.
However  this error function unfortunately cannot be directly applied to the present case. Note that
the planted group assignment of SBM is invariant under a global permutation of the group labels.
In other words  as long as the set of vertices in the same group share the same label  the label itself
can be anything. This is called the identiﬁability problem [31]. The cross-entropy error function is
not invariant under a global label permutation  and thus the classiﬁer cannot be trained unless the
degrees of freedom are constrained. A possible brute force approach is to explicitly evaluate all the
permutations in the error function [32]  although this obviously results in a considerable computa-
tional burden unless the number of groups is very small. Note also that semi-supervised clustering
does not suffer from the identiﬁability issue  because the permutation symmetry is explicitly broken.

5

(cid:21)(cid:23)(cid:25)(cid:18)(cid:17)(cid:68)(cid:17)(cid:15)(cid:17)(cid:17)(cid:15)(cid:18)(cid:17)(cid:15)(cid:19)(cid:17)(cid:15)(cid:20)(cid:17)(cid:15)(cid:21)(cid:17)(cid:15)(cid:22)(cid:54)(cid:79)(cid:69)(cid:70)(cid:85)(cid:70)(cid:68)(cid:85)(cid:66)(cid:67)(cid:77)(cid:70)(cid:37)(cid:70)(cid:85)(cid:70)(cid:68)(cid:85)(cid:66)(cid:67)(cid:77)(cid:70)(cid:17)(cid:15)(cid:17)(cid:17)(cid:15)(cid:18)(cid:17)(cid:15)(cid:19)(cid:17)(cid:15)(cid:20)(cid:17)(cid:15)(cid:21)(cid:17)(cid:19)(cid:17)(cid:21)(cid:17)(cid:23)(cid:17)(cid:36)(cid:80)(cid:87)(cid:66)(cid:83)(cid:74)(cid:66)(cid:79)(cid:68)(cid:70)(cid:84)(cid:46)(cid:70)(cid:66)(cid:79)(cid:14)(cid:112)(cid:70)(cid:77)(cid:69)(cid:16)(cid:38)(cid:89)(cid:81)(cid:70)(cid:83)(cid:74)(cid:78)(cid:70)(cid:79)(cid:85)(cid:36)(cid:46)(cid:39)(cid:18)(cid:18)(cid:36)(cid:46)(cid:39)(cid:18)(cid:19)(cid:36)(cid:70)(cid:89)(cid:81)(cid:18)(cid:18)(cid:36)(cid:70)(cid:89)(cid:81)(cid:18)(cid:19)(a)(b)Here  we instead propose the use of the normalized mutual information (NMI) as an error function
for the readout classiﬁer. The NMI is a comparison measure of two group assignments  which
naturally eliminates the permutation degrees of freedom. Let (cid:27) = f(cid:27)i = (cid:27)ji 2 V(cid:27)g be the labels
of the planted group assignments  and ^(cid:27) = f(cid:27)i = ^(cid:27)ji 2 V^(cid:27)g be the labels of the estimated group
assignments. First  the (unnormalized) mutual information is deﬁned as

K∑

K∑

I((cid:27); ^(cid:27)) =

P(cid:27) ^(cid:27) log

(cid:27)=1

^(cid:27)=1

P(cid:27) ^(cid:27)
P(cid:27)P^(cid:27)

;

(8)

where the joint probability P(cid:27) ^(cid:27) is the fraction of vertices that belong to the group (cid:27) in the planted
assignment and the group ^(cid:27) in the estimated assignment. Furthermore  P(cid:27) and P^(cid:27) are the marginals
of P(cid:27)^(cid:27)  and we let H((cid:27)) and H( ^(cid:27)) be the corresponding entropies. The NMI is deﬁned by

NMI((cid:27); ^(cid:27)) (cid:17)

2I((cid:27); ^(cid:27))

H((cid:27)) + H( ^(cid:27))

:

(9)

We adopt this measure as the error function for the readout classiﬁer. For the resulting state xT
i(cid:22) 
the estimated assignment probability pi(cid:27) that vertex i belongs to the group (cid:27) is deﬁned as pi(cid:27) (cid:17)
softmax(ai(cid:27))  where ai(cid:27) =

(cid:22) xi(cid:22)W out

∑

P(cid:27) ^(cid:27) =

1
N

∑

P(cid:27) =

^(cid:27)

In summary 

N∑

i=1

∑

(cid:22)(cid:27) . Each element of the NMI is then obtained as
P (i 2 V(cid:27); i 2 V^(cid:27)) =
∑

N∑

i2V(cid:27)

1
N

pi^(cid:27);

P(cid:27) ^(cid:27) = (cid:13)(cid:27);

P^(cid:27) =

P(cid:27) ^(cid:27) =

pi^(cid:27):

∑

(cid:27)

∑

(cid:27) ^(cid:27) P(cid:27) ^(cid:27) log P(cid:27)^(cid:27)

1
N

i=1

∑

)

:

(

∑

1 (cid:0)

(10)

(11)

NMI ([P(cid:27)^(cid:27)]) = 2

(cid:27) (cid:13)(cid:27) log (cid:13)(cid:27) +

(cid:27)^(cid:27) P(cid:27) ^(cid:27) log

(cid:27) P(cid:27) ^(cid:27)

This measure is permutation invariant  because the NMI counts the label co-occurrence patterns for
each vertex in (cid:27) and ^(cid:27).

5 Experiments

First  the consistency between our mean-ﬁeld theory and a speciﬁc implementation of an untrained
GNN is examined. The performance of the untrained GNN is evaluated by drawing phase diagrams.
For the SBMs with various values for the average degree c and the strength of group structure ϵ  the
overlap  i.e.  the fraction of vertices that coincide with their planted labels  is calculated. Afterward 
it is investigated whether a signiﬁcant improvement is achieved through the parameter learning of
the model. Note that because even a completely random clustering can correctly infer half of the
labels on average  the minimum of the overlap is 0:5.5 As mentioned above  we adopt ϕ = tanh as
the speciﬁc choice of activation function.

5.1 Untrained GNN with the k-means classiﬁer

We evaluate the performance of the untrained GNN in which the resulting state X is read out using
the k-means (more precisely k-means++ [30]) classiﬁer. In this case  no parameter learning takes
place. We set the dimension of the feature space to D = 100 and the number of layers to T = 100 
and each result represents the average over 30 samples.
Figure 3a presents the corresponding phase diagram. The overlap is indicated by colors  and the
solid line represents the detectability limit estimated by Eq. (7). The dashed line represents the
mean-ﬁeld estimate of the detectability limit of the spectral method6 [33  34]  and the shaded area

5For this reason  the overlap is sometimes standardized such that the minimum equals zero.
6Again  there are several choices for the matrix to be adopted in the spectral method. However  the Lapla-
cians and modularity matrix  for example  have the same detectability limit when the graph is regular or the
average degree is sufﬁciently large.

6

Figure 3: Performance of the untrained GNN using the k-means classiﬁer. (a) The same detectability
phase diagram as in Fig. 2. The heatmap represents the overlap obtained using the untrained GNN.
(b) The overlaps of the SBM with c = 8: The light shaded area represents the region above the
estimate using Eq. (7)  the dashed line represents the detectability limit of the spectral method  and
the dark shaded region represents the information-theoretically undetectable region.

represents the region above which the inference is information-theoretically impossible. It is known
that the detectability limit of the BP algorithm [13] coincides with this information-theoretic limit so
long as the model parameters are correctly learned. For the present model  it is also known that the
EM algorithm can indeed learn these parameters [35]. Note that it is natural that a Bayesian method
will outperform others as long as a consistent model is used  whereas it may perform poorly if the
assumed model is not consistent.
It can be observed that our mean-ﬁeld estimate exhibits a good agreement with the numerical exper-
iment. For a closer view  the overlaps of multiple graph sizes with c = 8 are presented in Fig. 3b.
(cid:3) (cid:25) 0:33  and this appears to coincide with the point at which the overlap
(
For c = 8  the estimate is ϵ
is almost 0:5. It should be noted that the performance can vary depending on the implementation
details. For example  while the k-means method is performed to X T in the present experiment  it
can instead be performed to ϕ
. An experiment concerning such a case is presented in the
supplemental material.

)

X T

5.2 GNN with backpropagation and a trained classiﬁer

Now  we consider a trained GNN  and compare its performance with the untrained one. A set
of SBM instances is provided as the training set. This consists of 1; 000 SBM instances with
N = 5; 000  where an average degree c 2 f3; 4; : : : ; 10g and strength of the group structure
ϵ 2 f0:05; 0:1; : : : ; 0:5g are adopted. For the validation (development) set  100 graph instances
of the same SBMs are provided. Finally  the SBMs with various values of ϵ and the average degree
c = 8 are provided as the test set.
We evaluated the performance of a GNN trained by backpropagation. We implemented the GNN
using Chainer (version 3.2.0) [36]. As in the previous section  the dimension of the feature space is
set to D = 100  and various numbers of layers are examined. For the error function of the readout
classiﬁer  we adopted the NMI error function described in Section 4. The model parameters are
optimized using the default setting of the Adam optimizer [37] in Chainer. Although we examined
various optimization procedures for ﬁne-tuning  the improvement was hardly observable.
We also employ residual networks (ResNets) [38] and batch normalization (BN) [39]. These are
also adopted in [32]. The ResNet imposes skip (or shortcut) connections on a deep network  i.e. 
xt+1
i(cid:22)   where s is the number of layers skipped  and is set as s = 5.
i(cid:22) =
The BN layer  which standardizes the distribution of the state X t  is placed at each intermediate
layer t. Finally  we note that the parameters of deep GNNs (e.g.  T > 25) cannot be learned
correctly without using the ResNet and BN techniques.
The results using the GNN trained as above are illustrated in Fig. 4. First  it can be observed from
Fig. 4a that a deep structure is important for a better accuracy. For sufﬁciently deep networks  the

(cid:23)(cid:22) + xt(cid:0)s
W t

∑

(

)

j(cid:23) Aijϕ

xt
j(cid:23)

7

(cid:21)(cid:23)(cid:25)(cid:18)(cid:17)(cid:17)(cid:15)(cid:18)(cid:17)(cid:15)(cid:19)(cid:17)(cid:15)(cid:20)(cid:17)(cid:15)(cid:21)(cid:17)(cid:15)(cid:22)(cid:54)(cid:79)(cid:69)(cid:70)(cid:85)(cid:70)(cid:68)(cid:85)(cid:66)(cid:67)(cid:77)(cid:70)(cid:37)(cid:70)(cid:85)(cid:70)(cid:68)(cid:85)(cid:66)(cid:67)(cid:77)(cid:70)(cid:17)(cid:15)(cid:22)(cid:17)(cid:15)(cid:23)(cid:31)(cid:48)(cid:87)(cid:70)(cid:83)(cid:77)(cid:66)(cid:81)(a)(b)(cid:17)(cid:15)(cid:17)(cid:17)(cid:15)(cid:19)(cid:17)(cid:15)(cid:21)(cid:17)(cid:15)(cid:22)(cid:17)(cid:15)(cid:23)(cid:17)(cid:15)(cid:24)(cid:17)(cid:15)(cid:25)(cid:17)(cid:15)(cid:26)(cid:18)(cid:15)(cid:17)(cid:48)(cid:87)(cid:70)(cid:83)(cid:77)(cid:66)(cid:81)(cid:47)(cid:20)(cid:18)(cid:19)(cid:23)(cid:19)(cid:21)(cid:18)(cid:19)(cid:22)(cid:17)(cid:19)(cid:22)(cid:17)(cid:17)(cid:22)(cid:17)(cid:17)(cid:17)(cid:18)(cid:17)(cid:17)(cid:17)(cid:17)Figure 4: Overlaps of the GNN with trained model parameters. (a) The overlaps of the GNN with
various number of layers T on the SBM with c = 8 and N = 5; 000. (b) The graph size dependence
of the overlap of the GNN with T = 100 on the SBM with c = 8. In both cases  the shaded regions
and dashed line are plotted in the same manner as in Fig. 3b.

overlaps obtained by the trained GNN are clearly better than those of the untrained counterpart (see
Fig. 3b). On the other hand  the region of ϵ where the overlap suddenly deteriorates still coincides
with our mean-ﬁeld estimate for the untrained GNN. This implies that in the limit N ! 1  the
detectability limit is not signiﬁcantly improved by training. To demonstrate the ﬁnite-size effect in
the result of Fig. 4a  the overlaps of various graph sizes are plotted in Fig. 4b. The variation of
(cid:3) (cid:25) 0:33 as the graph size is increased  implying the presence of
overlaps becomes steeper around ϵ
detectability phase transition around the value of ϵ predicted by our mean-ﬁeld estimate.
The untrained and trained GNNs exhibit a clear difference in overlap when X T is employed as the
readout classiﬁer. However  it should be noted that the untrained GNN where ϕ(X T ) is adopted
as the readout classiﬁer exhibits a performance close to that of the trained GNN. The reader should
also bear in mind that the computational cost required for training is not negligible.

6 Discussion

In a minimal GNN model  the adjacency matrix A is employed for the connections between inter-
mediate layers. In fact  there have been many attempts [5  32  40  41] to adopt a more complex
architecture rather than A. Furthermore  other types of applications of deep neural networks to
graph partitioning or related problems have been described [6  7  42]. The number of GNN vari-
eties can be arbitrarily extended by modifying the architecture and learning algorithm. Again  it is
important to clarify which elements are essential for the performance.
The present study offers a baseline answer to this question. Our mean-ﬁeld theory and numerical
experiment using the k-means readout classiﬁer clarify that an untrained GNN with a simple ar-
chitecture already performs well. It is worth noting that our mean-ﬁeld theory yields an accurate
estimate of the detectability limit in a compact form. The learning of the model parameters by
backpropagation does contribute to an improved accuracy  although this appears to be quantitatively
insigniﬁcant. Importantly  the detectability limit appears to remain (almost) the same.
The minimal GNN that we considered in this paper is not the state of the art for the inference of
the symmetric SBM. However  as described in Section 2  an advantage of the GNN is its ﬂexibility 
in that the model can be learned in a data-driven manner. For a more complicated example  such
as the graphs of chemical compounds in which each vertex has attributes  the GNN is expected to
generically outperforms other approaches. In such a case  the performance may be signiﬁcantly
improved thanks to backpropagation. This would constitute an interesting direction for future work.
In addition  the adequacy of the NMI error function that we introduced for the readout classiﬁer
should be examined in detail.

8

(cid:17)(cid:15)(cid:19)(cid:17)(cid:15)(cid:21)(cid:17)(cid:15)(cid:22)(cid:17)(cid:15)(cid:23)(cid:17)(cid:15)(cid:24)(cid:17)(cid:15)(cid:25)(cid:17)(cid:15)(cid:26)(cid:18)(cid:15)(cid:17)(cid:48)(cid:87)(cid:70)(cid:83)(cid:77)(cid:66)(cid:81)(cid:47)(cid:20)(cid:18)(cid:19)(cid:23)(cid:19)(cid:21)(cid:18)(cid:19)(cid:22)(cid:17)(cid:19)(cid:22)(cid:17)(cid:17)(cid:22)(cid:17)(cid:17)(cid:17)(cid:17)(cid:15)(cid:19)(cid:17)(cid:15)(cid:21)(cid:17)(cid:15)(cid:22)(cid:17)(cid:15)(cid:23)(cid:17)(cid:15)(cid:24)(cid:17)(cid:15)(cid:25)(cid:17)(cid:15)(cid:26)(cid:18)(cid:15)(cid:17)(cid:48)(cid:87)(cid:70)(cid:83)(cid:77)(cid:66)(cid:81)(cid:53)(cid:22)(cid:18)(cid:17)(cid:19)(cid:22)(cid:22)(cid:17)(cid:18)(cid:17)(cid:17)(a)(b)Acknowledgments

The authors are grateful to Ryo Karakida for helpful comments. This work was supported by the
New Energy and Industrial Technology Development Organization (NEDO) (T.K. and M. T.) and
JSPS KAKENHI No. 18K11463 (T. O.).

References
[1] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.

International Conference on Knowledge Discovery and Data Mining  2016.

In

[2] David K. Duvenaud  Dougal Maclaurin  Jorge Iparraguirre  Rafael Bombarell  Timothy Hirzel 
Alan Aspuru-Guzik  and Ryan P. Adams. Convolutional networks on graphs for learning
molecular ﬁngerprints. In Advances in Neural Information Processing Systems  2015.

[3] Franco Scarselli  Marco Gori  Ah Chung Tsoi  Markus Hagenbuchner  and Gabriele Monfar-

dini. The graph neural network model. Trans. Neur. Netw.  20(1):61–80  January 2009.

[4] David E Rumelhart  Geoffrey E Hinton  and Ronald J Williams. Learning representations by

back-propagating errors. nature  323(6088):533  1986.

[5] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. arXiv preprint arXiv:1609.02907  2016.

[6] Mathias Niepert  Mohamed Ahmed  and Konstantin Kutzkov. Learning convolutional neural
In International conference on machine learning  pages 2014–2023 

networks for graphs.
2016.

[7] Will Hamilton  Zhitao Ying  and Jure Leskovec.

Inductive representation learning on large
graphs. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and
R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 1024–1034.
Curran Associates  Inc.  2017.

[8] Tao Lei  Wengong Jin  Regina Barzilay  and Tommi Jaakkola. Deriving neural architectures
from sequence and graph kernels. In Doina Precup and Yee Whye Teh  editors  Proceedings of
the 34th International Conference on Machine Learning  volume 70 of Proceedings of Machine
Learning Research  pages 2024–2033  International Convention Centre  Sydney  Australia 
06–11 Aug 2017. PMLR.

[9] Ulrike Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416 

August 2007.

[10] M. E. J. Newman. Finding community structure in networks using the eigenvectors of matrices.

Phys. Rev. E  74:036104  2006.

[11] Karl Rohe  Sourav Chatterjee  and Bin Yu. Spectral clustering and the high-dimensional

stochastic blockmodel. The Annals of Statistics  39(4):1878–1915  2011.

[12] Gene H Golub and Charles F Van Loan. Matrix computations  volume 3. JHU Press  2012.

[13] Aurelien Decelle  Florent Krzakala  Cristopher Moore  and Lenka Zdeborová. Asymptotic
analysis of the stochastic block model for modular networks and its algorithmic applications.
Phys. Rev. E  84:066106  2011.

[14] Florent Krzakala  Cristopher Moore  Elchanan Mossel  Joe Neeman  Allan Sly  Lenka Zde-
borová  and Pan Zhang. Spectral redemption in clustering sparse networks. Proc. Natl. Acad.
Sci. U.S.A.  110(52):20935–40  December 2013.

[15] Jiaxuan You  Rex Ying  Xiang Ren  William L Hamilton  and Jure Leskovec. GraphRNN: A

deep generative model for graphs. arXiv preprint arXiv:1802.08773  2018.

[16] Justin Gilmer  Samuel S Schoenholz  Patrick F Riley  Oriol Vinyals  and George E Dahl. Neu-

ral message passing for quantum chemistry. arXiv preprint arXiv:1704.01212  2017.

9

[17] Michael Schlichtkrull  Thomas N Kipf  Peter Bloem  Rianne van den Berg  Ivan Titov  and
Max Welling. Modeling relational data with graph convolutional networks. arXiv preprint
arXiv:1703.06103  2017.

[18] Tiago P. Peixoto. Efﬁcient monte carlo and greedy heuristic for the inference of stochastic

block models. Phys. Rev. E  89:012804  Jan 2014.

[19] Tiago P Peixoto. Bayesian stochastic blockmodeling. arXiv preprint arXiv:1705.10225  2017.

[20] M. E. J. Newman and Gesine Reinert. Estimating the number of communities in a network.

Phys. Rev. Lett.  117:078301  Aug 2016.

[21] Elchanan Mossel  Joe Neeman  and Allan Sly. Reconstruction and estimation in the planted

partition model. Probability Theory and Related Fields  162(3):431–461  Aug 2015.

[22] Laurent Massoulié. Community detection thresholds and the weak ramanujan property.

In
Proceedings of the 46th Annual ACM Symposium on Theory of Computing  STOC ’14  pages
694–703  New York  2014. ACM.

[23] Cristopher Moore. The computer science and physics of community detection: landscapes 

phase transitions  and hardness. arXiv preprint arXiv:1702.00467  2017.

[24] Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha Sohl-Dickstein  and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos.
In D. D. Lee 
M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Infor-
mation Processing Systems 29  pages 3360–3368. Curran Associates  Inc.  2016.

[25] Samuel S Schoenholz  Justin Gilmer  Surya Ganguli  and Jascha Sohl-Dickstein. Deep infor-

mation propagation. arXiv preprint arXiv:1611.01232  2016.

[26] H. Sompolinsky and Annette Zippelius. Relaxational dynamics of the edwards-anderson model

and the mean-ﬁeld theory of spin-glasses. Phys. Rev. B  25:6860–6875  Jun 1982.

[27] A. Crisanti and H. Sompolinsky. Dynamics of spin systems with randomly asymmetric bonds:

Ising spins and glauber dynamics. Phys. Rev. A  37:4865–4874  Jun 1988.

[28] A Crisanti  HJ Sommers  and H Sompolinsky. Chaos in neural networks: chaotic solutions.

preprint  1990.

[29] Manfred Opper  Burak Çakmak  and Ole Winther. A theory of solving tap equations for ising
models with general invariant random matrices. Journal of Physics A: Mathematical and The-
oretical  49(11):114002  2016.

[30] David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding.

In
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms  SODA
’07  pages 1027–1035  Philadelphia  PA  USA  2007. Society for Industrial and Applied Math-
ematics.

[31] Krzysztof Nowicki and Tom A. B Snijders. Estimation and prediction for stochastic block-

structures. Journal of the American Statistical Association  96(455):1077–1087  2001.

[32] Joan Bruna and Xiang Li. Community detection with graph neural networks. arXiv preprint

arXiv:1705.08415  2017.

[33] Tatsuro Kawamoto and Yoshiyuki Kabashima. Limitations in the spectral method for graph
partitioning: Detectability threshold and localization of eigenvectors. Phys. Rev. E  91:062803 
Jun 2015.

[34] T. Kawamoto and Y. Kabashima. Detectability of the spectral method for sparse graph parti-

tioning. Eur. Phys. Lett.  112(4):40007  2015.

[35] Tatsuro Kawamoto. Algorithmic detectability threshold of the stochastic block model. Phys.

Rev. E  97:032301  Mar 2018.

10

[36] Seiya Tokui  Kenta Oono  Shohei Hido  and Justin Clayton. Chainer: a next-generation open
source framework for deep learning. In Proceedings of workshop on machine learning sys-
tems (LearningSys) in the twenty-ninth annual conference on neural information processing
systems  2015.

[37] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980  2014.

[38] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion  2016.

[39] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[40] Joan Bruna  Wojciech Zaremba  Arthur Szlam  and Yann LeCun. Spectral networks and locally

connected networks on graphs. arXiv preprint arXiv:1312.6203  2013.

[41] Michaël Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral ﬁltering. In D. D. Lee  M. Sugiyama  U. V. Luxburg 
I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29 
pages 3844–3852. Curran Associates  Inc.  2016.

[42] Liang Yang  Xiaochun Cao  Dongxiao He  Chuan Wang  Xiao Wang  and Weixiong Zhang.
Modularity based community detection with deep learning. In Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence  IJCAI’16  pages 2252–2258. AAAI
Press  2016.

11

,Tatsuro Kawamoto
Masashi Tsubaki
Tomoyuki Obuchi
Jonathan Ho
Evan Lohn
Pieter Abbeel