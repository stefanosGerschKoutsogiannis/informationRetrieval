2017,Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models,This paper studies the numerical computation of integrals  representing estimates or predictions  over the output $f(x)$ of a computational model with respect to a distribution $p(\mathrm{d}x)$ over uncertain inputs $x$ to the model. For the functional cardiac models that motivate this work  neither $f$ nor $p$ possess a closed-form expression and evaluation of either requires $\approx$ 100 CPU hours  precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem  with a joint model for both the a priori unknown function $f$ and the a priori unknown distribution $p$. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account  in a statistically principled manner  for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment.,Probabilistic Models for Integration Error in the

Assessment of Functional Cardiac Models

Chris. J. Oates1 5  Steven Niederer2  Angela Lee2  François-Xavier Briol3  Mark Girolami4 5

1Newcastle University  2King’s College London  3University of Warwick 

4Imperial College London  5Alan Turing Institute

Abstract

This paper studies the numerical computation of integrals  representing estimates
or predictions  over the output f (x) of a computational model with respect to
a distribution p(dx) over uncertain inputs x to the model. For the functional
cardiac models that motivate this work  neither f nor p possess a closed-form
expression and evaluation of either requires ≈ 100 CPU hours  precluding standard
numerical integration methods. Our proposal is to treat integration as an estimation
problem  with a joint model for both the a priori unknown function f and the a
priori unknown distribution p. The result is a posterior distribution over the integral
that explicitly accounts for dual sources of numerical approximation error due to a
severely limited computational budget. This construction is applied to account  in
a statistically principled manner  for the impact of numerical errors that (at present)
are confounding factors in functional cardiac model assessment.

1 Motivation: Predictive Assessment of Computer Models

This paper considers the problem of assessment for computer models [7]  motivated by an urgent
need to assess the performance of sophisticated functional cardiac models [25]. In concrete terms 
the problem that we consider can be expressed as the numerical approximation of integrals

(cid:90)

p(f ) =

f (x)p(dx) 

(1)

where f (x) denotes a functional of the output from a computer model and x denotes unknown inputs
(or ‘parameters’) of the model. The term p(x) denotes a posterior distribution over model inputs.
Although not our focus in this paper  we note that p(x) is deﬁned based on a prior π0(x) over these
inputs and training data y assumed to follow the computer model π(y|x) itself. The integral p(f )  in
our context  represents a posterior prediction of actual cardiac behaviour. The computational model
can be assessed through comparison of these predictions to test data generated from an experiment.
The challenging nature of cardiac models – and indeed computer models in general – is such that a
closed-form for both f (x) and p(dx) is precluded [23]. Instead  it is typical to be provided with a
ﬁnite collection of samples {xi}n
i=1 obtained from p(dx) through Monte Carlo (or related) methods
[32]. The integrand f (x) is then evaluated at these n input conﬁgurations  to obtain {f (xi)}n
i=1.
Limited computational budgets necessitate that the number n is small and  in such situations  the error
of an estimator for the integral p(f ) based on the data {(xi  f (xi))}n
i=1 is subject to strict information-
theoretic lower bounds [26]. The practical consequence is that an unknown (non-negligible) numerical
error is introduced in the numerical approximation of p(f )  unrelated to the performance of the model.
If this numerical error is ignored  it will constitute a confounding factor in the assessment of predictive
performance for the computer model. It is therefore unclear how a fair model assessment can proceed.
This motivates an attempt to understand the extent of numerical error in any estimate of p(f ). This is
non-trivial; for example  the error distribution of the arithmetic mean 1
i=1f (xi) depends on the

n Σn

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

unknown f and p  and attempts to estimate this distribution solely from data  e.g. via a bootstrap or a
central limit approximation  cannot succeed in general when the number of samples n is small [27].
Our ﬁrst contribution  in this paper  is to argue that approximation of p(f ) from samples {xi}n
and function evaluations {f (xi)}n
i=1
i=1 can be cast as an estimation task. Our second contribution is to
derive a posterior distribution over the unknown value p(f ) of the integral. This distribution provides
an interpretable quantiﬁcation of the extent of numerical integration error that can be reasoned
with and propagated through subsequent model assessment. Our third contribution is to establish
theoretical properties of the proposed method. The method we present falls within the framework
of Probabilistic Numerics and our work can be seen as a contribution to this emerging area [16  5].
In particular  the method proposed is reminiscent of Bayesian Quadrature (BQ) [9  28  29  15]. In
BQ  a Gaussian prior measure is placed on the unknown function f and is updated to a posterior
when conditioned on the information {(xi  f (xi))}n
i=1. This induces both a prior and a posterior
over the value of p(f ) as push-forward measures under the projection operator f (cid:55)→ p(f ). Since
its introduction  several authors have related BQ to other methods such as the ‘herding’ approach
from machine learning [17  3]  random feature approximations used in kernel methods [1]  classical
quadrature rules [33] and Quasi Monte Carlo (QMC) methods [4]. Most recently  [21] extended
theoretical results for BQ to misspeciﬁed prior models  and [22] who provided efﬁcient matrix
algebraic methods for the implementation of BQ. However  as an important point of distinction 
notice that BQ pre-supposes p(dx) is known in closed-form - it does not apply in situations where
p(dx) is instead sampled. In this latter case p(dx) will be called an intractable distribution and  for
model assessment  this scenario is typical.
To extend BQ to intractable distributions  this paper proposes to use a Dirichlet process mixture
prior to estimate the unknown distribution p(dx) from Monte Carlo samples {xi}n
i=1 [12]. It will be
demonstrated that this leads to a simple expression for the closed-form terms which are required to
implement the usual BQ. The overall method  called Dirichlet process mixture Bayesian quadrature
(DPMBQ)  constructs a (univariate) distribution over the unknown integral p(f ) that can be exploited
to tease apart the intrinsic performance of a model from numerical integration error in model
assessment. Note that BQ was used to estimate marginal likelihood in e.g. [30]. The present problem
is distinct  in that we focus on predictive performance (of posterior expectations) rather than marginal
likelihood  and its solution demands a correspondingly different methodological development.
On the computational front  DPMBQ costs O(n3). However  this cost is de-coupled from the often
orders-of-magnitude larger costs involved in both evaluation of f (x) and p(dx)  which form the main
computational bottleneck. Indeed  in the modern computational cardiac models that motivate this
research  the ≈ 100 CPU hour time required for a single simulation limits the number n of available
samples to ≈ 103 [25]. At this scale  numerical integration error cannot be neglected in model
assessment. This raises challenges when making assessments or comparisons between models  since
the intrinsic performance of models cannot be separated from numerical error that is introduced into
the assessment. Moreover  there is an urgent ethical imperative that the clinical translation of such
models is accompanied with a detailed quantiﬁcation of the unknown numerical error component in
model assessment. Our contribution explicitly demonstrates how this might be achieved.
The remainder of the paper proceeds as follows: In Section 2.1 we ﬁrst recall the usual BQ method 
then in Section 2.2 we present and analyse our novel DPMBQ method. Proofs of theoretical results
are contained in the electronic supplement. Empirical results are presented in Section 3 and the paper
concludes with a discussion in Section 4.

2 Probabilistic Models for Numerical Integration Error
Consider a domain Ω ⊆ Rd  together with a distribution p(dx) on Ω. As in Eqn. 1  p(f ) will be used
to denote the integral of the argument f with respect to the distribution p(dx). All integrands are
assumed to be (measurable) functions f : Ω → R such that the integral p(f ) is well-deﬁned. To
begin  we recall details for the BQ method when p(dx) is known in closed-form [9  28]:

2.1 Probabilistic Integration for Tractable Distributions (BQ)
In standard BQ [9  28]  a Gaussian Process (GP) prior f ∼ GP(m  k) is assigned to the integrand f 
with mean function m : Ω → R and covariance function k : Ω × Ω → R [see 31  for further details

2

p(f ) ∼ N(p(m)  p ⊗ p(k))

under our notational convention the so-called initial error p⊗p(k) is equal to(cid:82)(cid:82) k(x  x(cid:48))p(dx)p(dx(cid:48)).

on GPs]. The implied prior over the integral p(f ) is then the push-forward of the GP prior through
the projection f (cid:55)→ p(f ):
where p⊗p : Ω×Ω → R is the measure formed by independent products of p(dx) and p(dx(cid:48))  so that
Next  the GP is conditioned on the information in {(xi  f (xi))}n
i=1. The conditional GP takes a
conjugate form f|X  f (X) ∼ GP(mn  kn)  where we have written X = (x1  . . .   xn)  f (X) =
(f (x1)  . . .   f (xn))(cid:62). Formulae for the mean function mn : Ω → R and covariance function
kn : Ω × Ω → R are standard can be found in [31  Eqns. 2.23  2.24]. The BQ posterior over p(f ) is
the push forward of the GP posterior:
Formulae for p(mn) and p ⊗ p(kn) were derived in [28]:

p(f ) | X  f (X) ∼ N(p(mn)  p ⊗ p(kn))

p(mn) = f (X)(cid:62)k(X  X)−1µ(X)

(3)
(4)
where k(X  X) is the n × n matrix with (i  j)th entry k(xi  xj) and µ(X) is the n × 1 vector with
ith entry µ(xi) where the function µ is called the kernel mean or kernel embedding [see e.g. 35]:

p ⊗ p(kn) = p ⊗ p(k) − µ(X)(cid:62)k(X  X)−1µ(X)

(2)

µ(x) =

k(x  x(cid:48))p(dx(cid:48))

(5)

Computation of the kernel mean and the initial error each requires that p(dx) is known in general.
The posterior in Eqn. 2 was studied in [4]  where rates of posterior contraction were established
under further assumptions on the smoothness of the covariance function k and the smoothness of
the integrand. Note that the matrix inverse of k(X  X) incurs a (naive) computational cost of O(n3);
however this cost is post-hoc and decoupled from (more expensive) computation that involves the
computer model. Sparse or approximate GP methods could also be used.

2.2 Probabilistic Integration for Intractable Distributions

The dependence of Eqns. 3 and 4 on both the kernel mean and the initial error means that BQ cannot
be used for intractable p(dx) in general. To address this we construct a second non-parametric model
for the unknown p(dx)  presented next.

Dirichlet Process Mixture Model Consider an inﬁnite mixture model

p(dx) =

ψ(dx; φ)P (dφ) 

(6)
where ψ : Ω × Φ → [0 ∞) is such that ψ(·; φ) is a distribution on Ω with parameter φ ∈ Φ and P is
a mixing distribution deﬁned on Φ. In this paper  each data point xi is modelled as an independent
draw from p(dx) and is associated with a latent variable φi ∈ Φ according to the generative process
of Eqn. 6. i.e. xi ∼ ψ(·; φi). To limit scope  the extension to correlated xi is reserved for future
work.
The Dirichlet process (DP) is the natural conjugate prior for non-parametric discrete distributions
[12]. Here we endow P (dφ) with a DP prior P ∼ DP(α  Pb)  where α > 0 is a concentration
parameter and Pb(dφ) is a base distribution over Φ. The base distribution Pb coincides with the
prior expectation E[P (dφ)] = Pb(dφ)  while α determines the spread of the prior about Pb. The
DP is characterised by the property that  for any ﬁnite partition Φ = Φ1 ∪ ··· ∪ Φm  it holds that
(P (Φ1)  . . .   P (Φm)) ∼ Dir(αPb(Φ1)  . . .   αPb(Φm)) where P (S) denotes the measure of the set
S ⊆ Φ. For α → 0  the DP is supported on the set of atomic distributions  while for α → ∞  the DP
converges to an atom on the base distribution. This overall approach is called a DP mixture (DPM)
model [13].
For a random variable Z  the notation [Z] will be used as shorthand to denote the density function
of Z. It will be helpful to note that for φi ∼ P independent  writing φ1:n = (φ1  . . .   φn)  standard
conjugate results for DPs lead to the conditional

(cid:90)

(cid:90)

(cid:16)

n(cid:88)

(cid:17)

P | φ1:n ∼ DP

α + n 

α

α + n

Pb +

1

δφi

α + n

i=1

where δφi(dφ) is an atomic distribution centred at the location φi of the ith sample in φ1:n. In turn 
this induces a conditional [dp|φ1:n] for the unknown distribution p(dx) through Eqn. 6.

3

ϕj

iid∼ α

α + n

Pb +

1

α + n

i=1

δφi 

iid∼ Beta(1  α + n)

βj

In practice the sum in Eqn. 7 may be truncated at a large ﬁnite number of terms  N  with negligible
truncation error  since weights wj vanish at a geometric rate [18]. The truncated DP has been shown
to provide accurate approximation of integrals with respect to the original DP [19]. For a realisation
P (dφ) from Eqn. 7  observe that the induced distribution p(dx) over Ω is

n(cid:88)

∞(cid:88)

j=1

Kernel Means via Stick Breaking The stick breaking characterisation can be used to draw from
the conditional DP [34]. A generic draw from [P|φ1:n] can be characterised as

∞(cid:88)

j=1

j−1(cid:89)

j(cid:48)=1

(1 − βj(cid:48))

P (dφ) =

wjδϕj (dφ) 

wj = βj

(7)

where randomness enters through the ϕj and βj as follows:

p(dx) =

wjψ(dx; ϕj).

(8)

Thus we have an alternative characterisation of [p|φ1:n].
Our key insight is that one can take ψ and k to be a conjugate pair  such that both the kernel mean
µ(x) and the initial error p ⊗ p(k) will be available in an explicit form for the distribution in Eqn. 8
[see Table 1 in 4  for a list of conjugate pairs]. For instance  in the one-dimensional case  consider
ϕ = (ϕ1  ϕ2) and ψ(dx; ϕ) = N(dx; ϕ1  ϕ2) for some location and scale parameters ϕ1 and ϕ2.
Then for the Gaussian kernel k(x  x(cid:48)) = ζ exp(−(x − x(cid:48))2/2λ2)  the kernel mean becomes

µ(x) =

(λ2 + ϕj 2)1/2

j=1

ζλwj

exp

(cid:16) − (x − ϕj 1)2

2(λ2 + ϕj 2)

(cid:17)

and the initial variance can be expressed as

p ⊗ p(k) =

ζλwjwj(cid:48)

j=1

j(cid:48)=1

(λ2 + ϕj 2 + ϕj(cid:48) 2)1/2

(cid:16) − (ϕj 1 − ϕj(cid:48) 1)2

2(λ2 + ϕj 2 + ϕj(cid:48) 2)

(cid:17)

.

exp

∞(cid:88)

∞(cid:88)

∞(cid:88)

(9)

(10)

Similar calculations for the multi-dimensional case are straight-forward and provided in the Supple-
mental Information.

The Proposed Model To put this all together  let θ denote all hyper-parameters that (a) deﬁne the
GP prior mean and covariance function  denoted mθ and kθ below  and (b) deﬁne the DP prior  such
as α and the base distribution Pb. It is assumed that θ ∈ Θ for some speciﬁed set Θ. The marginal
posterior distribution for p(f ) in the DPMBQ model is deﬁned as

[p(f ) | X  f (X)] =

[p(f ) | X  f (X)  p  θ] [dp | X  θ] [dθ].

(11)

The ﬁrst term in the integral is BQ for a ﬁxed distribution p(dx). The second term represents the
DPM model for the unknown p(dx)  while the third term [dθ] represents a hyper-prior distribution
over θ ∈ Θ. The DPMBQ distribution in Eqn. 11 does not admit a closed-form expression. However 
it is straight-forward to sample from this distribution without recourse to f (x) or p(dx). In particular 
the second term can be accessed through the law of total probabilities:

(cid:90)(cid:90)

(cid:90)

[dp | X  θ] =

[dp | φ1:n] [φ1:n | X  θ] dφ1:n

where the ﬁrst term [dp | φ1:n] is the stick-breaking construction and the term [φ1:n | X  θ] can be
targeted with a Gibbs sampler. Full details of the procedure we used to sample from Eqn. 11  which
is de-coupled from the much larger costs associated with the computer model  are provided in the
Supplemental Information.

4

Theoretical Analysis The analysis reported below restricts attention to a ﬁxed hyper-parameter θ
and a one-dimensional state-space Ω = R. The extension of theoretical results to multiple dimensions
was beyond the scope of this paper.
Our aim in this section is to establish when DPMBQ is “consistent”. To be precise  a random
distribution Pn over an unknown parameter ζ ∈ R  whose true value is ζ0  is called consistent for ζ0
at a rate rn if  for all δ > 0  we have Pn[(−∞  ζ0 − δ) ∪ (ζ0 + δ ∞)] = OP (rn). Below we denote
with f0 and p0 the respective true values of f and p; our aim is to estimate ζ0 = p0(f0). Denote with
H the reproducing kernel Hilbert space whose reproducing kernel is k and assume that the GP prior
mean m is an element of H. Our main theoretical result below establishes that the DPMBQ posterior
distribution in Eqn. 11  which is a random object due to the n independent draws xi ∼ p(dx)  is
consistent:
Theorem. Let P0 denote the true mixing distribution. Suppose that:

1. f belongs to H and k is bounded on Ω × Ω.
2. ψ(dx; ϕ) = N(dx; ϕ1  ϕ2).
3. P0 has compact support supp(P0) ⊂ R × (σ  σ) for some ﬁxed σ  σ ∈ (0 ∞).
4. Pb has positive  continuous density on a rectangle R  s.t. supp(Pb) ⊆ R ⊆ R × [σ  σ].
5. Pb({(ϕ1  ϕ2) : |ϕ1| > t}) ≤ c exp(−γ|t|δ) for some γ  δ > 0 and ∀ t > 0.

Then the posterior Pn = [p(f ) | X  f0(X)] is consistent for the true value p0(f0) of the integral at
the rate n−1/4+ where the constant  > 0 can be arbitrarily small.

The proof is provided in the Supplemental Information. Assumption (1) derives from results on
consistent BQ [4] and can be relaxed further with the results in [21] (not discussed here)  while
assumptions (2-5) derive from previous work on consistent estimation with DPM priors [14]. For
the case of BQ when p(dx) is known and H a Sobolev space of order s > 1/2 on Ω = [0  1]  the
corresponding posterior contraction rate is exp(−Cn2s−) [4  Thm. 1]. Our work  while providing
only an upper bound on the convergence rate  suggests that there is an increase in the fundamental
complexity of estimation for p(dx) unknown compared to p(dx) known. Interestingly  the n−1/4+
rate is slower than the classical Bernstein-von Mises rate n−1/2 [36]. However  an out-of-hand
comparison between these two quantities is not straight forward  as the former involves the interaction
of two distinct non-parametric statistical models. It is known Bernstein-von Mises results can be
delicate for non-parametric problems [see  for example  the counter-examples in 10]. Rather  this
theoretical analysis guarantees consistent estimation in a regime that is non-standard.

3 Results

The remainder of the paper reports empirical results from application of DPMBQ to simulated data
and to computational cardiac models.

3.1 Simulation Experiments

To explore the empirical performance of DPMBQ  a series of detailed simulation experiments were
performed. For this purpose  a ﬂexible test bed was constructed wherein the true distribution p0 was
a normal mixture model (able to approximate any continuous density) and the true integrand f0 was
a polynomial (able to approximate any continuous function). In this set-up it is possible to obtain
closed-form expressions for all integrals p0(f0) and these served as a gold-standard benchmark.
To mimic the scenario of interest  a small number n of samples xi were drawn from p0(dx) and
the integrand values f0(xi) were obtained. This information X  f0(X) was provided to DPMBQ
and the output of DPMBQ  a distribution over p(f )  was compared against the actual value p0(f0)
of the integral. For all experiments in this paper the Gaussian kernel k deﬁned in Sec. 2.2 was
used; the integrand f was normalised and the associated amplitude hyper-parameter ζ = 1 ﬁxed 
whereas the length-scale hyper-parameter λ was assigned a Gam(2  1) hyper-prior. For the DPM  the
concentration parameter α was assigned a Exp(1) hyper-prior. These choices allowed for adaptation
of DPMBQ to the smoothness of both f and p in accordance with the data presented to the method.
The base distribution Pb for DPMBQ was taken to be normal inverse-gamma with hyper-parameters
µ0 = 0  λ0 = α0 = β0 = 1  selected to facilitate a simpliﬁed Gibbs sampler. Full details of the
simulation set-up and Gibbs sampler are reported in the Supplemental Information.

5

(a)

(b)

Figure 1: Simulated data results. (a) Comparison of coverage frequencies for the simulation exper-
iments. (b) Convergence assessment: Wasserstein distance (W ) between the posterior in Eqn. 11
and the true value of the integral  is presented as a function of the number n of data points. [Circles
represent independent realisations and the linear trend is shown in red.]

(cid:16) ¯f − t∗ s√

(cid:17)

For comparison  we considered the default 50% conﬁdence interval description of numerical error

  ¯f + t∗ s√
(12)
n
n
i=1(f (xi)− ¯f )2 and t∗ is the 50% level for a Student’s
i=1f (xi)  s2 = (n− 1)−1Σn
where ¯f = n−1Σn
t-distribution with n − 1 degrees of freedom. It is well-known that Eqn. 12 is a poor description of
numerical error when n is small [c.f. “Monte Carlo is fundamentally unsound” 27]. For example 
with n = 2  in the extreme case where  due to chance  f (x1) ≈ f (x2)  it follows that s ≈ 0 and no
numerical error is acknowledged. This fundamental problem is resolved through the use of prior
information on the form of both f and p in DPMBQ. The appropriateness of DPMBQ therefore
depends crucially on the prior. The proposed method is further distinguished from Eqn. 12 in that the
distribution over numerical error is fully non-parametric  not e.g. constrained to be Student-t.

convergence in the Wasserstein distance W =(cid:82) |p(f )− p0(f0)| d[p(f ) | X  f (X)]. In particular we

Empirical Results Coverage frequencies are shown in Fig. 1a for a speciﬁc integration task
(f0  p0)  that was deliberately selected to be difﬁcult for Eqn. 12 due to the rare event represented by
the mass at x = 2. These were compared against central 50% posterior credible intervals produced
under DPMBQ. These are the frequency with which the conﬁdence/credible interval contain the true
value of the integral  here estimated with 100 independent realisations for DPMBQ and 1000 for the
(less computational) standard method (standard errors are shown for both). Whilst it offers correct
coverage in the asymptotic limit  Eqn. 12 can be seen to be over-conﬁdent when n is small  with
coverage often less than 50%. In contrast  DPMBQ accounts for the fact p is being estimated and
provides conservative estimation about the extent of numerical error when n is small.
To present results that do not depend on a ﬁxed coverage level (e.g. 50%)  we next measured
explored whether the theoretical rate of n−1/4+ was realised. (Note that the theoretical result applied
just to ﬁxed hyper-parameters  whereas the experimental results reported involved hyper-parameters
that were marginalised  so that this is a non-trivial experiment.) Results in Fig. 1b demonstrated that
W scaled with n at a rate which was consistent with the theoretical rate claimed. Full experimental
results on our polynomial test bed  reported in detail in the Supplemental Information  revealed that W
was larger for higher-degree polynomials (i.e. more complex integrands f)  while W was insensitive
to the number of mixture components (i.e. to more complex distributions p). The latter observation
may be explained by the fact that the kernel mean µ is a smoothed version of the distribution p and so
is not expected to be acutely sensitive to variation in p itself.

3.2 Application to a Computational Cardiac Model

The Model The computation model considered in this paper is due to [24] and describes the
mechanics of the left and right ventricles through a heart beat. In brief  the model geometry (Fig. 2a 

6

100101102103n00.20.40.60.81cover. prob.OracleStudent-tDPMBQ-202x-0.500.5f(x)-202x024p(x)n100101102W10-1100(a)

(b)

Figure 2: Cardiac model results: (a) Computational cardiac model. A) Segmentation of the cardiac
MRI. B) Computational model of the left and right ventricles. C) Schematic image showing the
features of pressure (left) and volume transient (right). (b) Comparison of coverage frequencies  for
each of 10 numerical integration tasks deﬁned by functionals gj of the cardiac model output.

top right) is described by ﬁtting a C1 continuous cubic Hermite ﬁnite element mesh to segmented
magnetic resonance images (MRI; Fig. 2a  top left). Cardiac electrophysiology is modelled separately
by the solution of the mono-domain equations and provides a ﬁeld of activation times across the heart.
The passive material properties and afterload of the heart are described  respectively  by a transversely
isotropic material law and a three element Windkessel model. Active contraction is simulated using a
phenomenological cellular model  with spatial variation arising from the local electrical activation
times. The active contraction model is deﬁned by ﬁve input parameters: tr and td are the respective
constants for the rise and decay times  T0 is the reference tension  a4 and a6 respectively govern the
length dependence of tension rise time and peak tension. These ﬁve parameters were concatenated
into a vector x ∈ R5 and constitute the model inputs. The model is ﬁtted based on training data
y that consist of functionals gj : R5 → R  j = 1  . . .   10  of the pressure and volume transient
morphology during baseline activation and when the heart is paced from two leads implanted in
the right ventricle apex and the left ventricle lateral wall. These 10 functionals are deﬁned in the
Supplemental Information; a schematic of the model and ﬁtted measurements are shown in Fig. 2a
(bottom panel).

(cid:80)10
Test Functions The distribution p(dx) was taken to be the posterior distribution over model
inputs x that results from an improper ﬂat prior on x and a squared-error likelihood function:
j=1(yj − gj(x))2. The training data y = (y1  . . .   y10) were obtained
log p(x) = const. + 1
0.12
from clinical experiment. The task we considered is to compute posterior expectations for functionals
f (x) of the model output produced when the model input x is distributed according to p(dx). This
represents the situation where a ﬁtted model is used to predict response to a causal intervention 
representing a clinical treatment. For assessment of the DPMBQ method  which is our principle aim
in this experiment  we simply took the test functions f to be each of the physically relevant model
outputs gj in turn (corresponding to no causal intervention). This deﬁned 10 separate numerical
integration problems as a test bed. Benchmark values for p0(gj) were obtained  as described in
the Supplemental Information  at a total cost of ≈ 105 CPU hours  which would not be routinely
practical.

Empirical Results For each of the 10 numerical integration problems in the test bed  we computed
coverage probabilities  estimated with 100 independent realisations (standard errors are shown) 
in line with those discussed for simulation experiments. These are shown in Fig. 2b  where we
compared Eqn. 12 with central 50% posterior credible intervals produced under DPMBQ. It is seen
that Eqn. 12 is usually reliable but can sometimes be over-conﬁdent  with coverage probabilities
less than 50%. This over-conﬁdence can lead to spurious conclusions on the predictive performance
of the computational model. In contrast  DPMBQ provides a uniformly conservative quantiﬁcation

7

of numerical error (cover. prob. ≥ 50%). The DPMBQ method is further distinguished from Eqn.
12 in that it entails a joint distribution for the 10 integrals (the unknown p is shared across integrals
- an instance of transfer learning across the 10 integration tasks). Fig. 2b also appears to show a
correlation structure in the standard approach (black lines)  but this is an artefact of the common
sample set {xi}n
i=1 that was used to simultaneously estimate all 10 integrals; Eqn. 12 is still applied
independently to each integral.

4 Discussion

Numerical analysis often focuses the convergence order of numerical methods  but in non-asymptotic
regimes the language of probabilities can provide a richer  more intuitive and more useful description
of numerical error. This paper cast the computation of integrals p(f ) as an estimation problem
amenable to Bayesian methods [20  9  5]. The difﬁculty of this problem depends on our level of prior
knowledge (rendering the problem trivial if a closed-form solution is a priori known) and  in the
general case  on how much information we are prepared to obtain on the objects f and p through
numerical computation [16]. In particular  we distinguish between three states of prior knowledge:
(1) f known  p unknown  (2) f unknown  p known  (3) both f and p unknown. Case (1) is the
subject of Monte Carlo methods [32] and concerns classical problems in applied probability such as
estimating conﬁdence intervals for expectations based on Markov chains. Notable recent work in this
direction is [8]  who obtained a point estimate ˆp for p using a kernel smoother and then  in effect 
used ˆp(f ) as an estimate for the integral. The decision-theoretic risk associated with error in ˆp was
explored in [6]. Independent of integral estimation  there is a large literature on density estimation
[37]. Our probabilistic approach provides a Bayesian solution to this problem  as a special case of
our more general framework. Case (2) concerns functional analysis  where [26] provide an extensive
overview of theoretical results on approximation of unknown functions in an information complexity
framework. As a rule of thumb  estimation improves when additional smoothness can be a priori
assumed on the value of the unknown object [see 4]. The main focus of this paper was Case (3)  until
now unstudied  and a transparent  general statistical method called DPMBQ was proposed.
The path-ﬁnding nature of this work raises several important questions for future theoretical and
applied research. First  these methods should be extended to account for the low-rank phenomenon
that is often encountered in multi-dimensional integrals [11]. Second  there is no reason  in general 
to restrict attention to function values obtained at the locations in X. Indeed  one could ﬁrst estimate
p(dx)  then select suitable locations X(cid:48) from at which to evaluate f (X(cid:48)) [2]. This touches on aspects
of statistical experimental design; the practitioner seeks a set X(cid:48) that minimises an appropriate loss
functional at the level of p(f ); see again [6]. Third  whilst restricted to Gaussians in our experiments 
further methodological work will be required to establish guidance for the choice of kernel k in the
GP and choice of base distribution Pb in the DPM [c.f. chapter 4 of 31].

Acknowledgments

CJO and MG were supported by the Lloyds Register Foundation Programme on Data-Centric
Engineering. SN was supported by an EPSRC Intermediate Career Fellowship. FXB was supported
by the EPSRC grant [EP/L016710/1]. MG was supported by the EPSRC grants [EP/K034154/1 
EP/R018413/1  EP/P020720/1  EP/L014165/1]  and an EPSRC Established Career Fellowship 
[EP/J016934/1]. This material was based upon work partially supported by the National Science
Foundation (NSF) under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences
Institute. Opinions  ﬁndings  and conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reﬂect the views of the NSF.

References
[1] F Bach. On the Equivalence Between Quadrature Rules and Random Features. Journal of Machine

Learning Research  18:1–38  2017.

[2] F-X Briol  CJ Oates  J Cockayne  WY Chen  and M Girolami. On the sampling problem for kernel
quadrature. In Proceedings of the 34th International Conference on Machine Learning  pages 586–595 
2017.

[3] F-X Briol  CJ Oates  M Girolami  and MA Osborne. Frank-Wolfe Bayesian quadrature: Probabilistic
integration with theoretical guarantees. In Advances in Neural Information Processing Systems  pages
1162–1170  2015.

8

2016.

1988.

1986.

1973.

[4] F-X Briol  CJ Oates  M Girolami  MA Osborne  and D Sejdinovic. Probabilistic Integration: A Role for

Statisticians in Numerical Analysis? arXiv:1512.00933  2015.

[5] J Cockayne  CJ Oates  T Sullivan  and M Girolami. Bayesian probabilistic numerical methods.

arXiv:1702.03673  2017.

[6] SN Cohen. Data-driven nonlinear expectations for statistical uncertainty in decisions. arXiv:1609.06545 

[7] PS Craig  M Goldstein  JC Rougier  and AH Seheult. Bayesian Forecasting for Complex Systems Using

Computer Simulators. Journal of the American Statistical Association  96(454):717–729  2001.

[8] B Delyon and F Portier. Integral Approximation by Kernel Smoothing. Bernoulli  22(4):2177–2208  2016.
[9] P Diaconis. Bayesian Numerical Analysis. Statistical Decision Theory and Related Topics IV  1:163–175 

[10] P Diaconis and D Freedman. On the Consistency of Bayes Estimates. Annals of Statistics  14(1):1–26 

[11] J Dick  FY Kuo  and IH Sloan. High-Dimensional Integration: The Quasi-Monte Carlo Way. Acta

Numerica  22:133–288  2013.

[12] TS Ferguson. A Bayesian Analysis of Some Nonparametric Problems. Annals of Statistics  1(2):209–230 

[13] TS Ferguson. Bayesian Density Estimation by Mixtures of Normal Distributions. Recent Advances in

Statistics  24(1983):287–302  1983.

[14] S Ghosal and AW Van Der Vaart. Entropies and Rates of Convergence for Maximum Likelihood and

Bayes Estimation for Mixtures of Normal Densities. Annals of Statistics  29(5):1233–1263  2001.

[15] T Gunter  MA Osborne  R Garnett  P Hennig  and SJ Roberts. Sampling for Inference in Probabilistic
Models With Fast Bayesian Quadrature. In Advances in Neural Information Processing Systems  pages
2789–2797  2014.

[16] P Hennig  MA Osborne  and M Girolami. Probabilistic Numerics and Uncertainty in Computations.

Proceedings of the Royal Society A  471(2179):20150142  2015.

[17] F Huszár and D Duvenaud. Optimally-Weighted Herding is Bayesian Quadrature. In Uncertainty in

Artiﬁcial Intelligence  volume 28  pages 377–386  2012.

[18] H Ishwaran and LF James. Gibbs Sampling Methods for Stick-Breaking Priors. Journal of the American

Statistical Association  96(453):161–173  2001.

[19] H Ishwaran and M Zarepour. Exact and Approximate Sum Representations for the Dirichlet Process.

Canadian Journal of Statistics  30(2):269–283  2002.

[20] JB Kadane and GW Wasilkowski. Average case epsilon-complexity in computer science: A Bayesian view.

Bayesian Statistics 2  Proceedings of the Second Valencia International Meeting  pages 361–374  1985.
[21] M Kanagawa  BK Sriperumbudur  and K Fukumizu. Convergence Guarantees for Kernel-Based Quadrature
Rules in Misspeciﬁed Settings. In Advances in Neural Information Processing Systems  volume 30  2016.

[22] T Karvonen and S Särkkä. Fully symmetric kernel quadrature. arXiv:1703.06359  2017.
[23] MC Kennedy and A O’Hagan. Bayesian calibration of computer models. Journal of the Royal Statistical

Society: Series B  63(3):425–464  2001.

[24] AWC Lee  A Crozier  ER Hyde  P Lamata  M Truong  M Sohal  T Jackson  JM Behar  S Claridge 
A Shetty  E Sammut  G Plank  CA Rinaldi  and S Niederer. Biophysical Modeling to Determine the
Optimization of Left Ventricular Pacing Site and AV/VV Delays in the Acute and Chronic Phase of Cardiac
Resynchronization Therapy. Journal of Cardiovascular Electrophysiology  28(2):208–215  2016.

[25] GR Mirams  P Pathmanathan  RA Gray  P Challenor  and RH Clayton. White paper: Uncertainty and
Variability in Computational and Mathematical Models of Cardiac Physiology. The Journal of Physiology 
594(23):6833–6847  2016.

[26] E Novak and H Wo´zniakowski. Tractability of Multivariate Problems  Volume II : Standard Information

for Functionals. EMS Tracts in Mathematics 12  2010.

[27] A O’Hagan. Monte Carlo is fundamentally unsound. Journal of the Royal Statistical Society  Series D 

36(2/3):247–249  1987.

1991.

[28] A O’Hagan. Bayes–Hermite Quadrature. Journal of Statistical Planning and Inference  29(3):245–260 

[29] M Osborne  R Garnett  S Roberts  C Hart  S Aigrain  and N Gibson. Bayesian quadrature for ratios. In

Artiﬁcial Intelligence and Statistics  pages 832–840  2012.

[30] MA Osborne  DK Duvenaud  R Garnett  CE Rasmussen  SJ Roberts  and Z Ghahramani. Active learning
of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems 
2012.

[31] C Rasmussen and C Williams. Gaussian Processes for Machine Learning. MIT Press  2006.
[32] C Robert and G Casella. Monte Carlo Statistical Methods. Springer Science & Business Media  2013.
[33] S Särkkä  J Hartikainen  L Svensson  and F Sandblom. On the relation between Gaussian process

quadratures and sigma-point methods. Journal of Advances in Information Fusion  11(1):31–46  2016.

[34] J Sethuraman. A Constructive Deﬁnition of Dirichlet Priors. Statistica Sinica  4(2):639–650  1994.
[35] A Smola  A Gretton  L Song  and B Schölkopf. A Hilbert Space Embedding for Distributions. Algorithmic

Learning Theory  Lecture Notes in Computer Science  4754:13–31  2007.

[36] R Von Mises. Mathematical Theory of Probability and Statistics. Academic  London  1974.
[37] MP Wand and MC Jones. Kernel Smoothing. CRC Press  1994.

9

,Yao-Liang Yu
Chris Oates
Steven Niederer
Angela Lee
François-Xavier Briol
Mark Girolami
Vladimir Kniaz
Vladimir Knyaz
Fabio Remondino