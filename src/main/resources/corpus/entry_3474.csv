2016,Mistake Bounds for Binary Matrix Completion,We study the problem of completing a binary matrix in an online learning setting. On each trial we predict a matrix entry and then receive the true entry. We propose a Matrix Exponentiated Gradient algorithm [1] to solve this problem. We provide a mistake bound for the algorithm  which scales with the margin complexity [2  3] of the underlying matrix. The bound suggests an interpretation where each row of the matrix is a prediction task over a finite set of objects  the columns. Using this we show that the algorithm makes a number of mistakes which is comparable up to a logarithmic factor to the number of mistakes made by the Kernel Perceptron with an optimal kernel in hindsight. We discuss applications of the algorithm to predicting as well as the best biclustering and to the problem of predicting the labeling of a graph without knowing the graph in advance.,Mistake Bounds for Binary Matrix Completion

Mark Herbster

University College London

Department of Computer Science

London WC1E 6BT  UK

m.herbster@cs.ucl.ac.uk

Stephen Pasteris

University College London

Department of Computer Science

London WC1E 6BT  UK

s.pasteris@cs.ucl.ac.uk

Massimiliano Pontil

Istituto Italiano di Tecnologia

16163 Genoa  Italy

and

University College London

Department of Computer Science

London WC1E 6BT  UK
m.pontil@cs.ucl.ac.uk

Abstract

We study the problem of completing a binary matrix in an online learning setting.
On each trial we predict a matrix entry and then receive the true entry. We propose
a Matrix Exponentiated Gradient algorithm [1] to solve this problem. We provide a
mistake bound for the algorithm  which scales with the margin complexity [2  3] of
the underlying matrix. The bound suggests an interpretation where each row of
the matrix is a prediction task over a ﬁnite set of objects  the columns. Using this
we show that the algorithm makes a number of mistakes which is comparable up
to a logarithmic factor to the number of mistakes made by the Kernel Perceptron
with an optimal kernel in hindsight. We discuss applications of the algorithm to
predicting as well as the best biclustering and to the problem of predicting the
labeling of a graph without knowing the graph in advance.

Introduction

1
We consider the problem of predicting online the entries in an m ⇥ n binary matrix U. We formulate
this as the following game: nature queries an entry (i1  j1); the learner predicts ˆy1 2 {1  1} as the
matrix entry; nature presents a label y1 = Ui1 j1; nature queries the entry (i2  j2); the learner predicts
ˆy2; and so forth. The learner’s goal is to minimize the total number of mistakes M = |{t : ˆyt 6= yt}|.
If nature is adversarial  the learner will always mispredict  but if nature is regular or simple  there is
hope that a learner may make only a few mispredictions.
In our setting we are motivated by the following interpretation of matrix completion. Each of the
m rows represents a task (or binary classiﬁer) and each of the n columns is associated with an
object (or input). A task is the problem of predicting the binary label of each of the objects. For a
single task  if we were given a kernel matrix between the objects in advance we could then use the
Kernel Perceptron algorithm to sequentially label the objects and this algorithm would incur O(1/2)
mistakes  where  is the margin of the best linear classiﬁer in the inner product space induced by
the kernel. Unfortunately  in our setup  we do not know a good kernel in advance. However  we will
show that a remarkable property of our algorithm is that it enjoys  up to logarithmic factors  a mistake
bound of O(1/2) per task  where  is the largest possible margin (over the choice of the kernel)
which is achieved on all tasks.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

The problem of predicting online the labels of a ﬁnite set of objects under the assumption that the
similarity between objects can be described by a graph was introduced in [4]  building upon earlier
work in the batch setting [5  6]. In this and later research the common assumption is that two objects
are similar if there is an edge in the graph connecting them and the aim is to predict well when there
are few edges between objects with disagreeing labels. Lower bounds and an optimal algorithm (up
to logarithmic factors) for this problem were given in [7  8]. The problem of predicting well when
the graph is unknown was previously addressed in [9  10]. That research took the approach that when
receiving a vertex to predict  edges local to that vertex were then revealed. In this paper we take a
different approach - the graph structure is never revealed to the learner. Instead  we have a number of
tasks over the same unknown graph  and the hope is to perform comparably to the case in which the
graph in known in advance.
The general problem of matrix completion has been studied extensively in the batch statistical i.i.d.
setting  see for example [11  12  13] and references therein. These studies are concerned either with
Rademacher bounds or statistical oracle inequalities  both of which are substantially different from the
focus of the present paper. In the online mistake-bound setting a special form of matrix completion
was previously considered as the problem of learning a binary relation [14  15] (see Section 5). In
a more general online setting  with minimal assumptions on the loss function [16  17] bounded the
regret of the learner in terms of the trace-norm of the underlying matrix. Instead our bounds are
with respect to the margin complexity of the matrix. As a result  although our bounds have a more
restricted applicability they have the advantage that they become non-trivial after only ˜⇥(n) matrix
entries1 are observed as opposed to the required ˜⇥(n3/2) in [16] and ˜⇥(n7/4) in [17]. The notion
of margin complexity in machine learning was introduced in [2] where it was used to the study the
learnability of concept classes via linear embeddings and further studied in [3]  where it was linked
to the 2 norm. Here we adopt the terminology in [11] and refer to the 2 norm as the max-norm.
The margin complexity seems to be a more natural parameter as opposed to the trace-norm for the
0-1 loss as it only depends on the signs of the underlying comparator matrix. To the best of our
knowledge the bounds contained herein are the ﬁrst online matrix completion bounds in terms of the
margin complexity.
To obtain our results  we use an online matrix multiplicative weights algorithm  e.g.  see [1  18  17  19]
and references therein. These kinds of algorithms have been applied in a number of learning
scenarios  including online PCA [20]  online variance minimization [21]  solving SDPs [18]  and
online prediction with switching sequences [22]. These algorithms update a new hypothesis matrix
on each trial by trading off ﬁdelity to the previous hypothesis and the incorporation of the new label
information. The tradeoff is computed as an approximate spectral regularization via the quantum
relative entropy (see [1  Section 3.1]). The particular matrix multiplicative weights algorithm we
apply is Matrix Winnow [19]; we adapt this algorithm and its mistake bound analysis for our purposes
via selection of comparator  threshold  and appropriate “progress inequalities.”
The paper is organized as follows. In Section 2 we introduce basic notions used in the paper. In
Section 3 we present our algorithm and derive a mistake bound  also comparing it to related bounds
in the literature. In Section 4 we observe that our algorithm is able to exploit matrix structure to
perform comparably to the Kernel Perceptron with the best kernel known in advance. Finally  in
Section 5 we discuss the example of biclustered matrices  and argue that our bound is optimal up to a
polylogarithmic factor. The appendix contains proofs of the results only stated in the main body of
the paper  and other auxiliary results.

2 Preliminaries

We denote the set of the ﬁrst m positive integers as Nm = {1  . . .   m}. We denote the inner
i=1 xiwi and the norm as |w| = phw  wi. We
product of vectors x  w 2 Rn as hx  wi = Pn
let Rm⇥n be the set of all m ⇥ n real-valued matrices. If X 2 Rm⇥n then X i denotes the i-th
n-dimensional row vector and the (i  j) entry in X is Xij. The trace of a square matrix X 2 Rn⇥n
i=1 Xii. The trace norm of a matrix X 2 Rm⇥n is kXk1 = Tr(pX>X)  where
is Tr(X) =Pn
p· indicates the unique positive square root of a positive semi-deﬁnite matrix. For every matrix
U 2 {1  1}m⇥n  we deﬁne SP(U ) = {V 2 Rm⇥n : 8ijVijUij > 0}  the set of matrices which

1For simplicity we assume m 2 ⇥(n).

2

are sign consistent with U. We also deﬁne SP1(U ) = {V 2 Rm⇥n : 8ijVijUij  1}  that is the set
of matrices which are sign consistent to U with a margin of at least one.
The max-norm (or 2 norm [3]) of a matrix U 2 Rm⇥n is deﬁned by the formula

kUkmax := inf

P Q>=U⇢ max

1im|P i| max

1jn|Qj|  

where the inﬁmum is over all matrices P 2 Rm⇥k and Q 2 Rn⇥k and every integer k. The margin
complexity of a matrix U 2 Rm⇥n is

mc(U ) :=

inf

P Q>2SP(U )

max

ij

|P i||Qj|
|hP i  Qji|

.

This quantity plays a central role in the analysis of our algorithm. If we interpret the rows of U as m
different binary classiﬁcation tasks  and the columns as a ﬁnite set of objects which we wish to label 
the “min-max” margin with respect to an embedding is smallest of the m maximal margins over the
tasks. The quantity 1/ mc(U ) is then the maximum “min-max” margin with respect to all possible
embeddings. Speciﬁcally  the rows of matrix P represent the “weights” of the binary classiﬁers
and the rows of matrix Q the “input vectors” associated with the objects. The quantity |hP i Qji|
|P i||Qj|
is the margin of the i-th classiﬁer on the j-th input. Observe that margin complexity depends only
on the sign pattern of the matrix and not the magnitudes. The margin complexity is equivalently
mc(U ) = minV 2SP1(U ) kV kmax  see e.g.  [3  Lemma 3.1].
In our online
sequence
((i1  j1)  y1)  . . .   ((iT   jT )  yT ) 2 (Nm ⇥ Nn) ⇥ {1  1}. A sequence must be consistent  that is 
given examples ((i  j)  y) and ((i0  j0)  y0) if (i  j) = (i0  j0) then y = y0. We deﬁne the set of sign-
consistent matrices with a sequence S as cons(S) := {M 2 Rm⇥n : 0 < yMij  ((i  j)  y) 2S} .
We extend the notion of margin complexity to sequences via mc(S) := inf U2cons(S) mc(U ).
The number of margin violations in a sequence S at complexity  is deﬁned to be 

concerned with predicting an (example)

setting we

are

merr(S  ) :=

inf

P Q>2cons(S)⇢((i  j)  y) 2S :

|P i||Qj|
|hP i  Qji|

>

1

 .

In particular  note that merr(S  ) = 0 if   1
Finally  we introduce the following quantity  which plays a central role in the amortized analysis of
our algorithm.
Deﬁnition 2.1. The quantum relative entropy of symmetric positive semideﬁnite square matrices A
and B is

mc(S).

(A  B) := Tr(A log (A)  A log (B) + B  A).

(1)

(2)

(3)

3 Algorithm and Analysis

Algorithm 1 presents an adaptation of the Matrix Exponentiated Gradient algorithm [1  17  18  19] to
our setting. This algorithm is a matrix analog of the Winnow algorithm [19]; we refer to the above
papers for more insights into this family of algorithms.
The following theorem provides a mistake bound for the algorithm.
Theorem 3.1. The number of mistakes  M  on sequence S made by the Algorithm 1 with parameter
0 <  1 is upper bounded by

M  c(m + n) log(m + n)

1

2 + merr(S  )  

where c = 1/(3  e)  3.55 and the quantity merr(S  ) is given in equation (2).
Proof. Given U 2 Rm⇥n  let P 2 Rm⇥k and Q 2 Rn⇥k be such that P Q> = U. For every
i 2 Nm  we denote by P i the i-th row vector of P and for every j 2 Nn  we denote by Qj the j-th
row vector of Q. We construct the (m + n) ⇥ k matrix
 

  . . .  

  . . .  

1

R := diag✓ 1

|P 1|

|Qn|◆P
Q

1
|Q1|

1
|P m|
3

Algorithm 1 Predicting a binary matrix.
Parameters: Learning rate 0 <  1 .
Initialization: W (0) I
For t = 1  . . .   T

(m+n)  where I is the (m + n) ⇥ (m + n) identity matrix.

2 (eit + em+jt)(eit + em+jt)>  where ek is the k-th basis vector of Rm+n.

• Get pair (it  jt) 2 Nm ⇥ Nn.
• Deﬁne X (t) := 1
• Predict
ˆyt =(1
if Tr(W (t1)X (t))  1
otherwise.
• Receive label yt 2 {1  1} and if ˆyt 6= yt update
W (t) exp⇣log⇣W (t1)⌘ +

1


2

m+n  

(yt  ˆyt)X (t)⌘ .

1

and construct ˜U := (
ek is the k-th basis vector of Rm+n.
Note that Tr(X (t)) = 1  Tr( ˜U ) = 1 (since every row of R is normalized) and

m+n )RR>. Deﬁne matrix X (t) := 1

2 (eit + em+jt)(eit + em+jt)>  where

1

n + m
1

2(n + m)

1

2(n + m)

Tr( ˜U X (t)) =

=

=

=

=

Tr((RR>)

1
2

(eit + em+jt)(eit + em+jt)>)

(eit + em+jt)>RR>(eit + em+jt)

(R>(eit + em+jt))>(R>(eit + em+jt))

1

+

Qjt

|Qjt|◆✓ P it

2(n + m)✓ P it
(n + m)✓1 + hP it  Qjti
|P it||Qjt|◆ .

|P it|

|P it|

1

+

Qjt

|Qjt|◆>

> 1

For a trial t we say there is a margin violation if |P it||Qjt|
 . Let M denote the number of
|hP it  Qjti|
mistakes made in trials with margin violations and let M + denote the number of mistakes made in
trials without margin violations.
From Lemma A.3 in the appendix we have
( ˜U   W (t1))  ( ˜U   W (t)) 
then substituting in the above we have that

(yt  ˆyt) Tr( ˜U X (t)) +⇣1  e


2



2 (ytˆyt)⌘ Tr(W (t1)X (t))  
n + m✓1 + hP it  Qjti
|P it||Qjt|◆
+⇣1  e

2 (ytˆyt)⌘ Tr(W (t1)X (t)) .

1



( ˜U   W (t1))  ( ˜U   W (t)) 


2

(yt  ˆyt)

To further simplify the above we use Lemma A.4 presented in the appendix  which gives

( ˜U   W (t1))  ( ˜U   W (t)) 8<:

where c0 = 3  e.

(c0  1)
c0

4

1
n+m 2 

if there is a margin violation  

1
n+m 2 

otherwise.

Using a telescoping sum  this gives
( ˜U   W (0))  ( ˜U   W (0))  ( ˜U   W (T ))  M +c0

= (c0M +  (1  c0)M)

1

n + m

2

1

n + m

2 + M(c0  1)

1

n + m

2

and hence

We conclude that

We also have that

M + 

c0

1
1

n+m 2 ( ˜U   W (0)) +

1  c0
c0

M .

M = M + + M 

c0

1
1

n+m 2 ( ˜U   W (0)) +

1
c0

M .

( ˜U   W (0)) = Tr( ˜U log( ˜U ))  Tr( ˜U log(W (0))) + Tr(W (0))  Tr( ˜U )

= Tr( ˜U log( ˜U ))  Tr( ˜U log(W (0))) + 1  1
= Tr( ˜U log( ˜U ))  Tr( ˜U log(W (0))) .

Write the eigen-decomposition of ˜U asPm+n
i=1 i = Tr( ˜U ) = 1
so all eigenvalues i are in the range [0  1] meaning log(i)  0 so i log(i) < 0 which are the
eigenvalues of ˜U log( ˜U ) meaning that Tr( ˜U log( ˜U ))  0. Also  log(W (0)) = log(
n+m )I so
˜U log(W (0)) = log(
n+m ) Tr( ˜U ) = log(m + n).
1
So by the above we have

n+m ) ˜U and hence  Tr( ˜U log(W (0))) =  log(

i . Now we havePm+n

i=1 i↵i↵T

1

1

and hence putting together we get

( ˜U   W (0))  log(m + n)

M 

m + n
c02

log(m + n) +

M .

1
c0

Observe that in the simplifying case when we have no margin errors (merr(S  ) = 0) and the
learning rate is  := 1
mc(S) we have that the number of mistakes of Algorithm 1 is bounded by
˜O((n + m) mc2(S)). More generally although the learning rate is ﬁxed in advance  we may use a
“doubling trick” to avoid the need to tune the .
Corollary 3.2. For any value of ⇤ the number of mistakes M made by the following algorithm:
DOUBLING ALGORITHM:

Set  p2 and loop over

1. Run Algorithm 1 with  = 1

2. Set  p2

is upper bounded by

 until it has made d2c(m + n) log(m + n)2e mistakes

M  12c(m + n) log(m + n)

1

(⇤)2 + merr(S  ⇤)  

with c = 1/(3  e) ⇡ 3.55.
See the appendix for a proof. We now compare our bound to other online learning algorithms for
matrix completion. The algorithms of [16  17] address matrix completion in a signiﬁcantly more
general setting. Both algorithms operate with weak assumptions on the loss function  while our
algorithm is restricted to the 0–1 loss (mistake counting). Those papers present regret bounds 
whereas we apply the stronger assumption that there exists a consistent predictor. As a regret bound
is not possible for a deterministic predictor with the 0–1 loss  we compare Theorem 3.1 to their

5

1
3

1
3

2
3

1
2

1 .

(4)

the

above

assumptions 

mc(U )  3 min

bound when their algorithm is allowed to predict ˆy 2 [1  1] and uses absolute loss. For clarity in
our discussion we will assume that m 2 ⇥(n).
the
regret bound in [17  Corollary 7] becomes
Under
2pkUk1(m + n)1/2 log(m + n)T .
For simplicity we consider the simpliﬁed setting in
which each entry is predicted  that is T = mn; then absorbing polylogarithmic factors  their bound is
˜O(n5/4kUk
1 ). From Theorem 3.1 we have a bound of ˜O(n mc2(U )). Using [11  Theorem 10]  we
may upper bound the margin complexity in terms of the trace norm 
1  3kUk
V 2SP1(U )kV k
Substituting this into Theorem 3.1 our bound is ˜O(nkUk
1 ). Since the trace norm may be bounded
as n  kUk1  n3/2  both bounds become vacuous when kUk1 = n3/2  however if the trace norm
is bounded away from n3/2  the bound of Theorem 3.1 is smaller by a polynomial factor. An aspect
of the bounds which this comparison fails to capture is the fact that since [17  Corollary 7] is a regret
bound it will degrade more smoothly under adversarial noise than Theorem 3.1.
The algorithm in [16] is probabilistic and the regret bound is of ˜O(kUk1pn). Unlike [17]  the setting
of [16] is transductive  that is each matrix entry is seen only once  and thus less general. If we use the
upper bound from [11  Theorem 10] as in the discussion of [17] then [16] improves uniformly on
our bound and the bound in [17]. However  using this upper bound oversimpliﬁes the comparison
as 1  mc2(U )  n while n  kUk1  n3/2 for U 2 {1  1}m⇥n. In other words we have been
very conservative in our comparison; the bound (4) may be loose and our algorithm may often have a
much smaller bound. A speciﬁc example is provided by the class of (k  `)-biclustered matrices (see
also the discussion in Section 5 below) where mc2(U )  min(k  `)  in which case bound becomes
nontrivial after ˜⇥(min(k  `) n) examples while the bounds in [16] and [17] become nontrivial after
at least ˜⇥(n3/2) and ˜⇥(n7/4) examples  respectively.
With respect to computation our algorithm on each trial requires a single eigenvalue decomposition
of a PSD matrix  whereas the algorithm of [17] requires multiple eigenvalue decompositions per trial.
Although [16] does not discuss the complexity of their algorithm beyond the fact that it is polynomial 
in [17] it is conjectured that it requires at a minimum ⇥(n4) time per trial.

4 Comparison to the Best Kernel Perceptron

In this section  we observe that Algorithm 1 has a mistake bound that is comparable to Novikoff’s
bound [23] for the Kernel Perceptron with an optimal kernel in hindsight. To explain our observation 
we interpret the rows of matrix U as m different binary classiﬁcation tasks  and the columns as a ﬁnite
set of objects which we wish to label; think for example of users/movies matrix in recommendation
systems. If we solve the tasks independently using a Kernel Perceptron algorithm  we will make
O(1/2) mistakes per task  where  is the largest margin of a consistent hypothesis. If every task has
a margin larger than  we will make O(m/2) mistakes in total. This algorithm and the parameter 
crucially depend on the kernel used: if there exists a kernel which makes  large for all (or most of)
the tasks  then the Kernel Perceptron will incur a small number of mistakes on all (or most of) the
tasks. We now argue that our bound mimics this “oracle”  without knowing in advance the kernel.
Without loss of generality  we assume m  n (otherwise apply the same reasoning below to matrix
U >). In this scenario  Theorem 3.1 upper bounds the number of mistakes as

O✓ m log m
2 ◆

6

where  is chosen so that merr(S  ) = 0. To further illustrate our idea  we deﬁne the task complexity
of a matrix U 2 Rm⇥n as

⌧ (U ) = minh(V ) : V 2 SP1(U ) 

where

(5)
Note that the quantity V iK1V >
i max1jn Kjj is exactly the bound in Novikoff’s Theorem on
the number of mistakes of the Kernel Perceptron on the i-th task with kernel K. Hence the quantity

h(V ) = inf
K0

i max
1jn

max
1im

V iK1V >

Kjj .

h(V ) represents the best upper bound on the number of mistakes made by a Kernel Perceptron on
the worst (since we take the maximum over i) task.
Proposition 4.1. For every U 2 Rm⇥n  it holds that mc2(U ) = ⌧ (U ).
Proof. The result follows by Lemma A.6 presented in the appendix and by the formula mc(U ) =
minV 2SP1(U ) kV kmax  see  e.g.  [3  Lemma 3.1].
Returning to the interpretation of the bound in Theorem 3.1  we observe that if no more than r out of
the m tasks have margin smaller than a threshold  then in Algorithm 1 setting parameter  =  
Theorem 3.1 gives a bound of

O✓ (m  r) log m

2

+ rn◆ .

Thus we essentially “pay” linearly for every object in a difﬁcult task. Since we assume n  m 
provided r is small the bound is “robust” to the presence of bad tasks.
We specialize the above discussion to the case that each of the m tasks is a binary labeling of an
unknown underlying connected graph G := (V E) with n vertices and assume that m  n. We
let U 2 {1  1}m⇥n be the matrix  the rows of which are different binary labelings of the graph.
For every i 2 Nm  we interpret U i  the i-th row of matrix U  as the i-th labeling of the graph
and let i be the corresponding cutsize  namely  i := |{(j  j0) 2E : Uij 6= Uij0}| and deﬁne
max := max1im i. In order to apply Theorem 3.1  we need to bound the margin complexity of
U. Using the above analysis (Proposition 4.1)  this quantity is upper bounded by

mc2(U )  max
1im

U iK1U >

i max
1jn

Kjj.

(6)

We choose the kernel K := L+ + (R 11T )  where L is the graph Laplacian of G  the vector 1
has all components equal to one  and R = maxj L+
jj. Since the graph is connected then 1 is the
only eigenvector of L with zero eigenvalue. Hence K is invertible and K1 = L + (R 11T )+ =
i we obtain from (6)
L + (R n 1pn 11T 1pn )+ = L + 1
that

Rn2 11T . Then using the formula i = 1

4 U iLU >

mc2(U )  max

1im✓4i +

1

R◆ R .

Theorem 3.1 then gives a bound of M  O ((1 + maxR) m log m). The quantity R may be further
upper bounded by the graph resistance diameter  see for example [24].

5 Biclustering and Near Optimality

The problem of learning a (k  `)-binary-biclustered matrix  corresponds to the assumption that the
row indices and column indices represent k and ` distinct object types and that there exists a binary
relation on these objects which determines the matrix entry. Formally we have the following
Deﬁnition 5.1. The class of (k  `)-binary-biclustered matrices is deﬁned as

Bm n
k ` = {U 2 Rm⇥n : r 2 Nm

k   c 2 Nn

`   F 2 {1  1}k⇥`  Uij = Fricj   i 2 Nm  j 2 Nn} .

The intuition is that a matrix is (k  `)-biclustered if after a permutation of the rows and columns the
resulting matrix is a k ⇥ ` grid of rectangles and all entries in a given rectangle are either 1 or 1.
The problem of determining a (k  `)-biclustered matrix with a minimum number of “violated” entries
given a subset of entries was shown to be NP-hard in [25]. Thus although we do not give an algorithm
that provides a biclustering  we provide a bound in terms of the best consistent biclustering.
Lemma 5.2. If U 2 Bm n
Proof. We use Proposition 4.1 to upper bound mc2(U ) by h(U )  where the function h is given
in equation (5). We further upper bound h(U ) by choosing a kernel matrix in the underlying
optimization problem. By Deﬁnition 5.1  there exists r 2 Nm
` and F 2 {1  1}k⇥`

then mc2(U )  min(k  `).

k   c 2 Nn

k `

7

such that Uij = Fricj   for every i 2 Nm and every j 2 Nn. Then we choose the kernel matrix
K = (Kjj0)1j j0n such that

Kjj0 := cj c0j

+ ✏jj0

One veriﬁes that U iK1U >
i  ` for every i 2{ 1  . . .   m}  hence by taking the limit for ✏ ! 0
Proposition 4.1 gives that mc2(U )  ` . By the symmetry of our construction we can swap ` with k 
giving the bound.

Using this lemma with Theorem 3.1 gives us the following upper bound on the number of mistakes.
Corollary 5.3. The number of mistakes of Algorithm 1 applied to sequences generated by a (k  `)-
binary-biclustered matrix is upper bounded by O(min(k  `)(m + n) log(m + n)).
A special case of the setting in this corollary was ﬁrst studied in the mistake bound setting in [14].
In [15] the bound was improved and generalized to include robustness to noise (for simplicity we do
not compare in the noisy setting). In both papers the underlying assumption is that there are k distinct
row types and no restrictions on the number of columns thus ` = n. In this case they obtained an
upper bound of kn + min( m2
when k < n 1
and on other hand when k  n 1
We now establish that the mistake bound (3) is tight up to a poly-logarithmic factor.
Theorem 5.4. Given an online algorithm A that predicts the entries of a matrix U 2 {1  1}m⇥n
and given an ` 2 Nn there exists a sequence S constructed by an adversary with margin complexity
mc(S)  p`. On this sequence the algorithm A will make at least ` ⇥ m mistakes.

2e log2 e  mp3n log2 k). Comparing the two bounds we can see that

2✏ the bound in Corollary 5.3 improves over [15  Corollary 1] by a polynomial factor

2 we are no worse than a polylogarithmic factor.

See the appendix for a proof.

6 Conclusion

In this paper  we presented a Matrix Exponentiated Gradient algorithm for completing the entries of a
binary matrix in an online learning setting. We established a mistake bound for this algorithm  which
is controlled by the margin complexity of the underlying binary matrix. We discussed improvements
of the bound over related bounds for matrix completion. Speciﬁcally  we noted that our bound requires
fewer examples before it becomes non-trivial  as compared to the bounds in [16  17]. Here we require
only ˜⇥(m + n) examples as opposed to the required ˜⇥((m + n)3/2) in [16] and ˜⇥((m + n)7/4) 
respectively. Thus although our bound is more sensitive to noise  it captures structure more quickly
in the underlying matrix. When interpreting the rows of the matrix as binary tasks  we argued that
our algorithm performs comparably (up to logarithmic factors) to the Kernel Perceptron with the
optimal kernel in retrospect. Finally  we highlighted the example of completing a biclustered matrix
and noted that this is instrumental in showing the optimality of the algorithm in Theorem 5.4.
We observed that Algorithm 1 has a per trial computational cost which is smaller than currently
available algorithms for matrix completion with online guarantees. In the future it would be valuable
to study if improvements in this computation are possible by exploiting the special structure in our
algorithm. Furthermore  it would be very interesting to study a modiﬁcation of our analysis to the
case in which the tasks (rows of matrix U) grow over time  a setting which resembles the lifelong
learning frameworks in [26  27].

Acknowledgements. We wish to thank the anonymous reviewers for their useful comments. This work was
supported in part by EPSRC Grants EP/P009069/1  EP/M006093/1  and by the U.S. Army Research Laboratory
and the U.K. Defence Science and Technology Laboratory and was accomplished under Agreement Number
W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should
not be interpreted as representing the ofﬁcial policies  ether expressed or implied  of the U.S. Army Research
Laboratory  the U.S. Government  the U.K. Defence Science and Technology Laboratory or the U.K. Government.
The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation herein.

8

References
[1] K. Tsuda  G. Rätsch  and M.K. Warmuth. Matrix exponentiated gradient updates for on-line learning and

bregman projection. Journal of Machine Learning Research  6:995–1018  2005.

[2] S. Ben-David  N. Eiron  and H. U. Simon. Limitations of learning via embeddings in euclidean half spaces.

Journal of Machine Learning Research  3:441–461  2003.

[3] N. Linial  S. Mendelson  G. Schechtman  and A. Shraibman. Complexity measures of sign matrices.

Combinatorica  27(4):439–463  2007.

[4] M. Herbster  M. Pontil  and L. Wainer. Online learning over graphs.

International Conference on Machine Learning  pages 305–312  2005.

In Proceedings of the 22nd

[5] X. Zhu  Z. Ghahramani  and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In Proc. 20th International Conference on Machine Learning  pages 912–919  2003.

[6] M. Belkin and P. Niyogi. Semi-supervised learning on riemannian manifolds. Machine Learning  56:209–

239  2004.

[7] N. Cesa-Bianchi  C. Gentile  and F. Vitale. Fast and optimal prediction of a labeled tree. In Proceedings of

the 22nd Annual Conference on Learning Theory  2009.

[8] N. Cesa-Bianchi  C. Gentile  F. Vitale  and G. Zappella. Random spanning trees and the prediction of

weighted graphs. Journal of Machine Learning Research  14(1):1251–1284  2013.

[9] N. Cesa-Bianchi  C. Gentile  and F. Vitale. Predicting the labels of an unknown graph via adaptive

exploration. Theoretical Computer Science  412(19):1791–1804  2011.

[10] C. Gentile  M. Herbster  and S. Pasteris. Online similarity prediction of networked data from known and

unknown graphs. In Proceedings of the 26th Annual Conference on Learning Theory  2013.

[11] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. In Proceedings of the 18th Annual

Conference on Learning Theory  pages 545–560  2005.

[12] E. J. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans.

Inf. Theor.  56(5):2053–2080  May 2010.

[13] A. Maurer and M. Pontil. Excess risk bounds for multitask learning with trace norm regularization. In

Proceedings of The 27th Conference on Learning Theory (COLT)  pages pages 55–76  2013.

[14] S. A. Goldman  R. L. Rivest  and R. E. Schapire. Learning binary relations and total orders. SIAM J.

Comput.  22(5)  1993.

[15] S. A. Goldman and M. K. Warmuth. Learning binary relations using weighted majority voting.
Proceedings of the 6th Annual Conference on Computational Learning Theory  pages 453–462  1993.

In

[16] N. Cesa-Bianchi and O. Shamir. Efﬁcient online learning via randomized rounding. In Advances in Neural

Information Processing Systems 24  pages 343–351  2011.

[17] E. Hazan  S. Kale  and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. In Proc.

23rd Annual Conference on Learning Theory  volume 23:38.1-38.13. JMLR W&CP  2012.

[18] S. Arora and S. Kale. A combinatorial  primal-dual approach to semideﬁnite programs. In Proceedings of

the 29th Annual ACM Symposium on Theory of Computing  pages 227–236  2007.

[19] M.K. Warmuth. Winnowing subspaces. In Proceedings of the 24th International Conference on Machine

Learning  pages 999–1006  2007.

[20] J. Nie  W. Kotłowski  and M. K. Warmuth. Online PCA with optimal regrets. In Proceedings of the 24th

International Conference on Algorithmic Learning Theory  pages 98–112  2013.

[21] M. K. Warmuth and D. Kuzmin. Online variance minimization. Machine Learning  87(1):1–32  2012.
[22] M. Herbster  S. Pasteris  and S. Pontil. Predicting a switching sequence of graph labelings. Journal of

Machine Learning Research  16:2003–2022  2015.

[23] A.B. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathe-

matical Theory of Automata  pages 615–622  1962.

[24] M. Herbster and M. Pontil. Prediction on a graph with a perceptron. In Advances in Neural Information

Processing Systems 19  pages 577–584  2006.

[25] S. Wulff  R. Urner  and S. Ben-David. Monochromatic bi-clustering. In Proc. 30th International Conference

on Machine Learning  volume 28  pages 145–153. JMLR W&CP  2013.

[26] P. Alquier  T.-T. Mai  and M. Pontil. Regret bounds for lifelong learning. Preprint  2016.
[27] M.-F. Balcan  A. Blum  and S. Vempala. Efﬁcient representations for lifelong learning and autoencoding.

In Proc. 28th Conference on Learning Theory  pages 191–210  2015.

[28] R. Bhatia. Matrix Analysis. Springer Verlag  New York  1997.

9

,Mark Herbster
Stephen Pasteris
Massimiliano Pontil