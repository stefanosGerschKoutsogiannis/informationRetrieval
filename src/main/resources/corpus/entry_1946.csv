2018,Exponentially Weighted Imitation Learning for Batched Historical Data,We consider deep policy learning with only batched historical trajectories. The main challenge of this problem is that the learner no longer has a simulator or ``environment oracle'' as in most reinforcement learning settings. To solve this problem  we propose a monotonic advantage reweighted imitation learning strategy that is applicable to problems with complex nonlinear function approximation and works well with hybrid (discrete and continuous) action space. The method does not rely on the knowledge of the behavior policy  thus can be used to learn from data generated by an unknown policy. Under mild conditions  our algorithm  though surprisingly simple  has a policy improvement bound and outperforms most competing methods empirically. Thorough numerical results are also provided to demonstrate the efficacy of the proposed methodology.,Exponentially Weighted Imitation Learning for

Batched Historical Data

Qing Wang1

Jiechao Xiong1 Lei Han1 Peng Sun1 Han Liu12 Tong Zhang1

1Tencent AI Lab

2Northwestern University

{drwang  jcxiong  lxhan  pythonsun}@tencent.com

hanliu@northwestern.edu  tongzhang@tongzhang-ml.org

Abstract

We consider deep policy learning with only batched historical trajectories. The
main challenge of this problem is that the learner no longer has a simulator or
“environment oracle” as in most reinforcement learning settings. To solve this
problem  we propose a monotonic advantage reweighted imitation learning strategy
that is applicable to problems with complex nonlinear function approximation
and works well with hybrid (discrete and continuous) action space. The method
does not rely on the knowledge of the behavior policy  thus can be used to learn
from data generated by an unknown policy. Under mild conditions  our algorithm 
though surprisingly simple  has a policy improvement bound and outperforms most
competing methods empirically. Thorough numerical results are also provided to
demonstrate the efﬁcacy of the proposed methodology.

1

Introduction

In this article  we consider the problem of learning a deep policy with batched historical trajectories.
This problem is important and challenging. As in many real-world tasks  we usually have numerous
historical data generated by different policies  but is lack of a perfect simulator of the environment. In
this case  we want to learn a good policy from these data  to make decisions in a complex environment
with possibly continuous state space and hybrid action space of discrete and continuous parts.
Several existing ﬁelds of research concern the problem of policy learning from batched data. In
particular  imitation learning (IL) aims to ﬁnd a policy whose performance is close to that of the
data-generating policy [Abbeel and Ng  2004]. On the other hand  off-policy reinforcement learning
(RL) concerns the problem of learning a good (or possibly better) policy with data collected from a
behavior policy [Sutton and Barto  1998]. However  to the best of our knowledge  previous methods
do not have satisﬁable performance or are not directly applicable in a complex environment as ours
with continuous state and hybrid action space.
In this work  we propose a novel yet simple method  to imitate a better policy by monotonic advantage
reweighting. From theoretical analysis and empirical results  we ﬁnd the proposed method has several
advantages that

• From theoretical analysis  we show that the algorithm as proposed has policy improvement

lower bound under mild condition.

• Empirically  the proposed method works well with function approximation and hybrid action

space  which is crucial for the success of deep RL in practical problems.

• For off-policy learning  the method does not rely on the knowledge of action probability of
the behavior policy  thus can be used to learn from data generated by an unknown policy 
and is robust when current policy is deviated from the behavior policy.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In our real-world problem of a complex MOBA game  the proposed method has been successfully
applied on human replay data  which validates the effectiveness of the method.
The article is organized as follows: We ﬁrstly state some preliminaries (Sec. 2) and related works
(Sec. 3). Then we present our main method of imitating a better policy (Sec. 4)  with theoretical
analysis (Sec. 5) and empirical experiments (Sec. 6). Finally we conclude our discussion (Sec. 7).

2 Preliminaries
Consider a Markov decision process (MDP) with inﬁnite-horizon  denoted by M =
(S A  P  r  d0  γ)  where S is the state space  A is the action space  P is the transition probability de-
ﬁned on S ×A×S → [0  1]  r is the reward function S ×A → R  d0 is the distribution of initial state
s0  and γ ∈ (0  1) is the discount factor. A trajectory τ is a sequence of triplets of state  action and
reward  i.e.  τ = {(st  at  rt)}t=1 ... T   where T is the terminal step number. A stochastic policy de-
noted by π is deﬁned as S×A → [0  1]. We use the following standard notation of state-value V π(st) 
(cid:80)∞
action-value Qπ(st  at) and advantage Aπ(st  at)  deﬁned as V π(st) = Eπ|st
l=0 γlr(st+l  at+l) 
l=0 γlr(st+l  at+l)  and Aπ(st  at) = Qπ(st  at) − V π(st)  where Eπ|st
Qπ(st  at) = Eπ|st at
means al ∼ π(a|sl)  sl+1 ∼ P (sl+1|sl  al)  ∀l ≥ t  and Eπ|st at means sl+1 ∼ P (sl+1|sl  al) 
al+1 ∼ π(a|sl+1)  ∀l ≥ t. As the state space S may be prohibitively large  we approximate the
θ (s) with parameter θ ∈ Θ. We
policy and state-value with parameterized forms as πθ(s  a) and V π
a∈A π(s  a) = 1 ∀s ∈ S  a ∈ A} and
parametrized policy space as ΠΘ = {πθ|θ ∈ Θ}.
To measure the similarity between two policies π and π(cid:48)  we consider the Kullback–Leibler divergence
and total variance (TV) distance deﬁned as

denote the original policy space as Π = {π|π(s  a) ∈ [0  1] (cid:80)

(cid:80)∞

(cid:88)
KL(π(cid:48)||π) =
Dd
TV(π(cid:48)  π) = (1/2)
Dd

s

(cid:88)
(cid:88)

a

d(s)

π(cid:48)(a|s)
π(a|s)

π(cid:48)(a|s) log

(cid:88)

|π(cid:48)(a|s) − π(a|s)|

where d(s) is a probability distribution of states. The performance of a policy π is measured by its
expected discounted reward:

η(π) = Ed0 π

γtr(st  at)

d(s)

s

a

∞(cid:88)

t=0

(cid:88)

s

(cid:88)

(cid:88)

a

(cid:88)

where Ed0 π means s0 ∼ d0  at ∼ π(at|st)  and st+1 ∼ P (st+1|st  at). We omit the subscript d0
when there is no ambiguity. In [Kakade and Langford  2002]  a useful equation has been proved that

η(π(cid:48)) − η(π) =

1
1 − γ

dπ(cid:48)(s)

π(cid:48)(a|s)Aπ(s  a)

where dπ is the discounted visiting frequencies deﬁned as dπ(s) = (1 − γ)Ed0 π
and 1(·) is an indicator function. In addition  deﬁne Ld π(π(cid:48)) as

(cid:80)∞

t=0 γt1(st = s)

Ld π(π(cid:48)) =

1
1 − γ

d(s)

s

a

π(cid:48)(a|s)Aπ(s  a)

then from [Schulman et al.  2015  Theorem 1]  the difference of η(π(cid:48)) and η(π) can be approximated
by Ldπ π(π(cid:48))  where the approximation error is bounded by total variance Ddπ
TV(π(cid:48)  π)  which can be
further bounded by Ddπ
In the following sections  we mainly focus on maximizing Ldπ π(πθ) as a proxy for optimizing policy
performance η(πθ)  for πθ ∈ ΠΘ.

KL(π(cid:48)||π) or Ddπ

KL(π||π(cid:48)).

3 Related Work

Off-policy learning [Sutton and Barto  1998] is a broad region of research. For policy improvement
method with performance guarantee  conservative policy iteration [Kakade and Langford  2002] or

2

safe policy iteration [Pirotta et al.  2013] has long been an interesting topic in the literature. The
term “safety” or “conservative” usually means the algorithm described is guaranteed to produce a
series of monotonic improved policies. Exact or high-probability bounds of policy improvement are
often provided in these previous works [Thomas and Brunskill  2016  Jiang and Li  2016  Thomas
et al.  2015  Ghavamzadeh et al.  2016]. We refer readers to [Garcıa and Fernández  2015] for a
comprehensive survey of safe RL. However  to the best of our knowledge  these prior methods cannot
be directly applied in our problem of learning in a complex game environment with large scale replay
data  as they either need full-knowledge of the MDP or consider tabular case mainly for ﬁnite states
and discrete actions  with prohibitive computational complexity.
Constrained policy optimization problems in the parameter space are considered in previous works
[Schulman et al.  2015  Peters et al.  2010]. In [Peters et al.  2010]  they constrain the policy on the
distribution of pπ(s  a) = µπ(s)π(a|s)  while in [Schulman et al.  2015]  the constraint is on π(a|s) 
with ﬁxed state-wise weight d(s). Also  in [Schulman et al.  2015]  the authors have considered
KL(π||πθ) as a policy divergence constraint  while in [Peters et al.  2010] the authors considered
Ddπ
DKL(µππ||q). The connection with our proposed method is elaborated in Appendix B.1. A closely
related work is [Abdolmaleki et al.  2018] which present the exponential advantage weighting in an
EM perspective. Independently  we further generalize to monotonic advantage re-weighting and also
derive a lower bound for imitation learning.
Besides off-policy policy iteration algorithm  value iteration algorithm can also be used in off-policy
settings. For deep reinforcement learning  DQN [Mnih et al.  2013]  DQfD [Hester et al.  2018] works
primarily with discrete actions  while DDPG [Lillicrap et al.  2016] works well with continuous
actions. For hybrid action space  there are also works combining the idea of DQN and DDPG
[Hausknecht and Stone  2016]. In our preliminary experiments  we found value iteration method
failed to converge for the tasks in the HFO environment. It seems that the discrepancy between
behavior policy and the target policy (arg max policy in DQN) should be properly restrained  which
we think worth further research and investigation.
Also  there are existing related methods in the ﬁeld of imitation learning. For example  when expert
data is available  we can learn a policy directly by predicting the expert action [Bain and Sommut 
1999  Ross et al.  2011]. Another related idea is to imitate an MCTS policy [Guo et al.  2014  Silver
et al.  2016]. In the work of [Silver et al.  2016]  the authors propose to use Monte-Carlo Tree Search
(MCTS) to form a new policy ˜π = MCTS(π) where π is the base policy of network  then imitate
the better policy ˜π by minimizing DKL(˜π||πθ). Also in [Guo et al.  2014]  the authors use UCT
as a policy improvement operator and generate data from ˜π = UCT(π)  then perform regression
or classiﬁcation with the dataset  which can be seen as approximating the policy under normal
distribution or multinomial distribution parametrization.

4 Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)

To learn a policy from data  the most straight forward way is imitation learning (behavior cloning).
Suppose we have state-action pairs (st  at) in the data generated by a behavior policy π  then we can
minimize the KL divergence between π and πθ. To be speciﬁc  we would like to minimize

KL(π||πθ) = −Es∼d(s) a∼π(a|s)(log πθ(a|s) − log π(a|s))
Dd

(1)
under some state distribution d(s). However  this method makes no distinction between “good” and
“bad” actions. The learned πθ simply imitates all the actions generated by π. Actually  if we also have
reward rt in the data  we can know the consequence of taking action at  by looking at future state

st+1 and reward rt. Suppose we have estimation of the advantage of action at as (cid:98)Aπ(st  at)  we can

put higher sample weight on the actions with higher advantage  thus imitating good actions more
often. Inspired by this idea  we propose a monotonic advantage reweighted imitation learning method
(Algorithm 1) which maximizes

Es∼dπ(s) a∼π(a|s) exp(β(cid:98)Aπ(s  a)) log πθ(a|s)

(2)
where β is a hyper-parameter. When β = 0 the algorithm degenerates to ordinary imitation learning.
Ideally we would like to estimate the advantage function A(st  at) = Eπ|st at(Rt − V π(st)) using
l=t γl−trl. For example  one possible solution is to
use a neural network to estimate A(st  at)  by minimizing Eπ|st at(Aθ(st  at) − (Rt − Vθ(st)))2

cumulated discounted future reward Rt =(cid:80)T

3

Algorithm 1 Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)

Input: Historical data D generated by π  hyper-parameter β.

For each trajectory τ in D  estimate advantages (cid:98)Aπ(st  at) for time t = 1 ···   T .
Maximize E(st at)∈D exp(β(cid:98)Aπ(st  at)) log πθ(at|st) with respect to θ.

for Rt computed from different trajectories  where Vθ(st) is also estimated with a neural network
respectively. In practice we ﬁnd that good results can be achieved by simply using a single path

estimation as (cid:98)A(st  at) = (Rt − Vθ(st))/c  where we normalize the advantage by its average norm

c1 in order to make the scale of β stable across different environments. We use this method in our
experiments as it greatly simpliﬁes the computation.
Although the algorithm has a very simple formulation  it has many strengths as

1. Under mild conditions  we show that the proposed algorithm has policy improvement bound
by theoretical analysis. Speciﬁcally  the policy ˜π is uniformly as good as  or better than the
behavior policy π.

2. The method works well with function approximation as a complex neural network  as sug-
gested by theoretical analysis and validated empirically. The method is naturally compatible
with hybrid action of discrete and continuous parts  which is common in practical problems.
3. In contrast to most off-policy methods  the algorithm does not rely on importance sampling
with the value of π(at|st) – the action probability of the behavior policy  thus can be used
to learn from an unknown policy  and is also robust when current policy is deviated from the
behavior policy. We validate this with several empirical experiments.

In Section 5 we give a proposition of policy improvement by theoretical analysis. And in Section 6
we give experimental results of the proposed algorithm in off-policy settings.

5 Theoretical Analysis

In this section  we ﬁrstly show that in the ideal case Algorithm 1 is equivalent to imitating a new
policy ˜π. Then we show that the policy ˜π is indeed uniformly better than π. Thus Algorithm 1 can
also be regarded as imitating a better policy (IBP). For function approximation  we also provide a
policy improvement lower bound under mild conditions.

5.1 Equivalence to Imitating a New Policy

In this subsection  we show that in the ideal case when we know the advantage Aπ(st  at)  Algorithm
1 is equivalent to minimizing KL divergence between πθ and a hypothetic ˜π. Consider the problem
(3)

((1 − γ)βLdπ π(π(cid:48)) − Ddπ

˜π = arg max

KL(π(cid:48)||π))

π(cid:48)∈Π

which has an analytical solution in the policy space Π [Azar et al.  2012  Appendix A  Proposition 1]
(4)

˜π(a|s) = π(a|s) exp(βAπ(s  a) + C(s))

where C(s) is a normalizing factor to ensure that(cid:80)
(cid:88)

KL(˜π||πθ) = arg max
Dd

arg min

d(s)

θ

a

(cid:88)
(cid:88)

s

= arg max

d(s) exp(C(s))

θ

s

a

˜π(a|s) log πθ(a|s)

a∈A ˜π(a|s) = 1 for each state s. Then
(cid:88)

π(a|s) exp(βAπ(s  a)) log πθ(a|s)

(5)

θ

(cid:80)T

Thus Algorithm 1 is equivalent to minimizing Dd

KL(˜π||πθ) for d(s) ∝ dπ(s) exp(−C(s)). 2

1In our experiments  the average norm of advantage is approximated with a moving average estimation  by

c2 ← c2 + 10−8((Rt − Vθ(st))2 − c2).

2In the implementation of the algorithm  we omit the step discount in dπ 

π(s) =
t=0 1(st = s) where T is the terminal step. Sampling from dπ(s) is possible  but usually leads

Ed0 π
to inferior performance according to our preliminary experiments.

i.e.  using d(cid:48)

4

5.2 Monotonic Advantage Reweighting

In subsection 5.1  we have shown that the ˜π deﬁned in 4 is the analytical solution to the problem 3. In
this section  we further show that ˜π is indeed uniformly as good as  or better than π. To be rigorous  a
policy π(cid:48) is considered uniformly as good as  or better than π  if ∀s ∈ S  we have V π(cid:48)
(s) ≥ V π(s).
In Proposition 1  we give a family of ˜π which are uniformly as good as  or better than π. To be
speciﬁc  we have
Proposition 1. Suppose two policies π and ˜π satisfy

(6)
where g(·) is a monotonically increasing function  and h(s ·) is monotonically increasing for any
ﬁxed s. Then we have

g(˜π(a|s)) = g(π(a|s)) + h(s  Aπ(s  a))

V ˜π(s) ≥ V π(s)  ∀s ∈ S.

(7)

that is  ˜π is uniformly as good as or better than π.

The idea behind this proposition is simple. The condition (6) requires that the policy ˜π has positive
advantages for the actions where ˜π(a|s) ≥ π(a|s). Then it follows directly from the well-known
policy improvement theorem as stated in [Sutton and Barto  1998  Equation 4.8]. A short proof is
provided in Appendix A.1 for completeness.
When g(·) and h(s ·) in (6) are chosen as g(π) = log(π) and h(s  Aπ(s  a)) = βAπ(s  a) + C(s) 
then we recover the formula in 4. By Proposition (1) we have shown that ˜π deﬁned in 4 is as good as 
or better than policy π.
We note that there are other choice of g(·) and h(s ·) as well. For example we can choose g(π) =
log(π) and h(s  Aπ(s  a)) = log((βAπ(s  a))+ + ) + C(s)  where (·)+ is a positive truncation   is
a∈A ˜π(s  a) = 1. In this case 
a π(a|s)((βAπ(s  a))+ + ) log πθ(a|s) + C.

a small positive number  and C(s) is a normalizing factor to ensure(cid:80)

s d(s) exp(C(s))(cid:80)

KL(˜π||πθ) =(cid:80)

we can minimize Dd

5.3 Lower bound under Approximation

For practical usage  we usually seek a parametric approximation of ˜π. The following proposition
gives a lower bound of policy improvement for the parametric policy πθ.
Proposition 2. Suppose we use parametric policy πθ to approximate the improved policy ˜π deﬁned
in Formula 3  we have the following lower bound on the policy improvement
2γ˜π
π

1

√
2
1 − γ

1
2

δ

1 M πθ +

√
(1 − γ)2 δ

δ2 −
(8)
(1 − γ)β
KL (˜π||π)  π(cid:48)
π = maxs |Ea∼π(cid:48)Aπ(s  a)|  and

1
2
2

η(πθ) − η(π) ≥ −
KL (πθ||˜π)  Dd˜π

KL (˜π||πθ))  δ2 = Ddπ
where δ1 = min(Dd˜π
M π = maxs a |Aπ(s  a)| ≤ maxs a |r(s  a)|/(1 − γ).
A short proof can be found in Appendix A.2. Note that we would like to approximate ˜π under state
distribution d˜π in theory. However in practice we use a heuristic approximation to sample data from
trajectories generated by the base policy π as in Algorithm 1  which is equivalent to imitating ˜π under
a slightly different state distribution d as discussed in Sec.5.1.

6 Experimental Results

In this section  we provide empirical evidence that the algorithm is well suited for off-policy RL
tasks  as it does not need to know the probability of the behavior policy  thus is robust when learning
from replays from an unknown policy. We evaluate the proposed algorithm with HFO environment
under different settings (Sec. 6.1). Furthermore  we also provide two other environments (TORCS
and mobile MOBA game) to evaluate the algorithm in learning from replay data (Sec. 6.2  6.3).
Denote the behavior policy as π  the desired parametrized policy as πθ  the policy loss Lp for the
policy iteration algorithms considered are listed as following: (C is a θ-independent constant)

• (IL) Imitation learning  minimizing Ddπ

KL(π||πθ).

Lp = Ddπ

KL(π||πθ) = −Es∼dπ(s) a∼π(a|s) log πθ(a|s) + C

(9)

5

Lp = Ddπ

KL(π||πθ) − (1 − γ)βLdπ π(πθ)

(cid:18) πθ(a|s)

π(a|s)

(cid:19)

• (PG) Policy gradient with baseline and Ddπ

KL(π||πθ) regularization.

Lp = −Es∼dπ(s) a∼π(a|s)(βAπ(s  a) + 1) log πθ(a|s) + C

• (PGIS) Policy gradient with baseline and Ddπ

(10)
KL(π||πθ) regularization  with off-policy correc-
tion by importance sampling (IS)  as in TRPO [Schulman et al.  2015] and CPO [Achiam
et al.  2017]. Here we simply use penalized gradient algorithm to optimize the objective 
instead of using delegated optimization method as in [Schulman et al.  2015].

= −Es∼dπ(s) a∼π(a|s)

βAπ(s  a) + log πθ(s  a)

(11)

+ C

• (MARWIL) Minimizing Dd

KL(˜π||πθ) as in (5) and Algorithm 1.

Lp = Dd

KL(˜π||πθ) = −Es∼dπ(s) a∼π(a|s) log(πθ(a|s)) exp(βAπ(s  a)) + C

(12)

Note that IL simply imitates all the actions in the data  while PG needs the on-policy assumption to be
a reasonable algorithm. Both PGIS and MARWIL are derived under off-policy setting. However  the
importance ratio πθ/π used to correct off-policy bias for PG usually has large variance and may cause
severe problems when πθ is deviated far away from π [Sutton and Barto  1998]. Several methods are
proposed to alleviate this problem [Schulman et al.  2017  Munos et al.  2016  Precup et al.  2000].
On the other hand  we note that the algorithm MARWIL is naturally off-policy  instead of relying on
the importance sampling ratio πθ/π to do off-policy correction. We expect the proposed algorithm to
work better when learning from a possibly unknown behavior policy.

6.1 Experiments with Half Field Offense (HFO)

To compare the aforementioned algorithms  we employ Half Field Offense (HFO) as our primary
experiment environment. HFO is an abstraction of the full RoboCup 2D game  where an agent plays
soccer in a half ﬁeld. The HFO environment has continuous state space and hybrid (discrete and
continuous) action space  which is similar to our task in a MOBA game (Sec. 6.3). In this simpliﬁed
environment  we validate the effectiveness and efﬁciency of the proposed learning method.

6.1.1 Environment Settings

Like in [Hausknecht and Stone  2016]  we let the agent try to goal without a goalkeeper. We follow
[Hausknecht and Stone  2016] for the settings  as is briefed below.
The observation is a 59-d feature vector  encoding the relative position of several critical objects such
as the ball  the goal and other landmarks (See [Hausknecht  2017]). In our experiments  we use a
hybrid action space of discrete actions and continuous actions. 3 types of actions are considered in
our setting  which correspond to {“Dash”  “Turn”  “Kick”}. For each type k of action  we require
the policy to output a parameter xk ∈ R2. For the action “Dash” and “Kick”  the parameter xk is
interpreted as (r cos α  r sin α)  with r truncated to 1 when exceeding. Then α ∈ [0  2π] is interpreted
as the relative direction of that action  while r ∈ [0  1] is interpreted as the power/force of that action.
For the action “Turn”  the parameter xk is ﬁrstly normalized to (cos α  sin α) and then θ is interpreted
as the relative degree of turning. The reward is hand-crafted  written as:

rt = dt(b  a) − dt+1(b  a) + Ikick

t+1 + 3(dt(b  g) − dt+1(b  g)) + 5Igoal
t+1  

= 1 if the agent is close enough to kick the ball. Igoal

where dt(b  a) (or dt(b  g)) is the distance between the ball and the agent (or the center of goal).
Ikick
t = 1 if a successful goal happens. We
t
leverage Winning Rate = NG
to evaluate the ﬁnal performance  where NG is the number of goals
(G) achieved  NF is the number of failures (F)  due to either out-of-time (the agent does not kick the
ball in 100 frames or does not goal in 500 frames) or out-of-bound (the ball is out of the half ﬁeld).
When learning from data  the historical experience is generated with a mixture of a perfect (100%
winning rate) policy πperfect and a random policy πrandom. For the continuous part of the action  a
Gaussian distribution of σ = 0.2 or 0.4 is added to the model output  respectively. The mixture

NG+NF

6

Algorithm 2 Stochastic Gradient Algorithm for MARWIL

Input: Policy loss Lp being one of 9 to 12. base policy π  parameter m  cv.
Randomly initialize πθ. Empty replay memory D.
Fill D with trajectories from π and calculate Rt for each (st  at) in D.
for i = 1 to N do

Sample a batch B = {(sk  ak  Rk)}m from D.

Compute mini-batch gradient ∇θ(cid:98)Lp  ∇θ(cid:98)Lv of B.
Update θ: −∆θ ∝ ∇θ(cid:98)Lp + cv∇θ(cid:98)Lv

end for

Table 1: Performance of PG and MARWIL in TORCS  where β = 0 is the case of IL. For consistent
performance  β should be inversely proportional to the scale of (normalized) Aπ. Different β are
tested in the experiments. The performance is evaluated on the sum of rewards per episode.

β
PG

MARWIL

0.0
2710
(2710)

0.25
6396
5583

0.5
6735
6832

0.75
6758
7670

1.0
7152
9492

coefﬁcient  is used to adjust the proportion of “good” actions and “bad” actions. To be speciﬁc  for
each step  the action is taken as
at ∼

(cid:26) πperfect(·|st) + N (0  σ) w.p. 

(13)

πrandom(·|st) + N (0  σ) w.p. 1 − 

The parameter  is adjusted from 0.1 to 0.5. Smaller  means greater noise  in which case it is harder
for the algorithms to ﬁnd a good policy from the noisy data.

6.1.2 Algorithm Setting

For the HFO game  we model the 3 discrete actions with multinomial probabilities and the 2
continuous parameters for each action with normal distributions of known σ = 0.2 but unknown µ.
Parameters for different types of action are modeled separately. In total we have 3 output nodes for
discrete action probabilities and 6 output nodes for continuous action parameters  in the form of

πθ((k  xk)|s) = pθ(k|s)N (xk|µθ k  σ) 

k ∈ {1  2  3}  xk ∈ R2

where pθ(·|s) is computed as a soft-max for discrete actions and N (·|µθ  σ) is the probability density
function of Gaussian distribution.
When learning from data  the base policy (13) is used to generate trajectories into a replay memory
D  and the policy network is updated by different algorithms  respectively. We denote the policy loss
objective as Lp  being one of the formula (9) (10) (11) (12). Then we optimize the policy loss Lp
and the value loss Lv simultaneously  with a mixture coefﬁcient cv as a hyper-parameter (by default
cv = 1). The value loss Lv is deﬁned as Lv = Ed π(Rt − Vθ(st))2. A stochastic gradient algorithm
is given in Algorithm 2. Each experiment is repeated 3 times and the average of scores is reported in
Figure 1. Additional details of the algorithm settings are given in Appendix B.2.
We note that the explicit value π(at|st) is crucial for the correction used by most off-policy policy
iteration methods [Sutton and Barto  1998]  including [Munos et al.  2016  Wang et al.  2016 
Schulman et al.  2017  Wu et al.  2017] and many other works [Geist and Scherrer  2014]. Here for
a comparable experiment between policy gradient method and our proposed method  we consider
a simple off-policy correction by importance sampling as in (11). We test the performance of the
proposed method and previous works under different settings in Figure 1. We can see that the
proposed MARWIL achieves consistently better performance than other methods.3

6.2 Experiments with TORCS

We also evaluate the imitation learning and the proposed method within the TORCS [Wymann et al. 
2014] environment. In the TORCS environment  the observation is the raw screen with image size of

3We note that the gap between behavior policy and IL is partly due to the approximation we used. As we
have continuous action space  we use a gaussian model with ﬁxed σ  thus the variance of learned policy may be
lower than that of the behavior policy. A fair comparison should be made among IL  PG  PGIS  and MARWIL.

7

Figure 1: Left: Learning from data with additional noise σ = 0.2. Right: Learning from data with
additional noise σ = 0.4. The data is generated with a mixture of a perfect (100% winning rate)
policy πperfect and a random policy πrandom. For the continuous part of the action  a Gaussian noise
of σ = 0.2 (left) or 0.4 (right) is added to the model output  respectively. The mixture coefﬁcient 
is used to adjust the proportion of “good” actions and “bad” actions. Smaller  means less “good”
actions and harder problem. The performance of the behavior policy is plotted in black. We see that
the performance of IL is stable  while PG and PGIS may be affected by the increasing noise in the
data. In all settings we see that the proposed algorithm MARWIL performs best in this task.
64 × 64 × 3  the action is a scalar indicating the steering angle in [−π  π]  and the reward rt is the
momentary speed. When the car crushes  a −1 reward is received and the game terminates.
For the TORCS environment  a simple rule is leveraged to keep the car running and to prevent it
from crushing. Therefore  we can use the rule as the optimal policy to generate expert trajectories. In
addition  we generate noisy trajectories with random actions to intentionally confuse the learning
algorithms  and see whether the proposed method can learn a better policy from the data generated by
the deteriorated policy. We make the training data by generating 10 matches with the optimal policy
and another 10 matches with the random actions.
We train the imitation learning and the proposed method for 5 epochs to compare their performance.
Table 1 shows the test scores when varying the parameter β. From the results  we see that our
proposed algorithm is effective at learning a better policy from these noisy trajectories.

6.3 Experiments with King of Glory

We also evaluate the proposed algorithm with King of Glory – a mobile MOBA (Multi-player Online
Battle Arena) game popular in China. In the experiments  we collect human replay ﬁles in the size
of millions  equaling to tens of billions time steps in total. Evaluation is performed in the “solo”
game mode  where an agent ﬁghts against another AI in the opposite side. A DNN based function
approximator is adopted. In a proprietary test  we ﬁnd that our AI agent  trained with the proposed
method  can reach the level of an experienced human player in a solo game. Additional details of the
algorithm settings for King of Glory is given in Appendix B.3.

7 Conclusion

In this article  we present an off-policy learning algorithm that can form a better policy from
trajectories generated by a possibly unknown policy. When learning from replay data  the proposed
algorithm does not require the bahavior probability π over the actions  which is usually missing in
human generated data  and also works well with function approximation and hybrid action space.
The algorithm is preferable in real-world application  including playing video games. Experimental
results over several real world datasets validate the effectiveness of the proposed algorithm. We note
that the proposed MARWIL algorithm can also work as a full reinforcement learning method  when
applied iteratively on self-generated replay data. Due to the space limitation  a thorough study of our
method for full reinforcement learning is left to a future work.

8

0.10.20.30.40.50.00.20.40.60.81.0epsilonwinning ratebehaviorILPGPGISMARWIL0.10.20.30.40.50.00.20.40.60.81.0epsilonwinning ratebehaviorILPGPGISMARWILAcknowledgement We are grateful for the anonymous reviewers for their detailed and helpful
comments on this work. We also thank our colleagues in the project of King of Glory AI  particularly
Haobo Fu and Tengfei Shi  for their assistance on the game environment and parsing replay data.

References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of

the twenty-ﬁrst international conference on Machine learning  page 1. ACM  2004.

Abbas Abdolmaleki  Jost Tobias Springenberg  Yuval Tassa  Remi Munos  Nicolas Heess  and Martin Riedmiller.
Maximum a posteriori policy optimisation. In International Conference on Learning Representations  2018.

Joshua Achiam  David Held  Aviv Tamar  and Pieter Abbeel. Constrained policy optimization. In Proceedings

of the 34th International Conference on Machine Learning  pages 22–31  2017.

Mohammad Gheshlaghi Azar  Vicenç Gómez  and Hilbert J Kappen. Dynamic policy programming. Journal of

Machine Learning Research  13(Nov):3207–3245  2012.

Michael Bain and Claude Sommut. A framework for behavioural cloning. Machine intelligence  15(15):103 

1999.

Djork-Arné Clevert  Thomas Unterthiner  and Sepp Hochreiter. Fast and accurate deep network learning by

exponential linear units (elus). arXiv preprint arXiv:1511.07289  2015.

Rémi Coulom. Bayesian elo rating. https://www.remi-coulom.fr/Bayesian-Elo/  2005.

accessed 9-Feb-2018].

[Online;

Imre Csiszar and János Körner. Information theory: coding theorems for discrete memoryless systems. Cambridge

University Press  2011.

Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. Journal of

Machine Learning Research  16(1):1437–1480  2015.

Matthieu Geist and Bruno Scherrer. Off-policy learning with eligibility traces: A survey. The Journal of Machine

Learning Research  15(1):289–333  2014.

Mohammad Ghavamzadeh  Marek Petrik  and Yinlam Chow. Safe policy improvement by minimizing robust

baseline regret. In Advances in Neural Information Processing Systems  pages 2298–2306  2016.

Xiaoxiao Guo  Satinder Singh  Honglak Lee  Richard L Lewis  and Xiaoshi Wang. Deep learning for real-time
atari game play using ofﬂine monte-carlo tree search planning. In Advances in neural information processing
systems  pages 3338–3346  2014.

Matthew Hausknecht. Robocup 2d half ﬁeld offense technical manual. https://github.com/LARG/HFO/

blob/master/doc/manual.pdf  2017. [Online; accessed 9-Feb-2018].

Matthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. In Proceedings

of the International Conference on Learning Representations (ICLR)  2016.

Todd Hester  Matej Vecerik  Olivier Pietquin  Marc Lanctot  Tom Schaul  Bilal Piot  Dan Horgan  John Quan 
Andrew Sendonaris  Ian Osband  Gabriel Dulac-Arnold  John Agapiou  Joel Leibo  and Audrunas Gruslys.
Deep q-learning from demonstrations. In AAAI Conference on Artiﬁcial Intelligence  2018.

Daniel Jiang  Emmanuel Ekwedike  and Han Liu. Feedback-based tree search for reinforcement learning. In
Proceedings of the 35th International Conference on Machine Learning  volume 80  pages 2284–2293  2018.

Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Proceedings

of The 33rd International Conference on Machine Learning  volume 48  pages 652–661  2016.

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International

Conference on Machine Learning  pages 267–274  2002.

Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa  David Silver 
and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the International
Conference on Learning Representations (ICLR)  2016.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan Wierstra  and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602  2013.

9

Rémi Munos  Tom Stepleton  Anna Harutyunyan  and Marc Bellemare. Safe and efﬁcient off-policy reinforce-

ment learning. In Advances in Neural Information Processing Systems  pages 1054–1062  2016.

Jan Peters  Katharina Mülling  and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth AAAI

Conference on Artiﬁcial Intelligence  pages 1607–1612  2010.

Matteo Pirotta  Marcello Restelli  Alessio Pecorino  and Daniele Calandriello. Safe policy iteration.

International Conference on Machine Learning  pages 307–315  2013.

In

Doina Precup  Richard S Sutton  and Satinder P Singh. Eligibility traces for off-policy policy evaluation. In

ICML  pages 759–766. Citeseer  2000.

Stéphane Ross  Geoffrey Gordon  and Drew Bagnell. A reduction of imitation learning and structured prediction
to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence
and statistics  pages 627–635  2011.

John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region policy
optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15)  pages
1889–1897  2015.

John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347  2017.

David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driessche  Julian
Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al. Mastering the game of go with
deep neural networks and tree search. Nature  529(7587):484–489  2016.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press Cambridge  1998.

Philip Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforcement learning. In

International Conference on Machine Learning  pages 2139–2148  2016.

Philip Thomas  Georgios Theocharous  and Mohammad Ghavamzadeh. High conﬁdence policy improvement.

In International Conference on Machine Learning  pages 2380–2388  2015.

Ziyu Wang  Victor Bapst  Nicolas Heess  Volodymyr Mnih  Remi Munos  Koray Kavukcuoglu  and Nando

de Freitas. Sample efﬁcient actor-critic with experience replay. arXiv preprint arXiv:1611.01224  2016.

Yuhuai Wu  Elman Mansimov  Roger B Grosse  Shun Liao  and Jimmy Ba. Scalable trust-region method for
deep reinforcement learning using kronecker-factored approximation. In Advances in neural information
processing systems  pages 5285–5294  2017.

Bernhard Wymann  Eric Espié  Christophe Guionneau  Christos Dimitrakakis  Rémi Coulom  and Andrew

Sumner". TORCS  The Open Racing Car Simulator. http://www.torcs.org  2014.

Jiechao Xiong  Qing Wang  Zhuoran Yang  Peng Sun  Lei Han  Yang Zheng  Haobo Fu  Tong Zhang  Ji Liu  and
Han Liu. Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid
action space. arXiv preprint arXiv:1810.06394  2018.

10

,Qing Wang
Jiechao Xiong
peng sun
Han Liu
Tong Zhang