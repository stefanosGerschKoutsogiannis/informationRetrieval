2019,Massively scalable Sinkhorn distances via the Nyström method,The Sinkhorn "distance " a variant of the Wasserstein distance with entropic regularization  is an increasingly popular tool in machine learning and statistical inference. However  the time and memory requirements of standard algorithms for computing this distance grow quadratically with the size of the data  rendering them prohibitively expensive on massive data sets. In this work  we show that this challenge is surprisingly easy to circumvent: combining two simple techniques—the Nyström method and Sinkhorn scaling—provably yields an accurate approximation of the Sinkhorn distance with significantly lower time and memory requirements than other approaches. We prove our results via new  explicit analyses of the Nyström method and of the stability properties of Sinkhorn scaling. We validate our claims experimentally by showing that our approach easily computes Sinkhorn distances on data sets hundreds of times larger than can be handled by other techniques.,Massively Scalable Sinkhorn Distances

via the Nyström Method

Jason Altschuler

MIT

jasonalt@mit.edu

Francis Bach

INRIA - ENS - PSL

francis.bach@inria.fr

Alessandro Rudi
INRIA - ENS - PSL

alessandro.rudi@inria.fr

Jonathan Niles-Weed

NYU

jnw@cims.nyu.edu

Abstract

The Sinkhorn “distance ” a variant of the Wasserstein distance with entropic regular-
ization  is an increasingly popular tool in machine learning and statistical inference.
However  the time and memory requirements of standard algorithms for computing
this distance grow quadratically with the size of the data  making them prohibitively
expensive on massive data sets. In this work  we show that this challenge is sur-
prisingly easy to circumvent: combining two simple techniques—the Nyström
method and Sinkhorn scaling—provably yields an accurate approximation of the
Sinkhorn distance with signiﬁcantly lower time and memory requirements than
other approaches. We prove our results via new  explicit analyses of the Nyström
method and of the stability properties of Sinkhorn scaling. We validate our claims
experimentally by showing that our approach easily computes Sinkhorn distances
on data sets hundreds of times larger than can be handled by other techniques.

1

Introduction

Optimal transport is a fundamental notion in probability theory and geometry [42]  which has
recently attracted a great deal of interest in the machine learning community as a tool for image
recognition [26  35]  domain adaptation [11  12]  and generative modeling [5  9  20]  among many
other applications [see  e.g.  25  31].
The growth of this ﬁeld has been fueled in part by computational advances  many of them stemming
from an inﬂuential proposal of Cuturi [13] to modify the deﬁnition of optimal transport to include
an entropic penalty. The resulting quantity  which Cuturi [13] called the Sinkhorn “distance”1
after Sinkhorn [38]  is signiﬁcantly faster to compute than its unregularized counterpart. Though
originally attractive purely for computational reasons  the Sinkhorn distance has since become an
object of study in its own right because it appears to possess better statistical properties than the
unregularized distance both in theory and in practice [21  29  31  34  36]. Computing this distance as
quickly as possible has therefore become an area of active study.
We brieﬂy recall the setting. Let p and q be probability distributions supported on at most n points
in Rd. We denote by M(p  q) the set of all couplings between p and q  and for any P ∈ M(p  q) 
we denote by H(P ) its Shannon entropy. (See Section 2.1 for full deﬁnitions.) The Sinkhorn distance

1We use quotations since it is not technically a distance; see [13  Section 3.2] for details. The quotes are

dropped henceforth.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

between p and q is deﬁned as

Wη(p  q) := min

P∈M(p q)

(cid:88)

ij

Pij(cid:107)xi − xj(cid:107)2

2 − η−1H(P )  

(1)

for a parameter η > 0. We stress that we use the squared Euclidean cost in our formulation of the
Sinkhorn distance. This choice of cost—which in the unregularized case corresponds to what is
called the 2-Wasserstein distance [42]—is essential to our results  and we do not consider other costs
here. The squared Euclidean cost is among the most common in applications [9  12  16  21  36].
Many algorithms to compute Wη(p  q) are known. Cuturi [13] showed that a simple iterative proce-
dure known as Sinkhorn’s algorithm had very fast performance in practice  and later experimental
work has shown that greedy and stochastic versions of Sinkhorn’s algorithm perform even better in
certain settings [3  20]. These algorithms are notable for their versatility: they provably succeed for
any bounded  nonnegative cost. On the other hand  these algorithms are based on matrix manipula-
tions involving the n × n cost matrix C  so their running times and memory requirements inevitably
scale with n2. In experiments  Cuturi [13] and Genevay et al. [20] showed that these algorithms could
reliably be run on problems of size n ≈ 104.
Another line of work has focused on obtaining better running times when the cost matrix has special
structure. A preeminent example is due to Solomon et al. [40]  who focus on the Wasserstein distance
on a compact Riemannian manifold  and show that an approximation to the entropic regularized
Wasserstein distance can be obtained by repeated convolution with the heat kernel on the domain.
Solomon et al. [40] also establish that for data supported on a grid in Rd  signiﬁcant speedups are
possible by decomposing the cost matrix into “slices” along each dimension [see 31  Remark 4.17].
While this approach allowed Sinkhorn distances to be computed on signiﬁcantly larger problems
(n ≈ 108)  it does not extend to non-grid settings. Other proposals include using random sampling
of auxiliary points to approximate semi-discrete costs [41] or performing a Taylor expansion of the
kernel matrix in the case of the squared Euclidean cost [4]. These approximations both focus on the
η → ∞ regime  when the regularization term in (1) is very small  and do not apply to the moderately
regularized case η = O(1) typically used in practice. Moreover  the running time of these algorithms
scales exponentially in the ambient dimension  which can be very large in applications.

1.1 Our contributions

We show that a simple algorithm can be used to approximate Wη(p  q) quickly on massive data sets.
Our algorithm uses only known tools  but we give novel theoretical guarantees that allow us to show
that the Nyström method combined with Sinkhorn scaling provably yields a valid approximation
algorithm for the Sinkhorn distance at a fraction of the running time of other approaches.
We establish two theoretical results of independent interest: (i) New Nyström approximation results
showing that instance-adaptive low-rank approximations to Gaussian kernel matrices can be found
for data lying on a low-dimensional manifold (Section 3.) (ii) New stability results about Sinkhorn
projections  establishing that a sufﬁciently good approximation to the cost matrix can be used
(Section 4.)

1.2 Prior work

Computing the Sinkhorn distance efﬁciently is a well studied problem in a number of communities.
The Sinkhorn distance is so named because  as was pointed out by Cuturi [13]  there is an extremely
simple iterative algorithm due to Sinkhorn [38] which converges quickly to a solution to (1). This
algorithm  which we call Sinkhorn scaling  works very well in practice and can be implemented using
only matrix-vector products  which makes it easily parallelizable. Sinkhorn scaling has been analyzed
many times [3  14  17  24  27]  and forms the basis for the ﬁrst algorithms for the unregularized
optimal transport problem that run in time nearly linear in the size of the cost matrix [3  14]. Greedy
and stochastic algorithms related to Sinkhorn scaling with better empirical performance have also
been explored [3  20]. Another inﬂuential technique  due to Solomon et al. [40]  exploits the fact
that  when the distributions are supported on a grid  Sinkhorn scaling performs extremely quickly by
decomposing the cost matrix along lower-dimensional slices.
Other algorithms have sought to solve (1) by bypassing Sinkhorn scaling entirely. Blanchet et al. [8]
proposed to solve (1) directly using second-order methods based on fast Laplacian solvers [2  10].

2

Blanchet et al. [8] and Quanrud [32] have noted a connection to packing linear programs  which can
also be exploited to yield near-linear time algorithms for unregularized transport distances.
Our main algorithm relies on constructing a low-rank approximation of a Gaussian kernel matrix
from a small subset of its columns and rows. Computing such approximations is a problem with an
extensive literature in machine learning  where it has been studied under many different names  e.g. 
Nyström method [44]  sparse greedy approximations [39]  incomplete Cholesky decomposition [15] 
Gram-Schmidt orthonormalization [37] or CUR matrix decompositions [28]. The approximation
properties of these algorithms are now well understood [1  6  22  28]; however  in this work  we
require signiﬁcantly more accurate bounds than are available from existing results as well as adaptive
bounds for low-dimensional data. To establish these guarantees  we follow an approach based on
approximation theory [see  e.g.  7  33  43]  which consists of analyzing interpolation operators for the
reproducing kernel Hilbert space corresponding to the Gaussian kernel.
Finally  this paper adds to recent work proposing the use of low-rank approximation for Sinkhorn
scaling [4  41]. We improve upon those papers in several ways. First  although we also exploit the
idea of a low-rank approximation to the kernel matrix  we do so in a more sophisticated way that
allows for automatic adaptivity to data with low-dimensional structure. These new approximation
results are the key to our adaptive algorithm  and this yields a signiﬁcant improvement in practice.
Second  the analyses of Altschuler et al. [4] and Tenetov et al. [41] only yield an approximation to
Wη(p  q) when η → ∞. In the moderately regularized case when η = O(1)  which is typically used
in practice  neither the work of Altschuler et al. [4] nor of Tenetov et al. [41] yields a rigorous error
guarantee.

2 Main Result

2.1 Preliminaries and notation

∆n := {v ∈ Rn(cid:62)0 :(cid:80)n

Problem setup. Throughout  p and q are two probability distributions supported on a set X :=
{x1  . . .   xn} of points in Rd  with (cid:107)xi(cid:107)2 (cid:54) R for all i ∈ [n] := {1  . . .   n}. We deﬁne the
cost matrix C ∈ Rn×n by Cij = (cid:107)xi − xj(cid:107)2
2. We identify p and q with vectors in the simplex
i=1 vi = 1} whose entries denote the weight each distribution gives to the
points of X. We denote by M(p  q) the set of couplings between p and q  identiﬁed with the set
of P ∈ Rn×n(cid:62)0
satisfying P 1 = p and P (cid:62)1 = q  where 1 denotes the all-ones vector in Rn. The
Shannon entropy of a non-negative matrix P ∈ Rn×n(cid:62)0
  where
we adopt the standard convention that 0 log 1
Our goal is to approximate the Sinkhorn distance (1) to some additive accuracy ε > 0. By strict
convexity  this optimization problem has a unique minimizer  which we denote henceforth by P η.
For shorthand  in the sequel we write

is denoted H(P ) :=(cid:80)

0 = 0.

ij Pij log 1
Pij

VM (P ) := (cid:104)M  P(cid:105) − η−1H(P ) 

for a matrix M ∈ Rn×n. In particular  we have Wη(p  q) = minP∈M(p q) VC(P ). For the purpose
of simplifying some bounds  we assume throughout that n (cid:62) 2  η ∈ [1  n]  R (cid:62) 1  ε (cid:54) 1.

Sinkhorn scaling. Our approach is based on Sinkhorn scaling  an algorithm due to Sinkhorn [38]
and popularized for optimal transport by Cuturi [13]. We recall the following fundamental deﬁnition.
Deﬁnition 1. Given p  q ∈ ∆n and K ∈ Rn×n with positive entries  the Sinkhorn projec-
M(p q)(K) of K onto M(p  q) is the unique matrix in M(p  q) of the form D1KD2 for
tion ΠS
positive diagonal matrices D1 and D2.
Since p and q remain ﬁxed throughout  we abbreviate ΠS
the feasible set M(p  q) explicit.
Proposition 1 (45). Let K have strictly positive entries  and let log K be the matrix deﬁned by
(log K)ij := log(Kij). Then
ΠS
M(p q)(K) = argmin
P∈M(p q)

M(p q) by ΠS except when we want to make

(cid:104)− log K  P(cid:105) − H(P ) .

3

Note that the strict convexity of −H(P ) and the compactness of M(p  q) implies that the minimizer
exists and is unique.

This yields the following simple but key connection between Sinkhorn distances and Sinkhorn scaling.
Corollary 1.

P η = ΠS

M(p q)(K)  

where K is deﬁned by Kij = e−ηCij .
Notation. We deﬁne the probability simplices ∆n := {p ∈ Rn(cid:62)0 : p(cid:62)1 = 1} and ∆n×n := {P ∈
: 1(cid:62)P 1 = 1}. Elements of ∆n×n will be called joint distributions. The Kullback-Leibler
Rn×n(cid:62)0

divergence between two joint distributions P and Q is KL(P(cid:107)Q) :=(cid:80)
(cid:107)A(cid:107)1 its entrywise (cid:96)1 norm (i.e.  (cid:107)A(cid:107)1 :=(cid:80)

Throughout the paper  all matrix exponentials and logarithms will be taken entrywise  i.e.  (eA)ij :=
eAij and (log A)ij := log Aij for A ∈ Rn×n. Given a matrix A  we denote by (cid:107)A(cid:107)op its operator
norm (i.e.  largest singular value)  by (cid:107)A(cid:107)∗ its nuclear norm (i.e.  the sum of its singular values)  by
ij |Aij|)  and by (cid:107)A(cid:107)∞ its entrywise (cid:96)∞ norm (i.e. 
(cid:107)A(cid:107)∞ := maxij |Aij|). We abbreviate “positive semideﬁnite” by “PSD.”
The notation f = O(g) means that f (cid:54) Cg for some universal constant C  and g = Ω(f ) means
f = O(g). The notation ˜O(·) omits polylogarithmic factors depending on R  η  n  and ε.

ij Pij log Pij
Qij

.

2.2 Main result and proposed algorithm

Pseudocode for our proposed algorithm is given in Algorithm 1. NYS-SINK (pronounced “nice sink”)
computes a low-rank Nyström approximation of the kernel matrix via a column sampling procedure.
For reasons of space  full pseudocode and proofs of all claims are deferred to the supplement.
As noted in Section 1  the Nyström method constructs a low-rank approximation to a Gaussian kernel
matrix K = e−ηC based on a small number of its columns. In order to design an efﬁcient algorithm 
we aim to construct such an approximation with the smallest possible rank. The key quantity for
understanding the error of this algorithm is the so-called effective dimension (also sometimes called
the “degrees of freedom”) of the kernel matrix K [18  30  46].
Deﬁnition 2. Let λj(K) denote the jth largest eigenvalue of K (with multiplicity). Then the effective
dimension of K at level τ > 0 is

n(cid:88)

λj(K)

λj(K) + τ n

j=1

deff(τ ) :=

.

(2)

The effective dimension deff(τ ) indicates how large the rank of an approximation ˜K to K must be
in order to obtain the guarantee (cid:107) ˜K − K(cid:107)op (cid:54) τ n. For our application  we have K = eηC  and
we will show that it sufﬁces to obtain an approximate kernel ˜K satisfying (cid:107) ˜K − K(cid:107)op (cid:54) ε(cid:48)
2 e−4ηR2 
where ε(cid:48) = ˜O(εR−2). We are therefore motivated to deﬁne the following quantity  which informally
captures the smallest possible rank of an approximation of this quality.
Deﬁnition 3. Given X = {x1  . . .   xn} ⊆ Rd with (cid:107)xi(cid:107)2 (cid:54) R for all i ∈ [n]  η > 0  and ε(cid:48) ∈ (0  1) 
the approximation rank is

r∗(X  η  ε(cid:48)) := deff

(cid:16) ε(cid:48)
2n e−4ηR2(cid:17)

where deff(·) is the effective rank for the kernel matrix K := e−ηC.
As we show below  we adaptively construct an approximate kernel ˜K whose rank is at most a
logarithmic factor bigger than r∗(X  η  ε(cid:48)) with high probability. We also give concrete bounds on
r∗(X  η  ε(cid:48)) below.
Our proposed algorithm makes use of several subroutines. The ADAPTIVENYSTRÖM procedure
in Algorithm 1 combines an algorithm of Musco & Musco [30] with a doubling trick that enables
automatic adaptivity. It outputs the approximate kernel ˜K and its rank r.

4

The SINKHORN procedure in Algorithm 1 is the Sinkhorn scaling algorithm for projecting ˜K onto
M(p  q). We use a variant of the standard algorithm  which returns both the scaling matrices and
an approximation of the cost of an optimal solution. The ROUND procedure in Algorithm 1 is
Algorithm 2 of Altschuler et al. [3].
We emphasize that neither D1 ˜KD2 nor ˆP are ever represented explicitly  since this would take Ω(n2)
time. Instead  we maintain these matrices in low-rank factorized forms. This enables Algorithm 1
to be implemented efﬁciently in o(n2) time  since the procedures SINKHORN and ROUND can both
be implemented such that they depend on ˜K only through matrix-vector multiplications with ˜K.
Moreover  we also emphasize that all steps of Algorithm 1 are easily parallelizable since they can be
re-written in terms of matrix-vector multiplications.
We note also that although the present paper focuses speciﬁcally on the squared Euclidean cost
c(xi  xj) = (cid:107)xi−xj(cid:107)2
2 (corresponding to the 2-Wasserstein case of optimal transport pervasively used
in applications; see intro)  our algorithm NYS-SINK readily extends to other cases of optimal transport.
Indeed  since the Nyström method works not only for Gaussian kernel matrices Kij = e−η(cid:107)xi−xj(cid:107)2
2 
but in fact more generally for any PSD kernel matrix  our algorithm can be used on any optimal
transport instance for which the corresponding kernel matrix Kij = e−ηc(xi xj ) is PSD.

Algorithm 1 NYS-SINK
1: Input: X = {x1  . . .   xn} ⊆ Rd  p  q ∈ ∆n  ε  η > 0
2: Output: ˆP ∈ M(p  q)  ˆW ∈ R  r ∈ N
3: ε(cid:48) ← min(1 
4: ( ˜K  r) ← ADAPTIVENYSTRÖM(X  η  ε(cid:48)
5: (D1  D2  ˆW ) ← SINKHORN( ˜K  p  q  ε(cid:48))
6: ˆP ← ROUND(D1 ˜KD2  p  q)

{Approximate Sinkhorn projection and cost}

{Compute low-rank approximation}

2 e−4ηR2

50(4R2η+log

)

εη

)

n
ηε )

{Round to feasible set}

7: Return ˆP   ˆW

(cid:16)

(cid:16)

(cid:17)(cid:17)

Our main result is the following.
Theorem 1. Let ε  δ ∈ (0  1). Algorithm 1 runs in ˜O
time  uses O(n(r + d)) space 
and returns a feasible matrix ˆP ∈ M(p  q) in factored form and scalars ˆW ∈ R and r ∈ N  where
(3a)
(3b)
(3c)

|VC( ˆP ) − Wη(p  q)| (cid:54) ε 
KL( ˆP(cid:107)P η) (cid:54) ηε 
| ˆW − Wη(p  q)| (cid:54) ε 

r + ηR4
ε

nr

and  with probability 1 − δ 

for a universal constant c and where ε(cid:48) = ˜Ω(εR−2).

r (cid:54) c · r∗(X  η  ε(cid:48)) log n
δ  

(3d)

We note that  while our algorithm is randomized  we obtain a deterministic guarantee that ˆP is a good
solution. We also note that runtime dependence on the radius R—which governs the scale of the
problem—is inevitable since we seek an additive guarantee.
We show in Section 3 that r∗—which controls the running time of the algorithm with high probability
by (3d)—adapts to the intrinsic dimension of the data. This adaptivity is crucial in applications 
where data can have much lower dimension than the ambient space. We informally summarize this
behavior in the following theorem.
Theorem 2 (Informal). There exists an universal constant c > 0 such that  for any n points in a ball
of radius R in Rd  r∗(X  η  ε(cid:48)) (cid:54) (c(ηR2 + log n
ε(cid:48)η ))d. Moreover  for any k-dimensional manifold Ω
satisfying certain technical conditions and η > 0  there exists a constant cΩ η such that for any n
points lying on Ω  r∗(X  η  ε(cid:48)) (cid:54) cΩ η(log n

ε(cid:48) )5k/2.

5

The formal versions of these bounds appear in Section 3. The second bound is signiﬁcantly better
than the ﬁrst when k (cid:28) d  and clearly shows the beneﬁts of an adaptive procedure.
Combining Theorems 1 and 2 yields the following time and space complexity for our algorithm.
Corollary 2 (Informal). If X consists of n points lying in a ball of radius R in Rd  then with high
probability Algorithm 1 requires ˜O(nε−1(cηR2 + c log n
ε )d)
space. Moreover  if X lies on a k-dimensional manifold Ω  then with high probability Algorithm 1
requires ˜O(nε−1cΩ η(log n

ε )2d+1) time and ˜O(n(cηR2 + c log n

ε )5k) time and ˜O(ncΩ η(log n

ε )5k/2) space.

3 Kernel Approximation via the Nyström Method
Given points X = {x1  . . .   xn} with (cid:107)xi(cid:107)2 (cid:54) R for all i ∈ [n]  let K ∈ Rn×n denote the matrix
with entries Kij := kη(xi  xj)  where kη(x  x(cid:48)) := e−η(cid:107)x−x(cid:48)(cid:107)2. Note that kη(x  x(cid:48)) is the Gaussian
consider an approximation of the matrix K that is of the form (cid:101)K = V A−1V (cid:62)  where V ∈ Rn×r and
2η . For r ∈ N  we
kernel e−(cid:107)x−x(cid:48)(cid:107)2/(2σ2) between points x and x(cid:48) with bandwith parameter σ2 = 1
A ∈ Rr×r. Note that the matrix (cid:101)K is never computed explicitly. Indeed  our proposed Algorithm 1
only depends on (cid:101)K through computing matrix-vector products (cid:101)Kv  where v ∈ Rn  and these can be
computed efﬁciently as (cid:101)Kv = V (L−(cid:62)(L−1(V (cid:62)v)))  where L ∈ Rr×r is the lower triangular matrix

satisfying LL(cid:62) = A. Once a Cholesky decomposition of A has been obtained—at computational
cost O(r3)—matrix-vector products can therefore be computed in time O(nr). In the supplement 
we give pseudocode for the AdaptiveNyström subroutine  based on a simple doubling trick. It enjoys
the following guarantee:
Lemma 1. Let ˜K denote the (random) kernel output by ADAPTIVENYSTRÖM(X  η  τ )  and let
r := rank( ˜K). Then (cid:107)K − ˜K(cid:107)∞ (cid:54) τ  the algorithm used O(nr) space and terminated in O(nr2)
time  and there exists a universal constant c such that simultaneously for every δ > 0 

P(cid:16)

r (cid:54) c · deff

(cid:0) τ

n

(cid:1) log(cid:0) n

δ

(cid:1)(cid:17) (cid:62) 1 − δ.

3.1 General results: data points lie in a ball
In this section we assume no structure on X apart from the fact that X ⊆ Bd
R is a ball
of radius R in Rd centered around the origin  for some R > 0 and d ∈ N. First we characterize the
eigenvalues of K in terms of η  d  R  and then we use this to bound deff.
Theorem 3. Let X := {x1  . . . xn} ⊆ Bd
R  and let K ∈ Rn×n be the matrix with entries Kij :=
4e2ηR2 . 2. For τ ∈ (0  1] 
e−η(cid:107)xi−xj(cid:107)2. Then: 1. For t (cid:62) (2e)d  λt+1(K) (cid:54) ne

2e t1/d log d t1/d

R where Bd

− d

deff(τ ) (cid:54) 3(cid:0)6 + 41

d ηR2 + 3

d log 1

τ

(cid:1)d.

Corollary 3. Let ε(cid:48) ∈ (0  1) and η > 0. If X consists of n points lying in a ball of radius R around
the origin in Rd  then

(cid:18)

(cid:19)d

.

r∗(X  η  ε(cid:48)) (cid:54) 3

6 +

53
d

ηR2 +

3
d

log

2n
ε(cid:48)

3.2 Adaptivity: data points lie on a low dimensional manifold

In this section  we show that the quality of the Nyström approximation adapts to the intrinsic
dimension of the data. Let Ω ⊂ Rd be a smooth compact manifold without boundary of dimension k 
for k < d  and let (Ψj  Uj)j∈[T ]  with T ∈ N  be an atlas for Ω. We assume the following quantitative
control on the smoothness of the atlas.
Assumption 1. There exists Q > 0 such that

(cid:107)DαΨ−1

j (u)(cid:107) (cid:54) Q|α| 

α ∈ Nk  j ∈ [T ] 

sup
u∈Bk
rj

where |α| =(cid:80)k

j=1 αj and Dα =

∂|α|
1 ...∂u

∂uα1

αk
k

  for α ∈ Nk.

6

Theorem 4. Let Ω ⊂ Bd
R ⊂ Rd be a smooth compact manifold without boundary satisfying
Assumption 1. Let X := {x1  . . . xn} ⊆ Ω  and let K ∈ Rn×n be the matrix with entries Kij :=
e−η(cid:107)xi−xj(cid:107)2. Then: 1. There exists a constant c not depending on X or n  such that for t (cid:62) 0 
5k . 2. There exist c1  c2 not depending on X  n  or τ  such that for τ ∈ (0  1] 
λt+1(K) (cid:54) ne−ct

deff(τ ) (cid:54)(cid:0)c1 log 1

τ

2

(cid:1)5k/2

+ c2.

The result above is new  to our knowledge  and extends interpolation results on manifolds [19  23  43] 
from polynomial to exponential decay  generalizing a technique of Rieger & Zwicknagl [33] to a
subset of real analytic manifolds. The crucial point is that now the eigenvalue decay and the effective
dimension depend on the dimension of the manifold k and not the ambient dimension d (cid:29) k.
Corollary 4. Let ε(cid:48) ∈ (0  1)  η > 0  and let Ω ⊂ Rd be a manifold of dimension k (cid:54) d satisfying
Assumption 1. There exists cΩ η > 0 not depending on X or n such that

(cid:16)

(cid:17)5k/2

.

r∗(X  η  ε(cid:48)) (cid:54) cΩ η

log

n
ε(cid:48)

4 Sinkhorn Scaling an Approximate Kernel Matrix

εη

).

n
ηε )

50((cid:107)C(cid:107)∞η+log

The main result of this section  presented next  gives both a runtime bound and an error bound
on the approximate Sinkhorn scaling performed in Algorithm 1. The error bound shows that the
objective function VC(·) in (1) is stable with respect to both (i) Sinkhorn projecting an approximate
kernel matrix ˜K instead of the true kernel matrix K  and (ii) only performing an approximate
Sinkhorn projection.The results of this section apply to any bounded cost matrix C ∈ Rn×n  with
ε(cid:48) := min(1 
satisﬁes (cid:107) log K − log ˜K(cid:107)∞ (cid:54) ε(cid:48)  then the Sinkhorn
Theorem 5. If K = e−ηC and if ˜K ∈ Rn×n
subroutine in Algorithm 1 outputs D1  D2  and ˆW such that ˜P := D1 ˜KD2 satisﬁes (cid:107) ˜P 1 − p(cid:107)1 +
(cid:107) ˜P (cid:62)1 − q(cid:107)1 (cid:54) ε(cid:48)  |VC(P η) − VC( ˜P )| (cid:54) ε
2   and | ˆW − VC( ˜P )| (cid:54) ε
2 . Moreover  if matrix-
vector products can be computed with ˜K and ˜K(cid:62) in time TMULT  then this takes time ˜O((n +
TMULT)η(cid:107)C(cid:107)∞ε(cid:48)−1).
The running time bound in Theorem 5 for the time required to produce D1 and D2 follows directly
from prior work which has shown that Sinkhorn scaling can produce an approximation to the Sinkhorn
projection of a positive matrix in time nearly independent of the dimension n [3  14]. The error
bounds in Theorem 5 are based on Propositions 2 and 3.
Proposition 2. For any p  q ∈ ∆n and any K  ˜K ∈ Rn×n
+  

>0

(cid:107)ΠS (K) − ΠS ( ˜K)(cid:107)1 (cid:54) (cid:107) log K − log ˜K(cid:107)∞ .

Proposition 3. Given ˜K ∈ Rn×n
>0   let ˜C ∈ Rn×n satisfy ˜Cij := −η−1 log ˜Kij. Let D1 and D2 be
positive diagonal matrices such that ˜P := D1 ˜KD2 ∈ ∆n×n  with δ := (cid:107)p − ˜P 1(cid:107)1 + (cid:107)q − ˜P (cid:62)1(cid:107)1.
If δ (cid:54) 1  then

|V ˜C(ΠS ( ˜K)) − V ˜C( ˜P )| (cid:54) δ(cid:107) ˜C(cid:107)∞ + η−1δ log

2n
δ

.

5 Experimental Results

In this section we empirically validate our theoretical results. Details about the setup for each
experiment appear in the supplement.
We ﬁrst compare to the standard Sinkhorn algorithm. Fig. 1 plots the time-accuracy tradeoff for NYS-
SINK  compared to the standard SINKHORN algorithm. Fig. 1 shows that NYS-SINK is consistently
orders of magnitude faster to obtain the same accuracy.
We then investigate NYS-SINK’s dependence on the intrinsic dimension and ambient dimension of the
input. This is done by running NYS-SINK with a ﬁxed approximation rank on distributions supported
on 1-dimensional curves embedded in higher dimensions. Fig. 2 empirically validates the result in

7

Figure 1: Time-accuracy tradeoff for NYS-SINK and SINKHORN  for a range of regularization
parameters η (each corresponding to a different Sinkhorn distance Wη) and approximation ranks r.
Each experiment has been repeated 50 times; the variance is indicated by the shaded area around the
curves. Note that curves in the plot start at different points corresponding to the time required for
initialization.

Figure 2: Accuracy of NYS-SINK as a func-
tion of running time  for different ambient
dimensions.

Figure 3: Running time vs input size n for
NYS-SINK and SINKHORN. Top uses random
point cloud data as in Fig. 1  bottom uses
embedded curve data as in Fig. 2.

Corollary 4  namely that the required approximation rank – and consequently the computational
complexity of NYS-SINK – is independent of the ambient dimension.
Next  we demonstrate NYS-SINK’s dependence on the size n of the dataset. As Fig. 3 indicates  the
running time of NYS-SINK is empirically well-approximated by a line with slope 1 in the log-log
plane – representing a complexity of Θ(n) – whereas the running time of SINKHORN scales as Θ(n2).

Table 1: Performance of our algorithm on benchmark dataset.

Exp. 1 n = 3 × 105  d = 3  η = 15
Nys-Sink r = 2000  T = 20
Dual-Sink Multiscale + Anneal. r = 0.95
Dual-Sink + Anneal. r = 0.95
Exp. 2 n = 3.8 × 106  d = 3  η = 15
Nys-Sink r = 2000  T = 20
Dual-Sink Multiscale + Anneal. r = 0.95
Dual-Sink + Anneal. r = 0.95

8

W2

0.087 ± 0.008

0.090
0.087

W2

0.11 ± 0.01

0.11
0.10

time (s)
0.4 ± 0.1

3.4
35.4

time (s)
6.3 ± 0.8
103.6
1168

Moreover  SINKHORN saturates the RAM already for n ≈ 104  whereas NYS-SINK can scale to
n ≈ 106 on the same machine.
Finally  we evaluate the performance of our algorithm on a benchmark dataset used in computer
graphics. We measured Wasserstein distance between two 3D cloud points from the Stanford 3D
Scanning Repository.2 We ran two experiments  with n = 3 × 105 and n = 3.8 × 106 points 
respectively.
We ran T = 20 iterations of our algorithm (Nys-Sink) with approximation rank r = 2000 on a GPU
and compared to two optimized implementations in the library GeomLoss.3 The results appear in
Table 1. Each Nys-Sink experiment was repeated 50 times. Our method for moderate regularization
η is comparable with the other approaches in terms of precision  with a computational time that is
orders of magnitude smaller. We note here that we choose the parameters r  T in Nys-Sink by hand
to balance precision and time complexity.
We note that in these experiments  instead of using our doubling-trick algorithm to choose the rank
adaptively  we simply run experiments with a small ﬁxed choice of r. As our experiments demonstrate 
NYS-SINK achieves good empirical performance even when the rank r is smaller than our theoretical
analysis requires. Investigating this empirical success further is an interesting topic for future study.

6 Acknowledgments

We thank the reviewers for their helpful comments. We also thank Piotr Indyk  Pablo Parrilo  and
Philippe Rigollet for helpful discussions. JA was supported in part by NSF Graduate Research
Fellowship 1122374. FB and AR were supported in part by the European Research Council (grant
SEQUOIA 724063). JNW was supported in part by the Josephine de Kármán Fellowship.

References
[1] Alaoui  A. and Mahoney  M. W. Fast randomized kernel ridge regression with statistical

guarantees. In Adv. NIPS  pp. 775–783  2015.

[2] Allen-Zhu  Z.  Li  Y.  Oliveira  R.  and Wigderson  A. Much faster algorithms for matrix scaling.

In FOCS  pp. 890–901. 2017.

[3] Altschuler  J.  Weed  J.  and Rigollet  P. Near-linear time approximation algorithms for optimal

transport via Sinkhorn iteration. In Adv. NIPS  pp. 1961–1971  2017.

[4] Altschuler  J.  Bach  F.  Rudi  A.  and Weed  J. Approximating the quadratic transportation

metric in near-linear time. arXiv preprint arXiv:1810.10046  2018.

[5] Arjovsky  M.  Chintala  S.  and Bottou  L. Wasserstein GAN. arXiv preprint arXiv:1701.07875 

2017.

[6] Bach  F. Sharp analysis of low-rank kernel matrix approximations. In Conference on Learning

Theory  pp. 185–209  2013.

[7] Belkin  M. Approximation beats concentration? An approximation view on inference with

smooth radial kernels. arXiv preprint arXiv:1801.03437  2018.

[8] Blanchet  J.  Jambulapati  A.  Kent  C.  and Sidford  A. Towards optimal running times for

optimal transport. arXiv preprint arXiv:1810.07717  2018.

[9] Bousquet  O.  Gelly  S.  Tolstikhin  I.  Simon-Gabriel  C.-J.  and Schoelkopf  B. From optimal
transport to generative modeling: the VEGAN cookbook. arXiv preprint arXiv:1705.07642 
2017.

[10] Cohen  M. B.  Madry  A.  Tsipras  D.  and Vladu  A. Matrix scaling and balancing via box

constrained Newton’s method and interior point methods. In FOCS  pp. 902–913. 2017.

2http://graphics.stanford.edu/data/3Dscanrep/
3http://www.kernel-operations.io/geomloss/

9

[11] Courty  N.  Flamary  R.  and Tuia  D. Domain adaptation with regularized optimal transport. In

ECML PKDD  pp. 274–289  2014.

[12] Courty  N.  Flamary  R.  Tuia  D.  and Rakotomamonjy  A. Optimal transport for domain

adaptation. IEEE Trans. Pattern Anal. Mach. Intell.  39(9):1853–1865  2017.

[13] Cuturi  M. Sinkhorn distances: Lightspeed computation of optimal transport. In Adv. NIPS  pp.

2292–2300  2013.

[14] Dvurechensky  P.  Gasnikov  A.  and Kroshnin  A. Computational optimal transport: Com-
plexity by accelerated gradient descent is better than by Sinkhorn’s algorithm. arXiv preprint
arXiv:1802.04367  2018.

[15] Fine  S. and Scheinberg  K. Efﬁcient SVM training using low-rank kernel representations.

Journal of Machine Learning Research  2:243–264  2001.

[16] Forrow  A.  Hütter  J.-C.  Nitzan  M.  Rigollet  P.  Schiebinger  G.  and Weed  J. Statistical

optimal transport via factored couplings. arXiv preprint arXiv:1806.07348  2018.

[17] Franklin  J. and Lorenz  J. On the scaling of multidimensional matrices. Linear Algebra Appl. 

114/115:717–735  1989.

[18] Friedman  J.  Hastie  T.  and Tibshirani  R. The elements of statistical learning  volume 1.

Springer series in statistics New York  NY  USA:  2001.

[19] Fuselier  E. and Wright  G. B. Scattered data interpolation on embedded submanifolds with
restricted positive deﬁnite kernels: Sobolev error estimates. SIAM Journal on Numerical
Analysis  50(3):1753–1776  2012.

[20] Genevay  A.  Cuturi  M.  Peyré  G.  and Bach  F. Stochastic optimization for large-scale optimal

transport. In Adv. NIPS  pp. 3440–3448. 2016.

[21] Genevay  A.  Peyré  G.  and Cuturi  M. Learning generative models with Sinkhorn divergences.

In AISTATS  pp. 1608–1617  2018.

[22] Gittens  A. The spectral norm error of the naive Nyström extension. Arxiv preprint

arXiv:1110.5305  2011.

[23] Hangelbroek  T.  Narcowich  F. J.  and Ward  J. D. Kernel approximation on manifolds I:
bounding the Lebesgue constant. SIAM Journal on Mathematical Analysis  42(4):1732–1760 
2010.

[24] Kalantari  B.  Lari  I.  Ricca  F.  and Simeone  B. On the complexity of general matrix scaling
and entropy minimization via the RAS algorithm. Math. Program.  112(2  Ser. A):371–401 
2008.

[25] Kolouri  S.  Park  S. R.  Thorpe  M.  Slepcev  D.  and Rohde  G. K. Optimal mass transport:
Signal processing and machine-learning applications. IEEE Signal Process. Mag.  34(4):43–59 
2017.

[26] Li  P.  Wang  Q.  and Zhang  L. A novel earth mover’s distance methodology for image matching

with Gaussian mixture models. In ICCV  pp. 1689–1696  2013.

[27] Linial  N.  Samorodnitsky  A.  and Wigderson  A. A deterministic strongly polynomial algorithm

for matrix scaling and approximate permanents. In STOC  pp. 644–652. ACM  1998.

[28] Mahoney  M. W. and Drineas  P. CUR matrix decompositions for improved data analysis. PNAS 

106(3):697–702  2009.

[29] Montavon  G.  Müller  K.  and Cuturi  M. Wasserstein training of restricted Boltzmann machines.

In Adv. NIPS  pp. 3711–3719  2016.

[30] Musco  C. and Musco  C. Recursive sampling for the Nystrom method. In Adv. NIPS  pp.

3833–3845  2017.

10

[31] Peyré  G. and Cuturi  M. Computational optimal transport. Technical report  2017.

[32] Quanrud  K. Approximating optimal transport with linear programs. In SOSA  2019. To appear.

[33] Rieger  C. and Zwicknagl  B. Sampling inequalities for inﬁnitely smooth functions  with
applications to interpolation and machine learning. Advances in Computational Mathematics 
32(1):103  2010.

[34] Rigollet  P. and Weed  J. Entropic optimal transport is maximum-likelihood deconvolution.

Comptes Rendus Mathématique  2018. To appear.

[35] Rubner  Y.  Tomasi  C.  and Guibas  L. J. The earth mover’s distance as a metric for image

retrieval. International journal of computer vision  40(2):99–121  2000.

[36] Schiebinger  G.  Shu  J.  Tabaka  M.  Cleary  B.  Subramanian  V.  Solomon  A.  Liu  S.  Lin 
S.  Berube  P.  Lee  L.  et al. Reconstruction of developmental landscapes by optimal-transport
analysis of single-cell gene expression sheds light on cellular reprogramming. Cell  2019. To
appear.

[37] Shawe-Taylor  J. and Cristianini  N. Kernel Methods for Pattern Analysis. Camb. U. P.  2004.

[38] Sinkhorn  R. Diagonal equivalence to matrices with prescribed row and column sums. The

American Mathematical Monthly  74(4):402–405  1967.

[39] Smola  A. J. and Schölkopf  B. Sparse greedy matrix approximation for machine learning. In

Proc. ICML  2000.

[40] Solomon  J.  de Goes  F.  Peyré  G.  Cuturi  M.  Butscher  A.  Nguyen  A.  Du  T.  and Guibas 
L. Convolutional Wasserstein distances: Efﬁcient optimal transportation on geometric domains.
ACM Trans. Graph.  34(4):66:1–66:11  July 2015.

[41] Tenetov  E.  Wolansky  G.  and Kimmel  R. Fast Entropic Regularized Optimal Transport Using

Semidiscrete Cost Approximation. SIAM J. Sci. Comput.  40(5):A3400–A3422  2018.

[42] Villani  C. Optimal transport: old and new  volume 338. Springer Science & Business Media 

2008.

[43] Wendland  H. Scattered data approximation  volume 17. Cambridge university press  2004.

[44] Williams  C. and Seeger  M. Using the Nyström method to speed up kernel machines. In Adv.

NIPS  2001.

[45] Wilson  A. G. The use of entropy maximising models  in the theory of trip distribution  mode

split and route split. Journal of Transport Economics and Policy  pp. 108–126  1969.

[46] Zhang  T. Learning bounds for kernel regression using effective data dimensionality. Neural

Computation  17(9):2077–2098  2005.

11

,Linxi Liu
Dangna Li
Wing Hung Wong
Dalin Guo
Angela Yu
Jason Altschuler
Francis Bach
Alessandro Rudi
Jonathan Niles-Weed