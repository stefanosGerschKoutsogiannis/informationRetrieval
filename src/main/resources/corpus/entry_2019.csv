2010,Fast Large-scale Mixture Modeling with Component-specific Data Partitions,Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside  the E-step is linear in both the sample size and the number of mixture components  making it impractical for large-scale data. Based on the variational EM framework  we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear E-step in sample size  while the algorithm still maintains provable convergence. Our approach builds on previous work  but is significantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data.,Fast Large-scale Mixture Modeling with

Component-speciﬁc Data Partitions

Bo Thiesson∗

Microsoft Research

Chong Wang∗†

Princeton University

Abstract

Remarkably easy implementation and guaranteed convergence has made the EM
algorithm one of the most used algorithms for mixture modeling. On the downside 
the E-step is linear in both the sample size and the number of mixture components 
making it impractical for large-scale data. Based on the variational EM framework 
we propose a fast alternative that uses component-speciﬁc data partitions to obtain
a sub-linear E-step in sample size  while the algorithm still maintains provable
convergence. Our approach builds on previous work  but is signiﬁcantly faster and
scales much better in the number of mixture components. We demonstrate this
speedup by experiments on large-scale synthetic and real data.

1

Introduction

Probabilistic mixture modeling [7] has been widely used for density estimation and clustering
applications. The Expectation-Maximization (EM) algorithm [4  11] is one of the most used methods
for this task for clear reasons – elegant formulation of an iterative procedure  ease of implementation 
and guaranteed monotone convergence for the objective. On the other hand  the EM algorithm also
has some acknowledged shortcomings. In particular  the E-step is linear in both the number of
data points and the number of mixture components  and therefore computationally impractical for
large-scale applications. Our work was motivated by a large-scale geo-spatial problem  demanding a
mixture model of a customer base (a huge number of data points) for competing businesses (a large
number mixture components)  as the basis for site evaluation (where to locate a new store).
Several approximation schemes for EM have been proposed to address the scalability problem 
e.g. [2  12  14  10  17  16]   to mention a few. Besides [17  16]  none of these variants has both an
E-step that is truly sub-linear in sample size and also enjoys provable convergence for a well-deﬁned
objective function. More details are discussed in Section 5. Our work is inspired by the “chunky
EM” algorithm in [17  16]  a smart application of the variational EM framework [11]  where a lower
bound on the objective function increases at each iteration and convergence is guaranteed.
An E-step in standard EM calculates expected sufﬁcient statistics under mixture-component member-
ship probabilities calculated for each individual data point given the most recent model estimate. The
variational EM framework alters the E-step to use sufﬁcient statistics calculated under a variational
distribution instead. In chunky EM  the speedup is obtained by using a variational distribution with
shared (variational) membership probabilities for blocks of data (in an exhaustive partition for the
entire data into non-overlapping blocks of data). The chunky EM starts from a coarse partition of the
data and gradually reﬁnes the partition until convergence.
However  chunky EM does not scale well in the number of components  since all components
share the same partition. The individual components are different – in order to obtain membership
probabilities of appropriate quality  one component may need ﬁne-grained blocks in one area of
the data space  while another component is perfectly ﬁne with coarse blocks in that area. Chunky
EM expands the shared partition to match the needed granularity for the most demanding mixture
component in any area of the data space  which might unnecessarily increase the computational

*Equal contributors. †Work done during internship at Microsoft Research.

1

cost. Here  we derive a principled variation  called component-speciﬁc EM (CS-EM) that allows
component-speciﬁc partitions. We demonstrate a signiﬁcant performance improvement over standard
and chunky EM for experiments on synthetic and mentioned customer-business data.

2 Background: Variational and Chunky EM
Variational EM. Given a set of i.i.d. data x (cid:44) {x1 ···   xN}  we are interested in estimating the
parameters θ = {η1:K  π1:K} in the K-component mixture model with log-likelihood function

L(θ) =(cid:80)

n log(cid:80)

k p(xn|ηk)πk.

variational distribution factorizes in accordance with data points  i.e  q =(cid:81)

(1)
For this task  we consider a variational generalization [11] of standard EM [4]  which maximizes
a lower bound of L(θ) through the introduction of a variational distribution q. We assume that the
n qn  where each qn is an
arbitrary discrete distribution over mixture components k = 1  . . .   K. We can lower bound L(θ) by
multiplying each p(xn|ηk)πk in (1) with qn(k)

qn(k) and apply Jensen’s inequality to get

L(θ) ≥(cid:80)

(cid:80)
= L(θ) −(cid:80)
k qn(k)[log p(xn|ηk)πk − log qn(k)]
n KL (qn||p(·|xn  θ)) (cid:44) F(θ  q) 

n

(2)
(3)
where p(·|xn  θ) deﬁnes the posterior distribution of membership probabilities and KL(q||p) is the
Kullback-Leibler (KL) divergence between q and p. The variational EM algorithm alternates the
following two steps  i.e. coordinate ascent on F(θ  q)  until convergence.

If q is not restricted in any form  the E-step produces qt+1 = (cid:81)

E-step: qt+1 = arg maxq F(θt  q)  M-step: θt+1 = arg maxθ F(θ  qt+1).

n p(·|xn  θt)  because the KL-
divergence is the only term in (3) depending on q. The variational EM is in this case equivalent to
the standard EM  and hence produces the maximum likelihood (ML) estimate. In the following  we
consider certain ways of restricting q to attain speedup over standard EM  implying that the minimum
KL-divergence between qn and p(·|xn  θ) is not necessarily zero. Still the variational EM deﬁnes a
convergent algorithm  which instead optimizes a lower bound of the log-likelihood.

algorithms. In chunky EM  the variational distribution q = (cid:81)

Chunky EM. The chunky EM algorithm [17  16] falls into the framework of variational EM
n qn is restricted according to a
partition into exhaustive and mutually exclusive blocks of the data. For a given partition  if data
points xi and xj are in the same block  then qi = qj. The intuition is that data points in the same
block are somewhat similar and can be treated in the same way  which leads to computational savings
in the E-step. If M is the number of blocks in a given partition  the E-step for chunky EM has cost
O(KM ) whereas in standard EM the cost is O(KN ). The speedup can be tremendous for M (cid:28) N.
The speedup is gained by a trade-off between the tightness of the lower bound for the log-likelihood
and the restrictiveness of constraints. Chunky EM starts from a coarse partition and iteratively reﬁnes
it. This reﬁnement process always produces a tighter bound  since restrictions on the variational
distribution are gradually relaxed. The chunky EM algorithm stops when reﬁning any block in a
partition will not signiﬁcantly increase the lower bound.

3 Component-speciﬁc EM

In chunky EM  all mixture components share the same data partition. However  for a particular
block of data  the variation in membership probabilities differs across components  resulting in
varying differences from the equality constrained variational probabilities. Roughly  the variation in
membership probabilities is greatest for components closer to a block of data  and  in particular  for
components far away the membership probabilities are all so small that the variation is insigniﬁcant.
This intuition suggests that we might gain a computational speedup  if we create component-speciﬁc
data partitions  where a component pays more attention to nearby data (ﬁne-grained blocks) than data
far away (coarser blocks). Let Mk be the number of data blocks in the partition for component k. The
k Mk)  compared to O(KM ) in chunky EM. Our conjecture
k Mk signiﬁcantly smaller than
KM  resulting in a much faster E-step. Since our model maintains different partitions for different
mixture components  we call it the component-speciﬁc EM algorithm (CS-EM).

complexity for the E-step is then O((cid:80)
is that we can lower bound the log-likelihood equally well with(cid:80)

2

Figure 1: Trees 1-5 represent 5 mixture compo-
nents with individual tree-consistent partitions
(B1-B5) indicated by the black nodes. The
bottom-right ﬁgure is the corresponding MPT 
where {·} indicates the component marks and
a  b  c  d  e  f  g enumerate all the marked nodes.
This MPT encodes all the component-speciﬁc
information for the 5 mixtures.

Main Algorithm. Figure 2 (on p. 6) shows the main ﬂow of CS-EM. Starting from a coarse partition
for each component (see Section 4.1 for examples)  CS-EM runs variational EM to convergence
and then selectively reﬁne the component-speciﬁc partitions. This process continues until further
reﬁnements will not signiﬁcantly improve the lower bound. Sections 3.1-3.5 provide a detailed
description of basic concepts in support of this brief outline for the main structure of the algorithm.

3.1 Marked Partition Trees

It is convenient to organize the data into a pre-computed partition tree  where a node in the tree
represents the union of the data represented by its children. Individual data points are not actually
stored in each node  but rather  the sufﬁcient statistics necessary for our estimation operations
are pre-computed and stored here. (We discuss these statistics in Section 3.3.) Any hierarchical
decomposition of data that ensures some degree of similarity between data in a block is suitable for
constructing a partition tree. We exemplify our work by using KD-trees [9]. Creating a KD-tree and
storing the sufﬁcient statistics in its nodes has cost O(N log N )  where N is the number of data point.
We will in the following consider tree-consistent partitions  where each data block in a partition
corresponds to exactly one node for a cut (possibly across different levels) in the tree–see Figure 1.
Let us now deﬁne a marked partition tree (MPT)  a simple encoding of all component-speciﬁc
partitions  as follows. Let Bk be the data partition (a set of blocks) in the tree-consistent partition for
mixture component k. In Figure 1  for example  B1 is the partition into data blocks associated with
nodes {e  c  d}. In the shared data partition tree used to generate the component-speciﬁc partitions 
we mark the corresponding nodes for the data blocks in each Bk by the component identiﬁer k. Each
node v in the tree will in this way contain a (possibly empty) set of component marks  denoted by Kv.
The MPT is now the subtree obtained by pruning all unmarked nodes without marked descendants
from the tree. Figure 1 shows an example of a MPT. This example is special in the sense that all
nodes in the MPT are marked. In general  a MPT may have unmarked nodes at any location above
the leaves. For example  in chunky EM  the component-speciﬁc partitions are the same for each
mixture component. In this case  only the leaves in the MPT are marked  with each leaf marked by all
mixture components. The following important property for a MPT holds since all component-speciﬁc
partitions are constructed with respect to the same data partition tree.
Property 1. Let T denote a MPT. The marked nodes on a path from leaf to root in T mark exactly
one data block from each of the K component-speciﬁc data partitions.
In the following  it becomes important to identify the data block in a component-speciﬁc partition 
which embeds the block deﬁned by a leaf. Let L denote the set of leaves in T   and let BL denote
a partition with data blocks Bl ∈ BL according to these leaves. We let Bk(l) denote the speciﬁc
Bk ∈ Bk with the property that Bl ⊆ Bk. Property 1 ensures that Bk(l) exists for all l  k.
Example: In Figure 1  the path a → e → g in turn marks the components Ka = {3  4}  Ke = {1  2} 
and Kg = {5} and we see that each component is marked exactly once on this path  as stated in
Property 1. Accordingly  for the leaf a  (B3(a) = B4(a)) ⊆ (B1(a) = B2(a)) ⊆ B5(a).
3.2 The Variational Distribution

2

Our variational distribution q assigns the same variational membership probability to mixture compo-
nent k for all data points in a component-speciﬁc block Bk ∈ Bk. That is 

qn(k) = qBk for all xn ∈ Bk 

(4)

which we denote as the component-speciﬁc block constraint. Unlike chunky EM  we do not assume
that the data partition Bk is the same across different mixture components. The extra ﬂexibility
complicates the estimation of q in the E-step. This is the central challenge of our algorithm.

3

 !"## !"!"  !"=>?@ABCAABB==>>??@@C(cid:80)
k qBk(l) = 1 for all l ∈ L.

variational distributions qn(·) explicit. That is (cid:80)

To further drive intuition behind the E-step complication  let us make the sum-to-one constraint for the
k qn(k) = 1 for all data points n  which according
to the above block constraint and using Property 1 can be reformulated as the |L| constraints

(5)
Notice that since qBk can be associated with an internal node in T it may be the case that qBk(l)
represent the same qBk across different constraints in (5). In fact 

qBk(l) = qBk for all l ∈ {l ∈ L|Bl ⊆ Bk} 

(6)
implying that the constraints in (5) are intertwined according to the nested structure given by T . The
closer a data block Bk is to the root of T the more constraints simultaneously involve the same qBk.
Example: Consider the MPT in Figure 1. Here  qB5(a) = qB5(b) = qB5(c) = qB5(d)  and hence the
density for component 5 is the same across all four sum-to-one constraints. Similarly  qB1(a) = qB1(b) 
so the density is the same for component 1 in the two constraints associated with leaves a and b. 2

3.3 Efﬁcient Variational E-step
Accounting for the component-speciﬁc block constraint in (4)  the lower bound  F(θ  q)  in Eq. (2)
can be expressed as a sum of local parts  F(θ  qBk )  as follows

|Bk| qBk (gBk + log πk − log qBk ) =(cid:80)

F(θ  q) =(cid:80)

F(θ  qBk ) 

(cid:80)

(cid:80)

(7)

k

Bk∈Bk

k

where we have deﬁned the block-speciﬁc geometric mean

(8)
We integrate the sum-to-one constraints in (5) into the lower bound in (7) by using the standard
principle of Lagrange duality (see  e.g.  [1]). Accordingly  we construct the Lagrangian

x∈Bk

log p(x|ηk)/|Bk|.

Bk∈Bk

gBk = (cid:104)log p(x|ηk)(cid:105)Bk =(cid:80)
F(θ  q  λ) =(cid:80)
(cid:16)

(cid:80)
(1/|Bk|)(cid:80)

Bk

k

F(θ  qBk ) +(cid:80)

l λl((cid:80)
(cid:17)

λl − 1

where λ (cid:44) {λ1  . . .   λL} are the Lagrange multipliers for the constraints in Eq. (5). Recall the
relationship between qBk and qBk(l) in (6). By setting ∂F(θ  q  λ)/∂qBk = 0  we obtain

k qBk(l) − 1) 

l:Bl⊆Bk

qBk (λ) = exp

(9)
Solving the dual optimization problem λ∗ = arg minλ F(θ  q(λ)  λ) now leads to the primal solution
given by q∗

For chunky EM  the E-step is straightforward  because Bk(l) = Bl and therefore(cid:80)

λl = λl
for all k = 1  . . .   K. Substituting (9) into the sum-to-one constraints in (5) reveals that each λl can
be solved independently  leading to the following closed-form solution for qBk(l)

= qBk (λ∗).1

πk exp (gBk ) .

l:Bl⊆Bk(l)

Bk

where Z =(cid:80)

l = |Bl|(cid:0)1 + log(cid:80)

k πk exp(gBk(l) )(cid:1)   q∗

λ∗
k πk exp(gBk(l)) is a normalizing constant.

= πk exp(gBk(l))/Z 

(10)

Bk(l)

CS-EM does not enjoy a similar simple optimization  because of the intertwined constraints  as
described in Section 3.2. Fortunately  we can still obtain a closed-form solution. Essentially  we use
the nesting structure of the constraints to reduce Lagrange multipliers from the solution one at a time
until only one is left  in which case the optimization is easily solved. We describe the basic approach
here and defer the technical details (and pseudo-code) to the supplement.
Consider a leaf node l ∈ L and recall that Kl denotes the components with Bk(l) = Bl in their
partitions. The sum-to-one constraint in (5) that is associated with leaf l can therefore be written as

k∈Kl

qBk(l) +(cid:80)

(cid:80)
qBk(l) = exp (λl/|Bl| − 1)(cid:80)

k(cid:54)∈Kl

qBk(l) = 1.

Furthermore  for all k ∈ Kl the qBk(l)  as deﬁned in (9)  is a function of the same λl. Accordingly 
(11)
1Notice that Eq. (9) implies that positivity constraints qn(k) ≥ 0 are automatically satisﬁed during estimation.

πk exp(gBk(l)).

k∈Kl

k∈Kl

ql (cid:44)(cid:80)

4

Now  consider l’s leaf-node sibling  l(cid:48). For example  in Figure 1  node l = a and l(cid:48) = b. The two
leaves share the same path from their parent to the root in T . Hence  using Property 1  it must be
the case that Bk(l) = Bk(l(cid:48)) for k (cid:54)∈ Kl. The two sum-to-one constraints–one for each leaf–therefore
imply that ql = ql(cid:48). Using (11)  it now follows that

λl(cid:48) = |Bl(cid:48)|(λl/|Bl| + log(cid:80)

πk exp(gBk(l)) − log(cid:80)

k∈Kl

k∈Kl(cid:48) πk(cid:48) exp(gBk(l(cid:48) ))) (cid:44) f (λl).

Thus  we can replace λl(cid:48) with f (λl) in all qBk expressions. Further analysis (detailed in the supple-
ment) shows how we more efﬁciently account for this parameter reduction and continue the process 
now considering the parent node a new “leaf” node once all children have been processed. When
reaching the root  every qBk expression on the path from l only involves the single λl  and the optimal
λ∗
l can therefore be found analytically by solving the corresponding sum-to-one constraint in (5).
Following  all optimal q∗
Finally  it is important to notice that gBk is the only data-dependent part in the above E-step solution.
It is therefore key to the computational efﬁciency of the CS-EM algorithm that gBk can be calculated
from pre-computed statistics  which is in fact the case for the large class of exponential family
distributions. These are the statistics that are stored in the nodes of the MPT.
Example: Let p(x|ηk) be an exponential family distribution

l into the reduced qBk expressions.

are found by inserting λ∗

Bk

(12)
where ηk is the natural parameter  h(x) is the reference function  T (x) is the sufﬁcient statistic  and
A(ηk) is the normalizing constant. Then

p(x|ηk) = h(x) exp(ηT

k T (x) − A(ηk)) 

gBk = (cid:104)log h(x)(cid:105)Bk + ηT

k (cid:104)T (x)(cid:105)Bk − A(ηk) 

where (cid:104)log h(x)(cid:105)Bk and (cid:104)T (x)(cid:105)Bk are the statistics that we pre-compute for (8). In particular  if
(cid:1)  
p(x|ηk) = Nd (µk  Σk)  a Gaussian distribution  then
h(x) = 1  T (x) = (x  xxT )  ηk = (µkΣ
and the statistics (cid:104)log h(x)(cid:105)Bk = 0 and (cid:104)T (x)(cid:105)Bk = ((cid:104)x(cid:105)Bk  (cid:104)xxT(cid:105)Bk ) can be pre-computed. 2
3.4 Efﬁcient Variational M-step
In the variational M-step the model parameters θ = {η1:K  π1:K} are updated by maximizing Eq. (7)

(cid:0)d log(2π)+log |Σk|+µT

k /2)  A(ηk) = − 1
−1

w.r.t. θ under the constraint(cid:80)

k Σ−1µk

k  −Σ
−1

2

k πk = 1. Hereby  the update is
|Bk|qBk   ηk = arg maxηk

(cid:80)

Bk∈Bk

|Bk|qBk gBk .

(13)

Thus  the M-step can be efﬁciently computed using the pre-computed sufﬁcient statistics as well.
Example: If p(x|ηk) has the exponential family form in Eq. (12)  ηk is obtained by solving
|Bk|qBk )A(ηk).

T (x))ηk − ((cid:80)

(cid:80)

qBk

Bk∈Bk

Bk∈Bk

x∈Bk

Bk∈Bk

πk ∝(cid:80)
ηk = arg maxηk ((cid:80)

µk = ((cid:80)

In particular  if p(x|ηk) = Nd (µk  Σk)  then

|Bk|qBk(cid:104)x(cid:105)Bk )/ (N πk)   Σk = ((cid:80)

Bk∈Bk

Bk∈Bk

|Bk|qBk(cid:104)xxT(cid:105)Bk − µkµT

k )/ (N πk) . 2

3.5 Efﬁcient Variational R-step
Given the current component-speciﬁc data partitions  as marked in the MPT T   a reﬁning step (R-step)
selectively reﬁnes these partitions. Any reﬁnement enlarges the family of variational distributions 
and therefore always tightens the optimal lower bound for the log-likelihood. We deﬁne a reﬁnement
unit as the reﬁnement of one data block in the current partition for one component in the model. The
efﬁciency of CS-EM is affected by the number of reﬁnement units performed at each R-step. With
too few units we spend too much time on reﬁning  and with too many units some of the reﬁnements
may be far from optimal and therefore unnecessarily slow down the algorithm. We have empirically
found K reﬁnement units at each R-step to be a good choice. This introduces K new free variational
parameters  which is similar to a reﬁnement step in chunky EM. However  chunky EM reﬁnes the
same data block across all components  which is not the case in CS-EM.

5

Figure 2: The CS-EM algorithm.

Figure 3: Variational R-step algorithm.

1: Initialization: build KD-tree  set initial MPT  set
initial θ  run E-step to set q  set t  s = 0  compute
Ft Fs using (7).

values.

1: Initialize priority queue Q favoring high ∆Fv k
2: for each marked node v in T do
3:

4:
5:

end for

repeat

Run variational E-step and M-step.
Set t ← t + 1 and compute Ft using (7).

Insert candidate (v  k) into Q according to
∆Fv k.

Compute q via E-step with constraints as in
(14).
for all k ∈ Kv do

until (Ft − Ft−1)/(Ft − F0) < 10−4.
Run variational R-step.
Set s ← s + 1 and Fs = Ft.

that improve F the most. This demands the evaluation of an E-step for each of the(cid:80)

2: repeat
3:
4:
5:
6:
6:
7:
7: end for
8:
9: until (Fs − Fs−1)/(Fs − F0) < 10−4.
8: Select K top-ranked (v  k) in Q for reﬁnement.
Ideally  an R-step should select the reﬁnement units leading to optimal improvement for F. Good
candidates can be found by performing a single E-step for each candidate and then select the units
k Mk possible
reﬁnement units. Exact evaluation for this many full E-steps is prohibitively expensive  and we
therefore instead approximate these reﬁnement-guiding E-steps by a local computation scheme based
on the intuition that reﬁning a block for a speciﬁc component mostly affects components with similar
local partition structures. The algorithm is described in Figure 3 with details as follows.
Consider moving all component-marks for v ∈ T to its children ch(v)  where each child u ∈ ch(v)
receives a copy. Let ¯T denote the altered MPT  and ¯Kv  ¯Ku denote the set of marks at v  u ∈ ¯T .
Hence  ¯Kv = ∅ and ¯Ku = Ku ∪ Kv. To approximate the new variational distribution ¯q  we ﬁx the
value for each ¯qBk(l)  with k (cid:54)∈ ¯Ku and l ∈ L  to the value obtained for the distribution q before the
reﬁnement. In this case  the sum-to-one constraints for ¯q simpliﬁes as
¯qBk(l) + Rl = 1 for all l ∈ L 

(14)
qBk(l) = 0 for any leaf l
not under u  and that qBk(l) = qBk(u) and ¯qBk(l) = ¯qBk(u) for k ∈ ¯Ku and any leaf l under u. The
constraints in (14) therefore reduces to the following |ch(v)| independent constraints

with Rl = 1 −(cid:80)
Each ¯qBk(u)  k∈ ¯Ku now has a local closed form solution similar to (10)–with Z =(cid:80)

qBk(l) being the ﬁxed values. Notice that(cid:80)
(cid:80)

¯qBk(u)+Ru.
The improvement to F that is achieved by the reﬁnement-guiding E-step for the reﬁnement unit
reﬁning data block v for component k is denoted ∆Fv k  and can be computed as

¯qBk(u) + Ru = 1 for all u ∈ ch(v).

(cid:80)

k∈ ¯Ku

k∈ ¯Ku

k∈ ¯Ku

k∈ ¯Ku

k∈ ¯Ku

∆Fv k =(cid:80)

u∈ch(v) F(θ  ¯qBk(u) ) − F(θ  qBk(v) ).

This improvement is computed for all possible reﬁnement units and the K highest scoring units are
then selected in the R-step. Notice that this selective reﬁnement step will most likely not reﬁne the
same data block for all components and therefore creates component-speciﬁc partitions.
Example: In Figure 1  node e and its children {a  b} are marked Ke = {1  2} and Ka = Kb = {3  4}.
For the two candidate reﬁnement units associated with e  we have ¯Ke = ∅ and ¯Ka = ¯Kb = {1  2  3  4}.
With q5(u) held ﬁxed  we will for each child u ∈ {a  b} optimize ¯qBk(u)  k = 1  2  3  4  and following
(e  1) and (e  2) are inserted into the priority queue of candidates according to their ∆Fv k values. 2
4 Experiments

In this section we provide a systematic evaluation of CS-EM  chunky EM  and standard EM on
synthetic data  as well as a comparison between CS-EM and chunky EM on the business-customer
data  mentioned in Section 1. (Standard EM is too slow to be included in the latter experiment.)
4.1 Experimental setup
For the synthetic experiments  we generated random training and test data sets from Gaussian mixture
models (GMMs) by varying one (in a single case two) of the following default settings: #data points
N = 100  000  #mixture components K = 40  #dimensions d = 2  and c-separation2 c = 2.

2A GMM is c-separated [3]  if for any i (cid:54)= j  f (i  j) (cid:44) ||µi − µj||2/ max(λmax(Σi)  λmax(Σj)) ≥ dc2 

where λmax(Σ) denotes the maximum eigenvalue of Σ. We only require that Median [f (i  j)] ≥ dc2.

6

The (proprietary) business-customer data was obtained through collaboration with PitneyBowes
Inc. and Yellowpages.com LLC. For the experiments on this data  N = 6.5 million and d = 2 
corresponding to the latitude and longitude for potential customers in Washington state. The basic
assumption is that potential customers act as rational consumers and frequent the somewhat closest
business locations to purchase a good or service. The locations for competing stores of a particular
type  in this way  correspond to ﬁxed centers for components in a mixture model. (A less naive model
with the penetration level for a good or service and the relative attractiveness for stores  is the object
of related research  but is not important for the computational feasibility studied here.)
The synthetic experiments are initialized as follows. After constructing KD-tree  the ﬁrst tree-level
containing at least K nodes ((cid:100)log2 K(cid:101)) is used as the initial data partition for both chunky EM and
all components in CS-EM. For all algorithms (including standard EM)  we randomly chose K data
blocks from the initial partition and initialized parameters for the individual mixture components
accordingly. Mixture weights are initialized with a uniform distribution. The experiments on the
business-customer data are initialized in the same way  except that the component centers are ﬁxed
and the initial data blocks that cover these centers are used for initializing the remaining parameters.
For CS-EM we also considered an alternative initialization of data partitions  which better matches the
rationale behind component-speciﬁc partitions. It starts from the CS-EM initialization and recursively 
according to the KD-tree structure  merges two data blocks in a component-speciﬁc partition  if the
merge has little effect on that component.3 We name this variant as CS-EM∗.

4.2 Results

EM∗ are signiﬁcantly faster than chunky EM in all experiments. In general  the(cid:80)
setting  the ratio KM/(cid:80)

For the synthetic experiments  we compared the run-times for the competing algorithms to reach
a parameter estimate of same quality (and therefore similar clustering performance not counting
different local maxima)  deﬁned as follows. We recorded the log-likelihood for the test data at each
iteration of the EM algorithm  and before each S-step in chunky EM and the CS-EM. We ran all
algorithms to convergence at level 10−4  and the test log-likelihood for the algorithm with lowest
value was chosen as baseline.4 We now recorded the run-time for each algorithm to reach this
baseline  and computed the EM-speedup factors for chunky EM  CS-EM  and CS-EM∗  each deﬁned
as the standard EM run-time divided by the run-time for the alternative algorithm. We repeated all
experiments with ﬁve different parameter initializations and report the averaged results.
Figure 4 shows the EM-speedups for the synthetic data. First of all  we see that both CS-EM and CS-
k Mk variational
parameters needed for the CS-EM algorithms is far fewer than the KM parameters needed for
chunky EM in order to reach an estimate of same quality. For example  for the default experimental
k Mk is 2.0 and 2.1 for  respectively  CS-EM and CS-EM∗. We also see
that there is no signiﬁcant difference in speedup between CS-EM and CS-EM∗. This observation can
be explained by the fact that the resulting component-speciﬁc data partitions greatly reﬁne the initial
partitions  and any computational speedup due to the smarter initial partition in CS-EM∗ is therefore
overwhelmed. Hence  a simple initial partition  as in CS-EM  is sufﬁcient.
Finally  similar to results already reported for chunky EM in [17  16]  we see for all of chunky
EM  CS-EM  and CS-EM∗ that the number of data points and the amount of c-separation have a
positive effect on EM-speedup  while the number of dimensions and the number of components
have a negative effect. However  the last plot in Figure 4 reveals an important difference between
chunky EM and CS-EM: with a ﬁxed ratio between number of data points and number of clusters  the
EM-speedup declines a lot for chunky EM  as the number of clusters and data points increases. This
observation is important for the business-customer data  where increasing the area of investigation
(from city to county to state to country) has this characteristic for the data.
In the second experiment on the business-customer data  standard EM is computationally too de-
manding. For example  for the “Nail salon” example in Figure 5  a single EM iteration takes about
5 hours. In contrast  CS-EM runs to convergence in 20 minutes. To compare run-times for chunky

3Let µ and Σ be the mean and variance parameter for an initial component  and µp  µl  and µr denote the
sample mean for data in the considered parent  left and right child. We merge if |M D(µl  µ|Σ)/M D(µp  µ|Σ)−
1| < 0.05 and |M D(µr  µ|Σ)/M D(µp  µ|Σ) − 1| < 0.05  where M D(· ·|Σ) is the Mahalanobis distance.
4For the default experimental setting  for example  the baseline is reached at 96% of the log-likelihood

improvement from initialization to standard EM convergence.

7

Figure 4: EM-speedup factors on synthetic data.

Figure 5: A comparison of run-time and ﬁnal number
of variational parameters for Chunky EM vs. CS-EM
for exemplary business types with different number of
stores (mixture components).
#stores

Business type

Bowling
Dry cleaning
Nail salon
Pizza
Tax ﬁling
Conv. store

129
815
1290
1327
1459
1739

time
ratio
5.0
21.2
35.8
33.0
34.8
29.4

parameter

ratio
2.41
2.81
3.51
3.18
3.41
3.42

EM and CS-EM  we therefore slightly modiﬁed the way we ensure that the two algorithm reach a
parameter estimate of same quality. We use the lowest of the F values (on training data) obtained for
the two algorithms at convergence as the baseline  and record the time for each algorithm to reach
this baseline. Figure 5 shows the speedup (time ratio) and the reduction in number of variational
parameters (parameter ratio) for CS-EM compared to chunky EM  as evaluated on exemplary types
of businesses. Again  CS-EM is signiﬁcantly faster than chunky EM and the speedup is achieved by a
better targeting of variational distribution through the component-speciﬁc partitions.
5 Related and Future Work
Related work. CS-EM combines the best from two major directions in the literature regarding
speedup of EM for mixture modeling. The ﬁrst direction is based on powerful heuristic ideas  but
without provable convergence due to the lack of a well-deﬁned objective function. The work in
[10] is a prominent example  where KD-tree partitions were ﬁrst used for speeding up EM. As also
pointed out in [17  16]  the method will likely–but not provably–converge for ﬁne-grained partitions.
In contrast  CS-EM is provable convergent–even for arbitrary rough partitions  if extreme speedup
is needed. The granularity of partitions in [10] is controlled by a user-speciﬁed threshold on the
minimum and maximum membership probabilities that are reachable within the boundaries of a node
in the KD-tree. In contrast  we have almost no tuning parameters. We instead let the data speak by
itself by having the ﬁnal convergence determine the granularity of partitions. Finally  [10] “prunes” a
component (sets the membership probability to zero) for data far away from the component. It relates
to our component-speciﬁc partitions  but ours is more principled with convergence guarantees.
The second direction of speedup approaches are based on the variational EM framework [11]. In [11] 
a “sparse” EM was presented  which at some iterations  only updates part of the parameters and
hence relates it to the pruning idea in [10]. [14] presents an “incremental” and a “lazy” EM  which
gain speedup by performing E-steps on varying subsets of the data rather than the entire data. All
three methods guarantee convergence. However  they need to periodically perform an E-step over the
entire data  and  in contrast to CS-EM  their E-step is therefore not truly sub-linear in sample size 
making them potentially unsuitable for large-scale applications. The chunky EM in [17  16] is the
approach most similar to our CS-EM. Both are based on the variational EM framework and therefore
guarantees convergence  but CS-EM is faster and scales better in the number of clusters.
In addition  heuristic sub-sampling is common practice when faced with a large amount of data. One
could argue that chunky EM is an intelligent sub-sampling method  where 1) instead of sampled data
points it uses geometric averages for blocks of data in a given data partition  and 2) it automatically
chooses the “sampling size” by a learning curve method  where F is used to measure the utility
of increasing the granularity for the partition. Sub-sampling therefore has same computational
complexity as chunky EM  and our results therefore suggest that we should expect CS-EM to be
much faster than sub-sampling and scale better in the number of mixture components.
Finally  we exempliﬁed our work by using KD-trees as the tree-consistent partition structure for
generating the component-speciﬁc partitions in CS-EM  which limited its effectiveness in high
dimensions. However  any hierarchical partition structure can be used  and the work in [8] therefore
suggest that changing to an anchor tree (a special kind of metric tree [15]) will also render CS-EM
effective in high dimensions  under the assumption of lower intrinsic dimensionality for the data.
Future Work. Future work will include parallelization of the algorithm and extensions to 1) non-
probabilistic clustering methods  e.g.  k-means clustering [6  13  5] and 2) general EM applications
beyond mixture modeling.

8

References

[1] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[2] P. S. Bradley  U. M. Fayyad  and C. A. Reina. Scaling EM (expectation maximization) clustering

to large databases. Technical Report MSR-TR-98-3  Microsoft Research  1998.

[3] S. Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual Symposium

on Foundations of Computer Science  pages 634–644  1999.

[4] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via

the EM algorithm. Journal of the Royal Statistical Society  Series B  39(1):1–38  1977.

[5] G. Hamerly. Making k-means even faster. In SIAM International Conference on Data Mining

(SDM)  2010.

[6] T. Kanungo  D. M. Mount  N. S. Netanyahu  C. D. Piatko  R. Silverman  and A. Y. Wu. An
efﬁcient k-means clustering algorithm: Analysis and implementation. IEEE Transactions on
Pattern Analysis and Machine Intelligence  24(7):881–892  2002.

[7] G. J. McLachlan and D. Peel. Finite Mixture Models. Wiley Interscience  New York  USA 

2000.

[8] A. Moore. The anchors hierarchy: Using the triangle inequality to survive high-dimensional
data. In Proceedings of the Fourteenth Conference on Uncertainty in Artiﬁcial Intelligence 
pages 397–405. AAAI Press  2000.

[9] A. W. Moore. A tutorial on kd-trees. Technical Report 209  University of Cambridge  1991.
[10] A. W. Moore. Very fast EM-based mixture model clustering using multiresolution kd-trees. In
Advances in Neural Information Processing Systems  pages 543–549. Morgan Kaufman  1999.
[11] R. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental  sparse  and

other variants. In Learning in Graphical Models  pages 355–368  1998.

[12] L. E. Ortiz and L. P. Kaelbling. Accelerating EM: An empirical study. In Proceedings of the

Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence  pages 512–521  1999.

[13] D. Pelleg and A. Moore. Accelerating exact k-means algorithms with geometric reasoning. In
S. Chaudhuri and D. Madigan  editors  Proceedings of the Fifth International Conference on
Knowledge Discovery in Databases  pages 277–281. AAAI Press  1999.

[14] B. Thiesson  C. Meek  and D. Heckerman. Accelerating EM for large databases. Machine

Learning  45(3):279–299  2001.

[15] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Information

Processing Letters  40(4):175–179  1991.

[16] J. J. Verbeek  J. R. Nunnink  and N. Vlassis. Accelerated EM-based clustering of large data sets.

Data Mining and Knowledge Discovery  13(3):291–307  2006.

[17] J. J. Verbeek  N. Vlassis  and J. R. J. Nunnink. A variational EM algorithm for large-scale
mixture modeling. In In Proceedings of the 8th Annual Conference of the Advanced School for
Computing and Imaging (ASCI)  2003.

9

,Jiashi Feng
Huan Xu
Shie Mannor
Shuicheng Yan