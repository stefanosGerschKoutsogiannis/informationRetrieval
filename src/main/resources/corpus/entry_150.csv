2016,A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics,Estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital. In this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient. The algorithm addresses several known challenges with clinical state estimation such as eliminating bias introduced by therapeutic intervention censoring  increasing the timeliness of state estimation while ensuring a sufficient accuracy  and the ability to detect anomalous clinical states. These benefits are obtained by combining the tools of non-parametric Bayesian inference  permutation testing  and generalizations of the empirical Bernstein inequality. The algorithm is validated using real-world data from a cancer ward in a large academic hospital.,A Non-parametric Learning Method for Conﬁdently

Estimating Patient’s Clinical State and Dynamics

William Hoiles

Department of Electrical Engineering
University of California Los Angeles

Los Angeles  CA 90024

whoiles@ucla.edu

Mihaela van der Schaar

Department of Electrical Engineering
University of California Los Angeles

Los Angeles  CA 90024
mihaela@ee.ucla.edu

Abstract

Estimating patient’s clinical state from multiple concurrent physiological streams
plays an important role in determining if a therapeutic intervention is necessary and
for triaging patients in the hospital. In this paper we construct a non-parametric
learning algorithm to estimate the clinical state of a patient. The algorithm ad-
dresses several known challenges with clinical state estimation such as eliminating
the bias introduced by therapeutic intervention censoring  increasing the timeliness
of state estimation while ensuring a sufﬁcient accuracy  and the ability to detect
anomalous clinical states. These beneﬁts are obtained by combining the tools of
non-parametric Bayesian inference  permutation testing  and generalizations of the
empirical Bernstein inequality. The algorithm is validated using real-world data
from a cancer ward in a large academic hospital.

1

Introduction

Timely clinical state estimation can signiﬁcantly improve the quality of care for patient’s by informing
clinicians of patient’s that have entered a high-risk clinical state. This is a challenging problem as the
patient’s clinical state is not directly observable and must be inferred from the patient’s vital signs
and the clinician’s domain-knowledge. Several methods exist for estimating the patient’s clinical
state including clinical guidelines and risk scores [21  18]. The limitation with these population
based methods is that they are not personalized (e.g. patient models are not unique)  can not
detect anomalous patient dynamics  and most importantly  are biased due to therapeutic intervention
censoring [16]. Therapeutic intervention censoring occurs when a patient’s physiological signals are
misclassiﬁed in the training data as a result of the effects caused by therapeutic interventions. To
improve the quality of patient care  new methods are needed to overcome these limitations.
In this paper we develop an algorithm for estimating a patient’s clinical state based on previously
recorded electronic health record (EHR) data. A schematic of the algorithm is provided in Fig.1 which
contains three primary components: a) learning the patient’s stochastic model  b) using statistical
techniques to evaluate the quality of the estimated stochastic model  and c) performing clinical state
estimation for new patients based on their estimated models. The works by Fox et al. [10  9] and
Saria et al. [19] for temporal segmentation are the most related to our algorithm. However [10  19]
do not apply formal statistical techniques to validate and iteratively update the hyper-parameters
of the non-parametric Bayesian inference  are not personalized  do not remove the bias caused
by therapeutic intervention censoring  and do not utilize clinician domain knowledge for clinical
state estimation. Additionally  applying fully Bayesian methods [9] for clinical state estimation are
computationally prohibitive as the computational complexity of constructing the stochastic model of
all patients grows polynomially with the number of samples and maximum number of possible states
of all patients. The computational complexity of our algorithm is only polynomial in the number

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

of samples and states of a single patient. A detailed literature review is provided in the Supporting
Material.
The proposed algorithm (Fig.1) learns a combinatorial stochastic model for each patient based on
their measured vital signs. A non-parametric Bayesian learning algorithm based on the hierarchical
Dirichlet process hidden Markov model (HDP-HMM) [10] is used to learn the patient’s stochastic
model which is composed of a possibly inﬁnite state-space HMM where each state is associated with
a unique dynamic model. The algorithm dynamically adjusts the number of detected dynamic models
and their temporal duration based on the patient’s vital signs–that is  the algorithm has a data-driven
bound on the model complexity (e.g. number of detected states). The patient’s stochastic model
provides a ﬁne-grained personalized representation of each patient that is interpretable for clinicians 
and accounts for the patient’s speciﬁc dynamics which may result from therapeutic interventions and
medical complications (e.g. disease  paradoxical reaction to a drug  bone fracture). To ensure that
each detected dynamic model is associated with a unique clinical state  the hyper-parameters in the
HPD-HMM are updated iteratively using the results from an improved Bonferroni method [2]. This
mitigates the major weakness of non-parametric Bayesian inference methods of how to select the
hyper-parameters [14  12]. Additionally  the algorithm provides statistical guarantees on the dynamic
model parameters using generalizations of the scalar Bernstein inequality [13] to vector-valued
and matrix-valued random variables. In clinical applications it is desirable to relate a collection of
dynamic models from several patient’s to a unique clinical state of interest for the clinician (e.g.
detecting which patients have entered a high-risk clinical state). The clinician deﬁnes a supervised
training set that is composed of all previously observed patient’s dynamic models and their associated
clinical state  which is then used to construct a similarity metric. This construction of the similarity
metric between dynamic models and clinical states ensures that the bias introduced from therapeutic
intervention censoring is removed  and also allows for the detection of anomalous dynamic models
that are not associated with a previously deﬁned clinical state. When a new patient arrives the
algorithm will learn their stochastic model  and then use the similarity metric to map the detected
dynamic models to their associated clinical states of interest.
Though our algorithm is general and can be applied in several medical settings (e.g. mobile health 
wireless health) here we focus on detecting the clinical state of patients in hospital wards. Speciﬁcally
we apply our algorithm to patient’s in a cancer ward of a large academic hospital.

Model and Parameters

personalization

Segmentation

(cid:18) ﬁne-grained
(cid:18) ﬁne-grained

Segmentation

personalization

(cid:19)

(cid:19)

Ofﬂine Learning

no

Clinician

Validation

Label

yes
¯D
L

Similarity

Clinical State

Estimate

Electronic Health

Records D

New Patient
Vitals {yt}t∈T

Figure 1: Schematic of the proposed algorithm for learning the dynamic model and estimating the
clinical state of the patient. From D a valid segmentation ¯D is constructed and provided to the
clinician to construct the labeled dataset L. New patient vital signs are labeled using the dataset L.

t}t∈T i}i∈I  with yi

2 Non-parametric Learning Algorithm for Patient’s Stochastic Model
In this section we provide a method to segment patient’s electronic health record data D =
{{yi
t ∈ Rm the vital signs of patient i ∈ I at time t. To segment the tem-
poral data we assume that the vital signs of each patient originate from a switching multivariate
Gaussian (SMG) process. A Bayesian non-parametric learning algorithm is utilized to select the
switching times between the unique dynamic models–that is  we consider the observation dynamics
and model switching dynamics simultaneously. The ﬁnal result of the segmentation is the dataset:

¯D = {{yi

t}t∈T i

k

  k ∈ {1  . . .   K i} = Ki}i∈I

(1)

2

k the time samples for segment k and Ki the set of segments for patient i. Statistical methods
with T i
are used to ensure that each dynamic model is associated with a unique clinical state  refer to Sec.3
for details.
We assume that the switching process between models satisﬁes a HMM where each state of the HMM
is associated with a unique dynamic model given by:

εt(zt) ∼ N (µ(zt)  Σ(zt))

yt = εt(zt)

(2)
where zt ∈ Ki is the state of the patient  and εt(zt) is a Gaussian white noise term with covariance
matrix Σ(zt). For notational convenience we will suppress the indices i and only include explicitly
when required. For segmentation each of the patients is treated independently. Each state zt is assumed
to evolve according to a HMM with zt associated with a speciﬁc segment k ∈ K. Notice that we
must estimate the total number of states |K|  and the associated model parameters {µ(k)  Σ(k)}k∈K
using only the data {yt}t∈T .
To learn the cardinality of the HMM we use the tools of non-parametric Bayesian inference by placing
a prior on the HMM parameters to allow a data-driven estimation of cardinality of the state-space.
Recall that non-parametric here indicates that for larger sample size T   the number of possible states
(i.e. dynamic models) can also increase. To model the inﬁnite-HMM we use the hierarchical Dirichlet
process (HDP) [3  22]. The HDP can be interpreted as a HMM with a countably inﬁnite state-space.
That is  the HDP is a non-parametric prior for the inﬁnite-HMM. The main idea of the HDP is to
link a countably inﬁnite set of Dirichlet processes by sharing atoms among the DPs with each DP
associated with a speciﬁc state. The stick-breaking construction of the HDP is given by [8  22]:

∞(cid:88)

m−1(cid:89)

∞(cid:88)

m=1

φk =

m ∼ H  φ0 =

βmδm 

βm = vm

(1 − vl) 

vm ∼ Beta(1  γ) 

m=1

l=1

πkmδm  πk ∼ DP(α  β).

(3)

(cid:17)

(cid:16)

α + κ 

βk = vk

αβ + κδk

α + κ

k−1(cid:89)

l=1

k = 1  2  . . .

t = 1  2  . . .   T.

(1 − vl)  πk ∼ DP

Eq.(3) represents an inﬁnite state HMM with πkm the transition probability of transitioning from
state k ∈ K to state m ∈ K. πk represents the transition probabilities out of state k of the HMM with
β the shared prior parameter of the transition distribution  H is a prior on the transition probability
distribution  and α the concentration of the transition probability distribution of the HMM.
The patient’s stochastic model is constructed by combining the SMG (2) with the HDP (or inﬁnite
HMM) and is given by:
vk ∼ Beta(1  γ) 
zt ∼ π(·|zt−1) = πzt−1  yt = ε(zt)

(4)
The parameter γ controls how concentrated the state transition function is from state k to state k(cid:48).
This can be seen by setting κ = 0 and α = 0 such that E[πk] = β. If γ = 1 then the parameter
βk in β decays at approximately a geometric rate for increasing k. As γ increases  the decay of the
elements in β decrease. For α > 0 and κ > 0 then E[πk] = (αβ + κδk)/(α + κ)  as such κ controls
the bias of πk towards self-transitions–that is  π(k|k) is given a large weight. The parameter α + κ
controls the variability of πk and the base state transition distribution (αβ + κδk)/(α + κ).
Given the patient’s stochastic model (4)  non-parametric Bayesian inference are utilized to estimate
the model parameters from the patient’s vital signs {yt}t∈T . To utilize Bayesian inference we deﬁne
a prior and compute the associated posterior since a σ-ﬁnite density measure is present. The prior
distributions on β and π are given by:

β ∼ Dir(γ/L  . . .   γ/L)  πk ∼ Dir(αβ1  . . .   αβk + κ  . . .   βL) k ∈ {1  . . .   L}.

(5)
Eq.(5) is the weak limit approximation with truncation level L where L is the largest number of
expected states in the estimated HMM from {yt}t∈T [25]. Note that as L → ∞ then (5) approach
the HDP. If clinician domain knowledge is not available on the initial hyper-parameters γ  α  and κ 
then it is common to place Beta or Gamma priors on these distributions [25]. For the multivariate
Gaussian we utilize the Normal-Inverse-Wishart prior distribution [11]:

(cid:17)
(µ − µ0)(cid:48)Σ−1(µ − µ0))

(6)

p(µ  Σ|µ0  λ  S0  v) ∝ |Σ| v+m+1

2

exp

(cid:16) − 1

2

tr(vS0Σ−1 − λ
2

3

where v and S0 are the degrees of freedom and the scale matrix for the inverse-Wishart distribution
on Σ  µ0 is the prior mean  and λ is the number of prior measurements on the Σ scale. Given the
prior distribution with associated posterior distributions a MCMC or variational sampler (i.e. Gibbs
sampler [10]  Beam sampler [25]  variational Bayes [6  7]) can be utilized to estimate the parameters
of the patient’s stochastic model (4) given the data {yt}t∈T .

k

t}t∈T i

3 Statistical Methods to Evaluate Stochastic Model Quality
Given the segmented dataset ¯D (1) generated from all the patient’s estimated stochastic models (4) 
this section presents methods to evaluate the quality of ¯D. This includes testing if the vital signs
{yi
for each patient and unique dynamic model are consistent with a multivariate Gaussian
distribution  contain sufﬁcient samples to guarantee the accuracy of the dynamic model parameters 
and that the detected dynamic models for each patient are unique. If the estimated stochastic models
are of low quality then the hyper-parameters of the non-parametric Bayesian inference algorithm
can be iteratively updated to ensure that all the patient’s stochastic models accurately represent their
dynamics. This is a vital step in medical applications since the results of the non-parametric Bayesian
inference algorithm are sensitive to the selected hyper-parameters [14  12]. For example Fig.2(a)
illustrates a poor quality segmentation that results from poorly selected hyper-parameters.

3.1 Hypothesis Tests for Model Consistency with Segments
To ensure model consistency we must test if each segment in ¯D is consistent with a multivariate
Gaussian process (i.e. samples are independent and normally distributed). To test if the segment
{yt}t∈Tk ∈ ¯D contains independent samples we evaluate the autocorrelation function (ACF) [5]
for each segment. For {yt}t∈Tk the ACF must exponentially decay to zero which indicates that
the segment contains independent samples. Note that it is possible for a spurious autocorrelation
structure to be present in the segment if the segment is composed of a mixture of Gaussian processes.
If this is suspected then the hyper-parameters of the non-parametric Bayesian inference algorithm are
updated to increase the number of segments (for example by increasing L or decreasing κ). Since
there is no universally most powerful test for multivariate normality  we use the improved Bonferroni
method [23] which contains four afﬁne invariant hypothesis test statistics elevating the need to select
the most sensitive single test while retaining the beneﬁts of the these four multivariate normality tests.

3.2 Data-Driven Conﬁdence Bounds for Dynamic Model Estimation
An important consideration when evaluating the quality of the segmentation ¯D is that each segment
contains sufﬁcient samples to conﬁdently estimate the mean and covariance {µ  Σ} of the SMG
model. This is particularly important in medical applications as it provides an estimate of the
maximum number of samples needed to conﬁdently estimate {µ  Σ} which are used to estimate
the clinical state of the patient. Note that the estimated posterior distribution for {µ  Σ} can not be
used to bound the number of samples required. To estimate {µ  Σ} given {yt}t∈Tk  the maximum
likelihood estimators given by:

nk(cid:88)
(yt − ˆµ(k))(yt − ˆµ(k))(cid:48)

(7)

nk(cid:88)

t=1

ˆµ(k) =

1
nk

yt 

ˆΣ(k) =

1
nk

t=1

are used with nk = |Tk| is the total number of samples in segment k ∈ K. If each vital sign is
independent (i.e. spherical multivariate Gaussian distribution) then an empirical Bernstein bound [13]
can be constructed to estimate the error between the sample mean ˆµ and the actual mean µ. From the
empirical Bernstein bound  the minimum number of samples necessary to ensure that P (ˆµ(k  j) −
µ(k  j) ≥ ε) ≤ α for all segments k ∈ K and streams j ∈ {1  . . .   m} for some conﬁdence level
α > 0 and tolerance ε ≥ 0 is given by:

n(ε  α) ≥(cid:16) 6σ2

(cid:17)

max + 2∆maxε

3ε2

ln(

1
α

)

(8)

max the maximum possible variance and ∆max the maximum possible difference between the

with σ2
maximum and minimum values of all values in the vital sign data.

4

To construct a relaxed bound on the sample mean ˆµ ∈ Rm  and a bound on the sample covariance
ˆΣ ∈ Rm×m computed using (7)  we generalize the empirical Bernstein bound to the multidimensional
case. The goal is to construct a bound of the form P (||Z|| ≥ ε) ≤ α where || · || denotes the spectral
norm if Z is a matrix  or the 2-norm in the case Z is a vector. To construct a probabilistic bound on
the accuracy of the estimated mean we utilize the vector Bernstein inequality given by Theorem 1.
Theorem 1 Let {Y1  . . .   Yn} be a set of independent random vectors with Yt ∈ Rm for t ∈
{1  . . .   n}. Assume that each vector has uniform bounded deviation such that ||Yt|| ≤ L ∀t ∈

{1  . . .   n}. Writing Z =(cid:80)n

t=1 Yt  then
P (||Z|| ≥ ε) ≤ (2m) exp

(cid:16)

(cid:17)

−3ε2

6V (Z) + 2Lε

n(cid:88)

t=1

 

V (Z) =

E[||Yt||2
2].

(9)

The proof of Theorem 1 is provided in the Supporting Material. To construct the bound on the number
of samples necessary to estimate the mean we deﬁne Z = ˆµ − µ with Yt = (yt − µ)/n. Using the
triangle inequality  Jensen’s inequality  and assuming ||yt||2 ≤ B1 for some constant B1  we have
that:

1 − ||µ||2
(10)
Plugging (10) into (9) results in the minimum number of samples necessary to guarantee that
(cid:17)
P (|| ˆµ − µ|| ≥ ε) ≤ α with the number of samples n(ε  α) given by:
2m
α

n(ε  α) ≥(cid:16) 6(B2

1 − ||µ||2
3ε2

V (Z) ≤ 1
n

L ≤ 2B1
n

(cid:0)B2

2) + 4B1ε

(cid:1).

(11)

ln(

).

2

 

To bound the number of samples necessary to estimate Σ we utilize the corollary of Theorem 1
for real-symmetric matrices with Z = ˆΣ − Σ. The bound on the number of samples necessary to
guarantee P (|| ˆΣ − Σ|| ≥ ε) ≤ α  assuming ||Σ|| ≤ ||yt − ˆµ|| ≤ B2  is given by:

n(ε  α) ≥(cid:16) 6B2

(cid:17)

2 + 4B2ε

3ε2

ln(

2m
α

).

(12)

For a given α and ε  and an estimate of the maximum spectral norm of Σ and norm of µ  equations
(11) and (12) can be used to estimate the minimum number of samples necessary to sufﬁciently
estimate {µ  Σ}. To accurately compute the clinical state from the unique dynamic model  each
segment must satisfy (11) and (12)  otherwise any clinical state estimation may give unreliable results.

3.3 Statistical Tests for Statistically Identical Dynamic Models

In this section we construct a novel hypothesis test for mean and covariance equality with a given
conﬁdence  and design parameters that control the importance of the mean equality compared to
the covariance equality. The hypothesis test both evaluates the quality of the estimated stochastic
model  but can also be used to merge statistically identical segments to increase the accuracy of
the dynamic model parameter estimates. Given two segments of vital signs  each associated with a
supposedly unique dynamic model  we deﬁne the null hypothesis H0 as the equality of the mean and
covariance matrices from the two dynamic models  and the alternate hypothesis H1 that either the
mean or covariance are not equal. Formally:

H0 : Σ(k) = Σ(k(cid:48)) and µ(k) = µ(k(cid:48))  H1 : Σ(k) (cid:54)= Σ(k(cid:48)) or µ(k) (cid:54)= µ(k(cid:48)).

(13)
Several methods exist for testing for covariance equality [20] and for mean equality [24]  however
we wish to test for both covariance and location equality. To test for the global hypothesis H0 in (13) 
note that H0 and H1 can equivalently be stated as a combination of the sub-hypothesis as follows:
(14)
1 : Σ(k) (cid:54)= Σ(k(cid:48)). To
with H 1
construct the hypothesis test for H0 the non-parametric the permutation testing method [17] is used
which allows us to combine the sub-hypothesis tests for covariance and mean equality to construct a
hypothesis test for H0.
To test for the null hypothesis H 1
0 we utilize Hotelling’s T 2 test as it is asymptotically the most
powerful invariant test when the data associated with k and k(cid:48) are normally distributed [4]. Given that

H0 : H 1
1 : µ(k) (cid:54)= µ(k(cid:48))  H 2

0 : Σ(k) = Σ(k(cid:48))  and H 2

0 : µ(k) = µ(k(cid:48))  H 1

and H1 : H 1

0 ∩ H 2

1 ∪ H 2

1

0

5

yt are generated from a multivariate normal distribution  the test statistic τ 1 follows a T 2 distribution
such that τ 1 ∼ T 2(m  n(k)+n(k(cid:48))−2) where n(k) and n(k(cid:48)) are the number of samples in segments
k and k(cid:48) respectively. To test for the null hypothesis H 2
0 we utilize the modiﬁed likelihood ratio
statistic provided by Bartlett [1]  written Λ∗  which is uniformly the most power unbiased test for
covariance equality [15]. The test statistic for covariance equality is given by:

ρ = 1 − 2m2 + 3m − 1

6(m + 1)n

τ 2 = −2ρ log(Λ∗) 

(n/n(k) + n/n(k(cid:48)) − 1)  n = n(k) + n(k(cid:48)).

From (Theorem 8.2.7 in [15]) the asymptotic cumulative distribution function of τ 2 can be approxi-
mated by a linear combination of χ2 distributions which has a convergence rate of O((ρn)−3).
To construct the permutation test for H0 Tippett’s combining function [17] is used with H0:
τ = min(λ1/k1  λ2/k2) where λ1 and λ2 are the p-values of the sub-hypothesis tests H 1
0 and
H 2
0 respectively  and k1 and k2 are design parameters. If k1 > k2 then the mean equality is weighted
more then the covariance equality. If k1 = k2 then both mean equality and covariance equality
are weighted equally. For the test statistics τ 1 and τ 2 the p-values are given by λ1 = P (τ 1 ≥ τ 1
0 )
and λ2 = P (τ 2 ≥ τ 2
0 are realizations of the test statistics. To utilize τ as a test
statistic we require the cumulative distribution function of τ. Note that if H 1
0 is true (i.e. mean
equality) then the distributions of τ 1 and τ 2 are independent since τ 1 follows a T 2 distribution which
results in λ1 ∼ U(0  1) and λ2 ∼ U(0  1) [17]. The cumulative distribution function of τ is given by
P (τ ≤ x) = (k1 + k2)x− k1k2x2 for x ∈ [0  min(1/k1  1/k2)]. Given P (τ ≤ x)  for a signiﬁcance
level α  we reject the null hypothesis H0 if τ ≤ δ where δ is the solution to P (τ ≤ δ) = α. The

parameter δ is given by: δ =(cid:0)(k1 + k2) −(cid:112)(k1 + k2)2 − 4αk1k2(cid:1)/(2k1k2).

0 ) where τ 1

0 and τ 2

For a given signiﬁcance level α  and design parameters k1 and k2  we can test H0 for the samples
{yt}t∈Tk and {yt}t∈Tk(cid:48) by evaluating τ0 = min(λ1
0 the realizations of
the p-values for τ1 and τ2. By repeatedly applying this hypothesis test to segments {yt}t∈Tk for
k ∈ K we can detect any segments with equal mean and covariance with a signiﬁcance level α.
Similar segments can be merged to increase the accuracy of the estimated dynamic model parameters 
or be used to evaluate the quality of the patient’s stochastic model.

0/k2) with λ1

0 and λ2

0/k1  λ2

4 Estimating Patient’s Clinical State using Clinician Domain-Knowledge

In this section the Algorithm 1 (Fig.1) is presented which constructs stochastic models of patients
based on their historical EHR data and clinician domain-knowledge  and is used to classify the
clinical state of new patients.
Algorithm 1 is composed of ﬁve main steps. Step#1 to Step#2 are used to construct the stochastic
models of the patients based on the EHR data D  and to construct the segmented dataset ¯D (1). The
stochastic models are constructed using the non-parametric Bayesian inference algorithm from Sec.2.
Step#2 measures the quality of the stochastic models  and iteratively updates the hyper-parameters
of the Bayesian inference algorithm to guarantee the quality of the detected dynamic models as
discussed in Sec.3. In Step#3 each segment (e.g. dynamic model) in ¯D is labelled by the clinician 
based on the clinical states of interest  to construct the dataset L. Step#4 and Step#5 involves the
online portion of the algorithm which constructs stochastic models for new patients and estimates
their clinical state based on each patient’s estimated stochastic model. Step#4 constructs the
stochastic model for the new patient  then in Step#5 each unique dynamic model from Step#4 is
associated with a clinical state of interest using the labelled dataset L from Step#3. Note that L
contains several segments (e.g. dynamic models) that are associated with one clinical state. To
estimate the clinical state of the new patient a similarity metric based on the Bhattacharyya distance 
written DB(·)  is used. If the minimum Bhattacharyya distance between the new patients segment
k and next closest segment k(cid:48) ∈ L is greater then δth the segment is labelled as anomalous  otherwise
the segment is given the label of segment k(cid:48) ∈ L. Information on the computational complexity
and implementation details of Algorithm 1 are provided in the Supporting Material.

5 Real-World Clinical State Estimation in Cancer Ward

In this section Algorithm 1 is applied to a real-world EHR dataset composed of a cohort of patients
admitted to a cancer ward. A detailed description of the dataset is provided in the Supporting Material.

6

algorithm presented in Sec.2. Using the stochastic models construct the dataset ¯D (1).

Algorithm 1 Patient Clinical State Estimation
Step#1: Construct stochastic models for each patient using D and the non-parametric Bayesian
Step#2: To evaluate the quality of each stochastic model  each segment in ¯D from Step#1 is tested
for: i) model consistency  ii) sufﬁcient samples to guarantee accuracy of dynamic model
parameter estimates  and iii) statistical uniqueness of segments using the methods in Sec.3.
If the quality is not sufﬁcient then return to Step#1 with updated hyper-parameters for the
non-parametric Bayesian inference algorithm.
Step#3: Given ¯D and the clinical states of interest  the clinician constructs the labelled dataset
L = {({yi
k)  k ∈ {1  . . .   K i} = Ki}.
  li
t}t∈T 0  construct the stochastic model of the
Step#4: For a new patient i = 0 with vital signs {y0
patient using the Bayesian non-parametric learning algorithm. Then  based on the stochastic
model  construct the segmented vital sign data {{y0
Step#5: To estimate the label l(k)  written ˆl(k)  of each segment k ∈ K0 from Step#4  compute the

  k ∈ {1  . . .   K 0} = K0}.

t}t∈T 0

k

t}t∈T i

k

solution to the following optimization problem for each k:

l∈L{DB(k  k(cid:48))} ≥ δth then ˆl(k) = ∅  else ˆl(k) ∈ argmin
l∈L

if min
with ∅ the anomalous state  Ll ∈ L the set of segments that are labeled with l  L−l ∈ L the
set of all segments that are not labeled as l  and δth is a threshold. Return to Step#4.

mink(cid:48)∈L−l{DB(k  k(cid:48))}

(cid:110) mink(cid:48)∈Ll{DB(k  k(cid:48))}

(cid:111)

The ﬁrst step of Algorithm 1 is to segment the EHR data based on the estimated stochastic models
of the patients. Fig.2(a) illustrates the dynamic models of a speciﬁc patient’s estimated stochastic
model for κ = 0.1 and S0 = 0.1Im (Im is the identity matrix)  and for κ = 1 and S0 = Im. As
seen  for κ = 0.1 and S0 = 0.1Im several segments have insufﬁcient samples for estimating the
model parameters  and are not statistically unique. However the segments resulting from κ = 1 and
S0 = Im provide a stochastic model of sufﬁcient quality where each segment contains sufﬁcient
samples to accurately estimate the model parameters  the segments are statistically unique  and
satisfy the multivariate normality assumption. Therefore we set κ = 1 and S0 = Im to construct the
segmented dataset ¯D from D. The dataset L is constructed by providing the clinician with ¯D who
then labels each segment as either in the ICU admission clinical state  or non-ICU clinical state.

(a) Dynamic model estimates with {κ  S0} =
{0.1  0.1Im} (dotted)  and {1  Im} (solid).

(b) Estimated dynamic models for the intervals of
patient data in Fig.2(d).

(c) Trade off between the TPR and PPV. The
dashed cross-hair indicates the performance of Al-
gorithm 1 for δb = 1.

(d) Physiological signals from the patient with dis-
covered models in Fig.2(b).

Figure 2: Dynamic model discovery and performance of Algorithm 1.

7

Time[hours]050010001500DynamicModels051015Time[hours]0500100015002000DynamicModels12345678910ICUAdmissionPositivePredictiveValue0.10.20.30.40.50.6TruePositiveRate0.20.40.60.81Algorithm1RothmanMEWSTime[hours]0500100015002000PhysiologicalValues50100150200Heart-RateDiastolicSystolicOf critical importance in medical applications is the accuracy and timeliness of the detection of
the clinical state of the patient. Fig.2(b) provides the trade-off between the TPR and PPV between
Algorithm 1  Rothman index [18] which is a state-of-the-art method utilized in many hospitals today 
and MEWS [21] which are dependent on the threshold selected for each. As seen Algorithm 1
has a superior performance compared to these two popular risk scoring methods. For example if
we require the TPR = 71.9%  then the associated PPV values for the Rothman index and MEWS
are 26.1% and 18.0% respectively. There is a 11.3% increase in the PPV value for the Rothman
index  and 19.4% increase in the PPV for MEWS compared to the PPV of Algorithm 1. We also
compare with methods commonly used in medical with the results presented in Table 1. As seen 
Algorithm 1 outperforms all these methods for estimating the patient’s clinical state. There are several
possible reasons that Algorithm 1 outperforms these methods including accounting for therapeutic
interventions and utilizing ﬁne-grained personalization. Note that the results in Table 1 are computed
12 hours prior to ICU admission or hospital discharge. Additionally  the average detection time of
ICU admission or discharge using Algorithm 1 is approximately 24 hours prior to the clinician’s
decision. This timeliness ensures that the patient’s clinical state estimate provides clinicians with
sufﬁcient warning to apply a therapeutic intervention to stabilize the patient.

Table 1: Accuracy of Methods for Predicting ICU Admission

Algorithm
Algorithm 1

Rothman Index

SVMs

MEWS

Logistic Regression
Lasso Regularization

Random Forest

TPR(%)
71.9%
53.9%
28.1%
55.7%
55.8%
44.5%
32.2%

PPV(%)
37.4%
34.5%
26.3%
30.7%
30.3%
31.1%
29.9%

A key feature of Algorithm 1 is that it learns the number of unique dynamic models for each patient 
and as more data is collected the number of unique dynamic models discovered may increase. Fig.2(b)
illustrates this process for a patient with associated physiological signals given in Fig.2(d). The
horizontal dashed line indicates the intervals and associated discovered dynamic models. Note that
typical hospitalization time for cancer ward patients in the dataset range from 4 hours to over 85
days. As seen  as more samples are obtained for the patient the number of dynamic models that
describe the patient’s dynamics increase. Additionally  there is good agreement between where the
patient’s dynamics change for the different time intervals. For example the change point at 40 hours
after hospitalization occurs as a result of an increase in the systolic and diastolic blood pressure  and
a decrease in the heart-rate. At 1700 hours the change in state results from a dramatic increase in
both the systolic and diastolic blood pressure  and a decrease in the heart-rate. From Fig.2(d) these
physiological signals were not observed previously  therefore Algorithm 1 correctly detects that this
is a new unique state for the patient. Though Algorithm 1 can identify changes in patient state  the
domain-knowledge from the clinician is required to deﬁne the clinical state of the patient. Only
dynamic models 8 and 9 are associated with the ICU admission state.
Further results are provided in the Supporting Material that illustrate how current methods for
constructing risk scores suffer from the bias introduced from therapeutic intervention censoring  and
how a binary threshold δb can be introduced into Algorithm 1 for controlling the TPR and PPV for
clinical state estimation.

6 Conclusion

In this paper a novel non-parametric learning algorithm for conﬁdently learning stochastic models of
patient’s and classifying their associated clinical state was presented. Compared to state-of-the-art
clinical state estimation methods our algorithm eliminates the bias caused by therapeutic intervention
censoring  is personalized to the patient’s speciﬁc dynamics resulting from medical complication
(e.g. disease  drug interactions  physical contusions or fractures)  and can detect anomalous clinical
states. The algorithm was applied to real-world patient data from a cancer ward in a large academic
hospital  and found to have a signiﬁcant improvement in classifying patient’s clinical state in both
accuracy and timeliness compared with current state-of-the-art methods such as the Rothman index.
The algorithm provides valuable information to allow clinicians to make informed decisions about
selecting if a therapeutic intervention is necessary to improve the clinical state of the patients.

8

Acknowledgments
This research was supported by: NSF ECCS 1462245  and the Airforce DDDAS program.
References
[1] M. Bartlett. Properties of sufﬁciency and statistical tests. Proc. Roy. Soc. London A  160:268–282  1937.
[2] D. Basso  F. Pesarin  L. Salmaso  and A. Solari. Permutation Tests. Springer  2009.
[3] M. Beal  Z. Ghahramani  and C. Rasmussen. The inﬁnite hidden Markov model. In Advances in neural

information processing systems  pages 577–584  2001.

[4] M. Bilodeau and D. Brenner. Theory of Multivariate Statistics. Springer  2008.
[5] P. Brockwell and R. Davis. Time series: theory and methods. Springer Science & Business Media  2013.
[6] M. Bryant and E. Sudderth. Truly nonparametric online variational inference for hierarchical Dirichlet

processes. In Advances in Neural Information Processing Systems  pages 2699–2707  2012.

[7] T. Campbell  J. Straub  J. Fisher  and J. How. Streaming  distributed variational inference for Bayesian

nonparametrics. In Advances in Neural Information Processing Systems  pages 280–288  2015.

[8] T. Ferguson. A Bayesian analysis of some nonparametric problems. The annals of statistics  pages 209–230 

1973.

[9] E. Fox  M. Jordan  E. Sudderth  and A. Willsky. Sharing features among dynamical systems with beta

processes. In Advances in Neural Information Processing Systems  pages 549–557  2009.

[10] E. Fox  E. Sudderth  M. Jordan  and A. Willsky. An HDP-HMM for systems with state persistence. In

Proceedings of the 25th international conference on Machine learning  pages 312–319. ACM  2008.

[11] A. Gelman  J. Carlin  H. Stern  and D. Rubin. Bayesian data analysis  volume 2. Taylor & Francis  2014.
[12] A. Johnson  M. Ghassemi  S. Nemati  K. Niehaus  D. Clifton  and G. Clifford. Machine learning and

decision support in critical care. Proceedings of the IEEE  104(2):444–466  2016.

[13] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. COLT  2009.
[14] G. Montanez  S. Amizadeh  and N. Laptev.

Inertial Hidden Markov Models: Modeling change in

multivariate time series. In AAAI  pages 1819–1825  2015.

[15] R. Muirhead. Aspects of multivariate statistical theory. Wiley  1982.
[16] C. Paxton  A. Niculescu-Mizil  and S. Saria. Developing predictive models using electronic medical
records: challenges and pitfalls. In Annual Symposium proceedings/AMIA Symposium. AMIA Symposium 
volume 2013  pages 1109–1115. American Medical Informatics Association  2012.

[17] F. Pesarin and L. Salmaso. Permutation tests for complex data: theory  applications and software. John

Wiley & Sons  2010.

[18] M. Rothman  S. Rothman  and J. Beals. Development and validation of a continuous measure of patient
condition using the electronic medical record. Journal of biomedical informatics  46(5):837–848  2013.
[19] S. Saria  D. Koller  and A. Penn. Learning individual and population level traits from clinical temporal
data. In Proc. Neural Information Processing Systems (NIPS)  Predictive Models in Personalized Medicine
workshop. Citeseer  2010.

[20] J. Schott. A test for the equality of covariance matrices when the dimension is large relative to the sample

sizes. Computational Statistics & Data Analysis  51(12):6535–6542  2007.

[21] P. Subbe  M. Kruger  P. Rutherford  and L. Gemmel. Validation of a modiﬁed Early Warning Score in

medical admissions. Qjm  94(10):521–526  2001.

[22] Y. W. Teh  M. Jordan  M. Beal  and D. Blei. Hierarchical Dirichlet processes. Journal of the american

statistical association  2012.

[23] C. Tenreiro. An afﬁne invariant multiple test procedure for assessing multivariate normality. Computational

Statistics & Data Analysis  55(5):1980–1992  2011.

[24] N. Timm. Applied Multivariate Analysis  volume 1. Springer  2002.
[25] J. Van Gael  Y. Saatci  Y. W. Teh  and Z. Ghahramani. Beam sampling for the inﬁnite hidden Markov
model. In Proceedings of the 25th international conference on Machine learning  pages 1088–1095. ACM 
2008.

9

,Mohammad Gheshlaghi azar
Alessandro Lazaric
Emma Brunskill
William Hoiles
Mihaela van der Schaar
Sagie Benaim
Lior Wolf