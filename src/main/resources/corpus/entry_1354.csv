2014,Deep Learning Face Representation by Joint Identification-Verification,The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper  we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart  while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together  both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset  99.15% face verification accuracy is achieved. Compared with the best previous deep learning result on LFW  the error rate has been significantly reduced by 67%.,Deep Learning Face Representation by Joint

Identiﬁcation-Veriﬁcation

1Department of Information Engineering  The Chinese University of Hong Kong

2SenseTime Group

Yi Sun1

Yuheng Chen2

Xiaogang Wang3 4

Xiaoou Tang1 4

3Department of Electronic Engineering  The Chinese University of Hong Kong
4Shenzhen Institutes of Advanced Technology  Chinese Academy of Sciences

sy011@ie.cuhk.edu.hk chyh1990@gmail.com
xgwang@ee.cuhk.edu.hk xtang@ie.cuhk.edu.hk

Abstract

The key challenge of face recognition is to develop effective feature repre-
sentations for reducing intra-personal variations while enlarging inter-personal
differences. In this paper  we show that it can be well solved with deep learning
and using both face identiﬁcation and veriﬁcation signals as supervision. The
Deep IDentiﬁcation-veriﬁcation features (DeepID2) are learned with carefully
designed deep convolutional networks. The face identiﬁcation task increases the
inter-personal variations by drawing DeepID2 features extracted from different
identities apart  while the face veriﬁcation task reduces the intra-personal
variations by pulling DeepID2 features extracted from the same identity together 
both of which are essential to face recognition. The learned DeepID2 features
can be well generalized to new identities unseen in the training data. On the
challenging LFW dataset [11]  99.15% face veriﬁcation accuracy is achieved.
Compared with the best previous deep learning result [20] on LFW  the error rate
has been signiﬁcantly reduced by 67%.

1

Introduction

Faces of the same identity could look much different when presented in different poses  illumina-
tions  expressions  ages  and occlusions. Such variations within the same identity could overwhelm
the variations due to identity differences and make face recognition challenging  especially in
unconstrained conditions. Therefore  reducing the intra-personal variations while enlarging the
inter-personal differences is a central topic in face recognition.
It can be traced back to early
subspace face recognition methods such as LDA [1]  Bayesian face [16]  and uniﬁed subspace
[22  23]. For example  LDA approximates inter- and intra-personal face variations by using two
scatter matrices and ﬁnds the projection directions to maximize the ratio between them. More recent
studies have also targeted the same goal  either explicitly or implicitly. For example  metric learning
[6  9  14] maps faces to some feature representation such that faces of the same identity are close
to each other while those of different identities stay apart. However  these models are much limited
by their linear nature or shallow structures  while inter- and intra-personal variations are complex 
highly nonlinear  and observed in high-dimensional image space.
In this work  we show that deep learning provides much more powerful tools to handle the two types
of variations. Thanks to its deep architecture and large learning capacity  effective features for face
recognition can be learned through hierarchical nonlinear mappings. We argue that it is essential
to learn such features by using two supervisory signals simultaneously  i.e. the face identiﬁcation
and veriﬁcation signals  and the learned features are referred to as Deep IDentiﬁcation-veriﬁcation
features (DeepID2).
Identiﬁcation is to classify an input image into a large number of identity

1

classes  while veriﬁcation is to classify a pair of images as belonging to the same identity or not
(i.e. binary classiﬁcation). In the training stage  given an input face image with the identiﬁcation
signal  its DeepID2 features are extracted in the top hidden layer of the learned hierarchical nonlinear
feature representation  and then mapped to one of a large number of identities through another
function g(DeepID2). In the testing stage  the learned DeepID2 features can be generalized to other
tasks (such as face veriﬁcation) and new identities unseen in the training data. The identiﬁcation
supervisory signal tends to pull apart the DeepID2 features of different identities since they have to
be classiﬁed into different classes. Therefore  the learned features would have rich identity-related
or inter-personal variations. However  the identiﬁcation signal has a relatively weak constraint on
DeepID2 features extracted from the same identity  since dissimilar DeepID2 features could be
mapped to the same identity through function g(·). This leads to problems when DeepID2 features
are generalized to new tasks and new identities in test where g is not applicable anymore. We solve
this by using an additional face veriﬁcation signal  which requires that every two DeepID2 feature
vectors extracted from the same identity are close to each other while those extracted from different
identities are kept away. The strong per-element constraint on DeepID2 features can effectively
reduce the intra-personal variations. On the other hand  using the veriﬁcation signal alone (i.e. only
distinguishing a pair of DeepID2 feature vectors at a time) is not as effective in extracting identity-
related features as using the identiﬁcation signal (i.e. distinguishing thousands of identities at a
time). Therefore  the two supervisory signals emphasize different aspects in feature learning and
should be employed together.
To characterize faces from different aspects  complementary DeepID2 features are extracted from
various face regions and resolutions  and are concatenated to form the ﬁnal feature representation
after PCA dimension reduction. Since the learned DeepID2 features are diverse among different
identities while consistent within the same identity  it makes the following face recognition easier.
Using the learned feature representation and a recently proposed face veriﬁcation model [3]  we
achieved the highest 99.15% face veriﬁcation accuracy on the challenging and extensively studied
LFW dataset [11]. This is the ﬁrst time that a machine provided with only the face region achieves an
accuracy on par with the 99.20% accuracy of human to whom the entire LFW face image including
the face region and large background area are presented to verify.
In recent years  a great deal of efforts have been made for face recognition with deep learning
[5  10  18  26  8  21  20  27]. Among the deep learning works  [5  18  8] learned features or
deep metrics with the veriﬁcation signal  while DeepFace [21] and our previous work DeepID
[20] learned features with the identiﬁcation signal and achieved accuracies around 97.45% on
LFW. Our approach signiﬁcantly improves the state-of-the-art. The idea of jointly solving the
classiﬁcation and veriﬁcation tasks was applied to general object recognition [15]  with the focus on
improving classiﬁcation accuracy on ﬁxed object classes instead of hidden feature representations.
Our work targets on learning features which can be well generalized to new classes (identities) and
the veriﬁcation task.

2

Identiﬁcation-veriﬁcation guided deep feature learning

We learn features with variations of deep convolutional neural networks (deep ConvNets) [12].
The convolution and pooling operations in deep ConvNets are specially designed to extract visual
features hierarchically  from local low-level features to global high-level ones. Our deep ConvNets
take similar structures as in [20]. It contains four convolutional layers  with local weight sharing
[10] in the third and fourth convolutional layers. The ConvNet extracts a 160-dimensional DeepID2
feature vector at its last layer (DeepID2 layer) of the feature extraction cascade. The DeepID2
layer to be learned are fully-connected to both the third and fourth convolutional layers. We use
rectiﬁed linear units (ReLU) [17] for neurons in the convolutional layers and the DeepID2 layer.
An illustration of the ConvNet structure used to extract DeepID2 features is shown in Fig. 1 given
an RGB input of size 55 × 47. When the size of the input region changes  the map sizes in the
following layers will change accordingly. The DeepID2 feature extraction process is denoted as
f = Conv(x  θc)  where Conv(·) is the feature extraction function deﬁned by the ConvNet  x is the
input face patch  f is the extracted DeepID2 feature vector  and θc denotes ConvNet parameters to
be learned.

2

Figure 1: The ConvNet structure for DeepID2 feature extraction.

DeepID2 features are learned with two supervisory signals. The ﬁrst is face identiﬁcation signal 
which classiﬁes each face image into one of n (e.g.  n = 8192) different identities. Identiﬁcation is
achieved by following the DeepID2 layer with an n-way softmax layer  which outputs a probability
distribution over the n classes. The network is trained to minimize the cross-entropy loss  which we
call the identiﬁcation loss. It is denoted as

Ident(f  t  θid) = − n(cid:88)

i=1

pi log ˆpi = − log ˆpt  

(1)

where f is the DeepID2 feature vector  t is the target class  and θid denotes the softmax layer
parameters. pi is the target probability distribution  where pi = 0 for all i except pt = 1
for the target class t.
ˆpi is the predicted probability distribution. To correctly classify all
the classes simultaneously  the DeepID2 layer must form discriminative identity-related features
(i.e. features with large inter-personal variations). The second is face veriﬁcation signal  which
encourages DeepID2 features extracted from faces of the same identity to be similar. The veriﬁcation
signal directly regularize DeepID2 features and can effectively reduce the intra-personal variations.
Commonly used constraints include the L1/L2 norm and cosine similarity. We adopt the following
loss function based on the L2 norm  which was originally proposed by Hadsell et al.[7] for
dimensionality reduction 

(cid:40) 1
2 max(cid:0)0  m − (cid:107)fi − fj(cid:107)2
2 (cid:107)fi − fj(cid:107)2

2

(cid:1)2

if yij = 1
if yij = −1

 

(2)

Verif(fi  fj  yij  θve) =

1

where fi and fj are DeepID2 feature vectors extracted from the two face images in comparison.
yij = 1 means that fi and fj are from the same identity. In this case  it minimizes the L2 distance
between the two DeepID2 feature vectors. yij = −1 means different identities  and Eq. (2) requires
the distance larger than a margin m. θve = {m} is the parameter to be learned in the veriﬁcation loss
function. Loss functions based on the L1 norm could have similar formulations [15]. The cosine
similarity was used in [17] as

Verif(fi  fj  yij  θve) =

(yij − σ(wd + b))2  

1
2

(3)

fi·fj

(cid:107)fi(cid:107)2(cid:107)fj(cid:107)2

is the cosine similarity between DeepID2 feature vectors  θve = {w  b} are
where d =
learnable scaling and shifting parameters  σ is the sigmoid function  and yij is the binary target of
whether the two compared face images belong to the same identity. All the three loss functions are
evaluated and compared in our experiments.
Our goal is to learn the parameters θc in the feature extraction function Conv(·)  while θid and θve are
only parameters introduced to propagate the identiﬁcation and veriﬁcation signals during training.
In the testing stage  only θc is used for feature extraction. The parameters are updated by stochastic
gradient descent. The identiﬁcation and veriﬁcation gradients are weighted by a hyperparameter λ.
Our learning algorithm is summarized in Tab. 1. The margin m in Eq. (2) is a special case  which
cannot be updated by gradient descent since this will collapse it to zero. Instead  m is ﬁxed and
updated every N training pairs (N ≈ 200  000 in our experiments) such that it is the threshold of

3

Table 1: The DeepID2 feature learning algorithm.

input: training set χ = {(xi  li)}  initialized parameters θc  θid  and θve  hyperparame-
ter λ  learning rate η(t)  t ← 0
while not converge do

sample two training samples (xi  li) and (xj  lj) from χ

t ← t + 1
fi = Conv(xi  θc) and fj = Conv(xj  θc)
∇θid = ∂Ident(fi li θid)
+ ∂Ident(fj  lj  θid)
∇θve = λ · ∂Verif(fi fj  yij  θve)
∇fi = ∂Ident(fi li θid)
∇fj = ∂Ident(fj  lj  θid)
∇θc = ∇fi · ∂Conv(xi θc)
update θid = θid − η(t) · ∇θid  θve = θve − η(t) · ∇θve  and θc = θc − η(t) · ∇θc.

+ λ · ∂Verif(fi fj  yij  θve)
+ λ · ∂Verif(fi fj  yij  θve)
+ ∇fj · ∂Conv(xj  θc)

  where yij = 1 if li = lj  and yij = −1 otherwise.

∂θid

∂θid

∂θc

∂θc

∂θve

∂fi

∂fj

∂fi

∂fj

end while
output θc

Figure 2: Patches selected for feature extraction. The Joint Bayesian [3] face veriﬁcation accuracy
(%) using features extracted from each individual patch is shown below.

the feature distances (cid:107)fi − fj(cid:107) to minimize the veriﬁcation error of the previous N training pairs.
Updating m is not included in Tab. 1 for simplicity.

3 Face Veriﬁcation

To evaluate the feature learning algorithm described in Sec. 2  DeepID2 features are embedded into
the conventional face veriﬁcation pipeline of face alignment  feature extraction  and face veriﬁcation.
We ﬁrst use the recently proposed SDM algorithm [24] to detect 21 facial landmarks. Then the face
images are globally aligned by similarity transformation according to the detected landmarks. We
cropped 400 face patches  which vary in positions  scales  color channels  and horizontal ﬂipping 
according to the globally aligned faces and the position of the facial landmarks. Accordingly 
400 DeepID2 feature vectors are extracted by a total of 200 deep ConvNets  each of which is
trained to extract two 160-dimensional DeepID2 feature vectors on one particular face patch and
its horizontally ﬂipped counterpart  respectively  of each face.
To reduce the redundancy among the large number of DeepID2 features and make our system
practical  we use the forward-backward greedy algorithm [25] to select a small number of effective
and complementary DeepID2 feature vectors (25 in our experiment)  which saves most of the feature
extraction time during test. Fig. 2 shows all the selected 25 patches  from which 25 160-dimensional
DeepID2 feature vectors are extracted and are concatenated to a 4000-dimensional DeepID2 feature
vector. The 4000-dimensional vector is further compressed to 180 dimensions by PCA for face
veriﬁcation. We learned the Joint Bayesian model [3] for face veriﬁcation based on the extracted
DeepID2 features. Joint Bayesian has been successfully used to model the joint probability of two
faces being the same or different persons [3  4].

4

4 Experiments

We report face veriﬁcation results on the LFW dataset [11]  which is the de facto standard test set
for face veriﬁcation in unconstrained conditions. It contains 13  233 face images of 5749 identities
collected from the Internet. For comparison purposes  algorithms typically report the mean face
veriﬁcation accuracy and the ROC curve on 6000 given face pairs in LFW. Though being sound
as a test set  it is inadequate for training  since the majority of identities in LFW have only one
face image. Therefore  we rely on a larger outside dataset for training  as did by all recent high-
performance face veriﬁcation algorithms [4  2  21  20  13]. In particular  we use the CelebFaces+
dataset [20] for training  which contains 202  599 face images of 10  177 identities (celebrities)
collected from the Internet. People in CelebFaces+ and LFW are mutually exclusive. DeepID2
features are learned from the face images of 8192 identities randomly sampled from CelebFaces+
(referred to as CelebFaces+A)  while the remaining face images of 1985 identities (referred to as
CelebFaces+B) are used for the following feature selection and learning the face veriﬁcation models
(Joint Bayesian). When learning DeepID2 features on CelebFaces+A  CelebFaces+B is used as
a validation set to decide the learning rate  training epochs  and hyperparameter λ. After that 
CelebFaces+B is separated into a training set of 1485 identities and a validation set of 500 identities
for feature selection. Finally  we train the Joint Bayesian model on the entire CelebFaces+B data
and test on LFW using the selected DeepID2 features. We ﬁrst evaluate various aspect of feature
learning from Sec. 4.1 to Sec. 4.3 by using a single deep ConvNet to extract DeepID2 features
from the entire face region. Then the ﬁnal system is constructed and compared with existing best
performing methods in Sec. 4.4.

4.1 Balancing the identiﬁcation and veriﬁcation signals

i=1

x∈Di

(x − ¯xi) (x − ¯xi)

(cid:80)c
i=1 ni · (¯xi − ¯x) (¯xi − ¯x)
matrix is Sintra = (cid:80)c
(cid:80)

We investigates the interactions of identiﬁcation and veriﬁcation signals on feature learning  by
varying λ from 0 to +∞. At λ = 0  the veriﬁcation signal vanishes and only the identiﬁcation signal
takes effect. When λ increases  the veriﬁcation signal gradually dominates the training process. At
the other extreme of λ → +∞  only the veriﬁcation signal remains. The L2 norm veriﬁcation loss
in Eq. (2) is used for training. Figure 3 shows the face veriﬁcation accuracy on the test set by
comparing the learned DeepID2 features with L2 norm and the Joint Bayesian model  respectively.
It clearly shows that neither the identiﬁcation nor the veriﬁcation signal is the optimal one to learn
features. Instead  effective features come from the appropriate combination of the two.
This phenomenon can be explained from the view of inter- and intra-personal variations  which
could be approximated by LDA. According to LDA  the inter-personal scatter matrix is Sinter =
(cid:62)  where ¯xi is the mean feature of the i-th identity  ¯x is the mean of the
entire dataset  and ni is the number of face images of the i-th identity. The intra-personal scatter
(cid:62)  where Di is the set of features of the i-th
identity  ¯xi is the corresponding mean  and c is the number of different identities. The inter- and
intra-personal variances are the eigenvalues of the corresponding scatter matrices  and are shown in
Fig. 5. The corresponding eigenvectors represent different variation patterns. Both the magnitude
and diversity of feature variances matter in recognition. If all the feature variances concentrate on a
small number of eigenvectors  it indicates the diversity of intra- or inter-personal variations is low.
The features are learned with λ = 0  0.05  and +∞  respectively. The feature variances of each
given λ are normalized by the corresponding mean feature variance.
When only the identiﬁcation signal is used (λ = 0)  the learned features contain both diverse
inter- and intra-personal variations  as shown by the long tails of the red curves in both ﬁgures.
While diverse inter-personal variations help to distinguish different identities  large and diverse
intra-personal variations are disturbing factors and make face veriﬁcation difﬁcult. When both the
identiﬁcation and veriﬁcation signals are used with appropriate weighting (λ = 0.05)  the diversity
of the inter-personal variations keeps unchanged while the variations in a few main directions
become even larger  as shown by the green curve in the left compared to the red one. At the
same time  the intra-personal variations decrease in both the diversity and magnitude  as shown
by the green curve in the right. Therefore  both the inter- and intra-personal variations changes in
a direction that makes face veriﬁcation easier. When λ further increases towards inﬁnity  both the
inter- and intra-personal variations collapse to the variations in only a few main directions  since
without the identiﬁcation signal  diverse features cannot be formed. With low diversity on inter-

5

Figure 3: Face veriﬁcation accuracy by varying
the weighting parameter λ. λ is plotted in log
scale.

Figure 4: Face veriﬁcation accuracy of DeepID2
features learned by both the the face identiﬁcation
and veriﬁcation signals  where the number of
training identities (shown in log scale) used for
face identiﬁcation varies. The result may be
further improved with more than 8192 identities.

Figure 5: Spectrum of eigenvalues of the inter- and intra-personal scatter matrices. Best viewed in
color.

personal variations  distinguishing different identities becomes difﬁcult. Therefore the performance
degrades signiﬁcantly.
Figure 6 shows the ﬁrst two PCA dimensions of features learned with λ = 0  0.05  and +∞ 
respectively. These features come from the six identities with the largest numbers of face images in
LFW  and are marked by different colors. The ﬁgure further veriﬁes our observations. When λ = 0
(left)  different clusters are mixed together due to the large intra-personal variations  although the
cluster centers are actually different. When λ increases to 0.05 (middle)  intra-personal variations
are signiﬁcantly reduced and the clusters become distinguishable. When λ further increases towards
inﬁnity (right)  although the intra-personal variations further decrease  the cluster centers also begin
to collapse and some clusters become signiﬁcantly overlapped (as the red  blue  and cyan clusters in
Fig. 6 right)  making it hard to distinguish again.

4.2 Rich identity information improves feature learning

We investigate how would the identity information contained in the identiﬁcation supervisory signal
inﬂuence the learned features. In particular  we experiment with an exponentially increasing number
of identities used for identiﬁcation during training from 32 to 8192  while the veriﬁcation signal is
generated from all the 8192 training identities all the time. Fig. 4 shows how the veriﬁcation
accuracies of the learned DeepID2 features (derived from the L2 norm and Joint Bayesian) vary
on the test set with the number of identities used in the identiﬁcation signal.
It shows that

6

Figure 6: The ﬁrst two PCA dimensions of DeepID2 features extracted from six identities in LFW.

Table 2: Comparison of different veriﬁcation signals.

veriﬁcation signal
L2 norm (%)
Joint Bayesian (%)

L2
94.95
95.12

L2+
94.43
94.87

L2-
86.23
92.98

L1
92.92
94.13

cosine none
86.43
87.07
93.38
92.73

identifying a large number (e.g.  8192) of identities is key to learning effective DeepID2 feature
representation. This observation is consistent with those in Sec. 4.1. The increasing number of
identities provides richer identity information and helps to form DeepID2 features with diverse inter-
personal variations  making the class centers of different identities more distinguishable.

4.3

Investigating the veriﬁcation signals

As shown in Sec. 4.1  the veriﬁcation signal with moderate intensity mainly takes the effect of
reducing the intra-personal variations. To further verify this  we compare our L2 norm veriﬁcation
signal on all the sample pairs with those only constrain either the positive or negative sample pairs 
denoted as L2+ and L2-  respectively. That is  the L2+ only decreases the distances between
DeepID2 features of the same identity  while L2- only increases the distances between DeepID2
features of different identities if they are smaller than the margin. The face veriﬁcation accuracies
of the learned DeepID2 features on the test set  measured by the L2 norm and Joint Bayesian
respectively  are shown in Table 2. It also compares with the L1 norm and cosine veriﬁcation signals 
as well as no veriﬁcation signal (none). The identiﬁcation signal is the same (classifying the 8192
identities) for all the comparisons.
DeepID2 features learned with the L2+ veriﬁcation signal are only slightly worse than those learned
with L2. In contrast  the L2- veriﬁcation signal helps little in feature learning and gives almost
the same result as no veriﬁcation signal is used. This is a strong evidence that the effect of the
veriﬁcation signal is mainly reducing the intra-personal variations. Another observation is that the
face veriﬁcation accuracy improves in general whenever the veriﬁcation signal is added in addition
to the identiﬁcation signal. However  the L2 norm is better than the other compared veriﬁcation
metrics. This may be due to that all the other constraints are weaker than L2 and less effective in
reducing the intra-personal variations. For example  the cosine similarity only constrains the angle 
but not the magnitude.

4.4 Final system and comparison with other methods

Before learning Joint Bayesian  DeepID2 features are ﬁrst projected to 180 dimensions by PCA.
After PCA  the Joint Bayesian model is trained on the entire CelebFaces+B data and tested on the
6000 given face pairs in LFW  where the log-likelihood ratio given by Joint Bayesian is compared
to a threshold optimized on the training data for face veriﬁcation. Tab. 3 shows the face veriﬁcation
accuracy with an increasing number of face patches to extract DeepID2 features  as well as the time
used to extract those DeepID2 features from each face with a single Titan GPU. We achieve 98.97%
accuracy with all the 25 selected face patches. The feature extraction process is also efﬁcient and
takes only 35 ms for each face image. The face veriﬁcation accuracy of each individual face patch
is provided in Fig. 2. The short DeepID2 signature is extremely efﬁcient for face identiﬁcation and
face image search when matching a query image with a large number of candidates.

7

Table 3: Face veriﬁcation accuracy with DeepID2 features extracted from an increasing number of
face patches.

# patches
accuracy (%)
time (ms)

1
95.43
1.7

2
97.28
3.4

4
97.75
6.1

8
98.55
11

16
98.93
23

25
98.97
35

Table 4: Accuracy comparison with the previous best results on LFW.

method
High-dim LBP [4]
TL Joint Bayesian [2]
DeepFace [21]
DeepID [20]
GaussianFace [13]
DeepID2

accuracy (%)
95.17 ± 1.13
96.33 ± 1.08
97.35 ± 0.25
97.45 ± 0.26
98.52 ± 0.66
99.15 ± 0.13

Figure 7: ROC comparison with the previous best results on LFW. Best viewed in color.

To further exploit the rich pool of DeepID2 features extracted from the large number of patches  we
repeat the feature selection algorithm for another six times  each time choosing DeepID2 features
from the patches that have not been selected by previous feature selection steps. Then we learn
the Joint Bayesian model on each of the seven groups of selected features  respectively. We fuse the
seven Joint Bayesian scores on each pair of compared faces by further learning an SVM. In this way 
we achieve an even higher 99.15% face veriﬁcation accuracy. The accuracy and ROC comparison
with previous state-of-the-art methods on LFW are shown in Tab. 4 and Fig. 7  respectively. We
achieve the best results and improve previous results with a large margin.

5 Conclusion

This paper have shown that the effect of the face identiﬁcation and veriﬁcation supervisory signals
on deep feature representation coincide with the two aspects of constructing ideal features for face
recognition  i.e.  increasing inter-personal variations and reducing intra-personal variations  and the
combination of the two supervisory signals lead to signiﬁcantly better features than either one of
them. When embedding the learned features to the traditional face veriﬁcation pipeline  we achieved
an extremely effective system with 99.15% face veriﬁcation accuracy on LFW. The arXiv report of
this paper was published in June 2014 [19].

8

References
[1] P. N. Belhumeur  J. a. P. Hespanha  and D. J. Kriegman. Eigenfaces vs. Fisherfaces:

Recognition using class speciﬁc linear projection. PAMI  19:711–720  1997.

[2] X. Cao  D. Wipf  F. Wen  G. Duan  and J. Sun. A practical transfer learning algorithm for face

[3] D. Chen  X. Cao  L. Wang  F. Wen  and J. Sun. Bayesian face revisited: A joint formulation.

veriﬁcation. In Proc. ICCV  2013.

In Proc. ECCV  2012.

[4] D. Chen  X. Cao  F. Wen  and J. Sun. Blessing of dimensionality: High-dimensional feature

and its efﬁcient compression for face veriﬁcation. In Proc. CVPR  2013.

[5] S. Chopra  R. Hadsell  and Y. LeCun. Learning a similarity metric discriminatively  with

application to face veriﬁcation. In Proc. CVPR  2005.

[6] M. Guillaumin  J. Verbeek  and C. Schmid. Is that you? Metric learning approaches for face

[7] R. Hadsell  S. Chopra  and Y. LeCun. Dimensionality reduction by learning an invariant

[8] J. Hu  J. Lu  and Y.-P. Tan. Discriminative deep metric learning for face veriﬁcation in the

identiﬁcation. In Proc. ICCV  2009.

mapping. In Proc. CVPR  2006.

wild. In Proc. CVPR  2014.

[9] C. Huang  S. Zhu  and K. Yu. Large scale strongly supervised ensemble metric learning  with

applications to face veriﬁcation and retrieval. NEC Technical Report TR115  2011.

[10] G. B. Huang  H. Lee  and E. Learned-Miller. Learning hierarchical representations for face

veriﬁcation with convolutional deep belief networks. In Proc. CVPR  2012.

[11] G. B. Huang  M. Ramesh  T. Berg  and E. Learned-Miller. Labeled Faces in the Wild: A
database for studying face recognition in unconstrained environments. Technical Report 07-
49  University of Massachusetts  Amherst  2007.

[12] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  1998.

[13] C. Lu and X. Tang. Surpassing human-level face veriﬁcation performance on LFW with

GaussianFace. Technical report  arXiv:1404.3840  2014.

[14] A. Mignon and F. Jurie. PCCA: A new approach for distance learning from sparse pairwise

constraints. In Proc. CVPR  2012.

[15] H. Mobahi  R. Collobert  and J. Weston. Deep learning from temporal coherence in video. In

[16] B. Moghaddam  T. Jebara  and A. Pentland. Bayesian face recognition. PR  33:1771–1782 

[17] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. In

[18] Y. Sun  X. Wang  and X. Tang. Hybrid deep learning for face veriﬁcation. In Proc. ICCV 

Proc. ICML  2009.

Proc. ICML  2010.

2000.

2013.

[19] Y. Sun  X. Wang  and X. Tang. Deep learning face representation by joint identiﬁcation-

veriﬁcation. Technical report  arXiv:1406.4773  2014.

[20] Y. Sun  X. Wang  and X. Tang. Deep learning face representation from predicting 10 000

classes. In Proc. CVPR  2014.

[21] Y. Taigman  M. Yang  M. Ranzato  and L. Wolf. DeepFace: Closing the gap to human-level

performance in face veriﬁcation. In Proc. CVPR  2014.

[22] X. Wang and X. Tang. Uniﬁed subspace analysis for face recognition. In Proc. ICCV  2003.
[23] X. Wang and X. Tang. A uniﬁed framework for subspace face recognition. PAMI  26:1222–

[24] X. Xiong and F. De la Torre Frade. Supervised descent method and its applications to face

1228  2004.

alignment. In Proc. CVPR  2013.

[25] T. Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations.

IEEE Trans. Inf. Theor.  57:4689–4708  2011.

[26] Z. Zhu  P. Luo  X. Wang  and X. Tang. Deep learning identity-preserving face space. In Proc.

ICCV  2013.

[27] Z. Zhu  P. Luo  X. Wang  and X. Tang. Deep learning and disentangling face representation by

multi-view perceptron. In Proc. NIPS  2014.

9

,Nils Napp
Ryan Adams
Yi Sun
Yuheng Chen
Xiaogang Wang
Xiaoou Tang