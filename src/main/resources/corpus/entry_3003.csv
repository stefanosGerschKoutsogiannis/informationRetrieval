2018,Learning Signed Determinantal Point Processes through the Principal Minor Assignment Problem,Symmetric determinantal point processes (DPP) are a class of probabilistic models that encode the random selection of items that have a repulsive behavior. They have attracted a lot of attention in machine learning  where returning diverse sets of items is sought for. Sampling and learning these symmetric DPP's is pretty well understood. In this work  we consider a new class of DPP's  which we call signed DPP's  where we break the symmetry and allow attractive behaviors. We set the ground for learning signed DPP's through a method of moments  by solving the so called principal assignment problem for a class of matrices $K$ that satisfy $K_{i j}=\pm K_{j i}$  $i\neq j$  in polynomial time.,Learning Signed Determinantal Point Processes
through the Principal Minor Assignment Problem

Victor-Emmanuel Brunel
Department of Mathematics

Massachusetts Institute of Technology

Cambridge  MA 02139
vebrunel@mit.edu

Abstract

Symmetric determinantal point processes (DPP) are a class of probabilistic models
that encode the random selection of items that have a repulsive behavior. They
have attracted a lot of attention in machine learning  where returning diverse sets
of items is sought for. Sampling and learning these symmetric DPP’s is pretty
well understood. In this work  we consider a new class of DPP’s  which we call
signed DPP’s  where we break the symmetry and allow attractive behaviors. We
set the ground for learning signed DPP’s through a method of moments  by solving
the so called principal assignment problem for a class of matrices K that satisfy

Ki j=±Kj i  i≠ j  in polynomial time.

1

Introduction

Random point processes on ﬁnite spaces are probabilistic distributions that allow to model random
selections of sets of items from a ﬁnite collection. For example  the basket of a random customer
in a store is a random subset of items selected from that store. In some contexts  random point
processes are encoded as random binary vectors  where the 1 coordinates correspond to the selected
items. A very famous subclass of random point processes  much used in statistical mechanics  is
called the Ising model  where the log-likelihood function is a quadratic polynomial in the coordinates
of the binary vector. More generally  Markov random ﬁelds encompass models of random point
processes where stochastic dependence between the coordinates of the random vector is encoded in
an undirected graph. In recent years  a different family of random point processes has attracted a lot
of attention  mainly for its computational tractability: determinantal point processes (DPP’s). DPP’s
were ﬁrst studied and used in statistical mechanics [19]. Then  following the seminal work [15] 
discrete DPP’s have been used increasingly in various applications such as recommender systems
[10  11]  document and timeline summarization [18  27]  image search [15  1] and segmentation [17] 
audio signal processing [26]  bioinformatics [5] and neuroscience [24].
A DPP on a ﬁnite space is a random subset of that space whose inclusion probabilities are determined
by the principal minors of a given matrix. More precisely  encode the ﬁnite space with labels

[N]={1  2  . . .   N}  where N is the size of the space. A DPP is a random subset Y ⊆[N] such that
P[J⊆ Y]= det(KJ)  for all ﬁxed J⊆[N]  where K is an N× N matrix with real entries  called
the kernel of the DPP  and KJ=(Ki j)i j∈J is the square submatrix of K associated with the set J.
principal minors of the matrix L= K(I− K)1  where I is the N× N identity matrix. DPP’s with

In the applications cited above  it is assumed that K is a symmetric matrix. In that case  it is shown
(e.g.  see [16]) that a sufﬁcient and necessary condition for K to be the kernel of a DPP is that all
its eigenvalues are between 0 and 1. If  in addition  1 is not an eigenvalue of K  then the DPP with
kernel K is also known as an L-ensemble  where the probability mass function is proportional to the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

symmetric kernels  which we refer to as symmetric DPP’s  model repulsive interactions: Indeed  they
imply a strong negative dependence between items  called negative association [7].
Recently  symmetric DPP’s have become popular in recommender systems  e.g.  automatized systems
that seek for good recommendations for users on online shopping websites [10]. The main idea is to
model a random basket as a DPP and learn the kernel K based on previous observations. Then  for
a new customer  predict which items are the most likely to be selected next  given his/her current

basket  by maximizing the conditional probability P[J∪{i}⊆ YJ⊆ Y] over all items i that are

not yet in the current basket J. One very attractive feature of DPP’s is that if the ﬁnal basket Y of a
random user is modeled as a DPP  the latter conditional probability is tractable and can be computed
in a polynomial time in N. However  if the kernel K is symmetric  this procedure enforces diversity
in the baskets that are modeled  because of the negative association property. However  in general 
not all items should be modeled as repelling each other. For instance  say  on a website that sells
household goods  grounded coffee and coffee ﬁlters should rather be modeled as attracting each
other  since a user who buys grounded coffee is more likely to also buy coffee ﬁlters. In this work 
we extend the class of symmetric DPP’s in order to account for possible attractive interactions  by
considering nonsymmetric kernels. In the learning prospective  this extended model poses a question:
How to estimate the kernel  based on past observations? In the case of symmetric kernels  this
problem has been tackled in several works [12  1  20  4  9  10  11  21  8  25]. Here  we assume that
K is nonparametric  i.e.  it is not parametrized by a low dimensional parameter. As explained in
[8] in the symmetric case  the maximum likelihood approach requires to solve a highly non convex
optimization problem  and even though some algorithms have been proposed such as ﬁxed point
algorithms [21]  Expectation-Maximisation [12]  MCMC [1]  neither computational nor statistical
guarantees are given. The method of moments proposed in [25] provides a polynomial time algorithm
based on the estimation of a small number of principal minors of K  and ﬁnding a symmetric matrix
ˆK whose principal minors approximately match the estimated ones. This algorithm is closely related
to the principal minor assignment problem. Here  we are interested in learning a nonsymmetric
kernel given available estimates of its principal minors; In order to simplify the exposition  we always
assume that the available list of principal minors is exact  not approximate.
In Section 2  we recall the deﬁnition of DPP’s  we deﬁne a new class of nonsymmetric kernels  that
we call signed kernels and we characterize the set of admissible kernels under lack of symmetry. We
pose the questions of identiﬁability of the kernel of a signed DPP and show that this question  together
with the problem of learning the kernel  are related to the principal minor assignment problem. In
Section 3  we propose a solution to the principal minor assignment problem for signed kernels  which
yields a polynomial time learning algorithm for the kernel of a signed DPP.

2 Determinantal Point Processes

2.1 Deﬁnitions

following holds:

Deﬁnition 1 (Discrete Determinantal Point Process). A Determinantal Point Process (DPP) on the

P[J⊆ Y]= det(KJ)  ∀J⊆[N] 

(1)
where KJ is the submatrix of K obtained by keeping the columns and rows of K whose indices are

ﬁnite set[N] is a random subset Y ⊆[N] for which there exists a matrix K∈ RN×N such that the
in J. The matrix K is called the kernel of the DPP  and we write Y ∼ DPP(K).
Note that not all matrices K∈ RN×N give rise to a DPP since  for instance  the numbers det(KJ)
from (1) must all lie in[0  1]  and be nonincreasing with the set J. We call a matrix K∈ RN×N
following proposition where  for all J ⊆[N]  we denote by IJ the diagonal matrix whose j-th
diagonal entry is 1 if j∈ J  0 otherwise.
Proposition 1. A matrix K∈ RN×N is admissible if and only if(−1)J det(K− IJ)≥ 0  for all
J⊆[N].
Proof. By [16]  if Y ∼ DPP(K)  then  necessarily  0≤ P[Y = J]=(−1)N−J det(K− I ¯J) for
all J ⊆ [N]. Conversely  assume(−1)J det(K− IJ) ≥ 0 for all J ⊆ [N]. Denote by pJ =

In short  the inclusion probabilities of a DPP are given by the principal minors of some matrix K.

admissible if there exists a DPP with kernel K. As a simple consequence of [16]  we have the

2

admissible if and only if L is a P0-matrix  i.e.  all its principal minors are nonnegative. If  in addition 
K is invertible  then it is admissible if and only if L is a P -matrix  i.e.  all its principal minors are

J⊆[N] pJ = 1. Hence  one can
(−1) ¯J det(K− I ¯J)  for all J ⊆[N]. By a standard computation  Q
deﬁne a random subset Y ⊆ [N] with P[Y = J] = pJ for all J ⊆ [N]. A simple application
of the inclusion-exclusion principle yields that P[J ⊆ Y] = det(KJ) for all J ⊆ [N]  hence 
Y ∼ DPP(K).
Let K∈ RN×N . Assume that I− K is invertible and let L= K(I− K)−1. Then  I+ L=(I− K)−1
is invertible and by [16]  det(LJ)~ det(I+ L)=(−1) ¯J det(K− I ¯J) for all J⊆[N]. Hence  K is
positive  if and only if T K+(I− T)(I− K) is invertible for all diagonal matrices T with entries in
[0  1] (see [14  Theorem 3.3]). Hence  it is easy to see that any matrix K of the form D+ µA  where
D is a diagonal matrix with Di i∈[λ  1− λ]  i= 1  . . .   N  for some λ∈(0  1~2)  A∈[−1  1]N×N
and 0≤ µ< λ~(N− 1)  is admissible.
case  it is well known ([16]) that admissibility is equivalent to lie in the intersectionS of two copies of
the cone of positive semideﬁnite matrices: K 0 and I− K 0. Such processes possess a very strong
property of negative dependence: negative association. A simple observation is that if Y ∼ DPP(K)
for some symmetric K∈S  then cov(1i∈Y   1j∈Y)=−K 2
i j≤ 0  for all i  j∈[N]  i≠ j. Moreover  if
J  J′ are two disjoint subsets of[N]  then cov(1J⊆Y   1J′⊆Y)= det(KJ∪J′)− det(KJ) det(K′
J)≤
0. Negative association is the property that  more generally  cov(f(Y ∩ J)  g(Y ∩ J))≤ 0 for
all disjoint subsets J  J′ ⊆ [N] and for all nondecreasing functions f  g ∶ P([N]) → R (i.e. 
f(J1)≤ f(J2) ∀J1⊆ J2⊆[N])  whereP([N]) is the power set of[N]. We refer to [6] for more

Symmetric DPP’s Most commonly  DPP’s are deﬁned with a real symmetric kernel K. In that

details on the account of negative association. For their computational appeal  it is very tempting
to apply DPP’s in order to model interactions  e.g.  as an alternative to Ising models. However  the
negative association property of DPP’s with symmetric kernels is unreasonably restrictive in several
contexts  for it forces repulsive interactions between items. Next  we extend the class of DPP’s with
symmetric kernels in a simple way which is yet also allowing for attractive interactions.

Signed DPP’s We introduce the classT of signed kernels  i.e.  matrices K∈ RN×N such that for
all i  j∈[N] with i≠ j  Kj i=±Ki j  i.e.  Kj i= i jKi j for some i j∈{−1  1}. We call a signed
DPP any DPP with kernel K∈T . As of particular interest  one can also consider signed block DPP’s 
with kernels K∈T   where there is a partition of[N] into pairwise disjoint  nonempty groups such
that Kj i=−Ki j if i and j are in the same group (hence  i and j attract each other)  Kj i= Ki j if i

and j are in different groups (hence  i and j repel each other).

2.2 Learning DPP’s

The main purpose of this work is to understand how to learn the kernel of a nonsymmetric DPP 
given i.i.d. copies of that DPP. Namely  if Y1  . . .   Yn
to estimate K from the observation of Y1  . . .   Yn? First comes the question of identiﬁability of K:

i.i.d.∼ DPP(K) for some unknown K∈T   how
two matrices K  K′∈T can give rise to the same DPP. To be more speciﬁc  DPP(K)= DPP(K′)
if and only if K and K′ have the same list of principal minors. Hence  the kernel of a DPP is not

necessarily unique. It is actually easy to see that it is unique if and only if it is diagonal. A ﬁrst
natural question that arises in learning the kernel of a DPP is the following:

“What is the collection of all matrices K∈T that produce a given DPP?"

Given that the kernel of Y1 is not uniquely deﬁned  the goal is no longer to estimate K exactly  but
one possible kernel that would give rise to the same DPP as K. The route that we follow is similar to
that followed by [25]  which is based on a method of moments. However  lack of symmetry of K
requires signiﬁcantly different ideas. The idea is based on the fact that only few principal minors
of K are necessary in order to completely recover K up to identiﬁability. Moreover  each principal

minor ∆J∶= det(KJ) can be estimated from the samples by ˆ∆J= n−1∑n
i=1

1J⊆Yi. Since this last

step is straightforward  we only focus on the problem of complete recovery of K  up to identiﬁability 
given a list of few of its principal minors. In other words  we will ask the following question:

3

“Given an available list of prescribed principal minors  how to recover a matrix K∈T whose

principal minors are given by that list  using as few queries from that list as possible?"

This question  together with the one we asked for identiﬁability  is known as the principal minor
assignment problem  which we state precisely in the next section.

two questions:

2.3 The principal minor assignment problem

The principal minor assignment problem (PMA) is a well known problem in linear algebra that

consists of ﬁnding a matrix with a prescribed list of principal minors [23]. LetH⊆ CN×N be a
collection of matrices. Typically H is the set of Hermitian matrices  or real symmetric matrices or  in
this work H=T . Given a list(aJ)J⊆[N] J≠࢝ of 2N− 1 complex numbers  (PMA) asks the following
(PMA1) Find a matrix K∈H such that det(KJ)= aJ ∀J⊆[N]  J≠࢝.
solution exists  i.e.  the list(aJ)J⊆[N] J≠࢝ is a valid list of prescribed principal minors  and we aim

A third question  which we do not address here  is to decide whether (PMA1) has a solution. It is
known that this would require the aJ’s to satisfy polynomial equations [22]. Here  we assume that a

(PMA2) Describe the set of all solutions of (PMA1).

to answer (PMA1) efﬁciently  i.e.  output a solution in polynomial time in the size N of the problem 
and to answer (PMA2) at a purely theoretical level. In the framework of DPP’s  (PMA1) is related to
the problem of estimating K by a method of moments and (PMA2) concerns the identiﬁability of K.

3 Solving the principal minor assignment problem for nonsymmetric DPP’s

3.1 Preliminaries: PMA for symmetric matrices

Here  we brieﬂy describe the PMA problem for symmetric matrices  i.e. H=S  the set of real
symmetric N× N matrices. This will give some intuition for the next section.
The adjacency graph GK =([N]  EK) of a matrix a matrix K∈S is the undirected graph on N
vertices  where  for all i  j∈[N] {i  j}∈ EK ⇐⇒ Ki j≠ 0. As a consequence of Fact 1  we have:

Fact 1. The principal minors of order one and two of a symmetric matrix completely determine its
diagonal entries and the magnitudes of its off diagonal entries.

Fact 2. The adjacency graph of any symmetric solution of (PMA1) can be learned by querying the
principal minors of order one and two. Moreover  any two symmetric solutions of (PMA1) have the
same adjacency graph.

Then  the signs of the off diagonal entries of a symmetric solution of (PMA1) should be determined
using queries of higher order principal minors  and the idea is based on the next fact. For a matrix

K∈S and a cycle C in GK  denote by πK(C) the product of entries of K along the cycle C  i.e. 
πK(C)= M{i j}∈C∶i<j
Fact 3. For all matrices K∈S and all J⊆[N]  det(KJ) only depends on the diagonal entries of
KJ  the magnitude of its off diagonal entries and the πK(C)  for all cycles C in the subgraph of GK
where all vertices j∉ J have been deleted.

Ki j.

det(KJ)= Q
σ∈SJ

(−1)σM
j∈J

Kj σ(j) 

Fact 3 is a simple consequence of the fundamental formula:

as a product of cyclic permutations. Finally  every undirected graph has a cycle basis made of induced

where SJ is the group of permutations of J  Moreover  every permutation σ∈ S can be decomposed
cycles  i.e.  there is a small familyB of induced cycles such that every cycle (seen as a collection
of edges) in the graph can be decomposed as the symmetric difference of cycles that belong toB.
Then  it is easy to see that for all cycles C in the graph GK  πK(C) can be written as the product
of some πK( ˜C)  for some cycles ˜C∈B and of some K 2
i j’s  i≠ j. Moreover  for all induced cycles

(2)

4

i j be the product of the i j’s along

det(K{i j})= Ki iKj j− i jK 2

i j.

Fact 2  GK can be learned  what remains is to ﬁnd a cycle basis of GK  made of induced cycles only 
which can be performed in polynomial time (see [13  2]) and  for each cycle C in that basis  query the

C in the aforementioned basis. Finding such a sign assignment consists of solving a linear system in
GF2 (see Section 1 in the Supplementary Material).

C in GK  πK(C) can be determined from det(KJ)  where J is the set of vertices of C. Since  by
corresponding principal minor of K in order to learn πK(C). Finally  in order to determine the signs
of the off diagonal entries of K  ﬁnd a sign assignment that matches with the signs of the πK(C)  for
3.2 PMA whenH=T   general case
We now turn to the caseH=T . First  as in the symmetric case  the diagonal entries of any matrix
K∈T are given by its principal minors of order 1. Now  let i< j and consider the principal minor of
K corresponding to J={i  j}:
Hence Ki j and i j can be learned from the principal minors of K corresponding to the sets
{i} {j} and{i  j}.
Note that if K ∈T   one can still deﬁne its adjacency graph GK as in the symmetric case  since
Ki j≠ 0 ⇐⇒ Kj i≠ 0  for all i≠ j. Recall that we identify a cycle of a graph with its edge set. For
all K∈T and for all cycles C in GK  let K(C)= M{i j}∈C∶i<j
the edges of C  where i j∈{−1  1} is such that Ki j= i jKj i. Note that the condition “i< j" in
the deﬁnition of K(C) is only to ensure no repetition in the product. Now  unlike in the symmetric
case  we need to be more careful when deﬁning πK(C)  for a cycle C of GK  since the direction in
Deﬁnition 2. A signed graph is an undirected graph([N]  E) where each edge is assigned a sign
−1 or+1.
In the sequel  we make the adjacency graph GK of any matrix K∈T signed by assigning i j to each
edge{i  j} of the graph. As we noticed above  the signed adjacency graph of K can be learned from
of G whose vertex set coincides with that of C. The set of travelings of C is denoted by T(C).
In Figure 1  the cycle C= 1↔ 2↔ 3↔ 4↔ 1 has six travelings:
—→
—→
C3= 1→ 2→
C1= 1→ 2→ 3→ 4→ 1 
—→
C5= 1→ 4→ 2→ 3→ 1 and
4→ 3→ 1 
—→
C6= 1→ 3→ 2→ 4→ 1.
Formally  while we identify a cycle with its edge set (e.g.  C =
{{1  2} {2  3} {3  4} {1  4}}  we identify its travelings with sets
—→
C1=
{(1  2) (2  3) (3  4) (4  1)}). Also  for simplicity  we always de-
note oriented cycles using the symbol—→⋅
Deﬁnition 4. Let K∈T and C be a cycle in GK. We denote by πK(C)= Q—→

its principal minors of orders one and two. Unlike in the symmetric case  induced cycles might be of
no help to determine the signs of the off diagonal entries of K.

For instance  an induced cycle has exactly two travelings  corre-
sponding to the two possible orientations of C.

—→
C2= 1→ 4→ 3→ 2→ 1 

—→
C4= 1→ 3→ 4→ 2→ 1 

Deﬁnition 3. Let G be an undirected graph and C a cycle of G. A traveling of C is an oriented cycle

of ordered pairs corresponding to their oriented edges (e.g. 

(e.g. 

C as opposed to C  which would stand for an

which C is traveled matters.

unoriented cycle).

Figure 1:

A signed graph

C∈T(C) M(i j)∈—→

C

Ki j.

—→

5

where the oriented cycles

C3) K1 2K2 4K4 3K3 1

For example  if the graph in Figure 1 is the adjacency graph of some K ∈T and C is the cycle
C= 1↔ 2↔ 3↔ 4↔ 1  then 
πK(C)= K1 2K2 3K3 4K4 1+ K1 4K4 3K3 2K2 1+ K1 2K2 4K4 3K3 1+ K1 3K3 4K4 2K2 1
+ K1 4K4 2K2 3K3 1+ K1 3K3 2K2 4K4 1
=1+ K(—→
C1) K1 2K2 3K3 4K4 1+1+ K(—→
+1+ K(—→
C5) K1 4K4 2K2 3K3 1
= 2K1 3K3 2K2 4K4 1.
C5 are given above  and where we use the shortcut K(—→
—→
—→
—→
Cj)
(j= 1  3  5) to denote K(Cj)  where Cj is the unoriented version of
In the same example  there are only two triangles T (i.e.  cycles of size 3) that satisfy πK(T)≠ 0:
1↔ 3↔ 4↔ 1 and 2↔ 3↔ 4↔ 2.
Lemma 1. For all J ⊆ [N]  det(KJ) can be written as a function of the Ki i  K 2
i  j∈ J  i≠ j and πK(C)’s  for all cycles C in GKJ   the subgraph of GK where all vertices j∉ J
Proof. Write a permutation σ∈ SJ as a product of cyclic permutations σ= σ1○σ2○. . .○σp. For each
—→
j= 1  . . .   p  assume that σj correspond to an oriented cycle
p where  for all j= 1  . . .   p  σ′
1  . . .   σ′
can be decomposed as a product of p cyclic permutations σ′

Cj of GK  otherwise the contribution of
σ to the sum (2) is zero. Then  the lemma follows by grouping all permutations in the sum (2) that
j

The following result  yet a simple consequence of (2)  is fundamental.

i j  i j’s  for

are removed.

has the same support as σj.

C1 

C3 and

—→

Cj.

As a consequence  we note that unlike in the symmetric case  the signs of the off diagonal entries
can no longer be determined using a cycle basis of induced cycles  since such a basis may contain
only cycles which have no contribution to the principal minors of K. In the same example as above 
the only induced cycles of GK are triangles  and any cycle basis should contain at least three cycles.
However  there are only four triangles in that graph and two of them have a zero contribution to
the principal minors of K. Hence  in that case  it is necessary to query principal minors that do not
correspond to induced cycles in order to ﬁnd a solution to (PMA1).
In order to summarize  we state the following theorem.

Theorem 1. Let H  K∈T . The following statements are equivalent.

• H and K have the same list of principal minors.

• Hi i= Ki i andHi j=Ki j  for all i  j∈[N] with i≠ j  H and K have the same signed
adjacency graph and  for all cycles C in that graph  πK(C)= πH(C).

case)  is an open problem. However  in the next section  we reﬁne this result for a smaller class of
nonsymmetric kernels.

Theorem 1 does not provide any insight on how to solve (PMA2) efﬁciently  since the number of
cycles in a graph can be exponentially large in the size of the graph. A reﬁnement of this theorem 
where we would characterize a minimal set of cycles  that could be found efﬁciently and that would

characterize the principal minors of K ∈T (such as a basis of induced cycles  in the symmetric
3.3 PMA whenH=T   dense case
In this section  we only consider matrices K∈T such that for all i  j∈[N] with i≠ j  Ki j≠ 0. The
We also assume that for all pairwise distinct i  j  k  l∈[N] and all η1  η2  η3∈{−1  0  1} 
η1Ki jKj kKk lKl i+ η2Ki jKj lKl kKk i+ η3Ki kKk jKj lKl i= 0⇒ η1= η2= η3= 0.

(3)
Note that Condition (3) only depends on the magnitudes of the entries of K. Hence  if one solution of
(PMA1) satisﬁes (3)  then all the solutions must satisfy it too. Condition (3) is not a strong condition:
Indeed  any generic matrix with rank at least 4 is very likely to satisfy it.

adjacency graph of such a matrix is a signed version of the complete graph  which we denote by GN .

6

For the sake of simplicity  we restate (PMA1) and (PMA2) in the following way. Let K∈T be a
(PMA’1) Find a matrix H∈T such that det(HJ)= det(KJ) ∀J⊆[N]  J≠࢝.

ground kernel satisfying the two conditions above (i.e.  K is dense and satisﬁes Condition 3)  and
assume that K is unknown  but its principal minors are available.

(PMA’2) Describe the set of all solutions of (PMA’1).

with vertex set J.
The main result of this section is stated in the following theorem.

Moreover  recall that we would like to ﬁnd a solution to (PMA’1) that uses few queries from
the available list of principal minors of K  in order to design an algorithm that is not too costly
computationally.

Since K is assumed to be dense  every subset J⊆[N] of size at least 3 is the vertex set of a cycle.
Moreover  for all cycles C of GN   πK(C) only depends on the vertex set of C  not its edge set.
Therefore  in the sequel  for the ease of notation  we denote by πK(J)= πK(C) for any cycle C
Theorem 2. A matrix H ∈ T is a solution of (PMA’1) if and only if it satisﬁes the following
= Ki j

• Hi i= Ki i andHi j=Ki j  for all i  j∈[N] with i≠ j;
• H has the same signed adjacency graph as K  i.e.  GH= GK= GN and
all i≠ j;
• πH(J)= πK(J)  for all J⊆[N] of size 3 or 4.

requirements:

Hi j
Hj i

  for

Kj i

det(HJ)= det(KJ) 

Proof sketch Here  we only give a sketch of the proof of Theorem 2. All the details of the proof
can be found in the Supplementary Material.
The left to right implication follows directly from Theorem 1  which was a consequence of the whole
discussion in Section 3.2. Now  let H satisfy the four requirements; We want to prove that

Let us introduce some new notation for the rest of this proof sketch. For all oriented cycles

C
least 3. In the sequel  for each unoriented cycle C with vertex set J  let

for all J⊆[N]. If J has size 1 or 2  (4) is straightforward  by the ﬁrst three requirements. If J has
size 3 or 4  it is easy to see that det(HJ) only depends on Hi i  H 2
i j  i  j∈ J and πH(S)  S⊆ J 
hence  (4) is also granted. Now  let J⊆[N] have size at least 5. By Lemma 1  it is enough to check
πH(S)= πK(S) 
for all S⊆ J of size at least 3.
—→
Ki j and—→π H(—→
GN   we denote by—→π K(—→
C)=∏(i j)∈—→
Hi j. Let J⊆[N] of size at
C)=∏(i j)∈—→
—→
orientations of C  chosen arbitrarily. Denote by T+(J) the set of unoriented cycles C with vertex set
J  such that K(C)=+1. It is clear that
—→π H(—→
πH(J)= 2 Q
C) 
C∈T+(J)
and the same holds for K. Now  letJ+={(i  j  k)⊆[N]∶ i≠ j  i≠ k  j≠ k  i jj ki k=+1} be
the principal minors of K. The requirements on H ensure that Hi jHj kHi k= Ki jKj kKi k for all
C)=—→π K(—→
(i  j  k)∈J+ and  by Condition (3)  using (6)  that—→π H(—→
C)  for all cycles C of length 4
with K(C)= 1 (where  we recall that C is the unoriented version of the oriented cycle
C)=—→π K(—→
Let p be the size of S. By (6)  it is enough to check that—→π H(—→
C) for all positive oriented
—→
C of length p with K(C)=+1. Let us prove

the set of positive triangles  i.e.  the set of triples that deﬁne triangles in GN that do contribute to

C of length p  i.e.  for all oriented cycles

C be any of the two possible

—→

C ).

—→

cycles

(4)

(5)

C in

(6)

that

C

7

Algorithm 1 Find a solution H to (PMA’1)

C)  for all

—→
C ∈ T+(S)

Find an sign assignment for the off diagonal entries of H that matches all the signs found in the
previous step  by Gaussian elimination in GF2.

Input: List{aJ∶ J⊆[N]}.
Set Hi i= a{i} for all i= 1  . . .   N.
SetHi j=a{i}a{j}− a{i j} for all i≠ j.
Set i j= signa{i}a{j}− a{i j} for all i≠ j.
Find the setJ+ of all triples(i  j  j) of pairwise distinct indices such that i ji kj k= 1 and ﬁnd
the sign of Hi jHj kHi k for all(i  j  k)∈J+  using aJ   J⊆ i  j  k.
For all S⊆[N] of size 4  ﬁnd πH(S) and deduce the sign of—→π K(—→
this statement by induction on p. If p= 3 or 4  (5) is granted by the requirement imposed on H.
Let p= 5. Let
—→
C = 1→ 2→ 3→ 4→ 5→ 1. Since it is positive  it can have either 0  2 or 4 negative edges.
Suppose it has 0 negative edges  i.e.  all its edges are positive (i.e.  satisfy i j =+1). We call a
since GH = GK is the complete graph  all cycles have chords. If C has a positive chord  i.e.  if
there are two vertices i≠ j with j≠ i± 1 (mod5) and i j =+1  then C can be decomposed as
C)=—→π H(—→
—→π H(—→
C). If C has no positive chord  then
—→π H(—→
C)=—→π K(—→
If p≥ 6  a similar argument is employed: By distinguishing several cases  one can show that C can

C1)—→π H(—→
C1)—→π K(—→
C2)
C2)
C). A similar argument is used when

the symmetric difference of two positive cycles C1 and C2  one of length 3  one of length 4  with

chord of the cycle C any edge between two vertices of C  that is not an edge of C. Recall that

we show that it can be decomposed as the symmetric difference of three positive cycles  also yielding

=—→π K(—→

H 2
i j

K 2
i j

—→

that

C be a positive oriented cycle of length 5. Without loss of generality  let us assume

=—→π K(—→
—→

C has 2 or 4 negative edges.

always be decomposed as the symmetric difference of smaller positive cycles and use induction. (cid:3)
Finally  we provide an algorithm that ﬁnds a solution to (PMA’1) in polynomial time.
Theorem 3. Algorithm 1 ﬁnds a solution of (PMA’1) in polynomial time in N.

Proof. The fact that Algorithm 1 ﬁnds a solution of (PMA’1) is a straightforward consequence of
Theorem 2. Its complexity is of the order of that of Gaussian elimination for a linear system of

at most O(N 4) equations  corresponding to cycles of size at most 4 and with O(N 2) variables 

corresponding to the entries of H.

4 Conclusions

We have introduced signed DPP’s  which allow for both repulsive and attractive interactions. By
solving the PMA problem  we have characterized identiﬁcation of the kernel in the dense case

(Theorem 2) and we have given an algorithm that ﬁnds a dense matrix H ∈ T with prescribed

principal minors  in polynomial time in the size N of the unknown matrix. In practice  these principal
minors are unknown  but they can be estimated from observed samples from a DPP. As long as
the adjacency graph can be recovered exactly from the samples  which would be granted with high
probability for a large number of observations  and if all entries of H are bounded away from zero by
some known constant (that depends on N)  solving the PMA problem amounts in ﬁnding the signs
of the entries of H  up to identiﬁability  which can also be done exactly with high probability  if
the number of observed samples is large (see  e.g.  [25]). However  extending classical symmetric
DPP’s to non symmetric kernels poses some questions: We do not know how to sample a signed
DPP efﬁciently  since the strongly Rayleigh property is no longer valid (see [3]) and the role of
the eigenvalues of the kernel is not clear (in the symmetric case  a spectral decomposition of the
kernel can be used for sampling  see [16])  even though Lemma 1 in the Supplementary Material  for
instance  shows that they still determine the distribution of the size of the DPP.

8

References
[1] Raja Haﬁz Affandi  Emily B. Fox  Ryan P. Adams  and Benjamin Taskar. Learning the
parameters of determinantal point process kernels. In Proceedings of the 31th International
Conference on Machine Learning  ICML 2014  Beijing  China  21-26 June 2014  pages 1224–
1232  2014.

[2] Edoardo Amaldi  Claudio Iuliano  and Romeo Rizzi. Efﬁcient deterministic algorithms for
ﬁnding a minimum cycle basis in undirected graphs. In International Conference on Integer
Programming and Combinatorial Optimization  pages 397–410. Springer  2010.

[3] Nima Anari  Shayan Oveis Gharan  and Alireza Rezaei. Monte carlo markov chain algorithms
for sampling strongly rayleigh distributions and determinantal point processes. In Conference
on Learning Theory  pages 103–115  2016.

[4] Rémi Bardenet and Michalis Titsias.

Inference for determinantal point processes without
spectral knowledge. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett 
editors  Advances in Neural Information Processing Systems 28  pages 3393–3401. Curran
Associates  Inc.  2015.

[5] Nematollah Kayhan Batmanghelich  Gerald Quon  Alex Kulesza  Manolis Kellis  Polina Gol-
land  and Luke Bornn. Diversifying sparsity using variational determinantal point processes.
CoRR  abs/1411.6307  2014.

[6] Julius Borcea  Petter Brändén  and Thomas Liggett. Negative dependence and the geometry of

polynomials. Journal of the American Mathematical Society  22(2):521–567  2009.

[7] Julius Borcea  Petter Brändén  and Thomas M. Liggett. Negative dependence and the geometry

of polynomials. J. Amer. Math. Soc.  22(2):521–567  2009.

[8] Victor-Emmanuel Brunel  Ankur Moitra  Philippe Rigollet  and John Urschel. Rates of estima-

tion for determinantal point processes. In Conference On Learning Theory  2017.

[9] Christophe Dupuy and Francis Bach. Learning determinantal point processes in sublinear time.

arXiv:1610.05925  2016.

[10] Mike Gartrell  Ulrich Paquet  and Noam Koenigstein. Bayesian low-rank determinantal point
processes. In Proceedings of the 10th ACM Conference on Recommender Systems  RecSys ’16 
pages 349–356  New York  NY  USA  2016. ACM.

[11] Mike Gartrell  Ulrich Paquet  and Noam Koenigstein. Low-rank factorization of determinantal

point processes for recommendation. arXiv:1602.05436  2016.

[12] Jennifer Gillenwater  Alex Kulesza  Emily Fox  and Ben Taskar. Expectation-maximization for
learning determinantal point processes. In Proceedings of the 27th International Conference on
Neural Information Processing Systems  NIPS’14  pages 3149–3157  Cambridge  MA  USA 
2014. MIT Press.

[13] Joseph Douglas Horton. A polynomial-time algorithm to ﬁnd the shortest cycle basis of a graph.

SIAM Journal on Computing  16(2):358–366  1987.

[14] Charles R Johnson and Michael J Tsatsomeros. Convex sets of nonsingular and p–matrices.

Linear and Multilinear Algebra  38(3):233–239  1995.

[15] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-size determinantal point processes. In Proceedings
of the 28th International Conference on Machine Learning  ICML 2011  Bellevue  Washington 
USA  June 28 - July 2  2011  pages 1193–1200  2011.

[16] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now

Publishers Inc.  Hanover  MA  USA  2012.

[17] Donghoon Lee  Geonho Cha  Ming-Hsuan Yang  and Songhwai Oh.

Individualness and
determinantal point processes for pedestrian detection. In Computer Vision - ECCV 2016 - 14th
European Conference  Amsterdam  The Netherlands  October 11-14  2016  Proceedings  Part
VI  pages 330–346  2016.

9

[18] Hui Lin and Jeff A. Bilmes. Learning mixtures of submodular shells with application to
document summarization. In Proceedings of the Twenty-Eighth Conference on Uncertainty in
Artiﬁcial Intelligence  Catalina Island  CA  USA  August 14-18  2012  pages 479–490  2012.

[19] Odile Macchi. The coincidence approach to stochastic point processes. Advances in Appl.

Probability  7:83–122  1975.

[20] Zelda Mariet and Suvrit Sra. Fixed-point algorithms for learning determinantal point processes.
In Proceedings of the 32nd International Conference on Machine Learning (ICML-15)  pages
2389–2397  2015.

[21] Zelda E. Mariet and Suvrit Sra. Kronecker determinantal point processes.

In D. D. Lee 
M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information
Processing Systems 29  pages 2694–2702. Curran Associates  Inc.  2016.

[22] Luke Oeding. Set-theoretic deﬁning equations of the variety of principal minors of symmetric

matrices. Algebra Number Theory  5(1):75–109  2011.

[23] Justin Rising  Alex Kulesza  and Ben Taskar. An efﬁcient algorithm for the symmetric principal

minor assignment problem. Linear Algebra and its Applications  473:126 – 144  2015.

[24] Jasper Snoek  Richard S. Zemel  and Ryan Prescott Adams. A determinantal point process
latent variable model for inhibition in neural spiking data. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems
2013. Proceedings of a meeting held December 5-8  2013  Lake Tahoe  Nevada  United States. 
pages 1932–1940  2013.

[25] John Urschel  Victor-Emmanuel Brunel  Ankur Moitra  and Philippe Rigollet. Learning deter-

minantal point processes with moments and cycles. In ICML  2017.

[26] Haotian Xu and Haotian Ou. Scalable discovery of audio ﬁngerprint motifs in broadcast streams
with determinantal point process based motif clustering. IEEE/ACM Trans. Audio  Speech &
Language Processing  24(5):978–989  2016.

[27] Jin-ge Yao  Feifan Fan  Wayne Xin Zhao  Xiaojun Wan  Edward Y. Chang  and Jianguo Xiao.
Tweet timeline generation with determinantal point processes. In Proceedings of the Thirtieth
AAAI Conference on Artiﬁcial Intelligence  February 12-17  2016  Phoenix  Arizona  USA. 
pages 3080–3086  2016.

10

,Victor-Emmanuel Brunel