2016,Understanding the Effective Receptive Field in Deep Convolutional Neural Networks,We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks  as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field size  and show that it both has a Gaussian distribution and   only occupies a fraction of the full theoretical receptive field size. We analyze the effective receptive field in several architecture designs  and the effect of sub-sampling  skip connections  dropout and nonlinear activations on it. This leads to suggestions for ways to address its tendency to be too small.,Understanding the Effective Receptive Field in

Deep Convolutional Neural Networks

Wenjie Luo∗

Yujia Li∗
Department of Computer Science

Raquel Urtasun

{wenjie  yujiali  urtasun  zemel}@cs.toronto.edu

University of Toronto

Richard Zemel

Abstract

We study characteristics of receptive ﬁelds of units in deep convolutional networks.
The receptive ﬁeld size is a crucial issue in many visual tasks  as the output must
respond to large enough areas in the image to capture information about large
objects. We introduce the notion of an effective receptive ﬁeld  and show that it
both has a Gaussian distribution and only occupies a fraction of the full theoretical
receptive ﬁeld. We analyze the effective receptive ﬁeld in several architecture
designs  and the effect of nonlinear activations  dropout  sub-sampling and skip
connections on it. This leads to suggestions for ways to address its tendency to be
too small.

1

Introduction

Deep convolutional neural networks (CNNs) have achieved great success in a wide range of problems
in the last few years. In this paper we focus on their application to computer vision: where they are
the driving force behind the signiﬁcant improvement of the state-of-the-art for many tasks recently 
including image recognition [10  8]  object detection [17  2]  semantic segmentation [12  1]  image
captioning [20]  and many more.
One of the basic concepts in deep CNNs is the receptive ﬁeld  or ﬁeld of view  of a unit in a certain
layer in the network. Unlike in fully connected networks  where the value of each unit depends on the
entire input to the network  a unit in convolutional networks only depends on a region of the input.
This region in the input is the receptive ﬁeld for that unit.
The concept of receptive ﬁeld is important for understanding and diagnosing how deep CNNs work.
Since anywhere in an input image outside the receptive ﬁeld of a unit does not affect the value of that
unit  it is necessary to carefully control the receptive ﬁeld  to ensure that it covers the entire relevant
image region. In many tasks  especially dense prediction tasks like semantic image segmentation 
stereo and optical ﬂow estimation  where we make a prediction for each single pixel in the input image 
it is critical for each output pixel to have a big receptive ﬁeld  such that no important information is
left out when making the prediction.
The receptive ﬁeld size of a unit can be increased in a number of ways. One option is to stack more
layers to make the network deeper  which increases the receptive ﬁeld size linearly by theory  as
each extra layer increases the receptive ﬁeld size by the kernel size. Sub-sampling on the other hand
increases the receptive ﬁeld size multiplicatively. Modern deep CNN architectures like the VGG
networks [18] and Residual Networks [8  6] use a combination of these techniques.
In this paper  we carefully study the receptive ﬁeld of deep CNNs  focusing on problems in which
there are many output unites. In particular  we discover that not all pixels in a receptive ﬁeld contribute
equally to an output unit’s response. Intuitively it is easy to see that pixels at the center of a receptive

∗denotes equal contribution

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

ﬁeld have a much larger impact on an output. In the forward pass  central pixels can propagate
information to the output through many different paths  while the pixels in the outer area of the
receptive ﬁeld have very few paths to propagate its impact. In the backward pass  gradients from an
output unit are propagated across all the paths  and therefore the central pixels have a much larger
magnitude for the gradient from that output.
This observation leads us to study further the distribution of impact within a receptive ﬁeld on the
output. Surprisingly  we can prove that in many cases the distribution of impact in a receptive ﬁeld
distributes as a Gaussian. Note that in earlier work [20] this Gaussian assumption about a receptive
ﬁeld is used without justiﬁcation. This result further leads to some intriguing ﬁndings  in particular
that the effective area in the receptive ﬁeld  which we call the effective receptive ﬁeld  only occupies a
fraction of the theoretical receptive ﬁeld  since Gaussian distributions generally decay quickly from
the center.
The theory we develop for effective receptive ﬁeld also correlates well with some empirical observa-
tions. One such empirical observation is that the currently commonly used random initializations
lead some deep CNNs to start with a small effective receptive ﬁeld  which then grows during training.
This potentially indicates a bad initialization bias.
Below we present the theory in Section 2 and some empirical observations in Section 3  which aim
at understanding the effective receptive ﬁeld for deep CNNs. We discuss a few potential ways to
increase the effective receptive ﬁeld size in Section 4.

2 Properties of Effective Receptive Fields

i j  with x0

i j as the input to the network  and yi j = xn

We want to mathematically characterize how much each input pixel in a receptive ﬁeld can impact
the output of a unit n layers up the network  and study how the impact distributes within the receptive
ﬁeld of that output unit. To simplify notation we consider only a single channel on each layer  but
similar results can be easily derived for convolutional layers with more input and output channels.
Assume the pixels on each layer are indexed by (i  j)  with their center at (0  0). Denote the (i  j)th
pixel on the pth layer as xp
i j as the output
on the nth layer. We want to measure how much each x0
i j contributes to y0 0. We deﬁne the
effective receptive ﬁeld (ERF) of this central output unit as region containing any input pixel with a
non-negligible impact on that unit.
The measure of impact we use in this paper is the partial derivative ∂y0 0/∂x0
i j. It measures how
much y0 0 changes as x0
i j changes by a small amount; it is therefore a natural measure of the
importance of x0
i j with respect to y0 0. However  this measure depends not only on the weights of
the network  but are in most cases also input-dependent  so most of our results will be presented in
terms of expectations over input distribution.
The partial derivative ∂y0 0/∂x0
i j can be computed with back-propagation. In the standard setting 
back-propagation propagates the error gradient with respect to a certain loss function. Assuming we
have an arbitrary loss l  by the chain rule we have

=(cid:80)

∂l

∂yi(cid:48)  j(cid:48)
∂x0

i j

.

∂l
∂x0

i j

i(cid:48) j(cid:48)

∂yi(cid:48)  j(cid:48)

i j equals the desired ∂y0 0/∂x0

Then to get the quantity ∂y0 0/∂x0
i j  we can set the error gradient ∂l/∂y0 0 = 1 and ∂l/∂yi j = 0
for all i (cid:54)= 0 and j (cid:54)= 0  then propagate this gradient from there back down the network. The resulting
i j. Here we use the back-propagation process without an
∂l/∂x0
explicit loss function  and the process can be easily implemented with standard neural network tools.
In the following we ﬁrst consider linear networks  where this derivative does not depend on the input
and is purely a function of the network weights and (i  j)  which clearly shows how the impact of the
pixels in the receptive ﬁeld distributes. Then we move forward to consider more modern architecture
designs and discuss the effect of nonlinear activations  dropout  sub-sampling  dilation convolution
and skip connections on the ERF.
2.1 The simplest case: a stack of convolutional layers of weights all equal to one
Consider the case of n convolutional layers using k × k kernels with stride one  one single channel
on each layer and no nonlinearity  stacked into a deep linear CNN. In this analysis we ignore the
biases on all layers. We begin by analyzing convolution kernels with weights all equal to one.

2

Denote g(i  j  p) = ∂l/∂xp
i j as the gradient on the pth layer  and let g(i  j  n) = ∂l/∂yi j. Then
g(    0) is the desired gradient image of the input. The back-propagation process effectively convolves
g(    p) with the k × k kernel to get g(    p − 1) for each p.
In this special case  the kernel is a k × k matrix of 1’s  so the 2D convolution can be decomposed
into the product of two 1D convolutions. We therefore focus exclusively on the 1D case. We have the
initial gradient signal u(t) and kernel v(t) formally deﬁned as

u(t) = δ(t) 

v(t) =

δ(t − m)  where δ(t) =

t = 0
t (cid:54)= 0

(1)

and t = 0  1 −1  2 −2  ... indexes the pixels.
The gradient signal on the input pixels is simply o = u ∗ v ∗ ··· ∗ v  convolving u with n such v’s. To
compute this convolution  we can use the Discrete Time Fourier Transform to convert the signals into
the Fourier domain  and obtain

(cid:26) 1 

0 

k−1(cid:88)

m=0

U (ω) =

u(t)e−jωt = 1 

V (ω) =

v(t)e−jωt =

e−jωm

Applying the convolution theorem  we have the Fourier transform of o is

F(o) = F(u ∗ v ∗ ··· ∗ v)(ω) = U (ω) · V (ω)n =

Next  we need to apply the inverse Fourier transform to get back o(t):

∞(cid:88)

t=−∞

(cid:90) π

∞(cid:88)

t=−∞

k−1(cid:88)

(cid:33)n

m=0

e−jωm

(cid:32) k−1(cid:88)
(cid:33)n

m=0

o(t) =

e−jωm

ejωtdω

(cid:32) k−1(cid:88)

m=0

(cid:90) π
(cid:26) 1 

1
2π

−π

(2)

(3)

(4)

(5)

−π

1
2π

e−jωsejωtdω =

We can see that o(t) is simply the coefﬁcient of e−jωt in the expansion of

(cid:16)(cid:80)k−1
m=0 e−jωm(cid:17)n
(cid:16)(cid:80)k−1
m=0 e−jωm(cid:17)n
(1 + e−jω)n. The coefﬁcient for e−jωt is then the standard binomial coefﬁcient(cid:0)n
(cid:1)  so o(t) =(cid:0)n

Case k = 2: Now let’s consider the simplest nontrivial case of k = 2  where

s = t
s (cid:54)= t

0 

.

It is quite well known that binomial coefﬁcients distributes with respect to t like a Gaussian as n
becomes large (see for example [13])  which means the scale of the coefﬁcients decays as a squared
exponential as t deviates from the center. When multiplying two 1D Gaussian together  we get a 2D
Gaussian  therefore in this case  the gradient on the input plane is distributed like a 2D Gaussian.
Case k > 2: In this case the coefﬁcients are known as “extended binomial coefﬁcients” or “polyno-
mial coefﬁcients”  and they too distribute like Gaussian  see for example [3  16]. This is included as a
special case for the more general case presented later in Section 2.3.
2.2 Random weights
Now let’s consider the case of random weights. In general  we have

=

(cid:1).

t

t

g(i  j  p − 1) =

wp

a bg(i + a  i + b  p)

(6)

k−1(cid:88)

k−1(cid:88)

a=0

b=0

with pixel indices properly shifted for clarity  and wp
a b is the convolution weight at (a  b) in the
convolution kernel on layer p. At each layer  the initial weights are independently drawn from a ﬁxed
distribution with zero mean and variance C. We assume that the gradients g are independent from the
weights. This assumption is in general not true if the network contains nonlinearities  but for linear
networks these assumptions hold. As Ew[wp

k−1(cid:88)

k−1(cid:88)

a b] = 0  we can then compute the expectation
a b]Einput[g(i + a  i + b  p)] = 0  ∀p

Ew[wp

(7)

Ew input[g(i  j  p − 1)] =

a=0

b=0

3

k−1(cid:88)

k−1(cid:88)

a=0

b=0

k−1(cid:88)

k−1(cid:88)

a=0

b=0

Here the expectation is taken over w distribution as well as the input data distribution. The variance
is more interesting  as

Var[g(i  j  p−1)] =

Var[wp

a b]Var[g(i+a  i+b  p)] = C

Var[g(i+a  i+b  p)] (8)

This is equivalent to convolving the gradient variance image Var[g(    p)] with a k × k convolution
kernel full of 1’s  and then multiplying by C to get Var[g(    p − 1)].
Based on this we can apply exactly the same analysis as in Section 2.1 on the gradient variance
images. The conclusions carry over easily that Var[g(.  .  0)] has a Gaussian shape  with only a slight
change of having an extra C n constant factor multiplier on the variance gradient images  which does
not affect the relative distribution within a receptive ﬁeld.
2.3 Non-uniform kernels
More generally  each pixel in the kernel window can have different weights  or as in the random
weight case  they may have different variances. Let’s again consider the 1D case  u(t) = δ(t) as
m=0 w(m)δ(t − m)  where w(m) is the weight for the
mth pixel in the kernel. Without loss of generality  we can assume the weights are normalized  i.e.

before  and the kernel signal v(t) = (cid:80)k−1
(cid:80)

m w(m) = 1.

Applying the Fourier transform and convolution theorem as before  we get

U (ω) · V (ω)··· V (ω) =

w(m)e−jωm

(9)

(cid:32) k−1(cid:88)

m=0

(cid:33)n

exactly equals to the probability p(Sn = t)  where Sn =(cid:80)n

the space domain signal o(t) is again the coefﬁcient of e−jωt in the expansion; the only difference is
that the e−jωm terms are weighted by w(m).
These coefﬁcients turn out to be well studied in the combinatorics literature  see for example [3] and
the references therein for more details. In [3]  it was shown that if w(m) are normalized  then o(t)
i=1 Xi and Xi’s are i.i.d. multinomial
variables distributed according to w(m)’s  i.e. p(Xi = m) = w(m). Notice the analysis there
requires that w(m) > 0. But we can reduce to variance analysis for the random weight case  where
the variances are always nonnegative while the weights can be negative. The analysis for negative
w(m) is more difﬁcult and is left to future work. However empirically we found the implications of
the analysis in this section still applies reasonably well to networks with negative weights.
From the central limit theorem point of view  as n → ∞  the distribution of
n Sn − E[X])
converges to Gaussian N (0  Var[X]) in distribution. This means  for a given n large enough  Sn is
going to be roughly Gaussian with mean nE[X] and variance nVar[X]. As o(t) = p(Sn = t)  this
further implies that o(t) also has a Gaussian shape. When w(m)’s are normalized  this Gaussian has
the following mean and variance:

n( 1

√

(cid:32) k−1(cid:88)

m=0

(cid:33)2

m2w(m) −

mw(m)

(10)

k−1(cid:88)

E[Sn] = n

 k−1(cid:88)
ERF  then this size is(cid:112)Var[Sn] =(cid:112)nVar[Xi] = O(

mw(m)  Var[Sn] = n

√

m=0

m=0

n).

This indicates that o(t) decays from the center of the receptive ﬁeld squared exponentially according
to the Gaussian distribution. The rate of decay is related to the variance of this Gaussian. If we take
one standard deviation as the effective receptive ﬁeld (ERF) size which is roughly the radius of the

√
On the other hand  as we stack more convolutional layers  the theoretical receptive ﬁeld grows linearly 
therefore relative to the theoretical receptive ﬁeld  the ERF actually shrinks at a rate of O(1/
n) 
which we found surprising.
In the simple case of uniform weighting  we can further see that the ERF size grows linearly with
kernel size k. As w(m) = 1/k  we have

(cid:112)Var[Sn] =

√

n

(cid:118)(cid:117)(cid:117)(cid:116) k−1(cid:88)

m=0

−

m2
k

(cid:33)2

(cid:114)

m
k

=

(cid:32) k−1(cid:88)

m=0

4

n(k2 − 1)

12

√

n)

= O(k

(11)

√

n( 1

Remarks: The result derived in this section  i.e.  the distribution of impact within a receptive ﬁeld
in deep CNNs converges to Gaussian  holds under the following conditions: (1) all layers in the CNN
use the same set of convolution weights. This is in general not true  however  when we apply the
analysis of variance  the weight variance on all layers are usually the same up to a constant factor. (2)
The convergence derived is convergence “in distribution”  as implied by the central limit theorem.
This means that the cumulative probability distribution function converges to that of a Gaussian  but
at any single point in space the probability can deviate from the Gaussian. (3) The convergence result
n Sn−E[X]) → N (0  Var[X])  hence Sn approaches N (nE[X]  nVar[X])  however
states that
the convergence of Sn here is not well deﬁned as N (nE[X]  nVar[X]) is not a ﬁxed distribution  but
instead it changes with n. Additionally  the distribution of Sn can deviate from Gaussian on a ﬁnite
set. But the overall shape of the distribution is still roughly Gaussian.
2.4 Nonlinear activation functions
Nonlinear activation functions are an integral part of every neural network. We use σ to represent an
arbitrary nonlinear activation function. During the forward pass  on each layer the pixels are ﬁrst
passed through σ and then convolved with the convolution kernel to compute the next layer. This
ordering of operations is a little non-standard but equivalent to the more usual ordering of convolving
ﬁrst and passing through nonlinearity  and it makes the analysis slightly easier. The backward pass in
this case becomes

g(i  j  p − 1) = σp

i j

wp

a bg(i + a  i + b  p)

(12)

(cid:48) k−1(cid:88)

k−1(cid:88)

a=0

b=0

i j

a

(cid:80)

b Var[wp

(cid:48) = I[xp

(cid:48)2](cid:80)

(cid:48) to represent the gradient of the activation function for

a b]Var[g(i + a  i + b  p)]  and E[σp

where we abused notation a bit and use σp
i j
pixel (i  j) on layer p.
For ReLU nonlinearities  σp
i j > 0] where I[.] is the indicator function. We have to
i j
make some extra assumptions about the activations xp
i j to advance the analysis  in addition to
the assumption that it has zero mean and unit variance. A standard assumption is that xp
i j has a
symmetric distribution around 0 [7]. If we make an extra simplifying assumption that the gradients
σ(cid:48) are independent from the weights and g in the upper layers  we can simplify the variance as
Var[g(i  j  p− 1)] = E[σp
(cid:48)] =
1/4 is a constant factor. Following the variance analysis we can again reduce this case to the uniform
weight case.
Sigmoid and Tanh nonlinearities are harder to analyze. Here we only use the observation that when
the network is initialized the weights are usually small and therefore these nonlinearities will be in
the linear region  and the linear analysis applies. However  as the weights grow bigger during training
their effect becomes hard to analyze.
2.5 Dropout  Subsampling  Dilated Convolution and Skip-Connections
Here we consider the effect of some standard CNN approaches on the effective receptive ﬁeld.
Dropout is a popular technique to prevent overﬁtting; we show that dropout does not change the
Gaussian ERF shape. Subsampling and dilated convolutions turn out to be effective ways to increase
receptive ﬁeld size quickly. Skip-connections on the other hand make ERFs smaller. We present the
analysis for all these cases in the Appendix.

(cid:48)2] = Var[σp

i j

i j

3 Experiments

In this section  we empirically study the ERF for various deep CNN architectures. We ﬁrst use
artiﬁcially constructed CNN models to verify the theoretical results in our analysis. We then present
our observations on how the ERF changes during the training of deep CNNs on real datasets. For all
ERF studies  we place a gradient signal of 1 at the center of the output plane and 0 everywhere else 
and then back-propagate this gradient through the network to get input gradients.
3.1 Verifying theoretical results
We ﬁrst verify our theoretical results in artiﬁcially constructed deep CNNs. For computing the ERF
we use random inputs  and for all the random weight networks we followed [7  5] for proper random
initialization. In this section  we verify the following results:

5

5 layers  theoretical RF size=11

10 layers  theoretical RF size=21

Uniform

Random

Random + ReLU

Uniform

Random

Random + ReLU

20 layers  theoretical RF size=41

40 layers  theoretical RF size=81

Uniform

Random

Random + ReLU

Uniform

Random

Random + ReLU

Tanh

ReLU

Sigmoid

Figure 1: Comparing the effect of number of layers  random weight initialization and nonlinear
activation on the ERF. Kernel size is ﬁxed at 3 × 3 for all the networks here. Uniform: convolutional
kernel weights are all ones  no nonlinearity; Random: random kernel weights  no nonlinearity;
Random + ReLU: random kernel weights  ReLU nonlinearity.
ERFs are Gaussian distributed: As shown in Fig. 1  we can observe perfect Gaus-
sian shapes for uniformly and randomly weighted convolution kernels without nonlinear
activations  and near Gaussian shapes for
randomly weighted kernels with nonlinearity.
Adding the ReLU nonlinearity makes the distribution a
bit less Gaussian  as the ERF distribution depends on the
input as well. Another reason is that ReLU units output
exactly zero for half of its inputs and it is very easy to
get a zero output for the center pixel on the output plane 
which means no path from the receptive ﬁeld can reach
the output  hence the gradient is all zero. Here the ERFs are averaged over 20 runs with different
random seed. The ﬁgures on the right shows the ERF for networks with 20 layers of random weights 
with different nonlinearities. Here the results are averaged both across 100 runs with different
random weights as well as different random inputs. In this setting the receptive ﬁelds are a lot more
Gaussian-like.
√
n relative shrinkage: In Fig. 2  we show the change of ERF size and
the relative ratio of ERF over theoretical RF w.r.t number of convolution layers. The best ﬁtting line
for ERF size gives slope of 0.56 in log domain  while the line for ERF ratio gives slope of -0.43. This
indicates ERF size is growing linearly w.r.t
. Note
here we use 2 standard deviations as our measurement for ERF size  i.e. any pixel with value greater
than 1 − 95.45% of center point is considered as in ERF. The ERF size is represented by the square
root of number of pixels within ERF  while the theoretical RF size is the side length of the square in
which all pixel has a non-zero impact on the output pixel  no matter how small. All experiments here
are averaged over 20 runs.
Subsampling & dilated convolution increases receptive ﬁeld: The ﬁgure on the right shows
the effect of subsampling and dilated convolution. The reference baseline is a convnet with
15 dense convolution layers.
Its ERF is shown in the left-most ﬁgure. We then replace 3 of
the 15 convolutional layers with stride-2 convolution to get the ERF for the ‘Subsample’ ﬁgure 
and replace them with dilated convolution with factor
2 4 and 8 for the ‘Dilation’ ﬁgure. As we see  both of
them are able to increase the effect receptive ﬁeld signif-
icantly. Note the ‘Dilation’ ﬁgure shows a rectangular
ERF shape typical for dilated convolutions.

√
n absolute growth and 1/

N and ERF ratio is shrinking linearly w.r.t.

1√
N

√

3.2 How the ERF evolves during training
In this part  we take a look at how the ERF of units in the top-most convolutional layers of a
classiﬁcation CNN and a semantic segmentation CNN evolve during training. For both tasks  we
adopt the ResNet architecture which makes extensive use of skip-connections. As the analysis shows 
the ERF of this network should be signiﬁcantly smaller than the theoretical receptive ﬁeld. This is
indeed what we have observed initially. Intriguingly  as the networks learns  the ERF gets bigger  and
at the end of training is signiﬁcantly larger than the initial ERF.

Conv-Only

Subsample

Dilation

6

Figure 2: Absolute growth (left) and relative shrink (right) for ERF

CIFAR 10

CamVid

Before Training After Training Before Training After Training

Figure 3: Comparison of ERF before and after training for models trained on CIFAR-10 classiﬁcation
and CamVid semantic segmentation tasks. CIFAR-10 receptive ﬁelds are visualized in the image
space of 32 × 32.

For the classiﬁcation task we trained a ResNet with 17 residual blocks on the CIFAR-10 dataset. At
the end of training this network reached a test accuracy of 89%. Note that in this experiment we did
not use pooling or downsampling  and exclusively focus on architectures with skip-connections. The
accuracy of the network is not state-of-the-art but still quite high. In Fig. 3 we show the effective
receptive ﬁeld on the 32×32 image space at the beginning of training (with randomly initialized
weights) and at the end of training when it reaches best validation accuracy. Note that the theoretical
receptive ﬁeld of our network is actually 74 × 74  bigger than the image size  but the ERF is still not
able to fully ﬁll the image. Comparing the results before and after training  we see that the effective
receptive ﬁeld has grown signiﬁcantly.
For the semantic segmentation task we used the CamVid dataset for urban scene segmentation. We
trained a “front-end” model [21] which is a purely convolutional network that predicts the output
at a slightly lower resolution. This network plays the same role as the VGG network does in many
previous works [12]. We trained a ResNet with 16 residual blocks interleaved with 4 subsampling
operations each with a factor of 2. Due to these subsampling operations the output is 1/16 of the
input size. For this model  the theoretical receptive ﬁeld of the top convolutional layer units is quite
big at 505 × 505. However  as shown in Fig. 3  the ERF only gets a fraction of that with a diameter
of 100 at the beginning of training. Again we observe that during training the ERF size increases and
at the end it reaches almost a diameter around 150.

4 Reduce the Gaussian Damage

The above analysis shows that the ERF only takes a small portion of the theoretical receptive ﬁeld 
which is undesirable for tasks that require a large receptive ﬁeld.
New Initialization. One simple way to increase the effective receptive ﬁeld is to manipulate the
initial weights. We propose a new random weight initialization scheme that makes the weights at the
center of the convolution kernel to have a smaller scale  and the weights on the outside to be larger;
this diffuses the concentration on the center out to the periphery. Practically  we can initialize the
network with any initialization method  then scale the weights according to a distribution that has a
lower scale at the center and higher scale on the outside.

7

In the extreme case  we can optimize the w(m)’s to maximize the ERF size or equivalently the
variance in Eq. 10. Solving this optimization problem leads to the solution that put weights equally at
the 4 corners of the convolution kernel while leaving everywhere else 0. However  using this solution
to do random weight initialization is too aggressive  and leaving a lot of weights to 0 makes learning
slow. A softer version of this idea usually works better.
We have trained a CNN for the CIFAR-10 classiﬁcation task with this initialization method  with
several random seeds. In a few cases we get a 30% speed-up of training compared to the more
standard initializations [5  7]. But overall the beneﬁt of this method is not always signiﬁcant.
We note that no matter what we do to change w(m)  the effective receptive ﬁeld is still distributed
like a Gaussian so the above proposal only solves the problem partially.
Architectural changes. A potentially better approach is to make architectural changes to the CNNs 
which may change the ERF in more fundamental ways. For example  instead of connecting each unit
in a CNN to a local rectangular convolution window  we can sparsely connect each unit to a larger
area in the lower layer using the same number of connections. Dilated convolution [21] belongs to
this category  but we may push even further and use sparse connections that are not grid-like.
5 Discussion
Connection to biological neural networks. In our analysis we have established that the effective
receptive ﬁeld in deep CNNs actually grows a lot slower than we used to think. This indicates
that a lot of local information is still preserved even after many convolution layers. This ﬁnding
contradicts some long-held relevant notions in deep biological networks. A popular characterization
of mammalian visual systems involves a split into "what" and "where" pathways [19]. Progressing
along the what or where pathway  there is a gradual shift in the nature of connectivity: receptive
ﬁeld sizes increase  and spatial organization becomes looser until there is no obvious retinotopic
organization; the loss of retinotopy means that single neurons respond to objects such as faces
anywhere in the visual ﬁeld [9]. However  if the ERF is smaller than the RF  this suggests that
representations may retain position information  and also raises an interesting question concerning
changes in the size of these ﬁelds during development.
A second relevant effect of our analysis is that it suggests that convolutional networks may automati-
cally create a form of foveal representation. The fovea of the human retina extracts high-resolution
information from an image only in the neighborhood of the central pixel. Sub-ﬁelds of equal reso-
lution are arranged such that their size increases with the distance from the center of the ﬁxation.
At the periphery of the retina  lower-resolution information is extracted  from larger regions of the
image. Some neural networks have explicitly constructed representations of this form [11]. However 
because convolutional networks form Gaussian receptive ﬁelds  the underlying representations will
naturally have this character.
Connection to previous work on CNNs. While receptive ﬁelds in CNNs have not been studied
extensively  [7  5] conduct similar analyses  in terms of computing how the variance evolves through
the networks. They developed a good initialization scheme for convolution layers following the
principle that variance should not change much when going through the network.
Researchers have also utilized visualizations in order to understand how neural networks work. [14]
showed the importance of using natural-image priors and also what an activation of the convolutional
layer would represent. [22] used deconvolutional nets to show the relation of pixels in the image and
the neurons that are ﬁring. [23] did empirical study involving receptive ﬁeld and used it as a cue for
localization. There are also visualization studies using gradient ascent techniques [4] that generate
interesting images  such as [15]. These all focus on the unit activations  or feature map  instead of the
effective receptive ﬁeld which we investigate here.
6 Conclusion
In this paper  we carefully studied the receptive ﬁelds in deep CNNs  and established a few surprising
results about the effective receptive ﬁeld size. In particular  we have shown that the distribution of
impact within the receptive ﬁeld is asymptotically Gaussian  and the effective receptive ﬁeld only
takes up a fraction of the full theoretical receptive ﬁeld. Empirical results echoed the theory we
established. We believe this is just the start of the study of effective receptive ﬁeld  which provides a
new angle to understand deep CNNs. In the future we hope to study more about what factors impact
effective receptive ﬁeld in practice and how we can gain more control over them.

8

References
[1] Vijay Badrinarayanan  Ankur Handa  and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder

architecture for robust semantic pixel-wise labelling. arXiv preprint arXiv:1505.07293  2015.

[2] Xiaozhi Chen  Kaustav Kundu  Yukun Zhu  Andrew Berneshawi  Huimin Ma  Sanja Fidler  and Raquel

Urtasun. 3d object proposals for accurate object class detection. In NIPS  2015.

[3] Steffen Eger. Restricted weighted integer compositions and extended binomial coefﬁcients. Journal of

Integer Sequences  16(13.1):3  2013.

[4] Dumitru Erhan  Yoshua Bengio  Aaron Courville  and Pascal Vincent. Visualizing higher-layer features of

a deep network. University of Montreal  1341  2009.

[5] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In AISTATS  pages 249–256  2010.

[6] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

arXiv preprint arXiv:1512.03385  2015.

[7] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers: Surpassing

human-level performance on imagenet classiﬁcation. In ICCV  pages 1026–1034  2015.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual networks.

arXiv preprint arXiv:1603.05027  2016.

[9] Nancy Kanwisher  Josh McDermott  and Marvin M Chun. The fusiform face area: a module in human
extrastriate cortex specialized for face perception. The Journal of Neuroscience  17(11):4302–4311  1997.

[10] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS  pages 1097–1105  2012.

[11] Hugo Larochelle and Geoffrey E Hinton. Learning to combine foveal glimpses with a third-order boltzmann

machine. In NIPS  pages 1243–1251  2010.

[12] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic segmenta-

tion. In CVPR  pages 3431–3440  2015.

[13] L Lovsz  J Pelikn  and K Vesztergombi. Discrete mathematics: elementary and beyond  2003.

[14] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them.

In CVPR  pages 5188–5196. IEEE  2015.

[15] Alexander Mordvintsev  Christopher Olah  and Mike Tyka. Inceptionism: Going deeper into neural

networks. Google Research Blog. Retrieved June  20  2015.

[16] Thorsten Neuschel. A note on extended binomial coefﬁcients. Journal of Integer Sequences  17(2):3  2014.

[17] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object detection

with region proposal networks. In NIPS  pages 91–99  2015.

[18] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

CoRR  abs/1409.1556  2014.

[19] Leslie G Ungerleider and James V Haxby. ‘what’and ‘where’in the human brain. Current opinion in

neurobiology  4(2):157–165  1994.

[20] Kelvin Xu  Jimmy Ba  Ryan Kiros  Aaron Courville  Ruslan Salakhutdinov  Richard Zemel  and Yoshua
Bengio. Show  attend and tell: Neural image caption generation with visual attention. arXiv preprint
arXiv:1502.03044  2015.

[21] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint

arXiv:1511.07122  2015.

[22] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV 

pages 818–833. Springer  2014.

[23] Bolei Zhou  Aditya Khosla  Agata Lapedriza  Aude Oliva  and Antonio Torralba. Object detectors emerge

in deep scene cnns. arXiv preprint arXiv:1412.6856  2014.

9

,Wenjie Luo
Yujia Li
Raquel Urtasun
Richard Zemel