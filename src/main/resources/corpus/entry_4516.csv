2015,Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling,We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables  chosen according to an arbitrary distribution. In contrast to typical analysis  we directly bound the decrease of the primal-dual error (in expectation)  without the need to first analyze the dual error. Depending on the choice of the sampling  we obtain efficient serial and mini-batch variants of the method. In the serial case  our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching  our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data.,Quartz: Randomized Dual Coordinate Ascent

with Arbitrary Sampling

Zheng Qu

Department of Mathematics
The University of Hong Kong

Hong Kong

Peter Richt´arik

School of Mathematics

The University of Edinburgh
EH9 3FD  United Kingdom

zhengqu@maths.hku.hk

peter.richtarik@ed.ac.uk

Tong Zhang

Department of Statistics

Rutgers University

Piscataway  NJ  08854

tzhang@stat.rutgers.edu

Abstract

We study the problem of minimizing the average of a large number of smooth
convex functions penalized with a strongly convex regularizer. We propose and
analyze a novel primal-dual method (Quartz) which at every iteration samples and
updates a random subset of the dual variables  chosen according to an arbitrary
distribution.
In contrast to typical analysis  we directly bound the decrease of
the primal-dual error (in expectation)  without the need to ﬁrst analyze the dual
error. Depending on the choice of the sampling  we obtain efﬁcient serial and
mini-batch variants of the method. In the serial case  our bounds match the best
known bounds for SDCA (both with uniform and importance sampling). With
standard mini-batching  our bounds predict initial data-independent speedup as
well as additional data-driven speedup which depends on spectral and sparsity
properties of the data.

Keywords:
speedup.

empirical risk minimization  dual coordinate ascent  arbitrary sampling  data-driven

1

Introduction

In this paper we consider a primal-dual pair of structured convex optimization problems which has
in several variants of varying degrees of generality attracted a lot of attention in the past few years
in the machine learning and optimization communities [4  22  20  23  21  27].
Let A1  . . .   An be a collection of d-by-m real matrices and φ1  . . .   φn be 1/γ-smooth convex
functions from Rm to R  where γ > 0. Further  let g : Rd → R ∪ {+∞} be a 1-strongly convex
function and λ > 0 a regularization parameter. We are interested in solving the following primal
problem:

minw=(w1 ... wd)∈Rd

(1)
In the machine learning context  matrices {Ai} are interpreted as examples/samples  w is a (linear)
predictor  function φi is the loss incurred by the predictor on example Ai  g is a regularizer  λ
is a regularization parameter and (1) is the regularized empirical risk minimization problem.
In

P (w)

i w) + λg(w)

.

(cid:80)n
i=1 φi(A(cid:62)

def
= 1
n

(cid:105)

(cid:104)

1

this paper we are especially interested in problems where n is very big (millions  billions)  and
much larger than d. This is often the case in big data applications. Stochastic Gradient Descent
(SGD) [18  11  25] was designed for solving this type of large-scale optimization problems. In each
iteration SGD computes the gradient of one single randomly chosen function φi and approximates
the gradient using this unbiased but noisy estimation. Because of the variance of the stochastic
estimation  SGD has slow convergence rate O(1/). Recently  many methods achieving fast (linear)
convergence rate O(log(1/)) have been proposed  including SAG [19]  SVRG [6]  S2GD [8] 
SAGA [1]  mS2GD [7] and MISO [10]  all using different techniques to reduce the variance.
Another approach  such as Stochastic Dual Coordinate Ascent (SDCA) [22]  solves (1) by consid-
: Rm → R be the convex
ering its dual problem that is deﬁned as follows. For each i  let φ∗
i (u) = maxs∈Rm s(cid:62)u − φi(s) and similarly let g∗ : Rd → R be the
conjugate of φi  namely  φ∗
convex conjugate of g. The dual problem of (1) is deﬁned as:

i

(2)
where α = (α1  . . .   αn) ∈ RN = Rnm is obtained by stacking dual variables (blocks) αi ∈ Rm 
i = 1  . . .   n  on top of each other and functions f and ψ are deﬁned by

α=(α1 ... αn)∈RN =Rnm

D(α)

max

 

def

= −f (α) − ψ(α)

(cid:105)

i=1 Aiαi

ψ(α)

def
= 1
n

(3)

(cid:80)n
i=1 φ∗

i (−αi).

= λg∗(cid:0) 1

def

λn

(cid:80)n

f (α)

(cid:104)

(cid:1) ;

(cid:113) maxi Li

SDCA [22] and its proximal extension Prox-SDCA [20] ﬁrst solve the dual problem (2) by updating
uniformly at random one dual variable at each round and then recover the primal solution by setting
w = ∇g∗(α). Let Li = λmax(A(cid:62)

i Ai). It is known that if we run SDCA for at least

(cid:16)(cid:16)

(cid:17)

(cid:16)(cid:16)

O

n + maxi Li

λγ

log

n + maxi Li

λγ

(cid:17)(cid:17)

(cid:17) 1



λγ

iterations  then SDCA ﬁnds a pair (w  α) such that E[P (w) − D(α)] ≤ . By applying accelerated
randomized coordinate descent on the dual problem  APCG [9] needs at most ˜O(n +
)
number of iterations to get -accuracy. ASDCA [21] and SPDC [26] are also accelerated and ran-
domized primal-dual methods. Moreover  they can update a mini-batch of dual variables in each
round.
We propose a new algorithm (Algorithm 1)  which we call Quartz  for simultaneously solving the
primal (1) and dual (2) problems. On the dual side  at each iteration our method selects and updates
a random subset (sampling) ˆS ⊆ {1  . . .   n} of the dual variables/blocks. We assume that these sets
are i.i.d. throughout the iterations. However  we do not impose any additional assumptions on the
distribution of ˆS apart from the necessary requirement that each block i needs to be chosen with
= P(i ∈ ˆS) > 0. Quartz is the ﬁrst SDCA-like method analyzed for
a positive probability: pi
an arbitrary sampling. The dual updates are then used to perform an update to the primal variable
w and the process is repeated. Our primal updates are different (less aggressive) from those used
in SDCA [22] and Prox-SDCA [20]  thanks to which the decrease in the primal-dual error can be
bounded directly without ﬁrst establishing the dual convergence as in [20]  [23] and [9]. Our analysis
is novel and directly primal-dual in nature. As a result  our proof is more direct  and the logarithmic
term in our bound has a simpler form.

def

Main result. We prove that starting from an initial pair (w0  α0)  Quartz ﬁnds a pair (w  α) for
which P (w) − D(α) ≤  (in expectation) in at most

pi

(cid:16) 1
(cid:104)(cid:13)(cid:13)(cid:80)

(cid:16) P (w0)−D(α0)

(cid:17)



log

(cid:17)
(cid:13)(cid:13)2(cid:105) ≤(cid:80)n

max

i

+ vi

piλγn

(4)

iterations. The parameters v1  . . .   vn are assumed to satisfy the following ESO (expected separable
overapproximation) inequality:
E ˆS

(5)
where (cid:107)·(cid:107) denotes the standard Euclidean norm. Moreover  the parameters v1  . . .   vn are needed to
run the method (they determine stepsizes)  and hence it is critical that they can be cheaply computed

i=1 pivi(cid:107)hi(cid:107)2 

i∈ ˆS Aihi

2

before the method starts. We wish to point out that (5) always holds for some parameters {vi}.
Indeed  the left hand side is a quadratic function of h and hence the inequality holds for large-
enough vi. Having said that  the size of these parameters directly inﬂuences the complexity  and
hence one would want to obtain as tight bounds as possible. As we will show  for many samplings
of interest small enough parameter v can be obtained in time required to read the data {Ai}. In
particular  if the data matrix A = (A1  . . .   An) is sufﬁciently sparse  our iteration complexity
result (4) specialized to the case of standard mini-batching can be better than that of accelerated
methods such as ASDCA [21] and SPDC [26] even when the condition number maxi Li/λγ is
larger than n  see Proposition 4 and Figure 2.
As described above  Quartz uses an arbitrary sampling for picking the dual variables to be updated
in each iteration. To the best of our knowledge  only two papers exist in the literature where a
stochastic method using an arbitrary sampling was analyzed: NSync [16] for unconstrained mini-
mization of a strongly convex function and ALPHA [15] for composite minimization of non-strongly
convex function. Assumption (5) was for the ﬁrst time introduced in [16]. However  NSync is not
a primal-dual method. Besides NSync  the closest works to ours in terms of the generality of the
sampling are PCDM [17]  SPCDM [3] and APPROX [2]. All these are randomized coordinate
descent methods  and all were analyzed for arbitrary uniform samplings (i.e.  samplings satisfying
P(i ∈ ˆS) = P(i(cid:48) ∈ ˆS) for all i  i(cid:48) ∈ {1  . . .   n}). Again  none of these methods were analyzed in a
primal-dual framework.
In Section 2 we describe the algorithm  show that it admits a natural interpretation in terms of
Fenchel duality and discuss the ﬂexibility of Quartz. We then proceed to Section 3 where we state the
main result  specialize it to the samplings discussed in Section 2  and give detailed comparison of our
results with existing results for related primal-dual stochastic methods in the literature. In Section 4
we demonstrate how Quartz compares to other related methods through numerical experiments.

2 The Quartz Algorithm

Throughout the paper we consider the standard Euclidean norm  denoted by (cid:107) · (cid:107). A function φ :
Rm → R is (1/γ)-smooth if it is differentiable and has Lipschitz continuous gradient with Lispchitz
constant 1/γ: (cid:107)∇φ(x) − ∇φ(y)(cid:107) ≤ 1
γ(cid:107)x − y(cid:107)  for all x  y ∈ Rm. A function g : Rd → R ∪ {+∞}
is 1-strongly convex if g(w) ≥ g(w(cid:48)) + (cid:104)∇g(w(cid:48))  w − w(cid:48)(cid:105) + 1
2(cid:107)w − w(cid:48)(cid:107)2 for all w  w(cid:48) ∈ dom(g) 
where dom(g) denotes the domain of g and ∇g(w(cid:48)) is a subgradient of g at w(cid:48).
The most important parameter of Quartz is a random sampling ˆS  which is a random subset of
[n] = {1  2  . . .   n}. The only assumption we make on the sampling ˆS in this paper is the following:

Assumption 1 (Proper sampling) ˆS is a proper sampling; that is 

def

= P(i ∈ ˆS) > 0 

pi

i ∈ [n].

(6)

This assumption guarantees that each block (dual variable) has a chance to get updated by the
method. Prior to running the algorithm  we compute positive constants v1  . . .   vn satisfying (5)
to deﬁne the stepsize parameter θ used throughout in the algorithm:

θ = min

i

piλγn
vi+λγn .

(7)

Note from (5) that θ depends on both the data matrix A and the sampling ˆS. We shall show how to
compute in less than two passes over the data the parameter v satisfying (5) for some examples of
sampling in Section 2.2.

2.1

Interpretation of Quartz through Fenchel duality

3

(cid:80)n

i=1 Aiα0
i

vi+λγn; ¯α0 = 1

λn

Algorithm 1 Quartz

Parameters: proper random sampling ˆS and a positive vector v ∈ Rn
Initialization: α0 ∈ RN ; w0 ∈ Rd; pi = P(i ∈ ˆS); θ = min
for t ≥ 1 do
wt = (1 − θ)wt−1 + θ∇g∗(¯αt−1)
αt = αt−1
Generate a random set St ⊆ [n]  following the distribution of ˆS
for i ∈ St do
end for

i = (1 − θp−1
αt

¯αt = ¯αt−1 + (λn)−1(cid:80)

i ∇φi(A(cid:62)
i − θp−1
i wt)
i − αt−1
Ai(αt

)αt−1

piλγn

)

i

i

i

i∈St

end for
Output: wt  αt

(cid:80)n

=

(1)+(2)

λ (g(w) + g∗ (¯α)) + 1
λ(g(w) + g∗ (¯α) − (cid:104)w  ¯α(cid:105)

Quartz (Algorithm 1) has a natural interpretation in terms of Fenchel duality. Let (w  α) ∈ Rd×RN
and deﬁne ¯α = 1
λn
P (w) − D(α)

i=1 Aiαi. The duality gap for the pair (w  α) can be decomposed as:

(cid:125)
i w  αi(cid:105)
By Fenchel-Young inequality  GAPg(w  α) ≥ 0 and GAPφi(w  αi) ≥ 0 for all i  which proves
weak duality for the problems (1) and (2)  i.e.  P (w) ≥ D(α). The pair (w  α) is optimal when both
GAPg and GAPφi for all i are zero. It is known that this happens precisely when the following
optimality conditions hold:

(cid:123)(cid:122)
i (−αi) + (cid:104)A(cid:62)

(cid:80)n
(cid:80)n
i w) + φ∗
i=1 φi(A(cid:62)
(cid:125)
(cid:124)
i=1 φi(A(cid:62)
) + 1
n

i (−αi)
i w) + φ∗

GAPφi (w αi)

(cid:123)(cid:122)

GAPg(w α)

(cid:124)

=

n

(8)
(9)
We will now interpret the primal and dual steps of Quartz in terms of the above discussion. It is easy
to see that Algorithm 1 updates the primal and dual variables as follows:

i ∈ [n].

i w) 

w = ∇g∗(¯α)
αi = −∇φi(A(cid:62)

.

(cid:26) (cid:0)1 − θp−1

wt = (1 − θ)wt−1 + θ∇g∗(¯αt−1)
i + θp−1

(cid:1) αt−1

i

(cid:0)−∇φi(A(cid:62)
i wt)(cid:1)

i

i

αt

i =

(cid:80)n
αt−1
i=1 Aiαt−1

(11)
  θ is a constant deﬁned in (7) and St ∼ ˆS is a random subset of
where ¯αt−1 = 1
[n]. In other words  at iteration t we ﬁrst set the primal variable wt to be a convex combination of
its current value wt−1 and a value reducing GAPg to zero: see (10). This is followed by adjusting a
subset of dual variables corresponding to a randomly chosen set of examples St such that for each
example i ∈ St  the i-th dual variable αt
i is set to be a convex combination of its current value αt−1
and a value reducing GAPφi to zero  see (11).

λn

i

i

  i ∈ St
  i /∈ St

(10)

2.2 Flexibility of Quartz

Clearly  there are many ways in which the distribution of ˆS can be chosen  leading to numerous
variants of Quartz. The convex combination constant θ used throughout the algorithm should be
tuned according to (7) where v1  . . .   vn are constants satisfying (5). Note that the best possible v
is obtained by computing the maximal eigenvalue of the matrix (A(cid:62)A) ◦ P where ◦ denotes the
Hadamard (component-wise) product of matrices and P ∈ RN×N is an n-by-n block matrix with
all elements in block (i  j) equal to P(i ∈ ˆS  j ∈ ˆS)  see [14]. However  the worst-case complexity
of computing directly the maximal eigenvalue of (A(cid:62)A) ◦ P amounts to O(N 2)  which requires
unreasonable preprocessing time in the context of machine learning where N is assumed to be very
large. We now describe some examples of sampling ˆS and show how to compute in less than two
passes over the data the corresponding constants v1  . . .   vn. More examples including distributed
sampling are presented in the supplementary material.

4

Serial sampling. The most studied sampling in the literature on stochastic optimization is the
serial sampling  which corresponds to the selection of a single block i ∈ [n]. That is  | ˆS| = 1 with
probability 1. The name “serial” is pointing to the fact that a method using such a sampling will
typically be a serial (as opposed to being parallel) method; updating a single block (dual variable) at
a time. A serial sampling is uniquely characterized by the vector of probabilities p = (p1  . . .   pn) 
where pi is deﬁned by (6). For serial sampling ˆS  it is easy to see that (5) is satisﬁed for

where λmax(·) denotes the maximal eigenvalue.

vi = Li

def

= λmax(A(cid:62)

i Ai)  i ∈ [n] 

(12)

Standard mini-batching. We now consider ˆS which selects subsets of [n] of cardinality τ  uni-
formly at random. In the terminology established in [17]  such ˆS is called τ-nice. This sampling
satisﬁes pi = pj for all i  j ∈ [n]; and hence it is uniform. This sampling is well suited for parallel
computing. Indeed  Quartz could be implemented as follows. If we have τ processors available  then
at the beginning of iteration t we can assign each block (dual variable) in St to a dedicated processor.
The processor assigned to i would then compute ∆αt
i and apply the update. If all processors have
fast access to the memory where all the data is stored  as is the case in a shared-memory multicore
workstation  then this way of assigning workload to the individual processors does not cause any
major problems. For τ-nice sampling  (5) is satisﬁed for

vi = λmax(Mi) 

1 + (ωj−1)(τ−1)

n−1

A(cid:62)
jiAji 

i ∈ [n] 

j=1

where for each j ∈ [d]  ωj is the number of nonzero blocks in the j-th row of matrix A  i.e. 

Note that (13) follows from an extension of a formula given in [2] from m = 1 to m ≥ 1.

def

= |{i ∈ [n] : Aji (cid:54)= 0}| 

ωj

j ∈ [d].

3 Main Result

(13)

(14)

Mi =(cid:80)d

(cid:16)

(cid:17)

The complexity of our method is given by the following theorem. The proof can be found in the
supplementary material.
Theorem 2 (Main Result) Assume that g is 1-strongly convex and that for each i ∈ [n]  φi is convex
and (1/γ)-smooth. Let ˆS be a proper sampling (Assumption 1) and v1  . . .   vn be positive scalars
satisfying (5). Then the sequence of primal and dual variables {wt  αt}t≥0 of Quartz (Algorithm 1)
satisﬁes:

E[P (wt) − D(αt)] ≤ (1 − θ)t(P (w0) − D(α0)) 

where θ is deﬁned in (7). In particular  if we ﬁx  ≤ P (w0) − D(α0)  then for

(cid:17)

(cid:16) P (w0)−D(α0)

(cid:17) ⇒ E[P (wT ) − D(αT )] ≤ .

(15)

(16)

(cid:16) 1

pi

T ≥ max

i

+ vi

piλγn

log



In order to put the above result into context  in the rest of this section we will specialize the above
result to two special samplings: a serial sampling  and the τ-nice sampling.

When ˆS is a serial sampling  we just need to plug (12) into (16) and derive the bound

T ≥ max

+ Li

(17)
If in addition  ˆS is uniform  then pi = 1/n for all i ∈ [n] and we refer to this special case of Quartz
as Quartz-U. By replacing pi = 1/n in (17) we obtain directly the complexity of Quartz-U:

piλγn

log

=⇒ E[P (wT ) − D(αT )] ≤ .

pi

i

(cid:17)

(cid:17)

(cid:17)

(cid:16) P (w0)−D(α0)
(cid:17)
(cid:16) P (w0)−D(α0)



T ≥(cid:16)

n + maxi Li

λγ

log

=⇒ E[P (wT ) − D(αT )] ≤ .

(18)

3.1 Quartz with serial sampling

(cid:16) 1



5

Otherwise  we can seek to maximize the right-hand side of the inequality in (17) with respect to
the sampling probability p to obtain the best bound. A simple calculation reveals that the optimal
probability is given by:

(19)
We shall call Quartz-IP the algorithm obtained by using the above serial sampling probability. The
following complexity result of Quartz-IP can be derived easily by plugging (19) into (17):
=⇒ E[P (wT ) − D(αT )] ≤ .

P( ˆS = {i}) = p∗
(cid:80)n

T ≥(cid:16)

i=1 (Li + λγn) .

(cid:17)

n +

log

(20)

i=1 Li
nλγ



def

= (Li + λγn)/(cid:80)n
(cid:16) P (w0)−D(α0)

(cid:17)

i

Note that in contrast with the complexity result of Quartz-U (18)  we now have dependence on the
average of the eigenvalues Li.

T ≥(cid:16)

Quartz-U vs Prox-SDCA. Quartz-U should be compared to Proximal Stochastic Dual Coordinate
Ascent (Prox-SDCA) [22  20]. Indeed  the dual update of Prox-SDCA takes exactly the same form
of Quartz-U1  see (11). The main difference is how the primal variable wt is updated: while Quartz
performs the update (10)  Prox-SDCA (see also [24  5]) performs the more aggressive update wt =
∇g∗(¯αt−1) and the complexity result of Prox-SDCA is as follows:

(cid:17)(cid:16) D(α∗)−D(α0)

(cid:17)(cid:17) ⇒ E[P (wT ) − D(αT )] ≤  

λγ

log

n + maxi Li

n + maxi Li

(21)
where α∗ is the dual optimal solution. Notice that the dominant terms in (18) and (21) exactly match 
although our logarithmic term is better and simpler. This is due to a direct bound on the decrease
of the primal-dual error of Quartz  without the need to ﬁrst analyze the dual error  in contrast to the
typical approach for most of the dual coordinate ascent methods [22  23  20  21  9].

λγ



(cid:16)(cid:16)

(cid:17)

Quartz-IP vs Iprox-SDCA. The importance sampling (19) was previously used in the algorithm
Iprox-SDCA [27]  which extends Prox-SDCA to non-uniform serial sampling. The complexity of
Quartz-IP (20) should then be compared with the following complexity result of Iprox-SDCA [27]:

(cid:17)(cid:16) D(α∗)−D(α0)

(cid:17)(cid:17) ⇒ E[P (wT ) − D(αT )] ≤ .

T ≥(cid:16)

n +

i=1 Li
nλγ

log

n +

i=1 Li
nλγ

(cid:16)(cid:16)

(cid:80)n

(cid:80)n

(cid:17)

(22)



Again  the dominant terms in (20) and (22) exactly match but our logarithmic term is smaller.

3.2 Quartz with τ-nice Sampling (standard mini-batching)

(cid:17)

(cid:17)

(cid:16)

(cid:17)

We now specialize Theorem 2 to the case of the τ-nice sampling. We deﬁne ˜w such that:

maxi λmax

1 + (ωj−1)(τ−1)

n−1

j=1

A(cid:62)
jiAji

=

1 + (˜ω−1)(τ−1)

n−1

maxi Li

It is clear that 1 ≤ ˜w ≤ maxj wj ≤ n and can be considered as a measure of the density of the data.
By plugging (13) into (16) we obtain directly the following corollary.
Corollary 3 Assume ˆS is the τ-nice sampling and v is chosen as in (13). If we let  ≤ P (w0) −
D(α0) and

 n

(cid:18)

T ≥

τ +

(˜ω−1)(τ−1)

n−1

1+

λγτ

maxi Li

 log

(cid:16) P (w0)−D(α0)

(cid:17) ⇒ E[P (wT ) − D(αT )] ≤ .



(23)

(cid:16)(cid:80)d

(cid:16)

(cid:19)

Let us now have a detailed look at the above result  especially in terms of how it compares with the
serial uniform case (18). For fully sparse data  we get perfect linear speedup: the bound in (23) is a
def
1/τ fraction of the bound in (18). For fully dense data  the condition number (κ
= maxi Li/(λγ))
is unaffected by mini-batching. For general data  the behaviour of Quartz with τ-nice sampling
interpolates these two extreme cases. It is important to note that regardless of the condition number
κ  as long as τ ≤ 1 + (n − 1)/( ˜w − 1) the bound in (23) is at most a 2/τ fraction of the bound
in (18). Hence  for sparser problems  Quartz can achieve linear speedup for larger mini-batch sizes.

1In [20] the authors proposed ﬁve options of dual updating rule. Our dual updating formula (11) should be
compared with option V in Prox-SDCA. For the same reason as given in the beginning of [20  Appendix A.] 
Quartz implemented with the same other four options achieves the same complexity result as Theorem 2.

6

3.3 Quartz vs existing primal-dual mini-batch methods

We now compare the above result with existing mini-batch stochastic dual coordinate ascent meth-
ods. The mini-batch variants of SDCA  to which Quartz with τ-nice sampling can be naturally
compared  have been proposed and analyzed previously in [23]  [21] and [26]. In [23]  the authors
proposed to use a so-called safe mini-batching  which is precisely equivalent to ﬁnding the stepsize
parameter v satisfying (5) (in the special case of τ-nice sampling). However  they only analyzed
the case where the functions {φi}i are non-smooth. In [21]  the authors studied accelerated mini-
batch SDCA (ASDCA)  specialized to the case when the regularizer g is the squared L2 norm.
They showed that the complexity of ASDCA interpolates between that of SDCA and accelerated
gradient descent (AGD) [13] through varying the mini-batch size τ. In [26]  the authors proposed
a mini-batch extension of their stochastic primal-dual coordinate algorithm (SPDC). Both ASDCA
and SPDC reach the same complexity as AGD when the mini-batch size equals to n  thus should be
considered as accelerated algorithms 2. The complexity bounds for all these algorithms are summa-
rized in Table 1. In Table 2 we compare the complexities of SDCA  ASDCA  SPDC and Quartz in
several regimes.

Algorithm
SDCA [22]

ASDCA [21]

SPDC [26]

Quartz with τ-nice

sampling

(cid:26)
(cid:16)

4 × max

n
τ +

Iteration complexity

n + 1
λγ

(cid:113) n
(cid:113) n

λγτ  

n
τ +

λγτ

n
τ  

1
λγτ  

1 + ( ˜ω−1)(τ−1)

n−1

(cid:27)

2
3

g

1

1

2(cid:107) · (cid:107)2
2(cid:107) · (cid:107)2
general

general

1
3

n

(λγτ )

(cid:17) 1

λγτ

Table 1: Comparison of the iteration complexity of several primal-dual algorithms performing stochastic
coordinate ascent steps in the dual using a mini-batch of examples of size τ (with the exception of SDCA 
which is a serial method using τ = 1.

γλn = Θ(1)

γλn = Θ(τ )

γλn = Θ(

√
n)

n

n/τ

n/τ

n/τ

n

√
n/τ + n3/4/
√
n/τ + n3/4/
√
n/τ + ˜ω/

n

τ

τ

Algorithm
SDCA [22]
ASDCA

[21]

SPDC [26]

Quartz
(τ-nice)

γλn = Θ( 1√
n )
√
τ +

n3/2

n3/2/τ + n5/4/
n4/3/τ 2/3
√
n5/4/
τ
√

n3/2/τ + ˜ω

n
√
n/
τ
√
τ
n/

n

n/τ + ˜ω

Table 2: Comparison of leading factors in the complexity bounds of several methods in 5 regimes.

√

√

Looking at Table 2  we see that in the γλn = Θ(τ ) regime (i.e.  if the condition number is κ =
Θ(n/τ ))  Quartz matches the linear speedup (when compared to SDCA) of ASDCA and SPDC.
When the condition number is roughly equal to the sample size (κ = Θ(n))  then Quartz does better
than both ASDCA and SPDC as long as n/τ + ˜ω ≤ n/
τ. In particular  this is the case when
the data is sparse: ˜ω ≤ n/
τ. If the data is even more sparse (and in many big data applications
one has ˜ω = O(1)) and we have ˜ω ≤ n/τ  then Quartz signiﬁcantly outperforms both ASDCA
and SPDC. Note that Quartz can be better than both ASDCA and SPDC even in the domain of
accelerated methods  that is  when the condition number is larger than the number of examples:
κ = 1/(γλ) ≥ n. Indeed  we have the following result:
Proposition 4 Assume that nλγ ≤ 1 and that maxi Li = 1. If the data is sufﬁciently sparse so that
(24)

λγτ n ≥(cid:16)

1 + nλγ + (˜ω−1)(τ−1)

(cid:17)2

 

n−1

then the iteration complexity (in ˜O order) of Quartz is better than that of ASDCA and SPDC.

if n ≤ κ ≤ τ n/(1 + n/κ)2 (that is  τ ≥ λγτ n ≥
The result can be interpreted as follows:
(1 + nλγ)2)  then there are sparse-enough problems for which Quartz is better than both ASDCA
and SPDC.

2APCG [9] also reaches accelerated convergence rate but was not proposed in the mini-batch setting.

7

4 Experimental Results

In this section we demonstrate how Quartz specialized to different samplings compares with other
methods. All of our experiments are performed with m = 1  for smoothed hinge-loss functions {φi}
with γ = 1 and squared L2-regularizer g  see [20]. The experiments were performed on the three
datasets reported in Table 3  and three randomly generated large dataset [12] with n = 100  000
examples  d = 100  000 features with different sparsity. In Figure 1 we compare Quartz specialized
to serial sampling and for both uniform and optimal sampling with Prox-SDCA and Iprox-SDCA 
previously discussed in Section 3.1  on three datasets. Due to the conservative primal date in Quartz 
Quartz-U appears to be slower than Prox-SDCA in practice. Nevertheless  in all the experiments 
Quartz-IP shows almost identical convergence behaviour to that of Iprox-SDCA. In Figure 2 we
compare Quartz specialized to τ-nice sampling with mini-batch SPDC for different values of τ  in
the domain of accelerated methods (κ = 10n). The datasets are randomly generated following [13 
√
Section 6]. When τ = 1  it is clear that SPDC outperforms Quartz as the condition number is
larger than n. However  as τ increases  the number of data processed by SPDC is increased by
τ
as predicted by its theory but the number of data processed by Quartz remains almost the same by
taking advantage of the large sparsity of the data. Hence  Quartz is much better in the large τ regime.

Dataset
cov1
w8a
ijcnn1

# Training size n

522 911
49 749
49 990

# features d

54
300
22

Sparsity (# nnz/(nd))

22.22%
3.91%
59.09%

Table 3: Datasets used in our experiments.

(a) cov1; n = 522911; λ = 1e-06 (b) w8a; n = 49749; λ = 1e-05 (c) ijcnn1; n = 49990; λ = 1e-05

Figure 1: Comparison of Quartz-U (uniform sampling)  Quartz-IP (optimal importance sampling)  Prox-
SDCA (uniform sampling) and Iprox-SDCA (optimal importance sampling).

(a) Rand1; n = 105; λ = 1e-06

(b) Rand2; n = 105; λ = 1e-06

(c) Rand3; n = 105; λ = 1e-06

Figure 2: Comparison of Quartz with SPDC for different mini-batch size τ in the regime κ = 10n. The three
random datasets Random1  Random2 and Random2 have respective sparsity 0.01%  0.1% and 1%.

8

05010015010−1510−1010−5100nb of epochsPrimal dual gap Prox-SDCAQuartz-UIprox-SDCAQuartz-IP010020030010−1510−1010−5100nb of epochsPrimal dual gap Prox-SDCAQuartz-UIprox-SDCAQuartz-IP05010010−1510−1010−5100nb of epochsPrimal dual gap Prox-SDCAQuartz-UIprox-SDCAQuartz-IP010020030010−1010−810−610−410−2100nb of epochsPrimal dual gap Quartz-τ=1SPDC-τ=1Quartz-τ=100SPDC-τ=100Quartz-τ=1000SPDC-τ=1000010020030010−1010−810−610−410−2100nb of epochsPrimal dual gap Quartz-τ=1SPDC-τ=1Quartz-τ=10SPDC-τ=10Quartz-τ=100SPDC-τ=100010020030010−1010−810−610−410−2100nb of epochsPrimal dual gap Quartz-τ=1SPDC-τ=1Quartz-τ=10SPDC-τ=10Quartz-τ=40SPDC-τ=40References
[1] A. Defazio  F. Bach  and S. Lacoste-julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27 
pages 1646–1654. 2014.

[2] O. Fercoq and P. Richt´arik. Accelerated  parallel and proximal coordinate descent. SIAM Journal on

Optimization (after minor revision)  arXiv:1312.5799  2013.

[3] O. Fercoq and P. Richt´arik. Smooth minimization of nonsmooth functions by parallel coordinate descent.

arXiv:1309.5885  2013.

[4] C.-J. Hsieh  K.-W. Chang  C.-J. Lin  S.S. Keerthi  and S. Sundararajan. A dual coordinate descent method
for large-scale linear SVM. In Proc. of the 25th International Conference on Machine Learning  ICML
’08  pages 408–415  2008.

[5] M. Jaggi  V. Smith  M. Takac  J. Terhorst  S. Krishnan  T. Hofmann  and M.I. Jordan. Communication-
efﬁcient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems 27 
pages 3068–3076. Curran Associates  Inc.  2014.

[6] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In
C.j.c. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.q. Weinberger  editors  Advances in Neural
Information Processing Systems 26  pages 315–323. 2013.

[7] J. Koneˇcn´y  J. Lu  P. Richt´arik  and M. Tak´aˇc. mS2GD: Mini-batch semi-stochastic gradient descent in

the proximal setting. arXiv:1410.4744  2014.

[8] J. Koneˇcn´y and P. Richt´arik. S2GD: Semi-stochastic gradient descent methods. arXiv:1312.1666  2013.
[9] Q. Lin  Z. Lu  and L. Xiao. An accelerated proximal coordinate gradient method and its application to

regularized empirical risk minimization. Technical Report MSR-TR-2014-94  July 2014.

[10] J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine

learning. SIAM J. Optim.  25(2):829–855  2015.

[11] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM J. Optim.  19(4):1574–1609  2008.

[12] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM J.

Optim.  22(2):341–362  2012.

[13] Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program.  140(1  Ser. B):125–

161  2013.

[14] Z. Qu and P. Richt´arik. Coordinate descent methods with arbitrary sampling II: Expected separable

overapproximation. arXiv:1412.8063  2014.

[15] Z. Qu and P. Richt´arik. Coordinate descent methods with arbitrary sampling I: Algorithms and complexity.

arXiv:1412.8060  2014.

[16] P. Richt´arik and M. Tak´aˇc. On optimal probabilities in stochastic coordinate descent methods. Optimiza-

tion Letters  published online 2015.

[17] P. Richt´arik and M. Tak´aˇc. Parallel coordinate descent methods for big data optimization. Math. Program. 

published online 2015.

[18] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statistics  22:400–407  1951.
[19] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.

arXiv:1309.2388  2013.

[20] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv:1211.2717  2012.
[21] S. Shalev-shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances

in Neural Information Processing Systems 26  pages 378–385. 2013.

[22] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. J. Mach.

Learn. Res.  14(1):567–599  February 2013.

[23] M. Tak´aˇc  A.S. Bijral  P. Richt´arik  and N. Srebro. Mini-batch primal and dual methods for svms. In

Proc. of the 30th International Conference on Machine Learning (ICML-13)  pages 1022–1030  2013.
[24] T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent.

In

Advances in Neural Information Processing Systems 26  pages 629–637. 2013.

[25] T. Zhang. Solving large scale l.ear prediction problems using stochastic gradient descent algorithms. In

Proc. of the 21st International Conference on Machine Learning (ICML-04)  pages 919–926  2004.

[26] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimiza-
tion. In Proc. of the 32nd International Conference on Machine Learning (ICML-15)  pages 353–361 
2015.

[27] P. Zhao and T. Zhang. Stochastic optimization with importance sampling. ICML  2015.

9

,Amir Sani
Gergely Neu
Alessandro Lazaric
Zheng Qu
Peter Richtarik
Tong Zhang