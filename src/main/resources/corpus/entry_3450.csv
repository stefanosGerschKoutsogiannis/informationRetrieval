2011,Minimax Localization of Structural Information in Large Noisy Matrices,We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs  biological species and gene sequences  malware and signatures  etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance  and although several ad-hoc methods are available for biclustering  theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing  an area of statistics that has recently witnessed a flurry of activity. We make the following contributions: i) We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance  size of the matrix and bicluster of interest. ii) We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. iii) We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding  column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition.,Minimax Localization of Structural Information in

Large Noisy Matrices

Mladen Kolar†⇤

mladenk@cs.cmu.edu

Sivaraman Balakrishnan†⇤
sbalakri@cs.cmu.edu

Alessandro Rinaldo††

arinaldo@stat.cmu.edu

Aarti Singh†

aarti@cs.cmu.edu

† School of Computer Science and †† Department of Statistics  Carnegie Mellon University

Abstract

We consider the problem of identifying a sparse set of relevant columns and rows
in a large data matrix with highly corrupted entries. This problem of identify-
ing groups from a collection of bipartite variables such as proteins and drugs 
biological species and gene sequences  malware and signatures  etc is commonly
referred to as biclustering or co-clustering. Despite its great practical relevance 
and although several ad-hoc methods are available for biclustering  theoretical
analysis of the problem is largely non-existent. The problem we consider is also
closely related to structured multiple hypothesis testing  an area of statistics that
has recently witnessed a ﬂurry of activity. We make the following contributions

1. We prove lower bounds on the minimum signal strength needed for success-
ful recovery of a bicluster as a function of the noise variance  size of the
matrix and bicluster of interest.

2. We show that a combinatorial procedure based on the scan statistic achieves

this optimal limit.

3. We characterize the SNR required by several computationally tractable pro-
cedures for biclustering including element-wise thresholding  column/row
average thresholding and a convex relaxation approach to sparse singular
vector decomposition.

1

Introduction

Biclustering is the problem of identifying a (typically) sparse set of relevant columns and rows
in a large  noisy data matrix. This problem along with the ﬁrst algorithm to solve it were pro-
posed by Hartigan [14] as a way to directly cluster data matrices to produce clusters with greater
interpretability. Biclustering routinely arises in several applications such as discovering groups of
proteins and drugs that interact with each other [19]  learning phylogenetic relationships between
different species based on alignments of snippets of their gene sequences [30]  identifying malware
that have similar signatures [7] and identifying groups of users with similar tastes for commercial
products [29]. In these applications  the data matrix is often indexed by (object  feature) pairs and
the goal is to identify clusters in this set of bipartite variables.
In standard clustering problems  the goal is only to identify meaningful groups of objects and the
methods typically use the entire feature vector to deﬁne a notion of similarity between the objects.

⇤These authors contributed equally to this work

1

Biclustering can be thought of as high-dimensional clustering where only a subset of the features
are relevant for identifying similar objects  and the goal is to identify not only groups of objects
that are similar  but also which features are relevant to the clustering task. Consider  for instance
gene expression data where the objects correspond to genes  and the features correspond to their ex-
pression levels under a variety of experimental conditions. Our present understanding of biological
systems leads us to expect that subsets of genes will be co-expressed only under a small number
of experimental conditions. Although  pairs of genes are not expected to be similar under all ex-
perimental conditions it is critical to be able to discover local expression patterns  which can for
instance correspond to joint participation in a particular biological pathway or process. Thus  while
clustering aims to identify global structure in the data  biclustering take a more local approach by
jointly clustering both objects and features.
Prevalent techniques for ﬁnding biclusters are typically heuristic procedures with little or no theo-
retical underpinning. In order to study  understand and compare biclustering algorithms we consider
a simple theoretical model of biclustering [18  17  26]. This model is akin to the spiked covariance
model of [15] widely used in the study of PCA in high-dimensions.
We will focus on the following simple observation model for the matrix A 2 Rn1⇥n2:

: ui 6= 0} and K2 = {i

(1)
where  = {ij}i2[n1] j2[n2] is a random matrix whose entries are i.i.d. N (0  2) with 2 > 0
known  u = {ui : i 2 [n1]} and v = {vi : i 2 [n2]} are unknown deterministic unit vectors in
Rn1 and Rn2  respectively  and > 0 is a constant. To simplify the presentation  we assume that
u /{ 0  1}n1 and v /{ 0  1}n2. Let K1 = {i
: vi 6= 0} be the sets
indexing the non-zero components of the vectors u and v  respectively. We assume that u and v are
sparse  that is  k1 := |K1|⌧ n1 and k2 := |K2|⌧ n2. While the sets (K1  K2) are unknown 
we assume that their cardinalities are known. Notice that the magnitude of the signal for all the
pk1k2
coordinates in the bicluster K1 ⇥ K2 is
. The parameter  measures the strength of the signal 
and is the key quantity we will be studying.
We focus on the case of a single bicluster that appears as an elevated sub-matrix of size k1 ⇥ k2 with
signal strength  embedded in a large n1⇥n2 data matrix with entries corrupted by additive Gaussian
noise with variance 2. Under this model  the biclustering problem is formulated as the problem
of estimating the sets K1 and K2  based on a single noisy observation A of the unknown signal
matrix uv0. Biclustering is most subtle when the matrix is large with several irrelevant variables 
the entries are highly noisy  and the bicluster is small as deﬁned by a sparse set of rows/columns.
We provide a sharp characterization of tuples of (  n1  n2  k1  k2  2) under which it is possible to
recover the bicluster and study several common methods and establish the regimes under which they
succeed.
We establish minimax lower and upper bounds for the following class of models. Let

A = uv0 + 

Yij

⇥(0  k1  k2) := {(  K1  K2) :   0 |K1| = k1  K1 ⇢ [n1] |K2| = k2  K2 ⇢ [n2]}

(2)
be a set of parameters. For a parameter ✓ 2 ⇥  let P✓ denote the joint distribution of the entries of
A = {aij}i2[n1] j2[n2]  whose density with respect to the Lebesgue measure is

N (aij; (k1k2)1/2 1I{i 2 K1  j 2 K2}  2) 

(3)

where the notation N (z; µ  2) denotes the distribution p(z) ⇠N (µ  2) of a Gaussian random
variable with mean µ and variance 2  and 1I denotes the indicator function.
We derive a lower bound that identiﬁes tuples of (  n1  n2  k1  k2  2) under which we can recover
the true biclustering from a noisy high dimensional matrix. We show that a combinatorial pro-
cedure based on the scan statistic achieves the minimax optimal limits  however it is impractical
as it requires enumerating all possible sub-matrices of a given size in a large matrix. We analyze
the scalings (i.e. the relation between  and (n1  n2  k1  k2  2)) under which some computation-
ally tractable procedures for biclustering including element-wise thresholding  column/row average
thresholding and sparse singular vector decomposition (SSVD) succeed with high probability.
We consider the detection of both small and large biclusters of weak activation  and show that at the
minimax scaling the problem is surprisingly subtle (e.g.  even detecting big clusters is quite hard).

2

In Table 1  we describe our main ﬁndings and compare the scalings under which the various algo-
rithms succeed.

Combinatorial Thresholding

Row/Column Averaging

Sparse SVD

Algorithm
SNR scaling
Bicluster size

Minimax

Any

Theorem 2

Weak
Any

Theorem 3

(n1/2+↵

1

Intermediate
⇥ n1/2+↵
2
Theorem 4

) ↵ 2 (0  1/2)

Weak
Any

Theorem 5

Where the scalings are 

1. Minimax:  ⇠  max⇣pk1 log(n1  k1) pk2 log(n2  k2)⌘
2. Weak:  ⇠  max⇣pk1k2 log(n1  k1) pk1k2 log(n2  k2)⌘
3. Intermediate (for large clusters):  ⇠  max✓pk1k2 log(n1k1)

n↵
2

 

pk1k2 log(n2k2)

n↵
1

◆

Element-wise thresholding does not take advantage of any structure in the data matrix and hence
does not achieve the minimax scaling for any bicluster size.
If the clusters are big enough
row/column averaging performs better than element-wise thresholding since it can take advantage
of structure. We also study a convex relaxation for sparse SVD  based on the DSPCA algorithm pro-
posed by [11] that encourages the singular vectors of the matrix to be supported over a sparse set of
variables. However  despite the increasing popularity of this method  we show that it is only guaran-
teed to yield a sparse set of singular vectors when the SNR is quite high  equivalent to element-wise
thresholding  and fails for stronger scalings of the SNR.

1.1 Related work

Due to its practical importance and difﬁculty biclustering has attracted considerable attention (for
some recent surveys see [9  27  20  22]). Broadly algorithms for biclustering can be categorized as
either score-based searches  or spectral algorithms. Many of the proposed algorithms for identifying
relevant clusters are based on heuristic searches whose goal is to identify large average sub-matrices
or sub-matrices that are well ﬁt by a two-way ANOVA model. Sun et. al.
[26] provide some
statistical backing for these exhaustive search procedures. In particular  they show how to construct
a test via exhaustive search to distinguish when there is a small sub-matrix of weak activation from
the “null” case when there is no bicluster.
The premise behind the spectral algorithms is that if there was a sub-matrix embedded in a large
matrix  then this sub-matrix could be identiﬁed from the left and right singular vectors of A. In the
case when exactly one of u and v is random  the model (1) can be related to the spiked covariance
model of [15]. In the case when v is random  the matrix A has independent columns and dependent
rows. Therefore  A0A is a spiked covariance matrix and it is possible to use the existing theoretical
results on the ﬁrst eigenvalue to characterize the left singular vector of A. A lot of recent work has
dealt with estimation of sparse eigenvectors of A0A  see for example [32  16  24  31  2]. For biclus-
tering applications  the assumption that exactly one u or v is random  is not justiﬁable  therefore 
theoretical results for the spiked covariance model do not translate directly. Singular vectors of the
model (1) have been analyzed by [21]  improving on earlier results of [6]. These results however are
asymptotic and do not consider the case when u and v are sparse.
Our setup for the biclustering problem also falls in the framework of structured normal means multi-
ple hypothesis testing problems  where for each entry in the matrix the hypotheses are that the entry
has mean 0 versus an elevated mean. The presence of a bicluster (sub-matrix) however imposes
structure on which elements are elevated concurrently. Recently  several papers have investigated
the structured normal means setting for ordered domains. For example  [5] consider the detection of
elevated intervals and other parametric structures along an ordered line or grid  [4] consider detec-
tion of elevated connected paths in tree and lattice topologies  [3] considers nonparametric cluster
structures in a regular grid. In addition  [1] consider testing of different elevated structures in a gen-
eral but known graph topology. Our setup for the biclustering problem requires identiﬁcation of an
elevated submatrix in an unordered matrix. At a high level  all these results suggest that it is possible
to leverage the structure to improve the SNR threshold at which the hypothesis testing problem is

3

feasible. However  computationally efﬁcient procedures that achieve the minimax SNR thresholds
are only known for a few of these problems. Our results for biclustering have a similar ﬂavor  in
that the minimax threshold requires a combinatorial procedure whereas the computationally efﬁcient
procedures we investigate are often sub-optimal.
The rest of this paper is organized as follows.
In Section 2  we provide a lower bound on the
minimum signal strength needed for successfully identifying the bicluster. Section 3 presents a
combinatorial procedure which achieves the lower bound and hence is minimax optimal. We inves-
tigate some computationally efﬁcient procedures in Section 4. Simulation results are presented in
Section 5 and we conclude in Section 6. All proofs are deferred to the Appendix.

2 Lower bound

In this section  we derive a lower bound for the problem of identifying the correct bicluster  indexed
by K1 and K2  in model (1). In particular  we derive conditions on (  n1  n2  k1  k2  2) under
which any method is going to make an error when estimating the correct cluster. Intuitively  if either
the signal-to-noise ratio / or the cluster size is small  the minimum signal strength needed will
be high since it is harder to distinguish the bicluster from the noise.
Theorem 1. Let ↵ 2 (0  1
8 ) and
min = min(n1  n2  k1  k2  )

Then for all 0  min 
inf


sup

✓2⇥(0 k1 k2)

P✓[(A) 6= (K1(✓)  K2(✓))] 

pM
1 + pM ✓1  2↵ 

= p↵ max0@pk1 log(n1  k1) pk2 log(n2  k2) s k1k2 log(n1  k1)(n2  k1)
log M◆ n1 n2!1

! 12↵ 
(5)
where M = min(n1  k1  n2  k2)  ⇥(0  k1  k2) is given in (2) and the inﬁmum is over all
measurable maps : Rn1⇥n2 7! 2[n1] ⇥ 2[n2].
The result can be interpreted in the following way: for any biclustering procedure   if 0  min 
then there exists some element in the model class ⇥(0  k1  k2) such that the probability of incor-
rectly identifying the sets K1 and K2 is bounded away from zero.
The proof is based on a standard technique described in Chapter 2.6 of [28]. We start by identifying
a subset of parameter tuples that are hard to distinguish. Once a suitable ﬁnite set is identiﬁed  tools
for establishing lower bounds on the error in multiple-hypothesis testing can be directly applied.
These tools only require computing the Kullback-Leibler (KL) divergence between two distribu-
tions P✓1 and P✓2  which in the case of model (1) are two multivariate normal distributions. These
constructions and calculations are described in detail in the Appendix.

1A .

(4)

k1 + k2  1

2↵

3 Minimax optimal combinatorial procedure

We now investigate a combinatorial procedure achieving the lower bound of Theorem 1  in the sense
that  for any ✓ 2 ⇥(min  k1  k2)  the probability of recovering the true bicluster (K1  K2) tends to
one as n1 and n2 grow unbounded. This scan procedure consists in enumerating all possible pairs
of subsets of the row and column indexes of size k1 and k2  respectively  and choosing the one
whose corresponding submatrix has the largest overall sum. In detail  for an observed matrix A and
two candidate subsets ˜K1 ⇢ [n1] and ˜K2 ⇢ [n2]  we deﬁne the associated score S( ˜K1  ˜K2) :=
Pi2 ˜K1Pj2 ˜K2
aij. The estimated bicluster is the pair of subsets of sizes k1 and k2 achieving the
highest score:

 (A) := argmax

( ˜K1  ˜K2) S( ˜K1  ˜K2)

subject to | ˜K1| = k1  | ˜K2| = k2.

(6)

The following theorem determines the signal strength  needed for the decoder to ﬁnd the true
bicluster.

4

k1 + k2

Theorem 2. Let A ⇠ P✓ with ✓ 2 ⇥(  k1  k2) and assume that k1  n1/2 and k2  n2/2. If

  4 max0@pk1 log(n1  k1) pk2 log(n2  k2) s 2k1k2 log(n1  k1)(n2  k2)

1A (7)

then P[ (A) 6= (K1  K2)]  2[(n1  k1)1 + (n2  k2)1] where is the decoder deﬁned in (6).
Comparing to the lower bound in Theorem 1  we observe that the combinatorial procedure using the
decoder that looks for all possible clusters and chooses the one with largest score achieves the
lower bound up to constants. Unfortunately  this procedure is not practical for data sets commonly

k1n2
encountered in practice  as it requires enumerating alln1
k2 possible sub-matrices of size k1 ⇥

k2. The combinatorial procedure requires the signal to be positive  but not necessarily constant
throughout the bicluster. In fact it is easy to see that provided the average signal in the bicluster is
larger than that stipulated by the theorem this procedure succeeds with high probability irrespective
of how the signal is distributed across the bicluster. Finally  we remark that the estimation of the
cluster is done under the assumption that k1 and k2 are known. Establishing minimax lower bounds
and a procedure that adapts to unknown k1 and k2 is an open problem.

4 Computationally efﬁcient biclustering procedures

In this section we investigate the performance of various procedures for biclustering  that  unlike the
optimal scan statistic procedure studied in the previous section  are computationally tractable. For
each of these procedures however  computational ease comes at the cost of suboptimal performance:
recovery of the true bicluster is only possible if the  is much larger than the minimax signal strength
of Theorem 1.

4.1 Element-wise thresholding

The simplest procedure that we analyze is based on element-wise thresholding. The bicluster is
estimated as

 thr(A ⌧ ) := {(i  j) 2 [n1] ⇥ [n2] : |aij| ⌧}

(8)
where ⌧> 0 is a parameter. The following theorem characterizes the signal strength  required for
the element-wise thresholding to succeed in recovering the bicluster.
Theorem 3. Let A ⇠ P✓ with ✓ 2 ⇥(  k1  k2) and ﬁx > 0. Set the threshold ⌧ as

(n1  k1)(n2  k2) + k1(n2  k2) + k2(n1  k1)



.

If

⌧ = r2 log
 pk1k2 r2 log



k1k2

+r2 log

(n1  k1)(n2  k2) + k1(n2  k2) + k2(n1  k1)



!

then P[ thr(A ⌧ ) 6= K1 ⇥ K2] = o(/(k1k2)).
Comparing Theorem 3 with the lower bound in Theorem 1  we observe that
strength  needs to be O(max(pk1 pk2)) larger than the lower bound. This is not sur-
prising  since the element-wise thresholding is not exploiting the structure of the problem 
but is assuming that the large elements of the matrix A are positioned randomly. From the
if  
proof it
◆ for a small enough con-
cpk1k2✓q2 log k1k2
stant c then thresholding will no longer recover the bicluster with probability at least 1 . It is also
worth noting that thresholding neither requires the signal in the bicluster to be constant nor positive
provided it is larger in magnitude  at every entry  than the threshold speciﬁed in the theorem.

 +q2 log (n1k1)(n2k2)+k1(n2k2)+k2(n1k1)

this upper bound is tight up to constants 

is not hard to see that

the signal

i.e.



5

4.2 Row/Column averaging

Next  we analyze another a procedure based on column and row averaging. When the bicluster
is large this procedure exploits the structure of the problem and outperforms the simple element-
wise thresholding and the sparse SVD  which is discussed in the following section. The averaging
procedure works only well if the bicluster is “large”  as speciﬁed below  since otherwise the row or
column average is dominated by the noise.
More precisely  the averaging procedure computes the average of each row and column of A and
outputs the k1 rows and k2 columns with the largest average. Let {rr i}i2[n1] and {rc j}j2[n2] denote
the positions of rows and columns when they are ordered according to row and column averages in
descending order. The bicluster is estimated then as

 avg(A) := {i 2 [n1] : rr i  k1}⇥{ j 2 [n2] : rc j  k2}.

(9)
The following theorem characterizes the signal strength  required for the averaging procedure to
succeed in recovering the bicluster.
Theorem 4. Let A ⇠ P✓ with ✓ 2 ⇥(  k1  k2). If k1 =⌦( n1/2+↵
↵ 2 (0  1/2) is a constant and 

) and k2 =⌦( n1/2+↵

)  where

2

1

  4 max pk1k2 log(n1  k1)

n↵
2

 pk1k2 log(n2  k2)

n↵
1

!

1 + n1
2 ].

then P[ (A) 6= (K1  K2)]  [n1
Comparing to Theorem 3  we observe that the averaging requires lower signal strength than the
element-wise thresholding when the bicluster is large  that is  k1 =⌦( pn1) and k2 =⌦( pn2).
Unless both k1 = O(n1) and k2 = O(n2)  the procedure does not achieve the lower bound of
Theorem 1  however  the procedure is simple and computationally efﬁcient. It is also not hard to
show that this theorem is sharp in its characterization of the averaging procedure. Further  unlike
thresholding  averaging requires the signal to be positive in the bicluster.
It is interesting to note that a large bicluster can also be identiﬁed without assuming the normality
of the noise matrix . This non-parametric extension is based on a simple sign-test  and the details
are provided in Appendix.

4.3 Sparse singular value decomposition (SSVD)

An alternate way to estimate K1 and K2 would be based on the singular value decomposition (SVD) 
i.e. ﬁnding ˜u and ˜v that maximize h˜u  A˜vi  and then threshold the elements of ˜u and ˜v. Unfortu-
nately  such a method would perform poorly when the signal  is weak and the dimensionality is
high  since  due to the accumulation of noise  ˜u and ˜v are poor estimates of u and v and and do not
exploit the fact that u and v are sparse.
In fact  it is now well understood [8] that SVD is strongly inconsistent when the signal strength is
weak  i.e. \(˜u  u) ! ⇡/2 (and similarly for v) almost surely. See [26] for a clear exposition and
discussion of this inconsistency in the SVD setting.
To properly exploit the sparsity in the singular vectors  it seems natural to impose a cardinality
constraint to obtain a sparse singular vector decomposition (SSVD):

max

u2Sn11 v2Sn21hu  Avi

which can be further rewritten as

subject to ||u||0  k1  ||v||0  k2 

max

Z2Rn2⇥n1

tr AZ subject to Z = vu0  ||u||2 = 1  ||v||2 = 1  ||u||0  k1  ||v||0  k2.

(10)

The above problem is non-convex and computationally intractable.
Inspired by the convex relaxation methods for sparse principal component analysis proposed by
[11]  we consider the following relaxation the SSVD:

max

X2R(n1+n2)⇥(n1+n2)

tr AX21  10|X21|1 subject to X ⌫ 0  tr X11 = 1  tr X22 = 1 

(11)

6

where X is the block matrix

 X11 X12
X21 X22 

as

and

(12)

bicluster.

bK2 = {j 2 [n2] : bvj 6= 0}.

with the block X21 corresponding to Z in (10). If the optimal solution bX is of rank 1  then  nec-
essarily  bX =bu
bv(bu0bv0). Based on the sparse singular vectorsbu andbv  we estimate the bicluster
bK1 = {j 2 [n1] : buj 6= 0}
The user deﬁned parameter  controls the sparsity of the solution bX21  and  therefore  provided
the solution is of rank one  it also controls the sparsity of the vectorsbu andbv and of the estimated
The following theorem provides sufﬁcient conditions for the solutionbX to be rank one and to recover
the bicluster.
Theorem 5. Consider the model in (1). Assume k1 ⇣ k2 and k1  n1/2 and k2  n2/2. If
  2pk1k2 log(n1  k1)(n2  k2)
then the solutionbX of the optimization problem in (11) with  = 
1 O (k1
It is worth noting that SSVD correctly recovers signed vectorsbu andbv under this signal strength. In
particular  the procedure works even if the u and v in Equation 1 are signed.
The following theorem establishes necessary conditions for the SSVD to have a rank 1 solution that
correctly identiﬁes the bicluster.
Theorem 6. Consider the model in (1). Fix c 2 (0  1/2). Assume that k1 ⇣ k2 and k1 = o(n1/2c)
and k2 = o(n1/2c

1 ). Furthermore  we have that (bK1  bK2) = (K1  K2) with probability 1 O (k1

(13)
is of rank 1 with probability

2pk1k2

1 ).

with  = 

2pk1k2

(14)
then the optimization problem (11) does not have a rank 1 solution that correctly

  2pck1k2 log max(n1  k1  n2  k2) 

recovers the sparsity pattern with probability at least 1 O (exp((pk1 + pk2)2) for sufﬁciently
large n1 and n2.
From Theorem 6 observe that the sufﬁcient conditions of Theorem 5 are sharp. In particular  the two
theorems establish that the SSVD does not establish the lower bound given in Theorem 1. The signal
strength needs to be of the same order as for the element-wise thresholding  which is somewhat
surprising since from the formulation of the SSVD optimization problem it seems that the procedure
uses the structure of the problem. From numerical simulations in Section 5 we observe that although
SSVD requires the same scaling as thresholding  it consistently performs slightly better at a ﬁxed
signal strength.

). If

2

5 Simulation results

We test the performance of the three computationally efﬁcient procedures on synthetic data: thresh-
olding  averaging and sparse SVD. For sparse SVD we use an implementation posted online by [11].
We generate data from (1) with n = n1 = n2  k = k1 = k2  2 = 1 and u = v / (10k  00nk)0.
For each algorithm we plot the Hamming fraction (i.e. the Hamming distance between sbu and su
rescaled to be between 0 and 1) against the rescaled sample size. In each case we average the results
over 50 runs.
For thresholding and sparse SVD the rescaled scaling (x-axis) is

and for averaging the



n↵

kplog(nk)

. We observe that there is a sharp threshold between success

rescaled scaling (x-axis) is
and failure of the algorithms  and the curves show good agreement with our theory.
The vertical line shows the point after which successful recovery happens for all values of n. We can
make a direct comparison between thresholding and sparse SVD (since the curves are identically
rescaled) to see that at least empirically sparse SVD succeeds at a smaller scaling constant than
thresholding even though their asymptotic rates are identical.

kplog(nk)

7

1

n
o
i
t
c
a
r
f
 
g
n
m
m
a
H

i

0.8

0.6

0.4

0.2

 

0
1

 k = log(n)

 k = n1/3

 

n = 100
n = 200
n = 300
n = 400
n = 500

2

3

4

Signal strength

5

6

7

1

n
o
i
t
c
a
r
f
 
g
n
m
m
a
H

i

0.8

0.6

0.4

0.2

 

0
1

 

n = 100
n = 200
n = 300
n = 400
n = 500

2

3

4

Signal strength

5

6

7

1

n
o
i
t
c
a
r
f
 
g
n
m
m
a
H

i

0.8

0.6

0.4

0.2

 

0
1

 k = 0.2n

 

n = 100
n = 200
n = 300
n = 400
n = 500

2

3

4

Signal strength

5

6

7

Figure 1: Thresholding: Hamming fraction versus rescaled signal strength.

1

n
o
i
t
c
a
r
f
 
g
n
m
m
a
H

i

0.8

0.6

0.4

0.2

 

0
0

1

2

 k = n1/2 + α  α = 0.1

3

4

Signal strength

 

n = 100
n = 200
n = 300
n = 400
n = 500

1

n
o
i
t
c
a
r
f
 
g
n
m
m
a
H

i

0.8

0.6

0.4

0.2

5

6

7

 

0
0

1

2

 k = 0.2n

3

4

Signal strength

 

n = 100
n = 200
n = 300
n = 400
n = 500

5

6

7

Figure 2: Averaging: Hamming fraction versus rescaled signal strength.

1

n
o

i
t
c
a
r
f
 

i

g
n
m
m
a
H

0.8

0.6

0.4

0.2

 

0
1

 k = log(n)

 k = n1/3

 

n = 100
n = 200
n = 300
n = 400
n = 500

1.5

2

Signal strength

2.5

3

1

n
o

i
t
c
a
r
f
 

i

g
n
m
m
a
H

0.8

0.6

0.4

0.2

 

0
1

 

n = 100
n = 200
n = 300
n = 400
n = 500

1.5

2

Signal strength

2.5

3

n
o

i
t
c
a
r
f
 

i

g
n
m
m
a
H

1

0.8

0.6

0.4

0.2

 

0
1

 k = 0.2n

 

n = 100
n = 200
n = 300
n = 400
n = 500

1.5

2

Signal strength

2.5

3

Figure 3: Sparse SVD: Hamming fraction versus rescaled signal strength.

6 Discussion

In this paper  we analyze biclustering using a simple statistical model (1)  where a sparse rank one
matrix is perturbed with noise. Using this model  we have characterized the minimal signal strength
below which no procedure can succeed in recovering the bicluster. This lower bound can be matched
using an exhaustive search technique. However  it is still an open problem to ﬁnd a computationally
efﬁcient procedure that is minimax optimal.
Amini et. al. [2] analyze the convex relaxation procedure proposed in [11] for high-dimensional
sparse PCA. Under the minimax scaling for this problem they show that provided a rank-1 solution
exists it has the desired sparsity pattern (they were however not able to show that a rank-1 solution
exists with high probability). Somewhat surprisingly  we show that in the SVD case a rank-1 solution
with the desired sparsity pattern does not exist with high probability. The two settings however are
not identical since the noise in the spiked covariance model is Wishart rather than Gaussian  and
has correlated entries. It would be interesting to analyze whether our negative result has similar
implications for the sparse PCA setting.
The focus of our paper has been on a model with one cluster  which although simple  provides
several interesting theoretical insights. In practice  data often contains multiple clusters which need
to be estimated. Many existing algorithms (see e.g. [17] and [18]) try to estimate multiple clusters
and it would be useful to analyze these theoretically.
Furthermore  the algorithms that we have analyzed assume knowledge of the size of the cluster 
which is used to select the tuning parameters. It is a challenging problem of great practical relevance
to ﬁnd data driven methods to select these tuning parameters.
7 Acknowledgments
We would like to thank Arash Amini and Martin Wainwright for fruitful discussions  and Larry
Wasserman for his ideas  indispensable advice and wise guidance. This research is supported in
part by AFOSR under grant FA9550-10-1-0382 and NSF under grant IIS-1116458. SB would also
like to thank Jaime Carbonell and Srivatsan Narayanan for several valuable comments and thought-
provoking discussions.

8

References
[1] Louigi Addario-Berry  Nicolas Broutin  Luc Devroye  and G´abor Lugosi. On combinatorial testing prob-

lems. Ann. Statist.  38(5):3063–3092  2010.

[2] A.A. Amini and M.J. Wainwright. High-Dimensional Analysis Of Semideﬁnite Relaxations For Sparse

Principal Components. The Annals of Statistics  37(5B):2877–2921  2009.

[3] Ery Arias-Castro  Emmanuel J. Cand`es  and Arnaud Durand. Detection of an anomalous cluster in a

network. Ann. Stat.  39(1):278–304  2011.

[4] Ery Arias-Castro  Emmanuel J. Cand`es  Hannes Helgason  and Ofer Zeitouni. Searching for a trail of

evidence in a maze. Ann. Statist.  36(4):1726–1757  2008.

[5] Ery Arias-Castro  David L. Donoho  and Xiaoming Huo. Adaptive multiscale detection of ﬁlamentary

structures in a background of uniform random points. Ann. Statist.  34(1):326–349  2006.

[6] Jushan Bai. Inferential theory for factor models of large dimensions. Econometrica  71(1):pp. 135–171 

2003.

[7] Ulrich Bayer  Paolo Milani Comparetti  Clemens Hlauscheck  Christopher Kruegel  and Engin Kirda.
Scalable  Behavior-Based Malware Clustering. In 16th Symposium on Network and Distributed System
Security (NDSS)  2009.

[8] F. Benaych-Georges and R. Rao Nadakuditi. The singular values and vectors of low rank perturbations

of large rectangular random matrices. ArXiv e-prints  March 2011.

[9] S. Busygin  O. Prokopyev  and P.M. Pardalos. Biclustering in data mining. Computers & Operations

[10] Emmanuel J. Cand`es  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis?

Research  35(9):2964–2987  2008.

CoRR  abs/0912.3599  2009.

[11] Alexandre d’Aspremont  Laurent El Ghaoui  Michael I. Jordan  and Gert R. G. Lanckriet. A direct

formulation for sparse pca using semideﬁnite programming. SIAM Review  49:434–448  2007.

[12] K.R. Davidson and S.J. Szarek. Local operator theory  random matrices and Banach spaces. Handbook

of the geometry of Banach spaces  1:317–366  2001.

[13] R. Fletcher. Semi-deﬁnite matrix constraints in optimization. SIAM Journal on Control and Optimization 

[14] J. A. Hartigan. Direct clustering of a data matrix. Journal of the American Statistical Association 

[15] I.M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. The Annals

23:493  1985.

67(337):pp. 123–129  1972.

of Statistics  29(2):295–327  2001.

[16] I.M. Johnstone and A.Y. Lu. On consistency and sparsity for principal components analysis in high

dimensions. Journal of the American Statistical Association  104(486):682–693  2009.

[17] L. Lazzeroni and A. Owen. Plaid models for gene expression data. Statistica sinica  12:61–86  2002.
[18] Mihee Lee  Haipeng Shen  Jianhua Z. Huang  and J. S. Marron. Biclustering via sparse singular value

decomposition. Biometrics  66(4):1087–1095  2010.

[19] Jinze Liu and Wei Wang. Op-cluster: Clustering by tendency in high dimensional space. In Proceedings
of the Third IEEE International Conference on Data Mining  ICDM ’03  pages 187–  Washington  DC 
USA  2003. IEEE Computer Society.

[20] S.C. Madeira and A.L. Oliveira. Biclustering algorithms for biological data analysis: a survey. IEEE

Transactions on computational Biology and Bioinformatics  pages 24–45  2004.

[21] A. Onatski. Asymptotics of the principal components estimator of large factor models with weak factors.

Economics Department  Columbia University  2009.

[22] L. Parsons  E. Haque  and H. Liu. Subspace clustering for high dimensional data: a review. ACM SIGKDD

[23] R.T. Rockafellar. The theory of subgradients and its applications to problems of optimization. Convex

Explorations Newsletter  6(1):90–105  2004.

and nonconvex functions. Heldermann  1981.

[24] H. Shen and J.Z. Huang. Sparse principal component analysis via regularized low rank matrix approxi-

mation. Journal of multivariate analysis  99(6):1015–1034  2008.

[25] GW Stewart. Perturbation theory for the singular value decomposition. Computer Science Technical

Report Series; Vol. CS-TR-2539  page 13  1990.

[26] X. Sun and A. B. Nobel. On the maximal size of Large-Average and ANOVA-ﬁt Submatrices in a

Gaussian Random Matrix. ArXiv e-prints  September 2010.

[27] A. Tanay  R. Sharan  and R. Shamir. Biclustering algorithms: A survey. Handbook of computational

molecular biology  2004.

[28] A.B. Tsybakov. Introduction to nonparametric estimation. Springer  2009.
[29] Lyle Ungar and Dean P. Foster. A formal statistical approach to collaborative ﬁltering. In CONALD  98.
[30] S. Wang  R. R. Gutell  and D. P. Miranker. Biclustering as a method for RNA local multiple sequence

alignment. Bioinformatics  23:3289–3296  Dec 2007.

[31] D.M. Witten  R. Tibshirani  and T. Hastie. A penalized matrix decomposition  with applications to sparse

principal components and canonical correlation analysis. Biostatistics  10(3):515  2009.

[32] H. Zou  T. Hastie  and R. Tibshirani. Sparse principal component analysis. Journal of computational and

graphical statistics  15(2):265–286  2006.

9

,Zeyuan Allen-Zhu
Yuanzhi Li