2017,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation,In this work  we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge  this is the first scalable trust region natural gradient method for actor-critic methods. It is also the method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods  we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average  compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines,Scalable trust-region method for deep reinforcement
learning using Kronecker-factored approximation

Yuhuai Wu∗

University of Toronto

Vector Institute

ywu@cs.toronto.edu

Elman Mansimov∗
New York University

mansimov@cs.nyu.edu

Shun Liao

University of Toronto

Vector Institute

sliao3@cs.toronto.edu

Roger Grosse

University of Toronto

Vector Institute

rgrosse@cs.toronto.edu

Jimmy Ba

University of Toronto

Vector Institute

jimmy@psi.utoronto.ca

Abstract

In this work  we propose to apply trust region optimization to deep reinforce-
ment learning using a recently proposed Kronecker-factored approximation to
the curvature. We extend the framework of natural policy gradient and propose
to optimize both the actor and the critic using Kronecker-factored approximate
curvature (K-FAC) with trust region; hence we call our method Actor Critic using
Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge  this
is the ﬁrst scalable trust region natural gradient method for actor-critic methods.
It is also the method that learns non-trivial tasks in continuous control as well as
discrete control policies directly from raw pixel inputs. We tested our approach
across discrete domains in Atari games as well as continuous domains in the Mu-
JoCo environment. With the proposed methods  we are able to achieve higher
rewards and a 2- to 3-fold improvement in sample efﬁciency on average  compared
to previous state-of-the-art on-policy actor-critic methods. Code is available at
https://github.com/openai/baselines.

1

Introduction

Agents using deep reinforcement learning (deep RL) methods have shown tremendous success in
learning complex behaviour skills and solving challenging control tasks in high-dimensional raw
sensory state-space [24  17  12]. Deep RL methods make use of deep neural networks to represent
control policies. Despite the impressive results  these neural networks are still trained using simple
variants of stochastic gradient descent (SGD). SGD and related ﬁrst-order methods explore weight
space inefﬁciently. It often takes days for the current deep RL methods to master various continuous
and discrete control tasks. Previously  a distributed approach was proposed [17] to reduce training
time by executing multiple agents to interact with the environment simultaneously  but this leads to
rapidly diminishing returns of sample efﬁciency as the degree of parallelism increases.
Sample efﬁciency is a dominant concern in RL; robotic interaction with the real world is typically
scarcer than computation time  and even in simulated environments the cost of simulation often
dominates that of the algorithm itself. One way to effectively reduce the sample size is to use
more advanced optimization techniques for gradient updates. Natural policy gradient [10] uses the
technique of natural gradient descent [1] to perform gradient updates. Natural gradient methods

∗Equal contribution.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Performance comparisons on six standard Atari games trained for 10 million timesteps (1 timestep
equals 4 frames). The shaded region denotes the standard deviation over 2 random seeds.

follow the steepest descent direction that uses the Fisher metric as the underlying metric  a metric
that is based not on the choice of coordinates but rather on the manifold (i.e.  the surface).
However  the exact computation of the natural gradient is intractable because it requires inverting the
Fisher information matrix. Trust-region policy optimization (TRPO) [21] avoids explicitly storing
and inverting the Fisher matrix by using Fisher-vector products [20]. However  it typically requires
many steps of conjugate gradient to obtain a single parameter update  and accurately estimating the
curvature requires a large number of samples in each batch; hence TRPO is impractical for large
models and suffers from sample inefﬁciency.
Kronecker-factored approximated curvature (K-FAC) [15  6] is a scalable approximation to natural
gradient.
It has been shown to speed up training of various state-of-the-art large-scale neural
networks [2] in supervised learning by using larger mini-batches. Unlike TRPO  each update is
comparable in cost to an SGD update  and it keeps a running average of curvature information 
allowing it to use small batches. This suggests that applying K-FAC to policy optimization could
improve the sample efﬁciency of the current deep RL methods.
In this paper  we introduce the actor-critic using Kronecker-factored trust region (ACKTR; pro-
nounced “actor”) method  a scalable trust-region optimization algorithm for actor-critic methods. The
proposed algorithm uses a Kronecker-factored approximation to natural policy gradient that allows
the covariance matrix of the gradient to be inverted efﬁciently. To best of our knowledge  we are also
the ﬁrst to extend the natural policy gradient algorithm to optimize value functions via Gauss-Newton
approximation. In practice  the per-update computation cost of ACKTR is only 10% to 25% higher
than SGD-based methods. Empirically  we show that ACKTR substantially improves both sample
efﬁciency and the ﬁnal performance of the agent in the Atari environments [4] and the MuJoCo [26]
tasks compared to the state-of-the-art on-policy actor-critic method A2C [17] and the famous trust
region optimizer TRPO [21].
We make our source code available online at https://github.com/openai/baselines.

2 Background

2.1 Reinforcement learning and actor-critic methods

maximize the expected γ-discounted cumulative return J (θ) = Eπ[Rt] = Eπ[(cid:80)∞

We consider an agent interacting with an inﬁnite-horizon  discounted Markov Decision Process
(X  A  γ  P  r). At time t  the agent chooses an action at ∈ A according to its policy πθ(a|st) given
its current state st ∈ X . The environment in turn produces a reward r(st  at) and transitions to the
next state st+1 according to the transition probability P (st+1|st  at). The goal of the agent is to
i≥0 γir(st+i  at+i)]
with respect to the policy parameters θ. Policy gradient methods [28  25] directly parameterize a
policy πθ(a|st) and update parameter θ so as to maximize the objective J (θ). In its general form 

2

1M2M4M6M8M10MNumber of Timesteps01000200030004000500060007000Episode RewardsBeamRiderACKTRA2CTRPO1M2M4M6M8M10MNumber of Timesteps0100200300400Episode RewardsBreakoutACKTRA2CTRPO1M2M4M6M8M10MNumber of Timesteps201001020Episode RewardsPongACKTRA2CTRPO1M2M4M6M8M10MNumber of Timesteps025005000750010000125001500017500Episode RewardsQbertACKTRA2CTRPO1M2M4M6M8M10MNumber of Timesteps40060080010001200140016001800Episode RewardsSeaquestACKTRA2CTRPO1M2M4M6M8M10MNumber of Timesteps2004006008001000Episode RewardsSpaceInvadersACKTRA2CTRPOthe policy gradient is deﬁned as [22] 

∇θJ (θ) = Eπ[

∞(cid:88)

t=0

Ψt∇θ log πθ(at|st)] 

k−1(cid:88)

where Ψt is often chosen to be the advantage function Aπ(st  at)  which provides a relative measure
of value of each action at at a given state st. There is an active line of research [22] on designing an
advantage function that provides both low-variance and low-bias gradient estimates. As this is not
the focus of our work  we simply follow the asynchronous advantage actor critic (A3C) method [17]
and deﬁne the advantage function as the k-step returns with function approximation 

Aπ(st  at) =

γir(st+i  at+i) + γkV π

φ (st+k) − V π

φ (st) 

i=0

φ (st) is the value network  which provides an estimate of the expected sum of rewards from
where V π
φ (st) = Eπ[Rt]. To train the parameters of the value network 
the given state following policy π  V π
we again follow [17] by performing temporal difference updates  so as to minimize the squared
φ (st)||2.
difference between the bootstrapped k-step returns ˆRt and the prediction value 1

2|| ˆRt − V π

2.2 Natural gradient using Kronecker-factored approximation
To minimize a nonconvex function J (θ)  the method of steepest descent calculates the update
∆θ that minimizes J (θ + ∆θ)  subject to the constraint that ||∆θ||B < 1  where || · ||B is the
norm deﬁned by ||x||B = (xT Bx) 1
2   and B is a positive semideﬁnite matrix. The solution to the
constraint optimization problem has the form ∆θ ∝ −B−1∇θJ   where ∇θJ is the standard gradient.
When the norm is Euclidean  i.e.  B = I  this becomes the commonly used method of gradient
descent. However  the Euclidean norm of the change depends on the parameterization θ. This is
not favorable because the parameterization of the model is an arbitrary choice  and it should not
affect the optimization trajectory. The method of natural gradient constructs the norm using the
Fisher information matrix F   a local quadratic approximation to the KL divergence. This norm is
independent of the model parameterization θ on the class of probability distributions  providing a
more stable and effective update. However  since modern neural networks may contain millions of
parameters  computing and storing the exact Fisher matrix and its inverse is impractical  so we have
to resort to approximations.
A recently proposed technique called Kronecker-factored approximate curvature (K-FAC) [15] uses
a Kronecker-factored approximation to the Fisher matrix to perform efﬁcient approximate natural
gradient updates. We let p(y|x) denote the output distribution of a neural network  and L = log p(y|x)
denote the log-likelihood. Let W ∈ RCout×Cin be the weight matrix in the (cid:96)th layer  where Cout and
Cin are the number of output/input neurons of the layer. Denote the input activation vector to the
layer as a ∈ RCin  and the pre-activation vector for the next layer as s = W a. Note that the weight
gradient is given by ∇W L = (∇sL)a
(cid:124). K-FAC utilizes this fact and further approximates the block
F(cid:96) corresponding to layer (cid:96) as ˆF(cid:96) 

F(cid:96) = E[vec{∇W L}vec{∇W L}(cid:124)

(cid:124)

(cid:124)
] ⊗ E[∇sL(∇sL)

≈ E[aa
] and S denotes E[∇sL(∇sL)
(cid:124)

(cid:124)

] = E[aa

(cid:124)
(cid:124) ⊗ ∇sL(∇sL)

]

] := A ⊗ S := ˆF(cid:96) 

where A denotes E[aa
]. This approximation can be interpreted as
making the assumption that the second-order statistics of the activations and the backpropagated
derivatives are uncorrelated. With this approximation  the natural gradient update can be efﬁciently
computed by exploiting the basic identities (P ⊗Q)−1 = P −1⊗Q−1 and (P ⊗Q) vec(T ) = P T Q
(cid:124):

(cid:96) vec{∇WJ } = vec(cid:0)A−1 ∇WJ S−1(cid:1) .

vec(∆W ) = ˆF −1

From the above equation we see that the K-FAC approximate natural gradient update only requires
computations on matrices comparable in size to W . Grosse and Martens [6] have recently extended
the K-FAC algorithm to handle convolutional networks. Ba et al. [2] later developed a distributed
version of the method where most of the overhead is mitigated through asynchronous computa-
tion. Distributed K-FAC achieved 2- to 3-times speed-ups in training large modern classiﬁcation
convolutional networks.

3

Figure 2: In the Atari game of Atlantis  our agent (ACKTR) quickly learns to obtain rewards of 2 million in
1.3 hours  600 episodes of games  2.5 million timesteps. The same result is achieved by advantage actor critic
(A2C) in 10 hours  6000 episodes  25 million timesteps. ACKTR is 10 times more sample efﬁcient than A2C on
this game.

3 Methods

3.1 Natural gradient in actor-critic

Natural gradient was proposed to apply to the policy gradient method more than a decade ago
by Kakade [10]. But there still doesn’t exist a scalable  sample-efﬁcient  and general-purpose
instantiation of the natural policy gradient. In this section  we introduce the ﬁrst scalable and sample-
efﬁcient natural gradient algorithm for actor-critic methods: the actor-critic using Kronecker-factored
trust region (ACKTR) method. We use Kronecker-factored approximation to compute the natural
gradient update  and apply the natural gradient update to both the actor and the critic.
To deﬁne the Fisher metric for reinforcement learning objectives  one natural choice is to use the
policy function which deﬁnes a distribution over the action given the current state  and take the
expectation over the trajectory distribution:

where p(τ ) is the distribution of trajectories  given by p(s0)(cid:81)T

F = Ep(τ )[∇θ log π(at|st)(∇θ log π(at|st))

(cid:124)

] 

t=0 π(at|st)p(st+1|st  at). In practice 

one approximates the intractable expectation over trajectories collected during training.
We now describe one way to apply natural gradient to optimize the critic. Learning the critic can be
thought of as a least-squares function approximation problem  albeit one with a moving target. In the
setting of least-squares function approximation  the second-order algorithm of choice is commonly
Gauss-Newton  which approximates the curvature as the Gauss-Newton matrix G := E[J T J]  where
J is the Jacobian of the mapping from parameters to outputs [18]. The Gauss-Newton matrix is
equivalent to the Fisher matrix for a Gaussian observation model [14]; this equivalence allows us to
apply K-FAC to the critic as well. Speciﬁcally  we assume the output of the critic v is deﬁned to be
a Gaussian distribution p(v|st) ∼ N (v; V (st)  σ2). The Fisher matrix for the critic is deﬁned with
respect to this Gaussian output distribution. In practice  we can simply set σ to 1  which is equivalent
to the vanilla Gauss-Newton method.
If the actor and critic are disjoint  one can separately apply K-FAC updates to each using the metrics
deﬁned above. But to avoid instability in training  it is often beneﬁcial to use an architecture where the
two networks share lower-layer representations but have distinct output layers [17  27]. In this case 
we can deﬁne the joint distribution of the policy and the value distribution by assuming independence
of the two output distributions  i.e.  p(a  v|s) = π(a|s)p(v|s)  and construct the Fisher metric with
respect to p(a  v|s)  which is no different than the standard K-FAC except that we need to sample
the networks’ outputs independently. We can then apply K-FAC to approximate the Fisher matrix
Ep(τ )[∇ log p(a  v|s)∇ log p(a  v|s)T ] to perform updates simultaneously.
In addition  we use regular damping for regularization. We also follow [2] and perform the asyn-
chronous computation of second-order statistics and inverses required by the Kronecker approximation
to reduce computation time.

4

1M2M2.5MNumber of Timesteps0.5M1M2MEpisode RewardsAtlantisACKTRA2C0.00.20.40.60.81.01.21.4Hours0.5M1M2MEpisode RewardsAtlantisACKTRA2C0100200300400500600700Number of Episode0.5M1M2MEpisode RewardsAtlantisACKTRA2C3.2 Step-size Selection and trust-region optimization
Traditionally  natural gradient is performed with SGD-like updates  θ ← θ − ηF −1∇θL. But in the
context of deep RL  Schulman et al. [21] observed that such an update rule can result in large updates
to the policy  causing the algorithm to prematurely converge to a near-deterministic policy. They
advocate instead using a trust region approach  whereby the update is scaled down to modify the
policy distribution (in terms of KL divergence) by at most a speciﬁed amount. Therefore  we adopt
the trust region formulation of K-FAC introduced by [2]  choosing the effective step size η to be
)  where the learning rate ηmax and trust region radius δ are hyperparameters. If
min(ηmax 
the actor and the critic are disjoint  then we need to tune a different set of ηmax and δ separately for
both. The variance parameter for the critic output distribution can be absorbed into the learning rate
parameter for vanilla Gauss-Newton. On the other hand  if they share representations  we need to
tune one set of ηmax  δ  and also the weighting parameter of the training loss of the critic  with respect
to that of the actor.

(cid:113) 2δ

∆θ(cid:124) ˆF ∆θ

4 Related work

Natural gradient [1] was ﬁrst applied to policy gradient methods by Kakade [10]. Bagnell and
Schneider [3] further proved that the metric deﬁned in [10] is a covariant metric induced by the
path-distribution manifold. Peters and Schaal [19] then applied natural gradient to the actor-critic
algorithm. They proposed performing natural policy gradient for the actor’s update and using a
least-squares temporal difference (LSTD) method for the critic’s update. However  there are great
computational challenges when applying natural gradient methods  mainly associated with efﬁciently
storing the Fisher matrix as well as computing its inverse. For tractability  previous work restricted
the method to using the compatible function approximator (a linear function approximator). To
avoid the computational burden  Trust Region Policy Optimization (TRPO) [21] approximately
solves the linear system using conjugate gradient with fast Fisher matrix-vector products  similar
to the work of Martens [13]. This approach has two main shortcomings. First  it requires repeated
computation of Fisher vector products  preventing it from scaling to the larger architectures typically
used in experiments on learning from image observations in Atari and MuJoCo. Second  it requires
a large batch of rollouts in order to accurately estimate curvature. K-FAC avoids both issues by
using tractable Fisher matrix approximations and by keeping a running average of curvature statistics
during training. Although TRPO shows better per-iteration progress than policy gradient methods
trained with ﬁrst-order optimizers such as Adam [11]  it is generally less sample efﬁcient.
Several methods were proposed to improve the computational efﬁciency of TRPO. To avoid repeated
computation of Fisher-vector products  Wang et al. [27] solve the constrained optimization problem
with a linear approximation of KL divergence between a running average of the policy network and
the current policy network. Instead of the hard constraint imposed by the trust region optimizer 
Heess et al. [8] and Schulman et al. [23] added a KL cost to the objective function as a soft constraint.
Both papers show some improvement over vanilla policy gradient on continuous and discrete control
tasks in terms of sample efﬁciency.
There are other recently introduced actor-critic models that improve sample efﬁciency by introducing
experience replay [27]  [7] or auxiliary objectives [9]. These approaches are orthogonal to our work 
and could potentially be combined with ACKTR to further enhance sample efﬁciency.

5 Experiments

We conducted a series of experiments to investigate the following questions: (1) How does ACKTR
compare with the state-of-the-art on-policy method and common second-order optimizer baseline
in terms of sample efﬁciency and computational efﬁciency? (2) What makes a better norm for
optimization of the critic? (3) How does the performance of ACKTR scale with batch size compared
to the ﬁrst-order method?
We evaluated our proposed method  ACKTR  on two standard benchmark platforms. We ﬁrst
evaluated it on the discrete control tasks deﬁned in OpenAI Gym [5]  simulated by Arcade Learning
Environment [4]  a simulator for Atari 2600 games which is commonly used as a deep reinforcement
learning benchmark for discrete control. We then evaluated it on a variety of continuous control

5

Domain
Beamrider
Breakout

Pong
Q-bert
Seaquest

Space Invaders

ACKTR

A2C

TRPO (10 M)

Human level
5775.0
31.8
9.3
13455.0
20182.0
1652.0

Rewards
13581.4
735.7
20.9
21500.3
1776.0
19723.0

Episode Rewards
8148.1
581.6
19.9
15967.4
1754.0
1757.2

3279
4094
904
6422
N/A
14696

Episode Rewards
670.0
14.7
-1.2
971.8
810.4
465.1

8930
14464
4768
19168
N/A
N/A

Episode

N/A
N/A
N/A
N/A
N/A
N/A

Table 1: ACKTR and A2C results showing the last 100 average episode rewards attained after 50 million
timesteps  and TRPO results after 10 million timesteps. The table also shows the episode N  where N denotes
the ﬁrst episode for which the mean episode reward over the N th game to the (N + 100)th game crosses the
human performance level [16]  averaged over 2 random seeds.

benchmark tasks deﬁned in OpenAI Gym [5]  simulated by the MuJoCo [26] physics engine. Our
baselines are (a) a synchronous and batched version of the asynchronous advantage actor critic model
(A3C) [17]  henceforth called A2C (advantage actor critic)  and (b) TRPO [21]. ACKTR and the
baselines use the same model architecture except for the TRPO baseline on Atari games  with which
we are limited to using a smaller architecture because of the computing burden of running a conjugate
gradient inner-loop. See the appendix for other experiment details.

5.1 Discrete control

We ﬁrst present results on the standard six Atari 2600 games to measure the performance improvement
obtained by ACKTR. The results on the six Atari games trained for 10 million timesteps are shown
in Figure 1  with comparison to A2C and TRPO2. ACKTR signiﬁcantly outperformed A2C in terms
of sample efﬁciency (i.e.  speed of convergence per number of timesteps) by a signiﬁcant margin
in all games. We found that TRPO could only learn two games  Seaquest and Pong  in 10 million
timesteps  and performed worse than A2C in terms of sample efﬁciency.
In Table 1 we present the mean of rewards of the last 100 episodes in training for 50 million timesteps 
as well as the number of episodes required to achieve human performance [16] . Notably  on the
games Beamrider  Breakout  Pong  and Q-bert  A2C required respectively 2.7  3.5  5.3  and 3.0 times
more episodes than ACKTR to achieve human performance. In addition  one of the runs by A2C in
Space Invaders failed to match human performance  whereas ACKTR achieved 19723 on average 
12 times better than human performance (1652). On the games Breakout  Q-bert and Beamrider 
ACKTR achieved 26%  35%  and 67% larger episode rewards than A2C.
We also evaluated ACKTR on the rest of the Atari games; see Appendix for full results. We compared
ACKTR with Q-learning methods  and we found that in 36 out of 44 benchmarks  ACKTR is on par
with Q-learning methods in terms of sample efﬁciency  and consumed a lot less computation time.
Remarkably  in the game of Atlantis  ACKTR quickly learned to obtain rewards of 2 million in 1.3
hours (600 episodes)  as shown in Figure 2. It took A2C 10 hours (6000 episodes) to reach the same
performance level.

5.2 Continuous control

We ran experiments on the standard benchmark of continuous control tasks deﬁned in OpenAI Gym
[5] simulated in MuJoCo [26]  both from low-dimensional state-space representation and directly
from pixels. In contrast to Atari  the continuous control tasks are sometimes more challenging due to
high-dimensional action spaces and exploration. The results of eight MuJoCo environments trained
for 1 million timesteps are shown in Figure 3. Our model signiﬁcantly outperformed baselines on six
out of eight MuJoCo tasks and performed competitively with A2C on the other two tasks (Walker2d
and Swimmer).
We further evaluated ACKTR for 30 million timesteps on eight MuJoCo tasks and in Table 2 we
present mean rewards of the top 10 consecutive episodes in training  as well as the number of

2The A2C and TRPO Atari baseline results are provided to us by the OpenAI team  https://github.com/

openai/baselines.

6

Figure 3: Performance comparisons on eight MuJoCo environments trained for 1 million timesteps (1 timestep
equals 4 frames). The shaded region denotes the standard deviation over 3 random seeds.

Figure 4: Performance comparisons on 3 MuJoCo environments from image observations trained for 40 million
timesteps (1 timestep equals 4 frames).

episodes to reach a certain threshold deﬁned in [7]. As shown in Table 2  ACKTR reaches the
speciﬁed threshold faster on all tasks  except for Swimmer where TRPO achieves 4.1 times better
sample efﬁciency. A particularly notable case is Ant  where ACKTR is 16.4 times more sample
efﬁcient than TRPO. As for the mean reward score  all three models achieve results comparable with
each other with the exception of TRPO  which in the Walker2d environment achieves a 10% better
reward score.
We also attempted to learn continuous control policies directly from pixels  without providing low-
dimensional state space as an input. Learning continuous control policies from pixels is much
more challenging than learning from the state space  partially due to the slower rendering time
compared to Atari (0.5 seconds in MuJoCo vs 0.002 seconds in Atari). The state-of-the-art actor-
critic method A3C [17] only reported results from pixels on relatively simple tasks  such as Pendulum 
Pointmass2D  and Gripper. As shown in Figure 4 we can see that our model signiﬁcantly outperforms
A2C in terms of ﬁnal episode reward after training for 40 million timesteps. More speciﬁcally 
on Reacher  HalfCheetah  and Walker2d our model achieved a 1.6  2.8  and 1.7 times greater
ﬁnal reward compared to A2C. The videos of trained policies from pixels can be found at https:
//www.youtube.com/watch?v=gtM87w1xGoM. Pretrained model weights are available at https:
//github.com/emansim/acktr.

5.3 A better norm for critic optimization?

The previous natural policy gradient method applied a natural gradient update only to the actor. In
our work  we propose also applying a natural gradient update to the critic. The difference lies in the
norm with which we choose to perform steepest descent on the critic; that is  the norm || · ||B deﬁned
in section 2.2. In this section  we applied ACKTR to the actor  and compared using a ﬁrst-order
method (i.e.  Euclidean norm) with using ACKTR (i.e.  the norm deﬁned by Gauss-Newton) for critic
optimization. Figures 5 (a) and (b) show the results on the continuous control task HalfCheetah and
the Atari game Breakout. We observe that regardless of which norm we use to optimize the critic 
there are improvements brought by applying ACKTR to the actor compared to the baseline A2C.
However  the improvements brought by using the Gauss-Newton norm for optimizing the critic are
more substantial in terms of sample efﬁciency and episode rewards at the end of training. In addition 

7

200K400K600K800K1MNumber of Timesteps020040060080010001200Episode RewardInvertedPendulumACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps0200040006000800010000Episode RewardInvertedDoublePendulumACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps706050403020100Episode RewardReacherACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps05001000150020002500300035004000Episode RewardHopperACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps40200204060Episode RewardSwimmerACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps2000200400600800100012001400Episode RewardWalker2dACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps500050010001500200025003000Episode RewardHalfCheetahACKTRA2CTRPO200K400K600K800K1MNumber of Timesteps15001000500050010001500Episode RewardAntACKTRA2CTRPO10M20M40MNumber of Timesteps14121086420Episode RewardReacher (pixels)ACKTRA2C10M20M40MNumber of Timesteps0500100015002000Episode RewardWalker2d (pixels)ACKTRA2C10M20M40MNumber of Timesteps1000500050010001500200025003000Episode RewardHalfCheetah (pixels)ACKTRA2CDomain

Ant

HalfCheetah

Hopper

IP
IDP

Reacher
Swimmer
Walker2d

Threshold
3500 (6000)
4700 (4800)
2000 (3800)
950 (950)
9100 (9100)
-7 (-3.75)
90 (360)

3000 (N/A)

ACKTR

A2C

TRPO

Episodes
60156
21033
39426
29267
78519
14940
1571
27720

Rewards
4621.6
5586.3
3915.9
1000.0
9356.0
-1.5
138.0
6198.8

Episodes Rewards
4870.5
5343.7
3915.3
1000.0
9356.1
-1.7
140.7
5874.9

3660
12980
17033
6831
41996
3325
6475
15043

Episodes Rewards
5095.0
106186
5704.7
21152
3755.0
33481
1000.0
10982
82694
9320.0
-2.0
20591
136.4
11516
6874.1
26828

Table 2: ACKTR  A2C  and TRPO results  showing the top 10 average episode rewards attained within 30
million timesteps  averaged over the 3 best performing random seeds out of 8 random seeds. “Episode” denotes
the smallest N for which the mean episode reward over the N th to the (N + 10)th game crosses a certain
threshold. The thresholds for all environments except for InvertedPendulum (IP) and InvertedDoublePendulum
(IDP) were chosen according to Gu et al. [7]  and in brackets we show the reward threshold needed to solve the
environment according to the OpenAI Gym website [5].

the Gauss-Newton norm also helps stabilize the training  as we observe larger variance in the results
over random seeds with the Euclidean norm.
Recall that the Fisher matrix for the critic is constructed using the output distribution of the critic 
a Gaussian distribution with variance σ. In vanilla Gauss-Newton  σ is set to 1. We experimented
with estimating σ using the variance of the Bellman error  which resembles estimating the variance
of the noise in regression analysis. We call this method adaptive Gauss-Newton. However  we ﬁnd
adaptive Gauss-Newton doesn’t provide any signiﬁcant improvement over vanilla Gauss-Newton.
(See detailed comparisons on the choices of σ in Appendix.

5.4 How does ACKTR compare with A2C in wall-clock time?

We compared ACKTR to the baselines A2C and TRPO in terms of wall-clock time. Table 3 shows the
average timesteps per second over six Atari games and eight MuJoCo (from state space) environments.
The result is obtained with the same experiment setup as previous experiments. Note that in MuJoCo
tasks episodes are processed sequentially  whereas in the Atari environment episodes are processed in
parallel; hence more frames are processed in Atari environments. From the table we see that ACKTR
only increases computing time by at most 25% per timestep  demonstrating its practicality with large
optimization beneﬁts.

(Timesteps/Second)

batch size
ACKTR

A2C
TRPO

Atari
160
753
1038
161

80
712
1010
160

640
852
1162
177

1000
519
624
593

MuJoCo
2500
551
650
619

25000
582
651
637

Table 3: Comparison of computational cost. The average timesteps per second over six Atari games and eight
MuJoCo tasks during training for each algorithms. ACKTR only increases computing time at most 25% over
A2C.

5.5 How do ACKTR and A2C perform with different batch sizes?

In a large-scale distributed learning setting  large batch size is used in optimization. Therefore  in
such a setting  it is preferable to use a method that can scale well with batch size. In this section 
we compare how ACKTR and the baseline A2C perform with respect to different batch sizes. We
experimented with batch sizes of 160 and 640. Figure 5 (c) shows the rewards in number of timesteps.
We found that ACKTR with a larger batch size performed as well as that with a smaller batch size.
However  with a larger batch size  A2C experienced signiﬁcant degradation in terms of sample
efﬁciency. This corresponds to the observation in Figure 5 (d)  where we plotted the training curve
in terms of number of updates. We see that the beneﬁt increases substantially when using a larger
batch size with ACKTR compared to with A2C. This suggests there is potential for large speed-ups
with ACKTR in a distributed setting  where one needs to use large mini-batches; this matches the
observation in [2].

8

(a)

(b)

(c)

(d)

Figure 5: (a) and (b) compare optimizing the critic (value network) with a Gauss-Newton norm (ACKTR)
against a Euclidean norm (ﬁrst order). (c) and (d) compare ACKTR and A2C with different batch sizes.

6 Conclusion

In this work we proposed a sample-efﬁcient and computationally inexpensive trust-region-
optimization method for deep reinforcement learning. We used a recently proposed technique
called K-FAC to approximate the natural gradient update for actor-critic methods  with trust region
optimization for stability. To the best of our knowledge  we are the ﬁrst to propose optimizing both
the actor and the critic using natural gradient updates. We tested our method on Atari games as well
as the MuJoCo environments  and we observed 2- to 3-fold improvements in sample efﬁciency on
average compared with a ﬁrst-order gradient method (A2C) and an iterative second-order method
(TRPO). Because of the scalability of our algorithm  we are also the ﬁrst to train several non-trivial
tasks in continuous control directly from raw pixel observation space. This suggests that extending
Kronecker-factored natural gradient approximations to other algorithms in reinforcement learning is
a promising research direction.

Acknowledgements

We would like to thank the OpenAI team for their generous support in providing baseline results and
Atari environment preprocessing codes. We also want to thank John Schulman for helpful discussions.

References
[1] S. I. Amari. Natural gradient works efﬁciently in learning. Neural Computation  10(2):251–276 

1998.

[2] J. Ba  R. Grosse  and J. Martens. Distributed second-order optimization using Kronecker-

factored approximations. In ICLR  2017.

[3] J. A. Bagnell and J. G. Schneider. Covariant policy search. In IJCAI  2003.

[4] M. G. Bellemare  Y. Naddaf  J. Veness  and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research  47:253–279 
2013.

[5] G. Brockman  V. Cheung  L. Pettersson  J. Schneider  J. Schulman  J. Tang  and W. Zaremba.

OpenAI Gym. arXiv preprint arXiv:1606.01540  2016.

[6] R. Grosse and J. Martens. A Kronecker-factored approximate Fisher matrix for convolutional

layers. In ICML  2016.

[7] S. Gu  T. Lillicrap  Z. Ghahramani  R. E. Turner  and S. Levine. Q-prop: Sample-efﬁcient policy

gradient with an off-policy critic. In ICLR  2017.

[8] N. Heess  D. TB  S. Sriram  J. Lemmon  J. Merel  G. Wayne  Y. Tassa  T. Erez  Z. Wang 
S. M. A. Eslami  M. Riedmiller  and D. Silver. Emergence of locomotion behaviours in rich
environments. arXiv preprint arXiv:1707.02286  2017.

[9] M. Jaderberg  V. Mnih  W. M. Czarnecki  T. Schaul  J. Z. Leibo  D. Silver  and K. Kavukcuoglu.

Reinforcement learning with unsupervised auxiliary tasks. In ICLR  2017.

[10] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems 

2002.

9

200K400K600K800K1MNumber of Timesteps500050010001500200025003000Episode RewardHalfCheetahACKTR (Both Actor and Critic)ACKTR (Only Actor)A2C1M2M4M6M8M10MNumber of Timesteps0100200300400Episode RewardsBreakoutACKTR(Both Actor and Critic)ACKTR(Only Actor)A2C1M2M4M6M8M10MNumber of Timesteps0100200300400500Episode RewardsBreakoutACKTR (640)ACKTR (160)A2C (640)A2C (160)0100002000030000400005000060000Number of Updates0100200300400500Episode RewardsBreakoutACKTR (640)ACKTR (160)A2C (640)A2C (160)[11] D. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR  2015.

[12] T. P. Lillicrap  J. J. Hunt  A. Pritzel  N. Heess  T. Erez  Y. Tassa  D. Silver  and D. Wierstra.

Continuous control with deep reinforcement learning. In ICLR  2016.

[13] J. Martens. Deep learning via Hessian-free optimization. In ICML-10  2010.

[14] J. Martens. New insights and perspectives on the natural gradient method. arXiv preprint

arXiv:1412.1193  2014.

[15] J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate

curvature. In ICML  2015.

[16] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves 
M. Riedmiller  A. K. Fidjeland  G. Ostrovski  S. Petersen  C. Beattie  A. Sadik  I. Antonoglou 
H. King  D. Kumaran  D. Wierstra  S. Legg  and D. Hassabis. Human-level control through
deep reinforcement learning. Nature  518(7540):529–533  2015.

[17] V. Mnih  A. Puigdomenech Badia  M. Mirza  A. Graves  T. P. Lillicrap  T. Harley  D. Silver  and

K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML  2016.

[18] J. Nocedal and S. Wright. Numerical Optimization. Springer  2006.

[19] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing  71(7-9):1180–1190  2008.

[20] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.

Neural Computation  2002.

[21] J. Schulman  S. Levine  P. Abbeel  M. I. Jordan  and P. Moritz. Trust region policy optimization.

In Proceedings of the 32nd International Conference on Machine Learning (ICML)  2015.

[22] J. Schulman  P. Moritz  S. Levine  M. Jordan  and P. Abbeel. High-dimensional continuous
control using generalized advantage estimation. In Proceedings of the International Conference
on Learning Representations (ICLR)  2016.

[23] J. Schulman  F. Wolski  P. Dhariwal  A. Radford  and O. Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347  2017.

[24] D. Silver  A. Huang  C. J. Maddison  A. Guez  L. Sifre  G. Van Den Driessche  J. Schrittwieser 
I. Antonoglou  V. Panneershelvam  M. Lanctot  S. Dieleman  D. Grewe  J. Nham  N. Kalch-
brenner  I. Sutskever  T. Lillicrap  M. Leach  K. Kavukcuoglu  T. Graepel  and D. Hassabis.
Mastering the game of Go with deep neural networks and tree search. Nature  529(7587):484–
489  2016.

[25] R. S. Sutton  D. A. McAllester  S. Singh  and Y. Mansour. Policy gradient methods for reinforce-
ment learning with function approximation. In Advances in Neural Information Processing
Systems 12  2000.

[26] E. Todorov  T. Erez  and Y. Tassa. MuJoCo: A physics engine for model-based control.

IEEE/RSJ International Conference on Intelligent Robots and Systems  2012.

[27] Z. Wang  V. Bapst  N. Heess  V. Mnih  R. Munos  K. Kavukcuoglu  and N. de Freitas. Sample

efﬁcient actor-critic with experience replay. In ICLR  2016.

[28] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning  8(3):229–256  1992.

10

,Yuhuai Wu
Elman Mansimov
Roger Grosse
Shun Liao
Jimmy Ba
Ruqi Zhang
Christopher De Sa