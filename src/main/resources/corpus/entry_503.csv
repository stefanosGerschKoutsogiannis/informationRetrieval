2013,An Approximate  Efficient LP Solver for LP Rounding,Many problems in machine learning can be solved by rounding the solution of an appropriate linear program. We propose a scheme that is based on a quadratic program relaxation which allows us to use parallel stochastic-coordinate-descent to approximately solve large linear programs efficiently. Our software is an order of magnitude faster than Cplex (a commercial linear programming solver) and yields similar solution quality. Our results include a novel perturbation analysis of a quadratic-penalty formulation of linear programming and a convergence result  which we use to derive running time and quality guarantees.,An Approximate  Efﬁcient Solver for LP Rounding

Srikrishna Sridhar1  Victor Bittorf1  Ji Liu1  Ce Zhang1

Christopher R´e2  Stephen J. Wright1

1Computer Sciences Department  University of Wisconsin-Madison  Madison  WI 53706

2Computer Science Department  Stanford University  Stanford  CA 94305
{srikris vbittorf ji-liu czhang swright}@cs.wisc.edu

chrismre@cs.stanford.edu

Abstract

Many problems in machine learning can be solved by rounding the solution of an
appropriate linear program (LP). This paper shows that we can recover solutions
of comparable quality by rounding an approximate LP solution instead of the ex-
act one. These approximate LP solutions can be computed efﬁciently by applying
a parallel stochastic-coordinate-descent method to a quadratic-penalty formula-
tion of the LP. We derive worst-case runtime and solution quality guarantees of
this scheme using novel perturbation and convergence analysis. Our experiments
demonstrate that on such combinatorial problems as vertex cover  independent set
and multiway-cut  our approximate rounding scheme is up to an order of mag-
nitude faster than Cplex (a commercial LP solver) while producing solutions of
similar quality.

Introduction

1
A host of machine-learning problems can be solved effectively as approximations of such NP-hard
combinatorial problems as set cover  set packing  and multiway-cuts [8  11  16  22]. A popular
scheme for solving such problems is called LP rounding [22  chs. 12-26]  which consists of the
following three-step process: (1) construct an integer (binary) linear program (IP) formulation of a
given problem; (2) relax the IP to an LP by replacing the constraints x 2{ 0  1} by x 2 [0  1]; and
(3) round an optimal solution of the LP to create a feasible solution for the original IP problem. LP
rounding is known to work well on a range of hard problems  and comes with theoretical guarantees
for runtime and solution quality.
The Achilles’ heel of LP-rounding is that it requires solutions of LPs of possibly extreme scale.
Despite decades of work on LP solvers  including impressive advances during the 1990s  commercial
codes such as Cplex or Gurobi may not be capable of handling problems of the required scale. In this
work  we propose an approximate LP solver suitable for use in the LP-rounding approach  for very
large problems. Our intuition is that in LP rounding  since we ultimately round the LP to obtain an
approximate solution of the combinatorial problem  a crude solution of the LP may sufﬁce. Hence 
an approach that can ﬁnd approximate solutions of large LPs quickly may be suitable  even if it is
inefﬁcient for obtaining highly accurate solutions.
This paper focuses on the theoretical and algorithmic aspects of ﬁnding approximate solutions to an
LP  for use in LP-rounding schemes. Our three main technical contributions are as follows: First  we
show that one can approximately solve large LPs by forming convex quadratic programming (QP)
approximations  then applying stochastic coordinate descent to these approximations. Second  we
derive a novel convergence analysis of our method  based on Renegar’s perturbation theory for linear
programming [17]. Finally  we derive bounds on runtime as well as worst-case approximation ratio
of our rounding schemes. Our experiments demonstrate that our approach  called Thetis  produces
solutions of comparable quality to state-of-the-art approaches on such tasks as noun-phrase chunking
and entity resolution. We also demonstrate  on three different classes of combinatorial problems  that
Thetis can outperform Cplex (a state-of-the-art commercial LP and IP solver) by up to an order of
magnitude in runtime  while achieving comparable solution quality.

1

Related Work. Recently  there has been some focus on the connection between LP relaxations
and maximum a posteriori (MAP) estimation problems [16  19]. Ravikumar et. al [16] proposed
rounding schemes for iterative LP solvers to facilitate MAP inference in graphical models. In con-
trast  we propose to use stochastic descent methods to solve a QP relaxation; this allows us to take
advantage of recent results on asynchronous parallel methods of this type [12 14]. Recently  Makari
et. al [13] propose an intriguing parallel scheme for packing and covering problems. In contrast  our
results apply to more general LP relaxations  including set-partitioning problems like multiway-cut.
Additionally  the runtime of our algorithm is less sensitive to approximation error. For an error " 
the bound on runtime of the algorithm in [13] grows as "5  while the bound on our algorithm’s
runtime grows as "2.
2 Background: Approximating NP-hard problems with LP Rounding
In this section  we review the theory of LP-rounding based approximation schemes for NP-hard
combinatorial problems. We use the vertex cover problem as an example  as it is the simplest
nontrivial setting that exposes the main ideas of this approach.

Preliminaries. For a minimization problem   an algorithm ALG is an ↵-factor approximation
for   for some ↵> 1  if any solution produced by ALG has an objective value at most ↵ times
the value of an optimal (lowest cost) solution. For some problems  such as vertex cover  there is a
constant-factor approximation scheme (↵ = 2). For others  such as set cover  the value of ↵ can be
as large as O(log N )  where N is the number of sets.
An LP-rounding based approximation scheme for the problem  ﬁrst constructs an IP formulation
of  which we denote as “P ”. This step is typically easy to perform  but the IP formulation P is  in
theory  as hard to solve as the original problem . In this work  we consider applications in which
the only integer variables in the IP formulation are binary variables x 2{ 0  1}. The second step in
LP rounding is a relax / solve step: We relax the constraints in P to obtain a linear program LP (P ) 
replacing the binary variables with continuous variables in [0  1]  then solve LP (P ). The third step
is to round the solution of LP (P ) to an integer solution which is feasible for P   thus yielding a
candidate solution to the original problem . The focus of this paper is on the relax / solve step 
which is usually the computational bottleneck in an LP-rounding based approximation scheme.

Example: An Oblivious-Rounding Scheme For Vertex Cover. Let G(V  E) denote a graph with
vertex set V and undirected edges E ✓ (V ⇥ V ). Let cv denote a nonnegative cost associated with
each vertex v 2 V . A vertex cover of a graph is a subset of V such that each edge e 2 E is incident
to at least one vertex in this set. The minimum-cost vertex cover is the one that minimizes the sum of
terms cv  summed over the vertices v belonging to the cover. Let us review the “construct ” “relax /
solve ” and “round” phases of an LP-rounding based approximation scheme applied to vertex cover.
In the “construct” phase  we introduce binary variables xv 2{ 0  1}  8v 2 V   where xv is set to 1 if
the vertex v 2 V is selected in the vertex cover and 0 otherwise. The IP formulation is as follows:
(1)

cvxv s.t. xu + xv  1 for (u  v) 2 E and xv 2{ 0  1} for v 2 V.

min

x Xv2V
x Xv2V

min

Relaxation yields the following LP

cvxv s.t. xu + xv  1 for (u  v) 2 E and xv 2 [0  1] for v 2 V.

(2)

A feasible solution of the LP relaxation (2) is called a “fractional solution” of the original problem.
In the “round” phase  we generate a valid vertex cover by simply choosing the vertices v 2 V whose
2. It is easy to see that the vertex cover generated by such a rounding
fractional solution xv  1
scheme costs no more than twice the cost of the fractional solution. If the fractional solution chosen
for rounding is an optimal solution of (2)  then we arrive at a 2-factor approximation scheme for
vertex cover. We note here an important property: The rounding algorithm can generate feasible
integral solutions while being oblivious of whether the fractional solution is an optimal solution of
(2). We formally deﬁne the notion of an oblivious rounding scheme as follows.
Deﬁnition 1. For a minimization problem  with an IP formulation P whose LP relaxation is
denoted by LP(P )  a -factor ‘oblivious’ rounding scheme converts any feasible point xf 2 LP(P )
to an integral solution xI 2 P with cost at most  times the cost of LP(P ) at xf .

2

Problem Family Approximation Factor

Machine Learning Applications

Set Covering
Set Packing
Multiway-cut

log(N ) [20]
es + o(s) [1]
3/2  1/k [5]

Graphical Models Heuristic

Classiﬁcation [3]  Multi-object tracking [24].
MAP-inference [19]  Natural language [9].
Computer vision [4]  Entity resolution [10].
Semantic role labeling [18]  Clustering [21].

Figure 1: LP-rounding schemes considered in this paper. The parameter N refers to the number of
sets; s refers to s-column sparse matrices; and k refers to the number of terminals. e is the Euler’s
constant.

Given a -factor oblivious algorithm ALG to the problem   one can construct a -factor approxi-
mation algorithm for  by using ALG to round an optimal fractional solution of LP(P ). When we
have an approximate solution for LP(P ) that is feasible for this problem  rounding can produce an
↵-factor approximation algorithm for  for a factor ↵ slightly larger than   where the difference
between ↵ and  takes account of the inexactness in the approximate solution of LP(P ). Many
LP-rounding schemes (including the scheme for vertex cover discussed in Section 2) are oblivious.
We implemented the oblivious LP-rounding algorithms in Figure 1 and report experimental results
in Section 4.
3 Main results
In this section  we describe how we can solve LP relaxations approximately  in less time than tradi-
tional LP solvers  while still preserving the formal guarantees of rounding schemes. We ﬁrst deﬁne a
notion of approximate LP solution and discuss its consequences for oblivious rounding schemes. We
show that one can use a regularized quadratic penalty formulation to compute these approximate LP
solutions. We then describe a stochastic-coordinate-descent (SCD) algorithm for obtaining approx-
imate solutions of this QP  and mention enhancements of this approach  speciﬁcally  asynchronous
parallel implementation and the use of an augmented Lagrangian framework. Our analysis yields a
worst-case complexity bound for solution quality and runtime of the entire LP-rounding scheme.
3.1 Approximating LP Solutions
Consider the LP in the following standard form

min cT x s.t. Ax = b  x  0 
where c 2 Rn  b 2 Rm  and A 2 Rm⇥n and its corresponding dual

(3)

max bT u s.t. c  AT u  0.

(4)
Let x⇤ denote an optimal primal solution of (3). An approximate LP solution ˆx that we use for LP-
rounding may be infeasible and have objective value different from the optimum cT x⇤. We quantify
the inexactness in an approximate LP solution as follows.
Deﬁnition 2. A point ˆx is an (✏  )-approximate solution of the LP (3) if ˆx  0 and there exists
constants ✏> 0 and > 0 such that

kAˆx  bk1  ✏

and

|cT ˆx  cT x⇤| |cT x⇤|.

Using Deﬁnitions 1 and 2  it is easy to see that a -factor oblivious rounding scheme can round
a (0  ) approximate solution to produce a feasible integral solution whose cost is no more than
(1 + ) times the optimal solution of the P . The factor (1 + ) arises because the rounding
algorithm does not have access to an optimal fractional solution. To cope with the infeasibility  we
convert an (✏  )-approximate solution to a (0  ˆ) approximate solution where ˆ is not too large. For
vertex cover (2)  we prove the following result in Appendix C. (Here  ⇧[0 1]n(·) denotes projection
onto the unit hypercube in Rn.)
Lemma 3. Let ˆx be an ("  ) approximate solution to the linear program (2) with " 2 [0  1). Then 
˜x =⇧ [0 1]n((1  ")1 ˆx) is a (0  (1  ")1)-approximate solution.
Since ˜x is a feasible solution for (2)  the oblivious rounding scheme in Section 2 results in an 2(1 +
(1 ")1) factor approximation algorithm. In general  constructing (0  ˆ) from (✏  ) approximate
solutions requires reasoning about the structure of a particular LP. In Appendix C  we establish
statements analogous to Lemma 3 for packing  covering and multiway-cut problems.

3

3.2 Quadratic Programming Approximation to the LP
We consider the following regularized quadratic penalty approximation to the LP (3)  parameterized
by a positive constant   whose solution is denoted by x():

x() := arg min
x0

f(x) := cT x  ¯uT (Ax  b) +


2kAx  bk2 +

1
2kx  ¯xk2 

(5)

where ¯u 2 Rm and ¯x 2 Rn are arbitrary vectors. (In practice  ¯u and ¯x may be chosen as ap-
proximations to the dual and primal solutions of (3)  or simply set to zero.) The quality of the
approximation (5) depends on the conditioning of underlying linear program (3)  a concept that was
studied by Renegar [17]. Denoting the data for problem (3) by d := (A  b  c)  we consider perturba-
tions d := (A  b  c) such that the linear program deﬁned by d + d is primal infeasible. The
primal condition number P is the inﬁmum of the ratios kdk/kdk over all such vectors d. The
dual condition number D is deﬁned analogously. (Clearly both P and D are in the range [0  1];
smaller values indicate poorer conditioning.) We have the following result  which is proven in the
supplementary material.
Theorem 4. Suppose that P and D are both positive  and let (x⇤  u⇤) be any primal-dual solution
pair for (3)  (4). If we deﬁne C⇤ := max(kx⇤  ¯xk ku⇤  ¯uk)  then the unique solution x() of (5)
satisﬁes

If in addition the parameter  

kAx()  bk  (1/)(1 + p2)C⇤ 
 25C⇤

kdk min(P  D)  then we have
+ 6C2

|cT x⇤  cT x()|

kx()  x⇤k  p6C⇤.
⇤ + p6k¯xkC⇤ .

2P D

10C⇤

1

In practice  we solve (5) approximately  using an algorithm whose complexity depends on the thresh-
old ¯✏ for which the objective is accurate to within ¯✏. That is  we seek ˆx such that

1kˆx  x()k2  f(ˆx)  f(x())  ¯✏ 

where the left-hand inequality follows from the fact that f is strongly convex with modulus 1.
If we deﬁne

¯✏ :=

C2
20
3   C20 :=

25C⇤

2kdkP D

 

(6)

then by combining some elementary inequalities with the results of Theorem 4  we obtain the bounds

1

 25C⇤

⇤ + p6k¯xkC⇤  

P D

+ 6C2

|cT ˆx  cT x⇤|
The following result is almost an immediate consequence.
Theorem 5. Suppose that P and D are both positive and let (x⇤  u⇤) be any primal-dual optimal
pair. Suppose that C⇤ is deﬁned as in Theorem 4. Then for any given positive pair (✏  )  we have
that ˆx satisﬁes the inequalities in Deﬁnition 2 provided that  satisﬁes the following three lower
bounds:

kAˆx  bk 

1

(1 + p2)C⇤ +

25C⇤

2P D .

 

 

 

10C⇤

 

1

kdk min(P   D)

|cT x⇤| 25C⇤
✏(1 + p2)C⇤ +

P D

1

+ 6C2

⇤ + p6k¯xkC⇤  
2P D .

25C⇤

For an instance of vertex cover with n nodes and m edges  we can show that 1
n)1/2) and 1
We therefore obtain  = O(m1/2n1/2(m + n)(min{✏  |cT x⇤|})1).

P = O(n1/2(m +
D = O((m + n)1/2) (see Appendix D). The values ¯x = 1 and ¯u = ~0 yield C⇤  pm.

4

Algorithm 1 SCD method for (5)
1: Choose x0 2 Rn; j 0
2: loop
3:
4:

Choose i(j) 2{ 1  2  . . .   n} randomly with equal probability;
Deﬁne xj+1 from xj by setting [xj+1]i(j) max(0  [xj]i(j)  (1/Lmax)[rf(xj)]i(j)) 
leaving other components unchanged;
j j + 1;

5:
6: end loop

3.3 Solving the QP Approximation: Coordinate Descent
We propose the use of a stochastic coordinate descent (SCD) algorithm [12] to solve (5). Each step
of SCD chooses a component i 2{ 1  2  . . .   n} and takes a step in the ith component of x along the
partial gradient of (5) with respect to this component  projecting if necessary to retain nonnegativity.
This simple procedure depends on the following constant Lmax  which bounds the diagonals of the
Hessian in the objective of (5):

Lmax = ( max

i=1 2 ... n

AT

:i A:i) + 1 

(7)

where A:i denotes the ith column of A. Algorithm 1 describes the SCD method. Convergence
results for Algorithm 1 can be obtained from [12]. In this result  E(·) denotes expectation over
all the random variables i(j) indicating the update indices chosen at each iteration. We need the
following quantities:

l :=

1


  R := sup

j=1 2 ...nkxj  x()k2 

(8)

where xj denotes the jth iterate of the SCD algorithm. (Note that R bounds the maximum distance
that the iterates travel from the solution x() of (5).)
Theorem 6. For Algorithm 1 we have

(f(x0)  f⇤ )◆  
Ekxj  x()k2 +
where f⇤ := f(x()). We obtain high-probability convergence of f(xj) to f⇤ in the following
sense: For any ⌘ 2 (0  1) and any small ¯✏  we have P (f(xj)  f⇤ < ¯✏)  1  ⌘  provided that

E(f(xj)  f⇤ ) ✓1 

Lmax

Lmax

2

l

2

n(l + Lmax)◆j✓R2 +
(f(x0)  f⇤ )◆ .

Lmax

2

Lmax

2⌘¯✏ ✓R2 +

log

n(l + Lmax)

j 

l

Worst-Case Complexity Bounds. We now combine the analysis in Sections 3.2 and 3.3 to derive
a worst-case complexity bound for our approximate LP solver. Supposing that the columns of A
have norm O(1)  we have from (7) and (8) that l = 1 and Lmax = O(). Theorem 6 indicates
that we require O(n2) iterations to solve (5) (modulo a log term). For the values of  described in
Section 3.2  this translates to a complexity estimate of O(m3n2/✏2).
In order to obtain the desired accuracy in terms of feasibility and function value of the LP (captured
by ✏) we need to solve the QP to within the different  tighter tolerance ¯✏ introduced in (6). Both
tolerances are related to the choice of penalty parameter  in the QP. Ignoring here the dependence
on dimensions m and n  we note the relationships  ⇠ ✏1 (from Theorem 5) and ¯✏ ⇠ 3 ⇠
✏3 (from (6)). Expressing all quantities in terms of ✏  and using Theorem 6  we see an iteration
complexity of ✏2 for SCD (ignoring log terms). The linear convergence rate of SCD is instrumental
to this favorable value. By contrast  standard variants of stochastic-gradient descent (SGD) applied
to the QP yield poorer complexity. For diminishing-step or constant-step variants of SGD  we see
complexity of ✏7  while for robust SGD  we see ✏10. (Besides the inverse dependence on ¯✏ or its
square in the analysis of these methods  there is a contribution of order ✏2 from the conditioning of
the QP.)
3.4 Enhancements
We mention two important enhancements that improve the efﬁciency of the approach outlined above.
The ﬁrst is an asynchronous parallel implementation of Algorithm 1 and the second is the use of an
augmented Lagrangian framework rather than “one-shot” approximation by the QP in (5).

5

Task
CoNLL
TAC-KBP

Formulation

Skip-chain CRF

Factor graph

NNZ

P
PV
25M 51M .87
62K 115K .79

Thetis
F1
.89
.79

R
.90
.79

Rank
10/13
6/17

P
.86
.80

Gibbs Sampling

Rank
10/13
6/17

R
.90
.80

F1
.88
.80

Figure 2: Solution quality of our LP-rounding approach on two tasks. PV is the number of primal
variables and NNZ is the number of non-zeros in the constraint matrix of the LP in standard form.
The rank indicates where we would been have placed  had we participated in the competition.
Asynchronous Parallel SCD. An asynchronous parallel version of Algorithm 1  described in
[12]  is suitable for execution on multicore  shared-memory architectures. Each core  executing
a single thread  has access to the complete vector x. Each thread essentially runs its own version
of Algorithm 1 independently of the others  choosing and updating one component i(j) of x on
each iteration. Between the time a thread reads x and performs its update  x usually will have been
updated by several other threads. Provided that the number of threads is not too large (according to
criteria that depends on n and on the diagonal dominance properties of the Hessian matrix)  and the
step size is chosen appropriately  the convergence rate is similar to the serial case  and near-linear
speedup is observed.

Is Our Approximate LP-Rounding Scheme Useful in Graph Analysis Tasks?

Augmented Lagrangian Framework.
It is well known (see for example [2  15]) that the
quadratic-penalty approach can be extended to an augmented Lagrangian framework  in which a
sequence of problems of the form (5) are solved  with the primal and dual solution estimates ¯x and
¯u (and possibly the penalty parameter ) updated between iterations. Such a “proximal method of
multipliers” for LP was described in [23]. We omit a discussion of the convergence properties of
the algorithm here  but note that the quality of solution depends on the values of ¯x  ¯u and  at the
last iteration before convergence is declared. By applying Theorem 5  we note that the constant C⇤
is smaller when ¯x and ¯u are close to the primal and dual solution sets  thus improving the approx-
imation and reducing the need to increase  to a larger value to obtain an approximate solution of
acceptable accuracy.
4 Experiments
Our experiments address two main questions: (1) Is our approximate LP-rounding scheme useful in
graph analysis tasks that arise in machine learning? and (2) How does our approach compare to a
state-of-the-art commercial solver? We give favorable answers to both questions.
4.1
LP formulations have been used to solve MAP inference problems on graphical models [16]  but
general-purpose LP solvers have rarely been used  for reasons of scalability. We demonstrate that
the rounded solutions obtained using Thetis are of comparable quality to those obtained with state-
of-the-art systems. We perform experiments on two different tasks: entity linking and text chunking.
For each task  we produce a factor graph [9]  which consists of a set of random variables and a set
of factors to describe the correlation between random variables. We then run MAP inference on the
factor graph using the LP formulation in [9] and compare the quality of the solutions obtained by
Thetis with a Gibbs sampling-based approach [26]. We follow the LP-rounding algorithm in [16]
to solve the MAP estimation problem. For entity linking  we use the TAC-KBP 2010 benchmark1.
The input graphical model has 12K boolean random variables and 17K factors. For text chunking 
we use the CoNLL 2000 shared task2. The factor graph contained 47K categorical random variables
(with domain size 23) and 100K factors. We use the training sets provided by TAC-KBP 2010 and
CoNLL 2000 respectively. We evaluate the quality of both approaches using the ofﬁcial evaluation
scripts and evaluation data sets provided by each challenge. Figure 2 contains a description of the
three relevant quality metrics  precision (P)  recall (R) and F1-scores. Figure 2 demonstrates that our
algorithm produces solutions of quality comparable with state-of-the-art approaches for these graph
analysis tasks.
4.2 How does our proposed approach compare to a state-of-the-art commercial solver?
We conducted numerical experiments on three different combinatorial problems that commonly arise
in graph analysis tasks in machine learning: vertex cover  independent set  and multiway cuts. For

1http://nlp.cs.qc.cuny.edu/kbp/2010/
2http://www.cnts.ua.ac.be/conll2000/chunking/

6

each problem  we compared the performance of our LP solver against the LP and IP solvers of Cplex
(v12.5) (denoted as Cplex-LP and Cplex-IP respectively). The two main goals of this experiment
are to: (1) compare the quality of the integral solutions obtained using LP-rounding with the integral
solutions from Cplex-IP and (2) compare wall-clock times required by Thetis and Cplex-LP to solve
the LPs for the purpose of LP-rounding.

Datasets. Our tasks are based on two families of graphs. The ﬁrst family of instances (frb59-26-1
to frb59-26-5) was obtained from Bhoslib3 (Benchmark with Hidden Optimum Solutions); they are
considered difﬁcult problems [25]. The instances in this family are similar; the ﬁrst is reported in the
ﬁgures of this section  while the remainder appear in Appendix E. The second family of instances
are social networking graphs obtained from the Stanford Network Analysis Platform (SNAP)4.

System Setup. Thetis was implemented using a combination of C++ (for Algorithm 1) and Mat-
lab (for the augmented Lagrangian framework). Our implementation of the augmented Lagrangian
framework was based on [6]. All experiments were run on a 4 Intel Xeon E7-4450 (40 cores @
2Ghz) with 256GB of RAM running Linux 3.8.4 with a 15-disk RAID0. Cplex used 32 (of the
40) cores available in the machine  and for consistency  our implementation was also restricted to
32 cores. Cplex implements presolve procedures that detect redundancy  and substitute and elim-
inate variables to obtain equivalent  smaller LPs. Since the aim of this experiment is compare the
algorithms used to solve LPs  we ran both Cplex-LP and Thetis on the reduced LPs generated by
the presolve procedure of Cplex-LP. Both Cplex-LP and Thetis were run to a tolerance of ✏ = 0.1.
Additional experiments with Cplex-LP run using its default tolerance options are reported in Ap-
pendix E. We used the barrier optimizer while running Cplex-LP. All codes were provided with a
time limit of 3600 seconds excluding the time taken for preprocessing as well as the runtime of the
rounding algorithms that generate integral solutions from fractional solutions.

Tasks. We solved the vertex cover problem using the approximation algorithm described in Sec-
tion 2. We solved the maximum independent set problem using a variant of the es + o(s)-factor
approximation in [1] where s is the maximum degree of a node in the graph (see Appendix C for
details). For the multiway-cut problem (with k = 3) we used the 3/2  1/k-factor approximation
algorithm described in [22]. The details of the transformation from approximate infeasible solu-
tions to feasible solutions are provided in Appendix C. Since the rounding schemes for maximum-
independent set and multiway-cut are randomized  we chose the best feasible integral solution from
10 repetitions.

Minimization problems

Instance

frb59-26-1
Amazon
DBLP
Google+

VC

PV NNZ
0.37
0.12
1.17
0.39
1.13
0.37
0.71
2.14

S
2.8
8.4
8.3
9.0

Q
1.04
1.23
1.25
1.21

MC

PV NNZ
3.02
0.75
23.2
5.89
26.1
6.61
9.24
36.8

S
53.3
-
-
-

Q
1.01
0.42
0.33
0.83

Maximization problems

MIS

PV NNZ
0.38
0.12
1.17
0.39
1.13
0.37
0.71
2.14

S
5.3
7.4
8.5
10.2

Q
0.36
0.82
0.88
0.82

Figure 3: Summary of wall-clock speedup (in comparison with Cplex-LP) and solution quality (in
comparison with Cplex-IP) of Thetis on three graph analysis problems. Each code is run with a time
limit of one hour and parallelized over 32 cores  with ‘-’ indicating that the code reached the time
limit. PV is the number of primal variables while NNZ is the number of nonzeros in the constraint
matrix of the LP in standard form (both in millions). S is the speedup  deﬁned as the time taken by
Cplex-LP divided by the time taken by Thetis. Q is the ratio of the solution objective obtained by
Thetis to that reported by Cplex-IP. For minimization problems (VC and MC) lower Q is better; for
maximization problems (MIS) higher Q is better. For MC  a value of Q < 1 indicates that Thetis
found a better solution than Cplex-IP found within the time limit.

Results. The results are summarized in Figure 3  with additional details in Figure 4. We discuss
the results for the vertex cover problem. On the Bhoslib instances  the integral solutions from
Thetis were within 4% of the documented optimal solutions. In comparison  Cplex-IP produced

3http://www.nlsde.buaa.edu.cn/˜kexu/benchmarks/graph-benchmarks.htm
4http://snap.stanford.edu/

7

t (secs)

85.5
22.1

VC
(min)

frb59-26-1
Amazon
DBLP
Google+

MC
(min)

Cplex IP
BFS
1475
1.60⇥105
1.65⇥105
1.06⇥105
Cplex IP
BFS
346
12
15
6

Cplex IP
BFS
50

-

-

-
-
-

-

-

Gap (%)

0.67

-
-

0.01

t (secs)
2.48
24.8
22.3
40.1

Cplex LP

LP
767

RSol
1534
1.50⇥105 2.04⇥105
1.42⇥105 2.08⇥105
1.00⇥105 1.31⇥105
Cplex LP
RSol
346
-
-
-

LP
346
-
-
-

-
-
-

t (secs)
0.88
2.97
2.70
4.47

Thetis
LP
RSol
959.7
1532
1.50⇥105 1.97⇥105
1.42⇥105 2.06⇥105
1.00⇥105 1.27⇥105
RSol
349
5
5
5

-
-
-

t (secs)

35.4
17.3

Cplex LP

LP
767

Gap (%)

RSol
18

RSol
15

MIS
(max)

Gap (%)

18.0

-
NA
NA
NA

t (secs)
72.3

t (secs)
312.2

t (secs)
5.86
55.8
63.8
109.9

t (secs)
0.88
3.09
2.72
4.37

t (secs)
4.65
23.0
23.2
44.5

1.75⇥105
1.52⇥105
1.06⇥105

frb59-26-1
Amazon
DBLP
Google+

frb59-26-1
Amazon
DBLP
Google+

1.85⇥105 1.56⇥105
1.75⇥105 1.41⇥105
1.11⇥105 9.39⇥104

Thetis
LP
352.3
7.28
11.7
5.84
Thetis
LP
447.7
1.73⇥105 1.43⇥105
1.66⇥105 1.34⇥105
1.00⇥105 8.67⇥104
Figure 4: Wall-clock time and quality of fractional and integral solutions for three graph analysis
problems using Thetis  Cplex-IP and Cplex-LP. Each code was given a time limit of one hour  with
‘-’ indicating a timeout. BFS is the objective value of the best integer feasible solution found by
Cplex-IP. The gap is deﬁned as (BFSBB)/BFS where BB is the best known solution bound found
by Cplex-IP within the time limit. A gap of ‘-’ indicates that the problem was solved to within
0.01% accuracy and NA indicates that Cplex-IP was unable to ﬁnd a valid solution bound. LP is the
objective value of the LP solution  and RSol is objective value of the rounded solution.
integral solutions that were within 1% of the documented optimal solutions  but required an hour for
each of the instances. Although the LP solutions obtained by Thetis were less accurate than those
obtained by Cplex-LP  the rounded solutions from Thetis and Cplex-LP are almost exactly the same.
In summary  the LP-rounding approaches using Thetis and Cplex-LP obtain integral solutions of
comparable quality with Cplex-IP — but Thetis is about three times faster than Cplex-LP.
We observed a similar trend on the large social networking graphs. We were able to recover integral
solutions of comparable quality to Cplex-IP  but seven to eight times faster than using LP-rounding
with Cplex-LP. We make two additional observations. The difference between the optimal frac-
tional and integral solutions for these instances is much smaller than frb59-26-1. We recorded
unpredictable performance of Cplex-IP on large instances. Notably  Cplex-IP was able to ﬁnd the
optimal solution for the Amazon and DBLP instances  but timed out on Google+  which is of com-
parable size. On some instances  Cplex-IP outperformed even Cplex-LP in wall clock time  due to
specialized presolve strategies.
5 Conclusion
We described Thetis  an LP rounding scheme based on an approximate solver for LP relaxations
of combinatorial problems. We derived worst-case runtime and solution quality bounds for our
scheme  and demonstrated that our approach was faster than an alternative based on a state-of-the-
art LP solver  while producing rounded solutions of comparable quality.
Acknowledgements
SS is generously supported by ONR award N000141310129. JL is generously supported in part by
NSF awards DMS-0914524 and DMS-1216318 and ONR award N000141310129. CR’s work on
this project is generously supported by NSF CAREER award under IIS-1353606  NSF award un-
der CCF-1356918  the ONR under awards N000141210041 and N000141310129  a Sloan Research
Fellowship  and gifts from Oracle and Google. SJW is generously supported in part by NSF awards
DMS-0914524 and DMS-1216318  ONR award N000141310129  DOE award DE-SC0002283  and
Subcontract 3F-30222 from Argonne National Laboratory. Any recommendations  ﬁndings or opin-
ions expressed in this work are those of the authors and do not necessarily reﬂect the views of any
of the above sponsors.

8

References
[1] Nikhil Bansal  Nitish Korula  Viswanath Nagarajan  and Aravind Srinivasan. Solving packing integer

programs via randomized rounding with alterations. Theory of Computing  8(1):533–565  2012.

[2] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  1999.
[3] Jacob Bien and Robert Tibshirani. Classiﬁcation by set cover: The prototype vector machine. arXiv

preprint arXiv:0908.2284  2009.

[4] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms
IEEE Transactions on Pattern Analysis and Machine Intelligence 

for energy minimization in vision.
26:1124–1137  2004.

[5] Gruia C˘alinescu  Howard Karloff  and Yuval Rabani. An improved approximation algorithm for multiway
cut. In Proceedings of the thirtieth annual ACM symposium on Theory of Computing  pages 48–52. ACM 
1998.

[6] Jonathan Eckstein and Paulo JS Silva. A practical relative error criterion for augmented lagrangians.

Mathematical Programming  pages 1–30  2010.

[7] Dorit S Hochbaum. Approximation algorithms for the set covering and vertex cover problems. SIAM

Journal on Computing  11(3):555–556  1982.

[8] VK Koval and MI Schlesinger. Two-dimensional programming in image analysis problems. USSR

Academy of Science  Automatics and Telemechanics  8:149–168  1976.

[9] Frank R Kschischang  Brendan J Frey  and H-A Loeliger. Factor graphs and the sum-product algorithm.

Information Theory  IEEE Transactions on  47(2):498–519  2001.

[10] Taesung Lee  Zhongyuan Wang  Haixun Wang  and Seung-won Hwang. Web scale entity resolution using

relational evidence. Technical report  Microsoft Research  2011.

[11] Victor Lempitsky and Yuri Boykov. Global optimization for shape ﬁtting.
Computer Vision and Pattern Recognition (CVPR ’07)  pages 1–8. IEEE  2007.

In IEEE Conference on

[12] Ji Liu  Stephen J. Wright  Christopher R´e  and Victor Bittorf. An asynchronous parallel stochastic coor-

dinate descent algorithm. Technical report  University of Wisconsin-Madison  October 2013.

[13] F Manshadi  Baruch Awerbuch  Rainer Gemulla  Rohit Khandekar  Juli´an Mestre  and Mauro Sozio. A
distributed algorithm for large-scale generalized matching. Proceedings of the VLDB Endowment  2013.
[14] Feng Niu  Benjamin Recht  Christopher R´e  and Stephen J. Wright. Hogwild!: A lock-free approach to

parallelizing stochastic gradient descent. arXiv preprint arXiv:1106.5730  2011.
[15] Jorge Nocedal and Stephen J Wright. Numerical Optimization. Springer  2006.
[16] Pradeep Ravikumar  Alekh Agarwal  and Martin J Wainwright. Message-passing for graph-structured
linear programs: Proximal methods and rounding schemes. The Journal of Machine Learning Research 
11:1043–1080  2010.

[17] J. Renegar. Some perturbation theory for linear programming. Mathenatical Programming  Series A 

65:73–92  1994.

[18] Dan Roth and Wen-tau Yih.

Integer linear programming inference for conditional random ﬁelds.
Proceedings of the 22nd International Conference on Machine Learning  pages 736–743. ACM  2005.

In

[19] Sujay Sanghavi  Dmitry Malioutov  and Alan S Willsky. Linear programming analysis of loopy belief
propagation for weighted matching. In Advances in Neural Information Processing Systems  pages 1273–
1280  2007.

[20] Aravind Srinivasan.

Improved approximation guarantees for packing and covering integer programs.

SIAM Journal on Computing  29(2):648–670  1999.

[21] Jurgen Van Gael and Xiaojin Zhu. Correlation clustering for crosslingual link detection. In IJCAI  pages

1744–1749  2007.

[22] Vijay V Vazirani. Approximation Algorithms. Springer  2004.
[23] Stephen J. Wright.

Implementing proximal point methods for linear programming.

Optimization Theory and Applications  65(3):531–554  1990.

Journal of

[24] Zheng Wu  Ashwin Thangali  Stan Sclaroff  and Margrit Betke. Coupling detection and data association
for multiple object tracking. In Computer Vision and Pattern Recognition (CVPR)  2012 IEEE Conference
on  pages 1948–1955. IEEE  2012.

[25] Ke Xu and Wei Li. Many hard examples in exact phase transitions. Theoretical Computer Science 

355(3):291–302  2006.

[26] Ce Zhang and Christopher R´e. Towards high-throughput gibbs sampling at scale: A study across storage

managers. In SIGMOD Proceedings  2013.

9

,Srikrishna Sridhar
Stephen Wright
Christopher Re
Ji Liu
Victor Bittorf
Ce Zhang