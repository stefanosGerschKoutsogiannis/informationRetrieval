2008,Support Vector Machines with a Reject Option,We consider the problem of binary classification where the classifier may abstain instead of classifying each observation. The Bayes decision rule for this setup  known as Chow's rule  is defined by two thresholds on posterior probabilities. From simple desiderata  namely the consistency and the sparsity of the classifier  we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that  for suitable kernel machines  our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efficiently. We finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions.,Support Vector Machines with a Reject Option

Yves Grandvalet 1  2  Alain Rakotomamonjy 3  Joseph Keshet 2 and St´ephane Canu 3

1 Heudiasyc  UMR CNRS 6599

Universit´e de Technologie de Compi`egne

2 Idiap Research Institute

Centre du Parc

BP 20529  60205 Compi`egne Cedex  France CP 592  CH-1920 Martigny Switzerland

3 LITIS  EA 4108

Universit´e de Rouen & INSA de Rouen
76801 Saint Etienne du Rouvray  France

Abstract

We consider the problem of binary classiﬁcation where the classiﬁer may abstain
instead of classifying each observation. The Bayes decision rule for this setup 
known as Chow’s rule  is deﬁned by two thresholds on posterior probabilities.
From simple desiderata  namely the consistency and the sparsity of the classiﬁer 
we derive the double hinge loss function that focuses on estimating conditional
probabilities only in the vicinity of the threshold points of the optimal decision
rule. We show that  for suitable kernel machines  our approach is universally
consistent. We cast the problem of minimizing the double hinge loss as a quadratic
program akin to the standard SVM optimization problem and propose an active set
method to solve it efﬁciently. We ﬁnally provide preliminary experimental results
illustrating the interest of our constructive approach to devising loss functions.

1 Introduction

In decision problems where errors incur a severe loss  one may have to build classiﬁers that abstain
from classifying ambiguous examples. Rejecting these examples has been investigated since the
early days of pattern recognition. In particular  Chow (1970) analyses how the error rate may be
decreased thanks to the reject option.
There have been several attempts to integrate a reject option in Support Vector Machines (SVMs) 
using strategies based on the thresholding of SVMs scores (Kwok  1999) or on a new training cri-
terion (Fumera & Roli  2002). These approaches have however critical drawbacks: the former is
not consistent and the latter leads to considerable computational overheads to the original SVM
algorithm and lacks some of its most appealing features like convexity and sparsity.
We introduce a piecewise linear and convex training criterion dedicated to the problem of classi-
ﬁcation with the reject option. Our proposal  inspired by the probabilistic interpretation of SVM
ﬁtting (Grandvalet et al.  2006)  is a double hinge loss  reﬂecting the two thresholds in Chow’s rule.
Hence  we generalize the loss suggested by Bartlett and Wegkamp (2008) to arbitrary asymmetric
misclassiﬁcation and rejection costs. For the symmetric case  our probabilistic viewpoint motivates
another decision rule. We then propose the ﬁrst algorithm speciﬁcally dedicated to train SVMs with
a double hinge loss. Its implementation shows that our decision rule is at least at par with the one of
Bartlett and Wegkamp (2008).
The paper is organized as follows. Section 2 deﬁnes the problem and recalls Bayes rule for binary
classiﬁcation with a reject option. The proposed double hinge loss is derived in Section 3  together
with the decision rule associated with SVM scores. Section 4 addresses implementation issues: it
formalizes the SVM training problem and details an active set algorithm speciﬁcally designed for

1

training with the double hinge loss. This implementation is tested empirically in Section 5. Finally 
Section 6 concludes the paper.

2 Problem Setting and the Bayes Classiﬁer
Classiﬁcation aims at predicting a class label y ∈ Y from an observed pattern x ∈ X . For this
purpose  we construct a decision rule d : X → A  where A is a set of actions that typically consists
in assigning a label to x ∈ X . In binary problems  where the class is tagged either as +1 or −1  the
two types of errors are: (i) false positive  where an example labeled −1 is predicted as +1  incurring
a cost c−; (ii) false negative  where an example labeled +1 is predicted as −1  incurring a cost c+.
In general  the goal of classiﬁcation is to predict the true label for an observed pattern. However 
patterns close to the decision boundary are misclassiﬁed with high probability. This problem be-
comes especially eminent in cases where the costs  c− or c+  are high  such as in medical decision
making. In these processes  it might be better to alert the user and abstain from prediction. This
motivates the introduction of a reject option for classiﬁers that cannot predict a pattern with enough
conﬁdence. This decision to abstain  which is denoted by 0  incurs a cost  r− and r+ for examples
labeled −1 and +1  respectively.
The costs pertaining to each possible decision are recapped on the right-
hand-side. In what follows  we assume that all costs are strictly positive:
(1)
Furthermore  it should be possible to incur a lower expected loss by
choosing the reject option instead of any prediction  that is

c− > 0   c+ > 0   r− > 0   r+ > 0 .

+1 −1
0
c−

)
x
(
d

y

r+

r−

c− r+ + c+ r− < c− c+ .

(2)

c+

0

+1

0
−1

Bayes’ decision theory is the paramount framework in statistical decision theory  where decisions
are taken to minimize expected losses. For classiﬁcation with a reject option  the overall risk is

R(d) = c+ EXY [Y = 1  d(X) = −1] + c− EXY [Y = −1  d(X) = 1] +

r+ EXY [Y = 1  d(X) = 0] + r− EXY [Y = −1  d(X) = 0]

 

(3)

where X and Y denote the random variable describing patterns and labels.
The Bayes classiﬁer d∗ is deﬁned as the minimizer of the risk R(d). Since the seminal paper of
Chow (1970)  this rule is sometimes referred to as Chow’s rule:

( +1

−1
0

d∗(x) =

if P(Y = 1|X = x) > p+
if P(Y = 1|X = x) < p−
otherwise  

(4)

where p+ = c− − r−
c− − r− + r+

and p− =

r−

c+ − r+ + r−

.

Note that  assuming that (1) and (2) hold  we have 0 < p− < p+ < 1.
One of the major inductive principle is the empirical risk minimization  where one minimizes the
empirical counterpart of the risk (3).
In classiﬁcation  this principle usually leads to a NP-hard
problem  which can be circumvented by using a smooth proxy of the misclassiﬁcation loss. For
example  Vapnik (1995) motivated the hinge loss as a “computationally simple” (i.e.  convex) surro-
gate of classiﬁcation error. The following section is dedicated to the construction of such a surrogate
for classiﬁcation with a reject option.

3 Training Criterion

One method to get around the hardness of learning decision functions is to replace the conditional

probability P(Y = 1|X = x) with its estimationbP(Y = 1|X = x)  and then plug this estimation

back in (4) to build a classiﬁcation rule (Herbei & Wegkamp  2006). One of the most widespread

2

)
)
x
(
f
 
1
+

(
+
p
 

−
p
‘

)
)
x
(
f
 
1
−

(
+
p
 

−
p
‘

f(x)

f(x)

Figure 1: Double hinge loss function ‘p− p+ for positive (left) and negative examples (right)  with
p− = 0.4 and p+ = 0.8 (solid: double hinge  dashed: likelihood). Note that the decision thresholds
f+ and f− are not symmetric around zero.

representative of this line of attack is the logistic regression model  which estimates the conditional
probability using the maximum (penalized) likelihood framework.
As a starting point  we consider the generalized logistic regression model for binary classiﬁcation 
where

(5)
and the function f : X → R is estimated by the minimization of a regularized empirical risk on the
training sample T = {(xi  yi)}n

1 + exp(−yf(x))  

i=1

bP(Y = y|X = x) =

1

‘(yi  f(xi)) + λΩ(f)  

(6)

nX

i=1

where ‘ is a loss function and Ω(·) is a regularization functional  such as the (squared) norm of f in
a suitable Hilbert space Ω(f) = kfk2H  and λ is a regularization parameter. In the standard logistic
regression procedure  ‘ is the negative log-likelihoood loss

‘(y  f(x)) = log(1 + exp(−yf(x))) .

This loss function is convex and decision-calibrated (Bartlett & Tewari  2007)  but it lacks an ap-
pealing feature of the hinge loss used in SVMs  that is  it does not lead to sparse solutions. This
drawback is the price to pay for the ability to estimate the posterior probability P(Y = 1|X = x)
on the whole range (0  1) (Bartlett & Tewari  2007).
However  the deﬁnition of the Bayes’ rule (4) clearly shows that the estimation of P(Y = 1|X = x)
does not have to be accurate everywhere  but only in the vicinity of p+ and p−. This motivates the
construction of a training criterion that focuses on this goal  without estimating P(Y = 1|X = x)
on the whole range as an intermediate step. Our purpose is to derive such a loss function  without
sacrifying sparsity to the consistency of the decision rule.
Though not a proper negative log-likelihood  the hinge loss can be interpreted in a maximum a
posteriori framework: The hinge loss can be derived as a relaxed minimization of negative log-
likelihood (Grandvalet et al.  2006). According to this viewpoint  minimizing the hinge loss aims
at deriving a loose approximation to the the logistic regression model (5) that is accurate only at
f(x) = 0  thus allowing to estimate whether P(Y = 1|X = x) > 1/2 or not. More generally 
one can show that  in order to have a precise estimate of P(Y = 1|X = x) = p  the surrogate loss
should be tangent to the neg-log-likelihood at f = log(p/(1 − p)).
Following this simple constructive principle  we derive the double hinge loss  which aims at reliably
estimating P(Y = 1|X = x) at the threshold points p+ and p−. Furthermore  to encourage sparsity 
we set the loss to zero for all points classiﬁed with high conﬁdence. This loss function is displayed in
Figure 1. Formally  for the positive examples  the double hinge loss satisfying the above conditions
can be expressed as

‘p− p+(+1  f(x)) = max(cid:8) − (1 − p−)f(x) + H(p−)  −(1 − p+)f(x) + H(p+)  0(cid:9)  

(7)

3

  02.55.10f+f−  02.55.10f+f−and for the negative examples it can be expressed as

‘p− p+(−1  f(x)) = max(cid:8) p+f(x) + H(p+)  p−f(x) + H(p−)  0(cid:9)  

(8)
where H(p) = −p log(p) − (1 − p) log(1 − p). Note that  unless p− = 1 − p+  there is no simple
symmetry with respect to the labels.
After training  the decision rule is deﬁned as the plug-in estimation of (4) using the logistic regres-
sion probability estimation. Let f+ = log(p+/(1 − p+)) and f− = log(p−/(1 − p−))  the decision
rule can be expressed in terms of the function f as follows

dp− p+(x; f) =

if f(x) > f+
if f(x) < f−
otherwise .

(9)

( +1

−1
0

The following result shows that the rule dp− p+(·; f) is universally consistent when f is learned by
minimizing empirical risk based on ‘p− p+. Hence  in the limit  learning with the double hinge loss
is optimal in the sense that the risk for the learned decision rule converges to the Bayes’ risk.
Theorem 1. Let H be a functional space that is dense in the set of continuous functions. Suppose
that we have a positive sequence {λn} with λn → 0 and nλ2

n/ log n → ∞. We deﬁne f∗

n as

‘p− p+(yi  f(xi)) + λnkfk2H .
arg min
f∈H
n)) = R(d∗(X)) holds almost surely 
limn→∞ R(dp− p+(X; f∗

i=1

1
n

n) is strongly universally consistent.

Then 
dp− p+(·; f∗

that

is 

the classiﬁer

nX

Proof. Our theorem follows directly from (Steinwart  2005  Corollary 3.15)  since ‘p− p+ is regular
(Steinwart  2005  Deﬁnition 3.9). Besides mild regularity conditions that hold for ‘p− p+  a loss
function is said regular if  for every α ∈ [0  1]  and every tα such that

tα = arg min

t

α ‘p− p+(+1  t) + (1 − α) ‘p− p+(−1  t)  

we have that dp− p+(tα  x) agrees with d∗(x) almost everywhere.
Let f1 = −H(p−)/p−  f2 = −(H(p+) − H(p−))/(p+ − p−) and f3 = H(p+)/(1 − p+) denote
the hinge locations in ‘p− p+(±1  f(x)). Note that we have f1 < f− < f2 < f+ < f3  and that



tα ∈

[f1  f2] if α = p−

(−∞  f1] if 0 ≤ α < p−
{f2} if p− < α < p+
[f2  f3] if α = p+
[f3 ∞) if p+ < α ≤ 1

⇒ dp− p+(tα  x) =



−1 if P(Y = 1|x) < p−
−1 or 0 if P(Y = 1|x) = p−
0 if p− < P(Y = 1|x) < p+
0 or + 1 if P(Y = 1|x) = p+
+1 if P(Y = 1|x) > p+

which is the desired result.

Note also that the analysis of Bartlett and Tewari (2007) can be used to show that minimizing ‘p− p+
cannot provide consistent estimates of P(Y = 1|X = x) = p for p /∈ {p−  p+}. This property is
desirable regarding sparsity  since sparseness does not occur when the conditional probabilities can
be unambiguously estimated .
Note on a Close Relative A double hinge loss function has been proposed recently with a dif-
ferent perspective by Bartlett and Wegkamp (2008). Their formulation is restricted to symmetric
classiﬁcation  where c+ = c− = 1 and r+ = r− = r.
In this situation  rejection may occur
only if 0 ≤ r < 1/2  and the thresholds on the conditional probabilities in Bayes’ rule (4) are
p− = 1 − p+ = r.
For symmetric classiﬁcation  the loss function of Bartlett and Wegkamp (2008) is a scaled version
of our proposal that leads to equivalent solutions for f  but our decision rule differs. While our
probabilistic derivation of the double hinge loss motivates the decision function (9)  the decision rule
of Bartlett and Wegkamp (2008) has a free parameter (corresponding to the threshold f+ = −f−)
whose value is set by optimizing a generalization bound.
Our decision rule rejects more examples when the loss incurred by rejection is small and fewer
examples otherwise. The two rules are identical for r ’ 0.24. We will see in Section 5 that this
difference has noticeable outcomes.

4

4 SVMs with Double Hinge

In this section  we show how the standard SVM optimization problem is modiﬁed when the hinge
loss is replaced by the double hinge loss. The optimization problem is ﬁrst written using a compact
notation  and the dual problem is then derived.

4.1 Optimization Problem

Minimizing the regularized empirical risk (6) with the double hinge loss (7–8) is an optimization
problem akin to the standard SVM problem. Let C be an arbitrary constant  we deﬁne D = C(p+ −
p−)  Ci = C(1 − p+) for positive examples  and Ci = Cp− for negative examples. With the
introduction of slack variables ξ and η  the optimization problem can be stated as

nX

i=1

nX

ηi

min
f b ξ η
s. t.

kfk2H +

1
Ciξi + D
2
yi(f(xi) + b) ≥ ti − ξi
yi(f(xi) + b) ≥ τi − ηi
ξi ≥ 0  

ηi ≥ 0

i=1
i = 1  . . .   n
i = 1  . . .   n
i = 1  . . .   n  

where  for positive examples  ti = H(p+)/(1 − p+)  τi = −(H(p−) − H(p+))/(p− − p+)  while 
for negative examples ti = H(p−)/p−  τi = (H(p−) − H(p+))/(p− − p+).
For functions f belonging to a Hilbert space H endowed with a reproducing kernel k(· ·)  efﬁcient
optimization algorithms can be drawn from the dual formulation:





(10)

(11)

min
α γ
s. t.

1
2 γT Gγ − τ T γ − (t − τ )T α
yT γ = 0
0 ≤ αi ≤ Ci
i = 1  . . .   n
0 ≤ γi − αi ≤ D i = 1  . . .   n .

where y = (y1  . . .   yn)T   t = (t1  . . .   tn)T and τ = (τ1  . . .   τn)T are vectors of Rn and G is the
n × n Gram matrix with general entry Gij = yiyjk(xi  xj). Note that (11) is a simple quadratic
problem under box constraints. Compared to the standard SVM dual problem  one has an additional
vector to optimize  but  with the active set we developed  we only have to optimize a single vector of
Rn. The primal variables f and b are then derived from the Karush-Kuhn-Tucker (KKT) conditions.
i=1 γiyik(·  xi)  and b is obtained in the optimization process described

For f  we have: f(·) =Pn

below.

4.2 Solving the Problem

To solve (11)  we use an active set algorithm  following a strategy that proved to be efﬁcient in
SimpleSVM (Vishwanathan et al.  2003). This algorithm solves the SVM training problem by a
greedy approach  in which one solves a series of small problems. First  the repartition of training
examples in support and non-support vectors is assumed to be known  and the training criterion is
optimized considering that this partition ﬁxed. Then  this optimization results in an updated partition
of examples in support and non-support vectors. These two steps are iterated until some level of
accuracy is reached.
Partitioning the Training Set The training set is partitioned into ﬁve subsets deﬁned by the ac-
tivity of the box constraints of Problem (11). The training examples indexed by:

I0   deﬁned by I0 = {i|γi = 0}  are such that yi(f(xi) + b) > ti;
It   deﬁned by It = {i|0 < γi < Ci}  are such that yi(f(xi) + b) = ti;
IC   deﬁned by IC = {i|γi = Ci}  are such that τi < yi(f(xi) + b) ≤ ti;
Iτ   deﬁned by Iτ = {i|Ci < γi < Ci + D}  are such that yi(f(xi) + b) = τi;
ID   deﬁned by ID = {i|γi = Ci + D}  are such that yi(f(xi) + b) ≤ τi.

When example i belongs to one of the subsets described above  the KKT conditions yield that αi
is either equal to γi or constant. Hence  provided that the repartition of examples in the subsets I0 
It  IC  Iτ and ID is known  we only have to consider a problem in γ. Furthermore  γi has to be
computed only for i ∈ It ∪ Iτ .

5


where si = ti −P
P


j∈ID

X
X

j∈IT

i∈IT

Updating Dual Variables Assuming a correct partition  Problem (11) reduces to the considerably
smaller problem of computing γi for i ∈ IT = It ∪ Iτ :

i∈IT  j∈IT

γiγjGij −X
X
Ciyi + X
yiγi + X
(Cj + D) Gji for i ∈ It and si = τi −P

(Ci + D) yi = 0  

i∈ID

i∈IC

i∈IT

γisi

min

{γi|i∈IT }
s. t.

1
2

X
CjGji −P

i∈IT

CjGji −
(Cj + D) Gji for i ∈ Iτ . Note that the box constraints of Problem (11) do not appear here 

j∈ID

j∈IC

j∈IC

because we assumed the partition to be correct.
The solution of Problem (12) is simply obtained by solving the following linear system resulting
from the ﬁrst-order optimality conditions:

(12)

Gijγj + yiλ = si

yiγi = −X

Ciyi − X

for i ∈ IT

(Ci + D) yi  

(13)

i∈IC

i∈ID

where λ  which is the (unknown) Lagrange parameter associated to the equality constraint in (12) 
is computed along with γ. Note that the |IT| equations of the linear system given on the ﬁrst line
of (13) express that  for i ∈ It  yi(f(xi) + λ) = ti and for i ∈ Iτ   yi(f(xi) + λ) = τi. Hence  the
primal variable b is equal to λ.
Algorithm The algorithm  described in Algorithm 1  simply alternates updates of the partition of
examples in {I0  It  IC  Iτ   ID}  and the ones of coefﬁcients γi for the current active set IT . As for
standard SVMs  the initialization step consists in either using the solution obtained for a different
hyper-parameter  such as a higher value of C  or in picking one or several examples of each class to
arbitrarily initialize It to a non-empty set  and putting all the other ones in I0 = {1  . . .   n} \ It.

i = γold

i + ρ(γi − γold

i

) obey box constraints

Algorithm 1 SVM Training with a Reject Option
input {xi  yi}1≤i≤n and hyper-parameters C  p+  p−
initialize γold IT = {It  Iτ}  IT = {I0  IC  ID} 
repeat
solve linear system (13) → (γi)i∈IT   b = λ.
if any (γi)i∈IT violates the box constraints (11) then

Compute the largest ρ s. t.  for all i ∈ IT γnew
Let j denote the index of (γnew
)i∈IT at bound 
IT = IT \ {j}  IT = IT ∪ {j}
j = γnew
γold
for all i ∈ IT do γnew
i = γi
if any (yi(f(xi) + b))i∈IT

j

i

else

select i with violated constraint
IT = IT \ {i}  IT = IT ∪ {i}

else

exact convergence
end if
for all i ∈ IT do γold

i = γnew

i

end if

until convergence

output f  b.

violates primal constraints (10) then

The exact convergence is obtained when all constraints are fulﬁlled  that is  when all examples be-
long to the same subset at the begining and the end of the main loop. However  it is possible to relax
the convergence criteria while having a good control on the precision on the solution by monitor-
ing the duality gap  that is the difference between the primal and the dual objectives  respectively
provided in the deﬁnition of Problems (10) and (11).

6

Table 1: Performances in terms of average test loss  rejection rate and misclassiﬁcation rate (re-
jection is not an error) with r+ = r− = 0.45  for the three rejection methods over four different
datasets.

Average Test Loss Rejection rate (%) Error rate (%)

Wdbc

Naive
B&W’s
Our’s
Naive
B&W’s
Our’s
Thyroid Naive

Liver

.

Pima

B&W’s
Our’s
Naive
B&W’s
Our’s

2.9 ± 1.6
3.5 ± 1.8
2.9 ± 1.7
28.9 ± 5.4
30.9 ± 4.0
28.8 ± 5.1
4.1 ± 2.9
4.4 ± 2.7
3.7 ± 2.7
23.7 ± 1.9
24.7 ± 2.1
23.1 ± 1.3

0.7
3.9
1.2
3.3
34.5
7.9
0.9
6.1
2.1
7.5
24.3
6.9

2.6
1.8
2.4
27.4
15.4
25.2
3.7
1.6
2.8
20.3
13.8
20.0

Theorem 2. Algorithm 1 converges in a ﬁnite number of steps to the exact solution of (11).

Proof. The proof follows the ones used to prove the convergence of active set methods in general 
and SimpleSVM in particular  see Propositon 1 in (Vishwanathan et al.  2003)).

5 Experiments

We compare the performances of three different rejection schemes based on SVMs. For this purpose 
we selected the datasets from the UCI repository related to medical problems  as medical decision
making is an application domain for which rejection is of primary importance. Since these datasets
are small  we repeated 10 trials for each problem. Each trial consists in splitting randomly the
examples into a training set with 80 % of examples and an independent test set. Note that the
training examples were normalized to zero-mean and unit variance before cross-validation (test sets
were of course rescaled accordingly).
In a ﬁrst series of experiments  to compare our decision rule with the one proposed by Bartlett and
Wegkamp (2008) (B&W’s)  we used symmetric costs: c+ = c− = 1 and r+ = r− = r. We
also chose r = 0.45  which corresponds to rather low rejection rates  in order to favour different
behaviors between these two decision rules (recall that they are identical for r ’ 0.24). Besides
the double hinge loss  we also implemented a “naive” method that consists in running the standard
SVM algorithm (using the hinge loss) and selecting a symmetric rejection region around zero by
cross-validation.
For all methods  we used Gaussian kernels. Model selection is performed by cross-validation. This
includes the selection of the kernel widths  the regularization parameter C for all methods and
additionally of the rejection thresholds for the naive method. Note that B&W’s and our decision
rules are based on learning with the double-hinge loss. Hence  the results displayed in Table 1 only
differ due to the size of the rejection region  and to the disparities that arise from the choice of
hyper-parameters that may arise in the cross-validation process (since the decision rules differ  the
cross-validation scores differ also).
Table 1 summarizes the averaged performances over the 10 trials. Overall  all methods lead to
equivalent average test losses  with an unsigniﬁcant but consistent advantage for our decision rule.
We also see that the naive method tends to reject fewer test examples than the consistent methods.
This means that  for comparable average losses  the decision rules based on the scores learned by
minimizing the double hinge loss tend to classify more accurately the examples that are not rejected 
as seen on the last column of the table.
For noisy problems such as Liver and Pima  we observed that reducing rejection costs considerably
decrease the error rate on classiﬁed examples (not shown on the table). The performances of the
two learning methods based on the double-hinge get closer  and there is still no signiﬁcant gain

7

compared to the naive approach. Note however that the symmetric setting is favourable to the naive
approach  since we only have to estimate a single decision thershold. We are experimenting to see
whether the double-hinge loss shows more substantial improvements for asymmetric losses and for
larger training sets.

6 Conclusion

In this paper we proposed a new solution to the general problem of classiﬁcation with a reject
option. The double hinge loss was derived from the simple desiderata to obtain accurate estimates
of posterior probabilities only in the vicinity of the decision boundaries. Our formulation handles
asymmetric misclassiﬁcation and rejection costs and compares favorably to the one of Bartlett and
Wegkamp (2008).
We showed that for suitable kernels  including usual ones such as the Gaussian kernel  training a
kernel machine with the double hinge loss provides a universally consistent classiﬁer with reject
option. Furthermore  the loss provides sparse solutions  with a limited number of support vectors 
similarly to the standard L1-SVM classiﬁer.
We presented what we believe to be the ﬁrst principled and efﬁcient implementation of SVMs for
classiﬁcation with a reject option. Our optimization scheme is based on an active set method  whose
complexity compares to standard SVMs. The dimension of our quadratic program is bounded by
the number of examples  and is effectively limited to the number of support vectors. The only
computational overhead is brought by monitoring ﬁve categories of examples  instead of the three
ones considered in standard SVMs (support vector  support at bound  inactive example).
Our approach for deriving the double hinge loss can be used for other decision problems relying
on conditional probabilities at speciﬁc values or in a limited range or values. As a ﬁrst example 
one may target the estimation of discretized conﬁdence ratings  such as the ones reported in weather
forecasts. Multi-category classiﬁcation also belongs to this class of problems  since there  decisions
rely on having precise conditional probabilities within a predeﬁned interval.

Acknowledgements

This work was supported in part by the French national research agency (ANR) through project
GD2GS  and by the IST Programme of the European Community through project DIRAC.

References
Bartlett  P. L.  & Tewari  A. (2007). Sparseness vs estimating conditional probabilities: Some asymptotic

results. Journal of Machine Learning Research  8  775–790.

Bartlett  P. L.  & Wegkamp  M. H. (2008). Classiﬁcation with a reject option using a hinge loss. Journal of

Machine Learning Research  9  1823–1840.

Chow  C. K. (1970). On optimum recognition error and reject tradeoff. IEEE Trans. on Info. Theory  16  41–46.
Fumera  G.  & Roli  F. (2002). Support vector machines with embedded reject option. Pattern Recognition

with Support Vector Machines: First International Workshop (pp. 68–82). Springer.

Grandvalet  Y.  Mari´ethoz  J.  & Bengio  S. (2006). A probabilistic interpretation of SVMs with an application

to unbalanced classiﬁcation. NIPS 18 (pp. 467–474). MIT Press.

Herbei  R.  & Wegkamp  M. H. (2006). Classiﬁcation with reject option. The Canadian Journal of Statistics 

34  709–721.

Kwok  J. T. (1999). Moderating the outputs of support vector machine classiﬁers.

Networks  10  1018–1031.

IEEE Trans. on Neural

Steinwart  I. (2005). Consistency of support vector machine and other regularized kernel classiﬁers.

Trans. on Info. Theory  51  128–142.

IEEE

Vapnik  V. N. (1995). The nature of statistical learning theory. Springer Series in Statistics. Springer.
Vishwanathan  S. V. N.  Smola  A.  & Murty  N. (2003). SimpleSVM. Proceedings of the Twentieth Interna-

tional Conference on Machine Learning (pp. 68–82). AAAI.

8

,Deepti Pachauri
Risi Kondor
Vikas Singh