2017,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data,Several important applications  such as streaming PCA and semidefinite programming  involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates.  Because of storage limitations  it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nyström approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix.  Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.,Fixed-Rank Approximation of a

Positive-Semideﬁnite Matrix from Streaming Data

Joel A. Tropp

Caltech

Alp Yurtsever

EPFL

jtropp@caltech.edu

alp.yurtsever@epfl.ch

mru8@cornell.edu

volkan.cevher@epfl.ch

Madeleine Udell

Volkan Cevher

Cornell

EPFL

Abstract

Several important applications  such as streaming PCA and semideﬁnite program-
ming  involve a large-scale positive-semideﬁnite (psd) matrix that is presented as a
sequence of linear updates. Because of storage limitations  it may only be possible
to retain a sketch of the psd matrix. This paper develops a new algorithm for
ﬁxed-rank psd approximation from a sketch. The approach combines the Nyström
approximation with a novel mechanism for rank truncation. Theoretical analysis
establishes that the proposed method can achieve any prescribed relative error in
the Schatten 1-norm and that it exploits the spectral decay of the input matrix. Com-
puter experiments show that the proposed method dominates alternative techniques
for ﬁxed-rank psd matrix approximation across a wide range of examples.

1 Motivation

In recent years  researchers have studied many applications where a large positive-semideﬁnite (psd)
matrix is presented as a series of linear updates. A recurring theme is that we only have space to store
a small summary of the psd matrix  and we must use this information to construct an accurate psd
approximation with speciﬁed rank. Here are two important cases where this problem arises.
Streaming Covariance Estimation. Suppose that we receive a stream h1  h2  h3 ··· ∈ Rn of
high-dimensional vectors. The psd sample covariance matrix of these vectors has the linear dynamics

A(0) ← 0 and A(i) ← (1 − i−1)A(i−1) + i−1hih∗
i .

When the dimension n and the number of vectors are both large  it is not possible to store the vectors
or the sample covariance matrix. Instead  we wish to maintain a small summary that allows us to
compute the rank-r psd approximation of the sample covariance matrix A(i) at a speciﬁed instant i.
This problem and its variants are often called streaming PCA [3  12  14  15  25  32].
Convex Low-Rank Matrix Optimization with Optimal Storage. A primary application of
semideﬁnite programming (SDP) is to search for a rank-r psd matrix that satisﬁes additional con-
straints. Because of storage costs  SDPs are difﬁcult to solve when the matrix variable is large.
Recently  Yurtsever et al. [44] exhibited the ﬁrst provable algorithm  called SketchyCGM  that
produces a rank-r approximate solution to an SDP using optimal storage.
Implicitly  SketchyCGM forms a sequence of approximate psd solutions to the SDP via the iteration

A(0) ← 0 and A(i) ← (1 − ηi)A(i−1) + ηihih∗
i .

The step size ηi = 2/(i + 2)  and the vectors hi do not depend on the matrices A(i). In fact 
SketchyCGM only maintains a small summary of the evolving solution A(i). When the iteration
terminates  SketchyCGM computes a rank-r psd approximation of the ﬁnal iterate using the method
described by Tropp et al. [37  Alg. 9].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1.1 Notation and Background
The scalar ﬁeld F = R or F = C. Deﬁne α(R) = 1 and α(C) = 0. The asterisk ∗ is the (conjugate)
transpose  and the dagger † denotes the Moore–Penrose pseudoinverse. The notation A1/2 refers to
the unique psd square root of a psd matrix A. For p ∈ [1 ∞]  the Schatten p-norm (cid:107) · (cid:107)p returns the
(cid:96)p norm of the singular values of a matrix. As usual  σr refers to the rth largest singular value.
For a nonnegative integer r  the phrase “rank-r” and its variants mean “rank at most r.” For a

matrix M  the symbol(cid:74)M(cid:75)r denotes a (simultaneous) best rank-r approximation of the matrix
M with respect to any Schatten p-norm. We can take(cid:74)M(cid:75)r to be any r-truncated singular value

decomposition (SVD) of M [24  Sec. 6]. Every best rank-r approximation of a psd matrix is psd.

2 Sketching and Fixed-Rank PSD Approximation

We begin with a streaming data model for a psd matrix that evolves via a sequence of general
linear updates  and it describes a randomized linear sketch for tracking the psd matrix. To compute
a ﬁxed-rank psd approximation  we develop an algorithm based on the Nyström method [40]  a
technique from the literature on kernel methods. In contrast to previous approaches  our algorithm
uses a distinct mechanism to truncate the rank of the approximation.
The Streaming Model. Fix a rank parameter r in the range 1 ≤ r ≤ n. Initially  the psd matrix
A ∈ Fn×n equals a known psd matrix Ainit ∈ Fn×n. Then A evolves via a series of linear updates:
(2.1)
In many applications  the innovation H is low-rank and/or sparse. We assume that the evolving matrix
A always remains psd. At one given instant  we must produce an accurate rank-r approximation of
the psd matrix A induced by the stream of linear updates.
The Sketch. Fix a sketch size parameter k in the range r ≤ k ≤ n. Independent from A  we draw
and ﬁx a random test matrix

θi ∈ R  H ∈ Fn×n is (conjugate) symmetric.

A ← θ1A + θ2H where

Ω ∈ Fn×k.

(2.2)

See Sec. 3 for a discussion of possible distributions. The sketch of the matrix A takes the form

Y = AΩ ∈ Fn×k.

(2.3)

The sketch (2.3) supports updates of the form (2.1):

Y ← θ1Y + θ2HΩ.

(2.4)
To ﬁnd a good rank-r approximation  we must set the sketch size k larger than r. But storage costs
and computation also increase with k. One of our main contributions is to clarify the role of k.
Under the model (2.1)  it is more or less necessary to use a randomized linear sketch to track A [28].
For psd matrices  sketches of the form (2.2)–(2.3) appear explicitly in Gittens’s work [16  17  19].
Tropp et al. [37] relies on a more complicated sketch developed in [7  42].
The Nyström Approximation. The Nyström method is a general technique for low-rank psd matrix
approximation. Various instantiations appear in the papers [5  11  13  16  17  19  22  27  34  40].
Here is the application to the present situation. Given the test matrix Ω and the sketch Y = AΩ  the
Nyström method constructs a rank-k psd approximation of the psd matrix A via the formula

ˆAnys = Y (Ω∗Y )†Y ∗.

(2.5)
In most work on the Nyström method  the test matrix Ω depends adaptively on A  so these approaches
are not valid in the streaming setting. Gittens’s framework [16  17  19] covers the streaming case.
Fixed-Rank Nyström Approximation: Prior Art. To construct a Nyström approximation with
exact rank r from a sketch of size k  the standard approach is to truncate the center matrix to rank r:

(2.6)
The truncated Nyström approximation (2.6) appears in the many papers  including [5  11  18  34].
We have found (Sec. 5) that the truncation method (2.6) performs poorly in the present setting. This
observation motivated us to search for more effective techniques.

ˆAnysﬁx

r

= Y ((cid:74)Ω∗Y(cid:75)r)†Y ∗.

2

Fixed-Rank Nyström Approximation: Proposal. The purpose of this paper is to develop  analyze 
and evaluate a new approach for ﬁxed-rank approximation of a psd matrix under the streaming model.
We propose a more intuitive rank-r approximation:

ˆAr =(cid:74) ˆAnys(cid:75)r.

That is  we report a best rank-r approximation of the full Nyström approximation (2.5).
This “matrix nearness” approach to ﬁxed-rank approximation appears in the papers [21  22  37]. The
combination with the Nyström method (2.5) is totally natural. Let us emphasize that the approach (2.7)
also applies to Nyström approximations outside the streaming setting.
Summary of Contributions. This paper contains a number of advances over the prior art:

(2.7)

1. We propose a new technique (2.7) for truncating the Nyström approximation to rank r. This

formulation differs from the published literature on ﬁxed-rank Nyström approximations.

2. We present a stable numerical implementation of (2.7) based on the best practices outlined

in the paper [27]. This approach is essential for achieving high precision! (Sec. 3)

3. We establish informative error bounds for the method (2.7). In particular  we prove that it

attains (1 + ε)-relative error in the Schatten 1-norm when k = Θ(r/ε). (Sec. 4)

4. We document numerical experiments on real and synthetic data to demonstrate that our
method dominates existing techniques [18  37] for ﬁxed-rank psd approximation. (Sec. 5)
Psd matrix approximation is a ubiquitous problem  so we expect these results to have a broad impact.
Related Work. Randomized algorithms for low-rank matrix approximation were proposed in
the late 1990s and developed into a technology in the 2000s; see [22  30  41]. In the absence of
constraints  such as streaming  we recommend the general-purpose methods from [22  23  27].
Algorithms for low-rank matrix approximation in the important streaming data setting are discussed
in [4  7  8  15  22  37  41  42]. Few of these methods are designed for psd matrices.
Nyström methods for low-rank psd matrix approximation appear in [11  13  16  17  19  22  26  34 
37  40  43]. These works mostly concern kernel matrices; they do not focus on the streaming model.
We are only aware of a few papers [16  17  19  37] on algorithms for psd matrix approximation
that operate under the streaming model (2.1). These papers form the comparison group.
After this paper was submitted  we learned about two contemporary works [35  39] that propose the
ﬁxed-rank approximation (2.7) in the context of kernel methods. Our research is distinctive because
we focus on the streaming setting  we obtain precise error bounds  we address numerical stability 
and we include an exhaustive empirical evaluation.
Finally  let us mention two very recent theoretical papers [6  33] that present existential results on
algorithms for ﬁxed-rank psd matrix approximation. The approach in [6] is only appropriate for
sparse input matrices  while the work [33] is not valid in the streaming setting.

3

Implementation

Distributions for the Test Matrix. To ensure that the sketch is informative  we must draw the
test matrix (2.2) at random from a suitable distribution. The choice of distribution determines the
computational requirements for the sketch (2.3)  the linear updates (2.4)  and the matrix approxima-
tion (2.7). It also affects the quality of the approximation (2.7). Let us outline some of the most useful
distributions. A full discussion is outside the scope of our work  but see [17  19  22  29  30  37  41].
Isotropic Models. Mathematically  the most natural model is to construct a test matrix Ω ∈ Fn×k
whose range is a uniformly random k-dimensional subspace in Fn. There are two approaches:

1. Gaussian. Draw each entry of the matrix Ω ∈ Fn×k independently at random from the
2. Orthonormal. Draw a Gaussian matrix G ∈ Fn×k  as above. Compute a thin orthogonal–

standard normal distribution on F.
triangular factorization G = ΩR to obtain the test matrix Ω ∈ Fn×k. Discard R.

Gaussian and orthonormal test matrices both require storage of kn ﬂoating-point numbers in F for
the test matrix Ω and another kn ﬂoating-point numbers for the sketch Y . In both cases  the cost of
multiplying a vector in Fn into Ω is Θ(kn) ﬂoating-point operations.

3

Algorithm 1 Sketch Initialization. Implements (2.2)–(2.3) with a random orthonormal test matrix.
Input: Positive-semideﬁnite input matrix A ∈ Fn×n; sketch size parameter k
Output: Constructs test matrix Ω ∈ Fn×k and sketch Y = AΩ ∈ Fn×k

1
2
3
4

5
6

7
8

local: Ω  Y
function NYSTROMSKETCH(A; k)

Ω ← randn(n  k)
Ω ← randn(n  k) + i ∗ randn(n  k)

if F = R then

if F = C then
Ω ← orth(Ω)
Y ← AΩ

(cid:46) Internal variables for NYSTROMSKETCH
(cid:46) Constructor

(cid:46) Improve numerical stability

Algorithm 2 Linear Update. Implements (2.4).
Input: Scalars θ1  θ2 ∈ R and conjugate symmetric H ∈ Fn×n
Output: Updates sketch to reﬂect linear innovation A ← θ1A + θ2H

1
2
3

local: Ω  Y
function LINEARUPDATE(θ1  θ2  H)

Y ← θ1Y + θ2HΩ

(cid:46) Internal variables for NYSTROMSKETCH

For isotropic models  we can analyze the approximation (2.7) in detail. In exact arithmetic  Gaussian
and isotropic test matrices yield identical Nyström approximations (Supplement). In ﬂoating-point
arithmetic  orthonormal matrices are more stable for large k  but we can generate Gaussian matrices
with less arithmetic and communication. References for isotropic test matrices include [21  22  31].
Subsampled Scrambled Fourier Transform (SSFT). One shortcoming of the isotropic models is
the cost of storing the test matrix and the cost of multiplying a vector into the test matrix. We can
often reduce these costs using an SSFT test matrix. An SSFT takes the form

Ω = Π1F Π2F R ∈ Fn×k.

(3.1)
The Πi ∈ Fn×n are independent  signed permutation matrices 1 chosen uniformly at random. The
matrix F ∈ Fn×n is a discrete Fourier transform (F = C) or a discrete cosine transform (F = R).
The matrix R ∈ Fn×k is a restriction to k coordinates  chosen uniformly at random.
An SSFT Ω requires only Θ(n) storage  but the sketch Y still requires storage of kn numbers.
We can multiply a vector in Fn into Ω using Θ(n log n) arithmetic operations via an FFT or FCT
algorithm. Thus  for most choices of sketch size k  the SSFT improves over the isotropic models.
In practice  the SSFT yields matrix approximations whose quality is identical to those we obtain with
an isotropic test matrix (Sec. 5). Although the analysis for SSFTs is less complete  the empirical
evidence conﬁrms that the theory for isotropic models also offers excellent guidance for SSFTs.
References for SSFTs and related test matrices include [1  2  9  22  29  36  42].
Numerically Stable Implementation. It requires care to compute the ﬁxed-rank approximation (2.7).
The supplement shows that a poor implementation may produce an approximation with 100% error!
Let us outline a numerically stable and very accurate implementation of (2.7)  based on an idea
from [27  38]. Fix a small parameter ν > 0. Instead of approximating the psd matrix A directly  we
approximate the shifted matrix Aν = A + νI and then remove the shift. Here are the steps:

1. Construct the shifted sketch Yν = Y + νΩ.
2. Form the matrix B = Ω∗Yν.
3. Compute a Cholesky decomposition B = CC∗.
4. Compute E = YνC−1 by back-substitution.
5. Compute the (thin) singular value decomposition E = U ΣV ∗.

6. Form ˆAr = U(cid:74)Σ2 − νI(cid:75)rU∗.

1A signed permutation has exactly one nonzero entry in each row and column; the nonzero has modulus one.

4

Algorithm 3 Fixed-Rank PSD Approximation. Implements (2.7).
Input: Matrix A in sketch must be psd; rank parameter 1 ≤ r ≤ k
Output: Returns factors U ∈ Fn×r with orthonormal columns and nonnegative  diagonal Λ ∈ Fr×r

that form a rank-r psd approximation ˆAr = U ΛU∗ of the sketched matrix A
local: Ω  Y
function FIXEDRANKPSDAPPROX(r)

(cid:46) Internal variables for NYSTROMSKETCH
(cid:46) µ = 2.2 · 10−16 in double precision
(cid:46) Sketch of shifted matrix A + νI

(cid:46) Force symmetry
(cid:46) Solve least squares problem; form thin SVD
(cid:46) Truncate to rank r
(cid:46) Square to get eigenvalues; remove shift

1
2
3
4
5
6
7
8
9
10

ν ← µ norm(Y )
Y ← Y + νΩ
B ← Ω∗Y
C ← chol((B + B∗)/2)
(U   Σ ∼) ← svd(Y /C  ’econ’)
U ← U (:  1:r) and Σ ← Σ(1:r  1:r)
Λ ← max{0  Σ2 − νI}
return (U   Λ)

The pseudocode addresses some additional implementation details. Related  but distinct  methods
were proposed by Williams & Seeger [40] and analyzed in Gittens’s thesis [17].
Pseudocode. We present detailed pseudocode for the sketch (2.2)–(2.4) and the implementation of
the ﬁxed-rank psd approximation (2.7) described above. For simplicity  we only elaborate the case of
a random orthonormal test matrix; we have also developed an SSFT implementation for empirical
testing. The pseudocode uses both mathematical notation and MATLAB 2017A functions.
Algorithms and Computational Costs. Algorithm 1 constructs a random orthonormal test matrix 
and computes the sketch (2.3) of an input matrix. The test matrix and sketch require the storage of
2kn ﬂoating-point numbers. Owing to the orthogonalization step  the construction of the test matrix
requires Θ(k2n) ﬂoating-point operations. For a general input matrix  the sketch requires Θ(kn2)
ﬂoating-point operations; this cost can be removed by initializing the input matrix to zero.
Algorithm 2 implements the linear update (2.4) to the sketch. Nominally  the computation requires
Θ(kn2) arithmetic operations  but this cost can be reduced when H has structure (e.g.  low rank).
Using the SSFT test matrix (3.1) also reduces this cost.
Algorithm 3 computes the rank-r psd approximation (2.7). This method requires additional storage of
Θ(kn). The arithmetic cost is Θ(k2n) operations  which is dominated by the SVD of the matrix E.

4 Theoretical Results

Relative Error Bound. Our ﬁrst result is an accurate bound for the expected Schatten 1-norm error
in the ﬁxed-rank psd approximation (2.7).
Theorem 4.1 (Fixed-Rank Nyström: Relative Error). Assume 1 ≤ r < k ≤ n. Let A ∈ Fn×n be a
psd matrix. Draw a test matrix Ω ∈ Fn×k from the Gaussian or orthonormal distribution  and form
the sketch Y = AΩ. Then the approximation ˆAr given by (2.5) and (2.7) satisﬁes

(cid:18)

E(cid:107)A − ˆAr(cid:107)1 ≤

(cid:19)
E(cid:107)A − ˆAr(cid:107)∞ ≤ (cid:107)A −(cid:74)A(cid:75)r(cid:107)∞ +

1 +

k − r − α

r

· (cid:107)A −(cid:74)A(cid:75)r(cid:107)1;

r

k − r − α

· (cid:107)A −(cid:74)A(cid:75)r(cid:107)1.

(4.1)

(4.2)

The quantities α(R) = 1 and α(C) = 0. Similar results hold with high probability.

The proof appears in the supplement.
In contrast to all previous analyses of randomized Nyström methods  Theorem 4.1 yields explicit 
sharp constants. (The contemporary work [39  Thm. 1] contains only a less precise variant of (4.1).)
As a consequence  the formulae (4.1)–(4.2) offer an a priori mechanism for selecting the sketch size
k to achieve a desired error bound. In particular  for each ε > 0 

k = (1 + ε−1)r + α implies E(cid:107)A − ˆAr(cid:107)1 ≤ (1 + ε) · (cid:107)A −(cid:74)A(cid:75)r(cid:107)1.

5

Thus  we can attain an arbitrarily small relative error in the Schatten 1-norm. In the streaming setting 
the scaling k = Θ(r/ε) is optimal for this result [14  Thm. 4.2]. Furthermore  it is impossible [41 
Sec. 6.2] to obtain “pure” relative error bounds in the Schatten ∞-norm unless k = Ω(n).
The Role of Spectral Decay. To circumvent these limitations  it is necessary to develop a different
kind of error bound. Our second result shows that the ﬁxed-rank psd approximation (2.7) automatically
exploits decay in the spectrum of the input matrix.
Theorem 4.2 (Fixed-Rank Nyström: Spectral Decay). Instate the notation and assumptions of
Theorem 4.1. Then

E(cid:107)A − ˆAr(cid:107)1 ≤ (cid:107)A −(cid:74)A(cid:75)r(cid:107)1 + 2 min
E(cid:107)A − ˆAr(cid:107)∞ ≤ (cid:107)A −(cid:74)A(cid:75)r(cid:107)∞ + 2 min

<k−α

<k−α

1 +



k −  − α



1 +

k −  − α

(cid:20)(cid:18)
(cid:20)(cid:18)

(cid:19)
(cid:21)
· (cid:107)A −(cid:74)A(cid:75)(cid:107)1
(cid:21)
(cid:19)
· (cid:107)A −(cid:74)A(cid:75)(cid:107)1

;

(4.3)

.

(4.4)

The index  ranges over the natural numbers.

The proof of Theorem 4.2 appears in the supplement.
Here is one way to understand this result. As the index  increases  the quantity /(k−−α) increases
while the rank- approximation error decreases. Theorem 4.2 states that the approximation (2.7)
automatically achieves the best tradeoff between these two terms. When the spectrum of A decays 
the rank- approximation error may be far smaller than the rank-r approximation error. In this case 
Theorem 4.2 is tighter than Theorem 4.1  although the prediction is more qualitative.
Additional Results. The proofs can be extended to obtain high-probability bounds  as well as results
for other Schatten norms or for other test matrices (Supplement).

5 Numerical Performance

Experimental Setup. In many streaming applications  such as [44]  it is essential that the sketch
uses as little memory as possible and that the psd approximation achieves the best possible error. For
the methods we consider  the arithmetic costs of linear updates and psd approximation are roughly
comparable. Therefore  we only assess storage and accuracy.
For the numerical experiments  the ﬁeld F = C except when noted explicitly. Choose a psd input
matrix A ∈ Fn×n and a target rank r. Then ﬁx a sketch size parameter k with r ≤ k ≤ n. For each
trial  draw the test matrix Ω from the orthonormal or the SSFT distribution  and form the sketch
Y = AΩ of the input matrix. Using Algorithm 3  compute the rank-r psd approximation ˆAr deﬁned
in (2.7). We evaluate the performance using the relative error metric:

Schatten p-norm relative error =

− 1.

(5.1)

(cid:107)A − ˆAr(cid:107)p

(cid:107)A −(cid:74)A(cid:75)r(cid:107)p

We perform 20 independent trials and report the average error.
We compare our method (2.7) with the standard truncated Nyström approximation (2.6); the best
reference for this type of approach is [18  Sec. 2.2]. The approximation (2.6) is constructed from the
same sketch as (2.7)  so the experimental procedure is identical.
We also consider the sketching method and psd approximation algorithm [37  Alg. 9] based on earlier
work from [7  22  42]. We implemented this sketch with orthonormal matrices and also with SSFT
matrices. The sketch has two different parameters (k  (cid:96))  so we select the parameters that result in the
minimum relative error. Otherwise  the experimental procedure is the same.
We apply the methods to representative input matrices; see the Supplement for plots of the spectra.
Synthetic Examples. The synthetic examples are diagonal with dimension n = 103; results for
larger and non-diagonal matrices are similar. These matrices are parameterized by an effective rank
parameter R  which takes values in {5  10  20}. We compute approximations with rank r = 10.

1. Low-Rank + PSD Noise. These matrices take the form

A = diag(1  . . .   1

  0  . . .   0) + ξn−1W ∈ Fn×n.

(cid:124) (cid:123)(cid:122) (cid:125)

R

6

(A) PhaseRetrieval (r = 1)

(B) PhaseRetrieval (r = 5)

(C) MaxCut (r = 1)

(D) MaxCut (r = 14)

FIGURE 5.1: Application Examples  Approximation Rank r  Schatten 1-Norm Error. The data
series show the performance of three algorithms for rank-r psd approximation. Solid lines are
generated from the Gaussian sketch; dashed lines are from the SSFT sketch. Each panel displays the
Schatten 1-norm relative error (5.1) as a function of storage cost T . See Sec. 5 for details.

The matrix W ∈ Fn×n has the WISHART(n  n; F) distribution; that is  W = GG∗ where
G ∈ Fn×n is standard normal. The parameter ξ controls the signal-to-noise ratio. We
consider three examples: LowRankLowNoise (ξ = 10−4)  LowRankMedNoise (ξ = 10−2) 
LowRankHiNoise (ξ = 10−1).

2. Polynomial Decay. These matrices take the form

A = diag(1  . . .   1

  2−p  3−p  . . .   (n − R + 1)−p) ∈ Fn×n.

(cid:124) (cid:123)(cid:122) (cid:125)

R

(cid:124) (cid:123)(cid:122) (cid:125)

R

The parameter p > 0 controls the rate of polynomial decay. We consider three examples:
PolyDecaySlow (p = 0.5)  PolyDecayMed (p = 1)  PolyDecayFast (p = 2).

3. Exponential Decay. These matrices take the form

A = diag(1  . . .   1

  10−q  10−2q  . . .   10−(n−R)q) ∈ Fn×n.

The parameter q > 0 controls the rate of exponential decay. We consider three examples:
ExpDecaySlow (q = 0.1)  ExpDecayMed (q = 0.25)  ExpDecayFast (q = 1).

Application Examples. We also consider non-diagonal matrices inspired by the SDP algorithm [44].
1. MaxCut: This is a real-valued psd matrix with dimension n = 2 000  and its effective rank
R = 14. We form approximations with rank r ∈ {1  14}. The matrix is an approximate
solution to the MAXCUT SDP [20] for the sparse graph G40 [10].
2. PhaseRetrieval: This is a psd matrix with dimension n = 25 921. It has exact rank 250 
but its effective rank R = 5. We form approximations with rank r ∈ {1  5}. The matrix is
an approximate solution to a phase retrieval SDP; the data is drawn from our paper [44].

Experimental Results. Figures 5.1–5.2 display the performance of the three ﬁxed-rank psd approxi-
mation methods for a subcollection of the input matrices. The vertical axis is the Schatten 1-norm

7

Storage(T)1248163264128RelativeError(S1)10-810-610-410-2Storage(T)61224489619210-810-610-410-2100Storage(T)1248163264128RelativeError(S1)10-810-610-410-2Storage(T)16326412825610-1100101102[TYUC17 Alg.9]Standard(2.6)Proposed(2.7)(A) LowRankLowNoise

(B) LowRankMedNoise

(C) LowRankHiNoise

(D) PolyDecayFast

(E) PolyDecayMed

(F) PolyDecaySlow

(G) ExpDecayFast

(H) ExpDecayMed

(I) ExpDecaySlow

FIGURE 5.2: Synthetic Examples with Effective Rank R = 10  Approximation Rank r = 10 
Schatten 1-Norm Error. The data series show the performance of three algorithms for rank-r psd
approximation with r = 10. Solid lines are generated from the Gaussian sketch; dashed lines are
from the SSFT sketch. Each panel displays the Schatten 1-norm relative error (5.1) as a function of
storage cost T .

relative error (5.1). The variable T on the horizontal axis is proportional to the storage required for
the sketch only. For the Nyström-based approximations (2.6)–(2.7)  we have the correspondence
T = k. For the approximation [37  Alg. 9]  we set T = k + (cid:96).
The experiments demonstrate that the proposed method (2.7) has a signiﬁcant beneﬁt over the
alternatives for input matrices that admit a good low-rank approximation. It equals or improves on the
competitors for almost all other examples and storage budgets. The supplement contains additional
numerical results; these experiments only reinforce the message of Figures 5.1–5.2.
Conclusions. This paper makes the case for using the proposed ﬁxed-rank psd approximation (2.7)
in lieu of the alternatives (2.6) or [37  Alg. 9]. Theorem 4.1 shows that the proposed ﬁxed-rank psd
approximation (2.7) can attain any prescribed relative error  and Theorem 4.2 shows that it can exploit
spectral decay. Furthermore  our numerical work demonstrates that the proposed approximation
improves (almost) uniformly over the competitors for a range of examples. These results are timely
because of the recent arrival of compelling applications  such as [44]  for sketching psd matrices.

8

Storage(T)12244896192RelativeError(S1)10-1100[TYUC17 Alg.9]Standard(2.6)Proposed(2.7)Storage(T)1224489619210-210-1Storage(T)1224489619210-210-1Storage(T)12244896192RelativeError(S1)10-310-210-1100Storage(T)1224489619210-210-1Storage(T)1224489619210-210-1Storage(T)12244896192RelativeError(S1)10-810-610-410-2100Storage(T)1224489619210-810-610-410-2100Storage(T)1224489619210-810-610-410-2100Acknowledgments. The authors wish to thank Mark Tygert and Alex Gittens for helpful feedback
on preliminary versions of this work. JAT gratefully acknowledges partial support from ONR Award
N00014-17-1-2146 and the Gordon & Betty Moore Foundation. VC and AY were supported in
part by the European Commission under Grant ERC Future Proof  SNF 200021-146750  and SNF
CRSII2-147633. MU was supported in part by DARPA Award FA8750-17-2-0101.

References
[1] N. Ailon and B. Chazelle. The fast Johnson-Lindenstrauss transform and approximate nearest neighbors.

SIAM J. Comput.  39(1):302–322  2009.

[2] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized Hadamard

transform. SIAM J. Matrix Anal. Appl.  34(3):1301–1340  2013.

[3] C. Boutsidis  D. Garber  Z. Karnin  and E. Liberty. Online principal components analysis. In Proc. 26th

Ann. ACM-SIAM Symp. Discrete Algorithms (SODA)  pages 887–901  2015.

[4] C. Boutsidis  D. Woodruff  and P. Zhong. Optimal principal component analysis in distributed and

streaming models. In Proc. 48th ACM Symp. Theory of Computing (STOC)  2016.

[5] J. Chiu and L. Demanet. Sublinear randomized algorithms for skeleton decompositions. SIAM J. Matrix

Anal. Appl.  34(3):1361–1383  2013.

[6] K. Clarkson and D. Woodruff. Low-rank PSD approximation in input-sparsity time. In Proc. 28th Ann.

ACM-SIAM Symp. Discrete Algorithms (SODA)  pages 2061–2072  Jan. 2017.

[7] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proc. 41st ACM

Symp. Theory of Computing (STOC)  2009.

[8] M. B. Cohen  S. Elder  C. Musco  C. Musco  and M. Persu. Dimensionality reduction for k-means
clustering and low rank approximation. In Proc. 47th ACM Symp. Theory of Computing (STOC)  pages
163–172. ACM  New York  2015.

[9] M. B. Cohen  J. Nelson  and D. P. Woodruff. Optimal Approximate Matrix Product in Terms of Stable
Rank. In 43rd Int. Coll. Automata  Languages  and Programming (ICALP)  volume 55  pages 11:1–11:14 
2016.

[10] T. A. Davis and Hu. The University of Florida sparse matrix collection. ACM Trans. Math. Softw.  3(1):

1:1–1:25  2011.

[11] P. Drineas and M. W. Mahoney. On the Nyström method for approximating a Gram matrix for improved

kernel-based learning. J. Mach. Learn. Res.  6:2153–2175  2005.

[12] D. Feldman  M. Volkov  and D. Rus. Dimensionality reduction of massive sparse datasets using coresets.

In Adv. Neural Information Processing Systems 29 (NIPS)  2016.

[13] C. Fowlkes  S. Belongie  F. Chung  and J. Malik. Spectral grouping using the Nyström method. IEEE

Trans. Pattern Anal. Mach. Intell.  26(2):214–225  Jan. 2004.

[14] M. Ghasemi  E. Liberty  J. M. Phillips  and D. P. Woodruff. Frequent directions: Simple and deterministic

matrix sketching. SIAM J. Comput.  45(5):1762–1792  2016.

[15] A. C. Gilbert  J. Y. Park  and M. B. Wakin. Sketched SVD: Recovering spectral features from compressed

measurements. Available at http://arXiv.org/abs/1211.0361  Nov. 2012.

[16] A. Gittens. The spectral norm error of the naïve Nyström extension. Available at http:arXiv.org/abs/

1110.5305  Oct. 2011.

[17] A. Gittens. Topics in Randomized Numerical Linear Algebra. PhD thesis  California Institute of Technology 

2013.

[18] A. Gittens and M. W. Mahoney. Revisiting the Nyström method for improved large-scale machine learning.

Available at http://arXiv.org/abs/1303.1849  Mar. 2013.

[19] A. Gittens and M. W. Mahoney. Revisiting the Nyström method for improved large-scale machine learning.

J. Mach. Learn. Res.  17:Paper No. 117  65  2016.

[20] M. X. Goemans and D. P. Williamson.

Improved approximation algorithms for maximum cut and
satisﬁability problems using semideﬁnite programming. J. Assoc. Comput. Mach.  42(6):1115–1145  1995.
[21] M. Gu. Subspace iteration randomization and singular value problems. SIAM J. Sci. Comput.  37(3):

A1139–A1173  2015.

[22] N. Halko  P. G. Martinsson  and J. A. Tropp. Finding structure with randomness: probabilistic algorithms

for constructing approximate matrix decompositions. SIAM Rev.  53(2):217–288  2011.

9

[23] Nathan Halko  Per-Gunnar Martinsson  Yoel Shkolnisky  and Mark Tygert. An algorithm for the principal
component analysis of large data sets. SIAM J. Sci. Comput.  33(5):2580–2594  2011. ISSN 1064-8275.
doi: 10.1137/100804139. URL http://dx.doi.org/10.1137/100804139.

[24] N. J. Higham. Matrix nearness problems and applications. In Applications of matrix theory (Bradford 

1988)  pages 1–27. Oxford Univ. Press  New York  1989.

[25] P. Jain  C. Jin  S. M. Kakade  P. Netrapalli  and A. Sidford. Streaming PCA: Matching matrix Bernstein and
near-optimal ﬁnite sample guarantees for Oja’s algorithm. In 29th Ann. Conf. Learning Theory (COLT) 
pages 1147–1164  2016.

[26] S. Kumar  M. Mohri  and A. Talwalkar. Sampling methods for the Nyström method. J. Mach. Learn. Res. 

13:981–1006  Apr. 2012.

[27] H. Li  G. C. Linderman  A. Szlam  K. P. Stanton  Y. Kluger  and M. Tygert. Algorithm 971: An
implementation of a randomized algorithm for principal component analysis. ACM Trans. Math. Softw.  43
(3):28:1–28:14  Jan. 2017.

[28] Y. Li  H. L. Nguyen  and D. P. Woodruff. Turnstile streaming algorithms might as well be linear sketches.

In Proc. 2014 ACM Symp. Theory of Computing (STOC)  pages 174–183. ACM  2014.

[29] E. Liberty. Accelerated dense random projections. PhD thesis  Yale Univ.  New Haven  2009.
[30] M. W. Mahoney. Randomized algorithms for matrices and data. Found. Trends Mach. Learn.  3(2):123–224 

2011.

[31] P.-G. Martinsson  V. Rokhlin  and M. Tygert. A randomized algorithm for the decomposition of matrices.

Appl. Comput. Harmon. Anal.  30(1):47–68  2011.

[32] I. Mitliagkas  C. Caramanis  and P. Jain. Memory limited  streaming PCA. In Adv. Neural Information

Processing Systems 26 (NIPS)  pages 2886–2894  2013.

[33] C. Musco and D. Woodruff. Sublinear time low-rank approximation of positive semideﬁnite matrices.

Available at http://arXiv.org/abs/1704.03371  Apr. 2017.

[34] J. C. Platt. FastMap  MetricMap  and Landmark MDS are all Nyström algorithms. In Proc. 10th Int.

Workshop Artiﬁcial Intelligence and Statistics (AISTATS)  pages 261–268  2005.

[35] F. Pourkamali-Anaraki and S. Becker. Randomized clustered Nyström for large-scale kernel machines.

Available at http://arXiv.org/abs/1612.06470  Dec. 2016.

[36] J. A. Tropp. Improved analysis of the subsampled randomized Hadamard transform. Adv. Adapt. Data

Anal.  3(1-2):115–126  2011.

[37] J. A. Tropp  A. Yurtsever  M. Udell  and V. Cevher. Randomized single-view algorithms for low-rank
matrix approximation. ACM Report 2017-01  Caltech  Pasadena  Jan. 2017. Available at http://arXiv.
org/abs/1609.00048  v1.

[38] M. Tygert. Beta versions of Matlab routines for principal component analysis. Available at http:

//tygert.com/software.html  2014.

[39] S. Wang  A. Gittens  and M. W. Mahoney. Scalable kernel K-means clustering with Nyström approximation:

relative-error bounds. Available at http://arXiv.org/abs/1706.02803  June 2017.

[40] C. K. I. Williams and M. Seeger. Using the Nyström method to speed up kernel machines. In Adv. Neural

Information Processing Systems 13 (NIPS)  2000.

[41] D. P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor. Comput. Sci.  10

(1-2):iv+157  2014.

[42] F. Woolfe  E. Liberty  V. Rokhlin  and M. Tygert. A fast randomized algorithm for the approximation of

matrices. Appl. Comput. Harmon. Anal.  25(3):335–366  2008.

[43] T. Yang  Y.-F. Li  M. Mahdavi  R. Jin  and Z.-H. Zhou. Nyström method vs random Fourier features: A
theoretical and empirical comparison. In Adv. Neural Information Processing Systems 25 (NIPS)  pages
476–484  2012.

[44] A. Yurtsever  M. Udell  J. A. Tropp  and V. Cevher. Sketchy decisions: Convex low-rank matrix optimization
In Proc. 20th Int. Conf. Artiﬁcial Intelligence and Statistics (AISTATS)  Fort

with optimal storage.
Lauderdale  May 2017.

10

,Arindam Banerjee
Sheng Chen
Farideh Fazayeli
Vidyashankar Sivakumar
Jason Lee
Yuekai Sun
Jonathan Taylor
Joel Tropp
Alp Yurtsever
Madeleine Udell
Volkan Cevher