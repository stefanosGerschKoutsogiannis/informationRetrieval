2009,A Smoothed Approximate Linear Program,We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program -- the `smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First  we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. Second  experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude.,ASmoothedApproximateLinearProgramVijayV.DesaiIEOR ColumbiaUniversityvvd2101@columbia.eduVivekF.FariasMITSloanvivekf@mit.eduCiamacC.MoallemiGSB ColumbiaUniversityciamac@gsb.columbia.eduAbstractWepresentanovellinearprogramfortheapproximationofthedynamicprogrammingcost-to-gofunctioninhigh-dimensionalstochasticcontrolproblems.LPapproachestoapproximateDPnaturallyrestrictattentiontoapproximationsthatarelowerboundstotheoptimalcost-to-gofunc-tion.Ourprogram–the‘smoothedapproximatelinearprogram’–relaxesthisrestrictioninanappropriatefashionwhileremainingcomputation-allytractable.Doingsoappearstohaveseveraladvantages:First wedemonstratesuperiorboundsonthequalityofapproximationtotheop-timalcost-to-gofunctionaﬀordedbyourapproach.Second experimentswithourapproachonachallengingproblem(thegameofTetris)showthattheapproachoutperformstheexistingLPapproach(whichhaspreviouslybeenshowntobecompetitivewithseveralADPalgorithms)byanorderofmagnitude.1IntroductionManydynamicoptimizationproblemscanbecastasMarkovdecisionproblems(MDPs)andsolved inprinciple viadynamicprogramming.Unfortunately thisapproachisfrequentlyuntenableduetothe‘curseofdimensionality’.Approximatedynamicprogramming(ADP)isanapproachwhichattemptstoaddressthisdiﬃculty.ADPalgorithmsseektocomputegoodapproximationstothedynamicprogramingoptimalcost-to-gofunctionwithinthespanofsomepre-speciﬁedsetofbasisfunctions.Theapproximatelinearprogramming(ALP)approachtoADP[1 2]isonesuchwell-recognizedapproach.TheprogramemployedintheALPapproachisidenticaltotheLPusedforexactcom-putationoftheoptimalcost-to-gofunction withfurtherconstraintslimitingsolutionstothelow-dimensionalsubspacespannedbythebasisfunctionsused.Theresultinglowdi-mensionalLPimplicitlyrestrictsattentiontoapproximationsthatarelowerboundsontheoptimalcost-to-gofunction.Whilethestructureofthisprogramappearscrucialinestab-lishingapproximationguaranteesfortheapproach therestrictiontolowerboundsleadsonetoaskwhethertheALPisthe‘right’LP.Inparticular couldanappropriaterelaxationofthefeasibleregionoftheALPallowforbetterapproximationstothecost-to-gofunctionwhileremainingcomputationallytractable?Motivatedbythisquestion thepresentpaperpresentsanewlinearprogramforADPwecallthe‘smoothed’ALP(orSALP).TheSALPmaybeviewedasarelaxationoftheALPwhereinoneisallowedtoviolatetheALPconstraintsforanygivenstate.Auserdeﬁned‘violationbudget’parametercontrolsthe‘expected’violationacrossstates;abudgetof0thusyieldstheoriginalALP.Wespecifyachoiceofthisviolationbudgetthatyieldsarelaxationwithattractiveproperties.Inparticular weareabletoestablishstrongapproximationguaranteesfortheSALP;theseguaranteesaresubstantiallystrongerthanthecorrespondingguaranteesfortheALP.ThenumberofconstraintsandvariablesintheSALPscalewiththesizeoftheMDPstatespace.Wenonethelessestablishsamplecomplexityboundsthatdemonstratethatan1appropriate‘sampled’SALPprovidesagoodapproximationtotheSALPsolutionwithatractablenumberofsampledMDPstates.Thissampledprogramisnomorecomplexthanthe‘sampled’ALPand assuch wedemonstratethattheSALPisessentiallynohardertosolvethantheALP.WepresentacomputationalstudydemonstratingtheeﬃcacyofourapproachonthegameofTetris.TheALPhasbeendemonstratedtobecompetitivewithseveralADPapproachesforTetris(see[3]).IndetailedcomparisonswiththeALP weestimatethattheSALPprovidesanorderofmagnitudeimprovementovercontrollersdesignedviathatapproachforthegameofTetris.2ProblemFormulationOursettingisthatofadiscrete-time discountedinﬁnite-horizon cost-minimizingMDPwithaﬁnitestatespaceXandﬁniteactionspaceA.Giventhestateandactionattimet xtandat aper-stagecostg(xt at)isincurred.Thesubsequentstatext+1isdeterminedaccordingtothetransitionprobabilitykernelPat(xt ·).Astationarypolicyµ:X→Aisamappingthatdeterminestheactionateachtimeasafunctionofthestate.Giveneachinitialstatex0=x theexpecteddiscountedcost(cost-to-gofunction)ofthepolicyµisgivenbyJµ(x) Eµ"∞Xt=0αtg(xt µ(xt))(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x0=x#.where α∈(0 1)isthediscountfactor.DenotebyPµ∈RX×Xthetransitionprobabilitymatrixforthepolicyµ whose(x x0)thentryisPµ(x)(x x0).Denotebygµ∈RXthevectorwhosexthentryisg(x µ(x)).Then thecost-to-gofunctionJµistheuniquesolutiontotheequationTµJ=J wheretheoperatorTµisdeﬁnedbyTµJ=gµ+αPµJ.TheBellmanoperatorTcanbedeﬁnedaccordingtoTJ=minµTµJ.Bellman’sequationisthentheﬁxedpointequation TJ=J.Itisreadilyshownthattheoptimalcost-to-gofunctionJ∗istheuniquesolutiontoBellman’sequationandthatacorrespondingoptimalpolicyµ∗isgreedywithrespecttoJ∗;i.e. µ∗satisﬁesTJ∗=Tµ∗J∗.Bellman’sequationmaybesolvedexactlyviathefollowinglinearprogram:(1)maximizeJν>JsubjecttoJ≤TJ.Here ν∈RXisavectorwithpositivecomponentsthatareknownasthestate-relevanceweights.TheaboveprogramisindeedanLPsincetheconstraintJ(x)≤(TJ)(x)isequiva-lenttothesetoflinearconstraintsJ(x)≤g(x a)+αPx0∈XPa(x x0)J(x0) ∀a∈A.Wereferto(1)astheexactLP.NotethatifavectorJsatisﬁesJ≤TJ thenJ≤TkJ(bymonotonicityoftheBellmanoperator) andthusJ≤J∗(sincetheBellmanoperatorisacontractionwithuniqueﬁxedpointJ∗).Then everyfeasiblepointfor(1)isacomponent-wiselowerboundtoJ∗ andJ∗istheuniqueoptimalsolutiontotheexactLP(1).ForproblemswhereXisprohibitivelylarge anADPalgorithmseekstoﬁndagoodap-proximationtoJ∗.Speciﬁcally oneconsidersacollectionofbasisfunctions{φ1 ... φK}whereeachφi:X→R.DeﬁningΦ [φ1φ2...φK]tobeamatrixwithcolumnsconsistingofbasisfunctions oneseeksanapproximationoftheformJr=Φr withthehopethatJr∼J∗.TheALPforthistaskisthensimply(2)maximizerν>ΦrsubjecttoΦr≤TΦr.ThegeometricintuitionbehindtheALPisillustratedinFigure1(a).SupposedthatrALPisavectorthatisoptimalfortheALP.ThentheapproximatevaluefunctionΦrALPwilllieonthesubspacespannedbythecolumnsofΦ asillustratedbytheorangeline.ΦrALP2willalsosatisfytheconstraintsoftheexactLP illustratedbythedarkgrayregion;thisimpliesthatΦrALP≤J∗.Inotherwords theapproximatecost-to-gofunctionisnecessarilyapoint-wiselowerboundtothetruecost-to-gofunctioninthespanofΦ.J=ΦrΦrALPJ∗νJ(1)J(2)(a)ALPcase.J=ΦrΦrSALPJ∗νJ(1)J(2)(b)SALPcase.Figure1:AcartoonillustratingthefeasiblesetandoptimalsolutionfortheALPandSALP inthecaseofatwo-stateMDP.Theaxescorrespondtothecomponentsofthevaluefunction.AcarefulrelaxationfromthefeasiblesetoftheALPtothatoftheSALPcanyieldanimprovedapproximation.3TheSmoothedALPTheJ≤TJconstraintsintheexactLP whichcarryovertotheALP imposeastrongrestrictiononthecost-to-gofunctionapproximation:inparticulartheyrestrictustoap-proximationsthatarelowerboundstoJ∗ateverypointinthestatespace.Inthecasewherethestatespaceisverylarge andthenumberofbasisfunctionsis(relatively)small itmaybethecasethatconstraintsarisingfromrarelyvisitedorpathologicalstatesarebindingandinﬂuencetheoptimalsolution.Inmanycases ourultimategoalisnottoﬁndalowerboundontheoptimalcost-to-gofunction butratheragoodapproximationtoJ∗.Intheseinstances itmaybethecasethatrelaxingtheconstraintsintheALPsoasnottorequireauniformlowerboundmayallowforbetteroverallapproximationstotheoptimalcost-to-gofunction.ThisisalsoillustratedinFigure1.RelaxingthefeasibleregionoftheALPinFigure1(b)tothelightgrayregioninFigure1(b)wouldyieldthepointΦrSALPasanoptimalsolution.Therelaxationinthiscaseisclearlybeneﬁcial;itallowsustocomputeabetterapproximationtoJ∗thanthepointΦrSALP.Canweconstructafruitfulrelaxationofthissortingeneral?Thesmoothedapproximatelinearprogram(SALP)isgivenby:(3)maximizer sν>ΦrsubjecttoΦr≤TΦr+s π>s≤θ s≥0.Here avectors∈RXofadditionaldecisionvariableshasbeenintroduced.Foreachstatex s(x)isanon-negativedecisionvariable(aslack)thatallowsforviolationofthecorrespondingALPconstraint.Theparameterθ≥0isanon-negativescalar.Theparameterπ∈RXisaprobabilitydistributionknownastheconstraintviolationdistribution.Theparameterθisthusaviolationbudget:theexpectedviolationoftheΦr≤TΦrconstraint underthedistributionπ mustbelessthanθ.ThebalanceofthepaperisconcernedwithestablishingthattheSALPformsthebasisofausefulADPalgorithminlargescaleproblems:•WeidentifyaconcretechoiceofviolationbudgetθandanidealizedconstraintviolationdistributionπforwhichtheSALPprovidesausefulrelaxationinthattheoptimalsolutioncanbeabetterapproximationtotheoptimalcost-to-gofunction.ThisbringsthecartoonimprovementinFigure1tofruitionforgeneralproblems.3•WeshowthattheSALPistractable(i.eitiswellapproximatedbyanappropri-ate‘sampled’version)andpresentcomputationalexperimentsforahardproblem(Tetris)illustratinganorderofmagnitudeimprovementovertheALP.4AnalysisThissectionisdedicatedtoatheoreticalanalysisoftheSALP.Theoverarchingobjectiveofthisanalysisistoprovidesomeassuranceofthesoundnessoftheproposedapproach.Inaddition ouranalysiswillserveasacrucialguidetopracticalimplementationoftheSALP.Ouranalysiswillpresenttwotypesofresults:First weproveapproximationguarantees(Sections4.1and4.2)thatwillindicatethattheSALPcomputesapproximationsthatareofcomparablequalitytotheprojectionofJ∗onthelinearspanofΦ.Second weshow(Section4.3)thatanimplementable‘sampled’versionoftheSALPmaybeusedtoapproximatetheSALPwithatractablenumberofsamples.Allproofscanbefoundinthetechnicalappendix.IdealizedAssumptions:GiventhebroadscopeofproblemsaddressedbyADPalgo-rithms analysesofsuchalgorithmstypicallyrelyonan‘idealized’assumptionofsomesort.InthecaseoftheALP oneeitherassumestheabilitytosolvealinearprogramwithasmanyconstraintsastherearestates orabsentthat knowledgeofacertainidealizedsamplingdistribution sothatonecanthenproceedwithsolvinga‘sampled’versionoftheALP.OuranalysisoftheSALPinthissectionispredicatedontheknowledgeofanidealizedconstraintviolationdistribution whichisthissameidealizedsamplingdistribution.Inpar-ticular wewillrequireaccesstosamplesdrawnaccordingtothedistributionπµ∗ νgivenbyπ>µ∗ ν (1−α)ν>(I−αPµ∗)−1.Hereνisanarbitraryinitialdistributionoverstates.Thedistributionπµ∗ νmaybeinterpretedasyieldingthediscountedexpectedfrequencyofvisitstoagivenstatewhentheinitialstateisdistributedaccordingtoνandthesystemrunsundertheoptimalpolicyµ∗.Wenotethatthe‘sampled’ALPintroducedbydeFariasandVanRoy[2]requiresaccesstostatessampledaccordingtopreciselythisdistribution.4.1ASimpleApproximationGuaranteeWepresentaﬁrst simpleapproximationguaranteeforthefollowingspecializationoftheSALPin(3):(4)maximizer sν>ΦrsubjecttoΦr≤TΦr+s π>µ∗ νs≤θ s≥0.Beforeweproceedtostateourresult wedeﬁneausefulfunction:(5)‘(r θ) minimizes γγsubjecttoΦr−TΦr≤s+γ1 π>µ∗ νs≤θ s≥0.‘(r θ)istheminimumtranslation(inthedirectionofthevector1)ofanarbitraryweightvectorrsoastoresultinafeasiblevectorfor(4).Wewilldenotebys(r θ)thescomponentofthesolutionto(5).ThefollowingLemmacharacterizesl(r θ):Lemma1.Foranyr∈RKandθ≥0:(i)‘(r θ)isabounded decreasing piecewiselinear convexfunctionofθ.(ii)‘(r θ)≤(1+α)kJ∗−Φrk∞.(iii)∂∂r‘(r 0)=−1Px∈Ω(r)πµ∗ ν(x) whereΩ(r)=argmaxx∈XΦr(x)−TΦr(x).Armedwiththisdeﬁnition wearenowinapositiontostateourﬁrst crudeapproximationguarantee:4Theorem1.Let1beinthespanofΦandνbeaprobabilitydistribution.Let¯rbeanoptimalsolutiontotheSALP(4).Moreover letr∗satisfyr∗∈argminrkJ∗−Φrk∞.Then kJ∗−Φ¯rk1 ν≤kJ∗−Φr∗k∞+l(r∗ θ)+2θ1−α.Theabovetheoremallowsustointerpret‘(r∗ θ)+2θ1−αastheapproximationerrorassociatedwiththeSALPsolution¯r.Considersettingθ=0 inwhichcase(4)isidenticaltotheALP.Inthiscase wehavefromLemma1that‘(r∗ 0)≤(1+α)kJ∗−Φr∗k∞ sothattherighthandsideofourboundisatmost21−αkJ∗−Φr∗k∞.ThisispreciselyTheorem2indeFariasandVanRoy[1];werecovertheirapproximationguaranteefortheALP.Nextobservethat from(iii) ifthesetΩ(r∗)isofsmallprobabilityaccordingtothedistributionπµ∗ ν weexpectthat‘(r∗ θ)willdecreasedramaticallyasθisincreasedfrom0.IntheeventthatΦr∗(x)−TΦr∗(x)islargeforonlyasmallnumberofstates(thatis theBellmanerroroftheapproximationproducedbyr∗islargeforonlyasmallnumberofstates) wethusexpecttohaveachoiceofθforwhichl(r∗ θ)+2θ(cid:28)l(r∗ 0).Thus Theorem1reinforcestheintuition(shownviaFigure1)thattheSALPwillpermitcloserapproximationstoJ∗thantheALP.TheboundinTheorem1leavesroomforimprovement:1.Therighthandsideofourboundmeasuresprojectionerror kJ∗−Φr∗k∞intheL∞-norm.SinceitisunlikelythatthebasisfunctionsΦwillprovideauniformlygoodapproximationovertheentirestatespace therighthandsideofourboundcouldbequitelarge.2.Thechoiceofstaterelevanceweightscansigniﬁcantlyinﬂuencethesolution.Whilewedonotshowthishere thischoiceallowsustochooseregionsofthestatespacewherewewouldlikeabetterapproximationofJ∗.Therighthandsideofourbound however isindependentofν.3.Ourguaranteedoesnotsuggestaconcretechoiceoftheviolationbudget θ.Thenextsectionwillpresentasubstantiallyreﬁnedapproximationbound.4.2ABetterApproximationGuaranteeWiththeintentofderivingstrongerapproximationguarantees webeginthissectionbyintroducinga‘nicer’measureofthequalityofapproximationaﬀordedbyΦ.Inparticular insteadofmeasuringkJ∗−Φr∗kintheL∞normaswedidforourpreviousbounds wewilluseaweightedmaxnormdeﬁnedaccordingto:kJk∞ 1/ψ maxx∈X|J(x)|/ψ(x) whereψ:X→[1 ∞)isagivenweightingfunction.Theweightingfunctionψallowsustoweightapproximationerrorinanon-uniformfashionacrossthestatespaceandinthismannerpotentiallyignoreapproximationqualityinregionsofthestatespacethat‘don’tmatter’.Inadditiontospecifyingtheconstraintviolationdistributionπaswedidforourpreviousbound wewillspecify(implicitly)aparticularchoiceoftheviolationbudgetθ.Inparticular wewillconsidersolvingthefollowingSALP:(6)maximizer sν>Φr−2π>µ∗ νs1−αsubjecttoΦr≤TΦr+s s≥0.Itisclearthat(6)isequivalentto(4)foraspeciﬁcchoiceofθ.Wethenhave:Theorem2.LetΨ {y∈R|X|:y≥1}.Foreveryψ∈Ψ letβ(ψ)=maxµ(cid:13)(cid:13)(cid:13)Pµψψ(cid:13)(cid:13)(cid:13)∞.Then foranoptimalsolution(rSALP ¯s)to(6) wehave:kJ∗−ΦrSALPk1 ν≤infr ψ∈ΨkJ∗−Φrk∞ 1/ψ ν>ψ+2(π>µ∗ νψ+1)(αβ(ψ)+1)1−α!.5Itisworthplacingtheresultincontexttounderstanditsimplications.Forthis werecallacloselyrelatedresultshownbydeFariasandVanRoy[1]fortheALP.Inparticular deFariasandVanRoy[1]showedthatgivenanappropriateweighting(orintheircontext ‘Lyapunov’)functionψ onemaysolveanALP withψinthespanofthebasisfunctionsΦ;thesolutiontosuchanALPthensatisﬁes:kJ∗−Φ¯rk1 ν≤infrkJ∗−Φrk∞ 1/ψ2ν>ψ1−αβ(ψ)providedβ(ψ)≤1/α.Selectinganappropriateψintheircontextisviewedtobeanimportanttaskforpracticalperformanceandoftenrequiresagooddealofproblemspeciﬁcanalysis;deFariasandVanRoy[1]identifyappropriateψforseveralqueueingmodels(notethatthisisequivalenttoidentifyingadesirablebasisfunction).Incontrast theguaranteewepresentoptimizesoverallpossibleψ1.Thus theapproximationguaranteeofTheorem2allowsustoviewtheSALPasautomatingthecriticalprocedureofidentifyingagoodLyapunovfunctionforagivenproblem.4.3SampleComplexityOuranalysisthusfarhasassumedwehavetheabilitytosolvetheSALP aprogramwithapotentiallyintractablenumberofconstraintsandvariables.Asitturnsout asolutiontotheSALPiswellapproximatedbythesolutiontoacertain‘sampled’programwhichwenowdescribe:LetˆX={x1 x2 ... xS}beanorderedcollectionofSstatesdrawnindependentlyfromXaccordingtothedistributionπµ∗ ν.LetusconsidersolvingthefollowingprogramwhichwecallthesampledSALP:(7)maximizer sν>Φr−2(1−α)SPx∈ˆXs(x)subjectto(Φr)(x)≤(TΦr)(x)+s(x) ∀x∈ˆX r∈N s≥0.HereN∈RmisaparametersetchosentocontaintheoptimalsolutiontotheSALP(6) rSALP.Noticethat(7)isalinearprogramwithSvariablesandS|A|constraints.ForamoderatenumberofsamplesS thisisiseasilysolved.WewillprovideasamplecomplexityboundthatindicatesthatforanumberofsamplesSthatscaleslinearlywiththedimensionofΦ K andthatneednotdependonthesizeofthestatespace thesolutiontothesampledSALPsatisﬁes withhighprobability theapproximationguaranteepresentedfortheSALPsolutioninTheorem2.LetusdeﬁnetheconstantB supr∈Nk(Φr−TΦr)+k∞.ThisquantityiscloselyrelatedtothediameteroftheregionN.Wethenhave:Theorem3.UndertheconditionsofTheorem2 letrSALPbeanoptimalsolutiontotheSALP(6) andletˆrSALPbeanoptimalsolutiontothesampledSALP(7).AssumethatrSALP∈N.Further given∈(0 B]andδ∈(0 1/2] supposethatthenumberofsampledstatesSsatisﬁesS≥64B22(cid:18)2(K+2)log16eB+log8δ(cid:19).Then withprobabilityatleast1−δ−2−383δ128 kJ∗−ΦˆrSALPk1 ν≤infr∈Nψ∈ΨkJ∗−Φrk∞ 1/ψ ν>ψ+2(π>µ∗ νψ+1)(αβ(ψ)+1)1−α!+41−α.Theorem3establishesthatthesampledSALPprovidesacloseapproximationtothesolutionoftheSALP inthesensethattheapproximationguaranteesweestablishedfortheSALPareapproximatelyvalidforthesolutiontothesampledversionwithhighprobability.Thenumberofsampleswerequiretoaccomplishthistaskisspeciﬁedpreciselyviathetheorem.Thisnumberdependslinearlyonthenumberofbasisfunctionsandthediameterofthe1ThisincludesthoseψthatdonotsatisfytheLyapunovconditionβ(ψ)≤1/α.6feasibleregion butisotherwiseindependentofthesizeofthestatespacefortheMDPunderconsideration.ItisworthjuxtaposingoursamplecomplexityresultwiththatavailablefortheALP.Inparticular werecallthattheALPhasalargenumberofconstraintsbutasmallnumberofvariables;theSALPisthus atleastsuperﬁcially asigniﬁcantlymorecomplexprogram.ExploitingthefactthattheALPhasasmallnumberofvariables deFariasandVanRoy[2]establishasamplecomplexityboundforasampledversionoftheALPanalogous(7).ThenumberofsamplesrequiredforthissampledALPtoproduceagoodapproximationtotheALPcanbeshowntodependonthesameproblemparameterswehaveidentiﬁedhere viz.BandthenumberofbasisfunctionsK.ThesamplecomplexityinthatcaseisidenticaltothesamplecomplexityboundestablishedhereuptoconstantsandanadditionalmultiplicativefactorofB/(forthesampledSALP).Thus thetwosamplecomplexityboundsarewithinpolynomialtermsofeachotherandwehaveestablishedthattheSALPisessentiallynohardertosolvethantheALP.ThissectionplacestheSALPonsolidtheoreticalgroundbyestablishingstrongapproxima-tionguaranteesfortheSALPthatrepresentasubstantialimprovementoverthoseavailablefortheALPandsamplecomplexityresultsthatindicatedthattheSALPwasimplementableviasampling.WenextpresentacomputationalstudythatteststheSALPrelativetootherADPmethods(includingtheALP)onahardproblem(thegameofTetris).5CaseStudy:TetrisOurinterestinTetrisasacasestudyfortheSALPalgorithmismotivatedbyseveralfacts.TheoreticalresultssuggestthatdesignofanoptimalTetrisplayerisadiﬃcultproblem[4–6].TetrisrepresentspreciselythekindoflargeandunstructuredMDPforwhichitisdiﬃculttodesignheuristiccontrollers andhencepoliciesdesignedbyADPalgorithmsareparticularlyrelevant.Moreover Tetrishasbeenemployedbyanumberofresearchersasatestbedproblem[3 7–9].WefollowtheformulationofTetrisasaMDPpresentedbyFariasandVanRoy[3].TheSALPmethodologywasappliedasfollows:Basisfunctions.Weemployedthe22basisfunctionsoriginallyintroducedin[7].Statesampling.GivenasamplesizeS acollectionˆX⊂XofSstateswassampled.ThesesamplesweregeneratedinanIIDfashionfromthestationarydistributionofa(ratherpoor)baselinepolicy2.Optimization.GiventhecollectionˆXofsampledstates anincreasingsequenceofchoicesoftheviolationbudgetθ≥0isconsidered.Foreachchoiceofθ theoptimizationprogram(8)maximizer s1SPx∈ˆX(Φr)(x)subjecttoΦr(x)≤TΦr(x)+s(x) ∀x∈ˆX 1SPx∈ˆXs(x)≤θ s(x)≥0 ∀x∈ˆX wassolved.ThisprogramisaversionoftheoriginalSALP(3) butwithsampledempiricaldistributionsinplaceofthestate-relevanceweightsνandtheconstraintviolationdistribu-tionπ.Notethat(8)hasK+SdecisionvariablesandS|A|linearconstraints.Becauseofthesparsitystructureoftheconstraints however itisamenabletoeﬃcientsolutionviabarriermethods evenforlargevaluesofS.Evaluation.Givenavectorofweightsobtainedbysolving(8) theperformanceofthecorrespondingpolicyisevaluatedviaMonteCarlosimulationover3 000gamesofTetris.Performanceismeasuredintermsoftheaveragenumberoflinesclearedinasinglegame.Foreachpair(S θ) theresultingaverageperformance(averagedover10diﬀerentsetsofsampledstates)isshowninFigure2.ItprovidesexperimentalevidencefortheintuitionexpressedinSection3andtheanalyticresultofTheorem1:RelaxingtheconstraintsoftheALPbyallowingforaviolationbudgetallowsforbetterpolicyperformance.Astheviolationbudgetθisincreasedfrom0 performancedramaticallyimproves.Atθ=0.16384 theperformancepeaks andwegetpoliciesthatisanorderofmagnitudebetterthanALP andbeyondthattheperformancedeteriorates.2Ourbaselinepolicyhadanaverageperformanceof113points.750100150200250300×103024×103SampleSizeSAveragePerformanceθ=0.65536θ=0.16384θ=0.02048θ=0.01024θ=0.00256θ=0(ALP)Figure2:AverageperformanceofSALPfordiﬀerentvaluesofthenumberofsampledstatesSandtheviolationbudgetθ.Table1summarizestheperformanceofbestpoliciesobtainedbyvariousADPalgorithms.Notethatallofthesealgorithmsemploythesamebasisfunctionarchitecture.TheALPandSALPresultsarefromourexperiments whiletheotherresultsarefromtheliterature.ThebestperformanceresultsofSALPisbetterbyafactorof2incomparisontothecompetitors.AlgorithmBestPerformanceCPUTimeALP897hoursTD-Learning[7]3 183minutesALPwithbootstrapping[3]4 274hoursTD-Learning[8]4 471minutesPolicygradient[9]5 500daysSALP10 775hoursTable1:ComparisonoftheperformanceofthebestpolicyfoundwithvariousADPmethods.NotethatsigniﬁcantlybetterpoliciesarepossiblewiththisbasisfunctionarchitecturethananyoftheADPalgorithmsinTable1discover.Usingaheuristicoptimizationmethod SzitaandL˝orincz[10]reportpolicieswitharemarkableaverageperformanceof350 000.Theirmethodiscomputationallyintensive however requiringonemonthofCPUtime.Inaddition theapproachemploysanumberofratherarbitraryTetrisspeciﬁc‘modiﬁcations’thatareultimatelyseentobecriticaltoperformance-intheabsenceofthesemodiﬁcations themethodisunabletoﬁndapolicyforTetristhatscoresaboveafewhundredpoints.6FutureDirectionsThereareanumberofinterestingdirectionsthatremaintobeexplored.First notethattheboundsderivedinSections4.1and4.2areapproximationguarantees whichprovideboundsontheapproximationerrorgivenbytheSALPapproachversusthebestapproximationpos-siblewiththeparticularsetofbasisfunctions.Inpreliminarywork wehavealsodevelopedperformanceguarantees.TheseprovideboundsontheperformanceoftheresultingSALPpolicies asafunctionofthebasisarchitecture.Second notethatsamplepathvariationsoftheSALParepossible.Ratherthansolvingalargelinearprogram suchanalgorithmwouldoptimizeapolicyinanonlinefashionalongasinglesystemtrajectory.ThiswouldbeinamannerreminiscentofstochasticapproximationalgorithmslikeTD-learning.However asamplepathSALPvariationwouldinheritallofthetheoreticalboundsdevelopedhere.Thedesignandanalysisofsuchanalgorithmisanexcitingfuturedirection.8References[1]D.P.deFariasandB.VanRoy.Thelinearprogrammingapproachtoapproximatedynamicprogramming.OperationsResearch 51(6):850–865 2003.[2]D.P.deFariasandB.VanRoy.Onconstraintsamplinginthelinearprogrammingapproachtoapproximatedynamicprogramming.MathematicsofOperationsResearch 293(3):462–478 2004.[3]V.F.FariasandB.VanRoy.Tetris:Astudyofrandomizedconstraintsampling.InProbabilisticandRandomizedMethodsforDesignUnderUncertainty.Springer-Verlag 2006.[4]J.Brzustowski.CanyouwinatTetris?Master’sthesis UniversityofBritishColumbia 1992.[5]H.Burgiel.HowtoloseatTetris.MathematicalGazette page194 1997.[6]E.D.Demaine S.Hohenberger andD.Liben-Nowell.Tetrisishard eventoapprox-imate.InProceedingsofthe9thInternationalComputingandCombinatoricsConfer-ence 2003.[7]D.P.BertsekasandS.Ioﬀe.Temporaldiﬀerences–basedpolicyiterationandapplica-tionsinneuro–dynamicprogramming.TechnicalReportLIDS–P–2349 MITLabora-toryforInformationandDecisionSystems 1996.[8]D.P.BertsekasandJ.N.Tsitsiklis.Neuro-DynamicProgramming.AthenaScientiﬁc Belmont MA 1996.[9]S.Kakade.Anaturalpolicygradient.InAdvancesinNeuralInformationProcessingSystems14 Cambridge MA 2002.MITPress.[10]I.SzitaandA.L˝orincz.LearningTetrisusingthenoisycross-entropymethod.NeuralComputation 18:2936–2941 2006.[11]D.Haussler.DecisiontheoreticgeneralizationsofthePACmodelforneuralnetandotherlearningapplications.InformationandComputation 100:78–150 1992.9,TIAN TIAN
Jun Zhu