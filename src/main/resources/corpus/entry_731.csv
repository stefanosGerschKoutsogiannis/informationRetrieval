2013,Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search,Monte-Carlo tree search is drawing great interest in the domain of planning under uncertainty  particularly when little or no domain knowledge is available. One of the central problems is the trade-off between exploration and exploitation. In this paper we present a novel Bayesian mixture modelling and inference based Thompson sampling approach to addressing this dilemma. The proposed Dirichlet-NormalGamma MCTS (DNG-MCTS) algorithm represents the uncertainty of the accumulated reward for actions in the MCTS search tree as a mixture of Normal distributions and inferences on it in Bayesian settings by choosing conjugate priors in the form of combinations of Dirichlet and NormalGamma distributions. Thompson sampling is used to select the best action at each decision node. Experimental results show that our proposed algorithm has achieved the state-of-the-art comparing with popular UCT algorithm in the context of online planning for general Markov decision processes.,Bayesian Mixture Modeling and Inference based
Thompson Sampling in Monte-Carlo Tree Search

Aijun Bai

Univ. of Sci. & Tech. of China

baj@mail.ustc.edu.cn

Feng Wu

University of Southampton
fw6e11@ecs.soton.ac.uk

Abstract

Xiaoping Chen

Univ. of Sci. & Tech. of China

xpchen@ustc.edu.cn

Monte-Carlo tree search (MCTS) has been drawing great interest in recent years
for planning and learning under uncertainty. One of the key challenges is the
trade-off between exploration and exploitation. To address this  we present a
novel approach for MCTS using Bayesian mixture modeling and inference based
Thompson sampling and apply it to the problem of online planning in MDPs.
Our algorithm  named Dirichlet-NormalGamma MCTS (DNG-MCTS)  models
the uncertainty of the accumulated reward for actions in the search tree as a mix-
ture of Normal distributions. We perform inferences on the mixture in Bayesian
settings by choosing conjugate priors in the form of combinations of Dirichlet
and NormalGamma distributions and select the best action at each decision node
using Thompson sampling. Experimental results conﬁrm that our algorithm ad-
vances the state-of-the-art UCT approach with better values on several benchmark
problems.

1

Introduction

Markov decision processes (MDPs) provide a general framework for planning and learning under
uncertainty. We consider the problem of online planning in MDPs without prior knowledge on the
underlying transition probabilities. Monte-Carlo tree search (MCTS) can ﬁnd near-optimal policies
in our domains by combining tree search methods with sampling techniques. The key idea is to iter-
atively evaluate each state in a best-ﬁrst search tree by the mean outcome of simulation samples. It is
model-free and requires only a black-box simulator (generative model) of the underlying problems.
To date  great success has been achieved by MCTS in variety of domains  such as game play [1  2] 
planning under uncertainty [3  4  5]  and Bayesian reinforcement learning [6  7].
When applying MCTS  one of the fundamental challenges is the so-called exploration versus ex-
ploitation dilemma: an agent must not only exploit by selecting the best action based on the current
information  but should also keep exploring other actions for possible higher future payoffs. Thomp-
son sampling is one of the earliest heuristics to address this dilemma in multi-armed bandit problems
(MABs) according to the principle of randomized probability matching [8]. The basic idea is to s-
elect actions stochastically  based on the probabilities of being optimal. It has recently been shown
to perform very well in MABs both empirically [9] and theoretically [10]. It has been proved that
Thompson sampling algorithm achieves logarithmic expected regret which is asymptotically opti-
mal for MABs. Comparing to the UCB1 heuristic [3]  the main advantage of Thompson sampling
is that it allows more robust convergence under a wide range of problem settings.
In this paper  we borrow the idea of Thompson sampling and propose the Dirichlet-NormalGamma
MCTS (DNG-MCTS) algorithm — a novel Bayesian mixture modeling and inference based Thomp-
son sampling approach for online planning in MDPs. In this algorithm  we use a mixture of Normal
distributions to model the unknown distribution of the accumulated reward of performing a partic-
ular action in the MCTS search tree. In the present of online planning for MDPs  a conjugate prior

1

exists in the form of a combination of Dirichlet and NormalGamma distributions. By choosing the
conjugate prior  it is then relatively simple to compute the posterior distribution after each accumu-
lated reward is observed by simulation in the search tree. Thompson sampling is then used to select
the action to be performed by simulation at each decision node. We have tested our DNG-MCTS
algorithm and compared it with the popular UCT algorithm in several benchmark problems. Ex-
perimental results show that our proposed algorithm has outperformed the state-of-the-art for online
planning in general MDPs. Furthermore  we show the convergence of our algorithm  conﬁrming its
technical soundness.
The reminder of this paper is organized as follows. In Section 2  we brieﬂy introduce the neces-
sary background. Section 3 presents our main results — the DNG-MCTS algorithm. We show
experimental results on several benchmark problems in Section 4. Finally in Section 5 the paper is
concluded with a summary of our contributions and future work.

2 Background

In this section  we brieﬂy review the MDP model  the MAB problem  the MCTS framework  and
the UCT algorithm as the basis of our algorithm. Some related work is also presented.

optimal policy π that maximizes the expected reward deﬁned as Vπ(s) = E[(cid:80)H

2.1 MDPs and MABs
Formally  an MDP is deﬁned as a tuple (cid:104)S  A  T  R(cid:105)  where S is the state space  A is the action
space  T (s(cid:48)|s  a) is the probability of reaching state s(cid:48) if action a is applied in state s  and R(s  a)
is the reward received by the agent. A policy is a decision rule mapping from states to actions and
specifying which action should be taken in each state. The aim of solving an MDP is to ﬁnd the
t=0 γtR(st  π(st))] 
where H is the planing horizon  γ ∈ (0  1] is the discount factor  st is the state in time step t and
π(st) is the action selected by policy π in state st.
Intuitively  an MAB can be seen as an MDP with only one state s and a stochastic reward function
R(s  a) := Xa  where Xa is a random variable following an unknown distribution fXa (x). At each
time step t  one action at must be chosen and executed. A stochastic reward Xat is then received
accordingly. The goal is to ﬁnd a sequence of actions that minimizes the cumulative regret deﬁned

t=1(Xa∗ − Xat)]  where a∗ is the true best action.

as RT = E[(cid:80)T

2.2 MCTS and UCT

To solve MDPs  MCTS iteratively evaluates a state by: (1) selecting an action based on a given action
selection strategy; (2) performing the selected action by Monte-Carlo simulation; (3) recursively
evaluating the resulted state if it is already in the search tree  or inserting it into the search tree and
running a rollout policy by simulations. This process is applied to descend through the search tree
until some terminate conditions are reached. The simulation result is then back-propagated through
the selected nodes to update their statistics.
The UCT algorithm is a popular approach based on MCTS for planning under uncertainty [3]. It
treats each state of the search tree as an MAB  and selects the action that maximizes the UCB1

heuristic ¯Q(s  a) + c(cid:112)log N (s)/N (s  a)  where ¯Q(s  a) is the mean return of action a in state s
overall count N (s) =(cid:80)

from all previous simulations  N (s  a) is the visitation count of action a in state s  N (s) is the
a∈A N (s  a)  and c is the exploration constant that determines the relative
ratio of exploration to exploitation. It is proved that with an appropriate choice of c the probability
of selecting the optimal action converges to 1 as the number of samples grows to inﬁnity.

2.3 Related Work

The fundamental assumption of our algorithm is modeling unknown distribution of the accumulated
reward for each state-action pair in the search tree as a mixture of Normal distributions. A similar
assumption has been made in [11]  where they assumed a Normal distribution over the rewards.
Comparing to their approach  as we will show in Section 3  our assumption on Normal mixture is
more realistic for our problems. Tesauro et al.[12] developed a Bayesian UCT approach to MCTS

2

using Gaussian approximation. Speciﬁcally  their method propagates probability distributions of
rewards from leaf nodes up to the root node by applying MAX (or MIN) extremum distribution
operator for the interior nodes. Then  it uses modiﬁed UCB1 heuristics to select actions on the basis
of the interior distributions. However  extremum distribution operation on decision nodes is very
time-consuming because it must consider over all the child nodes. In contrast  we treat each decision
node in the search tree as an MAB  maintain a posterior distribution over the accumulated reward
for each applicable actions separately  and then select the best action using Thompson sampling.

3 The DNG-MCTS Algorithm

This section presents our main results — a Bayesian mixture modeling and inference based Thomp-
son sampling approach for MCTS (DNG-MCTS).

3.1 The Assumptions

For a given MDP policy π  let Xs π be a random variable that denotes the accumulated reward
of following policy π starting from state s  and Xs a π denotes the accumulated reward of ﬁrst
performing action a in state s and then following policy π thereafter. Our assumptions are: (1)
Xs π is sampled from a Normal distribution  and (2) Xs a π can be modeled as a mixture of Normal
distributions. These are realistic approximations for our problems with the following reasons.
Given policy π  an MDP reduces to a Markov chain {st} with ﬁnite state space S and the transition
function T (s(cid:48)|s  π(s)). Suppose that the resulting chain {st} is ergodic. That is  it is possible to
go from every state to every other state (not necessarily in one move). Let w denote the stationary
distribution of {st}. According to the central limit theorem on Markov chains [13  14]  for any
bounded function f on the state space S  we have:

f (st) − nµ) → N (0  σ2) as n → ∞ 

(1)

t=0 f (st) as a Normal distribution if n is sufﬁciently large.

where µ = Ew[f ] and σ is a constant depending only on f and w. This indicates that the sum of
f (st) follows N (nµ  nσ2) as n grows to inﬁnity. It is then natural to approximate the distribution

of(cid:80)n
Considering ﬁnite-horizon MDPs with horizon H  if γ = 1  Xs0 π =(cid:80)H
sufﬁciently large. On the other hand  if γ (cid:54)= 1  Xs0 π =(cid:80)H
linear combination of(cid:80)n

t=0 R(st  π(st)) is a sum
of f (st) = R(st  π(st)). Thus  Xs0 π is approximately normally distributed for each s0 ∈ S if H is
t=0 γtR(st  π(st)) can be rewritten as a

n(cid:88)

t=0

1√
n

(

t=0 f (st) for n = 0 to H as follow:
Xs0 π = (1 − γ)

γn

f (st) + γH

H−1(cid:88)

n(cid:88)

H(cid:88)

f (st)

(2)

n=0

t=0

t=0

Notice that a linear combination of independent or correlated normally distributed random variables
is still normally distributed. If H is sufﬁciently large and γ is close to 1  it is reasonable to approxi-
mate Xs0 π as a Normal distribution. Therefore  we assume that Xs π is normally distributed in both
cases.
If the policy π is not ﬁxed and may change over time (e.g.  the derived policy of an online algorithm
before it converges)  the real distribution of Xs π is actually unknown and could be very complex.
However  if the algorithm is guaranteed to converge in the limit (as explained in Section 3.5  this
holds for our DNG-MCTS algorithm)  it is convenient and reasonable to approximate Xs π as a
Normal distribution.
Now consider the accumulated reward of ﬁrst performing action a in s and following policy π
thereafter. By deﬁnition  Xs a π = R(s  a) + γXs(cid:48) π  where s(cid:48) is the next state distributed according
to T (s(cid:48)|s  a). Let Ys a π be a random variable deﬁned as Ys a π = (Xs a π − R(s  a))/γ. We can see
that the pdf of Ys a π is a convex combination of the pdfs of Xs(cid:48) π for each s(cid:48) ∈ S. Speciﬁcally  we
s(cid:48)∈S T (s(cid:48)|s  a)fXs(cid:48)  π (y). Hence it is straightforward to model the distribution
of Ys a π as a mixture of Normal distributions if Xs(cid:48) π is assumed to be normally distributed for each
s(cid:48) ∈ S. Since Xs a π is a linear function of Ys a π  Xs a π is also a mixture of Normal distributions
under our assumptions.

have fYs a π (y) =(cid:80)

3

3.2 The Modeling and Inference Methods

In Bayesian settings  the unknown distribution of a random variable X can be modeled as a para-
metric likelihood function L(x|θ) depending on the parameters θ. Given a prior distribution P (θ) 
and a set of past observations Z = {x1  x2  . . .}  the posterior distribution of θ can then be obtained

using Bayes’ rules: P (θ|Z) ∝(cid:81)

i L(xi|θ)P (θ).

Assumption (1) implies that it sufﬁces to model the distribution of Xs π as a Normal likelihood
N (µs  1/τs) with unknown mean µs and precision τs. The precision is deﬁned as the recipro-
cal of the variance  τ = 1/σ2. This is chosen for mathematical convenience of introducing the
NomralGamma distribution as a conjugate prior. A NormalGamma distribution is deﬁned by the
hyper-parameters (cid:104)µ0  λ  α  β(cid:105) with λ > 0  α ≥ 1 and β ≥ 0.
It is said that (µ  τ ) follows a
NormalGamma distribution N ormalGamma(µ0  λ  α  β) if the pdf of (µ  τ ) has the form

√
√
λ
2π

βα
Γ(α)

f (µ  τ|µ0  λ  α  β) =

τ α− 1

2 e−βτ e− λτ (µ−µ0)2

2

.

(3)

i=1(xi − ¯x)2/n is the sample variance.

By deﬁnition  the marginal distribution over τ is a Gamma distribution  τ ∼ Gamma(α  β)  and
the conditional distribution over µ given τ is a Normal distribution  µ ∼ N (µ0  1/(λτ )).
Let us brieﬂy recall the posterior of (µ  τ ). Suppose X is normally distributed with unknown mean
µ and precision τ  x ∼ N (µ  1/τ )  and that the prior distribution of (µ  τ ) has a NormalGamma
distribution  (µ  τ ) ∼ N ormalGamma(µ0  λ0  α0  β0). After observing n independent samples of
X  denoted {x1  x2  . . .   xn}  according to the Bayes’ theorem  the posterior distribution of (µ  τ )
is also a NormalGamma distribution  (µ  τ ) ∼ N ormalGamma(µn  λn  αn  βn)  where µn =
(λ0µ0+n¯x)/(λ0+n)  λn = λ0+n  αn = α0+n/2 and βn = β0+(ns+λ0n(¯x−µ0)2/(λ0+n))/2 

where ¯x =(cid:80)n
i=1 xi/n is the sample mean and s =(cid:80)n
tions Ys a π = (Xs a π − R(s  a))/γ ∼ (cid:80)
are the mixture weights such that ws a s(cid:48) ≥ 0 and (cid:80)

Based on Assumption 2  the distribution of Ys a π can be modeled as a mixture of Normal distribu-
s(cid:48)∈S ws a s(cid:48)N (µs(cid:48)  1/τs(cid:48))  where ws a s(cid:48) = T (s(cid:48)|s  a)
s(cid:48)∈S ws a s(cid:48) = 1  which are previous-
ly unknown in Monte-Carlo settings. A natural representation on these unknown weights is via
Dirichlet distributions  since Dirichlet distribution is the conjugate prior of a general discrete prob-
ability distribution. For state s and action a  a Dirichlet distribution  denoted Dir(ρs a) where
ρs a = (ρs a s1  ρs a s2 ··· )  gives the posterior distribution of T (s(cid:48)|s  a) for each s(cid:48) ∈ S if the
transition to s(cid:48) has been observed ρs a s(cid:48) − 1 times. After observing a transition (s  a) → s(cid:48)  the
posterior distribution is also Dirichlet and can simply be updated as ρs a s(cid:48) ← ρs a s(cid:48) + 1.
Therefore  to model the distribution of Xs π and Xs a π we only need to maintain a set of hyper-
parameters (cid:104)µs 0  λs  αs  βs(cid:105) and ρs a for each state s and action a encountered in the MCTS search
tree and update them by using Bayes’ rules.
Now we turn to the question of how to choose the priors by initializing hyper-parameters. While the
impact of the prior tends to be negligible in the limit  its choice is important especially when only
a small amount of data has been observed. In general  priors should reﬂect available knowledge of
the hidden model.
In the absence of any knowledge  uninformative priors may be preferred. According to the principle
of indifference  uninformative priors assign equal probabilities to all possibilities. For NormalGam-
ma priors  we hope that the sampled distribution of µ given τ  i.e.  N (µ0  1/(λτ ))  is as ﬂat as
possible. This implies an inﬁnite variance 1/(λτ ) → ∞  so that λτ → 0. Recall that τ follows
a Gamma distribution Gamma(α  β) with expectation E[τ ] = α/β  so we have in expectation
λα/β → 0. Considering the parameter space (λ > 0  α ≥ 1  β ≥ 0)  we can choose λ small
enough  α = 1 and β sufﬁciently large to approximate this condition. Second  we hope the sampled
distribution is in the middle of axis  so µ0 = 0 seems to be a good selection. It is worth noting that
intuitively β should not be set too large  or the convergence process may be very slow. For Dirichlet
priors  it is common to set ρs a s(cid:48) = δ where δ is a small enough positive for each s ∈ S  a ∈ A and
s(cid:48) ∈ S encountered in the search tree to have uninformative priors.
On the other hand  if some prior knowledge is available  informative priors may be preferred. By
exploiting domain knowledge  a state node can be initialized with informative priors indicating it-
s priority over other states.
In DNG-MCTS  this is done by setting the hyper-parameters based

4

on subjective estimation for states. According to the interpretation of hyper-parameters of Nor-
malGamma distribution in terms of pseudo-observations  if one has a prior mean of µ0 from λ
samples and a prior precision of α/β from 2α samples  the prior distribution over µ and τ is
N ormalGamma(µ0  λ  α  β)  providing a straightforward way to initialize the hyper-parameters
if some prior knowledge (such as historical data of past observations) is available. Specifying de-
tailed priors based on prior knowledge for particular domains is beyond the scope of this paper. The
ability to include prior information provides important ﬂexibility and can be considered an advan-
tage of the approach.

3.3 The Action Selection Strategy

In DNG-MCTS  action selection strategy is derived using Thompson sampling. Speciﬁcally  in
general Bayesian settings  action a is chosen with probability:

(cid:90)

(cid:20)

(cid:21)(cid:89)

E [Xa(cid:48)|θa(cid:48)]

Pa(cid:48)(θa(cid:48)|Z) dθ

(4)

P (a) =

1

a = argmax

a(cid:48)

tion of reward by applying a  E[Xa|θa] = (cid:82) xLa(x|θa) dx is the expectation of Xa given θa  and

where 1 is the indicator function  θa is the hidden parameter prescribing the underlying distribu-

a(cid:48)

θ = (θa1  θa2   . . . ) is the vector of parameters for all actions. Fortunately  this can efﬁciently be
approached by sampling method. To this end  a set of parameters θa is sampled according to the pos-
terior distributions Pa(θa|Z) for each a ∈ A  and the action a∗ = argmaxa
E[Xa|θa] with highest
expectation is selected.
In our implementation  at each decision node s of the search tree  we sample the mean µs(cid:48) and
mixture weights ws a s(cid:48) according to N ormalGamma(µs(cid:48) 0  λs(cid:48)  αs(cid:48)  βs(cid:48)) and Dir(ρs a) respec-
tively for each possible next state s(cid:48) ∈ S. The expectation of Xs a π is then computed as
s(cid:48)∈S ws a s(cid:48)µs(cid:48). The action with highest expectation is then selected to be performed

R(s  a) + γ(cid:80)

in simulation.

3.4 The Main Algorithm

The main process of DNG-MCTS is outlined in Figure 1.
It is worth noting that the function
ThompsonSampling has a boolean parameter sampling. If sampling is true  Thompson sam-
pling method is used to select the best action as explained in Section 3.3  otherwise a greedy action
is returned with respect to the current expected transition probabilities and accumulated rewards of

next states  which are E[ws a s(cid:48)] = ρs a s(cid:48)/(cid:80)

x∈S ρs a x and E[Xs π] = µs 0 respectively.

At each iteration  the function DNG-MCTS uses Thompson sampling to recursively select actions
to be executed by simulation from the root node to leaf nodes through the existing search tree T .
It inserts each newly visited node into the tree  plays a default rollout policy from the new node 
and propagates the simulated outcome to update the hyper-parameters for visited states and actions.
Noting that the rollout policy is only played once for each new node at each iteration  the set of past
observations Z in the algorithm has size n = 1.
The function OnlinePlanning is the overall procedure interacting with the real environment. It is
called with current state s  search tree T initially empty and the maximal horizon H. It repeatedly
calls the function DNG-MCTS until some resource budgets are reached (e.g.  the computation is
timeout or the maximal number of iterations is reached)  by when a greedy action to be performed
in the environment is returned to the agent.

3.5 The Convergency Property

For Thompson sampling in stationary MABs (i.e.  the underlying reward function will not change)  it
is proved that: (1) the probability of selecting any suboptimal action a at the current step is bounded
by a linear function of the probability of selecting the optimal action; (2) the coefﬁcient in this linear
function decreases exponentially fast with the increase in the number of selection of optimal action
[15]. Thus  the probability of selecting the optimal action in an MAB is guaranteed to converge to 1
in the limit using Thompson sampling.

5

OnlinePlanning(s : state  T : tree 
H : max horizon)
Initialize (µs 0  λs  αs  βs) for each s ∈ S
Initialize ρs a for each s ∈ S and a ∈ A
repeat

DNG-MCTS(s  T  H)

until resource budgets reached
return ThompsonSampling(s  H  F alse)

else

DNG-MCTS(s : state  T : tree  h : horizon)
if h = 0 or s is terminal then
else if node (cid:104)s  h(cid:105) is not in tree T then

return 0
Add node (cid:104)s  h(cid:105) to T
Play rollout policy by simulation for h steps
Observe the outcome r
return r
a ← ThompsonSampling(s  h  T rue)
Execute a by simulation
Observe next state s(cid:48) and reward R(s  a)
r ← R(s  a) + γ DNG-MCTS(s(cid:48)  T  h − 1)
αs ← αs + 0.5
βs ← βs + (λs(r − µs 0)2/(λs + 1))/2
µs 0 ← (λsµs 0 + r)/(λs + 1)
λs ← λs + 1
ρs a s(cid:48) ← ρs a s(cid:48) + 1
return r

ThompsonSampling(s : state  h : horizon 
sampling : boolean)
foreach a ∈ A do

qa ← QValue(s  a  h  sampling)

return argmaxa qa

QValue(s : state  a : action  h : horizon 
sampling : boolean)
r ← 0
foreach s(cid:48) ∈ S do

Sample ws(cid:48) according to Dir(ρs a)

if sampling = T rue then

else

ws(cid:48) ← ρs a s(cid:48) /(cid:80)

r ← r + ws(cid:48)Value(s(cid:48)  h − 1  sampling)

n∈S ρs a n

return R(s  a) + γr

Value(s : state  h : horizon 
sampling : boolean)
if h = 0 or s is terminal then

return 0

else

if sampling = T rue then

Sample (µ  τ ) according to
N ormalGamma(µs 0  λs  αs  βs)
return µ

else

return µs 0

Figure 1: Dirichlet-NormalGamma based Monte-Carlo Tree Search

The distribution of Xs π is determined by the transition function and the Q values given the policy π.
When the Q values converge  the distribution of Xs π becomes stationary with the optimal policy.
For the leaf nodes (level H) of the search tree  Thompson sampling will converge to the optimal
actions with probability 1 in the limit since the MABs are stationary. When all the leaf nodes
converge  the distributions of return values from them will not change. So the MABs of the nodes in
level H − 1 become stationary as well. Thus  Thompson sampling will also converge to the optimal
actions for nodes in level H − 1. Recursively  this holds for all the upper-level nodes. Therefore  we
conclude that DNG-MCTS can ﬁnd the optimal policy for the root node if unbounded computational
resources are given.

4 Experiments

We have tested our DNG-MCTS algorithm and compared the results with UCT in three common
MDP benchmark domains  namely Canadian traveler problem  racetrack and sailing. These prob-
lems are modeled as cost-based MDPs. That is  a cost function c(s  a) is used instead of the reward
function R(s  a)  and the min operator is used in the Bellman equation instead of the max operator.
Similarly  the objective of solving a cost-based MDPs is to ﬁnd an optimal policy that minimizes
the expected accumulated cost for each state. Notice that algorithms developed for reward-based
MDPs can be straightforwardly transformed and applied to cost-based MDPs by simply using the
min operator instead of max in the Bellman update routines. Accordingly  the min operator is used
in the function ThompsonSampling of our transformed DNG-MCTS algorithm. We implemented
our codes and conducted the experiments on the basis of MDP-engine  which is an open source
software package with a collection of problem instances and base algorithms for MDPs.1

1MDP-engine can be publicly accessed via https://code.google.com/p/mdp-engine/

6

Table 1: CTP problems with 20 nodes. The second column indicates the belief size of the trans-
formed MDP for each problem instance. UCTB and UCTO are the two domain-speciﬁc UCT im-
plementations [18]. DNG-MCTS and UCT run for 10 000 iterations. Boldface fonts are best in
whole table; gray cells show best among domain-independent implementations for each group. The
data of UCTB  UCTO and UCT are taken form [16].

prob.
20-1
20-2
20-3
20-4
20-5
20-6
20-7
20-8
20-9
20-10
total

belief
20 × 349
20 × 349
20 × 351
20 × 349
20 × 352
20 × 349
20 × 350
20 × 351
20 × 350
20 × 349

domain-speciﬁc UCT
UCTB
210.7±7
176.4±4
150.7±7
264.8±9
123.2±7
165.4±6
191.6±6
160.1±7
235.2±6
180.8±7
1858.9

UCTO
169.0±6
148.9±3
132.5±6
235.2±7
111.3±5
133.1±3
148.2±4
134.5±5
173.9±4
167.0±5
1553.6

random rollout policy

optimistic rollout policy

UCT
216.4±3
178.5±2
169.7±4
264.1±4
139.8±4
178.0±3
211.8±3
218.5±4
251.9±3
185.7±3
2014.4

DNG
223.9±4
178.1±2
159.5±4
266.8±4
133.4±4
169.8±3
214.9±4
202.3±4
246.0±3
188.9±4
1983.68

UCT
180.7±3
160.8±2
144.3±3
238.3±3
123.9±3
167.8±2
174.1±2
152.3±3
185.2±2
178.5±3
1705.9

DNG
177.1±3
155.2±2
140.1±3
242.7±4
122.1±3
141.9±2
166.1±3
151.4±3
180.4±2
170.5±3
1647.4

In each benchmark problem  we (1) ran the transformed algorithms for a number of iterations from
the current state  (2) applied the best action based on the resulted action-values  (3) repeated the loop
until terminating conditions (e.g.  a goal state is satisﬁed or the maximal number of running steps
is reached)  and (4) reported the total discounted cost. The performance of algorithms is evaluated
by the average value of total discounted costs over 1 000 independent runs.
In all experiments 
(µs 0  λs  αs  βs) is initialized to (0  0.01  1  100)  and ρs a s(cid:48) is initialized to 0.01 for all s ∈ S 
a ∈ A and s(cid:48) ∈ S. For fair comparison  we also use the same settings as in [16]: for each decision
node  (1) only applicable actions are selected  (2) applicable actions are forced to be selected once
before any of them are selected twice or more  and 3) the exploration constant for the UCT algorithm
is set to be the current mean action-values Q(s  a  d).
The Canadian traveler problem (CTP) is a path ﬁnding problem with imperfect information over a
graph whose edges may be blocked with given prior probabilities [17]. A CTP can be modeled as
a deterministic POMDP  i.e.  the only source of uncertainty is the initial belief. When transformed
to an MDP  the size of the belief space is n × 3m  where n is the number of nodes and m is the
number of edges. This problem has a discount factor γ = 1. The aim is to navigate to the goal state
as quickly as possible. It has recently been addressed by an anytime variation of AO*  named AOT
[16]  and two domain-speciﬁc implementations of UCT which take advantage of the speciﬁc MDP
structure of the CTP and use a more informed base policy  named UCTB and UCTO [18]. In this
experiment  we used the same 10 problem instances with 20 nodes as done in their papers.
When running DNG-MCTS and UCT in those CTP instances  the number of iterations for each
decision-making was set to be 10 000  which is identical to [16]. Two types of default rollout policy
were tested: the random policy that selects actions with equal probabilities and the optimistic policy
that assumes traversability for unknown edges and selects actions according to estimated cost. The
results are shown in Table 1. Similar to [16]  we included the results of UCTB and UCTO as
a reference. From the table  we can see that DNG-MCTS outperformed the domain-independent
version of UCT with random rollout policy in several instances  and particularly performed much
better than UCT with optimistic rollout policy. Although DNG-MCTS is not as good as domain-
speciﬁc UCTO  it is competitive comparing to the general UCT algorithm in this domain.
The racetrack problem simulates a car race [19]  where a car starts in a set of initial states and moves
towards the goal. At each time step  the car can choose to accelerate to one of the eight directions.
When moving  the car has a possibility of 0.9 to succeed and 0.1 to fail on its acceleration. We
tested DNG-MCTS and UCT with random rollout policy and horizon H = 100 in the instance of
barto-big  which has a state space with size |S| = 22534. The discount factor is γ = 0.95 and the
optimal cost produced is known to be 21.38. We reported the curve of the average cost as a function
of the number of iterations in Figure 2a. Each data point in the ﬁgure was averaged over 1 000

7

(a) Racetrack-barto-big with random policy

(b) Sailing-100 × 100 with random policy

Figure 2: Performance curves for Racetrack and Sailing

runs  each of which was allowed for running at most 100 steps. It can be seen from the ﬁgure that
DNG-MCTS converged faster than UCT in terms of sample complexity in this domain.
The sailing domain is adopted from [3]. In this domain  a sailboat navigates to a destination on
an 8-connected grid. The direction of the wind changes over time according to prior transition
probabilities. The goal is to reach the destination as quickly as possible  by choosing at each grid
location a neighbour location to move to. The discount factor in this domain is γ = 0.95 and the
maximum horizon is set to be H = 100. We ran DNG-MCTS and UCT with random rollout policy
in a 100× 100 instance of this domain. This instance has 80000 states and the optimal cost is 26.08.
The performance curve is shown in Figure 2b. A trend similar to the racetrack problem can be
observed in the graph: DNG-MCTS converged faster than UCT in terms of sample complexity.
Regarding computational complexity  although the total computation time of our algorithm is linear
with the total sample size  which is at most width × depth (width is the number of iterations
and depth is the maximal horizon)  our approach does require more computation than simple UCT
methods. Speciﬁcally  we observed that most of the computation time of DNG-MCTS is due to the
sampling from distributions in Thompson sampling. Thus  DNG-MCTS usually consumes more
time than UCT in a single iteration. Based on our experimental results on the benchmark problems 
DNG-MCTS typically needs about 2 to 4 times (depending on problems and the iterating stage of
the algorithms) of computational time more than UCT algorithm for a single iteration. However 
if the simulations are expensive (e.g.  computational physics in 3D environment where the cost of
executing the simulation steps greatly exceeds the time needed by action-selection steps in MCTS) 
DNG-MCTS can obtain much better performance than UCT in terms of computational complexity
because DNG-MCTS is expected to have lower sample complexity.

5 Conclusion

In this paper  we proposed our DNG-MCTS algorithm — a novel Bayesian modeling and inference
based Thompson sampling approach using MCTS for MDP online planning. The basic assumption
of DNG-MCTS is modeling the uncertainty of the accumulated reward for each state-action pair as
a mixture of Normal distributions. We presented the overall Bayesian framework for representing 
updating  decision-making and propagating of probability distributions over rewards in the MCTS
search tree. Our experimental results conﬁrmed that  comparing to the general UCT algorithm 
DNG-MCTS produced competitive results in the CTP domain  and converged faster in the domains
of racetrack and sailing with respect to sample complexity. In the future  we plan to extend our basic
assumption to using more complex distributions and test our algorithm on real-world applications.

8

2030405060708090100110100100010000100000avg.accumulatedcostnumberofiterationsUCTDNG-MCTS2530354045505560110100100010000100000avg.accumulatedcostnumberofiterationsUCTDNG-MCTSAcknowledgements

This work is supported in part by the National Hi-Tech Project of China under grant 2008AA01Z150
and the Natural Science Foundation of China under grant 60745002 and 61175057. Feng Wu is
supported in part by the ORCHID project (http://www.orchid.ac.uk). We are grateful to
the anonymous reviewers for their constructive comments and suggestions.

References
[1] S. Gelly and D. Silver. Monte-carlo tree search and rapid action value estimation in computer

go. Artiﬁcial Intelligence  175(11):1856–1875  2011.

[2] Mark HM Winands  Yngvi Bjornsson  and J Saito. Monte carlo tree search in lines of action.

IEEE Transactions on Computational Intelligence and AI in Games  2(4):239–250  2010.

[3] L. Kocsis and C. Szepesv´ari. Bandit based monte-carlo planning. In European Conference on

Machine Learning  pages 282–293  2006.

[4] D. Silver and J. Veness. Monte-carlo planning in large pomdps. In Advances in Neural Infor-

mation Processing Systems  pages 2164–2172  2010.

[5] Feng Wu  Shlomo Zilberstein  and Xiaoping Chen. Online planning for ad hoc autonomous
agent teams. In International Joint Conference on Artiﬁcial Intelligence  pages 439–445  2011.
[6] Arthur Guez  David Silver  and Peter Dayan. Efﬁcient bayes-adaptive reinforcement learning
In Advances in Neural Information Processing Systems  pages

using sample-based search.
1034–1042  2012.

[7] John Asmuth and Michael L. Littman. Learning is planning: near bayes-optimal reinforcement
learning via monte-carlo tree search. In Uncertainty in Artiﬁcial Intelligence  pages 19–26 
2011.

[8] William R. Thompson. On the likelihood that one unknown probability exceeds another in

view of the evidence of two samples. Biometrika  25:285–294  1933.

[9] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances

Neural Information Processing Systems  pages 2249–2257  2011.

[10] Emilie Kaufmann  Nathaniel Korda  and R´emi Munos. Thompson sampling: An optimal ﬁnite

time analysis. In Algorithmic Learning Theory  pages 199–213  2012.

[11] Richard Dearden  Nir Friedman  and Stuart Russell. Bayesian q-learning. In AAAI Conference

on Artiﬁcial Intelligence  pages 761–768  1998.

[12] Gerald Tesauro  V. T. Rajan  and Richard Segal. Bayesian inference in monte-carlo tree search.

In Uncertainty in Artiﬁcial Intelligence  pages 580–588  2010.

[13] Galin L Jones. On the markov chain central limit theorem. Probability surveys  1:299–320 

2004.

[14] Anirban DasGupta. Asymptotic theory of statistics and probability. Springer  2008.
[15] Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In

Artiﬁcial Intelligence and Statistics  pages 99–107  2013.

[16] Blai Bonet and Hector Geffner. Action selection for mdps: Anytime ao* vs. uct.

Conference on Artiﬁcial Intelligence  pages 1749–1755  2012.

In AAAI

[17] Christos H Papadimitriou and Mihalis Yannakakis. Shortest paths without a map. Theoretical

Computer Science  84(1):127–150  1991.

[18] Patrick Eyerich  Thomas Keller  and Malte Helmert. High-quality policies for the canadian

traveler’s problem. In AAAI Conference on Artiﬁcial Intelligence  pages 51–58  2010.

[19] A.G. Barto  S.J. Bradtke  and S.P. Singh. Learning to act using real-time dynamic program-

ming. Artiﬁcial Intelligence  72(1-2):81–138  1995.

9

,Aijun Bai
Feng Wu
Xiaoping Chen
Hsiao-Yu Tung
Alexander Smola
Jaya Kawale
Hung Bui
Branislav Kveton
Long Tran-Thanh
Sanjay Chawla
Stephan Rabanser
Stephan Günnemann
Zachary Lipton