2017,Stabilizing Training of Generative Adversarial Networks through Regularization,Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture  parameter initialization  and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution  causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.,Stabilizing Training of Generative Adversarial

Networks through Regularization

Department of Computer Science

Kevin Roth

ETH Zürich

Aurelien Lucchi

Department of Computer Science

ETH Zürich

kevin.roth@inf.ethz.ch

aurelien.lucchi@inf.ethz.ch

Sebastian Nowozin
Microsoft Research

Cambridge  UK

sebastian.Nowozin@microsoft.com

Thomas Hofmann

Department of Computer Science

ETH Zürich

thomas.hofmann@inf.ethz.ch

Abstract

Deep generative models based on Generative Adversarial Networks (GANs) have
demonstrated impressive sample quality but in order to work they require a careful
choice of architecture  parameter initialization  and selection of hyper-parameters.
This fragility is in part due to a dimensional mismatch or non-overlapping support
between the model distribution and the data distribution  causing their density ratio
and the associated f-divergence to be undeﬁned. We overcome this fundamental
limitation and propose a new regularization approach with low computational cost
that yields a stable GAN training procedure. We demonstrate the effectiveness
of this regularizer accross several architectures trained on common benchmark
image generation tasks. Our regularization turns GAN models into reliable building
blocks for deep learning. 1

1

Introduction

A recent trend in the world of generative models is the use of deep neural networks as data generating
mechanisms. Two notable approaches in this area are variational auto-encoders (VAEs) [14  28] as
well as generative adversarial networks (GAN) [8]. GANs are especially appealing as they move
away from the common likelihood maximization viewpoint and instead use an adversarial game
approach for training generative models. Let us denote by P(x) and Q✓(x) the data and model
distribution  respectively. The basic idea behind GANs is to pair up a ✓-parametrized generator
network that produces Q✓ with a discriminator which aims to distinguish between P and Q✓  whereas
the generator aims for making Q✓ indistinguishable from P. Effectively the discriminator represents
a class of objective functions F that measures dissimilarity of pairs of probability distributions. The
ﬁnal objective is then formed via a supremum over F  leading to the saddle point problem

min

✓ `(Q✓;F) := sup

F2F

F (P  Q✓) .

(1)

The standard way of representing a speciﬁc F is through a family of statistics or discriminants  2  
typically realized by a neural network [8  26]. In GANs  we use these discriminators in a logistic
classiﬁcation loss as follows

F (P  Q; ) = EP [g((x))] + EQ [g((x))]  

1Code available at https://github.com/rothk/Stabilizing_GANs

(2)

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

where g(z) = ln((z)) is the log-logistic function (for reference  ((x)) = D(x) in [8]).
As shown in [8]  for the Bayes-optimal discriminator ⇤ 2   the above generator objective reduces
to the Jensen-Shannon (JS) divergence between P and Q. The work of [25] later generalized this to
a more general class of f-divergences  which gives more ﬂexibility in cases where the generative
model may not be expressive enough or where data may be scarce.
We consider three different challenges for learning the model distribution:
(A) empirical estimation: the model family may contain the true distribution or a good approximation
thereof  but one has to identify it based on a ﬁnite training sample drawn from P. This is commonly
addressed by the use of regularization techniques to avoid overﬁtting  e.g. in the context of estimating
f-divergences with M-estimators [24]. In our work  we suggest a novel (Tikhonov) regularizer 
derived and motivated from a training-with-noise scenario  where P and Q are convolved with white
Gaussian noise [30  3]  namely

F(P  Q; ) := F (P ⇤ ⇤  Q ⇤ ⇤; )  ⇤= N (0  I) .

(3)

(B) density misspeciﬁcation: the model distribution and true distribution both have a density function
with respect to the same base measure but there exists no parameter for which these densities are
sufﬁciently similar. Here  the principle of parameter estimation via divergence minimization is
provably sound in that it achieves a well-deﬁned limit [1  21]. It therefore provides a solid foundation
for statistical inference that is robust with regard to model misspeciﬁcations.
(C) dimensional misspeciﬁcation: the model distribution and the true distribution do not have a
density function with respect to the same base measure or – even worse – supp(P) \ supp(Q) may
be negligible. This may occur  whenever the model and/or data are conﬁned to low-dimensional
manifolds [3  23]. As pointed out in [3]  a geometric mismatch can be detrimental for f-GAN
models as the resulting f-divergence is not ﬁnite (the sup in Eq. (1) is +1). As a remedy  it has
been suggested to use an alternative family of distance functions known as integral probability
metrics [22  31]. These include the Wasserstein distance used in Wasserstein GANs (WGAN) [3] as
well as RKHS-induced maximum mean discrepancies [9  16  6]  which all remain well-deﬁned. We
will provide evidence (analytically and experimentally) that the noise-induced regularization method
proposed in this paper effectively makes f-GAN models robust against dimensional misspeciﬁcations.
While this introduces some dependency on the (Euclidean) metric of the ambient data space  it does
so on a well-controlled length scale (the amplitude of noise or strength of the regularization ) and
by retaining the beneﬁts of f-divergences. This is a rather gentle modiﬁcation compared to the more
radical departure taken in Wasserstein GANs  which rely solely on the ambient space metric (through
the notion of optimal mass transport).
In what follows  we will take Eq. (3) as the starting point and derive an approximation via a regularizer
that is simple to implement as an integral operator penalizing the squared gradient norm. As opposed
to a naïve norm penalization  each f-divergence has its own characteristic weighting function over
the input space  which depends on the discriminator output. We demonstrate the effectiveness
of our approach on a simple Gaussian mixture as well as on several benchmark image datasets
commonly used for generative models. In both cases  our proposed regularization yields stable GAN
training and produces samples of higher visual quality. We also perform pairwise tests of regularized
vs. unregularized GANs using a novel cross-testing protocol.
In summary  we make the following contributions:

• We systematically derive a novel  efﬁciently computable regularization method for f-GAN.
• We show how this addresses the dimensional misspeciﬁcation challenge.
• We empirically demonstrate stable GAN training across a broad set of models.

2 Background

The fundamental way to learn a generative model in machine learning is to (i) deﬁne a parametric
family of probability densities {Q✓}  ✓ 2 ⇥ ✓ Rd  and (ii) ﬁnd parameters ✓⇤ 2 ⇥ such that Q✓ is
closest (in some sense) to the true distribution P. There are various ways to measure how close model
and real distribution are  or equivalently  various ways to deﬁne a distance or divergence function
between P and Q. In the following we review different notions of divergences used in the literature.

2

f-divergence. GANs [8] are known to minimize the Jensen-Shannon divergence between P and
Q. This was generalized in [25] to f-divergences induced by a convex functions f. An interesting
property of f-divergences is that they permit a variational characterization [24  27] via

Df (P||Q) := EQf 

dP

dQ =ZX

sup

u ✓u ·

dP

dQ  f c(u)◆ dQ 

(4)

where dP/dQ is the Radon-Nikodym derivative and f c(t) ⌘ supu2domf{ut  f (u)} is the Fenchel
dual of f. By deﬁning an arbitrary class of statistics 3 : X! R we arrive at the bound

 {EP[ ]  EQ[f c  ]} .

(5)

Df (P||Q)  sup

 Z ✓ ·

dP

dQ  f c  ◆ dQ = sup

Eq. (5) thus gives us a variational lower bound on the f-divergence as an expectation over P and
Q  which is easier to evaluate (e.g. via sampling from P and Q  respectively) than the density
based formulation. We can see that by identifying = g   and with the choice of f such that
f c =  ln(1  exp)  we get f c  =  ln(1  ()) = g() thus recovering Eq. (2).
Integral Probability Metrics (IPM). An alternative family of divergences are integral probability
metrics [22  31]  which ﬁnd a witness function to distinguish between P and Q. This class of
methods yields an objective similar to Eq. (2) that requires optimizing a distance function between
two distributions over a function class F. Particular choices for F yield the kernel maximum mean
discrepancy approach of [9  16] or Wasserstein GANs [3]. The latter distance is deﬁned as

W (P  Q) = sup

kfkL1{EP[f ]  EQ[f ]} 

(6)

where the supremum is taken over functions f which have a bounded Lipschitz constant.
As shown in [3]  the Wasserstein metric implies a different notion of convergence compared to the
JS divergence used in the original GAN. Essentially  the Wasserstein metric is said to be weak as it
requires the use of a weaker topology  thus making it easier for a sequence of distribution to converge.
The use of a weaker topology is achieved by restricting the function class to the set of bounded
Lipschitz functions. This yields a hard constraint on the function class that is empirically hard to
satisfy. In [3]  this constraint is implemented via weight clipping  which is acknowledged to be a
"terrible way" to enforce the Lipschitz constraint. As will be shown later  our regularization penalty
can be seen as a soft constraint on the Lipschitz constant of the function class which is easy to
implement in practice. Recently  [10] has also proposed a similar regularization; while their proposal
was motivated for Wasserstein GANs and does not extend to f-divergences it is interesting to observe
that both their and our regularization work on the gradient.

Training with Noise. As suggested in [3  30]  one can break the dimensional misspeciﬁcation
discussed in Section 1 by adding continuous noise to the inputs of the discriminator  therefore
smoothing the probability distribution. However  this requires to add high-dimensional noise  which
introduces signiﬁcant variance in the parameter estimation process. Counteracting this requires a
lot of samples and therefore ultimately leads to a costly or impractical solution. Instead we propose
an approach that relies on analytic convolution of the densities P and Q with Gaussian noise. As
we demonstrate below  this yields a simple weighted penalty function on the norm of the gradients.
Conceptually we think of this noise not as being part of the generative process (as in [3])  but rather
as a way to deﬁne a smoother family of discriminants for the variational bound of f-divergences.

Regularization for Mode Dropping. Other regularization techniques address the problem of mode
dropping and are complementary to our approach. This includes the work of [7] which incorporates a
supervised training signal as a regularizer on top of the discriminator target. To implement supervision
the authors use an additional auto-encoder as well as a two-step training procedure which might
be computationally expensive. A similar approach was proposed by [20] that stabilizes GANs by
unrolling the optimization of the discriminator. The main drawback of this approach is that the
computational cost scales with the number of unrolling steps. In general  it is not clear to what extent
these methods not only stabilize GAN training  but also address the conceptual challenges listed in
Section 1.

3

3 Noise-Induced Regularization
From now onwards  we consider the general f-GAN [25] objective deﬁned as

F (P  Q; ) ⌘ EP[ ]  EQ[f c  ].

(7)

3.1 Noise Convolution
From a practitioners point of view  training with noise can be realized by adding zero-mean random
variables ⇠ to samples x ⇠ P  Q during training. Here we focus on normal white noise ⇠ ⇠ ⇤=
N (0  I) (the same analysis goes through with a Laplacian noise distribution for instance). From a
theoretical perspective  adding noise is tantamount to convolving the corresponding distribution as
EPE⇤[ (x + ⇠)] =Z (x)Z p(x  ⇠)(⇠)d⇠ dx =Z (x)(p ⇤ )(x)dx = EP⇤⇤[ ].
where p and  are probability densities of P and ⇤  respectively  with regard to the Lebesgue measure.
The noise distribution ⇤ as well as the resulting P⇤⇤ are guaranteed to have full support in the ambient
space  i.e. (x) > 0 and (p ⇤ )(x) > 0 (8x). Technically  applying this to both P and Q makes the
resulting generalized f-divergence well-deﬁned  even when the generative model is dimensionally
misspeciﬁed. Note that approximating E⇤ through sampling was previously investigated in [30  3].

(8)

3.2 Convolved Discriminants
With symmetric noise  (⇠) = (⇠)  we can write Eq. (8) equivalently as

EP⇤⇤[ ] = EPE⇤[ (x + ⇠)] =Z p(x)Z (x  ⇠)(⇠) d⇠ dx = EP[ ⇤ ].

(9)

For the Q-expectation in Eq. (7) one gets  by the same argument  EQ⇤⇤[f c  ] = EQ [(f c  ) ⇤ ].
Formally  this generalizes the variational bound for f-divergences in the following manner:
(10)
F (P ⇤ ⇤  Q ⇤ ⇤; ) = F (P  Q; ⇤   (f c  ) ⇤ )  F (P  Q; ⇢  ⌧ ) := EP[⇢]  EQ[⌧ ]
Assuming that F is closed under ⇤ convolutions  the regularization will result in a relative weakening
of the discriminator as we take the sup over a smaller  more regular family. Clearly  the low-pass
effect of ⇤-convolutions can be well understood in the Fourier domain. In this equivalent formulation 
we leave P and Q unchanged  yet we change the view the discriminator can take on the ambient data
space: metaphorically speaking  the generator is paired up with a short-sighted adversary.

3.3 Analytic Approximations
In general  it may be difﬁcult to analytically compute ⇤  or – equivalently – E⇤[ (x + ⇠)].
However  for small  we can use a Taylor approximation of around ⇠ = 0 (cf. [5]):

 (x + ⇠) = (x) + [r (x)]T ⇠ +

1
2

⇠T [r2 (x)] ⇠ + O(⇠3)

(11)

where r2 denotes the Hessian  whose trace Tr(r2) = 4 is known as the Laplace operator. The
properties of white noise result in the approximation

E⇤[ (x + ⇠)] = (x) +


24 (x) + O(2)

(12)

and thereby lead directly to an approximation of F (see Eq. (3)) via F = F0 plus a correction  i.e.

F(P  Q; ) = F(P  Q; ) +


2 {EP [4 ]  EQ [4(f c  )]} + O(2) .

(13)

We can interpret Eq. (13) as follows: the Laplacian measures how much the scalar ﬁelds and
f c  differ at each point from their local average. It is thereby an inﬁnitesimal proxy for the (exact)
convolution.
The Laplace operator is a sum of d terms  where d is the dimensionality of the ambient data space. As
such it does not suffer from the quadratic blow-up involved in computing the Hessian. If we realize
the discriminator via a deep network  however  then we need to be able to compute the Laplacian
of composed functions. For concreteness  let us assume that = h  G  G = (g1  . . .   gk) and look

4

at a single input x  i.e. gi : R ! R  then
(h  G)0 =Xi

g0i · (@ih  G) 

(h  G)00 =Xi

g00i · (@ih  G) +Xi j

g0i · g0j · (@i@jh  G)

(14)

So at the intermediate layer  we would need to effectively operate with a full Hessian  which is
computationally demanding  as has already been observed in [5].

F(P  Q; ⇤) = F(P  Q; ⇤) 

(17)

3.4 Efﬁcient Gradient-Based Regularization
We would like to derive a (more) tractable strategy for regularizing   which (i) avoids the detrimental
variance that comes from sampling ⇠  (ii) does not rely on explicitly convolving the distributions
P and Q  and (iii) avoids the computation of Laplacians as in Eq. (13). Clearly  this requires to
make further simpliﬁcations. We suggest to exploit properties of the maximizer ⇤ of F that can be
characterized by [24]

(f c0  ⇤) dQ = dP =) EP[h] = EQ[(f c0  ⇤) · h]

(15)
The relevance of this becomes clear  if we apply the chain rule to 4(f c  )  assuming that f c is
twice differentiable
(16)

4(f c  ) = (f c00  ) · ||r ||2 +f c0  4  
as now we get a convenient cancellation of the Laplacians at = ⇤ + O()

(8h  integrable).

We can (heuristically) turn this into a regularizer by taking the leading terms 


2

EQh(f c00  ⇤) · kr ⇤k2i + O(2) .
⌦f (Q; )  ⌦f (Q; ) := EQh(f c00  ) · kr k2i .


2

F(P  Q; ) ⇡ F(P  Q; ) 

(18)
Note that we do not assume that the Laplacian terms cancel far away from the optimum  i.e. we do not
assume Eq. (15) to hold for far away from ⇤. Instead  the underlying assumption we make is that
optimizing the gradient-norm regularized objective F(P  Q; ) makes converge to ⇤ + O() 
for which we know that the Laplacian terms cancel [5  2].
The convexity of f c implies that the weighting function of the squared gradient norm is non-negative 
i.e. f c00  0  which in turn implies that the regularizer  
2 ⌦f (Q; ) is upper bounded (by zero).
Maximization of F(P  Q; ) with respect to is therefore well-deﬁned. Further considerations
regarding the well-deﬁnedness of the regularizer can be found in sec. 7.2 in the Appendix.
4 Regularizing GANs
We have shown that training with noise is equivalent to regularizing the discriminator. Inspired by
the above analysis  we propose the following class of f-GAN regularizers:

Regularized f-GAN

F(P  Q; ) = EP [ ]  EQ [f c  ] 
⌦f (Q; ) := EQh(f c00  )kr k2i


2

⌦f (Q; )

(19)

The regularizer corresponding to the commonly used parametrization of the Jensen-Shannon GAN
can be derived analogously as shown in the Appendix. We obtain 

Regularized Jensen-Shannon GAN

F(P  Q; ') = EP [ln(')] + EQ [ln(1  ')] 
⌦JS(P  Q; ') := EP⇥(1  '(x))2||r(x)||2⇤ + EQ⇥'(x)2||r(x)||2⇤

⌦JS(P  Q; ')


2

(20)

where  = 1(') denotes the logit of the discriminator '. We prefer to compute the gradient of 
as it is easier to implement and more robust than computing gradients after applying the sigmoid.

5

Algorithm 1 Regularized JS-GAN. Default values: 0 = 2.0  ↵ = 0.01 (with annealing)   = 0.1 (without
annealing)  n' = 1
Require: Initial noise variance 0  annealing decay rate ↵  number of discriminator update steps n'

per generator iteration  minibatch size m  number of training iterations T
Require: Initial discriminator parameters !0  initial generator parameters ✓0

for t = 1  ...  T do

 0 · ↵t/T # annealing
for 1  ...  n' do

1
m

Sample minibatch of real data {x(1)  ...  x(m)}⇠ P.
Sample minibatch of latent variables from prior {z(1)  ...  z(m)}⇠ p(z).
F (!  ✓) =

mXi=1 h ln⇣'!(x(i))⌘ + ln⇣1  '!(G✓(z(i)))⌘i
mXi=1 ⇣1  '!(x(i))⌘2r!(x(i))2 + '!G✓(z(i))2r˜x!(˜x)˜x=G✓(z(i))2

⌦(!  ✓) =

1
m


2

⌦(!  ✓)⌘ # gradient ascent

! ! + r!⇣F (!  ✓) 
end for
Sample minibatch of latent variables from prior {z(1)  ...  z(m)}⇠ p(z).
mXi=1
F (!  ✓) =

ln⇣1  '!(G✓(z(i)))⌘ or Falt(!  ✓) = 

mXi=1

1
m

1
m

✓ ✓  r✓F(!  ✓) # gradient descent

ln⇣'!(G✓(z(i)))⌘

end for
The gradient-based updates can be performed with any gradient-based learning rule. We used
Adam in our experiments.

4.1 Training Algorithm
Regularizing the discriminator provides an efﬁcient way to convolve the distributions and is thereby
sufﬁcient to address the dimensional misspeciﬁcation challenges outlined in the introduction. This
leaves open the possibility to use the regularizer also in the objective of the generator. On the one
hand  optimizing the generator through the regularized objective may provide useful gradient signal
and therefore accelerate training. On the other hand  it destabilizes training close to convergence
(if not dealt with properly)  since the generator is incentiviced to put probability mass where the
discriminator has large gradients. In the case of JS-GANs  we recommend to pair up the regularized
objective of the discriminator with the “alternative” or “non-saturating” objective for the generator 
proposed in [8]  which is known to provide strong gradients out of the box (see Algorithm 1).

4.2 Annealing
The regularizer variance  lends itself nicely to annealing. Our experimental results indicate that a
reasonable annealing scheme consists in regularizing with a large initial  early in training and then
(exponentially) decaying  to a small non-zero value. We leave to future work the question of how to
determine an optimal annealing schedule.

2D submanifold mixture of Gaussians in 3D space

5 Experiments
5.1
To demonstrate the stabilizing effect of the regularizer  we train a simple GAN architecture [20] on a
2D submanifold mixture of seven Gaussians arranged in a circle and embedded in 3D space (further
details and an illustration of the mixture distribution are provided in the Appendix). We emphasize
that this mixture is degenerate with respect to the base measure deﬁned in ambient space as it does
not have fully dimensional support  thus precisely representing one of the failure scenarios commonly

6

.

G
E
R
N
U

1
0
0

.

0
1

.

Figure 1: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized GANs
trained to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization . Even for small but non-zero noise variance  the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with one discriminator update per generator update step (indicated).

described in the literature [3]. The results are shown in Fig. 1 for both standard unregularized GAN
training as well as our regularized variant.
While the unregularized GAN collapses in literally every run after around 50k iterations  due to the
fact that the discriminator concentrates on ever smaller differences between generated and true data
(the stakes are getting higher as training progresses)  the regularized variant can be trained essentially
indeﬁnitely (well beyond 200k iterations) without collapse for various degrees of noise variance  with
and without annealing. The stabilizing effect of the regularizer is even more pronounced when the
GANs are trained with ﬁve discriminator updates per generator update step  as shown in Fig. 6.

5.2 Stability across various architectures
To demonstrate the stability of the regularized training procedure and to showcase the excellent
quality of the samples generated from it  we trained various network architectures on the CelebA [17] 
CIFAR-10 [15] and LSUN bedrooms [32] datasets. In addition to the deep convolutional GAN
(DCGAN) of [26]  we trained several common architectures that are known to be hard to train
[4  26  19]  therefore allowing us to establish a comparison to the concurrently proposed gradient-
penalty regularizer for Wasserstein GANs [10]. Among these architectures are a DCGAN without
any normalization in either the discriminator or the generator  a DCGAN with tanh activations and a
deep residual network (ResNet) GAN [11]. We used the open-source implementation of [10] for our
experiments on CelebA and LSUN  with one notable exception: we use batch normalization also for
the discriminator (as our regularizer does not depend on the optimal transport plan or more precisely
the gradient penalty being imposed along it).
All networks were trained using the Adam optimizer [13] with learning rate 2 ⇥ 104 and hyper-
parameters recommended by [26]. We trained all datasets using batches of size 64  for a total of
200K generator iterations in the case of LSUN and 100k iterations on CelebA. The results of these
experiments are shown in Figs. 3 & 2. Further implementation details can be found in the Appendix.

5.3 Training time
We empirically found regularization to increase the overall training time by a marginal factor
of roughly 1.4 (due to the additional backpropagation through the computational graph of the
discriminator gradients). More importantly  however  (regularized) f-GANs are known to converge
(or at least generate good looking samples) faster than their WGAN relatives [10].

7

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 2: Stability accross various architectures: ResNet  DCGAN  DCGAN without normalization
and DCGAN with tanh activations (details in the Appendix). All samples were generated from
regularized GANs with exponentially annealed 0 = 2.0 (and alternative generator loss) as described
in Algorithm 1. Samples were produced after 200k generator iterations on the LSUN dataset (see also
Fig. 8 for a full-resolution image of the ResNet GAN). Samples for the unregularized architectures
can be found in the Appendix.

UNREG.

0.5

1.0

2.0

Figure 3: Annealed Regularization. CelebA samples generated by (un)regularized ResNet GANs.
The initial level of regularization 0 is shown below each batch of images. 0 was exponentially
annealed as described in Algorithm 1. The regularized GANs can be trained essentially indeﬁnitely
without collapse  the superior quality is again evident. Samples were produced after 100k generator
iterations.

5.4 Regularization vs. explicitly adding noise

We compare our regularizer against the common practitioner’s approach to explicitly adding noise to
images during training. In order to compare both approaches (analytic regularizer vs. explicit noise) 
we ﬁx a common batch size (64 in our case) and subsequently train with different noise-to-signal
ratios (NSR): we take (batch-size/NSR) samples (both from the dataset and generated ones) to each
of which a number of NSR noise vectors is added and feed them to the discriminator (so that overall
both models are trained on the same batch size). We experimented with NSR 1  2  4  8 and show the
best performing ratio (further ratios in the Appendix). Explicitly adding noise in high-dimensional
ambient spaces introduces additional sampling variance which is not present in the regularized variant.
The results  shown in Fig. 4  conﬁrm that the regularizer stabilizes across a broad range of noise
levels and manages to produce images of considerably higher quality than the unregularized variants.

5.5 Cross-testing protocol

We propose the following pairwise cross-testing protocol to assess the relative quality of two GAN
models: unregularized GAN (Model 1) vs. regularized GAN (Model 2). We ﬁrst report the confusion
matrix (classiﬁcation of 10k samples from the test set against 10k generated samples) for each model
separately. We then classify 10k samples generated by Model 1 with the discriminator of Model 2
and vice versa. For both models  we report the fraction of false positives (FP) (Type I error) and false
negatives (FN) (Type II error). The discriminator with the lower FP (and/or lower FN) rate deﬁnes
the better model  in the sense that it is able to more accurately classify out-of-data samples  which
indicates better generalization properties. We obtained the following results on CIFAR-10:

8

UNREGULARIZED

0.01

EXPLICIT NOISE

0.1

1.0

0.001

0.01

REGULARIZED

0.1

1.0

Figure 4: CIFAR-10 samples generated by (un)regularized DCGANs (with alternative generator loss) 
as well as by training a DCGAN with explicitly added noise (noise-to-signal ratio 4). The level of
regularization or noise  is shown above each batch of images. The regularizer stabilizes across a
broad range of noise levels and manages to produce images of higher quality than the unregularized
variants. Samples were produced after 50 training epochs.

Regularized GAN ( = 0.1)

Unregularized GAN

True condition

Positive Negative
0.0002
0.9688
0.0312
0.9998

Predicted

Predicted

Positive
Negative
Cross-testing: FP: 0.0
For both models  the discriminator is able to recognize his own generator’s samples (low FP in the
confusion matrix). The regularized GAN also manages to perfectly classify the unregularized GAN’s
samples as fake (cross-testing FP 0.0) whereas the unregularized GAN classiﬁes the samples of the
regularized GAN as real (cross-testing FP 1.0). In other words  the regularized model is able to fool
the unregularized one  whereas the regularized variant cannot be fooled.

Positive
Negative
Cross-testing: FP: 1.0

True condition

Positive Negative
0.0013
0.9987

1.0
0.0

6 Conclusion

We introduced a regularization scheme to train deep generative models based on generative adversarial
networks (GANs). While dimensional misspeciﬁcations or non-overlapping support between the
data and model distributions can cause severe failure modes for GANs  we showed that this can be
addressed by adding a penalty on the weighted gradient-norm of the discriminator. Our main result is
a simple yet effective modiﬁcation of the standard training algorithm for GANs  turning them into
reliable building blocks for deep learning that can essentially be trained indeﬁnitely without collapse.
Our experiments demonstrate that our regularizer improves stability  prevents GANs from overﬁtting
and therefore leads to better generalization properties (cf cross-testing protocol). Further research on
the optimization of GANs as well as their convergence and generalization can readily be built upon
our theoretical results.

9

Acknowledgements

We would like to thank Devon Hjelm for pointing out that the regularizer works well with ResNets.
KR is thankful to Yannic Kilcher  Lars Mescheder and the dalab team for insightful discussions. Big
thanks also to Ishaan Gulrajani and Taehoon Kim for their open-source GAN implementations. This
work was supported by Microsoft Research through its PhD Scholarship Programme.

References
[1] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry. American Mathe-

matical Soc.  2007.

[2] Guozhong An. The effects of adding noise during backpropagation training on a generalization

performance. Neural Comput.  pages 643–674  1996.

[3] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-

sarial networks. In ICLR  2017.

[4] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial

networks. Proceedings of Machine Learning Research. PMLR  2017.

[5] Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computa-

tion  7:108–116  1995.

[6] Diane Bouchacourt  Pawan K Mudigonda  and Sebastian Nowozin. Disco nets: Dissimilarity
coefﬁcients networks. In Advances in Neural Information Processing Systems  pages 352–360 
2016.

[7] Tong Che  Yanran Li  Athul Paul Jacob  Yoshua Bengio  and Wenjie Li. Mode regularized

generative adversarial networks. arXiv preprint arXiv:1612.02136  2016.

[8] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative Adversarial Networks. In Advances in
Neural Information Processing Systems  pages 2672–2680  2014.

[9] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13:723–773  2012.

[10] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems 
2017.

[11] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  page
770–778  2016.

[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. Proceedings of Machine Learning Research  pages 448–456.
PMLR  2015.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. The

International Conference on Learning Representations (ICLR)  2014.

[14] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. The International

Conference on Learning Representations (ICLR)  2013.

[15] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[16] Yujia Li  Kevin Swersky  and Richard S Zemel. Generative moment matching networks. In

ICML  pages 1718–1727  2015.

10

[17] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE International Conference on Computer Vision  pages
3730–3738  2015.

[18] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of International Conference on Computer Vision (ICCV)  2015.

[19] Lars Mescheder  Sebastian Nowozin  and Andreas Geiger. The numerics of gans. In Advances

in Neural Information Processing Systems  2017.

[20] Luke Metz  Ben Poole  David Pfau  and Jascha Sohl-Dickstein. Unrolled generative adversarial

networks. In International Conference on Learning Representations (ICLR)  2016.

[21] Tom Minka. Divergence measures and message passing. Technical report  Microsoft Research 

2005.

[22] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances

in Applied Probability  29:429–443  1997.

[23] Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis.

In Advances in Neural Information Processing Systems  pages 1786–1794  2010.

[24] XuanLong Nguyen  Martin J Wainwright  and Michael I Jordan. Estimating divergence func-
tionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information
Theory  56(11):5847–5861  2010.

[25] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-GAN: Training generative neu-
ral samplers using variational divergence minimization. In Advances in Neural Information
Processing Systems  pages 271–279  2016.

[26] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[27] Mark D Reid and Robert C Williamson. Information  divergence and risk for binary experiments.

Journal of Machine Learning Research  12:731–817  2011.

[28] Danilo J Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International
Conference on Machine Learning  2014.

[29] David W Scott. Multivariate density estimation: theory  practice  and visualization. John Wiley

& Sons  2015.

[30] Casper Kaae Sønderby  Jose Caballero  Lucas Theis  Wenzhe Shi  and Ferenc Huszár. Amortised

map inference for image super-resolution. arXiv preprint arXiv:1610.04490  2016.

[31] Bharath K Sriperumbudur  Kenji Fukumizu  Arthur Gretton  Bernhard Schölkopf  and Gert RG
Lanckriet. On integral probability metrics  phi-divergences and binary classiﬁcation. arXiv
preprint arXiv:0901.2698  2009.

[32] Fisher Yu  Yinda Zhang  Shuran Song  Ari Seff  and Jianxiong Xiao. Lsun: Construction
of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365  2015.

11

,David Eigen
Rob Fergus
Kevin Roth
Aurelien Lucchi
Sebastian Nowozin
Thomas Hofmann
Kuang Xu