2019,Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity,We study finite sample expressivity  i.e.  memorization power of ReLU networks. Recent results require $N$ hidden nodes to memorize/interpolate arbitrary $N$ data points. In contrast  by exploiting depth  we show that 3-layer ReLU networks with $\Omega(\sqrt{N})$ hidden nodes can perfectly memorize most datasets with $N$ points. We also prove that width $\Theta(\sqrt{N})$ is necessary and sufficient for memorizing $N$ data points  proving tight bounds on memorization capacity. The sufficiency result can be extended to deeper networks; we show that an $L$-layer network with $W$ parameters in the hidden layers can memorize $N$ data points if $W = \Omega(N)$. Combined with a recent upper bound $O(WL\log W)$ on VC dimension  our construction is nearly tight for any fixed $L$. Subsequently  we analyze memorization capacity of residual networks under a general position assumption; we prove results that substantially reduce the known requirement of $N$ hidden nodes. Finally  we study the dynamics of stochastic gradient descent (SGD)  and show that when initialized near a memorizing global minimum of the empirical risk  SGD quickly finds a nearby point with much smaller empirical risk.,Small ReLU networks are powerful memorizers:

a tight analysis of memorization capacity

Chulhee Yun

MIT

Cambridge  MA 02139
chulheey@mit.edu

Suvrit Sra

MIT

Cambridge  MA 02139

suvrit@mit.edu

Ali Jadbabaie

MIT

Cambridge  MA 02139
jadbabai@mit.edu

Abstract

We study ﬁnite sample expressivity  i.e.  memorization power of ReLU networks.
Recent results require N hidden nodes to memorize/interpolate arbitrary N data
√
points. In contrast  by exploiting depth  we show that 3-layer ReLU networks with
√
N ) hidden nodes can perfectly memorize most datasets with N points. We also
Ω(
N ) is necessary and sufﬁcient for memorizing N data points 
prove that width Θ(
proving tight bounds on memorization capacity. The sufﬁciency result can be
extended to deeper networks; we show that an L-layer network with W parameters
in the hidden layers can memorize N data points if W = Ω(N ). Combined
with a recent upper bound O(W L log W ) on VC dimension  our construction is
nearly tight for any ﬁxed L. Subsequently  we analyze memorization capacity
of residual networks under a general position assumption; we prove results that
substantially reduce the known requirement of N hidden nodes. Finally  we study
the dynamics of stochastic gradient descent (SGD)  and show that when initialized
near a memorizing global minimum of the empirical risk  SGD quickly ﬁnds a
nearby point with much smaller empirical risk.

Introduction

1
Recent results in deep learning indicate that over-parameterized neural networks can memorize
arbitrary datasets [2  53]. This phenomenon is closely related to the expressive power of neural
networks  which have been long studied as universal approximators [12  18  21]. These results
suggest that sufﬁciently large neural networks are expressive enough to ﬁt any dataset perfectly.
With the widespread use of deep networks  recent works have focused on better understanding the
power of depth [13  17  30  33  37  38  44  45  49  50]. However  most existing results consider
expressing functions (i.e.  inﬁnitely many points) rather than ﬁnite number of observations; thus  they
do not provide a precise understanding the memorization ability of ﬁnitely large networks.
When studying ﬁnite sample memorization  several questions arise: Is a neural network capable of
memorizing arbitrary datasets of a given size? How large must a neural network be to possess such
capacity? These questions are the focus of this paper  and we answer them by studying universal
ﬁnite sample expressivity and memorization capacity; these concepts are formally deﬁned below.
Deﬁnition 1.1. We deﬁne (universal) ﬁnite sample expressivity of a neural network fθ(·)
(parametrized by θ) as the network’s ability to satisfy the following condition:

For all inputs {xi}N
exists a parameter θ such that fθ(xi) = yi for 1 ≤ i ≤ N.

i=1 ∈ Rdx×N and for all {yi}N

i=1 ∈ [−1  +1]dy×N   there

We deﬁne memorization capacity of a network to be the maximum value of N for which the network
has ﬁnite sample expressivity when dy = 1.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Memorization capacity is related to  but is different from VC dimension of neural networks [3  4].
Recall the deﬁnition of VC dimension of a neural network fθ(·):

The maximum value N such that there exists a dataset {xi}N
that for all {yi}N

i=1 ∈ Rdx×N such
i=1 ∈ {±1}N there exists θ such that fθ(xi) = yi for 1 ≤ i ≤ N.

Notice that the key difference between memorization capacity and VC dimension is in the quantiﬁers
in front of the xi’s. Memorization capacity is always less than or equal to VC dimension  which
means that an upper bound on VC dimension is also an upper bound on memorization capacity.
The study of ﬁnite sample expressivity and memorization capacity of neural networks has a long
history  dating back to the days of perceptrons [6  11  22–24  26  36  42  48]; however  the older studies
focus on shallow networks with traditional activations such as sigmoids  delivering limited insights for
deep ReLU networks. Since the advent of deep learning  some recent results on modern architectures
appeared  e.g.  fully-connected neural networks (FNNs) [53]  residual networks (ResNets) [20]  and
convolutional neural networks (CNNs) [35]. However  they impose assumptions on architectures that
are neither practical nor realistic. For example  they require a hidden layer as wide as the number of
data points N [35  53]  or as many hidden nodes as N [20]  causing their theoretical results to be
applicable only to very large neural networks; this can be unrealistic especially when N is large.
1.1 Summary of our contributions
Before stating our contributions  a brief comment on “network size” is in order. The size of a neural
network can be somewhat vague; it could mean width/depth  the number of edges  or the number of
hidden nodes. We use “size” to refer to the number of hidden nodes in a network. This also applies to
notions related to size; e.g.  by a “small network” we mean a network with a small number of hidden
nodes. For other measures of size such as width  we will use the words explicitly.
1. Finite sample expressivity of neural networks. Our ﬁrst set of results is on the ﬁnite sample
expressivity of FNNs (Section 3)  under the assumption of distinct data point xi’s. For simplicity  we
only summarize our results for ReLU networks  but they include hard-tanh networks as well.

• Theorem 3.1 shows that any 3-layer (i.e.  2-hidden-layer) ReLU FNN with hidden layer widths
d1 and d2 can ﬁt any arbitrary dataset if d1d2 ≥ 4N dy  where N is the number of data points
and dy is the output dimension. For scalar outputs  this means d1 = d2 = 2
N sufﬁces to ﬁt
arbitrary data. This width requirement is signiﬁcantly smaller than existing results on ReLU.

√

• The improvement is more dramatic for classiﬁcation. If we have dy classes  Proposition 3.2
shows that a 4-layer ReLU FNN with hidden layer widths d1  d2  and d3 can ﬁt any dataset if
d1d2 ≥ 4N and d3 ≥ 4dy. This means that 106 data points in 103 classes (e.g.  ImageNet) can
be memorized by a 4-layer FNN with hidden layer widths 2k-2k-4k.

• For dy = 1  note that Theorem 3.1 shows a lower bound of Ω(d1d2) on memorization capacity.
We prove a matching upper bound in Theorem 3.3: we show that for shallow neural networks (2
or 3 layers)  lower bounds on memorization capacity are tight.
• Proposition 3.4 extends Theorem 3.1 to deeper and/or narrower networks  and shows that if the
sum of the number of edges between pairs of adjacent layers satisﬁes dl1dl1+1+···+dlmdlm+1 =
Ω(N dy)  then universal ﬁnite sample expressivity holds. This gives a lower bound Ω(W ) on
memorization capacity  where W is the number of edges in the network. Due to an upper bound
O(W L log W ) (L is depth) on VC dimension [4]  our lower bound is almost tight for ﬁxed L.

Next  in Section 4  we focus on classiﬁcation using ResNets; here dx denotes the input dimension
and dy the number of classes. We assume here that data lies in general position.

• Theorem 4.1 proves that deep ResNets with 4N
+ 6dy ReLU hidden nodes can memorize
dx
arbitrary datasets. Using the same proof technique  we also show in Corollary 4.2 that a 2-layer
ReLU FNN can memorize arbitrary classiﬁcation datasets if d1 ≥ 4N
+ 4dy. With the general
position assumption  we can reduce the existing requirements of N to a more realistic number.

dx

2. Trajectory of SGD near memorizing global minima. Finally  in Section 5 we study the
behavior of stochastic gradient descent (SGD) on the empirical risk of universally expressive FNNs.
• Theorem 5.1 shows that for any differentiable global minimum that memorizes  SGD initialized
close enough (say  away) to the minimum  quickly ﬁnds a point that has empirical risk O(4)

2

and is at most 2 far from the minimum. We emphasize that this theorem holds not only for
memorizers explicitly constructed in Sections 3 and 4  but for all global minima that memorize.
We note that we analyze without replacement SGD that is closer to practice than the simpler with-
replacement version [19  40]; thus  our analysis may be of independent interest in optimization.

1.2 Related work
Universal ﬁnite sample expressivity of neural networks. Literature on ﬁnite sample expressivity
and memorization capacity of neural networks dates back to the 1960s. Earlier results [6  11  26  36 
42] study memorization capacity of linear threshold networks.
Later  results on 2-layer FNNs with sigmoids [24] and other bounded activations [23] show that N
hidden nodes are sufﬁcient to memorize N data points. It was later shown that the requirement of
N hidden nodes can be improved by exploiting depth [22  48]. Since these two works are highly
relevant to our own results  we defer a detailed discussion/comparison until we present the precise
theorems (see Sections 3.2 and 3.3).
With the advent of deep learning  there have been new results on modern activation functions and
architectures. Zhang et al. [53] prove that one-hidden-layer ReLU FNNs with N hidden nodes can
memorize N real-valued data points. Hardt and Ma [20] show that deep ResNets with N + dy hidden
nodes can memorize arbitrary dy-class classiﬁcation datasets. Nguyen and Hein [35] show that deep
CNNs with one of the hidden layers as wide as N can memorize N real-valued data points.
Soudry and Carmon [43] show that under a dropout noise setting  the training error is zero at every
differentiable local minimum  for almost every dataset and dropout-like noise realization. However 
this result is not comparable to ours because they assume that there is a multiplicative “dropout noise”
at each hidden node and each data point. At i-th node of l-th layer  the slope of the activation function
i l · s (if input is negative  s (cid:54)= 0) 
for the j-th data point is either (j)
where (j)
i l is the multiplicative random (e.g.  Gaussian) dropout noise. Their theorem statements hold
for all realizations of these dropout noise factors except a set of measure zero. In contrast  our setting
is free of these noise terms  and hence corresponds to a speciﬁc realization of such (n)

i l · 1 (if input is positive) or (j)

i l ’s.

Convergence to global minima. There exist numerous papers that study convergence of gradient
descent or SGD to global optima of neural networks. Many previous results [9  14  29  41  46  54  55]
study settings where data points are sampled from a distribution (e.g.  Gaussian)  and labels are
generated from a “teacher network” that has the same architecture as the one being trained (i.e. 
realizability). Here  the goal of training is to recover the unknown (but ﬁxed) true parameters. In
comparison  we consider arbitrary datasets and networks  under a mild assumption (especially for
overparametrized networks) that the network can memorize the data; the results are not directly
comparable. Others [10  47] study SGD on hinge loss under a bit strong assumption that the data is
linearly separable.
Other recent results [1  15  16  28  58] focus on over-parameterized neural networks. In these papers 
the widths of hidden layers are assumed to be huge  of polynomial order in N  such as Ω(N 4)  Ω(N 6)
or even greater. Although these works provide insights on how GD/SGD ﬁnds global minima easily 
their width requirement is still far from being realistic.
A recent work [57] provides a mixture of observation and theory about convergence to global minima.
The authors assume that networks can memorize the data  and that SGD follows a star-convex path
to global minima  which they validate through experiments. Under these assumptions  they prove
convergence of SGD to global minimizers. We believe our result is complementary: we provide
sufﬁcient conditions for networks to memorize the data  and our result does not assume anything
about SGD’s path but proves that SGD can ﬁnd a point close to the global minimum.

Remarks on generalization. The ability of neural networks to memorize and generalize at the
same time has been one of the biggest mysteries of deep learning [53]. Recent results on interpolation
and “double descent” phenomenon indicate that memorization may not necessarily mean lack of
generalization [5  7  8  31  32  34]. We note that our paper focuses mainly on the ability of neural
networks to memorize the training dataset  and that our results are separate from the discussion of
generalization.

3

2 Problem setting and notation
In this section  we introduce the notation used throughout the paper. For integers a and b  a < b  we
denote [a] := {1  . . .   a} and [a : b] := {a  a + 1  . . .   b}. We denote {(xi  yi)}N
i=1 the set of training
data points  and our goal is to choose the network parameters θ so that the network output fθ(xi) is
equal to yi  for all i ∈ [n]. Let dx and dy denote input and output dimensions  respectively. Given
input x ∈ Rdx  an L-layer fully-connected neural network computes output fθ(x) as follows:

a0(x) = x 
zl(x) = W lal−1(x) + bl 
fθ(x) = W LaL−1(x) + bL.

al(x) = σ(zl(x)) 

for l ∈ [L − 1] 

Let dl (for l ∈ [L − 1]) denote the width of l-th hidden layer. For convenience  we write d0 := dx
and dL := dy. Here  zl ∈ Rdl and al ∈ Rdl denote the input and output (a for activation) of the l-th
hidden layer  respectively. The output of a hidden layer is the entry-wise map of the input by the
activation function σ. The bold-cased symbols denote parameters: W l ∈ Rdl×dl−1 is the weight
matrix  and bl ∈ Rdl is the bias vector. We deﬁne θ := (W l  bl)L
l=1 to be the collection of all
parameters. We write the network output as fθ(·) to emphasize that it depends on parameters θ.
Our results in this paper consider piecewise linear activation functions. Among them  Sections 3 and
4 consider ReLU-like (σR) and hard-tanh (σH) activations  deﬁned as follows:

t ≥ 0 
t < 0 

σH(t) :=

t ≤ −1 
t ∈ (−1  1] 
t > 1 

=

σR(t + 1) − σR(t − 1) − s+ − s−

s+ − s−

 

(cid:26)s+t

s−t

σR(t) :=

−1

t
1

where s+ > s− ≥ 0. Note that σR includes ReLU and Leaky ReLU. Hard-tanh activation (σH) is a
piecewise linear approximation of tanh. Since σH can be represented with two σR  any results on
hard-tanh networks can be extended to ReLU-like networks with twice the width.

3 Finite sample expressivity of FNNs
In this section  we study universal ﬁnite sample expressivity of FNNs. For the training dataset  we
make the following mild assumption that ensures consistent labels:
Assumption 3.1. In the dataset {(xi  yi)}N
i=1 assume that all xi’s are distinct and all yi ∈ [−1  1]dy.
3.1 Main results
We ﬁrst state the main theorems on shallow FNNs showing tight lower and upper bounds on memo-
rization capacity. Detailed discussion will follow in the next subsection.
Theorem 3.1. Consider any dataset {(xi  yi)}N

i=1 that satisﬁes Assumption 3.1. If

• a 3-layer hard-tanh FNN fθ satisﬁes 4(cid:98)d1/2(cid:99)(cid:98)d2/(2dy)(cid:99) ≥ N; or
• a 3-layer ReLU-like FNN fθ satisﬁes 4(cid:98)d1/4(cid:99)(cid:98)d2/(4dy)(cid:99) ≥ N 

then there exists a parameter θ such that yi = fθ(xi) for all i ∈ [N ].
Theorem 3.1 shows that if d1d2 = Ω(N dy) then we can memorize arbitrary datasets; this means

that Ω((cid:112)N dy) hidden nodes are sufﬁcient for memorization  in contrary to Ω(N dy) requirements

√
of recent results. By adding one more hidden layer  the next theorem shows that we can perfectly
memorize any classiﬁcation dataset using Ω(
Proposition 3.2. Consider any dataset {(xi  yi)}N
yi ∈ {0  1}dy is the one-hot encoding of dy classes. Suppose one of the following holds:
• a 4-layer hard-tanh FNN fθ satisﬁes 4(cid:98)d1/2(cid:99)(cid:98)d2/2(cid:99) ≥ N  and d3 ≥ 2dy; or
• a 4-layer ReLU-like FNN fθ satisﬁes 4(cid:98)d1/4(cid:99)(cid:98)d2/4(cid:99) ≥ N  and d3 ≥ 4dy.

i=1 that satisﬁes Assumption 3.1. Assume that

N + dy) hidden nodes.

Then  there exists a parameter θ such that yi = fθ(xi) for all i ∈ [N ].
Notice that for scalar regression (dy = 1)  Theorem 3.1 proves a lower bound on memorization
capacity of 3-layer neural networks: Ω(d1d2). The next theorem shows that this bound is in fact tight.

4

Theorem 3.3. Consider FNNs with dy = 1 and piecewise linear activation σ with p pieces. If

• a 2-layer FNN fθ satisﬁes (p − 1)d1 + 2 < N; or
• a 3-layer FNN fθ satisﬁes p(p − 1)d1d2 + (p − 1)d2 + 2 < N 

then there exists a dataset {(xi  yi)}N
i ∈ [N ] such that yi (cid:54)= fθ(xi).
Theorems 3.1 and 3.3 together show tight lower and upper bounds Θ(d1d2) on memorization capacity
of 3-layer FNNs  which differ only in constant factors. Theorem 3.3 and the existing result on 2-layer
FNNs [53  Theorem 1] also show that the memorization capacity of 2-layer FNNs is Θ(d1).

i=1 satisfying Assumption 3.1 such that for all θ  there exists

Proof ideas. The proof of Theorem 3.1 is based on an intricate construction of parameters. Roughly
speaking  we construct parameters that make each data point have its unique activation pattern in
the hidden layers; more details are in Appendix B. The proof of Proposition 3.2 is largely based on
Theorem 3.1. By assigning each class j a unique real number ρj (which is similar to the trick in
Hardt and Ma [20])  we modify the dataset into a 1-D regression dataset; we then ﬁt this dataset using
the techniques in Theorem 3.1  and use the extra layer to recover the one-hot representation of the
original yi. Please see Appendix C for the full proof. The main proof idea of Theorem 3.3 is based
on counting the number of “pieces” in the network output fθ(x) (as a function of x)  inspired by
Telgarsky [44]. For the proof  please see Appendix D.

√

4(cid:112)N dy to 4

3.2 Discussion
Depth-width tradeoffs for ﬁnite samples. Theorem 3.1 shows that if the two ReLU hidden layers

satisfy d1 = d2 = 2(cid:112)N dy  then the network can ﬁt a given dataset perfectly. Proposition 3.2 is an

√
improvement for classiﬁcation  which shows that a 4-layer ReLU FNN can memorize any dy-class
classiﬁcation data if d1 = d2 = 2
As in other expressivity results  our results show that there are depth-width tradeoffs in the ﬁnite
sample setting. For ReLU FNNs it is known that one hidden layer with N nodes can memorize
√
any scalar regression (dy = 1) dataset with N points [53]. By adding a hidden layer  the hidden
node requirement is reduced to 4
N ) hidden nodes are
necessary and sufﬁcient. Ability to memorize N data points with N nodes is perhaps not surprising 
because weights of each hidden node can be tuned to memorize a single data point. In contrast  the
√
fact that width-2
N networks can memorize is far from obvious; each hidden node must handle

√
N  and Theorem 3.3 also shows that Θ(

N and d3 = 4dy.

N /2 data points on average  thus a more elaborate construction is required.

√

For dy-class classiﬁcation  by adding one more hidden layer  the requirement is improved from
N + 4dy nodes. This again highlights the power of depth in expressive power.
Proposition 3.2 tells us that we can ﬁt ImageNet1 (N ≈ 106  dy = 103) with three ReLU hidden
layers  using only 2k-2k-4k nodes. This “sufﬁcient” size for memorization is surprisingly smaller
(disregarding optimization aspects) than practical networks.
Implications for ERM.
It is widely observed in experiments that deep neural networks can achieve
zero empirical risk  but a concrete understanding of this phenomenon is still elusive. It is known that
all local minima are global minima for empirical risk of linear neural networks [25  27  51  52  56] 
but this property fails to extend to nonlinear neural networks [39  52]. This suggests that studying
the gap between local minima and global minima could provide explanations for the success of deep
neural networks. In order to study the gap  however  we have to know the risk value attained by global
minima  which is already non-trivial even for shallow neural networks. In this regard  our theorems
provide theoretical guarantees that even a shallow and narrow network can have zero empirical risk
at global minima  regardless of data and loss functions—e.g.  in a regression setting  for a 3-layer

ReLU FNN with d1 = d2 = 2(cid:112)N dy there exists a global minimum that has zero empirical risk.

The number of edges. We note that our results do not contradict the common “insight” that at least
N edges are required to memorize N data points. Our “small” network means a small number of
hidden nodes  and it still has more than N edges. The existing result [53] requires (dx + 2)N edges 
while our construction for ReLU requires 4N + (2dx + 6)

N + 1 edges  which is much fewer.

√

1after omitting the inconsistently labeled items

5

Relevant work on sigmoid. Huang [22] proves that a 2-hidden-layer sigmoid FNNs with d1 =
N/K + 2K and d2 = K  where K is a positive integer  can approximate N arbitrary distinct data
points. The author ﬁrst partitions N data points into K groups of size N/K each. Then  from the
fact that the sigmoid function is strictly increasing and non-polynomial  it is shown that if the weights
between input and ﬁrst hidden layer is sampled randomly  then the output matrix of ﬁrst hidden
layer for each group is full rank with probability one. This is not the case for ReLU or hard-tanh 
because they have “ﬂat” regions in which rank could be lost. In addition  Huang [22] requires extra
2K hidden nodes in d1 that serve as “ﬁlters” which let only certain groups of data points pass through.
Our construction is not an extension of this result because we take a different strategy (Appendix B);
we carefully choose parameters (instead of sampling) that achieve memorization with d1 = N/K
and d2 = K (in hard-tanh case) without the need of extra 2K nodes  which enjoys a smaller width
requirement and allows for more ﬂexibility in the architecture. Moreover  we provide a converse
result (Theorem 3.3) showing that our construction is rate-optimal in the number of hidden nodes.

3.3 Extension to deeper and/or narrower networks

√

What if the network is deeper than three layers and/or narrower than
N? Our next theorem shows
that universal ﬁnite sample expressivity is not limited to 3-layer neural networks  and still achievable
by exploiting depth even for narrower networks.
Proposition 3.4. Consider any dataset {(xi  yi)}N
i=1 that satisﬁes Assumption 3.1. For an L-layer
FNN with hard-tanh activation (σH)  assume that there exist indices l1  . . .   lm ∈ [L − 2] that satisfy

(cid:106) dlj −rj

• lj + 1 < lj+1 for j ∈ [m − 1] 

(cid:107)(cid:106) dlj +1−rj
• 4(cid:80)m
• dk ≥ dy + 1 for all k ∈(cid:83)

j=1

2dy

• dk ≥ dy for all k ∈ [lm + 2 : L − 1] 

2

j∈[m−1][lj + 2 : lj+1 − 1].

(cid:107) ≥ N  where rj = dy1{j > 1} + 1{j < m}  for j ∈ [m] 

where 1{·} is 0-1 indicator function. Then  there exists θ such that yi = fθ(xi) for all i ∈ [N ].

As a special case  note that for L = 3 (hence m = 1)  the conditions boil down to that of Theorem 3.1.
An immediate corollary of this fact is that the same result holds for ReLU(-like) networks with twice
the width. Moreover  using the same proof technique as Proposition 3.2  this theorem can also be
improved for classiﬁcation datasets  by inserting one additional hidden layer between layer lm + 1
and the output layer. Due to space limits  we defer the statement of these corollaries to Appendix A.
The proof of Proposition 3.4 is in Appendix E. We use Theorem 3.1 as a building block and construct a
network (see Figure 2 in appendix) that ﬁts a subset of dataset at each pair of hidden layers lj–(lj + 1).
If any two adjacent hidden layers satisfy dldl+1 = Ω(N dy)  this network can ﬁt N data points
(m = 1)  even when all the other hidden layers have only one hidden node. Even with networks

narrower than(cid:112)N dy (thus m > 1)  we can still achieve universal ﬁnite sample expressivity as long

as there are Ω(N dy) edges between disjoint pairs of adjacent layers. However  we have the “cost”
rj in the width of hidden layers; this is because we ﬁt subsets of the dataset using multiple pairs of
layers. To do this  we need rj extra nodes to propagate input and output information to the subsequent
layers. For more details  please refer to the proof.

Proposition 3.4 gives a lower bound Ω((cid:80)L−2

l=1 dldl+1) on memorization capacity for L-layer networks.
For ﬁxed input/output dimensions  this is indeed Ω(W )  where W is the number of edges in the
network. On the other hand  Bartlett et al. [4] showed an upper bound O(W L log W ) on VC
dimension  which is also an upper bound on memorization capacity. Thus  for any ﬁxed L  our lower
bound is nearly tight. We conjecture that  as we have proved in 2- and 3-layer cases  the memorization
capacity is Θ(W )  independent of L; we leave closing this gap for future work.
For sigmoid FNNs  Yamasaki [48] claimed that a scalar regression dataset can be memorized if
dx(cid:100) d1
2 − 1(cid:101) ≥ N. However  this claim was made under the
stronger assumption of data lying in general position (see Assumption 4.1). Unfortunately  Yamasaki
[48] does not provide a full proof of their claim  making it impossible to validate veracity of their
construction (and we could not ﬁnd their extended manuscript elsewhere).

2 − 1(cid:101) + ··· + (cid:98) dL−2

2 (cid:99)(cid:100) dL−1

2 (cid:101) + (cid:98) d1

2 (cid:99)(cid:100) d2

6

4 Classiﬁcation under the general position assumption
This section presents some results specialized in multi-class classiﬁcation task under a slightly stronger
assumption  namely the general position assumption. Since we are only considering classiﬁcation in
this section  we also assume that yi ∈ {0  1}dy is the one-hot encoding of dy classes.
Assumption 4.1. For a ﬁnite dataset {(xi  yi)}N
same afﬁne hyperplane. In other words  the data point xi’s are in general position.
We consider residual networks (ResNets)  deﬁned by the following architecture:

i=1  assume that no dx + 1 data points lie on the

h0(x) = x 
hl(x) = hl−1(x) + V lσ(U lhl−1(x) + bl) + cl  l ∈ [L − 1] 
gθ(x) = V Lσ(U LhL−1(x) + bL) + cL 

dx

which is similar to the previous work by Hardt and Ma [20]  except for extra bias parameters cl. In
this model  we denote the number hidden nodes in the l-th residual layer as dl; e.g.  U l ∈ Rdl×dx.
We now present a theorem showing that any dataset can be memorized with small ResNets.
Theorem 4.1. Consider any dataset {(xi  yi)}N
dx ≥ dy. Suppose one of the following holds:

i=1 that satisﬁes Assumption 4.1. Assume also that

• a hard-tanh ResNet gθ satisﬁes(cid:80)L−1
• a ReLU-like ResNet gθ satisﬁes(cid:80)L−1

+ 2dy and dL ≥ dy; or
+ 4dy and dL ≥ 2dy.

hidden nodes (i.e. (cid:80)L−1

l=1 dl ≥ 2N
l=1 dl ≥ 4N
Then  there exists θ such that yi = gθ(xi) for all i ∈ [N ].
The previous work by Hardt and Ma [20] proves universal ﬁnite sample expressivity using N + dy
l=1 dl ≥ N and dL ≥ dy) for ReLU activation  under the assumption that xi’s
are distinct unit vectors. Note that neither this assumption nor Assumption 4.1 implies the other;
however  our assumption is quite mild in the sense that for any given dataset  adding small random
Gaussian noise to xi’s makes the dataset satisfy the assumption  with probability 1.
The main idea for the proof is that under the general position assumption  for any choice of dx points
there exists an afﬁne hyperplane that contains only these dx points. Each hidden node can choose dx
data points and “push” them to the right direction  making perfect classiﬁcation possible. We defer
the details to Appendix F.1. Using the same technique  we can also prove an improved result for
2-layer (1-hidden-layer) FNNs. The proof of the following corollary can be found in Appendix F.2.
Corollary 4.2. Consider any dataset {(xi  yi)}N
i=1 that satisﬁes Assumption 4.1. Suppose one of the
following holds:

dx

dx

+ 2dy; or

• a 2-layer hard-tanh FNN fθ satisﬁes d1 ≥ 2N
• a 2-layer ReLU-like FNN fθ satisﬁes d1 ≥ 4N
Then  there exists θ such that yi = fθ(xi) for all i ∈ [N ].
Our results show that under the general position assumption  perfect memorization is possible with
only Ω(N/dx + dy) hidden nodes rather than N  in both ResNets and 2-layer FNNs. Considering that
dx is typically in the order of hundreds or thousands  our results reduce the hidden node requirements
down to more realistic network sizes. For example  consider CIFAR-10 dataset: N = 50  000 
dx = 3  072  and dy = 10. Previous results require at least 50k ReLUs to memorize this dataset 
while our results require 126 ReLUs for ResNets and 106 ReLUs for 2-layer FNNs.

+ 4dy.

dx

5 Trajectory of SGD near memorizing global minima
In this section  we study the behavior of without-replacement SGD near memorizing global minima.
We restrict dy = 1 for simplicity. We use the same notation as deﬁned in Section 2  and introduce
here some additional deﬁnitions. We assume that each activation function σ is piecewise linear with
at least two pieces (e.g.  ReLU or hard-tanh). Throughout this section  we slightly abuse the notation
θ to denote the concatenation of vectorizations of all the parameters (W l  bl)L

l=1.

7

We are interested in minimizing the empirical risk R(θ)  deﬁned as the following:

(cid:88)N

R(θ) := 1
N

(cid:96)(fθ(xi); yi) 

i=1

∗ is a memorizing global minimum of R(·) if (cid:96)(cid:48)(fθ∗ (xi); yi) = 0  ∀i ∈ [N ].
∗. Also 

where (cid:96)(z; y) : R (cid:55)→ R is the loss function parametrized by y. We assume the following:
Assumption 5.1. The loss function (cid:96)(z; y) is a strictly convex and three times differentiable function
of z. Also  for any y  there exists z ∈ R such that z is a global minimum of (cid:96)(z; y).
Assumption 5.1 on (cid:96) is satisﬁed by standard losses such as squared error loss. Note that logistic loss
does not satisfy Assumption 5.1 because the global minimum is not attained by any ﬁnite z.
Given the assumption on (cid:96)  we now formally deﬁne the memorizing global minimum.
Deﬁnition 5.1. A point θ
By convexity  (cid:96)(cid:48)(fθ∗ (xi); yi) = 0 for all i implies that R(θ) is (globally) minimized at θ
existence of a memorizing global minimum of R implies that all global minima are memorizing.
Although (cid:96) is a differentiable function of z  the empirical risk R(θ) is not necessarily differentiable
in θ because we are using piecewise linear activations. In this paper  we only consider differentiable
points of R(·); since nondifferentiable points lie in a set of measure zero and SGD never reaches
such points in reality  this is a reasonable assumption.
We consider minimizing the empirical risk R(θ) using without-replacement mini-batch SGD. We
use B as mini-batch size  so it takes E := N/B steps to go over N data points in the dataset.
For simplicity we assume that N is a multiple of B. At iteration t = kE  it partitions the dataset
at random  into E sets of cardinality B: B(kE)  B(kE+1)  . . .   B(kE+E−1)  and uses these sets to
estimate gradients. After each epoch (one pass through the dataset)  the data is “reshufﬂed” and
a new partition is used. Without-replacement SGD is known to be more difﬁcult to analyze than
with-replacement SGD (see [19  40] and references therein)  although more widely used in practice.
More concretely  our SGD algorithm uses the update rule θ(t+1) ← θ(t) − ηg(t)  where we ﬁx the
step size η to be a constant throughout the entire run and g(t) is the gradient estimate

(cid:88)

g(t) = 1
B

(cid:96)(cid:48)(fθ(t)(xi); yi)∇θfθ(t)(xi).

i∈B(t)

For each k (cid:83)kE+E−1

t=kE

B(t) = [N ]. Note also that if B = N  we recover vanilla gradient descent.
∗. We deﬁne vectors νi := ∇θfθ∗ (xi) for all
Now consider a memorizing global minimum θ
i ∈ [N ]. We can then express any iterate θ(t) of SGD as θ(t) = θ
+ ξ(t)  and then further
decompose the “perturbation” ξ(t) as the sum of two orthogonal components ξ(t)(cid:107) and ξ(t)⊥   where
ξ(t)(cid:107) ∈ span({νi}N
i=1)⊥. Also  for a vector v  let (cid:107)v(cid:107) denote its (cid:96)2 norm.
5.1 Main results and discussion
We now state the main theorem of the section. For the proof  please refer to Appendix G.
Theorem 5.1. Suppose a memorizing global minimum θ
ferentiable at θ
initialization θ(0) satisﬁes (cid:107)ξ(0)(cid:107) ≤ ρ  then

∗ of R(θ) is given  and that R(·) is dif-
∗. Then  there exist positive constants ρ  γ  λ  and τ satisfying the following: if

i=1) and ξ(t)⊥ ∈ span({νi}N

∗

R(θ(0)) − R(θ

∗

) = O((cid:107)ξ(0)(cid:107)2) 

and SGD with step size η < γ satisﬁes

(cid:107)ξ(kE+E)

(cid:107)

(cid:107) ≤ (1 − ηλ)(cid:107)ξ(kE)

(cid:107)

(cid:107)  and (cid:107)ξ(kE+E)(cid:107) ≤ (cid:107)ξ(kE)(cid:107) + ηλ(cid:107)ξ(kE)

(cid:107)

(cid:107) 

as long as (cid:107)ξ(t)(cid:107) (cid:107) ≥ τ(cid:107)ξ(t)(cid:107)2 holds for all t ∈ [kE  kE + E − 1]. As a consequence  at the ﬁrst
iterate t∗ ≥ 0 where the condition (cid:107)ξ(t)(cid:107) (cid:107) ≥ τ(cid:107)ξ(t)(cid:107)2 is violated  we have

(cid:107)ξ(t∗)(cid:107) ≤ 2(cid:107)ξ(0)(cid:107)  and R(θ(t∗)) − R(θ

∗

) ≤ C(cid:107)ξ(0)(cid:107)4 

for some positive constant C.

8

∗ to θ

∗

i=1 (cid:96)(cid:48)(cid:48)(fθ∗ (xi); yi)νiνT
+ ξ.

eigenvalues of H =(cid:80)N

The full description of constants ρ  γ  λ  τ  and C can be found in Appendix G. They are dependent
on a number of terms  such as N  B  the Taylor expansions of loss (cid:96)(fθ∗ (xi); yi) and network
∗  maximum and minimum strictly positive
output fθ∗ (xi) around the memorizing global minimum θ
i . The constant ρ must be small enough so that as long
as (cid:107)ξ(cid:107) ≤ ρ  the slopes of piecewise linear activation functions evaluated for data points xi do not
change from θ
Notice that for small perturbation ξ  the Taylor expansion of network output fθ∗ (xi) is written as
i ξ(cid:107) + O((cid:107)ξ(cid:107)2)  because νi ⊥ ξ⊥ by deﬁnition. From this perspective 
fθ∗+ξ(xi) = fθ∗ (xi) + νT
Theorem 5.1 shows that if initialized near global minima  the component in the perturbation ξ that
induces ﬁrst-order perturbation of fθ∗ (xi)  namely ξ(cid:107)  decays exponentially fast until SGD ﬁnds a
nearby point that has much smaller risk (O((cid:107)ξ(0)(cid:107)4)) than the initialization (O((cid:107)ξ(0)(cid:107)2)). Note also
that our result is completely deterministic  and independent of the partitions of the dataset taken by
the algorithm; the theorem holds true even if the algorithm is not “stochastic” and just cycles through
the dataset in a ﬁxed order without reshufﬂing.
We would like to emphasize that Theorem 5.1 holds for any memorizing global minima of FNNs  not
only for the ones explicitly constructed in Sections 3 and 4. Moreover  the result is not dependent on
the network size or data distribution. As long as the global minimum memorizes the data  our theorem
holds without any depth/width requirements or distributional assumptions  which is a noteworthy
difference that makes our result hold in more realistic settings than existing ones.
The remaining question is: what happens after t∗? Unfortunately  if (cid:107)ξ(t)(cid:107) (cid:107) ≤ τ(cid:107)ξ(t)(cid:107)2  we cannot
ensure exponential decay of (cid:107)ξ(t)(cid:107) (cid:107)  especially if it is small. Without exponential decay  one cannot
show an upper bound on (cid:107)ξ(t)(cid:107) either. This means that after t∗  SGD may even diverge or oscillate
near global minimum. Fully understanding the behavior of SGD after t∗ seems to be a more difﬁcult
problem  which we leave for future work.

6 Conclusion and future work
√
In this paper  we show that fully-connected neural networks (FNNs) with Ω(
N ) nodes are expressive
√
enough to perfectly memorize N arbitrary data points  which is a signiﬁcant improvement over the
recent results in the literature. We also prove the converse stating that at least Θ(
N ) nodes are
necessary; these two results together provide tight bounds on memorization capacity of neural
networks. We further extend our expressivity results to deeper and/or narrower networks  providing
a nearly tight bound on memorization capacity for these networks as well. Under an assumption
that data points are in general position  we prove that classiﬁcation datasets can be memorized
with Ω(N/dx + dy) hidden nodes in deep residual networks and one-hidden-layer FNNs  reducing
the existing requirement of Ω(N ). Finally  we study the dynamics of stochastic gradient descent
(SGD) on empirical risk  and showed that if SGD is initialized near a global minimum that perfectly
memorizes the data  it quickly ﬁnds a nearby point with small empirical risk. Several future topics
are open; e.g.  1) tight bounds on memorization capacity for deep FNNs and other architectures  2)
deeper understanding of SGD dynamics in the presence of memorizing global minima.

Acknowledgments

We thank Alexander Rakhlin for helpful discussion. All the authors acknowledge support from
DARPA Lagrange. Chulhee Yun also thanks Korea Foundation for Advanced Studies for their
support. Suvrit Sra also acknowledges support from an NSF-CAREER grant and an Amazon
Research Award.

References
[1] Z. Allen-Zhu  Y. Li  and Z. Song. A convergence theory for deep learning via over-

parameterization. arXiv preprint arXiv:1811.03962  2018.

[2] D. Arpit  S. Jastrz˛ebski  N. Ballas  D. Krueger  E. Bengio  M. S. Kanwal  T. Maharaj  A. Fischer 
A. Courville  Y. Bengio  et al. A closer look at memorization in deep networks. In International
Conference on Machine Learning  pages 233–242  2017.

9

[3] P. L. Bartlett  V. Maiorov  and R. Meir. Almost linear VC dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems  pages 190–196 
1999.

[4] P. L. Bartlett  N. Harvey  C. Liaw  and A. Mehrabian. Nearly-tight VC-dimension and pseudodi-
mension bounds for piecewise linear neural networks. Journal of Machine Learning Research 
20(63):1–17  2019. URL http://jmlr.org/papers/v20/17-612.html.

[5] P. L. Bartlett  P. M. Long  G. Lugosi  and A. Tsigler. Benign overﬁtting in linear regression.

arXiv preprint arXiv:1906.11300  2019.

[6] E. B. Baum. On the capabilities of multilayer perceptrons. Journal of complexity  4(3):193–215 

1988.

[7] M. Belkin  D. Hsu  S. Ma  and S. Mandal. Reconciling modern machine learning and the

bias-variance trade-off. arXiv preprint arXiv:1812.11118  2018.

[8] M. Belkin  A. Rakhlin  and A. B. Tsybakov. Does data interpolation contradict statistical

optimality? arXiv preprint arXiv:1806.09471  2018.

[9] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a ConvNet with Gaussian

inputs. In International Conference on Machine Learning  pages 605–614  2017.

[10] A. Brutzkus  A. Globerson  E. Malach  and S. Shalev-Shwartz. SGD learns over-parameterized
networks that provably generalize on linearly separable data. In International Conference on
Learning Representations  2018.

[11] T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers  (3):326–334 
1965.

[12] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control 

signals and systems  2(4):303–314  1989.

[13] O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In Advances in Neural

Information Processing Systems  pages 666–674  2011.

[14] S. S. Du  J. D. Lee  Y. Tian  B. Poczos  and A. Singh. Gradient descent learns one-hidden-layer

CNN: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779  2017.

[15] S. S. Du  J. D. Lee  H. Li  L. Wang  and X. Zhai. Gradient descent ﬁnds global minima of deep

neural networks. arXiv preprint arXiv:1811.03804  2018.

[16] S. S. Du  X. Zhai  B. Poczos  and A. Singh. Gradient descent provably optimizes over-

parameterized neural networks. arXiv preprint arXiv:1810.02054  2018.

[17] R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference

on Learning Theory  pages 907–940  2016.

[18] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks.

Neural networks  2(3):183–192  1989.

[19] J. Z. HaoChen and S. Sra. Random shufﬂing beats SGD after ﬁnite epochs. arXiv preprint

arXiv:1806.10077  2018.

[20] M. Hardt and T. Ma. Identity matters in deep learning. In International Conference on Learning

Representations  2017.

[21] K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal

approximators. Neural networks  2(5):359–366  1989.

[22] G.-B. Huang. Learning capability and storage capacity of two-hidden-layer feedforward

networks. IEEE Transactions on Neural Networks  14(2):274–281  2003.

10

[23] G.-B. Huang and H. A. Babri. Upper bounds on the number of hidden neurons in feedforward
networks with arbitrary bounded nonlinear activation functions. IEEE Transactions on Neural
Networks  9(1):224–229  1998.

[24] S.-C. Huang and Y.-F. Huang. Bounds on the number of hidden neurons in multilayer percep-

trons. IEEE transactions on neural networks  2(1):47–55  1991.

[25] K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information

Processing Systems  pages 586–594  2016.

[26] A. Kowalczyk. Estimates of storage capacity of multilayer perceptron with threshold logic

hidden units. Neural networks  10(8):1417–1433  1997.

[27] T. Laurent and J. Brecht. Deep linear networks with arbitrary loss: All local minima are global.

In International Conference on Machine Learning  pages 2908–2913  2018.

[28] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent
on structured data. In Advances in Neural Information Processing Systems  pages 8168–8177 
2018.

[29] Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with ReLU activation.

In Advances in Neural Information Processing Systems  pages 597–607  2017.

[30] S. Liang and R. Srikant. Why deep neural networks for function approximation? In International

Conference on Learning Representations  2017.

[31] T. Liang and A. Rakhlin. Just Interpolate: Kernel “Ridgeless” Regression Can Generalize.

arXiv preprint arXiv:1808.00387  2018.

[32] T. Liang  A. Rakhlin  and X. Zhai. On the risk of minimum-norm interpolants and restricted

lower isometry of kernels. arXiv preprint arXiv:1908.10292  2019.

[33] Z. Lu  H. Pu  F. Wang  Z. Hu  and L. Wang. The expressive power of neural networks: A view
from the width. In Advances in Neural Information Processing Systems  pages 6231–6239 
2017.

[34] S. Mei and A. Montanari. The generalization error of random features regression: Precise

asymptotics and double descent curve. arXiv preprint arXiv:1908.05355  2019.

[35] Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep CNNs. arXiv preprint

arXiv:1710.10928  2017.

[36] N. J. Nilsson. Learning machines. 1965.

[37] D. Rolnick and M. Tegmark. The power of deeper networks for expressing natural functions. In

International Conference on Learning Representations  2018.

[38] I. Safran and O. Shamir. Depth-width tradeoffs in approximating natural functions with neural

networks. In International Conference on Machine Learning  pages 2979–2987  2017.

[39] I. Safran and O. Shamir. Spurious local minima are common in two-layer ReLU neural networks.

arXiv preprint arXiv:1712.08968  2017.

[40] O. Shamir. Without-replacement sampling for stochastic gradient methods. In Advances in

neural information processing systems  pages 46–54  2016.

[41] M. Soltanolkotabi. Learning ReLUs via gradient descent. In Advances in Neural Information

Processing Systems  pages 2007–2017  2017.

[42] E. D. Sontag. Shattering all sets of ‘k’ points in “general position” requires (k—1)/2 parameters.

Neural Computation  9(2):337–348  1997.

[43] D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

11

[44] M. Telgarsky. Representation beneﬁts of deep feedforward networks.

arXiv:1509.08101  2015.

arXiv preprint

[45] M. Telgarsky. Beneﬁts of depth in neural networks. In Conference on Learning Theory  pages

1517–1539  2016.

[46] Y. Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning  pages 3404–3413  2017.

[47] G. Wang  G. B. Giannakis  and J. Chen. Learning ReLU networks on linearly separable data:

Algorithm  optimality  and generalization. arXiv preprint arXiv:1808.04685  2018.

[48] M. Yamasaki. The lower bound of the capacity for a neural network with multiple hidden layers.

In ICANN’93  pages 546–549. Springer  1993.

[49] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks  94:

103–114  2017.

[50] D. Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks.

arXiv preprint arXiv:1802.03620  2018.

[51] C. Yun  S. Sra  and A. Jadbabaie. Global optimality conditions for deep neural networks. In

International Conference on Learning Representations  2018.

[52] C. Yun  S. Sra  and A. Jadbabaie. Small nonlinearities in activation functions create bad local
minima in neural networks. In International Conference on Learning Representations  2019.

[53] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires
rethinking generalization. In International Conference on Learning Representations (ICLR) 
2017.

[54] X. Zhang  Y. Yu  L. Wang  and Q. Gu. Learning one-hidden-layer ReLU networks via gradient

descent. arXiv preprint arXiv:1806.07808  2018.

[55] K. Zhong  Z. Song  P. Jain  P. L. Bartlett  and I. S. Dhillon. Recovery guarantees for one-hidden-
layer neural networks. In International Conference on Machine Learning  pages 4140–4149 
2017.

[56] Y. Zhou and Y. Liang. Critical points of neural networks: Analytical forms and landscape

properties. In International Conference on Learning Representations  2018.

[57] Y. Zhou  J. Yang  H. Zhang  Y. Liang  and V. Tarokh. SGD converges to global minimum in
deep learning via star-convex path. In International Conference on Learning Representations 
2019.

[58] D. Zou  Y. Cao  D. Zhou  and Q. Gu. Stochastic gradient descent optimizes over-parameterized

deep ReLU networks. arXiv preprint arXiv:1811.08888  2018.

12

,Chulhee Yun
Suvrit Sra
Ali Jadbabaie