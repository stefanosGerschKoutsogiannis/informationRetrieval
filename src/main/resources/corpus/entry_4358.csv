2011,Practical Variational Inference for Neural Networks,Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently  minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.,Practical Variational Inference for Neural Networks

Alex Graves

Department of Computer Science

University of Toronto  Canada
graves@cs.toronto.edu

Abstract

Variational methods have been previously explored as a tractable approximation
to Bayesian inference for neural networks. However the approaches proposed so
far have only been applicable to a few simple network architectures. This paper
introduces an easy-to-implement stochastic variational method (or equivalently 
minimum description length loss function) that can be applied to most neural net-
works. Along the way it revisits several common regularisers from a variational
perspective. It also provides a simple pruning heuristic that can both drastically re-
duce the number of network weights and lead to improved generalisation. Exper-
imental results are provided for a hierarchical multidimensional recurrent neural
network applied to the TIMIT speech corpus.

1

Introduction

In the eighteen years since variational inference was ﬁrst proposed for neural networks [10] it has not
seen widespread use. We believe this is largely due to the difﬁculty of deriving analytical solutions
to the required integrals over the variational posteriors. Such solutions are complicated for even
the simplest network architectures  such as radial basis networks [2] and single layer feedforward
networks with linear outputs [10  1  14]  and are generally unavailable for more complex systems.
The approach taken here is to forget about analytical solutions and search instead for variational
distributions whose expectation values (and derivatives thereof) can be efﬁciently approximated with
numerical integration. While it may seem perverse to replace one intractable integral (over the true
posterior) with another (over the variational posterior)  the point is that the variational posterior is far
easier to draw probable samples from  and correspondingly more amenable to numerical methods.
The result is a stochastic method for variational inference with a diagonal Gaussian posterior that can
be applied to any differentiable log-loss parametric model—which includes most neural networks1
Variational inference can be reformulated as the optimisation of a Minimum Description length
(MDL; [21]) loss function; indeed it was in this form that variational inference was ﬁrst considered
for neural networks. One advantage of the MDL interpretation is that it leads to a clear separation
between prediction accuracy and model complexity  which can help to both analyse and optimise the
network. Another beneﬁt is that recasting inference as optimisation makes it to easier to implement
in existing  gradient-descent-based neural network software.

2 Neural Networks

For the purposes of this paper a neural network is a parametric model that assigns a conditional
probability Pr(D|w) to some dataset D  given a set w = {wi}W
i=1 of real-valued parameters  or
weights. The elements (x  y) of D  each consisting of an input x and a target y  are assumed to be
1An important exception are energy-based models such as restricted Boltzmann machines [24] whose log-

loss is intractable.

1

drawn independently from a joint distribution p(x  y)2. The network loss LN (w D) is deﬁned as
the negative log probability of the data given the weights.

LN (w D) = − ln Pr(D|w) = − (cid:88)

ln Pr(y|x  w)

(1)

(x y)∈D

The logarithm could be taken to any base  but to avoid confusion we will use the natural loga-
rithm ln throughout. We assume that the partial derivatives of LN (w D) with respect to the net-
work weights can be efﬁciently calculated (using  for example  backpropagation or backpropagation
through time [22]).

3 Variational Inference

Performing Bayesian inference on a neural network requires the posterior distribution of the net-
work weights given the data. If the weights have a prior probability P (w|α) that depends on some
parameters α  the posterior can be written Pr(w|D  α). Unfortunately  for most neural networks
Pr(w|D  α) cannot be calculated analytically  or even efﬁciently sampled from. Variational in-
ference addresses this problem by approximating Pr(w|D  α) with a more tractable distribution
Q(w|β). The approximation is ﬁtted by minimising the variational free energy F with respect to
the parameters β  where

(cid:28)

(cid:20) Pr(D|w)P (w|α)

(cid:21)(cid:29)

F = −

(2)
and for some function g of a random variable x with distribution p(x)  (cid:104)g(cid:105)x∼p denotes the expecta-
tion of g over p. A fully Bayesian approach would infer the prior parameters α from a hyperprior;
however in this paper they are found by simply minimising F with respect to α as well as β.

Q(w|β)

w∼Q(β)

ln

F =(cid:10)LN (w D)(cid:11)

4 Minimum Description Length
F can be reinterpreted as a minimum description length loss function [12] by rearranging Eq. (2)
and substituting in from Eq. (1) to get

w∼Q(β) + DKL(Q(β)||P (α)) 

(3)
where DKL(Q(β)||P (α)) is the Kullback-Leibler divergence between Q(β) and P (α). Shannon’s
source coding theorem [23] tells us that the ﬁrst term on the right hand side of Eq. (3) is a lower
bound on the expected amount of information (measured in nats  due to the use of natural loga-
rithms) required to transmit the targets in D to a receiver who knows the inputs  using the outputs
of a network whose weights are sampled from Q(β). Since this term decreases as the network’s
prediction accuracy increases  we identify it as the error loss LE(β D):

LE(β D) =(cid:10)LN (w D)(cid:11)

w∼Q(β)

(4)

Shannon’s bound can almost be achieved in practice using arithmetic coding [26]. The second term
on the right hand side of Eq. (3) is the expected number of nats required by a receiver who knows
P (α) to pick a sample from Q(β). Since this term measures the cost of ‘describing’ the network
weights to the receiver  we identify it as the complexity loss LC(α  β):

LC(α  β) = DKL(Q(β)||P (α))

(5)
LC(α  β) can be realised with bits-back coding [25  10]. Although originally conceived as a thought
experiment  bits-back coding has been used for an actual compression scheme [5]. Putting the terms
together F can be rephrased as an MDL loss function L(α  β D) that measures the total number of
nats required to transmit the training targets using the network  given α and β:

(6)
The network is then trained on D by minimising L(α  β D) with respect to α and β  just like
an ordinary neural network loss function. One advantage of using a transmission cost as a loss

L(α  β D) = LE(β D) + LC(α  β)

2Unsupervised learning can be treated as a special case where x = ∅

2

function is that we can immediately determine whether the network has compressed the targets past
a reasonable benchmark (such as that given by an off-the-shelf compressor). If it has  we can be
fairly certain that the network is learning underlying patterns in the data and not simply memorising
the training set. We would therefore expect it to generalise well to new data. In practice we have
found that as long as signiﬁcant compression is taking place  decreasing L(α  β D) on the training
set does not increase LE(β D) on the test set  and it is therefore unnecessary to sacriﬁce any training
data for early stopping.
Two transmission costs were ignored in the above discussion. One is the cost of transmitting the
model with w unspeciﬁed (for example software that implements the network architecture  the train-
ing algorithm etc.). The other is the cost of transmitting the prior. If either of these are used to encode
a signiﬁcant amount of information about D  the MDL principle will break down and the generali-
sation guarantees that come with compression will be lost. The easiest way to prevent this is to keep
both costs very small compared to D. In particular the prior should not contain too many parameters.

5 Choice of Distributions
We now derive the form of LE(β D) and LC(α  β) for various choices of Q(β) and P (α). We also
derive the gradients of LE(β D) and LC(α  β) with respect to β and the optimal values of α given
β. All continuous distributions are implicitly assumed to be quantised at some very ﬁne resolution 
i=1 qi(βi)  meaning that

and we will limit ourselves to diagonal posteriors of the form Q(β) =(cid:81)W
LC(α  β) =(cid:80)W

i=1 DKL(qi(βi)||P (α)).

5.1 Delta Posterior

Perhaps the simplest nontrivial distribution for Q(β) is a delta distribution that assigns probability
1 to a particular set of weights w and 0 to all other weights. In this case β = w  LE(β D) =
LN (w D) and LC(α  β) = LC(α  w) = −logP (w|α) + C. where C is a constant that depends
only on the discretisation of Q(β). Although C has no effect on the gradient used for training  it is
usually large enough to ensure that the network cannot compress the data using the coding scheme
described in the previous section3. If the prior is uniform  and all realisable weight values are equally
likely then LC(α  β) is a constant and we recover ordinary maximum likelihood training.

If the prior is a Laplace distribution then α = {µ  b}  P (w|α) =(cid:81)W

(cid:16)−|wi−µ|

(cid:17)

1
2b exp

i=1

b

LC(α  w) = W ln 2b +

1
b

|wi − µ| + C =⇒ ∂LC(α  w)

∂wi

sgn(wi − µ)

b

=

and

(7)

W(cid:88)

i=1

(cid:80)W
i=1 |wi − ˆµ|.

If µ = 0 and b is ﬁxed  this is equivalent to ordinary L1 regularisation. However we can instead
determine the optimal prior parameters ˆα for w as follows: ˆµ = µ1/2(w) (the median weight value)
and ˆb = 1
W

If the prior is Gaussian then α = {µ  σ2}  P (w|α) =(cid:81)W

(cid:16)− (wi−µ)2

(cid:17)

1√
2πσ2 exp

i=1

2σ2

With µ = 0 and σ2 ﬁxed this is equivalent to L2 regularisation (also known as weight decay for
neural networks). The optimal ˆα given w are ˆµ = 1
W

W

(wi − µ)2 + C =⇒ ∂LC(α  w)
(cid:80)W
i=1 (wi − ˆµ)2

i=1 wi and ˆσ2 = 1

(cid:80)W

∂wi

=

√
LC(α  w) = W ln(

2πσ2) +

1
2σ2

W(cid:88)

i=1

and
wi − µ
σ2

(8)

5.2 Gaussian Posterior

A more interesting distribution for Q(β) is a diagonal Gaussian. In this case each weight requires a
separate mean and variance  so β = {µ  σ2} with the mean vector µ and variance vector σ2 both
3The ﬂoating point resolution of the computer architecture used to train the network could in principle be
used to upper-bound the discretisation constant  and hence the compression; but in practice the bound would
be prohibitively high.

3

the same size as w. For a general network architecture we cannot compute either LE(β D) or its
derivatives exactly  so we resort to sampling. Applying Monte-Carlo integration to Eq. (4) gives

LE(β D) ≈ 1
S

LN (wk D)

(9)

S(cid:88)

k=1

with wk drawn independently from Q(β). A combination of the Gaussian characteristic function
and integration by parts can be used to derive the following identities for the derivatives of multi-
variate Gaussian expectations [18]:

∇µ (cid:104)V (a)(cid:105)a∼N = (cid:104)∇aV (a)(cid:105)a∼N  

(10)
where N is a multivariate Gaussian with mean vector µ and covariance matrix Σ  and V is an
arbitrary function of a. Differentiating Eq. (4) and applying these identities yields
∂LE(β D)

∂LN (wk D)

∇Σ (cid:104)V (a)(cid:105)a∼N =

(cid:104)∇a∇aV (a)(cid:105)a∼N

1
2

(cid:28) ∂LN (w D)
(cid:29)
(cid:28) ∂2LN (w D)

∂wi

(cid:29)

≈ 1
S

w∼Q(β)

∂w2
i

≈ 1
2

w∼Q(β)

S(cid:88)
(cid:42)(cid:20) ∂LN (w D)

∂wi

k=1

(cid:21)2(cid:43)

∂µi

∂LE(β D)

∂σ2
i

=

=

1
2

∂wi

w∼Q(β)

k=1

∂wi

≈ 1
2S

(11)

(cid:20) ∂LN (wk D)

(cid:21)2

S(cid:88)

(12)
where the ﬁrst approximation in Eq. (12) comes from substituting the negative diagonal of the em-
pirical Fisher information matrix for the diagonal of the Hessian. This approximation is exact if the
conditional distribution Pr(D|w) matches the empirical distribution of D (i.e. if the network per-
fectly models the data); we would therefore expect it to improve as LE(β D) decreases. For simple
networks whose second derivatives can be calculated efﬁciently the approximation is unnecessary
and the diagonal Hessian can be sampled instead.
A simpliﬁcation of the above distribution is to consider the variances of Q(β) ﬁxed and optimise
only the means. Then the sampling used to calculate the derivatives in Eq. (11) is equivalent to
adding zero-mean  ﬁxed-variance Gaussian noise to the network weights during training. In par-
ticular  if the prior P (α) is uniform and a single weight sample is taken for each element of D 
then minimising L(α  β D) is identical to minimising LN (w D) with weight noise or synaptic
noise [13]. Note that the quantisation of the uniform prior adds a large constant to LC(α  β)  mak-
ing it unfeasible to compress the data with our MDL coding scheme; in practice early stopping is
required to prevent overﬁtting when training with weight noise.
If the prior is Gaussian then α = {µ  σ2} and

LC(α  β) =

ln
µi − µ
σ2
The optimal prior parameters ˆα given β are

=⇒ ∂LC(α  β)

∂µi

i=1

=

 

W(cid:88)

i=1

ˆµ =

1
W

µi 

ˆσ2 =

1
W

W(cid:88)

σ
σi

+

1
2σ2

∂LC(α  β)

(cid:104)

(µi − µ)2 + σ2

i − σ2(cid:105)
(cid:20) 1
(cid:21)
σ2 − 1
i + (µi − ˆµ)2(cid:105)

σ2
i

1
2

=

σ2

∂σ2
i

W(cid:88)

(cid:104)

i=1

(13)

(14)

(15)

If a Gaussian prior is used with the ﬁxed variance ‘weight noise’ posterior described above  it is still
possible to choose the optimal prior parameters for each β. This requires only a slight modiﬁcation
of standard weight-noise training  with the derivatives on the left of Eq. (14) added to the weight
gradient and α optimised after every weight update. But because the prior is no longer uniform the
network is able to compress the data  making it feasible to dispense with early stopping.
The terms in the sum on the right hand side of Eq. (13) are the complexity costs of individual
network weights. These costs give valuable insight into the internal structure of the network  since
(with a limited budget of bits to spend) the network will assign more bits to more important weights.
Importance can be used  for example  to prune away spurious weights [15] or determine which
inputs are relevant [16].

4

6 Optimisation
If the derivatives of LE(β D) are stochastic  we require an optimiser that can tolerate noisy gradient
estimates. Steepest descent with momentum [19] and RPROP [20] both work well in practice.
Although stochastic derivatives should in principle be estimated using the same weight samples for
the entire dataset  it is in practice much more efﬁcient to pick different weight samples for each
(x  y) ∈ D. If both the prior and posterior are Gaussian this yields

∂L(α  β D)

∂µi

∂L(α  β D)

∂σ2
i

≈ µi − µ
σ2 +
(cid:20) 1
σ2 − 1

≈ 1
2

σ2
i

(cid:88)
(cid:21)

+

(x y)∈D

1
S

S(cid:88)
(cid:88)

k=1

∂LN (wk  x  y)

(cid:20) ∂LN (wk  x  y)

∂wi

S(cid:88)

(cid:21)2

(16)

(17)

1
2S

(x y)∈D

k=1

∂wi

where LN (wk  x  y) = − ln Pr(y|x  w) and a separate set of S weight samples {wk}S
k=1 is drawn
from Q(β) for each (x  y). For large datasets it is usually sufﬁcient to set S = 1; however perfor-
mance can in some cases be substantially improved by using more samples  at the cost of longer
training times.
If the data is divided into B equally-sized batches such that D = {bj}B
j=1  and an ‘online’ optimiser
is used  with the parameters updated after each batch gradient calculation  the following online loss
function (and corresponding derivatives) should be employed:

L(α  β  bj) =

1
B

LC(α  β) + LE(β  bj)

(18)

Note the 1/B factor for the complexity loss. This is because the weights (to which the complex-
ity cost applies) are only transmitted once for the entire dataset  whereas the error cost must be
transmitted separately for each batch.
During training  the prior parameters α should be set to their optimal values after every update to
β. For more complex priors where the optimal α cannot be found in closed form (such as mixture
distributions)  α and β can instead be optimised simultaneously with gradient descent [17  10].
Ideally a trained network should be evaluated on some previously unseen input x(cid:48) using the expected
distribution (cid:104)Pr(.|x(cid:48)  w)(cid:105)w∼Q(β). However the maximum a posteriori approximation Pr(.|x(cid:48)  w∗) 
where w∗ is the mode of Q(β)  appears to work well in practice (at least for diagonal Gaussian
posteriors). This is equivalent to removing weight noise during testing.

7 Pruning

Removing weights from a neural network (a process usually referred to as pruning) has been re-
peatedly proposed as a means of reducing complexity and thereby improving generalisation [15  7].
This would seem redundant for variational inference  which automatically limits the network com-
plexity. However pruning can reduce the computational cost and memory demands of the network.
Furthermore we have found that if the network is retrained after pruning  the ﬁnal performance can
be improved. A possible explanation is that pruning reduces the noise in the gradient estimates
(because the pruned weights are not sampled) without increasing network complexity.
Weights w that are more probable under Q(β) tend to give lower LN (w D) and pruning a weight
is equivalent to ﬁxing it to zero. These two facts suggest a pruning heuristic where a weight is
removed if its probability density at zero is sufﬁciently high under Q(β). For a diagonal posterior
we can deﬁne the relative probability of each wi at zero as the density of qi(βi) at zero divided by
the density of qi(βi) at its mode. We can then deﬁne a pruning heuristic by removing all weights
whose relative probability at zero exceeds some threshold γ  with 0 ≤ γ ≤ 1. If qi(βi) is Gaussian
this yields

(cid:18)

(cid:19)

exp

− µ2
i
2σ2
i

(cid:12)(cid:12)(cid:12)(cid:12) µi

σi

(cid:12)(cid:12)(cid:12)(cid:12) < λ

> γ =⇒

5

(19)

“In wage negotiations the industry bargains as a unit with a single union.”

Figure 1: Two representations of a TIMIT utterance. Note the lower resolution and greater
decorrelation of the MFC coefﬁcients (top) compared to the spectrogram (bottom).

√−2 ln γ  with λ ≥ 0.

λ =

2 ln

(20)

where we have used the reparameterisation λ =
If λ = 0 no weights
are pruned. As λ grows the amount of pruning increases  and the probability of the pruned weight
vector under Q(β) (and therefore the likely network performance) decreases. A good rule of thumb
for how high λ can safely be set is the point at which the pruned weights become less probable than
an average weight sampled from qi(βi). For a Gaussian this is
2 ≈ 0.83

(cid:113)

√

If the network is retrained after pruning  the cost of transmitting which weights have been removed
should in principle be added to LC(α  β) (since this information could be used to overﬁt the training
data). However the extra cost does not depend on the network parameters  and can therefore be
ignored for the purposes of optimisation.
When a Gaussian prior is used its mean tends to be near zero. This implies that ‘cheaper’ weights 
where qi(βi) ≈ P (α)  have high relative probability at zero and are thus more likely to be pruned.

8 Experiments

We tested all the combinations of posterior and prior described in Section 5 on a hierarchical mul-
tidimensional recurrent neural network [9] trained to do phoneme recognition on the TIMIT speech
corpus [4]. We also assessed the pruning heuristic from Section 7 by applying it with various thresh-
olds to a trained network and observing the impact on performance and network size.
TIMIT is a popular phoneme recognition benchmark. The core training and test sets (which we used
for our experiments) contain respectively 3696 and 192 phonetically transcribed utterances. We
deﬁned a validation set by randomly selecting 184 sequences from the training set. The reduced set
of 39 phonemes [6] was used during both training and testing. The audio data was presented to the
network in the form of spectrogram images. One such image is contrasted with the mel-frequency
cepstrum representation used for most speech recognition systems in Fig. 1.
Hierarchical multidimensional recurrent neural networks containing Long Short-Term Memory [11]
hidden layers and a CTC output layer [8] have proven effective for ofﬂine handwriting recogni-
tion [9]. The same architecture is employed here  with a spectrogram in place of a handwriting
image  and phoneme labels in place of characters. Since the network scans through the spectrogram
in all directions  both vertical and horizontal correlations can be captured.
The network topology was identical for all experiments. It was the same as that of the handwriting
recognition network in [9] except that the dimensions of the three subsampling windows used to
progressively decrease resolution were now 2× 4  2× 4 and 1× 4  and the CTC layer now contained
40 output units (one for each phoneme  plus an extra for ‘blank’). This gave a total of 15 layers 
1306 units (not counting the inputs or bias)  and 139 536 weights. All network parameters were
trained with online steepest descent (weight updates after every sequence) using a learning rate of
10−4 and a momentum of 0.9. For the networks with stochastic derivatives (i.e those with Gaussian
posteriors) a single weight sample was drawn for each sequence. Preﬁx search CTC decoding [8]
was used to transcribe the test set  with probability threshold 0.995. When parameters in the pos-
terior or prior were ﬁxed  the best value was found empirically. All networks were initialised with
random weights (or random weight means if the posterior was Gaussian)  chosen from a Gaussian

6

Adaptive weight noise

Adapt. prior weight noise

Weight noise

Maximum likelihood

Figure 2: Error curves for four networks during training. The green  blue and red curves cor-
respond to the average per-sequence error loss LE(β D) on the training  test and validation sets
respectively. Adaptive weight noise does not overﬁt  and normal weight noise overﬁts much more
slowly than maximum likelihood. Adaptive weight noise led to longer training times and noisier
error curves.

Table 1: Results for different priors and posteriors. All distribution parameters were learned by
the network unless ﬁxed values are speciﬁed. ‘Error’ is the phoneme error rate on the core test set
(total edit distance between the network transcriptions and the target transcriptions  multiplied by
100). ‘Epochs’ is the number of passes through the training set after which the error was recorded.
‘Ratio’ is the compression ratio of the training set transcription targets relative to a uniform code
over the 39 phoneme labels (≈ 5.3 bits per phoneme); this could only be calculated for the networks
with Gaussian priors and posteriors.

Name
Adaptive L1
Adaptive L2
Adaptive mean L2
L2
Maximum likelihood
L1
Adaptive mean L1
Weight noise
Adaptive prior weight noise Gauss σi = 0.075 Gauss
Adaptive weight noise
Gauss

Posterior
Delta
Delta
Delta
Delta
Delta
Delta
Delta
Gauss σi = 0.075 Uniform

Gauss

Prior
Laplace
Gauss
Gauss σ2 = 0.1
Gauss µ = 0  σ2 = 0.1
Uniform
Laplace µ = 0  b = 1/12
Laplace b = 1/12

Error
49.0
35.1
28.0
27.4
27.1
26.0
25.4
25.4
24.7
23.8

Epochs Ratio
7
421
53
59
44
545
765
220
260
384

–
–
–
–
–
–
–
–
0.542
0.286

with mean 0  standard deviation 0.1. For the adaptive Gaussian posterior  the standard deviations
of the weights were initialised to 0.075 then optimised during training; this ensured that the vari-
ances (which are the standard deviations squared) remained positive. The networks with Gaussian
posteriors and priors did not require early stopping and were trained on all 3696 utterances in the
training set; all other networks used the validation set for early stopping and hence were trained on
3512 utterances. These were also the only networks for which the transmission cost of the network
weights could be measured (since it did not depend on the quantisation of the posterior or prior).
The networks were evaluated on the test set using the parameters giving lowest LE(β D) on the
training set (or validation set if present). All experiments were stopped after 100 training epochs
with no improvement in either L(α  β D)  LE(β D) or the number of transcription errors on the
training or validation set. The reason for such conservative stopping criteria was that the error curves
of some of the networks were extremely noisy (see Fig. 2).
Table 1 shows the results for the different posteriors and priors. L2 regularisation was no better
than unregularised maximum likelihood  while L1 gave a slight improvement; this is consistent
with our previous experience of recurrent neural networks. The fully adaptive L1 and L2 networks
performed very badly  apparently because the priors became excessively narrow (σ2 ≈ 0.003 for
L2 and b ≈ 0.002 for L1). L1 with ﬁxed variance and adaptive mean was somewhat better than L1
with mean ﬁxed at 0 (although the adaptive mean was very close to zero  settling around 0.0064).
The networks with Gaussian posteriors outperformed those with delta posteriors  with the best score
obtained using a fully adaptive posterior.
Table 2 shows the effect of pruning on the trained ‘adaptive weight noise’ network from Table 1.
The pruned networks were retrained using the same optimisation as before  with the error recorded
before and after retraining. As well as being highly effective at removing weights  pruning led to
improved performance following retraining in some cases. Notice the slow increase in initial error
up to λ = 0.5 and sharp rise thereafter; this is consistent with the ‘safe’ threshold of λ ≈ 0.83

7

Table 2: Effect of Network Pruning. ‘λ’ is the threshold used for pruning. ‘Weights’ is the number
of weights left after pruning and ‘Percent’ is the same ﬁgure expressed as a percentage of the original
weights. ‘Initial Error’ is the test error immediately after pruning and ‘Retrain Error’ is the test error
following ‘Retrain Epochs’ of subsequent retraining. ‘Bits/weight’ is the average bit cost (as deﬁned
in Eq. (13)) of the unpruned weights.

λ
0
0.01
0.05
0.1
0.2
0.5
1
2

Weights
139 536
107 974
63 079
52 984
43 182
31 120
22 806
16 029

Percent
100%
77.4%
45.2%
37.9%
30.9%
22.3%
16.3%
11.5%

Initial error Retrain error Retrain Epochs Bits/weight
23.8
23.8
23.9
23.9
23.9
24.0
24.5
28.0

0.53
0.72
1.15
1.40
1.82
2.21
3.19
3.55

23.8
24.0
23.5
23.3
23.7
23.3
24.1
24.5

0
972
35
351
740
125
403
335

Figure 3: Weight costs in an 2D LSTM recurrent connection. Each dot corresponds to a weight;
the lighter the colour the more bits the weight costs. The vertical axis shows the LSTM cell the
weight comes from; the horizontal axis shows the LSTM unit the weight goes to. Note the low cost of
the ‘V forget gates’ (these mediate vertical correlations between frequency bands in the spectrogram 
which are apparently less important to transcription than horizontal correlations between timesteps);
the high cost of the ‘cells’ (LSTM’s main processing units); the bright horizontal and vertical bands
(corresponding to units with ‘important’ outputs and inputs respectively); and the bright diagonal
through the cells (corresponding to self connections).

mentioned in Section 7. The lowest ﬁnal phoneme error rate of 23.3 would until recently have been
the best recorded on TIMIT; however the application of deep belief networks has now improved the
benchmark to 20.5 [3].

Acknowledgements

I would like to thank Geoffrey Hinton  Christian Osendorfer  Justin Bayer and Thomas R¨uckstieß
for helpful discussions and suggestions. Alex Graves is a Junior Fellow of the Canadian Institute for
Advanced Research.

Figure 4: The ‘cell’ weights from Fig. 3 pruned at different thresholds. Black dots are pruned
weights  white dots are remaining weights. ‘Cheaper’ weights tend to be removed ﬁrst as λ grows.

8

input gatesH forget gatesV forget gatescellsoutput gatescellsReferences
[1] D. Barber and C. M. Bishop. Ensemble learning in Bayesian neural networks.  pages 215–237. Springer-

Verlag  Berlin  1998.

[2] D. Barber and B. Schottky. Radial basis functions: A bayesian treatment. In NIPS  1997.
[3] G. E. Dahl  M. Ranzato  A. rahman Mohamed  and G. Hinton. Phone recognition with the mean-
covariance restricted boltzmann machine. In J. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R. Zemel 
and A. Culotta  editors  Advances in Neural Information Processing Systems 23  pages 469–477. 2010.

[4] DARPA-ISTO. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)  speech disc

cd1-1.1 edition  1990.

[5] B. J. Frey. Graphical models for machine learning and digital communication. MIT Press  Cambridge 

MA  USA  1998.

[6] K. fu Lee and H. wuen Hon. Speaker-independent phone recognition using hidden markov models. IEEE

Transactions on Acoustics  Speech  and Signal Processing  1989.

[7] C. L. Giles and C. W. Omlin. Pruning recurrent neural networks for improved generalization performance.

IEEE Transactions on Neural Networks  5:848–851  1994.

[8] A. Graves  S. Fern´andez  F. Gomez  and J. Schmidhuber. Connectionist temporal classiﬁcation: La-
belling unsegmented sequence data with recurrent neural networks. In Proceedings of the International
Conference on Machine Learning  ICML 2006  Pittsburgh  USA  2006.

[9] A. Graves and J. Schmidhuber. Ofﬂine handwriting recognition with multidimensional recurrent neural

networks. In NIPS  pages 545–552  2008.

[10] G. E. Hinton and D. van Camp. Keeping the neural networks simple by minimizing the description length

of the weights. In COLT  pages 5–13  1993.

[11] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation  9(8):1735–1780 

1997.

[12] A. Honkela and H. Valpola. Variational learning and bits-back coding: An information-theoretic view to

bayesian learning. IEEE Transactions on Neural Networks  15:800–810  2004.

[13] K.-C. Jim  C. Giles  and B. Horne. An analysis of noise in recurrent neural networks: convergence and

generalization. Neural Networks  IEEE Transactions on  7(6):1424 –1438  nov 1996.

[14] N. D. Lawrence. Variational Inference in Probabilistic Models. PhD thesis  University of Cambridge 

2000.

[15] Y. Le Cun  J. Denker  and S. Solla. Optimal brain damage. In D. S. Touretzky  editor  Advances in Neural

Information Processing Systems  volume 2  pages 598–605. Morgan Kaufmann  San Mateo  CA  1990.

[16] D. J. C. MacKay. Probable networks and plausible predictions - a review of practical bayesian methods

for supervised neural networks. Neural Computation  1995.

[17] S. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight sharing. Neural Computation 

4:173–193  1992.

[18] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Computation 

21(3):786–792  2009.

[19] D. Plaut  S. Nowlan  and G. E. Hinton. Experiments on learning by back propagation. Technical Report
CMU-CS-86-126  Department of Computer Science  Carnegie Mellon University  Pittsburgh  PA  1986.
[20] M. Riedmiller and T. Braun. A direst adaptive method for faster backpropagation learning: The rprop

algorithm. In International Symposium on Neural Networks  1993.

[21] J. Rissanen. Modeling by shortest data description. Automatica  14(5):465 – 471  1978.
[22] D. E. Rumelhart  G. E. Hinton  and R. J. Williams. Learning representations by back-propagating errors 

pages 696–699. MIT Press  Cambridge  MA  USA  1988.

[23] C. E. Shannon. A mathematical theory of communication. Bell system technical journal  27  1948.
[24] P. Smolensky. Information processing in dynamical systems: foundations of harmony theory  pages 194–

281. MIT Press  Cambridge  MA  USA  1986.

[25] C. S. Wallace. Classiﬁcation by minimum-message-length inference. In Proceedings of the international
conference on Advances in computing and information  ICCI’90  pages 72–81  New York  NY  USA 
1990. Springer-Verlag New York  Inc.

[26] I. H. Witten  R. M. Neal  and J. G. Cleary. Arithmetic coding for data compression. Commun. ACM 

30:520–540  June 1987.

9

,Jason Hartford
James Wright
Kevin Leyton-Brown