2018,A^2-Nets: Double Attention Networks,Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work  we propose the “double attention block”  a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos  enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps  where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task  a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task  our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works.,A2-Nets: Double Attention Networks

Yunpeng Chen∗

National University of Singapore

chenyunpeng@u.nus.edu

Yannis Kalantidis
Facebook Research
yannisk@fb.com

Jianshu Li

National University of Singapore

jianshu@u.nus.edu

Shuicheng Yan

Qihoo 360 AI Institute

National University of Singapore

eleyans@nus.edu.sg

Jiashi Feng

National University of Singapore

elefjia@nus.edu.sg

Abstract

Learning to capture long-range relations is fundamental to image/video recognition.
Existing CNN models generally rely on increasing depth to model such relations
which is highly inefﬁcient. In this work  we propose the “double attention block”  a
novel component that aggregates and propagates informative global features from
the entire spatio-temporal space of input images/videos  enabling subsequent con-
volution layers to access features from the entire space efﬁciently. The component
is designed with a double attention mechanism in two steps  where the ﬁrst step
gathers features from the entire space into a compact set through second-order
attention pooling and the second step adaptively selects and distributes features
to each location via another attention. The proposed double attention block is
easy to adopt and can be plugged into existing deep neural networks conveniently.
We conduct extensive ablation studies and experiments on both image and video
recognition tasks for evaluating its performance. On the image recognition task  a
ResNet-50 equipped with our double attention blocks outperforms a much larger
ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number
of parameters and less FLOPs. On the action recognition task  our proposed model
achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with
signiﬁcantly higher efﬁciency than recent works.

1

Introduction

Deep Convolutional Neural Networks (CNNs) have been successfully applied in image and video
understanding during the past few years. Many new network topologies have been developed to
alleviate optimization difﬁculties [9  10] and increase the learning capacities [26  5]  which beneﬁt
recognition performance for both images [8  2] and videos [23] signiﬁcantly.
However  CNNs are inherently limited by their convolution operators which are dedicated to capturing
local features and relations  e.g. from a 7 × 7 region  and are inefﬁcient in modeling long-range
interdependencies. Though stacking multiple convolution operators can enlarge the receptive ﬁeld  it
also comes with a number of unfavorable issues in practice. First  stacking multiple operators makes
the model unnecessarily deep and large  resulting in higher computation and memory cost as well as
increased over-ﬁtting risks. Second  features far away from a speciﬁc location have to pass through a
stack of layers before affecting the location for both forward propagation and backward propagation 
increasing the optimization difﬁculties during the training. Third  the features visible to a distant
location are actually “delayed” ones from several layers behind  causing inefﬁcient reasoning. Though

∗Part of the work is done during internship at Facebook Research.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

some recent works [11  25] can partially alleviate the above issues  they are either non-ﬂexible [11]
or computationally expensive [25].
In this work  we aim to overcome these limitations by introducing a new network component
that enables a convolution layer to sense the entire spatio-temporal space2 from its adjacent layer
immediately. The core idea is to ﬁrst gather key features from the entire space into a compact set
and then distribute them to each location adaptively  so that the subsequent convolution layers can
sense features from the entire space even without a large receptive ﬁled. We develop a generic
function for such purpose and implement it with an efﬁcient double attention mechanism. The ﬁrst
second-order attention pooling operation selectively gathers key features from the entire space  while
the second adopts another attention mechanism to adaptively distribute a subset of key features that
are helpful to complement each spatio-temporal location for high-level tasks. We denote our proposed
double-attention block as A2-block and its resultant network as A2-Net.
The double-attention block is related to a number of recent works  including the Squeeze-and-
Excitation Networks [11]  covariance pooling [14]  the Non-local Neural Networks [25] and the
Transformer architecture of [24]. However  compared with these existing works  it enjoys several
unique advantages: Its ﬁrst attention operation implicitly computes second-order statistics of pooled
features and can capture complex appearance and motion correlations that cannot be captured by
the global average pooling used in SENet [11]. Its second attention operation adaptively allocates
features from a compact bag  which is more efﬁcient than exhaustively correlating the features from
all the locations with every speciﬁc location as in [25  24]. Extensive experiments on image and
video recognition tasks clearly validate the above advantages of our proposed method.
We summarize our contributions as follows:

• We propose a generic formulation for capturing long-range feature interdependencies via

universal gathering and distribution functions.

• We propose the double attention block for gathering and distributing long-range features  an
efﬁcient architecture that captures second-order feature statistics and makes adaptive feature
assignment. The block can model long-range interdependencies with a low computational
and memory footprint and at the same time boost image/video recognition performance
signiﬁcantly.

• We investigate the effect of our proposed A2-Net with extensive ablation studies and prove
its superior performance through comparison with the state-of-the-arts on a number of
public benchmarks for both image recognition and video action recognition tasks  including
ImageNet-1k  Kinetics and UCF-101.

The rest of the paper is organized as follows. We ﬁrst motivate and present our approach in Section 2 
where we also discuss the relation of our approach to recent works. We then evaluate and report
results in Section 3 and conclude the paper with Section 4.

2 Method

Convolutional operators are designed to focus on local neighborhoods and therefore fail to “sense”
the entire spatial and/or temporal space  e.g. the entire input frame or one location across multiple
frames. A CNN model thus usually employs multiple convolution layers (or recurrent units [6  17]) in
order to capture global aspects of the input. Meanwhile  self-attentive and correlation operators like
second-order pooling have been recently shown to work well in a wide range of tasks [24  14  15]. In
this section we present a component capable of gathering and distributing global features to each
spatial-temporal location of the input  helping subsequent convolution layers sense the entire space
immediately and capture complex relations. We ﬁrst formally describe this desired component by
providing a generic formulation and then introduce our double attention block  a highly efﬁcient
instantiation of such a component. We ﬁnally discuss the relation of our approach to other recent
related approaches.

2Here by “space” we mean the entire feature maps of an input frame and the complete spatio-temporal

features from a video sequence.

2

Figure 1: Illustration of the double-attention mechanism. (a) An example on a single frame input
for explaining the idea of our double attention method  where the set of global featues is computed
only once and then shared by all locations. Meanwhile  each location i will generate its own attention
vector based on the need of its local feature vi to select a desired subset of global features that is
helpful to complement current location and form the feature zi. (b) The double attention operation on
a three dimensional input array A. The ﬁrst attention step is shown on the top and produces a set of
global features. At location i  the second attention step generates the new local feature zi  as shown
at the bottom.

Let X ∈ Rc×d×h×w denote the input tensor for a spatio-temporal (3D) convolutional layer  where
c denotes the number of channels  d denotes the temporal dimension3 and h  w are the spatial
dimensions of the input frames. For every spatio-temporal input location i = 1  . . .   dhw with local
feature vi  let us deﬁne

zi = Fdistr (Ggather(X)  vi)  

(1)
to be the output of an operator that ﬁrst gathers features in the entire space and then distributes them
back to each input location i  taking into account the local feature vi of that location. Speciﬁcally 
Ggather adaptively aggregates features from the entire input space  and Fdistr distributes the gathered
information to each location i  conditioned on the local feature vector vi.
The idea of gathering and distributing information is motivated by the squeeze-and-excitation network
(SENet) [11]. Eqn. (1)  however  presents it in a more general form that leads to some interesting
insights and optimizations. In [11]  global average pooling is used in the gathering process  while the
resulted single global feature is distributed to all locations  ignoring different needs across locations.
Seeing these shortcomings  we introduce this genetic formulation and propose the Double Attention
block  where global information is ﬁrst gathered by second-order attention pooling (instead of ﬁrst-
order average pooling)  and the gathered global features are adaptively distributed conditioned on the
need of current local feature vi  by a second attention mechanism. In this way  more complex global
relations can be captured by a compact set of features and each location can receive its customized
global information that is complementary to the exiting local features  facilitating learning more
complex relations. The proposed component is illustrated in Figure 1 (a). At below  we ﬁrst describe
its architecture in details and then discuss some instantiations and its connections to other recent
related approaches.

2.1 The First Attention Step: Feature Gathering

A recent work [15] used bilinear pooling to capture second-order statistics of features and generate
global representations. Compared with the conventional average and max pooling which only
compute ﬁrst-order statistics  bilinear pooling can capture and preserve complex relations better.
Concretely  bilinear pooling gives a sum pooling of second-order features from the outer product of
all the feature vector pairs (ai  bi) within two input feature maps A and B:

Gbilinear(A  B) = AB(cid:62) =

aib(cid:62)
i  

(2)

(cid:88)

∀i

3For a spatial (2D) convolution  i.e. when the input is an image  d = 1.

3

whm×…AAttention Mapswh=m1stn-th…Global Descriptorsm…×…n=Global DescriptorsAttention VectorswhmZ(b) Visualization of the Double Attention Operation(a) Double Attention Methodb11stn-th1sthw-thbn1stn-thInputAttention Vector(conditioned on local descriptors vi)Attention MapsGlobal DescriptorsEncode backto inputwhere A = [a1 ···   adhw] ∈ Rm×dhw and B = [b1 ···   bdhw] ∈ Rn×dhw. In CNNs  A and B can
be the feature maps from the same layer  i.e. A = B  or from two different layers  i.e. A = φ(X; Wφ)
and B = θ(X; Wθ)  with parameters Wφ and Wθ.
By introducing the output variable G = [g1 ···   gn] ∈ Rm×n of the bilinear pooling and rewriting
the second feature B as B = [¯b1;··· ; ¯bn] where each ¯bi is a dhw-dimensional row vector  we can
reformulate Eqn. (2) as

gi = A¯b(cid:62)

i =

¯bijaj.

(3)

(cid:88)

∀j

Eqn. (3) gives a new perspective on the bilinear pooling result: instead of just computing second-
order statistics  the output of bilinear pooling G is actually a bag of visual primitives  where each
primitive gi is calculated by gathering local features weighted by ¯bi. This inspires us to develop
a new attention-based feature gathering operation. We further apply a softmax onto B to ensure
¯bij = 1  i.e. a valid attention weighting vector  which gives following second-order attention

(cid:80)

j

pooling process:

gi = A softmax(¯bi)(cid:62).

(4)
The ﬁrst row in Figure 1 (b) shows the second-order attention pooling that corresponds to Eqn. (4) 
where both A and B are outputs of two different convolution layers transforming the input X. In
implementation  we let A = φ(X; Wφ) and B = softmax (θ(X; Wθ)). The second-order attention
pooling offers an effective way to gather key features: it captures the global features  e.g. texture
and lighting  when ¯bi is densely attended on all locations; and it captures the existence of speciﬁc
semantic  e.g. an object and parts  when ¯bi is sparsely attended on a speciﬁc region. We note that
similar understandings were presented in [7]  in which they proposed a rank-1 approximation of a
bilinear pooling operation associated with a fully connected classiﬁer. However  in our work  we
propose to apply attention pooling to gather visual primitives at different locations into a bag of
global descriptors using softmax attention map and do not apply any low-rank constraint.

2.2 The Second Attention Step: Feature Distribution

The next step after gathering features from the entire space is to distribute them to each location of
the input  such that the subsequent convolution layer can sense the global information even with a
small convolutional kernel.
Instead of distributing the same summarized global features to all locations like SENet [11]  we
propose to get more ﬂexibility by distributing an adaptive bag of visual primitives based on the need
of feature vi at each location. In this way  each location can select features that are complementary to
the current feature which can make the training easier and help capture more complex relations. This
is achieved by selecting a subset of feature vectors from Ggather(X) with soft attention:

zi =

vijgj = Ggather(X)vi  where

vij = 1.

(5)

(cid:88)

∀j

(cid:88)

∀j

Eqn. (5) formulates the proposed soft attention for feature selection. In our implementation  we
apply the softmax function to normalize vi into the one with unit sum  which is found to give better
convergence. The second row in Figure 1 (b) shows the above feature selection step. Similar to
the way we generate the attention map  the set of attention weight vectors is also generated by a
convolution layer follow by a softmax normalizer  i.e. V = softmax (ρ(X; Wρ)) where Wρ contains
parameters for this layer.

2.3 The Double Attention Block

We combine the above two attention steps to form our proposed double-attention block  with its
computation graph in deep neural networks is given in Figure 2. To formulate the double attention
operation  we substitute Eqn. (4) and Eqn. (5) into Eqn. (1) and obtain

Z = Fdistr (Ggather(X)  V )

= Ggather(X)softmax (ρ(X; Wρ))

(cid:62)(cid:105)

(cid:104)

=

φ(X; Wφ)softmax (θ(X; Wθ))

4

softmax (ρ(X; Wρ)) .

(6)

Figure 2: The computational graph of the proposed double attention block. All convolution kernel
size is 1 × 1 × 1. We insert this double attention block to existing convolutional neural network  e.g.
residual networks [9]  to form the A2-Net.

Figure 1 (b) shows the combined double attention operation and Figure 2 shows the corresponding
computational graph  where the feature arrays A  B and V are generated by three different convolution
layers operating on the input feature array X followed by softmax normalization if necessary. The
output result Z is given by conducting two matrix multiplications with necessary reshape and
transpose operations. Here  an additional convolution layer is added at the end to expand the number
of channels for the output Z  such that it can be encoded back to the input X via element-wise
addition. During the training process  gradient of the loss function can be easily computed using
auto-gradient [3  18] with the chain rule.
There are two different ways to implement the computational graph of Eqn. (6). One is to use the left
association as given in Eqn. (6) with computation graph is shown in Figure 2. The other is to conduct
the right association  as formulate below:

Z = φ(X; Wφ)

softmax (θ(X; Wθ))

(cid:62)

softmax (ρ(X; Wρ))

(cid:104)

(cid:105)

.

(7)

We note these two different associations are mathematically equivalent and thus will produce the
same output. However  they have different computational cost and memory consumption. The
computational complexity of the second matrix multiplication in “left association” in Eqn. (6) is
O(mndhw)  while “right association” in Eqn. (7) has complexity of O(m(dhw)2). As for the
memory cost4  storing the output of the results of the ﬁrst matrix multiplication costs mn/218MB
and (dhw)2/218MB for the left and right associations respectively. In practice  an input data array X
with 32 28 × 28 frames and 512 channel size can easily cost more than 2GB memory when adopting
the right association  much more expensive than 1MB cost of the left association. In this case  left
association is also more computationally efﬁcient than the right one. Therefore  for common cases
where (dhw)2 > nm  we suggest implementation in Eqn. (6) with left association.

2.4 Discussion

It is interesting to observe that the implementation in Eqn. (7) with right association can be further
explained by the recent NL-Net [25]  where the ﬁrst multiplication captures pair-wise relations
between local features and gives an output relation matrix in Rdhw×dhw. The resulted relation
matrix is then applied to linearly combine the transformed features φ(X) into the output feature
Z. The difference is apparent in the design of the pair-wise relation function  where we propose
a new relation function  i.e. softmax (θ(X))
softmax (ρ(X)) rather than using the Embedded
Gaussian formulation [24] to capture the pair-wise relations. Meanwhile  as discussed above  any
such a method practically suffers from high computational and memory costs  and relies on the some
subsampling tricks to reduce the cost which may potentially hurts the accuracy. Since NL-Net is the
current state-of-the-art for video recognition tasks and also closely related  we directly compare and
extensively discuss performance between the two in the Experiments section. The results clearly
show that our proposed method not only outperforms NL-Net  but does so with higher efﬁciency
and accuracy. As the Embedded Gaussian NL-Net formulation that we compare in the experiments
is mathematically equivalent to the self-attention formulation of [24]  conclusions/comparisons to
NL-Net extend to the transformer networks as well.

(cid:62)

4All values are stored in 32-bit ﬂoat.

5

SoftmaxConvolution(1×1×1) MatrixMultiplicationFeature DistributionAttentionVectorsSoftmaxConvolution(1×1×1) BilinearPoolingFeature GatheringAttentionMapsInputDimensionReductionDimensionExtensionOutputTable 1: Three backbone Residual Networks for the video tasks. The input size for ResNet-26 and
ResNet-29 are 16×112×112  while the input size for ResNet-50 is 8×224×224. We follow [25] and
set k = [3  1  3]  [3  1  3  1  3  1]  [1  3  1] for ResNet-50 in last three stages and decrease the temporal
size to reduce computational cost.

stage

conv1

conv2

conv3

conv4

conv5

ResNet-26

ResNet-29

3×5×5  16  stride (1 2 2)

3×5×5  16  stride (1 2 2)

3×3×3  32
1×1×1  128

 1×1×1  32
 1×1×1  64
 1×1×1  128
 1×1×1  256

3×3×3  64
1×1×1  256

3×3×3  128
1×1×1  512

3×3×3  256
1×1×1  1024

 × 2
 × 2
 × 2
 × 2

3×3×3  32
1×1×1  128

 1×1×1  32
 1×1×1  64
 1×1×1  128
 1×1×1  256

3×3×3  64
1×1×1  256

3×3×3  128
1×1×1  512

3×3×3  256
1×1×1  1024

 × 2
 × 2
 × 3
 × 2

output

16×56×56

8×56×56

8×28×28

8×14×14

8×7×7

1×1×1

ResNet-50

3×5×5  32  stride (1 2 2)
max pooling  stride (1 2 2)

3×3×3  64
1×1×1  256

 1×1×1  64
 1×1×1  128
 1×1×1  256
 1×1×1  512

k×3×3  128
1×1×1  512

k×3×3  256
1×1×1  1024

k×3×3  512
1×1×1  2048

 × 3
 × 4
 × 6
 × 3

output

8×56×56

8×56×56

4×28×28

4×14×14

4×7×7

1×1×1

global average pool  fc  softmax

global average pool  fc  softmax

global average pool  fc  softmax

(#Params  FLOPs)

(7.0 M  8.3 G)

(7.6 M  9.2 G)

(33.4 M  31.3 G)

3 Experiments

In this section  we ﬁrst conduct extensive ablation studies to evaluate the proposed A2-Nets on the
Kinetics [12] video recognition dataset and compare it with the state-of-the-art NL-Net [25]. Then
we conduct more experiments using deeper and wider neural networks on both image recognition
and video recognition tasks and compare it with state-of-the-art methods.

3.1

Implementation Details

Backbone CNN We use the residual network [10] as our backbone CNN for all experiments.
Table 1 shows architecture details of the backbone CNNs for video recognition tasks  where we use
ResNet-26 for all ablation studies and ResNet-29 as one of the baseline methods. The computational
cost is measured by FLOPs  i.e. ﬂoating-point multiplication-adds  and the model complexity is
measured by #Params  i.e. total number of trained parameters. The ResNet-50 is almost 2× deeper
and wider than the ResNet-26 and thus only used for last several experiments when comparing with
the state-of-the-art methods. For the image recognition task  we use the same ResNet-50 but without
the temporal dimension for both the input/output data and convolution kernels.

Training and Testing Settings We use MXNet [3] to experiment on the image classiﬁcation task 
and PyTorch [18] on video classiﬁcation tasks. For image classiﬁcation  we report standard single
model single 224 × 224 center crop validation accuracy  following [9  10]. For experiments on video
datasets  we report both single clip accuracy and video accuracy. All experiments are conducted
using a distributed K80 GPU cluster and the networks are optimized by synchronized SGD. Code
and trained models will be released on GitHub soon.

3.2 Ablation Studies

For the ablation studies on Kinetics [1]  we use 32 GPUs per experiment with a total batch size of
512 training from scratch. All networks take 16 frames with resolution 112 × 112 as input. The
base learning rate is set to 0.2 and is reduced with a factor of 0.1 at the 20k-th  30k-th iterations  and
terminated at the 37k-th iteration. We set the number of output channels for three convolution layers
θ(·)  φ(·) and ρ(·) to be 1/4 of the number of input channels. Note that sub-sampling trick is not
adopted for all methods for fair comparison.
Single Block Table 2 shows the results when only one extra block is added to the backbone network.
The block is placed after the second residual unit of a certain stage. As can be seen from the last
three rows  our proposed A2-block constantly improves the performance compared with both the

6

Table 2: Comparisons between single nonlocal block [25] and single double attention block on the
Kinetics dataset. The performance of vanilla residual networks without extra block is shown in the
top row.

Model

ResNet-26
ResNet-29

ResNet-26 + NL [25]

ResNet-26 + A2

+ 1 Block

None
None

@ Conv2
@ Conv3
@ Conv4
@ Conv2
@ Conv3
@ Conv4

–

–

–

50.4 %
50.8 %

FLOPs ∆ FLOPs Clip @1 ∆ Clip@1 Video@1
#Params
8.3 G
60.7 %
7.043 M
7.620 M
61.6 %
9.2 G
7.061 M 49.0 G
7.112 M 13.7 G
7.312 M
9.3 G
8.7 G
7.061 M
8.7 G
7.112 M
7.312 M
8.7 G

900 M
40.69 G
5.45 G
1.04 G
463 M
463 M
463 M

+1.1 %
+1.3 %
+0.8 %
+1.5 %
+1.9 %

51.5 %
51.7 %
51.2 %
51.9 %
52.3 %

62.0 %
62.3 %
61.8 %
62.0 %
62.6 %

+0.5 %

–

–

Table 3: Comparisons between performance from multiple nonlocal blocks [25] and multiple double
attention blocks on Kinetics dataset. We report both top-1 clips accuracy and top-1 video accuracy
for all the methods. The vanilla residual networks without extra blocks are shown in the top row.

Model

ResNet-26
ResNet-29

ResNet-26 + NL [25]

ResNet-26 + A2

+N Blocks

None
None

1 @ Conv4
2 @ Conv4

4 @ Conv3&4

1 @ Conv4
2 @ Conv4

4 @ Conv3&4

–

–

FLOPs ∆ FLOPs Clip @1 ∆ Clip@1 Video @1
#Params
60.7 %
8.3 G
7.043 M
61.6 %
9.2 G
7.620 M
62.3 %
7.312 M
9.3 G
62.9 %
7.581 M 10.4 G
7.719 M 21.3 G
62.8 %
62.6 %
8.7 G
7.312 M
63.1 %
7.581 M
9.2 G
63.5 %
7.719 M 10.1 G

900 M
1.04 G
2.08 G
12.97 G
463 M
925 M
1.85 G

50.4 %
50.8 %
51.7 %
52.0 %
52.4 %
52.3 %
52.5 %
53.0 %

+0.5 %
+1.3 %
+1.6 %
+2.0 %
+1.9 %
+2.1 %
+2.6 %

baseline ResNet-26 and the deeper ResNet-29. Notably the extra cost is very little. We also ﬁnd that
the performance gain from placing A2-block on top layers is more signiﬁcant than placing it at lower
layers. This may be because the top layers give more semantically abstract representations that are
suitable for extracting global visual primitives. Comparatively  the Nonlocal Network [25] shows less
accuracy gain and more computational cost than ours. Since the computational cost for Nonlocal
Network is increased quadratically on bottom stage  we are even unable to ﬁnish the training when
the block is placed at Conv2.
Multiple Blocks Table 3 shows the performance gain when multiple blocks are added to the
backbone networks. As can be seen from the results  our proposed A2-Net monotonically improves
the accuracy when more blocks are added and costs less #FLOPs compared with its competitor. We
also ﬁnd that adding blocks to different stages can lead to more signiﬁcant accuracy gain than adding
all blocks to the same stage.

3.3 Experiments on Image Recognition

We evaluate the proposed A2-Net on ImageNet-1k [13] image classiﬁcation dataset  which contains
more than 1.2 million high resolution images in 1  000 categories. Our implementation is based on
√
the code released by [5] using 64 GPUs with a batch size of 2  048. The base learning rate is set to

0.1 and decreases with a factor of 0.1 when training accuracy is saturated.

Table 4: Comparison with state-of-the-
arts on ImageNet-1k.

Table 5: Comparisons with state-of-the-arts results on Ki-
netics. Only RGB information is used for input.

Model

ResNet [9]

SENet [11]

A2-Net

Backbone
ResNet-50
ResNet-152
ResNet-50
ResNet-50

Top-1
Top-5
75.3 % 92.2 %
77.0 % 93.3 %
76.7 % 93.4 %
77.0 % 93.5 %

Model

#Frames

FLOPs Video @1 Video @5

ConvNet+LSTM [1]

I3D [1]

R(2+1)D [23]

A2-Net

–
64
32
8

–

107.9 G
152.4 G
40.8 G

63.3 %
71.1 %
72.0 %
74.6 %

–

89.3 %
90.0 %
91.5 %

7

Table 6: Comparisons with state-of-the-arts results on UCF-101. The averaged Top-1 video accuracy
on three train/test splits is reported.

Method
C3D [21]
Res3D [22]
I3D-RGB [1]

R(2+1)D-RGB [23]

A2-Net

Backbone

VGG

ResNet-18
Inception
ResNet-34
ResNet-50

FLOPs
38.5 G
19.3 G
107.9 G
152.4 G
41.6 G

Video @1
82.3 %
85.8 %
95.6 %
96.8 %
96.4 %

As can be seen from Table 4  a ResNet-50 equipped with 5 extra A2-blocks at Conv3 and Conv4
outperforms a much larger ResNet-152 architecture. We note that the A2-blocks embedded ResNet-
50 is also over 40% more efﬁcient than ResNet-152 and only costs 6.5 GFLOPs and 33.0 M
parameters. Compared with the SENet [11]  the A2-Net also achieves better accuracy which proves
the effectiveness of the proposed double attention mechanism.

3.4 Experiment Results on Video Recognition

In this subsection  we evaluate the proposed method on learning video representations. We consider
the scenario where static image features are pretrained but motion features are learned from scratch by
training a model on the large-scale Kinetics [1] dataset  and the scenario where well-trained motion
features are transfered to small-scale UCF-101 [20] dataset.

Learning Motion from Scratch on Kinetics We use ResNet-50 pretrained on ImageNet and add 5
randomly initialized A2-blocks to build the 3D convolutional network. The corresponding backbone
is shown in Table 1. The network takes 8 frames (sampling stride: 8) as input and is trained for
32k iterations with a total batch size of 512 using 64 GPUs. The initial learning rate is set to 0.04
and decreased in a stepwise manner when training accuracy is saturated. The ﬁnal result is shown
in Table 5. Compared with the state-of-the-art I3D [1] and R(2+1)D [23]  our proposed model
shows higher accuracy even with a less number of sampled frames  which once again conﬁrms the
superiority of the proposed double-attention mechanism.

Transfer the Learned Feature to UCF-101 The UCF-101 contains about 13  320 videos from
101 action categories and has three train/test splits. The training set of UCF-101 is several times
smaller than the Kinetics dataset and we use it to evaluate the generality and robustness of the features
learned by our model pre-trained on Kinetics. The network is trained with a base learning rate of 0.01
which is decreased for three times with a factor 0.1  using 8 GPUs with a batch size of 104 clips and
tested with 224 × 224 input resolution on single scale. Table 6 shows results of our proposed model
and comparison with state-of-the-arts. Consistent with above results  the A2-Net achieves leading
performance with signiﬁcantly lower computational cost. This shows that the features learned by
A2-Net are robust and can be effectively transfered to new dataset in very low cost compared with
existing methods.

4 Conclusions

In this work  we proposed a double attention mechanism for deep CNNs to overcome the limitation
of local convolution operations. The proposed double attention method effectively captures the
global information and distributes it to every location in a two-step attention manner. We well
formulated the proposed method and instantiated it as an light-weight block that can be easily
inserted into to existing CNNs with little computational overhead. Extensive ablation studies and
experiments on a number of benchmark datasets  including ImageNet-1k  Kinetics and UCF-101 
conﬁrmed the effectiveness of the proposed A2-Net on both 2D image recognition tasks and 3D video
recognition tasks. In the future  we want to explore integrating the double attention in recent compact
network architectures [19  16  4]  to leverage the expressiveness of the proposed method for smaller 
mobile-friendly models.

8

References
[1] Joao Carreira and Andrew Zisserman. Quo vadis  action recognition? a new model and the
kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 4724–4733. IEEE  2017.

[2] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets  atrous convolution  and
fully connected crfs. arXiv preprint arXiv:1606.00915  2016.

[3] Tianqi Chen  Mu Li  Yutian Li  Min Lin  Naiyan Wang  Minjie Wang  Tianjun Xiao  Bing Xu 
Chiyuan Zhang  and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274  2015.

[4] Yunpeng Chen  Yannis Kalantidis  Jianshu Li  Shuicheng Yan  and Jiashi Feng. Multi-ﬁber
networks for video recognition. In European Conference on Computer Vision (ECCV)  2018.

[5] Yunpeng Chen  Jianan Li  Huaxin Xiao  Xiaojie Jin  Shuicheng Yan  and Jiashi Feng. Dual path

networks. In Advances in Neural Information Processing Systems  pages 4470–4478  2017.

[6] Jeffrey Donahue  Lisa Anne Hendricks  Sergio Guadarrama  Marcus Rohrbach  Subhashini
Venugopalan  Kate Saenko  and Trevor Darrell. Long-term recurrent convolutional networks for
visual recognition and description. In Proceedings of the IEEE conference on computer vision
and pattern recognition  pages 2625–2634  2015.

[7] Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition. In Advances in

Neural Information Processing Systems  pages 33–44  2017.

[8] Ross Girshick. Fast r-cnn. arXiv preprint arXiv:1504.08083  2015.

[9] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[10] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual

networks. In European Conference on Computer Vision  pages 630–645. Springer  2016.

[11] Jie Hu  Li Shen  and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on

Computer Vision and Pattern Recognition (CVPR)  2018.

[12] Will Kay  Joao Carreira  Karen Simonyan  Brian Zhang  Chloe Hillier  Sudheendra Vijaya-
narasimhan  Fabio Viola  Tim Green  Trevor Back  Paul Natsev  et al. The kinetics human
action video dataset. arXiv preprint arXiv:1705.06950  2017.

[13] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[14] Peihua Li  Jiangtao Xie  Qilong Wang  and Wangmeng Zuo. Is second-order information helpful

for large-scale visual recognition? arXiv preprint arXiv:1703.08050  2017.

[15] Tsung-Yu Lin  Aruni RoyChowdhury  and Subhransu Maji. Bilinear cnn models for ﬁne-grained
visual recognition. In Proceedings of the IEEE International Conference on Computer Vision 
pages 1449–1457  2015.

[16] Ningning Ma  Xiangyu Zhang  Hai-Tao Zheng  and Jian Sun. Shufﬂenet v2: Practical guidelines

for efﬁcient cnn architecture design. arXiv preprint arXiv:1807.11164  2018.

[17] Joe Yue-Hei Ng  Matthew Hausknecht  Sudheendra Vijayanarasimhan  Oriol Vinyals  Rajat
Monga  and George Toderici. Beyond short snippets: Deep networks for video classiﬁcation. In
Computer Vision and Pattern Recognition (CVPR)  2015 IEEE Conference on  pages 4694–4702.
IEEE  2015.

[18] Adam Paszke  Sam Gross  Soumith Chintala  and Gregory Chanan. Pytorch  2017.

9

[19] Mark Sandler  Andrew Howard  Menglong Zhu  Andrey Zhmoginov  and Liang-Chieh Chen.
Inverted residuals and linear bottlenecks: Mobile networks for classiﬁcation  detection and
segmentation. arXiv preprint arXiv:1801.04381  2018.

[20] Khurram Soomro  Amir Roshan Zamir  and Mubarak Shah. Ucf101: A dataset of 101 human

actions classes from videos in the wild. arXiv preprint arXiv:1212.0402  2012.

[21] Du Tran  Lubomir Bourdev  Rob Fergus  Lorenzo Torresani  and Manohar Paluri. Learning
spatiotemporal features with 3d convolutional networks. In Computer Vision (ICCV)  2015
IEEE International Conference on  pages 4489–4497. IEEE  2015.

[22] Du Tran  Jamie Ray  Zheng Shou  Shih-Fu Chang  and Manohar Paluri. Convnet architecture

search for spatiotemporal feature learning. arXiv preprint arXiv:1708.05038  2017.

[23] Du Tran  Heng Wang  Lorenzo Torresani  Jamie Ray  Yann LeCun  and Manohar Paluri. A
closer look at spatiotemporal convolutions for action recognition. In 2018 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  2018.

[24] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez 
Łukasz Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems  pages 6000–6010  2017.

[25] Xiaolong Wang  Ross Girshick  Abhinav Gupta  and Kaiming He. Non-local neural networks.

In Computer Vision and Pattern Recognition (CVPR)  2018.

[26] Saining Xie  Ross Girshick  Piotr Dollár  Zhuowen Tu  and Kaiming He. Aggregated residual
transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  pages 5987–5995. IEEE  2017.

10

,Yunpeng Chen
Yannis Kalantidis
Jianshu Li
Shuicheng Yan
Jiashi Feng