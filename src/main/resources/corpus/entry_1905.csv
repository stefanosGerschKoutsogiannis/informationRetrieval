2014,Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights,Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods  such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods  such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically  we approximate the posterior of the weights given the data using a “mean-field” factorized distribution  in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior  as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin  the resulting algorithm  Expectation BackPropagation (EBP)  is very similar to BP in form and efficiency. However  it has several additional advantages: (1) Training is parameter-free  given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems  where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips  thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks  EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly  EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.,Expectation Backpropagation: Parameter-Free
Training of Multilayer Neural Networks with

Continuous or Discrete Weights

Daniel Soudry1  Itay Hubara2  Ron Meir2

(1) Department of Statistics  Columbia University

(2) Department of Electrical Engineering  Technion  Israel Institute of Technology

daniel.soudry@gmail.com itayhubara@gmail.com rmeir@ee.technion.ac.il

Abstract

Multilayer Neural Networks (MNNs) are commonly trained using gradient
descent-based methods  such as BackPropagation (BP). Inference in probabilistic
graphical models is often done using variational Bayes methods  such as Expec-
tation Propagation (EP). We show how an EP based approach can also be used
to train deterministic MNNs. Speciﬁcally  we approximate the posterior of the
weights given the data using a “mean-ﬁeld” factorized distribution  in an online
setting. Using online EP and the central limit theorem we ﬁnd an analytical ap-
proximation to the Bayes update of this posterior  as well as the resulting Bayes
estimates of the weights and outputs.
Despite a different origin  the resulting algorithm  Expectation BackPropagation
(EBP)  is very similar to BP in form and efﬁciency. However  it has several addi-
tional advantages: (1) Training is parameter-free  given initial conditions (prior)
and the MNN architecture. This is useful for large-scale problems  where param-
eter tuning is a major challenge. (2) The weights can be restricted to have discrete
values. This is especially useful for implementing trained MNNs in precision lim-
ited hardware chips  thus improving their speed and energy efﬁciency by several
orders of magnitude.
We test the EBP algorithm numerically in eight binary text classiﬁcation tasks.
In all tasks  EBP outperforms: (1) standard BP with the optimal constant learning
rate (2) previously reported state of the art. Interestingly  EBP-trained MNNs with
binary weights usually perform better than MNNs with continuous (real) weights
- if we average the MNN output using the inferred posterior.

1

Introduction

Recently  Multilayer1 Neural Networks (MNNs) with deep architecture have achieved state-of-the-
art performance in various supervised learning tasks [10  13  7]. Such networks are often massive
and require large computational and energetic resources. A dense  fast and energetically efﬁcient
hardware implementation of trained MNNs could be built if the weights were restricted to discrete
values. For example  with binary weights  the chip in [12] can perform 1012 operations per second
with 1mW power efﬁciency. Such performances will enable the integration of massive MNNs into
small and low-power electronic devices.
Traditionally  MNNs are trained by minimizing some error function using BackPropagation (BP) or
related gradient descent methods [14]. However  such an approach cannot be directly applied if the
weights are restricted to binary values. Moreover  crude discretization of the weights is usually quite

1i.e.  having more than a single layer of adjustable weights.

1

destructive [19]. Other methods have been suggested in the 90’s (e.g.  [22  2  17])  but it is not clear
whether these approaches are scalable.
The most efﬁcient methods developed for training Single-layer2 Neural Networks (SNN) with binary
weights use approximate Bayesian inference  either implicitly [5  1] or explicitly [23  21]. In theory 
given a prior  the Bayes estimate of the weights can be found from their posterior given the data.
However  storing or updating the full posterior is usually intractable. To circumvent this problem 
these previous works used a factorized “mean-ﬁeld” form the posterior of the weights given the data.
As explained in [21]  this was done using a special case of the widely applicable Expectation Propa-
gation (EP) algorithm [18] - with an additional approximation that the fan-in of all neurons is large 
so their inputs are approximately Gaussian. Thus  given an error function  one can analytically
obtain the Bayes estimate of the weights or the outputs  using the factorized approximation of the
posterior. However  to the best of our knowledge  it is still unknown whether such an approach could
be generalized to MNNs  which are more relevant for practical applications.
In this work we derive such generalization  using similar approximations (section 3). The end result
is the Expectation BackPropagation (EBP  section 4) algorithm for online training of MNNs where
the weight values can be either continuous (i.e.  real numbers) or discrete (e.g.  ±1 binary). Notably 
the training is parameter-free (with no learning rate)  and insensitive to the magnitude of the input.
This algorithm is very similar to BP. Like BP  it is very efﬁcient in each update  having a linear
computational complexity in the number of weights.
We test the EBP algorithm (section 5) on various supervised learning tasks: eight high dimensional
tasks of classifying text into one of two semantic classes  and one low dimensional medical discrim-
ination task. Using MNNs with two or three weight layers  EBP outperforms both standard BP  as
well as the previously reported state of the art for these tasks [6]. Interestingly  the best performance
of EBP is usually achieved using the Bayes estimate of the output of MNNs with binary weights.
This estimate can be calculated analytically  or by averaging the output of several such MNNs  with
weights sampled from the inferred posterior.

2 Preliminaries

General Notation A non-capital boldfaced letter x denotes a column vector with components xi 
a boldfaced capital letter X denotes a matrix with components Xij. Also  if indexed  the compo-
nents of xl are denoted xi l and those of Xl are denoted Xij l. We denote by P (x) the proba-
bility distribution (in the discrete case) or density (in the continuous case) of a random variable X 
P (x|y) = P (x  y) /P (y) (cid:104)x(cid:105) =
xP (x|y) dx  Cov (x  y) = (cid:104)xy(cid:105)−(cid:104)x(cid:105)(cid:104)y(cid:105)
and Var (x) = Cov (x  x). Integration is exchanged with summation in the discrete case. For any
condition A  we make use of I {A}  the indicator function (i.e.  I {A} = 1 if A holds  and zero
otherwise)  and δij = I {i = j}  Kronecker’s delta function. If x ∼ N (µ  Σ) then it is Gaussian
with mean µ and covariance matrix Σ  and we denote its density by N (x|µ  Σ). Furthermore  we
use the cumulative distribution function Φ (x) =

xP (x) dx  (cid:104)x|y(cid:105) =

´

x

−∞ N (u|0  1) du.

´

´

Model We consider a general feedforward Multilayer Neural Network (MNN) with connections
between adjacent layers (Fig. 2.1). For analytical simplicity  we focus here on deterministic binary
(±1) neurons. However  the framework can be straightforwardly extended to other types of neurons
(deterministic or stochastic). The MNN has L layers  where Vl is the width of the l-th layer  and
l=1 is the collection of Vl × Vl−1 synaptic weight matrices which connect neuronal
W = {Wl}L
layers sequentially. The outputs of the layers are {vl}L
l=1 are
the hidden layers and vL is the output layer. In each layer 

l=0  where v0 is the input layer  {vl}L−1

vl = sign (Wlvl−1)

(2.1)

where each sign “activation function” (a neuronal layer) operates component-wise (i.e.  ∀i
(sign (x))i = sign (xi)). The output of the network is therefore

:

vL = g (v0 W) = sign (WLsign (WL−1sign (··· W1v0))) .

(2.2)

2i.e.  having only a single layer of adjustable weights.

2

We assume that the weights are constrained to some set
S  with the speciﬁc restrictions on each weight denoted
by Sij l  so Wij l ∈ Sij l and W ∈ S. If Sij l = {0} 
then we say that Wij l is “disconnected”. For simplic-
ity  we assume that in each layer the “fan-in” Kl =
|{j|Sij l (cid:54)= {0}}| is constant for all i. Biases can be op-
tionally included in the standard way  by adding a con-
stant output v0 l = 1 to each layer.

Task We examine a supervised classiﬁcation learning
task  in Bayesian framework. We are given a ﬁxed set of

sequentially labeled data pairs DN = (cid:8)x(n)  y(n)(cid:9)N

P (W|Dn) ∝ P

(cid:16)
y(n)|x(n) W(cid:17)

y(n)|x(n) W(cid:17)
x(n) W(cid:17)
(cid:16)
= I(cid:110)

g

(cid:16)

P

P (W|Dn−1)  

= y(n)(cid:111)

(3.3)

.

(3.4)

3MNN with stochastic activation functions will have a “smoothed out” version of this.

3

Figure 2.1: Our MNN model (Eq. 2.2).

n=1

(so D0 = ∅)  where each x(n) ∈ RV0 is a data point  and
each y(n) is a label taken from a binary set Y ⊂ {−1  +1}VL. For brevity  we will sometimes
suppress the sample index n  where it is clear from the context. As common for supervised learning
with MNNs  we assume that for all n the relation x(n) → y(n) can be represented by a MNN with
known architecture (the ‘hypothesis class’)  and unknown weights W ∈ S. This is a reasonable
assumption since a MNN can approximate any deterministic function  given that it has sufﬁcient
number of neurons [11] (if L ≥ 2). Speciﬁcally  there exists some W∗ ∈ S  so that y(n) =

f(cid:0)x(n) W∗(cid:1) (see Eq. 2.2). Our goals are: (1) estimate the most probable W∗ for this MNN  (2)

estimate the most probable y given some (possibly unseen) x.

3 Theory

In this section we explain how a speciﬁc learning algorithm for MNNs (described in section 4) arises
from approximate (mean-ﬁeld) Bayesian inference  used in this context (described in section 2).

3.1 Online Bayesian learning in MNNs

We approach this task within a Bayesian framework  where we assume some prior distribution on the
weights - P (W|D0). Our aim is to ﬁnd P (W|DN )  the posterior probability for the conﬁguration
of the weights W  given the data. With this posterior  one can select the most probable weight
conﬁguration - the Maximum A Posteriori (MAP) weight estimate
W∗ = argmaxW∈S P (W|DN )  

(3.1)
minimizing the expected zero-one loss over the weights (I {W∗ (cid:54)= W}). This weight estimate can
be implemented in a single MNN  which can provide an estimate of the label y for (possibly unseen)
data points x through y =g (x W∗). Alternatively  one can aim to minimize the expected loss over
the output - as more commonly done in the MNN literature. For example  if the aim is to reduce
classiﬁcation error then one should use the MAP output estimate

W

y∗ = argmaxy∈Y

I {g (x W) = y} P (W|DN )  

(3.2)
which minimizes the zero-one loss (I {y∗ (cid:54)= g (x W)}) over the outputs. The resulting estimator
does not generally have the form of a MNN (i.e.  y =g (x W) with W ∈ S)  but can be approxi-
mated by averaging the output over many such MNNs with W values sampled from the posterior.
Note that averaging the output of several MNNs is a common method to improve performance.
We aim to ﬁnd the posterior P (W|DN ) in an online setting  where samples arrive sequentially.
After the n-th sample is received  the posterior is updated according to Bayes rule:

(cid:88)

for n = 1  . . .   N. Note that the MNN is deterministic  so the likelihood (per data point) has the
following simple form3

Therefore  the Bayes update in Eq. 3.3 simply makes sure that P (W|Dn) = 0 in any “illegal” con-

ﬁguration (i.e.  any W 0 such that g(cid:0)x(k) W 0(cid:1) (cid:54)= y(k)) for some 1 ≤ k ≤ n. In other words  the

posterior is equal to the prior  restricted to the “legal” weight domain  and re-normalized appropri-
ately. Unfortunately  this update is generally intractable for large networks  mainly because we need
to store and update an exponential number of values for P (W|Dn). Therefore  some approximation
is required.

3.2 Approximation 1: mean-ﬁeld
In order to reduce computational complexity  instead of storing P (W|Dn)  we will store its factor-
ized (‘mean-ﬁeld’) approximation ˆP (W|Dn)  for which

ˆP (W|Dn) =

ˆP (Wij l|Dn)  

(3.5)

(cid:89)

where each factor must be normalized. Notably  it is easy to ﬁnd the MAP estimate of the weights
(Eq. 3.1) under this factorized approximation ∀i  j  l

i j l

W ∗

ˆP (Wij l|DN ) .

ij l = argmaxWij l∈Sij l

(3.6)
The factors ˆP (Wij l|Dn) can be found using a standard variational approach [4  23]. For each n 
we ﬁrst perform the Bayes update in Eq. 3.3 with ˆP (W|Dn−1) instead of P (W|Dn−1). Then  we
project the resulting posterior onto the family of distributions factorized as in Eq. 3.5  by minimiz-
ing the reverse Kullback-Leibler divergence (similarly to EP [18  21]). A straightforward calculation
shows that the optimal factor is just a marginal of the posterior (appendix A  available in the supple-
mentary material). Performing this marginalization on the Bayes update and re-arranging terms  we
obtain a Bayes-like update to the marginals ∀i  j  l

ˆP (Wij l|Dn) ∝ ˆP

y(n)|x(n)  Wij l  Dn−1

(cid:16)

(cid:17)

(cid:88)

=
W(cid:48):W (cid:48)

P
ij l=Wij l

(cid:17) ˆP (Wij l|Dn−1)  
ˆP(cid:0)W (cid:48)

y(n)|x(n) W(cid:48)(cid:17) (cid:89)

(cid:16)

{k r m}(cid:54)={i j l}

(cid:16)

where

ˆP

y(n)|x(n)  Wij l  Dn−1

(3.7)

(cid:1) (3.8)

kr m|Dn−1

is the marginal likelihood. Thus we can directly update the factor ˆP (Wij l|Dn) in a single step.
However  the last equation is still problematic  since it contains a generally intractable summation
over an exponential number of values  and therefore requires simpliﬁcation. For simplicity  from
now on we replace any ˆP with P   in a slight abuse of notation (keeping in mind that the distributions
are approximated).

3.3 Simplifying the marginal likelihood

In order to be able to use the update rule in Eq. 3.7  we must ﬁrst calculate the marginal likelihood

(cid:1) using Eq. 3.8. For brevity  we suppress the index n and the dependence

P(cid:0)y(n)|x(n)  Wij l  Dn−1

on Dn−1 and x  obtaining

(cid:88)

(cid:89)

P (y|Wij l) =

P (y|W(cid:48))

W(cid:48):W (cid:48)

ij l=Wij l

{k r m}(cid:54)={i j l}

where we recall that P (y|W(cid:48)) is simply an indicator function (Eq. 3.4). Since  by assumption 
P (y|W(cid:48)) arises from a feed-forward MNN with input v0 = x and output vL = y  we can perform
the summations in Eq. 3.9 in a more convenient way - layer by layer. To do this  we deﬁne

kr m

P(cid:0)W (cid:48)
(cid:1)  
Vm−1(cid:89)
P(cid:0)W (cid:48)

r=1

(3.9)

(cid:1) (3.10)

kr m

I

vk m

Vm−1(cid:88)

r=1

(cid:88)

Vm(cid:89)

W(cid:48)

m

k=1

P (vm|vm−1) =

vr m−1W (cid:48)

kr m > 0

and P (vl|vl−1  Wij l)  which is deﬁned identically to P (vl|vl−1)  except that the summation is
performed over all conﬁgurations in which Wij l is ﬁxed (i.e.  W(cid:48)
ij l = Wij l) and we set

l : W (cid:48)

4

P (Wij l) = 1. Now we can write recursively P (v1) = P (v1|v0 = x)

∀m ∈ {2  ..  l − 1} : P (vm) =

P (vl|Wij l) =

∀m ∈ {l + 1  l + 2  ..  L} : P (vm|Wij l) =

(cid:88)
(cid:88)
(cid:88)

vl−1

vm−1

P (vm|vm−1) P (vm−1)

P (vl|vl−1  Wij l) P (vl−1)

(3.11)

(3.12)

P (vm|vm−1) P (vm−1|Wij l)

(3.13)

Thus we obtain the result of Eq. 3.9  through P (y|Wij l) = P (vL = y|Wij l). However  this
computation is still generally intractable  since all of the above summations (Eqs. 3.10-3.13) are still
over an exponential number of values. Therefore  we need to make one additional approximation.

vm−1

3.4 Approximation 2: large fan-in

(cid:112)

Km ∼ N (µm  Σm) .

Next we simplify the above summations (Eqs. 3.10-3.13) assuming that the neuronal fan-in is
“large”. We keep in mind that i  j and l are the speciﬁc indices of the ﬁxed weight Wij l. All
the other weights beside Wij l can be treated as independent random variables  due to the mean ﬁeld
approximation (Eq. 3.5). Therefore  in the limit of a inﬁnite neuronal fan-in (∀m : Km → ∞) we
can use the Central Limit Theorem (CLT) and say that the normalized input to each neuronal layer 
is distributed according to a Gaussian distribution
∀m : um = Wmvm−1/

(3.14)
Since Km is actually ﬁnite  this would be only an approximation - though a quite common and
effective one (e.g.  [21]). Using the approximation in Eq. 3.14 together with vm = sign (um) (Eq.
2.1) we can calculate (appendix B) the distribution of um and vm sequentially for all the layers
m ∈ {1  . . .   L}  for any given value of v0 and Wij l. These effectively simplify the summations in
3.10-3.13 using Gaussian integrals (appendix B).
At the end of this “forward pass” we will be able to ﬁnd P (y|Wij l) = P (vL = y|Wij l)   ∀i  j  l.
This takes a polynomial number of steps (appendix B.3)  instead of a direct calculation through
Eqs. 3.11-3.13  which is exponentially hard. Using P (y|Wij l) and Eq. 3.7 we can now update the
distribution of P (Wij l). This immediately gives the Bayes estimate of the weights (Eq. 3.6) and
outputs (Eq. 3.2).
As we note in appendix B.3  the computational complexity of the forward pass is signiﬁcantly lower
if Σm is diagonal. This is true exactly only in special cases. For example  this is true if all hidden
neurons have a fan-out of one - such as in a 2-layer network with a single output. However  in order
to reduce the computational complexity in cases that Σm is not diagonal  we will approximate the
distribution of um with its factorized (‘mean-ﬁeld’) version. Recall that the optimal factor is the
marginal of the distribution (appendix A). Therefore  we can now ﬁnd P (y|Wij l) easily (appendix
B.1)  as all the off-diagonal components in Σm are zero  so Σkk(cid:48) m = σ2
A direct calculation of P (vL = y|Wij l) for every i  j  l would be computationally wasteful  since
In order to improve the algorithm’s efﬁciency 
we will repeat similar calculations many times.
we again exploit the fact that Kl is large. We approximate ln P (vL = y|Wij l) using a Taylor
expansion of Wij l around its mean  (cid:104)Wij l(cid:105)  to ﬁrst order in K
. The ﬁrst order terms in this
expansion can be calculated using backward propagation of derivative terms

k mδkk(cid:48) .

−1/2
l

(3.15)
similarly to the BP algorithm (appendix C). Thus  after a forward pass for m = 1  . . .   L  and a
backward pass for l = L  . . .   1  we obtain P (vL = y|Wij l) for all Wij l and update P (Wij l).

∆k m = ∂ ln P (vL = y) /∂µk m  

4 The Expectation Backpropagation Algorithm
Using our results we can efﬁciently update the posterior distribution P (Wij l|Dn) for all the weights
with O (|W|) operations  according to Eqs. 3.7. Next  we summarize the resulting general algorithm
- the Expectation BackPropgation (EBP) algorithm. In appendix D  we exemplify how to apply the

5

algorithm in the special cases of MNNs with binary  ternary or real (continuous) weights. Similarly
to the original BP algorithm (see review in [15])  given input x and desired output y  ﬁrst we perform
a forward pass to calculate the mean output (cid:104)vl(cid:105) for each layer. Then we perform a backward pass
to update P (Wij l|Dn) for all the weights.

Forward pass
In this pass we perform the forward calculation of probabilities  as in Eq. 3.11.
Recall that (cid:104)Wkr m(cid:105) is the mean of the posterior distribution P (Wkr m|Dn). We ﬁrst initialize the
MNN input (cid:104)vk 0(cid:105) = xk for all k and calculate recursively the following quantities for m = 1  . . .   L
and all k

Vm−1(cid:88)
Vm−1(cid:88)
(cid:10)W 2

r=1

r=1

µk m =

1√
Km

σ2
k m =

1
Km

(cid:104)Wkr m(cid:105)(cid:104)vr m−1(cid:105)

; (cid:104)vk m(cid:105) = 2Φ (µk m/σk m) − 1 .

(cid:16)(cid:104)vr m−1(cid:105)2 − 1

(cid:17)

(cid:17) − (cid:104)Wkr m(cid:105)2 (cid:104)vr m−1(cid:105)2  

+ 1

(cid:11)(cid:16)

kr m

δm 1

(4.1)

(4.2)

where µm and σ2
and (cid:104)vm(cid:105) is the resulting mean of the output of layer m.

m are  respectively  the mean and variance of um  the input of layer m (Eq. 3.14) 

Backward pass
expansion. Recall Eq. 3.15. We ﬁrst initialize4

In this pass we perform the Bayes update of the posterior (Eq. 3.7) using a Taylor

∆i L = yi

(cid:1)

i L
Φ (yiµi L/σi L)

N(cid:0)0|µi L  σ2
(cid:1) Vm(cid:88)
N(cid:0)0|µi l−1  σ2

i l−1

.

(4.3)

(4.4)

(4.5)

for all i. Then  for l = L  . . .   1 and ∀i  j we calculate

∆i l−1 =

2√
Kl

ln P (Wij l|Dn) = ln P (Wij l|Dn−1) +

(cid:104)Wji l(cid:105) ∆j l .

j=1

Wij l∆i l (cid:104)vj l−1(cid:105) + C  

1√
Kl

where C is some unimportant constant (which does not depend on Wij l).

Output Using the posterior distribution  the optimal conﬁguration can be immediately found
through the MAP weights estimate (Eq. 3.6) ∀i  j  l

W ∗

ij l = argmaxWij l∈Sij l

(4.6)
The output of a MNN implementing these weights would be g (x W∗) (see Eq. 2.2). We deﬁne this
to be the ‘deterministic‘ EBP output (EBP-D).
Additionally  the MAP output (Eq. 3.2) can be calculated directly

ln P (Wij l|Dn) .

(cid:34)(cid:88)

ln

k

(cid:18) 1 + (cid:104)vk L(cid:105)

1 − (cid:104)vk L(cid:105)

(cid:19)yk(cid:35)

(4.7)

y∗ = argmaxy∈Y ln P (vL = y) = argmaxy∈Y

using (cid:104)vk L(cid:105) from Eq. 4.1  or as an ensemble average over the outputs of all possible MNN with the
weights Wij l being sampled from the estimated posterior P (Wij l|Dn). We deﬁne the output in Eq.
4.7 to be the Probabilistic EBP output (EBP-P). Note that in the case of a single output Y = {−1  1} 
so this output simpliﬁes to y = sign ((cid:104)vk L(cid:105)).

4Due to numerical inaccuracy  calculating ∆i L using Eq. 4.3 can generate nonsensical values (±∞  NaN)

if |µi L/σi L| becomes to large. If this happens  we use instead the asymptotic form in that limit

√
∆i L = − µi L
KL

σ2

i L

I {yiµi L < 0}

6

5 Numerical Experiments

We use several high dimensional text datasets to assess the performance of the EBP algorithm in
a supervised binary classiﬁcation task. The datasets (taken from [6]) contain eight binary tasks
from four datasets: ‘Amazon (sentiment)’  ‘20 Newsgroups’  ‘Reuters’ and ‘Spam or Ham’. Data
speciﬁcation (N =#examples and M =#features) and results (for each algorithm) are described in
Table 1. More details on the data including data extraction and labeling can be found in [6].
We test the performance of EBP on MNNs with a 2-layer architecture of M → 120 → 1  and
bias weights. We examine two special cases: (1) MNNs with real weights (2) MNNs with binary
weights (and real bias). Recall the motivation for the latter (section 1) is that they can be efﬁciently
implemented in hardware (real bias has negligible costs). Recall also that for each type of MNN  the
algorithm gives two outputs - EBP-D (deterministic) and EBP-P (probabilistic)  as explained near
Eqs. 4.6-4.7.
To evaluate our results we compare EBP to: (1) the AROW algorithm  which reports state-of-the-art
results on the tested datasets [6] (2) the traditional Backpropagation (BP) algorithm  used to train an
M → 120 → 1 MNN with real weights. In the latter case  we used both Cross Entropy (CE) and
Mean Square Error (MSE) as loss functions. On each dataset we report the results of BP with the
loss function which achieved the minimal error. We use a simple parameter scan for both AROW
(regularization parameter) and the traditional BP (learning rate parameter). Only the results with
the optimal parameters (i.e.  achieving best results) are reported in Table 1. The optimal parameters
found were never at the edges of the scanned ﬁeld. Lastly  to demonstrate the destructive effect of
naive quantization  we also report the performance of the BP-trained MNNs  after all the weights
(except the bias) were clipped using a sign function.
During training the datasets were repeatedly presented in three epochs (in all algorithms  additional
epochs did not reduce test error). On each epoch the examples were shufﬂed at random order for BP
and EBP (AROW determines its own order). The test results are calculated after each epoch using
8-fold cross-validation  similarly to [6]. Empirically  EBP running time is similar to BP with real
weights  and twice slower with binary weights. For additional implementation details  see appendix
E.1. The code is available online 5.
The minimal values achieved over all three epochs are summarized in Table 1. As can be seen  in all
datasets EBP-P performs better then AROW  which performs better then BP. Also  EBP-P usually
perfroms better with binary weights. In appendix E.2 we show that this ranking remains true even if
the fan-in is small (in contrast to our assumptions)  or if a deeper 3-layer architecture is used.

Dataset

#Examples

Reuters news I6
Reuters news I8
Spam or ham d0
Spam or ham d1

20News group comp vs HW
20News group elec vs med

Amazon Book reviews
Amazon DVD reviews

2000
2000
2500
2500
1943
1971
3880
3880

#Features
11463
12167
26580
27523
29409
38699
221972
238739

Real EBP-D

14.5%
15.65%
1.28%
1.0%
5.06%
3.36%
2.14%
2.06%

Real EBP-P
11.35%
15.25%
1.11%
0.96%
4.96%
3.15%
2.09%
2.14%

Binary EBP-D

Binary EBP-P

21.7%
23.15%
7.93%
3.85%
7.54%
6.0%
2.45%
5.72%

9.95%
16.4%
0.76%
0.96%
4.44%
2.08%
2.01%
2.27%

BP

AROW
11.72% 13.3%
15.27% 18.2%
1.12% 1.32%
1.4%
1.36%
5.79% 7.02%
2.74% 3.96%
2.24% 2.96%
2.63% 2.94%

Clipped BP
26.15%
26.4%
7.97%
7.33%
13.07%
14.23%
3.81%
5.15%

Table 1: Data speciﬁcation  and test errors (with 8-fold cross-validation). Best results are boldfaced.

6 Discussion

Motivated by the recent success of MNNs  we developed the Expectation BackPropagation algo-
rithm (EBP - see section 4) for approximate Bayesian inference of the synaptic weights of a MNN.
Given a supervised classiﬁcation task with labeled training data and a prior over the weights  this
deterministic online algorithm can be used to train deterministic MNNs (Eq. 2.2) without the need
to tune learning parameters (e.g.  learning rate). Furthermore  each synaptic weight can be restricted
to some set - which can be either ﬁnite (e.g.  binary numbers) or inﬁnite (e.g.  real numbers). This
opens the possibility of implementing trained MNNs in power-efﬁcient hardware devices requiring
limited parameter precision.

5https://github.com/ExpectationBackpropagation/EBP_Matlab_Code/

7

This algorithm is essentially an analytic approximation to the intractable Bayes calculation of the
posterior distribution of the weights after the arrival of a new data point. To simplify the intractable
Bayes update rule we use several approximations. First  we approximate the posterior using a prod-
uct of its marginals - a ‘mean ﬁeld’ approximation. Second  we assume the neuronal layers have a
large fan-in  so we can approximate them as Gaussian. After these two approximations each Bayes
update can be tractably calculated in polynomial time in the size of the MNN. However  in order to
further improve computational complexity (to O (|W|) in each step  like BP)  we make two addi-
tional approximations. First  we use the large fan-in to perform a ﬁrst order expansion. Second  we
optionally6 perform a second ‘mean ﬁeld’ approximation - to the distribution of the neuronal inputs.
Finally  after we obtain the approximated posterior using the algorithm  the Bayes estimates of the
most probable weights and the outputs are found analytically.
Previous approaches to obtain these Bayes estimates were too limited for our purposes. The Monte
Carlo approach [20] achieves state-of-the-art performance for small MNNs [25]  but does not scale
well [24]. The Laplace approximation [16] and variational Bayes [9?   8] based methods re-
quire continuous-valued weights  tuning of the learning rate parameter  and stochastic neurons (to
“smooth” the likelihood). Previous EP [23  21] and message passing [5  1] (a special case of EP[4])
based methods were derived only for SNNs.
In contrast  the EBP allows parameter free and scalable training of various types of MNNs (deter-
ministic or stochastic) with discrete (e.g.  binary) or continuous weights. In appendix F  we see that
for continuous weights EBP is almost identical to standard BP with a speciﬁc choice of activation
function s (x) = 2Φ (x) − 1  CE loss and learning rate η = 1. The only difference is that the input
is normalized by its standard deviation (Eq. 4.1  right)  which depends on the weights and inputs
(Eq. 4.2). This re-scaling makes the learning algorithm invariant to the amplitude changes in the
neuronal input. This results from the same invariance of the sign activation functions. Note that in
standard BP algorithm the performance is directly affected by the amplitude of the input  so it is a
recommended practice to re-scale it in pre-processing [15].
We numerically evaluated the algorithm on binary classiﬁcation tasks using MNNs with two or three
synaptic layers. In all data sets and MNNs EBP performs better than standard BP with the optimal
constant learning rate  and even achieves state-of-the-art results in comparison to [6]. Surprisingly 
EBP usually performs best when it is used to train binary MNNs. As suggested by a reviewer  this
could be related to the type of problems examined here. In text classiﬁcation tasks have large sparse
input spaces (bag of words)  and presence/absence of features (words) is more important than their
real values (frequencies). Therefore  (distributions over) binary weights and a threshold activation
function may work well.
In order to get such a good performance in binary MNNs  one must average over the output the
inferred (approximate) posterior of the weights. The EBP-P output of the algorithm calculates this
average analytically. In hardware this output could be realizable by averaging the output of several
binary MNNs  by sampling weights from P (Wij l|Dn). This can be done efﬁciently (appendix G).
Our numerical testing mainly focused on high-dimensional text classiﬁcation tasks  where shallow
architectures seem to work quite well. In other domains  such as vision [13] and speech [7]  deep
architectures achieve state-of-the-art performance. Such deep MNNs usually require considerable
ﬁne-tuning and additional ‘tricks’ such as unsupervised pre-training [7] or weight sharing [13].
Integrating such methods into EBP and using it to train deep MNNs is a promising direction for
future work. Another important generalization of the algorithm  which is rather straightforward  is to
use activation functions other than sign (·). This is particularly important for the last layer - where a
linear activation function would be useful for regression tasks  and joint activation functions7 would
be useful for multi-class tasks[3].

Acknowledgments The authors are grateful to C. Baldassi  A. Braunstein and R. Zecchina for
helpful discussions and to A. Hallak  T. Knafo and U. Sümbül for reviewing parts of this manuscript.
The research was partially funded by the Technion V.P.R. fund  by the Intel Collaborative Research
Institute for Computational Intelligence (ICRI-CI)  and by the Gruss Lipper Charitable Foundation.

6This approximation is not required if all neurons in the MNN have a fan-out of one.
7i.e.  activation functions for which (f (x))i (cid:54)= f (xi)  such as softmax or argmax.

8

References
[1] C Baldassi  A Braunstein  N Brunel  and R Zecchina. Efﬁcient supervised learning in networks with

binary synapses. PNAS  104(26):11079–84  2007.

[2] R Battiti and G Tecchiolli. Training neural nets with the reactive tabu search. IEEE transactions on neural

networks  6(5):1185–200  1995.

[3] C M Bishop. Neural networks for pattern recognition. Oxford  1995.
[4] C M Bishop. Pattern recognition and machine learning. Springer  Singapore  2006.
[5] A Braunstein and R Zecchina. Learning by message passing in networks of discrete synapses. Physical

review letters  96(3)  2006.

[6] K Crammer  A Kulesza  and M Dredze. Adaptive regularization of weight vectors. Machine Learning 

91(2):155–187  March 2013.

[7] G E Dahl  D Yu  L Deng  and A Acero. Context-Dependent Pre-Trained Deep Neural Networks for

Large-Vocabulary Speech Recognition. Audio  Speech  and Language Processing  20(1):30–42  2012.

[8] A Graves. Practical variational inference for neural networks. Advances in Neural Information Processing

Systems  pages 1–9  2011.

[9] G E Hinton and D Van Camp. Keeping the neural networks simple by minimizing the description length

of the weights. In COLT ’93  1993.

[10] G E Hinton  L Deng  D Yu  G E Dahl  A R Mohamed  N Jaitly  A Senior  V Vanhoucke  P Nguyen  T N
Sainath  and B Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared
views of four research groups. Signal Processing Magazine  IEEE  29(6):82–97  2012.

[11] K Hornik.

Approximation capabilities of multilayer feedforward networks.

4(1989):251–257  1991.

Neural networks 

[12] R Karakiewicz  R Genov  and G Cauwenberghs. 1.1 TMACS/mW Fine-Grained Stochastic Resonant

Charge-Recycling Array Processor. IEEE Sensors Journal  12(4):785–792  2012.

[13] A Krizhevsky  I Sutskever  and G E Hinton.

networks. In NIPS  2012.

Imagenet classiﬁcation with deep convolutional neural

[14] Y LeCun and L Bottou. Gradient-based learning applied to document recognition. Proceedings of the

IEEE  86(11):2278–2324  1998.

[15] Y LeCun  L Bottou  G B Orr  and K R Müller. Efﬁcient Backprop. In G Montavon  G B Orr  and K-R

Müller  editors  Neural networks: Tricks of the Trade. Springer  Heidelberg  2nd edition  2012.

[16] D J C MacKay. A practical Bayesian framework for backpropagation networks. Neural computation 

472(1):448–472  1992.

[17] E Mayoraz and F Aviolat. Constructive training methods for feedforward neural networks with binary

weights. International journal of neural systems  7(2):149–66  1996.

[18] T P Minka. Expectation Propagation for Approximate Bayesian Inference. NIPS  pages 362–369  2001.
[19] P Moerland and E Fiesler. Neural Network Adaptations to Hardware Implementations. In Handbook of

neural computation. Oxford University Press  New York  1997.

[20] R M Neal. Bayesian learning for neural networks. PhD thesis  University of Toronto  1995.
[21] F Ribeiro and M Opper. Expectation propagation with factorizing distributions: a Gaussian approximation

and performance results for simple models. Neural computation  23(4):1047–69  April 2011.

[22] D Saad and E Marom. Training Feed Forward Nets with Binary Weights Via a Modiﬁed CHIR Algorithm.

Complex Systems  4:573–586  1990.

[23] S A Solla and O Winther. Optimal perceptron learning: an online Bayesian approach. In On-Line Learning

in Neural Networks. Cambridge University Press  Cambridge  1998.

[24] N Srivastava and G E Hinton. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting.

Journal of Machine Learning  15:1929–1958  2014.

[25] H Y Xiong  Y Barash  and B J Frey. Bayesian prediction of tissue-regulated splicing using RNA sequence

and cellular context. Bioinformatics (Oxford  England)  27(18):2554–62  October 2011.

9

,Dino Sejdinovic
Arthur Gretton
Wicher Bergsma
Daniel Soudry
Ron Meir
Yarin Gal
Zoubin Ghahramani