2017,Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks,Collecting large training datasets  annotated with high-quality labels  is costly and time-consuming. This paper proposes a novel framework for training deep convolutional neural networks from noisy labeled datasets that can be obtained cheaply. The problem is formulated using an undirected graphical model that represents the relationship between noisy and clean labels  trained in a semi-supervised setting. In our formulation  the inference over latent clean labels is tractable and is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.,Toward Robustness against Label Noise in

Training Deep Discriminative Neural Networks

Arash Vahdat

D-Wave Systems Inc.
Burnaby  BC  Canada

avahdat@dwavesys.com

Abstract

Collecting large training datasets  annotated with high-quality labels  is costly
and time-consuming. This paper proposes a novel framework for training deep
convolutional neural networks from noisy labeled datasets that can be obtained
cheaply. The problem is formulated using an undirected graphical model that
represents the relationship between noisy and clean labels  trained in a semi-
supervised setting. In our formulation  the inference over latent clean labels is
tractable and is regularized during training using auxiliary sources of information.
The proposed model is applied to the image labeling problem and is shown to be
effective in labeling unseen images as well as reducing label noise in training on
CIFAR-10 and MS COCO datasets.

1

Introduction

The availability of large annotated data collections such as ImageNet [1] is one of the key reasons
why deep convolutional neural networks (CNNs) have been successful in the image classiﬁcation
problem. However  collecting training data with such high-quality annotation is very costly and time
consuming. In some applications  annotators are required to be trained before identifying classes in
data  and feedback from many annotators is aggregated to reduce labeling error. On the other hand 
many inexpensive approaches for collecting labeled data exist  such as data mining on social media
websites  search engines  querying fewer annotators per instance  or the use of amateur annotators
instead of experts. However  all these low-cost approaches have one common side effect: label noise.
This paper tackles the problem of training deep CNNs for the image labeling task from datapoints
with noisy labels. Most previous work in this area has focused on modeling label noise for multiclass
classiﬁcation1 using a directed graphical model similar to Fig. 1.a. It is typically assumed that the
clean labels are hidden during training  and they are marginalized by enumerating all possible classes.
These techniques cannot be extended to the multilabel classiﬁcation problem  where exponentially
many conﬁgurations exist for labels  and the explaining-away phenomenon makes inference over
latent clean labels difﬁcult.
We propose a conditional random ﬁeld (CRF) [2] model to represent the relationship between noisy
and clean labels  and we show how modern deep CNNs can gain robustness against label noise using
our proposed structure. We model the clean labels as latent variables during training  and we design
our structure such that the latent variables can be inferred efﬁciently.
The main challenge in modeling clean labels as latent is the lack of semantics on latent variables.
In other words  latent variables may not semantically correspond to the clean labels when the joint
probability of clean and noisy labels is parameterized such that latent clean labels can take any

1Each sample is assumed to belong to only one class.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

xxx

ˆyyy

(a)

yyy

xxx

ˆyyy

yyy

hhh

(b)

Figure 1: a) The general directed graphical model used for modeling noisy labels. xxx  ˆyyy  yyy represent
a data instance  its clean label  and its noisy label  respectively. b) We represent the interactions
between clean and noisy labels using an undirected graphical model with hidden binary random
variables (hhh).

conﬁguration. To solve this problem  most previous work relies on either carefully initializing the
conditionals [3]  ﬁne-tuning the model on the noisy set after pretraining on a clean set
[4]  or
regularizing the transition parameters [5]. In contrast  we inject semantics to the latent variables
by formulating the training problem as a semi-supervised learning problem  in which the model is
trained using a large set of noisy training examples and a small set of clean training examples. To
overcome the problem of inferring clean labels  we introduce a novel framework equipped with an
auxiliary distribution that represents the relation between noisy and clean labels while relying on
information sources different than the image content.
This paper makes the following contributions: i) A generic CRF model is proposed for training deep
neural networks that is robust against label noise. The model can be applied to both multiclass and
multilabel classiﬁcation problems  and it can be understood as a robust loss layer  which can be
plugged into any existing network. ii) We propose a novel objective function for training the deep
structured model that beneﬁts from sources of information representing the relation between clean
and noisy labels. iii) We demonstrate that the model outperforms previous techniques.

2 Previous Work

Learning from Noisy Labels: Learning discriminative models from noisy-labeled data is an active
area of research. A comprehensive overview of previous work in this area can be found in [6].
Previous research on modeling label noise can be grouped into two main groups: class-conditional
and class-and-instance-conditional label noise models. In the former group  the label noise is assumed
to be independent of the instance  and the transition probability from clean classes to the noisy classes
is modeled. For example  class conditional models for binary classiﬁcation problems are considered
in [7  8] whereas multiclass counterparts are targeted in [9  5]. In the class-and-instance-conditional
group  label noise is explicitly conditioned on each instance. For example  Xiao et al. [3] developed a
model in which the noisy observed annotation is conditioned on binary random variables indicating
if an instance’s label is mistaken. Reed et al. [10] ﬁxes noisy labels by “bootstrapping” on the
labels predicted by a neural network. These techniques are all applied to either binary or multiclass
classiﬁcation problems in which marginalization over classes is possible. Among methods proposed
for noise-robust training  Misra et al. [4] target the image multilabeling problem but model the label
noise for each label independently. In contrast  our proposed CRF model represents the relation
between all noisy and clean labels while the inference over latent clean labels is still tractable.
Many works have focused on semi-supervised learning using a small clean dataset combined with
noisy labeled data  typically obtained from the web. Zhu et al. [11] used a pairwise similarity measure
to propagate labels from labeled dataset to unlabeled one. Fergus et al. [12] proposed a graph-based
label propagation  and Chen and Gupta [13] employed the weighted cross entropy loss. Recently Veit
et al. [14] proposed a multi-task network containing i) a regression model that maps noisy labels and
image features to clean labels ii) an image classiﬁcation model that labels input. However  the model
in this paper is trained using a principled objective function that regularizes the inference model using
extra sources of information without the requirement for oversampling clean instances.
Deep Structured Models: Conditional random ﬁelds (CRFs) [2] are discriminative undirected
graphical models  originally proposed for modeling sequential and structured data. Recently  they have
shown state-of-the-art results in segmentation [15  16] when combined with deep neural networks [17 
18  19]. The main challenge in training deep CNN-CRFs is how to do inference and back-propagate
gradients of the loss function through the inference. Previous approaches have focused on mean-ﬁeld

2

approximation [16  20]  belief propagation [21  22]  unrolled inference [23  24]  and sampling [25].
The CNN-CRFs used in this work are extensions of hidden CRFs introduced in [26  27].

3 Robust Discriminative Neural Network

Our goal in this paper is to train deep neural networks given a set of noisy labeled data and a small
set of cleaned data. A datapoint (an image in our case) is represented by xxx  and its noisy annotation
by a binary vector yyy = {y1  y2  . . .   yN} ∈ YN   where yi ∈ {0  1} indicates whether the ith label is
present in the noisy annotation. We are interested in inferring a set of clean labels for each datapoint.
The clean labels may be deﬁned on a set different than the set of noisy labels. This is typically the
case in the image annotation problem where noisy labels obtained from user tags are deﬁned over a
large set of textual tags (e.g.  “cat”  “kitten  “kitty”  “puppy”  “pup”  etc.)  whereas clean labels are
deﬁned on a small set of representative labels (e.g.  “cat”  “dog”  etc.). In this paper  the clean label is
represented by a stochastic binary vector ˆyyy = {ˆy1  ˆy2  . . .   ˆyC} ∈ YC.
We use the CRF model shown in Fig. 1.b. In our formulation  both ˆyyy and yyy may conditionally depend
on the image xxx. The link between ˆyyy and yyy captures the correlations between clean and noisy labels.
These correlations help us infer latent clean labels when only the noisy labels are observed. Since
noisy labels are deﬁned over a large set of overlapping (e.g.  “cat” and “pet”) or co-occurring (e.g. 
“road” and “car”) entities  p(yyy|ˆyyy  xxx) may have a multimodal form. To keep the inference simple and
still be able to model these correlations  we introduce a set of hidden binary variables represented by
hhh ∈ H. In this case  the correlations between components of yyy are modeled through hhh. These hidden
variables are not connected to ˆyyy in order to keep the CRF graph bipartite.
The CRF model shown in Fig. 1.b deﬁnes the joint probability distribution of yyy  ˆyyy  and hhh conditioned
on xxx using a parameterized energy function Eθθθ : YN × YC × H × X → R. The energy function
assigns a potential score Eθθθ(yyy  ˆyyy  hhh  xxx) to the conﬁguration of (yyy  ˆyyy  hhh  xxx)  and is parameterized by a
parameter vector θθθ. This conditional probability distribution is deﬁned using a Boltzmann distribution:

pθθθ(yyy  ˆyyy  hhh|xxx) =

1

Zθθθ(xxx)

exp(−Eθθθ(yyy  ˆyyy  hhh  xxx))
(cid:88)
(cid:88)

(cid:88)

(1)

exp(−Eθθθ(yyy  ˆyyy  hhh  xxx)). The

where Zθθθ(xxx) is the partition function deﬁned by Zθθθ(xxx) =

yyy∈YN
energy function in Fig. 1.b is deﬁned by the quadratic function:

ˆyyy∈YC

hhh∈H

Eθθθ(yyy  ˆyyy  hhh  xxx) = −aaaT

φφφ (xxx)ˆyyy − bbbT

φφφ (xxx)yyy − cccT hhh − ˆyyyT WWWyyy − hhhT W (cid:48)W (cid:48)W (cid:48)yyy

(2)

where the vectors aaaφφφ(xxx)  bbbφφφ(xxx)  ccc are the bias terms and the matrices WWW and W (cid:48)W (cid:48)W (cid:48) are the pairwise
interactions. In our formulation  the bias terms on the clean and noisy labels are functions of input
xxx and are deﬁned using a deep CNN parameterized by φφφ. The deep neural network together with
the introduced CRF forms our CNN-CRF model  parameterized by θθθ = {φφφ  ccc  WWW   W (cid:48)W (cid:48)W (cid:48)}. Note that in
order to regularize WWW and W (cid:48)W (cid:48)W (cid:48)  these matrices are not a function of xxx.
The structure of this graph is designed such that the conditional distribution pθθθ(ˆyyy  hhh|yyy  xxx) takes
(cid:81)
i pθθθ(ˆyi|yyy  xxx)(cid:81)
a simple factorial distribution that can be calculated analytically given θθθ using: pθθθ(ˆyyy  hhh|yyy  xxx) =
j pθθθ(hj|yyy) where pθθθ(ˆyi = 1|yyy  xxx) = σ(aaaφφφ(xxx)(i) +WWW (i :)yyy)  pθθθ(hj|yyy) = σ(ccc(j) +WWW
(cid:48)
(j :)yyy) 
1+exp(−u) is the logistic function  and aaaφφφ(xxx)(i) or WWW (i :) indicate the ith element

in which σ(u) =
and row in the corresponding vector or matrix respectively.

1

3.1 Semi-Supervised Learning Approach

The main challenge here is how to train the parameters of the CNN-CRF model deﬁned in Eq. 1. To
tackle this problem  we deﬁne the training problem as a semi-supervised learning problem where
clean labels are observed in a small subset of a larger training set annotated with noisy labels. In this
case  one can form an objective function by combining the marginal data likelihood deﬁned on both
the fully labeled clean set and noisy labeled set  and using the maximum likelihood method to learn
the parameters of the model. Assume that DN = {(xxx(n)  yyy(n))} and DC = {(xxx(c)  ˆyyy(c)  yyy(c))} are two
disjoint sets representing the noisy labeled and clean labeled training datasets respectively. In the

3

maximum likelihood method  the parameters are trained by maximizing the marginal log likelihood:

log pθθθ(yyy(c)  ˆyyy(c)|xxx(c))

(3)

max

where pθθθ(yyy(n)|xxx(n)) =(cid:80)

θθθ

1
|DN|

(cid:88)
yyy hhh pθθθ(yyy(n)  yyy  hhh|xxx(n)) and pθθθ(yyy(c)  ˆyyy(c)|xxx(c)) =(cid:80)

log pθθθ(yyy(n)|xxx(n)) +

(cid:88)

1
|DC|

n

c

hhh pθθθ(yyy(c)  ˆyyy(c)  hhh|xxx(c)). Due
to the marginalization of hidden variables in log terms  the objective function cannot be analytically
optimized. A common approach to optimizing the log marginals is to use the stochastic maximum
likelihood method which is also known as persistent contrastive divergence (PCD) [28  29  25].
The stochastic maximum likelihood method  or equivalently PCD  can be fundamentally viewed
as an Expectation-Maximization (EM) approach to training. The EM algorithm maximizes the
variational lower bound that is formed by subtracting the Kullback–Leibler (KL) divergence between
a variational approximating distribution q and the true conditional distribution from the log marginal
probability. For example  consider the bound for the ﬁrst term in the objective function:

log pθθθ(yyy|xxx) ≥ log pθθθ(yyy|xxx) − KL[q(ˆyyy  hhh|yyy  xxx)||pθθθ(ˆyyy  hhh|yyy  xxx)]

= Eq(ˆyyy hhh|yyy xxx)[log pθθθ(yyy  ˆyyy  hhh|xxx)] − Eq(ˆyyy hhh|yyy xxx)[log q(ˆyyy  hhh|yyy  xxx)] = Uθθθ(xxx  yyy).

(4)
(5)
If the incremental EM approach[30] is taken for training the parameters θθθ  the lower bound Uθθθ(xxx  yyy)
is maximized over the noisy training set by iterating between two steps. In the Expectation step
(E step)  θθθ is ﬁxed and the lower bound is optimized with respect to the conditional distribution
q(ˆyyy  hhh|yyy  xxx). Since this distribution is only present in the KL term in Eq. 4  the lower bound
is maximized simply by setting q(ˆyyy  hhh|yyy  xxx) to the analytic pθθθ(ˆyyy  hhh|yyy  xxx).
In the Maximization
step (M step)  q is ﬁxed  and the bound is maximized with respect to the model parameters θθθ 
which occurs only in the ﬁrst expectation term in Eq. 5. This expectation can be written as
Eq(ˆyyy hhh|yyy xxx)[−Eθθθ(yyy  ˆyyy  hhh  xxx)] − log Zθθθ(xxx)  which is maximized by updating θθθ in the direction of its
gradient  computed using −Eq(ˆyyy hhh|xxx yyy)[ ∂
∂θθθ Eθθθ(yyy  ˆyyy  hhh  xxx)]. Noting that
q(ˆyyy  hhh|yyy  xxx) is set to pθθθ(ˆyyy  hhh|yyy  xxx) in the E step  it becomes clear that the M step is equivalent to the
parameter updates in PCD.

∂θθθ Eθθθ(yyy  ˆyyy  hhh  xxx)] + Ep(yyy ˆyyy hhh|xxx)[ ∂

3.2 Semi-Supervised Learning Regularized by Auxiliary Distributions
The semi-supervised approach infers the latent variables using the conditional q(ˆyyy  hhh|yyy  xxx) =
pθθθ(ˆyyy  hhh|yyy  xxx). However  at the beginning of training when the model’s parameters are not trained yet 
sampling from the conditional distributions p(ˆyyy  hhh|yyy  xxx) does not necessarily generate the clean labels
accurately. The problem is more severe with the strong representation power of CNN-CRFs  as they
can easily ﬁt to poor conditional distributions that occur at the beginning of training. That is why the
impact of the noisy set on training must be reduced by oversampling clean instances [14  3].
In contrast  there may exist auxiliary sources of information that can be used to extract the relationship
between noisy and clean labels. For example  non-image-related sources may be formed from
semantic relatedness of labels [31]. We assume that  in using such sources  we can form an auxiliary
distribution paux(yyy  ˆyyy  hhh) representing the joint probability of noisy and clean labels and some hidden
binary states. Here  we propose a framework to use this distribution to train parameters in the semi-
supervised setting by guiding the variational distribution to infer the clean labels more accurately. To
do so  we add a new regularization term in the lower bound that penalizes the variational distribution
for being different from the conditional distribution resulting from the auxiliary distribution as
follows:
log pθθθ(yyy|xxx) ≥ U aux
(xxx  yyy) = log pθθθ(yyy|xxx)−KL[q(ˆyyy  hhh|yyy  xxx)||pθθθ(ˆyyy  hhh|yyy  xxx)]−αKL[q(ˆyyy  hhh|yyy  xxx)||paux(ˆyyy  hhh|yyy)]
where α is a non-negative scalar hyper-parameter that controls the impact of the added KL term.
Setting α = 0 recovers the original variational lower bound deﬁned in Eq. 4 whereas α → ∞ forces
the variational distribution q to ignore the pθθθ(ˆyyy  hhh|yyy  xxx) term. A value between these two extremes
makes the inference distribution intermediate between pθθθ(ˆyyy  hhh|yyy  xxx) and paux(ˆyyy  hhh|yyy). Note that this
new lower bound is actually looser than the original bound. This may be undesired if we were actually
interested in predicting noisy labels. However  our goal is to predict clean labels  and the proposed
framework beneﬁts from the regularization that is imposed on the variational distribution. Similar
ideas have been explored in the posterior regularization approach [32].
Similarly  we also deﬁne a new lower bound on the second log marginal in Eq. 3 by:

θθθ

log pθθθ(yyy  ˆyyy|xxx) ≥ Laux

θθθ

(xxx  yyy  ˆyyy) = log pθθθ(yyy  ˆyyy|xxx) − KL[q(hhh|yyy)||pθθθ(hhh|yyy)] − αKL[q(hhh|yyy)||paux(hhh|yyy)].

4

Auxiliary Distribution: In this paper  the auxiliary joint distribution paux(yyy  ˆyyy  hhh) is modeled by
an undirected graphical model in a special form of a restricted Boltzmann machine (RBM)  and is
trained on the clean training set. The structure of the RBM is similar to the CRF model shown in
Fig. 1.b with the fundamental difference that parameters of the model do not depend on xxx:

paux(yyy  ˆyyy  hhh) =

1

Zaux

exp(−Eaux(yyy  ˆyyy  hhh))

(6)

where the energy function is deﬁned by the quadratic function:

Eaux(yyy  ˆyyy  hhh) = −aaaT

(7)
and Zaux is the partition function  deﬁned similarly to the CRF’s partition function. The number of
hidden variables is set to 200 and the parameters of this generative model are trained using the PCD
algorithm [28]  and are ﬁxed while the CNN-CRF model is being trained.

auxyyy

auxyyy − cccT

auxhhh − ˆyyyT WWW auxyyy − hhhT W (cid:48)W (cid:48)W (cid:48)

auxˆyyy − bbbT

3.3 Training Robust CNN-CRF

In training  we seek θθθ that maximizes the proposed lower bounds on the noisy and clean training sets:

(cid:88)

n

(cid:88)

c

max

θθθ

1
|DN|

U aux

θθθ

(xxx(n)  yyy(n)) +

1
|DC|

Laux

θθθ

(xxx(c)  yyy(c)  ˆyyy(c)).

The optimization problem is solved in a two-step iterative procedure as follows:
E step: The objective function is optimized with respect to q(ˆyyy  hhh|yyy  xxx) for a ﬁxed θθθ. For U aux
this is done by solving the following problem:

θθθ

KL[q(ˆyyy  hhh|yyy  xxx)||pθθθ(ˆyyy  hhh|yyy  xxx)] + αKL[q(ˆyyy  hhh|yyy  xxx)||paux(ˆyyy  hhh|yyy)].

min

q

The weighted average of KL terms above is minimized with respect to q when:

(10)
which is a weighted geometric mean of the true conditional distribution and auxiliary distribution.
Given the factorial structure of these distributions  q(ˆyyy  hhh|yyy  xxx) is also a factorial distribution:

q(ˆyyy  hhh|yyy  xxx) ∝ [pθθθ(ˆyyy  hhh|yyy  xxx) · pα

aux(ˆyyy  hhh|yyy)]( 1

α+1 )  

(8)

(xxx  yyy) 

(9)

(cid:19)

(cid:19)

.

(11)

q(ˆyi = 1|yyy  xxx) = σ

q(hj = 1|yyy) = σ

(cid:18) 1
(cid:18) 1

α + 1

α + 1

(aaaφφφ(xxx)(i) + WWW (i :)yyy + αaaaaux(i) + αWWW aux(i :)yyy)

(ccc(j) + WWW

(cid:48)
(j :)yyy + αcccaux(j) + αWWW

(cid:48)
aux(j :)yyy)

Optimizing Laux

θθθ

(xxx  yyy  ˆyyy) w.r.t q(hhh|yyy) gives a similar factorial result:
α+1 ) .

q(hhh|yyy) ∝ [pθθθ(hhh|yyy) · pα

aux(hhh|yyy)]( 1

M step: Holding q ﬁxed  the objective function is optimized with respect to θθθ. This is achieved by
updating θθθ in the direction of the gradient of Eq(ˆyyy hhh|xxx yyy)[log pθθθ(yyy  ˆyyy  hhh|xxx)]  which is:

U aux

θθθ

∂
∂θθθ

(xxx  yyy) =

∂
∂θθθ

Eq(ˆyyy hhh|xxx yyy)[log pθθθ(yyy  ˆyyy  hhh|xxx)]

= −Eq(ˆyyy hhh|xxx yyy)[

∂
∂θθθ

Eθθθ(yyy  ˆyyy  hhh  xxx)] + Ep(yyy ˆyyy hhh|xxx)[

∂
∂θθθ

Eθθθ(yyy  ˆyyy  hhh  xxx)] 

(12)

where the ﬁrst expectation (the positive phase) is deﬁned under the variational distribution q and
the second expectation (the negative phase) is deﬁned under the CRF model p(yyy  ˆyyy  hhh|xxx). With the
factorial form of q  the ﬁrst expectation is analytically tractable. The second expectation is estimated
by PCD [28  29  25]. This approach requires maintaining a set of particles for each training instance
that are used for seeding the Markov chains at each iteration of training.
The gradient of the lower bound on the clean set is deﬁned similarly:

Laux

θθθ

∂
∂θθθ

(xxx  yyy  ˆyyy) =

Eq(hhh|yyy)[log pθθθ(yyy  ˆyyy  hhh|xxx)]

∂
∂θθθ

= −Eq(hhh|yyy)[

∂
∂θθθ

Eθθθ(yyy  ˆyyy  hhh  xxx)] + Ep(yyy ˆyyy hhh|xxx)[

∂
∂θθθ

Eθθθ(yyy  ˆyyy  hhh  xxx)]

(13)

5

with the minor difference that in the positive phase the clean label ˆyyy is given for each instance and the
variational distribution is deﬁned over only the hidden variables.
Scheduling ααα: Instead of setting α to a ﬁxed value during training  it is set to a very large value at
the beginning of training and is slowly decreased to smaller values. The rationale behind this is that at
the beginning of training  when pθθθ(ˆyyy  hhh|yyy  xxx) cannot predict the clean labels accurately  it is intuitive
to rely more on pretrained paux(ˆyyy  hhh|yyy) when inferring the latent variables. As training proceeds we
shift the variational distribution q more toward the true conditional distribution.
Algorithm 1 summarizes the learning procedure proposed for training our CRF-CNN. The training is
done end-to-end for both CNN and CRF parameters together. In the test time  samples generated by
Gibbs sampling from pθθθ(yyy  ˆyyy  hhh|xxx) for the test image xxx are used to compute the marginal pθθθ(ˆyyy|xxx).

Algorithm 1: Train robust CNN-CRF with simple gradient descent
: Noisy dataset DN and clean dataset DC  auxiliary distribution paux(yyy  ˆyyy  hhh)  a learning
Input
rate parameter ε and a schedule for α
Output :Model parameters: θθθ = {φφφ  ccc  WWW   W (cid:48)W (cid:48)W (cid:48)}
Initialize model parameters
while Stopping criteria is not met do

foreach minibatch {(xxx(n)  yyy(n))  (xxx(c)  ˆyyy(c)  yyy(c))} = getMinibatch(DN   DC) do

Compute q(ˆyyy  hhh|yyy(n)  xxx(n)) by Eq.10 for each noisy instance
Compute q(hhh|yyy(c)) by Eq. 11 for each clean instance
Do Gibbs sweeps to sample from the current pθθθ(yyy  ˆyyy  hhh|xxx(·)) for each clean/noisy instance
(mn  mc) ← (# noisy instances in minibatch  # clean instances in minibatch)

∂

∂θθθLaux

θθθ

(xxx(c)  yyy(c)  ˆyyy(c)) by Eq.12 and 13

θθθ ← θθθ + ε(cid:0) 1

(cid:80)

mn

n

(xxx(n)  yyy(n))(cid:1) + 1

(cid:80)

mc

c

∂

∂θθθU aux

θθθ

end

end

4 Experiments

In this section  we examine the proposed robust CNN-CRF model for the image labeling problem.

4.1 Microsoft COCO Dataset

The Microsoft COCO 2014 dataset is one of the largest publicly available datasets that contains both
noisy and clean object labels. Created from challenging Flickr images  it is annotated with 80 object
categories as well as captions describing the images. Following [4]  we use the 1000 most common
words in the captions as the set of noisy labels. We form a binary vector of this length for each
image representing the words present in the caption. We use 73 object categories as the set of clean
labels  and form binary vectors indicating whether the object categories are present in the image.
We follow the same 87K/20K/20K train/validation/test split as [4]  and use mean average precision
(mAP) measure over these 73 object categories as the performance assessment. Finally  we use 20%
of the training data as the clean labeled training set (DC). The rest of data was used as the noisy
training set (DN )  in which clean labels were ignored in training.
Network Architectures: We use the implementation of ResNet-50 [33] and VGG-16 [34] in Tensor-
Flow as the neural networks that compute the bias coefﬁcients in the energy function of our CRF
(Eq. 2). These two networks are applied in a fully convolutional setting to each image. Their features
in the ﬁnal layer are pooled in the spatial domain using an average pooling operation  and these are
passed through a fully connected linear layer to generate the bias terms. VGG-16 is used intentionally
in order to compare our method directly with [4] that uses the same network. ResNet-50 experiments
enable us to examine how our model works with other modern architectures. Misra et al. [4] have
reported results when the images were upsampled to 565 pixels. Using upsampled images improves
the performance signiﬁcantly  but they make cross validation signiﬁcantly slower. Here  we report
our results for image sizes of both 224 (small) and 565 pixels (large).
Parameters Update: The parameters of all the networks were initialized from ImageNet-trained
models that are provided in TensorFlow. The other terms in the energy function of our CRF were all

6

xxx

ˆyyy

xxx

yyy

xxx

xxx

xxx

xxx

ˆyyy

yyy

ˆyyy

yyy

ˆyyy

yyy

hhh

(a) Clean

(b) Noisy

(c) No link

(d) CRF w/o hhh

(e) CRF w/ hhh

yyy

ˆyyy
hhh
(f) CRF w/o xxx − yyy

Figure 2: Visualization of different variations of the model examined in the experiments.

initialized to zero. Our gradient estimates can be high variance as they are based on a Monte Carlo
estimate. For training  we use Adam [35] updates that are shown to be robust against noisy gradients.
The learning rate and epsilon for the optimizer are set to (0.001  1) and (0.0003  0.1) respectively in
VGG-16 and ResNet-50. We anneal α from 40 to 5 in 11 epochs.
Sampling Overhead: Fifty Markov chains per datapoint are maintained for PCD. In each iteration
of the training  the chains are retrieved for the instances in the current minibatch  and 100 iterations
of Gibbs sampling are applied for negative phase samples. After parameter updates  the ﬁnal state of
chains is stored in memory for the next epoch. Note that we are only required to store the state of the
chains for either (ˆyyy  hhh) or yyy. In this experiment  since the size of hhh is 200  the former case is more
memory efﬁcient. Storing persistent chains in this dataset requires only about 1 GB of memory. In
ResNet-50  sampling increases the training time only by 16% and 8% for small and large images
respectively. The overhead is 9% and 5% for small and large images in VGG-16.
Baselines: Our proposed method is compared against several baselines visualized in Fig. 2:

with the all clean labels. This deﬁnes a performance upper bound for each network.

• Cross entropy loss with clean labels: The networks are trained using cross entropy loss
• Cross entropy loss with noisy labels: The model is trained using only noisy labels. Then 
predictions on the noisy labels are mapped to clean labels using the manual mapping in [4].
• No pairwise terms: All the pairwise terms are removed and the model is trained using
• CRF without hidden: WWW is trained but WWW
• CRF with hidden: Both WWW and WWW
• CRF without xxx − yyy link: Same as the previous model but bbb is not a function of xxx.
• CRF without xxx − yyy link (α = 0
α = 0): Same as the previous model but trained with α = 0.
α = 0

analytic gradients without any sampling using our proposed objective function in Eq. 8.

(cid:48) is omitted from the model.

(cid:48) are present in the model.

The experimental results are reported in Table 1 under “Caption Labels.” A performance increase is
observed after adding each component to the model. However  removing the xxx − yyy link generally
improves the performance signiﬁcantly. This may be because removing this link forces the model
to rely on ˆyyy and its correlations with yyy for predicting yyy on the noisy labeled set. This can translate
to better recognition of clean labels. Last but not least  the CRF model with no xxx − yyy connection
trained using α = 0 performed very poorly on this dataset. This demonstrates the importance of the
introduced regularization in training.

4.2 Microsoft COCO Dataset with Flickr Tags

The images in the COCO dataset were originally gathered and annotated from the Flickr website. This
means that these image have actual noisy Flickr tags. To examine the performance of our model on
actual noisy labels  we collected these tags for the COCO images using Flickr’s public API. Similar
to the previous section  we used the 1024 most common tags as the set of noisy labels. We observed
that these tags have signiﬁcantly more noise compared to the noisy labels in the previous section;
therefore  it is more challenging to predict clean labels from them using the auxiliary distribution.
In this section  we only examine the ResNet-50 architecture for both small and large image sizes.
The different baselines introduced in the previous section are compared against each other in Table 1
under “Flickr Tags.”
Auxiliary Distribution vs. Variational Distribution: As the auxiliary distribution paux is ﬁxed 
and the variational distribution q is updated using Eq. 10 in each iteration  a natural question is how

7

Table 1: The performance of different baselines on the COCO dataset in terms of mAP (%).

Caption Labels (Sec. 4.1)

Flickr Tags (Sec. 4.2)

ResNet-50

Baseline
Cross entropy loss w/ clean
Cross entropy loss w/ noisy
No pairwise link
CRF w/o hidden
CRF w/ hidden
CRF w/o xxx − yyy link
CRF w/o xxx − yyy link (α = 0)
Misra et al. [4]
Fang et al. [36] reported in [4]

ResNet-50

Small Large
68.57
78.38
64.13
56.88
73.19
63.67
73.23
64.26
74.04
65.73
66.61
75.00
56.53
48.53

-
-

-
-

VGG-16

Small Large
71.99
75.50
62.75
58.59
71.78
66.18
71.78
67.73
71.92
68.35
69.89
73.16
56.39
56.76
66.8
63.7

-
-

Small
68.57

58.01
59.04
59.19
60.97
47.25

-

-
-

Large
78.38

67.84
67.22
67.33
67.57
58.74

-

-
-

q differs from paux. Since  we have access to the clean labels in the COCO dataset  we examine
the accuracy of q in terms of predicting clean labels on the noisy training set (DN ) using the mAP
measurement at the beginning and end of training the CRF-CNN model (ResNet-50 on large images).
We observed that at the beginning of training  when α is big  q is almost equal to paux  which obtains
49.4% mAP on this set. As training iterations proceed  the accuracy of q increases to 69.4% mAP.
Note that the 20.0% gain in terms of mAP is very signiﬁcant  and it demonstrates that combining the
auxiliary distribution with our proposed CRF can yield a signiﬁcant performance gain in inferring
latent clean labels. In other words  our proposed model is capable of cleaning the noisy labels
and proposing more accurate labels on the noisy set as training continues. Please refer to our
supplementary material for a qualitative comparison between q and paux.

4.3 CIFAR-10 Dataset

We also apply our proposed learning framework to the object classiﬁcation problem in the CIFAR-10
dataset. This dataset contains images of 10 objects resized to 32x32-pixel images. We follow the
settings in [9] and we inject synthesized noise to the original labels in training. Moreover  we
implement the forward and backward losses proposed in [9] and we use them to train ResNet [33] of
depth 32 with the ground-truth noise transition matrix.
Here  we only train the variant of our model shown in Fig. 2.c that can be trained analytically. For
the auxiliary distribution  we trained a simple linear multinomial logistic regression representing the
conditional paux(ˆyyy|yyy) with no hidden variables (hhh) . We trained this distribution such that the output
probabilities match the ground-truth noise transition matrix. We trained all models for 200 epochs.
For our model  we anneal α from 8 to 1 in 10 epochs. Similar to the previous section  we empirically
observed that it is better to stop annealing α before it reaches zero. Here  to compare our method
with the previous work  we do not work in a semi-supervised setting  and we assume that we have
access only to the noisy training dataset.
Our goal for this experiment is to demonstrate that a simple variant of our model can be used for
training from images with only noisy labels and to show that our model can clean the noisy labels.
To do so  we report not only the average accuracy on the clean test dataset  but also the recovery
accuracy. The recovery accuracy for our method is deﬁned as the accuracy of q in predicting the
clean labels in the noisy training set at the end of learning. For the baselines  we measure the accuracy
of the trained neural network p(ˆyyy|xxx) on the same set. The results are reported in Table 2. Overall  our
method achieves slightly better prediction accuracy on the CIFAR-10 dataset than the baselines. And 
in terms of recovering clean labels on the noisy training set  our model signiﬁcantly outperforms the
baselines. Examples of the recovered clean labels are visualized for the CIFAR-10 experiment in the
supplementary material.

5 Conclusion

We have proposed a general undirected graphical model for modeling label noise in training deep
neural networks. We formulated the problem as a semi-supervised learning problem  and we proposed
a novel objective function equipped with a regularization term that helps our variational distribution

8

Table 2: Prediction and recovery accuracy of different baselines on the CIFAR-10 dataset.

Noise (%)
Cross entropy loss
Backward [9]
Forward [9]
Our model

10
91.2
87.4
90.9
91.6

Prediction Accuracy (%)

20
90.0
87.4
90.3
91.0

30
89.1
84.6
89.4
90.6

40
87.1
76.5
88.4
89.4

Recovery Accuracy (%)

50
80.2
45.6
80.0
84.3

10
94.1
88.0
94.6
97.7

20
92.4
87.4
93.6
96.4

30
89.6
84.0
92.3
95.1

40
85.2
75.3
91.1
93.5

50
74.6
44.0
83.1
88.1

infer latent clean labels more accurately using auxiliary sources of information. Our model not only
predicts clean labels on unseen instances more accurately  but also recovers clean labels on noisy
training sets with a higher precision. We believe the ability to clean noisy annotations is a very
valuable property of our framework that will be useful in many application domains.

Acknowledgments

The author thanks Jason Rolfe  William Macready  Zhengbing Bian  and Fabian Chudak for their
helpful discussions and comments. This work would not be possible without the excellent technical
support provided by Mani Ranjbar and Oren Shklarsky.

References
[1] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. ImageNet: A large-scale hierarchical

image database. In Computer Vision and Pattern Recognition (CVPR)  2009.

[2] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data. In International Conference on Machine Learning (ICML)  2001.

[3] Tong Xiao  Tian Xia  Yi Yang  Chang Huang  and Xiaogang Wang. Learning from massive noisy labeled

data for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR)  2015.

[4] Ishan Misra  C. Lawrence Zitnick  Margaret Mitchell  and Ross Girshick. Seeing through the human

reporting bias: Visual classiﬁers from noisy human-centric labels. In CVPR  2016.

[5] Sainbayar Sukhbaatar  Joan Bruna  Manohar Paluri  Lubomir Bourdev  and Rob Fergus. Training convolu-

tional networks with noisy labels. arXiv preprint arXiv:1406.2080  2014.

[6] B. Frenay and M. Verleysen. Classiﬁcation in the presence of label noise: A survey. IEEE Transactions on

Neural Networks and Learning Systems  25(5):845–869  2014.

[7] Nagarajan Natarajan  Inderjit S. Dhillon  Pradeep K. Ravikumar  and Ambuj Tewari. Learning with noisy

labels. In Advances in neural information processing systems  pages 1196–1204  2013.

[8] Volodymyr Mnih and Geoffrey E. Hinton. Learning to label aerial images from noisy data. In International

Conference on Machine Learning (ICML)  pages 567–574  2012.

[9] Giorgio Patrini  Alessandro Rozza  Aditya Menon  Richard Nock  and Lizhen Qu. Making neural networks

robust to label noise: A loss correction approach. In Computer Vision and Pattern Recognition  2017.

[10] Scott Reed  Honglak Lee  Dragomir Anguelov  Christian Szegedy  Dumitru Erhan  and Andrew Rabinovich.
Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596  2014.

[11] Xiaojin Zhu  John Lafferty  and Zoubin Ghahramani. Combining active learning and semi-supervised

learning using Gaussian ﬁelds and harmonic functions. In ICML  2003.

[12] Rob Fergus  Yair Weiss  and Antonio Torralba. Semi-supervised learning in gigantic image collections. In

Advances in neural information processing systems  pages 522–530  2009.

[13] Xinlei Chen and Abhinav Gupta. Webly supervised learning of convolutional networks. In International

Conference on Computer Vision (ICCV)  2015.

[14] Andreas Veit  Neil Alldrin  Gal Chechik  Ivan Krasin  Abhinav Gupta  and Serge Belongie. Learning from

noisy large-scale datasets with minimal supervision. arXiv preprint arXiv:1701.01619  2017.

[15] Guosheng Lin  Chunhua Shen  Anton van den Hengel  and Ian Reid. Efﬁcient piecewise training of deep
structured models for semantic segmentation. In Computer Vision and Pattern Recognition (CVPR)  2016.

9

[16] Shuai Zheng  Sadeep Jayasumana  Bernardino Romera-Paredes  Vibhav Vineet  Zhizhong Su  Dalong Du 
Chang Huang  and Philip HS Torr. Conditional random ﬁelds as recurrent neural networks. In International
Conference on Computer Vision (ICCV)  2015.

[17] Jian Peng  Liefeng Bo  and Jinbo Xu. Conditional neural ﬁelds. In Advances in neural information

processing systems  pages 1419–1427  2009.

[18] Thierry Artieres et al. Neural conditional random ﬁelds. In Proceedings of the Thirteenth International

Conference on Artiﬁcial Intelligence and Statistics  pages 177–184  2010.

[19] Rohit Prabhavalkar and Eric Fosler-Lussier. Backpropagation training for multilayer conditional ran-
dom ﬁeld based phone recognition. In Acoustics Speech and Signal Processing (ICASSP)  2010 IEEE
International Conference on  pages 5534–5537. IEEE  2010.

[20] Philipp Krähenbühl and Vladlen Koltun. Efﬁcient inference in fully connected CRFs with Gaussian edge

potentials. In Advances in Neural Information Processing Systems (NIPS)  pages 109–117  2011.

[21] Liang-Chieh Chen  Alexander G. Schwing  Alan L. Yuille  and Raquel Urtasun. Learning deep structured

models. In ICML  pages 1785–1794  2015.

[22] Alexander G. Schwing and Raquel Urtasun. Fully connected deep structured networks. arXiv preprint

arXiv:1503.02351  2015.

[23] Zhiwei Deng  Arash Vahdat  Hexiang Hu  and Greg Mori. Structure inference machines: Recurrent neural

networks for analyzing relations in group activity recognition. In CVPR  2016.

[24] Stephane Ross  Daniel Munoz  Martial Hebert  and J. Andrew Bagnell. Learning message-passing inference

machines for structured prediction. In Computer Vision and Pattern Recognition (CVPR)  2011.

[25] Alexander Kirillov  Dmitrij Schlesinger  Shuai Zheng  Bogdan Savchynskyy  Philip HS Torr  and Carsten
Joint training of generic CNN-CRF models with stochastic optimization. arXiv preprint

Rother.
arXiv:1511.05067  2015.

[26] Ariadna Quattoni  Sybor Wang  Louis-Philippe Morency  Morency Collins  and Trevor Darrell. Hidden
conditional random ﬁelds. IEEE transactions on pattern analysis and machine intelligence  29(10)  2007.

[27] Laurens Maaten  Max Welling  and Lawrence K. Saul. Hidden-unit conditional random ﬁelds.

International Conference on Artiﬁcial Intelligence and Statistics  pages 479–488  2011.

In

[28] Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient.
In Proceedings of the 25th international conference on Machine learning  pages 1064–1071. ACM  2008.

[29] Laurent Younes. Parametric inference for imperfectly observed Gibbsian ﬁelds. Probability theory and

related ﬁelds  1989.

[30] Radford M. Neal and Geoffrey E. Hinton. A view of the em algorithm that justiﬁes incremental  sparse 

and other variants. In Learning in graphical models. 1998.

[31] Marcus Rohrbach  Michael Stark  György Szarvas  Iryna Gurevych  and Bernt Schiele. What helps where–
and why? Semantic relatedness for knowledge transfer. In Computer Vision and Pattern Recognition
(CVPR)  2010.

[32] Kuzman Ganchev  Jennifer Gillenwater  Ben Taskar  et al. Posterior regularization for structured latent

variable models. Journal of Machine Learning Research  2010.

[33] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In Computer Vision and Pattern Recognition  2016.

[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint arXiv:1409.1556  2014.

[35] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[36] Hao Fang  Saurabh Gupta  Forrest Iandola  Rupesh K. Srivastava  Li Deng  Piotr Dollár  Jianfeng Gao 
Xiaodong He  Margaret Mitchell  John C Platt  et al. From captions to visual concepts and back. In
Conference on Computer Vision and Pattern Recognition  2015.

10

,Arash Vahdat