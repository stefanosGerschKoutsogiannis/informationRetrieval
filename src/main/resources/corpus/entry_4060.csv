2010,Online Markov Decision Processes under Bandit Feedback,We consider online learning in finite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition  however  the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and assuming that stationary policies mix uniformly fast  we show that after T time steps  the expected regret of the new algorithm is O(T^{2/3} (ln T)^{1/3})  giving the first rigorously proved convergence rate result for the problem.,Online Markov Decision Processes under Bandit

Feedback

Gergely Neu∗†

∗Department of Computer Science and

Information Theory  Budapest University of

Technology and Economics  Hungary

neu.gergely@gmail.com

Andr´as Gy¨orgy

†Machine Learning Research Group
MTA SZTAKI Institute for Computer

Science and Control  Hungary

gya@szit.bme.hu

Csaba Szepesv´ari

Department of Computing Science 

University of Alberta  Canada
szepesva@ualberta.ca

Andr´as Antos

Machine Learning Research Group
MTA SZTAKI Institute for Computer

Science and Control  Hungary

antos@szit.bme.hu

Abstract

We consider online learning in ﬁnite stochastic Markovian environments where in
each time step a new reward function is chosen by an oblivious adversary. The
goal of the learning agent is to compete with the best stationary policy in terms
of the total reward received. In each time step the agent observes the current state
and the reward associated with the last transition  however  the agent does not
observe the rewards associated with other state-action pairs. The agent is assumed
to know the transition probabilities. The state of the art result for this setting is
a no-regret algorithm. In this paper we propose a new learning algorithm and 
assuming that stationary policies mix uniformly fast  we show that after T time

steps  the expected regret of the new algorithm is O(cid:0)T 2/3(ln T )1/3(cid:1)  giving the

ﬁrst rigorously proved regret bound for the problem.

1

Introduction

We consider online learning in ﬁnite Markov decision processes (MDPs) with a ﬁxed  known dy-
namics. The formal problem deﬁnition is as follows: An agent navigates in a ﬁnite stochastic envi-
ronment by selecting actions based on the states and rewards experienced previously. At each time
instant the agent observes the reward associated with the last transition and the current state  that is 
at time t + 1 the agent observes rt(xt  at)  where xt is the state visited at time t and at is the action
chosen. The agent does not observe the rewards associated with other transitions  that is  the agent
faces a bandit situation. The goal of the agent is to maximize its total expected reward ˆRT in T
steps. As opposed to the standard MDP setting  the reward function at each time step may be differ-
ent. The only assumption about this sequence of reward functions rt is that they are chosen ahead of
time  independently of how the agent acts. However  no statistical assumptions are made about the
choice of this sequence. As usual in such cases  a meaningful performance measure for the agent is
how well it can compete with a certain class of reference policies  in our case the set of all stationary
policies: If R∗
T denotes the expected total reward in T steps that can be collected by choosing the
best stationary policy (this policy can be chosen based on the full knowledge of the sequence rt) 
the goal of learning can be expressed as minimizing the total expected regret  ˆLT = R∗
In this paper we propose a new algorithm for this setting. Assuming that the stationary distributions
underlying stationary policies exist  are unique and they are uniformly bounded away from zero and

T − ˆRT .

1

algorithm in T time steps is O(cid:0)T 2/3(ln T )1/3(cid:1).

that these policies mix uniformly fast  our main result shows that the total expected regret of our

The ﬁrst work that considered a similar online learning setting is due to Even-Dar et al. (2005 
2009). In fact  this is the work that provides the starting point for our algorithm and analysis. The
major difference between our work and that of Even-Dar et al. (2005  2009) is that they assume
that the reward function is fully observed (i.e.  in each time step the learning agent observes the
whole reward function rt)  whereas we consider the bandit setting. The main result in these works
is a bound on the total expected regret  which scales with the square root of the number of time
steps under mixing assumptions identical to our assumptions. Another work that considered the full
information problem is due to Yu et al. (2009) who proposed new algorithms and proved a bound on

the expected regret of order O(cid:0)T 3/4+ε(cid:1) for arbitrary ε ∈ (0  1/3). The advantage of the algorithm

of Yu et al. (2009) to that of Even-Dar et al. (2009) is that it is computationally less expensive  which 
however  comes at the price of an increased bound on the regret. Yu et al. (2009) introduced another
algorithm (“Q-FPL”) and they have shown a sublinear (o(T )) almost sure bound on the regret.
All the works reviewed so far considered the full information case. The requirement that the full
reward function must be given to the agent at every time step signiﬁcantly limits their applicability.
There are only three papers that we know of where the bandit situation was considered.
The ﬁrst paper which falls into this category is due to Yu et al. (2009) who proposed an algorithm
(“Exploratory FPL”) for this setting and have shown an o(T ) almost sure bound on the regret.

Recently  Neu et al. (2010) gave O(cid:16)√

(cid:17)

T

regret bounds for a special bandit setting when the agent
interacts with a loop-free episodic environment. The algorithm and analysis in this work heavily
exploits the speciﬁcs of these environments (i.e.  that in the same episode no state can be visited
twice) and so they do not generalize to our setting.
Another closely related work is due to Yu and Mannor (2009a b) who considered the problem of
online learning in MDPs where the transition probabilities may also change arbitrarily after each
transition. This problem  however  is signiﬁcantly different from ours and the algorithms studied
are unsuitable for our setting. Further  the analysis in these papers seems to have gaps (see Neu
et al.  2010). Thus  currently  the only result for the case considered in this paper is an asymptotic
“no-regret” result.
The rest of the paper is organized as follows: The problem is laid out in Section 2  which is followed
by a section about our assumptions (Section 3). The algorithm and the main result are given in
Section 4  while a proof sketch of the latter is presented in Section 5.

2 Problem deﬁnition
Formally  a ﬁnite Markov Decision Process (MDP) M is deﬁned by a ﬁnite state space X   a ﬁnite
action set A  a transition probability kernel P : X × A × X → [0  1]  and a reward function
r : X × A → [0  1]. In time step t ∈ {1  2  . . .}  knowing the state xt ∈ X   an agent acting in
the MDP M chooses an action at ∈ A(xt) to be executed based on (xt  r(at−1  xt−1)  at−1  xt−1 
. . .   x2  r(a1  x1)  a1  x1).1 Here A(x) ⊂ A is the set of admissible actions at state x. As a result of
executing the chosen action the process moves to state xt+1 ∈ X with probability P (xt+1|xt  at)
and the agent receives reward r(xt  at). In the so-called average-reward problem  the goal of the
agent is to maximize the average reward received over time. For a more detailed introduction the
reader is referred to  for example  Puterman (1994).

2.1 Online learning in MDPs

In this paper we consider the online version of MDPs when the reward function is allowed to change
arbitrarily. That is  instead of a single reward function r  a sequence of reward functions {rt} is
given. This sequence is assumed to be ﬁxed ahead of time  and  for simplicity  we assume that
rt(x  a) ∈ [0  1] for all (x  a) ∈ X × A and t ∈ {1  2  . . .}. No other assumptions are made about
this sequence.

1We follow the convention that boldface letters denote random variables.

2

The learning agent is assumed to know the transition probabilities P   but is not given the sequence
{rt}. The protocol of interaction with the environment is unchanged: At time step t the agent
receives xt and then selects an action at which is sent to the environment. In response  the reward
rt(xt  at) and the next state xt+1 are communicated to the agent. The initial state x1 is generated
from a ﬁxed distribution P0.
The goal of the learning agent is to maximize its expected total reward

ˆRT = E

rt(xt  at)

.

(cid:35)

(cid:35)

(cid:34) T(cid:88)

t=1

(cid:34) T(cid:88)

t=1

An equivalent goal is to minimize the regret  that is  to minimize the difference between the expected
total reward received by the best algorithm within some reference class and the expected total reward
of the learning algorithm. In the case of MDPs a reasonable reference class  used by various previous
works (e.g.  Even-Dar et al.  2005  2009; Yu et al.  2009) is the class of stationary stochastic policies.2
A stationary stochastic policy  π  (or  in short: a policy) is a mapping π : A × X → [0  1]  where
π(a|x) ≡ π(a  x) is the probability of taking action a in state x. We say that a policy π is followed
in an MDP if the action at time t is drawn from π  independently of previous states and actions given
the current state x(cid:48)
t). The expected total reward while following a policy π is deﬁned
as

t ∼ π(·|x(cid:48)

t: a(cid:48)

T = E
Rπ

rt(x(cid:48)

t  a(cid:48)
t)

.

ˆLT = sup
π

T − ˆRT  
Rπ

t  a(cid:48)

t)} denotes the trajectory that results from following policy π from x(cid:48)

Here {(x(cid:48)
The expected regret (or expected relative loss) of the learning agent relative to the class of policies
(in short  the regret) is deﬁned as

1 ∼ P0.

where the supremum is taken over all (stochastic stationary) policies. Note that the optimal policy
is chosen in hindsight  depending acausally on the reward function. If the regret of an agent grows
sublinearly with T then we can say that in the long run it acts as well as the best (stochastic station-
ary) policy (i.e.  the average expected regret of the agent is asymptotically equal to that of the best
policy).

3 Assumptions

Deﬁne P π(x(cid:48)|x) =(cid:80)

In this section we list the assumptions that we make throughout the paper about the transition proba-
bility kernel (hence  these assumptions will not be mentioned in the subsequent results). In addition 
recall that we assume that the rewards are bound to [0  1].
Before describing the assumptions  a few more deﬁnitions are needed: Let π be a stationary policy.
a π(a|x)P (x(cid:48)|x  a). We will also view P π as a matrix: (P π)x x(cid:48) = P π(x(cid:48)|x) 
where  without loss of generality  we assume that X = {1  2  . . .  |X|}. In general  distributions will
also be treated as row vectors. Hence  for a distribution µ  µP π is the distribution over X that results
from using policy π for one step from µ (i.e.  the “next-state distribution” under π). Remember that
the stationary distribution of a policy π is a distribution µ which satisﬁes µP π = µ.

Assumption A1 Every policy π has a well-deﬁned unique stationary distribution µπ.

The stationary distributions are uniformly bounded away from zero:

Assumption A2
inf π x µπ(x) ≥ β for some β > 0.
Assumption A3 There exists some ﬁxed positive τ such that for any two arbitrary distributions µ
and µ(cid:48) over X  

where (cid:107) · (cid:107)1 is the 1-norm of vectors: (cid:107)v(cid:107)1 =(cid:80)

sup

π

i |vi|.

(cid:107)(µ − µ(cid:48))P π(cid:107)1 ≤ e−1/τ(cid:107)µ − µ(cid:48)(cid:107)1 

2This is a reasonable reference class because for a ﬁxed reward function one can always ﬁnd a member of

it which maximizes the average reward per time step  see Puterman (1994).

3

Note that Assumption A3 implies Assumption A1. The quantity τ is called the mixing time under-
lying P by Even-Dar et al. (2009) who also assume A3.

4 Learning in online MDPs under bandit feedback

In this section we shall ﬁrst introduce some additional  standard MDP deﬁnitions  which we will be
used later. That these are well-deﬁned follows from our assumptions on P and from standard results
to be found  for example  in the book by Puterman (1994). After the deﬁnitions  we specify our
algorithm. The section is ﬁnished by the statement of our main result concerning the performance
of the proposed algorithm.

4.1 Preliminaries
Fix an arbitrary policy π and t ≥ 1. Let {(x(cid:48)
transition probability kernel P . Deﬁne  ρπ
rt by

s  a(cid:48)

s)} be the random trajectory generated by π and the
t   the average reward per stage corresponding to π  P and
S(cid:88)
x µπ(x)(cid:80)

E[rt(x(cid:48)

s  a(cid:48)

s)] .

1
S

s=0

a π(a|x)rt(x  a)  where µπ is the stationary
An alternative expression for ρπ
t be the cor-
distribution underlying π. Let qπ
responding state-value function. These can be uniquely deﬁned as the solutions of the following
Bellman equations:

t is ρπ
t be the action-value function of π  P and rt and vπ

t (x  a) = rt(x  a) − ρπ
qπ

t +

P (x(cid:48)|x  a)vπ

t (x(cid:48)) 

vπ
t (x) =

π(a|x)qπ

t (x  a).

(cid:88)

a

ρπ
t = lim
S→∞

t =(cid:80)
(cid:88)

x(cid:48)

Now  consider the trajectory {(xt  at)} underlying a learning agent  where x1 is randomly chosen
from P0  and deﬁne

ut = ( x1  a1  r1(x1  a1)  x2  a2  r2(x2  a2)  . . .   xt  at  rt(xt  at) )

and πt(a|x) = P[at = a|ut−1  xt = x]. That is  πt denotes the policy followed by the agent at
time step t (which is computed based on past information and is therefore random). We will use the
following notation:

qt = qπt
t

 

vt = vπt
t

 

ρt = ρπt
t

.

Note that qt  vt satisfy the Bellman equations underlying πt  P and rt.
For reasons to be made clear later in the paper  we shall need the state distribution at time step t
given that we start from the state-action pair (x  a) at time t − N  conditioned on the policies used
between time steps t − N and t:

t x a(x(cid:48)) def= P [xt = x(cid:48) | xt−N = x  at−N = a  πt−N +1  . . .   πt−1]  
µN

x  x(cid:48) ∈ X   a ∈ A .
t x a(·) will be viewed
It will be useful to view µN
as one row of this matrix. To emphasize the conditional nature of this distribution  we will also use
t (·|x  a) instead of µN
µN

t as a matrix of dimensions |X ×A|×|X|. Thus  µN
t x a(·).

4.2 The algorithm

Our algorithm is similar to that of Even-Dar et al. (2009) in that we use an expert algorithm in each
state. Since in our case the full reward function rt is not observed  the agent uses an estimate of it.
The main difﬁculty is to come up with an unbiased estimate of rt with a controlled variance. Here
we propose to use the following estimate:

rt(x a)
t (x|xt−N  at−N )

πt(a|x)µN
0

if (x  a) = (xt  at)
otherwise 

(1)

(cid:40)

ˆrt(x  a) =

4

where t ≥ N + 1. Deﬁne ˆqt  ˆvt and ˆρ as the solution to the Bellman equations underlying the
average reward MDP deﬁned by (P  πt  ˆrt):

ˆqt(x  a) = ˆrt(x  a) − ˆρt +

P (x(cid:48)|x  a)ˆvt(x(cid:48)) 

ˆvt(x) =

πt(a|x)ˆqt(x  a)  

(cid:88)

(cid:88)

x(cid:48)

(cid:88)

ˆρt =

µπt(x)πt(a|x)ˆrt(x  a) .

a

(2)

x a

Note that if N is sufﬁciently large and πt changes sufﬁciently slowly then

(3)
almost surely  for arbitrary x ∈ X   t ≥ N + 1. This fact will be shown in Lemma 4. Now  assume
that πt is computed based on ut−N   that is  πt is measurable with respect to the σ-ﬁeld σ(ut−N )
generated by the history ut−N :

t (x|xt−N   at−N ) > 0 
µN

πt ∈ σ(ut−N ) .

(4)

(6)

Then also πt−1  . . .   πt−N ∈ σ(ut−N ) and µN

t can be computed using

t x a = exP aP πt−N +1 ··· P πt−1 
µN

(5)
where P a is the transition probability matrix when in every state action a is used and ex is the unit
row vector corresponding to x (and we assumed that X = {1  . . .  |X|}). Moreover  a simple but
tedious calculation shows that (3) and (4) ensure the conditional unbiasedness of our estimates  that
is 

E [ ˆrt(x  a)| ut−N ] = rt(x  a).

It then follows that
and  hence  by the uniqueness of the solutions of the Bellman equations  we have  for all (x  a) ∈
X × A 

E[ ˆρt|ut−N ] = ρt 

E[ˆqt(x  a)|ut−N ] = qt(x  a)

and E[ˆvt(x)|ut−N ] = vt(x).

As a consequence  we also have  for all (x  a) ∈ X × A  t ≥ N + 1 

E[ ˆρt] = E [ρt]   E[ˆqt(x  a)] = E [qt(x  a)]  

and E[ˆvt(x)] = E [vt(x)] .

(7)

(8)

The bandit algorithm that we propose is shown as Algorithm 1. It follows the approach of Even-Dar
et al. (2009) in that a bandit algorithm is used in each state which together determine the policy to be
used. These bandit algorithms are fed with estimates of action-values for the current policy and the
current reward. In our case these action-value estimates are ˆqt deﬁned earlier  which are based on the
reward estimates ˆrt. A major difference is that the policy computed based on the most recent action-
value estimates is used only N steps later. This delay allows us to construct unbiased estimates of
the rewards. Its price is that we need to store N policies (or weights  leading to the policies)  thus 
the memory needed by our algorithm scales with N |A||X|. The computational complexity of the
algorithm is dominated by the cost of computing ˆrt (and  in particular  by the cost of computing
µN
delay  we also need to deal with the fact that in our case qt and ˆqt can be both negative  which must
be taken into account in the proper tuning of the algorithm’s parameters.

t (·|xt−N   at−N )). The cost of this is O(cid:0)N|A||X|3(cid:1). In addition to the need of dealing with the

4.3 Main result

Our main result is the following bound concerning the performance of Algorithm 1.
Theorem 1. Let N = (cid:100)τ ln T(cid:101) 

η = T −2/3 · (ln|A|)2/3 ·

γ = T −1/3 · (2τ + 4)−2/3 ·

(cid:18) 4τ + 8
(cid:18) 2 ln|A|

(cid:0)(2τ + 4)τ|A| ln T + (3τ + 1)2(cid:1)(cid:19)−1/3
(cid:0)(2τ + 4)τ|A| ln T + (3τ + 1)2(cid:1)(cid:19)1/3

β

 

.

β

5

Algorithm 1 Algorithm for the online bandit MDP.
Set N ≥ 1  w1(x  a) = w2(x  a) = ··· = w2N (x  a) = 1  γ ∈ (0  1)  η ∈ (0  γ].
For t = 1  2  . . .   T   repeat

1. Set

πt(a|x) = (1 − γ)

for all (x  a) ∈ X × A.

(cid:80)

wt(x  a)
b wt(x  b)

+

γ
|A|

2. Draw an action at randomly  according to the policy πt(·|xt).
3. Receive reward rt(xt  at) and observe xt+1.
4. If t ≥ N + 1

t (x|xt−N   at−N ) for all x ∈ X using (5).

(a) Compute µN
(b) Construct estimates ˆrt using (1) and compute ˆqt using (2).
(c) Set wt+N (x  a) = wt+N−1(x  a)eηˆqt(x a) for all (x  a) ∈ X × A.

Then the regret can be bounded as

ˆLT ≤ 3 T 2/3 ·

β

(cid:18) (4τ + 8) ln|A|

(cid:0)(2τ + 4)τ|A| ln T + (3τ + 1)2(cid:1)(cid:19)1/3

+ O(cid:16)

T 1/3(cid:17)

.

It is interesting to note that  similarly to the regret bound of Even-Dar et al. (2009)  the main term
of the regret bound does not directly depend on the size of the state space  but it depends on it only
through β and the mixing time τ  deﬁned in Assumptions A2 and A3  respectively; however  we also
need to note that β > 1/|X|. While the theorem provides the ﬁrst rigorously proved ﬁnite sample
regret bound for the online bandit MDP problem  we suspect that the given convergence rate is not
sharp in the sense that it may be possible  in agreement with the standard bandit lower bound of

(cid:17)

Auer et al. (2002)  to give an algorithm with an O(cid:16)√
(cid:32) T(cid:88)

(cid:32)

(cid:33)

T − T(cid:88)

T − ˆRT =
Rπ

Rπ

ρπ
t

+

The proof of the theorem is similar to the proof of a similar bound done for the full-information case
T − ˆRT for an arbitrary ﬁx policy π. We
by Even-Dar et al. (2009). Clearly  it sufﬁces to bound Rπ
use the following decomposition of this difference (also used by Even-Dar et al.  2009):

t − T(cid:88)

ρπ

(cid:33)

(cid:32) T(cid:88)

(cid:33)

ρt

+

ρt − ˆRT

.

(9)

T

regret (up to some logarithmic factors).

t=1

t=1

t=1

t=1

(cid:16)

The ﬁrst term is bounded using the following standard MDP result.
Lemma 1 (Even-Dar et al.  2009). For any policy π and any T ≥ 1 it holds that
Rπ

(cid:17) ≤ 2(τ + 1).

T −(cid:80)T

t=1 ρπ
t

Hence  it remains to bound the expectation of the other terms  which is done in the following two
propositions.
Proposition 1. Let N ≥ (cid:100)τ ln T(cid:101). For any policy π and for all T large enough  we have

T(cid:88)

E [ρπ

t − ρt]

T(cid:88)

t=1

(cid:18)
(cid:18) 1

γ

|A|(cid:16)
(cid:19)

+ 4τ + 6

6

t=1

≤ (4τ + 10)N +

ln|A|

η

+ (2τ + 4) T

γ +

2η
β

Proposition 2. Let N ≥ (cid:100)τ ln T(cid:101). For any T large enough 

N (1/γ + 4τ + 6) + (e − 2)(2τ + 4)

E [ρt] − ˆRT ≤ T

2η
β

(3τ + 1)2 + 2T e−N/τ + 2N.

(cid:17)(cid:19)

.

(10)

Note that the choice of N ensures that the second term in (10) becomes O(1).
The proofs are broken into a number of statements presented in the next section. Due to space
constraints we present proof sketches only; the full proofs are presented in the extended version of
the paper.

5 Analysis

5.1 General tools

First  we show that if the policies that we follow up to time step t change slowly  µN
t
µπt:
Lemma 2. Let 1 ≤ N < t ≤ T and c > 0 be such that maxx
holds for 1 ≤ s ≤ t − 1. Then we have

t x a(x(cid:48)) − µπt(x(cid:48))(cid:12)(cid:12) ≤ c (3τ + 1)2 + 2e−N/τ .
(cid:12)(cid:12)µN

(cid:88)

(cid:80)
is “close” to
a |πs+1(a|x) − πs(a|x)| ≤ c

max
x a

x(cid:48)

In the next two lemmas we compute the rate of change of the policies produced by Exp3 and show
that for a large enough value of N  µN
Lemma 3. Assume that for some N + 1 ≤ t ≤ T   µN
Let c = 2η
β

t x a can be uniformly bounded form below by β/2.

(x(cid:48)) ≥ β/2 holds for all states x(cid:48).

γ + 4τ + 6

(cid:16) 1

t xt−N  at−N

. Then 

(cid:17)

|πt+N−1(a|x) − πt+N (a|x)| ≤ c.

(11)

(cid:88)

a

max

x

The previous results yield the following result that show that by choosing the parameters appropri-
ately  the policies will change slowly and µN
Lemma 4. Let c be as in Lemma 3. Assume that c(3τ + 1)2 < β/2  and let

t will be uniformly bounded away from zero.

(cid:24)

(cid:18)

(cid:19)(cid:25)

N ≥

τ ln

4

β − 2c(3τ + 1)2

.

maxx(cid:48)(cid:80)

Then 

for all N < t ≤ T   x  x(cid:48) ∈ X and a ∈ A  we have µN

a(cid:48) |πt+1(a(cid:48)|x(cid:48)) − πt(a(cid:48)|x(cid:48))| ≤ c.

(12)
t x a(x(cid:48)) ≥ β/2 and

This result is proved by ﬁrst ensuring that µt is uniformly lower bounded for t = N + 1  . . .   2N 
which can be easily seen since the policies do not change in this period. For the rest of the time
instants  one can proceed by induction  using Lemmas 2 and 3 in the inductive step.

t − ρt =
ρπ

µπ(x)π(a|x) [qt(x  a) − vt(x)] .

5.2 Proof of Proposition 1
The statement is trivial for T ≤ N. The following simple result is the ﬁrst step in proving Proposi-
tion 1 for T > N.
Lemma 5. (cf. Lemma 4.1 in Even-Dar et al.  2009) For any policy π and t ≥ 1 

(cid:88)
t=N +1 qt(x  a) and VT (x) = (cid:80)T
For every x  a deﬁne QT (x  a) = (cid:80)T
(cid:108)

t=N +1 vt(x). The pre-
ceding lemma shows that in order to prove Proposition 1  it sufﬁces to prove an upper bound on
E [QT (x  a) − VT (x)].
Lemma 6. Let c be as in Lemma 3. Assume that γ ∈ (0  1)  c(3τ + 1)2 < β/2  N ≥
τ ln
E [QT (x  a) − VT (x)]
ln|A|
≤ (4τ + 8)N +

2(1/γ +2τ +3)   and T > N hold. Then  for all (x  a) ∈ X × A 
(cid:17)(cid:19)

N (1/γ + 4τ + 6) + (e − 2)(2τ + 4)

|A|(cid:16)

  0 < η ≤

β−2c(3τ +1)2

+ (2τ + 4) T

(cid:17)(cid:109)

(cid:18)

(cid:16)

γ +

x a

β

4

.

η

2η
β

7

Proof sketch. The proof essentially follows the original proof of Auer et al. (2002) concerning the
regret bound of Exp3  although some details are more subtle in our case: our estimates have different
properties than the ones considered in the original proof  and we also have to deal with the N-step
delay.
Let

ˆVN

T (x) =

πt+N−1(a|x)ˆqt(x  a)

and

ˆQN

T (x  b) =

ˆqt(x  b).

T−N +1(cid:88)

(cid:88)

T−N +1(cid:88)

T−N +1(cid:88)

(cid:88)

t=N +1

a

t=N +1

a

t=N +1

Observe that although qt(x  a) is not necessarily positive (in contrast to the rewards in the Exp3
algorithm)  one can prove that πt(a|x)|ˆqt(x  a)| ≤ 4

β (τ + 2) and

(13)
Similarly  it can be easily seen that the constraint on η ensures that ηˆqt(x  a) ≤ 1 for all x  a  t.
Then  following the proof of Auer et al. (2002)  we can show that

E [|ˆqt(x  a)|] ≤ 2(τ + 2).

(14)

T (x)

− 4
β

|ˆqt(x  a)| .

(τ + 2) η(e − 2)

T (x) ≥ (1 − γ) ˆQN
ˆVN

Next  since the policies satisfy maxx
using (8) and (13)  that

T (x  b) − ln|A|
(cid:80)
η
a |πs+1(a|x) − πs(a|x)| ≤ c by Lemma 4  we can prove 
(cid:105) ≤ E [VT (x)] + 2(τ + 2) N (c T|A| + 1).
E(cid:104) ˆVN
Now  taking the expectation of both sides of (14) and using the bound on E(cid:104) ˆVN
(cid:88)
T−N +1(cid:88)
T (x  b)(cid:3) − ln|A|
E [VT (x)] ≥ (1 − γ)E(cid:2)QN
(cid:105)
where we used that E(cid:104) ˆQN
T (x  b)(cid:3) by (8). Since qt(x  b) ≤ 2(τ + 2) 
= E(cid:2)QN
E(cid:2)QN
T (x  b)(cid:3) ≤ E [QT (x  b)] + 2(τ + 2) N.

η
− 2(τ + 2) N (c T|A| + 1) 

(τ + 2) η(e − 2)

E [|ˆqt(x  a)|]

T (x)

we get

− 4
β

T (x  b)

t=N +1

a

(cid:105)

Combining the above results and using (13) again  then substituting the deﬁnition of c yields the
desired result.

Proof of Proposition 1. Under the conditions of the proposition  combining Lemmas 5-6 yields

T(cid:88)

t=1

E [ρπ

t − ρt]
(cid:88)

≤ 2N +

x a

≤ (4τ + 10)N +

ln|A|

η

proving Proposition 1.

Acknowledgments

µπ(x)π(a|x) E [QT (x  a) − VT (x)]

(cid:18)

|A|(cid:16)

N (1/γ + 4τ + 6) + (e − 2)(2τ + 4)

(cid:17)(cid:19)

 

+ (2τ + 4) T

γ +

2η
β

This work was supported in part by the Hungarian Scientiﬁc Research Fund and the Hungarian
National Ofﬁce for Research and Technology (OTKA-NKTH CNK 77782)  the PASCAL2 Network
of Excellence under EC grant no. 216886  NSERC  AITF  the Alberta Ingenuity Centre for Machine
Learning  the DARPA GALE project (HR0011-08-C-0110) and iCore.

8

References
Auer  P.  Cesa-Bianchi  N.  Freund  Y.  and Schapire  R. E. (2002). The nonstochastic multiarmed

bandit problem. SIAM J. Comput.  32(1):48–77.

Even-Dar  E.  Kakade  S. M.  and Mansour  Y. (2005). Experts in a Markov decision process. In Saul 
L. K.  Weiss  Y.  and Bottou  L.  editors  Advances in Neural Information Processing Systems 17 
pages 401–408.

Even-Dar  E.  Kakade  S. M.  and Mansour  Y. (2009). Online Markov decision processes. Mathe-

matics of Operations Research  34(3):726–736.

Neu  G.  Gy¨orgy  A.  and Szepesv´ari  C. (2010). The online loop-free stochastic shortest-path prob-

lem. In COLT-10.

Puterman  M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.

Wiley-Interscience.

Yu  J. Y. and Mannor  S. (2009a). Arbitrarily modulated Markov decision processes. In Joint 48th

IEEE Conference on Decision and Control and 28th Chinese Control Conference. IEEE Press.

Yu  J. Y. and Mannor  S. (2009b). Online learning in Markov decision processes with arbitrarily
changing rewards and transitions. In GameNets’09: Proceedings of the First ICST international
conference on Game Theory for Networks  pages 314–322  Piscataway  NJ  USA. IEEE Press.

Yu  J. Y.  Mannor  S.  and Shimkin  N. (2009). Markov decision processes with arbitrary reward

processes. Mathematics of Operations Research  34(3):737–757.

9

,James Martens
Arkadev Chattopadhya
Toni Pitassi
Richard Zemel
Vitaly Kuznetsov
Mehryar Mohri
Umar Syed
Zhe Li
Boqing Gong
Tianbao Yang
Xiangyu Zheng
Song Xi Chen