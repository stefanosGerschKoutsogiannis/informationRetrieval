2013,Perfect Associative Learning with Spike-Timing-Dependent Plasticity,Recent extensions of the Perceptron  as e.g. the Tempotron  suggest that this theoretical concept is highly relevant also for understanding networks of spiking neurons in the brain. It is not known  however  how the computational power of the Perceptron and of its variants might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufficient for realizing the original Perceptron Learning Rule if the respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons are efficiently learned. The proposed mechanism might underly the acquisition of mappings of spatio-temporal activity patterns in one area of the brain onto other spatio-temporal spike patterns in another region and of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.,Perfect Associative Learning with
Spike-Timing-Dependent Plasticity

Christian Albers

Institute of Theoretical Physics

University of Bremen

28359 Bremen  Germany

Maren Westkott

Institute of Theoretical Physics

University of Bremen

28359 Bremen  Germany

calbers@neuro.uni-bremen.de

maren@neuro.uni-bremen.de

Klaus Pawelzik

Institute of Theoretical Physics

University of Bremen

28359 Bremen  Germany

pawelzik@neuro.uni-bremen.de

Abstract

Recent extensions of the Perceptron as the Tempotron and the Chronotron sug-
gest that this theoretical concept is highly relevant for understanding networks of
spiking neurons in the brain. It is not known  however  how the computational
power of the Perceptron might be accomplished by the plasticity mechanisms of
real synapses. Here we prove that spike-timing-dependent plasticity having an
anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent
plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the
original Perceptron Learning Rule if these respective plasticity mechanisms act in
concert with the hyperpolarisation of the post-synaptic neurons. We also show that
with these simple yet biologically realistic dynamics Tempotrons and Chronotrons
are learned. The proposed mechanism enables incremental associative learning
from a continuous stream of patterns and might therefore underly the acquisition
of long term memories in cortex. Our results underline that learning processes
in realistic networks of spiking neurons depend crucially on the interactions of
synaptic plasticity mechanisms with the dynamics of participating neurons.

1 Introduction

Perceptrons are paradigmatic building blocks of neural networks [1]. The original Perceptron Learn-
ing Rule (PLR) is a supervised learning rule that employs a threshold to control weight changes 
which also serves as a margin to enhance robustness [2  3]. If the learning set is separable  the PLR
algorithm is guaranteed to converge in a ﬁnite number of steps [1]  which justiﬁes the term ’perfect
learning’.

Associative learning can be considered a special case of supervised learning where the activity of the
output neuron is used as a teacher signal such that after learning missing activities are ﬁlled in. For
this reason the PLR is very useful for building associative memories in recurrent networks where
it can serve to learn arbitrary patterns in a ’quasi-unsupervised’ way. Here it turned out to be far
more efﬁcient than the simple Hebb rule  leading to a superior memory capacity and non-symmetric
weights [4]. Note also that over-learning from repetitions of training examples is not possible with
the PLR because weight changes vanish as soon as the accumulated inputs are sufﬁcient  a property

1

which in contrast to the na¨ıve Hebb rule makes it suitable also for incremental learning of associative
memories from sequential presentation of patterns.

On the other hand  it is not known if and how real synaptic mechanisms might realize the success-
dependent self-regulation of the PLR in networks of spiking neurons in the brain. For example in
the Tempotron [5]  a generalization of the perceptron to spatio-temporal patterns  learning was con-
ceived even somewhat less biological than the PLR  since here it not only depends on the potential
classiﬁcation success  but also on a process that is not local in time  namely the localization of the
absolute maximum of the (virtual) postsynaptic membrane potential of the post-synaptic neuron.
The simpliﬁed tempotron learning rule  while biologically more plausible  still relies on a reward
signal which tells each neuron speciﬁcally that it should have spiked or not. Taken together  while
highly desirable  the feature of self regulation in the PLR still poses a challenge for biologically
realistic synaptic mechanisms.

The classical form of spike-timing-dependent plasticity (STDP) for excitatory synapses (here de-
noted CSTDP) states that the causal temporal order of ﬁrst pre-synaptic activity and then postsy-
naptic activity leads to long-term potentiation of the synapse (LTP) while the reverse order leads to
long-term depression (LTD)[6  7  8]. More recently  however  it became clear that STDP can exhibit
different dependencies on the temporal order of spikes. In particular  it was found that the reversed
temporal order (ﬁrst post- then presynaptic spiking) could lead to LTP (and vice versa; RSTDP) 
depending on the location on the dendrite [9  10]. For inhibitory synapses some experiments were
performed which indicate that here STDP exists as well and has the form of CSTDP [11]. Note that
CSTDP of inhibitory synapses in its effect on the postsynaptic neuron is equivalent to RSTDP of
excitatory synapses. Additionally it has been shown that CSTDP does not always rely on spikes  but
that strong subthreshold depolarization can replace the postsynaptic spike for LTD while keeping
the usual timing dependence [12]. We therefore assume that there exists a second threshold for the
induction of timing dependent LTD. For simplicity and without loss of generality  we restrict the
study to RSTDP for synapses that in contradiction to Dale’s law can change their sign.

It is very likely that plasticity rules and dynamical properties of neurons co-evolved to take advan-
tage of each other. Combining them could reveal new and desirable effects. A modeling example
for a beneﬁcial effect of such an interplay was investigated in [13]  where CSTDP interacted with
spike-frequency adaptation of the postsynaptic neuron to perform a gradient descent on a square
error. Several other studies investigate the effect of STDP on network function  however mostly
with a focus on stability issues (e.g. [14  15  16]). In contrast  we here focus on the construc-
tive role of STDP for associative learning. First we prove that RSTDP of excitatory synapses (or
CSTDP on inhibitory synapses) when acting in concert with neuronal after-hyperpolarisation and
depolarization-dependent LTD is sufﬁcient for realizing the classical Perceptron learning rule  and
then show that this plasticity dynamics realizes a learning rule suited for the Tempotron and the
Chronotron [17].

2 Ingredients

2.1 Neuron model and network structure

We assume a feed-forward network of N presynaptic neurons and one postsynaptic integrate-and-
ﬁre neuron with a membrane potential U governed by

τU ˙U = −U + Isyn + Iext 

(1)

where Isyn denotes the input from the presynaptic neurons  and Iext is an input which can be used
to drive the postsynaptic neuron to spike at certain times. When the neuron reaches a threshold
potential Uthr  it is reset to a reset potential Ureset < 0  from where it decays back to the resting
potential U∞ = 0 with time constant τU . Spikes and other signals (depolarization) take ﬁnite times
to travel down the axon (τa) and the dendrite (τd). Synaptic transmission takes the form of delta
pulses  which reach the soma of the postsynaptic neuron after time τa + τd  and are modulated by
pre:
the synaptic weight w. We denote the presynaptic spike train of neuron i as xi with spike times ti

xi(t) = Xti

pre

δ(t − ti

pre).

2

(2)

A

Uthr

Ust

U¥

z(t)

w(t)

x(t)

B

postsynaptic trace y

presynaptic spikes x

x

subthreshold events z(t)

Figure 1: Illustration of STDP mechanism. A: Upper trace (red) is the membrane potential of the
postsynaptic neuron. Shown are the ﬁring threshold Uthr and the threshold for LTD Ust. Middle
trace (black) is the variable z(t)  the train of LTD threshold crossing events. Please note that the ﬁrst
spike in z(t) occurs at a different time than the neuronal spike. Bottom traces show w(t) (yellow)
and ¯x (blue) of a selected synapse. The second event in z reads out the trace of the presynaptic
spike ¯x  leading to LTD. B: Learning rule (4) is equivalent to RSTDP. A postsynaptic spike leads
to an instantaneous jump in the trace ¯y (top left  red line)  which decays exponentially. Subsequent
presynaptic spikes (dark blue bars and corresponding thin gray bars in the STDP window) “read” out
the state of the trace for the respective ∆t = tpre − tpost. Similarly  z(t) reads out the presynaptic
trace ¯x (lower left  blue line). Sampling for all possible times results in the STDP window (right).

A postsynaptic neuron receives the input Isyn(t) = Pi wixi(t − τa − τd). The postsynaptic spike
train is similarly denoted by y(t) = Ptpost

δ(t − tpost).

2.2 The plasticity rule

The plasticity rule we employ mimics reverse STDP: A postsynaptic spike which arrives at the
synapse shortly before a presynaptic spike leads to synaptic potentiation. For synaptic depression
the relevant signal is not the spike  but the point in time where U (t) crosses an additional threshold
Ust from below  with U∞ < Ust < Uthr (“subthreshold threshold”). These events are modelled as
δ-pulses in the function z(t) = Pk δ(t−tk)  where tk are the times of the aforementioned threshold

crossing events (see Fig. 1 A for an illustration of the principle). The temporal characteristic of
(reverse) STDP is preserved: If a presynaptic spike occurs shortly before the membrane potential
crosses this threshold  the synapse depresses. Timing dependent LTD without postsynaptic spiking
has been observed  although with classical timing requirements [12].

We formalize this by letting pre- and postsynaptic spikes each drive a synaptic trace:

τpre ˙¯x = −¯x + x(t − τa)
τpost ˙¯y = −¯y + y(t − τd).

(3)

The learning rule is a read–out of the traces by spiking and threshold crossing events  respectively:

˙w ∝ ¯yx(t − τa) − γ ¯xz(t − τd) 

(4)

where γ is a factor which scales depression and potentiation relative to each other. Fig. 1 B shows
how this plasticity rule creates RSTDP.

3 Equivalence to Perceptron Learning Rule

The Perceptron Learning Rule (PLR) for positive binary inputs and outputs is given by

∆wµ

i ∝ xi µ

0 (2yµ

0 − 1)Θ [κ − (2yµ

0 − 1)(hµ − ϑ)]  

(5)

3

robustness against noise after convergence  hµ = Pi wixi µ

where xi µ
0 ∈ {0  1} denotes the activity of presynaptic neuron i in pattern µ ∈ {1  . . .   P } 
yµ
0 ∈ {0  1} signals the desired response to pattern µ  κ > 0 is a margin which ensures a certain
is the input to a postsynaptic neuron 
ϑ denotes the ﬁring threshold  and Θ(x) denotes the Heaviside step function. If the P patterns are
linearly separable  the perceptron will converge to a correct solution of the weights in a ﬁnite number
of steps. For random patterns this is generally the case for P < 2N . A ﬁnite margin κ reduces the
capacity.

0

Interestingly  for the case of temporally well separated synchronous spike patterns the combination
of RSTDP-like synaptic plasticity dynamics with depolarization-dependent LTD and neuronal hy-
perpolarization leads to a plasticity rule which can be mapped to the Perceptron Learning Rule. To
cut down unnecessary notation in the derivation  we drop the indices i and µ except where necessary
and consider only times 0 ≤ t ≤ τa + 2τd.
We consider a single postsynaptic neuron with N presynaptic neurons  with the condition τd < τa.
During learning  presynaptic spike patterns consisting of synchronous spikes at time t = 0 are
induced  concurrent with a possibly occuring postsynaptic spike which signals the class the presy-
naptic pattern belongs to. This is equivalent to the setting of a single layered perceptron with bi-
nary neurons. With x0 and y0 used as above we can write the pre- and postsynaptic activity as
x(t) = x0δ(t) and y(t) = y0δ(t). The membrane potential of the postsynaptic neuron depends on
y0:

U (t) = y0Ureset exp(−t/τU )

U (τa + τd) = y0Ureset exp(−(τa + τd)/τU ) = y0Uad.

Similarly  the synaptic current is

Isyn(t) = Xi
Isyn(τa + τd) = Xi

wixi

0δ(t − τa − τd)

wixi

0 = Iad.

The activity traces at the synapses are

¯x(t) = x0Θ(t − τa)

¯y(t) = y0Θ(t − τd)

exp(−(t − τa)/τpre)

τpre

exp(−(t − τd)/τpost)

τpost

.

(6)

(7)

(8)

The variable of threshold crossing z(t) depends on the history of the postsynaptic neurons  which
again can be written with the aid of y0 as:

z(t) = Θ(Iad + y0Uad − Ust)δ(t − τa − τd).

(9)
Here  Θ reﬂects the condition for induction of LTD. Only when the postsynaptic input at time
t = τa + τd is greater than the residual hyperpolarization (Uad < 0!) plus the threshold Ust  a
potential LTD event gets enregistered. These are the ingredients for the plasticity rule (4):

∆w ∝Z [¯yx(t − τa) − γ ¯xz(t − τd)] dt

=x0y0

exp(−(τa + τd)/τpost)

τpost

− γx0

exp(−2τd/τpre)

τpre

Θ(Iad + y0Uad − Ust).

(10)

We shorten this expression by choosing γ such that the factors of both terms are equal  which we
can drop subsequently:

We expand the equation by adding and substracting y0Θ(Iad + y0Uad − Ust):

∆w ∝ x0 (y0 − Θ(Iad + y0Uad − Ust)) .

(11)

∆w ∝x0 [y0(1 − Θ(Iad + y0Uad − Ust)) − (1 − y0)Θ(Iad + y0Uad − Ust)]

=x0 [y0Θ(−Iad − Uad + Ust) − (1 − y0)Θ(Iad − Ust)] .

(12)

We used 1 − Θ(x) = Θ(−x) in the last transformation  and dropped y0 from the argument of the
Heaviside functions  as the two terms are seperated into the two cases y0 = 0 and y0 = 1. We do a

4

similar transformation to construct an expression G that turns either into the argument of the left or
right Heaviside function depending on y0. That expression is

G = Iad − Ust + y0(−2Iad − Uad + 2Ust) 

with which we replace the arguments:

∆w ∝ x0y0Θ(G) − x0(1 − y0)Θ(G) = x0(2y0 − 1)Θ(G).

(13)

(14)

The last task is to show that G and the argument of the Heaviside function in equation (5) are
equivalent. For this we choose Iad = h  Uad = −2κ and Ust = ϑ − κ and keep in mind  that
ϑ = Uthr is the ﬁring threshold. If we put this into G we get

G =Iad − Ust + y0(−2Iad − Uad + 2Ust)

=h − ϑ + κ + 2y0h + 2y0κ + 2y0ϑ − 2y0κ
=κ − (2y0 − 1)(h − ϑ) 

(15)

which is the same as the argument of the Heaviside function in equation (5). Therefore  we have
shown the equivalence of both learning rules.

4 Associative learning of spatio-temporal spike patterns

4.1 Tempotron learning with RSTDP

The condition of exact spike synchrony used for the above equivalence proof can be relaxed to
include the association of spatio-temporal spike patterns with a desired postsynaptic activity. In the
following we take the perspective of the postsynaptic neuron which during learning is externally
activated (or not) to signal the respective class by spiking at time t = 0 (or not). During learning in
each trial presynaptic spatio-temporal spike patterns are presented in the time span 0 < t < T   and
plasticity is ruled by (4). For these conditions the resulting synaptic weights realize a Tempotron
with substantial memory capacity.

A Tempotron is an integrate-and-ﬁre neuron with input weights adjusted to perform arbitrary clas-
siﬁcations of (sparse) spike patterns [5  18]. To implement a Tempotron  we make two changes
to the model. First  we separate the time scales of membrane potential and hyperpolarization by
introducing a variable ν:

τν ˙ν = −ν .

(16)
Immediately after a postsynaptic spike  ν is reset to νspike < 0. The reason is that the length
of hyperpolarization determines the time window where signiﬁcant learning can take place. To
improve comparability with the Tempotron as presented originally in [5]  we set T = 0.5s and
τν = τpost = 0.2s  so that the postsynaptic neuron can learn to spike almost anywhere over the time
window  and we introduce postsynaptic potentials (PSP) with a ﬁnite rise time:

τs ˙Isyn = −Isyn +Xi

wixi(t − τa) 

(17)

where wi denotes the synaptic weight of presynaptic neuron i. With τs = 3ms and τU = 15ms the
PSPs match the ones used in the original Tempotron study. This second change has little impact on
the capacity or otherwise. With these changes  the membrane potential is governed by

τU ˙U = (ν − U ) + Isyn(t − τd).

(18)

A postsynaptic spike resets U to νspike = Ureset < 0. Ureset is the initial hyperpolarization which
is induced after a spike  which relaxes back to zero with the time constant τν ≫ τU . Presynaptic
spikes add up linearly  and for simplicity we assume that both the axonal and the dendritic delay are
negligibly small: τa = τd = 0.
It is a natural choice to set τU = τpre and τν = τpost. τU sets the time scale for the summation
of EPSP contributing to spurious spikes  τν sets the time window where the desired spikes can lie.
They therefore should coincide with LTD and LTP  respectivly.

5

Figure 2: Illustration of Perceptron learning with RSTDP with subthreshold LTD and postsynaptic
hyperpolarization. Shown are the traces ¯x  ¯y and U . Pre- and postsynaptic spikes are displayed as
black bars at t = 0. A: Learning in the case of y0 = 1  i.e. a postsynaptic spike as the desired
output. Initially the weights are too low and the synaptic current (summed PSPs) is smaller than
Ust. Weight change is LTP only until during pattern presentation the membrane potential hits Ust.
At this point LTP and LTD cancel exactly  and learning stops. B: Pattern completion for y0 = 1.
Shown are the same traces as in A at the absence of an inital postsynaptic spike. The membrane
potential after learning is drawn as a dashed line to highlight the amplitude. Without the initial hy-
perpolarization  the synaptic current after learning is large enough to cross the spiking threshold  the
postsynaptic neuron ﬁres the desired spike. Learning until Ust is reached ensures a minimum height
of synaptic currents and therefore robustness against noise. C: Pattern presentation and completion
for y0 = 0. Initially  the synaptic current during pattern presentation causes a spike and conse-
quently LTD. Learning stops when the membrane potential stays below Ust. Again  this ensures a
certain robustness against noise  analogous to the margin in the PLR.

6

A

B

Figure 3: Performance of Tempotron and Chronotron after convergence. A: Classiﬁcation perfor-
mance of the Tempotron. Shown is the fraction of pattern which elicit the desired postsynaptic activ-
ity upon presentation. Perfect recall for all N is achieved up to α = 0.18. Beyond that mark  some
of the patterns become incorrectly classiﬁed. The inset shows the learning curves for α = 7/16. The
ﬁnal fraction of correctly classiﬁed pattern is the average fraction of the last 500 blocks of each run.
B: Performance of the Chronotron. Shown is the fraction of pattern which during recall succeed in
producing the correct postsynaptic spike time in a window of length 30 ms after the teacher spike.
See supplemental material for a detailed description. Please note that the scale of the load axis is
different in A and B.

Table 1: Parameters for Tempotron learning

τU   τpre
15 ms

τν  τpost
200 ms

τs
3 ms

Uthr
20 mV 19 mV -20 mV 10−5

νspike

Ust

η

γ
2

4.1.1 Learning performance

We test the performance of networks of N input neurons at classifying spatio-temporal spike patterns
by generating P = αN patterns  which we repeatedly present to the network. In each pattern 
each presynaptic neuron spikes exactly once at a ﬁxed time in each presentation  with spike times
uniformly distributed over the trial. Learning is organized in learning blocks. In each block all P
patterns are presented in randomized order. Synaptic weights are initialized as zero  and are updated
after each pattern presentation. After each block  we test if the postsynaptic output matches the
desired activity for each pattern. If during training a postsynaptic spike at t = 0 was induced  the
output can lie anytime in the testing trial for a positive outcome. To test scaling of the capacity 
we generate networks of 100 to 600 neurons and present the patterns until the classiﬁcation error
reaches a plateau. Examples of learning curves (Classiﬁcation error over time) are shown in Fig. 3.
For each combination of α and N   we run 40 simulations. The ﬁnal classiﬁcation error is the mean
over the last 500 blocks  averaged over all runs. The parameters we use in the simulations are shown
in Tab. 1. Fig. 3 shows the ﬁnal classiﬁcation performance as a function of the memory load α  for
all network sizes we use. Up to a load of 0.18  the networks learns to perfectly classify each pattern.
Higher loads leave a residual error which increases with load. The drop in performance is steeper
for larger networks. In comparison  the simpliﬁed Tempotron learning rule proposed in [5] achieves
perfect classiﬁcation up to α ≈ 1.5  i.e. one order of magnitude higher.

4.2 Chronotron learning with RSTDP

In the Chronotron [17] input spike patterns become associated with desired spike trains. There are
different learning rules which can achieve this mapping  including E–learning  I–learning  ReSuMe
and PBSNLR [17  19  20]. The plasticity mechanism presented here has the tendency to generate
postsynaptic spikes as close in time as possible to the teacher spike during recall. The presented
learning principle is therefore a candidate for Chronotron learning. The average distance of these

7

spikes depends on the time constants of hyperpolarization and the learning window  especially τpost.
The modiﬁcations of the model necessary to implement Chronotron learning are described in the
supplement. The resulting capacity  i.e. the ability to generate the desired spike times within a short
window in time  is shown in Fig. 3 B. Up to a load of α = 0.01  the recall is perfect within the limits
of the learning window τlw = 30ms. Inspection of the spike times reveals that the average distance
of output spikes to the respective teacher spike is much shorter than the learning window (≈ 2ms
for α = 0.01  see supplemental Fig. 1).

5 Discussion

We present a new and biologically highly plausible approach to learning in neuronal networks.
RSTDP with subthreshold LTD in concert with hyperpolarisation is shown to be mathematically
equivalent to the Perceptron learning rule for activity patterns consisting of synchronous spikes 
thereby inheriting the highly desirable properties of the PLR (convergence in ﬁnite time  stop condi-
tion if performance is sufﬁcient and robustness against noise). This provides a biologically plausible
mechanism to build associative memories with a capacity close to the theoretical maximum. Equiv-
alence of STDP with the PRL was shown before in [21]  but this equivalence only holds on average.
We would like to stress that we here present a novel approach that ensures exact mathematical eqi-
valence to the PRL.

The mechanism proposed here is complementary to a previous approach [13] which uses CSTDP
in combination with spike frequency adaptation to perform gradient descent learning on a squared
error. However  that approach relies on an explicit teacher signal  and is not applicable to auto-
associative memories in recurrent networks. Most importantly  the approach presented here inherits
the important feature of selfregulation and fast convergence from the original Perceptron which is
absent in [13].

For sparse spatio-temporal spike patterns extensive simulations show that the same mechanism is
able to learn Tempotrons and Chronotrons with substantial memory capacity. In the case of the
Tempotron  the capacity achieved with this mechanism is lower than with a comparably plausible
learning rule. However  in the case of the Chronotron the capacity comes close to the one obtained
with a commonly employed  supervised spike time learning rule. Moreover  these rules are biolog-
ically quite unrealistic. A prototypical example for such a supervised learning rule is the Temptron
rule proposed by G¨utig and Sompolinski [5]. Essentially  after a pattern presentation the complete
time course of the membrane potential during the presentation is examined  and if classiﬁcation was
erroneous  the synaptic weights which contributed most to the absolute maximum of the potential
are changed. In other words  the neurons would have to able to retrospectivly disentangle contri-
butions to their membrane potential at a certain time in the past. As we showed here  RSTDP with
subthreshold LTD together with postsynaptic hyperpolarization for the ﬁrst time provides a realistic
mechanism for Tempotron and Chronotron learning.

Spike after-hyperpolarization is often neglected in theoretical studies or assumed to only play a role
in network stabilization by providing refractoriness. Depolarization dependent STDP receives little
attention in modeling studies (but see [22])  possibly because there are only few studies which show
that such a mechanism exists [12  23]. The novelty of the learning mechanism presented here lies
in the constructive roles both play in concert. After-hyperpolarization allows synaptic potentiation
for presynaptic inputs immediately after the teacher spike without causing additional non-teacher
spikes  which would be detrimental for learning. During recall  the absence of the hyperpolarization
ensures the then desired threshold crossing of the membrane potential (see Fig. 2 B). Subthreshold
LTD guarantees convergence of learning. It counteracts synaptic potentiation when the membrane
potential becomes sufﬁciently high after the teacher spike. The combination of both provides the
learning margin  which makes the resulting network robust against noise in the input. Taken together 
our results show that the interplay of neuronal dynamics and synaptic plasticity rules can give rise
to powerful learning dynamics.

Acknowledgments

This work was in part funded by the German ministry for Science and Education (BMBF)  grant
number 01GQ0964. We are grateful to the anonymus reviewers who pointed out an error in ﬁrst
version of the proof.

8

References
[1] Hertz J  Krogh A  Palmer RG (1991) Introduction to the Theory of Neural Computation.  Addison-Wesley.
[2] Rosenblatt F (1957) The Perceptron–a perceiving and recognizing automaton. Report 85-460-1.
[3] Minsky ML  Papert SA (1969) Perceptrons. Cambridge  MA: MIT Press.
[4] Diederich S  Opper M (1987) Learning of correlated patterns in spin-glass networks by local learning rules.

Physical Review Letters 58(9):949-952.

[5] G¨utig R  Sompolinsky H (2006) The Tempotron: a neuron that learns spike timing-based decisions. Nature

Neuroscience 9(3):420-8.

[6] Dan Y  Poo M (2004) Spike Timing-Dependent Plasticity of Neural Circuits. Neuron 44:2330.
[7] Dan Y  Poo M (2006) Spike timing-dependent plasticity: from synapse to perception. Physiological Re-

views 86(3):1033-48.

[8] Caporale N  Dan Y (2008) Spike TimingDependent Plasticity: A Hebbian Learning Rule. Annual Review

of Neuroscience 31:2546.

[9] Froemke RC  Poo MM  Dan Y (2005) Spike-timing-dependent synaptic plasticity depends on dendritic

location. Nature 434:221-225.

[10] Sj¨ostr¨om PJ  H¨ausser M (2006) A Cooperative Switch Determines the Sign of Synaptic Plasticity in Distal

Dendrites of Neocortical Pyramidal Neurons. Neuron 51:227-238.

[11] Haas JS  Nowotny T  Abarbanel HDI (2006) Spike-Timing-Dependent Plasticity of Inhibitory Synapses

in the Entorhinal Cortex. Journal of Neurophysiology 96(6):3305-3313.

[12] Sj¨ostr¨om PJ  Turrigiano GG  Nelson SB (2004) Endocannabinoid-Dependent Neocortical Layer-5 LTD

in the Absence of Postsynaptic Spiking. J Neurophysiol 92:3338-3343

[13] D’Souza P  Liu SC  Hahnloser RHR (2010) Perceptron learning rule derived from spike-frequency adap-

tation and spike-time-dependent plasticity. PNAS 107(10):47224727.

[14] Song S  Miller KD  Abbott LF (2000) Competitive Hebbian learning through spike-timing-dependent

synaptic plasticity. Nature Neuroscience 3:919-926.

[15] Izhikevich EM  Desai NS (2003) Relating STDP to BCM. Neural Computation 15:1511-1523
[16] Vogels TP  Sprekeler H  Zenkel F  Clopath C  Gerstner W (2011) Inhibitory Plasticity Balances Excitation

and Inhibition in Sensory Pathways and Memory Networks. Science 334(6062):1569-1573.

[17] Florian RV (2012) The Chronotron: A Neuron That Learns to Fire Temporally Precise Spike Patterns.

PLoS ONE 7(8): e40233

[18] Rubin R  Monasson R  Sompolinsky H (2010) Theory of Spike Timing-Based Neural Classiﬁers. Physical

Review Letters 105(21): 218102.

[19] Ponulak F  Kasinski  A (2010) Supervised Learning in Spiking Neural Networks with ReSuMe: Sequence

Learning  Classiﬁcation  and Spike Shifting. Neural Computation 22:467-510

[20] Xu Y  Zeng X  Zhong S (2013) A New Supervised Learning Algorithm for Spiking Neurons. Neural

Computation 25: 1475-1511

[21] Legenstein R  Naeger C  Maass W (2005) What Can a Neuron Learn with Spike-Timing-Dependent

Plasticity? Neural Computation 17:2337-2382

[22] Clopath C  B¨using L  Vasilaki E  Gerstner W (2010) Connectivity reﬂects coding: a model of voltage-

based STDP with homeostasis. Nature Neuroscience 13:344-355

[23] Fino E  Deniau JM  Venance L (2009) Brief Subthreshold Events Can Act as Hebbian Signals for Long-

Term Plasticity. PLoS ONE 4(8): e6557

9

,Christian Albers
Maren Westkott
Klaus Pawelzik
Kartik Ahuja
William Zame
Mihaela van der Schaar
Hongteng Xu
Wenlin Wang
Wei Liu
Lawrence Carin