2019,Differentially Private Distributed Data Summarization under Covariate Shift,We envision Artificial Intelligence marketplaces to be platforms where consumers  with very less data for a target task  can obtain a relevant model by accessing many private data sources with vast number of data samples.  One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end  we consider the following distributed data summarizataion problem. Given K private source datasets denoted by $[D_i]_{i\in [K]}$ and a small target validation set $D_v$  which may involve a considerable covariate shift with respect to the sources  compute a summary dataset $D_s\subseteq \bigcup_{i\in [K]} D_i$ such that its statistical distance from the validation dataset $D_v$ is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art  for example in prototype selection (Kim et al.  NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model  where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator does not access more than $O(K^{\frac{1}{3}}|D_s| + |D_v|)$ points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the  best known non-private greedy algorithm. Our protocol uses two hash functions  one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. We introduce a novel ``noiseless'' differentially private auctioning protocol  which may be of independent interest.  Apart from theoretical guarantees  we demonstrate the efficacy of our protocol using real-world datasets.,Differentially Private Distributed Data
Summarization under Covariate Shift ⇤

Kanthi K. Sarpatwar 1

IBM Research

sarpatwa@us.ibm.com

Karthikeyan Shanmugam 1

IBM Research AI

karthikeyan.shanmugam2@ibm.com

Venkata Sitaramagiridharganesh Ganapavarapu

IBM Research

giridhar.ganapavarapu@ibm.com

Ashish Jagmohan

IBM Research

ashishja@us.ibm.com

Roman Vaculin
IBM Research

vaculin@us.ibm.com

Abstract

We envision Artiﬁcial Intelligence marketplaces to be platforms where consumers 
with very less data for a target task  can obtain a relevant model by accessing many
private data sources with vast number of data samples. One of the key challenges is
to construct a training dataset that matches a target task without compromising on
privacy of the data sources. To this end  we consider the following distributed data
summarizataion problem. Given K private source datasets denoted by [Di]i2[K]
and a small target validation set Dv  which may involve a considerable covariate

shift with respect to the sources  compute a summary dataset Ds ✓Si2[K] Di such

that its statistical distance from the validation dataset Dv is minimized. We use the
popular Maximum Mean Discrepancy as the measure of statistical distance. The
non-private problem has received considerable attention in prior art  for example in
prototype selection (Kim et al.  NIPS 2016). Our work is the ﬁrst to obtain strong
differential privacy guarantees while ensuring the quality guarantees of the non-
private version. We study this problem in a Parsimonious Curator Privacy Model 
where a trusted curator coordinates the summarization process while minimizing
the amount of private information accessed. Our central result is a novel protocol
that (a) ensures the curator accesses at most O(K 1
3|Ds| + |Dv|) points (b) has
formal privacy guarantees on the leakage of information between the data owners
and (c) closely matches the best known non-private greedy algorithm. Our protocol
uses two hash functions  one inspired by the Rahimi-Recht random features method
and the second leverages state of the art differential privacy mechanisms. Further 
we introduce a novel “noiseless” differentially private auctioning protocol for
winner notiﬁcation  which may be of independent interest. Apart from theoretical
guarantees  we demonstrate the efﬁcacy of our protocol using real-world datasets.

1

Introduction

Integrating new types of data to drive analytics based decision-making can contribute signiﬁcant
economic impact across a broad spectrum of industries including healthcare  banking  insurance 

⇤1Equal contribution by these authors.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

travel  and urban planning. This has led to the emergence of complex data ecosystems consisting of
heterogeneous (and overlapping) data generators  aggregators  and analytics providers. In general 
participants in these ecosystems are looking to monetize a class of assets that we term AI assets; such
assets include raw and aggregated data  as well as models trained on such data. A recent Mckinsey
global survey found  for example  that more than half of the respondents in sectors including basic
materials and energy  ﬁnancial services  and high tech stated that their companies had begun to
monetize their data assets [Gottlieb & Khaled (2017)].
In light of the above  in this work we consider the basic setting of an AI Marketplace : a consumer
arrives with a small dataset  referred to as a “validation” dataset  and wants to build a prediction model
that performs well on this dataset. However  the model training process requires huge amount of data 
that it must acquire from multiple private sources. Fundamentally  the AI Marketplace must address a
transfer learning problem  where the distribution of data at different sources is considerably different
from each other and even from the validation dataset. The Marketplace must facilitate transactions
of data points from multiple sources towards the consumer’s task by forming a training dataset that
is close in some distance measure to the validation dataset. In the process  it must preserve data
ownership and privacy as much as possible.
Consider the following scenario in the health care domain  as an example. Suppose the consumer
is a newly established cancer hospital and the data sources are cancer institutions from different
geographical locations across the globe. The goal of the new hospital is to construct ML models
that  say  can predict early onset of some form of cancer. The quality of the model depends on the
demography of its patients and therefore it is crucial to collect data that matches a small validation
set that is representative of the demography. The individual sources clearly have widely different
demographic data. The goal of an AI Marketplace is to enable private collection of a dataset sampled
from these sources that matches the demography of the new institute. From the privacy perspective 
there are two desirable properties: (a) The multiple data owners are typically “competitors” and
therefore  individual data must be protected (for e.g. in a differentially private manner) from each
other. (b) The platform (we use the term curator) must be “parsimonious” in handling data  i.e.  it
should access information on a “need to know” basis.
Motivated by the above  we consider the following problem. We consider K data owners with private
datasets D1  . . .   DK  and a data consumer who wishes to build a model for a speciﬁc task. The
speciﬁc task is embodied by the consumer possessing a validation dataset Dv. The data consumer
would like to procure a subset of data from each private dataset which is well-matched to its task.
A parsimonious trusted curator does the collection of points. We call this the parsimonious curator
model. Although trusted  we wish to minimize the number of points accessed by the curator to
construct the the ﬁnal summary. In turn  the K data owners would like to ensure that their data is
private with respect to other data owners. The curator exchanges messages and data points with the
data owners. We seek to make the exchanges by the curator to the data owners differentially private.
Our Contributions: We propose a novel protocol  based on an iterative hash-exchange mechanism 
that enables the curator to construct a summary data set Ds from the K owner datasets. A central
result of the paper shows that the proposed protocol simultaneously satisﬁes the following desired
properties: (i) The constructed dataset Ds is well-matched to the validation dataset Dv  in terms
of having a small Maximum Mean Discrepancy (MMD) (Gretton et al. (2008)); (ii) The protocol
exchanges with any data owner i is (✏  )-differentially private with respect to the other owner
datasets [j6=iDj; and (iii) The parsimonious curator accesses at most O(K 1
3|Ds| +|Dv|) data points.
Qualitatively  we expect the protocol to produce data summaries that are useful for model building
while maintaining differential privacy. We show through empirical evaluation that this is indeed the
case; by examining generalization error on two example tasks  we show that the protocol pays only a
small price for its differential privacy guarantees.
Prior Work: Privacy Preserving Learning Algorithms: There is a long line of work that considers
private empirical risk minimization that seeks to optimize the trade-off between accuracy of a trained
classiﬁer and differential privacy guarantees with respect to the training set [Kasiviswanathan et al.
(2011); Chaudhuri et al. (2011); Song et al. (2013); Kifer et al. (2012); Bassily et al. (2014); Shokri
& Shmatikov (2015); Hamm et al. (2016); Wu et al. (2016); Abadi et al. (2016); Pathak et al. (2010);
Thakurta (2013); Rubinstein et al. (2012); Talwar et al. (2015); Dwork et al. (2014a)]. One of the most
notable in this line of work is the idea of adding noise to stochastic gradient iterations to preserve
privacy [Song et al. (2013); Abadi et al. (2016); Shokri & Shmatikov (2015)]. We do not consider the

2

problem of learning a classiﬁer directly. Our goal is to summarize diverse data sources in a distributed
private manner to match a given validation set in a transfer learning setting. All the above works of
privacy preserving learning algorithms can be applied after our summarization step. In Rubinstein
et al. (2012); Chaudhuri et al. (2011)  authors use noisy Rahimi-Recht Fourier features to release a
representation of the support vectors for differentially private SVM classiﬁer release. Our purpose of
using Rahimi-Recht Fourier features is different and is used to expose the partial MMD objective at
every round and in conjunction with novel private auctioning mechanisms.
Privately Aggregating Teacher Ensembles: Several works have considered the following setting: An
ensemble of teacher classiﬁers  each trained on private data sources  noisily predict labels on an
unlabelled public dataset that is further used to train a student model [Papernot et al. (2016)]. Again 
this is different from our transfer learning setting where the various distributions are matched to a
target task ﬁrst handling covariate shift.
Another related line of work is differentially private submodular optimization [Mitrovic et al. (2017)].
While they consider a single private source  we handle multiple private data sources in optimizing a
speciﬁc statistical distance (M M D). Our techniques leverage state of the art methods on privacy
preserving mechanisms found in Dwork et al. (2014b); Hardt et al. (2012); Hardt & Rothblum (2010).
Domain Adaptation Methods: For the transfer learning problem  the existing domain adaptation
methods [Ganin & Lempitsky (2014); Tzeng et al. (2014)] ensure the following:
they learn a
representation (x) such that (·) of the source and the target are similar in distance (MMD metric
has been used to regularize the distance penalty) and that classifying based on (·) on the source
have very high accuracy. However  most existing approaches used differentiable models like deep
learning to achieve this - to learn (·). In our methods  we ﬁrst match the distributions in the ambient
space by sub-selecting points and then train any suitable classiﬁer. One advantage is that we can train
any classiﬁer after the moment matching step (Xgboost  Decision Tree  SVMs etc.). If one wants to
make an existing domain adaptation algorithm private with respect to any pair of participants  one
has to add noise to gradients computed at every step. The state of the art in differential privacy for
deep learning [Abadi et al. (2016)] (in the non-transfer learning setting) adds Gaussian noise whose
variance is linear in the number of iterations per step which signiﬁcantly degrades the performance.
In our method  we gain on this aspect as we add noise per point acquisition.
Federated Learning: We also note there is a distinction between our transfer learning setting from
that of Federated learning McMahan et al. (2016). The validation set distribution is distinct from
each of the individual data source distributions. There are signiﬁcant covariate shifts between these.
Federated learning would assume a training distribution which is obtained by sampling from different
data sources uniformly at random or with a speciﬁc mixture distribution. In fact  in our experiments
we contrast with training done on uniform samples which is a proxy for federated learning.

2 Problem Setting

The setting has K data owners with private datasets denoted by D1  D2  . . . DK. Here  Di 2 Rmi⇥n
where mi denotes the number of points and n denotes the their dimension. Further  there exists a
“consumer” entity that wants to form a summary dataset (which can be used for downstream training
goals) Ds ✓Si Di and |Ds| = p. The quality of the summary set is measured by its closeness to a
target validation dataset Dv 2 Rm⇥n which is private to the consumer. We measure the closeness of
Ds to Dv using the MMD (Maximum Mean Discrepancy) statistical distance deﬁned below.
Deﬁnition: The sample MMD distance for ﬁnite datasets D 2 Rm1⇥n and D0 2 Rm2⇥n is given by:
(1)

m1m2 Xx2D y2D0
where k(· ·) is a kernel function underlying an RKHS (Reproducing Kernel Hilbert Space) function
space such that k(x  y) = k(y  x) and k(· ·) is positive deﬁnite.
Differential Privacy: We adopt the following deﬁnition of differential privacy [Dwork et al. (2006)] in
our work. On a high level  it means that two datasets that differ in at most one point should not cause
a differentially private algorithm to produce output that are very different statistically. Formally 

2 Xy y02D0

1
m2

1 Xx x02D

k(x  y) +

1
m2

k(y  y0)

MMD2(D  D0) =

k(x  x0) 

2

3

Deﬁnition: The output of a randomized algorithm A(D) is (✏  ) differentially private with respect
to the input dataset D if for any two neighboring datasets D  D0 that differs in one data point 

P (A(D) 2 E)  e✏P (A(D0) 2 E) + .

(2)

for all events E that can be deﬁned on the output space.
Parsimonious Curator Privacy Model: We assume that there exists a trusted curator  called aggregator 
that collects the summary data points Ds. The participants holding data Di wish to preserve the
privacy of their individual data points. The model satisﬁes the following constraints:a) During the
protocol run  the curator must not have access to more than ⇢(|Ds| + |Dv|) points. We refer to such
a protocol as ⇢-parsimonious protocol. The aggregator needs to collect points that closely match Dv
in MMD distance. Therefore the aggregator at least sees |Ds| points in this framework. This forms a
natural |Ds| + |Dv| lower bound on how many points the aggregator has to access. Therefore  we
deﬁne a ⇢-parsimonious aggregator who sees ⇢ times the minimum required. b) Communication to a
non-trusted participant i is differentially private with respect to all other datasets i.e. [j6=iDi [ Dv.
This setting can be viewed as an intermediate regime between the “centralized setting" and “localized
setting” [Nissim & Stemmer (2017)] considered in the prior works. In other words  the source Di
knowing all but one point in the union of other datasets as side information must not know much
(in the differential privacy sense) about the missing point given all the communication to it during
the protocol (standard informed adversary model with respect to union of other datasets Sj6=i
Dj).
Preservation of differential privacy across data sources constrains the aggregator to collect more
points than necessary (i.e. |Ds| + |Dv|).
Main Problem: Is there a (✏  ) differentially private protocol in the parsimonious curator model  that
outputs a subset Ds ✓Si Di : |Ds| = p that (approximately) minimizes E[MMD2(Ds  Dv)] ?
Incentives: The aggregator needs to train a downstream task on a test distribution that is similar to
Dv. To this end  |Ds| points (much larger is size than Dv) are being collected for training. In fact 
one could think of the aggregator paying for the points. Our protocol is approximately the best way
to obtain such points. There is no incentive for the aggregator to cheat since it has to pay for the
collected points. The data providers are happy to provide a set as long as they are compensated and
other data sources do not know about their data (in a differential privacy sense.).
Every data source would be able to monetize their contribution in proportion to the value they provide
to the summary. After the protocol ends  value of a data source’s contribution could be deemed
proportional to the sum of winning marginal bids from the source. Value attribution based on this
would be a incentive for data holders to participate. We address the problem of value attribution
to data sources in a companion paper (Sarpatwar et al. (2019)). We only focus on the privacy and
parsimonious constraints.
Our Approach: We brieﬂy summarize the greedy approach to solve the moment matching problem
without privacy constraints. Our fundamental contribution is to make it differentially private in the
parsimonious curator model.
Greedy Algorithm Without Privacy: Our objective is to form a summary Ds of size p by collecting
points from all the data owners. We maximize the following normalized MMD objective [Kim et al.
(2016)] as described below. For ﬁxed validation set Dv such that |Dv| = m and the summary set Ds 
the objective J(Ds) is as follows:
J(Ds) = Xi j2Dv
Note that our objective here is different from the one used in Kim et al. (2016)  in that we do not have
the property S ✓ V . Submodularity of this function does not follow from their work directly. In
Section E of appendix  we show that the function is submodular under some condition on the kernel
function. This condition is satisﬁed if the distance between any two points is ⌦(plog N ) and when
the RBF kernel k(x  y) = exp(kx  yk2
Theorem 1. Let N be the total number of points in the system. Given a diagonally dominant kernel
matrix K 2 RN⇥N satisfying ki i = k⇤  for any i 2 [N ] and ki j 
N 3+3N 2+N for any i 6= j  then
J(S) is a non-negative  monotone and submodular function.

m2  MMD2(Dv  Ds) = Xi2Dv j2Ds

2) is used with some constant > 0.

k(yi  yj)

k⇤

2k(yi  xj)

m|Ds|  Xi j2Ds

k(xi  xj)
|Ds|2

(3)

4

It has been proven that the following iterative greedy approach yields a constant factor approximation
guarantee  given that the objective is a non-negative monotone submodular [Nemhauser et al. (1978)]
function. Iteratively  until the required summary size is achieved: (a) each participant computes
its marginally best point y  i.e.  that maximizes J(Ds + y)  J(Ds) and (b) curator collects the
marginally best points from various participants and adds the best among them to the summary.
Our Private Algorithm: The focus of the paper is to adapt this greedy approach with privacy
guarantees in the parsimonious curator model. In our private protocol  the curator collects the data
points in Ds in a greedy fashion as above. However  there is a key challenge on the privacy front:
Challenges: During the implementation of the greedy algorithm  the curator maintains a set of
points Ds = {x1  . . .   xk}. To calculate the marginal gain with respect to Equation (3)  we observe
that the curator needs to expose a function of the formP ↵ik(xi ·) to every participant for some
constants ↵i (this will become clear later). However  sharing the points in the raw form would be a
violation of privacy constraints at the participants. Further  over the course of multiple releases  any
participant must not be able to acquire any information about previous data points of other participants.
Therefore  the key issue is that the releases of the curator must be differentially private while enabling
the computation of the (non-linear) marginal gain  over all the iterations of the protocol. Beyond
enabling the computation of “best” points  privacy concerns also arise in the actual collection of data
points. Indeed  even a private declaration of “winners” to data providers would result in the leakage
of information on the quality of other data providers.
Our Solution: To solve these issues  we use two hash functions:
(a) h1(·) based on the random Fourier features method of Rahimi-Recht to hash every data point at
the curator. This hash function is common to all the entities (i.e.  curator and the participants) and
satisﬁes the property that h1(x)T h1(y) ⇡ k(x  y) w.h.p.  which is useful to convert the non-linear
kernel computation (Equation (3)) to a linear one. This enables approximate kernel computation by
an entity external to the curator. Therefore  any entity can compute the marginal gain of a new point
y byP ↵ik(xi  y) ⇡P ↵ih1(xi)T h1(y). Thus  the curator needs to only shareP ↵ih1(xi).
(b) a second hash function h2(·)  whose randomness is private to the curator such that 
h2(P ↵ih1(xi))T h1(y) ⇡ P ↵ik(xi  y) and h2 is differentially private with respect to h1(xi).
A speciﬁc participant can observe multiple releases ofP ↵ih1(xi) and potentially ﬁnd out the last
point that was added. Therefore  the releases of the sum vectorP ↵ih1(xi) needs to be protected.
Our h2(·) is a novel adaptation of the well-known MWEM method [Hardt et al. (2012)]. The key
technical challenge is to match the performance of the greedy algorithm while ensuring privacy
properties of h2(·) in order to protect data releases from the curator. Further  to address the privacy
concerns in parsimonious data collection  we obtain a novel private auction mechanism that is
O(K 1
3 )-parsimonious and (✏  )-differentially private  with no further loss in optimality. Aside from
theory  we provide insights to make our protocol well-suited for practice and demonstrate its efﬁcacy
on real world datasets.

3 The Protocol

Our protocol uses two different hash functions that we refer to as h1(.) and h2(.). The hash function
h1(.) is shared between the various data owners and the aggregator. The hash function h2(.) is used
by the aggregator to hash the current summary dataset before being broadcast to various participating
entities (owners). We now describe both the hash functions h1(·)  h2(·).
The Hash Function h1(·): Our ﬁrst hash function  which is shared and used by various data owners
and the aggregator is based on a well known distance preserving hash function formulated by Rahimi
& Recht (2008). Formally  the hash function is deﬁned in Algorithm 1. The main purpose of this
hash function is to ensure that h1(x)T h1(y) ⇡ k(kx  yk). We assume an RBF kernel function
throughout the paper which is given by k() = exp(2). In Algorithm 1  p(!) is the distribution
2⇡R ej! T k()d. Due to the
deﬁned by the Fourier transform of the kernel k()  i.e. p(!) = 1
RBF kernel  p(!) = N (0  2In). The randomness in the hash function is due to d random points
drawn from this distribution as in Algorithm 1.

5

1: Input: Point x 2 Rn  parameter   dimension parameter d
2: Output: h1(x)
i=1 i.i.d from the same distribution p(!) = N (0  2In) only once at the beginning
3: Draw {!i}d
of the protocol and reuse it over subsequent calls to h1(·).
4: Draw samples {bi}i2[d] i.i.d uniformly from [0  2⇡] only once at the beginning of the protocol.
5: return h1(x) =q 2

d⇥cos(!T
Algorithm 1: Computing the hash function h1(·).

d x + bd)⇤T

2 x + b2) + . . . cos(!T

1 x + b1)  cos(!T

d  vij  q 2

The Hash Function h2(·): Consider a dataset D 2 Rq⇥d consisting of vectors {v1  v2 . . . vq} such
that vi 2 R1⇥d and q 2
d   1  i  q  1  j  d. The hash function h2(D)
approximately computes the vector sum w(D) = Pi vi in a differentially private manner. Let
w(D  j) =Pi vij. We now provide the description of the h2(·) in Algorithm 2. The algorithm has
two components: (a) The algorithm ﬁrst quantizes the q vectors in D to obtain DQ such that the
quantized coordinate values are from a grid S of points S = {1 1+⌘  1+2⌘... . . . 1⌘  1}  for
a parameter ⌘ (refer to Line 10 in Algorithm 2). (b) Then a random distribution Pavg over the space
of all possible quantized vectors S1⇥d is found such that the expected vector under this distribution is
close to the sum of the quantized vectors in DQ. Further  the releases are also differential private.
This second part relies on the MWEM mechanism of Hardt et al. (2012).
Full Algorithmic Description of h2(·) : Let ˜v1  ˜v2 . . . ˜vq 2 S1⇥d be the quantized vectors in DQ and
w(DQ  i) =Pq
j=1 ˜vji. Now  we will deﬁne probability mass functions Pt(s 2 S1⇥d) for every time
t over the ﬁnite set S1⇥d whose cardinality is |S|d. Pt will be dependent only on Pt1. We will deﬁne
the distribution iteratively over t  T iterations. Deﬁne w(P  i) = q(Ps2S sPi(s)) with respect to a
probability mass function P on S1⇥d where Pi(s) is the marginal pmf on the i-th coordinate. The
way Pt is computed is given in Algorithm 2 (Steps 6-7).

7: end for
8: Pavg = 1

1: Input: Dataset D  parameters "  ⌘ and T
2: Output: h2(D  T  ")
3: Obtain DQ QUANTIZATION(D  ⌘). Let P0 be the uniform distribution over the set S1⇥d
4: for all t  [T ] do
5:

where S = {1 1 + ⌘  1 + 2⌘... . . . 1  ⌘  1}.

1
q [w(Pavg  1) . . . w(Pavg  d)] = h2(D  ") = h2(DQ " )

function: i(DQ) = |w(Pt1  i)  w(DQ  i)|. Let the sampled coordinate be i(t)
Pt1(s) exp [si(t)(µi(t)  w(Pt1  i(t)) )/2q]

Sample a coordinate i 2 [d] with probability proportional to exp (" i(DQ)) where the score
Let µi(t) w(DQ  i(t)) + Lap(1/"). Compute the distribution satisfying Pt(s) /
T Pt2[T ] Pt.
Deﬁne Q(x) = (
return DQ =⇢Q✓q d

x+1k✏
Q(v = (v1  v2  . . .   vd)) = (Q(v1)  Q(v2)  . . .   Q(vd))

) where k = b(x + 1)/⌘c. Let

9: returnq 2

1 + (k + 1)⌘w.p.

2 vi◆q

1 + k⌘ 

w.p. (k+1)⌘1x

 

i=1

6:

11:

d

10: procedure QUANTIZATION(D  ⌘)

⌘

⌘

12:
13: end procedure

Algorithm 2: Computing the hash function h2(·).

Description of the Protocol: We now describe our protocol in Algorithm 3 and the protocol parame-
ters ✏v ✏ ` T used. The protocol ensures two properties at the data owner:
Approximate Marginal Gain Computation: The trusted aggregator at the beginning (Step 4) shares ˜g =
h1(x)k1 is very small. Therefore  ˜gT h1(y) when

h2(h1(Dv)). We show that kh2(h1(Dv))Px2Dv

6

` h1(y) ⇡Px2Ds

computed at a data owner with a new point y approximatesPx2Dv
h1(x)T h1(y). Similarly  over
any other iteration ` (in Step 4)  the hashed vector g` is such that gT
h1(x)T h1(y).
Since  h1 has the property that h1(x)T h1(y) ⇡ k(kx  yk)  we can ensure that the maximization in
Step 13 is approximately the marginal gain computation J(Ds + y)  J(Ds).
Differential Privacy: We also show that  due to application of h2  all the releases seen by any
data owner i are differentially private with respect to the current summary which also implies it is
differentially private with respect to [j6=iDi  Di. Another key ingredient in our proof is in showing
that the novel scheme in making the bid collection and winner notiﬁcation process differentially
private  while ensuring the parsimonious nature of the aggregator. Consider the Step 7 in Algorithm 3.
Upon making a decision on the winning bid  the aggregator needs to acquire the winning point from
the winner data source. Consider the following two naive ways of doing this: (a) Aggregator notiﬁes
the winner alone about the decision and acquires the data point. (b) Aggregator acquires data points
from all the data sources. Keeping only the winner point  it discards the other points. An important
observation here is that the ﬁrst alternative is not differentially private. Indeed  it leaks information
about the data points of the participating data sources. The second way is differentially private 
indeed  each data source learns nothing new about other data sources. However  it is highly wasteful
and contradicts the parsimonious nature of the aggregator. Indeed  in forming a summary of size p  it
collects Kp data points. Our novel private auction (Steps 11-16 of Algorithm 3) obtains best of both
scenarios  i.e.  it is differentially private and accesses at most O(pK 1

3 ) data points in total.

1: Input: Di i 2 [K]  validation dataset Dv  seed set Dinit  params {✏auc ✏ v {✏` T}p
2: Output: Summary Ds: Ds ✓ [i2[K]Di such that |Ds| = p.
3: Aggregator initializes summary Ds Dinit and broadcasts ˜g = h2(h1(Dv) ✏ v).
4: for ` = 1 . . . p do
5:
6:
7:

Aggregator broadcasts g` = h2(h1(Ds) ✏ ` T ).
Each Data owner i 2 [n] computes its “bid”: bi = maxx2Di gT
Aggregator chooses the best point through a private auction:
xi⇤ PRIVAUCTION(bi : i 2 [n])
Aggregator veriﬁes the data point against the bid value and updates Ds Ds [ xi⇤.

`+1.
` h1(x)  ˜gT h1(x) `

`=1 ⌧ }.

8:
9: end for
10: return Summary Ds  Dinit.
11: procedure PRIVAUCTION(bi : i 2 [n])
12:
13:
14:
15:

Aggregator orders the data owners  as D01  D02  . . .   D0K  by their decreasingly bid values.
Independently with probability P[xi] = e✏auc(i1)  Aggregator asks for the point xi.
If a certain data point x was chosen ⌧ times by a data source (Step 6)  Aggregator asks for it.
Aggregator chooses a point x⇤  with maximum bid value b⇤  from the pool of all the points
Data owners disconsider all the points sent to the Aggregator in the future iterations.

obtained so far and not yet included in the summary.

16:
17: end procedure

Algorithm 3: Description of the protocol.

"v

d  T = d2  "v = ✏

16T   "` T =

q16T` log( 1

In Algorithm 3  for
  and

) log5 p  d  16(log 2N )(log p)2
˜
  we obtain the following guarantees:

  |Dinit| 121 ⇤ 8d2 log2 d log( 1
˜ ) log p

Theorem 2. Let a 2 (0  1) and ˜ 2 (0  1/e) be any ﬁxed constants.
|Dv| 44p2pd log d log2 p
setting ⌘  1
(Differential Privacy) Releases of the aggregator to any data owner i is (✏  ˜)-differentially pri-
vate over all the iterations/epochs with respect to the datasets [j6=iDi. Similarly  we have (✏  ˜)-
differentially privacy over all the iterations w.r.t. validation set Dv.
(Approximation Guarantee) Let OPT denote an optimal summary set and Ds be the set of points ob-
tained by Algorithm 3. We have J(Ds)  (1 1
)+a+ 1
✏ log p <
1. Barring the  additive error the guarantees are close to the non-private greedy algorithm.
(Parsimoniousness Guarantee) Algorithm 3 is O( log 1
summary of size p  it needs to access at most O(pK 1

e )J(OPT)  where  < O( log ppln dpd

3 )-parsimonious  i.e.  in computing a

a2



✏ K 1

3 ) data points.

✏

7

Differences between PRIVAUCTION and the Exponential Mechanism: There may be a superﬁcial
resemblance between Step 13 in the PRIVAUCTION procedure of Algorithm 3 and the exponential
mechanism. Actually  our private auction is signiﬁcantly different. First note that the probability of
choosing the best bid is 1 which is not the case with the exponential mechanism. Secondly  while the
exponential mechanism selects one approximately "best" point  we ﬂip a coin for every bid whose
bias has an exponentially decreasing relationship to the position of the bid in sorting order. Then  we
choose multiple of them (instead of one) and a key proof point is to show that we can restrict the
number of the points chosen overall. Finally  the bias probabilities do not even depend on the bid
value (i.e.  "score") while it would be the case for exponential mechanism.
Extension to a Less Trusted Curator. In our parsimonious curator model  the ﬁnal summary
dataset needs to be revealed to the trusted aggregator in order to train diverse models downstream. In
Section F of the Appendix  we show that Algorithm 3 can be adapted to share just h1(x) hashes of
data points. We show that this approach has some interesting privacy guarantees  speciﬁcally that the
aggregator can only know the pairwise Euclidean distances between the points and nothing more.
These hashes would be useful to train kernel based models such as Support Vector Machines.

4 Experimental Evaluation

q

for `> 1.

We make an important observation that is crucial to obtain good performance in practice. According
to Theorem 3  in order to control the additive error in approximating the query w(D i)
  Algorithm 2
needs: (a) T (the number of iterations) in Algorithm 2 to be larger than d2 to match the distribution
Pavg to the empirical distribution of coordinate i in the current summary Ds  (b) Dinit  the size of the
initial seed summary also needs to be large enough because of this (refer Theorem 2). Over multiple
epochs of Algorithm 3 (Step 4)   we make the following changes to deal with these issues. First
Epoch (` = 1): In practice  we ‘seed’ the protocol with a small initial seed set Dinit to satisfy (b)
and set T = Tinit to be large enough (d1.5) to satisfy (a). Subsequent Epochs (`> 1): Clearly  the
summary Ds grows and hence (b) is satisﬁed. We set T = Tsub to be a constant for subsequent
iterations. This may seem to contradict the requirement (a). However  we observe that h2(·) operates
on a summary that is only differing in one point from the previous iteration. Intuitively  a single
point addition results in a small shift in the empirical distribution. Small incremental changes to the
empirical distribution need to be matched incrementally. Thus  it is sufﬁcient to have a signiﬁcantly
smaller number of iterations than that in Theorem 4. Therefore  Tsub is set to be small. We set the
parameters of our algorithm as follows: the RBF kernel parameter  = 0.1  dimension of Rahimi-
Recht hash function h1(.) as d = 140. We use two different T parameters for different epochs given
by Tinit(= T ` = 1) = d1.5 = 1656 and Tsubs(= T ` > 1) = 5. "v = 0.01 is the ✏ parameter for
h2(·) for the validation set and "` T is set for h2(·) on summaries Ds over epochs ` as 0.05 for ` = 1 
0.01ppTsubs
Differential Privacy: An important observation here is that we do not need to preserve the privacy
of the seed set  since it can be completely random. We now bound the differential privacy of our
parameters with respect to both the consumer data and the summary data points. Consumer Dataset
(Dv): We compute h2(h1(Dv)) only once i.e.  in the ﬁrst epoch. This involves Tinit = 1656 iterations
in Algorithm 2  with "v = 0.01. Applying Theorem 7 (in Appendix)  we see that the total differential
privacy measure " = 1.4 (setting ˜ = 0.01). Summary Dataset (Ds): Over p epochs of Algorithm 3 
we have 5 iterations each with differential privacy
. Thus  again by applying Theorem 7 (in
Appendix)  we obtain a total differential privacy of 0.043 (with ˜ = 0.0001).
Experiments on Real World Datasets: We now back our theoretical results with empirical experi-
ments. We compare three algorithms: a) Non-Private Greedy  where the aggregator broadcasts the
(exact) average of the hashed summary set (i.e.  W (Ds i)
) and hashed validation set (i.e.  W (Dv i)
m ).
This is equivalent to the approach of Kim et al. (2016). b) Private Greedy  which is the Algorithm 3
with parameters set as above. c) Uniform Sampling  where we draw equal number p
K of required
samples from each data provider to construct a summary of size p. We empirically show that private
greedy closely matches the performance of non-private greedy even under the strong differential
privacy constraints. For comparison  we show that our algorithm outperforms uniform sampling. The
motivation for choosing the latter as a candidate comes from the typical manner of using stochastic
gradient descent approaches such as Federated Learning [McMahan et al. (2016)] that perform

0.01ppTsubs

q

8

uniform sampling. We experiment with two real world datasets. We discuss one of them  which is
based on an Allstate insurance dataset from a Kaggle (2014) competition. We show similar results for
the MNIST dataset  that contains image data for recognizing hand written digits  in the Appendix G.

Figure 1: All State Insurance Data: (Top): Comparison of the percentage increase in M M D2 of
both the private and uniform sampling algorithms with respect to baseline greedy algorithm. Lower
values indicate better performance. The private algorithm performs consistently better than uniform
sampling. (Bottom): Comparison of the classiﬁcation accuracy of the three algorithms using a Linear
SVM classiﬁer. Higher numbers indicate better performance. Our private algorithm outperforms
uniform sampling by 6-10% and closely matches the performance of the base line greedy algorithm.

All State Insurance Data: The dataset contains insurance data of customers belonging to different
states of the U.S. The objective is to predict labels of one of the all-state products. In our setup  we
use data corresponding to two states - Florida and Connecticut. We have four data owner participants 
and an aggregator. The data is split up as follows: Training data: The training data is comprised of
all the Florida data and 70% of the Connecticut data. The Florida data is split uniformly among the
four data owners and Connecticut data is given to one of them. This allows us to create a skew in the
data quality across different participants. Validation data: From the remaining 30% of Connecticut
data we choose 25% of data as the validation data set. Note that we remove the labels from this
validation set before giving it to the consumer. Testing data: The remaining Connecticut data is set
aside as testing data. Thus the training data is solely comprised of Connecticut data. Further  we
use around 150 points of random seed data belonging to a different state (Ohio). In our experiments 
we vary the number of samples that need to be collected and compute the M M D2 objective in
each of these cases. In Figure 1  we compare the increase in M M D2 with respect to greedy  i.e. 
MM D2(ALGM )MM D2(GREEDY )
⇥ 100 where ALGM is either our private greedy algorithm or
the uniform sampling algorithm. Our results show that we consistently beat the uniform sampling
algorithm while preserving differential privacy. In Figure 1  we compare the performance of these
algorithms using a linear SVM. We ﬁnd that the private algorithm while closely matching greedy
beats uniform sampling by 6% to 10%.

MM D2(GREEDY )

5 Discussion

We consider a distributed data summarization problem in a transfer learning setting with privacy
constraints. Different data owners have privacy constraints and a subset of points matching a target
dataset needs to be formed. We provide a differentially private algorithm for this problem in the
parsimonious curator setting  where the data owners do not wish to reveal information to other data
owners and a curator entity can only access limited number of points.

9

Acknowledgement

We thank Naoki Abe and Michele Franceshini for helpful discussions in the initial stages of this
work. We also thank anonymous reviewers for their thoughtful suggestions that helped improve our
presentation of the paper.

References
Abadi  M.  Chu  A.  Goodfellow  I.  McMahan  H. B.  Mironov  I.  Talwar  K.  and Zhang  L. Deep
In Proceedings of the 2016 ACM SIGSAC Conference on

learning with differential privacy.
Computer and Communications Security  pp. 308–318. ACM  2016.

Bassily  R.  Smith  A.  and Thakurta  A. Private empirical risk minimization: Efﬁcient algorithms
and tight error bounds. In Foundations of Computer Science (FOCS)  2014 IEEE 55th Annual
Symposium on  pp. 464–473. IEEE  2014.

Chaudhuri  K.  Monteleoni  C.  and Sarwate  A. D. Differentially private empirical risk minimization.

Journal of Machine Learning Research  12(Mar):1069–1109  2011.

Dwork  C.  Kenthapadi  K.  McSherry  F.  Mironov  I.  and Naor  M. Our data  ourselves: Privacy via
distributed noise generation. In Annual International Conference on the Theory and Applications
of Cryptographic Techniques  pp. 486–503. Springer  2006.

Dwork  C.  Nikolov  A.  and Talwar  K. Using convex relaxations for efﬁciently and privately
releasing marginals. In Proceedings of the thirtieth annual symposium on Computational geometry 
pp. 261. ACM  2014a.

Dwork  C.  Roth  A.  et al. The algorithmic foundations of differential privacy. Foundations and
Trends R in Theoretical Computer Science  9(3–4):211–407  2014b.
Ganin  Y. and Lempitsky  V. Unsupervised domain adaptation by backpropagation. arXiv preprint

arXiv:1409.7495  2014.

Giraud  B. G. and Peschanski  R. From" dirac combs" to fourier-positivity. arXiv preprint

arXiv:1509.02373  2015.

Gottlieb 

J. and Khaled  R.

Fueling growth through data monetization.

https:

//www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/
fueling-growth-through-data-monetization  December 2017.

Gretton  A.  Borgwardt  K. M.  Rasch  M. J.  Schölkopf  B.  and Smola  A. J. A kernel method for

the two-sample problem. CoRR  abs/0805.2368  2008.

Hamm  J.  Cao  Y.  and Belkin  M. Learning privately from multiparty data.

Conference on Machine Learning  pp. 555–563  2016.

In International

Hardt  M. and Rothblum  G. N. A multiplicative weights mechanism for privacy-preserving data
analysis. In Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pp.
61–70. IEEE  2010.

Hardt  M.  Ligett  K.  and McSherry  F. A simple and practical algorithm for differentially private

data release. In Advances in Neural Information Processing Systems  pp. 2339–2347  2012.

Jukna  S. Extremal combinatorics: with applications in computer science. Springer Science &

Business Media  2011.

Kaggle.

Allstate purchase prediction challenge.
allstate-purchase-prediction-challenge  2014.

https://www.kaggle.com/c/

Kairouz  P.  Oh  S.  and Viswanath  P. The composition theorem for differential privacy. IEEE

Transactions on Information Theory  63(6):4037–4049  2017.

Kasiviswanathan  S. P.  Lee  H. K.  Nissim  K.  Raskhodnikova  S.  and Smith  A. What can we learn

privately? SIAM Journal on Computing  40(3):793–826  2011.

10

Kifer  D.  Smith  A.  and Thakurta  A. Private convex empirical risk minimization and high-

dimensional regression. In Conference on Learning Theory  pp. 25–1  2012.

Kim  B.  Khanna  R.  and Koyejo  O. O. Examples are not enough  learn to criticize! criticism for

interpretability. In Advances in Neural Information Processing Systems  pp. 2280–2288  2016.

Mardia  K. V. and Jupp  P. E. Directional statistics  volume 494. John Wiley & Sons  2009.
McMahan  H. B.  Moore  E.  Ramage  D.  Hampson  S.  et al. Communication-efﬁcient learning of

deep networks from decentralized data. arXiv preprint arXiv:1602.05629  2016.

Mitrovic  M.  Bun  M.  Krause  A.  and Karbasi  A. Differentially private submodular maximization:
Data summarization in disguise. In International Conference on Machine Learning  pp. 2478–2487 
2017.

Nemhauser  G. L.  Wolsey  L. A.  and Fisher  M. L. An analysis of approximations for maximizing

submodular set functions—i. Mathematical Programming  14(1):265–294  1978.

Nissim  K. and Stemmer  U. Clustering algorithms for the centralized and local models. arXiv

preprint arXiv:1707.04766  2017.

Papernot  N.  Abadi  M.  Erlingsson  U.  Goodfellow  I.  and Talwar  K. Semi-supervised knowledge

transfer for deep learning from private training data. arXiv preprint arXiv:1610.05755  2016.

Pathak  M.  Rane  S.  and Raj  B. Multiparty differential privacy via aggregation of locally trained

classiﬁers. In Advances in Neural Information Processing Systems  pp. 1876–1884  2010.

Rahimi  A. and Recht  B. Random features for large-scale kernel machines. In Advances in neural

information processing systems  pp. 1177–1184  2008.

Rubinstein  B. I.  Bartlett  P. L.  Huang  L.  and Taft  N. Learning in a large function space: Privacy-
preserving mechanisms for svm learning. Journal of Privacy and Conﬁdentiality  4(1):65–100 
2012.

Sarpatwar  K. K.  Ganapavarapu  V. S.  Shanmugam  K.  Rahman  A.  and Vaculín  R. Blockchain
enabled AI marketplace: The price you pay for trust. In IEEE Conference on Computer Vision and
Pattern Recognition Workshops  CVPR Workshops 2019  Long Beach  CA  USA  June 16-20  2019 
pp. 0  2019.

Shokri  R. and Shmatikov  V. Privacy-preserving deep learning. In Proceedings of the 22nd ACM

SIGSAC conference on computer and communications security  pp. 1310–1321. ACM  2015.

Song  S.  Chaudhuri  K.  and Sarwate  A. D. Stochastic gradient descent with differentially private
updates. In Global Conference on Signal and Information Processing (GlobalSIP)  2013 IEEE  pp.
245–248. IEEE  2013.

Talwar  K.  Thakurta  A. G.  and Zhang  L. Nearly optimal private lasso. In Advances in Neural

Information Processing Systems  pp. 3025–3033  2015.

Thakurta  A. G. Differentially private convex optimization for empirical risk minimization and

high-dimensional regression. The Pennsylvania State University  2013.

Tzeng  E.  Hoffman  J.  Zhang  N.  Saenko  K.  and Darrell  T. Deep domain confusion: Maximizing

for domain invariance. arXiv preprint arXiv:1412.3474  2014.

Wu  X.  Kumar  A.  Chaudhuri  K.  Jha  S.  and Naughton  J. F. Differentially private stochastic

gradient descent for in-rdbms analytics. arXiv preprint arXiv:1606.04722  2016.

11

,Kanthi Sarpatwar
Karthikeyan Shanmugam
Venkata Sitaramagiridharganesh Ganapavarapu
Ashish Jagmohan
Roman Vaculin