2016,Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models,We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of "interaction screening". The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization  the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.,Interaction Screening: Efﬁcient and Sample-Optimal

Learning of Ising Models

Marc Vuffray1  Sidhant Misra2  Andrey Y. Lokhov1 3  and Michael Chertkov1 3 4

1Theoretical Division T-4  Los Alamos National Laboratory  Los Alamos  NM 87545  USA
2Theoretical Division T-5  Los Alamos National Laboratory  Los Alamos  NM 87545  USA

3Center for Nonlinear Studies  Los Alamos National Laboratory  Los Alamos  NM 87545  USA

4Skolkovo Institute of Science and Technology  143026 Moscow  Russia

{vuffray  sidhant  lokhov  chertkov}@lanl.gov

Abstract

We consider the problem of learning the underlying graph of an unknown Ising
model on p spins from a collection of i.i.d. samples generated from the model. We
suggest a new estimator that is computationally efﬁcient and requires a number of
samples that is near-optimal with respect to previously established information-
theoretic lower-bound. Our statistical estimator has a physical interpretation in
terms of “interaction screening”. The estimator is consistent and is efﬁciently
implemented using convex optimization. We prove that with appropriate regulariza-
tion  the estimator recovers the underlying graph using a number of samples that is
logarithmic in the system size p and exponential in the maximum coupling-intensity
and maximum node-degree.

1

Introduction

A Graphical Model (GM) describes a probability distribution over a set of random variables which
factorizes over the edges of a graph. It is of interest to recover the structure of GMs from random
samples. The graphical structure contains valuable information on the dependencies between the
random variables. In fact  the neighborhood of a random variable is the minimal set that provides us
maximum information about this variable. Unsurprisingly  GM reconstruction plays an important
role in various ﬁelds such as the study of gene expression [1]  protein interactions [2]  neuroscience
[3]  image processing [4]  sociology [5] and even grid science [6  7].
The origin of the GM reconstruction problem is traced back to the seminal 1968 paper by Chow and
Liu [8]  where the problem was posed and resolved for the special case of tree-structured GMs. In
this special tree case the maximum likelihood estimator is tractable and is tantamount to ﬁnding a
maximum weighted spanning-tree. However  it is also known that in the case of general graphs with
cycles  maximum likelihood estimators are intractable as they require computation of the partition
function of the underlying GM  with notable exceptions of the Gaussian GM  see for instance [9] 
and some other special cases  like planar Ising models without magnetic ﬁeld [10].
A lot of efforts in this ﬁeld has focused on learning Ising models  which are the most general
GMs over binary variables with pairwise interaction/factorization. Early attempts to learn the Ising
model structure efﬁciently were heuristic  based on various mean-ﬁeld approximations  e.g. utilizing
empirical correlation matrices [11  12  13  14]. These methods were satisfactory in cases when
correlations decrease with graph distance. However it was also noticed that the mean-ﬁeld methods
perform poorly for the Ising models with long-range correlations. This observation is not surprising

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

in light of recent results stating that learning the structure of Ising models using only their correlation
matrix is  in general  computationally intractable [15  16].
Among methods that do not rely solely on correlation matrices but take advantage of higher-order
correlations that can be estimated from samples  we mention the approach based on sparsistency of
the so-called regularized pseudo-likelihood estimator [17]. This estimator  like the one we propose
in this paper  is from the class of M-estimators i.e. estimators that are the minimum of a sum of
functions over the sampled data [22]. The regularized pseudo-likelihood estimator is regarded as
a surrogate for the intractable likelihood estimator with an additive (cid:96)1-norm penalty to encourage
sparsity of the reconstructed graph. The sparsistency-based estimator offers guarantees for the
structure reconstruction  but the result only applies to GMs that satisfy a certain condition that is
rather restrictive and hard to verify. It was also proven that the sparsity pattern of the regularized
pseudo-likelihood estimator fails to reconstruct the structure of graphs with long-range correlations 
even for simple test cases [18].
Principal tractability of structure reconstruction of an arbitrary Ising model from samples was proven
only very recently. Bresler  Mossel and Sly in [19] suggested an algorithm which reconstructs the
graph without errors in polynomial time. They showed that the algorithm requires number of samples
that is logarithmic in the number of variables. Although this algorithm is of a polynomial complexity 
it relies on an exhaustive neighborhood search  and the degree of the polynomial is equal to the
maximal node degree.
Prior to the work reported in this manuscript the best known procedure for perfect reconstruction
of an Ising model was through a greedy algorithm proposed by Bresler in [20]. Bresler’s algorithm
is based on the observation that the mutual information between neighboring nodes in an Ising
model is lower bounded. This observation allows to reconstruct the Ising graph perfectly with only
a logarithmic number of samples and in time quasi-quadratic in the number of variables. On the
other hand  Bresler’s algorithm suffers from two major practical limitations. First  the number of
samples  hence the running time as well  scales double exponentially with respect to the largest node
degree and with respect to the largest coupling intensity between pairs of variables. This scaling is
rather far from the information-theoretic lower-bound reported in [21] predicting instead a single
exponential dependency on the two aforementioned quantities. Second  Bresler’s algorithm requires
prior information on the maximum and minimum coupling intensities as well as on the maximum
node degree  guarantees which  in reality  are not necessarily available.
In this paper we propose a novel estimator for the graph structure of an arbitrary Ising model which
achieves perfect reconstruction in quasi-quartic time (although we believe it can be provably reduced
to quasi-quadratic time) and with a number of samples logarithmic in the system size. The algorithm
is near-optimal in the sense that the number of samples required to achieve perfect reconstruction 
and the run time  scale exponentially with respect to the maximum node-degree and the maximum
coupling intensity  thus matching parametrically the information-theoretic lower bound of [21]. Our
statistical estimator has the structure of a consistent M-estimator implemented via convex optimization
with an additional thresholding procedure. Moreover it allows intuitive interpretation in terms of
what we coin the “interaction screening”. We show that with a proper (cid:96)1-regularization our estimator
reconstructs couplings of an Ising model from a number of samples that is near-optimal. In addition 
our estimator does not rely on prior information on the model characteristics  such as maximum
coupling intensity and maximum degree.
The rest of the paper is organized as follows. In Section 2 we give a precise deﬁnition of the
structure estimation problem for the Ising models and we describe in detail our method for structure
reconstruction within the family of Ising models. The main results related to the reconstruction
guarantees are provided by Theorem 1 and Theorem 2. In Section 3 we explain the strategy and the
sequence of steps that we use to prove our main theorems. Proofs of Theorem 1 and Theorem 2
are summarized at the end of this Section. Section 4 illustrates performance of our reconstruction
algorithm via simulations. Here we show on a number of test cases that the sample complexity of the
suggested method scales logarithmically with the number of variables and exponentially with the
maximum coupling intensity. In Section 5 we discuss possible generalizations of the algorithm and
future work.

2

2 Main Results
Consider a graph G = (V  E) with p vertexes where V = {1  . . .   p} is the vertex set and E ⊂ V × V
is the undirected edge set. Vertexes i ∈ V are associated with binary random variables σi ∈ {−1  +1}
that are called spins. Edges (i  j) ∈ E are associated with non-zero real parameters θ∗
ij (cid:54)= 0
that are called couplings. An Ising model is a probability distribution µ over spin conﬁgurations
σ = {σ1  . . .   σp} that reads as follows:

where Z is a normalization factor called the partition function.

(i j)∈E

 (cid:88)
 (cid:88)

(i j)∈E

(cid:88)

σ

  
 .

µ (σ) =

1
Z

exp

θ∗
ijσiσj

Z =

exp

θ∗
ijσiσj

(1)

(2)

(4)

(5)

(6)

Notice that even though the main innovation of this paper – the efﬁcient “interaction screening”
estimator – can be constructed for the most general Ising models  we restrict our attention in this
paper to the special case of the Ising models with zero local magnetic-ﬁeld. This simpliﬁcation is not
necessary and is done solely to simplify (generally rather bulky) algebra. Later in the text we will
thus refer to the zero magnetic ﬁeld model (2) simply as the Ising model.

2.1 Structure-Learning of Ising Models

observed spin conﬁguration σ(k) = {σ(k)

Suppose that n sequences/samples of p spins(cid:8)σ(k)(cid:9)
ments/samples we aim to construct an estimator (cid:98)E of the edge set that reconstructs the structure
(cid:105)
P(cid:104)(cid:98)E = E
(cid:1) is a prescribed reconstruction error.
where  ∈(cid:0)0  1

k=1 ... n are observed. Let us assume that each
p } is i.i.d. from (1). Based on these measure-

exactly with high probability  i.e.

1   . . .   σ(k)

= 1 −  

(3)

2

We are interested to learn structures of Ising models in the high-dimensional regime where the
number of observations/samples is of the order n = O (ln p). A necessary condition on the number
of samples is given in [21  Thm. 1]. This condition depends explicitly on the smallest and largest
coupling intensity

and on the maximal node degree

α := min
(i j)∈E

|θ

∗

ij|  β := max
(i j)∈E

|θ

∗

ij| 

d := max
i∈V

|∂i|  

where the set of neighbors of a node i ∈ V is denoted by ∂i := {j | (i  j) ∈ E}.
According to [21]  in order to reconstruct the structure of the Ising model with minimum coupling
intensity α  maximum coupling intensity β  and maximum degree d  the required number of samples
should be at least

 eβd ln

(cid:16) pd
4 − 1

(cid:17)

4dαeα

n ≥ max

 .

 

ln p

2α tanh α

We see from Eq. (6) that the exponential dependence on the degree and the maximum coupling
intensity are both unavoidable. Moreover  when the minimal coupling is small  the number of samples
should scale at least as α−2.
It remains unknown if the inequality (6) is achievable. It is shown in [21  Thm. 3] that there exists a

reconstruction algorithm with error probability  ∈(cid:0)0  1

(cid:1) if the number of samples is greater than

(16 log p + 4 ln (2/)) .

(7)

(cid:32)

βd(cid:0)3e2βd + 1(cid:1)

(cid:33)2

2

n ≥

sinh2 (α/4)

3

Unfortunately  the existence proof presented in [21] is based on an exhaustive search with the
intractable maximum likelihood estimator and thus it does not guarantee actual existence of an
algorithm with low computational complexity. Notice also that the number of samples in (7) scales as
exp (4βd) when d and β are asymptotically large and as α−4 when α is asymptotically small.

2.2 Regularized Interaction Screening Estimator

2

The main contribution of this paper consists in presenting explicitly a structure-learning algorithm
that is of low complexity and which is near-optimal with respect to bounds (6) and (7). Our algorithm
reconstructs the structure of the Ising model exactly  as stated in Eq. (3)  with an error probability

(cid:1)  and with a number of samples which is at most proportional to exp (6βd) and α−2. (See

 ∈(cid:0)0  1

Theorem 1 and Theorem 2 below for mathematically accurate statements.) Our algorithm consists of
two steps. First  we estimate couplings in the vicinity of every node. Then  on the second step  we
threshold the estimated couplings that are sufﬁciently small to zero. Resulting zero coupling means
that the corresponding edge is not present.
Denote the set of couplings around node u ∈ V by the vector θ
u ∈ Rp−1. In this  slightly abusive
∗
notation  we use the convention that if a coupling is equal to zero it reads as absence of the edge  i.e.
ui = 0 if and only if (u  i) /∈ E. Note that if the node degree is bounded by d  it implies that the
θ∗
vector of couplings θ
Our estimator for couplings around node u ∈ V is based on the following loss function coined the
Interaction Screening Objective (ISO):

∗
u is non-zero in at most d entries.

n(cid:88)

k=1

− (cid:88)

i∈V \u

Sn (θu) =

1
n

exp

θuiσ(k)

u σ(k)

i

 .

The ISO is an empirical weighted-average and its gradient is the vector of weighted pair-correlations
∗
u the exponential weight cancels exactly with the corresponding factor in the
involving σu. At θu = θ
distribution (1). As a result  weighted pair-correlations involving σu vanish as if σu was uncorrelated
with any other spins or completely “screened” from them  which explains our choice for the name of
the loss function. This remarkable “screening” feature of the ISO suggests the following choice of
the Regularized Interaction Screening Estimator (RISE) for the interaction vector around node u:

(cid:98)θu (λ) = argmin

θu∈Rp−1

Sn (θu) + λ(cid:107)θu(cid:107)1  

where λ > 0 is a tunable parameter promoting sparsity through the additive (cid:96)1-penalty. Notice
that the ISO is the empirical average of an exponential function of θu which implies it is convex.
Moreover  addition of the (cid:96)1-penalty preserves the convexity of the minimization objective in Eq. (9).
As expected  the performance of RISE does depend on the choice of the penalty parameter λ. If λ is

too small(cid:98)θu (λ) is too sensitive to statistical ﬂuctuations. On the other hand  if λ is too large(cid:98)θu (λ)
Theorem 1 (Square Error of RISE). Let(cid:8)σ(k)(cid:9)

has too much of a bias towards zero. In general  the optimal value of λ is hard to guess. Luckily  the
following theorem provides strong guarantees on the square error for the case when λ is chosen to be
sufﬁciently large.

k=1 ... n be n realizations of p spins drawn i.i.d. from
an Ising model with maximum degree d and maximum coupling intensity β. Then for any node u ∈ V
and for any 1 > 0  the square error of the Regularized Interaction Screening Estimator (9) with
penalty parameter λ = 4

is bounded with probability at least 1 − 1 by

(8)

(9)

(10)

n

√

(cid:115)

(cid:13)(cid:13)(cid:13)2

(cid:113) ln(3p/1)
(cid:13)(cid:13)(cid:13)(cid:98)θu (λ) − θ
(cid:110)
(i  j) ∈ V × V |(cid:98)θij (λ) +(cid:98)θji (λ) ≥ α

d (d + 1) e3βd

ln 3p
1
n

≤ 28

∗
u

 

(cid:98)E (λ  α) =

(cid:111)

whenever n ≥ 214d2 (d + 1)2 e6βd ln 3p2
Our structure estimator (for the second step of the algorithm)  Structure-RISE  takes RISE output and
thresholds couplings whose absolute value is less than α/2 to zero:

1

.

.

(11)

Performance of the Structure-RISE is fully quantiﬁed by the following Theorem.

4

Theorem 2 (Structure Learning of Ising Models). Let(cid:8)σ(k)(cid:9)
(cid:17) ≥ 1 − 2 

k=1 ... n be n realizations of p spins
drawn i.i.d. from an Ising model with maximum degree d  maximum coupling intensity β and minimal
coupling intensity α. Then for any 2 > 0  Structure-RISE with penalty parameter λ = 4
reconstructs the edge-set perfectly with probability

P(cid:16)(cid:98)E (λ  α) = E

(cid:113) ln(3p2/2)

whenever n ≥ max(cid:0)d/16  α−2(cid:1) 218d (d + 1)2 e6βd ln 3p3

(12)

n

.

2

Proofs of Theorem 1 and Theorem 2 are given in Subsection 3.3.
Theorem 1 states that RISE recovers not only the structure but also the correct value of the couplings
up to an error based on the available samples. It is possible to improve the square-error bound (10)
even further by ﬁrst  running Structure-RISE to recover edges  and then re-running RISE with λ = 0
for the remaining non-zero couplings.
The computational complexity of RISE is equal to the complexity of minimizing the convex ISO and 

as such  it scales at most as O(cid:0)np3(cid:1). Therefore  computational complexity of Structure-RISE scales
at most as O(cid:0)np4(cid:1) simply because one has to call RISE at every node. We believe that this running-
Structure-RISE with running-time O(cid:0)np2(cid:1).

time estimate can be proven to be quasi-quadratic when using ﬁrst-order minimization-techniques  in
the spirit of [23]. We have observed through numerical experiments that such techniques implement

Notice that in order to implement RISE there is no need for prior knowledge on the graph parameters.
This is a considerable advantage in practical applications where the maximum degree or bounds on
couplings are often unknown.

3 Analysis

The Regularized Interaction Screening Estimator (9) is from the class of the so-called regularized
M-estimators. Negahban et al. proposed in [22] a framework to analyze the square error of such
estimators. As per [22]  enforcing only two conditions on the loss function is sufﬁcient to get a handle
on the square error of an (cid:96)1-regularized M-estimator.
The ﬁrst condition links the choice of the penalty parameter to the gradient of the objective function.
Condition 1. The (cid:96)1-penalty parameter strongly enforces regularization if it is greater than any
partial derivatives of the objective function at θu = θ∗
λ ≥ 2(cid:107)∇Sn (θ∗

u  i.e.
u)(cid:107)∞ .

(13)

κ then the square-error is bounded by
d λ
≤ 3

√

(cid:13)(cid:13)(cid:13)(cid:98)θu − θ

∗
u

(cid:13)(cid:13)(cid:13)2

5

u has at most d non-zero entries  then the

Condition 1 guarantees that if the vector of couplings θ∗

estimation difference(cid:98)θu (λ) − θ

∗
u lies within the set

(cid:110)
√
∆ ∈ Rp−1 | (cid:107)∆(cid:107)1 ≤ 4

(cid:111)

d(cid:107)∆(cid:107)2

(14)
The second condition ensure that the objective function is strongly convex in a restricted subset of
Rp−1. Denote the reminder of the ﬁrst-order Taylor expansion of the objective function by

K :=

.

δSn (∆u  θ∗

u) := Sn (θ∗

u + ∆u) − Sn (θ∗

u) − (cid:104)∇Sn (θ∗

u)   ∆u(cid:105)  

where ∆u ∈ Rp−1 is an arbitrary vector. Then the second condition reads as follows.
Condition 2. The objective function is restricted strongly convex with respect to K on a ball of
u  if for all ∆u ∈ K such that (cid:107)∆u(cid:107)2 ≤ R  there exists a constant κ > 0
radius R centered at θu = θ∗
such that
Strong regularization and restricted strong convexity enables us to control that the minimizer(cid:98)θu of
(16)

the full objective (9) lies in the vicinity of the sparse vector of parameters θ∗
is given in the proposition following from [22  Thm. 1].
√
Proposition 1. If the (cid:96)1-regularized M-estimator of the form (9) satisﬁes Condition 1 and Condition
2 with R > 3

u. The precise formulation

u) ≥ κ(cid:107)∆u(cid:107)2
2 .

δSn (∆u  θ∗

(15)

d

λ
κ

.

(17)

3.1 Gradient Concentration
Like the ISO (8)  its gradient in any component l ∈ V \ u is an empirical average

∂

∂θul

Sn (θu) =

1
n

X (k)

ul (θu)  

k=1

n(cid:88)
− (cid:88)

i∈V \u

 .

Xul (θu) = −σuσl exp

θuiσuσi

where the random variables X (k)
to

ul (θu) are i.i.d and they are related to the spin conﬁgurations according

(18)

(19)

(20)

(22)

In order to prove that the ISO gradient concentrates we have to state few properties of the support 
the mean and the variance of the random variables (19)  expressed in the following three Lemmas.
The ﬁrst of the Lemmas states that at θu = θ
Lemma 1. For any Ising model with p spins and for all l (cid:54)= u ∈ V

∗
u  the random variable Xul (θ

∗
u) has zero mean.

E [Xul (θ

∗
u)] = 0.

E(cid:104)

u)2(cid:105)

∗

As a direct corollary of the Lemma 1  θu = θ
The second Lemma proves that at θu = θ
Lemma 2. For any Ising model with p spins and for all l (cid:54)= u ∈ V

∗
u  the random variable Xul (θ

∗
u is always a minimum of the averaged ISO (8).

∗
u) has a variance equal to one.

Xul (θ

= 1.

(21)

The next lemma states that at θu = θ
Lemma 3. For any Ising model with p spins  with maximum degree d and maximum coupling
intensity β  it is guaranteed that for all l (cid:54)= u ∈ V

∗
u  the random variable Xul (θ

∗
u) has a bounded support.

|Xul (θ

u)| ≤ exp (βd) .
∗

With Lemma 1  2 and 3  and using Berstein’s inequality we are now in position to prove that every
partial derivative of the ISO concentrates uniformly around zero as the number of samples grows.
Lemma 4. For any Ising model with p spins  with maximum degree d and maximum coupling
intensity β. For any 3 > 0  if the number of observation satisﬁes n ≥ exp (2βd) ln 2p
  then the
following bound holds with probability at least 1 − 3:

3

(cid:115)
u)(cid:107)∞ ≤ 2
∗

ln 2p
3
n

.

(23)

(cid:107)∇Sn (θ

3.2 Restricted Strong-Convexity

n(cid:88)

(cid:32)
−(cid:88)

(cid:33)

 (cid:88)

  

The remainder of the ﬁrst-order Taylor-expansion of the ISO  deﬁned in Eq. (15) is explicitly
computed

k=1

exp

u σ(k)

θ∗
uiσ(k)

δSn (∆u  θ∗) =

1
n
where f (z) := e−z − 1 + z.
In the following lemma we prove that Eq. (24) is controlled by a much simpler expression using a
lower-bound on f (z).
Lemma 5. For all ∆u ∈ Rp−1  the remainder of the ﬁrst-order Taylor expansion admits the following
lower-bound

∆uiσ(k)

u σ(k)

i∈V \u

i∈∂u

(24)

f

i

i

δSn (∆u  θ∗) ≥

e−βd

2 + (cid:107)∆u(cid:107)1

∆(cid:62)
u H n∆u

(cid:80)n

k=1 σ(k)

i σ(k)

j

(25)
for i  j ∈ V \ u.

where H n is an empirical covariance matrix with elements H n

ij = 1
n

6

Lemma 5 enables us to control the randomness in δSn (∆u  θ∗) through the simpler matrix H n that
is independent of ∆u. This last point is crucial as we show in the next lemma that H n concentrates
independently of ∆u towards its mean.
Lemma 6. Consider an Ising model with p spins  with maximum degree d and maximum coupling
intensity β. Let δ > 0  4 > 0 and n ≥ 2
. Then with probability greater than 1 − 4  we have
for all i  j ∈ V \ u

δ2 ln p2

4

(cid:12)(cid:12)H n

ij − Hij

(cid:12)(cid:12) ≤ δ 

where the matrix H is the covariance matrix with elements Hij = E [σiσj]  for i  j ∈ V \ u.
The last ingredient that we need is a proof that the smallest eigenvalue of the covariance matrix H is
bounded away from zero independently of the dimension p. Equivalently the next lemma shows that
the quadratic form associated with H is non-degenerate regardless of the value of p.
Lemma 7. Consider an Ising model with p spins  with maximum degree d and maximum coupling
intensity β. For all ∆u ∈ Rp−1 the following bound holds

u H∆u ≥ e−2βd
∆(cid:62)

d + 1

(cid:107)∆u(cid:107)2
2 .

(27)

We stress that Lemma 7 is a deterministic result valid for all ∆u ∈ Rp−1. We are now in position to
prove the restricted strong convexity of the ISO.
Lemma 8. Consider an Ising model with p spins  with maximum degree d and maximum coupling
intensity β. For all 4 > 0 and R > 0  when n ≥ 211d2 (d + 1)2 e4βd ln p2
the ISO (8) satisﬁes  with
probability at least 1 − 4  the restricted strong convexity condition

4

(26)

(28)

δSn (∆u  θ∗

(cid:16)

(cid:17) (cid:107)∆u(cid:107)2
u) ≥
4 (d + 1)
√
d(cid:107)∆u(cid:107)2 and (cid:107)∆u(cid:107)2 ≤ R.

e−3βd
1 + 2

√

dR

2  

for all ∆u ∈ Rp−1 such that (cid:107)∆u(cid:107)1 ≤ 4
3.3 Proof of the main Theorems

(cid:113) 1

Proof of Theorem 1 (Square Error of RISE). We seek to apply Proposition 1 to the Regularized Inter-
n ln 3p/1  it follows
action Screening Estimator (9). Using 3 = 21
that Condition 1 is satisﬁed with probability greater than 1 − 21/3  whenever n ≥ e2βd ln 3p
. Using
√
√
4 = 1/3 in Lemma 8  and observing that 12
dR) < R  for R = 2/
d
and n ≥ 214d2 (d + 1)2 e6βd ln 3p2
  we conclude that condition 2 is satisﬁed with probability greater
than 1 − 1

3 . Theorem 1 then follows by using a union bound and then applying Proposition 1.

3 in Lemma 4 and letting λ = 4

dλe3βd(d + 1)(1 + 2

√

1

1

The proof of Theorem 2 becomes an immediate application of Theorem 1 for achieving an estimation
of couplings at each node with squared-error of α/2 and with probability 1 − 1 = 1 − 2/p.

4 Numerical Results

(cid:113) 1

We test performance of the Struct-RISE  with the strength of the l1-regularization parametrized by
n ln(3p2/)  on Ising models over two-dimensional grid with periodic boundary conditions
λ = 4
(thus degree of every node in the graph is 4). We have observed that this topology is one of the
hardest for the reconstruction problem. We are interested to ﬁnd the minimal number of samples 
nmin  such that the graph is perfectly reconstructed with probability 1 −  ≥ 0.95. In our numerical
experiments  we recover the value of nmin as the minimal n for which Struct-RISE outputs the perfect
structure 45 times from 45 different trials with n samples  thus guaranteeing that the probability of
perfect reconstruction is greater than 0.95 with a statistical conﬁdence of at least 90%.
We ﬁrst verify the logarithmic scaling of nmin with respect to the number of spins p. The couplings
are chosen uniform and positive θ∗
ij = 0.7. This choice ensures that samples generated by Glauber

7

√

p × √

Figure 1: Left: Linear-exponential plot showing the observed relation between nmin and p. The graph
p two-dimensional grid with uniform and positive couplings θ∗ = 0.7. Right: Linear-
is a
exponential plot showing the observed relation between nmin and β. The graph is the two-dimensional
4 × 4 grid. In red the couplings are uniform and positive and in blue the couplings have uniform
intensity but random sign.

dynamics are i.i.d. according to (1). Values of nmin for p ∈ {9  16  25  36  49  64} are shown on the
left in Figure 1. Empirical scaling is  ≈ 1.1 × 105 ln p  which is orders of magnitude better than the
rather conservative prediction of the theory for this model  3.2 × 1015 ln p.
We also test the exponential scaling of nmin with respect to the maximum coupling intensity β. The
test is conducted over two different settings both with p = 16 spins: the ferromagnetic case where all
couplings are uniform and positive  and the spin glass case where the sign of couplings is assigned

uniformly at random. In both cases the absolute value of the couplings (cid:12)(cid:12)θ∗

(cid:12)(cid:12)  is uniform and equal

ij

to β. To ensure that the samples are i.i.d  we sample directly from the exhaustive weighted list of
the 216 possible spin conﬁgurations. The structure is recovered by thresholding the reconstructed
couplings at the value α/2 = β/2.
Experimental values of nmin for different values of the maximum coupling intensity  β  are shown
on the right in Fig. 1. Empirically observed exponential dependence on β is matched best by 
exp (12.8β)  in the ferromagnetic case and by  exp (5.6β)  in the case of the spin glass. Theoretical
bound for d = 4 predicts exp (24β). We observe that the difference in sample complexity depends
signiﬁcantly on the type of interaction. An interesting observation one can make based on these
experiments is that the case which is harder from the sample-generating perspective is easier for
learning and vice versa.

5 Conclusions and Path Forward

In this paper we construct and analyze the Regularized Interaction Screening Estimator (9). We show
that the estimator is computationally efﬁcient and needs an optimal number of samples for learning
Ising models. The RISE estimator does not require any prior knowledge about the model parameters
for implementation and it is based on the minimization of the loss function (8)  that we call the
Interaction Screening Objective. The ISO is an empirical average (over samples) of an objective
designed to screen an individual spin/variable from its factor-graph neighbors.
Even though we focus in this paper solely on learning pair-wise binary models  the “interaction
screening” approach we introduce here is generic. The approach extends to learning other Graphical
Models  including those over higher (discrete  continuous or mixed) alphabets and involving high-
order (beyond pair-wise) interactions. These generalizations are built around the same basic idea
pioneered in this paper – the interaction screening objective is (a) minimized over candidate GM
parameters at the actual values of the parameters we aim to learn; and (b) it is an empirical average over
samples. In the future  we plan to explore further theoretical and experimental power  characteristics
and performance of the generalized screening estimator.
Acknowledgment: We are thankful to Guy Bresler and Andrea Montanari for valuable discussions 
comments and insights. The work was supported by funding from the U.S. Department of Energy’s
Ofﬁce of Electricity as part of the DOE Grid Modernization Initiative.

8

References
[1] D. Marbach  J. C. Costello  R. Kuffner  N. M. Vega  R. J. Prill  D. M. Camacho  K. R. Allison  M. Kellis 
J. J. Collins  and G. Stolovitzky  “Wisdom of crowds for robust gene network inference ” Nat Meth  vol. 9 
pp. 796–804  Aug 2012.

[2] F. Morcos  A. Pagnani  B. Lunt  A. Bertolino  D. S. Marks  C. Sander  R. Zecchina  J. N. Onuchic  T. Hwa 
and M. Weigt  “Direct-coupling analysis of residue coevolution captures native contacts across many
protein families ” Proceedings of the National Academy of Sciences  vol. 108  no. 49  pp. E1293–E1301 
2011.

[3] E. Schneidman  M. J. Berry  R. Segev  and W. Bialek  “Weak pairwise correlations imply strongly correlated

network states in a neural population ” Nature  vol. 440  pp. 1007–1012  Apr 2006.

[4] S. Roth and M. J. Black  “Fields of experts: a framework for learning image priors ” in Computer Vision
and Pattern Recognition  2005. CVPR 2005. IEEE Computer Society Conference on  vol. 2  pp. 860–867
vol. 2  June 2005.

[5] N. Eagle  A. S. Pentland  and D. Lazer  “Inferring friendship network structure by using mobile phone

data ” Proceedings of the National Academy of Sciences  vol. 106  no. 36  pp. 15274–15278  2009.

[6] M. He and J. Zhang  “A dependency graph approach for fault detection and localization towards secure

smart grid ” IEEE Transactions on Smart Grid  vol. 2  pp. 342–351  June 2011.

[7] D. Deka  S. Backhaus  and M. Chertkov  “Structure learning and statistical estimation in distribution

networks ” submitted to IEEE Control of Networks; arXiv:1501.04131; arXiv:1502.07820  2015.

[8] C. Chow and C. Liu  “Approximating discrete probability distributions with dependence trees ” IEEE

Transactions on Information Theory  vol. 14  pp. 462–467  May 1968.

[9] A. d’Aspremont  O. Banerjee  and L. E. Ghaoui  “First-order methods for sparse covariance selection ”

SIAM Journal on Matrix Analysis and Applications  vol. 30  no. 1  pp. 56–66  2008.

[10] J. K. Johnson  D. Oyen  M. Chertkov  and P. Netrapalli  “Learning planar ising models ” Journal of Machine

Learning  in press; arXiv:1502.00916  2015.

[11] T. Tanaka  “Mean-ﬁeld theory of Boltzmann machine learning ” Phys. Rev. E  vol. 58  pp. 2302–2310  Aug

1998.

[12] H. J. Kappen and F. d. B. Rodríguez  “Efﬁcient learning in Boltzmann machines using linear response

theory ” Neural Computation  vol. 10  no. 5  pp. 1137–1156  1998.

[13] Y. Roudi  J. Tyrcha  and J. Hertz  “Ising model for neural data: Model quality and approximate methods

for extracting functional connectivity ” Phys. Rev. E  vol. 79  p. 051915  May 2009.

[14] F. Ricci-Tersenghi  “The Bethe approximation for solving the inverse Ising problem: a comparison with
other inference methods ” Journal of Statistical Mechanics: Theory and Experiment  vol. 2012  no. 08 
p. P08015  2012.

[15] G. Bresler  D. Gamarnik  and D. Shah  “Hardness of parameter estimation in graphical models ” in
Advances in Neural Information Processing Systems 27 (Z. Ghahramani  M. Welling  C. Cortes  N. D.
Lawrence  and K. Q. Weinberger  eds.)  pp. 1062–1070  Curran Associates  Inc.  2014.

[16] A. Montanari  “Computational implications of reducing data to sufﬁcient statistics ” Electron. J. Statist. 

vol. 9  no. 2  pp. 2370–2390  2015.

[17] P. Ravikumar  M. J. Wainwright  and J. D. Lafferty  “High-dimensional Ising model selection using

(cid:96)1-regularized logistic regression ” Ann. Statist.  vol. 38  pp. 1287–1319  06 2010.

[18] A. Montanari and J. A. Pereira  “Which graphical models are difﬁcult to learn? ” in Advances in Neural
Information Processing Systems 22 (Y. Bengio  D. Schuurmans  J. D. Lafferty  C. K. I. Williams  and
A. Culotta  eds.)  pp. 1303–1311  Curran Associates  Inc.  2009.

[19] G. Bresler  E. Mossel  and A. Sly  “Reconstruction of Markov random ﬁelds from samples: Some

observations and algorithms ” SIAM Journal on Computing  vol. 42  no. 2  pp. 563–578  2013.

[20] G. Bresler  “Efﬁciently learning Ising models on arbitrary graphs ” in Proceedings of the Forty-Seventh

Annual ACM on Symposium on Theory of Computing  pp. 771–782  ACM  2015.

[21] N. P. Santhanam and M. J. Wainwright  “Information-theoretic limits of selecting binary graphical models

in high dimensions ” IEEE Transactions on Information Theory  vol. 58  pp. 4117–4134  July 2012.

[22] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu  “A uniﬁed framework for high-dimensional

analysis of M-estimators with decomposable regularizers ” Statist. Sci.  vol. 27  pp. 538–557  11 2012.

[23] A. Agarwal  S. Negahban  and M. J. Wainwright  “Fast global convergence of gradient methods for

high-dimensional statistical recovery ” Ann. Statist.  vol. 40  pp. 2452–2482  10 2012.

9

,Aditya Bhaskara
Silvio Lattanzi
Vahab Mirrokni
Cengiz Pehlevan
Dmitri Chklovskii
Marc Vuffray
Sidhant Misra
Andrey Lokhov
Michael Chertkov