2010,Policy gradients in linearly-solvable MDPs,We present policy gradient results within the framework of linearly-solvable MDPs. For the first time  compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function  rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.,Policy gradients in linearly-solvable MDPs

Applied Mathematics and Computer Science & Engineering

Emanuel Todorov

University of Washington

todorov@cs.washington.edu

Abstract

We present policy gradient results within the framework of linearly-solvable
MDPs. For the ﬁrst time  compatible function approximators and natural pol-
icy gradients are obtained by estimating the cost-to-go function  rather than the
(much larger) state-action advantage function as is necessary in traditional MDPs.
We also develop the ﬁrst compatible function approximators and natural policy
gradients for continuous-time stochastic systems.

1 Introduction

Policy gradient methods [18] in Reinforcement Learning have gained popularity  due to the guar-
anteed improvement in control performance over iterations (which is often lacking in approximate
policy or value iteration) as well as the discovery of more efﬁcient gradient estimation methods.
In particular it has been shown that one can replace the true advantage function with a compatible
function approximator without affecting the gradient [8 14]  and that a natural policy gradient (with
respect to Fisher information) can be computed [2 5 11].
The goal of this paper is to apply policy gradient ideas to the linearly-solvable MDPs (or LMDPs)
we have recently-developed [15  16]  as well as to a class of continuous stochastic systems with
similar properties [4  7  16]. This framework has already produced a number of unique results –
such as linear Bellman equations  general estimation-control dualities  compositionality of optimal
control laws  path-integral methods for optimal control  etc. The present results with regard to policy
gradients are also unique  as summarized in Abstract. While the contribution is mainly theoretical
and scaling to large problems is left for future work  we provide simulations demonstrating rapid
convergence. The paper is organized in two sections  treating discrete and continuous problems.

2 Discrete problems

Since a number of papers on LMDPs have already been published  we will not repeat the general
development and motivation here  but instead only summarize the background needed for the present
paper. We will then develop the new results regarding policy gradients.

2.1 Background on LMDPs
An LMDP is deﬁned by a state cost  () over a (discrete for now) state space X   and a transition
probability density  (0|) corresponding to the notion of passive dynamics. In this paper we focus
on inﬁnite-horizon average-cost problems where  (0|) is assumed to be ergodic  i.e.
it has a
unique stationary density. The admissible "actions" are all transition probability densities  (0|)
which are ergodic and satisfy  (0|) = 0 whenever  (0|) = 0. The cost function is
(1)

 (  (·|)) =  () + KL ( (·|)|| (·|))

1

Thus the controller is free to modify the default/passive dynamics in any way it wishes  but incurs a
control cost related to the amount of modiﬁcation.
The average cost  and differential cost-to-go  () for given  (0|) satisfy the Bellman equation
(2)

+  (0)¶
 +  () =  () +P0  (0|)µlog
∗ + ∗ () =  () − logP0 (0|) exp (−∗ (0))

 (0|)
 (0|)

where  () is deﬁned up to a constant. The optimal ∗ and ∗ () can be shown to satisfy

and the optimal ∗ (0|) can be found in closed form given ∗ ():
 (0|) exp (−∗ (0))
P  (|) exp (−∗ ())

∗ (0|) =

Exponentiating equation (3) makes it linear in exp (−∗ ())  although this will not be used here.
2.2 Policy gradient for a general parameterization
Consider a parameterization  (0| w) which is valid in the sense that it satisﬁes the above con-
ditions and Ow   w exists for all w ∈ R. Let  ( w) be the corresponding stationary
density. We will also need the pair-wise density  ( 0 w) =  ( w)  (0| w). To avoid no-
tational clutter we will suppress the dependence on w in most of the paper; keep in mind that all
quantities that depend on  are functions of w.
Our objective here is to compute Ow. This is done by differentiating the Bellman equation (2) and
following the template from [14]. The result (see Supplement) is given by
Theorem 1. The LMDP policy gradient for any valid parameterization is

Ow =P ()P0Ow (0|)µlog

 (0|)
 (0|)

+  (0)¶

Let us now compare (5) to the policy gradient in traditional MDPs [14]  which is

Ow =P ()POw (|)  ( )

(6)
Here  (|) is a stochastic policy over actions (parameterized by w) and  ( ) is the correspond-
ing state-action cost-to-go. The general form of (5) and (6) is similar  however the term log ()+
in (5) cannot be interpreted as a -function. Indeed it is not clear what a -function means in the
LMDP setting. On the other hand  while in traditional MDPs one has to estimate  (or rather the
advantage function) in order to compute the policy gradient  it will turn out that in LMDPs it is
sufﬁcient to estimate .

2.3 A suitable policy parameterization

(3)

(4)

(5)

(7)

The relation (4) between the optimal policy ∗ and the optimal cost-to-go ∗ suggests parameter-
izing  as a -weighted Gibbs distribution. Since linear function approximators have proven very
successful  we will use an energy function (for the Gibbs distribution) which is linear in w :

 (0| w)  

 (0|) exp¡−wTf (0)¢
P  (|) exp (−wTf ())

Here f () ∈ R is a vector of features. One can verify that (7) is a valid parameterization. We will
also need the -expectation operator
(8)

deﬁned for both scalar and vector functions over X . The general result (5) is now specialized as
Theorem 2. The LMDP policy gradient for parameterization (7) is

(9)
As expected from (4)  we see that the energy function wTf () and the cost-to-go  () are related.
Indeed if they are equal the gradient vanishes (the converse is not true).

Ow =P0 ( 0) (Π [f ] () − f (0))¡ (0) − wTf (0)¢

Π [ ] ()  P (|)  ()

2

2.4 Compatible cost-to-go function approximation

One of the more remarkable aspects of policy gradient results [8  14] in traditional MDPs is that 
when the true  function is replaced with a compatible approximation satisfying certain conditions 
the gradient remains unchanged. Key to obtaining such results is making sure that the approximation
error is orthogonal to the remaining terms in the expression for the policy gradient. Our goal in this
section is to construct a compatible function approximator for LMDPs. The procedure is somewhat
elaborate and unusual  so we provide the derivation before stating the result in Theorem 3 below.
Given the form of (9)  it makes sense to approximate  () as a linear combination of the same

features f () used to represent the energy function: b ( r)   rTf (). Let us also deﬁne the
approximation error r ()    () −b ( r). If the policy gradient Ow is to remain unchanged
when  is replaced withb in (9)  the following quantity must be zero:

Expanding (10) and using the stationarity of   we can simplify d as

d (r)  P0 ( 0) (Π [f ] () − f (0)) r (0)
d (r) =P () (Π [f ] () Π [r] () − f () r ())

(11)
One can also incorporate an -dependent baseline in (9)  such as  () which is often used in tradi-
tional MDPs. However the baseline vanishes after the simpliﬁcation  and the result is again (11).

(10)

r   arg min

r P ()¡ () − rTf ()¢2

minimize the squared error weighted by . Denote the resulting weight vector r:

Now we encounter a complication. Suppose we were to ﬁt b to  in a least-squares sense  i.e.

(12)
This is arguably the best ﬁt one can hope for. The error r is now orthogonal to the features f  thus
for r = r the second term in (11) vanishes  but the ﬁrst term does not. Indeed we have veriﬁed
numerically (on randomly-generated LMDPs) that d (r) 6= 0.
If the best ﬁt is not good enough  what are we to do? Recall that we do not actually need a good ﬁt 
but rather a vector r such that d (r) = 0. Since d (r) and r are linearly related and have the same
dimensionality  we can directly solve this equation for r. Replacing r () with  () − rTf () and
using the fact that Π is a linear operator  we have d (r) = r − k where
  P ()³f () f ()T − Π [f ] () Π [f ] ()T´
(13)
k  P () (f ()  () − Π [f ] () Π [] ())

We are not done yet because k still depends on . The goal now is to approximate  in such a way
that k remains unchanged. To this end we use (2) and express Π [] in terms of :

 +  () −  () = Π [] ()

Here  () is shortcut notation for  (  (·| w)). Thus the vector k becomes
k =P () (g ()  () + Π [f ] () ( () − ))

where the policy-speciﬁc auxiliary features g () are related to the original features f () as

(14)

(15)

g ()   f () − Π [f ] ()

(16)

Theorem 3. The following procedure yields the exact LMDP policy gradient:

in (15) involves the projection of  on the auxiliary features g. This projection can be computed by

sense  as in (12) but using g () rather than f (). The approximation error is now orthogonal to the

The second term in (15) does not depend on ; it only depends on  =P ()  (). The ﬁrst term
deﬁning the auxiliary function approximatore ( s)   sTg () and ﬁtting it to  in a least-squares
auxiliary features g ()  and so replacing  () withe ( s) in (15) does not affect k. Thus we have
1. ﬁte ( s) to  () in a least squares sense  and also compute 
2. compute  from (13)  and k from (15) by replacing  () withe ( s)
3. "ﬁt"b ( r) by solving r = k
Ow =P0 ( 0) (f (0) − Π [f ] ()) f (0)T (w − r)

4. the policy gradient is

(17)

3

This is the ﬁrst policy gradient result with compatible function approximation over the state space
rather than the state-action space. The computations involve averaging over   which in practice will

restrictive  however an equivalent requirement arises in traditional MDPs [14].

be done through sampling (see below). The requirement that  −e be orthogonal to g is somewhat

2.5 Natural policy gradient

When the parameter space has a natural metric  (w)  optimization algorithms tend to work better
if the gradient of the objective function is pre-multiplied by  (w)−1. This yields the so-called
natural gradient [1]. In the context of policy gradient methods [5  11] where w parameterizes a
probability density  the natural metric is given by Fisher information (which depends on  because
w parameterizes the conditional density). Averaging over  yields the metric

 (w)  P0 ( 0) Ow log  (0|) Ow log  (0|)T

We then have the following result (see Supplement):
Theorem 4. With the vector r computed as in Theorem 3  the LMDP natural policy gradient is

Let us compare this result to the natural gradient in traditional MDPs [11]  which is

 (w)−1 Ow = w − r

(18)

(19)

 (w)−1 Ow = r

(20)
In traditional MDPs one maximizes reward while in LMDPs one minimizes cost  thus the sign
difference. Recall that in traditional MDPs the policy  is parameterized using features over the
state-action space while in LMDPs we only need features over the state space. Thus the vectors w r
will usually have lower dimensionality in (19) compared to (20).
Another difference is that in LMDPs the (regular as well as natural) policy gradient vanishes when
w = r  which is a sensible ﬁxed-point condition. In traditional MDPs the policy gradient vanishes
when r = 0  which is peculiar because it corresponds to the advantage function approximation
being identically 0. The true advantage function is of course different  but if the policy becomes
deterministic and only one action is sampled per state  the resulting data can be ﬁt with r = 0. Thus
any deterministic policy is a local maximum in traditional MDPs. At these local maxima the policy
gradient theorem cannot actually be applied because it requires a stochastic policy. When the policy
becomes near-deterministic  the number of samples needed to obtain accurate estimates increases
because of the lack of exploration [6]. These issues do not seem to arise in LMDPs.

2.6 A Gauss-Newton method for approximating the optimal cost-to-go

Instead of using policy gradient  we can solve (3) for the optimal ∗ directly. One option is approx-
imate policy iteration – which in our context takes on a simple form. Given the policy parameters
w() at iteration   approximate the cost-to-go function and obtain the feature weights r()  and then
set w(+1) = r(). This is equivalent to the above natural gradient method with step size 1  using a
biased approximator instead of the compatible approximator given by Theorem 3.
The other option is approximate value iteration – which is a ﬁxed-point method for solving (3)
while replacing ∗ () with wTf (). We can actually do better than value iteration here. Since
(3) has already been optimized over the controls and is differentiable  we can apply an efﬁcient
Gauss-Newton method. Up to an additive constant   the Bellman error from (3) is

4

Interestingly  the gradient of this Bellman error coincides with our auxilliary features g:

 ( w)   wTf () −  () + logP (|) exp¡−wTf ()¢

(21)

Ow ( w) = f () −P
where Π and g are the same as in (16  8). We now linearize:  ( w + w) ≈  ( w) + wTg ()
and proceed to minimize (with respect to  and w) the quantity
(23)

 (|) exp¡−wTf ()¢
P  (|) exp (−wTf ())
P ()¡ +  ( w) + wTg ()¢2

f () = f () − Π [f ] () = g ()

(22)

Figure 1: (A) Learning curves for a random LMDP. "resid" is the Gauss-Newton method. The
sampling versions use 400 samples per evaluation: 20 trajectories with 20 steps each  starting from
the stationary distribution. (B) Cost-to-go functions for the metronome LMDP. The numbers show
the average costs obtained. There are 2601 discrete states and 25 features (Gaussians). Convergence
was observed in about 10 evaluations (of the objective and the gradient) for both algorithms  exact
and sampling versions. The sampling version of the Gauss-Newton method worked well with 400
samples per evaluation; the natural gradient needed around 2500 samples.

Normally the density  () would be ﬁxed  however we have found empirically that the resulting
algorithm yields better policies if we set  () to the policy-speciﬁc stationary density  ( w)
at each iteration.
It is not clear how to guarantee convergence of this algorithm given that the
objective function itself is changing over iterations  but in practice we observed that simple damping
is sufﬁcient to make it convergent (e.g. w ← w + w2).
It is notable that minimization of (23) is closely related to policy evaluation via Bellman residual
minimization. More precisely  using (14  16) it is easy to see that TD(0) applied to our problem
would seek to minimize

P ( w)¡ −  ( w) + rTg ()¢2

(24)

(25)

The similarity becomes even more apparent if we write − ( w) more explicitly as
− ( w) = wTΠ [f ] () −  () + logP (|) exp¡−wTf ()¢

Thus the only difference from (21) is that one expression has the term wTf () at the place where
the other expression has the term wTΠ [f ] (). Note that the Gauss-Newton method proposed
here would be expected to have second-order convergence  even though the amount of computa-
tion/sampling per iteration is the same as in a policy gradient method.

2.7 Numerical experiments

We compared the natural policy gradient and the Gauss-Newton method  both in exact form and
with sampling  on two classes of LMDPs: randomly generated  and a discretization of a continuous

"metronome" problem taken from [17]. Fitting the auxiliary approximatore ( s) was done using

the LSTD() algorithm [3]. Note that Theorem 3 guarantees compatibility only for  = 1  however
lower values of  reduce variance and still provide good descent directions in practice (as one would
expect). We ended up using  = 02 after some experimentation. The natural gradient was used
with the BFGS minimizer "minFunc" [12].
Figure 1A shows typical learning curves on a random LMDP with 100 states  20 random features 
and random passive dynamics with 50% sparsity. In this case the algorithms had very similar per-
formance. On other examples we observed one or the other algorithm being slightly faster or pro-
ducing better minima  but overall they were comparable. The average cost of the policies found by
the Gauss-Newton method occasionally increased towards the end of the iteration.
Figure 1B compares the optimal cost-to-go ∗  the least-squares ﬁt to the known ∗ using our fea-
tures (which were a 5-by-5 grid of Gaussians)  and the solution of the policy gradient method ini-
tialized with w = 0. Note that the latter has lower cost compared to the least-squares ﬁt. In this case
both algorithms converged in about 10 iterations  although the Gauss-Newton method needed about
5 times fewer samples in order to achieve similar performance to the exact version.

5

3 Continuous problems

Unlike the discrete case where we focused exclusively on LMDPs  here we begin with a very general
problem formulation and present interesting new results. These results are then specialized to a
narrower class of problems which are continuous (in space and time) but nevertheless have similar
properties to LMDPs.

3.1 Policy gradient for general controlled diffusions

Consider the controlled Ito diffusion

x = b (x u)  +  (x) 

(26)
where  () is a standard multidimensional Brownian motion process  and u is now a traditional
control vector. Let  (x u) be a cost function. As before we focus on inﬁnite-horizon average-cost
optimal control problems. Given a policy u =  (x)  the average cost  and differential cost-to-go
 (x) satisfy the Hamilton-Jacobi-Bellman (HJB) equation

 =  (x  (x)) + L [] (x)
where L is the following 2nd-order linear differential operator:

L [] (x)   b (x  (x))T Ox (x) + 1

(28)
In can be shown [10] that L coincides with the inﬁnitesimal generator of (26)  i.e. it computes the
expected directional derivative of  along trajectories generated by (26). We will need
Lemma 1. Let L be the inﬁnitesimal generator of an Ito diffusion which has a stationary density  
and let  be a twice-differentiable function. Then

2 trace³ (x)  (x)T Oxx (x)´

(27)

(29)

Z  (x)L [ ] (x) x = 0

Proof: The adjoint L∗ of the inﬁnitesimal generator L is known to be the Fokker-Planck operator –
which computes the time-evolution of a density under the diffusion [10]. Since  is the stationary
density  L∗ [] (x) = 0 for all x  and so hL∗ []  i = 0. Since L and L∗ are adjoint  hL∗ []  i =
hL [ ]i. Thus hL [ ]i = 0.
This lemma seems important-yet-obvious so we would not be surprised if it was already known  but
we have not seen in the literature. Note that many diffusions lack stationary densities. For example
the density of Brownian motion initialized at the origin is a zero-mean Gaussian whose covariance
grows linearly with time – thus there is no stationary density. If however the diffusion is controlled
and the policy tends to keep the state within some region  then a stationary density would normally
exist. The existence of a stationary density may actually be a sensible deﬁnition of stability for
stochastic systems (although this point will not be pursued in the present paper).
Now consider any policy parameterization u =  (x w) such that (for the current value of w) the
diffusion (26) has a stationary density  and Ow exists. Differentiating (27)  and using the shortcut
notation b (x) in place of b (x  (x w)) and similarly for  (x)  we have

Ow = Ow (x) + Owb (x) Ox (x) + L [Ow] (x)

(30)
Here L [Ow] is meant component-wise. If we now average over   the last term will vanish due to
Lemma 1. This is essential for a policy gradient procedure which seeks to avoid ﬁnite differencing;
indeed Ow could not be estimated while sampling from a single policy. Thus we have
Theorem 5. The policy gradient of the controlled diffusion (26) is

Ow =Z  (x)³Ow (x) + Owb (x) Ox (x)´ x

(31)

Unlike most other results in stochastic optimal control  equation (31) does not involve the Hessian
Oxx  although we can obtain a Oxx-dependent term here if we allow  to depend on u. We now
illustrate Theorem 5 on a linear-quadratic-Gaussian (LQG) control problem.

6

Example (LQG). Consider dynamics  =  +  and cost  ( ) = 2 + 2. Let  = −
be the parameterized policy with   0. The differential cost-to-go is known to be in the form
 () = 2. Substituting in the HJB equation and matching powers of  yields  =  = 2+1
2  
and so the policy gradient can be computed directly as O = 1 − 2+1
22 . The stationary density
 () is a zero-mean Gaussian with variance 2 = 1
2 . One can now verify that the gradient given
by Theorem 5 is identical to the O computed above.
Another interesting aspect of Theorem 5 is that it is a natural generalization of classic results from
ﬁnite-horizon deterministic optimal control [13]  even though it cannot be derived from those results.
Suppose we have an open-loop control trajectory u ()  0 ≤  ≤    the resulting state trajectory
(starting from a given x0) is x ()  and the corresponding co-state trajectory (obtained by integrating
Pontryagin’s ODE backwards in time) is  (). It is known that the gradient of the total cost  w.r.t.
u is Ou + OubT. Now suppose u () is parameterized by some vector w. Then

Ow =Z Owu ()T Ou() =Z ³Ow (x ()  u ()) + Owb (x ()  u ())T  ()´ 

The co-state  () is known to be equal to the gradient Ox (x ) of the cost-to-go function for the
(closed-loop) deterministic problem. Thus (31) and (32) are very similar. Of course in ﬁnite-horizon
settings there is no stationary density  and instead the integral in (32) is over the trajectory. An RL
method for estimating Ow in deterministic problems was developed in [9].
Theorem 5 suggests a simple procedure for estimating the policy gradient via sampling: ﬁt a function

(32)

Owb (x). This however is not practical because learning targets for Ox are difﬁcult to obtain.

approximatorb to   and use Oxb in (31). Alternatively  a compatible approximation scheme can be
obtained by ﬁtting Oxb to Ox in a least-squares sense  using a linear approximator with features
Ideally we would construct a compatible approximation scheme which involves ﬁttingb rather than
Oxb. It is not clear how to do that for general diffusions  but can be done for a restricted problem

3.2 Natural gradient and compatible approximation for linearly-solvable diffusions

class as shown next.

We now focus on a more restricted family of stochastic optimal control problems which arise in
many situations (e.g. most mechanical systems can be described in this form):

x = (a (x) +  (x) u)  +  (x) 

(33)

 (x u) =  () + 1

2 uT (x) u

 (x w)   − (x)−1  (x)T Ox¡wTf (x)¢

Such problems have been studied extensively [13]. The optimal control law u∗ and the optimal
differential cost-go-to ∗ (x) are known to be related as u∗ = −−1TOx∗. As in the discrete
case we use this relation to motivate the choice of policy parameterization and cost-to-go function
approximator. Choosing some features f (x)  we deﬁneb (x r)   rTf (x) as before  and
It is convenient to also deﬁne the matrix  (x)   Oxf (x)T  so that Oxb (x r) =  (x) r. We can
now substitute these deﬁnitions in the general result (31)  replace  with the approximationb  and
Before addressing the issue of compatibility (i.e. whethereOw = Ow)  we seek a natural gradient

eOw =Z  (x)  (x)T  (x)  (x)−1  (x)T  (x) (w − r) x

version of (35). To this end we need to interpret  T−1T as Fisher information for the (in-
ﬁnitesimal) transition probability density of our parameterized diffusion. We do this by discretizing
the time axis with time step   and then dividing by . The -step explicit Euler discretization of the
stochastic dynamics (33) is given by the Gaussian

skipping the algebra  obtain the corresponding approximation to the policy gradient:

(35)

(34)

 (·|x w) = N³x + a (x) −  (x)  (x)−1  (x)T  (x) w;  (x)  (x)T´

Suppressing the dependence on x  Fisher information becomes

(36)

(37)

1

Z Ow log Ow log T

x0 =  T−1T¡T¢−1

−1T

7

Comparing to (35) we see that a natural gradient result is obtained when

Assuming (38) is satisﬁed  and deﬁning  (w) as the average of Fisher information over  (x) 

 (x)  (x)T =  (x)  (x)−1  (x)T

(38)

 (w)−1eOw = w − r

Π [ ] (x) =  (x) + L [ ] (x) + ¡2¢

(39)
Condition (38) is rather interesting. Elsewhere we have shown [16] that the same condition is needed
to make problem (33) linearly-solvable. More precisely  the exponentiated HJB equation for the
optimal ∗ in problem (33  38) is linear in exp (−∗). We have also shown [16] that the continuous
problem (33  38) is the limit (when  → 0) of continuous-state discrete-time LMDPs constructed
via Euler discretization as above. The compatible function approximation scheme from Theorem
3 can then be applied to these LMDPs. Recall (8). Since L is the inﬁnitesimal generator  for any
twice-differentiable function  we have
(40)
Substituting in (13)  dividing by  and taking the limit  → 0  the matrix  and vector k become
(41)

 =Z  (x)³−L [f ] (x) f (x)T − f (x)L [f ] (x)T´ x
k =Z  (x) (−L [f ] (x)  (x) + f (x) ( (x) − )) x
Compatibility is therefore achieved when the approximation error in  is orthogonal to L [f ]. Thus
the auxiliary function approximator is nowe (x s)   sTL [f ] (x)  and we have
Theorem 6. The following procedure yields the exact policy gradient for problem (33  38):
1. ﬁte (x s) to  (x) in a least-squares sense  and also compute 
2. compute  and k from (41)  replacing  (x) withe (x s)
3. "ﬁt"b (x r) by solving r = k
4. the policy gradient is (35)  and the natural policy gradient is (39)
This is the ﬁrst policy gradient result with compatible function approximation for continuous sto-
chastic systems. It is very similar to the corresponding results in the discrete case (Theorems 3 4)
except it involves the differential operator L rather than the integral operator Π.
4 Summary

Here we developed compatible function approximators and natural policy gradients which only re-
quire estimation of the cost-to-go function. This was possible due to the unique properties of the
LMDP framework. The resulting approximation scheme is unusual  using policy-speciﬁc auxiliary
features derived from the primary features. In continuous time we also obtained a new policy gradi-
ent result for control problems that are not linearly-solvable  and showed that it generalizes results
from deterministic optimal control. We also derived a somewhat heuristic but nevertheless promis-
ing Gauss-Newton method for solving for the optimal cost-to-go directly; it appears to be a hybrid
between value iteration and policy gradient.
One might wonder why we need policy gradients here given that the (exponentiated) Bellman equa-
tion is linear  and approximating its solution using features is faster than any other procedure in
Reinforcement Learning and Approximate Dynamic Programming. The answer is that minimizing
Bellman error does not always give the best policy – as illustrated in Figure 1B. Indeed a combined
approach may be optimal: solve the linear Bellman equation approximately [17]  and then use the
solution to initialize the policy gradient method. This idea will be explored in future work.
Our new methods require a model – as do all RL methods that rely on state values rather than state-
action values. We do not see this as a shortcoming because  despite all the effort that has gone
into model-free RL  the resulting methods do not seem applicable to truly complex optimal control
problems. Our methods involve model-based sampling which combines the best of both worlds:
computational speed  and grounding in reality (assuming we have a good model of reality).
Acknowledgements.
This work was supported by the US National Science Foundation. Thanks to Guillaume Lajoie and
Jan Peters for helpful discussions.

8

References
[1] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation  10:251–276 

1998.

[2] J. Bagnell and J. Schneider. Covariant policy search. In International Joint Conference on

Artiﬁcial Intelligence  2003.

[3] J. Boyan. Least-squares temporal difference learning. In International Conference on Machine

Learning  1999.

[4] W. Fleming and S. Mitter. Optimal control and nonlinear ﬁltering for nondegenerate diffusion

processes. Stochastics  8:226–261  1982.

[5] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems 

2002.

[6] S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis  University

College London  2003.

[7] H. Kappen. Linear theory for control of nonlinear stochastic systems. Physical Review Letters 

95  2005.

[8] V. Konda and J. Tsitsiklis. Actor-critic algorithms. SIAM Journal on Control and Optimization 

pages 1008–1014  2001.

[9] R. Munos. Policy gradient in continuous time. The Journal of Machine Learning Research 

7:771–791  2006.

[10] B. Oksendal. Stochastic Differential Equations (4th Ed). Springer-Verlag  Berlin  1995.
[11] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing  71:1180–1190  2008.
[12] M. Schmidt. minfunc. online material  2005.
[13] R. Stengel. Optimal Control and Estimation. Dover  New York  1994.
[14] R. Sutton  D. Mcallester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems 
2000.

[15] E. Todorov. Linearly-solvable Markov decision problems. Advances in Neural Information

Processing Systems  2006.

[16] E. Todorov. Efﬁcient computation of optimal actions. PNAS  106:11478–11483  2009.
[17] E. Todorov. Eigen-function approximation methods for linearly-solvable optimal control prob-

lems. IEEE ADPRL  2009.

[18] R. Williams. Simple statistical gradient following algorithms for connectionist reinforcement

learning. Machine Learning  pages 229–256  1992.

9

,David Adametz
Volker Roth
Eugene Ndiaye
Olivier Fercoq
Alexandre Gramfort
Joseph Salmon