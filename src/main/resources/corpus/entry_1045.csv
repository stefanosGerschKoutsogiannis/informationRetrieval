2009,Complexity of Decentralized Control: Special Cases,The worst-case complexity of general decentralized POMDPs  which are equivalent to partially observable stochastic games (POSGs) is very high  both for the cooperative and competitive cases.  Some reductions in complexity have been achieved by exploiting independence relations in some models.  We show that these results are somewhat limited:  when these independence assumptions are relaxed in very small ways  complexity returns to that of the general case.,Complexity of Decentralized Control: Special Cases

Martin Allen

martin.allen@conncoll.edu

Department of Computer Science

Connecticut College

New London  CT 06320

Shlomo Zilberstein

Department of Computer Science

University of Massachusetts

Amherst  MA 01003

shlomo@cs.umass.edu

Abstract

The worst-case complexity of general decentralized POMDPs  which are equiv-
alent to partially observable stochastic games (POSGs) is very high  both for the
cooperative and competitive cases. Some reductions in complexity have been
achieved by exploiting independence relations in some models. We show that
these results are somewhat limited: when these independence assumptions are
relaxed in very small ways  complexity returns to that of the general case.

1

Introduction

Decentralized and partially observable stochastic decision and planning problems are very common 
comprising anything from strategic games of chance to robotic space exploration. In such domains 
multiple agents act under uncertainty about both their environment and the plans and actions of
others. These problems can be represented as decentralized partially observable Markov decision
processes (Dec-POMDPs)  or the equivalent  partially observable stochastic games (POSGs)  al-
lowing for precise formulation of solution concepts and success criteria.
Alas  such problems are highly complex. As shown by Bernstein et al. [1  2]  the full  cooperative
problem—where all players share the same payoff  and strategies can depend upon entire observed
histories—is NEXP-complete. More recently  Goldsmith and Mundhenk [3] showed that the com-
petitive case can be worse: when teamwork is allowed among agents  complexity rises to NEXPNP
(problems solvable by a NEXP machine employing an NP set as an oracle). Much attention has
thus been paid to restricted cases  particularly those where some parts of the system dynamics be-
have independently. The complexity of ﬁnite-horizon Dec-POMDPs goes down—from NEXP to
NP—when agents interact only via a joint reward structure  and are otherwise independent. Un-
fortunately  our new results show that further reduction  based on other combinations of fully or
partially independent system dynamics are unlikely  if not impossible.
We show that if the situation were reversed  so that rewards alone are independent  the problem re-
mains NEXP-complete. Further  we consider two other Dec-POMDP sub-classes from the literature:
(a) domains where local agent sub-problems are independent except for a (relatively small) number
of event-based interactions  and (b) those where agents only interact inﬂuencing the set of currently
available actions. As it turns out  both types of problem are NEXP-complete as well—facts previ-
ously unknown. (In the latter case  this is a substantial increase in the known upper bound.) These
results provide further impetus to devise new tools for the analysis and classiﬁcation of problem
difﬁculty in decentralized problem solving.

2 Basic deﬁnitions

The cooperative  decentralized partially observable Markov decision process (Dec-POMDP) is a
highly general and powerful framework  capable of representing a wide range of real-world problem

1

domains. It extends the basic POMDP to multiple agents  operating in conjunction based on locally
observed information about the world  and collecting a single source of reward.
Deﬁnition 1 (Dec-POMDP). A (Dec-POMDP)  D  is speciﬁed by a tuple:

M = (cid:104){αi}  S  {Ai}  P  {Ωi}  O  R  T(cid:105)

(1)

with individual components as follows:

• Each αi is an agent; S is a ﬁnite set of world states with a distinguished initial state s0; Ai
is a ﬁnite set of actions  ai  available to αi; Ωi is a ﬁnite set of observations  oi  for αi; and
T is the (ﬁnite or inﬁnite) time-horizon of the problem.
• P is the Markovian state-action transition function. P (s  a1  . . .   an  s(cid:48)) is the probability
of going from state s to state s(cid:48)  given joint action (cid:104)a1  . . .   an(cid:105).
• O is the joint observation function for the set of agents  given each state-action transition.
O(a1  . . .   an  s(cid:48)  o1  . . .   on) is the probability of observing (cid:104)o1  . . .   on(cid:105)  if joint action
(cid:104)a1  . . .   an(cid:105) causes a transition to global state s(cid:48).
• R is the global reward function. R(s  a1  . . .   an) is the reward obtained for performing
joint action (cid:104)a1  . . .   an(cid:105) when in global state s.

The most important sub-instance of the Dec-POMDP model is the decentralized MDP (Dec-MDP) 
where the joint observation tells us everything we need to know about the system state.
Deﬁnition 2 (Dec-MDP). A decentralized Markov decision process (Dec-MDP) is a Dec-POMDP
that is jointly fully observable. That is  there exists a functional mapping  J : Ω1 × ··· × Ωn → S 
such that O(a1  . . .   an  s(cid:48)  o1  . . .   on) (cid:54)= 0 if and only if J(o1  . . .   on) = s(cid:48).
In a Dec-MDP  then  the sum total of the individual agent observations provides a complete pic-
ture of the state of the environment. It is important to note  however  that this does not mean that
any individual agent actually possesses this information. Dec-MDPs are still fully decentralized in
general  and individual agents cannot count on access to the global state when choosing actions.
Deﬁnition 3 (Policies). A local policy for an agent αi is a mapping from sequences of that agent’s
observations  oi = (cid:104)o1
i → Ai. A joint policy for n agents is a
collection of local policies  one per agent  π = (cid:104)π1  . . .   πn(cid:105).
A solution method for a decentralized problem seeks to ﬁnd some joint policy that maximizes ex-
pected value given the starting state (or distribution over states) of the problem. For complexity
purposes  the decision version of the Dec-(PO)MDP problem is to determine whether there exists
some joint policy with value greater at least k.

i (cid:105)  to its actions  πi : Ω(cid:63)

i   . . .   ok

3 Bernstein’s proof of NEXP-completeness

Before establishing our new claims  we brieﬂy review the NEXP-completeness result for ﬁnite-
horizon Dec-MDPs  as given by Bernstein et al. [1  2]. First  we note that the upper bound  namely
that ﬁnite-horizon Dec-POMDPs are in NEXP  will immediately establish the same upper bound for
all the problems that we will consider. (While we do not discuss the proof here  full details can be
found in the original  or the supplemental materials to this paper  §1.)
Theorem 1 (Upper Bound). The ﬁnite-horizon  n-agent decision problem Dec-POMDP ∈ NEXP.
More challenging (and interesting) is establishing lower bounds on these problems  which is per-
formed via our reduction from the known NEXP-complete TILING problem [4  5]. A TILING
problem instance consists of a board size n  given concisely in log n binary bits  a set of tile-
types L = {t0  . . .   tk}  and a collection of binary and vertical compatibility relations between
tiles H  V ⊆ L × L. A tiling is a mapping of board locations to tile-types  t : {0  . . .   n − 1} ×
{0  . . .   n − 1} → L; such a tiling is consistent just in case (i) the origin location of the board
receives tile-type 0 (t(0  0) = tile0); and (ii) all adjoint tile assignments are compatible:

(∀x  y) (cid:104)t(x  y)  t(x + 1  y)(cid:105) ∈ H & (cid:104)t(x  y)  t(x  y + 1)(cid:105) ∈ V.

The TILING problem is thus to decide  for a given instance  whether such a consistent tiling exists.
Figure 1 shows an example instance and consistent solution.

2

Figure 1: An example of the TILING problem  and a consistent solution.

The reduction transforms a given instance of TILING into a 2-agent Dec-MDP  where each agent is
queried about some location in the grid  and must answer with a tile to be placed there. By careful
design of the query and response mechanism  it is ensured that a policy with non-negative value
exists only if the agents already have a consistent tiling  thus showing the Dec-MDP to be as hard
as TILING. Together with Theorem 1  and the fact that the ﬁnite-horizon  2-agent Dec-MDP is a
special case of the general ﬁnite-horizon Dec-POMDP  the reduction establishes Bernstein’s main
complexity result (again  details are in the supplemental materials  §1):
Theorem 2 (NEXP-Completeness). The ﬁnite-horizon Dec-POMDP problem is NEXP-complete.

4 Factored Dec-POMDPs and independence

In general  the state transitions  observations  and rewards in a Dec-POMDP can involve probabilis-
tic dependencies between agents. An obvious restricted subcase is thus one in which these factors
are somehow independent. Becker et al. [6  7] have thus studied problems in which the global state-
space consists of the product of local states  so that each agent has its own individual state-space. A
Dec-POMDP can then be transition independent  observation independent  or reward independent 
as each the local effects given by each corresponding function are independent of one another.
Deﬁnition 4 (Factored Dec-POMDP). A factored  n-agent Dec-POMDP is a Dec-POMDP such
that the system state can be factored into n + 1 distinct components  so that S = S0 × S1 ×···× Sn 
and no state-variable appears in any Si  Sj  i (cid:54)= j.
As with the local (agent-speciﬁc) actions  ai  and observations  oi  in the general Dec-POMDP
deﬁnition  we now refer to the local state  ˆs ∈ Si × S0  namely that portion of the overall state-
space that is either speciﬁc to agent αi (si ∈ Si)  or shared among all agents (so ∈ S0). We use the
notation s−i for the sequence of all state-components except that for agent αi:

s−i = (s0  s1  . . .   si−1  si+1  . . .   sn)

(and similarly for action- or observation-sequences  a−i and o−i).
Deﬁnition 5 (Transition Independence). A factored  n-agent DEC-POMDP is transition inde-
pendent iff the state-transition function can be separated into n + 1 distinct transition functions
P0  . . .   Pn  where  for any next state s(cid:48)

i ∈ Si 

P (s(cid:48)

i | (s0  . . .   sn)  (a1  . . .   an)  s−i) =

(cid:26)P0(s(cid:48)

Pi(s(cid:48)

0 | s0)
i | ˆsi  ai  s(cid:48)
0)

if i = 0;
else.

In other words  the next local state of each agent is independent of the local states of all others  given
its previous local state and local action  and the external system features (S0).
Deﬁnition 6 (Observation Independence). A factored  n-agent Dec-POMDP is observation inde-
pendent iff the joint observation function can be separated into n separate probability functions
O1  . . .   On  where  for any local observation oi ∈ Ωi 
0  . . .   s(cid:48)

n)  o−i) = Oi(oi | ai  ˆs(cid:48)
i)

O(oi | (a1  . . .   an)  (s(cid:48)

In such cases  the probability of an agent’s individual observations is a function of their own local
states and actions alone  independent of the states of others  and of what those others do or observe.

3

n = 5 L = H = V = 0 1 2 0 1 0 2 0 1 1 1 1 2 2 0 0 1 0 2 1 0 1 2 2 0 0 1 0 1 2 0 0 1 0 2 0 1 2 0 2 0 0 2 0 2 1 2 0 1 0 A consistent solution Deﬁnition 7 (Reward Independence). A factored  n-agent Dec-POMDP is reward independent iff
the joint reward function can be represented by local reward functions R1  . . .   Rn  such that:

R((s0  . . . sn)  (a0  . . .   an)) = f(R1(ˆs1  a1)  . . .   Rn(ˆsn  an))

and

Ri(ˆsi  ai) ≥ Ri(ˆsi  a(cid:48)

i) ⇔ f(R1  . . .  Ri(ˆsi  ai)  . . .   Rn) ≥ f(R1  . . .   Ri(ˆsi  a(cid:48)

i)  . . .   Rn)

That is  joint reward is a function of local reward  constrained so that we maximize global reward if
and only if we maximize local rewards. A typical example is the additive sum:

R((s0  . . . sn)  (a0  . . .   an)) = R1(ˆs1  a1) + ··· + Rn(ˆsn  an).

It is important to note that each deﬁnition applies equally to Dec-MDPs; in such cases  joint full
observability of the overall state is often accompanied by full observability at the local level.
Deﬁnition 8 (Local Full Observability). A factored  n-agent Dec-MDP is locally fully observable
iff an agent’s local observation uniquely determines its local state: ∀oi ∈ Ωi  ∃ˆsi : P (ˆsi | oi) = 1.
Local full observability is not equivalent to independence of observations. In particular  a problem
may be locally fully observable without being observation independent (since agents may simply
observe outcomes of non-independent joint actions). On the other hand  it is easy to show that an
observation-independent Dec-MDP must be locally fully observable (supplementary  §2).

4.1 Shared rewards alone lead to reduced complexity

It is easy to see that if a Dec-MDP (or Dec-POMDP) has all three forms of independence given
by Deﬁnitions 5–7  it can be decomposed into n separate problems  where each agent αi works
solely within the local sub-environment Si × S0. Such single-agent problems are known to be P-
complete  and can generally be solved efﬁciently to high degrees of optimality. More interesting
results follow when only some forms of independence hold. In particular  it has been shown that
Dec-MDPs with both transition- and observation-independence  but not reward-independence  are
NP-complete [8  7]. (This result is discussed in detail in our supplementary material  §3.)
Theorem 3. A transition- and observation-independent Dec-MDP with joint reward is NP-complete.

5 Other subclasses of interactions

As our new results will now show  there is a limit to this sort of complexity reduction: other relatively
obvious combinations of independence relationships do not bear the same fruit. That is  we show
the NP-completeness result to be speciﬁc to fully transition- and observation-independent problems.
When these properties are not fully present  worst-case complexity is once again NEXP.

5.1 Reward-independent-only models are NEXP-complete

We begin with a result that is rather simple  but has not  to the best of our knowledge  been estab-
lished before. We consider the inverse of the NP-complete problem of Theorem 3: a Dec-MDP with
reward-independence (Df. 7)  but without transition- or observation-independence (Dfs. 5  6).
Theorem 4. Factored  reward-independent Dec-MDPs with n agents are NEXP-complete.

Proof Sketch. For the upper bound  we simply cite Theorem 1  immediately establishing that such
problems are in NEXP. For the lower bound  we simply modify the TILING Dec-MDP from Bern-
stein’s reduction proof so as to ensure that the reward-function factors appropriately into strictly
local rewards. (Full details are found in [9]  and the supplementary materials  §4.1.)

Thus we see that in some respects  transition and observation independence are fundamental to
the reduction of worst-case complexity from NEXP to NP. When only the rewards depend upon
the actions of both agents  the problems become easier; however  when the situation is reversed 

4

the general problem remains NEXP-hard. This is not entirely surprising: much of the complexity
of planning in decentralized domains stems from the necessity to take account of how one’s action-
outcomes are affected by the actions of others  and from the complications that ensue when observed
information about the system is tied to those actions as well. The structure of rewards  while ob-
viously key to the nature of the optimal (or otherwise) solution  is not as vital—even if agents can
separate their individual reward-functions  making them entirely independent  other dependencies
can still make the problem extremely complex.
We therefore turn to two other interesting special-case Dec-MDP frameworks  in which independent
reward functions are accompanied by restricted degrees of transition- and observation-based interac-
tion. While some empirical evidence has suggested that these problems may be easier on average to
solve  nothing has previously been shown about their worst-case complexity. We ﬁll in these gaps 
showing that even under such restricted dynamics  the problems remain NEXP-hard.

5.2 Event-driven-interaction models are NEXP-complete

The ﬁrst model we consider is one of Becker et al. [10]  which generalizes the notion of a fully
transition-independent Dec-MDP. In this model  a set of primitive events  consisting of state-action
transitions  is deﬁned for each agent. Such events can be thought of as occasions upon which
that agent takes the given action to generate the associated state transition. Dependencies are then
introduced in the form of relationships between one agent’s possible actions in given states and
another agent’s primitive events.
While no precise worst-case complexity results have been previously proven  the authors do point out
that the class of problems has an upper-bound deterministic complexity that is exponential in the size
of the state space  |S|  and doubly exponential in the number of deﬁned interactions. This potentially
bad news is mitigated by noting that if the number of interactions is small  then reasonably-sized
problems can still be solved. Here  we examine this issue in detail  showing that  in fact these
problems are NEXP-hard (indeed  NEXP-complete); however  when the number of dependencies is
a log-factor of the size of the problem state-space  worst-case NP-hardness is achieved.
We begin with the formal framework of the model. Again  we give all deﬁnitions in terms of Dec-
POMDPs; they apply immediately to Dec-MDPs in particular.
Deﬁnition 9 (History). A history for an agent αi in a factored  n-agent Dec-POMDP D is a sequence
of possible local states and actions  beginning in the agent’s initial state: Φi = [ˆs0
i   . . .].
When a problem has a ﬁnite time-horizon T   all possible complete histories will be of the form
ΦT
i = [ˆs0
Deﬁnition 10 (Events in a History). A primitive event e = (ˆsi  ai  ˆs(cid:48)
i) for an agent αi is a triple
representing a transition between two local states  given some action ai ∈ Ai. An event E =
{e1  e2  . . .   eh} is a set of primitive events. A primitive event e occurs in the history Φi  written
Φi (cid:15) e  if and only if the triple e is a sub-sequence of the sequence Φi. An event E occurs in the
history Φi  written Φi (cid:15) E  if and only if some component occurs in that history: ∃e ∈ E : Φi (cid:15) e.
Events can therefore be thought of disjunctively. That is  they specify a set of possible state-action
transitions from a Dec-POMDP  local to one of its agents. If the historical sequence of state-action
transitions that the agent encounters contains any one of those particular transitions  then the history
satisﬁes the overall event. Events can thus be used  for example  to represent such things as taking a
particular action in any one of a number of states over time  or taking one of several actions at some
particular state. For technical reasons  namely the use of a specialized solution algorithm  these
events are usually restricted in structure  as follows.
Deﬁnition 11 (Proper Events). A primitive event e is proper if it occurs at most once in any given
(cid:15)
history. That is  for any history Φi if Φi = Φ1
e) ∧ ¬(Φ2
(cid:15) e). An event E is proper if it consists of proper primitive events that are mutually
exclusive  in that no two of them both occur in any history:

i then neither sub-history contains e: ¬(Φ1

i   . . .   ˆsT−1

i   ˆsT
i ].

i   a0

i   ˆs1

i   a1

i e Φ2

i   a0

i   ˆs1

i   a1

  aT

i

i

∀Φi ¬∃x  y : (x (cid:54)= y) ∧ (ex ∈ E) ∧ (ey ∈ E) ∧ (Φi (cid:15) ex) ∧ (Φi (cid:15) ey).

i

Proper primitive events can be used  for instance  to represent actions that take place at particular
times (building the time into the local state ˆsi ∈ e). Since any given point in time can only occur
once in any history  the events involving such time-steps will be proper by default. A proper event

5

E can then be formed by collecting all the primitive events involving some single time-step  or by
taking all possible primitive events involving an unrepeatable action.
Our new model is then a Dec-MDP with:

1. Two (2) agents.1
2. A factored state-space: S = S0 × S1 × Sn.
3. Local full observability: each agent αi can determine its own portion of the state-space 
4. Independent (additive) rewards: R((cid:104)s0  s1  s2(cid:105)  a1  a2) = R1(ˆs1  a1) + R2(ˆs2  a2).

ˆsi ∈ S0 × Si  exactly.

Interactions between agents are given in terms of a set of dependencies between certain state-action
transitions for one agent  and events featuring transitions involving the other agent. Thus  if a history
contains one of the primitive events from the latter set  this can have some direct effect upon the
transition-model for the ﬁrst agent  introducing probabilistic transition-dependencies.
Deﬁnition 12 (Dependency). A dependency is a pair dk
deﬁned over primitive events for agent αi  and Dk
such that each pair occurs in at most one dependency:
¬(∃ k  k(cid:48)  sj  aj) (k (cid:54)= k(cid:48)) & (cid:104)sj  aj(cid:105) ∈ Dk

i is a proper event
j is a set of state-action pairs (cid:104)ˆsj  aj(cid:105) for agent αj 

ij & (cid:104)sj  aj(cid:105) ∈ Dk(cid:48)

j (cid:105)  where Ek

ij = (cid:104)Ek

j ∈ dk(cid:48)
ij .

j ∈ dk

i   Dk

Such a dependency is thus a collection of possible actions that agent αj can take in one of its local
state  each of which depends upon whether the other agent αi has made one of the state-transitions
in its own set of primitive events. Such structures can be used to model  for instance  cases where
one agent cannot successfully complete some task until the other agent has completed an enabling
sub-task  or where the precise outcome depends upon the groundwork laid by the other agent.
ij = (cid:104)Ek
j (cid:105) is satisﬁed when the
Deﬁnition 13 (Satisfying Dependencies). A dependency dk
current history for enabling agent αi contains the relevant event: Φi (cid:15) Ek
i . For any state-action pair
(cid:104)ˆsj  aj(cid:105)  we deﬁne a Boolean indicator variable bˆsj aj   which is true if and only if some dependency
that contains the pair is satisﬁed:

i   Dk

(cid:26)1

0

bˆsj aj =

ij = (cid:104)Ek

if (∃ dk
otherwise.

j (cid:105)) (cid:104)ˆsj  aj(cid:105) ∈ Dk

j & Φi (cid:15) Ek
i  

i   Dk

i | ˆsi  ai  bˆsiai). We can thus write Pi(ˆsi  ai  bˆsiai  ˆs(cid:48)

The existence of dependencies allows us to factor the overall state-transition function into two parts 
each of which depends only on an agent’s local state  action  and relevant indicator variable.
Deﬁnition 14 (Local Transition Function). The transition function for our Dec-MDP is factored
into two functions  P1 and P2  each deﬁning the distribution over next possible local states:
Pi(ˆs(cid:48)
When agents take some action in a state for which dependencies exist  they observe whether or not
the related events have occurred; that is  after taking any action aj in state sj  they can observe the
state of indicator variable bˆsj aj .
With these deﬁnitions in place  we can now show that the worst-case complexity of the event-based
problems is the same as the general Dec-POMDP class.
Theorem 5. Factored  ﬁnite-horizon  n-agent Dec-MDPs with local full observability  independent
rewards  and event-driven interactions are NEXP-complete.

i) for this transition probability.

Proof Sketch. Again  the upper bound is immediate from Theorem 1  since the event-based structure
is just a speciﬁc case of general reward-dependence  and such models can always be converted into
Dec-MDPs without any events. For the lower bound  we again provide a reduction from TILING 
constrained to our special case. Local reward independence  which was not present in the original
problem  is ensured by using event dependencies to affect future rewards of the other agent. Thus 
local immediate rewards remain dependent only upon the actions of the individual agent  but the
state in which that agent ﬁnds itself (and so the options available to its reward function) can depend
upon events involving the other agent. (See [9] and supplemental materials  §4.2.)

1The model can be extended to n agents with little real difﬁculty. Since we will show that the 2-agent case

is NEXP-hard  however  this will sufﬁce for the general claim.

6

5.2.1 A special  NP-hard case

The prior result requires allowing the number of dependencies in the problem to grow as a factor of
log n  for a TILING grid of size (n×n). Since the size of the state-space S in the reduced Dec-MDP
is also O(log n)  the number of dependencies is O(|S|). Thus  the NEXP-completeness result holds
for any event-based Dec-MDP where the number of dependencies is linear in the state-space. When
we are able to restrict the number of dependencies further  however  we can do better.
Theorem 6. A factored  ﬁnite-horizon  n-agent Dec-MDP with local full observability  independent
rewards  and event-driven interactions are solvable in nondeterministic polynomial time (NP) if the
number of dependencies is O(log |S|)  where S is the state-set of the problem.

Proof Sketch. As shown by Becker [10]  we can use the Coverage Set algorithm to generate an
optimal policy for a problem of this type  in time that is exponential in the number of dependencies.
Clearly  if this number is logarithmic in the size of the state-set  then solution time is polynomial in
the problem size. (See [9] and supplemental materials  §4.2.1.)

5.2.2 Discussion of the results

These results are interesting for two reasons. First  NEXP-completeness of the event-based case 
even with independent rewards and local full observability (Theorem 5)  means that many interest-
ing problems are potentially intractable. Becker et al. [10] show how to use event-dependencies
to represent common structures in the TAEMS task modeling language  used in many real-world
domains [11  12  13]; our complexity analysis thus extends to such practical problems. Second 
isolating where complexity is lower can help determine what task structures and agent interrelation-
ships lead to intractability. In domains where the dependency structure can be kept relatively simple 
it may be possible to derive optimal solutions feasibly. Both subjects are worth further study.

5.3 State-dependent-action models are NEXP-complete

Guo and Lesser [14  15  16] consider another specialized Dec-MDP subclass  with apparently even
more restricted types of interaction. Agent state-spaces are again separate  and all action-transitions
and rewards are independent. Such problems are not wholly decoupled  however  as the actions
available to each agent at any point depend upon the global system state. Thus  agents interact by
making choices that restrict or broaden the range of actions available to others.
Deﬁnition 15 (Dec-MDP with State-Dependent Actions). An n-agent Dec-MDP with state-
dependent actions is a tuple D = (cid:104)S0 {Si} {Ai} {Bi} {Pi} {Ri}  T(cid:105)  where:

• S0 is a set of shared states  and Si is the state-space of agent si  with global state space
S = S0 × S1 × ··· × Sn  and initial state s0 ∈ S; each Ai is the action-set for αi; T ∈ N
is the ﬁnite time-horizon of the problem.
• Each Bi : S → 2Ai is a mapping from global states of the system to some set of available
actions for each agent αi. For all s ∈ S  Bi(s) (cid:54)= ∅.
• Pi : (S0 × Si) × Ai(S0 × Si) is the state-transition function over local states for αi. The
• Ri : (S0 × Si) → (cid:60) is a local reward function for agent αi. We let the global reward

global transition function is simply the product of individual Pi.

function be the sum of local rewards.

Note that there need be no observations in such a problem; given local full observability  each agent
observes only its local states. Furthermore  it is presumed that each agent can observe its own
available actions in any state; a local policy is thus a mapping from local states to available actions.
For such cases  Guo presents a planning algorithm based on heuristic action-set pruning  along
with a learning algorithm. While empirical results show that these methods are capable of solving
potentially large instances  we again know very little about the analytical worst-case difﬁculty of
problems with state-dependent actions. An NP-hardness lower bound is given [14] for the overall
class  by reducing a normal-form game to the state-dependent model  but this is potentially quite
weak  since no upper bound has been established  and even the operative algorithmic complexity
of the given solution method is not well understood. We address this situation  showing that the
problem is also just as hard as the general case.

7

Theorem 7. Factored  ﬁnite-horizon  n-agent Dec-MDPs with local full observability  independent
rewards  and state-dependent action-sets are NEXP-complete.

Proof Sketch. Once more  we rely upon the general upper bound on the complexity of Dec-POMDPs
(Theorem 1). The lower bound is by another TILING reduction. Again  we “record” actions of each
agent in the state-space of the other  ensuring purely local rewards and local full observability. This
time  however  we use the fact that action-sets depend upon the global state (rather than events) to
enforce the desired dynamics. That is  we add special state-dependent actions that  based on their
availability (or lack thereof)  affect each agent’s local reward. (See [9]  and supplemental §4.3.)

5.3.1 Discussion of the result

Guo and Lesser [16  14] were able to show that deciding whether a decentralized problem with
state-based actions had an equilibrium solution with value greater than k was NP-hard. It was not
ascertained whether or not this lower bound was tight  however; this remained a signiﬁcant open
question. Our results show that this bound was indeed too low. Since an optimal joint policy will be
an equilibrium for the special case of additive rewards  the general problem can be no easier.
This is interesting  for reasons beyond the formal. Such decentralized problems indeed appear to be
quite simple in structure  requiring wholly independent rewards and action-transitions  so that agents
can only interact with one another via choices that affect which actions are available. (A typical
example involves two persons acting completely regardless of one another  except for the existence
of a single rowboat  used for crossing a stream; if either agent uses the rowboat to get to the other
side  then that action is no longer available to the other.) Such problems are intuitive  and common 
and not all of them are hard to solve  obviously. At the same time  however  our results show that
the same structures can be intractable in the worst case  establishing that even seemingly simple
interactions between agents can lead to prohibitively high complexity in decentralized problems.

6 Conclusions

This work addresses a number of existing models for decentralized problem-solving. In each case 
the models restrict agent interaction in some way  in order to produce a special sub-case of the
general Dec-POMDP problem. It has been known for some time that systems where agents act
entirely independently  but share rewards  have reduced worst-case complexity. We have shown that
this does not apply to other variants  where we relax the independence requirements even only a
little. In all of the cases addressed  the new problem variants are as hard as the general case. This
fact  combined with results showing many other decentralized problem models to be equivalent to
the general Dec-POMDP model  or strictly harder [17]  reveals the essential difﬁculty of optimal
planning in decentralized settings. Together  these results begin to suggest that optimal solutions to
many common multiagent problems must remain out of reach; in turn  this indicates that we must
look to approximate or heuristic methods  since such problems are so prevalent in practice.
At the same time  it must be stressed that the NEXP-complexity demonstrated here is a worst-case
measure. Not all decentralized domains are going to be intractable  and indeed the event-based
and action-set models have been shown to yield to specialized solution methods in many cases 
enabling us to solve interesting instances in reasonable amounts of time. When the number of action-
dependencies is small  or there are few ways that agents can affect available action-sets  it may well
be possible to provide optimal solutions effectively. That is  the high worst-case complexity is no
guarantee that average-case difﬁculty is likewise high. This remains a vital open problem in the ﬁeld.
While establishing the average case is often difﬁcult  if not impossible—given that the notion of an
“average” planning or decision problem is often ill-deﬁned—it is still worth serious consideration.

Acknowledgments

This material is based upon work supported by the the Air Force Ofﬁce of Scientiﬁc Research
under Award No. FA9550-05-1-0254. Any opinions  ﬁndings  and conclusions or recommendations
expressed in this publication are those of the authors and do not necessarily reﬂect the views of
AFOSR. The ﬁrst author also acknowledges the support of the Andrew W. Mellon Foundation CTW
Computer Science Consortium Fellowship.

8

References
[1] Daniel S. Bernstein  Shlomo Zilberstein  and Neil Immerman. The complexity of decentral-
In Proceedings of the Sixteenth Conference on

ized control of Markov decision processes.
Uncertainty in Artiﬁcial Intelligence  pages 32–37  Stanford  California  2000.

[2] Daniel S. Bernstein  Robert Givan  Neil Immerman  and Shlomo Zilberstein. The complexity
of decentralized control of Markov decision processes. Mathematics of Operations Research 
27(4):819–840  2002.
Judy Goldsmith and Martin Mundhenk. Competition adds complexity. In J.C. Platt  D. Koller 
Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 561–568. MIT Press  Cambridge  MA  2008.

[3]

[4] Harry R. Lewis. Complexity of solvable cases of the decision problem for predicate calculus.
In Proceedings of the Nineteenth Symposium on the Foundations of Computer Science  pages
35–47  Ann Arbor  Michigan  1978.

[5] Christos H. Papadimitriou. Computational Complexity. Addison-Wesley  Reading  Mas-

sachusetts  1994.

[6] Raphen Becker  Shlomo Zilberstein  Victor Lesser  and Claudia V. Goldman. Transition-
independent decentralized Markov decision processes. In Proceedings of the Second Interna-
tional Joint Conference on Autonomous Agents and Multi-Agent Systems  pages 41–48  Mel-
bourne  Australia  2003.

[7] Raphen Becker  Shlomo Zilberstein  Victor Lesser  and Claudia V. Goldman. Solving transition
independent decentralized MDPs. Journal of Artiﬁcial Intelligence Research  22:423–455 
November 2004.

[8] Claudia V. Goldman and Shlomo Zilberstein. Decentralized control of cooperative systems:
Categorization and complexity analysis. Journal of Artiﬁcial Intelligence Research  22:143–
174  2004.

[9] Martin Allen. Agent Interactions in Decentralized Environments. PhD thesis  University of
Massachusetts  Amherst  Massachusetts  2009. Available at http://scholarworks.
umass.edu/open_access_dissertations/1/.

[10] Raphen Becker  Victor Lesser  and Shlomo Zilberstein. Decentralized Markov decision pro-
cesses with event-driven interactions. In Proceedings of the Third International Joint Confer-
ence on Autonomous Agents and Multi-Agent Systems  pages 302–309  New York  New York 
2004.

[11] Keith S. Decker and Victor R. Lesser. Quantitative modeling of complex environments. Inter-
national Journal of Intelligent Systems in Accounting  Finance and Management  2:215–234 
1993.

[12] V. Lesser  K. Decker  T.Wagner  N. Carver  A. Garvey  B. Horling  D. Neiman  R. Podor-
ozhny  M. Nagendra Prasad  A. Raja  R. Vincent  P. Xuan  and X.Q Zhang. Evolution of the
GPGP/TAEMS domain-independent coordination framework. Autonomous Agents and Multi-
Agent Systems  9(1):87–143  2004.

[13] Tom Wagner  Valerie Guralnik  and John Phelps. TAEMS agents: Enabling dynamic dis-
tributed supply chain management. Journal of Electronic Commerce Research and Applica-
tions  2:114–132  2003.

[14] AnYuan Guo. Planning and Learning for Weakly-Coupled Distributed Agents. PhD thesis 

University of Massachusetts  Amherst  2006.

[15] AnYuan Guo and Victor Lesser. Planning for weakly-coupled partially observable stochastic
games. In Proceedings of the 19th International Joint Conference on Artiﬁcial Intelligence 
pages 1715–1716  Edinburgh  Scotland  2005.

[16] AnYuan Guo and Lesser Victor. Stochastic planning for weakly-coupled distributed agents.
In Proceedings of the Fifth Joint Conference on Autonomous Agents and Multiagent Systems 
pages 326–328  Hakodate  Japan  2006.

[17] Sven Seuken and Shlomo Zilberstein. Formal models and algorithms for decentralized deci-
sion making under uncertainty. Autonomous Agents and Multi-Agent Systems  17(2):190–250 
2008.

9

,Stefan Mathe
Cristian Sminchisescu
Fei Xia
Martin Zhang
James Zou
David Tse