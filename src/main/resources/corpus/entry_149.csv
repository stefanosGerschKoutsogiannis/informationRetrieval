2016,A Non-generative Framework and Convex Relaxations for Unsupervised Learning,We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First  it does not assume any generative model and based on a worst-case performance metric. Second  it is comparative  namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations.  We show how several families of unsupervised learning models  which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable  can be efficiently learned in our framework by convex optimization.,A Non-generative Framework and Convex
Relaxations for Unsupervised Learning

Elad Hazan

Princeton University
35 Olden Street 08540

ehazan@cs.princeton.edu.

Tengyu Ma

Princeton University

35 Olden Street  NJ 08540

tengyu@cs.princeton.edu.

Abstract

We give a novel formal theoretical framework for unsupervised learning with two
distinctive characteristics. First  it does not assume any generative model and
based on a worst-case performance metric. Second  it is comparative  namely
performance is measured with respect to a given hypothesis class. This allows
to avoid known computational hardness results and improper algorithms based
on convex relaxations. We show how several families of unsupervised learning
models  which were previously only analyzed under probabilistic assumptions and
are otherwise provably intractable  can be efﬁciently learned in our framework by
convex optimization.

1

Introduction

Unsupervised learning is the task of learning structure from unlabelled examples. Informally  the
main goal of unsupervised learning is to extract structure from the data in a way that will enable
efﬁcient learning from future labelled examples for potentially numerous independent tasks.
It is useful to recall the Probably Approximately Correct (PAC) learning theory for supervised learn-
ing [28]  based on Vapnik’s statistical learning theory [29]. In PAC learning  the learning can access
labelled examples from an unknown distribution. On the basis of these examples  the learner con-
structs a hypothesis that generalizes to unseen data. A concept is said to be learnable with respect to
a hypothesis class if there exists an (efﬁcient) algorithm that outputs a generalizing hypothesis with
high probability after observing polynomially many examples in terms of the input representation.
The great achievements of PAC learning that made it successful are its generality and algorithmic
applicability: PAC learning does not restrict the input domain in any way  and thus allows very
general learning  without generative or distributional assumptions on the world. Another important
feature is the restriction to speciﬁc hypothesis classes  without which there are simple impossibility
results such as the “no free lunch” theorem. This allows comparative and improper learning of
computationally-hard concepts.
The latter is a very important point which is often understated. Consider the example of sparse
regression  which is a canonical problem in high dimensional statistics. Fitting the best sparse vector
to linear prediction is an NP-hard problem [20]. However  this does not prohibit improper learning 
since we can use a `1 convex relaxation for the sparse vectors (famously known as LASSO [26]).
Unsupervised learning  on the other hand  while extremely applicative and well-studied  has not seen
such an inclusive theory. The most common approaches  such as restricted Boltzmann machines 
topic models  dictionary learning  principal component analysis and metric clustering  are based
almost entirely on generative assumptions about the world. This is a strong restriction which makes
it very hard to analyze such approaches in scenarios for which the assumptions do not hold. A
more discriminative approach is based on compression  such as the Minimum Description Length

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

criterion. This approach gives rise to provably intractable problems and doesn’t allow improper
learning.

Main results. We start by proposing a rigorous framework for unsupervised learning which al-
lows data-dependent  comparative learning without generative assumptions about the world. It is
general enough to encompass previous methods such as PCA  dictionary learning and topic models.
Our main contribution are optimization-based relaxations and efﬁcient algorithms that are shown to
improperly probably learn previous models  speciﬁcally:

1. We consider the classes of hypothesis known as dictionary learning. We give a more general
hypothesis class which encompasses and generalizes it according to our deﬁnitions. We
proceed to give novel polynomial-time algorithms for learning the broader class. These
algorithms are based on new techniques in sum-of-squares convex relaxations.
As far as we know  this is the ﬁrst result for efﬁcient improper learning of dictionaries with-
out generative assumptions. Moreover  our result handles polynomially over-complete dic-
tionaries  while previous works [4  8] apply to at most constant factor over-completeness.
2. We give efﬁcient algorithms for learning a new hypothesis class which we call spectral
autoencoders. We show that this class generalizes  according to our deﬁnitions  the class of
PCA (principal component analysis) and its kernel extensions.

Structure of this paper.
In the following chapter we a non-generative  distribution-dependent def-
inition for unsupervised learning which mirrors that of PAC learning for supervised learning. We
then proceed to an illustrative example and show how Principal Component Analysis can be for-
mally learned in this setting. The same section also gives a much more general class of hypothesis
for unsupervised learning which we call polynomial spectral decoding  and show how they can be
efﬁcient learned in our framework using convex optimization. Finally  we get to our main contri-
bution: a convex optimization based methodology for improper learning a wide class of hypothesis 
including dictionary learning.

1.1 Previous work

The vast majority of work on unsupervised learning  both theoretical as well as applicative  focuses
on generative models. These include topic models [11]  dictionary learning [13]  Deep Boltzmann
Machines and deep belief networks [24] and many more. Many times these models entail non-
convex optimization problems that are provably NP-hard to solve in the worst-case.
A recent line of work in theoretical machine learning attempts to give efﬁcient algorithms for these
models with provable guarantees. Such algorithms were given for topic models [5]  dictionary
learning [6  4]  mixtures of gaussians and hidden Markov models [15  3] and more. However  these
works retain  and at times even enhance  the probabilistic generative assumptions of the underlying
model. Perhaps the most widely used unsupervised learning methods are clustering algorithms such
as k-means  k-medians and principal component analysis (PCA)  though these lack generalization
guarantees. An axiomatic approach to clustering was initiated by Kleinberg [17] and pursued further
in [9]. A discriminative generalization-based approach for clustering was undertaken in [7] within
the model of similarity-based clustering.
Another approach from the information theory literature studies with online lossless compression.
The relationship between compression and machine learning goes back to the Minimum Description
Length criterion [23]. More recent work in information theory gives online algorithms that attain
optimal compression  mostly for ﬁnite alphabets [1  21]. For inﬁnite alphabets  which are the main
object of study for unsupervised learning of signals such as images  there are known impossibility
results [16]. This connection to compression was recently further advanced  mostly in the context
of textual data [22].
In terms of lossy compression  Rate Distortion Theory (RDT) [10  12] is intimately related to our
deﬁnitions  as a framework for ﬁnding lossy compression with minimal distortion (which would
correspond to reconstruction error in our terminology). Our learnability deﬁnition can be seen of
an extension of RDT to allow improper learning and generalization error bounds. Another learn-
ing framework derived from lossy compression is the information bottleneck criterion [27]  and its

2

learning theoretic extensions [25]. The latter framework assumes an additional feedback signal  and
thus is not purely unsupervised.
The downside of the information-theoretic approaches is that worst-case competitive compression
is provably computationally hard under cryptographic assumptions. In contrast  our compression-
based approach is based on learning a restriction to a speciﬁc hypothesis class  much like PAC-
learning. This circumvents the impossibility results and allows for improper learning.

2 A formal framework for unsupervised learning

The basis constructs in an unsupervised learning setting are:

1. Instance domain X   such as images  text documents  etc. Target space  or range  Y. We
usually think of X = Rd Y = Rk with d  k. (Alternatively  Y can be all sparse vectors
in a larger space. )

2. An unknown  arbitrary distribution D on domain X .
3. A hypothesis class of decoding and encoding pairs 

H✓{ (h  g) 2 {X 7! Y} ⇥ {Y 7! X}} 

where h is the encoding hypothesis and g is the decoding hypothesis.

4. A loss function ` : H ⇥ X 7! R>0 that measures the reconstruction error 

`((g  h)  x) .

2. The ratio-
For example  a natural choice is the `2-loss `((g  h)  x) = kg(h(x))  xk2
nale here is to learn structure without signiﬁcantly compromising supervised learning for
arbitrary future tasks. Near-perfect reconstruction is sufﬁcient as formally proved in Ap-
pendix 6.1. Without generative assumptions  it can be seen that near-perfect reconstruction
is also necessary.

For convenience of notation  we use f as a shorthand for (h  g) 2H   a member of the hypothesis
class H. Denote the generalization ability of an unsupervised learning algorithm with respect to a
distribution D as

loss
D

(f ) = E
x⇠D

[`(f  x)].

We can now deﬁne the main object of study: unsupervised learning with respect to a given hypothe-
sis class. The deﬁnition is parameterized by real numbers: the ﬁrst is the encoding length (measured
in bits) of the hypothesis class. The second is the bias  or additional error compared to the best
hypothesis. Both parameters are necessary to allow improper learning.
Deﬁnition 2.1. We say that instance D X is (k  )-C -learnable with respect to hypothesis class H if
exists an algorithm that for every   " > 0  after seeing m("  ) = poly(1/"  log(1/)  d) examples 
returns an encoding and decoding pair (h  g) (not necessarily from H) such that:

1. with probability at least 1    lossD((h  g)) 6 min(h g)2H lossD((h  g)) + " + .
2. h(x) has an explicit representation with length at most k bits.

For convenience we typically encode into real numbers instead of bits. Real encoding can often
(though not in the worst case) be trivially transformed to be binary with a loss of logarithmic factor.
Following PAC learning theory  we can use uniform convergence to bound the generalization error
of the empirical risk minimizer (ERM). Deﬁne the empirical loss for a given sample S ⇠D m as

loss
S

(f ) =

1

m ·Xx2S

`(f  x)

Deﬁne the ERM hypothesis for a given sample S ⇠D m as ˆfERM = arg min ˆf2H

lossS( ˆf ) .

3

For a hypothesis class H  a loss function ` and a set of m samples S ⇠D m  deﬁne the empirical
Rademacher complexity of H with respect to ` and S as  1
⇠{±1}m"sup
mXx2S
1

i`(f  x)#

RS `(H) =

f2H

E

Let the Rademacher complexity of H with respect to distribution D and loss ` as Rm(H) =
ES⇠Dm[RS `(H)]. When it’s clear from the context  we will omit the subscript `.
We can now state and apply standard generalization error results. The proof of following theorem is
almost identical to [19  Theorem 3.1]. For completeness we provide a proof in Appendix 6.
Theorem 2.1. For any > 0  with probability 1   the generalization error of the ERM hypothesis
is bounded by:

loss
D

( ˆfERM ) 6 min
f2H

loss
D

(f ) + 6Rm(H) +s 4 log 1

2m



An immediate corollary of the theorem is that as long as the Rademacher complexity of a hypothesis
class approaches zero as the number of examples goes to inﬁnity  it can be C learned by an inefﬁcient
algorithm that optimizes over the hypothesis class by enumeration and outputs an best hypothesis
with encoding length k and bias  = 0. Not surprisingly such optimization is often intractable and
hences the main challenge is to design efﬁcient algorithms. As we will see in later sections  we often
need to trade the encoding length and bias slightly for computational efﬁciency.
Notations: For every vector z 2 Rd1 ⌦Rd2  we can view it as a matrix of dimension d1⇥d2  which
is denoted as M(z). Therefore in this notation  M(u ⌦ v) = uv>. Let vmax(·) : (Rd)⌦2 ! Rd be
the function that compute the top right-singular vector of some vector in (Rd)⌦2 viewed as a matrix.
That is  for z 2 (Rd)⌦2  then vmax(z) denotes the top right-singular vector of M(z). We also
overload the notation vmax for generalized eigenvectors of higher order tensors. For T 2 (Rd)⌦` 
let vmax(T ) = argmaxkxk61 T (x  x  . . .   x) where T (·) denotes the multi-linear form deﬁned by
tensor T .
3 Spectral autoencoders: unsupervised learning of algebraic manifolds
3.1 Algebraic manifolds

The goal of the spectral autoencoder hypothesis class we deﬁne henceforth is to learn the represen-
tation of data that lies on a low-dimensional algebraic variety/manifolds. The linear variety  or linear
manifold  deﬁned by the roots of linear equations  is simply a linear subspace. If the data resides in
a linear subspace  or close enough to it  then PCA is effective at learning its succinct representation.
One extension of the linear manifolds is the set of roots of low-degree polynomial equations. For-
mally  let k  s be integers and let c1  . . .   cdsk 2 Rds be a set of vectors in ds dimension  and
consider the algebraic variety

M =x 2 Rd : 8i 2 [ds  k] hci  x⌦si = 0 .

Observe that here each constraint hci  x⌦si is a degree-s polynomial over variables x  and when
s = 1 the variety M becomes a liner subspace. Let a1  . . .   ak 2 Rds be a basis of the subspaces
orthogonal to all of c1  . . .   cdsk  and let A 2 Rk⇥ds contains ai as rows. Then we have that given
x 2M   the encoding

y = Ax⌦s

pins down all the unknown information regarding x. In fact  for any x 2M   we have A>Ax⌦s =
x⌦s and therefore x is decodable from y. The argument can also be extended to the situation when
the data point is close to M (according to a metric  as we discuss later). The goal of the rest of the
subsections is to learn the encoding matrix A given data points residing close to M.

1Technically  this is the Rademacher complexity of the class of functions ` H . However  since ` is usually

ﬁxed for certain problem  we emphasize in the deﬁnition more the dependency on H.

4

Warm up: PCA and kernel PCA.
In this section we illustrate our framework for agnostic unsu-
pervised learning by showing how PCA and kernel PCA can be efﬁciently learned within our model.
The results of this sub-section are not new  and given only for illustrative purposes. The class of hy-
pothesis corresponding to PCA operates on domain X = Rd and range Y = Rk for some k < d via
linear operators. In kernel PCA  the encoding linear operator applies to the s-th tensor power x⌦s
of the data. That is  the encoding and decoding are parameterized by a linear operator A 2 Rk⇥ds 

k s =(hA  gA) : hA(x) = Ax⌦s    gA(y) = A†y  
Hpca

where A† denotes the pseudo-inverse of A. The natural loss function here is the Euclidean norm 
`((g  h)  x) = kx⌦s  g(h(x))k2 = k(I  A†A)x⌦sk2 .
Theorem 3.1. For a ﬁxed constant s > 1  the class Hpca
length k and bias  = 0.
The proof of the Theorem follows from two simple components: a) ﬁnding the ERM among Hpca
k s
can be efﬁciently solved by taking SVD of covariance matrix of the (lifted) data points. b) The
Rademacher complexity of the hypothesis class is bounded by O(ds/m) for m examples. Thus by
Theorem 2.1 the minimizer of ERM generalizes. The full proof is deferred to Appendix A.

k s is efﬁciently C -learnable with encoding

3.2 Spectral Autoencoders
In this section we give a much broader set of hypothesis  encompassing PCA and kernel-PCA  and
show how to learn them efﬁciently. Throughout this section we assume that the data is normalized to
Euclidean norm 1  and consider the following class of hypothesis which naturally generalizes PCA:
Deﬁnition 3.1 (Spectral autoencoder). We deﬁne the class Hsa
k s as the following set of all hypothesis
(g  h) 

k =⇢(h  g) :
Hsa

g(y) = vmax(By)  B 2 Rds⇥k  .
h(x) = Ax⌦s  A 2 Rk⇥ds

(3.1)

We note that this notion is more general than kernel PCA: suppose some (g  h) 2H pca
k s has re-
construction error "  namely  A†Ax⌦s is "-close to x⌦s in Euclidean norm. Then by eigenvector
perturbation theorem  we have that vmax(A†Ax⌦s) also reconstructs x with O(") error  and there-
fore there exists a PSCA hypothesis with O(") error as well . Vice versa  it’s quite possible that for
every A  the reconstruction A†Ax⌦s is far away from x⌦s so that kernel PCA doesn’t apply  but
with spectral decoding we can still reconstruct x from vmax(A†Ax⌦s) since the top eigenvector of
A†Ax⌦s is close x.
Here the key matter that distinguishes us from kernel PCA is in what metric x needs to be close to
the manifold so that it can be reconstructed. Using PCA  the requirement is that x is in Euclidean
distance close to M (which is a subspace)  and using kernel PCA x⌦2 needs to be in Euclidean
distance close to the null space of ci’s. However  Euclidean distances in the original space and lifted
space typically are meaningless for high-dimensional data since any two data points are far away
with each other in Euclidean distance. The advantage of using spectral autoencoders is that in the
lifted space the geometry is measured by spectral norm distance that is much smaller than Euclidean
distance (with a potential gap of d1/2). The key here is that though the dimension of lifted space is
d2  the objects of our interests is the set of rank-1 tensors of the form x⌦2. Therefore  spectral norm
distance is a much more effective measure of closeness since it exploits the underlying structure of
the lifted data points.
We note that spectral autoencoders relate to vanishing component analysis [18]. When the data is
close to an algebraic manifold  spectral autoencoders aim to ﬁnd the (small number of) essential
non-vanishing components in a noise robust manner.

3.3 Learnability of polynomial spectral decoding
For simplicity we focus on the case when s = 2. Ideally we would like to learn the best encoding-
decoding scheme for any data distribution D. Though there are technical difﬁculties to achieve such
a general result. A natural attempt would be to optimize the loss function f (A  B) = kg(h(x)) 
xk2 = kx  vmax(BAx⌦2)k2. Not surprisingly  function f is not a convex function with respect to
A  B  and in fact it could be even non-continuous (if not ill-deﬁned)!

5

Here we make a further realizability assumption that the data distribution D admits a reasonable
encoding and decoding pair with reasonable reconstruction error.
Deﬁnition 3.2. We say a data distribution D is (k  ")-regularly spectral decodable if there exist
A 2 Rk⇥d2 and B 2 Rd2⇥k with kBAkop 6 ⌧ such that for x ⇠D   with probability 1  the
encoding y = Ax⌦2 satisﬁes that
(3.2)

M(By) = M(BAx⌦2) = xx> + E  
where kEkop 6 ". Here ⌧ > 1 is treated as a ﬁxed constant globally.
To interpret the deﬁnition  we observe that if data distribution D is (k  ")-regularly spectrally decod-
able  then by equation (3.2) and Wedin’s theorem (see e.g. [30] ) on the robustness of eigenvector to
perturbation  M(By) has top eigenvector2 that is O(")-close to x itself. Therefore  deﬁnition 3.2 is a
sufﬁcient condition for the spectral decoding algorithm vmax(By) to return x approximately  though
it might be not necessary. Moreover  this condition partially addresses the non-continuity issue of
using objective f (A  B) = kxvmax(BAx⌦2)k2  while f (A  B) remains (highly) non-convex. We
resolve this issue by using a convex surrogate.
Our main result concerning the learnability of the aforementioned hypothesis class is:
Theorem 3.2. The hypothesis class Hsa
 with respect to (k  ")-regular distributions in polynomial time.
Our approach towards ﬁnding an encoding and decoding matrice A  B is to optimize the objective 
(3.3)

k 2 is C - learnable with encoding length O(⌧ 4k4/4) and bias

minimize f (R) = EhRx⌦2  x⌦2opi

s.t. kRkS1 6 ⌧k

where k·k S1 denotes the Schatten 1-norm. Suppose D is (k  ")-regularly decodable  and suppose
hA and gB are the corresponding encoding and decoding function. Then we see that R = AB will
satisﬁes that R has rank at most k and f (R) 6 ". On the other hand  suppose one obtains some R
of rank k0 such that f (R) 6   then we can produce hA and gB with O() reconstruction simply by
choosing A 2 Rk0⇥d2B and B 2 Rd2⇥k0 such that R = AB.
We use (non-smooth) Frank-Wolfe to solve objective (3.3)  which in particular returns a low-rank
solution. We defer the proof of Theorem 3.2 to the Appendix A.1. With a slightly stronger assump-
tions on the data distribution D  we can reduce the length of the code to O(k2/"2) from O(k4/"4).
See details in Appendix B.
4 A family of optimization encodings and efﬁcient dictionary learning

In this section we give efﬁcient algorithms for learning a family of unsupervised learning algorithms
commonly known as ”dictionary learning”. In contrast to previous approaches  we do not construct
an actual ”dictionary”  but rather improperly learn a comparable encoding via convex relaxations.
We consider a different family of codes which is motivated by matrix-based unsupervised learning
models such as topic-models  dictionary learning and PCA. This family is described by a matrix
A 2 Rd⇥r which has low complexity according to a certain norm k·k ↵  that is  kAk↵ 6 c↵. We can
parametrize a family of hypothesis H according to these matrices  and deﬁne an encoding-decoding
pair according to

hA(x) = arg min
kyk6k

1
d |x  Ay|1   gA(y) = Ay

We choose `1 norm to measure the error mostly for convenience  though it can be quite ﬂexible.
The different norms k·k ↵ k·k  over A and y give rise to different learning models that have been
considered before. For example  if these are Euclidean norms  then we get PCA. If k·k ↵ is the max
column `2 or `1 norm and k·k b is the `0 norm  then this corresponds to dictionary learning (more
details in the next section).
The optimal hypothesis in terms of reconstruction error is given by

A? = arg min
kAk↵6c↵

x⇠D 1

d |x  gA(hA(x))|1 = arg min

kAk↵6c↵

E

x⇠D min

y2Rr:kyk6k

E

1

d |x  Ay|1 .

2Or right singular vector when M(By) is not symmetric

6

The loss function can be generalized to other norms  e.g.  squared `2 loss  without any essential
change in the analysis. Notice that this optimization objective derived from reconstruction error
is identically the one used in the literature of dictionary learning. This can be seen as another
justiﬁcation for the deﬁnition of unsupervised learning as minimizing reconstruction error subject to
compression constraints.
The optimization problem above is notoriously hard computationally  and signiﬁcant algorithmic
and heuristic literature attempted to give efﬁcient algorithms under various distributional assump-
tions(see [6  4  2] and the references therein). Our approach below circumvents this computational
hardness by convex relaxations that result in learning a different creature  albeit with comparable
compression and reconstruction objective.

Improper dictionary learning: overview

4.1
We assume the max column `1 norm of A is at most 1 and the `1 norm of y is assumed to be at
most k. This is a more general setting than the random dictionaries (up to a re-scaling) that previous
works [6  4] studied. 3In this case  the magnitude of each entry of x is on the order of pk if y has
k random ±1 entries. We think of our target error per entry as much smaller than 14. We consider
dict that are parametrized by the dictionary matrix A = Rd⇥r 
Hk

k =(hA  gA) : A 2 Rd⇥r kAk`1!`1 6 1  
Hdict
where hA(x) = arg min
kyk16k |x  Ay|1   gA(y) = Ay

Here we allow r to be larger than d  the case that is often called over-complete dictionary. The
choice of the loss can be replaced by `2 loss (or other Lipschitz loss) without any additional efforts 
though for simplicity we stick to `1 loss. Deﬁne A? to be the the best dictionary under the model
and "? to be the optimal error 

A? = arg minkAk`1!`161 Ex⇠D⇥miny2Rr:kyk16k |x  Ay|1⇤

"? = Ex⇠D⇥ 1

d · |x  gA?(hA?(x))|1⇤ .

Algorithm 1 group encoding/decoding for improper dictionary learning
Inputs: N data points X 2 Rd⇥N ⇠D N. Convex set Q. Sampling probability ⇢.

1. Group encoding: Compute

(4.1)

(4.2)

Z = arg min

C2Q |X  C|1  

and let Y = h(X) = P⌦(Z)   where P⌦(B) is a random sampling of B where each entry
is picked with probability ⇢.

2. Group decoding: Compute g(Y ) = arg minC2Q |P⌦(C)  Y |1 .

k

is C -learnable with encoding length

Theorem 4.1. For any > 0  p > 1  the hypothesis class Hdict
˜O(k2r1/p/2)  bias  + O("?) and sample complexity dO(p) in time nO(p2)
We note that here r can be potentially much larger than d since by choosing a large constant p the
overhead caused by r can be negligible. Since the average size of the entries is pk  therefore we
can get the bias  smaller than average size of the entries with code length roughly ⇡ k.
The proof of Theorem 4.1 is deferred to supplementary. To demonstrate the key intuition and tech-
nique behind it  in the rest of the section we consider a simpler algorithm that achieves a weaker
goal: Algorithm 1 encodes multiple examples into some codes with the matching average encoding
length ˜O(k2r1/p/2)  and these examples can be decoded from the codes together with reconstruc-
tion error "? + . Next  we outline the analysis of Algorithm 1  and we will show later that one can
reduce the problem of encoding a single examples to the problem of encoding multiple examples.
3The assumption can be relaxed to that A has `1 norm at most k and `2-norm at most pd straightforwardly.
4We are conservative in the scaling of the error here. Error much smaller than pk is already meaningful.

7

Here we overload the notation gA?(hA?(·)) so that gA?(hA?(X)) denotes the collection of all the
gA?(hA?(xj)) where xj is the j-th column of X. Algorithm 1 assumes that there exists a convex set
Q ⇢ Rd⇥N such that

gA?(hA?(X)) : X 2 Rd⇥N ⇢{ AY : kAk`1!`1 6 1 kY k`1!`1 6 k}⇢ Q .

(4.3)

That is  Q is a convex relaxation of the group of reconstructions allowed in the class Hdict. Algo-
rithm 1 ﬁrst uses convex programming to denoise the data X into a clean version Z  which belongs
to the set Q. If the set Q has low complexity  then simple random sampling of Z 2 Q serves as a
good encoding.
The following Lemma shows that if Q has low complexity in terms of sampling Rademacher width 
then Algorithm 1 will give a good group encoding and decoding scheme.
Lemma 4.2. Suppose convex Q ⇢ Rd⇥N satisﬁes condition (4.3). Then  Algorithm 1 gives a group
encoding and decoding pair such that with probability 1    the average reconstruction error is
bounded by "? + O(pSRW m(Q) + O(plog(1/)/m) where m = ⇢N d and SRW m(·) is the
sampling Rademacher width (deﬁned in appendix)  and the average encoding length is ˜O(⇢d).
Towards analyzing the algorithm  we will show that the difference between Z and X is comparable
to "?  which is a direct consequence of the optimization over a large set Q that contains optimal
reconstruction. Then we prove that the sampling procedure doesn’t lose too much information given
a denoised version of the data is already observed  and thus one can reconstruct Z from Y .
The novelty here is to use these two steps together to denoise and achieve a short encoding. The
typical bottleneck of applying convex relaxation on matrix factorization based problem (or any other
problem) is the difﬁculty of rounding. Here instead of pursuing a rounding algorithm that output the
factor A and Y   we look for a convex relaxation that preserves the intrinsic complexity of the set
which enables the trivial sampling encoding. It turns out that controlling the width/complexity of
the convex relaxation boils down to proving concentration inequalities with sum-of-squares (SoS)
proofs  which is conceptually easier than rounding.
Therefore  the remaining challenge is to design convex set Q that simultaneously has the following
properties (a) is a convex relaxation in the sense of satisfying condition (4.3). (b) admits an efﬁcient
optimization algorithm. (c) has low complexity (that is  sampling rademacher width ˜O(N poly(k))).
Concretely  we have the following theorem. We note that these three properties (with Lemma 4.2)
imply that Algorithm 1 with Q = Qsos
p and ⇢ = O(k2r2/pd1/2 · log d) gives a group encoding-
decoding pair with average encoding length O(k2r2/p/2 · log d) and bias .
Theorem 4.3. For every p > 4  let N = dc0p with a sufﬁciently large absolute constant c0. Then 
there exists a convex set Qsos
p ⇢ Rd⇥N such that (a) it satisﬁes condition 4.3; (b) The optimiza-
tion (4.2) and (2) are solvable by semideﬁnite programming with run-time nO(p2); (c) the sampling
p is bounded bypSRW m(Q) 6 ˜O(k2r2/pN/m).
Rademacher width of Qsos
5 Conclusions

We have deﬁned a new framework for unsupervised learning which replaces generative assumptions
by notions of reconstruction error and encoding length. This framework is comparative  and allows
learning of particular hypothesis classes with respect to an unknown distribution by other hypothesis
classes. We demonstrate its usefulness by giving new polynomial time algorithms for two unsuper-
vised hypothesis classes. First  we give new polynomial time algorithms for dictionary models in
signiﬁcantly broader range of parameters and assumptions. Another domain is the class of spectral
encodings  for which we consider a new class of models that is shown to strictly encompass PCA
and kernel-PCA. This new class is capable  in contrast to previous spectral models  learn algebraic
manifolds. We give efﬁcient learning algorithms for this class based on convex relaxations.

Acknowledgements

We thank Sanjeev Arora for many illuminating discussions and crucial observations in earlier phases
of this work  amongst them that a representation which preserves information for all classiﬁers
requires lossless compression.

8

References
[1]

Jayadev Acharya  Hirakendu Das  Ashkan Jafarpour  Alon Orlitsky  and Ananda Theertha Suresh. Tight bounds for universal compres-
sion of large alphabets. In Proceedings of the 2013 IEEE International Symposium on Information Theory  Istanbul  Turkey  July 7-12 
2013  pages 2875–2879  2013.

[2] Michal Aharon  Michael Elad  and Alfred Bruckstein. K-svd: Design of dictionaries for sparse representation. In IN: PROCEEDINGS

OF SPARS05  pages 9–12  2005.

[3] Animashree Anandkumar  Rong Ge  Daniel Hsu  Sham M. Kakade  and Matus Telgarsky. Tensor decompositions for learning latent

variable models. J. Mach. Learn. Res.  15(1):2773–2832  January 2014.

[4] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  efﬁcient  and neural algorithms for sparse coding. In Proceedings of

The 28th Conference on Learning Theory  COLT 2015  Paris  France  July 3-6  2015  pages 113–149  2015.

[5] Sanjeev Arora  Rong Ge  and Ankur Moitra. Learning topic models–going beyond svd. In Foundations of Computer Science (FOCS) 

2012 IEEE 53rd Annual Symposium on  pages 1–10. IEEE  2012.

[6] Sanjeev Arora  Rong Ge  and Ankur Moitra. New algorithms for learning incoherent and overcomplete dictionaries. arXiv preprint

arXiv:1308.6273  2013.

[7] Maria-Florina Balcan  Avrim Blum  and Santosh Vempala. A discriminative framework for clustering via similarity functions.

Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing  STOC ’08  pages 671–680  2008.

In

[8] Boaz Barak  Jonathan A. Kelner  and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In
Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing  STOC 2015  Portland  OR  USA  June 14-17 
2015  pages 143–151  2015.

[9] Shai Ben-David and Margareta Ackerman. Measures of clustering quality: A working set of axioms for clustering.

In D. Koller 
D. Schuurmans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information Processing Systems 21  pages 121–128. Curran
Associates  Inc.  2009.

[10] Toby Berger. Rate distortion theory: A mathematical basis for data compression. 1971.

[11] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res.  3:993–1022  March 2003.

[12] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing).

Wiley-Interscience  2006.

[13] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Trans. Inf. Theor.  47(7):2845–2862  September

2006.

[14] Elad Hazan and Satyen Kale. Projection-free online learning. In Proceedings of the 29th International Conference on Machine Learning 

ICML 2012  Edinburgh  Scotland  UK  June 26 - July 1  2012  2012.

[15] Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In Proceed-

ings of the 4th conference on Innovations in Theoretical Computer Science  pages 11–20. ACM  2013.

[16] Nikola Jevtic  Alon Orlitsky  and Narayana P. Santhanam. A lower bound on compression of unknown alphabets. Theor. Comput. Sci. 

332(1-3):293–311  2005.

[17]

Jon M. Kleinberg. An impossibility theorem for clustering. In S. Becker  S. Thrun  and K. Obermayer  editors  Advances in Neural
Information Processing Systems 15  pages 463–470. MIT Press  2003.

[18] Roi Livni  David Lehavi  Sagi Schein  Hila Nachlieli  Shai Shalev-Shwartz  and Amir Globerson. Vanishing component analysis. In

Proceedings of the 30th International Conference on Machine Learning  ICML 2013  2013.

[19] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learning. MIT press  2012.

[20] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Comput.  24(2):227–234  1995.

[21] Alon Orlitsky  Narayana P. Santhanam  and Junan Zhang. Universal compression of memoryless sources over unknown alphabets. IEEE

Trans. Information Theory  50(7):1469–1481  2004.

[22] Hristo S Paskov  Robert West  John C Mitchell  and Trevor Hastie. Compressive feature learning. In Advances in Neural Information

Processing Systems  pages 2931–2939  2013.

[23]

Jorma Rissanen. Modeling by shortest data description. Automatica  14(5):465–471  1978.

[24] Ruslan Salakhutdinov. Learning Deep Generative Models. PhD thesis  University of Toronto  2009. AAINR61080.

[25] Ohad Shamir  Sivan Sabato  and Naftali Tishby. Learning and Generalization with the Information Bottleneck  pages 92–107. Springer

Berlin Heidelberg  Berlin  Heidelberg  2008.

[26] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) 

58(1):267–288  1996.

[27] Naftali Tishby  Fernando C. N. Pereira  and William Bialek. The information bottleneck method. CoRR  physics/0004057  2000.

[28] Leslie G Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142  1984.

[29] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.

[30] Van Vu. Singular vectors under random perturbation. Random Structures and Algorithms  39(4):526–538  2011.

9

,Elad Hazan
Tengyu Ma