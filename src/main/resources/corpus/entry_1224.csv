2019,Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels,Many machine learning methods depend on human supervision to achieve optimal performance. However  in tasks such as DensePose  where the goal is to establish dense visual correspondences between images  the quality of manual annotations is intrinsically limited. We address this issue by augmenting neural network predictors with the ability to output a distribution over labels  thus explicitly and introspectively capturing the aleatoric uncertainty in the annotations.
Compared to previous works  we show that correlated error fields arise naturally in applications such as DensePose and these fields can be modeled by deep networks  leading to a better understanding of the annotation errors.
We show that these models  by understanding uncertainty better  can solve the original DensePose task more accurately  thus setting the new state-of-the-art accuracy in this benchmark.
Finally  we demonstrate the utility of the uncertainty estimates in fusing the predictions of produced by multiple models  resulting in a better and more principled approach to model ensembling which can further improve accuracy.,Correlated Uncertainty for Learning

Dense Correspondences from Noisy Labels

Natalia Neverova  David Novotny  Andrea Vedaldi

Facebook AI Research

{nneverova  dnovotny  vedaldi}@fb.com

Abstract

Many machine learning methods depend on human supervision to achieve optimal
performance. However  in tasks such as DensePose  where the goal is to establish
dense visual correspondences between images  the quality of manual annotations
is intrinsically limited. We address this issue by augmenting neural network
predictors with the ability to output a distribution over labels  thus explicitly and
introspectively capturing the aleatoric uncertainty in the annotations. Compared to
previous works  we show that correlated error ﬁelds arise naturally in applications
such as DensePose and these ﬁelds can be modelled by deep networks  leading
to a better understanding of the annotation errors. We show that these models 
by understanding uncertainty better  can solve the original DensePose task more
accurately  thus setting the new state-of-the-art accuracy in this benchmark. Finally 
we demonstrate the utility of the uncertainty estimates in fusing the predictions
produced by multiple models  resulting in a better and more principled approach to
model ensembling which can further improve accuracy.

1

Introduction

Deep neural networks achieve state-of-the-art performance in many applications  but at the cost of
collecting large quantities of annotated training data. Manual annotations are time consuming and  in
some cases  of limited quality. This is particularly true for quantitative labels such as the 3D shape of
objects in images or dense correspondence ﬁelds between objects. In these cases  one should consider
manual labels as a form of weak supervision and design learning algorithm accordingly.
An emerging approach to handle annotation noise is to task the network with predicting the aleatoric
uncertainty in the labels. Consider a predictor ˆy = Φ(x) mapping a data point x to an estimate ˆy of
its label. Given the “ground-truth” label y  the standard approach is to minimize a loss of the type
(cid:96)(y  ˆy) so that ˆy approaches y as much as possible. However  if the “ground-truth” value y is affected
by noise  then naively minimizing this quantity may be undesirable. An alternative approach is to
predict instead a distribution p(ˆy|x) = Φˆy(x) over possible values of the annotation y. This has
several advantages: (1) it can model the distribution of annotation errors speciﬁc to each data point x
(as not all data points are equally difﬁcult to annotate)  (2) it can model the prediction uncertainty
(as knowing x may not be sufﬁcient to fully determine y)  and (3) it allows the model to account
for its own limitations (by assessing the difﬁculty of the prediction task). Under such a model  the
point-wise loss is replaced by the negative log-likelihood (cid:96)(y  Φ(x)) = − log p(y|x) = − log Φy(x).
Approaches using these ideas have demonstrated their power in a number of applications. However 
most methods have adopted simplistic uncertainty models. In particular  when the goal is to predict
a label vector y ∈ Rn  errors have been assumed to be uncorrelated  so that − log p(y|x) =
i=1 log p(yi|x). However  this is very seldom the case. For example  if yi is a label associated to
a particular pixel i in an image x  we can expect annotation and prediction errors to be very strongly
correlated for pixels in the same neighborhood.

−(cid:80)n

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Systematic errors in manual dense correspondences (DensePose [5]). The annotators
are shown a set of points sampled randomly and uniformly over one of predeﬁned body parts of
a person in an image. Their task is to click on corresponding pixels in another image obtained by
rendering and therefore providing ground truth correspondences to a canonical 3D model of a human
body. Due to self-occlusions and ambiguities  errors made by the annotators tend to be correlated
within each given body part and can be partially described by global afﬁne transforms (translation 
rotation  scaling) w.r.t true locations. We model the structure of these errors by learning a neural
network that estimates a distribution over the correlated annotation ﬁeld.

In this paper  we investigate richer uncertainty models that can better exploit the data structure. As a
running example  we consider the task of dense pose estimation  or DensePose (ﬁg. 1): namely  given
an image x of a human  the goal is to associate to every pixel i a coordinate chart (pi  ui) where
pi ∈ {0  1  . . .   P} is a part label (where 0 is the background) and ui ∈ [0  1]2 is a chart coordinate 
speciﬁc to part pi (ﬁg. 1). This is an interesting test case for three reasons: (1) the task consists of
pixel-wise  and hence correlated  predictions; (2) the quality of human annotations is uneven and data
dependent advantages; (3) there is structure in the data (body parts) that may align with the structure
of the annotation errors.
We make three contributions. The ﬁrst is to apply a standard uncertainty model to this task of learning
dense correspondences; this was not done before and alone achieves a signiﬁcant improvement over
strong baselines trained with regression losses.
Our second  and more signiﬁcant  contribution is to propose more sophisticated uncertainty models.
These models allow to predict for each pixel i a direction for the error in labelling the chart coordinate
vectors. This can for example be used to express different degrees of uncertainty due to foreshortening
of a limb. They also allow to express a degree of correlation between all vectors that belong to a
common region  in this case identiﬁed as a human body part. While richer  our models can still be
integrated efﬁciently in deep neural networks for end-to-end training.
Our third contribution is a deeper departure from prior work. Instead of just modelling uncertainty
as distribution − log p(ˆy|x) = Φˆy(x) conditioned on the input data x  we consider the possibility of
conditioning the uncertainty on the annotation y directly. For example  in the DensePose task  the
annotation y can be used to predict image regions where uncertainty is likely to be higher even before
observing the image x.

2 Related work

Uncertainty in machine learning is usually decomposed into three types [9]: approximation  due to
the fact that the model is not sufﬁciently expressive to model the data-to-label association  aleatoric 

2

due to the intrinsic stochastic nature of the association  and epistemic  due to the model’s limited
knowledge about this association  which prevents it form determining it uniquely.
Uncertainty in deep neural network has been modelled using approximate Bayesian approaches [6 
1  7  8] using dropout as a way to generate the necessary samples [4  7]. Ensembling [3]  which
combines multiple models  has been explored in [15  10  16]. The recent method of [18] proposes a
frequentist method that can estimate both aleatoric and epistemic uncertainty.
The work of [7  14] is probably the most related to ours. They also model approximation and aleatoric
uncertainty by conﬁguring a deep neural network to produce an explicit estimate of the latter  in the
form of a parametric posterior distribution. In this paper  we build a similar model  but apply it to a
dense  structure image labelling tasks. We thus extend the model to express structured uncertainty 
where errors are highly correlated in a way which depends on the input image and annotation.
We apply our approach to the DensePose problem  originally introduced in [5]. We not only show
that we can accurately model uncertainty in the annotation process  but also learn better overall
DensePose regressor  outperforming the current state-of-the-art results of [12]  from which we borrow
our experimental setup.

3 Method
We consider the problem of predicting  given a data point x  a label vector y ∈ Rdn formed by n
subvectors yi ∈ Rd  i = 1  . . .   n of dimension d. In our test application  namely DensePose  these
subvectors are the chart coordinates yi = ui ∈ [0  1]2 that associate to pixels i of image x a particular
point on the human body. However  there are many other problems  including colorization  depth
prediction and inpainting  that can be modelled in a similar manner.
We denote by δ = ˆy − y the error between the predicted value ˆy of the label and the annotated
value y. In order to model uncertainty in the system  we train a predictive model Φ(x) that outputs
not only the point estimate ˆy ≈ y  but also a distribution p(y|x) over possible values. For simplicity 
we express the latter as
(1)
where q(δ|x) is an unbiased distribution of the residual (i.e. E[q(δ|x)] = 0 and argmaxδ q(δ|x) = 0).
Hence  the output of the neural network is a pair (ˆy  q) = Φ(x) comprising a point estimate ˆy and the
distribution of the residual q. This model can be trained by optimizing the negative log-likelihood
(cid:96)(y  Φ(x)) = − log q(y − ˆy).
Next  we discuss possible variants of the model with different complexity and expressive power.

p(y|x) = q(ˆy − y|x)

model then is given by q(δ|x) =(cid:81)n

3.1 Elementary uncertainty model
In the simplest case  we let y ∈ Rn and assume that subvectors yi are single scalars. The uncertainty
i=1 q(δi|x) which amounts to assuming that the residuals are
statistically independent. The simplest choice for q(δi|x) is a Gaussian N (0  σ2
i ) with standard
deviation σ2
i . Hence  the neural network (ˆy  σ) = Φ(x) outputs for each pixel i the prediction ˆyi as
well as an estimate σ2

i of the prediction uncertainty. In this case  the training loss expands as:

(cid:96)(y  Φ(x)) =

n
2

log 2π +

1
2

log σ2

i +

(cid:18)

n(cid:88)

i=1

(cid:19)

(ˆyi − y)2

σ2
i

i is obtained by setting σ2 = |ˆyi − y|. Hence  the predictor Φ
Note that minimum of the r.h.s. w.r.t. σ2
will try to output a value of σi which is equal to the error actually incurred at that pixel; crucially 
however  the model Φ cannot measure this error directly as it only receives the data point x (and not
the annotation y) as input. Hence  the model is encouraged to perform introspection.

3.2 Higher-order uncertainty models

The model of section 3.1 is simplistic as it assumes that errors are statistically independent  which
is seldom the case in applications. In order to address this limitation  we assume that residuals are
instead generated by the model

(2)

(3)

δi =  + ηi + ξiwi

3

1Id) is an overall isotropic offset  ηi ∼ N (0  σ2

where  ∼ N (0  σ2
2iId)  is a subvector speciﬁc
isotropic offset and ξi ∼ N (0  σ2
3i) is a subvector speciﬁc directional offset along the unit vector wi.
Here Id denotes the d × d identity matrix  where d is the dimensionality of the subvectors yi (d = 2
in the DensePose application).
This model extends (2) in several ways. First  the term  indicates that errors are overall correlated.
For instance  in DensePose it is likely that all points annotated for a given human body part would be
affected by a similar annotation shift compared to the “correct” annotation. This is because humans
are better at relative rather than absolute judgments when it comes to establishing ambiguous visual
correspondences. Second  the term ηi expresses local isotropic uncertainty  similar to (2). Third  the
term ξiwi express local directional uncertainty. This can be used to capture any expected directionally
in the error. For instance  in DensePose we expect errors to be larger in the direction of visual
foreshortening of a limb.
Next  we calculate the negative log-likelihood − log q(δ|x) under this model in order to evaluate and
learn it. The collection of residuals δ is a Gaussian vector with co-variance matrix

Σ = JJ(cid:62) + diag(Σ1  . . .   Σn)  Σi = σ2
···

(cid:62) ∈ Rdn×d.

2iId + σ2

3iwiw(cid:62)
i  

where J = σ1 · [Id
Some algebra shows that the determinant of the covariance matrix and the concentration matrix are
given by

Id]

(4)

det Σi  C = Σ−1 = ¯C − ¯CJK−1J(cid:62) ¯C  K = Id + σ2

Σ−1

i

.

(5)

i=1

i=1

Here ¯C = diag(C1  . . .   Cn) is the block-diagonal matrix containing the subvector-speciﬁc concen-
tration matrices Ci = Σ−1

. We can expand this further by noting that:

i

Ci =

1
σ2
2i

· Πi  Πi = Id − ρiwiw(cid:62)
i  

det Σi =

σ2d
2i
1 − ρi

 

ρi =

σ2
3i

σ2
2i + σ2
3i

(6)

where Πi can be interpreted as a projection operator and ρi as a correlation coefﬁcient. Then:1

det Σ = det K · n(cid:89)

1 · n(cid:88)

− log q(δ|x) =

nd
2

log 2π +

+

1
2

log det K − σ2
1
2

(cid:18)

n(cid:88)
(cid:32) n(cid:88)

1
2

i=1

i=1

log

σ2d
2i
1 − ρi

(cid:33)(cid:62)

K−1

Πiδi
σ2
2i

+

δ(cid:62)
i Πiδi
σ2
2i

(cid:32) n(cid:88)

Πiδi
σ2
2i

i=1

(cid:19)
(cid:33)

1 · n(cid:88)

  K = Id + σ2

i=1

Πi
σ2
2i

.

(8)

Spatially-independent model. Model (8) correlates the errors of all subvectors. We can loose this
condition by setting σ1 = 0. In this case K = Id and J = 0 and the model reduces to:

− log q(δ|x) =

nd
2

log 2π +

1
2

log

σ2d
2i
1 − ρi

+

δ(cid:62)
i Πiδi
σ2
2i

.

(9)

(cid:18)

n(cid:88)

i=1

(cid:19)

1 In practice  it is easier for a network to predict ri = σ3iwi ∈ R2 instead of ρi and wi separately  so we use

the parametrization

Furthermore  the negative sign in eq. (8) can lead to instabilities. We thus rewrite the equation as the sum of
squares:

Πi = Id −

rir(cid:62)
2i + (cid:107)ri(cid:107)2 .
σ2

i

(7)

(δi − µ)

(δi − µ) +

(cid:62) Πi
σ2
2i

µ(cid:62)µ
2σ2
1

σ2d
2i
1 − ρi

= σ2(d−1)

2i

2i + (cid:107)ri(cid:107)2) 

(σ2

ρi =

(cid:33)(cid:62)

(cid:32) n(cid:88)

i=1

δ(cid:62)
i Πiδi
σ2
2i

− σ2
1
2

Πiδi
σ2
2i

−1

K

(cid:107)ri(cid:107)2
2i + (cid:107)ri(cid:107)2  
σ2
(cid:33)
n(cid:88)

=

1
2

i=1

Πiδi
σ2
2i

(cid:32) n(cid:88)

i=1

where we have introduced the ‘mean’ vector:

µ = σ2

1K

−1

Πi
σ2
2i

δi.

n(cid:88)

i=1

4

Figure 2: Uncertainty-aware training pipeline. We extend a standard predictor based on a Hour-
glass architecture [13  12] with an additional uncertainty head to estimate uncertainty parameters.

This requires the model to estimate for each pixel the value of the variance σ2
directional correlation parameters ri ∈ R2.

2i as well as of the

Fully-independent model. The elementary model corresponding to eq. (2) is obtained by further
setting parameter ri = 0 in eq. (9).

3.3 Label-conditioned uncertainty

Next  we consider modifying the approach so that uncertainty is predicted not only from the input
data x  but also based on the label y itself.
On a ﬁrst glance  this may look as simple as adding the argument y to the predictor Φ(x) to obtain a
new predictor Φ(x  y). This is however nonsensical as (ˆy  q) = Φ(x  y) is tasked with producing an
estimate of ˆy itself  so this would immediately lead to a degenerate solution.
Instead  we consider two separate networks. The ﬁrst  ˆy = Φ1(x)  is tasked with predicting only
the label y. The second  q = Φ2(x  y)  is tasked with predicting only the uncertainty distribution q.
Without any constraint  this scheme still does not work as the distribution q can shift the prediction ˆy
arbitrarily. This is prevented by the fact that E[q(δ)] = 0; in fact  in practice we require q(y) to be
a simple uni-modal distribution. In this manner  q can effectively only predict the data uncertainty 
but ˆy must still try to predict the label correctly in order to minimize the log-likelihood loss.

Introspective ensemble

k=1 C (k))−1(cid:80)K

(cid:80)K
maximizer is then y = ((cid:80)K
k=1 log qk(y − ˆy(k)|x) = argmaxy

3.4
Assume that we have densities qk(δ|x)  k = 1  . . .   K generated from an ensemble of K models and
(cid:80)K
let ˆy(k) be the corresponding label estimates (we use the superscript to indicate that we index different
estimates instead of different components of a single vector estimate). We can fuse the estimates by
k=1(y − ˆy(k))(cid:62)C (k)(y − ˆy(k)). The
ﬁnding y = argmaxy
k=1 C (k) ˆy(k). Note that  while the section above gives us
the inverse of the concentration matrices of each model  in case of the probabilistic model of the
highest order eq. (8) we require the inverse of the sum  which must be obtained numerically. In time-
constrained applications  such as real time  rather than solving such a large system of equations  we
can utilize conjugate gradient descend to obtain an approximate solution starting from an initial guess
(which can be obtained as the average of the individual models’ predictions). For the simpler cases of
ensembles of spatially-independent models and fully-independent models  the fused estimates can
be computed in closed form. For the spatially independent model (eq. (9))  it follows that the fused
uv predictions yspa
i )  which now
only requires to invert a small 2x2 matrix formed by accumulating C (k)
above each pixel. In case of

i = ((cid:80)K

at position i are deﬁned as yspa

k=1 C (k)

i

k=1 C (k)

i ˆyk

i

5

)−1((cid:80)K

i

Figure 3: Example of predictions produced by our model. Ground truth locations (in red) and
predicted locations (in green) are shown together with learned isotropic offsets described by σ2.

i =(cid:80)K

k=1

i(cid:80)K

ˆσ
k=1 ˆσ

ˆy(k)
i

−2(k)
i

the fully independent model with isotropic covariance matrices (eq. (2))  the ensembled prediction
yiso

is a mere weighted sum of ˆy(k)

−2(k)

.

i

4 Application to DensePose

In this section we show in more detail how the ideas explained above can be applied to the DensePose
problem [5]. In this work  we adapt the DensePose setting of [12]  where the input is an image x ∈
R3×H×W tightly containing a person (DensePose can also be applied to full images in combination
with an object detector  but we are not concerned with that here). DensePose then trains a network Φ
to predict a label y ∈ RC×H×W where the C = 3 · P channels of vector yi for pixel i comprise a
P -dimensional indicator vector for the part that contains pixel i (e.g. left forearm) and the 2D location
in the chart of each part  accounting for 2P dimensions. Note that only one of the 2P predicted
locationa is used at pixel i  as indicated by the part selector  but all of them are still computed.
We extend the basic architecture with an additional uncertainty head that estimates the prediction con-
ﬁdences (see ﬁg. 2). Depending on which speciﬁc model is implemented  the output dimensionality
of this branch may differ. However  each variant amounts to predicting a certain number of additional
channels per part  estimating part-speciﬁc uncertainty values  and can be generally expressed as
Nu × P   where Nu is the number of uncertainty parameters. Note that depending on the application 
at test time the uncertainty head may be either utilized to get conﬁdence estimates  or ignored. In
the latter case  the uncertainty aware training results in a boost of model performance at no extra
computational cost during inference.
All uncertainty parameters are predicted by applying a set of two convolutional blocks to an interme-
diate feature level F  produced by the main network Φ1. As mentioned in section 3.3  in addition  we
explore a variant of the model where the sparse ground truth annotations are passed directly to the
uncertainty head as an additional input. The annotated points are ﬁrst mapped onto the image space 
preprocessed by a set of partial convolutional layers [11] and then concatenated with features F. This
process is illustrated in ﬁg. 2. An example of model predictions is shown in ﬁg. 3.

5 Experiments

Datasets. To gain deeper insights on the nature of human annotator errors on the dense labeling
task  we ﬁrst analyzed DensePose annotations obtained on a set of 88 synthetic images  where ground
truth UV mapping is known by design (analogously to Section 2.2 of [5]). These images were
rendered using the SMPL body model [2] and the rendering pipeline of [19].
We have empirically observed that  by taking all annotated points covering one body part in one
image and applying a simple global afﬁne transformation to them (such as translation or scaling)  the
mean error over the whole image set can be reduced by half. This conﬁrms our hypothesis of existing
strong correlation between individual errors.

6

Model

DensePose-RCNN (R50) [5]

HRNetV2-W48 [17]

HG  1 stack (Slim DensePose [12])

HG  2 stacks (Slim DensePose [12])

HG  8 stacks (Slim DensePose [12])

uv-loss

MSE

full (ours)

MSE

full (ours)

MSE

full (ours)

MSE

full (ours)

MSE

full (ours)

1 cm 2 cm
18.17
5.21
18.67
5.67
15.19
4.31
5.70
18.81
15.62
4.31
18.23
5.34
16.21
4.44
5.99
19.97
20.25
6.04
6.41
20.98

3 cm
31.01
32.70
27.14
31.88
28.30
31.51
29.64
34.16
35.10
35.17

5 cm 10 cm 20 cm
78.37
51.16
80.47
53.14
78.66
47.07
52.20
82.12
83.01
49.92
52.40
82.94
85.99
52.23
55.68
85.58
87.55
56.04
56.48
87.96

68.21
71.25
69.76
74.21
74.15
74.69
76.50
77.76
79.63
80.02

Table 1: Performance of uncertainty-based models on the DensePose-COCO dataset [5]. Our
models signiﬁcantly outperform the baseline variants  with no extra computational cost at inference
(when uncertainty estimates are not required by application).

Data

simple
simple-2D
iid
full

(2.9785)
(2.5748)
(3.2285)
(3.0683)

gt-real
1.0816
1.4246
1.7026
2.3448

SMPL renderings (synthetic)
MAP

gt-human

DensePose-COCO (real)
gt-human

MAP

1.1716
1.4825
1.8937
2.3574

(3.0797)
(2.5892)
(3.2038)
(2.9847)

1.3159
1.3651
1.4383
2.14057

Table 2: Negative log-likelihood of human annotations under different models with uncertainty.
More advanced models show monotonic increase in this metric w.r.t the ground truth locations.
gt-human stands for human annotations on synthetic data  gt-real for the ground truth UV maps.
simple-2D: assumes independent (but not isotropicaly nor identically distributed) per-pixel errors.

The majority of experiments in this work were conducted on the DensePose-COCO dataset [5] 
containing 48k densely annotated people in the training set and 2.3k in the validation set. We follow
the single person protocol for this task and use ground truth annotations for bounding boxes to crop
images around each person.

Metrics. For evaluation  we adapt a standard per-pixel metric used in [5] and [12] and report the
percentage of points predicted with an error lower than a set of predeﬁned thresholds  where the error
is expressed in geodesic distances measured on the surface of the 3D model. Since  in this work  we
focus speciﬁcally on the UV-regression part of the DensePose task while keeping the segmentation
pipeline standard  we additionally report performance w.r.t. stricter geodesic thresholds and in two
settings when the segmentation is either predicted by the same network or assumed to be perfect and
correspond to the ground truth at test time.

Implementation details. The architecture of the main DensePose predictor Φ1 is based on the
Hourglass network [13] adapted to this task by [12]. We benchmark performance on 1  2 and 8 stacks 
but conduct most of the ablation studies on a 1-stack network for speed. All networks are trained for
300 epochs with SGD  batch size 16 and learning rate of 0.1 decreasing by a factor of 10 after 180
and 270 epochs. Input images are normalized to the resolution of 256 × 256.

Uncertainty models.
In our experiments  we analyze several modiﬁcations of networks with
uncertainty heads  which we denote as follows: MSE stands for the uncertainty free baseline of [12]
trained with the MSE regression loss; simple corresponds to the elementary uncertainty model
given by 2; simple-2D is a variant of (2) with two distinct σu and σv learned separately for u- and
v-dimensions; iid denotes the spatially-independent model of (9) and full stands for the complete
model given deﬁned by (8).
As shown in Table 3  introducing uncertainty into the DensePose training brings signiﬁcant gains
in performance over the considered baseline. Our 2-stack architecture signiﬁcantly outperforms

7

Model

MSE
simple
simple-2D
iid
full

UV only (GT body parsing)

overall performance

1 cm 2 cm
19.92
5.57
22.80
6.71
23.40
7.02
6.95
23.39
23.08
6.78

3 cm
35.74
38.97
39.76
39.70
39.42

5 cm 10 cm 1 cm 2 cm
15.62
61.75
17.96
64.30
18.53
64.89
64.95
18.46
64.62
18.23

89.53
90.04
90.25
90.25
90.14

4.31
5.27
5.54
5.44
5.34

3 cm
28.30
31.08
31.80
31.70
31.51

5 cm 10 cm
74.15
49.92
74.39
52.04
74.98
52.67
52.66
74.84
74.69
52.40

Table 3: Ablation on uncertainty terms. The left part of the table reports upper bound results in
assumption of perfect body parsing at test time.

Model

full
+gt (train)

UV only (GT body parsing)

overall performance

1 cm 2 cm
6.78
23.08
23.84
7.18

3 cm
39.42
40.33

5 cm 10 cm 1 cm 2 cm
64.62
18.23
18.68
65.36

90.14
90.38

5.34
5.60

3 cm
31.51
31.97

5 cm 10 cm
52.40
74.69
74.28
52.57

Table 4: Label-conditioned uncertainty. Exploiting ground truth labels as an additional direct cue
has shown to accelerate learning and results in higher performance.

Model

MSE
simple
iid

Best model

Average

Ours

5 cm 10 cm 20 cm 5 cm 10 cm 20 cm 5 cm 10 cm 20 cm
49.92
52.04
52.66

83.01
82.74
83.01

74.15
74.39
74.84

75.09
75.49
75.29

83.82
82.84
82.56

50.49
54.26
54.15

–

54.46
54.55

–

75.55
75.59

–

82.86
82.77

Table 5: Introspective ensemble. We collect a number of models with identical architectures but
trained with different hyperparameters (weights on the UV-term: 0.1  0.2 and 0.5). More diverse
ensembles are expected to deliver higher gains in performance in all settings.

much more powerful 8-stack models  especially when measured on tighter geodesic thresholds. This
provides an additional evidence for the hypothesis that uncertainty-based training facilitates learning
with noisy annotations and allows the model to decrease the associated jitter in predictions.
The ablation on different variants of the uncertainty models is given in Table 3. In terms of accuracy
of UV-predictions  the more advanced models (iid and full) perform on par with their simpler
counterparts (simple and simple-2D) (note that test complexity of the main predictor is identical
for all models). Their advantage however becomes apparent when looking at the log-likelihood of the
ground truth labels evaluated by each of the models (see Table 2). In this setting  full model clearly
provides more meaningful representation of the learned distribution  which is no doubt critical for
numerous downstream tasks.

Label-conditioned uncertainty. Following the discussion of Section 3.3  we ablated the effect of
using ground truth annotations as a direct cue for learning uncertainty. Table 4 shows immediate
beneﬁts of doing so in terms of the target UV-metrics  but we also observed signiﬁcant increase
of log-likelihoods of the true labels evaluated by the gt-based model. Note that no ground truth
information is required by the model at test time  as long as uncertainty estimates are not utilized.

Introspective ensembles. Finally  we benchmark the performance of the proposed ensembling
techniques for several variants of the models with uncertainty. Exploiting uncertainty parameters
for ﬁnding the right balance consistently outperforms the averaging late fusion baseline in all tested
scenarios over a range of models (see Table 5).

6 Conclusions

In this paper we have investigated the use of introspective uncertainty prediction models to improve
the robustness and expressiveness of models for dense structured label prediction. We have introduced

8

a method to estimate  using a convolutional neural network  an uncertainty model which potentially
correlates the errors of all the pixels in an image. We have applied these ideas to the DensePose
tasks  showing how these approaches can result in signiﬁcant performance improvements compared
to the current state-of-the-art. Since the structure of the regressor is unchanged compared to the
latter approaches  these improvements are solely imputable to the models’ better understanding of
the uncertainty in the data. This is particularly beneﬁcial for problems  such as DensePose  where the
quality of manual labels is intrinsically limited.

References
[1] C. Blundell  J. Cornebise  K. Kavukcuoglu  and D. Wierstra. Weight uncertainty in neural

networks. In ICML  2015.

[2] Federica Bogo  Angjoo Kanazawa  Christoph Lassner  Peter Gehler  Javier Romero  and
Michael J Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a
single image. In ECCV  2016.

[3] L. Breiman. Bagging predictors. Machine Learning  24(2)  1996.
[4] Y. Gal and Z. Gharamani. Dropout as a bayesian approximation: Representing model uncertainty

in deep learning. In ICML  2016.

[5] Rıza Alp Güler  Natalia Neverova  and Iasonas Kokkinos. Densepose: Dense human pose

estimation in the wild. In CVPR  2018.

[6] J. M. Hernández-Lobato and R. P. Adams. Probabilistic backpropagation for scalable learning

of bayesian neural networks. In ICML  2015.

[7] A. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning for computer

vision? In NIPS  2017.

[8] M. E. Khan  D. Nielsen  V. Tangkaratt  W. Lin  Y. Gal  and A. Srivastava. Fast and scalable

bayesian deep learning by weight-perturbation in ADAM. In ICML  2018.

[9] A. Der Kiureghian and O. Ditlevsen. Aleatory or epistemic? does it matter?

Workshop on Risk Acceptance and Risk Communication  2007.

In Special

[10] B. Lakshminarayanan  A. Pritzel  and C. Blundell. Simple and scalable predictive uncertainty

estimation using deep ensembles. NIPS  2017.

[11] Guilin Liu  Fitsum A. Reda  Kevin J. Shih  Ting-Chun Wang  Andrew Tao  and Bryan Catanzaro.

Image inpainting for irregular holes using partial convolutions. In ECCV  2018.

[12] Natalia Neverova  James Thewlis  Rıza Alp Güler  Andrea Vedaldi  and Iasonas Kokkinos. Slim

densepose: Thrifty learning from sparse annotations and motion cues. In CVPR  2019.

[13] Alejandro Newell  Kaiyu Yang  and Jia Deng. Stacked hourglass networks for human pose

estimation. In ECCV  2016.

[14] D. Novotny  D. Larlus  and A. Vedaldi. Learning 3d object categories by looking around them.

In ICCV  2017.

[15] I. Osband  C. Blundell  A. Pritzel  and B. Van Roy. Deep exploration via bootstrapped DQN. In

NIPS  2016.

[16] T. Pearce  M. Zaki  A. Brintrup  and A. Neely. High-quality prediction intervals for deep

learning: A distribution-free  ensembled approach. In ICML  2018.

[17] Ken Su  Yang Zhao  Tianheeng Jiang  Borui andd Cheng  Bin Xiao  Dong Liu  Yadong Mu 
Xinggang Wang  Wenyu Liu  and Jingdong Wang. High-resolution representations for labeling
pixels and regions. In arXiv:1904.04514v1  2019.

[18] N. Tagasovska and D. Lopez-Paz. Frequentist uncertainty estimates for deep learning. In

arXiv:1811.00908  2019.

[19] Gül Varol  Javier Romero  Xavier Martin  Naureen Mahmood  Michael J Black  Ivan Laptev 

and Cordelia Schmid. Learning from synthetic humans. In CVPR  2017.

9

,Guilhem Chéron
Jean-Baptiste Alayrac
Ivan Laptev
Cordelia Schmid
Natalia Neverova
David Novotny
Andrea Vedaldi