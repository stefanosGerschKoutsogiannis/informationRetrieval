2019,Efficiently avoiding saddle points with zero order methods: No gradients required,We consider the case of derivative-free algorithms for non-convex optimization  also known as zero order algorithms  that use only function evaluations rather than gradients. For a wide variety of gradient approximators based on finite differences  we establish asymptotic convergence to second order stationary points using a carefully tailored application of the Stable Manifold Theorem.  Regarding efficiency  we introduce a noisy zero-order method that converges to second order stationary points  i.e avoids saddle points. Our algorithm uses only $\tilde{\mathcal{O}}(1 / \epsilon^2)$ approximate gradient calculations and  thus  it matches the converge rate guarantees of their exact gradient counterparts up to constants. In contrast to previous work  our convergence rate analysis avoids imposing additional dimension dependent slowdowns in the number of iterations required for non-convex zero order optimization.,Efﬁciently avoiding saddle points

with zero order methods: No gradients required

Lampros Flokas∗

Department of Computer Science

Columbia University
New York  NY 10025

Emmanouil V. Vlatakis-Gkaragkounis∗

Department of Computer Science

Columbia University
New York  NY 10025

lamflokas@cs.columbia.edu

emvlatakis@cs.columbia.edu

Georgios Piliouras

Engineering Systems and Design

Singapore University of Technology and Design

Singapore

georgios@sutd.edu.sg

Abstract

We consider the case of derivative-free algorithms for non-convex optimization 
also known as zero order algorithms  that use only function evaluations rather than
gradients. For a wide variety of gradient approximators based on ﬁnite differences 
we establish asymptotic convergence to second order stationary points using a
carefully tailored application of the Stable Manifold Theorem. Regarding efﬁciency 
we introduce a noisy zero-order method that converges to second order stationary
points  i.e avoids saddle points. Our algorithm uses only ˜O(1/2) approximate
gradient calculations and  thus  it matches the converge rate guarantees of their exact
gradient counterparts up to constants. In contrast to previous work  our convergence
rate analysis avoids imposing additional dimension dependent slowdowns in the
number of iterations required for non-convex zero order optimization.

Introduction

1
Given a function f : Rd → R  solving the problem

x∗ = arg min
x∈Rd

f (x)

is one of the building blocks that many machine learning algorithms are based on. The difﬁculty
of this problem varies signiﬁcantly depending on the properties of f and the way we can access
information about it. The general case of non-convex functions makes the problem signiﬁcantly more
challenging  since ﬁrst order stationary points can be global or local optima as well as saddle points.
In fact  discovering global optima is an NP hard problem in general and even for quartic functions
verifying local optima is a co-NP complete problem [MK87  LPP+19].
While local optima may be satisfactory for some applications in machine learning [CHM+15] 
saddle points can make high dimensional non convex optimization tasks signiﬁcantly more difﬁcult
[DPG+14  SQW18]. Therefore  researchers have focused their efforts on functions possessing
the strict saddle property. Under this property  Hessians of f evaluated at saddle points have at
least one negative eigenvalue making detection of saddle points tractable. Given this assumption 

∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

methods that use second order information like computing Hessians or Hessian-vector products
[NP06  CD16  AAB+17] can converge to second order stationary points (SOSPs) and thus avoid
strict saddle points. Recent work [GHJY15  Lev16  JGN+17  LPP+19  AL18  JNJ18] has also
showed that gradient descent (and its variants) can also avoid strict saddle points and converge to
local minima.
Unfortunately access to gradient evaluations is not available in all settings of interest. Even with
the advent of automatic differentiation software  there are several applications where computation
of gradients is either computationally inefﬁcient or even impossible. Examples of such applications
are hyper-parameter tuning of machine learning models [SLA12  SHCS17  CRS+18]  black-box
adversarial attacks on deep neural networks [PMG+17  MMS+18  CZS+17]  computer network
control [LCCH18]  variational approaches to graphical models [WJ08] and simulation based [RK16 
Spa03] or bandit feedback optimization [ADX10  CG19]. Zero order methods  also known as black-
box methods  try to address these issues by employing only evaluations of the function f during the
optimization procedure. The case of convex functions is well understood [NS17  DJWW15  ADX10].
For the non-convex case  there has been a considerable amount of work on the convergence to ﬁrst
order stationary points both for deterministic settings [NS17] and stochastic ones [GL13  WDBS18 
BG18  LKC+18  GHH16].
The case of SOSPs has been so far comparatively under-studied. It has been established that SOSPs
are achievable through zero order trust region methods that employ fully quadratic models [CSV09].
The disadvantage of trust region methods is that their computation cost per iteration is O(d4) which
becomes quickly prohibitive as we increase the number of dimensions d. More recently  the authors
of [JLGJ18] studied the case of ﬁnding local minima of functions having access only to approximate
function or gradient evaluations. They manage to reduce zero order optimization to the stochastic
ﬁrst order optimization of a Gaussian smoothed version of f. While this approach yields guarantees
of convergence to SOSPs   each stochastic gradient evaluation requires O(poly(d  1/)) number
of function evaluations. This leads to signiﬁcantly less efﬁcient optimization algorithms when
compared to their ﬁrst order counterparts. It is therefore yet unclear if there are scalable zero
order methods that can safely avoid strict saddle points and always converge to local minima
of f. To the best of our knowledge  our work is the ﬁrst one to establish a positive answer to
this important question.
Our results. We prove that zero order optimization methods solve general non-convex problems
efﬁciently. In a nutshell  we present a family of of zero order optimization methods which provably
converge to SOSPs . Our proof includes a new  elaborating analysis of Stable Manifold Theorem
(See Section 4). Additionally  the number of the approximate gradient evaluations match the standard
bounds for ﬁrst order methods in non-convex problems (see Table 1 & Section 5).

Algorithm
Oracle
Theorem 3
Approximate Gradient
[LPP+19]
Exact Gradient
Theorem 4
Approx. Gradient + Noise
FPSGD [JLGJ18] Approx. Gradient + Noise
ZPSGD [JLGJ18]
[JGN+17]

Function Evaluations + Noise
Exact Gradient + Noise

Evaluations of f

Iterations
Asymptotic Asymptotic
Asymptotic
˜O(1/2)
˜O(d/2)
˜O(1/2)
˜O(1/2)

-
˜O(d/2)
˜O(d4/4)
˜O(d2/5)
-

Table 1: Oracle model and iteration complexity to SOSPs .

Algorithms. Instead of focusing on a single ﬁnite differences algorithms  we construct a general
framework of approximate gradient oracles that generalizes over many ﬁnite differences approaches
in the literature. We then use these approximate gradient oracles to devise approximate gradient
descent algorithms. For more details see Section 3.3 and Deﬁnition 4.
Asymptotic convergence. We use the stable manifold theorem to prove that zero order methods can
almost surely avoid saddle points. In contrast to the analysis of [LPP+19] for ﬁrst order methods  the
zero order case is more demanding. Convergence to ﬁrst order stationary points requires changing
the gradient approximation accuracy over the iterations and  thus  the equivalent dynamical system is
time dependent. By reducing our time dependent dynamical system to a time invariant one deﬁned in

2

an expanded state  we are able to obtain provable guarantees about avoiding saddle points. To extend
our guarantees of convergence to deterministic choices of the initial accuracy  we provide a carefully
tailored application of the Stable Manifold Theorem that analyzes the structure of the stable manifolds
of the dynamical system. Our results on saddle point avoidance extend to functions with non isolated
critical points. To address this  we provide sufﬁcient conditions for point-wise convergence of the
iterates of approximate gradient descent methods for the case of analytic functions.
Convergence rates for noisy dynamics. In order to produce fast convergence rates  as in the case
of ﬁrst order methods [JGN+17]  it is useful to consider perturbed/noisy versions of the dynamics.
Once again the case of zero order methods poses distinct hurdles. Close to critical points of f 
approximations of the potentially arbitrarily small gradient can be very noisy. Iterates of exact
gradient descent and approximate gradient descent may diverge signiﬁcantly in this case. In fact 
provably escaping saddle points by guaranteeing decrease of value of f is more challenging for the
case of approximate gradient descent since it is not a descent algorithm. A key technical step is to
show that the negative curvature dynamics that enable gradient descent to escape saddle points are
robust to gradient approximation errors. As long as the gradient approximation error is smaller than
a ﬁxed a-priori known threshold  zero order methods can provably escape saddle points. Based on
this  we are able to prove that zero order methods can converge to approximate SOSPs with the same
number of approximate gradient evaluations provided by [JGN+17] up to constants.
It is worth pointing out that achieving an ˜O(−2) bound of approximate gradient evaluations requires
conceptually different techniques from other recent approaches in zero order methods. Indeed 
previous work on randomized and stochastic zero order optimization [NS17  GL13] has relied on
treating randomized approximate gradients of f as in expectation exact gradients of a carefully
constructed smoothed version of f. Then with some additional work  convergence arguments for
the smooth version of f can be transferred to f itself. Although these arguments are applicable to
our case as well  as shown by the work of [JLGJ18]  they also lead to a slowdown both in terms of
the dimension d and the required accuracy . The main reasons behind this slowdown are that the
Lipschitz constants of the smoothed version of f depend on d and the high variance of the stochastic
gradient estimators. To sidestep both issues  we analyze the effect of gradient approximation error
directly on the optimization of f.

2 Related Work

Our work builds and improves upon previous ﬁnite difference approaches for non-convex optimization
and provides SOSP guarantees previously only reserved to computationally expensive methods.
First Order Algorithms A recent line of work has shown that gradient descent and variations of it
can actually converge to SOSPs . Speciﬁcally  [LPP+19] shows that gradient descent starting from a
random point can eventually converge to SOSPs with probability one. [JGN+17  JNJ18] modiﬁed
standard gradient descent using perturbations to provide an algorithm that converges to SOSPs in
O(poly(log d  1/)) iterations. As noted in the introduction  the zero order case poses additional
hurdles compared to the ﬁrst order one. Our work  by addressing these hurdles effectively extends
the guarantees provided by [LPP+19  JGN+17] to zero order methods.
Zero Order Algorithms Approximating gradients using ﬁnite differences methods has been the
standard approach for both for convex and non-convex zero order optimization.[NS17] established
convergence properties even for randomized gradient oracles. Recently  [DJWW15] provided optimal
guarantees for stochastic convex optimization up to logarithmic factors. For the more general case
of stochastic non-convex optimization there has been extensive work covering several aspects of
the problem: distributed [HZ18]  asynchronous [LZH+16]  high-dimensional [WDBS18  BG18]
optimization and variance reduction [LKC+18  GHH16]. It is signiﬁcant to mention that the afore-
mentioned work is focused on convergence to −ﬁrst order stationary points.
Regarding SOSPs   [CSV09] showed that trust region methods that employ fully quadratic models can
converge to SOSPs at the cost of O(d4) operations per iteration. The authors of [JLGJ18] studied the
convergence to SOSPs using approximate function or gradient evaluations. While both approaches
are applicable for the zero order setting with exact function evaluations  as we will see in Section
3.4  this type of reduction results in algorithms that require substantially more function evaluations
to reach an -SOSP . Our work provides provable guarantees of convergence at signiﬁcantly faster
rates.

3

3 Preliminaries

3.1 Notation
We will use lower case bold letters x  y to denote vectors. (cid:107)·(cid:107) will be used to denote the spectral
norm and the (cid:96)2 vector norm. λmin(·) will be used to denote the minimum eigenvalue of a matrix. If
g is a vector valued differentiable function then Dg denotes the differential of function g. We will
use {e1  e2  . . . ed} to refer to the standard orthonormal basis of Rd. Also C n is the set of n times
continuously differentiable functions. Bx(r) refers to the ball of radius r centered at x. Finally  µ(S)
is the Lebesgue measure of a measurable set S ⊆ Rd.

3.2 Deﬁnitions
A function f : Rd → R is said to be L-continuous  (cid:96)-gradient  ρ-Hessian Lipschitz if for every
x  y ∈ Rd (cid:107)f (x) − f (y)(cid:107) ≤ L(cid:107)x − y(cid:107)  (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ (cid:96)(cid:107)x − y(cid:107)  (cid:107)∇2f (x) − ∇2f (y)(cid:107) ≤
ρ(cid:107)x − y(cid:107) correspondingly. Additionally  we can deﬁne approximate ﬁrst order stationary points as:
Deﬁnition 1 (-ﬁrst order stationary point). Let f : Rd → R be a differentiable function. Then
x ∈ Rd is a ﬁrst order stationary point of f if (cid:107)∇f (x)(cid:107) ≤ .
A ﬁrst order stationary point can be either a local minimum  a local maximum or a saddle point.
Following the terminology of [LPP+19] and [JGN+17]  we will include local maxima in saddle
points since they are both undesirable for our minimization task. Under this deﬁnition  strict saddle
points can be identiﬁed as follows:
Deﬁnition 2 (Strict saddle point). Let f : Rd → R be a twice differentiable function. Then x ∈ Rd
is a strict saddle point of f if (cid:107)∇f (x)(cid:107) = 0 and λmin(∇2f (x)) < 0.
To avoid convergence to strict saddle points  we need to converge to SOSPs . In order to study
the convergence rate of algorithms that converge to SOSPs   we need to deﬁne some notion of
approximate SOSPs . Following the convention of [JGN+17] we deﬁne the following:
Deﬁnition 3 (-SOSP ). Let f : Rd → R be a ρ-Hessian Lipschitz function. Then x ∈ Rd is an
-second order order stationary point of f if (cid:107)∇f (x)(cid:107) ≤  and λmin(∇2f (x)) ≥ −√
3.3 Gradient Approximation using Zero Order Information

ρ.

One of the key ways that enables zero order methods to converge quickly is using approximations of
the gradient based on ﬁnite differences approaches. Here we will show how forward differencing
can provide these approximate gradient calculations. Without much additional effort we can get the
same results for other ﬁnite differences approaches like backward and symmetric difference as well
as ﬁnite differences approaches with higher order accuracy guarantees. Let us deﬁne the gradient
approximation function based on forward difference rf : Rd × R → Rd

(cid:80)d



f (x + hel) − f (x)

el when h (cid:54)= 0

(1)

rf (x  h) =

l=0

h
∇f (x) if h = 0

This function takes two arguments: A vector x where the gradient should be approximated as well as
a scalar value h that controls the approximation accuracy of the estimator. An additional property
that will be of interest when we analyze approximate gradient descent is the fact that rf is Lipschitz.
Based on the deﬁnition one can show:
Lemma 1. Let f be (cid:96)-gradient Lipschitz. Then rf (·  h) as deﬁned in Equation 1 is
√
all h ∈ R and ∀h ∈ R  x ∈ Rd : (cid:107)rf (x  h) − ∇f (x)(cid:107) ≤ (cid:96)

d(cid:96) Lipschitz for

d|h|.

√

3.4 Black box reductions to ﬁrst order methods

As shown in the works of [NS17  GL13]  zero order optimization is reducible to stochastic ﬁrst
order optimization. The reduction relies on treating randomized approximate gradients of f as in
expectation exact gradients of a carefully constructed smoothed version of f. These arguments are
also applicable to our case as well. FPSG  one of the approaches of [JLGJ18]  naively leads to a large

4

√

poly(d) dependence in the convergence rate. More speciﬁcally one can show that [JLGJ18]’s FPSG
method needs ˜O(d3/4) evaluations of ∇g to converge to an -SOSP . The main reason behind this
dimension dependent slowdown is that the Hessian Lipschitz constant of the smoothed version of
g is O(ρ
d). An alternative approach in [JLGJ18] named ZPSG builds gradient estimators using
function evaluations directly. The main source slowdown here is the high variance of the stohastic
gradients. An analysis of those methods for the case where exact function evaluations are available
can be found in the Appendix.
In the next sections we will provide an alternative analysis that accounts for the gradient approximation
errors on the optimization of f directly. Thus  we will be able to sidestep the above issues and provide
faster convergence rates and better sample complexity.

4 Approximate Gradient Descent

4.1 Description

It is easy to see that conceptually any iterative optimization method can be expressed as a dynamical
system of the form {xk+1 = g(xk)} where xk is the current solution iterate that gets updated through
an update function g. Additionally  for ﬁrst order methods strict saddle points correspond to the
unstable ﬁxed points of the dynamical system. These key observations have motivated [LPP+19]
to use the Stable Manifold Theorem (SMT) [Shu87] in order to prove that gradient descent avoids
strict saddle points. Intuitively  SMT formalizes why convergence to unstable ﬁxed points is unlikely
starting from a local region around an unstable ﬁxed point. Adding the requirement that g is a global
diffeomorphism  [LPP+19] generalizes the conclusions of SMT to the whole space.
In order to prove similar guarantees for a zero order algorithm using approximate gradient evaluations 
we will need to construct a new dynamical system that is applicable to our zero order setting. The
state of our dynamical system χk consists of two parts: The current solution iterate xk that is a vector
in Rd and a scalar value h ∈ R that controls the quality of the gradient approximation. Speciﬁcally
we have

χk+1 = g0(χk) (cid:44)

(2)
where η  β ∈ R+ positive scalar parameters and functions qx : Rd × R → Rd and qh : R → R.
The function qx can be seen as the gradient approximation oracle used by the dynamical system as
described in Section 3.3. The function qh is responsible for controlling the accuracy of the gradient
approximation. As we shall see later  it is important that hk converges to 0 so that the stable points of
g0 are the same as in gradient descent.

βqh(hk)

hk+1

=

(cid:18)xk+1

(cid:19)

(cid:18)xk − ηqx(xk  hk)
(cid:19)

4.2 Avoiding Strict Saddle points

In this section we will provide sufﬁcient conditions that the parameters η  β must satisfy so that the
update rule of Equation 2 avoids convergence to strict saddle points. To do this we will need to
introduce some properties of g0.
Deﬁnition 4 ((L  B  c)-Well-behaved function). Let f : Rd → R ∈ C 2 be a (cid:96)-gradient Lipschitz
function. A function g0 of the form of Equation 2 is a (L  B  c)-well behaved function (for function f)
if it has the following properties: i) qx  qh ∈ C 1 with qh(0) = 0. ii) ∀h ∈ R : qx(·  h) is L Lipschitz
and 0 < ∂qh(h)

∂h ≤ B. iii) ∀(x  h) ∈ Rd+1 : (cid:107)qx(x  h) − ∇f (x)(cid:107) ≤ c|h|.

√

√

d(cid:96)  B = 1  c =

d(cid:96) using qx = rf and qh = h.

Given this deﬁnition and Lemma 1  it is clear that we can always construct (L  B  c)-well-behaved
functions for L =
In the following lemmas and theorems we will require that βB < 1. Under this assumption βqh is a
contraction having 0 as its only ﬁxed point so for all ﬁxed points of g0 we know that h = 0. Notice
also that when h = 0  we have qx(x  0) = ∇f (x) and therefore the x coordinates of ﬁxed points of
g0 must coincide with ﬁrst order stationary points of f. In fact  in the Appendix we prove that there
is a one to one mapping between strict saddles of f and unstable ﬁxed points of g0. Using the same
assumptions  we also get that det(Dg0(·)) (cid:54)= 0. Putting all together  we are able to prove our ﬁrst
main result.

5

Theorem 1. Let g0 be a (L  B  c)-well-behaved function for function f. Let X∗
saddle points of f. Then if η < 1

B : ∀h0 ∈ R : µ({x0 : limk→∞ xk ∈ X∗

L and β < 1

f}) = 0.

f be the set of strict

Notice that the random initialization refers only to the x0’s domain.
Indeed a straightforward
application of the result of [LPP+19] would guarantee a saddle-avoidance lemma only under an extra
random choice of h0. Such a result would not be able to clarify if saddle-avoidance stems from the
instability of the ﬁxed point  just like in ﬁrst order methods  or from the additional randomness of h0.
The key insight provided by the SMT is that the all the initialization points that eventually converge
to an unstable ﬁxed point lie in a low dimensional manifold. Thus  to obtain a stronger result we
have to understand how SMT restricts the dimensionality of this stable manifold for a ﬁxed h0. The
structure of the eigenvectors of the Jacobian of g0 around a ﬁxed point reveals that such an interesting
decoupling is ﬁnally achievable.

4.3 Convergence

2

(cid:0)(cid:107)∇f (xk)(cid:107)2 − (cid:107)εk(cid:107)2(cid:1) .

In the previous section we provided sufﬁcient conditions to avoid convergence to strict saddle points.
These results are meaningful however only if limk→∞ xk exists. Therefore  in this section we will
provide sufﬁcient conditions such that the dynamic system of g0 converges. Given that strict saddle
points are avoided  it is sufﬁcient to prove convergence to ﬁrst order stationary points. Let the error
of the gradient approximation be εk = qx(xk  hk) − ∇f (xk). Firstly we establish the zero order
analogue of the folklore lower bound for the decrease of the function:
Lemma 2 (Step-Convergence). Suppose that g0 is a (L  B  c)-well-behaved function for a (cid:96)-gradient
Lipschitz function f. If η ≤ 1
(cid:96) then we have that f (xk+1) ≤ f (xk) − η
Given this lemma we can prove convergence to ﬁrst order stationary points.
Theorem 2 (Convergence to ﬁrst order stationary points). Suppose that g0 is a (L  B  c)-well-
behaved function for a (cid:96)-gradient Lipschitz function f. Let η ≤ 1
B . Then if f is lower
bounded limk→∞(cid:107)∇f (xk)(cid:107) = 0.
The last theorem gives us a guarantee that the norm of the gradient is converging to zero but this
is not enough to prove convergence to a single stationary point if f has non isolated critical points.
In the Appendix  we prove that if the gradient approximation error decreases quickly enough then
convergence to a single stationary point is guaranteed for analytic functions. This allows us to
conclude our analysis with this ﬁnal theorem.
Theorem 3 (Convergence to minimizers). Let f : Rd → R ∈ C 2 be a (cid:96)-gradient Lipschitz function.
Let us also assume that f is analytic  has compact sub-level sets and all of its saddle points are strict.
Let g0 be a (L  B  c)-well-behaved function for f with η < min{ 1
B . If we pick a
L   1
random initialization point x0  then we have that for the xk iterates of g0
k→∞ xk = x∗) = 1

∀h0 ∈ R : Pr( lim

2(cid:96)} and β < 1−2η(cid:96)

(cid:96)   β < 1

where x∗ is a local minimizer of f.

5 Escaping Saddle Points Efﬁciently

5.1 Overview

In the previous subsections we provided sufﬁcient conditions for approximate gradient descent to
avoid strict saddle points. However  the stable manifold theorem guarantees that this will happen
asymptotically. In fact  convergence could be quite slow until we reach a neighborhood of a local
minimum. An analysis done for the ﬁrst order case by [DJL+17] showed that avoiding saddle points
could take exponential time in the worst case. In this section  we will use ideas from the work of
[JGN+17] in order to get a zero order algorithm that converges to SOSPs efﬁciently.
Convergence to SOSPs poses unique challenges to zero order methods when it comes to controlling
the gradient approximation accuracy. For convergence to ﬁrst order stationary points one can use
property iii) of Deﬁnition 4 and Lemma 2 to show that h = /c guarantees the decrease of f until
(cid:107)∇f (xk)(cid:107) ≤ . For SOSPs   this is not applicable as the norm of the gradient can become arbitrarily
small near saddle points. One could resort to iteratively trying smaller h to ﬁnd one that guarantees

6

the decrease of f. A surprising fact about our algorithm is that even if the gradient is arbitrarily small 
computationally burdensome searches for h can be totally avoided.

c

c

2

d

χ3 ·(cid:113) 3

ρ

5.2 Algorithm
Algorithm Initialization: ((cid:96)  ρ    c  δ  ∆f )
1: χ ← 3 max{log( d(cid:96)∆f
2:

c2δ )  4}  η ← c
ρ   S ← √

(cid:96)   r ← √
√
ρ   hlow ← 1

tthres ← χ
c2 ·

c
χ

(cid:96)√

ρ

ch

4ch

(cid:96)   gthres ← √
χ2 ·   fthres ← c
χ2 · 
min{gthres  rρδS
}
√
Algorithm 2 EscapeSaddle (ˆx)
1: ξ ∼ Unif(B0(r))
2: ˜x0 ← ˆx + ξ
3: for i = 0  1  . . . tthres do
4:
5:
6:
7:
8: end for
9: return ˆx

4 gthres then
xt+1 ← xt − ηzt
xt+1 ← EscapeSaddle (xt)
if xt+1 = xt then return xt

Algorithm 1 PAGD(x0)
1: for t = 0  1  . . . do
zt ← q(xt  gthres
2:
if (cid:107)zt(cid:107) ≥ 3
3:
4:
else
5:
6:
7:
8:
9: end for
Just like [JGN+17]  we will assume that f is (cid:96)−gradient Lipschitz and also ρ−Hessian Lipschitz. To
construct a zero order algorithm we will also need a gradient approximator q : Rd × R → Rd. We
will only require the error bound property on q  i.e.  there exists a constant ch such that

if f (ˆx) − f (˜xi) ≥ fthres then
end if
˜xi+1 ← ˜xi − ηq(˜xi  hlow)

return ˜xi

end if

)

∀x ∈ Rd  h ∈ R : (cid:107)q(x  h) − ∇f (x)(cid:107) ≤ ch|h|

The high level idea of Algorithm 1 is that given a point xt that is not an -SOSP the algorithm makes
progress by ﬁnding a xt+1 where f (xt+1) is substantially smaller than f (xt). By the deﬁnition of
-SOSPs either the gradient of f at xt is large or the Hessian has a substantially negative eigenvalue.
Separating these two cases is not as straightforward as in the ﬁrst order case. Given the norm of the
approximate gradient q(x  h)  we only know that (cid:107)∇f (x)(cid:107) ∈ (cid:107)q(x  h)(cid:107) ± ch|h|. In Algorithm 1
by choosing 3gthres/4 as the threshold to test for and h = gthres/(4ch)  we guarantee that in step 4
(cid:107)∇f (xt)(cid:107) ≥ gthres/2. This threshold is actually high enough to guarantee substantial decrease of f.
Indeed given that we have a lower bound on the exact gradient and using Lemma 2 we get

f (xt) − f (xt+1) ≥ η
2

(cid:0)(cid:107)∇f (xt)(cid:107)2 − (cid:107)εt(cid:107)2(cid:1) ≥ 3

32 ηg2

thres

where εt is the gradient approximation error at xt. This decrease is the same as in the ﬁrst order case
up to constants.
On the other hand  in Algorithm 2 we are guaranteed that (cid:107)∇f (ˆx)(cid:107) ≤ gthres. In this case our
approximate gradient cannot guarantee a substantial decrease of f. However  we know that the
Hessian has a substantially negative eigenvalue and therefore a direction of steep decrease of f
must exist. The problem is that we do not know which direction has this property. In [JGN+17]
it is proved that identifying this direction is not necessary for the ﬁrst order case. Adding a small
random perturbation to our current iterate (step 2) is enough so that with high probability we can get
a substantial decrease of f after at most tthres gradient descent steps (step 5). Of course this work is
not directly applicable to our case since we do not have access to exact gradients.
The work of [JGN+17] mainly depends on two arguments to provide its guarantees. The ﬁrst
argument is that if the ˜xi iterates do not achieve a decrease of fthres in tthres steps then they must
remain conﬁned in a small ball around ˜x0. Speciﬁcally for the exact gradient case we have that

(cid:107)˜xi − ˜x0(cid:107)2 ≤ 2ηfthrestthres.

The zero order case is deﬁnitely more challenging since each update in Algorithm 2 is not guaranteed
to decrease the value of f. Therefore  iterates may wander away from ˜x0 without even decreasing the
function value of f. To amend this argument for the zero order case we require that hlow ≤ gthres/ch.
This guarantees that even if gradient approximation errors amass over the iterations we will get the
same bound as the ﬁrst order case up to constants.

7

√

The second argument of [JGN+17] formalizes why the existence of a negative eigenvalue of the
Hessian is important. Let us run gradient descent starting from two points u0 and w0 such that
w0 − u0 = κe where e is the eigenvector corresponding to the most negative eigenvalue of the
Hessian and κ ≥ rδ/(2
d). Then at least one of the sequences {wi} {ui} is able to escape away
from its starting point in tthres iterations and by the ﬁrst argument it is also able to decrease the value of
f substantially. The proof of the claim is based on creating a recurrence relationship on vi = wi − ui.
The corresponding recurrence relationship for the zero order case is more complicated with additional
terms that correspond to the gradient approximation errors for wi and ui. However  we are able to
prove that if hlow ≤ rρδS/(2
d) then these additional terms cannot distort the exponential growth of
vi. Having extended both arguments of [JGN+17] we can establish the same guarantees for escaping
saddle points.
Theorem 4 (Analysis of PAGD). There exists absolute constant cmax such that: if f is (cid:96)-gradient
ρ   ∆f ≥ f (x0) − f (cid:63)  and constant
Lipschitz and ρ-Hessian Lipschitz  then for any δ > 0   ≤ (cid:96)2
c ≤ cmax  with probability 1 − δ  the output of PAGD(x0  (cid:96)  ρ    c  δ  ∆f ) will be an -SOSP   and
(cid:19)(cid:19)
have the following number of iterations until termination:

(cid:18) (cid:96)(f (x0) − f (cid:63))

(cid:18) d(cid:96)∆f

√

O

2

log4

2δ

6 Experiments

In this section we use simulations to verify our theoretical ﬁndings. Speciﬁcally we are interested in
verifying if zero order methods can avoid saddle points as efﬁciently as ﬁrst order methods. To do this
we use the two dimensional Rastrigin function  a popular benchmark in the non-convex optimization
literature. This function exhibits several strict saddle points so it will be an adequate benchmark for
our case. The two dimensional Rastrigin function can be deﬁned as

Ras(x1  x2) = 20 + x2

1 − 10 cos(2πx1) + x2

2 − 10 cos(2πx2).

For this experiment we selected 75 points randomly from [−1.5  1.5] × [−1  5  1.5]. In this domain
the Rastrigin function is (cid:96)-gradient Lipschitz with (cid:96) ≈ 63.33. Using these points as initialization
we run gradient descent and the approximate gradient descent dynamical system we introduced in
Section 4.2. For both gradient descent and approximate gradient descent we used η = 1/(4(cid:96)). Then
for approximate gradient descent we used symmetric differences to approximate the gradients and
β = 0.95 as well as h0 = 0.15. Figure 1 shows the contour plot of the Rastrigin function as well

Figure 1: Contour plots of the Rastrigin function along with the evolution of the iterates of gradient
descent and approximate gradient descent. Green points correspond to gradient descent whereas cyan
points correspond to approximate gradient descent.

as the evolution of the iterates of both methods. As expected  for points initialized closed to local
minima of the function convergence is quite fast. On the other hand  points starting close to saddle
points of the Rastrigin function take some more time to converge to minima. However  it is clear that
in both cases the behaviour of gradient descent and approximate gradient descent is similar in the
sense that for the same initialization there is no discrepancy in terms of convergence speed for the
two methods.
We also want to experimentally verify the performance of PAGD. To do this we use the octopus
function proposed by [DJL+17]. This function is is particularly relevant to our setting as it possesses

8

1.51.00.50.00.51.01.51.51.00.50.00.51.01.5Intial Points1.51.00.50.00.51.01.51.51.00.50.00.51.01.5Iteration 21.51.00.50.00.51.01.51.51.00.50.00.51.01.5Iteration 41.51.00.50.00.51.01.51.51.00.50.00.51.01.5Iteration 6051015202530354045a sequence of saddle points. The authors of [DJL+17] proved that for this function gradient descent
needs exponential time to avoid saddle points before converging to a local minimum. In contrast the
perturbed version of gradient descent (PGD) of [JGN+17] does not suffer from the same limitation.
Based on the results of Theorem 4  we expect PAGD to not have this limitation as well. We compare
gradient descent (GD)  PGD  AGD and PAGD on an octopus function of d = 15 dimensions. Figure 2
clearly shows that the zero order versions have the same iteration performance with the ﬁrst-order
ones. In fact  AGD is shown to behave even better than GD in this example thanks to the noise
induced by the gradient approximation.

Figure 2: Octopus function value varying the number of iterations. Parameters of the function τ = e 
L = e  γ = 1. Parameters of ﬁrst order methods taken from [DJL+17]. Zero order methods use
symmetric differencing with h = 0.01

7 Conclusion

This paper is the ﬁrst one to establish that zero order methods can avoid saddle points efﬁciently. To
achieve this we went beyond smoothing arguments used in prior work and studied the effect of the
gradient approximation error on ﬁrst order methods that converge to second order stationary points.
One important open question for future work is whether similar guarantees can be established for
other zero order methods used in practice like direct search methods and trust region methods using
linear models. Another generalization of interest would be to consider the performance of zero order
methods for instances of (non-convex) constrained optimization.

Acknowledgements

Georgios Piliouras acknowledges MOE AcRF Tier 2 Grant 2016-T2-1-170  grant PIE-SGP-AI-2018-
01 and NRF 2018 Fellowship NRF-NRFF2018-07. Emmanouil-Vasileios Vlatakis-Gkaragkounis
was supported by NSF CCF-1563155  NSF CCF-1814873  NSF CCF-1703925  NSF CCF-1763970.
We are grateful to Alexandros Potamianos for bringing this problem to our attention  and for helpful
discussions at an early stage of this project for its connection to Natural Language Processing tasks.
Finally  this work was supported by the Onassis Foundation - Scholarship ID: F ZN 010-1/2017-2018.

9

02004006008001000Iterations2000150010005000f(xk)GDPGDAGDPAGDReferences
[AAB+17] Naman Agarwal  Zeyuan Allen Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma.
Finding approximate local minima faster than gradient descent. In Proceedings of the
49th Annual ACM SIGACT Symposium on Theory of Computing  STOC 2017  Montreal 
QC  Canada  June 19-23  2017  pages 1195–1199  2017.

[ADX10] Alekh Agarwal  Ofer Dekel  and Lin Xiao. Optimal algorithms for online convex
optimization with multi-point bandit feedback. In COLT 2010 - The 23rd Conference
on Learning Theory  Haifa  Israel  June 27-29  2010  pages 28–40  2010.

[AL18] Zeyuan Allen-Zhu and Yuanzhi Li. NEON2: ﬁnding local minima via ﬁrst-order
oracles. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018 
Montréal  Canada.  pages 3720–3730  2018.

[BG18] Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochas-
tic optimization via conditional gradient and gradient updates. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada.  pages
3459–3468  2018.

[CD16] Yair Carmon and John C. Duchi. Gradient descent efﬁciently ﬁnds the cubic-regularized

non-convex newton step. CoRR  abs/1612.00547  2016.

[CG19] Tianyi Chen and Georgios B. Giannakis. Bandit convex optimization for scalable and

dynamic iot management. IEEE Internet of Things Journal  6(1):1276–1286  2019.

[CHM+15] Anna Choromanska  Mikael Henaff  Michaël Mathieu  Gérard Ben Arous  and Yann
LeCun. The loss surfaces of multilayer networks. In Proceedings of the Eighteenth
International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2015  San
Diego  California  USA  May 9-12  2015  2015.

[CRS+18] Krzysztof Choromanski  Mark Rowland  Vikas Sindhwani  Richard E. Turner  and
Adrian Weller. Structured evolution with compact architectures for scalable policy
optimization. In Proceedings of the 35th International Conference on Machine Learning 
ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  pages 969–977 
2018.

[CSV09] Andrew R. Conn  Katya Scheinberg  and Luís N. Vicente. Global convergence of general
derivative-free trust-region algorithms to ﬁrst- and second-order critical points. SIAM
Journal on Optimization  20(1):387–415  2009.

[CZS+17] Pin-Yu Chen  Huan Zhang  Yash Sharma  Jinfeng Yi  and Cho-Jui Hsieh. ZOO: zeroth
order optimization based black-box attacks to deep neural networks without training
substitute models. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence
and Security  AISec@CCS 2017  Dallas  TX  USA  November 3  2017  pages 15–26 
2017.

[DJL+17] Simon S. Du  Chi Jin  Jason D. Lee  Michael I. Jordan  Aarti Singh  and Barnabás Póczos.
Gradient descent can take exponential time to escape saddle points. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017  4-9 December 2017  Long Beach  CA  USA  pages 1067–1077 
2017.

[DJWW15] John C. Duchi  Michael I. Jordan  Martin J. Wainwright  and Andre Wibisono. Optimal
rates for zero-order convex optimization: The power of two function evaluations. IEEE
Trans. Information Theory  61(5):2788–2806  2015.

[DPG+14] Yann N. Dauphin  Razvan Pascanu  Çaglar Gülçehre  KyungHyun Cho  Surya Gan-
guli  and Yoshua Bengio. Identifying and attacking the saddle point problem in high-
dimensional non-convex optimization. In Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Information Processing Systems 2014  De-
cember 8-13 2014  Montreal  Quebec  Canada  pages 2933–2941  2014.

10

[GHH16] Bin Gu  Zhouyuan Huo  and Heng Huang. Zeroth-order asynchronous doubly stochastic

algorithm with variance reduction. arXiv preprint arXiv:1612.01425  2016.

[GHJY15] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points - online
stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on
Learning Theory  COLT 2015  Paris  France  July 3-6  2015  pages 797–842  2015.

[GL13] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst- and zeroth-order methods for
nonconvex stochastic programming. SIAM Journal on Optimization  23(4):2341–2368 
2013.

[HZ18] Davood Hajinezhad and Michael M. Zavlanos. Gradient-free multi-agent nonconvex
nonsmooth optimization. In 57th IEEE Conference on Decision and Control  CDC 2018 
Miami  FL  USA  December 17-19  2018  pages 4939–4944  2018.

[JGN+17] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M. Kakade  and Michael I. Jordan. How
to escape saddle points efﬁciently. In Proceedings of the 34th International Conference
on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages
1724–1732  2017.

[JLGJ18] Chi Jin  Lydia T. Liu  Rong Ge  and Michael I. Jordan. On the local minima of the
empirical risk. In Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December
2018  Montréal  Canada.  pages 4901–4910  2018.

[JNJ18] Chi Jin  Praneeth Netrapalli  and Michael I. Jordan. Accelerated gradient descent escapes
saddle points faster than gradient descent. In Sébastien Bubeck  Vianney Perchet  and
Philippe Rigollet  editors  Proceedings of the 31st Conference On Learning Theory 
volume 75 of Proceedings of Machine Learning Research  pages 1042–1085. PMLR 
06–09 Jul 2018.

[LCCH18] Sijia Liu  Jie Chen  Pin-Yu Chen  and Alfred Hero. Zeroth-order online alternating
direction method of multipliers: Convergence analysis and applications. In International
Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2018  9-11 April 2018 
Playa Blanca  Lanzarote  Canary Islands  Spain  pages 288–297  2018.

[Lev16] Kﬁr Y. Levy. The power of normalization: Faster evasion of saddle points. CoRR 

abs/1611.04831  2016.

[LKC+18] Sijia Liu  Bhavya Kailkhura  Pin-Yu Chen  Pai-Shun Ting  Shiyu Chang  and Lisa Amini.
Zeroth-order stochastic variance reduction for nonconvex optimization. In Advances in
Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada.  pages
3731–3741  2018.

[LPP+19] Jason D. Lee  Ioannis Panageas  Georgios Piliouras  Max Simchowitz  Michael I. Jordan 
and Benjamin Recht. First-order methods almost always avoid strict saddle points. Math.
Program.  176(1-2):311–337  2019.

[LZH+16] Xiangru Lian  Huan Zhang  Cho-Jui Hsieh  Yijun Huang  and Ji Liu. A comprehensive
linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-
order to ﬁrst-order. In Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems 2016  December 5-10  2016 
Barcelona  Spain  pages 3054–3062  2016.

[MK87] Katta G. Murty and Santosh N. Kabadi. Some np-complete problems in quadratic and

nonlinear programming. Math. Program.  39(2):117–129  1987.

[MMS+18] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International
Conference on Learning Representations  ICLR 2018  Vancouver  BC  Canada  April 30
- May 3  2018  Conference Track Proceedings  2018.

11

[NP06] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its

global performance. Math. Program.  108(1):177–205  2006.

[NS17] Yurii Nesterov and Vladimir G. Spokoiny. Random gradient-free minimization of convex

functions. Foundations of Computational Mathematics  17(2):527–566  2017.

[PMG+17] Nicolas Papernot  Patrick D. McDaniel  Ian J. Goodfellow  Somesh Jha  Z. Berkay
Celik  and Ananthram Swami. Practical black-box attacks against machine learning. In
Proceedings of the 2017 ACM on Asia Conference on Computer and Communications
Security  AsiaCCS 2017  Abu Dhabi  United Arab Emirates  April 2-6  2017  pages
506–519  2017.

[RK16] Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method 

volume 10. John Wiley & Sons  2016.

[SHCS17] Tim Salimans  Jonathan Ho  Xi Chen  and Ilya Sutskever. Evolution strategies as a

scalable alternative to reinforcement learning. CoRR  abs/1703.03864  2017.

[Shu87] Michael Shub. Global stability of dynamical systems. Springer Science & Business

Media  1987.

[SLA12] Jasper Snoek  Hugo Larochelle  and Ryan P. Adams. Practical bayesian optimization of
machine learning algorithms. In Advances in Neural Information Processing Systems 25:
26th Annual Conference on Neural Information Processing Systems 2012. Proceedings
of a meeting held December 3-6  2012  Lake Tahoe  Nevada  United States.  pages
2960–2968  2012.

[Spa03] James C. Spall. Introduction to Stochastic Search and Optimization. John Wiley & Sons 

Inc.  New York  NY  USA  1 edition  2003.

[SQW18] Ju Sun  Qing Qu  and John Wright. A geometric analysis of phase retrieval. Foundations

of Computational Mathematics  18(5):1131–1198  2018.

[WDBS18] Yining Wang  Simon S. Du  Sivaraman Balakrishnan  and Aarti Singh. Stochastic
zeroth-order optimization in high dimensions. In International Conference on Artiﬁcial
Intelligence and Statistics  AISTATS 2018  9-11 April 2018  Playa Blanca  Lanzarote 
Canary Islands  Spain  pages 1356–1365  2018.

[WJ08] Martin J. Wainwright and Michael I. Jordan. Graphical models  exponential families 
and variational inference. Foundations and Trends in Machine Learning  1(1-2):1–305 
2008.

12

,Qiang Liu
Dilin Wang
Emmanouil-Vasileios Vlatakis-Gkaragkounis
Lampros Flokas
Georgios Piliouras