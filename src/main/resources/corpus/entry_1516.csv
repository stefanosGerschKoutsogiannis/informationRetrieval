2007,Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation,When training and test samples follow different input distributions (i.e.  the situation called \emph{covariate shift})  the maximum likelihood estimator is known to lose its consistency. For regaining consistency  the log-likelihood terms need to be weighted according to the \emph{importance} (i.e.  the ratio of test and training input densities). Thus  accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However  since density estimation is a hard problem  this approach tends to perform poorly especially in high dimensional cases. In this paper  we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.,Direct Importance Estimation with Model Selection
and Its Application to Covariate Shift Adaptation

Masashi Sugiyama

Tokyo Institute of Technology
sugi@cs.titech.ac.jp

Shinichi Nakajima
Nikon Corporation

nakajima.s@nikon.co.jp

Hisashi Kashima

IBM Research

Paul von B¨unau

Technical University Berlin

Motoaki Kawanabe
Fraunhofer FIRST

hkashima@jp.ibm.com

buenau@cs.tu-berlin.de

nabe@first.fhg.de

Abstract

A situation where training and test samples follow different input distributions is
called covariate shift. Under covariate shift  standard learning methods such as
maximum likelihood estimation are no longer consistent—weighted variants ac-
cording to the ratio of test and training input densities are consistent. Therefore 
accurately estimating the density ratio  called the importance  is one of the key is-
sues in covariate shift adaptation. A naive approach to this task is to ﬁrst estimate
training and test input densities separately and then estimate the importance by
taking the ratio of the estimated densities. However  this naive approach tends to
perform poorly since density estimation is a hard task particularly in high dimen-
sional cases. In this paper  we propose a direct importance estimation method that
does not involve density estimation. Our method is equipped with a natural cross
validation procedure and hence tuning parameters such as the kernel width can be
objectively optimized. Simulations illustrate the usefulness of our approach.

1 Introduction

A common assumption in supervised learning is that training and test samples follow the same
distribution. However  this basic assumption is often violated in practice and then standard machine
learning methods do not work as desired. A situation where the input distribution P (x) is different
in the training and test phases but the conditional distribution of output values  P (yjx)  remains
unchanged is called covariate shift [8]. In many real-world applications such as robot control [10] 
bioinformatics [1]  spam ﬁltering [3]  brain-computer interfacing [9]  or econometrics [5]  covariate
shift is conceivable and thus learning under covariate shift is gathering a lot of attention these days.

The inﬂuence of covariate shift could be alleviated by weighting the log likelihood terms according
to the importance [8]: w(x) = pte(x)=ptr(x)  where pte(x) and ptr(x) are test and training input
densities. Since the importance is usually unknown  the key issue of covariate shift adaptation is
how to accurately estimate the importance.

A naive approach to importance estimation would be to ﬁrst estimate the training and test densities
separately from training and test input samples  and then estimate the importance by taking the ratio
of the estimated densities. However  density estimation is known to be a hard problem particularly
in high-dimensional cases. Therefore  this naive approach may not be effective—directly estimating
the importance without estimating the densities would be more promising.

Following this spirit  the kernel mean matching (KMM) method has been proposed recently [6] 
which directly gives importance estimates without going through density estimation. KMM is shown

1

to work well  given that tuning parameters such as the kernel width are chosen appropriately. In-
tuitively  model selection of importance estimation algorithms (such as KMM) is straightforward
by cross validation (CV) over the performance of subsequent learning algorithms. However  this is
highly unreliable since the ordinary CV score is heavily biased under covariate shift—for unbiased
estimation of the prediction performance of subsequent learning algorithms  the CV procedure itself
needs to be importance-weighted [9]. Since the importance weight has to have been ﬁxed when
model selection is carried out by importance weighted CV  it can not be used for model selection of
importance estimation algorithms.

The above fact implies that model selection of importance estimation algorithms should be per-
formed within the importance estimation step in an unsupervised manner. However  since KMM
can only estimate the values of the importance at training input points  it can not be directly applied
in the CV framework; an out-of-sample extension is needed  but this seems to be an open research
issue currently.

In this paper  we propose a new importance estimation method which can overcome the above
problems  i.e.  the proposed method directly estimates the importance without density estimation
and is equipped with a natural model selection procedure. Our basic idea is to ﬁnd an importance

estimate bw(x) such that the Kullback-Leibler divergence from the true test input density pte(x)
to its estimate bpte(x) = bw(x)ptr(x) is minimized. We propose an algorithm that can carry out

this minimization without explicitly modeling ptr(x) and pte(x). We call the proposed method the
Kullback-Leibler Importance Estimation Procedure (KLIEP). The optimization problem involved in
KLIEP is convex  so the unique global solution can be obtained. Furthermore  the solution tends to
be sparse  which contributes to reducing the computational cost in the test phase.

Since KLIEP is based on the minimization of the Kullback-Leibler divergence  its model selection
can be naturally carried out through a variant of likelihood CV  which is a standard model selection
technique in density estimation. A key advantage of our CV procedure is that  not the training
samples  but the test input samples are cross-validated. This highly contributes to improving the
model selection accuracy since the number of training samples is typically limited while test input
samples are abundantly available.

The simulation studies show that KLIEP tends to outperform existing approaches in importance
estimation including the logistic regression based method [2]  and it contributes to improving the
prediction performance in covariate shift scenarios.

2 New Importance Estimation Method

In this section  we propose a new importance estimation method.

2.1 Formulation and Notation

i gntr
Let D (cid:26) (Rd) be the input domain and suppose we are given i.i.d. training input samples fxtr
i=1
j=1 from a
from a training input distribution with density ptr(x) and i.i.d. test input samples fxte
test input distribution with density pte(x). We assume that ptr(x) > 0 for all x 2 D. Typically 
the number ntr of training samples is rather small  while the number nte of test input samples is
very large. The goal of this paper is to develop a method of estimating the importance w(x) from
fxtr

i=1 and fxte

j gnte
j=1:

j gnte

i gntr

w(x) =

pte(x)
ptr(x)

:

Our key restriction is that we avoid estimating densities pte(x) and ptr(x) when estimating the
importance w(x).

2.2 Kullback-Leibler Importance Estimation Procedure (KLIEP)

Let us model the importance w(x) by the following linear model:

(cid:11)‘’‘(x);

(1)

bw(x) =

bX‘=1

2

where f(cid:11)‘gb
such that

‘=1 are parameters to be learned from data samples and f’‘(x)gb

‘=1 are basis functions

i=1 and fxte
j=1  i.e.  kernel
‘=1 are chosen in Section 2.3.

j gnte

‘=1 in the model (1) so that the Kullback-Leibler divergence from

’‘(x) (cid:21) 0 for all x 2 D and for ‘ = 1; 2; : : : ; b:
i gntr
‘=1 could be dependent on the samples fxtr
Note that b and f’‘(x)gb
models are also allowed—we explain how the basis functions f’‘(x)gb

Using the model bw(x)  we can estimate the test input density pte(x) by
pte(x) tobpte(x) is minimized:

bpte(x) = bw(x)ptr(x):

We determine the parameters f(cid:11)‘gb

pte(x) log

pte(x)

dx

KL[pte(x)kbpte(x)] =ZD
=ZD

pte(x) log

bw(x)ptr(x)
dx (cid:0)ZD

pte(x)
ptr(x)

Since the ﬁrst term in the last equation is independent of f(cid:11)‘gb
second term. We denote it by J:

J =ZD

1
nte

(cid:25)

pte(x) logbw(x)dx
nteXj=1
logbw(xte

j ) =

1
nte

log bX‘=1

nteXj=1

(cid:11)‘’‘(xte

j )! ;
j gnte

pte(x) logbw(x)dx:

‘=1  we ignore it and focus on the

(2)

j gnte

j=1 is used from the ﬁrst
where the empirical approximation based on the test input samples fxte
line to the second line above. This is our objective function to be maximized with respect to the
parameters f(cid:11)‘gb
‘=1  which is concave [4]. Note that the above objective function only involves the
test input samples fxte
i=1 yet. As shown
i gntr
below  fxtr

j=1  i.e.  we did not use the training input samples fxtr

i=1 will be used in the constraint.

bw(x) is an estimate of the importance w(x) which is non-negative by deﬁnition. Therefore  it is
natural to impose bw(x) (cid:21) 0 for all x 2 D  which can be achieved by restricting
In addition to the non-negativity  bw(x) should be properly normalized sincebpte(x) (= bw(x)ptr(x))

(cid:11)‘ (cid:21) 0 for ‘ = 1; 2; : : : ; b:

is a probability density function:

i gntr

(3)

1 =ZDbpte(x)dx =ZD bw(x)ptr(x)dx
bX‘=1

ntrXi=1

ntrXi=1 bw(xtr

1
ntr

1
ntr

i ) =

(cid:25)

(cid:11)‘’‘(xtr

i );

where the empirical approximation based on the training input samples fxtr
ﬁrst line to the second line above.

i gntr

i=1 is used from the

Now our optimization criterion is summarized as follows.

(cid:11)‘’‘(xte

j )!35

nteXj=1

maximize
f(cid:11)‘gb

‘=1 24

log bX‘=1
ntrXi=1
bX‘=1
Figure 1-(a). Note that the solution fb(cid:11)‘gb

subject to

This is a convex optimization problem and the global solution can be obtained  e.g.  by simply
performing gradient ascent and feasibility satisfaction iteratively. A pseudo code is described in
‘=1 tends to be sparse [4]  which contributes to reducing the
computational cost in the test phase. We refer to the above method as Kullback-Leibler Importance
Estimation Procedure (KLIEP).

(cid:11)‘’‘(xtr

i ) = ntr and (cid:11)1; (cid:11)2; : : : ; (cid:11)b (cid:21) 0:

3

i=1  and fxte

j gnte
j=1

i gntr

‘=1  fxtr

Input: m = f’‘(x)gb
Output: bw(x)
Aj;‘ (cid:0) ’‘(xte
j );
Pntr
i=1 ’‘(xtr
b‘ (cid:0) 1
ntr
Initialize (cid:11) (> 0) and " (0 < " (cid:28) 1);
Repeat until convergence

i );

(cid:11) (cid:0) (cid:11) + "A>(1:=A(cid:11));
(cid:11) (cid:0) (cid:11) + (1 (cid:0) b>(cid:11))b=(b>b);
(cid:11) (cid:0) max(0; (cid:11));
(cid:11) (cid:0) (cid:11)=(b>(cid:11));

end

bw(x) (cid:0) Pb

‘=1 (cid:11)‘’‘(x);

(x)gb(k)

‘=1 g 

Input: M = fmk j mk = f’(k)
j gnte
j=1

i=1  and fxte

‘

i gntr
fxtr
Output: bw(x)
j gnte
Split fxte
for each model m 2 M

j=1 into R disjoint subsets fX te

r gR

r=1;

j gj6=r);

i gntr
i=1; fX te
log bwr(x);

for each split r = 1; : : : ; R

bwr(x) (cid:0) KLIEP(m; fxtr
r j Px2X te
bJr(m) (cid:0) 1
R PR

r=1 bJr(m);

end
bJ(m) (cid:0) 1

jX te

r

end
bm (cid:0) argmaxm2M bJ(m);
bw(x) (cid:0) KLIEP(bm; fxtr

i gntr

i=1; fxte

j gnte

j=1);

(a) KLIEP main code

(b) KLIEP with model selection

Figure 1: KLIEP algorithm in pseudo code. ‘./’ indicates the element-wise division and > denotes
the transpose. Inequalities and the ‘max’ operation for a vector are applied element-wise.

2.3 Model Selection by Likelihood Cross Validation

The performance of KLIEP depends on the choice of basis functions f’‘(x)gb
how they can be appropriately chosen from data samples.

‘=1. Here we explain

r

bJr =

j gnte

Since KLIEP is based on the maximization of the score J (see Eq.(2))  it would be natural to select
the model such that J is maximized. The expectation over pte(x) involved in J can be numer-
ically approximated by likelihood cross validation (LCV) as follows: First  divide the test sam-
ples fxte
fX te

j gj6=r and approximate the score J using X te

j=1 into R disjoint subsets fX te

r=1. Then obtain an importance estimate bwr(x) from
r j Xx2X te

1
jX te

r gR

r as

1
R

bJ =

We repeat this procedure for r = 1; 2; : : : ; R  compute the average of bJr over all r  and use the
average bJ as an estimate of J:
For model selection  we compute bJ for all model candidates (the basis functions f’‘(x)gb
the current setting) and choose the one that minimizes bJ. A pseudo code of the LCV procedure is

One of the potential limitations of CV in general is that it is not reliable in small sample cases
since data splitting by CV further reduces the sample size. On the other hand  in our CV procedure 
the data splitting is performed over the test input samples  not over the training samples. Since we
typically have a large number of test input samples  our CV procedure does not suffer from the small
sample problem.

summarized in Figure 1-(b)

‘=1 in

(4)

logbwr(x):
RXr=1 bJr:

A good model may be chosen by the above CV procedure  given that a set of promising model
candidates is prepared. As model candidates  we propose using a Gaussian kernel model centered at
the test input points fxte

j=1  i.e. 

j gnte

where K(cid:27)(x; x0) is the Gaussian kernel with kernel width (cid:27):

nteX‘=1

‘ );

(cid:11)‘K(cid:27)(x; xte

bw(x) =
K(cid:27)(x; x0) = exp(cid:26)(cid:0)kx (cid:0) x0k2

2(cid:27)2

4

(cid:27) :

(5)

j gnte

i gntr

j=1 as the Gaussian centers  not the training
The reason why we chose the test input points fxte
input points fxtr
i=1  is as follows. By deﬁnition  the importance w(x) tends to take large values
if the training input density ptr(x) is small and the test input density pte(x) is large; conversely 
w(x) tends to be small (i.e.  close to zero) if ptr(x) is large and pte(x) is small. When a function
is approximated by a Gaussian kernel model  many kernels may be needed in the region where the
output of the target function is large; on the other hand  only a small number of kernels would be
enough in the region where the output of the target function is close to zero. Following this heuristic 
we decided to allocate many kernels at high test input density regions  which can be achieved by
setting the Gaussian centers at the test input points fxte
Alternatively  we may locate (ntr +nte) Gaussian kernels at both fxtr
j=1. However 
in our preliminary experiments  this did not further improve the performance  but slightly increased
j gnte
the computational cost. Since nte is typically very large  just using all the test input points fxte
j=1
as Gaussian centers is already computationally rather demanding. To ease this problem  we practi-
j=1 as Gaussian centers for computational efﬁciency  i.e. 
cally propose using a subset of fxte

i=1 and fxte

j gnte
j=1.

i gntr

j gnte

j gnte
bw(x) =

bX‘=1

(cid:11)‘K(cid:27)(x; c‘);

(6)

j gnte
where c‘ is a template point randomly chosen from fxte
In the rest of this paper  we ﬁx the number of template points at

j=1 and b ((cid:20) nte) is a preﬁxed number.

and optimize the kernel width (cid:27) by the above CV procedure.

b = min(100; nte);

3 Experiments

In this section  we compare the experimental performance of KLIEP and existing approaches.

3.1

Importance Estimation for Artiﬁcial Data Sets

Let ptr(x) be the d-dimensional Gaussian density with mean (0; 0; : : : ; 0)> and covariance identity
and pte(x) be the d-dimensional Gaussian density with mean (1; 0; : : : ; 0)> and covariance identity.
The task is to estimate the importance at training input points:

wi = w(xtr

i ) =

pte(xtr
i )
ptr(xtr
i )

for i = 1; 2; : : : ; ntr:

We compare the following methods:

KLIEP((cid:27)): fwigntr

i=1 are estimated by KLIEP with the Gaussian kernel model (6). Since the per-
formance of KLIEP is dependent on the kernel width (cid:27)  we test several different values of
(cid:27).

KLIEP(CV): The kernel width (cid:27) in KLIEP is chosen based on 5-fold LCV (see Section 2.3).
KDE(CV): fwigntr

i=1 are estimated through the kernel density estimator (KDE) with the Gaussian
kernel. The kernel widths for the training and test densities are chosen separately based on
5-fold likelihood cross-validation.

KMM((cid:27)): fwigntr

i=1 are estimated by kernel mean matching (KMM) [6]. The performance of KMM
is dependent on tuning parameters such as B  (cid:15)  and (cid:27). We set B = 1000 and (cid:15) = (pntr (cid:0)
1)=pntr following the paper [6]  and test several different values of (cid:27). We used the CPLEX
software for solving quadratic programs in the experiments.

LogReg((cid:27)): Importance weights are estimated by logistic regression (LogReg) [2]. The Gaussian
kernels are used as basis functions. Since the performance of LogReg is dependent on the
kernel width (cid:27)  we test several different values of (cid:27). We used the LIBLINEAR implemen-
tation of logistic regression for the experiments [7].

LogReg(CV): The kernel width (cid:27) in LogReg is chosen based on 5-fold CV.

5

of the importance estimates fbwigntr

NMSE =

1
ntr

ntrXi=1(cid:18)

bwiPntr
i0=1 bwi0 (cid:0)

i0=1 wi0(cid:19)2
wiPntr

:

l

)
e
a
c
S
g
o
L

 

 

l

n
i
(
 
s
a
i
r
T
0
0
1

 

 

 
r
e
v
o
E
S
M
N
e
g
a
r
e
v
A

 

10−3

10−4

10−5

10−6

2

4

6

10

8
d (Input Dimension)

12

KLIEP(0.5)
KLIEP(2)
KLIEP(7)
KLIEP(CV)
KDE(CV)
KMM(0.1)
KMM(1)
KMM(10)
LogReg(0.5)
LogReg(2)
LogReg(7)
LogReg(CV)

14

16

18

20

l

)
e
a
c
S
g
o
L

 

 

l

n
i
(
 
s
a
i
r
T
0
0
1

 

 

 
r
e
v
o
E
S
M
N
e
g
a
r
e
v
A

 

KLIEP(0.5)
KLIEP(2)
KLIEP(7)
KLIEP(CV)
KDE(CV)
KMM(0.1)
KMM(1)
KMM(10)
LogReg(0.5)
LogReg(2)
LogReg(7)
LogReg(CV)

10−3

10−4

10−5

10−6

50

100

n
 (Number of Training Samples)
tr

150

(a) When input dimension is changed

(b) When training sample size is changed

Figure 2: NMSEs averaged over 100 trials in log scale.

We ﬁxed the number of test input points at nte = 1000 and consider the following two settings for
the number ntr of training samples and the input dimension d:

(a) ntr = 100 and d = 1; 2; : : : ; 20 
(b) d = 10 and ntr = 50; 60; : : : ; 150.

We run the experiments 100 times for each d  each ntr  and each method  and evaluate the quality

i=1 by the normalized mean squared error (NMSE):

NMSEs averaged over 100 trials are plotted in log scale in Figure 2. Figure 2(a) shows that the error
of KDE(CV) sharply increases as the input dimension grows  while KLIEP  KMM  and LogReg
with appropriate kernel widths tend to give smaller errors than KDE(CV). This would be the fruit
of directly estimating the importance without going through density estimation. The graph also
show that the performance of KLIEP  KMM  and LogReg is dependent on the kernel width (cid:27)—the
results of KLIEP(CV) and LogReg(CV) show that model selection is carried out reasonably well
and KLIEP(CV) works signiﬁcantly better than LogReg(CV).

Figure 2(b) shows that the errors of all methods tend to decrease as the number of training samples
grows. Again  KLIEP  KMM  and LogReg with appropriate kernel widths tend to give smaller
errors than KDE(CV). Model selection in KLIEP(CV) and LogReg(CV) works reasonably well and
KLIEP(CV) tends to give signiﬁcantly smaller errors than LogReg(CV).

Overall  KLIEP(CV) is shown to be a useful method in importance estimation.

3.2 Covariate Shift Adaptation with Regression and Classiﬁcation Benchmark Data Sets

k=1 into [0; 1]d and choose the test samples f(xte

Here we employ importance estimation methods for covariate shift adaptation in regression and
classiﬁcation benchmark problems (see Table 1).
k=1. We normalize all the input samples
Each data set consists of input/output samples f(xk; yk)gn
j )gnte
k=1 as
j ; yte
fxkgn
follows. We randomly choose one sample (xk; yk) from the pool and accept this with probabil-
ity min(1; 4(x(c)
is the c-th element of xk and c is randomly determined and ﬁxed
in each trial of experiments; then we remove xk from the pool regardless of its rejection or ac-
ceptance  and repeat this procedure until we accept nte samples. We choose the training samples
i=1 uniformly from the rest. Intuitively  in this experiment  the test input density tends
f(xtr

j=1 from the pool f(xk; yk)gn

i )gntr

k )2)  where x(c)

k

i ; ytr

6

to be lower than the training input density when x(c)
ntr = 100 and nte = 500 for all data sets. Note that we only use f(xtr
for training regressors or classiﬁers; the test output values fyte
generalization performance.

j gnte
We use the following kernel model for regression or classiﬁcation:

is small. We set the number of samples at
j gnte
j=1
j=1 are used only for evaluating the

i=1 and fxte

i )gntr

i ; ytr

k

where Kh(x; x0) is the Gaussian kernel (5) and m‘ is a template point randomly chosen from
j=1. We set the number of kernels at t = 50. We learn the parameter (cid:18) by importance-
fxte
weighted regularized least squares (IWRLS) [9]:

j gnte

tX‘=1

(cid:18)‘Kh(x; m‘);

bf (x; (cid:18)) =
" ntrXi=1 bw(xtr
The solutionb(cid:18)IW RLS is analytically given by
b(cid:18) = (K >cW K + (cid:21)I)(cid:0)1K>cW y;

b(cid:18)IW RLS (cid:17) argmin

i )(cid:16)bf (xtr

where I is the identity matrix and

i (cid:17)2
i ; (cid:18)) (cid:0) ytr

(cid:18)

+ (cid:21)k(cid:18)k2# :

(7)

The results are summarized in Table 1  where ‘Uniform’ denotes uniform weights  i.e.  no impor-
tance weight is used. The table shows that KLIEP(CV) compares favorably with Uniform  implying
that the importance weighted methods combined with KLIEP(CV) are useful for improving the pre-
diction performance under covariate shift. KLIEP(CV) works much better than KDE(CV); actually
KDE(CV) tends to be worse than Uniform  which may be due to high dimensionality. We tested
10 different values of the kernel width (cid:27) for KMM and described three representative results in the
table. KLIEP(CV) is slightly better than KMM with the best kernel width. Finally  LogReg(CV)
works reasonably well  but it sometimes performs poorly.

Overall  we conclude that the proposed KLIEP(CV) is a promising method for covariate shift adap-
tation.

4 Conclusions

In this paper  we addressed the problem of estimating the importance for covariate shift adaptation.
The proposed method  called KLIEP  does not involve density estimation so it is more advantageous
than a naive KDE-based approach particularly in high-dimensional problems. Compared with KMM

7

The kernel width h and the regularization parameter (cid:21) in IWRLS (7) are chosen by 5-fold importance
weighted CV (IWCV) [9]. We compute the IWCV score by

y = (y1; y2; : : : ; yntr )>;

Ki;‘ = Kh(xtr

i ; m‘);

1
jZ tr

cW = diag (bw1;bw2; : : : ;bwntr ) :
r bw(x)L(cid:16)bfr(x); y(cid:17) ;
r j X(x;y)2Z tr
L (by; y) =(cid:26)(by (cid:0) y)2
2 (1 (cid:0) signfbyyg)
L(cid:16)bf (xte
nteXj=1

j (cid:17) :

j ); yte

1
nte

1

(Regression) 
(Classiﬁcation).

where

We run the experiments 100 times for each data set and evaluate the mean test error:

Table 1: Mean test error averaged over 100 trials. The numbers in the brackets are the standard devi-
ation. All the error values are normalized so that the mean error by ‘Uniform’ (uniform weighting 
or equivalently no importance weighting) is one. For each data set  the best method and comparable
ones based on the Wilcoxon signed rank test at the signiﬁcance level 5% are described in bold face.
The upper half are regression data sets taken from DELVE and the lower half are classiﬁcation data
sets taken from IDA. ‘KMM((cid:27))’ denotes KMM with kernel width (cid:27).

Data

Dim Uniform

KLIEP
(CV)

KDE
(CV)

KMM
(0.01)

KMM
(0.3)

KMM

(1)

LogReg

(CV)

kin-8fh
1:00(0:34) 0:95(0:31) 1:22(0:52) 1:00(0:34) 1:12(0:37) 1:59(0:53) 1:30(0:40)
8
kin-8fm 8
1:00(0:39) 0:86(0:35) 1:12(0:57) 1:00(0:39) 0:98(0:46) 1:95(1:24) 1:29(0:58)
kin-8nh
8 1:00(0:26) 0:99(0:22) 1:09(0:20) 1:00(0:27) 1:04(0:17) 1:16(0:25) 1:06(0:17)
kin-8nm 8 1:00(0:30) 0:97(0:25) 1:14(0:26) 1:00(0:30) 1:09(0:23) 1:20(0:22) 1:13(0:25)
abalone
7 1:00(0:50) 0:94(0:67) 1:02(0:41) 1:01(0:51) 0:96(0:70) 0:93(0:39) 0:92(0:41)
image
18 1:00(0:51) 0:94(0:44) 0:98(0:45) 0:97(0:50) 0:97(0:45) 1:09(0:54) 0:99(0:48)
ringnorm 20
1:00(0:04) 0:99(0:06) 0:87(0:04) 1:00(0:04) 0:87(0:05) 0:87(0:05) 0:95(0:08)
twonorm 20
1:00(0:58) 0:91(0:52) 1:16(0:71) 0:99(0:50) 0:86(0:55) 0:99(0:70) 0:94(0:59)
waveform 21 1:00(0:45) 0:93(0:34) 1:05(0:47) 1:00(0:44) 0:93(0:32) 0:98(0:31) 0:95(0:34)
Average
1.06(0.37)

0.98(0.37)

0.94(0.35)

1.20(0.47)

1.00(0.38)

1.07(0.40)

1.00(0.36)

which also directly gives importance estimates  KLIEP is practically more useful since it is equipped
with a model selection procedure. Our experiments highlighted these advantages and therefore
KLIEP is shown to be a promising method for covariate shift adaptation.

In KLIEP  we modeled the importance function by a linear (or kernel) model  which resulted in a
convex optimization problem with a sparse solution. However  our framework allows the use of any
models. An interesting future direction to pursue would be to search for a class of models which has
additional advantages.

Finally  the range of application of importance weights is not limited to covariate shift adaptation.
For example  the density ratio could be used for novelty detection. Exploring possible application
areas will be important future directions.

Acknowledgments

This work was supported by MEXT (17700142 and 18300057)  the Okawa Foundation  the Mi-
crosoft CORE3 Project  and the IBM Faculty Award.

References
[1] P. Baldi and S. Brunak. Bioinformatics: The Machine Learning Approach. MIT Press  Cambridge  1998.
[2] S. Bickel  M. Br¨uckner  and T. Scheffer. Discriminative learning for differing training and test distribu-

tions. In Proceedings of the 24th International Conference on Machine Learning  2007.

[3] S. Bickel and T. Scheffer. Dirichlet-enhanced spam ﬁltering based on biased samples. In B. Sch¨olkopf 
J. Platt  and T. Hoffman  editors  Advances in Neural Information Processing Systems 19. MIT Press 
Cambridge  MA  2007.

[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  Cambridge  2004.
[5] J. J. Heckman. Sample selection bias as a speciﬁcation error. Econometrica  47(1):153–162  1979.
[6] J. Huang  A. Smola  A. Gretton  K. M. Borgwardt  and B. Sch¨olkopf. Correcting sample selection bias
by unlabeled data. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information
Processing Systems 19  pages 601–608. MIT Press  Cambridge  MA  2007.

[7] C.-J. Lin  R. C. Weng  and S. S. Keerthi. Trust region Newton method for large-scale logistic regression.

Technical report  Department of Computer Science  National Taiwan University  2007.

[8] H. Shimodaira.

Improving predictive inference under covariate shift by weighting the log-likelihood

function. Journal of Statistical Planning and Inference  90(2):227–244  2000.

[9] M. Sugiyama  M. Krauledat  and K.-R. M¨uller. Covariate shift adaptation by importance weighted cross

validation. Journal of Machine Learning Research  8:985–1005  May 2007.

[10] R. S. Sutton and G. A. Barto. Reinforcement Learning: An Introduction. MIT Press  Cambridge  MA 

1998.

8

,Margareta Ackerman
Sanjoy Dasgupta