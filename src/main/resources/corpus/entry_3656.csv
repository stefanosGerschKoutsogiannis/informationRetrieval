2010,Multivariate Dyadic Regression Trees for Sparse Learning Problems,We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs).  Unlike traditional dyadic decision trees (DDTs) or classification and regression trees (CARTs)  MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty.  Theoretically  we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions  and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of $(\alpha  C)$-smooth functions. Empirically  MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable  for large-scale learning problems  we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets.,Multivariate Dyadic Regression Trees for Sparse

Learning Problems

School of Computer Science  Carnegie Mellon University

Han Liu and Xi Chen

Pittsburgh  PA 15213

Abstract

We propose a new nonparametric learning method based on multivariate dyadic
regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or
classiﬁcation and regression trees (CARTs)  MDRTs are constructed using penal-
ized empirical risk minimization with a novel sparsity-inducing penalty. Theoret-
ically  we show that MDRTs can simultaneously adapt to the unknown sparsity
and smoothness of the true regression functions  and achieve the nearly optimal
rates of convergence (in a minimax sense) for the class of (α  C)-smooth func-
tions. Empirically  MDRTs can simultaneously conduct function estimation and
variable selection in high dimensions. To make MDRTs applicable for large-scale
learning problems  we propose a greedy heuristics. The superior performance of
MDRTs are demonstrated on both synthetic and real datasets.

1 Introduction

}

{

j   . . .   yn

j )T and kth dimension of x by xk = (x1

k  . . .   xn

(x1  y1)  . . .   (xn  yn)

Many application problems need to simultaneously predict several quantities using a common set
of variables  e.g. predicting multi-channel signals within a time frame  predicting concentrations of
several chemical constitutes using the mass spectra of a sample  or predicting expression levels of
many genes using a common set of phenotype variables. These problems can be naturally formulated
in terms of multivariate regression.
In particular  let
be n independent and identically distributed pairs of data
with xi ∈ X ⊂ Rd and yi ∈ Y ⊂ Rp for i = 1  . . .   n. Moreover  we denote the jth dimension of y
k)T . Without loss of generality 
by yj = (y1
we assume X = [0  1]d and the true model on yj is :
j = fj(xi) + ϵi
yi
(1)
where fj : Rd → R is a smooth function. In the sequel  let f = (f1  . . .   fp)  where f : Rd → Rp is
a p-valued smooth function. The vector form of (1) then becomes yi = f(xi)+ϵi  i = 1  . . .   n. We
also assume that the noise terms
are independently distributed and bounded almost surely.
This is a general setting of the nonparametric multivariate regression. From the minimax theory  we
know that estimating f in high dimensions is very challenging. For example  when f1  . . .   fp lie in
a d-dimensional Sobolev ball with order α and radius C  the best convergence rate for the minimax
risk is p · n
However  in many real world applications  the true regression function f may depend only on a
small set of variables. In other words  the problem is jointly sparse:
where xS = (xk : k ∈ S)  S ⊂ {1  . . .   d} is a subset of covariates with size r = |S| ≪ d. If
S has been given  the minimax lower bound can be improved to be p · n
−2(cid:11)=(2(cid:11)+r)  which is the
best possible rate can be expected. For sparse learning problems  our task is to develop an estimator 
which adaptively achieves this faster rate of convergence without knowing S in advance.

−2(cid:11)=(2(cid:11)+d). For a ﬁxed α  such rate can be very slow when d becomes large.

f(x) = f(xS) = (f1(xS)  . . .   fp(xS)) 

j  i = 1  . . .   n 

{

}

ϵi
j

i;j

1

d

∑

Previous research on these problems can be roughly divided into three categories: (i) parametric lin-
ear models  (ii) nonparametric additive models  and (iii) nonparametric tree models. The methods
in the ﬁrst category assume that the true models are linear and use some block-norm regulariza-
tion to induce jointly sparse solutions [16  11  13  5]. If the linear model assumptions are correct 
accurate estimates can be obtained. However  given the increasing complexity of modern appli-
cations  conclusions inferred under these restrictive linear model assumptions can be misleading.
Recently  signiﬁcant progress has been made on inferring nonparametric additive models with joint
sparsity constraints [7  10]. For additive models  each fj(x) is assumed to have an additive form:
fj(x) =
k=1 fjk(xk). Although they are more ﬂexible than linear models  the additivity assump-
tions might still be too stringent for real world applications.
A family of more ﬂexible nonparametric methods are based on tree models. One of the most popular
tree methods is the classiﬁcation and regression tree (CART) [2]. It ﬁrst grows a full tree by orthog-
onally splitting the axes at locally optimal splitting points  then prunes back the full tree to form
a subtree. Theoretically  CART is hard to analyze unless strong assumptions have been enforced
[8]. In contrast to CART  dyadic decision trees (DDTs) are restricted to only axis-orthogonal dyadic
splits  i.e. each dimension can only be split at its midpoint. For a broad range of classiﬁcation prob-
lems  [15] showed that DDTs using a special penalty can attain nearly optimal rate of convergence in
a minimax sense. [1] proposed a dynamic programming algorithm for constructing DDTs when the
penalty term has an additive form  i.e. the penalty of the tree can be written as the sum of penalties
on all terminal nodes. Though intensively studied for classiﬁcation problems  the dyadic decision
tree idea has not drawn much attention in the regression settings. One of the closest results we are
aware of is [4]  in which a single response dyadic regression procedure is considered for non-sparse
learning problems. Another interesting tree model  “Bayesian Additive Regression Trees (BART)” 
is proposed under Bayesian framework [6]  which is essentially a “sum-of-trees” model. Most of the
existing work adopt the number of terminal nodes as the penalty. Such penalty cannot lead to sparse
models since a tree with a small number of terminal nodes might still involve too many variables.
To obtain sparse models  we propose a new nonparametric method based on multivariate dyadic
regression trees (MDRTs). Similar to DDTs  MDRTs are constructed using penalized empirical
risk minimization. The novelty of MDRT is to introduce a sparsity-inducing term in the penalty 
which explicitly induces sparse solutions. Our contributions are two-fold: (i) Theoretically  we
show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true
regression functions  and achieve the nearly optimal rate of convergence for the class of (α  C)-
smooth functions. (ii) Empirically  to avoid computationally prohibitive exhaustive search in high
dimensions  we propose a two-stage greedy algorithm and its randomized version that achieve good
performance in both function estimation and variable selection. Note that our theory and algorithm
can be straightforwardly adapted to univariate sparse regression problem  which is a special case of
the multivariate one. To the best of our knowledge  this is the ﬁrst time such a sparsity-inducing
penalty is equipped to tree models for solving sparse regression problems.
The rest of this paper is organized as follows. Section 2 presents MDRTs in detail. Section 3
studies the statistical properties of MDRTs. Section 4 presents the algorithms which approximately
compute the MDRT solutions. Section 5 reports empirical results of MDRTs and their comparison
with CARTs. Conclusions are made in Section 6.

2 Multivariate Dyadic Regression Trees

∏

d

We adopt the notations in [15]. A MDRT T is a multivariate regression tree that recursively divides
the input space X by means of axis-orthogonal dyadic splits. The nodes of T are associated with
hyperrectangles (cells) in X = [0  1]d. The root node corresponds to X itself. If a node is associated
{
to the cell B =
j=1[aj  bj]  after being dyadically split on the dimension k  the two children are
associated to the subcells Bk;1 and Bk;2:
xi ∈ B | xi

and Bk;2 = B \ Bk;1.

≤ ak + bk

Bk;1 =

}

The set of terminal nodes of a MDRT T is denoted as term(T ). Let Bt be the cell in X induced by
a terminal node t  the partition induced by term(T ) can be denoted as π(T ) = {Bt|t ∈ term(T )}.

k

2

2

For each terminal node t  we can ﬁt a multivariate m-th order polynomial regression on data points
falling in Bt. Instead of using all covariates  such a polynomial regression is only ﬁtted on a set of
active variables  which is denoted as A(t). For each node b ∈ T (not necessarily a terminal node) 
A(b) can be an arbitrary subset of {1  . . .   d} satisfying two rules:

1. If a node is dyadically split perpendicular to the axis k  k must belong to the active sets of
2. For any node b  let par(b) be its parent node  then A(par(b)) ⊂ A(b).

its two children.

For a MDRT T   we deﬁne F m
T to be the class of p-valued measurable m-th order polynomials
corresponding to π(T ). Furthermore  for a dyadic integer N = 2L  let TN be the collection of all
MDRTs such that no terminal cell has a side length smaller than 2−L.
Given integers M and N  let F M;N be deﬁned as
n∑

The ﬁnal MDRT estimator with respect to F M;N   denoted as bf M;N   can then be deﬁned as

F M;N = ∪0≤m≤M ∪T∈TN

F m
T .

f∈F M;N

(2)
To deﬁne in detail pen(f) for f ∈ F M;N   let T and m be the MDRT and the order of polynomials
corresponding to f  pen(f) then takes the following form:

)
log n(rT + 1)m(NT + 1)rT + |π(T )| log d

(3)
where λ > 0 is a regularization parameter  rT = | ∪t∈term(T ) A(t)| corresponds to the number of
relevant dimensions and

pen(f) = λ · p
n

2 + pen(f).

bf M;N = arg min

∥yi − f(xi)∥2

(

1
n

i=1

 

NT = min{s ∈ {1  2  . . .   N}| T ∈ Ts}.

There are two terms in (3) within the parenthesis. The latter one penalizing the number of terminal
nodes |π(T )| has been commonly adopted in the existing tree literature. The former one is novel.
Intuitively  it penalizes non-sparse models since the number of relevant dimensions rT appears in
the exponent term. In the next section  we will show that this sparsity-inducing term is derived by
bounding the VC-dimension of the underlying subgraph of regression functions. Thus it has a very
intuitive interpretation.

3 Statistical Properties

p
j=1

∑

In this section  we present theoretical properties of the MDRT estimator. Our main technical result
is Theorem 1  which provides the nearly optimal rate of the MDRT estimator.
To evaluate the algorithm performance  we use the L2-risk with respect to the Lebesgue measure

∫
X |bfj(x) − fj(x)|2dµ(x)  where bf is the function

µ(·)  which is deﬁned as R(bf   f) = E

estimate constructed from n observed samples. Note that all the constants appear in this section are
generic constants  i.e. their values can change from one line to another in the analysis.
Let N0 = {0  1  . . .} be the set of natural number  we ﬁrst deﬁne the class of (α  C)-smooth func-
tions.
Deﬁnition 1 ((α  C)-smoothness) Let α = q + β for some q ∈ N0  0 < β ≤ 1  and let C > 0. A
function g : Rd → R is called (α  C)-smooth if for every α = (α1  . . .   αd)  αi ∈ N0 
j=1 αj =
q  the partial derivative

∑

@qg

d

exists and satisﬁes  for all x  z ∈ Rd 
−

(cid:12)(cid:12)(cid:12)(cid:12) ≤ C · ∥x − z∥(cid:12)

∂qg(z)
1 . . . ∂x(cid:11)d

∂x(cid:11)1

2 .

d

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:11)d
d

@x(cid:11)1
1 :::@x
∂qg(x)
1 . . . ∂x(cid:11)d

d

∂x(cid:11)1

In the following  we denote the class of (α  C)-smooth functions by D(α  C).
Assumption 1 We assume f1  . . .   fp ∈ D(α  C) for some α  C > 0 and for all j ∈ {1  . . .   p} 
fj(x) = fj(xS) with r = |S| ≪ d.
Theorem 3.2 of [9] shows that the lower minimax rate of convergence for class D(α  C) is exactly
the same as that for class of d-dimensional Sobolev ball with order α and radius C.

3

Proposition 1 The proof of this proposition can be found in [9].

lim inf
n→∞

1
p

· n2(cid:11)=(2(cid:11)+d) infbf

sup

f1;:::;fp∈D((cid:11);C)

R(bf   f) > 0.

−2(cid:11)=(2(cid:11)+r) when S is given.

Therefore  the lower minimax rate of convergence is p · n
−2(cid:11)=(2(cid:11)+d). Similarly  if the problem is
jointly sparse with the index set S and r = |S| ≪ d  the best rate of convergence can be improved
to p · n
The following is another technical assumption needed for the main theorem.
Assumption 2 Let 1 ≤ γ < ∞  we assume that

max
1≤j≤p

sup
x

|fj(x)| ≤ γ and max
1≤i≤n

∥yi∥∞ ≤ γ a.s.

This condition is mild. Indeed  we can even allow γ to increase with the sample size n at a certain
rate. This will not affect the ﬁnal result. For example  when {ϵi
}i;j are i.i.d. Gaussian random
variables  this assumption easily holds with γ = O(
log n)  which only contributes a logarithmic
term to the ﬁnal rate of convergence.
The next assumption speciﬁes the scaling of the relevant dimension r and ambient dimension d with
respect to the sample size n.

√

j

Assumption 3 r = O(1) and d = O(exp(n(cid:24))) for some 0 < ξ < 1.

Here  r = O(1) is crucial  since even if r increases at a logarithmic rate with respect to n  i.e.
r = O(log n)  it is hopeless to get any consistent estimator for the class D(α  C) since n
−(1= log n) =
1/e. On the other hand  the ambient dimension d can increase exponentially fast with the sample
size  which is a realistic scaling for high dimensional settings.
The following is the main theorem.

Theorem 1 Under Assumptions 1 to 3  there exist a positive number λ that only depends on α  γ
and r  such that

(

)
(log n)(rT + 1)m(NT + 1)rT + |π(T )| log d

For large enough M  N  the solution bf M;N obtained from (2) satisﬁes
)2(cid:11)=(2(cid:11)+r)

pen(f) = λ · p
n

(

R(bf M;N   f) ≤ c · p ·

log n + log d

 

 

(4)

(5)

where c is some generic constant.

n

Remark 1 As discussed in Proposition 1  the obtained rate of convergence in (5) is nearly optimal
up to a logarithmic term.

Remark 2 Since the estimator deﬁned in (2) does not need to know the smoothness α and the
sparsity level r in advance  MDRTs are simultaneously adaptive to the unknown smoothness and
sparsity level.

Proof of Theorem 1: To ﬁnd an upper bound of R(bf M;N   f)  we need to analyze and control
Without loss of generality  we always assume bf M;N obtained from (2) satisﬁes the condition that
(x)| ≤ γ. if this is not true  we can always truncate bf M;N at the rate γ and

the approximation and estimation errors separately. Our analysis closely follows the least squares
regression analysis in [9] and some speciﬁc coding scheme of trees in [15].

|f M;N

j

max1≤j≤p supx
obtain the desired result in Theorem 1.
Let S m
and let Gm

{
T be the class of all subgraphs of functions of S m
be the VC-dimension of Gm

(z  t) ∈ Rd × R; t ≤ g(z); g ∈ S m
T
T   we have the following lemma:

Gm
T =

T   i.e.

Let VGm

T

}

.

T be the class of scalar-valued measurable m-th order polynomials corresponding to π(T ) 

4

(6)

Lemma 1 Let rT and NT be deﬁned as in (3)  we know that

≤ (rT + 1)m · (NT + 1)rT .

VGm

T

(cid:3)

Sketch of Proof: From Theorem 9.5 of [9]  we only need to show the dimension of Gm
T is upper
bounded by the R.H.S. of (6). By the deﬁnition of rT and NT   the result follows from a straightfor-
ward combinatorial analysis.
The next lemma provides an upper bound of the approximation error for the class D(α  C).
Lemma 2 Let f = (f1  . . .   fp) be the true regression function  there exists a set of piecewise
polynomials h1  . . .   hp ∈ ∪T∈TK Sm
where K ≤ N  c is a generic constant depends on r.
Sketch of Proof: This is a standard approximation result using multivariate piecewise polynomials.
The main idea is based on a multivariate Taylor expansion of the function fj at a given point x0.
Then try to utilize Deﬁnition 1 to bound the remainder terms. For the sake of brevity  we omit the
technical details.
The next lemma is crucial  it provides an oracle inequality to bound the risk using an approximation
term and an estimation term. Its analysis follows from a simple adaptation of Theorem 12.1 on page
227 of [9].

∀j ∈ {1  . . .   p}  sup
x∈X

|fj(x) − hj(x)| ≤ cK

−(cid:11)

(cid:3)

T

First  we deﬁne eR(g  f) =

∑

∫

X |gj(x) − fj(x)|2dµ(x) 

p
j=1

Lemma 3 [9] Choose

pen(f) ≥ 5136 · p

γ4
n
for some preﬁx code [[T ]] > 0 satisfying

R(bf M;N   f) ≤ 12840 · p · γ4

T∈TN
+ 2 inf
T∈TN

n

(
∑

log(120eγ4n)VGm

)
[[T ]] log 2
2−[[T ]] ≤ 1. Then  we have

{
p · pen(g) + eR(g  f)

+

2

T

inf

g∈F M;N

}

.

(7)

(8)

One appropriate preﬁx code [[T ]] for each MDRT T is proposed in [15]  which speciﬁes that
[[T ]] = 3|π(T )| − 1 + (|π(T )| − 1) log d/ log 2. A simpler upper bound for [[T ]] is [[T ]] ≤
(3 + log d/ log 2)|π(T )|.
Remark 3 The derived constants in the Lemma 3 will be pessimistic due to the very large numerical
values. This may result in selecting oversimpliﬁed tree structures. In practice  we always use cross-
validation to choose the tuning parameters.
To prove Theorem 1  ﬁrst  using Assumption 1 and Lemma 2  we know that for any K ≤ N  there
′ ∈ TK 
must exists generic constants c1  c2  c3 and a function f
satisfying f
  f) ≤ c1 · p · K
(log n)(r + 1)M (K + 1)r

′ that is conformal with a MDRT T
′)| ≤ (K + 1)r such that
−2(cid:11) 

′(xS) and |π(T

log d(K + 1)r

eR(f

′(x) = f

and

(9)

′

n

+ c3

.

n

(10)

The desired result then follows by plugging (9) and (10) into (8) and balancing these three terms.

pen(f

′) ≤ c2

4 Computational Algorithm

Exhaustive search of bf M;N in the MDRT space has similar complexity as that of DDTs and could be

computationally very expansive. To make MDRTs scalable for high dimensional massive datasets 
using similar ideas as CARTs  we propose a two-stage procedure: (1) we grow a full tree in a greedy
manner; (2) we prune back the full tree to from the ﬁnal tree. Before going to the detail of the
algorithm  we ﬁrstly introduce some necessary notations.
Given a MDRT T   denote the corresponding multivariate m-th order polynomial ﬁt on π(T ) by
is the m-th order polynomial regression ﬁt on the partition Bt. For

}t∈(cid:25)(T )  where bf m

bf m
T = {bf m

t

t

5

each xi falling in Bt  let bf m
local squared error (LSE) on node t by bRm(t A(t)):
t (xi A(t)) be the predicted function value for xi. We denote the the
∑
bRm(t A(t)) =
It is worthwhile noting that bRm(t A(t)) is calculated as the average with respect to the total sample
size n  instead of the number of data points contained in Bt. The total MSE of the tree bR(T ) can

∥yi − bf m

xi∈Bt

1
n

then be computed by the following equation:

t (xi A(t))∥2
2.
bRm(t A(t)).

∑

bR(T ) =
bC(T ) = bR(T ) + pen(bf m

t∈term(T )

The total cost of T   which is deﬁned as the the right hand side of (2)  then can be written as:

(11)
Our goal is to ﬁnd the tree structure with the polynomial regression on each terminal node that can
minimize the total cost.
The ﬁrst stage is tree growing  in which a terminal node t is ﬁrst selected in each step. We then
perform one of two actions a1 and a2:

T ).

a1: adding another dimension k ̸∈ A(t) to A(t)  and reﬁt the regression model on all data
a2: dyadically splitting t perpendicular to the dimension k ∈ A(t).

points falling in Bt;

In each tree growing step  we need to decide which action to perform. For action a1  we denote the
drop in LSE as:

(12)
For action a2  let sl(t(k)) be the side length of Bt on dimension k ∈ A(t). If sl(t(k)) > 2−L  the
dimension k of Bt can then be dyadically split. In this case  let t(k)
R be the left and right
child of node t. The drop in LSE takes the following form:

L and t(k)

1 (t  k) = bRm(t A(t)) − bRm(t A(t) ∪ {k}).
∆bRm
L  A(t) − bRm(t(k)
2 (t  k) = bRm(t A(t)) − bRm(t(k)
∆bRm
∆bRm

∗) =

argmax

∗
(a

  k

a (t  k).

R  A(t)).
∗ on the dimension k

(13)
∗  which are deter-

For each terminal node t  we greedily perform the action a
mined by

(14)

a∈{1;2};k∈{1:::d}

2∑

∑

∆bRm

In high dimensional setting  the above greedy procedure may not lead to the optimal tree since suc-
cessively locally optimal splits cannot guarantee the global optimum. Once an irrelevant dimension
has been added in or split  the greedy procedure can never ﬁx the mistake. To make the algorithm
more robust  we propose a randomized scheme. Instead of greedily performing the action on the
dimension that leads the maximum drop in LSE  we randomly choose which action to perform ac-

cording to a multinomial distribution. In particular  we normalize ∆bR such that:

k

  k

a=1

a (t  k) = 1.

∗) is drawn from multinomial(1  ∆bR). The action a

(15)
∗ is then performed on the
∗. In general  when the randomized scheme is adopted  we need to repeat our algorithm

∗
And a sample (a
dimension k
many times to pick the best tree.
The second stage is cost complexity pruning. For each step  we either merge a pair of terminal nodes
or remove a variable from the active set of a terminal node such that the resulted tree has the smaller
cost. We repeat this process until the tree becomes a single root node with an empty active set. The
tree with the minimum cost in this process is returned as the ﬁnal tree. The pseudocode for the
growing stage and cost complexity pruning stage are presented in the Appendix. Moreover  to avoid
a cell with too few data points  we pre-deﬁne a quantity nmax. Let n(t) be the the number of data
points fall into Bt  if n(t) ≤ nmax  Bt will no longer be split. It is worthwhile noting that we ignore
those actions that lead to ∆R = 0. In addition  whenever we perform the mth order polynomial
regression on the active set of a node  we need to make sure it is not rank deﬁcient.

6

5 Experimental Results

In this section  we present numerical results for MDRTs applied to both synthetic and real datasets.
We compare ﬁve methods: [1] Greedy MDRT with M = 1 (MDRT(G  M=1)); [2] Randomized
MDRT with M = 1 (MDRT(R  M=1)); [3] Greedy MDRT with M = 0 (MDRT(G  M=0)); [4]
Randomized MDRT with M = 0 (MDRT(R  M=0)); [5] CART. For randomized scheme  we run
50 random trials and pick the minimum cost tree.
As for CART  we adopt the MATLAB package from [12]  which ﬁts piecewise constant on each
|π(T )|  where ρ is the tuning

terminal node with the cost complexity criterion: bC(T ) = bR(T ) + ρ p

parameter playing the same role as λ in (3).
Synthetic Data: For the synthetic data experiment  we consider the high dimensional compound
symmetry covariance structure of the design matrix with n = 200 and d = 100. Each dimension xj
is generated according to

n

xj = Wj + tU
1 + t

 

j = 1  . . .   d 

where W1  . . .   Wd and U are i.i.d. sampled from Uniform(0 1). Therefore the correlation between
xj and xk is t2/(1 + t2) for j ̸= k.
We study three models as shown below:
the ﬁrst one is linear; the second one is nonlinear but
additive; the third one is nonlinear with three-way interactions. All these models only involve four
relevant variables. The noise terms  denoted as ϵ   are independently drawn from a standard normal
distribution.

Model 1: yi
Model 2: yi
Model 3: yi

2 + 4xi

1 + 3xi

1 = 2xi
1 = exp(xi
1 = exp(2xi

1) + (xi
1xi

2 + xi

3 + 5xi
2)2 + 3xi
3) + xi

4 + ϵi
3 + 2xi
4 + ϵi

1

1

4 + ϵi

1

2 = 5xi
yi
2 = (xi
yi
2 = sin(xi
yi

1 + 4xi
1)2 + 2xi

2 + 3xi

3 + 2xi
2 + exp(xi

1xi

2) + (xi

3)2 + 2xi

2

4 + ϵi
3) + 3xi
4 + ϵi

2

4 + ϵi

2

We compare the performances of different methods using two criteria: (i) variable selection and (ii)
function estimation. For each model  we generate 100 designs and an equal-sized validation set per
design. For more detailed experiment protocols  we set nmax = 5 and L = 6. By varying the values
of λ or ρ from large to small  we obtain a full regularization path. The tree with the minimum MSE
on the validation set is then picked as the best tree. For criterion (i)  if the variables involved in the
best tree are exactly the ﬁrst four variables  the variable selection task for this design is deemed as
successful. The numerical results are presented in Table 1. For each method  the three quantities
reported in order are the number of success out of 100 designs  the mean and standard deviation of
the MSE on the validation set. Note that we omit “MDRT” in Table 1 due to space limitations.
From Table 1  the performance of MDRT with M = 1 is dominantly better in both variable selection
and estimation than those of the others. For linear models  MDRT with M = 1 always select
the correct variables even for large ts. For variable selection  MDRT with M = 0 has a better
performance compared with CART due to its sparsity-inducing penalty. In contrast  CART is more
ﬂexible in the sense that its splits are not necessarily dyadic. As a consequence  they are comparable
in function estimation. Moreover  the performance of randomized scheme is slightly better than
its deterministic version in variable selection. Another observation is that  when t becomes larger 
although the performance of variable selection decreases on all methods  the estimation performance
becomes slightly better. This might be counter-intuitive at the ﬁrst sight. In fact  with the increase
of t  all methods tend to select more variables. Due to the high correlations  even the irrelevant
variables are also helpful in predicting the responses. This is an expected effect.
Real Data: In this subsection  we compare these methods on three real datasets. The ﬁrst dataset
is the Chemometrics data (Chem for short)  which has been extensively studied in [3]. The data are
from a simulation of a low density tubular polyethylene reactor with n = 56  d = 22 and p = 6.
Following the same procedures in [3]  we log-transformed the responses because they are skewed.
The second dataset is Boston Housing 1 with n = 506  d = 10 and p = 1. We add 10 irrelevant
variables randomly drawn from Uniform(0 1) to evaluate the variable selection performance. The
third one  Space ga2  is an election data with spatial coordinates on 3107 US counties. Our task
is to predict the x  y coordinates of each county given 5 variables regarding voting information.

1Available from UCI Machine Learning Database Repository: http:archive.ics.uci.edu/ml
2Available from StatLib: http:lib.stat.cmu.edu/datasets/

7

Table 1: Comparison of Variable Selection and Function Estimation on Synthetic Datasets

Model 1
t = 0
t = 0:5
t = 1

Model 2
t = 0
t = 0:5
t = 1

Model 3
t = 0
t = 0:5
t = 1

R  M=1

G  M=1

R  M=0

100
100
100

2.03 (0.14)
2.05 (0.14)
2.05 (0.13)

100
100
100

2.08 (0.15)
2.06 (0.15)
2.05 (0.16)

100
76
19

5.84 (0.51)
5.42 (0.53)
5.40 (0.60)

97
68
20

G  M=0
5.74 (0.54)
5.36 (0.60)
5.56 (0.69)

CART
6.17 (0.55)
5.48 (0.51)
5.30 (0.58)

52
29
3

R  M=1

G  M=1

100
96
76

2.07 (0.13)
2.05 (0.15)
2.09 (0.14)

100
93
68

2.06 (0.15)
2.09 (0.17)
2.21 (0.19)

39
17
2

R  M=0
3.21 (0.26)
3.10 (0.25)
3.17 (0.30)

G  M=0
3.22 (0.28)
3.15 (0.26)
3.16 (0.26)

31
11
2

CART
3.52 (0.31)
3.20 (0.27)
3.16 (0.27)

25
5
1

R  M=1
2.68 (0.31)
2.56 (0.21)
2.51 (0.26)

98
84
65

G  M=1
2.67 (0.47)
2.52 (0.25)
2.62 (0.23)

95
86
50

R  M=0
3.90 (0.47)
3.63 (0.47)
3.75 (0.45)

75
32
3

G  M=0
4.03 (0.54)
3.60 (0.40)
3.88 (0.51)

63
32
4

CART
4.35 (0.73)
3.69 (0.38)
3.66 (0.38)

29
15
2

For Space ga  we normalize the responses to [0  1]. Similarly  we add other 15 irrelevant variables
randomly drawn from Uniform(0 1). For all these datasets  we scale the input variables into a unit
cube.
For evaluation purpose  each dataset is randomly split such that half data are used for training and
the other half for testing. We run a 5-fold cross-validation on the training set to pick the best tuning
∗.
∗ and ρ
parameter λ
We repeat this process 20 times and report the mean and standard deviation of the testing MSE in
Table 2. nmax is set to be 5 for the ﬁrst dataset and 20 for the latter two. For all datasets  we set
L = 6. Moreover  for randomized scheme  we run 50 random trials and pick the minimum cost tree.

∗. We then train MDRTs and CART on the entire training data using λ

∗ and ρ

Chem
Housing
Space ga

R  M=1
0.15 (0.09)
20.18 (2.94)
0.054 (7.8e-4)

Table 2: Testing MSE on Real Datasets
G  M=1
0.18 (0.12)
21.60 (2.83)
0.055 (8.0e-4)

R  M=0
0.38 (0.18)
24.67 (2.05)
0.068 (7.2e-4)

G  M=0
0.52 (0.06)
29.46 (1.95)
0.068 (9.2e-4)

CART
0.40 (0.09)
25.91 (3.05)
0.064 (8.3e-4)

From Table 2  we see that MDRT with M = 1 has the best estimation performance. Moreover 
randomized scheme does improve the performance compared to the deterministic counterpart. In
particularly  such an improvement is quite signiﬁcant when M = 0. The performance of MDRT(G 
M=0) is always worse than CART since CART can have more ﬂexible splits. However  using ran-
domized scheme  the performance of MDRT(R  M=0) achieves a comparable performance as CART.
As for variable selection of Housing data  in all the 20 runs  MDRT(G  M=1) and MDRT(R  M=1)
never select the artiﬁcially added variables. However  for the other three methods  nearly 10 out of
20 runs involve at least one extraneous variable. In particular  we compare our results with those
reported in [14]. They ﬁnd that there are 4 (indus  age  dis  tax) irrelevant variables in the Housing
data. Our experiments conﬁrm this result since in 15 out of the 20 trials  MDRT(G  M=1) and
MDRT(R  M=1) never select these four variables. Similarly  for Space ga data  there are only 2 and
1 times that MDRT(G  M=1) and MDRT(R  M=1) involve the artiﬁcially added variables.

6 Conclusions
We propose a novel sparse learning method based on multivariate dyadic regression trees (MDRTs).
Our approach adopts a new sparsity-inducing penalty that simultaneously conduct function estima-
tion and variable selection. Some theoretical analysis and practical algorithms have been developed.
To the best of our knowledge  it is the ﬁrst time that such a penalty is introduced in the tree literature
for high dimensional sparse learning problems.

8

References
[1] G. Blanchard  C. Sch¨afer  Y. Rozenholc  and K.-R. M¨uller. Optimal dyadic decision trees.

Machine Learning Journal  66(2-3):209–241  2007.

[2] Leo Breiman  Jerome Friedman  Charles J. Stone  and R.A. Olshen. Classiﬁcation and regres-

sion trees. Wadsworth Publishing Co Inc  1984.

[3] Leo Breiman and Jerome H. Friedman. Predicting multivariate responses in multiple linear

regression. J. Roy. Statist. Soc. B  59:3  1997.

[4] R. Castro  R. Willett  and R. Nowak. Fast rates in regression via active learning. NIPS  2005.
[5] Xi Chen  Weike Pan  James T. Kwok  and Jamie G. Carbonell. Accelerated gradient method

for multi-task sparse learning problem. In ICDM  2009.

[6] Hugh A. Chipman  Edward I. George  and Robert E. McCulloch. Bart: Bayesian additive re-
gression trees. Technical report  Department of Mathematics and Statistics  Acadia University 
Canada  2006.

[7] Jerome H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics  19:1–

67  1991.

[8] S. Gey and E. Nedelec. Model selection for cart regression trees. IEEE Tran. on Info. Theory 

51(2):658– 670  2005.

[9] L´aszl´o Gy¨orﬁ  Michael Kohler  Adam Krzy˙zak  and Harro Walk. A Distribution-Free Theory

of Nonparametric Regression. Springer-Verlag  2002.

[10] Han Liu  John Lafferty  and Larry Wasserman. Nonparametric regression and classiﬁcation

with joint sparsity constraints. In NIPS. MIT Press  2008.

[11] Han Liu and Jian Zhang. On the estimation consistency of the group lasso and its applications.

AISTATS  pages 376–383  2009.

[12] Wendy L. Martinez and Angel R. Martinez. Computational Statistics Handbook with MATLAB.

Chapman & Hall CRC  2 edition  2008.

[13] G. Obozinski  M. J. Wainwright  and M. I. Jordan. High-dimensional union support recovery

in multivariate regression. In NIPS. MIT Press  2009.

[14] Pradeep Ravikumar  Han Liu  John Lafferty  and Larry Wasserman. Spam: Sparse additive

models. In NIPS. MIT Press  2007.

[15] C. Scott and R.D. Nowak. Minimax-optimal classiﬁcation with dyadic decision trees. IEEE

Tran. on Info. Theory  52(4):1335–1353  2006.

[16] B.A. Turlach  W. N. Venables  and S. J. Wright. Simultaneous variable selection. Technomet-

rics  27:349–363  2005.

9

,Jiashi Feng
Huan Xu
Shuicheng Yan
Debarghya Ghoshdastidar
Ambedkar Dukkipati
ABHISEK KUNDU
Petros Drineas
Malik Magdon-Ismail
Ziang Yan
Yiwen Guo
Changshui Zhang