2019,Policy Poisoning in Batch Reinforcement Learning and Control,We study a security threat to batch reinforcement learning and control where the attacker aims to poison the learned policy. The victim is a reinforcement learner / controller which first estimates the dynamics and the rewards from a batch data set  and then solves for the optimal policy with respect to the estimates. The attacker can modify the data set slightly before learning happens  and wants to force the learner into learning a target policy chosen by the attacker. We present a unified framework for solving batch policy poisoning attacks  and instantiate the attack on two standard victims: tabular certainty equivalence learner in reinforcement learning and linear quadratic regulator in control. We show that both instantiation result in a convex optimization problem on which global optimality is guaranteed  and provide analysis on attack feasibility and attack cost. Experiments show the effectiveness of policy poisoning attacks.,Policy Poisoning

in Batch Reinforcement Learning and Control

Yuzhe Ma

University of Wisconsin–Madison

yzm234@cs.wisc.edu

Xuezhou Zhang

University of Wisconsin–Madison
zhangxz1123@cs.wisc.edu

Wen Sun

Microsoft Research New York
Sun.Wen@microsoft.com

Xiaojin Zhu

University of Wisconsin–Madison

jerryzhu@cs.wisc.edu

Abstract

We study a security threat to batch reinforcement learning and control where the
attacker aims to poison the learned policy. The victim is a reinforcement learner /
controller which ﬁrst estimates the dynamics and the rewards from a batch data set 
and then solves for the optimal policy with respect to the estimates. The attacker
can modify the data set slightly before learning happens  and wants to force the
learner into learning a target policy chosen by the attacker. We present a uniﬁed
framework for solving batch policy poisoning attacks  and instantiate the attack
on two standard victims: tabular certainty equivalence learner in reinforcement
learning and linear quadratic regulator in control. We show that both instantiation
result in a convex optimization problem on which global optimality is guaranteed 
and provide analysis on attack feasibility and attack cost. Experiments show the
effectiveness of policy poisoning attacks.

1

Introduction

With the increasing adoption of machine learning  it is critical to study security threats to learning
algorithms and design effective defense mechanisms against those threats. There has been signiﬁcant
work on adversarial attacks [2  9]. We focus on the subarea of data poisoning attacks where the
adversary manipulates the training data so that the learner learns a wrong model. Prior work on data
poisoning targeted victims in supervised learning [17  13  19  22] and multi-armed bandits [11  16  15].
We take a step further and study data poisoning attacks on reinforcement learning (RL). Given RL’s
prominent applications in robotics  games and so on  an intentionally and adversarially planted bad
policy could be devastating.
While there has been some related work in test-time attack on RL  reward shaping  and teaching
inverse reinforcement learning (IRL)  little is understood on how to training-set poison a reinforcement
learner. We take the ﬁrst step and focus on batch reinforcement learner and controller as the victims.
These victims learn their policy from a batch training set. We assume that the attacker can modify the
rewards in the training set  which we show is sufﬁcient for policy poisoning. The attacker’s goal is to
force the victim to learn a particular target policy (hence the name policy poisoning)  while minimizing
the reward modiﬁcations. Our main contribution is to characterize batch policy poisoning with a
uniﬁed optimization framework  and to study two instances against tabular certainty-equivalence
(TCE) victim and linear quadratic regulator (LQR) victim  respectively.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2 Related Work

Of particular interest is the work on test-time attacks against RL [10]. Unlike policy poisoning  there
the RL agent carries out an already-learned and ﬁxed policy ⇡ to e.g. play the Pong Game. The
attacker perturbs pixels in a game board image  which is part of the state s. This essentially changes
the RL agent’s perceived state into some s0. The RL agent then chooses the action a0 := ⇡(s0) (e.g.
move down) which may differ from a := ⇡(s) (e.g. move up). The attacker’s goal is to force some
speciﬁc a0 on the RL agent. Note ⇡ itself stays the same through the attack. In contrast  ours is a
data-poisoning attack which happens at training time and aims to change ⇡.
Data-poisoning attacks were previously limited to supervised learning victims  either in batch
mode [3  21  14  17] or online mode [19  22]. Recently data-poisoning attacks have been extended to
multi-armed bandit victims [11  16  15]  but not yet to RL victims.
There are two related but distinct concepts in RL research. One concept is reward shaping [18  1 
7  20] which also modiﬁes rewards to affect an RL agent. However  the goal of reward shaping
is fundamentally different from ours. Reward shaping aims to speed up convergence to the same
optimal policy as without shaping. Note the differences in both the target (same vs. different policies)
and the optimality measure (speed to converge vs. magnitude of reward change).
The other concept is teaching IRL [5  4  12]. Teaching and attacking are mathematically equivalent.
However  the main difference to our work is the victim. They require an IRL agent  which is
a specialized algorithm that estimates a reward function from demonstrations of (state  action)
trajectories alone (i.e. no reward given). In contrast  our attacks target more prevalent RL agents and
are thus potentially more applicable. Due to the difference in the input to IRL vs. RL victims  our
attack framework is completely different.

3 Preliminaries
A Markov Decision Process (MDP) is deﬁned as a tuple (S A  P  R  )  where S is the state space 
A is the action space  P : S⇥A! S is the transition kernel where S denotes the space of
probability distributions on S  R : S⇥A! R is the reward function  and  2 [0  1) is a discounting
factor. We deﬁne a policy ⇡ : S!A as a function that maps a state to an action. We denote
the Q function of a policy ⇡ as Q⇡(s  a) = E[P1⌧ =0 ⌧ R(s⌧   a⌧ ) | s0 = s  a0 = a  ⇡]  where the
expectation is over the randomness in both transitions and rewards. The Q function that corresponds
to the optimal policy can be characterized by the following Bellman optimality equation:

Q⇤(s0  a0) 

Q⇤(s  a) = R(s  a) + Xs02S

P (s0|s  a) max
a02A
and the optimal policy is deﬁned as ⇡⇤(s) 2 arg maxa2A Q⇤(s  a).
We focus on RL victims who perform batch reinforcement learning. A training item is a tuple
R ⇥S   where s is the current state  a is the action taken  r is the received
(s  a  r  s0) 2S⇥A⇥
reward  and s0 is the next state. A training set is a batch of T training items denoted by D =
(st  at  rt  s0t)t=0:T1. Given training set D  a model-based learner performs learning in two steps:
Step 1. The learner estimates an MDP ˆM = (S A  ˆP   ˆR  ) from D. In particular  we assume the
learner uses maximum likelihood estimate for the transition kernel ˆP : S ⇥ A 7! S

(1)

Note that we do not require (2) to have a unique maximizer ˆP . When multiple maximizers exist 
we assume the learner arbitrarily picks one of them as the estimate. We assume the minimizer ˆR is
always unique. We will discuss the conditions to guarantee the uniqueness of ˆR for two learners later.

log P (s0t|st  at) 
and least-squares estimate for the reward function ˆR : S ⇥ A 7! R

ˆP 2 arg max

P

ˆR = arg min

R

(rt  R(st  at))2.

(2)

(3)

T1Xt=0
T1Xt=0

2

Step 2. The learner ﬁnds the optimal policy ˆ⇡ that maximizes the expected discounted cumulative
reward on the estimated environment ˆM  i.e. 

ˆ⇡ 2 arg max
⇡:S7!A

E ˆP

1X⌧ =0

⌧ ˆR(s⌧  ⇡ (s⌧ )) 

(4)

where s0 is a speciﬁed or random initial state. Note that there could be multiple optimal policies 
thus we use 2 in (4). Later we will specialize (4) to two speciﬁc victim learners: the tabular certainty
equivalence learner (TCE) and the certainty-equivalent linear quadratic regulator (LQR).

4 Policy Poisoning

We study policy poisoning attacks on model-based batch RL learners. Our threat model is as follows:
Knowledge of the attacker. The attacker has access to the original training set D0 =
t   s0t)t=0:T1. The attacker knows the model-based RL learner’s algorithm. Importantly 
(st  at  r0
the attacker knows how the learner estimates the environment  i.e.  (2) and (3). In the case (2) has
multiple maximizers  we assume the attacker knows exactly the ˆP that the learner picks.
Available actions of the attacker. The attacker is allowed to arbitrarily modify the rewards r0 =
T1) in D0 into r = (r0  ...  rT1). As we show later  changing r’s but not s  a  s0 is
(r0
sufﬁcient for policy poisoning.
Attacker’s goals. The attacker has a pre-speciﬁed target policy ⇡†. The attack goals are to (1) force
the learner to learn ⇡†  (2) minimize attack cost kr  r0k↵ under an ↵-norm chosen by the attacker.
Given the threat model  we can formulate policy poisoning as a bi-level optimization problem1:

0  ...  r0

min
r  ˆR

s.t.

kr  r0k↵

R

ˆR = arg min

T1Xt=0
{⇡†} = arg max
⇡:S7!A

(rt  R(st  at))2
1X⌧ =0

E ˆP

⌧ ˆR(s⌧  ⇡ (s⌧ )).

(5)

(6)

(7)

The ˆP in (7) does not involve r and is precomputed from D0. The singleton set {⇡†} on the LHS
of (7) ensures that the target policy is learned uniquely  i.e.  there are no other optimal policies tied
with ⇡†. Next  we instantiate this attack formulation to two representative model-based RL victims.

4.1 Poisoning a Tabular Certainty Equivalence (TCE) Victim
In tabular certainty equivalence (TCE)  the environment is a Markov Decision Process (MDP) with
ﬁnite state and action space. Given original data D0 = (st  at  r0
t   s0t)0:T1  let Ts a = {t | st =
s  at = a}  the time indexes of all training items for which action a is taken at state s. We assume
Ts a  1  8s  a  i.e.  each state-action pair appears at least once in D0. This condition is needed to
ensure that the learner’s estimate ˆP and ˆR exist. Remember that we require (3) to have a unique
solution. For the TCE learner  ˆR is unique as long as it exists. Therefore  Ts a  1  8s  a is sufﬁcient
to guarantee a unique solution to (3). Let the poisoned data be D = (st  at  rt  s0t)0:T1. Instantiating
model estimation (2)  (3) for TCE  we have
ˆP (s0 | s  a) =

1 [s0t = s0]  

(8)

where 1 [] is the indicator function  and

ˆR(s  a) =

rt.

(9)

1

|Ts a| Xt2Ts a
|Ts a| Xt2Ts a

1

1As we will show  the constraint (7) could lead to an open feasible set (e.g.  in (10)) for the attack optimiza-
tion (5)-(7)  on which the minimum of the objective function (5) may not be well-deﬁned. In the case (7) induces
an open set  we will consider instead a closed subset of it  and optimize over the subset. How to construct the
closed subset will be made clear for concrete learners later.

3

The TCE learner uses ˆP   ˆR to form an estimated MDP ˆM  then solves for the optimal policy ˆ⇡ with
respect to ˆM using the Bellman equation (1). The attack goal (7) can be naively characterized by

Q(s  ⇡†(s)) > Q(s  a) 8s 2S  8a 6= ⇡†(s).

(10)

However  due to the strict inequality  (10) induces an open set in the Q space  on which the minimum
of (5) may not be well-deﬁned. Instead  we require a stronger attack goal which leads to a closed
subset in the Q space. This is deﬁned as the following "-robust target Q polytope.
Deﬁnition 1. ("-robust target Q polytope) The set of "-robust Q functions induced by a target policy
⇡† is the polytope

Q"(⇡†) = {Q : Q(s  ⇡†(s))  Q(s  a) + " 8s 2S  8a 6= ⇡†(s)}

for a ﬁxed "> 0.

(11)

The margin parameter " ensures that ⇡† is the unique optimal policy for any Q in the polytope. We
now have a solvable attack problem  where the attacker wants to force the victim’s Q function into
the "-robust target Q polytope Q"(⇡†):

min

r2RT   ˆR Q2R|S|⇥|A|
s.t.

kr  r0k↵
ˆR(s  a) =

(12)

(13)

(14)

1

rt

|Ts a| Xt2Ts a
Q(s  a) = ˆR(s  a) + Xs0
Q(s  ⇡†(s))  Q(s  a) + " 8s 2S  8a 6= ⇡†(s).

ˆP (s0|s  a) Qs0 ⇡ †(s0)  8s 8a

(15)
The constraint (14) enforces Bellman optimality on the value function Q  in which maxa02A Q(s0  a0)
is replaced by Qs0 ⇡ †(s0)  since the target policy is guaranteed to be optimal by (15). Note that
problem (12)-(15) is a convex program with linear constraints given that ↵  1  thus could be solved
to global optimality. However  we point out that (12)-(15) is a more stringent formulation than (5)-(7)
due to the additional margin parameter " we introduced. The feasible set of (12)-(15) is a subset
of (5)-(7). Therefore  the optimal solution to (12)-(15) could in general be a sub-optimal solution
to (5)-(7) with potentially larger objective value. We now study a few theoretical properties of policy
poisoning on TCE. All proofs are in the appendix. First of all  the attack is always feasible.
Proposition 1. The attack problem (12)-(15) is always feasible for any target policy ⇡†.

Proposition 1 states that for any target policy ⇡†  there exists a perturbation on the rewards that
teaches the learner that policy. Therefore  the attacker changing r’s but not s  a  s0 is already sufﬁcient
for policy poisoning.
We next bound the attack cost. Let the MDP estimated on the clean data be ˆM 0 = (S A  ˆP   ˆR0  ).
Let Q0 be the Q function that satisﬁes the Bellman optimality equation on ˆM 0. Deﬁne (") =
maxs2S[maxa6=⇡†(s) Q0(s  a)Q0(s  ⇡†(s))+"]+  where []+ takes the maximum over 0. Intuitively 
(") measures how suboptimal the target policy ⇡† is compared to the clean optimal policy ⇡0 learned
on ˆM 0  up to a margin parameter ".
Theorem 2. Assume ↵  1 in (12). Let r⇤  ˆR⇤ and Q⇤ be an optimal solution to (12)-(15)  then

1
2

(1  )(")✓min

s a |Ts a|◆ 1

↵

 kr⇤  r0k↵ 

1
2

(1 + )(")T

1

↵ .

(16)

Corollary 3. If ↵ = 1  then the optimal attack cost is O((")T ). If ↵ = 2  then the optimal attack
cost is O((")pT ). If ↵ = 1  then the optimal attack cost is O((")).

Note that both the upper and lower bounds on the attack cost are linear with respect to (")  which
can be estimated directly from the clean training set D0. This allows the attacker to easily estimate
its attack cost before actually solving the attack problem.

4

4.2 Poisoning a Linear Quadratic Regulator (LQR) Victim
As the second example  we study an LQR victim that performs system identiﬁcation from a batch
training set [6]. Let the linear dynamical system be

1
2

L(s  a) =

s>Qs + q>s + a>Ra + c

st+1 = Ast + Bat + wt 8t  0 

(17)
where A 2 Rn⇥n  B 2 Rn⇥m  st 2 Rn is the state  at 2 Rm is the control signal  and wt ⇠
N (0  2I) is a Gaussian noise. When the agent takes action a at state s  it suffers a quadratic loss of
the general form
(18)
for some Q ⌫ 0  R  0  q 2 Rn and c 2 R. Here we have redeﬁned the symbols Q and R in order to
conform with the notation convention in LQR: now we use Q for the quadratic loss matrix associated
with state  not the action-value function; we use R for the quadratic loss matrix associated with
action  not the reward function. The previous reward function R(s  a) in general MDP (section 3)
is now equivalent to the negative loss L(s  a). This form of loss captures various LQR control
problems. Note that the above linear dynamical system can be viewed as an MDP with transition
kernel P (s0 | s  a) = N (As + Ba   2I) and reward function L(s  a). The environment is thus
characterized by matrices A  B (for transition kernel) and Q  R  q  c (for reward function)  which are
all unknown to the learner.
We assume the clean training data D0 = (st  at  r0
t   st+1)0:T1 was generated by running the
linear system for multiple episodes following some random policy [6]. Let the poisoned data be
D = (st  at  rt  st+1)0:T1. Instantiating model estimation (2)  (3)  the learner performs system
identiﬁcation on the poisoned data:

( ˆA  ˆB) 2 arg min

(A B)

2

1
2

kAst + Bat  st+1k2

T1Xt=0
s>t Qst + q>st + a>t Rat + c + rt
T1Xt=0

1
2

1
2

(19)

(20)

2

2

.

( ˆQ  ˆR  ˆq  ˆc) = arg min

(Q⌫0 R⌫"I q c)

Note that in (20)  the learner uses a stronger constraint R ⌫ "I than the original constraint R  0 
which guarantees that the minimizer can be achieved. The conditions to further guarantee (20) having
a unique solution depend on the property of certain matrices formed by the clean training set D0 
which we defer to appendix D.
The learner then computes the optimal control policy with respect to ˆA  ˆB  ˆQ  ˆR  ˆq and ˆc. We assume
the learner solves a discounted version of LQR control

E" 1X⌧ =0

⌧ (

1
2

s>⌧

ˆQs⌧ + ˆq>s⌧ + ⇡(s⌧ )> ˆR⇡(s⌧ ) + ˆc)#

max
⇡:S7!A
s.t.

(22)
where the expectation is over w⌧ . It is known that the control problem has a closed-form solution
ˆa⌧ = ˆ⇡(s⌧ ) = Ks⌧ + k  where

s⌧ +1 = ˆAs⌧ + ˆB⇡(s⌧ ) + w⌧  8⌧  0.

K = ⇣ ˆR +  ˆB>X ˆB⌘1

ˆB>X ˆA 

k = ( ˆR +  ˆB>X ˆB)1 ˆB>x.

Here X ⌫ 0 is the unique solution of the Algebraic Riccati Equation 
X =  ˆA>X ˆA  2 ˆA>X ˆB⇣ ˆR +  ˆB>X ˆB⌘1

and x is a vector that satisﬁes

ˆB>X ˆA + ˆQ 

(25)
The attacker aims to force the victim into taking target action ⇡†(s) 8s 2 Rn. Note that in LQR  the
attacker cannot arbitrarily choose ⇡†  as the optimal control policy K and k enforce a linear structural
constraint between ⇡†(s) and s. One can easily see that the target action must obey ⇡†(s) = K†s+k†

x = ˆq + ( ˆA + ˆBK)>x.

(21)

(23)

(24)

5

for some (K†  k†) in order to achieve successful attack. Therefore we must assume instead that the
attacker has a target policy speciﬁed by a pair (K†  k†). However  an arbitrarily linear policy may
still not be feasible. A target policy (K†  k†) is feasible if and only if it is produced by solving some
Riccati equation  namely  it must lie in the following set:

{(K  k) : 9Q ⌫ 0  R ⌫ "I  q 2 Rn  c 2 R  such that (23)  (24)  and (25) are satisﬁed}.

(26)
Therefore  to guarantee feasibility  we assume the attacker always picks the target policy (K†  k†)
by solving an LQR problem with some attacker-deﬁned loss function. We can now pose the policy
poisoning attack problem:

min

r  ˆQ  ˆR ˆq ˆc X x

s.t.

ˆB>X ˆA = K†

kr  r0k↵
⇣ ˆR +  ˆB>X ˆB⌘1
⇣ ˆR +  ˆB>X ˆB⌘1
X =  ˆA>X ˆA  2 ˆA>X ˆB⇣ ˆR +  ˆB>X ˆB⌘1

x = ˆq + ( ˆA + ˆBK†)>x

ˆB>x = k†

( ˆQ  ˆR  ˆq  ˆc) = arg min

(Q⌫0 R⌫"I q c)

ˆB>X ˆA + ˆQ

(27)

(28)

(29)

(30)

(31)

(32)

2

2

T1Xt=0

1
2

s>t Qst + q>st + a>t Rat + c + rt

X ⌫ 0.

(33)
Note that the estimated transition matrices ˆA  ˆB are not optimization variables because the attacker
can only modify the rewards  which will not change the learner’s estimate on ˆA and ˆB. The attack
optimization (27)-(33) is hard to solve due to the constraint (32) itself being a semi-deﬁnite program
(SDP). To overcome the difﬁculty  we pull all the positive semi-deﬁnite constraints out of the lower-
level optimization. This leads to a more stringent surrogate attack optimization (see appendix C).
Solving the surrogate attack problem  whose feasible region is a subset of the original problem  in
general gives a suboptimal solution to (27)-(33). But it comes with one advantage: convexity.

5 Experiments

Throughout the experiments  we use CVXPY [8] to implement the optimization. All code can be
found in https://github.com/myzwisc/PPRL_NeurIPS19.

5.1 Policy Poisoning Attack on TCE Victim
Experiment 1. We consider a simple MDP with two states A  B and two actions: stay in the same
state or move to the other state  shown in ﬁgure 1a. The discounting factor is  = 0.9. The MDP’s Q
values are shown in table 1b. Note that the optimal policy will always pick action stay. The clean
training data D0 reﬂects this underlying MDP  and consists of 4 tuples:

(A  stay  1  A)

(A  move  0  B)

(B  stay  1  B)

(B  move  0  A)

Let the attacker’s target policy be ⇡†(s) =move  for any state s. The attacker sets " = 1 and
uses ↵ = 2  i.e. kr  r0k2 as the attack cost. Solving the policy poisoning attack optimization
problem (12)-(15) produces the poisoned data:
(A  move  1  B)

(B  move  1  A)

(B  stay  0  B)

(A  stay  0  A)

with attack cost kr  r0k2 = 2. The resulting poisoned Q values are shown in table 1c. To verify
this attack  we run TCE learner on both clean data and poisoned data. Speciﬁcally  we estimate
the transition kernel and the reward function as in (8) and (9) on each data set  and then run value
iteration until the Q values converge. In Figure 1d  we show the trajectory of Q values for state A 
where the x and y axes denote Q(A  stay) and Q(A  move) respectively. All trajectories start at
(0  0). The dots on the trajectory correspond to each step of value iteration  while the star denotes the
converged Q values. The diagonal dashed line is the (zero margin) policy boundary  while the gray

6

0

0

A

+1

B

+1

(a) A toy MDP with two states.

9
9

stay move
10
10

A
B
(b) Original Q values.
stay move
10
9
9
10

A
B

(c) Poisoned Q values.

(d) Trajectory for the Q values of state A during value iteration.

Figure 1: Poisoning TCE in a two-state MDP.

region is the "-robust target Q polytope with an offset " = 1 to the policy boundary. The trajectory of
clean data converges to a point below the policy boundary  where the action stay is optimal. With the
poisoned data  the trajectory of Q values converge to a point exactly on the boundary of the "-robust
target Q polytope  where the action move becomes optimal. This validates our attack.
We also compare our attack with reward shaping [18]. We let the potential function (s) be the
optimal value function V (s) for all s to shape the clean dataset. The dataset after shaping is

(A  stay  0  A)

(A  move 1  B)

(B  stay  0  B)

(B  move 1  A)

In Figure 1d  we show the trajectory of Q values after reward shaping. Note that same as on clean
dataset  the trajectory after shaping converges to a point also below the policy boundary. This means
reward shaping can not make the learner learn a different policy from the original optimal policy.
Also note that after reward shaping  value iteration converges much faster (in only one iteration) 
which matches the beneﬁts of reward shaping shown in [18]. More importantly  this illustrates the
difference between our attack and reward shaping.

+2.139
G
+0.238

-0.200

-0.221

-0.246

-0.274

-0.304

-0.338

-0.376

-0.572

-0.515

-0.464

-0.417

+0.464

+0.515

+0.572

S

-0.015

-0.115

G1

-0.115

-0.005

+0.262

G2

-0.040

-0.115

+0.005

-0.004

+0.029

-0.004

-0.044

+0.009

-0.008

+0.032

-0.043

S

+0.075

+0.088

+0.080

+0.068

-0.006

+0.036

+0.015

+0.008

-0.005

-0.032

-0.010

-0.013

-0.016

-0.015

-0.007

+0.020

+0.012

+0.004

-0.006

-0.020

-0.009

-0.014

-0.015

-0.011

+0.012

+0.007

+0.002

-0.003

-0.012

-0.012

-0.018

-0.018

-0.013

G

:2

:10

:1

G1

:1

G2

:2

:1

(a) Grid world with a single terminal state G.

(b) Grid world with two terminal states G1 and G2.

Figure 2: Poisoning TCE in grid-world tasks.

7

Experiment 2. As another example  we consider the grid world tasks in [5]. In particular  we focus
on two tasks shown in ﬁgure 2a and 2b. In ﬁgure 2a  the agent starts from S and aims to arrive at the
terminal cell G. The black regions are walls  thus the agent can only choose to go through the white or
gray regions. The agent can take four actions in every state: go left  right  up or down  and stays if the
action takes it into the wall. Reaching a gray  white  or the terminal state results in rewards 10  1 
2  respectively. After the agent arrives at the terminal state G  it will stay there forever and always
receive reward 0 regardless of the following actions. The original optimal policy is to follow the blue
trajectory. The attacker’s goal is to force the agent to follow the red trajectory. Correspondingly  we
set the target actions for those states on the red trajectory as along the trajectory. We set the target
actions for the remaining states to be the same as the original optimal policy learned on clean data.
The clean training data contains a single item for every state-action pair. We run the attack with
" = 0.1 and ↵ = 2. Our attack is successful: with the poisoned data  TCE generates a policy
that produces the red trajectory in Figure 2a  which is the desired behavior. The attack cost is
kr  r0k2 ⇡ 2.64  which is small compared to kr0k2 = 21.61. In Figure 2a  we show the poisoning
on rewards. Each state-action pair is denoted by an orange arrow. The value tagged to each arrow
is the modiﬁcation to that reward  where red value means the reward is increased and blue means
decreased. An arrow without any tagged value means the corresponding reward is not changed by
attack. Note that rewards along the red trajectory are increased  while those along the blue trajectory
are reduced  resulting in the red trajectory being preferred by the agent. Furthermore  rewards closer
to the starting state S suffer larger poisoning since they contribute more to the Q values. For the large
attack +2.139 happening at terminal state  we provide an explanation in appendix E.
Experiment 3. In Figure 2b there are two terminal states G1 and G2 with reward 1 and 2  respectively.
The agent starts from S. Although G2 is more proﬁtable  the path is longer and each step has a 1
reward. Therefore  the original optimal policy is the blue trajectory to G1. The attacker’s target
policy is to force the agent along the red trajectory to G2. We set the target actions for states as in
experiment 2. The clean training data contains a single item for every state-action pair. We run our
attack with " = 0.1 and ↵ = 2. Again  after the attack  TCE on the poisoned dataset produces the red
trajectory in ﬁgure 2b  with attack cost kr  r0k2 ⇡ 0.38  compared to kr0k2 = 11.09. The reward
poisoning follows a similar pattern to experiment 2.

5.2 Policy Poisoning Attack on LQR Victim

(a) Clean and poisoned vehicle trajectory.

(b) Clean and poisoned rewards.

Figure 3: Poisoning a vehicle running LQR in 4D state space.

t   vy

Experiment 4. We now demonstrate our attack on LQR. We consider a linear dynamical system that
approximately models a vehicle. The state of the vehicle consists of its 2D position and 2D velocity:
t ) 2 R4. The control signal at time t is the force at 2 R2 which will be applied
st = (xt  yt  vx
on the vehicle for h seconds. We assume there is a friction parameter ⌘ such that the friction force
is ⌘vt. Let m be the mass of the vehicle. Given small enough h  the transition matrices can be
approximated by (17) where
0
1
0 1  h⌘/m
0

0
0
h/m 0

1
0
0
0

h
0

0

(34)

375   B =264

0
0

0

375 .

h/m

A =264

0
h
0

8

1  h⌘/m

In this example  we let h = 0.1  m = 1  ⌘ = 0.5  and wt ⇠N (0  2I) with  = 0.01. The vehicle
starts from initial position (1  1) with velocity (1 0.5)  i.e.  s0 = (1  1  1 0.5). The true loss
function is L(s  a) = 1
2 s>Qs + a>Ra with Q = I and R = 0.1I (i.e.  Q = I  R = 0.1I  q = 0  c =
0 in (18)). Throughout the experiment  we let  = 0.9 for solving the optimal control policy in (21).
With the true dynamics and loss function  the computed optimal control policy is

K⇤ = 1.32

0

0

2.39

0

2.39    k⇤ = [ 0

0 ]  

(35)

0

1.32
which will drive the vehicle to the origin.
The batch LQR learner estimates the dynamics and the loss function from a batch training data. To
produce the training data  we let the vehicle start from state s0 and simulate its trajectory with a
random control policy. Speciﬁcally  in each time step  we uniformly sample a control signal at in a
unit sphere. The vehicle then takes action at to transit from current state st to the next state st+1  and
receives a reward rt = L(st  at). This gives us one training item (st  at  rt  st+1). We simulate a
total of 400 time steps to obtain a batch data that contains 400 items  on which the learner estimates
the dynamics and the loss function. With the learner’s estimate  the computed clean optimal policy is
ˆK0 = 1.31

1.97e2 1.35 1.14e2 2.42    ˆk0 = [ 4.88e5

4.95e6 ] . (36)
The clean optimal policy differs slightly from the true optimal policy due to the inaccuracy of the
learner’s estimate. The attacker has a target policy (K†  k†) that can drive the vehicle close to its
target destination (x†  y†) = (0  1) with terminal velocity (0  0)  which can be represented as a target
state s† = (0  1  0  0). To ensure feasibility  we assume that the attacker starts with the loss function
2 (s  s†)>Q(s  s†) + a>Ra where Q = I  R = 0.1I. Due to the offset this corresponds to setting
1
2 s†>Qs† = 0.5 in (18). The attacker then solves the Riccati
Q = I  R = 0.1I  q = s†  c = 1
equation with its own loss function and the learner’s estimates ˆA and ˆB to arrive at the target policy

1.00e2

2.03e3

2.41

K† = 1.31

1.97e2 1.35 1.14e2 2.42    k† = [ 0.01

9.99e3

2.02e3

2.41

1.35 ] .

(37)

We run our attack (27)-(33) with ↵ = 2 and " = 0.01 in (32). Figure 3 shows the result of our attack.
In Figure 3a  we plot the trajectory of the vehicle with policy learned on clean data and poisoned
data respectively. Our attack successfully forces LQR into a policy that drives the vehicle close to the
target destination. The wiggle on the trajectory is due to the noise wt of the dynamical system. On
the poisoned data  the LQR victim learns the policy
2.41

1.97e2 1.35 1.14e2 2.42    ˆk = [ 0.01

ˆK = 1.31

9.99e3

2.02e3

which matches exactly the target policy K†  k†. In Figure 3b  we show the poisoning on rewards. Our
attack leads to very small modiﬁcation on each reward  thus the attack is efﬁcient. The total attack
cost over all 400 items is only kr  r0k2 = 0.73  which is tiny small compared to kr0k2 = 112.94.
The results here demonstrate that our attack can dramatically change the behavior of LQR by only
slightly modifying the rewards in the dataset.
Finally  for both attacks on TCE and LQR  we note that by setting the attack cost norm ↵ = 1 in (5) 
the attacker is able to obtain a sparse attack  meaning that only a small fraction of the batch data
needs to be poisoned. Such sparse attacks have profound implications in adversarial machine learning
as they can be easier to carry out and harder to detect. We show detailed results in appendix E.

1.35 ]  

(38)

6 Conclusion

We presented a policy poisoning framework against batch reinforcement learning and control. We
showed the attack problem can be formulated as convex optimization. We provided theoretical
analysis on attack feasibility and cost. Experiments show the attack can force the learner into an
attacker-chosen target policy while incurring only a small attack cost.
Acknowledgments. This work is supported in part by NSF 1545481  1561512  1623605  1704117 
1836978 and the MADLab AF Center of Excellence FA9550-18-1-0166.

9

References
[1] John Asmuth  Michael L Littman  and Robert Zinkov. Potential-based shaping in model-
based reinforcement learning. In Proceedings of the 23rd national conference on Artiﬁcial
intelligence-Volume 2  pages 604–609. AAAI Press  2008.

[2] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine

learning. Pattern Recognition  84:317–331  2018.

[3] Battista Biggio  B Nelson  and P Laskov. Poisoning attacks against support vector machines. In
29th International Conference on Machine Learning  pages 1807–1814. ArXiv e-prints  2012.

[4] Daniel S Brown and Scott Niekum. Machine teaching for inverse reinforcement learning:
Algorithms and applications. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence 
volume 33  pages 7749–7758  2019.

[5] Maya Cakmak and Manuel Lopes. Algorithmic and human teaching of sequential decision

tasks. In Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence  2012.

[6] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. On the sample

complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688  2017.

[7] Sam Michael Devlin and Daniel Kudenko. Dynamic potential-based reward shaping.

In
Proceedings of the 11th International Conference on Autonomous Agents and Multiagent
Systems  pages 433–440. IFAAMAS  2012.

[8] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research  17(83):1–5  2016.

[9] Ling Huang  Anthony D Joseph  Blaine Nelson  Benjamin IP Rubinstein  and JD Tygar. Adver-
sarial machine learning. In Proceedings of the 4th ACM workshop on Security and artiﬁcial
intelligence  pages 43–58. ACM  2011.

[10] Sandy Huang  Nicolas Papernot  Ian Goodfellow  Yan Duan  and Pieter Abbeel. Adversarial

attacks on neural network policies. arXiv preprint arXiv:1702.02284  2017.

[11] Kwang-Sung Jun  Lihong Li  Yuzhe Ma  and Jerry Zhu. Adversarial attacks on stochastic

bandits. In Advances in Neural Information Processing Systems  pages 3640–3649  2018.

[12] P Kamalaruban  R Devidze  V Cevher  and A Singla. Interactive teaching algorithms for inverse
reinforcement learning. In 28th International Joint Conference on Artiﬁcial Intelligence  pages
604–609  2019.

[13] Pang Wei Koh  Jacob Steinhardt  and Percy Liang. Stronger data poisoning attacks break data

sanitization defenses. arXiv preprint arXiv:1811.00741  2018.

[14] Bo Li  Yining Wang  Aarti Singh  and Yevgeniy Vorobeychik. Data poisoning attacks on
In Advances in neural information processing

factorization-based collaborative ﬁltering.
systems  pages 1885–1893  2016.

[15] Fang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In International

Conference on Machine Learning  pages 4042–4050  2019.

[16] Yuzhe Ma  Kwang-Sung Jun  Lihong Li  and Xiaojin Zhu. Data poisoning attacks in contextual
bandits. In International Conference on Decision and Game Theory for Security  pages 186–204.
Springer  2018.

[17] Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on

machine learners. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence  2015.

[18] Andrew Y Ng  Daishi Harada  and Stuart Russell. Policy invariance under reward transfor-
mations: Theory and application to reward shaping. In ICML  volume 99  pages 278–287 
1999.

10

[19] Yizhen Wang and Kamalika Chaudhuri. Data poisoning attacks against online learning. arXiv

preprint arXiv:1808.08994  2018.

[20] Eric Wiewiora. Potential-based shaping and q-value initialization are equivalent. Journal of

Artiﬁcial Intelligence Research  19:205–208  2003.

[21] Huang Xiao  Battista Biggio  Gavin Brown  Giorgio Fumera  Claudia Eckert  and Fabio Roli.
Is feature selection secure against training data poisoning? In International Conference on
Machine Learning  pages 1689–1698  2015.

[22] Xuezhou Zhang and Xiaojin Zhu.

arXiv:1903.01666  2019.

Online data poisoning attack.

arXiv preprint

11

,Yuzhe Ma
Xuezhou Zhang
Wen Sun
Jerry Zhu