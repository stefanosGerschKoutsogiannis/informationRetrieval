2018,On Oracle-Efficient PAC RL with Rich Observations,We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation -- accessing policy and value function classes exclusively through standard optimization primitives -- and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics  we prove that the only known sample-efficient algorithm  OLIVE  cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.,On Oracle-Efﬁcient PAC RL with Rich Observations

Christoph Dann

Carnegie Mellon University

Pittsburgh  Pennsylvania

Nan Jiang∗

UIUC

Urbana  Illinois

Akshay Krishnamurthy

Microsoft Research
New York  New York

cdann@cdann.net

nanjiang@illinois.edu

akshay@cs.umass.edu

Alekh Agarwal

Microsoft Research

Redmond  Washington

alekha@microsoft.com

John Langford

Microsoft Research
New York  New York
jcl@microsoft.com

Robert E. Schapire
Microsoft Research
New York  New York

schapire@microsoft.com

Abstract

We study the computational tractability of PAC reinforcement learning with rich
observations. We present new provably sample-efﬁcient algorithms for environ-
ments with deterministic hidden state dynamics and stochastic rich observations.
These methods operate in an oracle model of computation—accessing policy and
value function classes exclusively through standard optimization primitives—and
therefore represent computationally efﬁcient alternatives to prior algorithms that
require enumeration. With stochastic hidden state dynamics  we prove that the only
known sample-efﬁcient algorithm  OLIVE [1]  cannot be implemented in the oracle
model. We also present several examples that illustrate fundamental challenges of
tractable PAC reinforcement learning in such general settings.

1

Introduction

We study episodic reinforcement learning (RL) in environments with realistically rich observations
such as images or text  which we refer to broadly as contextual decision processes. We aim for
methods that use function approximation in a provably effective manner to ﬁnd the best possible
policy through strategic exploration.
While such problems are central to empirical RL research [2]  most theoretical results on strategic
exploration focus on tabular MDPs with small state spaces [3–10]. Comparatively little work exists
on provably effective exploration with large observation spaces that require generalization through
function approximation. The few algorithms that do exist either have poor sample complexity
guarantees [e.g.  11–14] or require fully deterministic environments [15  16] and are therefore
inapplicable to most real-world applications and modern empirical RL benchmarks. This scarcity of
positive results on efﬁcient exploration with function approximation can likely be attributed to the
challenging nature of this problem rather than a lack of interest by the research community.
On the statistical side  recent important progress was made by showing that contextual decision
processes (CDPs) with rich stochastic observations and deterministic dynamics over M hidden
states can be learned with a sample complexity polynomial in M [17]. This was followed by an
algorithm called OLIVE [1] that enjoys a polynomial sample complexity guarantee for a broader
range of CDPs  including ones with stochastic hidden state transitions. While encouraging  these
efforts focused exclusively on statistical issues  ignoring computation altogether. Speciﬁcally  the
proposed algorithms exhaustively enumerate candidate value functions to eliminate the ones that
violate Bellman equations  an approach that is computationally intractable for any function class of

∗The work was done while NJ was a postdoc researcher at MSR NYC.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

practical interest. Thus  while showing that RL with rich observations can be statistically tractable 
these results leave open the question of computational feasibility.
In this paper  we focus on this difﬁcult computational challenge. We work in an oracle model of
computation  meaning that we aim to design sample-efﬁcient algorithms whose computation can be
reduced to common optimization primitives over function spaces  such as linear programming and
cost-sensitive classiﬁcation. The oracle-based approach has produced practically effective algorithms
for active learning [18]  contextual bandits [19]  structured prediction [20  21]  and multi-class
classiﬁcation [22]  and here  we consider oracle-based algorithms for challenging RL settings.
We begin by studying the setting of Krishnamurthy et al. [17] with deterministic dynamics over
M hidden states and stochastic rich observations. In Section 4  we use cost-sensitive classiﬁcation
and linear programming oracles to develop VALOR  the ﬁrst algorithm that is both computationally
and statistically efﬁcient for this setting. While deterministic hidden-state dynamics are somewhat
restrictive  the model is considerably more general than fully deterministic MDPs assumed by prior
work [15  16]  and it accurately captures modern empirical benchmarks such as visual grid-worlds in
Minecraft [23]. As such  this method represents a considerable advance toward provably efﬁcient RL
in practically relevant scenarios.
Nevertheless  we ultimately seek efﬁcient algorithms for more general settings  such as those with
stochastic hidden-state transitions. Working toward this goal  we study the computational aspects of
the OLIVE algorithm [1]  which applies to a wide range of environments. Unfortunately  in Section 5.1 
we show that OLIVE cannot be implemented efﬁciently in the oracle model of computation. As
OLIVE is the only known statistically efﬁcient approach for this general setting  our result establishes a
signiﬁcant barrier to computational efﬁciency. In the appendix  we also describe several other barriers 
and two other oracle-based algorithms for the deterministic-dynamics setting that are considerably
different from VALOR. The negative results identify where the hardness lies while the positive results
provide a suite of new algorithmic tools. Together  these results advance our understanding of efﬁcient
reinforcement learning with rich observations.

2 Related Work

There is abundant work on strategic exploration in the tabular setting [3–10]. The computation
in these algorithms often involves planning in optimistic models and can be solved efﬁciently via
dynamic programming. To extend the theory to the more practical settings of large state spaces 
typical approaches include (1) distance-based state identity test under smoothness assumptions [e.g. 
11–14]  or (2) working with factored MDPs [e.g.  24]. The former approach is similar to the use of
state abstractions [25]  and typically incurs exponential sample complexity in state dimension. The
latter approach does have sample-efﬁcient results  but the factored representation assumes relatively
disentangled state variables which cannot model rich sensory inputs (such as images).
Azizzadenesheli et al. [26] have studied regret minimization in rich observation MDPs  a special case
of contextual decision processes with a small number of hidden states and reactive policies. They do
not utilize function approximation  and hence incur polynomial dependence on the number of unique
observations in both sample and computational complexity. Therefore  this approach  along with
related works [27  28]  does not scale to the rich observation settings that we focus on here.
Wen and Van Roy [15  16] have studied exploration with function approximation in fully deterministic
MDPs  which is considerably more restrictive than our setting of deterministic hidden state dynamics
with stochastic observations and rewards. Moreover  their analysis measures representation com-
plexity using eluder dimension [29  30]  which is only known to be small for some simple function
classes. In comparison  our bounds scale with more standard complexity measures and can easily
extend to VC-type quantities  which allows our theory to apply to practical and popular function
approximators including neural networks [31].

3 Setting and Background

We consider reinforcement learning (RL) in a common special case of contextual decision pro-
cesses [17  1]  sometimes referred to as rich observation MDPs [26]. We assume an H-step process
where in each episode  a random trajectory s1  x1  a1  r1  s2  x2  . . .   sH   xH   aH   rH is generated.

2

Figure 1: Graphical representation of the problem class considered by our algorithm  VALOR: The
main assumptions that enable sample-efﬁcient learning are (1) that the small hidden state sh is
identiﬁable from the rich observation xh and (2) that the next state is a deterministic function of
the previous state and action. State and observation examples are from https://github.com/
Microsoft/malmo-challenge.

For each time step (or level) h ∈ [H]  sh ∈ S where S is a ﬁnite hidden state space  xh ∈ X where X
is the rich observation (context) space  ah ∈ A where A is a ﬁnite action space of size K  and rh ∈ R.
Each hidden state s ∈ S is associated with an emission process Os ∈ ∆(X )  and we use x ∼ s as a
shorthand for x ∼ Os. We assume that each rich observation contains enough information so that s
can in principle be identiﬁed just from x ∼ Os—hence x is a Markov state and the process is in fact
an MDP over X —but the mapping x (cid:55)→ s is unavailable to the agent and s is never observed. The
hidden states S introduce structure into the problem  which is essential since we allow the observation
space X to be inﬁnitely large.2 The issue of partial observability is not the focus of the paper.
Let Γ : S × A → ∆(S) deﬁne transition dynamics over the hidden states  and let Γ1 ∈ ∆(S)
denote an initial distribution over hidden states. R : X × A → ∆(R) is the reward function;
this differs from partially observable MDPs where reward depends only on s  making the problem
more challenging. With this notation  a trajectory is generated as follows: s1 ∼ Γ1  x1 ∼ s1 
r1 ∼ R(x1  a1)  s2 ∼ Γ(s1  a1)  x2 ∼ s2  . . .   sH ∼ Γ(sH−1  aH−1)  xH ∼ sH  rH ∼ R(xH   aH ) 
with actions a1:H chosen by the agent. We emphasize that s1:H are unobservable to the agent.
To simplify notation  we assume that each observation and hidden state can only appear at a particular
level. This implies that S is partitioned into {Sh}H
h=1 with size M := maxh∈[H] |Sh|. For regularity 
h=1 rh ≤ 1 almost surely.

assume rh ≥ 0 and(cid:80)H
V π := E[(cid:80)H
optimal value function g(cid:63) deﬁned as g(cid:63)(x) := E[(cid:80)H

In this setting  the learning goal is to ﬁnd a policy π : X → A that maximizes the expected return
h=1 rh | a1:H ∼ π]. Let π(cid:63) denote the optimal policy  which maximizes V π  with
h(cid:48)=h rh(cid:48)|xh = x  ah:H ∼ π(cid:63)]. As is standard  g(cid:63)

satisﬁes the Bellman equation: ∀x at level h 

g(cid:63)(x) = max

a∈A E[rh + g(cid:63)(xh+1)|xh = x  ah = a] 

Q(cid:63)(x  a) := E[(cid:80)H

with the understanding that g(cid:63)(xH+1) ≡ 0. A similar equation holds for the optimal Q-value function

h(cid:48)=h rh(cid:48)|xh = x  ah = a  ah+1:H ∼ π(cid:63)]  and π(cid:63) = argmaxa∈A Q(cid:63)(x  a).3

Below are two special cases of the setting described above that will be important for later discussions.
Tabular MDPs: An MDP with a ﬁnite and small state space is a special case of this model  where
X = S and Os is the identity map for each s. This setting is relevant in our discussion of oracle-
efﬁciency of the existing OLIVE algorithm in Section 5.1.
Deterministic dynamics over hidden states: Our algorithm  VALOR  works in this special case 
which requires Γ1 and Γ(s  a) to be point masses. Originally proposed by Krishnamurthy et al. [17] 

2Indeed  the lower bound in Proposition 6 in Jiang et al. [1] show that ignoring underlying structure precludes

provably-efﬁcient RL  even with function approximation.

3Note that the optimal policy and value functions depend on x and not just s even if s was known  since

reward is a function of x.

3

...small hidden staterich observationstate indenti(cid:31)able from observation but mapping unknownAssumption:timethis setting can model some challenging benchmark environments in modern reinforcement learning 
including visual grid-worlds common to the deep RL literature [e.g.  23]. In such tasks  the state
records the position of each game element in a grid but the agent observes a rendered 3D view.
Figure 1 shows a visual summary of this setting. We describe VALOR in detail in Section 4.
Throughout the paper  we use ˆED[·] to denote empirical expectation over samples from a data set D.

3.1 Function Classes and Optimization Oracles
As X can be rich  the agent must use function approximation to generalize across observations. To
that end  we assume a given value function class G ⊂ (X → [0  1]) and policy class Π ⊂ (X → A).
Our algorithm is agnostic to the speciﬁc function classes used  but for the guarantees to hold  they
must be expressive enough to represent the optimal value function and policy  that is  π(cid:63) ∈ Π and
g(cid:63) ∈ G. Prior works often use F ⊂ (X × A → [0  1]) to approximate Q(cid:63) instead  but for example
Jiang et al. [1] point out that their OLIVE algorithm can equivalently work with G and Π. This (G  Π)
representation is useful in resolving the computational difﬁculty in the deterministic setting  and has
also been used in practice [32].
When working with large and abstract function classes as we do here  it is natural to consider an
oracle model of computation and assume that these classes support various optimization primitives.
We adopt this oracle-based approach here  and speciﬁcally use the following oracles:
Cost-Sensitive Classiﬁcation (CSC) on Policies. A cost-sensitive classiﬁcation (CSC) oracle
receives as inputs a parameter sub and a sequence {(x(i)  c(i))}i∈[n] of observations x(i) ∈ X and cost
vectors c(i) ∈ RK  where c(i)(a) is the cost of predicting action a ∈ A for x(i). The oracle returns a
policy whose average cost is within sub of the minimum average cost  minπ∈Π
i=1 c(i)(π(x(i))).
While CSC is NP-hard in the worst case  CSC can be further reduced to binary classiﬁcation [33  34]
for which many practical algorithms exist and actually form the core of empirical machine learning.
As further motivation  the CSC oracle has been used in practically effective algorithms for contextual
bandits [35  19]  imitation learning [20]  and structured prediction [21].
Linear Programs (LP) on Value Functions. A linear program (LP) oracle considers an optimiza-
tion problem where the objective o : G → R and the constraints h1  . . . hm are linear functionals of G
i=1 αig(xi)
with coefﬁcients {αi}i∈[n] and contexts {xi}i∈[n]. Formally  for a program of the form

generated by ﬁnitely many function evaluations. That is  o and each hj have the form(cid:80)n

(cid:80)n

1
n

maxg∈G o(g)  subject to hj(g) ≤ cj  ∀j ∈ [m] 

with constants {cj}j∈[m]  an LP oracle with approximation parameters sub  feas returns a function ˆg
that is at most sub-suboptimal and that violates each constraint by at most feas. For intuition  if the
value functions G are linear with parameter vector θ ∈ Rd  i.e.  g(x) = (cid:104)θ  x(cid:105)  then this reduces to a
linear program in Rd for which a plethora of provably efﬁcient solvers exist. Beyond the linear case 
such problems can be practically solved using standard continuous optimization methods. LP oracles
are also employed in prior work focusing on deterministic MDPs [15  16].
Least-Squares (LS) Regression on Value Functions. We also consider a least-squares regression
(LS) oracle that returns the value function which minimizes a square-loss objective. Since VALOR
does not use this oracle  we defer details to the appendix.
We deﬁne the following notion of oracle-efﬁciency based on the optimization primitives above.
Deﬁnition 1 (Oracle-Efﬁcient). An algorithm is oracle-efﬁcient if it can be implemented with polyno-
mially many basic operations and calls to CSC  LP  and LS oracles.

Note that our algorithmic results continue to hold if we include additional oracles in the deﬁnition 
while our hardness results easily extend  provided that the new oracles can be efﬁciently implemented
in the tabular setting (i.e.  they satisfy Proposition 6; see Section 5).

4 VALOR: An Oracle-Efﬁcient Algorithm

In this section we propose and analyze a new algorithm  VALOR (Values stored Locally for RL)
shown in Algorithm 1 (with 2 & 3 as subroutines). As we will show  this algorithm is oracle-efﬁcient

4

and enjoys a polynomial sample-complexity guarantee in the deterministic hidden-state dynamics
setting described earlier  which was originally introduced by Krishnamurthy et al. [17].

Algorithm 2: Subroutine: Policy optimization
with local values
1 Function polvalfun()
2
3

ˆV (cid:63) ← V of the only dataset in D1;
for h = 1 : H do
// CSC-oracle
ˆπh ← argmax
π∈Πh

(D V {Va})∈Dh

(cid:80)

VD(π;{Va});

4

5

return ˆπ1:H   ˆV (cid:63);

// Alg.3

Notation:
VD(π;{Va}) := ˆED[K1{π(x) = a}(r + Va)]

// see exact values in Table 1 in the appendix
// accuracy of learned values at level h

Algorithm 1: Main Algorithm VALOR
1 Global: D1  . . .DH initialized as ∅;
2 Function MetaAlg
dfslearn (∅) ;
3
for k = 1  . . .   M H do
4
5
6

ˆπ(k)  ˆV (k) ← polvalfun() ; // Alg.2
T ← sample neval trajectories with ˆπ(k);
ˆV ˆπ(k) ← average return of T ;
if ˆV (k) ≤ ˆV ˆπ(k)
for h = 1 . . . H − 1 do

2 then return ˆπ(k) ;

for all a1:h of nexpl traj. ∈ T do

+ 

// Alg.3

dfslearn (a1:h) ;

return failure;

Algorithm 3: Subroutine: DFS Learning of local values
1 feas = sub = stat = ˜O(2/M H 3) ;
2 φh = (H + 1 − h)(6stat + 2sub + feas) ;
3 Function dfslearn(path p with length h − 1)
4
5

for a ∈ A do

ˆED(cid:48)[g(xh+1)]

D(cid:48) ← Sample ntest trajectories with actions p ◦ a ;
Vopt ← maxg∈Gh+1
if |Vopt − Vpes| ≤ 2φh+1 + 4stat + 2feas then
else

s.t. ∀(D  V  ) ∈ Dh+1 :

Va ← (Vopt + Vpes)/2 ;
Va ← dfslearn(p ◦ a) ;

˜D ← Sample ntrain traj. with p and ah ∼ Unif(K);
˜V ← maxπ∈Πh V ˜D(π;{Va});
Add ( ˜D  ˜V  {Va}a∈A) to Dh;
return ˜V ;

7

8
9
10
11

12

6

7
8
9
10

11
12
13
14

// compute optimistic / pessimistic values using LP-oracle

(and Vpes ← ming∈Gh+1
|V − ˆED[g(xh+1)]| ≤ φh+1 ;

ˆED(cid:48)[g(xh+1)])

// consensus among remaining functions

// no consensus  descend

// CSC-oracle

Since hidden states can be deterministically reached by sequences of actions (or paths)  from an
algorithmic perspective  the process can be thought of as an exponentially large tree where each
node is associated with a hidden state (such association is unknown to the agent). Similar to LSVEE
[17]  VALOR ﬁrst explores this tree (Line 3) with a form of depth ﬁrst search (Algorithm 3). To
avoid visiting all of the exponentially many paths  VALOR performs a state identity test (Algorithm 3 
Lines 5–8): the data collected so far is used to (virtually) eliminate functions in G (Algorithm 3 
Line 6)  and we do not descend to a child if the remaining functions agree on the value of the child
node (Algorithm 3  Line 7).
The state identity test prevents exploring the same hidden state twice but might also incorrectly
prune unvisited states if all functions happen to agree on the value. Unfortunately  with no data
from such pruned states  we are unable to learn the optimal policy on them. To address this issue 
after dfslearn returns  we ﬁrst use the stored data and values (Line 5) to compute a policy (see
Algorithm 2) that is near optimal on all explored states. Then  VALOR deploys the computed policy
(Line 6) and only terminates if the estimated optimal value is achieved (Line 8). If not  the policy
has good probability of visiting those accidentally pruned states (see Appendix B.5)  so we invoke
dfslearn on the generated paths to complement the data sets (Line 11).

5

In the rest of this section we describe VALOR in more detail  and then state its statistical and
computational guarantees. VALOR follows a dynamic programming style and learns in a bottom-up
fashion. As a result  even given stationary function classes (G  Π) as inputs  the algorithm can return
a non-stationary policy ˆπ1:H := (ˆπ1  . . .   ˆπH ) ∈ ΠH that may use different policies at different time
steps.4 To avoid ambiguity  we deﬁne Πh := Π and Gh := G for h ∈ [H]  to emphasize the time
point h under consideration. For convenience  we also deﬁne GH+1 to be the singleton {x (cid:55)→ 0}.
This notation also allows our algorithms to handle more general non-stationary function classes.
Details of depth-ﬁrst search exploration. VALOR maintains many data sets collected at paths
visited by dfslearn. Each data set D is collected from some path p  which leads to some hidden
state s. (Due to determinism  we will refer to p and s interchangeably throughout this section.) D
consists of tuples (x  a  r) where x ∼ p (i.e.  x ∼ Os)  a ∼ Unif(K)  and r is the instantaneous
reward. Associated with D  we also store a scalar V which approximates V (cid:63)(s)  and {Va}a∈A
which approximate {V (cid:63)(s ◦ a)}a∈A  where s ◦ a denotes the state reached when taking a in s. The
estimates {Va}a∈A of the future optimal values associated with the current path p ∈ Ah−1 are
either determined through a recursive call (Line 10)  or through a state-identity test (Lines 5–8 in
dfslearn). To check if we already know V (cid:63)(p ◦ a)  we solve constrained optimization problems to
compute optimistic and pessimistic estimates  using a small amount of data from p◦a. The constraints
eliminate all g ∈ Gh+1 that make incorrect predictions for V (cid:63)(s(cid:48)) for any previously visited s(cid:48) at
level h + 1. As such  if we have learned the value of s ◦ a on a different path  the optimistic and
pessimistic values must agree (“consensus”)  so we need not descend. Once we have the future values
Va  the value estimate ˜V (which approximates V (cid:63)(s)) is computed (in Line 12) by maximizing the
sum of immediate reward and future values  re-weighted using importance sampling to reﬂect the
policy under consideration π:

VD(π;{Va}) := ˆED[K1{π(x) = a}(r + Va)].

(1)

Details of policy optimization and exploration-on-demand. polvalfun performs a sequence of
policy optimization steps using all the data sets collected so far to ﬁnd a non-stationary policy that is
near-optimal at all explored states simultaneously. Note that this policy differs from that computed in
(Alg. 3  Line 12) as it is common for all datasets at a level h. And ﬁnally using this non-stationary
policy  MetaAlg estimates its suboptimality and either terminates successfully  or issues several other
calls to dfslearn to gather more data sets. This so-called exploration-on-demand scheme is due
to Krishnamurthy et al. [17]  who describe the subroutine in more detail.

4.1 What is new compared to LSVEE?

The overall structure of VALOR is similar to LSVEE [17]. The main differences are in the pruning
mechanism  where we use a novel state-identity test  and the policy optimization step in Algorithm 2.
LSVEE uses a Q-value function class F ⊂ (X × A → [0  1]) and a state identity test based on
Bellman errors on data sets D consisting of (x  a  r  x(cid:48)) tuples:

(cid:17)2(cid:21)

.

(cid:20)(cid:16)

ˆED

f (x  a) − r − ˆEx(cid:48)∼a maxa(cid:48)∈A f (x(cid:48)  a(cid:48))

This enables a conceptually simpler statistical analysis  but the coupling between value function and
the policy yield challenging optimization problems that do not obviously admit efﬁcient solutions.
In contrast  VALOR uses dynamic programming to propagate optimal value estimates from future
to earlier time points. From an optimization perspective  we ﬁx the future value and only optimize
the current policy  which can be implemented by standard oracles  as we will see. However  from a
statistical perspective  the inaccuracy of the future value estimates leads to bias that accumulates over
levels. By a careful design of the algorithm and through an intricate and novel analysis  we show
that this bias only accumulates linearly (as opposed to exponentially; see e.g.  Appendix E.1)  which
leads to a polynomial sample complexity guarantee.

4This is not rare in RL; see e.g.  Chapter 3.4 of Ross [36].

6

4.2 Computational and Sample Complexity of VALOR

VALOR requires two types of nontrivial computations over the function classes. We show that they
can be reduced to CSC on Π and LP on G (recall Section 3.1)  respectively  and hence VALOR is
oracle-efﬁcient.
First  Lines 4 in polvalfun and 12 in dfslearn involve optimizing VD(π;{Va}) (Eq. (1)) over Π 
which can be reduced to CSC as follows: We ﬁrst form tuples (x(i)  a(i)  y(i)) from D and {Va}
on which VD(π;{Va}) depends  where we bind xh to x(i)  ah to a(i)  and rh + Vah to y(i). From
the tuples  we construct a CSC data set (x(i) −[K1{a = a(i)}y(i)]a∈A). On this data set  the
cost-sensitive error of any policy (interpreted as a classiﬁer) is exactly −VD(π;{Va})  so minimizing
error (which the oracle does) maximizes the original objective.
Second  the state identity test requires solving the following problem over the function class G:

Vopt = max
g∈G

ˆED(cid:48)[g(xh)]

(and min for Vpes)

(2)

s.t. V − φh ≤ ˆED[g(xh)] ≤ V + φh ∀(D  V ) ∈ Dh.

The objective and the constraints are linear functionals of G  all empirical expectations involve
polynomially many samples  and the number of constraints is |Dh| which remains polynomial
throughout the execution of the algorithm  as we will show in the sample complexity analysis.
Therefore  the LP oracle can directly handle this optimization problem.
We now formally state the main computational and statistical guarantees for VALOR.
Theorem 2 (Oracle efﬁciency of VALOR). Consider a contextual decision process with deterministic
dynamics over M hidden states as described in Section 3. Assume π(cid:63) ∈ Π and g(cid:63) ∈ G. Then for any
  δ ∈ (0  1)  with probability at least 1 − δ  VALOR makes O
CSC oracle calls and
at most O
Theorem 3 (PAC bound of VALOR). Under the same setting and assumptions as in Theorem 2 
VALOR returns a policy ˆπ such that V (cid:63) − V ˆπ ≤  with probability at least 1 − δ  after collecting at
most ˜O

LP oracle calls with required accuracy feas = sub = ˜O(2/M H 3).

log(|G||Π|/δ) log3(1/δ)

(cid:16) M KH 2
(cid:16) M 3H 8K



(cid:16) M H 2



(cid:17)

log M H
δ

(cid:17)

log M H
δ

trajectories.5

(cid:17)

5

Note that this bound assumes ﬁnite value function and policy classes for simplicity  but can be
extended to inﬁnite function classes with bounded statistical complexity using standard tools  as in
Section 5.3 of Jiang et al. [1]. The resulting bound scales linearly with the Natarajan and Pseudo-
dimension of the function classes  which are generalizations of VC-dimension. We further expect that
one can generalize the theorems above to an approximate version of realizability as in Section 5.4
of Jiang et al. [1].
Compared to the guarantee for LSVEE [17]  Theorem 3 is worse in the dependence on M  H  and .
Yet  in Appendix B.7 we show that a version of VALOR with alternative oracle assumptions enjoys a
better PAC bound than LSVEE. Nevertheless  we emphasize that our main goal is to understand the
interplay between statistical and computational efﬁciency to discover new algorithmic ideas that may
lead to practical methods  rather than improve sample complexity bounds.

5 Toward Oracle-Efﬁcient PAC-RL with Stochastic Hidden State Dynamics

VALOR demonstrates that provably sample- and oracle-efﬁcient RL with rich stochastic observations
is possible and  as such  makes progress toward reliable and practical RL in many applications. In
this section  we discuss the natural next step of allowing stochastic hidden-state transitions.

5.1 OLIVE is not Oracle-Efﬁcient

For this more general setting with stochastic hidden state dynamics  OLIVE [1] is the only known
algorithm with polynomial sample complexity  but its computational properties remain underexplored.
5 ˜O(·) suppresses logarithmic dependencies on M  K  H  1/ and doubly-logarithmic dependencies on 1/δ 

|G|  and |Π|.

7

We show here that OLIVE is in fact not oracle-efﬁcient. A brief description of the algorithm is
provided below  and in the theorem statement  we refer to a parameter φ  which the algorithm uses as
a tolerance on deviations of empirical expectations.
Theorem 4. Assuming P (cid:54)= N P   even with algorithm parameter φ = 0 and perfect evaluation of
expectations  OLIVE is not oracle-efﬁcient  that is  it cannot be implemented with polynomially many
basic arithmetic operations and calls to CSC  LP  and LS oracles.

The assumptions of perfect evaluation of expectations and φ = 0 are merely to unclutter the
constructions in the proofs. We show this result by proving that even in tabular MDPs  OLIVE solves
an NP-hard problem to determine its next exploration policy  while all oracles we consider have
polynomial runtime in the tabular setting. While we only show this for CSC  LP  and LS oracles
explicitly  we expect other practically relevant oracles to also be efﬁcient in the tabular setting  and
therefore they could not help to implement OLIVE efﬁciently.
This theorem shows that there are no known oracle-efﬁcient PAC-RL methods for this general setting
and that simply applying clever optimization tricks to implement OLIVE is not enough to achieve a
practical algorithm. Yet  this result does not preclude tractable PAC RL altogether  and we discuss
plausible directions in the subsequent section. Below we highlight the main arguments of the proof.
Proof Sketch of Theorem 4.
OLIVE is round-based and follows the optimism in the face of
uncertainty principle. At round k it selects a value function and a policy to execute (ˆgk  ˆπk) that
promise the highest return while satisfying all average Bellman error constraints:

ˆgk  ˆπk = argmax
g∈G π∈Π

ˆED0 [g(x)]

(3)

s.t.

| ˆEDi[K1{a = π(x)}(g(x) − r − g(x(cid:48)))]| ≤ φ  ∀ Di∈D.

Here D0 is a data set of initial contexts x  D consists of data sets of (x  a  r  x(cid:48)) tuples collected in
the previous rounds  and φ is a statistical tolerance parameter. If this optimistic policy ˆπk is close to
optimal  OLIVE returns it and terminates. Otherwise we add a constraint to (3) by (i) choosing a time
point h  (ii) collecting trajectories with ˆπk but choosing the h-th action uniformly  and (iii) storing the
tuples (xh  ah  rh  xh+1) in the new data set Dk which is added to the constraints for the next round.
The following theorem shows that OLIVE’s optimization is NP-hard even in tabular MDPs.
Theorem 5. Let POLIVE denote the family of problems of the form (3)  parameterized by (X  A  Env  t) 
which describes the optimization problem induced by running OLIVE in the MDP Env (with states
X   actions A  and perfect evaluation of expectations) for t rounds. OLIVE is given tabular function
classes G = (X → [0  1]) and Π = (X → A) and uses φ = 0. Then POLIVE is NP-hard.
At the same time  oracles are implementable in polynomial time:
Proposition 6. For tabular value functions G = (X → [0  1]) and policies Π = (X → A)  the CSC 
LP  and LS oracles can be implemented in time polynomial in |X|  K = |A| and the input size.
Both proofs are in Appendix D. Proposition 6 implies that if OLIVE could be implemented with
polynomially many CSC/LP/LS oracle calls  its total runtime would be polynomial for tabular MDPs.
Assuming P (cid:54)= NP  this contradicts Theorem 5 which states that determining the exploration policy of
OLIVE in tabular MDPs is NP-hard. Combining both statements therefore proves Theorem 4.
We now give brief intuition for Proposition 6. To implement the CSC oracle  for each of the
polynomially many observations x ∈ X   we simply add the cost vectors for that observation
together and pick the action that minimizes the total cost  that is  compute the action ˆπ(x) as
i∈[n]: x(i)=x c(i)(a). Similarly  the square-loss objective of the LS-oracle decomposes
and we can compute the tabular solution one entry at a time. In both cases  the oracle runtime
is O(nK|X|). Finally  using one-hot encoding  G can be written as a linear function in R|X| for
which the LP oracle problem reduces to an LP in R|X|. The ellipsoid method [37] solves these
approximately in polynomial time.

mina∈A(cid:80)

5.2 Computational Barriers with Decoupled Learning Rules.

One factor contributing to the computational intractability of OLIVE is that (3) involves optimizing
over policies and values jointly.
It is therefore promising to look for algorithms that separate

8

optimizations over policies and values  as in VALOR. In Appendix E  we provide a series of examples
that illustrate some limitations of such algorithms. First  we show that methods that compute optimal
values iteratively in the style of ﬁtted value iteration [38] need additional assumptions on G and Π
besides realizability (Theorem 45). (Storing value estimates of states explicitly allows VALOR to only
require realizability.) Second  we show that with stochastic state dynamics  average value constraints 
as in Line 6 of Algorithm 3  can cause the algorithm to miss a high-value state (Proposition 46).
Finally  we show that square-loss constraints suffer from similar problems (Proposition 47).

5.3 Alternative Algorithms.

An important element of VALOR is that it explicitly stores value estimates of the hidden states 
which we call “local values.” Local values lead to statistical and computational efﬁciency under weak
realizability conditions  but this approach is unlikely to generalize to the stochastic setting where
the agent may not be able to consistently visit a particular hidden state. In Appendices B.7-C.2  we
therefore derive alternative algorithms which do not store local values to approximate the future
value g(cid:63)(xh+1). Inspired by classical RL algorithms  these algorithms approximate g(cid:63)(xh+1) by
either bootstrap targets ˆgh+1(xh+1) (as in TD methods) or Monte-Carlo estimates of the return
using a near-optimal roll-out policy ˆπh+1:H (as in PSDP [39]). Using such targets can introduce
additional errors  and stronger realizability-type assumptions on Π G are necessary for polynomial
sample-complexity (see Appendix C and E). Nevertheless  these algorithms are also oracle-efﬁcient
and while we only establish statistical efﬁciency with deterministic hidden state dynamics  we believe
that they considerably expand the space of plausible algorithms for the general setting.

6 Conclusion

This paper describes new RL algorithms for environments with rich stochastic observations and
deterministic hidden state dynamics. Unlike other existing approaches  these algorithms are com-
putationally efﬁcient in an oracle model  and we emphasize that the oracle-based approach has led
to practical algorithms for many other settings. We believe this work represents an important step
toward computationally and statistically efﬁcient RL with rich observations.
While challenging benchmark environments in modern RL (e.g. visual grid-worlds [23]) often have
the assumed deterministic hidden state dynamics  the natural goal is to develop efﬁcient algorithms
that handle stochastic hidden-state dynamics. We show that the only known approach for this setting
is not implementable with standard oracles  and we also provide several constructions demonstrating
other concrete challenges of RL with stochastic state dynamics. This provides insights into the key
open question of whether we can design an efﬁcient algorithm for the general setting. We hope to
resolve this question in future work.

References
[1] Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  John Langford  and Robert E. Schapire.
Contextual decision processes with low Bellman rank are PAC-learnable. In International
Conference on Machine Learning  2017.

[2] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G.
Bellemare  Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Pe-
tersen  Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan
Wierstra  Shane Legg  and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature  2015.

[3] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine Learning  2002.

[4] Ronen I. Brafman and Moshe Tennenholtz. R-max – a general polynomial time algorithm for

near-optimal reinforcement learning. Journal of Machine Learning Research  2003.

[5] Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval

estimation. In International Conference on Machine learning  2005.

9

[6] Alexander L. Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L. Littman. PAC
model-free reinforcement learning. In International Conference on Machine Learning  2006.

[7] Peter Auer  Thomas Jaksch  and Ronald Ortner. Near-optimal regret bounds for reinforcement

learning. In Advances in Neural Information Processing Systems  2009.

[8] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforce-

ment learning. In Advances in Neural Information Processing Systems  2015.

[9] Mohammad Gheshlaghi Azar  Ian Osband  and R´emi Munos. Minimax regret bounds for

reinforcement learning. In International Conference on Machine Learning  2017.

[10] Christoph Dann  Tor Lattimore  and Emma Brunskill. Unifying PAC and regret: Uniform PAC
bounds for episodic reinforcement learning. In Advances in Neural Information Processing
Systems  2017.

[11] Sham M. Kakade  Michael Kearns  and John Langford. Exploration in metric state spaces. In

International Conference on Machine Learning  2003.

[12] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space Markov decision

processes. In AAAI Conference on Artiﬁcial Intelligence  2013.

[13] Robert Grande  Thomas Walsh  and Jonathan How. Sample efﬁcient reinforcement learning

with gaussian processes. In International Conference on Machine Learning  2014.

[14] Jason Pazis and Ronald Parr. Efﬁcient PAC-optimal exploration in concurrent  continuous state

MDPs with delayed updates. In AAAI Conference on Artiﬁcial Intelligence  2016.

[15] Zheng Wen and Benjamin Van Roy. Efﬁcient exploration and value function generalization in

deterministic systems. In Advances in Neural Information Processing Systems  2013.

[16] Zheng Wen and Benjamin Van Roy. Efﬁcient reinforcement learning in deterministic systems

with value function generalization. Mathematics of Operations Research  2017.

[17] Akshay Krishnamurthy  Alekh Agarwal  and John Langford. PAC reinforcement learning with

rich observations. In Advances in Neural Information Processing Systems  2016.

[18] Daniel Joseph Hsu. Algorithms for active learning. PhD thesis  UC San Diego  2010.

[19] Alekh Agarwal  Daniel Hsu  Satyen Kale  John Langford  Lihong Li  and Robert E. Schapire.
Taming the monster: A fast and simple algorithm for contextual bandits. In International
Conference on Machine Learning  2014.

[20] Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive

no-regret learning. arXiv:1406.5979  2014.

[21] Kai-Wei Chang  Akshay Krishnamurthy  Alekh Agarwal  Hal Daume III  and John Langford.
Learning to search better than your teacher. In International Conference on Machine Learning 
2015.

[22] Erin L. Allwein  Robert E. Schapire  and Yoram Singer. Reducing multiclass to binary: A

unifying approach for margin classiﬁers. Journal of Machine Learning Research  2000.

[23] Matthew Johnson  Katja Hofmann  Tim Hutton  and David Bignell. The Malmo Platform
In International Joint Conference on Artiﬁcial

for artiﬁcial intelligence experimentation.
Intelligence  2016.

[24] Michael Kearns and Daphne Koller. Efﬁcient reinforcement learning in factored MDPs. In

International Joint Conference on Artiﬁcial Intelligence  1999.

[25] Lihong Li  Thomas J. Walsh  and Michael L. Littman. Towards a uniﬁed theory of state
abstraction for MDPs. In International Symposium on Artiﬁcial Intelligence and Mathematics 
2006.

10

[26] Kamyar Azizzadenesheli  Alessandro Lazaric  and Animashree Anandkumar. Reinforcement

learning in rich-observation MDPs using spectral methods. arXiv:1611.03907  2016.

[27] Kamyar Azizzadenesheli  Alessandro Lazaric  and Animashree Anandkumar. Reinforcement

learning of POMDPs using spectral methods. In Conference on Learning Theory  2016.

[28] Zhaohan Daniel Guo  Shayan Doroudi  and Emma Brunskill. A PAC RL algorithm for episodic

POMDPs. In Artiﬁcial Intelligence and Statistics  2016.

[29] Dan Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic

exploration. In Advances in Neural Information Processing Systems  2013.

[30] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder

dimension. In Advances in Neural Information Processing Systems  2014.

[31] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations.

Cambridge University Press  2009.

[32] Bo Dai  Albert Shaw  Lihong Li  Lin Xiao  Niao He  Zhen Liu  Jianshu Chen  and Le Song.
Sbeed: Convergent reinforcement learning with nonlinear function approximation. In Interna-
tional Conference on Machine Learning  pages 1133–1142  2018.

[33] Alina Beygelzimer  John Langford  and Pradeep Ravikumar. Error-correcting tournaments. In

International Conference on Algorithmic Learning Theory  2009.

[34] John Langford and Alina Beygelzimer. Sensitive error correcting output codes. In International

Conference on Computational Learning Theory  2005.

[35] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems  2008.

[36] Stephane Ross. Interactive learning for sequential decisions and predictions. PhD thesis 

Carnegie Mellon University  2013.

[37] Leonid G Khachiyan. Polynomial algorithms in linear programming. USSR Computational

Mathematics and Mathematical Physics  1980.

[38] Geoffrey J Gordon. Stable function approximation in dynamic programming. In International

Conference on Machine Learning  1995.

[39] J Andrew Bagnell  Sham M Kakade  Jeff G Schneider  and Andrew Y Ng. Policy search by

dynamic programming. In Advances in Neural Information Processing Systems  2004.

[40] Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a

meta-algorithm and applications. Theory of Computing  2012.

[41] R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of

Machine Learning Research  2008.

[42] Andr´as Antos  Csaba Szepesv´ari  and R´emi Munos. Learning near-optimal policies with
Bellman-residual minimization based ﬁtted policy iteration and a single sample path. Machine
Learning  2008.

[43] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Sch¨olkopf  and Alexander J.

Smola. A kernel two-sample test. Journal of Machine Learning Research  2012.

[44] Bernhard Sch¨olkopf and Alexander J. Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT Press  2002.

[45] Martin Gr¨otschel  L´aszl´o Lov´asz  and Alexander Schrijver. The ellipsoid method and its

consequences in combinatorial optimization. Combinatorica  1981.

[46] Damien Ernst  Pierre Geurts  and Louis Wehenkel. Tree-based batch mode reinforcement

learning. Journal of Machine Learning Research  2005.

[47] Amir-Massoud Farahmand  Csaba Szepesv´ari  and R´emi Munos. Error propagation for ap-
proximate policy and value iteration. In Advances in Neural Information Processing Systems 
2010.

11

,Christoph Dann
Nan Jiang
Akshay Krishnamurthy
Alekh Agarwal
John Langford
Robert Schapire