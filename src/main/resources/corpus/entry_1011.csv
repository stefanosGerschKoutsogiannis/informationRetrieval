2016,Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning,We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly)  yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.,Combining Adversarial Guarantees and
Stochastic Fast Rates in Online Learning

Centrum Wiskunde & Informatica

CWI and Leiden University

pdg@cwi.nl

Wouter M. Koolen

Peter GrÃ¼nwald

Science Park 123  1098 XG
Amsterdam  the Netherlands

wmkoolen@cwi.nl

Tim van Erven
Leiden University

Niels Bohrweg 1  2333 CA

Leiden  the Netherlands
tim@timvanerven.nl

Abstract

We consider online learning algorithms that guarantee worst-case regret rates
in adversarial environments (so they can be deployed safely and will perform
robustly)  yet adapt optimally to favorable stochastic environments (so they will
perform well in a variety of settings of practical importance). We quantify the
friendliness of stochastic environments by means of the well-known Bernstein
(a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint
for the Hedge setting and MetaGrad for online convex optimization) we show that
the particular form of their data-dependent individual-sequence regret guarantees
implies that they adapt automatically to the Bernstein parameters of the stochastic
environment. We prove that these algorithms attain fast rates in their respective
settings both in expectation and with high probability.

1

Introduction

âˆš

We consider online sequential decision problems. We focus on full information settings  encompassing
such interaction protocols as online prediction  classiï¬cation and regression  prediction with expert
advice or the Hedge setting  and online convex optimization (see Cesa-Bianchi and Lugosi 2006). The
goal of the learner is to choose a sequence of actions with small regret  i.e. such that his cumulative
loss is not much larger than the loss of the best ï¬xed action in hindsight. This has to hold even in
the worst case  where the environment is controlled by an adversary. After three decades of research
there exist many algorithms and analysis techniques for a variety of such settings. For many settings 
adversarial regret lower bounds of order
T are known  along with matching individual sequence
algorithms [Shalev-Shwartz  2011].
A more recent line of development is to design adaptive algorithms with regret guarantees that scale
with some more reï¬ned measure of the complexity of the problem. For the Hedge setting  results of
this type have been obtained  amongst others  by Cesa-Bianchi et al. [2007]  De Rooij et al. [2014] 
Gaillard et al. [2014]  Sani et al. [2014]  Even-Dar et al. [2008]  Koolen et al. [2014]  Koolen and
Van Erven [2015]  Luo and Schapire [2015]  Wintenberger [2015]. Interestingly  the price for such
adaptivity (i.e. the worsening of the worst-case regret bound) is typically extremely small (i.e. a
constant factor in the regret bound). For online convex optimization (OCO)  many different types of
adaptivity have been explored  including by Crammer et al. [2009]  Duchi et al. [2011]  McMahan
and Streeter [2010]  Hazan and Kale [2010]  Chiang et al. [2012]  Steinhardt and Liang [2014] 
Orabona et al. [2015]  Van Erven and Koolen [2016].
Here we are interested in the question of whether such adaptive results are strong enough to lead to
improved rates in the stochastic case when the data follow a â€œfriendlyâ€ distribution. In speciï¬c cases
it has been shown that fancy guarantees do imply signiï¬cantly reduced regret. For example Gaillard
et al. [2014] present a generic argument showing that a certain kind of second-order regret guarantees

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

implies constant expected regret (the fastest possible rate) for i.i.d. losses drawn from a distribution
with a gap (between expected loss of the best and all other actions). In this paper we signiï¬cantly
extend this result. We show that a variety of individual-sequence second-order regret guarantees
imply fast regret rates for distributions under much milder stochastic assumptions. In particular  we
will look at the Bernstein condition (see Bartlett and Mendelson 2006)  which is the key to fast rates
in the batch setting. This condition provides a parametrised interpolation (expressed in terms of the

Bernstein exponent Îºâˆˆ[0  1]) between the friendly gap case(Îº= 1) and the stochastic worst case
(Îº= 0). We show that appropriate second-order guarantees automatically lead to adaptation to these
parameters  for both the Hedge setting and for OCO. In the Hedge setting  we build on the guarantees
T regime for Îº= 0 and the fastest
available for the Squint algorithm [Koolen and Van Erven  2015] and for OCO we rely on guarantees
(doubly) logarithmic regime for Îº= 1. We show all this  not just in expectation (which is relatively
T 1âˆ’Îº
achieved by MetaGrad [Van Erven and Koolen  2016]. In both cases we obtain regret rates of order
2âˆ’Îº (Theorem 2). These rates include the slow worst-case
easy)  but also with high probability (which is much harder). Our proofs make use of a a convenient
novel notation (ESI  for exponential stochastic inequality) which allows us to prove such results
simultaneously  and which is of independent interest (Deï¬nition 5). Our proofs use that  for bounded
losses  the Bernstein condition is equivalent to the ESI-Bernstein condition  which we introduce.
The next section introduces the two settings we consider and the individual sequence guarantees we
will use in each. It also reviews the stochastic criteria for fast rates and presents our main result.
In Section 3 we consider a variety of examples illustrating the breadth of cases that we cover. In
Section 4 we introduce ESI and give a high-level overview of our proof.

âˆš

2 Setup and Main Result

2.1 Hedge Setting

We start with arguably the simplest setting of online prediction  the Hedge setting popularized by
Freund and Schapire [1997]. To be able to illustrate the full reach of our stochastic assumption

we will use a minor extension to countably inï¬nitely many actions kâˆˆ N={1  2  . . .}  customarily
wt = (w1
t   . . .) of the

t   . . .) on experts. Then the environment reveals the losses (cid:96)t = ((cid:96)1

called experts. The protocol is as follows. Each round t the learner plays a probability mass function

t âˆˆ[0  1]. The learner incurs losswt  (cid:96)t=âˆ‘k wk

t . The regret after T rounds

t   w2

t   (cid:96)2

t (cid:96)k

experts  where each (cid:96)k
compared to expert k is given by

T âˆ¶= TQ
t=1

Rk

wt  (cid:96)tâˆ’ (cid:96)k
t .

Rk

V k
T K k

Here V k

guarantees

T where K k

The goal of the learner is to keep the regret small compared to any expert k. We will make use
of Squint by Koolen and Van Erven [2015]  a self-tuning algorithm for playing wt. Koolen and

Van Erven [2015  Theorem 4] show that Squint with prior probability mass function Ï€=(Ï€1  Ï€2  . . .)
t2 is a second-order term that depends on the algorithmâ€™s own pre-
dictions wt. It is well-known that with K experts the worst-case lower bound is Î˜(âˆš
T ln K)
[Cesa-Bianchi and Lugosi  2006  Theorem 3.7]. Taking a fat-tailed prior Ï€  for example Ï€k=
k(k+1) 
T(ln k+ ln ln T)  matching the lower

T â‰¤ 
T+ K k
T âˆ¶=âˆ‘T
t=1wt  (cid:96)tâˆ’ (cid:96)k
T â‰¤ T   the above bound implies Rk

T = O(âˆ’ ln Ï€k+ ln ln T)
T â‰¤ O

and using V k
bound in some sense for all k simultaneously.
The question we study in this paper is what becomes of the regret when the sequence of losses
(cid:96)1  (cid:96)2  . . . is drawn from some distribution P  not necessarily i.i.d. But before we expand on such
stochastic cases  let us ï¬rst introduce another setting.

for any expert k.

(1)

1

2.2 Online Convex Optimization (OCO)

the set of actions is a compact convex setUâŠ† Rd. Each round t the learner plays a point wtâˆˆU.

We now turn to our second setting called online convex optimization [Shalev-Shwartz  2011]. Here

2

Ru

Ru

Then the environment reveals a convex loss function (cid:96)tâˆ¶Uâ†’ R. The loss of the learner is (cid:96)t(wt).
The regret after T rounds compared to uâˆˆU is given by
The goal is small regret compared to any point uâˆˆU. A common tool in the analysis of algorithms is

((cid:96)t(wt)âˆ’ (cid:96)t(u)) .

T âˆ¶= TQ
t=1

T â‰¤ ËœRu

the linear upper bound on the regret obtained from convexity of (cid:96)t (at non-differentiable points we
may take any sub-gradient)

T âˆ¶= TQ
wtâˆ’ u âˆ‡(cid:96)t(wt).
t=1
âˆš
T â‰¤ ODG
T and
T KT+ DGKT where KT = O(d ln T)
âˆš

We will make use of (the full matrix version of) MetaGrad by Van Erven and Koolen [2016]. In their
Theorem 8  they show that  simultaneously  ËœRu

T â‰¤ 
where D bounds the two-norm diameter ofU  G boundsâˆ‡(cid:96)t(wt)2 the two-norm of the gradients
t=1wtâˆ’ u âˆ‡(cid:96)t(wt)2. The ï¬rst bound matches the worst-case lower bound. The second
T â‰¤ G2D2T by Cauchy-Schwarz. Yet in this paper
assume from now on that DG= 1 (this can always be achieved by scaling the loss).
gradientsâˆ‡(cid:96)t(wt)) are drawn from a distribution P  not necessarily i.i.d. This includes the common
case of linear regression and classiï¬cation where (cid:96)t(u)= loss(u  xt  yt) with(xt  yt) sampled i.i.d.

and V u
bound (2) may be a factor
we will show fast rates in certain stochastic settings arising from (2). To simplify notation we will

To talk about stochastic settings we will assume that the sequence (cid:96)t of loss functions (and hence the

for any uâˆˆU 

T âˆ¶=âˆ‘T

KT worse  as V u

and loss a ï¬xed one-dimensional convex loss function (e.g. square loss  absolute loss  log loss  hinge
loss  . . . ).

ËœRu

V u

(2)

2.3 Parametrised Family of Stochastic Assumptions

We now recall the Bernstein [Bartlett and Mendelson  2006] stochastic condition. The idea behind
this assumption is to control the variance of the excess loss of the actions in the neighborhood of the
best action.
We do not require that the losses are i.i.d.  nor that the Bayes act is in the model. For the Hedge

tGtâˆ’1(cid:6)
setting it sufï¬ces if there is a ï¬xed expert kâˆ— that is always best  i.e. E(cid:96)k
almost surely for all t. (Here we denote byGtâˆ’1 the sigma algebra generated by (cid:96)1  . . .   (cid:96)tâˆ’1  and the
there is a ï¬xed point uâˆ—âˆˆU attaining minuâˆˆU E[(cid:96)t(u)Gtâˆ’1] at every round t. In either case there
almost surely quantiï¬cation refers to the distribution of (cid:96)1  . . .   (cid:96)tâˆ’1.) Similarly  for OCO we assume
may be multiple candidate kâˆ— or uâˆ—. In the succeeding we assume that one is selected. Note that
(cid:96)t are continuous  it is even automatic in the OCO case due to compactness ofU)  while it is very
time tâˆˆ N and expert/point kâˆˆ N/uâˆˆU as follows

strong beyond i.i.d. Yet it is not impossible (and actually interesting) as we will show by example in
Section 3.
Based on the loss minimiser  we deï¬ne the excess losses  a family of random variables indexed by

for i.i.d. losses the existence of a minimiser is not such a strong assumption (if the loss functions

t Gtâˆ’1(cid:6)= inf k E(cid:96)k

âˆ—

t

xk
t

xu
t

and

(OCO).

(Hedge)

(3)
Note that for the Hedge setting we work with the loss directly. For OCO instead we talk about the
linear upper bound on the excess loss  for this is the quantity that needs to be controlled to make use
of the MetaGrad bound (2). With these variables in place  from this point on the story is the same for

Hedge and for OCO. So let us writeF for either the set N of experts or the setU of points  and fâˆ—
for kâˆ— resp. uâˆ—  and let us consider the family{xf
t  fâˆˆF  tâˆˆ N}. We call fâˆˆF predictors. With
Condition 1. Fix Bâ‰¥ 0 and Îºâˆˆ[0  1]. The family (3) satisï¬es the(B  Îº)-Bernstein condition if
almost surely for all fâˆˆF and rounds tâˆˆ N.

t)2Gtâˆ’1 â‰¤ B Exf

this notation the Bernstein condition is the following.

tGtâˆ’1Îº

E(xf

âˆ¶= (cid:96)k
tâˆ’ (cid:96)k

âˆ—

âˆ¶= uâˆ’ u
âˆ—

 âˆ‡(cid:96)t(u)

3

The point of this stochastic condition is that it implies that the variance in the excess loss gets smaller
the closer a predictor gets to the optimum in terms of expected excess loss.

Some authors refer to the Îº= 1 case as the Massart condition. Van Erven et al. [2015] have shown

that the Bernstein condition is equivalent to the central condition  a fast-rate type of condition that has
been frequently used (without an explicit name) in density estimation under misspeciï¬cation. Two
more equivalent conditions appear in our proof sketch Section 4. We compare all four formulations
in Appendix B.

2.4 Main Result

âˆ—
âˆ—
In the stochastic case we evaluate the performance of algorithms by Rf
T   i.e. the regret compared

T ] is sometimes called the
to the predictor fâˆ— with minimal expected loss. The expectation E[Rf
Theorem 2. In any stochastic setting satisfying the(B  Îº)-Bernstein Condition 1  the guarantees (1)

pseudo-regret. The following result shows that second-order methods automatically adapt to the
Bernstein condition. (Proof sketch in Section 4.)

for Squint and (2) for MetaGrad imply fast rates for the respective algorithms both in expectation
and with high probability. That is 

âˆ—

2âˆ’Îº  
E[Rf
T ] = OK
1âˆ’Îº
2âˆ’Îº
and for any Î´> 0  with probability at least 1âˆ’ Î´ 
T T
T = O(KTâˆ’ ln Î´) 1
2âˆ’Îº T
where for Squint KT âˆ¶= K f

âˆ—
T from (1) and for MetaGrad KT is as in (2).

2âˆ’Îº  
1âˆ’Îº

Rf

âˆ—

1

rates  one has to impose further assumptions on P. A standard assumption made in such cases is a

Crucially  the bound provided by Theorem 2 is natural  and  in general  the best one can expect.
This can be seen from considering the statistical learning setting  which is a special case of our

(cid:96)f
In this setting one usually considers excess risk  which is the expected loss difference between the

We see that Squint and MetaGrad adapt automatically to the Bernstein parameters of the distribution 
without any tuning. Theorem 2 only uses the form of the second-order bounds and does not depend
on the details of the algorithms  so it also applies to any other method with a second-order regret
bound. In particular it holds for Adapt-ML-Prod by Gaillard et al. [2014]  which guarantees (1) with

KT = O(lnF+ln ln T) for ï¬nite sets of experts. Here we focus on Squint as it also applies to inï¬nite
sets. Appendix D provides an extension of Theorem 2 that allows using Squint with uncountableF.
setup. Here(xt  yt) are i.i.d.âˆ¼ P andF is a set of functions fromX to a set of predictionsA  with
t âˆ¶= (cid:96)(yt  f(xt)) for some loss function (cid:96)âˆ¶YÃ—Aâ†’[0  1] such as squared  0~1  or absolute loss.
learned Ë†f and the optimal fâˆ—. The minimax expected (over training sample(xt  yt)) risk relative
to fâˆ— is of order Tâˆ’1~2 (see e.g. Massart and NÃ©dÃ©lec [2006]  Audibert [2009]). To get better risk
Bernstein condition with exponent Îº> 0; see e.g. Koltchinskii [2006]  Bartlett and Mendelson [2006] 
IfF is sufï¬ciently â€˜simpleâ€™  e.g. a class with logarithmic entropy numbers (see Appendix D)  or  in
2âˆ’Îº. The bound
achieves  in expectation  a better excess risk bound of order O(log T)â‹… Tâˆ’ 1
interpolates between Tâˆ’1~2 for Îº= 0 and Tâˆ’1 for Îº= 1 (Massart condition). Results of Tsybakov
t= 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL 
follow-the-leader)  this suggests that we can achieve a cumulative expected regret E[Rf
T ] of order
2âˆ’Îº. Theorem 2 shows that this is  indeed  also the rate that Squint attains in such
O(log T)â‹… T 1âˆ’Îº
cases ifF is countable and the optimal fâˆ— has positive prior mass Ï€f
âˆ—> 0 (more on this condition

[2004]  Massart and NÃ©dÃ©lec [2006]  Audibert [2009] suggest that this rate can  in general  not be
improved upon  and exactly this rate is achieved by ERM and various other algorithms in various
settings by e.g. Tsybakov [2004]  Audibert [2004  2009]  Bartlett et al. [2006]. By summing from

Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov
margin and other conditions.

classiï¬cation  a VC class  then  if a Îº-Bernstein condition holds  ERM (empirical risk minimization)

below)â€” we thus see that Squint obtains exactly the rates one would expect from a statistical

âˆ—

4

Îº. GrÃ¼nwald [2012] provides a means to tune Î· automatically in terms of the data  but his method
â€” like ERM and all algorithms in the references above â€” may achieve linear regret in worst-case

learning/classiï¬cation perspective  and the minimax excess risk results in that setting suggests that
these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert
[2004] that  when equipped with an oracle to tune the learning rate Î· as a function of t  the rates

O(log T)â‹… T 1âˆ’Îº
2âˆ’Îº can also be achieved by Hedge  but the exact tuning depends on the unknown
settings  whereas Squint keeps the O(âˆš
Theorem 2 only gives the desired rate for Squint with inï¬niteF ifF is countable and Ï€f
âˆ—> 0. The
âˆ—= 0  as long asF admits sufï¬ciently small entropy
of uncountably inï¬niteF  which can have Ï€f
numbers. Incidentally  this also allows us to show that Squint achieves regret rate O(log T)â‹… T 1âˆ’Îº
2âˆ’Îº
whenF=i=1 2 ...Fi is a countably inï¬nite union ofFi with appropriate entropy numbers; in such
cases there can be  at every sample size  a classiï¬er Ë†fâˆˆF with 0 empirical error  so that ERM/FTL

combination of these two assumptions is strong or at least unnatural  and OCO cannot be readily used
in all such cases either  so in Appendix D we therefore show how to extend Theorem 2 to the case

T) guarantee for such cases.

will always over-ï¬t and cannot be used even if the Bernstein condition holds; Squint allows for
aggregation of such models. In the remainder of the main text  we concentrate on applications for
which Theorem 2 can be used directly  without extensions.

3 Examples

Our OCO examples were chosen to be natural and illustrate fast rates without curvature.

We give examples motivating and illustrating the Bernstein condition for the Hedge and OCO settings.

Our examples in the Hedge setting will illustrate Bernstein with Îº< 1 and non i.i.d. distributions.
3.1 Hedge Setting: Gap implies Bernstein with Îº= 1
In the Hedge setting  we say that a distribution P (not necessarily i.i.d.) of expert losses{(cid:96)k
t  t  kâˆˆ N}
has gap Î±> 0 if there is an expert kâˆ— such that
kâ‰ kâˆ— E(cid:96)k
tGtâˆ’1(cid:6)
almost surely for each round tâˆˆ N.
It is clear that the condition can only hold for kâˆ— the minimiser of the expected loss.
Lemma 3. A distribution with gap Î± is( 1
Î±   1)-Bernstein.
t)2Gtâˆ’1(cid:6)â‰¤ 1= 1
Exk
tGtâˆ’1(cid:6) .
Proof. For all kâ‰  kâˆ— and t  we have E(xk

t Gtâˆ’1+ Î± â‰¤ inf
E(cid:96)k

Î± Î±â‰¤ 1

T = O(KT)= O(ln ln T) rate. Gaillard et al. [2014] show constant

âˆ—

âˆ—

Î±

By Theorem 2 we get the Rk
regret for ï¬nitely many experts and i.i.d. losses with a gap. Our alternative proof above shows that
neither ï¬niteness nor i.i.d. are essential for fast rates in this case.

The next example illustrates that we can sometimes get the fast rates without a gap. And it also shows
that we can get any intermediate rate: we construct an example satisfying the Bernstein condition for

3.2 Hedge Setting: Any(1  Îº)-Bernstein
any Îºâˆˆ[0  1] of our choosing (such examples occur naturally in classiï¬cation settings such as those
Fix Îºâˆˆ[0  1]. Each expert k= 1  2  . . . is parametrised by a real number Î´kâˆˆ[0  1~2]. The only
assumption we make is that Î´k= 0 for some k  and inf k{Î´k Î´k> 0}= 0. For a concrete example let
us choose Î´1= 0 and Î´k= 1~k for k= 2  3  . . . Expert Î´k has loss 1~2Â± Î´k with probability 1Â±Î´2~Îºâˆ’1
  and so Î´1= 0 is best 
+ Î´2~Îº
with loss deterministically equal to 1~2. The squared excess loss of Î´k is Î´2
condition with exponent Îº (but no Îºâ€²> Îº) and constant 1  and the associated regret rate by Theorem 2.

independently between experts and rounds. Expert Î´k has mean loss 1
2

considered in the example in Appendix D).

k. So we have the Bernstein

k
2

k

5

âˆ—

âˆš

Note that for Îº= 0 (the hard case) all experts have mean loss equal to 1
we designate as the best expert our pseudo-regret E[Rk

âˆ—
2 independently at random. Hence  by the central limit theorem  with high
their losses deviate from 1
probability our regret Rk
T is of order
case)  we do not ï¬nd a gap. We still have experts arbitrary close to the best expert in mean  but their
expected excess loss squared equals their expected excess loss.
ERM/FTL (and hence all approaches based on it  such as [Bartlett and Mendelson  2006]) may fail

2. So no matter which kâˆ—
T ] is zero. Yet the experts do not agree  as
T . On the other side of the spectrum  for Îº= 1 (the best
completely on this type of examples. The clearest case is when{k Î´k> } is inï¬nite for some > 0.
of them will result in expected instantaneous regret at least 2~Îº  leading to linear regret overall.
The requirement Î´k= 0 for some k is essential. If instead Î´k> 0 for all k then there is no best expert

Then at any t there will be experts that  by chance  incurred their lower loss every round. Picking any

in the class. Theorem 19 in Appendix D shows how to deal with this case.

3.3 Hedge Setting: Markov Chains

2

6

2

2

1

2

regret of order

E(xf

3.4 OCO: Hinge Loss on the Unit Ball

T 2m. Then  if the data are actually generated by an m-th order Markov chain with

Suppose we model a binary sequence z1  z2  . . .   zT with m-th order Markov chains. As experts we

âˆš
t)2(ztâˆ’m  . . .   ztâˆ’1)= a= 1 

consider all possible functions fâˆ¶{0  1}mâ†’{0  1} that map a history of length m to a prediction
for the next outcome  and the loss of expert f is the 0~1-loss: (cid:96)f
t =f(ztâˆ’m  . . .   ztâˆ’1)âˆ’ zt. (We
initialize z1âˆ’m= . . .= z0= 0.) A uniform prior on this ï¬nite set of 22m experts results in worst-case
transition probabilities P(zt= 1(ztâˆ’m  . . .   ztâˆ’1)= a)= pa  we have fâˆ—(a)= 1{paâ‰¥ 1
} and

Exf
t(ztâˆ’m  . . .   ztâˆ’1)= a= 2paâˆ’ 1
for any f such that f(a)â‰  fâˆ—(a). So the Bernstein condition holds with Îº= 1 and B=
2 minapaâˆ’ 1
.
Let(x1  y1) (x2  y2)  . . . be classiï¬cation data  with ytâˆˆ{âˆ’1 +1} and xtâˆˆ Rd  and consider the
hinge loss (cid:96)t(u)= max{0  1âˆ’ ytxt  u}. Now suppose  for simplicity  that both xt and u come
from the d-dimensional unit Euclidean ball  such thatxt  uâˆˆ[âˆ’1 +1] and hence the hinge is never
active  i.e. (cid:96)t(u)= 1âˆ’ ytxt  u. Then  if the data turn out to be i.i.d. observations from a ï¬xed
distribution P  the Bernstein condition holds with Îº= 1 (the proof can be found in Appendix C):
xt  uâ‰¤ 1. If the data are i.i.d.  then the(B  Îº)-Bernstein condition is satisï¬ed with Îº= 1 and
B= 2Î»max
Âµ   where Î»max is the maximum eigenvalue of E[xx] and Âµ= E[yx]  provided thatÂµ> 0.
In particular  if xt is uniformly distributed on the sphere and yt = sign(Â¯u  xt) is the noiseless
classiï¬cation of xt according to the hyper-plane with normal vector Â¯u  then Bâ‰¤ câˆš
constant c> 0.
The excluded caseÂµ= 0 only happens in the degenerate case that there is nothing to learn  because
Âµ= 0 implies that the expected hinge loss is 1  its maximal value  for all u.
LetU=[0  1] be the unit interval. Consider the absolute loss (cid:96)t(u)=uâˆ’ xt where xtâˆˆ[0  1] are
drawn i.i.d. from P. Let uâˆ—âˆˆ arg minu Euâˆ’ x minimize the expected loss. In this case we may
simplifywâˆ’ uâˆ— âˆ‡(cid:96)(w)=(wâˆ’ uâˆ—) sign(wâˆ’ x). To satisfy the Bernstein condition  we therefore
want B such that  for all wâˆˆ[0  1] 
âˆ—) sign(wâˆ’ x)2 â‰¤ B E[(wâˆ’ u
âˆ—2âˆ’Îº â‰¤ B2Îº P(xâ‰¤ w)âˆ’ 1
wâˆ’ u

âˆ—) sign(wâˆ’ x)]Îº .
Îº.

Lemma 4 (Unregularized Hinge Loss Example). Consider the hinge loss setting above  where

E(wâˆ’ u

3.5 OCO: Absolute Loss

for some absolute

d

That is 

For instance  if the distribution of x has a strictly positive density p(x)â‰¥ m> 0  then uâˆ— is the
= P(xâ‰¤ w)âˆ’ P(xâ‰¤ uâˆ—)â‰¥ mwâˆ’ uâˆ—  so the condition holds with Îº= 1
median and P(xâ‰¤ w)âˆ’ 1
and B= 1
1âˆ’ p  the condition holds with Îº= 1 and B=
wâˆ’ uâˆ—â‰¤ 1 and P(xâ‰¤ w)âˆ’ 1
â‰¥pâˆ’ 1
.

2m. Alternatively  for a discrete distribution on two points a and b with probabilities p and
2  as can be seen by bounding

12pâˆ’1  provided that pâ‰  1

2

2

2

4 Proof Ideas

This section builds up to prove our main result Theorem 2. We ï¬rst introduce the handy ESI-
abbreviation that allows us to reason simultaneously in expectation and with high probability. We
then provide two alternative characterizations of the Bernstein condition that are equivalent for
bounded losses. Finally  we show how one of these  ESI-Bernstein  combines with individual-
sequence second-order regret bounds to give rise to Theorem 2.

4.1 Notation: Exponential Stochastic Inequality (ESI  pronounce easy)

Lemma 6. Exponential stochastic negativity/inequality has the following useful properties:

Deï¬nition 5. A random variable X is exponentially stochastically negative  denoted X  0  if
E[eX]â‰¤ 1. For any Î·â‰¥ 0  we write XÎ· 0 if Î·X 0. For any pair of random variables X and Y  
the exponential stochastic inequality (ESI) XÎ· Y is deï¬ned as expressing Xâˆ’ Y Î· 0; X Y is
deï¬ned as X1 Y .
1. (Negativity). Let X 0. As the notation suggests X is negative in expectation and with high
probability. That is E[X]â‰¤ 0 and P{Xâ‰¥âˆ’ ln Î´}â‰¤ Î´ for all Î´> 0.
2. (Convex combination). LetX f
probability distribution onF. If X f 0 for all f then Efâˆ¼w[X f] 0.
fâˆˆF be a family of random variables and let w be a
3. (Chain rule). Let X1  X2  . . . be adapted to ï¬ltrationG1âŠ†G2 . . . (i.e. Xt isGt-measurable
for each t). If XtGtâˆ’1 0 almost surely for all t  thenâˆ‘T
Proof. Negativity: By Jensenâ€™s inequality E[X]â‰¤ ln EeX(cid:6)â‰¤ 0  whereas by Markovâ€™s inequal-
 â‰¤ Î´ EeX(cid:6) â‰¤ Î´. Convex combination: By Jensenâ€™s inequality
ity P{Xâ‰¥âˆ’ ln Î´} = PeXâ‰¥ 1
Efâˆ¼w[X f]â‰¤ Efâˆ¼w EeX fâ‰¤ 1. Chain rule: By induction. The base case T= 0 holds trivially.
Ee
For T> 0 we have Eeâˆ‘T

t=1 Xt EeXTGTâˆ’1(cid:6)â‰¤ Eeâˆ‘Tâˆ’1

t=1 Xt 0 for all Tâ‰¥ 0.

t=1 Xt= Eeâˆ‘Tâˆ’1

t=1 Xtâ‰¤ 1.

Î´

âˆ—

âˆ—

4.2 The Bernstein Condition and Second-order Bounds

Our main result Theorem 2  bounds the regret Rf

âˆ—
T compared to the stochastically optimal predictor

T = O(
T â‰¤ ËœRf

fâˆ— when the sequence of losses (cid:96)1  (cid:96)2  . . . comes from a Bernstein distribution P. For simplicity we
T KT).
V fâˆ—
âˆ—
âˆ—
T is
âˆ—
âˆ—
T with high probability. Combination with the individual-sequence bound
T is bounded in terms of a function of itself. And solving the inequality for ËœRf

only consider the OCO setting in this sketch. Full details are in Theorem 11. Our starting point
will be the individual-sequence second-order bound (2)  which implies Rf
The crucial technical contribution of this paper is to establish that for Bernstein distributions V f
bounded in terms of ËœRf
âˆ—
then gives that ËœRf
establishes the fast rates for Rf
T .
âˆ—

âˆ—
T would be bounded in terms of ËœRf

t=1(xft
T =âˆ‘T
T =âˆ‘T
t )2 and ËœRf
t )2 in terms of xft
algorithm in round t. We will bound(xft
t=1 xft
Condition 1 for Îº= 1 directly yields
t  = B E ËœRf
Exft
t )2 â‰¤ B
E(xft
EV f
T  = TQ
T  .
TQ
t=1
t=1

âˆ—
T   we look at their relation
t where ft is the prediction of the
t separately for each round t. The Bernstein

To get a ï¬rst intuition as to why V f
in expectation. Recall that V f

(4)

âˆ—

T

âˆ—

âˆ—

7

set of linear inequalities:
Condition 7. The excess loss family (3) satisï¬es the linearized Îº-Bernstein condition if there are

For Îº< 1 the ï¬nal step of interchanging expectation and sums does not work directly  but we may use
zÎº= ÎºÎº(1âˆ’ Îº)1âˆ’Îº inf >0Îºâˆ’1z+ Îº for zâ‰¥ 0 to rewrite the Bernstein condition as the following
constants c1  c2> 0 such that we have:
t)2Gtâˆ’1âˆ’ Exf
c1â‹… 1âˆ’Îº EV f

a.s. for all > 0  fâˆˆF and tâˆˆ N.
T + c2â‹… Tâ‹… .

tGtâˆ’1 â‰¤ c2â‹… 
T  â‰¤ E ËœRf

This gives the following generalization of (4):
âˆ—

c1â‹… 1âˆ’Îºâ‹… E(xf

(5)

âˆ—

Together with the individual sequence regret bound and optimization of  this can be used to derive
the in-expectation part of Theorem 2.

âˆ—
Getting the in-probability part is more difï¬cult  however  and requires relating V f
T in
T
probability instead of in expectation. Our main technical contribution does exactly this  by showing
that the Bernstein condition is in fact equivalent to the following exponential strengthening of
Condition 7:

Condition 8. The family (3) satisï¬es the Îº-ESI-Bernstein condition if there are c1  c2> 0 such that:

and ËœRf

âˆ—

c1â‹… 1âˆ’Îºâ‹…(xf

t)2âˆ’ xf

tGtâˆ’1 1âˆ’Îº c2â‹… 

a.s. for all > 0  fâˆˆF and tâˆˆ N.

âˆ—

âˆ—

âˆ—

Condition 8 implies Condition 7 by Jensenâ€™s inequality (see Lemma 6 part 1). The surprising converse
is proved in Lemma 9 in the appendix. By telescoping over rounds using the chain rule from Lemma 6 
we see that ESI-Bernstein implies the following substantial strengthening of (5):

Now the second-order regret bound (2) can be rewritten  using 2

a.s. for all > 0  Tâˆˆ N.
T âˆ’ ËœRf
T 1âˆ’Îº c2â‹… Tâ‹… 
c1â‹… 1âˆ’Îºâ‹… V f
âˆš
ab= inf Î³ Î³a+ b~Î³  as:

T â‹… KT+ 2KT â‰¤ Î³â‹… V f
T + KT
+ 2KT .
T â‰¤ 2
for every Î³> 0âˆ¶ 2 ËœRf
V fâˆ—
Plugging in Î³= c11âˆ’Îº we can chain this inequality with (6) to give  for all > 0 
+ 2KT  
T + c2â‹… Tâ‹… + KT
T 1âˆ’Îº ËœRf
c1â‹… 1âˆ’Îº
and both parts of Theorem 2 now follow by rearranging  plugging in the minimiser Ã  K

(6)

(7)
T T 1âˆ’Îº
2âˆ’Îº
2âˆ’Îº  

1

2 ËœRf

âˆ—

âˆ—

âˆ—

Î³

and using Lemma 6 part 1.

Acknowledgments

Koolen acknowledges support by the Netherlands Organization for Scientiï¬c Research (NWO  Veni
grant 639.021.439).

References
J-Y. Audibert. PAC-Bayesian statistical learning theory. PhD thesis  UniversitÃ© Paris VI  2004.
J-Y. Audibert. Fast learning rates in statistical inference through aggregation. Ann. Stat.  37(4)  2009.
P. Bartlett and S. Mendelson. Empirical minimization. Probab. Theory Rel.  135(3):311â€“334  2006.
P. Bartlett  M. Jordan  and J. McAuliffe. Convexity  classiï¬cation  and risk bounds. J. Am. Stat. Assoc.  101
(473):138â€“156  2006.
N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
N. Cesa-Bianchi  Y. Mansour  and G. Stoltz. Improved second-order bounds for prediction with expert advice.
Machine Learning  66(2/3):321â€“352  2007.
C. Chiang  T. Yang  C. Le  M. Mahdavi  C. Lu  R. Jin  and S. Zhu. Online optimization with gradual variations.
In Proc. 25th Conf. on Learning Theory (COLT)  2012.
K. Crammer  A. Kulesza  and M. Dredze. Adaptive regularization of weight vectors. In NIPS 22  2009.

8

J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization.
Journal of Machine Learning Research  12:2121â€“2159  2011.
T. van Erven and W. Koolen. MetaGrad: Multiple learning rates in online learning. In Advances in Neural
Information Processing Systems 29  2016.
T. van Erven  P. GrÃ¼nwald  N. Mehta  M. Reid  and R. Williamson. Fast rates in statistical and online learning.
Journal of Machine Learning Research  16:1793â€“1861  2015.
E. Even-Dar  M. Kearns  Y. Mansour  and J. Wortman. Regret to the best vs. regret to the average. Machine
Learning  72(1-2)  2008.
Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to
boosting. Journal of Computer and System Sciences  55:119â€“139  1997.
P. Gaillard and S. Gerchinovitz. A chaining algorithm for online nonparametric regression. In Proc. 28th Conf.
on Learning Theory (COLT)  2015.
P. Gaillard  G. Stoltz  and T. van Erven. A second-order bound with excess losses. In Proc. 27th COLT  2014.
P. GrÃ¼nwald. The safe Bayesian: learning the learning rate via the mixability gap. In ALT â€™12. Springer  2012.
E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine
learning  80(2-3):165â€“188  2010.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Ann. Stat.  34(6):
2593â€“2656  2006.
W. Koolen. The relative entropy bound for Squint. Blog entry on blog.wouterkoolen.info/  August 2015.
W. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games. In Proc.
28th Conf. on Learning Theory (COLT)  pages 1155â€“1175  2015.
W. Koolen  T. van Erven  and P. GrÃ¼nwald. Learning the learning rate for prediction with expert advice. In
Advances in Neural Information Processing Systems 27  pages 2294â€“2302  2014.
H. Luo and R. Schapire. Achieving all with no parameters: Adaptive normalhedge. In Proc. 28th COLT  2015.
P. Massart and Ã‰. NÃ©dÃ©lec. Risk bounds for statistical learning. Ann. Stat.  34(5):2326â€“2366  2006.
B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In Proc. 23rd
Conf. on Learning Theory (COLT)  pages 244â€“256  2010.
N. Mehta and R. Williamson. From stochastic mixability to fast rates. In NIPS 27  2014.
F. Orabona  K. Crammer  and N. Cesa-Bianchi. A generalized online mirror descent with applications to
classiï¬cation and regression. Machine Learning  99(3):411â€“435  2015.
A. Rakhlin and K. Sridharan. Online nonparametric regression. In Proc. 27th COLT  2014.
S. de Rooij  T. van Erven  P. GrÃ¼nwald  and W. Koolen. Follow the leader if you can  Hedge if you must.
Journal of Machine Learning Research  15:1281â€“1316  April 2014.
A. Sani  G. Neu  and A. Lazaric. Exploiting easy data in online optimization. In NIPS 27  2014.
S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine
Learning  4(2):107â€“194  2011.
J. Steinhardt and P. Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In Proc.
31th Int. Conf. on Machine Learning (ICML)  pages 1593â€“1601  2014.
A. Tsybakov. Optimal aggregation of classiï¬ers in statistical learning. Ann. Stat.  32:135â€“166  2004.
O. Wintenberger. Optimal learning with Bernstein Online Aggregation. ArXiv:1404.1356  2015.

9

,Wouter Koolen
Peter GrÃ¼nwald
Tim van Erven