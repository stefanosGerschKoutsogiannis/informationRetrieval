2016,Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning,We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly)  yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.,Combining Adversarial Guarantees and
Stochastic Fast Rates in Online Learning

Centrum Wiskunde & Informatica

CWI and Leiden University

pdg@cwi.nl

Wouter M. Koolen

Peter Gr√ºnwald

Science Park 123  1098 XG
Amsterdam  the Netherlands

wmkoolen@cwi.nl

Tim van Erven
Leiden University

Niels Bohrweg 1  2333 CA

Leiden  the Netherlands
tim@timvanerven.nl

Abstract

We consider online learning algorithms that guarantee worst-case regret rates
in adversarial environments (so they can be deployed safely and will perform
robustly)  yet adapt optimally to favorable stochastic environments (so they will
perform well in a variety of settings of practical importance). We quantify the
friendliness of stochastic environments by means of the well-known Bernstein
(a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint
for the Hedge setting and MetaGrad for online convex optimization) we show that
the particular form of their data-dependent individual-sequence regret guarantees
implies that they adapt automatically to the Bernstein parameters of the stochastic
environment. We prove that these algorithms attain fast rates in their respective
settings both in expectation and with high probability.

1

Introduction

‚àö

We consider online sequential decision problems. We focus on full information settings  encompassing
such interaction protocols as online prediction  classiÔ¨Åcation and regression  prediction with expert
advice or the Hedge setting  and online convex optimization (see Cesa-Bianchi and Lugosi 2006). The
goal of the learner is to choose a sequence of actions with small regret  i.e. such that his cumulative
loss is not much larger than the loss of the best Ô¨Åxed action in hindsight. This has to hold even in
the worst case  where the environment is controlled by an adversary. After three decades of research
there exist many algorithms and analysis techniques for a variety of such settings. For many settings 
adversarial regret lower bounds of order
T are known  along with matching individual sequence
algorithms [Shalev-Shwartz  2011].
A more recent line of development is to design adaptive algorithms with regret guarantees that scale
with some more reÔ¨Åned measure of the complexity of the problem. For the Hedge setting  results of
this type have been obtained  amongst others  by Cesa-Bianchi et al. [2007]  De Rooij et al. [2014] 
Gaillard et al. [2014]  Sani et al. [2014]  Even-Dar et al. [2008]  Koolen et al. [2014]  Koolen and
Van Erven [2015]  Luo and Schapire [2015]  Wintenberger [2015]. Interestingly  the price for such
adaptivity (i.e. the worsening of the worst-case regret bound) is typically extremely small (i.e. a
constant factor in the regret bound). For online convex optimization (OCO)  many different types of
adaptivity have been explored  including by Crammer et al. [2009]  Duchi et al. [2011]  McMahan
and Streeter [2010]  Hazan and Kale [2010]  Chiang et al. [2012]  Steinhardt and Liang [2014] 
Orabona et al. [2015]  Van Erven and Koolen [2016].
Here we are interested in the question of whether such adaptive results are strong enough to lead to
improved rates in the stochastic case when the data follow a ‚Äúfriendly‚Äù distribution. In speciÔ¨Åc cases
it has been shown that fancy guarantees do imply signiÔ¨Åcantly reduced regret. For example Gaillard
et al. [2014] present a generic argument showing that a certain kind of second-order regret guarantees

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

implies constant expected regret (the fastest possible rate) for i.i.d. losses drawn from a distribution
with a gap (between expected loss of the best and all other actions). In this paper we signiÔ¨Åcantly
extend this result. We show that a variety of individual-sequence second-order regret guarantees
imply fast regret rates for distributions under much milder stochastic assumptions. In particular  we
will look at the Bernstein condition (see Bartlett and Mendelson 2006)  which is the key to fast rates
in the batch setting. This condition provides a parametrised interpolation (expressed in terms of the

Bernstein exponent Œ∫‚àà[0  1]) between the friendly gap case(Œ∫= 1) and the stochastic worst case
(Œ∫= 0). We show that appropriate second-order guarantees automatically lead to adaptation to these
parameters  for both the Hedge setting and for OCO. In the Hedge setting  we build on the guarantees
T regime for Œ∫= 0 and the fastest
available for the Squint algorithm [Koolen and Van Erven  2015] and for OCO we rely on guarantees
(doubly) logarithmic regime for Œ∫= 1. We show all this  not just in expectation (which is relatively
T 1‚àíŒ∫
achieved by MetaGrad [Van Erven and Koolen  2016]. In both cases we obtain regret rates of order
2‚àíŒ∫ (Theorem 2). These rates include the slow worst-case
easy)  but also with high probability (which is much harder). Our proofs make use of a a convenient
novel notation (ESI  for exponential stochastic inequality) which allows us to prove such results
simultaneously  and which is of independent interest (DeÔ¨Ånition 5). Our proofs use that  for bounded
losses  the Bernstein condition is equivalent to the ESI-Bernstein condition  which we introduce.
The next section introduces the two settings we consider and the individual sequence guarantees we
will use in each. It also reviews the stochastic criteria for fast rates and presents our main result.
In Section 3 we consider a variety of examples illustrating the breadth of cases that we cover. In
Section 4 we introduce ESI and give a high-level overview of our proof.

‚àö

2 Setup and Main Result

2.1 Hedge Setting

We start with arguably the simplest setting of online prediction  the Hedge setting popularized by
Freund and Schapire [1997]. To be able to illustrate the full reach of our stochastic assumption

we will use a minor extension to countably inÔ¨Ånitely many actions k‚àà N={1  2  . . .}  customarily
wt = (w1
t   . . .) of the

t   . . .) on experts. Then the environment reveals the losses (cid:96)t = ((cid:96)1

called experts. The protocol is as follows. Each round t the learner plays a probability mass function

t ‚àà[0  1]. The learner incurs losswt  (cid:96)t=‚àëk wk

t . The regret after T rounds

t   w2

t   (cid:96)2

t (cid:96)k

experts  where each (cid:96)k
compared to expert k is given by

T ‚à∂= TQ
t=1

Rk

wt  (cid:96)t‚àí (cid:96)k
t .

Rk

V k
T K k

Here V k

guarantees

T where K k

The goal of the learner is to keep the regret small compared to any expert k. We will make use
of Squint by Koolen and Van Erven [2015]  a self-tuning algorithm for playing wt. Koolen and

Van Erven [2015  Theorem 4] show that Squint with prior probability mass function œÄ=(œÄ1  œÄ2  . . .)
t2 is a second-order term that depends on the algorithm‚Äôs own pre-
dictions wt. It is well-known that with K experts the worst-case lower bound is Œò(‚àö
T ln K)
[Cesa-Bianchi and Lugosi  2006  Theorem 3.7]. Taking a fat-tailed prior œÄ  for example œÄk=
k(k+1) 
T(ln k+ ln ln T)  matching the lower

T ‚â§ 
T+ K k
T ‚à∂=‚àëT
t=1wt  (cid:96)t‚àí (cid:96)k
T ‚â§ T   the above bound implies Rk

T = O(‚àí ln œÄk+ ln ln T)
T ‚â§ O

and using V k
bound in some sense for all k simultaneously.
The question we study in this paper is what becomes of the regret when the sequence of losses
(cid:96)1  (cid:96)2  . . . is drawn from some distribution P  not necessarily i.i.d. But before we expand on such
stochastic cases  let us Ô¨Årst introduce another setting.

for any expert k.

(1)

1

2.2 Online Convex Optimization (OCO)

the set of actions is a compact convex setU‚äÜ Rd. Each round t the learner plays a point wt‚ààU.

We now turn to our second setting called online convex optimization [Shalev-Shwartz  2011]. Here

2

Ru

Ru

Then the environment reveals a convex loss function (cid:96)t‚à∂U‚Üí R. The loss of the learner is (cid:96)t(wt).
The regret after T rounds compared to u‚ààU is given by
The goal is small regret compared to any point u‚ààU. A common tool in the analysis of algorithms is

((cid:96)t(wt)‚àí (cid:96)t(u)) .

T ‚à∂= TQ
t=1

T ‚â§ ÀúRu

the linear upper bound on the regret obtained from convexity of (cid:96)t (at non-differentiable points we
may take any sub-gradient)

T ‚à∂= TQ
wt‚àí u ‚àá(cid:96)t(wt).
t=1
‚àö
T ‚â§ ODG
T and
T KT+ DGKT where KT = O(d ln T)
‚àö

We will make use of (the full matrix version of) MetaGrad by Van Erven and Koolen [2016]. In their
Theorem 8  they show that  simultaneously  ÀúRu

T ‚â§ 
where D bounds the two-norm diameter ofU  G bounds‚àá(cid:96)t(wt)2 the two-norm of the gradients
t=1wt‚àí u ‚àá(cid:96)t(wt)2. The Ô¨Årst bound matches the worst-case lower bound. The second
T ‚â§ G2D2T by Cauchy-Schwarz. Yet in this paper
assume from now on that DG= 1 (this can always be achieved by scaling the loss).
gradients‚àá(cid:96)t(wt)) are drawn from a distribution P  not necessarily i.i.d. This includes the common
case of linear regression and classiÔ¨Åcation where (cid:96)t(u)= loss(u  xt  yt) with(xt  yt) sampled i.i.d.

and V u
bound (2) may be a factor
we will show fast rates in certain stochastic settings arising from (2). To simplify notation we will

To talk about stochastic settings we will assume that the sequence (cid:96)t of loss functions (and hence the

for any u‚ààU 

T ‚à∂=‚àëT

KT worse  as V u

and loss a Ô¨Åxed one-dimensional convex loss function (e.g. square loss  absolute loss  log loss  hinge
loss  . . . ).

ÀúRu

V u

(2)

2.3 Parametrised Family of Stochastic Assumptions

We now recall the Bernstein [Bartlett and Mendelson  2006] stochastic condition. The idea behind
this assumption is to control the variance of the excess loss of the actions in the neighborhood of the
best action.
We do not require that the losses are i.i.d.  nor that the Bayes act is in the model. For the Hedge

tGt‚àí1(cid:6)
setting it sufÔ¨Åces if there is a Ô¨Åxed expert k‚àó that is always best  i.e. E(cid:96)k
almost surely for all t. (Here we denote byGt‚àí1 the sigma algebra generated by (cid:96)1  . . .   (cid:96)t‚àí1  and the
there is a Ô¨Åxed point u‚àó‚ààU attaining minu‚ààU E[(cid:96)t(u)Gt‚àí1] at every round t. In either case there
almost surely quantiÔ¨Åcation refers to the distribution of (cid:96)1  . . .   (cid:96)t‚àí1.) Similarly  for OCO we assume
may be multiple candidate k‚àó or u‚àó. In the succeeding we assume that one is selected. Note that
(cid:96)t are continuous  it is even automatic in the OCO case due to compactness ofU)  while it is very
time t‚àà N and expert/point k‚àà N/u‚ààU as follows

strong beyond i.i.d. Yet it is not impossible (and actually interesting) as we will show by example in
Section 3.
Based on the loss minimiser  we deÔ¨Åne the excess losses  a family of random variables indexed by

for i.i.d. losses the existence of a minimiser is not such a strong assumption (if the loss functions

t Gt‚àí1(cid:6)= inf k E(cid:96)k

‚àó

t

xk
t

xu
t

and

(OCO).

(Hedge)

(3)
Note that for the Hedge setting we work with the loss directly. For OCO instead we talk about the
linear upper bound on the excess loss  for this is the quantity that needs to be controlled to make use
of the MetaGrad bound (2). With these variables in place  from this point on the story is the same for

Hedge and for OCO. So let us writeF for either the set N of experts or the setU of points  and f‚àó
for k‚àó resp. u‚àó  and let us consider the family{xf
t  f‚ààF  t‚àà N}. We call f‚ààF predictors. With
Condition 1. Fix B‚â• 0 and Œ∫‚àà[0  1]. The family (3) satisÔ¨Åes the(B  Œ∫)-Bernstein condition if
almost surely for all f‚ààF and rounds t‚àà N.

t)2Gt‚àí1 ‚â§ B Exf

this notation the Bernstein condition is the following.

tGt‚àí1Œ∫

E(xf

‚à∂= (cid:96)k
t‚àí (cid:96)k

‚àó

‚à∂= u‚àí u
‚àó

 ‚àá(cid:96)t(u)

3

The point of this stochastic condition is that it implies that the variance in the excess loss gets smaller
the closer a predictor gets to the optimum in terms of expected excess loss.

Some authors refer to the Œ∫= 1 case as the Massart condition. Van Erven et al. [2015] have shown

that the Bernstein condition is equivalent to the central condition  a fast-rate type of condition that has
been frequently used (without an explicit name) in density estimation under misspeciÔ¨Åcation. Two
more equivalent conditions appear in our proof sketch Section 4. We compare all four formulations
in Appendix B.

2.4 Main Result

‚àó
‚àó
In the stochastic case we evaluate the performance of algorithms by Rf
T   i.e. the regret compared

T ] is sometimes called the
to the predictor f‚àó with minimal expected loss. The expectation E[Rf
Theorem 2. In any stochastic setting satisfying the(B  Œ∫)-Bernstein Condition 1  the guarantees (1)

pseudo-regret. The following result shows that second-order methods automatically adapt to the
Bernstein condition. (Proof sketch in Section 4.)

for Squint and (2) for MetaGrad imply fast rates for the respective algorithms both in expectation
and with high probability. That is 

‚àó

2‚àíŒ∫  
E[Rf
T ] = OK
1‚àíŒ∫
2‚àíŒ∫
and for any Œ¥> 0  with probability at least 1‚àí Œ¥ 
T T
T = O(KT‚àí ln Œ¥) 1
2‚àíŒ∫ T
where for Squint KT ‚à∂= K f

‚àó
T from (1) and for MetaGrad KT is as in (2).

2‚àíŒ∫  
1‚àíŒ∫

Rf

‚àó

1

rates  one has to impose further assumptions on P. A standard assumption made in such cases is a

Crucially  the bound provided by Theorem 2 is natural  and  in general  the best one can expect.
This can be seen from considering the statistical learning setting  which is a special case of our

(cid:96)f
In this setting one usually considers excess risk  which is the expected loss difference between the

We see that Squint and MetaGrad adapt automatically to the Bernstein parameters of the distribution 
without any tuning. Theorem 2 only uses the form of the second-order bounds and does not depend
on the details of the algorithms  so it also applies to any other method with a second-order regret
bound. In particular it holds for Adapt-ML-Prod by Gaillard et al. [2014]  which guarantees (1) with

KT = O(lnF+ln ln T) for Ô¨Ånite sets of experts. Here we focus on Squint as it also applies to inÔ¨Ånite
sets. Appendix D provides an extension of Theorem 2 that allows using Squint with uncountableF.
setup. Here(xt  yt) are i.i.d.‚àº P andF is a set of functions fromX to a set of predictionsA  with
t ‚à∂= (cid:96)(yt  f(xt)) for some loss function (cid:96)‚à∂Y√óA‚Üí[0  1] such as squared  0~1  or absolute loss.
learned ÀÜf and the optimal f‚àó. The minimax expected (over training sample(xt  yt)) risk relative
to f‚àó is of order T‚àí1~2 (see e.g. Massart and N√©d√©lec [2006]  Audibert [2009]). To get better risk
Bernstein condition with exponent Œ∫> 0; see e.g. Koltchinskii [2006]  Bartlett and Mendelson [2006] 
IfF is sufÔ¨Åciently ‚Äòsimple‚Äô  e.g. a class with logarithmic entropy numbers (see Appendix D)  or  in
2‚àíŒ∫. The bound
achieves  in expectation  a better excess risk bound of order O(log T)‚ãÖ T‚àí 1
interpolates between T‚àí1~2 for Œ∫= 0 and T‚àí1 for Œ∫= 1 (Massart condition). Results of Tsybakov
t= 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL 
follow-the-leader)  this suggests that we can achieve a cumulative expected regret E[Rf
T ] of order
2‚àíŒ∫. Theorem 2 shows that this is  indeed  also the rate that Squint attains in such
O(log T)‚ãÖ T 1‚àíŒ∫
cases ifF is countable and the optimal f‚àó has positive prior mass œÄf
‚àó> 0 (more on this condition

[2004]  Massart and N√©d√©lec [2006]  Audibert [2009] suggest that this rate can  in general  not be
improved upon  and exactly this rate is achieved by ERM and various other algorithms in various
settings by e.g. Tsybakov [2004]  Audibert [2004  2009]  Bartlett et al. [2006]. By summing from

Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov
margin and other conditions.

classiÔ¨Åcation  a VC class  then  if a Œ∫-Bernstein condition holds  ERM (empirical risk minimization)

below)‚Äî we thus see that Squint obtains exactly the rates one would expect from a statistical

‚àó

4

Œ∫. Gr√ºnwald [2012] provides a means to tune Œ∑ automatically in terms of the data  but his method
‚Äî like ERM and all algorithms in the references above ‚Äî may achieve linear regret in worst-case

learning/classiÔ¨Åcation perspective  and the minimax excess risk results in that setting suggests that
these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert
[2004] that  when equipped with an oracle to tune the learning rate Œ∑ as a function of t  the rates

O(log T)‚ãÖ T 1‚àíŒ∫
2‚àíŒ∫ can also be achieved by Hedge  but the exact tuning depends on the unknown
settings  whereas Squint keeps the O(‚àö
Theorem 2 only gives the desired rate for Squint with inÔ¨ÅniteF ifF is countable and œÄf
‚àó> 0. The
‚àó= 0  as long asF admits sufÔ¨Åciently small entropy
of uncountably inÔ¨ÅniteF  which can have œÄf
numbers. Incidentally  this also allows us to show that Squint achieves regret rate O(log T)‚ãÖ T 1‚àíŒ∫
2‚àíŒ∫
whenF=i=1 2 ...Fi is a countably inÔ¨Ånite union ofFi with appropriate entropy numbers; in such
cases there can be  at every sample size  a classiÔ¨Åer ÀÜf‚ààF with 0 empirical error  so that ERM/FTL

combination of these two assumptions is strong or at least unnatural  and OCO cannot be readily used
in all such cases either  so in Appendix D we therefore show how to extend Theorem 2 to the case

T) guarantee for such cases.

will always over-Ô¨Åt and cannot be used even if the Bernstein condition holds; Squint allows for
aggregation of such models. In the remainder of the main text  we concentrate on applications for
which Theorem 2 can be used directly  without extensions.

3 Examples

Our OCO examples were chosen to be natural and illustrate fast rates without curvature.

We give examples motivating and illustrating the Bernstein condition for the Hedge and OCO settings.

Our examples in the Hedge setting will illustrate Bernstein with Œ∫< 1 and non i.i.d. distributions.
3.1 Hedge Setting: Gap implies Bernstein with Œ∫= 1
In the Hedge setting  we say that a distribution P (not necessarily i.i.d.) of expert losses{(cid:96)k
t  t  k‚àà N}
has gap Œ±> 0 if there is an expert k‚àó such that
k‚â†k‚àó E(cid:96)k
tGt‚àí1(cid:6)
almost surely for each round t‚àà N.
It is clear that the condition can only hold for k‚àó the minimiser of the expected loss.
Lemma 3. A distribution with gap Œ± is( 1
Œ±   1)-Bernstein.
t)2Gt‚àí1(cid:6)‚â§ 1= 1
Exk
tGt‚àí1(cid:6) .
Proof. For all k‚â† k‚àó and t  we have E(xk

t Gt‚àí1+ Œ± ‚â§ inf
E(cid:96)k

Œ± Œ±‚â§ 1

T = O(KT)= O(ln ln T) rate. Gaillard et al. [2014] show constant

‚àó

‚àó

Œ±

By Theorem 2 we get the Rk
regret for Ô¨Ånitely many experts and i.i.d. losses with a gap. Our alternative proof above shows that
neither Ô¨Åniteness nor i.i.d. are essential for fast rates in this case.

The next example illustrates that we can sometimes get the fast rates without a gap. And it also shows
that we can get any intermediate rate: we construct an example satisfying the Bernstein condition for

3.2 Hedge Setting: Any(1  Œ∫)-Bernstein
any Œ∫‚àà[0  1] of our choosing (such examples occur naturally in classiÔ¨Åcation settings such as those
Fix Œ∫‚àà[0  1]. Each expert k= 1  2  . . . is parametrised by a real number Œ¥k‚àà[0  1~2]. The only
assumption we make is that Œ¥k= 0 for some k  and inf k{Œ¥k Œ¥k> 0}= 0. For a concrete example let
us choose Œ¥1= 0 and Œ¥k= 1~k for k= 2  3  . . . Expert Œ¥k has loss 1~2¬± Œ¥k with probability 1¬±Œ¥2~Œ∫‚àí1
  and so Œ¥1= 0 is best 
+ Œ¥2~Œ∫
with loss deterministically equal to 1~2. The squared excess loss of Œ¥k is Œ¥2
condition with exponent Œ∫ (but no Œ∫‚Ä≤> Œ∫) and constant 1  and the associated regret rate by Theorem 2.

independently between experts and rounds. Expert Œ¥k has mean loss 1
2

considered in the example in Appendix D).

k. So we have the Bernstein

k
2

k

5

‚àó

‚àö

Note that for Œ∫= 0 (the hard case) all experts have mean loss equal to 1
we designate as the best expert our pseudo-regret E[Rk

‚àó
2 independently at random. Hence  by the central limit theorem  with high
their losses deviate from 1
probability our regret Rk
T is of order
case)  we do not Ô¨Ånd a gap. We still have experts arbitrary close to the best expert in mean  but their
expected excess loss squared equals their expected excess loss.
ERM/FTL (and hence all approaches based on it  such as [Bartlett and Mendelson  2006]) may fail

2. So no matter which k‚àó
T ] is zero. Yet the experts do not agree  as
T . On the other side of the spectrum  for Œ∫= 1 (the best
completely on this type of examples. The clearest case is when{k Œ¥k> } is inÔ¨Ånite for some > 0.
of them will result in expected instantaneous regret at least 2~Œ∫  leading to linear regret overall.
The requirement Œ¥k= 0 for some k is essential. If instead Œ¥k> 0 for all k then there is no best expert

Then at any t there will be experts that  by chance  incurred their lower loss every round. Picking any

in the class. Theorem 19 in Appendix D shows how to deal with this case.

3.3 Hedge Setting: Markov Chains

2

6

2

2

1

2

regret of order

E(xf

3.4 OCO: Hinge Loss on the Unit Ball

T 2m. Then  if the data are actually generated by an m-th order Markov chain with

Suppose we model a binary sequence z1  z2  . . .   zT with m-th order Markov chains. As experts we

‚àö
t)2(zt‚àím  . . .   zt‚àí1)= a= 1 

consider all possible functions f‚à∂{0  1}m‚Üí{0  1} that map a history of length m to a prediction
for the next outcome  and the loss of expert f is the 0~1-loss: (cid:96)f
t =f(zt‚àím  . . .   zt‚àí1)‚àí zt. (We
initialize z1‚àím= . . .= z0= 0.) A uniform prior on this Ô¨Ånite set of 22m experts results in worst-case
transition probabilities P(zt= 1(zt‚àím  . . .   zt‚àí1)= a)= pa  we have f‚àó(a)= 1{pa‚â• 1
} and

Exf
t(zt‚àím  . . .   zt‚àí1)= a= 2pa‚àí 1
for any f such that f(a)‚â† f‚àó(a). So the Bernstein condition holds with Œ∫= 1 and B=
2 minapa‚àí 1
.
Let(x1  y1) (x2  y2)  . . . be classiÔ¨Åcation data  with yt‚àà{‚àí1 +1} and xt‚àà Rd  and consider the
hinge loss (cid:96)t(u)= max{0  1‚àí ytxt  u}. Now suppose  for simplicity  that both xt and u come
from the d-dimensional unit Euclidean ball  such thatxt  u‚àà[‚àí1 +1] and hence the hinge is never
active  i.e. (cid:96)t(u)= 1‚àí ytxt  u. Then  if the data turn out to be i.i.d. observations from a Ô¨Åxed
distribution P  the Bernstein condition holds with Œ∫= 1 (the proof can be found in Appendix C):
xt  u‚â§ 1. If the data are i.i.d.  then the(B  Œ∫)-Bernstein condition is satisÔ¨Åed with Œ∫= 1 and
B= 2Œªmax
¬µ   where Œªmax is the maximum eigenvalue of E[xx] and ¬µ= E[yx]  provided that¬µ> 0.
In particular  if xt is uniformly distributed on the sphere and yt = sign(¬Øu  xt) is the noiseless
classiÔ¨Åcation of xt according to the hyper-plane with normal vector ¬Øu  then B‚â§ c‚àö
constant c> 0.
The excluded case¬µ= 0 only happens in the degenerate case that there is nothing to learn  because
¬µ= 0 implies that the expected hinge loss is 1  its maximal value  for all u.
LetU=[0  1] be the unit interval. Consider the absolute loss (cid:96)t(u)=u‚àí xt where xt‚àà[0  1] are
drawn i.i.d. from P. Let u‚àó‚àà arg minu Eu‚àí x minimize the expected loss. In this case we may
simplifyw‚àí u‚àó ‚àá(cid:96)(w)=(w‚àí u‚àó) sign(w‚àí x). To satisfy the Bernstein condition  we therefore
want B such that  for all w‚àà[0  1] 
‚àó) sign(w‚àí x)2 ‚â§ B E[(w‚àí u
‚àó2‚àíŒ∫ ‚â§ B2Œ∫ P(x‚â§ w)‚àí 1
w‚àí u

‚àó) sign(w‚àí x)]Œ∫ .
Œ∫.

Lemma 4 (Unregularized Hinge Loss Example). Consider the hinge loss setting above  where

E(w‚àí u

3.5 OCO: Absolute Loss

for some absolute

d

That is 

For instance  if the distribution of x has a strictly positive density p(x)‚â• m> 0  then u‚àó is the
= P(x‚â§ w)‚àí P(x‚â§ u‚àó)‚â• mw‚àí u‚àó  so the condition holds with Œ∫= 1
median and P(x‚â§ w)‚àí 1
and B= 1
1‚àí p  the condition holds with Œ∫= 1 and B=
w‚àí u‚àó‚â§ 1 and P(x‚â§ w)‚àí 1
‚â•p‚àí 1
.

2m. Alternatively  for a discrete distribution on two points a and b with probabilities p and
2  as can be seen by bounding

12p‚àí1  provided that p‚â† 1

2

2

2

4 Proof Ideas

This section builds up to prove our main result Theorem 2. We Ô¨Årst introduce the handy ESI-
abbreviation that allows us to reason simultaneously in expectation and with high probability. We
then provide two alternative characterizations of the Bernstein condition that are equivalent for
bounded losses. Finally  we show how one of these  ESI-Bernstein  combines with individual-
sequence second-order regret bounds to give rise to Theorem 2.

4.1 Notation: Exponential Stochastic Inequality (ESI  pronounce easy)

Lemma 6. Exponential stochastic negativity/inequality has the following useful properties:

DeÔ¨Ånition 5. A random variable X is exponentially stochastically negative  denoted X  0  if
E[eX]‚â§ 1. For any Œ∑‚â• 0  we write XŒ∑ 0 if Œ∑X 0. For any pair of random variables X and Y  
the exponential stochastic inequality (ESI) XŒ∑ Y is deÔ¨Åned as expressing X‚àí Y Œ∑ 0; X Y is
deÔ¨Åned as X1 Y .
1. (Negativity). Let X 0. As the notation suggests X is negative in expectation and with high
probability. That is E[X]‚â§ 0 and P{X‚â•‚àí ln Œ¥}‚â§ Œ¥ for all Œ¥> 0.
2. (Convex combination). LetX f
probability distribution onF. If X f 0 for all f then Ef‚àºw[X f] 0.
f‚ààF be a family of random variables and let w be a
3. (Chain rule). Let X1  X2  . . . be adapted to Ô¨ÅltrationG1‚äÜG2 . . . (i.e. Xt isGt-measurable
for each t). If XtGt‚àí1 0 almost surely for all t  then‚àëT
Proof. Negativity: By Jensen‚Äôs inequality E[X]‚â§ ln EeX(cid:6)‚â§ 0  whereas by Markov‚Äôs inequal-
 ‚â§ Œ¥ EeX(cid:6) ‚â§ Œ¥. Convex combination: By Jensen‚Äôs inequality
ity P{X‚â•‚àí ln Œ¥} = PeX‚â• 1
Ef‚àºw[X f]‚â§ Ef‚àºw EeX f‚â§ 1. Chain rule: By induction. The base case T= 0 holds trivially.
Ee
For T> 0 we have Ee‚àëT

t=1 Xt EeXTGT‚àí1(cid:6)‚â§ Ee‚àëT‚àí1

t=1 Xt 0 for all T‚â• 0.

t=1 Xt= Ee‚àëT‚àí1

t=1 Xt‚â§ 1.

Œ¥

‚àó

‚àó

4.2 The Bernstein Condition and Second-order Bounds

Our main result Theorem 2  bounds the regret Rf

‚àó
T compared to the stochastically optimal predictor

T = O(
T ‚â§ ÀúRf

f‚àó when the sequence of losses (cid:96)1  (cid:96)2  . . . comes from a Bernstein distribution P. For simplicity we
T KT).
V f‚àó
‚àó
‚àó
T is
‚àó
‚àó
T with high probability. Combination with the individual-sequence bound
T is bounded in terms of a function of itself. And solving the inequality for ÀúRf

only consider the OCO setting in this sketch. Full details are in Theorem 11. Our starting point
will be the individual-sequence second-order bound (2)  which implies Rf
The crucial technical contribution of this paper is to establish that for Bernstein distributions V f
bounded in terms of ÀúRf
‚àó
then gives that ÀúRf
establishes the fast rates for Rf
T .
‚àó

‚àó
T would be bounded in terms of ÀúRf

t=1(xft
T =‚àëT
T =‚àëT
t )2 and ÀúRf
t )2 in terms of xft
algorithm in round t. We will bound(xft
t=1 xft
Condition 1 for Œ∫= 1 directly yields
t  = B E ÀúRf
Exft
t )2 ‚â§ B
E(xft
EV f
T  = TQ
T  .
TQ
t=1
t=1

‚àó
T   we look at their relation
t where ft is the prediction of the
t separately for each round t. The Bernstein

To get a Ô¨Årst intuition as to why V f
in expectation. Recall that V f

(4)

‚àó

T

‚àó

‚àó

7

set of linear inequalities:
Condition 7. The excess loss family (3) satisÔ¨Åes the linearized Œ∫-Bernstein condition if there are

For Œ∫< 1 the Ô¨Ånal step of interchanging expectation and sums does not work directly  but we may use
zŒ∫= Œ∫Œ∫(1‚àí Œ∫)1‚àíŒ∫ inf >0Œ∫‚àí1z+ Œ∫ for z‚â• 0 to rewrite the Bernstein condition as the following
constants c1  c2> 0 such that we have:
t)2Gt‚àí1‚àí Exf
c1‚ãÖ 1‚àíŒ∫ EV f

a.s. for all > 0  f‚ààF and t‚àà N.
T + c2‚ãÖ T‚ãÖ .

tGt‚àí1 ‚â§ c2‚ãÖ 
T  ‚â§ E ÀúRf

This gives the following generalization of (4):
‚àó

c1‚ãÖ 1‚àíŒ∫‚ãÖ E(xf

(5)

‚àó

Together with the individual sequence regret bound and optimization of  this can be used to derive
the in-expectation part of Theorem 2.

‚àó
Getting the in-probability part is more difÔ¨Åcult  however  and requires relating V f
T in
T
probability instead of in expectation. Our main technical contribution does exactly this  by showing
that the Bernstein condition is in fact equivalent to the following exponential strengthening of
Condition 7:

Condition 8. The family (3) satisÔ¨Åes the Œ∫-ESI-Bernstein condition if there are c1  c2> 0 such that:

and ÀúRf

‚àó

c1‚ãÖ 1‚àíŒ∫‚ãÖ(xf

t)2‚àí xf

tGt‚àí1 1‚àíŒ∫ c2‚ãÖ 

a.s. for all > 0  f‚ààF and t‚àà N.

‚àó

‚àó

‚àó

Condition 8 implies Condition 7 by Jensen‚Äôs inequality (see Lemma 6 part 1). The surprising converse
is proved in Lemma 9 in the appendix. By telescoping over rounds using the chain rule from Lemma 6 
we see that ESI-Bernstein implies the following substantial strengthening of (5):

Now the second-order regret bound (2) can be rewritten  using 2

a.s. for all > 0  T‚àà N.
T ‚àí ÀúRf
T 1‚àíŒ∫ c2‚ãÖ T‚ãÖ 
c1‚ãÖ 1‚àíŒ∫‚ãÖ V f
‚àö
ab= inf Œ≥ Œ≥a+ b~Œ≥  as:

T ‚ãÖ KT+ 2KT ‚â§ Œ≥‚ãÖ V f
T + KT
+ 2KT .
T ‚â§ 2
for every Œ≥> 0‚à∂ 2 ÀúRf
V f‚àó
Plugging in Œ≥= c11‚àíŒ∫ we can chain this inequality with (6) to give  for all > 0 
+ 2KT  
T + c2‚ãÖ T‚ãÖ + KT
T 1‚àíŒ∫ ÀúRf
c1‚ãÖ 1‚àíŒ∫
and both parts of Theorem 2 now follow by rearranging  plugging in the minimiser √† K

(6)

(7)
T T 1‚àíŒ∫
2‚àíŒ∫
2‚àíŒ∫  

1

2 ÀúRf

‚àó

‚àó

‚àó

Œ≥

and using Lemma 6 part 1.

Acknowledgments

Koolen acknowledges support by the Netherlands Organization for ScientiÔ¨Åc Research (NWO  Veni
grant 639.021.439).

References
J-Y. Audibert. PAC-Bayesian statistical learning theory. PhD thesis  Universit√© Paris VI  2004.
J-Y. Audibert. Fast learning rates in statistical inference through aggregation. Ann. Stat.  37(4)  2009.
P. Bartlett and S. Mendelson. Empirical minimization. Probab. Theory Rel.  135(3):311‚Äì334  2006.
P. Bartlett  M. Jordan  and J. McAuliffe. Convexity  classiÔ¨Åcation  and risk bounds. J. Am. Stat. Assoc.  101
(473):138‚Äì156  2006.
N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
N. Cesa-Bianchi  Y. Mansour  and G. Stoltz. Improved second-order bounds for prediction with expert advice.
Machine Learning  66(2/3):321‚Äì352  2007.
C. Chiang  T. Yang  C. Le  M. Mahdavi  C. Lu  R. Jin  and S. Zhu. Online optimization with gradual variations.
In Proc. 25th Conf. on Learning Theory (COLT)  2012.
K. Crammer  A. Kulesza  and M. Dredze. Adaptive regularization of weight vectors. In NIPS 22  2009.

8

J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization.
Journal of Machine Learning Research  12:2121‚Äì2159  2011.
T. van Erven and W. Koolen. MetaGrad: Multiple learning rates in online learning. In Advances in Neural
Information Processing Systems 29  2016.
T. van Erven  P. Gr√ºnwald  N. Mehta  M. Reid  and R. Williamson. Fast rates in statistical and online learning.
Journal of Machine Learning Research  16:1793‚Äì1861  2015.
E. Even-Dar  M. Kearns  Y. Mansour  and J. Wortman. Regret to the best vs. regret to the average. Machine
Learning  72(1-2)  2008.
Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to
boosting. Journal of Computer and System Sciences  55:119‚Äì139  1997.
P. Gaillard and S. Gerchinovitz. A chaining algorithm for online nonparametric regression. In Proc. 28th Conf.
on Learning Theory (COLT)  2015.
P. Gaillard  G. Stoltz  and T. van Erven. A second-order bound with excess losses. In Proc. 27th COLT  2014.
P. Gr√ºnwald. The safe Bayesian: learning the learning rate via the mixability gap. In ALT ‚Äô12. Springer  2012.
E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine
learning  80(2-3):165‚Äì188  2010.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Ann. Stat.  34(6):
2593‚Äì2656  2006.
W. Koolen. The relative entropy bound for Squint. Blog entry on blog.wouterkoolen.info/  August 2015.
W. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games. In Proc.
28th Conf. on Learning Theory (COLT)  pages 1155‚Äì1175  2015.
W. Koolen  T. van Erven  and P. Gr√ºnwald. Learning the learning rate for prediction with expert advice. In
Advances in Neural Information Processing Systems 27  pages 2294‚Äì2302  2014.
H. Luo and R. Schapire. Achieving all with no parameters: Adaptive normalhedge. In Proc. 28th COLT  2015.
P. Massart and √â. N√©d√©lec. Risk bounds for statistical learning. Ann. Stat.  34(5):2326‚Äì2366  2006.
B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In Proc. 23rd
Conf. on Learning Theory (COLT)  pages 244‚Äì256  2010.
N. Mehta and R. Williamson. From stochastic mixability to fast rates. In NIPS 27  2014.
F. Orabona  K. Crammer  and N. Cesa-Bianchi. A generalized online mirror descent with applications to
classiÔ¨Åcation and regression. Machine Learning  99(3):411‚Äì435  2015.
A. Rakhlin and K. Sridharan. Online nonparametric regression. In Proc. 27th COLT  2014.
S. de Rooij  T. van Erven  P. Gr√ºnwald  and W. Koolen. Follow the leader if you can  Hedge if you must.
Journal of Machine Learning Research  15:1281‚Äì1316  April 2014.
A. Sani  G. Neu  and A. Lazaric. Exploiting easy data in online optimization. In NIPS 27  2014.
S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine
Learning  4(2):107‚Äì194  2011.
J. Steinhardt and P. Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In Proc.
31th Int. Conf. on Machine Learning (ICML)  pages 1593‚Äì1601  2014.
A. Tsybakov. Optimal aggregation of classiÔ¨Åers in statistical learning. Ann. Stat.  32:135‚Äì166  2004.
O. Wintenberger. Optimal learning with Bernstein Online Aggregation. ArXiv:1404.1356  2015.

9

,Wouter Koolen
Peter Gr√ºnwald
Tim van Erven