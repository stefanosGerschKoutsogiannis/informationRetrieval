2019,Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions,We propose a projected Stein variational Newton (pSVN) method for high-dimensional Bayesian inference. To address the curse of dimensionality  we exploit the intrinsic low-dimensional geometric structure of the posterior distribution in the high-dimensional parameter space via its Hessian (of the log posterior) operator and perform a parallel update of the parameter samples projected into a low-dimensional subspace by an SVN method. The subspace is adaptively constructed using the eigenvectors of the averaged Hessian at the current samples. We demonstrate fast convergence of the proposed method  complexity independent of the parameter and sample dimensions  and parallel scalability.,Projected Stein Variational Newton: A Fast and

Scalable Bayesian Inference Method

in High Dimensions

Peng Chen  Keyi Wu  Joshua Chen  Thomas O’Leary-Roseberry  Omar Ghattas

Oden Institute for Computational Engineering and Sciences

The University of Texas at Austin

Austin  TX 78712.

{peng  keyi  joshua  tom  omar}@oden.utexas.edu

Abstract

We propose a projected Stein variational Newton (pSVN) method for high-
dimensional Bayesian inference. To address the curse of dimensionality  we exploit
the intrinsic low-dimensional geometric structure of the posterior distribution in
the high-dimensional parameter space via its Hessian (of the log posterior) op-
erator and perform a parallel update of the parameter samples projected into a
low-dimensional subspace by an SVN method. The subspace is adaptively con-
structed using the eigenvectors of the averaged Hessian at the current samples. We
demonstrate fast convergence of the proposed method  complexity independent of
the parameter and sample dimensions  and parallel scalability.

1

Introduction

Bayesian inference provides an optimal probability formulation for learning complex models from
observational or experimental data under uncertainty by updating the model parameters from their
prior distribution to a posterior distribution [30]. In Bayesian inference we typically face the task
of drawing samples from the posterior probability distribution to compute various statistics of some
given quantities of interest. However  this is often prohibitive when the posterior distribution is
high-dimensional; many conventional methods for Bayesian inference suffer from the curse of
dimensionality  i.e.  computational complexity grows exponentially or convergence deteriorates with
increasing parameter dimension.
To address this curse of dimensionality  several efﬁcient and dimension-independent methods have
been developed that exploit the intrinsic properties of the posterior distribution  such as its smooth-
ness  sparsity  and intrinsic low-dimensionality. Markov chain Monte Carlo (MCMC) methods
exploiting geometry of the log-likelihood function have been developed [16  21  24  12  3]  providing
more effective sampling than black-box MCMC. For example  the DILI MCMC method [12] uses
the low rank structure of the Hessian of the negative log likelihood in conjunction with operator-
weighted proposals that are well-deﬁned on function space to yield a sampler whose performance is
dimension-independent and effective at capturing information provided by the data. However  despite
these enhancements  MCMC methods remain prohibitive for problems with expensive-to-evaluate
likelihoods (i.e.  involving complex models) and in high parameter dimensions. Deterministic sparse
quadratures were developed in [28  26  8] and shown to converge rapidly with dimension-independent
rates for smooth and sparse problems. However  the fast convergence is lost when the posterior has
signiﬁcant local variations  despite enhancements with Hessian-based transformation [27  9].
Variational inference methods reformulate the sampling problem as an optimization problem that
approximates the posterior by minimizing its Kullback–Leibler divergence with a transformed prior

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

[22  20  4]  which can be potentially much faster than MCMC. In particular  Stein variational methods 
which seek a composition of a sequence of simple transport maps represented by kernel functions
using gradient descent (SVGD) [20  11  19] and especially Newton (SVN) [14] optimization methods 
are shown to achieve fast convergence in relatively low dimensions. However  these variational
optimization methods can again become deteriorated in convergence and accuracy in high dimensions.
The curse of dimensionality can be partially addressed by a localized SVGD on Markov blankets 
which relies on a conditional independence structure of the target distribution [32  31].
Contributions: In this work  we develop a projected Stein variational Newton method (pSVN)
to tackle the challenge of high-dimensional Bayesian inference by exploiting the intrinsic low-
dimensional geometric structure of the posterior distribution (where it departs from the prior)  as
characterized by the dominant spectrum of the prior-preconditioned Hessian of the negative log
likelihood. This low-rank structure  or fast decay of eigenvalues of the preconditioned Hessian  has
been proven for some inference problems and commonly observed in many others with complex
models [5  6  29  18  12  9  10  2  7]. By projecting the parameters into this data-informed low-
dimensional subspace and applying the SVN in this subspace  we can effectively mitigate the curse
of dimensionality. We demonstrate fast convergence of pSVN that is independent of the number of
parameters and samples. In particular  in two (both linear and nonlinear) experiments we show that
the intrinsic dimension is a few (6) and a few tens (40) with the nominal dimension over 1K and 16K 
respectively. We present a scalable parallel implementation of pSVN that yields rapid convergence 
minimal communication  and low memory footprint  thanks to this low-dimensional projection.
Below  we present background on Bayesian inference and Stein variational methods in Section
2  develop the projected Stein variational Newton method in Section 3  and provide numerical
experiments in Section 4.

2 Background

2.1 Bayesian inference
We consider a random parameter x ∈ Rd  d ∈ N  with a prior probability density function p0 : Rd →
R  and noisy observational data y of a parameter-to-observable map f : Rd → Rs  s ∈ N  i.e. 

(1)
where ξ ∈ Rs represents observation noise with probability density function pξ : Rs → R. The
posterior density p(·|y) : Rd → R of x conditioned on the data y is given by Bayes’ rule

y = f (x) + ξ 

p(x|y) =

py(x)  where py(x) := pξ(y − f (x)) p0(x) 

(2)
and the normalization constant Z  typically Z (cid:54)= 1 if pξ or p0 is known up to a constant  is given by
(3)

Z := Ep0[pξ(y − f (x))] =

pξ(y − f (x))p0(x)dx.

1
Z

(cid:90)

Rd

In practice  Z is computationally intractable  especially for large d.

2.2 Stein variational methods

While sampling from the prior is tractable  sampling from the posterior is a great challenge. One
method to sample from the posterior is to ﬁnd a transport map T : Rd → Rd in a certain function class
T that pushes forward the prior to the posterior by minimizing a Kullback–Leibler (KL) divergence
(4)

T∈T DKL(T∗p0|py).

min

Stein variational methods [20  14] simplify the minimization of (4) for one possibly very complex
and nonlinear transport map T to a sequence of simpler transport maps that are perturbations of the
identity  i.e.  T = TL ◦ TL−1 ◦ ··· ◦ T2 ◦ T1  L ∈ N  where

(5)
with I(x) = x  step size ε  and perturbation map Ql : Rd → Rd. Let pl denote the pushforward
density pl := (Tl ◦ ··· ◦ T1)∗p0. For l = 1  2  . . .   we deﬁne a cost functional Jl(Q) as

Tl(x) = I(x) + εQl(x) 

l = 1  . . .   L 

Jl(Q) := DKL((I + Q)∗pl−1|py).

(6)

2

Ql(x) =

cnkn(x) 

(8)

where cn ∈ Rd  n = 1  . . .   N  are unknown coefﬁcient vectors. It is shown in [14] that the coefﬁcient
vector c = (c(cid:62)

N )(cid:62) ∈ RN d is a solution of the linear system

1   . . .   c(cid:62)

N(cid:88)

n=1

Hc = −g 

Then at step l  Stein variational methods lead to

Ql = −H−1

l ∇Jl(0) 

(7)
where ∇Jl(0) : Rd → Rd is the Fréchet derivative of Jl(Q) evaluated at Q = 0  and Hl is a
preconditioner. For the SVGD method [20]  Hl = I  while for the SVN method [14]  Hl ≈ ∇2Jl(0) 
an approximation of the Hessian of the cost functional ∇2Jl(0).
Given basis functions kn : Rd → R  n = 1  . . .   N  an ansatz representation of Ql is deﬁned as

(9)

where g = (g(cid:62)

1   . . .   g(cid:62)

N )(cid:62) ∈ RN d is the gradient vector given by
gm := Epl−1[−∇x log(py)km − ∇xkm]  m = 1  . . .   N 

Hmn := Epl−1 [−∇2

(10)
and H ∈ RN d×N d is the Hessian matrix  speciﬁed as the identity for SVGD [20]  which leads to
cn = −gn  n = 1  . . .   N  while for SVN it is given with mn-block Hmn ∈ Rd×d by [14]
x log(py)knkm + ∇xkn(∇xkm)(cid:62)]  m  n = 1  . . .   N.

(11)
At each step l = 1  2  . . .   the expectation Epl−1 [·] in (10) and (11) are approximated by the sample
average approximation with samples xl−1
N   which are drawn from the prior at l = 1 and
pushed forward by (5) once the coefﬁcients c1  . . .   cN are obtained. We remark that in the original
SVGD method [20]  the samples are moved with the simpliﬁed perturbation Ql(xm) = cm.
In both [20] and [14]  the basis functions kn(x) are speciﬁed by a suitable kernel function kn(x) =
k(x  x(cid:48)) at x(cid:48) = xn  n = 1  . . .   N  e.g.  a Gaussian kernel given by
(x − x(cid:48))(cid:62)M (x − x(cid:48))

k(x  x(cid:48)) = exp

  . . .   xl−1

(cid:18)

(cid:19)

(12)

1

 

− 1
2

where M is a metric that measures the distance between x and x(cid:48) ∈ Rd. In [20]  it is speciﬁed
as rescaled identity matrix αI for α > 0 depending on the samples  while in [14]  M is given
by M = Epl−1 [−∇2
x log(py)]/d to account for the geometry of the posterior by averaged Hessian
information. This was shown to accelerate convergence for both SVGD and SVN compared to αI.
We remark that for high-dimensional complex models where a direct computation of the Hessian
∇2
x log(py) is not tractable  its low-rank decomposition by randomized algorithms can be applied.

3 Projected Stein variational Newton

3.1 Dimension reduction by projection

Stein variational methods suffer from the curse of dimensionality  i.e.  the sample estimate (e.g.  for
variance) deteriorates considerably in high dimensions because the global kernel function (12) cannot
represent the transport map well  as shown in [32  31] for SVGD. This challenge can be alleviated in
moderate dimensions by a suitable choice of the metric in (12) as demonstrated in [14]. However it is
still present when the dimension becomes high. An effective method to tackle this difﬁculty  which
relies on conditional independence of the posterior density  uses local kernel functions deﬁned over a
Markov blanket with much lower dimension  thus achieving effective dimension reduction [32  31].
In many applications  even when the nominal dimension of the parameter is very high  the intrinsic
parameter dimension informed by the data is typically low  i.e.  the posterior density is effectively
different from the prior density only in a low-dimensional subspace [5  6  29  18  12  9  10  2]. This is
because: (i) the prior p0 may have correlation in different dimensions  (ii) the parameter-to-observable
map f may be smoothing/regularizing  (iii) the data y may not be very informative  or a combined

3

r(cid:88)

i=1

r(cid:88)

effect. Let Ψ = (ψ1  . . .   ψr) ∈ Rd×r denote the basis of a subspace of dimension r (cid:28) d in Rd.
Then we can project the parameter x with mean ¯x into this subspace as

ψi(ψi  (x − ¯x))H = ¯x +

xr = ¯x + Pr(x − ¯x) = ¯x +

i=1

i Γ−1

ψiwi = ¯x + Ψw 

0 ψj = δij. We deﬁne the projected posterior as

(13)
where w = (w1  . . .   wr) ∈ Rr is a vector of coefﬁcients wi = (ψi  x − ¯x)H of the projection
0 (x − ¯x) where Γ0 is the prior
of x − ¯x to ψi in a suitable norm H  e.g.  (ψi  x − ¯x)H = ψT
covariance of x and ψT
pr(x|y) =
1
(14)
Z r pr
Then we can establish convergence under the following assumption. We deﬁne || · ||X as a suitable
norm  e.g.  ||x||2
X = xT Xx with X = I  the identity matrix or a mass matrix discretized from
identity operator in ﬁnite dimension approximation space in our numerical experiments.
Assumption 1. For Gaussian noise ξ ∈ N (0  Γ) with s.p.d. covariance Γ ∈ Rs×s. Let ||v||Γ :=
(vT Γ−1v)1/2 for any v ∈ Rs. Assume there exists a constant Cf > 0 such that for any xr in (13)

y(x) = pξ(y − f (xr))p0(x) and Z r = Ep0 [pξ(y − f (xr))].

y(x)  where pr

i Γ−1

For every b > 0  assume there is Cb > 0 such that for all x1  x2 with max{||x1||X  ||x2||X} < b 

Ep0 [||f (xr)||Γ] ≤ Cf and Ep0[||f (x)||Γ] ≤ Cf .

(15)

(16)

||f (x1) − f (x2)||Γ ≤ Cb||x1 − x2||X .

We state the convergence result for the projected posterior density in the following theorem  whose
proof is presented in Appendix A.
Theorem 1. Under Assumption 1  there exists a constant C independent of r such that

DKL(p(x|y)| pr(x|y)) ≤ C||x − xr||X .

(17)
Remark 1. Theorem 1 indicates that the projected posterior converges to the full one as along as
the projected parameter converges in X-norm  and that the convergence of the former is bounded by
the latter. In practical applications  the former may converge faster than the latter because it only
depends on the data-informed subspace while the latter is measured in data-independent X-norm.

3.2 Projected Stein variational Newton

Let pr

0 denote the prior densities for xr in (13). Let x⊥ = x − xr. Then the prior is decomposed as
(18)
0 (x⊥) if p0 is a Gaussian density. Then

0 (x⊥|xr) 
0 (x⊥|xr) is a conditional density  which becomes p⊥

0(xr)p⊥

p0(x) = pr

where p⊥
the projected posterior density pr

y(x) in (14) becomes
y(x) = pξ(y − f (xr))pr
pr

0(xr)p⊥

0 (x⊥|xr) 

y as a sample from pr

y(xr) = pξ(y − f (xr))pr

y(x) can be realized by sampling from pr

0 (x⊥|xr) for x⊥ conditioned on xr (or from p⊥

(19)
0(xr) for
so that sampling from pr
0 (x⊥) if p0 is Gaussian). To sample
xr and from p⊥
from the posterior  we can sample x from the prior  decompose it as x = xr + x⊥  freeze x⊥  push
xr to xr
y(xr) in the projection subspace  we seek a transport map T that pushes forward
To sample from pr
y(xr) by minimizing the KL divergence between them. Since the randomness of xr =
0(xr) to pr
pr
¯x + Ψw is fully represented by w given the projection basis Ψ  we just need to ﬁnd a transport
map that pushes forward π0(w) = pr
y(xr) in the (coefﬁcient) parameter space
Rr  where r (cid:28) d. Similarly in the full space  we look for a composition of a sequence of maps
T = TL ◦ TL−1 ◦ ··· ◦ T2 ◦ T1  L ∈ N  with

y(xr)  and construct the posterior sample as xy = xr

0(xr) to πy(w) = pr

y + x⊥.

(20)
where the perturbation map Ql is represented by the basis functions kn : Rr → R  n = 1  . . .   N  as

Tl(w) = I(w) + εQl(w) 

l = 1  . . .   L 

Ql(w) =

cnkn(w) 

(21)

N(cid:88)

n=1

4

Then the coefﬁcient vector c = ((c1)(cid:62)  . . .   (cN )(cid:62))(cid:62) ∈ RN r is the solution of the linear system

Hc = −g.
Here the m-th component of the gradient g is deﬁned as

gm := Eπl−1[−∇w log(πy)km − ∇wkm] 

(22)

(23)

(27)

(28)

and the mn-th component of the Hessian H for pSVN is deﬁned as

Hmn := Eπl−1[−∇2

(24)
The expectations in (23) and (24) are evaluated by sample average approximation at samples
wl−1
n = T (wl−1
n ) 
n = 1  . . .   N. By the deﬁnition of the projection (13)  we have

N   which are drawn from π0 for l = 1 and pushed forward by (20) as wl

  . . .   wl−1

w log(πy)knkm + ∇wkn(∇wkm)(cid:62)].

1

∇w log(πy(w)) = Ψ(cid:62)∇x log(pr

(25)
For the basis functions kn  n = 1  . . .   N  we use a Gaussian kernel kn(w) = k(w  wn) deﬁned as in
(12)  with the metric M given by an averaged Hessian at the current samples wl−1
N   i.e. 

  . . .   wl−1

y(xr))Ψ.

x log(pr

w log(πy(w)) = Ψ(cid:62)∇2

y(xr))  and ∇2

1

M = − 1
r

Eπl−1[∇2

w log(πy)] ≈ − 1
rN

(26)
We remark that the projected system (22) is of size N r × N r  which is a considerable reduction from
the full system (9) of size N d × N d  since r (cid:28) d. To further reduce the size of the coupled system
(22)  we use a classical “mass-lumping” technique to decouple it as N systems of size r × r

n )).

n=1

∇2
w log(πy(wl−1

N(cid:88)

where gm is given as in (25)  and Hm is given by the lumped Hessian

Hmcm = −gm  m = 1  . . .   N 

N(cid:88)

Hm :=

Hmn  m = 1  . . .   N 

with Hmn deﬁned in (24). We refer to [14] for this technique and a diagonalization Hm = Hmm.
Moreover  to ﬁnd a good step size ε in (20)  we adopt a classical line search [23]  see Appendix B.

n=1

3.3 Hessian-based subspace

i Γ−1

To construct a data-informed subspace of the parameter space  we exploit the geometry of the posterior
density characterized by its Hessian. More speciﬁcally  we seek the basis functions ψi  i = 1  . . .   r 
as the eigenvectors corresponding to the r largest eigenvalues of the generalized eigenvalue problem
(29)

xηy(x)]ψi = λiΓ−1

i = 1  . . .   r 

E[∇2

0 ψi 

where Γ0 is the covariance of x under the prior distribution (not necessarily Gaussian)  ψT
0 ψj =
δij  i  j = 1  . . .   r  E[∇2
xηy(x)]  with ηy(x) := − log(pξ(y − f (x)))  is the averaged Hessian of the
negative log-likelihood function w.r.t. a certain distribution  e.g.  the prior  posterior  or Gaussian
approximate distribution [13]. Here we propose to evaluate E[∇2
xηy(x)] by an adaptive sample
average approximation at the samples pushed from the prior to the posterior  and adaptively construct
the eigenvectors Ψ  as presented in next section. For linear Bayesian inference problems  with
f (x) = Ax for A ∈ Rs×d  a Gaussian prior distribution x ∼ N (¯x  Γ0) and a Gaussian noise
ξ ∼ N (0  Γξ) lead to a Gaussian posterior distribution given by N (xMAP  Γpost)  where [30]

post = ∇2
Γ−1

xηy + Γ−1
xηy  Γ−1

0   xMAP = ¯x − ΓpostAT Γ−1
0 )  with ∇2
xηy = AT Γ−1

ξ (y − A¯x).
(30)
Therefore  the eigenvalue λi of (∇2
ξ A  measures the relative variation
between the data-dependent log-likelihood and the prior in direction ψi. For λi (cid:28) 1  the data provides
negligible information in direction ψi  so the difference between the posterior and the prior in ψi is
negligible. In fact  it is shown in [29] that the subspace constructed by (29) is optimal for linear f.
Let (λi  ψi)1≤i≤r denote the r largest eigenpairs such that |λ1| ≥ |λ2| ≥ ··· ≥ |λr| ≥ ελ > |λr+1|
for some small tolerance ελ < 1. Then the Hessian-based subspace spanned by the eigenvectors
Ψ = (ψ1  . . .   ψr) captures the most variation of the parameter x informed by data y. We remark that
to solve the generalized Hermitian eigenvalue problem (29)  we employ a randomized SVD algorithm
[17]  which requires O(N rCh + dr2) ﬂops  where Ch is the cost of a Hessian action in a direction.

5

3.4 Parallel and adaptive pSVN algorithm

Given the bases Ψ as the data-informed parameter directions  we can draw samples x1  . . .   xN from
the prior distribution and push them by pSVN to match the posterior distribution in a low-dimensional
subspace  while keeping the components of the samples in the complementary subspace unchanged.
We set the stopping criterion as: (i) the maximum norm of the updates wl
m   m = 1  . . .   N 
is smaller than a given tolerance Tolg; (ii) the maximum norm of the gradients gm  m = 1  . . .   N 
is smaller than a given tolerance Tolw; or (iii) the number of iterations l reaches a preset number
L. Moreover  we take advantage of pSVN advantages in low-dimensional subspaces—including
fast computation  lightweight communication  and low memory footprint—and provide an efﬁcient
parallel implementation using MPI communication in Algorithm 1  with analysis in Appendix C.

m − wl−1

Algorithm 1 pSVN in parallel using MPI
1: Input: M prior samples  x1  . . .   xM   in each of K cores  bases Ψ  and density py in all cores.
2: Output: posterior samples xy
1  . . .   xy
3: Perform projection (13) to get xm = xr
4: Perform MPI_Allgather for wl−1
5: repeat
6:
7:
8:
9:

Compute the gradient and Hessian by (25).
Perform MPI_Allgather for the gradient and Hessian.
Compute the kernel and its gradient by (12) and (26).
Perform MPI_Allgather for km  m = 1  . . .   M 
m ∇wkm.

MPI_Allreduce w. sum for(cid:80)

m km and(cid:80)

m and the samples wl−1

M in each core.
m + x⊥

m   m = 1  . . .   M  at l = 1.

m   m = 1  . . .   M.

Assemble and solve system (27) for c1  . . .   cM .
Perform a line search to get wl
Perform MPI_Allgather for wl
Update the samples xr
m = Ψwl
Set l ← l + 1.

10:
11:
12:
13:
14:
15: until A stopping criterion is met.
16: Reconstruct samples xy

1  . . .   wl
m  m = 1  . . .   M.
m + ¯x  m = 1  . . .   M.

m  m = 1  . . .   M.

m + x⊥

m = xr

M .

In Algorithm 1  we assume that the bases Ψ for the projection are the data informed parameter
directions  which are obtained by the Hessian-based algorithm in Section 3.3 at the “representative”
samples x1  . . .   xN . However  we do not have these samples but only the prior samples at the
beginning. To address this problem  we propose an adaptive algorithm that adaptively construct the
bases Ψ based on samples pushed forward from the prior to the posterior  see Algorithm 2.

1  . . .   xy

Algorithm 2 Adaptive pSVN
1: Input: M prior samples  x1  . . .   xM   in each of K cores  and density py in all cores.
2: Output: posterior samples xy
3: Set level l2 = 1  xl2−1
4: repeat
5:
6:

Perform the eigendecomposition (29) at samples xl2−1
Apply Algorithm 1 to update the samples
  . . .   xl2−1
1   . . .   xl2
[xl2
Set l2 ← l2 + 1.

M ] = pSVN([xl2−1
7:
8: until A stopping criterion is met.

m = xm  m = 1  . . .   M.

M ]  K  Ψl2  py).

M in each core.

  . . .   xl2−1

1

1

M   and form the bases Ψl2.

4 Numerical experiments

We demonstrate the convergence  accuracy  and dimension-independence of the pSVN method by
two examples  one a linear problem with Gaussian posterior to demonstrate the convergence and
accuracy of pSVN in comparison with SVN and SVGD  the other a nonlinear problem to demonstrate
accuracy as well as the dimension-independent and sample-independent convergence of pSVN and
its scalability w.r.t. the number of processor cores. The code is described in Appendix D.

6

4.1 A linear inference problem

For the linear inference problem  we have the parameter-to-observable map

f (x) = Ax 

(31)
where the linear map A = O(Bx)  with an observation map O : Rd → Rs  and an inverse discrete
differential operator B = (L + M )−1 : Rd → Rd where L and M are the discrete Laplacian and
mass matrices in the PDE model −(cid:52)u + u = x  in (0  1)  u(0) = 0  u(1) = 1. s = 15 pointwise
observations of u with 1% noise are distributed with equal distance in (0  1). The input x is a random
ﬁeld with Gaussian prior N (0  Γ0)  where Γ0 is discretized from (I − 0.1(cid:52))−1 with identity I and
Laplace operator (cid:52). We discretize this forward model by a ﬁnite element method with piecewise
linear elements on a uniform mesh of size 2n  which leads to the parameter dimension d = 2n + 1.

Figure 1: Decay of the RMSE (with 10 trials in dashed lines) of the L2-norm of the mean (left) and
pointwise variance (middle) of the parameter w.r.t. dimension d = 16  64  256  1024 with N = 128
samples. Right: Decay of the RMSE of the L2-norm of the pointwise variance with N = 32  512
samples in parameter dimension d = 256 w.r.t. # iterations. Comparison for SVGD  SVN  pSVN.
Figure 1 compares the convergence and accuracy of SVGD  SVN  and pSVN by the decay of the
root mean square errors (RMSE) (using 10 trials and 10 iterations) of the sample mean and variance
(with L2-norm of errors computed against analytic values in (30)) w.r.t. parameter dimensions and
iterations. We observe much faster convergence and greater accuracy of pSVN relative to SVGD and
SVN  for both mean and especially variance  which measures the goodness of samples. In particular 
we see from the middle ﬁgure that the SVN estimate of variance deteriorates quickly with increasing
dimension  while pSVN leads to equally good variance estimate. Moreover  from the right ﬁgure we
can see that pSVN converges very rapidly in a subspace of dimension 6 (at tolerance ελ = 0.01 in
Section 3.3  i.e.  |λ7| < 0.01) and achieves higher accuracy with larger number of samples  while
SVN converges slowly and leads to large errors. With the same number of iterations of SVN and
pSVN  SVGD produces no evident error decay.

4.2 A nonlinear inference problem

We consider a nonlinear benchmark inference problem (which is often used for testing high-
dimensional inference methods [30  12  3])  whose forward map is given by f (x) = O(S(x)) 
with observation map O : Rd → Rs and a nonlinear solution map u = S(x) ∈ Rd of the lognormal
diffusion model −∇ · (ex∇u) = 0  in (0  1)2 with u = 1 on top and u = 0 on bottom boundaries 
and zero Neumann conditions on left and right boundaries. 49 pointwise observations of u are equally
distributed in (0  1)2. We use 10% noise to test accuracy against a DILI MCMC method [12] with
10 000 MCMC samples as reference and a challenging 1% noise for a dimension-independence test
of pSVN. The input x is a random ﬁeld with Gaussian prior N (0  Γ0)  where Γ0 is a discretization
of (I − 0.1(cid:52))−2. We solve this forward model by a ﬁnite element method with piecewise linear
elements on a uniform mesh of varying sizes  which leads to a sequence of parameter dimensions.
Figure 2 shows the comparison of the accuracy and convergence of pSVN and SVN for their sample
estimate of mean and variance. We can see that in high dimension  d = 1089  pSVN converges
faster and achieves higher accuracy than SVN for both mean and variance estimate. Moreover  SVN
using the kernel (12) in high dimensions (involving low-rank decomposition of the metric M for
high-dimensional nonlinear problems) is more expensive than pSVN per iteration.
We next demonstrate pSVN’s independence of the number of parameter and sample dimensions  and
its scalability w.r.t. processor cores. First  the dimension of the Hessian-based subspace r  which
determines the computational cost of pSVN  depends on the decay of the absolute eigenvalues |λi|

7

45678910Log2(Dimension)1.501.251.000.750.500.250.000.25Log10(RMSE of mean)SVGDSVNpSVN45678910Log2(Dimension)0.80.60.40.20.00.2Log10(RMSE of variance)SVGDSVNpSVN02468# iterations1.51.00.50.0Log10(RMSE of variance)SVGD N=32SVN N=32pSVN N=32SVGD N=512SVN N=512pSVN N=512Figure 2: Decay of the RMSE (with 10 trials in dashed lines) of the L2-norm of the mean (left) and
pointwise variance (right) of the parameter with dimension d = 1089 and N = 32  512 samples.

Figure 3: Left: Decay of eigenvalues log10(|λi|) with increasing dimension d. Middle: Decay
of a stopping criterion—the averaged norm of the update wl − wl−1 w.r.t. the iteration number l 
with increasing number of samples. Right: Decay of the wall clock time (seconds) of different
computational components w.r.t. increasing number of processor cores on a log-log scale.

as presented in Section 3.3. The left part of Figure 3 shows that with increasing d from 289 to
over 16K  r does not change  which implies that the convergence of pSVN is independent of the
number of nominal parameter dimensions. Second  as shown in the middle part of Figure 3  with
increasing number of samples for a ﬁxed parameter dimension d = 1089  the averaged norm of the
update wl − wl−1  as one convergence indicator presented in Subsection 3.4  decays similarly  which
demonstrates the independence of the convergence of pSVN w.r.t. the number of samples. Third  in
the right of Figure 3 we plot the total wall clock time of pSVN and the time for its computational
components in Algorithm 1 using different number of processor cores for the same work  i.e.  the
same number of samples (256)  including variation for forward model solve  gradient and Hessian
evaluation  as well as eigendecomposition  kernel for kernel and its gradient evaluation  solve for
solving the Newton system (27)  and sample for sample projection and reconstruction. We can
observe nearly perfect strong scaling w.r.t. increasing number of processor cores. Moreover  the time
for variation  which depends on parameter dimension d  dominates the time for all other components 
in particular kernel and solve whose cost only depends on r  not d.

5 Conclusion

We presented a fast and scalable variational method  pSVN  for Bayesian inference in high dimensions.
The method exploits the geometric structure of the posterior via its Hessian  and the intrinsic low-
dimensionality of the change from prior to posterior characteristic of many high-dimensional inference
problems via low rank approximation of the averaged Hessian of the log likelihood  computed
efﬁciently using randomized matrix-free SVD. The fast convergence and higher accuracy of pSVN
relative to SVGD and SVN  its complexity that is independent of parameter and sample dimensions 
and its scalability w.r.t. processor cores were demonstrated for linear and nonlinear inference problems.
Investigation of pSVN to tackle intrinsically high-dimensional inference problem (e.g.  performed in
local dimensions as the message passing scheme or combined with dimension-independent MCMC
to update samples in complement subspace) is ongoing. Further development and application of
pSVN to more general probability distributions  projection basis constructions  and forward models
such as deep neural network  and further analysis of the convergence and scalability of pSVN w.r.t.
the number of samples  parameter dimension reduction  and data volume  are of great interest.

8

051015# iterations1.21.00.80.60.40.20.0Log10(RMSE of mean)SVN 32pSVN 32SVN 512pSVN 512051015# iterations1.00.80.60.40.20.0Log10(RMSE of variance)SVN 32pSVN 32SVN 512pSVN 512010203040i101234Log10(|i|)# dimensions=289# dimensions=1 089# dimensions=4 225# dimensions=16 6412468# iterations2.01.51.00.50.0Log10(step norm)# samples=8# samples=32# samples=128# samples=512012345Log2(# processor cores)20246810Log2(time (s))totalvariationkernelsolvesampleO(N1)References
[1] Guillaume Alain  Nicolas Le Roux  and Pierre-Antoine Manzagol. Negative eigenvalues of the

Hessian in deep neural networks. arXiv preprint arXiv:1902.02366  2019.

[2] O. Bashir  K. Willcox  O. Ghattas  B. van Bloemen Waanders  and J. Hill. Hessian-based
model reduction for large-scale systems with initial condition inputs. International Journal for
Numerical Methods in Engineering  73:844–868  2008.

[3] Alexandros Beskos  Mark Girolami  Shiwei Lan  Patrick E. Farrell  and Andrew M. Stuart.
Geometric mcmc for inﬁnite-dimensional inverse problems. Journal of Computational Physics 
335:327 – 351  2017.

[4] David M Blei  Alp Kucukelbir  and Jon D McAuliffe. Variational inference: A review for

statisticians. Journal of the American Statistical Association  112(518):859–877  2017.

[5] T. Bui-Thanh and O. Ghattas. Analysis of the Hessian for inverse scattering problems: I. Inverse

shape scattering of acoustic waves. Inverse Problems  28(5):055001  2012.

[6] T. Bui-Thanh  O. Ghattas  J. Martin  and G. Stadler. A computational framework for inﬁnite-
dimensional bayesian inverse problems part I: The linearized case  with application to global
seismic inversion. SIAM Journal on Scientiﬁc Computing  35(6):A2494–A2523  2013.

[7] Peng Chen and Omar Ghattas. Hessian-based sampling for high-dimensional model reduction.

arXiv preprint arXiv:1809.10255  2018.

[8] Peng Chen and Christoph Schwab. Sparse-grid  reduced-basis Bayesian inversion. Computer

Methods in Applied Mechanics and Engineering  297:84 – 115  2015.

[9] Peng Chen  Umberto Villa  and Omar Ghattas. Hessian-based adaptive sparse quadrature for
inﬁnite-dimensional Bayesian inverse problems. Computer Methods in Applied Mechanics and
Engineering  327:147–172  2017.

[10] Peng Chen  Umberto Villa  and Omar Ghattas. Taylor approximation and variance reduction
for PDE-constrained optimal control problems under uncertainty. Journal of Computational
Physics  385:163–186  2019.

[11] Wilson Ye Chen  Lester Mackey  Jackson Gorham  François-Xavier Briol  and Chris J Oates.

Stein points. arXiv preprint arXiv:1803.10161  2018.

[12] Tiangang Cui  Kody JH Law  and Youssef M Marzouk. Dimension-independent likelihood-

informed MCMC. Journal of Computational Physics  304:109–137  2016.

[13] Tiangang Cui  Youssef Marzouk  and Karen Willcox. Scalable posterior approximations for
large-scale bayesian inverse problems via likelihood-informed parameter and state reduction.
Journal of Computational Physics  315:363–387  2016.

[14] Gianluca Detommaso  Tiangang Cui  Youssef Marzouk  Alessio Spantini  and Robert Scheichl.
A stein variational Newton method. In Advances in Neural Information Processing Systems 
pages 9187–9197  2018.

[15] Behrooz Ghorbani  Shankar Krishnan  and Ying Xiao. An Investigation into Neural Net

Optimization via Hessian Eigenvalue Density. arXiv preprint arXiv:1901.10159  2019.

[16] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
73(2):123–214  2011.

[17] Nathan Halko  Per-Gunnar Martinsson  and Joel A Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review 
53(2):217–288  2011.

[18] Tobin Isaac  Noemi Petra  Georg Stadler  and Omar Ghattas. Scalable and efﬁcient algorithms
for the propagation of uncertainty from data through inference to prediction for large-scale
problems  with application to ﬂow of the Antarctic ice sheet. Journal of Computational Physics 
296:348–368  September 2015.

9

[19] Chang Liu and Jun Zhu. Riemannian Stein variational gradient descent for Bayesian inference.

In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[20] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian
inference algorithm. In Advances In Neural Information Processing Systems  pages 2378–2386 
2016.

[21] J. Martin  L.C. Wilcox  C. Burstedde  and O. Ghattas. A stochastic Newton MCMC method for
large-scale statistical inverse problems with application to seismic inversion. SIAM Journal on
Scientiﬁc Computing  34(3):A1460–A1487  2012.

[22] Youssef Marzouk  Tarek Moselhy  Matthew Parno  and Alessio Spantini. Sampling via measure
transport: An introduction. In Handbook of Uncertainty Quantiﬁcation  pages 1–41. Springer 
2016.

[23] J. Nocedal and S. Wright. Numerical optimization. Springer Science & Business Media  2006.

[24] N. Petra  J. Martin  G. Stadler  and O. Ghattas. A computational framework for inﬁnite-
dimensional Bayesian inverse problems  Part II: Stochastic Newton MCMC with application to
ice sheet ﬂow inverse problems. SIAM J. Sci. Comput.  36(4):A1525–A1555  2014.

[25] Levent Sagun  Leon Bottou  and Yann LeCun. Eigenvalues of the hessian in deep learning:

Singularity and beyond. arXiv preprint arXiv:1611.07476  2016.

[26] Claudia Schillings and Christoph Schwab. Sparse  adaptive Smolyak quadratures for Bayesian

inverse problems. Inverse Problems  29(6):065011  2013.

[27] Claudia Schillings and Christoph Schwab. Scaling limits in computational Bayesian inversion.

ESAIM: Mathematical Modelling and Numerical Analysis  50(6):1825–1856  2016.

[28] Ch. Schwab and A.M. Stuart. Sparse deterministic approximation of Bayesian inverse problems.

Inverse Problems  28(4):045003  2012.

[29] A. Spantini  A. Solonen  T. Cui  J. Martin  L. Tenorio  and Y. Marzouk. Optimal low-rank
approximations of Bayesian linear inverse problems. SIAM Journal on Scientiﬁc Computing 
37(6):A2451–A2487  2015.

[30] A.M. Stuart. Inverse problems: a Bayesian perspective. Acta Numerica  19(1):451–559  2010.

[31] Dilin Wang  Zhe Zeng  and Qiang Liu. Stein variational message passing for continuous
graphical models. In International Conference on Machine Learning  pages 5206–5214  2018.

[32] Jingwei Zhuo  Chang Liu  Jiaxin Shi  Jun Zhu  Ning Chen  and Bo Zhang. Message passing

Stein variational gradient descent. arXiv preprint arXiv:1711.04425  2017.

10

,Peng Chen
Keyi Wu
Joshua Chen
Tom O'Leary-Roseberry
Omar Ghattas