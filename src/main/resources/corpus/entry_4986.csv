2018,Sequential Attend  Infer  Repeat: Generative Modelling of Moving Objects,We present Sequential Attend  Infer  Repeat (SQAIR)  an interpretable deep generative model for image sequences.
It can reliably discover and track objects through the sequence; it can also conditionally generate future frames  thereby simulating expected motion of objects. 
This is achieved by explicitly encoding object numbers  locations and appearances in the latent variables of the model.
SQAIR retains all strengths of its predecessor  Attend  Infer  Repeat (AIR  Eslami et. al. 2016)  including unsupervised learning  made possible by inductive biases present in the model structure.
We use a moving multi-\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects  and show how \textsc{sqair} overcomes them by leveraging temporal consistency of objects.
Finally  we also apply SQAIR to real-world pedestrian CCTV data  where it learns to reliably detect  track and generate walking pedestrians with no supervision.,Sequential Attend  Infer  Repeat: Generative

Modelling of Moving Objects

Adam R. Kosiorek∗ § †

Hyunjik Kim†

Ingmar Posner§

Yee Whye Teh†

§ Applied Artiﬁcial Intelligence Lab

Oxford Robotics Institute

University of Oxford

† Department of Statistics

University of Oxford

Abstract

We present Sequential Attend  Infer  Repeat (SQAIR)  an interpretable deep genera-
tive model for videos of moving objects. It can reliably discover and track objects
throughout the sequence of frames  and can also generate future frames condition-
ing on the current frame  thereby simulating expected motion of objects. This is
achieved by explicitly encoding object presence  locations and appearances in the
latent variables of the model. SQAIR retains all strengths of its predecessor  Attend 
Infer  Repeat (AIR  Eslami et al.  2016)  including learning in an unsupervised
manner  and addresses its shortcomings. We use a moving multi-MNIST dataset to
show limitations of AIR in detecting overlapping or partially occluded objects  and
show how SQAIR overcomes them by leveraging temporal consistency of objects.
Finally  we also apply SQAIR to real-world pedestrian CCTV data  where it learns
to reliably detect  track and generate walking pedestrians with no supervision.

1

Introduction

The ability to identify objects in their environments and to understand relations between them is a
cornerstone of human intelligence (Kemp and Tenenbaum  2008). Arguably  in doing so we rely on
a notion of spatial and temporal consistency which gives rise to an expectation that objects do not
appear out of thin air  nor do they spontaneously vanish  and that they can be described by properties
such as location  appearance and some dynamic behaviour that explains their evolution over time.
We argue that this notion of consistency can be seen as an inductive bias that improves the efﬁciency
of our learning. Equally  we posit that introducing such a bias towards spatio-temporal consistency
into our models should greatly reduce the amount of supervision required for learning.

One way of achieving such inductive biases is through model structure. While recent successes in deep
learning demonstrate that progress is possible without explicitly imbuing models with interpretable
structure (LeCun  Bengio  et al.  2015)  recent works show that introducing such structure into deep
models can indeed lead to favourable inductive biases improving performance e.g. in convolutional
networks (LeCun  Boser  et al.  1989) or in tasks requiring relational reasoning (Santoro et al. 
2017). Structure can also make neural networks useful in new contexts by signiﬁcantly improving
generalization  data efﬁciency (Jacobsen et al.  2016) or extending their capabilities to unstructured
inputs (Graves et al.  2016).

Attend  Infer  Repeat (AIR)  introduced by Eslami et al.  2016  is a notable example of such a structured
probabilistic model that relies on deep learning and admits efﬁcient amortized inference. Trained
without any supervision  AIR is able to decompose a visual scene into its constituent components and
to generate a (learned) number of latent variables that explicitly encode the location and appearance
of each object. While this approach is inspiring  its focus on modelling individual (and thereby
inherently static) scenes leads to a number of limitations. For example  it often merges two objects
that are close together into one since no temporal context is available to distinguish between them.

∗Corresponding author: adamk@robots.ox.ac.uk

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Similarly  we demonstrate that AIR struggles to identify partially occluded objects  e.g. when they
extend beyond the boundaries of the scene frame (see Figure 7 in Section 4.1).

Our contribution is to mitigate the shortcomings of AIR by introducing a sequential version that
models sequences of frames  enabling it to discover and track objects over time as well as to
generate convincing extrapolations of frames into the future. We achieve this by leveraging temporal
information to learn a richer  more capable generative model. Speciﬁcally  we extend AIR into a
spatio-temporal state-space model and train it on unlabelled image sequences of dynamic objects.
We show that the resulting model  which we name Sequential AIR (SQAIR)  retains the strengths of
the original AIR formulation while outperforming it on moving MNIST digits.

The rest of this work is organised as follows. In Section 2  we describe the generative model and
inference of AIR. In Section 3  we discuss its limitations and how it can be improved  thereby
introducing Sequential Attend  Infer  Repeat (SQAIR)  our extension of AIR to image sequences. In
Section 4  we demonstrate the model on a dataset of multiple moving MNIST digits (Section 4.1)
and compare it against AIR trained on each frame and Variational Recurrent Neural Network (VRNN)
of Chung et al.  2015 with convolutional architectures  and show the superior performance of SQAIR
in terms of log marginal likelihood and interpretability of latent variables. We also investigate the
utility of inferred latent variables of SQAIR in downstream tasks. In Section 4.2 we apply SQAIR on
real-world pedestrian CCTV data  where SQAIR learns to reliably detect  track and generate walking
pedestrians without any supervision. Code for the implementation on the MNIST dataset2 and the
results video3 are available online.

2 Attend  Infer  Repeat (AIR)

AIR  introduced by Eslami et al.  2016  is a structured variational auto-encoder (VAE) capable of
decomposing a static scene x into its constituent objects  where each object is represented as a
separate triplet of continuous latent variables z = {zwhat i  zwhere i  zpres i}n
i=1  n ∈ N being the
(random) number of objects in the scene. Each triplet of latent variables explicitly encodes position 
appearance and presence of the respective object  and the model is able to infer the number of objects
present in the scene. Hence it is able to count  locate and describe objects in the scene  all learnt in an
unsupervised manner  made possible by the inductive bias introduced by the model structure.

Generative Model The generative model of AIR is deﬁned as follows

n

pθ(n) = Geom(n | θ) 

pθ(zw | n) =

pθ(cid:0)zw i(cid:1) =

n

Yi=1

N(cid:0)zw i|0  I(cid:1) 

n

Yi=1
Xi=1

with yt =

hdec
θ

(zwhat i  zwhere i) 

(1)

pθ(x | z) = N(cid:0)x | yt  σ2

x

I(cid:1) 

where zw i ..= (zwhat i  zwhere i)  zpres i = 1 for i = 1 . . . n and hdec
is the object decoder with
parameters θ. It is composed of a glimpse decoder f dec
t  which constructs an image patch
and a spatial transformer (ST  Jaderberg et al.  2015)  which scales and shifts it according to zwhere;
see Figure 1 for details.

t 7→ yi

: gi

θ

θ

Inference Eslami et al.  2016 use a sequential inference algorithm  where latent variables are inferred
one at a time; see Figure 2. The number of inference steps n is given by zpres 1:n+1  a random vector
of n ones followed by a zero. The zi are sampled sequentially from

n

qφ(z | x) = qφ(cid:0)zpres n+1 = 0 | zw 1:n  x(cid:1)

Yi=1

qφ(cid:0)zw i  zpres i = 1 | z1:i−1  x(cid:1) 

(2)

where qφ is implemented as a neural network with parameters φ. To implement explain-
ing away  e.g. to avoid encoding the same object twice 
it is vital to capture the depen-
dency of zw i and zpres i on z1:i−1 and x. This is done using a recurrent neural network
(RNN) Rφ with hidden state hi  namely: ωi  hi = Rφ(x  zi−1  hi−1). The outputs ωi 
which are computed iteratively and depend on the previous latent variables (cf. Algorithm 3) 

parametrise qφ(cid:0)zw i  zpres i | z1:i−1  x(cid:1). For simplicity the latter is assumed to factorise such that
qφ(cid:0)zw  zpres | z1:i−1  x(cid:1) = qφ(cid:0)zpres n+1 = 0 | ωn+1(cid:1)Qn
i=1 qφ(cid:0)zw i | ωi(cid:1)qφ(cid:0)zpres i = 1 | ωi(cid:1).

2code: github.com/akosiorek/sqair
3video: youtu.be/-IUNQgSLE0c

2

Figure 1: Left: Generation in AIR. The image mean yt is generated by ﬁrst using the glimpse decoder f dec
to
map the what variables into glimpses gt  transforming them with the spatial transformer ST according to the
where variables and summing up the results. Right: Generation in SQAIR. When new objects enter the frame 
4
new latent variables (here  z
t ) are sampled from the discovery prior. The temporal evolution of already present
4
objects is governed by the propagation prior  which can choose to forget some variables (here  z
t+1)
when the object moves out of the frame. The image generation process  which mimics the left-hand side of the
ﬁgure  is abstracted in the decoder block.

3
t and z

θ

i
t at a time. Here  it decides that two latent variables are enough to explain the image and z

Figure 2: Left: Inference in AIR. The pink RNN attends to the image sequentially and produces one latent variable
3
t is not generated.
z
Right: Inference in SQAIR starts with the Propagation (PROP) phase. PROP iterates over latent variables from
the previous time-step t − 1 and updates them based on the new observation xt. The blue RNN runs forward in
time to update the hidden state of each object  to model its change in appearance and location throughout time.
The orange RNN runs across all current objects and models the relations between different objects. Here  when
1
attending to z
t−1  it decides that the corresponding object has disappeared from the frame and forgets it. Next 
the Discovery (DISC) phase detects new objects as in AIR  but in SQAIR it is also conditioned on the results of
PROP  to prevent rediscovering objects. See Figure 3 for details of the colored RNNS.

3 Sequential Attend-Infer-Repeat

While capable of decomposing a scene into objects  AIR only describes single images. Should we
want a similar decomposition of an image sequence  it would be desirable to do so in a temporally
consistent manner. For example  we might want to detect objects of the scene as well as infer
dynamics and track identities of any persistent objects. Thus  we introduce Sequential Attend  Infer 
Repeat (SQAIR)  whereby AIR is augmented with a state-space model (SSM) to achieve temporal
consistency in the generated images of the sequence. The resulting probabilistic model is composed
of two parts: Discovery (DISC)  which is responsible for detecting (or introducing  in the case of the
generation) new objects at every time-step (essentially equivalent to AIR)  and Propagation (PROP) 
responsible for updating (or forgetting) latent variables from the previous time-step given the new
observation (image)  effectively implementing the temporal SSM. We now formally introduce SQAIR
by ﬁrst describing its generative model and then the inference network.

Generative Model The model assumes that at every-time step  objects are ﬁrst propagated from the
previous time-step (PROP). Then  new objects are introduced (DISC). Let t ∈ N be the current time-
step. Let Pt be the set of objects propagated from the previous time-step and let Dt be the set of objects
discovered at the current time-step  and let Ot = Pt ∪Dt be the set of all objects present at time-step t.
Consequently  at every time step  the model retains a set of latent variables zPt
t}i∈Pt   and

t = {zi

3

generates a set of new latent variables zDt
the representation of the ith object zi
t
(as in AIR): zwhat i
and zwhere i
of the object  respectively. zpres i
given time-step or not.

t

t

t

t = {zi
..= [zwhat i

t}i∈Dt . Together they form zt
  zwhere i

]  where
] is composed of three components
are real vector-valued variables representing appearance and location
is a binary variable representing whether the object is present at the

  zpres i

..= [zPt
t

  zDt

t

t

t

t

t

t

  zwhere i

At the ﬁrst time-step (t = 1) there are no objects to propagate  so we sample D1  the number of
objects at t = 1  from the discovery prior pD(D1). Then for each object i ∈ Dt  we sample latent
variables zwhat i
objects from t = 1 are propagated to t = 2  and which objects disappear from the frame  using the
binary random variable (zpres i
)i∈Pt . The discovery step at t = 2 models new objects that enter
the frame  with a similar procedure to t = 1: sample D2 (which depends on zP2
2 ) then sample
(zwhat i
)i∈D2 . This procedure of propagation and discovery recurs for t = 2  . . . T . Once
the zt have been formed  we may generate images xt using the exact same generative distribution
pθ(xt | zt) as in AIR (cf. Equation (1)  Fig. 1  and Algorithm 1). In full  the generative model is:

1 | D1(cid:1). At time t = 2  the propagation step models which

from pD(cid:0)zi

  zwhere i

2

2

t

T

p(x1:T   z1:T   D1:T ) = pD(D1  zD1
1 )

pD(Dt  zDt
t

|zPt

t )pP (zPt

t

|zt−1)pθ(xt|zt) 

(3)

Yt=2

The discovery prior pD(Dt  zDt
|zPt
t ) samples latent variables for new objects that enter the frame.
t
The propagation prior pP (zPt
|zt−1) samples latent variables for objects that persist in the frame
t
and removes latents of objects that disappear from the frame  thereby modelling dynamics and
appearance changes. Both priors are learned during training. The exact forms of the priors are given
in Appendix B.

Inference Similarly to AIR  inference in SQAIR can capture the number of objects and the representa-
tion describing the location and appearance of each object that is necessary to explain every image in
a sequence. As with generation  inference is divided into PROP and DISC. During PROP  the inference
network achieves two tasks. Firstly  the latent variables from the previous time step are used to infer
the current ones  modelling the change in location and appearance of the corresponding objects 
thereby attaining temporal consistency. This is implemented by the temporal RNN RT
φ   with hidden
states hT
t (recurs in t). Crucially  it does not access the current image directly  but uses the output of
the relation RNN (cf. Santoro et al.  2017). The relation RNN takes relations between objects into
account  thereby implementing the explaining away phenomenon; it is essential for capturing any
interactions between objects as well as occlusion (or overlap  if one object is occluded by another).
See Figure 7 for an example. These two RNNs together decide whether to retain or to forget objects
that have been propagated from the previous time step. During DISC  the network infers further latent
variables that are needed to describe any new objects that have entered the frame. All latent variables
remaining after PROP and DISC are passed on to the next time step.

See Figures 2 and 3 for the inference network structure . The full variational posterior is deﬁned as

qφ(D1:t  z1:T | x1:T ) =

T

Yt=1

qD

φ(cid:16)Dt  zDt

t

| xt  zPt

t (cid:17) Yi∈Ot−1

t | zi

t−1  hT  i

t

qP

φ(cid:16)zi

  hR i

t (cid:17).

(4)

Discovery  described by qD
φ   is very similar to the full posterior of AIR  cf. Equation (2). The only
difference is the conditioning on zPt
  which allows for a different number of discovered objects at
t
each time-step and also for objects explained by PROP not to be explained again. The second term  or
qP
φ   describes propagation. The detailed structures of qD
φ are shown in Figure 3  while all the
pertinent algorithms and equations can be found in Appendices A and C  respectively.

φ and qP

Learning We train SQAIR as an importance-weighted auto-encoder (IWAE) of Burda et al.  2016.
Speciﬁcally  we maximise the importance-weighted evidence lower-bound LIWAE  namely

LIWAE = E

x1:T ∼pdata(x1:T )"Eq"log

1
K

pθ(x1:T   z1:T )

qφ(z1:T | x1:T )##.

(5)

K

Xk=1

To optimise the above  we use RMSPROP  K = 5 and batch size of 32. We use the VIMCO gradient
estimator of Mnih and Rezende  2016 to backpropagate through the discrete latent variables zpres 

4

Figure 3: Left: Interaction between PROP and DISC in
SQAIR. Firstly  objects are propagated to time t  and
object i = 7 is dropped. Secondly  DISC tries to dis-
cover new objects. Here  it manages to ﬁnd two objects:
i = 9 and i = 10. The process recurs for all remaining
time-steps. Blue arrows update the temporal hidden
state  orange ones infer relations between objects  pink
ones correspond to discovery. Bottom: Information
ﬂow in a single discovery block (left) and propagation
block (right). In DISC we ﬁrst predict where and extract
a glimpse. We then predict what and presence. PROP
starts with extracting a glimpse at a candidate location
and updating where. Then it follows a procedure similar
to DISC  but takes the respective latent variables from
the previous time-step into account. It is approximately
two times more computationally expensive than DISC.
For details  see Algorithms 2 and 3 in Appendix A.

and use reparameterisation for the continuous ones (Kingma and Welling  2013). We also tried to
use NVIL of Mnih and Gregor  2014 as in the original work on AIR  but found it very sensitive to
hyper-parameters  fragile and generally under-performing.

4 Experiments

We evaluate SQAIR on two datasets. Firstly  we perform an extensive evaluation on moving MNIST
digits  where we show that it can learn to reliably detect  track and generate moving digits (Section 4.1).
Moreover  we show that SQAIR can simulate moving objects into the future — an outcome it has not
been trained for. We also study the utility of learned representations for a downstream task. Secondly 
we apply SQAIR to real-world pedestrian CCTV data from static cameras (DukeMTMC  Ristani et al. 
2016)  where we perform background subtraction as pre-processing. In this experiment  we show that
SQAIR learns to detect  track  predict and generate walking pedestrians without human supervision.

4.1 Moving multi-MNIST

The dataset consists of sequences of length 10 of multiple moving MNIST digits. All images are of
size 50 × 50 and there are zero  one or two digits in every frame (with equal probability). Sequences
are generated such that no objects overlap in the ﬁrst frame  and all objects are present through
the sequence; the digits can move out of the frame  but always come back. See Appendix F for an
experiment on a harder version of this dataset. There are 60 000 training and 10 000 testing sequences
created from the respective MNIST datasets. We train two variants of SQAIR: the MLP-SQAIR uses

5

Figure 4: Input images (top) and SQAIR reconstructions with marked glimpse locations (bottom). For more
examples  see Figure 13 in Appendix H.

Figure 5: Samples from SQAIR. Both motion and appearance are consistent through time  thanks to the
propagation part of the model. For more examples  see Figure 15 in Appendix H.

only fully-connected networks  while the CONV-SQAIR replaces the networks used to encode images
and glimpses with convolutional ones; it also uses a subpixel-convolution network as the glimpse
decoder (Shi et al.  2016). See Appendix D for details of the model architectures and the training
procedure.

We use AIR and VRNN (Chung et al.  2015) as baselines for comparison. VRNN can be thought of
as a sequential VAE with an RNN as its deterministic backbone. Being similar to a VAE  its latent
variables are not structured  nor easily interpretable. For a fair comparison  we control the latent
dimensionality of VRNN and the number of learnable parameters. We provide implementation details
in Appendix D.3.

The quantitative analysis consists of comparing all models in terms of the marginal log-likelihood
log pθ(x1:T ) evaluated as the LIWAE bound with K = 1000 particles  reconstruction quality evaluated
as a single-sample approximation of Eqφ [log pθ(x1:T | z1:T )] and the KL-divergence between the
approximate posterior and the prior (Table 1). Additionally  we measure the accuracy of the number
of objects modelled by SQAIR and AIR. SQAIR achieves superior performance across a range of
metrics — its convolutional variant outperforms both AIR and the corresponding VRNN in terms of
model evidence and reconstruction performance. The KL divergence for SQAIR is almost twice as
low as for VRNN and by a yet larger factor for AIR. We can interpret KL values as an indicator of the
ability to compress  and we can treat SQAIR/AIR type of scheme as a version of run-length encoding.
While VRNN has to use information to explicitly describe every part of the image  even if some parts
are empty  SQAIR can explicitly allocate content information (zwhat) to speciﬁc parts of the image
(indicated by zwhere). AIR exhibits the highest values of KL  but this is due to encoding every frame
of the sequence independently — its prior cannot take what and where at the previous time-step into
account  hence higher KL. The ﬁfth column of Table 1 details the object counting accuracy  that is
indicative of the quality of the approximate posterior. It is measured as the sum of zpres
for a given
frame against the true number of objects in that frame. As there is no zpres for VRNN no score is
provided. Perhaps surprisingly  this metric is much higher for SQAIR than for AIR. This is because
AIR mistakenly infers overlapping objects as a single object. Since SQAIR can incorporate temporal

t

Figure 6: The ﬁrst three frames are input to SQAIR  which generated the rest conditional on the ﬁrst frames.

6

Figure 7: Inputs  reconstructions with
marked glimpse locations and recon-
structed glimpses for AIR (left) and
SQAIR (right). SQAIR can model par-
tially visible and heavily overlapping
objects by aggregating temporal infor-
mation.

CONV-SQAIR
MLP-SQAIR

MLP-AIR

CONV-VRNN
MLP-VRNN

log pθ(x1:T )

6784.8
6617.6
6443.6
6561.9
5959.3

log pθ(x1:T | z1:T ) KL(qφ || pθ) Counting Addition
0.9990
0.9998
0.8644
0.8536
0.8059

6923.8
6786.5
6830.6
6737.8
6108.7

134.6
164.5
352.6
270.2
218.3

0.9974
0.9986
0.9058

n/a
n/a

Table 1: SQAIR achieves higher performance than the baselines across a range of metrics. The third column
refers to the Kullback-Leibler (KL) divergence between the approximate posterior and the prior. Counting refers
to accuracy of the inferred number of objects present in the scene  while addition stands for the accuracy of a
supervised digit addition experiment  where a classiﬁer is trained on the learned latent representations of each
frame.

information  it does not exhibit this failure mode (cf. Figure 7). Next  we gauge the utility of the
learnt representations by using them to determine the sum of the digits present in the image (Table 1 
column six). To do so  we train a 19-way classiﬁer (mapping from any combination of up to two digits
in the range [0  9] to their sum) on the extracted representations and use the summed labels of digits
present in the frame as the target. Appendix D contains details of the experiment. SQAIR signiﬁcantly
outperforms AIR and both variants of VRNN on this tasks. VRNN under-performs due to the inability
of disentangling overlapping objects  while both VRNN and AIR suffer from low temporal consistency
of learned representations  see Appendix H. Finally  we evaluate SQAIR qualitatively by analyzing
reconstructions and samples produced by the model against reconstructions and samples from VRNN.
We observe that samples and reconstructions from SQAIR are of better quality and  unlike VRNN 
preserve motion and appearance consistently through time. See Appendix H for direct comparison
and additional examples. Furthermore  we examine conditional generation  where we look at samples
from the generative model of SQAIR conditioned on three images from a real sequence (see Figure 6).
We see that the model can preserve appearance over time  and that the simulated objects follow
similar trajectories  which hints at good learning of the motion model (see Appendix H for more
examples). Figure 7 shows reconstructions and corresponding glimpses of AIR and SQAIR. Unlike
SQAIR  AIR is unable to recognize objects from partial observations  nor can it distinguish strongly
overlapping objects (it treats them as a single object; columns ﬁve and six in the ﬁgure). We analyze
failure cases of SQAIR in Appendix G.

4.2 Generative Modelling of Walking Pedestrians

To evaluate the model in a more challenging  real-world setting  we turn to data from static CCTV
cameras of the DukeMTMC dataset (Ristani et al.  2016). As part of pre-precessing  we use standard
background subtraction algorithms (Itseez  2015). In this experiment  we use 3150 training and 350
validation sequences of length 5. For details of model architectures  training and data pre-processing 
see Appendix E. We evaluate the model qualitatively by examining reconstructions  conditional
samples (conditioned on the ﬁrst four frames) and samples from the prior (Figure 8 and Appendix I).
We see that the model learns to reliably detect and track walking pedestrians  even when they are
close to each other.

There are some spurious detections and re-detections of the same objects  which is mostly caused by
imperfections of the background subtraction pipeline — backgrounds are often noisy and there are
sudden appearance changes when a part of a person is treated as background in the pre-processing
pipeline. The object counting accuracy in this experiment is 0.5712 on the validation dataset  and we
noticed that it does increase with the size of the training set. We also had to use early stopping to
prevent overﬁtting  and the model was trained for only 315k iterations (> 1M for MNIST experiments).
Hence  we conjecture that accuracy and marginal likelihood can be further improved by using a
bigger dataset.

7

Figure 8: Inputs on the top  reconstructions in the second row  samples in the third row; rows four and ﬁve
contain inputs and conditional generation: the ﬁrst four frames in the last row are reconstructions  while the
remaining ones are predicted by sampling from the prior. There is no ground-truth  since we used sequences of
length ﬁve of training and validation.

5 Related Work

Object Tracking There have been many approaches to modelling objects in images and videos.
Object detection and tracking are typically learned in a supervised manner  where object bounding
boxes and often additional labels are part of the training data. Single-object tracking commonly use
Siamese networks  which can be seen as an RNN unrolled over two time-steps (Valmadre et al.  2017).
Recently  Kosiorek et al.  2017 used an RNN with an attention mechanism in the HART model to
predict bounding boxes for single objects  while robustly modelling their motion and appearance.
Multi-object tracking is typically attained by detecting objects and performing data association on
bounding-boxes (Bewley et al.  2016). Schulter et al.  2017 used an end-to-end supervised approach
that detects objects and performs data association. In the unsupervised setting  where the training
data consists of only images or videos  the dominant approach is to distill the inductive bias of
spatial consistency into a discriminative model. Cho et al.  2015 detect single objects and their
parts in images  and Kwak et al.  2015; Xiao and Jae Lee  2016 incorporate temporal consistency
to better track single objects. SQAIR is unsupervised and hence it does not rely on bounding boxes
nor additional labels for training  while being able to learn arbitrary motion and appearance models
similarly to HART (Kosiorek et al.  2017). At the same time  is inherently multi-object and performs
data association implicitly (cf. Appendix A). Unlike the other unsupervised approaches  temporal
consistency is baked into the model structure of SQAIR and further enforced by lower KL divergence
when an object is tracked.

Video Prediction Many works on video prediction learn a deterministic model conditioned on the
current frame to predict the future ones (Ranzato et al.  2014; Srivastava et al.  2015). Since these
models do not model uncertainty in the prediction  they can suffer from the multiple futures problem
— since perfect prediction is impossible  the model produces blurry predictions which are a mean of
possible outcomes. This is addressed in stochastic latent variable models trained using variational
inference to generate multiple plausible videos given a sequence of images (Babaeizadeh et al.  2017;
Denton and Fergus  2018). Unlike SQAIR  these approaches do not model objects or their positions
explicitly  thus the representations they learn are of limited interpretability.

Learning Decomposed Representations of Images and Videos Learning decomposed representa-
tions of object appearance and position lies at the heart of our model. This problem can be also
seen as perceptual grouping  which involves modelling pixels as spatial mixtures of entities. Greff 
Rasmus  et al.  2016 and Greff  Steenkiste  et al.  2017 learn to decompose images into separate
entities by iterative reﬁnement of spatial clusters using either learned updates or the Expectation
Maximization algorithm; Ilin et al.  2017 and Steenkiste et al.  2018 extend these approaches to videos 
achieving very similar results to SQAIR. Perhaps the most similar work to ours is the concurrently
developed model of Hsieh et al.  2018. The above approaches rely on iterative inference procedures 

8

but do not exhibit the object-counting behaviour of SQAIR. For this reason  their computational
complexities are proportional to the predeﬁned maximum number of objects  while SQAIR can be
more computationally efﬁcient by adapting to the number of objects currently present in an image.
Another interesting line of work is the GAN-based unsupervised video generation that decomposes
motion and content (Tulyakov et al.  2018; Denton and Birodkar  2017). These methods learn
interpretable features of content and motion  but deal only with single objects and do not explicitly
model their locations. Nonetheless  adversarial approaches to learning structured probabilistic models
of objects offer a plausible alternative direction of research.

Bayesian Nonparametric Models To the best of our knowledge  Neiswanger and Wood  2012 is the
only known approach that models pixels belonging to a variable number of objects in a video together
with their locations in the generative sense. This work uses a Bayesian nonparametric (BNP) model 
which relies on mixtures of Dirichlet processes to cluster pixels belonging to an object. However 
the choice of the model necessitates complex inference algorithms involving Gibbs sampling and
Sequential Monte Carlo  to the extent that any sensible approximation of the marginal likelihood is
infeasible. It also uses a ﬁxed likelihood function  while ours is learnable.
The object appearance-persistence-disappearance model in SQAIR is reminiscent of the Markov
Indian buffet process (MIBP) of Gael et al.  2009  another BNP model. MIBP was used as a
model for blind source separation  where multiple sources contribute toward an audio signal  and
can appear  persist  disappear and reappear independently. The prior in SQAIR is similar  but the
crucial differences are that SQAIR combines the BNP prior with ﬂexible neural network models for
the dynamics and likelihood  as well as variational learning via amortized inference. The interface
between deep learning and BNP  and graphical models in general  remains a fertile area of research.

6 Discussion

In this paper we proposed SQAIR  a probabilistic model that extends AIR to image sequences  and
thereby achieves temporally consistent reconstructions and samples. In doing so  we enhanced AIR’s
capability of disentangling overlapping objects and identifying partially observed objects.

This work continues the thread of Greff  Steenkiste  et al.  2017  Steenkiste et al.  2018 and  together
with Hsieh et al.  2018  presents unsupervised object detection & tracking with learnable likelihoods
by the means of generative modelling of objects. In particular  our work is the ﬁrst one to explicitly
model object presence  appearance and location through time. Being a generative model  SQAIR can
be used for conditional generation  where it can extrapolate sequences into the future. As such  it
would be interesting to use it in a reinforcement learning setting in conjunction with Imagination-
Augmented Agents (Weber et al.  2017) or more generally as a world model (Ha and Schmidhuber 
2018)  especially for settings with simple backgrounds  e. g.  games like Montezuma’s Revenge or
Pacman.

The framework offers various avenues of further research; SQAIR leads to interpretable representa-
tions  but the interpretability of what variables can be further enhanced by using alternative objectives
that disentangle factors of variation in the objects (Kim and Mnih  2018). Moreover  in its current
state  SQAIR can work only with simple backgrounds and static cameras. In future work  we would
like to address this shortcoming  as well as speed up the sequential inference process whose com-
plexity is linear in the number of objects. The generative model  which currently assumes additive
image composition  can be further improved by e. g.  autoregressive modelling (Oord et al.  2016).
It can lead to higher ﬁdelity of the model and improved handling of occluded objects. Finally  the
SQAIR model is very complex  and it would be useful to perform a series of ablation studies to further
investigate the roles of different components.

Acknowledgements

We would like to thank Ali Eslami for his help in implementing AIR  Alex Bewley and Martin
Engelcke for discussions and valuable insights and anonymous reviewers for their constructive
feedback. Additionally  we acknowledge that HK and YWT’s research leading to these results
has received funding from the European Research Council under the European Union’s Seventh
Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.

9

References

Babaeizadeh  M.  C. Finn  D. Erhan  R. H. Campbell  and S. Levine (2017). “Stochastic Variational

Video Prediction”. In: CoRR. arXiv: 1710.11252.

Bewley  A.  Z. Ge  L. Ott  F. T. Ramos  and B. Upcroft (2016). “Simple online and realtime tracking”.

In: ICIP.

Burda  Y.  R. Grosse  and R. Salakhutdinov (2016). “Importance Weighted Autoencoders”. In: ICLR.

arXiv: 1509.00519.

Cho  M.  S. Kwak  C. Schmid  and J. Ponce (2015). “Unsupervised object discovery and localization
in the wild: Part-based matching with bottom-up region proposals”. In: CoRR. arXiv: 1501.06170.
Chung  J.  K. Kastner  L. Dinh  K. Goel  A. Courville  and Y. Bengio (2015). “A Recurrent Latent

Variable Model for Sequential Data”. In: NIPS. arXiv: 1506.02216.

Clevert  D.-A.  T. Unterthiner  and S. Hochreiter (2015). “Fast and Accurate Deep Network Learning

by Exponential Linear Units (ELUs)”. In: CoRR. arXiv: 1511.07289.

Denton  E. and V. Birodkar (2017). “Unsupervised learning of disentangled representations from

video”. In: NIPS.

Denton  E. and R. Fergus (2018). “Stochastic Video Generation with a Learned Prior”. In: ICML.
Eslami  S. M. A.  N. Heess  T. Weber  Y. Tassa  D. Szepesvari  K. Kavukcuoglu  and G. E. Hinton
(2016). “Attend  Infer  Repeat: Fast Scene Understanding with Generative Models”. In: NIPS.
arXiv: 1603.08575.

Gael  J. V.  Y. W. Teh  and Z. Ghahramani (2009). “The Inﬁnite Factorial Hidden Markov Model”. In:

NIPS.

Graves  A.  G. Wayne  M. Reynolds  T. Harley  I. Danihelka  A. Grabska-Barwi´nska  S. G. Col-
menarejo  E. Grefenstette  T. Ramalho  J. Agapiou  A. P. Badia  K. M. Hermann  Y. Zwols  G.
Ostrovski  A. Cain  H. King  C. Summerﬁeld  P. Blunsom  K. Kavukcuoglu  and D. Hassabis
(2016). “Hybrid computing using a neural network with dynamic external memory”. In: Nature
538.7626.

Greff  K.  A. Rasmus  M. Berglund  T. H. Hao  H. Valpola  and J. Schmidhuber (2016). “Tagger:

Deep Unsupervised Perceptual Grouping”. In: NIPS.

Greff  K.  S. van Steenkiste  and J. Schmidhuber (2017). “Neural Expectation Maximization”. In:

NIPS.

Gulrajani  I.  K. Kumar  F. Ahmed  A. A. Taiga  F. Visin  D. Vazquez  and A. Courville (2016).

“Pixelvae: A latent variable model for natural images”. In: CoRR. arXiv: 1611.05013.

Ha  D. and J. Schmidhuber (2018). “World Models”. In: CoRR. arXiv: 1603.10122.
Hsieh  J.-T.  B. Liu  D.-A. Huang  L. Fei-Fei  and J. C. Niebles (2018). “Learning to Decompose and

Disentangle Representations for Video Prediction”. In: NIPS.

Ilin  A.  I. Prémont-Schwarz  T. H. Hao  A. Rasmus  R. Boney  and H. Valpola (2017). “Recurrent

Ladder Networks”. In: NIPS.

Itseez (2015). Open Source Computer Vision Library. https://github.com/itseez/opencv.
Jacobsen  J.-H.  J. Van Gemert  Z. Lou  and A. W. M. Smeulders (2016). “Structured Receptive Fields

in CNNs”. In: CVPR.

Jaderberg  M.  K. Simonyan  A. Zisserman  and K. Kavukcuoglu (2015). “Spatial Transformer

Networks”. In: NIPS. DOI: 10.1038/nbt.3343. arXiv: 1506.02025v1.

Kemp  C. and J. B. Tenenbaum (2008). “The discovery of structural form”. In: Proceedings of the

National Academy of Sciences 105.31.

Kim  H. and A. Mnih (2018). “Disentangling by factorising”. In: ICML. arXiv: 1802.05983.
Kingma  D. P. and J. Ba (2015). “Adam: A Method for Stochastic Optimization”. In: ICLR. arXiv:

1412.6980.

Kingma  D. P. and M. Welling (2013). “Auto-encoding variational bayes”. In: arXiv preprint

arXiv:1312.6114.

Kosiorek  A. R.  A. Bewley  and I. Posner (2017). “Hierarchical Attentive Recurrent Tracking”. In:

NIPS. arXiv: 1706.09262.

Kwak  S.  M. Cho  I. Laptev  J. Ponce  and C. Schmid (2015). “Unsupervised object discovery and

tracking in video collections”. In: ICCV. IEEE.

LeCun  Y.  Y. Bengio  and G. Hinton (2015). “Deep learning”. In: Nature 521.7553.
LeCun  Y.  B. Boser  J. S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D. Jackel (1989).

“Backpropagation applied to handwritten zip code recognition”. In: Neural computation 1.4.

Maddison  C. J.  J. Lawson  G. Tucker  N. Heess  M. Norouzi  A. Mnih  A. Doucet  and Y. Teh (2017).

“Filtering Variational Objectives”. In: Advances in Neural Information Processing Systems.

10

Mnih  A. and K. Gregor (2014). “Neural Variational Inference and Learning in Belief Networks”. In:

ICML. arXiv: 1402.0030v2.

Mnih  A. and D. J. Rezende (2016). “Variational inference for Monte Carlo objectives”. In: ICML.

arXiv: 1602.06725.

Neiswanger  W. and F. Wood (2012). “Unsupervised Detection and Tracking of Arbitrary Objects

with Dependent Dirichlet Process Mixtures”. In: CoRR. arXiv: 1210.3288.

Oord  A. van den  N. Kalchbrenner  O. Vinyals  L. Espeholt  A. Graves  and K. Kavukcuoglu (2016).

“Conditional Image Generation with PixelCNN Decoders”. In: NIPS. arXiv: 1606.05328.

Ranzato  M.  A. Szlam  J. Bruna  M. Mathieu  R. Collobert  and S. Chopra (2014). “Video (language)

modeling: a baseline for generative models of natural videos”. In: CoRR. arXiv: 1412.6604.

Ristani  E.  F. Solera  R. Zou  R. Cucchiara  and C. Tomasi (2016). “Performance measures and a

data set for multi-target  multi-camera tracking”. In: ECCV. Springer.

Santoro  A.  D. Raposo  D. G. Barrett  M. Malinowski  R. Pascanu  P. Battaglia  and T. Lillicrap
(2017). “A simple neural network module for relational reasoning”. In: NIPS. arXiv: 1706.01427.
Schulter  S.  P. Vernaza  W. Choi  and M. K. Chandraker (2017). “Deep Network Flow for Multi-object

Tracking”. In: CVPR.

Shi  W.  J. Caballero  F. Huszar  J. Totz  A. P. Aitken  R. Bishop  D. Rueckert  and Z. Wang (2016).
“Real-Time Single Image and Video Super-Resolution Using an Efﬁcient Sub-Pixel Convolutional
Neural Network”. In: CVPR.

Srivastava  N.  E. Mansimov  and R. Salakhudinov (2015). “Unsupervised learning of video represen-

tations using lstms”. In: ICML.

Steenkiste  S. van  M. Chang  K. Greff  and J. Schmidhuber (2018). “Relational Neural Expectation

Maximization: Unsupervised Discovery of Objects and their Interactions”. In: ICLR.

Tieleman  T. and G. Hinton (2012). Lecture 6.5—RmsProp: Divide the gradient by a running average

of its recent magnitude. COURSERA: Neural Networks for Machine Learning.

Tulyakov  S.  M.-Y. Liu  X. Yang  and J. Kautz (2018). “Mocogan: Decomposing motion and content

for video generation”. In: CVPR.

Valmadre  J.  L. Bertinetto  J. F. Henriques  A. Vedaldi  and P. H. S. Torr (2017). “End-to-end

representation learning for Correlation Filter based tracking”. In: CVPR. arXiv: 1704.06036.

Weber  T.  S. Racanière  D. P. Reichert  L. Buesing  A. Guez  D. J. Rezende  A. P. Badia  O. Vinyals 
N. Heess  Y. Li  et al. (2017). “Imagination-augmented agents for deep reinforcement learning”. In:
NIPS.

Xiao  F. and Y. Jae Lee (2016). “Track and segment: An iterative unsupervised approach for video

object proposals”. In: CVPR.

Zaheer  M.  S. Kottur  S. Ravanbakhsh  B. Póczos  R. R. Salakhutdinov  and A. J. Smola (2017).

“Deep Sets”. In: NIPS.

11

,Adam Kosiorek
Yee Whye Teh
Ingmar Posner