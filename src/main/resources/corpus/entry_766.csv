2016,Bayesian latent structure discovery from multi-neuron recordings,Neural circuits contain heterogeneous groups of neurons that differ in type  location  connectivity  and basic response properties. However  traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular  they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via Pólya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains.  We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina  revealing latent patterns of neural types and locations from spike trains alone.,Bayesian latent structure discovery from

multi-neuron recordings

Scott W. Linderman
Columbia University

swl2133@columbia.edu

Ryan P. Adams

Harvard University and Twitter

rpa@seas.harvard.edu

Jonathan W. Pillow
Princeton University

pillow@princeton.edu

Abstract

Neural circuits contain heterogeneous groups of neurons that differ in type  location 
connectivity  and basic response properties. However  traditional methods for
dimensionality reduction and clustering are ill-suited to recovering the structure
underlying the organization of neural circuits. In particular  they do not take
advantage of the rich temporal dependencies in multi-neuron recordings and fail
to account for the noise in neural spike trains. Here we describe new tools for
inferring latent structure from simultaneously recorded spike train data using a
hierarchical extension of a multi-neuron point process model commonly known as
the generalized linear model (GLM). Our approach combines the GLM with ﬂexible
graph-theoretic priors governing the relationship between latent features and neural
connectivity patterns. Fully Bayesian inference via Pólya-gamma augmentation
of the resulting model allows us to classify neurons and infer latent dimensions of
circuit organization from correlated spike trains. We demonstrate the effectiveness
of our method with applications to synthetic data and multi-neuron recordings in
primate retina  revealing latent patterns of neural types and locations from spike
trains alone.

1

Introduction

Large-scale recording technologies are revolutionizing the ﬁeld of neuroscience [e.g.  1  5  15]. These
advances present an unprecedented opportunity to probe the underpinnings of neural computation 
but they also pose an extraordinary statistical and computational challenge: how do we make sense
of these complex recordings? To address this challenge  we need methods that not only capture
variability in neural activity and make accurate predictions  but also expose meaningful structure
that may lead to novel hypotheses and interpretations of the circuits under study. In short  we need
exploratory methods that yield interpretable representations of large-scale neural data.
For example  consider a population of distinct retinal ganglion cells (RGCs). These cells only respond
to light within their small receptive ﬁeld. Moreover  decades of painstaking work have revealed a
plethora of RGC types [16]. Thus  it is natural to characterize these cells in terms of their type and
the location of their receptive ﬁeld center. Rather than manually searching for such a representation
by probing with different visual stimuli  here we develop a method to automatically discover this
structure from correlated patterns of neural activity.
Our approach combines latent variable network models [6  10] with generalized linear models of
neural spike trains [11  19  13  20] in a hierarchical Bayesian framework. The network serves as a
bridge  connecting interpretable latent features of interest to the temporal dynamics of neural spike
trains. Unlike many previous studies [e.g.  2  3  17]  our goal here is not necessarily to recover true
synaptic connectivity  nor is our primary emphasis on prediction. Instead  our aim is to explore
and compare latent patterns of functional organization  integrating over possible networks. To do
so  we develop an efﬁcient Markov chain Monte Carlo (MCMC) inference algorithm by leveraging

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: Components of the generative model. (a) Neurons inﬂuence one another via a sparse weighted network
of interactions. (b) The network parameterizes an autoregressive model with a time-varying activation. (c)
Spike counts are randomly drawn from a discrete distribution with a logistic link function. Each spike induces
an impulse response on the activation of downstream neurons. (d) Standard GLM analyses correspond to a
fully-connected network with Gaussian or Laplace distributed weights  depending on the regularization. (e-g) In
this work  we consider structured models like the stochastic block model (SBM)  in which neurons have discrete
latent types (e.g. square or circle)  and the latent distance model  in which neurons have latent locations that
determine their probability of connection  capturing intuitive and interpretable patterns of connectivity.

Pólya-gamma augmentation to derive collapsed Gibbs updates for the network. We illustrate the
robustness and scalability of our algorithm with synthetic data examples  and we demonstrate the
scientiﬁc potential of our approach with an application to retinal ganglion cell recordings  where we
recover the true underlying cell types and locations from spike trains alone  without reference to the
stimulus.

2 Probabilistic Model

Figure 1 illustrates the components of our framework. We begin with a prior distribution on networks
that generates a set of weighted connections between neurons (Fig. 1a). A directed edge indicates a
functional relationship between the spikes of one neuron and the activation of its downstream neighbor.
Each spike induces a weighted impulse response on the activation of the downstream neuron (Fig. 1b).
The activation is converted into a nonnegative ﬁring rate from which spikes are stochastically sampled
(Fig. 1c). These spikes then feed back into the subsequent activation  completing an autoregressive
loop  the hallmark of the GLM [11  19]. Models like these have provided valuable insight into
complex population recordings [13]. We detail the three components of this model in the reverse
order  working backward from the observed spike counts through the activation to the underlying
network.

2.1 Logistic Spike Count Models

Generalized linear models assume a stochastic spike generation mechanism. Consider a matrix of
spike counts  S ∈ NT×N   for T time bins and N neurons. The expected number of spikes ﬁred by
the n-th neuron in the t-th time bin  E[st n]  is modeled as a nonlinear function of the instantaneous
activation  ψt n  and a static  neuron-speciﬁc parameter  νn. Table 1 enumerates the three spike count
models considered in this paper  all of which use the logistic function  σ(ψ) = eψ(1 + eψ)−1  to
rectify the activation. The Bernoulli distribution is appropriate for binary spike counts  whereas the

2

cell 1cell 2cell 3cell Ncell 1cell 2cell 3cell NNetworkFiring RateSpike TrainA∼DenseW∼GaussianA∼BernoulliW∼DistanceA∼SBMW∼SBMA∼DistanceW∼SBM(a)(b)(c)(d)timetime(e)(f)(g)weightBin(ν  σ(ψ))

Standard Form

Distribution
Bern(σ(ψ))

p(s| ψ  ν)

(cid:1) σ(ψ)s σ(−ψ)ν−s
(cid:0)ν
σ(ψ)s σ(−ψ)1−s
(cid:0)ν+s−1
(cid:1) σ(ψ)s σ(−ψ)ν

σ(ψ) σ(−ψ)
νσ(ψ) σ(−ψ)
νeψ/σ(−ψ)
Table 1: Table of conditional spike count distributions  their parameterizations  and their properties.

νσ(ψ)
νeψ

NB(ν  σ(ψ))

(eψ)s
1+eψ

(cid:1) (eψ)s
(cid:0)ν
(cid:0)ν+s−1
(cid:1)

s

(1+eψ)ν

(eψ)s

Var(s)

s

(1+eψ)ν+s

s

s

E[s]
σ(ψ)

binomial and negative binomial have support for s ∈ [0  ν] and s ∈ [0 ∞)  respectively. Notably
lacking from this list is the Poisson distribution  which is not directly amenable to the augmentation
schemes we derive below; however  both the binomial and negative binomial distributions converge to
the Poisson under certain limits. Moreover  these distributions afford the added ﬂexibility of modeling
under- and over-dispersed spike counts  a biologically signiﬁcant feature of neural spiking data [4].
Speciﬁcally  while the Poisson has unit dispersion (its mean is equal to its variance)  the binomial
distribution is always under-dispersed  since its mean always exceeds its variance  and the negative
binomial is always over-dispersed  with variance greater than its mean.
Importantly  all of these distributions can be written in a standard form  as shown in Table 1. We
exploit this fact to develop an efﬁcient Markov chain Monte Carlo (MCMC) inference algorithm
described in Section 3.

2.2 Linear Activation Model

The instantaneous activation of neuron n at time t is modeled as a linear  autoregressive function of
preceding spike counts of neighboring neurons 

N(cid:88)

∆tmax(cid:88)

m=1

∆t=1

K(cid:88)

ψt n (cid:44) bn +

hm→n[∆t] · st−∆t m 

(1)

where bn is the baseline activation of neuron n and hm→n : {1  . . .   ∆tmax} → R is an impulse
response function that models the inﬂuence spikes on neuron m have on the activation of neuron n
at a delay of ∆t. To model the impulse response  we use a spike-and-slab formulation [8] 

hm→n[∆t] = am→n

w(k)

m→n φk[∆t].

(2)

k=1

m→n  ...  w(K)

the connections  weights  and ﬁltered spike trains and write the activation as 

Here  am→n ∈ {0  1} is a binary variable indicating the presence or absence of a connection
from neuron m to neuron n  the weight wm→n = [w(1)
m→n] denotes the strength of the
connection  and {φk}K
k=1 is a collection of ﬁxed basis functions. In this paper  we consider scalar
weights (K = 1) and use an exponential basis function  φ1[∆t] = e−∆t/τ   with time constant
the spike train and the basis function to obtain(cid:98)s(k)
of τ = 15ms. Since the basis function and the spike train are ﬁxed  we precompute the convolution of
∆t=1 φk[∆t] · st−∆t m. Finally  we combine
where an = [1  a1→n1K  ...  aN→n1K]  wn = [bn  w1→n  ...  wN→n]  and(cid:98)st = [1 (cid:98)s(1)
t 1   ... (cid:98)s(K)
t N ].
Here  (cid:12) denotes the Hadamard (elementwise) product and 1K is length-K vector of ones. Hence  all
of these vectors are of size 1 + N K. The difference between our formulation and the standard GLM
is that we have explicitly modeled the sparsity of the weights in am→n. In typical formulations [e.g. 
13]  all connections are present and the weights are regularized with (cid:96)1 and (cid:96)2 penalties to promote
sparsity. Instead  we consider structured approaches to modeling the sparsity and weights.

t m =(cid:80)∆tmax
ψt n = (an (cid:12) wn)T(cid:98)st 

(3)

2.3 Random Network Models

Patterns of functional interaction can provide great insight into the computations performed by neural
circuits. Indeed  many circuits are informally described in terms of “types” of neurons that perform
a particular role  or the “features” that neurons encode. Random network models formalize these

3

Name

Dense Model

Independent Model

Stochastic Block Model
Latent Distance Model

ρum→un
σ(−||un − vm||2

µvm→vn
2 + γ0) −||vn − vm||2

2 + µ0

Table 2: Random network models for the binary adjacency matrix or the Gaussian weight matrix.

ρ(um  un  θ)

µ(vm  vn  θ)

Σ(vm  vn  θ)

1
ρ

µ
µ

Σ
Σ

η2

Σvm→vn

intuitive descriptions. Types and features correspond to latent variables in a probabilistic model that
governs how likely neurons are to connect and how strongly they inﬂuence each other.
Let A = {{am→n}} and W = {{wm→n}} denote the binary adjacency matrix and the real-valued
array of weights  respectively. Now suppose {un}N
n=1 are sets of neuron-speciﬁc
latent variables that govern the distributions over A and W . Given these latent variables and global
parameters θ  the entries in A are conditionally independent Bernoulli random variables  and the
entries in W are conditionally independent Gaussians. That is 

n=1 and {vn}N

p(A  W |{un  vn}N

n=1  θ) =

N(cid:89)

N(cid:89)

m=1

n=1

Bern (am→n | ρ(um  un  θ))

× N (wm→n | µ(vm  vn  θ)  Σ(vm  vn  θ))  

(4)

where ρ(·)  µ(·)  and Σ(·) are functions that output a probability  a mean vector  and a covariance
matrix  respectively. We recover the standard GLM when ρ(·) ≡ 1  but here we can take advantage
of structured priors like the stochastic block model (SBM) [9]  in which each neuron has a discrete
type  and the latent distance model [6]  in which each neuron has a latent location. Table 2 outlines
the various models considered in this paper.
We can mix and match these models as shown in Figure 1(d-g). For example  in Fig. 1g  the adjacency
matrix is distance-dependent and the weights are block structured. Thus  we have a ﬂexible language
for expressing hypotheses about patterns of interaction. In fact  the simple models enumerated above
are instances of a rich family of exchangeable networks known as Aldous-Hoover random graphs 
which have been recently reviewed by Orbanz and Roy [10].

3 Bayesian Inference

Generalized linear models are often ﬁt via maximum a posteriori (MAP) estimation [11  19  13  20].
However  as we scale to larger populations of neurons  there will inevitably be structure in the
posterior that is not reﬂected with a point estimate. Technological advances are expanding the number
of neurons that can be recorded simultaneously  but “high-throughput” recording of many individuals
is still a distant hope. Therefore we expect the complexities of our models to expand faster than the
available distinct data sets to ﬁt them. In this situation  accurately capturing uncertainty is critical.
Moreover  in the Bayesian framework  we also have a coherent way to perform model selection
and evaluate hypotheses regarding complex underlying structure. Finally  after introducing a binary
adjacency matrix and hierarchical network priors  the log posterior is no longer a concave function of
model parameters  making direct optimization challenging (though see Soudry et al. [17] for recent
advances in tackling similar problems). These considerations motivate a fully Bayesian approach.
Computation in rich Bayesian models is often challenging  but through thoughtful modeling decisions
it is sometimes possible to ﬁnd representations that lead to efﬁcient inference. In this case  we have
carefully chosen the logistic models of the preceding section in order to make it possible to apply
the Pólya-gamma augmentation scheme [14]. The principal advantage of this approach is that  given
the Pólya-gamma auxiliary variables  the conditional distribution of the weights is Gaussian  and
hence is amenable to efﬁcient Gibbs sampling. Recently  Pillow and Scott [12] used this technique to
develop inference algorithms for negative binomial factor analysis models of neural spike trains. We
build on this work and show how this conditionally Gaussian structure can be exploited to derive
efﬁcient  collapsed Gibbs updates.

4

3.1 Collapsed Gibbs updates for Gaussian observations
Suppose the observations were actually Gaussian distributed  i.e. st n ∼ N (ψt n  νn). The most
challenging aspect of inference is then sampling the posterior distribution over discrete connec-
tions  A. There may be many posterior modes corresponding to different patterns of connectivity.
Moreover  am→n and wm→n are often highly correlated  which leads to poor mixing of naïve Gibbs
sampling. Fortunately  when the observations are Gaussian  we may integrate over possible weights
and sample the binary adjacency matrix from its collapsed conditional distribution.
We combine the conditionally independent Gaussian priors on {wm→n} and bn into a joint Gaussian
distribution  wn |{vn}  θ ∼ N (wn | µn  Σn)  where Σn is a block diagonal covariance matrix.
Since ψt n is linear in wn (see Eq. 3)  a Gaussian likelihood is conjugate with this Gaussian prior 

given an and(cid:98)S = {(cid:98)st}T
T(cid:89)
p(wn |(cid:98)S  an  µn  Σn) ∝ N (wn | µn  Σn)
t=1N (st n | (an (cid:12) wn)T(cid:98)st  νn) ∝ N (wn |(cid:101)µn (cid:101)Σn) 
(cid:16)(cid:98)S
(cid:105)−1
(cid:105)
(cid:101)Σn =
(cid:101)µn = (cid:101)Σn
n I)(cid:98)S

t=1. This yields the following closed-form conditional:

Σ−1
n µn +

(cid:16)(cid:98)S

Σ−1
n +

n I)s: n

(ν−1

(cid:17)

T

(ν−1

(cid:17)

(cid:104)

(cid:104)

T

(cid:12) (anaT
n)

 

(cid:12) an

.

Now  consider the conditional distribution of an  integrating out the corresponding weights.
The prior distribution over an is a product of Bernoulli distributions with parame-
m=1. The conditional distribution is proportional to the ratio of the prior
ters ρn = {ρ(um  un  θ)}N
and posterior partition functions 

p(an |(cid:98)S  ρn  µn  Σn) =

(cid:90)

p(an  wn |(cid:98)S  ρn  µn  Σn) dwn
nΣ−1
n(cid:101)Σ
n (cid:101)µn
2(cid:101)µT
− 1
n µn
−1
− 1

(cid:12)(cid:12)Σn
(cid:12)(cid:12)(cid:101)Σn

(cid:12)(cid:12)− 1
(cid:12)(cid:12)− 1

(cid:110)
(cid:110)

2 µT

2 exp

2 exp

(cid:111)
(cid:111) .

= p(an | ρn)

Thus  we perform a joint update of an and wn by collapsing out the weights to directly sample the
binary entries of an. We iterate over each entry  am→n  and sample it from its conditional distribution
given {am(cid:48)→n}m(cid:48)(cid:54)=m. Having sampled an  we sample wn from its Gaussian conditional.
3.2

Pólya-gamma augmentation for discrete observations

Now  let us turn to the non-conjugate case of discrete count observations. The Pólya-gamma aug-
mentation [14] introduces auxiliary variables  ωt n  conditioned upon which the discrete likelihood
appears Gaussian and our collapsed Gibbs updates apply. The integral identity underlying this scheme
is

c

(eψ)a

(1 + eψ)b = c 2−beκψ

e−ωψ2/2 pPG(ω | b  0) dω 

(5)

where κ = a − b/2 and p(ω | b  0) is the density of the Pólya-gamma distribution PG(b  0)  which
does not depend on ψ. Notice that the discrete likelihoods in Table 1 can all be rewritten like
the left-hand side of (5)  for some a  b  and c that are functions of s and ν. Using (5) along with
priors p(ψ) and p(ν)  we write the joint density of (ψ  s  ν) as

p(s  ν  ψ) =

p(ν) p(ψ) c(s  ν) 2−b(s ν)eκ(s ν)ψe−ωψ2/2 pPG(ω | b(s  ν)  0) dω.

(6)

The integrand of Eq. 6 deﬁnes a joint density on (s  ν  ψ  ω) which admits p(s  ν  ψ) as a marginal
density. Conditioned on the auxiliary variable  ω  the likelihood as a function of ψ is 

(cid:90) ∞

0

p(s| ψ  ν  ω) ∝ eκ(s ν)ψe−ωψ2/2 ∝ N

(cid:0)ω−1κ(s  ν)| ψ  ω−1(cid:1) .

Thus  after conditioning on s  ν  and ω  we effectively have a linear Gaussian likelihood for ψ.
We apply this augmentation scheme to the full model  introducing auxiliary variables  ωt n for each
spike count  st n. Given these variables  the conditional distribution of wn can be computed in closed

(cid:90) ∞

0

5

Figure 2: Weighted adjacency matrices showing inferred networks and connection probabilities for synthetic
data. (a d) True network. (b e) Posterior mean using joint inference of network GLM. (c f) MAP estimation.

form  as before. Let κn = [κ(s1 n  νn)  . . .   κ(sT n  νn)] and Ωn = diag([ω1 n  . . .   ωT n]). Then

we have p(wn | sn (cid:98)S  an  µn  Σn  ωn  νn) ∝ N (wn |(cid:101)µn (cid:101)Σn)  where

(cid:104)

(cid:101)Σn =

Σ−1
n +

(cid:16)(cid:98)S

Ωn(cid:98)S

T

(cid:17)

(cid:105)−1

 

(cid:104)

(cid:101)µn = (cid:101)Σn

(cid:12) (anaT
n)

(cid:16)(cid:98)S

(cid:17)

(cid:105)

Σ−1
n µn +

T

κn

(cid:12) an

.

Having introduced auxiliary variables  we must now derive Markov transitions to update them as
well. Fortunately  the Pólya-gamma distribution is designed such that the conditional distribution of
the auxiliary variables is simply a “tilted” Pólya-gamma distribution 

p(ωt n | st n  νn  ψt n) = pPG(ωt n | b(st n  νn)  ψt n).

These auxiliary variables are conditionally independent given the activation and hence can be
sampled in parallel. Moreover  efﬁcient algorithms are available to generate Pólya-gamma random
variates [21]. Our Gibbs updates for the remaining parameters and latent variables (νn  un  vn  and θ)
are described in the supplementary material. A Python implementation of our inference algorithm is
available at https://github.com/slinderman/pyglm.

4 Synthetic Data Experiments

The need for network models is most pressing in recordings of large populations where the network
is difﬁcult to estimate and even harder to interpret. To assess the robustness and scalability of our
framework  we apply our methods to simulated data with known ground truth. We simulate a one
minute recording (1ms time bins) from a population of 200 neurons with discrete latent types that
govern the connection strength via a stochastic block model and continuous latent locations that
govern connection probability via a latent distance model. The spikes are generated from a Bernoulli
observation model.
First  we show that our approach of jointly inferring the network and its latent variables can provide
dramatic improvements over alternative approaches. For comparison  consider the two-step procedure
of Stevenson et al. [18] in which the network is ﬁt with an (cid:96)1-regularized GLM and then a probabilistic
network model is ﬁt to the GLM connection weights. The advantage of this strategy is that the
expensive GLM ﬁtting is only performed once. However  when the data is limited  both the network
and the latent variables are uncertain. Our Bayesian approach ﬁnds a very accurate network (Fig. 2b)

6

MAPW  MAPW(d)(e)(f)(a)(b)(c)TrueTrueAMCMCFigure 3: Scalability of our inference algorithm as a function of: (a) the number of time bins  T ; (b) the number
of neurons  N; and (c) the average sparsity of the network  ρ. Wall-clock time is divided into time spent sampling
auxiliary variables (“Obs.”) and time spent sampling the network (“Net.”).

by jointly sampling networks and latent variables. In contrast  the standard GLM does not account
for latent structure and ﬁnds strong connections as well as spuriously correlated neurons (Fig. 2c).
Moreover  our fully Bayesian approach ﬁnds a set of latent locations that mimics the true locations
and therefore accurately estimates connection probability (Fig. 2e). In contrast  subsequently ﬁtting a
latent distance model to the adjacency matrix of a thresholded GLM network ﬁnds an embedding
that has no resemblance to the true locations  which is reﬂected in its poor estimate of connection
probability (Fig. 2f).
Next  we address the scalability of our MCMC algorithm. Three major parameters govern the
complexity of inference: the number of time bins  T ; the number of neurons  N; and the level of
sparsity  ρ. The following experiments were run on a quad-core Intel i5 with 6GB of RAM. As shown
in Fig. 3a  the wall clock time per iteration scales linearly with T since we must resample N T auxiliary
variables. We scale at least quadratically with N due to the network  as shown in Fig. 3b. However 
the total cost could actually be worse than quadratic since the cost of updating each connection could
depend on N. Fortunately  the complexity of our collapsed Gibbs sampling algorithm only depends
on the number of incident connections  d  or equivalently  the sparsity ρ = d/N. Speciﬁcally  we
must solve a linear system of size d  which incurs a cubic cost  as seen in Fig. 3c.

5 Retinal Ganglion Cells

Finally  we demonstrate the efﬁcacy of this approach with an application to spike trains simultaneously
recorded from a population of 27 retinal ganglion cells (RGCs)  which have previously been studied
by Pillow et al. [13]. Retinal ganglion cells respond to light shown upon their receptive ﬁeld. Thus  it
is natural to characterize these cells by the location of their receptive ﬁeld center. Moreover  retinal
ganglion cells come in a variety of types [16]. This population is comprised of two types of cells  on
and off cells  which are characterized by their response to visual stimuli. On cells increase their ﬁring
when light is shone upon their receptive ﬁeld; off cells decrease their ﬁring rate in response to light in
their receptive ﬁeld. In this case  the population is driven by a binary white noise stimulus. Given
the stimulus  the cell locations and types are readily inferred. Here  we show how these intuitive
representations can be discovered in a purely unsupervised manner given one minute of spiking data
alone and no knowledge of the stimulus.
Figure 4 illustrates the results of our analysis. Since the data are binned at 1ms resolution  we have
at most one spike per bin and we use a Bernoulli observation model. We ﬁt the 12 network models
of Table 2 (4 adjacency models and 3 weight models)  and we ﬁnd that  in terms of predictive log
likelihood of held-out neurons  a latent distance model of the adjacency matrix and SBM of the
weight matrix performs best (Fig. 4a). See the supplementary material for a detailed description of
this comparison. Looking into the latent locations underlying the adjacency matrix our network GLM
(NGLM)  we ﬁnd that the inferred distances between cells are highly correlated with the distances
between the true locations. For comparison  we also ﬁt a 2D Bernoulli linear dynamical system
(LDS) — the Bernoulli equivalent of the Poisson LDS [7] — and we take rows of the N×2 emission
matrix as locations. In contrast to our network GLM  the distances between LDS locations are nearly
uncorrelated with the true distances (Fig. 4b) since the LDS does not capture the fact that distance
only affects the probability of connection  not the weight. Not only are our distances accurate  the
inferred locations are nearly identical to the true locations  up to afﬁne transformation. In Fig. 4c 
semitransparent markers show the inferred on cell locations  which have been rotated and scaled to

7

(a)(b)(c)Figure 4: Using our framework  retinal ganglion cell types and locations can be inferred from spike trains alone.
(a) Model comparison. (b) True and inferred distances between cells. (c) True and inferred cell locations. (d-f)
Inferred network  connection probability  and mean weight  respectively. See main text for further details.

best align with the true locations shown by the outlined marks. Based solely on patterns of correlated
spiking  we have recovered the receptive ﬁeld arrangements.
Fig. 4d shows the inferred network  A (cid:12) W   under a latent distance model of connection probability
and a stochastic block model for connection weight. The underlying connection probabilities from
the distance model are shown in Fig. 4e. Finally  Fig. 4f shows that we have discovered not only
the cell locations  but also their latent types. With an SBM  the mean weight is a function of latent
type  and under the posterior  the neurons are clearly clustered into the two true types that exhibit the
expected within-class excitation and between-class inhibition.

6 Conclusion
Our results with both synthetic and real neural data provide compelling evidence that our methods can
ﬁnd meaningful structure underlying neural spike trains. Given the extensive work on characterizing
retinal ganglion cell responses  we have considerable evidence that the representation we learn from
spike trains alone is indeed the optimal way to summarize this population of cells. This lends us
conﬁdence that we may trust the representations learned from spike trains recorded from more
enigmatic brain areas as well. While we have omitted stimulus from our models and only used
it for conﬁrming types and locations  in practice we could incorporate it into our model and even
capture type- and location-dependent patterns of stimulus dependence with our hierarchical approach.
Likewise  the network GLM could be combined with the PLDS as in Vidne et al. [20] to capture
sources of low dimensional  shared variability.
Latent functional networks underlying spike trains can provide unique insight into the structure of
neural populations. Looking forward  methods that extract interpretable representations from complex
neural data  like those developed here  will be key to capitalizing on the dramatic advances in neural
recording technology. We have shown that networks provide a natural bridge to connect neural types
and features to spike trains  and demonstrated promising results on both real and synthetic data.
Acknowledgments. We thank E. J. Chichilnisky  A. M. Litke  A. Sher and J. Shlens for retinal data. SWL is
supported by the Simons Foundation SCGB-418011. RPA is supported by NSF IIS-1421780 and the Alfred P.
Sloan Foundation. JWP was supported by grants from the McKnight Foundation  Simons Collaboration on the
Global Brain (SCGB AWD1004351)  NSF CAREER Award (IIS-1150186)  and NIMH grant MH099611.

8

Weights(a)(b)(c)(d)(e)(f)On Cell LocationsInferred distance [a.u.]Pairwise DistancesTrue distance [a.u.]LDSNGLMReferences
[1] M. B. Ahrens  M. B. Orger  D. N. Robson  J. M. Li  and P. J. Keller. Whole-brain functional imaging at

cellular resolution using light-sheet microscopy. Nature methods  10(5):413–420  2013.

[2] D. R. Brillinger  H. L. Bryant Jr  and J. P. Segundo. Identiﬁcation of synaptic interactions. Biological

Cybernetics  22(4):213–228  1976.

[3] F. Gerhard  T. Kispersky  G. J. Gutierrez  E. Marder  M. Kramer  and U. Eden. Successful reconstruction of
a physiological circuit with known connectivity from spiking activity alone. PLoS Computational Biology 
9(7):e1003138  2013.

[4] R. L. Goris  J. A. Movshon  and E. P. Simoncelli. Partitioning neuronal variability. Nature Neuroscience 

17(6):858–865  2014.

[5] B. F. Grewe  D. Langer  H. Kasper  B. M. Kampa  and F. Helmchen. High-speed in vivo calcium imaging
reveals neuronal network activity with near-millisecond precision. Nature methods  7(5):399–405  2010.

[6] P. D. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data. Advances in

Neural Information Processing Systems 20  20:1–8  2008.

[7] J. H. Macke  L. Buesing  J. P. Cunningham  M. Y. Byron  K. V. Shenoy  and M. Sahani. Empirical models
of spiking in neural populations. In Advances in neural information processing systems  pages 1350–1358 
2011.

[8] T. J. Mitchell and J. J. Beauchamp. Bayesian Variable Selection in Linear Regression. Journal of the

American Statistical Association  83(404):1023—-1032  1988.

[9] K. Nowicki and T. A. B. Snijders. Estimation and prediction for stochastic blockstructures. Journal of the

American Statistical Association  96(455):1077–1087  2001.

[10] P. Orbanz and D. M. Roy. Bayesian models of graphs  arrays and other exchangeable random structures.

Pattern Analysis and Machine Intelligence  IEEE Transactions on  37(2):437–461  2015.

[11] L. Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network:

Computation in Neural Systems  15(4):243–262  Jan. 2004.

[12] J. W. Pillow and J. Scott. Fully bayesian inference for neural models with negative-binomial spiking. In
F. Pereira  C. Burges  L. Bottou  and K. Weinberger  editors  Advances in Neural Information Processing
Systems 25  pages 1898–1906. 2012.

[13] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  E. Chichilnisky  and E. P. Simoncelli. Spatio-
temporal correlations and visual signalling in a complete neuronal population. Nature  454(7207):995–999 
2008.

[14] N. G. Polson  J. G. Scott  and J. Windle. Bayesian inference for logistic models using Pólya–gamma latent

variables. Journal of the American Statistical Association  108(504):1339–1349  2013.

[15] R. Prevedel  Y.-G. Yoon  M. Hoffmann  N. Pak  G. Wetzstein  S. Kato  T. Schrödel  R. Raskar  M. Zimmer 
E. S. Boyden  et al. Simultaneous whole-animal 3d imaging of neuronal activity using light-ﬁeld microscopy.
Nature methods  11(7):727–730  2014.

[16] J. R. Sanes and R. H. Masland. The types of retinal ganglion cells: current status and implications for

neuronal classiﬁcation. Annual review of neuroscience  38:221–246  2015.

[17] D. Soudry  S. Keshri  P. Stinson  M.-h. Oh  G. Iyengar  and L. Paninski. A shotgun sampling solution for

the common input problem in neural connectivity inference. arXiv preprint arXiv:1309.3724  2013.

[18] I. H. Stevenson  J. M. Rebesco  N. G. Hatsopoulos  Z. Haga  L. E. Miller  and K. P. Körding. Bayesian
inference of functional connectivity and network structure from spikes. Neural Systems and Rehabilitation
Engineering  IEEE Transactions on  17(3):203–213  2009.

[19] W. Truccolo  U. T. Eden  M. R. Fellows  J. P. Donoghue  and E. N. Brown. A point process framework for
relating neural spiking activity to spiking history  neural ensemble  and extrinsic covariate effects. Journal
of Neurophysiology  93(2):1074–1089  2005. doi: 10.1152/jn.00697.2004.

[20] M. Vidne  Y. Ahmadian  J. Shlens  J. W. Pillow  J. Kulkarni  A. M. Litke  E. Chichilnisky  E. Simoncelli 
and L. Paninski. Modeling the impact of common noise inputs on the network activity of retinal ganglion
cells. Journal of computational neuroscience  33(1):97–121  2012.

[21] J. Windle  N. G. Polson  and J. G. Scott. Sampling Pólya-gamma random variates: alternate and approxi-

mate techniques. arXiv preprint arXiv:1405.0506  2014.

9

,Yuya Yoshikawa
Tomoharu Iwata
Hiroshi Sawada
Scott Linderman
Ryan Adams
Jonathan Pillow