2019,Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space,Quadratic regression involves modeling the response as a (generalized) linear function of not only the features $x^{j_1}$ but also of quadratic terms $x^{j_1}x^{j_2}$. The inclusion of such higher-order “interaction terms" in regression often provides an easy way to increase accuracy in already-high-dimensional problems. However  this explodes the problem dimension from linear $O(p)$ to quadratic $O(p^2)$  and it is common to look for sparse interactions (typically via heuristics). In this paper  we provide a new algorithm – Interaction Hard Thresholding (IntHT) which is the first one to provably accurately solve this problem in sub-quadratic time and space. It is a variant of Iterative Hard Thresholding; one that uses the special quadratic structure to devise a new way to (approx.) extract the top elements of a $p^2$ size gradient in sub-$p^2$ time and space. Our main result is to theoretically prove that  in spite of the many speedup-related approximations  IntHT linearly converges to a consistent estimate under standard high-dimensional sparse recovery assumptions. We also demonstrate its value via synthetic experiments. Moreover  we numerically show that IntHT can be extended to higher-order regression problems  and also theoretically analyze an SVRG variant of IntHT.,Interaction Hard Thresholding:

Consistent Sparse Quadratic Regression in

Sub-quadratic Time and Space

Shuo Yang ∗

Department of Computer Science

University of Texas at Austin

Austin  TX 78712

yangshuo_ut@utexas.edu

Yanyao Shen ∗
ECE Department

University of Texas at Austin

Austin  TX 78712

shenyanyao@utexas.edu

Sujay Sanghavi
ECE Department

University of Texas at Austin

Austin  TX 78712

sanghavi@mail.utexas.edu

Abstract

Quadratic regression involves modeling the response as a (generalized) linear
function of not only the features xj  but also of quadratic terms xj1xj2. The
inclusion of such higher-order “interaction terms" in regression often provides an
easy way to increase accuracy in already-high-dimensional problems. However 
this explodes the problem dimension from linear O(p) to quadratic O(p2)  and it is
common to look for sparse interactions (typically via heuristics).
In this paper we provide a new algorithm – Interaction Hard Thresholding (IntHT)
– which is the ﬁrst one to provably accurately solve this problem in sub-quadratic
time and space. It is a variant of Iterative Hard Thresholding; one that uses the
special quadratic structure to devise a new way to (approx.) extract the top elements
of a p2 size gradient in sub-p2 time and space.
Our main result is to theoretically prove that  in spite of the many speedup-related
approximations  IntHT linearly converges to a consistent estimate under standard
high-dimensional sparse recovery assumptions. We also demonstrate its value via
synthetic experiments.
Moreover  we numerically show that IntHT can be extended to higher-order regres-
sion problems  and also theoretically analyze an SVRG variant of IntHT.

Introduction

1
Simple linear regression aims to predict a response y via a (possibly generalized) linear function θ(cid:62)x
of the feature vector x. Quadratic regression aims to predict y as a quadratic function x(cid:62)Θx of the
features x

Linear Model
y ∼ θ(cid:62)x

Quadratic Model
y ∼ x(cid:62)Θ x

The inclusion of such higher-order interaction terms – in this case second-order terms of the form
xj1xj2 – is common practice  and has been seen to provide much more accurate predictions in

∗equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

several high-dimensional problem settings like recommendation systems  advertising  social network
modeling and computational biology [23  11  3]. In this paper we consider quadratic regression with
an additional (possibly non-linear) link function relating y to x(cid:62)Θ x.
One problem with explicitly adding quadratic interaction terms is that the dimension of the problem
now goes from p to p2. In most cases  the quadratic problem is high-dimensional and will likely
overﬁt the data; correspondingly  it is common to implicitly / explicitly impose low-dimensional
structure on the Θ – with sparsity of Θ being a natural choice. A concrete example for sparse
interaction would be the genome-wide association study  where for a given phenotype  the associated
genetic variants are usually a sparse subset of all possible variants. Those genes usually interact with
each other and leads to the given phenotype [15].
The naive approach to solving this problem involves recasting this as a big linear model that is now
in p2 dimensions  with the corresponding p2 features being all pairs of the form xj1 xj2. However 
this approach takes Ω(p2) time and space  since sparse linear regression cannot be done in time and
space smaller than its dimension – which in this case is p2 – even in cases where statistical properties
like restricted strong convexity / incoherence etc. hold. Fundamentally  the problem lies in the fact
that one needs to compute a gradient of the loss  and this is an Ω(p2) operation.
Our motivation: Can we learn a sparse quadratic model with time and space complexity that is
sub-quadratic? In particular  suppose we have data which is well modeled by a Θ∗ that is K-sparse 
with K being O(pγ) and γ < 1. Statistically  this can be possibly recovered from O(K log p)
samples  each of which is p-dimensional. Thus we have a setting where the input is sub-quadratic
with size O(Kp log p)  and the ﬁnal output is sub-quadratic with size O(K). Our aim is to have an
algorithm whose time and space complexity is also sub-quadratic for this case.
In this paper  we develop a new algorithm which has this desired sub-quadratic complexity  and
subsequently theoretically establish that it consistently recovers a sparse Θ∗. We brieﬂy overview
our setting and results below.

1.1 Main Contributions
Given n samples {(xi  yi)}n
sponding to a quadratic model:

i=1  we are interested in minimizing the following loss function corre-

f(cid:0)x(cid:62)

n−1(cid:88)

i=0

(cid:1)

(Quadratic Structure)

min

Θ:(cid:107)Θ(cid:107)0≤K

1
n

i Θxi  yi

:= Fn (Θ)

(1)

We develop a new algorithm – Interaction Hard Thresholding (IntHT)  outlined in Algorithm 1
– for this problem  and provide a rigorous proof of consistency for it under the standard settings
(Restricted strong convexity and smoothness of the loss) for which consistency is established for
sparse recovery problems. At a high level  it is based on the following key ideas:

(1) Because of the special quadratic structure  we show that the top 2k entries of the gradient can
be found in sub-quadratic time and space  using ideas from hashing and coding. The subroutine
in Algorithm 2 for doing this is based on the idea of [21] and Theorem 1 characterizes its
performance and approximation guarantee.

(2) We note a simple but key fact: in (stochastic) iterative hard thresholding  the new k-sparse Θt+1
that is produced has its support inside the union of two sets of size k and 2k: the support of the
previous Θt  and the top-2k elements of the gradient.

(3) While we do not ﬁnd the precise top-2k elements of the gradient  we do ﬁnd an approximation.
Using a new theoretical analysis  we show that this approximate-top-2k is still sufﬁcient to establish
linear convergence to a consistent solution. This is our main result  described in Theorem 4.

(4) As an extension  we show that our algorithm also works with popular SGD variants like SVRG
(Algorithm 4 in Appendix B)  with provable linear convergence and consistency in Appendix C.
We also demonstrate the extension of our algorithm to estimate higher order interaction terms with
a numerical experiment in Section 5 .

Notation We use [n] to represent the set {0 ···   n− 1}. We use fB (Θ) to denote the average loss on
batch B  where B is a subset of [n] with batch size m. We deﬁne (cid:104)A  B(cid:105) = tr (A(cid:62)B)  and supp(A)

2

to be the index set of A with non-zero entries. We let PS to be the projection operator onto the index

set S. We use standard Big-O notation for time/space complexity analysis  and Big-(cid:101)O notation which

ignores log factors.

2 Related Work

Learning with high-order interactions Regression with interaction terms has been studied in the
statistics community. However  many existing results consider under the assumption of strong/weak
hierarchical (SH/WH) structure: the coefﬁcient of the interaction term xj1 xj2 is non-zero only when
both coefﬁcients of xj1 and xj2 are (or at least one of them is) non-zero. Greedy heuristics [32  11]
and regularization based methods [7  3  16  25  10] are proposed accordingly. However  they could
potentially miss important signals that only contains the effect of interactions. Furthermore  several
of these methods also suffer from scaling problems due to the quadratic scaling of the parameter size.
There are also results considering the more general tensor regression  see  e.g.  [34  9]  among many
others. However  neither do these results focus on solutions with efﬁcient memory usage and time
complexity  which may become a potential issue when the dimension scales up. From a combinatorial
perspective  [18  13] learns sparse polynomial in Boolean domain using quite different approaches.
Sparse recovery  IHT and stochastic-IHT IHT [4] is one type of sparse recovery algorithms that is
proved to be effective for M-estimation [12] under the regular RSC/RSM assumptions. [20] proposes
and analyzes a stochastic version of IHT. [14  26] further consider variance reduced acceleration
algorithm under this high dimensional setting  [35] studies IHT in high dimensional setting with
nonlinear measurement. Notice that IHT  if used for our quadratic problem  still suffers from quadratic
space  similar to other techniques  e.g.  the Lasso  basis pursuit  least angle regression [29  6  8]. On
the other hand  [19] recently considers a variant of IHT  where for each sample  only a random subset
of features is observed. This makes each update cheap  but their sample size has linear dependence
on the ambient dimension  which is again quadratic. Apart from that  [20  17] also show that IHT can
potentially tolerate a small amount of error per iteration .
Maximum inner product search One key technique of our method is extracting the top elements
(by absolute value) of gradient matrix  which can be expressed as the inner product of two matrices.
This can be formulated as ﬁnding Maximum Inner Product (MIP) from two sets of vectors. In
practice  algorithms speciﬁcally designed for MIP are proposed based on locality sensitive hashing
[27]  and many other greedy type algorithms [2  33]. But they either can’t ﬁt into the regression
setting  or suffers from quadratic complexity. In theory  MIP is treated as a fundamental problem
in the recent development of complexity theory [1  31]. [1  5] shows the hardness of MIP  even for
Boolean vectors input. While in general hard  there are data dependent approximation guarantees 
using the compressed matrix multiplication method [21]  which inspired our work.
Others The quadratic problem we study also share similarities with several other problem settings 
including factorization machine [23] and kernel learning [24  22]. Different from factorization
machine  we do not require the input data to be sparse. While the factorization machine tries to learn
a low rank representation  we are interested in learning a sparse representation. Compared to kernel
learning  especially the quadratic / polynomial kernels  our task is to do feature selection and identify
the correct interactions.

3

Interaction Hard Thresholding

We now describe the main ideas motivating our approach  and then formally describe the algorithm.
Naively recasting as a linear model has p2 time and space complexity: As a ﬁrst step to our
method  let us see what happens with the simplest approach. Speciﬁcally  as noted before  problem
(1) can be recast as one of ﬁnding a sparse (generalized) linear model in the p2 size variable Θ:

n−1(cid:88)

i=0

(Recasting as linear model)

min

Θ:(cid:107)Θ(cid:107)0≤K

1
n

f ((cid:104)Xi  Θ(cid:105)  yi )

where matrix Xi := xix(cid:62)
i . Iterative hard thresholding (IHT) [4] is a state-of-the-art method (both in
terms of speed and statistical accuracy) for such sparse (generalized) linear problems. This involves

3

i=1  dimension p

3: Output: The parameter estimation (cid:98)Θ

Algorithm 1 INTERACTION HARD THRESHOLDING (INTHT)
1: Input: Dataset {xi  yi}n
2: Parameters: Step size η  estimation sparsity k  batch size m  round number T
4: Initialize Θ0 as a p × p zero matrix.
5: for t = 0 to T − 1 do
6:
7:
8:
9:
10:
11:
12:
13:

Draw a subset of indices Bt from [n] randomly.
Calculate the residual ui = u(Θt  xi  yi) based on eq. (2)  for every i ∈ Bt.
Set At ∈ Rp×m   where each column of At is uixi  i ∈ Bt.
Compute (cid:101)St = ATEE(At  Bt  2k).
Set Bt ∈ Rp×m  where each column of Bt is xi  i ∈ Bt. (where AtB(cid:62)
Set St = (cid:101)St ∪ supp(Θt).
Compute PSt(Gt) ← the gradient value Gt = 1
Update Θt+1 = Hk (Θt − ηPSt(Gt)).

(cid:80)
i∈Bt uixix(cid:62)

m

14: Return: (cid:98)Θ = ΘT

m gives the gradient)
—-/* approximate top elements extraction */—- X 2
U(cid:62)
—-/* inaccurate hard thresholding update */—- X 2
U(cid:62)

t

i only calculated on St.

Algorithm 2 APPROXIMATED TOP ELEMENTS EXTRACTION (ATEE)
1: Input: Matrix A  matrix B  top selection size k
2: Parameters: Output set size upper bound b  repetition number d  signiﬁcant level ∆
3: Expected Output: Set Λ: the top-k elements in AB(cid:62) with absolute value greater than ∆

4: Output: Set(cid:101)Λ of indices  with size at most b (approximately contains Λ)

5: Short Description: This algorithm is adopted directly from [21]. It follows from the matrix
compressed product via FFT (see section 2.2 of [21]) and sub-linear result extraction by error-
correcting code (see section 4 of [21])  which drastically reduces the complexity. The whole
process is repeated for d times to boost the success probability. The notation here matches [21]
exactly  except that we use p for dimension while n is used in [21] instead.
6: Intuitively  the algorithm will put all the elements of AB(cid:62) into b different "basket"s  with each
of the elements assigned a positive or negative sign. It then selects the "basket" whose magnitude
is greater than ∆. Further  one large element is recovered from each of the selected baskets.

the following update rule

(standard IHT)

Θt+1 = Hk

(cid:0) Θt − η ∇Fn(Θt)(cid:1)

where Fn(·) is the average loss deﬁned in (1)  and Hk(·) is the hard-thresholding operator that
chooses the largest k elements (in terms of absolute value) of the matrix given to it  and sets the rest
to 0. Here  k is the estimation sparsity parameter. In this update equation  the current iterate Θt has
k non-zero elements and so can be stored efﬁciently. But the gradient ∇Fn(Θt) is p2 dimensional;
this causes IHT to have Ω(p2) complexity. This issue remains even if the gradient is replaced by a
stochastic gradient that uses fewer samples  since even in a stochastic gradient the number of variables
remains p2.
A key observation: We only need to know the top-2k elements of this gradient ∇Fn(Θt)  because
of the following simple fact: if A is a k-sparse matrix  and B is any matrix  then

supp(Hk(A + B)) ⊂ supp(A) ∪ supp(H2k(B)).

That is  the support of the top k elements of the sum A + B is inside the union of the support of A 
and the top-2k elements of B. The size of this union set is at most 3k.
Thus  in the context of standard IHT  we do not really need to know the full (stochastic) gradient
∇Fn(Θt); instead we only need to know (a) the values and locations of its top-2k elements  and (b)
evaluate at most k extra elements of it – those corresponding to the support of the current Θt.
The key idea of our method is to exploit the special structure of the quadratic model to ﬁnd the top-2k
elements of the batch gradient ∇fB in sub-quadratic time. Speciﬁcally  ∇fB has the following form:
(2)

∇f(cid:0)x(cid:62)

u(Θ  xi  yi)xix(cid:62)
i  

(cid:1) =

(cid:88)

(cid:88)

i Θxi  yi

∇fB(Θ) (cid:44) 1
m

1
m

i∈B

i∈B

4

top-2k elements of the p2-dimensional stochastic gradient in (cid:101)O(k(p + k)) time and space  which is

where u(Θ  xi  yi) is a scalar related to the residual and the derivative of link function   and B
represents the mini-batch where B ⊂ [n]  |B| = m. This allows us to approximately ﬁnd the
sub-quadratic when k is O(pγ) for γ < 1.
Our algorithm is formally described in Algorithm 1. We use Approximate Top Elements Extraction
(ATEE) to approximately ﬁnd the top-2k elements of the gradient  which is brieﬂy summarized in
Algorithm 2  based on the idea of Pagh [21]. The full algorithm is re-organized and provided in
Appendix A for completeness. Our method  Interaction Hard Thresholding (IntHT) builds on IHT 
but needs a substantially new analysis for proof of consistency. The subsequent section goes into the
details of its analysis.

4 Theoretical Guarantees

In this section  we establish the consistency of Interaction Hard Thresholding  in the standard setting
where sparse recovery is established.
Speciﬁcally  we establish convergence results under deterministic assumptions on the data and
function  including restricted strong convexity (RSC) and smoothness (RSM). Then  we analyze
the sample complexity when features are generated from sub-gaussian distribution in the quadratic
regression setting  in order to have well-controlled RSC and RSM parameters. The analysis of
required sample complexity yields an overall complexity that is sub-quadratic in time and space.

4.1 Preliminaries

We ﬁrst describe the standard deterministic setting in which sparse recovery is typically analyzed.
Speciﬁcally  the samples (xi  yi) are ﬁxed and known. Our ﬁrst assumption deﬁnes how our intended
recovery target Θ(cid:63) relates to the resulting loss function Fn(·).
Assumption 1 (Standard identiﬁability assumption). There exists a Θ(cid:63) which is K-sparse such that
the following holds: given any batch B ⊂ [n] of m samples  the norm of batch gradient at Θ(cid:63) is
bounded by constant G. That is  (cid:107)∇fB(Θ(cid:63))(cid:107)F ≤ G  and (cid:107)Θ(cid:63)(cid:107)∞ ≤ ω.
In words  this says the the gradient at Θ(cid:63) is small. In a noiseless setting where data is generated
from Θ(cid:63)  e.g. when yi = x(cid:62)
i Θ(cid:63)xi  this gradient is 0; i.e. the above is satisﬁed with G = 0  and Θ(cid:63)
would be the exact sparse optimum of Fn(·). The above assumption generalizes this notion to noisy
and non-linear cases  relating our recovery target Θ(cid:63) to the loss function. This is a standard setup
assumption in sparse recovery.
Now that we have speciﬁed what Θ(cid:63) is and why it is special  we specify the properties the loss
function needs to satisfy. These are again standard in the sparse recovery literature [20  26  14].
Assumption 2 (Standard landscape properties of the loss). For any pair Θ1  Θ2 and s ≤ p2 such
that |supp(Θ1 − Θ2)| ≤ s
• The overall loss Fn satisﬁes αs-Restricted Strong Convexity (RSC):
αs
2

Fn(Θ1) − Fn(Θ2) ≥ (cid:104)Θ1 − Θ2 ∇ΘFn(Θ2)(cid:105) +

(cid:107)Θ1 − Θ2(cid:107)2

F

• The mini-batch loss fB satisﬁes Ls-Restricted Strong Smoothness (RSM):

(cid:107)∇fB(Θ1) − ∇fB(Θ2)(cid:107)F ≤ Ls (cid:107)Θ1 − Θ2(cid:107)F   ∀B ⊂ [n]   |B| = m

• fB satisﬁes Restricted Convexity (RC) (but not strong):

fB(Θ1) − fB(Θ2) − (cid:104)∇fB(Θ2)  Θ1 − Θ2(cid:105) ≥ 0  ∀B ⊂ [n]   |B| = m  s = 3k + K

Note: While our assumptions are standard  our result does not follow immediately from existing
analyses – because we cannot ﬁnd the exact top elements of the gradient. We need to do a new
analysis to show that even with our approximate top element extraction  linear convergence to Θ(cid:63)
still holds.

5

4.2 Main Results

Here we proceed to establish the sub-quadratic complexity and consistency of IntHT for parameter
estimation. Theorem 1 presents the analysis of ATEE. It provides the computation complexity
analysis  as well as the statistical guarantee of support recovery. Based on this  we show the per round
convergence property of Algorithm 1 in Theorem 3. We then establish our main statistical result  the
linear convergence of Algorithm 1 in Theorem 4.
Next  we discuss the batch size that guarantees support recovery in Theorem 5  focusing on the
quadratic regression setting  i.e.
the model is linear in both interaction terms and linear terms.
Combining all the established results  the sub-quadratic complexity is established in Corollary 6. All
the proofs in this subsection can be found in Appendix E.
Analysis of ATEE Consider ATEE with parameters set to be b  d  ∆. Recall this means that ATEE re-
Note that the desired index set (Λ) is composed by the top-2k elements of gradient ∇fB(Θ) whose
absolute value is greater than ∆. Suppose now the current estimate is Θ  and B is the batch. The

turns an index set ((cid:101)Λ) of size at most b  which is expected to contain the desired index set (Λ).
following theorem establishes when this output set ((cid:101)Λ) captures the top elements of the gradient.
F and d ≥ 48 log 2ck  then the index set ((cid:101)Λ)
Also in this case the time complexity of ATEE is (cid:101)O (m(p + b))  and space complexity is (cid:101)O (m(p + b)).

Theorem 1 (Recovering top-2k elements of the gradient  modiﬁed from [21]). With the setting above 
if we choose b  d  ∆ so that b∆2 ≥ 432(cid:107)∇fB(Θ)(cid:107)2
returned by ATEE contains the desired index set (Λ) with probability at least 1 − 1/c.

Theorem 1 requires that parameter b  ∆ are set to satisfy b∆2 ≥ 432(cid:107)∇fB(Θ)(cid:107)2
F . Note that ∆
controls the minimum magnitude of top-k element we can found. To avoid getting trivial extraction
result  we need to set ∆ as a constant that doesn’t scale with p. In order to control the scale of ∆
and b  to get consistent estimation and to achieve sub-quadratic complexity  we need to upper bound
(cid:107)∇fB(Θ)(cid:107)2
F . This is the compressibility estimation problem that was left open in [21]. In our case 
the batch gradient norm can be controlled by the RSM property. More formally  we have
Lemma 2 (Frobenius norm bound of gradient). The Frobenius norm of batch gradient at arbitrary
k-sparse Θ  with (cid:107)Θ(cid:107)∞ ≤ ω  can be bounded as (cid:107)∇fB(Θ)(cid:107)F ≤ 2L2k
kω + G  where G is the
uniform bound on (cid:107)∇fB(Θ(cid:63))(cid:107)F over all batches B and ω bounds (cid:107)Θ(cid:63)(cid:107)∞ (see Assumption 1).
Lemma 2 directly implies that Theorem 1 could allow b scale linearly with k while keep ∆ as a
constant2. This is the key ingredient to achieve sub-quadratic complexity and consistent estimation.
We postpone the discussion for complexity to later paragraph  and proceed to ﬁnish the statistical
analysis of gradient descent.
Convergence of IntHT: Consider IntHT with parameter set to be η  k. For the purpose of analysis 

we keep the deﬁnition of Λ and (cid:101)Λ from the analysis of ATEE and further deﬁne k∆ to be the

number of top-2k elements whose magnitude is below ∆. Recall that K is the sparsity of Θ(cid:63)  deﬁne
/2  ρ = K/k  where ν measures the error induced by exact IHT (see
ν = 1 +
Lemma 9 for detail). Denote Bt = {B0 B1  ... Bt}. We have
Theorem 3 (Per-round convergence of IntHT). Following the above notations  the per-round conver-
gence of Algorithm 1 satisﬁes the following:

(cid:16)
ρ +(cid:112)(4 + ρ)ρ

(cid:17)

√

2For now  we assume L2k to be a constant independent of p  k. We will discuss this in Theorem 5.

6

+ σ2

GD + σ2

F ail|GD 

EBt

• If ATEE succeeds  i.e.  Λ ⊆(cid:101)Λ  then
(cid:104)(cid:13)(cid:13)Θt − Θ(cid:63)(cid:13)(cid:13)2
where κ1 = ν(cid:0)1 − 2ηα2k + 2η2L2
• If ATEE fails  i.e.  Λ (cid:54)⊂(cid:101)Λ  then 
(cid:104)(cid:13)(cid:13)Θt − Θ(cid:63)(cid:13)(cid:13)2

(cid:105) ≤ κ1EBt−1
(cid:1)  σ2
(cid:105) ≤ κ2EBt−1

σ2
GD = max

|Ω|≤2k+K

EBt

(cid:104)

4νη

√

2k

F

kω (cid:107)PΩ (∇F (Θ(cid:63)))(cid:107)F + 2νη2EBt
(cid:105)

(cid:104)(cid:13)(cid:13)Θt−1 − Θ(cid:63)(cid:13)(cid:13)2

F

F

(cid:104)(cid:13)(cid:13)Θt−1 − Θ(cid:63)(cid:13)(cid:13)2

(cid:105)

√
∆|GD = 4

k∆η

√

F

+ σ2

GD + σ2

∆|GD 
kω∆ + 2k∆η2∆2  and

(cid:104)(cid:107)PΩ (∇fBt (Θ(cid:63)))(cid:107)2

F

(cid:105)(cid:105)

.

where κ2 = κ1 + 2νηL2k  σ2

F ail|GD = max|Ω|≤2k+K

(cid:104)

4νη

(cid:105)
kωEBt [(cid:107)PΩ (∇fBt (Θ(cid:63)))(cid:107)F ]

.

√

Remark 1. It is worth noting that σGD  σF ail|GD are both statistical errors  which in the noiseless
case are 0. In the case that the magnitude of top-2k elements in the gradient are all greater than ∆ 
we have k∆ = 0  which implies σ∆|GD = 0. In this case ATEE’s approximation doesn’t incur any
additional error compared with exact IHT.

2k/α2

2k)  η = α2k/2L2

Theorem 3 shows that by setting k = Θ(KL2
2k  the parameter estimation can
be improved geometrically when ATEE succeeds. We will show in Theorem 5 that with suffciently
large batch size m  α2k  L2k are controlled and don’t scale with k  p. When ATEE fails  it can’t make
the Θ estimation worse by too much. Given that success rate of ATEE is controlled in Theorem 1  it
naturally suggests that we can obtain the linear convergence in expectation. This leads to Theorem 4.
Deﬁne σ2
GD + σF ail|GD. Let φt to be the success indicator of
ATEE at time step t  and Φt = {φ0  φ1  ...  φt}. By Theorem 1  with d = 48 log 2ck  ATEE recovers
top-2k with probability at least (1 − 1/c)  we can easily show the convergence of Algorithm 1 as
Theorem 4 (Main result). Following the above notations  the expectation of the parameter recovery
error of Algorithm 1 is bounded by

∆|GD  and σ2

GD + σ2

1 = σ2

2 = σ2

(cid:104)(cid:13)(cid:13)Θt − Θ(cid:63)(cid:13)(cid:13)2

F

EBt Φt

(cid:34)(cid:18)

+

κ1 +

(κ2 − κ1)

1
c

(cid:18)
(cid:105) ≤
(cid:19)t − 1

(κ2 − κ1)

κ1 +

1
c

(cid:35)(cid:18) σ2

(cid:19)

(cid:19)t(cid:13)(cid:13)Θ0 − Θ(cid:63)(cid:13)(cid:13)2

F

1

κ1 − 1

κ2 − 1

c − cκ1 + κ1 − κ2

+

(cid:18) σ2

2

κ2 − 1

(cid:19)

.

− σ2
κ1 − 1

1

This shows that Algorithm 1 achieves linear convergence by setting c ≥ (κ2 − κ1)/(1 − κ1). With c
1/(1 − κ1). The proof follows directly by taking expectation
increasing  the error ball converges to σ2
of the result we obtain in Theorem 3 with the recovery success probability established in Theorem 1.
Computational analysis With the linear convergence  the computational complexity is dominated
by the complexity per iteration. Before discussing the complexity  we ﬁrst establish the dependency
between Lk  αk and m in the special case of quadratic regression  where the link function is identity.
Notice that similar results would hold for more general quadratic problems as well.
Theorem 5 (Minimum batch size). For feature vector x ∈ Rp  whose ﬁrst p − 1 coordinates
are drawn i.i.d. from a bounded distribution  and the p-th coordinate is constant 1. W.l.o.g.  we
assume the ﬁrst p − 1 coordinates to be zero mean  variance 1 and bounded by B. With batch size
m (cid:38) kB log p/2 we have αk ≥ 1 −   Lk ≤ 1 +  with high probability.
Note that the sample complexity requirement matches the known information theoretic lower bound
for recovering k-sparse Θ up to a constant factor. The proof is similar to the analysis of restricted
isometry property in sparse recovery. Recall that by Theorem 1  we have the per-iteration complexity

on the complexity:
Corollary 6 (Achieving sub-quadratic space and time complexity). In the case of quadratic regres-
sion  by setting the parameters as above  IntHT recovers Θ(cid:63) in expectation up to a noise ball with
when k is O(pγ) for γ < 1.

(cid:101)O(m(p + b)). Combining the results of Lemma 2  Theorems 4 and 5  we have the following corollary
linear convergence. The time and space complexity of IntHT is (cid:101)O(k(k + p))  which is sub-quadratic
complexity of IntHT is (cid:101)O(k(k + p))  which is nearly optimal.

Note that the optimal time and space complexity is Ω(kp)  since a minimum of Ω(k) samples
are required for recovery  and Ω(p) for reading all entries. Corollary 6 shows the time and space

5 Synthetic Experiments

To examine the sub-quadratic time and space complexity  we design three tasks to answer the
following three questions: (i) Whether Algorithm 1 maintains linear convergence despite the hard
thresholding not being accurate? (ii) What is the dependency between b and k to guarantee successful
recovery? (iii) What is the dependency between m and p to guarantee successful recovery? Recall that

7

the per-iteration complexity of Algorithm 1 is (cid:101)O(m(p + b))  where b upper bounds the size of ATEE’s

output set  p is the dimension of features and m is batch size and k is the sparsity of estimation. It
will be clear as we proceed how the three questions can support sub-quadratic complexity.
Experimental setting We generate feature vectors xi  whose coordinates follow i.i.d. uniform
distribution on [−1  1]. Constant 1 is appended to each feature vector to model the linear terms and
intercept. The true support is uniformly selected from all the interaction and linear terms  where
the non-zero parameters are then generated uniformly on [−20 −10] ∪ [10  20]. Note that for the
experiment concerning minimum batch size m  we instead use Bernoulli distribution to generate both
the features and the parameters  which reduces the variance for multiple random runs and makes our
phase transition plot clearer. The output yis  are generated following x(cid:62)
i Θ(cid:63)xi. On the algorithm
side  by default  we set p = 200  d = 3  K = 20  k = 3K  η = 0.2. Support recovery results with
different b-K combinations are averaged over 3 independent runs  results for m-p combinations are
averaged over 5 independent runs. All experiments are terminated after 150 iterations.

(a) Inaccurate recovery using differ-
ent ATEE’s output set sizes b

(b) Support recovery results with
different b and K

(c) Support recovery results with dif-
ferent m and p

Figure 1: Synthetic experiment results: note b  m are the parameters we used for IntHTand ATEE 
where b upper bounds the size of ATEE’s output set and m is the batch size used for IntHT. Recall
p is the dimension of features and K is the sparsity of Θ(cid:63). (a) the convergence behavior with
different choices of b. Linear convergence holds for small b  e.g.  360  when the parameter space is
around 20  000. (b) Support recovery results with different choices of (b  K). We observe a linear
dependence between b and K. (c) Support recovery results with different choices of (m  p). m scales
sub-linearly with p to ensure a success recovery.

Inaccurate support recovery with different b’s Figure 1-(a) demonstrates different convergence
results  measured by (cid:107)Θ − Θ(cid:63)(cid:107)F with multiple choices of b for ATEE in Algorithm 1. The dashed
curve is obtained by replacing ATEE with exact top elements extraction (calculates the gradient
exactly and picks the top elements). This is statistically optimal  but comes with quadratic complexity.
By choosing a moderately large b  the inaccuracy induced by ATEE has negligible impact on the
convergence. Therefore  Algorithm 1 can maintain the linear convergence despite the support recovery
in each iteration is inaccurate. This aligns with Theorem 3. With linear convergence  the per iteration
complexity will dominate the overall complexity.
Dependency between b and sparsity k We proceed to see the proper choice of b under different
sparsity k (we use k = 3K). We vary the sparsity K from 1 to 30  and apply Algorithm 1 with b
ranges from 30 to 600. As shown in Figure 1-(b)  the minimum proper choice of b scales no more
than linearly with k. This agrees with our analysis in Theorem 1. The per-iteration complexity then

collapse to (cid:101)O(m(p + k)).

Dependency between batch size m and dimension p Finally  we characterize the dependency
between minimum batch size m and the input dimension p. This will complete our discussion on the
per-iteration complexity. The batch size varies from 1 to 99  and the input dimension varies from 10
to 1000. In this experiment  we employ the Algorithm 1 with ATEE replaced by exact top-k elements
extraction. Figure 1-(c) demonstrates the support recovery success rate of each (k  p) combination. It
shows the minimum batch size scales in logarithm with dimension p  as we proved in Theorem 5.
Together with the previous experiment  it establishes the sub-quadratic complexity.

8

020406080100120140Iteration5102050100Parameter Estimation ErrorInaccurate recovery using different b sb=120b=240b=360b=480b=600Exact30130230330430530b30252015105Kb-K-dependency0.00.20.40.60.81.010210410610810p9979593919mm-p-dependency0.00.20.40.60.81.0Figure 2: 3-order regression support recovery using different ATEE’s output set sizes b

exploiting similar gradient structure(cid:80) rixi ⊗ xi ⊗ xi  where ri denotes the residual for (Xi  yi) 
(cid:80) Θi j kxixjxk  where Θ is now a three dimension tensor. Further  we set the dimension of x to 30

Higher order interaction IntHT is also extensible to higher order interactions. Speciﬁcally  by
⊗ denotes the outer product of vector  we can again combine sketching with high-dimensional
optimization to achieve nearly linear time and space (for constant sparsity).
For the experiment  we adopt the similar setting as for the Inaccurate support recovery with
different bs experiment. The main difference is that we change from yi = x(cid:62)
i Θ(cid:63)xi to yi =

and the sparsity K = 20. Figure 2 demonstrates the result of support recovering of 3-order interaction
terms with different setting of b  where b still bounds the size of ATEE’s output set. We can see that
IntHT still maintains the linear convergence in the higher order setting.

Acknowledgement

We would like to acknowledge NSF grants 1302435 and 1564000 for supporting this research.

9

020406080100Iteration125102050Parameter Estimation ErrorInaccurate recovery using different b sb=40b=80b=120b=160b=200ExactReferences
[1] Amir Abboud  Aviad Rubinstein  and Ryan Williams. Distributed pcp theorems for hardness
of approximation in p. In 2017 IEEE 58th Annual Symposium on Foundations of Computer
Science (FOCS)  pages 25–36. IEEE  2017.

[2] Grey Ballard  Tamara G Kolda  Ali Pinar  and C Seshadhri. Diamond sampling for approximate
maximum all-pairs dot-product (mad) search. In 2015 IEEE International Conference on Data
Mining  pages 11–20. IEEE  2015.

[3] Jacob Bien  Jonathan Taylor  and Robert Tibshirani. A lasso for hierarchical interactions. Annals

of statistics  41(3):1111  2013.

[4] Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing.

Applied and computational harmonic analysis  27(3):265–274  2009.

[5] Lijie Chen. On the hardness of approximate and exact (bichromatic) maximum inner product. In
33rd Computational Complexity Conference (CCC 2018). Schloss Dagstuhl-Leibniz-Zentrum
fuer Informatik  2018.

[6] Scott Shaobing Chen  David L Donoho  and Michael A Saunders. Atomic decomposition by

basis pursuit. SIAM review  43(1):129–159  2001.

[7] Nam Hee Choi  William Li  and Ji Zhu. Variable selection with the strong heredity constraint
and its oracle property. Journal of the American Statistical Association  105(489):354–364 
2010.

[8] Bradley Efron  Trevor Hastie  Iain Johnstone  Robert Tibshirani  et al. Least angle regression.

The Annals of statistics  32(2):407–499  2004.

[9] Botao Hao  Anru Zhang  and Guang Cheng. Sparse and low-rank tensor estimation via cubic

sketchings. arXiv preprint arXiv:1801.09326  2018.

[10] Ning Hao  Yang Feng  and Hao Helen Zhang. Model selection for high-dimensional quadratic
regression via regularization. Journal of the American Statistical Association  113(522):615–
625  2018.

[11] Ning Hao and Hao Helen Zhang. Interaction screening for ultrahigh-dimensional data. Journal

of the American Statistical Association  109(507):1285–1301  2014.

[12] Prateek Jain  Ambuj Tewari  and Purushottam Kar. On iterative hard thresholding methods for
high-dimensional m-estimation. In Advances in Neural Information Processing Systems  pages
685–693  2014.

[13] Murat Kocaoglu  Karthikeyan Shanmugam  Alexandros G Dimakis  and Adam Klivans. Sparse
In Advances in Neural Information Processing

polynomial learning and graph sketching.
Systems  pages 3122–3130  2014.

[14] Xingguo Li  Raman Arora  Han Liu  Jarvis Haupt  and Tuo Zhao. Nonconvex sparse learning via
stochastic optimization with progressive variance reduction. arXiv preprint arXiv:1605.02711 
2016.

[15] Yun Li  George T. O’Connor  Josée Dupuis  and Eric D. Kolaczyk. Modeling gene-covariate
interactions in sparse regression with group structure for genome-wide association studies.
Statistical applications in genetics and molecular biology  14 3:265–77  2015.

[16] Michael Lim and Trevor Hastie. Learning interactions via hierarchical group-lasso regulariza-

tion. Journal of Computational and Graphical Statistics  24(3):627–654  2015.

[17] Liu Liu  Yanyao Shen  Tianyang Li  and Constantine Caramanis. High dimensional robust

sparse regression. arXiv preprint arXiv:1805.11643  2018.

10

[18] Yishay Mansour. Randomized interpolation and approximation of sparse polynomials. SIAM

Journal on Computing  24(2):357–368  1995.

[19] Tomoya Murata and Taiji Suzuki. Sample efﬁcient stochastic gradient iterative hard thresholding
method for stochastic sparse linear regression with limited attribute observation. In Advances in
Neural Information Processing Systems  pages 5317–5326  2018.

[20] Nam Nguyen  Deanna Needell  and Tina Woolf. Linear convergence of stochastic itera-
tive greedy algorithms with sparse constraints. IEEE Transactions on Information Theory 
63(11):6869–6895  2017.

[21] Rasmus Pagh. Compressed matrix multiplication. ACM Transactions on Computation Theory

(TOCT)  5(3):9  2013.

[22] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2008.

[23] Steffen Rendle. Factorization machines. In 2010 IEEE International Conference on Data

Mining  pages 995–1000. IEEE  2010.

[24] John Shawe-Taylor  Nello Cristianini  et al. Kernel methods for pattern analysis. Cambridge

university press  2004.

[25] Yiyuan She  Zhifeng Wang  and He Jiang. Group regularized estimation under structural

hierarchy. Journal of the American Statistical Association  113(521):445–454  2018.

[26] Jie Shen and Ping Li. A tight bound of hard thresholding. The Journal of Machine Learning

Research  18(1):7650–7691  2017.

[27] Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum
inner product search (mips). In Advances in Neural Information Processing Systems  pages
2321–2329  2014.

[28] Michael Sipser and Daniel A Spielman. Expander codes. In Proceedings 35th Annual Sympo-

sium on Foundations of Computer Science  pages 566–576. IEEE  1994.

[29] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological)  58(1):267–288  1996.

[30] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[31] Ryan Williams. On the difference between closest  furthest  and orthogonal pairs: Nearly-linear
vs barely-subquadratic complexity. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms  pages 1207–1215. Society for Industrial and Applied
Mathematics  2018.

[32] Jing Wu  Bernie Devlin  Steven Ringquist  Massimo Trucco  and Kathryn Roeder. Screen
and clean: a tool for identifying interactions in genome-wide association studies. Genetic
Epidemiology: The Ofﬁcial Publication of the International Genetic Epidemiology Society 
34(3):275–285  2010.

[33] Hsiang-Fu Yu  Cho-Jui Hsieh  Qi Lei  and Inderjit S Dhillon. A greedy approach for budgeted
maximum inner product search. In Advances in Neural Information Processing Systems  pages
5453–5462  2017.

[34] Rose Yu and Yan Liu. Learning from multiway data: Simple and efﬁcient tensor regression. In

International Conference on Machine Learning  pages 373–381  2016.

[35] Kaiqing Zhang  Zhuoran Yang  and Zhaoran Wang. Nonlinear structured signal estimation
in high dimensions via iterative hard thresholding. In International Conference on Artiﬁcial
Intelligence and Statistics  pages 258–268  2018.

11

,Shuo Yang
Yanyao Shen
Sujay Sanghavi