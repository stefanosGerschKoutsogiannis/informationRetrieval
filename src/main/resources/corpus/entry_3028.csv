2018,Learning Versatile Filters for Efficient Convolutional Neural Networks,This paper introduces versatile filters to construct efficient convolutional neural network. Considering the demands of efficient deep learning techniques running on cost-effective hardware  a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways  e.g.  investigating small  sparse or binarized filters. In contrast  we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter. These secondary filters all inherit in the primary filter without occupying more storage  but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters  we additionally investigate versatile filters from the channel perspective. The new techniques are general to upgrade filters in existing CNNs. Experimental results on benchmark datasets and neural networks demonstrate that CNNs constructed with our versatile filters are able to achieve comparable accuracy as that of original filters  but require less memory and FLOPs.,Learning Versatile Filters for

Efﬁcient Convolutional Neural Networks

Yunhe Wang1  Chang Xu2  Chunjing Xu1  Chao Xu3  Dacheng Tao2

1 Huawei Noah’s Ark Lab

2 UBTECH Sydney AI Centre  SIT  FEIT  University of Sydney  Australia

3 Key Lab of Machine Perception (MOE)  Cooperative Medianet Innovation Center 

School of EECS  Peking University  Beijing  China

yunhe.wang@huawei.com  c.xu@sydney.edu.au  xuchunjing@huawei.com

xuchao@cis.pku.edu.cn  dacheng.tao@sydney.edu.au

Abstract

This paper introduces versatile ﬁlters to construct efﬁcient convolutional neural
network. Considering the demands of efﬁcient deep learning techniques running
on cost-effective hardware  a number of methods have been developed to learn
compact neural networks. Most of these works aim to slim down ﬁlters in different
ways  e.g. investigating small  sparse or binarized ﬁlters. In contrast  we treat
ﬁlters from an additive perspective. A series of secondary ﬁlters can be derived
from a primary ﬁlter. These secondary ﬁlters all inherit in the primary ﬁlter
without occupying more storage  but once been unfolded in computation they could
signiﬁcantly enhance the capability of the ﬁlter by integrating information extracted
from different receptive ﬁelds. Besides spatial versatile ﬁlters  we additionally
investigate versatile ﬁlters from the channel perspective. The new techniques are
general to upgrade ﬁlters in existing CNNs. Experimental results on benchmark
datasets and neural networks demonstrate that CNNs constructed with our versatile
ﬁlters are able to achieve comparable accuracy as that of original ﬁlters  but require
less memory and FLOPs.

1

Introduction

Considerable computer vision applications (e.g. image classiﬁcation [19]  object detection [15] 
subspace clustering [27]  and image segmentation [13]) have received remarkable progress with
the help of convolutional neural networks (CNNs) in last decade. Table 1 summarizes proﬁles of
benchmark CNNs on the ILSVRC 2012 dataset [17]. From the pioneering AlexNet [11] to the
recent ResNeXt-50 [25]  the storage of networks is slightly saved  but the classiﬁcation accuracy has
been continuously improved. This performance improvement comes from sophisticatedly designed
calculations introduced in these networks  e.g. residual modules in ResNet [7] and versatile modules
in GoogleNet [20]. These networks are widely used in the scenario of abundant computation and
storage resources  but they cannot easily adapt to mobile platforms  such as smartphones and cameras.
Taking ResNet-50 [7] with 54 convolutional layers as an example  about 97MB memory is required
to store all ﬁlters and over 4.0 × 109 times of ﬂoating number multiplications have to be operated for
an image.
Over the years  different techniques have been proposed to tackle the contradiction between resources
supply of low performance devices and demands of heavy neural networks. One common approach
is to explore and eliminate redundancy in pre-trained CNNs. For example  Han et.al. [6] discarded
subtle weights in convolution ﬁlters  Wang et.al. [23] investigated redundancy between weights 
Figurnov et.al. [5] removed redundant connections between input data and ﬁlters  Wang et.al. [22]

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Table 1: Properties of benchmark CNN models learned on the ILSVRC 2012 dataset.

Model

AlexNet [11]

VGGNet-16 [19]
GoogleNet [20]
ResNet-50 [7]

ResNeXt-50 [25]

Weights
6.1 × 107
13.8 × 107
0.7 × 107
2.6 × 107
2.5 × 107

Memory
232.5MB
526.4MB
26.3MB
97.2MB
95.3MB

FLOPs
0.7 × 109
15.4 × 109
1.5 × 109
4.1 × 109
4.2 × 109

Top1-err
42.9%
28.5%
34.2%
24.7%
22.6%

Top5-err
19.8%
9.9%
12.9%
7.8%
6.5%

explored compact feature maps for deep neural networks  and Wen et.al. [24] investigated the
sparsity from several aspects. There are also some methods to approximate the original neural
networks by employing more compact structures  e.g. quantization and binarization [1  14  3]  matrix
decomposition [4]  and teacher student learning paradigm [8  16]. Instead of patching pre-trained
CNNs  some highly efﬁcient network architectures have been designed for applications on mobile
devices. For example  ResNext [25] aggregated a set of transformations with the same topology 
Xception [2] and MobileNet [9] used separable convolutions with 1 × 1 ﬁlters  and ShufﬂeNet [26]
encouraged pointwise group convolutions and channel shufﬂe operations.
Most of these existing works learn efﬁcient CNNs through slimming down ﬁlters  e.g. making
great use of smaller ﬁlters (e.g. 1 × 1 ﬁlters) and developing various (e.g. sparse and low-rank)
approximation of ﬁlters. Given such lightweight ﬁlters  the network performance is struggling to keep
up  due to limited capacity of 1 × 1 ﬁlters or approximation error of ﬁlters. Rather then subtracting
(i.e. slimming down ﬁlters)  another thing to consider is adding. We must ask whether the value of a
normal ﬁlter has already been maximally explored and can a normal ﬁlter take more roles than usual.
In this paper  we propose versatile ﬁlters for efﬁcient convolutional neural networks. We produce
a series of smaller secondary ﬁlters from a primary ﬁlter based on some pre-deﬁned rules. These
secondary ﬁlters inherit weights from the primary ﬁlter  but they will have different receptive ﬁelds
and extract features from the spatial dimension. The neural network is composed of primary ﬁlters 
while the strength of the network will be fully disclosed through secondary ﬁlters in computation.
Speciﬁcally  we develop versatile ﬁlters in both spatial and channel dimensions. We provide detailed
feed-forward and back-propagation of the proposed versatile ﬁlters. Experiments on benchmarks
demonstrate that  equipping CNNs with our versatile ﬁlters can lead to lower memory usages and
FLOPs  but with comparable network accuracy.

2 Approach

In this section  we illustrate the design of versatile ﬁlters  which can be applied over any ﬁlter with
height and width greater than one. Besides spatial versatile ﬁlters  we additionally investigate versatile
ﬁlters from the channel perspective.

2.1 Spatial Versatile Filters
Consider the input data x ∈ RH×W×c  where H and W are height and width of the input data
respectively  and c is the channel number  i.e. the number of feature maps generated in the previous
layer. A convolution ﬁlter is denoted as f ∈ Rd×d×c  where d × d is the size of the convolution ﬁlter.
We focus on square ﬁlters  e.g. 5 × 5 and 3 × 3  which are most widely used in modern CNNs such
as ResNet [7]  VGGNet [19]  ResNeXt [25]  and ShufﬂeNet [26]. The conventional convolution can
be formulated as

(1)
is the output feature map of x  and H(cid:48) and W (cid:48)

where ∗ is the convolution operation  y ∈ RH(cid:48)×W (cid:48)
are its height and width  respectively.
Compared with traditional fully connected neural networks  one of the most important advantages
of CNNs is that the size (d × d) of ﬁlters in a convolutional layer can be much smaller than that
(H × W ) of the input. For example  7× 7 ﬁlters in the ﬁrst layer of ResNet-50 [7] are used to process
the 224 × 224 input. Fixing the output size  the complexity of ﬂoating number multiplications of a
ﬁlter in the fully-connected layer is O(cHW H(cid:48)W (cid:48))  while the complexity of a convolution ﬁlter is

y = f ∗ x 

2

Figure 1: An illustration of the proposed spatial versatile convolution ﬁlter. Given the input data (a) 
there are four sub-regions (b) covered by a 5× 5 convolution ﬁlter with stride 2  and their convolution
results are stacked into a feature map (c). In contrast  a spatial versatile ﬁlter will be applied three
times on each sub-region with different secondary ﬁlters  i.e.  5 × 5 blue  3 × 3 green  and 1 × 1 red
in (b) to generate three feature maps (d).

only O(cd2H(cid:48)W (cid:48)). In addition  convolution operations extract features from small regions  which is
beneﬁcial for subsequent tasks such as recognition and detection.
Receptive ﬁeld is an important concept introduced by convolutions. Larger receptive ﬁeld would
allow neurons to detect changes over a wider area  but result in a less precise perception. On the other
hand  smaller receptive ﬁeld would enable neurons to detect ﬁne details. It is therefore reasonable
to integrate neurons of larger receptive ﬁelds and smaller receptive ﬁelds to extract comprehensive
and accurate features. For example  versatile modules [20] introduce parallel paths with different
receptive ﬁeld sizes by making use of multiple ﬁlters with different sizes  e.g. 3 × 3 and 5 × 5
convolutions. Explicitly brining in ﬁlters of different sizes is a straightforward approach to process
the input information in different scales  but the signiﬁcant increase in storage of these ﬁlters could
be a new challenge. Most importantly  though ﬁlters of different sizes in the same layer have different
receptive ﬁelds  their receptive ﬁelds would have some overlap  which indicates the prospective
connections between their corresponding ﬁlters.
Taking f ∈ Rd×d as a primary ﬁlter  we propose to derive a series of secondary ﬁlters {f1  f2 ···   fs}
from f  where s = (cid:100)d/2(cid:101). To maximally explore the potential of primary ﬁlter f  each secondary
ﬁlters fi is directly inherited from f with a mask Mi 

if q  p ≥ i | p  q ≤ d + 1 − i 

otherwise 

Mi(p  q  c) =

(2)
and fi is calculated as fi = Mi ◦ f  where ◦ is the element-wise multiplication. More speciﬁcally  f1
is the ﬁlter f itself  f2 discards the outermost circle of parameters in f  and fs is the innermost circle
of parameters in f (i.e.  fs is a 1 × 1 ﬁlter given an odd d). Example secondary ﬁlters for a 5 × 5
ﬁlter can be seen in Figure 1 (b).
By concatenating convolution responses from these secondary ﬁlters  we get the feature map repre-
sented as

(cid:26) 1 

0 

y = [(M1 ◦ f ) ∗ x + b1  ...  (Ms ◦ f ) ∗ x + bs]  
i=1 ∈ {0  1}d×d×c 

s.t. s = (cid:100)d/2(cid:101)  {Mi}s

(3)

where b1  ...  bs are bias parameters.
By embedding Fcn. 3 into conventional CNNs  we can obtain convolution responses simultaneously
from s secondary ﬁlters of different receptive ﬁelds. The number of the output channels of the
proposed versatile ﬁlter is s times more than that of the original ﬁlter  and feature maps of a
convolutional layer using the proposed versatile ﬁlters contain features in different scales at the same
time.
Note that convolution operations (∗) in Fcn. 3 share the same stride and padding parameters for
the following two reasons: 1) dimensionalities of feature maps generated by secondary ﬁlters with

3

(a) Input data(b) Convolution operations(c) Original features(d) Spatial versatile featuresdifferent receptive ﬁelds have to be consistent for the subsequent calculation; 2) centers of these
secondary ﬁlters are the same  and the s-dimensional feature is thus a multi-scale representation of a
speciﬁc pixel at x. The schematic of the proposed versatile ﬁlters is shown in Fig. 1  and the detailed
back-propagation procedure of the proposed spatial versatile convolution ﬁlters can be found in the
supplementary materials.
Discussion: Besides the proposed method as shown in Fcn. 3  a naïve approach to aggregate features
from multiple secondary ﬁlters can be

s(cid:88)

y =

(Mi ◦ f ) ∗ x + b 

i=1

s.t. s = (cid:100)d/2(cid:101)  {Mi}s

i=1 ∈ {0  1}d×d×c 

(4)

which calculates the resulting feature map as a linear combination of features from different receptive
ﬁelds. Since the convolution ∗ is exactly an linear operation  the sum of different convolution
responses on the same input can be rewritten as the response of a combined convolution ﬁlter
employed on this data  i.e. 

s(cid:88)

s(cid:88)

y =

(Mi ◦ f ) ∗ x + b = [(

Mi) ◦ f ] ∗ x + b.

(5)

i=1

i=1

Therefore  Fcn. 4 is equivalent to adding a ﬁxed weight mask on conventional convolution ﬁlters 
which cannot produce more meaningful calculations and informative features in practice. We will
compare the performance of this naïve approach in experiments.

2.2 Analysis on Spatial Versatile Filters

Compared with original convolution ﬁlters  the proposed spatial versatile ﬁlters can provide more
feature maps without increasing the number of ﬁlters. Therefore  we further analyze the memory
usage and computation cost of neural networks using the proposed spatial versatile ﬁlters.
The proposed spatial versatile convolution operation as shown in Fcn. 3 can generate multiple feature
maps using a ﬁxed number of convolution ﬁlters. Thus the computational complexity and memory
usage of CNNs for extracting the same amount of features can be reduced signiﬁcantly as analyzed in
Proposition 1.
Proposition 1. Given a convolutional layer for extracting feature maps y ∈ RH(cid:48)×W (cid:48)×n using the
proposed spatial versatile ﬁlters (Fcn. 3)  the space complexity of d × d ﬁlters with c channels is

O(d2cn/s) and the computational complexity is O((cid:80)s

i=1(d − 2i + 2)2cH(cid:48)W n/s).

Proof. For the desired feature map y ∈ RH(cid:48)×W (cid:48)×n  where H(cid:48) and W (cid:48) are height and width of
y  respectively. Commonly  we need n convolution ﬁlters {fi}n
i=1 of size d × d × c. The space
complexity for storing these ﬁlters is O(d2cn)  and the computational complexity for generating y is
O(dcH(cid:48)W (cid:48)n).
In contrast  the proposed spatial versatile convolution operation can extract s = (cid:100)di/2(cid:101) sets of feature
maps simultaneously. Thus  for generating N feature maps  the space complexity for storing the
proposed spatial versatile convolution ﬁlters is

O(d2cn/s).

(6)
The computational complexity for generating feature maps using the proposed spatial versatile ﬁlters
in different scales is various  which affects by the size of convolution ﬁlters  i.e.  the number of
non-zero elements in each Mi. The number of non-zero elements in Mi is (d − 2i + 2)2 as shown in
Fcn. 2  thus the computational complexity for the i-th scale is O((d−2i+2)2cH(cid:48)W (cid:48)n/s). Therefore 
the computational complexity of the entire layer can be calculated as:

s(cid:88)

O(

(d − 2i + 2)2cH(cid:48)W (cid:48)n/s) 

(7)

which is deﬁnitely smaller than that O(d2cH(cid:48)W (cid:48)n) of the traditional convolution operation when
s > 2.

i=1

4

Figure 2: An illustration of the proposed channel versatile ﬁlters. The original ﬁlter can only generate
only one feature map for the given input data  and the proposed method can provide multiple feature
maps simultaneously according to the channel stride parameters. Each color represents a secondary
ﬁlter and its corresponding feature map.

2.3 Channel Versatile Filters

A spatial versatile ﬁlter was proposed in Fcn. 3  which generates a series of secondary convolution
ﬁlters by adjusting the height and width of a given convolution ﬁlter. However  there are still obvious
redundancy in these secondary ﬁlters  i.e.  the number of channels of each convolution ﬁlter is much
larger than its height and width. In addition  given 1 × 1 primary ﬁlters  Fcn. 3 will be reduced to
Fcn. 1 for conventional convolution operation. Considering the wide use of 1 × 1 ﬁlters in modern
CNN architectures such as ShufﬂeNet [26] and ResNeXt [25]  etc.  we proceed to develop versatile
ﬁlters from the channel perspective.
The most important property of convolution ﬁlters is that their weights are shared by the input data. A
convolution ﬁlter used to have the same depth as the input data  and slide along the width and height
of the input data with some stride parameters. If the depth of the input is 512  a 1 × 1 × 512 ﬁlter has
to take a large number of ﬂoating number multiplications to weight different channels and integrate
the information across different input channels. However  this coarse information summarization
over all channels is difﬁcult to highlight characters of individual channels  especially when there are
extremely many channels. Hence  we deﬁne secondary ﬁlters for original convolution ﬁlters with the
help of channel stride  i.e.

y = [f1 ∗ x + b1  f2 ∗ x + b2 ···   fn ∗ x + bn]  
s.t. ∀ i  fi ∈ Rd×d×c  n = (c − ˆc)/g + 1.

(8)

where g is the channel stride parameter and ˆc < c is the number of non-zero channels of secondary
ﬁlters. fi is the i-th unduplicated copy of primary ﬁlter f given the length ˆc and the stride g. Therefore 
a ﬁlter will be used n times simultaneously to generate more feature maps by introducing Fcn. 8.
Example secondary ﬁlters using the proposed channel stride approach are given in Figure 2. In
addition  the proposes channel versatile ﬁlters can also signiﬁcantly reduce the memory usage and
computational complexity of CNNs  which can be similarly derived as that in Proposition 1.

3 Experiments

In this section  we will implement experiments to validate the effectiveness of the proposed multi-
scale convolution ﬁlter on several benchmark image datasets  including MNIST [12]  ImageNet
(ILSVRC 2012 [17])  etc. Experimental results will be analyzed to further understand the beneﬁts of
the proposed approach.

3.1 Experiments on MNIST

The MNIST dataset consists of 70  000 images drawn from ten categories  which is split into 60  000
training and 10  000 testing images. Each sample in this dataset is a 28 × 28 gray-scale digit (from 0
to 9) image. In addition  the last 10  000 images in the training set is selected as the validation set for
determining the ﬁnal model.

5

(a) Original convolution(b) Channel versatile filtersSpatial versatile ﬁlters: We ﬁrst tested the performance of the proposed spatial versatile ﬁlter in
Fcn. 3 using a LeNet for classifying the MNIST dataset learned on MatConvNet [21]. The baseline
model has four convolutional layers of size 5 × 5 × 1 × 20  5 × 5 × 20 × 50  4 × 4 × 50 × 500 
and 1 × 1 × 500 × 10  respectively  which accounts about 1.6MB (ﬁlters are stored in 32-bit ﬂoating
values)  and the accuracy is 99.20%. Then  several models with different architectures and strategies
were trained  and their results are show in Table 2. Wherein  memory usage of convolution ﬁlters and
ﬂoating number multiplications (FLOPs) of each model are also provided.
Versatile-Model 1 is the network using the proposed versatile ﬁlters (Fcn. 4) with the same architecture
as that of the baseline model. Since it does not change the size of output data  its memory usage and
multiplications are also the same as those of the baseline model. Not surprisingly  there is not any
performance enhancement by exploiting this approach since the network can adjust parameters in

convolution ﬁlters according to the weight mask ((cid:80)s

i=1 Mi).

Table 2: The performance of the proposed spatial versatile ﬁlters on MNIST.

Model
Baseline

Versatile-Model 1
Versatile-Model 2
Versatile-Model 3

Weights
4.3 × 105
4.3 × 105
2.2 × 105
2.2 × 105

Memory
1681.6KB
1681.6KB
852.0KB
852.0KB

FLOPs
22.93×105
22.93×105
12.00×105
12.00×105

Accuracy
99.20%
99.20%
99.15%
99.22%

Versatile-Model 2 and Versatile-Model 3 adopted the proposed versatile convolution operation as
shown in Fcn. 3. There are multiple bias term b1  ...  bs in Fcn. 3 for controlling features generated
by different secondary ﬁlters from an versatile convolution ﬁlter. The difference between Model 2
and Model 3 is that  bias term of convolution ﬁlters in Model 3 are shared  i.e.  b1 =  ...  = bs  and
gradients of b are also averaged.
The proposed method can generate multiple feature maps using a convolution ﬁlter whose size is
larger than 2 × 2 (i.e.  s = (cid:100)di/2(cid:101) > 1)  which will increase the number of channels in the next
layer and make the convolutional neural network enormous. Therefore  we reduce the number
of convolution ﬁlters in each layer to make the amount of feature maps in Versatile-Model 2 and
Versatile-Model 3 similar to that in the original network  as shown in Table 2. For example  numbers
of ﬁlters in the ﬁrst convolutional layer of the base Model 3 are 20 and 7  respectively. However  their
output channels are 20 and 21  respectively  since a spatial versatile ﬁlter with size 5 × 5 can produce
three channels outputs simultaneously.
It can be found in Table 2  Model 3 obtained a higher result (99.22%) with this strategy  which is
slightly higher than that of the baseline model. The reason is that if differences between bias terms
are extremely large  gradients of secondary convolution ﬁlters will be fundamentally different  which
makes the training of entire convolution ﬁlters difﬁcult. The performance of Model 3 is slightly
higher than that of the baseline model  but has signiﬁcantly lower memory usage and FLOPs  which
demonstrates the effectiveness of the proposed versatile convolution ﬁlters. In addition  the detailed
structure of Versatile-Model 3 in Table 2 and the corresponding demo code for verifying the proposed
method can be found in our supplementary materials.
Filter Visualization: Convolution ﬁlters are used for extracting intrinsic information from natural
images. Thus  these ﬁlters often present some speciﬁc structures  such as line  blob  etc. However  the
proposed versatile convolution ﬁlters adopt a more complex approach to capture useful information
from input images  i.e.  a large ﬁlter consists of a series of smaller ﬁlters and each of them will employ
on the input images to generate feature maps. Therefore  it is necessary to visualize and compare
ﬁlters in original CNN and the network using the proposed versatile convolution ﬁlters for having an
explicit illustration.
Fig. 3 illustrates convolution ﬁlters in the ﬁrst layer of the Baseline and Model 3 in Table 2  respec-
tively. Since the proposed approach is fundamentally different to the original convolution ﬁlters 
ﬁlters using Fcn. 3 present more complex structures. Speciﬁcally  each 3× 3 area in Fig. 3 (b) still can
be seen as an independent convolution ﬁlter with complex structure and obvious magnitude change.
In contrast  some 3 × 3 areas in Fig. 3 (a) are extremely smooth  which cannot provide distinctive
information.

6

(a) Original convolution ﬁlters.

(b) Versatile convolution ﬁlters.

Figure 3: Visualization of example ﬁlters learned on MNIST.

Channel versatile ﬁlters: After investigating the effectiveness of the proposed spatial versatile
convolution operation  we shall further test the performance of the proposed channel versatile ﬁlters
as described in Fcn. 8  namely versatile v2. Note that  for the ﬁrst layer and the last layer in neural
networks  we do not apply the channel stride approach  since the input channel of the ﬁrst layer is
usually very small and the output channel of the last layer is exactly the number of ground-truth
labels.
There are two important parameters in Fcn. 8  i.e.  the number of channels ˆc of the convolution ﬁlter
ˆf and the stride g. We then established three models using the proposed versatile ﬁlter with different
ˆc and g  and trained them on the MNIST dataset as detailed in Table. 3.

Table 3: The performance of the proposed channel versatile ﬁlters on MNIST.

Model
Baseline

Versatile v2-Model 1
Versatile v2-Model 2
Versatile v2-Model 3

c − ˆc

-
1
1
2

g Weights
4.3 × 105
-
1.18 × 105
1
1.18 × 105
2
0.79 × 105
1

Memory
1681.6KB
460.5KB
460.5KB
309.1KB

FLOPs
22.93×105
12.17×105
11.17×105
12.12×105

Accuracy
99.20%
99.18%
99.15%
99.07%

As mentioned above  the channel versatile ﬁlters can reduce the number of convolution ﬁlters by
a factor of n = (c − ˆc)/g + 1  therefore  when we set ˆc − c = 1 and g = 1  we can reduce about
half convolution ﬁlters and maintain the similar amount of feature maps. For example  the size of
the second layer’s convolution ﬁlter in Versatile-Model 3 is 5 × 5 × 21 × 17  and the size of the
second layer’s convolution ﬁlter in Versatile v2-Model 1 is 5 × 5 × 21 × 9. As a result  the Versatile
v2-Model 1 achieved a 99.18 accuracy  which is slightly lower than that of the baseline model  but its
memory usage and FLOPs have been reduced signiﬁcantly.
Similarly  when we set ˆc − c = 1 and g = 2 (i.e.  Versatile v2-Model 2)  the network obtained
similar results to those of Versatile v2-Model 1. Furthermore  when ˆc − c = 2 and g = 1 in Versatile
v2-Model 3  the number of convolution ﬁlters will be further reduced. However  since the number of
ﬁlters is very small  the representability of this network is also lower. The Versatile v2-Model 3 with
the smallest memory usage and FLOPs obtained a 99.07% classiﬁcation accuracy. Therefore  we set
ˆc − c = 1 and g = 1 in the following experiments for having a best trade-off.

3.2 Large Scale Visual Recognition Experiments

Experiments in the above chapter show that the proposed spatial versatile ﬁlters in Fcn. 3 and the
channel versatile ﬁlters are able to replace the traditional convolution operation on the MNIST dataset.
We next employed the proposed method on an extremely large image dataset  namely ImageNet
ILSVRC 2012 dataset [17]  which contains over 1.2M training images and 50k validation images.
Three baseline architectures  AlexNet [11]  ResNet-50 [7] and ResNeXt-50 [25]  were selected for
conducting the following experiments. Note that  all training settings such as weight decay and
learning rate used the default setting to ensure fair comparisons.
AlexNet: AlexNet is one of the most classical deep CNN models for large scale visual recognition 
which has over 230MB parameters and a 80.2% accuracy on the ImageNet dataset with 1000 different
categories. This network has 8 convolutional layers  sizes of convolution ﬁlters in the ﬁrst six layers

7

are larger than 1× 1  i.e.  11× 11× 3× 96  5× 5× 48× 256  3× 3× 256× 384  3× 3× 192× 384 
3 × 3 × 192 × 256  and 6 × 6 × 256 × 4096.
Since sizes of convolution ﬁlters used in this network are much larger than that in other networks 
resources required by this network can be signiﬁcantly reduced by exploiting the proposed versatile
convolution operation. For example  the parameter for the ﬁrst convolutional layer is s1 = (cid:100)11/2(cid:101) =
6  thus the number of parameters in this layer with versatile convolution ﬁlters is only 11×11×3×16.
In this manner  we established a new network (Versatile-AlexNet in Table 4) and reduced the number
of ﬁlters in each convolutional layer according to its versatile parameter s. Speciﬁcally  sizes of
convolution ﬁlters in its ﬁrst six convolutional layers are 11 × 11 × 3 × 16  5 × 5 × 48 × 86 
3 × 3 × 258 × 192  3 × 3 × 192 × 192  3 × 3 × 192 × 128  and 6 × 6 × 256 × 1366  respectively.
After training the network on the ImageNet dataset  Versatile-AlexNet using Fcn. 3 obtained a 19.5%
top5-err and a 42.1% top1-err  which are better than those of the baseline model. The memory usage
of ﬁlters was reduced by a factor of 1.76×  and the FLOPs in Versatile-AlexNet is 1.95× less than
that in the baseline model.
Furthermore  we applied the channel versatile ﬁlters (Fcn. 8) on the Versatile-AlexNet model with
ˆc − c = 1  and g = 1  namely  Versatile v2-AlexNet. In this manner  the number of convolution layer
in each layer will be reduced by a factor of 1
2. As a result  this network achieved a 20.7% top5-err 
which is slightly higher than that of the baseline model. But  the memory usage of the entire network
is only 73.6MB  which is only about 30% to that of the baseline model.

Table 4: Statistics for versatile ﬁlters on the ImageNet 2012 dataset.

Model

AlexNet [11]

Versatile-AlexNet

Versatile v2-AlexNet

ResNet-50 [7]

Versatile-ResNet-50

Versatile v2-ResNet-50

ResNeXt-50 [25]

Versatile v2-ResNeXt-50

Weights Memory
6.1 × 107
232.5MB
3.5 × 107
131.8MB
1.9 × 107
73.7MB
2.6 × 107
97.2MB
1.9 × 107
75.6MB
1.1 × 107
41.7MB
2.5 × 107
95.3MB
1.3 × 107
50.0MB

FLOPs
0.7 × 109
0.4 × 109
0.4 × 109
4.1 × 109
3.2 × 109
3.0 × 109
4.2 × 109
4.0 × 109

Top1err
42.9%
42.1%
44.1%
24.7%
24.5%
25.5%
22.6%
23.8%

Top5err
19.8%
19.5%
20.7%
7.8%
7.6%
8.2%
6.5%
7.0%

ResNets: To further illustrate the superiority of the proposed scheme  we then employed it on the
ResNet-50 model. Although there are many layers with 1 × 1 ﬁlters in this network  it also has a
lot of convolutional layers with large ﬁlters  e.g. 3 × 3 and 7 × 7  which accounts for about half
memory usage of the entire network. In addition  ResNets introduce shortcut operations which also
provides considerable versatile features since receptive ﬁelds of neurons in different layer are various
as discussed in [18]. Therefore  it is meaningful to investigate the functionality of the versatile
convolution ﬁlters on this network.
Similarly  we reset the original convolutional layers with the proposed versatile convolution ﬁlters.
For instance  a convolutional layer of size 3 × 3 × 64 × 128 will be converted into a new layer of size
3 × 3 × 64 × 32 using the proposed versatile convolution ﬁlters. The performance of the original
ResNet-50 and the network using versatile ﬁlters were detailed in Table 4.
As mentioned above  there are still considerable ﬁlters in the ResNet whose sizes are larger than 1× 1.
Thus  its memory usage and FLOPs were reduced obviously by exploiting the proposed versatile
convolution ﬁlters. Versatile-ResNet-50 with the same amount feature maps achieved a 7.6% top-5
accuracy  which is slightly lower than that of the baseline models with only 75.6MB and 3.2 × 109
FLOPs.
In addition  Versatile v2-ResNet-50 with the same amount feature maps achieved a 8.2% top-5
accuracy  which is slightly higher than that of the baseline models. Its memory usage is only about
41.7MB  which is only about 1
2 to that of the original network. Therefore  our Versatile v2-ResNet-50 
which is a more portable alternative to the original ResNet-50 model.
Moreover  we attempted to replace original convolution ﬁlters by the proposed versatile convolution
ﬁlters in ResNeXt-50. This network is an enhanced version of ResNet-50  which divides convolutional
layers into several smaller groups and achieves a higher performance  and avoids larger convolution

8

ﬁlters. Since more than 90% convolution ﬁlters in this network are 1 × 1 ﬁlters  Fcn. 3 cannot obtain
an obvious enhancement. However  the proposed channel versatile scheme in Fcn. 8 can effectively
reduce the number of massive 1 × 1 convolution ﬁlters. Thus  we directly applied the versatile
v2 convolution ﬁlters with channel stride approach on it. After applying the proposed versatile
convolution ﬁlter on the ResNeXt-50 model  we obtained a 7.0% top-5 classiﬁcation error rate  which
is slightly higher than its baseline model with only about half memory usage. The detailed statistics
of the Versatile v2-ResNeXt-50 using the proposed versatile ﬁlters were also shown in Table 4.
Comparing Versatile v2-ResNet-50 and Versatile v2-ResNeXt-50  we found that the memory usage
of Versatile v2-ResNet-50 is lower than that of the Versatile v2-ResNeXt-50. This is because the
proposed versatile ﬁlters can effectively reduce the memory and FLOPs of ﬁlters whose sizes are
larger than 1 × 1  which provides a more ﬂexible way for designing CNNs with high performance
and portable architectures.

3.3 Comparing with Portable Architectures

Besides sophisticate CNNs such as AlexNet and ResNet-50 with heavy architectures  a variety of
recent works attempt to design neural networks with portable architectures and comparable perfor-
mance. MobileNet [9] utilized separable convolution to reduce memory usage and computational
cost of massive large convolution ﬁlters. ShufﬂeNet [26] further proposed shufﬂe operation to mixed
features in different groups and achieved higher results.

Table 5: An overall comparison of state-of-the-art portable CNNs on the ILSVRC2012 dataset.

Model

1.0 MobileNet-224 [9]
ShufﬂeNet 2× (g = 3) [26]

Versatile v2- ShufﬂeNet 2× (g = 3)

Weights Memory
0.4 × 107
16.0MB
0.7 × 107
20.6MB
0.4 × 107
14.0MB

FLOPs
0.5 × 109
0.5 × 109
0.5 × 109

Top1err
29.4%
26.3%
27.6%

Table 5 summarizes state-of-the-art CNN architectures  including their memory usages  FLOPs  and
recognition results on the ILSVRC 2012 dataset. Obviously  MobileNet has the smallest model size
and FLOPs  but its classiﬁcation accuracy is lower than those of other networks. ShufﬂeNet with the
similar FLOPs to that of the MobileNet achieves a higher accuracy  with a slightly higher memory
usage. By exploiting the proposed versatile convolution ﬁlters on the ShufﬂeNet 2× (g = 3)  we
reduced more than 30% weights of convolution ﬁlters and achieved the smallest model size with a
comparable accuracy  which is a more portable convolutional neural network.
In addition  to investigate the generalization ability of the proposed versatile convolution ﬁlter  we
further employed it on the single image super-resolution experiment. We selected the VDSR (Very
Deep CNN for Image Super-resolution [10]) as the baseline model and the Versatile-VDSR with the
same amount of feature maps but less memory usage and FLOPs achieved a higher performance.
Detailed experiments and analysis can be found in the supplementary materials.

4 Conclusions and Discussions

Exploring convolutional neural networks with low memory usage and computational complexity is
very essential so that these models can be used on mobile devices. In fact  the main waste in a general
neural network is that a convolution ﬁlter with massive parameters can only produce one feature for a
given data. In order to make full use of convolution ﬁlters  this paper proposes versatile convolution
ﬁlters from spatial and channel perspectives. Thus  we can use fewer parameters to generate the same
amount of useful features with a lower computational complexity at the same time. Experiments
conducted on benchmark image datasets and models show that the proposed method can not only
reduce the requirement of storage and computational resources  but also enhance performance of
CNNs  which is very effective for establishing portable CNNs with high accuracies. In addition 
the proposed method can be easily implemented using the existing convolution component  we will
further embed it into other applications such as object detection and image segmentation.
Acknowledgments: This work was supported in part by the ARC DE-180101438  FL-170100117 
DP-180103424  and NSFC under Grant 61876007  61872012. We also thank Huawei Hisilicon for
their technical supports.

9

References
[1] Sanjeev Arora  Aditya Bhaskara  Rong Ge  and Tengyu Ma. Provable bounds for learning some deep

representations. ICML  2014.

[2] François Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint

arXiv:1610.02357  2016.

[3] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights and

activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830  2016.

[4] Emily L Denton  Wojciech Zaremba  Joan Bruna  Yann LeCun  and Rob Fergus. Exploiting linear structure

within convolutional networks for efﬁcient evaluation. In NIPS  2014.

[5] Michael Figurnov  Dmitry Vetrov  and Pushmeet Kohli. Perforatedcnns: Acceleration through elimination

of redundant convolutions. NIPS  2016.

[6] Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural networks with

pruning  trained quantization and huffman coding. In ICLR  2016.

[7] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In CVPR  2016.

[8] Geoffrey Hinton  Oriol Vinyals  and Jeff Dean. Distilling the knowledge in a neural network. arXiv

preprint arXiv:1503.02531  2015.

[9] Andrew G Howard  Menglong Zhu  Bo Chen  Dmitry Kalenichenko  Weijun Wang  Tobias Weyand  Marco
Andreetto  and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861  2017.

[10] Jiwon Kim  Jung Kwon Lee  and Kyoung Mu Lee. Accurate image super-resolution using very deep

convolutional networks. In CVPR  2016.

[11] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS  2012.

[12] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[13] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic segmenta-

tion. In CVPR  2015.

[14] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet classiﬁca-

tion using binary convolutional neural networks. arXiv preprint arXiv:1603.05279  2016.

[15] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object detection

with region proposal networks. In NIPS  2015.

[16] Adriana Romero  Nicolas Ballas  Samira Ebrahimi Kahou  Antoine Chassang  Carlo Gatta  and Yoshua

Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550  2014.

[17] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. Imagenet large scale visual recognition challenge.
IJCV  115(3):211–252  2015.

[18] Pierre Sermanet and Yann LeCun. Trafﬁc sign recognition with multi-scale convolutional networks. In

IJCNN  2011.

[19] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. ICLR  2015.

[20] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru

Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. In CVPR  2015.

[21] Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional neural networks for matlab. In Proceedings

of the 23rd Annual ACM Conference on Multimedia Conference  2015.

[22] Yunhe Wang  Chang Xu  Dacheng Tao  and Chao Xu. Beyond ﬁlters: Compact feature map for portable

deep model. In ICML  2017.

10

[23] Yunhe Wang  Chang Xu  Shan You  Dacheng Tao  and Chao Xu. Cnnpack: Packing convolutional neural

networks in the frequency domain. In NIPS  2016.

[24] Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Learning structured sparsity in deep

neural networks. In NIPS  2016.

[25] Saining Xie  Ross Girshick  Piotr Dollár  Zhuowen Tu  and Kaiming He. Aggregated residual transforma-

tions for deep neural networks. In CVPR  2017.

[26] Xiangyu Zhang  Xinyu Zhou  Mengxiao Lin  and Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional

neural network for mobile devices. arXiv preprint arXiv:1707.01083  2017.

[27] Pan Zhou  Yunqing Hou  and Jiashi Feng. Deep adversarial subspace clustering. In CVPR  2018.

11

,Quanquan Gu
Zhaoran Wang
Han Liu
Rudy Bunel
Alban Desmaison
Pawan Mudigonda
Pushmeet Kohli
Philip Torr
Yunhe Wang
Chang Xu
Chunjing XU
Chao Xu
Dacheng Tao