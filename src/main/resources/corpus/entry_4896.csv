2012,Scaled Gradients on Grassmann Manifolds for Matrix Completion,This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices  while maintaining established global convegence and exact recovery guarantees. A connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established. The proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.,Scaled Gradients on Grassmann Manifolds

for Matrix Completion

Thanh T. Ngo and Yousef Saad

Department of Computer Science and Engineering

University of Minnesota  Twin Cities

Minneapolis  MN 55455

thango@cs.umn.edu  saad@cs.umn.edu

Abstract

This paper describes gradient methods based on a scaled metric on the Grassmann
manifold for low-rank matrix completion. The proposed methods signiﬁcantly
improve canonical gradient methods  especially on ill-conditioned matrices  while
maintaining established global convegence and exact recovery guarantees. A con-
nection between a form of subspace iteration for matrix completion and the scaled
gradient descent procedure is also established. The proposed conjugate gradient
method based on the scaled gradient outperforms several existing algorithms for
matrix completion and is competitive with recently proposed methods.

1

Introduction

Let A ∈ Rm×n be a rank-r matrix  where r ≪ m  n. The matrix completion problem is to re-
construct A given a subset of entries of A. This problem has attracted much attention recently
[8  14  13  18  21] because of its broad applications  e.g.  in recommender systems  structure from
motion  and multitask learning (see e.g. [19  9  2]).

1.1 Related work

Let Ω = {(i  j)|Aij is observed}. We deﬁne PΩ(A) ∈ Rm×n to be the projection of A onto the
observed entries Ω: PΩ(A)ij = Aij if (i  j) ∈ Ω and PΩ(A)ij = 0 otherwise.
If the rank is
unknown and there is no noise  the problem can be formulated as:

Minimize rank (X) subject to PΩ(X) = PΩ(A).

(1)
Rank minimization is NP-hard and so work has been done to solve a convex relaxation of it by
approximating the rank by the nuclear norm. Under some conditions  the solution of the relaxed
problem can be shown to be the exact solution of the rank minimization problem with overwhelming
probability [8  18]. Usually  algorithms to minimize the nuclear norm iteratively use the Singular
Value Decomposition (SVD)  speciﬁcally the singular value thresholding operator [7  15  17]  which
makes them expensive.

If the rank is known  we can formulate the matrix completion problem as follows:

Find matrix X to minimize ||PΩ(X) − PΩ(A)||F subject to rank (X) = r.

(2)
Keshavan et al. [14] have proved that exact recovery can be obtained with high probability by solv-
ing a non-convex optimization problem. A number of algorithms based on non-convex formulation
use the framework of optimization on matrix manifolds [14  22  6]. Keshavan et al. [14] propose
a steepest descent procedure on the product of Grassmann manifolds of r-dimensional subspaces.
Vandereycken [22] discusses a conjugate gradient algorithm on the Riemann manifold of rank-r ma-
trices. Boumal and Absil [6] consider a trust region method on the Grassmann manifold. Although

1

they do not solve an optimization problem on the matrix manifold  Wei et al. [23] perform a low rank
matrix factorization based on a successive over-relaxation iteration. Also  Srebro and Jaakkola [21]
discuss SVD-EM  one of the early ﬁxed-rank methods using truncated singular value decomposition
iteratively. Dai et al. [10] recently propose an interesting approach that does not use the Frobenius
norm of the residual as the objective function but instead uses the consistency between the current
estimate of the column space (or row space) and the observed entries. Guaranteed performance for
this method has been established for rank-1 matrices.

In this paper  we will focus on the case when the rank r is known and solve problem (2). In fact 
even when the rank is unknown  the sparse matrix which consists of observed entries can give us a
very good approximation of the rank based on its singular spectrum [14]. Also  a few values of the
rank can be used and the best one is selected. Moreover  the singular spectrum is revealed during
the iterations  so many ﬁxed rank methods can also be adapted to ﬁnd the rank of the matrix.

1.2 Our contribution

OptSpace [14] is an efﬁcient algorithm for low-rank matrix completion with global convergence and
exact recovery guarantees. We propose using a non-canonical metric on the Grassmann manifold to
improve OptSpace while maintaining its appealing properties. The non-canonical metric introduces
a scaling factor to the gradient of the objective function which can be interpreted as an adaptive
preconditioner for the matrix completion problem. The gradient descent procedure using the scaled
gradient is related to a form of subspace iteration for matrix completion. Each iteration of the
subspace iteration is inexpensive and the procedure converges very rapidly. The connection between
the two methods leads to some improvements and to efﬁcient implementations for both of them.
Throughout the paper  AΩ will be a shorthand for PΩ(A) and qf(U ) is the Q factor in the QR
factorization of U which gives an orthonormal basis for span (U ). Also  P ¯Ω(.) denotes the projection
onto the negation of Ω.

2 Subspace iteration for incomplete matrices

We begin with a form of subspace iteration for matrix completion depicted in Algorithm 1. If the

Algorithm 1 SUBSPACE ITERATION FOR INCOMPLETE MATRICES.
Input: Matrix AΩ  Ω  and the rank r.
Output: Left and right dominant subspaces U and V and associated singular values.
1: [U0  Σ0  V0] = svd(AΩ  r)  S0 = Σ0;
2: for i = 0 1 2 ... do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for

Xi+1 = P ¯Ω(UiSiV T
Ui+1 = Xi+1Vi; Vi+1 = X T
i+1Ui+1
Ui+1 = qf(Ui+1); Vi+1 = qf(Vi+1)
Si+1 = U T
if condition then

// Initialize U   V and Σ

i ) + AΩ

i+1Xi+1Vi+1

// Obtain new estimate of A
// Update subspaces
// Re-orthogonalize bases
// Compute new S for next estimate of A

// Diagonalize S to obtain current estimate of singular vectors and values
[RU   Σi+1  RV ] = svd(Si+1); Ui+1 = Ui+1RU ; Vi+1 = Vi+1RV ; Si+1 = Σi+1.

end if

matrix A is fully observed  U and V can be randomly initialized  line 3 is not needed and in lines
4 and 6 we use A instead of Xi+1 to update the subspaces. In this case  we have the classical two-
sided subspace iteration for singular value decomposition. Lines 6-9 correspond to a Rayleigh-Ritz
projection to obtain current approximations of singular vectors and singular values. It is known that
if the initial columns of U and V are not orthogonal to any of the ﬁrst r left and right singular vectors
of A respectively  the algorithm converges to the dominant subspaces of A [20  Theorem 5.1].
Back to the case when the matrix A is not fully observed  the basic idea of Algorithm 1 is to use
an approximation of A in each iteration to update the subspaces U and V and then from the new U
and V   we can obtain a better approximation of A for the next iteration. Line 3 is to compute a new
estimate of A by replacing all entries of UiSiV T
at the known positions by the true values in A.
i
The update in line 6 is to get the new Si+1 based on recently computed subspaces. Diagonalizing

2

Si+1 (lines 7-10) is optional for matrix completion. This step provides current approximations
of the singular values which could be useful for several purposes such as in regularization or for
convergence test. This comes with very little additional overhead  since Si+1 is a small r × r matrix.
Each iteration of Algorithm 1 can be seen as an approximation of an iteration of SVD-EM where a
few matrix multiplications are used to update U and V instead of using a truncated SVD to compute
the dominant subspaces of Xi+1. Recall that computing an SVD  e.g. by a Lanczos type procedure 
requires several  possibly a large number of  matrix multiplications of this type.

We now discuss efﬁcient implementations of Algorithm 1 and modiﬁcations to speed-up its conver-
gence. First  the explicit computation of Xi+1 in line 3 is not needed. Let ˆXi = UiSiV T
. Then
i ) + AΩ = ˆXi + Ei  where Ei = PΩ(A − ˆXi) is a sparse matrix of errors at
Xi+1 = P ¯Ω(UiSiV T
known entries which can be computed efﬁciently by exploiting the structure of ˆXi. Assume that each
Si is not singular (the non-singularity of Si will be discussed in Section 4). Then if we post-multiply
the update of U in line 4 by S−1

  the subspace remains the same  and the update becomes:

i

i

Ui+1 = Xi+1ViS−1

i = ( ˆXi + Ei)ViS−1

i = Ui + EiViS−1

i

 

(3)

The update of V can also be efﬁciently implemented. Here  we make a slight change  namely
i+1Ui (Ui instead of Ui+1). We observe that the convergence speed remains roughly the
Vi+1 = X T
same (when A is fully observed  the algorithm is a slower version of subspace iteration where the
convergence rate is halved). With this change  we can derive an update to V that is similar to (3) 

Vi+1 = Vi + ET

i UiS−T

i

 

(4)

We will point out in Section 3 that the updating terms EiViS−1
are related to the
gradients of a matrix completion objective function on the Grassmann manifold. As a result  to
improve the convergence speed  we can add an adaptive step size ti to the process  as follows:

i UiS−T

and ET

i

i

Ui+1 = Ui + tiEiViS−1

i

and Vi+1 = Vi + tiET

i UiS−T

i

.

This is equivalent to using ˆXi + tiEi as the estimate of A in each iteration. The step size can be
computed using a heuristic adapted from [23]. Initially  t is set to some initial value t0 (t0 = 1 in
our experiments). If the error kEikF decreases compared to the previous step  t is increased by a
factor α. Conversely  if the error increases  indicating that the step is too big  t is reset to t = t0.
The matrix Si+1 can be computed efﬁciently by exploiting low-rank structures and the sparsity.

Si+1 = (U T

i+1Ui)Si(V T

i Vi+1) + tiU T

i+1EiVi+1

(5)

There are also other ways to obtain Si+1 once Ui+1 and Vi+1 are determined to improve the current
approximation of A . For example we can solve the following quadratic program [14]:

Si+1 = argminSkPΩ(A − Ui+1SV T

i+1)k2

F

(6)

We summarize the discussion in Algorithm 2. A sufﬁciently small error kEikF can be used as a
Algorithm 2 GENERIC SUBSPACE ITERATION FOR INCOMPLETE MATRICES.
Input: Matrix AΩ  Ω  and number r.
Output: Left and right dominant subspaces U and V and associated singular values.
1: Initialize orthonormal matrices U0 ∈ Rm×r and V0 ∈ Rn×r.
2: for i = 0 1 2 ... do
3:
4:
5:
6:
7: end for

Compute Ei and appropriate step size ti
Ui+1 = Ui + tiEiViS−1
Orthonormalize Ui+1 and Vi+1
Find Si+1 such that PΩ(Ui+1Si+1V T

i+1) is close to AΩ (e.g. via (5)  (6)).

and Vi+1 = Vi + tiET

i UiS−T

i

i

stoppping criterion. Algorithm 1 can be shown to be equivalent to LMaFit algorithm proposed in
[23]. The authors in [23] also obtain results on local convergence of LMaFit. We will pursue a
different approach here. The updates (3) and (4) are reminiscent of the gradient descent steps for
minimizing matrix completion error on the Grassmann manifold that is introduced in [14] and the
next section discusses the connection to optimization on the Grassmann manifold.

3

3 Optimization on the Grassmann manifold

In this section  we show that using a non-canonical Riemann metric on the Grassmann manifold 
the gradient of the same objective function in [14] is of a form similar to (3) and (4). Based on this 
improvements to the gradient descent algorithms can be made and exact recovery results similar
to those of [14] can be maintained. The readers are referred to [1  11] for details on optimization
frameworks on matrix manifolds.

3.1 Gradients on the Grassmann manifold for matrix completion problem

Let G(m  r) be the Grassmann manifold in which each point corresponds to a subspace of dimension
r in Rm. One of the results of [14]  is that under a few assumptions (to be addressed in Section 4) 
one can obtain with high probability the exact matrix A by minimizing a regularized version of the
function F : G(m  r) × G(n  r) → R deﬁned below.

F (U  V ) = min

S∈Rr×r

F(U  S  V ) 

(7)

where F(U  S  V ) = (1/2)kPΩ(A − U SV T )k2
F   U ∈ Rm×k and V ∈ Rn×k are orthonormal
matrices. Here  we abuse the notation by denoting by U and V both orthonormal matrices as well
as the points on the Grassmann manifold which they span. Note that F only depends on the sub-
spaces spanned by matrices U and V . The function F (U  V ) can be easily evaluated by solving
the quadratic minimization problem in the form of (6). If G(m  r) is endowed with the canonical
inner product hW  W ′i = Tr (W T W ′)  where W and W ′ are tangent vectors of G(m  r) at U (i.e.
W  W ′ ∈ Rm×r such that W T U = 0 and W ′T U = 0) and similarly for G(n  r)  the gradients of
F (U  V ) on the product manifold are:

gradFU (U  V ) = (I − U U T )PΩ(U SV T − A)V ST
gradFV (U  V ) = (I − V V T )PΩ(U SV T − A)T U S.

(8)
(9)

In the above formulas  (I −U U T ) and (I −V V T ) are the projections of the derivatives PΩ(U SV T −
A)V ST and PΩ(U SV T − A)T U S onto the tangent space of the manifold at (U  V ). Notice that the
derivative terms are very similar to the updates in (3) and (4). The difference is in the scaling factors
where gradFU and gradFV use ST and S while those in Algorithm 2 use S−1 and S−T .
Assume that S is a diagonal matrix which can always be obtained by rotating U and V appropriately.
F (U  V ) would change more rapidly when the columns of U and V corresponding to larger entries
of S are changed. The rate of change of F would be approximately proportional to S2
ii when the
i-th columns of U and V are changed  or in other words  S2 gives us an approximate second order
information of F at the current point (U  V ). This suggests that the level set of F should be similar to
an “ellipse” with the shorter axes corresponding to the larger values of S. It is therefore compelling
to use a scaled metric on the Grassmann manifold.
Consider the inner product hW  W ′iD = Tr (DW T W ′)  where D ∈ Rr×r is a symmetric positive
deﬁnite matrix. We will derive the partial gradients of F on the Grassmann manifold endowed with
this scaled inner product. According to [11]  gradFU is the tangent vector of G(m  r) at U such that

Tr (F T

U W ) = h(gradFU )T   W iD 

(10)

for all tangent vectors W at U  where FU is the partial derivative of F with respect to U. Recall
that the tangent vectors at U are those W ’s such that W T U = 0. The solution of (10) with the
constraints that W T U = 0 and (gradFU )T U = 0 gives us the gradient based on the scaled metric 
which we will denote by gradsFU and gradsFV .

gradsFU (U  V ) = (I − U U T )FU D−1 = (I − U U T )PΩ(U SV T − A)V SD−1.
gradsFV (U  V ) = (I − V V T )FV D−1 = (I − V V T )PΩ(U SV T − A)T U SD−1.

(11)
(12)

Notice the additional scaling D appearing in these scaled gradients. Now if we use D = S2 (still
with the assumption that S is diagonal) as suggested by the arguments above on the approximate
shape of the level set of F   we will have gradsFU (U  V ) = (I − U U T )PΩ(U SV T − A)V S−1 and
gradsFV (U  V ) = (I − V V T )PΩ(U SV T − A)T U S−1 (note that S depends on U and V ).

4

If S is not diagonalized  we use SST and ST S to derive gradsFU and gradsFV respectively  and the
scalings appear exactly as in (3) and (4).

gradsFU (U  V ) = (I − U U T )PΩ(U SV T − A)V S−1
gradsFV (U  V ) = (I − V V T )PΩ(U SV T − A)T U S−T

(13)
(14)

This scaling can be interpreted as an adaptive preconditioning step similar to those that are popular
in the scientiﬁc computing literature [4]. As will be shown in our experiments  this scaled gradient
direction outperforms canonical gradient directions especially for ill-conditioned matrices.

The optimization framework on matrix manifolds allows to deﬁne several elements of the manifold
in a ﬂexible way. Here  we use the scaled-metric to obtain a good descent direction  while other
operations on the manifold can be based on the canonical metric which has simple and efﬁcient
computational forms. The next two sections describe algorithms using scaled-gradients.

3.2 Gradient descent algorithms on the Grassmann manifold

Gradient descent algorithms on matrix manifolds are based on the update

Ui+1 = R(Ui + tiWi)

(15)

where Wi is the gradient-related search direction  ti is the step size and R(U ) is a retraction on the
manifold which deﬁnes a projection of U onto the manifold [1]. We use R(U ) = span (U ) as the
retraction on the Grassmann manifold where span (U ) is represented by qf(U )  which is the Q factor
in the QR factorization of U. Optimization on the product of two Grassmann manifolds can be done
by treating each component as a coordinate component.

The step size t can be computed in several ways  e.g.  by a simple back-tracking method to ﬁnd the
point satisfying the Armijo condition [3]. Algorithm 3 is an outline of our gradient descent method
for matrix completion. We let gradsF (i)
V ≡ gradsFV (Ui  Vi). In
line 5  the exact Si+1 which realizes F (Ui+1  Vi+1) can be computed according to (6). A direct
method to solve (6) costs O(|Ω|r4). Alternatively  Si+1 can be computed approximately and we
found that (5) is fast (O((|Ω| + m + n)r2)) and gives the same convergence speed. If (5) fails
to yield good enough progress  we can always switch back to (6) and compute Si+1 exactly. The
subspace iteration and LMaFit can be seen as relaxed versions of this gradient descent procedure.
The next section goes further and described the conjugate gradient iteration.

U ≡ gradsFU (Ui  Vi) and gradsF (i)

Algorithm 3 GRADIENT DESCENT WITH SCALED-GRADIENT ON THE GRASSMANN MANIFOLD.
Input: Matrix AΩ  Ω  and number r.
Output: U and V which minimize F (U  V )  and S which realizes F (U  V ).
1: Initialize orthonormal matrices U0 and V0.
2: for i = 0 1 2 ... do
3:
4:

Compute gradsF (i)
Find an appropriate step size ti and compute

V according to (13) and (14).

U and gradsF (i)

(Ui+1  Vi+1) = (qf(Ui − tigradsF (i)

U )  qf(Vi − tigradsF (i)

V ))

Compute Si+1 according to (6) (exact) or (5) (approximate).

5:
6: end for

3.3 Conjugate gradient method on the Grassmann manifold

In this section  we describe the conjugate gradient (CG) method on the Grassmann manifold based
on the scaled gradients to solve the matrix completion problem. The main additional ingredient we
need is vector transport which is used to transport the old search direction to the current point on the
manifold. The transported search direction is then combined with the scaled gradient at the current
point  e.g. by Polak-Ribiere formula (see [11])  to derive the new search direction. After this  a line
search procedure is performed to ﬁnd the appropriate step size along this search direction.

Vector transport can be deﬁned using the Riemann connection  which in turn is deﬁned based on the
Riemann metric [1]. As mentioned at the end of Section 3.1  we will use the canonical metric to

5

derive vector transport when considering the natural quotient manifold structure of the Grassmann
manifold. The tangent W ′ at U will be transported to U + W as TU +W (W ′) where TU (W ′) =
(I − U U T )W ′. Algorithm 4 is a sketch of the resulting conjugate gradient procedure.
Algorithm 4 CONJUGATE GRADIENT WITH SCALED-GRADIENT ON THE GRASSMANN MANIFOLD.
Input: Matrix AΩ  Ω  and number r.
Output: U and V which minimize F (U  V )  and S which realizes F (U  V ).
1: Initialize orthonormal matrices U0 and V0.
2: Compute (η0  ξ0) = (gradsF (0)
U   gradsF (0)
V ).
3: for i = 0 1 2 ... do
4:
5:

Compute a step size ti and compute (Ui+1  Vi+1) = (qf(Ui + tiηi)  qf(Vi + tiξi))
Compute βi+1 (Polak-Ribiere) and set

(ηi+1  ξi+1) = (−gradsF (i)

U + βi+1TUi+1(ηi)  −gradsF (i)

V + βi+1TVi+1(ξi))

Compute Si+1 according to (6) or (5).

6:
7: end for

4 Convergence and exact recovery of scaled-gradient descent methods

Let A = U∗Σ∗V T
∗ be the singular value decomposition of A  where U∗ ∈ Rm×r  V∗ ∈ Rn×r and
Σ∗ ∈ Rr×r. Let us also denote z = (U  V ) a point on G(m  r) × G(n  r). Clearly  z∗ = (U∗  V∗)
is a minimum of F . Assume that A is incoherent [14]; A has bounded entries and the minimum
singular value of A is bounded away from 0. Let κ(A) be the condition number of A. It is shown
that  if the number of observed entries is of order O(max{κ(A)2n log n  κ(A)6n}) then  with high
probability  F is well approximated by a parabola and z∗ is the unique stationary point of F in a
sufﬁciently small neighborhood of z∗ ([14  Lemma 6.4&6.5]). From these observations  given an
initial point that is sufﬁciently close to z∗  a gradient descent procedure on F (with an additional
regularization term to keep the intermediate points incoherent) converges to z∗ and exact recovery
is obtained. The singular value decomposition of a trimmed version of the observerd matrix AΩ can
give us the initial point that ensures convergence. The readers are referred to [14] for details.

Cinc

i=1 G1( kU (i)k2

i=1 G1( kV (i)k2

) + Pn

From [14]  let G(U  V ) = Pm
)  where G1(x) = 0 if x ≤ 1
and G1(x) = e(x−1)2
− 1 otherwise; Cinc is a constant depending on the incoherence assumptions.
We consider the regularized version of F : ˜F (U  V ) = F (U  V ) + ρG(U  V )  where ρ is chosen
appropriately so that U and V remain incoherent during the execution of the algorithm. We can see
that z∗ is also the minimum of ˜F . We will now show that the scaled-gradients of ˜F are well-deﬁned
during the iterations and they are indeed descent directions of ˜F and only vanish at z∗. As a result 
the scaled-gradient-based methods can inherit all the convergence results in [14].

Cinc

min and σ∗

max of Σ∗: σmax ≤ 2σ∗

First  S must be non-singular during the iterations for the scaled-gradients to be well-deﬁned. As a
corollary of Lemma 6.4 in [14]  the extreme singular values of any intermediate S are bounded by
min. The second
extreme singular values σ∗
inequality implies that S is well-conditioned during the iterations.
The scaled-gradient is the descent direction of ˜F as a direct result from the fact that it is in-
deed the gradient of ˜F based on a non-canonical metric. Moreover  by Lemma 6.5 in [14] 
kgrad ˜F (z)k2 ≥ Cnǫ2(σ∗
min)4d(z  z∗)2 for some constant C  where k.k and d(.  .) are the canonical
norm and distance on the Grassmann manifold respectively. Based on this  a similar lower bound of
kgrads

˜F k can be derived. Let D1 = SST and D2 = ST S be the scaling matrices. Then 

max and σmin ≥ 1

2 σ∗

kgrads

˜F (z)k2 = kgrad ˜FU (z)D−1

1 k2
max(kgrad ˜FU (z)k2

F + kgrad ˜FV (z)D−1
2 k2
F + kgrad ˜FV (z)k2
F )

F

≥ σ−2
≥ (2σ∗
≥ (2σ∗

max)−2kgrad ˜F (z)k2
max)−2Cnǫ2(σ∗

min)4d(z  z∗)2 = C(σ∗

min)4(2σ∗

max)−2nǫ2d(z  z∗)2.

Therefore  the scaled gradients only vanish at z∗ which means the scaled-gradient descent procedure
must converge to z∗  which is the exact solution [3].

6

5 Experiments and results

The proposed algorithms were implemented in Matlab with some mex-routines to perform matrix
multiplications with sparse masks. For synthesis data  we consider two cases: (1) fully random
low-rank matrices  A = randn(m  r) ∗ randn(r  n) (in Matlab notations) whose singular values
tend to be roughly the same; (2) random low-rank matrices with chosen singular values by letting
U = qf(randn(m  r))  V = qf(randn(n  r)) and A = U SV T where S is a diagonal matrix with
chosen singular values. The initializations of all methods are based on the SVD of AΩ.
First  we illustrate the improvement of scaled gradients over canonical gradients for steepest descent
and conjugate gradient methods on 5000 × 5000 matrices with rank 5 (Figure 1). Note that Canon-
Grass-Steep is OptSpace with our implementation. In this experiment  Si is obtained exactly using
(6). The time needed for each iteration is roughly the same for all methods so we only present the
results in terms of iteration counts. We can see that there are some small improvements for the fully
random case (Figure 1a) since the singular values are roughly the same. The improvement is more
substantial for matrices with larger condition numbers (Figure 1b).

l

)
e
a
c
s
−
g
o
l
(
 

E
S
M
R

0

−5

−10

−15

 

5000x5000 − Rank 5 − 1.0% observed entries
Singular values [4774  4914  4979  5055  5146]

 

Canon−Grass−Steep
Canon−Grass−CG
Scaled−Grass−Steep
Scaled−Grass−CG

10

20

30

40

50

60

70

80

90

Iteration count

(a)

l

)
e
a
c
s
−
g
o
l
(
 

E
S
M
R

0

−2

−4

−6

−8

−10

−12

−14

 

5000x5000 − Rank 5 − 1.0% observed entries
Singular values [1000  2000  3000  4000  5000]

 

Canon−Grass−Steep
Canon−Grass−CG
Scaled−Grass−Steep
Scaled−Grass−CG

20

40

60

80

100

120

140

160

180

200

Iteration count

(b)

Figure 1: Log-RMSE for fully random matrix (a) and random matrix with chosen spectrum (b).

Now  we compare the relaxed version of the scaled conjugate gradient which uses (5) to compute Si
(ScGrass-CG) to LMaFit [23]  Riemann-CG [22]  RTRMC2 [6] (trust region method with second
order information)  SVP [12] and GROUSE [5] (Figure 2). These methods are also implemented in
Matlab with mex-routines similar to ours except for GROUSE which is entirely in Matlab (Indeed
GROUSE does not use sparse matrix multiplication as other methods do). The subspace iteration
method and the relaxed version of scaled steepest descent converge similarly to LMaFit  so we omit
them in the graph. Note that each iteration of GROUSE in the graph corresponds to one pass over
the matrix. It does not have exactly the same meaning as one iteration of other methods and is
much slower with its current implementation. We use the best step sizes that we found for SVP
and GROUSE. In terms of iteration counts  we can see that for the fully random case (upper row) 
RTRMC2 is the best while ScGrass-CG and Riemann-CG converge reasonably fast. However  each
iteraton of RTRMC2 is slower so in terms of time  ScGrass-CG and Riemann-CG are the fastest in
our experiments. When the condition number of the matrix is higher  ScGrass-CG converges fastest
both in terms of iteration counts and execution time.

Finally  we test the algorithms on Jester-1 and MovieLens-100K datasets which are assumed to
be low-rank matrices with noise (SVP and GROUSE are not tested because their step sizes need
to be appropriately chosen). Similarly to previous work  for the Jester dataset we randomly se-
lect 4000 users and randomly withhold 2 ratings for each user for testing. For the MovieLens
dataset  we use the common dataset prepared by [16]  and keep 50% for training and 50% for
testing. We run 100 different randomizations of Jester and 10 randomizations of MovieLens and
average the results. We stop all methods early  when the change of RMSE is less than 10−4  to
avoid overﬁtting. All methods stop well before one minute. The Normalized Mean Absolute Errors
(NMAEs) [13] are reported in Table 1. ScGrass-CG is the relaxed scaled CG method and ScGrass-
CG-Reg is the exact scaled CG method using a spectral-regularization version of F proposed in

7

10000x10000 − Rank 10 − 0.5% observed entries

Singular values [9612 9717 9806 9920 9987 10113 10128 10226 10248 10348]
 

SVP

LMaFit

l

)
e
a
c
s
−
g
o
l
(
 

E
S
M
R

2

0

−2

−4

−6

−8

−10

l

)
e
a
c
s
−
g
o
l
(
 

E
S
M
R

ScGrass−CG

−12

RTRMC2

−14

 

RiemannCG
20

40

60

GROUSE
80
100

120

140

160

180

200

Iteration count

10000x10000 − Rank 10 − 0.5% observed entries                      

Singular values [1000 2000 3000 4000 5000 6000 7000 8000 9000 10000]

 

GROUSE

SVP

LMaFit

RTRMC2

Riemann−CG

ScGrass−CG

l

)
e
a
c
s
−
g
o
l
(
 

E
S
M
R

10000x10000 − Rank 10 − 0.5% observed entries

Singular values [9612 9717 9806 9920 9987 10113 10128 10226 10248 10348]
 

GROUSE

SVP

2

0

−2

−4

−6

−8

−10

−12

−14

−16

 
0

2

0

−2

−4

−6

−8

−10

−12

ScGrass−CG

LMaFit

Riemann−CG

RTRMC2

5

10

15

20

25

30

35

40

45

50

Time [s]

10000x10000 − Rank 10 − 0.5% observed entries

Singular values [1000 2000 3000 4000 5000 6000 7000 8000 9000 10000]

 

SVP

GROUSE

RTRMC2

LMaFit

Riemann−CG

l

)
e
a
c
s
−
g
o
l
(
 

E
S
M
R

2

0

−2

−4

−6

−8

−10

−12

−14

 

50

100

150

200

250

300

Iteration count

ScGrassCG

−14

 
0

10

20

30

40

50

60

70

80

90

100

Time [s]

Figure 2: Log-RMSE. Upper row is fully random  lower row is random with chosen singular values.

Rank
5
7
5
7

ScGrass-CG ScGrass-CG-Reg LMaFit Riemann-CG RTRMC2
0.1588
0.1584
0.1808
0.1832

0.1588
0.1583
0.1884
0.2298

0.1591
0.1584
0.1781
0.1817

0.1588
0.1581
0.1828
0.1836

0.1588
0.1584
0.1758
0.1787

Table 1: NMAE on Jester dataset (ﬁrst 2 rows) and MovieLens 100K. NMAEs for a random guesser
are 0.33 on Jester and 0.37 on MovieLens 100K.

[13]: ˜F (U  V ) = minS(1/2)(kPΩ(U SV T − A)k + λkSk2
F ). All methods perform similarly and
demonstrate overﬁtting when k = 7 for MovieLens. We observe that ScGrass-CG-Reg suffers the
least from overﬁtting thanks to its regularization. This shows the importance of regularization for
noisy matrices and motivates future work in this direction.

6 Conlusion and future work

The gradients obtained from a scaled metric on the Grassmann manifold can result in improved
convergence of gradient methods on matrix manifolds for matrix completion while maintaining
good global convergence and exact recovery guarantees. We have established a connection between
scaled gradient methods and subspace iteration method for matrix completion. The relaxed versions
of the proposed gradient methods  adapted from the subspace iteration  are faster than previously
discussed algorithms  sometimes much faster depending on the conditionining of the data matrix.
In the future  we will investigate if these relaxed versions achieve similar performance guarantees.
We are also interested in exploring ways to regularize the relaxed versions to deal with noisy data.
The convergence condition of OptSpace depends on κ(A)6 and weakening this dependency for the
proposed algorithms is also an interesting future direction.

8

Acknowledgments

This work was supported by NSF grants DMS-0810938 and DMR-0940218.

References

[1] P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton

University Press  Princeton  NJ  2008.

[2] Y. Amit  M. Fink  N. Srebro  and S. Ullman. Uncovering shared structures in multiclass classiﬁcation. In

Proceedings of the 24th international conference on Machine learning  ICML ’07  pages 17–24  2007.

[3] L. Armijo. Minimization of functions having Lipschitz continuous ﬁrst partial derivatives. Paciﬁc Journal

of Mathematics  16(1):1–3  1966.

[4] J. Baglama  D. Calvetti  G. H. Golub  and L. Reichel. Adaptively preconditioned GMRES algorithms.

SIAM J. Sci. Comput.  20(1):243–269  December 1998.

[5] L. Balzano  R. Nowak  and B. Recht. Online identiﬁcation and tracking of subspaces from highly incom-

plete information. In Proceedings of Allerton  September 2010.

[6] N. Boumal and P.-A. Absil. Rtrmc: A riemannian trust-region method for low-rank matrix completion.

In NIPS  2011.

[7] J-F. Cai  E. J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20(4):1956–1982  2010.

[8] E. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion  2009.
[9] P. Chen and D. Suter. Recovering the Missing Components in a Large Noisy Low-Rank Matrix: Ap-
plication to SFM. IEEE Transactions on Pattern Analysis and Machine Intelligence  26(8):1051–1063 
2004.

[10] W. Dai  E. Kerman  and O. Milenkovic. A geometric approach to low-rank matrix completion. IEEE

Transactions on Information Theory  58(1):237–247  2012.

[11] A. Edelman  T. Arias  and S. T. Smith. The geometry of algorithms with orthogonality constraints. SIAM

J. Matrix Anal. Appl  20:303–353  1998.

[12] P. Jain  R. Meka  and I. S. Dhillon. Guaranteed rank minimization via singular value projection. In NIPS 

pages 937–945  2010.

[13] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. In Y. Bengio  D. Schuur-
mans  J. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in Neural Information Processing
Systems 22  pages 952–960. 2009.

[14] R. H. Keshavan  S. Oh  and A. Montanari. Matrix completion from a few entries. CoRR  abs/0901.3150 

2009.

[15] S. Ma  D. Goldfarb  and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-

tion. Math. Program.  128(1-2):321–353  2011.

[16] B. Marlin. Collaborative ﬁltering: A machine learning perspective  2004.
[17] R. Mazumder  T. Hastie  and R. Tibshirani. Spectral regularization algorithms for learning large incom-

plete matrices. J. Mach. Learn. Res.  11:2287–2322  August 2010.

[18] B. Recht. A simpler approach to matrix completion. CoRR  abs/0910.0651  2009.
[19] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.
In In Proceedings of the 22nd International Conference on Machine Learning (ICML  pages 713–719.
ACM  2005.

[20] Y. Saad. Numerical Methods for Large Eigenvalue Problems- classics edition. SIAM  Philadelpha  PA 

2011.

[21] N. Srebro and T. Jaakkola. Weighted low-rank approximations. In In 20th International Conference on

Machine Learning  pages 720–727. AAAI Press  2003.

[22] B. Vandereycken. Low-rank matrix completion by riemannian optimization. Technical report  Mathemat-

ics Section  Ecole Polytechnique Federale de de Lausanne  2011.

[23] Z. Wen  W. Yin  and Y. Zhang. Solving a low-rank factorization model for matrix completion using a

non-linear successive over-relaxation algorithm. In CAAM Technical Report. Rice University  2010.

9

,Wei Sun
Zhaoran Wang
Han Liu
Guang Cheng