2013,Parametric Task Learning,We introduce a novel formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle infinitely many tasks parameterized by a continuous parameter. Our key finding is that  for a certain class of PTL problems  the path of optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact  we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks efficiently. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity  cost-sensitive learning  and quantile regression  and demonstrate the usefulness of the proposed method experimentally in these scenarios.,Parametric Task Learning

Ichiro Takeuchi

Nagoya Institute of Technology

Nagoya  466-8555  Japan

Tatsuya Hongo

Nagoya Institute of Technology

Nagoya  466-8555  Japan

takeuchi.ichiro@nitech.ac.jp

hongo.mllab.nit@gmail.com

Masashi Sugiyama

Tokyo Institute of Technology

Tokyo  152-8552  Japan

Shinichi Nakajima
Nikon Corporation

Tokyo  140-8601  Japan

sugi@cs.titech.ac.jp

nakajima.s@nikon.co.jp

Abstract

We introduce an extended formulation of multi-task learning (MTL) called para-
metric task learning (PTL) that can systematically handle inﬁnitely many tasks
parameterized by a continuous parameter. Our key ﬁnding is that  for a certain
class of PTL problems  the path of the optimal task-wise solutions can be repre-
sented as piecewise-linear functions of the continuous task parameter. Based on
this fact  we employ a parametric programming technique to obtain the common
shared representation across all the continuously parameterized tasks. We show
that our PTL formulation is useful in various scenarios such as learning under
non-stationarity  cost-sensitive learning  and quantile regression. We demonstrate
the advantage of our approach in these scenarios.

1 Introduction

Multi-task learning (MTL) has been studied for learning multiple related tasks simultaneously. A
key assumption behind MTL is that there exists a common shared representation across the tasks.
Many MTL algorithms attempt to ﬁnd such a common representation and at the same time to learn
multiple tasks under that shared representation. For example  we can enforce all the tasks to share a
common feature subspace or a common set of variables by using an algorithm introduced in [1  2]
that alternately optimizes the shared representation and the task-wise solutions.

Although the standard MTL formulation can handle only a ﬁnite number of tasks  it is sometimes
more natural to consider inﬁnitely many tasks parameterized by a continuous parameter  e.g.  in
learning under non-stationarity [3] where learning problems change over continuous time  cost-
sensitive learning [4] where loss functions are asymmetric with continuous cost balance  and quan-
tile regression [5] where the quantile is a continuous variable between zero and one. In order to
handle these inﬁnitely many parametrized tasks  we propose in this paper an extended formulation
of MTL called parametric-task learning (PTL).

The key contribution of this paper is to show that  for a certain class of PTL problems  the optimal
common representation shared across inﬁnitely many parameterized tasks can be obtainable. Specif-
ically  we develop an alternating minimization algorithm `a la [1  2] for ﬁnding the entire continuum
of solutions and the common feature subspace (or the common set of variables) among inﬁnitely
many parameterized tasks. Our algorithm exploits the fact that  for those classes of PTL problems 
the path of task-wise solutions is piecewise-linear in the task parameter. We use the parametric
programming technique [6  7  8  9] for computing those piecewise linear solutions.

1

Notations: Let us denote by R  R+  and R++ the set of real  nonnegative  and positive numbers 
respectively  while we deﬁne Nn := f1; : : : ; ng for every natural number n. We denote by S d
++ the
set of d (cid:2) d positive deﬁnite matrices  and let I((cid:1)) be the indicator function.

2 Review of Multi-Task Learning (MTL)
In this section  we review an MTL method developed in [1  2]. Let f(xi; yi)gi2Nn be the set of
n training instances  where xi 2 X (cid:18) Rd is the input and yi 2 Y is the output. We deﬁne
wi(t) 2 [0; 1]; t 2 NT as the weight of the ith instance for the tth task  where T is the number
t x for each task  where (cid:12)t;0 2 R and
⊤
of tasks. We consider an afﬁne model ft(x) = (cid:12)t;0 + (cid:12)
(cid:12)t 2 Rd. For notational simplicity  we deﬁne augmented vectors ~(cid:12) := ((cid:12)0; (cid:12)1; : : : ; (cid:12)d)
⊤ 2 Rd+1
and ~x := (1; x1; : : : ; xd)
The multi-task feature learning method discussed in [1] is formulated as

⊤ 2 Rd+1  and write the afﬁne model as ft(x) = ~(cid:12)
∑

∑

∑

⊤
t ~x.

min
f ~(cid:12)tgt2N
++;tr(D)(cid:20)1

T

D2S d

t2NT

t2NT

wi(t)ℓt(r(yi; ~(cid:12)

⊤
t ~xi)) +

(cid:13)
T

t2NT

⊤
t D

(cid:12)

(cid:0)1(cid:12)t;

(1)

where tr(D) is the trace of D  ℓt : R ! R+ is the loss function for the tth task incurred on the
⊤
residual r(yi; ~(cid:12)
t ~xi)1  and (cid:13) > 0 is the regularization parameter2. It was shown [1] that the problem
(1) is equivalent to

∑

∑

t2NT

i2NN

min
f ~(cid:12)tgt2N

T

wi(t)ℓt(r(yi; ~(cid:12)

⊤
t ~xi)) +

jjBjj2
tr;

(cid:13)
T

⊤

where B is the d (cid:2) T matrix whose tth column is given by the vector (cid:12)t  and jjBjjtr

:=
)1=2) is the trace norm of B. As shown in [10]  the trace norm is the convex upper envelope
tr((BB
of the rank of B  and (1) can be interpreted as the problem of ﬁnding a common feature subspace
across T tasks. This problem is often referred to as multi-task feature learning. If the matrix D is
restricted to be diagonal  the formulation (1) is reduced to multi-task variable selection [11  12].
In order to solve the problem (1)  the alternating minimization algorithm was suggested in [1] (see
Algorithm 1). This algorithm alternately optimizes the task-wise solutions f ~(cid:12)tgt2NT and the com-
mon representation matrix D. It is worth noting that  when D is ﬁxed  each ~(cid:12)t can be independently
optimized (Step 1). On the other hand  when f ~(cid:12)tgt2NT are ﬁxed  the optimization of the matrix D
  and the
can be reduced to the minimization over d eigenvalues (cid:21)1; : : : ; (cid:21)d of the matrix C := BB
optimal D can be analytically computed (Step 2).

⊤

3 Parametric-Task Learning (PTL)

We consider the case where we have inﬁnitely many tasks parametrized by a single continuous
parameter. Let (cid:18) 2 [(cid:18)L; (cid:18)U] be a continuous task parameter. Instead of the set of weights wi(t); t 2
NT   we consider a weight function wi : [(cid:18)L; (cid:18)U] ! [0; 1] for each instance i 2 Nn. In PTL  we
learn a parameter vector ~(cid:12)(cid:18) 2 Rd+1 as a continuous function of the task parameter (cid:18):

wi((cid:18)) ℓ(cid:18)(r(yi; ~(cid:12)

⊤
(cid:18) ~xi)) d(cid:18) + (cid:13)

(cid:18)U

⊤
(cid:18) D

(cid:0)1(cid:12)(cid:18) d(cid:18);

(cid:12)

(2)

∫

∑

(cid:18)U

min
f ~(cid:12)(cid:18)g(cid:18)2[(cid:18)L ;(cid:18)U]
D2S d
++;tr(D)(cid:20)1

(cid:18)L

i2Nn

∫

(cid:18)L

where  note that  the loss function ℓ(cid:18) possibly depends on (cid:18).
As we will explain in the next section  the above PTL formulation is useful in various important
machine learning scenarios including learning under non-stationarity  cost-sensitive learning  and

t ~xi) = (yi (cid:0) ~(cid:12)
⊤
1For example  r(yi; ~(cid:12)

⊤

~xi)2 for regression problems with yi 2 R  while r(yi; ~(cid:12)
⊤
t ~xi) =

1 (cid:0) yi ~(cid:12)
t ~xi for binary classiﬁcation problems with yi 2 f(cid:0)1; 1g.
⊤

2In [1]  wi(t) takes either 1 or 0. It takes 1 only if the ith instance is used in the tth task. We slightly

generalize the setup so that each instance can be used in multiple tasks with different weights.

2

Algorithm 1 ALTERNATING MINIMIZATION ALGORITHM FOR MTL [1]
1: Input: Data f(xi; yi)gi2Nn and weights fwi(t)gi2Nn;t2NT ;
2: Initialize: D   Id=d (Id is d (cid:2) d identity matrix)
3: while convergence condition is not true do
4:

Step 1: For t = 1; : : : ; T do

~(cid:12)t   arg min

~(cid:12)

5:

Step 2:

wi(t)ℓt(r(yi; ~(cid:12)

⊤

~xi)) +

⊤

(cid:0)1(cid:12)

D

(cid:12)

(cid:13)
T

∑

i2Nn

D   C 1=2
tr(C)1=2

= arg

D2Sd

min
++;tr(D)(cid:20)1

where C := BB

whose (j; k)th element is deﬁned as Cj;k :=

t2NT

(cid:12)tj(cid:12)tk.

⊤

6: end while
7: Output: f ~(cid:12)tgt2NT and D;

∑

t2NT

(cid:0)1(cid:12)t;

(cid:12)

⊤
t D

∑

quantile regression. However  at ﬁrst glance  the PTL optimization problem (2) seems computa-
tionally intractable since we need to ﬁnd inﬁnitely many task-wise solutions as well as the common
feature subspace (or the common set of variables if D is restricted to be diagonal) shared by inﬁnitely
many tasks.

Our key ﬁnding is that  for a certain class of PTL problems  when D is ﬁxed  the optimal path of the
task-wise solutions ~(cid:12)(cid:18) is shown to be piecewise-linear in (cid:18). By exploiting this piecewise-linearity 
we can efﬁciently handle inﬁnitely many parameterized tasks  and the optimal solutions of those
class of PTL problems can be exactly computed.
In the following theorem  we prove that the task-wise solutions ~(cid:12)(cid:18) is piecewise-linear in (cid:18) if the
weight functions and the loss function satisfy certain conditions.
Theorem 1 For any d (cid:2) d positive-deﬁnite matrix D 2 S d
++  the optimal solution path of
⊤
wi((cid:18))ℓ(cid:18)(r(yi; ~(cid:12)

~(cid:12)(cid:18)   arg min

∑

~xi)) + (cid:13)(cid:12)

(cid:0)1(cid:12)

(3)

D

⊤

for (cid:18) 2 [(cid:18)L; (cid:18)U] is written as a piecewise-linear function of (cid:18) if the residual r(y; ~(cid:12)
~x) can be
written as an afﬁne function of ~(cid:12)  and the weight functions wi : [(cid:18)L; (cid:18)U] ! [0; 1]  i 2 Nn and the
loss function ℓ : R ! R+ satisfy either of the following conditions (a) or (b):

⊤

~(cid:12)

i2Nn

(a) All the weight functions are piecewise-linear functions  and the loss function is a convex
piecewise-linear function which does not depend on (cid:18);

∑

(b) All the weight functions are piecewise-constant functions  and the loss function is a convex
piecewise-linear function which depends on (cid:18) in the following form:

maxf(ah + bhr)(ch + dh(cid:18)); 0g;

h2NH

ℓ(cid:18)(r) =

(4)
where H is a positive integer  and ah; bh; ch; dh 2 R are constants such that ch + dh(cid:18) (cid:21) 0 for all
(cid:18) 2 [(cid:18)L; (cid:18)U].
In the proof in Appendix A  we show that  if the weight functions and the loss function satisfy the
conditions (a) or (b)  the problem (3) is reformulated as a parametric quadratic program (parametric
QP)  where the parameter (cid:18) only appears in the linear term of the objective function. As shown  for
example  in [9]  the optimal solution path of this class of parametric QP has a piecewise-linear form.
If ~(cid:12)(cid:18) is piecewise-linear in (cid:18)  we can exactly compute the entire solution path by using parametric
programming. In machine learning literature  parametric programming is often used in the context

3

Algorithm 2 ALTERNATING MINIMIZATION ALGORITHM FOR PTL
1: Input: Data f(xi; yi)gi2Nn and weight functions wi : [(cid:18)L; (cid:18)U] :! [0; 1] for all i 2 Nn;
2: Initialize: D   Id=d (Id is d (cid:2) d identity matrix)
3: while convergence condition is not true do
Step 1: For all the continuum of (cid:18) 2 [(cid:18)L; (cid:18)U] do
4:

~(cid:12)(cid:18)   arg min

~(cid:12)

wi((cid:18))ℓ(cid:18)(r(yi; ~(cid:12)

⊤

~xi)) + (cid:13)(cid:12)

⊤

(cid:0)1(cid:12)

D

∑

i2Nn

by using parametric programming;
Step 2:

5:

D   C 1=2
tr(C)1=2

= arg

D2S d

min
++;tr(D)(cid:20)1

∫

where (j; k)th element of C 2 Rd(cid:2)d is deﬁned as Cj;k :=

6: end while
7: Output: f ~(cid:12)(cid:18)g for (cid:18) 2 [(cid:18)L; (cid:18)U] and D;

(cid:18)U

∫

(cid:18)L

(cid:18)U
(cid:18)L

⊤
(cid:18) D

(cid:0)1(cid:12)(cid:18)d(cid:18);

(cid:12)

(5)

(cid:12)(cid:18);j(cid:12)(cid:18);kd(cid:18);

of regularization path-following [13  14  15]3. We start from the solution at (cid:18) = (cid:18)L  and follow
the path of the optimal solutions while (cid:18) is continuously increased. This is efﬁciently conducted by
exploiting the piecewise-linearity.

Our proposed algorithm for solving the PTL problem (2) is described in Algorithm 2  which is es-
sentially a continuous version of the MTL algorithm shown in Algorithm 1. Note that  by exploiting
the piecewise linearity of (cid:12)(cid:18)  we can compute the integral at Step 2 (Eq. (5)) in Algorithm 2.
Algorithm 2 can be changed to parametric-task variable selection if Step 2 is replaced with

D   diag((cid:21)1; : : : ; (cid:21)d) where (cid:21)j =

(cid:12)2
(cid:18);jd(cid:18)
(cid:18)U
(cid:18)L

(cid:12)2
(cid:18);j′d(cid:18)

for all j 2 Nd;

which can also be computed efﬁciently by exploiting the piecewise linearity of (cid:12)(cid:18).

4 Examples of PTL Problems

In this section  we present three examples where our PTL formulation (2) is useful.

√∫

√∫

(cid:18)U
(cid:18)L

∑

j′2Nd

Binary Classiﬁcation Under Non-Stationarity Suppose that we observe n training instances se-
quentially  and denote them as f(xi; yi; (cid:28)i)gi2Nn  where xi 2 Rd  yi 2 f(cid:0)1; 1g  and (cid:28)i is the time
when the ith instance is observed. Without loss of generality  we assume that (cid:28)1 < : : : < (cid:28)n. Under
non-stationarity  if we are requested to learn a classiﬁer to predict the output for a test input x ob-
served at time (cid:28)   the training instances observed around time (cid:28) should have more inﬂuence on the
classiﬁer than others.
Let wi((cid:28) ) denote the weight of the ith instance when training a classiﬁer for a test point at time (cid:28) .
We can for example use the following triangular weight function (see Figure1):

wi((cid:28) ) =

(cid:0)1((cid:28)i (cid:0) (cid:28) )
(cid:0)1((cid:28)i (cid:0) (cid:28) )

if (cid:28) (cid:0) s (cid:20) (cid:28)i < (cid:28);
if (cid:28) (cid:20) (cid:28)i < (cid:28) + s;
otherwise;

(6)

where s > 0 determines the width of the triangular time windows. The problem of training a
classiﬁer for time (cid:28) is then formulated as

wi((cid:28) ) max(0; 1 (cid:0) yi

~(cid:12)

⊤

~xi) + (cid:13)jj(cid:12)jj2
2;

8<: 1 + s
∑

1 (cid:0) s
0

min

~(cid:12)

i2Nn

where we used the hinge loss.

3In regularization path-following  one computes the optimal solution path w.r.t. the regularization parameter 

whereas we compute the optimal solution path w.r.t. the task parameter (cid:18).

4

Figure 1: Examples of weight functions fwi((cid:28) )gi2Nn in non-stationary time-series learning. Given a
training instances (xi; yi) at time (cid:28)i for i = 1; : : : ; n under non-stationary condition  it is reasonable
to use the weights fwi((cid:28) )gi2Nn as shown here when we learn a classiﬁer to predict the output of a
test input at time (cid:28) .

If we have the belief that a set of classiﬁers for different time should have some common structure 
we can apply our PTL approach to this problem. If we consider a time interval (cid:28) 2 [(cid:28)L; (cid:28)U]  the
parametric-task feature learning problem is formulated as
wi((cid:28) ) max(0; 1 (cid:0) yi

(cid:0)1(cid:12)(cid:28) d(cid:28):

⊤
(cid:28) ~xi) d(cid:28) + (cid:13)

∑

⊤
(cid:28) D

∫

∫

(7)

~(cid:12)

(cid:12)

(cid:28)U

(cid:28)U

min
f ~(cid:12)((cid:28) )g(cid:28)2[(cid:28)L ;(cid:28)U]
D2S d
++;tr(D)(cid:20)1

(cid:28)L

i2Nn

Note that the problem (7) satisﬁes the condition (a) in Theorem 1.

Joint Cost-Sensitive Learning Next  let us consider cost-sensitive binary classiﬁcation. When
the costs of false positives and false negatives are unequal  or when the numbers of positive and
negative training instances are highly imbalanced  it is effective to use the cost-sensitive learning
approach [16]. Suppose that we are given a set of training instances f(xi; yi)gi2Nn with xi 2 Rd and
yi 2 f(cid:0)1; 1g. If we know that the ratio of the false positive and false negative costs is approximately
(cid:18) : (1 (cid:0) (cid:18))  it is reasonable to solve the following cost-sensitive SVM [17]:
~xi) + (cid:13)jj(cid:12)jj2
2;

wi((cid:18)) max(0; 1 (cid:0) yi

∑

min

~(cid:12)

⊤

(cid:28)L

(cid:18)L

i2Nn
where the weight wi((cid:18)) is deﬁned as

~(cid:12)

{

wi((cid:18)) =

(cid:18)
1 (cid:0) (cid:18)

if yi = (cid:0)1;
if yi = +1:

When the exact false positive and false negative costs in the test scenario are unknown [4]  it is often
desirable to train several cost-sensitive SVMs with different values of (cid:18). If we have the belief that
a set of classiﬁers for different cost ratios should have some common structure  we can apply our
PTL approach to this problem. If we consider an interval (cid:18) 2 [(cid:18)L; (cid:18)U]  0 < (cid:18)L < (cid:18)U < 1  the
parametric-task feature learning problem is formulated as
wi((cid:18)) max(0; 1 (cid:0) yi

(cid:0)1(cid:12)(cid:18) d(cid:18):

⊤
(cid:18) ~xi) d(cid:18) + (cid:13)

∑

⊤
(cid:18) D

∫

∫

(8)

~(cid:12)

(cid:18)U

(cid:18)U

(cid:12)

min
f ~(cid:12)(cid:18)g(cid:18)2[(cid:18)L;(cid:18)U]
D2S d
++;tr(D)(cid:20)1

(cid:18)L

i2Nn

The problem (8) also satisﬁes the condition (a) in Theorem 1. Figure 2 shows an example of joint
cost-sensitive learning applied to a toy 2D binary classiﬁcation problem.
Joint Quantile Regression Given a set of training instances f(xi; yi)gi2Nn with xi 2 Rd and
yi 2 R drawn from a joint distribution P (X; Y )  quantile regression [19] is used to estimate the
Y jX=x((cid:28) ) as a function of x  where (cid:28) 2 (0; 1) and FY jX=x is the cumu-
(cid:0)1
conditional (cid:28) th quantile F
lative distribution function of the conditional distribution P (Y jX = x). Jointly estimating multiple
conditional quantile functions is often useful for exploring the stochastic relationship between X
and Y (see Section 5 for an example of joint quantile regression problems). Linear quantile regres-
sion along with L2 regularization [20] at order (cid:28) 2 (0; 1) is formulated as
(1 (cid:0) (cid:28) )jrj
(cid:28)jrj

~xi) + (cid:13)jj(cid:12)jj2

if r (cid:20) 0;
if r > 0:

(cid:26)(cid:28) (yi (cid:0) ~(cid:12)

2; (cid:26)(cid:28) (r) :=

∑

{

min

⊤

~(cid:12)

i2Nn

5

      (a) Independent cost-sensitive learning

(b) Joint cost-sensitive learning

Figure 2: An example of joint cost-sensitive learning on 2D toy dataset (2D input x is expanded to
n-dimension by radial basis functions centered on each xi). In each plot  the decision boundaries
of ﬁve cost-sensitive SVMs ((cid:18) = 0:1; 0:25; 0:5; 0:75; 0:9) are shown. (a) Left plot is the results ob-
tained by independently training each cost-sensitive SVMs. (b) Right plot is the results obtained by
jointly training inﬁnitely many cost-sensitive SVMs for all the continuum of (cid:18) 2 [0:05; 0:95] using
the methodology we present in this paper (both are trained with the same regularization parameter
(cid:13)). When independently trained  the inter-relationship among different cost-sensitive SVMs looks
inconsistent (c.f.  [18]).

If we have the belief that a family of quantile regressions at various (cid:28) 2 (0; 1) have some common
structure  we can apply our PTL framework to joint estimation of the family of quantile regressions
This PTL problem satisﬁes the condition (b) in Theorem 1  and is written as

f(cid:12)(cid:28)g(cid:28)2(0;1)

min
++;tr(D)(cid:20)1

D2Sd

(cid:26)(cid:28) (yi (cid:0) (cid:12)

⊤
(cid:28) xi)d(cid:28) + (cid:13)

1

⊤
(cid:28) D

(cid:0)1(cid:12)(cid:28) d(cid:28);

(cid:12)

where we do not need any weighting and omit wi((cid:28) ) = 1 for all i 2 Nn and (cid:28) 2 [0; 1].

∫

∑

1

0

i2Nn

∫

0

5 Numerical Illustrations

In this section  we illustrate various aspects of PTL with the three examples discussed in the previous
section.

n

n ; 2 2(cid:25)

Artiﬁcial Example for Learning under Non-stationarity We ﬁrst consider a simple artiﬁcial
problem with non-stationarity  where the data generating mechanism gradually changes. We assume
that our data generating mechanism produces the training set f(xi; yi; (cid:28)i)gi2Nn with n = 100 as
follows. For each (cid:28)i 2 f0; 1 2(cid:25)
g  the output yi is ﬁrst determined as yi = 1 if i
is odd  while yi = (cid:0)1 if i is even. Then  xi 2 Rd is generated as

n ; : : : ; (n (cid:0) 1) 2(cid:25)

xi1 (cid:24) N (yi cos (cid:28)i; 12); xi2 (cid:24) N (yi sin (cid:28)i; 12); xij (cid:24) N (0; 12);8j 2 f3; : : : ; dg;

(9)
where N ((cid:22); (cid:27)2) is the normal distribution with mean (cid:22) and variance (cid:27)2. Namely  only the ﬁrst
two dimensions of x differ in two classes  and the remaining d (cid:0) 2 dimensions are considered
In addition  according to the value of (cid:28)i  the means of the class-wise distributions in
as noise.
the ﬁrst two dimensions gradually change. The data distributions of the ﬁrst two dimensions for
(cid:28) = 0; 0:5(cid:25); (cid:25); 1:5(cid:25) are illustrated in Figure 3. Here  we applied our PT feature learning approach
with triangular time windows in (6) with s = 0:25(cid:25). Figure 4 shows the mis-classiﬁcation rate
of PT feature learning (PTFL) and ordinary independent learning (IND) on a similarly generated
test sample with size 1000. When the input dimension d = 2  there is no advantage for learning
common features since these two input dimensions are important for classiﬁcation. On the other
hand  as d increases  PT feature learning becomes more and more advantageous. Especially when
the regularization parameter (cid:13) is large  the independent learning approach is completely deteriorated
as d increases  while PTFL works reasonably well in all the setups.

6

-4-2 0 2 4-4-2 0 2 4 6x2x1-4-2 0 2 4-4-2 0 2 4 6x2x1Figure 3: The ﬁrst 2 input dimensions of artiﬁcial example at (cid:28) = 0; 0:5(cid:25); (cid:25); 1:5(cid:25). The class-wise
distributions in these two dimensions gradually change with (cid:28) 2 [0; 2(cid:25)].

Figure 4: Experimental results on artiﬁcial example under non-stationarity. Mis-classiﬁcation rate
on test sample with size 1000 for various setups d 2 f2; 5; 10; 20; 50; 100g and (cid:13) 2 f0:1; 1; 10g
are shown. The red symbols indicate the results of our PT feature learning (PTFL) whereas the
blue symbols indicate ordinary independent learning (IND). The plotted are average (and standard
deviation) over 100 replications with different random seeds. All the differences except d = 2 are
statistically signiﬁcant (p < 0:01).

Joint Cost-Sensitive SVM Learning on Benchmark Datasets Here  we report the experimental
results on joint cost-sensitive SVM learning discussed in Section 4. Although our main contribution
is not just claiming favorable generalization properties of parametric task learning solutions  we
compared  as an illustration  the generalization performances of PT feature learning (PTFL) and
PT variable selection (PTVS) with the ordinary independent learning approach (IND). In PTFL
and PTVS  we learned common feature subspaces and common sets of variables shared across the
continuum of cost-sensitive SVM for (cid:18) 2 [0:05; 0:95] for 10 benchmark datasets (see Table 1). In
∑
each data set  we divided the entire sample into training  validation  and test sets with almost equal
size. The average test errors (and the standard deviation) of 10 different data splits are reported
in Table 1. The total test errors for cost-sensitive SVMs with (cid:18) = 0:1; 0:2; : : : ; 0:9 are deﬁned
; where f(cid:18) is the
as
trained SVM with the cost ratio (cid:18). Model selection was conducted by using the same criterion on
validation sets. We see that  in most cases  PTFL or PTVS had better generalization performance
than IND.

∑
i:yi=(cid:0)1 I(f(cid:18)(xi) > 0) + (1 (cid:0) (cid:18))

i:yi=1 I(f(cid:18)(xi) (cid:20) 0)

∑

(

(cid:18)2f0:1;:::;0:9g

(cid:18)

)

Joint Quantile Regression Finally  we applied PT feature learning to joint quantile regression
problems. Here  we took a slightly different approach from what was described in the previous
section. Given a training set f(xi; yi)gi2Nn  we ﬁrst estimated conditional mean function E[Y jX =
x] by least-square regression  and computed the residual ri := yi (cid:0) ^E[Y jX = xi]  where ^E is the
estimated conditional mean function. Then  we applied PT feature learning to f(xi; ri)gi2Nn  and
Y jX=x((cid:28) ) := ^E[Y jX = xi] + ^fres(xj(cid:28) )  where
(cid:0)1
estimated the conditional (cid:28) th quantile function as ^F
^fres((cid:1)j(cid:28) ) is the estimated (cid:28) th quantile regression ﬁtted to the residuals.
When multiple quantile regressions with different (cid:28) s are independently learned  we often encounter
a notorious problem known as quantile crossing (see Section 2.5 in [5]). For example  in Figure 5(a) 
some of the estimated conditional quantile functions cross each other (which never happens in the
true conditional quantile functions). One possible approach to mitigate this problem is to assume
a model on the heteroscedastic structure. In the simplest case  if we assume that the data is ho-
moscedastic (i.e.  the conditional distribution P (Y jx) does not depend on x except its location) 

7

 0 0.1 0.2 0.3 0.4 0.525102050100Mis-classification RateInput DimensionPTLIND 0 0.1 0.2 0.3 0.4 0.525102050100Mis-classification RateInput DimensionPTLIND 0 0.1 0.2 0.3 0.4 0.525102050100Mis-classification RateInput DimensionPTLINDTable 1: Average (and standard deviation) of test errors obtained by joint cost-sensitive SVMs on
benchmark datasets. n is the sample size  d is the input dimension  Ind indicates the results when
each cost-sensitive SVM was trained independently  while PTFL and PTVS indicate the results from
PT feature learning and PT feature selection  respectively. The bold numbers in the table indicate
the best performance among three methods.

Data Name
Parkinson

Breast Cancer Diagnostic
Breast Cancer Prognostic

Australian
Diabetes
Fourclass
Germen
Splice

SVM Guide

DVowel

n
195
569
194
690
768
862
1000
1000
300
528

d
20
30
33
14
8
2
24
60
10
10

Ind

32.30 (10.60)
20.36 (7.77)
48.97 (12.92)
117.97 (22.97)
185.90 (21.13)
181.69 (22.13)
242.21 (18.35)
179.80 (24.22)
175.70 (15.55)
175.16 (13.78)

PTFL

30.21 (9.09)
18.49 (6.15)
49.28 ( 9.83)
106.25 (12.66)
179.89 (16.31)
179.30 (14.25)
219.66 (16.22)
151.69 (18.02)
170.16 (9.99)
175.74 (9.37)

PTVS

30.25 (8.53)
19.46 (5.89)
48.68 (5.89)
111.22 (15.95)
175.95 (16.26)
178.67 (19.24)
237.20 (15.78)
183.54 (21.27)
179.76 (14.76)
175.50 (7.38)

quantile regressions at different (cid:28) s can be obtained by just vertically shifting other quantile regres-
sion function (see Figure 5(f)).

Our PT feature learning approach  when applied to the joint quantile regression problem  allows us
to interpolate these two extreme cases. Figure 5 shows a joint QR example on the bone mineral
density (BMD) data [21]. We applied our approach after expanding univariate input x to a d = 5
dimensional vector by using evenly allocated RBFs. When (a) (cid:13) ! 0  our approach is identical
with independently estimating each quantile regression  while it coincides with homoscedastic case
when (f) (cid:13) ! 1. In our experience  the best solution is usually found somewhere between these
two extremes: in this example  (d) (cid:13) = 5 was chosen as the best model by 10-fold cross-validation.

(a) (cid:13) ! 0

(b) (cid:13) = 0:1

(c) (cid:13) = 1

(d) (cid:13) = 5

(e) (cid:13) = 10

(f) (cid:13) ! 1

Figure 5: Joint quantile regression examples on BMD data [21] for six different (cid:13)s.

6 Conclusions

In this paper  we introduced parametric-task learning (PTL) approach that can systematically handle
inﬁnitely many tasks parameterized by a continuous parameter. We illustrated the usefulness of this
approach by providing three examples that can be naturally formulated as PTL. We believe that there
are many other practical problems that falls into this PTL framework.

Acknowledgments

The authors thank the reviewers for fruitful comments. IT  MS  and SN thank the support from
MEXT Kakenhi 23700165  JST CREST Program  MEXT Kakenhi 23120004  respectively.

8

-2-1 0 1 2 3 4-2-1.5-1-0.5 0 0.5 1 1.5 2(Standardized) Relative BMD Change(Standardized) Age0.05  0.10  ...  0.95 conditional quantile functions-2-1 0 1 2 3 4-2-1.5-1-0.5 0 0.5 1 1.5 2(Standardized) Relative BMD Change(Standardized) Age0.05  0.10  ...  0.95 conditional quantile functions-2-1 0 1 2 3 4-2-1.5-1-0.5 0 0.5 1 1.5 2(Standardized) Relative BMD Change(Standardized) Age0.05  0.10  ...  0.95 conditional quantile functions-2-1 0 1 2 3 4-2-1.5-1-0.5 0 0.5 1 1.5 2(Standardized) Relative BMD Change(Standardized) Age0.05  0.10  ...  0.95 conditional quantile functions-2-1 0 1 2 3 4-2-1.5-1-0.5 0 0.5 1 1.5 2(Standardized) Relative BMD Change(Standardized) Age0.05  0.10  ...  0.95 conditional quantile functions-2-1 0 1 2 3 4-2-1.5-1-0.5 0 0.5 1 1.5 2(Standardized) Relative BMD Change(Standardized) Age0.05  0.10  ...  0.95 conditional quantile functionsReferences

[1] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. In Advances in Neural

Information Processing Systems  volume 19  pages 41–48. 2007.

[2] A. Argyriou  C. A. Micchelli  M. Pontil  and Y. Ying. A spectral regularization framework
for multi-task structure learning. In Advances in Neural Information Processing Systems  vol-
ume 20  pages 25–32. 2008.

[3] L. Cao and F. Tay. Support vector machine with adaptive parameters in ﬁnantial time series

forecasting. IEEE Transactions on Neural Networks  14(6):1506–1518  2003.

[4] F. R. Bach  D. Heckerman  and E. Horvits. Considering cost asymmetry in learning classiﬁers.

Journal of Machine Learning Research  7:1713–41  2006.

[5] R. Koenker. Quantile Regression. Cambridge University Press  2005.
[6] K. Ritter. On parametric linear and quadratic programming problems. mathematical Pro-
gramming: Proceedings of the International Congress on Mathematical Programming  pages
307–335  1984.

[7] E. L. Allgower and K. George. Continuation and path following. Acta Numerica  2:1–63 

1993.

[8] T. Gal. Postoptimal Analysis  Parametric Programming  and Related Topics. Walter de

Gruyter  1995.

[9] M. J. Best. An algorithm for the solution of the parametric quadratic programming problem.

Applied Mathemetics and Parallel Computing  pages 57–76  1996.

[10] M. Fazel  H. Hindi  and S. P. Boyd. A rank minimization heuristic with application to minimum
order system approximation. In Proceedings of the American Control Conference  volume 6 
pages 4734–4739  2001.

[11] B. A. Turlach  W. N. Venables  and S. J. Wright. Simultaneous variable selection. Technomet-

rics  47:349–363  2005.

[12] G. Obozinski  B. Taskar  and M. Jordan. Joint covariate selection and joint sbspace selection

for multiple classiﬁcation problems. Statistics and Computing  20(2):231–252  2010.

[13] M. R. Osborne  B. Presnell  and B. A. Turlach. A new approach to variable selection in least

squares problems. IMA Journal of Numerical Analysis  20(20):389–404  2000.

[14] B. Efron and R. Tibshirani. Least angle regression. Annals of Statistics  32(2):407–499  2004.
[15] T. Hastie  S. Rosset  R. Tibshirani  and J. Zhu. The entire regularization path for the support

vector machine. Journal of Machine Learning Research  5:1391–415  2004.

[16] Y. Lin  Y. Lee  and G. Wahba. Support vector machines for classiﬁcation in nonstandard

situations. Machine Learning  46:191–202  2002.

[17] M. A. Davenport  R. G. Baraniuk  and C. D. Scott. Tuning support vector machine for mini-
max and Neyman-Pearson classiﬁcation. IEEE Transactions on Pattern Analysis and Machine
Intelligence  2010.

[18] G. Lee and C. Scott. Nested support vector machines. IEEE Transactions on Signal Processing 

58(3):1648–1660  2010.

[19] R. Koenker. Quantile Regression. Cambridge University Press  2005.
[20] I. Takeuchi  Q. V. Le  T. Sears  and A. J. Smola. Nonparametric quantile estimation. Journal

of Machine Learning Research  7:1231–1264  2006.

[21] L. K. Bachrach  T. Hastie  M. C. Wang  B. Narasimhan  and R. Marcus. Acquisition in healthy
Asian  hispanic  black and caucasian youth. a longitudinal study. The Journal of Clinical
Endocrinology and Metabolism  84:4702–4712  1999.

[22] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

9

,Ichiro Takeuchi
Tatsuya Hongo
Masashi Sugiyama
Shinichi Nakajima