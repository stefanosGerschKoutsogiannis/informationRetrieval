2019,Improving Black-box Adversarial Attacks with a Transfer-based Prior,We consider the black-box adversarial setting  where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model  or based on the query feedback. However  these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems  we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks  which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.,Improving Black-box Adversarial Attacks with a

Transfer-based Prior

Shuyu Cheng∗  Yinpeng Dong∗  Tianyu Pang  Hang Su  Jun Zhu†

Dept. of Comp. Sci. and Tech.  BNRist Center  State Key Lab for Intell. Tech. & Sys. 

Institute for AI  THBI Lab  Tsinghua University  Beijing  100084  China

{chengsy18  dyp17  pty17}@mails.tsinghua.edu.cn  {suhangss  dcszj}@mail.tsinghua.edu.cn

Abstract

We consider the black-box adversarial setting  where the adversary has to gen-
erate adversarial perturbations without access to the target models to compute
gradients. Previous methods tried to approximate the gradient either by using a
transfer gradient of a surrogate white-box model  or based on the query feedback.
However  these methods often suffer from low attack success rates or poor query
efﬁciency since it is non-trivial to estimate the gradient in a high-dimensional space
with limited information. To address these problems  we propose a prior-guided
random gradient-free (P-RGF) method to improve black-box adversarial attacks 
which takes the advantage of a transfer-based prior and the query information
simultaneously. The transfer-based prior given by the gradient of a surrogate model
is appropriately integrated into our algorithm by an optimal coefﬁcient derived by
a theoretical analysis. Extensive experiments demonstrate that our method requires
much fewer queries to attack black-box models with higher success rates compared
with the alternative state-of-the-art methods.

1

Introduction

Although deep neural networks (DNNs) have achieved signiﬁcant success on various tasks [12]  they
have been shown to be vulnerable to adversarial examples [2  33  13]  which are crafted to fool the
models by modifying normal examples with human imperceptible perturbations. Many efforts have
been devoted to studying the generation of adversarial examples  which is crucial to identify the
weaknesses of deep learning algorithms [33  1]  serve as a surrogate to evaluate robustness [5]  and
consequently contribute to the design of robust deep learning models [24].
In general  adversarial attacks can be categorized into white-box attacks and black-box attacks. In
the white-box setting  the adversary has full access to the model  and can use various gradient-based
methods [13  20  5  24] to generate adversarial examples. In the more challenging black-box setting 
the adversary has no or limited knowledge about the model  and crafts adversarial examples without
any gradient information. The black-box setting is more practical in many real-world situations.
Many methods [30  6  3  7  18  27  35  19  9] have been proposed to perform black-box adversarial
attacks. A common idea is to use an approximate gradient instead of the true gradient for crafting
adversarial examples. The approximate gradient could be either the gradient of a surrogate model
(termed as transfer-based attacks) or numerically estimated by the zeroth-order optimization methods
(termed as query-based attacks). In transfer-based attacks  adversarial examples generated for a
different model are probable to remain adversarial for the target model due to the transferability [29].
Although various methods [7  23  8] have been introduced to improve the transferability  the attack
success rate is still unsatisfactory. The reason is that there lacks an adjustment procedure in transfer-
based attacks when the gradient of the surrogate model points to a non-adversarial region of the

∗Equal contribution. †Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

target model. In query-based adversarial attacks  the gradient can be estimated by various methods 
such as ﬁnite difference [6  27]  random gradient estimation [35]  and natural evolution strategy [18].
These methods usually result in a higher attack success rate compared with the transfer-based attack
methods [6  27]  but they require a tremendous number of queries to perform a successful attack. The
inefﬁciency mainly comes from the underutilization of priors  since the current methods are nearly
optimal to estimate the gradient [19].
To address the aforementioned problems and improve black-box attacks  we propose a prior-guided
random gradient-free (P-RGF) method to utilize the transfer-based prior for query-efﬁcient black-
box attacks under the gradient estimation framework. The transfer-based prior is given by the gradient
of a surrogate white-box model  which contains abundant prior knowledge of the true gradient. Our
method provides a gradient estimate by querying the target model with random samples that are biased
towards the transfer gradient and acquiring the corresponding loss values. We provide a theoretical
analysis on deriving the optimal coefﬁcient  which controls the strength of the transfer gradient.
Our method is also ﬂexible to integrate other forms of prior information. As a concrete example 
we incorporate the commonly used data-dependent prior [19] into our algorithm along with the
transfer-based prior. Extensive experiments demonstrate that our method signiﬁcantly outperforms
the previous state-of-the-art methods in terms of black-box attack success rate and query efﬁciency 
which veriﬁes the superiority of our method for black-box adversarial attacks.

2 Background

In this section  we review the background and the related work on black-box adversarial attacks.
2.1 Adversarial setup
Given a classiﬁer C(x) and an input-label pair (x  y)  the goal of attacks is to generate an adversarial
example xadv that is misclassiﬁed while the distance between the adversarial input and the normal
input measured by the (cid:96)p norm is smaller than a preset threshold  as
C(xadv) (cid:54)= y  s.t. (cid:107)xadv − x(cid:107)p ≤ .

(1)
Note that this corresponds to the untargeted attack. We present our framework and algorithm based
on the untargeted attack for clarity  while the extension to the targeted one is straightforward.
An adversarial example can be generated by solving the constrained optimization problem as

xadv = arg max
x(cid:48):(cid:107)x(cid:48)−x(cid:107)p≤

f (x(cid:48)  y) 

(2)

where f is a loss function on top of the classiﬁer C(x)  e.g.  the cross-entropy loss. Many gradient-
based methods [13  20  5  24] have been proposed to solve this optimization problem. The state-of-
the-art projected gradient descent (PGD) [24] iteratively generates adversarial examples as

t + η · gt) 

t

t

t

 y)
 y)(cid:107)2

xadv
t+1 = ΠBp(x )(xadv

(3)
where Π is the projection operation  Bp(x  ) is the (cid:96)p ball centered at x with radius   η is the step
size  and gt is the normalized gradient under the (cid:96)p norm  e.g.  gt = ∇xf (xadv
under the (cid:96)2
(cid:107)∇xf (xadv
norm  and gt = sign(∇xf (xadv
  y)) under the (cid:96)∞ norm. This method requires full access to the
gradient of the target model  which is designed under the white-box attack setting.
2.2 Black-box attacks
The direct access to the model gradient is unrealistic in many real-world applications  where we
need to perform attacks in the black-box manner. We can still adopt the PGD method to generate
adversarial examples  except that the true gradient ∇xf (x  y) is usually replaced by an approximate
gradient. Black-box attacks can be roughly divided into transfer-based attacks and query-based attacks.
Transfer-based attacks adopt the gradient of a surrogate white-box model to generate adversarial
examples  which are probable to fool the black-box model due to the transferability [30  23  7].
Query-based attacks estimate the gradient by the zeroth-order optimization methods  when the loss
values could be accessed through queries. Chen et al. [6] propose to use the symmetric difference
quotient [21] to estimate the gradient at each coordinate as

f (x + σei  y) − f (x − σei  y)

ˆgi =

≈ ∂f (x  y)

∂xi

 

(4)

2σ

2

q(cid:88)

i=1

1
q

f (x + σui  y) − f (x  y)

· ui 

where σ is a small constant  and ei is the i-th unit basis vector. Although query-efﬁcient mechanisms
have been developed [6  27]  the coordinate-wise gradient estimation inherently results in the query
complexity being proportional to the input dimension D  which is prohibitively large with high-
dimensional input space  e.g.  D ≈ 270 000 for ImageNet [31]. To improve query efﬁciency  the
approximated gradient ˆg can be estimated by the random gradient-free (RGF) method [26  11  10] as

σ

ˆg =

ˆgi  where ˆgi =

(5)
i=1 are the random vectors independently sampled from a distribution P on RD  and
where {ui}q
σ is the parameter to control the sampling variance. It is noted that ˆgi → u(cid:62)i ∇xf (x  y) · ui when
σ → 0  which is nearly an unbiased estimator of the gradient [10] when E[uiu(cid:62)i ] = I. ˆg is the
average estimation over q random directions to reduce the variance. The natural evolution strategy
(NES) [18] is another variant of Eq. (5)  which conducts the antithetic sampling over a Gaussian
distribution. Ilyas et al. [19] show that these methods are nearly optimal to estimate the gradient  but
their query efﬁciency could be improved by incorporating informative priors. They identify the time
and data-dependent priors for black-box attacks. Different from the alternative methods  our proposed
transfer-based prior is more effective as shown in the experiments. Moreover  the transfer-based
prior can also be used together with other priors. We demonstrate the ﬂexibility of our algorithm by
incorporating the commonly used data-dependent prior as an example.

2.3 Black-box attacks based on both transferability and queries

There are also several works that adopt both the transferability of adversarial examples and the model
queries for black-box attacks. Papernot et al. [30  29] train a local substitute model to mimic the
black-box model with a synthetic dataset  in which the labels are given by the black-box model
through queries. Then the black-box model is attacked by the adversarial examples generated for the
substitute model based on the transferability. A meta-model [28] can reverse-engineer the black-box
model and predict its attributes (such as architecture  optimization procedure  and training data)
through a sequence of queries. Given the predicted attributes of the black-box model  the attacker
can ﬁnd similar surrogate models  which are better to craft transferable adversarial examples against
the black-box model. These methods all use queries to obtain knowledge of the black-box model 
and train/ﬁnd surrogate models to generate adversarial examples  with the purpose of improving the
transferability. However  we do not optimize the surrogate model  but focus on utilizing the gradient
of a ﬁxed surrogate model to obtain a more accurate gradient estimate.
A recent work [4] also uses the gradient of a surrogate model to improve the efﬁciency of query-based
black-box attacks. This method focuses on a different attack scenario  where the model only provides
the hard-label outputs  but we consider the setting where the loss values could be accessed. Moreover 
this method controls the strength of the transfer gradient by a preset hyperparameter  but we obtain
its optimal value through a theoretical analysis based on the gradient estimation framework. It’s
worth mentioning that a similar but independent work [25] also uses surrogate gradients to improve
zeroth-order optimization  but they did not apply their method to black-box adversarial attacks.

3 Methodology

In this section  we ﬁrst introduce the gradient estimation framework. Then we propose the prior-
guided random gradient-free (P-RGF) algorithm. We further incorporate the data-dependent prior [19]
into our algorithm. We also provide an alternative algorithm for the same purpose in Appendix B.

3.1 Gradient estimation framework

The key challenge in black-box adversarial attacks is to estimate the gradient of a model  which can
be used to conduct gradient-based attacks. In this paper  we aim to estimate the gradient ∇xf (x  y)
of the black-box model f more accurately to improve black-box attacks. We denote the gradient
∇xf (x  y) by ∇f (x) in the following for simplicity. We assume that ∇f (x) (cid:54)= 0 in this paper. The
objective of gradient estimation is to ﬁnd the best estimator  which approximates the true gradient
∇f (x) by reaching the minimum value of the loss function as

ˆg∗ = arg min

L(ˆg) 

(6)

ˆg∈G

3

where ˆg is a gradient estimator given by any estimation algorithm  G is the set of all possible gradient
estimators  and L(ˆg) is a loss function to measure the performance of the estimator ˆg. Speciﬁcally 
we let the loss function of the gradient estimator ˆg be

L(ˆg) = min
b≥0

E(cid:107)∇f (x) − bˆg(cid:107)2
2 

(7)

where the expectation is taken over the randomness of the estimation algorithm to obtain ˆg. The
loss L(ˆg) is the minimum expected squared (cid:96)2 distance between the true gradient ∇f (x) and scaled
estimator bˆg. The previous work [35] also uses the expected squared (cid:96)2 distance E(cid:107)∇f (x) − ˆg(cid:107)2
2 as
the loss function  which is similar to ours. However  the value of this loss function will change with
different magnitude of the estimator ˆg. In generating adversarial examples  the gradient is usually
normalized [13  24]  such that the direction of the gradient estimator  instead of the magnitude  will
affect the performance of attacks. Thus  we incorporate a scaling factor b in Eq. (7) and minimize the
error w.r.t. b  which can neglect the impact of the magnitude on the loss of the estimator ˆg.

3.2 Prior-guided random gradient-free method

In this section  we present the prior-guided random gradient-free (P-RGF) method  which is a variant
of the random gradient-free (RGF) method. Recall that in RGF  the gradient can be estimated via
a set of random vectors {ui}q
i=1 as in Eq. (5) with q being the number of random vectors. Directly
using RGF without prior information will result in poor query efﬁciency as shown in our experiments.
In our method  we propose to sample the random vectors that are biased towards the transfer gradient 
to fully exploit the prior information.
Let v be the normalized transfer gradient of a surrogate model such that (cid:107)v(cid:107)2 = 1  and the cosine
similarity between the transfer gradient and the true gradient be

2 ∇f (x) 

α = v(cid:62)∇f (x) with ∇f (x) = (cid:107)∇f (x)(cid:107)−1

(8)
where ∇f (x) is the (cid:96)2 normalization of the true gradient ∇f (x).1 We assume that α ≥ 0 without
loss of generality  since we can reassign v ← −v when α < 0. Although the true value of α is
unknown  we could estimate it efﬁciently  which will be introduced in Sec. 3.3.
For the RGF estimator ˆg in Eq. (5)  we further assume that the sampling distribution P is deﬁned on
the unit hypersphere in the D-dimensional space  such that the random vectors {ui}q
i=1 drawn from
P satisfy (cid:107)ui(cid:107)2 = 1. Then  we can represent the loss of the RGF estimator by the following theorem.
Theorem 1. (Proof in Appendix A.1) If f is differentiable at x  the loss of the RGF estimator ˆg is

L(ˆg) = (cid:107)∇f (x)(cid:107)2

2 −

lim
σ→0

(9)
where σ is the sampling variance  C = E[uiu(cid:62)i ] with ui being the random vector  (cid:107)ui(cid:107)2 = 1  and q
is the number of random vectors as in Eq. (5).

q )∇f (x)(cid:62)C2∇f (x) + 1

q∇f (x)(cid:62)C∇f (x)

(1 − 1

 

(cid:0)∇f (x)(cid:62)C∇f (x)(cid:1)2

Given the deﬁnition of C  it needs to satisfy two constraints: (1) it should be positive semi-deﬁnite;
(2) its trace should be 1 since Tr(C) = E[Tr(uiu(cid:62)i )] = E[u(cid:62)i ui] = 1. It is noted from Theorem 1
that we can minimize L(ˆg) by optimizing C  i.e.  we can achieve an optimal gradient estimator by
carefully sampling the random vectors ui  yielding an query-efﬁcient adversarial attack.

Speciﬁcally  C can be decomposed as(cid:80)D
and orthonormal eigenvectors of C  and(cid:80)D

i=1 are the eigenvalues
i=1 λi = 1. In our method  since we propose to bias
ui towards v to exploit its prior information  we can specify an eigenvector to be v  and let the
corresponding eigenvalue be a tunable coefﬁcient. For the other eigenvalues  we set them to be equal
since we do not have any prior knowledge about the other eigenvectors. In this case  we let

i=1 λiviv(cid:62)i   where {λi}D

i=1 and {vi}D

C = λvv(cid:62) +

(I − vv(cid:62)) 

(10)

where λ ∈ [0  1] controls the strength of the transfer gradient that the random vectors {ui}q
i=1 are
biased towards. We can easily construct a random vector with unit length while satisfying Eq. (10)
(proof in Appendix A.2) as

1 − λ · (I − vv(cid:62))ξi 
1 We will use e to denote the (cid:96)2 normalization of a vector e in this paper.

λ · v +

ui =

√

√

(11)

1 − λ
D − 1

4

number of queries q; input dimension D.

Algorithm 1 Prior-guided random gradient-free (P-RGF) method
Input: The black-box model f; input x and label y; the normalized transfer gradient v; sampling variance σ;
Output: Estimate of the gradient ∇f (x).
1: Estimate the cosine similarity α = v(cid:62)∇f (x) (detailed in Sec. 3.3);
2: Calculate λ∗ according to Eq. (12) given α  q  and D;
3: if λ∗ = 1 then
return v;
4:
5: end if
6: ˆg ← 0;
7: for i = 1 to q do
8:
9:
10:
11: end for
12: return ∇f (x) ← 1
q

Sample ξi from the uniform distribution on the D-dimensional unit hypersphere;
ui =
ˆg ← ˆg +

1 − λ∗ · (I − vv(cid:62))ξi;
· ui;

f (x + σui  y) − f (x  y)

λ∗ · v +

√

√

ˆg.

σ

where ξi is sampled uniformly from the unit hypersphere. Hereby  the problem turns to optimizing λ
that minimizes L(ˆg). The previous work [35] can also be categorized as a special case of our method
D I  such that the random vectors are drawn from the uniform distribution
when λ = 1
on the hypersphere. When λ ∈ [0  1
D )  it indicates that the transfer gradient is worse than a random
vector  so we are encouraged to search in other directions by using a small λ. To ﬁnd the optimal λ 
we plug Eq. (10) into Eq. (9)  and obtain the closed-form solution (proof in Appendix A.3) as

D and C = 1

0

(1 − α2)(α2(D + 2q − 2) − 1)
2α2Dq − α4D(D + 2q − 2) − 1

1

if α2 ≤

1

D + 2q − 2
1

< α2 <

if
D + 2q − 2
if α2 ≥ 2q − 1
D + 2q − 2

2q − 1

D + 2q − 2

.

(12)



λ∗ =

Remark. It can be proven (in Appendix A.4) that λ∗ is a monotonically increasing function of α2 
and a monotonically decreasing function of q (when α2 > 1
D ). It means that a larger α or a smaller
q (when the transfer gradient is not worse than a random vector) would result in a larger λ∗  which
makes sense since we tend to rely on the transfer gradient more when (1) it approximates the true
gradient better; (2) the number of queries is not enough to provide much gradient information.
We summarize the P-RGF method in Algorithm 1. Note that when λ∗ = 1  we directly return the
transfer gradient as the estimate of ∇f (x)  which can save many queries.

3.3 Estimation of cosine similarity
To complete our algorithm  we also need to estimate α = v(cid:62)∇f (x) = v(cid:62)
  where v is the
normalized transfer gradient. Note that the inner product v(cid:62)∇f (x) can be easily estimated by the
ﬁnite difference method

∇f (x)
(cid:107)∇f (x)(cid:107)2

v(cid:62)∇f (x) ≈ f (x + σv  y) − f (x  y)

σ

 

(13)

using a small σ. Hence  the problem is reduced to estimating (cid:107)∇f (x)(cid:107)2.
Suppose that it is allowed to conduct S queries to estimate (cid:107)∇f (x)(cid:107)2. We ﬁrst draw a different set of
S random vectors {ws}S
s=1 independently and uniformly from the D-dimensional unit hypersphere 
and then estimate w(cid:62)s ∇f (x) using Eq. (13). Suppose that we have a r-degree homogeneous function
g of S variables  i.e.  g(az) = arg(z) where a ∈ R and z ∈ RS  then we have

g(cid:0)W(cid:62)∇f (x)(cid:1) = (cid:107)∇f (x)(cid:107)r

2 · g(cid:0)W(cid:62)∇f (x)(cid:1) 

(14)
where W is the collection of the random vectors as W = [w1  ...  wS]. In this case  the norm of

the gradient (cid:107)∇f (x)(cid:107)2 could be computed easily if both g(cid:0)W(cid:62)∇f (x)(cid:1) and g(cid:0)W(cid:62)∇f (x)(cid:1) are
available. Note that g(cid:0)W(cid:62)∇f (x)(cid:1) can be calculated since each w(cid:62)s ∇f (x) is available.

5

However  it is non-trivial to obtain the value of w(cid:62)s ∇f (x) as well as the function value g(cid:0)W(cid:62)∇f (x)(cid:1).
∇f (x)  thus we can compute the expectation of the function value E(cid:2)g(cid:0)W(cid:62)∇f (x)(cid:1)(cid:3). Based on

Nevertheless  we note that the distribution of w(cid:62)s ∇f (x) is the same regardless of the direction of
that  we use g(W(cid:62)
∇f (x))
E[g(W(cid:62)∇f (x))]
g(z) = 1
s=1 z2
S

2. In particular  we choose g as

s. Then r = 2  and we have

as an unbiased estimator of (cid:107)∇f (x)(cid:107)r

(cid:80)S
E(cid:2)g(cid:0)W(cid:62)∇f (x)(cid:1)(cid:3) = E(cid:2)(w(cid:62)1 ∇f (x))2] = ∇f (x)(cid:62)E[w1w(cid:62)1 ]∇f (x) =
(cid:16) f (x + σws  y) − f (x  y)

By plugging Eq. (15) into Eq. (14)  we can estimate the gradient norm by

S(cid:88)

S(cid:88)

1
D

.

(cid:107)∇f (x)(cid:107)2 ≈

(w(cid:62)s ∇f (x))2 ≈

(cid:118)(cid:117)(cid:117)(cid:116) D

S

(15)

(16)

(cid:17)2

.

(cid:118)(cid:117)(cid:117)(cid:116) D

S

s=1

s=1

σ

To save queries  we estimate the gradient norm periodically instead of in every iteration  since usually
it does not change very fast in the optimization process.

3.4

Incorporating the data-dependent prior

The proposed P-RGF method is generally ﬂexible to integrate other priors. As a concrete example  we
incorporate the commonly used data-dependent prior [19] along with the transfer-based prior into our
algorithm. The data-dependent prior is proposed to reduce query complexity  which suggests that we
can utilize the structure of the inputs to reduce the input-space dimension without sacriﬁcing much
estimation accuracy. This idea has also been adopted in several works [6  35  14  4]. We observe that
many works restrict the adversarial perturbations to lie in a linear subspace of the input space  which
allows the application of our theoretical framework.
Consider the RGF estimator in Eq. (5). To leverage the data-dependent prior  suppose ui = Vξi 
where V = [v1  v2  ...  vd] is a D × d matrix (d < D)  {vj}d
j=1 is an orthonormal basis in the
d-dimensional subspace of the input space  and ξi is a random vector sampled from the d-dimensional
unit hypersphere. When ξi is sampled from the uniform distribution  C = 1
d
Speciﬁcally  we focus on the data-dependent prior in [19]. In this method  the random vector ξi drawn
in Rd is up-sampled to ui in RD by the nearest neighbor algorithm  where d < D. The orthonormal
basis {vj}d
j=1 can be obtained by ﬁrst up-sampling the standard basis in Rd with the same method
and then applying normalization.
Now we consider incorporating the data-dependent prior into our algorithm. Similar to Eq. (10) 
we let one eigenvector of C be v to exploit the transfer-based prior  and the others are given by the
orthonormal basis in the subspace to exploit the data-dependent prior  as

(cid:80)d

i=1 viv(cid:62)i .

C = λvv(cid:62) +

1 − λ
d

d(cid:88)

i=1

viv(cid:62)i .

(17)



λ∗ =

where A2 =(cid:80)d

By plugging Eq. (17) into Eq. (9)  we can also obtain the optimal λ (proof in Appendix A.5) as

0

A2(A2 − α2(d + 2q − 2))

A4 + α4d2 − 2A2α2(q + dq − 1)

1

A2

if α2 ≤
d + 2q − 2
A2

if
d + 2q − 2
if α2 ≥ A2(2q − 1)

< α2 <

d

A2(2q − 1)

d

 

(18)

i=1(v(cid:62)i ∇f (x))2. Note that A should also be estimated. We use a similar method to
the one for estimating α in Sec. 3.3  which is provided in Appendix C.
The remaining problem is to construct a random vector ui satisfying E[uiu(cid:62)i ] = C  with C deﬁned
in Eq. (17). In general  it is difﬁcult since v is not orthogonal to the subspace. To address this issue 
we sample ui in a way that E[uiu(cid:62)i ] is a good approximation of C (explanation in Appendix A.6) 
which is similar to Eq. (11) as

√

λ · v +

√

ui =

1 − λ · (I − vv(cid:62))Vξi 

(19)

6

Figure 1: (a) The crafted adversarial examples for the Inception-v3 [34] model by RGF and our P-RGF w.r.t.
number of queries. We show the cross-entropy loss of each image. The images in the green boxes are successfully
misclassiﬁed  while those in the red boxes are not. (b) The estimation error of gradient norm with different S.
(c) The average cosine similarity between the estimated gradient and the true gradient. The estimate is given
by our method with ﬁxed λ and optimal λ  respectively. (d) The average λ∗ across attack iterations. (e) The
average cosine similarity between the transfer and the true gradients  and that between the estimated and the true
gradients  across attack iterations.

where ξi is sampled uniformly from the d-dimensional unit hypersphere.
Our algorithm with the data-dependent prior is similar to Algorithm 1. We ﬁrst estimate α and A 
and then calculate λ∗ by Eq. (18). If λ∗ = 1  we use the transfer gradient v as the estimate. If not 
we sample q random vectors by Eq. (19) and obtain the gradient estimation by Eq. (5).

4 Experiments

In this section  we present the experimental results to demonstrate the effectiveness of the proposed
method on attacking black-box classiﬁers.2 We perform untargeted attacks under both the (cid:96)2 and (cid:96)∞
norms on the ImageNet dataset [31]. We choose 1 000 images randomly from the validation set for
evaluation. Due to the space limitation  we leave the results based on the (cid:96)∞ norm in Appendix D.
The results for both norms are consistent. For all experiments  we use the ResNet-152 model [17] as
the surrogate model to generate the transfer gradient. We apply the PGD algorithm [24] to generate
adversarial examples with the estimated gradient given by each method. We set the perturbation size
0.001 · D and the learning rate as η = 2 in PGD under the (cid:96)2 norm  with images in [0  1].
as  =

√

4.1 Performance of gradient estimation

In this section  we conduct several experiments to show the performance of gradient estimation. All
experiments in this section are performed on the Inception-v3 [34] model.
First  we show the performance of gradient norm estimation in Sec. 3.3. The gradient norm (or cosine
similarity) is easier to estimate than the true gradient since it’s a scalar value. Fig. 1(b) shows the esti-

mation error of the gradient norm  deﬁned as the (normalized) RMSE—
w.r.t. the number of queries S  where (cid:107)∇f (x)(cid:107)2 is the true norm and (cid:92)(cid:107)∇f (x)(cid:107)2 is the estimated one.
We choose S = 10 in all experiments to reduce the number of queries while the estimation error is

(cid:107)∇f (x)(cid:107)2−(cid:107)∇f (x)(cid:107)2

(cid:114)
E(cid:0) (cid:92)

(cid:107)∇f (x)(cid:107)2

(cid:1)2 

2Our code is available at: https://github.com/thu-ml/Prior-Guided-RGF.

7

Loss=0.01Loss=0.02Loss=0.09Loss=0.01Loss=0.04Loss=2.28Loss=2.380100RGFP-RGF50010002000QueriesLoss=0.32Loss=7.08Loss=14.92(a)(b)(c)(d)(e)Table 1: The experimental results of black-box attacks against Inception-v3  VGG-16  and ResNet-50 under
the (cid:96)2 norm. We report the attack success rate (ASR) and the average number of queries (AVG. Q) needed to
generate an adversarial example over successful attacks.

Inception-v3

AVG. Q

VGG-16

AVG. Q

ResNet-50

AVG. Q

Methods
NES [18]
BanditsT [19]
BanditsTD [19]
AutoZoom [35]
RGF
P-RGF (λ = 0.5)
P-RGF (λ = 0.05)
P-RGF (λ∗)
RGFD
P-RGFD (λ = 0.5)
P-RGFD (λ = 0.05)
P-RGFD (λ∗)

ASR
95.5%
92.4%
97.2%
85.4%
97.7%
96.5%
97.8%
98.1%
99.1%
98.2%
99.1%
99.1%

1718
1560
874
2443
1309
1119
1021
745
910
1047
754
649

ASR
98.7%
94.0%
94.9%
96.2%
99.8%
97.3%
99.7%
99.8%
100.0%
99.3%
99.9%
99.7%

1081
584
278
1589
935
1075
888
521
464
917
482
370

ASR
98.4%
96.2%
96.8%
94.8%
99.5%
98.3%
99.6%
99.6%
99.8%
99.3%
99.6%
99.6%

969
1076
512
2065
809
990
790
452
521
893
526
352

Figure 2: We show the average number of queries per successful image at any desired success rate.

acceptable. We also estimate the gradient norm every 10 attack iterations in all experiments to reduce
the number of queries  since usually its value is relatively stable in the optimization process.
Second  we verify the effectiveness of the derived optimal λ (i.e.  λ∗) in Eq. (12) for gradient
estimation  compared with any ﬁxed λ ∈ [0  1]. We perform attacks against Inception-v3 using P-
RGF with λ∗  and calculate the cosine similarity between the estimated gradient and the true gradient.
We calculate λ∗ using the estimated α in Sec. 3.3 instead of its true value. Meanwhile  along the
PGD updates  we also use ﬁxed λ to get gradient estimates  and calculate the corresponding cosine
similarities. Note that λ∗ does not correspond to any ﬁxed value  since it varies during iterations.
The average cosine similarities of different values of λ are shown in Fig. 1(c). It can be observed
that when a suitable value of λ is chosen  the proposed P-RGF provides a better gradient estimate
D ≈ 0) and the transfer
than both the original RGF method with uniform distribution (when λ = 1
gradient (when λ = 1). Adopting λ∗ brings further improvement upon any ﬁxed λ  demonstrating
the applicability of our theoretical framework.
Finally  we show the average λ∗ over all images w.r.t. attack iterations in Fig 1(d). It shows that λ∗
decreases along with the iterations. Fig. 1(e) shows the average cosine similarity between the transfer
and the true gradients  and that between the estimated and the true gradients  across iterations. The
results show that the transfer gradient is useful at beginning  and becomes less useful along with the
iterations. However  the estimated gradient can remain higher cosine similarity with the true gradient 
which facilitates the adversarial attacks consequently. The results also prove that we need to use the
adaptive λ∗ in different attack iterations.

4.2 Results of black-box attacks on normal models

In this section  we perform attacks against three normally trained models  which are Inception-v3 [34] 
VGG-16 [32]  and ResNet-50 [16]. We compare the proposed prior-guided random gradient-free
(P-RGF) method with two baseline methods. The ﬁrst is the original RGF method with uniform
sampling. The second is the P-RGF method with a ﬁxed λ  which is set to 0.5 or 0.05. In these

8

⇤⇤⇤(a) Inception-v3(b) VGG-16(c) ResNet-50Table 2: The experimental results of black-box attacks against JPEG compression [15]  randomization [37]  and
guided denoiser [22] under the (cid:96)2 norm. We report the attack success rate (ASR) and the average number of
queries (AVG. Q) needed to generate an adversarial example over successful attacks.

Methods
NES [18]
SPSA [36]
RGF
P-RGF
RGFD
P-RGFD

AVG. Q

JPEG Compression [15]
ASR
47.3%
40.0%
41.5%
61.4%
70.4%
81.1%

3114
2744
3126
2419
2828
2120

AVG. Q

AVG. Q

Randomization [37] Guided Denoiser [22]
ASR
23.2%
9.6%
19.5%
60.4%
54.9%
82.3%

ASR
48.0%
46.0%
50.3%
51.4%
83.7%
89.6%

3632
3256
3259
2153
2819
1816

3633
3526
3569
2858
2230
1784

methods  we set the number of queries as q = 50  and the sampling variance as σ = 0.0001·√
D. We
also incorporate the data-dependent prior into these three methods for comparison (which are denoted
by adding a subscript “D”). We set the dimension of the subspace as d = 50 × 50 × 3. Besides  our
method is compared with the state-of-the-art attack methods  including the natural evolution strategy
(NES) [18]  bandit optimization methods (BanditsT and BanditsTD) [19]  and AutoZoom [35]. For
all methods  we restrict the maximum number of queries for each image to be 10 000. We report a
successful attack if a method can generate an adversarial example within 10 000 queries and the size
of perturbation is smaller than the budget (i.e.   =
We show the success rate of black-box attacks and the average number of queries needed to generate
an adversarial example over the successful attacks in Table 1. It can be seen that our method generally
leads to higher attack success rates and requires much fewer queries than other methods. Using a
ﬁxed λ cannot give a satisfactory result  which demonstrates the necessity of using the optimal λ
in our method. The results also show that the data-dependent prior is orthogonal to the proposed
transfer-based prior  since integrating the data-dependent prior leads to better results. We show an
example of attacks in Fig. 1(a). Fig. 2 shows the average number of queries over successful images
by reaching a desired success rate. Our method is much more query-efﬁcient than baseline methods.

0.001 · D).

√

4.3 Results of black-box attacks on defensive models

We further validate the effectiveness of the proposed method on attacking several defensive models 
including JPEG compression [15]  randomization [37]  and guided denoiser [22]. We utilize the
Inception-v3 model as the backbone classiﬁer for the JPEG compression and randomization defenses.
We compare P-RGF with RGF  NES [18]  and SPSA [36]. The experimental settings are the same
with those of attacking the normal models in Sec. 4.2. In our method  we use a smoothed version of
the transfer gradient [8] as the transfer-based prior for black-box attacks  since the smoothed transfer
gradient is better to defeat defensive models. The results in Table 2 also demonstrate the superiority
of our method for attacking the defensive models. Our method leads to much higher attack success
rates than other methods (20% ∼ 40% improvements in many cases)  and also reduces the query
complexity.

5 Conclusion

In this paper  we proposed a prior-guided random gradient-free (P-RGF) method to utilize the transfer-
based prior for improving black-box adversarial attacks. Our method appropriately integrated the
transfer gradient of a surrogate white-box model by the derived optimal coefﬁcient. The experimental
results consistently demonstrate the effectiveness of our method  which requires much fewer queries
to attack black-box models with higher success rates.

Acknowledgements

This work was supported by the National Key Research and Development Program of China (No.
2017YFA0700904)  NSFC Projects (Nos. 61620106010  61621136008  61571261)  Beijing NSF
Project (No. L172037)  Beijing Academy of Artiﬁcial Intelligence (BAAI)  Tiangong Institute for
Intelligent Computing  the JP Morgan Faculty Research Program and the NVIDIA NVAIL Program
with GPU/DGX Acceleration.

9

References
[1] Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In International Conference on Machine Learning
(ICML)  2018.

[2] Battista Biggio  Igino Corona  Davide Maiorca  Blaine Nelson  Pavel Laskov  Giorgio Giacinto  and Fabio
Roli. Evasion attacks against machine learning at test time. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases  pages 387–402  2013.

[3] Wieland Brendel  Jonas Rauber  and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In International Conference on Learning Representations
(ICLR)  2018.

[4] Thomas Brunner  Frederik Diehl  Michael Truong Le  and Alois Knoll. Guessing smart: Biased sampling
for efﬁcient black-box adversarial attacks. In The IEEE International Conference on Computer Vision
(ICCV)  2019.

[5] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy  2017.

[6] Pin Yu Chen  Huan Zhang  Yash Sharma  Jinfeng Yi  and Cho Jui Hsieh. Zoo: Zeroth order optimization
based black-box attacks to deep neural networks without training substitute models. In ACM Workshop on
Artiﬁcial Intelligence and Security (AISec)  pages 15–26  2017.

[7] Yinpeng Dong  Fangzhou Liao  Tianyu Pang  Hang Su  Jun Zhu  Xiaolin Hu  and Jianguo Li. Boosting
adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2018.

[8] Yinpeng Dong  Tianyu Pang  Hang Su  and Jun Zhu. Evading defenses to transferable adversarial examples
by translation-invariant attacks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2019.

[9] Yinpeng Dong  Hang Su  Baoyuan Wu  Zhifeng Li  Wei Liu  Tong Zhang  and Jun Zhu. Efﬁcient decision-
based black-box adversarial attacks on face recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  2019.

[10] John C Duchi  Michael I Jordan  Martin J Wainwright  and Andre Wibisono. Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory 
61(5):2788–2806  2015.

[11] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[12] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016. http:

//www.deeplearningbook.org.

[13] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial examples.

In International Conference on Learning Representations (ICLR)  2015.

[14] Chuan Guo  Jared S Frank  and Kilian Q Weinberger. Low frequency adversarial perturbation. arXiv

preprint arXiv:1809.08758  2018.

[15] Chuan Guo  Mayank Rana  Moustapha Cisse  and Laurens Van Der Maaten. Countering adversarial images

using input transformations. In International Conference on Learning Representations (ICLR)  2018.

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

[17] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual networks.

In European Conference on Computer Vision (ECCV)  2016.

[18] Andrew Ilyas  Logan Engstrom  Anish Athalye  and Jessy Lin. Black-box adversarial attacks with limited

queries and information. In International Conference on Machine Learning (ICML)  2018.

[19] Andrew Ilyas  Logan Engstrom  and Aleksander Madry. Prior convictions: Black-box adversarial attacks

with bandits and priors. In International Conference on Learning Representations (ICLR)  2019.

[20] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial examples in the physical world. The

International Conference on Learning Representations (ICLR) Workshops  2017.

10

[21] Peter D Lax and Maria Shea Terrell. Calculus with applications. Springer  2014.

[22] Fangzhou Liao  Ming Liang  Yinpeng Dong  Tianyu Pang  Xiaolin Hu  and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)  2018.

[23] Yanpei Liu  Xinyun Chen  Chang Liu  and Dawn Song. Delving into transferable adversarial examples and

black-box attacks. In International Conference on Learning Representations (ICLR)  2017.

[24] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learning
Representations (ICLR)  2018.

[25] Niru Maheswaranathan  Luke Metz  George Tucker  Dami Choi  and Jascha Sohl-Dickstein. Guided
evolutionary strategies: Augmenting random search with surrogate gradients. In International Conference
on Machine Learning (ICML)  2019.

[26] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Founda-

tions of Computational Mathematics  17(2):527–566  2017.

[27] Arjun Nitin Bhagoji  Warren He  Bo Li  and Dawn Song. Practical black-box attacks on deep neural
networks using efﬁcient query mechanisms. In European Conference on Computer Vision (ECCV)  2018.

[28] Seong Joon Oh  Max Augustin  Bernt Schiele  and Mario Fritz. Towards reverse-engineering black-box

neural networks. In International Conference on Learning Representations (ICLR)  2018.

[29] Nicolas Papernot  Patrick McDaniel  and Ian Goodfellow. Transferability in machine learning: from

phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277  2016.

[30] Nicolas Papernot  Patrick McDaniel  Ian Goodfellow  Somesh Jha  Z Berkay Celik  and Ananthram Swami.
Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security  2017.

[31] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision  115(3):211–252  2015.

[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. In International Conference on Learning Representations (ICLR)  2015.

[33] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfellow 
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations (ICLR)  2014.

[34] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)  2016.

[35] Chun-Chen Tu  Paishun Ting  Pin-Yu Chen  Sijia Liu  Huan Zhang  Jinfeng Yi  Cho-Jui Hsieh  and Shin-
Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box
neural networks. In Proceedings of the Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI) 
2019.

[36] Jonathan Uesato  Brendan O’Donoghue  Aaron van den Oord  and Pushmeet Kohli. Adversarial risk and
the dangers of evaluating against weak attacks. In International Conference on Machine Learning (ICML) 
2018.

[37] Cihang Xie  Jianyu Wang  Zhishuai Zhang  Zhou Ren  and Alan Yuille. Mitigating adversarial effects

through randomization. In International Conference on Learning Representations (ICLR)  2018.

11

,Shuyu Cheng
Yinpeng Dong
Tianyu Pang
Hang Su
Jun Zhu