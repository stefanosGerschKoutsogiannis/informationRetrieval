2017,Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach,Learning cooperative policies for multi-agent systems is often challenged by partial observability and a lack of coordination. In some settings  the structure of a problem allows a distributed solution with limited communication. Here  we consider a scenario where no communication is available  and instead we learn local policies for all agents that collectively mimic the solution to a centralized multi-agent static optimization problem. Our main contribution is an information theoretic framework based on rate distortion theory which facilitates analysis of how well the resulting fully decentralized policies are able to reconstruct the optimal solution. Moreover  this framework provides a natural extension that addresses which nodes an agent should communicate with to improve the  performance of its individual policy.,Fully Decentralized Policies for Multi-Agent Systems:

An Information Theoretic Approach

Electrical Engineering and Computer Science

Electrical Engineering and Computer Science

Roel Dobbe∗

David Fridovich-Keil∗

University of California  Berkeley

Berkeley  CA 94720

dobbe@eecs.berkeley.edu

University of California  Berkeley

Berkeley  CA 94720

dfk@eecs.berkeley.edu

Claire Tomlin

Electrical Engineering and Computer Science

University of California  Berkeley

Berkeley  CA 94720

tomlin@eecs.berkeley.edu

Abstract

Learning cooperative policies for multi-agent systems is often challenged by partial
observability and a lack of coordination. In some settings  the structure of a problem
allows a distributed solution with limited communication. Here  we consider a
scenario where no communication is available  and instead we learn local policies
for all agents that collectively mimic the solution to a centralized multi-agent static
optimization problem. Our main contribution is an information theoretic framework
based on rate distortion theory which facilitates analysis of how well the resulting
fully decentralized policies are able to reconstruct the optimal solution. Moreover 
this framework provides a natural extension that addresses which nodes an agent
should communicate with to improve the performance of its individual policy.

1

Introduction

Finding optimal decentralized policies for multiple agents is often a hard problem hampered by
partial observability and a lack of coordination between agents. The distributed multi-agent problem
has been approached from a variety of angles  including distributed optimization [Boyd et al.  2011] 
game theory [Aumann and Dreze  1974] and decentralized or networked partially observable Markov
decision processes (POMDPs) [Oliehoek and Amato  2016  Goldman and Zilberstein  2004  Nair
et al.  2005]. In this paper  we analyze a different approach consisting of a simple learning scheme to
design fully decentralized policies for all agents that collectively mimic the solution to a common
optimization problem  while having no access to a global reward signal and either no or restricted
access to other agents’ local state. This algorithm is a generalization of that proposed in our prior
work [Sondermeijer et al.  2016] related to decentralized optimal power ﬂow (OPF). Indeed  the
success of regression-based decentralization in the OPF domain motivated us to understand when and
how well the method works in a more general decentralized optimal control setting.
The key contribution of this work is to view decentralization as a compression problem  and then
apply classical results from information theory to analyze performance limits. More speciﬁcally  we
treat the ith agent’s optimal action in the centralized problem as a random variable u∗i   and model
its conditional dependence on the global state variables x = (x1  . . .   xn)  i.e. p(u∗i |x)  which we

∗Indicates equal contribution.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

assume to be stationary in time. We now restrict each agent i to observe only the ith state variable
xi. Rather than solving this decentralized problem directly  we train each agent to replicate what it
would have done with full information in the centralized case. That is  the vector of state variables
x is compressed  and the ith agent must decompress xi to compute some estimate ˆui ≈ u∗i . In our
approach  each agent learns a parameterized Markov control policy ˆui = ˆπi(xi) via regression. The
ˆπi are learned from a data set containing local states xi taken from historical measurements of system
state x and corresponding optimal actions u∗i computed by solving an ofﬂine centralized optimization
problem for each x.
In this context  we analyze the fundamental limits of compression. In particular  we are interested
in unraveling the relationship between the dependence structure of u∗i and x and the corresponding
ability of an agent with partial information to approximate the optimal solution  i.e. the difference –
or distortion – between decentralized action ˆui = ˆπi(xi) and u∗i . This type of relationship is well
studied within the information theory literature as an instance of rate distortion theory [Cover and
Thomas  2012  Chapter 13]. Classical results in this ﬁeld provide a means of ﬁnding a lower bound on
the expected distortion as a function of the mutual information – or rate of communication – between
u∗i and xi. This lower bound is valid for each speciﬁed distortion metric  and for any arbitrary strategy
of computing ˆui from available data xi. Moreover  we are able to leverage a similar result to provide
a conceptually simple algorithm for choosing a communication structure – letting the regressor ˆπi
depend on some other local states xj(cid:54)=i – in such a way that the lower bound on expected distortion
is minimized. As such  our method generalizes [Sondermeijer et al.  2016] and provides a novel
approach for the design and analysis of regression-based decentralized optimal policies for general
multi-agent systems. We demonstrate these results on synthetic examples  and on a real example
drawn from solving OPF in electrical distribution grids.

2 Related Work

Decentralized control has long been studied within the system theory literature  e.g. [Lunze  1992 
Siljak  2011]. Recently  various decomposition based techniques have been proposed for distributed
optimization based on primal or dual decomposition methods  which all require iterative computation
and some form of communication with either a central node [Boyd et al.  2011] or neighbor-to-
neighbor on a connected graph [Pu et al.  2014  Raffard et al.  2004  Sun et al.  2013]. Distributed
model predictive control (MPC) optimizes a networked system composed of subsystems over a time
horizon  which can be decentralized (no communication) if the dynamic interconnections between
subsystems are weak in order to achieve closed-loop stability as well as performance [Christoﬁdes
et al.  2013]. The work of Zeilinger et al. [2013] extended this to systems with strong coupling by
employing time-varying distributed terminal set constraints  which requires neighbor-to-neighbor
communication. Another class of methods model problems in which agents try to cooperate on
a common objective without full state information as a decentralized partially observable Markov
decision process (Dec-POMDP) [Oliehoek and Amato  2016]. Nair et al. [2005] introduce networked
distributed POMDPs  a variant of the Dec-POMDP inspired in part by the pairwise interaction
paradigm of distributed constraint optimization problems (DCOPs).
Although the speciﬁc algorithms in these works differ signiﬁcantly from the regression-based de-
centralization scheme we consider in this paper  a larger difference is in problem formulation. As
described in Sec. 3  we study a static optimization problem repeatedly solved at each time step. Much
prior work  especially in optimal control (e.g. MPC) and reinforcement learning (e.g. Dec-POMDPs) 
poses the problem in a dynamic setting where the goal is to minimize cost over some time horizon.
In the context of reinforcement learning (RL)  the time horizon can be very long  leading to the
well known tradeoff between exploration and exploitation; this does not appear in the static case.
Additionally  many existing methods for the dynamic setting require an ongoing communication
strategy between agents – though not all  e.g. [Peshkin et al.  2000]. Even one-shot static problems
such as DCOPs tend to require complex communication strategies  e.g. [Modi et al.  2005].
Although the mathematical formulation of our approach is rather different from prior work  the
policies we compute are similar in spirit to other learning and robotic techniques that have been
proposed  such as behavioral cloning [Sammut  1996] and apprenticeship learning [Abbeel and Ng 
2004]  which aim to let an agent learn from examples. In addition  we see a parallel with recent
work on information-theoretic bounded rationality [Ortega et al.  2015] which seeks to formalize
decision-making with limited resources such as the time  energy  memory  and computational effort

2

(a) Distributed multi-agent problem.

(b) Graphical model of dependency structure.

Figure 1: (a) shows a connected graph corresponding to a distributed multi-agent system. The circles
denote the local state xi of an agent  the dashed arrow denotes its action ui  and the double arrows
denote the physical coupling between local state variables. (b) shows the Markov Random Field
(MRF) graphical model of the dependency structure of all variables in the decentralized learning
problem. Note that the state variables xi and the optimal actions u∗i form a fully connected undirected
network  and the local policy ˆui only depends on the local state xi.

allocated for arriving at a decision. Our work is also related to swarm robotics [Brambilla et al. 
2013]  as it learns simple rules aimed to design robust  scalable and ﬂexible collective behaviors for
coordinating a large number of agents or robots.

3 General Problem Formulation

Consider a distributed multi-agent problem deﬁned by a graph G = (N  E)  with N denoting the
nodes in the network with cardinality |N| = N  and E representing the set of edges between
nodes. Fig. 1a shows a prototypical graph of this sort. Each node has a real-valued state vector
xi ∈ Rαi   i ∈ N . A subset of nodes C ⊂ N   with cardinality |C| = C  are controllable and
x = (xi  . . .   xN )(cid:62) ∈ R(cid:80)
hence are termed “agents.” Each of these agents has an action variable ui ∈ Rβi   i ∈ C. Let
i∈C βi = U
the stacked network optimization variable. Physical constraints such as spatial coupling are captured
through equality constraints g(x  u) = 0. In addition  the system is subject to inequality constraints
h(x  u) ≤ 0 that incorporate limits due to capacity  safety  robustness  etc. We are interested
in minimizing a convex scalar function fo(x  u) that encodes objectives that are to be pursued
cooperatively by all agents in the network  i.e. we want to ﬁnd

i∈N αi = X denote the full network state vector and u ∈ R(cid:80)

u∗ = arg min
u
s.t.

fo(x  u)  
g(x  u) = 0 

h(x  u) ≤ 0.

(1)

Note that (1) is static in the sense that it does not consider the future evolution of the state x or the
corresponding future values of cost fo. We apply this static problem to sequential control tasks by
repeatedly solving (1) at each time step. Note that this simpliﬁcation from an explicitly dynamic
problem formulation (i.e. one in which the objective function incorporates future costs) is purely for
ease of exposition and for consistency with the OPF literature as in [Sondermeijer et al.  2016]. We
could also consider the optimal policy which solves a dynamic optimal control or RL problem and
the decentralized learning step in Sec. 3.1 would remain the same.
Since (1) is static  applying the learned decentralized policies repeatedly over time may lead to
dynamical instability. Identifying when this will and will not occur is a key challenge in verifying the
regression-based decentralization method  however it is beyond the scope of this work.

3.1 Decentralized Learning

We interpret the process of solving (1) as applying a well-deﬁned function or stationary Markov
policy π∗ : X −→ U that maps an input collective state x to the optimal collective control or action
u∗. We presume that this solution exists and can be computed ofﬂine. Our objective is to learn C
decentralized policies ˆui = ˆπi(xi)  one for each agent i ∈ C  based on T historical measurements
of the states {x[t]}T
t=1 and the ofﬂine computation of the corresponding optimal actions {u∗[t]}T
t=1.
Although each policy ˆπi individually aims to approximate u∗i based on local state xi  we are able

3

x1x2x3x4x5x6u2u5u6uC*û1ûiûCui*u1*xjxNx1Figure 2: A ﬂow diagram explaining the key steps of the decentralized regression method  depicted
for the example system in Fig. 1a. We ﬁrst collect data from a multi-agent system  and then solve
the centralized optimization problem using all the data. The data is then split into smaller training
and test sets for all agents to develop individual decentralized policies ˆπi(xi) that approximate the
optimal solution of the centralized problem. These policies are then implemented in the multi-agent
system to collectively achieve a common global behavior.

to reason about how well their collective action can approximate π∗. Figure 2 summarizes the
decentralized learning setup.
More formally  we describe the dependency structure of the individual policies ˆπi : Rαi −→ Rβi
with a Markov Random Field (MRF) graphical model  as shown in Fig. 1b. The ˆui are only allowed
to depend on local state xi while the u∗i may depend on the full state x. With this model  we can
determine how information is distributed among different variables and what information-theoretic
constraints the policies {ˆπi}i∈C are subject to when collectively trying to reconstruct the centralized
policy π∗. Note that although we may refer to π∗ as globally optimal  this is not actually required
for us to reason about how closely the ˆπi approximate π∗. That is  our analysis holds even if (1) is
solved using approximate methods. In a dynamical reformulation of (1)  for example  π∗ could be
generated using techniques from deep RL.

3.2 A Rate-Distortion Framework

We approach the problem of how well the decentralized policies ˆπi can perform in theory from
the perspective of rate distortion. Rate distortion theory is a sub-ﬁeld of information theory which
provides a framework for understanding and computing the minimal distortion incurred by any given
compression scheme. In a rate distortion context  we can interpret the fact that the output of each
individual policy ˆπi depends only on the local state xi as a compression of the full state x. For a
detailed overview  see [Cover and Thomas  2012  Chapter 10]. We formulate the following variant of
the the classical rate distortion problem

D∗ = min
p(ˆu|u∗)
s.t.

E [d(ˆu  u∗)]  
I(ˆui; u∗j ) ≤ I(xi; u∗j ) (cid:44) γij  
I(ˆui; ˆuj) ≤ I(xi; xj) (cid:44) δij ∀i  j ∈ C  

(2)

where I(· ·) denotes mutual information and d(· ·) an arbitrary non-negative distortion measure. As
usual  the minimum distortion between random variable u∗ and its reconstruction ˆu may be found by
minimizing over conditional distributions p(ˆu|u∗).
The novelty in (2) lies in the structure of the constraints. Typically  D∗ is written as a function D(R) 
where R is the maximum rate or mutual information I(ˆu; u∗). From Fig. 1b however  we know that
pairs of reconstructed and optimal actions cannot share more information than is contained in the
intermediate nodes in the graphical model  e.g. ˆu1 and u∗1 cannot share more information than x1 and
u∗1. This is a simple consequence of the data processing inequality [Cover and Thomas  2012  Thm.
2.8.1]. Similarly  the reconstructed optimal actions at two different nodes cannot be more closely
related than the measurements xi’s from which they are computed. The resulting constraints are ﬁxed
by the joint distribution of the state x and the optimal actions u∗. That is  they are fully determined
by the structure of the optimization problem (1) that we wish to solve.

4

DecentralizedLearningDecentralizedLearningDecentralized LearningMulti-Agent SystemLocal training sets264x⇤1...x⇤6375 24u⇤2u⇤5u⇤635264x1...x6375 24u2u5u635ˆu2=ˆ⇡2(x2)ˆu5=ˆ⇡5(x5)ˆu6=ˆ⇡6(x6)Data gatheringOptimal dataLocal policiesapproximateCentralized Optimization{x2[t] u⇤2[t]}Tt=1We emphasize that we have made virtually no assumptions about the distortion function. For the
remainder of this paper  we will measure distortion as the deviation between ˆui and u∗i . However 
we could also deﬁne it to be the suboptimality gap fo(x  ˆu) − fo(x  u∗)  which may be much
more complicated to compute. This deﬁnition could allow us to reason explicitly about the cost of
decentralization  and it could address the valid concern that the optimal decentralized policy may
bear no resemblance to π∗. We leave further investigation for future work.

3.3 Example: Squared Error  Jointly Gaussian

To provide more intuition into the rate distortion framework  we consider an idealized example in
which the xi  ui ∈ R1. Let d(ˆu  u∗) = (cid:107)ˆu − u∗(cid:107)2
2 be the squared error distortion measure  and
assume the state x and optimal actions u∗ to be jointly Gaussian. These assumptions allow us to
derive an explicit formula for the optimal distortion D∗ and corresponding regression policies ˆπi.
We begin by stating an identity for two jointly Gaussian X  Y ∈ R with correlation ρ: I(X; Y ) ≤
γ ⇐⇒ ρ2 ≤ 1 − e−2γ   which follows immediately from the deﬁnition of mutual information
to be the correlation
and the formula for the entropy of a Gaussian random variable. Taking ρˆui u∗
between ˆui and u∗i   σ2
to be the variances of ˆui and u∗i respectively  and assuming that u∗i
ˆui
and ˆui are of equal mean (unbiased policies ˆπi)  we can show that the minimum distortion attainable
is

and σ2
u∗

i

i

D∗ = min
p(ˆu|u∗)

2(cid:3) : ρ2
E(cid:2)(cid:107)u∗ − ˆu(cid:107)2
i } {σ ˆui}(cid:88)i (cid:16)σ2

u∗

i

min

ˆui u∗

i ≤ 1 − e−2γii = ρ2
i  xi ∀i ∈ C  
u∗
i σˆui(cid:17) : ρ2
i ≤ ρ2
ˆui − 2ρˆui u∗
u∗
i  xi  

i σu∗

ˆui u∗

+ σ2

(3)

(4)

(5)

(6)

=

{ρ ˆui u∗

= min

{σ ˆui}(cid:88)i (cid:16)σ2
=(cid:88)i

σ2
u∗

i

(1 − ρ2
u∗
i  xi) .

+ σ2

u∗

i

ˆui − 2ρu∗

i  xiσu∗

i σˆui(cid:17)  

i

In (4)  we have solved for the optimal correlations ρˆui u∗
. Unsurprisingly  the optimal value turns out
to be the maximum allowed by the mutual information constraint  i.e. ˆui should be as correlated to
u∗i as possible  and in particular as much as u∗i is correlated to xi. Similarly  in (5) we solve for the
optimal σˆui  with the result that at optimum  σˆui = ρu∗
. This means that as the correlation
between the local state xi and the optimal action u∗i decreases  the variance of the estimated action ˆui
decreases as well. As a result  the learned policy will increasingly “bet on the mean” or “listen less”
to its local measurement to approximate the optimal action.
Moreover  we may also provide a closed form expression for the regressor which achieves the
minimum distortion D∗. Since we have assumed that each u∗i and the state x are jointly Gaussian  we
may write any u∗i as an afﬁne function of xi plus independent Gaussian noise. Thus  the minimum
mean squared estimator is given by the conditional expectation

i  xiσu∗

i

ˆui = ˆπi(xi) = E [u∗i |xi] = E [u∗i ] +

ρu∗

i xiσu∗
σxi

i

(xi − E [xi]) .

(7)

Thus  we have found a closed form expression for the best regressor ˆπi to predict u∗i from only xi in
the joint Gaussian case with squared error distortion. This result comes as a direct consequence of
knowing the true parameterization of the joint distribution p(u∗  x) (in this case  as a Gaussian).

3.4 Determining Minimum Distortion in Practice
Often in practice  we do not know the parameterization p(u∗|x) and hence it may be intractable to
determine D∗ and the corresponding decentralized policies ˆπi. However  if one can assume that
p(u∗|x) belongs to a family of parameterized functions (for instance universal function approximators
such as deep neural networks)  then it is theoretically possible to attain or at least approach minimum
distortion for arbitrary non-negative distortion measures.
Practically  one can compute the mutual information constraint I(u∗i   xi) from (2) to understand
how much information a regressor ˆπi(xi) has available to reconstruct u∗i . In the Gaussian case  we
were able to compute this mutual information in closed form. For data from general distributions

5

however  there is often no way to compute mutual information analytically. Instead  we rely on access
t=1  in order to estimate mutual informations numerically. In such
to sufﬁcient data {x[t]  u∗[t]}T
situations (e.g. Sec. 5)  we discretize the data and then compute mutual information with a minimax
risk estimator  as proposed by Jiao et al. [2014].

4 Allowing Restricted Communication

Suppose that a decentralized policy ˆπi suffers from insufﬁcient mutual information between its local
measurement xi and the optimal action u∗i . In this case  we would like to quantify the potential
beneﬁts of communicating with other nodes j (cid:54)= i in order to reduce the distortion limit D∗ from (2)
and improve its ability to reconstruct u∗i . In this section  we present an information-theoretic solution
to the problem of how to choose optimally which other data to observe  and we provide a lower
bound-achieving solution for the idealized Gaussian case introduced in Sec. 3.3. We assume that in
addition to observing its own local state xi  each ˆπi is allowed to depend on at most k other xj(cid:54)=i.
Theorem 1. (Restricted Communication)
If Si is the set of k nodes j (cid:54)= i ∈ N which ˆui is allowed to observe in addition to xi  then setting
(8)

I(u∗i ; xi {xj : j ∈ S}) : |S| = k  

Si = arg max
S

minimizes the best-case expectation of any distortion measure. That is  this choice of Si yields the
smallest lower bound D∗ from (2) of any possible choice of S.

Proof. By assumption  Si maximizes the mutual information between the observed local states
{xi  xj : j ∈ Si} and the optimal action u∗i . This mutual information is equivalent to the notion
of rate R in the classical rate distortion theorem [Cover and Thomas  2012]. It is well-known that the
distortion rate function D(R) is convex and monotone decreasing in R. Thus  by maximizing mutual
information R we are guaranteed to minimize distortion D(R)  and hence D∗.

Theorem 1 provides a means of choosing a subset of the state {xj : j (cid:54)= i} to communicate to each
decentralized policy ˆπi that minimizes the corresponding best expected distortion D∗. Practically
speaking  this result may be interpreted as formalizing the following intuition: “the best thing to do
is to transmit the most information.” In this case  “transmitting the most information” corresponds
to allowing ˆπi to observe the set S of nodes {xj : j (cid:54)= i} which contains the most information
about u∗i . Likewise  by “best” we mean that Si minimizes the best-case expected distortion D∗ 
for any distortion metric d. As in Sec. 3.3  without making some assumption about the structure
of the distribution of x and u∗  we cannot guarantee that any particular regressor ˆπi will attain D∗.
Nevertheless  in a practical situation where sufﬁcient data {x[t]  u∗[t]}T
t=1 is available  we can solve
(8) by estimating mutual information [Jiao et al.  2014].

4.1 Example: Joint Gaussian  Squared Error with Communication

Here  we reexamine the joint Gaussian-distributed  mean squared error distortion case from Sec. 3.3 
and apply Thm. 1. We will take u∗ ∈ R1  x ∈ R10 and u∗  x jointly Gaussian with zero mean and
arbitrary covariance. The speciﬁc covariance matrix Σ of the joint distribution p(u∗  x) is visualized
in Fig. 3a. For simplicity  we show the squared correlation coefﬁcients of Σ which lie in [0  1]. The
boxed cells in Σ in Fig. 3a indicate that x9 solves (8)  i.e. j = 9 maximizes I(u∗; x1  xj) the mutual
information between the observed data and regression target u∗. Intuitively  this choice of j is best
because x9 is highly correlated to u∗ and weakly correlated to x1  which is already observed by ˆu;
that is  it conveys a signiﬁcant amount of information about u∗ that is not already conveyed by x1.
Figure 3b shows empirical results. Along the horizontal axis we increase the value of k  the number
of additional variables xj which regressor ˆπi observes. The vertical axis shows the resulting average
distortion. We show results for a linear regressor of the form of (7) where we have chosen Si optimally
according to (8)  as well as uniformly at random from all possible sets of unique indices. Note that
the optimal choice of Si yields the lowest average distortion D∗ for all choices of k. Moreover  the
linear regressor of (7) achieves D∗ for all k  since we have assumed a Gaussian joint distribution.

6

(a) Squared correlation coefﬁcients.

(b) Comparison of communication strategies.

Figure 3: Results for optimal communication strategies on a synthetic Gaussian example. (a) shows
squared correlation coefﬁcients between of u∗ and all xi’s. The boxed entries correspond to x9 
which was found to be optimal for k = 1. (b) shows that the optimal communication strategy of Thm.
1 achieves the lowest average distortion and outperforms the average over random strategies.

5 Application to Optimal Power Flow

In this case study  we aim to minimize the voltage variability in an electric grid caused by intermittent
renewable energy sources and the increasing load caused by electric vehicle charging. We do so
by controlling the reactive power output of distributed energy resources (DERs)  while adhering
to the physics of power ﬂow and constraints due to energy capacity and safety. Recently  various
approaches have been proposed  such as [Farivar et al.  2013] or [Zhang et al.  2014]. In these
methods  DERs tend to rely on an extensive communication infrastructure  either with a central
master node [Xu et al.  2017] or between agents leveraging local computation [Dall’Anese et al. 
2014]. We study regression-based decentralization as outlined in Sec. 3 and Fig. 2 to the optimal
power ﬂow (OPF) problem [Low  2014]  as initially proposed by Sondermeijer et al. [2016]. We
apply Thm. 1 to determine the communication strategy that minimizes optimal distortion to further
improve the reconstruction of the optimal actions u∗i .
Solving OPF requires a model of the electricity grid describing both topology and impedances; this
is represented as a graph G = (N  E). For clarity of exposition and without loss of generality  we
introduce the linearized power ﬂow equations over radial networks  also known as the LinDistFlow
equations [Baran and Wu  1989]:

Pjk + pc

g
j − p
j  

Pij = (cid:88)(j k)∈E k(cid:54)=i
Qij = (cid:88)(j k)∈E k(cid:54)=i

Qjk + qc

j − q
vj = vi − 2 (rijPij + ξijQij)

g
j  

(9a)

(9b)

(9c)

i and qc

i and qg

In this model  capitals Pij and Qij represent real and reactive power ﬂow on a branch from node i to
node j for all branches (i  j) ∈ E  lower case pc
i are the real and reactive power consumption
at node i  and pg
i are its real and reactive power generation. Complex line impedances
rij +√−1ξij have the same indexing as the power ﬂows. The LinDistFlow equations use the squared
voltage magnitude vi  deﬁned and indexed at all nodes i ∈ N . These equations are included as
constraints in the optimization problem to enforce that the solution adheres to laws of physics.
To formulate our decentralized learning problem  we will treat xi (cid:44) (pc
i   pg
i ) to be the local state
i   qc
variable  and  for all controllable nodes  i.e. agents i ∈ C  we have ui (cid:44) qg
i   i.e. the reactive power
generation can be controlled (vi  Pij  Qij are treated as dummy variables). We assume that for all
i are predetermined respectively by
nodes i ∈ N   consumption pc
the demand and the power generated by a potential photovoltaic (PV) system. The action space is
constrained by the reactive power capacity |ui| =(cid:12)(cid:12)q

i(cid:12)(cid:12) ≤ ¯qi. In addition  voltages are maintained

i and real power generation pg

i   qc

g

7

u∗u∗x1x1x2x2x3x3x4x4x5x5x6x6x7x7x8x8x9x9x10x100.20.40.60.810246810AdditionalObservationsk0510152025MSEoptimalstrategyaveragerandomstrategy(a) Voltage output with and without control.

(b) Comparison of OPF communication strategies.

Figure 4: Results for decentralized learning on an OPF problem. (a) shows an example result of
decentralized learning - the shaded region represents the range of all voltages in a network over a
full day. As compared to no control  the fully decentralized regression-based control reduces voltage
variation and prevents constraint violation (dashed line). (b) shows that the optimal communication
strategy Si outperforms the average for random strategies on the mean squared error distortion metric.
The regressors used are stepwise linear policies ˆπi with linear or quadratic features.

within ±5% of 120V   which is expressed as the constraint v ≤ vi ≤ v . The OPF problem now reads
(10)

u∗ = arg min

qg

i   ∀i∈C (cid:88)i∈N
(9)   (cid:12)(cid:12)q

s.t.

|vi − vref|  

g

i(cid:12)(cid:12) ≤ ¯qi   v ≤ vi ≤ v .

Following Fig. 2  we employ models of real electrical distribution grids (including the IEEE Test
t=1 of load and PV
Feeders [IEEE PES  2017])  which we equip with with T historical readings {x[t]}T
data  which is composed with real smart meter measurements sourced from Pecan Street Inc. [2017].
We solve (10) for all data  yielding a set of minimizers {u∗[t]}T
t=1. We then separate the overall data
t=1   ∀i ∈ C and train linear policies with feature kernels
set into C smaller data sets {xi[t]  u∗i [t]}T
φi(·) and parameters θi of the form ˆπi(xi) = θ(cid:62)i φi(xi). Practically  the challenge is to select the best
feature kernel φi(·). We extend earlier work which showed that decentralized learning for OPF can
be done satisfactorily via a hybrid forward- and backward-stepwise selection algorithm [Friedman
et al.  2001  Chapter 3] that uses a quadratic feature kernels.
Figure 4a shows the result for an electric distribution grid model based on a real network from
Arizona. This network has 129 nodes and  in simulation  53 nodes were equipped with a controllable
DER (i.e. N = 129  C = 53). In Fig. 4a we show the voltage deviation from a normalized setpoint
on a simulated network with data not used during training. The improvement over the no-control
baseline is striking  and performance is nearly identical to the optimum achieved by the centralized
solution. Concretely  we observed: (i) no constraint violations  and (ii) a suboptimality deviation of
0.15% on average  with a maximum deviation of 1.6%  as compared to the optimal policy π∗.
In addition  we applied Thm. 1 to the OPF problem for a smaller network [IEEE PES  2017]  in order
to determine the optimal communication strategy to minimize a squared error distortion measure. Fig.
4b shows the mean squared error distortion measure for an increasing number of observed nodes k
and shows how the optimal strategy outperforms an average over random strategies.

6 Conclusions and Future Work

This paper generalizes the approach of Sondermeijer et al. [2016] to solve multi-agent static optimal
control problems with decentralized policies that are learned ofﬂine from historical data. Our rate
distortion framework facilitates a principled analysis of the performance of such decentralized policies
and the design of optimal communication strategies to improve individual policies. These techniques
work well on a model of a sophisticated real-world OPF example.
There are still many open questions about regression-based decentralization.
It is well known
that strong interactions between different subsystems may lead to instability and suboptimality in
decentralized control problems [Davison and Chang  1990]. There are natural extensions of our work

8

012345AdditionalObservationsk11.21.41.61.82MSE×10−3linear randomlinear optimalquadratic randomquadratic optimalto address dynamic control problems more explicitly  and stability analysis is a topic of ongoing work.
Also  analysis of the suboptimality of regression-based decentralization should be possible within
our rate distortion framework. Finally  it is worth investigating the use of deep neural networks to
parameterize both the distribution p(u∗|x) and local policies ˆπi in more complicated decentralized
control problems with arbitrary distortion measures.

Acknowledgments

The authors would like to acknowledge Roberto Calandra for his insightful suggestions and feedback
on the manuscript. This research is supported by NSF under the CPS Frontiers VehiCal project
(1545126)  by the UC-Philippine-California Advanced Research Institute under projects IIID-2016-
005 and IIID-2015-10  and by the ONR MURI Embedded Humans (N00014-16-1-2206). David
Fridovich-Keil was also supported by the NSF GRFP.

References
P. Abbeel and A. Y. Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Interna-

tional Conference on Machine Learning  New York  NY  USA  2004. ACM.

R. J. Aumann and J. H. Dreze. Cooperative games with coalition structures. International Journal of

Game Theory  3(4):217–237  Dec. 1974.

M. Baran and F. Wu. Optimal capacitor placement on radial distribution systems. IEEE Transactions

on Power Delivery  4(1):725–734  Jan. 1989.

S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed Optimization and Statistical
Learning via the Alternating Direction Method of Multipliers. Foundations and Trends R(cid:13) in
Machine Learning  3(1):1–122  July 2011.

M. Brambilla  E. Ferrante  M. Birattari  and M. Dorigo. Swarm robotics: a review from the swarm

engineering perspective. Swarm Intelligence  7(1):1–41  Mar. 2013.

P. D. Christoﬁdes  R. Scattolini  D. M. de la Pena  and J. Liu. Distributed model predictive control:
A tutorial review and future research directions. Computers & Chemical Engineering  51:21–41 
2013.

T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons  2012.

E. Dall’Anese  S. V. Dhople  and G. Giannakis. Optimal dispatch of photovoltaic inverters in
residential distribution systems. Sustainable Energy  IEEE Transactions on  5(2):487–497  2014.
URL http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6719562.

E. J. Davison and T. N. Chang. Decentralized stabilization and pole assignment for general proper

systems. IEEE Transactions on Automatic Control  35(6):652–664  1990.

M. Farivar  L. Chen  and S. Low. Equilibrium and dynamics of local voltage control in distribution
systems. In 2013 IEEE 52nd Annual Conference on Decision and Control (CDC)  pages 4329–4334 
Dec. 2013. doi: 10.1109/CDC.2013.6760555.

J. Friedman  T. Hastie  and R. Tibshirani. The elements of statistical learning  volume 1. Springer

series in statistics Springer  Berlin  2001.

C. V. Goldman and S. Zilberstein. Decentralized control of cooperative systems: Categorization
and complexity analysis. J. Artif. Int. Res.  22(1):143–174  Nov. 2004. ISSN 1076-9757. URL
http://dl.acm.org/citation.cfm?id=1622487.1622493.

IEEE PES.

IEEE Distribution Test Feeders  2017. URL http://ewh.ieee.org/soc/pes/

dsacom/testfeeders/.

J. Jiao  K. Venkat  Y. Han  and T. Weissman. Minimax Estimation of Functionals of Discrete

Distributions. arXiv preprint  June 2014. arXiv: 1406.6956.

S. Low. Convex Relaxation of Optimal Power Flow; Part I: Formulations and Equivalence. IEEE

Transactions on Control of Network Systems  1(1):15–27  Mar. 2014.

9

J. Lunze. Feedback Control of Large Scale Systems. Prentice Hall PTR  Upper Saddle River  NJ 

USA  1992. ISBN 013318353X.

P. J. Modi  W.-M. Shen  M. Tambe  and M. Yokoo. Adopt: Asynchronous distributed constraint
optimization with quality guarantees. Artif. Intell.  161(1-2):149–180  Jan. 2005. ISSN 0004-3702.
doi: 10.1016/j.artint.2004.09.003. URL http://dx.doi.org/10.1016/j.artint.2004.09.
003.

R. Nair  P. Varakantham  M. Tambe  and M. Yokoo. Networked Distributed POMDPs: A synthesis of

distributed constraint optimization and POMDPs. In AAAI  volume 5  pages 133–139  2005.

F. A. Oliehoek and C. Amato. A Concise Introduction to Decentralized POMDPs. Springer

International Publishing  1 edition  2016.

P. A. Ortega  D. A. Braun  J. Dyer  K.-E. Kim  and N. Tishby. Information-Theoretic Bounded

Rationality. arXiv preprint  2015. arXiv:1512.06789.

Pecan Street Inc. Dataport  2017. URL http://www.pecanstreet.org/.

L. Peshkin  K.-E. Kim  N. Meuleau  and L. P. Kaelbling. Learning to cooperate via policy search. In
Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence  UAI’00  pages
489–496  San Francisco  CA  USA  2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-709-9.
URL http://dl.acm.org/citation.cfm?id=2073946.2074003.

Y. Pu  M. N. Zeilinger  and C. N. Jones. Inexact fast alternating minimization algorithm for distributed
model predictive control. In Conference on Decision and Control  Los Angeles  CA  USA  2014.
IEEE.

R. L. Raffard  C. J. Tomlin  and S. P. Boyd. Distributed optimization for cooperative agents:
Application to formation ﬂight. In Conference on Decision and Control  Nassau  The Bahamas 
2004. IEEE.

C. Sammut. Automatic construction of reactive control systems using symbolic machine learning.

The Knowledge Engineering Review  11(01):27–42  1996.

D. D. Siljak. Decentralized control of complex systems. Dover Books on Electrical Engineering.

Dover  New York  NY  2011. URL http://cds.cern.ch/record/1985961.

O. Sondermeijer  R. Dobbe  D. B. Arnold  C. Tomlin  and T. Keviczky. Regression-based Inverter
Control for Decentralized Optimal Power Flow and Voltage Regulation. In Power and Energy
Society General Meeting  Boston  MA  USA  July 2016. IEEE.

A. X. Sun  D. T. Phan  and S. Ghosh. Fully decentralized AC optimal power ﬂow algorithms. In

Power and Energy Society General Meeting  Vancouver  Canada  July 2013. IEEE.

Y. Xu  Z. Y. Dong  R. Zhang  and D. J. Hill. Multi-Timescale Coordinated Voltage/Var Control of
High Renewable-Penetrated Distribution Systems. IEEE Transactions on Power Systems  PP(99):
1–1  2017. ISSN 0885-8950. doi: 10.1109/TPWRS.2017.2669343.

M. N. Zeilinger  Y. Pu  S. Riverso  G. Ferrari-Trecate  and C. N. Jones. Plug and play distributed
model predictive control based on distributed invariance and optimization. In Conference on
Decision and Control  Florence  Italy  2013. IEEE.

B. Zhang  A. Lam  A. Dominguez-Garcia  and D. Tse. An Optimal and Distributed Method for
Voltage Regulation in Power Distribution Systems. IEEE Transactions on Power Systems  PP(99):
1–13  2014. ISSN 0885-8950. doi: 10.1109/TPWRS.2014.2347281.

10

,Xiaoxiao Guo
Satinder Singh
Honglak Lee
Richard Lewis
Xiaoshi Wang
Roel Dobbe
David Fridovich-Keil
Claire Tomlin