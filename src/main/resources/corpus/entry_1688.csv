2013,Efficient Algorithm for Privately Releasing Smooth Queries,We study differentially private mechanisms for answering \emph{smooth} queries on databases consisting of data points in $\mathbb{R}^d$. A $K$-smooth query is specified by a function whose partial derivatives up to order $K$ are all bounded. We develop an $\epsilon$-differentially private mechanism which for the class of $K$-smooth queries has accuracy $O (\left(\frac{1}{n}\right)^{\frac{K}{2d+K}}/\epsilon)$. The mechanism first outputs a summary of the database. To obtain an answer of a query  the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time $O(n^{1+\frac{d}{2d+K}})$  and the evaluation algorithm for answering a query runs in time $\tilde O (n^{\frac{d+2+\frac{2d}{K}}{2d+K}} )$. Our mechanism is based on $L_{\infty}$-approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efficiently computable coefficients.,Efﬁcient Algorithm for Privately Releasing Smooth

Queries

Ziteng Wang

School of EECS
Peking University

Jiaqi Zhang

School of EECS
Peking University

Kai Fan

School of EECS
Peking University

Liwei Wang

School of EECS
Peking University

Key Laboratory of Machine Perception  MOE

Key Laboratory of Machine Perception  MOE

wangzt@cis.pku.edu.cn

interfk@hotmail.com

Key Laboratory of Machine Perception  MOE

Key Laboratory of Machine Perception  MOE

Zhangjq@cis.pku.edu.cn

wanglw@cis.pku.edu.cn

Abstract

We study differentially private mechanisms for answering smooth queries on
databases consisting of data points in Rd. A K-smooth query is speciﬁed by a
function whose partial derivatives up to order K are all bounded. We develop an
-differentially private mechanism which for the class of K-smooth queries has
accuracy O(n− K
2d+K /). The mechanism ﬁrst outputs a summary of the database.
To obtain an answer of a query  the user runs a public evaluation algorithm which
contains no information of the database. Outputting the summary runs in time
O(n1+ d
2d+K )  and the evaluation algorithm for answering a query runs in time
d+2+ 2d
K
˜O(n
2d+K ). Our mechanism is based on L∞-approximation of (transformed)
smooth functions by low degree even trigonometric polynomials with small and
efﬁciently computable coefﬁcients.

1

Introduction

Privacy is an important problem in data analysis. Often people want to learn useful information from
data that are sensitive. But when releasing statistics of sensitive data  one must tradeoff between the
accuracy and the amount of privacy loss of the individuals in the database.
In this paper we consider differential privacy [9]  which has become a standard concept of privacy.
Roughly speaking  a mechanism which releases information about the database is said to preserve
differential privacy  if the change of a single database element does not affect the probability distri-
bution of the output signiﬁcantly. Differential privacy provides strong guarantees against attacks. It
ensures that the risk of any individual to submit her information to the database is very small. An
adversary can discover almost nothing new from the database that contains the individual’s infor-
mation compared with that from the database without the individual’s information. Recently there
have been extensive studies of machine learning  statistical estimation  and data mining under the
differential privacy framework [29  5  18  17  6  30  20  4].
Accurately answering statistical queries is an important problem in differential privacy. A simple
and efﬁcient method is the Laplace mechanism [9]  which adds Laplace noise to the true answers.
Laplace mechanism is especially useful for query functions with low sensitivity  which is the max-
imal difference of the query values of two databases that are different in only one item. A typical

1

It can answer at most O(n2) queries.

class of queries that has low sensitivity is linear queries  whose sensitivity is O(1/n)  where n is the
size of the database.
The Laplace mechanism has a limitation.
If the number
of queries is substantially larger than n2  Laplace mechanism is not able to provide differentially
private answers with nontrivial accuracy. Considering that potentially there are many users and
each user may submit a set of queries  limiting the number of total queries to be smaller than n2 is
too restricted in some situations. A remarkable result due to Blum  Ligett and Roth [2] shows that
information theoretically it is possible for a mechanism to answer far more than n2 linear queries
while preserving differential privacy and nontrivial accuracy simultaneously.
There are a series of works [10  11  21  16] improving the result of [2]. All these mechanisms
are very powerful in the sense that they can answer general and adversely chosen queries. On the
other hand  even the fastest algorithms [16  14] run in time linear in the size of the data universe to
answer a query. Often the size of the data universe is much larger than that of the database  so these
mechanisms are inefﬁcient. Recently  [25] shows that there is no polynomial time algorithm that
can answer n2+o(1) general queries while preserving privacy and accuracy (assuming the existence
of one-way function).
Given the hardness result  recently there are growing interests in studying efﬁcient and differentially
private mechanisms for restricted class of queries. From a practical point of view  if there exists a
class of queries which is rich enough to contain most queries used in applications and allows one to
develop fast mechanisms  then the hardness result is not a serious barrier for differential privacy.
One class of queries that attracts a lot of attentions is the k-way conjunctions. The data universe for
this problem is {0  1}d. Thus each individual record has d binary attributes. A k-way conjunction
query is speciﬁed by k features. The query asks what fraction of the individual records in the
database has all these k features being 1. A series of works attack this problem using several different
techniques [1  13  7  15  24] . They propose elegant mechanisms which run in time poly(n) when
k is a constant. Another class of queries that yields efﬁcient mechanisms is sparse query. A query
is m-sparse if it takes non-zero values on at most m elements in the data universe. [3] develops
mechanisms which are efﬁcient when m = poly(n).
When the data universe is [−1  1]d  where d is a constant  [2] considers rectangle queries. A rectangle
query is speciﬁed by an axis-aligned rectangle. The answer to the query is the fraction of the data
points that lie in the rectangle. [2] shows that if [−1  1]d is discretized to poly(n) bits of precision 
then there are efﬁcient mechanisms for the class of rectangle queries. There are also works studying
related range queries [19].
In this paper we study smooth queries deﬁned also on data universe [−1  1]d for constant d. A smooth
query is speciﬁed by a smooth function  which has bounded partial derivatives up to a certain order.
The answer to the query is the average of the function values on data points in the database. Smooth
functions are widely used in machine learning and data analysis [28]. There are extensive studies
on the relation between smoothness  regularization  reproducing kernels and generalization ability
[27  22].
Our main result is an -differentially private mechanism for the class of K-smooth queries  which
are speciﬁed by functions with bounded partial derivatives up to order K. The mechanism has
(α  β)-accuracy  where α = O(n− K
2d+K ). The mechanism ﬁrst outputs a
summary of the database. To obtain an answer of a smooth query  the user runs a public evaluation
procedure which contains no information of the database. Outputting the summary has running time
d+2+ 2d
K
2d+K ). The
O
mechanism has the advantage that both the accuracy and the running time for answering a query
improve quickly as K/d increases (see also Table 1 in Section 3).
Our algorithm is a L∞-approximation based mechanism and is motivated by [24]  which considers
approximation of k-way conjunctions by low degree polynomials. The basic idea is to approximate
the whole query class by linear combination of a small set of basis functions. The technical difﬁcul-
ties lie in that in order that the approximation induces an efﬁcient and differentially private mech-
anism  all the linear coefﬁcients of the basis functions must be small and efﬁciently computable.
To guarantee these properties  we ﬁrst transform the query function. Then by using even trigono-

  and the evaluation procedure for answering a query runs in time ˜O(n

n1+ d

2d+K

2d+K /) for β ≥ e−O(n

d

(cid:16)

(cid:17)

2

metric polynomials as basis functions we prove a constant upper bound for the linear coefﬁcients.
The smoothness of the functions also allows us to use an efﬁcient numerical method to compute the
coefﬁcients to a precision so that the accuracy of the mechanism is not affected signiﬁcantly.

2 Background
Let D be a database containing n data points in the data universe X . In this paper  we consider the
case that X ⊂ Rd where d is a constant. Typically  we assume that the data universe X = [−1  1]d.
Two databases D and D(cid:48) are called neighbors if |D| = |D(cid:48)| = n and they differ in exactly one data
point. The following is the formal deﬁnition of differential privacy.
Deﬁnition 2.1 ((  δ)-differential privacy). A sanitizer S which is an algorithm that maps input
database into some range R is said to preserve (  δ)-differential privacy  if for all pairs of neighbor
databases D  D(cid:48) and for any subset A ⊂ R  it holds that

P(S(D) ∈ A) ≤ P(S(D(cid:48)) ∈ A) · e + δ.

If S preserves (  0)-differential privacy  we say S is -differentially private.

We consider linear queries. Each linear query qf is speciﬁed by a function f which maps data
universe [−1  1]d to R  and qf is deﬁned by qf (D) := 1|D|
Let Q be a set of queries. The accuracy of a mechanism with respect to Q is deﬁned as follows.
Deﬁnition 2.2 ((α  β)-accuracy). Let Q be a set of queries. A sanitizer S is said to have (α  β)-
accuracy for size n databases with respect to Q  if for every database D with |D| = n the following
holds

x∈D f (x).

P(∃q ∈ Q 

|S(D  q) − q(D)| ≥ α) ≤ β 

(cid:80)

where S(D  q) is the answer to q given by S.

2σ exp(−|x|/σ).

We will make use of Laplace mechanism [9] in our algorithm. Laplace mechanism adds Laplace
noise to the output. We denote by Lap(σ) the random variable distributed according to the Laplace
distribution with parameter σ: P(Lap(σ) = x) = 1
We will design a differentially private mechanism which is accurate with respect to a query set
Q possibly consisting of inﬁnite number of queries. Given a database D  the sanitizer outputs a
summary which preserves differential privacy. For any qf ∈ Q  the user makes use of an evaluation
procedure to measure f on the summary and obtain an approximate answer of qf (D). Although we
may think of the evaluation procedure as part of the mechanism  it does not contain any information
of the database and therefore is public. We will study the running time for the sanitizer outputting
the summary. Ideally it is O(nc) for some constant c not much larger than 1. For the evaluation
procedure  the running time per query is the focus. Ideally it is sublinear in n. Here and in the rest
of the paper  we assume that calculating the value of f on a data point x can be done in unit time.
In this work we will frequently use trigonometric polynomials. For the univariate case  a function
l=1 (al cos lθ + bl sin lθ) 
If p(θ) is an even function  we say that it is an even trigonomet-
l=1 al cos lθ. For the multivariate case  if p(θ1  . . .   θd) =
l=(l1 ... ld) al cos(l1θ1) . . . cos(ldθd)  then p is said to be an even trigonometric polynomial (with

p(θ) is called a trigonometric polynomial of degree m if p(θ) = a0 +(cid:80)m
ric polynomial  and p(θ) = a0 +(cid:80)m
(cid:80)

where al  bl are constants.

respect to each variable)  and the degree of θi is the upper limit of li.

3 Efﬁcient differentially private mechanism

Let us ﬁrst describe the set of queries considered in this work. Since each query qf is speciﬁed by a
function f  a set of queries QF can be speciﬁed by a set of functions F . Remember that each f ∈ F
maps [−1  1]d to R. For any point x = (x1  . . .   xd) ∈ [−1  1]d  if k = (k1  . . .   kd) is a d-tuple
with nonnegative integers  then we deﬁne

Dk := Dk1

1 ··· Dkd

d :=

∂k1
∂xk1
1

··· ∂kd
∂xkd
d

.

3

Parameters: Privacy parameters   δ > 0; Failure probability β > 0;

Input: Database D ∈(cid:0)[−1  1]d(cid:1)n.

Smoothness order K ∈ N; Set t = n

1

2d+K .

Output: A td-dimensional vector as the summary.
Algorithm:

For each x = (x1  . . .   xd) ∈ D:
For every d-tuple of nonnegative integers m = (m1  . . .   md)  where (cid:107)m(cid:107)∞ ≤ t − 1

Set: θi(x) = arccos(xi)  i = 1  . . .   d;

(cid:80)
(cid:99)Sum(D) ← Sum(D) + Lap

Compute: Sum(D) = 1
n

(cid:16)(cid:99)Sum(D)

(cid:17)

(cid:107)m(cid:107)∞≤t−1

Let(cid:99)Su(D) =
Return:(cid:99)Su(D).

(cid:16) td

(cid:17)

;

x∈D cos (m1θ1(x)) . . . cos (mdθd(x));

n
be a td dimensional vector;

Algorithm 1: Outputting the summary

1

Parameters: t = n
Input: A query qf   where f : [−1  1]d → R and f ∈ C K
B  

Summary(cid:99)Su(D) (a td-dimensional vector).

2d+K .

Output: Approximate answer to qf (D).
Algorithm:

Let gf (θ) = f (cos(θ1)  . . .   cos(θd))  θ = (θ1  . . .   θd) ∈ [−π  π]d;
Compute a trigonometric polynomial approximation pt(θ) of gf (θ) 

where the degree of each θi is t;

Denote pt(θ) =(cid:80)
Return: the inner product < c (cid:99)Su(D) >.

Let c = (cm)(cid:107)m(cid:107)∞<t be a td-dimensional vector;

m=(m1 ... md) (cid:107)m(cid:107)∞<t cm cos(m1θ1) . . . cos(mdθd);

// see Section 4 for details of computation.

Algorithm 2: Answering a query

Let |k| := k1 + . . . + kd. Deﬁne the K-norm as
(cid:107)f(cid:107)K := sup
|k|≤K

sup

x∈[−1 1]d

|Dkf (x)|.

B

B := {f :

B   denoted as QCK

B which contains all smooth functions whose derivatives up to order K have
(cid:107)f(cid:107)K ≤ B}. The set
  is our focus. Smooth functions have been studied in

We will study the set C K
∞-norm upper bounded by a constant B > 0. Formally  C K
of queries speciﬁed by C K
depth in machine learning [26  28  27] and found wide applications [22].
The following theorem is our main result. It says that if the query class is speciﬁed by smooth
functions  then there is a very efﬁcient mechanism which preserves -differential privacy and good
accuracy. The mechanism consists of two parts: One for outputting a summary of the database 
the other for answering a query. The two parts are described in Algorithm 1 and Algorithm 2
respectively. The second part of the mechanism contains no private information of the database.
B }  where K ∈ N
Theorem 3.1. Let the query set be QCK
and B > 0 are constants. Let the data universe be [−1  1]d  where d ∈ N is a constant. Then the
mechanism S given in Algorithm 1 and Algorithm 2 satisﬁes that for any  > 0  the following hold:
(cid:17)
1) The mechanism is -differentially private.
2) For any β ≥ 10 · e− 1
and the hidden constant depends only on d  K and B.

2d+K ) the mechanism is (α  β)-accurate  where α = O

= {qf = 1

(cid:1) K

(cid:16)(cid:0) 1

f ∈ C K

x∈D f (x) :

(cid:80)

2d+K /

5 (n

n

n

 

B

d

4

Order of smoothness

Table 1: Performances vs. Order of smoothness
Accuracy α

Time: Outputting summary Time: Answering a query

K = 1

K = 2d

O(( 1
n )

1

2d+1 )

O( 1√

n )

d

K = 0 (cid:28) 1

O(( 1

n )1−20 )

O(n 3
2 )

O(n 5
4 )

˜O(n

3

2 + 1

4d+2 )

˜O(n 1

4 + 3/4

d )

O(n1+0 )

˜O(n0(1+ 3

d ))

3d+K

2d+K ).

d+2+ 2d
K
2d+K polylog(n)).

3) The running time for S to output the summary is O(n
4) The running time for S to answer a query is O(n
The proof of Theorem 3.1 is given in the supplementary material. To have a better idea of how
the performances depend on the order of smoothness  let us consider three cases. The ﬁrst case
is K = 1  i.e.  the query functions only have the ﬁrst order derivatives. Another extreme case is
K (cid:29) d  and we assume d/K = 0 (cid:28) 1. We also consider a case in the middle by assuming
K = 2d. Table 1 gives simpliﬁed upper bounds for the error and running time in these cases. We
have the following observations:
2d ) to nearly O(n−1) as K increases.
1) The accuracy α improves dramatically from roughly O(n− 1
For K > 2d  the error is smaller than the sampling error O( 1√
n ).

2) The running time for outputting the summary does not change too much  because reading through
the database requires Ω(n) time.

d

3) The running time for answering a query reduces signiﬁcantly from roughly O(n3/2) to nearly
O(n0) as K getting large. When K = 2d  it is about n1/4 if d is not too small. In practice  the
speed for answering a query may be more important than that for outputting the summary since
the sanitizer only output the summary once. Thus having an nc-time (c (cid:28) 1) algorithm for query
answering will be appealing.
Conceptually our mechanism is simple. First  by change of variables we have gf (θ1  . . .   θd) =
f (cos θ1  . . .   cos θd). It also transforms the data universe from [−1  1]d to [−π  π]d. Note that for
each variable θi  gf is an even function. To compute the summary  the mechanism just gives noisy
answers to queries speciﬁed by even trigonometric monomials cos(m1θ1) . . . cos(mdθd). For each
2d+K ). The
trigonometric monomial  the highest degree of any variable is t := maxd md = O(n
2d+K )-dimensional vector. To answer a query speciﬁed by a smooth function f 
summary is a O(n
the mechanism computes a trigonometric polynomial approximation of gf . The answer to the query
qf is a linear combination of the summary by the coefﬁcients of the approximation trigonometric
polynomial.
Our algorithm is an L∞-approximation based mechanism  which is motivated by [24]. An approx-
imation based mechanism relies on three conditions: 1) There exists a small set of basis functions
such that every query function can be well approximated by a linear combination of them; 2) All the
linear coefﬁcients are small; 3) The whole set of the linear coefﬁcients can be computed efﬁciently.
If these conditions hold  then the mechanism just outputs noisy answers to the set of queries speciﬁed
by the basis functions as the summary. When answering a query  the mechanism computes the
coefﬁcients with which the linear combination of the basis functions approximate the query function.
The answer to the query is simply the inner product of the coefﬁcients and the summary vector.
The following theorem guarantees that by change of variables and using even trigonometric poly-
nomials as the basis functions  the class of smooth functions has all the three properties described
above.
Theorem 3.2. Let γ > 0. For every f ∈ C K

B deﬁned on [−1  1]d  let

1

gf (θ1  . . .   θd) = f (cos θ1  . . .   cos θd) 

5

θi ∈ [−π  π].

Then  there is an even trigonometric polynomial p whose degree of each variable is t(γ) =

p(θ1  . . .   θd) =

cl1 ... ld cos(l1θ1) . . . cos(ldθd) 

(cid:88)

0≤l1 ... ld<t(γ)

such that
1) (cid:107)gf − p(cid:107)∞ ≤ γ.
2) All the linear coefﬁcients cl1 ... ld can be uniformly upper bounded by a constant M independent
of t(γ) (i.e.  M depends only on K  d  and B).

3) The whole set of the linear coefﬁcients can be computed in time O

d+2

K + 2d

K2 · polylog( 1
γ )

( 1
γ )

(cid:16)

Theorem 3.2 is proved in Section 4. Based on Theorem 3.2  the proof of Theorem 3.1 is mainly
the argument for Laplace mechanism together with an optimization of the approximation error γ
trading-off with the Laplace noise. (Please see the supplementary material.)

4 L∞-approximation of smooth functions: small and efﬁciently computable

coefﬁcients

In this section we prove Theorem 3.2. That is  for every f ∈ C K
B the corresponding gf can be
approximated by a low degree trigonometric polynomial in L∞-norm. We also require that the
linear coefﬁcients of the trigonometric polynomial are all small and can be computed efﬁciently.
These properties are crucial for the differentially private mechanism to be accurate and efﬁcient.
In fact  L∞-approximation of smooth functions in C K
B by polynomial (and other basis functions) is
an important topic in approximation theory. It is well-known that for every f ∈ C K
B there is a low
degree polynomial with small approximation error. However  it is not clear whether there is an upper
bound for the linear coefﬁcients that is sufﬁciently good for our purpose. Instead we transform f to
gf and use trigonometric polynomials as the basis functions in the mechanism. Then we are able
to give a constant upper bound for the linear coefﬁcients. We also need to compute the coefﬁcients
efﬁciently. But results from approximation theory give the coefﬁcients as complicated integrals.
We adopt an algorithm which fully exploits the smoothness of the function and thus can efﬁciently
compute approximations of the coefﬁcients to certain precision so that the errors involved do not
affect the accuracy of the differentially private mechanism too much.
Below  Section 4.1 describes the classical theory on trigonometric polynomial approximation of
smooth functions. Section 4.2 shows that the coefﬁcients have a small upper bound and can be
efﬁciently computed. Theorem 3.2 then follows from these results.

4.1 Trigonometric polynomial approximation with generalized Jackson kernel

This section mainly contains known results of trigonometric polynomial approximation  stated in a
way tailored to our problem. For a comprehensive description of univariate approximation theory 
please refer to the excellent book of [8]; and to [23] for multivariate approximation theory.
Let gf be the function obtained from f ∈ C K
B ([−1  1]d): gf (θ1  . . .   θd) = f (cos θ1  . . .   cos θd).
B(cid:48)([−π  π]d) for some constant B(cid:48) depending only on B  K  d  and gf is even
Note that gf ∈ C K
with respect to each variable. The key tool in trigonometric polynomial approximation of smooth
functions is the generalized Jackson kernel.
Deﬁnition 4.1. Deﬁne the generalized Jackson kernel as Jt r(s) = 1
λt r

(cid:16) sin(ts/2)

  where λt r is

(cid:17)2r

sin(s/2)

determined by(cid:82) π

−π Jt r(s)ds = 1.

Jt r(s) is an even trigonometric polynomial of degree r(t − 1). Let Ht r(s) = Jt(cid:48) r(s)  where
t(cid:48) = (cid:98)t/r(cid:99) + 1. Then Ht r is an even trigonometric polynomial of degree at most t. We write

(cid:17)1/K

(cid:16) 1

γ

:

(cid:17)

.

Ht r(s) = a0 +

al cos ls.

(1)

t(cid:88)

l=1

6

Suppose that g is a univariate function deﬁned on [−π  π] which satisﬁes that g(−π) = g(π). Deﬁne
the approximation operator It K as

It K(g)(x) = −

Ht r(s)

(−1)l

g(x + ls)ds 

(2)

(cid:90) π

−π

K+1(cid:88)

l=1

(cid:18)K + 1

(cid:19)

l

where r = (cid:100) K+3
2 (cid:101). It is not difﬁcult to see that It K maps g to a trigonometric polynomial of degree
at most t.
Next suppose that g is a d-variate function deﬁned on [−π  π]d  and is even with respect to each
t K as sequential composition of It K 1  . . .   It K d  where It K j is
variable. Deﬁne an operator I d
the approximation operator given in (2) with respect to the jth variable of g. Thus I d
t K(g) is a
trigonometric polynomial of d-variables and each variable has degree at most t.
Theorem 4.1. Suppose that g is a d-variate function deﬁned on [−π  π]d  and is even with respect
to each variable. Let D(K)
g be the Kth order partial derivative of g respect to the j-th variable. If
(cid:107)D(K)

g(cid:107)∞ ≤ M for some constant M for all 1 ≤ j ≤ d  then there is a constant C such that

j

j

(cid:107)g − I d

t K(g)(cid:107)∞ ≤ C

tK+1  

where C depends only on M  d and K.

4.2 The linear coefﬁcients

In this subsection we study the linear coefﬁcients in the trigonometric polynomial I d
previous subsection established that gf can be approximated by I d
sider the upper bound and approximate computation of the coefﬁcients. Since I d
is even with respect to each variable  we write

t K(gf ). The
t K(gf ) for a small t. Here we con-
t K(gf )(θ1  . . .   θd)

I d
t K(gf )(θ1  . . .   θd) =

cn1 ... nd cos(n1θ1) . . . cos(ndθd).

(cid:88)

0≤n1 ... nd≤t
t K(gf ) can be written as

cn1 ... nd = (−1)d (cid:88)
(cid:19)(cid:32)(cid:90)
(cid:18)K + 1

d(cid:89)

(−1)kiali

1≤k1 ... kd≤K+1
0≤l1 ... ld≤t
li=ki·ni∀i∈[d]

ki

[−π π]d

i=1

ml1 k1 ... ld kd 

d(cid:89)

i=1

cos

(cid:18) li

ki

(cid:19)

(cid:33)

θi

gf (θ)dθ

 

(3)

(4)

(5)

Fact 4.2. The coefﬁcients cn1 ... nd of I d

where

ml1 k1 ... ld kd =

and ali is the linear coefﬁcient of cos(lis) in Ht r(s) as given in (1).

The following lemma shows that the coefﬁcients cn1 ... nd of I d
bounded by a constant independent of t.
Lemma 4.3. There exists a constant M which depends only on K  B  d but independent of t  such
that for every f ∈ C K

t K(gf ) can be uniformly upper

B   all the linear coefﬁcients cn1 ... nd of I d

t K(gf ) satisfy

|cn1 ... nd| ≤ M.

The proof of Lemma 4.3 is given in the supplementary material. Now we consider the computation
of the coefﬁcients cn1 ... nd of I d
t K(gf ). Note that each coefﬁcient involves d-dimensional integra-
tions of smooth functions  so we have to numerically compute approximations of them. For function
class C K
τ )d/K) in
order that the error is less than τ. Here we adopt the sparse grids algorithm due to Gerstner and
Griebel [12] which fully exploits the smoothness of the integrand. By choosing a particular quadra-
ture rule as the algorithm’s subroutine  we are able to prove that the running time of the sparse grids

B deﬁned on [−1  1]d  traditional numerical integration methods run in time O(( 1

7

is bounded by O(( 1
τ )2/K). The sparse grids algorithm  the theorem giving the bound for the running
time and its proof are all given in the supplementary material. Based on these results  we establish
the running time for computing the approximate coefﬁcients of the trigonometric polynomial  which
is stated in the following Lemma.
Lemma 4.4. Let ˆcn1 ... nd be an approximation of the coefﬁcient cn1 ... nd of I d
t K(gf ) obtained by
approximately computing the integral in (5) with a version of the sparse grids algorithm [12] (given
in the supplementary material). Let

(cid:88)

γ

ˆI d
t K(gf )(θ1  . . .   θd) =

0≤n1 ... nd≤t

ˆcn1 ... nd cos(n1θ1) . . . cos(ndθd).

t K(gf )(cid:107)∞ ≤ O(cid:0)t−K(cid:1)  it sufﬁces that the
(cid:16)

(cid:17)

t(1+ 2

t K(gf ) − I d

B   in order that (cid:107) ˆI d

Then for every f ∈ C K
computation of all the coefﬁcients ˆcn1 ... nd runs in time O
maxn1 ... nd |ˆcn1 ... nd − cn1 ... nd| = o(1) as t → ∞.
The proof of Lemma 4.4 is given in the supplementary material. Theorem 3.2 then follows easily
(cid:17)1/K
from Lemma 4.3 and Lemma 4.4.

Proof of Theorem 3.2. Setting t = t(γ) =
m K(gf ). Combining Lemma 4.3
and Lemma 4.4  and note that the coefﬁcients ˆcn1 ... nd are upper bounded by a constant  the theorem
follows.

K )d+2 · polylog(t)

. Let p = ˆI d

. In addition 

(cid:16) 1

5 Conclusion

K

n

2K

d+2+2d/K

(cid:1) K

3d+2K log( 1
δ )

2d+K )  and is ˜O(n

queries. The accuracy of the mechanism is O((cid:0) 1

In this paper we propose an -differentially private mechanism for efﬁciently releasing K-smooth
2d+K ). The running time for outputting the sum-
mary is O(n1+ d
2d+K ) for answering a query. The result can be generalized
to (  δ)-differential privacy straightforwardly using the composition theorem [11]. The accuracy
3d+2K )  while the running time for outputting the summary
improves slightly to O(( 1
n )
and answering the query increase slightly. Our mechanism is based on approximation of smooth
functions by linear combination of a small set of basis functions with small and efﬁciently com-
B ([−1  1]d) by polynomials does not
putable coefﬁcients. Directly approximating functions in C K
guarantee small coefﬁcients and is less efﬁcient. To achieve these goals we use trigonometric poly-
nomials to approximate a transformation of the query functions.
It is worth pointing out that the approximation considered here for differential privacy is L∞-
approximation  because the accuracy is deﬁned in the worst case sense with respect to databases and
queries. L∞-approximation is different to L2-approximation  which is simply the Fourier transform
if we use trigonometric polynomials as the basis functions. L2-approximation does not guarantee
(worst case) accuracy.
For the class of smooth functions deﬁned on [−1  1]d where d is a constant  in fact it is not difﬁcult
to design a poly(n) time differentially private mechanism. One can discretize [−1  1]d to O( 1√
n )
precision  and use the differentially private mechanism for answering general queries (e.g.  [16]).
However the mechanism runs in time ˜O(nd/2) to answer a query  and provides ˜O(n−1/2) accura-
cy. In contrast our mechanism exploits higher order smoothness of the queries. It is always more
efﬁcient  and for queries highly smooth it is more accurate.

Acknowledgments

This work was supported by NSFC(61222307  61075003) and a grant from MOE-Microsoft Key
Laboratory of Statistics and Information Technology of Peking University. We also thank Di He for
very helpful discussions.

8

References

[1] B. Barak  K. Chaudhuri  C. Dwork  S. Kale  F. McSherry  and K. Talwar. Privacy  accuracy  and consis-

tency too: a holistic solution to contingency table release. In PODS  pages 273–282. ACM  2007.

[2] A. Blum  K. Ligett  and A. Roth. A learning theory approach to non-interactive database privacy.

STOC  pages 609–618. ACM  2008.

In

[3] A. Blum and A. Roth. Fast private data release algorithms for sparse queries. arXiv preprint arX-

iv:1111.6842  2011.

[4] K. Chaudhuri and D. Hsu. Sample complexity bounds for differentially private learning. In COLT  2011.
[5] K. Chaudhuri  C. Monteleoni  and A.D. Sarwate. Differentially private empirical risk minimization.

JMLR  12:1069  2011.

[6] K. Chaudhuri  A. Sarwate  and K. Sinha. Near-optimal differentially private principal components. In

NIPS  pages 998–1006  2012.

[7] M. Cheraghchi  A. Klivans  P. Kothari  and H.K. Lee. Submodular functions are noise stable. In SODA 

pages 1586–1592. SIAM  2012.

[8] R.A. DeVore and G. G. Lorentz. Constructive approximation  volume 303. Springer Verlag  1993.
[9] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private data analysis.

TCC  pages 265–284  2006.

[10] C. Dwork  M. Naor  O. Reingold  G.N. Rothblum  and S. Vadhan. On the complexity of differentially

private data release: efﬁcient algorithms and hardness results. In STOC  pages 381–390. ACM  2009.

[11] C. Dwork  G.N. Rothblum  and S. Vadhan. Boosting and differential privacy. In FOCS  pages 51–60.

IEEE  2010.

[12] T. Gerstner and M. Griebel. Numerical integration using sparse grids. Numerical algorithms  18(3-

4):209–232  1998.

[13] A. Gupta  M. Hardt  A. Roth  and J. Ullman. Privately releasing conjunctions and the statistical query

barrier. In STOC  pages 803–812. ACM  2011.

[14] M. Hardt  K. Ligett  and F. McSherry. A simple and practical algorithm for differentially private data

release. In NIPS  2012.

[15] M. Hardt  G. N. Rothblum  and R. A. Servedio. Private data release via learning thresholds. In SODA 

pages 168–187. SIAM  2012.

[16] M. Hardt and G.N. Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis.

In FOCS  pages 61–70. IEEE Computer Society  2010.

[17] D. Kifer and B.R. Lin. Towards an axiomatization of statistical privacy and utility.

147–158. ACM  2010.

In PODS  pages

[18] J. Lei. Differentially private M-estimators. In NIPS  2011.
[19] C. Li  M. Hay  V. Rastogi  G. Miklau  and A. McGregor. Optimizing linear counting queries under

differential privacy. In PODS  pages 123–134. ACM  2010.

[20] Pravesh K. Prateek J. and Abhradeep T. Differentially private online learning. In COLT  2012.
[21] A. Roth and T. Roughgarden. Interactive privacy via the median mechanism. In STOC  pages 765–774.

ACM  2010.

[22] A. Smola  B. Sch¨olkopf  and K. M¨uller. The connection between regularization operators and support

vector kernels. Neural Networks  11(4):637–649  1998.

[23] V.N. Temlyakov. Approximation of periodic functions. Nova Science Pub Inc  1994.
[24] J. Thaler  J. Ullman  and S. Vadhan. Faster algorithms for privately releasing marginals. In ICALP  pages

810–821. Springer  2012.

[25] J. Ullman. Answering n2+o(1) counting queries with differential privacy is hard. In STOC. ACM  2013.
[26] A. van der Vart and J.A. Wellner. Weak Convergence and Empirical Processes. Springer  1996.
[27] G. Wahba et al. Support vector machines  reproducing kernel Hilbert spaces and the randomized gacv.

Advances in Kernel Methods-Support Vector Learning  6:69–87  1999.

[28] L. Wang. Smoothness  disagreement coefﬁcient  and the label complexity of agnostic active learning.

Journal of Machine Learning Research  12(2269-2292):5–2  2011.

[29] S. Wasserman  L.and Zhou. A statistical framework for differential privacy. Journal of the American

Statistical Association  105(489):375–389  2010.

[30] O. Williams and F. McSherry. Probabilistic inference and differential privacy. In NIPS  2010.

9

,Ziteng Wang
Kai Fan
Jiaqi Zhang
Liwei Wang