2015,Learning with Group Invariant Features: A Kernel Perspective.,We analyze in this paper a random feature map based on a  theory of invariance (\emph{I-theory}) introduced in \cite{AnselmiLRMTP13}. More specifically  a group invariant  signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods  as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a  set of $N$ points. Moreover  we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally  we quantify error rates of the convergence of the empirical risk minimization  as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification  in a classical supervised learning setting,Learning with Group Invariant Features:

A Kernel Perspective.

Youssef Mroueh
IBM Watson Group

mroueh@us.ibm.com

Tomaso Poggio
CBMM  MIT .

tp@ai.mit.edu

Stephen Voinea∗
CBMM  MIT.
∗Co-ﬁrst author

voinea@mit.edu

Abstract

We analyze in this paper a random feature map based on a theory of invariance
(I-theory) introduced in [1]. More speciﬁcally  a group invariant signal signature
is obtained through cumulative distributions of group-transformed random pro-
jections. Our analysis bridges invariant feature learning with kernel methods  as
we show that this feature map deﬁnes an expected Haar-integration kernel that is
invariant to the speciﬁed group action. We show how this non-linear random fea-
ture map approximates this group invariant kernel uniformly on a set of N points.
Moreover  we show that it deﬁnes a function space that is dense in the equivalent
Invariant Reproducing Kernel Hilbert Space. Finally  we quantify error rates of
the convergence of the empirical risk minimization  as well as the reduction in the
sample complexity of a learning algorithm using such an invariant representation
for signal classiﬁcation  in a classical supervised learning setting.

1

Introduction

Encoding signals or building similarity kernels that are invariant to the action of a group is a key
problem in unsupervised learning  as it reduces the complexity of the learning task and mimics how
our brain represents information invariantly to symmetries and various nuisance factors (change in
lighting in image classiﬁcation and pitch variation in speech recognition) [1  2  3  4]. Convolutional
neural networks [5  6] achieve state of the art performance in many computer vision and speech
recognition tasks  but require a large amount of labeled examples as well as augmented data  where
we reﬂect symmetries of the world through virtual examples [7  8] obtained by applying identity-
preserving transformations such as shearing  rotation  translation  etc.  to the training data. In this
work  we adopt the approach of [1]  where the representation of the signal is designed to reﬂect
the invariant properties and model the world symmetries with group actions. The ultimate aim is
to bridge unsupervised learning of invariant representations with invariant kernel methods  where
we can use tools from classical supervised learning to easily address the statistical consistency and
sample complexity questions [9  10]. Indeed  many invariant kernel methods and related invariant
kernel networks have been proposed. We refer the reader to the related work section for a review
(Section 5) and we start by showing how to accomplish this invariance through group-invariant Haar-
integration kernels [11]  and then show how random features derived from a memory-based theory
of invariances introduced in [1] approximate such a kernel.

1.1 Group Invariant Kernels

We start by reviewing group-invariant Haar-integration kernels introduced in [11]  and their use in a
binary classiﬁcation problem. This section highlights the conceptual advantages of such kernels as
well as their practical inconvenience  putting into perspective the advantage of approximating them
with explicit and invariant random feature maps.

1

K(x  z) =

(cid:90)

(cid:90)

Invariant Haar-Integration Kernels. We consider a subset X of the hypersphere in d dimensions
Sd−1. Let ρX be a measure on X . Consider a kernel k0 on X   such as a radial basis function kernel.
Let G be a group acting on X   with a normalized Haar measure µ. G is assumed to be a compact
and unitary group. Deﬁne an invariant kernel K between x  z ∈ X through Haar-integration [11] as
follows:

k0(gx  g(cid:48)z)dµ(g)dµ(g(cid:48)).

G

G

(1)
As we are integrating over the entire group  it is easy to see that: K(g(cid:48)x  gz) = K(x  z)  ∀g  g(cid:48) ∈
G ∀x  z ∈ X . Hence the Haar-integration kernel is invariant to the group action. The symmetry of
K is obvious. Moreover  if k0 is a positive deﬁnite kernel  it follows that K is positive deﬁnite as
well [11]. One can see the Haar-integration kernel framework as another form of data augmentation 
since we have to produce group-transformed points in order to compute the kernel.
Invariant Decision Boundary. Turning now to a binary classiﬁcation problem  we assume that we
are given a labeled training set: S = {(xi  yi) | xi ∈ X   yi ∈ Y = {±1}}N
i=1. In order to learn a
decision function f : X → Y  we minimize the following empirical risk induced by an L-Lipschitz 
convex loss function V   with V (cid:48)(0) < 0 [12]: minf∈HK ˆEV (f ) := 1
i=1 V (yif (xi))  where we
restrict f to belong to a hypothesis class induced by the invariant kernel K  the so called Reproducing
Kernel Hilbert Space HK. The representer theorem [13] shows that the solution of such a problem 
i K(x  xi). Since the
or the optimal decision boundary f∗
kernel K is group-invariant it follows that : f∗
i=1 αiK(x  xi) =
N (x)  ∀g ∈ G. Hence the the decision boundary f∗is group-invariant as well  and we have:
f∗
f∗
N (gx) = f∗
Reduced Sample Complexity. We have shown that a group-invariant kernel induces a group-
invariant decision boundary  but how does this translate to the sample complexity of the learning
algorithm? To answer this question  we will assume that the input set X has the following structure:
X = X0 ∪ GX0  GX0 = {z|z = gx  x ∈ X0  g ∈ G/ {e}}  where e is the identity group element.
This structure implies that for a function f in the invariant RKHS HK  we have:

(cid:80)N
N (x) =(cid:80)N
i=1 αiK(gx  xi) =(cid:80)N

N (gx) =(cid:80)N

N has the following form: f∗

N (x) ∀g ∈ G ∀x ∈ X .

i=1 α∗

N

∀z ∈ GX0 ∃ x ∈ X0 ∃ g ∈ G such that  z = gx  and f (z) = f (x).

Let ρy(x) = P(Y = y|x) be the label posteriors. We assume that ρy(gx) = ρy(x) ∀g ∈ G. This
is a natural assumption since the label is unchanged given the group action. Assume that the set X
is endowed with a measure ρX that is also group-invariant. Let f be the group-invariant decision
function and consider the expected risk induced by the loss V   EV (f )  deﬁned as follows:

V (yf (x))ρy(x)ρX (x)dx 

(2)

(cid:90)

(cid:88)

X

y∈Y

EV (f ) =

EV (f ) is a proxy to the misclassiﬁcation risk [12]. Using the invariant properties of the function
class and the data distribution we have by invariance of f  ρy  and ρ:

V (yf (x))ρy(x)ρX (x)dx +

V (yf (z))ρy(z)ρX (z)dz

(cid:90)

(cid:88)

GX0

y∈Y

(cid:88)

y∈Y

(cid:90)
(cid:90)
(cid:90)
(cid:90)

X0

G

G

dµ(g)

(cid:88)

X0

y∈Y

dµ(g)

X0

(cid:90)
(cid:90)

(cid:88)
(cid:88)

y∈Y

X0

y∈Y

EV (f ) =

=

=

=

V (yf (x))ρy(x)ρX (x)dx.

V (yf (gx))ρy(gx)ρX (x)dx

V (yf (x))ρy(x)ρX (x)dx (By invariance of f  ρy  and ρ )

Hence  given an invariant kernel to a group action that is identity preserving  it is sufﬁcient to
minimize the empirical risk on the core set X0  and it generalizes to samples in GX0.
Let us imagine that X is ﬁnite with cardinality |X|; the cardinality of the core set X0 is a small
fraction of the cardinality of X : |X0| = α|X|  where 0 < α < 1. Hence  when we sample training
points from X0  the maximum size of the training set is N = α|X| << |X|  yielding a reduction in
the sample complexity.

2

1.2 Contributions

We have just reviewed the group-invariant Haar-integration kernel. In summary  a group-invariant
kernel implies the existence of a decision function that is invariant to the group action  as well as
a reduction in the sample complexity due to sampling training points from a reduced set  a.k.a the
core set X0.
Kernel methods with Haar-integration kernels come at a very expensive computational price at both
training and test time: computing the Kernel is computationally cumbersome as we have to integrate
over the group and produce virtual examples by transforming points explicitly through the group
action. Moreover  the training complexity of kernel methods scales cubicly in the sample size.
Those practical considerations make the usefulness of such kernels very limited.
The contributions of this paper are on three folds:

1. We ﬁrst show that a non-linear random feature map Φ : X → RD derived from a memory-
based theory of invariances introduced in [1] induces an expected group-invariant Haar-
integration kernel K. For ﬁxed points x  z ∈ X   we have: E(cid:104)Φ(x)  Φ(z)(cid:105) = K(x  z) 
where K satisﬁes: K(gx  g(cid:48)z) = K(x  z) ∀g  g(cid:48) ∈ G  x  z ∈ X .

2. We show a Johnson-Lindenstrauss type result that holds uniformly on a set of N points that
assess the concentration of this random feature map around its expected induced kernel. For
sufﬁciently large D  we have (cid:104)Φ(x)  Φ(z)(cid:105) ≈ K(x  z)  uniformly on an N points set.
3. We show that  with a linear model  an invariant decision function can be learned in this
N (x) ≈ (cid:104)w∗  Φ(x)(cid:105)
random feature space by sampling points from the core set X0 i.e: f∗
and generalizes to unseen points in GX0  reducing the sample complexity. Moreover  we
show that those features deﬁne a function space that approximates a dense subset of the
invariant RKHS  and assess the error rates of the empirical risk minimization using such
random features.

4. We demonstrate the validity of these claims on three datasets:

(MNIST)  and speech (TIDIGITS).

text (artiﬁcial)  vision

2 From Group Invariant Kernels to Feature Maps
In this paper we show that a random feature map based on I-theory [1]: Φ : X → RD approximates
a group-invariant Haar-integration kernel K having the form given in Equation (1):

(cid:104)Φ(x)  Φ(z)(cid:105) ≈ K(x  z).

We start with some notation that will be useful for deﬁning the feature map. Denote the cumulative
distribution function of a random variable X by 

FX (τ ) = P(X ≤ τ ) 

Fix x ∈ X   Let g ∈ G be a random variable drawn according to the normalized Haar measure µ and
let t be a random template whose distribution will be deﬁned later. For s > 0  deﬁne the following
truncated cumulative distribution function (CDF) of the dot product (cid:104)x  gt(cid:105):

ψ(x  t  τ ) = Pg((cid:104)x  gt(cid:105) ≤ τ ) = F(cid:104)x gt(cid:105)(τ )  τ ∈ [−s  s]  x ∈ X  

Let ε ∈ (0  1). We consider the following Gaussian vectors (sampling with rejection) for the tem-
plates t:

(cid:18)

(cid:19)

t = n ∼ N

0 

Id

1
d

  if (cid:107)n(cid:107)2

2 < 1 + ε  t =⊥ else .

The reason behind this sampling is to keep the range of (cid:104)x  gt(cid:105) under control: The squared norm
(cid:107)n(cid:107)2
2 will be bounded by 1 + ε with high probability by a classical concentration result (See proof
of Theorem 1 for more details). The group being unitary and x ∈ Sd−1  we know that : |(cid:104)x  gt(cid:105)| ≤
(cid:107)n(cid:107)2 <
Remark 1. We can also consider templates t  drawn uniformly on the unit sphere Sd−1. Uniform
templates on the sphere can be drawn as follows:

1 + ε ≤ 1 + ε  for ε ∈ (0  1).

√

t =

ν
(cid:107)ν(cid:107)2

  ν ∼ N (0  Id) 

3

√

since the norm of a gaussian vector is highly concentrated around its mean
d  we can use the
gaussian sampling with rejection. Results proved for gaussian templates (with rejection) will hold
true for templates drawn at uniform on the sphere with different constants.

Deﬁne the following kernel function 

Ks(x  z) = Et

ψ(x  t  τ )ψ(z  t  τ )dτ 

Let ¯g ∈ G. As the group is closed  we have ψ(t  ¯gx  τ ) = (cid:82)
(cid:82)

where s will be ﬁxed throughout the paper to be s = 1+ε since the gaussian sampling with rejection
controls the dot product to be in that range.
G 1I(cid:104)g¯gx t(cid:105)≤τ dµ(g) =
G 1I(cid:104)gx t(cid:105)≤τ dµ(g) = ψ(t  x  τ ) and hence K(gx  g(cid:48)z) = K(x  z)  for all g  g(cid:48) ∈ G. It is clear
now that K is a group-invariant kernel.
In order to approximate K  we sample |G| elements uniformly and independently from the group
G  i.e. gi  i = 1 . . .|G|  and deﬁne the normalized empirical CDF :

(cid:90) s

−s

We discretize the continuous threshold τ as follows:

|G|(cid:88)
1I(cid:104)git x(cid:105)≤τ   − s ≤ τ ≤ s.
|G|(cid:88)

i=1

1I(cid:104)git x(cid:105)≤ s

n k  − n ≤ k ≤ n.

1

m

|G|√
√
s√
nm|G|

φ(x  t  τ ) =

(cid:18)

φ

x  t 

=

(cid:19)

sk
n

(cid:20)

(cid:18)

Φ(x) =

φ

x  tj 

i=1

(cid:19)(cid:21)

sk
n

We sample m templates independently according to the Gaussian sampling with rejection  tj  j =
1 . . . m. We are now ready to deﬁne the random feature map Φ:

It is easy to see that:

lim
n→∞

Et g (cid:104)Φ(x)  Φ(z)(cid:105)R(2n+1)×m = lim
n→∞

Et g

j=1...m k=−n...n

(cid:18)

m(cid:88)

n(cid:88)

j=1

k=−n

∈ R(2n+1)×m.

(cid:19)

(cid:18)

sk
n

φ

x  tj 

φ

z  tj 

(cid:19)

sk
n

= Ks(x  z).

In Section 3 we study the geometric information captured by this kernel by stating explicitly the
similarity it computes.
Remark 2 (Efﬁciency of the representation). 1) The main advantage of such a feature map  as
outlined in [1]  is that we store transformed templates in order to compute Φ  while if we wanted
to compute an invariant kernel of type K (Equation (1))  we would need to explicitly transform
the points. The latter is computationally expensive. Storing transformed templates and computing
the signature Φ is much more efﬁcient. It falls in the category of memory-based learning  and is
biologically plausible [1].
2) As |G| m n get large enough  the feature map Φ approximates a group-invariant Kernel  as we
will see in next section.

3 An Equivalent Expected Kernel and a Uniform Concentration Result

In this section we present our main results  with proofs given in the supplementary material . Theo-
rem 1 shows that the random feature map Φ  deﬁned in the previous section  corresponds in expec-
tation to a group-invariant Haar-integration kernel Ks(x  z). Moreover  s − Ks(x  z) computes the
average pairwise distance between all points in the orbits of x and z  where the orbit is deﬁned as
the collection of all group-transformations of a given point x : Ox = {gx  g ∈ G}.
Theorem 1 (Expectation). Let ε ∈ (0  1) and x  z ∈ X . Deﬁne the distance dG between the orbits
Ox and Oz:

1√
2πd
and the group-invariant expected kernel

dG(x  z) =

(cid:107)gx − g(cid:48)z(cid:107)2 dµ(g)dµ(g(cid:48)) 

(cid:90)

(cid:90)

G

G

(cid:90) s

−s

ψ(x  t  τ )ψ(z  t  τ )dτ  s = 1 + ε.

Ks(x  z) = lim
n→∞

Et g (cid:104)Φ(x)  Φ(z)(cid:105)R(2n+1)×m = Et

4

1. The following inequality holds with probability 1:

ε − δ2(d  ε) ≤ Ks(x  z) − (1 − dG(x  z)) ≤ ε + δ1(d  ε) 

(3)

where δ1(ε  d) = e−dε2/16√

d

− 1

2

e−εd/2(1+ε)

√

d

d
2

and δ2(ε  δ) = e−dε2 /16√

d

+ (1 + ε)e−dε2/8.

2. For any ε ∈ (0  1) as the dimension d → ∞ we have δ1(ε  d) → 0 and δ2(ε  d) → 0  and

we have asymptotically Ks(x  z) → 1 − dG(x  z) + ε = s − dG(x  z).

3. Ks is symmetric and Ks is positive semi-deﬁnite.

Remark 3. 1) ε  δ1(d  ε)  and δ2(d  ε) are not errors due to results holding with high probability
but are due to the truncation and are a technical artifact of the proof. 2) Local invariance can be
deﬁned by restricting the sampling of the group elements to a subset G ⊂ G. Assuming that for each
g ∈ G  g−1 ∈ G  the equivalent kernel has asymptotically the following form:
(cid:107)gx − g(cid:48)z(cid:107)2 dµ(g)dµ(g(cid:48)).

Ks(x  z) ≈ s − 1√

(cid:90)

(cid:90)

3) The norm-one constraint can be relaxed  let R = supx∈X (cid:107)x(cid:107)2 < ∞  hence we can set s =
R(1 + ε)  and

2πd

G

G

−δ2(d  ε) ≤ Ks(x  z) − (R(1 + ε) − dG(x  z)) ≤ δ1(d  ε) 

(4)

where δ1(ε  d) = R e−dε2 /16√

d

− R

2

e−εd/2(1+ε)

√

d

d
2

and δ2(ε  δ) = R e−dε2/16√

d

+ R(1 + ε)e−dε2/8.

Theorem 2 is  in a sense  an invariant Johnson-Lindenstrauss [14] type result where we show that
the dot product deﬁned by the random feature map Φ   i.e (cid:104)Φ(x)  Φ(z)(cid:105)  is concentrated around the
invariant expected kernel uniformly on a data set of N points  given a sufﬁciently large number of
templates m  a large number of sampled group elements |G|  and a large bin number n. The error
naturally decomposes to a numerical error ε0 and statistical errors ε1  ε2 due to the sampling of the
templates and the group elements respectively.
Theorem 2. [Johnson-Lindenstrauss type Theorem- N point Set] Let D = {xi | xi ∈ X}N
be a ﬁnite dataset. Fix ε0  ε1  ε2  δ1  δ2 ∈ (0  1). For a number of bins n ≥ 1
  templates m ≥
)  where C1  C2 are universal numeric constants 

)  and group elements |G| ≥ C2

i=1

ε0

log( N m
δ2

ε2
2

C1
log( N
ε2
δ1
1
we have:

|(cid:104)Φ(xi)  Φ(xj)(cid:105) − Ks(xi  xj)| ≤ ε0 + ε1 + ε2  i = 1 . . . N  j = 1 . . . N 

(5)

with probability 1 − δ1 − δ2.
Putting together Theorems 1 and 2  the following Corollary shows how the group-invariant random
feature map Φ captures the invariant distance between points uniformly on a dataset of N points.
Corollary 1 (Invariant Features Maps and Distances between Orbits). Let D = {xi | xi ∈ X}N
be a ﬁnite dataset. Fix ε0  δ ∈ (0  1). For a number of bins n ≥ 3
δ ) 
log( N
and group elements |G| ≥ 9C2
δ )  where C1  C2 are universal numeric constants  we have:
(6)

ε − δ2(d  ε) − ε0 ≤ (cid:104)Φ(xi)  Φ(xj)(cid:105) − (1 − dG(xi  xj)) ≤ ε0 + ε + δ1(d  ε) 

  templates m ≥ 9C1

i = 1 . . . N  j = 1 . . . N  with probability 1 − 2δ.
Remark 4. Assuming that the templates are unitary and drawn form a general distribution p(t)  the
equivalent kernel has the following form:

log( N m

i=1

ε2
0

ε2
0

ε0

dµ(g)dµ(g(cid:48))

s − max((cid:104)x  gt(cid:105)  (cid:104)z  g(cid:48)t(cid:105))p(t)dt

.

Ks(x  z) =

G

G

Indeed when we use the gaussian sampling with rejection for the templates 

(cid:82) max((cid:104)x  gt(cid:105)  (cid:104)z  g(cid:48)t(cid:105))p(t)dt is asymptotically proportional to

the integral
. It is interesting
to consider different distributions that are domain-speciﬁc for the templates and assess the number
of the templates needed to approximate such kernels. It is also interesting to ﬁnd the optimal tem-
plates that achieve the minimum distortion in equation 6  in a data dependent way  but we will
address these points in future work.

(cid:13)(cid:13)(cid:13)g−1x − g

(cid:13)(cid:13)(cid:13)2

(cid:48) −1z

(cid:90)

(cid:90)

(cid:18)(cid:90)

(cid:19)

5

4 Learning with Group Invariant Random Features

(cid:80)|G|

t. The RKHS deﬁned by the invariant kernel Ks  Ks(x  z) = (cid:82)(cid:82) s

In this section  we show that learning a linear model in the invariant  random feature space  on a
training set sampled from the reduced core set X0  has a low expected risk  and generalizes to unseen
test points generated from the distribution on X = X0 ∪ GX0. The architecture of the proof follows
ideas from [15] and [16]. Recall that given an L-Lipschitz convex loss function V   our aim is to
minimize the expected risk given in Equation (2). Denote the CDF by ψ(x  t  τ ) = P((cid:104)gt  x(cid:105) ≤ τ ) 
and the empirical CDF by ˆψ(x  t  τ ) = 1|G|
i=1 1I(cid:104)git x(cid:105)≤τ . Let p(t) be the distribution of templates
−s ψ(x  t  τ )ψ(z  t  τ )p(t)dtdτ
denoted HKs   is the completion of the set of all ﬁnite linear combinations of the form:
(cid:27)

Similarly to [16]  we deﬁne the following inﬁnite-dimensional function space:
|w(t  τ )|

(cid:88)
(cid:90) (cid:90) s

αiKs(x  xi)  xi ∈ X   αi ∈ R.

f (x) =

(cid:26)

(7)

i

Fp =

Lemma 1. Fp is dense in HKs. For f ∈ Fp we have EV (f ) =(cid:82)

w(t  τ )ψ(x  t  τ )dtdτ | sup

f (x) =

−s

τ t

(cid:80)

X0

p(t)
y∈Y V (yf (x))ρy(x)dρX (x) 

where X0 is the reduced core set.
(cid:104) ˆψ(cid:0)x  tj  sk
Since Fp is dense in HKs  we can learn an invariant decision function in the space Fp  instead
of learning in HKs. Let Ψ(x) =
. Ψ  and Φ are equivalent up to
constants. We will approximate the set Fp as follows:
n(cid:88)
m(cid:88)

j=1...m k=−n...n

(cid:1)(cid:105)
(cid:18)

(cid:19)

n

f (x) = (cid:104)w  Ψ(x)(cid:105) =

sk
n

  tj ∼ p  j = 1 . . . m | (cid:107)w(cid:107)∞ ≤ C
m

wj k

ˆψ

x  tj 

 .

˜F =

s
n

j=1

k=−n

≤ C

.

Hence  we learn the invariant decision function via empirical risk minimization where we restrict
the function to belong to ˜F  and the sampling in the training set is restricted to the core set X0. Note
that with this function space we are regularizing for convenience the norm inﬁnity of the weights
but this can be relaxed in practice to a classical Tikhonov regularization.
Theorem 3 (Learning with Group invariant features). Let S = {(xi  yi) | xi ∈ X0  yi ∈
Y  i = 1 . . . N}  a training set sampled from the core set X0. Let f∗
N = arg minf∈ ˜F ˆEV (f ) =

(cid:80)N

1
N

i=1 V (yif (xi)).Fix δ > 0  then
1√
EV (f∗
N

N ) ≤ min
f∈Fp

EV (f ) + 2

(cid:32)

(cid:115)

+

2sLC√
m

(cid:32)
(cid:18) 1

δ

(cid:19)(cid:33)

4LsC + 2V (0) + LC

(cid:32)

(cid:18)

2sC(cid:112)|G|

(cid:115)

log

1
2

(cid:114)

(cid:19)(cid:33)
(cid:18) 1
(cid:17)(cid:19)
(cid:16) m

δ

δ

(cid:33)

 

+

2sC

n

1 +

2 log

+ L

1 +

2 log

with probability at least 1 − 3δ on the training set and the choice of templates and group elements.
The proof of Theorem 3 is given in Appendix B. Theorem 3 shows that learning a linear model
in the invariant random feature space deﬁned by Φ (or equivalently Ψ)  has a low expected
risk. More importantly  this risk is arbitrarily close to the optimal risk achieved in an inﬁnite-
dimensional class of functions  namely Fp. The training set is sampled from the reduced core
set X0  and invariant learning generalizes to unseen test points generated from the distribution
on X = X0 ∪ GX0  hence the reduction in the sample complexity. Recall that Fp is dense in
the RKHS of the Haar-integration invariant Kernel  and so the expected risk achieved by a linear
model in the invariant random feature space is not far from the one attainable in the invariant
RKHS. Note that the error decomposes into two terms. The ﬁrst  O( 1√
)  is statistical and it
depends on the training sample complexity N. The other is governed by the approximation error of
functions Fp  with functions in ˜F  and depends on the number of templates m  number of group
elements sampled |G|  the number of bins n  and has the following form O( 1√
n.
+ 1

(cid:16)(cid:113) log m|G|

m )+O

(cid:17)

N

6

5 Relation to Previous Work

We now put our contributions in perspective by outlining some of the previous work on invariant
kernels and approximating kernels with random features.
Approximating Kernels. Several schemes have been proposed for approximating a non-linear ker-
nel with an explicit non-linear feature map in conjunction with linear methods  such as the Nystr¨om
method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels
[15]. Our features fall under the random sampling techniques where  unlike previous work  we sam-
ple both projections and group elements to induce invariance with an integral representation. We
note that the relation between random features and quadrature rules has been thoroughly studied in
[18]  where sharper bounds and error rates are derived  and can apply to our setting.
Invariant Kernels. We focused in this paper on Haar-integration kernels [11]  since they have an
integral representation and hence can be represented with random features [18]. Other invariant
kernels have been proposed: In [19] authors introduce transformation invariant kernels  but unlike
our general setting  the analysis is concerned with dilation invariance. In [20]  multilayer arccosine
kernels are built by composing kernels that have an integral representation  but does not explicitly
induce invariance. More closely related to our work is [21]  where kernel descriptors are built for vi-
sual recognition by introducing a kernel view of histogram of gradients that corresponds in our case
to the cumulative distribution on the group variable. Explicit feature maps are obtained via kernel
PCA  while our features are obtained via random sampling. Finally the convolutional kernel network
of [22] builds a sequence of multilayer kernels that have an integral representation  by convolution 
considering spatial neighborhoods in an image. Our future work will consider the composition of
Haar-integration kernels  where the convolution is applied not only to the spatial variable but to the
group variable akin to [2].

6 Numerical Evaluation

F + λ||W||2

F

(cid:8) 1
N ||Y − Φ(X)W||2

(cid:9)  where ||·||F is the Frobenius norm 

In this paper  and speciﬁcally in Theorems 2 and 3  we showed that the random  group-invariant
feature map Φ captures the invariant distance between points  and that learning a linear model
trained in the invariant  random feature space will generalize well to unseen test points.
In this
section  we validate these claims through three experiments. For the claims of Theorem 2  we
will use a nearest neighbor classiﬁer  while for Theorem 3  we will rely on the regularized least
squares (RLS) classiﬁer  one of the simplest algorithms for supervised learning. While our proofs
focus on norm-inﬁnity regularization  RLS corresponds to Tikhonov regularization with square
loss. Speciﬁcally  for performing T−way classiﬁcation on a batch of N training points in Rd 
summarized in the data matrix X ∈ RN×d and label matrix Y ∈ RN×T   RLS will perform the
optimization  minW∈Rm×T
λ is the regularization parameter  and Φ is the feature map  which for the representation described
in this paper will be a CDF pooling of the data projected onto group-transformed random templates.
All RLS experiments in this paper were completed with the GURLS toolbox [23]. The three
datasets we explore are:
Xperm (Figure 1): An artiﬁcial dataset consisting of all sequences of length 5 whose elements
come from an alphabet of 8 characters. We want to learn a function which assigns a positive value
to any sequence that contains a target set of characters (in our case  two of them) regardless of their
position. Thus  the function label is globally invariant to permutation  and so we project our data
onto all permuted versions of our random template sequences.
MNIST (Figure 2): We seek local invariance to translation and rotation  and so all random templates
are translated by up to 3 pixels in all directions and rotated between -20 and 20 degrees.
TIDIGITS (Figure 3): We use a subset of TIDIGITS consisting of 326 speakers (men  women 
children) reading the digits 0-9 in isolation  and so each datapoint is a waveform of a single word.
We seek local invariance to pitch and speaking rate [25]  and so all random templates are pitch
shifted up and down by 400 cents and warped to play at half and double speed. The task is 10-way
classiﬁcation with one class-per-digit. See [24] for more detail.

Acknowledgements: Stephen Voinea acknowledges the support of a Nuance Foundation Grant.
This work was also supported in part by the Center for Brains  Minds and Machines (CBMM) 
funded by NSF STC award CCF 1231216.

7

Figure 1: Classiﬁcation accuracy as a function of training set size  averaged over 100 random
training samples at each size. Φ = CDF(n  m) refers to a random feature map with n bins and m
templates. With 25 templates  the random feature map outperforms the raw features and a bag-of-
words representation (also invariant to permutation) and even approaches an RLS classiﬁer with a
Haar-integration kernel. Error bars were removed from the RLS plot for clarity. See supplement.

Figure 2: Left Plot) Mean classiﬁcation accuracy as a function of number of bins and templates 
averaged over 30 random sets of templates. Right Plot) Classiﬁcation accuracy as a function of
training set size  averaged over 100 random samples of the training set at each size. At 1000 exam-
ples per class  we achieve an accuracy of 98.97%.

Figure 3: Mean classiﬁcation accuracy as a function of number of bins and templates  averaged
over 30 random sets of templates. In the “Speaker” dataset  we test on unseen speakers  and in the
“Gender” dataset  we test on a new gender  giving us an extreme train/test mismatch. [25].

8

0.40.50.60.70.80.91.0101001000Number of Training Points Per ClassAccuracyΦ RawBag−Of−WordsHaarCDF(25 1)CDF(25 10)CDF(25 25)Xperm Sample Complexity RLS101001000Number of Training Points Per ClassΦ RawBag−Of−WordsCDF(25 1)Xperm Sample Complexity 1−NN0.10.20.30.40.50.60.70.80.91.0110100Number of TemplatesAccuracyBins525MNIST Accuracy RLS (1000 Points Per Class)0.50.60.70.80.91.0101001000Number of Training Points Per ClassΦ RawCDF(50 500)MNIST Sample Complexity RLS0.00.10.20.30.40.50.60.70.80.91.0101001000Number of TemplatesAccuracyBins525100TIDIGITS Speaker RLS101001000Number of TemplatesBins525100TIDIGITS Gender RLSReferences
[1] F. Anselmi  J. Z. Leibo  L. Rosasco  J. Mutch  A. Tacchetti  and T. Poggio  “Unsupervised
learning of invariant representations in hierarchical architectures. ” CoRR  vol. abs/1311.4158 
2013.

[2] J. Bruna and S. Mallat  “Invariant scattering convolution networks ” CoRR  vol. abs/1203.1513 

2012.

[3] G. Hinton  A. Krizhevsky  and S. Wang  “Transforming auto encoders ” ICANN-11  2011.
[4] Y. Bengio  A. C. Courville  and P. Vincent  “Representation learning: A review and new per-

spectives ” IEEE Trans. Pattern Anal. Mach. Intell.  vol. 35  no. 8  pp. 1798–1828  2013.

[5] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document

recognition ” in Proceedings of the IEEE  vol. 86  pp. 2278–2324  1998.

[6] A. Krizhevsky  I. Sutskever  and G. E. Hinton  “Imagenet classiﬁcation with deep convolutional

neural networks. ” in NIPS  pp. 1106–1114  2012.

[7] P. Niyogi  F. Girosi  and T. Poggio  “Incorporating prior information in machine learning by

creating virtual examples ” in Proceedings of the IEEE  pp. 2196–2209  1998.

[8] Y.-A. Mostafa  “Learning from hints in neural networks ” Journal of complexity  vol. 6 

pp. 192–198  June 1990.

[9] V. N. Vapnik  Statistical learning theory. A Wiley-Interscience Publication 1998.
[10] I. Steinwart and A. Christmann  Support vector machines. Information Science and Statistics 

New York: Springer  2008.

[11] B. Haasdonk  A. Vossen  and H. Burkhardt  “Invariance in kernel methods by haar-integration

kernels. ” in SCIA   Springer  2005.

[12] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe  “Convexity  classiﬁcation  and risk bounds ”

Journal of the American Statistical Association  vol. 101  no. 473  pp. 138–156  2006.

[13] G. Wahba  Spline models for observational data  vol. 59 of CBMS-NSF Regional Conference

Series in Applied Mathematics. Philadelphia  PA: SIAM  1990.

[14] W. B. Johnson and J. Lindenstrauss  “Extensions of lipschitz mappings into a hilbert space. ”

Conference in modern analysis and probability  1984.

[15] A. Rahimi and B. Recht  “Weighted sums of random kitchen sinks: Replacing minimization

with randomization in learning. ” in NIPS 2008.

[16] A. Rahimi and B. Recht  “Uniform approximation of functions with random bases ” in Pro-

ceedings of the 46th Annual Allerton Conference  2008.

[17] C. Williams and M. Seeger  “Using the nystrm method to speed up kernel machines ” in NIPS 

2001.

[18] F. R. Bach  “On the equivalence between quadrature rules and random features ” CoRR 

vol. abs/1502.06800  2015.

[19] C. Walder and O. Chapelle  “Learning with transformation invariant kernels ” in NIPS  2007.
[20] Y. Cho and L. K. Saul  “Kernel methods for deep learning ” in NIPS  pp. 342–350  2009.
[21] L. Bo  X. Ren  and D. Fox  “Kernel descriptors for visual recognition ” in NIPS.  2010.
[22] J. Mairal  P. Koniusz  Z. Harchaoui  and C. Schmid  “Convolutional kernel networks ” in NIPS 

2014.

[23] A. Tacchetti  P. K. Mallapragada  M. Santoro  and L. Rosasco  “Gurls: a least squares library

for supervised learning ” CoRR  vol. abs/1303.0934  2013.

[24] S. Voinea  C. Zhang  G. Evangelopoulos  L. Rosasco  and T. Poggio  “Word-level invariant

representations from acoustic waveforms ” vol. 14  pp. 3201–3205  September 2014.

[25] M. Benzeghiba  R. De Mori  O. Deroo  S. Dupont  T. Erbes  D. Jouvet  L. Fissore  P. Laface 
A. Mertins  C. Ris  R. Rose  V. Tyagi  and C. Wellekens  “Automatic speech recognition and
speech variability: A review ” Speech Communication  vol. 49  pp. 763–786  01 2007.

9

,Youssef Mroueh
Stephen Voinea
Tomaso Poggio