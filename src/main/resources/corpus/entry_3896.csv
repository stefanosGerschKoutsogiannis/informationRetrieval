2009,On Learning Rotations,An algorithm is presented for online learning of rotations. The proposed algorithm involves matrix exponentiated gradient updates and is motivated by the Von Neumann divergence. The additive updates are skew-symmetric matrices with trace zero which comprise the Lie algebra of the rotation group. The orthogonality and unit determinant of the matrix  parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact Lie groups. The stability and the computational complexity of the algorithm are discussed.,On Learning Rotations

Raman Arora

University of Wisconsin-Madison

Department of Electrical and Computer Engineering

1415 Engineering Drive  Madison  WI 53706

rmnarora@u.washington.edu

Abstract

An algorithm is presented for online learning of rotations. The proposed algorithm
involves matrix exponentiated gradient updates and is motivated by the von Neu-
mann divergence. The multiplicative updates are exponentiated skew-symmetric
matrices which comprise the Lie algebra of the rotation group. The orthonormal-
ity and unit determinant of the matrix parameter are preserved using matrix log-
arithms and exponentials and the algorithm lends itself to intuitive interpretation
in terms of the differential geometry of the manifold associated with the rotation
group. A complexity reduction result is presented that exploits the eigenstructure
of the matrix updates to simplify matrix exponentiation to a quadratic form.

1 Introduction

The problem of learning rotations ﬁnds application in many areas of signal processing and machine
learning. It is an important problem since many problems can be reduced to that of learning rota-
tions; for instance Euclidean motion in Rn−1 is simply rotation in Rn. A conformal embedding was
presented in [1] that extends rotations to a representation for all Euclidean transformations. Further-
more  the rotation group provides a universal representation for all Lie groups. This was established
in [2] by showing that any Lie algebra can be expressed as a bivector algebra. Since the Lie algebra
describes the structure of the associated Lie group completely  any Lie group can be represented as
rotation group.
The batch version of the problem was originally posed as the problem of estimating the attitude of
satellites by Wahba in 1965 [3]. In psychometrics  it was presented as the orthogonal Procrustes
problem [4]. It has been studied in various forms over the last few decades and ﬁnds application in
many areas of computer vision [5  6  7]  face recognition [8]  robotics [9  10]  crystallography[11]
and physics [12].
While the batch version of the problem is well understood  the online learning of rotations from
vector instances is challenging since the manifold associated with the rotation group is a curved
space and it is not possible to form updates that are linear combinations of rotations [13]. The set
of rotations about the origin in n-dimensional Euclidean space forms a compact Lie group  SO(n) 
under the operation of composition. The manifold associated with the n-dimensional rotation group
is the unit sphere Sn−1 in n dimensional Euclidean space.

1.1 Related Work

The online version of learning rotations was posed as an open problem by Smith and Warmuth
[13]. Online learning algorithms were recently presented for some matrix groups. In [14]  an online
algorithm was proposed for learning density matrix parameters and was extended in [15] to the
problem of learning subspaces of low rank. However  the extension of these algorithms to learning
rotations will require repeated projection and approximation [13]. Adaptive algorithms were also

1

studied in [16] for optimization under unitary matrix constraint. The proposed methods are steepest
descent methods on Riemannian manifolds.

1.2 Our Approach

This paper presents an online algorithm for learning rotations that utilizes the Bregman matrix di-
vergence with respect to the quantum relative entropy (also known as von Neumann divergence) as a
distance measure between two rotation matrices. The resulting algorithm has matrix-exponentiated
gradient (MEG) updates [14]. The key ingredients of our approach are (a) von Neumann Divergence
between rotation matrices [17]  (b) squared error loss function and (c) matrix exponentiated gradient
(MEG) updates.
Any Lie group is also a smooth manifold and the updates in the proposed algorithm have an intuitive
interpretation in terms of the differential topology of the associated manifold. We also utilize various
elementary Lie algebra concepts to provide intuitive interpretation of the updates. The development
in the paper closely follows that of the matrix exponentiated gradient (MEG) updates in [14] for
density matrix parameters. The form of the updates are similar to steepest descent methods of [16] 
but are derived for learning rotations from vector instances using an information-theoretic approach.
The MEG updates are reduced to a quadratic form in the Lie algebra element corresponding to the
gradient of loss function on the rotation group.
The paper is organized as follows. The problem is formulated in Section 2. Section 3 presents
mathematical preliminaries in differential geometry and Bregman matrix divergence. The matrix
exponentiated gradient updates are developed in Section 4. The MEG updates are simpliﬁed in
Section 5. Experimental results are discussed in Section 6.

2 Problem Statement
Let xt be a stream of instances of n-dimensional unit vectors. Let R∗ be an unknown n× n rotation
matrix that acts on xt to give the rotated vector yt = R∗xt. The matrix ˆRt denotes the estimate
of R∗ at instance t and ˆyt = ˆRt xt represents the prediction for the rotated vector yt. The loss
incurred due to error in prediction is Lt( ˆRt) = d(ˆyt  yt)  where d(· ·) is a distance function. The
estimate of the rotation needs to be updated based on the loss incurred at every instance and the
objective is to develop an algorithm for learning R∗ that has a bounded regret.
We seek adaptive updates that solve the following optimization problem at each step 

ˆRt+1 = arg min

R

∆F (R  ˆRt) + η Lt(R) 

(1)

where ˆRt is the estimated rotation matrix at instance t  η is the learning rate or the step-size and ∆F
is a matrix divergence that measures the discrepancy between matrices. This is a typical problem
formulation in online learning where the objective comprises a loss function and a divergence term.
The parameter η balances the trade-off between the two conﬂicting goals at each update: incurring
small loss on the new data versus conﬁdence in the estimate from the previously observed data.
Minimizing the weighted objective therefore results in smooth updates as well as minimizes the loss
function.
In this paper  the updates are smoothed using the von Neumann divergence which is deﬁned for
matrices as

∆F (R  ˆRt) = tr(R log R − R log ˆRt − R + ˆRt) 

(2)
where tr(A) is the trace of the matrix A. The search is over all R ∈ SO(n)  i.e. over all n × n
matrices such that RT R = I  RRT = I and det(R) = 1.

3 Mathematical Preliminaries

This section reviews some basic deﬁnitions and concepts in linear algebra and differential geometry
that are utilized for the development of the updates in the next section.

2

3.1 Matrix Calculus
Given a real-valued matrix function F : Rn×n → R  the gradient of the function with respect to the
matrix R ∈ Rn×n is deﬁned to be the matrix [18] 

 ∂F

...

∂R11

∂F

∂Rn1

 .

···
...
···

∂F

∂R1n

...

∂F

∂Rnn

∇RF (R) =

(3)

(6)

(7)

Some of the matrix derivatives that are used later in the paper are following: for a constant matrix
Γ ∈ Rn×n 

1. ∇R tr(ΓRRT ) = (Γ + ΓT )R 
2. ∇R det(R) = det(R)(R−1)T  
3. ∇R(y − Rx)T (y − Rx) = −2(y − Rx)xT .

A related concept in differential geometry is that of the space of vectors tangent to a group at the
identity element of the group. This is deﬁned to be the Lie algebra associated with the group. It is
a convenient way of describing the inﬁnitesimal structure of a topological group about the identity
element and completely determines the associated group. The utility of the Lie algebra is due to the
fact that it is a vector space and thus it is much easier to work with it than with the linear group.
A real n × n matrix A is in the Lie algebra of the rotation group SO(n) if and only if it is a skew-
symmetric matrix (i.e. AT = −A). Furthermore  for any matrix A in the Lie algebra of SO(n) 
exp(ηA) is a one-parameter subgroup of the rotation group  parametrized by η ∈ R [19].
The matrix exponential and logarithm play an important role in relating a matrix Lie group G and
the associated Lie algebra g. The exponential of a matrix R ∈ Rn×n is given by the following
series 

exp(R) = I + R +

(4)
Given an element A ∈ g  the matrix exponential exp(A) is the corresponding element in the group.
The matrix logarithm log (R) is deﬁned to be the inverse of the matrix exponential: it maps from the
Lie group G into the Lie algebra g. The matrix logarithm is a well-deﬁned map since the exponential
map is a local diffeomorphism between a neighborhood of the zero matrix and a neighborhood of
the identity matrix [19  20].

R2 +

1
2!

R2 + ···

1
3!

3.2 Riemannian Gradient
Consider a real-valued differentiable function  Lt : SO(n) → R  deﬁned on the rotation group. The
Riemannian gradient ˜∇RLt of the function Lt on the Lie group SO(n) evaluated at the rotation
matrix R and translated to the identity (to get a Lie algebra element) is given as [16]

(5)
where ∇RLt is the matrix derivative of the cost function in the Euclidean space deﬁned in (3) at
matrix R.

RLt 

˜∇RLt = ∇RLt RT − R ∇T

3.3 Von Neumann Divergence

In any online learning problem  the choice of divergence between the parameters dictates the result-
ing updates. This paper utilizes the von Neumann divergence which is a special case of the Bregman
divergence and measures discrepancy between two matrices.
Let F be convex differentiable function deﬁned on a subset of Rn×n with the gradient f(R) =
∇RF (R). The Bregman divergence between two matrices R1 and R2 is deﬁned as

∆F (R1  R2) := F (R1) − F (R2) − tr((R1 − R2)f(R2)T ).

The gradient of Bregman divergence with respect to R1 is given as 
∇R1∆F (R1  R2) = f(R1) − f(R2).

3

Choosing the function F in the deﬁnition of Bregman divergence to be the von Neumann entropy 
given as F (R) = tr(R log R − R))  obtain the von Neumann divergence [14  17]:

∆F (R1  R2) = Tr(R1 log R1 − R1 log R2 − R1 + R2).

(8)
Finally  the gradient of the von Neumann entropy was shown to be f(R) = ∇RF (R) = log R in
[14]. Consequently  the gradient of the von Neumann divergence can be expressed as

∇R1∆F (R1  R2) = log (R1) − log (R2).

4 Online Algorithm

The problem of online learning of rotations can be expressed as the optimization problem

ˆRt+1 = arg min

R

∆F (R  ˆRt) + ηLt(R)

s.t.

RT R = I  RRT = I

det(R) = 1

(9)

(10)

where ˆRt is the estimate of the rotation matrix at time instance t and Lt is the loss incurred in the
prediction of yt. The proposed adaptive updates are matrix exponentiated gradient (MEG) updates
given as

(cid:32)

ˆRt+1 = exp

log ˆRt − η skew

t ∇RLt( ˆRt)

(11)

(cid:16) ˆRT

(cid:17)(cid:33)

 

where ∇RLt( ˆRt) is the gradient of the cost function in the Euclidean space with respect to the
rotation matrix R and skew (·) is the skew-symmetrization operator on the matrices  skew (A) =
A − AT . The updates seem intuitive  given the following elementary facts about the Lie algebraic
structure of the rotation group: (a) the gradient of loss function gives geodesic direction and velocity
vector on the unit sphere  (b) a skew-symmetric matrix is an element of Lie algebra [19  20]  (c) the
matrix logarithm maps a rotation matrix to the corresponding Lie algebra element  (d) composition
of two elements of Lie algebra yields another Lie algebra element and (e) the matrix exponential
maps a Lie algebra element to corresponding rotation matrix.
The loss function is deﬁned to be the squared error loss function and therefore the gradient of the
loss function is given by the matrix ∇RLt( ˆRt) = 2(ˆyt − yt)xT
t . This results in the online updates

ˆRt+1 = exp

log ˆRt − 2η skew

t (ˆyt − yt)xT

t

= ˆRt exp

− 2η skew

t (ˆyt − yt)xT

t

(cid:32)

(cid:32)

(cid:16) ˆRT
(cid:16) ˆRT

(cid:17)(cid:33)
(cid:17)(cid:33)

.

 

(12)

4.1 Updates Motivated by von-Neumann Divergence

The optimization problem in (10) is solved using the method of Lagrange multipliers. First observe
that the constraints RT R = I and RRT = I are redundant since one implies the other. Introducing
the Lagrangian multiplier matrix Γ for the orthonormality constraint and Lagrangian multiplier λ
for the unity determinant constraint  the objective function can be written as

J (R  Γ  λ) = ∆F (R  ˆRt) + ηLt(R) + tr(Γ(RRT − I)) + λ(det(R) − 1).

(13)

Taking the gradient on both sides of equation with respect to the matrix R  get

∇R J (R  Γ  λ) = ∇R ∆F (R  ˆRt) + η ˜∇R Lt(R)

+(Γ + ΓT )R + λ det(R)(R−1)T  

(14)

4

using the matrix derivatives from Section 3.1 and the Riemannian gradient for the loss function from
eqn. (5). Putting ∇R J (R  Γ  λ) = 0 and using the fact that ∇R∆F (R  ˆRt) = f(R) − f( ˆRt)  get
+ (Γ + ΓT )R + λ det(R)(R−1)T . (15)

0 = f(R) − f( ˆRt) + η skew

(cid:17)
t ∇RLt(R)
(cid:17) − (Γ + ΓT )R − λ det(R)(R−1)T(cid:17)

(cid:16) ˆRT
(cid:16) ˆRT

R = f−1(cid:16)

Given that f is a bijective map  write
f( ˆRt) − η skew

forces the rotation constraint. Choosing λ = det(R)−1 and Γ = −(1/2)(cid:0)R−1(cid:1)T R−1 yields the

Since the objective is convex  it is sufﬁcient to produce a choice of Lagrange multipliers that en-

t ∇RLt(R)

. (16)

following implicit update

ˆRt+1 = exp

log ˆRt − η skew

t ∇RLt( ˆRt+1)

(17)

As noted by Tsuda et. al. in [14]  the implicit updates of the form above are usually not solvable in
closed form. However  by approximating ∇RLt( ˆRt+1) with ∇RLt( ˆRt) (as in [21  14])  we obtain
an explicit update

ˆRt+1 = exp

log ˆRt − η skew

t ∇RLt( ˆRt)

.

(18)

(cid:16) ˆRT

(cid:16) ˆRT

(cid:32)

(cid:32)

(cid:17)(cid:33)

.

(cid:17)(cid:33)

The next result ensures the closure property for the matrix exponentiated gradient updates in the
equation above. In other words  the estimates for the rotation matrix do not steer away from the
manifold associated with the rotation group. Therefore  if ˆR0 ∈ SO(n) then ˆRt+1 ∈ SO(n).
Lemma 1. If ˆRt ∈ SO(n) then ˆRt+1 given by the updates in (18) is a rotation matrix in SO(n).

Proof. Using the properties of matrix logarithm and matrix exponential  express (18) as

ˆRt+1 = ˆRt exp(−ηS) 

(19)
RLt(R) ˆRt is an n × n dimensional skew-symmetric matrix with

where S = ˆRT
trace zero. Then

t ∇RLt(R) − ∇T

ˆRT

t+1

ˆRt+1 =

(cid:16) ˆRt e−ηS(cid:17)T(cid:16) ˆRt e−ηS(cid:17)
= (cid:0)e−ηS(cid:1)T ˆRT
(cid:0)e−ηS(cid:1)  
= (cid:0)e−ηS(cid:1)T (cid:0)e−ηS(cid:1)  

ˆRt

 

t

where we used the facts that ˆRt ∈ SO(n) (cid:0)eS(cid:1)T = eST   ST = −S and that e0 = I. Similarly 

= eη(−ST −S) = eη(S−S) = I 

ˆRt+1 ˆRT

t+1 = I. Finally  note that

det( ˆRt+1) = det( ˆRt e−ηS) = det( ˆRt) · det(e−ηS) = e−η Tr (S) 

since determinant of exponential of a matrix is equal to the exponential of the trace of the matrix.
And since S is a trace zero matrix  det( ˆRt+1) = 1.

4.2 Differential Geometrical Interpretation

The resulting updates in (18) have nice interpretation in terms of the differential geometry of the
rotation group. The gradient of the cost function  ∇RLt( ˆRt)  in the Euclidean space gives a tangent
direction at the current estimate of the rotation matrix. The Riemannian gradient is computed as
∇RLt( ˆRt) − ˆRt ∇T
RLt( ˆRt) ˆRt. The Riemannian gradient at the identity element of the group is
obtained by de-rotation by ˆRt  giving ˜∇RLt( ˆRt)  as in (5). The gradient corresponds to an element
of the Lie algebra  so(n)  of the rotation group. The exponential map gives the corresponding rota-
tion matrix which is the multiplicative update to the estimate of the rotation matrix at the previous
instance.

5

5 Complexity Reduction of MEG Updates

The matrix exponentiated gradient updates ensure that the estimates for the rotation matrix stay on
the manifold associated with the rotation group at each iteration. However  with the matrix ex-
ponentiation at each step  the updates are computationally intensive and in fact the computational
complexity of the updates is comparable to other approaches that would require repeated approxima-
tion and projection on to the manifold. This section discusses a fundamental complexity reduction
result to establish a simpler update by exploiting the eigen-structure of the update matrix. First ob-
serve that the matrix in the exponential in eqn. (12) (for the case of squared error loss function) can
be written as

t

 

(cid:16) ˆRT
(cid:16) ˆRT
(cid:16)

(cid:17)
t (ˆyt − yt)xT
(cid:17)
t ( ˆRtxt − R∗xt)xT
(cid:17)
t − ˆRT
t R∗xtxT
xtxT
t
t − xtxT
t RT∗ ˆRt

t

 

 

(cid:17)

 

S = −2η skew
= −2η skew
= −2η skew

(cid:16) ˆRT

t R∗xtxT

= 2η
= AT X − XA 

(cid:32)

(cid:33)

(20)
t and A ≡ 2ηRT∗ ˆRt. Each term in the matrix S is a rank-one matrix (due to pre
where X ≡ xtxT
t   respectively). Thus S is at most rank-two. Since S is skew-
and post-multiplication with xtxT
symmetric  it has (at most) two eigenvalues in a complex conjugate pair ±jλ (and n − 2 zero
eigenvalues) [22]  which allows the following simpliﬁcation.
Lemma 2. The matrix exponentiated gradient updates in eqn. (12) are equivalent to the following
updates 

sin(λ)

1 − cos(λ)

ˆRt+1 = ˆRt

 

λ

λ2

I +

S +

S2

(cid:113)
1 −(cid:0)yT

t ˆyt

(cid:1)2 and S is the skew-symmetric matrix given in eqn. (20) with eigenval-

where λ = 2η
ues ±jλ.
Note that yt  ˆyt are unit vectors in Rn and therefore λ is real-valued. The proof of the complex-
ity reduction follows easily from a generalization of the Rodrigues’ formula for computing matrix
exponentials for skew-symmetric matrix. The proof is not presented here due to space constraints
but the interested reader is referred to [23  24]. Owing to the result above the matrix exponential
reduces to a simple quadratic form involving an element from the Lie algebra of the rotation group.
The pseudocode is given in Algorithm 1.

(21)

Choose η
Initialize R1 = I
for t = 1  2  . . . do

Obtain an instance of unit vector xt ∈ Rn;
Predict the rotated vector ˆyt = ˆRt xt;
Receive the true rotated vector yt = R∗ xt;
Incur the loss Lt( ˆRt) = |yt − ˆyt|2;
Compute the matrix S = 2η

Compute the eigenvalues λ = 2η
Update the rotation matrix ˆRt+1 = ˆRt

(cid:17)

;

t ytxT

(cid:16) ˆRT
(cid:113)
1 −(cid:0)yT
t − xtyT
(cid:18)
t ˆyt
I + sin(λ)

(cid:1)2;

ˆRt

t

(cid:19)

S2

λ S + 1−cos(λ)

λ2

end

Algorithm 1: Pseudocode for Learning rotations using Matrix Exponentiated Gradient updates

6

6 Experimental Results

This section presents experimental results with the proposed algorithm for online learning of rota-
tions. The performance of the algorithm is evaluated in terms of the Frobenius norm of the difference
of the true rotation matrix and the estimate. Figure 1 shows the error plot with respect to time. The
unknown rotation is a 12 × 12 dimensional matrix and changes randomly every 200 instances. The
trajectories are averaged over 1000 random simulations. It is clear from the plot that the estimation
error decays rapidly to zero and estimates of the rotation matrices are exact.

Figure 1: Online learning of rotations: Estimate of unknown rotation is updated every time new
instance of rotation is observed. The true rotation matrix is randomly changing at regular interval
(N=200). The error in Frobenius norm is plotted against the instance index.

The online algorithm is also found robust to small amount of additive white Gaussian noise in
observations of the true rotated vectors  i.e. the observations are now given as yt = R∗xt + α wt 
where α determines the signal to noise ratio. The performance of the algorithm is studied with
various noisy conditions. Figure 2 shows error plots with respect to time for various noisy conditions
in R20. The Frobenius norm error decays quickly to a noise ﬂoor determined by the SNR as well as
the step size η. In the simulations in Fig. 2 the step size was decreased gradually over time. It is not
clear immediately how to pick the optimal step size but a classic step size adaptation rule or Armijo
rule may be followed [25  16].
The tracking performance of the online algorithm is compared with the batch version. In Figure
3  the unknown rotation R∗ ∈ SO(30) changes slightly after every 30 instances. The smoothly
changing rotation is induced by composing R∗ matrix with a matrix Rδ every thirty iterations. The
matrix Rδ is composed of 3 × 3 block-diagonal matrices  each corresponding to rotation about the
X-axis in 3D space by π/360 radians. The batch version stores the last 30 instances in an 30 × 30
matrix X and corresponding rotated vectors in matrix Y. The estimate of the unknown rotation is
given as YX−1. The batch version achieves zero error only at time instances when all the data in
X  Y correspond to the same rotation whereas the online version consistently achieves a low error
and tracks the changing rotation.
It is clear from the simulations that the Frobenius norm decreases at each iteration. It is easy to
show this global stability of the updates proposed here in noise-free scenario [24]. The proposed
algorithm was also applied to learning and tracking the rotations of 3D objects. Videos showing
experimental results with the 3D Stanford bunny [26] are posted online at [27].

7 Conclusion

In this paper  we have presented an online algorithm for learning rotations. The algorithm was
motivated using the von Neumann divergence and squared error loss function and the updates were

7

020040060080010001200012345Estimation error versus time − SO(12)Time (instance index)Estimation Error  Frobenius normSpectral normFigure 2: Average error plotted against instance index for various noise levels.

Figure 3: Comparing the performance of tracking rotations for the batch version versus the online
algorithm. The rotation matrix changes smoothly every M = 30 instances.

developed in the Lie algebra of the rotation group. The resulting matrix exponentiated gradient
updates were reduced to a simple quadratic form. The estimation performance of the proposed
algorithm was studied under various scenarios. Some of the future directions include identifying
alternative loss functions that exploit the spherical geometry as well as identifying regret bounds for
the proposed updates.
Acknowledgements: The author would like to thank W. A. Sethares  M. R. Gupta and A. B. Frigyik
for helpful discussions and feedback on early drafts of the paper.

References

[1] Rich Wareham  Jonathan Cameron  and Joan Lasenby  “Applications of conformal geometric

algebra in computer vision and graphics ” in IWMM/GIAE  2004  pp. 329–349.

[2] C. Doran  D. Hestenes  F. Sommen  and N. Van Acker  “Lie groups as spin groups ” Journal

of Mathematical Physics  vol. 34  no. 8  pp. 36423669  August 1993.

[3] Grace Wahba  “Problem 65-1  a least squares estimate of satellite attitude ” SIAM Review  vol.

7  no. 3  July 1965.

8

0100200300400500600012345Time (instance index)Estimation Error (Frobenius norm)Avg. Estimation error (Frobenius norm) vs time − SO(20)  05× 10−41x10−31.5x10−32x10−3100200300400500600012345Tracking rotations in SO(30)Time (instance index)Error (Frobenius norm)  Batch versionOnline algorithm[4] P. Schonemann  “A generalized solution of the orthogonal Procrustes problem ” Psychome-

trika  vol. 31  no. 1  pp. 3642–3669  March 1966.

[5] P. Besl and N. McKay  “A method for registration of 3D shapes ” . IEEE Trans. on Pattern

Analysis and Machine Intelligence  vol. 14  pp. 239–256  1992.

[6] Hannes Edvardson and ¨Orjan Smedby  “Compact and efﬁcient 3D shape description through
radial function approximation ” Computer Methods and Programs in Biomedicine  vol. 72  no.
2  pp. 89–97  2003.

[7] D.W. Eggert  A. Lorusso  and R.B. Fisher  “Estimating 3D rigid body transformations: a
comparison of four major algorithms ” Machine Vision and Applications  Springer  vol. 9  no.
5-6  Mar 1997.

[8] R. Sala Llonch  E. Kokiopoulou  I. Tosic  and P. Frossard  “3D face recognition with sparse

spherical representations ” Preprint  Elsiever  2009.

[9] Ameesh Makadia and Kostas Daniilidis  “Rotation recovery from spherical images without
correspondences ” IEEE Trans. Pattern Analysis Machine Intelligence  vol. 28  no. 7  pp.
1170–1175  2006.

[10] Raman Arora and Harish Parthasarathy  “Navigation using a spherical camera ” in Interna-

tional Conference on Pattern Recognition (ICPR)  Tampa  Florida  Dec 2008.

[11] Philip R. Evans  “Rotations and rotation matrices ” Acta Cryst.  vol. D57  pp. 1355–1359 

2001.

[12] Richard L. Liboff  Introductory Quantum Mechanics  Addison-Wesley  2002.
[13] Adam Smith and Manfred Warmuth  “Learning rotations ” in Conference on Learning Theory

(COLT)  Finland  Jun 2008.

[14] Koji Tsuda  Gunnar Ratsch  and Manfred K Warmuth  “Matrix exponentiated gradient updates
for on-line learning and Bregman projection ” Journal of Machine Learning Research  vol. 6 
Jun 2005.

[15] Manfred K Warmuth  “Winnowing subspaces ” in Proc. 24th Int. Conf. on Machine Learning 

2007.

[16] T.E. Abrudan  J. Eriksson  and V. Koivunen  “Steepest descent algorithms for optimization
under unitary matrix constraint ” Signal Processing  IEEE Transactions on  vol. 56  no. 3  pp.
1134–1147  March 2008.

[17] M. A. Nielsen and I. L. Chuang  Quantum Computation and Quantum Information  Cam-

bridge  2000.

[18] Kaare Brandt Petersen and Michael Syskind Pedersen  “The matrix cookbook ” http://

matrixcookbook.com  November 14  2008.

[19] Michael Artin  Algebra  Prentice Hall  1991.
[20] John A. Thorpe  Elementary topics in Differential Geometry  Springer-Verlag  1994.
[21] J. Kivinen andM. K.Warmuth  “Exponentiated gradient versus gradient descent for linear pre-

dictors ” Information and Computation  vol. 132  no. 1  pp. 1–63  Jan 1997.

[22] L. J. Butler  Applications of Matrix Theory to Approximation Theory  MS Thesis  Texas Tech

University  Aug. 1999.

[23] J. Gallier and D. Xu  “Computing exponentials of skew-symmetric matrices and logarithms of
orthogonal matrices ” International Journal of Robotics and Automation  vol. 17  no. 4  2002.
[24] Raman Arora  Group theoretical methods in signal processing: learning similarities  trans-
formation and invariants  Ph.D. thesis  University of Wisconsin-Madison  Madison  August
2009.

[25] E. Polak  Optimization: Algorithms and Consistent Approximations  Springer-Verlag  1997.
[26] Stanford University Computer Graphics Laboratory  “The Stanford 3D scanning repository ”

http://graphics.stanford.edu/data/.

[27] Raman Arora  “Tracking rotations of 3D Stanford bunny ” http://www.cae.wisc.edu/

˜sethares/links/raman/LearnROT/vids.html  2009.

9

,Peiqi Wang
Xinfeng Xie
Lei Deng
Guoqi Li
Dongsheng Wang
Yuan Xie