2010,Avoiding False Positive in Multi-Instance Learning,In multi-instance learning  there are two kinds of prediction failure  i.e.  false negative and false positive. Current research mainly focus on avoding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis  we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.,Avoiding False Positive in Multi-Instance Learning

Yanjun Han  Qing Tao  Jue Wang

Institute of Automation  Chinese Academy of Sciences

Beijing  100190  China

yanjun.han  qing.tao  jue.wang@ia.ac.cn

Abstract

In multi-instance learning  there are two kinds of prediction failure  i.e.  false
negative and false positive. Current research mainly focus on avoiding the for-
mer. We attempt to utilize the geometric distribution of instances inside positive
bags to avoid both the former and the latter. Based on kernel principal com-
ponent analysis  we deﬁne a projection constraint for each positive bag to clas-
sify its constituent instances far away from the separating hyperplane while place
positive instances and negative instances at opposite sides. We apply the Con-
strained Concave-Convex Procedure to solve the resulted problem. Empirical re-
sults demonstrate that our approach offers improved generalization performance.

1 Introduction

Multi-instance Learning (MIL) was ﬁrst proposed by Dietterich et.al. in [1] to predict the binding
ability of a drug from its biochemical structure. A certain drug molecule corresponds to a set of
conformations which cannot be differentiated via chemical experiments. A drug is labeled positive
if any of its constituent conformations has the binding ability greater than the threshold  otherwise
negative. Therefore  each sample (a drug) is a bag of instances (its constituent conformations). In
multi-instance learning the label information for positive samples is incomplete in that the instances
in a certain positive bag are all labeled positive. Generally  methods for multi-instance learning are
modiﬁed versions of approaches for supervised learning by shifting the focus from discrimination
on instances to discrimination on bags.
The earliest exploration were the APR algorithms proposed in [1]. From then on  a number of
approaches emerged. Examples include Diverse Density [2]  Citation k−NN [3]  MI-SVMs [4]  MI-
kernels [5]  reg-SVM [6]  MissSVM [7]  sbMIL  stMIL [8]  PPMM [9]  MIGraphs [10]  etc. Many
real-world applications can be regarded as Multi-instance learning problems. Examples include
image classiﬁcation [11]  document categorization [12]  computer aided diagnosis [13]  etc.
As far as positive bags are concerned  current research usually treat them as labyrinths in which
witnesses (responsible positive instances) are encaged  and consider nonwitnesses (other instances)
therein to be useless or even distractive. The information carried by nonwitnesses is not well utilized.
Factually  nonwitnesses are indispensable for characterizing the overall instance distribution  and
thus help to improve the learner. Several researchers realized the importance of nonwitnesses and
attempted to utilize them. In MI-kernels [5] and reg-SVM [6]  nonwitnesses together with witnesses
are squeezed into the kernel matrix. In mi-SVM [4]  the labels of all nonwitnesses are treated as
unknown integer variables to be optimized. mi-SVM tends to misclassify negative instances in
positive bags since the resulted margin will be larger. And we will elaborate on this ﬂaw in section
3.1. In MissSVM [7] and stMIL [8]  multi-instance learning is addressed from the view of semi-
supervised learning  and nonwitnesses are treated as unlabeled data  whose labels should be assigned
to maximize the margin. sbMIL [8] attempt to estimate the ratio of positive instances inside positive
bags and utilize this information in the subsequent classiﬁcation. MissSVM  sbMIL and stMIL
suffer from the same ﬂaw as mi-SVM.

1

Figure 1: Illustration of the False Positive Phenomenon: The top image is a positive training sample 
and the bottom image is a negative testing sample. The symbol ⊕ and ⊖ respectively denote positive
and negative instances. Enveloped points are instances in a positive bag. The Point not enveloped
is a negative bag of just one instance. Separating plane Fi corresponds to f (x) = i  and Gi corre-
sponds to g(x) = i. The learners f and g are obtained with and without the projection constraint 
respectively. Instances are labeled according to f. For details  please refer to the passage below.

The neglect of nonwitnesses in positive bags may lead to false positive and cause a model to misclas-
sify unseen negative samples. For example  in natural scene classiﬁcation  each image is segmented
to a bag of instances beforehand  and each instance is a patch (ROI  Regions Of Interest) charac-
terized by one feature vector describing its color. The task is to predict whether an image contains
a waterfall or not (Figure 1). A positive image contains some positive instances corresponding to
waterfall and some negative instances from other categories such as sky  stone  grass  etc.  while
a negative bag exclusively contains negative instances from other categories. Naturally  some neg-
ative instances (patches) only exist in positive bags. For instance  the end of a waterfall is often
surrounded by mist. The aforementioned approaches tend to misclassify negative instances in posi-
tive bags. Therefore  the patch corresponding to mist is misclassiﬁed as positive. Given an unseen
image with cirrus cloud and without waterfall  the obtained learner will misclassify this image as
positive because cirrus cloud and mist are similar to each other.
To avoid both false negative and false positive  we attempt to classify instances inside positive bags
far from the separating hyperplane and place positive and negative instances at opposite sides. We
achieve this by introducing projection constraints based on kernel principal component analysis into
MI-SVM [4]. Each constraint is deﬁned on a positive bag to encourage large variance of its con-
stituent instances along the normal direction of the separating hyperplane. We apply the Constrained
Concave-Convex Procedure (CCCP) to solve the resulted optimization problems.
The remainder of the paper is organized as follows: Section 2 introduces notation convention and
the CCCP. In Section 3 we bring out the projection constraint and the corresponding formulation
for multi-instance learning. In Section 4  the algorithm is evaluated on real world data sets. Finally 
conclusions are drawn in Section 5.

2 Preliminaries

2.1 Notation Convention
The origin of multi-instance learning [1] has been presented in section 1. Let X ⊆ Rp be the
space containing instances and D = {(Bi  yi)}m
i=1 be the training data  where Bi is the ith bag of
instances {xi1 ···   xini
} and yi ∈ Y is the label for Bi. Y is {+1 −1} for classiﬁcation and R
for regression. In addition  denote the index set for instances xij of Bi by Ii. The task is to train

2

a learner to predict the label of an unseen bag. Compared with traditional supervised learning  the
X to Y instead of from X to Y. Denote the index sets for positive and
learner is a mapping from 2
negative bags by I+ and I− respectively. Without loss of generality  assume that the instances are
}. We index instances by a function
ordered in the sequence {x11 ···   x1n1  ···   xm1 ···   xmnm
I(xij) =

nk + j. And I(Bi) returns a vector (

nk + 1 ···  

nk + ni).

i−1∑

i−1∑

i−1∑

k=1

k=1

k=1

2.2 Constrained Concave-Convex Procedure

Non-convex optimizations are undesirable because few algorithms effectively converge even to a
local optimum. However  if both objective function and constraints take the form of a difference be-
tween two convex functions  then a non-convex problem can be solved efﬁciently by the constrained
concave-convex procedure [14]. The fundamental is to eliminate the non-convexity by changing
non-convex parts to their ﬁrst-order Taylor expansions. The original problem is as follows:

f0(x) − g0(x)

min

x

s.t. fi(x) − gi(x) ≤ ci 

i = 1 ···   m

(1)
where fi  gi(i = 0 ···   m) are real-valued  convex and differentiable functions on Rn. Starting
from a random x(0)  (1) is approximated by a sequence of successive convex optimization problems.
At the t + 1th iteration  the non-convex parts in the objective and constraints are substituted by their
ﬁrst-order Taylor expansions  and the resulted optimization problem is as follows:

[
]
[
]
g0(x(t)) + ∇g0(x(t))T (x − x(t))
gi(x(t)) + ∇gi(x(t))T (x − x(t))

min

f0(x) −
s.t. fi(x) −

x

(2)

≤ ci

where x(t) is the optimal solution to (2) at the tth iteration. The above procedure is repeated until
convergence. In [14] it is proved that the CCCP converges to a local optimum of (1).

3 Multi-Instance Classiﬁcation

3.1 Support Vector Machine Formulation

Our work is based on the support vector machine (SVM) formulations for multi-instance learning 
to be exact  the MI-SVM [4] as follows:
∥w∥2 + C

(3)

ξi +

ξij

∑

]

[∑

min
w;b;(cid:24)

1
2

i∈I+

j∈Ii;i∈I(cid:0)

(wT xij + b) ≥ 1 − ξi  ξi ≥ 0  i ∈ I+

s.t. max
j∈Ii
− wT xij − b ≥ 1 − ξij  ξij ≥ 0  j ∈ Ii  i ∈ I−

Compared with the conventional SVM  in MI-SVM the notion of slack variables for positive samples
is extended from individual instances to bags while that for negative samples remains unchanged.
As shown by the ﬁrst set of max constraints  only the “most positive” instance in a positive bag  or
the witness  could affect the margin. And other instances  or nonwitnesses  become irrelevant for
determining the position of the separating plane once the witness is speciﬁed.
The max constraint at ﬁrst sight seems to well embody the characteristic of multi-instance learning.
Indeed  it helps to avoid the false negative  i e.  the misclassiﬁcation of positive samples. However 
it may incur false positive due to the following two reasons. Firstly  the max constraint aims at
discovering the witness  and tends to skip nonwitnesses. Thus each positive bag is approximately
oversimpliﬁed to a single pattern  i.e.  the witness. Most information in positive bags is wasted.
Secondly  due to the characteristic of the max function and the greediness of optimization methods 
the predictions of nonwitnesses are often adjusted above zero in the learning process. Besides  there
is no mechanism to draw the predictions of nonwitenesses below zero. Nevertheless  many nonwit-
nesses in positive bags are factually negative instances. For example  in natural scene classiﬁcation 

3

many image patches in a positive bag are from the irrelevant background; in document categoriza-
tion  many posts in a positive bag are not from the target category. Hence  many nonwitnesses are
mislabeled as positive  and we obtain a falsely large margin.
As shown in Figure 1  MI-SVM classiﬁes half instances in the training sample as positive  and some
negative instances are mislabeled. This false positive will impair the generalization performance.

3.2 Projection Constraint

The above problem is not unique for MI-SVM. Any approach without properly utilizing nonwit-
nesses has the same problem. In our preliminary work before this paper  we tried three solutions.
Firstly  we treat the labels of all nonwitnesses as unknown integer variables to be optimized. In the
SVM framework  it is exactly the mi-SVM [4] as follows:

∑

(4)

ξij

1
2

w;b;(cid:24)

∥w∥2 + C
∑

j∈Ii; i∈I+∪I(cid:0)

min{yij} min
s.t. yij(wT xij + b) ≥ 1 − ξij  ξij ≥ 0  j ∈ Ii  i ∈ I+
i ∈ I+
j∈Ii
− wT xij − b ≥ 1 − ξij  ξij ≥ 0  j ∈ Ii  i ∈ I−

≥ 1 

yij + 1

2

∑

It seems that assigning labels over all nonwitnesses should lead to a reasonable model. Nevertheless 
nonwitnesses are usually labeled positive since the consequent margin will be larger. Thus  many
of nonwitnesses are misclassiﬁed. As far as the example in Figure 1 is concerned  the obtained
learner is g(x) instead of f (x). MissSVM [7] takes an unsupervised approach. For every instance
in positive bags  two slack variables are introduced  measuring the distances from the instance to
the positive boundary f (x) = +1 and the negative boundary f (x) = −1 respectively  and the label
of the instance depends on the smaller slack variable. stMIL [8] takes a similar approach. As mi-
SVM  MissSVM and stMIL also suffers from misclassiﬁcation of nonwitnesses. sbMIL [8] tackles
multi-instance learning in two steps. The ﬁrst step is similar to MI-SVM  and the second step is a
traditional SVM. Still  there is no mechanism in sbMIL to avoid false positive.
In the second solution  we simultaneously seek for the “most positive” instance and the “most neg-
ative” instance in a positive bag by adding the following constraints to (3):
(wT xij + b) ≥ −1 − ζi  ζi ≥ 0  i ∈ I+

(5)

(−1) · min
j∈Ii

i∈I+

i∈I+

ξi in the objective of (3) is changed to

(ξi + ζi). Although misclas-
And the term
siﬁcation of nonwitnesses is alleviated since at least the “most negative” nonwitness is classiﬁed
correctly  the information carried by most nonwitnesses are not fully utilized. As far as the example
in Figure 1 is concerned  the obtained learner is still g(x) instead of f (x). Besides  this solution is
not appropriate for applications which involve positive bags only with positive instances.
The third solution is the projection constraint proposed in this paper. In a maximum margin frame-
work we want to classify instances in a positive bag far away from the separating hyperplane while
place positive instances and negative instances at opposite sides. From another point of view  in the
feature (kernel) space  we want to maximize the variance of instances in a positive bag along w  the
normal vector of the separating hyperplane. Therefore  the principal component analysis (PCA) [15]
is just the technique that we need. To tackle complicated real world datasets  we directly develop our
approach in the Reproducing Kernel Hilbert Space (RKHS). Let X be the space of instances  and H
be a RKHS of functions f : X → R with associated kernel function k(· ·). Note that f is both a
function on X and a vector in H. With an abuse of notation  we will not differentiate them unless
necessary. Denote the RKHS norm of H by ∥f∥H. Then MI-SVM can be rewritten as follows:

[∑

∑

]

∑

min
f∈H;(cid:24)

1
2

∥f∥2 + C

ξi +

i∈I+

j∈Ii;i∈I(cid:0)

ξij

(f (xij)) ≥ 1 − ξi  ξi ≥ 0  i ∈ I+

s.t. max
j∈Ii
− (f (xij)) ≥ 1 − ξij  ξij ≥ 0  j ∈ Ii  i ∈ I−

(6)

4

Figure 2: Illustration of the Effect of the Projection Constraint: Please note that the projection
constraint is effective for datasets with any geometric distribution once an appropriate kernel is
selected. Enveloped points are instances in a positive bag. Points not enveloped are negative bags of
just one instance. Separating plane Fi corresponds to f (x) = i  and Gi corresponds to g(x) = i.
The learner f and g are obtained with and without the projection constraint  respectively. Instances
are labeled according to f. ⊕ and ⊖ denote positive instances and negative instance respectively.

According to the representer theorem [16]  each minimizer f ∈ H of (6) has the following form:

αijϕ(xij)

(7)

∑

∑

f =

i∈I+∪I(cid:0)

j∈Ii

where all αi ∈ R  and ϕ(·) induced by k(· ·) is the feature mapping from X to H.
Next  we will propose our key contribution  i.e.  the projection constraint. Given a positive bag
Bi  denote its instances by {xij}ni
j=1  and denote the normal vector of the separating plane in the
RKHS by f. According to the theory of PCA [15  17]  maximizing the variance of mapped instances
{ϕ(xij)}ni
j=1 along f equals to minimizing the sum of the Euclidean distances from the centralized
ni∑
data points to their projections on the normalized vector

f∥f∥2

  as follows:
− (ϕ(xij) − ϕ(mi))∥2

2

(8)

Ji(f ) =

j=1

∥cj

f∥f∥2

ni∑

where ϕ(mi) = 1
ni

ϕ(xij)  the mean of {ϕ(xij)}ni
j=1.
projection point of ϕ(xij). After simple algebra  we get:

j=1

|cj| is the distance from ϕ(mi) to the

f T
∥f∥2
Substituting (9) and (7) into (8)  we arrive at:

cj =

(ϕ(xij) − ϕ(mi))

(9)

Ji((cid:11)) = oi − (cid:11)T L2
i (cid:11)
(cid:11)T K(cid:11)

(10)
where K is a n × n kernel matrix deﬁned on all the instances of both positive bags and negative
bags  oi = trace(KI(Bi)) − 1
1T KI(Bi)1 where KI(Bi) is a ni × ni matrix formed by extracting
the I(Bi) columns (Please refer to section 2.1) and the I(Bi) rows of the overall kernel matrix K 
and L2

i is the “centralized” L2

i as follows:

ni

i = LT
L2

i Li − 1nLT

i Li − LT

i Li1n + 1nLT

i Li1n

(11)

5

⊕⊕⊕⊖⊖⊖⊖⊖⊖⊖⊖⊖⊖⊖⊖⊕⊕⊖⊖⊖⊖⊕⊕⊖⊖⊖⊖⊖⊖⊖⊖⊖⊖⊖F+1F0F−1G−1G0G+1where 1n is a matrix with all elements equal to 1
I(Bi) rows of K and setting all the elements in other rows to 0:

{

Li(p  q) =

K(p  q)

0

n  and Li is a n × n matrix formed by keeping the
if p ∈ I(Bi) ∀q ∈ {1 ···   n}
otherwise

Generally  the optimal normal vector f varies for different positive bags. Hence it is meaningless to
solve (10) for its optimum. Instead  we average (10) by the bag size ni  and use a common threshold
λ to bound the averaged projection distance for different bags from above. We name the obtained
inequality “the projection constraint”  as follows:

(

)

1
ni

oi − (cid:11)T L2
i (cid:11)
(cid:11)T K(cid:11)

≤ λ

(12)

This is equivalent to bounding variance of instances in positive bags along f from below [15].
Substituting (7) into (6)  and adding the projection constraint (12) for each positive bag to the re-
sulted problem  we arrive at the following optimization problem:

[∑

∑

]

1
2

min
(cid:11);b;(cid:24)

ξij

ξi +

(cid:11)T K(cid:11) + C
s.t. 1 − ξi − max
j∈Ii

j∈Ii;i∈I(cid:0)

i∈I+
I(xij )(cid:11) + b) ≤ 0  ξi ≥ 0  i ∈ I+
(kT
I(xij )α + b ≤ −1 + ξij  ξij ≥ 0  j ∈ Ii  i ∈ I−
kT
(cid:11)T (oi · K − L2
i )(cid:11) − λni · (cid:11)T K(cid:11) ≤ 0  i ∈ I+

3.3 Optimization via the CCCP

In the problem (13)  the objective function and the second set of constraints are convex. The ﬁrst
set of constraints are all in the form of difference of two convex functions since the max function
is convex. According to the deﬁnition of Ji(f ) in (8)  J((cid:11)) in (10) is not less than 0 for any (cid:11).
Thus for any i ∈ I+  oi · K − L2
i is semi-deﬁnite positive. Consequently  the third set of constraints
are all in the form of difference of two convex functions. Therefore  we can apply the Constrained
Concave-Convex Procedure (CCCP) introduced in section 2.2 to solve the problem (13).
Since the function max in the ﬁrst set of constraints is nonsmooth  we have to change gradients to
subgradients to use the CCCP. The subgradient is usually not unique  and we adopt the deﬁnition
used in [6] for the subgradient of max
j∈Ii

I(xij )(cid:11):
kT

(13)

(14)

(15)

(16)

(17)

where

0

βij =

if kT
otherwise
where na is the number of xij that maximize kT
estimate for (cid:11) and βij by (cid:11)(t) and β(t)
ij
max
j∈Ii

I(xij )(cid:11) is as follows:
kT

1
na

According to (15)  we have ∑

∑

j∈Ii

∂(max
j∈Ii

kT
I(xij )(cid:11)) =

βijkT

I(xij )

{

I(xij )(cid:11) ̸= max
j∈Ii

kT
I(xij )(cid:11)

max
j∈Ii

kT
I(xij )(cid:11)(t) +

β(t)
ij kT

I(xij )((cid:11) − (cid:11)(t))

∑

j∈Ii

j∈Ii

β(t)
ij kT

I(xij )(cid:11)(t) = max
j∈Ii

(kT

I(xij )(cid:11)(t))

6

I(xij )(cid:11). At the tth iteration  denote the current
respectively. Then the ﬁrst order Taylor expansion of

Using (17)  (16) reduces to

∑

j∈Ii

β(t)
ij kT

I(xij )(cid:11)

(18)

Replacing max
j∈Ii
straints by their ﬁrst order Taylor expansions  ﬁnally we get:

I(xij )(cid:11) in the ﬁrst set of constraints by (18) and (cid:11)T Kα in the third set of con-
kT

min
(cid:11);b;(cid:24)

1
2

(cid:11)T K(cid:11) + C

ξi +

ξi

(19)

∑

]

i∈Ii;I∈I(cid:0)

[∑

i∈I+
β(t)
ij kT

∑

j∈Ii

s.t. 1 − ξi − (

I(xij )(cid:11) + b) ≤ 0  ξi ≥ 0  i ∈ I+

I(xij )(cid:11) + b ≤ −1 + ξij  ξij ≥ 0  j ∈ Ii  i ∈ I−
kT
K((cid:11) − (cid:11)(t)) ≤ 0  i ∈ I+
(cid:11)T Si(cid:11) − 2λni · (cid:11)(t)T

where Si = oi· K− L2
i . The problem (19) is a quadratically constrained quadratic program (QCQP)
with a convex objective function and convex constraints  and thus can be readily solved via interior
point methods [18]. Following the CCCP  we can do the iteration until (19) converges.

4 Experiments

4.1 Classiﬁcation: Benchmark

Benchmark data sets comes from two areas. Musk 1 and Musk 2 data sets [1] are two biochemical
tasks which directly promoted the research of multi-instance learning. The aim is to predict activ-
ity of drugs from structural information. Each drug molecule is a bag of potential conformations
(instances). The Musk 1 data set consists of 47 positive bags  45 negative bags  and totally 476
instances. The Musk 2 data set consists of 39 positive bags  63 negative bags  and totally 6598 in-
stances. Each instance is represented by a 166 dimensional vector. Elephant  tiger and fox are three
data sets from image categorization. The aim is to differentiate images with elephant  tiger  and fox
[4] from those without  respectively. A bag here is a group of ROIs (Region Of Interests) drawn
from a certain image. Each data set contains 100 positive bags and 100 negative bags  and each
ROI as an instance is a 230 dimensional vector. Related methods for comparison includes Diverse

Table 1: Test Accuracy(%) On Benchmark: Rows and columns correspond to methods and datasets
respectively.

miGraph

MIGraph

Fox
65:7

Algorithm Musk 1 Musk 2 Elep
Tiger
89:8
PC-SVM 90.6
83.8
±1:2 ±1:4 ±1.3
±2.7
81.9
85.1
90.0
±2.8 ±1.7 ±1.5
±3.8
86:0
86.8
88.9
±3.3
±0.7 ±2.8 ±1:0
84.3
88.0
84.2
±1.6 ±1.9 ±1.0
±3.1
MI-Kernel
MI-SVM 77.9
84.0
81.4
stMIL
79.5
81.6
74.7
sbMIL
83.0
88.6
91:8
N/A
N/A
DD
88.0
EM-DD
84.8
78.3
72.1

91:3
±3:2
90.0
±2.7
90.3
±2.6
89.3
±1.5
84.3
68.4
87.7
84.0
84.9

59.4
60.7
69.8
N/A
56.1

61.2

61.6

60.3

Density (DD [2])  EM-DD [19]  MI-SVM [4]  MI-Kernel [5]  stMIL [8]  sbMIL [8]  MIGraph and
miGraph [10]. When applied for multi-instance classiﬁcation  our approach involves three parame-
ters  namely  the bias/variance trade-off factor C  the kernel parameter (e.g.: γ in RBF kernel)  and
the bound parameter λ in the projection constraint. In the experiment  C  γ  and λ are selected from

7

{0.01 0.1 1 10 50 100}  {0.2 0.4 0.6 0.8 1.0} and {0.01 0.1 1 10 100} respectively. We employ the
MOSEK toolbox 1 to solve the resulted QCQP problem (19). The other experiment uses the same
parameter setting.
The ten-times 10-fold cross validation results (except Diverse Density) are shown in Table 1. The
results for other methods are replicated from their original papers. The results not available are
marked by N/A. The bolded ﬁgure indicates that result is better than all other methods. Table 1
shows that the performance of our approach (PC-SVM) is competitive. Recall that the difference
between our approach and MI-SVM is just the projection constraint. Therefore  as discussed in
section 3.2  the results in Table 1 demonstrates that the strength of nonwitnesses is well utilized via
the projection constraint.

4.2 Classiﬁcation: COREL Image Data Sets

Table 2: Test Accuracy(%) On COREL: Rows and columns correspond to methods and datasets
respectively.

Algorithm 1000-Image
PC-SVM 85:6 : [84:3; 86:9]
reg-SVM 84.4 : [83.0  85.8]
MIGraph
83.9 : [81.2  85.7]
miGraph
82.4 : [80.2  82.6]
MI-Kernel
81.8 : [80.1  83.6]
MI-SVM 74.7 : [74.1  75.3]
DD-SVM 81.5 : [78.5  84.5]

2000-Image
75:8 : [74:4; 77:2]
N/A
72.1 : [71.0  73.2]
70.5 : [68.7  72.3]
72.0 : [71.2  72.8]
54.6 : [53.1  56.1]
67.5 : [66.1  68.9]

COREL is a collection of natural scene images which have been categorized according to the pres-
ence of certain objects. Each image is regarded as a bag  and the nine dimensional ROIs (Region Of
Interests) in it are regarded as its constituent instances. In experiments  we use the 1000-Image data
set and the 2000-Image data set which contain ten and twenty categorizes  respectively. Following
the methodology in [10]  on both of the two data sets the related methods are compared by their ﬁve
times 2-fold cross validation results. The algorithm for comparison include Diverse Density (DD) 
MI-SVM  MIGraph  miGraph   MI-Kernel and reg-SVM. In the last four algorithms one-against-all
strategy is employed to tackle this multi-class task. In our approach this strategy is also used. Table
2 shows the overall accuracy as well as the 95% interval. As in benchmark data sets  our approach is
competitive with the latest methods. The results again suggest that fully utilizing the nonwitnesses
is important for multi-instance classiﬁcation.

5 Conclusion

We design a projection constraint to fully exploit nonwitnesses to avoid false positive. Since our
approach is basically MI-SVM with projection constraints  the improved results on real world data
sets validate the strength of nonwitnesses. We will introduce the universal projection constraint
into other existing approaches for multi-instance learning  and related learning tasks  such as multi-
instance regression  multi-label multi-instance learning  generalized multi-instance learning  etc.

Acknowledgments

We gratefully acknowledge reviewers for their insightful remarks and editors for their assiduous
work. We also deeply appreciate Kuijun Ma’s careful proof-reading. Finally  we are extremely
thankful to Runing Liu for the fascinating illustrations. This work was partially supported by Na-
tional Basic Research Program of China under Grant No.2004CB318103 and National Natural Sci-
ence Foundation of China under award No.60835002 and 60975040.

1http://www.mosek.com/

8

References

[1] T. G. Dietterich  R. H. Lathrop  and T. Lozano-P´erez. Solving the multiple-instance problem with axis-

parallel rectangles. Artiﬁcial Intelligence  89(1-2):31–71  1997.

[2] O. Maron and T. Lozano-P´erez. A framework for multiple-instance learning. Advances in neural infor-

mation processing systems  pages 570–576  1998.

[3] J. Wang and J.D. Zucker. Solving the multiple-instance problem: A lazy learning approach.

In Pro-
ceedings of the Seventeenth International Conference on Machine Learning  pages 1119–1126. Citeseer 
2000.

[4] S. Andrews  I. Tsochantaridis  and T. Hofmann. Support vector machines for multiple-instance learning.

Advances in neural information processing systems  pages 577–584  2003.

[5] T. G¨artner  P.A. Flach  A. Kowalczyk  and A.J. Smola. Multi-instance kernels. In Proceedings of the

Nineteenth International Conference on Machine Learning  pages 179–186. Citeseer  2002.

[6] P.M. Cheung and J.T. Kwok. A regularization framework for multiple-instance learning. In Proceedings

of the 23rd international conference on Machine learning  page 200. ACM  2006.

[7] Z.H. Zhou and J.M. Xu. On the relation between multi-instance learning and semi-supervised learning.

In Proceedings of the 24th international conference on Machine learning  page 1174. ACM  2007.

[8] R.C. Bunescu and R.J. Mooney. Multiple instance learning for sparse positive bags. In Proceedings of

the 24th international conference on Machine learning  page 112. ACM  2007.

[9] H.Y. Wang  Q. Yang  and H. Zha. Adaptive p-posterior mixture-model kernels for multiple instance
learning. In Proceedings of the 25th international conference on Machine learning  pages 1136–1143.
ACM  2008.

[10] Z. H. Zhou  Y. Y. Sun  and Yu. F. Li. Multi-instance learning by treating instances as non-I.I.D. samples. In
L´eon Bottou and Michael Littman  editors  Proceedings of the 26th International Conference on Machine
Learning  pages 1249–1256  Montreal  June 2009. test  Omnipress.

[11] Y. Chen and J.Z. Wang. Image categorization by learning and reasoning with regions. The Journal of

Machine Learning Research  5:913–939  2004.

[12] B. Settles  M. Craven  and S. Ray. Multiple-instance active learning. Advances in Neural Information

Processing Systems (NIPS)  20:1289–1296  2008.

[13] G. Fung  M. Dundar  B. Krishnapuram  and R.B. Rao. Multiple instance learning for computer aided

diagnosis. In NIPS2007  page 425. The MIT Press  2007.

[14] A.J. Smola  SVN Vishwanathan  and T. Hofmann. Kernel methods for missing variables. In Proceedings

of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics. Citeseer  2005.

[15] R.O. Duda  P.E. Hart  and D.G. Stork. Pattern classiﬁcation. John Wiley & Sons  2001.
[16] B. Sch¨olkopf and A.J. Smola. Learning with kernels. Citeseer  2002.
[17] Q. Tao  D.J. Chu  and J. Wang. Recursive support vector machines for dimensionality reduction. IEEE

Transactions on Neural Networks  19(1):189–193  2008.

[18] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Pr  2004.
[19] Q. Zhang and S.A. Goldman. Em-dd: An improved multiple-instance learning technique. Advances in

neural information processing systems  2:1073–1080  2002.

9

,Haipeng Luo
Robert Schapire