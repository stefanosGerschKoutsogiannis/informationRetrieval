2011,The Kernel Beta Process,A new Le ́vy process prior is proposed for an uncountable collection of covariate- dependent feature-learning measures; the model is called the kernel beta process (KBP). Available covariates are handled efficiently via the kernel construction  with covariates assumed observed with each data sample (“customer”)  and latent covariates learned for each feature (“dish”). Each customer selects dishes from an infinite buffet  in a manner analogous to the beta process  with the added constraint that a customer first decides probabilistically whether to “consider” a dish  based on the distance in covariate space between the customer and dish. If a customer does consider a particular dish  that dish is then selected probabilistically as in the beta process. The beta process is recovered as a limiting case of the KBP. An efficient Gibbs sampler is developed for computations  and state-of-the-art results are presented for image processing and music analysis tasks.,The Kernel Beta Process

Electrical & Computer Engineering Dept.

Electrical & Computer Engineering Dept.

Lu Ren∗

Duke University

Durham  NC 27708
lr22@duke.edu

David Dunson

Yingjian Wang∗

Duke University

Durham  NC 27708
yw65@duke.edu

Lawrence Carin

Duke University

Durham  NC 27708
lcarin@duke.edu

Department of Statistical Science

Electrical & Computer Engineering Dept.

Duke University

Durham  NC 27708

dunson@stat.duke.edu

Abstract

A new L´evy process prior is proposed for an uncountable collection of covariate-
dependent feature-learning measures; the model is called the kernel beta process
(KBP). Available covariates are handled efﬁciently via the kernel construction 
with covariates assumed observed with each data sample (“customer”)  and latent
covariates learned for each feature (“dish”). Each customer selects dishes from an
inﬁnite buffet  in a manner analogous to the beta process  with the added constraint
that a customer ﬁrst decides probabilistically whether to “consider” a dish  based
on the distance in covariate space between the customer and dish. If a customer
does consider a particular dish  that dish is then selected probabilistically as in
the beta process. The beta process is recovered as a limiting case of the KBP. An
efﬁcient Gibbs sampler is developed for computations  and state-of-the-art results
are presented for image processing and music analysis tasks.

1

Introduction

Feature learning is an important problem in statistics and machine learning  characterized by the goal
of (typically) inferring a low-dimensional set of features for representation of high-dimensional data.
It is desirable to perform such analysis in a nonparametric manner  such that the number of features
may be learned  rather than a priori set. A powerful tool for such learning is the Indian buffet
process (IBP) [4]  in which the data samples serve as “customers”  and the potential features serve
as “dishes”. It has recently been demonstrated that the IBP corresponds to a marginalization of a
beta-Bernoulli process [15]. The IBP and beta-Bernoulli constructions have found signiﬁcant utility
in factor analysis [7  17]  in which one wishes to infer the number of factors needed to represent
data of interest. The beta process was developed originally by Hjort [5] as a L´evy process prior for
“hazard measures”  and was recently extended for use in feature learning [15]  the interest of this
paper; we therefore here refer to it as a “feature-learning measure.”
The beta process is an example of a L´evy process [6]  another example of which is the gamma
process [1]; the normalized gamma process is well known as the Dirichlet process [3  14]. A key
characteristic of such models is that the data samples are assumed exchangeable  meaning that the
order/indices of the data may be permuted with no change in the model.

∗The ﬁrst two authors contributed equally to this work.

1

An important line of research concerns removal of the assumption of exchangeability  allowing
incorporation of covariates (e.g.  spatial/temporal coordinates that may be available with the data).
As an example  MacEachern introduced the dependent Dirichlet process [8]. In the context of feature
learning  the phylogenetic IBP removes the assumption of sample exchangeability by imposing
prior knowledge on inter-sample relationships via a tree structure [9]. The form of the tree may be
constituted as a result of covariates that are available with the samples  but the tree is not necessarily
unique. A dependent IBP (dIBP) model has been introduced recently  with a hierarchical Gaussian
process (GP) used to account for covariate dependence [16]; however  the use of a GP may constitute
challenges for large-scale problems. Recently a dependent hierarchical beta process (dHBP) has
been developed  yielding encouraging results [18]. However  the dHBP has the disadvantage of
assigning a kernel to each data sample  and therefore it scales unfavorably as the number of samples
increases.
In this paper we develop a new L´evy process prior  termed the kernel beta process (KBP)  which
yields an uncountable number of covariate-dependent feature-learning measures  with the beta pro-
cess a special case. This model may be interpreted as inferring covariates x∗
i for each feature (dish) 
indexed by i. The generative process by which the nth data sample  with covariates xn  selects
features may be viewed as a two-step process. First the nth customer (data sample) decides whether
ni ∼ Bernoulli(K(xn  x∗
to “examine” dish i by drawing z(1)
i are dish-dependent
kernel parameters that are also inferred (the {ψ∗
i } deﬁning the meaning of proximity/locality in co-
variate space). The kernels are designed to satisfy K(xn  x∗
i ) = 1 
and K(xn  x∗
ni = 1  customer n draws
ni ∼ Bernoulli(πi)  and if z(2)
z(2)
ni = 1  the feature associated with dish i is employed by data sample
n. The parameters {x∗
i   πi} are inferred by the model. After computing the posterior distribu-
tion on model parameters  the number of kernels required to represent the measures is deﬁned by
the number of features employed from the buffet (typically small relative to the data size); this is a
signiﬁcant computational savings relative to [18  16]  for which the complexity of the model is tied
to the number of data samples  even if a small number of features are ultimately employed.
In addition to introducing this new L´evy process  we examine its properties  and demonstrate how
it may be efﬁciently applied in important data analysis problems. The hierarchical construction of
the KBP is fully conjugate  admitting convenient Gibbs-sampling (complicated sampling methods
were required for the method in [18]). To demonstrate the utility of the model we consider image-
processing and music-analysis applications  for which state-of-the-art performance is demonstrated
compared to other relevant methods.

i ))  where ψ∗
i ) ∈ (0  1]  K(x∗

i (cid:107)2 → ∞. In the second step  if z(1)

i ) → 0 as (cid:107)xn − x∗

i ; ψ∗
i ; ψ∗

i ; ψ∗

i   ψ∗

i   x∗

i ; ψ∗

2 Kernel Beta Process

2.1 Review of beta and Bernoulli processes
A beta process B ∼ BP(c  B0) is a distribution on positive random measures over the space (Ω F).
Parameter c(ω) is a positive function over ω ∈ Ω  and B0 is the base measure deﬁned over Ω. The
beta process is an example of a L´evy process  and the L´evy measure of BP(c  B0) is

(1)
To draw B  one draws a set of points (ωi  πi) ∈ Ω × [0  1] from a Poisson process with measure ν 
yielding

ν(dπ  dω) = c(ω)π−1(1 − π)c(ω)−1dπB0(dω)

B =

πiδωi

(2)

(cid:82)

λ =(cid:82)

where δωi
is a unit point measure at ωi; B is therefore a discrete measure  with probabil-
ity one. The inﬁnite sum in (2) is a consequence of drawing Poisson(λ) atoms {ωi  πi}  with

[0 1] ν(dω  dπ) = ∞. Additionally  for any set A ⊂ F  B(A) =(cid:80)

i: ωi∈A πi.
If Zn ∼ BeP(B) is the nth draw from a Bernoulli process  with B deﬁned as in (2)  then

Ω

∞(cid:88)

i=1

∞(cid:88)

i=1

2

Zn =

bniδωi  

bni ∼ Bernoulli(πi)

(3)

A set of N such draws  {Zn}n=1 N   may be used to deﬁne whether feature ωi ∈ Ω is utilized to
represent the nth data sample  where bni = 1 if feature ωi is employed  and bni = 0 otherwise. One
may marginalize out the measure B analytically  yielding conditional probabilities for the {Zn} that
correspond to the Indian buffet process [15  4].

2.2 Covariate-dependent L´evy process
In the above beta-Bernoulli construction  the same measure B ∼ BP(c  B0) is employed for gen-
eration of all {Zn}  implying that each of the N samples have the same probabilities {πi} for use
of the respective features {ωi}. We now assume that with each of the N samples of interest there
are an associated set of covariates  denoted respectively as {xn}  with each xn ∈ X . We wish to
impose that if samples n and n(cid:48) have similar covariates xn and xn(cid:48)  that it is probable that they will
employ a similar subset of the features {ωi}; if the covariates are distinct it is less probable that
feature sharing will be manifested.
Generalizing (2)  consider

∞(cid:88)
speciﬁc to covariate x ∈ X being Bx = (cid:80)∞

(4)
where γi = {γi(x) : x ∈ X} is a stochastic process (random function) from X → [0  1] (drawn in-
dependently from the {ωi}). Hence  B is a dependent collection of L´evy processes with the measure
i=1 γi(x)δωi. This constitutes a general speciﬁcation 
with several interesting special cases. For example  one might consider γi(x) = g{µi(x)}  where
g : R → [0  1] is any monotone differentiable link function and µi(x) : X → R may be modeled
as a Gaussian process [10]  or related kernel-based construction. To choose g{µi(x)} one can po-
tentially use models for the predictor-dependent breaks in probit  logistic or kernel stick-breaking
processes [13  11  2]. In the remainder of this paper we propose a special case for design of γi(x) 
termed the kernel beta process (KBP).

γiδωi   ωi ∼ B0

B =

i=1

2.3 Characteristic function of the kernel beta process
Recall from Hjort [5] that B ∼ BP(c(ω)  B0) is a beta process on measure space (Ω F) if its
characteristic function satisﬁes

(cid:90)

[0 1]×A

E[ejuB(A)] = exp{

(ejuπ − 1)ν(dπ  dω)}

(5)

√−1  and A is any subset in F. The beta process is a particular class of the L´evy
where here j =
process  with ν(dπ  dω) deﬁned as in (1).
For kernel K(x  x∗; ψ∗)  let x ∈ X   x∗ ∈ X   and ψ∗ ∈ Ψ; it is assumed that K(x  x∗; ψ∗) ∈ [0  1]
for all x  x∗ and ψ∗. As a speciﬁc example  for the radial basis function K(x  x∗; ψ∗) =
exp[−ψ∗(cid:107)x − x∗(cid:107)2]  where ψ∗ ∈ R+. Let x∗ represent random variables drawn from proba-
bility measure H  with support on X   and ψ∗ is also a random variable drawn from an appropriate
probability measure Q with support over Ψ (e.g.  in the context of the radial basis function  ψ∗ are
drawn from a probability measure with support over R+). We now deﬁne a new L´evy measure

νX = H(dx∗)Q(dψ∗)ν(dπ  dω)

(6)

where ν(dπ  dω) is the L´evy measure associated with the beta process  deﬁned in (1).
Theorem 1 Assume parameters {x∗
following measure is constituted

i   πi  ωi} are drawn from measure νX in (6)  and that the
∞(cid:88)

i )δωi
which may be evaluated for any covariate x ∈ X .
For any ﬁnite set of co-
variates S = {x1  . . .   x|S|}  we deﬁne the |S|-dimensional
random vector K =
(K(x1  x∗; ψ∗)  . . .   K(x|S|  x∗; ψ∗))T   with random variables x∗ and ψ∗ drawn from H and
the B evaluated at covariates S  on the set A 
Q  respectively.

For any set A ⊂ F 

πiK(x  x∗

Bx =

i ; ψ∗

(7)

i   ψ∗

i=1

3

(cid:80)
yields an |S|-dimensional random vector B(A) = (Bx1 (A)  . . .  Bx|S| (A))T   where Bx(A) =
i: ωi∈A πiK(x  x∗
i ). Expression (7) is a covariate-dependent L´evy process with L´evy mea-
sure (6)  and characteristic function for an arbitrary set of covariates S satisfying

i ; ψ∗

E[ej<u B(A)>] = exp{

(ej<u Kπ> − 1)νX (dx∗  dψ∗  dπ  dω)}

(8)

X×Ψ×[0 1]×A

2

A proof is provided in the Supplemental Material. Additionally  for notational convenience  below a
draw of (7)  valid for all covariates in X   is denoted B ∼ KBP(c  B0  H  Q)  with c and B0 deﬁning
ν(dπ  dω) in (1).

(cid:90)

i ; ψ∗

xi z(2)

xi   with z(1)

Zx =(cid:80)∞

i=1 bxiδωi  with bxi ∼ Bernoulli(πiK(x  x∗

2.4 Relationship to the beta-Bernoulli process
If the covariate-dependent measure Bx in (7) is employed to deﬁne covariate-dependent feature us-
i   πi}  the feature-usage measure is
age  then Zx ∼ BeP(Bx)  generalizing (3). Hence  given {x∗
i )). Note that it is equivalent in distribution
xi ∼ Bernoulli(K(x  x∗
xi ∼ Bernoulli(πi). This
to express bxi = z(1)
model therefore yields the two-step generalization of the generative process of the beta-Bernoulli
process discussed in the Introduction. The condition z(1)
xi = 1 only has a high probability when
observed covariates x are near the (latent/inferred) covariates x∗
i . It is deemed attractive that this
intuitive generative process comes as a result of a rigorous L´evy process construction  the properties
of which are summarized next.
2.5 Properties of B
For all Borel subsets A ∈ F  if B is drawn from the KBP and for covariates x  x(cid:48) ∈ X   we have

i   ψ∗
i ; ψ∗

i )) and z(2)

(cid:90)

E[Bx(A)] = B0(A)E(Kx)

(cid:90)

B0(dω)(1 − B0(dω))

Cov(Bx(A) Bx(cid:48)(A)) = E(KxKx(cid:48))

where  E(Kx) = (cid:82)

X×Ψ K(x  x∗; ψ∗)H(dx∗)Q(dψ∗).

0 (dω)
If K(x  x∗; ψ∗) = 1 for all x ∈ X  
E(Kx) = E(KxKx(cid:48)) = 1  and Cov(Kx  Kx(cid:48)) = 0  and the above results reduce to the those
for the original BP [15].
Assume c(ω) = c  where c ∈ R+ is a constant  and let Kx = (K(x  x∗
1; ψ∗
represent an inﬁnite-dimensional vector  then for ﬁxed kernel parameters {x∗

1)  K(x  x∗
i } 
i   ψ∗

− Cov(Kx  Kx(cid:48))

2)  . . . )T

2; ψ∗

c(ω) + 1

A

B2

A

Corr(Bx(A) Bx(cid:48)(A)) =

(9)
where it is assumed < Kx  Kx(cid:48) >  (cid:107)Kx(cid:107)2  (cid:107)Kx(cid:48)(cid:107)2 are ﬁnite; the latter condition is always met
when we (in practice) truncate the number of terms used in (7). The expression in (9) clearly imposes
the desired property of high correlation in Bx and Bx(cid:48) when x and x(cid:48) are proximate.
Proofs of the above properties are provided in the Supplemental Material.

< Kx  Kx(cid:48) >
(cid:107)Kx(cid:107)2 · (cid:107)Kx(cid:48)(cid:107)2

3 Applications

3.1 Model construction

We develop a covariate-dependent factor model  generalizing [7  17]  which did not consider covari-
ates. Consider data yn ∈ RM with associated covariates xn ∈ RL  with n = 1  . . .   N. The factor
loadings in the factor model here play the role of “dishes” in the buffet analogy  and we model the
data as

Zxn ∼ BeP(Bxn) 

yn = D(wn ◦ bn) + n
B ∼ KBP(c  B0  H  Q) 

wn ∼ N (0  α−1

1 IT ) 

n ∼ N (0  α−1

B0 ∼ DP(α0G0)
2 IM )

(10)

4

with gamma priors placed on α0  α1 and α2  with ◦ representing the pointwise (Hadamard) vector
product  and with IM representing the M × M identity matrix. The Dirichlet process [3] base
measure G0 = N (0  1
M IM )  and the KBP base measure B0 is a mixture of atoms (factor loadings).
For the applications considered it is important that the same atoms be reused at different points {x∗
i }
in covariate space  to allow for repeated structure to be manifested as a function of space or time 
within the image and music applications  respectively. The columns of D are deﬁned respectively
by (ω1  ω2  . . . ) in B  and the vector bn = (bn1  bn2  . . . ) with bnk = Zxn (ωk). Note that B is
drawn once from the KBP  and when drawing the Zxn we evaluate B as deﬁned by the respective
covariate xn.
When implementing the KBP  we truncate the sum in (7) to T terms  and draw the πi ∼
Beta(1/T  1)  which corresponds to setting c = 1. We set T large  and the model infers the subset
of {πi}i=1 T that have signiﬁcant amplitude  thereby estimating the number of factors needed for
representation of the data. In practice we let H and Q be multinomial distributions over a discrete
and ﬁnite set of  respectively  locations for {x∗
i }  details of which
are discussed in the speciﬁc examples.
In (10)  the ith column of D  denoted Di  is drawn from B0  with B0 drawn from a Dirichlet
process (DP). There are multiple ways to perform such DP clustering  and here we apply the P´olya
urn scheme [3]. Assume D1  D2  . . .   Di−1 are a series of i.i.d. random draws from B0  then the
successive conditional distribution of Di is of the following form:

i } and kernel parameters for {ψ∗

Di|D1  . . .   Di−1  α0  G0 ∼ Nu(cid:88)

n∗

l

i − 1 + α0

l =(cid:80)i−1

(11)
l }l=1 Nu are the unique dictionary elements shared by the ﬁrst i − 1 columns of D  and
j=1 δ(Dj = D∗
l ). For model inference  an indicator variable ci is introduced for each Di 
l   with l = 1  . . .   Nu  with ci equal to Nu + 1 with
l ; otherwise Di is
Nu+1 is hence introduced.

where {D∗
n∗
and ci = l with a probability proportional to n∗
a probability controlled by α0. If ci = l for l = 1  . . .   Nu  Di takes the value D∗
drawn from the prior G0 = N (0  1

M IM )  and a new dish/factor loading D∗

i − 1 + α0

δD∗

G0 

l=1

+

α0

l

3.2 Extensions

It is relatively straightforward to include additional model sophistication into (10)  one example
of which we will consider in the context of the image-processing example. Speciﬁcally  in many
applications it is inappropriate to assume a Gaussian model for the noise or residual n. In Section
4.3 we consider the following augmented noise model:

n = λn ◦ mn + ˆn

(12)

λn ∼ N (0  α−1

λ IM )  mnp ∼ Bernoulli(˜πn)  ˜πn ∼ Beta(a0  b0)  ˆn ∼ N (0  α−1

with gamma priors placed on αλ and α2  and with p = 1  . . .   M. The term λn ◦ mn accounts for
“spiky” noise  with potentially large amplitude  and ˆπn represents the probability of spiky noise in
data sample n. This type of noise model was considered in [18]  with which we compare.

3 IM )

3.3 Inference

The model inference is performed with a Gibbs sampler. Due to the limited space  only those
variables having update equations distinct from those in the BP-FA of [17] are included here.
Assume T is the truncation level for the number of dictionary elements  {Di}i=1 T ; Nu is the
number of unique dictionary elements values in the current Gibbs iteration  {D∗
l }l=1 Nu. For the
applications considered in this paper  K(xn  x∗
i ) is deﬁned based on the Euclidean distance:
K(xn  x∗
i are updated from multi-
nomial distributions (deﬁning Q and H  respectively) over a set of discretized values with a uniform
prior for each; more details on this are discussed in Sec. 4.

i ||2] for i = 1  . . .   T ; both ψ∗

i ) = exp[−ψ∗

i ||xn − x∗

i and x∗

i ; ψ∗

i ; ψ∗

• Update {D∗

l ∼ N (µl  Σl) 
l }l=1 L: D∗
N(cid:88)
(cid:88)
(bniwni)y−l

µl = Σl[α2

n ]  Σl = [α2

n=1

i:ci=l

5

N(cid:88)

(cid:88)

n=1

i:ci=l

(bniwni)2 + M ]−1IM  

n = yn −(cid:80)
(cid:40) n∗

where y−l

i:ci(cid:54)=l Di(bniwni).

−i

p(ci = l|−) ∝

• Update {ci}i=1 T : p(ci) ∼ Mult(pi) 

(cid:81)N
(cid:81)N
n=1 exp{− α2
T−1+α0
n=1 exp{− α2
T−1+α0
where n∗
j:j(cid:54)=i δ(Dj = D∗
l )  and y−i
by normalizing the above equation.

−i =(cid:80)

α0

l

l

2 (cid:107)y−i
2 (cid:107)y−i

n − D∗
n − D∗

l (bniwni)(cid:107)2
2} 
if l is previously used 
lnew (bniwni)(cid:107)2
2} 
k:k(cid:54)=i Dk(bnkwnk); pi is realized
for Zxn  update each component p(bni) ∼ Bernoulli(vni) for

n = yn −(cid:80)

if l = lnew 

• Update {Zxn}n=1 N :

i = 1  . . .   K 

(cid:2)DT

(cid:3)}πiK(xn  x∗

i ; ψ∗
i )

.

exp{− α2

2

p(bni = 1)
p(bni = 0)

=

i Diw2

ni − 2wniDT
1 − πiK(xn  x∗

i y−i
i ; ψ∗
i )

n

vni is calculated by normalizing p(bni) with the above constraint.

• Update {πi}i=1 T :

ni }i=1 T for each data yn.
ni }i=1 T and {z(2)
i ; ψ∗
i )). For each speciﬁc n 

ni = 1 and z(2)

– If bni = 1  z(1)

Introduce two sets of auxiliary variables {z(1)
Assume z(1)

ni ∼ Bernoulli(K(xn  x∗

ni ∼ Bernoulli(πi) and z(2)
ni = 1;
i ;ψ∗
ni = 0|bni = 0) =
1−πiK(xn x∗
i ;ψ∗
i )
ni = 1|bni = 0) = (1−πi)K(xn x∗
i ;ψ∗
i )
1−πiK(xn x∗
i ;ψ∗
i )
i ;ψ∗
ni = 0|bni = 0) =
πi
1−πiK(xn x∗
i ;ψ∗
i )
From the above equations  we derive the conditional distribution for πi 

(1−πi)(cid:0)1−K(xn x∗
(cid:0)1−K(xn x∗
i )(cid:1)
(cid:88)
ni )(cid:1).

ni = 0  z(2)
ni = 0  z(2)
ni = 1  z(2)

πi ∼ Beta(cid:0) 1



– If bni = 0 

p(z(1)
p(z(1)

(1 − z(1)

(cid:88)

z(1)
ni   1 +

p(z(1)

i )(cid:1)

+

T

n

n

4 Results

4.1 Hyperparameter settings
For both α1 and α2 the corresponding prior was set to Gamma(10−6  10−6); the concentration pa-
rameter α0 was given a prior Gamma(1  0.1). For both experiments below  the number of dictionary
elements T was truncated to 256  the number of unique dictionary element values was initialized
to 100  and {πi}i=1 T were initialized to 0.5. All {ψ∗
i }i=1 T were initialized to 10−5 and updated
from a set {10−5  10−4  10−3  10−2  10−1  1} with a uniform prior Q. The remaining variables were
initialized randomly. No parameter tuning or optimization has been performed.

4.2 Music analysis

We consider the same music piece as described in [12]: “A Day in the Life” from the Beatles’ album
Sgt. Pepper’s Lonely Hearts Club Band. The acoustic signal was sampled at 22.05 KHz and divided
into 50 ms contiguous frames; 40-dimensional Mel frequency cepstral coefﬁcients (MFCCs) were
extracted from each frame  shown in Figure 1(a).
A typical goal of music analysis is to infer interrelationships within the music piece  as a function
of time [12]. For the audio data  each MFCC vector yn has an associated time index  the latter used
as the covariate xn. The ﬁnite set of temporal sample points (covariates) were employed to deﬁne a
library for the {x∗
i }  and H is a uniform distribution over this set. After 2000 burn-in iterations  we
collected samples every ﬁve iterations. Figure 1(b) shows the frequency for the number of unique
dictionary elements used by the data  based on the 1600 collected samples; and Figure 1(c) shows
the frequency for the number of total dictionary elements used.
With the model deﬁned in (10)  the sparse vector bn◦wn indicates the importance of each dictionary
element from {Di}i=1 T to data yn. Each of these N vectors {bn ◦ wn}n=1 N was normalized

6

(a)

(b)

(c)

Figure 1: (a) MFCCs features used in music analysis  where the horizontal axis corresponds to time  for
“A Day in the Life”. Based on the Gibbs collection samples: (b) frequency on number of unique dictionary
elements  and (c) total number of dictionary elements.
within each Gibbs sample  and used to compute a correlation matrix associated with the N time
points in the music. Finally  this matrix was averaged across the collection samples  to yield a
correlation matrix relating one part of the music to all others. For a fair comparison between our
methods and the model proposed in [12] (which used an HMM  and computed correlations over
windows of time)  we divided the whole piece into multiple consecutive short-time windows. Each
temporal window includes 75 consecutive feature vectors  and we compute the average correlation
coefﬁcients between the features within each pair of windows. There were 88 temporal windows
in total (each temporal window is de noted as a sequence in Figure 2)  and the dimension of the
correlation matrix is accordingly 88 × 88. The computed correlation matrix for the proposed KBP
model is presented in Figure 2(a).
We compared KBP performance with results based on BP-FA [17] in which covariates are not em-
ployed  and with results from the dynamic clustering model in [12]  in which a dynamic HMM is
employed (in [12] a dynamic HDP  or dHDP  was used in concert with an HMM). The BP-FA results
correspond to replacing the KBP with a BP. The correlation matrix computed from the BP-FA and
the dHDP-HMM [12] are shown in Figures 2(b) and (c)  respectively. The dHDP-HMM results yield
a reasonably good segmentation of the music  but it is unable to infer subtle differences in the music
over time (for example  all voices in the music are clustered together  even if they are different).
Since the BP-FA does not capture as much localized information in the music (the probability of
dictionary usage is the same for all temporal positions)  it does not manifest as good a music seg-
mentation as the dHDP-HMM. By contrast  the KBP-FA model yields a good music segmentation 
while also capturing subtle differences in the music over time (e.g.  in voices). Note that the use of
the DP to allow repeated use of dictionary elements as a function of time (covariates) is important
here  due to the repetition of structure in the piece. One may listen to the music and observe the
segmentation at http://www.youtube.com/watch?v=35YhHEbIlEI.

(a)

(b)

(c)

Figure 2: Inference of relationships in music as a function of time  as computed via a correlation of the
dictionary-usage weights  for (a) and (b)  and based upon state usage in an HMM  for (c). Results are shown
for “A Day in the Life.” The results in (c) are from [12]  as a courtesy from the authors of that paper. (a)
KBP-FA  (b) BP-FA  (c) dHDP-HMM .

4.3

Image interpolation and denoising

We consider image interpolation and denoising as two additional potential applications. In both of
these examples each image is divided into N 8 × 8 overlapping patches  and each patch is stacked
into a vector of length M = 64  constituting observation yn ∈ RM . The covariate xn represents the

7

observation indexfeature values  100020003000400050006000510152025303540−6−4−2024253035404550550100200300400500600The number of unique dictionary elementsFrequency calculated from the collected samples165170175180185190195200205050100150200250300The number of dictionary elements taken by the dataFrequency calculated from the collected samplesSequence indexSequence index  10203040506070801020304050607080−0.100.10.20.30.40.50.60.70.80.9Sequence indexSequence index  102030405060708010203040506070800.80.850.90.951sequence indexsequence index102030405060708010203040506070800.10.20.30.40.50.60.70.80.9patch coordinates in the 2-D space. The probability measure H corresponds to a uniform distribution
over the centers of all 8 × 8 patches. The images were recovered based on the average of the
collection samples  and each pixel was averaged across all overlapping patches in which it resided.
For the image-processing examples  5000 Gibbs samples were run  with the ﬁrst 2000 discarded as
burn-in.
For image interpolation  we only observe a fraction of the image pixels  sampled uniformly at ran-
dom. The model infers the underlying dictionary D in the presence of this missing data  as well as
the weights on the dictionary elements required for representing the observed components of {yn};
using the inferred dictionary and associated weights  one may readily impute the missing pixel val-
ues. In Table 1 we present average PSNR values on the recovered pixel values  as a function of
the fraction of pixels that are observed (20% in Table 1 means that 80% of the pixels are missing
uniformly at random). Comparisons are made between a model based on BP and one based on the
proposed KBP; the latter generally performs better  particularly when a large fraction of the pixels
are missing. The proposed algorithm yields results that are comparable to those in [18]  which also
employed covariates within the BP construc tion. However  the proposed KBP construction has
the signiﬁcant computational advantages of only requiring kernels centered at the locations of the
i }  while the model in [18] has a kernel for each of the image
dictionary-dependent covariates {x∗
patches  and therefore it scales unfavorably for large images.
Table 1: Comparison of BP and KBP for interpolating images with pixels missing uniformly at random 
using standard image-processing images. The top and bottom rows of each cell show results of BP and KBP 
respectively. Results are shown when 20%  30% and 50% of the pixels are observed  selected uniformly at
random.
RATIO

BARBARA

20%

30%

50%

C.MAN
23.75
24.02
25.59
25.75
28.66
28.78

HOUSE
29.75
30.89
33.09
34.02
38.26
38.35

PEPPERS

25.56
26.29
28.64
29.29
32.53
32.69

LENA
30.97
31.38
33.30
33.33
36.79
35.89

26.84
28.93
30.13
31.46
35.95
36.03

BOATS
27.84
28.11
30.20
30.24
33.05
33.18

F.PRINT
26.49
26.89
29.23
29.37
33.50
32.18

MAN
28.29
28.37
29.89
30.12
33.19
32.35

COUPLE
27.76
28.03
29.97
30.33
33.61
32.35

HILL
29.38
29.67
31.19
31.25
34.19
32.60

In the image-denoising example in Figure 3 the images were corrupted with both white Gaussian
noise (WGN) and sparse spiky noise  as considered in [18]. The sparse spiky noise exists in partic-
ular pixels  selected uniformly at random  with amplitude distributed uniformly between −255 and
255. For the pepper image  15% of the pixels were corrupted by spiky noise  and the standard devi-
ation of the WGN was 15; for the house image  10% of the pixels were corrupted by spiky noise and
the standard deviation of WGN was 10. We compared with different methods on both two images:
the augmented KBP-FA model (KBP-FA+) in Sec. 3.2  the BP-FA model augmented with a term for
spiky noise (BP-FA+) and the original BP-FA model. The model proposed with KBP showed the
best denoising result for both visual and quantitative evaluations. Again  these results are compara-
ble to those in [18]  with the signiﬁcant computational advant age discussed above. Note that here
the imposition of covariates and the KBP yields marked improvements in this application  relative
to BP-FA alone.

Figure 3: Denoising Result: the ﬁrst column shows the noisy images (PSNR is 15.56 dB for Peppers and
17.54 dB for House); the second and third column shows the results inferred from the BP-FA model (PSNR
is 16.31 dB for Peppers and 17.95 dB for House)  with the dictionary elements shown in column two and the
reconstruction in column three; the fourth and ﬁfth columns show results from BP-FA+ (PSNR is 23.06 dB
for Peppers and 26.71 dB for House); the sixth and seventh column shows the results of the KBP-FA+ (PSNR
is 27.37 dB for Peppers and 34.89 dB for House). In each case the dictionaries are ordered based on their
frequency of usage  starting from top-left.

8

5 Summary

A new L´evy process  the kernel beta process  has been developed for the problem of nonparametric
Bayesian feature learning  with example results presented for music analysis  image denoising  and
image interpolation. In addition to presenting theoretical properties of the model  state-of-the-art
results are realized on these learning tasks. The inference is performed via a Gibbs sampler  with
analytic update equations. Concerning computational costs  for the music-analysis problem  for
example  the BP model required around 1 second per Gibbs iteration  with KBP requiring about 3
seconds  with results run on a PC with 2.4GHz CPU  in non-optimized MatlabTM.

Acknowledgment

The research reported here was supported by AFOSR  ARO  DARPA  DOE  NGA and ONR.

References
[1] D. Applebaum. Levy Processes and Stochastic Calculus. Cambridge University Press  2009.
[2] D. B. Dunson and J.-H. Park. Kernel stick-breaking processes. Biometrika  95:307–323  2008.
[3] T. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics  1973.
[4] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In NIPS 

2005.

[5] N. L. Hjort. Nonparametric Bayes estimators based on beta processes in models for life history data.

Annals of Statistics  1990.

[6] J.F.C. Kingman. Poisson Processes. Oxford Press  2002.
[7] D. Knowles and Z. Ghahramani.

analysis. In Independent Component Analysis and Signal Separation  2007.

Inﬁnite sparse factor analysis and inﬁnite independent components

[8] S. N. MacEachern. Dependent Nonparametric Processes. In In Proceedings of the Section on Bayesian

Statistical Science  1999.

[9] K. Miller  T. Grifﬁths  and M. I. Jordan. The phylogenetic Indian buffet process: A non-exchangeable

nonparametric prior for latent features. In UAI  2008.

[10] C.E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press  2006.
[11] L. Ren  L. Du  L. Carin  and D. B. Dunson. Logistic stick-breaking process. J. Machine Learning

Research  2011.

[12] L. Ren  D. Dunson  S. Lindroth  and L. Carin. Dynamic nonparametric bayesian models for analysis of

music. Journal of The American Statistical Association  105:458–472  2010.

[13] A. Rodriguez and D. B. Dunson. Nonparametric bayesian models through probit stickbreaking processes.

Univ. California Santa Cruz Technical Report  2009.

[14] J. Sethuraman. A constructive deﬁnition of dirichlet priors. 1994.
[15] R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. In AISTATS 

2007.

[16] S. Williamson  P. Orbanz  and Z. Ghahramani. Dependent Indian buffet processes. In AISTATS  2010.
[17] M. Zhou  H. Chen  J. Paisley  L. Ren  G. Sapiro  and L. Carin. Non-parametric Bayesian dictionary

learning for sparse image representations. In NIPS  2009.

[18] M. Zhou  H. Yang  G. Sapiro  D. Dunson  and L. Carin. Dependent hierarchical beta process for image

interpolation and denoising. In AISTATS  2011.

9

,Akihiro Kishimoto
Radu Marinescu
Adi Botea