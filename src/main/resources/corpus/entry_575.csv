2013,Approximate inference in latent Gaussian-Markov models from continuous time observations,We propose an approximate inference algorithm for continuous time Gaussian-Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for the discrete time terms and (2) variational updates for the continuous time term. We introduce corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations  enabling continuous-time inference in a variety of models  including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.,Approximate inference in latent Gaussian-Markov

models from continuous time observations

Botond Cseke1

Manfred Opper2

Guido Sanguinetti1

1School of Informatics

University of Edinburgh  U.K.

{bcseke gsanguin}@inf.ed.ac.uk

2Computer Science
TU Berlin  Germany

manfred.opper@tu-berlin.de

Abstract

We propose an approximate inference algorithm for continuous time Gaussian Markov
process models with both discrete and continuous time likelihoods. We show that the
continuous time limit of the expectation propagation algorithm exists and results in a
hybrid ﬁxed point iteration consisting of (1) expectation propagation updates for discrete
time terms and (2) variational updates for the continuous time term. We introduce post-
inference corrections methods that improve on the marginals of the approximation. This
approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian ob-
servations  enabling continuous-time inference in a variety of models  including spiking
neuronal models (state-space models with point process observations) and box likelihood
models. Experimental results on real and simulated data demonstrate high distributional
accuracy and signiﬁcant computational savings compared to discrete-time approaches in
a neural application.

1

Introduction

Continuous time stochastic processes provide a ﬂexible and popular framework for data modelling in
a broad spectrum of scientiﬁc and engineering disciplines. Their intrinsically non-parametric  inﬁnite-
dimensional nature also makes them a challenging ﬁeld for the development of efﬁcient inference algo-
rithms. Recent years have seen several such algorithms being proposed for a variety of models [Opper
and Sanguinetti  2008  Opper et al.  2010  Rao and Teh  2012]. Most inference work has focused on the
scenario when observations are available at a ﬁnite set of time points  however  modern technologies are
making effectively continuous time observations increasingly common: for example  high speed imaging
technologies now enable the acquisition of biological data at around 100Hz for extended periods of time.
Other scenarios give intrinsically continuous time observations: for example  sensors monitoring the transit
of a particle through a barrier provide continuous time data on the particle’s position. To the best of our
knowledge  this problem has not been addressed in the statistical machine learning community.
In this paper  we propose an expectation-propagation (EP)-type algorithm [Opper and Winther  2000 
Minka  2001] for latent diffusion processes observed in either discrete or continuous time. We derive
ﬁxed-point update equations by considering a continuous time limit of the parallel EP algorithm [e.g. Op-
per and Winther  2005  Cseke and Heskes  2011b]: these ﬁxed point updates naturally become differential
equations in the continuous time limit. Remarkably  we show that  in the presence of continuous time
observations  the update equations for the EP algorithm reduce to updates for a variational Gaussian ap-
proximation [Archambeau et al.  2007]. We also generalise to the continuous-time limit the EP correction
scheme of [Cseke and Heskes  2011b]  which enable us to capture some of the non-Gaussian behaviour of
the time marginals.

1

2 Models and methods

We consider dynamical systems described by multivariate stochastic differential equations (SDEs) of
Ornstein-Uhlenbeck (OU) type over the [0  1] time interval

t dWt 

dxt = (Atxt + ct)dt + B1/2

(1)
where {Wt}t is the standard Wiener process [Gardiner  2002] and At  Bt and ct are time dependent matrix
and vector valued functions respectively with Bt being positive deﬁnite for all t ∈ [0  1]. Even though the
process does not posses a formulation through density functions (with respect to the Lebesgue measure) 
in order to be able to symbolically represent and manipulate the variables of the process in the Bayesian
formalism  we will use the proxy p0({xt}) to denote their distribution.
The process can be observed (noisily) both at discrete time points  and for continuous time intervals; we
t   t ∈ [0  1] accordingly. We assume that the likelihood
will partition the observations in yd
function admits the general formulation
ti|xti ) × exp￿−￿ 1

ti  ti ∈ Td and yc
t}|{xt}) ∝ ￿ti∈Td

We refer to p(yd
t   xt) as discrete time likelihood term and continuous time loss function 
respectively. We notice that  using Girsanov’s theorem and Ito’s lemma  non-linear diffusion equations
with constant (diagonal) diffusion matrix can be re-written in the form (1)-(2)  provided the drift can be
obtained as the gradient of a potential function [e.g. Øksendal  2010].
Our aim is to propose approximate inference methods to compute the marginals p(xt|{yd
posterior distribution

ti|xti) and V (t  yc

t   xt)￿ .

t}) of the

ti}i {yc

dtV (t  yc

p({yd

ti}i {yc

p(yd

(2)

0

p({xt}t|{yd

ti}i {yc

t}) ∝ p({yd

ti}i {yc

t}|{xt}) × p0({xt}).

2.1 Exact inference in Gaussian models
We start form the exact case of Gaussian observations and quadratic loss function. The linearity of equa-
tion (1) implies that the marginal distributions of the process at every time point are Gaussian (assuming
Gaussian initial conditions). The time evolution of the marginal mean mt and covariance Vt is governed
by the pair of differential equations [Gardiner  2002]

d
dt

mt = Atmt + ct

and

d
dt

Vt = AtVt + VtAT

t + Bt.

(3)

t Qc

In the case of Gaussian observations and a quadratic loss function V (t  yc
t +
txt  these equations  together with their backward analogues  enable an exact recursive inference
1
2 xT
algorithm  known as the Kalman-Bucy smoother [e.g. S¨arkk¨a  2006]. This algorithm arises because we can
recast the loss function as an auxiliary (observation) process

t   xt) = const. − xT

t hc

dyc

t = xtdt + R1/2

t dWt 

(4)

t dyc

t = Qc

t /dt = hc

t and R−1

t. This follows by the Gaussianity of the observation process and

where R−1
the fundamental property of Ito’s calculus dW 2
The Kalman-Bucy algorithm computes the posterior marginal means and covariances by solving the differ-
ential equations in a forward-backward fashion. These can be combined with classical Kalman ﬁltering to
account for discrete-time observations. The exact form of the equations as well as the variational derivation
of the Kalman-Bucy problem are given in Section B of the Supplementary Material.

t = Idt.

2.2 Approximate inference

In this section we use an Euler discretisation of the prior and the continuous time likelihood to turn our
model into a multivariate latent Gaussian model. We review the EP algorithm for such models and then we
show that when taking the limit ∆t → 0 the updates of the EP algorithm exist. The resulting approximate
posterior process is again an OU process and we compute its parameters. Finally  we show how corrections
to the marginals proposed [Cseke and Heskes  2011b] can be extended to the continuous time case.

2

p(yc|x) ∝ exp￿−￿k

∆tkV (tk  yc

tk   xtk )￿  

where yc is the matrix yc = [yc
sian model

t1  . . .   yc

tK ]. Consequently we approximate our model by the latent Gaus-

p({yd

ti}i  yc  x) = p0(x) ×￿i

where we remark that the prior p0 has a block-diagonal precision structure. To simplify notation  in the
following we use the aliases φd

i (xti) = p(yd

p(yd

ti|xti )￿k
ti|xti) and φc

exp￿−∆tkV (tk  yc
tk   xtk )￿
k(xtk ;∆ tk) = exp￿−∆tkV (tk  yc

tk   xtk )￿.

2.2.2 Inference using expectation propagation
Expectation propagation [Opper and Winther  2000  Minka  2001] is a well known algorithm that provides
good approximations of the posterior marginals in latent Gaussian models. We use here the parallel EP
approach [e.g. Cseke and Heskes  2011b]; similar continuous time limiting arguments can be made for the
original (sequential) EP approach. The algorithm approximates the posterior p(x|{yd
ti}i  yc) by a Gaussian

2.2.1 Euler discretisation
Let T = {t1 = 0  t2  . . .   tK−1  tK = 1} be a discretisation of the [0  1] interval and let the matrix
x = [xt1  . . .   xtK ] represent the process {xt}t using the discretisation given by T . Without loss of
generality we can assume that Td ⊂ T . We assume the Euler-Maruyama approach and approximate
p({xt}) by1

N (xtk+1 ; xtk + (Atk xtk + ctk )∆tk  ∆tkBtk )

p0(x) = N (x0; m0  V0)￿k

and in a similar fashion we approximate the continuous time likelihood by

q0(x) ∝ p0(x)￿i

˜φd

i (xti )￿k

˜φc
k(xtk ;∆ tk) 

where ˜φd
the ﬁxed point iteration

i and ˜φc

k are Gaussian functions. When applied to our model the algorithm proceeds by performing

(5)

[ ˜φd

i (xti )]

new

∝

Collapse(φd

i (xti ) ˜φd

q0(xti )
k(xtk ;∆ tk) ˜φc

i (xti )−1q0(xti );N )

× ˜φd

i (xti )
k(xtk ;∆ tk)−1q0(xtk );N )

new

Collapse(φc

for all ti ∈ Td 
× ˜φc

k(xtk ;∆ tk)

∝

q0(xtk )

k(xtk ;∆ tk)]

i (xti) ˜φd

i (xti) ˜φd

k(xtk ;∆ tk) ˜φc

for all tk ∈ T 

[ ˜φc
(6)
where Collapse(p(z);N ) = argminq∈N D[p(z)||q(z)] denotes the projection of the density p(z) into
In other words  Collapse(p(z);N ) is the Gaussian density that
the Gaussian family denoted by N .
matches the ﬁrst and second moments of p(z). Readers familiar with the classical formulation of
EP [Minka  2001] will recognise in equation (5) the so-called term updates  where ˜φd
i (xti)−1q0(xti)
i (xti)−1q0(xti) the tilted distribution. Equations (5-6) imply
is the cavity distribution and φd
that at any ﬁxed point of the iterations we have q(xti) = Collapse(φd
i (xti)−1q0(xti);N ) and
k(xtk ;∆ tk)−1q0(xtk );N ). The algorithm can also be derived and jus-
q(xtk ) = Collapse(φc
tiﬁed as a constrained optimisation problem of a Gibbs free energy formulation [Heskes et al.  2005]; this
alternative approach can also be shown to extend to the continuous time limit (see Section A.2 of the
Supplementary Material) and provides a useful tool for approximate evidence calculations.
Equation (5) does not depend on the time discretisation  and hence provides a valid update equation also
working directly with the continuous time process. On the other hand  the quantities in equation (6) de-
pend explicitly on ∆tk  and it is necessary to ensure that they remain well deﬁned (and computable) in
the continuous time limit. In order to derive the limiting behaviour of (6) we introduce the the follow-
ing notation: (i) we use f (z) = (z −zzT /2) to denote the sufﬁcient statistic of a multivariate Gaus-
sian (ii)  we use λd
ti) as the canonical parameters corresponding to the Gaussian function
˜φd
tk ) as the canonical parameters corresponding
i (xti) ∝ exp{λd
tk = (hc
to the Gaussian function ˜φc
tk · f (xtk )}  and ﬁnally  (iv) we use Collapse(p(z); f ) as
1We remark that one could also integrate the OU process between time steps  yielding an exact ﬁnite dimensional
2We use “·” as scalar product for general (concatenated) vector objects  for example  x·y = xT y when x  y ∈ Rn.

ti = (hd
ti · f (xti)}2  (iii) we use λc
k(xtk ) ∝ exp{∆tkλc

marginal of the prior. In the limit however both procedures are equivalent.

tk   Qc

ti  Qd

3

the canonical parameters corresponding to the density Collapse(p(z);N ). By using this notation we can
rewrite (6) as

[λc

tk ]new = λc

tk +

1

∆tk￿Collapse(qc(xtk ); f ) − Collapse(q0(xtk ); f )￿

with

(7)

(8)

(9)

The approximating density can then be written as

qc(xtk ) ∝ exp(−∆tk[V (tk  xtk ) + λc
q0(x) ∝ p0(x) × exp￿￿i
ti · f (xti ) +￿k

λd

tk · f (xtk )])q0(xtk ).

∆tkλc

tk · f (xtk )￿.

By direct Taylor expansion of Collapse(qc(xtk ); f ) one can show that the update equation (7) remains
ﬁnite when we take the limit ∆tk → 0. A slightly more general perspective however affords greater insight
into the algorithm  as shown below.

2.2.3 Continuous time limit of the update equations
Let µtk = Collapse(q0(xtk ); f ) and denote by Z(∆tk  µtk ) and Z(µtk ) the normalisation constant of
qc(xtk ) and q0(xtk ) respectively. The notation emphasises that qc(xtk ) differs from q0(xtk ) by a term
dependent on the granularity of the discretisation ∆tk. We exploit the well known fact that the derivatives
with respect to the canonical parameters of the log normalisation constant of a distribution within the
exponential family give the moment parameters of the distribution. From the deﬁnition of qc(xtk ) in
equation (8) we then have that its ﬁrst two moments can be computed as ∂µtk
log Z(∆tk  µtk ). The
Collapse operation in (7) can then be rewritten as

Collapse(qc(xtk ); f ) =Ψ( ∂µtk

log Z(∆tk  µtk )) 

(10)

where Ψ is the function transforming the moment parameters of a Gaussian into its (canonical) parameters.
We now assume ∆tk to be small and expand Z(∆tk  µtk ) to ﬁrst order in ∆tk. By using the property that
limα→0+ ￿g(z)α￿1/α

p(z) = exp(￿log g(x)￿p) for any distribution p(z) and g(z) > 0  one can write

lim

∆tk→0

1
∆tk

[log Z(∆tk  µtk ) − log Z(µtk )] = log lim

∆tk→0￿exp{−∆tk[V (tk  xtk ) + λc
tk · f (xtk )]￿q0(xtk )
= −￿[V (tk  xtk ) + λc
= −￿V (tk  xtk )￿q0(xtk ) − Ψ−1(µtk )λc
tk  

tk · f (xtk )]}￿1/∆tk

q0(xtk )

(11)
where we exploited the fact that ￿f (xtk )￿q0(xtk ) are the moments of the q0(xtk ) distribution. We can now
exploit the fact that ∆tk is small and linearise the nonlinear map Ψ about the moments of q0(xtk ) to obtain
a ﬁrst order approximation to equation (10) as

Collapse(qc(xtk ); f ) ￿ µtk − ∆tkλc

tk − ∆tkJΨ(µtk )∂µtk ￿V (tk  xtk )￿q0(xtk )

(12)

where JΨ(µtk ) denotes the Jacobian matrix of the map Ψ evaluated at µtk. The second term on the r.h.s.
of equation (12) follows from the obvious identity ∂µtk
By substituting (12) into (7)  we take the limit ∆tk → 0 and obtain the update equations

Ψ(Ψ−1(µtk )) = I.

Notice that the updating of λc
tained in the parameters µtk. Since λc
we can use the representation λc
t = (hc
tion of Gaussians we write the ﬁxed point iteration as

t ]new = −JΨ(µt)∂µt ￿V (t  xt)￿q0(xt)
[λc
(13)
t is somewhat hidden in equation (13); the “old” parameters are in fact con-
t corresponds to the canonical parameters of a multivariate Gaussian 
t) and after some algebra on the moment-canonical transforma-
t  Qc

for all t ∈ [0  1].

[hc
t ]new = −∂mt ￿V (t  xt)￿q0(xt) + 2∂Vt ￿V (t  xt)￿q0(xt) mt

(14)
where mt and Vt are the marginal means and covariances of q0 at the ∆tk → 0. Algorithmically  comput-
ing the marginal moments and covariances of the discretised Gaussian q0(x) in (9) can be done by solving a
sparse linear system and doing partial matrix inversion using the Cholesky factorisation and the Takahashi
equations as in Cseke and Heskes [2011b]. This corresponds to a junction tree algorithm on a (block) chain
graph [Davis  2006] which  in the continuous time limit  can be reduced to a set of differential equations

t ]new = ∂Vt ￿V (t  xt)￿q0(xt)  

[Qc

and

4

due to the chain structure of the graph. Alternatively  one can notice that  in the continuous time limit 
the structure of q0(x) in equation (9) deﬁnes a posterior process for an OU process p0({xt}) observed
at discrete times with Gaussian noise (corresponding to the terms ˜φd
i (xti) with canonical parameters λd
ti)
and with a quadratic continuous time loss  which is computed using equation (14). The moments there-
fore be computed using the Kalman-Bucy algorithm; details of the algorithm are given in Section B.1 of
the Supplementary Material. The derivation above illustrates another interesting characteristic of working
with continuous-time likelihoods. Readers familiar with the fractional free energies and the power EP al-
gorithm may notice that the time lag ∆tk plays a similar role as the fractional or power parameter α. It
is well known property that in the α → 0 limit the algorithm and the free energy collapses to variational
[e.g. Wiegerinck and Heskes  2003  Cseke and Heskes  2011a] and thus  intuitively  the collapse and the
existence of the limit is related to this property.
ti) corresponding to
Overall  we arrive to a hybrid algorithm in which: (i) the canonical parameters (hd
the discrete time terms are updated by the usual EP updates in (5)  (ii) the canonical parameters (hc
t  Qc
t)
corresponding to the continuous loss function V (t  xt) are updated by the variational updates in (14) (iii) 
the marginal moment parameters of q0(xt) are computed by the forward-backward differential equations
referred to in Section 2.1. We can use either parallel or a forward-backward type scheduling. A more
detailed description of the inference algorithm is given in Section C of the Supplementary Material. The
algorithm performs well in the comfort zone of EP  that is  log-concave discrete likelihood terms and convex
loss. Non-convergence can occur in case of multimodal likelihoods and loss functions and alternative
options to optimise the free energy have to be explored [e.g. Heskes et al.  2005  Archambeau et al.  2007].

ti  Qd

2.2.4 Parameters of the approximating OU process
The ﬁxed point iteration scheme computes only the marginal means and covariances of q0({xt}) and it
does not provide a parametric OU process as an approximation. However  this can be computed by ﬁnding
the parameters of an OU process that matches q0 in the moment matching Kullback-Leibler divergence.
That is  if q∗({xt}) minimises D[q0({xt})||q∗({xt})]  then the parameters of q∗ are given by

where mbw
t
are somewhat lengthy; a full derivation can be found in Section B.3 of the Supplementary Material.

(15)
are computed by the backward Kalman-Bucy ﬁltering equations. The computations

A∗t = At − Bt[V bw

c∗t = ct + Bt[V bw

and B∗t = Bt 

and V bw

]−1mbw
t

t

]−1 

t

t

2.2.5 Corrections to the marginals
In this section we extend the factorised correction method for multivariate latent Gaussian models intro-
duced in Cseke and Heskes [2011b] to continuous time observations. Other correction schemes [e.g. Opper
et al.  2009] can in principle also be applied. We start again from the discretised representation and then
take the ∆tk → 0. To begin with  we focus on the corrections from the continuous time observation pro-
cess. By removing the Gaussian terms (with canonical parameters λc
tk) from the approximate posterior and
replacing them with the exact likelihood  we can rewrite the exact discretised posterior as

The exact posterior marginal at time tj is thus given by

p(x) ∝ q0(x) × exp￿ −￿k

∆tk[V (tk  xtk ) + λc

tk · f (xt)]￿.

with

p(xtj ) ∝ q0(xtj )× exp￿ − ∆tj[V (tj  xtj + λc

cT (xtj ) =￿ dx\tj q0(x\tj|xtj ) × exp￿ −￿k￿=j

tj · f (xtj ))]￿ × cT (xtj )

∆tk[V (tk  xtk ) + λc

tk · f .(xtk )]￿ 

where the subscript \j indicates the whole vector with the j-th entry removed. By approximating the joint
conditional q0(x\tj|xtj ) with a product of its marginals and taking the ∆tk → 0 limit  we obtain

c(xt) ￿ exp￿ −￿ 1

0

ds￿V (s  xs) + λc

s · f (xs)￿q0(xs|xt)￿.

When combining the continuous part and the factorised discrete time corrections—by adding the discrete
time terms to the formalism above—we arrive to the corrected approximate marginal

˜p(xt) ∝ q0(xt) exp￿ −￿ 1

0

ds￿V (s  xs) + λc

s · f (xs)￿q0(xs|xt)￿ ×￿i ￿

p(yd
exp{λd

ti|xti )

ti · f (xti )}￿q0(xti|xt)

.

For any ﬁxed t one can compute the correlations in linear time by using the parametric form of the approx-
imation in 15. The evaluations for a ﬁxed xt are also linear in time.

5

3

2.5

2

1.5

1

0.5

 
0
−1

Marginal distributions at t=0.3351

 

sampling at 10−3
variational. corr
variational Gaussian

−0.8

−0.6

−0.4

−0.2

0

0.2

0.4

0.6

0.8

1

Figure 1: Inference results for the toy model in Section 3.1. The continuous time potential is deﬁned as V (t  xt) =
(2xt)8I[1/2 2/3](t) and we assume two hard box discrete likelihood terms I[−0.25 0.25](xt1 ) and I[−0.25 0.25](xt2 )
placed at t1 = 1/3 and t2 = 2/3. The prior is deﬁned by the parameters at = −1  ct = 4π cos(4πt) and bt = 4. The
left panel shows the prior’s and the posterior approximation’s marginal means and standard deviations. The right panel
shows the marginal approximations at t = 0.3351  a region where we expect the corrections to be strongly inﬂuenced
by both types of likelihoods. Samples were generated by using the lag ∆t = 10−3  the approximate inference was run
using RK4 at ∆t = 10−4.

3 Experiments

3.1 Inference in a (soft) box
The ﬁrst example we consider is a mixed discrete-continuous time inference under box and soft box likeli-
hood observations respectively. We consider a diffusing particle on the line under an OU prior process of
the form

dxt = (−axt + ct)dt + √bdWt

with a = −1  ct = 4π cos(4πt) and b = 4. The likelihood model is given by the loss function V (t  xt) =
(2xt)8 for all t ∈ [1/2  2/3] and 0 otherwise  effectively conﬁning the process to a narrow strip near zero
(soft box). This likelihood is therefore an approximation to physically realistic situations where particles
can perform diffusion in a conﬁned environment. The box has hard gates: two discrete time likelihoods
given by the indicator functions I[−0.25 0.25](xt1) and I[−0.25 0.25](xt2) placed at the ends of the interval 
that is  Td = {1/3  2/3}. The left panel in Figure 1 shows the prior and approximate posterior processes
(mean ± one standard deviation) in pink and cyan respectively: the conﬁnement of the process to the box is
in clear evidence  as well as the narrowing of the conﬁdence intervals corresponding to the two discrete time
observations. The right panel in Figure 1 shows the marginal approximations at a time point shortly after
the “gate” to the box  these are: (i) sampling (grey) (ii) the Gaussian EP approximation (blue line)  and (iii)
its corrected version (red line). The time point was chosen as we expect the strongest non-Gaussian effects
to be felt near the discrete likelihoods; the corrected distribution does indeed show strong skewness. To
benchmark the method  we compare it to MCMC sampling obtained by using slice sampling [Murray et al. 
2010] on the discretised model with ∆t = 10−3. We emphasise that this is an approximation to the model 
hence the benchmark is not a true gold standard; however  we are not aware of sampling schemes that
would be able to perform inference under the exact continuous time likelihood. The histogram in Figure 1
was generated from a sample size of 105 following a burn in of 104. The Gaussian EP approach gives a
very good reconstruction of the ﬁrst two moments of the distribution. The corrected EP approximation is
very close to the MCMC results.

3.2 Log Gaussian Cox processes
Another family of models where one encounters continuous time likelihoods is point processes; these
processes ﬁnd wide application in a number of disciplines  from neuroscience Smith and Brown [2003] to
conﬂict modelling Zammit-Mangion et al. [2012]. We assume that we have a multivariate log Gaussian
Cox process model [Kingman  1992]: this is deﬁned by a d-variate Ornstein-Uhlenbeck process {xt}t

6

Figure 2: A toy example for the point process model
The prior is deﬁned by A =
t = 4iπ cos(2iπt)  B = 4I. We use µi = 0. The prior means
[−2  1  0  1; 1 −2  1  0; 0  1 −2  1; 1  0  1 −2]  ci
and standard deviations  the sampled process path  and the sampled events are shown on the left panel while the
posterior approximations are shown on the right panel.

in Section 3.2.

on the [0  1] interval. Conditioned on {xt}t we have d Poisson point processes with intensities given by
t = eµi+xi
t for all i = 1  . . .   d and t ∈ [0  1]. The likelihood of this point process model is formed by
λi
both discrete time (point probabilities) and continuous time (void probability) terms and can be written as

log￿i

p(Yi|{xi

t}t)

.

=￿i ￿ − eµi￿ 1

0

dtexi

t + |Yi|µi + ￿tk∈Yi

xi

t￿ 

t}t. Clearly  the discrete time obser-
where Yi denotes the set of observed event times corresponding to {xi
vations in this model are (degenerate) Gaussians  therefore  one may opt for starting with an OU process
with a translated drift  however  for consistency reasons  we treat them as discrete time observations.
In this example we chose d = 4 and A = [−2  1  0  1; 1 −2  1  0; 0  1 −2  1; 1  0  1 −2]  thus coupling the
t = 4iπ cos(2iπt)  B = 4I and µi = 0. We generate a sample path { ˜xt}t 
various processes. We chose ci
draw observations Yi based on {˜xi
The results are shown in Figure 2  with four colours distinguishing the four processes. The left panel shows
prior processes (mean ± standard deviation)  sample paths and (bottom row) the sampled points (i.e. the
data). The right panel shows the corresponding posterior processes approximations. The results reﬂect the
general pattern characteristic of ﬁtting point process data: in regions with a substantial number of events
the sampled path can be inferred with great accuracy (accurate mean  low standard deviation) whereas in
regions with no or only a few events the ﬁt reverts to a skewed/shifted prior path  as the void probability
dominates.

t}t and perform inference.

3.3 Point process modelling of neural spikes trains

In a third example we consider continuous time point process inference for spike time recordings from a
population of neurons. This type of data is frequently modelled using (discrete time) state-space models
with point process observations (SSPP) [Smith and Brown  2003  Zammit Mangion et al.  2011  Macke
et al.  2011]; parameter estimation from such models can reveal biologically relevant facts about the neu-
ron’s electrophysiology which are not apparent from the spike trains themselves. We consider a dataset
from Di Lorenzo and Victor [2003]  available at www.neurodatabase.org  consisting of recordings of
spiking patterns of taste response cells in Sprague-Dawley rats during presentation of different taste stim-
uli. The recordings are 10s each at a resolution of 10−3s  and four different taste stimuli: (i) NaCL  (ii)
Quinine HCl  (iii) Quinine HCl  and (iv) Sucrose are presented to the subjects for the duration of the ﬁrst
5s of the 10s recording window. We modelled the spike train recordings by univariate log Gaussian Cox
process models (see Section 3.2) with homogeneous OU priors  that is  At  ct and Bt were considered
constant. We use the variational EM algorithm (discrete time likelihoods are Gaussian) to learn the prior

7

5

4.5

4

3.5

3

2.5

2

1.5

µ

The fitted (c µ) parameters

 

NaCl
Quinine
HCl
Sucrose

1
 
(cid:239)10

0

10
c

20

30

Figure 3: Inference results on data from cell 9 form the dataset in Section 3.3. The top-left  bottom-left and centre
panels show the intensity ﬁt  event count and the Q-Q plot corresponding to one of the recordings  whereas the right
panel shows the learned c and µ parameters for all spike trains in cell 9.

parameters A  c and µ and initial conditions for each individual recording. We scaled the 10s window into
the unit interval [0  1] and used a 10−4 resolution.
Fig 3 shows example results of this procedure. The right panel shows an emergent pattern of stimulus
based clustering of µ and c as in Zammit Mangion et al. [2011]. We observe that discrete-time approaches
such as [Smith and Brown  2003  Zammit Mangion et al.  2011] are usually forced to take very ﬁne time
discretisation by the requirement that at most one spike happens during one time step. This leads to signiﬁ-
cant computational resources being invested in regions with few spikes. Our continuous time approach  on
the other hand  handles uneven observations naturally.

4 Conclusion

Inference methodologies for continuous time stochastic processes are a subject of intense research  both
for fundamental and applied research. This paper contributes a novel approach which allows inference
from both discrete time and continuous time observations. Our results show that the method is effective
in accurately reconstructing marginal posterior distributions  and can be deployed effectively on real world
problems. Furthermore  it has recently been shown [Kappen et al.  2012] that optimal control problems can
be recast in inference terms: in many cases  the relevant inference problem is of the same type as the one
considered here  hence this methodology could in principle also be used in control problems. The method
is based on the parallel EP formulation of Cseke and Heskes [2011b]:
interestingly  we show that the
EP updates from continuous time observations collapse to variational updates [Archambeau et al.  2007].
Algorithmically  our approach results in efﬁcient forward-backward updates  compared to the gradient
ascent algorithm of Archambeau et al. [2007]. Furthermore  the EP perspective allows us to compute
corrections to the Gaussian marginals; in our experiments  these turned out to be highly accurate.
Our modelling framework assumes a latent linear diffusion process; however  as mentioned before  some
non-linear diffusion processes are equivalent to posterior processes for OU processes observed in contin-
uous time [Øksendal  2010]. Our approach  hence  can also be viewed as a method for accurate marginal
computations in (a class of) nonlinear diffusion processes observed with noise. In general  all non-linear
diffusion processes can be recast in a form similar to the one considered here; the important difference
though is that the continuous time likelihood is in general an Ito integral  not a regular integral. In the
future  it would be interesting to explore the extension of this approach to general non-linear diffusion
processes  as well as discrete and hybrid stochastic processes [Rao and Teh  2012  Ocone et al.  2013].

Acknowledgements

B.Cs. is funded by BBSRC under grant BB/I004777/1. M.O. would like to thank for the support by EU
grant FP7-ICT-270327 (Complacs). G.S. acknowledges support from the ERC under grant MLCS-306999.

8

References
C. Archambeau  D. Cornford  M. Opper  and J. Shawe-Taylor. Gaussian process approximations of stochastic differ-

ential equations. Journal of Machine Learning Research - Proceedings Track  1:1–16  2007.

B. Cseke and T. Heskes. Properties of Bethe free energies and message passing in Gaussian models. Journal of

Artiﬁcial Intelligence Research  41:1–24  2011a.

B. Cseke and T. Heskes. Approximate marginals in latent Gaussian models. Journal of Machine Learning Research 

12:417–457  2011b.

T. A. Davis. Direct Methods for Sparse Linear Systems (Fundamentals of Algorithms 2). Society for Industrial and

Applied Mathematics  Philadelphia  2006.

P. M. Di Lorenzo and J. D. Victor. Taste response variability and temporal coding in the nucleus of the solitary tract of

the rat. Journal of Neurophysiology  90:1418–1431  2003.

C. W. Gardiner. Handbook of stochastic methods: for physics  chemistry and the natural sciences. Springer series in

synergetics  13. Springer  2002.

T. Heskes  M. Opper  W. Wiegerinck  O. Winther  and O. Zoeter. Approximate inference techniques with expectation

constraints. Journal of Statistical Mechanics: Theory and Experiment  2005.

H. J. Kappen  V. G´omez  and M. Opper. Optimal control as a graphical model inference problem. Machine Learning 

87(2):159–182  2012.

J. F. C. Kingman. Poisson Processes. Oxford Statistical Science Series. Oxford University Press  New York  1992.
S. L. Lauritzen. Graphical Models. Oxford Statistical Science Series. Oxford University Press  New York  1996.
J. H. Macke  L. Buesing  J. P. Cunningham  B. M. Yu  K. V. Shenoy  and M. Sahani. Empirical models of spiking in

neural populations. In Advances in Neural Information Processing Systems 24  pages 1350–1358. 2011.

T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  MIT  2001.
I. Murray  R. P. Adams  and D. J.C. MacKay. Elliptical slice sampling.

In Proceedings of the 13th International

Conference on Artiﬁcial Intelligence and Statistics  pages 541–548. 2010.

A. Ocone  A.J. Millar  and G. Sanguinetti. Hybrid regulatory models: a statistically tractable approach to model

regulatory network dynamics. Bioinformatics  29(7):910–916  2013.

B. Øksendal. Stochastic differential equations. Universitext. Springer  2010.
M. Opper and G. Sanguinetti. Variational inference for Markov jump processes. In Advances in Neural Information

Processing Systems 20  2008.

M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean-ﬁeld algorithms. Neural Computation  12(11):

2655–2684  2000.

M. Opper and O. Winther. Expectation consistent approximate inference. Journal of Machine Learing Research  6:

2177–2204  2005.

M. Opper  U. Paquet  and O. Winther. Improving on Expectation Propagation. In Advances in Neural Information

Processing Systems 21  pages 1241–1248. MIT  Cambridge  MA  US  2009.

M. Opper  A. Ruttor  and G. Sanguinetti. Approximate inference in continuous time Gaussian-Jump processes. In

Advances in Neural Information Processing Systems 23  pages 1831–1839  2010.

V. Rao and Y-W Teh. MCMC for continuous-time discrete-state systems. In Advances in Neural Information Process-

ing Systems 25  pages 710–718  2012.

S. S¨arkk¨a. Recursive Bayesian Inference on Stochastic Differential Equations. PhD thesis  Helsinki University of

Technology  2006.

A. C. Smith and E. N. Brown. Estimating a state-space model from point process observations. Neural Computation 

15(5):965–991  2003.

W. Wiegerinck and T. Heskes. Fractional Belief Propagation. In Advances in Neural Information Processing Systems

15  pages 438–445  Cambridge  MA  2003. The MIT Press.

J. S. Yedidia  W. T. Freeman  and Y. Weiss. Generalized belief propagation.

Processing Systems 12  pages 689–695  Cambridge  MA  2000. The MIT Press.

In Advances in Neural Information

A. Zammit Mangion  K. Yuan  V. Kadirkamanathan  M. Niranjan  and G. Sanguinetti. Online variational inference for

state-space models with point-process observations. Neural Computation  23(8):1967–1999  2011.

A. Zammit-Mangion  G. Dewar  M.  Kadirkamanathan V.  A.  and G. Sanguinetti. Point process modelling of the

Afghan war diary. Proceeding of the National Academy of Sciences  2012. doi: 10.1073/pnas.1203177109.

9

,Botond Cseke
Manfred Opper
Guido Sanguinetti
Shenlong Wang
Sanja Fidler
Raquel Urtasun