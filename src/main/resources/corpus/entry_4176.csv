2019,Outlier Detection and Robust PCA Using a Convex Measure of Innovation,This paper presents a provable  and strong algorithm  termed Innovation Search (iSearch)  to robust Principal Component Analysis (PCA) and outlier detection.  An outlier by definition is a data point which  does not participate in forming a low dimensional structure with a large number of data points in the data.  In other word  an outlier carries some innovation with respect to most of the other data points. iSearch ranks the data points based on their values of innovation. A convex optimization problem is proposed whose optimal value is used as our measure of  innovation.  We derive analytical performance guarantees for the proposed robust PCA method  under different models for the distribution of the outliers including randomly distributed outliers  clustered outliers  and linearly dependent outliers.  Moreover  it is shown that iSearch provably recovers the span of the inliers when the inliers lie in a union of subspaces.  In the challenging scenarios in which the outliers are close to each other or they are close to the span of the inliers  iSearch is shown to  outperform most of the existing methods.,Outlier Detection and Robust PCA Using a Convex

Measure of Innovation

Mostafa Rahmani and Ping Li

Cognitive Computing Lab

Baidu Research

10900 NE 8th St. Bellevue  WA 98004  USA
{mostafarahmani liping11}@baidu.com

Abstract

This paper presents a provable and strong algorithm  termed Innovation Search
(iSearch)  to robust Principal Component Analysis (PCA) and outlier detection.
An outlier by deﬁnition is a data point which does not participate in forming a
low dimensional structure with a large number of data points in the data. In other
words  an outlier carries some innovation with respect to most of the other data
points. iSearch ranks the data points based on their values of innovation. A convex
optimization problem is proposed whose optimal value is used as our measure of
innovation. We derive analytical performance guarantees for the proposed robust
PCA method under different models for the distribution of the outliers including
randomly distributed outliers  clustered outliers  and linearly dependent outliers.
Moreover  it is shown that iSearch provably recovers the span of the inliers when
the inliers lie in a union of subspaces. In the challenging scenarios in which the
outliers are close to each other or they are close to the span of the inliers  iSearch
is shown to outperform most of the existing methods.

1

Introduction

Outlier detection is an important research problem in unsupervised machine learning. Outliers are
associated with important rare events such as malignant tissues [14]  the failures of a system [10 
12  31]  web attacks [16]  and misclassiﬁed data points [9  27]. In this paper  the proposed outlier
detection method is introduced as a robust Principal Component Analysis (PCA) algorithm  i.e. 
the inliers lie in a low dimensional subspace. In the literature of robust PCA  two main models for
the data corruption are considered: the element-wise model and the column-wise model. These
two models are corresponding to two different robust PCA problems. In the element-wise model 
it is assumed that a small subset of the elements of the data matrix are corrupted and the support
of the corrupted elements is random. This problem is known as the low rank plus sparse matrix
decomposition problem [1  3  4  23  24]. In the column-wise model  a subset of the columns of the
data are affected by the data corruption [5  7  8  11  17  20  25  26  36–39]. Section 2 provides a review
of the robust (to column-wise corruption) PCA methods. This paper focuses on the column-wise
model  i.e.  we assume that the given data follows Data Model 1.
Data Model 1. The data matrix D ∈ RM1×M2 can be expressed as D = [B (A + N)] T   where
A ∈ Rm×ni  B ∈ Rm×no  T is an arbitrary permutation matrix  and [B (A + N)] represents
the concatenation of B and (A + N). The columns of A lie in an r-dimensional subspace U. The
columns of B do not lie entirely in U  i.e.  the ni columns of A are the inliers and the no columns of
B are the outliers. The matrix N represents additive noise. The orthonormal matrix U ∈ RM1×r is
a basis for U. Evidently  M2 = ni + no.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In the robust PCA problem  the main task is to recover U. Clearly  if U is estimated accurately  the
outliers can be located using a simple subspace projection [22].
Summary of Contributions: The main contributions can be summarized as follows.

• The proposed approach introduces a new idea to the robust PCA problem. iSearch uses a
convex optimization problem to measure the Innovation of the data points. It is shown that
iSearch mostly outperforms the exiting methods in handling close outliers and noisy data.

• To the best of our knowledge  the proposed approach and the CoP method presented
in [27] are the only robust PCA methods which are supported with analytical performance
guarantees under different models for the distributions of the outliers including the randomly
distributed outliers  the clustered outliers  and the linearly dependent outliers.

• In addition to considering different models for the distribution of the outliers  we provide
analytical performance guarantees under different models for the distributions of the inliers
too. The presumed models include the union of subspaces and the uniformly at random
distribution on U ∩ SM1−1 where SM1−1 denotes the unit (cid:96)2-norm sphere in RM1.

Notation: Given a matrix A  (cid:107)A(cid:107) denotes its spectral norm. For a vector a  (cid:107)a(cid:107)p denotes its (cid:96)p-norm
and a(i) its ith element. Given two matrices A1 and A2 with an equal number of rows  the matrix
A3 = [A1 A2] is the matrix formed by concatenating their columns. For a matrix A  ai denotes
its ith column. The subspace U⊥ is the complement of U. The cardinality of set I is deﬁned as |I|.
Also  for any positive integer n  the index set {1  ...  n} is denoted [n]. The coherence between vector
a and subspace H with orthonormal basis H is deﬁned as (cid:107)aT H(cid:107)2.

2 Related Work

In this section  we brieﬂy review some of the related works. We refer readers to [18  27] for a more
comprehensive review on the topic. One of the early approaches to robust PCA was to replace the
Frobenius norm in the cost function of PCA with (cid:96)1-norm because (cid:96)1-norm were shown to be robust to
the presence of the outliers [2 15]. The method proposed in [6] leveraged the column-wise structure of
the corruption matrix and replaced the (cid:96)1-norm minimization problem with an (cid:96)1 2-norm minimization
problem. In [19] and [39]  the optimization problem used in [6] was relaxed to a convex optimization
problem and it was proved that under some sufﬁcient conditions the optimal point is a projection
matrix which spans U. In [34]  a provable outlier rejection method was presented. However  [34]
assumed that the outliers are randomly distributed on SS−1 and the inliers are distributed randomly
on U ∩ SM1−1. In [36]  a convex optimization problem was proposed which decomposes the data into
a low rank component and a column sparse component. The approach presented in [36] is provable
but it requires no to be signiﬁcantly smaller than ni. In [32]  it was assumed that the outliers are
randomly distributed on SM1−1 and a small number of them are not linearly dependent. The method
presented in [32] detects a data point as an outlier if it does not have a sparse representation with
respect to the other data points.
Connection and Contrast to Coherence Pursuit: In [27]  Coherence Pursuit (CoP) was proposed
as a provable robust PCA method. CoP computes the Coherence Values for all the data points to rank
the data points. The Coherence value corresponding to data column d is a measure of resemblance
between d and the rest of the data columns. CoP uses the inner product between d and the rest of the
data points to measure the resemblance between d and the rest of data. In sharp contrast  iSearch
ﬁnds an optimal direction corresponding to each data column. The optimal direction corresponding
to data column d is used to measure the innovation of d with respect to the rest of the data columns.
We show through theoretical studies and numerical experiments that ﬁnding the optimal directions
makes iSearch signiﬁcantly stronger than CoP in detecting outliers which carry weak innovation.
Connection and Contrast to Innovation Pursuit: In [28  29]  Innovation Pursuit was proposed as
a new subspace clustering method. The optimization problem proposed in [28] ﬁnds a direction in the
span of the data such that it is orthogonal to the maximum number of data points. We present a new
discovery about the applications of Innovation Pursuit. It is shown that the idea of innovation search
can be used to design a strong outlier detection algorithm. iSearch uses an optimization problem
similar to the linear optimization problem used in [28] to measure the innovation of the data points.

2

Algorithm 1 Subspace Recovery Using iSearch
1. Data Preprocessing. The input is data matrix D ∈ RM1×M2.
1.1 Deﬁne Q ∈ RM1×rd as the matrix of ﬁrst rd left singular vectors of D where rd is the number of
non-zero singular values. Set D = QT D. If dimensionality reduction is not required  skip this step.
1.2 Normalize the (cid:96)2-norm of the columns of D  i.e.  set di equal to di/(cid:107)di(cid:107)2 for all 1 ≤ i ≤ M2.
2. Direction Search. Deﬁne C∗ ∈ Rrd×M2 such that c∗
subject to

i ∈ Rrd×1 is the optimal point of

(cid:107)cT D(cid:107)1

cT di = 1

min

c

or deﬁne C∗ ∈ Rrd×M2 as the optimal point of

(cid:107)(CT D)T(cid:107)1

C

min

subject to

(1)
3. Computing the Innovation Values. Deﬁne vector x ∈ RM2×1 such that x(i) = 1/(cid:107)DT c∗
i (cid:107)1.
4. Building Basis. Construct matrix Y from the columns of D corresponding to the smallest
elements of x such that they span an r-dimensional subspace.
Output: The column-space of Y is the identiﬁed subspace.

diag(CT D) = 1 .

3 Proposed Approach

Algorithm 1 presents the proposed method along with the deﬁnition of the used symbols. iSearch
consists of 4 steps. In the next subsections  Step 2 and Step 4 are discussed. In this paper  we use an
ADMM solver to solve (1). The computation complexity of the solver is O(max(M1M 2
1 M2)).
If PCA is used in the prepossessing step to reduce the dimensionality of the data to rd  the computation
complexity of the solver is O(max(rdM 2

2   M 2

2   r2

dM2)) 1.

3.1 An Illustrative Example for Innovation Value

We use a synthetic numerical example to explain the idea behind the proposed approach. Suppose
D ∈ R20×250  ni = 200  no = 50  and r = 3. Assume that D follows Assumption 1.
Assumption 1. The columns of A are drawn uniformly at random from U ∩ SM1−1. The columns of
B are drawn uniformly at random from SM1−1. To simplify the exposition and notation  it is assumed
without loss of generality that T in Data Model 1 is the identity matrix  i.e  D = [B A].
Suppose d is a column of D  deﬁne c∗ as the optimal point of

(cid:107)cT D(cid:107)1

c

min

subject to

cT d = 1  

(2)
and deﬁne the Innovation Value corresponding to d as 1/(cid:107)DT c∗(cid:107)1. The main idea of iSearch is that
c∗ has two completely different behaviours with respect to U (when d is an outlier and when d is
an inlier). Suppose d is an outlier. The optimization problem (2) searches for a direction whose
projection on d is non-zero and it has the minimum projection on the rest of the data points. As d is
an outlier  d has a non-zero projection on U⊥. In addition  as ni is large  (2) searches for a direction
in the ambient whose projection on U is as weak as possible. Thus  c∗ lies in U⊥ or it is close to U⊥.
The left plot of Figure 1 shows DT c∗ when d is an outlier. In this case  c∗ is orthogonal to all the
inliers. Accordingly  when d is an outliers  (cid:107)DT c∗(cid:107)1 is approximately equal to (cid:107)BT c∗(cid:107)1. On the
other hand  when d is an inlier  the linear constraint strongly discourages c∗ to lie in U⊥ or to be
close to U⊥. Inliers lie in a low dimensional subspace and mostly they are close to each other. Since
c∗ has a strong projection on d  it has strong projections on many of the inliers. Accordingly  the
value of (cid:107)AT c∗(cid:107)1 is much larger when d is an inlier. Therefore  the Innovation Value corresponding
to an inlier is smaller than the Innovation Value corresponding to an outlier because (cid:107)AT c∗(cid:107)1 is
much larger when d is an inliers. Figure 1 compares the vector DT c∗ when d is an outliers with the
same vector when d is an inlier. In addition  it shows the vector of Innovation Values (right plot).
One can observe that the Innovation Values make the outliers clearly distinguishable.

1If the data is noisy  rd should be set equal to the number of dominant singular values. In this paper  we do
not theoretically analyze iSearch in the presence of noise. In the numerical experiments  we set rd equal to the
index of the largest singular value which is less than or equal to 0.01 % of the ﬁrst singular value.

3

Figure 1: The ﬁrst 50 columns are outliers. The left panel shows vector DT c∗ when d is an outlier.
The middle panel depicts DT c∗ when d is an inlier. The right panel shows the Innovation Values
corresponding to all the data points (vector x was deﬁned in Algorithm 1).

3.2 Building the Basis Matrix

The data points corresponding to the least Innovation Values are used to construct the basis matrix Y.
If the data follows Assumption 1  the r data points corresponding to the r smallest Innovation Values
span U with overwhelming probability [35]. In practise  the algorithm should continue adding new
columns to Y until the columns of Y spans an r-dimensional subspace. This approach requires to
check the singular values of Y several times. We propose two techniques to avoid this extra steps.
The ﬁrst approach is based on the side information that we mostly have about the data. In many
applications  we can have an upper-bound on no because outliers are mostly associated with rare
events. If we know that the number of outliers is less than y percent of the data  matrix Y can be
constructed using (1− y) percent of the data columns which are corresponding to the least Innovation
Values. The second approach is the adaptive column sampling method proposed in [27]. The adaptive
column sampling method avoids sampling redundant columns.

4 Theoretical Studies

In this section  we analyze the performance of the proposed approach with three different models
for the distribution of the outliers: unstructured outliers  clustered outliers  and linearly dependent
outliers. Moreover  we analyze iSearch with two different models for the distribution of the inliers.
These models include the union of subspaces and uniformly at random distribution on U ∩ SM1−1.
Due to space limitation  in this paper we do not include theoretical guarantees with noisy data and
we refer the reader for the analysis of iSearch with noisy data to [30]. In Section 5  it is shown with
real and synthetic data that iSearch accurately detects the outliers even in the low signal to noise
ratio cases and it mostly outperforms the existing approaches when the data is noisy. The theoretical
results are followed by short discussions which highlight the important aspects of the theorems. The
proofs of the presented theorems are available in an extended version of this work [30].

4.1 Randomly Distributed Outliers

In this section  it is assumed that D follows Assumption 1. In order to guarantee the performance of
the proposed approach  it is enough to show that the Innovation Values corresponding to the outliers
are greater than the Innovation Values corresponding to the inliers. In other word  it sufﬁces to show

i

4

(cid:16){1/(cid:107)DT c∗

max

i (cid:107)1}M2

i=no+1

(cid:17)

< min(cid:0){1/(cid:107)DT c∗

(cid:1) .

j(cid:107)1}no

j=1

z = max(cid:0){|I i

Before we state the theorem  let us provide the following deﬁnitions and remarks.
Deﬁnition 1. Deﬁne c∗
(cid:48)

(cid:107)cT D(cid:107)1. In addition  deﬁne χ = max(cid:0){(cid:107)c∗

j = arg min
j c=1

dT

0|}no

n
0| is the number of outliers which are orthogonal to c∗
|I i
i .
Remark 1. In Assumption 1  the outliers are randomly distributed. Thus  if no is signiﬁcantly larger
than M1  n

(cid:48)
z is signiﬁcantly smaller than no with overwhelming probability.

T bi = 0} and bi is the ith column of B. The value

0 = {i ∈ [no] : c∗

(cid:1) where I i

j(cid:107)2}no

i=1

i=1

(3)

(cid:1)  and

00.20.40.60.811.2|DT c*|d is an outlier050100150200250Element Index00.20.40.60.81|DT c*|d is an inlier050100150200250Element Index00.20.40.60.81xInnovation Values050100150200250Element IndexTheorem 1. Suppose D follows Assumption 1 and deﬁne A =

(cid:113) 1

2π

r − √

ni√

ni −(cid:113) ni log 1

2r−2 . If

δ

+

(cid:48)(cid:48)
noc
δ log no/δ
M 2
1

(cid:115)
(cid:35)(cid:114) 4M1cδ
(cid:115)

M1 − cδr

2χno log 1
δ
M − 1

  2n

A >

+ 2

A > max

(cid:34)

no
M1

(cid:115)

(cid:48)
z

n

(cid:16)
(cid:16) 4

(cid:115)
(cid:114) no
(cid:115)(cid:18) no

M1

+

2no log 1/δ
(M1 − 1)M1

+

(cid:19)

+

ηδ
M1

log no/δ

√

χ) + 2

no(1 +

M 2
1
√

+ 2

(cid:113) 8M1 log n0/δ
(cid:17)

(M1−1)r

(cid:17)

 

(cid:113)

+

(cid:48)(cid:48)
c
δ
M 2
1

no√
M1

χ
(cid:113) 8M1π
(cid:113)

max

3 log 2M1
δ  

4 no
M1

log 2M1
δ

.

(cid:115)

+ 2

(cid:48)
z

and

(cid:114) cδr
(cid:113) 8M1π

M1

(cid:18)

nocδr log no/δ

M1

(cid:113) 16M1 log n0/δ

(cid:19)

(4)

  

√

then (3) holds and U is recovered exactly with probability at least 1 − 7δ where
3 max

1 

1 

(cid:48)(cid:48)
c
δ = 3 max

(M1−1)r  

M1−1  

M1−1

cδ =
  and ηδ =

Theorem 1 shows that as long as ni/r is sufﬁciently larger than no/M1  the proposed approach is
guaranteed to detect the randomly distributed outliers exactly.
It is important to note that in the
sufﬁcient conditions ni is scaled with 1/r but no is scaled with 1/M1. It shows that if r is sufﬁciently
smaller than M1  iSearch provably detects the unstructured outliers even if no is much larger than ni.
The numerical experiments presented in Section 5 conﬁrms this feature of iSearch and they show that
if the outliers are unstructured  iSearch can yield exact recovery even if no > 100 ni. It is important
to note that when the outliers are structured  by the deﬁnition of outlier  no cannot be larger than ni.

4.2 Structured Outliers

In this section  we analyze the proposed approach with structured outliers.
In contrast to the
unstructured outliers  structured outliers can form a low dimensional structure different from the
structure of the majority of the data points. Structured outliers are associated with important rare
events such as malignant tissues [14] or web attacks [16]. In this section  we assume that the
outliers form a cluster outside of U. The following assumption speciﬁes the presumed model for the
distribution of the structured outliers.
Assumption 2. A column of B is formed as bi = 1√
not lie in U  {vi}no
According to Assumption 2  the outliers cluster around vector q where q (cid:54)∈ U. In Algorithm 1  if the
dimensionality reduction step is performed  the direction search optimization problem is applied to
QT D. Thus  (2) is equivalent to
(cid:107)cT D(cid:107)1

i=1 are drawn uniformly at random from SM1−1  and η is a positive number.

subject to cT d = 1 and c ∈ Q  

(q + ηvi). The unit (cid:96)2-norm vector q does

min

(5)

1+η2

c

where c ∈ RM1×1 and D ∈ RM1×M2. The subspace Q is the column-space of D. In this section  we
are interested in studying the performance of iSearch in identifying tightly clustered outliers because
some of the existing outlier detection algorithms fail if the outliers form a tight cluster. For instance 
the thresholding based method [13] and the sparse representation based algorithm [32] fail when the
outliers are close to each other. Therefore  we assume that the span of Q is approximately equal to
√
the column-space of [U q]. The following Theorem shows that even if the outliers are close to each
other  iSearch successfully identiﬁes the outliers provided that ni/
r is sufﬁciently larger than no.
Theorem 2. Suppose the distribution of the inliers/outliers follows Assumption-1/Assumption-2.
Assume that Q is equal to the column-space of [U q]. Deﬁne q⊥ = (I−UUT )q
  deﬁne β =
(cid:107)(I−UUT )q(cid:107)2
i as the optimal point of (5) with d = di  and assume that

: di ∈ B}(cid:1)  deﬁne c∗

max(cid:0){1/|dT

i q⊥|

5

η < |qT q⊥|. In addition  deﬁne A =

√

1+η2
2β

ni −(cid:113) 2ni log 1

r−1

δ

(cid:19)

. If

√
r − 2
ni√

π

(cid:18)(cid:113) 2
(cid:115)
(cid:115)

A > no(cid:107)UT q(cid:107)2 + η

A > no|qT q⊥| + noη

norcδ log no/δ

M1

c(cid:48)(cid:48)
δ log no/δ

M1

 

 

(6)

then (3) holds and U is recovered exactly with probability at least 1 − 5δ.

√
In contrast to (4)  in (6) no is not scaled with 1/
M1. Theorem 2 shows that in contrast to the
unstructured outliers  the number of the structured outliers should be sufﬁciently smaller than the
number of the inliers for the small values of η. This is consistent with our intuition regarding the
detection of structured outliers. If the columns of B are highly structured and most of the data points
are outliers  it violates the deﬁnition of outlier to label the columns of B as outliers.
The presence of parameter β emphasizes that the closer the outliers are to U  the harder it is to
distinguish them.
In Section 5  it is shown that iSearch signiﬁcantly outperforms the existing
methods when the outliers are close to U. The main reason is that even if an outlier is close to
U  its corresponding optimal direction obtained by (2) is highly incoherent with U. Therefore  its
corresponding optimal direction is incoherent with the inliers.
When the outliers are very close to the span of the inliers  the norm of c∗ should be large to satisfy
the linear constraint of (2) because c∗ is orthogonal or nearly orthogonal to U. Accordingly  in the
applications in which the outliers are highly coherent with U  the (cid:96)2-norm of c∗ should be normalized
before computing the Innovation Values.

4.3 Linearly Dependent Outliers

In some applications  the outliers are linearly dependent. For instance  in [9]  it was shown that a
robust PCA algorithm can be used to reduce the clustering error of a subspace segmentation method.
In this application  a small subset of the outliers can be linearly dependent. This section focuses on
detecting linearly dependent outliers. The following assumption speciﬁes the presumed model for
matrix B and Theorem 3 provides the guarantees.
Assumption 3. Deﬁne subspace Uo with dimension ro such that Uo /∈ U and U /∈ Uo. The outliers
are randomly distributed on SM1−1 ∩ Uo. The orthonormal matrix Uo ∈ RM1×ro is a basis for Uo.

Theorem 3. Suppose the distribution of the inliers/outliers follows Assumption-1/Assumption-3.
Deﬁne A =
A > 2n

r−1

√

. If

π

(cid:48)

δ

(cid:115)(cid:18) no

ro

+ 2

(cid:19)

+ η

(cid:48)
δ

log

+ n

(cid:48)
z

no
δ

 

(7)

(cid:33)

2(cid:107)UT Uo(cid:107)

r − 2
ni√
(cid:32)

ni −(cid:113) 2ni log 1
(cid:115)

(cid:113) 2
z(cid:107)UT Uo(cid:107) + 2(cid:107)UT Uo(cid:107)(cid:112)no log no/δ  
(cid:107)UT
 χno√
(cid:16) 4

2no log 1
δ
ro − 1

2no log 1
δ
ro − 1

no√
ro

(cid:115)

(cid:113)

χno +

no +

(cid:17)

+ 2

+ 2

√

ξ

ro

√

χ

3 log 2(ro)/δ  

4 no
ro

log 2rd
δ

and ξ =

A >

A >

(cid:48)
δ = max

η

o U⊥(cid:107)  
(cid:16){(cid:107)bT

min

(cid:17)
then (3) holds and U is recovered exactly with probability at

j U⊥(cid:107)2}no
(cid:107)UT
o U⊥(cid:107)

j=1

least 1 − 5δ where

.

Theorem 3 indicates that ni/r should be sufﬁciently larger than no/ro. If ro is comparable to r  it
is in fact a necessary condition because we can not label the columns of B as outliers if no is also
comparable with ni. If ro is large  the sufﬁcient condition is similar to the sufﬁcient conditions of
Theorem 1 in which the outliers are distributed randomly on SM1−1.

6

It is informative to compare the requirements of iSearch with the requirements of CoP. With iSearch 
(cid:107)UoU⊥(cid:107) to guarantee that the algorithm distinguishes the
ni/r should be sufﬁciently larger than no
ro
outliers successfully. With CoP  ni/ri should be sufﬁciently larger than no/ro +(cid:107)UT
o U(cid:107)ni/ri [9 27].
The reason that CoP requires a stronger condition is that iSearch ﬁnds a direction for each outlier
which is highly incoherent with U.

4.4 Outlier Detection When the Inliers are Clustered

(cid:19)

√
− 2

ng√
d

π

ng −(cid:113) 2ng log 1

r−1

δ

(cid:80)m

In the analysis of the robust PCA methods  mostly it is assumed that the inliers are randomly
distributed in U. In practise the inliers form several clusters in the column-space of the data. In
this section  it is assumed that the inliers form m clusters. The following assumption speciﬁes the
presumed model and Theorems 4 provides the sufﬁcient conditions.
Assumption 4. The matrix of inliers can be written as A = [A1 ... Am]TA where Ak ∈ RM1×nik 
k=1 nik = ni  and TA is an arbitrary permutation matrix. The columns of Ak are drawn uniformly
at random from the intersection of subspace Uk and SM1−1 where Uk is a d-dimensional subspace.
k=1 and (U1 ⊕ ... ⊕ Um) = U
In other word  the columns of A lie in a union of subspaces {Uk}m
where ⊕ denotes the direct sum operator.

Theorem 4. Suppose the distribution of the outliers/inliers follows Assumptions 1 to 4. Further
deﬁne A = ρ

(cid:18)(cid:113) 2
(cid:80)m
k=1 (cid:107)δTUk(cid:107)2 . If the sufﬁcient conditions in (4) are satisﬁed  then (3) holds and U is

ρ = inf
δ∈U
(cid:107)δ(cid:107)=1
recovered exactly with probability at least 1 − 7δ.
Since the dimensions of the subspaces {Uk}m
k=1 are equal and the distribution of the inliers inside
(cid:80)m
√
these subspace are similar  roughly g = arg mink nik [19]. Thus  the sufﬁcient conditions indicate
that the population of the smallest cluster scaled by 1/
d should be sufﬁciently larger than no/M1.
k=1 (cid:107)δTUk(cid:107)2 is similar to the permeance statistic introduced in [19]. It
The parameter ρ = inf
δ∈U
(cid:107)δ(cid:107)=1
shows how well the inliers are distributed in U. Evidently  if the inliers populate all the directions
inside U  a subspace recovery algorithm is more likely to recover U correctly. However  having a
large value of permeance statistic is not a necessary condition. The reason that permeance statistic
appears in the sufﬁcient conditions is that we establish the sufﬁcient conditions to guarantee the
performance of iSearch in the worst case scenarios. In fact  if the inliers are close to each other or
the subspaces {Ui}m
i=1 are close to each other  generally the performance of iSearch improves. The
reason is that the more inliers are close to each other  the smaller their Innovation Values are.

where g = arg mink inf
δ∈Uk
(cid:107)δ(cid:107)=1

(cid:107)δT Ak(cid:107)1  and

5 Numerical Experiments

A set of experiments with synthetic data and real data are presented to study the performance and
the properties of the iSearch algorithm. In the presented experiments  iSearch is compared with the
existing methods including FMS [17]  GMS [39]  CoP [27]  OP [36]  and R1-PCA [6].

5.1 Phase Transition

In this experiment  the phase transition of iSearch is studied. Deﬁne ˆU as an orthonormal basis for
the recovered subspace. A trial is considered successful if

(cid:107)(I − UUT ) ˆU(cid:107)F

(cid:107)U(cid:107)F

< 10−2 .

The data follows Assumption 1 with r = 4 and M1 = 100. The left plot of Figure 2 shows the phase
transition of iSearch versus ni/r and no/M1. White indicates correct subspace recovery and black
designates incorrect recovery. Theorem 1 indicated that if ni/r is sufﬁciently large  iSearch yields
exact recovery even if no is larger than ni. This experiment conﬁrms the theoretical result. According
to Figure 2  even when no = 3000  40 inliers are enough to guarantee exact subspace recovery.

7

Figure 2: Left panel: The phase transition of iSearch in presence of the unstructured outliers versus
ni/r and no/M1 (M1 = 100 and r = 4). Middle panel: The probability of accurate subspace
recovery versus the number of structured outliers (ni = 100  η = 0.1  M1 = 100  and r = 10).
Right panel: The probability of exact outlier detection versus SNR. The data contains 10 structured
outliers and 300 unstructured outliers (ni = 100  no = 310  r = 5  and M1 = 100).

5.2 Structured Outliers

In this experiment  we consider structured outliers. The distribution of the outliers follows Assump-
tion 2 with η = 0.1 and M1 = 100. In addition  the inliers are clustered and they lie in a union of
5 2-dimensional linear subspaces. There are 20 data points in each subspace (i.e.  ni = 100) and
r = 10. A successful trial is deﬁned similar to Section 5.1. We are interested in investigating the
performance of iSearch in identifying structured outliers when they are close to U. Therefore  we
generate vector q  the center of the cluster of the outliers  close to U. Vector q is constructed as
  where the unit (cid:96)2-norm vector p ∈ RM1×1 is generated as a random direction on
q = [U p]h
(cid:107)[U p]h(cid:107)2
SM1−1 and the elements of h ∈ R(r+1)×1 are sampled independently from N (0  1). The generated
vector q is close to U with high probability because the column-space of [U p] is close to the
column-space of U. The middle plot of Figure 2 shows the probability of accurate subspace recovery
versus the number of outliers. The number of evaluation runs was 50. One can observe that in contrast
to the unstructured outliers  the robust PCA methods tolerate few number of structured outliers.

5.3 Noisy Data

In this section  we consider the simultaneous presence of noise  the structured outliers and the
unstructured outliers. In this experiment  M1 = 100  r = 5  and ni = 100. The data contains 300
unstructured and 10 structured outliers. The distribution of the structured outliers follow Assumption 2
with η = 0.1. The vector q  the center of the cluster of the structured outliers  is generated as a
random direction on SM1−1. The generated data in this experiment can be expressed as D = [B An].
The matrix An = A + ζN where N represents the additive Gaussian noise  and ζ controls the power
of the additive noise. Deﬁne SNR =
. Since the data is noisy  the algorithms can not achieve
exact subspace recovery. Therefore  we examine the probability that an algorithm distinguishes all
the outliers correctly. Deﬁne vector f ∈ RM2×1 such that f (k) = (cid:107)(I − ˆU ˆUT )dk(cid:107)2. A trial is
considered successful if

(cid:107)A(cid:107)2
(cid:107)ζN(cid:107)2

F

F

(cid:19)

(cid:18)

(cid:19)

(cid:18)

max

{f (k) : k > no}

< min

{f (k) : k ≤ no}

.

The right plot of Figure 2 shows the probability of exact outlier detection versus SNR. It shows that
iSearch robustly distinguishes the outliers in the strong presence of noise. The number of evaluation
runs was 50.

5.4 Outlier Detection in Real Data

An application of the outlier detection methods is to identify the misclassiﬁed data points of a
clustering method [9 27]. In each identiﬁed cluster  the misclassiﬁed data points can be considered as
outliers. In this experiment  we assume an imaginary clustering method whose clustering error is
25 %. The robust PCA method is applied to each cluster to ﬁnd the misclassiﬁed data points. The
clustering is re-evaluated after identifying the misclassiﬁed data points. We use the Hopkins155

8

510152025no / M15101520ni / r5101520no00.20.40.60.81Probability of Accurate RecoveryiSearchOutlier PursuitFMSGMSCoP012345SNR00.20.40.60.81Probability of exact outlier detectioniSearchOutlier PursuitFMSGMSCoPdataset [33]  which contains data matrices with 2 or 3 clusters. In this experiment  27 matrices with
3 clusters are used (i.e.  the columns of each data matrix lie in 3 clusters). The outliers are linearly
dependent and they are very close to the span of the inliers since the clusters in the Hopkins155
dataset are close to each other. In addition  the inliers form a tight cluster. Evidently  the robust PCA
methods which assume that the outliers are randomly distributed fail in this task. This experiment
with real data contains most of the challenges that a robust PCA method can encounter. For more
details about this experiment  we refer the reader to [9  27].
Table 1 shows the average clustering error after applying the robust PCA methods to the output of the
clustering method. One can observe that iSearch signiﬁcantly outperforms the other methods. The
main reason is that iSearch is robust against outliers which are closed to U. In addition  the coherency
between the inliers enhances the performance of iSearch.

Table 1: Clustering error after using the robust PCA methods to detect the misclassiﬁed data points.

iSearch CoP

FMS

2 %

PCA
7 % 20.3 % 16.8 % 12.1 %

R1-PCA

5.5 Activity Detection in Real Noisy Data

In this experiment  we use the robust PCA methods to identify a rare event in a video ﬁle. We use
the Waving Tree video ﬁle [21]. In this video  a tree is smoothly waving and in the middle of the
video a person crosses the frame. The frames which only contain the background (the tree and the
environment) are inliers and the few frames corresponding to the event  the presence of the person 
are the outliers. Since the tree is waving  the inliers are noisy and we use r = 3 for all the methods.
In addition  we identify column d as outlier if (cid:107)d − ˆU ˆUd(cid:107)2/(cid:107)d(cid:107)2 ≥ 0.2 where ˆU is the recovered
subspace. In this experiments  the outliers are very similar to each other since the consecutive frames
are quite similar to each other. We use iSearch  CoP  FMS  and R1-PCA to detect the outlying frames.
iSearch  CoP  and FMS identiﬁed all the outlying frames correctly. R1-PCA could not identify those
frames in which the person does not move. The reason is that those frames are exactly similar to each
other. Figure 3 shows some of the outlying frames which is missed by R1-PCA.

+

Figure 3: Some of the frames of the Waving Tree video ﬁle. The highlighted frames are detected as
outliers by R1-PCA.

6 Conclusion

A new discovery about the applications of Innovation Search was presented. It was shown that
the directions of innovation can be utilized to measure the innovation of the data points and to
identify the outliers as the most innovative data points. In the robust PCA setting  the proposed
approach recovers the span of the inliers using the least innovative data points. It was shown that
iSearch can provably recover the span of the inliers with different models for the distribution of the
outliers including randomly distributed outliers  linearly dependent outliers  and clustered outliers. In
addition  analytical performance guarantees with clustered inliers were presented. The theoretical
and numerical results showed that ﬁnding the optimal directions makes iSearch signiﬁcantly robust
to the outliers which carry weak innovation. Moreover  the experiments with real and synthetic data
demonstrate the robustness of the proposed method against the strong presence of noise.

9

References

[1] Emmanuel J Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component

analysis? Journal of the ACM  58(3):11  2011.

[2] Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE Transactions on

Information Theory  51(12):4203–4215  2005.

[3] Venkat Chandrasekaran  Sujay Sanghavi  Pablo A Parrilo  and Alan S Willsky. Rank-sparsity
incoherence for matrix decomposition. SIAM Journal on Optimization  21(2):572–596  2011.
[4] Adam Charles  Ali Ahmed  Aditya Joshi  Stephen Conover  Christopher Turnes  and Mark
Davenport. Cleaning up toxic waste: removing nefarious contributions to recommendation
In IEEE International Conference on Acoustics  Speech and Signal Processing
systems.
(ICASSP)  pages 6571–6575  Vancouver  Canada  2013.

[5] Vartan Choulakian. L1-norm projection pursuit principal component analysis. Computational

Statistics & Data Analysis  50(6):1441–1451  2006.

[6] Chris Ding  Ding Zhou  Xiaofeng He  and Hongyuan Zha. R1-PCA: rotational invariant L1-
norm principal component analysis for robust subspace factorization. In Proceedings of the
23rd International Conference on Machine Learning (ICML)  pages 281–288  Pittsburgh  PA 
2006.

[7] Jiashi Feng  Huan Xu  and Shuicheng Yan. Robust PCA in high-dimension: A deterministic
approach. In Proceedings of the 29th International Conference on Machine Learning (ICML) 
Edinburgh  UK  2012.

[8] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. Communications of the
ACM  24(6):381–395  1981.

[9] Andrew Gitlin  Biaoshuai Tao  Laura Balzano  and John Lipor. Improving k-subspaces via

coherence pursuit. IEEE Journal of Selected Topics in Signal Processing  2018.

[10] Yoshiyuki Harada  Yoriyuki Yamagata  Osamu Mizuno  and Eun-Hye Choi. Log-based anomaly

detection of cps using a statistical method. arXiv:1701.03249  2017.

[11] Moritz Hardt and Ankur Moitra. Algorithms and hardness for robust subspace recovery. In The

26th Annual Conference on Learning Theory (COLT)  pages 354–375  Princeton  NJ  2013.

[12] Milos Hauskrecht  Iyad Batal  Michal Valko  Shyam Visweswaran  Gregory F Cooper  and
Gilles Clermont. Outlier detection for patient monitoring and alerting. Journal of Biomedical
Informatics  46(1):47–55  2013.

[13] Reinhard Heckel and Helmut Bölcskei. Robust subspace clustering via thresholding. IEEE

Transactions on Information Theory  61(11):6320–6342  2015.

[14] Seppo Karrila  Julian Hock Ean Lee  and Greg Tucker-Kellogg. A comparison of methods for
data-driven cancer outlier discovery  and an application scheme to semisupervised predictive
biomarker discovery. Cancer Informatics  10:109  2011.

[15] Qifa Ke and Takeo Kanade. Robust L1 norm factorization in the presence of outliers and
missing data by alternative convex programming. In 2005 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition (CVPR)  pages 739–746  San Diego  CA  2005.
[16] Christopher Kruegel and Giovanni Vigna. Anomaly detection of web-based attacks. In Proceed-
ings of the 10th ACM Conference on Computer and Communications Security (CCS)  pages
251–261  Washington  DC  2003.

[17] Gilad Lerman and Tyler Maunu. Fast  robust and non-convex subspace recovery. Information

and Inference: A Journal of the IMA  7(2):277–336  2017.

[18] Gilad Lerman and Tyler Maunu. An overview of robust subspace recovery. Proceedings of the

IEEE  106(8):1380–1410  2018.

[19] Gilad Lerman  Michael B McCoy  Joel A Tropp  and Teng Zhang. Robust computation of linear
models by convex relaxation. Foundations of Computational Mathematics  15(2):363–410 
2015.

10

[20] Guoying Li and Zhonglian Chen. Projection-pursuit approach to robust dispersion matrices and
principal components: primary theory and monte carlo. Journal of the American Statistical
Association  80(391):759–766  1985.

[21] Liyuan Li  Weimin Huang  Irene Yu-Hua Gu  and Qi Tian. Statistical modeling of com-
plex backgrounds for foreground object detection. IEEE Transactions on Image Processing 
13(11):1459–1472  2004.

[22] Xingguo Li and Jarvis Haupt. Identifying outliers in large matrices via randomized adaptive

compressive sampling. IEEE Transactions on Signal Processing  63(7):1792–1807  2015.

[23] Guangcan Liu and Ping Li. Recovery of coherent data via low-rank dictionary pursuit. In
Advances in Neural Information Processing Systems (NIPS)  pages 1206–1214  Montreal 
Canada  2014.

[24] Guangcan Liu and Ping Li. Low-rank matrix completion in the presence of high coherence.

IEEE Transactions on Signal Processing  64(21):5623–5633  2016.

[25] Panos P Markopoulos  George N Karystinos  and Dimitris A Pados. Optimal algorithms for
l1-subspace signal processing. IEEE Transactions on Signal Processing  62(19):5046–5058 
2014.

[26] Michael McCoy  Joel A Tropp  et al. Two proposals for robust PCA using semideﬁnite

programming. Electronic Journal of Statistics  5:1123–1160  2011.

[27] Mostafa Rahmani and George K Atia. Coherence pursuit: Fast  simple  and robust principal

component analysis. IEEE Transactions on Signal Processing  65(23):6260–6275  2017.

[28] Mostafa Rahmani and George K Atia.

Innovation pursuit: A new approach to subspace

clustering. IEEE Transactions on Signal Processing  65(23):6276–6291  2017.

[29] Mostafa Rahmani and George K Atia. Subspace clustering via optimal direction search. IEEE

Signal Processing Letters  24(12):1793–1797  2017.

[30] Mostafa Rahmani and Ping Li. Outlier detection and data clustering via innovation search.

Technical report  arXiv:1912.12988  2019.

[31] Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine Learning

Research  12:3413–3430  2011.

[32] Mahdi Soltanolkotabi and Emmanuel J Candes. A geometric analysis of subspace clustering

with outliers. The Annals of Statistics  pages 2195–2238  2012.

[33] Roberto Tron and René Vidal. A benchmark for the comparison of 3-d motion segmentation
algorithms. In 2007 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 1–8  Minneapolis  MN  2007.

[34] Manolis C Tsakiris and René Vidal. Dual principal component pursuit. In 2015 IEEE Interna-
tional Conference on Computer Vision Workshop  ICCV Workshops  pages 10–18  Santiago 
Chile  2015.

[35] Roman Vershynin.

Introduction to the non-asymptotic analysis of random matrices.

arXiv:1011.3027  2010.

[36] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust PCA via outlier pursuit. In
Advances in Neural Information Processing Systems (NIPS)  pages 2496–2504  Vancouver 
Canada  2010.

[37] Chong You  Daniel P Robinson  and René Vidal. Provable self-representation based outlier
detection in a union of subspaces. In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 4323–4332  2017.

[38] Teng Zhang. Robust subspace recovery by Tyler’s m-estimator. Information and Inference: A

Journal of the IMA  5(1):1–21  2016.

[39] Teng Zhang and Gilad Lerman. A novel m-estimator for robust PCA. The Journal of Machine

Learning Research  15(1):749–808  2014.

11

,Mostafa Rahmani
Ping Li