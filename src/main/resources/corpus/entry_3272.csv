2014,Log-Hilbert-Schmidt metric between positive definite operators on Hilbert spaces,This paper introduces a novel mathematical and computational framework  namely {\it Log-Hilbert-Schmidt metric} between positive definite operators on a Hilbert space. This is a generalization of the Log-Euclidean metric on the Riemannian manifold of positive definite matrices to the infinite-dimensional setting. The general framework is applied in particular to compute distances between covariance operators on a Reproducing Kernel Hilbert Space (RKHS)  for which we obtain explicit formulas via the corresponding Gram matrices. Empirically  we apply our formulation to the task of multi-category image classification  where each image is represented by an infinite-dimensional RKHS covariance operator. On several challenging datasets  our method significantly outperforms approaches based on covariance matrices computed directly on the original input features  including those using the Log-Euclidean metric  Stein and Jeffreys divergences  achieving new state of the art results.,Log-Hilbert-Schmidt metric between positive deﬁnite

operators on Hilbert spaces

H`a Quang Minh

Marco San Biagio

Vittorio Murino

{minh.haquang marco.sanbiagio vittorio.murino}@iit.it

Via Morego 30  Genova 16163  ITALY

Istituto Italiano di Tecnologia

Abstract

This paper introduces a novel mathematical and computational framework 
namely Log-Hilbert-Schmidt metric between positive deﬁnite operators on a
Hilbert space. This is a generalization of the Log-Euclidean metric on the Rie-
mannian manifold of positive deﬁnite matrices to the inﬁnite-dimensional setting.
The general framework is applied in particular to compute distances between co-
variance operators on a Reproducing Kernel Hilbert Space (RKHS)  for which we
obtain explicit formulas via the corresponding Gram matrices. Empirically  we
apply our formulation to the task of multi-category image classiﬁcation  where
each image is represented by an inﬁnite-dimensional RKHS covariance operator.
On several challenging datasets  our method signiﬁcantly outperforms approaches
based on covariance matrices computed directly on the original input features 
including those using the Log-Euclidean metric  Stein and Jeffreys divergences 
achieving new state of the art results.

1

Introduction and motivation

Symmetric Positive Deﬁnite (SPD) matrices  in particular covariance matrices  have been playing
an increasingly important role in many areas of machine learning  statistics  and computer vision 
with applications ranging from kernel learning [12]  brain imaging [9]  to object detection [24  23].
One key property of SPD matrices is the following. For a ﬁxed n ∈ N  the set of all SPD matrices
of size n × n is not a subspace in Euclidean space  but is a Riemannian manifold with nonpositive
curvature  denoted by Sym++(n). As a consequence of this manifold structure  computational
methods for Sym++(n) that simply rely on Euclidean metrics are generally suboptimal.
In the current literature  many methods have been proposed to exploit the non-Euclidean structure
of Sym++(n). For the purposes of the present work  we brieﬂy describe three common approaches
here  see e.g. [9] for other methods. The ﬁrst approach exploits the afﬁne-invariant metric  which
is the classical Riemannian metric on Sym++(n) [18  16  3  19  4  24]. The main drawback of this
framework is that it tends to be computationally intensive  especially for large scale applications.
Overcoming this computational complexity is one of the main motivations for the recent develop-
ment of the Log-Euclidean metric framework of [2]  which has been exploited in many computer
vision applications  see e.g. [25  11  17]. The third approach deﬁnes and exploits Bregman diver-
gences on Sym++(n)  such as Stein and Jeffreys divergences  see e.g. [12  22  8]  which are not
Riemannian metrics but are fast to compute and have been shown to work well on nearest-neighbor
retrieval tasks.
While each approach has its advantages and disadvantages  the Log-Euclidean metric possesses
several properties which are lacking in the other two approaches. First  it is faster to compute than
the afﬁne-invariant metric. Second  unlike the Bregman divergences  it is a Riemannian metric
on Sym++(n) and thus can better capture its manifold structure. Third  in the context of kernel

1

learning  it is straightforward to construct positive deﬁnite kernels  such as the Gaussian kernel 
using this metric. This is not always the case with the other two approaches: the Gaussian kernel
constructed with the Stein divergence  for instance  is only positive deﬁnite for certain choices of
parameters [22]  and the same is true with the afﬁne-invariant metric  as can be numerically veriﬁed.
Our contributions:
In this work  we generalize the Log-Euclidean metric to the inﬁnite-
dimensional setting  both mathematically  computationally  and empirically. Our novel metric 
termed Log-Hilbert-Schmidt metric (or Log-HS for short)  measures the distances between positive
deﬁnite unitized Hilbert-Schmidt operators  which are scalar perturbations of Hilbert-Schmidt oper-
ators on a Hilbert space and which are inﬁnite-dimensional generalizations of positive deﬁnite ma-
trices. These operators have recently been shown to form an inﬁnite-dimensional Riemann-Hilbert
manifold by [14  1  15]  who formulated the inﬁnite-dimensional version of the afﬁne-invariant
metric from a purely mathematical viewpoint. While our Log-Hilbert-Schmidt metric framework
includes the Log-Euclidean metric as a special case  the inﬁnite-dimensional formulation is signiﬁ-
cantly different from its corresponding ﬁnite-dimensional version  as we demonstrate throughout the
paper. In particular  one cannot obtain the inﬁnite-dimensional formulas from the ﬁnite-dimensional
ones by letting the dimension approach inﬁnity.
Computationally  we apply our abstract mathematical framework to compute distances between co-
variance operators on an RKHS induced by a positive deﬁnite kernel. From a kernel learning per-
spective  this is motivated by the fact that covariance operators deﬁned on nonlinear features  which
are obtained by mapping the original data into a high-dimensional feature space  can better cap-
ture input correlations than covariance matrices deﬁned on the original data. This is a viewpoint
that goes back to KernelPCA [21]. In our setting  we obtain closed form expressions for the Log-
Hilbert-Schmidt metric between covariance operators via the Gram matrices.
Empirically  we apply our framework to the task of multi-class image classiﬁcation. In our approach 
the original features extracted from each input image are implicitly mapped into the RKHS induced
by a positive deﬁnite kernel. The covariance operator deﬁned on the RKHS is then used as the rep-
resentation for the image and the distance between two images is the Log-Hilbert-Schmidt distance
between their corresponding covariance operators. On several challenging datasets  our method sig-
niﬁcantly outperforms approaches based on covariance matrices computed directly on the original
input features  including those using the Log-Euclidean metric  Stein and Jeffreys divergences.
Related work: The approach most closely related to our current work is [26]  which computed
probabilistic distances in RKHS. This approach has recently been employed by [10] to compute
Bregman divergences between RKHS covariance operators. There are two main theoretical issues
with the approach in [26  10]. The ﬁrst issue is that it is assumed implicitly that the concepts of
trace and determinant can be extended to any bounded linear operator on an inﬁnite-dimensional
Hilbert space H. This is not true in general  as the concepts of trace and determinant are only well-
deﬁned for certain classes of operators. Many quantities involved in the computation of the Bregman
divergences in [10] are in fact inﬁnite when dim(H) = ∞  which is the case if H is the Gaussian
RKHS  and only cancel each other out in special cases 1. The second issue concerns the use of
the Stein divergence by [10] to deﬁne the Gaussian kernel  which is not always positive deﬁnite  as
discussed above. In contrast  the Log-HS metric formulation proposed in this paper is theoretically
rigorous and it is straightforward to deﬁne many positive deﬁnite kernels  including the Gaussian
kernel  with this metric. Furthermore  our empirical results consistently outperform those of [10].
Organization: After some background material in Section 2  we describe the manifold of positive
deﬁnite operators in Section 3. Sections 4 and 5 form the core of the paper  where we develop the
general framework for the Log-Hilbert-Schmidt metric together with the explicit formulas for the
case of covariance operators on an RKHS. Empirical results for image classiﬁcation are given in
Section 6. The proofs for all mathematical results are given in the Supplementary Material.

2 Background

The Riemannian manifold of positive deﬁnite matrices: The manifold structure of Sym++(n)
has been studied extensively  both mathematically and computationally. This study goes as far

1We will provide a theoretically rigorous formulation for the Bregman divergences between positive deﬁnite

operators in a longer version of the present work.

2

back as [18]  for more recent treatments see e.g. [16  3  19  4]. The most commonly encountered
Riemannian metric on Sym++(n) is the afﬁne-invariant metric  in which the geodesic distance
between two positive deﬁnite matrices A and B is given by

d(A  B) = || log(A−1/2BA−1/2)||F  

(1)
where log denotes the matrix logarithm operation and F is an Euclidean norm on the space of
symmetric matrices Sym(n). Following the classical literature  in this work we take F to be the
Frobenious norm  which is induced by the standard inner product on Sym(n). From a practical
viewpoint  the metric (1) tends to be computationally intensive  which is one of the main motivations
for the Log-Euclidean metric of [2]  in which the geodesic distance between A and B is given by
(2)
The main goal of this paper is to generalize the Log-Euclidean metric to what we term the Log-
Hilbert-Schmidt metric between positive deﬁnite operators on an inﬁnite-dimensional Hilbert space
and apply this metric in particular to compute distances between covariance operators on an RKHS.
Covariance operators: Let the input space X be an arbitrary non-empty set. Let x = [x1  . . .   xm]
be a data matrix sampled from X   where m ∈ N is the number of observations. Let K be a
positive deﬁnite kernel on X × X and HK its induced reproducing kernel Hilbert space (RKHS).
Let Φ : X → HK be the corresponding feature map  which gives the (potentially inﬁnite) mapped
data matrix Φ(x) = [Φ(x1)  . . .   Φ(xm)] of size dim(HK) × m in the feature space HK. The
corresponding covariance operator for Φ(x) is deﬁned to be

dlogE(A  B) = || log(A) − log(B)||F .

Φ(x)JmΦ(x)T : HK → HK 

1
m

CΦ(x) =

m 1m1T

(3)
m with 1m = (1  . . .   1)T ∈ Rm.
where Jm is the centering matrix  deﬁned by Jm = Im − 1
The matrix Jm is symmetric  with rank(Jm) = m − 1  and satisﬁes J 2
m = Jm. The covariance
operator CΦ(x) can be viewed as a (potentially inﬁnite) covariance matrix in the feature space HK 
with rank at most m− 1. If X = Rn and K(x  y) = (cid:104)x  y(cid:105)Rn  then CΦ(x) = Cx  the standard n× n
covariance matrix encountered in statistics. 2
Regularization: Generally  covariance matrices may not be full-rank and thus may only be positive
semi-deﬁnite. In order to apply the theory of Sym++(n)  one needs to consider the regularized
version (Cx + γIRn) for some γ > 0. In the inﬁnite-dimensional setting  with dim(HK) = ∞ 
CΦ(x) is always rank-deﬁcient and regularization is always necessary. With γ > 0  (CΦ(x) + γIHK )
is strictly positive and invertible  both of which are needed to deﬁne the Log-Hilbert-Schmidt metric.

3 Positive deﬁnite unitized Hilbert-Schmidt operators
Throughout the paper  let H be a separable Hilbert space of arbitrary dimension. Let L(H) be
the Banach space of bounded linear operators on H and Sym(H) be the subspace of self-adjoint
operators in L(H). We ﬁrst describe in this section the manifold of positive deﬁnite unitized Hilbert-
Schmidt operators on which the Log-Hilbert-Schmidt metric is deﬁned. This manifold setting is
motivated by the following two crucial differences between the ﬁnite and inﬁnite-dimensional cases.
(A) Positive deﬁnite: If A ∈ Sym(H) and dim(H) = ∞  in order for log(A) to be well-deﬁned
and bounded  it is not sufﬁcient to require that all eigenvalues of A be strictly positive. Instead  it is
necessary to require that all eigenvalues of A be bounded below by a positive constant (Section 3.1).
(B) Unitized Hilbert-Schmidt: The inﬁnite-dimensional generalization of the Frobenious norm is the
Hilbert-Schmidt norm. However  if dim(H) = ∞  the identity operator I is not Hilbert-Schmidt and
would have inﬁnite distance from any Hilbert-Schmidt operator. To have a satisfactory framework 
it is necessary to enlarge the algebra of Hilbert-Schmidt operators to include I (Section 3.2).
These differences between the cases dim(H) = ∞ and dim(H) < ∞ are sharp and manifest
themselves in the concrete formulas for the Log-Hilbert-Schmidt metric which we obtain in Sections
In particular  the formulas for the case dim(H) = ∞ are not obtainable from their
4.2 and 5.
corresponding ﬁnite-dimensional versions when dim(H) → ∞.

2 One can also deﬁne CΦ(x) = 1

m−1 Φ(x)JmΦ(x)T . This should not make much practical difference if m

is large.

3

3.1 Positive deﬁnite operators

dim(H)(cid:88)
dim(H)(cid:88)

k=1

Positive and strictly positive operators: Let us discuss the ﬁrst crucial difference between the
ﬁnite and inﬁnite-dimensional settings. Recall that an operator A ∈ Sym(H) is said to be positive
if (cid:104)Ax  x(cid:105) ≥ 0 ∀x ∈ H. The eigenvalues of A  if they exist  are all nonnegative. If A is positive and
(cid:104)Ax  x(cid:105) = 0 ⇐⇒ x = 0  then A is said to be strictly positive  and all its eigenvalues are positive. We
denote the sets of all positive and strictly positive operators on H  respectively  by Sym+(H) and
Sym++(H). Let A ∈ Sym++(H). Assume that A is compact  then A has a countable spectrum of
positive eigenvalues {λk(A)}dim(H)
  counting multiplicities  with limk→∞ λk(A) = 0 if dim(H) =
∞. Let {φk(A)}dim(H)

denote the corresponding normalized eigenvectors  then

k=1

k=1

(4)
where φk(A) ⊗ φk(A) : H → H is deﬁned by (φk(A) ⊗ φk(A))w = (cid:104)w  φk(A)(cid:105)φk(A)  w ∈ H.
The logarithm of A is deﬁned by

A =

λk(A)φk(A) ⊗ φk(A) 

log(λk(A))φk(A) ⊗ φk(A).

k=1

log(A) =

(5)
Clearly  log(A) is bounded if and only if dim(H) < ∞  since for dim(H) = ∞  we have
limk→∞ log(λk(A)) = −∞. Thus  when dim(H) = ∞  the condition that A be strictly positive is
not sufﬁcient for log(A) to be bounded. Instead  the following stronger condition is necessary.
Positive deﬁnite operators: A self-adjoint operator A ∈ L(H) is said to be positive deﬁnite (see
e.g. [20]) if there exists a constant MA > 0 such that
(cid:104)Ax  x(cid:105) ≥ MA||x||2

(6)
The eigenvalues of A  if they exist  are bounded below by MA. This condition is equivalent to
requiring that A be strictly positive and invertible  with A−1 ∈ L(H). Clearly  if dim(H) < ∞ 
then strict positivity is equivalent to positive deﬁniteness. Let P(H) denote the open cone of self-
adjoint  positive deﬁnite  bounded operators on H  that is
Throughout the remainder of the paper  we use the following notation: A > 0 ⇐⇒ A ∈ P(H).

P(H) = {A ∈ L(H)  A∗ = A ∃MA > 0 s.t. (cid:104)Ax  x(cid:105) ≥ MA||x||2 ∀x ∈ H}.

for all x ∈ H.

(7)

3.2 The Riemann-Hilbert manifold of positive deﬁnite unitized Hilbert-Schmidt operators
Let HS(H) denote the two-sided ideal of Hilbert-Schmidt operators on H in L(H)  which is a
Banach algebra with the Hilbert-Schmidt norm  deﬁned by

||A||2

HS = tr(A∗A) =

λk(A∗A).

(8)

dim(H)(cid:88)

k=1

We now discuss the second crucial difference between the ﬁnite and inﬁnite-dimensional settings. If
dim(H) = ∞  then the identity operator I is not Hilbert-Schmidt  since ||I||HS = ∞. Thus  given
γ (cid:54)= µ > 0  we have || log(γI) − log(µI)||HS = | log(γ) − log(µ)| ||I||HS = ∞  that is even the
distance between two different multiples of the identity operator is inﬁnite. This problem is resolved
by considering the following extended (or unitized) Hilbert-Schmidt algebra [14  1  15]:

HR = {A + γI : A∗ = A  A ∈ HS(H)  γ ∈ R}.

(9)

This can be endowed with the extended Hilbert-Schmidt inner product

(cid:104)A + γI  B + µI(cid:105)eHS = tr(A∗B) + γµ = (cid:104)A  B(cid:105)HS + γµ 

(10)
under which the scalar operators are orthogonal to the Hilbert-Schmidt operators. The corresponding
extended Hilbert-Schmidt norm is given by
eHS = ||A||2

where A ∈ HS(H).

||(A + γI)||2

HS + γ2 

If dim(H) < ∞  then we set || ||eHS = || ||HS  with ||(A + γI)||eHS = ||A + γI||HS.
Manifold of positive deﬁnite unitized Hilbert-Schmidt operators: Deﬁne

Σ(H) = P(H) ∩ HR = {A + γI > 0 : A∗ = A  A ∈ HS(H)  γ ∈ R}.

(12)
If (A + γI) ∈ Σ(H)  then it has a countable spectrum {λk(A) + γ}dim(H)
satisfying λk + γ ≥ MA
for some constant MA > 0. Thus (A + γI)−1 exists and is bounded  and log(A + γI) as deﬁned
by (5) is well-deﬁned and bounded  with log(A + γI) ∈ HR.

(11)

k=1

4

The main results of [15] state that when dim(H) = ∞  Σ(H) is an inﬁnite-dimensional Riemann-
Hilbert manifold and the map log : Σ(H) → HR and its inverse exp : HR → Σ(H) are diffeomor-
phisms. The Riemannian distance between two operators (A + γI)  (B + µI) ∈ Σ(H) is given by
(13)

d[(A + γI)  (B + µI)] = || log[(A + γI)−1/2(B + µI)(A + γI)−1/2]||eHS.

This is the inﬁnite-dimensional version of the afﬁne-invariant metric (1) 3.

4 Log-Hilbert-Schmidt metric

This section deﬁnes and develops the Log-Hilbert-Schmidt metric  which is the inﬁnite-dimensional
generalization of the Log-Euclidean metric (2). The general formulation presented in this section is
then applied to RKHS covariance operators in Section 5.

4.1 The general setting
Consider the following operations on Σ(H):

(A + γI) (cid:12) (B + µI) = exp(log(A + γI) + log(B + µI)) 

λ(cid:16) (A + γI) = exp(λ log(A + γI)) = (A + γI)λ  λ ∈ R.

(14)
(15)
Vector space structure on Σ(H): The key property of the operation (cid:12) is that  unlike the usual
space  which is isomorphic to the vector space (HR  + ·)  as shown by the following.

operator product  it is commutative  making (Σ(H) (cid:12)) an abelian group and (Σ(H) (cid:12) (cid:16)) a vector
Theorem 1. Under the two operations (cid:12) and(cid:16)  (Σ(H) (cid:12) (cid:16)) becomes a vector space  with (cid:12)
acting as vector addition and(cid:16) acting as scalar multiplication. The zero element in (Σ(H) (cid:12) (cid:16))

is the identity operator I and the inverse of (A + γI) is (A + γI)−1. Furthermore  the map

ψ : (Σ(H) (cid:12) (cid:16)) → (HR  + ·) deﬁned by ψ(A + γI) = log(A + γI) 

is a vector space isomorphism  so that for all (A + γI)  (B + µI) ∈ Σ(H) and λ ∈ R 

(16)

(17)

ψ((A + γI) (cid:12) (B + µI)) = log(A + γI) + log(B + µI) 

ψ(λ(cid:16) (A + γI)) = λ log(A + γI) 

where + and · denote the usual operator addition and multiplication operations  respectively.
Metric space structure on Σ(H): Motivated by the vector space isomorphism between

(Σ(H) (cid:12) (cid:16)) and (HR  + ·) via the mapping ψ  the following is our generalization of the Log-

Euclidean metric to the inﬁnite-dimensional setting.
Deﬁnition 1. The Log-Hilbert-Schmidt distance between two operators (A + γI) ∈ Σ(H)  (B +
µI) ∈ Σ(H) is deﬁned to be

dlogHS[(A + γI)  (B + µI)] =(cid:13)(cid:13)log[(A + γI) (cid:12) (B + µI)−1](cid:13)(cid:13)eHS .

based on the one-to-one correspondence between the algebraic structures of (Σ(H) (cid:12) (cid:16)) and

(18)
Remark 1. For our purposes in the current work  we focus on the Log-HS metric as deﬁned above
(HR  + ·). An in-depth treatment of the Log-HS metric in connection with the manifold structure of
Σ(H) will be provided in a longer version of the paper.
The following theorem shows that the Log-Hilbert-Schmidt distance satisﬁes all the axioms of a met-
ric  making (Σ(H)  dlogHS) a metric space. Furthermore  the square Log-Hilbert-Schmidt distance
decomposes uniquely into a sum of a square Hilbert-Schmidt norm plus a scalar term.
Theorem 2. The Log-Hilbert-Schmidt distance as deﬁned in (18)
is a metric  making
(Σ(H)  dlogHS) a metric space. Let (A + γI) ∈ Σ(H)  (B + µI) ∈ Σ(H). If dim(H) = ∞ 
then there exist unique operators A1  B1 ∈ HS(H) ∩ Sym(H) and scalars γ1  µ1 ∈ R such that

A + γI = exp(A1 + γ1I)  B + µI = exp(B1 + µ1I) 
HS + (γ1 − µ1)2.
logHS[(A + γI)  (B + µI)] = (cid:107)A1 − B1(cid:107)2
d2

and
(20)
If dim(H) < ∞  then (19) and (20) hold with A1 = log(A + γI)  B1 = log(B + µI)  γ1 = µ1 = 0.

(19)

3We give a more detailed discussion of Eqs. (12) and (13) in the Supplementary Material.

5

Log-Euclidean metric: Theorem 2 states that when dim(H) < ∞  we have dlogHS[(A + γI)  (B +
µI)] = dlogE[(A + γI)  (B + µI)]. We have thus recovered the Log-Euclidean metric as a special
case of our framework.

Hilbert space structure on (Σ(H) (cid:12) (cid:16)): Motivated by formula (20)  whose right hand side is a
square extended Hilbert-Schmidt distance  we now show that (Σ(H) (cid:12) (cid:16)) can be endowed with

an inner product  under which it becomes a Hilbert space.
Deﬁnition 2. Let (A + γI)  (B + µI) ∈ Σ(H). Let A1  B1 ∈ HS(H)∩ Sym(H) and γ1  µ1 ∈ R be
the unique operators and scalars  respectively  such that A + γI = exp(A1 + γ1I) and B + µI =
exp(B1 + µ1I)  as in Theorem 2. The Log-Hilbert-Schmidt inner product between (A + γI) and
(B + µI) is deﬁned by

(cid:104)A + γI  B + µI(cid:105)logHS = (cid:104)log(A + γI)  log(B + µI)(cid:105)eHS = (cid:104)A1  B1(cid:105)HS + γ1µ1.

Theorem 3. The inner product (cid:104)   (cid:105)logHS as given in (21) is well-deﬁned on (Σ(H) (cid:12) (cid:16)). En-
dowed with this inner product  (Σ(H) (cid:12) (cid:16) (cid:104)   (cid:105)logHS) becomes a Hilbert space. The correspond-

ing Log-Hilbert-Schmidt norm is given by

(21)

In terms of this norm  the Log-Hilbert-Schmidt distance is given by

||A + γI||2

logHS = || log(A + γI)||2

dlogHS[(A + γI)  (B + µI)] =(cid:13)(cid:13)(A + γI) (cid:12) (B + µI)−1(cid:13)(cid:13)logHS .

eHS = ||A1||2

HS + γ2
1 .

(22)

(23)

(24)
(25)

quence of the Hilbert space structure of (Σ(H) (cid:12) (cid:16) (cid:104)   (cid:105)logHS) is that it is straightforward to

Positive deﬁnite kernels deﬁned with the Log-Hilbert-Schmidt metric: An important conse-
generalize many positive deﬁnite kernels on Euclidean space to Σ(H) × Σ(H).
Corollary 1. The following kernels deﬁned on Σ(H) × Σ(H) are positive deﬁnite:

K[(A + γI)  (B + µI)] = (c + (cid:104)A + γI  B + µI(cid:105)logHS)d 

c > 0  d ∈ N 
logHS[(A + γI)  (B + µI)]/σ2)  0 < p ≤ 2.

K[(A + γI)  (B + µI)] = exp(−dp

4.2 Log-Hilbert-Schmidt metric between regularized positive operators
For our purposes in the present work  we focus on the following subset of Σ(H):

Σ+(H) = {A + γI : A ∈ HS(H) ∩ Sym+(H)   γ > 0} ⊂ Σ(H).

(26)
Examples of operators in Σ+(H) are the regularized covariance operators (CΦ(x) + γI) with γ > 0.
In this case the formulas in Theorems 2 and 3 have the following concrete forms.
Theorem 4. Assume that dim(H) = ∞. Let A  B ∈ HS(H) ∩ Sym+(H). Let γ  µ > 0. Then

logHS[(A + γI)  (B + µI)] = || log(
d2

1
γ

A + I) − log(

1
µ

B + I)||2

HS + (log γ − log µ)2.

(27)

Their Log-Hilbert-Schmidt inner product is given by

1
γ

(cid:104)(A + γI)  (B + µI)(cid:105)logHS = (cid:104)log(

A + I)  log(

(28)
Finite dimensional case: As a consequence of the differences between the cases dim(H) < ∞ and
dim(H) = ∞  we have different formulas for the case dim(H) < ∞  which depend on dim(H)
and which are surprisingly more complicated than in the case dim(H) = ∞.
Theorem 5. Assume that dim(H) < ∞. Let A  B ∈ Sym+(H). Let γ  µ > 0. Then
+ I)||2

logHS[(A + γI)  (B + µI)] = || log(
d2

+ I) − log(

B + I)(cid:105)HS + (log γ)(log µ).

1
µ

HS

A
γ

B
µ

+2(log γ − log µ)tr[log(

A
γ

+ I) − log(

B
µ

+ I)] + (log γ − log µ)2 dim(H).

(29)

The Log-Hilbert-Schmidt inner product between (A + γI) and (B + µI) is given by
+ I)(cid:105)HS
+ I)] + (log γ log µ) dim(H).

(cid:104)(A + γI)  (B + µI)(cid:105)logHS = (cid:104)log(
B
µ

+ I)] + (log µ)tr[log(

+(log γ)tr[log(

+ I)  log(

B
µ

A
γ

A
γ

(30)

6

5 Log-Hilbert-Schmidt metric between regularized covariance operators
Let X be an arbitrary non-empty set. In this section  we apply the general results of Section 4 to
compute the Log-Hilbert-Schmidt distance between covariance operators on an RKHS induced by a
positive deﬁnite kernel K on X ×X . In this case  we have explicit formulas for dlogHS and the inner
product (cid:104)   (cid:105)logHS via the corresponding Gram matrices. Let x = [xi]m
i=1  m ∈ N 
be two data matrices sampled from X and CΦ(x)  CΦ(y) be the corresponding covariance operators
induced by the kernel K  as deﬁned in Section 2. Let K[x]  K[y]  and K[x  y] be the m × m
Gram matrices deﬁned by (K[x])ij = K(xi  xj)  (K[y])ij = K(yi  yj)  (K[x  y])ij = K(xi  yj) 
1 ≤ i  j ≤ m. Let A = 1√

γm Φ(x)Jm : Rm → HK  B = 1√

i=1  y = [yi]m

µm Φ(y)Jm : Rm → HK  so that
JmK[x  y]Jm.

1√
γµm

(31)

AT A =

1
γm

JmK[x]Jm  BT B =

1
µm

JmK[y]Jm  AT B =

Let NA and NB be the numbers of nonzero eigenvalues of AT A and BT B  respectively. Let ΣA
and ΣB be the diagonal matrices of size NA × NA and NB × NB  and UA and UB be the matrices
of size m × NA and m × NB  respectively  which are obtained from the spectral decompositions
(32)

JmK[y]Jm = UBΣBU T
B .
In the following  let ◦ denote the Hadamard (element-wise) matrix product. Deﬁne

JmK[x]Jm = UAΣAU T
A  

1
µm

1
γm

CAB = 1T

NA

log(INA + ΣA)Σ−1

A (U T

A AT BUB ◦ U T

A AT BUB)Σ−1

Theorem 6. Assume that dim(HK) = ∞. Let γ > 0  µ > 0. Then

B log(INB + ΣB)1NB .

d2
logHS[(CΦ(x) + γI)  (CΦ(y) + µI)] = tr[log(INA + ΣA)]2 + tr[log(INB + ΣB)]2
−2CAB + (log γ − log µ)2.

The Log-Hilbert-Schmidt inner product between (CΦ(x) + γI) and (CΦ(y) + µI) is

(cid:104)(CΦ(x) + γI)  (CΦ(y) + µI)(cid:105)logHS = CAB + (log γ)(log µ).

Theorem 7. Assume that dim(HK) < ∞. Let γ > 0  µ > 0. Then
logHS[(CΦ(x) + γI)  (CΦ(y) + µI)] = tr[log(INA + ΣA)]2 + tr[log(INB + ΣB)]2 − 2CAB
d2

(33)

(34)

(35)

+2(log

γ
µ

)(tr[log(INA + ΣA)] − tr[log(INB + ΣB)]) + (log

)2 dim(HK). (36)

γ
µ

The Log-Hilbert-Schmidt inner product between (CΦ(x) + γI) and (CΦ(y) + µI) is
(cid:104)(CΦ(x) + γI)  (CΦ(y) + µI)(cid:105)logHS = CAB + (log µ)tr[log(INA + ΣA)]
+(log γ)tr[log(INB + ΣB)] + (log γ log µ) dim(HK).

(37)

6 Experimental results

This section demonstrates the empirical performance of the Log-HS metric on the task of multi-
category image classiﬁcation. For each input image  the original features extracted from the image
are implicitly mapped into the inﬁnite-dimensional RKHS induced by the Gaussian kernel. The co-
variance operator deﬁned on the RKHS is called the GaussianCOV and is used as the representation
for the image. In a classiﬁcation algorithm  the distance between two images is the Log-HS distance
between their corresponding GaussianCOVs. This is compared with the directCOV representation 
that is covariance matrices deﬁned using the original input features. In all of the experiments  we
employed LIBSVM [7] as the classiﬁcation method. The following algorithms were evaluated in
our experiments: Log-E (directCOV and Gaussian SVM using the Log-Euclidean metric)  Log-HS
(GaussianCOV and Gaussian SVM using the Log-HS metric)  Log-HS∆ (GaussianCOV and SVM
with the Laplacian kernel K(x  y) = exp(−||x−y||
)). For all experiments  the kernel parameters
were chosen by cross validation  while the regularization parameters were ﬁxed to be γ = µ = 10−8.
We also compare with empirical results by the different algorithms in [10]  namely J-SVM and S-
SVM (SVM with the Jeffreys and Stein divergences between directCOVs  respectively)  JH-SVM
and SH-SVM (SVM with the Jeffreys and Stein divergences between GaussianCOVs  respectively) 
and results of the Covariance Discriminant Learning (CDL) technique of [25]  which can be consid-
ered as the state-of-the-art for COV-based classiﬁcation. All results are reported in Table1.

σ

7

Table 1: Results over all the datasets

KTH-TIPS2b (RGB)

Fish

Methods

Log-HS
Log-HS∆

SH-SVM[10]
JH-SVM[10]

Log-E

S-SVM[10]
J-SVM[10]
CDL [25]

Kylberg texture
92.58%(±1.23)
92.56%(±1.26)
91.36%(±1.27)
91.25%(±1.33)
87.49%(±1.54)
81.27%(±1.07)
82.19%(±1.30)
79.87%(±1.06)

KTH-TIPS2b
81.91%(±3.3)
81.50%(±3.90)
80.10%(±4.60)
79.90%(±3.80)
74.11%(±7.41)
78.30%(±4.84)
74.70%(±2.81)
76.30%(±5.10)

V
O
C
n
a
i
s
s
u
a
G

V
O
C
t
c
e
r
i
d

-
-

-
-
-

79.94%(±4.6)
77.53%(±5.2)

56.74%(±2.87)
56.43%(±3.02)

74.13%(±6.1)

42.70%(±3.45)

-
-

-
-
-

(cid:12)(cid:12)(cid:3)  where
(cid:12)(cid:12) are the 20 Gabor ﬁlters at 4 orientations and 5

(cid:12)(cid:12)   . . .(cid:12)(cid:12)G4 5

x y

x y

x y

Texture classiﬁcation: For this task  we used the Kylberg texture dataset [13]  which contains
28 texture classes of different natural and man-made surfaces  with each class consisting of 160
images. For this dataset  we followed the validation protocol of [10]  where each image is resized
to a dimension of 128 × 128  with m = 1024 observations computed on a coarse grid (i.e.  every
4 pixels in the horizontal and vertical direction). At each point  we extracted a set of n = 5 low-
level features F(x  y) = [Ix y |Ix|  |Iy|  |Ixx|  |Iyy|]   where I  Ix  Iy  Ixx and Iyy  are the intensity 
ﬁrst- and second-order derivatives of the texture image. We randomly selected 5 images in each class
for training and used the remaining ones as test data  repeating the entire procedure 10 times. We
report the mean and the standard deviation values for the classiﬁcation accuracies for the different
experiments over all 10 random training/testing splits.
Material classiﬁcation: For this task  we used the KTH-TIPS2b dataset [6]  which contains images
of 11 materials captured under 4 different illuminations  in 3 poses  and at 9 scales. The total number
of images per class is 108. We applied the same protocol as used for the previous dataset [10] 

extracting 23 low-level dense features: F(x  y) = (cid:2)Rx y  Gx y  Bx y (cid:12)(cid:12)G0 0
Rx y  Gx y  Bx y are the color intensities and(cid:12)(cid:12)Go s

scales. We report the mean and the standard deviation values for all the 4 splits of the dataset.
Fish recognition: The third dataset used is the Fish Recognition dataset [5]. The ﬁsh data are
acquired from a live video dataset resulting in 27370 veriﬁed ﬁsh images. The whole dataset is
divided into 23 classes. The number of images per class ranges from 21 to 12112  with a medium
resolution of roughly 150 × 120 pixels. The signiﬁcant variations in color  pose and illumination
inside each class make this dataset very challenging. We apply the same protocol as used for the
previous datasets  extracting the 3 color intensities from each image to show the effectiveness of our
method: F(x  y) = [Rx y  Gx y  Bx y]. We randomly selected 5 images from each class for training
and 15 for testing  repeating the entire procedure 10 times.
Discussion of results: As one can observe in Table1  in all of the datasets  the Log-HS framework 
operating on GaussianCOVs  signiﬁcantly outperforms approaches based on directCOVs computed
using the original input features  including those using Log-Euclidean  Stein and Jeffreys diver-
gences. Across all datasets  our improvement over the Log-Euclidean metric is up to 14% in ac-
curacy. This is consistent with kernel-based learning theory  because GaussianCOVs  deﬁned on
the inﬁnite-dimensional RKHS  can better capture nonlinear input correlations than directCOVs  as
we expected. To the best of our knowledge  our results in the Texture and Material classiﬁcation
experiments are the new state of the art results for these datasets. Furthermore  our results  which
are obtained using a theoretically rigorous framework  also consistently outperform those of [10].
The computational complexity of our framework  its two-layer kernel machine interpretation  and
other discussions are given in the Supplementary Material.
Conclusion and future work
We have presented a novel mathematical and computational framework  namely Log-Hilbert-
Schmidt metric  that generalizes the Log-Euclidean metric between SPD matrices to the inﬁnite-
dimensional setting. Empirically  on the task of image classiﬁcation  where each image is repre-
sented by an inﬁnite-dimensional RKHS covariance operator  the Log-HS framework substantially
outperforms other approaches based on covariance matrices computed directly on the original input
features. Given the widespread use of covariance matrices  we believe that the Log-HS framework
can be potentially useful for many problems in machine learning  computer vision  and other ap-
plications. Many more properties of the Log-HS metric  along with further applications  will be
reported in a longer version of the current paper and in future work.

8

References
[1] E. Andruchow and A. Varela. Non positively curved metric in the space of positive deﬁnite inﬁnite

matrices. Revista de la Union Matematica Argentina  48(1):7–15  2007.

[2] V. Arsigny  P. Fillard  X. Pennec  and N. Ayache. Geometric means in a novel vector space structure on

symmetric positive-deﬁnite matrices. SIAM J. on Matrix An. and App.  29(1):328–347  2007.

[3] R. Bhatia. Positive Deﬁnite Matrices. Princeton University Press  2007.
[4] D. A. Bini and B. Iannazzo. Computing the Karcher mean of symmetric positive deﬁnite matrices. Linear

Algebra and its Applications  438(4):1700–1710  2013.

[5] B. J. Boom  J. He  S. Palazzo  P. X. Huang  C. Beyan  H.-M. Chou  F.-P. Lin  C. Spampinato  and R. B.
Fisher. A research tool for long-term and continuous analysis of ﬁsh assemblage in coral-reefs using
underwater camera footage. Ecological Informatics  in press  2013.

[6] B. Caputo  E. Hayman  and P. Mallikarjuna. Class-speciﬁc material categorisation.

1597–1604  2005.

In ICCV  pages

[7] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Trans. Intell. Syst.

Technol.  2(3):27:1–27:27  May 2011.

[8] A. Cherian  S. Sra  A. Banerjee  and N. Papanikolopoulos. Jensen-Bregman LogDet divergence with

application to efﬁcient similarity search for covariance matrices. TPAMI  35(9):2161–2174  2013.

[9] I.L. Dryden  A. Koloydenko  and D. Zhou. Non-Euclidean statistics for covariance matrices  with appli-

cations to diffusion tensor imaging. Annals of Applied Statistics  3:1102–1123  2009.

[10] M. Harandi  M. Salzmann  and F. Porikli. Bregman divergences for inﬁnite dimensional covariance

matrices. In CVPR  2014.

[11] S. Jayasumana  R. Hartley  M. Salzmann  Hongdong Li  and M. Harandi. Kernel methods on the Rieman-

nian manifold of symmetric positive deﬁnite matrices. In CVPR  2013.

[12] B. Kulis  M. A. Sustik  and I. S. Dhillon. Low-rank kernel learning with Bregman matrix divergences.

The Journal of Machine Learning Research  10:341–376  2009.

[13] G. Kylberg. The Kylberg texture dataset v. 1.0. External report (Blue series) 35  Centre for Image

Analysis  Swedish University of Agricultural Sciences and Uppsala University  2011.

[14] G. Larotonda. Geodesic Convexity  Symmetric Spaces and Hilbert-Schmidt Operators. PhD thesis  Uni-

versidad Nacional de General Sarmiento  Buenos Aires  Argentina  2005.

[15] G. Larotonda. Nonpositive curvature: A geometrical approach to Hilbert–Schmidt operators. Differential

Geometry and its Applications  25:679–700  2007.

[16] J. D. Lawson and Y. Lim. The geometric mean  matrices  metrics  and more. The American Mathematical

Monthly  108(9):797–812  2001.

[17] P. Li  Q. Wang  W. Zuo  and L. Zhang. Log-Euclidean kernels for sparse representation and dictionary

learning. In ICCV  2013.

[18] G.D. Mostow. Some new decomposition theorems for semi-simple groups. Memoirs of the American

Mathematical Society  14:31–54  1955.

[19] X. Pennec  P. Fillard  and N. Ayache. A Riemannian framework for tensor computing.

Journal of Computer Vision  66(1):41–66  2006.

International

[20] W.V. Petryshyn. Direct and iterative methods for the solution of linear operator equations in Hilbert

spaces. Transactions of the American Mathematical Society  105:136–175  1962.

[21] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue problem.

Neural Comput.  10(5)  July 1998.

[22] S. Sra. A new metric on the manifold of kernel matrices with application to matrix geometric means. In

NIPS  2012.

[23] D. Tosato  M. Spera  M. Cristani  and V. Murino. Characterizing humans on Riemannian manifolds.

TPAMI  35(8):1972–1984  Aug 2013.

[24] O. Tuzel  F. Porikli  and P. Meer. Pedestrian detection via classiﬁcation on Riemannian manifolds. TPAMI 

30(10):1713–1727  2008.

[25] R. Wang  H. Guo  L. S. Davis  and Q. Dai. Covariance discriminative learning: A natural and efﬁcient

approach to image set classiﬁcation. In CVPR  pages 2496–2503  2012.

[26] S. K. Zhou and R. Chellappa. From sample similarity to ensemble similarity: Probabilistic distance

measures in reproducing kernel Hilbert space. TPAMI  28(6):917–929  2006.

9

,Minh Ha Quang
Marco San Biagio
Vittorio Murino
Huan Li
Zhouchen Lin