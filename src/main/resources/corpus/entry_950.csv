2019,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components,Abstract Neural networks are state-of-the-art classification approaches but are generally difficult to interpret. This issue can be partly alleviated by constructing a precise decision process within the neural network. In this work  a network architecture  denoted as Classification-By-Components network (CBC)  is proposed. It is restricted to follow an intuitive reasoning based decision process inspired by Biederman's recognition-by-components theory from cognitive psychology. The network is trained to learn and detect generic components that characterize objects. In parallel  a class-wise reasoning strategy based on these components is learned to solve the classification problem. In contrast to other work on reasoning  we propose three different types of reasoning: positive  negative  and indefinite. These three types together form a probability space to provide a probabilistic classifier. The decomposition of objects into generic components combined with the probabilistic reasoning provides by design a clear interpretation of the classification decision process. The evaluation of the approach on MNIST shows that CBCs are viable classifiers. Additionally  we demonstrate that the inherent interpretability offers a profound understanding of the classification behavior such that we can explain the success of an adversarial attack. The method's scalability is successfully tested using the ImageNet dataset.,Classiﬁcation-by-Components: Probabilistic

Modeling of Reasoning over a Set of Components

Sascha Saralajew1 ∗ Lars Holdijk1 ∗ Maike Rees1 Ebubekir Asan1 Thomas Villmann2 ∗

1Dr. Ing. h.c. F. Porsche AG  Weissach  Germany 

sascha.saralajew@porsche.de

2University of Applied Sciences Mittweida  Mittweida  Germany 

thomas.villmann@hs-mittweida.de

Abstract

Neural networks are state-of-the-art classiﬁcation approaches but are generally
difﬁcult to interpret. This issue can be partly alleviated by constructing a pre-
cise decision process within the neural network. In this work  a network archi-
tecture  denoted as Classiﬁcation-By-Components network (CBC)  is proposed.
It is restricted to follow an intuitive reasoning based decision process inspired
by BIEDERMAN’s recognition-by-components theory from cognitive psychology.
The network is trained to learn and detect generic components that characterize
objects. In parallel  a class-wise reasoning strategy based on these components is
learned to solve the classiﬁcation problem. In contrast to other work on reasoning 
we propose three different types of reasoning: positive  negative  and indeﬁnite.
These three types together form a probability space to provide a probabilistic clas-
siﬁer. The decomposition of objects into generic components combined with the
probabilistic reasoning provides by design a clear interpretation of the classiﬁ-
cation decision process. The evaluation of the approach on MNIST shows that
CBCs are viable classiﬁers. Additionally  we demonstrate that the inherent inter-
pretability offers a profound understanding of the classiﬁcation behavior such that
we can explain the success of an adversarial attack. The method’s scalability is
successfully tested using the IMAGENET dataset.

1

Introduction

Neural Networks (NNs) dominate the ﬁeld of machine learning in terms of image classiﬁcation
accuracy. Due to their design  considered as black boxes  it is however hard to gain insights into
their decision making process and to interpret why they sometimes behave unexpectedly. In general 
the interpretability of NNs is under controversial discussion [1–4] and pushed researchers to new
methods to improve the weaknesses [5–7]. This is also highlighted in the topic of robustness of NNs
against adversarial examples [8]. Prototype-based classiﬁers like Learning Vector Quantizers [9  10]
are more interpretable and can provide insights into their classiﬁcation processes. Unfortunately 
they are still hindered by their low base accuracies.
The method proposed in this work aims to answer the question of interpretability by drawing in-
spirations from BIEDERMAN’s theory recognition-by-components [11] from the ﬁeld of cognitive
psychology. Roughly speaking  BIEDERMAN’s theory describes how humans recognize complex
objects by assuming that objects can be decomposed into generic parts that operate as structural
primitives  called components. Objects are then classiﬁed by matching the extracted decomposition
plan with a class Decomposition Plan (DP) for each potential object class. Intuitively  the class DPs

∗Authors contributed equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: An example realization of the classiﬁcation process of a CBC on a digit classiﬁcation task.
For simplicity  we illustrate a discrete case where “1” corresponds to detection / positive reasoning 
“0” to no detection / negative reasoning  and “(cid:2)” to indeﬁnite reasoning.

describe which components are important to be detected and which components are important to not
be detected for an object to belong to a speciﬁc class. For example  if we consider the classiﬁcation
of a digit as illustrated in Fig. 1  the detection of a component representing a vertical bar provides
evidence in favor of the class 1. In other words  we reason positively over the vertical bar compo-
nent for the class 1. Similarly  we can reason negatively over all curved components. In contrast
to other work on reasoning  the presented approach extends these two intuitive reasoning states by
a third type considering indeﬁnite reasoning. In Fig. 1  not all components will be important for
the recognition of a 1. For instance  we reason neither positively nor negatively over the serif and
bottom stroke because not all writing styles use them. In Sec. 2  a network architecture is introduced
that models the described classiﬁcation process in an end-to-end trainable framework such that the
components as well as the class DPs can be learned. In line with BIEDERMAN’s theory  we call this
a Classiﬁcation-By-Components network (CBC).
In summary  the contribution of this paper is a classiﬁcation method  called CBC  with the fol-
lowing important characteristics: (1) The method classiﬁes its input by applying positive  negative 
and indeﬁnite reasoning over an extracted DP. To the best of our knowledge  this is the ﬁrst time
that optionality of components / features is explicitly modeled. (2) The method uses a probabilis-
tic reasoning process that directly outputs class hypothesis probabilities without requiring heuristic
squashing methods such as softmax. (3) The reasoning process is easily interpretable and simpliﬁes
the understanding of the classiﬁcation decision. (4) The method retains advantages of NNs such as
being end-to-end trainable on large scale datasets and achieving high accuracies on complex tasks.

2 The classiﬁcation-by-components network

In the following  we will describe the CBC architecture and how to train it. We present the ar-
chitecture using full-size components and consecutively generalize this to patch components. Both
principles are used in the evaluation in Sec. 4. The architectures are deﬁned (without loss of gener-
ality) for vectorial inputs but can be extended to higher dimensional inputs like images.

2.1 Reasoning over a set of full-size components

The proposed framework relies on a probabilistic model based on a probability tree diagram T . This
tree T can be decomposed into sub-trees Tc for each class c with the prior class probability P (c)
on the starting edge. Such a sub-tree is depicted in Fig. 2. The whole probability tree diagram is
modeled over ﬁve random variables: c  indicator variable of the class; k  indicator variable of the
component; I  binary random variable for importance; R  binary random variable for reasoning by
detection; D  binary random variable for detection. The probabilities in the tree Tc are interpreted
in the following way: P (k)  probability that the k-th component occurs; P (I|k  c)  probability that
the k-th component is important for the class c; P (R|k  c)  probability that the k-th component has
to be detected for the class c; P (D|k  x)  probability that the k-th component is detected in the

input x. The horizontal bar indicates the complementary event  i. e. P(cid:0)D|k  x(cid:1) is the probability

that the k-th component is not detected in the input x. Based on these deﬁnitions we derive the CBC
architecture.
Extracting the decomposition plan Given an input x ∈ Rnx and a set of trainable full-size
components K = {κk ∈ Rnκ|k = 1  ...  #K} with nx = nκ  the ﬁrst part of the network detects

2

⋮⋮✔✔✔✔✔✔Figure 2: The probability tree diagram Tc that represents the reasoning about a class c. For better
readability  the variable of class c is dropped in the mathematical expressions and we only show the
full sub-tree for the ﬁrst component. The solid line paths are the paths of agreement.

the presence of a component κk in x. A feature extractor f (x) = f (x; θ) with trainable weights θ
takes an input and outputs a feature vector f (x) ∈ Rmx. The feature extractor is used in a Siamese
architecture [12] to extract the features of the input x and of all the components {f (κk)}k. The
extracted features are used to measure the probability P (D|k  x) for the detection of a component
by a detection probability function dk (x) = d (f (x)   f (κk)) ∈ [0  1] with the requirement that
f (x) = f (κk) implies dk (x) = 1. Examples of suitable detection probability functions are the
negative exponential over the squared Euclidean distance or the cosine similarity with a suitable
handling of its negative part. To ﬁnalize the ﬁrst part of the network  the detection probabilities are
collected into the extracted DP as a vector d (x) = (d1 (x)   ...  d#K (x))T ∈ [0  1]#K.
Modeling of the class decomposition plans The second part of the network models the class DPs
for each class c ∈ C = {1  ...  #C} using the three forms of reasoning discussed earlier. Therefore 
we deﬁne the reasoning probabilities  r+
c k as trainable parameters of the model. Pos-
c k = P (I  R|k  c): The probability that the k-th component is important and must
itive reasoning r+
be detected to support the class hypothesis c. Negative reasoning r−
bility that the k-th component is important and must not be detected to support the class hypothesis
c. Indeﬁnite reasoning r0
for the class hypothesis c.2 Together they form a probability space and hence r+
All reasoning probabilities are collected class-wise into vectors r+
and r−

c k = P(cid:0)I  R|k  c(cid:1): The proba-
c k = P(cid:0)I|k  c(cid:1): The probability that the k-th component is not important

c k = 1.
c #K)T ∈ [0  1]#K

c k + r−

c k  and r0

c k  r−

c = (r+

c 1  ...  r+

c k + r0

c   r0

c   respectively.

Reasoning We compute the class hypothesis probability pc (x) regarding the paths of agreement
under the condition of importance. An agreement A is a path in the tree T where either a component
is detected (D) and requires reasoning by detection (R)  or a component is not detected (D) and
requires reasoning by no detection (R). The paths of agreement are marked with solid lines in
Fig. 2. Hence  we model pc (x) by P (A|I  x  c):

(cid:0)P (R|k  c) P (D|k  x) + P(cid:0)R|k  c(cid:1) P(cid:0)D|k  x(cid:1)(cid:1) P (I|k  c) P (k)

(cid:80)

P (A|I  x  c) =

k

(cid:80)

k

(cid:0)1 − P(cid:0)I|k  c(cid:1)(cid:1) P (k)

.

Substituting by the short form notations for the probabilities  assuming that P (k) = 1
rewriting it with matrix calculus yields

#K   and

pc (x) =

(d (x))T · r+

c + (1 − d (x))T · r−
1T · (1 − r0
c)

c

= (d (x))T · ¯r+

c + (1 − d (x))T · ¯r−
c  

(1)

vector as(cid:80)

where 1 is the one vector of dimension #K and ¯r±
c are the normalized effective reasoning possibility
vectors. The probabilities for all classes are then collected into the class hypothesis possibility vector
p (x) = (p1 (x)   ...  p#C (x))T to create the network output. We emphasize that p (x) is a possibility
c pc (x) = 1 does not necessarily hold. See the supplementary material Sec. B.1 for a
detailed derivation of Eq. (1) and Sec. B.2 for a transformation of p (x) into a class probability
vector.

2Note that the idea to explicitly model the state that a component does not contribute and avoid the general

c k = 1 − r

−
c k is related to the DEMPSTER–SHAFER theory of evidence [13].

probabilistic approach r+

3

⋮⋯⋮Figure 3: CBC with patch components and spatial reasoning for image inputs.

Training of a CBC We train the networks end-to-end by minimizing the contrastive loss

l (x  y) = φ (max{pc (x)|c (cid:54)= y  c ∈ C} − py (x))

(2)
where y ∈ C is the class label of x  using stochastic gradient descent learning. The function φ :
[−1  1] → R is a monotonically increasing  almost everywhere differentiable squashing function.
It regulates the generalization-robustness-trade-off over the probability gap between the correct and
highest probable incorrect class. This loss is similar to commonly used functions in prototype-based
c for all c ∈ C.
learning [14  15]. The trainable parameters of a CBC are θ  all κ ∈ K  and r+
We refer to the supplementary material Sec. D for detailed information about the training procedure.

c  r−

c   r0

2.2 Extension to patch components

Assume the feature extractor f processes different input sizes down to a minimum (receptive ﬁeld)
dimension of n0  similar to most Convolutional NNs (CNNs). To relax the assumption nx = nκ
of full-size components and to step closer to the motivating example of Fig. 1  we use a set K
of trainable patch components with nx ≥ nκ ≥ n0 such that f (κk) ∈ Rmκ where mx ≥ mκ.
Moreover  dk (x) is extended to a sliding operation [16  17]  denoted as (cid:126). The result is a detection
possibility stack (extracted spatial DP) of size vd × #K where vd is the spatial dimension after the
sliding operation  see Fig. 3 for an image processing CBC. However  Eq. (1) can only handle one
detection probability for each component and thus the reasoning process has to be redeﬁned:
Downsampling A simple approach is to downsample the detection possibility stack over the spa-
tial dimension vd such that the output is a detection possibility vector and Eq. (1) can be applied.
This can be achieved by applying global pooling techniques like global max pooling.
Spatial reasoning Another approach is the extension of the reasoning process to work on the
spatial DP which we call spatial reasoning. For this  the detection possibility stack of size vd × #K
is kept as depicted in Fig. 3. To compute the class hypothesis probabilities pc (x)  Eq. (1) is redeﬁned
to be a weighted mean over the reasoning at each spatial position i = 1  ...  vd. Thereby  αc i ∈ [0  1]
i αc i = 1 are the (non)-trainable class-wise pixel probabilities resembling the importance

with(cid:80)

of each pixel position i. See the supplementary material in Sec. C for a further extension.

3 Related Work

Reasoning in neural networks
In its simplest form  one can argue that a NN already yields de-
cisions based on reasoning. If one considers a NN to be entirely similar to a multilayer perceptron 
the sign of each weight can be interpreted as either negative or positive reasoning over the corre-
sponding feature. In this case  a weight of zero would model indeﬁnite reasoning. However  the
use of the Rectiﬁed Linear Unit (ReLU) activations forces NNs to be positive reasoning driven only.
Nevertheless  this interpretation of the weights is used in interpretation techniques such as Class-
Activation-Mapping (CAM) [5]  which is similar to heatmap visualizations of CBCs.
Explicit modeling of reasoning The use of components  and the inclusion of the negative and
indeﬁnite reasoning can be seen as an extension of the work in [7]. However  CBCs do not rely on
the complicated three step training procedure presented in the paper and are built upon a probabilistic
reasoning model. In [18]  a form of reasoning is introduced similar to the indeﬁnite reasoning state
by occluding parts of the learned representation. Their components are  however  modeled in a
textual form. In general  the reasoning process has slight similarities to ideas mentioned in [19] and
the modeling of knowledge via graph structures [20–22].

4

⋯⋯a

b

c

d

e

f

g

h

i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

a b c d e f g h i

Figure 4: Learned reasoning process of a CBC with 9 components on MNIST. Top row: The learned
components. Bottom row: The learned reasoning probabilities collected in reasoning matrices. The
class is indicated by the MNIST digit below. The top row corresponds to r+
c k 
and bottom row to r−

c k. White squares depict a probability of one and black squares of zero.

c k  middle row to r0

Feature visualization If the components are deﬁned as trainable parameters in the input space 
then the learned components become similar to feature visualization techniques of NNs [23–25].
In contrast  the components are the direct visualizations of the penultimate layer weights (detection
probability layer)  are not computed via a post-processing  and have a probabilistic interpretation.
Moreover  we are not applying regularizations to the components to resemble realistic images.
Prototype-based classiﬁcation rules and similarity learning A key ingredient of the proposed
network is a Siamese architecture to learn a similarity measure [12  26–28] and the idea to incor-
porate a kind of prototype-based classiﬁcation rule into NNs [29–35]. Currently  the prototype3
classiﬁcation principle is gaining a lot of attention in few-shot learning due to its ability to learn fast
from few data [29  30  36–38]. The idea to replace prototypes with patches in similarity learning has
also been gaining attraction  as can be seen in [39] for the use of object tracking.

4 Evaluation

In this section  the evaluation of the CBCs is presented. Throughout the evaluation  interpretability is
considered as an important characteristic. In this case  something is interpretable if it has a meaning
to experts. We evaluate CBCs on MNIST [40] and IMAGENET [41]. The input spaces are deﬁned
over [0  1] and the datasets are normalized appropriately. Moreover  components that are deﬁned in
the input space are constrained to this space as well. The CBCs use the cosine similarity with ReLU
activation as detection probability function. They are trained with the margin loss deﬁned as Eq. (2)
with φ (x) = ReLU(x + β)  where β is a margin parameter  using the Adam optimizer [42]. An ex-
tended evaluation including an ablation study regarding the network setting on MNIST is presented
in the supplementary material in Sec. E. Where possible  we report mean and standard deviation of
the results. The source code is available at www.github.com/saralajew/cbc_networks.

4.1 MNIST

The CNN feature extractors are implemented without the use of batch normalization [43]  with
Swish activation [44]  and the convolutional ﬁlters constraint to a Euclidean norm of one. We trained
the components and reasoning probabilities from scratch using random initialization. Moreover  the
margin parameter β was set to 0.3.

4.1.1 Negative reasoning: Beyond the best matching prototype principle

The CBC architecture in this experiment uses a 4-layer CNN feature extractor and full-size com-
ponents. During the ablation study we found that in nearly all cases this CBC with 10 components
converged to the Best Matching Prototype Principle (BMPP) [45] and formed prototypical compo-
nents. This means that the reasoning for one class is performed with only strong positive reasoning
over one and indeﬁnite reasoning over all the other components  e. g. see the reasoning matrix of
class 0 in Fig. 4 and the corresponding prototypical component d. To analyze if the network is able
to classify using negative reasoning  we restricted the number of components to be smaller than the
number of classes.

3In contrast to prototypes  components are not class-dependent.

5

Figure 5: Visualization of the α-CBC heatmaps and the (cid:31)-CBC reconstructions for an adversarial

input. For simplicity  we illustrate the more meaningful visualization for each model. The model
visualizations correspond to the best matching reasoning stack regarding the input. We use the color
coding “JET” to map probabilities of 0 to blue and 1 to red.

Fig. 4 shows the learned reasoning process of a CBC with 9 components. Similar to the 10 compo-
nent version  the CBC learns to classify as many classes as possible by the BMPP. In the example 
these are all classes except the class 1  for which the CBC uses weak positive reasoning over the
components a  c  f  and h but mostly depends on negative reasoning over component i. This in-
dicates that if an input image is classiﬁed as a 1  the network requires it to not look like an 8. A
comparison of the shapes of the digits 1 and 8 supports this observation  the 8 only consists of curved
edges while the 1 does not contain any and on average contains the least white pixels while the 8
requires the most. This result shows that by incorporating the negative and indeﬁnite reasoning state 
the CBCs are able to learn both the well understood BMPP and unrestricted approaches beyond the
intuitive classiﬁcation principles by themselves. Both networks achieved close to the state-of-the-art
test accuracies over three runs of (99.32 ± 0.09)%.

4.1.2 Interpretation of the reasoning

−1  denoted as (cid:31)-CBC. (2) Generate an

In this section  we show the interpretability of CBCs. Similar to interpretation techniques from
NNs we do this by considering input dependent and input independent visualizations. Moreover  to
stress the visualizations in such a way that they really show how the model classiﬁes  we: (1) Train
two patch component CBCs similar to Fig. 3  one with trainable  denoted as α-CBC  and one with
non-trainable pixel probabilities ﬁxed to αc i j = (vd · hd)
adversarial image for both models with the boundary attack [46] and show how they fool the model.
Both CBCs use 8 patch components4 of size vκ  hκ = 7. The feature extractor is a 2-layer CNN
which extracts feature stacks of spatial size v(cid:48)
x = 22. The spatial reasoning
size of vd  hd = 7 was obtained by including a ﬁnal max pooling operation of pool size 3 in d (x).
Additionally for each class  two reasoning possibility stacks were learned and winner-take-all was
applied to determine pc (x). We call this multiple reasoning as we allow the model to learn multiple
concepts for each class. The ﬁnal test accuracies of both models are quasi equivalent and on average
over three runs (97.33 ± 0.19) %. Similar to the previous section  the patch components start to
resemble realistic digit parts like strokes  arcs  line-endings  etc.
The interpretability of the CBCs is based on visualizations of how the probability mass is distributed
over the tree T . The class hypothesis probability pc (x)  see Eq. (1)  is the probability of agreement
under the condition of importance  denoted by A|I. This event describes the correct matching of the
extracted and class DP. Moreover  we decompose this event into the positive and negative reasoning
part: Positive A|I is the event that a component is detected that should be detected and is denoted by

κ  h(cid:48)

k = 1 and v(cid:48)

x  h(cid:48)

4The idea is to learn patches of: four quarters of a circle plus two diagonal  horizontal  and vertical lines.

6

A+|I. Negative A|I is the event that a component that should not be detected is not detected and is
denoted by A−|I. Both events can be related to paths in the trees Tc from the root to the leaves  i. e.
A+|I is the upper solid line path and A−|I is the lower solid line path in Fig. 2. The probability of
A|I can be thought of as evidence in favor of a class. Similarly  we can consider the complementary
event of A|I which is disagreement under the condition of importance  denoted by A|I  and occurs
when the extracted DP does not match the class DP. Again  this occurs either as positive A|I when a
+|I  or as negative
component over which the CBC reasons positively is not detected  denoted by A
−|I. The related paths in
A|I when a component with negative reasoning is detected  denoted by A
the tree Tc in Fig. 2 are the dashed line paths excluding non-importance. In general  the probability
of A|I is evidence against a class.
Accordingly to Eq. (1)  the visualizations are based on the probabilities in the tree T for respective
detection possibility vectors zi j. These probabilities are collected into the following possibility
vectors:5 zi j ◦ ¯r+
c i j for A−|I; zi j ◦ ¯r−
−|I. Moreover  we collect all the possibility vectors of one event for all i  j in a stack. Using
for A
such a stack we create the visualizations by three procedures: Probability heatmaps: Upsample
a stack to the input size and sum over k. This visualizes the probabilities for the respective event
x × #K  scale each patch
at the certain position. Reconstructions: Upsample a stack to v(cid:48)
component κk by the respective probability and draw them onto an initially black image of size
vx×hx at the respective position. After a normalization step  the resulted reconstruction image gives
an impression of the combination of the patches that is used to classify the image. Incorporation of
pixel probabilities: Upsample the class-wise pixel probability maps αc to vx × hx and normalize
by the maximum value such that the most important pixels have a value of one. This map is ﬁnally
overlaid over the heatmaps and reconstructions to highlight the impact of each pixel to the overall
classiﬁcation decision.

c i j for A+|I; (1 − zi j) ◦ ¯r+

x × h(cid:48)

c i j for A

+|I; (1 − zi j) ◦ ¯r−

c i j

Input independent interpretation Input independent interpretations are calculated by setting zi j
to the optimal vector with 1 for positive and for 0 negative A|I. They provide an answer to the
question: “What has the model learned about the dataset?”  see Fig. 5 “x independent”. For both
models  the learned concepts of the clean and adversarial class are visualized by the optimal A+|I
and A−|I. As visible in the heatmaps  the α-CBC learned to recognize only as few parts as needed
to distinguish the two classes. In case of the 4  this consists of a check that there is no stroke at the
bottom and top  see A−|I  while there is a corner on the left  see A+|I. Such a radical sparse coding
is learned for all classes. The reasoning for the 9 is similar except that it requires A+|I instead of
A−|I for the top stroke. In contrast  the (cid:31)-CBC learned the whole concept for digits and not just
a sparse coding as the reconstructions show real digit shapes in the A+|I. Moreover  the model
performs interpretable “sanity checks” via A−|I  e. g. no top stroke at the 4.

Input dependent interpretation Input dependent interpretations are obtained by setting zi j to
di j (x). To understand why the adversarial images fool the models by human imperceptible “noise”
we answer the following question: “Which parts of the input provide evidence for / against the cur-
rent classiﬁcation decision?”  see Fig. 5 “x dependent”. By considering the clean probability his-
togram p (x) of the α-CBC we see that the clean input perfectly ﬁts the learned concept of a 4 as it
had a probability of 1. The adversarial attack has turned the input into a 4 and 9 at the same time 
see adversarial p (x). Remarkably  the attack found the high similarity between the two learned
concepts and attacks the model by highlighting a few pixels in the top bar region in form of a patch
– the manipulation only changes one pixel in d (x). Hence  the concept of a 4 is slightly violated
−|I. This causes the probability drop of
as we see a highlighting of the top stroke region in the A
the class 4. At the same time  these few pixels provide A+|I for the top stroke of a 9 and  hence 
raise the probability. For the (cid:31)-CBC  the attack behavior is totally different. Since the clean input
already does not match the learned concept perfectly as p4 (x) ≈ 0.8  the attack fools the model by
+|I the model highlights that the
reducing the contrast via background noise. For example  via the A
clear detection of the upper part of the 4 is not given. Moreover  it recognizes that there could be a
top / bottom stroke  see A

−|I. A similar interpretation holds for the adversarial class.

5The symbol “◦” denotes the Hadamard product (element-wise multiplication).

7

1.00

1.00

1.00

1.00

0.99

0.98

0.88

0.84

0.72

0.70

1.00

1.00

1.00

1.00

1.00

0.98

0.75

0.65

0.61

0.61

1.00

1.00

1.00

1.00

0.83

0.79

0.65

0.61

0.55

0.53

Figure 6: The 10 components with the highest r+
c k for three different classes in the IMAGENET
dataset. From top to bottom the classes are: dalmatian  giant panda  and trolleybus. Below
each component the r+

c k (rounded to two digits) is given with respect to the class in question.

−1 is trained to learn a strong concept as it
Overall result The (cid:31)-CBC with αc i j = (vd · hd)
can only reach py (x) ≈ 1 if it reasons perfectly at each pixel position. Therefore  the probability
histogram shows a relatively high base probability for all classes  as the overlap between encoded
digits to a spatial size of vd  hd = 7 is often around 50%. Moreover  this restrictive classiﬁcation
principle violates the motivating example in Fig. 1 as the model cannot apply indeﬁnite reasoning
over a pixel region. In contrast  the α-CBC is capable of modeling the motivating example but is
at the same time a clear example of what happens if we optimize without any constraints as usually
performed in NNs. Since the model is trained by minimizing an energy function  it learns to classify
correctly with the lowest effort and  hence  oversimpliﬁes. Therefore  the classiﬁcation will be
performed in a non-intuitive way. Moreover  the interpretation shows that the classiﬁcation of both
CBCs is based on non-robust features of f as both are highly sensitive to background manipulations.

4.2

IMAGENET

(cid:17)

1 − r+

c k

c k ·(cid:16)

c k was determined by r+

To evaluate CBCs on more complex data  we trained a CBC on the IMAGENET dataset. The
CBC trained on IMAGENET was implemented using a pre-trained ResNet-50 [47] as non-trainable
feature extractor.
In contrast to the CBCs discussed earlier  the patch components of shape
mκ = 2 × 2 × 2048 are deﬁned directly in the feature space. This removes the relation between
the components and the input space but drastically improves training time. After downsampling the
detection possibility stack of size vd  hd = 6 by global max pooling  the reasoning is applied  see
Sec. 2.2. The components were initialized by cropping the center of 5 images from each class and
consecutively processing them through the feature extractor  resulting in 5 000 patch components. If
the component κk was initialized by a sample from the class c  then we initialized r+
c k as a uniform
random value of [z  1] where z = 0.75 and as a uniform random value of [0  1 − z] otherwise. Af-
terwards  the initialization of r−
. Hence  we biased the model
with positive reasoning to components that were sampled from the respective class. The CBC was
trained with the margin loss and β = 0.1. In compliance with earlier work on IMAGENET  the
input images were rescaled  by ﬁrst rescaling the shortest side to 224 and then performing a center
cropping of size 224 × 224. For the same reason  no image augmentation was used.
Interpretability In Fig. 6  the 10 components with the highest positive reasoning probabilities for
three exemplary classes are presented. After training the components in the feature space  the input
representation of the components is determined by searching for the highest detection probability
in the training set for the given component and cropping the corresponding image area in the input
space. This method is similar to the approach from [7]. In general  the components with a high
positive reasoning probability (above the initialization bound of z) are found to be conceptually
meaningful for the respective class. Further investigation of the components shows that the detection
of the component with the second highest positive reasoning probability for the dalmatian class in
an image also provides evidence in favor of the giant panda class. Similarly  the component with
the ﬁfth highest positive reasoning probability for the dalmatian class is also highly important for
the classes hyena  snow leopard  and english setter while the component with the ﬁfth highest

8

positive reasoning probability for the class trolleybus is also important for the class trolley
car. Similar shared components can be found across many classes  which shows that the CBC is
capable of learning complex class-independent structures.
Averaged across all classes a positive reasoning probability greater than z was learned for 5.2 ± 0.8
components per class while a negative reasoning probability greater than z was assigned to
2 781.8 ± 23.3 out of 5 000 components. As can be seen in Fig. 6  in most cases the positive
reasoning probabilities assigned to components are close to 1.00. This includes components that
were not initialized with a bias towards the class in question. For example  the component with
the ﬁfth highest positive reasoning probability for the dalmatian class was initially biased towards
the english setter class. The ratio between the number of positive and negative reasoning com-
ponents suggest that the model heavily relies on negative reasoning to establish a baseline for its
classiﬁcation decision. We hypothesize that in this higher dimensional setting with a large number
of components positive reasoning is primarily utilized to ﬁne tune the models classiﬁcation decision
after rough categorization by negative reasoning.
Performance To evaluate the performance of CBCs  we compare both the accuracy and inference
time to that of a CNN. The resulting CBC had an inference time of (371± 6) images / sec  similar to
(369± 2) images / sec of a normal ResNet-50 with global average pooling and fully-connected layer.
This shows that the CBC generates no signiﬁcant computational overhead. The top-5 validation
accuracy of 82.4% is on par with earlier CNN generations such as AlexNet with 82.8% [48]. Note
that the used CBC had a non-trainable feature extractor and no parameter tuning was performed. We
are conﬁdent that the accuracy of CBCs on IMAGENET can be improved with further studies. The
CBC was evaluated using one NVIDIA Tesla V100 32 GB GPU.

5 Conclusion and outlook

In this paper  we have presented a probabilistic classiﬁcation model called classiﬁcation-by-
components network together with several possible realizations. Boiling down to the essential
change we made  this is the deﬁnition of a probabilistic framework for the ﬁnal and penultimate
layer of a NN. The detection probability layer is an extension of a convolution layer with the require-
ment to measure the detection of convolutional ﬁlters called components  expressed in probabilities.
Moreover  the ﬁnal reasoning layer is still afﬁne but follows a special implicit constraint deﬁned by
the probability model. The overall output is a probability value for each class without any artiﬁcial
squashing. Independently of the feature extractor used in the CBC  we can always take advantage of
this relation during inference by redeﬁning the network to a single feedforward NN such that almost
no computational overhead is created. This is shown in the experiment on IMAGENET.
Depending on the training setup  the method inherently contains a lot of different interpretation
properties which are all founded on the new probability framework. As shown in the MNIST exper-
iments with Siamese architectures  the method can produce human understandable components and
is able to converge to the BMPP without any explicit regularization. Additionally  we have shown
that the models can answer questions about the classiﬁcation decision by an experiment with patch
components on MNIST. More precisely  the model shows what causes the failure on an adversarial
example. The conclusion drawn here supports the recently published results in [49]. A drawback of
the Siamese architecture is the training overhead and the potential introduction of a lot of parameters
due to components in the input space. In the non Siamese training  CBCs have almost no downsides
to NNs. To be able to use all the presented interpretation techniques  the back projection strategy
presented in [7] can be applied  as we have shown on IMAGENET. The evaluation on IMAGENET
also showed that CBCs are capable of learning high dimensional components that can be utilized by
multiple classes. Investigation of these shared components can provide additional insight into the
model’s classiﬁcation approach. The heatmap visualizations are always applicable and extend the
familiar CAM method by the option to visualize disagreement.
The CBC is a promising new method for classiﬁcation and motivates further research. An initial ro-
bustness evaluation and the use of the class hypothesis possibility vectors for outlier detection show
promising results  see supplementary material in Sec. E.2.4. Nevertheless  the following remain
unanswered: What are proper regularizations for αc i? What are more suitable detection probability
functions? What are the advantages of the explicit injection of knowledge into the network in the
form of trainable or non-trainable components  as we partly applied in the IMAGENET experiment?

9

Acknowledgements

We would like to thank Peter Schlicht and Jacek Bodziony from Volkswagen AG  Jensun Ravichan-
dran from the University of Applied Sciences Mittweida  and Frank-Michael Schleif from the Uni-
versity of Applied Sciences Würzburg-Schweinfurt for their valuable input on previous versions
of the manuscript. We would also like to thank the whole team at the Innovation Campus from
Porsche AG  especially Emilio Oldenziel  Philip Elspas  Mathis Brosowsky  Simon Isele  Simon
Mates  and Sebastian Söhner for their continued support and input. Lastly  we would like to thank
our attentive anonymous reviewers whose comments have greatly improved this manuscript.

References
[1] B. M. Lake  T. D. Ullman  J. B. Tenenbaum  and S. J. Gershman. Building machines that learn

and think like people. Behavioral and Brain Sciences  40  2017.

[2] J. Adebayo  J. Gilmer  M. Muelly  I. Goodfellow  M. Hardt  and B. Kim. Sanity checks for
In Advances in Neural Information Processing Systems  pages 9505–9515 

saliency maps.
2018.

[3] R. Geirhos  P. Rubisch  C. Michaelis  M. Bethge  F. A. Wichmann  and W. Brendel. ImageNet-
trained CNNs are biased towards texture; increasing shape bias improves accuracy and robust-
ness. In International Conference on Learning Representations  2019.

[4] C. Rudin. Stop explaining black box machine learning models for high stakes decisions and

use interpretable models instead. Nature Machine Intelligence  1(5):206  2019.

[5] B. Zhou  A. Khosla  A. Lapedriza  A. Oliva  and A. Torralba. Learning deep features for
discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 2921–2929  2016.

[6] Y. Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model uncer-
tainty in deep learning. In International Conference on Machine Learning  pages 1050–1059 
2016.

[7] C. Chen  O. Li  C. Tao  A. J. Barnett  J. Su  and C. Rudin. This looks like that: Deep learning

for interpretable image recognition. arXiv preprint arXiv:1806.10574  2018.

[8] I. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples. In

International Conference on Learning Representations  2015.

[9] J. Bien and R. Tibshirani. Prototype selection for interpretable classiﬁcation. The Annals of

Applied Statistics  5(4):2403–2424  2011.

[10] M. Biehl  B. Hammer  and T. Villmann. Prototype-based models in machine learning. Wiley

Interdisciplinary Reviews: Cognitive Science  7(2):92–111  2016.

[11] I. Biederman. Recognition-by-components: A theory of human image understanding. Psycho-

logical review  94(2):115  1987.

[12] J. Bromley  I. Guyon  Y. LeCun  E. Säckinger  and R. Shah. Signature veriﬁcation using a
"Siamese" time delay neural network. In Advances in Neural Information Processing Systems 
pages 737–744  1994.

[13] G. Shafer. A mathematical theory of evidence  volume 42. Princeton university press  1976.

[14] A. Sato and K. Yamada. Generalized Learning Vector Quantization. In Advances in Neural

Information Processing Systems  pages 423–429  1996.

[15] K. Crammer  R. Gilad-Bachrach  A. Navot  and A. Tishby. Margin analysis of the LVQ algo-

rithm. In Advances in Neural Information Processing Systems  pages 479–486  2003.

[16] Kamaledin Ghiasi-Shirazi. Generalizing the convolution operator in convolutional neural net-

works. Neural Processing Letters  pages 1–20  2019.

10

[17] Sascha Saralajew  Lars Holdijk  Maike Rees  and Thomas Villmann. Prototype-based neural

network layers: incorporating vector quantization. arXiv preprint arXiv:1812.01214  2018.

[18] P. Tokmakov  Y.-X. Wang  and M. Hebert. Learning compositional representations for few-shot

recognition. arXiv preprint arXiv:1812.09213  2018.

[19] Z. Akata  F. Perronnin  Z. Harchaoui  and C. Schmid. Label-embedding for attribute-based
classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition  pages 819–826  2013.

[20] K. Marino  R. Salakhutdinov  and A. Gupta. The more you know: Using knowledge graphs for
image classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 2673–2681  2017.

[21] C. Jiang  H. Xu  X. Liang  and L. Lin. Hybrid knowledge routed modules for large-scale object

detection. In Advances in Neural Information Processing Systems  pages 1559–1570  2018.

[22] X. Chen  L.-J. Li  L. Fei-Fei  and A. Gupta. Iterative visual reasoning beyond convolutions.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
7239–7248  2018.

[23] D. Erhan  Y. Bengio  A. Courville  and P. Vincent. Visualizing higher-layer features of a deep

network. University of Montreal  1341(3):1  2009.

[24] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Euro-

pean Conference on Computer Vision  pages 818–833. Springer  2014.

[25] A. Nguyen  J. Yosinski  and J. Clune. Understanding neural networks via feature visualization:

A survey. arXiv preprint arXiv:1904.08939  2019.

[26] S. Chopra  R. Hadsell  and Y. LeCun. Learning a similarity metric discriminatively  with
application to face veriﬁcation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 539–546  2005.

[27] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neigh-

bourhood structure. In Artiﬁcial Intelligence and Statistics  pages 412–419  2007.

[28] G. Koch  R. Zemel  and R. Salakhutdinov. Siamese neural networks for one-shot image recog-
nition. In International Conference on Machine Learning – Deep Learning Workshop  2015.

[29] T. Mensink  J. Verbeek  F. Perronnin  and G. Csurka. Metric learning for large scale image
classiﬁcation: Generalizing to new classes at near-zero cost. In European Conference on Com-
puter Vision  pages 488–501. Springer  2012.

[30] J. Snell  K. Swersky  and R. Zemel. Prototypical networks for few-shot learning. In Advances

in Neural Information Processing Systems  pages 4077–4087  2017.

[31] O. Li  H. Liu  C. Chen  and C. Rudin. Deep learning for case-based reasoning through proto-
types: A neural network that explains its predictions. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence  2018.

[32] N. Papernot and P. McDaniel. Deep k-nearest neighbors: Towards conﬁdent  interpretable and

robust deep learning. arXiv preprint arXiv:1803.04765  2018.

[33] H.-M. Yang  X.-Y. Zhang  F. Yin  and C.-L. Liu. Robust classiﬁcation with convolutional
prototype learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 3474–3482  2018.

[34] T. Plötz and S. Roth. Neural nearest neighbors networks. In Advances in Neural Information

Processing Systems  pages 1093–1104  2018.

[35] S. O. Arik and T. Pﬁster. Attention-based prototypical learning towards interpretable  conﬁdent

and robust deep neural networks. arXiv preprint arXiv:1902.06292  2019.

11

[36] O. Vinyals  C. Blundell  T. Lillicrap  K. Kavukcuoglu  and D. Wierstra. Matching networks for
one shot learning. In Advances in Neural Information Processing Systems  pages 3630–3638 
2016.

[37] A. Santoro  S. Bartunov  M. Botvinick  D. Wierstra  and T. Lillicrap. Meta-learning with
In International Conference on Machine Learning 

memory-augmented neural networks.
pages 1842–1850  2016.

[38] S. Gidaris and N. Komodakis. Dynamic few-shot visual learning without forgetting. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 4367–
4375  2018.

[39] L. Bertinetto  J. Valmadre  J. F. Henriques  A. Vedaldi  and P. H. S. Torr. Fully-convolutional
In European Conference on Computer Vision  pages

siamese networks for object tracking.
850–865. Springer  2016.

[40] Y. LeCun  C. Cortes  and C. J.C. Burges. The MNIST database of handwritten digits. 1998.

http://yann.lecun.com/exdb/mnist/.

[41] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei.

ImageNet: A large-scale hi-
erarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 248–255  2009.

[42] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[43] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning  pages 448–456 
2015.

[44] P. Ramachandran  B. Zoph  and Q. V. Le. Searching for activation functions. arXiv preprint

arXiv:1710.05941  2017.

[45] T. Villmann  A. Bohnsack  and M. Kaden. Can Learning Vector Quantization be an alterna-
tive to SVM and deep learning? - Recent trends and advanced variants of Learning Vector
Quantization for classiﬁcation learning. Journal of Artiﬁcial Intelligence and Soft Computing
Research  7(1):65–81  2017.

[46] W. Brendel  J. Rauber  and M. Bethge. Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In International Conference on Learning Repre-
sentations  2018.

[47] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

[48] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems  pages 1097–1105 
2012.

[49] A. Ilyas  S. Santurkar  D. Tsipras  L. Engstrom  B. Tran  and A. Madry. Adversarial examples

are not bugs  they are features. arXiv preprint arXiv:1905.02175  2019.

12

,Roy Frostig
Sida Wang
Percy Liang
Christopher Manning
Sascha Saralajew
Lars Holdijk
Maike Rees
Ebubekir Asan
Thomas Villmann