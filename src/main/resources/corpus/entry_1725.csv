2012,Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,Counterfactual Regret Minimization (CFR) is a popular  iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper  we present a new MCCFR algorithm  Average Strategy Sampling (AS)  that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new  tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition  we prove a similar  tighter bound for AS and other popular MCCFR variants. Finally  we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff.,Efﬁcient Monte Carlo Counterfactual Regret

Minimization in Games with Many Player Actions

Richard Gibson  Neil Burch  Marc Lanctot  and Duane Szafron

Department of Computing Science  University of Alberta
{rggibson | nburch | lanctot | dszafron}@ualberta.ca

Edmonton  Alberta  T6G 2E8  Canada

Abstract

Counterfactual Regret Minimization (CFR) is a popular  iterative algorithm for
computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR)
variants reduce the per iteration time cost of CFR by traversing a smaller  sampled
portion of the tree. The previous most effective instances of MCCFR can still be
very slow in games with many player actions since they sample every action for a
given player. In this paper  we present a new MCCFR algorithm  Average Strat-
egy Sampling (AS)  that samples a subset of the player’s actions according to the
player’s average strategy. Our new algorithm is inspired by a new  tighter bound on
the number of iterations required by CFR to converge to a given solution quality.
In addition  we prove a similar  tighter bound for AS and other popular MCCFR
variants. Finally  we validate our work by demonstrating that AS converges faster
than previous MCCFR algorithms in both no-limit poker and Bluff.

1

Introduction

An extensive-form game is a common formalism used to model sequential decision making prob-
lems containing multiple agents  imperfect information  and chance events. A typical solution con-
cept in games is a Nash equilibrium proﬁle. Counterfactual Regret Minimization (CFR) [12] is an
iterative algorithm that  in 2-player zero-sum extensive-form games  converges to a Nash equilib-
rium. Other techniques for computing Nash equilibria of 2-player zero-sum games include linear
programming [8] and the Excessive Gap Technique [6]. Theoretical results indicate that for a ﬁxed
solution quality  CFR takes a number of iterations at most quadratic in the size of the game [12  The-
orem 4]. Thus  as we consider larger games  more iterations are required to obtain a ﬁxed solution
quality. Nonetheless  CFR’s versatility and memory efﬁciency make it a popular choice.
Monte Carlo CFR (MCCFR) [9] can be used to reduce the traversal time per iteration by considering
only a sampled portion of the game tree. For example  Chance Sampling (CS) [12] is an instance of
MCCFR that only traverses the portion of the game tree corresponding to a single  sampled sequence
of chance’s actions. However  in games where a player has many possible actions  such as no-limit
poker  iterations of CS are still very time consuming. This is because CS considers all possible
player actions  even if many actions are poor or only factor little into the algorithm’s computation.
Our main contribution in this paper is a new MCCFR algorithm that samples player actions and is
suitable for games involving many player choices. Firstly  we provide tighter theoretical bounds on
the number of iterations required by CFR and previous MCCFR algorithms to reach a ﬁxed solution
quality. Secondly  we use these new bounds to propel our new MCCFR sampling algorithm. By
using a player’s average strategy to sample actions  convergence time is signiﬁcantly reduced in
large games with many player actions. We prove convergence and show that our new algorithm
approaches equilibrium faster than previous sampling schemes in both no-limit poker and Bluff.

1

(cid:81)

2 Background
A ﬁnite extensive game contains a game tree with nodes corresponding to histories of actions h ∈ H
and edges corresponding to actions a ∈ A(h) available to player P (h) ∈ N ∪ {c} (where N is the
set of players and c denotes chance). When P (h) = c  σc(h  a) is the (ﬁxed) probability of chance
generating action a at h. Each terminal history z ∈ Z has associated utilities ui(z) for each player
i. We deﬁne ∆i = maxz z(cid:48)∈Z ui(z) − ui(z(cid:48)) to be the range of utilities for player i. Non-terminal
histories are partitioned into information sets I ∈ Ii representing the different game states that
player i cannot distinguish between. For example  in poker  player i does not see the private cards
dealt to the opponents  and thus all histories differing only in the private cards of the opponents are
in the same information set for player i. The action sets A(h) must be identical for all h ∈ I  and we
denote this set by A(I). We deﬁne |Ai| = maxI∈Ii |A(I)| to be the maximum number of actions
available to player i at any information set. We assume perfect recall that guarantees players always
remember information that was revealed to them and the order in which it was revealed.
A (behavioral) strategy for player i  σi ∈ Σi  is a function that maps each information set I ∈ Ii to
a probability distribution over A(I). A strategy proﬁle is a vector of strategies σ = (σ1  ...  σ|N|) ∈
Σ  one for each player. Let ui(σ) denote the expected utility for player i  given that all players play
according to σ. We let σ−i refer to the strategies in σ excluding σi. Let πσ(h) be the probability
of history h occurring if all players choose actions according to σ. We can decompose πσ(h) =
i (h) is the contribution to this probability from player i when playing
according to σi (or from chance when i = c). Let πσ−i(h) be the product of all players’ contributions
(including chance) except that of player i. Let πσ(h  h(cid:48)) be the probability of history h(cid:48) occurring
after h  given h has occurred. Furthermore  for I ∈ Ii  the probability of player i playing to reach I
is πσ
A best response to σ−i is a strategy that maximizes player i’s expected payoff against σ−i. The
best response value for player i is the value of that strategy  bi(σ−i) = maxσ(cid:48)
i  σ−i). A
strategy proﬁle σ is an -Nash equilibrium if no player can unilaterally deviate from σ and gain
more than ; i.e.  ui(σ) +  ≥ bi(σ−i) for all i ∈ N. A game is two-player zero-sum if N = {1  2}
and u1(z) = −u2(z) for all z ∈ Z. In this case  the exploitability of σ  e(σ) = (b1(σ2)+b2(σ1))/2 
measures how much σ loses to a worst case opponent when players alternate positions. A 0-Nash
equilibrium (or simply a Nash equilibrium) has zero exploitability.
Counterfactual Regret Minimization (CFR) [12] is an iterative algorithm that  for two-player zero
sum games  computes an -Nash equilibrium proﬁle with  → 0. CFR has also been shown to work
well in games with more than two players [1  3]. On each iteration t  the base algorithm  “vanilla”
CFR  traverses the entire game tree once per player  computing the expected utility for player i at
each information set I ∈ Ii under the current proﬁle σt  assuming player i plays to reach I. This
ui(z)πσ−i(z[I])πσ(z[I]  z) 
where ZI is the set of terminal histories passing through I and z[I] is that history along z contained
in I. For each action a ∈ A(I)  these values determine the counterfactual regret at iteration t 

expectation is the counterfactual value for player i  vi(I  σ) =(cid:80)

i (h) for any h ∈ I  which is well-deﬁned due to perfect recall.

i (h)  where πσ

i∈Σi ui(σ(cid:48)

i∈N∪{c} πσ

i (I) = πσ

z∈ZI

rt
i(I  a) = vi(I  σt

(I→a)) − vi(I  σt) 

where σ(I→a) is the proﬁle σ except that at I  action a is always taken. The regret rt
i(I  a) measures
how much player i would rather play action a at I than play σt. These regrets are accumulated to
obtain the cumulative counterfactual regret  RT
i(I  a)  and are used to update
the current strategy proﬁle via regret matching [5  12] 

i (I  a) = (cid:80)T

t=1 rt

where x+ = max{x  0} and actions are chosen uniformly at random when the denominator is zero.
It is well-known that in a two-player zero-sum game  if both players’ average (external) regret 

i

RT +

(I  a)
b∈A(I) RT +

(I  b)

i

 

(1)

σT +1(I  a) =

(cid:80)
T(cid:88)
(cid:0)ui(σ(cid:48)
i (I  a) = (cid:80)T

1
T

t=1

= max
i∈Σi
σ(cid:48)

RT
i
T

2

i   σt−i)(cid:1)  

i  σt−i) − ui(σt

is at most /2  then the average proﬁle ¯σT is an -Nash equilibrium. During computation  CFR
stores a cumulative proﬁle sT
i (I  a) and outputs the average proﬁle

i (I)σt

t=1 πσt

i (I  a)/(cid:80)

i (I  a) = sT
¯σT
is bounded by the sum of the positive parts of the cumulative counterfactual regrets RT +

i (I  b). The original CFR analysis shows that player i’s regret

b∈A(I) sT

(I  a):

i

Theorem 1 (Zinkevich et al. [12])

i ≤(cid:88)

I∈I

RT

max
a∈A(I)

RT +

i

(I  a).

Regret matching minimizes the average of the cumulative counterfactual regrets  and so player i’s
average regret is also minimized by Theorem 1. For each player i  let Bi be the partition of Ii such
that two information sets I  I(cid:48) are in the same part B ∈ Bi if and only if player i’s sequence of
actions leading to I is the same as the sequence of actions leading to I(cid:48). Bi is well-deﬁned due to

perfect recall. Next  deﬁne the M-value of the game to player i to be Mi =(cid:80)

(cid:112)|B|. The

B∈Bi

best known bound on player i’s average regret is:

Theorem 2 (Lanctot et al. [9]) When using vanilla CFR  average regret is bounded by

(cid:112)|Ai|

T

.

√
≤ ∆iMi

RT
i
T

We prove a tighter bound in Section 3. For large games  CFR’s full game tree traversals can be very
expensive. Alternatively  one can traverse a smaller  sampled portion of the tree on each iteration
using Monte Carlo CFR (MCCFR) [9]. Let Q = {Q1  ...  QK} be a set of subsets  or blocks  of
the terminal histories Z such that the union of Q spans Z. For example  Chance Sampling (CS)
[12] is an instance of MCCFR that partitions Z into blocks such that two histories are in the same
block if and only if no two chance actions differ. On each iteration  a block Qj is sampled with
k=1 qk = 1. In CS  we generate a block by sampling a single action a at
each history h ∈ H with P (h) = c according to its likelihood of occurring  σc(h  a). In general  the
sampled counterfactual value for player i is

probability qj  where(cid:80)K

˜vi(I  σ) =

ui(z)πσ−i(z[I])πσ(z[I]  z)/q(z) 

(cid:88)

z∈ZI∩Qj

where q(z) = (cid:80)

k:z∈Qk

i(I  a) = ˜vi(I  σt
t=1 ˜rt

i (I  a) =(cid:80)T

qk is the probability that z was sampled. For example  in CS  q(z) =
(I→a))−
c (z). Deﬁne the sampled counterfactual regret for action a at I to be ˜rt
πσ
˜vi(I  σt). Strategies are then generated by applying regret matching to ˜RT
i(I  a).
CS has been shown to signiﬁcantly reduce computing time in poker games [11  Appendix A.5.2].
Other instances of MCCFR include External Sampling (ES) and Outcome Sampling (OS) [9].
ES takes CS one step further by considering only a single action for not only chance  but also for
the opponents  where opponent actions are sampled according to the current proﬁle σt−i. OS is the
most extreme version of MCCFR that samples a single action at every history  walking just a single
trajectory through the tree on each traversal (Qj = {z}). ES and OS converge to equilibrium faster
than vanilla CFR in a number of different domains [9  Figure 1].
ES and OS yield a probabilistic bound on the average regret  and thus provide a probabilistic guar-
antee that ¯σT converges to a Nash equilibrium. Since both algorithms generate blocks by sampling
i∈N∪{c} qi(z) so that qi(z) is the probability

actions independently  we can decompose q(z) = (cid:81)

contributed to q(z) by sampling player i’s actions.

Theorem 3 (Lanctot et al. [9]) 1 Let X be one of ES or OS (assuming OS also samples opponent
actions according to σ−i)  let p ∈ (0  1]  and let δ = minz∈Z qi(z) > 0 over all 1 ≤ t ≤ T . When
using X  with probability 1 − p  average regret is bounded by

(cid:32)

(cid:112)2|Ii||Bi|

(cid:33)(cid:18) 1

(cid:19) ∆i

≤

RT
i
T

Mi +

√

p

δ

(cid:112)|Ai|√

T

.

used Mi ≥(cid:112)|Ii||Bi|  which is actually incorrect. The bound we present here is correct.

1The bound presented by Lanctot et al. appears slightly different  but the last step of their proof mistakenly

3

3 New CFR Bounds

(cid:80)

While Zinkevich et al. [12] bound a player’s regret by a sum of cumulative counterfactual re-
grets (Theorem 1)  we can actually equate a player’s regret to a weighted sum of counterfac-
tual regrets. For a strategy σi ∈ Σi and an information set I ∈ Ii  deﬁne RT
i (I  σi) =
i ∈ Σi be a player i strategy such that
i   σt−i) =

(cid:80)T
t=1 ui(σ(cid:48)
i is a best response to the opponent’s average strategy after T iterations.

i  σt−i). Note that in a two-player game  (cid:80)T

i   ¯σT−i)  and thus σ∗

t=1 ui(σ∗

In addition 

let σ∗

i (I  a).

a∈A(I) σi(I  a)RT
σ∗
i = arg maxσ(cid:48)
i∈Σi
T ui(σ∗
Theorem 4

πσ∗
i (I)RT

i (I  σ∗
i ).

be Mi(σi) =(cid:80)

All proofs in this paper are provided in full as supplementary material. Theorem 4 leads to a tighter
bound on the average regret when using CFR. For a strategy σi ∈ Σi  deﬁne the M-value of σi to
i (I). Clearly  Mi(σi) ≤ Mi for all
B∈Bi
σi ∈ Σi since πσ
i (B) ≤ 1. For vanilla CFR  we can simply replace Mi in Theorem 2 with Mi(σ∗
i ):
Theorem 5 When using vanilla CFR  average regret is bounded by

i (B) = maxI∈B πσ

πσ

RT

i =

(cid:88)
i (B)(cid:112)|B|  where πσ

I∈Ii

i )(cid:112)|Ai|

.

T

≤ ∆iMi(σ∗
√

RT
i
T

For MCCFR  we can show a similar improvement to Theorem 3. Our proof includes a bound for CS
that appears to have been omitted in previous work. Details are in the supplementary material.
Theorem 6 Let X be one of CS  ES  or OS (assuming OS samples opponent actions according to
σ−i)  let p ∈ (0  1]  and let δ = minz∈Z qi(z) > 0 over all 1 ≤ t ≤ T . When using X  with
probability 1 − p  average regret is bounded by

(cid:32)

≤

RT
i
T

Mi(σ∗

i ) +

(cid:112)2|Ii||Bi|

√

(cid:33)(cid:18) 1

(cid:19) ∆i

p

δ

(cid:112)|Ai|√

T

.

Theorem 4 states that player i’s regret is equal to the weighted sum of player i’s counterfactual
regrets at each I ∈ Ii  where the weights are equal to player i’s probability of reaching I under σ∗
i .
Since our goal is to minimize average regret  this means that we only need to minimize the average
cumulative counterfactual regret at each I ∈ Ii that σ∗
i plays to reach. Therefore  when using
MCCFR  we may want to sample more often those information sets that σ∗
i plays to reach  and less
often those information sets that σ∗

i avoids. This inspires our new MCCFR sampling algorithm.

4 Average Strategy Sampling

i on hand. Recall that in a two-player game  σ∗

Leveraging the theory developed in the previous section  we now introduce a new MCCFR sam-
pling algorithm that can minimize average regret at a faster rate than CS  ES  and OS. As we just
described  we want our algorithm to sample more often the information sets that σ∗
i plays to reach.
Unfortunately  we do not have the exact strategy σ∗
i is
a best response to the opponent’s average strategy  ¯σT−i. However  for two-player zero-sum games 
we do know that the average proﬁle ¯σT converges to a Nash equilibrium. This means that player i’s
average strategy  ¯σT
i   converges to a best response of ¯σT−i. While the average strategy is not an exact
best response  it can be used as a heuristic to guide sampling within MCCFR. Our new sampling al-
gorithm  Average Strategy Sampling (AS)  selects actions for player i according to the cumulative
proﬁle and three predeﬁned parameters. AS can be seen as a sampling scheme between OS and ES
where a subset of player i’s actions are sampled at each information set I  as opposed to sampling
i (I ·) on iteration
one action (OS) or sampling every action (ES). Given the cumulative proﬁle sT
T   an exploration parameter  ∈ (0  1]  a threshold parameter τ ∈ [1 ∞)  and a bonus parameter
β ∈ [0 ∞)  each of player i’s actions a ∈ A(I) are sampled independently with probability

ρ(I  a) = max

 

 

(2)

(cid:40)

(cid:41)

β +(cid:80)

β + τ sT

i (I  a)

b∈A(I) sT

i (I  b)

4

if h ∈ Z then return ui(h)/q end if
if h ∈ P (c) then Sample action a ∼ σc(h ·)  return WalkTree(ha  i  q) end if
I ← Information set containing h   σ(I ·) ← RegretMatching(r(I ·))
if h /∈ P (i) then

Algorithm 1 Average Strategy Sampling (Two-player version)
1: Require: Parameters   τ  β
2: Initialize regret and cumulative proﬁle: ∀I  a : r(I  a) ← 0  s(I  a) ← 0
3:
4: WalkTree(history h  player i  sample prob q):
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

for a ∈ A(I) do r(I  a) ← r(I  a) + ˜v(a) −(cid:80)
return(cid:80)

ρ ← max
if Random(0  1) < ρ then ˜v(a) ← WalkTree(ha  i  q · min{1  ρ}) end if

for a ∈ A(I) do s(I  a) ← s(I  a) + (σ(I  a)/q) end for
Sample action a ∼ σ(I ·)  return WalkTree(ha  i  q)

end if
for a ∈ A(I) do
 

a∈A(I) σ(I  a)˜v(a) end for

  ˜v(a) ← 0

end for

(cid:110)

(cid:111)

β+(cid:80)

β+τ s(I a)

b∈A(I) s(I b)

a∈A(I) σ(I  a)˜v(a)

b∈A(I) sT

b∈A(I) sT

or with probability 1 if either ρ(I  a) > 1 or β +(cid:80)
i (I  a)/(cid:80)

i (I  b) = 0. As in ES  at opponent and
chance nodes  a single action is sampled on-policy according to the current opponent proﬁle σT−i
and the ﬁxed chance probabilities σc respectively.
If τ = 1 and β = 0  then ρ(I  a) is equal to the probability that the average strategy ¯σT
i =
i (I  b) plays a at I  except that each action is sampled with probability at least
sT
. For choices greater than 1  τ acts as a threshold so that any action taken with probability at least
1/τ by the average strategy is always sampled by AS. Furthermore  β’s purpose is to increase the
rate of exploration during early AS iterations. When β > 0  we effectively add β as a bonus to the
i (I  a) before normalizing. Since player i’s average strategy ¯σT
cumulative value sT
is not a good
approximation of σ∗
i
i for small T   we include β to avoid making ill-informed choices early-on. As
i (I ·) grows over time  β eventually becomes negligible. In Section 5  we
the cumulative proﬁle sT
present a set of values for   τ  and β that work well across all of our test games.
Pseudocode for a two-player version of AS is presented in Algorithm 1. In Algorithm 1  the recursive
function WalkTree considers four different cases. Firstly  if we have reached a terminal node  we
return the utility scaled by 1/q (line 5)  where q = qi(z) is the probability of sampling z contributed
from player i’s actions. Secondly  when at a chance node  we sample a single action according to σc
and recurse down that action (line 6). Thirdly  at an opponent’s choice node (lines 8 to 11)  we again
sample a single action and recurse  this time according to the opponent’s current strategy obtained
via regret matching (equation (1)). At opponent nodes  we also update the cumulative proﬁle (line
9) for reasons that we describe in a previous paper [2  Algorithm 1]. For games with more than two
players  a second tree walk is required and we omit these details.
The ﬁnal case in Algorithm 1 handles choice nodes for player i (lines 7 to 17). For each action a  we
compute the probability ρ of sampling a and stochastically decide whether to sample a or not  where
Random(0 1) returns a random real number in [0  1). If we do sample a  then we recurse to obtain
(I→a)) (line 14). Finally  we update the regrets at I
the sampled counterfactual value ˜v(a) = ˜vi(I  σt

(line 16) and return the sampled counterfactual value at I (cid:80)

a∈A(I) σ(I  a)˜v(a) = ˜vi(I  σt).

Repeatedly running WalkTree(∅  i  1) ∀i ∈ N provides a probabilistic guarantee that all players’
average regret will be minimized. In the supplementary material  we prove that AS exhibits the same
regret bound as CS  ES  and OS provided in Theorem 6. Note that δ in Theorem 6 is guaranteed
to be positive for AS by the inclusion of  in equation (2). However  for CS and ES  δ = 1 since
all of player i’s actions are sampled  whereas δ ≤ 1 for OS and AS. While this suggests that fewer
iterations of CS or ES are required to achieve the same regret bound compared to OS and AS 
iterations for OS and AS are faster as they traverse less of the game tree. Just as CS  ES  and OS

5

have been shown to beneﬁt from this trade-off over vanilla CFR  we will show that in practice  AS
can likewise beneﬁt over CS and ES and that AS is a better choice than OS.

5 Experiments

In this section  we compare the convergence rates of AS to those of CS  ES  and OS. While AS can
be applied to any extensive game  the aim of AS is to provide faster convergence rates in games
involving many player actions. Thus  we consider two domains  no-limit poker and Bluff  where we
can easily scale the number of actions available to the players.
No-limit poker. The two-player poker game we consider here  which we call 2-NL Hold’em(k) 
is inspired by no-limit Texas Hold’em. 2-NL Hold’em(k) is played over two betting rounds. Each
player starts with a stack of k chips. To begin play  the player denoted as the dealer posts a small
blind of one chip and the other player posts a big blind of two chips. Each player is then dealt two
private cards from a standard 52-card deck and the ﬁrst betting round begins. During each betting
round  players can either fold (forfeit the game)  call (match the previous bet)  or raise by any
number of chips in their remaining stack (increase the previous bet)  as long as the raise is at least as
big as the previous bet. After the ﬁrst betting round  three public community cards are revealed (the
ﬂop) and a second and ﬁnal betting round begins. If a player has no more chips left after a call or a
raise  that player is said to be all-in. At the end of the second betting round  if neither player folded 
then the player with the highest ranked ﬁve-card poker hand wins all of the chips played. Note that
the number of player actions in 2-NL Hold’em(k) at one information set is at most the starting stack
size  k. Increasing k adds more betting options and allows for more actions before being all-in.
Bluff. Bluff(D1  D2) [7]  also known as Liar’s Dice  Perduo  and Dudo  is a two-player dice-bidding
game played with six-sided dice over a number of rounds. Each player i starts with Di dice. In each
round  players roll their dice and look at the result without showing their opponent. Then  players
alternate by bidding a quantity q of a face value f of all dice in play until one player claims that
the other is blufﬁng (i.e.  claims that the bid does not hold). To place a new bid  a player must
increase q or f of the current bid. A face value of six is considered “wild” and counts as any other
face value. The player calling bluff wins the round if the opponent’s last bid is incorrect  and loses
otherwise. The losing player removes one of their dice from the game and a new round begins.
Once a player has no more dice left  that player loses the game and receives a utility of −1  while
the winning player earns +1 utility. The maximum number of player actions at an information set
is 6(D1 + D2) + 1 as increasing Di allows both players to bid higher quantities q.
Preliminary tests. Before comparing AS to CS  ES  and OS  we ﬁrst run some preliminary exper-
iments to ﬁnd a good set of parameter values for   τ  and β to use with AS. All of our preliminary
experiments are in two-player 2-NL Hold’em(k). In poker  a common approach is to create an ab-
stract game by merging similar card dealings together into a single chance action or “bucket” [4]. To
keep the size of our games manageable  we employ a ﬁve-bucket abstraction that reduces the branch-
ing factor at each chance node down to ﬁve  where dealings are grouped according to expected hand
strength squared as described by Zinkevich et al. [12].
Firstly  we ﬁx τ = 1000 and test different values for  and β in 2-NL Hold’em(30). Recall that
τ = 1000 implies actions taken by the average strategy with probability at least 0.001 are always
sampled by AS. Figure 1a shows the exploitability in the ﬁve-bucket abstract game  measured in
milli-big-blinds per game (mbb/g)  of the proﬁle produced by AS after 1012 nodes visited. Recall
that lower exploitability implies a closer approximation to equilibrium. Each data point is averaged
over ﬁve runs of AS. The  = 0.05 and β = 105 or 106 proﬁles are the least exploitable proﬁles
within statistical noise (not shown).
Next  we ﬁx  = 0.05 and β = 106 and test different values for τ. Figure 1b shows the abstract
game exploitability over the number of nodes visited by AS in 2-NL Hold’em(30)  where again each
data point is averaged over ﬁve runs. Here  the least exploitable strategies after 1012 nodes visited
are obtained with τ = 100 and τ = 1000 (again within statistical noise). Similar results to Figure
1b hold in 2-NL Hold’em(40) and are not shown. Throughout the remainder of our experiments  we
use the ﬁxed set of parameters  = 0.05  β = 106  and τ = 1000 for AS.

6

(a) τ = 1000

(b)  = 0.05  β = 106

Figure 1: (a) Abstract game exploitability of AS proﬁles for τ = 1000 after 1012 nodes visited
in 2-NL Hold’em(30). (b) Log-log plot of abstract game exploitability over the number of nodes
visited by AS with  = 0.05 and β = 106 in 2-NL Hold’em(30). For both ﬁgures  units are in
milli-big-blinds per hand (mbb/g) and data points are averaged over ﬁve runs with different random
seeds. Error bars in (b) indicate 95% conﬁdence intervals.

Main results. We now compare AS to CS  ES  and OS in both 2-NL Hold’em(k) and Bluff(D1  D2).
Similar to Lanctot et al. [9]  our OS implementation is -greedy so that the current player i samples
a single action at random with probability  = 0.5  and otherwise samples a single action according
to the current strategy σi.
Firstly  we consider two-player 2-NL Hold’em(k) with starting stacks of k = 20  22  24  ...  38 
and 40 chips  for a total of eleven different 2-NL Hold’em(k) games. Again  we apply the same
ﬁve-bucket card abstraction as before to keep the games reasonably sized. For each game  we ran
each of CS  ES  OS  and AS ﬁve times  measured the abstract game exploitability at a number of
checkpoints  and averaged the results. Figure 2a displays the results for 2-NL Hold’em(36)  a game
with approximately 68 million information sets and 5 billion histories (nodes). Here  AS achieved
an improvement of 54% over ES at the ﬁnal data points. In addition  Figure 2b shows the average
exploitability in each of the eleven games after approximately 3.16 × 1012 nodes visited by CS  ES 
and AS. OS performed much worse and is not shown. Since one can lose more as the starting stacks
are increased (i.e.  ∆i becomes larger)  we “normalized” exploitability across each game by dividing
the units on the y-axis by k. While there is little difference between the algorithms for the smaller
20 and 22 chip games  we see a signiﬁcant beneﬁt to using AS over CS and ES for the larger games
that contain many player actions. For the most part  the margins between AS  CS  and ES increase
with the game size.
Figure 3 displays similar results for Bluff(1  1) and Bluff(2  1)  which contain over 24 thousand and
3.5 million information sets  and 294 thousand and 66 million histories (nodes) respectively. Again 
AS converged faster than CS  ES  and OS in both Bluff games tested. Note that the same choices
of parameters ( = 0.05  β = 106  τ = 1000) that worked well in 2-NL Hold’em(30) also worked
well in other 2-NL Hold’em(k) games and in Bluff(D1  D2).

6 Conclusion

This work has established a number of improvements for computing strategies in extensive-form
games with CFR  both theoretically and empirically. We have provided new  tighter bounds on the
average regret when using vanilla CFR or one of several different MCCFR sampling algorithms.
These bounds were derived by showing that a player’s regret is equal to a weighted sum of the
player’s cumulative counterfactual regrets (Theorem 4)  where the weights are given by a best re-
sponse to the opponents’ previous sequence of strategies. We then used this bound as inspiration for
our new MCCFR algorithm  AS. By sampling a subset of a player’s actions  AS can provide faster

7

Exploitability (mbb/g)100101102103104105106107108109β0.010.050.10.20.30.40.5ε 0 0.2 0.4 0.6 0.8 110-1100101102101010111012Abstract game exploitability (mbb/g)Nodes Visitedτ=100τ=101τ=102τ=103τ=104τ=105τ=106(a) 2-NL Hold’em(36)

(b) 2-NL Hold’em(k)  k ∈ {20  22  ...  40}

Figure 2: (a) Log-log plot of abstract game exploitability over the number of nodes visited by CS 
ES  OS  and AS in 2-NL Hold’em(36). The initial uniform random proﬁle is exploitable for 6793
mbb/g  as indicated by the black dashed line. (b) Abstract game exploitability after approximately
3.16 × 1012 nodes visited over the game size for 2-NL Hold’em(k) with even-sized starting stacks
k between 20 and 40 chips. For both graphs  units are in milli-big-blinds per hand (mbb/g) and data
points are averaged over ﬁve runs with different random seeds. Error bars indicate 95% conﬁdence
intervals. For (b)  units on the y-axis are normalized by dividing by the starting chip stacks.

(a) Bluff(1  1)

(b) Bluff(2  1)

Figure 3: Log-log plots of exploitability over number of nodes visited by CS  ES  OS  and AS in
Bluff(1  1) and Bluff(2  1). The initial uniform random proﬁle is exploitable for 0.780 and 0.784
in Bluff(1  1) and Bluff(2  1) respectively  as indicated by the black dashed lines. Data points are
averaged over ﬁve runs with different random seeds and error bars indicate 95% conﬁdence intervals.

convergence rates in games containing many player actions. AS converged faster than previous MC-
CFR algorithms in all of our test games. For future work  we would like to apply AS to games with
many player actions and with more than two players. All of our theory still applies  except that
player i’s average strategy is no longer guaranteed to converge to σ∗
i . Nonetheless  AS may still ﬁnd
strong strategies faster than CS and ES when it is too expensive to sample all of a player’s actions.

Acknowledgments

We thank the members of the Computer Poker Research Group at the University of Alberta for help-
ful conversations pertaining to this work. This research was supported by NSERC  Alberta Innovates
– Technology Futures  and computing resources provided by WestGrid and Compute Canada.

8

10-1100101102103104101010111012Abstract game exploitability (mbb/g)Nodes VisitedCSESOSAS 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16106107108Abstract game exploitability (mbb/g) / kGame size (# information sets)k=20k=30k=40CSESAS10-510-410-310-210-11001071081091010101110121013ExploitabilityNodes VisitedCSESOSAS10-510-410-310-210-11001071081091010101110121013ExploitabilityNodes VisitedCSESOSASReferences
[1] Nick Abou Risk and Duane Szafron. Using counterfactual regret minimization to create com-
petitive multiplayer poker agents. In Ninth International Conference on Autonomous Agents
and Multiagent Systems (AAMAS)  pages 159–166  2010.

[2] Richard Gibson  Marc Lanctot  Neil Burch  Duane Szafron  and Michael Bowling. Generalized
sampling and variance in counterfactual regret minimization. In Twenty-Sixth Conference on
Artiﬁcial Intelligence (AAAI)  pages 1355–1361  2012.

[3] Richard Gibson and Duane Szafron. On strategy stitching in large extensive form multiplayer
In Advances in Neural Information Processing Systems 24 (NIPS)  pages 100–108 

games.
2011.

[4] Andrew Gilpin and Tuomas Sandholm. A competitive Texas Hold’em poker player via au-
In Twenty-First Conference on

tomated abstraction and real-time equilibrium computation.
Artiﬁcial Intelligence (AAAI)  pages 1007–1013  2006.

[5] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equi-

librium. Econometrica  68:1127–1150  2000.

[6] Samid Hoda  Andrew Gilpin  Javier Pe˜na  and Tuomas Sandholm. Smoothing techniques
for computing Nash equilibria of sequential games. Mathematics of Operations Research 
35(2):494–512  2010.

[7] Reiner Knizia. Dice Games Properly Explained. Blue Terrier Press  2010.
[8] Daphne Koller  Nimrod Megiddo  and Bernhard von Stengel. Fast algorithms for ﬁnding
In Annual ACM Symposium on Theory of Computing

randomized strategies in game trees.
(STOC’94)  pages 750–759  1994.

[9] Marc Lanctot  Kevin Waugh  Martin Zinkevich  and Michael Bowling. Monte Carlo sampling
for regret minimization in extensive games. In Advances in Neural Information Processing
Systems 22 (NIPS)  pages 1078–1086  2009.

[10] Marc Lanctot  Kevin Waugh  Martin Zinkevich  and Michael Bowling. Monte Carlo sampling
for regret minimization in extensive games. Technical Report TR09-15  University of Alberta 
2009.

[11] Martin Zinkevich  Michael Johanson  Michael Bowling  and Carmelo Piccione. Regret min-
imization in games with incomplete information. Technical Report TR07-14  University of
Alberta  2007.

[12] Martin Zinkevich  Michael Johanson  Michael Bowling  and Carmelo Piccione. Regret mini-
mization in games with incomplete information. In Advances in Neural Information Processing
Systems 20 (NIPS)  pages 905–912  2008.

9

,Deepti Pachauri
Risi Kondor
Vikas Singh