2014,Algorithms for CVaR Optimization in MDPs,In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures  and because of its computational efficiencies has gained popularity in finance and operations research. In this paper  we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally  we demonstrate the usefulness of our algorithms in an optimal stopping problem.,Algorithms for CVaR Optimization in MDPs

Institute of Computational & Mathematical Engineering  Stanford University

Yinlam Chow∗

Mohammad Ghavamzadeh†

Adobe Research & INRIA Lille - Team SequeL

Abstract

In many sequential decision-making problems we may want to manage risk by
minimizing some measure of variability in costs in addition to minimizing a stan-
dard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure
that addresses some of the shortcomings of the well-known variance-related risk
measures  and because of its computational efﬁciencies has gained popularity in
ﬁnance and operations research. In this paper  we consider the mean-CVaR op-
timization problem in MDPs. We ﬁrst derive a formula for computing the gradi-
ent of this risk-sensitive objective function. We then devise policy gradient and
actor-critic algorithms that each uses a speciﬁc method to estimate this gradient
and updates the policy parameters in the descent direction. We establish the con-
vergence of our algorithms to locally risk-sensitive optimal policies. Finally  we
demonstrate the usefulness of our algorithms in an optimal stopping problem.

Introduction

1
A standard optimization criterion for an inﬁnite horizon Markov decision process (MDP) is the
expected sum of (discounted) costs (i.e.  ﬁnding a policy that minimizes the value function of the
initial state of the system). However in many applications  we may prefer to minimize some measure
of risk in addition to this standard optimization criterion. In such cases  we would like to use a
criterion that incorporates a penalty for the variability (due to the stochastic nature of the system)
induced by a given policy. In risk-sensitive MDPs [16]  the objective is to minimize a risk-sensitive
criterion such as the expected exponential utility [16]  a variance-related measure [24  14]  or the
percentile performance [15]. The issue of how to construct such criteria in a manner that will be
both conceptually meaningful and mathematically tractable is still an open question.
Although most losses (returns) are not normally distributed  the typical Markowitz mean-variance
optimization [18]  that relies on the ﬁrst two moments of the loss (return) distribution  has domi-
nated the risk management for over 50 years. Numerous alternatives to mean-variance optimization
have emerged in the literature  but there is no clear leader amongst these alternative risk-sensitive
objective functions. Value-at-risk (VaR) and conditional value-at-risk (CVaR) are two promising
such alternatives that quantify the losses that might be encountered in the tail of the loss distribu-
tion  and thus  have received high status in risk management. For (continuous) loss distributions 
while VaRα measures risk as the maximum loss that might be incurred w.r.t. a given conﬁdence
level α  CVaRα measures it as the expected loss given that the loss is greater or equal to VaRα.
Although VaR is a popular risk measure  CVaR’s computational advantages over VaR has boosted
the development of CVaR optimization techniques. We provide the exact deﬁnitions of these two
risk measures and brieﬂy discuss some of the VaR’s shortcomings in Section 2. CVaR minimization
was ﬁrst developed by Rockafellar and Uryasev [23] and its numerical effectiveness was demon-
strated in portfolio optimization and option hedging problems. Their work was then extended to
objective functions consist of different combinations of the expected loss and the CVaR  such as the
minimization of the expected loss subject to a constraint on CVaR. This is the objective function

∗Part of the work is completed during Yinlam Chow’s internship at Adobe Research.
†Mohammad Ghavamzadeh is at Adobe Research  on leave of absence from INRIA Lille - Team SequeL.

1

that we study in this paper  although we believe that our proposed algorithms can be easily extended
to several other CVaR-related objective functions. Boda and Filar [9] and B¨auerle and Ott [20  3]
extended the results of [23] to MDPs (sequential decision-making). While the former proposed to
use dynamic programming (DP) to optimize CVaR  an approach that is limited to small problems 
the latter showed that in both ﬁnite and inﬁnite horizon MDPs  there exists a deterministic history-
dependent optimal policy for CVaR optimization (see Section 3 for more details).
Most of the work in risk-sensitive sequential decision-making has been in the context of MDPs
(when the model is known) and much less work has been done within the reinforcement learning
(RL) framework. In risk-sensitive RL  we can mention the work by Borkar [10  11] who considered
the expected exponential utility and those by Tamar et al. [26] and Prashanth and Ghavamzadeh [17]
on several variance-related risk measures. CVaR optimization in RL is a rather novel subject.
Morimura et al. [19] estimate the return distribution while exploring using a CVaR-based risk-
sensitive policy. Their algorithm does not scale to large problems. Petrik and Subramanian [22]
propose a method based on stochastic dual DP to optimize CVaR in large-scale MDPs. However 
their method is limited to linearly controllable problems. Borkar and Jain [12] consider a ﬁnite-
horizon MDP with CVaR constraint and sketch a stochastic approximation algorithm to solve it.
Finally  Tamar et al. [27] have recently proposed a policy gradient algorithm for CVaR optimization.
In this paper  we develop policy gradient (PG) and actor-critic (AC) algorithms for mean-CVaR
optimization in MDPs. We ﬁrst derive a formula for computing the gradient of this risk-sensitive
objective function. We then propose several methods to estimate this gradient both incrementally
and using system trajectories (update at each time-step vs. update after observing one or more tra-
jectories). We then use these gradient estimations to devise PG and AC algorithms that update the
policy parameters in the descent direction. Using the ordinary differential equations (ODE) ap-
proach  we establish the asymptotic convergence of our algorithms to locally risk-sensitive optimal
policies. Finally  we demonstrate the usefulness of our algorithms in an optimal stopping prob-
lem. In comparison to [27]  while they develop a PG algorithm for CVaR optimization in stochastic
shortest path problems that only considers continuous loss distributions  uses a biased estimator for
VaR  is not incremental  and has no comprehensive convergence proof  here we study mean-CVaR
optimization  consider both discrete and continuous loss distributions  devise both PG and (several)
AC algorithms (trajectory-based and incremental – plus AC helps in reducing the variance of PG
algorithms)  and establish convergence proof for our algorithms.

2 Preliminaries

tation is denoted by c(x  a) = E(cid:2)C(x  a)(cid:3); P (·|x  a) is the transition probability distribution; and

We consider problems in which the agent’s interaction with the environment is modeled as a MDP.
A MDP is a tuple M = (X  A  C  P  P0)  where X = {1  . . .   n} and A = {1  . . .   m} are the state
and action spaces; C(x  a) ∈ [−Cmax  Cmax] is the bounded cost random variable whose expec-
P0(·) is the initial state distribution. For simplicity  we assume that the system has a single initial
state x0  i.e.  P0(x) = 1{x = x0}. All the results of the paper can be easily extended to the case that
the system has more than one initial state. We also need to specify the rule according to which the
agent selects actions at each state. A stationary policy µ(·|x) is a probability distribution over ac-
tions  conditioned on the current state. In policy gradient and actor-critic methods  we deﬁne a class

of parameterized stochastic policies(cid:8)µ(·|x; θ)  x ∈ X   θ ∈ Θ ⊆ Rκ1(cid:9)  estimate the gradient of a
γ (x|x0) = (1 − γ)(cid:80)∞
deﬁne the value-at-risk at the conﬁdence level α ∈ (0  1) as VaRα(Z) = min(cid:8)z | F (z) ≥ α(cid:9).

performance measure w.r.t. the policy parameters θ from the observed system trajectories  and then
improve the policy by adjusting its parameters in the direction of the gradient. Since in this setting a
policy µ is represented by its κ1-dimensional parameter vector θ  policy dependent functions can be
written as a function of θ in place of µ. So  we use µ and θ interchangeably in the paper. We denote
γ (x|x0)µ(a|x) the
by dµ
γ-discounted visiting distribution of state x and state-action pair (x  a) under policy µ  respectively.
Let Z be a bounded-mean random variable  i.e.  E[|Z|] < ∞  with the cumulative distribution
function F (z) = P(Z ≤ z) (e.g.  one may think of Z as the loss of an investment strategy µ). We

k=0 γkP(xk = x|x0 = x0; µ) and πµ

γ (x  a|x0) = dµ

Here the minimum is attained because F is non-decreasing and right-continuous in z. When F
is continuous and strictly increasing  VaRα(Z) is the unique z satisfying F (z) = α  otherwise 
the VaR equation can have no solution or a whole range of solutions. Although VaR is a popular
risk measure  it suffers from being unstable and difﬁcult to work with numerically when Z is not

2

normally distributed  which is often the case as loss distributions tend to exhibit fat tails or empirical
discreteness. Moreover  VaR is not a coherent risk measure [1] and more importantly does not
quantify the losses that might be suffered beyond its value at the α-tail of the distribution [23].
An alternative measure that addresses most of the VaR’s shortcomings is conditional value-at-risk 
CVARα(Z)  which is the mean of the α-tail distribution of Z. If there is no probability atom at

VaRα(Z)  CVaRα(Z) has a unique value that is deﬁned as CVaRα(Z) = E(cid:2)Z | Z ≥ VaRα(Z)(cid:3).

Rockafellar and Uryasev [23] showed that

CVaRα(Z) = min

(1)
where (x)+ = max(x  0) represents the positive part of x. Note that as a function of ν  Hα(·  ν) is
ﬁnite and convex (hence continuous).

ν∈R Hα(Z  ν)

1 − α

ν +

.

(cid:110)

(cid:52)
= min
ν∈R

E(cid:2)(Z − ν)+(cid:3)(cid:111)

1

3 CVaR Optimization in MDPs

k=0 γkC(xk  ak) | x0 = x  µ and Dθ(x  a) = (cid:80)∞

policy µ  i.e.  Dθ(x) = (cid:80)∞
functions of policy µ  i.e.  V θ(x) = E(cid:2)Dθ(x)(cid:3) and Qθ(x  a) = E(cid:2)Dθ(x  a)(cid:3). The goal in the

For a policy µ  we deﬁne the loss of a state x (state-action pair (x  a)) as the sum of (discounted)
costs encountered by the agent when it starts at state x (state-action pair (x  a)) and then follows
k=0 γkC(xk  ak) | x0 =
x  a0 = a  µ. The expected value of these two random variables are the value and action-value
standard discounted formulation is to ﬁnd an optimal policy θ∗ = argminθ V θ(x0).
For CVaR optimization in MDPs  we consider the following optimization problem: For a given
conﬁdence level α ∈ (0  1) and loss tolerance β ∈ R 

V θ(x0)

min

θ

subject to

CVaRα

V θ(x0)

min
θ ν

subject to

Hα

(cid:0)Dθ(x0)(cid:1) ≤ β.
(cid:0)Dθ(x0)  ν(cid:1) ≤ β.

(2)

(3)

(4)

By Theorem 16 in [23]  the optimization problem (2) is equivalent to (Hα is deﬁned by (1))

To solve (3)  we employ the Lagrangian relaxation procedure [4] to convert it to the following
unconstrained problem:

(cid:18)

max
λ≥0

min
θ ν

L(θ  ν  λ)

(cid:52)
= V θ(x0) + λ

Hα

(cid:16)

(cid:0)Dθ(x0)  ν(cid:1) − β

(cid:17)(cid:19)

 

where λ is the Lagrange multiplier. The goal here is to ﬁnd the saddle point of L(θ  ν  λ)  i.e.  a point
(θ∗  ν∗  λ∗) that satisﬁes L(θ  ν  λ∗) ≥ L(θ∗  ν∗  λ∗) ≥ L(θ∗  ν∗  λ) ∀θ  ν ∀λ ≥ 0. This is achieved by
descending in (θ  ν) and ascending in λ using the gradients of L(θ  ν  λ) w.r.t. θ  ν  and λ  i.e. 1

(cid:18)

∇θL(θ  ν  λ) = ∇θV θ(x0) +

∂ν L(θ  ν  λ) = λ

1 +

∇λL(θ  ν  λ) = ν +

1

(1 − α)
1

(1 − α)

 

λ

(1 − α)

∇θE(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105)
(cid:18)
∂νE(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105)(cid:19)
E(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105) − β.
(cid:0)Dθ(x0)(cid:1) ≤ β (feasibility assump-

P(cid:0)Dθ(x0) ≥ ν(cid:1)(cid:19)

(1 − α)

(cid:51) λ

1 −

(7)

(5)

 

(6)

1

We assume that there exists a policy µ(·|·; θ) such that CVaRα
tion). As discussed in Section 1  B¨auerle and Ott [20  3] showed that there exists a deterministic
history-dependent optimal policy for CVaR optimization. The important point is that this policy
does not depend on the complete history  but only on the current time step k  current state of the

system xk  and accumulated discounted cost(cid:80)k

i=0 γiC(xi  ai).

In the following  we present a policy gradient (PG) algorithm (Sec. 4) and several actor-critic (AC)
algorithms (Sec. 5) to optimize (4). While the PG algorithm updates its parameters after observing
several trajectories  the AC algorithms are incremental and update their parameters at each time-step.

1The notation (cid:51) in (6) means that the right-most term is a member of the sub-gradient set ∂ν L(θ  ν  λ).

3

4 A Trajectory-based Policy Gradient Algorithm
In this section  we present a policy gradient algorithm to solve the optimization problem (4). The
unit of observation in this algorithm is a system trajectory generated by following the current policy.
At each iteration  the algorithm generates N trajectories by following the current policy  use them
to estimate the gradients in Eqs. 5-7  and then use these estimates to update the parameters θ  ν  λ.
Let ξ = {x0  a0  x1  a1  . . .   xT−1  aT−1  xT} be a trajectory generated by following the policy θ 
where x0 = x0 and xT is usually a terminal state of the system. After xk visits the terminal state 
it enters a recurring sink state xS at the next time step  incurring zero cost  i.e.  C(xS   a) = 0 
∀a ∈ A. Time index T is referred to as the stopping time of the MDP. Since the transition is
stochastic  T is a non-deterministic quantity. Here we assume that the policy µ is proper  i.e. 
P(xk = x|x0 = x0  µ) < ∞ for every x (cid:54)∈ {xS}. This further means that with probability 1 
the MDP exits the transient states and hits xS (and stays in xS) in ﬁnite time T . For simplicity  we
assume that the agent incurs zero cost at the terminal state. Analogous results for the general case
with a non-zero terminal cost can be derived using identical arguments. The loss and probability of ξ
k=0 µ(ak|xk; θ)P (xk+1|xk  ak) 

(cid:80)∞
are deﬁned as D(ξ) =(cid:80)T−1
respectively. It can be easily shown that ∇θ log Pθ(ξ) =(cid:80)T−1

k=0 γkc(xk  ak) and Pθ(ξ) = P0(x0)(cid:81)T−1

k=0 ∇θ log µ(ak|xk; θ).

k=0

Algorithm 1 contains the pseudo-code of our proposed policy gradient algorithm. What appears
inside the parentheses on the right-hand-side of the update equations are the estimates of the gradi-
ents of L(θ  ν  λ) w.r.t. θ  ν  λ (estimates of Eqs. 5-7) (see Appendix A.2 of [13]). Γθ is an operator
that projects a vector θ ∈ Rκ1 to the closest point in a compact and convex set Θ ⊂ Rκ1  and
Γν and Γλ are projection operators to [− Cmax
1−γ ] and [0  λmax]  respectively. These projection
operators are necessary to ensure the convergence of the algorithm. The step-size schedules satisfy
the standard conditions for stochastic approximation algorithms  and ensure that the VaR parameter

ν update is on the fastest time-scale(cid:8)ζ3(i)(cid:9)  the policy parameter θ update is on the intermediate
time-scale(cid:8)ζ2(i)(cid:9)  and the Lagrange multiplier λ update is on the slowest time-scale(cid:8)ζ1(i)(cid:9) (see

Appendix A.1 of [13] for the conditions on the step-size schedules). This results in a three time-
scale stochastic approximation algorithm. We prove that our policy gradient algorithm converges to
a (local) saddle point of the risk-sensitive objective function L(θ  ν  λ) (see Appendix A.3 of [13]).

1−γ   Cmax

Algorithm 1 Trajectory-based Policy Gradient Algorithm for CVaR Optimization

Input: parameterized policy µ(·|·; θ)  conﬁdence level α  and loss tolerance β
Initialization: policy parameter θ = θ0  VaR parameter ν = ν0  and the Lagrangian parameter λ = λ0
for i = 0  1  2  . . . do
for j = 1  2  . . . do

j=1 by starting at x0 = x0 and following the current policy θi.

Generate N trajectories {ξj i}N

end for

N(cid:88)

1(cid:8)D(ξj i) ≥ νi

(cid:9)(cid:19)(cid:21)

νi − ζ3(i)

λi −

λi

ν Update: νi+1 = Γν

θ Update:

θi+1 = Γθ

θi − ζ2(i)

(cid:20)
(cid:20)

(cid:18)
(cid:18) 1

N

j=1

(1 − α)N

N(cid:88)
∇θ log Pθ(ξj i)|θ=θi D(ξj i)
N(cid:88)

j=1

j=1

∇θ log Pθ(ξj i)|θ=θi
N(cid:88)

1

(1 − α)N

j=1

(cid:0)D(ξj i) − νi
(cid:0)D(ξj i) − νi

(cid:9)(cid:19)(cid:21)
(cid:1)1(cid:8)D(ξj i) ≥ νi
(cid:9)(cid:19)(cid:21)
(cid:1)1(cid:8)D(ξj i) ≥ νi

+

(cid:20)

λi

(1 − α)N

(cid:18)

λi + ζ1(i)

νi − β +

λ Update: λi+1 = Γλ

end for
return parameters ν  θ  λ

Incremental Actor-Critic Algorithms

5
As mentioned in Section 4  the unit of observation in our policy gradient algorithm (Algorithm 1) is
a system trajectory. This may result in high variance for the gradient estimates  especially when the
length of the trajectories is long. To address this issue  in this section  we propose two actor-critic

4

algorithms that use linear approximation for some quantities in the gradient estimates and update the
parameters incrementally (after each state-action transition). We present two actor-critic algorithms
for optimizing the risk-sensitive measure (4). These algorithms are based on the gradient estimates
of Sections 5.1-5.3. While the ﬁrst algorithm (SPSA-based) is fully incremental and updates all the
parameters θ  ν  λ at each time-step  the second one updates θ at each time-step and updates ν and λ
only at the end of each trajectory  thus given the name semi trajectory-based. Algorithm 2 contains
the pseudo-code of these algorithms. The projection operators Γθ  Γν  and Γλ are deﬁned as in
Section 4 and are necessary to ensure the convergence of the algorithms. The step-size schedules
satisfy the standard conditions for stochastic approximation algorithms  and ensures that the critic

update is on the fastest time-scale(cid:8)ζ4(i)(cid:9)  the policy and VaR parameter updates are on the interme-
diate time-scale  with ν-update(cid:8)ζ3(i)(cid:9) being faster than θ-update(cid:8)ζ2(i)(cid:9)  and ﬁnally the Lagrange
multiplier update is on the slowest time-scale(cid:8)ζ1(i)(cid:9) (see Appendix B.1 of [13] for the conditions

on these step-size schedules). This results in four time-scale stochastic approximation algorithms.
We prove that these actor-critic algorithms converge to a (local) saddle point of the risk-sensitive
objective function L(θ  ν  λ) (see Appendix B.4 of [13]).
5.1 Gradient w.r.t. the Policy Parameters θ
The gradient of our objective function w.r.t. the policy parameters θ in (5) may be rewritten as

(cid:18)
E(cid:2)Dθ(x0)(cid:3) +

λ

∇θL(θ  ν  λ) = ∇θ

(1 − α)

(cid:26) λ(−s)+/(1 − α)

(8)
Given the original MDP M = (X  A  C  P  P0) and the parameter λ  we deﬁne the augmented MDP
¯M = ( ¯X   ¯A  ¯C  ¯P   ¯P0) as ¯X = X × R  ¯A = A  ¯P0(x  s) = P0(x)1{s0 = s}  and

if s(cid:48) =(cid:0)s − C(x  a)(cid:1)/γ
k=0 γkC(xk  ak)(cid:1).
We deﬁne a class of parameterized stochastic policies(cid:8)µ(·|x  s; θ)  (x  s) ∈ ¯X   θ ∈ Θ ⊆ Rκ1(cid:9) for

¯C(x  s  a) =
where xT is any terminal state of the original MDP M and sT is the value of the s part of the state
when a policy θ reaches a terminal state xT after T steps  i.e.  sT = 1
γT

if x = xT
(cid:48)
otherwise   ¯P (x

(cid:48)|x  s  a) =

otherwise

C(x  a)

  s

0

this augmented MDP. Thus  the total (discounted) loss of this trajectory can be written as

E(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105)(cid:19)
(cid:26) P (x(cid:48)|x  a)

.

(cid:0)ν −(cid:80)T−1
(cid:0)Dθ(x0) − ν(cid:1)+.

T−1(cid:88)

k=0

γkC(xk  ak) + γT ¯C(xT   sT   a) = Dθ(x0) +

λ

(1 − α)

(9)

From (9)  it is clear that the quantity in the parenthesis of (8) is the value function of the policy θ at
state (x0  ν) in the augmented MDP ¯M  i.e.  V θ(x0  ν). Thus  it is easy to show that (the second
equality in Eq. 10 is the result of the policy gradient theorem [21])

∇θL(θ  ν  λ) = ∇θV θ(x0  ν) =

1
1 − γ

γ(x  s  a|x0  ν) ∇ log µ(a|x  s; θ) Qθ(x  s  a) 
πθ

(10)

(cid:88)

x s a

where πθ
function of policy θ in the augmented MDP ¯M. We can show that

γ is the discounted visiting distribution (deﬁned in Section 2) and Qθ is the action-value
an unbiased estimate of ∇θL(θ  ν  λ)  where δk = ¯C(xk  sk  ak) + γ(cid:98)V (xk+1  sk+1) − (cid:98)V (xk  sk)
1−γ∇ log µ(ak|xk  sk; θ) · δk is
is the temporal-difference (TD) error in ¯M  and (cid:98)V is an unbiased estimator of V θ (see e.g.  [6  7]).
v(cid:62)φ(x  s) = (cid:101)V θ v(x  s)  where the feature vector φ(·) belongs to the low-dimensional space Rκ2.

In our actor-critic algorithms  the critic uses linear approximation for the value function V θ(x  s) ≈

1

5.2 Gradient w.r.t. the Lagrangian Parameter λ
We may rewrite the gradient of our objective function w.r.t. the Lagrangian parameters λ in (7) as
∇λL(θ  ν  λ) = ν − β +∇λ
= ν − β +∇λV θ(x0  ν). (11)

E(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105)(cid:19) (a)

(cid:18)
E(cid:2)Dθ(x0)(cid:3) +

λ

(1 − α)

Similar to Section 5.1  (a) comes from the fact that the quantity in the parenthesis in (11) is
V θ(x0  ν)  the value function of the policy θ at state (x0  ν) in the augmented MDP ¯M. Note that
the dependence of V θ(x0  ν) on λ comes from the deﬁnition of the cost function ¯C in ¯M. We now
derive an expression for ∇λV θ(x0  ν)  which in turn will give us an expression for ∇λL(θ  ν  λ).

5

(cid:88)

Lemma 1 The gradient of V θ(x0  ν) w.r.t. the Lagrangian parameter λ may be written as

1
1 − γ

∇λV θ(x0  ν) =

γ(x  s  a|x0  ν)
πθ

1

1{x = xT}(−s)+.

1

x s a

(1 − α)

(12)
(cid:4)
Proof. See Appendix B.2 of [13].
(1−γ)(1−α) 1{x = xT}(−s)+ is an unbiased
From Lemma 1 and (11)  it is easy to see that ν − β +
estimate of ∇λL(θ  ν  λ). An issue with this estimator is that its value is ﬁxed to νk − β all along
a system trajectory  and only changes at the end to νk − β +
(1−γ)(1−α) (−sT )+. This may affect
the incremental nature of our actor-critic algorithm. To address this issue  we propose a different
approach to estimate the gradients w.r.t. θ and λ in Sec. 5.4 (of course this does not come for free).
Another important issue is that the above estimator is unbiased only if the samples are generated
(1−α) 1{xk =
from the distribution πθ
xT}(−sk)+ as an estimate for ∇λL(θ  ν  λ). Note that this is an issue for all discounted actor-critic
algorithms that their (likelihood ratio based) estimate for the gradient is unbiased only if the samples
are generated from πθ
γ  and not when we simply follow the policy. This might be a reason that we
have no convergence analysis (to the best of our knowledge) for (likelihood ratio based) discounted
actor-critic algorithms.2
5.3 Sub-Gradient w.r.t. the VaR Parameter ν
We may rewrite the sub-gradient of our objective function w.r.t. the VaR parameter ν (Eq. 6) as

γ(·|x0  ν). If we just follow the policy  then we may use νk−β+ γk

1

(cid:18)

P(cid:16) ∞(cid:88)

(cid:17)(cid:19)

(cid:16)

P(cid:0)sT ≤ 0 | x0 = x0  s0 = ν; θ(cid:1)(cid:17)

∂ν L(θ  ν  λ) (cid:51) λ

1 −

(13)
From the deﬁnition of the augmented MDP ¯M  the probability in (13) may be written as P(sT ≤
0 | x0 = x0  s0 = ν; θ)  where sT is the s part of the state in ¯M when we reach a terminal state 
i.e.  x = xT (see Section 5.1). Thus  we may rewrite (13) as

(1 − α)

k=0

.

γkC(xk  ak) ≥ ν | x0 = x0; θ

1

∂ν L(θ  ν  λ) (cid:51) λ

1 −

1

.

(1 − α)

(14)
From (14)  it is easy to see that λ− λ1{sT ≤ 0}/(1− α) is an unbiased estimate of the sub-gradient
of L(θ  ν  λ) w.r.t. ν. An issue with this (unbiased) estimator is that it can be only applied at the
end of a system trajectory (i.e.  when we reach the terminal state xT )  and thus  using it prevents
us of having a fully incremental algorithm. In fact  this is the estimator that we use in our semi
trajectory-based actor-critic algorithm.
One approach to estimate this sub-gradient incrementally is to use simultaneous perturbation
stochastic approximation (SPSA) method [8]. The idea of SPSA is to estimate the sub-gradient
g(ν) ∈ ∂νL(θ  ν  λ) using two values of g at ν− = ν − ∆ and ν+ = ν + ∆  where ∆ > 0 is a
positive perturbation (see [8  17] for the detailed description of ∆).3 In order to see how SPSA can
help us to estimate our sub-gradient incrementally  note that

(cid:18)

E(cid:2)Dθ(x0)(cid:3) +

λ

(1 − α)

E(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105)(cid:19) (a)

= λ + ∂ν V θ(x0  ν).

(15)

∂ν L(θ  ν  λ) = λ + ∂ν

Similar to Sections 5.1  (a) comes from the fact that the quantity in the parenthesis in (15) is
V θ(x0  ν)  the value function of the policy θ at state (x0  ν) in the augmented MDP ¯M. Since
the critic uses a linear approximation for the value function  i.e.  V θ(x  s) ≈ v(cid:62)φ(x  s)  in our
actor-critic algorithms (see Section 5.1 and Algorithm 2)  the SPSA estimate of the sub-gradient

would be of the form g(ν) ≈ λ + v(cid:62)(cid:2)φ(x0  ν+) − φ(x0  ν−)(cid:3)/2∆.

5.4 An Alternative Approach to Compute the Gradients
In this section  we present an alternative way to compute the gradients  especially those w.r.t. θ and
λ. This allows us to estimate the gradient w.r.t. λ in a (more) incremental fashion (compared to the
method of Section 5.3)  with the cost of the need to use two different linear function approximators

2Note that the discounted actor-critic algorithm with convergence proof in [5] is based on SPSA.
3SPSA-based gradient estimate was ﬁrst proposed in [25] and has been widely used in various settings 
especially those involving high-dimensional parameter. The SPSA estimate described above is two-sided. It
can also be implemented single-sided  where we use the values of the function at ν and ν+. We refer the readers
to [8] for more details on SPSA and to [17] for its application in learning in risk-sensitive MDPs.

6

(cid:26) (−s)+/(1 − α)

0

(instead of one used in Algorithm 2).
In this approach  we deﬁne the augmented MDP slightly
different than the one in Section 5.3. The only difference is in the deﬁnition of the cost function 
which is deﬁned here as (note that C(x  a) has been replaced by 0 and λ has been removed)

if x = xT  
otherwise 

¯C(x  s  a) =

E(cid:104)(cid:0)Dθ(x0) − ν(cid:1)+(cid:105)

1

It is easy to see that he term
appearing in the gradients of Eqs. 5-7 is the value function of the pol-

where xT is any terminal state of the original MDP M.
(1−α)
icy θ at state (x0  ν) in this augmented MDP. As a result  we have
Gradient w.r.t. θ: It is easy to see that now this gradient (Eq. 5) is the gradient of the value function
of the original MDP  ∇θV θ(x0)  plus λ times the gradient of the value function of the augmented
MDP  ∇θV θ(x0  ν)  both at the initial states of these MDPs (with abuse of notation  we use V
for the value function of both MDPs). Thus  using linear approximators u(cid:62)f (x  s) and v(cid:62)φ(x  s)
for the value functions of the original and augmented MDPs  ∇θL(θ  ν  λ) can be estimated as
∇θ log µ(ak|xk  sk; θ) · (k + λδk)  where k and δk are the TD-errors of these MDPs.
Gradient w.r.t. λ: Similar to the case for θ  it is easy to see that this gradient (Eq. 7) is ν − β plus
the value function of the augmented MDP  V θ(x0  ν)  and thus  can be estimated incrementally as
∇λL(θ  ν  λ) ≈ ν − β + v(cid:62)φ(x  s).
Sub-Gradient w.r.t. ν: This sub-gradient (Eq. 6) is λ times one plus the gradient w.r.t. ν of the
value function of the augmented MDP  ∇νV θ(x0  ν)  and thus  it can be estimated incrementally

v(cid:62)(cid:2)φ(x0 ν+)−φ(x0 ν−)(cid:3)

using SPSA as λ(cid:0)1 +

(cid:1) .

2∆

Algorithm 3 in Appendix B.3 of [13] contains the pseudo-code of the resulting algorithm.
Algorithm 2 Actor-Critic Algorithms for CVaR Optimization

Input: Parameterized policy µ(·|·; θ) and value function feature vector φ(·) (both over the augmented
MDP ¯M)  conﬁdence level α  and loss tolerance β
Initialization: policy parameters θ = θ0; VaR parameter ν = ν0; Lagrangian parameter λ = λ0; value
function weight vector v = v0
// (1) SPSA-based Algorithm:
for k = 0  1  2  . . . do

Draw action ak ∼ µ(·|xk  sk; θk);
Observe next state (xk+1  sk+1) ∼ ¯P (·|xk  sk  ak);

TD Error:
Critic Update:

δk = ¯C(xk  sk  ak) + γv
vk+1 = vk + ζ4(k)δkφ(xk  sk)

k φ(xk+1  sk+1) − v
(cid:62)

v(cid:62)

(cid:16)

νk − ζ3(k)

(cid:32)
(cid:16)
(cid:16)
λk + ζ1(k)(cid:0)νk − β +

θk − ζ2(k)
1 − γ

λk +

k

ν Update: νk+1 = Γν

θ Update:

θk+1 = Γθ

λ Update: λk+1 = Γλ

(cid:62)
k φ(xk  sk)

Observe cost ¯C(xk  sk  ak) (with λ = λk);

// note that sk+1 = (sk − C(cid:0)xk  ak)(cid:1)/γ
(cid:17)(cid:33)
(cid:1) − φ(x0  νk − ∆k)(cid:3)
(cid:2)φ(cid:0)x0  νk + ∆k
(cid:17)
1{xk = xT}(−sk)+(cid:1)(cid:17)

2∆k

1

(1 − α)(1 − γ)

(16)
(17)

(18)

(19)

(20)

∇θ log µ(ak|xk  sk; θ) · δk

then set (xk+1  sk+1) = (x0  νk+1)

if xk = xT (reach a terminal state) 

end for
// (2) Semi Trajectory-based Algorithm:
for k = 0  1  2  . . . do

if xk (cid:54)= xT then

Draw action ak ∼ µ(·|xk  sk; θk)  observe cost ¯C(xk  sk  ak) (with λ = λk)  and next state
(xk+1  sk+1) ∼ ¯P (·|xk  sk  ak); Update (δk  vk  θk  λk) using Eqs. 16  17  19  and 20

else

Update (δk  vk  θk  λk) using Eqs. 16  17  19  and 20; Update ν as
λk − λk
1 − α

ν Update: νk+1 = Γν

νk − ζ3(k)

(cid:18)

(cid:16)

1(cid:8)sT ≤ 0(cid:9)(cid:17)(cid:19)

(21)

Set (xk+1  sk+1) = (x0  νk+1)

end if
end for
return policy and value function parameters θ  ν  λ  v

7

6 Experimental Results
We consider an optimal stopping problem in which the state at each time step k ≤ T consists of the
cost ck and time k  i.e.  x = (ck  k)  where T is the stopping time. The agent (buyer) should decide
either to accept the present cost or wait. If she accepts or when k = T   the system reaches a terminal
state and the cost ck is received  otherwise  she receives the cost ph and the new state is (ck+1  k+1) 
where ck+1 is fuck w.p. p and fdck w.p. 1 − p (fu > 1 and fd < 1 are constants). Moreover  there
is a discounted factor γ ∈ (0  1) to account for the increase in the buyer’s affordability. The problem
has been described in more details in Appendix C of [13]. Note that if we change cost to reward
and minimization to maximization  this is exactly the American option pricing problem  a standard
testbed to evaluate risk-sensitive algorithms (e.g.  [26]). Since the state space is continuous  ﬁnding
an exact solution via DP is infeasible  and thus  it requires approximation and sampling techniques.
We compare the performance of our risk-sensitive policy gradient Algorithm 1 (PG-CVaR) and two
actor-critic Algorithms 2 (AC-CVaR-SPSA AC-CVaR-Semi-Traj) with their risk-neutral counterparts
(PG and AC) (see Appendix C of [13] for the details of these experiments). Figure 1 shows the
distribution of the discounted cumulative cost Dθ(x0) for the policy θ learned by each of these
algorithms. The results indicate that the risk-sensitive algorithms yield a higher expected loss  but
less variance  compared to the risk-neutral methods. More precisely  the loss distributions of the risk-
sensitive algorithms have lower right-tail than their risk-neutral counterparts. Table 1 summarizes
the performance of these algorithms. The numbers reiterate what we concluded from Figure 1.

Figure 1: Loss distributions for the policies learned by the risk-sensitive and risk-neutral policy
gradient and actor critic algorithms. The two left ﬁgures correspond to the PG methods  and the two
right ﬁgures correspond to the AC algorithms. In all cases  the loss tolerance equals to β = 40.

PG-CVaR

PG

AC

AC-CVaR-SPSA

AC-CVaR-Semi-Traj.

E(Dθ(x0))

16.08
19.75
16.96
22.86
23.01

σ(Dθ(x0)) CVaR(Dθ(x0))

17.53
7.06
32.09
3.40
4.98

69.18
25.75
122.61
31.36
34.81

Table 1: Performance comparison for the policies learned by the risk-sensitive and risk-neutral algorithms.

7 Conclusions and Future Work
We proposed novel policy gradient and actor critic (AC) algorithms for CVaR optimization in MDPs.
We provided proofs of convergence (in [13]) to locally risk-sensitive optimal policies for the pro-
posed algorithms. Further  using an optimal stopping problem  we observed that our algorithms
resulted in policies whose loss distributions have lower right-tail compared to their risk-neutral
counterparts. This is extremely important for a risk averse decision-maker  especially if the right-
tail contains catastrophic losses. Future work includes: 1) Providing convergence proofs for our
AC algorithms when the samples are generated by following the policy and not from its discounted
visiting distribution  2) Using importance sampling methods [2  27] to improve gradient estimates
in the right-tail of the loss distribution (worst-case events that are observed with low probability) of
the CVaR objective function  and 4) Evaluating our algorithms in more challenging problems.
Acknowledgement
for their comments that helped us with some technical details in the proofs of the algorithms.

The authors would like to thank Professor Marco Pavone and Lucas Janson

8

−40−20020406000.020.040.06RewardProbability Mean−CVaRMean−5005010000.050.10.15RewardProbability Mean−CVaRMean−CVaR SPSAMeanReferences
[1] P. Artzner  F. Delbaen  J. Eber  and D. Heath. Coherent measures of risk. Journal of Mathematical

Finance  9(3):203–228  1999.

[2] O. Bardou  N. Frikha  and G. Pag`es. Computing VaR and CVaR using stochastic approximation and
adaptive unconstrained importance sampling. Monte Carlo Methods and Applications  15(3):173–210 
2009.

[3] N. B¨auerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Mathematical

Methods of Operations Research  74(3):361–379  2011.

[4] D. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[5] S. Bhatnagar. An actor-critic algorithm with function approximation for discounted cost constrained

Markov decision processes. Systems & Control Letters  59(12):760–766  2010.

[6] S. Bhatnagar  R. Sutton  M. Ghavamzadeh  and M. Lee. Incremental natural actor-critic algorithms. In

Proceedings of Advances in Neural Information Processing Systems 20  pages 105–112  2008.

[7] S. Bhatnagar  R. Sutton  M. Ghavamzadeh  and M. Lee. Natural actor-critic algorithms. Automatica  45

(11):2471–2482  2009.

[8] S. Bhatnagar  H. Prasad  and L.A. Prashanth. Stochastic Recursive Algorithms for Optimization  volume

434. Springer  2013.

[9] K. Boda and J. Filar. Time consistent dynamic risk measures. Mathematical Methods of Operations

Research  63(1):169–186  2006.

[10] V. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems &

Control Letters  44:339–346  2001.

[11] V. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research  27:294–311  2002.
[12] V. Borkar and R. Jain. Risk-constrained Markov decision processes. IEEE Transaction on Automatic

Control  2014.

[13] Y. Chow  M. Ghavamzadeh  L. Janson  and M. Pavone. Algorithms for CVaR optimization in MDPs.

arXiv:1406.3339  2014.

[14] J. Filar  L. Kallenberg  and H. Lee. Variance-penalized Markov decision processes. Mathematics of

Operations Research  14(1):147–161  1989.

[15] J. Filar  D. Krass  and K. Ross. Percentile performance criteria for limiting average Markov decision

processes. IEEE Transaction of Automatic Control  40(1):2–10  1995.

[16] R. Howard and J. Matheson. Risk sensitive Markov decision processes. Management Science  18(7):

356–369  1972.

[17] Prashanth L.A. and M. Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In Proceedings

of Advances in Neural Information Processing Systems 26  pages 252–260  2013.

[18] H. Markowitz. Portfolio Selection: Efﬁcient Diversiﬁcation of Investment. John Wiley and Sons  1959.
[19] T. Morimura  M. Sugiyama  M. Kashima  H. Hachiya  and T. Tanaka. Nonparametric return distribu-
tion approximation for reinforcement learning. In Proceedings of the 27th International Conference on
Machine Learning  pages 799–806  2010.

[20] J. Ott. A Markov Decision Model for a Surveillance Application and Risk-Sensitive Markov Decision

Processes. PhD thesis  Karlsruhe Institute of Technology  2010.

[21] J. Peters  S. Vijayakumar  and S. Schaal. Natural actor-critic. In Proceedings of the Sixteenth European

Conference on Machine Learning  pages 280–291  2005.

[22] M. Petrik and D. Subramanian. An approximate solution method for large risk-averse Markov decision
processes. In Proceedings of the 28th International Conference on Uncertainty in Artiﬁcial Intelligence 
2012.

[23] R. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk  26:1443–1471 

2002.

[24] M. Sobel. The variance of discounted Markov decision processes. Applied Probability  pages 794–802 

1982.

[25] J. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.

IEEE Transactions on Automatic Control  37(3):332–341  1992.

[26] A. Tamar  D. Di Castro  and S. Mannor. Policy gradients with variance related risk criteria. In Proceedings

of the Twenty-Ninth International Conference on Machine Learning  pages 387–396  2012.

[27] A. Tamar  Y. Glassner  and S. Mannor. Policy gradients beyond expectations: Conditional value-at-risk.

arXiv:1404.3862v1  2014.

9

,Yinlam Chow
Mohammad Ghavamzadeh
Kevin Jamieson
Lalit Jain