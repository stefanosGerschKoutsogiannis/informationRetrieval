2019,On Single Source Robustness in Deep Fusion Models,Algorithms that fuse multiple input sources benefit from both complementary and shared information. Shared information may provide robustness against faulty or noisy inputs  which is indispensable for safety-critical applications like self-driving cars. We investigate learning fusion algorithms that are robust against noise added to a single source. We first demonstrate that robustness against single source noise is not guaranteed in a linear fusion model. Motivated by this discovery  two possible approaches are proposed to increase robustness: a carefully designed loss with corresponding training algorithms for deep fusion models  and a simple convolutional fusion layer that has a structural advantage in dealing with noise. Experimental results show that both training algorithms and our fusion layer make a deep fusion-based 3D object detector robust against noise applied to a single source  while preserving the original performance on clean data.,On Single Source Robustness in Deep Fusion Models

The University of Texas at Austin

The University of Texas at Austin

Joydeep Ghosh

Austin  TX

jghosh@utexas.edu

Taewan Kim∗

Austin  TX

twankim@utexas.edu

Abstract

Algorithms that fuse multiple input sources beneﬁt from both complementary and
shared information. Shared information may provide robustness against faulty or
noisy inputs  which is indispensable for safety-critical applications like self-driving
cars. We investigate learning fusion algorithms that are robust against noise added
to a single source. We ﬁrst demonstrate that robustness against single source
noise is not guaranteed in a linear fusion model. Motivated by this discovery  two
possible approaches are proposed to increase robustness: a carefully designed
loss with corresponding training algorithms for deep fusion models  and a simple
convolutional fusion layer that has a structural advantage in dealing with noise.
Experimental results show that both training algorithms and our fusion layer make
a deep fusion-based 3D object detector robust against noise applied to a single
source  while preserving the original performance on clean data.

1

Introduction

Deep learning models have accomplished superior performance in several machine learning problems
[26] including object recognition [24  40  42  15  18]  object detection [37  16  7  36  30  35] and
speech recognition [17  14  38  5  2  4]  which use either visual or audio sources. One natural way
of improving a model’s performance is to make use of multiple input sources relevant to a given
task so that enough information can be extracted to build strong features. Therefore  deep fusion
models have recently attracted considerable attention for autonomous driving [21  3  33  25]  medical
imaging [23  48  39  29]  and audio-visual speech recognition [19  32  41  6].
Two beneﬁts are expected when fusion-based learning models are selected for a given problem.
First  given adequate data  more information from multiple sources can enrich the model’s feature
space to achieve higher prediction performance  especially  when different input sources provide
complementary information to the model. This expectation coincides with a simple information
theoretic fact: if we have multiple input sources X1 ···   Xns and a target variable Y   mutual
information I(; ) obeys I(Y ; X1 ···   Xns ) ≥ I(Y ; Xi) (∀i ∈ [ns]).
The second expected advantage is increased robustness against single source faults  which is the
primary concern of our work. An underlying intuition comes from the fact that different sources may
have shared information so one sensor can partially compensate for others. This type of robustness
is critical in real-world fusion models  because each source may be exposed to different types of
corruption but not at the same time. For example  self-driving cars using an RGB camera and ranging
sensors like LIDAR and radar are exposed to single source corruption. LIDARs and radars work ﬁne
at night whereas RGB cameras do not. Also  each source used in the model may have its own sensing
device  and hence not necessarily be corrupted by some physical attack simultaneously with others.
It would be ideal if the structure of machine learning based fusion models and shared information
could compensate for the corruption and automatically guarantee robustness without additional steps.
∗This work was done when Taewan Kim was at the University of Texas at Austin  prior to joining Amazon.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

This paper shows that a fusion model needs a supplementary strategy and a specialized structure to
avoid vulnerability to noise or corruption on a single source. Our contributions are as follows:

• We show that a fusion model learned with a standard robustness is not guaranteed to provide
robustness against noise on a single source. Inspired by the analysis  a novel loss is proposed
to achieve the desired robustness (Section 3).

to ensure robustness without impacting performance on clean data (Section 4.1).

• Two efﬁcient training algorithms for minimizing our loss in deep fusion models are devised
• We introduce a simple but an effective fusion layer which naturally reduces error by applying

ensembling to latent convolutional features (Section 4.2).

We apply our loss and the fusion layer to a complex deep fusion-based 3D object detector used
in autonomous driving for further investigation in practice. Note that our ﬁndings can be easily
generalized to other applications exhibiting intermittent defects in a subset of input sources  e.g. 
robustness given k of ns corrupted sources  and single source robustness should be studied in depth
prior to more general cases.

2 Related Works

Deep fusion models have been actively studied in object detection for autonomous vehicles. There
exist two major streams classiﬁed according to their algorithmic structures: two-stage detectors with
R-CNN (Region-based Convolutional Neural Networks) technique [12  11  37  7  16]  and single
stage detectors for faster inference speed [36  35  30].
Earlier deep fusion models extended Fast R-CNN [11] to provide better quality of region proposals
from multiple sources [21  1]. With a high-resolution LIDAR  point cloud was used as a major source
of the region proposal stage before the fusion step [8]  whereas F-PointNet [33] used it for validating
2D proposals from RGB images and predicting 3D shape and location within the visual frustum.
MV3D [3] extended the idea of region proposal network (RPN) [37] by generating proposals from
RGB image  and LIDAR’s front view and BEV (bird’s eye view) maps. Recent works tried to remove
region proposal stages for faster inference and directly fused LIDAR’s front view depth image [22]
or BEV image [47] with RGB images. ContFuse [27] utilizes both RGB and LIDAR’s BEV images
with a new continuous fusion scheme  which is further improved in MMF [28] by handling multiple
tasks at once. Our experimental results are based on AVOD [25]  a recent open-sourced 3D object
detector that generates region proposals from RPN using RGB and LIDAR’s BEV images.
Compared to the active efforts in accomplishing higher performance on clean data  very few works
have focused on robust learning methods in multi-source settings to the best of our knowledge.
Adaptive fusion methods using gating networks weight the importance of each source automatically
[31  46]  but these works lack in-depth studies of the robustness against single source faults. A
recent work proposed a gated fusion at the feature level and applied data augmentation techniques
with randomly chosen corruption methods [20]. In contrast  our training algorithms are surrogate
minimization schemes for the proposed loss function  which is grounded from the analyses on
underlying weakness of fusion methods. Also the fusion layer proposed in this paper focuses more on
how to mix convolutional feature maps channel-wise with simple trainable procedures. For extensive
literature reviews  please refer to the recent survey papers about deep multi-modal learning methods
in general [34] and for autonomous driving [9].

3 Single Source Robustness of Fusion Models

3.1 Regression on linear fusion data

To show the vulnerability of naive fusion models  we introduce a simple data model and a fusion
algorithm. Suppose y is a linear function consisting of three different inherent (latent) components
zi ∈ Rdi (i ∈ {1  2  3}). There are two input sources  x1 and x2. Here ψ’s are unknown functions.
(1)

T zi  where z1 = ψ1(x1)  z2 = ψ2(x2)  z3 = ψ3 1(x1) = ψ3 2(x2)

y =

βi

3(cid:88)i=1

2

Our simple data model simulates a target variable y relevant to two different sources  where each
source has its own special information z1 and z2 and a shared one z3. For example  if two sources
are obtained from an RGB camera and a LIDAR sensor  one can imagine that any features related
to objectness are captured in z3 whereas colors and depth information may be located in z1 and z2 
respectively. Our objective is to build a regression model by effectively incorporating information
from the sources (x1  x2) to predict the target variable y.
Now  consider a fairly simple setting x1 = [z1; z3] ∈ Rd1+d3 and x2 = [z2; z3] ∈ Rd2+d3  where
(ψ1  ψ2  ψ3 1  ψ3 2) can be deﬁned accordingly to satisfy (1). A straightforward fusion approach is to
stack the sources  i.e. x = [x1; x2] ∈ Rd1+d2+2d3  and learn a linear model. Then  it is easy to show
that there exists a feasible error-free model for noise-free data:

fdirect(x1  x2) = hT

(2)
where h1 = [β1; g1]  h2 = [β2; g2]. Parameter vectors responsible for the shared information z3 are
denoted by g1 and g2.2

2 z3)  s.t. g1 + g2 = β3

1 z3) + (βT

2 z2 + gT

1 x1 + hT

2 x2 = (βT

1 z1 + gT

Unbalanced robustness (Motivation) Suppose the true parameters of data are scalar values  i.e.
βi = ci ∈ R and inﬂuence of the complementary information is relatively small  c1 ≈ c2 and c3 (cid:29) c1.
Assume that the obtained error-free solution’s parameters for z3 are unbalanced  i.e. g1 = ∆ and
g2 = c3 − ∆ with some weight parameter ∆ (cid:28) c3  so that g1 gives a negligible contribution. Then
add single source corruption δ1 = [1; 3] and δ2 = [2; 3] and compute absolute difference between
the true data y and the prediction from the corrupted data:

|y − fdirect(x1 + δ1  x2)| = |c11 + ∆3| 

|y − fdirect(x1  x2 + δ2)| = |c22 + (c3 − ∆)3|

In this case  adding noise to the source x2 will give signiﬁcant corruption to the prediction while x1
is relatively robust because |(c3 − ∆)3| (cid:29) |∆3| for any noise 3 affecting z3. This simple example
illustrates that additional training strategies or components are indispensable to achieve robust fusion
model working even if one of the sources is disturbed. The next section introduces a novel loss for a
balanced robustness against a fault in a single source.

3.2 Robust learning for single source noise

Fusion methods are not guaranteed to provide robustness against faults in a single source without
additional supervision. Also  we demonstrate that naive regularization or robust learning methods
are not sufﬁcient for the robustness later in this section. Therefore  a supplementary constraint or
strategy needs to be considered in training which can correctly guide learning parameters for the
desired robustness.
One essential requirement of fusion models is showing balanced performance regardless of corruption
added to any source. If the model is signiﬁcantly vulnerable to corruption in one source  this model
becomes untrustworthy and we need to balance the degradation levels of different input sources’
faults. For example  suppose there is a model robust against noise in RGB channels but shows huge
degradation in performance for any fault of LIDAR. Then the overall system should be considered
untrustworthy  because there exist certain corruption or environments which can consistently fool the
model. Our loss  MAXSSN (Maximum Single Source Noise)  for such robustness is introduced to
handle this issue and further analyses are provided under the linear fusion data model explained in
Section 3.1. This loss makes the model focus more on corruption of a single source  SSN  rather than
focusing on noise added to all the sources at once  ASN.
Deﬁnition 1. For multiple sources x1 ···   xns and a target variable y  denote a predeﬁned loss
function by L. If each source xi is perturbed with some additive noise i for i ∈ [ns]  MAXSSN loss
for a model f is deﬁned as follows:
(cid:20)h1
(cid:21)
i {L (y  f (x1 ···   xi−1  xi + i  xi+1 ···   xns))}ns
LMAXSSN(f  ) (cid:44) max
has to be solved for X1 ∈ Rn×(d1+d3)  X2 ∈ Rn×(d2+d3) and Y ∈ Rn
with enough number of n data samples. Then a standard least squares solution using a pseudo-inverse gives
h1 = [β1; β3/2]  h2 = [β2; β3/2]. This is equivalent to the solution robust against random noise added to all
the sources at once  which is vulnerable to single source faults (Section 3.2).

2In practice  Y = [X1  X2]

i=1

h2

3

Another key principle in our robust training is to retain the model’s performance on clean data.
Although techniques like data augmentation help improving a model’s generalization error in general 
learning a model robust against certain types perturbation including adversarial attacks may harm the
model’s accuracy on non-corrupt data [43]. Deterioration in the model’s ability on normal data is an
unwanted side effect  and hence our approach aims to avoid this.

Random noise To investigate the importance of our MAXSSN loss  we revisit the linear fusion
data model with the optimal direct fusion model fdirect of the regression problem introduced in
Section 3.1. Suppose the objective is to ﬁnd a model with robustness against single source noises 
while preserving error-free performance  i.e.  unchanged loss under clean data. For the noise model 
consider  = [δ1; δ2] where δ1 = [1; 3] and δ2 = [2; 4]  which satisfy E[i] = 0  V ar(i) = σ2I 
and E[iT
j ] = 0 for i (cid:54)= j. Note that noises added to the shared information  3 and 4  are not
identical  which resembles direct perturbation to the input sources in practice. For example  noise
directly affecting a camera lens does not need to perturb other sources.

2 z1 + gT

Optimal fusion model for MAXSSN The robust linear fusion model f (x1  x2) = (wT
1 z3)+
2 z3) is found by minimizing LMAXSSN(f  ) over parameters w1  w2  g1 and g2. As shown
(wT
in the previous section  any fdirect satisfying w1 = β1  w2 = β2 and g1 + g2 = β3 should achieve
zero-error. Therefore  overall optimization problem can be reduced to the following one:

1 z1+gT

min
g1 g2

1

2

2

2

2

(4)

2

2

2

otherwise

2

+ ||β3||2

2



s.t. g1 + g2 = β3

max{L (y  fdirect(x1 + δ1  x2))  L (y  fdirect(x1  x2 + δ2))}

(3)
If we use a standard expected squared loss L(y  f (x1  x2)) = E[(y − f (x1  x2))2] and solve the
optimization problem  the following solution L∗MAXSSN with corresponding parameters g∗1  g∗2 can be
obtained  and there exist three cases based on the relative sizes of ||βi||2’s.
if ||β1||2
2 + ||β3||2
if ||β2||2
2 + ||β3||2
(cid:17)  
2)2
2−||β1||2
4||β3||2
(cid:17)(cid:17)
2−||β1||2
||β3||2

2  β3  0(cid:1)
(cid:0)σ2||β2||2
2  0  β3(cid:1)
(cid:0)σ2||β1||2
(cid:16)σ2(cid:16)||β1||2
2(cid:16)1 + ||β2||2

(L∗MAXSSN  g∗1  g∗2) =

2 ≤ ||β2||2
2 ≤ ||β1||2

4 + (||β2||2

2−||β1||2
||β3||2

2+||β2||2
2

The three cases reﬂect the relative inﬂuence of each weight vector for zi. For instance  if z2 has
larger importance compared to the rest in generating y  the optimal way of balancing the effect
of noise over z3 is to remove all the inﬂuence of z2 in x2 by setting g2 = 0. When neither of

2(cid:16)1 − ||β2||2
(cid:17)   1
(cid:12)(cid:12)(cid:12) < 1  the optimal solution tries to make
z1 nor z2 dominates the importance  i.e. (cid:12)(cid:12)(cid:12)||β2||2

2−||β1||2
||β3||2
L (y  fdirect(x1 + δ1  x2)) = L (y  fdirect(x1  x2 + δ2)).
Comparison with the standard robust fusion model Minimizing loss with noise added to a
model’s input is a standard process in robust learning. The same strategy can be applied to learn
fusion models by considering all sources as a single combined source  then add noise to all the
sources at once. However  this simple strategy cannot achieve low error in terms of the single source
robustness. The optimal solution to ming1 g2 E[(y − fdirect(x1 + δ1  x2 + δ2))2]  a least squares
2 . The corresponding MAXSSN loss can be evaluated
solution  is achieved when g1 = g2 = β3
2(cid:9). A nontrivial gap exists between
as L(cid:48)MAXSSN = σ2 max(cid:8)||β1||2
2 ||β2||2
4||β3||2
LMAXSSN and L(cid:48)MAXSSN  which is directly proportional to the data model’s inherent characteristics:
if (cid:12)(cid:12)(cid:12)||β2||2
(5)

L(cid:48)MAXSSN − L∗MAXSSN ≥(cid:40) 1

If either z1 or z2 has more inﬂuence on the target value y than the other components  single source
robustness of the model trained by MAXSSN loss is better than the fusion model for the general noise
robustness with an amount proportional to the inﬂuence of shared feature z3. Otherwise  the gap’s
2|/4.
lower bound is proportional to the difference in complementary information  |||β2||2
Remark 1. In linear systems such as the one studied above  having redundant information in the
feature space is similar to multicollinearity in statistics. In this case  feature selection methods usually
try to remove such redundancy. However  this redundant or shared information helps preventing
degradation of the fusion model when a subset of the input sources are corrupted.
Remark 2. Similar analyses and a loss deﬁnition against adversarial attacks [13] are provided in
appendix A.2.

4(cid:12)(cid:12)||β2||2

(cid:12)(cid:12)(cid:12) ≥ 1

2−||β1||2
||β3||2

2 − ||β1||2

2 − ||β1||2

4||β3||2

2

4||β3||2

otherwise

2(cid:12)(cid:12)

2 + 1

2 + 1

1

2

2

2

2

4

4 Robust Deep Fusion Models

In simple linear settings  our analyses illustrate that using MAXSSN loss can effectively minimize
the degradation of a fusion model’s performance. This suggests a training strategy for complex
deep fusion models to be equipped with robustness against single source faults. A principal factor
considered in designing a common framework for our algorithms is the preservation of model’s
performance on clean data while minimizing a loss for defending corruption. Therefore  our training
algorithms use data augmentation to encounter both clean and corrupted data. The second way of
achieving robustness is to take advantage of the fusion method’s structure. A simple but effective
method of mixing convolutional features coming from different input sources is introduced later in
this section.

4.1 Robust training algorithms for single source noise

In the previous section  we solve problem (3) by optimizing over ﬂexible parameters g1 and g2. If
the parts of input sources contributing to z3 are known  then indeed we can achieve this goal. In
practice however  it is difﬁcult to know which parts of an input source (or latent representation) are
related to shared information and which parameters are ﬂexible. Therefore  our common training
framework alternately provides clean samples and corrupted samples per iteration to preserve the
original performance of the model on uncontaminated data.3 On top of this strategy  one standard
robust training scheme and two algorithms for minimizing MAXSSN loss are introduced for handling
robustness against noise in different sources.

Standard robust training method A standard robust training algorithm can be developed by
considering all ns sources as a single combined source. Given noise generating functions ϕi(·) (i ∈
[ns])  the algorithm generates and adds corruption to all the sensors at once. Then the corresponding
loss can be computed to update parameters using back-propagation. This algorithm is denoted by
TRAINASN and tested in experiments to investigate whether the procedure is also able to cover
robustness against single source noise.

Algorithm 1 TRAINSSN

Algorithm 2 TRAINSSNALT

for iiter = 1 to m do
Sample (y {xi}ns
i=1)
if iiter ≡ 1 (mod 2) then
for j = 1 to ns do

Generate noise j = ϕj(xj)
ˆL(iiter)
j ← L(y  f ({xj + j  x−j}))
end for
L(iiter) ← maxj ˆL(iiter)
else
L(iiter) ← L(y  f ({xi}ns

i=1))

j

end if
Update f using ∇L(iiter)

end for

for iiter = 1 to m do
Sample (y {xi}ns
i=1)
if iiter ≡ 1 (mod 2) then
j ← ((cid:98)iiter/2(cid:99) mod ns) + 1
Generate noise j = ϕj(xj)
L(iiter) ← L(y  f ({xj + j  x−j}))
L(iiter) ← L(y  f ({xi}ns

i=1))

else

end if
Update f using ∇L(iiter)

end for

Minimization of MAXSSN loss Minimization of the MAXSSN loss requires ns (number of
input sources) forward-propagations within one iteration. Each propagation needs a different set
of corrupted samples generated by adding single source noise to the ﬁxed clean mini-batch of
data. There are two possible approaches to compute gradients properly from these multiple passes.
First  we can run back-propagation ns times to save the gradients temporarily without updating
any parameters  then the saved gradients with the maximum loss is used for updating parameters.
3We also try ﬁne-tuning only a subset of the model’s parameters  θfusion ⊂ f  to preserve essential parts for
extracting features from normal data. Although this strategy is similar to optimizing over only g1 and g2 in our
linear fusion case  training the whole network from the beginning shows better performance in practice. See
Appendix B for a detailed comparison.

5

However  this process requires not only ns forward and backward passes but also large memory
usage proportional to ns for saving the gradients. Another reasonable approach is to run ns forward
passes to ﬁnd the maximum loss and compute gradients by going back to the corresponding set
of corrupted samples. Algorithm 1 adopts this idea for its efﬁciency  ns + 1 forward passes and
one back-propagation. A faster version of the algorithm  TRAINSSNALT  is also considered since
multiple forward passes may take longer as the number of sources increases. This algorithm ignores
the maximum loss and alternately augments corrupted data. By a slight abuse of notation  symbols
used in our algorithms also represent the iteration steps with the size of mini-batches greater than one.
Also  f (x1 ···   xj−1  xj + j  xj+1 ···   xns) is shortened to f ({xj + j  x−j}) in the algorithms.

4.2 Feature fusion methods

Fusion of features extracted from multiple input sources can be done in various ways [3]. One of
the popular methods is to fuse via an element-wise mean operation [25]  but this assumes that each
feature must have a same shape  i.e.  width  height  and number of channels for a 3D feature. An
element-wise mean can be also viewed as averaging channels from different 3D features  and it has
an underlying assumption that the channels of each feature should share same information regardless
of the input source origin. Therefore  the risk of becoming vulnerable to single source corruption
may increase with this simple mean fusion method.

Figure 1: Latent ensemble layer (LEL)

Our fusion method  latent ensemble layer (LEL)  is devised for three objectives: (i) maintaining the
known advantage—error reduction—of ensemble methods [45  44]  (ii) admitting source-speciﬁc
features to survive even after the fusion procedure  and (iii) allowing each source to provide a different
number of channels. The proposed layer learns parameters so that channels of the 3D features from
the different sources can be selectively mixed. Sparse constraints are introduced to let the training
procedure ﬁnd good subsets of channels to be fused across the ns feature maps. For example  mixing
the ith channel of the convolutional feature from an RGB image with the jth and kth channels of the
LIDAR’s latent feature is possible in our LEL  whereas in an element-wise mean layer the ith latent
channel from RGB is only mixed with the other sources’ ith channels.
In practice  this layer can be easily constructed by using 1 × 1 convolutions with the ReLU activation
and (cid:96)1 constraints. We also apply an activation function to supplement a semi-adaptive behavior to the
fusion procedure. Depth of the output channel is set to ˆd = maxi{di} and we set the hyper-parameter
for (cid:96)1 constraint as 0.01 in the experiments. Deﬁnition 2 explains the details of our LEL  and Figure
1 visualizes the overall process.
Deﬁnition 2 (Latent ensemble layer). Suppose we have ns convolutional features zi ∈ Ra×b×di
from different input sources (i ∈ [ns])  which can be stacked as z = (z1 ···   zm) ∈ Ra×b×dsum
(dsum =(cid:80)m
i=1 di). The kth channel of the stacked feature is denoted by [z]k ∈ Ra×b. Let wj =
1  ···   w(j)
(w(j)
dsum) be a dsum-dimensional weight vector to mix zi’s in channel-wise fashion. Then LEL
k [z]k(cid:17) 
outputs ˆz ∈ Ra×b× ˆd where each channel is computed as [ˆz]j = φ(wj (cid:12) z) (cid:44) φ(cid:16)(cid:80)dsum
with some activation function φ and sparse constraints ||wj||0 ≤ t for all j ∈ {1 ···   ˆd}.

k=1 w(j)

6

𝐳"𝑎𝑏𝑑"𝐳&’𝑎𝑏𝑑&’⋮Latent	EnsembleLayer(LEL)8𝐳𝑎𝑏9𝑑wj=(w(j)1 ··· w(j)dsum)<latexit sha1_base64="BBTAsFOT1Xx2/xDgG+MWxu6q1Ww=">AAAFhnicbdTdbtMwFABgb6wwys82uOQmYkLq0FQ1E2jcIHVi2rjYRen6u6ZEjuOu3pw4sx2yzso78DTcwmvwNiRrJqcxlqIcne8c23EkexElQrZaf9fWH23UHj/ZfFp/9vzFy63tnVcDwWKOcB8xyvjIgwJTEuK+JJLiUcQxDDyKh971l9yHPzAXhIU9uYjwNICXIZkRBGWWcrffO17iXn1uJK79XTWu9tJ9B/lMiv3EVb6rRByk6RL23O3dVrN1PywzsItgFxSj4+5sHDs+Q3GAQ4koFGJityI5VZBLgihO604scATRNbzEkywMYYDFVN1/VGq9yzK+NWM8e0Jp3WfLHQoGQiwCL6sMoJyLquXJ/9kklrNPU0XCKJY4RMuFZjG1JLPyE7J8wjGSdJEFEHGS7dVCc8ghktk5rszUs6cq31w+zcryQUwl4SxJ63UnxAliQQBDXznebaqcfEfeTN2maQUXGhcGCo3CwCONRwZyjdzASGNk4I3GGwOTDD1G/fwfMKoSo6Cvu/sGDjQODBxqHBo40jgycKxxbOC5xnMD7zTeGdhZ/dZOtaD70Oypblr96eisUASpOqu2onFJjT2j05KeGnpS0hNz4W6Ju0bzqKTGSaKLkl48TC3nmHEcqOKdql4RrKiPZyQk+f2STorcVB3r5EpxxFnERLW6U8pm945dvWXMYHDQtLP424fd9kFxA22CN+AtaAAbHII2+Ao6oA8Q+Al+gd/gT22z1qx9rB0uS9fXip7XYGXU2v8AZQ3/iA==</latexit><latexit sha1_base64="BBTAsFOT1Xx2/xDgG+MWxu6q1Ww=">AAAFhnicbdTdbtMwFABgb6wwys82uOQmYkLq0FQ1E2jcIHVi2rjYRen6u6ZEjuOu3pw4sx2yzso78DTcwmvwNiRrJqcxlqIcne8c23EkexElQrZaf9fWH23UHj/ZfFp/9vzFy63tnVcDwWKOcB8xyvjIgwJTEuK+JJLiUcQxDDyKh971l9yHPzAXhIU9uYjwNICXIZkRBGWWcrffO17iXn1uJK79XTWu9tJ9B/lMiv3EVb6rRByk6RL23O3dVrN1PywzsItgFxSj4+5sHDs+Q3GAQ4koFGJityI5VZBLgihO604scATRNbzEkywMYYDFVN1/VGq9yzK+NWM8e0Jp3WfLHQoGQiwCL6sMoJyLquXJ/9kklrNPU0XCKJY4RMuFZjG1JLPyE7J8wjGSdJEFEHGS7dVCc8ghktk5rszUs6cq31w+zcryQUwl4SxJ63UnxAliQQBDXznebaqcfEfeTN2maQUXGhcGCo3CwCONRwZyjdzASGNk4I3GGwOTDD1G/fwfMKoSo6Cvu/sGDjQODBxqHBo40jgycKxxbOC5xnMD7zTeGdhZ/dZOtaD70Oypblr96eisUASpOqu2onFJjT2j05KeGnpS0hNz4W6Ju0bzqKTGSaKLkl48TC3nmHEcqOKdql4RrKiPZyQk+f2STorcVB3r5EpxxFnERLW6U8pm945dvWXMYHDQtLP424fd9kFxA22CN+AtaAAbHII2+Ao6oA8Q+Al+gd/gT22z1qx9rB0uS9fXip7XYGXU2v8AZQ3/iA==</latexit><latexit sha1_base64="BBTAsFOT1Xx2/xDgG+MWxu6q1Ww=">AAAFhnicbdTdbtMwFABgb6wwys82uOQmYkLq0FQ1E2jcIHVi2rjYRen6u6ZEjuOu3pw4sx2yzso78DTcwmvwNiRrJqcxlqIcne8c23EkexElQrZaf9fWH23UHj/ZfFp/9vzFy63tnVcDwWKOcB8xyvjIgwJTEuK+JJLiUcQxDDyKh971l9yHPzAXhIU9uYjwNICXIZkRBGWWcrffO17iXn1uJK79XTWu9tJ9B/lMiv3EVb6rRByk6RL23O3dVrN1PywzsItgFxSj4+5sHDs+Q3GAQ4koFGJityI5VZBLgihO604scATRNbzEkywMYYDFVN1/VGq9yzK+NWM8e0Jp3WfLHQoGQiwCL6sMoJyLquXJ/9kklrNPU0XCKJY4RMuFZjG1JLPyE7J8wjGSdJEFEHGS7dVCc8ghktk5rszUs6cq31w+zcryQUwl4SxJ63UnxAliQQBDXznebaqcfEfeTN2maQUXGhcGCo3CwCONRwZyjdzASGNk4I3GGwOTDD1G/fwfMKoSo6Cvu/sGDjQODBxqHBo40jgycKxxbOC5xnMD7zTeGdhZ/dZOtaD70Oypblr96eisUASpOqu2onFJjT2j05KeGnpS0hNz4W6Ju0bzqKTGSaKLkl48TC3nmHEcqOKdql4RrKiPZyQk+f2STorcVB3r5EpxxFnERLW6U8pm945dvWXMYHDQtLP424fd9kFxA22CN+AtaAAbHII2+Ao6oA8Q+Al+gd/gT22z1qx9rB0uS9fXip7XYGXU2v8AZQ3/iA==</latexit><latexit sha1_base64="BBTAsFOT1Xx2/xDgG+MWxu6q1Ww=">AAAFhnicbdTdbtMwFABgb6wwys82uOQmYkLq0FQ1E2jcIHVi2rjYRen6u6ZEjuOu3pw4sx2yzso78DTcwmvwNiRrJqcxlqIcne8c23EkexElQrZaf9fWH23UHj/ZfFp/9vzFy63tnVcDwWKOcB8xyvjIgwJTEuK+JJLiUcQxDDyKh971l9yHPzAXhIU9uYjwNICXIZkRBGWWcrffO17iXn1uJK79XTWu9tJ9B/lMiv3EVb6rRByk6RL23O3dVrN1PywzsItgFxSj4+5sHDs+Q3GAQ4koFGJityI5VZBLgihO604scATRNbzEkywMYYDFVN1/VGq9yzK+NWM8e0Jp3WfLHQoGQiwCL6sMoJyLquXJ/9kklrNPU0XCKJY4RMuFZjG1JLPyE7J8wjGSdJEFEHGS7dVCc8ghktk5rszUs6cq31w+zcryQUwl4SxJ63UnxAliQQBDXznebaqcfEfeTN2maQUXGhcGCo3CwCONRwZyjdzASGNk4I3GGwOTDD1G/fwfMKoSo6Cvu/sGDjQODBxqHBo40jgycKxxbOC5xnMD7zTeGdhZ/dZOtaD70Oypblr96eisUASpOqu2onFJjT2j05KeGnpS0hNz4W6Ju0bzqKTGSaKLkl48TC3nmHEcqOKdql4RrKiPZyQk+f2STorcVB3r5EpxxFnERLW6U8pm945dvWXMYHDQtLP424fd9kFxA22CN+AtaAAbHII2+Ao6oA8Q+Al+gd/gT22z1qx9rB0uS9fXip7XYGXU2v8AZQ3/iA==</latexit>j2{1 ··· ˆd}<latexit sha1_base64="3zmZC5IlqhpKIBJNs8pcryF4v48=">AAAFdnicbdRPb9MwFABwb7Qwyp91cAMJRVQTHKYpmZDguIlp47BD6fp3TVU5jrOaOXFmO3SdFYlPwxW+Dt+EI0mXyUmMpShP7/ee7TiSvZgSIW37z8bmg0bz4aOtx60nT589327vvBgKlnCEB4hRxsceFJiSCA8kkRSPY45h6FE88q4+5z76jrkgLOrLVYxnIbyMSEAQlFlq3n71zXJJZLnK2XORz6TYcxdQKj9103m7Y+/b62GZgVMEHVCM7nyncez6DCUhjiSiUIipY8dypiCXBFGcttxE4BiiK3iJp1kYwRCLmVp/RGrtZhnfChjPnkha62y5Q8FQiFXoZZUhlAtRtzz5P5smMvg0UySKE4kjdLdQkFBLMis/EcsnHCNJV1kAESfZXi20gBwimZ1bZaa+M1P55vJpKsuHCZWEs2XaarkRXiIWhjDylevdpMrNd+QF6iZNa7jSuDJQaBQGHmk8MpBr5AbGGmMDrzVeG7jM0GPUz/8Bo2ppFAx098DAocahgSONIwPHGscGTjRODDzXeG7grcZbA7vVb+3WC3r3zZ7qpfWfjs4KRZCqs3ormpTU2DM6LempoSclPTEX7pW4ZzSPS2qcJLoo6cX91HKBGcehKt6p6hdBRX0ckIjk90k6LXIzdayTleKYs5iJenW3lM3uHad+y5jB8GDfyeKvHzqHB8UNtAVeg7fgPXDAR3AIvoAuGAAEfoCf4Bf43fjbfNPcbb67K93cKHpegspo2v8AC+D6Bw==</latexit><latexit sha1_base64="3zmZC5IlqhpKIBJNs8pcryF4v48=">AAAFdnicbdRPb9MwFABwb7Qwyp91cAMJRVQTHKYpmZDguIlp47BD6fp3TVU5jrOaOXFmO3SdFYlPwxW+Dt+EI0mXyUmMpShP7/ee7TiSvZgSIW37z8bmg0bz4aOtx60nT589327vvBgKlnCEB4hRxsceFJiSCA8kkRSPY45h6FE88q4+5z76jrkgLOrLVYxnIbyMSEAQlFlq3n71zXJJZLnK2XORz6TYcxdQKj9103m7Y+/b62GZgVMEHVCM7nyncez6DCUhjiSiUIipY8dypiCXBFGcttxE4BiiK3iJp1kYwRCLmVp/RGrtZhnfChjPnkha62y5Q8FQiFXoZZUhlAtRtzz5P5smMvg0UySKE4kjdLdQkFBLMis/EcsnHCNJV1kAESfZXi20gBwimZ1bZaa+M1P55vJpKsuHCZWEs2XaarkRXiIWhjDylevdpMrNd+QF6iZNa7jSuDJQaBQGHmk8MpBr5AbGGmMDrzVeG7jM0GPUz/8Bo2ppFAx098DAocahgSONIwPHGscGTjRODDzXeG7grcZbA7vVb+3WC3r3zZ7qpfWfjs4KRZCqs3ormpTU2DM6LempoSclPTEX7pW4ZzSPS2qcJLoo6cX91HKBGcehKt6p6hdBRX0ckIjk90k6LXIzdayTleKYs5iJenW3lM3uHad+y5jB8GDfyeKvHzqHB8UNtAVeg7fgPXDAR3AIvoAuGAAEfoCf4Bf43fjbfNPcbb67K93cKHpegspo2v8AC+D6Bw==</latexit><latexit sha1_base64="3zmZC5IlqhpKIBJNs8pcryF4v48=">AAAFdnicbdRPb9MwFABwb7Qwyp91cAMJRVQTHKYpmZDguIlp47BD6fp3TVU5jrOaOXFmO3SdFYlPwxW+Dt+EI0mXyUmMpShP7/ee7TiSvZgSIW37z8bmg0bz4aOtx60nT589327vvBgKlnCEB4hRxsceFJiSCA8kkRSPY45h6FE88q4+5z76jrkgLOrLVYxnIbyMSEAQlFlq3n71zXJJZLnK2XORz6TYcxdQKj9103m7Y+/b62GZgVMEHVCM7nyncez6DCUhjiSiUIipY8dypiCXBFGcttxE4BiiK3iJp1kYwRCLmVp/RGrtZhnfChjPnkha62y5Q8FQiFXoZZUhlAtRtzz5P5smMvg0UySKE4kjdLdQkFBLMis/EcsnHCNJV1kAESfZXi20gBwimZ1bZaa+M1P55vJpKsuHCZWEs2XaarkRXiIWhjDylevdpMrNd+QF6iZNa7jSuDJQaBQGHmk8MpBr5AbGGmMDrzVeG7jM0GPUz/8Bo2ppFAx098DAocahgSONIwPHGscGTjRODDzXeG7grcZbA7vVb+3WC3r3zZ7qpfWfjs4KRZCqs3ormpTU2DM6LempoSclPTEX7pW4ZzSPS2qcJLoo6cX91HKBGcehKt6p6hdBRX0ckIjk90k6LXIzdayTleKYs5iJenW3lM3uHad+y5jB8GDfyeKvHzqHB8UNtAVeg7fgPXDAR3AIvoAuGAAEfoCf4Bf43fjbfNPcbb67K93cKHpegspo2v8AC+D6Bw==</latexit><latexit sha1_base64="3zmZC5IlqhpKIBJNs8pcryF4v48=">AAAFdnicbdRPb9MwFABwb7Qwyp91cAMJRVQTHKYpmZDguIlp47BD6fp3TVU5jrOaOXFmO3SdFYlPwxW+Dt+EI0mXyUmMpShP7/ee7TiSvZgSIW37z8bmg0bz4aOtx60nT589327vvBgKlnCEB4hRxsceFJiSCA8kkRSPY45h6FE88q4+5z76jrkgLOrLVYxnIbyMSEAQlFlq3n71zXJJZLnK2XORz6TYcxdQKj9103m7Y+/b62GZgVMEHVCM7nyncez6DCUhjiSiUIipY8dypiCXBFGcttxE4BiiK3iJp1kYwRCLmVp/RGrtZhnfChjPnkha62y5Q8FQiFXoZZUhlAtRtzz5P5smMvg0UySKE4kjdLdQkFBLMis/EcsnHCNJV1kAESfZXi20gBwimZ1bZaa+M1P55vJpKsuHCZWEs2XaarkRXiIWhjDylevdpMrNd+QF6iZNa7jSuDJQaBQGHmk8MpBr5AbGGmMDrzVeG7jM0GPUz/8Bo2ppFAx098DAocahgSONIwPHGscGTjRODDzXeG7grcZbA7vVb+3WC3r3zZ7qpfWfjs4KRZCqs3ormpTU2DM6LempoSclPTEX7pW4ZzSPS2qcJLoo6cX91HKBGcehKt6p6hdBRX0ckIjk90k6LXIzdayTleKYs5iJenW3lM3uHad+y5jB8GDfyeKvHzqHB8UNtAVeg7fgPXDAR3AIvoAuGAAEfoCf4Bf43fjbfNPcbb67K93cKHpegspo2v8AC+D6Bw==</latexit>1)	Concatenation2)	1x1 Convolution with sparse constraints𝐳"𝐳&’⋯[z]k2Ra⇥b<latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="ZMbiAovKCqgADSwdiDxCCt4WFJ8=">AAAFTHicbdRPb9MwFABwd7RQwoDtzCViQuI0JVzgyMS0cdghdP27Npocx1mtOXFmO3SdlS/AlU+H+DI4XSYnMZaqPr3fe/ZLIjnKKRHS8/709p71B89fDF86r/ad12/eHuxPBSs4whPEKOPzCApMSYYnkkiK5znHMI0onkW33yqf/cRcEJaN5TbHYQpvMpIQBKVOBdcHR96xt1uuHfh1cATqdX3YP13FDBUpziSiUIil7+UyVJBLgigunVUhcA7RLbzBSx1mMMUiVLs5S/eDzsRuwrj+ZdLdZZsdCqZCbNNIV6ZQrkXXquT/bFnI5EuoSJYXEmfo8aCkoK5kbvXQbkw4RpJudQARJ3pWF60hh0jqV9PaaeyHqhqu2qZ1fFpQSTjblI6zyvAGsTSFWaxW0X2pVtVEUaLuy7KDW4NbC4VBYeGJwRMLuUFuYW4wt/DO4J2FG40Ro3H1DRhVG6tgYronFk4NTi2cGZxZODc4t3BhcGHhpcFLCx8MPlgYtJ816BaMnpojNSq7Hx1d1IogVRfdVrRoqDUzOm/ouaVnDT2zDx41eGQ1zxtqvUl01dCrp63lGjOOU1X/l2pcBy2NcUIyUl0Z5bLOherUJFvFOWc5E93qoJHV147fvWTsYPrp2NfxDw8MwTvwHnwEPvgMvoLvIAATgEAMfoHf/b+D3mD4eD3t9ep76hC01sD5B0VE7aU=</latexit><latexit sha1_base64="2zwzsB9XIVqZiAm5org/3Ovsu9o=">AAAFaXicbdRPb9MwFABwb7QwyoCOExIcIqZJnKaECxyZmDYOO5Su/7YmVI7jrlYdO7MdutYKn4YrfB++CGeSkslJjKUoT+/nZzuO9MKEEqlc9/fO7oNW++GjvcedJ/tPnz3vHuyPJE8FwkPEKReTEEpMCcNDRRTFk0RgGIcUj8Plp8LH37CQhLOBWic4iOENI3OCoMpTs+7LqR9ugtnSJ8zvf9XQVyTG0gmzWffQPXa3w7EDrwwOQTl6s4PWqR9xlMaYKUShlFPPTVSgoVAEUZx1/FTiBKIlvMHTPGQw3yjQ20/InKM8EzlzLvKHKWebrVZoGEu5jsN8ZgzVQjatSP7Ppqmafwg0YUmqMEP/Npqn1FHcKe7DiYjASNF1HkAkSH5WBy2ggEjlt1ZbaeAFujhcsUxt+ziligi+yjodn+EV4nEMWaT98C7TfnGicK7vsqyBa4NrC6VBaeGJwRMLhUFhYWIwsfDW4K2FqxxDTqPiH3CqV9aEoakeWjgyOLJwbHBs4cTgxMIrg1cWXhq8tHBjcGNhr/6tveaE/n1xqPtZ86eji1IRpPqiWYquKmqdGZ1X9NzSs4qe2Rv3K9y3iicVtW4SXVf0+n5ptcBc4FiX70wPyqCmEZ4TRopukk3LXKBPTbI2ORE84bI5u1fJ5n3Ha3YZOxi9O/by+IsL9sAr8Aa8BR54Dz6Cz6AHhgCB7+AH+Al+tf60X7fLHrW7U7aqF6A22kd/AfHp+O4=</latexit><latexit sha1_base64="2zwzsB9XIVqZiAm5org/3Ovsu9o=">AAAFaXicbdRPb9MwFABwb7QwyoCOExIcIqZJnKaECxyZmDYOO5Su/7YmVI7jrlYdO7MdutYKn4YrfB++CGeSkslJjKUoT+/nZzuO9MKEEqlc9/fO7oNW++GjvcedJ/tPnz3vHuyPJE8FwkPEKReTEEpMCcNDRRTFk0RgGIcUj8Plp8LH37CQhLOBWic4iOENI3OCoMpTs+7LqR9ugtnSJ8zvf9XQVyTG0gmzWffQPXa3w7EDrwwOQTl6s4PWqR9xlMaYKUShlFPPTVSgoVAEUZx1/FTiBKIlvMHTPGQw3yjQ20/InKM8EzlzLvKHKWebrVZoGEu5jsN8ZgzVQjatSP7Ppqmafwg0YUmqMEP/Npqn1FHcKe7DiYjASNF1HkAkSH5WBy2ggEjlt1ZbaeAFujhcsUxt+ziligi+yjodn+EV4nEMWaT98C7TfnGicK7vsqyBa4NrC6VBaeGJwRMLhUFhYWIwsfDW4K2FqxxDTqPiH3CqV9aEoakeWjgyOLJwbHBs4cTgxMIrg1cWXhq8tHBjcGNhr/6tveaE/n1xqPtZ86eji1IRpPqiWYquKmqdGZ1X9NzSs4qe2Rv3K9y3iicVtW4SXVf0+n5ptcBc4FiX70wPyqCmEZ4TRopukk3LXKBPTbI2ORE84bI5u1fJ5n3Ha3YZOxi9O/by+IsL9sAr8Aa8BR54Dz6Cz6AHhgCB7+AH+Al+tf60X7fLHrW7U7aqF6A22kd/AfHp+O4=</latexit><latexit sha1_base64="cUEU4gZsFYq4RZcC8qSSsCt8sPE=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZrSXcZxE9PGYYfS9e+aUDmOs1p14sx26ForfBqu8H34IpxJukxOYixFeXq/92zHkezFlAhp2392dh81mo+f7D1tPXv+4uWr9v7rkWAJR3iIGGV84kGBKYnwUBJJ8STmGIYexWNv+Tn38XfMBWHRQK5j7IbwJiIBQVBmqXn77czxNu586ZDI6X9T0JEkxMLy0nm7Yx/a22GZQbcIOqAYvfl+48zxGUpCHElEoRCzrh1LV0EuCaI4bTmJwDFES3iDZ1kYwWwhV20/IbUOsoxvBYxnTyStbbbcoWAoxDr0ssoQyoWoW578n80SGXxyFYniROII3S8UJNSSzMrPw/IJx0jSdRZAxEm2VwstIIdIZqdWmWnQdVW+uXyayvJhQiXhbJW2Wk6EV4iFIYx85Xh3qXLyHXmBukvTGq41rg0UGoWBpxpPDeQauYGxxtjAW423Bq4y9Bj183/AqFoZBUPdPTRwpHFk4Fjj2MCJxomBU41TA680Xhm40bgxsFf91l69oP/Q7Kl+Wv/p6LJQBKm6rLeiaUmNPaOLkl4Yel7Sc3Phfon7RvOkpMZJouuSXj9MLReYcRyq4p2qQRFU1McBiUh+m6SzIueqM52sFMecxUzUq3ulbHbvdOu3jBmMjg67WfzV7pwcFTfQHngHPoCPoAuOwQn4AnpgCBD4AX6CX+B342/zfbPTPLgv3d0pet6Aymge/gO9wvm9</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit><latexit sha1_base64="h8Vx0MFprOJrleQWPPMAmMO/75g=">AAAFdHicbdRPb9MwFABwb7Qwyr8OLkhwiKgmcZqaCQmOm5g2DjuUrn/XhMpxnNWqE2e2Q9da4dNwhe/DF+FM0mVyEmMpytP7vWc7jmQvpkTIbvfPzu6DRvPho73HrSdPnz1/0d5/ORIs4QgPEaOMTzwoMCURHkoiKZ7EHMPQo3jsLT/nPv6OuSAsGsh1jN0QXkckIAjKLDVvv5453sadLx0SOf1vCjqShFhYXjpvd7qH3e2wzMAugg4oRm++3zh1fIaSEEcSUSjEzO7G0lWQS4IoTltOInAM0RJe41kWRjBbyFXbT0itgyzjWwHj2RNJa5stdygYCrEOvawyhHIh6pYn/2ezRAafXEWiOJE4QncLBQm1JLPy87B8wjGSdJ0FEHGS7dVCC8ghktmpVWYa2K7KN5dPU1k+TKgknK3SVsuJ8AqxMISRrxzvNlVOviMvULdpWsO1xrWBQqMw8ETjiYFcIzcw1hgbeKPxxsBVhh6jfv4PGFUro2Cou4cGjjSODBxrHBs40TgxcKpxauClxksDNxo3Bvaq39qrF/Tvmz3VT+s/HV0UiiBVF/VWNC2psWd0XtJzQ89KemYu3C9x32ielNQ4SXRV0qv7qeUCM45DVbxTNSiCivo4IBHJb5N0VuRcdaqTleKYs5iJenWvlM3uHbt+y5jB6OjQzuKvHzrHR8UNtAfegHfgPbDBR3AMvoAeGAIEfoCf4Bf43fjbfNvsNA/uSnd3ip5XoDKah/8AvwL5wQ==</latexit>dsum=nsXi=1di<latexit sha1_base64="u2+FxLCKBXNELuBazQE0HTVdmF4=">AAAFfnicbdRPb9MwFABwb6wwyp91cOQSrYC4MBqExC6TNjFtHHYoXf+uKZHjuKs1J85sh7azfOfTcIWvwrch6TI5jbEU5en93rMdR3KQUCJkq/V3Y/PBVu3ho+3H9SdPnz3faey+6AuWcoR7iFHGhwEUmJIY9ySRFA8TjmEUUDwIrr/kPviBuSAs7splgicRvIrJlCAos5Tf2At95Um8kEqkkdaHXvbyFTl09XcV+0KHPvEbzdZ+azUcO3CLoAmK0fZ3t068kKE0wrFEFAoxdluJnCjIJUEU67qXCpxAdA2v8DgLYxhhMVGrj9HOmywTOlPGsyeWzipb7lAwEmIZBVllBOVMVC1P/s/GqZweTBSJk1TiGN0tNE2pI5mTn4wTEo6RpMssgIiTbK8OmkEOkczOb22mrjtR+ebyadaWj1IqCWdzXa97MZ4jFkUwDpUXLLTy8h0FU7XQuoJLg0sLhUFh4bHBYwu5QW5hYjCx8MbgjYXzDANGw/wfMKrmVkHPdPcs7BvsWzgwOLBwaHBo4cjgyMILgxcW3hq8tbC9/q3takHnvjlQHV396ei8UASpOq+2olFJrT2js5KeWXpa0lN74U6JO1bzsKTWSaLLkl7eTy1nmHEcqeKtVbcI1jTEUxKT/F7R4yI3UScmuVaccJYwUa1ul7LZveNWbxk76H/cd7P426fm0eviBtoGr8AeeAdc8Bkcga+gDXoAgZ/gF/gN/tRA7W3tfe3DXenmRtHzEqyN2sE/Ci79zQ==</latexit><latexit sha1_base64="u2+FxLCKBXNELuBazQE0HTVdmF4=">AAAFfnicbdRPb9MwFABwb6wwyp91cOQSrYC4MBqExC6TNjFtHHYoXf+uKZHjuKs1J85sh7azfOfTcIWvwrch6TI5jbEU5en93rMdR3KQUCJkq/V3Y/PBVu3ho+3H9SdPnz3faey+6AuWcoR7iFHGhwEUmJIY9ySRFA8TjmEUUDwIrr/kPviBuSAs7splgicRvIrJlCAos5Tf2At95Um8kEqkkdaHXvbyFTl09XcV+0KHPvEbzdZ+azUcO3CLoAmK0fZ3t068kKE0wrFEFAoxdluJnCjIJUEU67qXCpxAdA2v8DgLYxhhMVGrj9HOmywTOlPGsyeWzipb7lAwEmIZBVllBOVMVC1P/s/GqZweTBSJk1TiGN0tNE2pI5mTn4wTEo6RpMssgIiTbK8OmkEOkczOb22mrjtR+ebyadaWj1IqCWdzXa97MZ4jFkUwDpUXLLTy8h0FU7XQuoJLg0sLhUFh4bHBYwu5QW5hYjCx8MbgjYXzDANGw/wfMKrmVkHPdPcs7BvsWzgwOLBwaHBo4cjgyMILgxcW3hq8tbC9/q3takHnvjlQHV396ei8UASpOq+2olFJrT2js5KeWXpa0lN74U6JO1bzsKTWSaLLkl7eTy1nmHEcqeKtVbcI1jTEUxKT/F7R4yI3UScmuVaccJYwUa1ul7LZveNWbxk76H/cd7P426fm0eviBtoGr8AeeAdc8Bkcga+gDXoAgZ/gF/gN/tRA7W3tfe3DXenmRtHzEqyN2sE/Ci79zQ==</latexit><latexit sha1_base64="u2+FxLCKBXNELuBazQE0HTVdmF4=">AAAFfnicbdRPb9MwFABwb6wwyp91cOQSrYC4MBqExC6TNjFtHHYoXf+uKZHjuKs1J85sh7azfOfTcIWvwrch6TI5jbEU5en93rMdR3KQUCJkq/V3Y/PBVu3ho+3H9SdPnz3faey+6AuWcoR7iFHGhwEUmJIY9ySRFA8TjmEUUDwIrr/kPviBuSAs7splgicRvIrJlCAos5Tf2At95Um8kEqkkdaHXvbyFTl09XcV+0KHPvEbzdZ+azUcO3CLoAmK0fZ3t068kKE0wrFEFAoxdluJnCjIJUEU67qXCpxAdA2v8DgLYxhhMVGrj9HOmywTOlPGsyeWzipb7lAwEmIZBVllBOVMVC1P/s/GqZweTBSJk1TiGN0tNE2pI5mTn4wTEo6RpMssgIiTbK8OmkEOkczOb22mrjtR+ebyadaWj1IqCWdzXa97MZ4jFkUwDpUXLLTy8h0FU7XQuoJLg0sLhUFh4bHBYwu5QW5hYjCx8MbgjYXzDANGw/wfMKrmVkHPdPcs7BvsWzgwOLBwaHBo4cjgyMILgxcW3hq8tbC9/q3takHnvjlQHV396ei8UASpOq+2olFJrT2js5KeWXpa0lN74U6JO1bzsKTWSaLLkl7eTy1nmHEcqeKtVbcI1jTEUxKT/F7R4yI3UScmuVaccJYwUa1ul7LZveNWbxk76H/cd7P426fm0eviBtoGr8AeeAdc8Bkcga+gDXoAgZ/gF/gN/tRA7W3tfe3DXenmRtHzEqyN2sE/Ci79zQ==</latexit><latexit sha1_base64="u2+FxLCKBXNELuBazQE0HTVdmF4=">AAAFfnicbdRPb9MwFABwb6wwyp91cOQSrYC4MBqExC6TNjFtHHYoXf+uKZHjuKs1J85sh7azfOfTcIWvwrch6TI5jbEU5en93rMdR3KQUCJkq/V3Y/PBVu3ho+3H9SdPnz3faey+6AuWcoR7iFHGhwEUmJIY9ySRFA8TjmEUUDwIrr/kPviBuSAs7splgicRvIrJlCAos5Tf2At95Um8kEqkkdaHXvbyFTl09XcV+0KHPvEbzdZ+azUcO3CLoAmK0fZ3t068kKE0wrFEFAoxdluJnCjIJUEU67qXCpxAdA2v8DgLYxhhMVGrj9HOmywTOlPGsyeWzipb7lAwEmIZBVllBOVMVC1P/s/GqZweTBSJk1TiGN0tNE2pI5mTn4wTEo6RpMssgIiTbK8OmkEOkczOb22mrjtR+ebyadaWj1IqCWdzXa97MZ4jFkUwDpUXLLTy8h0FU7XQuoJLg0sLhUFh4bHBYwu5QW5hYjCx8MbgjYXzDANGw/wfMKrmVkHPdPcs7BvsWzgwOLBwaHBo4cjgyMILgxcW3hq8tbC9/q3takHnvjlQHV396ei8UASpOq+2olFJrT2js5KeWXpa0lN74U6JO1bzsKTWSaLLkl7eTy1nmHEcqeKtVbcI1jTEUxKT/F7R4yI3UScmuVaccJYwUa1ul7LZveNWbxk76H/cd7P426fm0eviBtoGr8AeeAdc8Bkcga+gDXoAgZ/gF/gN/tRA7W3tfe3DXenmRtHzEqyN2sE/Ci79zQ==</latexit>Learnable	parametersdsum<latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="ZMbiAovKCqgADSwdiDxCCt4WFJ8=">AAAFTHicbdRPb9MwFABwd7RQwoDtzCViQuI0JVzgyMS0cdghdP27Npocx1mtOXFmO3SdlS/AlU+H+DI4XSYnMZaqPr3fe/ZLIjnKKRHS8/709p71B89fDF86r/ad12/eHuxPBSs4whPEKOPzCApMSYYnkkiK5znHMI0onkW33yqf/cRcEJaN5TbHYQpvMpIQBKVOBdcHR96xt1uuHfh1cATqdX3YP13FDBUpziSiUIil7+UyVJBLgigunVUhcA7RLbzBSx1mMMUiVLs5S/eDzsRuwrj+ZdLdZZsdCqZCbNNIV6ZQrkXXquT/bFnI5EuoSJYXEmfo8aCkoK5kbvXQbkw4RpJudQARJ3pWF60hh0jqV9PaaeyHqhqu2qZ1fFpQSTjblI6zyvAGsTSFWaxW0X2pVtVEUaLuy7KDW4NbC4VBYeGJwRMLuUFuYW4wt/DO4J2FG40Ro3H1DRhVG6tgYronFk4NTi2cGZxZODc4t3BhcGHhpcFLCx8MPlgYtJ816BaMnpojNSq7Hx1d1IogVRfdVrRoqDUzOm/ouaVnDT2zDx41eGQ1zxtqvUl01dCrp63lGjOOU1X/l2pcBy2NcUIyUl0Z5bLOherUJFvFOWc5E93qoJHV147fvWTsYPrp2NfxDw8MwTvwHnwEPvgMvoLvIAATgEAMfoHf/b+D3mD4eD3t9ep76hC01sD5B0VE7aU=</latexit><latexit sha1_base64="x5+RN/uP5kxsM2gwn9PgzPbc0Nc=">AAAFXXicbdTPbtMwGABwb7QwyoCOC0JcIiYkTlPCZRyZmDYOO5Suf9eEynHc1ZodZ7ZD21l5D67wVrwCT0HSZXISYynKp+/3+U8c6QsTSqRy3T87u49a7cdP9p52nu0/f/Gye7A/kjwVCA8Rp1xMQigxJTEeKqIoniQCQxZSPA5vvhQ+/oGFJDweqE2CAwavY7IgCKo89T2aa1/htdIyZVk27x66R+52OHbglcEhKEdvftA69SOOUoZjhSiUcua5iQo0FIogirOOn0qcQHQDr/EsD2PIsAz09tiZ8z7PRM6Ci/yJlbPNVmdoyKTcsDCvZFAtZdOK5P9slqrFp0CTOEkVjtH9RouUOoo7xR04EREYKbrJA4gEyc/qoCUUEKn8pmorDbxAF4crlqltz1KqiOCrrNPxY7xCnDEYR9oP15n2ixOFC73OsgZuDG4slAalhScGTywUBoWFicHEwluDtxaucgw5jYp/wKleWQVDM3to4cjgyMKxwbGFE4MTC6cGpxZeGry08M7gnYW9+rf2mgX9h8mh7mfNn44uSkWQ6ovmVDStqHVmdF7Rc0vPKnpmb9yvcN+aPKmodZPoqqJXD0urJeYCM12+Mz0og5pGeEFiUnSQbFbmAn1qkrXiRPCEy2Z1r5LN+47X7DJ2MPp45OXxNxfsgbfgHfgAPHAMPoOvoAeGAAEBfoJf4Hfrb7vbfn3foXZ3ylb1CtRG+80/qib1PA==</latexit><latexit sha1_base64="x5+RN/uP5kxsM2gwn9PgzPbc0Nc=">AAAFXXicbdTPbtMwGABwb7QwyoCOC0JcIiYkTlPCZRyZmDYOO5Suf9eEynHc1ZodZ7ZD21l5D67wVrwCT0HSZXISYynKp+/3+U8c6QsTSqRy3T87u49a7cdP9p52nu0/f/Gye7A/kjwVCA8Rp1xMQigxJTEeKqIoniQCQxZSPA5vvhQ+/oGFJDweqE2CAwavY7IgCKo89T2aa1/htdIyZVk27x66R+52OHbglcEhKEdvftA69SOOUoZjhSiUcua5iQo0FIogirOOn0qcQHQDr/EsD2PIsAz09tiZ8z7PRM6Ci/yJlbPNVmdoyKTcsDCvZFAtZdOK5P9slqrFp0CTOEkVjtH9RouUOoo7xR04EREYKbrJA4gEyc/qoCUUEKn8pmorDbxAF4crlqltz1KqiOCrrNPxY7xCnDEYR9oP15n2ixOFC73OsgZuDG4slAalhScGTywUBoWFicHEwluDtxaucgw5jYp/wKleWQVDM3to4cjgyMKxwbGFE4MTC6cGpxZeGry08M7gnYW9+rf2mgX9h8mh7mfNn44uSkWQ6ovmVDStqHVmdF7Rc0vPKnpmb9yvcN+aPKmodZPoqqJXD0urJeYCM12+Mz0og5pGeEFiUnSQbFbmAn1qkrXiRPCEy2Z1r5LN+47X7DJ2MPp45OXxNxfsgbfgHfgAPHAMPoOvoAeGAAEBfoJf4Hfrb7vbfn3foXZ3ylb1CtRG+80/qib1PA==</latexit><latexit sha1_base64="d/x+lI/VL77AF50V2XPa7efPBTA=">AAAFaHicbdTPbtMwGABwb7Qwyr8ODghxiVaQOE0JFzhuYto47FC6/l0TKsdxVmtOnNkObWflPbjCW/EKPAVJl8lJjKUon77f99mOI9lPKBHStv/s7D5otR8+2nvcefL02fMX3f2XY8FSjvAIMcr41IcCUxLjkSSS4mnCMYx8iif+9ZfCJz8wF4TFQ7lJsBfBq5iEBEGZp74HC+VKvJZKpFGWLbo9+9DeDssMnDLogXL0F/utEzdgKI1wLBGFQswdO5GeglwSRHHWcVOBE4iu4RWe52EMIyw8td12Zr3PM4EVMp4/sbS22WqHgpEQm8jPKyMol6JpRfJ/Nk9l+NlTJE5SiWN0t1CYUksyqzgDKyAcI0k3eQARJ/leLbSEHCKZn1RtpqHjqWJzxTS15aOUSsLZKut03BivEIsiGAfK9deZcosd+aFaZ1kDNxo3BgqNwsBjjccGco3cwERjYuCNxhsDVzn6jAbFP2BUrYyCke4eGTjWODZwonFi4FTj1MCZxpmBFxovDLzVeGtgv/6t/WbB4L7ZV4Os+dPReakIUnXebEWzihp7RmcVPTP0tKKn5sKDCg+M5mlFjZNElxW9vJ9aLjHjOFLlO1PDMqhpgEMSk+IGyeZlzlMnOlkrTjhLmGhW9yvZ/N5xmreMGYw/Hjp5/M3uHb0rb6A98BYcgA/AAZ/AEfgK+mAEEODgJ/gFfrf+trvt1+03d6W7O2XPK1Ab7YN/Ssf18Q==</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit><latexit sha1_base64="2cRWE4aSkUtF2XbQ11U+ruro9Vs=">AAAFaHicbdTdbtMwFABgb7Qwyl8HFwhxE60gcTU1ExJcbmLauNhF6fq7pkyO46zW7DizHdrOyntwC2/FK/AUJF0mJzGWohyd7xzbcST7MSVSdbt/trYfNJoPH+08bj15+uz5i/buy5HkiUB4iDjlYuJDiSmJ8FARRfEkFhgyn+Kxf/0l9/EPLCTh0UCtYzxn8CoiIUFQZanvwaX2FF4pLROWppftTne/uxmOHbhF0AHF6F3uNo69gKOE4UghCqWcud1YzTUUiiCK05aXSBxDdA2v8CwLI8iwnOvNtlPnfZYJnJCL7ImUs8mWOzRkUq6Zn1UyqBaybnnyfzZLVPh5rkkUJwpH6G6hMKGO4k5+Bk5ABEaKrrMAIkGyvTpoAQVEKjupykwDd67zzeXTVJZnCVVE8GXaankRXiLOGIwC7fmrVHv5jvxQr9K0hmuDawulQWnhkcEjC4VBYWFsMLbwxuCNhcsMfU6D/B9wqpdWwdB0Dy0cGRxZODY4tnBicGLh1ODUwnOD5xbeGry1sFf91l69oH/f7Ot+Wv/p6KxQBKk+q7eiaUmtPaPTkp5aelLSE3vhfon7VvOkpNZJoouSXtxPrRaYC8x08U71oAgqGuCQRCS/QdJZkZvrY5OsFMeCx1zWq3ulbHbvuPVbxg5GB/tuFn/72Dl8V9xAO+At2AMfgAs+gUPwFfTAECAgwE/wC/xu/G22m6+bb+5Kt7eKnlegMpp7/wBMB/X1</latexit>z2Ra⇥b⇥dsum<latexit sha1_base64="1TWIau4N+98tAuAme7Vm8/6k4lk=">AAAFlnicbdRNb9MwGABgb6wwylcHFyQuERUSp6lFk0Ac0CbGymGH0vVzTakcx1mtOXFmO/TD8n/h13CFK/+GpE3lNMZSlbfv8/ozir2YEiEbjb97+/cOKvcfHD6sPnr85Omz2tHzvmAJR7iHGGV86EGBKYlwTxJJ8TDmGIYexQPv9nPmgx+YC8KirlzGeBLCm4gEBEGZpqa1j24I5cwL1Eo7LomczV9PdfR3BV1JQiwcL3/6U+VKvJBKJKHWelqrN44b6+bYQTMP6iBv7enRwbnrM5SEOJKIQiHGzUYsJwpySRDFuuomAscQ3cIbPE7DCKazTtR6k9p5k2Z8J2A8/UXSWWeLPRQMhViGXlqZ7UGULUv+z8aJDD5MFIniROIIbSYKEupI5mQn5viEYyTpMg0g4iRdq4NmkEMk03PdGanbnKhscdkwO9OHCZWEs7muVt0IzxELQxj5yvUWWm1fwELrEi4NLi0UBoWFZwbPLOQGuYWxwdjCO4N3Fs5T9Bj1s3fAqJpbBT3Tu2dh32DfwoHBgYVDg0MLRwZHFl4ZvLJwZXBlYXt3r+1yQWfbOfuQyi8dXeaKIFWX5a5oVFBrzahV0JalFwW9sCfuFLhjdR4W1DpJdF3Q6+3QcoYZx6HKn1p182BHfRyQiGT3jR7nuYk6N8md4pizmIlydbuQTe+dZvmWsYP+u+NmGn87qZ828hvoELwCr8Fb0ATvwSn4CtqgBxD4CX6B3+BP5WXlU+VLpbUp3d/L+7wAO63S/gdkKggW</latexit><latexit sha1_base64="1TWIau4N+98tAuAme7Vm8/6k4lk=">AAAFlnicbdRNb9MwGABgb6wwylcHFyQuERUSp6lFk0Ac0CbGymGH0vVzTakcx1mtOXFmO/TD8n/h13CFK/+GpE3lNMZSlbfv8/ozir2YEiEbjb97+/cOKvcfHD6sPnr85Omz2tHzvmAJR7iHGGV86EGBKYlwTxJJ8TDmGIYexQPv9nPmgx+YC8KirlzGeBLCm4gEBEGZpqa1j24I5cwL1Eo7LomczV9PdfR3BV1JQiwcL3/6U+VKvJBKJKHWelqrN44b6+bYQTMP6iBv7enRwbnrM5SEOJKIQiHGzUYsJwpySRDFuuomAscQ3cIbPE7DCKazTtR6k9p5k2Z8J2A8/UXSWWeLPRQMhViGXlqZ7UGULUv+z8aJDD5MFIniROIIbSYKEupI5mQn5viEYyTpMg0g4iRdq4NmkEMk03PdGanbnKhscdkwO9OHCZWEs7muVt0IzxELQxj5yvUWWm1fwELrEi4NLi0UBoWFZwbPLOQGuYWxwdjCO4N3Fs5T9Bj1s3fAqJpbBT3Tu2dh32DfwoHBgYVDg0MLRwZHFl4ZvLJwZXBlYXt3r+1yQWfbOfuQyi8dXeaKIFWX5a5oVFBrzahV0JalFwW9sCfuFLhjdR4W1DpJdF3Q6+3QcoYZx6HKn1p182BHfRyQiGT3jR7nuYk6N8md4pizmIlydbuQTe+dZvmWsYP+u+NmGn87qZ828hvoELwCr8Fb0ATvwSn4CtqgBxD4CX6B3+BP5WXlU+VLpbUp3d/L+7wAO63S/gdkKggW</latexit><latexit sha1_base64="1TWIau4N+98tAuAme7Vm8/6k4lk=">AAAFlnicbdRNb9MwGABgb6wwylcHFyQuERUSp6lFk0Ac0CbGymGH0vVzTakcx1mtOXFmO/TD8n/h13CFK/+GpE3lNMZSlbfv8/ozir2YEiEbjb97+/cOKvcfHD6sPnr85Omz2tHzvmAJR7iHGGV86EGBKYlwTxJJ8TDmGIYexQPv9nPmgx+YC8KirlzGeBLCm4gEBEGZpqa1j24I5cwL1Eo7LomczV9PdfR3BV1JQiwcL3/6U+VKvJBKJKHWelqrN44b6+bYQTMP6iBv7enRwbnrM5SEOJKIQiHGzUYsJwpySRDFuuomAscQ3cIbPE7DCKazTtR6k9p5k2Z8J2A8/UXSWWeLPRQMhViGXlqZ7UGULUv+z8aJDD5MFIniROIIbSYKEupI5mQn5viEYyTpMg0g4iRdq4NmkEMk03PdGanbnKhscdkwO9OHCZWEs7muVt0IzxELQxj5yvUWWm1fwELrEi4NLi0UBoWFZwbPLOQGuYWxwdjCO4N3Fs5T9Bj1s3fAqJpbBT3Tu2dh32DfwoHBgYVDg0MLRwZHFl4ZvLJwZXBlYXt3r+1yQWfbOfuQyi8dXeaKIFWX5a5oVFBrzahV0JalFwW9sCfuFLhjdR4W1DpJdF3Q6+3QcoYZx6HKn1p182BHfRyQiGT3jR7nuYk6N8md4pizmIlydbuQTe+dZvmWsYP+u+NmGn87qZ828hvoELwCr8Fb0ATvwSn4CtqgBxD4CX6B3+BP5WXlU+VLpbUp3d/L+7wAO63S/gdkKggW</latexit><latexit sha1_base64="1TWIau4N+98tAuAme7Vm8/6k4lk=">AAAFlnicbdRNb9MwGABgb6wwylcHFyQuERUSp6lFk0Ac0CbGymGH0vVzTakcx1mtOXFmO/TD8n/h13CFK/+GpE3lNMZSlbfv8/ozir2YEiEbjb97+/cOKvcfHD6sPnr85Omz2tHzvmAJR7iHGGV86EGBKYlwTxJJ8TDmGIYexQPv9nPmgx+YC8KirlzGeBLCm4gEBEGZpqa1j24I5cwL1Eo7LomczV9PdfR3BV1JQiwcL3/6U+VKvJBKJKHWelqrN44b6+bYQTMP6iBv7enRwbnrM5SEOJKIQiHGzUYsJwpySRDFuuomAscQ3cIbPE7DCKazTtR6k9p5k2Z8J2A8/UXSWWeLPRQMhViGXlqZ7UGULUv+z8aJDD5MFIniROIIbSYKEupI5mQn5viEYyTpMg0g4iRdq4NmkEMk03PdGanbnKhscdkwO9OHCZWEs7muVt0IzxELQxj5yvUWWm1fwELrEi4NLi0UBoWFZwbPLOQGuYWxwdjCO4N3Fs5T9Bj1s3fAqJpbBT3Tu2dh32DfwoHBgYVDg0MLRwZHFl4ZvLJwZXBlYXt3r+1yQWfbOfuQyi8dXeaKIFWX5a5oVFBrzahV0JalFwW9sCfuFLhjdR4W1DpJdF3Q6+3QcoYZx6HKn1p182BHfRyQiGT3jR7nuYk6N8md4pizmIlydbuQTe+dZvmWsYP+u+NmGn87qZ828hvoELwCr8Fb0ATvwSn4CtqgBxD4CX6B3+BP5WXlU+VLpbUp3d/L+7wAO63S/gdkKggW</latexit>[ˆz]j=(wjz)  dsumXk=1w(j)k[z]k!<latexit sha1_base64="Ws6No730wat7iw+S6wiXraS5ouU=">AAAF5XicbdTPbtMwGADwdKwwyr8NxAkJIiak7jI1CAkukzYxbRx2KN36Z2tC5Dhu6zWJM9uh6yw/AAduiCvPxVPwCjhbhpOYSFU+fb/vs524cZBGmPFO53dj5c5q8+69tfutBw8fPX6yvvF0wEhGIepDEhE6CgBDEU5Qn2MeoVFKEYiDCA2D+cfch18RZZgkJ3yZIi8G0wRPMARcpfz1b2N3BrhwY8BnwURcSen55ztuOsNtNyBRyJaxuomF9M9dEhKuC7dcTjFIphG6yMvdCE1422VZ7Iv5jiO/iNAXLkeXXKiclHKh8irbPt+SYz2K589diqczvuWvb3a2O9eXbQZOEWxaxdX1N1b33ZDALEYJhxFgbOx0Uu4JQDmGEZItN2MoBXAOpmiswgTEiHni+p1J+43KhPaEUPVLuH2dLXcIELP84VVlvlhWtzz5PxtnfPLBEzhJM44SeDPRJItsTux8A+wQUwR5tFQBgBSrtdpwBiiAXG1TZaQTxxP54vJhKtPHWcQxJQvZarkJWkASxyAJhRtcyn87eSllDZcalwYyjczAPY17BlKN1MBUY2rghcYLAxcKK3/AekFfd/cNHGgcGDjUODRwpHFk4KnGUwOPNR4beCXL31gNu9Vn7dYLerfNgejJ+qbDo0IhiMRRvRWeltRYMzws6aGhByU9MCfulbhnNI9KarxJeFbSs9uh+QwRimJR3KU4KYKKhmiCE5wfX3Jc5Dyxr5OV4pSSlLB6dbeUVeeOUz9lzGDwdttR8ed3m7ud4gRas15Yr6225VjvrV3rk9W1+ha0/jSeN142XjWnze/NH82fN6UrjaLnmVW5mr/+AmRdKII=</latexit><latexit sha1_base64="Ws6No730wat7iw+S6wiXraS5ouU=">AAAF5XicbdTPbtMwGADwdKwwyr8NxAkJIiak7jI1CAkukzYxbRx2KN36Z2tC5Dhu6zWJM9uh6yw/AAduiCvPxVPwCjhbhpOYSFU+fb/vs524cZBGmPFO53dj5c5q8+69tfutBw8fPX6yvvF0wEhGIepDEhE6CgBDEU5Qn2MeoVFKEYiDCA2D+cfch18RZZgkJ3yZIi8G0wRPMARcpfz1b2N3BrhwY8BnwURcSen55ztuOsNtNyBRyJaxuomF9M9dEhKuC7dcTjFIphG6yMvdCE1422VZ7Iv5jiO/iNAXLkeXXKiclHKh8irbPt+SYz2K589diqczvuWvb3a2O9eXbQZOEWxaxdX1N1b33ZDALEYJhxFgbOx0Uu4JQDmGEZItN2MoBXAOpmiswgTEiHni+p1J+43KhPaEUPVLuH2dLXcIELP84VVlvlhWtzz5PxtnfPLBEzhJM44SeDPRJItsTux8A+wQUwR5tFQBgBSrtdpwBiiAXG1TZaQTxxP54vJhKtPHWcQxJQvZarkJWkASxyAJhRtcyn87eSllDZcalwYyjczAPY17BlKN1MBUY2rghcYLAxcKK3/AekFfd/cNHGgcGDjUODRwpHFk4KnGUwOPNR4beCXL31gNu9Vn7dYLerfNgejJ+qbDo0IhiMRRvRWeltRYMzws6aGhByU9MCfulbhnNI9KarxJeFbSs9uh+QwRimJR3KU4KYKKhmiCE5wfX3Jc5Dyxr5OV4pSSlLB6dbeUVeeOUz9lzGDwdttR8ed3m7ud4gRas15Yr6225VjvrV3rk9W1+ha0/jSeN142XjWnze/NH82fN6UrjaLnmVW5mr/+AmRdKII=</latexit><latexit sha1_base64="Ws6No730wat7iw+S6wiXraS5ouU=">AAAF5XicbdTPbtMwGADwdKwwyr8NxAkJIiak7jI1CAkukzYxbRx2KN36Z2tC5Dhu6zWJM9uh6yw/AAduiCvPxVPwCjhbhpOYSFU+fb/vs524cZBGmPFO53dj5c5q8+69tfutBw8fPX6yvvF0wEhGIepDEhE6CgBDEU5Qn2MeoVFKEYiDCA2D+cfch18RZZgkJ3yZIi8G0wRPMARcpfz1b2N3BrhwY8BnwURcSen55ztuOsNtNyBRyJaxuomF9M9dEhKuC7dcTjFIphG6yMvdCE1422VZ7Iv5jiO/iNAXLkeXXKiclHKh8irbPt+SYz2K589diqczvuWvb3a2O9eXbQZOEWxaxdX1N1b33ZDALEYJhxFgbOx0Uu4JQDmGEZItN2MoBXAOpmiswgTEiHni+p1J+43KhPaEUPVLuH2dLXcIELP84VVlvlhWtzz5PxtnfPLBEzhJM44SeDPRJItsTux8A+wQUwR5tFQBgBSrtdpwBiiAXG1TZaQTxxP54vJhKtPHWcQxJQvZarkJWkASxyAJhRtcyn87eSllDZcalwYyjczAPY17BlKN1MBUY2rghcYLAxcKK3/AekFfd/cNHGgcGDjUODRwpHFk4KnGUwOPNR4beCXL31gNu9Vn7dYLerfNgejJ+qbDo0IhiMRRvRWeltRYMzws6aGhByU9MCfulbhnNI9KarxJeFbSs9uh+QwRimJR3KU4KYKKhmiCE5wfX3Jc5Dyxr5OV4pSSlLB6dbeUVeeOUz9lzGDwdttR8ed3m7ud4gRas15Yr6225VjvrV3rk9W1+ha0/jSeN142XjWnze/NH82fN6UrjaLnmVW5mr/+AmRdKII=</latexit><latexit sha1_base64="Ws6No730wat7iw+S6wiXraS5ouU=">AAAF5XicbdTPbtMwGADwdKwwyr8NxAkJIiak7jI1CAkukzYxbRx2KN36Z2tC5Dhu6zWJM9uh6yw/AAduiCvPxVPwCjhbhpOYSFU+fb/vs524cZBGmPFO53dj5c5q8+69tfutBw8fPX6yvvF0wEhGIepDEhE6CgBDEU5Qn2MeoVFKEYiDCA2D+cfch18RZZgkJ3yZIi8G0wRPMARcpfz1b2N3BrhwY8BnwURcSen55ztuOsNtNyBRyJaxuomF9M9dEhKuC7dcTjFIphG6yMvdCE1422VZ7Iv5jiO/iNAXLkeXXKiclHKh8irbPt+SYz2K589diqczvuWvb3a2O9eXbQZOEWxaxdX1N1b33ZDALEYJhxFgbOx0Uu4JQDmGEZItN2MoBXAOpmiswgTEiHni+p1J+43KhPaEUPVLuH2dLXcIELP84VVlvlhWtzz5PxtnfPLBEzhJM44SeDPRJItsTux8A+wQUwR5tFQBgBSrtdpwBiiAXG1TZaQTxxP54vJhKtPHWcQxJQvZarkJWkASxyAJhRtcyn87eSllDZcalwYyjczAPY17BlKN1MBUY2rghcYLAxcKK3/AekFfd/cNHGgcGDjUODRwpHFk4KnGUwOPNR4beCXL31gNu9Vn7dYLerfNgejJ+qbDo0IhiMRRvRWeltRYMzws6aGhByU9MCfulbhnNI9KarxJeFbSs9uh+QwRimJR3KU4KYKKhmiCE5wfX3Jc5Dyxr5OV4pSSlLB6dbeUVeeOUz9lzGDwdttR8ed3m7ud4gRas15Yr6225VjvrV3rk9W1+ha0/jSeN142XjWnze/NH82fN6UrjaLnmVW5mr/+AmRdKII=</latexit>(||wj||1t)<latexit sha1_base64="4IZYMuhF+yiRg8CqJFAoizNr9zw=">AAAFe3icbdRPb9MwFABwb7Qwyr8OjhyIqJAGQlODQHDcxLRx2KF0/bsmqhzHXc2cOLUdus7NkU/DFT4MHwaJpMvkJMZSlKf3e8923MpeRImQ7fafre07tfrdezv3Gw8ePnr8pLn7dCBYzBHuI0YZH3lQYEpC3JdEUjyKOIaBR/HQu/yc+fA75oKwsCdXEXYDeBGSGUFQpqlp88Xeeu14jPpiFaQvtUym39brqW05FC8s+XrabLX325thmYGdBy2Qj850t3bk+AzFAQ4lolCIid2OpKsglwRRnDScWOAIokt4gSdpGMIAC1dtviSxXqUZ35oxnj6htDbZYoeCgch2mlYGUM5F1bLk/2wSy9knV5EwiiUO0c1Cs5haklnZsVg+4RhJukoDiDhJ92qhOeQQyfTwSjP1bFdlm8umKS0fxFQSzpZJo+GEeIlYEMDQV453lSgn25E3U1dJUsGVxpWBQqMw8FDjoYFcIzcw0hgZuNC4MHCZYunfUi3o6+6+gQONAwOHGocGjjSODBxrHBt4pvHMwGuN1wZ2yt/aqRZ0b5s91U2qPzo6zRVBqk6rrWhcUGPP6KSgJ4YeF/TYXLhb4K7RPCqocZLovKDnt1PLOWYcByp/J6qXByX18YyEJLtUkkmec9WRTpaKI84iJqrVnUI2vXfs6i1jBoN3+3Yaf33fOmjnN9AOeA5egj1gg4/gAHwBHdAHCPwAP8Ev8Lv2t96qv6m/vSnd3sp7noHSqH/4ByDI/KE=</latexit><latexit sha1_base64="4IZYMuhF+yiRg8CqJFAoizNr9zw=">AAAFe3icbdRPb9MwFABwb7Qwyr8OjhyIqJAGQlODQHDcxLRx2KF0/bsmqhzHXc2cOLUdus7NkU/DFT4MHwaJpMvkJMZSlKf3e8923MpeRImQ7fafre07tfrdezv3Gw8ePnr8pLn7dCBYzBHuI0YZH3lQYEpC3JdEUjyKOIaBR/HQu/yc+fA75oKwsCdXEXYDeBGSGUFQpqlp88Xeeu14jPpiFaQvtUym39brqW05FC8s+XrabLX325thmYGdBy2Qj850t3bk+AzFAQ4lolCIid2OpKsglwRRnDScWOAIokt4gSdpGMIAC1dtviSxXqUZ35oxnj6htDbZYoeCgch2mlYGUM5F1bLk/2wSy9knV5EwiiUO0c1Cs5haklnZsVg+4RhJukoDiDhJ92qhOeQQyfTwSjP1bFdlm8umKS0fxFQSzpZJo+GEeIlYEMDQV453lSgn25E3U1dJUsGVxpWBQqMw8FDjoYFcIzcw0hgZuNC4MHCZYunfUi3o6+6+gQONAwOHGocGjjSODBxrHBt4pvHMwGuN1wZ2yt/aqRZ0b5s91U2qPzo6zRVBqk6rrWhcUGPP6KSgJ4YeF/TYXLhb4K7RPCqocZLovKDnt1PLOWYcByp/J6qXByX18YyEJLtUkkmec9WRTpaKI84iJqrVnUI2vXfs6i1jBoN3+3Yaf33fOmjnN9AOeA5egj1gg4/gAHwBHdAHCPwAP8Ev8Lv2t96qv6m/vSnd3sp7noHSqH/4ByDI/KE=</latexit><latexit sha1_base64="4IZYMuhF+yiRg8CqJFAoizNr9zw=">AAAFe3icbdRPb9MwFABwb7Qwyr8OjhyIqJAGQlODQHDcxLRx2KF0/bsmqhzHXc2cOLUdus7NkU/DFT4MHwaJpMvkJMZSlKf3e8923MpeRImQ7fafre07tfrdezv3Gw8ePnr8pLn7dCBYzBHuI0YZH3lQYEpC3JdEUjyKOIaBR/HQu/yc+fA75oKwsCdXEXYDeBGSGUFQpqlp88Xeeu14jPpiFaQvtUym39brqW05FC8s+XrabLX325thmYGdBy2Qj850t3bk+AzFAQ4lolCIid2OpKsglwRRnDScWOAIokt4gSdpGMIAC1dtviSxXqUZ35oxnj6htDbZYoeCgch2mlYGUM5F1bLk/2wSy9knV5EwiiUO0c1Cs5haklnZsVg+4RhJukoDiDhJ92qhOeQQyfTwSjP1bFdlm8umKS0fxFQSzpZJo+GEeIlYEMDQV453lSgn25E3U1dJUsGVxpWBQqMw8FDjoYFcIzcw0hgZuNC4MHCZYunfUi3o6+6+gQONAwOHGocGjjSODBxrHBt4pvHMwGuN1wZ2yt/aqRZ0b5s91U2qPzo6zRVBqk6rrWhcUGPP6KSgJ4YeF/TYXLhb4K7RPCqocZLovKDnt1PLOWYcByp/J6qXByX18YyEJLtUkkmec9WRTpaKI84iJqrVnUI2vXfs6i1jBoN3+3Yaf33fOmjnN9AOeA5egj1gg4/gAHwBHdAHCPwAP8Ev8Lv2t96qv6m/vSnd3sp7noHSqH/4ByDI/KE=</latexit><latexit sha1_base64="4IZYMuhF+yiRg8CqJFAoizNr9zw=">AAAFe3icbdRPb9MwFABwb7Qwyr8OjhyIqJAGQlODQHDcxLRx2KF0/bsmqhzHXc2cOLUdus7NkU/DFT4MHwaJpMvkJMZSlKf3e8923MpeRImQ7fafre07tfrdezv3Gw8ePnr8pLn7dCBYzBHuI0YZH3lQYEpC3JdEUjyKOIaBR/HQu/yc+fA75oKwsCdXEXYDeBGSGUFQpqlp88Xeeu14jPpiFaQvtUym39brqW05FC8s+XrabLX325thmYGdBy2Qj850t3bk+AzFAQ4lolCIid2OpKsglwRRnDScWOAIokt4gSdpGMIAC1dtviSxXqUZ35oxnj6htDbZYoeCgch2mlYGUM5F1bLk/2wSy9knV5EwiiUO0c1Cs5haklnZsVg+4RhJukoDiDhJ92qhOeQQyfTwSjP1bFdlm8umKS0fxFQSzpZJo+GEeIlYEMDQV453lSgn25E3U1dJUsGVxpWBQqMw8FDjoYFcIzcw0hgZuNC4MHCZYunfUi3o6+6+gQONAwOHGocGjjSODBxrHBt4pvHMwGuN1wZ2yt/aqRZ0b5s91U2qPzo6zRVBqk6rrWhcUGPP6KSgJ4YeF/TYXLhb4K7RPCqocZLovKDnt1PLOWYcByp/J6qXByX18YyEJLtUkkmec9WRTpaKI84iJqrVnUI2vXfs6i1jBoN3+3Yaf33fOmjnN9AOeA5egj1gg4/gAHwBHdAHCPwAP8Ev8Lv2t96qv6m/vSnd3sp7noHSqH/4ByDI/KE=</latexit>(a) Original

(b) Gaussian noise

(c) Downsampling

Figure 2: Visualization of corrupted samples  (left) RGB images (right) LIDAR point clouds. The
point clouds are projected onto the 2D image plane for easier visual comparison.

5 Experimental Results

We test our algorithms and the LEL fusion method on 3D and BEV object detection tasks using the
car class of the KITTI dataset [10]. 3D detection is both an important problem in self-driving cars and
one where multiple sensors can contribute fruitfully by providing both complementary and shared
information. In contrast  models for 2D object detection heavily rely on RGB data  which typically
dominates other modalities. As our experiments include random generation of corruption  each task
is evaluated 5 times to compare average scores (reported with 95% conﬁdence intervals)  and thus
a validation set is used for ease of manipulating data and repetitive evaluation. We follow the split
of Ku et al. [25]  3712 and 3769 frames for training and validation sets  respectively. Results are
reported based on three difﬁculty levels deﬁned by KITTI (easy  medium  hard) and a standard metric
Average Precision (AP) is used. A recent open-sourced 3D object detector AVOD [25] with a feature
pyramid network is selected as a baseline algorithm.
Four different algorithms are compared: AVOD trained on (i) clean data  (ii) data augmented with
ASN samples (TRAINASN)  (iii) SSN augmented data with direct MAXSSN loss minimization
(TRAINSSN)  and (iv) SSN augmented data (TRAINSSNALT). The AVOD architecture is varied to
use either element-wise mean fusion layers or our LELs. We follow the original training setups of
AVOD  e.g.  120k iterations using an ADAM optimizer with an initial learning rate of 0.0001.4

Corruption methods Gaussian noise generated i.i.d. with N (0  σ2
Gaussian) is directly added to the
pixel value of an image (r  g  b) and the coordinate value of a LIDAR’s point (x  y  z). σGaussian is
set to 0.75τ experimentally with τRGB = 255 and τLIDAR = 0.2. The second method downsampling
selects only 16 out of 64 lasers of LIDAR data. To match this effect  3 out of 4 horizontal lines of
an RGB image are deleted. Effects of corruption on each input source are visualized in Figure 2 
where the color of a 2D LIDAR image represents a distance from the sensor. Although our analyses
in Section 3.2 assume the noise variances to be identical  it is nontrivial to set equal noise levels
for different modalities in practice  e.g.  RGB pixels vs. points in a 3D space. Nevertheless  an
underlying objective of our MAXSSN loss  balancing the degradation rates of different input sources’
faults  does not depend on the choice of noise types or levels.

4Our methods are implemented with TensorFlow on top of the ofﬁcial AVOD code. The computing
machine has a Intel Xeon E5-1660v3 CPU with Nvidia Titan X Pascal GPUs. The source code is available at
https://github.com/twankim/avod_ssn.

7

Evaluation metrics for single source robustness To assess the robustness against single source
noise  a new metric minAP is introduced. The AP score is evaluated on the dataset with a single
corrupted input source  then after going over all ns sources  minAP reports the lowest score among
the ns AP scores. Our second metric maxDiffAP computes the maximum absolute difference among
the scores  which measures the balance of different input sources’ single source robustness; low value
of maxDiffAP means the well-balanced robustness.

Table 1: Car detection (3D/BEV) performance of AVOD with element-wise mean fusion layers and
latent ensemble layers (LEL) against Gaussian SSN on the KITTI validation set.

(Data) Train Algo.

Easy

Moderate

Hard

Easy

Moderate

Hard

(Clean Data)

AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

(Gaussian SSN)

AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

(Gaussian SSN)
AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

(Clean Data)

AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

(Gaussian SSN)

AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

Fusion method: Mean

76.41
75.96
76.28
77.46

AP3D(%)

72.74
66.68
67.10
67.61

66.86
65.97
66.51
66.06

89.33
88.63
88.86
89.68

min AP3D(%)
41.84±0.17
52.72±0.08
62.14±0.08
57.61±0.12

max DiffAP3D(%)

36.47±0.16
47.25±0.13
56.78±0.12
55.90±0.11

22.42±0.29
12.72±0.33
3.42±0.25
8.73±0.32

20.92±0.25
11.18±0.27
7.50±0.25
2.91±0.22

65.63±0.28
87.71±0.14
88.21±0.08
89.42±0.04

22.27±0.41
0.88±0.22
0.36±0.17
0.09±0.14

Fusion method: Latent Ensemble Layer

47.41±0.28
61.53±0.57
71.65±0.31
71.66±0.48

26.70±0.52
14.48±0.82
3.71±0.46
5.55±0.81

77.79
75.00
74.25
76.04

61.97±0.55
74.24±0.38
68.16±0.88
68.63±0.40

AP3D(%)

67.69
64.75
65.00
66.42

min AP3D(%)
53.95±0.42
58.25±0.16
60.39±0.38
55.48±0.16

APBEV (%)

86.49
79.45
79.60
86.71

min APBEV (%)

79.44
78.79
79.11
79.41

58.02±0.23
78.37±0.06
78.90±0.09
79.56±0.06

50.43±0.14
77.85±0.08
77.92±0.11
77.92±0.05

max DiffAPBEV (%)

20.76±0.33
0.48±0.13
0.04±0.15
0.13±0.11

APBEV (%)

85.64
78.60
78.84
79.53

20.09±0.20
0.28±0.12
0.71±0.17
0.18±0.11

78.86
77.23
77.66
78.53

min APBEV (%)

66.31
58.28
63.83
64.41

88.90
88.30
87.88
88.80

47.24±0.27
56.13±0.10
56.04±0.28
54.42±0.17

79.44±0.09
88.10±0.26
88.12±0.16
86.51±0.46

72.46±3.14
78.19±0.13
78.17±0.06
76.85±0.11

68.25±0.06
70.42±0.07
70.21±0.05
71.95±2.72

Table 2: Car detection (3D/BEV) performance of AVOD with latent ensemble layers (LEL) against
downsampling SSN on the KITTI validation set.

(Data) Train Algo.

Easy

Moderate

Hard

Easy

Moderate

Hard

(Clean Data)

AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

(Downsample SSN)

AVOD [25]
+TRAINASN
+TRAINSSN
+TRAINSSNALT

77.79
71.74
75.54
76.22

61.70
65.74
73.33
64.77

AP3D(%)

APBEV (%)

67.69
61.78
66.26
66.05

min AP3D(%)

51.66
53.49
57.85
53.34

66.31
60.26
63.72
63.87

46.17
51.35
54.91
48.29

88.90
87.29
88.07
89.00

85.64
77.08
79.18
79.65

78.86
75.89
78.03
78.03

min APBEV (%)

86.08
82.27
86.61
85.27

69.99
67.88
76.07
69.87

61.55
65.79
68.59
67.77

Results When the fusion model uses the element-wise mean fusion (Table 1)  TRAINSSN algo-
rithm shows the best single source robustness against Gaussian SSN while preserving the original
performance on clean data (only small decrease in the moderate BEV detection)5. Also a balance
5In practice  it is difﬁcult to identify ﬂexible parameters related to shared information in advance  and also the
design goal becomes a soft rather than a hard constraint. Therefore there is minor degradation in performance  to
pay for the added robustness.

8

of the both input sources’ performance is dramatically decreased compared to the models trained
without robust learning and a naive TRAINASN method.
Encouragingly  AVOD model constructed with our LEL method already achieves relatively high
robustness without any robust learning strategies compared to the mean fusion layers. For all the
tasks  minAP scores are dramatically increased  e.g.  61.97 vs. 47.41 for the easy 3D detection task 
and the maxDiffAP scores are decreased (maxDiffAP scores for AVOD with LEL are reported in
Appendix B.). Then the robustness is further improved by minimizing our MAXSSN loss. As our
LEL’s structure inherently handles corruption on a single source well  even the TRAINASN algorithm
can successfully guide the model to be equipped with the desired robustness.
A corruption method with a different style  downsampling  is also tested with our LEL fusion method.
Table 2 shows that the model trained with our TRAINSSN algorithm achieves the best robustness
among the four algorithms for this complex and realistic perturbation.
Remark 3. A simple TRAINSSNALT achieves fairly robust models in both fusion methods against
Gaussian noise  and two reasons may explain this phenomenon. First  all parameters are updated
instead of ﬁne-tuning only fusion related parts. Therefore  unlike our analyses on the linear model 
the latent representation can be transformed to meet the objective function. In fact  TRAINSSNALT
performs poorly when we ﬁne-tune the model with concatenation fusion layers as shown in the
supplement. Secondly  the loss function L inside our LMAXSSN is usually non-convex so that it may
be enough to use an indirect approach for small number of sources  ns = 2.
Remark 4. Without applying fancier approaches which could increase computational cost  our LEL
showed appealing effectiveness even with simple implementation.

6 Conclusion

We study two strategies to improve robustness of fusion models against single source corruption.
Motivated by analyses on linear fusion models  a loss function is introduced to balance performance
degradation of deep fusion models caused by corruption in different sources. We also demonstrate
the importance of a fusion method’s structure by proposing a simple ensemble layer achieving such
robustness inherently. Our experimental results show that deep fusion models can effectively use
complementary and shared information of different input sources by training with our loss and
fusion layer to obtain both robustness and high accuracy. We hope our results motivate further work
to improve the single source robustness of more complex fusion models with either large number
of input sources or adaptive networks. Another interesting direction is to investigate the single
source robustness against adversarial attacks in deep fusion models  which can be compared with our
analyses in the supplementary material.

References
[1] Markus Braun  Qing Rao  Yikang Wang  and Fabian Flohr. Pose-rcnn: Joint object detection and pose
estimation using 3d object proposals. In IEEE 19th international conference on intelligent transportation
systems (ITSC)  pages 1546–1551  2016.

[2] William Chan  Navdeep Jaitly  Quoc Le  and Oriol Vinyals. Listen  attend and spell: A neural network for
large vocabulary conversational speech recognition. In IEEE international conference on acoustics  speech
and signal processing (ICASSP)  pages 4960–4964  2016.

[3] Xiaozhi Chen  Huimin Ma  Ji Wan  Bo Li  and Tian Xia. Multi-view 3d object detection network for
autonomous driving. In IEEE conference on computer vision and pattern recognition (CVPR)  pages
1907–1915  2017.

[4] Chung-Cheng Chiu  Tara N Sainath  Yonghui Wu  Rohit Prabhavalkar  Patrick Nguyen  Zhifeng Chen 
Anjuli Kannan  Ron J Weiss  Kanishka Rao  Ekaterina Gonina  et al. State-of-the-art speech recognition
with sequence-to-sequence models. In IEEE international conference on acoustics  speech and signal
processing (ICASSP)  pages 4774–4778  2018.

[5] Jan K Chorowski  Dzmitry Bahdanau  Dmitriy Serdyuk  Kyunghyun Cho  and Yoshua Bengio. Attention-
based models for speech recognition. In Advances in neural information processing systems (NeurIPS) 
pages 577–585  2015.

9

[6] Joon Son Chung  Andrew Senior  Oriol Vinyals  and Andrew Zisserman. Lip reading sentences in the wild.

In IEEE conference on computer vision and pattern recognition (CVPR)  pages 3444–3453  2017.

[7] Jifeng Dai  Yi Li  Kaiming He  and Jian Sun. R-fcn: Object detection via region-based fully convolutional

networks. In Advances in neural information processing systems (NeurIPS)  pages 379–387  2016.

[8] Xinxin Du  Marcelo H Ang  and Daniela Rus. Car detection for autonomous vehicle: Lidar and vision
fusion approach through deep learning framework. In IEEE/RSJ international conference on intelligent
robots and systems (IROS)  pages 749–754  2017.

[9] Di Feng  Christian Haase-Schuetz  Lars Rosenbaum  Heinz Hertlein  Fabian Duffhauss  Claudius Glaeser 
Werner Wiesbeck  and Klaus Dietmayer. Deep multi-modal object detection and semantic segmentation
for autonomous driving: Datasets  methods  and challenges. arXiv preprint arXiv:1902.07830  2019.

[10] Andreas Geiger  Philip Lenz  and Raquel Urtasun. Are we ready for autonomous driving? the kitti
vision benchmark suite. In IEEE conference on computer vision and pattern recognition (CVPR)  pages
3354–3361  2012.

[11] Ross Girshick. Fast r-cnn. In IEEE international conference on computer vision (ICCV)  pages 1440–1448 

2015.

[12] Ross Girshick  Jeff Donahue  Trevor Darrell  and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In IEEE conference on computer vision and pattern recognition
(CVPR)  pages 580–587  2014.

[13] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial examples.

In International conference on learning representations (ICLR)  2015.

[14] Alex Graves  Abdel-rahman Mohamed  and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In IEEE international conference on acoustics  speech and signal processing (ICASSP) 
pages 6645–6649  2013.

[15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In IEEE conference on computer vision and pattern recognition (CVPR)  pages 770–778  2016.

[16] Kaiming He  Georgia Gkioxari  Piotr Dollár  and Ross Girshick. Mask r-cnn. In IEEE international

conference on computer vision (ICCV)  pages 2961–2969  2017.

[17] Geoffrey Hinton  Li Deng  Dong Yu  George Dahl  Abdel-rahman Mohamed  Navdeep Jaitly  Andrew
Senior  Vincent Vanhoucke  Patrick Nguyen  Brian Kingsbury  et al. Deep neural networks for acoustic
modeling in speech recognition. IEEE signal processing magazine  29  2012.

[18] Gao Huang  Zhuang Liu  Laurens Van Der Maaten  and Kilian Q Weinberger. Densely connected
convolutional networks. In IEEE conference on computer vision and pattern recognition (CVPR)  pages
4700–4708  2017.

[19] Jing Huang and Brian Kingsbury. Audio-visual deep learning for noise robust speech recognition. In IEEE
international conference on acoustics  speech and signal processing (ICASSP)  pages 7596–7599  2013.

[20] Jaekyum Kim  Junho Koh  Yecheol Kim  Jaehyung Choi  Youngbae Hwang  and Jun Won Choi. Robust
deep multi-modal learning based on gated information fusion network. In Asian conference on computer
vision (ACCV)  2018.

[21] Taewan Kim and Joydeep Ghosh. Robust detection of non-motorized road users using deep learning on
optical and lidar data. In IEEE 19th international conference on intelligent transportation systems (ITSC) 
pages 271–276  2016.

[22] Taewan Kim  Michael Motro  Patrícia Lavieri  Saharsh Samir Oza  Joydeep Ghosh  and Chandra Bhat.
Pedestrian detection with simpliﬁed depth prediction. In IEEE 21st international conference on intelligent
transportation systems (ITSC)  pages 2712–2717  2018.

[23] Ryan Kiros  Karteek Popuri  Dana Cobzas  and Martin Jagersand. Stacked multiscale feature learning
for domain independent medical image segmentation. In International workshop on machine learning in
medical imaging  pages 25–32. Springer  2014.

[24] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information processing systems (NeurIPS)  pages 1097–1105 
2012.

10

[25] Jason Ku  Melissa Moziﬁan  Jungwook Lee  Ali Harakeh  and Steven L Waslander. Joint 3d proposal
generation and object detection from view aggregation. In IEEE/RSJ international conference on intelligent
robots and systems (IROS)  pages 1–8  2018.

[26] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. nature  521(7553):436  2015.

[27] Ming Liang  Bin Yang  Shenlong Wang  and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d

object detection. In European conference on computer vision (ECCV)  pages 641–656  2018.

[28] Ming Liang  Bin Yang  Yun Chen  Rui Hui  and Raquel Urtasun. Multi-task multi-sensor fusion for 3d

object detection. In IEEE conference on computer vision and pattern recognition (CVPR)  2019.

[29] Siqi Liu  Sidong Liu  Weidong Cai  Hangyu Che  Sonia Pujol  Ron Kikinis  Dagan Feng  Michael J Fulham 
et al. Multimodal neuroimaging feature learning for multiclass diagnosis of alzheimer’s disease. IEEE
transactions on biomedical engineering  62(4):1132–1140  2015.

[30] Wei Liu  Dragomir Anguelov  Dumitru Erhan  Christian Szegedy  Scott Reed  Cheng-Yang Fu  and
Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision
(ECCV)  pages 21–37. Springer  2016.

[31] Oier Mees  Andreas Eitel  and Wolfram Burgard. Choosing smartly: Adaptive multimodal fusion for
object detection in changing environments. In IEEE/RSJ international conference on intelligent robots and
systems (IROS)  pages 151–156  2016.

[32] Youssef Mroueh  Etienne Marcheret  and Vaibhava Goel. Deep multimodal learning for audio-visual
speech recognition. In IEEE international conference on acoustics  speech and signal processing (ICASSP) 
pages 2130–2134  2015.

[33] Charles R Qi  Wei Liu  Chenxia Wu  Hao Su  and Leonidas J Guibas. Frustum pointnets for 3d object
detection from rgb-d data. In IEEE conference on computer vision and pattern recognition (CVPR)  pages
918–927  2018.

[34] Dhanesh Ramachandram and Graham W Taylor. Deep multimodal learning: A survey on recent advances

and trends. IEEE signal processing magazine  34(6):96–108  2017.

[35] Joseph Redmon and Ali Farhadi. Yolo9000: better  faster  stronger. In IEEE conference on computer vision

and pattern recognition (CVPR)  pages 7263–7271  2017.

[36] Joseph Redmon  Santosh Divvala  Ross Girshick  and Ali Farhadi. You only look once: Uniﬁed  real-time
object detection. In IEEE conference on computer vision and pattern recognition (CVPR)  pages 779–788 
2016.

[37] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object detection
with region proposal networks. In Advances in neural information processing systems (NeurIPS)  pages
91–99  2015.

[38] Tara N Sainath  Abdel-rahman Mohamed  Brian Kingsbury  and Bhuvana Ramabhadran. Deep convo-
lutional neural networks for lvcsr. In IEEE international conference on acoustics  speech and signal
processing (ICASSP)  pages 8614–8618  2013.

[39] Martin Simonovsky  Benjamín Gutiérrez-Becker  Diana Mateus  Nassir Navab  and Nikos Komodakis. A
deep metric for multimodal registration. In International conference on medical image computing and
computer-assisted intervention  pages 10–18. Springer  2016.

[40] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. In International conference on learning representations (ICLR)  2015.

[41] Chao Sui  Mohammed Bennamoun  and Roberto Togneri. Listening with your eyes: Towards a practical
visual speech recognition system using deep boltzmann machines. In IEEE international conference on
computer vision (ICCV)  pages 154–162  2015.

[42] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru
Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. In IEEE conference
on computer vision and pattern recognition (CVPR)  pages 1–9  2015.

[43] Dimitris Tsipras  Shibani Santurkar  Logan Engstrom  Alexander Turner  and Aleksander Madry. Ro-
bustness may be at odds with accuracy. In International conference on learning representations (ICLR) 
2019.

11

[44] Kagan Tumer and Joydeep Ghosh. Analysis of decision boundaries in linearly combined neural classiﬁers.

Pattern Recognition  29(2):341–348  1996.

[45] Kagan Tumer and Joydeep Ghosh. Error correlation and error reduction in ensemble classiﬁers. Connection

science  8(3-4):385–404  1996.

[46] Abhinav Valada  Johan Vertens  Ankit Dhall  and Wolfram Burgard. Adapnet: Adaptive semantic segmen-
tation in adverse environmental conditions. In IEEE international conference on robotics and automation
(ICRA)  pages 4644–4651  2017.

[47] Zining Wang  Wei Zhan  and Masayoshi Tomizuka. Fusing bird’s eye view lidar point cloud and front
view camera image for 3d object detection. In IEEE intelligent vehicles symposium (IV)  pages 1–6  2018.

[48] Pengcheng Wu  Steven CH Hoi  Hao Xia  Peilin Zhao  Dayong Wang  and Chunyan Miao. Online
In 21st ACM international

multimodal deep similarity learning with application to image retrieval.
conference on multimedia  pages 153–162. ACM  2013.

12

,Mehryar Mohri
Scott Yang
Taewan Kim
Joydeep Ghosh