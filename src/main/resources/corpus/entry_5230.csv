2018,Stochastic Cubic Regularization for Fast Nonconvex Optimization,This paper proposes a stochastic variant of a classic algorithm---the cubic-regularized Newton method [Nesterov and Polyak]. The proposed algorithm efficiently escapes saddle points and finds approximate local minima for general smooth  nonconvex functions in only $\mathcal{\tilde{O}}(\epsilon^{-3.5})$ stochastic gradient and stochastic Hessian-vector product evaluations. The latter can be computed as efficiently as stochastic gradients. This improves upon the $\mathcal{\tilde{O}}(\epsilon^{-4})$ rate of stochastic gradient descent. Our rate matches the best-known result for finding local minima without requiring any delicate acceleration or variance-reduction techniques.,Stochastic Cubic Regularization for Fast Nonconvex

Optimization

Nilesh Tripuraneni⇤ Mitchell Stern⇤ Chi Jin

Jeffrey Regier Michael I. Jordan

{nilesh_tripuraneni mitchell chijin regier}@berkeley.edu

jordan@cs.berkeley.edu

University of California  Berkeley

Abstract

This paper proposes a stochastic variant of a classic algorithm—the cubic-
regularized Newton method [Nesterov and Polyak  2006]. The proposed algorithm
efﬁciently escapes saddle points and ﬁnds approximate local minima for general
smooth  nonconvex functions in only ˜O(✏3.5) stochastic gradient and stochastic
Hessian-vector product evaluations. The latter can be computed as efﬁciently as
stochastic gradients. This improves upon the ˜O(✏4) rate of stochastic gradient
descent. Our rate matches the best-known result for ﬁnding local minima without
requiring any delicate acceleration or variance-reduction techniques.

1

Introduction

We consider the problem of nonconvex optimization in the stochastic approximation framework
[Robbins and Monro  1951]:

min
x2Rd

f (x) := E⇠⇠D[f (x; ⇠)].

(1)

In this setting  we only have access to the stochastic function f (x; ⇠)  where the random variable ⇠ is
sampled from an underlying distribution D. The task is to optimize the expected function f (x)  which
in general may be nonconvex. This framework covers a wide range of problems  including the ofﬂine
setting where we minimize the empirical loss over a ﬁxed amount of data  and the online setting
where data arrives sequentially. One of the most prominent applications of stochastic optimization
has been in large-scale statistics and machine learning problems  such as the optimization of deep
neural networks.
Classical analysis in nonconvex optimization only guarantees convergence to a ﬁrst-order stationary
point (i.e.  a point x satisfying krf (x)k = 0)  which can be a local minimum  a local maximum 
or a saddle point. This paper goes further  proposing an algorithm that escapes saddle points and
converges to a local minimum. A local minimum is deﬁned as a point x satisfying krf (x)k = 0
and r2f (x) ⌫ 0. Finding such a point is of special interest for a large class of statistical learning
problems where local minima are global or near-global solutions (e.g. Choromanska et al. [2015] 
Sun et al. [2016a b]  Ge et al. [2017]).
Among ﬁrst-order stochastic optimization algorithms  stochastic gradient descent (SGD) is perhaps
the simplest and most versatile. While SGD is computationally inexpensive  the best current guarantee
for ﬁnding an ✏-approximate local minimum (see Deﬁnition 1) requires O(✏4poly(d)) iterations
[Ge et al.  2015]  which is inefﬁcient in the high-dimensional regime.

⇤Equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In contrast  second-order methods which have access to the Hessian of f can exploit curvature to more
effectively escape saddles and arrive at local minima. However  constructing the full Hessian can be
prohibitively expensive in high-dimensions. Thus  recent work has explored the use of Hessian-vector
products r2f (x) · v  which can be computed as efﬁciently as gradients in many cases including
neural networks [Pearlmutter  1994].
Among second-order algorithms  one of the most natural extensions of the gradient descent algorithm
is the cubic-regularized Newton method of Nesterov and Polyak [2006]. Whereas gradient descent
ﬁnds the minimizer of a local second-order Taylor expansion at each step 

f (xt) + rf (xt)>(x  xt) +

`

2kx  xtk2  

xGD
t+1 = argmin

x

the cubic regularized Newton method ﬁnds the minimizer of a local third-order Taylor expansion 

xCubic
t+1 = argmin

x

hf (xt) + rf (xt)>(x  xt) +

1
2

(x  xt)>r2f (xt)(x  xt) +

⇢

6kx  xtk3i.

Most previous work on the cubic-regularized Newton method has focused on the non-stochastic or
partially-stochastic settings. This leads us to ask the central questions of this paper: Can we design
a fully stochastic variant of the cubic-regularized Newton method? If so  is such an algorithm
faster than SGD?
In this work  we answer both questions in the afﬁrmative  bridging the gap between its use in the
non-stochastic and stochastic settings.
Speciﬁcally  we propose a stochastic variant of the cubic-regularized Newton method. We pro-
vide a non-asymptotic analysis of its complexity  showing that the proposed algorithm ﬁnds an
✏-second-order stationary point using only ˜O(✏3.5) stochastic gradient and stochastic Hessian-vector
evaluations  where ˜O(·) hides poly-logarithmic factors.1 Our rate improves upon the ˜O(✏4) rate of
stochastic gradient descent  and matches the best-known result for ﬁnding local minima without the
need for any delicate acceleration or variance reduction techniques (see Section 1.1 for details).
We also empirically show that the stochastic cubic-regularized Newton method proposed in this
paper performs favorably on both synthetic and real non-convex problems relative to state-of-the-art
optimization methods.
1.1 Related Work

There has been a recent surge of interest in optimization methods that can escape saddle points and
ﬁnd ✏-approximate local minima (see Deﬁnition 1) in various settings. We provide a brief summary
of these results. All iteration complexities in this section are stated in terms of ﬁnding approximate
local minima  and only highlight the dependency on ✏ and d.

1.1.1 Singleton Function
This line of work optimizes over a general function f without any special structural assumptions.
In this setting  the optimization algorithm has direct access to the gradient or Hessian oracles at
each iteration. The work of Nesterov and Polyak [2006] ﬁrst proposed the cubic-regularized Newton
method  which requires O(✏1.5) gradient and Hessian oracle calls to the entire f  to ﬁnd an ✏-
second-order stationary point. Later  the ARC algorithm [Cartis et al.  2011] and trust-region methods
[Curtis et al.  2017] were also shown to achieve the same guarantee with similar Hessian oracle
access. However  these algorithms rely on having access to the full Hessian at each iteration  which is
prohibitive in high dimensions.
Recently  instead of using the full Hessian  Carmon and Duchi [2016] showed that using a gradient
descent solver for the cubic regularization subproblem allows their algorithm to ﬁnd ✏-second-order
stationary points in ˜O(✏2) Hessian-vector product evaluations. With acceleration techniques  the
number of Hessian-vector products can be reduced to ˜O(✏1.75) [Carmon et al.  2016  Agarwal et al. 
2017  Royer and Wright  2017].

1A single query  for a single realization ⇠ ⇠ D  to rf (x  ⇠) or r2f (x  ⇠)· v (for pre-speciﬁed v) is referred

to as a stochastic gradient or stochastic Hessian-vector oracle evaluation.

2

Method

Runtime

Variance Reduction

Stochastic Gradient Descent [Ge et al.  2015]

Natasha 2 [Allen-Zhu  2017]

Stochastic Cubic Regularization (this paper)

O(✏4poly(d))
˜O(✏3.5)2
˜O(✏3.5)

not needed

needed

not needed

Table 1: Comparison of our results to existing results for stochastic  nonconvex optimization with
provable convergence to approximate local minima.

Meanwhile  in the realm of entirely Hessian-free results  Jin et al. [2017] showed that a simple variant
of gradient descent can ﬁnd ✏-second stationary points in ˜O(✏2) gradient evaluations.
Note this line of work doesn’t accommodate stochastic gradients or stochastic Hessians. Restricting
access to only stochastic function queries makes the optimization problem more challenging.

✏3/2 + n3/4d

nPn

1.1.2 Finite-Sum Setting
In the ﬁnite-sum setting (also known as the ofﬂine setting) where f (x) := 1
i=1 fi(x)  one
assumes that algorithms have access to the individual functions fi. In this setting  variance reduction
techniques can be exploited [Johnson and Zhang  2013]. Agarwal et al. [2017] give an algorithm
requiring ˜O( nd
✏7/4 ) Hessian-vector oracle calls (each to an fi(x)) to ﬁnd an ✏-approximate
local minimum. A similar result is achieved by the algorithm proposed by Reddi et al. [2017].
1.1.3 Stochastic Approximation
The framework of stochastic approximation where f (x) := E⇠⇠D[f (x; ⇠)] only assumes access to
a stochastic gradient and Hessian via f (x; ⇠). In this setting  Ge et al. [2015] showed that the total
gradient iteration complexity for SGD to ﬁnd an ✏-second-order stationary point was O(✏4poly(d)).
More recently  Kohler and Lucchi [2017] consider a subsampled version of the cubic regularization
algorithm  but do not provide a non-asymptotic analysis for their algorithm to ﬁnd an approximate
local minimum; they also assume access to exact (expected) function values at each iteration which
are not available in the fully stochastic setting. Xu et al. [2017] consider the case of stochastic
Hessians  but also require access to exact gradients and function values at each iteration. Recently 
Allen-Zhu [2017] proposed an algorithm with a mechanism exploiting variance reduction that ﬁnds a
second-order stationary point with ˜O(✏3.5)2 Hessian-vector product evaluations.
After this work  Allen-Zhu and Li [2017] and Xu and Yang [2017] show how to use gradient evalua-
tions to efﬁciently approximate Hessian-vector products. Using this technique along with variance
reduction  they both provide algorithms achieving an ˜O(✏3.5) rate using gradient evaluations.
We note that our result matches the best results so far  using a simpler approach without any delicate
acceleration or variance-reduction techniques. See Table 1 for a brief comparison.
2 Preliminaries
We are interested in stochastic optimization problems of the form minx2Rd f (x) := E⇠⇠D[f (x; ⇠)] 
where ⇠ is a random variable with distribution D. In general  the function f (x) may be nonconvex.
This formulation covers both the standard ofﬂine setting where the objective function can be expressed
as a ﬁnite sum of n individual functions f (x  ⇠i)  as well as the online setting where data arrives
sequentially.
Our goal is to minimize the function f (x) using only stochastic gradients rf (x; ⇠) and stochastic
Hessian-vector products r2f (x; ⇠) · v  where v is a vector of our choosing. Although it is expensive
and often intractable in practice to form the entire Hessian  computing a Hessian-vector product is as
cheap as computing a gradient when our function is represented as an arithmetic circuit [Pearlmutter 
1994]  as is the case for neural networks.
Notation: We use bold uppercase letters A  B to denote matrices and bold lowercase letters x  y to
denote vectors. For vectors we use k·k to denote the `2-norm  and for matrices we use k·k to denote
2The original paper reports a rate of ˜O(✏3.25) due to a different deﬁnition of ✏-second-order stationary

point  min(r2f (x))  O(✏1/4)   which is weaker than the standard deﬁnition as in Deﬁnition 1.

3

the spectral norm and min(·) to denote the minimum eigenvalue. Unless otherwise speciﬁed  we use
the notation O(·) to hide only absolute constants which do not depend on any problem parameter 
and the notation ˜O(·) to hide only absolute constants and logarithmic factors.
2.1 Assumptions
Throughout the paper  we assume that the function f (x) is bounded below by some optimal value f⇤.
We also make the following assumptions about function smoothness:
Assumption 1. The function f (x) has

• `-Lipschitz gradients and ⇢-Lipschitz Hessians: for all x1 and x2 

krf (x1)  rf (x2)]k  `kx1  x2k;r2f (x1)  r2f (x2)  ⇢kx1  x2k.

The above assumptions state that the gradient and the Hessian cannot change dramatically in a small
local area  and are standard in prior work on escaping saddle points and ﬁnding local minima.
Next  we make the following variance assumptions about stochastic gradients and stochastic Hessians:
Assumption 2. The function f (x  ⇠) has

• for all x  Ehkrf (x  ⇠)  rf (x)k2i  2
• for all x  Ehr2f (x  ⇠)  r2f (x)2i  2

1 and krf (x  ⇠)  rf (x)k  M1 a.s.;

2 andr2f (x  ⇠)  r2f (x)  M2 a.s.

We note that the above assumptions are not essential to our result  and can be replaced by any
conditions that give rise to concentration. Moreover  the a.s. bounded Hessian assumption can be
removed if one further assumes f (x  ⇠) has `0-Lipschitz gradients for all ⇠  which guarantees Hessian
concentration with no further assumption on the variance of r2f (x  ⇠) [Tropp et al.  2015].
2.2 Cubic Regularization

Our target in this paper is to ﬁnd an ✏-second-order stationary point  which we deﬁne as follows:
Deﬁnition 1. For a ⇢-Hessian Lipschitz function f  we say that x is an ✏-second-order stationary
point (or ✏-approximate local minimum) if

krf (x)k  ✏

and min(r2f (x))  p⇢✏.

(2)

An ✏-second-order stationary point not only has a small gradient  but also has a Hessian which is
close to positive semi-deﬁnite. Thus it is often also referred to as an ✏-approximate local minimum.
In the deterministic setting  cubic regularization [Nesterov and Polyak  2006] is a classic algorithm
for ﬁnding a second-order stationary point of a ⇢-Hessian-Lipschitz function f (x). In each iteration 
it ﬁrst forms a local upper bound on the function using a third-order Taylor expansion around the
current iterate xt:

mt(x) = f (xt) + rf (xt)>(x  xt) +

1
2

(x  xt)>r2f (xt)(x  xt) +

⇢
6kx  xtk3.

This is called the cubic submodel. Then  cubic regularization minimizes this cubic submodel to obtain
the next iterate: xt+1 = argminx mt(x). When the cubic submodel can be solved exactly  cubic

regularization requires O⇣p⇢(f (x0)f⇤)

✏1.5

⌘ iterations to ﬁnd an ✏-second-order stationary point.

To apply this algorithm in the stochastic setting  three issues need to be addressed: (1) we only have
access to stochastic gradients and Hessians  not the true gradient and Hessian; (2) our only means of
interaction with the Hessian is through Hessian-vector products; (3) the cubic submodel cannot be
solved exactly in practice  only up to some tolerance. We discuss how to overcome each of these
obstacles in our paper.

3 Main Results
We begin with a general-purpose stochastic cubic regularization meta-algorithm in Algorithm 1 
which employs a black-box subroutine to solve stochastic cubic subproblems. At a high level  in

4

Algorithm 1 Stochastic Cubic Regularization (Meta-algorithm)
Input: mini-batch sizes n1  n2  initialization x0  number of iterations Tout  and ﬁnal tolerance ✏.
1: for t = 0  . . .   Tout do
Sample S1 {⇠i}n1
2:
gt 1
3:
4: Bt[·] 1
5:   m Cubic-Subsolver(gt  Bt[·]  ✏)
6:
7:
8:
9:
10:
end if
11:
12: end for
Output: x⇤ if the early termination condition was reached  otherwise the ﬁnal iterate xTout+1.

i=1  S2 {⇠i}n2
i=1.
|S1|P⇠i2S1 rf (xt; ⇠i)
|S2|P⇠i2S2 r2f (xt  ⇠i)(·)
100q ✏3
 Cubic-Finalsolver(gt  Bt[·]  ✏)
x⇤ xt + 
break

xt+1 xt + 
if m   1

⇢ then

order to deal with stochastic gradients and Hessians  we sample two independent minibatches S1 and
S2 at each iteration. Denoting the average gradient and average Hessian by

gt =

1

|S1| X⇠i2S1

rf (xt  ⇠i)  Bt =

1

|S2| X⇠i2S2

r2f (xt  ⇠i) 

this implies a stochastic cubic submodel:

(3)

(4)

mt(x) = f (xt) + (x  xt)>gt +

1
2

(x  xt)>Bt(x  xt) +

⇢
6kx  xtk3.

Although the subproblem depends on Bt  we note that our meta-algorithm never explicitly formulates
this matrix  only providing the subsolver access to Bt through Hessian-vector products  which
we denote by Bt[·] : Rd ! Rd. We hence assume that the subsolver performs gradient-based
optimization to solve the subproblem  as rmt(x) depends on Bt only via Bt[x  xt].
After sampling minibatches for the gradient and the Hessian  Algorithm 1 makes a call to a black-box
cubic subsolver to optimize the stochastic submodel mt(x). The subsolver returns a parameter
change   i.e.  an approximate minimizer of the submodel  along with the corresponding change
in submodel value  m := mt(xt + )  mt(xt). The algorithm then updates the parameters by
adding  to the current iterate  and checks whether m satisﬁes a stopping condition.
In more detail  the Cubic-Subsolver subroutine takes a vector g and a function for computing Hessian-
vector products B[·]  then optimizes the third-order polynomial ˜m() = >g + 1
2 >B[] +
6kk3. Let ? = argmin ˜m() denote the minimizer of this polynomial. In general  the
⇢
subsolver cannot return the exact solution ?. We hence tolerate a certain amount of suboptimality:
Condition 1. For any ﬁxed  small constant c  Cubic-Subsolver(g  B[·]  ✏) terminates within T (✏)
gradient iterations (which may depend on c)  and returns a  satisfying at least one of the following:

1. max{ ˜m()  f (xt + )  f (xt)}  ⌦(p✏3/⇢). (Case 1)
2. kk  k?k + cq ✏

⇢ and  if k?k  1

2p✏/⇢  then ˜m()  ˜m(?) + c

(Case 2)

12 · ⇢k?k3.

The ﬁrst condition is satisﬁed if the parameter change  results in submodel and function decreases
that are both sufﬁciently large (Case 1). If that fails to hold  the second condition ensures that 
is not too large relative to the true solution ?  and that the cubic submodel is solved to precision
c · ⇢k?k3 when k?k is large (Case 2).
As mentioned above  we assume the subsolver uses gradient-based optimization to solve the subprob-
lem so that it will only access the Hessian only through Hessian-vector products. Accordingly  it
can be any standard ﬁrst-order algorithm such as gradient descent  Nesterov’s accelerated gradient

5

Algorithm 2 Cubic-Subsolver via Gradient Descent
Input: g  B[·]  tolerance ✏.
1: if kgk  `2
⇢ then
2: Rc  g>B[g]
g
3:  Rc
kgk
4: else
5:  0   c0 p✏⇢

⇢kgk2 +r⇣ g>B[g]
⇢kgk2 ⌘2
`   ⌘ 1

+ 2kgk⇢

20`

2kk)

˜g g + ⇣ for ⇣ ⇠ Unif(Sd1)
for t = 1  . . .  T (✏) do
   ⌘(˜g + B[] + ⇢
end for

6:
7:
8:
9:
10: end if
11: m g> + 1
Output:   m

2 >B[] + ⇢

6kk3

Algorithm 3 Cubic-Finalsolver via Gradient Descent
Input: g  B[·]  tolerance ✏.
1:  0  gm g  ⌘ 1
2 do
2: while kgmk > ✏
3:    ⌘gm
4:
5: end while
Output: 

gm g + B[] + ⇢

2kk

20`

descent  etc. Gradient descent is of particular interest as it can be shown to satisfy Condition 1 (see
Lemma 1).
Having given an overview of our meta-algorithm and veriﬁed the existence of a suitable subsolver 
we are ready to present our main theorem:
Theorem 1. There exists an absolute constant c such that if f (x) satisﬁes Assumptions 1  2 
c2✏2 ) log⇣ dp⇢f
CubicSubsolver satisﬁes Condition 1 with c  n1  max( M1
max( M2
put an ✏-second-order stationary point of f with probability at least 1   within
⇢✏◆ · T (✏)◆◆

✏1.5c ⌘  and n2 
✏1.5c ⌘  then for all  > 0 and f  f (x0)  f⇤  Algorithm 1 will out-
✏1.5 ✓max✓ M1

c2⇢✏ ) log⇣ dp⇢f
˜O✓p⇢f

✏2◆ + max✓ M2p⇢✏

cp⇢✏   2

c✏   2

(5)

2
2

2
1

✏

 

 

1

2

total stochastic gradient and Hessian-vector product evaluations.

In the limit where ✏ is small the result simpliﬁes:

1

 

c1M1

4
2
c2
2M 2

2 ⇢o  then under the settings of Theorem 1 we can conclude that
Remark 1. If ✏  minn 2
Algorithm 1 will output an ✏-second-order stationary point of f with probability at least 1   within
˜O✓p⇢f
✏1.5 ✓ 2
(6)

⇢✏ · T (✏)◆◆

total stochastic gradient and Hessian-vector product evaluations.
Theorem 1 states that after Tout = ˜O(
) iterations  stochastic cubic regularization
(Algorithm 1) will have found an ✏-second-order stationary point w.h.p. Within each iteration  we
require n1 = ˜O( 2
⇢✏ ) samples for Hessian-vector
product averaging. Recall that the averaged gradient gt is ﬁxed for a given cubic submodel  while
the averaged Hessian-vector product Bt[v] needs to be recalculated every time the cubic subsolver

✏2 ) samples for gradient averaging and n2 = ˜O( 2

p⇢(f (x0)f⇤)

✏2 +

2
2

✏1.5

1

2

1

6

2

1

✏2 + 2

queries the gradient r ˜m(·). At most T (✏) such queries will be made by deﬁnition. Thus  each
iteration takes ˜O( 2
⇢✏ · T (✏)) stochastic gradient/Hessian-vector product evaluations when ✏ is
small (see Remark 1).
Finally  we note that lines 8-11 of Algorithm 1 give the termination condition of our meta-algorithm.
When the decrease in submodel value m is too small  our theory guarantees xt + ? is an ✏-
second-order stationary point  where ? is the optimal solution of the cubic submodel. However 
Cubic-Subsolver may only produce an inexact solution  satisfying Condition 1  which is not
sufﬁcient for xt +  to be an ✏-second-order stationary point. We therefore call Cubic-Finalsolver
(which just uses gradient descent and is detailed in Algorithm 3) to solve the subproblem with higher
precision.
3.1 Gradient Descent as a Cubic-Subsolver
One concrete example of a cubic subsolver is a simple variant of gradient descent (Algorithm 2)
as studied in Carmon and Duchi [2016]. The two main differences relative to standard gradient
descent are: (1) lines 1–3: when g is large  the submodel ˜m() may be ill-conditioned  so instead of
doing gradient descent  the iterate only moves one step in the g direction  which already guarantees
sufﬁcient descent; (2) line 6: the algorithm adds a small perturbation to g to avoid a certain “hard"
case for the cubic submodel. We refer readers to Carmon and Duchi [2016] for more details about
Algorithm 2.
Adapting their result for our setting  we obtain the following lemma  which states that the gradient
descent subsolver satisﬁes our Condition 1.
Lemma 1. There exists an absolute constant c0  such that under the same assumptions on f (x)
and the same choice of parameters n1  n2 as in Theorem 1  Algorithm 2 satisﬁes Condition 1 with
probability at least 1  0 with T (✏)  ˜O( `p⇢✏ ).
Our next corollary applies gradient descent (Algorithm 2) as the approximate cubic subsolver in our
meta-algorithm (Algorithm 1)  which immediately gives the total number of gradient and Hessian-
vector evaluations for the full algorithm.

Corollary 1. Under the same settings as Theorem 1  if ✏  minn 2
2 ⇢o  and if we instantiate
the Cubic-Subsolver subroutine with Algorithm 2  then with probability greater than 1   Algorithm
1 will output an ✏-second-order stationary point of f (x) within
p⇢✏◆◆

˜O✓p⇢f

✏1.5 ✓ 2

2
2
⇢✏ ·

4
2
c2
2M 2

1

✏2 +

(7)

total stochastic gradient and Hessian-vector product evaluations.

1

c1M1

 

`

From Corollary 1  we observe that the dominant term in solving the submodel is 2
✏2 when ✏ is
sufﬁciently small  giving a total iteration complexity of ˜O(✏3.5) when other problem-dependent
parameters are constant. This improves on the ˜O(✏4poly(d)) complexity attained by SGD.
4 Proof Sketch

1

This section sketches the crucial steps needed to understand and prove our main theorem (Theorem
1). We describe our high-level approach  and provide a proof sketch in Appendix A in the stochastic
setting  assuming oracle access to an exact subsolver. For the case of an inexact subsolver and full
proof details  we defer to Appendix B.
Recall that at iteration t of Algorithm 1  a stochastic cubic submodel mt is constructed around the
current iterate xt with the form given in the following Equation  mt(x) = f (xt) + (x  xt)>gt +
6kx  xtk3  where gt and Bt are the averaged stochastic gradients and
2 (x  xt)>Bt(x  xt) + ⇢
1
Hessians. At a high level  we will show that for each iteration  the following two claims hold:
Claim 1. If xt+1 is not an ✏-second-order stationary point of f (x)  the cubic submodel has large
descent mt(xt+1)  mt(xt).
Claim 2. If the cubic submodel has large descent mt(xt+1)  mt(xt)  then the true function also
has large descent f (xt+1)  f (xt).

7

Figure 1: The piecewise cubic function w(x) used along one of the dimensions in the synthetic
experiment. The other dimension uses a scaled quadratic.

Given these claims  it is straightforward to argue for the correctness of our approach. We know that if
we observe a large decrease in the cubic submodel value mt(xt+1)  mt(xt) during Algorithm 1 
then by Claim 2 the function will also have large descent. But since f is bounded below  this cannot
happen indeﬁnitely  so we must eventually encounter an iteration with small cubic submodel descent.
When that happens  we can conclude by Claim 1 that xt+1 is an ✏-second-order stationary point.
We note that Claim 2 is especially important in the stochastic setting  as we no longer have access to
the true function but only the submodel. Claim 2 ensures that progress in mt still indicates progress
in f  allowing the algorithm to terminate at the correct time. A more detailed proof sketch and full
proofs can be found in the Appendix.
5 Experiments
In this section  we provide empirical results on synthetic and real-world data sets to demonstrate the
efﬁcacy of our approach. All experiments are implemented using TensorFlow [Abadi et al.  2016] 
which allows for efﬁcient computation of Hessian-vector products using the method described by
Pearlmutter [1994].

5.1 Synthetic Nonconvex Problem

We begin by constructing a nonconvex problem with a saddle point to compare our proposed approach
against stochastic gradient descent. Let w(x) be the W-shaped scalar function depicted in Figure 1 
with a local maximum at the origin and two local minima on either side. We defer the exact form of
w(x) to Appendix D.
We aim to solve the problem

min

x2R2⇥w(x1) + 10x2
2⇤  

with independent noise drawn from N (0  1) added separately to each component of every gradient
and Hessian-vector product evaluation. By construction  the objective function has a saddle point at
the origin with Hessian eigenvalues of -0.2 and 20  providing a simple but challenging test case.
We ran our method and SGD on this problem  plotting the objective value versus the number of oracle
calls in Figure 2. The batch sizes and learning rates for each method are tuned separately to ensure a
fair comparison; see Appendix D for details. We observe that our method is able to escape the saddle
point at the origin and converge to one of the global minima faster than SGD.
5.2 Deep Autoencoder
In addition to the synthetic problem above  we also investigate the performance of our approach
on a more practical problem from deep learning  namely training a deep autoencoder on MNIST
[LeCun and Cortes  2010]. Our architecture consists of a fully connected encoder with dimensions
(28 ⇥ 28) ! 512 ! 256 ! 128 ! 32 together with a symmetric decoder. We use a softplus
nonlinearity (deﬁned as softplus(x) = log(1 + exp(x))) for each hidden layer  an elementwise
sigmoid for the ﬁnal layer  and a pixelwise `2 loss between the input and the reconstructed output
as our objective function. Results on this autoencoding task are presented in Figure 3. In addition
to training the model with our method and SGD  we also include results using AdaGrad  a popular
adaptive ﬁrst-order method with strong empirical performance [Duchi et al.  2011]  together with
results for a method combining variance reduction and second-order information proposed by Reddi
et al. [2017]. More details about our experimental setup can be found in Appendix D.

8

-0.50.5-0.006-0.004-0.0020.0020.0040.006Figure 2: Results on our synthetic nonconvex
optimization problem. Stochastic cubic regular-
ization escapes the saddle point at the origin and
converges to a global optimum faster than SGD.

Figure 3: Results on the MNIST deep autoencod-
ing task. Multiple saddle points are present in
the optimization problem. Stochastic cubic reg-
ularization is able to escape them most quickly 
allowing it to reach a local minimum faster than
other methods.

We observe that stochastic cubic regularization quickly escapes two saddle points and descends toward
a local optimum  while AdaGrad takes two to three times longer to escape each saddle point  and
SGD is slower still. This demonstrates that incorporating curvature information via Hessian-vector
products can assist in escaping saddle points in practice. The hybrid method consisting of SVRG
interleaved with Oja’s algorithm for Hessian descent also improves upon SGD but does not match the
performance of our method or AdaGrad.

6 Conclusion
In this paper  we presented a stochastic algorithm based on the classic cubic-regularized Newton
method for nonconvex optimization. Our algorithm provably ﬁnds ✏-approximate local minima in
˜O(✏3.5) total gradient and Hessian-vector product evaluations  improving upon the ˜O(✏4poly(d))
rate of SGD. Our experiments show the favorable performance of our method relative to SGD on
both a synthetic and a real-world problem.

References
Martin Abadi  Paul Barham  et al. Tensorﬂow: A system for large-scale machine learning. In 12th

USENIX Symposium on Operating Systems Design and Implementation  pages 265–283  2016.

Naman Agarwal  Zeyuan Allen-Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Finding approximate
local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing  2017.

Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD.

arXiv:1708.08694  2017.

arXiv preprint

Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via ﬁrst-order oracles. arXiv

preprint arXiv:1711.06673  2017.

Yair Carmon and John Duchi. Gradient descent efﬁciently ﬁnds the cubic-regularized non-convex

Newton step. arXiv preprint arXiv:1612.00547  2016.

Yair Carmon  John Duchi  Oliver Hinder  and Aaron Sidford. Accelerated methods for non-convex

optimization. arXiv preprint arXiv:1611.00756  2016.

Coralia Cartis  Nicholas Gould  and Philippe Toint. Adaptive cubic regularisation methods for
unconstrained optimization. Part II: worst-case function-and derivative-evaluation complexity.
Mathematical Programming  130(2):295–319  2011.

9

Anna Choromanska  Mikael Henaff  Michael Mathieu  Gerard Ben Arous  and Yann LeCun. The
loss surfaces of multilayer networks. In Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics  2015.

Frank E Curtis  Daniel P Robinson  and Mohammadreza Samadi. A trust region algorithm with a
worst-case iteration complexity of O(✏3/2) for nonconvex optimization. Mathematical Program-
ming  162(1-2):1–32  2017.
John C. Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning

and stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2011.

Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points — online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory 
2015.

Rong Ge  Chi Jin  and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
uniﬁed geometric analysis. In Proceedings of the 34th International Conference on Machine
Learning  2017.

Prateek Jain  Chi Jin  Sham M Kakade  Praneeth Netrapalli  and Aaron Sidford. Streaming PCA:
Matching matrix Bernstein and near-optimal ﬁnite sample guarantees for Oja’s algorithm. In
Conference on Learning Theory  pages 1147–1164  2016.

Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M. Kakade  and Michael I. Jordan. How to escape
saddle points efﬁciently. In Proceedings of the 34th International Conference on Machine Learning 
2017.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  2013.

Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex opti-

mization. In Proceedings of the 34th International Conference on Machine Learning  2017.

Yann LeCun and Corinna Cortes. MNIST Handwritten Digit Database  2010. URL http://yann.

lecun.com/exdb/mnist/.

Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course  volume 87. Springer

Science & Business Media  2013.

Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance.

Mathematical Programming  108(1):177–205  2006.

Barak A Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation  6(1):147–160 

1994.

Sashank J Reddi  Manzil Zaheer  et al. A generic approach for escaping saddle points. arXiv preprint

arXiv:1709.01434  2017.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  pages 400–407  1951.

Clément W Royer and Stephen J Wright. Complexity analysis of second-order line-search algorithms

for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131  2017.

Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery over the sphere I: Overview and

the geometric picture. IEEE Transactions on Information Theory  2016a.

Ju Sun  Qing Qu  and John Wright. A geometric analysis of phase retrieval. In IEEE International

Symposium on Information Theory. IEEE  2016b.

in Machine Learning  8(1-2):1–230  2015.

Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends R
Peng Xu  Farbod Roosta-Khorasani  and Michael W Mahoney. Newton-type methods for non-convex

optimization under inexact Hessian information. arXiv preprint arXiv:1708.07164  2017.

Yi Xu and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost

linear time. arXiv preprint arXiv:1711.01944  2017.

10

,Jimmy Ba
Russ Salakhutdinov
Roger Grosse
Brendan Frey
Xiangru Lian
Huan Zhang
Cho-Jui Hsieh
Yijun Huang
Ji Liu
Nilesh Tripuraneni
Mitchell Stern
Chi Jin
Jeffrey Regier
Michael Jordan