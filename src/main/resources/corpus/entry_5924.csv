2008,Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning,Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice  Minsky once came to him as he sat hacking at the PDP-6. ``What are you doing?'' asked Minsky. ``I am training a randomly wired neural net to play tic-tac-toe '' Sussman replied. ``Why is the net wired randomly?'' asked Minsky. Sussman replied  ``I do not want it to have any preconceptions of how to play.'' Minsky then shut his eyes. ``Why do you close your eyes?'' Sussman asked his teacher. ``So that the room will be empty '' replied Minsky. At that moment  Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Specifically  we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classification performance  and bound their test error in terms of the size of the dataset and the number of random nonlinearities.,Weighted Sums of Random Kitchen Sinks: Replacing

minimization with randomization in learning

Paper #858

Abstract

Randomized neural networks are immortalized in this AI Koan:

In the days when Sussman was a novice  Minsky once came to him as he sat

hacking at the PDP-6.

“What are you doing?” asked Minsky. “I am training a randomly wired
neural net to play tic-tac-toe ” Sussman replied. “Why is the net wired ran-
domly?” asked Minsky. Sussman replied  “I do not want it to have any precon-
ceptions of how to play.”

Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked
his teacher. “So that the room will be empty ” replied Minsky. At that moment 
Sussman was enlightened.

We analyze shallow random networks with the help of concentration of measure
inequalities. Speciﬁcally  we consider architectures that compute a weighted sum
of their inputs after passing them through a bank of arbitrary randomized nonlin-
earities. We identify conditions under which these networks exhibit good classi-
ﬁcation performance  and bound their test error in terms of the size of the dataset
and the number of random nonlinearities.

1 Introduction

In the earliest days of artiﬁcial intelligence  the bottom-most layer of neural networks consisted
of randomly connected “associator units” that computed random binary functions of their inputs
[1]. These randomized shallow networks have largely been superceded by optimally  or nearly
optimally  tuned shallow architectures such as weighted sums of positive deﬁnite kernels (as in
Support Vector Machines)  or weigted sums of weak classiﬁers (as in Adaboost). But recently 
architectures that randomly transform their inputs have been resurfacing in the machine learning
community [2  3  4  5]  largely motivated by the fact that randomization is computationally cheaper
than optimization. With the help of concentration of measure inequalities on function spaces  we
show that training a shallow architecture by randomly choosing the nonlinearities in the ﬁrst layer
results in a classiﬁer that is not much worse than one constructed by optimally tuning the non-
linearities. The main technical contributions of the paper are an approximation error bound (Lemma
1)  and a synthesis of known techniques from learning theory to analyze random shallow networks.
Consider the problem of ﬁtting a function f : X → R to a training data set of m input-output pairs
{xi  yi}i=1...m  drawn iid from some unknown distribution P (x  y)  with xi ∈ X and yi ∈ ±1. The
ﬁtting problem consists of ﬁnding an f that minimizes the empirical risk

m(cid:88)

i=1

1

Remp[f] ≡ 1
m

c(f(xi)  yi).

(1)

The loss c(y  y(cid:48)) penalizes the deviation between the prediction f(x) and the label y. Popular choices
for c are the hinge loss  max(0  1 − yy(cid:48))  used in the Support Vector Machine [6]  the exponential
  used in Adaboost [7  8]  and the quadratic loss  (y − y(cid:48))2  used in matching pursuit [9]
loss  e−yy(cid:48)
and regularized least squares classiﬁcation [10].

i=1 α(wi)φ(x; wi) or f(x) = (cid:82) α(w)φ(x; w) dw  where feature functions φ : X × Ω → R 
(cid:80)∞

Similarly to kernel machines and Adaboost  we will consider functions of the form f(x) =
parameterized by some vector w ∈ Ω  are weighted by a function α : Ω → R. In kernel machines 
the feature functions φ are the eigenfunctions of a positive deﬁnite kernel k  and in Adaboost they
are typically decision trees or stumps. Adaboost [8  7] and matching pursuit [11  9] ﬁnd approximate
empirical risk minimizer over this class of functions by greedily minimizing over a ﬁnite number of
scalar weights α and parameter vectors w jointly:

(cid:35)

(cid:34) K(cid:88)

k=1

w1  . . .   wK ∈ Ω

minimize
α ∈ A

Remp

φ(x; wk)αk

.

(2)

But it is also possible to randomize over w and minimize over α. Rather than jointly optimizing over
α and w  the following algorithm ﬁrst draws the parameters of the nonlinearities randomly from a
pre-speciﬁcied distribution p. Then with w ﬁxed  it ﬁts the weights α optimally via a simple convex
optimization:

Algorithm 1 The Weighted Sum of Random Kitchen Sinks ﬁtting procedure.
Input: A dataset {xi  yi}i=1...m of m points  a bounded feature function |φ(x; w)| ≤ 1  an integer

K  a scalar C  and a probability distribution p(w) on the parameters of φ.

Output: A function ˆf(x) =(cid:80)K

k=1 φ(x; wk)αk.

Draw w1  . . .   wK iid from p.
Featurize the input: zi ← [φ(xi; w1)  . . .   φ(xi; wK)](cid:62).
With w ﬁxed  solve the empirical risk minimization problem

minimize

α∈RK

1
m

c(cid:0)α(cid:62)zi  yi

m(cid:88)

i=1

(cid:1)

(3)

(4)

s.t. (cid:107)α(cid:107)∞ ≤ C/K.

In pratice  we let C be large enough that the constraint (4) remains inactive. The when c is the
quadratic loss  the minimization (3) is simple linear least squares  and when c is the hinge loss  it
amounts of ﬁtting a linear SVM to a dataset of m K-dimensional feature vectors.
Randomly setting the nonlinearities is appealing for several reasons. First  the ﬁtting procedure
is simple: Algorithm 1 can be implemented in a few lines of MATLAB code even for complex
feature functions φ  whereas ﬁtting nonlinearities with Adaboost requires much more care. This
ﬂexibility allows practioners to experiment with a wide variety of nonlinear feature fuctions without
ﬁrst having to devise ﬁtting procedures for them. Second  the algorithm is fast: experiments show
between one and three orders of magnitude speedup over Adaboost. On the down side  one might
expect to have to tune the sampling distribution p for each dataset. But in practice  we ﬁnd that to
obtain accuracies that are competitive with Adaboost  the same sampling distribution can be used
for all the datasets we considered if the coordinates of the data are ﬁrst zero-meaned and rescaled to
unit variance.
Formally  we show that Algorithm 1 returns a function that has low true risk. The true risk of a
function f is

R[f] ≡ E

(x y)∼P

c(f(x)  y) 

(5)

and measures the expected loss of f on as-yet-unseen test points  assuming these test points are
generated from the same distribution that generated the training data. The following theorem states
that with very high probability  Algorithm 1 returns a function whose true risk is near the lowest true
risk attainable by functions in the class Fp deﬁned below:
Theorem 1 (Main result). Let p be a distribution on Ω  and let φ satisfy supx w |φ(x; w)| ≤ 1.
Deﬁne the set

(cid:26)

(cid:90)

Fp ≡

f(x) =

α(w)φ(x; w) dw

(cid:12)(cid:12)(cid:12)(cid:12) |α(w)| ≤ Cp(w)

(cid:27)

.

(6)

Ω

2

Suppose c(y  y(cid:48)) = c(yy(cid:48))  with c(yy(cid:48)) L-Lipschitz. Then for any δ > 0  if the training data
{xi  yi}i=1...m are drawn iid from some distribution P   Algorithm 1 returns a function ˆf that satis-
ﬁes

R[ ˆf] − min
f∈Fp

R[f] ≤ O

(7)
with probability at least 1 − 2δ over the training dataset and the choice of the parameters
w1  . . .   wK.

log 1
δ

LC

+

m

1√
K

(cid:18)(cid:18) 1√

(cid:19)

(cid:113)

(cid:19)

Note that the dependence on δ in the bound is logarithmic  so even small δ’s do not cause the
bound to blow up. The set Fp is a rich class of functions. It consists of functions whose weights
α(w) decays more rapidly than the given sampling distribution p. For example  when φ(x; w) are
sinusoids with frequency w  Fp is the set of all functions whose Fourier transforms decay faster than
C p(w).
We prove the theorem in the next section  and demonstrate the algorithm on some sample datasets in
Section 4. The proof of the theorem provides explicit values for the constants in the big O notation.

2 Proof of the Main Theorem

Algorithm 1 returns a function that lies in the random set

(cid:40)

K(cid:88)

k=1

ˆFw ≡

f(x) =

αkφ(x; wk)

(cid:12)(cid:12)(cid:12)(cid:12) |αk| ≤ C

K

(cid:41)

.

(8)

The bound in the main theorem can be decomposed in a standard way into two bounds:

1. An approximation error bound that shows that the lowest true risk attainable by a function

in ˆFw is not much larger than the lowest true risk attainable in Fp (Lemma 2).

2. An estimation error bound that shows that the true risk of every function in ˆFw is close to

its empirical risk (Lemma 3).

The following Lemma is helpful in bounding the approximation error:
Lemma 1. Let µ be a measure on X   and f∗ a function in Fp. If w1  . . .   wK are drawn iid from p 
then for any δ > 0  with probability at least 1 − δ over w1  . . .   wK  there exists a function ˆf ∈ ˆFw
so that

(cid:18)

(cid:113)

(cid:19)

dµ(x) ≤ C√
K

1 +

2 log 1
δ

.

(9)

(cid:115)(cid:90)

X

(cid:16) ˆf(x) − f∗(x)
(cid:17)2

The proof relies on Lemma 4 of the Appendix  which states that the average of bounded vectors in
a Hilbert space concentrates towards its expectation in the Hilbert norm exponentially fast.

Proof. Since f∗ ∈ Fp  we can write f∗(x) =(cid:82)
product (cid:104)f  g(cid:105) =(cid:82) f(x)g(x) dµ(x)  (cid:107)βkφ(·; wk)(cid:107) ≤ C. The Lemma follows by applying Lemma

Ω α(w)φ(x; w) dw. Construct the functions fk =
βkφ(·; wk)  k = 1 . . . K  with βk ≡ α(ωk)
K φ(x; ωk) be the
βk
sample average of these functions. Then ˆf ∈ ˆFw because |βk/K| ≤ C/K. Also  under the inner

p(ωk)   so that E fk = f∗. Let ˆf(x) =(cid:80)K

4 to f1  . . .   fK under this inner product.
Lemma 2 (Bound on the approximation error). Suppose c(y  y(cid:48)) is L-Lipschitz in its ﬁrst argument.
Let f∗ be a ﬁxed function in Fp. If w1  . . .   wK are drawn iid from p  then for any δ > 0  with
probability at least 1 − δ over w1  . . .   wK  there exists a function ˆf ∈ ˆFw that satisﬁes

k=1

(cid:18)

(cid:113)

(cid:19)

1 +

2 log 1
δ

.

(10)

R[ ˆf] ≤ R[f∗] + LC√
K

3

Proof. For any two functions f and g  the Lipschitz condition on c followed by the concavity of
square root gives

R[f] − R[g] = E c(f(x)  y) − c(g(x)  y) ≤ E|c(f(x)  y) − c(g(x)  y)|

≤ L E|f(x) − g(x)| ≤ L(cid:112)E(f(x) − g(x))2.

(11)
(12)

The lemma then follows from Lemma 1.

Next  we rely on a standard result from statistical learning theory to show that for a given choice of
w1  . . .   wK the empirical risk of every function in ˆFw is close to its true risk.
Lemma 3 (Bound on the estimation error). Suppose c(y  y(cid:48)) = c(yy(cid:48))  with c(yy(cid:48)) L-Lipschitz. Let
w1 ···   wK be ﬁxed. If {xi  yi}i=1...m are drawn iid from a ﬁxed distribution  for any δ > 0  with
probability at least 1 − δ over the dataset  we have

∀f∈ ˆFw

|R[f] − Remp[f]| ≤ 1√
m

4LC + 2|c(0)| + LC

2 log 1

δ

.

(13)

(cid:18)

(cid:113) 1

(cid:19)

Proof sketch. By H¨older  the functions in ˆFw are bounded above by C. The Rademacher complexity
√
of ˆFw can be shown to be bounded above by C/
m (see the Appendix). The theorem follows by
results from [12] which are summarized in Theorem 2 of the Appendix.
Proof of Theorem 1. Let f∗ be a minimizer of R over Fp  ˆf a minimizer of Remp over ˆFw (the
output of the algorithm)  and ˆf∗ a minimizer of R over ˆFw. Then

R[ ˆf] − R[f∗] = R[ ˆf] − R[ ˆf∗] + R[ ˆf∗] − R[f∗]
≤ |R[ ˆf] − R[ ˆf∗]| + R[ ˆf∗] − R[f∗].

(14)
(15)

The ﬁrst term in the right side is an estimation error: By Lemma 3  with probability at least
1 − δ  |R[ ˆf∗] − Remp[ ˆf∗]| ≤ est and simultaneously  |R[ ˆf] − Remp[ ˆf]| ≤ est  where est
is the right side of the bound in Lemma 3. By the optimality of ˆf  Remp[ ˆf] ≤ Remp[ ˆf∗].
Combining these facts gives that with probability at least 1 − δ  |R[ ˆf] − R[ ˆf∗]| ≤ 2est =
2√
m

4LC + 2|c(0)| + LC

(cid:113) 1

2 log 1

(cid:17)

(cid:16)

.

δ

The second term in Equation (15) is the approximation error  and by Theorem 1  with probability at
least 1 − δ  it is bounded above by app = LC√
By the union bound  with probability at least 1−2δ  the right side of Equation (15) is bounded above
by 2est + app.

2 log 1
δ

1 +

K

.

(cid:16)

(cid:113)

(cid:17)

3 Related Work

Greedy algorithms for ﬁtting networks of the form (2) have been analyzed  for example  in [7  11  9].
Zhang analyzed greedy algorithms and a randomized algorithm similar to Algorithm 1 for ﬁtting
sparse Gaussian processes to data  a more narrow setting than we consider here. He obtained bounds
on the expected error for this sparse approximation problem by viewing these methods as stochastic
gradient descent.
Approximation error bounds such as that of Maurey [11][Lemma 1]  Girosi [13] and Gnecco and
Sanguineti [14] rely on random sampling to guarantee the existence of good parameters w1  . . .   wk 
but they require access to the representation of f∗ to actually produce these parameters. These ap-
proximation bounds cannot be used to guarantee the performance of Algorithm 1 because Algorithm
1 is oblivious of the data when it generates the parameters. Lemma 2 differs from these bounds in
that it relies on f∗ only to generate the weights α1  . . .   αK  but it remains oblivious to f∗ when
generating the parameters by sampling them from p instead. Furthermore  because ˆFw is smaller
than the classes considered by [11  14]  the approximation error rate in Lemma 1 matches those of
existing approximation error bounds.

4

Figure 1: Comparisons between Random Kitchen Sinks and Adaboosted decision stumps on adult
(ﬁrst row)  activity (second row)  and KDDCUP99 (third row). The ﬁrst column plots test error
of each classiﬁer as a function of K. The accuracy of Random Kitchen Sinks catches up to that of
Adaboost as K grows. The second column plots the total training and testing time as a function
of K. For a given K  Random Kitchen Sinks is between two and three orders of magnitude faster
than Adaboost. The third column combines the previous two columns. It plots testing+training time
required to achieve a desired error rate. For a given error rate  Random Kitchen Sinks is between
one and three orders of magnitude faster than Adaboost.

4 Experiments

Since others have already empirically demonstrated the beneﬁts of random featurization [2  3  4  5] 
we only a present a few illustrations in this section.
We compared Random Kitchen Sinks with Adaboost on three classiﬁcation problems: The adult
dataset has roughly 32 000 training instances. Each categorical variable was replaced by a binary in-
dicator variable over the categories  resulting in 123 dimensions per instance. The test set consists of
15 000 instances. KDDCUP99 is a network intrusion detection problem with roughly 5 000 000 127-
dimensional training instances  subsampled to 50 000 instances. The test set consists of 150 000 in-
stances. activity is a human activity recognition dataset with 20 0000 223-dimensional instance 
of which about 200 are irrelevant for classiﬁcation. The test set constists of 50 000 instances. The
datasets were preprocessed by zero-meaning and rescaling each dimension to unit variance. The
feature functions in these experiments were decision stumpsφ(x; w) = sign(xwd − wt)  which sim-
ply determine whether the wdth dimension of x is smaller or greater than the threshold wt. The
sampling distribution p for Random Kitchen Sinks drew the threshold parameter wt from a normal
distribution and the coordinate wd from a uniform distribution over the coorindates. For some ex-
periments  we could afford to run Random Kitchen Sinks for larger K than Adaboost  and these runs
are included in the plots. We used the quadratic loss  but ﬁnd no substantial differences in quality
under the hinge loss (though there is degradation in speed by a factor of 2-10). We used MATLAB
optimized versions of Adaboost and Random Kitchen Sinks  and report wall clock time in seconds.
Figure 1 compares the results on these datasets. Adaboost expends considerable effort in choosing
the decision stumps and achieves good test accuracy with a few of them. Random Kitchen Sinks

5

0100200300400500600700800900100014161820222426# weak learners (K)% error  AdaboostRKS0100200300400500600700800900100010−1100101102# weak learners (K)training+testing time (seconds)  AdaboostRKS141516171819202122232410−1100101102% errortraining+testing time (seconds)  AdaboostRKS0501001502002503003504001012141618202224262830# weak learners (K)% error  AdaboostRKS050100150200250300350400100101102103# weak learners (K)training+testing time (seconds)  AdaboostRKS1012141618202224262830100101102103% errortraining+testing time (seconds)  AdaboostRKS010020030040050060070068101214161820# weak learners (K)% error  AdaboostRKS010020030040050060070010−310−210−1100101102103# weak learners (K)training+testing time (seconds)  AdaboostRKS6810121416182010−310−210−1100101102103% errortraining+testing time (seconds)  AdaboostRKSFigure 2: The L∞ norm of α returned by RKS for 500 different runs of RKS with various settings
of K on adult. (cid:107)α(cid:107)∞ decays with K  which justiﬁes dropping the constraint (4) in practice.

requires more nonlinearities to achieve similar accuracies. But because it is faster than Adaboost  it
can produce classiﬁers that are just as accurate as Adaboost’s with more nonlinearities in less total
time. In these experiments  Random Kitchen Sinks is almost as accurate as Adaboost but faster by
one to three orders of magnitude.
We defer the details of the following experiments to a technical report: As an alternative to Adaboost 
we have experimented with conjugate gradient-descent based ﬁtting procedures for (2)  and ﬁnd
again that randomly generating the nonlinearities produces equally accurate classiﬁers using many
more nonlinearities but in much less time. We obtain similar results as those of Figure 1 with the
random features of [4]  and random sigmoidal ridge functions φ(x; w) = σ(w(cid:48)x) 
To simplify the implementation of Random Kitchen Sinks  we ignore the constraint (4) in practice.
The scalar C controls the size of ˆFw and Fp  and to eliminate the constraint  we implicitly set C it
to a large value so that the constraint is never tight. However  for the results of this paper to hold  C
cannot grow faster than K. Figure 2 shows that the L∞ norm of the unconstrained optimum of (3)
for the adult dataset does decays linearly with K  so that there exists a C that does not grow with
K for which the constraint is never tight  thereby justifying dropping the constraint.

5 Discussion and Conclusions

Various hardness of approximation lower bounds for ﬁxed basis functions exist (see  for example
[11]). The guarantee in Lemma 1 avoids running afoul of these lower bounds because it does not
seek to approximate every function in Fp simultaneously  but rather only the true risk minimizer
with high probability.
It may be surprising that Theorem 1 holds even when the feature functions φ are nearly orthogo-
nal. The result works because the importance sampling constraint |α(w)| ≤ Cp(w) ensures that
a feature function does not receive a large weight if it is unlikely to be sampled by p. When
the feature functions are highly linearly dependent  better bounds can be obtained because any

f(x) =(cid:82) α(w)φ(x; w) can be rewritten as f(x) =(cid:82) α(cid:48)(w)φ(x; w) with |α(cid:48)|/p ≤ |α|/p  improving
the importance ratio C. This intuition can be formalized via the the Rademacher complexity of φ  a
result which we leave for future work.
(cid:82)
One may wonder whether Algorithm 1 has good theoretical guarantees on Fp because Fp is
Indeed  when φ are the Fourier bases  |α|/p ≤ C implies
too small small class of functions.
Ω |α(w)| dw ≤ C  so every function in Fp has an absolutely integrable Frourier transform. Thus
Fp is smaller than the set considered by Jones [9] for greedy matching pursuit  and for which he
√
K). The most reliable way to show that Fp is rich enough
obtained an approximation rate of O(1/
for practical applications is to conduct experiments with real data. The experiment show that Fp
indeed contains good predictors.
√
The convergence rate for Adaboost [7] is exponentially fast in K  which at ﬁrst appears to be much
faster than 1/
K. However  the base of the exponent is the minimum weighted margin encountered
by the algorithm through all iterations  a quantity that is difﬁcult to bound a priori. This makes a
direct comparison of the bounds difﬁcult  though we have tried to provide empirical comparisons.

6

501001502002503003504004500.150.20.250.30.350.4K||α||∞A Exponentially Fast Concentration of Averages towards the Mean in a

Hilbert Space

(cid:80)K

Lemma 4. Let X = {x1 ···   xK} be iid random variables in a ball H of radius M centered
around the origin in a Hilbert space. Denote their average by X = 1
k=1 xk. Then for any
δ > 0  with probability at least 1 − δ 
K

(cid:13)(cid:13)X − E X(cid:13)(cid:13) ≤ M√

(cid:18)

(cid:113)

(cid:19)

1 +

2 log 1
δ

K

Proof. We use McDiarmid’s inequality to show that the scalar function f(X) = (cid:13)(cid:13)X − EX X(cid:13)(cid:13) is

√
K).
concentrated about its mean  which shrinks as O(1/
The function f is stable under perturbation of its ith argument. Let ˜X = {x1 ···   ˜xi ···   xK}
be a copy of X with the ith element replaced by an arbitrary element of H. Applying the triangle
inequality twice gives

.

(16)

|f(X) − f( ˜X)| = |(cid:107)X − E X(cid:107) − (cid:107) ˜X − E X(cid:107)| ≤ (cid:107)X − ˜X(cid:107) ≤ (cid:107)xi − ˜xi(cid:107)

≤ 2M
K

.

(17)

K

To bound the expectation of f  use the familiar identity about the variance of the average of iid
random variables

in conjunction with Jensen’s inequality and the fact that (cid:107)x(cid:107) ≤ M to get

1
K

E(cid:13)(cid:13)X − E X(cid:13)(cid:13)2 =
E f(X) ≤(cid:112)E f 2(X) =
(cid:20)

(cid:21)

(cid:0)E(cid:107)x(cid:107)2 − (cid:107) E x(cid:107)2(cid:1)  
(cid:113)
E(cid:13)(cid:13)X − E X(cid:13)(cid:13)2 ≤ M√

K

.

(cid:18)

(cid:21)

(cid:20)

Pr
X

f(X) − M√
K

≥ 

≤ Pr

X

f(X) − E f(X) ≥ 

≤ exp

− K2
2M 2

This bound for the expectation of f and McDiarmid’s inequality give

(18)

(19)

(20)

(cid:19)

To get the ﬁnal result  set δ to the right hand side  solve for   and rearrange.

B Generalization bounds that use Rademacher complexity
One measure of the size of a class F of functions is its Rademacher complexity:

(cid:34)

m(cid:88)

i=1

(cid:35)

Rm[F] ≡

E

x1 ···  xm
σ1 ···  σm

1
m

sup
f∈F

σif(xi)

 

The variables σ1 ···   σm are iid Bernouli random variables that take on the value -1 or +1 with
equal probability and are independent of x1  . . .   xm.
The Rademacher complexity of

ˆFw can be bounded as

Deﬁne S ≡

follows.

(cid:8)α ∈ RK (cid:12)(cid:12) (cid:107)α(cid:107)∞ ≤ C

Rm[ ˆFw] = E

σ X

(cid:32) K(cid:88)

k=1

sup
α∈S

K

m

σi

(cid:9):
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
m(cid:88)
K(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)E
m(cid:88)
K(cid:88)

k=1

i=1

i=1

m

1
m2

σ

C
K

k=1

k=1

≤ E

σ X

= E

X

C
K

σiφ(xi; ωk)

φ2(xi; ωk) ≤ C
K

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ E

X

7

αkφ(xi; ωk)

σiφ(xi; ωk)

m(cid:88)

αk

1
m

i=1

(cid:33)2

σiφ(xi; ωk)

σ X

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = E
(cid:118)(cid:117)(cid:117)(cid:116)E
K(cid:88)
(cid:114) 1
K(cid:88)

k=1

σ

C
K

m

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) K(cid:88)
m(cid:88)

k=1

k=1

1
m

sup
α∈S

(cid:32)

√
≤ C/
m 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(21)

(22)

(23)

where the ﬁrst inequality follows by H¨older  the second by the concavity of square root  the third by
the fact that conditioned on ω  Eσ σiφ(xi; ω)σjφ(xj; ω) = 0 when i (cid:54)= j  and the fourth follows by
the boundedness of φ.
The following theorem is a summary of the results from [12]:
Theorem 2. Let F be a class of bounded functions so that supx |f(x)| ≤ C for all f ∈ F  and
suppose c(y  y(cid:48)) = c(yy(cid:48))  with c(yy(cid:48)) L-Lipschitz. Then with probability at least 1− δ with respect
to training samples {xi  yi}m drawn from a probabilisty distribution P on X × {−1  +1}  every
function in F satisﬁes

(cid:114) 1

2m

R[f] ≤ Remp[f] + 4LRm[F] +

2|c(0)|√

m

+ LC

log 1
δ .

(24)

References
[1] H. D. Block. The perceptron: a model for brain functioning. Review of modern physics 

34:123–135  January 1962.

[2] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural

Computation  9(7):1545–1588  1997.

[3] F. Moosmann  B. Triggs  and F. Jurie. Randomized clustering forests for building fast and
In Advances in Neural Information Processing Systems

discriminative visual vocabularies.
(NIPS)  2006.

[4] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

Neural Information Processing Systems (NIPS)  2007.

[5] W. Maass and H. Markram. On the computational power of circuits of spiking neurons. Journal

of Computer and System Sciences  69:593–616  December 2004.

[6] E. Osuna  R. Freund  and F. Girosi. Training support vector machines: an application to face

detection. In Computer Vision and Pattern Recognition (CVPR)  1997.

[7] R. E. Schapire. The boosting approach to machine learning: An overview. In D. D. Denison 
M. H. Hansen  C. Holmes  B. Mallick  and B. Yu  editors  Nonlinear Estimation and Classiﬁ-
cation. Springer  2003.

[8] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: a statistical view of

boosting. Technical report  Dept. of Statistics  Stanford University  1998.

[9] L. K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence
rates for projection pursuit regression and neural network training. The Annals of Statistics 
20(1):608–613  March 1992.

[10] R. Rifkin  G. Yeo  and T. Poggio. Regularized least squares classiﬁcation. Advances in Learn-
ing Theory: Methods  Model and Applications  NATO Science Series III: Computer and Sys-
tems Sciences  190  2003.

[11] A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.

IEEE Transactions on Information Theory  39:930–945  May 1993.

[12] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research (JMLR)  3:463–482  2002.

[13] F. Girosi. Approximation error bounds that use VC-bounds. In International Conference on

Neural Networks  pages 295–302  1995.

[14] G. Gnecco and M. Sanguineti. Approximation error bounds via Rademacher’s complexity.

Applied Mathematical Sciences  2(4):153–176  2008.

8

,Nils Kriege
Pierre-Louis Giscard
Richard Wilson