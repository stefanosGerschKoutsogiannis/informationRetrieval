2013,Efficient Online Inference for Bayesian Nonparametric Relational Models,Stochastic block models characterize observed network relationships via latent community memberships. In large social networks  we expect entities to participate in multiple communities  and the number of communities to grow with the network size. We introduce a new model for these phenomena  the hierarchical Dirichlet process relational model  which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning  we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks  we also propose an efficient structured mean field variational bound  and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models  we show significantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis  a large network of who-knows-who at the heights of business and government.,Efﬁcient Online Inference for Bayesian

Nonparametric Relational Models

Dae Il Kim1  Prem Gopalan2  David M. Blei2  and Erik B. Sudderth1

1Department of Computer Science  Brown University  {daeil sudderth}@cs.brown.edu

2Department of Computer Science  Princeton University  {pgopalan blei}@cs.princeton.edu

Abstract

Stochastic block models characterize observed network relationships via latent
community memberships. In large social networks  we expect entities to partic-
ipate in multiple communities  and the number of communities to grow with the
network size. We introduce a new model for these phenomena  the hierarchical
Dirichlet process relational model  which allows nodes to have mixed membership
in an unbounded set of communities. To allow scalable learning  we derive an on-
line stochastic variational inference algorithm. Focusing on assortative models of
undirected networks  we also propose an efﬁcient structured mean ﬁeld variational
bound  and online methods for automatically pruning unused communities. Com-
pared to state-of-the-art online learning methods for parametric relational models 
we show signiﬁcantly improved perplexity and link prediction accuracy for sparse
networks with tens of thousands of nodes. We also showcase an analysis of Little-
Sis  a large network of who-knows-who at the heights of business and government.

1

Introduction

A wide range of statistical models have been proposed for the discovery of hidden communities
within observed networks. The simplest stochastic block models [20] create communities by clus-
tering nodes  aiming to identify demographic similarities in social networks  or proteins with related
functional interactions. The mixed-membership stochastic blockmodel (MMSB) [1] allows nodes
to be members of multiple communities; this generalization substantially improves predictive accu-
racy in real-world networks. These models are practically limited by the need to externally specify
the number of latent communities. We propose a novel hierarchical Dirichlet process relational
(HDPR) model  which allows mixed membership in an unbounded collection of latent communities.
By adapting the HDP [18]  we allow data-driven inference of the number of communities underlying
a given network  and growth in the community structure as additional nodes are observed.
The inﬁnite relational model (IRM) [10] previously adapted the Dirichlet process to deﬁne a non-
parametric relational model  but restrictively associates each node with only one community. The
more ﬂexible nonparametric latent feature model (NLFM) [14] uses an Indian buffet process
(IBP) [7] to associate nodes with a subset of latent communities. The inﬁnite multiple member-
ship relational model (IMRM) [15] also uses an IBP to allow multiple memberships  but uses a
non-conjugate observation model to allow more scalable inference for sparse networks. The non-
parametric metadata dependent relational (NMDR) model [11] employs a logistic stick-breaking
prior on the node-speciﬁc community frequencies  and thereby models relationships between com-
munities and metadata. All of these previous nonparametric relational models employed MCMC
learning algorithms. In contrast  the conditionally conjugate structure of our HDPR model allows us
to easily develop a stochastic variational inference algorithm [17  2  9]. Its online structure  which
incrementally updates global community parameters based on random subsets of the full graph  is
highly scalable; our experiments consider social networks with tens of thousands of nodes.

1

While the HDPR is more broadly applicable  our focus in this paper is on assortative models for
undirected networks  which assume that the probability of linking distinct communities is small.
This modeling choice is appropriate for the clustered relationships found in friendship and collab-
oration networks. Our work builds on stochastic variational inference methods developed for the
assortative MMSB (aMMSB) [6]  but makes three key technical innovations. First  adapting work
on HDP topic models [19]  we develop a nested family of variational bounds which assign posi-
tive probability to dynamically varying subsets of the unbounded collection of global communities.
Second  we use these nested bounds to dynamically prune unused communities  improving compu-
tational speed  predictive accuracy  and model interpretability. Finally  we derive a structured mean
ﬁeld variational bound which models dependence among the pair of community assignments associ-
ated with each edge. Crucially  this avoids the expensive and inaccurate local optimizations required
by naive mean ﬁeld approximations [1  6]  while maintaining computation and storage requirements
that scale linearly (rather than quadratically) with the number of hypothesized communities.
In this paper  we use our assortative HDPR (aHDPR) model to recover latent communities in so-
cial networks previously examined with the aMMSB [6]  and demonstrate substantially improved
perplexity scores and link prediction accuracy. We also use our learned community structure to
visualize business and governmental relationships extracted from the LittleSis database [13].

2 Assortative Hierarchical Dirichlet Process Relational Models

We introduce the assortative HDP relational (aHDPR) model  a nonparametric generalization of the
aMMSB for discovering shared memberships in an unbounded collection of latent communities.
We focus on undirected binary graphs with N nodes and E = N (N − 1)/2 possible edges  and let
yij = yji = 1 if there is an edge between nodes i and j. For some experiments  we assume the yij
variables are only partially observed to compare the predictive performance of different models.
As summarized in the graphical models of Fig. 1  we begin by deﬁning a global Dirichlet process to
capture the parameters associated with each community. Letting βk denote the expected frequency
of community k  and γ > 0 the concentration  we deﬁne a stick-breaking representation of β:

βk = vk

(1 − v(cid:96)) 

vk ∼ Beta(1  γ) 

k = 1  2  . . .

(1)

k−1(cid:89)(cid:96)=1

Adapting a two-layer hierarchical DP [18]  the mixed community memberships for each node i are
then drawn from DP with base measure β  πi ∼ DP(αβ). Here  E[πi | α  β] = β  and small
precisions α encourage nodes to place most of their mass on a sparse subset of communities.
To generate a possible edge yij between nodes i and j  we ﬁrst sample a pair of indicator variables
from their corresponding community membership distributions  sij ∼ Cat(πi)  rij ∼ Cat(πj). We
then determine edge presence as follows:

p(yij = 1 | sij = rij = k) = wk 

(2)
For our assortative aHDPR model  each community has its own self-connection probability
wk ∼ Beta(τa  τb). To capture the sparsity of real networks  we ﬁx a very small probability of
between-community connection   = 10−30. Our HDPR model could easily be generalized to more
ﬂexible likelihoods in which each pair of communities k  (cid:96) have their own interaction probability [1] 
but motivated by work on the aMMSB [6]  we do not pursue this generalization here.

p(yij = 1 | sij (cid:54)= rij) = .

3 Scalable Variational Inference

Previous applications of the MMSB associate a pair of community assignments  sij and rij  with
each potential edge yij. In assortative models these variables are strongly dependent  since present
edges only have non-negligible probability for consistent community assignments. To improve ac-
curacy and reduce local optima  we thus develop a structured variational method based on joint
conﬁgurations of these assignment pairs  which we denote by eij = (sij  rij). See Figure 1.
Given this alternative representation  we aim to approximate the joint distribution of the observed
edges y  local community assignments e  and global community parameters π  w  β given ﬁxed

2

Figure 1: Alternative graphical representations of the aHDPR model  in which each of N nodes has mixed
membership πi in an unbounded set of latent communities  wk are the community self-connection probabilities 
and yij indicates whether an edge is observed between nodes i and j. Left: Conventional representation 
in which source sij and receiver rij community assignments are independently sampled. Right: Blocked
representation in which eij = (sij  rij) denotes the pair of community assignments underlying yij.

hyperparameters τ  α  γ. Mean ﬁeld variational methods minimize the KL divergence between a
family of approximating distributions q(e  π  w  β) and the true posterior  or equivalently maximize
the following evidence lower bound (ELBO) on the marginal likelihood of the observed edges y:

L(q) (cid:44) Eq[log p(y  e  π  w  β | τ  α  γ)] − Eq[log q(e  π  w  β)].

(3)
For the nonparametric aHDPR model  the number of latent community parameters wk  βk  and the
dimensions of the community membership distributions πi  are both inﬁnite. Care must thus be
taken to deﬁne a tractably factorized  and ﬁnitely parameterized  variational bound.

3.1 Variational Bounds via Nested Truncations
We begin by deﬁning categorical edge assignment distributions q(eij | φij) = Cat(eij | φij) 
where φijk(cid:96) = q(eij = (k  (cid:96))) = q(sij = k  rij = (cid:96)). For some truncation level K  which will be
dynamically varied by our inference algorithms  we constrain φijk(cid:96) = 0 if k > K or (cid:96) > K.
Given this restriction  all observed interactions are explained by one of the ﬁrst (and under the
stick-breaking prior  most probable) K communities. The resulting variational distribution has K 2
parameters. This truncation approach extends prior work for HDP topic models [19  5].
For the global community parameters  we deﬁne an untruncated factorized variational distribution:

q(β  w | v∗  λ) =

k (vk)Beta(wk | λka  λkb) 
δv∗

βk(v∗) = v∗k

(1 − v∗(cid:96) ).

(4)

k−1(cid:89)(cid:96)=1

∞(cid:89)k=1

Our later derivations show that for communities k > K above the truncation level  the optimal
variational parameters equal the prior: λka = τa  λkb = τb. These distributions thus need not be
explicitly represented. Similarly  the objective only depends on v∗k for k ≤ K  deﬁning K + 1
probabilities: the frequencies of the ﬁrst K communities  and the aggregate frequency of all others.
Matched to this  we associate a (K + 1)-dimensional community membership distribution πi to
each node  where the ﬁnal component contains the sum of all mass not assigned to the ﬁrst K.
Exploiting the fact that the Dirichlet process induces a Dirichlet distribution on any ﬁnite partition 
we let q(πi | θi) = Dir(πi | θi)  θi ∈ RK+1. The overall variational objective is then

Eq[log p(wk | τa  τb)] − Eq[log q(wk | λka  λkb)] + Eq[log p(v∗k | γ)]
Eq[log p(πi | α  β(v∗))] − Eq[log q(πi | θi)]
Eq[log p(yij|w  eij)] + Eq[log p(eij|πi  πj)] − Eq[log q(eij|φij)].

(5)

Unlike truncations of the global stick-breaking process [4]  our variational bounds are nested  so that
lower-order approximations are special cases of higher-order ones with some zeroed parameters.

L(q) =(cid:80)k
+(cid:80)i
+(cid:80)ij

3.2 Structured Variational Inference with Linear Time and Storage Complexity

Conventional  coordinate ascent variational inference algorithms iteratively optimize each parameter
given ﬁxed values for all others. Community membership and interaction parameters are updated as
(6)

k=1 φijkk(1 − yij) 

k=1 φijkkyij 

λka = τa +(cid:80)E

ij(cid:80)K

λkb = τb +(cid:80)E

ij(cid:80)K

(cid:96)=1 φijk(cid:96).

θik = αβk +(cid:80)(i j)∈E(cid:80)K

(7)

3

∞∞⌧yijsijrijk⇡i↵ENwk∞∞⌧yijk⇡i↵ENeijwkHere  the ﬁnal summation is over all potential edges (i  j) linked to node i. Updates for assignment
distributions depend on expectations of log community assignment probabilities:

Eq[log(wk)] = ψ(λka) − ψ(λka + λkb) 

Eq[log(1 − wk)] = ψ(λkb) − ψ(λka + λkb) 

˜πik (cid:44) exp{Eq[log(πik)]} = exp{ψ(θik) − ψ((cid:80)K+1

(cid:96)=1 θi(cid:96))} 

Given these sufﬁcient statistics  the assignment distributions can be updated as follows:

˜πi (cid:44)(cid:80)K

k=1 ˜πik.

(8)
(9)

φijkk ∝ ˜πik ˜πjkf (wk  yij) 
φijk(cid:96) ∝ ˜πik ˜πj(cid:96)f (  yij) 

(10)
(11)
Here  f (wk  yij) = exp{yijEq[log(wk)] + (1− yij)Eq[log(1− wk)]}. More detailed derivations of
related updates have been developed for the MMSB [1].
A naive implementation of these updates would require O(K 2) computation and storage for each
assignment distribution q(eij | φij). Note  however  that the updates for q(wk | λk) in Eq. (6)
depend only on the K probabilities φijkk that nodes select the same community. Using the updates
for φijk(cid:96) from Eq. (11)  the update of q(πi | θi) in Eq. (7) can be expanded as follows:

(cid:96) (cid:54)= k.

θik = αβk +(cid:80)(i j)∈E φijkk + 1
= αβk +(cid:80)(i j)∈E φijkk + 1

Zij(cid:80)(cid:96)(cid:54)=k ˜πik ˜πj(cid:96)f (  yij)

˜πikf (  yij)(˜πj − ˜πjk).

Zij

Note that ˜πj need only be computed once  in O(K) operations. The normalization constant Zij 
which is deﬁned so that φij is a valid categorical distribution  can also be computed in linear time:

(12)

(13)

Zij = ˜πi ˜πjf (  yij) +(cid:80)K

k=1 ˜πik ˜πjk(f (wk  yij) − f (  yij)).

Finally  to evaluate our variational bound and assess algorithm convergence  we still need to calculate
the likelihood and entropy terms dependent on φijk(cid:96). However  we can compute part of our bound
by caching our partition function Zij in linear time. See ‡A.2 for details regarding the full derivation
of this ELBO and its extensions.

3.3 Stochastic Variational Inference

Standard variational batch updates become computationally intractable when N becomes very large.
Recent advancements in applying stochastic optimization techniques within variational inference [8]
showed that if our variational mean-ﬁeld family of distributions are members of the exponential
family  we can derive a simple stochastic natural gradient update for our global parameters λ  θ  v.
These gradients can be calculated from only a subset of the data and are noisy approximations of the
true natural gradient for the variational objective  but represent an unbiased estimate of that gradient.
To accomplish this  we deﬁne a new variational objective with respect to our current set of obser-
vations. This function  in expectation  is equivalent to our true ELBO. By taking natural gradients
with respect to our new variational objective for our global variables λ  θ  we have

(14)

(cid:96)=1 φijk(cid:96) + αβk − θik 

g(i j) φijkkyij + τa − λka;
g(i j)(cid:80)(i j)∈E(cid:80)K

∇λ∗ka = 1
∇θ∗ik = 1
(15)
where the natural gradient for ∇λ∗kb is symmetric to ∇λ∗ka and where yij in Eq. (14) is replaced by
(1 − yij). Note that(cid:80)(i j)∈E(cid:80)K
(cid:96)=1 φijk(cid:96) was shown in the previous section to be computable in
O(K). The scaling term g(i  j) is needed for an unbiased update to our expectation. If g(i  j) =
2/N (N − 1)  then this would represent a uniform distribution over possible edge selections in our
undirected graphs. In general  g(i  j) can be an arbitrary distribution over possible edge selections
such as a distribution over sets of edges as long as the expectation with respect to this distribution is
equivalent to the original ELBO [6]. When referring to the scaling constant associated with sets  we
consider the notation of h(T ) instead of g(i  j).
We optimize this ELBO with a Robbins-Monro algorithm which iteratively steps along the direction
of this noisy gradient. We specify a learning rate ρt (cid:44) (µ0 + t)−κ at time t where κ ∈ (.5  1] and
t < ∞ and

µ0 ≥ 0 downweights the inﬂuence of earlier updates. With the requirement that(cid:80)t ρ2

4

(cid:80)t ρt = ∞  we will provably converge to a local optimum. For our global variational parameters

{λ  θ}  the updates at iteration t are now

ka + ρt(∇λ∗ka) = (1 − ρt)λt−1
ik + ρt(∇θ∗ik) = (1 − ρt)θt−1

ka = λt−1
λt
ik = θt−1
θt
k = (1 − ρt)vt−1
vt

ik + ρt(

k + ρt(v∗k) 

(17)
(18)
where v∗k is obtained via a constrained optimization task using the gradients derived in ‡A.3. Deﬁn-
ing an update on our global parameters given a single edge observation can result in very poor local
optima. In practice  we specify a mini-batch T   a set of unique observations in determining a noisy
gradient that is more informative. This results in a simple summation over the sufﬁcient statistics
associated with the set of observations as well as a change to g(i  j) to reﬂect the necessary scaling
of our gradients when we can no longer assume our samples are uniformly chosen from our dataset.

(cid:96)=1 φijk(cid:96) + αβk);

ka + ρt(

1

g(i j) φijkkyij + τa);
1

g(i j)(cid:80)(i j)∈E(cid:80)K

(16)

3.4 Restricted Stratiﬁed Node Sampling

Stochastic variational inference provides us with the ability to choose a sampling scheme that al-
lows us to better exploit the sparsity of real world networks. Given the success of stratiﬁed node
sampling [6]  we consider this technique for all our experiments. Brieﬂy  stratiﬁed node-sampling
randomly selects a single node i and either chooses its associated links or a set of edges from
m equally sized non-link edge sets. For this mini-batch strategy  h(T ) = 1/N for link sets and
h(T ) = 1/N m for a partitioned non-link set. In [6]  all nodes in π were considered global param-
eters and updated after each mini-batch. For our model  we also treat π similarly  but maintain a
separate learning rate ρi for each node. This allows us to focus on updating only nodes that are rele-
vant to our mini-batch as well as limit the computational costs associated with this global update. To
ensure that our Robbins-Monro conditions are still satisﬁed  we set the learning rate for nodes that
are not part of our mini-batch to be 0. When a new minibatch contains this particular node  we look
to the most previous learning rate and assume this value as the previous learning rate. This modiﬁed
it < ∞ and that
subsequence of learning rates maintains our convergence criterion so that the(cid:80)t ρ2
(cid:80)t ρit = ∞. We show how performing this simple modiﬁcation results in signiﬁcant improvements

in both perplexity and link prediction scores.

3.5 Pruning Moves

Our nested truncation requires setting an initial number of communities K. A large truncation lets
the posterior ﬁnd the best number of communities  but can be computationally costly. A truncation
set too small may not be expressive enough to capture the best approximate posterior. To remedy
this  we deﬁne a set of pruning moves aimed at improving inference by removing communities that
have very small posterior mass. Pruning moves provide the model with a more parsimonious and
interpretable latent structure  and may also signiﬁcantly reduce the computational cost of subsequent
iterations. Figure 2 provides an example illustrating how pruning occurs in our model.
To determine communities which are good candidates for pruning  for each community k we ﬁrst
k=1 θik). Any community for which Θk < (log K)/N for
t∗ = N/2 consecutive iterations is then evaluated in more depth. We scale t∗ with the number of
nodes N within the graph to ensure that a broad set of observations are accounted for. To estimate
an approximate but still informative ELBO for the pruned model  we must associate a set of relevant
observations to each pruning candidate. In particular  we approximate the pruned ELBO L(qprune) by
considering observations yij among pairs of nodes with signiﬁcant mass in the pruned community.
We also calculate L(qold) from these same observations  but with the old model parameters. We
then compare these two values to accept or reject the pruning of the low-weight community.

compute Θk = ((cid:80)N

i=1 θik)/((cid:80)N

i=1(cid:80)K

4 Experiments

In this section we perform experiments that compare the performance of the aHDPR model to the
aMMSB. We show signiﬁcant gains in AUC and perplexity scores by using the restricted form of

5

We specify a new model by redistributing its mass (cid:80)N

a  λ∗

Figure 2: Pruning extraneous communities. Suppose that community k = 3 is considered for removal.
i=1 θi3 uniformly across the remaining communities
θi(cid:96)  (cid:96) (cid:54)= 3. An analogous operation is used to generate {v∗  β∗  λ∗
b   θ∗}. To accurately estimate the true
change in ELBO for this pruning  we select the n∗ = 10 nodes with greatest participation θi3 in community 3.
Let S denote the set of all pairs of these nodes  and yij∈S their observed relationships. From these observations
ij∈S for a model in which community k = 3 is pruned  and a corresponding ELBO L(qprune).
we can estimate φ∗
Using the data from the same sub-graph  but the old un-pruned model parameters  we estimate an alternative
ELBO L(qold). We accept if L(qprune) > L(qold)  and reject otherwise. Because our structured mean-ﬁeld
approach provides simple direct updates for φ∗

ij∈S  the calculation of L(qold) and L(qprune) is efﬁcient.

stratiﬁed node sampling  a quick K-means initialization1 for θ  and our efﬁcient structured mean-
ﬁeld approach combined with pruning moves. We perform a detailed comparison on a synthetic toy
dataset  as well as the real-world relativity collaboration network  using a variety of metrics to show
the beneﬁts of each contribution. We then show signiﬁcant improvements over the baseline aMMSB
model in both AUC and perplexity metrics on several real-world datasets previously analyzed by [6].
Finally  we perform a qualitative analysis on the LittleSis network and demonstrate the usefulness
of using our learned latent community structure to create visualizations of large networks. For
additional details on the parameters used in these experiments  please see ‡A.1.

4.1 Synthetic and Collaboration Networks

The synthetic network we use for testing is generated from the standards and software outlined
in [12] to produce realistic synthetic networks with overlapping communities and power-law degree
distributions. For these purposes  we set the number of nodes N = 1000  with the minimum degree
per node set to 10 and its maximum to 60. On this network the true number of latent communities
was found to be K = 56. Our real world networks include 5 undirected networks originally ranging
from N = 5  242 to N = 27  770. These raw networks  however  contain several disconnected com-
ponents. Both the aMMSB and aHDPR achieve highest posterior probability by assigning each con-
nected component distinct  non-overlapping communities; effectively  they analyze each connected
sub-graph independently. To focus on the more challenging problem of identifying overlapping
community structure  we take the largest connected component of each graph for analysis.
Initialization and Node-Speciﬁc Learning Rates. The upper-left panels in Fig. 3 compare differ-
ent aHDPR inference algorithms  and the perplexity scores achieved on various networks. Here we
demonstrate the beneﬁts of initializing θ via K-means  and our restricted stratiﬁed node sampling
procedure. For our random initializations  we initalized θ in the same fashion as the aMMSB. Using
a combination of both modiﬁcations  we achieve the best perplexity scores on these datasets. The
node-speciﬁc learning rates intuitively restrict updates for θ to batches containing relevant observa-
tions  while our K-means initialization quickly provides a reasonable single-membership partition
as a starting point for inference.
Naive Mean-Field vs. Structured Mean-Field. The naive mean-ﬁeld approach is the aHDPR
model where the community indicator assignments are split into sij and rij. This can result in
severe local optima due to their coupling as seen in some experiments in Fig. 4. The aMMSB in some

1Our K-means initialization views the rows of the adjacency matrix as distinct data points and produces a
single community assignment zi for each node. To initialize community membership distributions based on
these assignments  we set θizi = N − 1 and θi\zi = α.

6

Adjacency Matrix ✓K communities N nodes New Model ⇥k=(PNi=1✓ik)/(PNi=1PKk=1✓ik)✓⇤Prune k=3 Uniformly redistribute mass. Perform similar operation for other latent variables. ⇥k<(logK)/NSelect nodes relevant to pruned topic and its corresponding sub-graph (red box) to generate a new ELBO: L(qprune) If L(qprune) > L(qold)  accept or else reject and continue inference with old model YL(qold) v⇤ ⇤ ⇤✓⇤ ⇤ij2SL(qprune) v  ✓ ij2SFigure 3: The upper left shows beneﬁts of a restricted update and a K-means initialization for stratiﬁed node
sampling on both synthetic and relativity networks. The upper right shows the sensitivity of the aMMSB as
K varies versus the aHDPR. The lower left shows various perplexity scores for the synthetic and relativity
networks with the best performing model (aHDPR-Pruning) scoring an average AUC of 0.9675± .0017 on the
synthetic network and 0.9466± .0062 on the relativity network. The lower right shows the pruning process for
the toy data and the ﬁnal K communities discovered on our real-world networks.

instances performs better than the naive mean-ﬁeld approach  but this can be due to differences in our
initialization procedures. However  by changing our inference procedure to an efﬁcient structured
mean-ﬁeld approach  we see signiﬁcant improvements across all datasets.
Beneﬁts of Pruning Moves. Pruning moves were applied every N/2 iterations with a maximum
of K/10 communities removed per move.
If the number of prune candidates was greater than
K/10  then K/10 communities with the lowest mass were chosen. The lower right portion of Fig. 3
shows that our pruning moves can learn close to the true underlying number of clusters (K=56) on a
synthetic network even when signiﬁcantly altering its initial K. Across several real world networks 
there was low variance between runs with respect to the ﬁnal K communities discovered  suggesting
a degree of robustness. Furthermore  pruning moves improved perplexity and AUC scores across
every dataset as well as reducing computational costs during inference.

Figure 4: Analysis of four real-world collaboration networks. The ﬁgures above show that the aHDPR with
pruning moves has the best performance  in terms of both perplexity (top) and AUC (bottom) scores.

4.2 The LittleSis Network

The LittleSis network was extracted from the website (http://littlesis.org)  which is an organization
that acts as a watchdog network to connect the dots between the world’s most powerful people

7

246810121416x 1063456789101112Mini−Batch Strategies TOY  K=56 (aHDPR−Fixed)Number of Observed EdgesPerplexity Random Init−AllRandom Init−RestrictedKmeans Init−AllKmeans Init−Restricted12345x 107510152025303540Mini−Batch Strategies Relativity  K=250 (aHDPR−Fixed)Number of Observed EdgesPerplexity Random Init−AllRandom Init−RestrictedKmeans Init−AllKmeans Init−Restricted20405660801002.533.544.555.566.57Average Perplexity vs. K (Toy)PerplexityNumber of Communities K (aMMSB) aMMSBaHDPR−K100aHDPR−Pruning−K100aHDPR−Pruning−K2001502002503003504002468101214161820Average Perplexity vs. K (Relativity)PerplexityNumber of Communities K (aMMSB) aMMSBaHDPR−K500aHDPR−Pruning246810121416x 1062345678910Perplexity TOY N=1000Number of Observed EdgesPerplexity aMMSB−K20aMMSB−K56aHDPR−Naive−K56aHDPR−Batch−K56aHDPR−K56aHDPR−Pruning−K200aHDPR−Truth12345x 10751015202530Perplexity Relativity N=4158Number of Observed EdgesPerplexity aMMSB−K150aMMSB−K200aHDPR−Naive−K500aHDPR−K500aHDPR−Pruning105106107020406080100120140160180200Pruning process for Toy DataNumber of Observed Edges (Log Axis)Number of Communities Init K=100Init K=200True K=56relhep2hepastrocm250300350400Number of Communities UsedK after Pruning (aHDPR Initial K=500)123456789x 10710152025303540455055Perplexity Hep2 N=7464Number of Observed EdgesPerplexity aMMSB−K150aMMSB−K200aHDPR−Naive−K500aHDPR−K500aHDPR−Pruning2468101214x 10751015202530Perplexity Hep N=11204Number of Observed EdgesPerplexity aMMSB−K250aMMSB−K300aHDPR−Naive−K500aHDPR−K500aHDPR−Pruning0.511.52x 108101520253035Perplexity AstroPhysics N=17903Number of Observed EdgesPerplexity aMMSB−K300aMMSB−K350aHDPR−Naive−K500aHDPR−K500aHDPR−Pruning0.511.522.5x 10810203040506070Perplexity Condensed Matter N=21363Number of Observed EdgesPerplexity aMMSB−K400aMMSB−K450aHDPR−Naive−K500aHDPR−K500aHDPR−Pruning0.60.650.70.750.80.850.90.951AUC Hep2 N=7464AUC QuantilesaMMSBK200aMMSBK250aHDPRNaive−K500aHDPRK500aHDPRPruning0.60.650.70.750.80.850.90.951AUC Hep N=11204AUC QuantilesaMMSBK250aMMSBK300aHDPRNaive−K500aHDPRK500aHDPRPruning0.60.650.70.750.80.850.90.951AUC AstroPhysics N=17903AUC QuantilesaMMSBK300aMMSBK350aHDPRNaive−K500aHDPRK500aHDPRPruning0.60.650.70.750.80.850.90.951AUC Condensed Matter N=21363AUC QuantilesaMMSBK400aMMSBK450aHDPRNaive−K500aHDPRK500aHDPRPruningFigure 5: The LittleSis Network. Near the center in violet we have prominent government ﬁgures such as
Larry H. Summers (71st US Treasury Secretary) and Robert E. Rubin (70th US Treasury Secretary) with ties
to several distinct communities  representative of their high posterior bridgness. Conversely  within the beige
colored community  individuals with small posterior bridgness such as Wendy Neu can reﬂect a career that was
highly focused in one organization. A quick internet search shows that she is currently the CEO of Hugo Neu 
a green-technology ﬁrm where she has worked for over 30 years. An analysis on this type of network might
provide insights into the structures of power that shape our world and the key individuals that deﬁne them.

and organizations. Our ﬁnal graph contained 18 831 nodes and 626 881 edges  which represents
a relatively sparse graph with edge density of 0.35% (for details on how this dataset was pro-
cessed see ‡A.3). For this analysis  we ran the aHDPR with pruning on the entire dataset using
the same settings from our previous experiments. We then took the top 200 degree nodes and gen-
erated weighted edges based off of a variational distance between their learned expected variational
posteriors such that dij = 1 − |Eq [πi]−Eq [πj ]|
. This weighted edge was then included in our visual-
ization software [3] if dij > 0.5. Node sizes were determined by posterior bridgness [16] where
k=1(Eq[πik] − 1
K )2and measures the extent to which a node is involved
with multiple communities. Larger nodes have greater posterior bridgeness while node colors rep-
resent its dominant community membership. Our learned latent communities can drive these types
of visualizations that otherwise might not have been possible given the raw subgraph (see ‡A.4).

bi = 1 −(cid:112)K/(K − 1)(cid:80)K

2

5 Discussion

Our model represents the ﬁrst Bayesian nonparametric relational model to use a stochastic varia-
tional approach for efﬁcient inference. Our pruning moves allow us to save computation and im-
prove inference in a principled manner while our efﬁcient structured mean-ﬁeld inference procedure
helps us escape local optima. Future extensions of interest could entail advanced split-merge moves
that can grow the number of communities as well as extending these scalable inference algorithms
to more sophisticated relational models.

8

Zalmay_KhalilzadRussell_L_CarsonTerry_J_LundgrenLeon_D_BlackWilliam_DonaldsonRonald_L_OlsonWilliam_C_RudinJerry_SpeyerPaul_H_O_NeillRoger_C_AltmanNeal_S_WolinRalph_L_SchlossteinVikram_S_PanditThomas_R_NidesWilliam_M_Lewis_JrRobert_H_HerzSuresh_SundaresanRebecca_M_BlankWilliam_A_HaseltineRobert_D_ReischauerStephen_M_WolfMaya_MacGuineasTom_ClausenVernon_E_Jordan_JrRichard_N_HaassRichard_D_ParsonsLarry_SummersTodd_SternPeter_J_WallisonThomas_E_DonilonPeter_OrszagNancy_KilleferScooter_LibbyTrenton_ArthurStephanie_CutterRobert_BoorstinThomas_F_McLarty_IIITony_FrattoRon_BloomRichard_L__Jake__SiewertStanton_AndersonGlenn_H_HutchinsThurgood_Marshall_JrRoger_B_PorterRobert_GatesMark_B_McClellanRobert_E_RubinStephen_FriedmanPeter_G_PetersonShirley_Ann_JacksonKenneth_M_DubersteinSylvia_Mathews_BurwellLaura_D_TysonTim_GeithnerRichard_C_HolbrookeMartin_S_FeldsteinWilliam_McDonoughWarren_Bruce_RudmanPaul_VolckerJohn_C_WhiteheadMaurice_R_GreenbergStrobe_TalbottWilliam_S_CohenStephen_W_BosworthThomas_R_PickeringReynold_LevyMichael_H_MoskowThomas_S_FoleyVincent_A_MaiWarren_ChristopherVin_WeberRichard_N_CooperMadeleine_K_AlbrightMichael_FromanSusan_RicePeter_J_SolomonValerie_JarrettRobert_WolfReed_E_HundtRichard_J_DanzigPenny_PritzkerWilliam_E_KennardWendy_AbramsRobert_BauerRobert_Lane_GibbsTom_BernsteinRon_MoelisMark_T_GalloglySusan_MandelTony_WestMichael_StrautmanisLouis_B_SusmanNorm_EisenPaul_J_TaubmanWendy_NeuMichael_LyntonSteven_GlucksternWilliam_Von_HoeneJason_FurmanMelissa_E_WinterWahid_HamidPaul_Tudor_Jones_IIPaula_H_CrownRonald_KirkRobert_M_PerkowitzRobert_ZoellickThomas_SteyerWillem_BuiterRichard_KauffmanSuzanne_Nora_JohnsonRobert_K_SteelThomas_K_MontagWilliam_WickerRobert_D_HormatsTimothy_M_GeorgePeter_BassWilliam_C_DudleyThomas_J_HealeyTracy_R_WolstencroftRajat_K_GuptaMeredith_BroomePeter_K_ScaturroRobert_S_KaplanReuben_Jeffery_IIIRobin_Jermyn_BrooksWilliam_P_BoardmanJohn_L_ThorntonSarah_G_SmithSteven_T_MnuchinSharmin_Mossavar_RahmaniRobert_B_MenschelRobert_J_HurstWilliam_W_GeorgeRon_Di_RussoLloyd_C_BlankfeinWilliam_M_YarbenetThomas_L_Kempner_JrKenneth_M_JacobsJohn_A_ThainJosh_BoltenRichard_C_PerryThomas_E_TuftMarti_ThomasJudd_Alan_GreggWalter_W_Driver_JrNeel_KashkariMark_PattersonNick_O_DonohoeMario_DraghiPete_ConewayScott_KapnickReferences
[1] E. Airoldi  D. Blei  S. Fienberg  and E. Xing. Mixed membership stochastic blockmodels. JMLR  9  2008.
[2] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation  10(2):251–276  1998.
[3] M. Bastian  S. Heymann  and M. Jacomy. Gephi: An open source software for exploring and manipulating

networks  2009.

[4] D. M. Blei and M. I. Jordan. Variational methods for the dirichlet process. In ICML  2004.
[5] M. Bryant and E. B. Sudderth. Truly nonparametric online variational inference for hierarchical dirichlet

[6] P. Gopalan  D. M. Mimno  S. Gerrish  M. J. Freedman  and D. M. Blei. Scalable inference of overlapping

processes. In NIPS  pages 2708–2716  2012.

communities. In NIPS  pages 2258–2266  2012.

[7] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. Technical

Report 2005-001  Gatsby Computational Neuroscience Unit  May 2005.

[8] M. Hoffman  D. Blei  C. Wang  and J. Paisley. Stochastic variational inference.

arXiv preprint

[9] M. D. Hoffman  D. M. Blei  and F. R. Bach. Online learning for latent dirichlet allocation. In NIPS  pages

arXiv:1206.7051  2012.

856–864  2010.

[10] C. Kemp  J. Tenenbaum  T. Grifﬁths  T. Yamada  and N. Ueda. Learning systems of concepts with an

inﬁnite relational model. In AAAI  2006.

[11] D. Kim  M. C. Hughes  and E. B. Sudderth. The nonparametric metadata dependent relational model. In

[12] A. Lancichinetti and S. Fortunato. Benchmarks for testing community detection algorithms on directed

and weighted graphs with overlapping communities. Phys. Rev. E  80(1):016118  July 2009.

[13] littlesis.org. Littlesis is a free database detailing the connections between powerful people and organiza-

[14] K. Miller  T. Grifﬁths  and M. Jordan. Nonparametric latent feature models for link prediction. In NIPS 

ICML  2012.

tions  June 2009.

2009.

[15] M. Morup  M. N. Schmidt  and L. K. Hansen. Inﬁnite multiple membership relational modeling for com-
plex networks. In Machine Learning for Signal Processing (MLSP)  2011 IEEE International Workshop
on  pages 1–6. IEEE  2011.

[16] T. Nepusz  A. Petrczi  L. Ngyessy  and F. Bazs. Fuzzy communities and the concept of bridgeness in

complex networks. Phys Rev E Stat Nonlin Soft Matter Phys  77(1 Pt 2):016107  2008.

[17] M. Sato. Online model selection based on the variational bayes. Neural Computation  13(7):1649–1681 

[18] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes.

JASA 

2001.

101(476):1566–1581  Dec. 2006.

[19] Y. W. Teh  K. Kurihara  and M. Welling. Collapsed variational inference for hdp. In NIPS  2007.
[20] Y. Wang and G. Wong. Stochastic blockmodels for directed graphs. JASA  82(397):8–19  1987.

9

,Dae Il Kim
Prem Gopalan
David Blei
Erik Sudderth
Aryeh Kontorovich
Sivan Sabato
Ruth Urner
Di Wang
Marco Gaboardi
Jinhui Xu