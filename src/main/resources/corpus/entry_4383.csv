2008,Predictive Indexing for Fast Search,We tackle the computational problem of query-conditioned search. Given a machine-learned scoring rule and a query distribution  we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries. The predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements. The general approach is applicable to webpage ranking  internet advertisement  and approximate nearest neighbor search. It is particularly effective in settings where standard techniques (e.g.  inverted indices) are intractable. We experimentally find substantial improvement over existing methods for internet advertisement and approximate nearest neighbors.,Predictive Indexing for Fast Search

Sharad Goel
Yahoo! Research

New York  NY 10018

goel@yahoo-inc.com

John Langford
Yahoo! Research

New York  NY 10018
jl@yahoo-inc.com

Abstract

Alex Strehl

Yahoo! Research

New York  NY 10018

strehl@yahoo-inc.com

We tackle the computational problem of query-conditioned search. Given a
machine-learned scoring rule and a query distribution  we build a predictive in-
dex by precomputing lists of potential results sorted based on an expected score
of the result over future queries. The predictive index datastructure supports an
anytime algorithm for approximate retrieval of the top elements. The general ap-
proach is applicable to webpage ranking  internet advertisement  and approximate
nearest neighbor search. It is particularly effective in settings where standard tech-
niques (e.g.  inverted indices) are intractable. We experimentally ﬁnd substantial
improvement over existing methods for internet advertisement and approximate
nearest neighbors.

1 Introduction

The Problem. The objective of web search is to quickly return the set of most relevant web pages
given a particular query string. Accomplishing this task for a ﬁxed query involves both determining
the relevance of potential pages and then searching over the myriad set of all pages for the most
relevant ones. Here we consider only the second problem. More formally  let Q ⊆ Rn be an input
space  W ⊆ Rm a ﬁnite output space of size N  and f : Q × W (cid:55)→ R a known scoring function.
Given an input (search query) q ∈ Q  the goal is to ﬁnd  or closely approximate  the top-k output
objects (web pages) p1  . . .   pk in W (i.e.  the top k objects as ranked by f(q ·)).
The extreme speed constraint  often 100ms or less  and the large number of web pages (N ≈ 1010)
makes web search a computationally-challenging problem. Even with perfect 1000-way paralleliza-
tion on modern machines  there is far too little time to directly evaluate against every page when a
particular query is submitted. This observation limits the applicability of machine-learning methods
for building ranking functions. The question addressed here is: “Can we quickly return the highest
scoring pages as ranked by complex scoring rules typical of learning algorithms?”
Predictive Indexing. We describe a method for rapidly retrieving the top elements over a large set
as determined by general scoring functions. The standard method for mitigating the computational
difﬁculties of search is to pre-process the data so that far less computation is necessary at runtime.
Taking the empirical probability distribution of queries into account  we pre-compute collections of
web pages that have a large expected score conditioned on the query falling into particular sets of
related queries {Qi}. For example  we may pre-compute and store the list of web pages that have the
highest average score when the query contains the phrase “machine learning”. To yield a practical
algorithm  these sets should form meaningful groups of pages with respect to the scoring function
and query distribution. At runtime  we then optimize only over those collections of top-scoring web
pages for sets Qi containing the submitted query.
Our main contribution is optimizing the search index with respect to the query distribution. The em-
pirical evidence presented shows that predictive indexing is an effective technique  making general
machine learning style prediction methods viable for quickly ranking over large numbers of objects.

1

The general methodology applies to other optimization problems as well  including approximate
nearest neighbor search.
In the remainder of Section 1 we describe existing solutions to large-scale search  and their applica-
bility to general scoring functions. Section 2 describes the predictive indexing algorithm and covers
an example and lemma suggesting that predictive indexing has signiﬁcant advantages over existing
techniques. We present empirical evaluation of the method in Section 3  using both proprietary web
advertising data and public data for nearest neighbor search.

1.1 Feature Representation

One concrete way to map web search into the general predictive index framework is to represent
both queries and pages as sparse binary feature vectors in a high-dimensional Euclidean space.
Speciﬁcally  we associate each word with a coordinate: A query (page) has a value of 1 for that
coordinate if it contains the word  and a value of 0 otherwise. We call this the word-based feature
representation  because each query and page can be summarized by a list of its features (i.e.  words)
that it contains. The general predictive framework supports many other possible representations 
including those that incorporate the difference between words in the title and words in the body of
the web page  the number of times a word occurs  or the IP address of the user entering the query.

1.2 Related Work

problem for linear scoring functions of the form f(q  p) =(cid:80)n

Given the substantial importance of large-scale search  a variety of techniques have been developed
to address the rapid ranking problem. Past work that has referenced the query distribution includes
(Cheng et al.  2006; Chierichetti et al.  2008). Here we describe two commonly applied methods
related to the predictive index approach.
Fagin’s Threshold Algorithm. Fagin’s threshold algorithm (Fagin et al.  2003) supports the top-k
i=1 qigi(p)  where qi ∈ {0  1} is the
ith coordinate of the query q  and gi : W (cid:55)→ R are partial scores for pages as determined by the ith
feature1. For each query feature i  construct an ordered list Li containing every web page  sorted
in descending order by their partial scores gi(p). We refer to this as the projective order  since it
is attained by projecting the scoring rule onto individual coordinates. Given a query q  we evaluate
web pages in the lists Li that correspond to features of q. The algorithm maintains two statistics 
upper and lower bounds on the score of the top-kth page  halting when these bounds cross. The
lower bound is the score of the kth best page seen so far; the upper bound is the sum of the partial
scores (i.e.  gi(p)) for the next-to-be-scored page in each list. Since the lists are ordered by the
partial scores  the upper threshold does in fact bound the score of any page yet to be seen.
The threshold algorithm is particularly effective when a query contains a small number of features 
facilitating fast convergence of the upper bound. In our experiments  we ﬁnd that the halting con-
dition is rarely satisﬁed within the imposed computational restrictions. One can  of course  simply
halt the algorithm when it has expended the computational budget (Fagin  2002)  which we refer to
as the Halted Threshold Algorithm.
Inverted Indices. An inverted index is a datastructure that maps every page feature x to a list of
pages p that contain x. When a new query arrives  a subset of page features relevant to the query is
ﬁrst determined. For instance  when the query contains “dog”  the page feature set might be {“dog” 
“canine”  “collar”  ...}. Note that a distinction is made between query features and page features  and
in particular  the relevant page features may include many more words than the query itself. Once a
set of page features is determined  their respective lists (i.e.  inverted indices) are searched  and from
them the ﬁnal list of output pages is chosen. One method for searching over these lists is to execute
Fagin’s threshold algorithm. Other methods  such as the “Weighted-And” algorithm (Broder et al. 
2003)  use one global order for pages in the lists and walk down the lists synchronously to compute
page scores. See (Zobel & Moffat  2006) for an overview of inverted indices applied to web search.
Standard approaches based on inverted indices suffer from a shortcoming. The resulting algorithms
are efﬁcient only when it is sufﬁcient to search over a relatively small set of inverted indices for each

1More general monotone scoring functions (e.g.  coordinate-wise product and max) are in fact supported;

for clarity  however  we restrict to the linear case.

2

query. They require  for each query q  that there exists a small set2 Xq of page features such that the
score of any page against q depends only on its intersection with Xq. In other words  the scoring
rule must be extremely sparse  with most words or features in the page having zero contribution to
the score for q. In Section 3.1  we consider a machine-learned scoring rule  derived from internet
advertising data  with the property that almost all page features have substantial inﬂuence on the
score for every query  making any straightforward approach based on inverted indices intractable.
Furthermore  algorithms that use inverted indices do not typically optimize the datastructure against
the query distribution and our experiments suggest that doing so may be beneﬁcial.

2 An Algorithm for Rapid Approximate Ranking

Suppose we are provided with a categorization of possible queries into related  potentially overlap-
ping  sets. For example  these sets might be deﬁned as  “queries containing the word ‘France’ ”
or “queries with the phrase ‘car rental’.” For each query set  the associated predictive index is an
ordered list of web pages sorted by their expected score for random queries drawn from that set. In
particular  we expect web pages at the top of the ‘France’ list to be good  on average  for queries
containing the word ‘France.’ In contrast to an inverted index  the pages in the ‘France’ list need not
themselves contain the word ‘France’. To retrieve results for a particular query (e.g.  “France car
rental”)  we optimize only over web pages in the relevant  pre-computed lists. Note that the predic-
tive index is built on top of an already existing categorization of queries  a critical  and potentially
difﬁcult initial step. In the applications we consider  however  we ﬁnd that predictive indexing works
well even when applied to naively deﬁned query sets. Furthermore  in our application to approxi-
mate nearest neighbor search  we found predictive indexing to be robust to cover sets generated via
random projections whose size and shape were varied across experiments.
We represent queries and web pages as points in  respectively  Q ⊆ Rn and W ⊆ Rm. This setting
is general  but for the experimental application we consider n  m ≈ 106  with any given page or
query having about 102 non-zero entries (see Section 3.1 for details). Thus  pages and points are
typically sparse vectors in very high dimensional spaces. A coordinate may indicate  for example 
whether a particular word is present in the page/query  or more generally  the number of times that
word appears. Given a scoring function f : Q × W (cid:55)→ R  and a query q  we attempt to rapidly ﬁnd
the top-k pages p1  . . .   pk. Typically  we ﬁnd an approximate solution  a set of pages ˆp1  . . .   ˆpk
that are among the top l for l ≈ k. We assume queries are generated from a probability distribution
D that may be sampled.

2.1 Predictive Indexing for General Scoring Functions
Consider a ﬁnite collection Q of sets Qi ⊆ Q that cover the query space (i.e.  Q ⊆ ∪iQi). For each
Qi  deﬁne the conditional probability distribution Di over queries in Qi by Di(·) = D(·|Qi)  and
deﬁne fi : W (cid:55)→ R as fi(p) = Eq∼Di[f(q  p)]. The function fi(p) is the expected score of the
web page p for the (related) queries in Qi. The hope is that any page p has approximately the same
score for any query q ∈ Qi. If  for example  Qi is the set of queries that contain the word “dog”  we
may expect every query in Qi to score high against pages about dogs and to score low against those
pages not about dogs.
For each set of queries Qi we pre-compute a sorted list Li of pages pi1   pi2  . . .   piN ordered in
descending order of fi(p). At runtime  given a query q  we identify the query sets Qi containing
q  and compute the scoring function f only on the restricted set of pages at the beginning of their
associated lists Li. We search down these lists for as long as the computational budget allows.
In general  it is difﬁcult to compute exactly the conditional expected scores of pages fi(p). One can 
however  approximate these scores by sampling from the query distribution D. Algorithm 1 outlines
the construction of the sampling-based predictive indexing datastructure. Algorithm 2 shows how
the method operates at run time.
Note that in the special case where we cover Q with a single set  we end up with a global ordering
of web pages  independent of the query  which is optimized for the underlying query distribution.

2The size of these sets are typically on the order of 100 or smaller.

3

Algorithm 1 Construct-Predictive-Index(Cover Q  Dataset S)

Lj[s] = 0 for all objects s and query sets Qj.
for t random queries q ∼ D do

for all objects s in the data set do

for all query sets Qj containing q do

Lj[s] ← Lj[s] + f(q  s)

end for

end for

end for
for all lists Lj do

sort Lj

end for
return {L}

Algorithm 2 Find-Top(query q  count k)

i = 0
top-k list V = ∅
while time remains do

for each query set Qj containing q do

s ← Lj[i]
if f(q  s) > kth best seen so far then

insert s into ordered top-k list V

end if
end for
i ← i + 1

end while
return V

While this global ordering may not be effective in isolation  it could perhaps be used to order pages
in traditional inverted indices.

2.2 Discussion

We present an elementary example to help develop intuition for why we can sometimes expect
predictive indexing to improve upon projective datastructures such as those used in Fagin’s threshold
two query features t1 and t2; three possible queries q1 = {t1} 
algorithm. Suppose we have:
q2 = {t2} and q3 = {t1  t2}; and three web pages p1  p2 and p3. Further suppose we have a simple
linear scoring function deﬁned by

f(q  p1) = It1∈q − It2∈q

f(q  p2) = It2∈q − It1∈q

f(q  p3) = .5 · It2∈q + .5 · It1∈q

where I is the indicator function. That is  pi is the best match for query qi  but p3 does not score
highly for either query feature alone. Thus  an ordered  projective datastructure would have

t1 → {p1  p3  p2}

t2 → {p2  p3  p1}.

Suppose  however  that we typically only see query q3. In this case  if we know t1 is in the query 
we infer that t2 is likely to be in the query (and vice versa)  and construct the predictive index

t1 → {p3  p1  p2}

t2 → {p3  p2  p1}.

On the high probability event  namely query q3  we see the predictive index outperforms the projec-
tive  query independent  index.
We expect predictive indices to generally improve on datastructures that are agnostic to the query
distribution. In the simple case of a single cover set (i.e.  a global web page ordering) and when
we wish to optimize the probability of returning the highest-scoring object  Lemma 2.1 shows that
a predictive ordering is the best ordering relative to any particular query distribution.

4

(Hq ∈ {s1  ...  sk}) =

Pr
q∼D

max

permutations π

Pr
q∼D

(Hq ∈ {sπ(1)  ...  sπ(k)}).

Lemma 2.1. Suppose we have a set of points S  a query distribution D  and a function f that scores
queries against points in S. Further assume that for each query q  there is a unique highest scoring
point Hq. For s ∈ S  let h(s) = Prq∼D(s = Hq)  and let s1  s2  . . .   sN be ordered according to
h(s). For any ﬁxed k 

pearing in the top k entries equals(cid:80)k

Proof. For any ordering of points  sπ(1)  . . .   sπ(k)  the probability of the highest scoring point ap-
j=1 h(sπ(j)). This sum is clearly maximized by ordering the
list according to h(·).

3 Empirical Evaluation

We evaluate predictive indexing for two applications: Internet advertising and approximate nearest
neighbor.

3.1

Internet Advertising

form f(p  a) = (cid:80)

We present results on Internet advertising  a problem closely related to web search. We have ob-
tained proprietary data  both testing and training  from an online advertising company. The data are
comprised of logs of events  where each event represents a visit by a user to a particular web page
p  from a set of web pages Q ⊆ Rn. From a large set of advertisements W ⊆ Rm  the commercial
system chooses a smaller  ordered set of ads to display on the page (generally around 4). The set of
ads seen and clicked by users is logged. Note that the role played by web pages has switched  from
result to query. The total number of ads in the data set is |W| ≈ 6.5 × 105. Each ad contains  on
average  30 ad features  and a total of m ≈ 106 ad features are observed. The training data consist
of 5 million events (web page × ad displays). The total number of distinct web pages is 5 × 105.
Each page consists of approximately 50 page features  and a total of n ≈ 9× 105 total page features
are observed.
We used a sparse feature representation (see Section 1.1) and trained a linear scoring rule f of the
i j wi jpiaj  to approximately rank the ads by their probability of click. Here 
wi j are the learned weights (parameters) of the linear model. The search algorithms we compare
were given the scoring rule f  the training pages  and the ads W for the necessary pre-computations.
They were then evaluated by their serving of k = 10 ads  under a time constraint  for each page
in the test set. There was a clear separation of test and training. We measured computation time
in terms of the number of full evaluations by the algorithm (i.e.  the number of ads scored against
a given page). Thus  the true test of an algorithm was to quickly select the most promising T ads
to fully score against the page  where T ∈ {100  200  300  400  500} was externally imposed and
varied over the experiments. These numbers were chosen to be in line with real-world computational
constraints.
We tested four methods: halted threshold algorithm (TA)  as described in Section 1.2  two variants
of predictive indexing (PI-AVG and PI-DCG)  and a fourth method  called best global ordering
(BO)  which is a degenerate form of PI discussed in Section 2.1. An inverted index approach is
prohibitively expensive since almost all ad features have substantial inﬂuence on the score for every
web page (see Section 1.2).
PI-AVG and PI-DCG require a covering of the web page space. We used the natural covering sug-
gested by the binary features—each page feature i corresponds to a cover set consisting of precisely
those pages p that contain i. The resulting datastructure is therefore similar to that maintained by
the TA algorithm—lists  for each page feature  containing all the ads. However  while TA orders ads
j wi jpiaj for each ﬁxed page feature i  the predictive methods order by expected
score. PI-AVG sorts ad lists by expected score of f  Ep∼Di[f(p  a)] = Ep∼D[f(p  a)|i ∈ p]  condi-
tioned on the page containing feature i. PI-DCG and BO optimize the expected value of a modiﬁed
scoring rule  DCGf (p  a) = Ir(p a)≤16/ log2 (r(p  a) + 1)  where r is the rank function and I is the
indicator function. Here  r(p  a) = j indicates that ad a has rank j according to f(p  a) over all ads
in W . BO stores a single list of all ads  sorted by expected DCGf (p  a)  while PI-DCG stores a list
for each page feature i sorted by Ep∼Di[DCGf (p  a)]. We chose this measure because:

by partial score(cid:80)

5

1. Compared with using the average score of f  we empirically observe that expected DCGf

greatly improves the performance of BO on these data.

2. It is related to “discounted cumulative gain”  a common measure for evaluating search

results in the information retrieval literature (J¨arvelin & Kek¨al¨ainen  2002).

3. Expected DCGf is zero for many ads  enabling signiﬁcant compression of the predictive

index.

4. Lemma 2.1 suggests ordering by the probability an ad is in the top 10. The DCGf score is

a softer version of indicator of top 10.

All three predictive methods were trained by sampling from the training set  as described in 2.1.
Figure 3.1 plots the results of testing the four algorithms on the web advertising data. Each point in
the ﬁgure corresponds to one experiment  which consisted of executing each algorithm on 1000 test
pages. Along the x-axis we vary the time constraint imposed on the algorithm. The y-axis plots the
frequency  over the test pages  that the algorithm succeeded in serving the top scoring ad for position
1 (Figure 1(a)) and for position 10 (Figure 1(b)). Thus  vertical slices through each plot show the
difference in performance between the algorithms when they are given the same amount of serving
time per page. The probabilities were computed by off-line scoring of all 6.5× 105 ads for each test
page and computing the true top-10 ads. Serving correctly for position 10 is more difﬁcult than for
position 1  because it also requires correctly serving ads for positions 1 through 9. We see that all
three methods of predictive indexing are superior to Fagin’s halted threshold algorithm. In addition 
the use of a richer covering  for PI-DCG and PI-AVG  provides a large boost in performance. These
latter two predictive indexing methods attain relatively high accuracy even when fully evaluating
only 0.05% of the potential results.

(a)

(b)

Figure 1: Comparison of the ﬁrst and tenth results returned from the four serving algorithms on the
web advertisement dataset.

Our implementation of the predictive index  and also the halted threshold algorithm  required about
50ms per display event when 500 ad evaluations are allowed. The RAM use for the predictive index
is also reasonable  requiring about a factor of 2 more RAM than the ads themselves.

3.2 Approximate Nearest Neighbor Search

A special case application of predictive indexing is approximate nearest neighbor search. Given a set
of points W in n-dimensional Euclidean space  and a query point x in that same space  the nearest
neighbor problem is to quickly return the top-k neighbors of x. This problem is of considerable
interest for a variety of applications  including data compression  information retrieval  and pattern
recognition. In the predictive indexing framework  the nearest neighbor problem corresponds to

6

lllll1002003004005000.00.20.40.60.81.0Comparison of Serving AlgorithmsNumber of Full EvaluationsProbability of Exact Retrieval−1st ResultlllllllPI−AVGPI−DCGFixed OrderingHalted TAlllll1002003004005000.00.20.40.60.81.0Comparison of Serving AlgorithmsNumber of Full EvaluationsProbability of Exact Retrieval−10th ResultlllllllPI−AVGPI−DCGFixed OrderingHalted TAminimizing a scoring function  f(x  y) = ||x − y||2  deﬁned by Euclidean distance. We assume
query points are generated from a distribution D that can be sampled.
To start  we deﬁne a covering Q of the input space Rn  which we borrow from locality-sensitive
hashing (LSH) (Gionis et al.  1999; Datar et al.  2004)  a commonly suggested scheme for the
approximate nearest neighbor problem. Fix positive integer parameters α  β. First  we form α
random partitions of the input space. Geometrically  each partition splits the n-dimensional space
on β random hyperplanes. Formally  for all 1 ≤ i ≤ α and 1 ≤ j ≤ β  generate a random unit-
n ) ∈ Rn from the Gaussian (normal) distribution. For ﬁxed
norm n-vector Y ij = (Y ij
i ∈ {1  . . .   α} and subset J ⊆ {1  . . .   β} deﬁne the cover set Qi J = {x ∈ Rn : x · Y ij ≥
0 if and only if j ∈ J}. Note that for ﬁxed i  the set {Qi J|J ⊆ {1  . . .   k}} partitions the space by
random planes.

Given a query point x  consider the union Ux =(cid:83){Qi J∈Q | x ∈ Qi J} Qi J of all cover sets contain-

1   . . .   Y ij

ing x. Standard LSH approaches to the nearest neighbor problem work by scoring points in the set
Qx = W ∩ Ux. That is  LSH considers only those points in W that are covered by at least one of
the same α sets as x. Predictive indexing  in contrast  maps each cover set Qi J to an ordered list
of points sorted by their probability of being a top-10 nearest point to points in Qi J. That is  the
lists are sorted by hQi J (p) = Prq∼D|Qi J (p is one of the nearest 10 points to q). For the query x 
we then consider those points in W with large probability hQi J for at least one of the sets Qi J that
cover x.
We compare LSH and predictive indexing over four data sets: (1) MNIST—60 000 training and
10 000 test points in 784 dimensions; (2) Corel—37 749 points in 32 dimensions  split randomly
into 95% training and 5% test subsets; (3) Pendigits—7494 training and 3498 test points in 17
dimensions; and (4) Optdigits—3823 training and 1797 test points in 65 dimensions. The MNIST
data is available at http://yann.lecun.com/exdb/mnist/ and the remaining three data
sets are available at the UCI Machine Learning Repository (http://archive.ics.uci.edu/
ml/). Random projections were generated for each experiment  inducing a covering of the space that
was provided to both LSH and predictive indexing. The predictive index was generated by sampling
over the training set as discussed in Section 2.1. The number of projections β per partition was set to
24 for the larger sets (Corel and MNIST) and 63 for the smaller sets (Pendigits and Optdigits)  while
the number of partitions α was varied as an experimental parameter. Larger α corresponds to more
full evaluations per query  resulting in improved accuracy at the expense of increased computation
time. Both algorithms were restricted to the same average number of full evaluations per query.
Predictive indexing offers substantial improvements over LSH for all four data sets. Figure 2(a)
displays the true rank of the ﬁrst point returned by LSH and predictive indexing on the MNIST data
set as a function of α  averaged over all points in the test set and over multiple trials. Predictive
indexing outperforms LSH at each parameter setting  with the difference particularly noticeable
when fewer full evaluation are permitted (i.e.  small α). Figure 2(b) displays the performance of
LSH and predictive indexing for the tenth point returned  over all four data sets  with values of α
varying from 5 to 70  averaged over the test sets  and replicated by multiple runs. In over 300 trials 
we did not observe a single instance of LSH outperforming predictive indexing.
Recent work has proposed more sophisticated partitionings for LSH (Andoni & Indyk  2006). Ap-
proaches based on metric trees (Liu et al.  2004)  which take advantage of the distance metric struc-
ture  have also been shown to perform well for approximate nearest neighbor. Presumably  taking
advantage of the query distribution could further improve these algorithms as well  although that is
not studied here.

4 Conclusion

Predictive indexing is the ﬁrst datastructure capable of supporting scalable  rapid ranking based on
general purpose machine-learned scoring rules. In contrast  existing alternatives such as the Thresh-
old Algorithm (Fagin  2002) and Inverted Index approaches (Broder et al.  2003) are either substan-
tially slower  inadequately expressive  or both  for common machine-learned scoring rules. In the
special case of approximate nearest neighbors  predictive indexing offers substantial and consistent
improvements over the Locality Sensitive Hashing algorithm.

7

(a) The y-axis  “Rank of 1st Result” measures the
true rank of the ﬁrst result returned by each method.
As the number of partitions α is increased  improved
accuracy is achieved at the expense of longer com-
putation time.

(b) Each point represents the outcome of a single ex-
periment for one of the four data sets at various pa-
rameter settings.

Figure 2: Comparison of the ﬁrst and tenth results returned from LSH and predictive indexing.

References
Andoni  A.  & Indyk  P. (2006). Near-optimal hashing algorithms for near neighbor problem in high dimen-

sions. Proceedings of the Symposium on Foundations of Computer Science (FOCS’06).

Broder  A. Z.  Carmel  D.  Herscovici  M.  Soffer  A.  & Zien  J. (2003). Efﬁcient query evaluation using a
two-level retrieval process. CIKM ’03: Proceedings of the twelfth international conference on Information
and knowledge management (pp. 426–434).

Cheng  C.-S.  Chung  C.-P.  & Shann  J. J.-J. (2006). Fast query evaluation through document identiﬁer assign-

ment for inverted ﬁle-based information retrieval systems. Inf. Process. Manage.  42  729–750.

Chierichetti  F.  Lattanzi  S.  Mari  F.  & Panconesi  A. (2008). On placing skips optimally in expectation.
WSDM ’08: Proceedings of the international conference on Web search and web data mining (pp. 15–24).
New York  NY  USA: ACM.

Datar  M.  Immorlica  N.  Indyk  P.  & Mirrokni  V. S. (2004). Locality-sensitive hashing scheme based on p-
stable distributions. SCG ’04: Proceedings of the twentieth annual symposium on Computational geometry
(pp. 253–262). New York  NY  USA: ACM.

Fagin  R. (2002). Combining fuzzy information: an overview. SIGMOD Rec.  31  109–118.
Fagin  R.  Lotem  A.  & Naor  M. (2003). Optimal aggregation algorithms for middleware. J. Comput. Syst.

Sci.  66  614–656.

Gionis  A.  Indyk  P.  & Motwani  R. (1999). Similarity search in high dimensions via hashing. The VLDB

Journal (pp. 518–529).

J¨arvelin  K.  & Kek¨al¨ainen  J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions

on Information Systems  20  422–446.

Liu  T.  Moore  A.  Gray  A.  & Yang  K. (2004). An investigation of practical approximate nearest neighbor

algorithms. Neural Information Processing Systems.

Zobel  J.  & Moffat  A. (2006). Inverted ﬁles for text search engines. ACM Comput. Surv.  38  6.

8

llllll203040506070010203040LSH vs. Predictive Indexing on MNIST DataNumber of Partitions aaRank of 1st ResultlLSHPredictive Indexingllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll2040608010020406080100LSH vs. Predictive Indexing − All Data SetsLSH − Rank of 10th ResultPredictive Indexing − Rank of 10th Result,Daniel Bartz
Klaus-Robert Müller