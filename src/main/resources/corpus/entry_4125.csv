2019,Optimal Stochastic and Online Learning with Individual Iterates,Stochastic composite mirror descent (SCMD) is a simple and efficient method able to capture both geometric and composite structures of optimization problems in machine learning. Existing strategies require to take either an average or a random selection of iterates to achieve optimal convergence rates  which  however  can either destroy the sparsity of solutions or slow down the practical training speed. In this paper  we propose a theoretically sound strategy to select an individual iterate of the vanilla SCMD  which is able to achieve optimal rates for both convex and strongly convex problems in a non-smooth learning setting. This strategy of outputting an individual iterate can preserve the sparsity of solutions which is crucial for a proper interpretation in sparse learning problems. We report experimental comparisons with several baseline methods to show the effectiveness of our method in achieving a fast training speed as well as in outputting sparse solutions.,Optimal Stochastic and Online Learning with

Individual Iterates

Yunwen Lei1 2 Peng Yang1 Ke Tang1∗ Ding-Xuan Zhou3

1University Key Laboratory of Evolving Intelligent Systems of Guangdong Province 

Department of Computer Science and Engineering 

Southern University of Science and Technology  Shenzhen 518055  China

Technical University of Kaiserslautern  Kaiserslautern 67653  Germany

2Department of Computer Science 

3School of Data Science and Department of Mathematics 
City University of Hong Kong  Kowloon  Hong Kong  China

{leiyw  yangp  tangk3}@sustech.edu.cn

mazhou@cityu.edu.hk

Abstract

Stochastic composite mirror descent (SCMD) is a simple and efﬁcient method able
to capture both geometric and composite structures of optimization problems in
machine learning. Existing strategies require to take either an average or a random
selection of iterates to achieve optimal convergence rates  which  however  can
either destroy the sparsity of solutions or slow down the practical training speed. In
this paper  we propose a theoretically sound strategy to select an individual iterate
of the vanilla SCMD  which is able to achieve optimal rates for both convex and
strongly convex problems in a non-smooth learning setting. This strategy of out-
putting an individual iterate can preserve the sparsity of solutions which is crucial
for a proper interpretation in sparse learning problems. We report experimental
comparisons with several baseline methods to show the effectiveness of our method
in achieving a fast training speed as well as in outputting sparse solutions.

1

Introduction

Gradient-based methods have found wide applications to solve various optimization problems. A
basic and representative method of this type is the gradient descent  which iteratively moves iterates
along the minus gradient direction of the current iterate. However  gradient descent applied to
machine learning problems requires to go through all training examples at each iteration  which is not
efﬁcient when the sample size is large. Stochastic gradient descent (SGD) relieves this computational
burden by approximating the true gradient of the objective function with an unbiased estimation
based on a randomly selected training example. With this strategy  SGD can achieve sample-size
independent computational cost per iteration and therefore can be successfully applied to very large
datasets which are becoming ubiquitous in the big data era [2  4  44].
From different viewpoints  SGD has been extended in various ways. For example  the trick of variance-
reduction has been introduced to exploit the ﬁnite summation structure of objective functions for
reducing the inherent variance [16  33  40  43]. Adaptive step sizes were proposed to dynamically
incorporate the knowledge of the geometry of the data observed [9]. Decreasing step sizes in a
stagewise manner is used as a common trick in practice [14  19  41]. The trick of momentum is
widely used to accelerate SGD by choosing an appropriate direction to pursue per iteration [18  26  29].
Proximal operators have been introduced to capture a structure of optimization problems [28]  which

∗Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

separates the regularizer and data-ﬁtting terms to achieve a desired regularization effect. The concept
of mirror map has been introduced to induce a Bregman distance for reﬂecting the geometry of the
associated optimization problem [3  25]. Empirical studies have shown that these variants can further
improve the practical performance of SGD [4]. In this paper  we consider stochastic composite mirror
descent (SCMD)  which combines the technique of mirror map and proximal operator to capture both
the composite and geometry structure of the associated optimization problem.
Theoretical studies of SCMD have attracted much attention since its appearance and many results
have been derived to understand its promising behavior in practice. If the objective function is
convex  then the expected suboptimality of the uniform average of iterates decays with the rate
O(T − 1
2 ) after T iterations [6–8  30  46]. If the objective function is strongly convex  then the
expected suboptimality of the uniform average of iterates decays with the rate O(T −1 log T ) [17  35] 
which is unimprovable if the objective function is not smooth [31]. Surprisingly  some averaging
schemes have been proposed to achieve the optimal rate O(T −1) [14  20  31  37]. Although taking
averages of previous iterates can achieve theoretically minimax optimal rates [1]  it can have some
practical side effects [31  36  37]2. For example  it can destroy the sparsity of the solution which
is often crucial for a proper interpretation of models in many applications. Moreover  averaging
can affect the practical training speed due to the existence of some possibly poor iterates in the
iterate sequence produced by SCMD [31  36]. The side effect of destroying sparsity can be resolved
by randomly drawing an individual iterate with probabilities proportional to the weights in the
optimal averaging scheme. However  this strategy of selecting a random iterate would introduces
new variances  which may further slow down the training speed due to the possibility of selecting
a poor iterate. Outputting the last SGD iterate is a natural strategy in practice  which would not
affect sparsity and practical training speed. However  the suboptimality for the last SGD iterate
converges with suboptimal rates O(T − 1
2 log T ) and O(T −1 log T ) in the convex and strongly convex
cases [37]  respectively. Very recently  it was shown that the last SGD iterate can attain at most a rate
O(T −1 log T ) with high probabilities [13]. These facts motivate us to ask a natural question: can we
develop an algorithm which can inherit the optimal convergence rate from the averaging strategies as
well as the sparsity-preservation and fast practical training speed from the last SGD iterate?
In this paper  we aim to give an afﬁrmative answer to the above question by proposing a novel strategy
to select an individual iterate able to achieve theoretically optimal rates and fast training speed in
practice. We ﬁrst consider the case with the number of iterations known to us  which allows us to
divide the implementation into two stages. The ﬁrst stage produces an output with some averaging
scheme  which is then used in the second stage to select an individual iterate according to the one-step
progress evolution of SCMD. We then extend this algorithm to the online learning setting where
training examples arrive in a sequential way. We show that our algorithm can achieve the optimal
rates O(T − 1
2 ) and O(T −1) in the convex and strongly convex case  respectively. Rooted on a careful
one-step progress analysis  our selection of iterates is based on the difference of two successive
iterates  which shares some spirits with the common heuristic trick of terminating an algorithm if
the distance of two successive iterates is lower than a threshold. Our analysis also removes bounded
subgradient assumptions in the existing discussions of optimal learning rates [14  20  31  37]. We
report experimental results to conﬁrm the effectiveness of the proposed algorithm in both attaining a
fast training speed and producing a sparse solution.
We present algorithms with motivation in Section 2. Theoretical results and discussions are given in
Sections 3 and 4. Experimental results and conclusions are presented in Sections 5 and 6.

2 Algorithms with Motivations

2.1 Background

In supervised learning  we aim to infer an unknown relationship between input and output variables
from a sequence of training examples {zt = (xt  yt)}t∈N drawn from a probability measure deﬁned
in a sample space Z = X × Y with an input space X ⊂ Rd and an output space Y ⊂ R  where
d ∈ N is the dimension. A very elementary and powerful approximation of this relationship is a linear
model of the form x (cid:55)→ (cid:104)w  x(cid:105) with w ∈ Rd  whose behavior at a single training example (x  y) can
2When mentioning averaging  individual iterates or last iterate  our attention is with respect to the iterate sequence
produced by (2.2) below.

2

be quantiﬁed by a function f (w  z) = (cid:96)((cid:104)w  x(cid:105)  y)  where the loss function (cid:96) : R2 (cid:55)→ R+ is convex
with respect to (w.r.t.) the ﬁrst argument and (cid:104)w  x(cid:105) denotes the inner product between w and x. The
learning process can be often formulated as an optimization problem of a composite structure

min
w∈Rd

φ(w) = Ez[f (w  z)] + r(w) 

(2.1)
where Ez[·] denotes the expectation w.r.t. z and r : Rd (cid:55)→ R+ is a regularizer possibly inducing
sparsity. With speciﬁc instantiations of (cid:96) and r  Eq. (2.1) covers many famous learning problems in a
unifying framework  including least squares  SVMs  logistic regression  lasso and elastic-net [8  39].
SCMD provides an efﬁcient ﬁrst-order method to exploit the composite and geometry structure of the
problem (2.1) [8  24]. It extends SGD by using a strongly convex and differentiable mirror map Ψ to
generate an appropriate Bregman distance [3  25] DΨ(w  ˜w) = Ψ(w)− Ψ( ˜w)−(cid:104)w− ˜w ∇Ψ( ˜w)(cid:105) 
where ∇Ψ( ˜w) denotes the gradient of Ψ at ˜w. Let w1 ∈ Rd be an initial point and {ηt}t∈N
be a step size sequence. Up on the arrival of zt at the t-th iteration  we calculate a subgradient
f(cid:48)(wt  zt) ∈ ∂wf (wt  zt) as an unbiased estimate of F (cid:48)(wt) ∈ ∂F (wt)  where ∂wf (wt  zt) denotes
the subdifferential of f (·  zt) at wt and F (w) = Ez[f (w  z)]. SCMD updates the model by

(cid:0)(cid:104)w − wt  f(cid:48)(wt  zt)(cid:105) + r(w)(cid:1).

DΨ(w  wt) + ηt

wt+1 = arg min
w∈Rd

(2.2)
Intuitively  SCMD uses f(cid:48)(wt  zt) to form a ﬁrst-order approximation of f (·  zt) at wt and uses the
Bregman distance DΨ(w  wt) to keep wt+1 not far away from the current iterate. The regularizer r is
kept intact here to preserve a regularization effect. Typical choices of mirror maps include the p-norm
p (1 < p ≤ 2)  which works favorably when the solution of (2.1) is
divergence Ψp(w) = 1

sparse by setting p close to 1 [8  39]. Here (cid:107)·(cid:107)p is the p-norm deﬁned by (cid:107)w(cid:107)p =(cid:0)(cid:80)d

for w = (w(1)  . . .   w(d))(cid:62) ∈ Rd. SCMD recovers the stochastic proximal gradient descent by
taking Ψ = Ψ2 [7] and stochastic mirror descent by taking r(w) = 0 [14]. It should be mentioned
that ρ is not necessarily to be an empirical distribution over training examples  and therefore the
setting we consider here is more general than stochastic learning to minimize an empirical risk.

i=1 |w(i)|p(cid:1)1/p

2(cid:107)w(cid:107)2

sw ← w1  s ← 1
sw ← 6η1w1  s ← 6η1

Algorithm 1: SCMDI
Input: {ηt}t  σφ  w1 and T .
Output: an approximate solution of (2.1)
1 if σφ == 0 then
2
3 else
4
5 for t = 1  2 to T − 1 do
6
7
8
9
10
11

sw ← sw + wt+1  s ← s + 1
sw ← sw + (t + 2)(t + 3)ηt+1wt+1
s ← s + (t + 2)(t + 3)ηt+1

calculate wt+1 by (2.2)
if σφ == 0 then

else

12 ¯wT ← sw/s
13 for t = T  T + 1 to 2T − 1 do
14
15
16
17
18 return wT ∗

T ∗ ← t  wT ∗ ← wt

calculate wt+1 by (2.2)
(cid:52) ← DΨ( ¯wT   wt) − DΨ( ¯wT   wt+1)
if (cid:52) ≤ T −1DΨ( ¯wT   wT ) then

Algorithm 2: OCMDI
Input: {ηt}t  σφ and w1.
Output: an approximate solution of (2.1)
1 if σφ == 0 then
sw ← w1  s ← 1
2
3 else
sw ← 6η1w1  s ← 6η1
4
5 ¯w ← w1  ˆw ← w1  k ← 1
6 for t = 1  2 ··· do
7
8
9
10

calculate wt+1 by (2.2)
(cid:52) ← DΨ( ¯w  wt) − DΨ( ¯w  wt+1)
if (cid:52) ≤ 21−kDΨ( ¯w  ˆw) then

if σφ == 0 then

˜w ← wt
sw ← sw + wt+1  s ← s + 1
sw ← sw + (t + 2)(t + 3)ηt+1wt+1
s ← s + (t + 2)(t + 3)ηt+1
k ← k + 1  ¯w ← sw/s  ˆw ← wt

if t == 2k − 1 then

11
12
13
14
15

else

16
17
18 return ˜w

2.2 Algorithms

We now present our ﬁrst algorithm (Algorithm 1) which performs SCMD in two stages. In the ﬁrst
stage  we simply perform updates according to (2.2). An output ¯wT is then produced by taking some

3

averages of the previous iterates. It is clear that ¯wT is a simple average with uniform weights in the
convex case  while the construction of ¯wT in the strongly convex case is motivated by theoretical
analysis (see Theorem 4) [20]. In the second stage  other than updating {wt} with (2.2)  we also
search a time index (line 17 of Algorithm 1) at which the difference between two consecutive Bregman
distance is less than a small number. We refer to Algorithm 1 as Stochastic Composite Mirror Descent
with Individual iterates (SCMDI) since it outputs an individual iterate of the vanilla SCMD.
Remark 1. For any t  denote At := DΨ( ¯wT   wt) − DΨ( ¯wT   wt+1). It should be mentioned that
the function t (cid:55)→ At is not monotonically w.r.t. t. Therefore  not all t ≥ T ∗ can necessarily satisfy
the condition in line 16 of Algorithm 1  which is a requirement to achieve optimal convergence rates
in our analysis.
In Algorithm 1  we update T ∗ once we ﬁnd a new t satisfying At ≤ T −1DΨ( ¯wT   wT ). Another
feasible strategy is to set T ∗ = arg mint∈{T T +1 ... 2T−1} At. However  experiments show that this
strategy does not work as well as Algorithm 1.

Algorithm 2 is an extension of Algorithm 1 by removing the information on T . It differs from the
vanilla SCMD in two aspects. Firstly  it computes some average ¯wT of previous iterates at the (2k−1)-
th iteration  k ∈ N. Secondly  it searches an index t such that DΨ( ¯wT   wt) − DΨ( ¯wT   wt+1) ≤
21−kDΨ( ¯wT   wT ) and outputs this individual iterate. Algorithm 2 with 2k ≤ t < 2k+1 recovers
Algorithm 1 with T = 2k or T = 2k−1  depending on whether an index t ≥ 2k satisfying At ≤
21−kDΨ( ¯wT   wT ) is found or not. Therefore  Algorithm 2 can achieve the same convergence rates
as Algorithm 1 in both convex and strongly convex cases. It does not need the information on T and
therefore applies to the online learning setting. We refer to Algorithm 2 as Online Composite Mirror
Descent with Individual iterates (OCMDI).

2.3 Motivation

Before giving theoretical results  we sketch the key idea underlying the design of Algorithm 1 which
also forms a key foundation of our theoretical analysis. Let w∗ = argminw∈Rd φ(w). Typically we
can get the following one-step error bounds for {wt}t∈N produced by (2.2) (Lemma A.1 in Appendix)

ηtE[φ(wt)−φ(w)] ≤ E[DΨ(w  wt)−DΨ(w  wt+1)]+η2

for any w ∈ Rd independent of zt. Here (cid:101)C is a constant independent of t. If we choose w = w∗

and can show E[DΨ(w∗  wt) − DΨ(w∗  wt+1)] = O(η2
√
t )  then we would immediately obtain
E[φ(wt)] − φ(w∗) = O(ηt). This would immediately imply the rate O(1/
t) in the convex case
and the rate O(1/t) in the strongly convex case since in these two cases the typical step size choices
t and ηt = 1/t (ignoring constant factors)  respectively [8]. Since w∗ is unknown  any
are ηt = 1/
algorithm requiring an access to w∗ is not implementable. A good surrogate ¯wT of w∗ should satisfy
E[φ( ¯wT )]− φ(w∗) = O(ηT ) to enjoy a tight rate  for which a natural choice should be some average
of {wt}t≤T . Then  we take w = ¯wT in (2.3) and need to ﬁnd an index T ∗ ∈ {T  T + 1  . . .   2T − 1}
such that E[DΨ( ¯wT   wT ∗ ) − DΨ( ¯wT   wT ∗+1)] = O(η2
T ). By the non-negativity of Bregman
distance  there always exists a T ∗ ∈ {T  T + 1  . . .   2T − 1} satisfying (see Lemma A.2 in Appendix)

(2.3)

√

t (cid:101)C

DΨ( ¯wT   wT ∗ ) − DΨ( ¯wT   wT ∗+1) ≤ T −1DΨ( ¯wT   wT ).

This motivates us to search the time index T ∗ by Algorithm 1 (line 17). It is clear from (2.3) that

E[φ(wT ∗ ) − φ( ¯wT )] ≤ (T ηT ∗ )−1E[DΨ( ¯wT   wT )] + ηT ∗(cid:101)C.

The term E[φ(wT ∗ )] − φ(w∗) then can be estimated by the following error decomposition

E[φ(wT ∗ )] − φ(w∗) = E[φ(wT ∗ ) − φ( ¯wT )] + E[φ( ¯wT ) − φ(w∗)]

≤ (T ηT ∗ )−1E[DΨ( ¯wT   wT )]+ηT ∗(cid:101)C +E[φ( ¯wT )−φ(w∗)].

To derive the desired bound E[φ(wT ∗ )] − φ(w∗) = O(ηT )  it sufﬁces to show

(2.4)

(2.5)

(T ηT ∗ )−1E[DΨ( ¯wT   wT )] = O(ηT )

and E[φ( ¯wT ) − φ(w∗)] = O(ηT ).

More speciﬁcally  we need to show E[DΨ( ¯wT   wT )] = O(1)  E[φ( ¯wT )− φ(w∗)] = O(T − 1
√
2 ) in the
t  and E[DΨ( ¯wT   wT )] = O(T −1)  E[φ( ¯wT ) − φ(w∗)] = O(T −1) in
convex case with ηt = 1/
the strongly convex case with ηt = 1/t. We will show this is possible by choosing T ∗ in Algorithm 1.

4

3 Optimal Convergence Rates

(cid:107)f(cid:48)(w  z)(cid:107)2∗ ≤ Af (w  z) + B and (cid:107)r(cid:48)(w)(cid:107)2∗ ≤ Ar(w) + B

We present here optimal convergence rates for SCMDI for both convex and strongly convex objectives.
To this aim  we need to impose some standard assumptions. We assume that the mirror map Ψ is
σΨ-strongly convex w.r.t. a norm (cid:107)·(cid:107) in the sense DΨ(w  ˜w) ≥ 2−1σΨ(cid:107)w− ˜w(cid:107)2 for all w  ˜w ∈ Rd
(σΨ > 0). We always assume the existence of A and B > 0 such that ((cid:107)·(cid:107)∗ is the dual norm of (cid:107)·(cid:107))
(3.1)
for any w ∈ Rd  z ∈ Z and any f(cid:48)(w  z) ∈ ∂f (w  z)  r(cid:48)(w) ∈ ∂r(w). Many popular learning
methods use loss functions and regularizers satisfying (3.1) [7  44]. For example  if |(cid:96)(cid:48)(a  y)|2 ≤
˜A(cid:96)(a  y) + ˜B for some ˜A  ˜B > 0 and all a  y  then f (w  z) = (cid:96)((cid:104)w  x(cid:105)  y) would satisfy the ﬁrst
inequality of (3.1) if X is bounded. Here (cid:96)(cid:48)(a  y) denotes a subgradient of (cid:96) w.r.t. the ﬁrst argument.
Examples of such (cid:96) include all smooth functions and all Lipschitz continuous functions widely used
in machine learning [42  44]. Examples of r satisfying (3.1) include r(w) = λ(cid:107)w(cid:107)p
p with p ∈ [1  2].
We say Ψ is LΨ-smooth w.r.t. (cid:107) · (cid:107) if DΨ(w  ˜w) ≤ LΨ
2 (cid:107)w − ˜w(cid:107)2 for all w  ˜w ∈ Rd.
We always assume the existence of σF   σr ≥ 0 such that for all w  ˜w ∈ Rd
F (w)−F ( ˜w)−(cid:104)w− ˜w  F (cid:48)( ˜w)(cid:105) ≥ σF DΨ(w  ˜w)  r(w)−r( ˜w)−(cid:104)w− ˜w  r(cid:48)( ˜w)(cid:105) ≥ σrDΨ(w  ˜w)
(3.2)
For σφ := σF + σr  the cases σφ = 0 and σφ > 0 correspond to convex and strongly convex
objectives  respectively.

3.1 Convex objectives
Our ﬁrst result is an optimal rate O(T − 1
2 ) for convex objectives under a boundedness assumption of
iterates. The boundedness assumptions E[DΨ(w∗  wt)] ≤ D ∀t ∈ N  imposed also in the literature
[7  8  37]  always hold for the regularizer of the form r(w) = IW0(w) + ˜r(w)  where W0 is a convex
and compact domain  IW0 is an indicator function with IW0(w) = 0 if w ∈ W0 and ∞ otherwise 
and ˜r : Rd → R+ is convex. Proofs of theoretical results in this subsection are given in Appendix B.
Theorem 1. Let D > 0. Assume E[DΨ(w∗  wt)] ≤ D for all t ∈ N and E[DΨ( ¯wT   wT )] ≤ D.
√
Let wT ∗ be deﬁned by Algorithm 1 with ηt = µ/

t satisfying µ ≤ σΨ(2A)−1. Then  E(cid:2)φ(wT ∗ ) −

φ(w∗)(cid:3) ≤ (cid:101)C1√

T

  where (cid:101)C1 = (4 + 2

√

2)µ−1D + 10µσ−1

Ψ (Aφ(w∗) + 2B).

Theorem 1 requires to impose a boundedness assumption on iterates  which is removed in the
following theorem on convergence rates. The assumption DΨ(w  ˜w) ≤ LΨ(cid:107)w − ˜w(cid:107)α is milder than
assuming a smoothness of Ψ  the latter of which is satisﬁed if Ψ = Ψ2.
Theorem 2. Let wT ∗ be produced by Algorithm 1 with a non-increasing step size sequence and η1 ≤
σΨ(2A)−1. Assume that there exists α ∈ [0  2] and LΨ > 0 such that DΨ(w  ˜w) ≤ LΨ(cid:107)w − ˜w(cid:107)α
for all w  ˜w ∈ Rd. Let DT = DΨ(w∗  w1) + σ−1
E[φ(wT ∗ )] − φ(w∗) ≤ 2LΨ

Ψ αDT + 1(cid:3) + 4DT
(cid:2)4σ−1

Ψ (Aφ(w∗) + 2B)(cid:80)T

t=1 η2
2Aφ(w∗) + 4B

T(cid:88)

t . Then 

ηt

. (3.3)

(cid:104)

(cid:105)

+

T η2T

σΨ

2
T

ηT +

(cid:80)t

t=1

Corollary 3. Suppose the assumptions in Theorem 2 hold.

= 0  then
for t = 1  . . .   2T   then E[φ(wT ∗ )]−φ(w∗) = O(T − 1
2 ).

lim
t→∞

(a) If

˜t=1 η2
˜t
tηt

E[φ(wT ∗ )] = φ(w∗). (b) If ηt = µ√

T

lim
T→∞

3.2 Strongly convex objectives

We now turn to Algorithm 1 for strongly convex objectives  towards which a ﬁrst step is the following
theorem on the performance of vanilla SCMD. Theorem 4 shows that both the suboptimality of
some weighted averaged iterates and the Bregman distance between the last iterate and w∗ converge
with the rate O(T −1). Theorem 4 is an extension of the results in [20] on the projected SGD
to SCMD. Discussions in [20] require to impose a boundedness assumption on subgradients as
EZ[(cid:107)f(cid:48)(wt  Z)(cid:107)2
in [20] to show that this boundedness assumption holds for an SVM-like objective. Theorem 4
shows that we can derive the same convergence rate without this boundedness assumption and

2] ≤ (cid:101)D for some (cid:101)D > 0 and all t ∈ N. Indeed  an independent section was included

5

σr = 0  then ¯wT deﬁned in Theorem 4 can be simpliﬁed as ¯wT =(cid:2)(cid:80)T

is therefore more applicable. Theorem 5 gives a sufﬁcient condition on step size sequences for
the convergence. Upper and lower bounds on convergence rates are established in Theorem 6 and
Theorem 7  respectively. The proofs of theoretical results in this subsection are given in Appendix C. If
t=1(t + 1)wt.
.

Theorem 4. Assume (3.2) holds with σφ > 0. Let {wt}t∈N be generated by (2.2) with ηt =
Deﬁne

t=1(t + 1)(cid:3)−1(cid:80)T

σφt+2σF

2

(cid:104) T(cid:88)

t=1

(cid:105)−1 T(cid:88)

t=1

¯wT =

(t + 1)(t + 2)ηt

(t + 1)(t + 2)ηtwt.

Then  there exists a constant (cid:101)C2 > 0 independent of T and σφ (explicitly given in the proof) such that
and(cid:80)∞

(3.4)
Theorem 5. Assume (3.2) holds with σφ > 0. Let {wt}t∈N be generated by (2.2). If limt→∞ ηt = 0

t=1 ηt = ∞  then limT→∞ E[DΨ(w∗  wT )] = 0.

and E[DΨ(w∗  wT +1)] ≤

E[φ( ¯wT ) − φ(w∗)] ≤

4(cid:101)C2

σ2
φ(T + 2)

σφ(T + 1)

(cid:101)C2

.

With Theorem 4 at hand  we can provide optimal rates for the output produced by Algorithm 1.
Theorem 6. Assume (3.2) holds with σφ > 0. Let wT ∗ be generated by Algorithm 1 with ηt =
  then there exists a constant
.

(cid:101)C3 > 0 independent of T and σφ (explicitly given in the proof) such that E(cid:2)φ(wT ∗ )−φ(w∗)(cid:3) ≤ (cid:101)C3

. Assume that Ψ is LΨ-strongly smooth (LΨ > 0). If T ≥ 4A

σφt+2σF

σΨσφ

2

T σφ

Theorem 7 presents lower bounds matching the above upper bounds up to a constant factor  which
shows that the selected iterate has achieved the best possible rate and there are no stronger results. The
matching upper and lower bounds here apply to the speciﬁc SGD and speciﬁc optimization problems 
while minimax bounds is related to the existence of a problem for any algorithm [1]. Similar results
were derived in [21] for online mirror descent. Here we give a different and simpler proof.
Theorem 7. Let {wt}t be the sequence produced by (2.2) with Ψ(w) = 1
2 and r(w) = 0.
Suppose f is differentiable  φ is Lφ-smooth w.r.t. (cid:107) · (cid:107)2 and ηt ≤ 1/(2Lφ). If for any w ∈ Rd 
E[(cid:107)∇f (w  z) − ∇F (w)(cid:107)2

2(cid:107)w(cid:107)2

E[(cid:107)wt+1 − w∗(cid:107)2

2] ≥ σ2 for some σ > 0  then
2] ≥ min{(cid:107)w1 − w∗(cid:107)2

2  η1σ2/(2Lφ)  . . .   ηtσ2/(2Lφ)}.

4 Discussions

We discuss related work on stochastic/online learning algorithms with different averaging schemes.
√
A very common scheme is to output an average of all iterates with uniform weights (UNI-AVE) [44 
T ) and the suboptimal rate O(T −1 log T ) in the
46]  which is able to attain the optimal rate O(1/
convex and strongly convex case  respectively [8]. A counterpart is established to show that the rate
O(T −1 log T ) is the best possible one for this simple averaging scheme [31].
For strongly convex objectives  the ﬁrst algorithm able to attain the optimal rate O(T −1) is the
epoch-GD algorithm proposed in [14]  which performs stochastic mirror descent in each epoch with
a ﬁxed step size. The step size decreases exponentially in each consecutive epoch and the averaged
iterate of the last epoch is outputted as the solution. Since then  several other averaging schemes
were developed to attain optimal rates  including a sufﬁx-averaging scheme (SUFFIX) returning the
uniform average of the last half of SGD iterates [31]  a weighted averaging scheme (WEI-AVE) with
a weight of t + 1 for wt [20] and a polynomial-decay averaging scheme [37].
Motivated by the side effects of averaging in either destroying the sparsity of the solution or slowing
down the training speed [31]  the property of the last iterate (LAST) has also received a lot of attention.
For smooth objective functions  SGD with the last iterate can achieve the optimal rate [31]. The
analysis of last iterate for non-smooth objective functions is much more challenging  for which an
interesting technique to relate the last iterate with sufﬁx-average of iterates was developed [37  44].
Based on this  rates O(T − 1
2 log T ) and O(T −1 log T ) were established in the convex and strongly-
convex cases [23  37]  respectively. These results motivate a natural question of whether the last
iterate achieves the optimal rate in the non-smooth scenario. This open question was resolved in [13]

6

by constructing a problem for which SGD with the last iterate achieves at most the rate Ω(T −1 log T ).
An interesting probabilistic rate O(T −1 log T ) was also developed for SGD with the last iterate [13].
All the above mentioned methods require an averaging scheme to attain the optimal convergence
rates  which  however  may destroy the sparsity of solutions and slow down the practical training
speed. A simple scheme to preserve the sparsity of solutions while still enjoying the optimal rate is
to draw an individual iterate randomly from the iterate sequence by (2.2) [10  16]. Indeed  one can
randomly draw the output according to a distribution over the iterate sequence with the probability
mass function determined by any optimal weighted averaging scheme [20  31  37]. However  this
scheme introduces new variances due to the choice of a random iterate as the output. Moreover  as we
will verify in experiments  this scheme can slow down the practical training speed since the randomly
selected iterate is not necessarily to be a favorable one in the iterate sequence. Furthermore  it requires
to either store all the iterate sequence or determine the number of iterations beforehand  which does
not apply to the online learning scenario. After the acceptance of the paper  we noticed an interesting
step-size modiﬁcation scheme to get optimal convergence rates [15]. However  the step-size scheme
there requires to use the information of T   and therefore is not applicable to the online learning
scenario. Furthermore  the algorithm and optimal convergence rates there are developed for the
particular T -th iterate and not for other iterates.
In this paper  we propose a novel stochastic/online learning algorithm able to achieve optimal rates. As
compared to the averaging scheme in [14  20  31  37]  our scheme of outputting an individual iterate is
able to preserve the sparsity structure of solutions. As compared to the scheme of randomly choosing
an individual iterate  our method avoids the added variance as well as the drawback of slowing down
the training speed due to the random iterate index. Our idea is not to output the last iterate but to
select an individual iterate based on a careful analysis on the one-step progress bound sketched in
Section 2.3. Indeed  by (2.3)  the quality of wt depends on At = DΨ( ¯wT   wt) − DΨ( ¯wT   wt+1).
The smaller the At  the better the quality. This motivates us to select a T ∗ with small AT ∗. Intuitively 
At is related to the distance between wt and wt+1. Therefore  our selection of the iterate shares some
spirit with the widely used heuristic of terminating the algorithm when the successive iterates are
close. Our analysis also reﬁnes existing studies of optimal rates of SGD by removing the boundedness
assumption on subgradients [20  31  37]. It should be mentioned that the boundedness assumption

was relaxed as Ez[(cid:107)f(cid:48)(wt  z)(cid:107)2∗] ≤ (cid:101)A + (cid:101)B(cid:107)F (cid:48)(wt)(cid:107)2∗ in [4] for (cid:101)A  (cid:101)B > 0 and removed in [27]  the

latter of which  however  requires the objective function to be smooth.

5 Experimental Results

In this section  we justify the effectiveness of our algorithm by presenting experimental comparisons
with the following averaging strategies: WEI-AVE [20]  UNI-AVE  LAST  SUFFIX [31] and RAND
(outputting a random iterate chosen from the uniform distribution over the last half of iterates). We
consider two applications: binary classiﬁcation and tomography reconstruction in image processing.

5.1 Binary classiﬁcation

We ﬁrst consider SVM models with linear kernels. We use 16 real-world datasets whose information
is summarized in Table C.1 in Appendix D.13. For each dataset  we use 80 percents of the data for
training and reserve the remaining 20 percents for testing. The objective function we consider for a
training dataset {(x1  y1)  . . .   (xn  yn)} is
(cid:107)w(cid:107)2

max{0  1 − yi(cid:104)w  xi(cid:105)} 

n(cid:88)

φ(w) =

2 +

λ
2

1
n

i=1

which is λ-strongly convex w.r.t. (cid:107)·(cid:107)2. Analogous to [20]  we set λ to the reciprocal of training sample
size. For datasets with multiple class labels  we group the ﬁrst half of labels into the positive label  and
the second half into the negative label. We consider step sizes of the form ηt = µ/(λt) and tune the
parameter µ in the set 2{−12 −11 ... 4} by 10-fold cross validation. We repeat the experiment 40 times
and report the average of experimental results. We consider two approaches to optimize the objective
function φ by whether to separate the regularizer and the data-ﬁtting term. In the ﬁrst approach 
2 + max{0  1 − y(cid:104)w  x(cid:105)} and r(w) = 0 
we apply (2.2) with Ψ(w) = 1

2  f (w  z) = λ

2(cid:107)w(cid:107)2

2(cid:107)w(cid:107)2

3We display experimental results for 4 datasets here due to space limit. Complete results are in Appendix.

7

2(cid:107)w(cid:107)2

2(cid:107)w(cid:107)2

which is just the SGD. In the second approach  we apply (2.2) with Ψ(w) = 1
2  f (w  z) =
max{0  1 − y(cid:104)w  x(cid:105)} and r(w) = λ
2  which is a stochastic proximal gradient descent (SPGD).
In Figure 1 and Figure 2  we plot the objective function values on the testing dataset against iteration
numbers for SGD and SPGD  respectively. It is clear that UNI-AVE is always the worst strategy in
our experiments. WEI-AVE assigns more weights to recent iterates and can improve the performance 
which is further outperformed by SUFFIX. RAND ﬂuctuates a bit since the randomly selected index
at the t-th iteration may not be an increasing function of t. LAST is overwritten by OCMDI due
to their similar behavior  which behave best in our experiments especially in the beginning of the
optimization process. The similarity between OCMDI and LAST can be explained as follows. If At
(deﬁned in Section 4) is small then it is likely that At+1 is also small. Therefore  our algorithm is
prone to select the last part of iterates. We also plot the objective function values on training datasets
against T   which show similar behavior and are deferred to Figures D.4  D.3 in Appendix.

(a) german

(d) letter
Figure 1: Objective function values on test datasets versus iteration numbers for SGD.

(b) splice

(c) w8a

(a) german
(d) letter
Figure 2: Objective function values on test datasets versus iteration numbers for SPGD.

(b) splice

(c) w8a

5.2 Tomography reconstruction

†
i|)2) to y

parameters and consider the mirror map Ψ( λ)(w) = λ(cid:80)d

We now consider tomography reconstruction in image processing. We use AIR toolbox [12] to create
a CT-measurement matrix A ∈ Rn×d  an output vector y† ∈ Rn and a N × N sparse image encoded
by a vector w† ∈ Rd with d = N 2. Each row of A together with the corresponding row in y indicates
a line integral from a fan bean projection geometry. Therefore  the true image w† satisﬁes Aw† = y†.
†
We consider a noisy case by adding Gaussian noise N (0  (0.05|y
i and get the noisy output
y. Our aim is to reconstruct the image w† from the matrix A and the noisy measurements y by
ﬁnding an approximate solution of the equation Aw = y. Since many components of the true
image w† varnish  we apply randomized sparse Kaczmarz algorithm  which is a simple and efﬁcient
algorithm to generate sparse approximate solutions to linear systems [5  34]. Let  > 0  λ > 0 be two
2  where g(ξ) = ξ2
2 for |ξ| > . At each iteration t  we randomly select an index it from the
for |ξ| ≤  and |ξ| − 
uniform distribution over {1  . . .   n} and denote xit = A(cid:62)
it is the transpose of Ait. Given
w1 ∈ Rd and v1 = ∇Ψ( λ)(w1)  the randomized sparse Kaczmarz algorithm updates the model as
(5.1)
where Sλ  : Rd (cid:55)→ Rd is deﬁned component-wisely by the soft-thresholding function Sλ  : R (cid:55)→ R
given as [5]

vt+1 = vt − ηt((cid:104)wt  xit(cid:105) − yit)xit  wt+1 = Sλ (vt+1) 

i=1 g(w(i)) + 1
it  where A(cid:62)

2(cid:107)w(cid:107)2

2

(cid:26)(ξ)/(λ + ) 

Sλ (ξ) :=

sgn(ξ) max(|ξ| − λ  0)  otherwise.

if |ξ| ≤ λ + 

Here sgn(a) denotes the sign of a ∈ R. Algorithm (5.1) can be equivalently formulated as [22]

wt+1 = arg min
w∈Rd

ηt(cid:104)w − wt  ((cid:104)wt  xit(cid:105) − yit)xit(cid:105) + DΨ( λ)(w  wt).

8

It is clear from the above equivalent formulation that (5.1) is an instantiation of (2.2) with f (w  z) =
2 ((cid:104)w  x(cid:105) − y)2  r(w) = 0 and Ψ(w) = Ψ( λ)(w) to minimize F (w) = 1
2. It was
1
σmin((cid:101)A) denotes the minimal positive eigenvalue of a matrix (cid:101)A. Therefore  our theoretical analysis
shown that F (w) satisﬁes (3.2) with σF = σmin(A(cid:62)A/n)  w = w∗ and ˜w = wt  t ∈ N [22]  where

n(cid:107)Aw − y(cid:107)2

in Section 3.2 applies4. We randomly choose w1 from the uniform distribution in [0  1]d and set
λ = 1   = 10−8 as suggested in [5]. We repeat the experiment 40 times and report the average of
results.

(a) True Image.

(b) Reconstructed Image.

(c) Error vs. T .

(d) NNCs vs. T

Figure 3: Tomography reconstruction with N = 64  n = 23040 and 5% relative noise. Panel (a) and
(b) are the true image and the reconstructed image by OCMDI  respectively. Panel (c) and (d) plot
the errors and NNCs versus iteration numbers.
In Figure 3  we present results with N = 64  d = 4096 and n = 23040. Panels (a) and (b) display
the true image and the image output by OCMDI  respectively. In Panels (c) and (d)  we plot the
errors and the number of non-zero components (NNCs) for the output of algorithms with different
averaging strategies. According to Figure 3  LAST is overwritten by OCMDI since they behave
analogously  both of which achieve the fastest training speed among all considered methods. RAND
is outperformed by SUFFIX in terms of training speed. Moreover  OCMDI  RAND and LAST
can produce much more sparse solutions than other methods. Indeed  the true image w† has 1686
non-zero components  while UNI-AVE  WEI-AVE  SUFFIX  RAND and OCMDI produce outputs
with 4096  3727  1943  1823 and 1828 non-zero components  respectively. The advantage of OCMDI
over LAST is that OCMDI can theoretically achieve optimal convergence rate. Similar phenomenon
also appears for the case with N = 32 and n = 11520  which we plot in Figure D.5 in Appendix.

6 Conclusion

We propose a novel variant of SCMD with optimal convergence rates. An advantage of this algorithm
over existing optimal learning algorithms is that it outputs an individual iterate without a random
selection of index  which is able to preserve the sparsity structure without slowing down the training
speed. Experimental results in both the domain of binary classiﬁcation and tomography reconstruction
demonstrate the ability of our algorithm in getting a fast training speed as well as in producing
sparse solutions. Some interesting work include a theoretical justiﬁcation on the advantage of
sparsity of our method over other methods [11  38] and an extension of our analysis to non-convex
problems [10  32  45].

Acknowledgments

The work of Y. Lei  P. Yang and K. Tang is supported partially by the National Key Research
and Development Program of China (Grant No. 2017YFB1003102)  the National Natural Science
Foundation of China (Grant Nos. 61806091  61806090 and 61672478)  the Program for University
Key Laboratory of Guangdong Province (Grant No. 2017KSYS008) and Shenzhen Peacock Plan
(Grant No. KQTD2016112514355531). The work of D.-X. Zhou is supported partially by the
Research Grants Council of Hong Kong [Project No. CityU 11338616] and by National Nature
Science Foundation of China [Grant No. 11671307]. Y. Lei also acknowledges a Humboldt Research
Fellowship from the Alexander von Humboldt Foundation.

References
[1] A. Agarwal  M. J. Wainwright  P. L. Bartlett  and P. K. Ravikumar. Information-theoretic lower bounds on
the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems 

4In our analysis  we only use (3.2) for w = w∗ and ˜w = wt (a restricted strong convexity in literature).

9

pages 1–9  2009.

[2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate

O(1/n). In Advances in Neural Information Processing Systems  pages 773–781  2013.

[3] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex

optimization. Operations Research Letters  31(3):167–175  2003.

[4] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning. SIAM

Review  60(2):223–311  2018.

[5] J.-F. Cai  S. Osher  and Z. Shen. Linearized bregman iterations for compressed sensing. Mathematics of

Computation  78(267):1515–1536  2009.

[6] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning algorithms.

IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[7] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. In Advances

in Neural Information Processing Systems  pages 495–503  2009.

[8] J. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent. In Conference

on Learning Theory  pages 14–26  2010.

[9] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[10] S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming.

SIAM Journal on Optimization  23(4):2341–2368  2013.

[11] Z.-C. Guo  D.-H. Xiang  X. Guo  and D.-X. Zhou. Thresholded spectral algorithms for sparse approxima-

tions. Analysis and Applications  15(03):433–455  2017.

[12] P. C. Hansen and M. Saxild-Hansen. Air tools—a matlab package of algebraic iterative reconstruction

methods. Journal of Computational and Applied Mathematics  236(8):2167–2178  2012.

[13] N. J. A. Harvey  C. Liaw  Y. Plan  and S. Randhawa. Tight analyses for non-smooth stochastic gradient
descent. In A. Beygelzimer and D. Hsu  editors  Conference on Learning Theory  pages 1579–1613  2019.
[14] E. Hazan and S. Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-

convex optimization. Journal of Machine Learning Research  15(1):2489–2512  2014.

[15] P. Jain  D. Nagaraj  and P. Netrapalli. Making the last iterate of sgd information theoretically optimal. In

A. Beygelzimer and D. Hsu  editors  Conference on Learning Theory  pages 1752–1755  2019.

[16] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems  pages 315–323  2013.

[17] S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex programming

algorithms. In Advances in Neural Information Processing Systems  pages 801–808  2009.

[18] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations  2015.

[19] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems  pages 1097–1105  2012.

[20] S. Lacoste-Julien  M. Schmidt  and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002  2012.

[21] Y. Lei and D.-X. Zhou. Convergence of online mirror descent. Applied and Computational Harmonic

Analysis  2018. doi: https://doi.org/10.1016/j.acha.2018.05.005.

[22] Y. Lei and D.-X. Zhou. Learning theory of randomized sparse Kaczmarz method. SIAM Journal on

Imaging Sciences  11(1):547–574  2018.

[23] J. Lin  L. Rosasco  and D.-X. Zhou. Iterative regularization for learning with convex loss functions. Journal

of Machine Learning Research  17(77):1–38  2016.

[24] P.-L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on

Numerical Analysis  16(6):964–979  1979.

[25] A.-S. Nemirovsky and D.-B. Yudin. Problem Complexity and Method Efﬁciency in Optimization. John

Wiley & Sons  1983.

[26] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course  volume 87. Springer Science

& Business Media  2013.

[27] L. M. Nguyen  P. H. Nguyen  M. van Dijk  P. Richtárik  K. Scheinberg  and M. Takác. SGD and hogwild!
convergence without the bounded gradients assumption. In International Conference on Machine Learning 
pages 3747–3755  2018.

[28] N. Parikh and S. P. Boyd. Proximal algorithms. Foundations and Trends in optimization  1(3):127–239 

2014.

[29] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational

Mathematics and Mathematical Physics  4(5):1–17  1964.

[30] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on

Control and Optimization  30(4):838–855  1992.

10

[31] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex stochastic

optimization. In International Conference on Machine Learning  pages 449–456  2012.

[32] S. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for nonconvex

optimization. In International Conference on Machine Learning  pages 314–323  2016.

[33] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.

Mathematical Programming  162(1-2):83–112  2017.

[34] F. Schöpfer and D. A. Lorenz. Linear convergence of the randomized sparse Kaczmarz method. Mathe-

matical Programming  pages 1–28  2018.

[35] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

svm. Mathematical programming  127(1):3–30  2011.

[36] O. Shamir. Open problem: Is averaging needed for strongly convex stochastic gradient descent? 2012.
[37] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization convergence results and

optimal averaging schemes. In International Conference on Machine Learning  pages 71–79  2013.

[38] J. Steinhardt  S. Wager  and P. Liang. The statistics of streaming sparse regression. arXiv preprint

arXiv:1412.4182  2014.

[39] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research  11:2543–2596  2010.

[40] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization  24(4):2057–2075  2014.

[41] Y. Xu  Q. Lin  and T. Yang. Stochastic convex optimization: Faster local growth implies faster global

convergence. In International Conference on Machine Learning  pages 3821–3830  2017.

[42] Y. Ying and D.-X. Zhou. Unregularized online learning algorithms with general loss functions. Applied

and Computational Harmonic Analysis  42(2):224—-244  2017.

[43] L. Zhang  M. Mahdavi  and R. Jin. Linear convergence with condition number independent access of full

gradients. In Advances in Neural Information Processing Systems  pages 980–988  2013.

[44] T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In

International Conference on Machine Learning  pages 919–926  2004.

[45] Z. Zhou  P. Mertikopoulos  N. Bambos  S. Boyd  and P. W. Glynn. Stochastic mirror descent in variationally
coherent optimization problems. In Advances in Neural Information Processing Systems  pages 7043–7052 
2017.

[46] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In International

Conference on Machine Learning  pages 928–936  2003.

11

,Yunwen Lei
Peng Yang
Ke Tang
Ding-Xuan Zhou