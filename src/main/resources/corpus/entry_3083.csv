2018,Learning latent variable structured prediction models with Gaussian perturbations,The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs. The large-margin formulation including latent variables not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution  with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally  as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally  we provide a formulation using random samples and relaxations that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy  which enables a polynomial time evaluation of the objective function. We illustrate the method with synthetic experiments and a computer vision application.,Learning latent variable structured prediction models

with Gaussian perturbations

Department of Computer Science

Department of Computer Science

Jean Honorio

Purdue University

West Lafayette  IN  USA
jhonorio@purdue.edu

Kevin Bello

Purdue University

West Lafayette  IN  USA
kbellome@purdue.edu

Abstract

The standard margin-based structured prediction commonly uses a maximum loss
over all possible structured outputs [26  1  5  25]. The large-margin formulation
including latent variables [30  21] not only results in a non-convex formulation but
also increases the search space by a factor of the size of the latent space. Recent
work [11] has proposed the use of the maximum loss over random structured
outputs sampled independently from some proposal distribution  with theoretical
guarantees. We extend this work by including latent variables. We study a new
family of loss functions under Gaussian perturbations and analyze the effect of
the latent space on the generalization bounds. We show that the non-convexity
of learning with latent variables originates naturally  as it relates to a tight upper
bound of the Gibbs decoder distortion with respect to the latent space. Finally  we
provide a formulation using random samples and relaxations that produces a tighter
upper bound of the Gibbs decoder distortion up to a statistical accuracy  which
enables a polynomial time evaluation of the objective function. We illustrate the
method with synthetic experiments and a computer vision application.

1

Introduction

Structured prediction is of high interest in many domains such as computer vision [19]  natural
language processing [32  33]  and computational biology [14]. Some standard methods for structured
prediction are conditional random ﬁelds (CRFs) [13] and structured SVMs (SSVMs) [25  26].
In many tasks it is crucial to take into account latent variables. For example  in machine translation 
one is usually given a sentence x and its translation y  but not the linguistic structure h that connects
them (e.g. alignments between words). Even if h is not observable it is important to include this
information in the model in order to obtain better prediction results. Examples also arise in computer
vision  for instance  most images in indoor scene understanding [28] are cluttered by furniture and
decorations  whose appearances vary drastically across scenes  and can hardly be modeled (or even
hand-labeled) consistently. In this application  the input x is an image  the structured output y is the
layout of the faces (ﬂoor  ceiling  walls) and furniture  while the latent structure h assigns a binary
label to each pixel (clutter or non-clutter.)
During past years  there has been several solutions to address the problem of latent variables in
structured prediction. In the ﬁeld of computer vision  hidden conditional random ﬁelds (HCRF)
[23  29  22] have been widely applied for object recognition and gesture detection. In natural language
processing there is also work in applying discriminative probabilistic latent variable models  for
example the training of probabilistic context free grammars with latent annotations in a discriminative
manner [20]. The work of Yu and Joachims [30] extends the margin re-scaling SSVM in [26] by
introducing latent variables (LSSVM) and obtains a formulation that is optimized using the concave-

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

convex procedure (CCCP) [31]. The work of Ping et al. [21] considers a smooth objective in LSSVM
by incorporating marginal maximum a posteriori inference that “averages” over the latent space.
Some of the few works in deriving generalization bounds for structured prediction include the work of
McAllester [16]  which provides PAC-Bayesian guarantees for arbitrary losses  and the work of Cortes
et al. [7]  which provides data-dependent margin guarantees for a general family of hypotheses  with
an arbitrary factor graph decomposition. However  with the exception of [11]  both aforementioned
works do not focus on producing computationally appealing methods. Moreover  prior generalization
bounds have not focused on latent variables.

Contributions. We focus on the learning aspects of structured prediction problems using latent
variables. We ﬁrst extend the work of [16] by including latent variables  and show that the non-convex
formulation using the slack re-scaling approach with latent variables is related to a tight upper bound
of the Gibbs decoder distortion. This motivates the apparent need of the non-convexity in different
formulations using latent variables (e.g.  [30  10]). Second  we provide a tighter upper bound of
the Gibbs decoder distortion by randomizing the search space of the optimization problem. That is 
instead of having a formulation over all possible structures and latent variables (usually exponential in
size)  we propose a formulation that uses i.i.d. samples coming from some proposal distribution. This
approach is also computationally appealing in cases where the margin can be computed in poly-time
(for example  when the latent space is polynomial in size or when a relaxation in the maximization
over the latent space can be computed in poly-time)  since it would lead to a fully polynomial time
evaluation of the formulation. The use of standard Rademacher arguments and the analysis of [11]
would lead to a prohibitive upper bound that is proportional to the size of the latent space. We
provide a way to obtain an upper bound that is logarithmic in the size of the latent space. Finally  we
provide experimental results in synthetic data and in a computer vision application  where we obtain
improvements in the average test error with respect to the values reported in [9].

2 Background
We denote the input space as X   the output space as Y  and the latent space as H. We assume a
distribution D over the observable space X × Y. We further assume that we are given a training set
S of n i.i.d. samples drawn from the distribution D  i.e.  S ∼ Dn.
Let Yx (cid:54)= ∅ denote the countable set of feasible outputs or decodings of x. In general  |Yx| is
exponential with respect to the input size. Likewise  let Hx (cid:54)= ∅ denote the countable set of feasible
latent decodings of x.
We consider a ﬁxed mapping Φ from triples to feature vectors to describe the relation among
input x  output y  and latent variable h  i.e.  for any triple (x  y  h)  we have the feature vector
Φ(x  y  h) ∈ Rk \ {0}. For a parameter w ∈ W ⊆ Rk \ {0}  we consider linear decoders of the
form:

fw(x) = argmax

(y h)∈Yx×Hx

Φ(x  y  h) · w.

(1)

(2)

The problem of computing this argmax is typically referred as the inference or prediction problem.
In practice  very few cases of the above general inference problem are tractable  while most are
NP-hard and also hard to approximate within a ﬁxed factor. (For instance  see Section 6.1 in [11] for
a thorough discussion.)
We denote by d : Y ×Y ×H → [0  1] the distortion function  which measures the dissimilarity among
two elements of the output space Y and one element of the latent space H. (Note that the distortion
function is general in the sense that the latent element may not be used in some applications.)
Therefore  the goal is to ﬁnd a w ∈ W that minimizes the decoder distortion  that is:

(cid:2)d(y (cid:104)fw(x)(cid:105))(cid:3) .

min
w∈W

E

(x y)∼D

In the above equation  the angle brackets indicate that we are inserting a pair (ˆy  ˆh) = fw(x) into
the distortion function. From the computational point of view  the above optimization problem is
intractable since d(y (cid:104)fw(x)(cid:105)) is discontinuous with respect to w. From the statistical viewpoint 
eq.(2) requires access to the data distribution D and would require an inﬁnite amount of data. In
practice  one only has access to a ﬁnite number of samples.

2

Furthermore  even if one were able to compute w using the objective in eq.(2)  this parameter w 
while achieving low distortion  could potentially be in a neighborhood of parameters with high
distortion. Therefore  we can optimize a more robust objective that takes into account perturbations.
In this paper we consider Gaussian perturbations. More formally  let α > 0 and let Q(w) be a
unit-variance Gaussian distribution centered at αw of parameters w(cid:48) ∈ W. The Gibbs decoder
distortion of the perturbation distribution Q(w) and data distribution D  is deﬁned as:

(cid:34)

(cid:2)d(y (cid:104)fw(cid:48)(x)(cid:105))(cid:3)(cid:35)

L(Q(w)  D) = E

(x y)∼D

E

w(cid:48)∼Q(w)

(3)

Then  the optimization problem using the Gibbs decoder distortion can be written as:

min
w∈W

L(Q(w)  D).

We deﬁne the margin m(x  y  y(cid:48)  h(cid:48)  w) as follows:

m(x  y  y(cid:48)  h(cid:48)  w) = max
h∈Hx

Φ(x  y  h) · w − Φ(x  y(cid:48)  h(cid:48)) · w.

Note that since we are considering latent variables  our deﬁnition of margin differs from [16  11].
Let h∗ = argmaxh∈Hx Φ(x  y  h) · w. In this case h∗ can be interpreted as the latent variable that
best explains the pair (x  y). Then  for a ﬁxed w  the margin computes the amount by which the pair
(y  h∗) is preferred to the pair (y(cid:48)  h(cid:48)).
Next we introduce the concept of “parts”  also used in [16]. Let c(p  x  y  h) be a nonnegative integer
that gives the number of times that the part p ∈ P appears in the triple (x  y  h). For a part p ∈ P 
we deﬁne the feature p as follows:

Φp(x  y  h) ≡ c(p  x  y  h)

We let Px (cid:54)= ∅ denote the set of p ∈ P such that there exists (y  h) ∈ Yx × Hx with c(p  x  y  h) > 0.

Structural SVMs with latent variables.
[30] extend the formulation of margin re-scaling given
in [26] incorporating latent variables. The motivation to extend such formulation is that it leads
to a difference of two convex functions  which allows the use of CCCP [31]. The aforementioned
formulation is:
(cid:107)w(cid:107)2

[Φ(x  ˆy  ˆh) · w + d(y  ˆy  ˆh)] − C

Φ(x  y  h) · w (4)

(cid:88)

(cid:88)

2 + C

min
w

1
2

max

(ˆy ˆh)∈Yx×Hx

(x y)∈S

max
h∈Hx

(x y)∈S

In the case of standard SSVMs (without latent variables)  [26] discuss two advantages of the slack re-
scaling formulation over the margin re-scaling formulation  these are: the slack re-scaling formulation
is invariant to the scaling of the distortion function  and the margin re-scaling potentially gives
signiﬁcant score to structures that are not even close to being confusable with the target structures.
[1  6  25] proposed similar formulations to the slack re-scaling formulation. Despite its theoretical
advantages  the slack re-scaling has been less popular than the margin re-scaling approach due to
computational requirements. In particular  both formulations require optimizing over the output
space  but while margin re-scaling preserves the structure of the score and error functions  the slack
re-scaling does not. This results in harder inference problems during training. [11] also analyze the
slack re-scaling approach and theoretically show that using random structures one can obtain a tighter
upper bound of the Gibbs decoder distortion. However  these works do not take into account latent
variables.
The following formulation corresponds to the slack re-scaling approach with latent variables:

(cid:88)

min
w

1
n

max

(ˆy ˆh)∈Yx×Hx

(x y)∈S

(cid:104)

d(y  ˆy  ˆh) 1

m(x  y  ˆy  ˆh  w) ≤ 1

+ λ(cid:107)w(cid:107)2

2

(5)

(cid:105)

We take into account the loss of structures whose margin is less than one (i.e.  m(·) ≤ 1) instead of
the Hamming distance as done in [11]. This is because the former gave better results in preliminary
experiments. Also  it is more related to current practice (e.g.  [30]). In order to obtain an SSVM-like
formulation  the hinge loss is used instead of the discontinuous 0/1 loss in the above formulation.
Note however  that both eq.(4) and eq.(5) are now non-convex problems with respect to the learning
parameter w even if the hinge loss is used.

3

3 The maximum loss over all structured outputs and latent variables

decoder distortion (eq.(3)) up to an statistical accuracy of O((cid:112)log n/n) for n training samples.

In this section we extend the work of McAllester [16] by including latent variables. In the following
theorem  we show that the slack re-scaling objective function (eq.(5)) is an upper bound of the Gibbs
Theorem 1. Assume that there exists a ﬁnite integer value r such that |Yx×Hx| ≤ r for all (x  y) ∈ S.
Assume also that (cid:107)Φ(x  y  h)(cid:107)2 ≤ γ for any triple (x  y  h). Fix δ ∈ (0  1). With probability at least
1 − δ/2 over the choice of n training samples  simultaneously for all parameters w ∈ W and
unit-variance Gaussian perturbation distributions Q(w) centered at wγ
2)  we
have:

8 log (rn/(cid:107)w(cid:107)2

(cid:113)

(cid:88)
(cid:115)

(x y)∈S

L(Q(w)  D) ≤ 1
n

+

max

d(y  ˆy  ˆh) 1

m(x  y  ˆy  ˆh  w) ≤ 1

+

4(cid:107)w(cid:107)2

(ˆy ˆh)∈Yx×Hx
2 γ2 log (rn/(cid:107)w(cid:107)2
2(n − 1)

2) + log (2n/δ)

(cid:104)

(cid:105)

(cid:107)w(cid:107)2
n

2

(cid:105)

(See Appendix A for detailed proofs.)
For the proof of the above we used the PAC-Bayes theorem and well-known Gaussian concentration
inequalities. Note that the average sum in the right-hand side  i.e.  the objective function  can be
equivalently written as:

(cid:88)

1
n

max

(ˆy ˆh)∈Yx×Hx

min
h∈Hx

(x y)∈S

(cid:104)

d(y  ˆy  ˆh) 1

Φ(x  y  h) · w − Φ(x  ˆy  ˆh) · w ≤ 1

.

Remark 1. It is clear that the above formulation is tight with respect to the latent space Hx due
to the minimization. This is an interesting observation because it reinforces the idea that a non-
convex formulation is required in models using latent variables  i.e.  an attempt to “convexify” the
formulation will result in looser upper bounds and consequently might produce worse predictions.
Some other examples of non-convex formulations for latent-variable models are found in [30  10].
Note also that the upper bound has a maximization over Yx × Hx (usually exponential in size) and a
minimization over Hx (potentially in exponential size). We state two important observations in the
following remark.
Remark 2. First  in the minimization  it is clear that the use of a subset of Hx would lead to a looser

relaxation not only can tighten the bound but also can allow the margin to be computed in polynomial
time. See for instance some analyses of LP-relaxations in [12  15  17]. Second  in contrast  using a
subset of Yx × Hx in the maximization would lead to a tighter upper bound.

upper bound. However  using a superset (cid:101)Hx ⊇ Hx would lead to a tighter upper bound. The latter
From the ﬁrst observation above  we will now introduce a new deﬁnition of margin  (cid:101)m  which
performs a maximization over a superset (cid:101)Hx ⊇ Hx.
(cid:101)m for (cid:101)H being a set of binary strings. That is  we can encode any DAG (in H) as a binary string (in
(cid:101)H)  but not all binary strings are DAGs. Later  in Section 6  we provide an empirical comparison of
the use of m and (cid:101)m. We next present a similar upper bound to the one obtained in Theorem 1 but
now using the margin (cid:101)m.
(cid:113)

Theorem 2 (Relaxed margin bound.). Assume that there exists a ﬁnite integer value r such that
|Yx × Hx| ≤ r for all (x  y) ∈ S. Assume also that (cid:107)Φ(x  y  h)(cid:107)2 ≤ γ for any triple (x  y  h). Fix
δ ∈ (0  1). With probability at least 1 − δ/2 over the choice of n training samples  simultaneously
for all parameters w ∈ W and unit-variance Gaussian perturbation distributions Q(w) centered at
wγ

Several examples are NP-hard m for H (DAGs  trees or cardinality constrained sets)  but poly-time

(cid:101)m(x  y  y(cid:48)  h(cid:48)  w) = max
h∈(cid:101)Hx

Φ(x  y  h) · w − Φ(x  y(cid:48)  h(cid:48)) · w.

8 log (rn/(cid:107)w(cid:107)2

2)  we have:

(cid:88)

L(Q(w)  D) ≤ 1
n

max

(ˆy ˆh)∈Yx×Hx

(x y)∈S

d(y  ˆy  ˆh) 1

(cid:104)(cid:101)m(x  y  ˆy  ˆh  w) ≤ 1

(cid:105)

(cid:107)w(cid:107)2
n

2

+

4

(cid:115)

+

4(cid:107)w(cid:107)2

2 γ2 log (rn/(cid:107)w(cid:107)2
2(n − 1)

2) + log (2n/δ)

From the second observation in Remark 2  it is natural to ask what elements should constitute this
subset in order to control the statistical accuracy with respect to the Gibbs decoder. Finally  if the
number of elements is polynomial then we also have an efﬁcient computation of the maximum. We
provide answers to these questions in the next section.

4 The maximum loss over random structured outputs and latent variables

In this section  we show the relation between PAC-Bayes bounds and the maximum loss over random
structured outputs and latent variables sampled i.i.d. from some proposal distribution.

Instead of using a maximization over Yx × Hx  we will perform a
A more efﬁcient evaluation.
maximization over a set T (w  x) of random elements sampled i.i.d. from some proposal distribution
R(w  x) with support on Yx × Hx. More explicitly  our new formulation is:

(cid:104)(cid:101)m(x  y  ˆy  ˆh  w) ≤ 1

(cid:105)

+ λ(cid:107)w(cid:107)2
2.

(6)

max

(ˆy ˆh)∈T (w x)

d(y  ˆy  ˆh) 1

(cid:88)

(x y)∈S

min
w

1
n

We make use of the following two assumptions in order for |T (w  x)| to be polynomial  even when
|Yx × Hx| is exponential with respect to the input size.
Assumption A (Maximal distortion  [11]). The proposal distribution R(w  x) fulﬁlls the following
condition. There exists a value β ∈ [0  1) such that for all (x  y) ∈ S and w ∈ W:

P

(y(cid:48) h(cid:48))∼R(w x)

[d(y  y(cid:48)  h(cid:48)) = 1] ≥ 1 − β

Assumption B (Low norm). The proposal distribution R(w  x) fulﬁlls the condition for all
(x  y) ∈ S and w ∈ W:1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

E

(y(cid:48) h(cid:48))∼R(w x)

(cid:2)Φ(x  y  h∗) − Φ(x  y(cid:48)  h(cid:48))(cid:3)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

√
≤ 1
2

n

≤ 1

2(cid:107)w(cid:107)2

 

where h∗ = argmaxh∈Hx Φ(x  y  h) · w.
In Section 5 we provide examples for Assumptions A and B which allow us to obtain |T (w  x)| =
. Note that β plays an important role in the number of samples that we

O(cid:16)

(cid:17)

log(1/(β+e

1
−1/(γ2(cid:107)w(cid:107)2

2)))

need to draw from the proposal distribution R(w  x).

Statistical analysis.
In this approach  randomness comes from two sources  from the training data
S and the random set T (w  x). That is  in Theorem 1  randomness only stems from the training set S.
Now we need to produce generalization results that hold for all the sets T (w  x)  and for all possible
proposal distributions R(w  x). The following assumption will allow us to upper-bound the number
of possible proposal distributions R(w  x).
Assumption C (Linearly inducible ordering  [11]). The proposal distribution R(w  x) depends
solely on the linear ordering induced by the parameter w ∈ W and the mapping Φ(x · ·). More
formally  let r(x) ≡ |Yx × Hx| and thus Yx × Hx ≡ {(y1  h1) . . . (yr(x)  hr(x))}. Let w  w(cid:48) ∈ W
be any two arbitrary parameters. Let π(x) = (π1 . . . πr(x)) be a permutation of {1 . . . r(x)} such that
Φ(x  yπ1  hπ1) · w < ··· < Φ(x  yπr(x)  hπr(x)) · w. Let π(cid:48)(x) = (π(cid:48)
1 . . . π(cid:48)
r(x)) be a permutation of
) · w(cid:48). For all w  w(cid:48) ∈ W and
{1 . . . r(x)} such that Φ(x  yπ(cid:48)
  hπ(cid:48)
distribution fulﬁlls R(π(x)  x) ≡ R(w  x).

x ∈ X   if π(x) = π(cid:48)(x) then KL(cid:0)R(w  x)(cid:13)(cid:13)R(w(cid:48)  x)(cid:1) = 0. In this case  we say that the proposal

) · w(cid:48) < ··· < Φ(x  yπ(cid:48)

  hπ(cid:48)

r(x)

r(x)

1

1

1The second inequality follows from an implicit assumption made in Theorem 1  i.e. (cid:107)w(cid:107)2

2 /n ≤ 1 since the

distortion function d is at most 1.

5

In Assumption C  geometrically speaking  for a ﬁxed x we ﬁrst project the feature vectors Φ(x  y  h)
of all (y  h) ∈ Yx × Hx onto the lines w and w(cid:48). Let π(x) and π(cid:48)(x) be the resulting ordering of
the structured outputs after projecting them onto w and w(cid:48) respectively. Two proposal distributions
R(w  x) and R(w(cid:48)  x) are the same provided that π(x) = π(cid:48)(x). That is  the speciﬁc values of
Φ(x  y  h) · w and Φ(x  y  h) · w(cid:48) are irrelevant  and only their ordering matters.
In Section 5 we show an example that fulﬁlls Assumption C  which corresponds to a generalization
of Algorithm 2 proposed in [11] for any structure with computationally efﬁcient local changes.
In the following theorem  we show that our new formulation in eq.(6) is related to an upper bound of
√
the Gibbs decoder distortion up to statistical accuracy of O(log2 n/
and |(cid:101)Hx| ≤ ˜r for all (x  y) ∈ S  | ∪(x y)∈S Px| ≤ (cid:96)  and (cid:107)Φ(x  y  h)(cid:107)2 ≤ γ for any triple (x  y  h).
Theorem 3. Assume that there exist ﬁnite integer values r  ˜r  (cid:96)  and γ such that |Yx × Hx| ≤ r
Assume that the proposal distribution R(w  x) with support on Yx × Hx fulﬁlls Assumption A with
(cid:112)(cid:96)(r + 1) + 1. With probability at least 1 − δ over the
value β  as well as Assumptions B and C. Assume that (cid:107)w(cid:107)2
128γ2 log(1/(1−β)) . Fix δ ∈ (0  1)
and an integer s such that 3 ≤ 2s + 1 ≤ 9
choice of both n training samples and n sets of random structured outputs and latent variables 
simultaneously for all parameters w ∈ W with (cid:107)w(cid:107)0 ≤ s  unit-variance Gaussian perturbation
distributions Q(w) centered at wγ
2)  and for sets of random structured outputs
T (w  x) sampled i.i.d. from the proposal distribution R(w  x) for each training sample (x  y) ∈ S 
such that |T (w  x)| =

n) for n training samples.

8 log (rn/(cid:107)w(cid:107)2

(cid:113)

2 ≤

20

1

log(1/(β+e

log n
−1/(128γ2(cid:107)w(cid:107)2

2)))

L(Q(w)  D) ≤ 1
n

max

(ˆy ˆh)∈T (w x)

d(y  ˆy  ˆh) 1

  we have:

(cid:25)
(cid:104)(cid:101)m(x  y  ˆy  ˆh  w) ≤ 1
(cid:114) 1

(cid:114)

(cid:105)

+

+ 3

n

(cid:107)w(cid:107)2
n

2

+

n

1
2

(cid:24)
(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 4(cid:107)w(cid:107)2

(x y)∈S

+

+

2 γ2 log rn(cid:107)w(cid:107)2
2(n − 1)

2

+ log 2n
δ

(cid:115)

s(log (cid:96) + 2 log (nr)) + log (4/δ)

1

log(1/(β + e−1/(128γ2(cid:107)w(cid:107)2

2 )))

(2s + 1) log((cid:96)(n˜r + 1) + 1) log3(n + 1)

n

The proof of the above is based on Theorem 2 as a starting point. In order to account for the
computational aspect of requiring sets T (w  x) of polynomial size  we use Assumptions A and B
for bounding a deterministic expectation. In order to account for the statistical aspects  we use
Assumption C and Rademacher complexity arguments for bounding a stochastic quantity for all sets
T (w  x) of random structured outputs and latent variables  and all possible proposal distributions
R(w  x).
Remark 3. A straightforward application of Rademacher complexity in the analysis of [11] leads
to a bound of O(|Hx|/
n). Technically speaking  a classical Rademacher complexity states that:
let F and G be two hypothesis classes. Let min(F G) = {min(f  g)|f ∈ F  g ∈ G}. Then
√
R(min(F G)) ≤ R(F) + R(G). If we apply this  then Theorem 3 would contain an O(|Hx|/
√
n)
term  or equivalently O(r/
n). This would be prohibitive since r is typically exponential size  and
√
one would require a very large number of samples n in order to have a useful bound  i.e.  to make
O(r/

n) close to zero. In the proof we provide a way to tighten the bound to O((cid:112)log |Hx|/n).

√

5 Examples

Here we provide several examples that fulﬁll the three main assumptions of our theoretical result.

Examples for Assumption A. First we argue that we can perform a change of measure between
different proposal distributions. This allows us to focus on uniform proposals afterwards.
Claim i (Change of measure). Let R(w  x) and R(cid:48)(w  x) two proposal distributions  both with
support on Yx × Hx. Assume that R(w  x) fulﬁlls Assumption A with value β1. Let rw x(·) and

6

w x(·) be the probability mass functions of R(w  x) and R(cid:48)(w  x) respectively. Assume that the
r(cid:48)
total variation distance between R(w  x) and R(cid:48)(w  x) fulﬁlls for all (x  y) ∈ S and w ∈ W:

(cid:88)

(y h)

T V (R(w  x)(cid:107)R(cid:48)(w  x)) ≡ 1
2

|rw x(y  h) − r(cid:48)

w x(y  h)| ≤ β2

Then R(cid:48)(w  x) fulﬁlls Assumption A with β = β1 + β2 provided that β1 + β2 ∈ [0  1).
Next  we present a new result for permutations and for a distortion that returns the number of different
positions. We later use this result for an image matching application in the experiments section.
Claim ii (Permutations). Let Yx be the set of all permutations of v elements  such that v > 1. Let yi
be the i-th element in the permutation y. Let d(y  y(cid:48)  h) = 1
distribution R(w  x) = R(x) with support on Yx × Hx fulﬁlls Assumption A with β = 2/3.
The authors in [11] present several examples of distortion functions of the form d(y  y(cid:48))  for directed
spanning trees  directed acyclic graphs and cardinality-constrained sets  and a distortion function
that returns the number of different edges/elements; as well as for any type of structured output
and binary distortion functions. For our setting we can make use of these examples by deﬁning
d(y  y(cid:48)  h) = d(y  y(cid:48)). Note that even if we ignore the latent variable in the distortion function  we
still use the latent variables in the feature vectors Φ(x  y  h) and thus in the calculation of the margin.

(cid:3). The uniform proposal

1(cid:2)yi (cid:54)= y(cid:48)

(cid:80)v

i=1

v

i

x  where the partition Υp

Examples for Assumption B. The claim below is for a particular instance of a sparse mapping
and a uniform proposal distribution.
Claim iii (Sparse mapping). Let b > 0 be an arbitrary integer value. For all (x  y) ∈ S with
h∗ = argmaxh∈Hx Φ(x  y  h) · w  let Υx = ∪p∈Px Υp
x is deﬁned as follows
for all p ∈ Px:
x ≡ {(y(cid:48)  h(cid:48)) | |Φp(x  y  h∗) − Φp(x  y(cid:48)  h(cid:48))| ≤ b and (∀q (cid:54)= p) Φq(x  y  h∗) = Φq(x  y(cid:48)  h(cid:48))}
Υp
If n ≤ |Px|/(4b2) for all (x  y) ∈ S  then the uniform proposal distribution R(w  x) = R(x) with
support on Yx × Hx fulﬁlls Assumption B.
The claim below is for a particular instance of a dense mapping and an arbitrary proposal distribution.
Claim iv
Let
|Φp(x  y  h∗) − Φp(x  y(cid:48)  h(cid:48))| ≤ b|Px| for all (x  y) ∈ S with h∗ = argmaxh∈Hx Φ(x  y  h) · w 
(y(cid:48)  h(cid:48)) ∈ Yx × Hx and p ∈ Px. If n ≤ |Px|/(4b2) for all (x  y) ∈ S  then any arbitrary proposal
distribution R(w  x) fulﬁlls Assumption B.

(Dense mapping). Let

arbitrary

integer

value.

b > 0

be

an

Examples for Assumption C.
In the case of modeling without latent variables  [32  33] presented
an algorithm for directed spanning trees in the context of dependency parsing in natural language
processing. Later  [11] extended the previous algorithm to any structure with computationally efﬁcient
local changes  which includes directed acyclic graphs (traversed in post-order) and cardinality-
constrained sets. Next  we generalize Algorithm 2 in [11] by including latent variables.

Algorithm 1 Procedure for sampling a structured output (y(cid:48)  h(cid:48)) ∈ Yx × Hx from a greedy local proposal
distribution R(w  x)
1: Input: parameter w ∈ W  observed input x ∈ X
2: Draw uniformly at random a structured output (ˆy  ˆh) ∈ Yx × Hx
3: repeat
4: Make a local change to (ˆy  ˆh) in order to increase Φ(x  ˆy  ˆh) · w
5: until no reﬁnement in last iteration
6: Output: structured output and latent variable (y(cid:48)  h(cid:48)) ← (ˆy  ˆh)

The above algorithm has the following property:
Claim v (Sampling for any type of structured output and latent variable). Algorithm 1 fulﬁlls
Assumption C.

7

Table 1: Average over 30 repetitions  and standard error at 95% conﬁdence level. All (LSSVM) indicates the use
of exact learning and exact inference. Rand and Rand/All indicate use of random learning  and random and exact

inference respectively. (S) indicates the use of superset (cid:101)H in the calculation of the margin. Rand/All obtains a

similar or sightly better test performance than All in the different study cases. Note that the runtime for learning
using the randomized approach is much less than exact learning  while still having a good test performance.

Problem
Directed
spanning
trees

Directed
acyclic
graphs

Method
All (LSSVM)
Rand (S)
Rand/All (S)
Rand
Rand/All
All (LSSVM)
Rand (S)
Rand/All (S)
Rand
Rand/All

Cardinality All (LSSVM)
constrained Rand (S)
sets

Rand/All (S)
Rand
Rand/All

Training runtime

1000 ± 15
44 ± 1
126 ± 5
1000 ± 21
63 ± 0
353 ± 5
1000 ± 5
75 ± 0
182 ± 3

Training distortion Test runtime Test distortion
8.2% ± 1.3%
22% ± 1.9%
8.2% ± 1.3%
24% ± 3.2%
8.2% ± 1.4%
21% ± 2.4%
28% ± 1.9%
20% ± 1.9%
25% ± 1.4%
19% ± 1.6%
6% ± 1.2%
18% ± 1.8%
6% ± 1.3%
17% ± 1.2%
6% ± 2.2%

8.4% ± 1.4%
22% ± 2.2%
23% ± 3.0%
17% ± 1.7%
24% ± 1.5%
21% ± 1.1%
6.3% ± 1.0%
18% ± 1.8%
15% ± 3.2%

18.9 ± 0.1
0.92 ± 0
19 ± 0.1
3 ± 0.4
17 ± 0.8
19 ± 0.2
1.5 ± 0
19 ± 0.2
8 ± 1
15 ± 0.2
19.5 ± 0.1
1.7 ± 0
19.5 ± 0.1
3.1 ± 1
19.4 ± 0.1

6 Experiments

In this section we illustrate the use of our approach by using the formulation in eq.(6). The goal of
the synthetic experiments is to show the improvement in prediction results and runtime of our method.
While the goal of the real-world experiment is to show the usability of our method in practice.

Synthetic experiments. We present experimental results for directed spanning trees  directed
acyclic graphs and cardinality-constrained sets. We performed 30 repetitions of the following
procedure. We generated a ground truth parameter w∗ with independent zero-mean and unit-
variance Gaussian entries. Then  we generated a training set of n = 100 samples. Our map-
ping Φ(x  y  h) is as follows. For every pair of possible edges/elements i and j  we deﬁne
In order to generate each training sample
(x  y) ∈ S  we generated a random vector x with independent Bernoulli entries  each with equal
probability of being 1 or 0. The latent space H is the set of binary strings with two entries being 1 
where these two entries share a common edge or element  i.e.  hij = hik = 1 ∀ i  j  k. To the best
of our knowledge there is no efﬁcient way to exactly compute the maximization in the margin m

Φij(x  y  h) = 1(cid:2)(hij xor xij) and i ∈ y and j ∈ y(cid:3).
under this latent space. Thus  we deﬁne (cid:101)H (relaxed set) as the set of all binary strings with exactly

two entries being 1. We then can efﬁciently compute the margin ˜m by a greedy approach since our
feature vector is constructed using linear operators. After generating x  we set (y  h) = fw∗ (x). That
is  we solved eq.(1) in order to produce the structured output y  and disregard h. (More details of the
experiment in Appendix B.3.)
We compared three training methods: the maximum loss over all possible structured outputs and
latent variables with slack re-scaling as in eq.(5). We also evaluated the maximum loss over random
structured outputs and latent variables  using the original latent space  as well as  the superset
relaxation as in eq.(6). We considered directed spanning trees of 4 nodes  directed acyclic graphs of 4
nodes and 2 parents per node  and sets of 3 elements chosen from 9 possible elements. After training 
for inference on an independent test set  we used eq.(1) for the maximum loss over all possible
structured outputs and latent variables. For the maximum loss over random structured outputs and
latent variables  we use the following approximate inference approach:

Φ(x  y  h) · w.

(7)

(cid:101)fw(x) ≡ argmax

(y h)∈T (w x)

Note that we used small structures and latent spaces in order to compare to exact learning  i.e.  going
through all possible structures as in eq.(5) and eq.(4). Bigger structures would result in exponential
number of structures  making exact methods intractable to compare against our method. For purposes

8

of testing  we tried cardinality constrained sets of 4 elements out of 100 (note that in this case
|Y| ≈ 108  |H| ≈ 1016) and training only took 11 minutes under our approach.
Table 1 shows the runtime  the training distortion as well as the test distortion in an independently
generated set of 100 samples. In the different study cases  the maximum loss over random structured
outputs and latent variables obtains similar test performance than the maximum loss over all possible
structured outputs and latent variables. However  note that our method is considerable faster.

Image matching. We illustrate our approach for image matching on video frames from the Buffy
Stickmen dataset (http://www.robots.ox.ac.uk/~vgg/data/stickmen/). The goal of the
experiment is to match the keypoints representing different body parts  between two images. Each
frame contains 18 keypoints representing different parts of the body. From a total of 187 image pairs
(from different episodes and people)  we randomly selected 120 pairs for training and the remaining
67 pairs for testing. We performed 30 repetitions. Ground truth keypoint matching is provided in the
dataset.
Following [9  27]  we represent the matching as a permutation of keypoints. Let x = (I  I(cid:48)) be
a pair of images  and let y be a permutation of {1 . . . 18}. We model the latent variable h as a
R2×2 matrix representing an afﬁne transformation of a keypoint  where h11  h22 ∈ {0.8  1  1.2}  and
h12  h21 ∈ {−0.2  0  0.2}. Our mapping Φ(x  y  h) uses SIFT features  and the distance between
coordinates after using h. (Details in Appendix B.3.)
We used the distortion function and β = 2/3 as prescribed by Claim ii. After learning  for a given
x from the test set  we performed 100 iterations of random inference as in eq.(7). We obtained an
average error of 0.3878 (6.98 incorrectly matched keypoints) in the test set  which is an improvement
to the values of 8.47 for maximum-a-posteriori perturbations and 8.69 for max-margin  as reported in
[9]. Finally  we show an example from the test set in Figure 1.

Figure 1: Image matching on the Buffy Stickmen dataset  predicted by our randomized approach with latent
variables. The problem is challenging since the dataset contains different episodes and people.

7 Future directions

The randomization of the latent space in the calculation of the margin is of high interest. Despite
leading to a looser upper bound of the Gibbs decoder distortion  if one could control the statistical
accuracy under this approach then one could obtain a fully polynomial-time evaluation of the objective
function  even if |H| is exponential. Therefore  whether this method is feasible  and under what
technical conditions  are potential future work. The analysis of other non-Gaussian perturbation
models from the computational and statistical viewpoints is also of interest. Finally  it would be
interesting to analyze approximate inference for prediction on an independent test set.

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No.
1716609-IIS.

References
[1] Altun  Y. and Hofmann  T. [2003]  ‘Large margin methods for label sequence learning’  Euro-

pean Conference on Speech Communication and Technology pp. 145–152.

9

[2] Bennett  J. [1956]  ‘Determination of the number of independent parameters of a score matrix

from the examination of rank orders’  Psychometrika 21(4)  383–393.

[3] Bennett  J. and Hays  W. [1960]  ‘Multidimensional unfolding: Determining the dimensionality

of ranked preference data’  Psychometrika 25(1)  27–43.

[4] Choi  H.  Meshi  O. and Srebro  N. [2016]  Fast and scalable structural svm with slack rescaling 

in ‘Artiﬁcial Intelligence and Statistics’  pp. 667–675.

[5] Collins  M. [2004]  Parameter estimation for statistical parsing models: Theory and practice
of distribution-free methods  in ‘New Developments in Parsing Technology’  Vol. 23  Kluwer
Academic  pp. 19–55.

[6] Collins  M. and Roark  B. [2004]  ‘Incremental parsing with the perceptron algorithm’  Annual

Meeting of the Association for Computational Linguistics pp. 111–118.

[7] Cortes  C.  Kuznetsov  V.  Mohri  M. and Yang  S. [2016]  Structured prediction theory based
on factor graph complexity  in ‘Advances in Neural Information Processing Systems’  pp. 2514–
2522.

[8] Cover  T. [1967]  ‘The number of linearly inducible orderings of points in d-space’  SIAM

Journal on Applied Mathematics 15(2)  434–439.

[9] Gane  A.  Hazan  T. and Jaakkola  T. [2014]  Learning with maximum a-posteriori perturbation

models  in ‘Artiﬁcial Intelligence and Statistics’  pp. 247–256.

[10] Hinton  G. E. [2012]  A practical guide to training restricted boltzmann machines  in ‘Neural

networks: Tricks of the trade’  Springer  pp. 599–619.

[11] Honorio  J. and Jaakkola  T. [2016]  ‘Structured prediction: from gaussian perturbations to

linear-time principled algorithms’  UAI .

[12] Kulesza  A. and Pereira  F. [2007]  ‘Structured learning with approximate inference’  Neural

Information Processing Systems 20  785–792.

[13] Lafferty  J.  McCallum  A. and Pereira  F. C. [2001]  ‘Conditional random ﬁelds: Probabilistic

models for segmenting and labeling sequence data’.

[14] Li  M.-H.  Lin  L.  Wang  X.-L. and Liu  T. [2007]  ‘Protein–protein interaction site prediction

based on conditional random ﬁelds’  Bioinformatics 23(5)  597–604.

[15] London  B.  Meshi  O. and Weller  A. [2016]  ‘Bounding the integrality distance of lp relaxations

for structured prediction’  NIPS workshop on Optimization for Machine Learning .

[16] McAllester  D. [2007]  Generalization bounds and consistency  in ‘Predicting Structured Data’ 

MIT Press  pp. 247–261.

[17] Meshi  O.  Mahdavi  M.  Weller  A. and Sontag  D. [2016]  ‘Train and test tightness of lp

relaxations in structured prediction’  International Conference on Machine Learning .

[18] Neylon  T. [2006]  Sparse Solutions for Linear Prediction Problems  PhD thesis  New York

University.

[19] Nowozin  S.  Lampert  C. H. et al. [2011]  ‘Structured learning and prediction in computer

vision’  Foundations and Trends® in Computer Graphics and Vision 6(3–4)  185–365.

[20] Petrov  S. and Klein  D. [2008]  Discriminative log-linear grammars with latent variables  in

‘Advances in neural information processing systems’  pp. 1153–1160.

[21] Ping  W.  Liu  Q. and Ihler  A. [2014]  ‘Marginal structured SVM with hidden variables’ 

International Conference on Machine Learning pp. 190–198.

[22] Quattoni  A.  Collins  M. and Darrell  T. [2005]  Conditional random ﬁelds for object recognition 

in ‘Advances in neural information processing systems’  pp. 1097–1104.

10

[23] Quattoni  A.  Wang  S.  Morency  L.-P.  Collins  M. and Darrell  T. [2007]  ‘Hidden conditional

random ﬁelds’  IEEE transactions on pattern analysis and machine intelligence 29(10).

[24] Sarawagi  S. and Gupta  R. [2008]  Accurate max-margin training for structured output spaces 
in ‘Proceedings of the 25th international conference on Machine learning’  ACM  pp. 888–895.

[25] Taskar  B.  Guestrin  C. and Koller  D. [2003]  ‘Max-margin Markov networks’  Neural Infor-

mation Processing Systems 16  25–32.

[26] Tsochantaridis  I.  Joachims  T.  Hofmann  T. and Altun  Y. [2005]  ‘Large margin methods
for structured and interdependent output variables’  Journal of machine learning research
6(Sep)  1453–1484.

[27] Volkovs  M. and Zemel  R. S. [2012]  Efﬁcient sampling for bipartite matching problems  in

‘Advances in Neural Information Processing Systems’  pp. 1313–1321.

[28] Wang  H.  Gould  S. and Roller  D. [2013]  ‘Discriminative learning with latent variables for

cluttered indoor scene understanding’  Communications of the ACM 56(4)  92–99.

[29] Wang  S. B.  Quattoni  A.  Morency  L.-P.  Demirdjian  D. and Darrell  T. [2006]  Hidden
conditional random ﬁelds for gesture recognition  in ‘Computer Vision and Pattern Recognition 
2006 IEEE Computer Society Conference on’  Vol. 2  IEEE  pp. 1521–1527.

[30] Yu  C. and Joachims  T. [2009]  ‘Learning structural SVMs with latent variables’  International

Conference on Machine Learning pp. 1169–1176.

[31] Yuille  A. L. and Rangarajan  A. [2002]  The concave-convex procedure (cccp)  in ‘Advances in

neural information processing systems’  pp. 1033–1040.

[32] Zhang  Y.  Lei  T.  Barzilay  R. and Jaakkola  T. [2014]  ‘Greed is good if randomized: New in-
ference for dependency parsing’  Empirical Methods in Natural Language Processing pp. 1013–
1024.

[33] Zhang  Y.  Li  C.  Barzilay  R. and Darwish  K. [2015]  ‘Randomized greedy inference for
joint segmentation  POS tagging and dependency parsing’  North American Chapter of the
Association for Computational Linguistics pp. 42–52.

11

,Jacob Steinhardt
Percy Liang
Kevin Bello
Jean Honorio