2012,On Multilabel Classification and Ranking with Partial Feedback,We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods  and relies on upper-confidence bounds to trade-off exploration and exploitation.  We analyze this algorithm in a partial adversarial setting  where covariates can be adversarial  but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\log T)$ regret bounds  which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets  often obtaining comparable performance.,On Multilabel Classiﬁcation and Ranking with

Partial Feedback

Claudio Gentile

DiSTA  Universit`a dell’Insubria  Italy

claudio.gentile@uninsubria.it

Francesco Orabona
TTI Chicago  USA

francesco@orabona.com

Abstract

We present a novel multilabel/ranking algorithm working in partial information
settings. The algorithm is based on 2nd-order descent methods  and relies on
upper-conﬁdence bounds to trade-off exploration and exploitation. We analyze
this algorithm in a partial adversarial setting  where covariates can be adversarial 
but multilabel probabilities are ruled by (generalized) linear models. We show
O(T 1/2 log T ) regret bounds  which improve in several ways on the existing re-
sults. We test the effectiveness of our upper-conﬁdence scheme by contrasting
against full-information baselines on real-world multilabel datasets  often obtain-
ing comparable performance.

Introduction

1
Consider a book recommendation system. Given a customer’s proﬁle  the system recommends a few
possible books to the user by means of  e.g.  a limited number of banners placed at different positions
on a webpage. The system’s goal is to select books that the user likes and possibly purchases.
Typical feedback in such systems is the actual action of the user or  in particular  what books he has
bought/preferred  if any. The system cannot observe what would have been the user’s actions had
other books got recommended  or had the same book ads been placed in a different order within the
webpage. Such problems are collectively referred to as learning with partial feedback. As opposed
to the full information case  where the system (the learning algorithm) knows the outcome of each
possible response (e.g.  the user’s action for each and every possible book recommendation placed
in the largest banner ad)  in the partial feedback setting  the system only observes the response to
very limited options and  speciﬁcally  the option that was actually recommended. In this and many
other examples of this sort  it is reasonable to assume that recommended options are not given the
same treatment by the system  e.g.  large banners which are displayed on top of the page should
somehow be more committing as a recommendation than smaller ones placed elsewhere. Moreover 
it is often plausible to interpret the user feedback as a preference (if any) restricted to the displayed
alternatives.
We consider instantiations of this problem in the multilabel and learning-to-rank settings. Learning
proceeds in rounds  in each time step t the algorithm receives an instance xt and outputs an ordered
subset ˆYt of labels from a ﬁnite set of possible labels [K] = {1  2  . . .   K}. Restrictions might apply
to the size of ˆYt (due  e.g.  to the number of available slots in the webpage). The set ˆYt corresponds
to the aforementioned recommendations  and is intended to approximate the true set of preferences
associated with xt. However  the latter set is never observed. In its stead  the algorithm receives
Yt ∩ ˆYt  where Yt ⊆ [K] is a noisy version of the true set of user preferences on xt. When we are
restricted to | ˆYt| = 1 for all t  this becomes a multiclass classiﬁcation problem with bandit feedback
– see below.
Related work. This paper lies at the intersection between online learning with partial feedback and
multilabel classiﬁcation/ranking. Both ﬁelds include a substantial amount of work  so we can hardly
do it justice here. We outline some of the main contributions in the two ﬁelds  with an emphasis on
those we believe are the most related to this paper.

1

A well-known and standard tool of facing the problem of partial feedback in online learning is to
trade off exploration and exploitation through upper conﬁdence bounds [16]. In the so-called bandit
setting with contextual information (sometimes called bandits with side information or bandits with
covariates  e.g.  [3  4  5  7  15]  and references therein) an online algorithm receives at each time
step a context (typically  in the form of a feature vector x) and is compelled to select an action
(e.g.  a label)  whose goodness is quantiﬁed by a predeﬁned loss function. Full information about
the loss function is not available. The speciﬁcs of the interaction model determines which pieces
of loss will be observed by the algorithm  e.g.  the actual value of the loss on the chosen action 
some information on more proﬁtable directions on the action space  noisy versions thereof  etc. The
overall goal is to compete against classes of functions that map contexts to (expected) losses in a
regret sense  that is  to obtain sublinear cumulative regret bounds. For instance  [1  3  5  7] work in
a ﬁnite action space where the mappings context-to-loss for each action are linear (or generalized
linear  as in [7]) functions of the features. They all obtain T 1/2-like regret bounds  where T is
the time horizon. This is extended in [15]  where the loss function is modeled as a sample from
a Gaussian process over the joint context-action space. We are using a similar (generalized) linear
modeling here. Linear multiclass classiﬁcation problems with bandit feedback are considered in 
e.g.  [4  11  13]  where either T 2/3 or T 1/2 or even logarithmic regret bounds are proven  depending
on the noise model and the underlying loss functions.
All the above papers do not consider structured action spaces  where the learner is afforded to select
sets of actions  which is more suitable to multilabel and ranking problems. Along these lines are
the papers [10  14  19  20  22]. The general problem of online minimization of a submodular loss
function under both full and bandit information without covariates is considered in [10]  achieving a
regret T 2/3 in the bandit case. In [22] the problem of online learning of assignments is considered 
where an algorithm is requested to assign positions (e.g.  rankings) to sets of items (e.g.  ads) with
given constraints on the set of items that can be placed in each position. Their problem shares
similar motivations as ours but  again  the bandit version of their algorithm does not explicitly take
side information into account  and leads to a T 2/3 regret bound. In [14] the aim is to learn a suitable
ordering of the available actions. Among other things  the authors prove a T 1/2 regret bound in
the bandit setting with a multiplicative weight updating scheme. Yet  no contextual information is
incorporated. In [20] the ability of selecting sets of actions is motivated by a problem of diverse
retrieval in large document collections which are meant to live in a general metric space. The
generality of this approach does not lead to strong regret guarantees for speciﬁc (e.g.  smooth) loss
functions. [19] uses a simple linear model for the hidden utility function of users interacting with a
web system and providing partial feedback in any form that allows the system to make signiﬁcant
progress in learning this function. A regret bound of T 1/2 is again provided that depends on the
degree of informativeness of the feedback. It is experimentally argued that this feedback is typically
made available by a user that clicks on relevant URLs out of a list presented by a search engine.
Despite the neatness of the argument  no formal effort is put into relating this information to the
context information at hand or to the way data are generated. Finally  the recent paper [2] investigates
classes of graphical models for contextual bandit settings that afford richer interaction between
contexts and actions leading again to a T 2/3 regret bound.
The literature on multilabel learning and learning to rank is overwhelming. The wide attention this
literature attracts is often motivated by its web-search-engine or recommender-system applications 
and many of the papers are experimental in nature. Relevant references include [6  9  23]  along
with references therein. Moreover  when dealing with multilabel  the typical assumption is full
supervision  an important concern being modeling correlations among classes. In contrast to that 
the speciﬁc setting we are considering here need not face such a modeling. Other related references
are [8  12]  where learning is by pairs of examples. Yet  these approaches need i.i.d. assumptions on
the data  and typically deliver batch learning procedures. To summarize  whereas we are technically
close to [1  3  4  5  7  15]  from a motivational standpoint we are perhaps closest to [14  19  22].
Our results. We investigate the multilabel and learning-to-rank problems in a partial feedback
scenario with contextual information  where we assume a probabilistic linear model over the labels 
although the contexts can be chosen by an adaptive adversary. We consider two families of loss func-
tions  one is a cost-sensitive multilabel loss that generalizes the standard Hamming loss in several
respects  the other is a kind of (unnormalized) ranking loss. In both cases  the learning algorithm is
maintaining a (generalized) linear predictor for the probability that a given label occurs  the ranking
being produced by upper conﬁdence-corrected estimated probabilities. In such settings  we prove

2

T 1/2 log T cumulative regret bounds — these bounds are optimal  up to log factors  when the label
probabilities are fully linear in the contexts. A distinguishing feature of our user feedback model is
that  unlike previous papers (e.g.  [1  10  15  22])  we are not assuming the algorithm is observing a
noisy version of the risk function on the currently selected action. In fact  when a generalized linear
model is adopted  the mapping context-to-risk turns out to be nonconvex in the parameter space.
Furthermore  when operating on structured action spaces this more traditional form of bandit model
does not seem appropriate to capture the typical user preference feedback. Our approach is based on
having the loss decouple from the label generating model  the user feedback being a noisy version of
the gradient of a surrogate convex loss associated with the model itself. As a consequence  the algo-
rithm is not directly dealing with the original loss when making exploration. Though the emphasis is
on theoretical results  we also validate our algorithms on two real-world multilabel datasets w.r.t. a
number of loss functions  showing good comparative performance against simple multilabel/ranking
baselines that operate with full information.

2 Model and preliminaries
We consider a setting where the algorithm receives at time t the side information vector xt ∈ Rd 
is allowed to output at a (possibly ordered) subset ˆYt ⊆ [K] of the set of possible labels  then the
subset of labels Yt ⊆ [K] associated with xt is generated  and the algorithm gets as feedback ˆYt∩Yt.
The loss suffered by the algorithm may take into account several things: the distance between Yt
and ˆYt (both viewed as sets)  as well as the cost for playing ˆYt. The cost c( ˆYt) associated with
ˆYt might be given by the sum of costs suffered on each class i ∈ ˆYt  where we possibly take into
account the order in which i occurs within ˆYt (viewed as an ordered list of labels). Speciﬁcally 
given constant a ∈ [0  1] and costs c = {c(i  s)  i = 1  . . .   s  s ∈ [K]}  such that 1 ≥ c(1  s) ≥
c(2  s) ≥ . . . c(s  s) ≥ 0  for all s ∈ [K]  we consider the loss function

(cid:96)a c(Yt  ˆYt) = a|Yt \ ˆYt| + (1 − a)(cid:80)

i∈ ˆYt\Yt

c(ji | ˆYt|) 

i∈ ˆYt\Yt

(cid:80)

where ji is the position of class i in ˆYt  and c(ji ·) depends on ˆYt only through its size | ˆYt|. In the
above  the ﬁrst term accounts for the false negative mistakes  hence there is no speciﬁc ordering of
labels therein. The second term collects the loss contribution provided by all false positive classes 
taking into account through the costs c(ji | ˆYt|) the order in which labels occur in ˆYt. The constant
a serves as weighting the relative importance of false positive vs. false negative mistakes. As a
speciﬁc example  suppose that K = 10  the costs c(i  s) are given by c(i  s) = (s − i + 1)/s  i =
1  . . .   s  the algorithm plays ˆYt = (4  3  6)  but Yt is {1  3  8}. In this case  |Yt \ ˆYt| = 2  and
c(ji | ˆYt|) = 3/3 + 1/3  i.e.  the cost for mistakingly playing class 4 in the top slot of ˆYt is
more damaging than mistakingly playing class 6 in the third slot. In the special case when all costs
are unitary  there is no longer need to view ˆYt as an ordered collection  and the above loss reduces to
a standard Hamming-like loss between sets Yt and ˆYt  i.e.  a|Yt \ ˆYt| + (1− a)| ˆYt \ Yt|. Notice that
the partial feedback ˆYt ∩ Yt allows the algorithm to know which of the chosen classes in ˆYt are good
or bad (and to what extent  because of the selected ordering within ˆYt). Yet  the algorithm does not
observe the value of (cid:96)a c(Yt  ˆYt) because Yt \ ˆYt remains hidden.
Working with the above loss function makes the algorithm’s output ˆYt become a ranked list of
classes  where ranking is restricted to the deemed relevant classes only. In our setting  only a rele-
vance feedback among the selected classes is observed (the set Yt ∩ ˆYt)  but no supervised ranking
information (e.g.  in the form of pairwise preferences) is provided to the algorithm within this set.
Alternatively  we can think of a ranking framework where restrictions on the size of ˆYt are set by an
exogenous (and possibly time-varying) parameter of the problem  and the algorithm is required to
provide a ranking complying with these restrictions. More on the connection to the ranking setting
with partial feedback is in Section 4.
The problem arises as to which noise model we should adopt so as to encompass signiﬁ-
cant real-world settings while at the same time affording efﬁcient implementation of the result-
ing algorithms. For any subset Yt ⊆ [K]  we let (y1 t  . . .   yK t) ∈ {0  1}K be the cor-
i=1 yi t + (1 −
. Moreover  because the ﬁrst sum does not de-

responding indicator vector. Then it is easy to see that (cid:96)a c(Yt  ˆYt) = a(cid:80)K
a)(cid:80)

(cid:16)

c(ji | ˆYt|) −(cid:16) a

(cid:17)
1−a + c(ji | ˆYt|)

yi t

i∈ ˆYt

(cid:17)

3

pend on ˆYt  for the sake of optimizing over ˆYt we can equivalently deﬁne
1−a + c(ji | ˆYt|)

(cid:96)a c(Yt  ˆYt) = (1 − a)

(cid:16)

c(ji | ˆYt|) −(cid:16) a

(cid:17)

(cid:17)

.

yi t

(1)

Let Pt(·) be a shorthand for the conditional probability Pt(·| xt)  where the side information vec-
tor xt can in principle be generated by an adaptive adversary as a function of the past. Then
Pt(y1 t  . . .   yK t) = P(y1 t  . . .   yK t | xt)  where the marginals Pt(yi t = 1) satisfy1

(cid:88)

i∈ ˆYt

Pt(yi t = 1) =

g(−u(cid:62)
i xt) + g(−u(cid:62)

i xt)

(cid:17)

(cid:17)

pi t

 

(cid:88)

i∈ ˆYt

(cid:16)

c(ji | ˆYt|) −(cid:16) a

 

i xt)

g(u(cid:62)

i = 1  . . .   K 

(2)
for some K vectors u1  . . .   uK ∈ Rd and some (known) function g : D ⊆ R → R+. The
i x ∈ D for all i and all x ∈ Rd chosen by the adversary. We assume
model is well deﬁned if u(cid:62)
for the sake of simplicity that ||xt|| = 1 for all t. Notice that at this point the variables yi t need
not be conditionally independent. We are only deﬁnining a family of allowed joint distributions
Pt(y1 t  . . .   yK t) through the properties of their marginals Pt(yi t).
The function g above will be instantiated to the negative derivative of a suitable convex and nonin-
creasing loss function L which our algorithm will be based upon. For instance  if L is the square
loss L(∆) = (1− ∆)2/2  then g(∆) = 1− ∆  resulting in Pt(yi t = 1) = (1 + u(cid:62)
i xt)/2  under the
assumption D = [−1  1]. If L is the logistic loss L(∆) = ln(1 + e−∆)  then g(∆) = (e∆ + 1)−1 
and Pt(yi t = 1) = eu(cid:62)
Set for brevity ∆i t = u(cid:62)
expected loss of the algorithm playing ˆYt as

i xt/(eu(cid:62)
i xt. Taking into account (1)  this model allows us to write the (conditional)

i xt + 1)  with domain D = R.

g(−∆i t)

1−a + c(ji | ˆYt|)

t = argminY =(j1 j2 ... j|Y |)⊆[K]

Et[(cid:96)a c(Yt  ˆYt)] = (1 − a)
(3)
g(∆i t)+g(−∆i t)  and the expectation Et above is w.r.t. the generation of labels Yt 
where pi t =
conditioned on both xt  and all previous x and Y . A key aspect of this formalization is that the
Bayes optimal ordered subset Y ∗
Et[(cid:96)a c(Yt  Y )] can be computed
efﬁciently when knowing ∆1 t  . . .   ∆K t. This is handled by the next lemma. In words  this lemma
says that  in order to minimize (3)  it sufﬁces to try out all possible sizes s = 0  1  . . .   K for Y ∗
and  for each such value  determine the sequence Y ∗
t
s t that minimizes (3) over all sequences of size
s t can be computed just by sorting classes i ∈ [K] in decreasing order of pi t  sequence
s. In turn  Y ∗
Y ∗
s t being given by the ﬁrst s classes in this sorted list.2
Lemma 1. With the notation introduced so far  let pi1 t ≥ pi2 t ≥ . . . piK  t be the sequence of pi t
sorted in nonincreasing order. Then we have that Y ∗
s t)]  where
Y ∗
s t = (i1  i2  . . .   is)  and Y ∗
Notice the way costs c(i  s) inﬂuence the Bayes optimal computation. We see from (3) that placing
class i within ˆYt in position ji is beneﬁcial (i.e.  it leads to a reduction of loss) if and only if pi t >
c(ji | ˆYt|)/( a
1−a + c(ji | ˆYt|)). Hence  the higher is the slot ij in ˆYt the larger should be pi t in order
for this inclusion to be convenient.3 It is Y ∗
that we interpret as the true set of user preferences on
xt.
We would like to compete against the above Y ∗

in a cumulative regret sense  i.e.  we would like to
t )] with high probability. We use a similar but
largely more general analysis than [4]’s  to devise an online second-order descent algorithm whose
updating rule makes the comparison vector U = (u1  . . .   uK) ∈ RdK deﬁned through (2) be Bayes
optimal w.r.t. a surrogate convex loss L(·) such that g(∆) = −L(cid:48)(∆). Observe that the expected
loss function (3) is  generally speaking  nonconvex in the margins ∆i t (consider  for instance the
logistic case g(∆) = 1

bound RT =(cid:80)T

e∆+1). Thus  we cannot directly minimize this expected loss.

Et[(cid:96)a c(Yt  ˆYt)] − Et[(cid:96)a c(Yt  Y ∗

t = argmins=0 1 ...K

Et[(cid:96)a c(Yt  Y ∗

0 t = ∅.

t

t

t=1

1The reader familiar with generalized linear models will recognize the derivative of the function p(∆) =
g(−∆)
g(∆)+g(−∆) as the (inverse) link function of the associated canonical exponential family of distributions [17].

2Due to space limitations  all proofs are given in the supplementary material.
3Notice that this depends on the actual size of ˆYt  so we cannot decompose this problem into K independent
problems. The decomposition does occur if the costs c(i  s) are constants  independent of i and s  and the
criterion for inclusion becomes pi t ≥ θ  for some constant threshold θ.

4

1. Get instance xt ∈ Rd : ||xt|| = 1;

i t = x(cid:62)

t w(cid:48)

i t  where

w

(cid:48)
i t =

2. For i ∈ [K]  set (cid:98)∆(cid:48)
wi t
where : (cid:98)pi t =

3. Output

i t = x(cid:62)
2
t A

−1
i t−1xt

ˆYt = argminY =(j1 j2 ...j|Y |)⊆[K]

Parameters: loss parameters a ∈ [0  1]  cost values c(i  s)  interval D = [−R  R]  function g :
D → R  conﬁdence level δ ∈ [0  1].
Initialization: Ai 0 = I ∈ Rd×d  i = 1  . . .   K  wi 1 = 0 ∈ Rd  i = 1  . . .   K;
For t = 1  2 . . .   T :

A

−1
i t−1xt

(cid:19)
c(ji |Y |) −(cid:16) a
(cid:16) c(cid:48)
(cid:1) + 12

L

 

c(cid:48)(cid:48)

L

c(cid:48)(cid:48)

L

i txt ∈ [−R  R] 

if w(cid:62)
otherwise;

1−a + c(ji |Y |)
(cid:17)

+ 3L(−R)

 

(cid:17) (cid:98)pi t

(cid:17)(cid:17)
(cid:17)
i t ∇i t  where
−1
A

ln K(t+4)

;

δ

(cid:18) w(cid:62)

i txt)

wi t −

i txt−R sign(w(cid:62)
−1
i t−1xt

x(cid:62)
t A

i∈Y

i t+i t]D )

(cid:16)
(cid:16)(cid:80)
g(−[(cid:98)∆(cid:48)
(cid:16)
L)2 ln(cid:0)1 + t−1
i t+i t]D )+g(−[(cid:98)∆(cid:48)
g([(cid:98)∆(cid:48)
U 2 + d c(cid:48)
(c(cid:48)(cid:48)
1

si t =

L

d

i t+i t]D )

4. Get feedback Yt ∩ ˆYt;
5. For i ∈ [K]  update Ai t = Ai t−1 + |si t|xtx(cid:62)
If i ∈ Yt ∩ ˆYt

t   wi t+1 = w(cid:48)

i t − 1
c(cid:48)(cid:48)

L

and ∇i t = ∇wL(si t w(cid:62)xt)|w=w(cid:48)

i t

−1 If i ∈ ˆYt \ Yt = ˆYt \ (Yt ∩ ˆYt)
0

otherwise;

= −g(si t(cid:98)∆(cid:48)

i t) si t xt.

Figure 1: The partial feedback algorithm in the (ordered) multiple label setting.

3 Algorithm and regret bounds

t w(cid:48)

i t+i t]D)

i t+i t]D)

g(−∆i t)

g([(cid:98)∆(cid:48)

i t = x(cid:62)

1 t  . . .   w(cid:48)

K t  being w(cid:48)

i t  and ∆i t = u(cid:62)

i xt  i ∈ [K]. The algorithm uses (cid:98)∆(cid:48)

we let (cid:98)∆(cid:48)
derlying ∆i t according to the (upper conﬁdence) approximation scheme ∆i t ≈ [(cid:98)∆(cid:48)

In Figure 1 is our bandit algorithm for (ordered) multiple labels. The algorithm is based on replacing
the unknown model vectors u1  . . .   uK with prototype vectors w(cid:48)
i t the time-
t approximation to ui  satisying similar constraints we set for the ui vectors. For the sake of brevity 
i t as proxies for the un-
i t + i t]D 
where i t ≥ 0 is a suitable upper-conﬁdence level for class i at time t  and [·]D denotes the
clipping-to-D operation  i.e.  [x]D = max(min(x  R) −R). The algorithm’s prediction at time
t has the same form as the computation of the Bayes optimal sequence Y ∗
t   where we replace
the true (and unknown) pi t =
g(∆i t)+g(−∆i t) with the corresponding upper conﬁdence proxy
. Computing ˆYt can be done by mimicking the computation of

i t+i t]D)+g(−[(cid:98)∆(cid:48)

g(−[(cid:98)∆(cid:48)
the Bayes optimal Y ∗
Thus the algorithm is producing a ranked list of relevant classes based on upper-conﬁdence-corrected

(cid:98)pi t =
t (just replace pi t by(cid:98)pi t)  i.e.  order of K log K running time per prediction.
scores(cid:98)pi t. Class i is deemed relevant and ranked high among the relevant ones when either (cid:98)∆(cid:48)

i t
is a good approximation to ∆i t and pi t is large  or when the algorithm is not very conﬁdent on its
own approximation about i (that is  the upper conﬁdence level i t is large).
The algorithm receives in input the loss parameters a and c(i  s)  the model function g(·) and the
associated margin domain D = [−R  R]  and maintains both K positive deﬁnite matrices Ai t of
dimension d (initially set to the d × d identity matrix)  and K weight vector wi t ∈ Rd (initially
set to the zero vector). At each time step t  upon receiving the d-dimensional instance vector xt
the algorithm uses the weight vectors wi t to compute the prediction vectors w(cid:48)
i t. These vectors
can easily be seen as the result of projecting wi t onto the space of w where |w(cid:62)xt| ≤ R w.r.t.
i t = argminw∈Rd : w(cid:62)xt∈D di t−1(w  wi t)  i ∈ [K]  where
the distance function di t−1  i.e.  w(cid:48)
(cid:98)∆(cid:48)
di t(u  w) = (u − w)(cid:62) Ai t (u − w) . Vectors w(cid:48)
i t are then used to produce prediction values
i t involved in the upper-conﬁdence calculation of ˆYt ⊆ [K]. Next  the feedback Yt ∩ ˆYt is
observed  and the algorithm in Figure 1 promotes all classes i ∈ Yt ∩ ˆYt (sign si t = 1)  demotes

5

all classes i ∈ ˆYt \ Yt (sign si t = −1)  and leaves all remaining classes i /∈ ˆYt unchanged (sign
i t → wi t+1 is based on the gradients ∇i t of a loss function L(·) satisfying
si t = 0). The update w(cid:48)
L(cid:48)(∆) = −g(∆). On the other hand  the update Ai t−1 → Ai t uses the rank one matrix4 xtx(cid:62)
t .
In both the update of w(cid:48)
i t and the one involving Ai t−1  the reader should observe the role played
by the signs si t. Finally  the constants c(cid:48)
L and c(cid:48)(cid:48)
i t are related to
smoothness properties of L(·) – see next theorem.
Theorem 2. Let L : D = [−R  R] ⊆ R → R+ be a C 2(D) convex and nonincreasing function
of its argument  (u1  . . .   uK) ∈ RdK be deﬁned in (2) with g(∆) = −L(cid:48)(∆) for all ∆ ∈ D  and
such that (cid:107)ui(cid:107) ≤ U for all i ∈ [K]. Assume there are positive constants cL  c(cid:48)
L such that:
i. L(cid:48)(∆) L(cid:48)(cid:48)(−∆)+L(cid:48)(cid:48)(∆) L(cid:48)(−∆)
L hold for all
∆ ∈ D. Then the cumulative regret RT of the algorithm in Figure 1 satisﬁes  with probability at
least 1 − δ 

L and c(cid:48)(cid:48)
L  and iii. L(cid:48)(cid:48)(∆) ≥ c(cid:48)(cid:48)

≥ −cL and ii. (L(cid:48)(∆))2 ≤ c(cid:48)

L occurring in the expression for 2

(L(cid:48)(∆)+L(cid:48)(−∆))2

(cid:16)
L)2 ln(cid:0)1 + T

RT = O

L

(cid:113)
T C d ln(cid:0)1 + T
(cid:17)
(cid:16) c(cid:48)
L)2 + L(−R)
c(cid:48)(cid:48)
(c(cid:48)(cid:48)

ln KT
δ

(cid:17)

L

L

d

(cid:1)(cid:17)

 

(1 − a) cL K

(cid:1) +

(cid:16)

U 2 + d c(cid:48)
(c(cid:48)(cid:48)

.

d

L = 4 and c(cid:48)(cid:48)

where C = O
It is easy to see that when L(·) is the square loss L(∆) = (1 − ∆)2/2 and D = [−1  1]  we
L = 1; when L(·) is the logistic loss L(∆) = ln(1 + e−∆) and
have cL = 1/2  c(cid:48)
D = [−R  R]  we have cL = 1/4  c(cid:48)
L =
Remark 1. A drawback of Theorem 2 is that 
the upper conﬁ-
dence levels i t  we assume prior knowledge of the norm upper bound U. Because this
information is often unavailable  we present here a simple modiﬁcation to the algorithm
that copes with this limitation. We change the deﬁnition of 2
i t =
i t

2(1+cosh(R))  where cosh(x) = ex+e−x
in order to properly set

L ≤ 1 and c(cid:48)(cid:48)

in Figure 1 to 2

.

2

1

(cid:17)

(cid:41)

(cid:17)

(cid:40)

max

x(cid:62)A−1

i t−1x

+ 3L(−R)

ln K(t+4)

δ

  4 R2

. This immedi-

(cid:1) + 12

c(cid:48)(cid:48)

L

(cid:16) c(cid:48)

L

c(cid:48)(cid:48)

L

L

(c(cid:48)(cid:48)

(cid:16) 2 d c(cid:48)
L)2 ln(cid:0)1 + t−1
(cid:113)

d

ately leads to the following result.
Theorem 3. With the same assumptions and notation as in Theorem 2  if we replace 2
above we have that  with probability at least 1 − δ  RT satisﬁes

T C d ln(cid:0)1 + T

(cid:1) + (1 − a) cL K R d

d

(cid:16)

(cid:16) (c(cid:48)(cid:48)

exp

L)2 U 2
c(cid:48)
L d

i t as explained

(cid:17) − 1
(cid:17)(cid:17)

.

RT = O

(1 − a) cL K

(cid:16)

g(−∆)

4 On ranking with partial feedback
As Lemma 1 points out  when the cost values c(i  s) in (cid:96)a c are stricly decreasing then the Bayes
optimal ordered sequence Y ∗
t on xt can be obtained by sorting classes in decreasing values of pi t 
and then decide on a cutoff point5 induced by the loss parameters  so as to tell relevant classes
g(∆)+g(−∆) is increasing in ∆  this ordering
apart from irrelevant ones. In turn  because p(∆) =
corresponds to sorting classes in decreasing values of ∆i t. Now  if parameter a in (cid:96)a c is very close6
t | = K  and the algorithm itself will produce ordered subsets ˆYt such that | ˆYt| = K.
to 1  then |Y ∗
Moreover  it does so by receiving full feedback on the relevant classes at time t (since Yt ∩ ˆYt =
Yt). As is customary (e.g.  [6])  one can view any multilabel assignment Y = (y1  . . .   yK) ∈
yi > yj. The (unnormalized) ranking loss function (cid:96)rank(Y (cid:98)f ) between the multilabel Y and a
{0  1}K as a ranking among the K classes in the most natural way: i preceeds j if and only if
ranking function (cid:98)f : Rd → RK  representing degrees of class relevance sorted in a decreasing
order (cid:98)fj1 (xt) ≥ (cid:98)fj2(xt) ≥ . . . ≥ (cid:98)fjK (xt) ≥ 0  counts the number of class pairs that disagree
2 {(cid:98)fi(xt) = (cid:98)fj(xt)}(cid:17)
in the two rankings: (cid:96)rank(Y (cid:98)f ) =(cid:80)

(cid:16){(cid:98)fi(xt) < (cid:98)fj(xt)} + 1

i j∈[K] : yi>yj

 

4Notice that A

−1
i t can be computed incrementally in O(d2) time per update. [4] and references therein also

use diagonal approximations thereof  reporting good empirical performance with just O(d) time per update.

5This is called the zero point in [9].
6If a = 1  the algorithm only cares about false negative mistakes  the best strategy being always predicting

ˆYt = [K]. Unsurprisingly  this yields zero regret in both Theorems 2 and 3.

6

where {. . .} is the indicator function of the predicate at argument. As pointed out in [6]  the ranking

function (cid:98)f (xt) = (p1 t  . . .   pK t) is also Bayes optimal w.r.t. (cid:96)rank(Y (cid:98)f )  no matter if the class

we can set (the factor St below serves as balancing the contribution of the two main terms):

labels yi are conditionally independent or not. Hence we can use this algorithm for tackling ranking
problems derived from multilabel ones  when the measure of choice is (cid:96)rank and the feedback is
full.
In fact  a partial information version of the above can easily be obtained. Suppose that at each
time t  the environment discloses both xt and a maximal size St for the ordered subset ˆYt =
(j1  j2  . . .   j| ˆYt|) (both xt and St can be chosen adaptively by an adversary). Here St might be
the number of available slots in a webpage or the number of URLs returned by a search engine in
response to query xt. Then it is plausible to compete in a regret sense against the best time-t ofﬂine
ranking of the form f (xt) = (f1(xt)  f2(xt)  . . .   fh(xt)  0  . . .   0)  with h ≤ St. Further  the rank-
ing loss could be reasonably restricted to count the number of class pairs disagreeing within ˆYt plus a

quantity related to number of false negative mistakes. E.g.  if (cid:98)fj1(xt) ≥ (cid:98)fj2(xt) ≥ . . . ≥ (cid:98)fj| ˆYt|(xt) 
(cid:96)rank t(Y (cid:98)f ) =(cid:80)
It is not hard to see that if classes are conditionally independent  Pt(y1 t  ...  yK t) = (cid:81)
i∈[K] pi t 
then the Bayes optimal ranking for (cid:96)rank t is given by f∗(xt; St) = (pi1 t  . . .   piSt  t  0  . . .   0). If
according to decreasing values of(cid:98)pi t)  one can prove the following ranking version of Theorem 2.
we put on the argmin (Step 3 in Figure 1) the further constraint |Y | ≤ St (we are still sorting classes
conditionally independent  i.e.  Pt(y1 t  ...  yK t) = (cid:81)
RT =(cid:80)T
Et[(cid:96)rank t(Yt  ((cid:98)pj1 t  ... (cid:98)pjSt  t  0  ...  0))] − Et[(cid:96)rank t(Yt  (pi1 t  ...  piSt  t  0  ...  0))] 
where(cid:98)pj1 t ≥ . . . ≥(cid:98)pjSt  t ≥ 0 and pi1 t ≥ . . . ≥ piSt  t ≥ 0. Then  with probability at least 1 − δ 
(cid:16)

Theorem 4. With the same assumptions and notation as in Theorem 2  let the classes in [K] be
i∈[K] pi t for all t  and let the cumulative

regret RT w.r.t. (cid:96)rank t be deﬁned as

(cid:16){(cid:98)fi(xt) < (cid:98)fj(xt)} + 1

2 {(cid:98)fi(xt) = (cid:98)fj(xt)}(cid:17)

+ St |Yt \ ˆYt| .

i j∈ ˆYt : yi>yj

(cid:113)

S K T C d ln(cid:0)1 + T

d

(cid:1)(cid:17)

t=1

we have RT = O

cL

  where S = maxt=1 ... T St.

√

The proof (see the appendix) is very similar to the one of Theorem 2. This suggests that  to some
extent  we are decoupling the label generating model from the loss function (cid:96) under consideration.
Notice that the linear dependence on the total number of classes K (which is often much larger
than S in a multilabel/ranking problem) is replaced by
SK. One could get similar beneﬁts out of
Theorem 2. Finally  one could also combine Theorem 4 with the argument contained in Remark 1.
5 Experiments and conclusions
The experiments we report here are meant to validate the exploration-exploitation tradeoff imple-
mented by our algorithm under different conditions (restricted vs. nonrestricted number of classes) 
loss measures ((cid:96)a c  (cid:96)rank t  and Hamming loss) and model/parameter settings (L = square loss  L =
logistic loss  with varying R).
Datasets. We used two multilabel datasets. The ﬁrst one  called Mediamill  was introduced in a
video annotation challenge [21]. It comprises 30 993 training samples and 12 914 test ones. The
number of features d is 120  and the number of classes K is 101. The second dataset is Sony CSL
Paris [18]  made up of 16 452 train samples and 16 519 test samples  each sample being described
by d = 98 features. The number of classes K is 632. In both cases  feature vectors have been
normalized to unit L2 norm.
Parameter setting and loss measures. We used the algorithm in Figure 1 with two different
loss functions  the square loss and the logistic loss  and varied the parameter R for the latter. The
setting of the cost function c(i  s) depends on the task at hand  and for this preliminary experiments
we decided to evaluate two possible settings only. The ﬁrst one  denoted by “decreasing c” is
c(i  s) = s−i+1
  i = 1  . . .   s  the second one  denoted by “constant c”  is c(i  s) = 1  for all i and
s. In all experiments  the a parameter was set to 0.5  so that (cid:96)a c with constant c reduces to half
the Hamming loss. In the decreasing c scenario  we evaluated the performance of the algorithm on
the loss (cid:96)a c that the algorithm is minimizing  but also its ability to produce meaningful (partial)

s

7

Figure 2: Experiments on the Sony CSL Paris dataset.

Figure 3: Experiments on the Mediamill dataset.

rankings through (cid:96)rank t. On the constant c setting  we evaluated the Hamming loss. As is typical
of multilabel problems  the label density  i.e.  the average fraction of labels associated with the
examples  is quite small. For instance  on Mediamill this is 4.3%. Hence  it is clearly beneﬁcial
to impose an upper bound S on | ˆYt|. For the constant c and ranking loss experiments we tried out
different values of S  and reported the ﬁnal performance.
Baseline. As baseline  we considered a full information version of Algorithm 1 using the square
loss  that receives after each prediction the full array of true labels Yt for each sample. We call
this algorithm OBR (Online Binary Relevance)  because it is a natural online adaptation of the
binary relevance algorithm  widely used as a baseline in the multilabel literature. Comparing to
OBR stresses the effectiveness of the exploration/exploitation rule above and beyond the details of
underlying generalized linear predictor. OBR was used to produce subsets (as in the Hamming loss
case)  and restricted rankings (as in the case of (cid:96)rank t).
Results. Our results are summarized in Figures 2 and 3. The algorithms have been trained by
sweeping only once over the training data. Though preliminary in nature  these experiments allow
us to draw a few conclusions. Our results for the avarage (cid:96)a c(Yt  ˆYt) with decreasing c are contained
in the two left plots. We can see that the performance is improving over time on both datasets  as
predicted by Theorem 2. In the middle plots are the ﬁnal cumulative Hamming losses with constant
c divided by the number of training samples  as a function of S. Similar plots are on the right with
the ﬁnal average ranking losses (cid:96)rank t divided by S. In both cases we see that there is an optimal
value of S that allows to balance the exploration and the exploitation of the algorithm. Moreover
the performance of our algorithm is always pretty close to the performance of OBR  even if our
algorithm is receiving only partial feedback. In many experiments the square loss seems to give
better results. Exception is the ranking loss on the Mediamill dataset (Figure 3  right).
Conclusions. We have used generalized linear models to formalize the exploration-exploitation
tradeoff in a multilabel/ranking setting with partial feedback  providing T 1/2-like regret bounds un-
der semi-adversarial settings. Our analysis decouples the multilabel/ranking loss at hand from the

label-generation model. Thanks to the usage of calibrated score values(cid:98)pi t  our algorithm is capable

of automatically inferring where to split the ranking between relevant and nonrelevant classes [9] 
the split being clearly induced by the loss parameters in (cid:96)a c. We are planning on using more gen-
eral label models that explicitly capture label correlations to be applied to other loss functions (e.g. 
F-measure  0/1  average precision  etc.). We are also planning on carrying out a more thorough ex-
perimental comparison  especially to full information multilabel methods that take such correlations
into account. Finally  we are currenty working on extending our framework to structured output
tasks  like (multilabel) hierarchical classiﬁcation.

8

100101102103104105050100150Number of samplesRunning average la cSony CSL Paris − Loss(a c) and decreasing c  SquareLogistic (R=1.5)Logistic (R=2.0)Logistic (R=2.5)Logistic (R=3.0)100101102303540455055SFinal Average Hamming lossSony CSL Paris − Hamming loss and constant c  OBRSquareLogistic (R=1.5)Logistic (R=2.0)Logistic (R=2.5)Logistic (R=3.0)510152025303524252627282930313233SFinal Average Rank loss / SSony CLS Paris − Ranking loss  OBRSquareLogistic (R=1.5)Logistic (R=2.0)Logistic (R=2.5)Logistic (R=3.0)1001011021031041050510152025Number of samplesRunning average la cMediamill − Loss(a c) and decreasing c  SquareLogistic (R=1.5)Logistic (R=2.0)Logistic (R=2.5)Logistic (R=3.0)10010110233.544.555.566.57SFinal Average Hamming lossMediamill − Hamming loss and constant c  OBRSquareLogistic (R=1.5)Logistic (R=2.0)Logistic (R=2.5)Logistic (R=3.0)51015202511.21.41.61.82SFinal Average Rank loss / SMediamill − Ranking loss  OBRSquareLogistic (R=1.5)Logistic (R=2.0)Logistic (R=2.5)Logistic (R=3.0)References
[1] Y. Abbasi-Yadkori  D. Pal  and C. Szepesv´ari. Improved algorithms for linear stochastic ban-

dits. In 25th NIPS  2011.

[2] K. Amin  M. Kearns  and U. Syed. Graphical models for bandit problems. In UAI  2011.
[3] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. JMLR  3  2003.
[4] K. Crammer and C. Gentile. Multiclass classiﬁcation with bandit feedback using adaptive

regularization. In 28th ICML  2011.

[5] V. Dani  T. Hayes  and S. Kakade. Stochastic linear optimization under bandit feedback. In

21th Colt  2008.

[6] K. Dembczynski  W. Waegeman  W. Cheng  and E. Hullermeier. On label dependence and loss

minimization in multi-label classiﬁcation. Machine Learning  to appear.

[7] S. Filippi  O. Capp´e  A. Garivier  and C. Szepesv´ari. Parametric bandits: The generalized

linear case. In NIPS  pages 586–594  2010.

[8] Y. Freund  R. D. Iyer  R. E. Schapire  and Y. Singer. An efﬁcient boosting algorithm for

combining preferences. JMLR  4:933–969  2003.

[9] J. Furnkranz  E. Hullermeier  E. Loza Menca  and K. Brinker. Multilabel classiﬁcation via

calibrated label ranking. Machine Learning  73:133–153  2008.

[10] E. Hazan and S. Kale. Online submodular minimization. In NIPS 22  2009.
[11] E. Hazan and S. Kale. Newtron: an efﬁcient bandit algorithm for online multiclass prediction.

In NIPS  2011.

[12] R. Herbrich  T. Graepel  and K. Obermayer. Large margin rank boundaries for ordinal regres-

sion. In Advances in Large Margin Classiﬁers. MIT Press  2000.

[13] S. Kakade  S. Shalev-Shwartz  and A. Tewari. Efﬁcient bandit algorithms for online multiclass

prediction. In 25th ICML  2008.

[14] S. Kale  L. Reyzin  and R. Schapire. Non-stochastic bandit slate problems. In 24th NIPS  2010.
[15] A. Krause and C. S. Ong. Contextual gaussian process bandit optimization. In 25th NIPS 

2011.

[16] T. H. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Adv. Appl. Math 

6  1985.

[17] P. McCullagh and J.A. Nelder. Generalized linear models. Chapman and Hall  1989.
[18] F. Pachet and P. Roy. Improving multilabel analysis of music titles: A large-scale validation
of the correction approach. IEEE Trans. on Audio  Speech  and Lang. Proc.  17(2):335–343 
February 2009.

[19] P. Shivaswamy and T. Joachims. Online structured prediction via coactive learning. In 29th

ICML  2012  to appear.

[20] A. Slivkins  F. Radlinski  and S. Gollapudi. Learning optimally diverse rankings over large

document collections. In 27th ICML  2010.

[21] C. G. M. Snoek  M. Worring  J.C. van Gemert  J.-M. Geusebroek  and A. W. M. Smeulders.
The challenge problem for automated detection of 101 semantic concepts in multimedia. In
Proc. of the 14th ACM international conference on Multimedia  MULTIMEDIA ’06  pages
421–430  New York  NY  USA  2006.

[22] M. Streeter  D. Golovin  and A. Krause. Online learning of assignments. In 23rd NIPS  2009.
[23] G. Tsoumakas  I. Katakis  and I. Vlahavas. Random k-labelsets for multilabel classiﬁcation.

IEEE Transactions on Knowledge and Data Engineering  23:1079–1089  2011.

9

,Long Jin
Justin Lazarow
Zhuowen Tu