2018,Using Large Ensembles of Control Variates for Variational Inference,Variational inference is increasingly being addressed with stochastic optimization. In this setting  the gradient's variance plays a crucial role in the optimization procedure  since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained  control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates  which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.,Using Large Ensembles of Control Variates for

Variational Inference

College of Information and Computer Science

College of Information and Computer Science

Justin Domke

University of Massachusetts

Amherst  MA 01003
domke@cs.umass.edu

Tomas Geffner

University of Massachusetts

Amherst  MA 01003

tgeffner@cs.umass.edu

Abstract

Variational inference is increasingly being addressed with stochastic optimization.
In this setting  the gradient’s variance plays a crucial role in the optimization proce-
dure  since high variance gradients lead to poor convergence. A popular approach
used to reduce gradient’s variance involves the use of control variates. Despite
the good results obtained  control variates developed for variational inference are
typically looked at in isolation. In this paper we clarify the large number of control
variates that are available by giving a systematic view of how they are derived. We
also present a Bayesian risk minimization framework in which the quality of a
procedure for combining control variates is quantiﬁed by its effect on optimization
convergence rates  which leads to a very simple combination rule. Results show
that combining a large number of control variates this way signiﬁcantly improves
the convergence of inference over using the typical gradient estimators or a reduced
number of control variates.

1

Introduction

Variational Inference (VI) [29  2  11] is a framework for approximate probabilistic inference. It
has been successfully applied in several areas including topic modeling [3  21]  generative models
[13  5  22]  reinforcement learning [6]  and parsing [15]  among others. Recently  VI has been able to
address a wider range of problems by adopting a "black box" [25] view based on only evaluating the
value or gradient of the target distribution. Then  the target can be optimized via stochastic gradient
descent. It is desirable to reduce the variance of the gradient estimate  since this governs convergence.
Control variates (CVs)  a classical technique from statistics  is often used to accomplish this.
This paper investigates how to use many CVs in concert. We present a systematic view of existing
CVs  which starts by splitting the exact gradient into four terms (Eq. 2). Then  a CV is obtained by
application of a generic "recipe": Pick a term  possibly approximate it  and take the difference of two
estimators (Fig. 2). This suggests many possible CVs  including some seemingly not used before.
With many possible CVs  one can naturally ask how to use many together. In principle  the optimal
combination is well known (Eq. 6). However  this requires unknown (intractable) expectations. We
address this using decision theory. The goal is a “decision rule” that takes a minibatch of evaluations
together with the set of CVs to be used  and returns a gradient estimate. We adopt a Bayesian risk
measuring how gradient variance impacts convergence rates of stochastic optimization  with simple
prior over gradients and sets of CVs. A simple optimal decision rule emerges  where the intractable
expectations are replaced with "regularized" empirical estimates (Thm 4.1). To share information
across iterations  we suggest combining this Bayesian approach with exponential averaging by using
an “effective” minibatch size.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

We demonstrate practicality on logistic regression problems  where careful combination of many
CVs improves performance. For all learning rates  convergence is improved over any single CV.

1.1 Contributions

The contribution of this work is twofold. First  in
Section 3  we propose a systematic view of how
to generate many existing control variates. Second 
we propose a an algorithm to use multiple control
variates simultaneously  described in Section 4. As
shown in Section 5  combining these two ideas result
in gradients with low variance that allow the use of
larger learning rates  while retaining convergence.

2 Preliminaries

Figure 1: An example of how combining con-
trol variates reduces gradient variance for the
same sequence of weights (australian dataset).
Variational Inference (VI) works by transforming an inference problem into an optimization  by
decomposing the marginal likelihood of the observed data x given latent variables z as:

log p(x) =

E

Z∼qw(Z)� log
��
�

p(Z  x)

qw(Z)�
�

ELBO(w)

.

+ KL(qw(Z)||p(Z|x))
�
�

KL-divergence

��

Here  the variational distribution qw(z) is used to approximate the true posterior distribution p(z|x).
VI’s goal is to ﬁnd the parameters w that minimize the KL-divergence between qw(z) and the true
posterior p(z|x). Since log p(x) does not depend on w  minimizing the KL-divergence is equivalent
to maximizing the ELBO (Evidence Lower BOund).
Historically  models and variational families for which expectations were simple enough to allow
closed-form updates of w were used [2  3  32]. However  for more complex models  closed form
expressions are usually not available  which has led to widespread use of stochastic optimization
methods [8  18  19  20  26]. These require approximating the target’s gradient

g(w) = ∇wELBO(w) = ∇w

E

Z∼qw(Z)� log p(Z  x) − log qw(Z)�.

(1)

Good gradient estimates play an important role  since high variance will negatively impact on
convergence and optimization speed. Several methods have been developed to improve gradient
estimates  including Rao-Blackwellization [20]  control variates [7  17  18  19  20  28  30  33] 
closed-form solutions for certain expectations [27]  discarding terms [23]  and different estimators.

2.1 Control variates

A control variate (CV) is a random variable with expectation zero that is added to another random
variable in the hope of reducing variance. Let X be a random variable with unknown mean  and let C
be a random variable with mean zero. Then for any scalar a  Y = X +a C has the same expectation as
X but (usually) different variance. A standard result from statistics is that the value of a that minimizes
the variance of Y is a = Cov(X  C)/Var(C)  for which Var(Y ) = Var(X)(1 − Corr(X  C)2).
Thus  a good control variate for X is a random variable C that is highly correlated with X.

3 Systematic generation of control variates

This section gives a generic recipe for creating control variates (Fig. 2) and reviews how existing
control variates are an instance of it (see also Sec. 6.4 in the appendix). We begin by splitting the
ELBO gradient into four terms as
g(w) = ∇w E

+ ∇w E

− ∇w E

−∇w E

log p(Z)

qw

qw

qv

log qv(Z)��v=w
��
�

�

.

log qw(Z)��v=w
��
�

(2)

qw

log p(x|Z)
g1(w): Data term

��

�

�

�

��

�

g2(w): Prior term

g3(w): Variational term

g4(w): Score term

�

2

����������������������������������������������������������������������������Pick Term t(w).
(Part of g1  g2  g3)

t

Approximate
Term (optional)

˜t

˜t

1st estimate

(SF  RP  CF  etc.)

2nd Estimate

(SF  RP  CF  etc.)

T

T �

Take Difference

T − T �

Figure 2: Generic control variate recipe. (SF: score function RP: reparameterization CF: closed form.)
Sec. 6.4 (appendix) casts several existing ideas [23  19  17  30  28  7] as instances of this recipe.

The ﬁrst three terms all correspond to the inﬂuence of w on the expectation of some function
independent of w. Control variates for these terms  and for any combination of them  are discussed
in Sec. 3.1-3.2. The score term  discussed in Sec. 3.3  is different  since the function inside the
expectation depends on w. (Roeder et al. [23] give a related decomposition  albeit speciﬁcally for
reparameterization estimators.)

3.1 Control Variates from Pairs of estimators

The basic technique for deriving CVs is to take the difference between a pair of unbiased estimators
of a general term t(w) (any of g1  g2  g3 or a combination of them)  which must therefore have
expectation zero. The terms g1  g2 or g3 are all the expectation (over qw) of some function f
(independent of w). 1 Thus  t(w) can be written as

t(w) = ∇w E

qw(Z)

[f (Z)]

or

�∇w E

qw(Z)

[fv(Z)]����v=w

.

t(w) = E



Many methods exist to estimate gradients of this type. Mathematically  we think of these as random
variables (with a corresponding generation algorithm). A few estimators are summarized in Eq. 3
(dropping dependence of f on v). If we write T a for an estimator for t(w) using method a  then

w (�))

Score function

T SF = f (Z)∇w log qw(Z)
T RP1 = ∇wf (T 1
T RP2 = ∇wf (T 2
T GR = f (Z)∇w log qw(Z) + ∇wf (Tw(�)) Gen. Reparam.
Closed Form
T CF = ∇w Eqw [f (Z)]

Z ∼ qw
Reparameterization � ∼ ¯q
Other Reparam.
� ∼ ¯q
� ∼ ¯qw  Z = Tw(�)

w (�))

(3)
Score function (SF) estimation  or REINFORCE [31]  uses the equality ∇wqw(z) =
qw(z)∇w log qw(z) [18  20]. This gives t(w) = Eqw T SF   with T SF as in Eq. 3. Unbiased
estimates for the gradient can be obtained using Monte Carlo sampling  with samples from qw(z).
Reparameterization (RP) estimators [13  17  26] are based on splitting the procedure to sample
from qw into sampling and transformation steps. First  sample � ∼ ¯q(�); second  transform z = Tw(�).
Here  ¯q is a ﬁxed distribution (indep. of w) and Tw is a deterministic transformation. When sampling
is done this way  it follows that Eqw f (Z) = E¯q f (Tw(�))  rendering the expectation independent of
w. The general term can therefore be written as t(w) = E¯q T RP   with T RP = ∇wf (Tw(�)). The
multivariate Gaussian distribution N (µw  Σw) illustrates this: A sample can be generated by drawing
� ∼ N (0  I) and setting Tw(�) = Mw � + µw  where Mw is a matrix such that MwM T
Multiple reparameterizations are typically possible. For example  the above estimator for the
multivariate Gaussian is valid with any Mw such that MwM T
w = Σw. For instance  Mw could be a
lower triangular matrix obtained via the Cholesky factorization of Σw [4  26]. (Often  entries of w
directly specify entries in the Cholesky factorization  obviating the need to explicitly compute it.)
Another option is the matrix square root of Σw [14]. All valid reparameterizations give unbiased
gradients  but with different statistical properties.

w = Σw.

1For g1  g2  and g3  use f (z) = log p(x|z)  f (z) = log p(z)  and fv(z) = log qv(z) respectively.

3

Generalized reparameterization (GR) is intended for distributions where reparameterization is
not applicable  e.g.
the gamma or beta [24]. Take a transformation Tw and a base distribution
¯qw(�) (both dependent on w) such that Tw(�) is distributed identically to qw(Z). Then  Eqw f (Z) =
E¯qw f (Tw(�)). The dependence of this expectation on w is mediated partially through w’s inﬂuence
on ¯qw and partially through w’s inﬂuence on Tw. This leads to a representation of a general term as
t(w) = E¯qw(�) T GR  where T GR is as in Eq. 3. This has essentially has a score function-like term
and a reparameterization-like term  corresponding to w’s inﬂuence on ¯q and Tw  respectively.
Closed form (CF) expressions are sometimes available for general terms involving g2 and g3  but
rarely for g1. This is because a closed-form expression needs q and f to be simple enough  that is
rarely the case for the data term g1  which is usually estimated with one of the methods described
above [17  19  20  24]. However  there are some cases for which g1 can be computed exactly [4].
Data Subsampling is often applied to the data term g1 [12]. If the likelihood treats x as i.i.d.  then
f (z) = log p(x|z) can be approximated without bias from a minibatch of data. If fd(z) is that
estimate  an equivalent representation of the data term is g1(w) = ED ∇w Eqw(Z) fD(Z) where D
is uniform over subsets of data. Thus  one can deﬁne an unbiased estimator by using one of the
techniques above (to cope with Eqw(Z)) on a random minibatch D (to cope with ED). With large
datasets this can be much faster  but sampling D acts as an additional source of variance.

3.2 Control Variates from approximations

The previous section used that the difference of two unbiased estimators of a term has expectation
zero  and so is a control variate. Another class of control variates uses the insight that if a general term
t(w) is replaced with an approximation  the difference between two estimators of the (approximate)
general term still produces a valid control variate. The motivation is that approximations might allow
the use of high-quality estimators (e.g. a closed-form) not otherwise available.
Fundamentally  the randomness in the above estimators is due to two types of sampling. First 
expectations over qw are approximated by sampling  introducing "distributional sampling error".
Second  with large data  the data term can be approximated by drawing a minibatch  introducing
"data subsampling error". Approximations to terms have been devised so that expectations (either
over qw or the full dataset) can be efﬁciently computed.
Correcting for distributional sampling: Here  the goal is to approximate f with some function ˜f
so as to make E[ ˜f (Z)] easier to estimate – typically so admits a closed-form solution. Paisley et al.
[19] approximate the data term with either a Taylor approximation in z or a bound and then deﬁne a
control variate as the difference between E[ ˜f (Z)] computed exactly and its estimator using the score
function method  which greatly reduces the variance of their gradient estimate  obtained with the
score function method. Miller et al. [17] also use a Taylor approximation of the data term  but use the
difference between E[ ˜f (Z)] computed exactly and and its estimator using reparameterization. They
use this control variate together with a base gradient estimate obtained via reparameterization.
Correcting for data subsampling: As discussed in Sec. 3.1 it is common with large datasets to
deﬁne estimators for the data term that only evaluate the likelihood on random subsets of data. To
reduce the variance introduced by this subsampling  Wang et al. [30] propose to approximate fd(z)
with a Taylor expansion in x  leading to an approximate data term ˜g1(z) = ∇w Eqw ED ˜fD(z). For
some models the inner expectation (over D) can be computed efﬁciently by caching the 1st and
2nd order empirical moments of the data. Since the outer expectation (over qw) usually remains
intractable  a ﬁnal control variate is obtained by applying one of the estimation methods described in
Sec. 3.1 (SF  RP  etc) to both fD(z) and ED fD(z) and taking the difference.
Both correction mechanisms described above represent particular scenarios that are included in
the proposed framework shown in Fig. 2  which also includes other control variates based on
approximations. First  it imposes no restrictions on other approximations  such as the ones based on
approximating the distribution qw instead of f. And second  it includes control variates based on the
difference of two estimates of an approximate general term  despite neither being CF. These two ideas
are used in the control variate introduced by Tucker et al. [28]  which use a continuous relaxation
[9  16] to approximate the distribution qw (discrete in this case)  and construct a control variate by
taking the difference between the SF and RP estimates of the resulting term based on the relaxation.

4

Following a similar idea  Grathwohl  et al. [7] use a neural network as a surrogate for f  and use as
control variate the difference between the SF and RP estimation of the term involving the surrogate.

3.3 Control variate from the score term (g4)

It’s easy to show that the score term is always zero  i.e. g4(w) = 0 (proof in appendix). Thus  it does
not need to be estimated. However  since it has expectation zero  one can use the naive control variate
T4 = ∇w log qw(Z)  Z ∼ qw [20  23].
4 Combining multiple control variates

In order to use control variates we need to deﬁne a base gradient estimator h(w) ∈ RD and a set of
control variates  {c1  ...  cL}  ci ∈ RD  that we want to use to reduce the base gradient’s variance. We
multiply each control variate ci with a scalar weight ai to get the estimator

ˆg(w) = h(w) +

ai ci(w).

L�i=1

(4)

Deﬁning a ∈ RL as the vector of weights and C ∈ RD×L as the matrix with ci as the i-th column  ˆg
can be equivalently expressed as

ˆg(w) = h(w) + C(w)a.

(5)

The goal is to ﬁnd a such that the ﬁnal gradient has low variance. This follows from theoretical results
on stochastic optimization with a ﬁrst-order unbiased gradient oracle that indicate that convergence is
governed by the expected squared norm E�ˆg�2 of the gradient oracle [1]  which is equivalent (up to
a constant) to the trace of the variance. In particular  in the case in which the CVs are all differences
between unbiased estimators for different terms  ﬁnding the optimal a is equivalent to ﬁnding the
best afﬁne combination of the estimators.2
Lemma 4.1. Let h(w) ∈ RD be a random variable  C(w) ∈ RL×D a matrix of random variables
such that each element has mean zero. For a ∈ RL  deﬁne ˆg(w) = h(w) + C(w)a. The value of a
that minimizes E�ˆg(w)�2 for a given w is

a∗(w) = − E

p(C h|w)�C T C�−1 E�C T h�.

(6)

Variants of this result are known [30]. Of course  this requires the expectations E[C T C] and E[C T h] 
which are usually not available in closed form. One solution is  given some observed gradients
h1  ...  hM and control variates C1  ...  CM   to estimate a∗ using empirical expectations in place of
the true ones. However  this approach does not account for how errors in the estimates of these
expectations affect a and therefore the ﬁnal variance of ˆg.

4.1 Bayesian regularization

We deal with this problem from a "risk minimization" perspective. We imagine that the joint
distribution over C and h is governed by some (unknown) parameter vector θ. Then  we can deﬁne
the loss for selecting the vector of weights a when the true parameter vector is θ as

We seek a "decision rule"

L(a  θ) = E

C h|θ �h + Ca�2.

α(C1  h1  ...  CM   hM )

that takes as input a "minibatch" of M evaluations of h and C and returns a weight vector a. Then 
for a pre-speciﬁed probabilistic model p(C  h  θ)  we can deﬁne the Bayesian regret as

2Intuitively  given two estimators  if one is used as the base estimator and the difference as a CV  then ﬁnding

the best weight for that CV is equivalent to ﬁnding the best mixture of the estimators.

5

BayesRegret(α) = E
θ

E

C1 h1 ... CM  hM|θ

[L (α(C1  h1  ...  CM   hM )  θ)] .

The following theorem shows that if we model p(C  h|θ) jointly as a Gaussian with canonical
parameters θ = (η  Λ)  and use a Normal-Wishart prior for p(θ)  then the decision rule α minimizing
the Bayesian risk ends up being similar to Eq. 6  with two modiﬁcations. First  the unknown
expectations are replaced with empirical expectations. Second  the empirical expectation of C T C is
"regularized" by a term determined by the prior. For simplicity  the following result is stated assuming
that the Normal-Wishart prior uses V0 being a constant times the identity. However  in the appendix
we state (and prove) a more general result where V0 is arbitrary. This can also be implemented
efﬁciently  although the result is more clumsy to state.
Theorem 4.1. If p(C  h|θ) is a Gaussian parameterized as

p(C  h|θ = (η  Λ)) = Gaussian� [vec(C)  h]���µ = Λ−1η  Σ = Λ−1� 

and the prior is a Normal-Wishart  parameterized as p(θ = (η  Λ)) ∝ exp(tT
n0A(η  Λ))  then the decision rule that minimizes the Bayesian regret for V0 = v0I is

0 η − trace(V T

0 Λ) −

C T h

(7)

α∗(C1  h1  ...  CM   hM ) = −� d v0
M�M

m and C T h = 1

I + C T C�−1
M�M

M

mhm.

m=1 C T

m=1 CmC T

Where h ∈ Rd  C T C = 1
The proof idea is as follows: Since the loss is the expected squared norm  the optimal decision rule
can be reduced to a form similar to Eq. 6 but with the expectations replaced by posterior expectations
conditioned on the observations C1  ...  CM and h1  ...  hM . For exponential families with conjugate
priors (e.g. the Gaussian with a Normal-Wishart prior)  the posterior expectation of sufﬁcient statistics
given observations has a simple closed-form solution [10]. The sufﬁcient statistics for the Gaussian
are the ﬁrst and second joint moments of [vec(C)  h]  from which the expectations needed for the
optimal decision rule can be extracted.
The rule in Eq. 7 is surprisingly simple: just compute the empirical averages and add a diagonal
regularizer before solving the linear system. Using a large M provides better estimates for the
expectation and thus reduces the amount of “regularization” applied  while using a small M provides
worse estimates  which are regularized more heavily.

4.2 Empirical Averages

The probabilistic model described above does not explicitly mention the parameters w. One way
to use this would be to apply it separately in each iteration. It is desirable  however  to exploit
the fact that the parameters change slowly during learning. Algorithmically  the procedure above
requires as input only empirical expectations for C T C and C T h. Instead of using samples from a
single step alone  we propose using an exponential average. At every step we compute a weighted
average of the previous empirical expectation and the current one. This results in the update rule
Et = (1 − γ)Et−1 + γ ˆEt  ; γ ∈ [0  1] where E represents either C T C or C T h  and ˆEt is the
empirical average obtained using the samples drawn at step t. To combine this with the Bayesian
regularization procedure  we use an “effective M”  Mef f = B�T
t=1(1 − γ)t  which indicates how
many samples are effectively being included in the empirical averages  where B is the minibatch size.
Mef f is used instead of M in equation 7. Technically  the regularization procedure assumes that the
samples for the empirical expectations are independent of those actually used for the ﬁnal gradient
estimate ˆg. To reﬂect this  we compute α at step t using the empirical average from step t − 1  Et−1.
5 Experiments and Results

We tried several control variates and the combination algorithm on a Bayesian binary logistic
regression model with a standard Gaussian prior  using three well known datasets: ionosphere 
australian  and sonar. We use simple SGD with momentum (β = 0.9) as our optimization algorithm 

6

Figure 3: For each dataset  optimization results for different gradients with different learning rates.
Legends indicate what control variates are used together with the base gradient. The right column
shows results with the best learning rate retrospectively selected for each iteration. For clarity we
limit the y-axis of the plots  which leaves some of the results (worst ones) out of the range being plot.

minibatches of size 10  a decay factor of γ = 0.02 for the exponentially decayed empirical averages 
and v0 = 10−3  value based on results obtained for the sensitivity analysis carried out (see Sec. 5.1).
We chose a full covariance Gaussian as variational distribution qw(z) parameterized using the mean
and a Cholesky factorization of the covariance. Since both the prior and the variational distribution
are Gaussian  the prior and variational terms can be computed in closed form.
As base gradient we use what seems to be the most common estimator  with reparameterization (RP1)
to estimate the data term g1 (with the local reparameterization trick [12]) and the prior term g2  and
a closed form expression for the variational/entropy term g3. Here  RP1 is the reparameterization

estimator using T (�; w) = Cholesky(Σw)� + µw  while RP2 uses T (�; w) = √Σw� + µw [14] with

the matrix square root. For CVs  we chose to use the following seven  which provide a reasonable
coverage of the different methods described in Section 3:

• c1: The difference between the RP1 and closed-form estimates of the variational term.
• c2: The difference between the RP1 and closed-form estimates of the prior term.
• c3: The difference between the RP1 and RP2 estimates of the prior term.
• c4: The difference between the RP1 and RP2 estimates of the data term.
• c5: Taylor expansion of the RP1 estimate of the data term  correcting for data subsampling [30].
• c6: Taylor expansion of the RP2 estimate of the data term  correcting for data subsampling [30].
• c7: Taylor expansion of the RP1 estimate of the data term  correcting for sampling from qw(z).
This control variate is based on the work of Miller  et al [17]  but adapted to a full covariance
(rather than diagonal) Gaussian (see appendix).

We compare the optimization results obtained using the base gradient alone and the base gradient com-
bined with different subsets of CVs  which were chosen following a simple approach: We tried each

7

������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������Figure 4: ELBO after 500 iterations for each gradient vs. learning rate (legends as in Fig. 3).

CV in isolation  and chose the four worst performing ones as one subset  the ﬁve worst performing
ones as another subset  and so on. The ﬁnal subsets of CVs obtained this way are S4 = {c2  c1  c3  c4} 
S5 = {c2  c1  c3  c4  c6}  S6 = {c2  c1  c3  c4  c6  c5}  and S7 = {c2  c1  c3  c4  c6  c5  c7}. We also
show results for the two best control variates  c5 and c7  used in isolation. All the results shown in
this section  ﬁgures and tables  were obtained averaging the results from 50 runs.
Best learning rate. Table 1 shows the ELBO value achieved after 500 iterations  with the largest
learning rate3 for which optimization converged with at least one estimator. It can be seen that
increasing the number of CVs often leads to higher ﬁnal values for the ELBO and that  in all cases 
the higher ELBOs (better) were achieved by using all CVs together.

Table 1: Average ELBO achieved after 500 iterations for each dataset using the base gradient with
different subsets of control variates and particular learning rates (lr).

-

S4

Control variates used

Dataset (lr)
S7
Ion. (0.4)
−110.1 −75.6
−72
Aus. (0.4)
−251.8 −259.4 −254.4
Sonar (0.2) −442.4 −270.2 −149.1 −148.3 −117.1 −200.6 −120.2

S5
−157.3 −112.5 −85.3
−378.2 −357.2 −255.1

S6
−85.3
−255

c5

c7

Comparing across learning rates. Now we compare the performance achieved using each gradient
estimator with different learning rates. To do so we present two sets of images. First  the two leftmost
columns of Fig. 3 show  for each dataset  the ELBO vs. iterations for two different learning rates;
while the third column shows  for each gradient estimator and iteration  the ELBO for the best
learning rate (vs. iteration). As in Table 1 it can be seen that for a given learning rate (or when
choosing the best at each iteration) the gradients that combine more control variates are better suited
for optimization and display a strictly dominant performance.
Finally  Fig. 4 shows  for several gradients  the ﬁnal ELBO (after 500 iterations) vs. learning rate used 
providing a systematic comparison of how the gradient estimates perform with different learning rates.
Again  estimates employing more CVs display a dominant performance  with larger improvements at
larger learning rates. Furthermore  the “best” learning rate increases with better estimators.

5.1 Sensitivity analysis

It is natural to ask how the variance of the gradient estimate is related to the choice of the prior
parameter v0 and the minibatch size M. Recall from Thm. 4.1 that a larger value of v0 corresponds
to a more concentrated prior  and is thus a more conservative choice – essentially it results in more
"regularization" of the empirical moments. To answer this we carried out a simple experiment 
where we ﬁx w and estimate E||ˆg(w)||2 with a variety of v0 and M. To choose w  we applied SGD
with a low-variance gradient (computed with many samples)  and a learning rate of 0.08 and same
initialization as in the previous section  and selected the parameters found after 25 iterations. This is
intended to be "typical"  in that it is neither at the start nor the end of optimization.
Estimating ˆg(w) is a three step process: (1) Use one set of evaluations of C and h to estimate
E[C T C] and E[C T h]. (2) Apply the prior to compute a from those estimate (Eq. 7). Recall from

3The loss is normalized by the number of samples in the dataset. If it was not the equivalent learning rates

would be smaller

8

����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������Figure 5: Expected squared norm of the gradient estimate vs v0  for different minibatch sizes. The
two left most images were obtained with estimates of E[C T C] and E[C T h] using the current weights
w  while for the image on the right moments were estimated using gradients from an older iteration.
The sonar and australian datasets give results similar to those of ionosphere.

Thm. 4.1 that a larger v0 corresponds to a more concentrated prior  essentially "regularizing" more.
(3) Use a second set of evaluations of C and h to compute ˆg(w)  using weights a (Eq. 4 / 5).
In a ﬁrst experiment  we tested exactly that procedure  drawing two independent evaluations of C
and h using the current weights w. Results are shown in Figure 5. We found a small artiﬁcial dataset
illustrative  with samples x ∈ R2. For this "2D" dataset  with small minibatches  a fairly large value
of v0 provided the best results. However with ionosphere  even a very small v0 tended to perform
well.
For efﬁciency  our logistic regression experiments used exponential averaging over previous iterations
to estimate E[C T C] and E[C T h]  rather than drawing two evaluations at each iteration. So  even
with large value of M these are not fully reliable. To roughly simulate this  we performed a second
"lagged" experiment estimating E[C T C] and E[C T h] from evaluations of C and h at the weight
from 10 iterations previous during SGD. (This was chosen considering the "average age" of gradients
when using exponential averaging  and that 0.08 is a relatively small learning rate.) The results of
this are shown on the right of Fig. 5. Lagged evaluations result in stochastic gradients with more
variance  with a different dependence on v0. (Note  however  that the gradient remains unbiased 
lagging is cheaper  and that all estimators have a variance decreasing with M.)
We emphasize that several somewhat arbitrary decisions were made for these experiments  such as
the learning rate  the choice of iteration  the amount of "lag". However  we believe that the results
illustrate an important phenomenon related to the use of regularization: when using past gradient
information (as exponential averaging does) larger values of v0 are beneﬁcial and result in gradients
with lower variance. While intuitively plausible  note that this beneﬁt of regularization for countering
errors introduced by the use of old gradients is not really captured by our theoretical analysis in
Section 4 which is entirely based on "single-iteration" reasoning.

6 Conclusion

This work focuses on how to obtain low variance gradients given a ﬁxed set of control variates. We
ﬁrst present a uniﬁed view that attempts to explain how most control variates used for variational
inference are derived  which sheds light on the large number of CVs available. We then propose
a combination algorithm to use multiple control variates in concert. We show experimentally that 
given a set of control variates  the combination algorithm provides a simple and effective combination
rule that leads to gradients with less variance than those obtained using a reduced number of CVs (or
no CVs at all). The algorithm assumes that a ﬁxed set of control variates to be used is given  and
minimizes the ﬁnal gradient’s variance using them  without analyzing how favorable using all the
CVs actually is. A “smarter” algorithm could  for instance  decide whether to use all the CVs given
or a just a subset. We leave the development of such algorithm for future work.

9

�������������������������������������������������������������������������������������������������������������������������������������������������References
[1] Alekh Agarwal  Peter L. Bartlett  Pradeep Ravikumar  and Martin J. Wainwright. Information-
theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans.
Information Theory  58(5):3235–3249  2012.

[2] David M Blei  Alp Kucukelbir  and Jon D McAuliffe. Variational inference: A review for

statisticians. Journal of the American Statistical Association  112(518):859–877  2017.

[3] David M Blei  Andrew Y Ng  and Michael I Jordan. Latent dirichlet allocation. Journal of

machine Learning research  3(Jan):993–1022  2003.

[4] Edward Challis and David Barber. Concave gaussian variational approximations for inference in
large-scale bayesian linear models. In Proceedings of the Fourteenth International Conference
on Artiﬁcial Intelligence and Statistics  pages 199–207  2011.

[5] Otto Fabius and Joost R van Amersfoort. Variational recurrent auto-encoders. arXiv preprint

arXiv:1412.6581  2014.

[6] Thomas Furmston and David Barber. Variational methods for reinforcement learning.

In
Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics 
pages 241–248  2010.

[7] Will Grathwohl  Dami Choi  Yuhuai Wu  Geoff Roeder  and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123  2017.

[8] Matthew D Hoffman  David M Blei  Chong Wang  and John Paisley. Stochastic variational

inference. The Journal of Machine Learning Research  14(1):1303–1347  2013.

[9] Eric Jang  Shixiang Gu  and Ben Poole. Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144  2016.

[10] Michael I. Jordan. The exponential family: Conjugate priors. https://people.eecs.

berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf.

[11] Michael I Jordan  Zoubin Ghahramani  Tommi S Jaakkola  and Lawrence K Saul. An in-
troduction to variational methods for graphical models. Machine learning  37(2):183–233 
1999.

[12] Diederik P Kingma  Tim Salimans  and Max Welling. Variational dropout and the local
reparameterization trick. In Advances in Neural Information Processing Systems  pages 2575–
2583  2015.

[13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[14] Steven Cheng-Xian Li and Benjamin M. Marlin. A scalable end-to-end gaussian process adapter
for irregularly sampled time series classiﬁcation. In Advances in Neural Information Processing
Systems  pages 1804–1812  2016.

[15] Percy Liang  Slav Petrov  Michael Jordan  and Dan Klein. The inﬁnite pcfg using hierarchical
dirichlet processes.
In Proceedings of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational Natural Language Learning (EMNLP-
CoNLL)  2007.

[16] Chris J Maddison  Andriy Mnih  and Yee Whye Teh. The concrete distribution: A continuous

relaxation of discrete random variables. arXiv preprint arXiv:1611.00712  2016.

[17] Andrew C Miller  Nicholas J Foti  Alexander D’Amour  and Ryan P Adams. Reducing

reparameterization gradient variance. arXiv preprint arXiv:1705.07880  2017.

[18] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks.

arXiv preprint arXiv:1402.0030  2014.

10

[19] John Paisley  David Blei  and Michael Jordan. Variational bayesian inference with stochastic

search. arXiv preprint arXiv:1206.6430  2012.

[20] Rajesh Ranganath  Sean Gerrish  and David Blei. Black box variational inference. In Artiﬁcial

Intelligence and Statistics  pages 814–822  2014.

[21] Rajesh Ranganath  Linpeng Tang  Laurent Charlin  and David Blei. Deep exponential families.

In Artiﬁcial Intelligence and Statistics  pages 762–771  2015.

[22] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082  2014.

[23] Geoffrey Roeder  Yuhuai Wu  and David Duvenaud. Sticking the landing: An asymptotically
zero-variance gradient estimator for variational inference. arXiv preprint arXiv:1703.09194 
2017.

[24] Francisco Ruiz  Titsias Michalis  and David Blei. The generalized reparameterization gradient.

In Advances in Neural Information Processing Systems  pages 460–468  2016.

[25] Francisco JR Ruiz  Michalis K Titsias  and David M Blei. Overdispersed black-box variational

inference. arXiv preprint arXiv:1603.01140  2016.

[26] Michalis Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational bayes for non-
conjugate inference. In Proceedings of the 31st International Conference on Machine Learning
(ICML-14)  pages 1971–1979  2014.

[27] Michalis Titsias and Miguel Lázaro-Gredilla. Local expectation gradients for black box vari-
ational inference. In Advances in neural information processing systems  pages 2638–2646 
2015.

[28] George Tucker  Andriy Mnih  Chris J Maddison  John Lawson  and Jascha Sohl-Dickstein.
In

Rebar: Low-variance  unbiased gradient estimates for discrete latent variable models.
Advances in Neural Information Processing Systems  pages 2627–2636  2017.

[29] Martin J Wainwright  Michael I Jordan  et al. Graphical models  exponential families  and
variational inference. Foundations and Trends R� in Machine Learning  1(1–2):1–305  2008.
[30] Chong Wang  Xi Chen  Alexander J Smola  and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems  pages 181–189 
2013.

[31] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine learning  8(3-4):229–256  1992.

[32] John Winn and Christopher M Bishop. Variational message passing. Journal of Machine

Learning Research  6(Apr):661–694  2005.

[33] Cheng Zhang  Judith Butepage  Hedvig Kjellstrom  and Stephan Mandt. Advances in variational

inference. arXiv preprint arXiv:1711.05597  2017.

11

,Tomas Geffner
Justin Domke