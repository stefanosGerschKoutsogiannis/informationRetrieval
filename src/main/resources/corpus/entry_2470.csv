2013,Higher Order Priors for Joint Intrinsic Image  Objects  and Attributes Estimation,Many methods have been proposed to recover the intrinsic scene properties such as shape  reflectance and illumination from a single image. However  most of these models have been applied on laboratory datasets. In this work we explore the synergy effects between intrinsic scene properties recovered from an image  and the objects and attributes present in the scene. We cast the problem in a joint energy minimization framework; thus our model is able to encode the strong correlations between intrinsic properties (reflectance  shape  illumination)  objects (table  tv-monitor)  and materials (wooden  plastic) in a given scene. We tested our approach on the NYU and Pascal datasets  and observe both qualitative and quantitative improvements in the overall accuracy.,Higher Order Priors for Joint Intrinsic Image 

Objects  and Attributes Estimation

Vibhav Vineet

Oxford Brookes University  UK

vibhav.vineet@gmail.com

Carsten Rother

TU Dresden  Germany

carsten.rother@tu-dresden.de

Philip H.S. Torr

University of Oxford  UK
philip.torr@eng.ox.ac.uk

Abstract

Many methods have been proposed to solve the problems of recovering intrinsic
scene properties such as shape  reﬂectance and illumination from a single image 
and object class segmentation separately. While these two problems are mutually
informative  in the past not many papers have addressed this topic. In this work we
explore such joint estimation of intrinsic scene properties recovered from an im-
age  together with the estimation of the objects and attributes present in the scene.
In this way  our uniﬁed framework is able to capture the correlations between
intrinsic properties (reﬂectance  shape  illumination)  objects (table  tv-monitor) 
and materials (wooden  plastic) in a given scene. For example  our model is able to
enforce the condition that if a set of pixels take same object label  e.g. table  most
likely those pixels would receive similar reﬂectance values. We cast the problem
in an energy minimization framework and demonstrate the qualitative and quanti-
tative improvement in the overall accuracy on the NYU and Pascal datasets.

Introduction

1
Recovering scene properties (shape  illumination  reﬂectance) that led to the generation of an image
has been one of the fundamental problems in computer vision. Barrow and Tenebaum [13] posed
this problem as representing each scene properties with its distinct “intrinsic” images. Over the
years  many decomposition methods have been proposed [5  16  17]  but most of them focussed on
recovering a reﬂectance image and a shading1 image without explicitly modelling illumination or
shape. But in the recent years a breakthrough in the research on intrinsic images came with the works
of Barron and Malik [1-4] who presented an algorithm that jointly estimated the reﬂectance  the
illumination and the shape. They formulate this decomposition problem as an energy minimization
problem that captures prior information about the structure of the world.
Further  recognition of objects and their material attributes is central to our understanding of the
world. A great deal of work has been devoted to estimating the objects and their attributes in the
scene: Shotton et.al. [22] and Ladicky et.al. [9] propose approaches to estimate the object labels at
the pixel level. Separately  Adelson [20]  Farhadi et.al. [6]  Lazebnik et.al. [23] deﬁne and estimate
the attributes at the pixel  object and scene levels. Some of these attributes are material properties
such as woollen  metallic  shiny  and some are structural properties such as rectangular  spherical.
While these methods for estimating the intrinsic images  objects and attributes have separately been
successful in generating good results on laboratory and real-world datasets  they fail to capture the
strong correlation existing between these properties. Knowledge about the objects and attributes
in the image can provide strong prior information about the intrinsic properties. For example  if a
set of pixels takes the same object label  e.g. table  most likely those pixels would receive similar
reﬂectance values. Thus recovering the objects and their attributes can help reduce the ambiguities
present in the world leading to better estimation of the reﬂectance and other intrinsic properties.

1shading is the product of some shape and some illumination model which includes effects such as shadows 

indirect lighting etc.

1

Input Image

Input Depth Image

Reﬂectance

Shading

Depth

Object

Attributes

Object-color coding

Attribute-color coding
Figure 1: Given a RGBD image  our algorithm jointly estimates the intrinsic properties such as
reﬂectance  shading and depth maps  along with the per-pixel object and attribute labels.

Additionally such a decomposition might be useful for per-pixel object and attribute segmentation
tasks. For example  using reﬂectance (illumination invariant) should improve the results-when esti-
mating per-pixel object and attribute labels [24]. Moreover if a set of pixels have similar reﬂectance
values  they are more likely to have the same object and attribute class.
Some of the previous research has looked at the correlation of objects and intrinsic properties by
propagating results from one step to the next. Osadchy et.al. [18] use specular highlights to improve
recognition of transparent  shiny objects. Liu et.al. [15] recognize material categories utilizing the
correlation between the materials and their reﬂectance properties (e.g. glass is often translucent).
Weijer et.al. [14] use knowledge of the objects present in the scene to better separate the illumination
from the reﬂectance images. However  the problem with these approaches is that the errors in one
step can propagate to the next steps with no possibility of recovery. Joint estimation of the intrinsic
images  objects and attributes can be used to overcome these issues. For instance  in the context of
joint object recognition and depth estimation such positive synergy effects have been shown in e.g.
[8].
In this work  our main contribution is to explore such synergy effects existing between the intrinsic
properties  objects and material attributes present in a scene (see Fig. 1). Given an image  our
algorithm jointly estimates the intrinsic properties such as reﬂectance  shading and depth maps 
along with per-pixel object and attribute labels. We formulate it in a global energy minimization
framework  and thus our model is able to enforce the consistency among these terms. Finally 
we use an approximate dual decomposition based strategy to efﬁciently perform inference in the
joint model consisting of both the continuous (reﬂectance  shape and illumination) and discrete
(objects and attributes) variables. We demonstrate the potential of our approach on the aNYU and
aPascal datasets  which are extended versions of the NYU [25] and Pascal [26] datasets with per-
pixel attribute labels. We evaluate both the qualitative and quantitative improvements for the object
and attribute labelling  and qualitative improvement for the intrinsic images estimation.
We introduce the problem in Sec. 2. Section 3 provides details about our joint model  section 4
describes our inference and learning  Sec. 5 and 6 provide experimentation and discussion.
2 Problem Formulation
Our goal is to jointly estimate the intrinsic properties of the image  i.e.
reﬂectance  shape and
illumination  along with estimating the objects and attributes at the pixel level  given an image
array ¯C = ( ¯C1... ¯CV ) where ¯Ci ∈ R3 is the ith pixel’s associated RGB value in the image with
i ∈ V = {1...V }. Before going into the details of the joint formulation  we consider the formulations
for independently solving these problems. We ﬁrst brieﬂy describe the SIRFS (shape  illumination
and reﬂectance from shading) model [2] for estimating the intrinsic properties for a single given
object  and then a CRF model for estimating objects  and attributes [12].
2.1 SIRFS model for a single  given object mask
We build on the SIRFS model [2] for estimating the intrinsic properties of an image. They formu-
late the problem of recovering the shape  illumination and reﬂectance as an energy minimization
problem given an image. Let R = (R1...RV )  Z = (Z1...ZV ) be the reﬂectance  and depth maps
respectively  where Ri ∈ R3 and Zi ∈ R3  and the illumination L be a 27-dimensional vector of
spherical harmonics [10]. Further  let S(Z  L) be a function that generates a shading image given
the depth map Z and the illumination L. Here Si ∈ R3 and subsumes all light-dependent properties 
e.g. shadows  inter-reﬂections (refer to [2] for details). The SIRFS model then minimizes the energy

minimizeR Z L ESIRFS = ER(R) + EZ(Z) + EL(L)

¯C = R · S(Z  L)

(1)

subject to

2

where ”·” represents componentwise multiplication  and ER(R)  EZ(Z) and EL(L) are the costs
for the reﬂectance  depth and illumination respectively. The most likely solution is then estimated by
using a multi-scale L-BFGS  a limited-memory approximation of the Broyden-Fletcher-Goldfarb-
Shanno algorithm [2]  strategy which in practice ﬁnds better local optima than other gradient descent
strategies. The SIRFS model is limited to estimating the intrinsic properties for a single object mask
within an image. The recently proposed Scene-SIRFS model [4] proposes an approach to recover
the intrinsic properties of whole image by embedding a mixture of shapes in a soft segmentation
of the scene. In Sec. 3 we will also extend the SIRFS model to handle multiple objects. The main
difference to Scene-SIRFS is that we perform joint optimization over the object (and attributes)
labelling and intrinsic image properties per-pixel.
2.2 Multilabel Object and Attribute Model
The problem of estimating the per-pixel objects and attributes labels can also be formulated in a
CRF framework [12]. Let O = (O1...OV ) and A = (A1...AV ) be the object and attribute variables
associated with all V pixels  where each object variable Oi takes one out of K discrete labels such as
table  monitor  or ﬂoor. Each attribute variable Ai takes a label from the power set of the M attribute
labels  for example the subset of attribute labels can be Ai = {red  shiny  wet}. Efﬁcient inference
i ∈
is performed by ﬁrst representing each attributes subset Ai by M binary attribute variables Am
{0  1}  meaning that Am
i = 0.
Under this assumption  the most likely solution for the objects and the attributes correspond to
minimizing the following energy function

i = 1 if the ith pixel takes the mth attribute and it is absent when Am

EOA(O  A) =

ψi(Oi) +

ψi m(Am

i )+

ψij(Oi  Oj)+

ψij(Am

i   Am
j )

(2)

(cid:88)

i∈V

(cid:88)

(cid:88)

m

i∈V

(cid:88)

i<j∈V

(cid:88)

(cid:88)

i<j∈V

m

i   Am

i ) are the object and per-binary attribute dependent unary terms respec-
Here ψi(Oi) and ψi m(Am
tively. Similarly  ψij(Oi  Oj) and ψij(Am
j ) are the pairwise terms deﬁned over the object and
per-binary attribute variables. Finally the best conﬁguration for the object and attributes are esti-
mated using a mean-ﬁeld based inference approach [12]. Further details about the form of the unary 
pairwise terms and the inference approach are described in our technical report [29].
3 Joint Model for Intrinsic Images  Objects and Attributes
Now  we provide the details of our formulation for jointly estimating the intrinsic images (R  Z  L)
along with the objects (O) and attribute (A) properties given an image ¯C in a probabilistic frame-
work. We deﬁne the posterior probability and the corresponding joint energy function E as:
P (R  Z  L  O  A|I) = 1/Z(I) exp{−E(R  Z  L  O  A|I)}
E(R  Z  L  O  A|I) = ESIRFSG(R  Z  L|O  A) +ERO(R  O)+ERA(R  A)+EOA(O  A)
(3)
We deﬁne ESIRFSG = ER(R) + EZ(Z) + EL(L)  a new global energy term. The terms
ERO(R  O) and ERA(R  A) capture correlations between the reﬂectance  objects and/or attribute
labels assigned to the pixels. These terms take the form of higher order potentials deﬁned on
the image segments or regions of pixels generated using unsupervised segmentation approach of
Felzenswalb and Huttenlocker [21]. Let S corresponds to the set of these image segments. These
terms are described in detail below.
3.1 SIRFS model for a scene
Given this representation of the scene  we model the scene speciﬁc ESIRFSG by a mixture of
reﬂectance  and depth terms embedded into the segmentation of the image and an illumination term
as:

¯C = R · S(Z  L)

subject to

ESIRFSG(R  Z  L|O  A) =

ER(Rc) + EZ(Zc)

+ EL(L)

(4)

(cid:17)

(cid:16)

(cid:88)

c∈S

where R = {Rc}  Z = {Zc}. Here ER(Rc) and EZ(Zc) are the reﬂectance and depth terms
respectively deﬁned over segments c ∈ S. In the current formulation  we have assumed that we have
a single model of illumination L for whole scene which corresponds to a 27-dimensional vector of
spherical harmonics [2].

3

3.2 Reﬂectance  Objects term
The joint reﬂectance-object energy term ERO(R  O) captures the relations between the objects
present in the scene and their reﬂectance properties. Our higher order term takes following form:

ERO(R  O) =

πc
oψ(Rc) +

πc
rψ(Oc)

(5)

(cid:88)

c∈S

(cid:88)

c∈S

oψ(Rc) is an object
where Rc  Oc are the labeling for the subset of pixels c respectively. Here πc
dependent quality sensitive higher order cost deﬁned over the reﬂectance variables  and πc
rψ(Oc) is
a reﬂectance dependent quality sensitive higher order cost deﬁned over the object variables. The term
ψ(Rc) reduces the variance of the reﬂectance values within a clique and takes the form ψ(Rc) =
(cid:107)c(cid:107)θα (θp + θvGr(c)) where

Gr(c) = exp
Here (cid:107)c(cid:107) is the size of the clique  µc =
and θα  θp  θv  θβ are constants. Further in order
to measure the quality of the reﬂectance assignment to the segment  we weight the higher order cost
ψ(Rc) with an object dependent πc
o takes
following form:

o that measures the quality of the segment. In our case  πc

i∈c Ri
(cid:107)c(cid:107)

(cid:107)c(cid:107)

(6)

.

(cid:26)1

λo

πc
o =

if Oi = l  ∀i ∈ c
otherwise

where λo < 1 is a constant. This term allows variables within a segment to take different reﬂectance
o gives rise to a
values if the pixels in that segment take different object labels. Currently the term πc
hard constraint on the penalty but can be extended to one that penalizes the cost softly as in [29].
Similarly we enforce higher order consistency over the object labeling in a clique c ∈ S. The term
ψ(Oc) takes the form of pattern-based P N -Potts model [7] as:

(cid:19)

i∈c(Ri − µc)2(cid:107)

(cid:107)(cid:80)

(cid:18)

−θβ
(cid:80)

(7)

(8)

(9)

(cid:26)γo

l
γo
max

if Oi = l  ∀i ∈ c
otherwise

if Gr(c) < K  ∀i ∈ c
otherwise

ψ(Oc) =

(cid:26)1

λr

πc
r =

l   γo

where γo
sensitive term πc
terms on all constituent pixels of a segment  i.e.  Gr(c) (deﬁne earlier). Thus πc
form:

max are constants. Further we weight this term with a reﬂectance dependent quality
r. In our experiment we measure this term based on the variance of reﬂectance
r takes following

where K and λr < 1 are constants. Essentially  this quality measurement allows the pixels within
a segment to take different object labels  if the variation in the reﬂectance terms within the segment
is above a threshold. To summarize  these two higher order terms enforce the cost of inconsistency
within the object and reﬂectance labels.
3.3 Reﬂectance  Attributes term
Similarly we deﬁne the term ERA(R  A) which enforces a higher order consistency between re-
ﬂectance and attribute variables. Such higher order consistency takes the following form:

(cid:88)

(cid:16)(cid:88)

m

c∈S

(cid:17)

(cid:88)

c∈S

ERA(R  A) =

πc
a mψ(Rc) +

rψ(Am
πc
c )

(10)

rψ(Am

a mψ(Rc) and πc

where πc
c ) are the higher order terms deﬁned over the reﬂectance image and
the attribute image corresponding to the mth attribute respectively. Forms of these terms are similar
to the one deﬁned for the object-reﬂectance higher order terms; these terms are further explained in
the supplementary material.
4
Given the above model  our optimization problem involves solving following joint energy function
to get the most likely solution for (R  Z  L  O  A):

Inference and Learning

E(R  Z  L  O  A|I) = ESIRFSG(R  Z  L) + ERO(R  O) + ERA(R  A) + EOA(O  A)

(11)

4

However  this problem is very challenging since it consists of both the continuous variables
(R  Z  L) and discrete variables (O  A). Thus in order to minimize the function efﬁciently with-
out losing accuracy we follow an approximate dual decomposition strategy [28].
We ﬁrst introduce a set of duplicate variables for the reﬂectance (R1  R2  R3)  objects (O1  O2) 
and attributes (A1  A2) and a set of new equality constraints to enforce the consistency on these
duplicate variables. Our optimization problem thus takes the following form:

minimize

R1 R2 R3 Z L O1 O2

E(R1  Z  L) + E(O1  A1) + E(R2  O2) + E(R3  A2)

subject to

(12)
From now on we have removed the subscripts and superscripts from the energy terms for simplicity
of the notations. Now we formulate it as an unconstrained optimization problem by introducing a
set of lagrange multipliers θ1
r   θo  θa and decompose the dual problem into four sub-problems as:

R1 = R2 = R3; O1 = O2; A1 = A2

r   θ2

E(R1  Z  L) + E(O1  A1) + E(R2  O2) + E(R3  A2) + θ1
r (R2 − R3) + θo(O1 − O2) + θa(A1 − A2)

+ θ2
= g1(R1  Z  L) + g2(O1  A1) + g3(O2  R2) + g4(A2  R3) 

r (R1 − R2)

(13)

where

g1(R1  Z  L) = minimizeR1 Z L E(R1  Z  L) + θ1
g2(O1  A1) = minimizeO1 A1 E(O1  A1) + θoO1 + θaA1
g3(O2  R2) = minimizeO2 R2 E(O2  R2) − θoO2 − θ1
r R2
g4(A2  R3) = minimizeA2 R3 E(A2  R3) − θaA2 − θ2
r R3

r R1

r   θ2

(14)
are the slave problems which are optimized separately and efﬁciently while treating the dual vari-
ables θ1
r   θo  θa constant  and the master problem then optimizes these dual variables to enforce
consistency. Next  we solve each of the sub-problems and the master problem.
Solving subproblem g1(R1  Z  L): Solving the sub-problem g1(R1  Z  L) requires optimizing
with only continuous variables (R1  Z  L). We follow a multi-scale LBFGS strategy [2] to opti-
mize this part. Each step of the LBFGS approach requires evaluating the gradient of g1(R1  Z  L)
wrt. R1  Z  L.
Solving subproblem g2(O1  A1): The second sub-problem g2(O1  A1) involves only discrete
variables (O1  A1). The dual variable dependent terms add θoO1 to the object unary potential
ψi(O1) and θaA1 to the attribute unary potential ψi(A1). Let ψ(cid:48)(O1) and ψ(cid:48)(A1) be the updated
object and attribute unary potentials. We follow a ﬁlter-based mean-ﬁeld strategy [11  12] for the op-
timization. In the mean-ﬁeld framework  given the true distribution P = exp(−g2(O1 A1))
  we ﬁnd
an approximate distribution Q  where approximation is measured in terms of the KL-divergence
between the P and Q distributions. Here ¯Z is the normalizing constant. Based on the model in
is a multi-class
Sec. 2.2  Q takes the form as Qi(O1
i m is a binary distribution over {0 1}. With this  the
distribution over the object variable  and QA
mean-ﬁeld updates for the object variables take the following form:

m)  where QO

i ) = QO

m QA

i m(Ai

i (O1

i   A1

¯Z

1

i

QO
i (O1

i = l) =

1
Z O
i

exp{−ψ(cid:48)

i(O1

QO
j (O1

j = l(cid:48))(ψij(O1

i   O1

j ))}

(15)

i )(cid:81)
(cid:88)

i ) − (cid:88)

l(cid:48)∈1..K

j(cid:54)=i

where ψij is a potts term modulated by a contrast sensitive pairwise cost deﬁned by a mixture of
Gaussian kernels [12]  and ZO
is per-pixel normalization factor. Given this form of the pairwise
terms  as in [12]  we can efﬁciently evaluate the pairwise summations in Eq. 15 using K Gaussian
convolutions. The updates for the attribute variables also take similar form (refer to the supplemen-
tary material).
Solving subproblems g3(O2  R2)  g4(A2  R3): These two problems take the following forms:

i

g3(O2  R2) = minimizeO2 R2

g4(A2  R3) = minimizeA2 R3

πc
r2 ψ(O2

c ) − θoO2 − θ1

(16)

r R2

(cid:17)−θaA2−θ2

r R3

πc
r3 ψ(A2 m

c

)

(cid:88)

(cid:88)
(cid:88)

c∈S

πc
o2 ψ(R2

c) +

(cid:16)(cid:88)

c∈S
πc
a2 mψ(R3

c)+

(cid:88)

c∈S

m

c∈S

5

Solving of these two sub-problems requires optimization with both the continuous R2 and discrete
O2  A2 variables respectively. However since these two sub-problems consist of higher order terms
(described in Eq. 8) and dual variable dependent terms  we follow a simple co-ordinate descent
strategy to update the reﬂectance and the object (and attribute) variables iteratively. The optimization
of the object (and attribute) variables are performed in a mean-ﬁeld framework  and a gradient
descent based approach is used for the reﬂectance variables.
Solving master problem The master problem then updates the dual-variables θ1
r   θo  θa given
r   θ2
r; the updates
the current solution from the slaves. Here we provide the update equations for θ1
for the other dual variables take similar form. The master calculates the gradient of the problem
E(R  Z  L  O  A|I) wrt. θ1

r  and then iteratively updates the values of θ1

r as:

(cid:17)

(cid:16)

θ1
r = θ1

r + α1
r

gθ1
1 (R1  Z  L) + gθ1

r

r

3 (O2  R2)

(17)

r

1   gθ1

r

r is the step size tth iteration and gθ1

3 are the gradients w.r.t. to the θ1

where αt
r. It should be noted
that we do not guarantee the convergence of our approach since the subproblems g1(.) and g2(.) are
solved approximately. Further details on our inference techniques are provided in the supplementary
material.
Learning:
In the model described above  there are many parameters joining each of these terms.
We use a cross-validation strategy to estimate these parameters in a sequential manner and thus
ensuring efﬁcient strategy to estimate a good set of parameters. The unary potentials for the objects
and attributes are learnt using a modiﬁed TextonBoost model of Ladicky et.al. [9] which uses a
colour  histogram of oriented gradient (HOG)  and location features.
5 Experiments
We demonstrate our joint estimation approach on both the per-pixel object and attribute labelling
tasks  and estimation of the intrinsic properties of the images. For the object and attribute labelling
tasks  we conduct experiments on the NYU 2 [25] and Pascal [26] datasets both quantitatively and
qualitatively. To this end  we annotate the NYU 2 and the Pascal datasets with per-pixel attribute
labels. As a baseline  we compare our joint estimation approach against the mean-ﬁeld based method
[12]  and the graph-cuts based α-expansion method [9]. We assess the accuracy in terms of the
overall percentage of the pixels correctly labelled  and the intersection/union score per class (deﬁned
in terms of the true/false positives/negatives for a given class as TP/(TP+FP+FN)). Additionally we
also evaluate our approach in estimating better intrinsic properties of the images though qualitatively
only  since it is extremely difﬁcult to generate the ground truths for the intrinsic properties  e.g.
reﬂectance  depth and illumination for any general image. We compare our intrinsic properties
results against the model of Barron and Malik2[2  4]  Gehler et.al. [5] and the Retinex model [17].
Further  only visually we also show how our approach is able to recover better smooth and de-noised
depth maps compared to the raw depth provided by the Kinect [25]. In all these cases  we use the
code provided by the authors for the AHCRF [9]  mean-ﬁeld approach [11  12]. Details of all the
experiments are provided below.
5.1
We ﬁrst conduct experiment on aNYU 2 RGBD dataset  an extended version of the indoor NYU
2 dataset [25]. The dataset consists of 725 training images  100 validation and 624 test images.
Further  the dataset consists of per-pixel object and attribute labels (see Fig. 1 and 3 for per-pixel
attribute labels). We select 15 object and 8 attribute classes that have sufﬁcient number of instances
to train the unary classiﬁer responses. The object labels corresponds to some indoor object classes
as ﬂoor  wall  .. and attribute labels corresponds to material properties of the objects as wooden 
painted  .... Further  since this dataset has depth from the Kinect depths  we use them to initialize
the depth maps Z for both our joint estimation approach and the Barron and Malik models [2-4].
We show quantitative and qualitative results in Tab. 1 and Fig. 3 respectively. As shown  our joint
approach achieves an improvement of almost 2.3%   and 1.2% in the overall accuracy and average
intersection-union (I/U) score over the model of AHCRF [9]  and almost 1.5 % improvement in the

aNYU 2 dataset

2We extended the SIRFS [2] model to our Scene-SIRFS using a mixture of reﬂectance and depth maps 
and a single illumination model. These mixtures of reﬂectance and depth maps were embedded in the soft
segmentation of the scene generated using the approach of Felzenswalb et.al. [21]. We call this model: Barron
and Malik [2 4].

6

Algorithm
AHCRF [9]

DenseCRF [12]
Ours (OA+Intr)

Av. I/U Oveall(% corr)
28.88
29.66
30.14

51.06
50.70
52.23

Algorithm
AHCRF [9]

DenseCRF [12]
Ours (OA+Intr)

21.9
22.02
24.175

40.7
37.6
39.25

Av. I/U Oveall(% corr)

(a) Object Accuracy

(b) Attribute Accuracy

Table 1: Quantitative results on aNYU 2 dataset for both the object segmentation (a)  and attributes
segmentation (b) tasks. The table compares performance of our approach (last line) against three
baselines. The importance of our joint estimation for intrinsic images  objects and attributes is
conﬁrmed by the better performance of our algorithm compared to the graph-cuts based (AHCRF)
method [9] and mean-ﬁeld based approach [12] for both the tasks. Here intersection vs. union (I/U)
is deﬁned as

T P +F N +F P and ’% corr’ as the total proportional of correctly labelled pixels.

T P

Input Image

our reﬂectance

our shading

our normals

our depth

reﬂectance [17]

reﬂectance[5]

Kinect depth

reﬂectance [2 4]

shading [2 4]

normals [2 4]

depth [2 4]

shading [17]

shading[5]

Input Image

our reﬂectance

our shading

our normals

our depth

reﬂectance [17]

reﬂectance[5]

Kinect depth

reﬂectance [2 4]

shading [2 4]

normals [2 4]

depth [2 4]

shading [17]

shading[5]

Figure 2: Given an image and its depth image for the aNYU dataset  these ﬁgures qualitatively com-
pare our algorithm in jointly estimating better the intrinsic properties such as reﬂectance  shading 
normals and depth maps. We compare against the model Barron and Malik [2 4]  the Retinex model
[17] (2nd last column) and the Gehler et.al. approach [5] (last column).

average I/U over the model of [12] for the object class segmentation . Similarly we also observe an
improvement of almost 2.2 % and 0.5 % in the overall accuracy and I/U score over AHCRF [12] 
and almost 2.1 % and 1.6 % in the overall accuracy and average I/U over the model of [12] for the
per-pixel attribute labelling task. These quantitative improvement suggests that our model is able to
improve the object and attribute labelling using the intrinsic properties information. Qualitatively
also we observe an improvement in the output of both the object and attribute segmentation tasks as
shown in Fig. 3.
Further  we show the qualitative improvement in the results of the intrinsic properties in the Fig. 2.
As shown our joint approach helps to recover better depth map compared to the noisy kinect depth
maps; justifying the uniﬁcation of reconstruction and objects and attributes based recognition tasks.
Further  our reﬂectance and shading images visually look much better than the models of Retinex
[17] and Gehler et.al. [5]  and similar to the Barron and Malik approach [2 4].
5.2
We also show experiments on aPascal dataset  our extended Pascal dataset with per-pixel attribute
labels. We select a subset of 517 images with the per-pixel object labels from the Pascal dataset and
annotate it with 7 material attribute labels at the pixel level. These attributes correspond to wooden 
skin  metallic  glass  shiny... etc. Further for the Pascal dataset we do not have any initial depth
estimate. Thus  we start with a depth map where each point in the space is given same constant
depth value.
Some quantitative and qualitative results are shown in Tab. 2 and Fig. 3 respectively. As shown  our
approach achieves an improvement of almost 2.0 % and 0.5 % in the I/U score for the object and

aPascal dataset

7

Algorithm
AHCRF [9]

DenseCRF [12]
Ours (OA + Intr)

Av. I/U Oveall(% corr)
32.53
36.9
38.1

82.30
79.4
81.4

Algorithm
AHCRF [9]

DenseCRF [12]
Ours (OA+Intr)

17.4
18.28
18.85

95.1
96.2
96.7

Av. I/U Oveall(% corr)

(a) Object Accuracy

(b) Attribute Accuracy

Table 2: Quantitative results on aPascal dataset for both the object segmentation (a)  and attributes
segmentation (b) tasks. The table compares performance of our approach (last line) against three
baselines. The importance of our joint estimation for intrinsic images  objects and attributes is
conﬁrmed by the better performance of our algorithm compared to the graph-cuts based (AHCRF)
method [9] and mean-ﬁeld based approach [12] for both the tasks. Here intersection vs. union (I/U)
is deﬁned as

T P +F N +F P and ’% corr’ as the total proportional of correctly labelled pixels.

T P

attribute labelling tasks respectively over the model of [12]. We observe qualitative improvement in
the accuracy shown in Fig. 3.

Input Image

Reﬂectance

Depth

Ground truth

Output [9]

Output [10]

Our Object

Our Attribute

NYU Object-color coding

Attribute-color coding

Figure 3: Qualitative results on aNYU (ﬁrst 2 lines) and aPascal (last line) dataset. From left to
right: input image  reﬂectance  depth images  ground truth  output from [9] (AHCRF)  output from
[12]  our output for the object segmentation. Last column shows our attribute segmentation output.
(Attributes for NYU dataset: wood  painted  cotton  glass  brick  plastic  shiny  dirty; Attributes for
Pascal dataset: skin  metal  plastic  wood  cloth  glass  shiny.)

6 Discussion and Conclusion
In this work  we have explored the synergy effects between intrinsic properties of an images  and
the objects and attributes present in the scene. We cast the problem in a joint energy minimization
framework; thus our model is able to encode the strong correlations between intrinsic properties
(reﬂectance  shape illumination)  objects (table  tv-monitor)  and materials (wooden  plastic) in a
given scene. We have shown that dual-decomposition based techniques can be effectively applied to
perform optimization in the joint model. We demonstrated its applicability on the extended versions
of the NYU and Pascal datasets. We achieve both the qualitative and quantitative improvements for
the object and attribute labeling  and qualitative improvement for the intrinsic images estimation.
Future directions include further exploration of the possibilities of integrating priors based on the
structural attributes such as slanted  cylindrical to the joint intrinsic properties  objects and attributes
model. For instance  knowledge that the object is slanted would provide a prior for the depth and
distribution of the surface normals. Further  the possibility of incorporating a mixture of illumination
models to better model the illumination in a natural scene remains another future direction.
Acknowledgements. This work was supported by the IST Programme of the European Commu-
nity  under the PASCAL2 Network of Excellence  IST-2007-216886. P.H.S. Torr is in receipt of
Royal Society Wolfson Research Merit Award.
References
[1] Barron  J.T. & Malik  J. (2012) Shape  albedo  and illumination from a single image of an unknown object.
In IEEE CVPR  pp. 334-341. Providence  USA.
[2] Barron  J.T. & Malik  J. (2012) Color constancy  intrinsic images  and shape estimation. In ECCV  pp.
57-70. Florence  Italy.

8

[3] Barron  J.T. & Malik  J. (2012) High-frequency shape and albedo from shading using natural image statis-
tics. In IEEE CVPR  pp. 2521-2528. CO  USA.
[4] Barron  J.  & Malik  J. (2013) Intrinsic scene properties from a single RGB-D image. In IEEE CVPR.
[5] Gehler  P.V.  Rother  C.  Kiefel  M.  Zhang  L. & Bernhard  S. (2011) Recovering intrinsic images with a
global sparsity prior on reﬂectance. In NIPS  pp. 765-773. Granada  Spain.
[6] Farhadi  A.  Endres  I.  Hoiem  D. & Forsyth D.A.  (2009) Describing objects by their attributes. In IEEE
CVPR  pp. 1778-1785. Miami  USA.
[7] Kohli  P.  Kumar  M.P.  & Torr  P.H.S. (2009) P & beyond: move making algorithms for solving higher
order functions. In IEEE PAMI  pp. 1645-1656.
[8] Ladicky  L.  Sturgess  P.  Russell C.  Sengupta  S.  Bastnlar  Y.  Clocksin  W.F.  & Torr P.H.S. (2012) Joint
optimization for object class segmentation and dense stereo reconstruction. In IJCV  pp. 739-746.
[9] Ladicky  L.  Russell C.  Kohli P. & Torr P.H.S.  (2009) Associative hierarchical CRFs for object class image
segmentation. In IEEE ICCV  pp. 739-746. Kyoto  Japan.
[10] Sloan  P.P.  Kautz  J.  & Snyder  J.  (2002) Precomputed radiance transfer for real-time rendering in dy-
namic  low-frequency lighting environments. In SIGGRAPH  pp. 527-536.
[11] Vineet  V.  Warrell J.  & Torr P.H.S.  (2012) Filter-based mean-ﬁeld inference for random ﬁelds with
higher-order terms and product label-spaces . In IEEE ECCV  pp. 31-44. Florence  Italy.
[12] Kr¨ahenb¨uhl P. & Koltun V.  (2011) Efﬁcient inference in fully connected CRFs with Gaussian edge poten-
tials. In IEEE NIPS  pp. 109-117. Granada  Spain.
[13] Barrow  H.G. & Tenenbaum  J.M. (1978) Recovering intrinsic scene characteristics from images. In A.
Hanson and E. Riseman  editors  Computer Vision Systems  pp. 3-26. Academic Press  1978.
[14] Weijer  J.V.d.  Schmid  C. & Verbeek  J. (2007) Using high-level visual information for color constancy.
In IEEE  ICCV  pp. 1-8.
[15] Liu  C.  Sharan  L.  Adelson  E.H.  & Rosenholtz  R. (2010) Exploring features in a bayesian framework
for material recognition. In IEEE  CVPR  pp. 239-246.
[16] Horn  B.K.P. (1970) Shape from shading: a method for obtaining the shape of a smooth opaque object
from one view. Technical Report  MIT.
[17] Land  E.H.  & McCann  J.J. (1971) Lightness and retinex theory. In JOSA.
[18] Osadchy  M.  Jacobs  D.W. & Ramamoorthi  R. (2003) Using specularities for recognition . In IEEE ICCV.
[19] Adelson  E.H. (2000) Lightness perception and lightness illusions. The New Cognitive Neuroscience  2nd
Ed. MIT Press  pp. 339-351.
[20] Adelson  E.H. (2001) On seeing stuff: the perception of materials by humans and machines. SPIE  vol.
4299  pp. 1-12.
[21] Felzenswalb  P.F.  & Huttenlocker  D.P. (2004) Efﬁcient graph-based image segmentation. In IJCV.
[22] Shotton  J.  Winn  J.  Rother  C.  & Criminisi  A. (2003) TextonBoost for Image Understanding: Multi-
Class Object Recognition and Segmentation by Jointly Modeling Texture  Layout  and Context. In IEEE IJCV.
[23] Tighe  J. & Lazebnik  S. (2011) Understanding scenes on many levels. In IEEE ICCV pp. 335-342.
[24] LeCun  Y.  Huang  F.J.  & Bottou  L. (2004) Learning methods for generic object recognition with invari-
ance to pose and lighting. In IEEE CVPR pp. 97-104.
[25] Silberman  N.  Hoim  D.  Kohli  P.  & Fergus  R. (2012) Indoor segmentation and support inference from
RGBD images. In IEEE ECCV pp. 746-760.
[26] Everingham  M.  Gool  L.J.V.  Williams  C.K.I.  Winn  J.M. & Zisserman  A. (2010) The pascal visual
object classes (VOC) challenge. In IEEE IJCV pp. 303-338.
[27] Cheng  M. M.  Zheng  S.  Lin  W.Y.  Warrell  J.  Vineet  V.  Sturgess  P.  Mitra  N.  Crook  N.  & Torr 
P.H.S. (2013) ImageSpirit: Verbal Guided Image Parsing. Oxford Brookes Technical Report.
[28] Domj  Q. T.  Necoara  I.  & Diehl  M. (2013) Fast Inexact Decomposition Algorithms for Large-Scale
Separable Convex Optimization. In JOTA.
[29] Kohli  P.  Ladicky  L.  & Torr  P.H.S. (2008) on. In IEEE CVPR  2008.

9

,Vibhav Vineet
Carsten Rother
Philip Torr
Changyou Chen
Jun Zhu
Xinhua Zhang
Sida Wang
Arun Tejasvi Chaganty
Percy Liang
Gamaleldin Elsayed
Simon Kornblith
Quoc Le