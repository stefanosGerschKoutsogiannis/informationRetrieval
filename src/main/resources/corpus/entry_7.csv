2019,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs,We consider the graph matching/similarity problem of determining how similar two given graphs $G_0 G_1$ are and recovering the permutation $\pi$ on the vertices of $G_1$ that minimizes the symmetric difference between the edges of $G_0$ and $\pi(G_1)$. Graph matching/similarity has applications for pattern matching  vision  social network anonymization  malware analysis  and more. We give the first efficient algorithms proven to succeed in the correlated Erdös-Rényi model (Pedarsani and Grossglauser  2011). Specifically  we give a polynomial time algorithm for the graph similarity/hypothesis testing task which  works for every constant level of correlation between the two graphs that can be arbitrarily close to zero. We also give a quasi-polynomial ($n^{O(\log n)}$ time)  algorithm for the graph matching task of recovering the permutation minimizing the symmetric difference in this model.  This is the first algorithm to do so without requiring as additional input a ``seed'' of the values of the ground truth permutation  on at least $n^{\Omega(1)}$ vertices. Our algorithms follow a general framework of counting the occurrences of subgraphs from a particular family of graphs allowing for tradeoffs between efficiency and accuracy.,(Nearly) Efﬁcient Algorithms for the Graph Matching

Problem on Correlated Random Graphs

School of Engineering and Applied Science

School of Engineering and Applied Science

Boaz Barak∗

Harvard University

Cambridge  MA  02138

b@boazbarak.org

Zhixian Lei∗

Chi-Ning Chou∗

Harvard University

Cambridge  MA  02138

chiningchou@g.harvard.edu

Tselil Schramm∗

School of Engineering and Applied Science

School of Engineering and Applied Science

Harvard University

Cambridge  MA  02138

leizhixian.research@gmail.com

Harvard University

Cambridge  MA  02138

tselil@seas.harvard.edu

Yueqi Sheng∗

School of Engineering and Applied Science

Harvard University

Cambridge  MA  02138
ysheng@g.harvard.edu.

Abstract

We consider the graph matching/similarity problem of determining how similar two
given graphs G0  G1 are and recovering the permutation π on the vertices of G1 that
minimizes the symmetric difference between the edges of G0 and π(G1). Graph
matching/similarity has applications for pattern matching  computer vision  social
network anonymization  malware analysis  and more. We give the ﬁrst efﬁcient
algorithms proven to succeed in the correlated Erdös-Rényi model (Pedarsani
and Grossglauser  2011). Speciﬁcally  we give a polynomial time algorithm for
the graph similarity/hypothesis testing task which works for every constant level
of correlation between the two graphs that can be arbitrarily close to zero. We
also give a quasi-polynomial (nO(log n) time) algorithm for the graph matching
task of recovering the permutation minimizing the symmetric difference in this
model. This is the ﬁrst algorithm to do so without requiring as additional input a
“seed” of the values of the ground truth permutation on at least nΩ(1) vertices. Our
algorithms follow a general framework of counting the occurrences of subgraphs
from a particular family of graphs allowing for tradeoffs between efﬁciency and
accuracy.

1

Introduction

The graph matching and graph similarity problems are well-studied computational problems with
applications in a great many areas. Some examples include machine learning [1]  computer vi-
sion [2]  pattern recognition [3]  computational biology [4  5]  social network analysis [6]  de-

∗Supported by NSF awards CCF 1565264 and CNS 1618026.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

anonymization [7]  and malware detection [8].2 The graph matching problem is the task of computing 
given a pair (G0  G1) of n vertex graphs  the permutation

(1)

π∗ = arg min
π∈Sn

(cid:107)G0 − π(G1)(cid:107)0

where we identify the graphs with their adjacency matrices  and write π(G1) for the matrix obtained by
permuting the rows and columns according to π (i.e.  the matrix P (cid:62)G1P where P is the permutation
matrix corresponding to π). The graph similarity problem is to merely determine whether or not G0
is similar to G1 or more generally to obtain an efﬁciently computable distance measure on G0 and
G1 that provides a rough approximation to minπ∈Sn (cid:107)G0 − π(G1)(cid:107)0.
In this paper we obtain new algorithms with provable guarantees for both problems. These problems
are NP hard in the worst case3 and hence our focus is on average case complexity and speciﬁcally
the correlated Erdös-Rényi model introduced by [11] and studied in [6  12  13  14  15  16]. For
n a positive integer and 0 < p  γ < 1  the correlated Erdös-Rényi model with parameters n  p  γ
is the following distribution over triples (G0  G1  π) where G0  G1 are n-vertex graphs and π is
permutation on [n]: (i) We sample a base graph B from the Erdös-Rényi random graph distribution
G(n  p)  (ii) We let G  G(cid:48) to be two independent random subgraphs of B  where every edge from
B is kept in G and G(cid:48) with probability γ independently  (iii) We choose a random permutation π
and output (G  π(G(cid:48))  π).4 We denote this distribution by Dstruct(n  p; γ). We say that (G0  G1) are
sampled from Dstruct(n  p; γ) if they are obtained by sampling (G0  G1  π) from this distribution and
discarding the permutation π. We use Dnull(n  p; γ) for the product distribution G(n  pγ) × G(n  pγ).
Note that the marginals over G0  G1 are the same in both Dstruct and Dnull but the two graphs are
correlated in the former distribution and independent in the latter. We consider the following two
computational problems:
Graph similarity: hypothesis testing. Given (G0  G1) sampled either from Dstruct(n  p; γ) or from
Dnull(n  p; γ). The goal is to distinguish which distribution the input (G0  G1) was sampled
from. Graph similarity (for general models) has been proposed as a tool for malware
detection [17  18]  chemical structure similarity [19  20]  comparing biological networks [21]
and more.
Graph matching: recovery. Given (G0  G1) sampled from Dstruct(n  p; γ)  the goal is to recover the
permutation π. Graph matching has a long history in pattern recognition [3]  social network
de-anonymization [7] and more.

1.1 Our contributions
It is known as long as pγ2 (cid:29) log n/n  if (G0  G1  π) is drawn from Dstruct(n  pγ) then π will be the
minimizer of the right-hand side of (1)  but prior to this work it was not known whether there is an
efﬁcient algorithm to recover π (see Section 1.2 for related work). In this work we give algorithms for
both the hypothesis testing and recovery problems on the correlated Erdös-Rényi model G(n  p; γ)
for every constant (and even slightly sub-constant) γ and a wide range of p.
Theorem 1.1 (Hypothesis testing). For every  > 0  sufﬁciently small δ > 0  and γ > 0 there is
a polynomial time algorithm A that distinguishes with success probability at least 1 −  between
the case that (G0  G1) are sampled from Dstruct(n  nδ−1; γ) and the case that they are sampled from
Dnull(n  nδ−1; γ).
Theorem 1.2 (Recovery). For every  > 0  sufﬁciently small δ > 0  and γ > 0  there is a
randomized algorithm A with running time nO(log n) such that with probability at least 1 −  over
(G0  G1  π∗) ∼ Dstruct(n  nδ−1; γ) and over the choices of A  we have A(G0  G1) = π∗.
These are the ﬁrst algorithms that run in better than exponential time for these problems (see Table 1).
While the main contribution of this paper is theoretical  we believe that our techniques are of
independent interest and applicability beyond the correlated Erdös-Rényi model. Key to our work
is the notion of identifying a large family of subgraphs (a “ﬂock of black swans”) each of which is

2See the surveys [9  10]  the latter of which is titled “Thirty Years of Graph Matching in Pattern Recognition”.
3Hamiltonian path is NP hard and can be reduced to graph matching by matching the input with a cycle.
4Some works also studied a more general variant where G0 and G1 use different subsampling parameters

γ0  γ1. Our work extends to this setting as well but for simplicity we focus on the γ0 = γ1 case.

2

highly unlikely to occur as a subgraph in a random graph but satisfying some near-independence
conditions that imply that with high probability some members of the family will occur. The existence
of such a family is by no means easy to establish— showing this accounts for much of the technical
work in this paper and there are still ranges of parameters for which we conjecture that such families
exist but have not been able to prove so. However  for any given distribution of graphs  one can
search for subgraphs that will serve as useful features for both graph similarity and graph matching.

Cullina & Kivayash

Paper

[15  16]

Algorithm

exhaustive search

(information theoretic bound)

Yartseva & Grossglauser

percolation from seed set

[12]

This paper

subgraph
matching

Runtime
O(n!)

exp(n1−δ−Θ(δ2))

nO(1) for testing

nO(log n) for recovery.

Table 1: Comparison with prior algorithms rigorously analyzed for recovery or testing in the correlated Erdös-
Rényi model  when (G0  G1  π) ∼ Dstruct(n  nδ−1; γ) for δ > 0. Prior algorithms were analyzed in this model
for the recovery task which subsumes testing. See related work section for a full discussion.
Remark 1.3. While we state our results for “sufﬁciently small” δ they actually hold in a broader
3 ≤ δ < 1). Under a certain combinatorial conjecture our
setting (i.e.  for 0 < δ ≤ 1/153 or 2
algorithms works for all 0 < δ < 1  see supplementary material.

1.2 Related work

n

n

There has been signiﬁcant amount of work on the correlated Erdös-Rényi model. Cullina and
Kivayash [15  16] precisely characterized the parameters p  γ for which information theoretic recovery
is possible. Speciﬁcally  they showed recovery is possible (in the information-theoretic sense  via an
and impossible when pγ2 < log n−ω(1)
exhaustive search over all permutations) if pγ2 > log n+ω(1)
.
Yartseva and Grossglauser [12] analyzed a simple algorithm known as Percolation Graph Matching
(PGM)  which was used successfully by Narayanan and Shmatikov [7] to de-anonymize many real-
world networks. (Similar algorithms were also analyzed by [6  14  13].) This algorithm starts with
a "seed set" S of vertices in G0 that are mapped by π to G1  and for which the mapping π|S is
given. It propagates this information according to a simple percolation  until it recovers the original
permutation. Yartseva and Grossglauser gave precise characterization of the size of the seed set
required as a function of p and γ [12]. Speciﬁcally  in the case that γ = Ω(1) and p = n−1+δ (where
the expected degree of G0 and G1 is Θ(nδ))  the size of the seed set required is |S| = n1−δ−Θ(δ2). In
the general setting when one is not given such a seed set  we would require about n|S| steps to obtain
it by brute force  which yields an exp(nΩ(1)) time algorithm in this regime. Lyzinski et al. [22] also
gave negative results for popular convex relaxations for graph matching on random correlated graphs.
We use a variant on the PGM algorithm as a component in our work to “boost” an initial partial
permutation into a the full knowledge. As part of that  we extend the analysis of PGM to show it
works even in the case where the partial assignment is noisy and the seed set itself might not be
random but rather can be adversarially chosen  see Lemma 4.2.
There have been many works on heuristics for both graph matching and graph similarity (see the
surveys [9  10]). In particular [23  24  21  25  26] studied the graph similarity problem of deciding
whether two graphs are similar to one another. [27  28  29  30] trained a deep neural network to
extract features of graphs for graph similarity.

2 Approaches and Techniques

In this section  we illustrate our approach and techniques. For simplicity and concreteness  we set the
noise parameter γ to half  and focus on the hypothesis testing task of distinguishing whether graphs
(G0  G1) are sampled from Dnull(n  nδ−1; 1
Warm-up: degree sequence. Since graph matching is a noisy version of graph isomorphism  as a
warm-up let us consider one of the most common heuristics for graph isomorphism which measures
similarity of the graphs using their degree sequence. Namely  using the vector of sorted degrees of the

2 ) for some small constant δ > 0.

2 ) or Dstruct(n  nδ−1; 1

3

vertices in the graph as a feature vector. If G0 and G1 were isomorphic then the two vectors will be
identical  while for two independent graphs the vectors are highly likely to differ. While this heuristic
is quite successful in the setting of (noiseless) graph isomorphism setting in getting at least an initial
assignment  it completely fails in our noisy setting of the graph matching and similarity problems.
√
Intuitively  this is due to the fact degrees in a random graph are highly concentrated (generally of the
form pn ± O(
pn)) and so even adding a small constant amount of noise will have a large effect on
the order of the vertices in the sorting  hence making corresponding coordinates of the two vectors
independent from one another. A similar phenomenon holds for the case where we use the sorted top
eigenvectors of the adjacency matrix as a feature vector. While the degree and eigenvectors are poorly
suited for handling noisy graphs  it turns out we can design better features by looking at subgraph
counts for carefully chosen families of graphs. This is what we do.

2.1 The “black swan” approach
Our approach can be viewed as “using a ﬂock of black swans”. Speciﬁcally  for each b ∈ {0  1}  we
map the graphs G0  G1 into a pair of feature vectors v0  v1 ∈ Zk as follows: Let H = {H1  . . .   Hk}
be a carefully chosen family of small graphs. Next  for b ∈ {0  1} and j ∈ {1  2  . . .   k}  deﬁne the
jth coordinate of vb to be the number of occurrences of the graph Hj as being a subgraph of Gb.5 We
choose the family H to satisfy the following two conditions:
"Black swan": For every H ∈ H  the probability that H occurs as a subgraph of a random graph G
Pairwise independence (informal): For H (cid:54)= H(cid:48) in H  the probability both H and H(cid:48) both occur
as subgraphs in a random graph G from G(n  p) is up to a constant factor the product of the
probabilities that each one of them occurs individually.

from G(n  p) is a small number µ (cid:28) 1.

j and v1

Before going to the details of the technical properties of black swans  let us ﬁrst take a look at why
this would be useful for the hypothesis testing problem. Let’s assume for simplicity that all the graphs
in H have e edges for some constant e. If G0  G1 are γ correlated then for every j ∈ {1  2  . . .   k} 
the coordinates v0
j will have a correlation of γ2e. In contrast  if G0  G1 are independently
chosen then v0 and v1 are completely independent and hence v0
j have zero correlation. The
number γe is very small  but the pairwise independence condition implies that if the size |H| of the
family is much larger than (1/γ)2e then the vectors v0 and v1 will have a signiﬁcantly larger inner
product in the correlated case than they do in the null case. We instantiate the above idea into a
hypothesis testing algorithm in Section 3.
Remark 2.1 (Black-swan based algorithm for recovery). The above approach can be extended to the
recovery problem as well. The idea is that for every vertex i of Gb we deﬁne a vector vb i ∈ Zk such
that for all (cid:96) ∈ [k]  vb i
is equal to the number of subgraphs of Gb isomorphic to H(cid:96) that touch the
vertex i. The intuition is that for vertices i of G0 and j of G1  the vectors v0 i and v1 j are much more
likely to have signiﬁcant inner product if π(i) = j  this can be used to obtain partial information on
the permutation that can later be “boosted” to recover the full permutation. We instantiate the above
idea into a recovery algorithm in Section 4.

j and v1

(cid:96)

2.2 Constructing the black swan family
We now describe more precisely the properties that our family H of “black swans” or test graphs
needs to satisfy so the above algorithm will succeed. It is encapsulated in the following theorem:6
Theorem 2.2 (General overview of test graph properties). For any rational scalar d ∈ (2  2 + 1
76 )
or d ∈ Z≥3 or d ≥ 6  and integer v0 there exists v ≥ v0 and set Hv

d of v-vertex graphs s.t.:

1. (Low likelihood of appearing) Every H ∈ Hv

edges of H is e = dv/2.

d has average degree d. That is  the number of

5More formally  vb

j = XHj (Gb) where XH (G) is the number of injective homomorphisms of H to G 

divided by the number of automorphisms of H.

6The range of values of p our algorithm is proven to succeed for corresponds to the degrees achievable in
Theorem 2.2. We conjecture that a family achieving these properties can be obtained with any density e/v > 1 
which would extend our analysis to p = n1−δ for all δ ∈ (0  1) (see the supplementary materials).

4

2. (Strong strict balance) For every H ∈ Hv

d and induced subgraph H(cid:48) of H with e(1 − )
edges and v(cid:48) vertices satisﬁes e(cid:48)/v(cid:48) < e/v − η for a constant η depending only on  and d .7

3. Every H ∈ Hv
4. (Pairwise near independence) For every pair of distinct graphs H  H(cid:48) ∈ Hv

d has no non-trivial automorphisms.

d if J is a shared
subgraph of H and H(cid:48) of e(cid:48)(cid:48) edges and v(cid:48)(cid:48) vertices then e(cid:48)(cid:48)/v(cid:48)(cid:48) < e/v − η(cid:48) where η(cid:48) is a
constant depending only on d.

5. (Largeness) The size of the family is |H| = vcv where c is a constant depending only on d.

The proof of Theorem 2.2 is quite involved  and we leave it to the supplementary materials. Here 
we sketch the construction of Hv
d where d = 2 + δ for a small constant δ > 0. This is the most
interesting parameter regime  as it corresponds to the sparse graph case where the degree of G0  G1
is ∼ nδ. We can express the number (1 − δ)/1.5δ as a convex combination kα + (k + 1)(1 − α)
of two integers k  k + 1. We choose a large enough integer v so that δv  1.5δv and α1.5δv are all
integers. Now we choose a random three-regular graph H on δv vertices (and hence 1.5δv edges) 
pick 1.5αδv of the edges of H uniformly at random  and replace them with paths of length k (i.e. 
subdivide the edge with k vertices) and replace the remaining (1− α)· 1.5δv edges with k + 1 length
paths. The resulting graph H(cid:48) will have average degree 2 + δ as desired. The bulk of the analysis is
to prove that with high probability the graph H(cid:48) will satisfy the strong strict balance property  and
moreover we can repeat this process vΩ(v) times and get a family of graphs  every pair of which
satisﬁes the pairwise near independence property. See Figure 1 for an example.

Figure 1: An example of the construction where d = 2 + δ and k = 0.

3 Algorithm for Hypothesis Testing

In this section  we describe the algorithm for the hypothesis testing based on the “black swan”
approach introduced in Section 2. Let H be the family of graphs constructed in Theorem 2.2  we
deﬁne the following correlation polynomial.

(cid:0)XH (G0) − EG∼G(n pγ)XH (G)(cid:1)(cid:0)XH (G1) − EG∼G(n pγ)XH (G)(cid:1) .

PH(G0  G1) =

1
|H|

(cid:88)

H∈H

Intuitively  the expectation of PH(G0  G1) is zero under Dnull but large under Dstruct. Speciﬁcally  we
prove the following theorem:
Theorem 3.1. For any n large enough  sufﬁciently small δ > 0  and any γ ∈ (0  1)  let H = Hv
d| ≥ (400/γ2)dv. Then EDnull[PH(G0  G1)] =
obtained from Theorem 2.2 where d = 2
0  and EDstruct [PH(G0  G1)] ≥ 40 · max
where all distributions above are for n vertices  p = nδ−1 and noise γ.

VarDnull (PH(G0  G1))1/2   VarDstruct (PH(G0  G1))1/2(cid:17)

(cid:16)
1−δ and |Hv

d

The proof of Theorem 3.1 is provided in the supplementary materials. The degree of the polynomial
PH(G0  G1) is 2e where e is the number of edges in any member of the family  and so its number
of monomials (and hence computation time) will be nO(e) = nO(1) where the constant in the
O(1) depends on the size of the representation of (1 − δ)−1 as a ratio of two integers. Combining
Theorem 3.1 with Chebyshev’s inequality  the following algorithm solves the hypothesis testing
problem in the parameter regime stated in Theorem 1.1.

7This condition is a strengthening of the “strict balance” condition in the random graph literature [31].

5

1−δ .

Dstruct(n  p; γ).

Algorithm 1 HYPOTHESISTESTING
Input: Parameters n  p  γ where p = nδ−1. Graphs G0  G1 sampled from either Dnull(n  p; γ) or
Output: “(G0  G1) came from Dnull” or “(G0  G1) came from Dstruct”.
1: d ← 2
2: Choose v be a sufﬁciently large even number such that vc > 400/γ2 where c is the constant
d where Hv
3: H ← Hv
4: Compute µstruct ← E(G(cid:48)
1)∼Dstruct(n p;γ)[PH(G(cid:48)
5: if PH(G0  G1) > 1
6: else Output “(G0  G1) came from Dnull”. end if

d is obtained from Theorem 2.2.
0  G(cid:48)
3 µstruct then Output “(G0  G1) came from Dstruct”.

from Theorem 2.2 so that |Hv

d| ≥ v cd
2 v.

0 G(cid:48)

1)].

4 Algorithm for Recovery

In this section we present our algorithm for the recovery (i.e.  graph matching) task. All proofs are
provided in the supplemantary material. Our algorithm follows the following general template:

Algorithm 2 RECOVERY
Input: Parameters n  p  γ and graphs G0  G1 sampled from Dstruct(n  p; γ).
Output: A permutation π ∈ Sn.
1: H ← INITIALIZERECOVERY(n  p  γ).
d(cid:48) by Theorem 2.2.
2: π0 ← PARTIALASSIGNMENT(n  p  γ  G0  G1 H).
(cid:46) Find an initial partial assignment π0.
3: π ← BOOSTING(n  p  γ  G0  G1  π0). (cid:46) Boost the partial assignment π0 to ﬁnal assignment π.
4: return π.

(cid:46) Initialize a graph family H = Hv

There are three steps in the above general template algorithm RECOVERY  each of them is of
independent interest. In the ﬁrst step  one construct a family of subgraphs of nice structure so that in
the second step these subgraphs can be used to efﬁciently come up with a partial assignment π0 to
the recovery problem. A partial assignment correctly matches a good fraction of vertices between G0
and G1  however  one does not know which vertices are correctly matched. Thus  in the last step  the
boosting algorithm transforms an arbitrary partial assignment π0 to a ﬁnal assignment π that correctly
matches every vertex. The main contribution of this paper lies in the ﬁrst two steps which use the
black swan approach while the last step is a variant of the previous seed-set based algorithms. In the
following  we instantiate RECOVERY using the test graph family constructed in Theorem 2.2 and
prove Theorem 1.2.

Step 1: Construct graph family Here we describe the algorithm INITIALIZERECOVERY as
follows. For p = nδ−1  if 0 < δ < 1
153  choose v = Θ(log n)  to be the smallest even integer so that
3 ≤ δ < 1  choose
λv is also an integer  for some λ ∈ ( 2δ
1−δ + log log n
1−δ   2δ
v = Θ(log n)  to be the smallest even integer so that there is some d(cid:48) ∈ ( 2δ
4 log n )  so that
(d(cid:48) − (cid:98)d(cid:48)(cid:99)v) is also an integer. Finally  pick H to be Hv
d(cid:48) is obtained from Theorem 2.2.

log n ) and set d(cid:48) = 2 + λ. If 2
1−δ   2δ

d(cid:48) where Hv

1−δ + log log n

n

Step 2: Partial assignment The second part of the recovery algorithm is a procedure in ﬁnding a
noisy seed set. Speciﬁcally  if (G0  G1  π∗) are sampled from Dstruct(n  p; γ)  and 0 < θ  η ≤ 1 are
some constants then an (θ  η) partial assignment is a partial function π : V (G0) → V (G1) that is
one-to-one deﬁned on at least θ fraction of the inputs s..t. for at least η fraction of the inputs u on
which π is deﬁned  π(u) = π∗(u). We prove that algorithm PARTIALASSIGNMENT below gives a
O(log log n)   1 − o(1))-partial assignment with probability 1 − o(1) over the Erdös-Rényi model and
(
the randomness of the algorithm.
Lemma 4.1. Suppose that (G0  G1) ∼ Dstruct(n  p; γ) and H = Hv
d from INITIALIZERECOVERY.
v1/8 )-
Then under the conditions of Theorem 1.2  PARTIALASSIGNMENT outputs a ( n
partial assignment with probability 1 − o(1) over the choice of (G0  G1) ∼ Dstruct(n  p; γ) and the
randomness of the algorithm.

log v   1 − 1

6

Algorithm 3 PARTIALASSIGNMENT
Input: Parameters n  p  γ  graphs G0  G1 sampled from Dstruct(n  p; γ)  and a family of graphs H.
Output: A permutation π0 ∈ Sn.
1: v ← |V (H)|  e ← |E(H)|  ∀H ∈ H.
2: d(cid:48) ← 2e
v .
3: π0(u) ← ∅ for all u ∈ V (G0).
4: for u ∈ V (G0) do
5:
6:
7:
8:
9:
10:
11: end for
12: return π0.

Pick H ← Hu at random.
w ← the corresponding vertex of u in the copy of H in G1.
if ¬∃u(cid:48) (cid:54)= u such that π0(u(cid:48)) = w then π0(u) ← w. end if

Hu ← {H ∈ H : u is incident to a copy of H in G0 and H appears in G1}.
if |Hu| ≥ 1

2|H| · v · nv−1(pγ2)e then

end if

Step 3: Boosting Finally  in the last step of the recovery algorithm  we boost the partial assignment
to a full permutation from V (G0) to V (G1). This step is based on the “Percolation Graph Matching“
used in works such as [12  13  32  33  14]. However  we need a stronger analysis of this step  since
the partial knowledge obtained from PARTIALASSIGNMENT can be noisy and (more importantly)
might have arbitrary correlation with the random graph  and hence we need to assume that it might
O(log log n)   1 − o(1)) partial
be adversarially chosen. Speciﬁcally  we show that we can boost an (
assignment to a the full ground truth:
Lemma 4.2 (Boosting from partial knowledge). Let p  γ  n  η  c  θ be such that pγn ≥ logc n
for c > 1  ηθ = o(γ2) and θ = Ω(log1−c n). Then with probability 1 − o(1) over the choice
of (G0  G1  π∗) from Dstruct(n  p; γ)  if BOOSTING is given G0  G1 and any (θn  1 − η) partial
assignment π  then it outputs the ground truth permutation π∗.

n

2: ∆ ←(cid:4)θγ2np/100(cid:5).
6: ∆(cid:48) ←(cid:4)γ2np/100(cid:5).

Algorithm 4 BOOSTING
Input: Parameters n  p  γ  graphs G0  G1 sampled from Dstruct(n  p; γ)  a partial assignment π0 ∈ Sn.
Output: A permutation π ∈ Sn.
1: (θ  η) ← Lemma 4.1 and π ← π0.
3: for u ∈ V (G0)  w ∈ V (G1) do N (u  w) ← |{u(cid:48) ∈ V (G0) : u ∼ u(cid:48)  π(u(cid:48)) ∼ w}|. end for
4: while u ∈ V (G0) where π(u) = ∅ and ∃w ∈ V (G1)  N (u  w) ≥ ∆ do π(u) ← w. end while
5: if π is not a permutation then Complete π arbitrarily. end if
7: while ∃u ∈ V (G0)  w ∈ V (G1) such that N (u  w) ≥ ∆(cid:48) and N (u  π(u))  N (π−1(w)  w) <
8: return π.

∆(cid:48)/10 do Modify π by mapping u to w and mapping π−1(w) to π(u). end while

(cid:46) π0 is a (θ  η)-partial assignment.

References
[1] Timothee Cour  Praveen Srinivasan  and Jianbo Shi. Balanced graph matching. In Advances in

Neural Information Processing Systems  pages 313–320  2007.

[2] Minsu Cho and Kyoung Mu Lee. Progressive graph matching: Making a move of graphs
via probabilistic voting. In Computer Vision and Pattern Recognition (CVPR)  2012 IEEE
Conference on  pages 398–405. IEEE  2012.

[3] Alexander C Berg  Tamara L Berg  and Jitendra Malik. Shape matching and object recognition
using low distortion correspondences. In Computer Vision and Pattern Recognition  2005.
CVPR 2005. IEEE Computer Society Conference on  volume 1  pages 26–33. IEEE  2005.

7

[4] Rohit Singh  Jinbo Xu  and Bonnie Berger. Global alignment of multiple protein interaction
networks with application to functional orthology detection. Proceedings of the National
Academy of Sciences  105(35):12763–12768  2008.

[5] Joshua T Vogelstein  John M Conroy  Louis J Podrazik  Steven G Kratzer  Eric T Harley 
Donniell E Fishkind  R Jacob Vogelstein  and Carey E Priebe. Large (brain) graph matching via
fast approximate quadratic programming. arXiv preprint arXiv:1112.5507  2011.

[6] Nitish Korula and Silvio Lattanzi. An efﬁcient reconciliation algorithm for social networks.

Proceedings of the VLDB Endowment  7(5):377–388  2014.

[7] Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In Security and

Privacy  2009 30th IEEE Symposium on  pages 173–187. IEEE  2009.

[8] Younghee Park  Douglas Reeves  Vikram Mulukutla  and Balaji Sundaravel. Fast malware
classiﬁcation by automated behavioral graph matching. In Proceedings of the Sixth Annual
Workshop on Cyber Security and Information Intelligence Research  CSIIRW ’10  pages 45:1–
45:4  New York  NY  USA  2010. ACM.

[9] Lorenzo Livi and Antonello Rizzi. The graph matching problem. Pattern Analysis and

Applications  16(3):253–283  2013.

[10] Donatello Conte  Pasquale Foggia  Carlo Sansone  and Mario Vento. Thirty years of graph
matching in pattern recognition. International journal of pattern recognition and artiﬁcial
intelligence  18(03):265–298  2004.

[11] Pedram Pedarsani and Matthias Grossglauser. On the privacy of anonymized networks. In
Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining  pages 1235–1243. ACM  2011.

[12] Lyudmila Yartseva and Matthias Grossglauser. On the performance of percolation graph
In Proceedings of the ﬁrst ACM conference on Online social networks  pages

matching.
119–130. ACM  2013.

[13] Vince Lyzinski  Donniell E Fishkind  and Carey E Priebe. Seeded graph matching for correlated

erdös-rényi graphs. Journal of Machine Learning Research  15(1):3513–3540  2014.

[14] Ehsan Kazemi  S Hamed Hassani  and Matthias Grossglauser. Growing a graph matching from

a handful of seeds. Proceedings of the VLDB Endowment  8(10):1010–1021  2015.

[15] Daniel Cullina and Negar Kiyavash. Improved achievability and converse bounds for erdos-renyi
graph matching. In ACM SIGMETRICS Performance Evaluation Review  volume 44  pages
63–72. ACM  2016.

[16] Daniel Cullina and Negar Kiyavash. Exact alignment recovery for correlated erdos renyi graphs.

arXiv preprint arXiv:1711.06783  2017.

[17] Hugo Gascon  Fabian Yamaguchi  Daniel Arp  and Konrad Rieck. Structural detection of
android malware using embedded call graphs. In Proceedings of the 2013 ACM workshop on
Artiﬁcial intelligence and security  pages 45–54. ACM  2013.

[18] Neha Runwal  Richard M Low  and Mark Stamp. Opcode graph similarity and metamorphic

detection. Journal in Computer Virology  8(1-2):37–52  2012.

[19] John W Raymond  Eleanor J Gardiner  and Peter Willett. Heuristics for similarity searching
of chemical graphs using a maximum common edge subgraph algorithm. Journal of chemical
information and computer sciences  42(2):305–316  2002.

[20] Masahiro Hattori  Yasushi Okuno  Susumu Goto  and Minoru Kanehisa. Heuristics for chemical

compound matching. Genome Informatics  14:144–153  2003.

[21] Maureen Heymans and Ambuj K Singh. Deriving phylogenetic trees from the similarity analysis

of metabolic pathways. Bioinformatics  19(suppl_1):i138–i146  2003.

8

[22] V. Lyzinski  D. E. Fishkind  M. Fiori  J. T. Vogelstein  C. E. Priebe  and G. Sapiro. Graph
IEEE Transactions on Pattern Analysis and Machine

matching: Relax at your own risk.
Intelligence  38(1):60–73  Jan 2016.

[23] Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM

(JACM)  46(5):604–632  1999.

[24] Sergey Melnik  Hector Garcia-Molina  and Erhard Rahm. Similarity ﬂooding: A versatile graph
matching algorithm and its application to schema matching. In Proceedings 18th International
Conference on Data Engineering  pages 117–128. IEEE  2002.

[25] Elizabeth A Leicht  Petter Holme  and Mark EJ Newman. Vertex similarity in networks.

Physical Review E  73(2):026120  2006.

[26] Laura A Zager and George C Verghese. Graph similarity scoring and matching. Applied

mathematics letters  21(1):86–94  2008.

[27] Yunsheng Bai  Hao Ding  Yizhou Sun  and Wei Wang. Convolutional set matching for graph

similarity. arXiv preprint arXiv:1810.10866  2018.

[28] Yunsheng Bai  Hao Ding  Song Bian  Ting Chen  Yizhou Sun  and Wei Wang. Simgnn: A
neural network approach to fast graph similarity computation. In Proceedings of the Twelfth
ACM International Conference on Web Search and Data Mining  WSDM ’19  pages 384–392 
New York  NY  USA  2019. ACM.

[29] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural

networks? arXiv preprint arXiv:1810.00826  2018.

[30] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. arXiv preprint arXiv:1609.02907  2016.

[31] Béla Bollobás. Random graphs. pages 80–102. Cambridge University Press  1981.

[32] Vince Lyzinski  Sancar Adali  Joshua T Vogelstein  Youngser Park  and Carey E Priebe.
Seeded graph matching via joint optimization of ﬁdelity and commensurability. arXiv preprint
arXiv:1401.3813  2014.

[33] Svante Janson  Tomasz Luczak  Tatyana Turova  Thomas Vallier  et al. Bootstrap percolation on

the random graph gn p. The Annals of Applied Probability  22(5):1989–2047  2012.

9

,Jianfei Chen
Jun Zhu
Yee Whye Teh
Boaz Barak
Chi-Ning Chou
Zhixian Lei
Tselil Schramm
Yueqi Sheng