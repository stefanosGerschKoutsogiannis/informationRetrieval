2017,Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees,Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity  effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms  respectively. In this paper  we consider the intermediate case of optimization over the convex cone  parametrized as the conic hull of a generic atom set  leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular  we derive sublinear (O(1/t)) convergence on general smooth and convex objectives  and linear convergence (O(e^{-t})) on strongly convex objectives  in both cases for general sets of atoms. Furthermore  we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions  and hence are directly applicable to a large variety of learning settings.,Greedy Algorithms for Cone Constrained
Optimization with Convergence Guarantees

Francesco Locatello

MPI for Intelligent Systems - ETH Zurich

Michael Tschannen

ETH Zurich

locatelf@ethz.ch

michaelt@nari.ee.ethz.ch

Gunnar Rätsch

ETH Zurich

Martin Jaggi

EPFL

raetsch@inf.ethz.ch

martin.jaggi@epfl.ch

Abstract

Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe
(FW) algorithms regained popularity in recent years due to their simplicity  effec-
tiveness and theoretical guarantees. MP and FW address optimization over the
linear span and the convex hull of a set of atoms  respectively. In this paper  we
consider the intermediate case of optimization over the convex cone  parametrized
as the conic hull of a generic atom set  leading to the ﬁrst principled deﬁnitions
of non-negative MP algorithms for which we give explicit convergence rates and
demonstrate excellent empirical performance. In particular  we derive sublinear
(O(1/t)) convergence on general smooth and convex objectives  and linear con-
vergence (O(e−t)) on strongly convex objectives  in both cases for general sets
of atoms. Furthermore  we establish a clear correspondence of our algorithms
to known algorithms from the MP and FW literature. Our novel algorithms and
analyses target general atom sets and general objective functions  and hence are
directly applicable to a large variety of learning settings.

1

Introduction

In recent years  greedy optimization algorithms have attracted signiﬁcant interest in the domains
of signal processing and machine learning thanks to their ability to process very large data sets.
Arguably two of the most popular representatives are Frank-Wolfe (FW) [12  21] and Matching
Pursuit (MP) algorithms [34]  in particular Orthogonal MP (OMP) [9  49]. While the former targets
minimization of a convex function over bounded convex sets  the latter apply to minimization over a
linear subspace. In both cases  the domain is commonly parametrized by a set of atoms or dictionary
elements  and in each iteration  both algorithms rely on querying a so-called linear minimization
oracle (LMO) to ﬁnd the direction of steepest descent in the set of atoms. The iterate is then updated
as a linear or convex combination  respectively  of previous iterates and the newly obtained atom
from the LMO. The particular choice of the atom set allows to encode structure such as sparsity and
non-negativity (of the atoms) into the solution. This enables control of the trade-off between the
amount of structure in the solution and approximation quality via the number of iterations  which
was found useful in a large variety of use cases including structured matrix and tensor factorizations
[50  53  54  18].
In this paper  we target an important “intermediate case” between the two domain parameterizations
given by the linear span and the convex hull of an atom set  namely the parameterization of the
optimization domain as the conic hull of a possibly inﬁnite atom set. In this case  the solution
can be represented as a non-negative linear combination of the atoms  which is desirable in many

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

applications  e.g.  due to the physics underlying the problem at hand  or for the sake of interpretability.
Concrete examples include unmixing problems [11  16  3]  model selection [33]  and matrix and
tensor factorizations [4  24]. However  existing convergence analyses do not apply to the currently
used greedy algorithms. In particular  all existing MP variants for the conic hull case [5  38  52] are
not guaranteed to converge and may get stuck far away from the optimum (this can be observed in
the experiments in Section 6). From a theoretical perspective  this intermediate case is of paramount
interest in the context of MP and FW algorithms. Indeed  the atom set is not guaranteed to contain
an atom aligned with a descent direction for all possible suboptimal iterates  as is the case when the
optimization domain is the linear span or the convex hull of the atom set [39  32]. Hence  while conic
constraints have been widely studied in the context of a manifold of different applications  none of
the existing greedy algorithms enjoys explicit convergence rates.
We propose and analyze new MP algorithms tailored for the minimization of smooth convex functions
over the conic hull of an atom set. Speciﬁcally  our key contributions are:

• We propose the ﬁrst (non-orthogonal) MP algorithm for optimization over conic hulls
guaranteed to converge  and prove a corresponding sublinear convergence rate with ex-
plicit constants. Surprisingly  convergence is achieved without increasing computational
complexity compared to ordinary MP.
• We propose new away-step  pairwise  and fully corrective MP variants  inspired by variants
of FW [28] and generalized MP [32]  respectively  that allow for different degrees of weight
corrections for previously selected atoms. We derive corresponding sublinear and linear (for
strongly convex objectives) convergence rates that solely depend on the geometry of the
atom set.
• All our algorithms apply to general smooth convex functions. This is in contrast to all prior
work on non-negative MP  which targets quadratic objectives [5  38  52]. Furthermore  if
the conic hull of the atom set equals its linear span  we recover both algorithms and rates
derived in [32] for generalized MP variants.
• We make no assumptions on the atom set which is simply a subset of a Hilbert space  in

particular we do not assume the atom set to be ﬁnite.

Before presenting our algorithms (Section 3) along with the corresponding convergence guarantees
(Section 4)  we brieﬂy review generalized MP variants. A detailed discussion of related work can
be found in Section 5 followed by illustrative experiments on a least squares problem on synthetic
data  and non-negative matrix factorization as well as non-negative garrote logistic regression as
applications examples on real data (numerical evaluations of more applications and the dependency
between constants in the rate and empirical convergence can be found in the supplementary material).
Notation. Given a non-empty subset A of some Hilbert space  let conv(A) be the convex hull
of A  and let lin(A) denote its linear span. Given a closed set A  we call its diameter diam(A) =
maxz1 z2∈A (cid:107)z1 − z2(cid:107) and its radius radius(A) = maxz∈A (cid:107)z(cid:107). (cid:107)x(cid:107)A := inf{c > 0 : x ∈
c · conv(A)} is the atomic norm of x over a set A (also known as the gauge function of conv(A)).
We call a subset A of a Hilbert space symmetric if it is closed under negation.

2 Review of Matching Pursuit Variants
Let H be a Hilbert space with associated inner product (cid:104)x  y(cid:105)  ∀ x  y ∈ H. The inner product induces
the norm (cid:107)x(cid:107)2 := (cid:104)x  x(cid:105)  ∀ x ∈ H. Let A ⊂ H be a compact set (the “set of atoms” or dictionary)
and let f : H → R be convex and L-smooth (L-Lipschitz gradient in the ﬁnite dimensional case).
If H is an inﬁnite-dimensional Hilbert space  then f is assumed to be Fréchet differentiable. The
generalized MP algorithm studied in [32]  presented in Algorithm 1  solves the following optimization
problem:
(1)

f (x).

min

x∈lin(A)

In each iteration  MP queries a linear minimization oracle (LMO) solving the following linear
problem:

(2)
for a given query y ∈ H. The MP update step minimizes a quadratic upper bound gxt(x) =
f (xt) + (cid:104)∇f (xt)  x − xt(cid:105) + L
2 (cid:107)x − xt(cid:107)2 of f at xt  where L is an upper bound on the smoothness

LMOA(y) := arg min

(cid:104)y  z(cid:105)

z∈A

2

constant of f with respect to a chosen norm (cid:107) · (cid:107). Optimizing this norm problem instead of f
directly allows for substantial efﬁciency gains in the case of complicated f. For symmetric A and for
2(cid:107)y − x(cid:107)2  y ∈ H  Algorithm 1 recovers MP (Variant 0) [34] and OMP (Variant 1) [9  49] 
f (x) = 1
see [32] for details.

Algorithm 1 Norm-Corrective Generalized Match-
ing Pursuit
1: init x0 ∈ lin(A)  and S := {x0}
2: for t = 0 . . . T
3:
4:
5:
6:

Find zt := (Approx-)LMOA(∇f (xt))
S := S ∪ {zt}
Let b := xt − 1
Variant 0:

L∇f (xt)

Update xt+1 := arg min
z:=xt+γzt

γ∈R

(cid:107)z − b(cid:107)2

Approximate linear oracles. Solving the
LMO deﬁned in (2) exactly is often hard in
practice  in particular when applied to matrix
(or tensor) factorization problems  while ap-
proximate versions can be much more efﬁcient.
Algorithm 1 allows for an approximate LMO.
For given quality parameter δ ∈ (0  1] and
given direction d ∈ H  the approximate LMO
for Algorithm 1 returns a vector ˜z ∈ A such
that

(3)
relative to z = LMOA(d) being an exact solu-
tion.

(cid:104)d  ˜z(cid:105) ≤ δ(cid:104)d  z(cid:105) 

7:

Variant 1:

Update xt+1 := arg min
z∈lin(S)

(cid:107)z − b(cid:107)2

8:
9: end for

Optional: Correction of some/all atoms z0...t

Discussion and limitations of MP. The anal-
ysis of the convergence of Algorithm 1 in [32]
critically relies on the assumption that the ori-
gin is in the relative interior of conv(A) with
respect to its linear span. This assumption originates from the fact that the convergence of MP- and
FW-type algorithms fundamentally depends on an alignment assumption of the search direction
returned by the LMO (i.e.  zt in Algorithm 1) and the gradient of the objective at the current iteration
(see third premise in [39]). Speciﬁcally  for Algorithm 1  the LMO is assumed to select a descent
direction  i.e.  (cid:104)∇f (xt)  zt(cid:105) < 0  so that the resulting weight (i.e.  γ for Variant 0) is always positive.
In this spirit  Algorithm 1 is a natural candidate to minimize f over the conic hull of A. However 
if the optimization domain is a cone  the alignment assumption does not hold as there may be
non-stationary points x in the conic hull of A for which minz∈A(cid:104)∇f (x)  z(cid:105) = 0. Algorithm 1 is
therefore not guaranteed to converge when applied to conic problems. The same issue arises for
essentially all existing non-negative variants of MP  see  e.g.  Alg. 2 in [38] and in Alg. 2 in [52]. We
now present modiﬁcations corroborating this issue along with the resulting MP-type algorithms for
conic problems and corresponding convergence guarantees.

3 Greedy Algorithms on Conic Hulls
The cone cone(A − y) tangent to the convex set conv(A) at a point y is formed by the half-lines
emanating from y and intersecting conv(A) in at least one point distinct from y. Without loss of
generality we consider 0 ∈ A and assume the set cone(A) (i.e.  y = 0) to be closed. If A is ﬁnite
i=1 αiai s.t. ai ∈ A  αi ≥ 0 ∀i}. We

the cone constraint can be written as cone(A) := {x : x =(cid:80)|A|

consider conic optimization problems of the form:

min

f (x).

x∈cone(A)

(4)
Note that if the set A is symmetric or if the origin is in the relative interior of conv(A) w.r.t. its linear
span then cone(A) = lin(A). We will show later how our results recover known MP rates when the
origin is in the relative interior of conv(A).
As a ﬁrst algorithm to solve problems of the form (4)  we present the Non-Negative Generalized
Matching Pursuit (NNMP) in Algorithm 2 which is an extension of MP to general f and non-negative
weights.

Discussion: Algorithm 2 differs from Algorithm 1 (Variant 0) in line 4  adding the iteration-
dependent atom − xt(cid:107)xt(cid:107)A to the set of possible search directions1. We use the atomic norm for the
1This additional direction makes sense only if xt (cid:54)= 0. Therefore  we set − xt(cid:107)xt(cid:107)A = 0 if xt = 0  i.e.  no

direction is added.

3

Algorithm 2 Non-Negative Matching Pursuit
1: init x0 = 0 ∈ A
2: for t = 0 . . . T
z∈(cid:110)
3:
4:

Find ¯zt := (Approx-)LMOA(∇f (xt))
(cid:111)(cid:104)∇f (xt)  z(cid:105)
zt = arg min

¯zt 

−xt
(cid:107)xt(cid:107)A

(cid:104)−∇f (xt) zt(cid:105)

γ :=
Update xt+1 := xt + γzt

L(cid:107)zt(cid:107)2

5:
6:
7: end for

Figure 1: Two dimensional example for TA(xt) where
A = {a1  a2}  for three different iterates x0  x1 and
x2. The shaded area corresponds to TA(xt) and the
white area to lin(A) \ TA(xt).

normalization because it yields the best constant in the convergence rate. In practice  one can replace
it with the Euclidean norm  which is often much less expensive to compute. This iteration-dependent
additional search direction allows to reduce the weights of the atoms that were previously selected 
thus admitting the algorithm to “move back” towards the origin while maintaining the cone constraint.
This idea is informally explained here and formally studied in Section 4.1.
Recall the alignment assumption of the search direction and the gradient of the objective at the current
iterate discussed in Section 2 (see also [39]). Algorithm 2 obeys this assumption. The intuition
behind this is the following. Whenever xt is not a minimizer of (4) and minz∈A(cid:104)∇f (xt)  z(cid:105) = 0 
the vector − xt(cid:107)xt(cid:107)A is aligned with ∇f (xt) (i.e.  (cid:104)∇f (xt) − xt(cid:107)xt(cid:107)A(cid:105) < 0)  preventing the algorithm
from stopping at a suboptimal iterate. To make this intuition more formal  let us deﬁne the set of
feasible descent directions of Algorithm 2 at a point x ∈ cone(A) as:

(cid:26)

d ∈ H : ∃z ∈ A ∪(cid:110) − x

(cid:111)

(cid:27)

s.t. (cid:104)d  z(cid:105) < 0

(cid:107)x(cid:107)A

TA(x) :=

(5)
If at some iteration t = 0  1  . . . the gradient ∇f (xt) is not in TA(xt) Algorithm 2 terminates as
minz∈A(cid:104)d  z(cid:105) = 0 and (cid:104)d −xt(cid:105) ≥ 0 (which yields zt = 0). Even though  in general  not every
direction in H is a feasible descent direction  ∇f (xt) /∈ TA only occurs if xt is a constrained
minimum of Equation 4:
Lemma 1. If ˜x ∈ cone(A) and ∇f (˜x) (cid:54)∈ TA then ˜x is a solution to minx∈cone(A) f (x).
Initializing Algorithm 2 with x0 = 0 guarantees that the iterates xt always remain inside cone(A)
even though this is not enforced explicitly (by convexity of f  see proof of Theorem 2 in Appendix D
for details).

.

representation of xt =(cid:80)t−1

Limitations of Algorithm 2: Let us call active the atoms which have nonzero weights in the
i=0 αizi computed by Algorithm 2. Formally  the set of active atoms is
deﬁned as S := {zi : αi > 0  i = 0  1  . . .   t − 1}. The main drawback of Algorithm 2 is that when
the direction − xt(cid:107)xt(cid:107)A is selected  the weight of all active atoms is reduced. This can lead to the
algorithm alternately selecting − xt(cid:107)xt(cid:107)A and an atom from A  thereby slowing down convergence in a
similar manner as the zig-zagging phenomenon well-known in the Frank-Wolfe framework [28]. In
order to achieve faster convergence we introduce the corrective variants of Algorithm 2.

3.1 Corrective Variants

To achieve faster (linear) convergence (see Section 4.2) we introduce variants of Algorithm 2  termed
Away-steps MP (AMP) and Pairwise MP (PWMP)  presented in Algorithm 3. Here  inspired by the
away-steps and pairwise variants of FW [12  28]  instead of reducing the weights of the active atoms
uniformly as in Algorithm 2  the LMO is queried a second time on the active set S to identify the
direction of steepest ascent in S. This allows  at each iteration  to reduce the weight of a previously
selected atom (AMP) or swap weight between atoms (PWMP). This selective “reduction” or “swap
of weight” helps to avoid the zig-zagging phenomenon which prevent Algorithm 2 from converging
linearly.
At each iteration  Algorithm 3 updates the weights of zt and vt as αzt = αzt + γ and αvt = αvt − γ 
respectively. To ensure that xt+1 ∈ cone(A)  γ has to be clipped according to the weight which is
currently on vt  i.e.  γmax = αvt. If γ = γmax  we set αvt = 0 and remove vt from S as the atom vt
is no longer active. If dt ∈ A (i.e.  we take a regular MP step and not an away step)  the line search
is unconstrained (i.e.  γmax = ∞).

4

For both algorithm variants  the second LMO query increases the computational complexity. Note
that an exact search on S is feasible in practice as |S| has at most t elements at iteration t.
Taking an additional computational burden allows to update the weights of all active atoms in the
spirit of OMP. This approach is implemented in the Fully Corrective MP (FCMP)  Algorithm 4.

Algorithm 3 Away-steps (AMP) and Pairwise
(PWMP) Non-Negative Matching Pursuit
1: init x0 = 0 ∈ A  and S := {x0}
2: for t = 0 . . . T
3:
4:
5:
6:
7:
8:

Find zt := (Approx-)LMOA(∇f (xt))
Find vt := (Approx-)LMOS (−∇f (xt))
S = S ∪ zt
AMP: dt = arg mind∈{zt −vt}(cid:104)∇f (xt)  d(cid:105)
PWMP: dt = zt − vt
γ := min
L(cid:107)dt(cid:107)2
Update αzt  αvt and S according to γ
Update xt+1 := xt + γdt

(cid:110)(cid:104)−∇f (xt) dt(cid:105)

(γmax see text)

(γ see text)

  γmax

(cid:111)

9:

Algorithm 4 Fully Corrective Non-Negative
Matching Pursuit (FCMP)
1: init x0 = 0 ∈ A S = {x0}
2: for t = 0 . . . T
3:
4:
5:

Find zt := (Approx-)LMOA(∇f (xt))
S := S ∪ {zt}
Variant 0:

(cid:107)x−(xt− 1

L∇f (xt))(cid:107)2

xt+1 = arg min
x∈cone(S)

6:

Variant 1:
Remove atoms with zero weights from S

xt+1 = arg minx∈cone(S) f (x)

7:
8: end for

10:
11: end for
At each iteration  Algorithm 4 maintains the set of active atoms S by adding zt and removing atoms
with zero weights after the update. In Variant 0  the algorithm minimizes the quadratic upper bound
gxt(x) on f at xt (see Section 2) imitating a gradient descent step with projection onto a “varying”
target  i.e.  cone(S). In Variant 1  the original objective f is minimized over cone(S) at each iteration 
which is in general more efﬁcient than minimizing f over cone(A) using a generic solver for cone
2(cid:107)y − x(cid:107)2  y ∈ H  Variant 1 recovers Algorithm 1 in [52] and
constrained problems. For f (x) = 1
the OMP variant in [5] which both only apply to this speciﬁc objective f.

3.2 Computational Complexity

O(1/t)

C + O(d)

convergence

C + O(d + td)

cost per iteration

algorithm
NNMP
PWMP
AMP
FCMP v. 0
FCMP v. 1

We brieﬂy discuss the computa-
tional complexity of the algorithms
we introduced. For H = Rd  sums
and inner products have cost O(d).
Let us assume that each call of the
LMO has cost C on the set A and
O(td) on S. The variants 0 and 1
of FCMP solve a cone problem at
each iteration with cost h0 and h1 
respectively. In general  h0 can be
much smaller than h1. In Table 1
we report the cost per iteration for every algorithm along with the asymptotic convergence rates
derived in Section 4.

Table 1: Computational complexity versus convergence rate (see Sec-
tion 4) for strongly convex objectives

C + O(d + td)
C + O(d) + h0
C + O(d) + h1

O(cid:0)e−βk(t)(cid:1)
(cid:16)
2 k(t)(cid:17)
O(cid:0)e−βk(t)(cid:1)
O(cid:0)e−βk(t)(cid:1)

3|A|!+1

3|A|!+1

e− β

-
t

t

t

k(t)

t/2

O

4 Convergence Rates

In this section  we present convergence guarantees for Algorithms 2  3  and 4. All proofs are deferred
to the Appendix in the supplementary material. We write x(cid:63) ∈ arg minx∈cone(A) f (x) for an optimal
solution. Our rates will depend on the atomic norm of the solution and the iterates of the respective
algorithm variant:

ρ = max{(cid:107)x(cid:63)(cid:107)A (cid:107)x0(cid:107)A . . .  (cid:107)xT(cid:107)A} .

(6)
If the optimum is not unique  we consider x(cid:63) to be one of largest atomic norm. A more intuitive
and looser notion is to simply upper-bound ρ by the diameter of the level set of the initial iterate
x0 measured by the atomic norm. Then  boundedness follows since the presented method is a
descent method (due to Lemma 1 and line search on the quadratic upper bound  each iteration strictly

5

f (xt) − f (x(cid:63)) ≤ 4(cid:0) 2

(cid:1)

 

δ Lρ2 radius(A)2 + ε0

δt + 4

decreases the objective and our method stops only at the optimum). This justiﬁes the statement
f (xt) ≤ f (x0). Hence  ρ must be bounded for any sequence of iterates produced by the algorithm 
and the convergence rates presented in this section are valid as T goes to inﬁnity. A similar notion to
measure the convergence of MP was established in [32]. All of our algorithms and rates can be made
afﬁne invariant. We defer this discussion to Appendix B.

4.1 Sublinear Convergence

We now present the convergence results for the non-negative and Fully-Corrective Matching Pursuit
algorithms. Sublinear convergence of Algorithm 3 is addressed in Theorem 3.
Theorem 2. Let A ⊂ H be a bounded set with 0 ∈ A  ρ := max{(cid:107)x(cid:63)(cid:107)A (cid:107)x0(cid:107)A  . . .  (cid:107)xT(cid:107)A }
and f be L-smooth over ρ conv(A ∪ −A). Then  Algorithms 2 and 4 converge for t ≥ 0 as

where δ ∈ (0  1] is the relative accuracy parameter of the employed approximate LMO (see Equa-
tion (3)).
Relation to FW rates. By rescaling A by a large enough factor τ > 0  FW with τA as atom
set could in principle be used to solve (4). In fact  for large enough τ  only the constraints of (4)
become active when minimizing f over conv(τA). The sublinear convergence rate obtained with
this approach is up to constants identical to that in Theorem 2 for our MP variants  see [21]. However 
as the correct scaling is unknown  one has to either take the risk of choosing τ too small and hence
failing to recover an optimal solution of (4)  or to rely on too large τ which can result in slow
convergence. In contrast  knowledge of ρ is not required to run our MP variants.

If A is symmetric  we have that lin(A) = cone(A) and it is easy to show
Relation to MP rates.
that the additional direction − xt(cid:107)xt(cid:107) in Algorithm 2 is never selected. Therefore  Algorithm 2 becomes
equivalent to Variant 0 of Algorithm 1  while Variant 1 of Algorithm 1 is equivalent to Variant 0 of
Algorithm 4. The rate speciﬁed in Theorem 2 hence generalizes the sublinear rate in [32  Theorem 2]
for symmetric A.

4.2 Linear Convergence

We start by recalling some of the geometric complexity quantities that were introduced in the context
of FW and are adapted here to the optimization problem we aim to solve (minimization over cone(A)
instead of conv(A)).
Directional Width. The directional width of a set A w.r.t. a direction r ∈ H is deﬁned as:

(7)
Pyramidal Directional Width [28]. The Pyramidal Directional Width of a set A with respect to a
direction r and a reference point x ∈ conv(A) is deﬁned as:

dirW (A  r) := max
s v∈A

(cid:10) r(cid:107)r(cid:107)   s − v(cid:11)

dirW (S ∪ {s(A  r)}  r) 

P dirW (A  r  x) := minS∈Sx

(8)
where Sx := {S | S ⊂ A and x is a proper convex combination of all the elements in S} and
s(A  r) := maxs∈A(cid:104) r(cid:107)r(cid:107)   s(cid:105).
Inspired by the notion of pyramidal width in [28]  which is the minimal pyramidal directional width
computed over the set of feasible directions  we now deﬁne the cone width of a set A where only
the generating faces (g-faces) of cone(A) (instead of the faces of conv(A)) are considered. Before
doing so we introduce the notions of face  generating face  and feasible direction.
Face of a convex set. Let us consider a set K with a k−dimensional afﬁne hull along with a
point x ∈ K. Then  K is a k−dimensional face of conv(A) if K = conv(A) ∩ {y : (cid:104)r  y − x(cid:105) =
0} for some normal vector r and conv(A) is contained in the half-space determined by r  i.e. 
(cid:104)r  y − x(cid:105) ≤ 0  ∀ y ∈ conv(A). Intuitively  given a set conv(A) one can think of conv(A) being a
dim(conv(A))−dimensional face of itself  an edge on the border of the set a 1-dimensional face and
a vertex a 0-dimensional face.

6

Face of a cone and g-faces. Similarly  a k−dimensional face of a cone is an open and unbounded
set cone(A) ∩ {y : (cid:104)r  y − x(cid:105) = 0} for some normal vector r and cone(A) is contained in the half
space determined by r. We can deﬁne the generating faces of a cone as:

g-faces(cone(A)) :={B ∩ conv(A) :B ∈ faces(cone(A))} .

Note that g-faces(cone(A)) ⊂ faces(conv(A)) and conv(A) ∈ g-faces(cone(A)). Furthermore 
for each K ∈ g-faces(cone(A))  cone(K) is a k−dimensional face of cone(A).
We now introduce the notion of feasible directions. A direction d is feasible from x ∈ cone(A) if it
points inwards cone(A)  i.e.  if ∃ε > 0 s.t. x + εd ∈ cone(A). Since a face of the cone is itself a
cone  if a direction is feasible from x ∈ cone(K) \ 0  it is feasible from every positive rescaling of x.
We therefore can consider only the feasible directions on the generating faces (which are closed and
bounded sets). Finally  we deﬁne the cone width of A.

Cone Width.

CWidth(A) :=

min
x∈K

K∈g-faces(cone(A))
r∈cone(K−x)\{0}

P dirW (K ∩ A  r  x)

(9)

We are now ready to show the linear convergence of Algorithms 3 and 4.
Theorem 3. Let A ⊂ H be a bounded set with 0 ∈ A and let the objective function f : H → R be
both L-smooth and µ-strongly convex over ρ conv(A ∪ −A). Then  the suboptimality of the iterates
of Algorithms 3 and 4 decreases geometrically at each step in which γ < αvt (henceforth referred to
as “good steps”) as:

εt+1 ≤ (1 − β) εt 

(10)
where β := δ2 µ CWidth(A)2
L diam(A)2 ∈ (0  1]  εt := f (xt)−f (x(cid:63)) is the suboptimality at step t and δ ∈ (0  1]
is the relative accuracy parameter of the employed approximate LMO (3). For AMP (Algorithm 3) 
βAMP = β/2. If µ = 0 Algorithm 3 converges with rate O(1/k(t)) where k(t) is the number of
“good steps” up to iteration t.

Discussion. To obtain a linear convergence rate  one needs to upper-bound the number of “bad
steps” t− k(t) (i.e.  steps with γ ≥ αvt). We have that k(t) = t for Variant 1 of FCMP (Algorithm 4) 
k(t) ≥ t/2 for AMP (Algorithm 3) and k(t) ≥ t/(3|A|! + 1) for PWMP (Algorithm 3) and Variant 0
of FCMP (Algorithm 4). This yields a global linear convergence rate of εt ≤ ε0 exp (−βk(t)). The
bound for PWMP is very loose and only meaningful for ﬁnite sets A. However  it can be observed
in the experiments in the supplementary material (Appendix A) that only a very small fraction of
iterations result in bad PWMP steps in practice. Further note that Variant 1 of FCMP (Algorithm 4)
does not produce bad steps. Also note that the bounds on the number of good steps given above are
the same as for the corresponding FW variants and are obtained using the same (purely combinatorial)
arguments as in [28].
Relation to previous MP rates. The linear convergence of the generalized (not non-negative) MP
variants studied in [32] crucially depends on the geometry of the set which is characterized by the
Minimal Directional Width mDW(A):

mDW(A) := min
d∈lin(A)
d(cid:54)=0

z∈A(cid:104) d

(cid:107)d(cid:107)   z(cid:105) .

max

(11)

The following Lemma relates the Cone Width with the minimal directional width.
Lemma 4. If the origin is in the relative interior of conv(A) with respect to its linear span  then
cone(A) = lin(A) and CWidth(A) = mDW(A).
Now  if the set A is symmetric or  more generally  if cone(A) spans the linear space lin(A) (which
implies that the origin is in the relative interior of conv(A))  there are no bad steps. Hence  by
Lemma 4  the linear rate obtained in Theorem 3 for non-negative MP variants generalizes the one
presented in [32  Theorem 7] for generalized MP variants.

7

Relation to FW rates. Optimization over conic hulls with non-negative MP is more similar to FW
than to MP itself in the following sense. For MP  every direction in lin(A) allows for unconstrained
steps  from any iterate xt. In contrast  for our non-negative MPs  while some directions allow for
unconstrained steps from some iterate xt  others are constrained  thereby leading to the dependence
of the linear convergence rate on the cone width  a geometric constant which is very similar in spirit
to the Pyramidal Width appearing in the linear convergence bound in [28] for FW. Furthermore  as
for Algorithm 3  the linear rate of Away-steps and Pairwise FW holds only for good steps. We ﬁnally
relate the cone width with the Pyramidal Width [28]. The Pyramidal Width is deﬁned as

PWidth(A) :=

P dirW (K ∩ A  r  x).

min
x∈K

K∈faces(conv(A))
r∈cone(K−x)\{0}

We have CWidth(A) ≥ PWidth(A) as the minimization in the deﬁnition (9) of CWidth(A) is only
over the subset g-faces(cone(A)) of faces(conv(A)). As a consequence  the decrease per iteration
characterized in Theorem 3 is larger than what one could obtain with FW on the rescaled convex set
τA (see Section 4.1 for details about the rescaling). Furthermore  the decrease characterized in [28]
scales as 1/τ 2 due to the dependence on 1/ diam(conv(A))2.

5 Related Work

The line of recent works by [44  46  47  48  37  32] targets the generalization of MP from the
least-squares objective to general smooth objectives and derives corresponding convergence rates
(see [32] for a more in-depth discussion). However  only little prior work targets MP variants with
non-negativity constraint [5  38  52]. In particular  the least-squares objective was addressed and
no rigorous convergence analysis was carried out. [5  52] proposed an algorithm equivalent to our
Algorithm 4 for the least-squares case. More speciﬁcally  [52] then developed an acceleration heuristic 
whereas [5] derived a coherence-based recovery guarantee for sparse linear combinations of atoms.
Apart from MP-type algorithms  there is a large variety of non-negative least-squares algorithms 
e.g.  [30]  in particular also for matrix and tensor spaces. The gold standard in factorization problems
is projected gradient descent with alternating minimization  see [43  4  45  23]. Other related works
are [40]  which is concerned with the feasibility problem on symmetric cones  and [19]  which
introduces a norm-regularized variant of problem (4) and solves it using FW on a rescaled convex
set. To the best of our knowledge  in the context of MP-type algorithms  we are the ﬁrst to combine
general convex objectives with conic constraints and to derive corresponding convergence guarantees.

Boosting:
In an earlier line of work  a ﬂavor of the generalized MP became popular in the context
of boosting  see [35]. The literature on boosting is vast  we refer to [42  35  7] for a general overview.
Taking the optimization perspective given in [42]  boosting is an iterative greedy algorithm minimizing
a (strongly) convex objective over the linear span of a possibly inﬁnite set called hypothesis class.
The convergence analysis crucially relies on the assumption of the origin being in the relative interior
of the hypothesis class  see Theorem 1 in [17]. Indeed  Algorithm 5.2 of [35] might not converge
if the [39] alignment assumption is violated. Here  we managed to relax this assumption while
preserving essentially the same asymptotic rates in [35  17]. Our work is therefore also relevant in
the context of (non-negative) boosting.

Illustrative Experiments

6
We illustrate the performance of the presented algorithms on three different exemplary tasks  showing
that our algorithms are competitive with established baselines across a wide range of objective func-
tions  domains  and data sets while not being speciﬁcally tailored to any of these tasks (see Section 3.2
for a discussion of the computational complexity of the algorithms). Additional experiments targeting
KL divergence NMF  non-negative tensor factorization  and hyperspectral image unmixing can be
found in the appendix.

Synthetic data. We consider minimizing the least squares objective on the conic hull
of 100 unit-norm vectors sampled at random in the ﬁrst orthant of R50. We compare
the convergence of Algorithms 2  3  and 4 with the Fast Non-Negative MP (FNNOMP)
of [52]  and Variant 3 (line-search) of the FW algorithm in [32] on the atom set rescaled
by τ = 10(cid:107)y(cid:107) (see Section 4.1)  observing linear convergence for our corrective variants.

8

Figure 2 shows the suboptimality εt  averaged over 20
realizations of A and y  as a function of the iteration t. As
expected  FCMP achieves fastest convergence followed
by PWMP  AMP and NNMP. The FNNOMP gets stuck
instead. Indeed  [52] only show that the algorithm termi-
nates and not its convergence.

Figure 2: Synthetic data experiment.

Non-negative matrix factorization. The second task
consists of decomposing a given matrix into the product
of two non-negative matrices as in Equation (1) of [20].
We consider the intersection of the positive semideﬁnite
cone and the positive orthant. We parametrize the set A as
the set of matrices obtained as an outer product of vectors
from A1 = {z ∈ Rk : zi ≥ 0 ∀ i} and A2 = {z ∈ Rd : zi ≥ 0 ∀ i}. The LMO is approximated
using a truncated power method [55]  and we perform atom correction with greedy coordinate descent
see  e.g.  [29  18]  to obtain a better objective value while maintaining the same (small) number of
atoms. We consider three different datasets: The Reuters Corpus2  the CBCL face dataset3 and the
KNIX dataset4. The subsample of the Reuters corpus we used is a term frequency matrix of 7 769
documents and 26 001 words. The CBCL face dataset is composed of 2 492 images of 361 pixels
each  arranged into a matrix. The KNIX dataset contains 24 MRI slices of a knee  arranged in a
matrix of size 262  144 × 24. Pixels are divided by their overall mean intensity. For interpretability
reasons  there is interest to decompose MRI data into non-negative factorizations [25]. We compare
PWMP and FCMP against the multiplicative (mult) and the alternating (als) algorithm of [4]  and the
greedy coordinate descent (GCD) of [20]. Since the Reuters corpus is much larger than the CBCL
and the KNIX dataset we only used the GCD for which a fast implementation in C is available. We
report the objective value for ﬁxed values of the rank in Table 2  showing that FCMP outperform all
the baselines across all the datasets. PWMP achieves smallest error on the Reuters corpus.

Non-negative garrote. We consider the non-negative garrote which is a common approach to
model order selection [6]. We evaluate NNMP  PWMP  and FCMP in the experiment described
in [33]  where the non-negative garrote is used to perform model order selection for logistic regression
(i.e.  for a non-quadratic objective function). We evaluated training and test accuracy on 100 random
splits of the sonar dataset from the UCI machine learning repository. In Table 3 we compare the
median classiﬁcation accuracy of our algorithms with that of the cyclic coordinate descent algorithm
(NNG) from [33].

-
-

CBCL
K = 50

algorithm Reuters
CBCL
KNIX
K = 10
K = 10
K = 10
mult
2.4241e3 1.1405e3 2.4471e03
2.7292e03
2.73e3
als
GCD
5.9799e5 2.2372e3
2.2372e03
PWMP
5.9591e5 2.2494e3 789.901 2.2494e03
FCMP
5.9762e5 2.2364e3
2.2364e03
Table 2: Objective value for least-squares non-negative
matrix factorization with rank K.

3.84e3
806

786.15

training accuracy

test accuracy
NNMP 0.8345 ± 0.0242 0.7419 ± 0.0389
PWMP 0.8379 ± 0.0240 0.7419 ± 0.0392
FCMP 0.8345 ± 0.0238 0.7419 ± 0.0403
0.8069 ± 0.0518 0.7258 ± 0.0602
NNG

Table 3: Logistic Regression with non-negative
Garrote  median ± std. dev.

7 Conclusion

In this paper  we considered greedy algorithms for optimization over the convex cone  parametrized
as the conic hull of a generic atom set. We presented a novel formulation of NNMP along with a
comprehensive convergence analysis. Furthermore  we introduced corrective variants with linear
convergence guarantees  and veriﬁed this convergence rate in numerical applications. We believe that
the generality of our novel analysis will be useful to design new  fast algorithms with convergence
guarantees  and to study convergence of existing heuristics  in particular in the context of non-negative
matrix and tensor factorization.

2http://www.nltk.org/book/ch02.html
3http://cbcl.mit.edu/software-datasets/FaceData2.html
4http://www.osirix-viewer.com/resources/dicom-image-library/

9

01020304050Iteration10-410-2100102104SuboptimalitySynthetic dataPWMP (Alg. 3)NNMP (Alg. 2)FCMP (Alg. 4)FWAMP (Alg. 3)FNNOMPReferences
[1] Animashree Anandkumar  Rong Ge  Daniel J Hsu  Sham M Kakade  and Matus Telgarsky.
Tensor decompositions for learning latent variable models. Journal of Machine Learning
Research  15(1):2773–2832  2014.

[2] Mário César Ugulino Araújo  Teresa Cristina Bezerra Saldanha  Roberto Kawakami Harrop
Galvao  Takashi Yoneyama  Henrique Caldas Chame  and Valeria Visani. The successive projec-
tions algorithm for variable selection in spectroscopic multicomponent analysis. Chemometrics
and Intelligent Laboratory Systems  57(2):65–73  2001.

[3] Jonas Behr  André Kahles  Yi Zhong  Vipin T Sreedharan  Philipp Drewe  and Gunnar Rätsch.
Mitie: Simultaneous rna-seq-based transcript identiﬁcation and quantiﬁcation in multiple
samples. Bioinformatics  29(20):2529–2538  2013.

[4] Michael W Berry  Murray Browne  Amy N Langville  V Paul Pauca  and Robert J Plemmons.
Algorithms and applications for approximate nonnegative matrix factorization. Computational
statistics & data analysis  52(1):155–173  2007.

[5] Alfred M Bruckstein  Michael Elad  and Michael Zibulevsky. On the uniqueness of nonnegative
sparse solutions to underdetermined systems of equations. IEEE Transactions on Information
Theory  54(11):4813–4820  2008.

[6] P Bühlmann and B Yu. Boosting  model selection  lasso and nonnegative garrote. Technical

Report 127  Seminar für Statistik ETH Zürich  2005.

[7] Peter Bühlmann and Bin Yu. Boosting. Wiley Interdisciplinary Reviews: Computational

Statistics  2(1):69–74  2010.

[8] Martin Burger. Inﬁnite-dimensional optimization and optimal design. 2003.
[9] Sheng Chen  Stephen A Billings  and Wan Luo. Orthogonal least squares methods and their
application to non-linear system identiﬁcation. International Journal of control  50(5):1873–
1896  1989.

[10] Andrzej Cichocki and PHAN Anh-Huy. Fast local algorithms for large scale nonnegative matrix
and tensor factorizations. IEICE transactions on fundamentals of electronics  communications
and computer sciences  92(3):708–721  2009.

[11] Ernie Esser  Yifei Lou  and Jack Xin. A method for ﬁnding structured sparse solutions to
nonnegative least squares problems with applications. SIAM Journal on Imaging Sciences 
6(4):2010–2046  2013.

[12] M Frank and P Wolfe. An algorithm for quadratic programming. Naval research logistics

quarterly  1956.

[13] Nicolas Gillis. Successive nonnegative projection algorithm for robust nonnegative blind source

separation. SIAM Journal on Imaging Sciences  7(2):1420–1450  2014.

[14] Nicolas Gillis and François Glineur. Accelerated multiplicative updates and hierarchical als
algorithms for nonnegative matrix factorization. Neural Computation  24(4):1085–1105  2012.
[15] Nicolas Gillis  Da Kuang  and Haesun Park. Hierarchical clustering of hyperspectral images
using rank-two nonnegative matrix factorization. IEEE Transactions on Geoscience and Remote
Sensing  53(4):2066–2078  2015.

[16] Nicolas Gillis and Robert Luce. A fast gradient method for nonnegative sparse regression with

self dictionary. arXiv preprint arXiv:1610.01349  2016.

[17] Alexander Grubb and J Andrew Bagnell. Generalized boosting algorithms for convex optimiza-

tion. arXiv preprint arXiv:1105.2054  2011.

[18] Xiawei Guo  Quanming Yao  and James T Kwok. Efﬁcient sparse low-rank tensor completion

using the Frank-Wolfe algorithm. In AAAI Conference on Artiﬁcial Intelligence  2017.

[19] Zaid Harchaoui  Anatoli Juditsky  and Arkadi Nemirovski. Conditional gradient algorithms for
norm-regularized smooth convex optimization. Mathematical Programming  152(1-2):75–112 
2015.

[20] Cho-Jui Hsieh and Inderjit S Dhillon. Fast coordinate descent methods with variable selection
for non-negative matrix factorization. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining  pages 1064–1072. ACM  2011.

[21] Martin Jaggi. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization. In ICML

2013 - Proceedings of the 30th International Conference on Machine Learning  2013.

10

[22] Hyunsoo Kim  Haesun Park  and Lars Elden. Non-negative tensor factorization based on
alternating large-scale non-negativity-constrained least squares. In Bioinformatics and Bioengi-
neering  2007. BIBE 2007. Proceedings of the 7th IEEE International Conference on  pages
1147–1151. IEEE  2007.

[23] Jingu Kim  Yunlong He  and Haesun Park. Algorithms for nonnegative matrix and tensor
factorizations: A uniﬁed view based on block coordinate descent framework. Journal of Global
Optimization  58(2):285–319  2014.

[24] Jingu Kim and Haesun Park. Fast nonnegative tensor factorization with an active-set-like

method. In High-Performance Scientiﬁc Computing  pages 311–326. Springer  2012.

[25] Ivica Kopriva and Andrzej Cichocki. Nonlinear band expansion and 3d nonnegative tensor
factorization for blind decomposition of magnetic resonance image of the brain. In International
Conference on Latent Variable Analysis and Signal Separation  pages 490–497. Springer  2010.
[26] Abhishek Kumar  Vikas Sindhwani  and Prabhanjan Kambadur. Fast conical hull algorithms for

near-separable non-negative matrix factorization. In ICML (1)  pages 231–239  2013.

[27] Simon Lacoste-Julien and Martin Jaggi. An Afﬁne Invariant Linear Convergence Analysis for
Frank-Wolfe Algorithms. In NIPS 2013 Workshop on Greedy Algorithms  Frank-Wolfe and
Friends  December 2013.

[28] Simon Lacoste-Julien and Martin Jaggi. On the Global Linear Convergence of Frank-Wolfe

Optimization Variants. In NIPS 2015  pages 496–504  2015.

[29] Sören Laue. A Hybrid Algorithm for Convex Semideﬁnite Optimization. In ICML  2012.
[30] Charles L Lawson and Richard J Hanson. Solving least squares problems  volume 15. SIAM 

1995.

[31] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In

Advances in neural information processing systems  pages 556–562  2001.

[32] Francesco Locatello  Rajiv Khanna  Michael Tschannen  and Martin Jaggi. A uniﬁed optimiza-
tion view on generalized matching pursuit and frank-wolfe. In Proc. International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS)  2017.

[33] Enes Makalic and Daniel F Schmidt. Logistic regression with the nonnegative garrote. In

Australasian Joint Conference on Artiﬁcial Intelligence  pages 82–91. Springer  2011.

[34] Stéphane Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE

Transactions on Signal Processing  41(12):3397–3415  1993.

[35] Ron Meir and Gunnar Rätsch. An introduction to boosting and leveraging. In Advanced lectures

on machine learning  pages 118–183. Springer  2003.

[36] José MP Nascimento and José MB Dias. Vertex component analysis: A fast algorithm to unmix
hyperspectral data. IEEE transactions on Geoscience and Remote Sensing  43(4):898–910 
2005.

[37] Hao Nguyen and Guergana Petrova. Greedy strategies for convex optimization. Calcolo  pages

1–18  2014.

[38] Robert Peharz  Michael Stark  and Franz Pernkopf. Sparse nonnegative matrix factorization

using l0-constraints. In IEEE  editor  Proceedings of MLSP  pages 83 – 88  Aug 2010.

[39] Javier Pena and Daniel Rodriguez. Polytope conditioning and linear convergence of the frank-

wolfe algorithm. arXiv preprint arXiv:1512.06142  2015.

[40] Javier Pena and Negar Soheili. Solving conic systems via projection and rescaling. Mathematical

Programming  pages 1–25  2016.

[41] Aleksei Pogorelov. Extrinsic geometry of convex surfaces  volume 35. American Mathematical

Soc.  1973.

[42] Gunnar Rätsch  Sebastian Mika  Manfred K Warmuth  et al. On the convergence of leveraging.

In NIPS  pages 487–494  2001.

[43] F Sha  LK Saul  and Daniel D Lee. Multiplicative updates for nonnegative quadratic program-
ming in support vector machines. Advances in Neural Information Processing Systems  15 
2002.

[44] Shai Shalev-Shwartz  Nathan Srebro  and Tong Zhang. Trading Accuracy for Sparsity in
Optimization Problems with Sparsity Constraints. SIAM Journal on Optimization  20:2807–
2832  2010.

11

[45] Amnon Shashua and Tamir Hazan. Non-negative tensor factorization with applications to
statistics and computer vision. In Proceedings of the 22nd international conference on Machine
learning  pages 792–799. ACM  2005.

[46] Vladimir Temlyakov. Chebushev Greedy Algorithm in convex optimization. arXiv.org  Decem-

ber 2013.

[47] Vladimir Temlyakov. Greedy algorithms in convex optimization on Banach spaces. In 48th

Asilomar Conference on Signals  Systems and Computers  pages 1331–1335. IEEE  2014.

[48] VN Temlyakov. Greedy approximation in convex optimization. Constructive Approximation 

41(2):269–296  2015.

[49] Joel A Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Transactions

on Information Theory  50(10):2231–2242  2004.

[50] Zheng Wang  Ming jun Lai  Zhaosong Lu  Wei Fan  Hasan Davulcu  and Jieping Ye. Rank-one

matrix pursuit for matrix completion. In ICML  pages 91–99  2014.

[51] Max Welling and Markus Weber. Positive tensor factorization. Pattern Recognition Letters 

22(12):1255–1261  2001.

[52] Mehrdad Yaghoobi  Di Wu  and Mike E Davies. Fast non-negative orthogonal matching pursuit.

IEEE Signal Processing Letters  22(9):1229–1233  2015.

[53] Yuning Yang  Siamak Mehrkanoon  and Johan A K Suykens. Higher order Matching Pursuit

for Low Rank Tensor Learning. arXiv.org  March 2015.

[54] Quanming Yao and James T Kwok. Greedy learning of generalized low-rank models. In IJCAI 

2016.

[55] Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. J.

Mach. Learn. Res.  14(1):899–925  April 2013.

12

,Dongqu Chen
Francesco Locatello
Michael Tschannen
Gunnar Raetsch
Martin Jaggi