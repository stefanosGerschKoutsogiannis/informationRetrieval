2017,A Regularized Framework for Sparse and Structured Neural Attention,Modern neural networks are often augmented with an attention mechanism  which tells the network where to focus within the input.  We propose in this paper a new framework for sparse and structured attention  building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities  suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However  we also show how our framework can incorporate modern structured penalties  resulting in more interpretable attention mechanisms  that focus on entire segments or groups of an input.  We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms  enabling their use in a neural network trained with backpropagation.  To showcase their potential as a drop-in replacement for existing ones  we evaluate our attention mechanisms on three large-scale tasks: textual entailment  machine translation  and sentence summarization.  Our attention mechanisms improve interpretability without sacrificing performance; notably  on textual entailment and summarization  we outperform the standard attention mechanisms based on softmax and sparsemax.,A Regularized Framework for

Sparse and Structured Neural Attention

Vlad Niculae∗
Cornell University

Ithaca  NY

vlad@cs.cornell.edu

Mathieu Blondel

NTT Communication Science Laboratories

Kyoto  Japan

mathieu@mblondel.org

Abstract

Modern neural networks are often augmented with an attention mechanism  which
tells the network where to focus within the input. We propose in this paper a
new framework for sparse and structured attention  building upon a smoothed
max operator. We show that the gradient of this operator deﬁnes a mapping from
real values to probabilities  suitable as an attention mechanism. Our framework
includes softmax and a slight generalization of the recently-proposed sparsemax as
special cases. However  we also show how our framework can incorporate modern
structured penalties  resulting in more interpretable attention mechanisms  that
focus on entire segments or groups of an input. We derive efﬁcient algorithms to
compute the forward and backward passes of our attention mechanisms  enabling
their use in a neural network trained with backpropagation. To showcase their
potential as a drop-in replacement for existing ones  we evaluate our attention
mechanisms on three large-scale tasks: textual entailment  machine translation  and
sentence summarization. Our attention mechanisms improve interpretability with-
out sacriﬁcing performance; notably  on textual entailment and summarization  we
outperform the standard attention mechanisms based on softmax and sparsemax.

1

Introduction

Modern neural network architectures are commonly augmented with an attention mechanism  which
tells the network where to look within the input in order to make the next prediction. Attention-
augmented architectures have been successfully applied to machine translation [2  29]  speech
recognition [10]  image caption generation [44]  textual entailment [38  31]  and sentence summariza-
tion [39]  to name but a few examples. At the heart of attention mechanisms is a mapping function
that converts real values to probabilities  encoding the relative importance of elements in the input.
For the case of sequence-to-sequence prediction  at each time step of generating the output sequence 
attention probabilities are produced  conditioned on the current state of a decoder network. They are
then used to aggregate an input representation (a variable-length list of vectors) into a single vector 
which is relevant for the current time step. That vector is ﬁnally fed into the decoder network to
produce the next element in the output sequence. This process is repeated until the end-of-sequence
symbol is generated. Importantly  such architectures can be trained end-to-end using backpropagation.

Alongside empirical successes  neural attention—while not necessarily correlated with human
attention—is increasingly crucial in bringing more interpretability to neural networks by help-
ing explain how individual input elements contribute to the model’s decisions. However  the most
commonly used attention mechanism  softmax  yields dense attention weights: all elements in the in-
put always make at least a small contribution to the decision. To overcome this limitation  sparsemax
was recently proposed [31]  using the Euclidean projection onto the simplex as a sparse alternative to

∗Work performed during an internship at NTT Commmunication Science Laboratories  Kyoto  Japan.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Attention weights produced by the proposed fusedmax  compared to softmax and sparsemax 
on sentence summarization. The input sentence to be summarized (taken from [39]) is along the
x-axis. From top to bottom  each row shows where the attention is distributed when producing
each word in the summary. All rows sum to 1  the grey background corresponds to exactly 0 (never
achieved by softmax)  and adjacent positions with exactly equal weight are not separated by borders.
Fusedmax pays attention to contiguous segments of text with equal weight; such segments never
occur with softmax and sparsemax. In addition to enhancing interpretability  we show in §4.3 that
fusedmax outperforms both softmax and sparsemax on this task in terms of ROUGE scores.

softmax. Compared to softmax  sparsemax outputs more interpretable attention weights  as illustrated
in [31] on the task of textual entailment. The principle of parsimony  which states that simple expla-
nations should be preferred over complex ones  is not  however  limited to sparsity: it remains open
whether new attention mechanisms can be designed to beneﬁt from more structural prior knowledge.

Our contributions. The success of sparsemax motivates us to explore new attention mechanisms
that can both output sparse weights and take advantage of structural properties of the input through
the use of modern sparsity-inducing penalties. To do so  we make the following contributions:

1) We propose a new general framework that builds upon a max operator  regularized with a strongly
convex function. We show that this operator is differentiable  and that its gradient deﬁnes a mapping
from real values to probabilities  suitable as an attention mechanism. Our framework includes as
special cases both softmax and a slight generalization of sparsemax. (§2)

2) We show how to incorporate the fused lasso [42] in this framework  to derive a new attention
mechanism  named fusedmax  which encourages the network to pay attention to contiguous segments
of text when making a decision. This idea is illustrated in Figure 1 on sentence summarization. For
cases when the contiguity assumption is too strict  we show how to incorporate an OSCAR penalty
[7] to derive a new attention mechanism  named oscarmax  that encourages the network to pay equal
attention to possibly non-contiguous groups of words. (§3)

3) In order to use attention mechanisms deﬁned under our framework in an autodiff toolkit  two
problems must be addressed: evaluating the attention itself and computing its Jacobian. However 
our attention mechanisms require solving a convex optimization problem and do not generally
enjoy a simple analytical expression  unlike softmax. Computing the Jacobian of the solution of
an optimization problem is called argmin/argmax differentiation and is currently an area of active
research (cf. [1] and references therein). One of our key algorithmic contributions is to show how
to compute this Jacobian under our general framework  as well as for fused lasso and OSCAR. (§3)

4) To showcase the potential of our new attention mechanisms as a drop-in replacement for existing
ones  we show empirically that our new attention mechanisms enhance interpretability while achieving
comparable or better accuracy on three diverse and challenging tasks: textual entailment  machine
translation  and sentence summarization. (§4)

Notation. We denote the set {1  . . .   d} by [d]. We denote the (d − 1)-dimensional probability
simplex by ∆d := {x ∈ Rd : kxk1 = 1  x ≥ 0} and the Euclidean projection onto it by P∆d (x) :=
arg miny∈∆d ky − xk2. Given a function f : Rd → R ∪ {∞}  its convex conjugate is deﬁned by
f ∗(x) := supy∈dom f yTx−f (y). Given a norm k·k  its dual is deﬁned by kxk∗ := supkyk≤1 yTx.
We denote the subdifferential of a function f at y by ∂f (y). Elements of the subdifferential are
called subgradients and when f is differentiable  ∂f (y) contains a single element  the gradient of f
at y  denoted by ∇f (y). We denote the Jacobian of a function g : Rd → Rd at y by Jg(y) ∈ Rd×d
and the Hessian of a function f : Rd → R at y by Hf (y) ∈ Rd×d.

2

russiandefenseministerivanovcalledsundayforthecreationofajointfrontforcombatingglobalterrorism.russiandefenseministercallsforjointfrontagainstterrorism<EOS>fusedmaxrussiandefenseministerivanovcalledsundayforthecreationofajointfrontforcombatingglobalterrorism.softmaxrussiandefenseministerivanovcalledsundayforthecreationofajointfrontforcombatingglobalterrorism.sparsemaxFigure 2: The proposed maxΩ(x) operator up to a constant (left) and the proposed ΠΩ(x) mapping
(right)  illustrated with x = [t  0] and γ = 1. In this case  maxΩ(x) is a ReLu-like function and
ΠΩ(x) is a sigmoid-like function. Our framework recovers softmax (negative entropy) and sparsemax
(squared 2-norm) as special cases. We also introduce three new attention mechanisms: sq-pnorm-max
(squared p-norm  here illustrated with p = 1.5)  fusedmax (squared 2-norm + fused lasso)  and
oscarmax (squared 2-norm + OSCAR; not pictured since it is equivalent to fusedmax in 2-d). Except
for softmax  which never exactly reaches 0  all mappings shown on the right encourage sparse outputs.

2 Proposed regularized attention framework

2.1 The max operator and its subgradient mapping

To motivate our proposal  we ﬁrst show in this section that the subgradients of the maximum operator
deﬁne a mapping from Rd to ∆d  but that this mapping is highly unsuitable as an attention mechanism.
The maximum operator is a function from Rd to R and can be deﬁned by

max(x) := max
i∈[d]

xi = sup
y∈∆d

yTx.

The equality on the r.h.s comes from the fact that the supremum of a linear form over the simplex
is always achieved at one of the vertices  i.e.  one of the standard basis vectors {ei}d
i=1. Moreover 
it is not hard to check that any solution y⋆ of that supremum is precisely a subgradient of max(x):
∂ max(x) = {ei⋆ : i⋆ ∈ arg maxi∈[d] xi}. We can see these subgradients as a mapping Π : Rd →
∆d that puts all the probability mass onto a single element: Π(x) = ei for any ei ∈ ∂ max(x).
However  this behavior is undesirable  as the resulting mapping is a discontinuous function (a
Heaviside step function when x = [t  0])  which is not amenable to optimization by gradient descent.

2.2 A regularized max operator and its gradient mapping

These shortcomings encourage us to consider a regularization of the maximum operator. Inspired by
the seminal work of Nesterov [35]  we apply a smoothing technique. The conjugate of max(x) is

For a proof  see for instance [33  Appendix B]. We now add regularization to the conjugate

max∗(y) =(cid:26)0 

∞ 

if y ∈ ∆d
o.w.

.

max∗

Ω(y) :=(cid:26)γΩ(y) 

∞ 

if y ∈ ∆d
o.w.

 

where we assume that Ω : Rd → R is β-strongly convex w.r.t. some norm k · k and γ > 0 controls
the regularization strength. To deﬁne a smoothed max operator  we take the conjugate once again

maxΩ(x) = max∗∗

Ω (x) = sup
y∈Rd

yTx − max∗

Ω(y) = sup
y∈∆d

yTx − γΩ(y).

(1)

Our main proposal is a mapping ΠΩ : Rd → ∆d  deﬁned as the argument that achieves this supremum.

ΠΩ(x) := arg max

yTx − γΩ(y) = ∇maxΩ(x)

y∈∆d

Ω(y⋆) ⇔ y⋆ ∈ ∂maxΩ(x) and ii)
The r.h.s. holds by combining that i) maxΩ(x) = (y⋆)Tx − max∗
∂maxΩ(x) = {∇maxΩ(x)}  since (1) has a unique solution. Therefore  ΠΩ is a gradient mapping.
We illustrate maxΩ and ΠΩ for various choices of Ω in Figure 2 (2-d) and in Appendix C.1 (3-d).

3

42024t01234max([t 0])+ constmaxsoftmaxsparsemaxsq-pnorm-maxfusedmax42024t0.000.250.500.751.00([t 0])1=max([t 0])1Importance of strong convexity. Our β-strong convexity assumption on Ω plays a crucial role and
should not be underestimated. Recall that a function f : Rd → R is β-strongly convex w.r.t. a norm
k · k if and only if its conjugate f ∗ is 1
β -smooth w.r.t. the dual norm k · k∗ [46  Corollary 3.5.11]
[22  Theorem 3]. This is sufﬁcient to ensure that maxΩ is 1
γβ -smooth  or  in other words  that it is
differentiable everywhere and its gradient  ΠΩ  is 1

γβ -Lipschitz continuous w.r.t. k · k∗.

Training by backpropagation. In order to use ΠΩ in a neural network trained by backpropagation 
two problems must be addressed for any regularizer Ω. The ﬁrst is the forward computation: how
to evaluate ΠΩ(x)  i.e.  how to solve the optimization problem in (1). The second is the backward
computation: how to evaluate the Jacobian of ΠΩ(x)  or  equivalently  the Hessian of maxΩ(x). One
of our key contributions  presented in §3  is to show how to solve these two problems for general
differentiable Ω  as well as for two structured regularizers: fused lasso and OSCAR.

2.3 Recovering softmax and sparsemax as special cases

Before deriving new attention mechanisms using our framework  we now show how we can recover
softmax and sparsemax  using a speciﬁc regularizer Ω.

i=1 yi log yi  the negative entropy. The conjugate of the negative
entropy restricted to the simplex is the log sum exp [9  Example 3.25]. Moreover  if f (x) = γg(x)
for γ > 0  then f ∗(y) = γg∗(y/γ). We therefore get a closed-form expression: maxΩ(x) =
i=1 exi/γ. Since the negative entropy is 1-strongly convex w.r.t.
γ -smooth w.r.t. k · k∞. We obtain the classical softmax  with

Softmax. We choose Ω(y) = Pd
γ log sum exp(x/γ) := γ logPd

k · k1 over ∆d  we get that maxΩ is 1
temperature parameter γ  by taking the gradient of maxΩ(x) 

ΠΩ(x) =

 

(softmax)

ex/γ
i=1 exi/γ

Pd

where ex/γ is evaluated element-wise. Note that some authors also call maxΩ a “soft max.” Although
ΠΩ is really a soft arg max  we opt to follow the more popular terminology. When x = [t  0]  it can
be checked that maxΩ(x) reduces to the softplus [16] and ΠΩ(x)1 to a sigmoid.
Sparsemax. We choose Ω(y) = 1
operator theory [35  36]. Since 1
w.r.t. k · k2. In addition  it is easy to verify that

2  also known as Moreau-Yosida regularization in proximal
γ -smooth

2 kyk2

2 kyk2

2 is 1-strongly convex w.r.t. k·k2  we get that maxΩ is 1

ΠΩ(x) = P∆d (x/γ) = arg min

y∈∆d

ky − x/γk2.

(sparsemax)

This mapping was introduced as is in [31] with γ = 1 and was named sparsemax  due to the fact that
it is a sparse alternative to softmax. Our derivation thus gives us a slight generalization  where γ
controls the sparsity (the smaller  the sparser) and could be tuned; in our experiments  however  we
follow the literature and set γ = 1. The Euclidean projection onto the simplex  P∆d   can be computed
exactly [34  15] (we discuss the complexity in Appendix B). Following [31]  the Jacobian of ΠΩ is

JΠΩ (x) =

1
γ

JP∆d (x/γ) =

1

γ (cid:0)diag(s) − ssT/ksk1(cid:1)  

where s ∈ {0  1}d indicates the nonzero elements of ΠΩ(x). Since ΠΩ is Lipschitz continuous 
Rademacher’s theorem implies that ΠΩ is differentiable almost everywhere. For points where ΠΩ is
not differentiable (where maxΩ is not twice differentiable)  we can take an arbitrary matrix in the set
JΠΩ (xt) [31].
of Clarke’s generalized Jacobians [11]  the convex hull of Jacobians of the form lim
xt→x

3 Deriving new sparse and structured attention mechanisms

3.1 Differentiable regularizer Ω

Before tackling more structured regularizers  we address in this section the case of general differen-
tiable regularizer Ω. Because ΠΩ(x) involves maximizing (1)  a concave function over the simplex 
it can be computed globally using any off-the-shelf projected gradient solver. Therefore  the main
challenge is how to compute the Jacobian of ΠΩ. This is what we address in the next proposition.

4

Proposition 1 Jacobian of ΠΩ for any differentiable Ω (backward computation)
Assume that Ω is differentiable over ∆d and that ΠΩ(x) = arg maxy∈∆d yTx − γΩ(y) = y⋆ has
been computed. Then the Jacobian of ΠΩ at x  denoted JΠΩ   can be obtained by solving the system

where we deﬁned the shorthands A := JP∆d (y⋆ − γ∇Ω(y⋆) + x)

and B := γHΩ(y⋆).

(I + A(B − I)) JΠΩ = A 

The proof is given in Appendix A.1. Unlike recent work tackling argmin differentiation through matrix
differential calculus on the Karush–Kuhn–Tucker (KKT) conditions [1]  our proof technique relies on
differentiating the ﬁxed point iteration y∗ = P∆d (y⋆ − ∇f (y⋆)). To compute JΠΩ v for an arbitrary
v ∈ Rd  as required by backpropagation  we may directly solve (I + A(B − I)) (JΠΩ v) = Av. We
show in Appendix B how this system can be solved efﬁciently thanks to the structure of A.

Squared p-norms. As a useful example of a differentiable function over the simplex  we consider
squared p-norms: Ω(y) = 1
  where y ∈ ∆d and p ∈ (1  2]. For this choice
of p  it is known that the squared p-norm is strongly convex w.r.t. k · kp [3]. This implies that maxΩ is
γ(p−1) smooth w.r.t. k.kq  where 1
q = 1. We call the induced mapping function sq-pnorm-max:

p =(cid:16)Pd

i(cid:17)2/p

i=1 yp

2 kyk2

p + 1

1

ΠΩ(x) = arg min

y∈∆d

γ
2

kyk2

p − yTx.

(sq-pnorm-max)

The gradient and Hessian needed for Proposition 1 can be computed by ∇Ω(y) = y

p−1
kykp−2

p

and

HΩ(y) = diag(d) + uuT  where d =

(p − 1)
kykp−2

p

yp−2

and u =s (2 − p)

kyk2p−2

p

yp−1 

with the exponentiation performed element-wise. sq-pnorm-max recovers sparsemax with p = 2
and  like sparsemax  encourages sparse outputs. However  as can be seen in the zoomed box in
Figure 2 (right)  the transition between y⋆ = [0  1] and y⋆ = [1  0] can be smoother when 1 < p < 2.
Throughout our experiments  we use p = 1.5.

3.2 Structured regularizers: fused lasso and OSCAR

Fusedmax. For cases when the input is sequential and the order is meaningful  as is the case
for many natural languages  we propose fusedmax  an attention mechanism based on fused lasso
[42]  also known as 1-d total variation (TV). Fusedmax encourages paying attention to contiguous
segments  with equal weights within each one. It is expressed under our framework by choosing
Ω(y) = 1
i=1 |yi+1 − yi|  i.e.  the sum of a strongly convex term and of a 1-d TV penalty.
It is easy to verify that this choice yields the mapping

2 kyk2

2 + λPd−1

ΠΩ(x) = arg min

y∈∆d

1
2

ky − x/γk2 + λ

d−1

Xi=1

|yi+1 − yi|.

(fusedmax)

Oscarmax. For situations where the contiguity assumption may be too strict  we propose oscarmax 
based on the OSCAR penalty [7]  to encourage attention weights to merge into clusters with the
same value  regardless of position in the sequence. This is accomplished by replacing the 1-d
TV penalty in fusedmax with an ∞-norm penalty on each pair of attention weights  i.e.  Ω(y) =
1
2 kyk2

2 + λPi<j max(|yi|  |yj|). This results in the mapping

ΠΩ(x) = arg min

max(|yi|  |yj|).

(oscarmax)

y∈∆d

1
2

ky − x/γk2 + λXi<j

Forward computation. Due to the y ∈ ∆d constraint  computing fusedmax/oscarmax does not
seem trivial on ﬁrst sight. The next proposition shows how to do so  without any iterative method.

Proposition 2 Computing fusedmax and oscarmax (forward computation)

fusedmax: ΠΩ(x) = P∆d (PTV (x/γ))  

PTV(x) := arg min

y∈Rd

oscarmax: ΠΩ(x) = P∆d (POSC (x/γ))   POSC(x) := arg min

y∈Rd

1
2
1
2

d−1

ky − xk2 + λ

Xi=1
ky − xk2 + λXi<j

|yi+1 − yi|.

max(|yi|  |yj|).

5

Here  PTV and POSC indicate the proximal operators of 1-d TV and OSCAR  and can be computed
exactly by [13] and [47]  respectively. To remind the reader  P∆d denotes the Euclidean projection
onto the simplex and can be computed exactly using [34  15]. Proposition 2 shows that we can
compute fusedmax and oscarmax using the composition of two functions  for which exact non-
iterative algorithms exist. This is a surprising result  since the proximal operator of the sum of two
functions is not  in general  the composition of the proximal operators of each function. The proof
follows by showing that the indicator function of ∆d satisﬁes the conditions of [45  Corollaries 4 5].
Groups induced by PTV and POSC. Let z⋆ be the optimal solution of PTV(x) or POSC(x). For PTV 
we denote the group of adjacent elements with the same value as z⋆
i   ∀i ∈ [d]. Formally 
G⋆
i = [a  b] ∩ N with a ≤ i ≤ b where a and b are the minimal and maximal indices such that
z⋆
i = z⋆
i as the indices of elements with the same absolute
j |}. Because P∆d (z⋆) = max(z⋆ − θ  0) for
value as z⋆
i | = |z⋆
some θ ∈ R  fusedmax/oscarmax either shift a group’s common value or set all its elements to zero.

i . For POSC  we deﬁne G⋆
i = {j ∈ [d] : |z⋆

i   more formally G⋆

j for all j ∈ G⋆

i by G⋆

λ controls the trade-off between no fusion (sparsemax) and all elements fused into a single trivial
group. While tuning λ may improve performance  we observe that λ = 0.1 (fusedmax) and λ = 0.01
(oscarmax) are sensible defaults that work well across all tasks and report all our results using them.

Backward computation. We already know that the Jacobian of P∆d is the same as that of sparsemax
with γ = 1. Then  by Proposition 2  if we know how to compute the Jacobians of PTV and POSC  we
can obtain the Jacobians of fusedmax and oscarmax by straightforward application of the chain rule.
However  although PTV and POSC can be computed exactly  they lack analytical expressions. We next
show that we can nonetheless compute their Jacobians efﬁciently  without needing to solve a system.

Proposition 3 Jacobians of PTV and POSC (backward computation)

Assume z⋆ = PTV(x) or POSC(x) has been computed. Deﬁne the groups derived from z⋆ as above.

Then  [JPTV(x)]i j =( 1

|G⋆
i |
0

if j ∈ G⋆
i  
o.w.

and [JPOSC (x)]i j =( sign(z⋆

i z⋆
j )
|G⋆
i |

0

i and z⋆

i 6= 0 

if j ∈ G⋆
o.w.

.

The proof is given in Appendix A.2. Clearly  the structure of these Jacobians permits efﬁcient
Jacobian-vector products; we discuss the computational complexity and implementation details in
Appendix B. Note that PTV and POSC are differentiable everywhere except at points where groups
change. For these points  the same remark as for sparsemax applies  and we can use Clarke’s Jacobian.

4 Experimental results

We showcase the performance of our attention mechanisms on three challenging natural language
tasks: textual entailment  machine translation  and sentence summarization. We rely on available 
well-established neural architectures  so as to demonstrate simple drop-in replacement of softmax with
structured sparse attention; quite likely  newer task-speciﬁc models could lead to further improvement.

4.1 Textual entailment (a.k.a. natural language inference) experiments

Textual entailment is the task of deciding  given a text T and an hypothesis H  whether a human
reading T is likely to infer that H is true [14]. We use the Stanford Natural Language Inference (SNLI)
dataset [8]  a collection of 570 000 English sentence pairs. Each pair consists of a sentence and an
hypothesis  manually labeled with one of the labels ENTAILMENT  CONTRADICTION  or NEUTRAL.

We use a variant of the neural attention–based classiﬁer proposed for
this dataset by [38] and follow the same methodology as [31] in terms
of implementation  hyperparameters  and grid search. We employ the
CPU implementation provided in [31] and simply replace sparsemax
with fusedmax/oscarmax; we observe that training time per epoch
is essentially the same for each of the four attention mechanisms
(timings and more experimental details in Appendix C.2).

Table 1 shows that  for this task  fusedmax reaches the highest ac-
curacy  and oscarmax slightly outperforms softmax. Furthermore 

Table 1: Textual entailment
test accuracy on SNLI [8].

attention

accuracy

softmax
sparsemax

fusedmax
oscarmax

81.66
82.39

82.41
81.76

6

Figure 3: Attention weights when considering the contradicted hypothesis “No one is dancing.”

fusedmax results in the most interpretable feature groupings: Figure 3 shows the weights of the
neural network’s attention to the text  when considering the hypothesis “No one is dancing.” In this
case  all four models correctly predicted that the text “A band is playing on stage at a concert and the
attendants are dancing to the music ” denoted along the x-axis  contradicts the hypothesis  although
the attention weights differ. Notably  fusedmax identiﬁes the meaningful segment “band is playing”.

4.2 Machine translation experiments

Sequence-to-sequence neural machine translation (NMT) has recently become a strong contender in
machine translation [2  29]. In NMT  attention weights can be seen as an alignment between source
and translated words. To demonstrate the potential of our new attention mechanisms for NMT  we ran
experiments on 10 language pairs. We build on OpenNMT-py [24]  based on PyTorch [37]  with all
default hyperparameters (detailed in Appendix C.3)  simply replacing softmax with the proposed ΠΩ.

OpenNMT-py with softmax attention is optimized for the GPU. Since sparsemax  fusedmax  and
oscarmax rely on sorting operations  we implement their computations on the CPU for simplicity 
keeping the rest of the pipeline on the GPU. However  we observe that  even with this context
switching  the number of tokens processed per second was within 3/4 of the softmax pipeline. For
sq-pnorm-max  we observe that the projected gradient solver used in the forward pass  unlike the
linear system solver used in the backward pass  could become a computational bottleneck. To mitigate
this effect  we set the tolerance of the solver’s stopping criterion to 10−2.

Quantitatively  we ﬁnd that all compared attention mechanisms are always within 1 BLEU score
point of the best mechanism (for detailed results  cf. Appendix C.3). This suggests that structured
sparsity does not restrict accuracy. However  as illustrated in Figure 4  fusedmax and oscarmax often
lead to more interpretable attention alignments  as well as to qualitatively different translations.

Figure 4: Attention weights for French to English translation  using the conventions of Figure 1.
Within a row  weights grouped by oscarmax under the same cluster are denoted by “•”. Here  oscarmax
ﬁnds a slightly more natural English translation. More visulizations are given in Appendix C.3.

4.3 Sentence summarization experiments

Attention mechanisms were recently explored for sentence summarization in [39]. To generate
sentence-summary pairs at low cost  the authors proposed to use the title of a news article as a
noisy summary of the article’s leading sentence. They collected 4 million such pairs from the
Gigaword dataset and showed that this seemingly simplistic approach leads to models that generalize

7

Abandisplayingonstageataconcertandtheattendantsaredancingtothemusic.0.00.10.2softmaxAbandisplayingonstageataconcertandtheattendantsaredancingtothemusic.0.00.10.2sparsemaxAbandisplayingonstageataconcertandtheattendantsaredancingtothemusic.0.00.10.20.3fusedmaxAbandisplayingonstageataconcertandtheattendantsaredancingtothemusic.0.00.10.2oscarmaxLacoalitionpourl'aideinternationaledevraitlelireavecattention.thecoalitionforinternationalaidshouldreaditcarefully.<EOS>fusedmaxLacoalitionpourl'aideinternationaledevraitlelireavecattention.theinternationalaidcoalitionshouldreaditcarefully.<EOS>oscarmaxLacoalitionpourl'aideinternationaledevraitlelireavecattention.thecoalitionforinternationalaidshouldreaditcarefully.<EOS>softmaxTable 2: Sentence summarization results  following the same experimental setting as in [39].

DUC 2004

Gigaword

attention ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L

softmax
sparsemax

fusedmax
oscarmax
sq-pnorm-max

27.16
27.69

28.42
27.84
27.94

9.48
9.55

9.96
9.46
9.28

24.47
24.96

25.55
25.14
25.08

35.13
36.04

36.09
35.36
35.94

17.15
17.78

17.62
17.23
17.75

32.92
33.64

33.69
33.03
33.66

surprisingly well. We follow their experimental setup and are able to reproduce comparable results to
theirs with OpenNMT when using softmax attention. The models we use are the same as in §4.2.

Our evaluation follows [39]: we use the standard DUC 2004 dataset (500 news articles each paired
with 4 different human-generated summaries) and a randomly held-out subset of Gigaword  released
by [39]. We report results on ROUGE-1  ROUGE-2  and ROUGE-L. Our results  in Table 2  indicate that
fusedmax is the best under nearly all metrics  always outperforming softmax. In addition to Figure 1 
we exemplify our enhanced interpretability and provide more detailed results in Appendix C.4.

5 Related work

Smoothed max operators. Replacing the max operator by a differentiable approximation based
on the log sum exp has been exploited in numerous works. Regularizing the max operator with a
squared 2-norm is less frequent  but has been used to obtain a smoothed multiclass hinge loss [41] or
smoothed linear programming relaxations for maximum a-posteriori inference [33]. Our work differs
from these in mainly two aspects. First  we are less interested in the max operator itself than in its
gradient  which we use as a mapping from Rd to ∆d. Second  since we use this mapping in neural
networks trained with backpropagation  we study and compute the mapping’s Jacobian (the Hessian
of a regularized max operator)  in contrast with previous works.

Interpretability  structure and sparsity in neural networks. Providing interpretations alongside
predictions is important for accountability  error analysis and exploratory analysis  among other
reasons. Toward this goal  several recent works have been relying on visualizing hidden layer
activations [20  27] and the potential for interpretability provided by attention mechanisms has been
noted in multiple works [2  38  39]. Our work aims to fulﬁll this potential by providing a uniﬁed
framework upon which new interpretable attention mechanisms can be designed  using well-studied
tools from the ﬁeld of structured sparse regularization.

Selecting contiguous text segments for model interpretations is explored in [26]  where an explanation
generator network is proposed for justifying predictions using a fused lasso penalty. However  this
network is not an attention mechanism and has its own parameters to be learned. Furthemore 
[26] sidesteps the need to backpropagate through the fused lasso  unlike our work  by using a
stochastic training approach. In constrast  our attention mechanisms are deterministic and drop-in
replacements for existing ones. As a consequence  our mechanisms can be coupled with recent
research that builds on top of softmax attention  for example in order to incorporate soft prior
knowledge about NMT alignment into attention through penalties on the attention weights [12].

A different approach to incorporating structure into attention uses the posterior marginal probabilities
from a conditional random ﬁeld as attention weights [23]. While this approach takes into account
structural correlations  the marginal probabilities are generally dense and different from each other.
Our proposed mechanisms produce sparse and clustered attention weights  a visible beneﬁt in
interpretability. The idea of deriving constrained alternatives to softmax has been independently
explored for differentiable easy-ﬁrst decoding [32]. Finally  sparsity-inducing penalties have been
used to obtain convex relaxations of neural networks [5] or to compress models [28  43  40]. These
works differ from ours  in that sparsity is enforced on the network parameters  while our approach
can produce sparse and structured outputs from neural attention layers.

8

6 Conclusion and future directions

We proposed in this paper a uniﬁed regularized framework upon which new attention mechanisms can
be designed. To enable such mechanisms to be used in a neural network trained by backpropagation 
we demonstrated how to carry out forward and backward computations for general differentiable
regularizers. We further developed two new structured attention mechanisms  fusedmax and oscarmax 
and demonstrated that they enhance interpretability while achieving comparable or better accuracy
on three diverse and challenging tasks: textual entailment  machine translation  and summarization.

The usefulness of a differentiable mapping from real values to the simplex or to [0  1] with sparse or
structured outputs goes beyond attention mechanisms. We expect that our framework will be useful
to sample from categorical distributions using the Gumbel trick [21  30]  as well as for conditional
computation [6] or differentiable neural computers [18  19]. We plan to explore these in future work.

Acknowledgements

We are grateful to André Martins  Takuma Otsuka  Fabian Pedregosa  Antoine Rolet  Jun Suzuki  and
Justine Zhang for helpful discussions. We thank the anonymous reviewers for the valuable feedback.

References

[1] B. Amos and J. Z. Kolter. OptNet: Differentiable optimization as a layer in neural networks. In

Proc. of ICML  2017.

[2] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align

and translate. In Proc. of ICLR  2015.

[3] K. Ball  E. A. Carlen  and E. H. Lieb. Sharp uniform convexity and smoothness inequalities for

trace norms. Inventiones Mathematicae  115(1):463–482  1994.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[5] Y. Bengio  N. Le Roux  P. Vincent  O. Delalleau  and P. Marcotte. Convex neural networks. In

Proc. of NIPS  2005.

[6] Y. Bengio  N. Léonard  and A. Courville. Estimating or propagating gradients through stochastic

neurons for conditional computation. In Proc. of NIPS  2013.

[7] H. D. Bondell and B. J. Reich. Simultaneous regression shrinkage  variable selection  and

supervised clustering of predictors with OSCAR. Biometrics  64(1):115–123  2008.

[8] S. R. Bowman  G. Angeli  C. Potts  and C. D. Manning. A large annotated corpus for learning

natural language inference. In Proc. of EMNLP  2015.

[9] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press  2004.

[10] J. K. Chorowski  D. Bahdanau  D. Serdyuk  K. Cho  and Y. Bengio. Attention-based models for

speech recognition. In Proc. of NIPS  2015.

[11] F. H. Clarke. Optimization and nonsmooth analysis. SIAM  1990.

[12] T. Cohn  C. D. V. Hoang  E. Vymolova  K. Yao  C. Dyer  and G. Haffari. Incorporating structural
alignment biases into an attentional neural translation model. In Proc. of NAACL-HLT  2016.

[13] L. Condat. A direct algorithm for 1-d total variation denoising. IEEE Signal Processing Letters 

20(11):1054–1057  2013.

[14] I. Dagan  B. Dolan  B. Magnini  and D. Roth. Recognizing textual entailment: Rational 

evaluation and approaches. Natural Language Engineering  15(4):i–xvii  2009.

[15] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the ℓ1-ball

for learning in high dimensions. In Proc. of ICML  2008.

9

[16] C. Dugas  Y. Bengio  F. Bélisle  C. Nadeau  and R. Garcia. Incorporating second-order functional

knowledge for better option pricing. Proc. of NIPS  2001.

[17] J. Friedman  T. Hastie  H. Höﬂing  and R. Tibshirani. Pathwise coordinate optimization. The

Annals of Applied Statistics  1(2):302–332  2007.

[18] A. Graves  G. Wayne  and I. Danihelka. Neural Turing Machines. In Proc. of NIPS  2014.

[19] A. Graves  G. Wayne  M. Reynolds  T. Harley  I. Danihelka  A. Grabska-Barwi´nska  S. G.
Colmenarejo  E. Grefenstette  T. Ramalho  J. Agapiou  et al. Hybrid computing using a neural
network with dynamic external memory. Nature  538(7626):471–476  2016.

[20] O. Irsoy. Deep sequential and structural neural models of compositionality. PhD thesis  Cornell

University  2017.

[21] E. Jang  S. Gu  and B. Poole. Categorical reparameterization with Gumbel-Softmax. In Proc. of

ICLR  2017.

[22] S. M. Kakade  S. Shalev-Shwartz  and A. Tewari. Regularization techniques for learning with

matrices. Journal of Machine Learning Research  13:1865–1890  2012.

[23] Y. Kim  C. Denton  L. Hoang  and A. M. Rush. Structured attention networks. In Proc. of ICLR 

2017.

[24] G. Klein  Y. Kim  Y. Deng  J. Senellart  and A. M. Rush. OpenNMT: Open-source toolkit for

neural machine translation. arXiv e-prints  2017.

[25] P. Koehn  H. Hoang  A. Birch  C. Callison-Burch  M. Federico  N. Bertoldi  B. Cowan  W. Shen 
C. Moran  R. Zens  C. Dyer  O. Bojar  A. Constantin  and E. Herbst. Moses: Open source
toolkit for statistical machine translation. In Proc. of ACL  2007.

[26] T. Lei  R. Barzilay  and T. Jaakkola. Rationalizing neural predictions. In Proc. of EMNLP 

2016.

[27] J. Li  X. Chen  E. Hovy  and D. Jurafsky. Visualizing and understanding neural models in NLP.

In Proc. of NAACL-HLT  2016.

[28] B. Liu  M. Wang  H. Foroosh  M. Tappen  and M. Pensky. Sparse convolutional neural networks.

In Proc. of ICCVPR  2015.

[29] M.-T. Luong  H. Pham  and C. D. Manning. Effective approaches to attention-based neural

machine translation. In Proc. of EMNLP  2015.

[30] C. J. Maddison  A. Mnih  and Y. W. Teh. The concrete distribution: A continuous relaxation of

discrete random variables. In Proc. of ICLR  2017.

[31] A. F. Martins and R. F. Astudillo. From softmax to sparsemax: A sparse model of attention and

multi-label classiﬁcation. In Proc. of ICML  2016.

[32] A. F. Martins and J. Kreutzer. Learning what’s easy: Fully differentiable neural easy-ﬁrst

taggers. In Proc. of EMNLP  2017.

[33] O. Meshi  M. Mahdavi  and A. G. Schwing. Smooth and strong: MAP inference with linear

convergence. In Proc. of NIPS  2015.

[34] C. Michelot. A ﬁnite algorithm for ﬁnding the projection of a point onto the canonical simplex

of Rn. Journal of Optimization Theory and Applications  50(1):195–200  1986.

[35] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103(1):127–152  2005.

[36] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends R(cid:13) in Optimization 

1(3):127–239  2014.

[37] PyTorch. http://pytorch.org  2017.

10

[38] T. Rocktäschel  E. Grefenstette  K. M. Hermann  T. Kocisky  and P. Blunsom. Reasoning about

entailment with neural attention. In Proc. of ICLR  2016.

[39] A. M. Rush  S. Chopra  and J. Weston. A neural attention model for abstractive sentence

summarization. In Proc. of EMNLP  2015.

[40] S. Scardapane  D. Comminiello  A. Hussain  and A. Uncini. Group sparse regularization for

deep neural networks. Neurocomputing  241:81–89  2017.

[41] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for

regularized loss minimization. Mathematical Programming  155(1):105–145  2016.

[42] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via
the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
67(1):91–108  2005.

[43] W. Wen  C. Wu  Y. Wang  Y. Chen  and H. Li. Learning structured sparsity in deep neural

networks. In Proc. of NIPS  2016.

[44] K. Xu  J. Ba  R. Kiros  K. Cho  A. Courville  R. Salakhudinov  R. Zemel  and Y. Bengio. Show 
attend and tell: Neural image caption generation with visual attention. In Proc. of ICML  2015.

[45] Y. Yu. On decomposing the proximal map. In Proc. of NIPS  2013.

[46] C. Zalinescu. Convex analysis in general vector spaces. World Scientiﬁc  2002.

[47] X. Zeng and M. A. Figueiredo. Solving OSCAR regularization problems by fast approximate

proximal splitting algorithms. Digital Signal Processing  31:124–135  2014.

[48] X. Zeng and F. A. Mario. The ordered weighted ℓ1 norm: Atomic formulation  dual norm  and

projections. arXiv e-prints  2014.

[49] L. W. Zhong and J. T. Kwok. Efﬁcient sparse modeling with automatic feature grouping. IEEE

transactions on neural networks and learning systems  23(9):1436–1447  2012.

11

,Mingjun Zhong
Nigel Goddard
Charles Sutton
CHRISTOS THRAMPOULIDIS
Ehsan Abbasi
Babak Hassibi
Vlad Niculae
Mathieu Blondel