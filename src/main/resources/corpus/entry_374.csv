2013,Actor-Critic Algorithms for Risk-Sensitive MDPs,In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance related risk measures are among the most common risk-sensitive criteria in finance and operations research. However  optimizing many such criteria is known to be a hard problem. In this paper  we consider both discounted and average reward Markov decision processes. For each formulation  we first define a measure of variability for a policy  which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria  we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally  we demonstrate the usefulness of our algorithms in a traffic signal control application.,Actor-Critic Algorithms for Risk-Sensitive MDPs

Prashanth L.A.

INRIA Lille - Team SequeL

Mohammad Ghavamzadeh∗

INRIA Lille - Team SequeL & Adobe Research

Abstract

In many sequential decision-making problems we may want to manage risk by
minimizing some measure of variability in rewards in addition to maximizing a
standard criterion. Variance-related risk measures are among the most common
risk-sensitive criteria in ﬁnance and operations research. However  optimizing
many such criteria is known to be a hard problem. In this paper  we consider both
discounted and average reward Markov decision processes. For each formulation 
we ﬁrst deﬁne a measure of variability for a policy  which in turn gives us a set of
risk-sensitive criteria to optimize. For each of these criteria  we derive a formula
for computing its gradient. We then devise actor-critic algorithms for estimating
the gradient and updating the policy parameters in the ascent direction. We estab-
lish the convergence of our algorithms to locally risk-sensitive optimal policies.
Finally  we demonstrate the usefulness of our algorithms in a trafﬁc signal control
application.

1

Introduction

The usual optimization criteria for an inﬁnite horizon Markov decision process (MDP) are the ex-
pected sum of discounted rewards and the average reward. Many algorithms have been developed to
maximize these criteria both when the model of the system is known (planning) and unknown (learn-
ing). These algorithms can be categorized to value function based methods that are mainly based on
the two celebrated dynamic programming algorithms value iteration and policy iteration; and policy
gradient methods that are based on updating the policy parameters in the direction of the gradient
of a performance measure (the value function of the initial state or the average reward). However in
many applications  we may prefer to minimize some measure of risk as well as maximizing a usual
optimization criterion. In such cases  we would like to use a criterion that incorporates a penalty
for the variability induced by a given policy. This variability can be due to two types of uncertain-
ties: 1) uncertainties in the model parameters  which is the topic of robust MDPs (e.g.  [12  7  24]) 
and 2) the inherent uncertainty related to the stochastic nature of the system  which is the topic of
risk-sensitive MDPs (e.g.  [10]).
In risk-sensitive sequential decision-making  the objective is to maximize a risk-sensitive criterion
such as the expected exponential utility [10]  a variance-related measure [19  8]  or the percentile
performance [9]. The issue of how to construct such criteria in a manner that will be both con-
ceptually meaningful and mathematically tractable is still an open question. Although risk-sensitive
sequential decision-making has a long history in operations research and ﬁnance  it has only recently
grabbed attention in the machine learning community. This is why most of the work on this topic
(including those mentioned above) has been in the context of MDPs (when the model is known) and
much less work has been done within the reinforcement learning (RL) framework. In risk-sensitive
RL  we can mention the work by Borkar [4  5] who considered the expected exponential utility and
the one by Tamar et al. [22] on several variance-related measures. Tamar et al. [22] study stochas-
tic shortest path problems  and in this context  propose a policy gradient algorithm for maximizing
several risk-sensitive criteria that involve both the expectation and variance of the return random
variable (deﬁned as the sum of rewards received in an episode).

∗Mohammad Ghavamzadeh is at Adobe Research  on leave of absence from INRIA Lille - Team SequeL.

1

In this paper  we develop actor-critic algorithms for optimizing variance-related risk measures in
both discounted and average reward MDPs. Our contributions can be summarized as follows:
• In the discounted reward setting we deﬁne the measure of variability as the variance of the return
(similar to [22]). We formulate a constrained optimization problem with the aim of maximizing the
mean of the return subject to its variance being bounded from above. We employ the Lagrangian
relaxation procedure [1] and derive a formula for the gradient of the Lagrangian. Since this re-
quires the gradient of the value function at every state of the MDP (see the discussion in Sections 3
and 4)  we estimate the gradient of the Lagrangian using two simultaneous perturbation methods: si-
multaneous perturbation stochastic approximation (SPSA) [20] and smoothed functional (SF) [11] 
resulting in two separate discounted reward actor-critic algorithms.1
• In the average reward formulation  we ﬁrst deﬁne the measure of variability as the long-run vari-
ance of a policy  and using a constrained optimization problem similar to the discounted case  derive
an expression for the gradient of the Lagrangian. We then develop an actor-critic algorithm with
compatible features [21  13] to estimate the gradient and to optimize the policy parameters.
• Using the ordinary differential equations (ODE) approach  we establish the asymptotic conver-
gence of our algorithms to locally risk-sensitive optimal policies. Further  we demonstrate the use-
fulness of our algorithms in a trafﬁc signal control problem.
In comparison to [22]  which is the closest related work  we would like to remark that while the au-
thors there develop policy gradient methods for stochastic shortest path problems  we devise actor-
critic algorithms for both discounted and average reward settings. Moreover  we note the difﬁculty
in the discounted formulation that requires to estimate the gradient of the value function at every
state of the MDP  and thus  motivated us to employ simultaneous perturbation techniques.
2 Preliminaries
We consider problems in which the agent’s interaction with the environment is modeled as a
MDP. A MDP is a tuple (X  A  R  P  P0) where X = {1  . . .   n} and A = {1  . . .   m} are the
state and action spaces; R(x  a) is the reward random variable whose expectation is denoted by

r(x  a) = E(cid:2)R(x  a)(cid:3); P (·|x  a) is the transition probability distribution; and P0(·) is the initial
stochastic policies(cid:8)µ(·|x; θ)  x ∈ X   θ ∈ Θ ⊆ Rκ1(cid:9)  estimate the gradient of a performance mea-

state distribution. We also need to specify the rule according to which the agent selects actions
at each state. A stationary policy µ(·|x) is a probability distribution over actions  conditioned on
the current state. In policy gradient and actor-critic methods  we deﬁne a class of parameterized

sure w.r.t. the policy parameters θ from the observed system trajectories  and then improve the policy
by adjusting its parameters in the direction of the gradient. Since in this setting a policy µ is rep-
resented by its κ1-dimensional parameter vector θ  policy dependent functions can be written as a
function of θ in place of µ. So  we use µ and θ interchangeably in the paper.
We denote by dµ(x) and πµ(x  a) = dµ(x)µ(a|x) the stationary distribution of state x and state-
action pair (x  a) under policy µ  respectively. In the discounted formulation  we also deﬁne the
discounted visiting distribution of state x and state-action pair (x  a) under policy µ as dµ
γ (x|x0) =

t=0 γt Pr(xt = x|x0 = x0; µ) and πµ

γ (x  a|x0) = dµ

γ (x|x0)µ(a|x).

(1 − γ)(cid:80)∞

3 Discounted Reward Setting
For a given policy µ  we deﬁne the return of a state x (state-action pair (x  a)) as the sum of dis-
counted rewards encountered by the agent when it starts at state x (state-action pair (x  a)) and then
follows policy µ  i.e. 

Dµ(x) =

γtR(xt  at) | x0 = x  µ 

Dµ(x  a) =

γtR(xt  at) | x0 = x  a0 = a  µ.

∞(cid:88)

∞(cid:88)

t=0

µ  i.e.  V µ(x) = E(cid:2)Dµ(x)(cid:3) and Qµ(x  a) = E(cid:2)Dµ(x  a)(cid:3). The goal in the standard discounted

The expected value of these two random variables are the value and action-value functions of policy
reward formulation is to ﬁnd an optimal policy µ∗ = arg maxµ V µ(x0)  where x0 is the initial state
of the system. This can be easily extended to the case that the system has more than one initial state
µ∗ = arg maxµ

P0(x)V µ(x).

(cid:80)

t=0

x∈X

1We note here that our algorithms can be easily extended to other variance-related risk criteria such as the

Sharpe ratio  which is popular in ﬁnancial decision-making [18] (see Appendix D of [17]).

2

The most common measure of the variability in the stream of rewards is the variance of the return

Λµ(x) = E(cid:2)Dµ(x)2(cid:3) − V µ(x)2 = U µ(x) − V µ(x)2 

= E(cid:2)Dµ(x)2(cid:3) is the square reward value function

(1)

(cid:52)

ﬁrst introduced by Sobel [19]. Note that U µ(x)
of state x under policy µ. Although Λµ of (1) satisﬁes a Bellman equation  unfortunately  it lacks
the monotonicity property of dynamic programming (DP)  and thus  it is not clear how the related
risk measures can be optimized by standard DP algorithms [19]. This is why policy gradient and
actor-critic algorithms are good candidates to deal with this risk measure. We consider the following
risk-sensitive measure for discounted MDPs: for a given α > 0 

V θ(x0)

max

θ

subject to

Λθ(x0) ≤ α.

(2)

To solve (2)  we employ the Lagrangian relaxation procedure [1] to convert it to the following
unconstrained problem:

(cid:16)

= −V θ(x0) + λ(cid:0)Λθ(x0) − α(cid:1)(cid:17)

(cid:52)

max

λ

min

θ

L(θ  λ)

 

(3)

(cid:88)
(cid:88)

x a

x a

where λ is the Lagrange multiplier. The goal here is to ﬁnd the saddle point of L(θ  λ)  i.e.  a
point (θ∗  λ∗) that satisﬁes L(θ  λ∗) ≥ L(θ∗  λ∗) ≥ L(θ∗  λ) ∀θ ∀λ > 0. This is achieved by de-
scending in θ and ascending in λ using the gradients ∇θL(θ  λ) = −∇θV θ(x0) + λ∇θΛθ(x0) and
∇λL(θ  λ) = Λθ(x0) − α  respectively. Since ∇Λθ(x0) = ∇U θ(x0) − 2V θ(x0)∇V θ(x0)  in order
to compute ∇Λθ(x0)  we need to calculate ∇U θ(x0) and ∇V θ(x0). From the Bellman equation of
Λµ(x)  proposed by Sobel [19]  it is straightforward to derive Bellman equations for U µ(x) and the
square reward action-value function W µ(x  a)
these deﬁnitions and notations we are now ready to derive expressions for the gradient of V θ(x0)
and U θ(x0) that are the main ingredients in calculating ∇θL(θ  λ).
Lemma 1 Assuming for all (x  a)  µ(a|x; θ) is continuously differentiable in θ  we have

= E(cid:2)Dµ(x  a)2(cid:3) (see Appendix B.1 of [17]). Using

(cid:52)

(1 − γ)∇V θ(x0) =

γ(x  a|x0)∇ log µ(a|x; θ)Qθ(x  a) 
πθ

(cid:48)

) 

+ 2γ

x a x(cid:48)

γ(x  a|x0)P (x

(cid:48)|x  a)r(x  a)∇V θ(x

(1 − γ2)∇U θ(x0) =

γ(x  a|x0)∇ log µ(a|x; θ)W θ(x  a)

γ(x  a|x0) = (cid:101)dθ

(cid:101)πθ
(cid:88)
(cid:101)πθ
γ(x|x0) = (1− γ2)(cid:80)∞
γ(x|x0)µ(a|x) and (cid:101)dθ
γ and(cid:101)πθ

where(cid:101)πθ
t=0 γ2t Pr(xt = x|x0 = x0; θ).
The proof of the above lemma is available in Appendix B.2 of [17]. It is challenging to devise an
efﬁcient method to estimate ∇θL(θ  λ) using the gradient formulas of Lemma 1. This is mainly
because 1) two different sampling distributions (πθ
γ) are used for ∇V θ(x0) and ∇U θ(x0) 
and 2) ∇V θ(x(cid:48)) appears in the second sum of ∇U θ(x0) equation  which implies that we need
to estimate the gradient of the value function V θ at every state of the MDP. These are the main
motivations behind using simultaneous perturbation methods for estimating ∇θL(θ  λ) in Section 4.
4 Discounted Reward Algorithms
In this section  we present actor-critic algorithms for optimizing the risk-sensitive measure (2) that
are based on two simultaneous perturbation methods: simultaneous perturbation stochastic approx-
imation (SPSA) and smoothed functional (SF) [3]. The idea in these methods is to estimate the
gradients ∇V θ(x0) and ∇U θ(x0) using two simulated trajectories of the system corresponding to
policies with parameters θ and θ+ = θ + β∆. Here β > 0 is a positive constant and ∆ is a perturba-
value and square value functions  i.e.  (cid:98)V (x) ≈ v(cid:62)φv(x) and (cid:98)U (x) ≈ u(cid:62)φu(x)  where the features
tion random variable  i.e.  a κ1-vector of independent Rademacher (for SPSA) and Gaussian N (0  1)
(for SF) random variables. In our actor-critic algorithms  the critic uses linear approximation for the
φv(·) and φu(·) are from low-dimensional spaces Rκ2 and Rκ3  respectively.
SPSA-based gradient estimates were ﬁrst proposed in [20] and have been widely studied and found
to be highly efﬁcient in various settings  especially those involving high-dimensional parameters.
The SPSA-based estimate for ∇V θ(x0)  and similarly for ∇U θ(x0)  is given by:

3

Figure 1: The overall ﬂow of our simultaneous perturbation based actor-critic algorithms.

∂θ(i)(cid:98)V θ(x0) ≈ (cid:98)V θ+β∆(x0) −(cid:98)V θ(x0)

β∆(i)

 

i = 1  . . .   κ1 

(4)

where ∆ is a vector of independent Rademacher random variables. The advantage of this estimator
is that it perturbs all directions at the same time (the numerator is identical in all κ1 components).
So  the number of function measurements needed for this estimator is always two  independent of
the dimension κ1. However  unlike the SPSA estimates in [20] that use two-sided balanced estimates
(simulations with parameters θ−β∆ and θ +β∆)  our gradient estimates are one-sided (simulations
with parameters θ and θ+β∆) and resemble those in [6]. The use of one-sided estimates is primarily
because the updates of the Lagrangian parameter λ require a simulation with the running parameter
θ. Using a balanced gradient estimate would therefore come at the cost of an additional simulation
(the resulting procedure would then require three simulations)  which we avoid by using one-sided
gradient estimates.
SF-based method estimates not the gradient of a function H(θ) itself  but rather the convolution of
∇H(θ) with the Gaussian density function N (0  β2I)  i.e. 

CβH(θ) =

Gβ(θ − z)∇zH(z)dz =

∇zGβ(z)H(θ − z)dz =

1
β

−z

(cid:48)G1(z

(cid:48)

)H(θ − βz

(cid:48)

(cid:48)

 

)dz

(cid:90)

(cid:90)

(cid:90)

where Gβ is a κ1-dimensional probability density function. The ﬁrst equality above follows by
−z
using integration by parts and the second one by using the fact that ∇zGβ(z) =
β2 Gβ(z) and by
substituting z(cid:48) = z/β. As β → 0  it can be seen that CβH(θ) converges to ∇θH(θ) (see Chapter 6
(cid:17)
of [3]). Thus  a one-sided SF estimate of ∇V θ(x0) is given by

(cid:16)(cid:98)V θ+β∆(x0) −(cid:98)V θ(x0)

∂θ(i)(cid:98)V θ(x0) ≈ ∆(i)

i = 1  . . .   κ1 

(5)

 

β

where ∆ is a vector of independent Gaussian N (0  1) random variables.
The overall ﬂow of our proposed actor-critic algorithms is illustrated in Figure 1 and involves the
following main steps at each time step t:
(1) Take action at ∼ µ(·|xt; θt)  observe the reward r(xt  at) and next state xt+1 in the ﬁrst trajectory.
(2) Take action a+
t+1 in the second
trajectory.
(3) Critic Update: Calculate the temporal difference (TD)-errors δt  δ+
the square value functions using (7)  and update the critic parameters vt  v+
for the square value functions as follows:

t for
t for the value and ut  u+

t )  observe the reward r(x+

t for the value and t  +

t ) and next state x+

t ∼ µ(·|x+

t   a+

t ; θ+

t

vt+1 = vt + ζ3(t)δtφv(xt) 

ut+1 = ut + ζ3(t)tφu(xt) 

v+
t+1 = v+
u+
t+1 = u+

t + ζ3(t)δ+
t + ζ3(t)+

t φv(x+
t φu(x+

t ) 
t ) 

(6)

where the TD-errors δt  δ+

in (6) are computed as

t   t  +
t
t φv(xt+1) − v
(cid:62)

δt = r(xt  at) + γv
t = r(xt  at)2 + 2γr(xt  at)v
+
t = r(x+
t   a+

t )2 + 2γr(x+

t   a+

(cid:62)
t φv(xt) 

(cid:62)
t φv(xt+1) + γ2u
t )v+(cid:62)

t φv(x+

t+1) + γ2u+(cid:62)

δ+
t = r(x+
t φu(xt+1) − u
(cid:62)
t φu(x+

t ) + γv+(cid:62)
t   a+
(cid:62)
t φu(xt) 
t+1) − u+(cid:62)

t φu(x+

t ).

t φv(x+

t+1) − v+(cid:62)

t φv(x+

t ) 

(7)

4

θt+β∆ta+t∼µ(·|x+t;θ+t)r+tat∼µ(·|xt;θt)rtδ+t ￿+t v+t u+tCriticδt ￿t vt utCriticθt+1ActorUpdateusingθt(8) or  (9)This TD algorithm to learn the value and square value functions is a straightforward extension of the
algorithm proposed in [23] to the discounted setting. Note that the TD-error  for the square value
function U comes directly from the Bellman equation for U (see Appendix B.1 of [17]).
(4) Actor Update: Estimate the gradients ∇V θ(x0) and ∇U θ(x0) using SPSA (4) or SF (5) and
update the policy parameter θ and the Lagrange multiplier λ as follows: For i = 1  . . .   κ1 

θ(i)
t+1 = Γi

θ(i)
t+1 = Γi

θ(i)
t +

θ(i)
t +

ζ2(t)
β∆(i)
t
ζ2(t)∆(i)
t

λt+1 = Γλ

λt + ζ1(t)

u

(cid:62)

t − vt)

(cid:16)(cid:0)1 + 2λtv
(cid:16)(cid:0)1 + 2λtv
t φu(x0) −(cid:0)v

t φv(x0)(cid:1)(v+
t φv(x0)(cid:1)(v+
t φv(x0)(cid:1)2 − α

(cid:62)

(cid:62)

(cid:62)

(cid:17)(cid:21)

.

β

(cid:16)

(cid:20)
(cid:20)
(cid:20)

(cid:62)

φv(x0) − λt(u+

t − ut)

(cid:62)

φu(x0)

  SPSA (8)

t − vt)

(cid:62)

φv(x0) − λt(u+

t − ut)

(cid:62)

φu(x0)

  SF (9)

(cid:17)(cid:21)

(cid:17)(cid:21)

(10)

Note that 1) the λ-update is the same for both SPSA and SF methods  2) ∆(i)
t ’s are independent
Rademacher and Gaussian N (0  1) random variables in SPSA and SF updates  respectively  3) Γ
is an operator that projects a vector θ ∈ Rκ1 to the closest point in a compact and convex set
C ⊂ Rκ1  and Γλ is a projection operator to [0  λmax]. These projection operators are necessary to
ensure convergence of the algorithms  and 4) the step-size schedules {ζ3(t)}  {ζ2(t)}  and {ζ1(t)}
are chosen such that the critic updates are on the fastest time-scale  the policy parameter update
is on the intermediate time-scale  and the Lagrange multiplier update is on the slowest time-scale
(see Appendix A of [17] for the conditions on the step-size schedules). A proof of convergence
(cid:98)L(θ  λ)
of the SPSA and SF algorithms to a (local) saddle point of the risk-sensitive objective function

= −(cid:98)V θ(x0) + λ((cid:98)Λθ(x0) − α) is given in Appendix B.3 of [17].

(cid:52)

5 Average Reward Setting
The average reward per step under policy µ is deﬁned as (see Sec. 2 for the deﬁnitions of dµ and πµ)

ρ(µ) = lim
T→∞

E

1
T

Rt | µ

=

dµ(x)µ(a|x)r(x  a).

(cid:34)T−1(cid:88)

(cid:35)

(cid:88)

t=0

x a

The goal in the standard (risk-neutral) average reward formulation is to ﬁnd an average optimal
policy  i.e.  µ∗ = arg maxµ ρ(µ). Here a policy µ is assessed according to the expected differential
reward associated with states or state-action pairs. For all states x ∈ X and actions a ∈ A  the
differential action-value and value functions of policy µ are deﬁned as

∞(cid:88)

t=0

E(cid:2)Rt − ρ(µ) | x0 = x  a0 = a  µ(cid:3) 
(cid:88)

πµ(x  a)(cid:2)r(x  a) − ρ(µ)(cid:3)2 = lim

T→∞

Λ(µ) =

x a

In the context of risk-sensitive MDPs  different criteria have been proposed to deﬁne a measure of
variability  among which we consider the long-run variance of µ [8] deﬁned as

(cid:0)Rt − ρ(µ)(cid:1)2 | µ

(cid:35)

.

(11)

Qµ(x  a) =

V µ(x) =

µ(a|x)Qµ(x  a).

(cid:88)

a

1
T

E

(cid:34)T−1(cid:88)
(cid:88)

t=0

x a

This notion of variability is based on the observation that it is the frequency of occurrence of state-
action pairs that determine the variability in the average reward. It is easy to show that

Λ(µ) = η(µ) − ρ(µ)2 

where η(µ) =

πµ(x  a)r(x  a)2.

We consider the following risk-sensitive measure for average reward MDPs in this paper:

max

θ

ρ(θ)

subject to

Λ(θ) ≤ α 

(12)

for a given α > 0. As in the discounted setting  we employ the Lagrangian relaxation procedure to
convert (12) to the unconstrained problem

(cid:16)

= −ρ(θ) + λ(cid:0)Λ(θ) − α(cid:1)(cid:17)

(cid:52)

.

max

λ

min

θ

L(θ  λ)

Similar to the discounted case  we descend in θ using ∇θL(θ  λ) = −∇θρ(θ) + λ∇θΛ(θ) and ascend
in λ using ∇λL(θ  λ) = Λ(θ) − α  to ﬁnd the saddle point of L(θ  λ). Since ∇Λ(θ) = ∇η(θ) −

5

2ρ(θ)∇ρ(θ)  in order to compute ∇Λ(θ) it would be enough to calculate ∇η(θ). Let U µ and W µ
denote the differential value and action-value functions associated with the square reward under
policy µ  respectively. These two quantities satisfy the following Poisson equations:

η(µ) + U µ(x) =

η(µ) + W µ(x  a) = r(x  a)2 +

a

(cid:48)|x  a)U µ(x
(cid:48)

).

P (x

)(cid:3) 

(cid:48)|x  a)U µ(x
(cid:48)

P (x

(cid:88)

µ(a|x)(cid:2)r(x  a)2 +

(cid:88)

x(cid:48)

(cid:88)

x(cid:48)

(13)

(14)

(15)

We calculate the gradients of ρ(θ) and η(θ) as (see Lemma 5 of Appendix C.1 in [17]):

(cid:88)
(cid:88)

x a

∇ρ(θ) =

∇η(θ) =

π(x  a; θ)∇ log µ(a|x; θ)Q(x  a; θ) 

π(x  a; θ)∇ log µ(a|x; θ)W (x  a; θ).

x a

Note that (15) for calculating ∇η(θ) has close resemblance to (14) for ∇ρ(θ)  and thus  similar
to what we have for (14)  any function b : X → R can be added or subtracted to W (x  a; θ)
on the RHS of (15) without changing the result of the integral (see e.g.  [2]). So  we can replace
W (x  a; θ) with the square reward advantage function B(x  a; θ) = W (x  a; θ)−U (x; θ) on the RHS
of (15) in the same manner as we can replace Q(x  a; θ) with the advantage function A(x  a; θ) =
Q(x  a; θ) − V (x; θ) on the RHS of (14) without changing the result of the integral. We deﬁne the
temporal difference (TD) errors δt and t for the differential value and square value functions as

δt = R(xt  at) −(cid:98)ρt+1 +(cid:98)V (xt+1) −(cid:98)V (xt) 
t = R(xt  at)2 −(cid:98)ηt+1 + (cid:98)U (xt+1) − (cid:98)U (xt).
If(cid:98)V   (cid:98)U (cid:98)ρ  and(cid:98)η are unbiased estimators of V µ  U µ  ρ(µ)  and η(µ)  respectively  then we can show
that δt and t are unbiased estimates of the advantage functions Aµ and Bµ  i.e.  E[ δt| xt  at  µ] =
Aµ(xt  at)  and E[ t| xt  at  µ] = Bµ(xt  at) (see Lemma 6 in Appendix C.2 of [17]). From this 
we notice that δtψt and tψt are unbiased estimates of ∇ρ(µ) and ∇η(µ)  respectively  where ψt =
ψ(xt  at) = ∇ log µ(at|xt) is the compatible feature (see e.g.  [21  13]).
6 Average Reward Algorithm

We now present our risk-sensitive actor-critic algorithm for average reward MDPs. Algorithm 1
presents the complete structure of the algorithm along with update rules for the average rewards

(cid:98)ρt (cid:98)ηt; TD errors δt  t; critic vt  ut; and actor θt  λt parameters. The projection operators Γ and Γλ

are as deﬁned in Section 4  and similar to the discounted setting  are necessary for the convergence
proof of the algorithm. The step-size schedules satisfy the standard conditions for stochastic approx-
imation algorithms  and ensure that the average and critic updates are on the (same) fastest time-scale
{ζ4(t)} and {ζ3(t)}  the policy parameter update is on the intermediate time-scale {ζ2(t)}  and the
differential value and square value functions  i.e.  (cid:98)V (x) = v(cid:62)φv(x) and (cid:98)U (x) = u(cid:62)φu(x)  where
Lagrange multiplier is on the slowest time-scale {ζ1(t)}. This results in a three time-scale stochastic
approximation algorithm. As in the discounted setting  the critic uses linear approximation for the
φv(·) and φu(·) are feature vectors of size κ2 and κ3  respectively. Although our estimates of ρ(θ)
and η(θ) are unbiased  since we use biased estimates for V θ and U θ (linear approximations in the
critic)  our gradient estimates ∇ρ(θ) and ∇η(θ)  and as a result ∇L(θ  λ)  are biased. Lemma 7 in
Appendix C.2 of [17] shows the bias in our estimate of ∇L(θ  λ). We prove that our actor-critic
algorithm converges to a (local) saddle point of the risk-sensitive objective function L(θ  λ) (see
Appendix C.3 of [17]).
7 Experimental Results

We evaluate our algorithms in the context of a trafﬁc signal control application. The objective in our
formulation is to minimize the total number of vehicles in the system  which indirectly minimizes
the delay experienced by the system. The motivation behind using a risk-sensitive control strategy
is to reduce the variations in the delay experienced by road users.
We consider both inﬁnite horizon discounted as well average settings for the trafﬁc signal
control MDP  formulated as in [15]. We brieﬂy recall their formulation here: The state at
each time t  xt 
is the vector of queue lengths and elapsed times and is given by xt =

6

Algorithm 1 Template of the Average Reward Risk-Sensitive Actor-Critic Algorithm

Input: parameterized policy µ(·|·; θ) and value function feature vectors φv(·) and φu(·)
Initialization: policy parameters θ = θ0; value function weight vectors v = v0 and u = u0; initial state
x0 ∼ P0(x)
for t = 0  1  2  . . . do

Draw action at ∼ µ(·|xt; θt)
Observe next state xt+1 ∼ P (·|xt  at)
Observe reward R(xt  at)

Average Updates: (cid:98)ρt+1 =(cid:0)1 − ζ4(t)(cid:1)(cid:98)ρt + ζ4(t)R(xt  at) 

(cid:98)ηt+1 =(cid:0)1 − ζ4(t)(cid:1)(cid:98)ηt + ζ4(t)R(xt  at)2

TD Errors:

Critic Updates:
Actor Updates:

δt = R(xt  at) −(cid:98)ρt+1 + v
t = R(xt  at)2 −(cid:98)ηt+1 + u

vt+1 = vt + ζ3(t)δtφv(xt) 

t φv(xt+1) − v
(cid:62)
(cid:62)
t φv(xt)
t φu(xt+1) − u
(cid:62)
(cid:62)
t φu(xt)

θt − ζ2(t)(cid:0) − δtψt + λt(tψt − 2(cid:98)ρt+1δtψt)(cid:1)(cid:17)
(cid:16)
(cid:16)
λt + ζ1(t)((cid:98)ηt+1 −(cid:98)ρ2

t+1 − α)

(cid:17)

θt+1 = Γ

λt+1 = Γλ

ut+1 = ut + ζ3(t)tφu(xt)

(16)

(17)

(18)

end for
return policy and value function parameters θ  λ  v  u

(cid:0)q1(t)  . . .   qN (t)  t1(t)  . . .   tN (t)(cid:1). Here qi and ti denote the queue length and elapsed time since

the signal turned to red on lane i. The actions at belong to the set of feasible sign conﬁgurations.
The single-stage cost function h(xt) is deﬁned as follows:

h(xt) = r1

r2 · qi(t) +

s2 · qi(t)

+ s1

r2 · ti(t) +

s2 · ti(t)

 

(19)

(cid:105)

(cid:88)

i /∈Ip

(cid:105)

(cid:104)(cid:88)

i∈Ip

(cid:104)(cid:88)

i∈Ip

(cid:88)

i /∈Ip

where ri  si ≥ 0 such that ri + si = 1 for i = 1  2 and r2 > s2. The set Ip is the set of prioritized
lanes in the road network considered. While the weights r1  s1 are used to differentiate between the
queue length and elapsed time factors  the weights r2  s2 help in prioritization of trafﬁc.
Given the above trafﬁc control setting  we aim to minimize both the long run discounted as well
average sum of the cost function h(xt). The underlying policy for all the algorithms is a parame-
terized Boltzmann policy (see Appendix F of [17]). We implement the following algorithms in the
discounted setting:
(i) Risk-neutral SPSA and SF algorithms with the actor update as follows:

(cid:33)

(cid:32)
(cid:32)

θ(i)
t+1 = Γi

θ(i)
t +

ζ2(t)
β∆(i)
t

t − vt)
(v+

(cid:62)

φv(x0)

θ(i)
t+1 = Γi

θ(i)
t +

ζ2(t)∆(i)
t

β

t − vt)
(cid:62)
(v+

φv(x0)

SPSA 

(cid:33)

SF 

where the critic parameters v+
t   vt are updated according to (6). Note that these are two-timescale
algorithms with a TD critic on the faster timescale and the actor on the slower timescale.
(ii) Risk-sensitive SPSA and SF algorithms (RS-SPSA and RS-SF) of Section 4 that attempt to
solve (2) and update the policy parameter according to (8) and (9)  respectively. In the average
setting  we implement (i) the risk-neutral AC algorithm from [14] that incorporates an actor-critic
scheme  and (ii) the risk-sensitive algorithm of Section 6 (RS-AC) that attempts to solve (12) and
updates the policy parameter according to (17).
All our algorithms incorporate function approximation owing to the curse of dimensionality asso-
ciated with larger road networks. For instance  assuming only 20 vehicles per lane of a 2x2-grid
network  the cardinality of the state space is approximately of the order 1032 and the situation is
aggravated as the size of the road network increases. The choice of features used in each of our al-
gorithms is as described in Section V-B of [16]. We perform the experiments on a 2x2-grid network.
The list of parameters and step-sizes chosen for our algorithms is given in Appendix F of [17].
Figures 2(a) and 2(b) show the distribution of the discounted cumulative reward Dθ(x0) for the
SPSA and SF algorithms  respectively. Figure 3(a) shows the distribution of the average reward ρ for
the algorithms in the average setting. From these plots  we notice that the risk-sensitive algorithms

7

(a) SPSA vs. RS-SPSA

(b) SF vs. RS-SF

Figure 2: Performance comparison in the discounted setting using the distribution of Dθ(x0).

(a) Distribution of ρ

(b) Average junction waiting time

Figure 3: Comparison of AC vs. RS-AC in the average setting using two different metrics.

that we propose result in a long-term (discounted or average) reward that is higher than their risk-
neutral variants. However  from the empirical variance of the reward (both discounted as well as
average) perspective  the risk-sensitive algorithms outperform their risk-neutral variants.
We use average junction waiting time (AJWT) to compare the algorithms from a trafﬁc signal control
application standpoint. Figure 3(b) presents the AJWT plots for the algorithms in the average setting
(see Appendix F of [17] for similar results for the SPSA and SF algorithms in the discounted setting).
We observe that the performance of our risk-sensitive algorithms is not signiﬁcantly worse than their
risk-neutral counterparts. This coupled with the observation that our algorithms exhibit low variance 
makes them a suitable choice in risk-constrained systems.

8 Conclusions and Future Work
We proposed novel actor critic algorithms for control in risk-sensitive discounted and average reward
MDPs. All our algorithms involve a TD critic on the fast timescale  a policy gradient (actor) on
the intermediate timescale  and dual ascent for Lagrange multipliers on the slowest timescale. In
the discounted setting  we pointed out the difﬁcultly in estimating the gradient of the variance of
the return and incorporated simultaneous perturbation based SPSA and SF approaches for gradient
estimation in our algorithms. The average setting  on the other hand  allowed for an actor to employ
compatible features to estimate the gradient of the variance. We provided proofs of convergence
(in the appendix of [17]) to locally (risk-sensitive) optimal policies for all the proposed algorithms.
Further  using a trafﬁc signal control application  we observed that our algorithms resulted in lower
variance empirically as compared to their risk-neutral counterparts.
In this paper  we established asymptotic limits for our discounted and average reward risk-sensitive
actor-critic algorithms. To the best of our knowledge  there are no convergence rate results available
for multi-timescale stochastic approximation schemes and hence for actor-critic algorithms. This is
true even for the actor-critic algorithms that do not incorporate any risk criterion. It would be an
interesting research direction to obtain ﬁnite-time bounds on the quality of the solution obtained by
these algorithms.

8

 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 20 25 30 35 40 45 50ProbabilityDθ(x0)SPSARS-SPSA 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 30 35 40 45 50 55ProbabilityDθ(x0)SFRS-SF 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 20 30 40 50 60 70ProbabilityρACRS-AC 0 5 10 15 20 25 30 0 2000 4000 6000 8000 10000AJWTtimeRS-ACACReferences
[1] D. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[2] S. Bhatnagar  R. Sutton  M. Ghavamzadeh  and M. Lee. Natural actor-critic algorithms. Automatica  45

(11):2471–2482  2009.

[3] S. Bhatnagar  H. Prasad  and L.A. Prashanth. Stochastic Recursive Algorithms for Optimization  volume

434. Springer  2013.

[4] V. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems &

Control Letters  44:339–346  2001.

[5] V. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research  27:294–311  2002.
[6] H. Chen  T. Duncan  and B. Pasik-Duncan. A Kiefer-Wolfowitz algorithm with randomized differences.

IEEE Transactions on Automatic Control  44(3):442–453  1999.

[7] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncer-

tainty. Operations Research  58(1):203–213  2010.

[8] J. Filar  L. Kallenberg  and H. Lee. Variance-penalized Markov decision processes. Mathematics of

Operations Research  14(1):147–161  1989.

[9] J. Filar  D. Krass  and K. Ross. Percentile performance criteria for limiting average Markov decision

processes. IEEE Transaction of Automatic Control  40(1):2–10  1995.

[10] R. Howard and J. Matheson. Risk sensitive Markov decision processes. Management Science  18(7):

356–369  1972.

[11] V. Katkovnik and Y. Kulchitsky. Convergence of a class of random search algorithms. Automatic Remote

Control  8:81–87  1972.

[12] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matri-

ces. Operations Research  53(5):780–798  2005.

[13] J. Peters  S. Vijayakumar  and S. Schaal. Natural actor-critic. In Proceedings of the Sixteenth European

Conference on Machine Learning  pages 280–291  2005.

[14] L.A. Prashanth and S. Bhatnagar. Reinforcement learning with average cost for adaptive control of trafﬁc
lights at intersections. In Proceedings of the Fourteenth International IEEE Conference on Intelligent
Transportation Systems  pages 1640–1645. IEEE  2011.

[15] L.A. Prashanth and S. Bhatnagar. Reinforcement Learning With Function Approximation for Trafﬁc

Signal Control. IEEE Transactions on Intelligent Transportation Systems  12(2):412–421  june 2011.

[16] L.A. Prashanth and S. Bhatnagar. Threshold Tuning Using Stochastic Optimization for Graded Signal

Control. IEEE Transactions on Vehicular Technology  61(9):3865–3880  Nov. 2012.

[17] L.A. Prashanth and M. Ghavamzadeh. Actor-Critic Algorithms for Risk-Sensitive MDPs. Technical

report inria-00794721  INRIA  2013.

[18] W. Sharpe. Mutual fund performance. Journal of Business  39(1):119–138  1966.
[19] M. Sobel. The variance of discounted Markov decision processes. Applied Probability  pages 794–802 

1982.

[20] J. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.

IEEE Transactions on Automatic Control  37(3):332–341  1992.

[21] R. Sutton  D. McAllester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement learning
with function approximation. In Proceedings of Advances in Neural Information Processing Systems 12 
pages 1057–1063  2000.

[22] A. Tamar  D. Di Castro  and S. Mannor. Policy gradients with variance related risk criteria. In Proceedings

of the Twenty-Ninth International Conference on Machine Learning  pages 387–396  2012.

[23] A. Tamar  D. Di Castro  and S. Mannor. Temporal difference methods for the variance of the reward to go.

In Proceedings of the Thirtieth International Conference on Machine Learning  pages 495–503  2013.

[24] H. Xu and S. Mannor. Distributionally robust Markov decision processes. Mathematics of Operations

Research  37(2):288–300  2012.

9

,Prashanth L.A.
Mohammad Ghavamzadeh
Matthew Joseph
Michael Kearns
Jamie Morgenstern
Aaron Roth