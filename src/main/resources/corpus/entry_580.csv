2019,Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems,Building an open-domain conversational agent is a challenging problem. Current evaluation methods  mostly post-hoc judgments of static conversation  do not capture conversation quality in a realistic interactive context. In this paper  we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel  model-agnostic  and dataset-agnostic method to approximate it. In particular  we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date  achieving a significant Pearson correlation (r>.7  p<.05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations  we perform extended experiments with a set of models  including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally  we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efficiently deploy and evaluate dialog models.,Approximating Interactive Human Evaluation with

Self-Play for Open-Domain Dialog Systems

Asma Ghandeharioun‚àó  Judy Hanwen Shen‚àó  Natasha Jaques‚àó 
Craig Ferguson  Noah Jones  Agata Lapedriza  Rosalind Picard

Department of Media Arts and Science
Massachusetts Institute of Technology

Cambridge  MA 02139

{asma_gh judyshen jaquesn}@mit.edu

{fergusoc ncjones agata}@mit.edu  picard@media.mit.edu

Abstract

Building an open-domain conversational agent is a challenging problem. Current
evaluation methods  mostly post-hoc judgments of static conversation  do not
capture conversation quality in a realistic interactive context. In this paper  we
investigate interactive human evaluation and provide evidence for its necessity; we
then introduce a novel  model-agnostic  and dataset-agnostic method to approxi-
mate it. In particular  we propose a self-play scenario where the dialog system talks
to itself and we calculate a combination of proxies such as sentiment and semantic
coherence on the conversation trajectory. We show that this metric is capable of
capturing the human-rated quality of a dialog model better than any automated
metric known to-date  achieving a signiÔ¨Åcant Pearson correlation (r > .7  p < .05).
To investigate the strengths of this novel metric and interactive evaluation in com-
parison to state-of-the-art metrics and human evaluation of static conversations  we
perform extended experiments with a set of models  including several that make
novel improvements to recent hierarchical dialog generation architectures through
sentiment and semantic knowledge distillation on the utterance level. Finally 
we open-source the interactive evaluation platform we built and the dataset we
collected to allow researchers to efÔ¨Åciently deploy and evaluate dialog models.

1

Introduction

The goal of an open-domain conversational agent is to carry out natural social interactions with
humans. Current state-of-the-art generative neural networks fail in producing key aspects of good
natural conversation  including staying on topic  not being repetitive  and generating emotionally
appropriate responses. One of the biggest challenges in training better dialog systems relates to
the difÔ¨Åculty of evaluating them. Automatic metrics such as BLEU score relate poorly to human
judgment of dialog quality [1]  and while embedding-distance based metrics provide an alternative
[2]  we will show that they also do not correlate well with human evaluation. Without a reliable
metric to optimize  training high quality dialog models remains difÔ¨Åcult.
Since humans are the ultimate authority on what constitutes a good conversation  many authors rely
on human ratings to evaluate their methods [3‚Äì5]. The predominant procedure for obtaining human
ratings uses static evaluation: a context of several sentences  often originating from the dataset that
dialog model was trained on  is used as input to generate a response (either a single utterance or
multiple consecutive utterances). This particular generated response for this particular context is then
provided for a human to evaluate. However  such observational evaluation of a static text sample

‚àóEqual contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Static evaluation fails to capture a lack of diversity in a dialog model‚Äôs responses  as well as its inability
to track the conversation and respond in emotionally appropriate ways. We argue interactive evaluation is needed
to evaluate dialog models  and show that our Emotion+Infersent (EI) models trained on a larger and more diverse
corpus  produce better interactive dialog. We present strong evidence that our novel dialog self-play framework
combined with psychologically motivated novel automated metrics can accurately estimate quality of a model
with respect to its ability to carry out multi-turn open-domain conversations.

Static
HRED - Cornell

Interactive
HRED - Cornell

[Context]: you‚Äôre weird.
[HRED]: i‚Äôm sorry.

[User]: i‚Äôm feeling sad today.
[HRED]: no.
[User]: yes i am!
[HRED]: i‚Äôm sorry  i‚Äôm sorry.
i don‚Äôt want to know.
[User]: you‚Äôre weird.
[HRED]: i‚Äôm sorry.

Interactive
EI HRED - Reddit
[User] talk to me
[EI HRED]: it‚Äôs my job.
[User]: i love you
[EI HRED]: thank you for the
kind words.
[User]: do you love me?
[EI HRED]: it‚Äôs a bit personal 
so I sent you a dm.
[User]: i‚Äôm blushing

Self-play
EI HRED - Reddit
[EI HRED] i can‚Äôt wait to see
what i can do.
[EI HRED] that‚Äôs a good idea.
[EI HRED] how do you deal
with people?
[EI HRED] i‚Äôve been on reddit
for a long time.
[EI HRED] i am in the same boat.

is limited in capturing common failure modes of open-domain dialog systems  such as a lack of
diversity in the responses  inability to track long-term aspects of the conversation  and inability to
maintain a consistent persona [6]. Despite this  static evaluation is commonly used for evaluating
these exact qualities [7  8].
Conversation is inherently a process. In this paper  we argue that multi-turn interactive human
evaluation is essential for evaluating this process  and for making progress in improving open-domain
dialog models. A multi-turn interactive evaluation is open-ended  and allows the users to derive and
control the trajectory of the conversation by saying whatever they want. As opposed to post-hoc
observations of a generated response to a static context  this is an ultimate test of generalization. Table
1 illustrates an example where a bot generates a coherent static response  but interactive evaluation
shows that the distribution of its responses has collapsed onto repeatedly saying the same phrase  a
common problem in open domain dialog [9].
The relative sparsity of interactive human evaluation of dialog systems may relate to the difÔ¨Åculty
and expense of collecting human data. Therefore  we develop a way to approximate human judgment
of interactive dialog quality using a novel form of dialog self-play. We begin by moving beyond
superÔ¨Åcial word-level mapping by proposing a series of metrics to evaluate the quality of conversation
motivated by Ô¨Åndings in psychology. SpeciÔ¨Åcally  inspired by the effectiveness of sense of humor
in creating solidarity [10]  style matching for forming relationship stability and social cohesiveness
[11  12]  and the importance of active listening through forming follow up questions [13]  we propose
metrics to capture sentiment  semantics  and user engagement. We then Ô¨Åt a function that predicts
human assessments of conversation quality given these metrics. This function is used to predict bot
quality through self-play: for a Ô¨Åxed number of turns  the bot generates utterances which are fed back
into itself as input in the next turn. The same metrics described above are computed on the self-play
generated conversation  and the same function Ô¨Åt to human data is used to predict the bot quality. We
show a very high Pearson correlation (r > .7  p < .05) between the predicted quality scores and the
ground-truth human judgments of bot quality  suggesting self-play is a good proxy for interactive
conversation assessment.
To demonstrate the relevance of the interactive evaluation and the proposed self-play evaluation  we
perform extended experiments with different hierarchical architectures. In particular  we compare
three recent hierarchical baselines: HRED [5]  VHRED [3]  VHCR [4]. Motivated by sentiment and
semantics being key aspects of producing high quality conversations  we regularize the top level of
the hierarchy to ensure it encodes such information  using model distillation [14]. Our results show
the effectiveness of the proposed regularization in interactive evaluation in both the human-bot and
the self-play scenarios.
This paper makes three main contributions: 1) demonstrates the necessity of multi-turn interactive
evaluation to capture the quality of the dialog systems; 2) presents a novel self-play framework to
estimate a new psychology-motivated hybrid quality score. These estimations are highly correlated
with quality scores obtained from interactive human evaluation  more strongly than the state-of-the-
art automated metrics; 3) proposes a new method of regularizing hierarchical seq2seq models with
knowledge distillation. All the code  data  and interactive evaluation platform resulting from our
work are publicly available.

2

2 Related work

Interactive evaluation in dialog has been mostly limited to presenting the results of competitions
(e.g. the Alexa prize [15  16]  or the Conversational Intelligence Challenge [6]). Those Ô¨Åndings
reveal that most bots do not perform well in interactive evaluation  due to repetitiveness  inability to
balance dialog acts across the conversation  and inability to maintain a consistent persona [6]. Even
work aimed at maintaining a persona does not test in an interactive setting [7  8]. To the best of our
knowledge  no prior work has compared multi-turn  interactive human evaluations of open-domain
dialog models to traditional forms of evaluation.
Dialog systems remain difÔ¨Åcult to train due to the lack of metrics that can effectively capture good
dialog quality. Several authors have proposed training automatic predictors of human judgment or to
combine human judgment with automatic metrics [17‚Äì19]. However  a state-of-the-art model trained
to predict human judgments achieved a Pearson correlation of .44 with the ground truth [18].
Perhaps the lack of research into interactive evaluation relates to the difÔ¨Åculty and cost of collecting
human ratings. We show that human judgments of the quality of an interactive evaluation can
be automatically and reliably approximated using dialog model self-play. There is limited work
investigating self-play for dialog systems: Shah et al. [20] use a task schema and user simulator to
generate samples for input to a goal-directed dialog system  while Li et al. [9] use a copy of a dialog
model to compute a reward function that can be optimized with reinforcement learning. However  we
are not aware of prior work using self-play for approximating interactive human evaluation.
Interactive conversation necessitates tracking long-term aspects of the dialog like the topic and tone.
Hierarchical recurrent neural networks (RNNs) have been proposed as a way to improve long-term
tracking of the conversation  through maintaining both a word- and utterance-level RNN [3‚Äì5  21  22].
Yet dialog is more than language modeling  it requires topic and social coherence. Prior performance
improvements to dialog models using topic information include appending topic as an additional
input [23]  or extracting topic information using Latent Dirichlet Allocation [24  25]. Towards social
and emotional coherence  previous works have investigated various features and loss functions based
on emotion [26‚Äì30]. Given research highlighting the ineffectiveness of LDA for short texts [31] 
such as those involved in casual conversation  and the unavailability of topic and tone supervision
at-scale  approaches overcoming these limitations are preferred. To the best of our knowledge 
transferring sentiment and semantic information from a pre-trained model directly into a dialog model
using knowledge distillation [14] has not been studied. Thus  we select a set of recent hierarchical
dialog models and their improved versions through knowledge distillation for a thorough multi-turn
interactive evaluation and comparison to traditional evaluation.

3 Knowledge distillation for sentiment and semantic regularization

To systematically compare multi-turn interactive evaluation of open-domain dialog with traditional
forms of evaluation  we include a diverse set of models. Particularly  we build on three existing
hierarchical seq2seq architectures designed for dialog. Here  we provide a brief summary; for detailed
information  see [5  3  4]. The Ô¨Årst baseline model  Hierarchical Recurrent Encoder Decoder (HRED)
[5] extends a traditional seq2seq model by adding a third recurrent neural network (RNN)  which
is only updated after each dialog turn  or utterance. The idea behind this Context RNN is that it
could potentially track longer term aspects of the conversation  such as the topic; however  there is
no guarantee that it will learn to do so. The decoder of the HRED model conditions on both the
embedding produced by the encoder for the current utterance  he
n  and the embedding of the Context
RNN for the previous utterance  hc
The second baseline model  Variational HRED (VHRED) [3]  extends HRED with a variational
constraint on the utterance embedding space z. Let xn = [w1n  w2n . . . wmn] be the n-th utterance
composed of tokens w1..m. VHRED predicts xn as follows:
he
n = f e(xn‚àí1)
hc
n‚àí1 = f c(xn‚àí1  he
¬µ  Œ£ = f (hc

n‚àí1.

n‚àí1)

n‚àí1)
pŒ∏(zn|x<n) = N (z|¬µ  Œ£)
p(xn|x<n) = f d(hc

n‚àí1  zn)

3

(1)
(2)
(3)
(4)
(5)

Figure 1: Illustration of the EI regularization (blue-solid) applied to VHRED baseline (red-checkered)
to enforce encoding sentiment and semantics of an utterance in the Context RNN. The EI regulariza-
tion can be similarly applied to HRED and VHCR.

Equations (1)-(5) describe the computation of VHRED at inference time where f e  f c  and f d are
Gated Recurrent Unit (GRU) networks for the encoder  context  and decoder RNNs  respectively;
at training time  it allows the computation of z  ¬µ  and Œ£ to condition on the encoding of the target
n  giving the posterior distribution pŒ®(zn|x‚â§n). A Kullback-Leibler (KL) divergence
utterance  he
constraint is placed between the posterior and prior  DKL(pŒ®||pŒ∏).
The third model  Variational Hierarchical Conversation RNN (VHCR) [4] further extends VHRED
by drawing a prior encoding zconv ‚àº N (0  I) for each conversation  allowing all parts of the model
(f c  ¬µ  Œ£) to condition on zconv  which is unchanging throughout the conversation.

3.1 Emotion and Infersent regularization (EI)

While the hierarchical design of these models is motivated by a desire to allow tracking high-level 
slow-changing aspects of the conversation like topic or tone  it is unclear that the network will be able
to model these aspects without additional structure or information. We thus propose a regularization
to the top level of the hierarchy  the Context RNN  to force it to encode both the sentiment and
semantics of the utterance. To do this  we leverage a state-of-the-art sentiment detection model
trained on a large Twitter corpus [32]  as well as the recently proposed Infersent sentence-embedding
model trained to predict the meaning (i.e. entailment  contradiction) of sentences [33]  and distill
them into the Context RNN.
First  we use these models to predict the emotional content  fE(xn)  and infersent embedding 
fI (xn) of each input utterance. We then add an additional network to the hierarchical models
which predicts these values based on the context RNN embedding of the utterance: f distill(hc
n) =
< fE(xn)  fI (xn) >. The goal is to transfer knowledge of emotion and semantics in text into the
context RNN via knowledge distillation [14].
Figure 1 illustrates  in blue color  the EI regularization applied to the VHRED model. The
regularization can be similarly applied to HRED and VHCR. In our experiments we refer to
the regularized models as HRED-EI  VHRED-EI  and VHCR-EI  respectively  or  more gener-
ally  EI models as opposed to baseline models. The code for all our models is available at
https://github.com/natashamjaques/neural_chat and was originally based on [4]. For
details regarding hyper-parameter tuning refer to ¬ßA.12.

4

Interactive evaluation methodologies

4.1 Traditional evaluation

Automatic metrics Embedding-based metrics compare generated sentences to ground truth sentences
using a vector representation of words [2]. In this work  we use three embedding metrics: embedding
average  vector extrema  and greedy matching. These three metrics are used in previous open-domain
dialog models [1  3  4]. We also use perplexity as a standard measure of the likelihood of the

4

Distillationw1Word-level encoderRNNContext RNNWord-level decoderRNNw2wNw1w2wNùùªùù®ùùªùù®PosteriorPriorKLFigure 2: Screenshots of our Interactive Evaluation Platform (available at https://neural.chat): (a) chat
window (left) and Ô¨Årst part of the evaluation form (right); (b) second part of the evaluation form (to show all
evaluation questions asked).

generated sentences with respect to the target outputs. Another common metric for variational models
is the KL-Divergence between the posterior and the prior distribution  as a way of assessing the
information encoded into the latent variables [21] (Figure 1 illustrates KL for the VHRED model).
More information regarding embedding metrics can be found in ¬ßA.7.
Conventional static human evaluation We employ a similar method to previous work for our static
human evaluation of generated responses [3  4]  sampling contexts from each corpus and asking
humans to compare the generated responses. To reduce ambiguity  we exclude contexts shorter
than 10 tokens and contexts containing <unknown> tokens. We recruited participants from Amazon
Mechanical Turk (AMT) to compare generated sentences. Annotators could also select a third ‚Äútied‚Äù
option. For each example (context and pair of generated sentences)  we asked annotators to compare
generated sentences based on quality  Ô¨Çuency  diversity  contingency  and empathy. Each batch of
100 pairwise comparisons were labeled by 6 - 8 annotators.

4.2

Interactive human evaluation

To address the limitations of static human evaluation  we built a platform for conducting interactive
evaluation of dialog models with humans  which we make available in open-source to the community
(see Figure 2). Annotators rated quality  Ô¨Çuency  diversity  relatedness  and empathy of a bot after
interacting with it for at least 3 turns. Participants can also upvote or downvote each bot response.
For more information about this platform  see ¬ßA.10. Our goal is to make this work transparent
and reproducible  while adding diversity to the platforms future practitioners can choose to use (e.g.
ParlAI [34]  Plato Research Dialog System [35]  ChatEval [36]).

4.3 Novel metrics and self-play

Inspired by real-world human interactions  we introduce novel metrics to capture the morphology
of a conversation  i.e.  how the users‚Äô responses progress over time and how the bot‚Äôs responses
interact with them. We propose a hybrid combination of these metrics  MH  that is optimized to
predict conversation quality on human data. We then apply MH to self-play  i.e.  the trajectory of
bot-generated responses  and investigate how it relates to human ratings of conversation quality.
Sentiment metrics To approximate emotional tone of an utterance  we use a state-of-the-art sentiment
detector trained on a large Twitter corpus [32]. This pre-trained model outputs an emotion embedding
‚Äì a probability distribution over 64 most-frequently used emojis. To estimate the Sentiment Coherence
between user‚Äôs query and generated samples  we calculate the cosine similarity between their emotion
embeddings. We deÔ¨Åne a set of weights over the 64 emojis and calculate the weighted sum over an
emotion embedding vector to derive a Sentiment score which is higher for positive sentiment and
lower for negative sentiment (See ¬ßA.11). We deÔ¨Åne Sentiment Transition as the change between
user‚Äôs Sentiment before and after a bot response. Additionally  Sentiment Min-Max is deÔ¨Åned by the
slope of change between min and max Sentiment in user utterances over the course of a conversation.
Since humor can be used to create solidarity [10]  we count the number of ‚Äúha"s in the user response
as a proxy for Laughter. The combination of these metrics provides a snapshot of the trajectory of
sentiment in a conversation and quantiÔ¨Åes if the bot is able to elicit positive emotions in the user.

5

 a)b)Semantic metrics Language style matching is a strong predictor of relationship stability [11] and
social cohesiveness [12]; thus  we introduce metrics to capture lexical similarity. We use Infersent 
a state-of-the-art sentence-embedding model to encode the user and bot responses into a 4096-
dimensional embedding space [33]. Infersent was trained to distinguish if two sentences are support-
ing  contradicting  or have a neutral relationship. We estimate Semantic Similarity by calculating the
cosine similarity between the infersent embedding of the user‚Äôs query and the generated bot sample.
Additionally  we use the classic Word2Vec embeddings trained on Google News Corpus along with
average  extrema  and greedy aggregation methods similar to Section 4.1 to derive Average Word
Coherence  Extrema Word Coherence  and Greedy Word Coherence between user and bot responses.
Engagement metrics Asking questions is an important active listening skill which is linked to
conversation management  attentiveness  and responsiveness [13  37]. Therefore  we deÔ¨Åne Question
Score to quantify if the bot is using question words and/or a question mark. We also introduce # Words
as a proxy for user engagement that counts the number of words in their response.
Hybrid metric (MH) We combine the aforementioned metrics (Mi) using linear regression  and
optimize their coefÔ¨Åcients (Œªi) to best predict human judgment of interactive conversation quality:

MH =(cid:80) Œªi ‚àó Mi + M0. We use a leave-bot-out scenario where we isolate all the human conversa-

tions with one of the dialog models  œáj  as the hold-out test set. We train the Œªi j on the remaining
quality ratings. We found that the learned Œªis were stable across the training folds  only exhibiting
small variations. Other researchers are encouraged to use our learned coefÔ¨Åcients directly or adjust
them according to their own interactive human evaluation dataset. See ¬ßA.2 for more details about
the learned Œªis.
Self-play as an approximation for interactive evaluation Since interactive human evaluation is
costly  we propose a self-play scenario where the dialog system talks to itself  i.e. the bot generated
responses are fed back into it as the next turn input. For each model œáj  we generate 100 random
conversations  Ô¨Åxed at 10 turns. The self-play trajectories created using model œáj are treated as the
hold-out set. Therefore  the trained Œªi j values based on all conversations except for the ones with œáj
are used to calculate MH on each generated bot-bot conversation trajectory for œáj. The estimated
MH values are averaged across conversation samples for œáj. This value is used for comparison
against the ground-truth interactive quality ratings aggregated on the bot-level.

5 Experiments

5.1 Datasets

A common source of data for open-domain dialog systems is movie scripts  among which the COR-
NELL dataset [38] is the largest and most commonly used. Therefore  we use it to benchmark against
previous state-of-the-art results [4]. Its median conversation length is 3 utterances and the conversa-
tions are strictly between pairs of speakers. Recognizing that movie lines have limited conversation
diversity  we also built a new corpus  REDDIT. Between the many different subreddits available 
the conversations vastly differ on topic  language style  and participation patterns. We select the
Casual Conversations forum (r/CasualConversations)  a community of 607K conversationalists
discussing a variety of topics. We collect a dataset of 109K conversations of at least 3 turns with the
median conversation containing 7 utterances from conversational exchanges on the platform in 20182.
More more details about this dataset refer to ¬ßA.6.

5.2

Interactive human evaluation

Table 1 (in ¬ß1) illustrates how EI regularization produces a higher quality conversation when compared
to baseline. Rather than cherry-picking results  we make all of the bots evaluated in the study available
at https://neural.chat/BRFZACDCOA/ for readers to assess interactively.
Table 2 summarizes human ratings of baseline and EI models obtained via interactive evaluation. In
total  565 ratings were captured. Each dialog model has been evaluated by a number of annotators 
ranging from 36 to 56. For additional information about human annotators refer to ¬ßA.9. We
ran a 3-factor ANOVA on the sum of user scores  where the independent variables are model
architecture (HRED  VHRED  VHCR)  EI regularization (Baseline  EI)  and dataset (CORNELL 

2This REDDIT dataset is available at https://affect.media.mit.edu/neural_chat/datasets.

6

Table 2: Mean human ratings for Baseline and EI (Emotion+Infersent) models for HRED  VHRED  and VHCR
architectures with 90% conÔ¨Ådence intervals. See ¬ß5.2 for 3-factor ANOVA results.

Model

HRED

VHRED

VHCR

Metric
quality
Ô¨Çuency
diversity
contingency
empathy
quality
Ô¨Çuency
diversity
contingency
empathy
quality
Ô¨Çuency
diversity
contingency
empathy

Baseline
2.182 ¬± 0.305
3.909 ¬± 0.387
2.836 ¬± 0.374
2.200 ¬± 0.291
2.673 ¬± 0.352
2.022 ¬± 0.309
3.109 ¬± 0.351
3.565 ¬± 0.442
2.261 ¬± 0.287
2.739 ¬± 0.374
2.132 ¬± 0.247
2.679 ¬± 0.306
3.755 ¬± 0.340
2.189 ¬± 0.270
2.340 ¬± 0.316

Cornell
EI
2.347 ¬± 0.313
4.000 ¬± 0.381
2.735 ¬± 0.380
2.469 ¬± 0.336
2.490 ¬± 0.350
2.333 ¬± 0.252
3.949 ¬± 0.396
4.385 ¬± 0.371
2.487 ¬± 0.346
2.564 ¬± 0.367
2.548 ¬± 0.380
3.976 ¬± 0.380
4.238 ¬± 0.421
2.571 ¬± 0.356
2.714 ¬± 0.368

Baseline
2.527 ¬± 0.310
4.436 ¬± 0.349
3.418 ¬± 0.386
2.382 ¬± 0.288
3.018 ¬± 0.329
2.694 ¬± 0.392
4.250 ¬± 0.496
5.00 ¬± 0.468
2.472 ¬± 0.362
3.000 ¬± 0.393
2.615 ¬± 0.350
3.923 ¬± 0.433
4.436 ¬± 0.455
2.077 ¬± 0.298
2.974 ¬± 0.434

Reddit
EI
2.714 ¬± 0.299
4.786 ¬± 0.316
3.554 ¬± 0.372
2.536 ¬± 0.322
3.107 ¬± 0.337
2.864 ¬± 0.341
4.477 ¬± 0.402
4.705 ¬± 0.353
2.773 ¬± 0.370
3.341 ¬± 0.385
2.692 ¬± 0.298
4.308 ¬± 0.395
4.231 ¬± 0.382
2.692 ¬± 0.354
3.288 ¬± 0.379

Table 3: Results of automatic traditional metrics for 1-turn responses of models per context of baseline and EI
(Emotion + Infersent) models. PPL: perplexity  KL: KL divergence  Avg: Average  Ext: Extrema  Grd: Greedy

Model
HRED

VHRED

VHCR

Version
baseline
EI
baseline
EI
baseline
EI

PPL
52.311
47.636
49.414
50.526
61.000
49.243

KL
-
-
.264
.517
.562
.475

Cornell
Avg
.471
.560
.539
.545
.532
.588

Ext
.329
.383
.352
.355
.345
.369

Grd
.331
.400
.395
.394
.382
.444

PPL
41.730
41.245
36.240
35.510
36.736
37.198

KL
-
-
.188
.167
.267
.231

Reddit
Avg
.649
.651
.635
.636
.619
.639

Ext
.394
.398
.383
.392
.371
.394

Grd
.474
.482
.464
.465
.448
.469

REDDIT). We found a signiÔ¨Åcant main effect of EI regularization and dataset  but no signiÔ¨Åcant
difference between the three types of hierarchical models. We found that adding emotion and
infersent (EI) regularization to baseline models improved the interactive chat experience signiÔ¨Åcantly 
F (554  1) = 9.016  p = .003. Further  the models trained on the REDDIT dataset performed
signiÔ¨Åcantly better  F (554  1) = 30.796  p < .001. This Ô¨Ånding validates the hypothesis that
distilling information about topic and tone into the top level of the hierarchy is useful for good
conversation  and suggests that the REDDIT dataset could provide more realistic training for open-
domain dialog and be valuable to the community. Additional ablation results are provided in ¬ßA.1.

5.3 Traditional metrics

Automatic metrics Several prior works have focused on ensuring that the variational KL term
remains high in order to improve model quality (e.g. [4  21]). However  we observe there is no
consistency between human quality rating and KL (Table 3). See ¬ßA.8 for details about other human
metrics  e.g. Ô¨Çuency  diversity  contingency  and empathy. Thus  it is not evident that KL captures
human judgements of dialog quality. Even perplexity (a transformation of the cross-entropy loss used
to train our models) falls short of capturing human quality judgments  underscoring the difÔ¨Åculty
in effectively training good language models. We Ô¨Ånd embedding metrics show more promise in
preserving the order of human quality ratings  but have only weak correlation with human ratings.
We present evidence for our novel hybrid metric being a much stronger alternative.
Human static evaluation As shown in Table 4  while static human evaluation suggests EI regular-
ization is effective due to a higher number of win judgments3  the results are noisy and difÔ¨Åcult to
interpret due to large conÔ¨Ådence intervals and a high percentage of ties. The median inter-annotator
agreement measured pairwise through Cohen‚Äôs Œ∫ [39] for our human evaluation was only 0.176 and
0.120 for CORNELL and REDDIT respectively. This level of annotator agreement is lower than the

3We follow [4] to highlight the higher value between wins/losses and reporting 90% conÔ¨Ådence intervals.

7

Table 4: Results from human static evaluation for EI (Emotion+Infersent) vs. BL (baseline) models as measured
by pairwise comparisons of Quality with 90% conÔ¨Ådence intervals.

Model
HRED-EI
VHRED-EI
VHCR-EI

Wins %
40.8 ¬± 4.9
36.9 ¬± 4.7
33.0 ¬± 6.1

Cornell
Losses %
24.5 ¬± 4.9
36.6 ¬± 5.6
29.0 ¬± 5.4

Ties %
34.8 ¬± 9.2
26.6 ¬± 6.9
38.0 ¬± 10.1

Wins %
31.3 ¬± 5.2
39.0 ¬± 7.0
33.7 ¬± 7.9

Reddit
Losses %
29.5 ¬± 6.6
34.0 ¬± 5.3
27.3 ¬± 3.3

Ties %
39.3 ¬± 10.7
27.0 ¬± 8.9
39.0 ¬± 8.6

median Cohen‚Äôs Œ∫ of previous work [1] and explains the larger conÔ¨Ådence intervals. Even after
removing ambiguous examples (i.e. where equal number of annotators select each response as being
better)  large annotation variation persists. This may be due to subjectivity and ambiguity arising
from different interpretations of <unknown> tokens or the short length of contexts in the CORNELL
corpus (e.g. median length of conversation of 3 utterances). These Ô¨Åndings further highlight the
importance of an interactive evaluation as opposed to limited static responses.

5.4 Novel metrics applied to human data and self-play

We examine how the novel psychologically-inspired metrics relate to the trajectories of the 100 best
and 100 worst quality conversations. This is only feasible with interactive evaluation. As shown
in Figure 3  we observe that appropriate sentiment  coherent semantics  and engaging users are
indispensable to attaining high quality ratings in interactive interaction. Comparing EI and baseline
conditions  we see a replication of these trends (Figure 4). For example  EI elicits longer responses
from users (greater engagement)  with more laughter and higher semantic coherence.
Figure 5 summarizes the relationships between interactive human ratings and the automated metrics4.
We observe that our sentiment metric applied to human data on its own has higher correlation
with interactive human ratings than the commonly used metrics such as perplexity and embedding
distance metrics. Most importantly  our novel hybrid metric  MH  applied to self-play 5 aggregated
on the model-level is strongly correlated with all human ratings (r > .7)  while previous metrics
achieved r < .5. This is a signiÔ¨Åcant Ô¨Ånding  suggesting that even without running interactive human
evaluation  we can automatically approximate it through self-play. This metric is agnostic to the
training set and model type and can be calculated on the trajectory of self-play utterances for any
chatbot  regardless of its architecture. One interpretation is that the self-play framework keeps the
conversation within the training set distribution  and the model is less likely to produce <unknown>
tokens. Therefore  MH and its sub-components have meaningful values and can be useful for quality
approximation.
On a realistic conversation trajectory  MH is a hybrid of conÔ¨Çicting objectives and thus is less
susceptible to exploitation [40]. However  the purpose of the self-play metric ( ÀÜMH) in its current
form is a post-hoc evaluation of a dialog model. There are precautions if one intends to directly
optimize for ÀÜMH or its sub-components  for example in a reinforcement learning scenario. The
current formulation of self-play uses trajectories entirely generated by the same model. If one intends
to optimize ÀÜMH  we suggest calculating it on conversation trajectories between the bot and an external
baseline model or a Ô¨Åxed copy [41]  or adopting adversarial learning by maintaining a discriminator
to distinguish between real/fake conversations [42]. This implicitly enforces generating realistic
language. Additionally  we have shown how to successfully learn using sub-components of ÀÜMH as
reward functions [43].

6 Conclusions

A major obstacle in open-domain dialog generation is the predominant optimization of an objective
function that does not closely match human judgment of conversation quality in a naturalistic chat.
In this paper  we have argued that it is necessary to go beyond static evaluation by investigating the

4 For additional correlation results across the human metrics  between Mis and human metrics on a bot-level 

and Spearman and Kendall rank coefÔ¨Åcients  see ¬ßA.3  ¬ßA.4  and ¬ßA.5 respectively.

5Analyzing utterance overlap shows that these self-play conversations are distinct from the training corpus

and exhibit high diversity for variational models. Details can be found in ¬ßA.13.

8

(a)

(b)

(c)

(d)

Figure 3: One hundred highest vs. lowest quality conversation trajectories; lines: mean  shaded area: 90%
conÔ¨Ådence intervals  x-axis: conversation turns. (a) Timing of upvote/downvote ratings: A bad Ô¨Årst impression
impedes overall rating. (b) Participants talk longer and use more words in conversations rated higher. (c)
High-quality conversations elicit more positive user sentiment; many participants leave after expressing negative
sentiment. (d) High-quality conversations are more semantically similar as measured by average word coherence
between user query and bot responses. Users tend to leave the conversation when the bot responses are
semantically dissimilar.

(a)

(b)

(c)

Figure 4: EI vs. baseline conversation trajectories; lines: mean  shaded area: 90% conÔ¨Ådence intervals  x-axis:
conversation turns. (a) EI elicits longer responses from users  suggesting that they are more engaged compared
to the baseline models. (b) EI evokes more laughter from users compared to baseline. (c) EI has higher semantic
coherence as measured by average word coherence. The same pattern applies to greedy and extrema word
coherence.

Figure 5: Pearson correlations between Ô¨Åve human metrics and automated metrics. Sentiment -U has higher
correlation with interactive human ratings than prior metrics. Hybrid Metric MH -B/B  our novel self-play
based metric  has higher correlation across all human metrics more than any other metric proposed to-date.
Notes: -U: Calculated on user response  -B: Calculated on bot response  -U/B: Calculated between user and bot
response  -B/B: Calculated between consecutive bot utterances.

strengths of interactive evaluation and highlighting blind-spots of traditional static evaluation methods.
To alleviate this problem  we have combined interactive human data with psychologically-motivated
measures and introduced a novel hybrid metric. Using this metric in a self-play framework provides
results that are strongly correlated with human judgment of chatbot empathy (r > .8) and quality
(r > .7). Additionally  we have demonstrated a signiÔ¨Åcant improvement to several hierarchical
seq2seq generative models using regularization of the utterance level of the hierarchy with knowledge
distillation. Finally  we have open-sourced the platform together with a new REDDIT dataset.

9

010203040501012Manual RatingsManual Ratings (Bot)QualityBestWorst010203040500.00.20.40.60.81.0# Words# Words (User)QualityBestWorst0102030405021012SentimentSentiment (User)QualityBestWorst010203040500.00.20.40.60.8Average Word CoherenceAverage Word Coherence (User-Bot)QualityBestWorst0102030400.00.10.20.3# Words# Words (User)ModelEIBaseline0102030400.50.00.51.01.5LaughterLaughter (User)ModelEIBaseline0102030400.00.20.40.60.8Average Word CoherenceAverage Word Coherence (User-Bot)ModelEIBaselineQualityDiversityFluencyContingencyEmpathy Human Interactive Evaluation Automatic M. Sentiment M.Semantic M.Engagement M.Bits per WordPerplexityAverageExtremaGreedySentiment-USent. Transition-USent. Min-Max-ULaughter-USent. Coher.-U/BAvg. Word Coher. U/BExtrema Word Coher.-U/BGreedy Word Coher. -U/BQuestion Score-B# Words-UHybrid Metric (MH)-B/BSemantic Coher.-U/BAcknowledgments

We thank Ardavan Saeedi  Max Kleiman-Weiner  Oliver Saunders Wilder  Kyle Kastner  Sebastian
Zepf  Ryan Lowe  Abdul Saleh  and Kristy Johnson for helpful discussions  and many others
for helping test-drive our bots. We thank the MIT Quest for Intelligence  and MIT Stephen A.
Schwarzman College of Computing  Machine Learning Across Disciplines Challenge for providing
computing resources  and MIT Media Lab Consortium and RTI2018-095232-B-C22 grant from the
Spanish Ministry of Science for supporting this research.

References
[1] Chia-Wei Liu  Ryan Lowe  Iulian Serban  Mike Noseworthy  Laurent Charlin  and Joelle Pineau.
How not to evaluate your dialogue system: An empirical study of unsupervised evaluation
metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing  pages 2122‚Äì2132  2016.

[2] Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. proceedings

of ACL-08: HLT  pages 236‚Äì244  2008.

[3] Iulian Vlad Serban  Alessandro Sordoni  Ryan Lowe  Laurent Charlin  Joelle Pineau  Aaron
Courville  and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for
generating dialogues. In Thirty-First AAAI Conference on ArtiÔ¨Åcial Intelligence  2017.

[4] Yookoon Park  Jaemin Cho  and Gunhee Kim. A hierarchical latent structure for variational
conversation modeling. In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies  Volume 1
(Long Papers)  pages 1792‚Äì1801  2018.

[5] Iulian V Serban  Alessandro Sordoni  Yoshua Bengio  Aaron Courville  and Joelle Pineau.
Building end-to-end dialogue systems using generative hierarchical neural network models. In
Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence  2016.

[6] Emily Dinan  Varvara Logacheva  Valentin Malykh  Alexander Miller  Kurt Shuster  Jack Ur-
banek  Douwe Kiela  Arthur Szlam  Iulian Serban  Ryan Lowe  et al. The second conversational
intelligence challenge (convai2). arXiv preprint arXiv:1902.00098  2019.

[7] Pierre-Emmanuel Mazare  Samuel Humeau  Martin Raison  and Antoine Bordes. Training
millions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing  pages 2775‚Äì2779  2018.

[8] Jiwei Li  Michel Galley  Chris Brockett  Georgios Spithourakis  Jianfeng Gao  and Bill Dolan.
A persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)  volume 1  pages 994‚Äì1003 
2016.

[9] Jiwei Li  Will Monroe  Alan Ritter  Dan Jurafsky  Michel Galley  and Jianfeng Gao. Deep
reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing  pages 1192‚Äì1202  2016.

[10] Jennifer Hay. Functions of humor in the conversations of men and women. Journal of

pragmatics  32(6):709‚Äì742  2000.

[11] Molly E Ireland  Richard B Slatcher  Paul W Eastwick  Lauren E Scissors  Eli J Finkel  and
James W Pennebaker. Language style matching predicts relationship initiation and stability.
Psychological science  22(1):39‚Äì44  2011.

[12] Amy L Gonzales  Jeffrey T Hancock  and James W Pennebaker. Language style matching as a

predictor of social dynamics in small groups. Communication Research  37(1):3‚Äì19  2010.

[13] Karen Huang  Michael Yeomans  Alison Wood Brooks  Julia Minson  and Francesca Gino.
It doesn‚Äôt hurt to ask: Question-asking increases liking. Journal of personality and social
psychology  113(3):430  2017.

10

[14] Geoffrey Hinton  Oriol Vinyals  and Jeff Dean. Distilling the knowledge in a neural network.

arXiv preprint arXiv:1503.02531  2015.

[15] Iulian V Serban  Chinnadhurai Sankar  Mathieu Germain  Saizheng Zhang  Zhouhan Lin 
Sandeep Subramanian  Taesup Kim  Michael Pieper  Sarath Chandar  Nan Rosemary Ke  et al.
A deep reinforcement learning chatbot. arXiv preprint arXiv:1709.02349  2017.

[16] Anu Venkatesh  Chandra Khatri  Ashwin Ram  Fenfei Guo  Raefer Gabriel  Ashish Nagar 
Rohit Prasad  Ming Cheng  Behnam Hedayatnia  Angeliki Metallinou  et al. On evaluating and
comparing conversational agents. arXiv preprint arXiv:1801.03625  4:60‚Äì68  2018.

[17] Chikara Hashimoto and Manabu Sassano. Detecting absurd conversations from intelligent
In Proceedings of the 2018 World
assistant logs by exploiting user feedback utterances.
Wide Web Conference on World Wide Web  pages 147‚Äì156. International World Wide Web
Conferences Steering Committee  2018.

[18] Ryan Lowe  Michael Noseworthy  Iulian Vlad Serban  Nicolas Angelard-Gontier  Yoshua
Bengio  and Joelle Pineau. Towards an automatic turing test: Learning to evaluate dialogue
responses. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)  pages 1116‚Äì1126  2017.

[19] Tatsunori B Hashimoto  Hugh Zhang  and Percy Liang. Unifying human and statistical evalua-

tion for natural language generation. arXiv preprint arXiv:1904.02792  2019.

[20] Pararth Shah  Dilek Hakkani-Tur  Bing Liu  and Gokhan Tur. Bootstrapping a neural conver-
sational agent with dialogue self-play  crowdsourcing and on-line reinforcement learning. In
Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies  Volume 3 (Industry Papers)  pages
41‚Äì51  2018.

[21] Xiaoyu Shen  Hui Su  Shuzi Niu  and Vera Demberg. Improving variational encoder-decoders

in dialogue generation. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence  2018.

[22] Tiancheng Zhao  Ran Zhao  and Maxine Eskenazi. Learning discourse-level diversity for neural
dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)  pages
654‚Äì664  2017.

[23] Shalini Ghosh  Oriol Vinyals  Brian Strope  Scott Roy  Tom Dean  and Larry Heck. Contextual

lstm (clstm) models for large scale nlp tasks. arXiv preprint arXiv:1602.06291  2016.

[24] Jiwei Li and Dan Jurafsky. Neural net models of open-domain discourse coherence.

In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing 
pages 198‚Äì209  2017.

[25] Chen Xing  Wei Wu  Yu Wu  Jie Liu  Yalou Huang  Ming Zhou  and Wei-Ying Ma. Topic aware
neural response generation. In Thirty-First AAAI Conference on ArtiÔ¨Åcial Intelligence  2017.

[26] Hao Zhou  Minlie Huang  Tianyang Zhang  Xiaoyan Zhu  and Bing Liu. Emotional chatting
machine: Emotional conversation generation with internal and external memory. In Thirty-
Second AAAI Conference on ArtiÔ¨Åcial Intelligence  2018.

[27] Xianda Zhou and William Yang Wang. Mojitalk: Generating emotional responses at scale.
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers)  pages 1128‚Äì1137  2018.

[28] Chenyang Huang  Osmar Zaiane  Amine Trabelsi  and Nouha Dziri. Automatic dialogue gener-
ation with expressed emotions. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies 
Volume 2 (Short Papers)  pages 49‚Äì54  2018.

[29] Hannah Rashkin  Eric Michael Smith  Margaret Li  and Y-Lan Boureau. I know the feeling:

Learning to converse with empathy. arXiv preprint arXiv:1811.00207  2018.

11

[30] Hannah Rashkin  Eric Michael Smith  Margaret Li  and Y-Lan Boureau. Towards empathetic
open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics  pages 5370‚Äì5381  2019.

[31] Xiaohui Yan  Jiafeng Guo  Yanyan Lan  and Xueqi Cheng. A biterm topic model for short texts.
In Proceedings of the 22nd international conference on World Wide Web  pages 1445‚Äì1456.
ACM  2013.

[32] Bjarke Felbo  Alan Mislove  Anders S√∏gaard  Iyad Rahwan  and Sune Lehmann. Using millions
of emoji occurrences to learn any-domain representations for detecting sentiment  emotion and
sarcasm. In 2017 Conference on Empirical Methods in Natural Language ProcessingConfer-
ence on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics  2017.

[33] Alexis Conneau  Douwe Kiela  Holger Schwenk  Lo√Øc Barrault  and Antoine Bordes. Super-
vised learning of universal sentence representations from natural language inference data. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing 
pages 670‚Äì680  2017.

[34] Alexander H Miller  Will Feng  Adam Fisch  Jiasen Lu  Dhruv Batra  Antoine Bordes  Devi
Parikh  and Jason Weston. Parlai: A dialog research software platform. arXiv preprint
arXiv:1705.06476  2017.

[35] Alexandros Papangelis  Yi-Chia Wang  Piero Molino  and Gokhan Tur. Collaborative multi-
agent dialogue model training via reinforcement learning. arXiv preprint arXiv:1907.05507 
2019.

[36] Joao Sedoc  Daphne Ippolito  Arun Kirubarajan  Jai Thirani  Lyle Ungar  and Chris Callison-
Burch. Chateval: A tool for chatbot evaluation. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics (Demonstrations) 
pages 60‚Äì65  2019.

[37] Graham D Bodie  Kellie St. Cyr  Michelle Pence  Michael Rold  and James Honeycutt. Listening
competence in initial interactions i: Distinguishing between what listening is and what listeners
do. International Journal of Listening  26(1):1‚Äì28  2012.

[38] Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A
new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the
2nd Workshop on Cognitive Modeling and Computational Linguistics  pages 76‚Äì87. Association
for Computational Linguistics  2011.

[39] Joseph L Fleiss  Jacob Cohen  and Brian S Everitt. Large sample standard errors of kappa and

weighted kappa. Psychological Bulletin  72(5):323  1969.

[40] Kalyanmoy Deb. Multi-objective optimization. In Search methodologies  pages 403‚Äì449.

Springer  2014.

[41] Abdelrhman Saleh  Natasha Jaques  Asma Ghandeharioun  Judy Hanwen Shen  and Ros-
alind Picard. Hierarchical reinforcement learning for open-domain dialog. arXiv preprint
arXiv:1909.07547  2019.

[42] Jiwei Li  Will Monroe  Tianlin Shi  S√©bastien Jean  Alan Ritter  and Dan Jurafsky. Adversarial
learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing  pages 2157‚Äì2169  2017.

[43] Natasha Jaques  Asma Ghandeharioun  Judy Hanwen Shen  Craig Ferguson  Agata Lapedriza 
Noah Jones  Shixiang Gu  and Rosalind Picard. Way off-policy batch deep reinforcement
learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456  2019.

12

,Asma Ghandeharioun
Judy Hanwen Shen
Natasha Jaques
Craig Ferguson
Noah Jones
Agata Lapedriza
Rosalind Picard