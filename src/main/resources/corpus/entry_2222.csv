2011,Gaussian Process Training with Input Noise,In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme  which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods.,Gaussian Process Training with Input Noise

Andrew McHutchon

Department of Engineering

Cambridge University
Cambridge  CB2 1PZ
ajm257@cam.ac.uk

Carl Edward Rasmussen
Department of Engineering

Cambridge University
Cambridge  CB2 1PZ
cer54@cam.ac.uk

Abstract

In standard Gaussian Process regression input locations are assumed to be noise
free. We present a simple yet effective GP model for training on input points cor-
rupted by i.i.d. Gaussian noise. To make computations tractable we use a local
linear expansion about each input point. This allows the input noise to be recast
as output noise proportional to the squared gradient of the GP posterior mean.
The input noise variances are inferred from the data as extra hyperparameters.
They are trained alongside other hyperparameters by the usual method of max-
imisation of the marginal likelihood. Training uses an iterative scheme  which
alternates between optimising the hyperparameters and calculating the posterior
gradient. Analytic predictive moments can then be found for Gaussian distributed
test points. We compare our model to others over a range of different regression
problems and show that it improves over current methods.

1

Introduction

Over the last decade the use of Gaussian Processes (GPs) as non-parametric regression models has
grown signiﬁcantly. They have been successfully used to learn mappings between inputs and outputs
in a wide variety of tasks. However  many authors have highlighted a limitation in the way GPs
handle noisy measurements. Standard GP regression [1] makes two assumptions about the noise
in datasets: ﬁrstly that measurements of input points  x  are noise-free  and  secondly  that output
points  y  are corrupted by constant-variance Gaussian noise. For some datasets this makes intuitive
sense: for example  an application in Rasmussen and Williams (2006) [1] is that of modelling CO2
concentration in the atmosphere over the last forty years. One can viably assume that the date is
available noise-free and the CO2 sensors are affected by signal-independent sensor noise.
However  in many datasets  either or both of these assumptions are not valid and lead to poor mod-
elling performance. In this paper we look at datasets where the input measurements  as well as the
output  are corrupted by noise. Unfortunately  in the GP framework  considering each input location
to be a distribution is intractable. If  as an approximation  we treat the input measurements as if they
were deterministic  and inﬂate the corresponding output variance to compensate  this leads to the
output noise variance varying across the input space  a feature often called heteroscedasticity. One
method for modelling datasets with input noise is  therefore  to hold the input measurements to be
deterministic and then use a heteroscedastic GP model. This approach has been strengthened by the
breadth of research published recently on extending GPs to heteroscedastic data.
However  referring the input noise to the output in this way results in heteroscedasticity with a very
particular structure. This structure can be exploited to improve upon current heteroscedastic GP
models for datasets with input noise. One can imagine that in regions where a process is changing
its output value rapidly  corrupted input measurements will have a much greater effect than in regions

Pre-conference version

1

where the output is almost constant. In other words  the effect of the input noise is related to the
gradient of the function mapping input to output. This is the intuition behind the model we propose
in this paper.
We ﬁt a local linear model to the GP posterior mean about each training point. The input noise vari-
ance can then be referred to the output  proportional to the square of the posterior mean function’s
gradient. This approach is particularly powerful in the case of time-series data where the output
at time t becomes the input at time t + 1. In this situation  input measurements are clearly not
noise-free: the noise on a particular measurement is the same whether it is considered an input or
output. By also assuming the inputs are noisy  our model is better able to ﬁt datasets of this type.
Furthermore  we can estimate the noise variance on each input dimension  which is often very useful
for analysis.
Related work lies in the ﬁeld of heteroscedastic GPs. A common approach to modelling changing
variance with a GP  as proposed by Goldberg et al. [2]  is to make the noise variance a random
variable and attempt to estimate its form at the same time as estimating the posterior mean. Goldberg
et al. suggested using a second GP to model the noise level as a function of the input location.
Kersting et al. [3] improved upon Goldberg et al.’s Monte Carlo training method with a “most likely”
training scheme and demonstrated its effectiveness; related work includes Yuan and Wahba [4]  and
Le at al. [5] who proposed a scheme to ﬁnd the variance via a maximum-a-posteriori estimate set
in the exponential family. Snelson and Ghahramani [6] suggest a different approach whereby the
importance of points in a pseudo-training set can be varied  allowing the posterior variance to vary
as well. Recently Wilson and Ghahramani broadened the scope still further and proposed Copula
and Wishart Process methods [7  8].
Although all of these methods could be applied to datasets with input noise  they are designed for a
more general class of heteroscedastic problems and so none of them exploits the structure inherent in
input noise datasets. Our model also has a further advantage in that training is by marginal likelihood
maximisation rather than by an approximate inference method  or one such as maximum likelihood 
which is more susceptible to overﬁtting. Dallaire et al. [9] train on Gaussian distributed input points
by calculating the expected the covariance matrix. However  their method requires prior knowledge
of the noise variance  rather than inferring it as we do in this paper.

2 The Model

In this section we formally derive our model  which we refer to as NIGP (noisy input GP).
Let x and y be a pair of measurements from a process  where x is a D dimensional input to the
process and y is the corresponding scalar output. In standard GP regression we assume that y is a
noisy measurement of the actual output of the process ˜y 

where  y ∼ N(cid:0)0  σ2

(cid:1). In our model  we further assume that the inputs are also noisy measurements

y = ˜y + y

(1)

y
of the actual input ˜x 
(2)
where x ∼ N (0  Σx). We assume that each input dimension is independently corrupted by noise 
thus Σx is diagonal. Under a model f (.)  we can write the output as a function of the input in the
following form 

x = ˜x + x

(3)
For a GP model the posterior distribution based on equation 3 is intractable. We therefore consider
a Taylor expansion about the latent state ˜x 

y = f (˜x + x) + y

f (˜x + x) = f (˜x) + T
x

∂f (˜x)

∂ ˜x

+ . . . (cid:39) f (x) + T

x

∂f (x)

∂x

+ . . .

(4)

We don’t have access to the latent variable ˜x so we approximate it with the noisy measurements.
Now the derivative of a Gaussian Process is another Gaussian Process [10]. Thus  the exact treatment
would require the consideration of a distribution over Taylor expansions. Although the resulting dis-
tribution is not Gaussian  its ﬁrst and second moments can be calculated analytically. However  these
calculations carry a high computational load and previous experiments showed this exact treatment

2

provided no signiﬁcant improvement over the much quicker approximate method we now describe.
Instead we take the derivative of the mean of the GP function  which we will denote ∂ ¯f   a D-
dimensional vector  for the derivative of one GP function value w.r.t. the D-dimensional input  and
∆ ¯f   an N by D matrix  for the derivative of N function values. Differentiating the mean function
corresponds to ignoring the uncertainty about the derivative. If we expand up to the ﬁrst order terms
we get a linear model for the input noise 

y = f (x) + T

x ∂ ¯f + y

(5)

The probability of an observation y is therefore 

P (y | f ) = N (f  σ2

y + ∂T

¯f Σx ∂ ¯f )

¯f }(cid:3)−1

¯f }(cid:3)−1

y

yI + diag{∆ ¯f Σx ∆T

(6)
We keep the usual Gaussian Process prior  P (f | X) = N (0  K(X  X))  where K(X  X) is the N
by N training data covariance matrix and X is an N by D matrix of input observations. Combining
these probabilities gives the predictive posterior mean and variance as 
yI + diag{∆ ¯f Σx ∆T

E [f∗ | X  y  x∗] = k(x∗  X)(cid:2)K(X  X) + σ2
V [f∗ | X  y  x∗] = k(x∗  x∗) − k(x∗  X)(cid:2)K(X  X) + σ2

k(X  x∗)
(7)
to treating the inputs as deterministic and adding a corrective term 
This is equivalent
¯f }  to the output noise. The notation “diag{.}” results in a diagonal matrix  the
diag{∆ ¯f Σx ∆T
elements of which are the diagonal elements of its matrix argument. Note that if the posterior mean
gradient is constant across the input space the heteroscedasticity is removed and our model is essen-
tially identical to a standard GP.
An advantage of our approach can be seen in the case of multiple output dimensions. As the input
noise levels are the same for each of the output dimensions  our model can use data from all of the
outputs when learning the input noise variances. Not only does this give more information about the
noise variances without needing further input measurements but it also reduces over-ﬁtting as the
learnt noise variances must agree with all E output dimensions.
For time-series datasets (where the model has to predict the next state given the current)  each
dimension’s input and output noise variance can be constrained to be the same since the noise level
on a measurement is independent of whether it is an input or output. This further constraint increases
the ability of the model to recover the actual noise variances. The model is thus ideally suited to the
common task of multivariate time series modelling.

3 Training

Our model introduces an extra D hyperparameters compared to the standard GP - one noise variance
hyperparameter per input dimension. A major advantage of our model is that these hyperparameters
can be trained alongside any others by maximisation of the marginal likelihood. This approach
automatically includes regularisation of the noise parameters and reduces the effect of over-ﬁtting.
In order to calculate the marginal likelihood of the training data we need the posterior distribution 
and the slope of its mean  at each of the training points. However  evaluating the posterior mean
from equation 7 with x∗ ∈ X  results in an analytically unsolvable differential equation: ¯f is a
complicated function of ∆ ¯f   its own derivative. Therefore  we deﬁne a two-step approach: ﬁrst we
evaluate a standard GP with the training data  using our initial hyperparameter settings and ignoring
the input noise. We then ﬁnd the slope of the posterior mean of this GP at each of the training points
¯f }. This process is summarised in
and use it to add in the corrective variance term  diag{∆ ¯f Σx ∆T
ﬁgures 1a and 1b.
The marginal likelihood of the GP with the corrected variance is then computed  along with its
derivatives with respect to the initial hyperparameters  which include the input noise variances. This
step involves chaining the derivatives of the marginal likelihood back through the slope calculation.
Gradient descent can then be used to improve the hyperparameters. Figure 1c shows the GP posterior
for the trained hyperparameters and shows how NIGP can reduce output noise level estimates by
taking input noise into account. Figure 1d shows the NIGP ﬁt for the trained hyperparameters.

3

Figure 1: Training with NIGP. (a) A standard GP posterior distribution can be computed from an
initial set of hyperparameters and a training data set  shown by the blue crosses. The gradients of the
posterior mean at each training point can then be found analytically. (b) The NIGP method increases
the posterior variance by the square of the posterior mean slope multiplied by the current setting of
the input noise variance hyperparameter. The marginal likelihood of this ﬁt is then calculated along
with its derivatives w.r.t. initial hyperparameter settings. Gradient descent is used to train the hyper-
parameters. (c) This plot shows the standard GP posterior using the newly trained hyperparameters.
Comparing to plot (a) shows that the output noise hyperparameter has been greatly reduced. (d) This
¯f }.
plot shows the NIGP ﬁt - plot(c) with the input noise corrective variance term  diag{∆ ¯f Σx ∆T
Plot (d) is related to plot (c) in the same way that plot (b) is related to plot (a).

To improve the ﬁt further we can iterate this procedure: we use the slopes of the current trained
NIGP  instead of a standard GP  to calculate the effect of the input noise  i.e. replace the ﬁt in ﬁgure
1a with the ﬁt from ﬁgure 1d and re-train.

4 Prediction

We turn now to the task of making predictions at noisy input locations with our model. To be true to
our model we must use the same process in making predictions as we did in training. We therefore
use the trained hyperparameters and the training data to deﬁne a GP posterior mean  which we
differentiate at each test point and each training point. The calculated gradients are then used to add
in the corrective variance terms. The posterior mean slope at the test points is only used to calculate
the variance over observations  where we increase the predictive variance by the noise variances.
There is an alternative option  however.
If a single test point is considered to have a Gaussian
distribution and all the training points are certain then  although the GP posterior is unknown  its
mean and variance can be calculated exactly [11]. As our model estimates the input noise variance
∗ ∼ N (x∗  Σx).
Σx during training  we can consider a test point to be Gaussian distributed: x(cid:48)
[11] then gives the mean and variance of the posterior distribution  for a squared exponential kernel
(equation 12)  to be 

yI + Σx∂ ¯f

y

q

(8)

(cid:16)(cid:2)K + σ2

¯f∗ =

2(cid:3)−1

(cid:17)T

4

−10123456Targeta) Initial hyperparameters & trainingdata define a GP fitb) Extra variance added proportionalto squared slope0123456−10123456InputTargetc) Standard GP with NIGP trainedhyperparameters0123456Inputd) The NIGP fit including variancefrom input noisewhere 

(xi − x∗)T (Σx + Λ)
where Λ is a diagonal matrix of the squared lengthscale hyperparameters.

qi = σ2
f

(cid:12)(cid:12)ΣxΛ−1 + I(cid:12)(cid:12)− 1

V [f∗] = σ2

f − tr

2

2 exp(cid:0) − 1
(cid:16)(cid:2)K + σ2
(cid:16)

exp

yI + Σx∂ ¯f

2(cid:3)−1
(z − x∗)T(cid:0)Λ +

(cid:17)

with 

Qij =

k(xi  x∗)k(xj  x∗)
|2ΣxΛ−1 + I| 1

2

−1 (xi − x∗)(cid:1)

+ αT Qα − ¯f 2∗

Q

x Λ(cid:1)−1

ΛΣ−1

1
2

(z − x∗)

(cid:17)

(9)

(10)

(11)

with z = 1
2 (xi+xj). This method is computationally slower than using equation 7 and is vulnerable
to worse results if the learnt input noise variance Σx is very different from the true value. However 
it gives proper consideration to the uncertainty surrounding the test point and exactly computes the
moments of the correct posterior distribution. This often leads it to outperform predictions based on
equation 7.

5 Results

We tested our model on a variety of functions and datasets  comparing its performance to stan-
dard GP regression as well as Kersting et al.’s ‘most likely heteroscedastic GP’ (MLHGP) model  a
state-of-the-art heteroscedastic GP model. We used the squared exponential kernel with Automatic
Relevance Determination 

f exp(cid:0) − 1

(xi − xj)T Λ−1(xi − xj)(cid:1)

2

k(xi  xj) = σ2

(12)

where Λ is a diagonal matrix of the squared lengthscale hyperparameters and σ2
hyperparameter. Code to run NIGP is available on the author’s website.

f is a signal variance

Standard GP

Kersting et al.

This paper

Figure 2: Posterior distribution for a near-square wave with σy = 0.05  σx = 0.3  and 60 data points.
The solid line represents the predictive mean and the dashed lines are two standard deviations either
side. Also shown are the training points and the underlying function. The left image is for standard
GP regression  the middle uses Kersting et al.’s MLHGP algorithm  the right image shows our model.
While the predictive means are similar  both our model and MLHGP pinch in the variance around the
low noise areas. Our model correctly expands the variance around all steep areas whereas MLHGP
can only do so where high noise is observed (see areas around x= -6 and x = 1).

Figure 2 shows an example comparison between standard GP regression  Kersting et al.’s MLHGP 
and our model for a simple near-square wave function. This function was chosen as it has areas

5

−10−50510−1.5−1−0.500.511.5−10−50510−1.5−1−0.500.511.5−10−50510−1.5−1−0.500.511.5of steep gradient and near ﬂat gradient and thus suffers from the heteroscedastic problems we are
trying to solve. The posterior means are very similar for the three models  however the variances
are quite different. The standard GP model has to take into account the large noise seen around the
steep sloped areas by assuming large noise everywhere  which leads to the much larger error bars.
Our model can recover the actual noise levels by taking the input noise into account. Both our model
and MLHGP pinch the variance in around the ﬂat regions of the function and expand it around the
steep areas. For the example shown in ﬁgure 2 the standard GP estimated an output noise standard
deviation of 0.16 (much too large) compared to our estimate of 0.052  which is very close to the
correct value of 0.050. Our model also learnt an input noise standard deviation of 0.305  very close
to the real value of 0.300. MLHGP does not produce a single estimate of noise levels.
Predictions for 1000 noisy measurements were made using each of the models and the log proba-
bility of the test set was calculated. The standard GP model had a log probability per data point of
0.419  MLHGP 0.740  and our model 0.885  a signiﬁcant improvement. Part of the reason for our
improvement over MLHGP can be seen around x = 1: our model has near-symmetric ‘horns’ in
the variance around the corners of the square wave  whereas MLHGP only has one ‘horn’. This is
because in our model  the amount of noise expected is proportional to the derivative of the mean
squared  which is the same for both sides of the square wave. In Kersting et al.’s model the noise
is estimated from the training points themselves. In this example the training points around x = 1
happen to have low noise and so the learnt variance is smaller. The same problem can be seen around
x = −6 where MLHGP has much too small variance. This illustrates an important aspect of our
model: the accuracy in plotting the varying effect of noise is only dependent on the accuracy of the
mean posterior function and not on an extra  learnt noise model. This means that our model typically
requires fewer data points to achieve the same accuracy as MLHGP on input noise datasets. To test
the models further  we trained them on a suite of six functions. The functions were again chosen
to have varying gradients across the input space. The training set consisted of twenty ﬁve points in
the interval [-10  10] and the test set one thousand points in the same interval. Trials were run for
different levels of input noise. For each trial  ten different initialisations of the hyperparameters were
tried. In order to remove initialisation effects the best initialisations for each model were chosen at
each step. The entire experiment was run on twenty different random seeds. For our model  NIGP 
we trained both a single model for all output dimensions  as well as separate models for each of the
outputs  to see what the effect of using the cross-dimension information was.
Figure 3 shows the results for this experiment. The ﬁgure shows that NIGP performs very well on
all the functions  always outperforming the standard GP when there is input noise and nearly always
MLHGP; wherever there is a signiﬁcant difference our model is favoured. Training on all the outputs
at once only gives an improvement for some of the functions  which suggests that  for the others 
the input noise levels could be estimated from the individual functions alone. The predictions using
stochastic test-points  equations 8 and 10  generally outperformed the predictions made using deter-
ministic test-points  equation 7. The RMSEs are quite similar to each other for most of the functions
as the posterior means are very similar  although where they do differ signiﬁcantly  again  it is to
favour our model. These results show our model consistently calculates a more accurate predictive
posterior variance than either a standard GP or a state-of-the-art heteroscedastic GP model.
As previously mentioned  our model can be adapted to work more effectively with time-series data 
where the outputs become subsequent inputs. In this situation the input and output noise variance
will be the same. We therefore combine these two parameters into one. We tested NIGP on a time-
series dataset and compared the two modes (with separate input and output noise hyperparameters
and with combined) and also to standard GP regression (MLHGP was not available for multiple
input dimensions). The dataset is a simulated pendulum without friction and with added noise.
There are two variables: pendulum angle and angular velocity. The choice of time interval between
observations is important: for very small time intervals  and hence small changes in the angle  the
dynamics are approximately linear  as sin θ ≈ θ. As discussed before  our model will not bring
any beneﬁt to linear dynamics  so in order to see the difference in performance a much longer time
interval was chosen. The range of initial angular velocities was chosen to allow the pendulum to
spin multiple times at the extremes  which adds extra non-linearity. Ten different initialisations
were tried  with the one achieving the highest training set marginal likelihood chosen  and the whole
experiment was repeated ﬁfty times with different random seeds.
The plots show the difference in log probability of the test set between four versions of NIGP and a
standard GP model trained on the same data. All four versions of our model perform better than the

6

Figure 3: Comparison of models for suite of 6 test functions. The solid line is our model with
‘deterministic test-point’ predictions  the solid line with triangles is our model with ‘stochastic test-
point’ predictions. Both these models were trained on all 6 functions at once  the respective dashed
lines were trained on the functions individually. The dash-dot line is a standard GP regression model
and the dotted line is MLHGP. RMSE has been normalised by the RMS value of the function. In both
plots lower values indicate better performance. The plots show our model has lower negative log
posterior predictive than standard GP on all the functions  particularly the exponentially decaying
sine wave and the multiplication between tan and sin.

standard GP. Once again the stochastic test point version outperforms the deterministic test points.
There was a slight improvement in RMSE using our model but the differences were within two
standard deviations of each other. There is also a slight improvement using the combined noise
levels although  again  the difference is contained within the error bars.
A better comparison between the two modes is to look at the input noise variance values recovered.
The real noise standard deviations used were 0.2 and 0.4 for the angle and angular velocity respec-
tively. The model which learnt the variances separately found standard deviations of 0.3265 and
0.8026 averaged over the trials  whereas the combined model found 0.2429 and 0.8948. This is a
signiﬁcant improvement on the ﬁrst dimension. Both modes struggle to recover the correct noise
level on the second dimension and this is probably why the angular velocity prediction performance
shown in ﬁgure 4 is worse than the angle prediction performance. Training with more data signif-

7

00.10.20.30.40.50.60.70.80.91−2−1.5−1−0.500.51Negative log predictive posteriorsin(x)  00.10.20.30.40.50.60.70.80.91−1.5−1−0.500.5Near−square wave  00.10.20.30.40.50.60.70.80.91−1.5−1−0.500.511.52exp(−0.2*x)*sin(x)  00.10.20.30.40.50.60.70.80.91−1−0.500.511.52Input noise standard deviationNegative log predictive posteriortan(0.15*(x))*sin(x)  00.10.20.30.40.50.60.70.80.9100.511.522.533.54Input noise standard deviation0.2*x2*tanh(cos(x))  00.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.6Input noise standard deviation0.5*log(x2*(sin(2*x)+2)+1)  NIGP DTP all o/pNIGP DTP indiv. o/pNIGP STP indiv. o/pNIGP STP all o/pKersting et al.Standard GP00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91Normalised test set RMSEsin(x)00.10.20.30.40.50.60.70.80.910.050.10.150.20.250.30.350.40.450.50.55Near−square wave00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91exp(−0.2*x)*sin(x)00.10.20.30.40.50.60.70.80.910.10.20.30.40.50.60.70.80.911.1Input noise standard deviationNormalised test set RMSEtan(0.15*(x))*sin(x)00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.8Input noise standard deviation0.2*x2*tanh(cos(x))00.10.20.30.40.50.60.70.80.910.020.040.060.080.10.120.140.160.180.20.22Input noise standard deviation0.5*log(x2*(sin(2*x)+2)+1)Figure 4: The difference between four versions of NIGP and a standard GP model on a pendulum
prediction task. DTP stands for deterministic test point and STP is stochastic test point. Comb. and
sep. indicate whether the model combined the input and output noise parameters or treated them
separately. The error bars indicate plus/minus two standard deviations.

icantly improved the recovered noise value although the difference between the two NIGP modes
then shrank as there was sufﬁcient information to correctly deduce the noise levels separately.

6 Conclusion

The correct way of training on input points corrupted by Gaussian noise is to consider every input
point as a Gaussian distribution. This model is intractable  however  and so approximations must
be made. In our model  we refer the input noise to the output by passing it through a local linear
expansion. This adds a term to the likelihood which is proportional to the squared posterior mean
gradient. Not only does this lead to tractable computations but it makes intuitive sense - input
noise has a larger effect in areas where the function is changing its output rapidly. The model 
although simple in its approach  has been shown to be very effective  outperforming Kersting et
al.’s model and a standard GP model in a variety of different regression tasks. It can make use of
multiple outputs and can recover a noise variance parameter for each input dimension  which is
often useful for analysis. In our approximate model  exact inference can be performed as the model
hyperparameters can be trained simultaneously by marginal likelihood maximisation.
A proper handling of time-series data would constrain the speciﬁc noise levels on each training point
to be the same for when they are considered inputs and outputs. This would be computationally very
expensive however. By allowing input noise and ﬁxing the input and output noise variances to be
identical  our model is a computationally efﬁcient alternative. Our results showed that NIGP gives a
substantial improvement over the often-used standard GP for modelling time-series data.
It is important to state that this model has been designed to tackle a particular situation  that of
constant-variance input noise  and would not perform so well on a general heteroscedastic prob-
lem. It could not be expected to improve over a standard GP on problems where noise levels are
proportional to the function or input value for example. We do not see this limitation as too re-
stricting however  as we maintain that constant input noise situations (including those where this is
a sufﬁcient approximation) are reasonably common. Throughout the paper we have taken particular
care to avoid functions or systems which are linear  or approximately linear  as in these cases our
model can be reduced to standard GP regression. However  for the problems for which NIGP has
been designed  such as the various non-linear problems we have presented in this paper  our model
outperforms current methods.
This paper considers a ﬁrst order Taylor expansion of the posterior mean function. We would expect
this to be a good approximation for any function providing the input noise levels are not too large
(i.e. small perturbations around the point we linearised about). In practice  we could require that
the input noise level is not larger than the input characteristic length scale. A more accurate model
could use a second order Taylor series  which would still be analytic although computationally
the algorithm would then scale with D3 rather than the current D2. Another reﬁnement could be
achieved by doing a Taylor series for the full posterior distribution (not just its mean  as we have
done here)  again at considerably higher computational cost. These are interesting areas for future
research  which we are actively pursuing.

8

References
[1] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine

Learning. MIT Press  2006.

[2] Paul W. Goldberg  Christopher K. I. Williams  and Christopher M. Bishop. Regression with

input-dependent noise: A Gaussian Process treatment. NIPS-98  1998.

[3] Kristian Kersting  Christian Plagemann  Patrick Pfaff  and Wolfram Burgard. Most likely

heteroscedastic Gaussian Process regression. ICML-07  2007.

[4] Ming Yuan and Grace Wahba. Doubly penalized likelihood estimator in heteroscedastic re-

gression. Statistics and Probability Letter  69:11–20  2004.

[5] Quoc V. Le  Alex J. Smola  and Stephane Canu. Heteroscedastic Gaussian Process regression.

Procedings of ICML-05  pages 489–496  2005.

[6] Edward Snelson and Zoubin Ghahramani. Variable noise and dimensionality reduction for

sparse gaussian processes. Procedings of UAI-06  2006.

[7] A.G. Wilson and Z. Ghahramani. Copula processes. In J. Lafferty  C. K. I. Williams  J. Shawe-
Taylor  R.S. Zemel  and A. Culotta  editors  Advances in Neural Information Processing Sys-
tems 23  pages 2460–2468. 2010.

[8] Andrew Wilson and Zoubin Ghahramani. Generalised Wishart Processes. In Proceedings of
the Twenty-Seventh Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence
(UAI-11)  pages 736–744  Corvallis  Oregon  2011. AUAI Press.

[9] P. Dallaire  C. Besse  and B. Chaib-draa. Learning Gaussian Process Models from Uncertain

Data. 16th International Conference on Neural Information Processing  2008.

[10] E. Solak  R. Murray-Smith  W.e. Leithead  D.J. Leith  and C.E. Rasmussen. Derivative obser-

vations in Gaussian Process models of dynamic systems. NIPS-03  pages 1033–1040  2003.

[11] Agathe Girard  Carl Edward Rasmussen  Joaquin Quinonero Candela  and Roderick Murray-
Smith. Gaussian Process priors with incertain inputs - application to multiple-step ahead time
series forecasting. Advances in Neural Information Processing Systems 16  2003.

9

,Sebastian Tschiatschek
Rishabh Iyer
Jeff Bilmes
Alaa Saade
Florent Krzakala
Lenka Zdeborová
Qinshi Wang
Wei Chen