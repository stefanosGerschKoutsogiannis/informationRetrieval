2019,A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning,Minimizing  a non-smooth function over the Grassmannian appears in many applications in machine learning. In this paper we show that if the objective satisfies a certain Riemannian regularity condition with respect to some point in the Grassmannian  then a Riemannian subgradient method with appropriate initialization and geometrically diminishing step size converges at a linear rate to that point. We show that for both the robust subspace learning method Dual Principal Component Pursuit (DPCP) and the Orthogonal Dictionary Learning (ODL) problem  the Riemannian regularity condition is satisfied with respect to appropriate points of interest  namely the subspace orthogonal to the sought subspace for DPCP and the orthonormal dictionary atoms for ODL. Consequently  we obtain in a unified framework significant improvements for the convergence theory of both methods.,A Linearly Convergent Method for Non-Smooth Non-Convex

Optimization on the Grassmannian with Applications to

Robust Subspace and Dictionary Learning

Zhihui Zhu

MINDS

Tianyu Ding

AMS

Johns Hopkins University

Johns Hopkins University

ShanghaiTech University

zzhu29@jhu.edu

tding1@jhu.edu

mtsakiris@shanghaitech.edu.cn

Manolis C. Tsakiris

SIST

Daniel P. Robinson

ISE

Lehigh University

daniel.p.robinson@lehigh.edu

René Vidal

MINDS

Johns Hopkins University

rvidal@jhu.edu

Abstract

Minimizing a non-smooth function over the Grassmannian appears in many appli-
cations in machine learning. In this paper we show that if the objective satisﬁes a
certain Riemannian regularity condition (RRC) with respect to some point in the
Grassmannian  then a projected Riemannian subgradient method with appropriate
initialization and geometrically diminishing step size converges at a linear rate
to that point. We show that for both the robust subspace learning method Dual
Principal Component Pursuit (DPCP) and the Orthogonal Dictionary Learning
(ODL) problem  the RRC is satisﬁed with respect to appropriate points of interest 
namely the subspace orthogonal to the sought subspace for DPCP and the orthonor-
mal dictionary atoms for ODL. Consequently  we obtain in a uniﬁed framework
signiﬁcant improvements for the convergence theory of both methods.

1

Introduction

Optimization problems on the Grassmannian G(c  D) (a.k.a. the Grassmann manifold that consists of
the set of linear c-dimensional subspaces in RD)  such as principal component analysis (PCA)  appear
in a wide variety of applications including subspace tracking [3]  system identiﬁcation [43]  action
recognition [36]  object categorization [20]  dictionary learning [34  39]  robust subspace recovery
[26  42]  subspace clustering [41]  and blind deconvolution [50]. However  a key challenge is that the
associated optimization problems are often non-convex since the Grassmannian is a non-convex set.
One approach to solving optimization problems on the Grassmanian is to exploit the fact that the
Grassmannian is a Riemannian manifold and develop generic Riemannian optimization techniques.
When the objective function is twice differentiable  [4] shows that Riemannian gradient descent and
Riemannian trust-region methods converge to ﬁrst- and second-order stationary solutions  respectively.
When Riemannian gradient descent is randomly initialized  [23] further shows that it converges to a
second-order stationary solution almost surely  but without any guarantee on the convergence rate.
Non-smooth trust region algorithms [19]  gradient sampling methods [9  8]  and proximal gradient
methods [7] have also been proposed for non-smooth manifold optimization when the objective
function is not continuously differentiable. However  the available theoretical results establish

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

convergence to stationary points from an arbitrary initialization with either no rate of convergence
guarantee  or at best a sublinear rate.1
On the other hand  when the constraint set is convex  [11  10  27] show that subgradient methods
can handle non-smooth and non-convex objective functions as long as the problem satisﬁes certain
regularity conditions called sharpness and weak convexity. In such a case  R-linear convergence1 is
guaranteed (e.g.  see robust phase retrieval [13] and robust low-rank matrix recovery [27]). Analogous
to other regularity conditions for smooth problems  such as the regularity condition of [6] and the error
bound condition in [29]  sharpness and weak convexity capture regularity properties of non-convex
and non-smooth optimization problems. However  these two properties have not yet been exploited
for solving problems on the Grassmannian  or other non-convex manifolds.
A related regularity condition  which in this paper is called the Riemannian Regularity Condition
(RRC)  has already been exploited for orthogonal dictionary learning (ODL) [2]  which solves an
(cid:96)1 minimization problem on the sphere  a manifold parameterizing G(1  D). However  under this
RRC  projected Riemannian subgradient methods have only been proved to converge at a sublinear
rate. On the other hand  a projected subgradient method has been successfully used and proved to
converge at a piecewise linear rate for Dual Principal Component Pursuit (DPCP) [42  51]  a method
that ﬁts a linear subspace to data corrupted by outliers. However  i) the convergence analysis does not
reveal where the improvement in the convergence rate comes from and ii) is restricted to optimization
on the sphere (G(1  D)) even for subspaces of codimension higher than one.
In this paper we make the following speciﬁc contributions:
• In Theorem 1 we prove that the projected Riemannian subgradient method for the Grassmannian
(Algorithm 1)  with an appropriate initialization and geometrically diminishing step size  converges
to a point of interest at an R-linear rate if the problem satisﬁes the RRC (Deﬁnition 1). The
RRC characterizes a local geometric property of the Grassmannian-constrained non-convex and
non-smooth problem relative to a point of interest. Informally  the RRC requires that  in the
neighborhood of the point of interest  the negative of the Riemannian subgradient should have a
sufﬁciently small angle with the direction pointing toward the point of interest (Figure 1).

• We prove that the optimization problem associated with DPCP satisﬁes the RRC  which allows us
to apply our new result and conclude that the projected Riemannian subgradient method converges
at an R-linear rate to a basis for the orthogonal complement of the underlying (D − c)-dimensional
linear subspace. This is the ﬁrst result to extend previous guarantees [42  51  12] from codimension
1 to higher codimensions  enabling us to efﬁciently ﬁnd the entire orthogonal basis by solving
the learning problem directly on G(c  D)  as opposed to the less efﬁcient approach of solving a
sequence of c problems on G(1  D)[42]. Even for subspaces of codimension 1 (i.e.  hyperplanes) 
our result improves upon [51] by allowing for (i) a much simpler step size selection strategy that
requires little ﬁne-tuning  and (ii) a weaker condition on the required initialization.

• Together with the already established RRC for ODL in [2]  our new result implies that the projected
Riemannian subgradient method converges at an R-linear rate to atoms of the underlying dictionary 
thus improving upon [2]  which established only a sublinear convergence rate.

2 Background and Notation

In this paper  we consider minimization problems on the Grassmannian G(c  D). For computations  it
is desirable to parameterize points on the Grassmannian. An element of G(c  D) can be represented by
an orthonormal matrix in O(c  D) =: {B ∈ RD×c : B
B = Ic}  which is the well-known Stiefel
manifold. When D = c  we denote O(c  c) by O(c)  the orthogonal group. This matrix representation
is not unique since Span(BQ) = Span(B) for any Q ∈ O(c). Thus  we say A ∈ G(c  D) is
equivalent to B if Span(A) = Span(B). With this understanding  we use B to represent the
equivalence class [B] = {BQ : Q ∈ O(c)} and consider the parameterized problem [14  20]

(cid:62)

minimize
B∈O(c D)

f (B) 

(1)

1Suppose the sequence {xk} converges to x(cid:63). We say it converges sublinearly if limk→∞ (cid:107)xk+1 −
x(cid:63)(cid:107)/(cid:107)xk − x(cid:63)(cid:107) = 1  and R-linearly if there exists C > 0  q ∈ (0  1) such that (cid:107)xk − x(cid:63)(cid:107) ≤ Cqk  ∀k ≥ 0.

2

where f : RD×c → R is locally Lipschitz  possibly non-convex and non-smooth  and invariant to the
action of O(c)  i.e.  f (B) = f (BQ) for any Q ∈ O(c). Again  the global minimum of (1) is not
unique as if B(cid:63) is a global minimum  then any point in [B(cid:63)] is also a global minimum.
For any A  B ∈ O(c  D)  the principal angles between Span(A) and Span(B) are deﬁned as [38]
B)) for i = 1  . . .   c  where σi(·) denotes the i-th singular value. We
φi(A  B) = arccos(σi(A
then deﬁne the distance between A and B as

(cid:62)

(cid:118)(cid:117)(cid:117)(cid:116)2

c(cid:88)

i=1

(cid:0)1 − cos(φi(A  B))(cid:1) = min

Q∈O(c)(cid:107)B − AQ(cid:107)F  

dist(A  B) :=

(2)

(3)

where the last term is also known as the orthogonal Procrustes problem. The last equality in (2)
follows from the result [21] according to which the optimal rotation matrix Q minimizing (cid:107)B −
(cid:62)
(cid:62) is the SVD of A
B. Thus  dist(A  B) = 0 iff Span(A) =
AQ(cid:107)F is Q(cid:63) = U V
Span(B). We also deﬁne the projection of B onto [A] as

(cid:62)  where U ΣV

PA(B) = AQ(cid:63)  where Q(cid:63) = arg min

Q∈O(c) (cid:107)B − AQ(cid:107)F .
(cid:62)

Here  AQ(cid:63) is in [A]  with Q(cid:63) representing a nonlinear transformation of A
B  as described above.
Since f can be non-smooth and non-convex  we utilize the Clarke subdifferential  which generalizes
the gradient for smooth functions and the subdifferential in convex analysis. The Clarke subdifferential
of a locally Lipschitz function f at B is deﬁned as [2]

(cid:110)
i→∞∇f (Bi) : Bi → B  f differentiable at Bi
lim

(cid:111)

 

∂f (B) := conv

where conv denotes the convex hull. When f is differentiable at B  its Clarke subdifferential is
simply {∇f (B)}. When f is not differentiable at B  the Clarke subdifferential is the convex hull of
the limit of gradients taken at differentiable points. Note that the Clarke subdifferential ∂f (B) is a
nonempty and convex set since a locally Lipschitz function is differentiable almost everywhere.
Since we consider problems on the Grassmannian  we use tools from Riemannian geometry to
state optimality conditions. From [14]  the tangent space of the Grassmannian at [B] is deﬁned
as TB := {W ∈ RD×c : W
B = 0}  and the orthogonal projector onto the tangent space is
for any A ∈ [B]. We generalize the deﬁnition of the Clarke subdifferential and denote by(cid:101)∂f the
(cid:62)
I − BB
= BB
(cid:101)∂f (B) := conv
Riemannian subdifferential of f [2]:
(cid:62)
We say that B is a critical point of (1) if and only if 0 ∈ (cid:101)∂f (B)  which is a necessary condition for

(cid:111)
)∇f (Bi) : Bi → B  f differentiable at Bi ∈ O(c  D)

(cid:62)  which is well-deﬁned and does not depend on the class representative as AA

i→∞(I − BB
lim

(cid:110)

.

(4)

(cid:62)

(cid:62)

being a minimizer to (1).

3 Projected Riemannian Subgradient Method

In this section  we state our key Riemannian regularitity condition (RRC §3.1)  propose a projected
Riemannian subgradient method (§3.2) based on RRC  and analyze its convergence properties (§3.3).

(α    B(cid:63))-Riemannian Regularity Condition (RRC)

3.1
Deﬁnition 1. We say that f : RD×c → R satisﬁes the (α    B(cid:63))-Riemannian regularity condi-
dist(B  B(cid:63)) ≤   there exists a Riemannian subgradient G(B) ∈ (cid:101)∂f (B) such that
tion (RRC)2 for parameters {α  } > 0 and B(cid:63) ∈ O(c  D)  if for every B ∈ O(c  D) satisfying

(cid:104)PB(cid:63) (B) − B −G(B)(cid:105) ≥ α dist(B  B(cid:63)).

(5)

2Strictly speaking  Deﬁnition 1 is extrinsic since we view the Grassmannian as embedded in the Euclidean

space and (5) involves the standard inner product in the Euclidean space.

3

Recently  a particular instance of Deﬁnition 1 was shown
to hold [2] in the context of ODL (see §4.2). We note
G(B) ∈ (cid:101)∂f (B)  and that the set of allowable Riemannian
that −G(B) is not necessarily a descent direction for all
norm element from (cid:101)∂f (B) even though that one is known
subgradients that satisfy (5) need not include the minimum

ξ := sup{(cid:107)G(B)(cid:107)F : dist(B  B(cid:63)) ≤ }

to be a descent direction [18]. In §4  we show that a natu-
ral choice of Riemannian subgradient satisﬁes (5) for DPCP
and ODL  where B(cid:63) is a target solution. As illustrated in
Figure 1  condition (5) implies that the negative of the cho-
sen Riemannian subgradient G(B) has small angle to the
Figure 1: Illustration of Deﬁnition 1.
direction PB(cid:63) (B) − B. To see this  let
Red nodes denote [B(cid:63)]  with the top
one closest to B. Inequality (5) re-
(6)
quires the angle between PB(cid:63) (B) −
denote an upper bound on the size of the Riemannian sub-
B (purple arrow) and −G(B) (blue
gradients in a neighbrohood of B(cid:63). Assume ξ < ∞.
arrow) to be sufﬁciently small.
From (5) we have (cid:104)PB(cid:63) (B) − B −G(B)(cid:105)/(cid:107)PB(cid:63) (B) −
B(cid:107)F(cid:107)G(B)(cid:107)F ≥ α/ξ  which gives a bound on the sum of the cosines of the principal angles
between PB(cid:63) (B) − B and −G(B) and implies that ξ ≥ α.
In §3.3 we prove that if the (α    B(cid:63))-RRC holds  then a projected Riemannian subgradient method
will converge to B(cid:63) when an appropriate initialization and step size strategy are used.
Deﬁnition 1 is similar in nature to other regularity conditions that characterize geometric properties of
the objective function. Perhaps the most closely related ones for non-smooth functions are sharpness
and weak convexity. Consider a function h : RD → R and assume that the set
of global minima of h is non-empty. Then  h is said to be sharp with parameter ν > 0 (see [5]) if

X := {z ∈ RD : h(z) ≤ h(x) for all x ∈ Rn}

h(x) − min
z∈RD

h(z) ≥ ν dist(x X )

(7)
holds for all x ∈ RD. The function h is said to be weakly convex with parameter τ ≥ 0 if
2(cid:107)x(cid:107)2 is convex [44]. If h is both sharp and weakly convex  then [10  27] show that
x (cid:55)→ h(x) + τ
(8)
for any x ∈ RD and any d ∈ ∂h(x)  where PX is the orthogonal projector onto the set X . Note that
(8) is useful when its right-hand side is nonnegative  i.e.  when dist(x X ) ≤ (2ν)/τ. Thus  for any
 < (2ν)/τ  we have
(9)
whenever x satisﬁes dist(x X ) ≤ . Noting the similarity between (9) and (5) (B(cid:63) can be taken as a
minimizer of h)  the RRC (5) can be viewed as a generalization of (9) (the consequence of sharpness
and weak convexity) to the Riemannian manifold. There are two main differences. First  (5) differs
from (9) in that its left-hand side involves the Riemannian subgradient due to the Grassmannian
constraint. Second  (5) is only required to hold for a particular Riemannian subgradient at B  while
(9) holds for all subgradients  thus imposing a slightly stronger regularity condition on the problem.

(cid:104)PX (x) − x  d(cid:105) ≥ ν dist(x X ) − τ
(cid:104)PX (x) − x  d(cid:105) ≥(cid:0)ν − τ

2 (cid:1) dist(x X ) for all d ∈ ∂h(x)

2 dist2(x X )

3.2 Projected Riemannian Subgradient Method on the Grassmannian

We propose to solve (1) using the projected Riemannian subgradient method in Algorithm 1. Given
the kth iterate Bk  the next iterate Bk+1 is obtained by ﬁrst moving in a direction opposite to a
Riemannian subgradient at Bk that satisﬁes the regularity condition in (5)  and then performing
orthonormalization. In Section 4  we will show that such a projected Riemannian subgradient can

be easily computed for ODL and DPCP. Note that (cid:98)Bk+1 in (10) always has full column rank since
multiple ways to orthonormalize (cid:98)Bk+1  although for our purpose they are all equivalent since they all
G(Bk) is orthogonal to Bk; see the supplementary material for a formal proof. Also  there are
for Span((cid:98)Bk+1). For example  one can compute Bk+1 to be the Gram-Schmidt orthonormalization
correspond to the same subspace. In (10)  orth refers to any method that ﬁnds an orthonormal basis
of (cid:98)Bk+1  or as the ﬁrst c left singular vectors of (cid:98)Bk+1. Finally  note that no speciﬁc step size rule is

provided in Algorithm 1  whereas speciﬁc choices are made for the convergence analysis in §3.3.

4

B<latexit sha1_base64="hyAWEBfAdvzJDhSRMMd5Vr0dovI=">AAAmDnicrZpLc9y4EYBnN6+18/Imx1xYUVTlddlajTabVKW2tmxLsiRbj9FblulygSSGQ4sgaJJDzZjF/INU5ZT8k9xSueYv5IfkngbYnCHR0CaHzEEi+2uAQKPR3eCMl8ZRXqyv/+uTT7/3/R/88Eef3bv/45/89Gc/f/D5Ly5yOc18fu7LWGZXHst5HCX8vIiKmF+lGWfCi/mld7Op+GXJszySyVkxT/lbwcIkGkc+K0B06orn7x6srK+t649DL4Z4sTLAz+jd57/5txtIfyp4Uvgxy/M3w/W0eFuxrIj8mNf33WnOU+bfsJBX0yzuCd5MokAN9iZ/W01gPFnGx/0WTOSCFZPH8L+YCPUvnwtP/ffy+WNP9LU9U5CyjCm79aUzbam+zCIKM5ZOIn8GUrzMUxhLVa19OY7C/MvaGGocyiyCUd4hjnzziUIZzRCyVC1FX5hPPat8zBJ/PgnMmUQFmH3VFOUyM58Vc3AG0+I8mQpQh1ncdwM+Bk/SpqkClt2k8jbgmRdPeV1loVdX4BmPwT021J/fwUMrt+CzwtrAUR9Qcc4mXGZc5Lp7x4VLUMydyk1klARgEbj0xs5JI1+rXceFdm7Cb4umZaVsXGE39eqbnPvKNm+d1Yd7Y2cup84tg15Q24H5eDDNoNdFzIVgdbWv/3W7WFXDvIX1clCG7dd6zX3wlWpTZjKOWTbvdNDTSjOZ1tUI/so80st3h6K2c6NRbS2vv0P7f9GDx3v68bD9Rd+CsE+nYMNn6l/rWEv8qAoli6FpzBLT8o3RF9Y3jdqa9L6LwryYx7xqFrmvjbLqpGW9Fqlt2K1wOSnwp3HE4wC8KeOgqTYVS4LKPaorV0UOz6uO6qaXBTtZshPF0A9ZrNa+cpbM6cL8LniiWvZ77D5tc/m0zcXTgKYxn1nIZkOcPup2eL3s8Npkh0t2WKst3IUxeLwb83EBV0mog3MXZwpnUTgp4NLCYdO7Hg+jpOIfpjpjmE/nSoXD1Z0KCenjEe0kMXqhKh+6/SQsy9icdvNh2Y9dxQNLLfpRN1k0I900Oqqbpcaqg+uHwYF43wmv3wzfVq6EnMYKmSVMcHD0urF+tTJEO6un9VvuCUtLENpadtv5MnnftBQ3PEsgVX8tpq6EAK+KAZQ+0VLoo3NXdxsoj9nikMczfgBe9CxOJ8zjReNSY5/F1dHBaV35Ip/XFQSXRNlC7UrmRXFUzB0Yi5MXrDCnNbLMqnXUUd3M7mE7uS+MuW3Xd7Tcrk0zBNHYVA5MnZJlpo4S6WACU/Sn8ZQsaNCM/7F+cgYxuF4ZNg1kAvNNitx5eAtpvuCJEyVOJqHZYyca61TEgy8gH5tbCXvi1AfeL+n7GvOw45bcv5VZAEGoN/ZHFQDtk2qhWeZAwcG9jIHgHhDQchJZ6H1kmMEvmkl5EuInVFUyBvfSPTmOdnWf5/0mopjd3cSLpX/jNI809tniUY0fmW2afWXuzfZZvTb3oiThmXK5YOoXxvA0axopf/qm9advDQf4RoXBNsL1Tf+tCoGIwIAFXOeQvfljZ5dnUBdFalnTnE8D+SRKVC3NzS52l6u3a7re2ZKdmSyF/gAHLAxhFjDV8TQJmCoRIb1ACQjFJ1mQw2kcm76sZWbGgylxU7ERGpqnKUtMRS1TI2qlxijASj7pvRHSKFWailpGUlFyY+ppGdnuLCT7XckMvULy1NQ7UzJz9lsjMnkQGVqj0y1TS4kMrTENM2MdZnpaasuaalpm6OVRSNZFy0w9ixrVOrX0dkrVAnV0IuYFWVusxKGDn86OaiqmDsl6BCLEglgDmTkG2AK5Lo3NgYgmZuccoq9KPAUU2rEM54ZFeDf4eE0g6XtlLPNpRjzYh210DyJ8Q81QU/AskmTpQE6qDKn2MZwSDFWPJCbwXMGhX4tH41QhJOhCHyICgz/zPDK2YgCJJ6KmkoLUNWmkD7WmKshNVTiZS6uuAqYyJCE49CzsnfuZNjjYMYQoCafxNk21PalpSTiDiOgjJqlOAXKESo+gNzjtgI5aJXXGrBb39Z0t2Kzfor2/swXLQqGtp/Xh7o/q9jvV2aynzmaLvZEXxMHztWINDrMqtUg402RGVdDsXd2ZZc/mUeIvsLo2400sZSf7ubEWOCtDpy3qGw3D93kUdxup+24bzU3XyWv9N4rbg2NnXwTqbVI7kGbLCWf4DvP3k//bx9hkqji9ag8/ulK9IhtRiY96OuRoCMJnqKH4Mwt/3uHPLXyzw8nZDYRbHU6SBgi3O3zbwl90+AsL3+nwHQvf7XBSoIBwr8P3LPxlh7+08Fcd/srC9zt838IPOvzAwg87/NDCjzrctr6jDiepHYTHHX5s4ScdTg77IDzt8FMLP+twUgSC8LzDzy38osMvLPyywy8t/KrDyR4B4esOf23h1x2u3j/oj3HAqvXxomLk6OUh8QjxkZC4VgZI6EGOIyF1ZjlGMiYkREJqxHKCZEJIhISkxvI9kveE3CAhNWsZIyFVeimQkGxdJkhIViglEpKJyxQJqXLLD0g+EJIhIfVnmSPJCSmQkMqnnCKZElIiIZV/eYvklpAZEvKOppwjIW94yo9IPtLCsmTq7QZyt7kh7sqLhYa+Jh6IWRB12luL2n9TKaG+6HfWkRAnZuolK+o1N8TLmPCChQ7eEbdqV8hN6Bqlk9bvXXVJcL7EOcUBj5e2a26IVZYKNtvOFv3PaPdiMXIxtaztR561G2OdbpmkDRxDHb/67zh0/i9mNPeL50hI1hebSEi+F1tISKYX20hIjhcvkJDsLnaQkLwudpGQjC5eIiG5WrxCQrK02EdC8rM4QEIyszhEQnKyOEJCsrEYISF5WBwjIRlYnCAhuVecIiFZV5whIflWnCMhmVZcICE5VlwiIdlVXCEheVW8RkIyqrhGck3dWOzgPlfc3bHtc7GF+0yrbNn2mdhvQ4HW2beGAnEkeLjQaW7IaumAoBVGNCCIUb7ENCCI0yhczqW5IYuxCIBa6XwR/Uy77EFNoL8lJAWq2J4hov7eBAUNSVAQOihoNqQP9HRJ41lrGk8XDp61cvB0OvOs+czTCc2zZjRPpzSvzWlGiPL0LvSs29DTkcCzhgJP+6fXOqjRa84LFfrgHw19IHyOjAQ/EG4iI+EPhFvISAAE4TYyEgJB+AIZCYIg3EFGwiAId5GR2YNwDxnxGhC+REbcBoSvkJFACcJ9ZCRUgvAAGVklEB4iI+EShEfISMAE4QgZCZkgPEZGgiYIT5CRsAnCU2QkcILwDBkJnSA8R0aCJwgvkJHwCcJLZCSAgvAKGQmhIHyNjARREF4j00cS45V8nqutXeo36I+qJ0Mhaq0F+r6cqld51QGbHeivITZlDNob6+p9m5uykOP30bF6r3b/7p+C6AHFPAlB7EYivI2CYlLrZxBp9dXacOPrpr/uOI8vm9cmyx9RZFydfiAovVFfRTrHl3+onZXhW3P+iSy42bT5mcay7SHo2FtnvIzyO9rTt6U8iArrMLWq1vWkjDlLquJWgjkvml8cNbawo2rM4tz8vrlRqd9swKOicTGBJQUl9ULV1kXtqAHAnw31zbxh2JEcj3vH/nfN6KdppYg5xVGTgyzazZcL0P+7BytD82dR9OJiY2341drG8W9Xnj7Fn0x9NvjV4NeDh4Ph4PeDp4PdwWhwPvAH4eBPg78M/rr659W/rf599R+N6qefYJtfDnqf1X/+B9jw3gM=</latexit>G(B)<latexit sha1_base64="+JI18PAoMVk8whbhFR2ZPBDwsRs=">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</latexit>PB?(B)<latexit sha1_base64="kKZ9oj5kOCvCxqwMVSf/JawnXQs=">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</latexit>B?<latexit sha1_base64="H2K9ECUuwiUKsam/5pmddRQWDzU=">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</latexit>Algorithm 1 Projected Riemannian Subgradient Method
Initialization: set B0 and µ0;
1: for k = 0  1  . . . do
2:
3:
4:

obtain G(Bk) ∈ (cid:101)∂f (Bk) satisfying (5) with B = Bk;
update the iterate: (cid:98)Bk+1 ← Bk − µkG(Bk) and Bk+1 ← orth((cid:98)Bk+1);

compute a step size µk according to a certain rule;

5: end for

(10)

3.3 Convergence Analysis

Our convergence analysis for Algorithm 1 relies in the RRC of Deﬁnition 1. When this regularity
condition holds  we show that the iterates of Algorithm 1 exhibit the following properties: (i) they
converge to a neighborhood of the set B(cid:63) when a constant step size is used  and (ii) they converge at
an R-linear rate to B(cid:63) when a geometrically diminishing step size is used.

3.3.1 Constant step size

We ﬁrst consider the convergence of Algorithm 1 when a constant step size is used.
Proposition 1. Suppose that for some (α    B(cid:63)) the function f satisﬁes the (α    B(cid:63))-RRC in
Deﬁnition 1. Let {Bk} be generated by Algorithm 1 with step size µk ≡ µ ≤ α/ξ2 and initial
iterate B0 satisfying dist(B0  B(cid:63)) ≤   where ξ is deﬁned in (6). Then  for all k ≥ 0  it holds that
(11)

(cid:110)

(cid:111)

dist(Bk  B(cid:63)) ≤ max

dist(B0  B(cid:63)) − µαk/2  µξ2/α

.

Towards interpreting Proposition 1  ﬁrst consider the case dist(B0  B(cid:63)) > µξ2/α  in which
case (11) implies that after at most K = 2(dist(B0  B(cid:63)) − µξ2/α)/(µα) iterates  the inequal-
ity dist(Bk  B(cid:63)) ≤ µξ2/α will hold for all k ≥ K. In that sense  Proposition 1 essentially says that
no further decay of dist(Bk  B(cid:63)) can be guaranteed. This agrees with empirical evidence regarding
Algorithm 1 with constant step size (see Section 4). Note that (11) also suggests a tradeoff in selecting
the step size µ. A larger step size µ leads to a faster decrease on the bound but a larger universal
upper bound of µξ2/α  which may even exceed dist(B0  B(cid:63)) if µ is too large.

3.3.2 Geometrically diminishing step size

A useful strategy to balance the tradeoff discussed in the previous paragraph is to use a diminishing
step size that starts relatively large and decreases to zero as the iterates proceed. As the universal upper
bound µξ2
α in (11) is proportional to µ  it is expected that the decay rate of the step size will determine
the convergence rate of the iterates. In this section  we consider a geometrically diminishing step
size scheme  i.e.  we decrease the step size by a ﬁxed fraction between iterations. Our argument is
inspired by [10  27]. Convergence with geometrically diminishing step size is guaranteed by the
following result  which shows that if we choose the decay rate and initial step size properly  then the
projected Riemannian subgradient method converges to B(cid:63) at an R-linear rate.
Theorem 1. Suppose that f satisﬁes the (α    B(cid:63))-RRC in Deﬁnition 1. Let {Bk} be the sequence
generated by Algorithm 1 with step size

µk = µ0βk

(12)

and initialization B0 satisfying dist(B0  B(cid:63)) ≤ . Assume

(cid:115)

α dist(B0  B(cid:63))

2ξ2

µ0 ≤

and

1 − 2

αµ0

dist(B0  B(cid:63))

+

µ2
0ξ2

dist2(B0  B(cid:63))

=: β ≤ β < 1 

(13)

where ξ is deﬁned in (6). Then  the sequence {Bk} satisﬁes

dist(Bk  B(cid:63)) ≤ dist(B0  B(cid:63))βk for all k ≥ 0.

(14)

5

The rate at which {dist(Bk  B(cid:63))}k≥0 tends to zero in (14) is determined by β which satisﬁes
(13). Note that β is well deﬁned and is strictly less than 1 in (13). To see this  on one hand 
µ0 ≤ α dist(B0  B(cid:63))/2ξ2 and ξ ≥ α together imply 1 − 2αµ0/dist(B0  B(cid:63)) ≥ 0. On the
(cid:112)
0ξ2/dist2(B0  B(cid:63)) < 0 is a decreasing function of µ0 when
other hand  −2αµ0/dist(B0  B(cid:63)) + µ2
In particular  when µ0 = α dist(B0  B(cid:63))/2ξ2  we have β =
µ0 ∈ (0  α dist(B0  B(cid:63))/2ξ2].
1 − 3α2/4ξ2  giving the fastest decaying rate by setting β = β. Finally  if dist(B0  B(cid:63)) is not
known a priori  then one can replace it by its upper boud  in (13) and (14) and the results still hold.

4 Applications

In this section  we show that Algorithm 1 achieves an R-linear convergence rate for Dual Principal
Component Pursuit (DPCP) [42  51] and Orthogonal Dictionary Learning (ODL) [39  2].

4.1 DPCP for Robust Subspace Learning

We begin with the problem of learning a subspace from data corrupted by outliers [25]. Important
methods include Random Sampling And Consensus (RANSAC) [17]  fast median subspace [24] 
geodesic gradient descent [31]  coherence pursuit [35]  and many that solve convex formulations
based on (cid:96)1 and nuclear norm optimization [46  49  26  37  48]   but require either the dimension of the
subspace or the number of outliers to be sufﬁciently small. On the other hand  DPCP [40  41  42  51]
solves a non-convex problem  can provably handle subspaces of high dimension  and can provably
tolerate as many outliers as the square of the number of inliers. DPCP has been successfully applied
in three-view geometry problems [42] and road plane detection from 3D point cloud data [51  12] 
has been shown to outperform RANSAC  and has been applied in the multiple-hyperplane case [41].
The main principle behind DPCP is the computation of a basis for the orthogonal complement

of the subspace to be learned. Speciﬁcally  given a dataset (cid:101)X = [X O]Γ ∈ RD×L  where the
columns of X ∈ RD×N are inlier points spanning a d-dimensional subspace S of RD  the columns
of O ∈ RD×M are outlier points  and Γ is an unknown permutation  DPCP solves

f (B) := (cid:107)(cid:101)X (cid:62)

B(cid:107)1 2 ≡

minimize
B∈O(c D)

L(cid:88)

i=1

(cid:107)(cid:101)x

(cid:62)
i B(cid:107)2 

(15)

where c = D − d is the codimension of S. An iterative reweighted least squares (IRLS) algorithm
has been empirically utilized to solve (15) in [42]  but without formal guarantees.
Veriﬁcation of the regularity condition. We will show that the DPCP problem (15) satisﬁes the
RRC  which will then be used to establish convergence rates. Since the objective function f in (15)
)∂f (B). Also note that the (cid:96)2 norm is

is regular  it follows from [47] that (cid:101)∂f (B) = (I − BB

subdifferentially regular  thus by the chain rule one choice for the Riemannian subgradient is

(cid:62)

G(B) = (I − BB

(cid:62)

)

L(cid:88)

i=1

(cid:101)xi sign((cid:101)x

(cid:62)
i B)  where sign(a) :=

if a (cid:54)= 0 
if a = 0.

(16)

(cid:26)a/(cid:107)a(cid:107)2
)(cid:80)M

0

(cid:13)(cid:13)(I−BB

(cid:13)(cid:13)F

(cid:62)

M maxB∈O(c D)

i=1 oi sign(o(cid:62)

To analyze (15)  we deﬁne two quantities. The ﬁrst one characterizes the maximum Riemannian
subgradient related to outliers: ηO := 1
  which
appears in [51] when B is on O(1  D). The second one is related to the inliers and is given
N minb∈S∩O(1 D) (cid:107)X (cid:62)b(cid:107)1  which is referred to as the permeance statistic in [26].
by cX  min := 1
These quantities reﬂect how well distributed the inliers and outliers are  with larger values of
cX  min (respectively  smaller values of ηO) corresponding to a more uniform distributions of inliers
(cid:1)  the DPCP problem (15) satisﬁes the (α    S
(respectively  outliers). One of the key insights in this paper is that the DPCP problem (15) satisﬁes
the RRC of Deﬁnition 1 as we now state.
Theorem 2. For any  <
RRC with α = ((1 − 2/2)N cX  min − M ηO)/√2c and any orthonormal basis S
⊥ for S
(cid:107)G(B)(cid:107)F ≤ √N (cid:107)X(cid:107)2 + M ηO for all B ∈ O(c  D)  where (cid:107) · (cid:107)2 denotes the spectral norm.

2(cid:0)1 − M ηO/N cX  min

⊥
)-
⊥. Also 

(cid:113)

i B)

Combining this with Theorem 1 allows us to conclude the linear convergence of Algorithm 1 to S

6

⊥.

⊥

⊥

⊥

⊥

Corollary 1. Suppose that the initialization B0 satisﬁes dist2(B0  S
) < 2 (1 − M ηO/N cX  min)  
where dist(B0  S
) is deﬁned in (2). Let {Bk} be the sequence generated by Algorithm 1 for solving
the DPCP problem (15) with G(Bk) in (16) and step size µk = µ0βk  where µ0 and β satisfy (13)
)  α = ((1 − 2/2)N cX  min − M ηO)/√2c  and ξ = √N (cid:107)X(cid:107)2 + M ηO.
⊥
with  = dist(B0  S
⊥ at an R-linear rate  i.e.  dist(Bk  S
) for all k ≥ 0.
Then  Bk converges to S
Corollary 1 implies that the Riemannian subgradient method with a good initialization converges to an
⊥ at an R-linear rate. When c = 1  a projected subgradient method was proved
orthonormal basis of S
to have a piecewise linear convergence rate in [51]. In this case  Corollary 1 improves upon [51]
in three ways: (i) it allows for a simpler strategy for selecting the step size than does the piecewise
geometrically diminishing step size  which has two more parameters controlling when and how
often to decay the step size; (ii) it provides a more transparent convergence analysis since its proof
follows directly from the RRC and Theorem 1; and (iii) it places a slightly weaker requirement on the

initialization  which in practice we compute as the bottom eigenvectors of (cid:101)X (cid:101)X (cid:62) as in [42  51]. In the

) ≤ βk dist(B0  S

supplementary material  we show this spectral initialization satisﬁes the requirement in Corollary 1.

(a) Performance on problem (15)
with different step size choices µk.

(b) Performance on problem (15) for
the step size µk = 0.01βk.

(c) Performance on problem (17)
with different step sizes choices µk.

Figure 2: Performance of Algorithm 1 on the DPCP problem (15) and the ODL problem (17).

Experiments. Synthetic data for the DPCP problem is generated as follows: randomly sample
a subspace S of co-dimension c = 10 in ambient dimension D = 100  and uniformly at random
ratio is M/(M + N ) = 0.7. As an initialization B0  we use the bottom c eigenvectors of (cid:101)X (cid:101)X (cid:62)
sample N = 1500 inliers from S ∩ O(1  D) and M = 3500 outliers from O(1  D) so that the outlier
[42  51] as we described before.
Figure 2a displays the convergence of the projected Riemannian subgradient method with different
choices of step size. We observe linear convergence for the geometrically diminishing step size 
which converges much faster than when a constant step size or classical diminishing step size (O(1/k)
and O(1/√k)) is used. In Figure 2b  we illustrate the effect of the decay factor β for Algorithm 1
with geometrically diminishing step size µk = 0.01βk. First observe that  as expected  β controls the
convergence speed. When β is too small (e.g.  β ∈ {0.5  0.6}) convergence may not occur  which
agrees with (13) and (14). However  when β ≥ 0.7 the algorithm converges at an R-linear rate  with
larger values of β resulting in slower convergence speeds.

4.2 Orthogonal Dictionary Learning

Given a dataset (cid:101)X ∈ RD×N   DL [32] aims to learn a sparse representation Θ ∈ RM×N for (cid:101)X by
ﬁnding a dictionary A ∈ RD×M such that (cid:101)X ≈ AΘ with Θ sparse. Several DL methods have been

proposed in the literature  including the method of optimal directions (MOD) [16]  K-SVD [15] 
and alternating minimization [1]  as well as the Riemannian trust region method [39] and projected
Riemannian subgradient method [2] for ODL. Here  we consider the ODL problem [39  2] in which
the dictionary is square and orthogonal and the data is generated by the following random model.3
matrix. The data is generated as (cid:101)X = AΘ  where each column of Θ ∈ RD×D is an i.i.d. Bernoulli-
Deﬁnition 2 (Random model for ODL [2]). Assume A ∈ RD×D is a ﬁxed but unknown orthonormal
Gaussian random vector with parameter ρ ∈ (0  1) that controls the sparsity.

3Extensions to other models including deterministic models are the subject of future work.

7

0100200300iteration10-1010-505010015020010-1010-5100 = 0.5 = 0.6 = 0.7 = 0.8 = 0.902004006008001000iteration10-5100If b is a column of A  then A

b is a standard basis vector and (cid:101)X (cid:62)b = Θ(cid:62)

minimizing (cid:107)(cid:101)X (cid:62)b(cid:107)0 over the sphere is expected to yield the column of A that is least used in the

representation Θ. For computational reasons  the (cid:96)0 semi-norm is replaced by the (cid:96)1 norm [2] 4 thus
leading to the problem5

b is sparse. Thus 

A

(cid:62)

(cid:62)

minimize
b∈O(1 D)

f (b) = 1

b(cid:107)1.

(17)

N (cid:107)(cid:101)X (cid:62)

Veriﬁcation of the regularity condition. We show that the regularity condition in (5) is satisﬁed
for problem (17). The primary difference with our previous analysis is that the data is now random.
Thus  (5) will only be proved to hold with high probability. Towards that end  similar to (16)  a
Riemannian subgradient for (17) is

(cid:0)I − bb

(cid:62)(cid:1)(cid:16) N(cid:88)

i=1

G(b) = 1

N

(cid:17)

sign((cid:101)x

i b)(cid:101)xi

(cid:62)

.

(18)

b2
i

maxj(cid:54)=i b2

j ≥ 1 + ζ

(cid:110)
b ∈ O(1  D) :

The projected Riemannian subgradient method has been utilized in [2] for solving (17)  but only
with a sublinear rate of convergence guarantee  even though the function has been proved to satisfy
(5) with high probability. Based on this condition  we will show that Algorithm 1 can solve (17)
more efﬁciently  indeed with a linear convergence rate. To describe the RRC for (17)  suppose
without loss of generality that the orthonormal dictionary A is the identity matrix. Then  the
(cid:111)
goal is to ﬁnd the standard basis vectors {±e1  . . .  ±eD}  where the sign is irrelevant because
f (b) = f (−b). We now deﬁne a region of interest that is near each basis vector ei and −ei as
  where ζ > 0 and bj is the j-th entry of b. Each region I i
I i
ζ =
contains all unit vectors whose i-th entry is at least √1 + ζ larger (in absolute value) than the other
entries. The RRC for (17) is then captured by the following result.
Theorem 3. [2  Theorem 3.6] Assume ρ ∈ [1/D  1/2] in the random model of Deﬁnition 2. There
exist universal constants C  c > 0 such that if N ≥ CD4ζ−2ρ−2 log(D/ζ) for all ζ ∈ (0  1)  then
with probability at least 1 − exp(−cN ρ3ζ 2D−3/ log N ) the ODL problem (17) satisﬁes (5) for any
b ∈ I i
ζ  but not all b that is -close to ei.
Note that Theorem 3 ensures that (5) holds only for all b ∈ I i
Fortunately  [2  Proposition D.2] ensures the iterates generated by Algorithm 1 do stay within I i
ζ 
which together with Theorem 1 guarantees the convergence of Algorithm 1.
Corollary 2. Let {bk} be the sequence generated by Algorithm 1 for the ODL problem (17) with
64 ) and step size µk = µ0βk  where µ0 and β satisfy the conditions in Theorem 1
b0 ∈ I i
with ξ = 2 and  = √2  and α = 1
2 . Under the same setup as in Theorem 3  with
probability at least 1 − exp(−cN ρ3ζ 2D−3/ log N )  {bk} converges to ei at an R-linear rate  i.e. 
(19)

ζ with G(b) in (18) and B(cid:63) = ei for any i  and α = 1

16 ρ(1 − ρ)ζD− 3

16 ρ(1 − ρ)ζD− 3

ζ (ζ ≤ 55

2 .

ζ

dist(bk  ei) ≤ βk dist(b0  ei).

Corollary 2 improves upon [2  Theorem 3.8]  according to which under the setup in Corollary 2
and with step size µk = O(1/k3/8)  it follows that mink(cid:48)≤k dist(bk(cid:48)  ei) = O(1/k3/8). Indeed  our
result (19) gives a direct bound on the k-th iteration and not on the best iteration obtained so far.

Experiments We use the same setup in [2] by ﬁrst generating a random orthogonal dictionary
A ∈ RD×D with D = 70  sparsity level ρ = 0.3  and number of data points N = 5857 ≈ 10D1.5.
As in [2]  the initialization b0 is randomly generated from the unit sphere O(1  D) and belongs to one
of the D sets {I i
1/5 log D : i = 1  . . .   D} with probability at least one-half [2  Lemma 3.9]. Figure 2c
4[39] considered a smoothed version of (17)  allowing one to use gradient-based algorithms. However  the

obtained solution is perturbed from the targeted one and thus a rounding step is needed.

5All the columns of A can be obtained by repeating this process with the removal of the contribution from
the previously learned columns. Alternatively  as will been seen in Corollary 2  the column that Algorithm 1
converges to depends on the initialization. Thus  one may simply repeat Corollary 2 with different initializations
(e.g.  random initializations) each time [2]. It is of interest to extend (17) in order to estimate the whole dictionary 

e.g.  minimizing (cid:107)(cid:101)X (cid:62)B(cid:107)1  s. t. B ∈ O(D)  where (cid:96)1 counts the sum of the elements of the matrix. Note that

this is not an optimization on the Grassmannian since the objective is not rotation invariant.

8

displays the convergence of the Riemannian subgradient method for different choices of the step
size for solving (17). We observe linear convergence for geometrically diminishing step size  which
converges much faster than the others  in particular when µk = O( 1

k3/8 ) as is used in [2].

5 Conclusion and Discussion

We proved that a projected Riemannian subgradient method with geometrically diminishing step
sizes converges linearly for non-convex and non-smooth problems on the Grassmannian that satisfy
a certain regularity condition on the Riemannian subgradient. We also showed that our regularity
condition is satisﬁed by (cid:96)1 co-sparse formulations for orthogonal dictionary learning and robust
subspace learning  which led to improved convergence rates when compared to existing results.
We conclude this paper by pointing out several interesting directions for future work.
Extension to intrinsic methods. In this paper we take an extrinsic approach because extrinsic
methods are typically easier to implement  e.g. when the projection map is easier to compute than the
geodesic distance. Extending the current analysis to an intrinsic optimization method—where the
iterates are taken along a geodesic direction—is worth exploring. For example  the geodesic gradient
descent (GGD) [31] has been proved to converge at a piecewise linear rate for the robust subspace
learning problem. And extension of the current analysis may allow the GGD to use a simpler step
size selection strategy (i.e.  a geometrically diminishing step size) to obtain a linear convergence rate.
Extension to other submanifolds of Euclidean space. Although we focus on optimization problems
over the Grassmannian  the Riemannian regularity condition can be extended to other submanifolds
of Euclidean space with an appropriate deﬁnition of the Riemanniann metric and distance. Using
this condition to analyze the convergence of the projected Riemannian subgradient method for other
manifolds (such as the Stiefel manifold) is the subject of ongoing work.
Application to other problems. Aside from the robust subspace and dictionary learning problems
considered here  other problems in machine learning and signal processing can be formulated as
minimizing a non-smooth function over the sphere or Stiefel manifold and thus (potentially) can
be efﬁciently solved by the Riemannian subgradient method. These problems include the (cid:96)1-norm
(kernel) PCA [22  30  45]  multi-channel sparse blind deconvolution [28  33]  etc.

Acknowledgment

This research is supported in part by NSF grant 1704458  ShanghaiTech grant 2017F0203-000-16 
and the Northrop Grumman Mission Systems Research in Applications for Learning Machines
(REALM) initiative. Zhihui Zhu would like to thank Xiao Li and Dr. Anthony Man-Cho So (CUHK)
for fruitful discussions about regularity conditions for non-smooth optimization problems.

References
[1] S. Arora  R. Ge  T. Ma  and A. Moitra. Simple  efﬁcient  and neural algorithms for sparse coding. Journal

of Machine Learning Research  2015.

[2] Y. Bai  Q. Jiang  and J. Sun. Subgradient Descent Learns Orthogonal Dictionaries. In International

Conference on Learning Representations  2019.

[3] L. Balzano  R. Nowak  and B. Recht. Online identiﬁcation and tracking of subspaces from highly
In Communication  Control  and Computing (Allerton)  2010 48th Annual

incomplete information.
Allerton Conference on  pages 704–711. IEEE  2010.

[4] N. Boumal  P.-A. Absil  and C. Cartis. Global rates of convergence for nonconvex optimization on

manifolds. IMA Journal of Numerical Analysis  2016.

[5] J. V. Burke and M. C. Ferris. Weak sharp minima in mathematical programming. SIAM Journal on Control

and Optimization  31(5):1340–1359  1993.

[6] E. J. Candès  X. Li  and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and algorithms.

IEEE Transactions on Information Theory  61(4):1985–2007  2015.

9

[7] S. Chen  S. Ma  A. M.-C. So  and T. Zhang. Proximal gradient method for manifold optimization. arXiv

preprint arXiv:1811.00980  2018.

[8] F. E. Curtis  T. Mitchell  and M. L. Overton. A bfgs-sqp method for nonsmooth  nonconvex  constrained
optimization and its evaluation using relative minimization proﬁles. Optimization Methods and Software 
32(1):148–181  2017.

[9] F. E. Curtis and X. Que. A quasi-newton algorithm for nonconvex  nonsmooth optimization with global

convergence guarantees. Mathematical Programming Computation  7(4):399–428  2015.

[10] D. Davis  D. Drusvyatskiy  K. J. MacPhee  and C. Paquette. Subgradient methods for sharp weakly convex

functions. arXiv preprint arXiv:1803.02461  2018.

[11] D. Davis  D. Drusvyatskiy  and C. Paquette. The nonsmooth landscape of phase retrieval. arXiv preprint

arXiv:1711.03247  2017.

[12] T. Ding  Z. Zhu  T. Ding  Y. Yang  D. Robinson  R. Vidal  and M. Tsakiris. Noisy dual principal component

pursuit. In Proceedings of the International Conference on Machine learning  2019.

[13] J. C. Duchi and F. Ruan. Solving (most) of a set of quadratic equalities: Composite optimization for robust

phase retrieval. arXiv preprint arXiv:1705.02356  2017.

[14] A. Edelman  T. Arias  and S. T. Smith. The geometry of algorithms with orthogonality constraints. SIAM

Journal of Matrix Analysis Applications  20(2):303–353  1998.

[15] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries.

IEEE Transactions on Image Processing  15(12):3736–3745  2006.

[16] K. Engan  S. O. Aase  and J. H. Husoy. Method of optimal directions for frame design. IEEE International

Conference on Acoustics  Speech  and Signal Processing  1999.

[17] M. A. Fischler and R. C. Bolles. RANSAC random sample consensus: A paradigm for model ﬁtting with
applications to image analysis and automated cartography. Communications of the ACM  26:381–395 
1981.

[18] J.-L. Gofﬁn. Subgradient optimization in nonsmooth optimization (including the Soviet revolution). Groupe

d’études et de recherche en analyse des décisions  2012.

[19] P. Grohs and S. Hosseini. Nonsmooth trust region algorithms for locally Lipschitz functions on Riemannian

manifolds. IMA Journal of Numerical Analysis  36(3):1167–1192  2015.

[20] J. Hamm and D. D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based learning. In

Proceedings of the 25th international conference on Machine learning  pages 376–383. ACM  2008.

[21] N. Higham and P. Papadimitriou. Matrix procrustes problems. Rapport technique  University of Manchester 

1995.

[22] C. Kim and D. Klabjan. A simple and fast algorithm for l1-norm kernel pca. IEEE transactions on pattern

analysis and machine intelligence  2019.

[23] J. D. Lee  I. Panageas  G. Piliouras  M. Simchowitz  M. I. Jordan  and B. Recht. First-order methods almost

always avoid saddle points. arXiv preprint arXiv:1710.07406  2017.

[24] G. Lerman and T. Maunu. Fast  robust and non-convex subspace recovery. Information and Inference: A

Journal of the IMA  7(2):277–336  2017.

[25] G. Lerman and T. Maunu. An overview of robust subspace recovery. Proceedings of the IEEE  106(8):1380–

1410  2018.

[26] G. Lerman  M. B. McCoy  J. A. Tropp  and T. Zhang. Robust computation of linear models by convex

relaxation. Foundations of Computational Mathematics  15(2):363–410  2015.

[27] X. Li  Z. Zhu  A. M.-C. So  and R. Vidal. Nonconvex robust low-rank matrix recovery. arXiv preprint

arXiv:1809.09237  2018.

[28] Y. Li and Y. Bresler. Global geometry of multichannel sparse blind deconvolution on the sphere. In

Advances in Neural Information Processing Systems  pages 1132–1143  2018.

[29] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a general

approach. Annals of Operations Research  46(1):157–178  1993.

10

[30] P. P. Markopoulos  G. N. Karystinos  and D. A. Pados. Optimal algorithms for l_{1}-subspace signal

processing. IEEE Transactions on Signal Processing  62(19):5046–5058  2014.

[31] T. Maunu  T. Zhang  and G. Lerman. A well-tempered landscape for non-convex robust subspace recovery.

Journal of Machine Learning Research  20(37):1–59  2019.

[32] B. Olshausen and D. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code

for natural images. Nature  381(6583):607–609  1996.

[33] Q. Qu  X. Li  and Z. Zhu. A nonconvex approach for exact and efﬁcient multichannel sparse blind

deconvolution. In Advances in Neural Information Processing Systems  2019.

[34] Q. Qu  J. Sun  and J. Wright. Finding a sparse vector in a subspace: Linear sparsity using alternating

directions. In Advances in Neural Information Processing Systems  pages 3401–3409  2014.

[35] M. Rahmani and G. Atia. Coherence pursuit: Fast  simple  and robust principal component analysis. arXiv

preprint arXiv:1609.04789  2016.

[36] R. Slama  H. Wannous  M. Daoudi  and A. Srivastava. Accurate 3d action recognition using learning on

the grassmann manifold. Pattern Recognition  48(2):556–567  2015.

[37] M. Soltanolkotabi  E. Elhamifar  and E. Candès. Robust subspace clustering. http://arxiv.org/abs/1301.2603 

2013.

[38] G. W. Stewart and J. Sun. Matrix perturbation theory. Academic press  1990.

[39] J. Sun  Q. Qu  and J. Wright. Complete dictionary recovery over the sphere i: Overview and the geometric

picture. IEEE Transactions on Information Theory  63(2):853–884  2017.

[40] M. Tsakiris and R. Vidal. Dual principal component pursuit. In ICCV Workshop on Robust Subspace

Learning and Computer Vision  pages 10–18  2015.

[41] M. C. Tsakiris and R. Vidal. Hyperplane clustering via dual principal component pursuit. In International

Conference on Machine Learning  2017.

[42] M. C. Tsakiris and R. Vidal. Dual principal component pursuit. Journal of Machine Learning Research 

18(19):1–50  2018.

[43] K. Usevich and I. Markovsky. Optimization on a Grassmann manifold with application to system identiﬁ-

cation. Automatica  50(6):1656–1662  2014.

[44] J.-P. Vial. Strong and weak convexity of sets and functions. Mathematics of Operations Research 

8(2):231–259  1983.

[45] P. Wang  H. Liu  and A. M.-C. So. Globally convergent accelerated proximal alternating maximization
method for l1-principal component analysis. In IEEE International Conference on Acoustics  Speech and
Signal Processing (ICASSP)  pages 8147–8151. IEEE  2019.

[46] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. IEEE Transactions on Information

Theory  5(58):3047–3064  2012.

[47] W. H. Yang  L.-H. Zhang  and R. Song. Optimality conditions for the nonlinear programming problems on

riemannian manifolds. Paciﬁc Journal of Optimization  10(2):415–434  2014.

[48] C. You  D. P. Robinson  and R. Vidal. Provable self-representation based outlier detection in a union of

subspaces. In IEEE Conference on Computer Vision and Pattern Recognition  pages 4323–4332  2017.

[49] T. Zhang and G. Lerman. A novel m-estimator for robust pca. The Journal of Machine Learning Research 

15(1):749–808  2014.

[50] Y. Zhang  H.-W. Kuo  and J. Wright. Structured local minima in sparse blind deconvolution. In Advances

in Neural Information Processing Systems  pages 2322–2331  2018.

[51] Z. Zhu  Y. Wang  D. P. Robinson  D. Naiman  R. Vidal  and M. C. Tsakiris. Dual principal component

pursuit: Improved analysis and efﬁcient algorithms. In Neural Information Processing Systems  2018.

11

,Zhihui Zhu
Tianyu Ding
Daniel Robinson
Manolis Tsakiris
René Vidal