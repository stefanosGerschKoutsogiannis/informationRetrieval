2019,The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies,We study the relationship between the frequency of a function and the speed at which a neural network learns it.  We build on recent results that show that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system.  When normalized training data is uniformly distributed on a hypersphere  the eigenfunctions of this linear system are spherical harmonic functions.  We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model.  This bias term had been omitted from the linear network model without significantly affecting previous theoretical results.  However  we show theoretically and experimentally that a shallow neural network without bias cannot represent or learn simple  low frequency functions with odd frequencies.  Our results lead to specific predictions of the time it will take a network to learn functions of varying frequency.  These predictions match the empirical behavior of both shallow and deep networks.,The Convergence Rate of Neural Networks for
Learned Functions of Different Frequencies

Ronen Basri1

David Jacobs2

Yoni Kasten1

Shira Kritchman1

1Department of Computer Science  Weizmann Institute of Science  Rehovot  Israel

2Department of Computer Science  University of Maryland  College Park  MD

Abstract

We study the relationship between the frequency of a function and the speed at
which a neural network learns it. We build on recent results that show that the
dynamics of overparameterized neural networks trained with gradient descent
can be well approximated by a linear system. When normalized training data is
uniformly distributed on a hypersphere  the eigenfunctions of this linear system
are spherical harmonic functions. We derive the corresponding eigenvalues for
each frequency after introducing a bias term in the model. This bias term had been
omitted from the linear network model without signiﬁcantly affecting previous
theoretical results. However  we show theoretically and experimentally that a
shallow neural network without bias cannot represent or learn simple  low frequency
functions with odd frequencies. Our results lead to speciﬁc predictions of the time
it will take a network to learn functions of varying frequency. These predictions
match the empirical behavior of both shallow and deep networks.

1

Introduction

Neural networks have proven effective even though they often contain a large number of trainable
parameters that far exceeds the training data size. This deﬁes conventional wisdom that such
overparameterization would lead to overﬁtting and poor generalization. The dynamics of neural
networks trained with gradient descent can help explain this phenomenon. If networks explore
simpler solutions before complex ones  this would explain why even overparameterized networks
settle on simple solutions that do not overﬁt. It will also imply that early stopping can select simpler
solutions that generalize well  [13]. This is demonstrated in Figure 1-left.
We analyze the dynamics of neural networks using a frequency analysis (see also [21  27  26  9] 
discussed in Section 2). Building on [25  7  2] (and under the same assumptions) we show that
when a network is trained with a regression loss to learn a function over data drawn from a uniform
distribution  it learns the low frequency components of the function signiﬁcantly more rapidly than
the high frequency components (see Figure 2).
Speciﬁcally  [7  2] show that the time needed to learn a function  f  is determined by the projection
of f onto the eigenvectors of a matrix H∞  and their corresponding eigenvalues. [25] had previously
noted that for uniformly distributed training data  the eigenvectors of this matrix are spherical
harmonic functions (analogs to the Fourier basis on hyperspheres). This work makes a number of
strong assumptions. They analyze shallow  massively overparameterized networks with no bias. Data
is assumed to be normalized.
Building on these results  we compute the eigenvalues of this linear system. Our computation allows
us to make speciﬁc predictions about how quickly each frequency of the target function will be
learned. For example  for the case of 1D functions  we show that a function of frequency k can be

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Left: We train a CNN on MNIST data with 50% of the labels randomly changed. As the network
trains  accuracy on uncorrupted test data (in blue) ﬁrst improves dramatically  suggesting that the network ﬁrst
successfully ﬁts the uncorrupted data. Test accuracy then decreases as the network memorizes the incorrectly
labeled data. The green curve shows accuracy on test data with mixed correctly/incorrectly labeled data  while
the red curve shows training accuracy. (Other papers also mention this phenomenon  e.g.  [18]) Right: Given the
1D training data points (x1  ...  x32 ∈ S1) marked in black  a two layer network learns the function represented
by the orange curve  interpolating the missing data to form an approximate sinusoid of low frequency.

Figure 2: Network prediction (dark blue) for a superposition of two sine waves with frequencies k = 4  14
(light blue). The network ﬁts the lower frequency component of the function after 50 epochs  while ﬁtting the
full function only after ∼22K epochs.

learned in time that scales as k2. We show experimentally that this prediction is quite accurate  not
only for the simpliﬁed networks we study analytically  but also for realistic deep networks.
Bias terms in the network may be neglected without affecting previous theoretical results. However 
we show that without bias  two-layer neural networks cannot learn or even represent functions with
odd frequencies. This means that in the limit of large data  the bias-free networks studied by [25  7  2]
cannot learn certain simple  low-frequency functions. We show experimentally that a real shallow
network with no bias cannot learn such functions in practice. We therefore modify the model to
include bias. We show that with bias added  the eigenvectors remain spherical harmonics  and that
odd frequencies can be learned at a rate similar to even frequencies.
Our results show that essentially a network ﬁrst ﬁts the training data with low frequency functions
and then gradually adds higher and higher frequencies to improve the ﬁt. Figure 1-right shows a
rather surprising consequence of this. A deep network is trained on the black data points. The
orange curve shows the function the network learns. Notice that where there is data missing  the
network interpolates with a low frequency function  rather than with a more direct curve. This is
because a more straightforward interpolation of the data  while fairly smooth  would contain some
high frequency components. The function that is actually learned is almost purely low frequency1.
This example is rather extreme. In general  our results help to explain why networks generalize well
and don’t overﬁt. Because networks learn low frequency functions faster than high frequency ones  if
there is a way to ﬁt the data with low-frequency  the network will do this instead of overﬁtting with a
complex  high-frequency function.

2 Prior Work

Some prior work has examined the way that the dynamics or architecture of neural networks is
related to the frequency of the functions they learn. [21] bound the Fourier transform of the function
computed by a deep network and of each gradient descent (GD) update. Their method makes the

1 [10] show a related ﬁgure. In the context of meta-learning they show that a network trained to regress to
sine waves can learn a new sine wave from little training data. Our ﬁgure shows a different phenomenon  that 
when possible  a generic network will ﬁt data with low-frequency sine waves.

2

0400800Epochs00.51Accuracy-0-101Epoch = 0Epoch = 50Epoch = 500Epoch = 22452strong assumption that the network produces zeros outside a bounded domain. A related analysis for
shallow networks is presented in [27  26]. Neither paper makes an explicit prediction of the speed of
convergence. [9] derive bounds that show that for band limited functions two-layer networks converge
to a generalizable solution. [20  24  8] show that deeper networks can learn high frequency functions
that cannot be learned by shallow networks with a comparable number of units. [22] analyzes the
ability of networks to learn based on the frequency of functions computed by their components.
Recent papers study the relationship between the dynamics of gradient descent and the ability to
generalize. [23] shows that in logistic regression gradient descent leads to max margin solutions
for linearly separable data. [5] shows that with the hinge loss a two layer network provably ﬁnds a
generalizeable solution for linearly separable data. [14  17] provide related results. [16] studies the
effect of gradient descent on the alignment of the weight matrices for linear neural networks. [2] uses
the model discussed in this paper to study generalization.
It has been shown that the weights of heavily overparameterized networks change little during training 
allowing them to be accurately approximated by linear models that capture the nonlinearities caused
by ReLU at initialization [25  7  2]. These papers and others analyze neural networks without an
explicit bias term [28  19  12  1]. As [1] points out  bias can be ignored without loss of generality for
these results  because a constant value can be appended to the training data after it is normalized. [4] 
building on the work of [3]  perform a frequency analysis of the inductive bias of networks  using the
Neural Tangent Kernel. They produce results related to ours for bias-free networks. We also analyze
the signiﬁcant effect that bias has on the eigenvalues of these linear systems.
Some recent work (e.g.  [6]  [12]) raises questions about the relevance of this lazy training to practical
systems. Interestingly  our experiments indicate that our theoretical predictions  based on lazy training 
ﬁt the behavior of real  albeit simple  networks. The relevance of results based on lazy training to
large-scale real-world systems remains an interesting topic for future research.

3 Background

3.1 A Linear Dynamics Model

We begin with a brief review of [7  2]’s linear dynamics model. We consider a network with two
layers  implementing the function

f (x; W  a) =

1√
m

m(cid:88)

r=1

arσ(wT

r x) 

(1)

where x ∈ Rd+1 is the input and (cid:107)x(cid:107) = 1 (denoted x ∈ Sd)  W = [w1  ...  wm] ∈ R(d+1)×m and
a = [a1  ...  am]T ∈ Rm respectively are the weights of the ﬁrst and second layers  and σ denotes the
ReLU function  σ(x) = max(x  0). This model does not explicitly include bias. Let the training data
consist of n pairs {xi  yi}n
i=1  xi ∈ Sd and yi ∈ R. Gradient descent (GD) minimizes the L2 loss

n(cid:88)

i=1

Φ(W ) =

1
2

(yi − f (xi; W  a))2 

where we initialize the network with wr(0) ∼ N (0  κ2I). We further set ar ∼ Uniform{−1  1} and
maintain it ﬁxed throughout the training.
For the dynamic model we deﬁne the (d + 1)m × n matrix

 a1I11x1

a2I21x1

...

Z =

1√
m

a1I12x2
a2I22x2

...
...

a1I1nxn
a2I2nxn

...

  

amIm1x1 amIm2x2
i xj ≥ 0 and zero otherwise. Note that this indicator changes from
where the indicator Iij = 1 if wT
one GD iteration to the next  and so Z = Z(t). The network output over the training data can be
expressed as u(t) = Z T w ∈ Rn  where w = (wT
m)T . We further deﬁne the n × n Gram
matrix H = H(t) = Z T Z with Hij = 1

... amImnxn

(cid:80)m

1   ...  wT
r=1

IriIrj.

m xT

i xj

(2)

(3)

3

Next we deﬁne the main object of analysis  the n × n matrix H∞  deﬁned as the expectation of H
over the possible initializations. Its entries are given by

H∞
ij = Ew∼N (0 κ2I)Hij =

Thm. 4.1 in [2] relates the convergence of training a shallow network with GD to the eigenvalues of
H∞. For a network with m = Ω
denotes the minimal eigenvalue of H∞)  then with probability 1 − δ over the random initializations

units  κ = O

λ4

n2

n

1
2π

i xj(π − arccos(xT
xT

i xj)).

(cid:17)
(cid:16) δ√
and learning rate η = O(cid:0) λ0
(cid:33)1/2
(1 − ηλi)2t(cid:0)vT
i y(cid:1)2

±  

(4)

(cid:1) (λ0

(5)

(cid:107)y − u(t)(cid:107)2 =

(cid:17)
(cid:16) n7
(cid:32) n(cid:88)

0κ22δ

i=1

where v1  ...  vn and λ1  ...  λn respectively are the eigenvectors and eigenvalues of H∞.
3.2 The Eigenvectors of H∞ for Uniform Data
As is noted in [25]  when the training data distributes uniformly on a hypersphere the eigenvectors of
H∞ are the spherical harmonics. In this case H∞ forms a convolution matrix. A convolution on a
hypersphere is deﬁned by

K ∗ f (u) =

K(uT v)f (v)dv 

(6)

(cid:90)

Sd

(cid:80)n
j=1 H∞

where the kernel K(u  v) = K(uT v) is measureable and absolutely integrable on the hypersphere.
It is straightforward to verify that in S1 this deﬁnition is consistent with the standard 1-D convolution
with a periodic (and even) kernel  since K depends through the cosine function on the angular
difference between u and v. For d > 1 this deﬁnition requires the kernel to be rotationally symmetric
around the pole. This is essential in order for its rotation on Sd to make sense. We formalize this
observation in a theorem.
Theorem 1. Suppose the training data {xi}n
i=1 is distributed uniformly in Sd  then H∞ forms a
convolution matrix in Sd.
Proof. Let f : Sd → R be a scalar function  and let f ∈ Rn be a vector whose entries are the
function values at the training points  i.e.  fi = f (xi). Consider the application of H∞ to f 
ij fj  where A(Sd) denotes the total surface area of Sd. As n → ∞ this sum
gi = A(Sd)
i xj)f (xj)dxj  where dxj denotes a surface element
of Sd. Let the kernel K∞ be deﬁned as in (4)  i.e.  K∞(xi  xj) = 1
i xj)).
2π xT
Clearly  K∞ is rotationally symmetric around xi  and therefore g = K∞ ∗ f. H∞ moreover forms a
discretization of K∞  and its rows are phase-shifted copies of each other.
Theorem 1 implies that for uniformly distributed data the eigenvectors of H∞ are the Fourier series
in S1 or  using the Funk-Hecke Theorem (as we will discuss)  the spherical harmonics in Sd  d > 1.
We ﬁrst extend the dynamic model to allow for bias  and then derive the eigenvalues for both cases.
4 Harmonic Analysis of H∞

approaches the integral g(xi) = (cid:82)

i xj(π − arccos(xT

Sd K∞(xT

n

These results in the previous section imply that we can determine how quickly a network can learn
functions of varying frequency by ﬁnding the eigenvalues of the eigenvectors that correspond to
these frequencies. In this section we address this problem both theoretically and experimentally2.
Interestingly  as we establish in Theorem 2 below  the bias-free network deﬁned in (1) is not universal
as it cannot represent functions that contain odd frequencies greater than one. As a consequence the
odd frequencies lie in the null space of the kernel K∞ and cannot be learned – a signiﬁcant deﬁciency
in the model of [7  2]. We have the following:
Theorem 2. In the harmonic expansion of f (x) in (1)  the coefﬁcients corresponding to odd frequen-
cies k ≥ 3 are zero.

2Code for experiments shown in this paper can be found at https://github.com/ykasten/Convergence-Rate-

NN-Different-Frequencies.

4

Figure 3: Left: Fitting a bias-free two-layer network (with 2000 hidden units) to training data comprised of 51
points drawn from f (θ) = cos(3θ) (black dots). The orange  solid curve depicts the network output. Consistent
with Thm. 2  the network ﬁts the data points perfectly with just even frequencies  yielding poor interpolation
between data points. The right panel shows in comparison ﬁtting the network (solid line) to training data points
(black dots) drawn from f (θ) = cos(4θ). Fit was achieved by ﬁxing the ﬁrst layer weights at their random
(Gaussian) initialization and optimizing over the second layer weights.

Proof. We show this for d ≥ 2. The theorem also applies to the case that d = 1 with a similar proof.
Consider the output of one unit  g(x) = σ(wT x) and assume ﬁrst that w = (0  ...  0  1)T . In this
case g(x) = max{xd+1  0} and it is a linear combination of just the zonal harmonics. The zonal
harmonic coefﬁcients of g(x) are given by

gk = V ol(Sd−1)

max{t  0}Pk d(t)(1 − t2)

d−2
2 dt 

(7)

(cid:90) 1

−1

where V ol(Sd−1) denotes the volume of the hypersphere Sd−1 and Pk d(t) denotes the Gegenbauer
polynomial  given by the formula:

1
(1 − t2)
Γ is Euler’s gamma function. Eq. (7) can be written as

Γ( d
2 )
Γ(k + d
2 )

Pk d(t) =

(−1)k
2k

dk

dtk (1 − t2)k+ d−2

2 .

d−2

2

(cid:90) 1
(cid:90) 1

0

1

(8)

(9)

(10)

gk = V ol(Sd−1)

tPk d(t)(1 − t2)

d−2
2 dt.

For odd k  Pk d(t) is antisymmetric. Therefore  for such k

gk =

1
2

V ol(Sd−1)

tPk d(t)(1 − t2)

d−2
2 dt.

This is nothing but the (scaled) inner product of the ﬁrst order harmonic t with a harmonic of degree
k  and due to the orthogonality of the harmonic functions this integral vanishes for all odd values of
k except k = 1. This result remains unchanged if we use a general weight vector for w  as it only
rotates g(x)  resulting in a phase shift of the ﬁrst order harmonic. Finally  f is a linear combination
of single unit functions  and consequently its harmonic coefﬁcients at odd frequencies k ≥ 3 are
zero.

In Figure 3 we use a bias-free  two-layer network to ﬁt data drawn from the function cos(3θ).
Indeed  as the network cannot represent odd frequencies k ≥ 3 it ﬁts the data points perfectly with
combinations of even frequencies  hence yielding poor generalization.
This can be overcome by extending the model to use homogeneous coordinates  which introduce bias.
(xT   1)T ∈ Rd+2  and apply (1) to ¯x. Clearly  since (cid:107)x(cid:107) = 1
For a point x ∈ Sd we denote ¯x = 1√
also (cid:107)¯x(cid:107) = 1. We note that the proofs of [7  2] directly apply when both the weights and the biases
are initialized using a normal distribution with the same variance. It is also straightforward to modify
these theorems to account for bias initialized at zero  as is common in many practical applications.
We assume bias is initialized at 0  and construct the corresponding ¯H∞ matrix. This matrix takes the
form

2

¯H∞
ij = Ew∼N (0 κ2I)

¯Hij =

i xj + 1)(π − arccos(xT

(xT

i xj)).

(11)

1
4π

5

--/20/2-3-1.501.53--/20/2-3-1.501.53Figure 4: The six leading eigenvectors and three least signiﬁcant eigenvectors of the bias-free H∞ in descending
order of eigenvalues. Note that the least signiﬁcant eigenvectors resemble low odd frequencies.

···

Figure 5: The nine leading eigenvectors (k = 0  ...  4) of ¯H∞ in descending order of eigenvalues. Note that
now the leading eigenvectors include both the low even and odd frequencies.

Finally note that the bias adjusted kernel ¯K∞(xT
i xj)  deﬁned as in (11)  also forms a convolution on
the original (non-homogeneous) points. Therefore  since we assume that in Sd the data is distributed
uniformly  the eigenfunctions of ¯K∞ are also the spherical harmonics.
We next analyze the eigenfunctions and eigenvalues of K∞ and ¯K∞. We ﬁrst consider data distributed
uniformly over the circle S1 and subsequently discuss data in arbitrary dimension.

4.1 Eigenvalues in S1
(cid:82) π
Since both K∞ and ¯K∞ form convolution kernels on the circle  their eigenfunctions include the
Fourier series. For the bias-free kernel  K∞  the eigenvalues for frequencies k ≥ 0 are derived using
−π K∞(θ) cos(kθ)dθ where z0 = 2π and zk = π for k > 0. (Note that since K∞ is an
k = 1
a1
zk
even function its integral with sin(θ) vanishes.) This yields

a1
k =

1
π2
1
4
2(k2+1)
π2(k2−1)2
0

k = 0
k = 1
k ≥ 2 even
k ≥ 2 odd

(12)

H∞ is a discrete matrix that represents convolution with K∞. It is circulant symmetric (when
constructed with points sampled with uniform spacing) and its eigenvectors are real. Each frequency
except the DC is represented by two eigenvectors  one for sin(kθ) and the other cos(kθ).
(12) allows us to make two predictions. First  the eigenvalues for the even frequencies k shrink at
the asymptotic rate of 1/k2. This suggests  as we show below  that high frequency components are
quadratically slower to learn than low frequency components. Secondly  the eigenvalues for the odd
frequencies (for k ≥ 3) vanish. A network without bias cannot learn or even represent these odd
frequencies. Du et al.’s convergence results critically depend on the fact that for a ﬁnite discretization
H∞ is positive deﬁnite. In fact  H∞ does contain eigenvectors with small eigenvalues that match the
odd frequencies on the training data  as shown in Figure 4  which shows the numerically computed
eigenvectors of H∞. The leading eigenvectors include k = 1 followed by the low even frequencies 
whereas the eigenvectors with smallest eigenvalues include the low odd frequencies. However  a
bias-free network can only represent those functions as a combination of even frequencies. These
match the odd frequencies on the training data  but have wild behavior off the training data (see
Fig. 3). In fact  our experiments show that a network cannot even learn to ﬁt the training data when
labeled with odd frequency functions with k ≥ 3.
With bias  the kernel ¯K∞ passes all frequencies  and the odd frequencies no longer belong to its null
space. The Fourier coefﬁcients for this kernel are





c1
k =

1

1

8

2π2 + 1
π2 + 1
8
k2+1
π2(k2−1)2

1

π2k2

k = 0
k = 1
k ≥ 2 even
k ≥ 2 odd

(13)

Figure 5 shows that with bias  the highest eigenvectors include even and odd frequencies.
Thm. 4.1 in [2] tells us how fast a network learning each Fourier component should converge  as
a function of the eigenvalues computed in (13). Let yi be an eigenvector of ¯H∞ with eigenvalue

6

--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05--/20/2-0.0500.05Figure 6: Convergence times as a function of frequency. Left: S1 no bias (m = 4000  n = 1001  κ = 1 
η = 0.01; training odd frequencies was stopped after 1800 iterations had no signiﬁcant effect on error). Left-
center: S1 with bias (m = 4000  n = 1001  κ = 2.5  η = 0.01). Right-center: deep net (5 hidden layers with
bias  m = 256  n = 1001  η = 0.05  weight initialized as in [15]  bias - uniform). Right: deep residual network
(10 hidden layers with same parameters except η = 0.01). The data lies on a 1D circle embedded in R30 at a
random rotation. We estimate the growth in these graphs  from left  as O(k2.15)  O(k1.93)  O(k1.94)  O(k2.11).
Theoretical predictions (in orange) were scaled by a multiplicative constant to ﬁt the measurements. This constant
reﬂects the length of each gradient step (e.g.  due to the learning rate and size of training set). Convergence is
declared when a 5% ﬁtting error is obtained.

η¯λi

− log(¯δ+)

¯λi and denote by ti the number of iterations needed to achieve an accuracy ¯δ. Then  according
to (5)  (1 − η¯λi)ti < ¯δ + . Noting that since η is small  log(1 − η¯λi) ≈ −η¯λi  we obtain that
. Combined with (13) we get that asymptotically in k the convergence time should
ti >
grow quadratically for all frequencies.
We perform experiments to compare theoretical predictions to empirical behavior. We generate
uniformly distributed  normalized training data  and assign labels from a single harmonic function.
We then train a neural network until the error is reduced to 5% of its original value  and count the
number of epochs needed. For odd frequencies and bias-free 2-layer networks we halt training
when the network fails to signiﬁcantly reduce the error in a large number of epochs. We run
experiments with shallow networks and with deep fully connected networks and deep networks
with skip connections. We primarily use an L2 loss  but in supplementary material we show results
with a cross-entropy loss. Quadratic behavior is observed in all these cases  see Figure 6. The
actual convergence times may vary with the details of the architecture and initialization. For very
low frequencies the run time is affected more strongly by the initialization  yielding slightly slower
convergence times than predicted.
Thm. 5.1 in [2] further allows us to bound the generalization error incurred when learning band
k=0 αke2πikx. According to this theorem  and noting that the
eigenvalues of ( ¯H∞)−1 ≈ πk2  with sufﬁciently many iterations the population loss LD computed
over the entire data distribution is bounded by

limited functions. Suppose y =(cid:80)¯k
(cid:114)

(cid:115)

2π(cid:80)¯k

kk2

k=1 α2
n

.

(14)

LD (cid:47)

2y( ¯H∞)−1y

≈

n

As expected  the lower the frequency is  the lower the generalization bound is. For a pure sine wave
the bound increases linearly with frequency k.

4.2 Eigenvalues in Sd  d ≥ 2
To analyze the eigenvectors of H∞ when the input is higher dimensional  we must make use of
generalizations of the Fourier basis and convolution to functions on a high dimensional hypersphere.
Spherical harmonics provide an appropriate generalization of the Fourier basis (see [11] as a reference
for the following discussion). As with the Fourier basis  we can express functions on the hypersphere
as linear combinations of spherical harmonics. Since the kernel is rotationally symmetric  and
therefore a function of one variable  it can be written as a linear combination of the zonal harmonics.
For every frequency  there is a single zonal harmonic which is also a function of one variable. The
zonal harmonic is given by the Gegenbauer polynomial  Pk d where k denotes the frequency  and d
denotes the dimension of the hypersphere.

7

01020300.0k0.5k1.0k1.5kIterationsTheoretical fitQuadratic fitMeasurements0102030 0k 2k 4k 6kIterationsTheoretical fitQuadratic fitMeasurements01020300k20k40kIterationsQuadratic FitMeasurements01020300k20k40k60kIterationsQuadratic FitMeasurementsWe have already deﬁned convolution in (6) in a way that is general for convolution on the hyper-
sphere. The Funk-Hecke theorem provides a generalization of the convolution theorem for spherical
(cid:82) 1
harmonics  allowing us to perform a frequency analysis of the convolution kernel. It states:
Theorem 3. (Funk-Hecke) Given any measurable function K on [−1  1]  such that the integral:
−1 (cid:107)K(t)(cid:107)(1 − t2)

2 dt < ∞  for every spherical harmonic H(σ) of frequency k  we have:
d−2

K(σ · ξ)H(ξ)dξ =

Vol(Sd−1)

K(t)Pk d(t)(1 − t2)

d−2
2 dt

H(σ).

(cid:90)

Sd

(cid:18)

(cid:19)

Here Vol(Sd−1) denotes the volume of Sd−1 and Pk d(t) denotes the Gegenbauer polynomial deﬁned
in (8). This tells us that the spherical harmonics are the eigenfunctions of convolution. The eigen-
values can be found by taking an inner product between K and the zonal harmonic of frequency k.
Consequently  we see that for uniformly distributed input  in the limit for n → ∞  the eigenvectors
of H∞ are the spherical harmonics in Sd.
Similar to the case of S1  in the bias free case the odd harmonics with k ≥ 3 lie in the null space of
K∞. This is proved in the following theorem.
Theorem 4. The eigenvalues of convolution with K∞ vanish when they correspond to odd harmonics
with k ≥ 3.

Proof. Consider the vector function z(w  x) = I(wT x > 0)x and note that K∞(xi  xj) =
Sd zT (w  xi)z(w  xj)dw. Let y(x) be an odd order harmonic of frequency k > 1. The appli-

(cid:82)
cation of z to y takes the form(cid:90)

Sd

z(w  x)y(x)dx =

I(wT x > 0)g(x)dx 

(15)

where g(x) = y(x)x. g(x) is a (d + 1)-vector whose lth coordinate is gl(x) = xly(x). We ﬁrst note
that gl(x) has no DC component. This is because gl is the product of two harmonics  the scaled ﬁrst
order harmonic  xl  and the odd harmonic y(x) (with k > 1)  so their inner product vanishes.
Next we will show that the kernel I(wT x > 0) annihilates the even harmonics  for k > 1. Note that
the odd/even harmonics can be written as a sum of monomials of odd/even degrees. Since g is the
sum of even harmonics (the product of xl and an odd harmonic) this will imply that (15) vanishes.
Using the Funk-Hecke theorem  the even coefﬁcients of the kernel (with k > 1) are

k = V ol(Sd−1)
rd

= V ol(Sd−1)

I(t > 0)Pk d(t)(1 − t2)

Pk d(t)(1 − t2)

d−2
2 dt =

d−2
2 dt
V ol(Sd−1)

(cid:90) 1

2

−1

(16)

Pk d(t)(1 − t2)

d−2
2 dt = 0.

When we align the kernel with the zonal harmonic  wT x = t  justifying the second equality. The
third equality is due to the symmetry of the even harmonics  and the last equality is because the
harmonics of k > 0 are zero mean.

Next we compute the eigenvalues of both K∞ and ¯K∞ (for simplicity we show only the case of even
d  see supplementary material for the calculations). We ﬁnd for networks without bias:

(cid:90) 1
(cid:90) 1

−1

0

(cid:90) 1

−1

(cid:90)

Sd

8



C1(d  0)

(cid:1)
(cid:0) d
C1(d  1)(cid:80)d
C1(d  k)(cid:80)k+ d−2

d2d+1

d
2

1

q=(cid:100) k

0

ad
k =

with

q=1 C2(q  d  1)

1

2(2q+1)

2

2 (cid:101) C2(q  d  k)

1

2(2q−k+2)

(cid:16)

1 −

1

22q−k+2

C1(d  k) =

(−1)k
2k

2

π d
( d
2 )

1

Γ(k + d
2 )

 

C2(q  d  k) = (−1)q

(17)

2q−k+2

(cid:1)(cid:17)
(cid:0)2q−k+2
(cid:18)k + d−2

2

2

k = 0

k = 1
k ≥ 2 even
k ≥ 2 odd 

(cid:19) (2q)!

q

(2q − k)!

.

Figure 7: Convergence times as a function of frequency for data in S2. Left: no bias (m = 16000  n = 1001 
κ = 1  and η = 0.01; training odd frequencies was stopped after 40K iterations with no signiﬁcant reduction
of error). Left-center: with bias (same parameters). Right-center: deep residual network (10 hidden layers
with m = 256  n = 5000  η = 0.001  weight initialization as in [15]  bias - uniform). The data lies on a
2D sphere embedded in R30 at a random rotation. Growth estimates from left  O(k2.74)  O(k2.87)  O(k3.13).
Right: Convergence exponent as a function of dimension. g(d) = limk→∞ − log cd
log k estimated by calculating the
coefﬁcients up to k = 1000  indicating that the coefﬁcients decay roughly as 1/kd.

k

Adding bias to the network  the eigenvalues for ¯K∞ are:

(cid:1) + 2d−1

d(d−1
)

d
2

− 1

2



(cid:32)

1

d
2

d2d+1

1
2 C1(d  0)

(cid:0) d
2 C1(d  1)(cid:80)k+ d−2
2 C1(d  k)(cid:80)k+ d−2
2 C1(d  k)(cid:80)k+ d−2

q=(cid:100) k

q=(cid:100) k

q=(cid:100) k

1

1

1

2

2

2

2 (cid:101) C2(q  d  1)
2 (cid:101) C2(q  d  k)
2 (cid:101) C2(q  d  k)

cd
k =

2

2
q

(cid:16)

q=0 (−1)q(cid:0) d−2
(cid:80) d−2
(cid:16)

1 − 1

2(2q−k+2)
1 −

22q

4q

1

1

1

−1

1

2(2q+1) + 1

2(2q−k+1) +

2(2q−k+1)

22q−k+1

(cid:16)
(cid:16)
(cid:16)

2q+1

(cid:33)
(cid:1) 1
(cid:1)(cid:17)(cid:17)
(cid:0)2q
(cid:16)
(cid:0)2q−k+1

1 −

2q−k+1

q

2

22q−k+2

1

(cid:1)(cid:17)(cid:17)

(cid:1)(cid:17)(cid:17)

(cid:0)2q−k+2

2q−k+2

2

k = 0

k = 1
k ≥ 2 even
k ≥ 2 odd.
(18)

We trained two layer networks with and without bias  as well as a deeper network  on data representing
pure spherical harmonics in S2. Convergence times are plotted in Figure 7. These times increase
roughly as k3  matching our predictions in (17) and (18). We further estimated numerically the
anticipated convergence times for data of higher dimension. As the ﬁgure shows (right panel) 
convergence times are expected to grow roughly as kd. We note that this is similar to the bound
derived in [21] under quite different assumptions.

5 Discussion

We have developed a quantitative understanding of the speed at which neural networks learn functions
of different frequencies. This shows that they learn high frequency functions much more slowly than
low frequency functions. Our analysis addresses networks that are heavily overparameterized  but
our experiments suggest that these results apply to real neural networks.
This analysis allows us to understand gradient descent as a frequency based regularization. Essentially 
networks ﬁrst ﬁt low frequency components of a target function  then they ﬁt high frequency
components. This suggests that early stopping regularizes by selecting smoother functions. It also
suggests that when a network can represent many functions that would ﬁt the training data  gradient
descent causes the network to ﬁt the smoothest function  as measured by the power spectrum of the
function. In signal processing  it is commonly the case that the noise contains much larger high
frequency components than the signal. Hence smoothing reduces the noise while preserving most of
the signal. Gradient descent may perform a similar type of smoothing in neural networks.
Acknowledgments. The authors thank Adam Klivans  Boaz Nadler  and Uri Shaham for helpful discussions.
This material is based upon work supported by the National Science Foundation under Grant No. DMS1439786
while the authors were in residence at the Institute for Computational and Experimental Research in Mathematics
in Providence  RI  during the Computer Vision program. This research is supported by the National Science
Foundation under grant no. IIS-1526234.

9

0102030024Iterations104Theoretical fitCubic fitMeasurements01020300246Iterations104Theoretical fitCubic fitMeasurements01020300k50k100k150kIterationsCubic FitMeasurements25010035095.8References
[1] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. On the convergence rate of training recurrent neural

networks. arXiv preprint arXiv:1810.12065  2018.

[2] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584 
2019.

[3] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine

Learning Research  18:1–53  2017.

[4] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint

arXiv:1905.12173  2019.

[5] Alon Brutzkus  Amir Globerson  Eran Malach  and Shai Shalev-Shwartz. Sgd learns over-parameterized
networks that provably generalize on linearly separable data. In International Conference on Learning
Representations  2018.

[6] Lenaic Chizat  Edouard Oyallon  and Francis Bach. On lazy training in differentiable programming. In

Advances in Neural Information Processing Systems  2019.

[7] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes over-

parameterized neural networks. International Conference on Learning Representations (ICLR)  2019.

[8] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on

learning theory  pages 907–940  2016.

[9] Farzan Farnia  Jesse Zhang  and David Tse. A spectral approach to generalization and optimization in

neural networks. 2018.

[10] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
1126–1135. JMLR. org  2017.

[11] Jean Gallier. Notes on spherical harmonics and linear representations of lie groups. preprint  2009.

[12] Behrooz Ghorbani  Song Mei  Theodor Misiakiewicz  and Andrea Montanari. Linearized two-layers neural

networks in high dimension. arXiv preprint arXiv:1904.12191  2019.

[13] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep learning. MIT press  2016.

[14] Suriya Gunasekar  Jason D Lee  Daniel Soudry  and Nati Srebro. Implicit bias of gradient descent on linear
convolutional networks. In Advances in Neural Information Processing Systems  pages 9461–9471  2018.

[15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In International Conference on Computer Vision
(ICCV)  pages 1026–1034  2015.

[16] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv preprint

arXiv:1810.02032  2018.

[17] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint

arXiv:1803.07300  2018.

[18] Mingchen Li  Mahdi Soltanolkotabi  and Samet Oymak. Gradient descent with early stopping is provably

robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680  2019.

[19] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent

on structured data. In Advances in Neural Information Processing Systems  pages 8157–8166  2018.

[20] Guido F Montufar  Razvan Pascanu  Kyunghyun Cho  and Yoshua Bengio. On the number of linear regions
of deep neural networks. In Advances in neural information processing systems  pages 2924–2932  2014.

[21] Nasim Rahaman  Devansh Arpit  Aristide Baratin  Felix Draxler  Min Lin  Fred A Hamprecht  Yoshua Ben-
gio  and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint arXiv:1806.08734 
2018.

[22] Shai Shalev-Shwartz  Ohad Shamir  and Shaked Shammah. Weight sharing is crucial to succesful

optimization. arXiv preprint arXiv:1706.00687  2017.

10

[23] Daniel Soudry  Elad Hoffer  Mor Shpigel Nacson  Suriya Gunasekar  and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research  19(1):2822–2878 
2018.

[24] Matus Telgarsky. beneﬁts of depth in neural networks.

1517–1539  2016.

In Conference on Learning Theory  pages

[25] Bo Xie  Yingyu Liang  and Le Song. Diverse neural network learns true target functions. In International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  Fort Lauderdale  Florida  pages 1216–1224 
2017.

[26] Zhi-Qin John Xu  Yaoyu Zhang  Tao Luo  Yanyang Xiao  and Zheng Ma. Frequency principle: Fourier

analysis sheds light on deep neural networks. CoRR  abs/1901.06523  2019.

[27] Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis. CoRR 

abs/1808.04295  2018.

[28] Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent optimizes over-

parameterized deep relu networks. arXiv preprint arXiv:1811.08888  2018.

11

,Basri Ronen
David Jacobs
Yoni Kasten
Shira Kritchman