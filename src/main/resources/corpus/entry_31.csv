2018,A Block Coordinate Ascent Algorithm for Mean-Variance Optimization,Risk management in dynamic decision problems is a primary concern in many fields  including financial investment  autonomous driving  and healthcare. The mean-variance function is one of the most widely used objective functions in risk management due to its simplicity and interpretability. Existing algorithms for mean-variance optimization are based on multi-time-scale stochastic approximation  whose learning rate schedules are often hard to tune  and have only asymptotic convergence proof. In this paper  we develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). Our starting point is a reformulation of the original mean-variance function with its Fenchel dual  from which we propose a stochastic block coordinate ascent policy search algorithm. Both the asymptotic convergence guarantee of the last iteration's solution and the convergence rate of the randomly picked solution are provided  and their applicability is demonstrated on several benchmark domains.,A Block Coordinate Ascent Algorithm for

Mean-Variance Optimization

Tengyang Xie∗
UMass Amherst

txie@cs.umass.edu

Bo Liu∗

Auburn University
boliu@auburn.edu

Yangyang Xu

Rensselaer Polytechnic Institute

xuy21@rpi.edu

Mohammad Ghavamzadeh

Facebook AI Research

Yinlam Chow

Google DeepMind

Daoming Lyu

Auburn University

mgh@fb.com

yinlamchow@google.com

daoming.lyu@auburn.edu

Daesub Yoon

ETRI

eyetracker@etri.re.kr

Abstract

Risk management in dynamic decision problems is a primary concern in many
ﬁelds  including ﬁnancial investment  autonomous driving  and healthcare. The
mean-variance function is one of the most widely used objective functions in risk
management due to its simplicity and interpretability. Existing algorithms for
mean-variance optimization are based on multi-time-scale stochastic approxima-
tion  whose learning rate schedules are often hard to tune  and have only asymptotic
convergence proof. In this paper  we develop a model-free policy search frame-
work for mean-variance optimization with ﬁnite-sample error bound analysis (to
local optima). Our starting point is a reformulation of the original mean-variance
function with its Legendre-Fenchel dual  from which we propose a stochastic
block coordinate ascent policy search algorithm. Both the asymptotic convergence
guarantee of the last iteration’s solution and the convergence rate of the randomly
picked solution are provided  and their applicability is demonstrated on several
benchmark domains.

1

Introduction

Risk management plays a central role in sequential decision-making problems  common in ﬁelds such
as portfolio management [Lai et al.  2011]  autonomous driving [Maurer et al.  2016]  and health-
care [Parker  2009]. A common risk-measure is the variance of the expected sum of rewards/costs and
the mean-variance trade-off function [Sobel  1982; Mannor and Tsitsiklis  2011] is one of the most
widely used objective functions in risk-sensitive decision-making. Other risk-sensitive objectives
have also been studied  for example  Borkar [2002] studied exponential utility functions  Tamar
et al. [2012] experimented with the Sharpe Ratio measurement  Chow et al. [2018] studied value
at risk (VaR) and mean-VaR optimization  Chow and Ghavamzadeh [2014]  Tamar et al. [2015b] 
and Chow et al. [2018] investigated conditional value at risk (CVaR) and mean-CVaR optimization
in a static setting  and Tamar et al. [2015a] investigated coherent risk for both linear and nonlinear
system dynamics. Compared with other widely used performance measurements  such as the Sharpe
Ratio and CVaR  the mean-variance measurement has explicit interpretability and computational

∗Equal contribution. Corresponding to: boliu@auburn.edu

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

advantages [Markowitz et al.  2000; Li and Ng  2000]. For example  the Sharpe Ratio tends to lead to
solutions with less mean return [Tamar et al.  2012]. Existing mean-variance reinforcement learning
(RL) algorithms [Tamar et al.  2012; Prashanth and Ghavamzadeh  2013  2016] often suffer from
heavy computational cost  slow convergence  and difﬁculties in tuning their learning rate schedules.
Moreover  all their analyses are asymptotic and no rigorous ﬁnite-sample complexity analysis has
been reported. Recently  Dalal et al. [2018] provided a general approach to compute ﬁnite sample
analysis in the case of linear multiple time scales stochastic approximation problems. However 
existing multiple time scales algorithms like [Tamar et al.  2012] consist of nonlinear term in its
update  and cannot be analyzed via the method in Dalal et al. [2018]. All these make it difﬁcult to
use them in real-world problems. The goal of this paper is to propose a mean-variance optimization
algorithm that is both computationally efﬁcient and has ﬁnite-sample analysis guarantees. This paper
makes the following contributions: 1) We develop a computationally efﬁcient RL algorithm for
mean-variance optimization. By reformulating the mean-variance function with its Legendre-Fenchel
dual [Boyd and Vandenberghe  2004]  we propose a new formulation for mean-variance optimization
and use it to derive a computationally efﬁcient algorithm that is based on stochastic cyclic block
coordinate descent. 2) We provide the sample complexity analysis of our proposed algorithm. This
result is novel because although cyclic block coordinate descent algorithms usually have empirically
better performance than randomized block coordinate descent algorithms  yet almost all the reported
analysis of these algorithms are asymptotic [Xu and Yin  2015].
Here is a roadmap for the rest of the paper. Section 2 offers a brief background on risk-sensitive
RL and stochastic variance reduction. In Section 3  the problem is reformulated using the Legendre-
Fenchel duality and a novel algorithm is proposed based on stochastic block coordinate descent.
Section 4 contains the theoretical analysis of the paper that includes both asymptotic convergence
and ﬁnite-sample error bound. The experimental results of Section 5 validate the effectiveness of the
proposed algorithms.

2 Backgrounds

This section offers a brief overview of risk-sensitive RL  including the objective functions and
algorithms. We then introduce block coordinate descent methods. Finally  we introduce the Legendre-
Fenchel duality  the key ingredient in formulating our new algorithms.

2.1 Risk-Sensitive Reinforcement Learning

Reinforcement Learning (RL) [Sutton and Barto  1998] is a class of learning problems in which an
agent interacts with an unfamiliar  dynamic  and stochastic environment  where the agent’s goal is to
optimize some measures of its long-term performance. This interaction is conventionally modeled
ss(cid:48)  r  γ)  where S and A are
as a Markov decision process (MDP)  deﬁned as the tuple (S A  P0  P a
the sets of states and actions  P0 is the initial state distribution  P a
ss(cid:48) is the transition kernel that
speciﬁes the probability of transition from state s ∈ S to state s(cid:48) ∈ S by taking action a ∈ A 
r(s  a) : S × A → R is the reward function bounded by Rmax  and 0 ≤ γ < 1 is a discount factor.
A parameterized stochastic policy πθ(a|s) : S × A → [0  1] is a probabilistic mapping from states to
actions  where θ is the tunable parameter and πθ(a|s) is a differentiable function w.r.t. θ.
One commonly used performance measure for policies in episodic MDPs is the return or cumulative
k=1 r(sk  ak)  where s1 ∼ P0 and τ is the ﬁrst
passage time to the recurrent state s∗ [Puterman  1994; Tamar et al.  2012]  and thus  τ := min{k >
0 | sk = s∗}. In risk-neutral MDPs  the algorithms aim at ﬁnding a near-optimal policy that
maximizes the expected sum of rewards J(θ) := Eπθ [R] = Eπθ
the square-return M (θ) := Eπθ [R2] = Eπθ
drop the subscript πθ to simplify the notation.
In risk-sensitive mean-variance optimization MDPs  the objective is often to maximize J(θ) with a
variance constraint  i.e. 

sum of rewards from the starting state  i.e.  R =(cid:80)τ
(cid:2)(cid:80)τ
k=1 r(sk  ak)(cid:3). We also deﬁne
(cid:104)(cid:0)(cid:80)τ
k=1 r(sk  ak)(cid:1)2(cid:105)

. In the following  we sometimes

J(θ) = Eπθ [R]
max
s.t. Varπθ (R) ≤ ζ 

θ

2

(1)

where Varπθ (R) = M (θ) − J 2(θ) measures the variance of the return random variable R  and
ζ > 0 is a given risk parameter [Tamar et al.  2012; Prashanth and Ghavamzadeh  2013]. Using the
Lagrangian relaxation procedure [Bertsekas  1999]  we can transform the optimization problem (1)
to maximizing the following unconstrained objective function:

Jλ(θ) :=Eπθ [R] − λ(cid:0)Varπθ (R) − ζ)
=J(θ) − λ(cid:0)M (θ) − J(θ)2 − ζ(cid:1).

(2)
It is important to note that the mean-variance objective function is NP-hard in general [Mannor and
Tsitsiklis  2011]. The main reason for the hardness of this optimization problem is that although
the variance satisﬁes a Bellman equation [Sobel  1982]  unfortunately  it lacks the monotonicity
property of dynamic programming (DP)  and thus  it is not clear how the related risk measures can be
optimized by standard DP algorithms [Sobel  1982].
The existing methods to maximize the objective function (2) are mostly based on stochastic approxi-
mation that often converge to an equilibrium point of an ordinary differential equation (ODE) [Borkar 
2008]. For example  Tamar et al. [2012] proposed a policy gradient algorithm  a two-time-scale
stochastic approximation  to maximize (2) for a ﬁxed value of λ (they optimize over λ by selecting its
best value in a ﬁnite set)  while the algorithm in Prashanth and Ghavamzadeh [2013] to maximize (2)
is actor-critic and is a three-time-scale stochastic approximation algorithm (the third time-scale
optimizes over λ). The stochastic compositional optimization method [Wang et al.  2017] also
needs two-time-scale stepsize tuning for mean-variance optimization  and dual embeddings [Dai
et al.  2017] assume the embedded problem can be solved exactly. These approaches suffer from
certain drawbacks: 1) Most of the analyses of ODE-based methods are asymptotic  with no sample
complexity analysis. 2) It is well-known that multi-time-scale approaches are sensitive to the choice
of the stepsize schedules  which is a non-trivial burden in real-world problems. 3) The ODE approach
does not allow extra penalty functions. Adding penalty functions can often strengthen the robustness
of the algorithm  encourages sparsity and incorporates prior knowledge into the problem [Hastie
et al.  2001].

2.2 Coordinate Descent Optimization

Coordinate descent (CD)1 and the more general block coordinate descent (BCD) algorithms solve
a minimization problem by iteratively updating variables along coordinate directions or coordinate
hyperplanes [Wright  2015]. At each iteration of BCD  the objective function is (approximately)
minimized w.r.t. a coordinate or a block of coordinates by ﬁxing the remaining ones  and thus  an
easier lower-dimensional subproblem needs to be solved. A number of comprehensive studies on
BCD have already been carried out  such as Luo and Tseng [1992] and Nesterov [2012] for convex
problems  and Tseng [2001]  Xu and Yin [2013]  and Razaviyayn et al. [2013] for nonconvex cases
(also see Wright 2015 for a review paper). For stochastic problems with a block structure  Dang and
Lan [2015] proposed stochastic block mirror descent (SBMD) by combining BCD with stochastic
mirror descent [Beck and Teboulle  2003; Nemirovski et al.  2009]. Another line of research on this
topic is block stochastic gradient coordinate descent (BSG) [Xu and Yin  2015]. The key difference
between SBMD and BSG is that at each iteration  SBMD randomly picks one block of variables to
update  while BSG cyclically updates all block variables.
In this paper  we develop mean-variance optimization algorithms based on both nonconvex stochastic
BSG and SBMD. Since it has been shown that the BSG-based methods usually have better empirical
performance than their SBMD counterparts  the main algorithm we report  analyze  and evaluate
in the paper is BSG-based. We report our SBMD-based algorithm in Appendix C and use it as a
baseline in the experiments of Section 5. The ﬁnite-sample analysis of our BSG-based algorithm
reported in Section 4 is novel because although there exists such analysis for convex stochastic BSG
methods [Xu and Yin  2015]  we are not aware of similar results for their nonconvex version to the
best our knowledge.

3 Algorithm Design

In this section  we ﬁrst discuss the difﬁculties of using the regular stochastic gradient ascent to
maximize the mean-variance objective function. We then propose a new formulation of the mean-

1Note that since our problem is maximization  our proposed algorithms are block coordinate ascent.

3

variance objective function that is based on its Legendre-Fenchel dual and derive novel algorithms
that are based on the recent results in stochastic nonconvex block coordinate descent. We conclude
this section with an asymptotic analysis of a version of our proposed algorithm.

3.1 Problem Formulation

i.e.  Rt =(cid:80)τt

∇θJλ(θt) =∇θJ(θt) − λ∇θVar(R)

In this section  we describe why the vanilla stochastic gradient cannot be used to maximize Jλ(θ)
deﬁned in Eq. (2). Taking the gradient of Jλ(θ) w.r.t. θ  we have

=∇θJ(θt) − λ(cid:0)∇θM (θ) − 2J(θ)∇θJ(θ)(cid:1).
k=1 rk  which is possibly a nonconvex function  and ωt(θ) =(cid:80)τt

(3)
Computing ∇θJλ(θt) in (3) involves computing three quantities: ∇θJ(θ) ∇θM (θ)  and
J(θ)∇θJ(θ). We can obtain unbiased estimates of ∇θJ(θ) and ∇θM (θ) from a single trajec-
tory generated by the policy πθ using the likelihood ratio method [Williams  1992]  as ∇θJ(θ) =
E[Rtωt(θ)] and ∇θM (θ) = E[R2
t ωt(θ)]. Note that Rt is the cumulative reward of the t-th episode 
k=1 ∇θ ln πθ(ak|sk)
is the likelihood ratio derivative. In the setting considered in the paper  an episode is the trajectory
between two visits to the recurrent state s∗. For example  the t-th episode refers to the trajectory
between the (t-1)-th and the t-th visits to s∗. We denote by τt the length of this episode.
However  it is not possible to compute an unbiased estimate of J(θ)∇θJ(θ) without having access to
a generative model of the environment that allows us to sample at least two next states s(cid:48) for each
state-action pair (s  a). As also noted by Tamar et al. [2012] and Prashanth and Ghavamzadeh [2013] 
computing an unbiased estimate of J(θ)∇θJ(θ) requires double sampling (sampling from two
different trajectories)  and thus  cannot be done using a single trajectory. To circumvent the double-
sampling problem  these papers proposed multi-time-scale stochastic approximation algorithms  the
former a policy gradient algorithm and the latter an actor-critic algorithm that uses simultaneous
perturbation methods [Bhatnagar et al.  2013]. However  as discussed in Section 2.1  multi-time-scale
stochastic approximation approach suffers from several weaknesses such as no available ﬁnite-sample
analysis and difﬁcult-to-tune stepsize schedules. To overcome these weaknesses  we reformulate the
mean-variance objective function and use it to present novel algorithms with in-depth analysis in the
rest of the paper.

3.2 Block Coordinate Reformulation

In this section  we present a new formulation for Jλ(θ) that is later used to derive our algorithms
and do not suffer from the double-sampling problem in estimating J(θ)∇θJ(θ). We begin with the
following lemma.
Lemma 1. For the quadratic function f (z) = z2  z ∈ R  we deﬁne its Legendre-Fenchel dual as
f (z) = z2 = maxy∈R(2zy − y2).
This is a special case of the Lengendre-Fenchel duality [Boyd and Vandenberghe  2004] that has
been used in several recent RL papers (e.g.  Liu et al. 2015; Du et al. 2017; Liu et al. 2018). Let
4λ2 − ζ. Since λ > 0 is a constant 
2λ  we may

(cid:1)2− M (θ)  which follows Fλ(θ) = Jλ(θ)

maximizing Jλ(θ) is equivalent to maximizing Fλ(θ). Using Lemma 1 with z = J(θ) + 1
reformulate Fλ(θ) as

Fλ(θ) :=(cid:0)J(θ) + 1

λ + 1

2λ

Using (4)  the maximization problem maxθ Fλ(θ) is equivalent to

(cid:16)

y

1
2λ

2y(cid:0)J(θ) +
ˆfλ(θ  y) := 2y(cid:0)J(θ) +

ˆfλ(θ  y) 

(cid:1) − y2(cid:17) − M (θ).
(cid:1) − y2 − M (θ).

1
2λ

Fλ(θ) = max

max
θ y

where

(4)

(5)

Our optimization problem is now formulated as the standard nonconvex coordinate ascent problem (5).
We use three stochastic solvers to solve (5): SBMD method [Dang and Lan  2015]  BSG method [Xu
and Yin  2015]  and the vanilla stochastic gradient ascent (SGA) method [Nemirovski et al.  2009].
We report our BSG-based algorithm in Section 3.3 and leave the details of the SBMD and SGA based
algorithms to Appendix C. In the following sections  we denote by βθ
t the stepsizes of θ and
y  respectively  and by the subscripts t and k the episode and time-step numbers.

t and βy

4

3.3 Mean-Variance Policy Gradient

We now present our main algorithm that is based on a block coordinate update to maximize (5). Let
t and gy
gθ

t be block gradients and ˜gθ

− 2yt
t = E[˜gy
gy
t ] = 2yt+1∇θJ(θt) − ∇θM (θt)
t = E[˜gθ
gθ

t ] = 2J(θt) +

t and ˜gy
t be their sample-based estimations deﬁned as
1
λ

˜gy
t = 2Rt +

− 2yt 

1
λ

 

(cid:16)

2yt+1Rt − (Rt)2(cid:17)

(6)

(7)

 

˜gθ
t =

ωt(θt).

The block coordinate updates are

yt+1 =yt + βy
θt+1 =θt + βθ
t   we shall update y (to obtain yt+1) prior to computing gθ

t ˜gy
t  
t ˜gθ
t .

To obtain unbiased estimates of gy
t at
each iteration. Now it is ready to introduce the Mean-Variance Policy Gradient (MVP) Algorithm 1.
Before presenting our theoretical analysis  we ﬁrst introduce the assumptions needed for these results.

t and gθ

Algorithm 1 Mean-Variance Policy Gradient (MVP)
1: Input: Stepsizes {βθ

t } and {βy

t }  and number of iterations N

t } satisfy the Robbins-Monro condition

t are set to be constants

t } and {βy
t and βy
2: for episode t = 1  . . .   N do
3:
4:
5:
6:
7:

Option I: {βθ
Option II: βθ
Generate the initial state s1 ∼ P0
while sk (cid:54)= s∗ do
end while
Update the parameters

Take the action ak ∼ πθt(a|sk) and observe the reward rk and next state sk+1

rk

τt(cid:88)
τt(cid:88)

k=1

k=1

Rt =

ωt(θt) =

∇θ ln πθt(ak|sk)
(cid:18)
(cid:19)
2yt+1Rt − (Rt)2(cid:17)
(cid:16)

− 2yt

2Rt +

1
λ

yt+1 =yt + βy
t

θt+1 =θt + βθ
t

ωt(θt)

8: end for
9: Output ¯xN :

Option I: Set ¯xN = xN = [θN   yN ](cid:62)
Option II: Set ¯xN = xz = [θz  yz](cid:62)  where z is uniformly drawn from {1  2  . . .   N}

Assumption 1 (Bounded Gradient and Variance). There exist constants G and σ such that

t (cid:107)2

(cid:107)∇y
E[(cid:107)∆y

ˆfλ(x)(cid:107)2 ≤ G  (cid:107)∇θ
2] ≤ σ2  E[(cid:107)∆θ

ˆfλ(x)(cid:107)2 ≤ G 
2] ≤ σ2 
t(cid:107)2
for any t and x  where (cid:107) · (cid:107)2 denotes the Euclidean norm  ∆y
t := ˜gy
Assumption 1 is standard in nonconvex coordinate descent algorithms [Xu and Yin  2015; Dang and
Lan  2015]. We also need the following assumption that is standard in the policy gradient literature.
Assumption 2 (Ergodicity). The Markov chains induced by all the policies generated by the algo-
rithm are ergodic  i.e.  irreducible  aperiodic  and recurrent.

t − gθ
t .

t and ∆θ

t − gy

t := ˜gθ

In practice  we can choose either Option I with the result of the ﬁnal iteration as output or Option II
with the result of a randomly selected iteration as output. In what follows in this section  we report an

5

asymptotic convergence analysis of MVP with Option I  and in Section 4  we derive a ﬁnite-sample
analysis of MVP with Option II.

Theorem 1 (Asymptotic Convergence). Let(cid:8)xt = (θt  yt)(cid:9) be the sequence of the outputs gener-
satisfying the Robbins-Monro condition  i.e. (cid:80)∞
and(cid:80)∞

t } are time-diminishing real positive sequences
t = ∞ 
t=1 βθ

t )2 < ∞  then Algorithm 1 will converge such that limt→∞ E[(cid:107)∇ ˆfλ(xt)(cid:107)2] = 0.

ated by Algorithm 1 with Option I. If {βθ

t = ∞ (cid:80)∞

< ∞ (cid:80)∞

t } and {βy

t=1 (βθ
t )

t=1 βy

t=1 (βy

2

The proof of Theorem 1 follows from the analysis in Xu and Yin [2013]. Due to space constraint  we
report it in Appendix A.
Algorithm 1 is a special case of nonconvex block stochastic gradient (BSG) methods. To the best of
our knowledge  no ﬁnite-sample analysis has been reported for this class of algorithms. Motivated
by the recent papers by Nemirovski et al. [2009]  Ghadimi and Lan [2013]  Xu and Yin [2015] 
and Dang and Lan [2015]  in Section 4  we provide a ﬁnite-sample analysis for general nonconvex
block stochastic gradient methods and apply it to Algorithm 1 with Option II.

4 Finite-Sample Analysis
In this section  we ﬁrst present a ﬁnite-sample analysis for the general class of nonconvex BSG
algorithms [Xu and Yin  2013]  for which there are no established results  in Section 4.1. We then
use these results and prove a ﬁnite-sample bound for our MVP algorithm with Option II  that belongs
to this class  in Section 4.2. Due to space constraint  we report the detailed proofs in Appendix A.

4.1 Finite-Sample Analysis of Nonconvex BSG Algorithms

In this section  we provide a ﬁnite-sample analysis of the general nonconvex block stochastic gradient
(BSG) method  where the problem formulation is given by

f (x) = Eξ[F (x  ξ)].

min
x∈Rn

t : i = 1 ···   b}∞

t=1 are denoted as the stepsizes. Also  let βmax

where xi ∈ Rni denotes the i-th block of variables  and(cid:80)b

(8)
ξ is a random vector  and F (·  ξ) : Rn → R is continuously differentiable and possibly nonconvex
for every ξ. The variable x ∈ Rn can be partitioned into b disjoint blocks as x = {x1  x2  . . .   xb} 
i=1 ni = n. For simplicity  we use
x<i for (xi  . . .   xi−1)  and x≤i x>i  and x≥i are deﬁned correspondingly. We also use ∇xi to
denote ∂
∂xi for the partial gradient with respect to xi. Ξt is the sample set generated at t-th iteration 
and Ξ[t] = (Ξ1  . . .   Ξt) denotes the history of sample sets from the ﬁrst through t-th iteration.
{βi
t.
t = mini βi
Similar to Algorithm 1  the BSG algorithm cyclically updates all blocks of variables in each iteration 
and the detailed algorithm for BSG method is presented in Appendix B.
Without loss of generality  we assume a ﬁxed update order in the BSG algorithm. Let Ξt =
{ξt 1  . . .   ξt mt} be the samples in the t-th iteration with size mt ≥ 1. Therefore  the stochastic
; ξt l). Similar to Section 3  we deﬁne
partial gradient is computed as ˜gi
t = ∇xif (x<i
t − gi
t. We assume that the objective
gi
function f is bounded and Lipschitz smooth  i.e.  there exists a positive Lipschitz constant L > 0
such that (cid:107)∇xif (x) − ∇xif (y)(cid:107)2 ≤ L(cid:107)x − y(cid:107)2  ∀i ∈ {1  . . .   b} and ∀x  y ∈ Rn. Each block
gradient of f is also bounded  i.e.  there exist a positive constant G such that (cid:107)∇xif (x)(cid:107)2 ≤ G 
for any i ∈ {1  . . .   b} and any x ∈ Rn. We also need Assumption 1 for all block variables  i.e. 
E[(cid:107)∆i
Lemma 2. For any i and t  there exist a positive constant A  such that

(cid:80)mt
l=1 ∇xiF (x<i
≥i
t )  and the approximation error as ∆i

t(cid:107)2] ≤ σ  for any i and t. Then we have the following lemma.

t = maxi βi

≥i
t+1  x
t

t  and βmin

t = 1
mt

t = ˜gi

t+1  x

(cid:107)E[∆i

t|Ξ[t−1]](cid:107)2 ≤ Aβmax

t

.

(9)

The proof of Lemma 2 is in Appendix B. It should be noted that in practice  it is natural to take the
ﬁnal iteration’s result as the output as in Algorithm 1. However  a standard strategy for analyzing
nonconvex optimization methods is to pick up one previous iteration’s result randomly according to a
discrete probability distribution over {1  2  . . .   N} [Nemirovski et al.  2009; Ghadimi and Lan  2013;

6

Dang and Lan  2015]. Similarly  our ﬁnite-sample analysis is based on the strategy that randomly
pick up ¯xN = xz according to

Pr(z = t) =

(cid:80)N

t − L
βmin
t=1(βmin

t − L

2 (βmax

)2
t
2 (βmax

t

  t = 1  . . .   N.

)2)

(10)

Now we provide the ﬁnite-sample analysis result for the general nonconvex BSG algorithm as in [Xu
and Yin  2015].
Theorem 2. Let the output of the nonconvex BSG algorithm be ¯xN = xz according to Eq. (10). If
stepsizes satisfy 2βmin

t > L(βmax

E(cid:2)(cid:107)∇f (¯xN )(cid:107)2

t

)2 for t = 1 ···   N  then we have
t=1(βmax
2 (βmax

(cid:3) ≤ f (x1) − f∗ +(cid:80)N
(cid:80)N
(cid:113)(cid:80)
j<i(G2 + σ2) + b(cid:0)AG + L
)(cid:80)b

t − L

)2Ct
)2)

t=1(βmin

i=1 L

2

 

t

t

t

2 βmax

where f∗ = minx f (x). Ct = (1 − L
G is the gradient bound  L is the Lipschitz constants  σ is the variance bound  and A is deﬁned in
Eq. (9).
√
As a special case  we discuss the convergence rate with constant stepsizes O(1/
N ) in Corollary 1 
which implies that the sample complexity N = O(1/ε2) in order to ﬁnd ε-stationary solution of
problem (8).
Corollary 1. If we take constant stepsize such that βi

√
t = βi = O(1/

N ) for any t  and let

βmax := maxi βi  βmin := mini βi  then we have E(cid:2)(cid:107)∇f (¯xN )(cid:107)2
b(cid:0)AG + L
2 σ2(cid:1) .

Ct in Eq. (11) reduces to a constant C deﬁned as C = (1 − L

(cid:3) ≤ O
2 βmax)(cid:80)b

(cid:18)(cid:113) f (x1)−f∗+C
(cid:113)(cid:80)

j<i(G2 + σ2) +

  where

(cid:19)

i=1 L

N

2

(11)

2 σ2(cid:1)   where

4.2 Finite-Sample Analysis of Algorithm 1

We present the major theoretical results of this paper  i.e.  the ﬁnite-sample analysis of Algorithm 1
with Option II. The proof of Theorem 3 is in Appendix A.
t } are constants
Theorem 3. Let the output of the Algorithm 1 be ¯xN as in Theorem 2. If {βθ
as in Option II in Algorithm 1  and also satisﬁes 2βmin

)2 for t = 1 ···   N  we have

t }  {βy

t > L(βmax

t

E(cid:104)(cid:107)∇ ˆfλ(¯xN )(cid:107)2

2

(cid:105) ≤ ˆf∗

λ − ˆfλ(x1) + N (βmax
t
N (βmin
2 (βmax
)2)

t − L

t

)2C

(12)

λ = maxx

ˆfλ(x)  and

where ˆf∗
C =(1 − L
2

βmax
t

)(L2βmax

t

(G2 + σ2) + L(2G2 + σ2)) + AG + Lσ2 + 2L(1 + Lβmax

t

)(3σ2 + 2G2).

Proof Sketch. The proof follows the following major steps.
(I). First  we need to prove the bound of each block coordinate gradient  i.e.  E[(cid:107)gθ
which is bounded as

t (cid:107)2

2] and E[(cid:107)gy

t (cid:107)2
2] 

t − L
2 + (cid:107)gy
(βmin
2
≤E[ ˆfλ(xt+1)] − E[ ˆfλ(xt)] + (βmax

)2)E[(cid:107)gθ

(βmax

t (cid:107)2

t (cid:107)2
2]
)2AMρ + L(βmax

t

t

t

)2σ2 + 2Lβmax

t

(βmax

t + L(βmax

t

)2)(3σ2 + 2G2).

N(cid:88)

t=1

Summing up over t  we have

t − L
(βmin
2

)2)E[(cid:107)gθ

t (cid:107)2

2 + (cid:107)gy

t (cid:107)2
2]

(βmax

t

N(cid:88)

≤ ˆf∗

λ − ˆfλ(x1) +

[(βmax

t

)2AG + L(βmax

t

)2σ2 + 2Lβmax

t

(βmax

t + L(βmax

t

)2)(3σ2 + 2G2)].

t=1

7

(a) Portfolio management domain (b) American-style option domain
Figure 1: Empirical results of the distributions of the return (cumulative rewards) random variable.
Note that markers only indicate different methods.

(c) Optimal stopping domain

(II). Next  we need to bound E[(cid:107)∇ ˆfλ(xt)(cid:107)2

E[(cid:107)∇ ˆfλ(xt)(cid:107)2

2] ≤ L2(βmax

t

t (cid:107)2
2 + (cid:107)gy
2] using E[(cid:107)gθ
(2G2 + σ2) + E[(cid:107)gθ
)2(G2 + σ2) + Lβmax

2]  which is proven to be
t (cid:107)2
2].

t (cid:107)2

t (cid:107)2

2 + (cid:107)gy

t

(III). Finally  combining (I) and (II)  and rearranging the terms  Eq. (12) can be obtained as a special
case of Theorem 2  which completes the proof.

5 Experimental Study

In this section  we evaluate our MVP algorithm with Option I in three risk-sensitive domains: the
portfolio management [Tamar et al.  2012]  the American-style option [Tamar et al.  2014]  and the
optimal stopping [Chow and Ghavamzadeh  2014; Chow et al.  2018]. The baseline algorithms are the
vanilla policy gradient (PG)  the mean-variance policy gradient in Tamar et al. [2012]  the stochastic
gradient ascent (SGA) applied to our optimization problem (5)  and the randomized coordinate ascent
policy gradient (RCPG)  i.e.  the SBMD-based version of our algorithm. Details of SGA and RCPG
can be found in Appendix C. For each algorithm  we optimize its Lagrangian parameter λ by grid
search and report the mean and variance of its return random variable as a Gaussian.2 Since the
algorithms presented in the paper (MVP and RCPG) are policy gradient  we only compare them with
Monte-Carlo based policy gradient algorithms and do not use any actor-critic algorithms  such as
those in Prashanth and Ghavamzadeh [2013] and TRPO [Schulman et al.  2015]  in the experiments.

5.1 Portfolio Management

nl or rhigh
nl

The portfolio domain Tamar et al. [2012] is composed of the liquid and non-liquid assets. A liquid
asset has a ﬁxed interest rate rl and can be sold at any time-step k ≤ τ. A non-liquid asset can be sold
only after a ﬁxed period of W time-steps with a time-dependent interest rate rnl(k)  which can take
either rlow
  and the transition follows a switching probability pswitch. The non-liquid asset also
suffers a default risk (i.e.  not being paid) with a probability prisk. All investments are in liquid assets at
the initial time-step k = 0. At the k-th step  the state is denoted by x(k) ∈ RW +2  where x1 ∈ [0  1]
is the portion of the investment in liquid assets  x2 ···   xW +1 ∈ [0  1] is the portion in non-liquid
assets with time to maturity of 1 ···   W time-steps  respectively  and xW +2(k) = rnl(k)− E[rnl(k)].
The investor can choose to invest a ﬁxed portion η (0 < η < 1) of his total available cash in the
non-liquid asset or do nothing. More details about this domain can be found in Tamar et al. [2012].
Figure 1(a) shows the results of the algorithms. PG has a large variance and the Tamar’s method has
the lowest mean return. The results indicate that MVP yields a higher mean return with less variance
compared to the competing algorithms.

5.2 American-style Option

An American-style option Tamar et al. [2014] is a contract that gives the buyer the right to buy or
sell the asset at a strike price W at or before the maturity time τ. The initial price of the option is
x0  and the buyer has bought a put option with the strike price Wput < x0 and a call option with the

2Note that the return random variables are not necessarily Gaussian  we only use Gaussian for presentation

purposes.

8

..:2: 90/#0 7/!74- -90389:3.943!'!$#!% 2 7!..:2: 90/#0 7/!74- -90389:3.943!'!$#!% 2 7!strike price Wcall > x0. At the k-th step (k ≤ τ)  the state is {xk  k}  where xk is the current price
of the option. The action ak is either executing the option or holding it. xk+1 is fuxk w.p. p and
fdxk w.p. 1 − p  where fu and fd are constants. The reward is 0 unless an option is executed and
the reward for executing an option is rk = max(0  Wput − xk) + max(0  xk − Wcall). More details
about this domain can be found in Tamar et al. [2014]. Figure 1(b) shows the performance of the
algorithms. The results suggest that MVP can yield a higher mean return with less variance compared
to the other algorithms.

5.3 Optimal Stopping

The optimal stopping problem [Chow and Ghavamzadeh  2014; Chow et al.  2018] is a continuous
state domain. At the k-th time-step (k ≤ τ  τ is the stopping time)  the state is {xk  k}  where
xk is the cost. The buyer decide either to accept the present cost or wait. If the buyer accepts or
when k = T   the system reaches a terminal state and the cost xk is received  otherwise  the buyer
receives the cost ph and the new state is {xk+1  k + 1}  where xk+1 is fuxk w.p. p and fdxk w.p.
1 − p (fu > 1 and fd < 1 are constants). More details about this domain can be found in Chow and
Ghavamzadeh [2014]. Figure 1(c) shows the performance of the algorithms. The results indicate that
MVP is able to yield much less variance without affecting its mean return. We also summarize the
performance of these algorithms in all three risk-sensitive domains as Table 1  where Std is short for
Standard Deviation.

Portfolio Management American-style Option Optimal Stopping
Mean
29.754
29.170
Tamar
28.575
SGA
29.679
RCPG 29.340

Mean
-1.4767
-1.4769
-2.8553
-1.4805
-1.4872

Mean
0.2478
0.2477
0.2240
0.2470
0.2447

Std

0.00482
0.00922
0.00694
0.00679
0.00819

Std

0.00456
0.00754
0.00415
0.00583
0.00721

MVP
PG

Std
0.325
1.177
0.857
0.658
0.789

Table 1: Performance Comparison among Algorithms

6 Conclusion
This paper is motivated to provide a risk-sensitive policy search algorithm with provable sample
complexity analysis to maximize the mean-variance objective function. To this end  the objective
function is reformulated based on the Legendre-Fenchel duality  and a novel stochastic block co-
ordinate ascent algorithm is proposed with in-depth analysis. There are many interesting future
directions on this research topic. Besides stochastic policy gradient  deterministic policy gradient
[Silver et al.  2014] has shown great potential in large discrete action space. It is interesting to design
a risk-sensitive deterministic policy gradient method. Secondly  other reformulations of the mean-
variance objective function are also worth exploring  which will lead to new families of algorithms.
Thirdly  distributional RL [Bellemare et al.  2016] is strongly related to risk-sensitive policy search 
and it is interesting to investigate the connections between risk-sensitive policy gradient methods
and distributional RL. Last but not least  it is interesting to test the performance of the proposed
algorithms together with other risk-sensitive RL algorithms on highly-complex risk-sensitive tasks 
such as autonomous driving problems and other challenging tasks.

Acknowledgments

Bo Liu  Daoming Lyu  and Daesub Yoon were partially supported by a grant (18TLRP-B131486-
02) from Transportation and Logistics R&D Program funded by Ministry of Land  Infrastructure
and Transport of Korean government. Yangyang Xu was partially supported by the NSF grant
DMS-1719549.

9

References
Beck  A. and Teboulle  M. (2003). Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31:167–175.

Bellemare  M. G.  Dabney  W.  and Munos  R. (2016). A distributional perspective on reinforcement

learning. In International Conference on Machine Learning.

Bertsekas  D. P. (1999). Nonlinear programming. Athena scientiﬁc Belmont.

Bhatnagar  S.  Prasad  H.  and Prashanth  L. (2013). Stochastic Recursive Algorithms for Optimization 

volume 434. Springer.

Borkar  V. (2002). Q-learning for risk-sensitive control. Mathematics of operations research 

27(2):294–311.

Borkar  V. (2008). Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University

Press.

Boyd  S. and Vandenberghe  L. (2004). Convex Optimization. Cambridge University Press.

Chow  Y. and Ghavamzadeh  M. (2014). Algorithms for CVaR optimization in MDPs. In Advances

in Neural Information Processing Systems  pages 3509–3517.

Chow  Y.  Ghavamzadeh  M.  Janson  L.  and Pavone  M. (2018). Risk-constrained reinforcement

learning with percentile risk criteria. Journal of Machine Learning Research.

Dai  B.  He  N.  Pan  Y.  Boots  B.  and Song  L. (2017). Learning from conditional distributions via
dual embeddings. In The 20th International Conference on Artiﬁcial Intelligence and Statistics.

Dalal  G.  Thoppe  G.  Sz¨or´enyi  B.  and Mannor  S. (2018). Finite sample analysis of two-timescale
stochastic approximation with applications to reinforcement learning. In Proceedings of the 31st
Conference On Learning Theory  pages 1199–1233. PMLR.

Dang  C. D. and Lan  G. (2015). Stochastic block mirror descent methods for nonsmooth and

stochastic optimization. SIAM Journal on Optimization  25(2):856–881.

Du  S. S.  Chen  J.  Li  L.  Xiao  L.  and Zhou  D. (2017). Stochastic variance reduction methods for

policy evaluation. arXiv preprint arXiv:1702.07944.

Ghadimi  S. and Lan  G. (2013). Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368.

Hastie  T.  Tibshirani  R.  and Friedman  J. (2001). The Elements of Statistical Learning. Springer.

Lai  T.  Xing  H.  and Chen  Z. (2011). Mean-variance portfolio optimization when means and

covariances are unknown. The Annals of Applied Statistics  pages 798–823.

Li  D. and Ng  W. (2000). Optimal dynamic portfolio selection: Multiperiod mean-variance formula-

tion. Mathematical Finance  10(3):387–406.

Liu  B.  Gemp  I.  Ghavamzadeh  M.  Liu  J.  Mahadevan  S.  and Petrik  M. (2018). Proximal
gradient temporal difference learning: Stable reinforcement learning with polynomial sample
complexity. Journal of Artiﬁcial Intelligence Research.

Liu  B.  Liu  J.  Ghavamzadeh  M.  Mahadevan  S.  and Petrik  M. (2015). Finite-sample analysis of

proximal gradient td algorithms. In Conference on Uncertainty in Artiﬁcial Intelligence.

Luo  Z. and Tseng  P. (1992). On the convergence of the coordinate descent method for convex

differentiable minimization. Journal of Optimization Theory and Applications  72(1):7–35.

Mairal  J. (2013). Stochastic majorization-minimization algorithms for large-scale optimization. In

Advances in Neural Information Processing Systems  pages 2283–2291.

Mannor  S. and Tsitsiklis  J. (2011). Mean-variance optimization in markov decision processes. In

Proceedings of the 28th International Conference on Machine Learning (ICML-11).

10

Markowitz  H. M.  Todd  G. P.  and Sharpe  W. F. (2000). Mean-variance analysis in portfolio choice

and capital markets  volume 66. John Wiley & Sons.

Maurer  M.  Gerdes  C.  Lenz  B.  and Winner  H. (2016). Autonomous driving: technical  legal and

social aspects. Springer.

Nemirovski  A.  Juditsky  A.  Lan  G.  and Shapiro  A. (2009). Robust stochastic approximation

approach to stochastic programming. SIAM Journal on optimization  19(4):1574–1609.

Nesterov  Y. (2012). Efﬁciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization  22(2):341–362.

Parker  D. (2009). Managing risk in healthcare: understanding your safety culture using the manch-

ester patient safety framework. Journal of nursing management  17(2):218–222.

Prashanth  L. A. and Ghavamzadeh  M. (2013). Actor-critic algorithms for risk-sensitive mdps. In

Advances in Neural Information Processing Systems  pages 252–260.

Prashanth  L. A. and Ghavamzadeh  M. (2016). Variance-constrained actor-critic algorithms for

discounted and average reward mdps. Machine Learning Journal  105(3):367–417.

Puterman  M. L. (1994). Markov Decision Processes. Wiley Interscience  New York  USA.
Razaviyayn  M.  Hong  M.  and Luo  Z. (2013). A uniﬁed convergence analysis of block successive
minimization methods for nonsmooth optimization. SIAM Journal on Optimization  23(2):1126–
1153.

Schulman  J.  Levine  S.  Abbeel  P.  Jordan  M.  and Moritz  P. (2015). Trust region policy optimiza-

tion. In International Conference on Machine Learning  pages 1889–1897.

Silver  D.  Lever  G.  Heess  N.  Degris  T.  Wierstra  D.  and Riedmiller  M. (2014). Deterministic

policy gradient algorithms. In ICML  pages 387–395.

Sobel  M. J. (1982). The variance of discounted markov decision processes. Journal of Applied

Probability  19(04):794–802.

Sutton  R. and Barto  A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Tamar  A.  Castro  D.  and Mannor  S. (2012). Policy gradients with variance related risk criteria. In

ICML  pages 935–942.

Tamar  A.  Chow  Y.  Ghavamzadeh  M.  and Mannor  S. (2015a). Policy gradient for coherent risk

measures. In NIPS  pages 1468–1476.

Tamar  A.  Glassner  Y.  and Mannor  S. (2015b). Optimizing the cvar via sampling. In AAAI

Conference on Artiﬁcial Intelligence.

Tamar  A.  Mannor  S.  and Xu  H. (2014). Scaling up robust mdps using function approximation. In

International Conference on Machine Learning  pages 181–189.

Tseng  P. (2001). Convergence of a block coordinate descent method for nondifferentiable minimiza-

tion. Journal of optimization theory and applications  109(3):475–494.

Wang  M.  Fang  E. X.  and Liu  H. (2017). Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions. Mathematical Programming  161(1-
2):419–449.

Williams  R. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning  8(3-4):229–256.

Wright  S. (2015). Coordinate descent algorithms. Mathematical Programming  151(1):3–34.
Xu  Y. and Yin  W. (2013). A block coordinate descent method for regularized multiconvex opti-
mization with applications to nonnegative tensor factorization and completion. SIAM Journal on
imaging sciences  6(3):1758–1789.

Xu  Y. and Yin  W. (2015). Block stochastic gradient iteration for convex and nonconvex optimization.

SIAM Journal on Optimization  25(3):1686–1716.

11

,Tengyang Xie
Bo Liu
Yangyang Xu
Mohammad Ghavamzadeh
Yinlam Chow
Daoming Lyu
Daesub Yoon