2017,Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes,This work constructs a hypothesis test for detecting whether an data-generating function $h: \real^p \rightarrow \real$ belongs to a specific reproducing kernel Hilbert space $\mathcal{H}_0$  where the structure of $\mathcal{H}_0$ is only partially known. Utilizing the theory of reproducing kernels  we reduce this hypothesis to a simple one-sided score test for a scalar parameter  develop a testing procedure that is robust against the mis-specification of kernel functions  and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method  we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test  under different data-generating functions and estimation strategies for the null model. Our results revealed interesting connection between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test)  and also highlighted unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.,Robust Hypothesis Test for Nonlinear Effect

with Gaussian Processes

Jeremiah Zhe Liu  Brent Coull

Department of Biostatistics

Harvard University

Cambridge  MA 02138

{zhl112@mail  bcoull@hsph}.harvard.edu

Abstract

This work constructs a hypothesis test for detecting whether an data-generating
function h : Rp → R belongs to a speciﬁc reproducing kernel Hilbert space
H0  where the structure of H0 is only partially known. Utilizing the theory of
reproducing kernels  we reduce this hypothesis to a simple one-sided score test
for a scalar parameter  develop a testing procedure that is robust against the mis-
speciﬁcation of kernel functions  and also propose an ensemble-based estimator
for the null model to guarantee test performance in small samples. To demonstrate
the utility of the proposed method  we apply our test to the problem of detecting
nonlinear interaction between groups of continuous features. We evaluate the
ﬁnite-sample performance of our test under different data-generating functions and
estimation strategies for the null model. Our results reveal interesting connections
between notions in machine learning (model underﬁt/overﬁt) and those in statistical
inference (i.e. Type I error/power of hypothesis test)  and also highlight unexpected
consequences of common model estimating strategies (e.g. estimating kernel
hyperparameters using maximum likelihood estimation) on model inference.

1

Introduction

We study the problem of constructing a hypothesis test for an unknown data-generating function h :
Rp → R  when h is estimated with a black-box algorithm (speciﬁcally  Gaussian Process regression)
from n observations {yi  xi}n

i=1. Speciﬁcally  we are interested in testing for the hypothesis:
H0 : h ∈ H0

Ha : h ∈ Ha

v.s.

where H0 Ha are the function spaces for h under the null and the alternative hypothesis. We assume
only partial knowledge about H0. For example  we may assume H0 = {h|h(xi) = h(xi 1)} is the
space of functions depend only on x1  while claiming no knowledge about other properties (linearity 
smoothness  etc) about h. We pay special attention to the setting where the sample size n is small.
This type of tests carries concrete signiﬁcance in scientiﬁc studies.
In areas such as genetics 
drug trials and environmental health  a hypothesis test for feature effect plays a critical role in
answering scientiﬁc questions of interest. For example  assuming for simplicity x2×1 = [x1  x2]T  
an investigator might inquire the effect of drug dosage x1 on patient’s biometric measurement y
(corresponds to the null hypothesis H0 = {h(x) = h(x2)})  or whether the adverse health effect
of air pollutants x1 is modiﬁed by patients’ nutrient intake x2 (corresponds to the null hypothesis
H0 = {h(x) = h1(x1) + h2(x2)}). In these studies  h usually represents some complex  nonlinear
biological process whose exact mathematical properties are not known. Sample size in these studies
are often small (n ≈ 100 − 200)  due to the high monetary and time cost in subject recruitment and
the lab analysis of biological samples.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

hand  Gaussian process (GP) [16] models h(xi) =(cid:80)n

There exist two challenges in designing such a test. The ﬁrst challenge arises from the low inter-
pretability of the blackbox model. It is difﬁcult to formulate a hypothesis about feature effect in
these models  since the blackbox models represents ˆh implicitly using a collection of basis functions
constructed from the entire feature vector x  rather than a set of model parameters that can be
interpreted in the context of some effect of interest. For example  consider testing for the interaction
effect between x1 and x2. With linear model h(xi) = xi1β1 + xi2β2 + xi1xi2β3  we can simply
represent the interaction effect using a single parameter β3  and test for H0 : β3 = 0. On the other
j=1 k(xi  xj)αj using basis functions deﬁned
by the kernel function k. Since k is an implicit function that takes the entire feature vector as input  it
is not clear how to represent the interaction effect in GP models. We address this challenge assuming
h belongs to a reproducing kernel Hilbert space (RKHS) governed by the kernel function kδ  such
that H = H0 when δ = 0  and H = Ha otherwise. In this way  δ encode exactly the feature effect
of interest  and the null hypothesis h ∈ H0 can be equivalently stated as H0 : δ = 0. To test for the
hypothesis  we re-formulate the GP estimates as the variance components of a linear mixed model
(LMM) [13]  and derive a variance component score test which requires only model estimates under
the null hypothesis.
Clearly  performance of the hypothesis test depends on the quality of the model estimate under the
null hypothesis  which give rise to the second challenge: estimating the null model when only having
partial knowledge about H0. In the case of Gaussian process  this translates to only having partial
knowledge about the kernel function k0. The performance of Gaussian process is sensitive to the
choices of the kernel function k(z  z(cid:48)). In principle  the RKHS H generated by a proper kernel
function k(z  z(cid:48)) should be rich enough so it contains the data-generating function h  yet restrictive
enough such that ˆh does not overﬁt in small samples. Choosing a kernel function that is too restrictive
or too ﬂexible will lead to either model underﬁt or overﬁt  rendering the subsequent hypothesis
tests not valid. We address this challenge by proposing an ensemble-based estimator for h we term
Cross-validated Kernel Ensemble (CVEK). Using a library of base kernels  CVEK learns a proper H
from data by directly minimizing the ensemble model’s cross-validation error  therefore guaranteeing
robust test performance for a wide range of data-generating functions.
The rest of the paper is structured as follows. After a brief review of Gaussian process and its
connection with linear mixed model in Section 2  we introduce the test procedure for general
hypothesis h ∈ H0 in Section 3  and its companion estimation procedure CVEK in Section 4. To
demonstrate the utility of the proposed test  in section 5  we adapt our test to the problem of detecting
nonlinear interaction between groups of continuous features  and in section 6 we conduct simulation
studies to evaluate the ﬁnite-sample performance of the interaction test  under different kernel
estimation strategies  and under a range of data-generating functions with different mathematical
properties. Our simulation study reveals interesting connection between notions in machine learning
and those in statistical inference  by elucidating the consequence of model estimation (underﬁt /
overﬁt) on the Type I error and power of the subsequent hypothesis test. It also cautions against the
use of some common estimation strategies (most notably  selecting kernel hyperparameters using
maximum likelihood estimation) when conducting hypothesis test in small samples  by highlighting
inﬂated Type I errors from hypothesis tests based on the resulting estimates. We note that the methods
and conclusions from this work is extendable beyond the Gaussian Process models  due to GP’s
connection to other blackbox models such as random forest [5] and deep neural network [19].

2 Background on Gaussian Process

Assume we observe data from n independent subjects. For the ith subject  let yi be a continuous
response  xi be the set of p continuous features that has nonlinear effect on yi. We assume that the
outcome yi depends on features xi through below data-generating model:
iid∼ N (0  λ)

(1)
and h : Rp → R follows the Gaussian process prior GP(0  k) governed by the positive deﬁnite kernel
function k  such that the function evaluated at the observed record follows the multivariate normal
(MVN) distribution:

yi|h = µ + h(xi) + i

where i

h = [h(x1)  . . .   h(xn)] ∼ M V N (0  K)

2

with covariance matrix Ki j = k(xi  xj). Under above construction  the predictive distribution of h
evaluated at the samples is also multivariate normal:

h|{yi  xi}n

i=1 ∼ M V N (h∗  K∗)
h∗ = K(K + λI)−1(y − u)
K∗ = K − K(K + λI)−1K

To understand the impact of λ and k on h∗  recall that Gaussian process can be understood as the
Bayesian version of the kernel machine regression  where h∗ equivalently arise from the below
optimization problem:

h∗ = argmin
h∈Hk

||y − µ − h(x)||2 + λ||h||2H

where Hk is the RKHS generated by kernel function k. From this perspective  h∗ is the element
in a spherical ball in Hk that best approximates the observed data y. The norm of h∗  ||h||2H  is
constrained by the tuning parameter λ  and the mathematical properties (e.g. smoothness  spectral
density  etc) of h∗ are governed by the kernel function k. It should be noticed that although h∗ may
arise from Hk  the probability of the Gaussian Process h ∈ Hk is 0 [14].
Gaussian Process as Linear Mixed Model
[13] argued that if deﬁne τ = σ2

λ   h∗ can arise exactly from a linear mixed model (LMM):

y = µ + h + 

where

h ∼ N (0  τ K)

 ∼ N (0  σ2I)

(2)

Therefore λ can be treated as part of the LMM’s variance components parameters. If K is cor-
rectly speciﬁed  then the variance components parameters (τ  σ2) can be estimated unbiasedly by
maximizing the Restricted Maximum Likelihood (REML)[12]:

LREML(µ  τ  σ2|K) = −log|V| − log|1T V−11| − (y − µ)T V−1(y − µ)

(3)
where V = τ K + σ2I  and 1 a n × 1 vector whose all elements are 1. However  it is worth noting
that REML is a model-based procedure. Therefore improper estimates for λ = σ2
τ may arise when
the family of kernel functions are mis-speciﬁed.
3 A recipe for general hypothesis h ∈ H0
The GP-LMM connection introduced in Section 2 opens up the arsenal of statistical tools from Linear
Mixed Model for inference tasks in Gaussian Process. Here  we use the classical variance component
test [12] to construct a testing procedure for the hypothesis about Gaussian process function:

H0 : h ∈ H0.

(4)
We ﬁrst translate above hypothesis into a hypothesis in terms of model parameters. The key of our
approach is to assume that h lies in a RKHS generated by a garrote kernel function kδ(z  z(cid:48)) [15] 
which is constructed by including an extra garrote parameter δ to a given kernel function. When
generates exactly H0  the space of
δ = 0  the garrote kernel function k0(x  x(cid:48)) = kδ(x  x(cid:48))
functions under the null hypothesis. In order to adapt this general hypothesisio to their hypothesis of
interest  practitioners need only to specify the form of the garrote kernel so that H0 corresponds to
the null hypothesis. For example  If kδ(x) = k(δ ∗ x1  x2  . . .   xp)  δ = 0 corresponds to the null
hypothesis H0 : h(x) = h(x2  . . .   xp)  i.e. the function h(x) does not depend on x1. (As we’ll see
in section 5  identifying such k0 is not always straightforward). As a result  the general hypothesis
(4) is equivalent to

(cid:12)(cid:12)(cid:12)δ=0

H0 : δ = 0.

(5)

We now construct a test statistic ˆT0 for (5) by noticing that the garrote parameter δ can be treated as a
variance component parameter in the linear mixed model. This is because the Gaussian process under
garrote kernel can be formulated into below LMM:

y = µ + h + 

where

h ∼ N (0  τ Kδ)

 ∼ N (0  σ2I)

3

where Kδ is the kernel matrix generated by kδ(z  z(cid:48)). Consequently  we can derive a variance
component test for H0 by calculating the square derivative of LREML with respect δ under H0 [12]:

ˆT0 = ˆτ ∗ (y − ˆµ)T V−1

0

∂K0

0 (y − ˆµ)
V−1

(6)

where V0 = ˆσ2I + ˆτ K0. In this expression  K0 = Kδ

  and ∂K0 is the null derivative kernel

(cid:104)

(cid:105)
(cid:12)(cid:12)(cid:12)δ=0

(cid:17) ∗ ˆT(cid:104)

(cid:105)

.

(cid:12)(cid:12)(cid:12)δ=0
(cid:16) ˆτ

matrix whose (i  j)th entry is ∂

∂δ kδ(x  x(cid:48))

As discussed previously  misspecifying the null kernel function k0 negatively impacts the performance
of the resulting hypothesis test. To better understand the mechanism at play  we express the test
statistic ˆT0 from (6) in terms of the model residual ˆ = y − ˆµ − ˆh:

ˆT0 =

ˆ 

ˆσ4

∂K0

(7)
0 (y − ˆµ) = (ˆσ2)−1(ˆ) [10]. As shown  the test statistic ˆT0 is a
where we have used the fact V−1
scaled quadratic-form statistic that is a function of the model residual. If k0 is too restrictive  model
estimates will underﬁt the data even under the null hypothesis  introducing extraneous correlation
among the ˆi’s  therefore leading to overestimated ˆT0 and eventually underestimated p-value under
the null. In this case  the test procedure will frequently reject the null hypothesis (i.e. suggest the
existence of nonlinear interaction) even when there is in fact no interaction  yielding an invalid test
due to inﬂated Type I error. On the other hand  if k0 is too ﬂexible  model estimates will likely
overﬁt the data in small samples  producing underestimated residuals  an underestimated test statistic 
and overestimated p-values. In this case  the test procedure will too frequently fail to reject the
null hypothesis (i.e. suggesting there is no interaction) when there is in fact interaction  yielding an
insensitive test with diminished power.
The null distribution of ˆT can be approximated using a scaled chi-square distribution κχ2
Satterthwaite method [20] by matching the ﬁrst two moments of T :

ν using

κ ∗ ν = E(T ) 
with solution (see Appendix for derivation):

(cid:104)

(cid:16)

2 ∗ κ2 ∗ ν = V ar(T )

(cid:17)(cid:105)

(cid:104)

(cid:16)

ˆκ = ˆIδδ/

ˆτ ∗ tr

V−1

0 ∂K0

ˆν =

ˆτ ∗ tr

V−1

0 ∂K0

(cid:17)(cid:105)2

/(2 ∗ ˆIδδ)

where ˆIδθ and ˆIδθ are the submatrices of the REML information matrix. Numerically more accurate 
but computationally less efﬁcient approximation methods are also available [2].
Finally  the p-value of this test is calculated by examining the tail probability of ˆκχ2
ˆν:

p = P (ˆκχ2

ˆν > ˆT ) = P (χ2

ˆν > ˆT /ˆκ)

A complete summary of the proposed testing procedure is available in Algorithm 1.
Algorithm 1 Variance Component Test for h ∈ H0
1: procedure VCT FOR INTERACTION

Input: Null Kernel Matrix K0  Derivative Kernel Matrix ∂K0  Data (y  X)
Output: Hypothesis Test p-value p
# Step 1: Estimate Null Model using REML
( ˆµ  ˆτ   ˆσ2) = argmin LREML(µ  τ  σ2|K0) as in (3)
(cid:17)(cid:105)
(cid:104)
(cid:16)
0 (y − Xˆβ)
ˆT0 = ˆτ ∗ (y − Xˆβ)T V−1
0 ∂K0 V−1
ˆκ = ˆIδθ/
ˆν =
0 ∂K0

0 ∂K0
# Step 3: Compute p-value and reach conclusion

/(2 ∗ ˆIδθ)

(cid:17)(cid:105)2

ˆτ ∗ tr

ˆτ ∗ tr

V−1

V−1

(cid:16)

(cid:104)

 

# Step 2: Compute Test Statistic and Null Distribution Parameters

2:

3:

4:

p = P (ˆκχ2
5:
6: end procedure

ˆν > ˆT ) = P (χ2

ˆν > ˆT /ˆκ)

4

In light of the discussion about model misspeciﬁcation in Introduction section  we highlight the fact
that our proposed test (6) is robust against model misspeciﬁcation under the alternative [12]  since
the calculation of test statistics do not require detailed parametric assumption about kδ. However  the
test is NOT robust against model misspeciﬁcation under the null  since the expression of both test
statistic ˆT0 and the null distribution parameters (ˆκ  ˆν) still involve the kernel matrices generated by
k0 (see Algorithm 1). In the next section  we address this problem by proposing a robust estimation
procedure for the kernel matrices under the null.

4 Estimating Null Kernel Matrix using Cross-validated Kernel Ensemble

Observation in (7) motivates the need for a kernel estimation strategy that is ﬂexible so that it does
not underﬁt under the null  yet stable so that it does not overﬁt under the alternative. To this end  we
propose estimating h using the ensemble of a library of ﬁxed base kernels {kd}D
2 = 1} 

u ∈ ∆ = {u|u ≥ 0 ||u||2

D(cid:88)

ud

ˆhd(x)

ˆh(x) =

d=1:

(8)

d=1

where ˆhd is the kernel predictor generated by dth base kernel kd. In order to maximize model stability 
the ensemble weights u are estimated to minimize the overall cross-validation error of ˆh. We term
this method the Cross-Validated Kernel Ensemble (CVEK). Our proposed method belongs to the
well-studied family of algorithms known as ensembles of kernel predictors (EKP) [7  8  3  4]  but with
specialized focus in maximizing the algorithm’s cross-validation stability. Furthermore  in addition to
producing ensemble estimates ˆh  CVEK will also produce the ensemble estimate of the kernel matrix
ˆK0 that is required by Algorithm 1. The exact algorithm proceeds in three stages as follows:
Stage 1: For each basis kernel in the library {kd}D
d=1  we ﬁrst estimate ˆhd = Kd(Kd + ˆλdI)−1y 
the prediction based on dth kernel  where the tunning parameter ˆλd is selected by minimizing the
leave-one-out cross validation (LOOCV) error [6]:

LOOCV(λ|kd) = (I − diag(Ad λ))−1(y − ˆhd λ) where Ad λ = Kd(Kd + λI)−1.

(9)

We denote estimate the ﬁnal LOOCV error for dth kernel ˆd = LOOCV
Stage 2: Using the estimated LOOCV errors {ˆd}D
such that it minimizes the overall LOOCV error:

d=1  estimate the ensemble weights u = {ud}D

d=1

(cid:16)ˆλd|kd

(cid:17)

.

ˆu = argmin

u∈∆

udˆd||2

where ∆ = {u|u ≥ 0 ||u||2

2 = 1} 

|| D(cid:88)

d=1

D(cid:88)

d=1

and produce the ﬁnal ensemble prediction:

where ˆA =(cid:80)D

d=1 ˆudAd ˆλd

ˆh =

ˆudhd =

is the ensemble hat matrix.

D(cid:88)

d=1

ˆudAd ˆλd

y = ˆAy 

Stage 3: Using the ensemble hat matrix ˆA  estimate the ensemble kernel matrix ˆK by solving:

ˆK( ˆK + λI)−1 = ˆA.

Speciﬁcally  if we denote UA and {δA k}n
the form (see Appendix for derivation):

k=1 the eigenvector and eigenvalues of ˆA  then ˆK adopts

(cid:17)

(cid:16) δA k

1 − δA k

UT
A

ˆK = UAdiag

5 Application: Testing for Nonlinear Interaction
Recall in Section 3  we assume that we are given a kδ that generates exactly H0. However  depending
on the exact hypothesis of interest  identifying such k0 is not always straightforward. In this section 

5

we revisit the example about interaction testing discussed in challenge 1 at the Introduction section 
and consider how to build a k0 for below hypothesis of interest

H0 : h(x) = h1(x1) + h2(x2)
Ha : h(x) = h1(x1) + h2(x2) + h12(x1  x2)

where h12 is the "pure interaction" function that is orthogonal to main effect functions h1 and h2.
Recall as discussed previously  this hypothesis is difﬁcult to formulate with Gaussian process models 
since the kernel functions k(x  x(cid:48)) in general do not explicitly separate the main and the interaction
effect. Therefore rather than directly deﬁne k0  we need to ﬁrst construct H0 and Ha that corresponds
to the null and alternative hypothesis  and then identify the garrote kernel function kδ such it generates
exactly H0 when δ = 0 and Ha when δ > 0.
We build H0 using the tensor-product construction of RKHS on the product domain (x1 i  x2 i) ∈
Rp1 × Rp2 [9]  due to this approach’s unique ability in explicitly characterizing the space of "pure
interaction" functions. Let 1 = {f|f ∝ 1} be the RKHS of constant functions  and H1 H2
be the RKHS of centered functions for x1x2  respectively. We can then deﬁne the full space as
H = ⊗2
m=1(1 ⊕ Hm). H describes the space of functions that depends jointly on {x1  x2}  and
adopts below orthogonal decomposition:

H = (1 ⊕ H1) ⊗ (1 ⊕ H2)

= 1 ⊕(cid:110)H1 ⊕ H2

(cid:111) ⊕(cid:110)H1 ⊗ H2

(cid:111)

= 1 ⊕ H⊥

12 ⊕ H12

12 = H1 ⊕ H2 and H12 = H1 ⊗ H2  respectively. We see that H12 is
where we have denoted H⊥
indeed the space of“pure interaction" functions   since H12 contains functions on the product domain
Rp1 × Rp2  but is orthogonal to the space of additive main effect functions H⊥
12. To summarize  we
have identiﬁed two function spaces H0 and Ha that has the desired interpretation:

H0 = H⊥

12

Ha = H⊥

12 ⊕ H12

We are now ready to identify the garrote kernel kδ(x  x(cid:48)). To this end  we notice that both H0 and
H12 are composite spaces built from basis RKHSs using direct sum and tensor product. If denote
m) the reproducing kernel associated with Hm  we can construct kernel functions for
km(xm  x(cid:48)
composite spaces H0 and H12 as [1]:

k0(x  x(cid:48)) = k1(x1  x(cid:48)
k12(x  x(cid:48)) = k1(x1  x(cid:48)
and consequently  the garrote kernel function for Ha:

1) + k2(x2  x(cid:48)
2)
1) k2(x2  x(cid:48)
2)

kδ(x  x(cid:48)) = k0(x  x(cid:48)) + δ ∗ k12(x  x(cid:48)).

(10)

Finally  using the chosen form of the garrote kernel function  the (i  j)th element of the null derivative
∂δ kδ(x  x(cid:48)) = k12(x  x(cid:48))  i.e. the null derivative kernel matrix ∂K0 is simply
kernel matrix K0 is ∂
the kernel matrix K12 that corresponds to the interaction space. Therefore the score test statistic ˆT0
in (6) simpliﬁes to:

ˆT0 = ˆτ ∗ (y − Xˆβ)T V−1

0 K12 V−1

0 (y − Xˆβ)

(11)

where V0 = ˆσ2I + ˆτ K0.

6 Simulation Experiment

We evaluated the ﬁnite-sample performance of the proposed interaction test in a simulation study
that is analogous to a real nutrition-environment interaction study. We generate two groups of input
features (xi 1  xi 2) ∈ Rp1 × Rp2 independently from standard Gaussian distribution  representing
normalized data representing subject’s level of exposure to p1 environmental pollutants and the levels
of a subject’s intake of p2 nutrients during the study. Throughout the simulation scenarios  we keep
n = 100  and p1 = p2 = 5. We generate the outcome yi as:

yi = h1(xi 1) + h2(xi 2) + δ ∗ h12(xi 1  xi 2) + i

(12)

6

where h1  h2  h12 are sampled from RKHSs H1 H2 and H1 ⊗ H2  generated using a ground-truth
kernel ktrue. We standardize all sampled functions to have unit norm  so that δ represents the strength
of interaction relative to the main effect.
For each simulation scenario  we ﬁrst generated data using δ and ktrue as above  then selected a
kmodel to estimate the null model and obtain p-value using Algorithm 1. We repeated each scenario
1000 times  and evaluate the test performance using the empirical probability ˆP (p ≤ 0.05). Under
null hypothesis H0 : δ = 0  ˆP (p ≤ 0.05) estimates the test’s Type I error  and should be smaller or
equal to the signiﬁcance level 0.05. Under alternative hypothesis Ha : δ > 0  ˆP (p ≤ 0.05) estimates
the test’s power  and should ideally approach 1 quickly as the strength of interaction δ increases.
In this study  we varied ktrue to produce data-generating functions hδ(xi 1  xi 2) with different
smoothness and complexity properties  and varied kmodel to reﬂect different common modeling
strategies for the null model in addition to using CVEK. We then evaluated how these two aspects
impact the hypothesis test’s Type I error and power.
Data-generating Functions
We sampled the data-generate function by using ktrue from Matérn kernel family [16]:

(cid:16)√

2νσ||r||(cid:17)ν

(cid:16)√

2νσ||r||(cid:17)

Kν

 

where

r = x − x(cid:48) 

k(r|ν  σ) =

21−ν
Γ(ν)

with two non-negative hyperparameters (ν  σ). For a function h sampled using Matérn kernel  ν
determines the function’s smoothness  since h is k-times mean square differentiable if and only
if ν > k. In the case of ν → ∞  Matérn kernel reduces to the Gaussian RBF kernel which is
inﬁnitely differentiable. σ determines the function’s complexity  this is because in Bochner’s spectral
decomposition[16]

k(r|ν  σ) =

e2πisT rdS(s|ν  σ) 

(13)

(cid:90)

2   5

σ determines how much weight the spectral density S(s) puts on the slow-varying  low-frequency
basis functions. In this work  we vary ν ∈ { 3
2  ∞} to generate once-  twice  and inﬁnitely-
differentiable functions  and vary σ ∈ {0.5  1  1.5} to generate functions with varying degree of
complexity.
Modeling Strategies
Polynomial Kernels is a family of simple parametric kernels that is equivalent to the polynomial
ridge regression model favored by statisticians due to its model interpretability. In this work  we
use the linear kernel klinear(x  x(cid:48)|p) = xT x(cid:48) and quadratic kernel kquad(x  x(cid:48)|p) = (1 + xT x(cid:48))2
which are common choices from this family.
Gaussian RBF Kernels kRBF(x  x(cid:48)|σ) = exp(−σ||x− x(cid:48)||2) is a general-purpose kernel family that
generates nonlinear  but inﬁnitely differentiable (therefore very smooth) functions. Under this kernel 
we consider two hyperparameter selection strategies common in machine learning applications: RBF-
Median where we set σ to the sample median of {||xi − xj||}i(cid:54)=j  and RBF-MLE who estimates σ
by maximizing the model likelihood.
Matérn and Neural Network Kernels are two ﬂexible kernels favored by machine learning practi-
tioners for their expressiveness. Matérn kernels generates functions that are more ﬂexible compared
to that of Gaussian RBF due to the relaxed smoothness constraint [17]. In order to investigate the
consequences of added ﬂexibility relative to the true model  we use Matern 1/2  Matern 3/2 and
Matern 5/2  corresponding to Matérn kernels with ν = 1/2  3/2  and 5/2. Neural network kernels
[16] knn(x  x(cid:48)|σ) = 2
  on the other hand  represent a 1-layer
Bayesian neural network with inﬁnite hidden unit and probit link function  with σ being the prior
variance on hidden weights. Therefore knn is ﬂexible in the sense that it is an universal approximator
for arbitrary continuous functions on the compact domain [11]. In this work  we denote NN 0.1  NN
1 and NN 10 to represent Bayesian networks with different prior constraints σ ∈ {0.1  1  10}.
Result
The simulation results are presented graphically in Figure 1 and documented in detail in the Appendix.
We ﬁrst observe that for reasonably speciﬁed kmodel  the proposed hypothesis test always has the

π ∗ sin−1(cid:16)

(1+2σ˜xT ˜x)(1+2σ˜x(cid:48)T ˜x(cid:48))

2σ˜xT ˜x(cid:48)

(cid:17)

√

7

(a) ktrue = Matérn 3/2  σ = 0.5

(b) ktrue = Matérn 3/2  σ = 1

(c) ktrue = Matérn 3/2  σ = 1.5

(d) ktrue = Matérn 5/2  σ = 0.5

(e) ktrue = Matérn 5/2  σ = 1

(f) ktrue = Matérn 5/2  σ = 1.5

(g) ktrue = Gaussian RBF  σ = 0.5 (h) ktrue = Gaussian RBF  σ = 1 (i) ktrue = Gaussian RBF  σ = 1.5

Figure 1: Estimated ˆP (p < 0.05) (y-axis) as a function of Interaction Strength δ ∈ [0  1] (x-axis).
Skype Blue: Linear (Solid) and Quadratic (Dashed) Kernels  Black: RBF-Median (Solid) and RBF-
MLE (Dashed)  Dark Blue: Matérn Kernels with ν = 1
2  Purple: Neural Network Kernels
with σ = 0.1  1  10  Red: CVEK based on RBF (Solid) and Neural Networks (Dashed).
Horizontal line marks the test’s signiﬁcance level (0.05). When δ = 0  ˆP should be below this line.

2   5

2   3

correct Type I error and reasonable power. We also observe that the complexity of the data-generating
function hδ (12) plays a role in test performance  in the sense that the power of the hypothesis
tests increases as the Matérn ktrue’s complexity parameter σ becomes larger  which corresponds to
functions that put more weight on the complex  fast-varying eigenfunctions in (13).
We observed clear differences in test performance from different estimation strategies. In general 
polynomial models (linear and quadratic) are too restrictive and appear to underﬁt the data under
both the null and the alternative  producing inﬂated Type I error and diminished power. On the other
hand  lower-order Matérn kernels (Matérn 1/2 and Matérn 3/2  dark blue lines) appear to be too
ﬂexible. Whenever data are generated from smoother ktrue  Matérn 1/2 and 3/2 overﬁt the data and
produce deﬂated Type I error and severely diminished power  even if the hyperparameter σ are ﬁxed
at true value. Therefore unless there’s strong evidence that h exhibits behavior consistent with that
described by these kernels  we recommend avoid the use of either polynomial or lower-order Matérn
kernels for hypothesis testing. Comparatively  Gaussian RBF works well for a wider range of ktrue’s 
but only if the hyperparameter σ is selected carefully. Speciﬁcally  RBF-Median (black dashed line)
works generally well  despite being slightly conservative (i.e. lower power) when the data-generation
function is smooth and of low complexity. RBF-MLE (black solid line)  on the other hand  tends to
underﬁt the data under the null and exhibits inﬂated Type I error  possibly because of the fact that σ is
not strongly identiﬁed when the sample size is too small [18]. The situation becomes more severe as
hδ becomes rougher and more complex  in the moderately extreme case of non-differentiable h with
σ = 1.5  the Type I error is inﬂated to as high as 0.238. Neural Network kernels also perform well
for a wide range of ktrue  and its Type I error is more robust to the speciﬁcation of hyperparameters.
Finally  the two ensemble estimators CVEK-RBF (based on kRBF’s with log(σ) ∈ {−2 −1  0  1  2})
and CVEK-NN (based on kNN’s with σ ∈ {0.1  1  10  50}) perform as well or better than the non-
ensemble approaches for all ktrue’s  despite being slightly conservative under the null. As compared
to CVEK-NN  CVEK-RBF appears to be slightly more powerful.

8

0.00.20.40.60.81.0LinearQuadraticRBF_MLERBF_MedianMatern 1/2Matern 3/2Matern 5/20.00.20.40.60.81.0NN 0.1NN 1NN 10CVKE_RBFCVKE_NN0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0References
[1] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical

Society  68(3):337–404  1950.

[2] D. A. Bodenham and N. M. Adams. A comparison of efﬁcient approximations for a weighted

sum of chi-squared random variables. Statistics and Computing  26(4):917–928  July 2016.

[3] C. Cortes  M. Mohri  and A. Rostamizadeh. Two-Stage Learning Kernel Algorithms. 2010.

[4] C. Cortes  M. Mohri  and A. Rostamizadeh. Ensembles of Kernel Predictors. arXiv:1202.3712

[cs  stat]  Feb. 2012. arXiv: 1202.3712.

[5] A. Davies and Z. Ghahramani. The Random Forest Kernel and other kernels for big data from

random partitions. arXiv:1402.4293 [cs  stat]  Feb. 2014. arXiv: 1402.4293.

[6] A. Elisseeff and M. Pontil. Leave-one-out Error and Stability of Learning Algorithms with
Applications. In J. Suykens  G. Horvath  S. Basu  C. Micchelli  and J. Vandewalle  editors 
Learning Theory and Practice. IOS Press  2002.

[7] T. Evgeniou  L. Perez-Breva  M. Pontil  and T. Poggio. Bounds on the Generalization Per-
formance of Kernel Machine Ensembles. In Proceedings of the Seventeenth International
Conference on Machine Learning  ICML ’00  pages 271–278  San Francisco  CA  USA  2000.
Morgan Kaufmann Publishers Inc.

[8] T. Evgeniou  M. Pontil  and A. Elisseeff. Leave One Out Error  Stability  and Generalization of

Voting Combinations of Classiﬁers. Machine Learning  55(1):71–97  Apr. 2004.

[9] C. Gu. Smoothing Spline ANOVA Models. Springer Science & Business Media  Jan. 2013.

Google-Books-ID: 5VxGAAAAQBAJ.

[10] D. A. Harville. Maximum Likelihood Approaches to Variance Component Estimation and to

Related Problems. Journal of the American Statistical Association  72(358):320–338  1977.

[11] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks 

4(2):251–257  1991.

[12] X. Lin. Variance component testing in generalised linear models with random effects.

Biometrika  84(2):309–326  June 1997.

[13] D. Liu  X. Lin  and D. Ghosh. Semiparametric Regression of Multidimensional Genetic Pathway
Data: Least-Squares Kernel Machines and Linear Mixed Models. Biometrics  63(4):1079–1088 
Dec. 2007.

[14] M. N. Luki´c and J. H. Beder. Stochastic Processes with Sample Paths in Reproducing Kernel
Hilbert Spaces. Transactions of the American Mathematical Society  353(10):3945–3969  2001.

[15] A. Maity and X. Lin. Powerful tests for detecting a gene effect in the presence of possible
gene-gene interactions using garrote kernel machines. Biometrics  67(4):1271–1284  Dec. 2011.

[16] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. University

Press Group Limited  Jan. 2006. Google-Books-ID: vWtwQgAACAAJ.

[17] J. Snoek  H. Larochelle  and R. P. Adams. Practical Bayesian Optimization of Machine Learning

Algorithms. arXiv:1206.2944 [cs  stat]  June 2012. arXiv: 1206.2944.

[18] G. Wahba. Spline Models for Observational Data. SIAM  Sept. 1990. Google-Books-ID:

ScRQJEETs0EC.

[19] A. G. Wilson  Z. Hu  R. Salakhutdinov  and E. P. Xing. Deep Kernel Learning. arXiv:1511.02222

[cs  stat]  Nov. 2015. arXiv: 1511.02222.

[20] D. Zhang and X. Lin. Hypothesis testing in semiparametric additive mixed models. Biostatistics

(Oxford  England)  4(1):57–74  Jan. 2003.

9

,Theodore Bluche
Jeremiah Liu
Brent Coull