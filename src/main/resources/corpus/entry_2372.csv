2013,Graphical Models for Inference with Missing Data,We address the problem of deciding whether there exists a consistent estimator of a given relation Q  when data are missing not at random. We employ a formal representation called `Missingness Graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation  we define the notion of \textit{recoverability} which ensures that  for a given missingness-graph $G$ and a given query $Q$ an algorithm exists such that in the limit of large samples  it produces an estimate of $Q$ \textit{as if} no data were missing. We further present conditions that the graph should satisfy in order for recoverability to hold and devise algorithms to detect the presence of these conditions.,Graphical Models for Inference with Missing Data

Karthika Mohan

Judea Pearl

Jin Tian

Dept. of Computer Science

Dept. of Computer Science

Dept. of Computer Science

Univ. of California  Los Angeles Univ. of California  Los Angeles

Los Angeles  CA 90095

karthika@cs.ucla.edu

Los Angeles  CA 90095
judea@cs.ucla.edu

Iowa State University

Ames  IA 50011

jtian@iastate.edu

Abstract

We address the problem of recoverability i.e. deciding whether there exists a con-
sistent estimator of a given relation Q  when data are missing not at random. We
employ a formal representation called ‘Missingness Graphs’ to explicitly portray
the causal mechanisms responsible for missingness and to encode dependencies
between these mechanisms and the variables being measured. Using this represen-
tation  we derive conditions that the graph should satisfy to ensure recoverability
and devise algorithms to detect the presence of these conditions in the graph.

1

Introduction

The “missing data” problem arises when values for one or more variables are missing from recorded
observations. The extent of the problem is evidenced from the vast literature on missing data in such
diverse ﬁelds as social science  epidemiology  statistics  biology and computer science. Missing
data could be caused by varied factors such as high cost involved in measuring variables  failure of
sensors  reluctance of respondents in answering certain questions or an ill-designed questionnaire.
Missing data also plays a major role in survival data analysis and has been treated primarily using
Kaplan-Meier estimation [30].
In machine learning  a typical example is the Recommender System [16] that automatically gen-
erates a list of products that are of potential interest to a given user from an incomplete dataset of
user ratings. Online portals such as Amazon and eBay employ such systems. Other areas such as
data mining [7]  knowledge discovery [18] and network tomography [2] are also plagued by miss-
ing data problems. Missing data can have several harmful consequences [23  26]. Firstly they can
signiﬁcantly bias the outcome of research studies. This is mainly because the response proﬁles of
non-respondents and respondents can be signiﬁcantly different from each other. Hence ignoring
the former distorts the true proportion in the population. Secondly  performing the analysis using
only complete cases and ignoring the cases with missing values can reduce the sample size thereby
substantially reducing estimation efﬁciency. Finally  many of the algorithms and statistical tech-
niques are generally tailored to draw inferences from complete datasets. It may be difﬁcult or even
inappropriate to apply these algorithms and statistical techniques on incomplete datasets.

1.1 Existing Methods for Handling Missing Data

There are several methods for handling missing data  described in a rich literature of books  articles
and software packages  which are brieﬂy summarized here1. Of these  listwise deletion and pairwise
deletion are used in approximately 96% of studies in the social and behavioral sciences [24].
Listwise deletion refers to a simple method in which cases with missing values are deleted [3]. Un-
less data are missing completely at random  listwise deletion can bias the outcome [31]. Pairwise

1For detailed discussions we direct the reader to the books- [1  6  13  17].

1

deletion (or “available case”) is a deletion method used for estimating pairwise relations among vari-
ables. For example  to compute the covariance of variables X and Y   all those cases or observations
in which both X and Y are observed are used  regardless of whether other variables in the dataset
have missing values.
The expectation-maximization (EM) algorithm is a general technique for ﬁnding maximum like-
lihood (ML) estimates from incomplete data. It has been proven that likelihood-based inference
while ignoring the missing data mechanism  leads to unbiased estimates under the assumption of
missing at random (MAR) [13]. Most work in machine learning assumes MAR and proceeds with
ML or Bayesian inference. Exceptions are recent works on collaborative ﬁltering and recommender
systems which develop probabilistic models that explicitly incorporate missing data mechanism
[16  14  15]. ML is often used in conjunction with imputation methods  which in layman terms 
substitutes a reasonable guess for each missing value [1]. A simple example is Mean Substitution  in
which all missing observations of variable X are substituted with the mean of all observed values of
X. Hot-deck imputation  cold-deck imputation [17] and Multiple Imputation [26  27] are examples
of popular imputation procedures. Although these techniques work well in practice  performance
guarantees (eg: convergence and unbiasedness) are based primarily on simulation experiments.
Missing data discussed so far is a special case of coarse data  namely data that contains observations
made in the power set rather than the sample space of variables of interest [12]. The notion of coars-
ening at random (CAR) was introduced in [12] and identiﬁes the condition under which coarsening
mechanism can be ignored while drawing inferences on the distribution of variables of interest [10].
The notion of sequential CAR has been discussed in [9]. For a detailed discussion on coarsened data
refer to [30].
Missing data literature leaves many unanswered questions with regard to theoretical guarantees for
the resulting estimates  the nature of the assumptions that must be made prior to employing various
procedures and whether the assumptions are testable. For a gentle introduction to the missing data
problem and the issue of testability refer to [22  19]. This paper aims to illuminate missing data
problems using causal graphs [See Appendix 5.2 for justiﬁcation]. The questions we pose are:
Given a target relation Q to be estimated and a set of assumptions about the missingness process
encoded in a graphical model  under what conditions does a consistent estimate exist and how can
we elicit it from the data available?
We answer these questions with the aid of Missingness Graphs (m-graphs in short) to be described
in Section 2. Furthermore  we review the traditional taxonomy of missing data problems and cast it
in graphical terms. In Section 3 we deﬁne the notion of recoverability - the existence of a consistent
estimate - and present graphical conditions for detecting recoverability of a given probabilistic query
Q. Conclusions are drawn in Section 4.

2 Graphical Representation of the Missingness Process

2.1 Missingness Graphs

Figure 1: m-graphs for data that are: (a) MCAR  (b) MAR  (c) & (d) MNAR; Hollow and solid
circles denote partially and fully observed variables respectively.

Graphical models such as DAGs (Directed Acyclic Graphs) can be used for encoding as well as
portraying conditional independencies and causal relations  and the graphical criterion called d-
separation (refer Appendix-5.1  Deﬁnition-3) can be used to read them off the graph [21  20]. Graph-
ical Models have been used to analyze missing information in the form of missing cases (due to
sample selection bias)[4]. Using causal graphs  [8]- analyzes missingness due to attrition (partially

2

Y*Ry(a)XYY*RyXY(b)Y**XRyRx(c)XYY**XRyRxXY(d)observed outcome) and [29]- cautions against the indiscriminate use of auxiliary variables. In both
papers missing values are associated with one variable and interactions among several missingness
mechanisms remain unexplored.
The need exists for a general approach capable of modeling an arbitrary data-generating process and
deciding whether (and how) missingness can be outmaneuvered in every dataset generated by that
process. Such a general approach should allow each variable to be governed by its own missingness
mechanism  and each mechanism to be triggered by other (potentially) partially observed variables
in the model. To achieve this ﬂexibility we use a graphical model called “missingness graph” (m-
graph  for short) which is a DAG (Directed Acyclic Graph) deﬁned as follows.
Let G(V  E) be the causal DAG where V = V ∪ U ∪ V ∗ ∪ R. V is the set of observable nodes.
Nodes in the graph correspond to variables in the data set. U is the set of unobserved nodes (also
called latent variables). E is the set of edges in the DAG. Oftentimes we use bi-directed edges as
a shorthand notation to denote the existence of a U variable as common parent of two variables in
Vo ∪ Vm ∪ R. V is partitioned into Vo and Vm such that Vo ⊆ V is the set of variables that are
observed in all records in the population and Vm ⊆ V is the set of variables that are missing in
at least one record. Variable X is termed as fully observed if X ∈ Vo and partially observed if
X ∈ Vm.
Associated with every partially observed variable Vi ∈ Vm are two other variables Rvi and V ∗
i  
where V ∗
is a proxy variable that is actually observed  and Rvi represents the status of the causal
mechanism responsible for the missingness of V ∗

i

i ; formally 

(cid:26) vi

m

v∗
i = f (rvi  vi) =

if rvi = 0
if rvi = 1

(1)

Contrary to conventional use  Rvi is not treated merely as the missingness indicator but as a driver
(or a switch) that enforces equality between Vi and V ∗
i . V ∗ is a set of all proxy variables and
R is the set of all causal mechanisms that are responsible for missingness. R variables may not
be parents of variables in V ∪ U. This graphical representation succinctly depicts both the causal
relationships among variables in V and the process that accounts for missingness in some of the
variables. We call this graphical representation Missingness Graph or m-graph for short. Since
every d-separation in the graph implies conditional independence in the distribution [21]  the m-
graph provides an effective way of representing the statistical properties of the missingness process
and  hence  the potential of recovering the statistics of variables in Vm from partially missing data.

2.2 Taxonomy of Missingness Mechanisms

It is common to classify missing data mechanisms into three types [25  13]:
Missing Completely At Random (MCAR) : Data are MCAR if the probability that Vm is missing
is independent of Vm or any other variable in the study  as would be the case when respondents
decide to reveal their income levels based on coin-ﬂips.
Missing At Random (MAR) : Data are MAR if for all data cases Y   P (R|Yobs  Ymis) = P (R|Yobs)
where Yobs denotes the observed component of Y and Ymis  the missing component. Example:
Women in the population are more likely to not reveal their age.
Missing Not At Random (MNAR) or “non-ignorable missing”: Data that are neither MAR nor
MCAR are termed as MNAR. Example: Online shoppers rate an item with a high probability either
if they love the item or if they loathe it. In other words  the probability that a shopper supplies a
rating is dependent on the shopper’s underlying liking [16].
Because it invokes speciﬁc values of the observed and unobserved variables  (i.e.  Yobs and Ymis) 
many authors ﬁnd Rubin’s deﬁnition difﬁcult to apply in practice and prefer to work with deﬁnitions
expressed in terms of independencies among variables (see [28  11  6  17]). In the graph-based
interpretation used in this paper  MCAR is deﬁned as total independence between R and Vo∪Vm∪U
i.e. R⊥⊥(Vo ∪ Vm ∪ U )  as depicted in Figure 1(a). MAR is deﬁned as independence between R and
Vm∪U given Vo i.e. R⊥⊥Vm∪U|Vo  as depicted in Figure 1(b). Finally if neither of these conditions
hold  data are termed MNAR  as depicted in Figure 1(c) and (d). This graph-based interpretation uses
slightly stronger assumptions than Rubin’s  with the advantage that the user can comprehend  encode
and communicate the assumptions that determine the classiﬁcation of the problem. Additionally  the
conditional independencies that deﬁne each class are represented explicitly as separation conditions

3

in the corresponding m-graphs. We will use this taxonomy in the rest of the paper  and will label
data MCAR  MAR and MNAR according to whether the deﬁning conditions  R⊥⊥Vo ∪ Vm ∪ U (for
MCAR)  R⊥⊥Vm ∪ U|Vo (for MAR) are satisﬁed in the corresponding m-graphs.

3 Recoverability

In this section we will examine the conditions under which a bias-free estimate of a given proba-
bilistic relation Q can be computed. We shall begin by deﬁning the notion of recoverability.
Deﬁnition 1 (Recoverability). Given a m-graph G  and a target relation Q deﬁned on the variables
in V   Q is said to be recoverable in G if there exists an algorithm that produces a consistent estimate
of Q for every dataset D such that P (D) is (1) compatible with G and (2) strictly positive over
complete cases i.e. P (Vo  Vm  R = 0) > 0.2
Here we assume that the observed distribution over complete cases P (Vo  Vm  R = 0) is strictly
positive  thereby rendering recoverability a property that can be ascertained exclusively from the
m-graph.
Corollary 1. A relation Q is recoverable in G if and only if Q can be expressed in terms of the
probability P (O) where O = {R  V ∗  Vo} is the set of observable variables in G. In other words 
for any two models M1 and M2 inducing distributions P M1 and P M2 respectively  if P M1(O) =
P M2(O) > 0 then QM1 = QM2.

Proof: (sketch) The corollary merely rephrases the requirement of obtaining a consistent estimate to
that of expressibility in terms of observables.
Practically  what recoverability means is that if the data D are generated by any process compatible
with G  a procedure exists that computes an estimator ˆQ(D) such that  in the limit of large samples 
ˆQ(D) converges to Q. Such a procedure is called a “consistent estimator.” Thus  recoverability is
the sole property of G and Q  not of the data available  or of any routine chosen to analyze or process
the data.
Recoverability when data are MCAR For MCAR data we have R⊥⊥(Vo ∪ Vm). Therefore  we
can write P (V ) = P (V |R) = P (Vo  V ∗|R = 0). Since both R and V ∗ are observables  the joint
probability P (V ) is consistently estimable (hence recoverable) by considering complete cases only
(listwise deletion)  as shown in the following example.
Example 1. Let X be the treatment and Y be the outcome as depicted in the m-graph in Fig. 1
(a). Let it be the case that we accidentally deleted the values of Y for a handful of samples  hence
Y ∈ Vm. Can we recover P (X  Y )?
From D  we can compute P (X  Y ∗  Ry). From the m-graph G  we know that Y ∗ is a collider and
hence by d-separation  (X ∪ Y )⊥⊥Ry. Thus P (X  Y ) = P (X  Y |Ry). In particular  P (X  Y ) =
P (X  Y |Ry = 0). When Ry = 0  by eq. (1)  Y ∗ = Y . Hence 

P (X  Y ) = P (X  Y ∗|Ry = 0)

(2)

The RHS of Eq. 2 is consistently estimable from D; hence P (X  Y ) is recoverable.
Recoverability when data are MAR When data are MAR  we have R⊥⊥Vm|Vo. Therefore
P (V ) = P (Vm|Vo)P (Vo) = P (Vm|Vo  R = 0)P (Vo). Hence the joint distribution P (V ) is re-
coverable.
Example 2. Let X be the treatment and Y be the outcome as depicted in the m-graph in Fig. 1 (b).
Let it be the case that some patients who underwent treatment are not likely to report the outcome 
hence the arrow X → Ry. Under the circumstances  can we recover P (X  Y )?
From D  we can compute P (X  Y ∗  Ry). From the m-graph G  we see that Y ∗ is a collider and X is
a fork. Hence by d-separation  Y ⊥⊥Ry|X. Thus P (X  Y ) = P (Y |X)P (X) = P (Y |X  Ry)P (X).
2In many applications such as truncation by death  the problem forbids certain combinations of events
from occurring  in which case the deﬁnition need be modiﬁed to accommodate such constraints as shown in
Appendix-5.3. Though this modiﬁcation complicates the deﬁnition of “recoverability”  it does not change the
basic results derived in this paper.

4

In particular  P (X  Y ) = P (Y |X  Ry = 0)P (X). When Ry = 0  by eq. (1)  Y ∗ = Y . Hence 

P (X  Y ) = P (Y ∗|X  Ry = 0)P (X)

(3)

and since X is fully observable  P (X  Y ) is recoverable.

Note that eq. (2) permits P (X  Y ) to be recovered by listwise deletion  while eq. (3) does not; it
requires that P (X) be estimated ﬁrst over all samples  including those in which Y is missing. In
this paper we focus on recoverability under large sample assumption and will not be dealing with
the shrinking sample size issue.

Recoverability when data are MNAR Data that are neither MAR nor MCAR are termed MNAR.
Though it is generally believed that relations in MNAR datasets are not recoverable  the following
example demonstrates otherwise.
Example 3. Fig. 1 (d) depicts a study where (i) some units who underwent treatment (X = 1) did
not report the outcome (Y ) and (ii) we accidentally deleted the values of treatment for a handful
of cases. Thus we have missing values for both X and Y which renders the dataset MNAR. We
shall show that P (X  Y ) is recoverable. From D  we can compute P (X∗  Y ∗  Rx  Ry). From the
m-graph G  we see that X⊥⊥Rx and Y ⊥⊥(Rx ∪ Ry)|X. Thus P (X  Y ) = P (Y |X)P (X) =
P (Y |X  Ry = 0  Rx = 0)P (X|Rx = 0). When Ry = 0 and Rx = 0 we have (by Equation (1) ) 
Y ∗ = Y and X∗ = X. Hence 

P (X  Y ) = P (Y ∗|X∗  Rx = 0  Ry = 0)P (X∗|Rx = 0)

(4)

Therefore  P (X  Y ) is recoverable.

The estimand in eq. (4) also dictates how P (X  Y ) should be estimated from the dataset. In the ﬁrst
step  we delete all cases in which X is missing and create a new data set D(cid:48) from which we estimate
P (X). Dataset D(cid:48) is further pruned to form dataset D(cid:48)(cid:48) by removing all cases in which Y is missing.
P (Y |X) is then computed from D(cid:48)(cid:48). Note that order matters; had we deleted cases in the reverse
order  Y and then X  the resulting estimate would be biased because the d-separations needed for
establishing the validity of the estimand: P (X|Y )P (Y )  are not supported by G. We will call this
sequence of deletions as deletion order.
Several features are worth noting regarding this graph-based taxonomy of missingness mechanisms.
First  although MCAR and MAR can be veriﬁed by inspecting the m-graph  they cannot  in general
be veriﬁed from the data alone. Second  the assumption of MCAR allows an estimation procedure
that amounts (asymptotically) to listwise deletion  while MAR dictates a procedure that amounts
to listwise deletion in every stratum of Vo. Applying MAR procedure to MCAR problem is safe 
because all conditional independencies required for recoverability under the MAR assumption also
hold in an MCAR problem  i.e. R⊥⊥(Vo  Vm) ⇒ R⊥⊥Vm|Vo. The converse  however  does not
hold  as can be seen in Fig. 1 (b). Applying listwise deletion is likely to result in bias  because the
necessary condition R⊥⊥(Vo  Vm) is violated in the graph. An interesting property which evolves
from this discussion is that recoverability of certain relations does not require RVi⊥⊥Vi|Vo ; a subset
of Vo would sufﬁce as shown below.
Property 1. P (Vi) is recoverable if ∃W ⊆ Vo such that RVi⊥⊥V |W .

Proof: P (Vi) may be decomposed as: P (Vi) =(cid:80)

and W ⊆ Vo. Hence P (Vi) is recoverable.
It is important to note that the recoverability of P (X  Y ) in Fig. 1(d) was feasible despite the fact
that the missingness model would not be considered Rubin’s MAR (as deﬁned in [25]). In fact  an
overwhelming majority of the data generated by each one of our MNAR examples would be outside
Rubin’s MAR. For a brief discussion on these lines  refer to Appendix- 5.4.
Our next question is: how can we determine if a given relation is recoverable? The following
theorem provides a sufﬁcient condition for recoverability.

w P (V ∗

i |Rvi = 0  W )P (W ) since Vi⊥⊥RVi|W

3.1 Conditions for Recoverability
Theorem 1. A query Q deﬁned over variables in Vo ∪ Vm is recoverable if it is decomposable into
terms of the form Qj = P (Sj|Tj) such that Tj contains the missingness mechanism Rv = 0 of
every partially observed variable V that appears in Qj.

5

Proof: If such a decomposition exists  every Qj is estimable from the data  hence the entire expres-
sion for Q is recoverable.
Example 4. Equation (4) demonstrates a decomposition of Q = P (X  Y ) into a product of two
terms Q1 = P (Y |X  Rx = 0  Ry = 0) and Q2 = P (X|Rx = 0) that satisfy the condition of
Theorem 1. Hence Q is recoverable.
Example 5. Consider the problem of recovering Q = P (X  Y ) from the m-graph of Fig. 3(b).
Attempts to decompose Q by the chain rule  as was done in Eqs. (3) and (4) would not satisfy the
conditions of Theorem 1. To witness we write P (X  Y ) = P (Y |X)P (X) and note that the graph
does not permit us to augment any of the two terms with the necessary Rx or Ry terms; X is
independent of Rx only if we condition on Y   which is partially observed  and Y is independent of
Ry only if we condition on X which is also partially observed. This deadlock can be disentangled
however using a non-conventional decomposition:

Q = P (X  Y ) = P (X  Y )

P (Rx  Ry|X  Y )
P (Rx  Ry|X  Y )
P (Rx  Ry)P (X  Y |Rx  Ry)
P (Rx|Y  Ry)P (Ry|X  Rx)

=

the denominator was obtained using the

(5)
independencies Rx⊥⊥(X  Ry)|Y and
where
Ry⊥⊥(Y  Rx)|X shown in the graph.
The ﬁnal expression above satisﬁes Theorem 1 and
renders P (X  Y ) recoverable. This example again shows that recovery is feasible even when data
are MNAR.
Theorem 2 operationalizes the decomposability requirement of Theorem 1.
Theorem 2 (Recoverability of the Joint P (V )). Given a m-graph G with no edges between the R
variables and no latent variables as parents of R variables  a necessary and sufﬁcient condition
for recovering the joint distribution P (V ) is that no variable X be a parent of its missingness
mechanism RX. Moreover  when recoverable  P (V ) is given by

(cid:81)
i P (Ri = 0|pao

P (R = 0  v)
  pam
ri

ri

P (v) =

 

= 0)

  RP am
ri

where P ao
ri

⊆ Vo and P am

ri

⊆ Vm are the parents of Ri.

(6)

(7)

Proof. (sufﬁciency) The observed joint distribution may be decomposed according to G as

P (R = 0  v) =

P (v  u)P (R = 0|v  u)

(cid:88)

u

= P (v)

(cid:89)

i

P (Ri = 0|pao

ri

  pam
ri

) 

ri

ri

∪ P am
P (Ri = 0|pao

). Therefore 
  pam
ri

ri

ri

where we have used the facts that there are no edges between the R variables  and that there are no
latent variables as parents of R variables. If Vi is not a parent of Ri (i.e. Vi (cid:54)∈ P am
)  then we have
Ri⊥⊥RP am

|(P ao

ri

) = P (Ri = 0|pao

  pam
ri

(8)
Given strictly positive P (R = 0  Vm  Vo)  we have that all probabilities P (Ri =
0|pao
= 0) are strictly positive. Using Equations (7) and (8)   we conclude that
P (V ) is recoverable as given by Eq. (6).
(necessity) If X is a parent of its missingness mechanism RX  then P (X) is not recoverable based
on Lemmas 3 and 4 in Appendix 5.5. Therefore the joint P (V ) is not recoverable.

  RP am
ri

  RP am
ri

  pam
ri

= 0).

ri

ri

The following theorem gives a sufﬁcient condition for recovering the joint distribution in a Marko-
vian model.
Theorem 3. Given a m-graph with no latent variables (i.e.  Markovian) the joint distribution P (V )
is recoverable if no missingness mechanism RX is a descendant of its corresponding variable X.
Moreover  if recoverable  then P (V ) is given by

(cid:89)
P (vi|pao
i Vi∈Vo
i ⊆ Vo and P am

(cid:89)
i   pam
j Vj∈Vm
i ⊆ Vm are the parents of Vi.

i   RP am

= 0)

i

P (v) =

where P ao

P (vj|pao

j   pam

j   RVj = 0  RP am

j

= 0) 

(9)

6

Proof: Refer Appendix-5.6
(cid:81)
Deﬁnition 2 (Ordered factorization). An ordered factorization over a set O of ordered V vari-
ables Y1 < Y2 < . . . < Yk  denoted by f (O)  is a product of conditional probabilities f (O) =
i P (Yi|Xi) where Xi ⊆ {Yi+1  . . .   Yn} is a minimal set such that Yi⊥⊥({Yi+1  . . .   Yn}\Xi)|Xi.
Theorem 4. A sufﬁcient condition for recoverability of a relation Q is that Q be decomposable into
an ordered factorization  or a sum of such factorizations  such that every factor Qi = P (Yi|Xi)
satisﬁes Yi⊥⊥(Ryi  Rxi )|Xi. A factorization that satisﬁes this condition will be called admissible.

Figure 2: Graph in which (a) only P (X|Y ) is recoverable (b) P (X4) is recoverable only when
conditioned on X1 as shown in Example 6 (c) P (X  Y  Z) is recoverable (d) P (X  Z) is recoverable.

Proof. follows from Theorem-1 noting that ordered factorization is one speciﬁc form of decompo-
sition.

Theorem 4 will allow us to conﬁrm recoverability of certain queries Q in models such as those in
Fig. 2(a)  (b) and (d)  which do not satisfy the requirement in Theorem 2. For example  by applying
Theorem 4 we can conclude that  (1) in Figure 2 (a)  P (X|Y ) = P (X|Rx = 0  Ry = 0  Y ) is
recoverable  (2) in Figure 2 (c)  P (X  Y  Z) = P (Z|X  Y  Rz = 0  Rx = 0  Ry = 0)P (X|Y  Rx =
0  Ry = 0)P (Y |Ry = 0) is recoverable and (3) in Figure 2 (d)  P (X  Z) = P (X  Z|Rx = 0  Rz =
0) is recoverable.
Note that the condition of Theorem 4 differs from that of Theorem 1 in two ways. Firstly  the
decomposition is limited to ordered factorizations i.e. Yi is a singleton and Xi a set. Secondly  both
Yi and Xi are taken from Vo ∪ Vm  thus excluding R variables.
Example 6. Consider the query Q = P (X4) in Fig. 2(b). Q can be decomposed in a variety of
ways  among them being the factorizations:

(a) P (X4) =(cid:80)
(b) P (X4) =(cid:80)
(c) P (X4) =(cid:80)
satisﬁes Theorem 4. Speciﬁcally  (c) can be written as(cid:80)

P (X4|X3)P (X3) for the order X4  X3
P (X4|X2)P (X2) for the order X4  X2
P (X4|X1)P (X1) for the order X4  X1

x1

x3

x2

Although each of X1  X2 and X3 d-separate X4 from RX4  only (c) is admissible since each factor
4|X1  RX4 = 0)P (X1) and can
be estimated by the deletion schedule (X1  X4)  i.e.  in each stratum of X1  we delete samples for
which RX4 = 1 and compute P (X∗
4   Rx4 = 0  X1). In (a) and (b) however  Theorem-4 is not
satisﬁed since the graph does not permit us to rewrite P (X3) as P (X3|Rx3 = 0) or P (X2) as
P (X2|Rx2 = 0).

P (X∗

x1

3.2 Heuristics for Finding Admissible Factorization

Consider the task of estimating Q = P (X)  where X is a set  by searching for an admissible
factorization of P (X) (one that satisﬁes Theorem 4)  possibly by resorting to additional variables 
Z  residing outside of X that serve as separating sets. Since there are exponentially large number
of ordered factorizations  it would be helpful to rule out classes of non-admissible ordering prior
to their enumeration whenever non-admissibility can be detected in the graph. In this section  we
provide lemmata that would aid in pruning process by harnessing information from the graph.
Lemma 1. An ordered set O will not yield an admissible decomposition if there exists a partially
observed variable Vi in the order O which is not marginally independent of RVi such that all minimal
separators (refer Appendix-5.1  Deﬁnition-4) of Vi that d-separate it from Rvi appear before Vi.

Proof: Refer Appendix-5.7

7

X1X3X2X4RXRY(b)(a)x3RRZRYRXZXY(c)(d)RYRZRXx4x2RRXYXYZFigure 3: demonstrates (a) pruning in Example-7 (b) P (X  Y ) is recoverable in Example-5

Applying lemma-1 requires a solution to a set of disjunctive constraints which can be represented
by directed constraint graphs [5].
Example 7. Let Q = P (X) be the relation to be recovered from the graph in Fig. 3 (a). Let
X = {A  B  C  D  E} and Z = F . The total number of ordered factorizations is 6! = 720.
The independencies implied by minimal separators (as required by Lemma-1) are: A⊥⊥RA|B 
B⊥⊥RB|φ  C⊥⊥RC|{D  E}  ( D⊥⊥RD|A or D⊥⊥RD|C or D⊥⊥RD|B ) and (E⊥⊥RE|{B  F} or
E⊥⊥RE|{B  D} or E⊥⊥RE|C). To test whether (B A D E C F) is potentially admissible we need
not explicate all 6 variables; this order can be ruled out as soon as we note that A appears after B.
Since B is the only minimal separator that d-separates A from RA and B precedes A  Lemma-1 is
violated. Orders such as (C  D  E  A  B  F )  (C  D  A  E  B  F ) and (C  E  D  A  F  B) satisfy the
condition stated in Lemma 1 and are potential candidates for admissibility.

The following lemma presents a simple test to determine non-admissibility by specifying the condi-
tion under which a given order can be summarily removed from the set of candidate orders that are
likely to yield admissible factorizations.
Lemma 2. An ordered set O will not yield an admissible decomposition if it contains a partially
observed variable Vi for which there exists no set S ⊆ V that d-separates Vi from RVi.
Proof: The factor P (Vi|Vi+1  . . .   Vn) corresponding to Vi can never satisfy the condition required
by Theorem 4.
An interesting consequence of Lemma 2 is the following corollary that gives a sufﬁcient condition
under which no ordered factorization can be labeled admissible.
Corollary 2. For any disjoint sets X and Y   there exists no admissible factorization for recovering
the relation P (Y |X) by Theorem 4 if Y contains a partially observed variable Vi for which there
exists no set S ⊆ V that d-separates Vi from RVi.

4 Conclusions

We have demonstrated that causal graphical models depicting the data generating process can serve
as a powerful tool for analyzing missing data problems and determining (1) if theoretical imped-
iments exist to eliminating bias due to data missingness  (2) whether a given procedure produces
consistent estimates  and (3) whether such a procedure can be found algorithmically. We formalized
the notion of recoverability and showed that relations are always recoverable when data are missing
at random (MCAR or MAR) and  more importantly  that in many commonly occurring problems 
recoverability can be achieved even when data are missing not at random (MNAR). We further
presented a sufﬁcient condition to ensure recoverability of a given relation Q (Theorem 1) and oper-
ationalized Theorem 1 using graphical criteria (Theorems 2  3 and 4). In summary  we demonstrated
some of the insights and capabilities that can be gained by exploiting causal knowledge in missing
data problems.
Acknowledgment
This research was supported in parts by grants from NSF #IIS-1249822 and #IIS-1302448 and ONR
#N00014-13-1-0153 and #N00014-10-1-0933

References
[1] P.D. Allison. Missing data series: Quantitative applications in the social sciences  2002.

8

RCRARERBRDRYRX(a)(b)ACDEBFYX[2] T. Bu  N. Dufﬁeld  F.L. Presti  and D. Towsley. Network tomography on general topologies. In ACM

SIGMETRICS Performance Evaluation Review  volume 30  pages 21–30. ACM  2002.

[3] E.R. Buhi  P. Goodson  and T.B. Neilands. Out of sight  not out of mind: strategies for handling missing

data. American journal of health behavior  32:83–92  2008.

[4] R.M. Daniel  M.G. Kenward  S.N. Cousens  and B.L. De Stavola. Using causal diagrams to guide analysis

in missing data problems. Statistical Methods in Medical Research  21(3):243–256  2012.

[5] R. Dechter  I. Meiri  and J. Pearl. Temporal constraint networks. Artiﬁcial intelligence  1991.
[6] C.K. Enders. Applied Missing Data Analysis. Guilford Press  2010.
[7] U.M. Fayyad. Data mining and knowledge discovery: Making sense out of data. IEEE expert  11(5):20–

25  1996.

[8] F. M. Garcia. Deﬁnition and diagnosis of problematic attrition in randomized controlled experiments.

Working paper  April 2013. Available at SSRN: http://ssrn.com/abstract=2267120.

[9] R.D. Gill and J.M. Robins. Sequential models for coarsening and missingness. In Proceedings of the

First Seattle Symposium in Biostatistics  pages 295–305. Springer  1997.

[10] R.D. Gill  M.J. Van Der Laan  and J.M. Robins. Coarsening at random: Characterizations  conjec-
tures  counter-examples. In Proceedings of the First Seattle Symposium in Biostatistics  pages 255–294.
Springer  1997.

[11] J.W Graham. Missing Data: Analysis and Design (Statistics for Social and Behavioral Sciences).

Springer  2012.

[12] D.F. Heitjan and D.B. Rubin. Ignorability and coarse data. The Annals of Statistics  pages 2244–2253 

1991.

[13] R.J.A. Little and D.B. Rubin. Statistical analysis with missing data. Wiley  2002.
[14] B.M. Marlin and R.S. Zemel. Collaborative prediction and ranking with non-random missing data. In

Proceedings of the third ACM conference on Recommender systems  pages 5–12. ACM  2009.

[15] B.M. Marlin  R.S. Zemel  S. Roweis  and M. Slaney. Collaborative ﬁltering and the missing at random

assumption. In UAI  2007.

[16] B.M. Marlin  R.S. Zemel  S.T. Roweis  and M. Slaney. Recommender systems: missing data and statisti-

cal model estimation. In IJCAI  2011.

[17] P.E. McKnight  K.M. McKnight  S. Sidani  and A.J. Figueredo. Missing data: A gentle introduction.

Guilford Press  2007.

[18] Harvey J Miller and Jiawei Han. Geographic data mining and knowledge discovery. CRC  2009.
[19] K. Mohan and J. Pearl. On the testability of models with missing data. To appear in the Proceedings of

AISTAT-2014; Available at http://ftp.cs.ucla.edu/pub/stat ser/r415.pdf.

[20] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kauf-

mann  1988.

[21] J. Pearl. Causality: models  reasoning and inference. Cambridge Univ Press  New York  2009.
[22] J. Pearl and K. Mohan. Recoverability and testability of missing data: Introduction and summary of

results. Technical Report R-417  UCLA  2013. Available at http://ftp.cs.ucla.edu/pub/stat ser/r417.pdf.

[23] C.Y.J. Peng  M. Harwell  S.M. Liou  and L.H. Ehman. Advances in missing data methods and implications

for educational research. Real data analysis  pages 31–78  2006.

[24] J.L. Peugh and C.K. Enders. Missing data in educational research: A review of reporting practices and

suggestions for improvement. Review of educational research  74(4):525–556  2004.

[25] D.B. Rubin. Inference and missing data. Biometrika  63:581–592  1976.
[26] D.B. Rubin. Multiple Imputation for Nonresponse in Surveys. Wiley Online Library  New York  NY 

1987.

[27] D.B. Rubin. Multiple imputation after 18+ years.

91(434):473–489  1996.

Journal of the American Statistical Association 

[28] J.L. Schafer and J.W. Graham. Missing data: our view of the state of the art. Psychological Methods 

7(2):147–177  2002.

[29] F. Thoemmes and N. Rose. Selection of auxiliary variables in missing data problems: Not all auxiliary

variables are created equal. Technical Report Technical Report R-002  Cornell University  2013.

[30] M.J. Van der Laan and J.M. Robins. Uniﬁed methods for censored longitudinal data and causality.

Springer Verlag  2003.

[31] W. Wothke. Longitudinal and multigroup modeling with missing data. Lawrence Erlbaum Associates

Publishers  2000.

9

,Karthika Mohan
Judea Pearl
Jin Tian
Francesco Orabona