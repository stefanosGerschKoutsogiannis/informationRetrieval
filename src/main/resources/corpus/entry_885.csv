2007,CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation,This paper proposes constraint propagation relaxation (CPR)  a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms SP(&#961;)  ranging from belief propagation (&#961; = 0) to (pure) survey propagation(&#961; = 1). More importantly  the approach elucidates the implicit  but fundamental assumptions underlying SP(&#961;)  thus shedding some light on its effectiveness and leading to applications beyond k-SAT.,CPR for CSPs: A Probabilistic Relaxation of

Constraint Propagation

Luis E. Ortiz

ECE Dept  Univ. of Puerto Rico  Mayag¨uez  PR 00681-9042

leortiz@ece.uprm.edu

Abstract

This paper proposes constraint propagation relaxation (CPR)  a probabilistic ap-
proach to classical constraint propagation that provides another view on the whole
parametric family of survey propagation algorithms SP(ρ). More importantly  the
approach elucidates the implicit  but fundamental assumptions underlying SP(ρ) 
thus shedding some light on its effectiveness and leading to applications beyond
k-SAT.

1 Introduction

Survey propagation (SP) is an algorithm for solving k-SAT recently developed in the physics com-
munity [1  2] that exhibits excellent empirical performance on “hard” instances. To understand the
behavior of SP and its effectiveness  recent work (see Maneva et al. [3] and the references therein)
has concentrated on establishing connections to belief propagation (BP) [4]  a well-known approxi-
mation method for computing posterior probabilities in probabilistic graphical models. Instead  this
paper argues that it is perhaps more natural to establish connections to constraint propagation (CP) 
another message-passing algorithm tailored to constraint satisfaction problems (CSPs) that is well-
known in the AI community. The ideas behind CP were ﬁrst proposed by Waltz [5] 1 Yet  CP has
received considerably less attention than BP lately.
This paper reconnects BP to CP in the context of CSPs by proposing a probabilistic relaxation
of CP that generalizes it. Through the approach  it is easy to see the exact  implicit underlying
assumptions behind the entire family of survey propagation algorithms SP(ρ). (Here  the approach
is presented in the context of k-SAT; it will be described in full generality in a separate document.)
In short  the main point of this paper is that survey propagation algorithms are instances of a natural
generalization of constraint propagation and have simple interpretations in that context.

2 Constraint Networks and Propagation

This section presents a brief introduction to the graphical representation of CSPs and CP  and con-
centrates on the aspects that are relevant to this paper. 2
A constraint network (CN) is the graphical model for CSPs used in the AI community. Of interest
here is the CN based on the hidden transformation. (See Bacchus et al. [9] for more information
on the different transformations and their properties.) It has a bipartite graph where every variable
and constraint is each represented by a node or vertex in the graph and there is an edge between a
variable i and a constraint a if and only if a is a function of i (see ﬁgure 1). From now on  a CN with
a tree graph is referred to as a tree CN  and a CN with an arbitrary graph as an arbitrary CN.

1See also Pearl [4]  section 4.1.1  and the ﬁrst paragraph of section 4.1.2.
2Please refer to Russell and Norvig [6] for a general introduction  Kumar [7] for a tutorial and Dechter [8]

for a more comprehensive treatment of these topics and additional references.

1

1

4

2

a

variables
3

Constraint propagation is typically used as part
of a depth-ﬁrst search algorithm for solving
CSPs. The search algorithm works by extend-
ing partial assignments  usually one variable
at a time  during the search. The algorithm
is called backtracking search because one can
backtrack and change the value of a previously
assigned variable when the search reaches an
illegal assignment.
CP is often applied either as a preprocessing
step or after an assignment to a variable is
made. The objective is to reduce the domains
of the variables by making them locally consis-
tent with the current partial assignment. The
propagation process starts with the belief that
for every value assignment vi in the domain of
each variable i there exists a solution with vi as-
signed to i. The process then attempts to correct
this a priori belief by locally propagating con-
straint information.
It is well-known that CP 
unlike BP  always converges  regardless of the
structure of the CN graph. This is because no
possible solution is ignored at the start and none
ever removed during the process. In the end  CP
produces potentially reduced variable domains
that are in fact locally consistent. In turn  the
resulting search space is at worst no larger than the original but potentially smaller while still con-
taining all possible solutions. The computational efﬁciency and effectiveness of CP in practice has
made it a popular algorithm in the CSP community.

Figure 1: The graph of the constraint network cor-
responding to the 3-SAT formula f(x) = (x1 ∨
x2 ∨ x3) ∧ (x2 ∨ ¯x3 ∨ x4)  which has four vari-
ables and two clauses; the ﬁrst and second clause
are denoted in the ﬁgure by a and b  respectively.
Following the convention of the SP community 
clause and variable nodes are drawn as boxes and
circles  respectively; also  if a variable appears as
a negative literal in a clause (e.g.  variable 3 in
clause b)  the edge between them is drawn as a
dashed line.

clauses

b

3 Terminology and Notation

3

4

1

2

fb→2

a(i) and C u

Let V (a) be the set of variables that appear in
constraint a and C(i) the set of constraints in
which variable i appears. Let also Vi(a) ≡
V (a) − {i} and Ca(i) ≡ C(i) − {a}. In k-
SAT  the constraints are the clauses  each vari-
able is binary  with domain {0  1}  and a solu-
tion corresponds to a satisfying assignment. If
i ∈ V (a)  denote by sa i the value assignment
to variable i that guarantees the satisﬁability of
clause a; and denote the other possible assign-
a (i)
ment to i by ua i. Finally  let C s
be the set of clauses in Ca(i) where variable i
appears in the same and different literal form as
it does in clause a  respectively.
The k-SAT formula under consideration is de-
noted by f.
It is convenient to introduce no-
tation for formulae associated to the CN that
results from removing variables or constraints
from f. Let fa be the function that results from
removing clause a from f (see ﬁgure 2)  and
similarly  abusing notation  let fi be the function that results from removing variable i from f. Let
fa→i be the function that corresponds to the connected component of the CN graph for fa that con-
tains variable i ∈ V (a)  and let fi→a be the function that corresponds to the connected component
of the CN graph for fi that contains a ∈ C(i). (Naturally  if node a is not a separator of the CN
graph for f  fa has a single connected component  which leads to fa→i = fa; similarly for fi.)

Figure 2: The graph inside the continuous curve is
the CN graph for the formula fb that results from
removing clause b from f. The graph inside the
dashed curve is the CN graph for fb→2  which cor-
responds to the formula for the connected compo-
nent of the CN graph for fb that contains variable
2.

fb

a

b

2

clausesvariablesIt is convenient to use a simple  if perhaps unusual  representation of sets in order to track the
domains of the variables during the propagation process. Each subset A of a set S of size m is
represented as a bit array of m elements where component k in the array is set to 1 if k is in A and
to 0 otherwise. For instance  if S = {0  1}  then the array [00] represents ∅  and similarly  [01]  [10]
and [11] represent {0}  {1} and {0  1}  respectively.
It is also useful to introduce the concept of (globally) consistent domains of variables and SAT
functions. Let Sf = {x|x satisﬁes f} be the set of assignments that satisfy f. Given a complete
assignment x  denote by x−i the assignments to all the variables except i; thus  x = (x1  . . .   xn) =
(xi  x−i). Let the set Wi be the consistent domain of variable i in f if Wi = {xi|x = (xi  x−i) ∈
Sf for some x−i}; that is  Wi contains the set of all possible values that variable i can take in an
assignment that satisﬁes f. Let the set W be the consistent domain of f if W = ×n
i=1Wi and  for
all i  Wi is the consistent domain of variable i in f.
Finally  some additional terminology classiﬁes variables of a SAT function given a satisfying assign-
ment. Given a function f and a satisfying assignment x  let variable i be ﬁxed if changing only its
assignment xi in x does not produce another satisfying assignment for f; and be free otherwise.

4 Propagation Algorithms for Satisﬁability

Constraint Propagation.
In CP for k-SAT  the message Ma→i that clause a sends to variable i
is an array of binary values indexed by the elements of the domain of i; similarly  for the message
Mi→a that variable i sends to clause a. Intuitively  for all xi ∈ {0  1}  Mi→a(xi) = 1 if and only
if assigning value xi to variable i is “ok” with all clauses other than a. Formally  Mi→a(xi) = 1
if and only if fa→i has a satisfying assignment with xi assigned to variable i (or in other words 
xi is in the consistent domain of i in fa→i). Similarly  Ma→i(xi) = 1 if and only if clause a
is “ok” with assigning value xi to variable i; or formally  Ma→i(xi) = 1 if and only if fi→a
has a satisfying assignment with xi assigned to variable i  or assigning xi to variable i by itself
satisﬁes a. It is convenient to denote Mi→a(xi) and Mi→a(xi) by M xi
a→i  respectively.
In addition  M sa i
i→a  M s
a→i and
a→i  respectively.
M u
In summary  we can write CP for k-SAT as follows.
• Messages that clause a sends to variable i:

a→i are simply denoted by M s

a→i and M xi
i→a  M u

a→i and M ua i

i→a  M ua i

i→a  M sa i

a→i = 1 if and only if xi = sa i or  there exists j ∈ Vi(a)  s.t. M s
M xi

j→a = 1.

• Messages that variable i sends to clause a:

i→a = 1 if and only if for all b ∈ Ca(i)  M xi
M xi

b→i = 1.

(1)

(2)

It is convenient to express CP mathematically as follows.

• Messages that clause a sends to variable i:

(cid:26) 1 
1 −(cid:81)

M xi

a→i =

• Messages that variable i sends to clause a: M xi

j∈Vi(a)(1 − M s

j→a) 

i→a =(cid:81)

if xi = sa i 
if xi = ua i.

b∈Ca(i) M xi

b→i.

a→i = 1  and naturally  M s

i→a =
In order to guarantee convergence  the message values in CP are initialized as M s
1  M u
a→i = 1. This initialization encodes the a priori belief that every
assignment is a solution. CP attempts to “correct” or update this belief through the local propagation
of constraint information. In fact  the expressions in CP force the messages to be locally consistent.
By being initially conservative about the consistent domains  no satisfying assignment is discarded
during the propagation process.
for
Once CP converges 

its
becomes
a→i = 1} ∈ 2{0 1}. For general CSPs 
M u
CP is usually very effective because it can signiﬁcantly reduce the original domain of the variables 

a→i = 1} = {xi|(cid:81)

variable
i 
a∈C(i):xi=ua i

{xi|(cid:81)

locally-consistent

i→a = 1  M u

a∈C(i) M xi

domain

each

3

leading to a smaller search space of possible assignments. It should be noted that in the particular
case of k-SAT with arbitrary CNs  CP is usually only effective after some variables have already
being assigned during the search  because those (partial) assignments can lead to “boundary
conditions.” Without such boundary conditions  however  CP never reduces the domain of the
variables in k-SAT  as can be easily seen from the expressions above.
On the other hand  when CP is applied to tree CNs  it exhibits additional special properties. For
example  convergence is actually guaranteed regardless of how the messages are initialized  because
of the boundary conditions imposed by the leaves of the tree. Also  the ﬁnal messages are in fact
globally consistent (i.e.  all the messages are consistent with their deﬁnition). Therefore  the locally-
consistent domains are in fact the consistent domains. Whether the formula is satisﬁable  or not 
can be determined immediately after applying CP. If the formula is not satisﬁable  the consistent
domains will be empty sets. If the formula is in fact satisﬁable  applying depth-ﬁrst search always
ﬁnds a satisfying assignment without the need to backtrack.
We can express CP in a way that looks closer to SP and BP. Using the reparametrization Γa→i =
1 − M u

a→i  we get the following expression of CP.

• Message that clause a sends to variable i: Γa→i =(cid:81)
i→a =(cid:81)

• Message that variable i sends to clause a: M s

j∈Vi(a)(1 − M s
b∈Cu

j→a).
a (i)(1 − Γb→i).

Survey Propagation. Survey propagation has become a very popular propagation algorithm for
k-SAT. It was developed in the physics community by M´ezard et al. [2]. The excitement around
SP comes from its excellent empirical performance on hard satisﬁability problems; that is  k-SAT
formulae with a ratio α of the number of clauses to the number of variables near the so called
satisﬁability threshold αc.
The following is a description of an SP-inspired family of message-passing procedures  parametrized
by ρ ∈ [0  1]. It is often denoted by SP(ρ)  and contains BP (ρ = 0) and (pure) SP (ρ = 1).

• Message that clause a sends to variable i:

ηa→i =(cid:81)

Πu
j→a
j→a+Π∗
j→a+Πs

Πu

j→a

j∈Vi(a)
• Messages that variable i sends to clause a:

i→a =

(cid:16)
i→a = (cid:81)
i→a = (cid:81)

Πu

Πs
Π∗

1 − ρ(cid:81)
(cid:16)
a (i)(1 − ηb→i)(cid:81)
a (i)(1 − ηb→i)

(cid:17)(cid:81)
1 −(cid:81)
a (i)(1 − ηb→i)
a(i)(1 − ηb→i) =(cid:81)

b∈Cs
a(i)(1 − ηb→i)

(cid:17)
a(i)(1 − ηb→i)
b∈Ca(i)(1 − ηb→i)

b∈Cu
b∈Cu

b∈Cu

b∈Cs

b∈Cs

SP was originally derived via arguments and concepts from physics. A simple derivation based on a
probabilistic interpretation of CP is given in the next section of the paper. The derivation presented
here elucidates the assumptions that SP algorithms make about the satisﬁability properties and struc-
ture of k-SAT formulae. However  it is easy to establish strong equivalence relations between the
different propagation algorithms even at the basic level  before introducing the probabilistic inter-
pretation (details omitted).

5 A Probabilistic Relaxation of Constraint Propagation for Satisﬁability

The main idea behind constraint propagation relaxation (CPR) is to introduce a probabilistic model
for the k-SAT formula and view the messages as random variables in that model. If the formula f
has n variables  the sample space Ω = (2{0 1})n is the set of the n-tuple whose components are
subsets of the set of possible values that each variable i can take (i.e.  subsets of {0  1}). The “true
probability law” Pf of a SAT formula f that corresponds to CP is deﬁned in terms of the consistent
domain of f: for all W ∈ Ω 

(cid:26) 1 

Pf (W) =

0  otherwise.

if W is the consistent domain of f 

4

Clearly  if we could compute the consistent domains of the remaining variables after each variable
assignment during the search  there would be no need to backtrack. But  while it is easy to compute
consistent domains for tree CNs  it is actually hard in general for arbitrary CNs. Thus  it is generally
hard to compute Pf . (CNs with graphs of bounded tree-width are a notable exception.)
However  the probabilistic interpretation will allow us to introduce “bias” on Ω  which leads to a
heuristic for dynamically ordering both the variables and their values during search. As shown in
this section  it turns out that for arbitrary CNs  survey propagation algorithms attempt to compute
different “approximations” or “relaxations” of Pf by making different assumptions about its “prob-
abilistic structure.”
Let us now view each message M s
i→a for each variable i and clause
a as a (Bernoulli) random variable in some probabilistic model with sample space Ω and a  now
arbitrary  probability law P. 3 Formally  for each clause a  variable i and possible assignment value
xi ∈ {0  1}  we deﬁne

i→a  and M u

a→i  M u

a→i  M s

a→i ∼ Bernoulli(pxi
M xi
a→i = 1) and pxi

a→i) and M xi

i→a ∼ Bernoulli(pxi

i→a)

i = P(M u

i→a = P(M xi

a→i = P(M xi

i ≡ P(M xi

a→i = 1 for all a ∈ C−(i)) and p0

a→i because it is always 1  by the deﬁnition of M s

i→a = 1). This is a distribution over all possible
where pxi
subsets (i.e.  the power set) of the domain of each variable  not just over the variable’s domain itself.
Also  clearly we do not need to worry about ps
a→i.
The following is a description of how we can use those probabilities during search.
In the SP
community  the resulting heuristic search is called “decimation” [1  2]. If we believe that P “closely
a→i = 1 for all a ∈ C(i)) that xi is in
approximates” Pf   and know the probability pxi
the consistent domain for variable i of f  for every variable i  clause a and possible assignment
xi  we can use them to dynamically order both the variables and the values they can take during
a→i =
search. Speciﬁcally  we ﬁrst compute p1
1 for all a ∈ C +(i)) for each variable i  where C +(i) and C−(i) are the sets of clauses where
variable i appears as a positive and a negative literal  respectively. Using those probability values 
we then compute what the SP community calls the “bias” of i: |p1
i|. The variable to assign next
i − p0
is the one with the largest bias. 4 We would set that variable to the value of largest probability; for
instance  if variable i has the largest bias  then we set i next  to 1 if p1
i .
i < p0
The objective is then to compute or estimate those probabilities.
The following are (independence) assumptions about the random variables (i.e.  messages) used in
this section. The assumptions hold for tree CNs and  as formally shown below  are inherent to the
survey propagation process.
j→a for all j ∈ Vi(a) are
Assumption 1. For each clause a and variable i  the random variables M s
independent.
b→i for all clauses b ∈
Assumption 2. For each clause a and variable i  the random variables M u
C u
b→i for all clauses b ∈
Assumption 3. For each clause a and variable i  the random variables M u
C s

a (i) are independent.

i   and to 0 if p1

i = P(M u

a(i) are independent.

i > p0

Without any further assumptions  we can derive the following  by applying assumption 1 and the
expression for M u

a→i that results from 1:

a→i = P(M u
pu

j∈Vi(a) P(M s
Similarly  by assumption 2 and the expression for M s

a→i = 1) = 1 −(cid:81)
i→a = 1) =(cid:81)

i→a = P(M s
ps

j→a = 0) = 1 −(cid:81)
b→i = 1) =(cid:81)

j∈Vi(a)(1 − ps
i→a that results from 2  we derive
b→i.

b∈Cu

a (i) pu

j→a).

Using the reparametrization ηa→i = P(M u
passing procedure.

a (i) P(M u

b∈Cu
a→i = 0) = 1 − pu

a→i  we obtain the following message-

3Given clause a and variable i of SAT formula f  let Dj
i→a(W) = 1 iff Wj ⊂ Dj

a→i be the (globally) consistent domain of fa→i
for variable j. The random variables corresponding to the messages from variable i to clause a are deﬁned as
M xi
a→i. The other random variables are
j→a(W)) for all W.
then deﬁned as M s

a→i for every variable j of fa→i; and xi ∈ Di
j∈Vi(a)(1 − M s

a→i(W) = 1 −Q

a→i(W) = 1 and M u

4For both variable and value ordering  we can break ties uniformly at random. Also  the description of
SP(ρ) used often  sets a fraction β of the variables that remained unset during search. While clearly this
speeds up the process of getting a full assignment  the effect that heuristic might have on the completeness of
the search procedure is unclear  even in practice.

5

• Message that clause a sends to variable i: ηa→i =(cid:81)
i→a =(cid:81)

• Message that variable i sends to clause a: ps

j∈Vi(a)(1 − ps
b∈Cu

i→a)
a (i)(1 − ηb→i)

i→a as(cid:81)

We can then use assumption 3 to estimate pu
Note that this message-passing procedure is exactly “classical” CP if we initialize ηa→i = 0 and
i→a = 1 for all variables i and clause a. However  the version here allows the messages to be in
ps
[0  1]. At the same time  for tree CNs  this algorithm is the same as classical CP (i.e.  produces the
same result)  regardless of how the messages ηa→i and ps
i→a are initialized. In fact  in the tree case 
the ﬁnal messages uniquely identify P = Pf .

b∈Cs

a(i)(1 − ηb→i).

Making Assumptions about Satisﬁability. Let us make the following assumption about the
“probabilistic satisﬁability structure” of the k-SAT formula.
Assumption 4. For some ρ ∈ [0  1]  for each clause a and variable i 

P(M s

i→a = 0  M u

i→a = 0) = (1 − ρ)P(M s

i→a = 1  M u

i→a = 1).

i→a = 0  M u

i→a = 0  M u
i→a = 1)  which  interestingly  is equivalent to the condition P(M s

For ρ = 1  the last assumption essentially says that fa→i has a satisfying assignment;
i.e. 
P(M s
i→a = 0) = 0. For ρ = 0  it essentially says that the likelihood that fa→i does
not have a satisfying assignment is the same as the likelihood that fa→i has a satisfying assignment
i→a =
where variable i is free. Formally  in this case  we have P(M s
1  M u
i→a =
1) = 1.
Let us introduce a ﬁnal assumption about the random variables associated to the messages from
variables to clauses.
Assumption 5. For each clause a and variable i  the random variables M s
independent.

i→a = 0) = P(M s
i→a = 1) + P(M u

i→a and M u

i→a are

Note that assumptions 2  3 and 5 hold (simultaneously) if and only if for each clause a and variable
i  the random variables M u
The following theorem is the main result of this paper.
Theorem 1. (Sufﬁcient Assumptions) Let assumptions 1  2 and 3 hold. The message-passing
procedure that results from CPR as presented above is

b→i for all clauses b ∈ Ca(i) are independent.

1. belief propagation (i.e.  SP(0))  if assumption 4  with ρ = 0  holds  and
2. a member of the family of survey propagation algorithms SP(ρ)  with 0 < ρ ≤ 1  if

assumption 4  with the given ρ  and assumption 5 hold.

These assumptions are also necessary in a strong sense (details omitted)  Assumptions 1  2  3  and
even 5 might be obvious to some readers  but assumption 4 might not be  and it is essential.

Proof. As in the last subsection  assumption 1 leads to pu
assumptions 2 and 3 lead to ps
b→i and pu

b∈Cu

a (i) pu

Note also that assumption 4 is equivalent to ps
allows us to express

i→a + pu

i→a − ρ P(M s

j→a)  while

j∈Vi(a)(1 − ps
a(i) pu

b→i.
b∈Cs
i→a = 1  M u

i→a = 1) = 1. This

i→a =(cid:81)

a→i = 1 −(cid:81)
i→a =(cid:81)

P(M s

i→a = 1) = ps

i→a =

ps
i→a
i→a − ρ P(M s

i→a + pu
ps

i→a = 1  M u

i→a = 1)  

which implies

P(M s

i→a = 0) =

i→a − ρ P(M s
pu

i→a − ρ P(M s
pu

i→a = 1  M u

i→a = 1)
i→a = 1) + ps

i→a = 1  M u

.

i→a

If ρ = 0  then the last expression simpliﬁes to

P(M s

i→a = 0) =

pu
i→a
i→a + ps
pu

i→a

.

6

a→i = 0) = 1 − pu
Using the reparametrization ηa→i ≡ P(M u
i→a  leads to BP (i.e.  SP(0)).
i→a = 1) = ps
and Πs
Otherwise  if 0 < ρ ≤ 1  then using the reparametrization ηa→i ≡ P(M u

i→a ≡ P(M s

i→a + Π∗

a→i  Πu

a→i = 0) 

i→a ≡ P(M u

i→a = 1) = pu

i→a

i→a = 1) − ρ P(M s
i→a = 0  M u
i→a = 1  M u
i→a = 1  M u
and applying assumption 5 leads to SP(ρ).

i→a ≡ P(M u
Πu
= P(M s
i→a ≡ P(M s
Πs
i→a ≡ P(M s
Π∗

i→a = 1  M u

i→a = 1)

i→a = 1) + (1 − ρ)P(M s
i→a = 0)  and
i→a = 1) 

i→a = 1  M u

i→a = 1) 

The following are some remarks that can be easily derived using CPR.

On the Relationship Between SP and BP.
SP essentially assumes that every sub-formula fa→i
has a satisfying assignment  while BP assumes that for every clause a and variable i ∈ V (a)  variable
i is equally likely not to have a satisfying assignment or being free in fa→i  as it is easy to see from
assumption 4. The parameter ρ just modulates the relative scaling of those two likelihoods. While
the same statement about pure SP is not novel  the statement about BP  and more generally  the class
SP(ρ) for 0 ≤ ρ < 1  seems to be.

On the Solutions of SAT formula f. Note that Pf may not satisfy all or any of the assumptions.
Yet  satisfying an assumption imposes constraints on what Pf actually is and thus on the solution
space of f. For example  if Pf satisﬁes assumption 4 for any ρ < 1  which includes BP when ρ = 0 
i→a = 0  M u
and for all clauses a and variables i  then Pf (M s
i→a =
1) = 0 and therefore either Pf (M s
i→a = 1) = 1
i→a = 0) = 1 or Pf (M s
i→a = 1  M u
holds  but not both of course. That implies f must have a unique solution!

i→a = 0) = Pf (M s

i→a = 0  M u

i→a = 1  M u

i→a = 0  M u

On SP. This result provides additional support to previous informal conjectures as to why SP is
so effective near the satisﬁability threshold: SP concentrates all its efforts on ﬁnding a satisfying
assignment when they are scarce and “scattered” across the space of possible assignments. Thus  SP
assumes that the set of satisfying assignments has in fact special structure.
i→a = 0) = 0
To see that  note that assumptions 4  with ρ = 1  and 5 imply that P(M s
or P(M s
i→a = 1) = 0 must hold. This says that in every assignment that satisﬁes
fa→i  variable i is either free or always has the same value assignment. This observation is relevant
because it has been argued that as we approach the satisﬁability threshold  the set of satisfying
assignments decomposes into many “local” or disconnected subsets.
It follows easily from the
discussion here that SP assumes such a structure  therefore potentially making it most effective
under those conditions (see Maneva et al. [3] for more information).
Similarly  it has also been empirically observed that SP is more effective for ρ close to  but strictly
less than 1. The CPR approach suggests that such behavior might be because  with respect to any
P that satisﬁes assumption 4  unlike pure SP  for such values of ρ < 1  SP(ρ) guards against the
possibility that fa→i is not satisﬁable  while still being somewhat optimistic by giving more weight
to the event that variable i is free in fa→i. Naturally  BP  which is the case of ρ = 0  might be too
pessimistic in this sense.

i→a = 1  M u

i→a = 0  M u

i→a = 1  M u

On BP. For BP (ρ = 0)  making the additional assumption that the formula fa→i is satisﬁable
(i.e.  P(M s
i→a = 0) = 0) implies that there are no assignments with free variables (i.e. 
P(M s
i→a = 1) = 0). Therefore  the only possible consistent domain is the singleton
{sa i} or {ua i} (i.e.  P(M s
i→a = 1) = 1). Thus 
either 0 or 1 can possibly be a consistent value assignment  but not both. This suggests that BP is
concentrating its efforts on ﬁnding satisfying assignments without free variables.

i→a = 0) + P(M s

i→a = 1  M u

i→a = 0  M u

On Variable and Value Ordering. To complete the picture of the derivation of SP(ρ) via CPR 
we need to compute p0
i for all variables i to use for variable and value ordering during search.
We can use the following  slightly stronger versions of assumptions 2 and 3 for that.
a→i for all clauses a ∈ C−(i) are
Assumption 6. For each variable i  the random variables M u
independent.

i and p1

7

Assumption 7. For each variable i  the random variables M u
independent.

(cid:81)
Using assumptions 6 and 7  we can easily derive that p1
a∈C+(i)(1 − ηa→i)  respectively.

i = (cid:81)

a→i for all clauses a ∈ C +(i) are

a∈C−(i)(1 − ηa→i) and p0

i =

On Generalizations. The approach provides a general  simple and principled way to introduce
possibly uncertain domain knowledge into the problem by making assumptions about the structure
of the set of satisfying assignments and incorporating them through P. That can lead to more
effective propagation algorithms for speciﬁc contexts.

Related Work. Dechter and Mateescu [10] also connect BP to CP but in the context of the in-
ference problem of assessing zero posterior probabilities. Hsu and McIlraith [11] give an intuitive
explanation of the behavior of SP and BP from the perspective of traditional local search methods.
They provide a probabilistic interpretation  but the distribution used there is over the biases.
Braunstein and Zecchina [12] showed that pure SP is equivalent to BP on a particular MRF over
an extended domain on the variables of the SAT formula  which adds a so called “joker” state.
Maneva et al. [3] generalized that result by showing that SP(ρ) is only one of many families of
algorithms that are equivalent to performing BP on a particular MRF. In both cases  one can easily
interpret those MRFs as ultimately imposing a distribution over Ω  as deﬁned here  where the joker
state corresponds to the domain {0  1}. Here  the only particular distribution explicitly deﬁned is
Pf   the “optimal” distribution. This paper does not make any explicit statements about any speciﬁc
distribution P for which applying CPR leads to SP(ρ).

6 Conclusion

This paper strongly connects survey and constraint propagation. In fact  the paper shows how survey
propagation algorithms are instances of CPR  the probabilistic generalization of classical constraint
propagation proposed here. The general approach presented not only provides a new view on survey
propagation algorithms  which can lead to a better understanding of them  but can also be used to
easily develop potentially better algorithms tailored to speciﬁc classes of CSPs.

References
[1] A. Braunstein  M. M´ezard  and R. Zecchina. Survey propagation: An algorithm for satisﬁability. Random

Structures and Algorithms  27:201  2005.

[2] M. M´ezard  G. Parisi  and R. Zecchina. Analytic and Algorithmic Solution of Random Satisﬁability

Problems. Science  297(5582):812–815  2002.

[3] E. Maneva  E. Mossel  and M. J. Wainwright. A new look at survey propagation and its generalizations.

ACM  54(4):2–41  July 2007.

[4] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Networks of Plausible Inference. Morgan Kauf-

mann  1988.

[5] D. L. Waltz. Generating semantic descriptions from drawings of scenes with shadows. Technical Report

271  MIT AI Lab  Nov. 1972. PhD Thesis.

[6] S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach  chapter 5  pages 137–160. Prentice

Hall  second edition  1995.

[7] V. Kumar. Algorithms for constraint-satisfaction problems: A survey. AI Magazine  13(1):32–44  1992.
[8] R. Dechter. Constraint Processing. Morgan Kaufmann  2003.
[9] F. Bacchus  X. Chen  P. van Beek  and T. Walsh. Binary vs. non-binary constraints. AI  140(1-2):1–37 

Sept. 2002.

[10] R. Dechter and R. Mateescu. A simple insight into iterative belief propagation’s success. In UAI  2003.
[11] E. I. Hsu and S. A. McIlraith. Characterizing propagation methods for boolean satisﬁability. In SAT 

2006.

[12] A. Braunstein and R. Zecchina. Survey propagation as local equilibrium equations. JSTAT  2004.

8

,Fang Han
Han Liu
Robert Wang
Xiang Li
Charles Ling