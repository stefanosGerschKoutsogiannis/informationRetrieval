2019,Multiview Aggregation for Learning Category-Specific Shape Reconstruction,We investigate the problem of learning category-specific 3D shape reconstruction from a variable number of RGB views of previously unobserved object instances. Most approaches for multiview shape reconstruction operate on sparse shape representations  or assume a fixed number of views. We present a method that can estimate dense 3D shape  and aggregate shape across multiple and varying number of input views. Given a single input view of an object instance  we propose a representation that encodes the dense shape of the visible object surface as well as the surface behind line of sight occluded by the visible surface. When multiple input views are available  the shape representation is designed to be aggregated into a single 3D shape using an inexpensive union operation. We train a 2D CNN to learn to predict this representation from a variable number of views (1 or more). We further aggregate multiview information by using permutation equivariant layers that promote order-agnostic view information exchange at the feature level. Experiments show that our approach is able to produce dense 3D reconstructions of objects that improve in quality as more views are added.,Multiview Aggregation for Learning

Category-Speciﬁc Shape Reconstruction

Srinath Sridhar1 Davis Rempe1 Julien Valentin2 Soﬁen Bouaziz2 Leonidas J. Guibas1 3

R ssrinath@cs.stanford.edu

(cid:140) geometry.stanford.edu/projects/xnocs

1Stanford University

2Google Inc.

3Facebook AI Research

Abstract

We investigate the problem of learning category-speciﬁc 3D shape reconstruction
from a variable number of RGB views of previously unobserved object instances.
Most approaches for multiview shape reconstruction operate on sparse shape
representations  or assume a ﬁxed number of views. We present a method that
can estimate dense 3D shape  and aggregate shape across multiple and varying
number of input views. Given a single input view of an object instance  we propose
a representation that encodes the dense shape of the visible object surface as well
as the surface behind line of sight occluded by the visible surface. When multiple
input views are available  the shape representation is designed to be aggregated
into a single 3D shape using an inexpensive union operation. We train a 2D CNN
to learn to predict this representation from a variable number of views (1 or more).
We further aggregate multiview information by using permutation equivariant
layers that promote order-agnostic view information exchange at the feature level.
Experiments show that our approach is able to produce dense 3D reconstructions
of objects that improve in quality as more views are added.

1

Introduction

Learning to estimate the 3D shape of objects observed from one or more views is an important
problem in 3D computer vision with applications in robotics  3D scene understanding  and augmented
reality. Humans and many animals perform well at this task  especially for known object categories 
even when observed object instances have never been encountered before [27]. We are able to infer
the 3D surface shape of both object parts that are directly visible  and of parts that are occluded
by the visible surface. When provided with more views of the instance  our conﬁdence about its
shape increases. Endowing machines with this ability would allow us to operate and reason in
new environments and enable a wide range of applications. We study this problem of learning
category-speciﬁc 3D surface shape reconstruction given a variable number of RGB views (1 or more)
of an object instance.
There are several challenges in developing a learning-based solution for this problem. First  we need
a representation that can encode the 3D geometry of both the visible and occluded parts of an object
while still being able to aggregate shape information across multiple views. Second  for a given
object category  we need to learn to predict the shape of new instances from a variable number of
views at test time. We address these challenges by introducing a new representation for encoding
category-speciﬁc 3D surface shape  and a method for learning to predict shape from a variable number
of views in an order-agnostic manner.
Representations such as voxel grids [6]  point clouds [9  17]  and meshes [11  40] have previously
been used for learning 3D shape. These representations can be computationally expensive to operate
on  often produce only sparse or smoothed-out reconstructions  or decouple 3D shape from 2D

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: An input RGB view of a previously unseen object instance (a). Humans are capable of
inferring the shape of the visible object surface (original colors in (b)) as well as the parts that are
outside the line of sight (separated by red line in (b)). We propose an extended version of the NOCS
map representation [39] to encode both the visible surface (c) and the occluded surface furthest from
the current view  the X-NOCS map (d). Note that (c) and (d) are in exact pixel correspondence to
(a)  and their point set union yields the complete 3D shape of the object. RGB colors denote the XYZ
position within NOCS. We learn category-speciﬁc 3D reconstruction from one or more views.

projection losing 2D–3D correspondence. To overcome these issues  we build upon the normalized
object coordinate space maps (NOCS maps) representation [39]—a 2D projection of a shared
category-level 3D object shape space that can encode intra-category shape variation (see Figure 1). A
NOCS map can be interpreted as a 3D surface reconstruction in a canonical space of object pixels
directly visible in an image. NOCS maps retain the advantages of point clouds and are implicitly
grounded to the image since they provide a strong pixel–shape correspondence—a feature that allows
us to copy object texture from the input image. However  a NOCS map only encodes the surface
shape of object parts directly in the line of sight. We extend it to also encode the 3D shape of object
parts that are occluded by the visible surface by predicting the shape of the object surface furthest
and hidden from the view—called X-NOCS maps (see Figure 1). Given a single RGB view of an
object instance  we aim to reconstruct the NOCS maps corresponding to the visible surface and the
X-NOCS map of the occluded surface. Given multiple views  we aggregate the predicted NOCS and
X-NOCS maps from each view into a single 3D shape using an inexpensive union operation.
To learn to predict these visible and occluded NOCS maps for one or more views  we use an encoder-
decoder architecture based on SegNet [3]. We show that a network can learn to predict shape
independently for each view. However  independent learning does not exploit multiview overlap
information. We therefore propose to aggregate multiview information in a view order-agnostic
manner by using permutation equivariant layers [43] that promote information exchange among
the views at the feature level. Thus  our approach aggregates multiview information both at the
shape level  and at the feature level enabling better reconstructions. Our approach is trained on a
variable number of input views and can be used on a different variable number of views at test time.
Extensive experiments show that our approach outperforms other state-of-the-art approaches  is able
to reconstruct object shape with ﬁne details  and accurately captures dense shape while improving
reconstruction as more views are added  both during training and testing.

2 Related Work

Extensive work exists on recognizing and reconstructing 3D shape of objects from images. This
review focuses on learning-based approaches which have dominated recent state of the art  but we
brieﬂy summarize below literature on techniques that rely purely on geometry and constraints.
Non-Learning 3D Reconstruction Methods: The method presented in [22] requires user input to
estimate both camera intrinsics and multiple levels of reconstruction detail using primitives which
allow complete 3D reconstruction. The approach of [38] also requires user input but is more data-
driven and targets class-based 3D reconstruction of the objects on the Pascal VOC dataset. [4] is
another notable approach for class-based 3D reconstruction a parametric 3D model and corresponding
parameters per object-instance are predicted with minimal user intervention. We now focus on
learning-based methods for single and multiview reconstruction.
Single-View Reconstruction:
Single-view 3D reconstruction of objects is a severely under-
constrained problem. Probabilistic or generative techniques have been used to impose constraints
on the solution space. For instance  [21] uses structure from motion to estimate camera parameters

2

and learns category-speciﬁc generative models. The approach of [9] learns a generative model of
un-ordered point clouds. The method of [15] also argue for learning generative models that can
predict 3D shape  pose and lighting from a single image. Most techniques implicitly or explicitly
learn class-speciﬁc generative models  but there are some  e.g.  [36]  that take a radically different
approach and use multiple views of the same object to impose a geometric loss during training. The
approach of [42] predicts 2.5D sketches in the form of depth  surface normals  and silhouette images
of the object. It then infers the 3D object shape using a voxel representation. In [13]  the authors
present a technique that uses silhouette constraints. That loss is not well suited for non-convex objects
and hence the authors propose to use another set of constraints coming from a generative model
which has been taught to generate 3D models. Finally  [44] propose an approach that ﬁrst predicts
depth from a 2D image which is then projected onto a spherical map. This map is inpainted to ﬁll
holes and backprojected into a 3D shape.
Multiview Reconstruction: Multiple views of an object add more constraints to the reconstructed
3D shape. Some of the most popular constraints in computer vision are multiview photometric
consistency  depth error  and silhouette constraints [18  41]. In [20]  the authors assume that the pose
of the camera is given and extract image features that are un-projected in 3D and iteratively fused
with the information from other views into a voxel grid. Similarly  [16] uses structure from motion to
extract camera calibration and pose. [23] proposes an approach to differentiable point-cloud rendering
that effectively deals with the problem of visibility. Some approaches jointly perform the tasks of
estimating the camera parameters as well as reconstructing the object in 3D [17  45].
Permutation Invariance and Equivariance: One of the requirements of supporting a variable
number of input views is that the network must be agnostic to the order of the inputs. This is not
the case with [6] since their RNN is sensitive to input view order. In this work  we use ideas of
permutation invariance and equivariance from DeepSets [29  43]. Permutation invariance has been
used in computer vision in problems such as burst image deblurring [2]  shape recognition [35]  and
3D vision [28]. Permutation equivariance is not as widely used in vision but is common in other
areas [29  30]. Other forms of approximate equivariance have been used in multiview networks [7].
A detailed theoretical analysis is provided by [25].
Shape Representations: There are two dominant families of shape representations used in literature:
volumetric and surface representations  each with their trade-offs in terms of memory  closeness to
the actual surface and ease of use in neural networks. We offer a brief review and refer the reader to
[1  34] for a more extensive study.
The voxel representation is the most common volumetric representation because of its regular grid
structure  making convolutional operators easy to implement. As illustrated in [6] which performs
single and multiview reconstructions  voxels can be used as an occupancy grid  usually resulting
in coarse surfaces. [26] demonstrates high quality reconstruction and geometry completion results.
However  voxels have high memory cost  especially when combined with 3D convolutions. This
has been noted by several authors  including [31] who propose to ﬁrst predict a series of 6 depth
maps observed from each face of a cube containing the object to reconstruct. Each series of 6 depth
map represent a different surface  allowing to efﬁciently capture both the outside and the inside
(occluded) parts of objects. These series of depth maps are coined shape layers and are combined in
an occupancy grid to obtain the ﬁnal reconstructions.
Surface representations have advantages such as compactness  and are amenable to differentiable
operators that can be applied on them. They are gaining popularity in learning 3D reconstruction with
works like [19]  where the authors present a technique for predicting category-speciﬁc mesh (and
texture) reconstructions from single images  or explorations like in [9]  which introduces a technique
for reconstructing the surface of objects using point clouds. Another interesting representation is
scene coordinates which associates each pixel in the image with a 3D position on the surface of the
object or scene being observed. This representation has been successfully used for several problems
including camera pose estimation [37] and face reconstruction [10]. However  it requires a scene- or
instance-speciﬁc scan to be available. Finally  geometry images [12] have been proposed to encode
3D shape in images. However  they lack input RGB pixel to shape correspondence.
In this work  we propose a category-level surface representation that has the advantages of point
clouds but encodes strong pixel–3D shape correspondence which allows multiview shape aggregation
without explicit correspondences.

3

Figure 2: Given canonically aligned and scaled instances from an object category [5]  the NOCS
representation [39] can be used to encode intra-category shape variation. For a single view (a)  a
NOCS map encodes the shape of the visible parts of the object (b). We extend this representation
to also encode the occluded parts called an X-NOCS map (c). Multiple (X-)NOCS maps can be

trivially combined using a set union operation ((cid:83)) into a single dense shape (rightmost). We can also

efﬁciently represent the texture of object surfaces that are not directly observable (d). Inputs to our
method are shown in green boxes  predictions are in red  and optional predictions are in orange.

3 Background and Overview

In this section  we provide a description of our shape representation  relevant background  and a
general overview of our method.
Shape Representation: Our goal is to design a shape representation that can capture dense shapes of
both the visible and occluded surfaces of objects observed from any given viewpoint. We would like
a representation that can support computationally efﬁcient signal processing (e.g.  2D convolution)
while also having the advantages of 3D point clouds. This requires a strong coupling between image
pixels and 3D shapes. We build upon the NOCS map [39] representation  which we describe below.

The Normalized Object Coordinates Space (NOCS) can
be described as the 3D space contained within a unit cube
as shown in Figure 2. Given a collection of shapes from
a category which are consistently oriented and scaled  we
build a shape space where the XYZ coordinates within
NOCS represent the shape of an instance. A NOCS map
is a 2D projection of the 3D NOCS points of an instance
as seen from a particular viewpoint. Each pixel in the
NOCS map denotes the 3D position of that object point
in NOCS (color coded in Figure 2). NOCS maps are
dense shape representations that scale with the size of the
object in the view—objects that are closer to the camera
with more image pixels are denser than object further
away. They can readily be converted to a point cloud by
reading out the pixel values  but still retain 3D shape–pixel
correspondence. Because of this correspondence we can
obtain camera pose in the canonical NOCS space using the direct linear transform algorithm [14].
However  NOCS maps only encode the shape of the visible surface of the object.
Depth Peeling: To overcome this limitation and encode the shape of the occluded object surface  we
build upon the idea of depth peeling [8] and layered depth images [33]. Depth peeling is a technique
used to generate more accurate order-independent transparency effects when blending transparent
objects. As shown in Figure 3  this process refers to the extraction of object depth or  alternatively 
NOCS coordinates corresponding to the kth intersection of a ray passing through a given image
pixel. By peeling a sufﬁciently large number of layers (e.g.  k = 10)  we can accurately encode the
interior and exterior shape of an object. However  using many layers can be unnecessarily expensive 
especially if the goal is to estimate only the external object surface. We therefore propose to use 2
layers to approximate the external surfaces corresponding the ﬁrst and last ray intersections. These
intersections faithfully capture the visible and occluded parts of most common convex objects. We

Figure 3: We use depth peeling to extract
X-NOCS maps corresponding to differ-
ent ray intersections. The top row shows
4 intersections. The bottom row shows
our representation which uses the ﬁrst
and last intersections.

4

refer to the maps corresponding to the occluded surface (i.e.  last ray intersection) as X-NOCS maps 
similar to X-ray images.
Both NOCS and X-NOCS maps support multiview shape aggregation into a single 3D shape using an
inexpensive point set union operation. This is because NOCS is a canonical and normalized space
where multiple views correspond to the same 3D space. Since these maps preserve pixel–shape
correspondence  they also support estimation of object or camera pose in the canonical NOCS
space [39]. We can use the direct linear transform [14] to estimate camera pose  up to an unknown
scale factor (see supplementary document). Furthermore  we can support the prediction of the texture
of the occluded parts of the object by hallucinating a peeled color image (see Figure 2 (d)).
Learning Shape Reconstruction: Given the X-NOCS map representation that encodes the 3D shape
both of occluded object surfaces  our goal is to learn to predict both maps from a variable number of
input views and aggregate multiview predictions. We adopt a supervised approach for this problem.
We generated a large corpus of training data with synthetic objects from 3 popular categories—cars 
chairs  and airplanes. For each object we render multiple viewpoints  as well the corresponding
ground truth X-NOCS maps. Our network learns to predict the (X-)NOCS maps corresponding to
each view using a SegNet-based [3] encoder-decoder architecture. Learning independently on each
view does not exploit the available multiview overlap information. We therefore aggregate multiview
information at the feature level by using permutation equivariant layers that combine input view
information in an order-agnostic manner. The multiview aggregation that we perform at the NOCS
shape and feature levels allows us to reconstruct dense shape with details as we show in Section 5.

4 Method

Our goal is to learn to predict the both NOCS and X-NOCS maps corresponding to a variable number
of input RGB views of previously unobserved object instances. We adopt a supervised learning
approach and restrict ourselves to speciﬁc object categories. We ﬁrst describe our general approach
to this problem and then discuss how we aggregate multiview information.

4.1 Single-View (X-)NOCS Map Prediction

the output (X-)NOCS maps are combined into a single 3D point cloud as P = R(Nv)(cid:83)R(No) 

The goal of this task is to predict the NOCS maps for the visible (Nv) and X-NOCS maps for the
occluded parts (No) of the object given a single RGB view I. We assume that no other multiview
inputs are available at train or test time. For this pixel-level prediction task we use an encoder-
decoder architecture similar to SegNet [3] (see Figure 4). Our architecture takes a 3 channel RGB
image as input and predicts 6 output channels corresponding to the NOCS and X-NOCS maps
(N i = {Nv  No})  and optionally also predicts a peeled color map (Cp) encoding the texture of the
occluded object surface (see Figure 2 (d)). We include skip connections between the encoder and
decoder to promote information sharing and consistency. To obtain the 3D shape of object instances 
where R denotes a readout operation that converts each map to a 3D point set.
(X-)NOCS Map Aggregation: While single-view (X-)NOCS map prediction is trained indepen-
dently on each view  it can still be used for multiview shape aggregation. Given multiple input
views  {I0  . . .   In}  we predict the (X-)NOCS maps {N 0  . . .   N n} for each view independently.
NOCS represents a canonical and normalized space and thus (X-)NOCS maps can also be interpreted
as dense correspondences between pixels and 3D NOCS space. Therefore any set of (X-)NOCS
maps will map into the same space—multiview consistency is implicit in the representation. Given
multiple independent (X-)NOCS maps  we can combine them into a single 3D point cloud as

Pn =(cid:83)n

i=0 R(N i).

Loss Functions: We experimented with several loss functions for (X-)NOCS map prediction
including a pixel-level L2 loss  and a combined pixel-level mask and L2 loss. The L2 loss is deﬁned
as

(cid:88)||y − ˆy||2  ∀y ∈ Nv  No ∀ˆy ∈ ˆNv  ˆNo 

(1)
where y  ˆy ∈ R3 denote the ground truth and predicted 3D NOCS value  ˆNv  ˆNo are the predicted
NOCS and X-NOCS maps  and n is the total number of pixels in the X-NOCS maps. However  this
function computes the loss for all pixels  even those that do not belong to the object thus wasting

Le(y  ˆy) =

1
n

5

Figure 4: We use an encoder-decoder architecture based on SegNet [3] to predict NOCS and X-NOCS
maps from an input RGB view independently. To better exploit multiview information  we propose to
use the same architecture but with added permutation equivariant layers (bottom right) to combine
multiview information at the feature level. Our network can operate on a variable number of input
views in an order-agnostic manner. The features extracted for each view during upsampling and
downsampling operations are combined using permutation equivariant layers (orange bars).

network capacity. We therefore use object masks to restrict the loss computation only to the object
pixels in the image. We predict 2 masks corresponding to the NOCS and X-NOCS maps—8 channels
in total. We predict 2 independent masks since they could be different for thin structures like airplane
tail ﬁns. The combined mask loss is deﬁned as Lm = Lv + Lo  where the loss for the visible NOCS
map and mask is deﬁned as

(cid:88)||y − ˆy||2  ∀y ∈ Mv ∀ˆy ∈ ˆMv 

(2)

Lv(y  ˆy) = wm M(Mv  ˆMv) + wl

1
m

where ˆMv is the predicted mask corresponding to the visible NOCS map  Mv is the ground truth
mask  M is the binary cross entropy loss on the mask  and m is the number of masked pixels. Lo
is identical to Lv but for the X-NOCS map. We empirically set the weights wm and wl to be 0.7
and 0.3 respectively. Experimentally  we observe that the combined pixel-level mask and L2 loss
outperforms the L2 loss since more network capacity can be utilized for shape prediction.

4.2 Multiview (X-)NOCS Map Prediction

The above approach predicts (X-)NOCS maps independently and aggregates them to produce a 3D
shape. However  multiview images of an object have strong inter-view overlap information which we
have not made use of. To promote information exchange between views both during training and
testing  and to support a variable number of input views  we propose to use permutation equivariant
layers [43] that are agnostic to the order of the views.
Feature Level Multiview Aggregation: Our multiview aggregation network is illustrated in Figure 4.
The network is identical to the single-view network except for the addition of several permutation
equivariant layers (orange bars). A network layer is said to be permutation equivariant if and only if
the off diagonal elements of the learned weight matrix are equal  as are the diagonal elements [43].
In practice  this can be achieved by passing each feature map through a pool-subtract operation
followed by a non-linear function. The pool-subtract operation pools features extracted from different
viewpoints and subtracts the pooled feature from the individual features (see Figure 4). We use
multiple permutation equivariant layers after each downsampling and upsampling operation in the
encoder-decoder architecture (vertical orange bars in Figure 4). Both average pooling and max
pooling can used but experimentally average pooling worked best. Our permutation equivariant layers
consist of an average-subtraction operation and the non-linearity from the next convolutional layer.
Hallucinating Occluded Object Texture: As an additional feature  we train both our single and
multiview networks to also predict the texture of the occluded surface of the object (see Figure 2 (d)).
This is predicted as 3 additional output channels with the same loss as Lv. This optional prediction
can be used to hallucinate the texture of hidden object surfaces.

6

5 Experiments

Dataset: We generated our own dataset  called ShapeNetCOCO  consisting of object instances
from 3 categories commonly used in related work: chairs  cars  and airplanes. We use thousands
of instances from the ShapeNet [5] repository and render 20 different views for each instance and
additionally augment backgrounds with randomly chosen COCO images [24]. This dataset is harder
than previously proposed datasets because of random backgrounds  and widely varying camera
distances. To facilitate comparisons with previous work [6  17]  we also generated a simpler dataset 
called ShapeNetPlain  with white backgrounds and 5 views per object following the camera placement
procedure of [17]. Except for comparisons and Table 3  we report results from the more complex
dataset. We follow the train/test protocol of [36]. Unless otherwise speciﬁed  we use a batch size of 1
(multiview) or 2 (single-view)  a learning rate of 0.0001  and the Adam optimizer.
Metrics: For all experiments  we evaluate point cloud reconstruction using the 2-way Chamfer
distance multiplied by 100. Given two point sets S1 and S2 the Chamfer distance is deﬁned as

min
y∈S2

(cid:107)x − y(cid:107)2

2 +

1
|S2|

(cid:107)x − y(cid:107)2
2.

min
x∈S1

(3)

(cid:88)

x∈S1

d(S1  S2) =

1
|S1|

5.1 Design Choices

(cid:88)

y∈S2

Table 1: Single-view reconstruction performance using various
losses and outputs. For each category  the Chamfer distance is
shown. Using the joint loss with L2 and the mask signiﬁcantly
outperforms just L2. Predicting peeled color further improves
reconstruction.

We ﬁrst justify our loss function
choice and network outputs. As
described  we experiment with
two loss functions—L2 losses
with and without a mask. Further 
there are several outputs that we
predict in addition to the NOCS
and X-NOCS maps i.e.  mask and
peeled color. In Table 1  we sum-
marize the average Chamfer dis-
tance loss for all variants trained
independently on single views (ShapeNetCOCO dataset). Using the loss function which jointly
accounts for NOCS map  X-NOCS maps and mask output clearly outperforms a vanilla L2 loss on the
NOCS and X-NOCS maps. We also observe that predicting peeled color along with the (X-)NOCS
maps gives better performance on all categories.

Output
(X-)NOCS+Peel
(X-)NOCS+Mask
(X-)NOCSS+Mask+Peel

Airplanes Chairs
7.9072
4.4716
0.4401
0.3037
0.2659
0.4288

Loss
L2
L2+Mask
L2+Mask

Cars
3.6573
0.5093
0.3714

5.2 Multiview Aggregation

Model

Category

Table 2: Comparison of different forms of multiview ag-
gregation. Aggregating multiple views using set union
improves performance with further improvements using
feature space aggregation.

Next we show that our multiview aggre-
gation approach is capable of estimating
better reconstructions when more views
are available (ShapeNetCOCO dataset).
Table 2 shows that the reconstruction
from the single view network improves
as we aggregate more views into NOCS
space (using set union) without any fea-
ture space aggregation. When we train
with feature space aggregation from 5
views using the permutation equivariant
layers we see further improvements as
more views are added. Table 3 shows
variations of our multiview model: one trained on a ﬁxed number of views  one trained on a variable
number of views up to a maximum of 5  and one trained on a variable number up to 10 views.
All these models are trained on the ShapeNetPlain dataset for 100 epochs. We see that both ﬁxed
and variable models take advantage of the additional information from more views  almost always
increasing performance from left to right. Although the ﬁxed multiview models perform best  we
hypothesize that the variable view models will be able to better handle the widening gap between the
number of train-time and test-time views. In Figure 5  we visualize our results in 3D which shows
the small scale details such as airplane engines reconstructed by our method.

2 views
Single-View 0.4206
Multiview 0.3789
Single-View 0.1760
Multiview 0.2387
Single-View 0.4249
Multiview 0.3649

5 views
0.3692
0.2731
0.1619
0.1277
0.3600
0.2457

3 views
0.3974
0.3537
0.1677
0.1782
0.3813
0.2860

Airplanes

Cars

Chairs

7

Figure 5: Qualitative reconstructions produced by our method. Each rows shows the input RGB
views  NOCS map ground truth and prediction of the central view  and the ground truth and predicted
3D shape. These visualizations are produced by the variable multiview model trained on up to 5
views and evaluated on 5 views. We post-process the both NOCS and X-NOCS maps with a bilateral
ﬁlter followed by a statistical outlier ﬁlter [32]  and use the input RGB images to color the point
cloud. Best viewed zoomed and in color.

5.3 Comparisons

2 views
0.2645
0.2896
0.2992
0.1318
0.1418
0.1847
0.2967
0.2642
0.2643

Category

Cars

Airplanes

Chairs

5 views
0.1721
0.1955
0.3095
0.0604
0.0991
0.1049
0.1314
0.1695
0.1693

3 views
0.1645
0.1989
0.2447
0.1571
0.1006
0.1309
0.1845
0.2072
0.2070

Model
Fixed Multi
Variable Multi (5)
Variable Multi (10)
Fixed Multi
Variable Multi (5)
Variable Multi (10)
Fixed Multi
Variable Multi (5)
Variable Multi (10)

Table 3: Multiview reconstruction variations. We observe
that both ﬁxed and variable models take advantage of the
additional information from more views.

We compare our method to two previous
works. The ﬁrst  called differentiable
point clouds (DPC) [17] directly predicts
a point cloud given a single image of an
object. We train a separate single-view
model for cars  airplanes  and chairs to
predict the NOCS maps  X-NOCS maps 
mask and peeled color (ShapeNetPlain
dataset). To evaluate the Chamfer dis-
tance for DPC outputs  we ﬁrst scale the
predicted output point cloud such that
the bounding box diagonal is one  then
we follow the alignment procedure from
their paper to calculate the transforma-
tion from the network’s output frame to the ground truth point cloud frame. As seen in Table 4  the
X-NOCS map representation allows our network to outperform DPC in all three categories.
We next compare our multiview permutation equivari-
ant model to the multiview method 3D-R2N2 [6]. In
each training batch  both methods are given a random
subset of 5 views of an object  so that they may be eval-
uated with up to 5 views at test time. Since 3D-R2N2
outputs a volumetric 32x32x32 voxel grid  we ﬁrst ﬁnd
all surface voxels of the output then place a point at
the center of these surface voxels to obtain a 3D point cloud. This point cloud is scaled to have a
unit-diagonal bounding box to match the ground truth ShapeNet objects. We limit our comparison to
only chairs since we were unable to make their method converge on the other categories.

Table 4: Single-view reconstruction com-
parison to DPC [17].

Method Cars
DPC
Ours

0.2932
0.1569

Airplanes Chairs
0.4314
0.2549
0.1855
0.3803

Table 5: Multiview reconstruction performance compared
to 3D-R2N2 [6] on the chairs category in ShapeNetPlain.

Method
3D-R2N2
Ours

2 views
0.2511
0.2508

3 views
0.2191
0.1952

5 views
0.1932
0.1576

Table 5 shows the performance of both
methods when trained on the chairs cat-
egory and evaluated on 2  3  and 5 views
(ShapeNetPlain dataset). For 2 views
the methods perform similar but when
combining more views to reconstruct the
shape  our method becomes more accu-

rate. We again see the trend of increasing performance as more views are used.

8

Figure 6: More qualitative reconstructions produced by our method. For each box  ground truth is
shown leftmost. Here we show reconstructions from (a) the permutation equivariant network trained
and tested on 10 views for a car  and (b  c) permutation equivariant network trained on chairs with 5
views and tested on 5. A reconstruction with higher shape variance that fails to capture small scale
detail is shown in (d). Finally  in (e) we show a visual comparison with the reconstruction produced
by [6] which lacks detail such as the armrest although it sees 5 different views. Best viewed in color.

Limitations and Future Work: While we reconstruct dense 3D shapes  there is still some variance
in our predicted shape. We can further improve the quality of our reconstructions by incorporating
surface topology information. We currently use the DLT algorithm [14] to predict camera pose in our
canonical NOCS space  however we would need extra information such as depth [39] to estimate
metric pose. Jointly estimating pose and shape is a tightly coupled problem and an interesting
future direction. Finally  we observed that Chamfer distance  although used widely to evaluate shape
reconstruction quality  is not the ideal metric to help differentiate ﬁne scale detail and overall shape.
We plan to explore the use of the other metrics to evaluate reconstruction quality.

6 Conclusion

In this paper we introduced X-NOCS maps  a new and efﬁcient surface representation that is well
suited for the task of 3D reconstruction of objects  even of occluded parts  from a variable number of
views. We demonstrate how this representation can be used to estimate the ﬁrst and the last surface
point that would project on any pixel in the observed image  and also to estimate the appearance
of these surface points. We then show how adding a permutation equivariant layer allows the
proposed method to be agnostic to the number of views and their associated viewpoints  but also
how our aggregation network is able to efﬁciently combine these observations to yield even higher
quality results compared to those obtained with a single observation. Finally  extensive analysis and
experiments validate that our method reaches state-of-the-art results using a single observation  and
signiﬁcantly improves upon existing techniques.
Acknowledgments: This work was supported by the Google Daydream University Research Pro-
gram  AWS Machine Learning Awards Program  and the Toyota-Stanford Center for AI Research.
We would like to thank Jiahui Lei  the anonymous reviewers  and members of the Guibas Group for
useful feedback. Toyota Research Institute (“TRI”) provided funds to assist the authors with their
research but this article solely reﬂects the opinions and conclusions of its authors and not TRI or any
other Toyota entity.

References
[1] Eman Ahmed  Alexandre Saint  Abd El Rahman Shabayek  Kseniya Cherenkova  Rig Das  Gleb Gu-
sev  Djamila Aouada  and Bjorn Ottersten. A survey on deep learning advances on different 3d data
representations. arXiv preprint arXiv:1808.01462  2018.

[2] Miika Aittala and Frédo Durand. Burst image deblurring using permutation invariant convolutional neural
networks. In Proceedings of the European Conference on Computer Vision (ECCV)  pages 731–747  2018.

9

[3] Vijay Badrinarayanan  Alex Kendall  and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder
architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence 
39(12):2481–2495  2017.

[4] Thomas J Cashman and Andrew W Fitzgibbon. What shape are dolphins? building 3d morphable models
from 2d images. IEEE transactions on pattern analysis and machine intelligence  35(1):232–244  2012.

[5] Angel X Chang  Thomas Funkhouser  Leonidas Guibas  Pat Hanrahan  Qixing Huang  Zimo Li  Silvio
Savarese  Manolis Savva  Shuran Song  Hao Su  et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012  2015.

[6] Christopher Bongsoo Choy  Danfei Xu  JunYoung Gwak  Kevin Chen  and Silvio Savarese. 3d-r2n2: A

uniﬁed approach for single and multi-view 3d object reconstruction. CoRR  abs/1604.00449  2016.

[7] Carlos Esteves  Yinshuang Xu  Christine Allen-Blanchette  and Kostas Daniilidis. Equivariant multi-view

networks. arXiv preprint arXiv:1904.00993  2019.

[8] Cass Everitt. Interactive order-independent transparency. White paper  nVIDIA  2(6):7  2001.

[9] Haoqiang Fan  Hao Su  and Leonidas J Guibas. A point set generation network for 3d object reconstruction
from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 605–613  2017.

[10] Yao Feng  Fan Wu  Xiaohu Shao  Yanfeng Wang  and Xi Zhou. Joint 3d face reconstruction and dense
alignment with position map regression network. In Proceedings of the European Conference on Computer
Vision (ECCV)  pages 534–551  2018.

[11] Thibault Groueix  Matthew Fisher  Vladimir G. Kim  Bryan C. Russell  and Mathieu Aubry. Atlasnet: A

papier-mâché approach to learning 3d surface generation. CoRR  abs/1802.05384  2018.

[12] Xianfeng Gu  Steven J Gortler  and Hugues Hoppe. Geometry images. In ACM Transactions on Graphics

(TOG)  volume 21  pages 355–361. ACM  2002.

[13] JunYoung Gwak  Christopher B Choy  Manmohan Chandraker  Animesh Garg  and Silvio Savarese. Weakly
supervised 3d reconstruction with adversarial constraint. In 2017 International Conference on 3D Vision
(3DV)  pages 263–272. IEEE  2017.

[14] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university

press  2003.

[15] Paul Henderson and Vittorio Ferrari. Learning single-image 3d reconstruction by generative modelling of

shape  pose and shading. arXiv preprint arXiv:1901.06447  2019.

[16] Po-Han Huang  Kevin Matzen  Johannes Kopf  Narendra Ahuja  and Jia-Bin Huang. Deepmvs: Learning
multi-view stereopsis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 2821–2830  2018.

[17] Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised learning of shape and pose with differentiable

point clouds. In Advances in Neural Information Processing Systems  pages 2807–2817  2018.

[18] Mengqi Ji  Juergen Gall  Haitian Zheng  Yebin Liu  and Lu Fang. Surfacenet: An end-to-end 3d neural
network for multiview stereopsis. In Proceedings of the IEEE International Conference on Computer
Vision  pages 2307–2315  2017.

[19] Angjoo Kanazawa  Shubham Tulsiani  Alexei A Efros  and Jitendra Malik. Learning category-speciﬁc
mesh reconstruction from image collections. In Proceedings of the European Conference on Computer
Vision (ECCV)  pages 371–386  2018.

[20] Abhishek Kar  Christian Häne  and Jitendra Malik. Learning a multi-view stereo machine. In Advances in

neural information processing systems  pages 365–376  2017.

[21] Abhishek Kar  Shubham Tulsiani  Joao Carreira  and Jitendra Malik. Category-speciﬁc object recon-
struction from a single image. In Proceedings of the IEEE conference on computer vision and pattern
recognition  pages 1966–1974  2015.

[22] Akash M Kushal  Gaurav Chanda  Kanishka Srivastava  Mohit Gupta  Subhajit Sanyal  TVN Sriram  Prem
Kalra  and Subhashis Banerjee. Multilevel modelling and rendering of architectural scenes. In Proc.
EuroGraphics  2003.

10

[23] Chen-Hsuan Lin  Chen Kong  and Simon Lucey. Learning efﬁcient point cloud generation for dense 3d

object reconstruction. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2018.

[24] Tsung-Yi Lin  Michael Maire  Serge Belongie  James Hays  Pietro Perona  Deva Ramanan  Piotr Dollár 
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on
computer vision  pages 740–755. Springer  2014.

[25] Haggai Maron  Heli Ben-Hamu  Nadav Shamir  and Yaron Lipman. Invariant and equivariant graph

networks. arXiv preprint arXiv:1812.09902  2018.

[26] Jeong Joon Park  Peter Florence  Julian Straub  Richard Newcombe  and Steven Lovegrove. Deepsdf:
Learning continuous signed distance functions for shape representation. arXiv preprint arXiv:1901.05103 
2019.

[27] Alex Pentland. Shape information from shading: a theory about human perception. In [1988 Proceedings]

Second International Conference on Computer Vision  pages 404–413. IEEE  1988.

[28] Charles Ruizhongtai Qi  Li Yi  Hao Su  and Leonidas J Guibas. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space. In Advances in Neural Information Processing Systems 30  pages
5099–5108  2017.

[29] Siamak Ravanbakhsh  Jeff Schneider  and Barnabas Poczos. Deep learning with sets and point clouds.

arXiv preprint arXiv:1611.04500  2016.

[30] Siamak Ravanbakhsh  Jeff Schneider  and Barnabas Poczos. Equivariance through parameter-sharing. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages 2892–2901.
JMLR. org  2017.

[31] Stephan R. Richter and Stefan Roth. Matryoshka networks: Predicting 3d geometry via nested shape
layers. In 2018 IEEE Conference on Computer Vision and Pattern Recognition  CVPR 2018  Salt Lake
City  UT  USA  June 18-22  2018  pages 1936–1944  2018.

[32] Radu Bogdan Rusu and Steve Cousins. 3D is here: Point Cloud Library (PCL). In IEEE International

Conference on Robotics and Automation (ICRA)  Shanghai  China  May 9-13 2011.

[33] Jonathan Shade  Steven Gortler  Li-wei He  and Richard Szeliski. Layered depth images. 1998.

[34] Daeyun Shin  Charless C Fowlkes  and Derek Hoiem. Pixels  voxels  and views: A study of shape
representations for single view 3d object shape prediction. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 3061–3069  2018.

[35] Hang Su  Subhransu Maji  Evangelos Kalogerakis  and Erik Learned-Miller. Multi-view convolutional
neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer
vision  pages 945–953  2015.

[36] Shubham Tulsiani  Alexei A Efros  and Jitendra Malik. Multi-view consistency as supervisory signal
for learning shape and pose prediction. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 2897–2905  2018.

[37] Julien Valentin  Matthias Nießner  Jamie Shotton  Andrew Fitzgibbon  Shahram Izadi  and Philip HS Torr.
Exploiting uncertainty in regression forests for accurate camera relocalization. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 4400–4408  2015.

[38] Sara Vicente  Joao Carreira  Lourdes Agapito  and Jorge Batista. Reconstructing pascal voc. In Proceedings

of the IEEE conference on computer vision and pattern recognition  pages 41–48  2014.

[39] He Wang  Srinath Sridhar  Jingwei Huang  Julien Valentin  Shuran Song  and Leonidas J Guibas. Nor-
malized object coordinate space for category-level 6d object pose and size estimation. arXiv preprint
arXiv:1901.02970  2019.

[40] Nanyang Wang  Yinda Zhang  Zhuwen Li  Yanwei Fu  Wei Liu  and Yu-Gang Jiang. Pixel2mesh:
Generating 3d mesh models from single rgb images. In Proceedings of the European Conference on
Computer Vision (ECCV)  pages 52–67  2018.

[41] Olivia Wiles and Andrew Zisserman. Silnet: Single-and multi-view reconstruction by learning from

silhouettes. arXiv preprint arXiv:1711.07888  2017.

[42] Jiajun Wu  Yifan Wang  Tianfan Xue  Xingyuan Sun  Bill Freeman  and Josh Tenenbaum. Marrnet: 3d
shape reconstruction via 2.5 d sketches. In Advances in neural information processing systems  pages
540–550  2017.

11

[43] Manzil Zaheer  Satwik Kottur  Siamak Ravanbakhsh  Barnabas Poczos  Ruslan R Salakhutdinov  and
Alexander J Smola. Deep sets. In Advances in neural information processing systems  pages 3391–3401 
2017.

[44] Xiuming Zhang  Zhoutong Zhang  Chengkai Zhang  Josh Tenenbaum  Bill Freeman  and Jiajun Wu.
In Advances in Neural Information Processing

Learning to reconstruct shapes from unseen classes.
Systems  pages 2263–2274  2018.

[45] Rui Zhu  Chaoyang Wang  Chen-Hsuan Lin  Ziyan Wang  and Simon Lucey. Object-centric photometric
bundle adjustment with deep shape prior. In 2018 IEEE Winter Conference on Applications of Computer
Vision (WACV)  pages 894–902. IEEE  2018.

12

,Mahdi Milani Fard
Yuri Grinberg
Amir-massoud Farahmand
Joelle Pineau
Doina Precup
Srinath Sridhar
Davis Rempe
Julien Valentin
Leonidas Guibas