2019,Limitations of Lazy Training of Two-layers Neural Network,We study the supervised learning problem under either of the following two models:
(1) Feature vectors x_i are d-dimensional Gaussian and responses are y_i = f_*(x_i) for f_* an unknown quadratic function;
(2) Feature vectors x_i are distributed as a mixture of two d-dimensional centered Gaussians  and y_i's are the corresponding class labels. 
We use two-layers neural networks with quadratic activations  and compare three  different learning regimes: the random features (RF) regime in which we only train the second-layer weights; the neural tangent (NT) regime in which we train a linearization of the neural network around its initialization; the fully trained neural network (NN) regime in  which we train all the weights in the network. We prove that  even for the simple quadratic model of point (1)  there is a potentially unbounded gap between the prediction risk achieved in these three training regimes  when the number of neurons is smaller than the ambient dimension. When the number of neurons is larger than the number of dimensions  the problem is significantly easier and both NT and NN learning achieve zero risk.,Limitations of Lazy Training of
Two-layers Neural Networks

Behrooz Ghorbani

Department of Electrical Engineering

Stanford University

Song Mei

ICME

Stanford University

ghorbani@stanford.edu

songmei@stanford.edu

Theodor Misiakiewicz
Department of Statistics

Stanford University

misiakie@stanford.edu

Andrea Montanari

Department of Electrical Engineering

and Department of Statistics

Stanford University

montanar@stanford.edu

Abstract

We study the supervised learning problem under either of the following two models:
(1) Feature vectors xi are d-dimensional Gaussians and responses are yi = f∗(xi)

for f∗ an unknown quadratic function;

(2) Feature vectors xi are distributed as a mixture of two d-dimensional centered

Gaussians  and yi’s are the corresponding class labels.

We use two-layers neural networks with quadratic activations  and compare three
different learning regimes: the random features (RF) regime in which we only
train the second-layer weights; the neural tangent (NT) regime in which we train
a linearization of the neural network around its initialization; the fully trained
neural network (NN) regime in which we train all the weights in the network. We
prove that  even for the simple quadratic model of point (1)  there is a potentially
unbounded gap between the prediction risk achieved in these three training regimes 
when the number of neurons is smaller than the ambient dimension. When the num-
ber of neurons is larger than the number of dimensions  the problem is signiﬁcantly
easier and both NT and NN learning achieve zero risk.

Introduction

1
Consider the supervised learning problem in which we are given i.i.d. data {(xi  yi)}i≤n  where
xi ∼ P a probability distribution over Rd  and yi = f∗(xi). 1 We would like to learn the unknown
function f∗ as to minimize the prediction risk E{(f (x) − f∗(x))2}. We will assume throughout
f∗ ∈ L2(Rd  P)  i.e. E{f∗(x)2} < ∞.
The function class of two-layers neural networks (with N neurons) is deﬁned by:

f (x) = c +

FNN N =

aiσ((cid:104)wi  x(cid:105)) : c  ai ∈ R  wi ∈ Rd  i ∈ [N ]

(1)
Classical universal approximation results [9] imply that any f∗ ∈ L2(Rd  P) can be approximated
arbitrarily well by an element in FNN = ∪NFNN N (under mild conditions). At the same time 
1For simplicity  we focus our introductory discussion on the case in which the response yi is a noiseless

i=1

function of the feature vector xi: some of our results go beyond this setting.

(cid:110)

N(cid:88)

(cid:111)

.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

fN (x) =

aiσ((cid:104)wi  x(cid:105)) : ai ∈ R  i ∈ [N ]

(cid:111)

 

(cid:110)
(cid:110)

FRF N (W ) =

FNT N (W ) =

N(cid:88)

i=1

N(cid:88)

we know that such an approximation can be constructed in polynomial time only for a subset of
functions f∗. Namely  there exist sets of functions f∗ for which no algorithm can construct a good
approximation in FNN N in polynomial time [19  24]  even having access to the full distribution P
(under certain complexity-theoretic assumptions).
These facts lead to the following central question in neural network theory:

For which subset of function Ftract ⊆ L2(Rd  P) can a neural network approxima-
tion be learnt efﬁciently?

Here ‘efﬁciently’ can be formalized in multiple ways: in this paper we will focus on learning via
stochastic gradient descent.
Signiﬁcant amount of work has been devoted to two subclasses of FNN N which we will refer to as
the random feature model (RF) [22]  and the neural tangent model (NT) [18]:

(cid:111)

.

(2)

(3)

fN (x) = c +

σ(cid:48)((cid:104)wi  x(cid:105))(cid:104)ai  x(cid:105) : c ∈ R  ai ∈ Rd  i ∈ [N ]

i=1

Here W = (w1  . . .   wN ) ∈ Rd×N are weights which are not optimized and instead drawn at
random. Through this paper  we will assume (wi)i≤N ∼iid N(0  Γ). 2
We can think of RF and NT as tractable inner bounds of the class of neural networks NN:

• Tractable. Both FRF N (W )  FNT N (W ) are ﬁnite-dimensional linear spaces  and minimiz-

ing the empirical risk over these classes can be performed efﬁciently.

• Inner bounds. Indeed FRF N (W ) ⊆ FNN N : the random feature model is simply obtained
by ﬁxing all the ﬁrst layer weights. Further FNT(W ) ⊆ cl(FNN 2N ) (the closure of the
class of neural networks with 2N neurons). This follows from ε−1[σ((cid:104)wi + εai  x(cid:105)) −
σ((cid:104)wi  x(cid:105))] = (cid:104)ai  x(cid:105)σ(cid:48)((cid:104)wi  x(cid:105)) + o(1) as ε → 0.

It is possible to show that the class of neural networks NN is signiﬁcantly more expressive than the
two linearization RF  NT  see e.g. [26  15]. In particular  [15] shows that  if the feature vectors
xi are uniformly random over the d-dimensional sphere  and N  d are large with N = O(d)  then
FRF N (W ) can only capture linear functions  while FNT N (W ) can only capture quadratic functions.
Despite these ﬁndings  it could still be that the subset of functions Ftract ⊆ L2(Rd  P) for which we
can learn efﬁciently a neural network approximation is well described by RF and NT. Indeed  several
recent papers show that –in a certain highly overparametrized regime– this description is accurate
[12  11  20]. A speciﬁc counterexample is given in [26]: if the function to be learnt is a single neuron
f∗(x) = σ((cid:104)w∗  x(cid:105)) then gradient descent (in the space of neural networks with N = 1 neurons)
efﬁciently learns it [21]; on the other hand  RF or NT require a number of neurons exponential in the
dimension to achieve vanishing risk.

1.1 Summary of Main Results

In this paper we explore systematically the gap between RF  NT and NN  by considering two speciﬁc
data distributions:

(qf) Quadratic functions: feature vectors are distributed according to xi ∼ N(0  Id) and re-

sponses are quadratic functions yi = f∗(xi) ≡ b0 + (cid:104)xi  Bxi(cid:105) with B (cid:23) 0.

(mg) Mixture of Gaussians: yi = ±1 with equal probability 1/2  and xi|yi = +1 ∼ N(0  Σ(1)) 

xi|yi = −1 ∼ N(0  Σ(2)).

2Notice that we do not add an offset in the RF model  and will limit ourselves to target functions f∗ that are

centered: this choice simpliﬁes some calculations without modifying the results.

2

Figure 1: Left frame: Prediction (test) error of a two-layer neural networks in ﬁtting a quadratic
function in d = 450 dimensions  as a function of the number of neurons N. We consider the large
sample (population) limit n → ∞ and compare three training regimes: random features (RF)  neural
tangent (NT)  and fully trained neural networks (NN). Lines are analytical predictions obtained in
this paper  and dots are empirical results. Right frame: Evolution of the risk for NT and NN with the
number of samples. Dashed lines are our analytic prediction for the large n limit.

Let us emphasize that the choice of quadratic functions in model qf is not arbitrary: in a sense  it is
the most favorable case for NT training. Indeed [15] proves that3 (when N = O(d)): (i) Third- and
higher-order polynomials cannot be approximated nontrivially by FNT N (W ); (ii) Linear functions
are already well approximated within FRF N (W ).
For clarity  we will ﬁrst summarize our result for the model qf  and then discuss generalizations to
mg. The prediction risk achieved within any of the regimes RF  NT  NN is deﬁned by

E(cid:8)(f∗(x) − ˆf (x))2(cid:9)   M ∈ {RF  NT  NN} .

(4)

RM N (f∗) =

RNN N (f∗; (cid:96)  ε) = E(cid:8)(f∗(x) − ˆfSGD(x; (cid:96)  ε))2(cid:9)  

min

ˆf∈FM N (W )

(5)
where ˆfSGD(· ; (cid:96)  ε) is the neural network produced by (cid:96) steps of stochastic gradient descent (SGD)
where each sample is used once  and the stepsize is set to ε (see Section 2.3 for a complete deﬁnition).
Notice that the quantities RM N (f∗)  RNN N (f∗; (cid:96)  ε) are random variables because of the random
weights W   and the additional randomness in SGD.
Our results are summarized by Figure 1  which compares the risk achieved by the three approaches
above in the population limit n → ∞  using quadratic activations σ(u) = u2 + c0. We consider the
large-network  high-dimensional regime N  d → ∞  with N/d → ρ ∈ (0 ∞). Figure 1 reports the
risk achieved by various approaches in numerical simulations  and compares them with our theoretical
predictions for each of three regimes RF  NT  and NN  which are detailed in the next sections.
The agreement between analytical predictions and simulations is excellent but  more importantly  a
clear picture emerges. We can highlight a few phenomena that are illustrated in this ﬁgure:
Random features do not capture quadratic functions. The random features risk RRF N (f∗) remains
generally bounded away from zero for all values of ρ = N/d. It is further highly dependent on the
distribution of the weight vectors wi ∼ N(0  Γ). Section 2.1 characterizes explicitly this dependence 
for general activation functions σ. For large ρ = N/d  the optimal distribution of the weight vectors
uses covariance Γ∗ ∝ B  but even in this case the risk is bounded away from zero unless ρ → ∞.
The neural tangent model achieves vanishing risk on quadratic functions for N > d. However  the
risk is bounded away from zero if N/d → ρ ∈ (0  1). Section 2.1 provides explicit expressions
for the minimum risk as a function of ρ. Roughly speaking NT ﬁts the quadratic function f∗ along
random subspace determined by the random weight vectors wi. For N ≥ d  these vectors span the
3Note that [15] considers feature vectors xi uniformly random over the sphere rather than Gaussian. However 
the results of [15] can be generalized  with certain modiﬁcations  to the Gaussian case. Roughly speaking 
for Gaussian features  NT with N = O(d) neurons can represent quadratic functions  and a low-dimensional
subspace of higher order polynomials.

3

102103Number of Hidden Units  N0.00.20.40.60.81.0R=R0NNNT(I)RF(I)RF(¡¤)N=d010000200003000040000n=d0.00.20.40.60.81.0R=R0NNlimn!1 NNNT(I)limn!1 NT(I)whole space Rd and hence the limiting risk vanishes. For N < d only a fraction of the space is
spanned  and not the most important one (i.e. not the principal eigendirections of B).

Fully trained neural networks achieve vanishing risk on quadratic functions for N > d: this is to be
expected on the basis of the previous point. For N/d → ρ ∈ (0  1) the risk is generally bounded away
from 0  but its value is smaller than for the neural tangent model. Namely  in Section 2.3 we give an
explicit expression for the asymptotic risk (holding for B (cid:23) 0) implying that  for some GAP(ρ) > 0
(independent of N  d) 

E{(f (x) − f∗(x))2} ≤ RNT N (f∗) − GAP(ρ) .

(6)

t→∞ lim
lim
ε→0

RNN N (f∗; (cid:96) = t/ε  ε) = inf

f∈FNN N

We prove this result by showing convergence of SGD to gradient ﬂow in the population risk  and
then proving a strict saddle property for the population risk. As a consequence the limiting risk
on the left-hand side coincides with the minimum risk over the whole space of neural networks
E{(f (x) − f∗(x))2}. We characterize the latter and shows that it amounts to ﬁtting f∗
inf f∈FNN N
along the N principal eigendirections of B. This mechanism is very different from the one arising in
the NT regime.

The picture emerging from these ﬁndings is remarkably simple. The fully trained network learns the
most important eigendirections of the quadratic function f∗(x) and ﬁts them  hence surpassing the
NT model which is conﬁned to a random set of directions.
Let us emphasize that the above separation between NT and NN is established only for N ≤ d. It
is natural to wonder whether this separation generalizes to N > d for more complicated classes of
functions  or if instead it always vanishes for wide networks. We expect the separation to generalize
to N > d by considering higher order polynomial  instead of quadratic functions. Partial evidence
in this direction is provided by [15]: for third- or higher-order polynomials NT does not achieve
vanishing risk at any ρ ∈ (0 ∞). The mechanism unveiled by our analysis of quadratic functions
is potentially more general: neural networks are superior to linearized models such as RF or NT 
because they can learn a good representation of the data.
Our results for quadratic functions are formally presented in Section 2. In order to conﬁrm that the
picture we obtain is general  we establish similar results for mixture of Gaussians in Section 3. More
precisely  our results of RF and NT for mixture of Gaussians are very similar to the quadratic case.
In this model  however  we do not prove a convergence result for NN analogous to (6)  although we
believe it should be possible by the same approach outlined above. On the other hand  we characterize
E{(y − f (x))2} and prove it is strictly
the minimum prediction risk over neural networks inf f∈FNN N
smaller than the minimum achieved by RF and NT. Finally  Section 4 contains background on our
numerical experiments.

1.2 Further Related Work

The connection (and differences) between two-layers neural networks and random features models
has been the object of several papers since the original work of Rahimi and Recht [22]. An incomplete
list of references includes [5  2  6  7  23]. Our analysis contributes to this line of work by establishing
a sharp asymptotic characterization  although in more speciﬁc data distributions. Sharp results have
recently been proven in [15]  for the special case of random weights wi uniformly distributed over
a d-dimensional sphere. Here we consider the more general case of anisotropic random features
with covariance Γ (cid:54)∝ I. This clariﬁes a key reason for suboptimality of random features: the data
representation is not adapted to the target function f∗. We focus on the population limit n → ∞.
Complementary results characterizing the variance as a function of n are given in [17].
The NT model (3) is much more recent [18]. Several papers show that SGD optimization within the
original neural network is well approximated by optimization within the model NT as long as the
number of neurons is large compared to a polynomial in the sample size N (cid:29) nc0 [12  11  3  28].
Empirical evidence in the same direction was presented in [20  4].
Chizat and Bach [8] clariﬁed that any nonlinear statistical model can be approximated by a linear one
in an early (lazy) training regime. The basic argument is quite simple. Given a model x (cid:55)→ f (x; θ)
with parameters θ  we can Taylor-expand around a random initialization θ0. Setting θ = θ0 + β  we
get

f (x; θ) ≈ f (x; θ0) + βT∇θf (x; θ0) ≈ βT∇θf (x; θ0) .

(7)

4

Here the second approximation holds since  for many random initializations  f (x; θ0) ≈ 0 because
of random cancellations. The resulting model βT∇θf (x; θ0) is linear  with random features.
Our objective is complementary to this literature: we prove that RF and NT have limited approxima-
tion power  and signiﬁcant gain can be achieved by full training.
Finally  our analysis of fully trained networks connects to the ample literature on non-convex statistical
estimation. For two layers neural networks with quadratic activations  Soltanolkotabi  Javanmard and
Lee [25] showed that  as long as the number of neurons satisﬁes N ≥ 2d there are no spurious local
minimizers. Du and Lee [10] showed that the same holds as long as N ≥ d ∧ √
2n where n is the
sample size. Zhong et. al. [27] established local convexity properties around global optima. Further
related landscape results include [14  16  13].

2 Main Results: Quadratic Functions
As mentioned in the previous section  our results for quadratic functions (qf) assume xi ∼ N(0  Id)
and yi = f∗(xi) where

f∗(x) ≡ b0 + (cid:104)x  Bx(cid:105) .

(8)

2.1 Random Features
We consider random feature model with ﬁrst-layer weights (wi)i≤N ∼ N(0  Γ). We make the
following assumptions:

A2. We ﬁx the weights’ normalization by requiring E{(cid:107)wi(cid:107)2

A1. The activation function σ veriﬁes σ(u)2 ≤ c0 exp(c1u2/2) for some constants c0  c1 with
c1 < 1. Further it is nonlinear (i.e. there is no a0  a1 ∈ R such that σ(u) = a0 + a1 u
almost everywhere).
2} = Tr(Γ) = 1. We assume
the operator norm (cid:107)d · Γ(cid:107)op ≤ C for some constant C  and that the empirical spectral
distribution of d · Γ converges weakly  as d → ∞ to a probability distribution D over R≥0.
Theorem 1. Let f∗ be a quadratic function as per Eq. (8)  with E(f∗) = 0. Assume conditions A1
and A2 to hold. Denote by λk = EG∼N(0 1)[σ(G)Hek(G)] the k-th Hermite coefﬁcient of σ and
assume λ0 = 0. Deﬁne ˜λ = EG∼N(0 1)[σ(G)2] − λ2

1. Let ψ > 0 be the unique solution of
λ2
1t
1 + λ2
1tψ
Then  the following holds as N  d → ∞ with N/d → ρ:

−˜λ = − ρ
ψ

D(dt) .

(cid:90)

(9)

+

(cid:32)

(cid:107)B(cid:107)2

F

2d(cid:107)Γ(cid:107)2

F

2d(cid:104)Γ  B(cid:105)2

ψλ2

(cid:0)2 + ψλ2
(cid:18)

(cid:33)
(cid:1) + od P(1)
(cid:19)

RRF N (f∗) = (cid:107)f∗(cid:107)2

L2

1 −

.

(10)

Moreover  assuming (cid:104)Γ  B(cid:105)2/(cid:107)Γ(cid:107)2
ρ → ∞:

F(cid:107)B(cid:107)2

F to have a limit as d → ∞  (10) simpliﬁes as follows for

lim
ρ→∞

lim

d→∞ N/d→ρ

RRF N (f∗)
(cid:107)f∗(cid:107)2

L2

= lim
d→∞

1 − (cid:104)Γ  B(cid:105)2
F(cid:107)B(cid:107)2

(cid:107)Γ(cid:107)2

F

.

(11)

L2

Notice that RRF N (f∗)/(cid:107)f∗(cid:107)2
is the RF risk normalized by the risk of the trivial predictor f (x) = 0.
The asymptotic result in (11) is remarkably simple. By Cauchy-Schwartz  the normalized risk is
bounded away from zero even as the number of neurons per dimension diverges ρ = N/d → ∞ 
unless Γ ∝ B  i.e. the random features are perfectly aligned with the function to be learned. For
isotropic random features  the right-hand side of Eq. (11) reduces to 1 − Tr(B)2/(d(cid:107)B(cid:107)2
particular  RF performs very poorly when Tr(B) (cid:28) √
F ). In
d(cid:107)B(cid:107)F   and no better than the trivial predictor

f (x) = 0 if Tr(B) = 0.
Notice that the above result applies to quite general activation functions. The formulas simplify
signiﬁcantly for quadratic activations.

5

Corollary 1. Under the assumptions of Theorem 1  further assume σ(x) = x2 − 1. Then we have 
as N  d → ∞ with N/d → ρ:

RRF N (f∗) = (cid:107)f∗(cid:107)2

L2

1 −

(cid:32)

ρd(cid:104)B  Γ(cid:105)2

(cid:0)1 + ρd(cid:107)Γ(cid:107)2

F

(cid:107)B(cid:107)2

F

(cid:33)
(cid:1) + od P(1)

.

(12)

The right-hand side of Eq. (12) is plotted in Fig. 1 for isotropic features Γ = I/d  and for optimal
features Γ = Γ∗ ∝ B.

(cid:16)

(cid:110)

E[RNT N (f∗)] = (cid:107)f∗(cid:107)2

2.2 Neural Tangent
For the NT regime  we focus on quadratic activations and isotropic weights wi ∼ N(0  Id/d).
Theorem 2. Let f∗ be a quadratic function as per Eq. (8)  with E(f∗) = 0  and assume σ(x) = x2.
Then  we have for N  d → ∞ with N/d → ρ
(1 − ρ)2

1 − Tr(B)2
d(cid:107)B(cid:107)2
where the expectation is taken over wi ∼i.i.d N(0  Id/d).
As for the case of random features  the NT risk depends on the target function f∗(x) only through
the ratio Tr(B)2/(d(cid:107)B(cid:107)2
F ). However  the normalized risk is always smaller than the baseline
RNT N (f∗) = (cid:107)f∗(cid:107)2
L2 + od(1) 
with this worst case achieved when B ∝ I. In particular  E[RNT N (f∗)] vanishes asymptotically for
ρ ≥ 1. This comes at the price of a larger number of parameters to be ﬁtted  namely N d instead of
N.

L2. Note that  by Cauchy-Schwartz  E[RNT N (f∗)] ≤ (1 − ρ)+(cid:107)f∗(cid:107)2

+ (1 − ρ)+

Tr(B)2
d(cid:107)B(cid:107)2

+ od(1)

(cid:111)

(cid:17)

L2

+

F

F

.

2.3 Neural Network

For the analysis of SGD-trained neural networks  we assume f∗ to be a quadratic function as per
Eq. (8)  but we will now restrict to the positive semideﬁnite case B (cid:23) 0. We consider quadratic
activations σ(x) = x2  and we ﬁx the second layers weights to be 1:

N(cid:88)

ˆf (x; W   c) =

(cid:104)wi  x(cid:105)2 + c.

i=1

Notice that we use an explicit offset to account for the mismatch in means between f∗ and ˆf. It is
useful to introduce the population risk  as a function of the network parameters W   c:

L(W   c) = E[(f∗(x) − ˆf (x; W   c))2] = E(cid:104)(cid:16)(cid:104)xxT  B − W W T(cid:105) + b0 − c
(cid:17)2

Here expectation is with respect to x ∼ N(0  Id). We will study a one-pass version of SGD  whereby
at each iteration k we perform a stochastic gradient step with respect to a fresh sample (xk  f∗(xk))

(cid:17)2(cid:105)

(cid:16)

.

(W k+1  ck+1) = (W k  ck) − ε∇W  c

f∗(xk) − ˆf (xk; W   c)

 

and deﬁne

RNN N (f∗; (cid:96)  ε) ≡ L(W (cid:96)  c(cid:96)) = Ex∼N(0 Id)[(f∗(x) − ˆf (x; W (cid:96)  c(cid:96)))2].

Notice that this is the risk with respect to a new sample  independent from the ones used to train
W (cid:96)  c(cid:96). It is the test error. Also notice that (cid:96) is the number of SGD steps but also (because of the
one-pass assumption) the sample size. Our next theorem characterizes the asymptotic risk achieved
by SGD. This prediction is reported in Figure 1.
Theorem 3. Let f∗ be a quadratic function as per Eq. (8)  with B (cid:23) 0. Consider SGD with
initialization (W 0  c0) whose distribution is absolutely continuous with respect to the Lebesgue
measure. Let RNN N (f∗; (cid:96)  ε) be the test prediction error after (cid:96) SGD steps with step size ε.
Then we have (probability is over the initialization (W 0  c0) and the samples)

P(cid:16)(cid:12)(cid:12)(cid:12)RNN N (f∗; (cid:96) = t/ε  ε) − inf

(cid:12)(cid:12)(cid:12) ≥ δ) = 0 

lim
t→∞ lim
ε→0
where λ1(B) ≥ λ2(B) ≥ ··· ≥ λd(B) are the ordered eigenvalues of B.

L(W   c)

inf
W  c

W  c

d(cid:88)

i=N +1

L(W   c) = 2

λi(B)2 

6

Figure 2: Left frame: Prediction (test) error of a two-layer neural networks in ﬁtting a mixture of
Gaussians in d = 450 dimensions  as a function of the number of neurons N  within the three regimes
RF  NT  NN. Lines are analytical predictions obtained in this paper  and dots are empirical results
(both in the population limit). Dotted line is the Bayes error. Right frame: Evolution of the risk for
NT and NN with the number of samples.

The proof of this theorem depends on the following proposition concerning the landscape of the
population risk  which is of independent interest.
Proposition 1. Let f∗ be a quadratic function as per Eq. (8)  with B (cid:23) 0. For any sub-level set of
the risk function Ω(B0) = {x = (W   c) : L(W   c) ≤ B0}  there exists constants ε  δ > 0 such that
L is (ε  δ)-strict saddle in the region Ω(B0). Namely  for any x ∈ Ω(B0) with (cid:107)∇L(x)(cid:107)2 ≤ ε  we
have λmin(∇2L(x)) < −δ.
We can now compare the risk achieved within the regimes RF  NT and NN. Gathering the results of
Corollary 1  and Theorems 2  3 (using wi ∼ N(0  I/d) for RF and NT)  we obtain



1 − ρ
1 + ρ
(1 − ρ)2

1 −

+ + ρ(1 − ρ)+
(cid:80)d∧N

i=1 λi(B)2
(cid:107)B(cid:107)2

F

RM N (f∗)
(cid:107)f∗(cid:107)2

L2

≈

Tr(B)2
d(cid:107)B(cid:107)2

F

for M = RF 

Tr(B)2
d(cid:107)B(cid:107)2

F

for M = NT 

(13)

for M = NN.

As anticipated  NN learns the most important directions in f∗  while RF  NT do not.

3 Main Results: Mixture of Gaussians
In this section  we consider the mixture of Gaussian setting (mg): yi = ±1 with equal probability
1/2  and xi|yi = +1 ∼ N(0  Σ(1))  xi|yi = −1 ∼ N(0  Σ(2)). We parametrize the covariances as
Σ(1) = Σ − ∆ and Σ(2) = Σ + ∆  and will make the following assumptions:

M1. There exists constants 0 < c1 < c2 such that c1Id (cid:22) Σ (cid:22) c2Id;
√
M2. (cid:107)∆(cid:107)op = Θd(1/

d).
√
The scaling in assumption M2 ensures the signal-to-noise ratio to be of order one. If the eigenvalues
of ∆ are much larger than 1/
d  then it is easy to distinguish the two classes with high probability
(they are asymptotically mutually singular). If (cid:107)∆(cid:107)op = od(1/
d) then no non-trivial classiﬁer
exists.
We will denote by PΣ ∆ the joint distribution of (y  x) under the (mg) model  and by EΣ ∆ or E(y x)
the corresponding expectation. The minimum prediction risk within any of the regimes RF  NT  NN
is deﬁned by

√

RM N (P) = inf

f∈FM N

E(y x){(y − f (x))2}   M ∈ {RF  NT  NN} .

7

102103Number of Hidden Units  N0.30.40.50.60.70.80.91.0R=R0NNNT(I)RF(I)MMSEN=d05000100001500020000250003000035000n=d0.50.60.70.80.91.0R=R0NNlimn!1 NNNT(I)limn!1 NT(I)As mentioned in the introduction  the picture emerging from our analysis of the mg model is aligned
with the results obtained in the previous section. We will limit ourselves to stating the results without
repeating comments that were made above. Our results are compared with simulations in Figure
2. Notice that  in this case  the Bayes error (MMSE) is not achieved even for very wide networks
N/d (cid:29) 1 either by NT or NN.

3.1 Random Features
As in the previous section  we generate random ﬁrst-layer weights (wi)i≤N ∼ N(0  Γ). We consider
a general activation function satisfying condition A1. We make the following assumption on Γ  Σ:
B2. We ﬁx the weights’ normalization by requiring E{(cid:104)wi  Σwi(cid:105)} = Tr(ΓΣ) = 1. We assume
that there exists a constant C such that (cid:107)d · Γ(cid:107)op ≤ C  and that the empirical spectral
distribution of d · (Γ1/2ΣΓ1/2) converges weakly  as d → ∞ to a probability distribution
D over R≥0.

Theorem 4. Consider the mg distribution  with Σ and ∆ satisfying condition M1 and M2. Assume
conditions A1 and B2 to hold. Deﬁne λk = EG∼N(0 1)[σ(G)Hek(G)] to be the k-th Hermite
coefﬁcient of σ and assume without loss of generality λ0 = 0. Deﬁne ˜λ = E[σ(G)2] − λ2
1. Let ψ > 0
be the unique solution of

(14)
Deﬁne ζ1(d) ≡ d Tr(ΣΓΣΓ)/2  ζ2(d) ≡ d Tr(∆Γ)2/4. Then  the following holds as N  d → ∞
with N/d → ρ:

+

λ2
1t
1 + λ2
1tψ

D(dt) .

−˜λ = − ρ
ψ

(cid:90)

RRF N (PΣ ∆) =

(15)
Moreover  assume ζ1(d) ζ2(d) to have limits as d → ∞  i.e. we have limd→∞ ζj(d) = ζj ∗ for
j = 1  2. Then the following holds as ρ → ∞:

1 + (ζ1(d) + ζ2(d))λ2

+ od P(1)  .

2ψ

1 + ζ1(d)λ2

2ψ

lim
ρ→∞

lim

d→∞ N/d→ρ

RRF N (PΣ ∆) =

ζ1 ∗

ζ1 ∗ + ζ2 ∗

.

(16)

3.2 Neural Tangent
For the NT model  we ﬁrst state our theorem for general Σ and wi ∼ N(0  Γ) and then give an
explicit concentration result in the case Σ = I and isotropic weights wi ∼ N(0  I/d).
Theorem 5. Let PΣ ∆ be the mixture of Gaussian distribution  with Σ and ∆ satisfying conditions
M1 and M2. Further assume σ(x) = x2. Then  the following holds for almost every W ∈ Rd×N
(with respect to the Lebesgue measure):
RNT N (PΣ ∆) =

+ od(1) 

2

2 + (cid:107) ˜∆(cid:107)2

F − (cid:107)P ⊥ ˜∆P ⊥(cid:107)2

F

where ˜∆ = Σ−1/2∆Σ−1/2 and P ⊥ = I − Σ1/2W (W TΣW )−1W TΣ1/2 is the projection
perpendicular to span(Σ1/2W ).
Assuming further that Σ = I and wi ∼i.i.d. N(0  Id/d)  we have as N  d → ∞ with N/d → ρ:

RNT N (PI ∆) =

2 + κ(ρ  ∆)(cid:107)∆(cid:107)2

F

2

(cid:16)

κ(ρ  ∆) = 1 − (1 − ρ)2

1 − Tr(∆)2
d(cid:107)∆(cid:107)2
In particular  for ρ ≥ 1  we have (for almost every W )
1

+

F

RNT N (PI ∆) =

+ od P(1) 

(cid:17) − (1 − ρ)+

Tr(∆)2
d(cid:107)∆(cid:107)2

F

 

1 + (cid:107)∆(cid:107)2

F /2

+ od P(1).

8

3.3 Neural Network

(cid:80)N
i=1 ai(cid:104)wi  x(cid:105)2 + c. This is optimized over (ai  wi)i≤N and c.

We consider quadratic activations with general offset and coefﬁcients ˆf (x; W   a  c) =

Theorem 6. Let PΣ ∆ be the mixture of Gaussian distribution  with Σ and ∆ satisfying conditions
M1 and M2. Then  the following holds

RNN N (PΣ ∆) =

2 +(cid:80)N∧d

2

i=1 λi( ˜∆)2

+ od(1) 

where ˜∆ = Σ−1/2∆Σ−1/2 and λ1( ˜∆) ≥ λ1( ˜∆) ≥ ··· ≥ λd( ˜∆) are the singular values of ˜∆. In
particular  for ρ ≥ 1  we have

RNN N (PI ∆) =

1

1 + (cid:107) ˜∆(cid:107)2

F /2

+ od(1).

Let us emphasize that  for this setting  we do not have a convergence result for SGD as for the model
qf  cf. Theorem 3. However  because of certain analogies between the two models  we expect a
similar result to hold for mixtures of Gaussians.
We can now compare the risks achieved within the regimes RF  NT and NN. Gathering the results
of Theorems 4  5 and 6 for Σ = I and σ(x) = x2 − 1 (using wi ∼ N(0  I/d) for RF and NT)  we
obtain



RM N (PI ∆) ≈

1

1 + ρ

1+2ρ · tr(∆)2
1 + κ(ρ  ∆)(cid:107)∆(cid:107)2

2d

1

1 +(cid:80)N∧d

1

i=1 λi(∆)2/2

F /2

for M = RF 

for M = NT 

for M = NN.

(17)

We recover a similar behavior as in the case of the (qf) model: NN learns the most important directions
of ∆  while RF  NT do not. Note that the Bayes error is not achieved in this model.

4 Numerical Experiments

For the experiments illustrated in Figures 1 and 2  we use feature size of d = 450  and number of
hidden units N ∈ {45 ···   4500}. NT and NN models are trained with SGD in TensorFlow [1]. We
run a total of 2 × 105 SGD steps for each (qf) model and 1.4 × 105 steps for each (mg) model. The
SGD batch size is ﬁxed at 100 and the step size is chosen from the grid {0.001 ···   0.03} where the
hyper-parameter that achieves the best ﬁt is used for the ﬁgures. RF models are ﬁtted directly by
solving KKT conditions with 5 × 105 observations. After ﬁtting the model  the test error is evaluated
on 104 fresh samples. In our ﬁgures  each RF data point corresponds to the test error averaged over
10 models with independent realizations of W .
For (qf) experiments  we choose B to be diagonal with diagonal elements chosen i.i.d from standard
exponential distribution with parameter 1. For (mg) experiments  ∆ is also diagonal with the
}. Experiments with non-diagonal ∆
diagonal element chosen uniformly from the set { 2√
are presented in the appendix.
While we are only able to provide theory for NN and NT when the activations are quadratic  we have
performed extensive experiments examining the behavior of these models with other nonlinearities.
These results are reported in the appendix. In general  the phenomena we observe in the case of
quadratic activations persist when other activations are used. In particular  the positive gap between
NN and NT is still present when N < d.

  1√
d

  1.5√
d

d

Acknowledgements

This work was partially supported by grants NSF DMS-1613091  CCF-1714305  IIS-1741162  and
ONR N00014-18-1-2729  NSF DMS-1418362  NSF DMS-1407813.

9

References
[1] Martín Abadi  Paul Barham  Jianmin Chen  Zhifeng Chen  Andy Davis  Jeffrey Dean  Matthieu
Devin  Sanjay Ghemawat  Geoffrey Irving  Michael Isard  et al. Tensorﬂow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16)  pages 265–283  2016.

[2] Ahmed El Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with
statistical guarantees. In Advances in Neural Information Processing Systems  pages 775–783 
2015.

[3] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via

over-parameterization. arXiv:1811.03962  2018.

[4] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  Ruslan Salakhutdinov  and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 
2019.

[5] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on

Learning Theory  pages 185–209  2013.

[6] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal

of Machine Learning Research  18(1):629–681  2017.

[7] Francis Bach. On the equivalence between kernel quadrature rules and random feature expan-

sions. The Journal of Machine Learning Research  18(1):714–751  2017.

[8] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable program-

ming. arXiv:1812.07956  2018.

[9] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of

control  signals and systems  2(4):303–314  1989.

[10] Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with

quadratic activation. arXiv:1803.01206  2018.

[11] Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds

global minima of deep neural networks. arXiv:1811.03804  2018.

[12] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes

over-parameterized neural networks. arXiv:1810.02054  2018.

[13] Rong Ge  Chi Jin  and Yi Zheng. No spurious local minima in nonconvex low rank problems:
A uniﬁed geometric analysis. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70  pages 1233–1242. JMLR. org  2017.

[14] Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with

landscape design. arXiv:1711.00501  2017.

[15] Behrooz Ghorbani  Song Mei  Theodor Misiakiewicz  and Andrea Montanari. Linearized

two-layers neural networks in high dimension. arXiv:1904.12191  2019.

[16] Benjamin Haeffele  Eric Young  and Rene Vidal. Structured low-rank matrix factorization:
Optimality  algorithm  and applications to image processing. In International conference on
machine learning  pages 2007–2015  2014.

[17] Trevor Hastie  Andrea Montanari  Saharon Rosset  and Ryan J Tibshirani. Surprises in high-

dimensional ridgeless least squares interpolation. arXiv:1903.08560  2019.

[18] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems  pages
8571–8580  2018.

[19] Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In
Approximation  Randomization  and Combinatorial Optimization. Algorithms and Techniques
(APPROX/RANDOM 2014). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik  2014.

10

[20] Jaehoon Lee  Lechao Xiao  Samuel S Schoenholz  Yasaman Bahri  Jascha Sohl-Dickstein  and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv:1902.06720  2019.

[21] Song Mei  Yu Bai  Andrea Montanari  et al. The landscape of empirical risk for nonconvex

losses. The Annals of Statistics  46(6A):2747–2774  2018.

[22] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2008.

[23] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random

features. In Advances in Neural Information Processing Systems  pages 3215–3225  2017.

[24] Ohad Shamir. Distribution-speciﬁc hardness of learning neural networks. The Journal of

Machine Learning Research  19(1):1135–1163  2018.

[25] Mahdi Soltanolkotabi  Adel Javanmard  and Jason D Lee. Theoretical insights into the op-
timization landscape of over-parameterized shallow neural networks. IEEE Transactions on
Information Theory  65(2):742–769  2019.

[26] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under-

standing neural networks. arXiv:1904.00687  2019.

[27] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery
guarantees for one-hidden-layer neural networks. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70  pages 4140–4149. JMLR. org  2017.

[28] Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent optimizes

over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888  2018.

11

,Behrooz Ghorbani
Song Mei
Theodor Misiakiewicz
Andrea Montanari