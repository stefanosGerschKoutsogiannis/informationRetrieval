2019,Limits of Private Learning with Access to Public Data,We consider learning problems where the training set consists of two types of examples: private and public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. This setting interpolates between private learning (where all examples are private) and classical learning (where all examples are public). 

We study the limits of learning in this setting in terms of private and public sample complexities. We show that any hypothesis class of VC-dimension $d$ can be agnostically learned up to an excess error of $\alpha$ using only (roughly) $d/\alpha$ public examples and $d/\alpha^2$ private labeled examples. This result holds even when the public examples are unlabeled. This gives a quadratic improvement over the standard $d/\alpha^2$ upper bound on the public sample complexity (where private examples can be ignored altogether if the public examples are labeled). Furthermore  we give a nearly matching lower bound  which we prove via a generic reduction from this setting to the one of private learning without public data.,Limits of Private Learning with Access to Public Data

Noga Alon

Department of Mathematics

Princeton University

nalon@math.princeton.edu

Raef Bassily∗

Department of Computer Science & Engineering

The Ohio State University
bassily.1@osu.edu

Shay Moran
Google AI
Princeton

shaymoran1@gmail.com

Abstract

We consider learning problems where the training set consists of two types of
examples: private and public. The goal is to design a learning algorithm that
satisﬁes differential privacy only with respect to the private examples. This setting
interpolates between private learning (where all examples are private) and classical
learning (where all examples are public).
We study the limits of learning in this setting in terms of private and public sam-
ple complexities. We show that any hypothesis class of VC-dimension d can be
agnostically learned up to an excess error of α using only (roughly) d/α public
examples and d/α2 private labeled examples. This result holds even when the
public examples are unlabeled. This gives a quadratic improvement over the stan-
dard d/α2 upper bound on the public sample complexity (where private examples
can be ignored altogether if the public examples are labeled). Furthermore  we give
a nearly matching lower bound  which we prove via a generic reduction from this
setting to the one of private learning without public data.

1

Introduction

In this work  we study a relaxed notion of differentially private (DP) supervised learning which was
introduced by Beimel et al. in [BNS13]  where it was coined semi-private learning. In this setting  the
learning algorithm takes as input a training set that is comprised of two parts: (i) a private sample that
contains personal and sensitive information  and (ii) a “public” sample that poses no privacy concerns.
We assume that the private sample is always labeled  while the public sample can be either labeled or
unlabeled. The algorithm is required to satisfy DP only with respect to the private sample. The goal
is to design algorithms that can exploit as little public data as possible to achieve non-trivial gains in
accuracy (or  equivalently savings in sample complexity) over standard DP learning algorithms  while
still providing strong privacy guarantees for the private dataset. Similar settings have been studied
before in literature (see “Related Work” section below).
There are several motivations for studying this problem. First  in practical scenarios  it is often not
hard to collect reasonable amount of public data from users or organizations. For example  in the
language of consumer privacy  there is considerable amount of data collected from the so-called
“opt-in” users  who voluntarily offer or sell their data to companies or organizations. Such data is
deemed by its original owner to have no threat to personal privacy. There are also a variety of other

∗Part of this work was done while visiting the Simons Institute for the Theory of Computing.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

sources of public data that can be harnessed. Moreover  in many scenarios  it is often much easier to
collect unlabeled than labeled data.
Another motivation emerges from several pessimistic results in DP learning that either limit or
eliminate the possibility of differentially private learning  even for elementary problems such as
one-dimensional thresholds which are trivially learnable without privacy constraints [BNSV15 
ALMM19]. It is therefore natural to explore whether a small amount of public data circumvents these
impossibility results.
A third motivation arises from the following observation: consider a learning problem in which the
marginal distribution DX over the domain X is completely known to the algorithm  but the target
concept c : X → {0  1} is unknown. One can show that in this setting every VC class can be learned
privately with (roughly) the same sample complexity as in the standard  non-private  case. The other
extreme is the standard PAC-setting in which both DX and c are unknown to the algorithm. As
mentioned earlier  in this case even very simple classes such as one-dimensional thresholds can not
be learned privately. In the setting considered in this work  the distribution DX is unknown but the
learner has access to some public examples from it. This naturally interpolates between these two
extremes: the case when DX is unknown that corresponds to having no public examples  and the
case when DX is known that corresponds to having an unbounded amount of public examples. It
is therefore natural to study the intermediate behaviour as the number of public examples grows
from 0 to ∞. The same question can be also asked in the “easier” case where the public examples are
labeled.
We will generally refer to the setting described above as semi-private learning  and to algorithms
in that setting as semi-private learners. (See Section 2  for precise deﬁnitions.) Following previous
works in private learning  we consider two types of semi-private learners: those that satisfy the notion
of pure DP (the stronger notion of DP)  as well as those that satisfy approximate DP. We will call the
former type pure semi-private learners  and call the latter approximate semi-private learners.

Main Results

In this work we concentrate on the sample complexity of semi-private learners in the agnostic setting.
We especially focus on the minimal number of public examples with which it is possible to learn
every VC class.
1. Upper bound: Every hypothesis class H can be learned up to excess error α by a pure semi-
private algorithm whose private sample complexity is (roughly) VC(H)/α2 and public sample
complexity is (roughly) VC(H)/α. Moreover  the input public sample can be unlabeled.
Recall that VC(H)/α2 examples are necessary to learn in the agnostic setting (even without
privacy constraints); therefore  this result establishes a quadratic saving.
2. Lower bound: Assume H has an inﬁnite Littlestone dimension2. Then  any approximate semi-
private learner for H must have public sample complexity Ω(1/α)  where α is the excess error.
This holds even when the public sample is labeled.
One example of a class with an inﬁnite Littlestone dimension is the class of thresholds over R.
This class has VC dimension 1  and therefore demonstrates that the upper and lower bounds above
nearly match.
3. Dichotomy for pure semi-private learning: Every hypothesis class H satisﬁes exactly one of
the following:
(i) H is learnable by a pure DP algorithm  and therefore can be semi-privately learned without
(ii) Any pure semi-private learner for H must have public sample complexity Ω (1/α)  where α

any public examples.

is the excess error.

Techniques

Upper bound: The idea of the construction for the upper bound is to use the (unlabeled) public
data to construct a ﬁnite class H(cid:48) that forms a “good approximation” of the original class H  then

2The Littlestone dimension is a combinatorial parameter that arises in online learning [Lit87  BPS09].

2

reduce the problem to DP learning of a ﬁnite class. Such approximation is captured via the notion of
α-covering (Deﬁnition 2.7). By standard uniform-convergence arguments  it is not hard to see that
(roughly) VC(H)/α2 public examples sufﬁce to construct such an approximation. We show that the
number of public examples can be reduced to only about VC(H)/α  even in the agnostic setting. Our
construction is essentially the same as a construction due to Beimel et al. [BNS13]  but our proof
technique is different (see the “Related Work” section for a more detailed comparison).
Lower bounds: The lower bounds boil down to a public-data-reduction lemma which shows that if
we are given a semi-private learner whose public sample complexity is << 1/α  we can transform
it to a fully private learner (which uses no public examples) whose excess error is a small constant
(say 1/100). Stated contra-positively  this implies that if a class can not be privately learned up to an
excess loss of 1/100 then it can not be semi-privately learned with << 1/α public examples. This
allows us to exploit known lower bounds for private learning to derive a lower bound on the public
sample complexity.
Related Work: Our algorithm for the upper bound is essentially the same as a construction due to
Beimel et al. [BNS13]. Although [BNS13] focuses on the realizable case of semi-private learning 
their analysis can be extended to the agnostic case to yield a similar upper bound to the one we
present here. However  the proof technique we give here is different from theirs. In particular  our
proof relies on and emphasizes the use of α-coverings  which provides a direct argument for both the
realizable and agnostic case. We believe the notion of α-covering can be a useful tool in the analysis
of other differentially private algorithms even outside the learning context.
There are also several other works that considered similar problems. A similar notion known as
“label-private learning” was considered in [CH11] (see also references therein) and in [BNS13]. In
this notion  only the labels in the training set are considered private. This notion is weaker than
semi-private learning. In particular  any semi-private learner can be easily transformed into a label-
private learner. Another line of work considers the problem of private knowledge transfer [HCB16] 
[PAE+17]  [PSM+18]  and [BTT18]. In this problem  ﬁrst a DP classiﬁcation algorithm with input
private sample is used to provide labels for an unlabeled public dataset. Then  the result is used
to train a non-private learner. [BTT18] gives sample complexity bounds in the setting when the
DP algorithm is required to label the public data in an online fashion. Their bounds are thus not
comparable to ours.

2 Preliminaries
Let X denote an arbitrary domain  let Z = X × {0  1} denote the examples domain  and let
Z∗ = ∪∞
n=1Z n. A function h : X → {0  1} is called a concept/hypothesis  a set of hypotheses
H ⊆ {0  1}X is called a concept/hypothesis class. The VC dimension of H is denoted by VC(H).
We use D to denote a distribution over Z  and DX to denote the marginal distribution over X . We
use S ∼ Dn to denote a sample/dataset S = {(x1  y1)  . . .   (xn  yn)} of n i.i.d. draws from D.
Expected error: The expected/population error of a hypothesis h : X → {0  1} with respect to a
distribution D over Z is deﬁned by err(h;D) (cid:44) E
A distribution D is called realizable by H if there exists h∗ ∈ H such that err(h∗;D) = 0. In this
case  the data distribution D is described by a distribution DX over X and a hypothesis h∗ ∈ H. For
realizable distributions  the expected error of a hypothesis h will be denoted by err (h; (DX   h∗)) (cid:44)
E
x∼DX
Empirical error: The empirical error of a hypothesis h : X → {0  1} with respect to a labeled

dataset S = {(x1  y1)  . . .   (xn  yn)} will be denoted by(cid:99)err (h; S) (cid:44) 1

(x y)∼D [1 (h(x) (cid:54)= y)].

[1 (h(x) (cid:54)= h∗(x))] .

(cid:80)n
i=1 1 (h(xi) (cid:54)= yi) .

n

Expected disagreement: The expected disagreement between a pair of hypotheses h1 and h2 with
respect to a distribution DX over X is deﬁned as dis (h1  h2; DX ) (cid:44) E
[1 (h1(x) (cid:54)= h2(x))] .
x∼DX

3

(cid:80)n
i=1 1 (h1(xi) (cid:54)= h2(xi)) .

an unlabeled dataset T = {x1  . . .   xn} is deﬁned as (cid:99)dis (h1  h2; T ) =

Empirical disagreement: The empirical disagreement between a pair of hypotheses h1
and h2 w.r.t.
1
n
Deﬁnition 2.1 (Differential Privacy [DMNS06  DKM+06]). Let   δ > 0. A (randomized) algorithm
A with input domain Z∗ and output range R is called (  δ)-differentially private if for all pairs of
datasets S  S(cid:48) ∈ Z∗ that differs in exactly one data point  and every measurable O ⊆ R  we have

Pr (A(S) ∈ O) ≤ e · Pr (A(S(cid:48)) ∈ O) + δ 

where the probability is over the random coins of A. When δ = 0  we say that A is pure -differentially
private.

We study learning algorithms that take as input two datasets: a private dataset Spriv and a public
∗ is
dataset Spub  and output a hypothesis h : X → {0  1}. The private set Spriv ∈ (X × {0  1})
labeled. We distinguish between two settings of the learning problem depending on whether the
public dataset is labeled or not. To avoid confusion  we denote an unlabeled public set as Tpub ∈ X ∗ 
and use Spub to denote a labeled public set. We formally deﬁne learners in these two settings.
Deﬁnition 2.2 ((α  β    δ)- Semi-Private Learner). Let H ⊂ {0  1}X be a hypothesis class. A
randomized algorithm A is (α  β    δ)-SP (semi-private) learner for H with private sample size npriv
and public sample size npub if the following conditions hold:

1. For every distribution D over Z = X × {0  1}  given datasets Spriv ∼ Dnpriv and Spub ∼
Dnpub as inputs to A  with probability at least 1 − β (over the choice of Spriv  Spub  and the
random coins of A)  A outputs a hypothesis A (Spriv  Spub) = ˆh ∈ {0  1}X satisfying

(cid:16)ˆh; D(cid:17) ≤ inf

err

h∈H err (h; D) + α.

2. For all S ∈ Z npub   A (·  S) is (  δ)-differentially private.

When the second condition is satisﬁed with δ = 0 (i.e.  pure differential privacy)  we refer to A as
(α  β  )-SP learner (i.e.  pure semi-private learner).
As a special case of the above deﬁnition  we say that an algorithm A is an (α  β    δ)-semi-privately
learner for a class H under the realizability assumption if it satisﬁes the ﬁrst condition in the deﬁnition
only with respect to all distributions that are realizable by H.
Deﬁnition 2.3 (Semi-Privately Learnable Class). We say that a class H is semi-privately learnable if
there are functions npriv : (0  1)2 → N  npub : (0  1)2 → N  where npub(α ·) = o(1/α2)  and there
is an algorithm A such that for every α  β ∈ (0  1)  when A is given private and public samples of
sizes npriv = npriv(α  β)  and npub = npub(α  β)  it (α  β  0.1  negl (npriv))-semi-privately learns H.
Note that in the deﬁnition above  the privacy parameters are set as follows:  = 0.1 and δ is negligible
function in the private sample size (and δ = 0 for a pure semi-private learner).
The restriction npub = o(1/α2) in the above deﬁnition is because taking Ω(VC(H)/α2) public
examples sufﬁces for learning the class without any private examples.
Deﬁnition 2.4 ((α  β    δ)-Semi-Supervised Semi-Private Learner). The deﬁnition is analogous to
Deﬁnition 2.2 except that the public sample is unlabeled. An algorithm that satisﬁes this deﬁnition is
referred to as (α  β    δ)-SS-SP (semi-supervised semi-private) learner.

Private learning without public data: In the standard setting of (  δ)-differentially private learning 
the learner has no access to public data. We note that this setting can be viewed as a special case of
Deﬁnitions 2.2 and 2.4 by taking npub = 0. In such case  we refer to the learner as (α  β    δ)-private
learner. As before  when δ = 0  we call the learner pure private learner. The notion of privately
learnable class H is deﬁned analogously to Deﬁnition 2.3 with npub(α  β) = 0 for all α  β.
We will use the following lemma due to Beimel et al. [BNS15]:
Lemma 2.5 (Special case of Theorem 4.16 in [BNS15]). Any class H that is privately learnable
under realizability assumption is also privately learnable (i.e.  in the agnostic setting).

4

The following fact follows from the private boosting technique due to [DRV10]:
Lemma 2.6 (follows from Theorem 6.1 [DRV10] (the full version)). For any class H  under the
realizability assumption  if there is a (0.1  0.1  0.1)-pure private learner for H  then H is privately
learnable by a pure private algorithm.

We note that no analogous statement to the one in Lemma 2.6 is known for approximate private
learners (see the full version [ABM19] for a discussion).
We will also use the following notion of coverings:

Deﬁnition 2.7 (α-cover for a hypothesis class). A family of hypotheses (cid:101)H is said to form an α-cover
there is ˜h ∈ (cid:101)H such that dis

for a hypothesis class H ⊆ {0  1}X with respect to a distribution DX over X if for every h ∈ H 

(cid:17) ≤ α.

h  ˜h; DX

(cid:16)

3 Upper Bound
In this section we show that every VC class H can be semi-privately learned in the agnostic case with
only ˜O(VC(H)/α) public examples:
Theorem 3.1 (Upper bound). Let H be a hypothesis class and let VC (H) = d. For any α  β ∈
(0  1)   > 0  ASSPP is an (α  β  )-semi-supervised semi-private agnostic learner for H with private
and public sample complexities:

(cid:18)(cid:0)d log(1/α) + log(1/β)(cid:1) max
(cid:19)
(cid:18) d log(1/α) + log(1/β)

(cid:18) 1

1
 α

α2  

(cid:19)(cid:19)

 

npriv = O

npub = O

α

.

Proof overview. The upper bound is based on a reduction to the fact that any ﬁnite hypothesis class H(cid:48)
can be learned privately with sample complexity (roughly) O(log|H(cid:48)|) via the exponential mechanism
[KLN+08]. In more detail  we use the (unlabeled) public data to construct a ﬁnite class H(cid:48) that forms
a “good enough approximation” of the (possibly inﬁnite) original class H (See Algorithm 1). The
relevant notion of approximation is captured by the deﬁnition of α-cover (Deﬁnition 2.7). Indeed  it
sufﬁces to output an hypothesis h(cid:48) ∈ H(cid:48) that “α-approximates” an optimal hypothesis h∗ ∈ H.
Thus  the crux of the proof boils down to the question: How many samples from DX are needed in
order to construct an α-cover for H? It is not hard to see that (roughly) O(VC(H)/α2) examples
sufﬁce: indeed  these many examples sufﬁce to approximate the distances dis (h(cid:48)  h(cid:48)(cid:48); DX ) for
every h(cid:48)  h(cid:48)(cid:48) ∈ H  which sufﬁces to construct the α-cover. We show how to reduce the number of
examples to only (roughly) O(VC(H)/α) examples (Lemma 3.3)  which  by our lower bound  is
nearly optimal.
Algorithm 1 ASSPP: Semi-Supervised Semi-Private Agnostic Learner
Input: Private labeled dataset: Spriv = {(x1  y1)  . . .   (xnpriv   ynpriv )} ∈ Z npriv  a public unlabeled
dataset: Tpub = (˜x1 ···   ˜xnpub ) ∈ X npub  a hypothesis class H ⊂ {0  1}X   and a privacy
1: Let (cid:101)T = {ˆx1  . . .   ˆx ˆm} be the set of points x ∈ X appearing at least once in Tpub.
parameter  > 0.
2: Let ΠH((cid:101)T ) = {(h(ˆx1)  . . .   h(ˆx ˆm)) : h ∈ H} .
3: Initialize (cid:101)HTpub = ∅.
4: for each c = (c1  . . .   c ˆm) ∈ ΠH((cid:101)T ): do
Add to (cid:101)HTpub arbitrary h ∈ H that satisﬁes h(ˆxj) = cj for every j = 1  . . .   ˆm.
6: Use the exponential mechanism with inputs Spriv  (cid:101)HTpub   and score function q(Spriv  h) (cid:44)
−(cid:99)err(h; Spriv) to select hpriv ∈ (cid:101)HTpub.

5:

7: return hpriv.

The proof of Theorem 3.1 relies on the following lemmas. (The full proof of Theorem 3.1 can be
found in the full version [ABM19]).

5

(cid:17)

(cid:16) d log(1/α)+log(1/β)

Lemma 3.2. For all Tpub ∈ X npub   ASSPP(·  Tpub) is -differentially private.
The proof is straightforward and is deferred to the full version [ABM19].
probability at least 1 − β  the family (cid:101)HTpub constructed in Step 5 of Algorithm 1 is an α-cover for H
Lemma 3.3 (α-cover for H). Let Tpub ∼ DnpubX   where npub = O
w.r.t. DX .
We need to show that with high probability  for every h ∈ H there exists ˜h ∈ (cid:101)HTpub such that
We now prove Lemma 3.3. (We include a a more detailed version in the full version [ABM19]).
dis(h  ˜h;DX ) ≤ α. Let (cid:101)T = {ˆx1  . . .   ˆx ˆm} be as deﬁned in ASSPP (Algorithm 1)  and deﬁne
h((cid:101)T ) = (h(ˆx1)  . . .   h(ˆx ˆm)). By construction  there must exist ˜h ∈ (cid:101)HTpub such that ∀j ∈ [ ˆm]
˜h(ˆxj) = h(ˆxj); that is (cid:99)dis

(cid:110)∃h1  h2 ∈ H : dis (h1  h2;DX ) > α and (cid:99)dis (h1  h2; Tpub) = 0
(cid:111)

= 0. For Tpub ∼ DnpubX   deﬁne the event

(cid:16)˜h  h; Tpub

. Then  with

(cid:17)

α

Bad =

We will show that

(cid:18) 2e npub

(cid:19)2d

d

e−α npub/4.

P

Tpub∼DnpubX

[Bad] ≤ 2

Before we do so  we ﬁrst show that (1) sufﬁces to prove the lemma. Indeed  if dis
for some h ∈ H then the event Bad occurs. Hence 

(cid:104)(cid:101)HTpub is not an α-cover

(cid:105) ≤ 2

(cid:18) 2e npub

(cid:19)2d

d

P

Tpub∼DnpubX

(cid:16)˜h  h; DX

(cid:17)

(1)

> α

e−α npub/4.

(cid:16) d log(1/α)+log(1/β)

(cid:17)

Now  via standard manipulation  this bound is at most β when npub = O
which yields the desired bound and ﬁnishes the proof.
Now  it is left to prove (1). To do so  we use a standard VC-based uniform convergence bound (a.k.a
α-net bound) on the class H∆ (cid:44) {h1∆h2 : h1  h2 ∈ H} where h1∆h2 : X → {0  1} is deﬁned as

α

 

h1∆h2(x) (cid:44) 1 (h1(x) (cid:54)= h2(x))

∀x ∈ X

(cid:1)2d

Note that GH∆ (m) ≤(cid:0) e m
ΠH(V ). Hence  GH∆ (m) ≤ (GH(m))2 ≤(cid:0) e m

|ΠH∆ (V )|  where
Let GH∆ denote the growth function of H∆; i.e.  for any m  GH∆ (m) (cid:44) max
V :|V |=m
ΠH∆ (V ) is the set of all possible dichotomies that can be generated by H∆ on a set V of size m.
. This follows from the fact that for any set V of size m  we have
|ΠH∆ (V )| ≤ |ΠH(V )|2 since every dichotomy in ΠH∆ is determined by a pair of dichotomies in
(cid:104)∃h ∈ H∆ : dis (h  h0; DX ) > α and (cid:99)dis (h  h0; Tpub) = 0
(cid:105)
  where the last inequality follows from Sauer’s

Lemma [Sau72]. Now  by invoking a uniform convergence argument  we have

(cid:1)2d

[Bad] =

P

P

d

d

Tpub∼DnpubX

Tpub∼DnpubX

≤ 2GH∆ (2 npub) e−α npub/4 ≤ 2

e−α npub/4.

(cid:18) 2e npub

(cid:19)2d

d

The ﬁrst bound in the second line follows from the so-called double-sample argument used in virtually
all VC-based uniform convergence bounds (e.g.  [SSBD14]). This completes the proof of Lemma 3.3.

4 Lower Bound

In this section we establish that our upper bound on the public sample complexity is nearly tight.
Theorem 4.1 (Lower bound for classes of inﬁnite Littlestone dimension). Let H be any class with
an inﬁnite Littlestone dimension (e.g.  the class of thresholds over R). Then  any semi-private learner
for H must have public sample of size npub = Ω(1/α)  where α is the excess error.

6

In the case of pure differentially privacy we get a stronger statement which manifests a dichotomy
that applies for every class:
Theorem 4.2 (Pure private vs. pure semi-private learners). Every class H must satisfy exactly one of
the following:

1. H is learnable by a pure private learner.
2. Any pure semi-private learner for H must have npub = Ω(1/α)  where α is the excess error

1

10 then it can not be semi-privately learned with excess error of α with less than

Proof overview. The crux of the argument is a public-data-reduction lemma (Lemma 4.4)  which
shows how one can reduce the number of public examples at the price of a proportional increase in
the excess error. This lemma implies  for example  that if H can be learned up to an excess error
of α with less than
1000α public examples then it can also be privately learned without any public
10. Stating contra-positively  if H can not be privately learned with
examples and excess error < 1
excess error < 1
1000α
public examples. This yields a lower bound of Ω(1/α) on the public sample complexity for every
class H which is not privately learnable with constant excess error
One example for such a class is any class with inﬁnite Littlestone dimension (e.g.  the class of
1-dimensional thresholds over an inﬁnite domain). This follows from the result in [ALMM19]:
Theorem 4.3 (Restatement of Corollary 2 in [ALMM19]). Let H be any class of inﬁnite Littlestone
dimension (e.g.  the class of thresholds over an inﬁnite domain X ⊆ R). For any n ∈ N  given a
-private learner for H (even in the
private sample of size n  there is no
realizable case).

(cid:16) 1

16   0.1 

16   1

100 n2 log(n)

1

(cid:17)

1

(cid:101).

10 npub

16     δ(cid:1)-private learner that learns any

and public sample size npub. Then  there is a(cid:0)100 npub α  1

The aforementioned reduction we use for the lower bound holds even when the public sample is
labeled  and it holds for both pure and approximate private/semi-private learners.
We now state and prove the reduction lemma outlined above.
Lemma 4.4 (Public data reduction lemma). Let 0 < α ≤ 1/100   > 0  δ ≥ 0. Suppose there is an
18     δ)-agnostic semi-private learner for an hypothesis class H with private sample size npriv
(α  1
distribution realizable by H with input sample size (cid:100) npriv
Proof. Let A denote the assumed agnostic-case semi-private learner for H with input private sample
of size npriv and input public sample of size npub. Using A  we construct a realizable-case private
learner for H  which we denote by B. The description of B appears in Algorithm 2.
The following two claims about B sufﬁce to prove the lemma.
Algorithm 2 Description of the private learner B:
Input: Private sample ˜S = (˜z1  . . .   ˜z˜n) of size ˜n = (cid:100)npriv/(10 · npub)(cid:101).
1: Pick a ﬁxed (dummy) distribution D0 over Z = X × {0  1} where the label y ∈ {0  1} is drawn
2: Set p = 1/(100 · npub).
3: Using ˜S and D0  construct samples Spriv  Spub using procedures PrivSamp( ˜S D0  p  npriv) and
4: Return ˜h = A(Spriv  Spub).

uniformly at random from {0  1} independently from x ∈ X .

PubSamp(D0  npub) given by Algorithms 3 and 4 below.

Claim 4.5 (Privacy guarantee of B). B is (  δ)-differentially private
The above claim easily follows since A is a semi-private learner  Spub does not contain any points
from ˜S  and each point in ˜S appears at most once in Spriv.
Claim 4.6 (Accuracy guarantee of B). Let D be any distribution over Z that is realizable by H.
Suppose ˜S ∼ D ˜n. Then  except with probability at most 1/16 (over the choice of ˜S and internal
randomness in B)  the output hypothesis ˜h satisﬁes: err(˜h; D) ≤ 100 npub α.

7

Algorithm 3 Private Sample Generator PrivSamp:
Input: Sample ˜S = (˜z1  . . .   ˜z˜n)  Distribution D0  parameter p  sample size npriv.
1: i := 1
2: while ˜S (cid:54)= ∅ and i ≤ npriv: do
3:

Sample bi ∼ Ber(p) (independently for each i)  where Ber(p) is Bernoulli distribution with
mean p.
if bi = 1: then

i = ˜zji  where ji =(cid:80)i

k=1 bk.

i

4:
5:
6:
7:
8:
9:
10: return Spriv = (zprv

to be the next element in ˜S  i.e.  zprv
Set zprv
Remove this element from ˜S: ˜S ← ˜S \ ˜zji.
else
Set zprv
i ← i + 1

i   where z0

i = z0

1   . . .   zprv

npriv ).

i is a fresh independent example from the “dummy” distribution D0.

Algorithm 4 Public Sample Generator PubSamp:
Input: Distribution D0  sample size npub.
1: for i = 1  . . .   npub : do
2:
3: return Spub = (zpub

i where z0

i = z0

Set zpub

  . . .   zpub
npub

)

1

i is a fresh independent example from D0.

1  . . .   z0

Let D(p) denote the mixture distribution p·D+(1−p)·D0 (recall the deﬁnition of p from Algorithm 2).
To prove Claim 4.6  we ﬁrst show that both Spriv and Spub can be “viewed” as being sampled from
D(p). The claim will then follow since A learns H with respect to D(p).
First  note that since ˜n = 10 · p · npriv  then by Chernoff’s bound  except with probability < 0.01 
Algorithm 3 exits the WHILE loop with i = npriv. Thus  except with probability < 0.01  we have
(2)

|Spriv| = npriv  hence  Spriv ∼ Dnpriv
(p) .

. We will show that Dnpub

As for Spub  note that Spub = (z0
variation to Dnpub
i ∈ [npub]  ˆzi = bi vi + (1 − bi) z0

(p) . Let (cid:98)Spub = (ˆz1  . . .   ˆznpub ) be i.i.d. sequence generated as follows: for each
It is clear that (cid:98)Spub ∼ Dnpub
i   where (b1  . . .   bnpub ) ∼ (Ber(p))npub  and (v1  . . .   vn) ∼ Dnpub.
P(cid:104)(cid:98)Spub = Spub
Note that P(cid:104)(cid:98)Spub (cid:54)= Spub
(cid:105)
by D0) is at most 0.01. In particular  the probability of any event w.r.t. the distribution of (cid:98)Spub is at

is the probability measure attributed to the ﬁrst component of the mixture
distribution D(p) of ˆSpub (i.e.  the component from D). Hence  it follows that the total variation
between the distribution of ˆSpub (induced by the mixture D(p)) and the distribution of Spub (induced

(cid:105) ≥ P [bi = 0 ∀ i ∈ [npub]] =

(cid:19)npub ≥ 0.99

(p) . Moreover  observe that

is close in total

) ∼ Dnpub

most 0.01 far from the probability of the same event w.r.t. the distribution of Spub. Hence 

100 npub

(cid:18)

1 −

npub

1

0

0

(cid:21)

Spriv Spub A
−
P

(cid:1) − min
(cid:21)
h∈H err(h;D(p)) > α
(cid:1) − min
h∈H err(h;D(p)) > α
(cid:21)
(cid:1) − min
Now  from (2) and the premise that A is agnostic semi-private learner  we have
h∈H err(h;D(p)) > α
(cid:1) − min
h∈H err(h;D(p)) ≤ α.

(cid:20)
err(cid:0)A(Spriv  Spub); D(p)
(cid:20)
P
err(cid:0)A(Spriv  Spub); D(p)
Spriv (cid:98)Spub A
(cid:20)
err(cid:0)A(Spriv  Spub); D(p)
err(cid:0)A(Spriv  Spub); D(p)

Hence  using (3)  we conclude that except with probability < 1/16 

Spriv (cid:98)Spub A

P

≤ 1
17

≤ 0.01

(3)

(4)

8

Note that for any h  err(h; D(p)) = p· err(h; D) + (1− p)· err(h;D0) = p· err(h; D) + 1
2 (1− p) 
where the last equality follows from the fact that the labels generated by D0 are completely noisy
h∈H err(h;D). That is 
(uniformly random labels). Hence  we have arg min
the optimal hypothesis with respect to the realizable distribution D is also optimal with respect to
the mixture distribution D(p). Let h∗ ∈ H denote such hypothesis. Note that err(h∗;D) = 0 and
err(h∗;D(p)) = 1
2 (1 − p). These observations together with (4) imply that except with probability
< 1/16  we have

h∈H err(h;D(p)) = arg min

(cid:16)B( ˜S); D(cid:17)

α ≥ p · err (A(Spriv  Spub); D)

= err (A(Spriv  Spub); D) ≤ 100 · npub · α. This completes the proof.

Hence  err

With Lemma 4.4  we are now ready to prove the main results for this section:

Proof of Theorem 4.1
Proof. Suppose A is a semi-private learner for H with sample complexities npriv  npub. In particular 
18 ) private and public examples  A is (α  1
priv log(npriv) )-semi-
given npriv(α  1
private learner for H. Hence  by Lemma 4.4  there is (100npubα  1
priv log(npriv) )-private
learner for H. Thus  Theorem 4.3 implies that 100npubα > 1
1600 α as
required.

16 and hence that npub > 1

18 )  npub(α  1

16   0.1 

18   0.1 

100 n2
1

1

100 n2

Proof of Theorem 4.2
Proof. First  if H is learnable by a pure private learner  then trivially the second condition cannot
hold since H can be learned without any public examples. Now  suppose that the ﬁrst item does not
hold. Note that by Lemma 2.5  this implies that there is no pure private learner for H with respect
private learner for H with respect to realizable distributions. Now  suppose A is a pure semi-private
learner for H with sample complexities npriv(α  1

16   0.1(cid:1)-pure
18   0.1(cid:1)-pure semi-private
16   0.1(cid:1)-pure private learner for H w.r.t. realizable distributions. This together with

to realizable distributions. By Lemma 2.6  this in turn implies that there is no(cid:0) 1
learner A for H. Then  this implies that for any α > 0  A is an(cid:0)α  1
(cid:0)100 npub α  1

18 ). Hence  by Lemma 4.4  there is a

18 )  npub(α  1

16   1

the earlier conclusion implies that 100 npub α > 1
that the condition in the second item holds.

16   and therefore that npub > 1

1600 α   which shows

Acknowledgements

N. Alon’s research is supported in part by NSF grant DMS-1855464  BSF grant 2018267  and the
Simons Foundation. R. Bassily’s research is supported by NSF Awards AF-1908281  SHF-1907715 
Google Faculty Research Award  and OSU faculty start-up support.

9

References
[ABM19] Noga Alon  Raef Bassily  and Shay Moran. Limits of private learning with access to

public data. arXiv:1910.11519 [cs.LG]  2019.

[ALMM19] Noga Alon  Roi Livni  Maryanthe Malliaris  and Shay Moran. Private pac learn-
ing implies ﬁnite littlestone dimension. STOC 2019  pp. 852-860 (arXiv preprint
arXiv:1806.00949)  2019.

[BNS13] Amos Beimel  Kobbi Nissim  and Uri Stemmer. Private learning and sanitization:
In Approximation  Randomization  and
Pure vs. approximate differential privacy.
Combinatorial Optimization. Algorithms and Techniques  pages 363–378. Springer 
2013.

[BNS15] Amos Beimel  Kobbi Nissim  and Uri Stemmer. Learning privately with labeled and
unlabeled examples. In Proceedings of the twenty-sixth annual ACM-SIAM symposium
on Discrete algorithms  pages 461–477. Society for Industrial and Applied Mathematics 
2015.

[BNSV15] Mark Bun  Kobbi Nissim  Uri Stemmer  and Salil Vadhan. Differentially private release
and learning of threshold functions. In Foundations of Computer Science (FOCS)  2015
IEEE 56th Annual Symposium on  pages 634–649. IEEE  2015.

[BPS09] Shai Ben-David  Dávid Pál  and Shai Shalev-Shwartz. Agnostic online learning. In
COLT 2009 - The 22nd Conference on Learning Theory  Montreal  Quebec  Canada 
June 18-21  2009  2009.

[BTT18] Raef Bassily  Abhradeep Guha Thakurta  and Om Dipakbhai Thakkar. Model-agnostic
private learning. In Advances in Neural Information Processing Systems  pages 7102–
7112  2018.

[CH11] Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially
private learning. In Proceedings of the 24th Annual Conference on Learning Theory 
pages 155–186  2011.

[DKM+06] Cynthia Dwork  Krishnaram Kenthapadi  Frank McSherry  Ilya Mironov  and Moni
Naor. Our data  ourselves: Privacy via distributed noise generation. In EUROCRYPT 
2006.

[DMNS06] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise
to sensitivity in private data analysis. In Theory of Cryptography Conference  pages
265–284. Springer  2006.

[DRV10] Cynthia Dwork  Guy N. Rothblum  and Salil P. Vadhan. Boosting and differential privacy.
In FOCS (Full version: https://guyrothblum.ﬁles.wordpress.com/2014/11/drv10.pdf) 
2010.

[HCB16] Jihun Hamm  Yingjun Cao  and Mikhail Belkin. Learning privately from multiparty

data. In International Conference on Machine Learning  pages 555–563  2016.

[KLN+08] Shiva Prasad Kasiviswanathan  Homin K. Lee  Kobbi Nissim  Sofya Raskhodnikova 
In FOCS  pages 531–540. IEEE

and Adam Smith. What can we learn privately?
Computer Society  2008.

[Lit87] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-

threshold algorithm. Machine Learning  2(4):285–318  1987.

[PAE+17] Nicolas Papernot  Martın Abadi  Úlfar Erlingsson  Ian Goodfellow  and Kunal Talwar.
Semi-supervised knowledge transfer for deep learning from private training data. stat 
1050  2017.

[PSM+18] Nicolas Papernot  Shuang Song  Ilya Mironov  Ananth Raghunathan  Kunal Talwar  and
Úlfar Erlingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908 
2018.

10

[Sau72] Norbert Sauer. On the density of families of sets. Journal of Combinatorial Theory 

Series A  13(1):145–147  1972.

[SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From

theory to algorithms. Cambridge university press  2014.

11

,Noga Alon
Raef Bassily
Shay Moran