2017,A Sample Complexity Measure with Applications to Learning Optimal Auctions,We introduce a new sample complexity measure  which we refer to as split-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size $m$  the split-sample growth rate $\hat{\tau}_H(m)$ counts how many different hypotheses can empirical risk minimization output on any sub-sample of $S$ of size $m/2$. We show that the expected generalization error is upper bounded by $O\left(\sqrt{\frac{\log(\hat{\tau}_H(2m))}{m}}\right)$. Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure  greatly simplifies the analysis of the sample complexity of optimal auction design  for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes  ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample.,A Sample Complexity Measure with Applications to

Learning Optimal Auctions

Vasilis Syrgkanis
Microsoft Research

vasy@microsoft.com

Abstract

We introduce a new sample complexity measure  which we refer to as split-sample
growth rate. For any hypothesis H and for any sample S of size m  the split-
sample growth rate ˆτH (m) counts how many different hypotheses can empirical
risk minimization output on any sub-sample of S of size m/2. We show that
the expected generalization error is upper bounded by O
. Our
result is enabled by a strengthening of the Rademacher complexity analysis of
the expected generalization error. We show that this sample complexity measure 
greatly simpliﬁes the analysis of the sample complexity of optimal auction design 
for many auction classes studied in the literature. Their sample complexity can
be derived solely by noticing that in these auction classes  ERM on any sample or
sub-sample will pick parameters that are equal to one of the points in the sample.

(cid:18)(cid:113) log(ˆτH (2m))

(cid:19)

m

1

Introduction

The seminal work of [11] gave a recipe for designing the revenue maximizing auction in auction
settings where the private information of players is a single number and when the distribution over
this number is completely known to the auctioneer. The latter raises the question of how has the
auction designer formed this prior distribution over the private information. Recent work  starting
from [4]  addresses the question of how to design optimal auctions when having access only to
samples of values from the bidders. We refer the reader to [5] for an overview of the existing results
in the literature. [4  9  10  2] give bounds on the sample complexity of optimal auctions without
computational efﬁciency  while recent work has also focused on getting computationally efﬁcient
learning bounds [5  13  6].
This work solely focuses on sample complexity and not computational efﬁciency and thus is more
related to [4  9  10  2]. The latter work  uses tools from supervised learning  such as pseudo-
dimension [12] (a variant of VC dimension for real-valued functions)  compression bounds [8] and
Rademacher complexity [12  14] to bound the sample complexity of simple auction classes. Our
work introduces a new measure of sample complexity  which is a strengthening the Rademacher
complexity analysis and hence could also be of independent interest outside the scope of the sample
complexity of optimal auctions. Moreover  for the case of auctions  this measure greatly simpliﬁes
the analysis of their sample complexity in many cases.
In particular  we show that in general PAC learning settings  the expected generalization error is upper
bounded by the Rademacher complexity not of the whole class of hypotheses  but rather only over
the class of hypotheses that could be the outcome of running Expected Risk Minimization (ERM)
on a subset of the samples of half the size. If the number of these hypotheses is small  then the
latter immediately yields a small generalization error. We refer to the growth rate of the latter set of
hypotheses as the split-sample growth rate. This measure of complexity is not restricted to auction
design and could be relevant to general statistical learning theory.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

We then show that for many auction classes such as single-item auctions with player-speciﬁc reserves 
single item t-level auctions and multiple-item item pricing auctions with additive buyers  the split-
sample growth rate can be very easily bounded. The argument boils down to just saying that the
Empirical Risk Minimization over this classes will set the parameters of the auctions to be equal to
some value of some player in the sample. Then a simple counting argument gives bounds of the same
order as in prior work in the literature that used the pseudo-dimension [9  10]. In multi-item settings
we also get improvements on the sample complexity bound.
Split-sample growth rate is similar in spirit to the notion of local Rademacher complexity [3]  which
looks at the Rademacher complexity on a subset of hypotheses with small empirical error.
In
particular  our proof is based on a reﬁnement of the classic analysis Rademacher complexity analysis
of generalization error (see e.g. [14]). However  our bound is more structural  restricting the set
to outcomes of the chosen ERM process on a sub-sample of half the size. Moreover  we note that
counting the number of possible outputs of ERM also has connections to a counting argument made
in [1] in the context of pricing mechanisms. However  in essence the argument there is restricted to
transductive settings where the sample “features” are known in advance and ﬁxed and thereby the
argument is much more straightforward and more similar to standard notions of “effective hypothesis
space” used in VC-dimension arguments.
Our new measure of sample complexity is applicable in the general statistical learning theory
framework and hence could have applications beyond auctions. To convey a high level intuition of
settings where split-sample growth could simplify the sample complexity analysis  suppose that the
output hypothesis of ERM is uniquely deﬁned by a constant number of sample points (e.g. consider
linear separators and assume that the loss is such that the output of ERM is uniquely characterized
by choosing O(d) points from the sample). Then this means that the number of possible hypotheses

on any subset of size m/2  is at most O((cid:0)m
(cid:1)) = O(md). Then the split sample growth rate analysis
immediately yields that the expected generalization error is O((cid:112)d · log(m)/m)  or equivalently the

sample complexity of learning over this hypothesis class to within an  error is O(d · log(1/)/2).

d

2 Preliminaries

We look at the sample complexity of optimal auctions. We consider the case of m items  and n
bidders. Each bidder has a value function vi drawn independently from a distribution Di and we
denote with D the joint distribution.
We assume we are given a sample set S = {v1  . . .   vm}  of m valuation vectors  where each vt ∼ D.
Let H denote the class of all dominant strategy truthful single item auctions (i.e. auctions where no
player has incentive to report anything else other than his true value to the auction  independent of
what other players do). Moreover  let

n(cid:88)

r(h  v) =

ph
i (v)

(1)

where ph
valuation vector v. Finally  let

i (·) is the payment function of mechanism h  and r(h  v) is the revenue of mechanism h on
(2)

RD(h) = Ev∼D [r(h  v)]

i=1

ES [RD(hS)] ≥ sup
h∈H

RD(h) − (m)

be the expected revenue of mechanism h under the true distribution of values D.
Given a sample S of size m  we want to compute a dominant strategy truthful mechanism hS  such
that:

(3)
where (m) → 0 as m → ∞. We refer to (m) as the expected generalization error. Moreover  we
deﬁne the sample complexity of an auction class as:
Deﬁnition 1 (Sample Complexity of Auction Class). The (additive error) sample complexity of an
auction class H and a class of distributions D  for an accuracy target  is deﬁned as the smallest
number of samples m()  such that for any m ≥ m():
ES [RD(hS)] ≥ sup
h∈H

RD(h) − 

(4)

2

We might also be interested in a multiplcative error sample complexity  i.e.

ES [RD(hS)] ≥ (1 − ) sup
h∈H

RD(h)

(5)

The latter is exactly the notion that is used in [4  5]. If one assumes that the optimal revenue on the
distribution is lower bounded by some constant quantity  then an additive error implies a multiplicative
error. For instance  if one assumes that player values are bounded away from zero with signiﬁcant
probability  then that implies a lower bound on revenue. Such assumptions for instance  are made in
the work of [9]. We will focus on additive error in this work.
We will also be interested in proving high probability guarantees  i.e. with probability 1 − δ:

RD(hS) ≥ sup
h∈H

RD(h) − (m  δ)

(6)

where for any δ  (m  δ) → 0 as m → ∞.

3 Generalization Error via the Split-Sample Growth Rate

We turn to the general PAC learning framework  and we give generalization guarantees in terms of a
new notion of complexity of a hypothesis space H  which we denote as split-sample growth rate.
Consider an arbitrary hypothesis space H and an arbitrary data space Z  and suppose we are given
a set S of m samples {z1  . . .   zm}  where each zt is drawn i.i.d. from some distribution D on Z.
We are interested in maximizing some reward function r : H × Z → [0  1]  in expectation over
distribution D. In particular  denote with RD(h) = Ez∼D [r(h  z)].
We will look at the Expected Reward Maximization algorithm on S  with some ﬁxed tie-breaking
rule. Speciﬁcally  if we let

m(cid:88)

t=1

RS(h) =

1
m

r(h  zt)

(7)

(8)

then ERM is deﬁned as:

hS = arg sup
h∈H

RS(h)

where ties are broken based on some pre-deﬁned manner.
We deﬁne the notion of a split-sample hypothesis space:
Deﬁnition 2 (Split-Sample Hypothesis Space). For any sample S  let ˆHS  denote the set of all
hypothesis hT output by the ERM algorithm (with the pre-deﬁned tie-breaking rule)  on any subset
T ⊂ S  of size (cid:100)|S|/2(cid:101)  i.e.:

ˆHS = {hT : T ⊂ S |T| = (cid:100)|S|/2(cid:101)}

(9)

Based on the split-sample hypothesis space  we also deﬁne the split-sample growth rate of a hypothesis
space H at value m  as the largest possible size of ˆHS for any set S of size m.
Deﬁnition 3 (Split-Sample Growth Rate). The split-sample growth rate of a hypothesis H and an
ERM process for H  is deﬁned as:

ˆτH (m) = sup

S:|S|=m

| ˆHS|

(10)

We ﬁrst show that the generalization error is upper bounded by the Rademacher complexity evaluated
on the split-sample hypothesis space of the union of two samples of size m. The Rademacher
complexity R(S  H) of a sample S of size m and a hypothesis space H is deﬁned as:

R(S  H) = Eσ

2
m

sup
h∈H

σt · r(h  zt)

(11)

where σ = (σ1  . . .   σm) and each σt is an independent binary random variable taking values {−1  1} 
each with equal probability.

3

(cid:34)

(cid:88)

zt∈S

(cid:35)

Lemma 1. For any hypothesis space H  and any ﬁxed ERM process  we have:

RD(h) − ES S(cid:48)
where S and S(cid:48) are two independent samples of some size m.

ES [RD(hS)] ≥ sup
h∈H

(cid:104)R(S  ˆHS∪S(cid:48))

(cid:105)

 

(12)

Proof. Let h∗ be the optimal hypothesis for distribution D. First we re-write the left hand side  by
adding and subtracting the expected empirical reward:

ES [RD(hS)] = ES [RS(hS)] − ES [RS(hS) − RD(hS)]
≥ ES [RS(h∗)] − ES [RS(hS) − RD(hS)]
= RD(h∗) − ES [RS(hS) − RD(hS)]

(hS maximizes empirical reward)
(h∗ is independent of S)

Thus it sufﬁces to upper bound the second quantity in the above equation.
Since RD(h) = ES(cid:48) [RS(cid:48)(h)] for a fresh sample S(cid:48) of size m  we have:

ES [RS(hS) − RD(hS)] = ES [RS(hS) − ES(cid:48) [RS(cid:48)(hS)]]

= ES S(cid:48) [RS(hS) − RS(cid:48)(hS)]

Now  consider the set ˆHS∪S(cid:48). Since S is a subset of S ∪ S(cid:48) of size |S ∪ S(cid:48)|/2  we have by the
deﬁnition of the split-sample hypothesis space that hS ∈ ˆHS∪S(cid:48). Thus we can upper bound the latter
quantity by taking a supremum over h ∈ ˆHS∪S(cid:48):

(cid:35)

ES [RS(hS) − RD(hS)] ≤ ES S(cid:48)

RS(h) − RS(cid:48)(h)

sup

h∈ ˆHS∪S(cid:48)

m(cid:88)

sup

= ES S(cid:48)

1
m
h∈ ˆHS∪S(cid:48)
t ∈ S(cid:48) to zt. By doing show
Now observe  that we can rename any sample zt ∈ S to z(cid:48)
t and sample z(cid:48)
we do not change the distribution. Moreover  we do not change the quantity HS∪S(cid:48)  since S ∪ S(cid:48) is
invariant to such swaps. Finally  we only change the sign of the quantity (r(h  zt) − r(h  z(cid:48)
(cid:35)
t)). Thus
m(cid:88)
if we denote with σt ∈ {−1  1}  a Rademacher variable  we get the above quantity is equal to:
σt (r(h  zt) − r(h  z(cid:48)
ES S(cid:48)
t))

(r(h  zt) − r(h  z(cid:48)
t))

(r(h  zt) − r(h  z(cid:48)
t))

m(cid:88)

= ES S(cid:48)

(cid:34)

(cid:35)

(cid:34)

sup

t=1

1
m

h∈ ˆHS∪S(cid:48)

t=1

sup

h∈ ˆHS∪S(cid:48)

1
m

t=1

(13)
for any vector σ = (σ1  . . .   σm) ∈ {−1  1}m. The latter also holds in expectation over σ  where σt
is randomly drawn between {−1  1} with equal probability. Hence:

ES [RS(hS) − RD(hS)] ≤ ES S(cid:48) σ

σt (r(h  zt) − r(h  z(cid:48)
t))

By splitting the supremma into a positive and negative part and observing that the two expected
quantities are identical  we get:

ES [RS(hS) − RD(hS)] ≤ 2ES S(cid:48) σ

σtr(h  zt)

m(cid:88)

t=1

sup

h∈ ˆHS∪S(cid:48)

1
m

(cid:34)
(cid:104)R(S  ˆHS∪S(cid:48))

h∈ ˆHS∪S(cid:48)

sup

1
m

(cid:105)

m(cid:88)

t=1

= ES S(cid:48)

(cid:34)
(cid:34)

(cid:34)

(cid:35)

(cid:35)

(cid:35)

where R(S  H) denotes the Rademacher complexity of a sample S and hypothesis H.

Observe  that the latter theorem is a strengthening of the fact that the Rademacher complexity upper
bounds the generalization error  simply because:

(cid:104)R(S  ˆHS∪S(cid:48))

(cid:105) ≤ ES S(cid:48) [R(S  H)] = ES [R(S  H)]

ES S(cid:48)

(14)

Thus if we can bound the Rademacher complexity of H  then the latter lemma gives a bound on the
generalization error. However  the reverse might not be true. Finally  we show our main theorem 
which shows that if the split-sample hypothesis space has small size  then we immediately get a
generalization bound  without the need to further analyze the Rademacher complexity of H.

4

Theorem 2 (Main Theorem). For any hypothesis space H  and any ﬁxed ERM process  we have:

ES [RD(hS)] ≥ sup
h∈H

Moreover  with probability 1 − δ:

RD(h) −

2 log(ˆτH (2m))

m

RD(hS) ≥ sup
h∈H

RD(h) − 1
δ

2 log(ˆτH (2m))

m

Proof. By applying Massart’s lemma (see e.g. [14]) we have that:

R(S  ˆHS∪S(cid:48)) ≤

2 log(| ˆHS∪S(cid:48)|)

≤

2 log(ˆτH (2m))

m
Combining the above with Lemma 1  yields the ﬁrst part of the theorem.
Finally 
the random variable
suph∈H RD(h) − RD(hS) is non-negative and by applying Markov’s inequality: with probabil-
ity 1 − δ

the high probability statement follows from observing that

m

RD(h) − RD(hS) ≤ 1
δ

ES

sup
h∈H

sup
h∈H

RD(h) − RD(hS)

≤ 1
δ

2 log(ˆτH (2m))

m

(18)

(cid:21)

(cid:114)

(cid:115)

(cid:20)

(15)

(16)

(17)

(cid:114)
(cid:114)

(cid:114)

(cid:114)



The latter theorem can be trivially extended to the case when r : H × Z → [α  β]  leading to a bound
of the form:

ES [RD(hS)] ≥ sup
h∈H

RD(h) − (β − α)

2 log(ˆτH (2m))

m

(19)

We note that unlike the standard Rademacher complexity  which is deﬁned as R(S  H)  our bound 
which is based on bounding R(S  ˆHS∪S(cid:48)) for any two datasets S  S(cid:48) of equal size  does not imply a
high probability bound via McDiarmid’s inequality (see e.g. Chapter 26 of [14] of how this is done
for Rademacher complexity analysis)  but only via Markov’s inequality. The latter yields a worse
dependence on the conﬁdence δ on the high probability bound of 1/δ  rather than log(1/δ). The
reason for the latter is that the quantity R(S  ˆHS∪S(cid:48))  depends on the sample S  not only in terms
of on which points to evaluate the hypothesis  but also on determining the hypothesis space ˆHS∪S(cid:48).
Hence  the function:

f (z1  . . .   zm) = ES(cid:48)

1
m
does not satisfy the stability property that |f (z) − f (z(cid:48)(cid:48)
m. The reason being that the
supremum is taken over a different hypothesis space in the two inputs. This is unlike the case of the
function:

σt (r(h  zt) − r(h  z(cid:48)
t))

i   z−i)| ≤ 1

h∈ ˆH{z1  ... zm}∪S(cid:48)

(20)

sup

(cid:35)

(cid:34)

f (z1  . . .   zm) = ES(cid:48)

1
m

sup
h∈H

σt (r(h  zt) − r(h  z(cid:48)
t))

(21)

m(cid:88)

t=1

which is used in the standard Rademacher complexity bound analysis  which satisﬁes the latter
stability property. Resolving whether this worse dependence on δ is necessary is an interesting open
question.

4 Sample Complexity of Auctions via Split-Sample Growth

We now present the application of the latter measure of complexity to the analysis of the sample
complexity of revenue optimal auctions. Thoughout this section we assume that the revenue of
any auction lies in the range [0  1]. The results can be easily adapted to any other range [α  β]  by

5

m(cid:88)

t=1



re-scaling the equations  which will lead to blow-ups in the sample complexity of the order of an
extra (β − α) multiplicative factor. This limits the results here to bounded distributions of values.
However  as was shown in [5]  one can always cap the distribution of values up to some upper bound 
for the case of regular distributions  by losing only an  fraction of the revenue. So one can apply the
results below on this capped distribution.

Single bidder and single item. Consider the case of a single bidder and single item auction. In this
setting  it is known by results in auction theory [11] that an optimal auction belongs to the hypothesis
class H = {post a reserve price r for r ∈ [0  1]}. We consider  the ERM rule  which for any set S 
in the case of ties  it favors reserve prices that are equal to some valuation vt ∈ S. Wlog assume that
samples v1  . . .   vm are ordered in increasing order. Observe  that for any set S  this ERM rule on any
subset T of S  will post a reserve price that is equal to some value vt ∈ T . Any other reserve price
in between two values [vt  vt+1] is weakly dominated by posting r = vt+1  as it does not change
which samples are allocated and we can only increase revenue. Thus the space ˆHS is a subset of
{post a reserve price r ∈ {v1  . . .   vm}. The latter is of size m. Thus the split-sample growth of H
is ˆτH (m) ≤ m. This yields:

ES [RD(hS)] ≥ sup
h∈H

(cid:114)
(cid:16) log(1/)
(cid:17)

RD(h) −

2

2 log(2m)

m

.

Equivalently  the sample complexity is mH () = O

Multiple i.i.d. regular bidders and single item.
In this case  it is known by results in auction
theory [11] that the optimal auction belongs to the space of hypotheses H consisting of second price
auctions with some reserve r ∈ [0  1]. Again if we consider ERM which in case of ties favors a
reserve that equals to a value in the sample (assuming that is part of the tied set  or outputs any other
value otherwise)  then observe that for any subset T of a sample S  ERM on that subset will pick a
reserve price that is equal to one of the values in the samples S. Thus ˆτH (m) ≤ n · m. This yields:

(22)

(23)

ES [RD(hS)] ≥ sup
h∈H

RD(h) −

Equivalently  the sample complexity is mH () = O

2 log(2 · n · m)

(cid:114)
(cid:16) log(n/2)

2

(cid:17)

m

.

Non-i.i.d. regular bidders  single item  second price with player speciﬁc reserves.
In this case 
it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses
HSP consisting of second price auctions with some reserve ri ∈ [0  1] for each player i. Again if we
consider ERM which in case of ties favors a reserve that equals to a value in the sample (assuming
that is part of the tied set  or outputs any other value otherwise)  then observe that for any subset T
of a sample S  ERM on that subset will pick a reserve price ri that is equal to one of the values vi
t
of player i in the sample S. There are m such possible choices for each player  thus mn possible
choices of reserves in total. Thus ˆτH (m) ≤ mn. This yields:

ES [RD(hS)] ≥ sup
h∈HSP

RD(h) −

2n log(2m)

m

(24)

If H is the space of all dominant strategy truthful mechanisms  then by prophet inequalities (see [7]) 
we know that suph∈HSP RD(h) ≥ 1

2 suph∈H RD(h). Thus:
RD(h) −

ES [RD(hS)] ≥ 1
2

sup
h∈H

2n log(2m)

m

(25)

irregular bidders single item.

Non-i.i.d.
In this case it is known by results in auction theory
[11] that the optimal auction belongs to the space of hypotheses H consisting of all virtual welfare
maximizing auctions: For each player i  pick a monotone function ˆφi(vi) ∈ [−1  1] and allocate to
the player with the highest non-negative virtual value  charging him the lowest value he could have
bid and still win the item. In this case  we will ﬁrst coarsen the space of all possible auctions.

6

(cid:114)

(cid:114)

s ≤ θi

0 ≤ θi

1 ≤ . . . ≤ θi

In particular  we will consider the class of t-level auctions of [9]. In this class  we constrain the value
functions ˆφi(vi) to only take values in the discrete  grid in [0  1]. We will call this class H. An
equivalent representation of these auctions is by saying that for each player i  we deﬁne a vector of
thresholds 0 = θi
s+1 = 1  with s = 1/. The index of a player is the largest
j for which vi ≥ θj. Then we allocate the item to the player with the highest index (breaking ties
lexicographically) and charge the minimum value he has to bid to continue to win.
Observe that on any sample S of valuation vectors  it is always weakly better to place the thresholds
j on one of the values in the set S. Any other threshold is weakly dominated  as it does not change
θi
the allocation. Thus for any subset T of a set S of size m  we have that the thresholds of each player
i will take one of the values of player i that appears in set S. We have 1/ thresholds for each player 
hence m1/ combinations of thresholds for each player and mn/ combinations of thresholds for all
players. Thus ˆτH (m) ≤ mn/. This yields:

(cid:114)

ES [RD(hS)] ≥ sup
h∈H

RD(h) −

2n log(2m)

 · m

(26)

(27)

(28)

Moreover  by [9] we also have that:

(cid:16) 2n log(2m)

(cid:17)1/3

m

sup
h∈H

  we get:

Picking   =

RD(h) ≥ sup
h∈H

RD(h) − 

ES [RD(hS)] ≥ sup
h∈H

RD(h) − 2

Equivalently  the sample complexity is mH () = O

(cid:19)1/3

(cid:18) 2n log(2m)
(cid:16) n log(1/)

(cid:17)

m

.

3

k items  n bidders  additive valuations  grand bundle pricing.
If the reserve price was anony-
mous  then the reserve price output by ERM on any subset of a sample S of size m  will take the
value of one of the m total values for the items of the buyers in S. So ˆτH (m) = m · n. If the reserve
price was not anonymous  then for each buyer ERM will pick one of the m total item values  so
ˆτH (m) ≤ mn. Thus the sample complexity is mH () = O

(cid:16) n log(1/)

(cid:17)

.

2

k items  n bidders  additive valuations  item prices.
If reserve prices are anonymous  then each
reserve price on item j computed by ERM on any subset of a sample S of size m  will take the value
of one of the player’s values for item j  i.e. n · m. So ˆτH (m) = (n · m)k. If reserve prices are not
anonymous  then the reserve price on item j for player i will take the value of one of the player’s
values for the item. So ˆτH (m) ≤ mn·k. Thus the sample complexity is mH () = O
.

(cid:16) nk log(1/)

(cid:17)

2

k items  n bidders  additive valuations  best of grand bundle pricing and item pricing. ERM
on the combination will take values on any subset of a sample S of size m  that is at most the
product of the values of each of the classes (bundle or item pricing). Thus  for anonymous pricing:
ˆτH (m) = (m · n)k+1 and for non-anonymous pricing: ˆτH (m) ≤ mn(k+1). Thus the sample
complexity is mH () = O

(cid:16) n(k+1) log(1/)

(cid:17)

.

2

In the case of a single bidder  we know that the best of bundle pricing or item pricing is a 1/8
approximation to the overall best truthful mechanism for the true distribution of values  assuming
values for each item are drawn independently. Thus in the latter case we have:

ES [RD(hS)] ≥ 1
6

sup
h∈H

RD(h) −

where H is the class of all truthful mechanisms.

2(k + 1) log(2m)

m

(29)

(cid:114)

Comparison with [10]. The latter three applications were analyzed by [10]  via the notion of the
pseudo-dimension  but their results lead to sample complexity bounds of O( nk log(nk) log(1/)
). Thus
the above simpler analysis removes the extra log factor on the dependence.

2

7

References
[1] M. F. Balcan  A. Blum  J. D. Hartline  and Y. Mansour. Mechanism design via machine learning.
In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS’05)  pages
605–614  Oct 2005.

[2] Maria-Florina F Balcan  Tuomas Sandholm  and Ellen Vitercik. Sample complexity of au-
tomated mechanism design. In Advances in Neural Information Processing Systems  pages
2083–2091  2016.

[3] Peter L. Bartlett  Olivier Bousquet  and Shahar Mendelson. Local rademacher complexities.

Ann. Statist.  33(4):1497–1537  08 2005.

[4] Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In 46th 

pages 243–252. ACM  2014.

[5] Nikhil R. Devanur  Zhiyi Huang  and Christos-Alexandros Psomas. The sample complexity of
auctions with side information. In Proceedings of the Forty-eighth Annual ACM Symposium on
Theory of Computing  STOC ’16  pages 426–439  New York  NY  USA  2016. ACM.

[6] Yannai A. Gonczarowski and Noam Nisan. Efﬁcient empirical revenue maximization in single-

parameter auction environments. CoRR  abs/1610.09976  2016.

[7] Jason D. Hartline and Tim Roughgarden. Simple versus optimal mechanisms. In Proceedings
of the 10th ACM Conference on Electronic Commerce  EC ’09  pages 225–234  New York  NY 
USA  2009. ACM.

[8] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold

algorithm. Machine learning  2(4):285–318  1988.

[9] Jamie Morgenstern and Tim Roughgarden. The pseudo-dimension of near-optimal auctions. In
Proceedings of the 28th International Conference on Neural Information Processing Systems 
NIPS’15  pages 136–144  Cambridge  MA  USA  2015. MIT Press.

[10] Jamie Morgenstern and Tim Roughgarden. Learning simple auctions. In COLT 2016  2016.

[11] Roger B Myerson. Optimal auction design. Mathematics of operations research  6(1):58–73 

1981.

[12] D. Pollard. Convergence of Stochastic Processes. Springer Series in Statistics. 2011.

[13] Tim Roughgarden and Okke Schrijvers. Ironing in the dark. In Proceedings of the 2016 ACM
Conference on Economics and Computation  EC ’16  pages 1–18  New York  NY  USA  2016.
ACM.

[14] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algo-
rithms. Understanding Machine Learning: From Theory to Algorithms. Cambridge University
Press  2014.

8

,Vasilis Syrgkanis
Ari Morcos
Maithra Raghu
Samy Bengio
Weiyang Liu
Zhen Liu
James Rehg
Le Song