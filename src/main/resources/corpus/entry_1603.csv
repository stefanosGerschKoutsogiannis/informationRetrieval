2017,Diving into the shallows: a computational perspective on large-scale shallow learning,Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been  particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side  shallow methods (such as kernel methods) have  encountered obstacles in scaling to large data  despite excellent performance on smaller datasets  and extensive theoretical analysis. Practical methods  such as variants of gradient descent used so successfully in deep learning  seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow  architecture.   In this paper we  identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic  persisting even in the limit of infinite data.  To address this shortcoming in practice  we introduce EigenPro iteration  a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small  computationally inexpensive and SGD-compatible  amount of approximate second-order information leads to major improvements in convergence. For large data  this leads to a  significant performance  boost over the state-of-the-art kernel methods. In particular  we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.,Diving into the shallows: a computational perspective

on large-scale shallow learning

Siyuan Ma

Mikhail Belkin

Department of Computer Science and Engineering

The Ohio State University

{masi  mbelkin}@cse.ohio-state.edu

Abstract

Remarkable recent success of deep neural networks has not been easy to analyze
theoretically. It has been particularly hard to disentangle relative signiﬁcance of
architecture and optimization in achieving accurate classiﬁcation on large datasets.
On the ﬂip side  shallow methods (such as kernel methods) have encountered
obstacles in scaling to large data  despite excellent performance on smaller datasets 
and extensive theoretical analysis. Practical methods  such as variants of gradient
descent used so successfully in deep learning  seem to perform below par when
applied to kernel methods. This difﬁculty has sometimes been attributed to the
limitations of shallow architecture.
In this paper we identify a basic limitation in gradient descent-based optimization
methods when used in conjunctions with smooth kernels. Our analysis demon-
strates that only a vanishingly small fraction of the function space is reachable
after a polynomial number of gradient descent iterations. That drastically limits
the approximating power of gradient descent leading to over-regularization. The
issue is purely algorithmic  persisting even in the limit of inﬁnite data.
To address this shortcoming in practice  we introduce EigenPro iteration  a simple
and direct preconditioning scheme using a small number of approximately com-
puted eigenvectors. It can also be viewed as learning a kernel optimized for gradient
descent. Injecting this small  computationally inexpensive and SGD-compatible 
amount of approximate second-order information leads to major improvements in
convergence. For large data  this leads to a signiﬁcant performance boost over the
state-of-the-art kernel methods. In particular  we are able to match or improve the
results reported in the literature at a small fraction of their computational budget.
For complete version of this paper see https://arxiv.org/abs/1703.10622.

Introduction

1
In recent years we have witnessed remarkable advances in many areas of artiﬁcial intelligence. Much
of this progress has been due to machine learning methods  notably deep neural networks  applied
to very large datasets. These networks are typically trained using variants of stochastic gradient
descent (SGD)  allowing training on large data with modern GPU hardware. Despite intense recent
research and signiﬁcant progress on SGD and deep architectures  it has not been easy to understand
the underlying causes of that success. Broadly speaking  it can be attributed to (a) the structure of the
function space represented by the network or (b) the properties of the optimization algorithms used.
While these two aspects of learning are intertwined  they are distinct and may be disentangled.
As learning in deep neural networks is still largely resistant to theoretical analysis  progress can
be made by exploring the limits of shallow methods on large datasets. Shallow methods  such as
kernel methods  are a subject of an extensive and diverse literature  both theoretical and practical.
In particular  kernel machines are universal learners  capable of learning nearly arbitrary functions
given a sufﬁcient number of examples [STC04  SC08]. Still  while kernel methods are easily
implementable and show state-of-the-art performance on smaller datasets (see [CK11  HAS+14 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

DXH+14  LML+14  MGL+17] for some comparisons with DNN’s) there has been signiﬁcantly less
progress in applying these methods to large modern data. The goal of this work is to make a step
toward understanding the subtle interplay between architecture and optimization and to take practical
steps to improve performance of kernel methods on large data.
The paper consists of two main parts. First  we identify a basic underlying limitation in using gradient
descent-based methods in conjunction with smooth (inﬁnitely differentiable) kernels typically used in
machine learning  showing that only very smooth functions can be approximated after polynomially
many steps of gradient descent. This phenomenon is a result of fast spectral decay of smooth kernels
and can be readily understood in terms of the spectral structure of the gradient descent operator in the
least square regression/classiﬁcation setting  which is the focus of our discussion. Slow convergence
leads to severe over-regularization (over-smoothing) and suboptimal approximation for less smooth
functions  which are arguably very common in practice  at least in the classiﬁcation setting  where we
expect fast transitions near the class boundaries.
This shortcoming of gradient descent is purely algorithmic and is not related to the sample complexity
of the data. It is also not an intrinsic ﬂaw of the kernel architecture  which is capable of approximating
arbitrary functions but potentially requiring a very large number of gradient descent steps. The issue
is particularly serious for large data  where direct second order methods cannot be used due to the
computational constraints. While many approximate second-order methods are available  they rely on
low-rank approximations and  as we discuss below  lead to over-regularization (approximation bias).
In the second part of the paper we propose EigenPro iteration (see http://www.github.com/EigenPro
for the code)  a direct and simple method to alleviate slow convergence resulting from fast eigen-decay
for kernel (and covariance) matrices. EigenPro is a preconditioning scheme based on approximately
computing a small number of top eigenvectors to modify the spectrum of these matrices. It can also
be viewed as constructing a new kernel  speciﬁcally optimized for gradient descent. While EigenPro
uses approximate second-order information  it is only employed to modify ﬁrst-order gradient descent 
leading to the same mathematical solution as gradient descent (without introducing a bias). EigenPro
is also fully compatible with SGD  using a low-rank preconditioner with a low overhead per iteration.
We analyze the step size in the SGD setting and provide a range of experimental results for different
kernels and parameter settings showing ﬁve to 30-fold acceleration over the standard methods  such
as Pegasos [SSSSC11]. For large data  when the computational budget is limited  that acceleration
translates into signiﬁcantly improved accuracy. In particular  we are able to improve or match the
state-of-the-art results reported for large datasets in the kernel literature with only a small fraction of
their computational budget.
2 Gradient descent for shallow methods
Shallow methods. In the context of this paper  shallow methods denote the family of algorithms
consisting of a (linear or non-linear) feature map φ : RN → H to a (ﬁnite or inﬁnite-dimensional)
Hilbert space H followed by a linear regression/classiﬁcation algorithm. This is a simple yet powerful
setting amenable to theoretical analysis. In particular  it includes the class of kernel methods  where
H is a Reproducing Kernel Hilbert Space (RKHS).
Linear regression. Consider n labeled data points {(x1  y1)  ...  (xn  yn) ∈ H × R}. To simplify
(cid:80)n
the notation let us assume that the feature map has already been applied to the data  i.e.  xi = φ(zi).
Least square linear regression aims to recover the parameter vector α∗ that minimize the empirical
i=1((cid:104)α  xi(cid:105)H − yi)2. When α∗ is not
loss such that α∗ = arg minα∈H L(α) where L(α)
uniquely deﬁned  we can choose the smallest norm solution.
Minimizing the empirical loss is related to solving a linear system of equations. Deﬁne the data
def
matrix X
= (y1  ...  yn)T   as well as the (non-centralized)
covariance matrix/operator  H
2. Since
i . Rewrite the loss as L(α) = 1
∇L(α) |α=α∗ = 0  minimizing L(α) is equivalent to solving the linear system
(1)
with b = X T y. When d = dim(H) < ∞  the time complexity of solving the linear system in Eq. 1
directly (using Gaussian elimination or other methods typically employed in practice) is O(d3). For
kernel methods we frequently have d = ∞. Instead of solving Eq. 1  one solves the dual n× n system
Kα − y = 0 where K
def
= [k(zi  zj)]i j=1 ... n is the kernel matrix . The solution can be written as

(cid:80)n
i=1 k(zi ·)α(zi). A direct solution would require O(n3) operations.

def
= (x1  ...  xn)T and the label vector y

n (cid:107)Xα − y(cid:107)2

def
= 1
n

(cid:80)n

def
= 1
n

i=1 xixT

Hα − b = 0

2

def

def

a2

Gradient descent (GD). While linear systems of equations can be solved by direct methods  such as
Gaussian elimination  their computational demands make them impractical for large data. Gradient
descent-type methods potentially require a small number of O(n2) matrix-vector multiplications 
a much more manageable task. Moreover  these methods can typically be used in a stochastic
setting  reducing computational requirements and allowing for efﬁcient GPU implementations.
These schemes are adopted in popular kernel methods implementations such as NORMA [KSW04] 
SDCA [HCL+08]  Pegasos [SSSSC11]  and DSGD [DXH+14]. For linear systems of equations
gradient descent takes a simple form known as the Richardson iteration [Ric11]. It is given by

α(t+1) = α(t) − η(Hα(t) − b)

(2)
It is easy to see that for convergence of αt to α∗ as t → ∞ we need to ensure that (cid:107)I − ηH(cid:107) < 1 
and hence 0 < η < 2/λ1(H). The explicit formula is

α(t+1) − α∗ = (I − ηH)t(α(1) − α∗)

(3)
We can now describe the computational reach of gradient descent CRt  i.e. the set of vectors which
can be -approximated by gradient descent after t steps  CRt()
= {v ∈ H  s.t.(cid:107)(I − ηH)tv(cid:107) <
(cid:107)v(cid:107)}. It is important to note that any α∗ /∈ CRt() cannot be -approximated by gradient descent in
less than t + 1 iterations. Note that we typically care about the quality of the solution (cid:107)Hα(t) − b(cid:107) 
rather than the error estimating the parameter vector (cid:107)α(t) − α∗(cid:107) which is reﬂected in the deﬁnition.
We will assume that the initialization α(1) = 0. Choosing a different starting point does not change
the analysis unless second order information is incorporated in the initialization conditions.
To get a better idea of the space CRt() consider the eigendecomposition of H. Let λ1 ≥
λ2 ≥ . . . be its eigenvalues and e1  e2  . . . the corresponding eigenvectors/eigenfunctions.
i . Writing Eq. 3 in terms of eigendirection yields α(t+1) −
= (cid:104)ei  v(cid:105) gives CRt() =
i < 2 (cid:107)v(cid:107)2}. Recalling that η < 2/λ1 and using the fact that (1 − 1/z)z ≈
1/e  we see that a necessary condition for v ∈ CRt is 1
i (1 − ηλi)2ta2
i <
2 (cid:107)v(cid:107)2. This is a convenient characterization  we will denote CR(cid:48)
a2
i <
2 (cid:107)v(cid:107)2} ⊃ CRt(). Another convenient but less precise necessary condition for v ∈ CRt is that

We have H = (cid:80) λieieT
α∗ = (cid:80) (1 − ηλi)t(cid:104)ei  α(1) − α∗(cid:105)ei.
{v  s.t.(cid:80) (1 − ηλi)2ta2
(cid:12)(cid:12)(cid:12)(1 − 2λi/λ1)t (cid:104)ei  v(cid:105)(cid:12)(cid:12)(cid:12) < (cid:107)v(cid:107). Noting that log(1 − x) < −x and assuming λ1 > 2λi  we have

i <(cid:80)
= {v  s.t.(cid:80)

Hence putting ai

i s.t.λi< λ1
2t
def

i s.t.λi< λ1
2t

t()

(cid:80)

3

t > λ1(2λi)−1 log

(4)
The condition number. We are primarily interested in the case when d is inﬁnite or very large
and the corresponding operators/matrices are extremely ill-conditioned with inﬁnite or approaching
inﬁnity condition number. In that case instead of a single condition number  one should consider the
properties of eigenvalue decay.
Gradient descent  smoothness and kernel methods. We now proceed to analyze the computational
reach for kernel methods. We will start by discussing the case of inﬁnite data (the population case).
It is both easier to analyze and allows us to demonstrate the purely computational (non-statistical)
nature of limitations of gradient descent. We will see that when the kernel is smooth  the reach of
gradient descent is limited to very smooth  at least inﬁnitely differentiable functions. Moreover  to
approximate a function with less smoothness within some accuracy  in the L2 norm one needs a
super-polynomial (or even exponential) in 1/ number of iterations of gradient descent. Let the data
be sampled from a probability with a smooth density µ on a compact domain Ω ⊂ Rp. In the case of
inﬁnite data H becomes an integral operator corresponding to a positive deﬁnite kernel k(· ·) such
that Kf (x)
Ω k(x  z)f (z)dµz. This is a compact self-adjoint operator with an inﬁnite positive
spectrum λ1  λ2  . . .  limi→∞ λi = 0. We have (see the full paper for discussion and references):
Theorem 1. If k is an inﬁnitely differentiable kernel  the rate of eigenvalue decay is super-polynomial 
∀P ∈ N. Moreover  if k is the Gaussian kernel  there exist constants C  C(cid:48) > 0
i.e. λi = O(i−P )

such that for large enough i  λi < C(cid:48) exp(cid:0)−Ci1/p(cid:1).
which form an orthonormal basis for L2(Ω). We can write a function f ∈ L2(Ω) as f =(cid:80)∞

The computational reach of kernel methods. Consider the eigenfunctions of K  Kei = λiei 
i=1 aiei.
We have (cid:107)f(cid:107)2
i . We can now describe the reach of kernel methods with smooth kernel
(in the inﬁnite data setting). Speciﬁcally  functions which can be approximated in a polynomial
number of iterations must have super-polynomial coefﬁcient decay.

L2 =(cid:80)∞

=(cid:82)

i=1 a2

def

(cid:16)|(cid:104)ei  v(cid:105)|−1 (cid:107)v(cid:107)−1(cid:17)

3

(cid:17)

√

1

j=1 3 ...

i>

2 ln 2t

s

exp

2πs

(cid:80)

(cid:16)− (x−z)2

4s

Fourier series f = (cid:80)∞
(cid:80)

√

Theorem 2. Suppose f ∈ L2(Ω) is such that it can be approximated within  using a polynomial
in 1/ number of gradient descent iterations  i.e. ∀>0f ∈ CR−M () for some M ∈ N. Then any
N ∈ N and i large enough |ai| < i−N .
Corollary 1. Any f ∈ L2(Ω) which can be -approximated with polynomial in 1/ number of steps
of gradient descent is inﬁnitely differentiable. In particular  f function must belong to the intersection
of all Sobolev spaces on Ω.
Gradient descent for periodic functions on R. Let us now consider a simple but important
special case  where the reach can be analyzed very explicitly. Let Ω be a circle with the uni-
form measure  or  equivalently  consider periodic functions on the interval [0  2π]. Let ks(x  z)
be the heat kernel on the circle [Ros97]. This kernel is very close to the Gaussian kernel
ks(x  z) ≈ 1√
. The eigenfunctions ej of the integral operator K correspond-
ing to ks(x  z) are simply the Fourier harmonics sin jx and cos jx. The corresponding eigenvalues
are {1  e−s  e−s  e−4s  e−4s  . . .   e−(cid:98)j/2+1(cid:99)2s  . . .}. Given a function f on [0  2π]  we can write its
j=0 ajej. A direct computation shows that for any f ∈ CRt()  we have
2 ln 2ts grows
extremely slowly as the number of iterations t increases. As a simple example consider the Heaviside
step function f (x) (on a circle)  taking 1 and −1 values for x ∈ (0  π] and x ∈ (π  2π]  respectively.
j sin(jx). From the analysis above  we
The step function can be written as f (x) = 4
π
need O(exp( s
2 )) iterations of gradient descent to obtain an -approximation to the function. It
is important to note that the Heaviside step function is a rather natural example  especially in the
classiﬁcation setting  where it represents the simplest two-class classiﬁcation problem. The situation
is not much better for functions with more smoothness unless they happen to be extremely smooth
with super-exponential Fourier component decay. In contrast  a direct computation of inner products
(cid:104)f  ei(cid:105) yields exact function recovery for any function in L2([0  2π]) using the amount of computation
equivalent to just one step of gradient descent. Thus  we see that the gradient descent is an extremely
inefﬁcient way to recover Fourier series for a general periodic function. The situation is only mildly

improved in dimension d  where the span of at most O∗(cid:0)(log t)d/2(cid:1) eigenfunctions of a Gaussian
kernel or O(cid:0)t1/p(cid:1) eigenfunctions of an arbitrary p-differentiable kernel can be approximated in t

i < 32 (cid:107)v(cid:107)2. We see that the space f ∈ CRt() is “frozen" as
a2

iterations. The discussion above shows that the gradient descent with a smooth kernel can be viewed
as a heavy regularization of the target function. It is essentially a band-limited approximation no more
than O(ln t) Fourier harmonics. While regularization is often desirable from a generalization/ﬁnite
sample point of view   especially when the number of data points is small  the bias resulting from the
application of the gradient descent algorithm cannot be overcome in a realistic number of iterations
unless target functions are extremely smooth or the kernel itself is not inﬁnitely differentiable.
Remark: Rate of convergence vs statistical ﬁt. Note that we can improve convergence by changing
the shape parameter of the kernel  i.e. making it more “peaked” (e.g.  decreasing the bandwidth s
in the deﬁnition of the Gaussian kernel) While that does not change the exponential nature of the
asymptotics of the eigenvalues  it slows their decay. Unfortunately improved convergence comes at
the price of overﬁtting. In particular  for ﬁnite data  using a very narrow Gaussian kernel results in an
approximation to the 1-NN classiﬁer  a suboptimal method which is up to a factor of two inferior to
the Bayes optimal classiﬁer in the binary classiﬁcation case asymptotically.
Finite sample effects  regularization and early stopping. It is well known (e.g.  [B+05  RBV10])
that the top eigenvalues of kernel matrices approximate the eigenvalues of the underlying integral
operators. Therefore computational obstructions encountered in the inﬁnite case persist whenever the
data set is large enough. Note that for a kernel method  t iterations of gradient descent for n data
points require t · n2 operations. Thus  gradient descent is computationally pointless unless t (cid:28) n.
That would allow us to ﬁt only about O(log t) eigenvectors. In practice we need t to be much smaller
than n  say  t < 1000. At this point we should contrast our conclusions with the important analysis of
early stopping for gradient descent provided in [YRC07] (see also [RWY14  CARR16]). The authors
analyze gradient descent for kernel methods obtaining the optimal number of iterations of the form
t = nθ  θ ∈ (0  1). That seems to contradict our conclusion that a very large  potentially exponential 
number of iterations may be needed to guarantee convergence. The apparent contradiction stems from
the assumption in [YRC07] that the regression function f∗ belongs to the range of some power of
the kernel operator K. For an inﬁnitely differentiable kernel  that implies super-polynomial spectral
i ) for any N > 0). In particular  it implies that f∗ belongs to any Sobolev space.
decay (ai = O(λN
We do not typically expect such high degree of smoothness in practice  particularly in classiﬁcation
problems  where the Heaviside step function seems to be a reasonable model. In particular  we expect

4

1

L2 loss

Dataset

Metric

sharp transitions of label probabilities across class boundaries to be typical for many classiﬁcations
datasets. These areas of near-discontinuity will necessarily result in slow decay of Fourier coefﬁcients
and require many iterations of gradient descent to approximate1.
To illustrate this point  we show (right ta-
ble) the results of gradient descent for two
datasets of 10000 points (see Section 6).
The regression error on the training set is
roughly inverse to the number of iterations 
i.e. every extra bit of precision requires
twice the number of iterations for the previous bit. For comparison  we see that the minimum
regression (L2) error on both test sets is achieved at over 10000 iterations. This results is at least
cubic computational complexity equivalent to that of a direct method.
Regularization. Note that typical regularization  e.g.  adding λ(cid:107)f(cid:107)  results in discarding information
along the directions with small eigenvalues (below λ). While this improves the condition number it
comes at a high cost in terms of over-regularization. In the Fourier analysis example this is similar to

considering band-limited functions with ∼(cid:112)log(1/λ)/s Fourier components. Even for λ = 10−16

81920
1280
2.17e-5
2.60e-2
3.55e-2
4.59e-2
3.26% 2.39% 2.49%
4.21e-3
3.08e-2
3.34e-2
3.42e-2

9.61e-2
4.07e-1
4.07e-1
9.74e-2
38.50% 7.60%
4.58e-2
8.25e-2
7.98e-2
4.24e-2

train
L2 loss
test
c-error (test)
train
test

Number of iterations
80

10240
2.36e-3
3.64e-2

1.83e-2
3.14e-2

HINT-M-10k

MNIST-10k

(limit of double precision) and s = 1 we can only ﬁt about 10 Fourier components. We argue that
there is little need for explicit regularization for most iterative methods in the big data regimes.
3 Extending the reach of gradient descent: EigenPro iteration
We will now propose practical measures to alleviate the over-regularization of linear regression by
gradient descent. As seen above  one of the key shortcomings of shallow learning methods based on
smooth kernels (and their approximations  e.g.  Fourier and RBF features) is their fast spectral decay.
That suggests modifying the corresponding matrix H by decreasing its top eigenvalues  enabling
the algorithm to approximate more target functions in the same number of iterations. Moreover 
this can be done in a way compatible with stochastic gradient descent thus obviating the need to
materialize full covariance/kernel matrices in memory. Accurate approximation of top eigenvectors
can be obtained from a subsample of the data with modest computational expenditure. Combining
these observations we propose EigenPro  a low overhead preconditioned Richardson iteration.
Preconditioned (stochastic) gradient descent. We will modify the linear system in Eq. 1 with an
invertible matrix P   called a left preconditioner. P Hα − P b = 0. Clearly  this modiﬁed system and
the original system in Eq. 1 have the same solution. The Richardson iteration corresponding to the
modiﬁed system (preconditioned Richardson iteration) is

(5)
It is easy to see that as long as η(cid:107)P H(cid:107) < 1 it converges to α∗  the solution of the original linear
system. Preconditioned SGD can be deﬁned similarly by

α(t+1) = α(t) − ηP (Hα(t) − b)

α ← α − ηP (Hmα − bm)

(6)

def
= 1

def
= 1

m X T

m X T

def
= P 1

mXm and bm

mym using sampled mini-batch (Xm  ym).

where we deﬁne Hm
Preconditioning as a linear feature map. It is easy to see that the preconditioned iteration is in
fact equivalent to the standard Richardson iteration in Eq. 2 on a dataset transformed with the linear
feature map  φP (x)
2 x. This is a convenient point of view as the transformed data can be stored
for future use. It also shows that preconditioning is compatible with most computational methods
both in practice and  potentially  in terms of analysis.
Linear EigenPro. We will now discuss properties desired to make preconditioned GD/SGD meth-
ods effective on large scale problems. Thus for the modiﬁed iteration in Eq. 5 we would like
to choose P to meet the following targets: (Acceleration) The algorithm should provide high ac-
curacy in a small number of iterations.
(Initial cost) The preconditioning matrix P should be
accurately computable  without materializing the full covariance matrix. (Cost per iteration) Pre-
conditioning by P should be efﬁcient per iteration in terms of computation and memory. The
convergence of the preconditioned algorithm with the along the i-th eigendirection is dependent
on the ratio of eigenvalues λi(P H)/λ1(P H). This leads us to choose the preconditioner P to
maximize the ratio λi(P H)/λ1(P H) for each i. We see that modifying the top eigenvalues of
H makes the most difference in convergence. For example  decreasing λ1 improves convergence
along all directions  while decreasing any other eigenvalue only speeds up convergence in that

1Interestingly they can lead to lower sample complexity for optimal classiﬁers (cf. Tsybakov margin

condition [Tsy04]).

5

= I − k(cid:88)

def

P

i=1

direction. However  decreasing λ1 below λ2 does not help unless λ2 is decreased as well. Therefore
it is natural to decrease the top k eigenvalues to the maximum amount  i.e. to λk+1  leading to

(1 − λk+1/λi)eieT

i

(7)

Algorithm: EigenPro(X  y  k  m  η  τ  M )
input training data (X  y)  number of eigen-
directions k  mini-batch size m  step size η 
damping factor τ  subsample size M

def

def

(Xm  ym) ← m rows sampled from (X  y)
without replacement
g ← 1
α ← α − ηP g

m(Xmα) − X T

m (X T

mym)

6:
7:
8: end while

(cid:80)k
i=1 (1 − τ ˆλk+1/ˆλi)ˆeiˆeT

output weight of the linear model α
1: [E  Λ  ˆλk+1] = RSVD(X  k + 1  M )
= I − E(I − τ ˆλk+1Λ−1)ET
2: P
3: Initialize α ← 0
4: while stopping criteria is False do
5:

We see that P -preconditioned iteration increases
convergence by a factor λ1/λk. However  exact
construction of P involves computing the eigen-
decomposition of the d × d matrix H  which
is not feasible for large data. Instead we use
subsampled randomized SVD [HMT11] to ob-
= I −
tain an approximate preconditioner ˆPτ
i . Here algorithm
RSVD (detailed in the full paper ) computes the
approximate top eigenvectors E ← (ˆe1  . . .   ˆek)
and eigenvalues Λ ← diag(ˆλ1  . . .   ˆλk) and
ˆλk+1 for subsample covariance matrix HM . We introduce the parameter τ to counter the effect of
approximate top eigenvectors “spilling” into the span of the remaining eigensystem. Using τ < 1 is
preferable to the obvious alternative of decreasing the step size η as it does not decrease the step size
in the directions nearly orthogonal to the span of (ˆe1  . . .   ˆek). That allows the iteration to converge
faster in those directions. In particular  when (ˆe1  . . .   ˆek) are computed exactly  the step size in
other eigendirections will not be affected by the choice of τ. We call SGD with the preconditioner
ˆPτ (Eq. 6) EigenPro iteration. See Algorithm EigenPro for details. Moreover  the key step size
parameter η can be selected in a theoretically sound way discussed below.
Kernel EigenPro. We will now discuss modiﬁcations needed to work directly in the RKHS (primal)
(cid:80)n
setting. A positive deﬁnite kernel k(· ·) : RN × RN → R implies a feature map from X to an
RKHS space H. The feature map can be written as φ : x (cid:55)→ k(x ·)  RN → H. This feature map
(cid:80)n
i=1 ((cid:104)f  k(xi ·)(cid:105)H − yi)2. Using properties of
leads to the learning problem f∗ = arg minf∈H 1
(cid:80)n
RKHS  EigenPro iteration in H becomes f ← f − η P(K(f ) − b) where b
i=1 yik(xi ·)
and covariance operator K = 1
i=1 k(xi ·) ⊗ k(xi ·). The top eigensystem of K forms the
rem [Aro50]  f∗ admits a representation of the form(cid:80)n
i=1 (1 − τ λk+1(K)/λi(K)) ei(K) ⊗ ei(K). By the Representer theo-
preconditioner P
i=1 αi k(xi ·). Parameterizing the above
iteration accordingly and applying some linear algebra lead to the following iteration in a ﬁnite-
dimensional vector space  α ← α−ηP (Kα−y) where K
= I −(cid:80)k
def
= [k(xi  xj)]i j=1 ... n is the kernel matrix
and EigenPro preconditioner P is deﬁned using the top eigensystem of K (assume Kei = λiei) 
i . This differs from that for the linear case (Eq. 7) (with an
P
extra factor of 1/λi) due to the difference between the parameter space of α and the RKHS space.
EigenPro as kernel learning. Another way to view EigenPro is in terms of kernel learning. Assum-
ing that the preconditioner is computed exactly  EigenPro is equivalent to computing the (distribution-
dependent) kernel  kEP (x  z)
i=k+1 λiei(x)ei(z). Notice that the
RKHS spaces corresponding to kEP and k contain the same functions but have different norms. The
norm in kEP is a ﬁnite rank modiﬁcation of the norm in the RKHS corresponding to k  a setting
reminiscent of [SNB05] where unlabeled data was used to “warp” the norm for semi-supervised
learning. However  in our paper the “warping" is purely for computational efﬁciency.
Acceleration. EigenPro can obtain acceleration factor of up to λ1
over the standard gradient
λk+1
descent. That factor assumes full gradient descent and exact computation of the preconditioner. See
below for an acceleration analysis in the SGD setting.
Initial cost. To construct the preconditioner P   we perform RSVD to compute the approximate top
eigensystem of covariance H. RSVD has time complexity O(M d log k +(M +d)k2) (see [HMT11]).
The subsample size M can be much smaller than the data size n while preserving the accuracy of
estimation. In addition  extra kd memory is needed to store the eigenvectors.
Cost per iteration. For standard SGD using d kernel centers (or random Fourier features) and
mini-batch of size m  the computational cost per iteration is O(md). In comparison  EigenPro
iteration using top-k eigen-directions costs O(md + kd). Speciﬁcally  applying preconditioner P in
EigenPro requires left multiplication by a matrix of rank k. This involves k vector-vector dot products
resulting in k · d additional operations per iteration. These can be implemented efﬁciently on a GPU.

i=1 λk+1ei(x)ei(z) +(cid:80)∞

−1(1 − τ λk+1/λi)eieT

= I−(cid:80)k

def

= (cid:80)k

def

i=1 λi

def
= 1
n

def

n

n

6

4 Step Size Selection for EigenPro Preconditioned Methods
We will now discuss the key issue of the step size selection for EigenPro iteration. For iteration
−1 = (cid:107)H(cid:107)−1 results in optimal (within a factor of 2) con-
involving covariance matrix H  λ1(H)
vergence. This suggests choosing the corresponding step size η = (cid:107)P H(cid:107)−1 = λ−1
k+1. In practice
this will lead to divergence due to (1) approximate computation of eigenvectors (2) the randomness
inherent in SGD. One (costly) possibility is to compute (cid:107)P Hm(cid:107) at every step. As the mini-batch
can be assumed to be chosen at random  we propose using a lower bound on (cid:107)Hm(cid:107)−1 (with high
probability) as the step size to guarantee convergence at each iteration.
Linear EigenPro. Consider the EigenPro preconditioned SGD in Eq. 6. For this analysis assume
that P is formed by the exact eigenvectors.Interpreting P 1
2 as a linear feature map as in Section 2 
makes P 1
2 a random subsample on the dataset XP 1
2 . Using matrix Bernstein [Tro15] yields
2 ≤ κ for any x ∈ X and λk+1 = λk+1(H)  with probability at least 1 − δ 
Theorem 3. If (cid:107)x(cid:107)2
Kernel EigenPro. For EigenPro iteration in RKHS  we can bound (cid:107)P◦Km(cid:107) with a very similar
result based on operator Bernstein [Min17]. Note that dimension d in Theorem 3 is replaced by the
intrinsic dimension [Tro15]. See the arXiv version of this paper for details.
Choice of the step size. In the spectral norm bounds λk+1 is the dominant term when the mini-batch

(cid:107)P Hm(cid:107) ≤ λk+1 + 2(λk+1 + κ)(3m)−1(ln 2dδ−1) +(cid:112)2λk+1κm−1(ln 2dδ−1).
size m is large. However  in most large-scale settings  m is small  and(cid:112)2λk+1κ/m becomes the
dominant term. This suggests choosing step size η ∼ 1/(cid:112)λk+1 leading to acceleration on the order
of λ1/(cid:112)λk+1 over the standard (unpreconditioned) SGD. That choice works well in practice.

2 HmP 1

5 EigenPro and Related Work
Large scale machine learning imposes fairly speciﬁc limitations on optimization methods. The
computational budget allocated to the problem must not exceed O(n2) operations  a small number of
matrix-vector multiplications. That rules out most direct second order methods which require O(n3)
operations. Approximate second order methods are far more efﬁcient. However  they typically rely
on low rank matrix approximation  a strategy which (similarly to regularization) in conjunction with
smooth kernels discards information along important eigen-directions with small eigenvalues. On the
other hand  ﬁrst order methods can be slow to converge along eigenvectors with small eigenvalues.
An effective method must thus be a hybrid approach using approximate second order information in a
ﬁrst order method. EigenPro is an example of such an approach as the second order information is
used in conjunction with a ﬁrst order method. The things that make EigenPro effective are as follows:
1. The second order information (eigenvalues and eigenvectors) is computed efﬁciently from a
subsample of the data. Due to the quadratic loss function  that computation needs to be conducted
only once. Moreover  the step size can be ﬁxed throughout the iterations.
2. Preconditioning by a low rank modiﬁcation of the identity matrix results in low overhead per
iteration. The update is computed without materializing the full preconditioned covariance matrix.
3. EigenPro iteration converges (mathematically) to the same result even if the second order
approximation is not accurate. That makes EigenPro relatively robust to errors in the second order
preconditioning term P   in contrast to most approximate second order methods.
Related work: First order optimization methods. Gradient based methods  such as gradient de-
scent (GD)  stochastic gradient descent (SGD)  are classical methods [She94  DJS96  BV04  Bis06].
Recent success of neural networks had drawn signiﬁcant attention to improving and accelerating
these methods. Methods like SAGA [RSB12] and SVRG [JZ13] improve stochastic gradient by peri-
odically evaluating full gradient to achieve variance reduction. Algorithms in [DHS11  TH12  KB14]
compute adaptive step size for each gradient coordinate.
Scalable kernel methods. There is a signiﬁcant literature on scalable kernel methods includ-
ing [KSW04  HCL+08  SSSSC11  TBRS13  DXH+14] Most of these are ﬁrst order optimization
methods. To avoid the O(n2) computation and memory requirement typically involved in construct-
ing the kernel matrix  they often adopt approximations like RBF features [WS01  QB16  TRVR16]
or random Fourier features [RR07  LSS13  DXH+14  TRVR16].
Second order/hybrid optimization methods. Second order methods use the inverse of the Hessian
matrix or its approximation to accelerate convergence [SYG07  BBG09  MNJ16  BHNS16  ABH16].
These methods often need to compute the full gradient every iteration [LN89  EM15  ABH16]
making less suitable for large data. [EM15] analyzed a hybrid ﬁrst/second order method for general
convex optimization with a rescaling term based on the top eigenvectors of the Hessian. That can
be viewed as preconditioning the Hessian at every GD iteration. A related recent work [GOSS16]

7

analyses a hybrid method designed to accelerate SGD convergence for ridge regression. The data are
preprocessed by rescaling points along the top singular vectors of the data matrix. Another second
order method PCG [ACW16] accelerates the convergence of conjugate gradient for large kernel ridge
regression using a preconditioner which is the inverse of an approximate covariance generated with
random Fourier features. [TRVR16] achieves similar preconditioning effects by solving a linear
system involving a subsampled kernel matrix every iteration. While not strictly a preconditioner
Nyström with gradient descent(NYTRO) [CARR16] also improves the condition number. Compared
to many of these methods EigenPro directly addresses the underlying issues of slow convergence
without introducing a bias in directions with small eigenvalues. Additionally EigenPro incurs only a
small overhead per iteration both in memory and computation.
6 Experimental Results
Computing Resource/Data/Metrics. Experiments were run on a workstation with 128GB main
memory  two Intel Xeon(R) E5-2620 CPUs  and one GTX Titan X (Maxwell) GPU. For multiclass
datasets  we report classiﬁcation error (c-error) for binary valued labels and mean squared error
(mse) for real valued labels. See the arXiv version for details and more experimental results.
Kernel methods/Hyperparameters. For smaller datasets direct solution of kernel regularized least
squares (KRLS) is used to obtain the reference error. We compare with the primal method Pe-
gasos [SSSSC11]. For even larger datasets  we use Random Fourier Features [RR07] (RF) with
SGD as in [DXH+14  TRVR16]. The results of these methods are presented as baselines. For
consistent comparison  all iterative methods use mini-batch of size m = 256. EigenPro pre-
conditioner is constructed using the top k = 160 eigenvectors of a subsampled dataset of size
M = 4800. For EigenPro-RF  we set the damping factor τ = 1/4. For primal EigenPro τ = 1.
Acceleration for different kernels. The
table on the right presents the number of
epochs needed by EigenPro and Pegasos to
reach the error of the optimal kernel clas-
siﬁer. We see that EigenPro provides ac-
celeration of 6 to 35 times in terms of the
number of epochs required without any loss of accuracy. The actual acceleration is about 20% less
due to the overhead of maintaining and applying a preconditioner.
Comparisons on large datasets. Table below compares EigenPro to Pegasos/SGD-RF on several
large datasets for 10 epochs. We see that EigenPro consistently outperforms Pegasos/SGD-RF within
a ﬁxed computational budget. Note that we adopt Gaussian kernel and 2 · 105 random features.
SGD-RF

Dataset
MNIST
CIFAR-10

Size
6 · 104
5 · 104
7 · 104
5 · 104

Pega
77
56
54
164

Pega
143
136
297
308

Pega
78
107
191
126

SVHN
HINT-S

EigenPro-RF

EigenPro

4
13
14
15

7
6
17
13

7
5
8
19

Gaussian

Pegasos

Laplace

EigPro

Cauchy

EigPro

EigPro

Size Metric
Dataset
2 · 105
HINT-S
1 · 106
TIMIT
MNIST-8M 1 · 106
8 · 106
1 · 106
HINT-M
7 · 106

c-error

mse

result
10.0%
31.7%
0.8%

2.3e-2

-

-

GPU hours

0.1
3.2
3.0

result GPU hours
11.7%
33.0%
1.1%

0.1
2.2
2.7

1.9

2.7e-2

1.5

-

-

result GPU hours
10.3%
32.6%
0.8%
0.7%
2.4e-2
2.1e-2

0.2
1.5
0.8
7.2
0.8
5.8

result GPU hours
11.5%
33.3%
1.0%
0.8%
2.7e-2
2.4e-2

0.1
1.0
0.7
6.0
0.6
4.1

Comparisons to state-of-the-art. In the below table  we provide a comparison to several large scale
kernel results reported in the literature. EigenPro improves or matches performance on each dataset at
a much lower computational budget. We note that [MGL+17] achieves error 30.9% on TIMIT using
an AWS cluster. The method uses a novel supervised feature selection method  hence is not directly
comparable. EigenPro can plausibly further improve the training error using this new feature set.

Dataset

MNIST

Size
1 · 106
6.7 · 106
2 · 106
4 · 106

GPU hours

EigenPro (use 1 GTX Titan X)
error
epochs
0.70%
0.80%†
31.7%
(32.5%)‡
19.8%

16
10
10
0.6

4.8
0.8
3.2
0.1

Reported results

source

[ACW16]
[LML+14]
[HAS+14]
[TRVR16]
[CAS16]

description

error
0.72% 1.1 hours/189 epochs/1344 AWS vCPUs
0.85%
33.5%
33.5%
≈ 20%

less than 37.5 hours on 1 Tesla K20m

512 IBM BlueGene/Q cores

7.5 hours on 1024 AWS vCPUs

TIMIT
SUSY
† The result is produced by EigenPro-RF using 1 × 106 data points.
‡ Our TIMIT training set (1 × 106 data points) was generated
following a standard practice in the speech community [PGB+11] by taking 10ms frames and dropping the glottal stop ’q’ labeled frames in
core test set (1.2% of total test set). [HAS+14] adopts 5ms frames  resulting in 2 × 106 data points  and keeping the glottal stop ’q’. In the
worst case scenario EigenPro  if we mislabel all glottal stops  the corresponding frame-level error increases from 31.7% to 32.5%.
Acknowledgements. We thank Adam Stiff  Eric Fosler-Lussier  Jitong Chen  and Deliang Wang
for providing TIMIT and HINT datasets. This work is supported by NSF IIS-1550757 and NSF
CCF-1422830. Part of this work was completed while the second author was at the Simons Institute
at Berkeley. In particular  he thanks Suvrit Sra  Daniel Hsu  Peter Bartlett  and Stefanie Jegelka for
many discussions and helpful suggestions.

0.6 hours on IBM POWER8

8

References
[ABH16] Naman Agarwal  Brian Bullins  and Elad Hazan. Second order stochastic optimization in linear

time. arXiv preprint arXiv:1602.03943  2016.

[ACW16] H. Avron  K. Clarkson  and D. Woodruff. Faster kernel ridge regression using sketching and

preconditioning. arXiv preprint arXiv:1611.03220  2016.

[Aro50] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical

society  68(3):337–404  1950.

[B+05] Mikio Ludwig Braun et al. Spectral properties of the kernel matrix and their relation to kernel

methods in machine learning. PhD thesis  University of Bonn  2005.

[BBG09] Antoine Bordes  Léon Bottou  and Patrick Gallinari. SGD-QN: Careful quasi-newton stochastic

gradient descent. JMLR  10:1737–1754  2009.

[BHNS16] Richard H Byrd  SL Hansen  Jorge Nocedal  and Yoram Singer. A stochastic quasi-newton method

for large-scale optimization. SIAM Journal on Optimization  26(2):1008–1031  2016.

[Bis06] Christopher M Bishop. Pattern recognition. Machine Learning  128  2006.
[BV04] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press  2004.
[CARR16] Raffaello Camoriano  Tomás Angles  Alessandro Rudi  and Lorenzo Rosasco. NYTRO: When

subsampling meets early stopping. In AISTATS  pages 1403–1411  2016.

[CAS16] Jie Chen  Haim Avron  and Vikas Sindhwani. Hierarchically compositional kernels for scalable

nonparametric learning. arXiv preprint arXiv:1608.00860  2016.

[CK11] Chih-Chieh Cheng and Brian Kingsbury. Arccosine kernels: Acoustic modeling with inﬁnite neural

networks. In ICASSP  pages 5200–5203. IEEE  2011.

[DHS11] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. JMLR  12:2121–2159  2011.

[DJS96] John E Dennis Jr and Robert B Schnabel. Numerical methods for unconstrained optimization and

nonlinear equations. SIAM  1996.

[DXH+14] B. Dai  B. Xie  N. He  Y. Liang  A. Raj  M. Balcan  and L. Song. Scalable kernel methods via

doubly stochastic gradients. In NIPS  pages 3041–3049  2014.

[EM15] M. Erdogdu and A. Montanari. Convergence rates of sub-sampled newton methods. In NIPS  2015.
[GOSS16] Alon Gonen  Francesco Orabona  and Shai Shalev-Shwartz. Solving ridge regression using sketched

preconditioned svrg. In ICML  pages 1397–1405  2016.

[HAS+14] Po-Sen Huang  Haim Avron  Tara N Sainath  Vikas Sindhwani  and Bhuvana Ramabhadran. Kernel

methods match deep neural networks on timit. In ICASSP  pages 205–209. IEEE  2014.

[HCL+08] Cho-Jui Hsieh  Kai-Wei Chang  Chih-Jen Lin  S Sathiya Keerthi  and Sellamanickam Sundarara-
jan. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th
international conference on Machine learning  pages 408–415. ACM  2008.

[HMT11] Nathan Halko  Per-Gunnar Martinsson  and Joel A Tropp. Finding structure with randomness: Prob-
abilistic algorithms for constructing approximate matrix decompositions. SIAM review  53(2):217–
288  2011.

[JZ13] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS  pages 315–323  2013.

[KB14] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[KSW04] Jyrki Kivinen  Alexander J Smola  and Robert C Williamson. Online learning with kernels. Signal

Processing  IEEE Transactions on  52(8):2165–2176  2004.

[LML+14] Zhiyun Lu  Avner May  Kuan Liu  Alireza Bagheri Garakani  Dong Guo  Aurélien Bellet  Linxi
Fan  Michael Collins  Brian Kingsbury  Michael Picheny  et al. How to scale up kernel methods to
be as good as deep neural nets. arXiv preprint arXiv:1411.4000  2014.

[LN89] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.

Mathematical programming  45(1-3):503–528  1989.

[LSS13] Quoc Le  Tamás Sarlós  and Alex Smola. Fastfood-approximating kernel expansions in loglinear

time. In Proceedings of the international conference on machine learning  2013.

9

[MGL+17] Avner May  Alireza Bagheri Garakani  Zhiyun Lu  Dong Guo  Kuan Liu  Aurélien Bellet  Linxi Fan 
Michael Collins  Daniel Hsu  Brian Kingsbury  et al. Kernel approximation methods for speech
recognition. arXiv preprint arXiv:1701.03577  2017.

[Min17] Stanislav Minsker. On some extensions of bernstein’s inequality for self-adjoint operators. Statistics

& Probability Letters  2017.

[MNJ16] P. Moritz  R. Nishihara  and M. Jordan. A linearly-convergent stochastic l-bfgs algorithm. In

AISTATS  2016.

[PGB+11] D. Povey  A. Ghoshal  G. Boulianne  L. Burget  O. Glembek  N. Goel  M. Hannemann  P. Motlicek 

Y. Qian  P. Schwarz  et al. The kaldi speech recognition toolkit. In ASRU  2011.

[QB16] Qichao Que and Mikhail Belkin. Back to the future: Radial basis function networks revisited. In

AISTATS  pages 1375–1383  2016.

[RBV10] Lorenzo Rosasco  Mikhail Belkin  and Ernesto De Vito. On learning with integral operators.

Journal of Machine Learning Research  11(Feb):905–934  2010.

[Ric11] Lewis Fry Richardson. The approximate arithmetical solution by ﬁnite differences of physical
problems involving differential equations  with an application to the stresses in a masonry dam.
Philosophical Transactions of the Royal Society of London. Series A  210:307–357  1911.

[Ros97] Steven Rosenberg. The Laplacian on a Riemannian manifold: an introduction to analysis on

manifolds. Number 31. Cambridge University Press  1997.

[RR07] A. Rahimi and B. Recht. Random features for large-scale kernel machines.

1177–1184  2007.

In NIPS  pages

[RSB12] Nicolas L Roux  Mark Schmidt  and Francis R Bach. A stochastic gradient method with an
exponential convergence _rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems  pages 2663–2671  2012.

[RWY14] G. Raskutti  M. Wainwright  and B. Yu. Early stopping and non-parametric regression: an optimal

data-dependent stopping rule. JMLR  15(1):335–366  2014.

[SC08] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business

Media  2008.

[She94] Jonathan Richard Shewchuk. An introduction to the conjugate gradient method without the

agonizing pain  1994.

[SNB05] Vikas Sindhwani  Partha Niyogi  and Mikhail Belkin. Beyond the point cloud: from transductive
to semi-supervised learning. In Proceedings of the 22nd international conference on Machine
learning  pages 824–831. ACM  2005.

[SSSSC11] Shai Shalev-Shwartz  Yoram Singer  Nathan Srebro  and Andrew Cotter. Pegasos: Primal estimated

sub-gradient solver for SVM. Mathematical programming  127(1):3–30  2011.

[STC04] John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge

university press  2004.

[SYG07] Nicol N Schraudolph  Jin Yu  and Simon Günter. A stochastic quasi-newton method for online

convex optimization. In AISTATS  pages 436–443  2007.

[TBRS13] Martin Takác  Avleen Singh Bijral  Peter Richtárik  and Nati Srebro. Mini-batch primal and dual

methods for SVMs. In ICML (3)  pages 1022–1030  2013.

[TH12] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning  4:2  2012.
arXiv preprint

An introduction to matrix concentration inequalities.

[Tro15] Joel A Tropp.

arXiv:1501.01571  2015.

[TRVR16] S. Tu  R. Roelofs  S. Venkataraman  and B. Recht. Large scale kernel learning using block

coordinate descent. arXiv preprint arXiv:1602.05310  2016.

[Tsy04] Alexandre B Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics 

pages 135–166  2004.

[WS01] Christopher Williams and Matthias Seeger. Using the Nyström method to speed up kernel machines.

In NIPS  pages 682–688  2001.

[YRC07] Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. On early stopping in gradient descent

learning. Constructive Approximation  26(2):289–315  2007.

10

,SIYUAN MA
Mikhail Belkin
Nitin Bansal
Xiaohan Chen
Zhangyang Wang