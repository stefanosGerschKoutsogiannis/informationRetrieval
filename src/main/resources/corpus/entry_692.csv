2015,Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff,Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of $\widetilde{O}(T^{5/6})$  while the best known lower bound is $\Omega(T^{1/2})$. Many attemptshave been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case  the best known algorithm guarantees a regret of $\widetilde{O}(T^{2/3})$. We present an efficient algorithm for the banditsmooth convex optimization problem that guarantees a regret of $\widetilde{O}(T^{5/8})$. Our result rules out an $\Omega(T^{2/3})$ lower bound and takes a significant step towards the resolution of this open problem.,Bandit Smooth Convex Optimization:
Improving the Bias-Variance Tradeoff

Ofer Dekel

Microsoft Research

Redmond  WA

oferd@microsoft.com

Ronen Eldan

Weizmann Institute

Rehovot  Israel

roneneldan@gmail.com

Tomer Koren

Technion
Haifa  Israel

tomerk@technion.ac.il

Abstract

Bandit convex optimization is one of the fundamental problems in the ﬁeld of
online learning. The best algorithm for the general bandit convex optimiza-

is ⌦(T 1/2). Many attempts have been made to bridge the huge gap between these
bounds. A particularly interesting special case of this problem assumes that the
loss functions are smooth. In this case  the best known algorithm guarantees a re-

tion problem guarantees a regret of eO(T 5/6)  while the best known lower bound
gret of eO(T 2/3). We present an efﬁcient algorithm for the bandit smooth convex
optimization problem that guarantees a regret of eO(T 5/8). Our result rules out

an ⌦(T 2/3) lower bound and takes a signiﬁcant step towards the resolution of this
open problem.

1

Introduction

Bandit convex optimization [11  5] is the following online learning problem. First  an adversary
privately chooses a sequence of bounded and convex loss functions f1  . . .   fT deﬁned over a con-
vex domain K in d-dimensional Euclidean space. Then  a randomized decision maker iteratively
chooses a sequence of points x1  . . .   xT   where each xt 2 K. On iteration t  after choosing the
point xt  the decision maker incurs a loss of ft(xt) and receives bandit feedback: he observes the
value of his loss but he does not receive any other information about the function ft. The decision
maker uses the feedback to make better choices on subsequent rounds. His goal is to minimize re-
gret  which is the difference between his loss and the loss incurred by the best ﬁxed point in K. If
the regret grows sublinearly with T   it indicates that the decision maker’s performance improves as
the length of the sequence increases  and therefore we say that he is learning.
Finding an optimal algorithm for bandit convex optimization is an elusive open problem. The ﬁrst
algorithm for this problem was presented in Flaxman et al. [11] and guarantees a regret of R(T ) =
eO(T 5/6) for any sequence of loss functions (here and throughout  the asymptotic eO notation hides
a polynomial dependence on the dimension d as well as logarithmic factors). Despite the ongoing
effort to improve on this rate  it remains the state of the art. On the other hand  Dani et al. [9]
proves that for any algorithm there exists a worst-case sequence of loss functions for which R(T ) =
⌦(T 1/2)  and the gap between the upper and lower bounds is huge.
While no progress has been made on the general form of the problem  some progress has been made
in interesting special cases. Speciﬁcally  if the bounded convex loss functions are also assumed to be

tions are smooth (namely  their gradients are Lipschitz)  Saha and Tewari [15] present an algorithm

Lipschitz  Flaxman et al. [11] improves their regret guarantee to R(T ) = eO(T 3/4). If the loss func-
with a guaranteed regret of eO(T 2/3). Similarly  if the loss functions are bounded  Lipschitz  and
strongly convex  the guaranteed regret is eO(T 2/3) [3]. If even stronger assumptions are made  an
optimal regret rate ofe⇥(T 1/2) can be guaranteed; namely  when the loss functions are both smooth

1

and strongly-convex [12]  when they are Lipschitz and linear [2]  and when Lipschitz loss functions
are not generated adversarially but drawn i.i.d. from a ﬁxed and unknown distribution [4].
Recently  Bubeck et al. [8] made progress that did not rely on additional assumptions  such as
Lipschitz  smoothness  or strong convexity  but instead considered the general problem in the one-
dimensional case. That result proves that there exists an algorithm with optimal e⇥(T 1/2) regret
for arbitrary univariate convex functions ft : [0  1] 7! [0  1]. Subsequently  and after the current
paper was written  Bubeck and Eldan [7] generalized this result to bandit convex optimization in
general Euclidean spaces (albeit requiring a Lipschitz assumption). However  the proofs in both
papers are non-constructive and do not give any hint on how to construct a concrete algorithm  nor
any indication that an efﬁcient algorithm exists.
The current state of the bandit convex optimization problem has given rise to two competing con-
jectures. Some believe that there exists an efﬁcient algorithm that matches the current lower bound.
Meanwhile  others are trying to prove larger lower bounds  in the spirit of [10]  even under the as-
sumption that the loss functions are smooth; if the ⌦(T 1/2) lower bound is loose  a natural guess

any sequence of bounded  convex  smooth loss functions. Compare this result to the previous state-

of the true regret rate would be e⇥(T 2/3).1 In this paper  we take an important step towards the
resolution of this problem by presenting an algorithm that guarantees a regret of e⇥(T 5/8) against
of-the-art result of e⇥(T 2/3) (noting that 2/3 = 0.666... and 5/8 = 0.625). This result rules out

the possibility of proving a lower bound of ⌦(T 2/3) with smooth functions. While there remains
a sizable gap with the T 1/2 lower bound  our result brings us closer to ﬁnding the elusive optimal
algorithm for bandit convex optimization  at least in the case of smooth functions.
Our algorithm is a variation on the algorithms presented in [11  1  15]  with one new idea. These
algorithms all follow the same template: on each round  the algorithm computes an estimate of
rft(xt)  the gradient of the current loss function at the current point  by applying a random pertur-
bation to xt. The sequence of gradient estimates is then plugged into a ﬁrst-order online optimization
technique. The technical challenge in the analysis of these algorithms is to bound the bias and the
variance of these gradient estimates. Our idea is take a window of consecutive gradient estimates
and average them  producing a new gradient estimate with lower variance and higher bias. Overall 
the new bias-variance tradeoff works in our favor and allows us to improve the regret upper-bound.
Averaging uncorrelated random vectors to reduce variance is a well-known technique  but applying
it in the context of bandit convex optimization algorithm is easier said than done and requires us to
overcome a number of technical difﬁculties. For example  the gradient estimates in our window are
taken at different points  which introduces a new type of bias. Another example is the difﬁculty that
arrises when the sequence xs  . . .   xt travels adjacent to the boundary of the convex set K (imagine
transitioning from one face of a hypercube to another); the random perturbation applied to xs and
xt could be supported on orthogonal directions  yet we average the resulting gradient estimates
and expect to get a meaningful low-variance gradient estimate. While the basic idea is simple  our
non-trivial technical analysis is not  and may be of independent interest.

2 Preliminaries

We begin by deﬁning smooth bandit convex optimization more formally  and recalling several basic
results from previous work on the problem (Flaxman et al. [11]  Abernethy et al. [2]  Saha and Tewari
[15]) that we use in our analysis. We also review the necessary background on self-concordant
barrier functions.

2.1 Smooth Bandit Convex Optimization

In the bandit convex optimization problem  an adversary ﬁrst chooses a sequence of convex functions
f1  . . .   fT : K 7! [0  1]  where K is a closed and convex domain in Rd. Then  on each round
t = 1  . . .   T   a randomized decision maker has to choose a point xt 2 K  and after committing
to his decision he incurs a loss of ft(xt)  and observes this loss as feedback. The decision maker’s
expected loss (where expectation is taken with respect to his random choices) is E[PT
t=1ft(xt)] and
1In fact  we are aware of at least two separate research groups that invested time trying to prove such

an ⌦(T 2/3) lower bound.

2

his regret is

R(T ) = E" TXt=1

ft(xt)#  min

x2K

TXt=1

ft(x) .

Throughout  we use the notation Et[·] to indicate expectations conditioned on all randomness up to
and including round t  1.
We make the following assumptions. First  we assume that each of the functions f1  . . .   fT is L-
Lipschitz with respect to the Euclidean norm k·k2  namely that |ft(x)  ft(y)| Lkx  yk2 for all
x  y 2 K. We further assume that ft is H-smooth with respect to k·k 2  which is to say that
In particular  this implies that ft is continuously differentiable over K. Finally  we assume that the
Euclidean diameter of the decision domain K is bounded by D > 0.

krft(x)  rft(y)k2  H kx  yk2 .

8 x  y 2 K  

2.2 First Order Algorithms with Estimated Gradients

The online convex optimization problem becomes much easier in the full information setting  where
the decision maker’s feedback includes the vector gt = rft(xt)  the gradient (or subgradient) of
ft at the point xt. In this setting  the decision maker can use a ﬁrst-order online algorithm  such as
the projected online gradient descent algorithm [17] or dual averaging [13] (sometimes known as
follow the regularized leader [16])  and guarantee a regret of O(T 1/2). The dual averaging approach
sets xt to be the solution to the following optimization problem 

xt = arg min

x2K (x ·

t1Xs=1

↵s tgs + R(x))  

(1)

where R is a suitably chosen regularizer  and for all t = 1  . . .   T and s = 1  . . .   t we deﬁne a
non-negative weight ↵s t. Typically  all of the weights (↵s t) are set to a constant value ⌘  called the
learning rate parameter.
However  since we are not in the full information setting and the decision maker does not observe gt 
the algorithms mentioned above cannot be used directly. The key observation of Flaxman et al. [11] 
which is later reused in all of the follow-up work  is that gt can be estimated by randomly perturbing
the point xt. Speciﬁcally  on round t  the algorithm chooses the point

yt = xt + Atut  

(2)
instead of the original point xt  where > 0 is a parameter that controls the magnitude of the
perturbation  At is a positive deﬁnite d ⇥ d matrix  and ut is drawn from the uniform distribution
on the unit sphere. In Flaxman et al. [11]  At is simply set to the identity matrix whereas in Saha
and Tewari [15]  At is more carefully tailored to the point xt (see details below). In any case  care
should be taken to ensure that the perturbed point yt remains in the convex set K. The observed
value ft(yt) is then used to compute the gradient estimate

ˆgt =

d


t ut  

ft(yt)A1

ˆf t(x) = Et[ft(x + Atv)]  

(3)
and this estimate is fed to the ﬁrst-order optimization algorithm. While ˆgt is not an unbiased es-
timator of rft(xt)  it is an unbiased estimator for the gradient of a different function  ˆf t  deﬁned
by
(4)
where v 2 Rd is uniformly drawn from the unit ball. The function ˆf t(x) is a smoothed version of
ft  which plays a key role in our analysis and in many of the previous results on this topic. The main
property of ˆf t is summarized in the following lemma.
Lemma 1 (Flaxman et al. [11]  Saha and Tewari [15  Lemma 5]). For any differentiable function
f : Rd 7! R  positive deﬁnite matrix A  x 2 Rd  and  2 (0  1]  deﬁne ˆg = (d/)f (x+Au)·A1u 
where u is uniform on the unit sphere. Also  let ˆf (x) = E[f (x + Av)] where v is uniform on the
unit ball. Then E[ˆg] = rˆf (x).
The difference between rft(xt) and rˆf t(xt) is the bias of the gradient estimator ˆgt. The analysis
in Flaxman et al. [11]  Abernethy et al. [2]  Saha and Tewari [15] focuses on bounding the bias and
the variance of ˆgt and their effect on the ﬁrst-order optimization algorithm.

3

2.3 Self-Concordant Barriers

Following [2  1  15]  our algorithm and analysis rely on the properties of self-concordant barrier
functions. Intuitively  a barrier is a function deﬁned on the interior of the convex body K  which is
rather ﬂat in most of the interior of K and explodes to 1 as we approach its boundary. Addition-
ally  a self-concordant barrier has some technical properties that are useful in our setting. Before
giving the formal deﬁnition of a self-concordant barrier  we deﬁne the local norm deﬁned by a
self-concordant barrier.
Deﬁnition 2 (Local Norm Induced by a Self-Concordant Barrier [14]). Let R : int(K) 7! R be a
self-concordant barrier. The local norm induced by R at the point x 2 int(K) is denoted by kzkx
and deﬁned as kzkx =pzTr2R(x)z. Its dual norm is kzkx ⇤ =pzT(r2R(x))1z.
In words  the local norm at x is the Mahalanobis norm deﬁned by the Hessian of R at the point x 
namely  r2R(x). We now give a formal deﬁnition of a self-concordant barrier.
Deﬁnition 3 (Self-Concordant Barrier [14]). Let K ✓ Rd be a convex body. A function R :
int(K) 7! R is a #-self-concordant barrier for K if (i) R is three times continuously differentiable 
(ii) R(x) ! 1 as x ! @K   and (iii) for all x 2 int(K) and y 2 Rd  R satisﬁes
p#kykx .

|r3R(x)[y  y  y]| 2kyk3

and |rR(x) · y|

x

This deﬁnition is given for completeness  and is not directly used in our analysis. Instead  we rely
on some useful properties of self-concordant barriers. First and foremost  there exists a O(d)-self-
concordant barrier for any convex body [14  6]. Efﬁciently-computable self-concordant barriers
are only known for speciﬁc classes of convex bodies  such as polytopes  yet we make the standard
assumption that we have an efﬁciently computable #-self-concordant barrier for the set K.
Another key feature of a self-concordant barrier is the set of Dikin ellipsoids that it deﬁnes. The
Dikin ellipsoid at x 2 int(K) is simply the unit ball with respect to the local norm at x. A key
feature of the Dikin ellipsoid is that it is entirely contained in the convex body K  for any x [see 14 
Theorem 2.1.1]. Another technical property of a self-concordant barriers is that its Hessian changes
slowly with respect to its local norm.
Theorem 4 (Nesterov and Nemirovskii [14  Theorem 2.1.1]). Let K be a convex body with self-
concordant barrier R. For any x 2 int(K) and z 2 Rd such that kzkx < 1  it holds that

(1  kzkx)2 r2R(x)  r2R(x + z)  (1  kzkx)2 r2R(x) .

While the self-concordant barrier explodes to inﬁnity at the boundary of K  it is quite ﬂat at points
that are far from the boundary. To make this statement formal  we deﬁne an operation that mul-
tiplicatively shrinks the set K toward the minimizer of R (called the analytic center of K). Let
y = arg min R(x) and assume without loss of generality that R(y) = 0. For any ✏ 2 (0  1) let Ky ✏
denote the set {y + (1 ✏)(x y) : x 2 K}. The next theorem states that the barrier is ﬂat in Ky ✏
and explodes to 1 in the thin shell between Ky ✏ and K.
Theorem 5 (Nesterov and Nemirovskii [14  Propositions 2.3.2-3]). Let K be a convex body with
#-self-concordant barrier R  let y = arg min R(x)  and assume that R(y) = 0. For any ✏ 2 (0  1] 
it holds that

8 x 2 Ky ✏

R(x)  # log

1
✏

.

Our assumptions on the loss functions  as the Lipschitz assumption or the smoothness assumption 
are stated in terms of the standard Euclidean norm (which we denote by k·k 2). Therefore  we will
need to relate the Euclidean norm to the local norms deﬁned by the self-concordant barrier. This is
accomplished by the following lemma (whose proof appears in the supplementary material).
Lemma 6. Let K be a convex body with self-concordant barrier R and let D be the (Euclidean)
diameter of K. For any x 2 K  it holds that D1kzkx ⇤  kzk2  Dkzkx for all z 2 Rd.
2.4 Self-Concordant Barrier as a Regularizer

Looking back at the dual averaging strategy deﬁned in Eq. (1)  we can now ﬁll in some of the details
that were left unspeciﬁed: [1  15] set the regularization R in Eq. (1) to be a #-self-concordant barrier
for the set K. We use the following useful lemma from Abernethy and Rakhlin [1] in our analysis.

4

Algorithm 1: Bandit Smooth Convex Optimization
Parameters: perturbation parameter  2 (0  1]  dual averaging weights (↵s t)  self-concordant
barrier R : int(K) 7! R
Initialize: y1 2 K arbitrarily
for t = 1  . . .   T

s=1 ↵s tˆgs + R(x)}

s=1 ⌘gs + R(x)}. Then 

At (r2R(xt))1/2
draw ut uniformly from the unit sphere
yt xt + Atut
choose yt  receive feedback ft(yt)
ˆgt (d/)ft(yt) · A1
t ut
xt+1 arg minx2K{x ·Pt
Lemma 7 (Abernethy and Rakhlin [1]). Let K be a convex body with #-self-concordant barrier
R  let g1  . . .   gT be vectors in Rd  and let ⌘> 0 be such that ⌘kgtkxt ⇤  1
4 for all t. Deﬁne
xt = arg minx2K{x ·Pt1
(i) for all t it holds that kxt  xt+1kxt  2⌘kgtkxt ⇤;
(ii) for any x? 2 K it holds thatPT
Algorithms for bandit convex optimization that use a self-concordant regularizer also use the same
self-concordant barrier to obtain gradient estimates. Namely  these algorithms perturb the dual av-
eraging solution xt as in Eq. (3)  with the perturbation matrix At set to (r2R(xt))1/2  the root of
the inverse Hessian of R at the point xt. In other words  the distribution of yt is supported on the
Dikin ellipsoid centered at xt  scaled by . Since  2 (0  1]  this form of perturbation guarantees
that yt 2 K. Moreover  if yt is generated in this way and used to construct the gradient estimator
ˆgt  then the local norm of ˆgt is bounded  as speciﬁed in the following lemma.
Lemma 8 (Saha and Tewari [15  Lemma 5]). Let K ✓ Rd be a convex body with self-concordant
barrier R. For any differentiable function f : K 7! [0  1]   2 (0  1]  and x 2 int(K)  deﬁne
ˆg = (d/)f (y) · A1u  where A = (r2R(x))1/2  y = x + Au  and u is drawn uniformly from
the unit sphere. Then kˆgkx  d/.
3 Main Result

⌘ R(x?) + 2⌘PT

t=1 gt · (xt  x?)  1

t=1 kgtk2

xt ⇤

.

Our algorithm for the bandit smooth convex optimization problem is a variant of the algorithm in
Saha and Tewari [15]  and appears in Algorithm 1. Following Abernethy and Rakhlin [1]  Saha and
Tewari [15]  we use a self-concordant function as the dual averaging regularizer and we use its Dikin
ellipsoids to perturb the points xt. The difference between our algorithm and previous ones is the
introduction of dual averaging weights (↵s t)  for t = 1  . . .   T and s = 1  . . .   t  which allow us to
vary the weight of each gradient in the dual averaging objective function.
In addition to the parameters   ⌘  and ✏  we introduce a new buffering parameter k  which takes
non-negative integer values. We set the dual averaging weights in Algorithm 1 to be

↵s t = (⌘

ts+1
k+1 ⌘

if s  t  k
if s > t  k  

(5)

where ⌘> 0 is a global learning rate parameter. This choice of (↵s t) effectively decreases the
inﬂuence of the feedback received on the most recent k rounds. If k = 0  all of the (↵s t) become
equal to ⌘ and Algorithm 1 reduces to the algorithm in Saha and Tewari [15]. The surprising result
is that there exists a different setting of k > 0 that gives a better regret bound.
We introduce a slight abuse of notation  which helps us simplify the presentation of our regret bound.
We will eventually achieve the desired regret bound by setting the parameters ⌘    and k to be some
functions of T . Therefore  from now on  we treat the notation ⌘    and k as an abbreviation for the
functional forms ⌘(T )  (T )  and k(T ) respectively. The beneﬁt is that we can now use asymptotic
notation (e.g.  O(⌘k)) to sweep meaningless low-order terms under the rug.

5

We prove the following regret bound for this algorithm.
Theorem 9. Let f1  . . .   fT be a sequence of loss functions where each ft : K 7! [0  1] is dif-
ferentiable  convex  H-smooth and L-Lipschitz  and where K ✓ Rd is a convex body of diameter
D > 0 with #-self-concordant barrier R. For any   ⌘ 2 (0  1] and k 2{ 0  1  . . .   T} assume that
Algorithm 1 is run with these parameters and with the weights deﬁned in Eq. (5) (using k and ⌘) to
generate the sequences x1  . . .   xT and y1  . . .   yT . If 12k⌘d   and for any ✏ 2 (0  1) it holds that
+ T⌘k◆ .

R(T )  HD22T +
that R(T ) = O(pd T 5/8 log T ).
Note that if we set k = 0 in our theorem  we recover the eO(T 2/3) bound in Saha and Tewari [15] up

Speciﬁcally  if we set  = d1/4T 3/16  ⌘ = d1/2T 5/8  k = d1/2T 1/8  and ✏ = T 100  we get

to a small numerical constant (namely  the dependence on L  H  D  #  d  and T is the same).

12(HD2 + DL)d⌘pkT

+ O✓ T✏

64d2⌘T
2(k + 1)

# log 1
✏

+

+

⌘





4 Analysis

b

c

a

|

. (6)

regret as

+ E" TXt=1
|

ft(yt)  ˆf t(xt)#
}
{z

ˆf t(x?)  ft(x?)#
}
{z

ˆf t(xt)  ˆf t(x?)#
}
{z

Using the notation x? = arg minx2KPT
t=1 ft(x)  the decision maker’s regret becomes R(T ) =
E⇥PT
t=1 ft(yt)  ft(x?)⇤. Following Flaxman et al. [11]  Saha and Tewari [15]  we rewrite the
+ E" TXt=1
R(T ) = E" TXt=1
|

This decomposition essentially adds a layer of hallucination to the analysis: we pretend that the
loss functions are ˆf 1  . . .   ˆf T instead of f1  . . .   fT and we also pretend that we chose the points
x1  . . .   xT rather than y1  . . .   yT . We then analyze the regret in this pretend world (this regret is
the expression in Eq. (6b)). Finally  we tie our analysis back to the real world by bounding the
difference between that which we analyzed and the regret of the actual problem (this difference is
the sum of Eq. (6a) and Eq. (6c)). The advantage of our pretend world over the real world is that we
have unbiased gradient estimates ˆg1  . . .   ˆgT that can plug into the dual averaging algorithm.
The algorithm in Saha and Tewari [15] sets all of the dual averaging weights (↵s t) equal to the
constant learning rate ⌘> 0. It decomposes the regret as in Eq. (6) and their main technical result is
the following bound for the individual terms:
Theorem 10 (Saha and Tewari [15]). Let f1  . . .   fT be a sequence of loss functions where each
ft : K 7! [0  1] is differentiable  convex  H-smooth and L-Lipschitz  and where K ✓ Rd is a convex
body of diameter D > 0 and #-self-concordant barrier R. Assume that Algorithm 1 is run with
perturbation parameter  2 (0  1] and generates the sequences x1  . . .   xT and y1  . . .   yT . Then
for any ✏ 2 (0  1) it holds that (6a) + (6c)  (HD22 + ✏L)T . If  additionally  the dual averaging
weights (↵s t) are all set to the constant learning rate ⌘ then (6b)  # log(1/✏)⌘1 + d22⌘T .
The analysis in Saha and Tewari [15] goes on to obtain a regret bound of eO(T 2/3) by choosing

optimal values for the parameters ⌘   and ✏ and plugging those values into Theorem 10. Our
analysis uses the ﬁrst part of Theorem 10 to bound (6a) + (6c) and shows that our careful choice of
the dual averaging weights (↵s t) results in the following improved bound on (6b).
We begin our analysis by deﬁning a moving average of the functions ˆf 1  . . .   ˆf T   as follows:

8 t = 1  . . .   T  

¯f t(x) =

1

k + 1

ˆf ti(x)  

(7)

where  for soundness  we let ¯f s ⌘ 0 for s  0. Also  deﬁne a moving average of gradient estimates:

kXi=0
kXi=0

8 t = 1  . . .   T  

¯gt =

1

k + 1

6

ˆgti  

again  with ˆgs = 0 for s  0. In Section 4 below  we show how each ¯gt can be used as a biased
estimate of r¯f t(xt). Also note that the choice of the dual averaging weights (↵s t) in Eq. (5) is such
thatPt
s=1 ¯gs for all t. Therefore  the last step in Algorithm 1 basically performs
dual averaging with the gradient estimates ¯g1  . . .   ¯gT uniformly weighted by ⌘.
We use the functions ¯f t to rewrite Eq. (6b) as

+ E" TXt=1
|

¯f t(xt)  ¯f t(x?)#
}
{z

b

+ E" TXt=1
|

¯f t(x?)  ˆf t(x?)#
}
{z

c

.

(8)

s=1 ↵s tˆgs = ⌘Pt
E" TXt=1
|

ˆf t(xt)  ¯f t(xt)#
}
{z

a

This decomposition essentially adds yet another layer of hallucination to the analysis: we pretend
that the loss functions are ¯f 1  . . .   ¯f T instead of ˆf 1  . . .   ˆf T (which are themselves pretend loss
functions  as described above). Eq. (8b) is the regret in our new pretend scenario  while Eq. (8a) +
Eq. (8c) is the difference between this regret and the regret in Eq. (6b).
The following lemma bounds each of the terms in Eq. (8) separately  and summarizes the main
technical contribution of our paper.
Lemma 11. Under the conditions of Theorem 9  for any ✏ 2 (0  1) it holds that (8c)  0  
+ T⌘k◆ .

+ O(1 + ⌘)k  

+ O✓ T✏

(8a) 
(8b) 

12HD2d⌘pkT

12DLd⌘pkT

64d2⌘T
2(k + 1)


# log 1
✏

and

+

+

⌘





4.1 Proof Sketch of Lemma 11

As mentioned above  the basic intuition of our technique is quite simple: average the gradients to
decrease their variance. Yet  applying this idea in the analysis is tricky. We begin by describing the
main source of difﬁculty in proving Lemma 11.
Recall that our strategy is to pretend that the loss functions are ¯f 1  . . .   ¯f T and to use the random
vector ¯gt as a biased estimator of r¯f t(xt). Naturally  one of our goals is to show that this bias
is small. Recall that each ˆgs is an unbiased estimator of rˆf s(xs) (conditioned on the history up
to round t). Speciﬁcally  note that each vector in the sequence ˆgtk  . . .   ˆgt is a gradient estimate
at a different point. Yet  we average these vectors and claim that they accurately estimate r¯f t at
the current point  xt. Luckily  ˆf t is H-smooth  so rˆf ti(xti) should not be much different than
rˆf ti(xt)  provided that we show that xti and xt are close to each other in Euclidean distance.
To show that xti and xt are close  we exploit the stability of the dual averaging algorithm. Particu-
larly  the ﬁrst claim in Lemma 7 states that kxs  xs+1kxs is controlled by k¯gskxs ⇤ for all s  so now
we need to show that k¯gskxs ⇤ is small. However  ¯gt is the average of k + 1 gradient estimates taken
at different points; each ˆgti is designed to have a small norm with respect to its own local norm
k·k xti ⇤; for all we know  it may be very large with respect to the current local norm k·k xt ⇤. So
now we need to show that the local norms at xti and xt are similar. We could prove this if we knew
that xti and xt are close to each other—which is exactly what we set out to prove in the beginning.
This chicken-and-egg situation complicates our analysis considerably.
Another non-trivial component of our proof is the variance reduction analysis. The motivation
to average ˆgtk  . . .   ˆgt is to generate new gradient estimates with a smaller variance. While the
random vectors ˆg1  . . .   ˆgT are not independent  we show that their randomness is uncorrelated.
Therefore  the variance of ¯gt is k + 1 times smaller than the variance of each ˆgt. However  to make
this argument formal  we again require the local norms at xti and xt to be similar.
To make things more complicated  there is the recurring need to move back and forth between local
norms and the Euclidean norm  since the latter is used in the deﬁnition of Lipschitz and smoothness.
All of this has to do with bounding Eq. (8b)  the regret with respect to the pretend loss functions
¯f 1  . . .   ¯f T - an additional bias term appears in the analysis of Eq. (8a).
We conclude the paper by stating our main lemmas and sketching the proof Lemma 11. The full
technical proofs are all deferred to the supplementary material and replaced with some high level
commentary.

7

To break the chicken-and-egg situation described above  we begin with a crude bound on k¯gtkxt ⇤ 
which does not beneﬁt at all from the averaging operation. We simultaneously prove that the local
norms at xti and xt are similar.
Lemma 12. If the parameters k  ⌘  and  are chosen such that 12k⌘d   then for all t 

(i) k¯gtkxt ⇤  2d/;
(ii) for any 0  i  k such that t  i  1 it holds that 1
Lemma 12 itself has a chicken-and-egg aspect  which we resolve using an inductive proof technique.
Armed with the knowledge that the local norms at xti and xt are similar  we go on to prove the
more reﬁned bound on Et⇥k¯gtk2
Lemma 13. If the parameters k  ⌘  and  are chosen such that 12k⌘d   then

2kzkxti ⇤  kzkxt ⇤  2kzkxti ⇤.

xt ⇤⇤  which does beneﬁt from averaging.
Et⇥k¯gtk2

xt ⇤⇤  2D2L2 +

2(k + 1)

32d2

.

The proof constructs a martingale difference sequence and uses the fact that its increments are un-
correlated. Compare the above to Lemma 8  which proves that kˆgtik2
xti ⇤  d2/2 and note the
extra k + 1 in our denominator—all of our hard work was aimed at getting this factor.
Next  we set out to bound the expected Euclidean distance between xti and xt. This bound is later
needed to exploit the L-Lipschitz and H-smooth assumptions. The crude bound on k¯gskxs ⇤ from
Lemma 12 is enough to satisfy the conditions of Lemma 7  which then tells us that E[kxsxs+1kxs]
is controlled by E[k¯gskxs ⇤]. The latter enjoys the improved bound due to Lemma 13. Integrating
the resulting bound over time  we obtain the following lemma.
Lemma 14. If the parameters k  ⌘  and  are chosen such that 12k⌘d   then for all t and any
0  i  k such that t  i  1  we have

E[kxti  xtk2] 

12Dd⌘pk



+ O(⌘k) .

Notice that xti and xt may be k rounds apart  but the bound scales only with pk. Again  this is
the work of the averaging technique.
Finally  we have all the tools in place to prove our main result  Lemma 11.
Proof sketch. The ﬁrst term  Eq. (8a)  is bounded by rewriting ¯f t(xt) = 1
ˆf ti(xt) and
then proving that ˆf ti(xt) is not very far from ˆf t(xt). This follows from the fact that ˆf t is L-
Lipschitz and from Lemma 14. To bound the second term  Eq. (8b)  we use the convexity of each ¯f t
to write

i=0

k+1Pk
r¯f t(xt) · (xt  x?)# .

using the fact that ˆf t is H-smooth and again using Lemma 14. Then  we upper bound the above
⇤
using Lemma 7  Theorem 5  and Lemma 13.

Acknowledgments
We thank Jian Ding for several critical contributions during the early stages of this research. Parts
of this work were done while the second and third authors were at Microsoft Research  the support
of which is gratefully acknowledged.

8

We relate the right-hand side above to

E" TXt=1

¯f t(xt)  ¯f t(x?)#  E" TXt=1
¯gt · (xt  x?)#  

E" TXt=1

References
[1] J. Abernethy and A. Rakhlin. Beating the adaptive bandit with high probability. In Information

Theory and Applications Workshop  2009  pages 280–289. IEEE  2009.

[2] J. Abernethy  E. Hazan  and A. Rakhlin. Competing in the dark: An efﬁcient algorithm for
bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory
(COLT)  2008.

[3] A. Agarwal  O. Dekel  and L. Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning The-
ory (COLT)  2010.

[4] A. Agarwal  D. P. Foster  D. Hsu  S. M. Kakade  and A. Rakhlin. Stochastic convex optimiza-
In Advances in Neural Information Processing Systems (NIPS) 

tion with bandit feedback.
2011.

[5] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[6] S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal universal self-concordant

barrier. arXiv preprint arXiv:1412.1587  2015.

mization. arXiv preprint arXiv:1507.06580  2015.

[7] S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex opti-
[8] S. Bubeck  O. Dekel  T. Koren  and Y. Peres. Bandit convex optimization: pT regret in one
In In Proceedings of the 28st Annual Conference on Learning Theory (COLT) 

dimension.
2015.

[9] V. Dani  T. Hayes  and S. M. Kakade. The price of bandit information for online optimization.

In Advances in Neural Information Processing Systems (NIPS)  2008.

[10] O. Dekel  J. Ding  T. Koren  and Y. Peres. Bandits with switching costs: T 2/3 regret.

Proceedings of the 46th Annual Symposium on the Theory of Computing  2014.

In

[11] A. D. Flaxman  A. Kalai  and H. B. McMahan. Online convex optimization in the bandit
setting: gradient descent without a gradient.
In Proceedings of the sixteenth annual ACM-
SIAM symposium on Discrete algorithms  pages 385–394. Society for Industrial and Applied
Mathematics  2005.

[12] E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in

Neural Information Processing Systems (NIPS)  2014.

[13] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical program-

ming  120(1):221–259  2009.

[14] Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming 

volume 13. SIAM  1994.

[15] A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with
bandit feedback. In International Conference on Artiﬁcial Intelligence and Statistics (AISTAT) 
pages 636–642  2011.

[16] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning  4(2):107–194  2011.

[17] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML’03)  pages
928–936  2003.

9

,Özlem Aslan
Xinhua Zhang
Dale Schuurmans
Ofer Dekel
Tomer Koren
Alina Beygelzimer
Daniel Hsu
John Langford
Chicheng Zhang
Matteo Turchetta
Felix Berkenkamp
Andreas Krause