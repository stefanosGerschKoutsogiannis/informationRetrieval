2011,Non-conjugate Variational Message Passing for Multinomial and Binary Regression,Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP  which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity  allowing choice in how expectations are calculated  and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.,Non-conjugate Variational Message Passing for

Multinomial and Binary Regression

David A. Knowles

Department of Engineering
University of Cambridge

Thomas P. Minka
Microsoft Research

Cambridge  UK

Abstract

Variational Message Passing (VMP) is an algorithmic implementation of the Vari-
ational Bayes (VB) method which applies only in the special case of conjugate
exponential family models. We propose an extension to VMP  which we refer to
as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate
this restriction while maintaining modularity  allowing choice in how expecta-
tions are calculated  and integrating into an existing message-passing framework:
Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regres-
sion. In the multinomial case we introduce a novel variational bound for the soft-
max factor which is tighter than other commonly used bounds whilst maintaining
computational tractability.

1

Introduction

Variational Message Passing [20] is a message passing implementation of the mean-ﬁeld approxima-
tion [1  2]  also known as variational Bayes (VB). Although Expectation Propagation [12] can have
more desirable properties as a result of the particular Kullback-Leibler divergence that is minimised 
VMP is more stable than EP under certain circumstances  such as multi-modality in the posterior
distribution.
Unfortunately  VMP is effectively limited to conjugate-exponential models since otherwise the mes-
sages become exponentially more complex at each iteration. In conjugate exponential models this
is avoided due to the closure of exponential family distributions under multiplication. There are
many non-conjugate problems which arise in Bayesian statistics  for example logistic regression or
learning the hyperparameters of a Dirichlet.
Previous work extending Variational Bayes to non-conjugate models has focused on two aspects.
The ﬁrst is how to ﬁt the variational parameters when the VB free form updates are not viable.
Various authors have used standard numerical optimization techniques [15  17  3]  or adapted such
methods to be more suitable for this problem [7  8]. A disadvantage of this approach is that the
convenient and efﬁcient message-passing formulation is lost.
The second line of work applying VB to non-conjugate models involves deriving lower bounds
to approximate the expectations [9  18  5  10  11] required to calculate the KL divergence. We
contribute to this line of work by proposing and evaluating a new bound for the useful softmax factor 
which is tighter than other commonly used bounds whilst maintaining computational tractability. We
also demonstrate  in agreement with [19] and [14]  that for univariate expectations such as required
for logistic regression  carefully designed quadrature methods can be effective.
Existing methods typically represent a compromise on modularity or performance. To maintain
modularity one is effectively constrained to use exponential family bounds (e.g. quadratic in the
Gaussian case [9  5]) which we will show often gives sub-optimal performance. Methods which uses
more general bounds  e.g. [3]  must then resort to numerical optimisation  and sacriﬁce modularity.

1

This is a particular disadvantage for an inference framework such as Infer.NET [13] where we want
to allow modular construction of inference algorithms from arbitrary deterministic and stochastic
factors. We propose a novel message passing algorithm  which we call Non-conjugate Variational
Message Passing (NCVMP)  which generalises VMP and gives a recipe for calculating messages
out of any factor. NCVMP gives much greater freedom in how expectations are taken (using bounds
or quadrature) so that performance can be maintained along with modularity.
The outline of the paper is as follows. In Sections 2 and 3 we brieﬂy review VB and VMP. Section 4
is the main contribution of the paper: Non-conjugate VMP. Section 5 describes the binary logistic
and multinomial softmax regression models  and implementation options with and without NCVMP.
Results on synthetic and standard UCI datasets are given in Section 6 and some conclusions are
drawn in Section 7.

2 Mean-ﬁeld approximation

Our aim is to approximate some model p(x)  represented as a factor graph p(x) = (cid:81)
variational posterior q(x) =(cid:81)
(cid:90)

a fa(xa)
where factor fa is a function of all x ∈ xa. The mean-ﬁeld approximation assumes a fully-factorised
i qi(xi) where qi(xi) is an approximation to the marginal distribution
of xi (note however xi might be vector valued  e.g. with multivariate normal qi). The variational
approximation q(x) is chosen to minimise the Kullback-Leibler divergence KL(q||p)  given by

where H[q(x)] = −(cid:82) q(x) log q(x)dx is the entropy. It can be shown [1] that if the functions qi(xi)

KL(q||p) =

q(x) log q(x)

p(x) dx = −H[q(x)] −

are unconstrained then minimising this functional can be achieved by coordinate descent  setting
qi(xi) = exp(cid:104)log p(x)(cid:105)¬qi(xi)  iteratively for each i  where (cid:104)...(cid:105)¬qi(xi) implies marginalisation of
all variables except xi.

(cid:90)

q(x) log p(x)dx.

(1)

3 Variational Message Passing on factor graphs

xi∈xa

VMP is an efﬁcient algorithmic implementation of the mean-ﬁeld approximation which lever-
ages the fact that the mean-ﬁeld updates only requires local operations on the factor graph.
The variational distribution q(x) factorises into approximate factors ˜fa(xa). As a result of the
fully factorised approximation  the approximate factors themselves factorise into messages  i.e.
ma→i(xi) where the message from factor a to variable i is ma→i(xi) =
exp(cid:104)log fa(xa)(cid:105)¬qi(xi). The message from variable i to factor a is the current variational poste-
a∈N (i) ma→i(xi) where N (i) are the

˜fa(xa) = (cid:81)
rior of xi  denoted qi(xi)  i.e. mi→a(xi) = qi(xi) = (cid:81)

factors connected to variable i.
For conjugate-exponential models the messages to a particular variable xi  will all be in the same
exponential family. Thus calculating qi(xi) simply involves summing sufﬁcient statistics. If  how-
ever  our model is not conjugate-exponential  there will be a variable xi which receives incoming
messages which are in different exponential families  or which are not even exponential family dis-
tributions at all. Thus qi(xi) will be some more complex distribution. Computing the required
expectations becomes more involved  and worse still the complexity of the messages (e.g. the num-
ber of possible modes) grows exponentially per iteration.

4 Non-conjugate Variational Message Passing

In this section we give some criteria under which the algorithm was conceived. We set up required
notation and describe the algorithm  and prove some important properties. Finally we give some
intuition about what the algorithm is doing. The approach we take aims to fulﬁll certain criteria:

1. provides a recipe for any factor
2. reduces to standard VMP in the case of conjugate exponential factors
3. allows modular implementation and combining of deterministic and stochastic factors

2

NCVMP ensures the gradients of the approximate KL divergence implied by the message match the
gradients of the true KL. This means that we will have a ﬁxed point at the correct point in parameter
space: the algorithm will be at a ﬁxed point if the gradient of the KL is zero.
We use the following notation: variable xi has current variational posterior qi(xi; θi)  where θi is
the vector of natural parameters of the exponential family distribution qi. Each factor fa which is
a neighbour of xi sends a message ma→i(xi; φa→i) to xi  where ma→i is in the same exponential
family as qi  i.e. ma→i(xi; φ) = exp(φT u(xi)−κ(φ)) and qi(xi; θ) = exp(θT u(xi)−κ(θ)) where
u(·) are sufﬁcient statistics  and κ(·) is the log partition function. We deﬁne C(θ) as the Hessian of
κ(·) evaluated at θ  i.e. Cij(θ) = ∂2κ(θ)
. It is straightforward to show that C(θ) = cov(u(x)|θ)
so if the exponential family qi is identiﬁable  C will be symmetric positive deﬁnite  and therefore

invertible. The factor fa contributes a term Sa(θi) = (cid:82) qi(xi; θi)(cid:104)log fa(x)(cid:105)¬qi(xi)dxi to the KL

divergence  where we have only made the dependence on θi explicit: this term is also a function of
the variational parameters of the other variables neighbouring fa. With this notation in place we are
now able to describe the NCVMP algorithm.

∂θi∂θj

for all neighbouring factors a ∈ N (i) do

Algorithm 1 Non-conjugate Variational Message Passing
1: Initialise all variables to uniform θi := 0∀i
2: while not converged do
for all variables i do
3:
4:
5:
6:
7:
end for
8:
9: end while

φa→i := C(θi)−1 ∂Sa(θi)

θi :=(cid:80)

a∈N (i) φa→i

end for

∂θi

To motivate Algorithm 1 we give a rough proof that we will have a ﬁxed point at the correct point in
parameter space: the algorithm will be at a ﬁxed point if the gradient of the KL divergence is zero.
Theorem 1. Algorithm 1 has a ﬁxed point at {θi : ∀i} if and only if {θi : ∀i} is a stationary point
of the KL divergence KL(q||p).

Proof. Firstly deﬁne the function

˜Sa(θ; φ) :=

(cid:90)

qi(xi; θ) log ma→i(xi; φ)dxi 

(2)

(cid:90)

which is an approximation to the function Sa(θ). Since qi and ma→i belong to the same exponential
family we can simplify as follows 

˜Sa(θ; φ) =

qi(xi; θ)(φT u(xi) − κ(φ))dxi = φT(cid:104)u(xi)(cid:105)θ − κ(φ) = φT ∂κ(θ)
∂θ

(3)
where (cid:104)·(cid:105)θ implies expectation wrt qi(xi; θ) and we have used the standard property of exponential
families that (cid:104)u(xi)(cid:105)θ = ∂κ(θ)
= C(θ)φ. Now  the
update in Algorithm 1  Line 5 for φa→i ensures that

∂θ . Taking derivatives wrt θ we have ∂ ˜Sa(θ;φ)

− κ(φ) 

∂θ

C(θ)φ = ∂Sa(θ)

⇔ ∂ ˜Sa(θ; φ)

= ∂Sa(θ)

.

(4)
Thus this update ensures that the gradients wrt θi of S and ˜S match. The update in Algorithm 1 
Line 7 for θi is minimising an approximate local KL divergence for xi:

∂θ

∂θ

∂θ

θi := arg min
θ

−H[qi(xi  θi)] − (cid:88)

∂
∂θi

˜Sa(θi; φa→i)

a∈N (i)

where H[.] is the entropy. If and only if we are at a ﬁxed point of the algorithm  we will have
∂ ˜Sa(θi; φa→i)

˜S(θ; φa→i)

a∈N (i)

 = ∂H[qi(xi  θi)]

∂θi

3

 = (cid:88)
− (cid:88)

a∈N (i)

a∈N (i)

φa→i

(5)

= 0

∂θi

−H[qi(xi  θ)] − (cid:88)

for all variables i. By (4)  if and only if we are at a ﬁxed point (so that θi has not changed since
updating φ) we have

∂Sa(θi)

∂θi

= ∂KL(q||p)

∂θi

= 0

(6)

− ∂H[qi(xi  θi)]

∂θi

for all variables i.

− (cid:88)

a∈N (i)

Theorem 1 showed that if NCVMP converges to a ﬁxed point then it is at a stationary point of
the KL divergence KL(q||p).
In practice this point will be a minimum because any maximum
would represent an unstable equilibrium. However  unlike VMP we have no guarantee to decrease
KL(q||p) at every step  and indeed we do sometimes encounter convergence problems which require
damping to ﬁx: see Section 7. Theorem 1 also gives some intuition about what NCVMP is doing.
˜Sa is a conjugate approximation to the true Sa function  chosen to have the correct gradients at the
current θi. The update at variable xi for θi combines all these approximations from factors involving
xi to get an approximation to the local KL  and then moves θi to the minimum of this approximation.
Another important property of Non-conjugate VMP is that it reduces to standard VMP for conjugate
factors.
Theorem 2. If (cid:104)log fa(x)(cid:105)¬qi(xi) as a function of xi can be written µT u(xi) − c where c is a con-
stant  then the NCVMP message ma→i(xi  φa→i) will be the standard VMP message ma→i(xi  µ).
Proof. To see this note that (cid:104)log fa(x)(cid:105)¬qi(xi) = µT u(xi) − c ⇒ Sa(θ) = µT(cid:104)u(xi)(cid:105)θ − c 
where µ is the expected natural statistic under the messages from the variables connected to fa other
than xi. We have Sa(θ) = µT ∂κ(θ)
∂θ = C(θ)µ so from Algorithm 1  Line 7 we
have φa→i := C(θ)−1 ∂Sa(θ)

∂θ = C(θ)−1C(θ)µ = µ  the standard VMP message.

∂θ − c ⇒ ∂Sa(θ)

The update for θi in Algorithm 1  Line 7 is the same as for VMP  and Theorem 2 shows that for
conjugate factors the messages sent to the variables are the same as for VMP. Thus NCVMP is a
generalisation of VMP.
NCVMP can alternatively be derived by assuming the incoming messages to xi are ﬁxed apart from
ma→i(xi; φ) and calculating a ﬁxed point update for ma→i(xi; φ). Gradient matching for NCVMP
can be seen as analogous to moment matching in EP. Due to space limitations we defer the details
to the supplementary material.

4.1 Gaussian variational distribution

Here we describe the NCVMP updates for a Gaussian variational distribution q(x) = N(x; m  v)
and approximate factor ˜f(x; mf   vf ). Although these can be derived from the generic formula using
natural parameters it is mathematically more convenient to use the mean and variance (NCVMP is
parameterisation invariant so it is valid to do this).

1
vf

= −2 dS(m  v)

dv

 

mf
vf

= m
vf

+ dS(m  v)

dm

.

(7)

5 Logistic regression models

of the model is standard: gkn = (cid:80)D
We illustrate NCVMP on Bayesian binary and multinomial logistic regression. The regression part
d=1 WkdXdn + mk where g is the auxiliary variable  W is a
matrix of weights with standard normal prior  X is the design matrix and m is a per class mean 
which is also given a standard normal prior. For binary regression we just have k = 1  and the
observation model is p(y = 1|g1n) = σ(g1n) where σ(x) = 1/(1 + e−x) is the logistic function.
In the multinomial case p(y = k|g:n) = σk(g:n) where σk(x) = exk(cid:80)
l exl is the “softmax” function.
The VMP messages for the regression part of the model are standard so we omit the details due to
space limitations.

4

5.1 Binary logistic regression
For logistic regression we require the following factor: f(s  x) = σ(x)s(1 − σ(x))1−s where we
assume s is observed. The log factor is sx − log(1 + ex). There are two problems: we cannot
analytically compute expectations wrt to x  and we need to optimise the variational parameters. [9]
propose the “quadratic” bound on the integrand
σ(x) ≥ ˜σ(x  t) = σ(t) exp

(cid:18)
(x − t)/2 − λ(t)
2

(x2 − t2)

(cid:19)

(8)

 

where λ(t) = tanh (t/2)
. It is straightforward to analytically optimise t to make the
bound as tight as possible. The bound is conjugate to a Gaussian  but its performance can be poor.
An alternative proposed in [18] is to bound the integral:

t

t

= σ(t)−1/2

(cid:104)log f(x)(cid:105)q ≥ sm − 1

2 a2v − log(1 + em+(1−2a)v/2)) 

(9)

= a(1−a)  mf

where m  v are the mean and variance of q(x) and a is a variational parameter which can be opti-
mised using the ﬁxed point iteration a := σ(m−(1−2a)v/2). We refer to this as the “tilted” bound.
This bound is not conjugate to a Gaussian  but we can calculate the NCVMP message  which has
+s−a  where we have assumed a has been optimised. A ﬁnal
parameters: 1
vf
possibility is to use quadrature to calculate the gradients of S(m  v) directly. The NCVMP message
+ s − (cid:104)σ(x)(cid:105)q. The univariate expecta-
then has parameters 1
vf
tions (cid:104)σ(x)(cid:105)q and (cid:104)xσ(x)(cid:105)q can be efﬁciently computed using Gauss-Hermite or Clenshaw-Curtis
quadrature.

= (cid:104)xσ(x)(cid:105)q−m(cid:104)σ(x)(cid:105)q

= m
vf

= m
vf

  mf
vf

vf

v

5.2 Multinomial softmax regression

k=1 δ (pk − σk(x))  where xk are real valued and p is a
probability vector with current Dirichlet variational posterior q(p) = Dir(p; d). We can integrate
l exl where we deﬁne
k=1 N(xk; mk  vk). How should
l exl term? The approach used by [3] is a linear Taylor expansion of the log 

Consider the softmax factor f(x  p) = (cid:81)K
out p to give the log factor log f(x) =(cid:80)K
d. :=(cid:80)K
we deal with the log(cid:80)
exi(cid:105) ≤ log(cid:88)

k=1 dk. Let the incoming message from x be q(x) =(cid:81)K
(cid:104)exi(cid:105) = log(cid:88)

k=1(dk − 1)xk − (d. − K) log(cid:80)

which is accurate for small variances v:

(cid:104)log(cid:88)

emi+vi/2 

(10)

i

i

i

which we refer to as the “log” bound. The messages are still not conjugate  so some numerical
method must still be used to learn m and v: while [3] used LBFGS we will use NCVMP. Another
bound was proposed by [5]:

K(cid:88)

K(cid:88)

log

exk ≤ a +

log(1 + exk−a) 

(11)

k=1

k=1

where a is a new variational parameter. Combining with (8) we get the “quadratic bound” on the
integrand  with K + 1 variational parameters. This has conjugate updates  so modularity can be
achieved without NCVMP  but as we will see  results are often poor. [5] derives coordinate ascent
ﬁxed point updates to optimise a  but reducing to a univariate optimisation in a and using Newton’s
method is much faster (see supplementary material).
Inspired by the univariate “tilted” bound in Equation 9 we propose the multivariate tilted bound:

(cid:104)log(cid:88)

i

exi(cid:105) ≤ 1
2

(cid:88)

j

j vj + log(cid:88)

a2

i

emi+(1−2ai)vi/2

(12)

σ(cid:2)m + 1

Setting ak = 0 for all k we recover Equation 10 (hence this is the “tilted” version). Maximisation
with respect to a can be achieved by the ﬁxed point update (see supplementary material): a :=

2(1 − 2a) · v(cid:3). This is a O(K) operation since the denominator of the softmax function

is shared. For the softmax factor quadrature is not viable because of the high dimensionality of the
integrals. From Equation 7 the NCVMP messages using the tilted bound have natural parameters

5

= mk
vkf

vkf

= (d. − K)ak(1 − ak)  mkf

+ dk − 1 − (d. − K)ak where we have assumed a has
1
vkf
been optimised. As an alternative we suggest choosing whether to send the message resulting from
the quadratic bound or tilted bound depending on which is currently the tightest  referred to as the
“adaptive” method. Finally we consider a simple Taylor series expansion of the integrand around
the mean of x  denoted “Taylor”  and the multivariate quadratic bound of [4]  denoted “Bohning”
(see the Supplementary material for details).

6 Results

Here we aim to present the typical compromise between performance and modularity that NCVMP
addresses. We will see that for both binary logistic and multinomial softmax models achieving
conjugate updates by being constrained to quadratic bounds is sub-optimal  in terms of estimates of
variational parameters  marginal likelihood estimation  and predictive performance. NCVMP gives
the freedom to choose a wider class of bounds  or even use efﬁcient quadrature methods in the
univariate case  while maintaining simplicity and modularity.

6.1 The logistic factor

We ﬁrst test the logistic factor methods of Section 5.1 at the task of estimating the toy model
σ(x)π(x) with varying Gaussian prior π(x) (see Figure 1(a)). We calculate the true mean and vari-
ance using quadrature. The quadratic bound has the largest errors for the posterior mean  and the
posterior variance is severely underestimated. In contrast  NCVMP using quadrature  while being
slightly more computationally costly  approximates the posterior much more accurately: the error
here is due only to the VB approximation. Using the tilted bound with NCVMP gives more robust
estimates of the variance than the quadratic bound as the prior mean changes. However  both the
quadratic and tilted bounds underestimate the variance as the prior variance increases.

(a) Posterior mean and variance estimates of σ(x)π(x) with varying
prior π(x). Left: varying the prior mean with ﬁxed prior variance
v = 10. Right: varying the prior variance with ﬁxed prior mean
m = 0.

(b) Log likelihood of the true regres-
sion coefﬁcients under the approxi-
mate posterior for 10 synthetic logistic
regression datasets.

Figure 1: Logistic regression experiments.

6.2 Binary logistic regression

We generated ten synthetic logistic regression datasets with N = 30 data points and P = 8 co-
variates. We evaluated the results in terms of the log likelihood of the true regression coefﬁcients
under the approximate posterior  a measure which penalises poorly estimated posterior variances.
Figure 1(b) compares the performance of non-conjugate VMP using quadrature and VMP using
the quadratic bound. For four of the ten datasets the quadratic bound ﬁnds very poor solutions.
Non-conjugate VMP ﬁnds a better solution in seven out of the ten datasets  and there is marginal

6

−20−1001020priormean−0.3−0.2−0.10.00.10.20.3errorinposteriormean05101520priorvariance−0.6−0.5−0.4−0.3−0.2−0.10.00.1errorinposteriormeanNCVMPquadNCVMPtiltedVMPquadratic−20−1001020priormean−3.5−3.0−2.5−2.0−1.5−1.0−0.50.0errorinposteriorvariance05101520priorvariance−4−3−2−10errorinposteriorvariancedifference in the other three. Non-conjugate VMP (with no damping) also converges faster in gen-
eral  although some oscillation is seen for one of the datasets.

6.3 Softmax bounds

To have some idea how the various bounds for the softmax integral Eq[log(cid:80)K
(cid:81)
k=1 exk] com-
pare empirically we calculated relative absolute error on 100 random distributions q(x) =
k N(xk; mk  v). We sample mk ∼ N(0  u). When not being varied  K = 10  u = 1  v = 1.
Ground truth was calculated using 105 Monte Carlo samples. We vary the number of classes  K  the
distribution variance v and spread of the means u. Results are shown in Figure 2. As expected the
tilted bound (12) dominates the log bound (10)  since it is a generalisation. As K is increased the
relative error made using the quadratic bound increases  whereas both the log and the tilted bound
get tighter. In agreement with [5] we ﬁnd the strength of the quadratic bound (11) is in the high
variance case  and Bohning’s bound [4] is very loose under all conditions. Both the log and tilted
bound are extremely accurate for variances v < 1. In fact  the log and tilted bounds are asymp-
totically optimal as v → 0. “Taylor” gives accurate results but is not a bound  so convergence is
not guaranteed and the global bound on the marginal likelihood is lost. The spread of the means
u does not have much of an effect on the tightness of these bounds. These results show that even
when quadrature is not an option  much tighter bounds can be found if the constraint of requiring
quadratic bounds imposed by VMP is relaxed. For the remainder of the paper we consider only the
quadratic  log and tilted bounds.

Figure 2: Log10 of the relative absolute error approximating E log(cid:80) exp  averaged over 100 runs.

6.4 Multinomial softmax regression

Synthetic data. For synthetic data sampled from the generative model we know the ground truth
coefﬁcients and can control characteristics of the data. We ﬁrst investigate the performance with
sample size N  with ﬁxed number of features P = 6  classes K = 4  and no noise (apart from
the inherent noise of the softmax function). As expected our ability to recover the ground truth
regression coefﬁcients improves with increasing N (see Figure 3(a)  left). However  we see that
the methods using the tilted bound perform best  closely followed by the log bound. Although the
quadratic bound has comparable performance for small N < 200 it performs poorly with larger
N due to its weakness at small variances. The choice of bound impacts the speed of convergence
(see Figure 3(a)  right). The log bound performed almost as well as the tilted bound at recovering
coefﬁcients it takes many more iterations to converge. The extra ﬂexibility of the tilted bound allows
faster convergence  analogous to parameter expansion [16]. For small N the tilted bound  log bound
and adaptive method converge rapidly  but as N increases the quadratic bound starts to converge
much more slowly  as do the tilted and adaptive methods to a lesser extent. “Adaptive” converges
fastest because the quadratic bound gives good initial updates at high variance  and the tilted bound
takes over once the variance decreases. We vary the level of noise in the synthetic data  ﬁxing
N = 200  in Figure 3(b). For all but very large noise values the tilted bound performs best.

UCI datasets. We test the multinomial regression model on three standard UCI datasets: Iris (N =
150  D = 4  K = 3)  Glass (N = 214  D = 8  K = 6) and Thyroid (N = 7200  D = 21  K = 3) 

7

101102103classesK−10−8−6−4−2024log(relativeabserror)10−1100101102inputvariancev−12−10−8−6−4−202quadraticlogtiltedBohningTaylor10−1100101meanvarianceu−7−6−5−4−3−2−10(a) Varying sample size

(b) Varying noise level

Figure 3: Left: root mean squared error of inferred regression coefﬁcients. Right: iterations to
convergence. Results are shown as quartiles on 16 random synthetic datasets. All the bounds except
“quadratic” were ﬁt using NCVMP.
Iris
Quadratic
Probit
−65 ± 3.5
−37.3 ± 0.79
Marginal likelihood
Predictive likelihood −0.216 ± 0.07
−0.215 ± 0.034
0.0892 ± 0.039
0.0592 ± 0.03
Predictive error
Glass
Quadratic
Probit
−319 ± 5.6
−201 ± 2.6
Marginal likelihood
Predictive likelihood −0.58 ± 0.12
−0.503 ± 0.095
0.197 ± 0.032
0.195 ± 0.035
Predictive error
Thyroid
Quadratic
Probit
−1814 ± 43
−840 ± 18
Marginal likelihood
Predictive likelihood −0.114 ± 0.019 −0.0793 ± 0.014 −0.0753 ± 0.008 −0.0916 ± 0.010
0.0241 ± 0.0026
0.0276 ± 0.0028
Predictive error

Tilted
−31.2 ± 2
−0.201 ± 0.039
0.065 ± 0.038
Tilted
−193 ± 5.4
−0.531 ± 0.1
0.200 ± 0.032
Tilted
−916 ± 31
0.0226 ± 0.0023

Adaptive
−31.2 ± 2
−0.201 ± 0.039
0.0642 ± 0.037
Adaptive
−193 ± 3.9
−0.542 ± 0.11
0.200 ± 0.032
Adaptive
−909 ± 30
0.0225 ± 0.0024

Table 1: Average results and standard deviations on three UCI datasets  based on 16 random 50 : 50
training-test splits. Adaptive and tilted use NCVMP  quadratic and probit use VMP.

see Table 1. Here we have also included “Probit”  corresponding to a Bayesian multinomial probit
regression model  estimated using VMP  and similar in setup to [6]  except that we use EP to approx-
imate the predictive distribution  rather than sampling. On all three datasets the marginal likelihood
calculated using the tilted or adaptive bounds is optimal out of the logistic models (“Probit” has a
different underlying model  so differences in marginal likelihood are confounded by the Bayes fac-
tor). In terms of predictive performance the quadratic bound seems to be slightly worse across the
datasets  with the performance of the other methods varying between datasets. We did not compare
to the log bound since it is dominated by the tilted bound and is considerably slower to converge.

7 Discussion

f→i(xi)α where mold

NCVMP is not guaranteed to converge. Indeed  for some models we have found convergence to be
a problem  which can be alleviated by damping: if the NCVMP message is mf→i(xi) then send
the message mf→i(xi)1−αmold
f→i(xi) was the previous message sent to i and
0 ≤ α < 1 is a damping factor. The ﬁxed points of the algorithm remained unchanged.
We have introduced Non-conjugate Variational Message Passing  which extends variational Bayes
to non-conjugate models while maintaining the convenient message passing framework of VMP and
allowing freedom to choose the most accurate available method to approximate required expecta-
tions. Deterministic and stochastic factors can be combined in a modular fashion  and conjugate
parts of the model can be handled with standard VMP. We have shown NCVMP to be of practical
use for ﬁtting Bayesian binary and multinomial logistic models. We derived a new bound for the
softmax integral which is tighter than other commonly used bounds  but has variational parameters
that are still simple to optimise. Tightness of the bound is valuable both in terms of better approxi-
mating the posterior and giving a closer approximation to the marginal likelihood  which may be of
interest for model selection.

8

101102103samplesizeN0.00.20.40.60.8RMSerrorofcoefﬁcentsadaptivetiltedquadraticlog101102103samplesizeN01020304050Iterationstoconvergence10−310−210−1100syntheticnoisevariance0.200.250.300.350.400.450.500.55RMSerrorofcoefﬁcents10−310−210−1100syntheticnoisevariance01020304050IterationstoconvergenceReferences
[1] H. Attias. A variational Bayesian framework for graphical models. Advances in neural infor-

mation processing systems  12(1-2):209215  2000.

[2] M. Beal and Z. Ghahramani. Variational Bayesian learning of directed graphical models with

hidden variables. Bayesian Analysis  1(4):793832  2006.

[3] D. Blei and J. Lafferty. A correlated topic model of science. Annals of Applied Statistics  2007.
[4] D. Bohning. Multinomial logistic regression algorithm. Annals of the Institute of Statistical

Mathematics  44:197–200  1992. 10.1007/BF00048682.

[5] G. Bouchard. Efﬁcient bounds for the softmax and applications to approximate inference in

hybrid models. In NIPS workshop on approximate inference in hybrid models  2007.

[6] M. Girolami and S. Rogers. Variational bayesian multinomial probit regression with gaussian

process priors. Neural Computation  18(8):1790–1817  2006.

[7] A. Honkela  T. Raiko  M. Kuusela  M. Tornio  and J. Karhunen. Approximate riemannian
conjugate gradient learning for ﬁxed-form variational bayes. Journal of Machine Learning
Research  11:3235–3268  2010.

[8] A. Honkela  M. Tornio  T. Raiko  and J. Karhunen. Natural conjugate gradient in variational
inference. In M. Ishikawa  K. Doya  H. Miyamoto  and T. Yamakawa  editors  ICONIP (2) 
volume 4985 of Lecture Notes in Computer Science  pages 305–314. Springer  2007.

[9] T. S. Jaakkola and M. I. Jordan. A variational approach to bayesian logistic regression models
and their extensions. In International Conference on Artiﬁcial Intelligence and Statistics  1996.
[10] M. E. Khan  B. M. Marlin  G. Bouchard  and K. P. Murphy. Variational bounds for mixed-data

factor analysis. In Advances in Neural Information Processing (NIPS) 23  2010.

[11] B. M. Marlin  M. E. Khan  and K. P. Murphy. Piecewise bounds for estimating bernoulli-
logistic latent gaussian models. In Proceedings of the 28th Annual International Conference
on Machine Learning  2011.

[12] T. P. Minka. Expectation propagation for approximate bayesian inference. In Uncertainty in

Artiﬁcial Intelligence  volume 17  2001.

[13] T. P. Minka  J. M. Winn  J. P. Guiver  and D. A. Knowles. Infer.NET 2.4  2010. Microsoft

Research Cambridge. http://research.microsoft.com/infernet.

[14] H. Nickisch and C. E. Rasmussen. Approximations for binary gaussian process classiﬁcation.

Journal of Machine Learning Research  9:2035–2078  Oct. 2008.

[15] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural

Computation  21(3):786–792  2009.

[16] Y. A. Qi and T. Jaakkola. Parameter expanded variational bayesian methods. In B. Sch¨olkopf 
J. C. Platt  and T. Hoffman  editors  Advances in Neural Information Processing (NIPS) 19 
pages 1097–1104. MIT Press  2006.

[17] T. Raiko  H. Valpola  M. Harva  and J. Karhunen. Building blocks for variational bayesian
learning of latent variable models. Journal of Machine Learning Research  8:155–201  2007.
[18] L. K. Saul and M. I. Jordan. A mean ﬁeld learning algorithm for unsupervised neural networks.

Learning in graphical models  1999.

[19] M. P. Wand  J. T. Ormerod  S. A. Padoan  and R. Fruhwirth. Variational bayes for elaborate

distributions. In Workshop on Recent Advances in Bayesian Computation  2010.

[20] J. Winn and C. M. Bishop. Variational message passing. Journal of Machine Learning Re-

search  6(1):661  2006.

9

,Ashok Cutkosky
Róbert Busa-Fekete