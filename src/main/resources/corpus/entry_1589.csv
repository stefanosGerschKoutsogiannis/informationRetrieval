2019,Distributional Policy Optimization: An Alternative Approach for Continuous Control,We identify a fundamental problem in policy gradient-based methods in continuous control. As policy gradient methods require the agent's underlying probability distribution  they limit policy representation to parametric distribution classes. We show that optimizing over such sets results in local movement in the action space and thus convergence to sub-optimal solutions. We suggest a novel distributional framework  able to represent arbitrary distribution functions over the continuous action space. Using this framework  we construct a generative scheme  trained using an off-policy actor-critic paradigm  which we call the Generative Actor Critic (GAC). Compared to policy gradient methods  GAC does not require knowledge of the underlying probability distribution  thereby overcoming these limitations. Empirical evaluation shows that our approach is comparable and often surpasses current state-of-the-art baselines in continuous domains.,Distributional Policy Optimization:

An Alternative Approach for Continuous Control

Chen Tessler∗  Guy Tennenholtz∗ and Shie Mannor

chen.tessler@campus.technion.ac.il  guytenn@gmail.com  shie@ee.technion.ac.il

Technion Institute of Technology  Haifa  Israel

∗ Equal Contribution

Abstract

We identify a fundamental problem in policy gradient-based methods in continu-
ous control. As policy gradient methods require the agent’s underlying probability
distribution  they limit policy representation to parametric distribution classes. We
show that optimizing over such sets results in local movement in the action space
and thus convergence to sub-optimal solutions. We suggest a novel distributional
framework  able to represent arbitrary distribution functions over the continuous
action space. Using this framework  we construct a generative scheme  trained us-
ing an off-policy actor-critic paradigm  which we call the Generative Actor Critic
(GAC). Compared to policy gradient methods  GAC does not require knowledge
of the underlying probability distribution  thereby overcoming these limitations.
Empirical evaluation shows that our approach is comparable and often surpasses
current state-of-the-art baselines in continuous domains.

1

Introduction

Model-free Reinforcement Learning (RL) is a learning paradigm which aims to maximize a cumu-
lative reward signal based on experience gathered through interaction with an environment [Sutton
and Barto  1998]. It is divided into two primary categories. Value-based approaches involve learning
the value of each action and acting greedily with respect to it (i.e.  selecting the action with highest
value). On the other hand  policy-based approaches (the focus of this work) learn the policy directly 
thereby explicitly learning a mapping from state to action.
Policy gradients (PGs) [Sutton et al.  2000b] have been the go-to approach for learning policies
in empirical applications. The combination of the policy gradient with recent advances in deep
learning has enabled the application of RL in complex and challenging environments. Such domains
include continuous control problems  in which an agent controls complex robotic machines both in
simulation [Schulman et al.  2015  Haarnoja et al.  2017  Peng et al.  2018] as well as real life
[Levine et al.  2016  Andrychowicz et al.  2018  Riedmiller et al.  2018]. Nevertheless  there exists a
fundamental problem when PG methods are applied to continuous control regimes. As the gradients
require knowledge of the probability of the performed action P (a| s)  the PG is empirically limited
to parametric distribution functions. Common parametric distributions used in the literature include
the Gaussian [Schulman et al.  2015  2017]  Beta [Chou et al.  2017] and Delta [Silver et al.  2014 
Lillicrap et al.  2015  Fujimoto et al.  2018] distribution functions.
In this work  we show that while the PG is properly deﬁned over parametric distribution functions 
it is prone to converge to sub-optimal exterma (Section 3). The leading reason is that these distri-
butions are not convex in the distribution space1 and are thus limited to local improvement in the

1As an example  consider the Gaussian distribution  which is known to be non-convex.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

action space itself. Inspired by Approximate Policy Iteration schemes  for which convergence guar-
antees exist [Puterman and Brumelle  1979]  we introduce the Distributional Policy Optimization
(DPO) framework in which an agent’s policy evolves towards a distribution over improving actions.
This framework requires the ability to minimize a distance (loss function) which is deﬁned over two
distributions  as opposed to the policy gradient approach which requires an explicit differentiation
through the density function.
DPO establishes the building blocks for our generative algorithm  the Generative Actor Critic2. It is
composed of three elements: a generative model which represents the policy  a value  and a critic.
The value and the critic are combined to obtain the advantage of each action. A target distribution is
then deﬁned as one which improves the value (i.e.  all actions with negative advantage receive zero
probability mass). The generative model is optimized directly from samples without the explicit
deﬁnition of the underlying probability distribution using quantile regression and Autoregressive
Implicit Quantile Networks (see Section 4). Generative Actor Critic is evaluated on tasks in the
MuJoCo control suite (Section 5)  showing promising results on several difﬁcult baselines.

2 Preliminaries

We consider an inﬁnite-horizon discounted Markov Decision Process (MDP) with a continuous
action space. An MDP is deﬁned as the 5-tuple (S A  P  r  γ) [Puterman  1994]  where S is a
countable state space  A the continuous action space  P : S × S × A (cid:55)→ [0  1] is a transition kernel 
r : S × A → [0  1] is a reward function  and γ ∈ (0  1) is the discount factor. Let π : S (cid:55)→ B(A) be
a stationary policy  where B(A) is the set of probability measures on the Borel sets of A. We denote
by Π the set of stationary stochastic policies. In addition to Π  often one is interested in optimizing
over a set of parametric distributions. We denote the set of possible distribution parameters by Θ
(e.g.  the mean µ and variance σ of a Gaussian distribution).
in RL are the value and action-value functions vπ ∈ R|S| and
Two measures of interest
Qπ ∈ R|S|×|A|  respectively. The value of a policy π  starting at state s and performing action a
t=0 γtr(st  at) | s0 = s  a0 = a]. The value function is then de-
ﬁned by vπ = Eπ[Qπ(s  a)]. Given the action-value and value functions  the advantage of an action
a ∈ A at state s ∈ S is deﬁned by Aπ(s  a) = Qπ(s  a) − vπ(s). The optimal policy is deﬁned by
π∗ = arg maxπ∈Π vπ and the optimal value by v∗ = vπ∗

is deﬁned by Qπ(s  a) = Eπ [(cid:80)∞

.

3 From Policy Gradient to Distributional Policy Optimization

Current practical approaches leverage the Policy Gradient Theorem [Sutton et al.  2000b] in order
to optimize a policy  which updates the policy parameters according to

θk+1 = θk + αkE

s∼d(πθk )

Ea∼πθk (·| s)∇θ log πθ(a| s) |θ=θk Qπθk (s  a)  

(1)

where d (π) is the stationary distribution of states under π. Since this update rule requires knowl-
edge of the log probability of each action under the current policy log πθ(a| s)  empirical methods
in continuous control resort to parametric distribution functions. Most commonly used are the Gaus-
sian [Schulman et al.  2017]  Beta [Chou et al.  2017] and deterministic Delta [Lillicrap et al.  2015]
distribution functions. However  as we show in Proposition 1  this approach is not ensured to con-
verge  even though there exists an optimal policy which is deterministic (i.e.  Delta) - a policy which
is contained within this set.
The sub-optimality of uni-modal policies such as Gaussian or Delta distributions does not occur due
to the limitation induced by their parametrization (e.g.  the neural network)  but is rather a result of
the predeﬁned set of policies. As an example  consider the set of Delta distributions. As illustrated
in Figure 1  while this set is convex in the parameter µ (the mean of the distribution)  it is not convex
in the set Π. This is due to the fact that (1−α)δµ1 +αδµ2 results in a stochastic distribution over two
supports  which cannot be represented using a single Delta function. Parametric distributions such
as Gaussian and Delta functions highlight this issue  as the policy gradient considers the gradient
w.r.t.
the parameters µ  σ. This results in local movement in the action space. Clearly such an
approach can only guarantee convergence to a locally optimal solution and not a global one.

2Code provided in the following anonymous repository: github.com/tesslerc/GAC

2

(b) Delta

(c) Gaussian

(a) Policy vs. Parameter Space

Figure 1: (a): A conceptual diagram comparing policy optimization in parameter space Θ (black
dots) in contrast to distribution space Π (white dots). Plots depict Q values in both spaces. As
parameterized policies are non-convex in the distribution space  they are prone to converge to a
local optima. Considering the entire policy space ensures convergence to the global optima. (b c):
Policy evolution of Delta and Gaussian parameterized policies for multi-modal problems.

(cid:107)v∗ − vπ∞(cid:107)∞ > L  

Proposition 1. For any initial Gaussian policy π0 ∼ N (µ0  Σ) and L ∈ [0  v∗
2 ) there exists an
MDP M such that π∞ satisﬁes
(2)
where π∞ is the convergent result of a PG method with step size bounded by α. Moreover  given M
the result follows even when µ0 is only known to lie in some ball of radius R around ˜µ0  BR(˜µ0).
Proof sketch. For brevity we prove for the case of a ∈ R  such that BR is a ﬁnite interval [a  b].
We also assume [a  b] ⊆ [µ0 − 2α  µ0 + 2α]  and σ → 0. The general case proof can be found in
the supplementary material. Let  > 0. We consider a single state MDP (i.e.  x-armed bandit) with
action space A = R and a multi-modal reward function (similar to the illustration in Figure 1b) 
deﬁned by

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) (Wµ0−2α µ0+2α + (1 − )Wµ0+2α µ0+6α)  

r(a) =

(a−µ0)

(cid:18) 2π

8α

(cid:12)(cid:12)(cid:12)(cid:12)cos
(cid:26)1

0

z ∈ [x  y]
else

where Wx y(z) =

is the window function.

d

let us consider the derivative with respect
dµ log πµ(a) |µ=µk = − 1

(cid:8)Ea∼N (µk σ)
that sign(cid:8)Ea∼N (µk σ) (a−µk) r(a)(cid:9) = sign(cid:8) d

In PG  we assume µ is parameterized by some parameters θ. Without
loss of general-
ity 
iteration k the deriva-
tive can be written as
thus update the pol-
it holds
icy parameter µ by µk+1 = µk + αk
3 and
µk ∈ [µ0 − 2α  µ0 + 2α] then so is µk+1. Then  µ∞ ∈ [µ0 − 2α  µ0 + 2α]. That is  the pol-
icy can never reach the interval [µ0 + 2α  µ0 + 6α] in which the optimal solution lies. Hence 
(cid:107)v∗ − vπ∞(cid:107)∞ = 1 − 2 and the result follows for  < 1
3.

At
2σ2 (µk − a) . PG will

2σ2 (a−µk) r(a)(cid:9) . As σ → 0 
(cid:9) . It follows that if  < 1

d a r(a) |a=µk

to θ = µ.

1

3.1 Distributional Policy Optimization (DPO)

In order to overcome issues present in parametric distribution functions  we consider an alterna-
tive approach. In our solution  the policy does not evolve based on the gradient w.r.t. distribution
parameters (e.g.  µ  σ)  but rather updates the policy distribution according to

πk+1 = Γ (πk − αk∇πd(Dπk

I πk   π) |π=πk )  

where Γ is a projection operator onto the set of distributions  d : Π × Π → [0 ∞) is a dis-
tance measure (e.g.  Wasserstein distance)  and Dπ
I π (s) is a distribution deﬁned over the support
I π(s) = {a : Aπ(s  a) > 0} (i.e.  the positive advantage). Table 1 provides examples of such distri-
butions.

3

(cid:16)

(cid:17)

Algorithm 1 Distributional Policy Optimization (DPO)
1: Input: learning rates αk (cid:29) βk (cid:29) δk
2: πk+1 = Γ
3: Qπ(cid:48)
4: vπ(cid:48)
5: π(cid:48)

(cid:16)
πk − αk∇πd(Dπ(cid:48)
  π) |π=πk
I π(cid:48)
(cid:16)
(cid:82)
r(s  a) + γvπ(cid:48)
k (s  a) + βk
k (s  a) − vπ(cid:48)
Qπ(cid:48)
A
k + δk(πk − π(cid:48)
k)

k+1(s  a) = Qπ(cid:48)
k+1(s) = vπ(cid:48)
k + βk
k+1 = π(cid:48)

k (s)

k

k

(cid:17)
k (s) − Qπ(cid:48)

k (s  a)

(cid:17)

Table 1: Examples of target distributions over the set of improving actions

Dπ
I π(s)(a| s) = δarg maxa∈I(π) Aπ(s a)(a| s)
Argmax
Dπ
I π(s)(a| s) = 1{a∈I π}
Linear
Iπ (s) Aπ(s a(cid:48))d a(cid:48)
I π(s)(a| s) = 1{a∈I π}
Boltzmann (β > 0) Dπ
exp( 1
Iπ (s) exp( 1
Dπ
I π(s)(a| s) = Uniform(I π(s))
Uniform

Aπ(s a)

(cid:82)
(cid:82)

β Aπ(s a))
β Aπ(s a(cid:48)))d a(cid:48)

Algorithm 1 describes the Distributional Policy Optimization (DPO) framework as a three time-
scale approach to learning the policy. It can be shown  under standard stochastic approximation
assumptions [Borkar  2009  Konda and Tsitsiklis  2000  Bhatnagar and Lakshmanan  2012  Chow
et al.  2017]  to converge to an optimal solution. DPO consists of 4 elements: (1) A policy π on a fast
timescale  (2) a delayed policy π(cid:48) on a slow timescale  (3) a value and (4) a critic  which estimate
the quality of the delayed policy π(cid:48) on an intermediate timescale. Unlike the PG approach  DPO
does not require access to the underlying p.d.f. In addition  π which is updated on the fast timescale
views the delayed policy π(cid:48)  the value and critic as quasi-static  and as such it can be optimized using
supervised learning techniques3. Finally  we note that in DPO  the target distribution Dπ(cid:48)
I π(cid:48) induces
a higher value than the current policy π(cid:48)  ensuring an always improving policy.
The concept of policy evolution using positive advantage is depicted in Figure 2. While the policy
starts as a uni-modal distribution  it is not restricted to this subset of policies. As the policy evolves 
less actions have positive advantage  and the process converges to an optimal solution. In the next
section we construct a practical algorithm under the DPO framework using a generative actor.

4 Method

In this section we present our method  the Generative Actor Critic  which learns a policy based on
the Distributional Policy Optimization framework (Section 3). Distributional Policy Optimization
requires a model which is both capable of representing arbitrarily complex distributions and can be
optimized by minimizing a distributional distance. We consider the Autoregressive Implicit Quantile
Network [Ostrovski et al.  2018]  which is detailed below.

4.1 Quantile Regression & Autoregressive Implicit Quantile Networks

As seen in Algorithm 1  DPO requires the ability to minimize a distance between two distributions.
The Implicit Quantile Network (IQN) [Dabney et al.  2018a] provides such an approach using the
Wasserstein metric. The IQN receives a quantile value τ ∈ [0  1] and is tasked at returning the
value of the corresponding quantile from a target distribution. As the IQN learns to predict the
value of the quantile  it allows one to sample from the underlying distribution (i.e.  by sampling
τ ∼ U ([0  1]) and performing a forward pass). Learning such a model requires the ability to estimate
the quantiles. The quantile regression loss [Koenker and Hallock  2001] provides this ability. It is
given by ρτ (u) = (τ − 1{u ≤ 0})u  where τ ∈ [0  1] is the quantile and u the error.

3Assuming the target distribution is ’ﬁxed’  the policy π can be trained using a supervised learning loss 

e.g.  GAN  VAE or AIQN.

4

(a) π0

(c) π2

(b) π1

(d) πk

Figure 2: Policy evolution of a general  non-parametric policy  where the target policy is a distribu-
tion over the actions with positive advantage. The horizontal dashed line denotes the current value
of the policy  the colored green region denotes the target distribution (i.e.  the actions with a positive
advantage) and πk denotes the policy after multiple updates. As opposed to Delta and Gaussian
distributions  the ﬁxed point of this approach is the optimal policy.

likelihoods FX(x) = P(cid:0)X 1 ≤ x1  . . .   X n ≤ xn(cid:1) = Πn

Nevertheless  the IQN is only capable of coping with univariate (scalar) distribution functions. Os-
trovski et al. [2018] proposed to extend the IQN to the multi-variate case using quantile autore-
gression [Koenker and Xiao  2006]. Let X = (X1  . . .   Xk) be an n-dimensional random variable.
Given a ﬁxed ordering of the n dimensions  the c.d.f. can be written as the product of conditional
i=1FX i|X i−1 ... X 1(xi) . The Autoregres-
sive Implicit Quantile Network (AIQN)  receives an i.i.d. vector τ ∼ U ([0  1]n). The network
architecture then ensures each output dimension xi is conditioned on the previously generated val-
ues x1  . . .   xi−1; trained by minimizing the quantile regression loss.

4.2 Generative Actor Critic (GAC)

Next  we introduce a practical implementation of the DPO framework. As shown in Section 3 
DPO is composed of 4 elements: an actor  a delayed actor  a value  and an action-value estimator.
The Generative Actor Critic (GAC) uses a generative actor trained using an AIQN  as described
below. Contrary to parametric distribution functions  a generative neural network acts as a universal
function approximator  enabling us to represent arbitrarily complex distributions  as corollary of the
following lemma.
Lemma (Kernels and Randomization [Kallenberg  2006]). Let π be a probability kernel from a mea-
surable space S to a Borel space A. Then there exists some measurable function f : S × [0  1] → A
such that if θ is U (0  1)  then f (s  θ) has distribution π(a| s) for every s ∈ S.
Actor: DPO deﬁnes the actor as one which is capable of representing arbitrarily complex policies.
To obtain this we construct a generative neural network  an AIQN. The AIQN learns a mapping
from a sampled noise vector τ ∼ U ([0  1]n) to a target distribution.
As illustrated in Figure 3  the actor network contains a recurrent cell which enables sequential gen-
eration of the action. This generation schematic ensures the autoregressive nature of the model.
Each generated action dimension is conditioned only on the current sampled noise scalar τ i and the
previous action dimensions ai−1  . . .   a1. In order to train the generative actor  the AIQN requires
the ability to produce samples from the target distribution Dπ(cid:48)
I π(cid:48) . Although we are unable to sample
from this distribution  given an action  we are able to estimate its probability. An unbiased estima-
tor of the loss can be attained by uniformly sampling actions and then multiplying them by their
corresponding weight. More speciﬁcally  the weighted autoregressive quantile loss is deﬁned by

Dπ(cid:48)
I π(cid:48) (aj | s)

j −πφ(τ i
(ai

j| ai−1

j

ρk
τ i
j

  . . .   a1

j ))  

(3)

(cid:88)

aj∼U (A)

n(cid:88)

i=1

j is the ith coordinate of action aj  and ρk
τ i
j

where ai
et al.  2018b]. Estimation of I π(cid:48)

is the Huber quantile loss [Huber  1992  Dabney
in the target distribution is obtained using the estimated advantage.

5

Delayed Actor: The delayed actor  also known as Polyak aver-
aging [Polyak  1990]  is an appealing requirement as it is com-
mon in off-policy actor-critic schemes [Lillicrap et al.  2015].
The delayed actor is an additional AIQN πθ(cid:48)  which tracks πθ.
k+1 = (1− α)θ(cid:48)
It is updated based on θ(cid:48)
k + αθk and is used for
training the value and critic networks.
Value and Action-Value: While it is possible to train a critic
and use its empirical mean w.r.t.
the policy as a value esti-
mate  we found it to be noisy  resulting in bad convergence.
We therefore train a value network to estimate the expectation
of the critic w.r.t. the delayed policy. In addition  as suggested
in Fujimoto et al. [2018]  we train two critic networks in par-
allel. During both policy and value updates  we refer to the
minimal value of the two critics. We observed that this indeed
reduced variance and improved overall performance.
To summarize  GAC combines 4 elements. The delayed actor
tracks the actor using a Polyak averaging scheme. The value
and critic networks estimate the performance of the delayed
actor. Provided Q and v estimations  we are able to estimate
the advantage of each action and thus propose the weighted
autoregressive quantile loss  used to train the actor network.
We refer the reader to the supplementary material for an exhaustive overview of the algorithm and
architectural details.

Figure 3: Illustration of the actor’s
architecture. ⊗ is the hadamard
product  ⊕ a concatenation opera-
tor  and ψ a mapping [0  1] (cid:55)→ Rd.

5 Experiments

In order to evaluate our approach  we test GAC on a variety of continuous control tasks in the
MuJoCo control suite [Todorov et al.  2012]. The agents are composed of n joints: from 2 joints
in the simplistic Swimmer task and up to 17 in the Humanoid robot task. The state is a vector
representation of the agent  containing the spatial location and angular velocity of each element.
The action is a continuous n dimensional vector  representing how much torque to apply to each
joint. The task in these domains is to move forward as much as possible within a given time-limit.
We run each task for 1 million steps and  as GAC is an off-poicy approach  evaluate the policy
every 5000 steps and report the average over 10 evaluations. We train GAC using a batch size
of 128 and uncorrelated Gaussian noise for exploration. Results are depicted in Figure 4. Each
curve presented is a product of 5 training procedures with a randomly sampled seed. In addition to
our raw results  we compare to the relevant baselines4  including: (1) DDPG [Lillicrap et al.  2015] 
(2) TD3 [Fujimoto et al.  2018]  an off-policy actor critic approach which represents the policy using
a deterministic delta distribution  and (3) PPO [Schulman et al.  2017]  an on-policy method which
represents the policy using a Gaussian distribution.
As we have shown in the previous sections  DPO and GAC only require some target distribution to
be deﬁned  namely  a distribution over actions with positive advantage. In our results we present
two such distributions: the linear and Boltzmann distributions (see Table 1). We also test a non-
autoregressive version of our model 5 using an IQN. For completeness  we provide additional dis-
cussion regarding the various parameters and how they performed  in addition to a pseudo-code
illustration of our approach  in the supplementary material.
Comparison to the policy gradient baselines: Results in Figure 4 show the ability of GAC to
solve complex  high dimensional problems. GAC attains competitive results across all domains 
often outperforming the baseline policy gradient algorithms and exhibiting lower variance. This is
somewhat surprising  as GAC is a vanila algorithm  it is not supported by numerous improvements
apparent in recent PG methods. In addition to these results  we provide numerical results in the
supplementary material  which emphasize this claim.

4We use the implementations of DDPG and PPO from the OpenAI baselines repo [Dhariwal et al.  2017] 

and TD3 [Fujimoto et al.  2018] from the authors GitHub repository.

5Theoretically  the dimensions of the actions may be correlated and thus should be represented using an

auto-regressive model.

6

Figure 4: Training curves on continuous control benchmarks. For the Generative Actor Critic ap-
proach we present both the Autoregressive and Non-autoregressive approaches  the exact hyperpa-
rameters for each domain are provided in the appendix.

Table 2: Relative best GAC results compared to the best policy gradient baseline

Environment
Relative Result +3447 (+595%) +533 (+14%) +467 (+17%)

Humanoid-v2

Walker2d-v2

Hopper-v2

HalfCheetah-v2
−381 (−4%)

Ant-v2

Swimmer-v2
−444 (−8%) +107 (+81%)

Parameter Comparison: Below we discuss how various parameters affect the behavior of GAC in
terms of convergence rates and overall performance:

1. At each step  the target policy is approximated through samples using the weighted quan-
tile loss (Equation (3)). The results presented in Figure 4 are obtained using 32 (256 for
HalfCheetah and Walker) samples at each step. 32 (128) samples are taken uniformly over
the action space and 32 (128) from the delayed policy π(cid:48) (a form of combining exploration
and exploitation). Ablation tests showed that increasing the number of samples improved
stability and overall performance. Moreover  we observed that the combination of both
sampling methods is crucial for success.

2. Not presented is the Uniform distribution  which did not work well. We believe this is due
to the fact that the Uniform target provides an equal weight to actions which are very good
while also to those which barely improve the value.

3. We observed that in most tasks  similar to the observations of Korenkevych et al. [2019] 

the AIQN model outperforms the IQN (non-autoregressive) one.

6 Related Work

Distributional RL: Recent interest in distributional methods for RL has grown with the introduction
of deep RL approaches for learning the distribution of the return. Bellemare et al. [2017] presented
the C51-DQN which partitions the possible values [−vmax  vmax] into a ﬁxed number of bins and
estimates the p.d.f. of the return over this discrete set. Dabney et al. [2017] extended this work by
representing the c.d.f. using a ﬁxed number of quantiles. Finally  Dabney et al. [2018a] extended the
QR-DQN to represent the entire distribution using the Implicit Quantile Network (IQN). In addition
to the empirical line of work  Qu et al. [2018] and Rowland et al. [2018] have provided fundamental
theoretical results for this framework.
Generative Modeling: Generative Adversarial Networks (GANs) [Goodfellow et al.  2014] com-
bine two neural networks in a game-theoretic approach which attempt to ﬁnd a Nash Equilbirium.
This equilibrium is found when the generative model is capable of “fooling” the discriminator (i.e. 
the discriminator is no longer capable of distinguishing between samples produced from the real
distribution and those from the generator). Multiple GAN models and training methods have been
introduced  including the Wasserstein-GAN [Arjovsky et al.  2017] which minimizes the Wasser-
stein loss. However  as the optimization scheme is highly non-convex  these approaches are not
proven to converge and may thus suffer from instability and mode collapse [Salimans et al.  2016].

7

Policy Learning: Learning a policy is generally performed using one of two methods. The Policy
Gradient (PG) [Williams  1992  Sutton et al.  2000a] deﬁnes the gradient as the direction which
maximizes the reward under the assumed policy parametrization class. Although there have been a
multitude of improvements  including the ability to cope with deterministic policies [Silver et al. 
2014  Lillicrap et al.  2015]  stabilize learning through trust region updates [Schulman et al.  2015 
2017] and bayesian approaches [Ghavamzadeh et al.  2016]  these methods are bounded to para-
metric distribution sets (as the gradient is w.r.t.
the log probability of the action). An alternative
line of work formulates the problem as a maximum entropy [Haarnoja et al.  2018]  this enables the
deﬁnition of the target policy using an energy functional. However  training is performed via mini-
mizing the KL-divergence. The need to know the KL-divergence limits practical implementation to
parametric distributions functions  similar to PG methods.

7 Discussion and Future Work

In this work we presented limitations inherent to empirical Policy Gradient (PG) approaches in
continuous control. While current PG methods in continuous control are computationally efﬁcient 
they are not ensured to converge to a global extrema. As the policy gradient is deﬁned w.r.t. the log
probability of the policy  the gradient results in local changes in the action space (e.g.  changing the
mean and variance of a Gaussian policy). These limitations do not occur in discrete action spaces.
In order to ensure better asymptotic results  it is often needed to use methods that are more complex
and computationally demanding (i.e.  “No Free Lunch” [Wolpert et al.  1997]). Existing approaches
attempting to mitigate these issues  either enrich the policy space using mixture models  or discretize
the action space. However  while the discretization scheme is appealing  there is a clear trade-off
between optimality and efﬁciency. While ﬁner discretization improves guarantees  the complexity
(number of discrete actions) grows exponentially in the action dimension [Tang and Agrawal  2019].
Similar to the limitations inherent in PG approaches  these limitations also exist when considering
mixture models  such as Gaussian Mixtures. A mixture model of k-Gaussians provides a categorical
distribution over k Gaussian distributions. The policy gradient w.r.t. these parameters  similarly to
the single Gaussian model  directly controls the mean µ and variance σ of each Gaussian indepen-
dently. As such  even a mixture model is conﬁned to local improvement in the action space.
In practical scenarios  and as the number of Gaussians grows  it is likely that the modes of the mix-
ture would be located in a vicinity of a global optima. A Gaussian Mixture model may therefore
be able to cope with various non-convex continuous control problems. Nevertheless  we note that
Gaussian Mixture models  unlike a single Gaussian  are numerically unstable. Due to the summation
over Gaussians  the log probability of a mixture of Gaussians does not result in a linear representa-
tion. This can cause numerical instability  and thus hinder the learning process. These insights lead
us to question the optimality of current PG approaches in continuous control  suggesting that  al-
though these approaches are well understood  there is room for research into alternative policy-based
approaches.
In this paper we suggested the Distributional Policy Optimization (DPO) framework and its empir-
ical implementation - the Generative Actor Critic (GAC). We evaluated GAC on a series of con-
tinuous control tasks under the MuJoCo control suite. When considering overall performance  we
observed that despite the algorithmic maturity of PG methods  GAC attains competitive performance
and often outperforms the various baselines. Nevertheless  as noted above  there is “no free lunch”.
While GAC remains as sample efﬁcient as the current PG methods (in terms of the batch size during
training and number of environment interactions)  it suffers from high computational complexity.
Finally  the elementary framework presented in this paper can be extended in various future research
directions. First  improving the computational efﬁciency is a top priority for GAC to achieve de-
ployment in real robotic agents. In addition  as the target distribution is deﬁned w.r.t. the advantage
function  future work may consider integrating uncertainty estimates in order to improve exploration.
Moreover  PG methods have been thoroughly researched and many of their improvements  such as
trust region optimization [Schulman et al.  2015]  can be adapted to the DPO framework. Finally 
DPO and GAC can be readily applied to other well-known frameworks such as the Soft-Actor-Critic
[Haarnoja et al.  2018]  in which entropy of the policy is encouraged through an augmented reward
function. We believe this work is a ﬁrst step towards a principal alternative for RL in continuous
action space domains.

8

8 Acknowledgement

We thank Yonathan Efroni for his fruitful comments that greatly improved this paper.

References
Marcin Andrychowicz  Bowen Baker  Maciek Chociej  Rafal Jozefowicz  Bob McGrew  Jakub Pa-
chocki  Arthur Petron  Matthias Plappert  Glenn Powell  Alex Ray  et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177  2018.

Martin Arjovsky  Soumith Chintala  and L´eon Bottou. Wasserstein generative adversarial networks.

In International Conference on Machine Learning  pages 214–223  2017.

Marc G Bellemare  Will Dabney  and R´emi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 449–458. JMLR. org  2017.

Shalabh Bhatnagar and K Lakshmanan. An online actor–critic algorithm with function approxima-
tion for constrained markov decision processes. Journal of Optimization Theory and Applications 
153(3):688–708  2012.

Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint  volume 48. Springer 

2009.

Po-Wei Chou  Daniel Maturana  and Sebastian Scherer. Improving stochastic policy gradients in
continuous control with deep reinforcement learning using the beta distribution. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70  pages 834–843. JMLR.
org  2017.

Yinlam Chow  Mohammad Ghavamzadeh  Lucas Janson  and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. The Journal of Machine Learning Research  18
(1):6070–6120  2017.

Will Dabney  Mark Rowland  Marc G Bellemare  and R´emi Munos. Distributional reinforcement

learning with quantile regression. arXiv preprint arXiv:1710.10044  2017.

Will Dabney  Georg Ostrovski  David Silver  and R´emi Munos.

Implicit quantile networks for

distributional reinforcement learning. arXiv preprint arXiv:1806.06923  2018a.

Will Dabney  Mark Rowland  Marc G Bellemare  and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
2018b.

Prafulla Dhariwal  Christopher Hesse  Oleg Klimov  Alex Nichol  Matthias Plappert  Alec Radford 
John Schulman  Szymon Sidor  Yuhuai Wu  and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines  2017.

Scott Fujimoto  Herke van Hoof  and David Meger. Addressing function approximation error in

actor-critic methods. arXiv preprint arXiv:1802.09477  2018.

Mohammad Ghavamzadeh  Yaakov Engel  and Michal Valko. Bayesian policy gradient and actor-

critic algorithms. The Journal of Machine Learning Research  17(1):2319–2371  2016.

Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair 
Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems  pages 2672–2680  2014.

Tuomas Haarnoja  Haoran Tang  Pieter Abbeel  and Sergey Levine. Reinforcement learning with
In Proceedings of the 34th International Conference on Machine

deep energy-based policies.
Learning-Volume 70  pages 1352–1361. JMLR. org  2017.

Tuomas Haarnoja  Aurick Zhou  Pieter Abbeel  and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning  pages 1856–1865  2018.

9

Peter J Huber. Robust estimation of a location parameter.

492–518. Springer  1992.

In Breakthroughs in statistics  pages

Olav Kallenberg. Foundations of modern probability. Springer Science & Business Media  2006.

Roger Koenker and Kevin Hallock. Quantile regression: An introduction. Journal of Economic

Perspectives  15(4):43–56  2001.

Roger Koenker and Zhijie Xiao. Quantile autoregression. Journal of the American Statistical Asso-

ciation  101(475):980–990  2006.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information

processing systems  pages 1008–1014  2000.

Dmytro Korenkevych  A Rupam Mahmood  Gautham Vasan  and James Bergstra. Autoregressive
policies for continuous control deep reinforcement learning. arXiv preprint arXiv:1903.11524 
2019.

Sergey Levine  Chelsea Finn  Trevor Darrell  and Pieter Abbeel. End-to-end training of deep visuo-

motor policies. The Journal of Machine Learning Research  17(1):1334–1373  2016.

Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa 
David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971  2015.

Georg Ostrovski  Will Dabney  and R´emi Munos. Autoregressive quantile networks for generative

modeling. arXiv preprint arXiv:1806.05575  2018.

Xue Bin Peng  Pieter Abbeel  Sergey Levine  and Michiel van de Panne.

Deepmimic:
Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint
arXiv:1804.02717  2018.

Boris T Polyak. New stochastic approximation type procedures. Automat. i Telemekh  7(98-107):2 

1990.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons  1994.

Martin L Puterman and Shelby L Brumelle. On the convergence of policy iteration in stationary

dynamic programming. Mathematics of Operations Research  4(1):60–69  1979.

Chao Qu  Shie Mannor  and Huan Xu. Nonlinear distributional gradient temporal-difference learn-

ing. arXiv preprint arXiv:1805.07732  2018.

Martin Riedmiller  Roland Hafner  Thomas Lampe  Michael Neunert  Jonas Degrave  Tom Wiele 
Vlad Mnih  Nicolas Heess  and Jost Tobias Springenberg. Learning by playing solving sparse
reward tasks from scratch. In International Conference on Machine Learning  pages 4341–4350 
2018.

Mark Rowland  Marc G Bellemare  Will Dabney  R´emi Munos  and Yee Whye Teh. An analysis of

categorical distributional reinforcement learning. arXiv preprint arXiv:1802.08163  2018.

Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems 
pages 2234–2242  2016.

Tim Salimans  Jonathan Ho  Xi Chen  Szymon Sidor  and Ilya Sutskever. Evolution strategies as a

scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864  2017.

John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889–1897  2015.

John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

10

David Silver  Guy Lever  Nicolas Heess  Thomas Degris  Daan Wierstra  and Martin Riedmiller.

Deterministic policy gradient algorithms. In ICML  2014.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction  volume 1. MIT

press Cambridge  1998.

Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems  pages 1057–1063  2000a.

Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems  pages 1057–1063  2000b.

Yunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization.

arXiv preprint arXiv:1901.10500  2019.

Emanuel Todorov  Tom Erez  and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS)  2012 IEEE/RSJ International Conference on  pages
5026–5033. IEEE  2012.

Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez 
Łukasz Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems  pages 5998–6008  2017.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning  8(3-4):229–256  1992.

David H Wolpert  William G Macready  et al. No free lunch theorems for optimization.

transactions on evolutionary computation  1(1):67–82  1997.

IEEE

11

,Quanquan Gu
Huan Gui
Jiawei Han
Kent Quanrud
Daniel Khashabi
ChenHan Jiang
Hang Xu
Xiaodan Liang
Liang Lin
Chen Tessler
Guy Tennenholtz
Shie Mannor