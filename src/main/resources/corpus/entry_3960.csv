2018,On the Convergence and Robustness of Training GANs with Regularized Optimal Transport,Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately  minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex  non-smooth  and even hard to compute. In this work  we show that obtaining gradient information of the smoothed Wasserstein GAN formulation  which is based on regularized Optimal Transport (OT)  is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently  we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation  our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images.  Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.,On the Convergence and Robustness of Training

GANs with Regularized Optimal Transport

Maziar Sanjabi

University of Southern California

sanjabi@usc.edu

Meisam Razaviyayn

University of Southern California

razaviya@usc.edu

Jimmy Ba

University of Toronto

jimmy@cs.toronto.edu

Jason D. Lee

University of Southern California
jasonlee@marshall.usc.edu

Abstract

Generative Adversarial Networks (GANs) are one of the most practical methods
for learning data distributions. A popular GAN formulation is based on the use of
Wasserstein distance as a metric between probability distributions. Unfortunately 
minimizing the Wasserstein distance between the data distribution and the genera-
tive model distribution is a computationally challenging problem as its objective is
non-convex  non-smooth  and even hard to compute. In this work  we show that ob-
taining gradient information of the smoothed Wasserstein GAN formulation  which
is based on regularized Optimal Transport (OT)  is computationally effortless and
hence one can apply ﬁrst order optimization methods to minimize this objective.
Consequently  we establish theoretical convergence guarantee to stationarity for a
proposed class of GAN optimization algorithms. Unlike the original non-smooth
formulation  our algorithm only requires solving the discriminator to approximate
optimality. We apply our method to learning MNIST digits as well as CIFAR-10
images. Our experiments show that our method is computationally efﬁcient and
generates images comparable to the state of the art algorithms given the same
architecture and computational power.

1

Introduction

Generative Adversarial Networks (GANs) have gained popularity for unsupervised learning due to
their unique ability to learn the generation of realistic samples. In the absence of labels  GANs aims
at ﬁnding the mapping from a known distribution  e.g. Gaussian  to an unknown data distribution 
which is only represented by empirical samples. In order to measure the mapping quality  various
metrics between the probability distributions have been proposed. While the original work on
GANs proposed Jensen-Shannon distance [21]  other works have proposed other metrics such as f-
divergences [37  16]. Recently  a seminal work by [4] re-surged Wasserstein distance [44] as a metric
for measuring the distance between the distributions. One major advantage of Wasserstein distance 
compared to Jensen-Shannon  is its continuity. In addition  Wasserstein distance is differentiable
with respect to the generator parameters almost everywhere [4]. As a result  it is more appealing
from optimization perspective. From the generator perspective  this objective function is not smooth
with respect to the generator parameters. As we will see in this paper  this non-smoothness results
in difﬁculties for optimization algorithms. Hence  we propose to use a smooth surrogate for the
Wasserstein distance.
The introduction of Wasserstein distance as a metric for GANs re-surged the interest in the ﬁeld
of optimal transport [44]. [4] provided a game-representation for their proposed Wasserstein GAN

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this game-
formulation based on the dual form of the resulting optimal transport problem.
representation  the discriminator is comprised of a 1-Lipschitz function and aims at differentiating
between real and fake samples. From optimization perspective  enforcing the Lipschitz constraint is
challenging. Therefore  many heuristics  such as weight clipping [4] and gradient norm penalty [22] 
have been developed to impose such constraint on the discriminator. As we will see  our smoothed
surrogate objective results in a natural penalizing term to softly impose various constraints such as
Lipschitzness.
Studying the convergence of algorithms for optimizing GANs is an active area of research. The
algorithms and analyses developed for optimizing GANs can be divided into two general categories
based on the amount of effort spent on solving the discriminator problem. In the ﬁrst category 
which puts the same emphasis on the discriminator and the generator problem  a simultaneous or
successive generator-discriminator (stochastic) gradient descent-ascent update is used for solving
both problems. These approaches are inspired by the mirror/proximal gradient descent method
which was developed for solving convex-concave games [34]. Although the GAN problem does not
conform to convex-concave paradigm in general  researchers have found this procedure successful in
some special practical GANs; and unsuccessful in some others [30]. The theoretical convergence
guarantees for these methods are local and based on limiting assumptions which are typically not
satisﬁed/veriﬁable in almost all practical GANs. More precisely  they either assume some (local)
stability of the iterates or local/global convex-concave structure [33  31  14]. In all of these works 
similar to our setting  some form of regularization is necessary for obtaining convergence. Compared
to these methods  we do not limit the number of discriminator steps to one. But our convergence is
global and only depends on the quality of the discriminator.
As opposed to the ﬁrst line of work  some developed algorithm put more emphasis on the discriminator
problem [32  27  23]. For example  [27] proved global convergence to optimality for the ﬁrst order
update rule on a speciﬁc problem of learning a one dimensional mixture of two Gaussians when the
discriminator problem is solved to global optimality at each step. Another line of analysis  which
also prioritize the discriminator problem  is based on the strategy of learning the discriminator much
faster than the generator [23]. These analyses  which are inspired by the variants of the two time
scale dynamic proposed by [8]  do not directly require the convex-concavity of the objective function.
However  they require some kind of local (global) stability which is difﬁcult to achieve unless there
is local (global) convex-concave structure. Compared to these results  our convergence analysis is
agnostic to the method for solving the discriminator problem provided that the discriminator problem
is solved to optimality within some accuracy. Based on our analysis  the amount of this accuracy
then dictates the closeness to stationarity. Therefore  our result suggests that in order to obtain a
better quality solution  it is not enough to just increase the number of steps for the generator  but one
also needs to maintain high enough accuracy in the discriminator. It also suggests that the simple
descent-ascent update rule might not converge– as it has also been observed before in the literature;
see  e.g.  [29] . Therefore  one should use algorithms similar to the two time scale approaches that
give discriminator increasingly more advantage. Note that  unlike [27]  we do not assume perfect
discriminator which is not feasible in practice. It is also worth noting that the dual formulation
of our regularized Wasserstein GAN is an unconstrained smooth convex problem in the functional
domain. Therefore  it is theoretically feasible to solve it non-parametrically to any accuracy with
polynomial number of steps in the functional space. To the best of our knowledge  our convergence
analysis is the ﬁrst result in the literature with mild assumptions that proves the global convergence
to a stationary solution with polynomial number of generator steps and with approximate solutions
to the discriminator at each step. Notice that this is possible thanks to the regularizer added to the
discriminator problem in our formulation.
1.1 Related works and contributions
We study the problem of solving Wasserstein GANs from optimization perspective. In short  we
solidify the intuitions that the use of regularized Wasserstein distance is beneﬁcial in learning GANs
[13  7  42  18  41] through a rigorous and novel algorithmic convergence analysis. There are three
steps for obtaining such convergence guarantee.

• We prove that the regularized Wasserstein distance  when used in GAN problems  is smooth
with respect to the generator parameters.
• We also prove that by approximately solving the regularized Wasserstein distance (discriminator
steps)  we can control the error in the computation of the (stochastic) gradients for the generator.

2

Such an error control could not be achieved for the original Wasserstein distance or other GAN
formulations in general; see Proposition 3 in [9].

• Having approximate ﬁrst order information and smoothness  we prove the convergence of
vanilla stochastic gradient descent (SGD) method to a stationary solution. Our results suggests
that converging to stationarity of the ﬁnal solution not only depends on the number of steps in
the generator  but also depends on the quality of solving the discriminator problem.

Note that our convergence result relies on the smoothness of regularized Wasserstein distance with
respect to the generator parameters. The use of regularization as a means for smoothing has a long
history in optimization literature [35]. In the optimal transport literature  the regularization has been
used as a means to derive faster methods for computing the optimal transport. The most prominent
example is Sinkhorn distance [13]  which is based on regularizing the optimal transport problem
with a negative entropy term. There are many efﬁcient algorithms proposed for ﬁnding the Sinkhorn
distance [25  43  3]. Recently Blondel et al. [7] noted that using strongly convex regularizers on the
optimal transport problem would result in an unconstrained dual formulation which is computationally
easier to solve. This unconstrained form is essential in using parametric methods  such as neural
networks  for solving regularized optimal transport problems  as also observed in [42].
In [42] a regularized optimal transport with very small regularization weight is considered as an
objective for learning GANs. Choosing a small regularization parameter is important as a strong
regularization introduces bias. Although  our convergence guarantee applies to this objective  our
smoothness analysis predicts that small weight regularization leads to an unstable algorithm. This
fact has also been observed in our experiments.Thus  we use Sinkhorn loss [19  41  5] for which
our convergence guarantee holds. We show that this loss does not introduce bias into ﬁnding the
correct generative model regardless of the amount of regularization. Finally  using the insights from
our theoretical analysis  we put together all the pieces of different methods for solving GANs with
regularized optimal transport [42  18  41] to get an algorithm that is competitive both in terms of
computational efﬁciency and quality with the state of the art methods.

2 Background
Given a cost function c : Rd ⇥ Rd ! R  the optimal transport cost between two distributions p and
q can be deﬁned as
(1)

⇡(x  y)c(x  y)dxdy 

dc(p  q) = min

⇡2⇧(p q)ZYZX

where ⇧(p  q) is the set of all joint distributions having marginal distributions p and q  i.e.
⇡(x  y)dx = p(y) and RY
RX
⇡(x  y)dy = q(x). Note that X and Y deﬁne the spaces of all
possible x’s and y’s respectively. In practice  the distributions are replaced by their empirical samples 
thus X and Y have ﬁnite supports. In such cases we still use integrals to represent ﬁnite sums.
Throughout the paper  X and Y are assumed to have ﬁnite support  unless otherwise is noted.
The optimal transport cost (1) could be used as an objective for learning generative models. To be
more speciﬁc  we assume that we have a base distribution p and among a set of parameterized family
of functions {G✓  ✓ 2 ⇥}  we aim at learning a mapping G✓⇤ such that the cost dc(G✓⇤(q)  p) 1 is
minimized. In other words the problem of generative model learning is

h0(✓) = dc(G✓(q)  p) = min

⇡2⇧(p q)ZXZY

min
✓2⇥

⇡(x  y) c(G✓(x)  y) dy dx.

(2)

2.1 Generative adversarial networks with Wc objective

In [4] authors propose to use the dual formulation of the generative problem (2) as it is easier to
parametrize the dual functions instead of the transport plan ⇡. They call this formulation Wasserstein

1 Throughout the paper we have the hidden technical assumption that G✓ is a one-to-one mapping on X . This
is a reasonable assumption since the space of Y is usually a low dimensional manifold in higher dimensional
space which could be approximated by a mapping of low dimensional code words x 2 X . Therefore  the
mappings are going to be from low dimensions to high dimensions.

3

GAN (WGAN). Based on Kantorovich theorem [44] the dual form of (2) could be written as

min

✓

Ex⇠p ↵(G✓(x))  Ey⇠q (y) 

max
↵ 
s.t. ↵(G✓(x))  (y)  c(G✓(x)  y) 8(x  y)

(3)

dc (p  q) = min

⇡2⇧(p q)

where for practical considerations we have assumed that the dual functions/discriminators   and  
belong to the set of parametric functions with parameters ↵ and  respectively. Note that the inner
problem has a constraint over the functions  and . In the case where c is a distance  then  = 
and the constraint is enforcing the 1-Lipschitz constraint on the functions =  with respect to c.
This 1-Lipschitzness is not easy to enforce. In practice  it is usually imposed heuristically by adding
some regularizer [22].
2.2 Regularized optimal transport
For any strongly convex function I(⇡)  we can deﬁne the regularized optimal transport as

H(⇡  ✓) =Z Z ⇡(x  y) c(x  y) dxdy + I(⇡).

2Z Z ⇡(x  y)2

q(x)p(y)◆dxdy & norm-2: I(⇡) =

(4)
We also deﬁne ¯dc (p  q) = dc (p  q)  I(⇡⇤)  where ⇡⇤ is the optimal solution of (4). Note that 
r✓dc (G✓(q)  p) = r✓ ¯dc (G✓(q)  p). Among all strongly convex regularizers  the following two
are the most popular ones [7]:
KL: I(⇡) =Z Z ⇡(x  y) log✓ ⇡(x  y)
When c is a proper distance  one can show desirable properties for dc  and ¯dc ; see [13] and
Appendix A. It is also possible to prove the uniform convergence of function dc (p  q) to dc(p  q)
as  ! 0 when p and q have ﬁnite support; see [7] and Appendix B. In the case of continuous
distributions [42] proves a point-wise convergence between the two distances as  ! 0.
2.3 Dual formulation for regularized optimal transport
The dual formulation for regularized optimal transport has also been covered in other recent works
[42  7]. Thus  we just present a summary of the results in Lemma 2.1 and highlight the important part
in the remarks that follow; see Appendix D for a more comprehensive discussion.
Lemma 2.1. Let (x) and (y) be the dual variables for the constraints in the regularized optimal
transport problem (4). Let us also deﬁne the violation function V (x  y) = (x)  (y)  c(x  y).
Then  the dual of the regularized optimal transport is:
(5)

Ex⇠q[(x)]  Ey⇠p[ (y)]  Ex y⇠q⇥p [f(V (x  y))] 

dc (p  q) = max
  

q(x)p(y)

dx dy.

1

e e v

 for KL regularization and f(v) = (v+)2

2 for norm-2 regularization. Further-
where f(v) = 
more  given the optimal dual variables  and   the optimal primal transport plan could be computed
 for
as ⇡(x  y) = q(x)p(y)M (V (x  y))  where M (v) = 1
norm-2 regularization.
Remark 2.1.1. The dual of the regularized optimal transport is a large scale unconstrained concave
maximization which can be solved as one. But it is also amenable to the use of parametric method 
i.e.  neural networks  for representing the dual functions.

 for KL regularization and M (v) = v+

e e v

Note that V (x  y) in Lemma 2.1 represents the amount of violation from the hard constraint in the
original dual formulation (3). Therefore  by adding the regularizer in the primal  we are relaxing the
hard constraint in the dual representation to a soft one in the objective function. By looking at the
problem from this perspective  one can ﬁnd similarities between our approach and the one in [22]
where the authors drop the 1-Lipschitz constraint on the discriminator and try to softly enforce it by
regularizing the objective using the Jacobian of the discriminator function.
Remark 2.1.2. Lemma 2.1 also provides a mapping that translates the dual solutions  and to a
corresponding pseudo-transport plan

(6)
Note that although ⇡ may not be a feasible transport plan  (6) can be used to compute an approximate
gradient of the generator problem  as discussed in Section 4.

⇡(x  y) = q(x)p(y)M (V (x  y)).

4

In what follows  we assume that at each iteration of the procedure for ﬁnding the optimal generator 
we have access to an oracle which solves the resulting dual of regularized optimal transport to some
predeﬁned accuracy. We say a solution (  ) is an ✏-accurate solution for (5)  if the value that it
achieves is within ✏ of the optimal value for (5). Such an oracle could be realized through convex
optimization methods and non-parametric representations; see Appendix D.2. However  due to
practical computational barriers  we opt for parametric realizations of the oracle  i.e neural networks.

3 Smoothness of the generative objective

Given two ﬁxed distributions q and p  let us deﬁne h(✓) = dc (G✓(q)  p). In this section we prove
that h(✓) is smooth with respect to ✓ in contrast to the original metric h0(✓). This is particularly
important when we only solve the inner optimal transport problem within some accuracy. Due to
space limitations  we only state our result on the smoothness of h(✓) when the regularizer is KL
divergence; similar result for norm-2 regularizer can be found in Theorem E.1 in the Appendix. The
only difference when changing the regularizer comes from the fact that these two regularizers are
strongly convex with respect to different norms.
Theorem 3.1. Assume X and Y are compact  p and q have bounded entropy  c is bounded from
below  and there exist non-negative constants L1 and L0  such that:

• For any feasible ✓1  ✓2  supx y kr✓c(G✓1(x)  y)  r✓c(G✓2(x)  y)k  L1k✓1  ✓2k 
• For any feasible ✓1  supx y kr✓c(G✓1(x)  y)k  L0 

0

 . Moreover  for any two parameters

 k✓1  ✓2k  where ⇡⇤(✓) = arg min⇡2⇧(p q) H(⇡  ✓).

then the function h(✓) is L-Lipschitz smooth2 with L = L1 + L2
✓1 and ✓2  k⇡⇤(✓1)  ⇡⇤(✓2)k1  L0
Remark 3.1.1. Theorem 3.1 holds for both continuous and discrete distributions. In addition  when
X and Y have ﬁnite support  the entropies of p and q are automatically bounded.
The proof of this theorem is inspired by [35] and is relegated to Appendix C  where we ﬁrst prove the
result for distributions with ﬁnite support  and then we extend the proof to continuous distributions.
Note that unlike the non-regularized original formulation  small changes in ✓ results in a small change
in the corresponding optimal transport plan in the regularized formulation. Consequently  after
updating ✓  solving the inner problem would be easier as the optimal discriminator has not moved
very far from the last iterate. It is also worth noticing that the assumptions of Theorem 3.1 is satisﬁed
when the functions c and G are smooth and the domain of x  y is compact.

4 Solving the generator problem to stationarity using ﬁrst order methods

First order methods  including SGD and its variants such as Adam [24] or SVRG [2]  are the work-
horse for large scale optimization. These methods are built on top of an oracle that can generate a
close approximation of the (stochastic) gradients.
Unfortunately  the original non-regularized GAN objective h0(✓) is non-smooth. Moreover  it is
impossible to obtain guaranteed good quality approximations of the its sub-gradients even if we solve
the discriminator problem with high accuracy; see Proposition 3 in [9]. In contrast  we proved that
the h(✓) is smooth. Next we will prove that one can obtain decent quality estimates of its gradient
by solving the corresponding regularized dual problem approximately.
Theorem 4.1. Under the same assumptions as in Theorem 3.1  let (  ) be an ✏-accurate solution
to the dual formulation of regularized optimal transport for a given ✓. Let ⇡ be the transport
plan corresponding to (  )  derived using (6). Let us also deﬁne m(x  y) = ⇡(x y)
q(x)p(y) and G =

Ex y⇠q⇥p⇥m(x  y)r✓c(G✓(x)  y)⇤. Then 

kG  rh(✓)k   = O✓r ✏
◆

2A function f (✓) : Rd ! R is said to be L-Lipschitz smooth if it is differentiable and its derivative is

L-Lipschitz  i.e. 8 ✓1  ✓2 : krf (✓1)  rf (✓2)k  Lk✓1  ✓2k.

(7)

5

We only prove this result for the case where X and Y have ﬁnite support. This is the scenario that
happens in practice where the distributions are replaced by their ﬁnite samples. While we believe a
more general version of this result could be proved for continuous distributions by delicately repeating
the same steps  such a proof goes beyond the scope of this paper. See Appendix G for the proof.
Also note that it is possible to verify the quality of the discriminator/dual solutions. Due to the space
limitation  we relegate the discussion on veriﬁcation to Appendix D.3.
The above theorem guarantees that using the dual solver  we can generate approximate (stochastic)
gradients for h(✓). In other words  the discriminator steps in solving GANs could be viewed
as a way of obtaining approximate gradient information for h(✓). Using the above approximate
(stochastic) gradients  one can provide algorithms with guaranteed convergence to approximate
stationary solutions for GANs. We describe one such algorithm based on the vanilla mini-batch SGD
and state its convergence guarantee.

Algorithm 1 Oracle based Non-Convex SGD for GANs

INPUT: q  p    S  ✓0  {↵t > 0}T1
for t = 0 ···   T  1 do
Call the oracle to ﬁnd ✏-approximate maximizer (t  t) for the dual formulation
Sample I.I.D. points x1

t=0

t ⇠ q  y1
t  ···   xS
S2Xi j
gt =

1

t  ···   yS

⇡t(G✓(xi

t ⇠ p and compute
t)  yj
t )
t ) r✓c(G✓(xi
t)p(yj

q(xi

t)  yj
t )

where ⇡t is computed using (t  t) based on (6).
Update ✓t+1 ✓t  ↵tgt

end for

Remark 4.1.1. In Algorithm 1  if we deﬁne Gt = E[gt|⇡t  ✓t]  then Theorem 4.1 simply states that
kGt  rh(✓t)k   = Op ✏
.
The following theorem establishes the convergence of Algorithm 1 to an approximate stationary
solution of h.
Theorem 4.2. Let L be the Lipschitz constant of the gradient of h. Set  = h(✓0)inf ✓ h(✓) and
let Gt = E[gt|⇡t  ✓t]. Furthermore  assume kGtrh(✓t)k   and E[kgtGtk2|⇡t  ✓t]  2  8t.

• If T < 2L
• If T  2L

2   setting ↵t = 1

2   setting ↵t =q 2

T PT

L  we have 1

L2T   we have 1

t=1 E[krh(✓t)k2]  2L
t=1 E⇥krh(✓t)k2
T PT
E[kr✓h(✓t)k2]  O✓r L

T ◆ + O✓ ✏
◆.

min

t=1 ···  T

T + 2 + 2.

F⇤  q8 L

T + 2.

The proof of this theorem is inspired by [20] and is presented in Appendix H.
Remark 4.2.1. The second regime in Theorem 4.2 results in the following asymptotic convergence
rate of expected norm of the gradient as T ! 1:

(8)

It is worth noting that our convergence analysis also guarantees the convergence of the algorithm in
[42] for generative learning which is similar to Algorithm 1.
Remark 4.2.2. When the error in gradient approximation at each step t is t  Theorem 4.2 is still
valid with 2 = 1
t . Thus  the algorithm only needs to keep the average error in solving the
t=1 2
inner problem small enough.
4.1 Sinkhorn loss: a more robust generative objective

T PT

In practice  when using regularized Wasserstein distance h(✓) as an objective for generative models 
one needs to use very small value of  as noted by [42]. This is because a large  would introduce bias
into measuring the Wasserstein distance. In fact  choosing large  may lead to undesired solutions;
see Corollary F.0.2 for an example. A naïve approach to deal with this bias is to reduce . However 
a reduced  have three dire effects: (i) based on Theorem 3.1  any change in the generator parameters
would result in large changes in the optimal discriminator parameters. (ii) According to (8)  smaller

6

 requires smaller error ✏ for solving discriminator to obtain the same convergence guarantee. Thus 
solving the discriminator problem requires more effort. (iii) Lipschitz smoothness constant of h(✓)
increases with the decrease in . Thus  we have to choose smaller step-size  based on Theorem 4.2 
which means slower convergence. In our experiments we observed that this situation worsens with
the complexity and scale of the problem. A proposed solution in literature is to use Sinkhorn loss
[19  41] to reduce the bias in measuring the distance between the two distributions without reducing
 and get an objective which is meaningful even for large values of . The Sinkhorn loss between
two distributions p and q is deﬁned as

L(p  q) = 2 ¯dc (p  q)  ¯dc (p  p)  ¯dc (q  q)

(9)
In [19] Genevay et al. proved that  when c is a distance  as  ! 1  L converges to Maximum Mean
Discrepancy Distance (MMD)[15]; and when  ! 0  L converges to 2dc(p  q). Next  we present a
result that shows the robustness of L with respect to the choice of  2 (0 1) in identifying the true
generator parameters; see the proof in Appendix J.
Lemma 4.3. Assume c is symmetric  i.e.  c(x  y) = c(y  x). If there exists ✓0 for which q = G✓0(p) 
then ✓0 is a stationary solution of L(G✓(p)  q). Moreover  L(G✓0(p)  q) = 0 for any  > 0.
Notice that the above does not hold for dc (G✓(p)  q) unless  ! 0. Based on the above Lemma
we opt to use the following Sinkhorn loss as our generative objective

min

✓

ˆh(✓) = L(G✓(q)  p) = 2 ¯dc (G✓(q)  p)  ¯dc (G✓(q)  G✓(q))  ¯dc (p  p)

(10)

Note that only the ﬁrst two terms in ˆh(✓) depend on ✓. To compute approximate gradients for these
two terms  we need to call the discriminator oracle twice; and approximately solve dc (G✓(q)  p)
and dc (G✓(q)  G✓(q)). With the returned discriminator solutions  we have two approximate
gradients  one for each term. If each one of the gradients has error   obtained by applying (7)  the
error in approximating the overall gradient is bounded by ˆ = 3 . Now if we further assume that
sub-sampling would generate a stochastic gradient of variance 2 for each term  the overall variance
of the noise in gradient would be bounded by ˆ2 = 5 2. We summarize the SGD based method
for solving (10) in Algorithm 2 in the Appendix. With these assumptions  we can easily extend the
convergence result of Theorem 4.2 to Algorithm 2.
Corollary 4.3.1. By replacing  and  in the convergence guarantees of Theorem 4.2 with ˆ and ˆ
respectively  we obtain a convergence guarantee for the SGD based method  described above  for
solving the generative Sinkhorn loss optimization problem (10).
5 Experiments
In this section we test a family of methods which we generally call Smoothed WGAN (SWGAN).
They all use the two variants of regularized OT formulation  i.e. h(✓) and ˆh(✓)  as their objective.
We differentiate between the two objectives by explicitly mentioning Sinkhorn loss when it is used.
We also investigate the choice of different cost functions  i.e. L1 and Cosine distances. Unlike
[41  18] that solve regularized OT with a parametric approach  we use neural networks to solve the
OT (discriminator) similar to [42]. In contrast with [41  18] that use large batch-sizes to get unbiased
gradient estimates  our gradient estimates are always unbiased due to the use of neural networks as
discriminator. As a benchmark  we compare the SWGAN methods with the gradient penalty WGAN
(WGAN-GP) [22] and other methods that uses the regularized OT objective [41  19]. All algorithms
were implemented in TensorFlow [1].

5.1 Learning handwritten digits

In this section we apply SWGAN methods to learn handwritten digits on the MNIST data set. Our
main goal is to see the effect of different choices of objective and cost function on the performance of
SWGAN methods. For details of hyper parameters and networks structures see Appendix L.3.
The ﬁrst and second row of Fig. 1 corresponds to SWGAN methods with L1 and Cosine cost
respectively. As the SWGAN formulations allows ﬂexibility in the choice of the cost function  we
apply these costs on different representation of the images. In Fig. 1 (a)  (b)  (e) and (f)  the cost
function is applied on the pixel domain (no latent representation)  while in Fig. 1 (c)  (d)  (g) and (h)
the cost is applied on a lower dimensional latent representation of the image [41  18]  parameterized

7

SWGAN

L1

SWGAN
Cosine

(a) Pixel

(b) Pixel Sinkhorn loss

(c) Latent

(d) Latent Sinkhorn loss

(e) Pixel

(f) Pixel Sinkhorn loss

(g) Latent

(h) Latent Sinkhorn loss

Figure 1: Generated MNIST samples using different SWGAN and benchmark methods

(i) Latent
solver [19]

Sinkhorn

(j) WGAN-GP [22]

by a Convolution Neural Network. As proposed by [41  18]  this latent representation could be
adversarially trained to improve the quality of the ﬁnal results; see Appendix L.1 for more details.
Comparing SWGAN methods with and without latent representation  we ﬁnd that the ones with
latent representation perform better. This might be due to the existence of many bad local minima in
the high dimensional pixel space cost function which the algorithm cannot avoid. In contrast  the
ones with lower dimensional latent representations seem to have easier time avoiding such bad local
minima when the latent representation is also being updated. Based on Lemma 4.3  we conjecture that
in these cases  ground truth is the only solution that is stationary regardless of the latent representation.
Thus  updating the latent representation once in a while prevents the generative parameters from
converging to local minima  i.e. over-ﬁtting to a speciﬁc representation. Note that this conjecture is
not a direct result of Lemma 4.3.
It is also interesting to note the difference between Fig. 1 (a) and (f)  where (f) outperforms (a) which
produces many faint images. We believe that the change of objective from regularized OT to Sinkhorn
loss helps the method ﬁnd a better stationary solution  which is closer to the underlying ground truth
as predicted by Lemma 4.3. This difference is more pronounced in the experiments on CIFAR-10.
We have also included samples generated by other methods [18  22] in the last row of Fig. 1.
Compared to these methods  specially [18] which uses Sinkhorn algorithm to solve regularized OT 
SWGAN methods are capable of generating higher quality images. We also noted that SWGAN
methods qualitatively converge faster than other methods. We will formalize this comparison in the
experiments on CIFAR-10 using the inception score [40].

5.2 Generating tiny color images
To further investigate the performance of SWGAN  we apply it to model 32x32 color images from
CIFAR-10 [26]. We compare the SWGAN approach with WGAN-GP [22]  OT-GAN [41] and
Sinkhorn solver [18]. All the methods are trained using the same architecture and batch-size of 150;
see Appendix M for the details and a list of hyper-parameters. We use inception score [40] to compare
the quality of generated samples. Learning CIFAR-10 images is a more challenging problem than
MNIST; and as we predicted in Section 4.1 SWGAN methods with regularized OT objective cannot
generate high quality samples  even with carefully tuned hyper-parameters; see Fig. 4 in Appendix.
Due to high computational cost  we only evaluate latent Sinkhorn loss SWGAN with L1 and Cosine
cost on CIFAR-10. As can be seen in Fig. 2 (c)  given the same architecture and computational
power  the SWGAN methods have faster convergence compared to other algorithms. In Fig. 2 all the
methods have been running for roughly the same amount of time. Note that OT-GAN [41] is slower
as it uses two batches for each label  i.e. fake and real  and requires more computations. We also
depict samples of the generated images by SWGAN methods in Fig. 2 (a) and (b). We noted that

8

(a) SWGAN Cosine

(b) Inception scores

(c) SWGAN L1

Figure 2: Generated CIFAR-10 samples and inception scores.

SWGAN with L1 cost converges faster than the cosine distance in terms of inception scores  but the
samples from the cosine distance model are more visually appealing than the ones from L1.

Acknowledgments
MS and JDL acknowledge support from ARO W911NF-11-1-0303. The authors would like to thank
the anonymous reviewers whose comments/suggestions helped improve the quality/clarity of the
paper.

References
[1] M. Abadi  P. Barham  J. Chen  Z. Chen  A. Davis  J. Dean  M. Devin  S. Ghemawat  G. Irving  M. Isard 
et al. Tensorﬂow: A system for large-scale machine learning. In OSDI  volume 16  pages 265–283  2016.

[2] Z. Allen-Zhu and Y. Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex objectives. In

International conference on machine learning  pages 1080–1089  2016.

[3] J. Altschuler  J. Weed  and P. Rigollet. Near-linear time approximation algorithms for optimal transport via

sinkhorn iteration. In Advances in Neural Information Processing Systems  pages 1961–1971  2017.

[4] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875  2017.

[5] M. G. Bellemare  I. Danihelka  W. Dabney  S. Mohamed  B. Lakshminarayanan  S. Hoyer  and R. Munos.
The cramer distance as a solution to biased wasserstein gradients. arXiv preprint arXiv:1705.10743  2017.

[6] P. Bernhard and A. Rapaport. On a theorem of danskin with an application to a theorem of von neumann-

sion. Nonlinear analysis  24(8):1163–1182  1995.

[7] M. Blondel  V. Seguy  and A. Rolet. Smooth and sparse optimal transport. arXiv preprint arXiv:1710.06276 

2017.

[8] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters  29(5):291–294 

1997.

[9] O. Bousquet  S. Gelly  I. Tolstikhin  C.-J. Simon-Gabriel  and B. Schoelkopf. From optimal transport to

generative modeling: the VEGAN cookbook. arXiv preprint arXiv:1705.07642  2017.

[10] G. Carlier  V. Duval  G. Peyré  and B. Schmitzer. Convergence of entropic schemes for optimal transport

and gradient ﬂows. SIAM Journal on Mathematical Analysis  49(2):1385–1418  2017.

[11] G. Carlier  V. Duval  G. Peyré  and B. Schmitzer. Convergence of entropic schemes for optimal transport

and gradient ﬂows. SIAM Journal on Mathematical Analysis  49(2):1385–1418  2017.

9

[12] F. Cicalese  L. Gargano  and U. Vaccaro. How to ﬁnd a joint probability distribution of minimum entropy

(almost)  given the marginals. arXiv preprint arXiv:1701.05243  2017.

[13] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural

information processing systems  pages 2292–2300  2013.

[14] C. Daskalakis  A. Ilyas  V. Syrgkanis  and H. Zeng. Training GANs with optimism. arXiv preprint

arXiv:1711.00141  2017.

[15] G. K. Dziugaite  D. M. Roy  and Z. Ghahramani. Training generative neural networks via maximum mean

discrepancy optimization. arXiv preprint arXiv:1505.03906  2015.

[16] F. Farnia and D. Tse. A convex duality framework for gans. In Advances in Neural Information Processing

Systems  pages 5254–5263  2018.

[17] S. Feizi  C. Suh  F. Xia  and D. Tse. Understanding GANs:

arXiv:1710.10793  2017.

the LQG setting.

arXiv preprint

[18] A. Genevay  M. Cuturi  G. Peyré  and F. Bach. Stochastic optimization for large-scale optimal transport. In

Advances in Neural Information Processing Systems  pages 3440–3448  2016.

[19] A. Genevay  G. Peyré  and M. Cuturi. Learning generative models with sinkhorn divergences.

International Conference on Artiﬁcial Intelligence and Statistics  pages 1608–1617  2018.

In

[20] S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming.

SIAM Journal on Optimization  23(4):2341–2368  2013.

[21] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.
Generative adversarial nets. In Advances in neural information processing systems  pages 2672–2680 
2014.

[22] I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. C. Courville. Improved training of wasserstein

GANs. In Advances in Neural Information Processing Systems  pages 5769–5779  2017.

[23] M. Heusel  H. Ramsauer  T. Unterthiner  B. Nessler  and S. Hochreiter. GANs trained by a two time-scale
update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems 
pages 6629–6640  2017.

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980  2014.

[25] P. A. Knight. The sinkhorn–knopp algorithm: convergence and applications. SIAM Journal on Matrix

Analysis and Applications  30(1):261–275  2008.

[26] A. Krizhevsky  V. Nair  and G. Hinton. The cifar-10 dataset. online: http://www. cs. toronto. edu/kriz/cifar.

html  2014.

[27] J. Li  A. Madry  J. Peebles  and L. Schmidt. Towards understanding the dynamics of generative adversarial

networks. arXiv preprint arXiv:1706.09884  2017.

[28] J. H. Lim and J. C. Ye. Geometric GAN. arXiv preprint arXiv:1705.02894  2017.

[29] L. Mescheder. On the convergence properties of GAN training. arXiv preprint arXiv:1801.04406  2018.

[30] L. Mescheder  A. Geiger  and S. Nowozin. Which training methods for GANs do actually converge? arXiv

preprint arXiv:1801.04406  2018.

[31] L. Mescheder  S. Nowozin  and A. Geiger. The numerics of GANs. In Advances in Neural Information

Processing Systems  pages 1823–1833  2017.

[32] L. Metz  B. Poole  D. Pfau  and J. Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint

arXiv:1611.02163  2016.

[33] V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. In Advances in Neural

Information Processing Systems  pages 5591–5600  2017.

[34] A. Nemirovski. Prox-method with rate of convergence O (1/t) for variational inequalities with lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on
Optimization  15(1):229–251  2004.

10

[35] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming  103(1):127–152 

2005.

[36] Y. Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Science &

Business Media  2013.

[37] S. Nowozin  B. Cseke  and R. Tomioka. f-GAN: Training generative neural samplers using variational
divergence minimization. In Advances in Neural Information Processing Systems  pages 271–279  2016.

[38] E. Posner. Random coding strategies for minimum entropy. IEEE Transactions on Information Theory 

21(4):388–391  1975.

[39] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[40] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved techniques for

training GANs. In Advances in Neural Information Processing Systems  pages 2234–2242  2016.

[41] T. Salimans  H. Zhang  A. Radford  and D. Metaxas. Improving GANs using optimal transport. arXiv

preprint arXiv:1803.05573  2018.

[42] V. Seguy  B. B. Damodaran  R. Flamary  N. Courty  A. Rolet  and M. Blondel. Large-scale optimal

transport and mapping estimation. arXiv preprint arXiv:1711.02283  2017.

[43] A. Thibault  L. Chizat  C. Dossal  and N. Papadakis. Overrelaxed Sinkhorn-Knopp algorithm for regularized

optimal transport. arXiv preprint arXiv:1711.01851  2017.

[44] C. Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media  2008.

11

,Maziar Sanjabi
Meisam Razaviyayn
Jason Lee