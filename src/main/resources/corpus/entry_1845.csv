2016,Error Analysis of Generalized Nyström Kernel Regression,Nystr\"{o}m method has been used successfully to improve the computational efficiency of  kernel ridge regression (KRR). Recently  theoretical analysis of Nystr\"{o}m KRR  including generalization bound and convergence rate  has been established based on  reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However  in real world applications  RKHS is not always  optimal  and  kernel function is not necessary to be  symmetric or positive semi-definite.  In this paper  we consider  the generalized Nystr\"{o}m  kernel regression (GNKR) with $\ell_2$ coefficient regularization  where the kernel just requires the continuity and boundedness.  Error analysis is provided to characterize its generalization performance  and the column norm sampling is introduced to construct the refined hypothesis space. In particular   the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling.,Error Analysis of Generalized Nyström Kernel

Regression

Hong Chen

Computer Science and Engineering

University of Texas at Arlington

Arlington  TX  76019

chenh@mail.hzau.edu.cn

Haifeng Xia

Mathematics and Statistics

Huazhong Agricultural University

Wuhan 430070 China

haifeng.xia0910@gmail.com

Weidong Cai

School of Information Technologies

University of Sydney
NSW 2006  Australia

tom.cai@sydney.edu.au

Heng Huang

Computer Science and Engineering

University of Texas at Arlington

Arlington  TX  76019

heng@uta.edu

Abstract

Nyström method has been successfully used to improve the computational efﬁ-
ciency of kernel ridge regression (KRR). Recently  theoretical analysis of Nyström
KRR  including generalization bound and convergence rate  has been established
based on reproducing kernel Hilbert space (RKHS) associated with the symmetric
positive semi-deﬁnite kernel. However  in real world applications  RKHS is not
always optimal and kernel function is not necessary to be symmetric or positive
semi-deﬁnite. In this paper  we consider the generalized Nyström kernel regression
(GNKR) with (cid:96)2 coefﬁcient regularization  where the kernel just requires the conti-
nuity and boundedness. Error analysis is provided to characterize its generalization
performance and the column norm sampling strategy is introduced to construct the
reﬁned hypothesis space. In particular  the fast learning rate with polynomial decay
is reached for the GNKR. Experimental analysis demonstrates the satisfactory
performance of GNKR with the column norm sampling.

1

Introduction

The high computational complexity makes kernel methods unfeasible to deal with large-scale data.
Recently  the Nyström method and its alternatives (e.g.  the random Fourier feature technique [15] 
the sketching method [25]) have been used to scale up kernel ridge regression (KRR) [4  23  27]. The
key step of Nyström method is to construct a subsampled matrix  which only contains part columns
of the original empirical kernel matrix. Therefore  the sampling criterion on the matrix column
affects heavily on the learning performance. The subsampling strategies of Nyström method can be
categorized into two types: uniform sampling and non-uniform sampling. The uniform sampling is
the simplest strategy  which has shown satisfactory performance on some applications [16  23  24].
From different theoretical aspects  several non-uniform sampling approaches have been proposed
such as the square (cid:96)2 column-norm sampling [3  4]  the leverage score sampling [5  8  12]  and the
adaptive sampling [11]. Besides the sampling strategies  there exist learning bounds for Nyström
kernel regression from three measurements: the matrix approximation [4  5  11]  the coefﬁcient
approximation [9  10]  and the excess generalization error [2  16  24].
Despite rapid progress on theory and applications  the following critical issues should be further
addressed for Nyström kernel regression.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

• Nyström regression with general kernel. The previous algorithms are mainly limited to
KRR with symmetric and positive semi-deﬁnite kernels. For real-world applications  this
restriction may be not necessary. Several general kernels have shown the competitive
performance in machine learning  e.g.  the indeﬁnite kernels for regularized algorithms
[14  20  26] and PCA [13]. Therefore  it is important to formulate the learning algorithm for
Generalized Nyström Kernel Regression (GNKR).
• Generalization analysis and sampling criterion. Previous theoretical results rely on the
symmetric positive semi-deﬁnite (SPSD) matrix associated with a Mercer kernel [17].
However  this condition is not satisﬁed for GNKR  which induces the additional difﬁculty on
error analysis. Can we get the generalization error analysis for GNKR? It is also interesting
to explore the sampling strategy for GNKR  e.g.  the column-norm sampling in [3  4].

To address the above issues  we propose the GNKR algorithm and investigate its theoretical properties
on generalization bound and learning rate. Inspired from the recent studies for data dependent
hypothesis spaces [7  19]  we establish the error analysis for GNKR  which implies that the learning
rate with polynomial decay can be reached under proper parameter selection. Meanwhile  we extend
the (cid:96)2 column norm subsampling in the linear regression [16  22] to the GNKR setting.
The main contributions of this paper can be summarized as below:

• GNKR with (cid:96)2 regularization. Due to the lack of Mercer condition associated with general
kernel  coefﬁcient regularization becomes a natural choice to replace the kernel norm
regularization in KRR. Note that Nyström approximation has the similar role with the (cid:96)1
regularization in [7  18  20]  which addresses the sample sparsity on hypothesis function.
Hence  we formulate GNKR by combining the Nyström method and the least squares
regression with (cid:96)2 regularization in [19  21].
• Theoretical and empirical evaluations. From the view of learning with data dependent
hypothesis spaces  theoretical analysis of GNKR is established to illustrate its generalization
bound and learning rate. In particular  the fast learning rate arbitrarily close to O(m−1) is
obtained under mild conditions  where m is the size of subsampled set. The effectiveness of
GNKR is also supported by experiments on synthetic and real-world data sets.

2 Related Works

Due to the ﬂexibility and adaptivity  least squares regression algorithms with general kernel have been
proposed involving various types of regularization  e.g.  the (cid:96)1-regularizer [18  21]  the (cid:96)2-regularizer
[19  20]  and the elastic net regularization [7]. For the Mercer kernel  these algorithms are related
closely with the KRR  which has been well understand in learning theory. For the general kernel
setting  theoretical foundations of regression with coefﬁcient regularization have been studied recently
via the analysis techniques with the operator approximation [20] and the empirical covering numbers
[7  18  19]. Although rich results on theoretical analysis  the previous works mainly focus on the
prediction accuracy without considering the computation complexity for large scale data.
Nyström approximation has been studied extensively for kernel methods recently. Almost all existing
studies are relied on the fast approximation of SPSD matrix associated with a Mercer kernel. For the
ﬁxed design setting  the expectation of the excess generalization error is bounded for least square
regression with the regularizer in RKHS [1  2]. Recently  the probabilistic error bounds have been
estimated for Nyström KRR in [16  24]. In [24]  the fast learning rate with O(m−1) is derived for the
ﬁxed design regression under the conditions on kernel matrix eigenvalues. In [16]  the convergence
rate is obtained under the capacity assumption and the regularity assumption. It is worthy notice
that the learning bound in [16] is based on the estimates of the sample error  the computation error 
and the approximation error. Indeed  the computation error is related with the sampling subset and
can be considered as the hypothesis error in [18]  which is induced by the variance of hypothesis
spaces. Differently from previous works  our theoretical analysis of GNKR is dependent on general
continuous kernel and (cid:96)2 coefﬁcient regularization.

3 Generalized Nyström Kernel Regression
Let ρ be a probability distribution on Z := X × Y  where X ⊂ Rd and Y ⊂ R are viewed as the
input space and the output space  respectively. Let ρ(·|x) be the conditional distribution of ρ for

2

fρ(x) =

ydρ(y|x)

based on the empirical risk

Ez(f ) =

1
n

(yi − f (xi))2.

(cid:90)

Y

n(cid:88)

i=1

n(cid:88)

given x ∈ X and let F be a measurable function space on X . In statistical learning  the samples
z := {zi}n
i=1 are drawn independently and identically from an unknown distribution
ρ. The task of least squares regression is to ﬁnd a prediction function f : X → R such that the
expected risk

i=1 = {(xi  yi)}n

(cid:90)

Z

E(f ) =

(y − f (x))2dρ(x  y)

as small as possible. From the viewpoint of approximation theory  this means to search a good
approximation of the regression function

Let K : X × X → R be a continuous and bounded kernel function. Without loss of generality  we
assume that κ := sup
x x(cid:48)∈X

K(x  x(cid:48)) ≤ 1 and for all |y| ≤ 1 for all y ∈ Y throughout this paper.

Besides the given samples z  the hypothesis function space is crucial to reach well learning perfor-
mance. The following data dependent hypothesis space has been used for kernel regression with
coefﬁcient regularization:

˜αiK(xi  x) : ˜α = (˜α1  ...  ˜αn) ∈ Rn  x ∈ X(cid:111)

.

(cid:110)

Hn =

f (x) =

n(cid:88)

i=1

Given z  kernel regression with (cid:96)2 regularization [19  20] is formulated as

˜fz = f ˜αz =

˜αz iK(xi ·)

(1)

with

(cid:110) 1

n

˜αz = arg min

˜α∈Rn

i=1

(cid:107)Knn ˜α − Y (cid:107)2

2 + λn˜αT ˜α

(cid:111)

 

where Knn = (K(xi  xj))n
Even the positive semi-deﬁniteness is not required for the kernel  (3) also can be solved by the
following linear system (see Theorem 3.1 in [20])

i j=1  Y = (y1 ···   yn)T   and λ > 0 is a regularization parameter.

(K T
where In is the n-order unit matrix.
From the viewpoint of learning function in Hn  (1) can be rewritten as

nnKnn + λn2In)˜α = K T

nnY 

where

(cid:107)f(cid:107)2

(cid:96)2

= inf

˜fz = arg min
f∈Hn

(cid:110) n(cid:88)

(cid:111)

 

(cid:96)2

(cid:110)Ez(f ) + λn(cid:107)f(cid:107)2
n(cid:88)

˜α2

i : f =

˜αiK(xi ·)

(cid:111)

.

i=1

i=1

(2)

(3)

In a standard implementation of (2)  the computational complexity is O(n3). This computation
requirement becomes the bottleneck of (3) when facing large data sets. To reduce the computational
burden  we consider to ﬁnd the predictor in a smaller hypothesis space

Hm =

f (x) =

αiK(¯xi  x) : α = (α1  ...  αm) ∈ Rm  x ∈ X  {¯xi}m

i=1 ⊂ {xi}n

i=1

(cid:110)

m(cid:88)

(cid:111)

.

i=1

3

The generalized Nyström kernel regression (GNKR) can be formulated as

(4)
Denote (Knm)ij = K(xi  ¯xj)  (Kmm)jk = K(¯xi  ¯xj) for i ∈ {1  ...  n}  j  k ∈ {1  ...  m}. We can
deduce that

fz = arg min
f∈Hm

(cid:96)2

.

(cid:111)

(cid:110)Ez(f ) + λm(cid:107)f(cid:107)2
m(cid:88)

αz iK(¯xi ·)

fz =

with

i=1

(K T

nmKnm + λmnIm)αz = K T

nmY.

(5)

The key problem of (4) is how to select the subset {¯xi}m
i=1 such that the computational complexity
can be decreased efﬁciently while satisfactory accuracy can be guaranteed. For the KRR  there
are several strategies to select the subset with different motivations [5  11  12]. In this paper we
preliminarily consider the following two strategies with low computational complexity:

{xi}n

• Uniform Subsampling. The subset {¯xi}m
• Column-norm Subsampling. The subset {¯xi}m

i=1.

probabilities pi =

(cid:80)n
(cid:107)Ki(cid:107)2
i=1 (cid:107)Ki(cid:107)2

i=1 is drawn from {xi}n

i=1 independently with

  where Ki = (K(x1  xi)  ...  K(xn  xi))T ∈ Rn.

i=1 is drawn uniformly at random from the input

Some discussions for the column-norm subsampling will be provided in Section 4.

4 Learning Theory Analysis

In this section  we will introduce our theoretical results on generalization bound and learning rate.
The detailed proofs can be found in the supplementary materials.
Inspired from analysis technique in [7  19]  we introduce the intermediate function for error decom-
position ﬁrstly. Let F be the square integrable space on X with norm (cid:107) · (cid:107)L2
ρX . For any bounded
continuous kernel K : X × X → R  the integral operator LK : F → F is deﬁned as

LKf (x) =

(cid:90)
(cid:110)
g = LKf  f ∈ F(cid:111)

X

H =

where ρX is the marginal distribution of ρ. Given F and LK  introduce the function space

K(x  t)f (t)dρX (t) ∀x ∈ X  

with (cid:107)g(cid:107)H = inf(cid:8)(cid:107)f(cid:107)L2
(cid:110)E(LKf ) − E(fρ) + λ(cid:107)f(cid:107)2

ρX

(cid:111)

L2

ρX

: g = LKf(cid:9).

Since H is sample independent  the intermediate function can be constructed as gλ = LKfλ  where
(6)

fλ = arg min

.

f∈F

In learning theory  usually gλ is called as the regularized function and

D(λ) = inf

g∈H{E(g) − E(fρ) + λ(cid:107)g(cid:107)2H} = E(LKfλ) − E(fρ) + λ(cid:107)fλ(cid:107)2

L2

ρX

is called the approximation error
To further bridge the gap between gλ and fz  we construct the stepping stone function

m(cid:88)

i=1

ˆgλ =

1
m

fλ(¯xi)K(¯xi ·).

(7)

The following condition on K is used in this paper  which has been well studied in learning theory
literature [18  19]. Examples include Gaussian kernel  the sigmoid kernel [17]  and the fractional
power polynomials [13].

4

Deﬁnition 1 The kernel function K is a C s kernel with s > 0 if there exists some constant cs > 0 
such that

|K(t  x) − K(t  x(cid:48))| ≤ cs(cid:107)x − x(cid:48)(cid:107)s

2  ∀t  x  x(cid:48) ∈ X .

The deﬁnition of fρ tells us |fρ(x)| ≤ 1  so it is natural to restrict the predictor to [−1  1]. The
projection operator

π(f )(x) = min{1  f (x)}I{f (x) ≥ 0} + max{−1  f (x)}I{f (x) < 0}

has been extensively used in learning theory analysis  e.g. [6].
It is a position to present our result on the generalization error bound.
Theorem 1 Suppose that X is compact subset of Rd and K ∈ C s(X × X ) for some s > 0. For any
0 < δ < 1  with conﬁdence 1 − δ  there holds
E(π(fz)) − E(fρ) ≤ ˜c1 log2(8/δ)
where constant ˜c1 is independent of m  n  δ  and

(1 + m−1λ−1 + m−2λ−2 + n

− 2
2+p λ−2)D(λ) + n

(cid:16)

2+p λ

− p

− 2

2+p

(cid:17)

 

(cid:40) 2d/(d + 2s) 

p =

2d/(d + 2) 
d/s 

if 0 < s ≤ 1;
if 1 < s ≤ 1 + d/2;
if s > 1 + d/2.

(8)

L2

ρX

.

(9)

L2

ρX

− p

2+p

− 2

2+p λ

(cid:17)

(cid:16)

≤ O

} + n

c(m  n  λ) inf
f

{E(LKf ) − E(fρ) + λ(cid:107)f(cid:107)2

Theorem 1 is a general result that applies to Lipschitz continuous kernel. Although the statement
appears somewhat complicated at ﬁrst sight  it yields fast convergence rate on the error when
specialized to particular kernels. Before doing so  let us provide a few heuristic arguments for
intuition. Theorem 1 guarantees an upper bound of the form
(cid:107)π(fz) − fρ(cid:107)2
Note that a smaller value of λ reduces the approximation error term  but increases the second term
associated with the sample error. This inequality demonstrates that the proper λ should be selected
to balance the two terms. This quantitative relationship (9) also can be considered as the oracle
inequality for GNKR  where the approximation error D(λ) only can be obtained by an oracle knowing
the distribution.
Theorem 1 tells us that the generalization bound of GNKR depends on the numbers of samples m  n 
the continuous degree s  and the approximation error D(λ). In essential  the subsampling number m
has double impact on generalization error: one is the complexity of data dependent hypothesis space
Hm and the other is the selection of parameter λ.
Now we introduce the characterization of approximation error  which has been studied in [19  20].
Deﬁnition 2 The target function fρ can be approximated with exponent 0 < β ≤ 1 in H if there
exists a constant cβ ≥ 1 such that D(λ) ≤ cβλβ for any λ > 0.
If the kernel is not symmetric or positive semi-deﬁnite  the approximation condition holds true for
ρX   where L ˜K is the integral operator associated with ˜K(u  v) =
β = 2r
X K(u  x)K(v  x)dρX   (u  v) ∈ X 2 (see [7]).
Now we state our main results on the convergence rate.
Theorem 2 Let X be a compact subset of Rd. Assume that fρ can be approximated with exponent
0 < β ≤ 1 in H and K ∈ C s(X × X ) for some s > 0. Choose m ≤ n
2+p and λ = m−θ for some
θ > 0. For any 0 < δ < 1  with conﬁdence 1 − δ  there holds

3 when fρ ∈ L−r

∈ L2

(cid:82)

˜K

E(π(fz)) − E(fρ) ≤ ˜c2 log2(8/δ)m−γ 

where constant ˜c2 is independent of m  δ  and

(cid:110)

γ = min

2 − pθ
2 + p

1

(cid:111)

.

  2 + βθ − 2θ  βθ  1 + βθ − θ

5

Theorem 2 states the polynomial convergence rate of GNKR and indicates its dependence on the
subsampling size m as n ≥ m2+p. Similar observation also can be found in Theorem 2 [16] for
Nyström KRR  where the fast learning rate also is relied on the grow of m under ﬁxed hypothesis
space complexity. However  even we do not consider the complexity of hypothesis space  the increase
of m will add the computation complexity. Hence  a suitable size of m is a trade off between the
approximation performance and the computation complexity. When p ∈ (0  2)  m = n
2+p means
that m can be chosen between n 1
2 under the conditions in Theorem 4. In particular  the fast
convergence rate O(m−1) can be obtained as K ∈ C∞  θ → 1  and β → 1.
The most related works with Theorems 1 and 2 are presented in [16  24]  where learning bounds are
established for Nyström KRR. Compared with the previous results  the features of this paper can be
summarized as below.

4 and 1

1

• Learning model. This paper considered Nyström regression with data dependent hypothesis
space and coefﬁcient regularization  which can employ general kernel including the indeﬁ-
nite kernel and nonsymmetric kernel. However  the previous analysis just focuses on the
positive semi-deﬁnite kernel and the regularizer in RKHS. For a ﬁxed design KRR  the fast
convergence O(m−1) in [24] depends on the eigenvalue condition of kernel matrix. Differ-
ently from [24]  our result relies on the Lipschitz continuity of kernel and the approximation
condition D(λ) for the statistical learning setting.

• Analysis technique. The previous analysis in [16  24] utilizes the theoretical techniques for
operator approximation and matrix decomposition  which depends heavily on the symmetric
positive semi-deﬁnite kernel. For GNKR (4)  the previous analysis is not valid directly since
the kernel is not necessary to satisfy the positive semi-deﬁnite or symmetric condition. The
ﬂexibility on kernel and the adaptivity on hypothesis space induce the additional difﬁculty
on error analysis. Fortunately  the error analysis is obtained by incorporating the error
decomposition ideas in [7] and the concentration estimate techniques in [18  19]. An
interesting future work is to establish the optimal bound of GNKR to extend Theorem 2 in
[16] to the general setting.

the objective function S(p) := S(p1  ...  pn) = (cid:80)n

For the proofs of Theorem 1 and 2  the key idea is using ˆgλ as the stepping stone function to bridge fz
and gλ. Additionally  the connection between gλ = LKfλ and fρ has been well studied in learning
theory. Hence  the proofs in Appendix follow from the approximation decomposition.
In remainder of this section  we present a simple analysis for column-norm subsampling.
Given the full samples z = {(xi  yi)}n
i=1 and sampling number m  the key of subsampling is to
select a subset of z with strong inference ability. In other words  we should select the subset with
small divergence with the full sample estimator. Following this idea  the optimal subsampling
criterion is studied in [28  22] for the linear regression. Given z = {zi}n
i=1 and Knn  we introduce
2 by extending (16) in [28] to
i=1 are the sampling probabilities with respect to {xi}n
the kernel-based setting. Here {pi}n
i=1 and
nn)ii  i ∈ {1  ...  n} are basic leverage values obtained from
Lii = (Knn(K T
i α0 + εi  i = 1  ...  n  α0 ∈ Rn  where {εi}n
(2). For the ﬁxed design setting  assume that yi = K T
are drawn identically and independently from N (0  σ2). Then  for λ = 0  min
i=1
S(p1  ...  pn) can
Etr((Knn)T (diag(p))−1Knn)  which is related with the A-optimality or
be transformed as min
A-criterion for subset selection in [22].
When Lii → 0 for any i ∈ {1  ...  n}  we can get the following sampling probabilities.
Theorem 3 When hii = o(1) for 1 ≤ i ≤ n  the minimizer of S(p1  ...  pn) can be approximated by

nnKnn + λn2In)−1K T

(cid:107)Ki(cid:107)2

p

1−Lii

i=1

pi

p

(cid:80)n
(cid:107)Ki(cid:107)2
i=1 (cid:107)Ki(cid:107)2

pi =

  i ∈ {1  ...  n}.

Usually  the leverage values are computed by fast approximation algorithms [1  16] since Lii involves
the inverse matrix. Different from the leverage values  the sampling probabilities in Theorem 3 can
be computed directly  which just involves the (cid:96)2 column-norm of empirical matrix.

6

Table 1: Average RMSE of GNKR with Gaussian(G)/Epanechnikov(E) kernel under different
sampling strategies and sampling size. US:=Uniform subsampling  CS: Column-norm subsampling.

Function

f1(x) = x sin x

x ∈ [0  2π]

f2(x) = sin x
x ∈ [−2π  2π]

x

f3(x) = sign(x)

x ∈ [−3  3]

f4(x) = cos(ex) + sin x

x

x ∈ [−2  4]

Algorithm

G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS

(cid:93)300
0.03412
0.03420
0.10159
0.09941
0.03442
0.03444
0.04786
0.04607
0.29236
0.29319
0.16170
0.16500
0.34916
0.34909
0.22298
0.21624

(cid:93)400
0.03145
0.03086
0.09653
0.09414
0.03434
0.03423
0.04191
0.03865
0.29102
0.29071
0.15822
0.15579
0.35158
0.35171
0.21012
0.20783

(cid:93)500
0.02986
0.02954
0.09081
0.08908
0.03418
0.03419
0.04073
0.03709
0.29009
0.28983
0.15537
0.15205
0.35155
0.35168
0.20265
0.20024

(cid:93)600
0.02919
0.02911
0.08718
0.08631
0.03409
0.03408
0.03692
0.03573
0.28908
0.28975
0.15188
0.15201
0.35148
0.35133
0.19977
0.19698

(cid:93)700
0.02897
0.02890
0.08515
0.08450
0.03404
0.03397
0.03582
0.03510
0.28867
0.28903
0.15086
0.14949
0.35156
0.35153
0.19414
0.19260

(cid:93)800
0.02906
0.02878
0.08278
0.08237
0.03400
0.03397
0.03493
0.03441
0.28839
0.28833
0.14889
0.14698
0.35140
0.35145
0.19126
0.18996

(cid:93)900
0.02896
0.02891
0.08198
0.08118
0.03398
0.03396
0.03470
0.03316
0.28755
0.28797
0.14730
0.14597
0.35136
0.35141
0.18916
0.18702

(cid:93)1000
0.02908
0.02889
0.08024
0.07898
0.03395
0.03389
0.03440
0.03383
0.28742
0.28768
0.14726
0.14566
0.35139
0.35138
0.18560
0.18662

5 Experimental Analysis

Since kernel regression with different types of regularization has been well studied in [7  20  21]  this
section just presents the empirical evaluation of GNKR to illustrate the roles of sampling strategy and

kernel function. Gaussian kernel KG(x  t) = exp(cid:8) − (cid:107)x−t(cid:107)2
data. Epanechnikov kernel KE(x  t) =(cid:0)1 − (cid:107)x−t(cid:107)2

(cid:9) is used for simulated data and real

(cid:1)

+ is used in the simulated experiment. Here  σ
denotes the scale parameter selected form [10−5 : 10 : 104]. Following the discussion on parameter
selection in [16]  we select the regularization parameter of GNKR from [10−15 : 10 : 10−3]. The
best results are reported according to the measure of Root Mean Squared Error (RMSE).

2σ2

2

2

2σ2

5.1 Experiments on synthetic data
Following the empirical studies in [20  21]  we design simulation experiments on f1(x) = x sin x  x ∈
x   x ∈ [−2π  2π]  f3(x) = sign(x)  x ∈ [−3  3]  and f4(x) = cos(ex) +
[0  2π]  f2(x) = sin x
x   x ∈ [−2  4]. The function fi is considered as the truly regression function for 1 ≤ i ≤ 4. Note
sin x
that f1  f2 are smooth  f3 is not continuous  and f4 embraces a highly oscillatory part. First  we select
10000 points randomly from the preset interval and generate the dependent variable y according to
the corresponding function. Then we divided these data into two parts with equal size. we chose one
part as the training samples and the other is regarded as testing samples. For the training samples  the
output y is contaminated by Gaussian noise N (0  1). For each function and each kernel  we run the
experiment 20 times. The average RMSE is shown in Table 1. The results indicate that the column
norm subsampling can achieve the satisfactory performance. In particular  GNKR with the indeﬁnite
Epanechnikov kernel has better performance than Gaussian kernel for the noncontinuous function f3
and the non-ﬂat function f4. This observation is consistent with the empirical result in [21].

5.2 Experiments on real data

In order to better evaluate the empirical performance  four data sets are used in our study including
the Wine Quality  CASP  Year Prediction datasets (http://archive.ics.uci.edu/ml/) and the census-house
dataset (http://www.cs.toronto.edu/ delve/data/census-house/desc.html). The detailed information
about the data sets are showed in Table 2. Firstly  each data set is standardized by subtracting its
mean and dividing its standard deviation. Then  each input vector is unitized. For CASP and Year
Prediction  20000 samples are drawn randomly from data sets  where half is used for training and the
rest is for testing. For other datasets  we random select part samples to training and use the rest part
as test set. Table 3 reports the average RMSE over ten trials.
Table 3 shows the performance of two sampling strategies. For CASP  and Year Prediction  we can
see that GNKR with 100 selected samples can achieve the satisfactory performance  which reduce
the computation complexity of (2) efﬁciently. Additionally  the competitive performance of GNKR
with Epanechnikov kernel is demonstrated via the experimental results on the four data sets. These
empirical examples support the effectiveness of the proposed method.

7

Table 2: Statistics of data sets

Dataset

Wine Quality
Year Prediction

#Features

#Instances

12
90

4898
515345

#Train
2000
10000

#Test
2898
10000

Dataset
CASP

census-house

#Feature

#Instance

9
139

45730
22784

#Train
10000
12000

#Test
10000
10784

Table 3: Average RMSE (×10−3) with Gaussian(G)/Epanechnikov(E) kernel under different sampling
levels and strategies. US:=Uniform subsampling  CS: Column-norm subsampling.

Function

Wine Quality

CASP

Year Prediction

census-house

Algorithm

G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS
G-GNKR-US
G-GNKR-CS
E-GNKR-US
E-GNKR-CS

(cid:93)50
14.567
14.563
13.990
13.969
9.275
9.220
4.282
4.206
8.806
8.806
7.013
7.006
111.084
111.083
102.731
102.703

(cid:93)100
14.438
14.432
13.928
13.899
9.238
9.196
4.196
4.249
8.802
8.801
6.842
6.861
111.083
111.080
99.535
99.528

(cid:93)200
14.382
14.394
13.807
13.798
9.205
9.205
4.213
4.206
8.798
8.798
6.739
6.804
111.082
111.080
99.698
99.697

(cid:93)400
14.292
14.225
13.636
13.601
9.222
9.193
4.153
4.182
8.795
8.793
6.700
6.705
111.079
111.079
99.718
99.716

(cid:93)600
14.189
14.138
13.473
13.445
9.204
9.198
4.181
4.172
8.792
8.792
6.676
6.697
111.077
111.075
99.715
99.714

(cid:93)800
14.103
14.014
13.381
13.362
9.207
9.199
4.174
4.165
8.790
8.789
6.671
6.663
111.074
111.071
99.714
99.714

(cid:93)1000
13.936
13.936
13.217
13.239
9.205
9.198
4.180
4.118
8.782
8.781
6.637
6.662
111.071
111.068
99.713
99.712

6 Conclusion

This paper focuses on the learning theory analysis of Nyström kernel regression. One key difference
with the previous related work is that GNKR uses general continuous kernel function and (cid:96)2 coefﬁcient
regularization. The stepping-stone functions are constructed to overcome the analysis difﬁculty
induced by the difference. The learning bound with fast convergence is derived under mild conditions
and empirical analysis is provided to verify our theoretical analysis.

Acknowledgments

This work was partially supported by U.S. NSF-IIS 1302675  NSF-IIS 1344152  NSF-DBI 1356628 
NSF-IIS 1619308  NSF-IIS 1633753  NIH AG049371  and by National Natural Science Foundation
of China (NSFC) 11671161. We thank the anonymous NIPS reviewers for insightful comments.

References
[1] A. Alaoui and M. W. Mahoney. Fast randomized kernel methods with statistical guarantees. In

NIPS  pp. 775–783  2015.

[2] F. Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT  2013.

[3] P. Drineas  R. Kannan  and M.W. Mahoney. Fast Monte Carlo algorithms for matrices I:
Computing a low-rank approximation to a matrix. SIAM Journal on Computing  pp. 158–183 
2006.

[4] P. Drineas and M.W. Mahoney. On the Nyström method for approximating a Gram matrix for

improved kernel-based learning. J. Mach. Learn. Res.  6: 2153–2175  2005.

[5] P. Drineas  M. Magdon-Ismail  M.W. Mahoney  and D.P. Woodruff. Fast approximation of

matrix coherence and statistical leverage. J. Mach. Learn. Res.  13: 3475–3506  2012.

[6] M. Eberts and I. Steinwart. Optimal learning rates for least squares SVMs using Gaussian

kernels. In NIPS  pp. 1539–1547  2011.

[7] Y. Feng  S. Lv  H. Huang  and J. Suykens. Kernelized elastic net regularization: generalization

bounds and sparse recovery. Neural Computat.  28: 1–38  2016.

[8] A. Gittens and M.W. Mahoney. Revisiting the Nyström method for improved large-scale machine

learning. In ICML  pp. 567–575  2013

8

[9] C.J. Hsieh  S. Si  and I.S. Dhillon. Fast prediction for large scale kernel machines. In NIPS  pp.

3689–3697  2014.

[10] R. Jin  T. Yang  M. Mahdavi  Y. Li  and Z. Zhou. Improved bounds for the Nyström method

with application to kernel classiﬁcation. IEEE Trans. Inf. Theory  59(10): 6939–6949  2013.

[11] S. Kumar  M. Mohri  and A. Talwalkar. Sampling methods for the Nyström method. J. Mach.

Learn. Res.  13: 981–1006  2012.

[12] W. Lim  M. Kim  H. Park  and K. Jung. Double Nyström method: An efﬁcient and accurate

Nyström scheme for large-scale data sets. In ICML  pp. 1367–1375  2015.

[13] C. Liu. Gabor-based kernel PCA with fractional power polynomial models for face recognition.

IEEE Trans. Pattern Anal. Mach. Intell.  26: 572–581  2004.

[14] E. Pekalska and B. Haasdonk. Kernel discriminant analysis with positive deﬁnite and indeﬁnite

kernels. IEEE Trans. Pattern. Anal. Mach. Intell. 31: 1017–1032  2009.

[15] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  pp. 1177–

1184  2007.

[16] A. Rudi  R. Camoriano  R. Rosasco. Less is more: Nyström computation regularization. In

NIPS  1657–1665  2015.

[17] B. Schölkopf and A.J. Smola. Learning with Kernels: Support Vector Machines  Regularization 

Optimization  and Beyond . MIT Press  2001.

[18] L. Shi  Y. Feng  and D.X. Zhou. Concentration estimates for learning with (cid:96)1-regularizer and

data dependent hypothesis spaces. Appl. Comput. Harmon. Anal.  31(2): 286–302  2011.

[19] L. Shi. Learning theory estimates for coefﬁcient-based regularized regression. Appl. Comput.

Harmon. Anal.  34(2): 252–265  2013.

[20] H. Sun and Q. Wu. Least square regression with indeﬁnite kernels and coefﬁcient regularization.

Appl. Comput. Harmon. Anal.  30(1): 96–109  2011.

[21] H. Sun and Q. Wu. Sparse representation in kernel machines. IEEE Trans. Neural Netw. Learning

Syst.  26(10): 2576–2582  2015.

[22] Y. Wang and A. Singh. Minimax subsampling for estimation and prediction in low-dimensional

linear regression. arXiv  2016 (https://arxiv.org/pdf/1601.02068v2.pdf).

[23] C. Williams and M. Seeger. Using the Nyström method to speed up kernel machines. In NIPS 

pp. 682–688  2001.

[24] T. Yang  Y.F. Li  M. Mahdavi  R. Jin  and Z.H. Zhou. Nyström method vs random Fourier

features: A theoretical and empirical comparison. In NIPS  2012  pp. 485–493.

[25] Y. Yang  M. Pilanci and M. J. Wainwright. Randomized sketches for kernels: Fast and optimal

non-parametric regression. arxiv:1501.06195  2015.(http://arxiv.org/abs/1501.06195).

[26] Y. Ying  C. Campbell  and M. Girolami. Analysis of SVM with indeﬁnite kernels. In NIPS  pp.

2205–2213  2009.

[27] K. Zhang  I.W. Tsang  and J.T. Kwok. Improved Nyström low-rank approximation and error

analysis. In ICML  pp. 1232–1239  2008.

[28] R. Zhu  P. Ma  M.W. Mahoney  and B. Yu. Optimal subsampling approaches for large sample

linear regression. arXiv:1509.05111  2015 (http://arxiv.org/abs/1509.05111).

9

,Sanjoy Dasgupta
Samory Kpotufe
Jessa Bekker
Jesse Davis
Arthur Choi
Adnan Darwiche
Guy Van den Broeck
Hong Chen
Haifeng Xia
Heng Huang
Weidong Cai
Jen Ning Lim
Makoto Yamada
Bernhard Schölkopf
Wittawat Jitkrittum