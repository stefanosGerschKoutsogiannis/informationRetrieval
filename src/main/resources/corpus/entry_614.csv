2019,Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance,Minimum expected distance estimation (MEDE) algorithms have been widely used for probabilistic models with intractable likelihood functions and they have become increasingly popular due to their use in implicit generative modeling (e.g.\ Wasserstein generative adversarial networks  Wasserstein autoencoders). Emerging from computational optimal transport  the Sliced-Wasserstein (SW) distance has become a popular choice in MEDE thanks to its simplicity and computational benefits. While several studies have reported empirical success on generative modeling with SW  the theoretical properties of such estimators have not yet been established. In this study  we investigate the asymptotic properties of estimators that are obtained by minimizing SW. We first show that convergence in SW implies weak convergence of probability measures in general Wasserstein spaces. Then we show that estimators obtained by minimizing SW (and also an approximate version of SW) are asymptotically consistent. We finally prove a central limit theorem  which characterizes the asymptotic distribution of the estimators and establish a convergence rate of $\sqrt{n}$  where $n$ denotes the number of observed data points. We illustrate the validity of our theory on both synthetic data and neural networks.,Asymptotic Guarantees for Learning Generative

Models with the Sliced-Wasserstein Distance

Kimia Nadjahi1  Alain Durmus2  Umut ¸Sim¸sekli1 3  Roland Badeau1

1: LTCI  Télécom Paris  Institut Polytechnique de Paris  France
2: CMLA  ENS Cachan  CNRS  Université Paris-Saclay  France

3: Department of Statistics  University of Oxford  UK

{kimia.nadjahi  umut.simsekli  roland.badeau}@telecom-paris.fr

alain.durmus@cmla.ens-cachan.fr

Abstract

Minimum expected distance estimation (MEDE) algorithms have been widely
used for probabilistic models with intractable likelihood functions and they have
become increasingly popular due to their use in implicit generative modeling (e.g.
Wasserstein generative adversarial networks  Wasserstein autoencoders). Emerging
from computational optimal transport  the Sliced-Wasserstein (SW) distance has
become a popular choice in MEDE thanks to its simplicity and computational
beneﬁts. While several studies have reported empirical success on generative
modeling with SW  the theoretical properties of such estimators have not yet been
established. In this study  we investigate the asymptotic properties of estimators
that are obtained by minimizing SW. We ﬁrst show that convergence in SW implies
weak convergence of probability measures in general Wasserstein spaces. Then we
show that estimators obtained by minimizing SW (and also an approximate version
of SW) are asymptotically consistent. We ﬁnally prove a central limit theorem 
which characterizes the asymptotic distribution of the estimators and establish a
convergence rate of √n  where n denotes the number of observed data points. We
illustrate the validity of our theory on both synthetic data and neural networks.

1

Introduction

Minimum distance estimation (MDE) is a generalization of maximum-likelihood inference  where
the goal is to minimize a distance between the empirical distribution of a set of independent and
identically distributed (i.i.d.) observations Y1:n = (Y1  . . .   Yn) and a family of distributions indexed
by a parameter θ. The problem is formally deﬁned as follows [1  2]:

(1)
where D denotes a distance (or a divergence in general) between probability measures  µθ denotes a
probability measure indexed by θ  Θ denotes the parameter space  and

ˆθn = argminθ∈Θ D(ˆµn  µθ)  

ˆµn =

δYi

(2)

1

n(cid:88)n

i=1

denotes the empirical measure of Y1:n  with δY being the Dirac distribution with mass on the point Y .
When D is chosen as the Kullback-Leibler divergence  this formulation coincides with the maximum
likelihood estimation (MLE) [2].
While MDE provides a fruitful framework for statistical inference  when working with generative
models  solving the optimization problem in (1) might be intractable since it might be impossible to
evaluate the probability density function associated with µθ. Nevertheless  in various settings  even if

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the density is not available  one can still generate samples from the distribution µθ  and such samples
turn out to be useful for making inference. More precisely  under such settings  a natural alternative
to (1) is the minimum expected distance estimator  which is deﬁned as follows [3]:

Here 

ˆθn m = argminθ∈Θ

1

ˆµθ m =

E [D(ˆµn  ˆµθ m)|Y1:n] .
m(cid:88)m

δZi

i=1

(3)

(4)

denotes the empirical distribution of Z1:m  that is a sequence of i.i.d. random variables with distribu-
tion µθ. This algorithmic framework has computationally favorable properties since one can replace
the expectation with a simple Monte Carlo average in practical applications.
In the context of MDE  distances that are based on optimal transport (OT) have become increasingly
popular due to their computational and theoretical properties [4  5  6  7  8]. For instance  if we
replace the distance D in (3) with the Wasserstein distance (deﬁned in Section 2 below)  we obtain
the minimum expected Wasserstein estimator [3]. In the classical statistical inference setting  the
typical use of such an estimator is to infer the parameters of a measure whose density does not
admit an analytical closed-form formula [2]. On the other hand  in the implicit generative modeling
(IGM) setting  this estimator forms the basis of two popular IGM strategies: Wasserstein generative
adversarial networks (GAN) [4] and Wasserstein variational auto-encoders (VAE) [5] (cf. [9] for their
relation). The goal of these two methods is to ﬁnd the best parametric transport map Tθ  such that Tθ
transforms a simple distribution µ (e.g. standard Gaussian or uniform) to a potentially complicated
data distribution ˆµn by minimizing the Wasserstein distance between the transported distribution
µθ = Tθ(cid:93)µ and ˆµn  where (cid:93) denotes the push-forward operator  to be deﬁned in the next section.
In practice  θ is typically chosen as a neural network  for which it is often impossible to evaluate
the induced density µθ. However  one can easily generate samples from µθ by ﬁrst generating a
sample from µ and then applying Tθ to that sample  making minimum expected distance estimation
(3) feasible for this setting. Motivated by its practical success  the theoretical properties of this
estimator have been recently taken under investigation [10  11] and very recently Bernton et al. [3]
have established the consistency (for the general setting) and the asymptotic distribution (for one
dimensional setting) of this estimator.
Even though estimation with the Wasserstein distance has served as a fertile ground for many
generative modeling applications  except for the case when the measures are supported on R1  the
computational complexity of minimum Wasserstein estimators rapidly becomes excessive with the
increasing problem dimension  and developing accurate and efﬁcient approximations is a highly
non-trivial task. Therefore  there have been several attempts to use more practical alternatives to the
Wasserstein distance [12  6]. In this context  the Sliced-Wasserstein (SW) distance [13  14  15] has
been an increasingly popular alternative to the Wasserstein distance  which is deﬁned as an average
of one-dimensional Wasserstein distances  which allows it to be computed in an efﬁcient manner.
While several studies have reported empirical success on generative modeling with SW [16  17  18 
19]  the theoretical properties of such estimators have not yet been fully established. Bonnotte [14]
proved that SW is a proper metric  and in compact domains SW is equivalent to the Wasserstein
distance  hence convergence in SW implies weak convergence in compact domains. [14] also analyzed
the gradient ﬂows based on SW  which then served as a basis for a recently proposed IGM algorithm
[18]. Finally  recent studies [16  20] investigated the sample complexity of SW and established
bounds for the SW distance between two measures and their empirical instantiations.
In this paper  we investigate the asymptotic properties of estimators given in (1) and (3) when D is
replaced with the SW distance. We ﬁrst prove that convergence in SW implies weak convergence
of probability measures deﬁned on general domains  which generalizes the results given in [14].
Then  by using similar techniques to the ones given in [3]  we show that the estimators deﬁned by
(1) and (3) are consistent  meaning that as the number of observations n increases the estimates will
get closer to the data-generating parameters. We ﬁnally prove a central limit theorem (CLT) in the
multidimensional setting  which characterizes the asymptotic distribution of these estimators and
establishes a convergence rate of √n. The CLT that we prove is stronger than the one given in [3] in
the sense that it is not restricted to the one-dimensional setting as opposed to [3].
We support our theory with experiments that are conducted on both synthetic and real data. We ﬁrst
consider a more classical statistical inference setting  where we consider a Gaussian model and a

2

multidimensional α-stable model whose density is not available in closed-form. In both models 
the experiments validate our consistency and CLT results. We further observe that  especially for
high-dimensional problems  the estimators obtained by minimizing SW have signiﬁcantly better
computational properties when compared to the ones obtained by minimizing the Wasserstein distance 
as expected. In the IGM setting  we consider the neural network-based generative modeling algorithm
proposed in [16] and show that our results also hold in the real data setting as well.

N∗

2 Preliminaries and Technical Background
We consider a probability space (Ω F  P) with associated expectation operator E  on which all
the random variables are deﬁned. Let (Yk)k∈N be a sequence of random variables associated with
observations  where each observation takes value in Y ⊂ Rd. We assume that these observations are
i.i.d. according to µ(cid:63) ∈ P(Y)  where P(Y) stands for the set of probability measures on Y.
A statistical model is a family of distributions on Y and is denoted by M = {µθ ∈ P(Y)  θ ∈ Θ} 
where Θ ⊂ Rdθ is the parametric space. In this paper  we focus on parameter inference for purely
from µθ  but the
generative models: for all θ ∈ Θ  we can generate i.i.d. samples (Zk)k∈N∗ ∈ Y
associated likelihood is numerically intractable. In the sequel  (Zk)k∈N∗ denotes an i.i.d. sequence
from µθ with θ ∈ Θ  and for any m ∈ N∗  ˆµθ m = (1/m)(cid:80)m
i=1 δZi denotes the corresponding
empirical distribution.
Throughout our study  we assume that the following conditions hold: (1) Y  endowed with the
Euclidean distance ρ  is a Polish space  (2) Θ  endowed with the distance ρΘ  is a Polish space  (3) Θ
is a σ-compact space  i.e. the union of countably many compact subspaces  and (4) parameters are
identiﬁable  i.e. µθ = µθ(cid:48) implies θ = θ(cid:48). We endow P(Y) with the Lévy-Prokhorov distance dP 
which metrizes the weak convergence by [21  Theorem 6.8] since Y is assumed to be a Polish space.
We denote by Y the Borel σ-ﬁeld of (Y  ρ).
Wasserstein distance. For p ≥ 1  we denote by Pp(Y) the set of probability measures on Y with
ﬁnite p’th moment: Pp(Y) =(cid:8)µ ∈ P(Y) : (cid:82)Y (cid:107)y − y0(cid:107)p dµ(y) < +∞  for some y0 ∈ Y(cid:9). The
Wasserstein distance of order p between any µ  ν ∈ Pp(Y) is deﬁned by [22] 
γ∈Γ(µ ν)(cid:26)(cid:90)Y×Y (cid:107)x − y(cid:107)p dγ(x  y)(cid:27)  

where Γ(µ  ν) is the set of probability measures γ on (Y × Y Y ⊗ Y) satisfying γ(A × Y) = µ(A)
and γ(Y × A) = ν(A) for any A ∈ B(Y). The space Pp(Y) endowed with the distance Wp is a
Polish space by [22  Theorem 6.18] since (Y  ρ) is assumed to be Polish.
The one-dimensional case is a favorable scenario for which computing the Wasserstein distance of
order p between µ  ν ∈ Pp(R) becomes relatively easy since it has a closed-form formula  given by
[23  Theorem 3.1.2.(a)]:

p(µ  ν) = inf

Wp

(5)

Wp

p(µ  ν) =(cid:90) 1

µ (t) − F −1

ν

0 (cid:12)(cid:12)F −1

(t)(cid:12)(cid:12)p

dt =(cid:90)R(cid:12)(cid:12)s − F −1

ν

(Fµ(s))(cid:12)(cid:12)p

dµ(s)  

(6)

ν

µ

and F −1

where Fµ and Fν denote the cumulative distribution functions (CDF) of µ and ν respectively  and
are the quantile functions of µ and ν respectively. For empirical distributions  (6) is
F −1
calculated by simply sorting the n samples drawn from each distribution and computing the average
cost between the sorted samples.
Sliced-Wasserstein distance. The analytical form of the Wasserstein distance for one-dimensional
distributions is an attractive property that gives rise to an alternative metric referred to as the Sliced-
Wasserstein (SW) distance [13  15]. The idea behind SW is to ﬁrst  obtain a family of one-dimensional
representations for a higher-dimensional probability distribution through linear projections  and then 
compute the average of the Wasserstein distance between these one-dimensional representations.

More formally  let Sd−1 =(cid:8)u ∈ Rd : (cid:107)u(cid:107) = 1(cid:9) be the d-dimensional unit sphere  and denote by
(cid:104)· ·(cid:105) the Euclidean inner-product. For any u ∈ S  we deﬁne u(cid:63) the linear form associated with u
for any y ∈ Y by u(cid:63)(y) = (cid:104)u  y(cid:105). The Sliced-Wasserstein distance of order p is deﬁned for any
µ  ν ∈ Pp(Y) as 
(7)

SWp

Wp

p(u(cid:63)

(cid:93) µ  u(cid:63)

(cid:93) ν)dσ(u)

p(µ  ν) =(cid:90)Sd−1

3

where σ is the uniform distribution on Sd−1 and for any measurable function f : Y → R and
ζ ∈ P(Y)  f(cid:93)ζ is the push-forward measure of ζ by f  i.e. for any A ∈ B(R)  f(cid:93)ζ(A) = ζ(f−1(A))
where f−1(A) = {y ∈ Y : f (y) ∈ A}.
SWp is a distance on Pp(Y) [14] and has important practical implications: in practice  the integration
in (7) is approximated using a Monte Carlo scheme that randomly draws a ﬁnite set of samples from
σ on Sd−1 and replaces the integral with a ﬁnite-sample average. Therefore  the evaluation of the
SW distance between µ  ν ∈ Pp(Y) has signiﬁcantly lower computational requirements than the
Wasserstein distance  since it consists in solving several one-dimensional optimal transport problems 
which have closed-form solutions.

3 Asymptotic Guarantees for Minimum Sliced-Wasserstein Estimators

We deﬁne the minimum Sliced-Wasserstein estimator (MSWE) of order p as the estimator obtained by
plugging SWp in place of D in (1). Similarly  we deﬁne the minimum expected Sliced-Wasserstein
estimator (MESWE) of order p as the estimator obtained by plugging SWp in place of D in (3). In
the rest of the paper  MSWE and MESWE will be denoted by ˆθn and ˆθn m respectively.
We present the asymptotic properties that we derived for MSWE and MESWE  namely their existence
and consistency. We study their measurability in Section 2.2 of the supplementary document. We
also formulate a CLT that characterizes the asymptotic distribution of MSWE and establishes a
convergence rate for any dimension. We provide all the proofs in Section 3 of the supplementary doc-
ument. Note that  since the Sliced-Wasserstein distance is an average of one-dimensional Wasserstein
distances  some proofs are  inevitably  similar to the proofs done in [3]. However  the adaptation of
these techniques to the SW case is made possible by the identiﬁcation of novel properties regarding
the topology induced by the SW distance  to the best of our knowledge  which we establish for the
ﬁrst time in this study.

3.1 Topology induced by the Sliced-Wasserstein distance

We begin this section by a useful result which we believe is interesting on its own and implies that the
topology induced by SWp on Pp(Rd) is ﬁner than the weak topology induced by the Lévy-Prokhorov
metric dP.
Theorem 1. Let p ∈ [1  +∞). The convergence in SWp implies the weak convergence in P(Rd). In
other words  if (µk)k∈N is a sequence of measures in Pp(Rd) satisfying limk→+∞ SWp(µk  µ) = 0 
with µ ∈ Pp(Rd)  then (µk)k∈N w
The property that convergence in SWp implies weak convergence has already been proven in [14]
only for compact domains. While the implication of weak convergence is one of the most crucial
requirements that a distance metric should satisfy  to the best of our knowledge  this implication has
not been proved for general domains before. In [14]  the main proof technique was based on showing
that SWp is equivalent to Wp in compact domains  whereas we follow a different path and use the
Lévy characterization.

−→ µ.

3.2 Existence and consistency of MSWE and MESWE

In our next set of results  we will show that both MSWE and MESWE are consistent  in the sense
that  when the number of observations n increases  the estimators will converge to a parameter θ(cid:63)
that minimizes the ideal problem θ (cid:55)→ SWp(µ(cid:63)  µθ). Before we make this argument more precise 
let us ﬁrst present the assumptions that will imply our results.
A1. The map θ (cid:55)→ µθ is continuous from (Θ  ρΘ) to (P(Y)  dP )  i.e. for any sequence (θn)n∈N in
Θ  satisfying limn→+∞ ρΘ(θn  θ) = 0  we have (µθn )n∈N w
A2. The data-generating process is such that limn→+∞ SWp(ˆµn  µ(cid:63)) = 0  P-almost surely.
A3. There exists  > 0  such that setting (cid:63) = inf θ∈Θ SWp(µ(cid:63)  µθ)  the set Θ(cid:63)
SWp(µ(cid:63)  µθ) ≤ (cid:63) + } is bounded.
These assumptions are mostly related to the identiﬁability of the statistical model and the regularity
of the data generating process. They are arguably mild assumptions  analogous to those that have

 = {θ ∈ Θ :

−→ µθ.

4

already been considered in the literature [3]. Note that  without Theorem 1  the formulation and use
of A2 in our proofs in the supplementary document would not be possible. In the next result  we
establish the consistency of MSWE.
Theorem 2 (Existence and consistency of MSWE). Assume A1  A2 and A3. There exists E ∈ F
with P(E) = 1 such that  for all ω ∈ E 
(8)

SWp(µ(cid:63)  µθ)  and

lim

inf
θ∈Θ

SWp(ˆµn(ω)  µθ) = inf
θ∈Θ

n→+∞
argminθ∈ΘSWp(ˆµn(ω)  µθ) ⊂ argminθ∈ΘSWp(µ(cid:63)  µθ)  

(9)

lim sup
n→+∞

where ˆµn is deﬁned by (2). Besides  for all ω ∈ E  there exists n(ω) such that  for all n ≥ n(ω)  the
set argminθ∈ΘSWp(ˆµn(ω)  µθ) is non-empty.
Our proof technique is similar to the one given in [3]. This result shows that  when the number of
observations goes to inﬁnity  the estimate ˆθn will converge to a global minimizer of the problem
minθ∈Θ SWp(µ(cid:63)  µθ).
In our next result  we prove a similar property for MESWEs as min(m  n) goes to inﬁnity. In order
to increase clarity  and without loss of generality  in this setting  we consider m as a function of n
such that limn→+∞ m(n) = +∞. Now  we derive an analogous version of Theorem 2 for MESWE.
For this result  we need to introduce another continuity assumption.
A4. If limn→+∞ ρΘ(θn  θ) = 0  then limn→+∞
The next theorem establishes the consistency of MESWE.
Theorem 3 (Existence and consistency of MESWE). Assume A1  A2  A3 and A4. Let (m(n))n∈N∗
be an increasing sequence satisfying limn→+∞ m(n) = +∞. There exists a set E ⊂ Ω with
P(E) = 1 such that  for all w ∈ E 

E[SWp(µθn   ˆµθn n)|Y1:n] = 0.

lim

inf
n→+∞
θ∈Θ
argminθ∈Θ

lim sup
n→+∞

E(cid:2)SWp(ˆµn  ˆµθ m(n))(cid:12)(cid:12)Y1:n(cid:3) = inf
E(cid:2)SWp(ˆµn  ˆµθ m(n))(cid:12)(cid:12)Y1:n(cid:3) ⊂ argminθ∈Θ SWp(µ(cid:63)  µθ)  
E[SWp(ˆµn  ˆµθ m(n))|Y1:n] is non-empty.

where ˆµn and ˆµθ m(n) are deﬁned by (2) and (4) respectively. Besides  for all ω ∈ E  there exists
n(ω) such that  for all n ≥ n(ω)  the set argminθ∈Θ
Similar to Theorem 2  this theorem shows that  when the number of observations goes to inﬁnity  the
estimator obtained with the expected distance will converge to a global minimizer.

SWp(µ(cid:63)  µθ)  and

(11)

(10)

θ∈Θ

3.3 Convergence of MESWE to MSWE

In practical applications  we can only use a ﬁnite number of generated samples Z1:m.
In this
subsection  we analyze the case where the observations Y1:n are kept ﬁxed while the number of
generated samples increases  i.e. m → +∞ and we show in this scenario that MESWE converges to
MSWE  assuming the latter exists.
Before deriving this result  we formulate a technical assumption below.
A5. For some  > 0 and n = inf θ∈Θ SWp(ˆµn  µθ)  the set Θ n = {θ ∈ Θ : SWp(ˆµn  µθ) ≤
n + } is bounded almost surely.
Theorem 4 (MESWE converges to MSWE as m → +∞). Assume A1  A4 and A5. Then 

lim

inf
m→+∞
θ∈Θ
argminθ∈Θ

lim sup
m→+∞

E [SWp(ˆµn  ˆµθ m)|Y1:n] = inf
θ∈Θ
E [SWp(ˆµn  ˆµθ m)|Y1:n] ⊂ argminθ∈ΘSWp(ˆµn  µθ)

SWp(ˆµn  µθ)

(12)

(13)
E [SWp(ˆµn  ˆµθ m)|Y1:n] is

Besides  there exists m∗ such that  for any m ≥ m∗  the set argminθ∈Θ
non-empty.

This result shows that MESWE would be indeed promising in practice  as one get can more accurate
estimations by increasing m.

5

3.4 Rate of convergence and the asymptotic distribution

In our last set of theoretical results  we investigate the asymptotic distribution of MSWE and we
establish a rate of convergence. We now suppose that we are in the well-speciﬁed setting  i.e. there
exists θ(cid:63) in the interior of Θ such that µθ(cid:63) = µ(cid:63)  and we consider the following two assumptions.
For any u ∈ Sd−1 and t ∈ R  we deﬁne Fθ(u  t) =(cid:82)Y
1(−∞ t]((cid:104)u  y(cid:105))dµθ(y). Note that for any
u ∈ Sd−1  Fθ(u ·) is the cumulative distribution function (CDF) associated to the measure u(cid:63)
(cid:93) µθ.
A6. For all  > 0  there exists δ > 0 such that inf θ∈Θ: ρΘ(θ θ(cid:63))≥ SW1(µθ(cid:63)   µθ) > δ .
Let L1(Sd−1×R) denote the class of functions that are absolutely integrable on the domain Sd−1×R 
with respect to the measure dσ ⊗ Leb  where Leb denotes the Lebesgue measure.
A7. Assume that there exists a measurable function D(cid:63) = (D(cid:63) 1  . . .   D(cid:63) dθ ) : Sd−1 × R (cid:55)→ Rdθ
such that for each i = 1  . . .   dθ  D(cid:63) i ∈ L1(Sd−1 × R) and

(cid:90)Sd−1(cid:90)R |Fθ(u  t) − Fθ(cid:63) (u  t) − (cid:104)θ − θ(cid:63)  D(cid:63)(u  t)(cid:105)| dtdσ(u) = (ρΘ(θ  θ(cid:63)))  

i=1 are linearly independent in

where  : R+ → R+ satisﬁes limt→0 (t) = 0. Besides  {D(cid:63) i}dθ
L1(Sd−1 × R).
For any u ∈ Sd−1  and t ∈ R  deﬁne: ˆFn(u  t) = n−1 card{i ∈ {1  . . .   n} : (cid:104)u  Yi(cid:105) ≤ t}  where
card denotes the cardinality of a set  and for any u ∈ Sd−1  ˆFn(u ·) is the CDF associated to the
measure u(cid:63)
A8. There exists a random element G(cid:63) : Sd−1×R (cid:55)→ R such that the stochastic process √n( ˆFn−Fθ(cid:63) )
converges weakly in L1(Sd−1 × R) to G(cid:63)
Theorem 5. Assume A1  A2  A3  A6  A7 and A8. Then  the asymptotic distribution of the goodness-
of-ﬁt statistic is given by 

(cid:93) ˆµn.

1.

√n inf
θ∈Θ

SW1(ˆµn  µθ)

w

−→ inf

θ∈Θ(cid:90)Sd−1(cid:90)R |G(cid:63)(u  t) − (cid:104)θ  D(cid:63)(u  t)(cid:105)| dtdσ(u) 

as n → +∞  

where ˆµn is deﬁned by (2).
Theorem 6. Assume A1  A2  A3  A6  A7 and A8. Suppose also that the random map θ (cid:55)→
(cid:82)Sd−1(cid:82)R |G(cid:63)(u  t) − (cid:104)θ  D(cid:63)(u  t)(cid:105)| dtdσ(u) has a unique inﬁmum almost surely. Then  MSWE with
p = 1 satisﬁes 
√n(ˆθn − θ(cid:63))
as n → +∞  

−→ argminθ∈Θ(cid:90)Sd−1(cid:90)R |G(cid:63)(u  t) − (cid:104)θ  D(cid:63)(u  t)(cid:105)| dtdσ(u) 

w

where ˆθn is deﬁned by (1) with SW1 in place of D.

These results show that the estimator and the associated goodness-of-ﬁt statistics will converge to a
random variable in distribution  where the rate of convergence is √n. Note that G(cid:63) is deﬁned as a
random element (see A8)  therefore we can not claim that the convergence in distribution derived in
Theorem 5 and 6 implies the convergence in probability.
This CLT is also inspired by [3]  where they identiﬁed the asymptotic distribution associated to the
minimum Wasserstein estimator. However  since Wp admits an analytical form only when d = 1 
their result is restricted to the scalar case  and in their conclusion  [3] conjecture that the rate of the
minimum Wasserstein estimators would depend negatively on the dimension of the observation space.
On the contrary  since SWp is deﬁned in terms of one-dimensional Wp distances  we circumvent
the curse of dimensionality and our result holds for any ﬁnite dimension. While the perceived
computational burden has created a pessimism in the machine learning community about the use of
Wasserstein-based methods in large dimensional settings  which motivated the rise of regularized
optimal transport [26]  we believe that our ﬁndings provide an interesting counter-example to this
conception.

1Under mild assumptions on the tails of u(cid:63)

(cid:93) µ(cid:63) for any u ∈ Sd−1  we believe that one can prove that A8 holds

in general by extending [24  Proposition 3.5] and [25  Theorem 2.1a].

6

(a) MSWE vs. n

(b) MESWE vs. n = m

(c) MESWE with n = 2000 vs. m
Figure 2: Min. SW estimation on Gaussians in R10. Figure 2a and Figure 2b show the mean
squared error between (m(cid:63)  σ2
n n)) for n
from 10 to 10 000  illustrating Theorems 2 and 3. Figure 2c shows the error between ( ˆmn  ˆσ2
n) and
n m) for 2000 observations and m from 10 to 10 000  to illustrate Theorem 4. Results are
( ˆmn m  ˆσ2
averaged over 100 runs  the shaded areas represent the standard deviation.

(cid:63)) = (0  1) and MSWE ( ˆmn  ˆσ2

n) (resp. MESWE ( ˆmn n  ˆσ2

4 Experiments

We conduct experiments on synthetic and real data to empirically conﬁrm our theorems. We explain
in Section 4 of the supplementary document the optimization methods used to ﬁnd the estimators.
Speciﬁcally  we can use stochastic iterative optimization algorithm (e.g.  stochastic gradient descent).
Note that  since we calculate (expected) SW with Monte Carlo approximations over a ﬁnite set of
projections (and a ﬁnite number of ‘generated datasets’)  MSWE and MESWE fall into the category
of doubly stochastic algorithms. Our experiments on synthetic data actually show that using only one
random projection and one randomly generated dataset at each iteration of the optimization process
is enough to illustrate our theorems. We provide the code to reproduce the experiments.2
Multivariate Gaussian distributions: We con-
sider the task of estimating the parameters of a
10-dimensional Gaussian distribution using our
SW estimators: we are interested in the model

M =(cid:8)N (m  σ2I) : m ∈ R10  σ2 > 0(cid:9) and

we draw i.i.d. observations with (m(cid:63)  σ2
(cid:63)) =
(0  1). The advantage of this simple setting
is that the density of the generated data has a
closed-form expression  which makes MSWE
tractable. We empirically verify our central limit
theorem: for different values of n  we compute
500 times MSWE of order 1 using one random
projection  then we estimate the density of ˆσ2
n
with a kernel density estimator. Figure 1 shows
the distributions centered and rescaled by √n
for each n  and conﬁrms the convergence rate
that we derived (Theorem 6). To illustrate the consistency property in Theorem 2  we approximate
MSWE of order 2 for different numbers of observed data n using one random projection and we report
for each n the mean squared error between the estimate mean and variance and the data-generating
parameters (m(cid:63)  σ2
(cid:63)). We proceed the same way to study the consistency of MESWE (Theorem 3) 
which we approximate using one random projections and one generated dataset z1:m of size m = n
for different values of n. We also verify the convergence of MESWE to MSWE (Theorem 4): we
compute these estimators on a ﬁxed set of n = 2000 observations for different m  and we measure

Figure 1: Probability density estimates of the
n of order 1  centered and rescaled by
MSWE ˆσ2
√n  on the 10-dimensional Gaussian model for
different values of n.

2See https://github.com/kimiandj/min_swe.

7

101102103104numberofobservationsn0.000.050.100.15kbmn−m?k22101102103104numberofobservationsn0.0000.0250.0500.0750.100kbσ2n−σ2?k22101102103104numberofobservationsn0.000.050.100.15kbmn n−m?k22101102103104numberofobservationsn0.00.10.2kbσ2n n−σ2?k22101102103104numberofgeneratedsamplesm0.00000.00020.00040.0006kbmn∗ m−bmn∗k22101102103104numberofgeneratedsamplesm0.0000.0020.0040.0060.008kbσ2n∗ m−bσ2n∗k22−2−1012√n(bσ2n−σ2?)0.00.20.40.60.8densityn=250n=500n=750n=1000n=2500n=5000n=10000(a) Comparison of MESWE and MEWE

(b) MESWE

∗
(c) MESWE  n

= 100

Figure 3: Min. SW estimation for the location parameter of multivariate elliptically contoured stable
distributions. Figure 3a compares the quality of the estimation provided by SW and Wasserstein-based
estimators as well as their average computational time  for different values of dimension d. Figure 3b
and Figure 3c illustrate  for d = 10  the consistency of MESWE ˆmn m and its convergence to the
MSWE ˆmn. Results are averaged over 100 runs  the shaded area represent the standard deviation.

the error between them for each m. Results are shown in Figure 2. We see that our estimators
indeed converge to (m(cid:63)  σ2
(cid:63)) as the number of observations increases (Figures 2a  2b)  and on a ﬁxed
observed dataset  MESWE converges to MSWE as we generate more samples (Figure 2c).
Multivariate elliptically contoured stable distributions: We focus on parameter inference for a
subclass of multivariate stable distributions  called elliptically contoured stable distributions and
denoted by EαSc [27]. Stable distributions refer to a family of heavy-tailed probability distributions
that generalize Gaussian laws and appear as the limit distributions in the generalized central limit
theorem [28]. These distributions have many attractive theoretical properties and have been proven
useful in modeling ﬁnancial [29] data or audio signals [30  31]. While special univariate cases include
Gaussian  Lévy and Cauchy distributions  the density of stable distributions has no general analytic
form  which restricts their practical application  especially for the multivariate case.
If Y ∈ Rd ∼ EαSc(Σ  m)  then its joint characteristic function is deﬁned for any t ∈ Rd as
E[exp(itT Y )] = exp(cid:0)−(tT Σt)α/2 + itT m(cid:1)  where Σ is a positive deﬁnite matrix (akin to a
correlation matrix)  m ∈ Rd is a location vector (equal to the mean if it exists) and α ∈ (0  2)
controls the thickness of the tail. Even though their densities cannot be evaluated easily  it is
straightforward to sample from EαSc [27]  therefore it is particularly relevant here to apply MESWE
instead of MLE.
To demonstrate the computational advantage of MESWE over the minimum expected Wasserstein
estimator [3  MEWE]  we consider observations in Rd i.i.d. from EαSc(I  m(cid:63)) where each compo-
nent of m(cid:63) is 2 and α = 1.8  and M =(cid:8)EαSc(I  m) : m ∈ Rd(cid:9). The Wasserstein distance on
multivariate data is either computed exactly by solving the linear program in (5)  or approximated
by solving a regularized version of this problem with Sinkhorn’s algorithm [12]. The MESWE
is approximated using 10 random projections and 10 sets of generated samples. Then  following
the approach in [3]  we use the gradient-free optimization method Nelder-Mead to minimize the
Wasserstein and SW distances. We report on Figure 3a the mean squared error between each estimate
and m(cid:63)  as well as their average computational time for different values of dimension d. We see
that MESWE provides the same quality of estimation as its Wasserstein-based counterparts while
considerably reducing the computational time  especially in higher dimensions. We focus on this
model in R10 and we illustrate the consistency of the MESWE ˆmn m  approximated with one random
projection and one generated dataset  the same way as for the Gaussian model: see Figure 3b. To
conﬁrm the convergence of ˆmn m to the MSWE ˆmn  we ﬁx n = 100 observations and we compute
the mean squared error between the two approximate estimators (using one random projection and
one generated dataset) for different values of m (Figure 3c). Note that the MSWE is approximated
with the MESWE obtained for a large enough value of m: ˆmn ≈ ˆmn 10 000.
High-dimensional real data using GANs: Finally  we run experiments on image generation using
the Sliced-Wasserstein Generator (SWG)  an alternative GAN formulation based on the minimization
of the SW distance [16]. Speciﬁcally  the generative modeling approach consists in introducing a
random variable Z which takes value in Z with a ﬁxed distribution  and then transforming Z through
a neural network. This deﬁnes a parametric function Tθ : Z → Y that is able to produce images from
a distribution µθ.

8

2510dimension0.0000.0050.0100.0150.020kˆm−m?k22MESWEapprox.MEWEexactMEWE2510dimension05001000averagetime(s)101102103104numberofobservationsn0.00.10.20.30.40.5kbmn n−m?k22101102103numberofgeneratedsamplesm0.00000.00050.00100.0015kbmn∗ m−bmn∗k22The goal is to optimize the neural net-
work parameters such that the gen-
erated images are close to the ob-
served ones.
[16] proposes to min-
imize the SW distance between µθ
and the real data distribution over θ
as the generator objective  and train
on MESWE in practice. For our ex-
periments  we design a neural network
with the fully-connected conﬁguration
given in [16  Appendix D] and we use
the MNIST dataset  made of 60 000
training images and 10 000 test im-
ages of size 28 × 28. Our training ob-
jective is MESWE of order 2 approxi-
mated with 20 random projections and
20 different generated datasets. We
study the consistent behavior of the MESWE by training the neural network on different sizes n
of training data and different numbers m of generated samples and by comparing the ﬁnal training
loss and test loss to the ones obtained when learning on the whole training dataset (n = 60 000)
and m = 200. Results are averaged over 10 runs and shown on Figure 4  where the shaded areas
correspond to the standard deviation over the runs. We observe that our results conﬁrm Theorem 3.
We would like to point out that  in all of our experiments  the random projections used in the Monte
Carlo average that estimates the integral in (7) were picked uniformly on Sd−1 (see Section 4 in the
supplementary document for more details). The sampling on Sd−1 directly impacts the quality of
the resulting approximation of SW  and might induce variance in practice when learning generative
models. On the theoretical side  studying the asymptotic properties of SW-based estimators obtained
with a ﬁnite number of projections is an interesting question (e.g.  their behavior might depend on the
sampling method or the number of projections used). We leave this study for future research.

for (n  m) ∈ (cid:8)(1  1)  (100  20)  (1000  40)  (10 000  60)(cid:9)

Figure 4: Mean-squared error between the training (test) loss

and the training (test) loss for (n  m) = (60 000  200) on
MNIST using the SW generator. We trained for 20 000 itera-
tions with the ADAM optimizer [32].

5 Conclusion

The Sliced-Wasserstein distance has been an attractive metric choice for learning in generative
models  where the densities cannot be computed directly. In this study  we investigated the asymptotic
properties of estimators that are obtained by minimizing SW and the expected SW. We showed that (i)
convergence in SW implies weak convergence of probability measures in general Wasserstein spaces 
(ii) the estimators are consistent  (iii) the estimators converge to a random variable in distribution
with a rate of √n. We validated our mathematical results on both synthetic data and neural networks.
We believe that our techniques can be further extended to the extensions of SW such as [20  33  34].

Acknowledgements

The authors are grateful to Pierre Jacob for his valuable comments on an earlier version of this
manuscript. This work is partly supported by the French National Research Agency (ANR) as a part
of the FBIMATRIX project (ANR-16-CE23-0014) and by the industrial chair Machine Learning
for Big Data from Télécom ParisTech. Alain Durmus acknowledges support from Polish National
Science Center grant: NCN UMO-2018/31/B/ST1/00253.

References
[1] J. Wolfowitz. The minimum distance method. Ann. Math. Statist.  28(1):75–88  03 1957.

[2] A. Basu  H. Shioya  and C. Park. Statistical Inference: The Minimum Distance Approach.

Chapman & Hall/CRC Monographs on Statistics & Applied Probability. CRC Press  2011.

[3] E. Bernton  P. E. Jacob  M. Gerber  and C. P. Robert. On parameter estimation with the

Wasserstein distance. Information and Inference: A Journal of the IMA  Jan 2019.

9

1100100010000trainingdatasize0.00.10.20.30.40.5meansquarederrorofthelosstrainlosstestloss[4] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial

networks. In International Conference on Machine Learning  pages 214–223  2017.

[5] Ilya Tolstikhin  Olivier Bousquet  Sylvain Gelly  and Bernhard Schoelkopf. Wasserstein auto-

encoders. arXiv preprint arXiv:1711.01558  2017.

[6] Aude Genevay  Gabriel Peyré  and Marco Cuturi. Learning generative models with Sinkhorn

divergences. arXiv preprint arXiv:1706.00292  2017.

[7] Giorgio Patrini  Marcello Carioni  Patrick Forre  Samarth Bhargav  Max Welling  Rianne
van den Berg  Tim Genewein  and Frank Nielsen. Sinkhorn autoencoders. arXiv preprint
arXiv:1810.01118  2018.

[8] Jonas Adler and Sebastian Lunz. Banach Wasserstein GAN. In Advances in Neural Information

Processing Systems  pages 6754–6763  2018.

[9] Aude Genevay  Gabriel Peyré  and Marco Cuturi. GAN and VAE from an optimal transport

point of view. arXiv preprint arXiv:1706.01807  2017.

[10] Olivier Bousquet  Sylvain Gelly  Ilya Tolstikhin  Carl-Johann Simon-Gabriel  and Bernhard
Schoelkopf. From optimal transport to generative modeling: the vegan cookbook. arXiv preprint
arXiv:1705.07642  2017.

[11] Shuang Liu  Olivier Bousquet  and Kamalika Chaudhuri. Approximation and convergence
properties of generative adversarial learning. In Advances in Neural Information Processing
Systems  pages 5545–5553  2017.

[12] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances

in Neural Information Processing Systems  pages 2292–2300  2013.

[13] J. Rabin  G. Peyré  J. Delon  and M. Bernot. Wasserstein barycenter and its application to
texture mixing. In Alfred M. Bruckstein  Bart M. ter Haar Romeny  Alexander M. Bronstein 
and Michael M. Bronstein  editors  Scale Space and Variational Methods in Computer Vision 
pages 435–446  2012.

[14] Nicolas Bonnotte. Unidimensional and Evolution Methods for Optimal Transportation. PhD

thesis  Paris 11  2013.

[15] N. Bonneel  J. Rabin  G. Peyré  and H. Pﬁster. Sliced and Radon Wasserstein barycenters of

measures. Journal of Mathematical Imaging and Vision  51(1):22–45  2015.

[16] Ishan Deshpande  Ziyu Zhang  and Alexander G Schwing. Generative modeling using the sliced
Wasserstein distance. In IEEE Conference on Computer Vision and Pattern Recognition  pages
3483–3491  2018.

[17] Soheil Kolouri  Phillip E. Pope  Charles E. Martin  and Gustavo K. Rohde. Sliced Wasserstein

auto-encoders. In International Conference on Learning Representations  2019.

[18] Antoine Liutkus  Umut ¸Sim¸sekli  Szymon Majewski  Alain Durmus  and Fabian-Robert Stoter.
Sliced-Wasserstein ﬂows: Nonparametric generative modeling via optimal transport and diffu-
sions. In International Conference on Machine Learning  2019.

[19] Jiqing Wu  Zhiwu Huang  Wen Li  Janine Thoma  and Luc Van Gool. Sliced wasserstein

generative models. In IEEE Conference on Computer Vision and Pattern Recognition  2019.

[20] Ishan Deshpande  Yuan-Ting Hu  Ruoyu Sun  Ayis Pyrros  Nasir Siddiqui  Sanmi Koyejo 
Zhizhen Zhao  David Forsyth  and Alexander Schwing. Max-Sliced Wasserstein distance and
its use for GANs. In IEEE Conference on Computer Vision and Pattern Recognition  2019.

[21] Patrick Billingsley. Convergence of probability measures. Wiley Series in Probability and
Statistics: Probability and Statistics. John Wiley & Sons Inc.  New York  second edition  1999.
A Wiley-Interscience Publication.

[22] Cédric Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wis-

senschaften. Springer  2009 edition  September 2008.

10

[23] S. T. Rachev and L. Rüschendorf. Mass transportation problems. Vol. I. Probability and its

Applications (New York). Springer-Verlag  New York  1998. Theory.

[24] Sophie Dede. An empirical central limit theorem in l1 for stationary sequences. Stochastic

Processes and their Applications  119(10):3494–3515  2009.

[25] Eustasio del Barrio  Evarist Giné  and Carlos Matrán. Central limit theorems for the wasserstein
distance between the empirical and the true distributions. Ann. Probab.  27(2):1009–1071  04
1999.

[26] G. Peyré  M. Cuturi  et al. Computational optimal transport. Foundations and Trends R(cid:13) in

Machine Learning  11(5-6):355–607  2019.

[27] John P. Nolan. Multivariate elliptically contoured stable distributions: theory and estimation.

Computational Statistics  28(5):2067–2089  Oct 2013.

[28] G. Samorodnitsky and M.S. Taqqu. Stable Non-Gaussian Random Processes: Stochastic Models

with Inﬁnite Variance. Stochastic Modeling Series. Taylor & Francis  1994.

[29] B. B. Mandelbrot. Fractals and Scaling in Finance: Discontinuity  Concentration  Risk. Selecta

Volume E. Springer Science & Business Media  2013.

[30] U. ¸Sim¸sekli  A. Liutkus  and A. T. Cemgil. Alpha-stable matrix factorization. IEEE Signal

Processing Letters  22(12):2289–2293  2015.

[31] Simon Leglaive  Umut ¸Sim¸sekli  Antoine Liutkus  Roland Badeau  and Gaël Richard. Alpha-
In 2017 IEEE International Conference on

stable multichannel audio source separation.
Acoustics  Speech and Signal Processing (ICASSP)  pages 576–580. IEEE  2017.

[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd
International Conference on Learning Representations  ICLR 2015  San Diego  CA  USA  May
7-9  2015  Conference Track Proceedings  2015.

[33] François-Pierre Paty and Marco Cuturi. Subspace robust wasserstein distances. In International

Conference on Machine Learning  2019.

[34] S. Kolouri  K. Nadjahi  U. Simsekli  R. Badeau  and G. K. Rohde. Generalized Sliced Wasser-

stein Distances. In Advances in Neural Information Processing Systems  2019.

11

,Liam MacDermed
Charles Isbell
Andrea Montanari
Daniel Reichman
Ofer Zeitouni
Kimia Nadjahi
Alain Durmus
Umut Simsekli
Roland Badeau