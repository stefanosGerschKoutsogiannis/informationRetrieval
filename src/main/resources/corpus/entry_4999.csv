2019,Shallow RNN:  Accurate Time-series Classification on Resource Constrained Devices,Recurrent Neural Networks (RNNs) capture long dependencies and context  and
2 hence are the key component of typical sequential data based tasks. However  the
sequential nature of RNNs dictates a large inference cost for long sequences even if
the hardware supports parallelization. To induce long-term dependencies  and yet
admit parallelization  we introduce novel shallow RNNs. In this architecture  the
first layer splits the input sequence and runs several independent RNNs. The second
layer consumes the output of the first layer using a second RNN thus capturing
long dependencies. We provide theoretical justification for our architecture under
weak assumptions that we verify on real-world benchmarks. Furthermore  we show
that for time-series classification  our technique leads to substantially improved
inference time over standard RNNs without compromising accuracy. For example 
we can deploy audio-keyword classification on tiny Cortex M4 devices (100MHz
processor  256KB RAM  no DSP available) which was not possible using standard
RNN models. Similarly  using SRNN in the popular Listen-Attend-Spell (LAS)
architecture for phoneme classification [4]  we can reduce the lag inphoneme
classification by 10-12x while maintaining state-of-the-art accuracy.,Shallow RNNs: A Method for Accurate Time-series

Classiﬁcation on Tiny Devices

Don Kurian Dennis˚

Durmus Alp Emre Acar

Vikram Mandikal:

Carnegie Mellon University

Boston University

University of Texas at Austin

Vinu Sankar Sadasivan:

Harsha Vardhan Simhadri

Venkatesh Saligrama

IIT Gandhinagar

Microsoft Research India

Boston University

Prateek Jain

Microsoft Research India

Abstract

Recurrent Neural Networks (RNNs) capture long dependencies and context  and
hence are the key component of typical sequential data based tasks. However  the
sequential nature of RNNs dictates a large inference cost for long sequences even if
the hardware supports parallelization. To induce long-term dependencies  and yet
admit parallelization  we introduce novel shallow RNNs. In this architecture  the
ﬁrst layer splits the input sequence and runs several independent RNNs. The second
layer consumes the output of the ﬁrst layer using a second RNN thus capturing
long dependencies. We provide theoretical justiﬁcation for our architecture under
weak assumptions that we verify on real-world benchmarks. Furthermore  we show
that for time-series classiﬁcation  our technique leads to substantially improved
inference time over standard RNNs without compromising accuracy. For example 
we can deploy audio-keyword classiﬁcation on tiny Cortex M4 devices (100MHz
processor  256KB RAM  no DSP available) which was not possible using standard
RNN models. Similarly  using ShaRNN in the popular Listen-Attend-Spell (LAS)
architecture for phoneme classiﬁcation [4]  we can reduce the lag in phoneme
classiﬁcation by 10-12x while maintaining state-of-the-art accuracy.

Introduction

1
We focus on the challenging task of time-series classiﬁcation on tiny devices  a problem arising in
several industrial and consumer applications [25  22  30]  where tiny edge-devices perform sensing 
monitoring and prediction in a limited time and resource budget. A prototypical example is an
interactive cane for people with visual impairment  capable of recognizing gestures that are observed
as time-traces on a sensor embedded onto the cane [24].

Time series or sequential data naturally exhibit temporal dependencies. Sequential models such as
RNNs are particularly well-suited in this context because they can account for temporal dependencies
by attempting to derive relations from the previous inputs. Nevertheless  directly leveraging RNNs
for prediction in constrained scenarios mentioned above is challenging. As observed by several
authors [28  14  29  9]  the sequential nature by which RNNs process data fundamentally limits
parallelization leading to large training and inference costs. In particular  in time-series classiﬁcation 
at inference time  the processing time scales with the size  T   of the receptive window  which is
unacceptable in resource constrained settings.

˚Work done as a Research Fellow at Microsoft Research India.
:Work done during internships at Microsoft Research India.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

A solution proposed in literature [28  14  29  9] is to replace sequential processing with parallelizable
feed-forward and convolutional networks. A key insight exploited here is that most applications
require relatively small receptive window  and that this size can be increased with tree-structured
networks and dilated convolutions. Nevertheless  feedforward/convolutional networks utilize sub-
stantial working memory  which makes them difﬁcult to deploy on tiny devices. For this reason 
other methods such as [32  2] also are not applicable for our setting. For example  a standard audio
keyword detection task with a relatively modest setup of 32 conv ﬁlters would itself need a working
memory of 500KB and about 32X more computation than a baseline RNN model (see Section 5).

Shallow RNNs. To address these challenges  we design a novel layered RNN architecture that is
parallelizable/limited-recurrence while still maintaining the receptive ﬁeld length (T ) and the size of
the baseline RNN. Concretely  we propose a simple 2-layer architecture that we refer to as ShaRNN.
Both the layers of ShaRNN are composed of a collection of shallow recurrent neural networks that
operate independently. More precisely  each sequential data point (receptive window) is divided into
independent parts called bricks of size k  and a shared RNN operates on each brick independently 
thus ensuring a small model size and short recurrence. That is  ShaRNN’s bottom layer restarts from
an initial state after every k ăă T steps  and hence only has a short recurrence. The outputs of T{k
parallel RNNs are input as a sequence into a second layer RNN  which then outputs a prediction
after T{k time. In this way  for k « Op?Tq we obtain a speedup of Op?Tq in inference time in the
(a) Parallelization: here we parallelize inference over T{k independent RNNs thus admitting

following two settings:

speed-ups on multi-threaded architectures 

(b) Streaming: here we utilize receptive (sliding) windows and reuse computation from older

sliding window/receptive ﬁelds.

We also note that  in contrast to the proposed feed-forward methods or truncated RNN methods [23] 
our proposal admits fully receptive ﬁelds and thus does not result in loss of information. We further
enhance ShaRNN by combining it with the recent MI-RNN method [10] to reduce the receptive
window sizes; we call the resulting method MI-ShaRNN.

While a feedforward layer could be used in lieu of our RNN in the next layer  such layers lead to
signiﬁcant increase in model size and working RAM to be admissible in tiny devices.

Performance and Deployability. We compare the two-layer MI-ShaRNN approach against other
state-of-art methods  on a variety of benchmark datasets  tabulating both accuracy and budgets. We
show that the proposed 2-layer MI-ShaRNN exhibits signiﬁcant improvement in inference time
while also improving accuracy. For example  on Google-13 dataset  MI-ShaRNN achieves 1%
higher accuracy than baseline methods while providing 5-10x improvement in inference cost. A
compelling aspect of the architecture is that it allows for reuse of most of the computation  which
leads to its deployability on the tiniest of devices. In particular  we show empirically that the method
can be deployed for real-time time-series classiﬁcation on devices as those based on the tiny ARM
Cortex M4 microprocessor3 with just 256KB RAM  100MHz clock-speed and no dedicated Digital
Signal Processing (DSP) hardware. Finally  we demonstrate that we can replace bi-LSTM based
encoder-decoder of the LAS architecture [4] by ShaRNN while maintaining close to best accuracy
on publicly-available TIMIT dataset [13]. This enables us to deploy LAS architecture in streaming
fashion with a lag of 1 second in phoneme prediction and Op1q amortized cost per time-step; standard
LAS model would incur lag of about 8 seconds as it processes the entire 8 seconds of audio before
producing predictions.

Theory. We provide theoretical justiﬁcation for the ShaRNN architecture and show that signiﬁcant
parallelization can be achieved if the network satisﬁes some relatively weak assumptions. We also
point out that additional layers can be introduced in the architecture leading to hierarchical processing.
While we do not experiment with this concept here  we note that  it offers potential for exponential
improvement in inference time.

3https://en.wikipedia.org/wiki/ARM_Cortex-M#Cortex-M4

2

In summary  the following are our main contributions:

• We show that under relatively weak assumptions  recurrence in RNNs and consequently  the

inference cost can be reduced signiﬁcantly.

• We demonstrate this inference efﬁciency via a two-layer ShaRNN (and MI-ShaRNN) architecture

that uses only shallow RNNs with a small amount of recurrence.

• We benchmark MI-ShaRNN (enhancement of ShaRNN with MI-RNN) on several datasets and
observe that it learns nearly as accurate models as standard RNNs and MI-RNN. Due to limited
recurrence  ShaRNN saves 5-10x computation cost over baseline methods. We deploy MI-ShaRNN
model on a tiny microcontroller for real-time audio keyword detection  which  prior to this work 
was not possible with standard RNNs due to large inference cost with receptive (sliding) windows.
We also deploy ShaRNN in LAS architecture to enable streaming phoneme classiﬁcation with less
than 1 second of lag in prediction.

2 Related Work

Stacked Architecture. Our multi-layered RNN resembles stacked RNNs studied in the literature [15 
16  27] but they are unrelated. The goal of Stacked RNNs is to produce complex models and subsume
conventional RNNs. Each layer is fully recurrent  and feeds output of the ﬁrst layer to the next level.
The next level is another fully recurrent RNN. As such  stacked RNN architectures lead to increased
model size and recurrence  which results in worse inference time than standard RNNs.

Recurrent Nets (Training). Conventional works on RNNs primarily address challenges arising during
training. In particular for large receptive window T   RNNs suffer from vanishing and exploding
gradient issues. A number of works propose to circumvent this issue in a number of ways such
as Gated architectures [7  17] or adding residual connections in RNNs [18  1  21] or through
constraining the learnt parameters [31]. Several recent works attempt to reduce the number of gates
and parameters [8  6  21] to reduce model size but as such suffer from poor inference time  since they
are still fully recurrent. Different from these works  our focus is on reducing model size as well as
inference time and view these works as complementary to our paper.

Recurrent Nets (Inference Time). Recent works have begun to focus on RNN inference cost. [3]
proposes to learn skip connections that can avoid evaluating all the hidden states. [10] exploits
domain knowledge that true signature is signiﬁcantly shorter than the time-trace to trim down length
of the sliding windows. Both of these approaches are complementary and we indeed leverage the
second in our approach. A recent work on dilated RNNs [5] is interesting. While it could serve as
a potential solution  we note that  in its original form  dilated RNN also has a fully recurrent ﬁrst
layer  which is therefore infeasible. One remedy is to introduce dilation in the ﬁrst layer to improve
inference time. But  dilation skips steps and hence can miss out on critical local context.

Finally  CNN based methods [28  14  29  9  2] allow higher parallelization in the sequential tasks but
as discussed in Section 1  also lead to signiﬁcantly larger working RAM requirement when compared
to RNNs  thus cannot be considered for deployment on tiny devices (see Section 5).

3 Problem Formulation and Proposed ShaRNN Method

In this paper  we primarily focus on the time-series classiﬁcation problem  although the techniques
apply to more general sequence-to-sequence problems like phoneme classiﬁcation problem discussed
in Section 5. Let Z “ tpX1  y1q  . . .  pXn  ynqu where Xi is the i-th sequential data point with Xi “
rxi 1  xi 2  . . .   xi Ts P RdˆT and xi t P Rd is the t-th time-step data point. yi P rCs is the label of
Xi where C is the number of class labels. xi t:t`k is the shorthand for xi t:t`k “ rxi t  . . .   xi t`ks.
Given training data Z  the goal is to learn a classiﬁer f : RdˆT Ñ rCs that can be used for efﬁcient
inference  especially on tiny devices. Recurrent Neural Networks (RNN) are popularly used for
ˆd at the t-th step that is
modeling such sequential problems and maintain a hidden state ht´1 P R
updated using:

ht “ Rpht´1  xtq  t P rTs 

ˆy “ fphTq 

where ˆy is the prediction by applying a classiﬁer f on hT and ˆd is the dimensionality of the hidden
state. Due to the sequential nature of RNN  inference cost of RNN is ΩpTq even if the hardware

3

supports large amount of parallelization. Furthermore  practical applications require handling a
continuous stream of data  e.g.  smart-speaker listening for certain audio keywords.

A standard approach is to use sliding windows (receptive ﬁeld) to form a stream of test points on
which inference can be applied. That is  given a stream X “ rx1  x2  . . .  s  we form sliding windows
X s “ xps´1q¨ω`1:ps´1q¨ω`T P RdˆT which stride by ω ą 0 time-steps after each inference. RNN is
then applied to each sliding window X s which implies amortized cost for processing each time-step
data point (xt) is Θp T
ωq. To ensure high-resolution in prediction  ω is required to be a fairly small
constant independent of T . Thus  amortized inference cost for each time-step point is OpTq which is
prohibitively large for tiny devices. So  we study the following key question: “Can we process each
time-step point in a data stream in opTq computational steps?”

3.1 ShaRNN

Shallow RNNs (ShaRNN) are a hierarchical collection of RNNs organized at two levels. T
k RNNs at
ground-layer operate completely in parallel with fully shared parameters and activation functions 
thus ensuring small model size and parallel execution. An RNN at the next level take inputs from the
ground-layer and subsequently outputs a prediction.
Formally  given a sequential point X “ rx1  . . .   xTs (e.g. sliding window in streaming data)  we
split it into bricks of size k  where k is a parameter of the algorithm. That is  we form T{k bricks:
B “ rB1  . . .   BT {ks where Bj “ xppj´1q¨k`1q:pj¨kq. Now  ShaRNN applies a standard recurrent
ˆd1 on each brick  where ˆd1 is the dimensionality of hidden states of Rp1q.
model Rp1q : Rdˆk Ñ R

That is 

νp1q
j “ Rp1qpBjq  j P rT{ks.

Note that Rp1q can be any standard RNN model like GRU  LSTM etc. We now feed output of each
layer into another RNN to produce the ﬁnal state/feature vector that is then fed into a feed forward
layer. That is 

νp2q
T {k “ Rp2qprνp1q

1   . . .   νp1q

T {ksq 

ˆy “ fpνp2q
T {kq 

where Rp2q is the second layer RNN and can also be any standard RNN model. νp2q
hidden-state obtained by applying Rp2q to νp1q
1:T {k. f applies the standard feed-forward network to
νp2q
T {k. See Figure 1 for a block-diagram of the architecture. That is  ShaRNN is deﬁned by parameters
Λ composed of shared RNN parameters at the ground-level  RNN parameters at the next level  and
classiﬁer weights for making a prediction. We train the ShaRNN based on minimizing an empirical
loss function over training set Z.

T {k P R

ˆd2 is the

Naturally  ShaRNN is an approximation of a true RNN and in principle has less modeling power (and
recurrence). But as discussed in Section 4 and shown by our empirical results in Section 5  ShaRNN
can still capture enough context from the entire sequence to effectively model a variety of time-series
classiﬁcation problems with large T (typically T ě 100). Due to parallel k RNNs in the bottom layer
that are processed by R2 in the second layer  ShaRNN inference cost can be reduced to OpT{k ` kq
for multi-threaded architectures with k-wise parallelization; k “ ?T leads to smallest inference cost.
Streaming. Recall that in the streaming setting  we form sliding windows X s “ xs¨ω`1:s¨ω`T P
RdˆT by striding each window by ω ą 0 time-steps. Hence  if ω “ k ¨ q for q P N then the inference
cost of X s`1 can be reduced by reusing previously computed νp1q
vectors @j P rq ` 1  T{ks for X s.

j

Below claim provides a formal result for the same.

Claim 1. Let both layers RNNs Rp1q and Rp2q of ShaRNN have same hidden-size and per-time step
computation complexity C1. Then  given T and ω  the additional cost of applying ShaRNN to X s`1
given X s is OpT{k ` q ¨ kq ¨ C1  where X s “ xps´1q¨ω`1:ps´1q¨ω`T   ω is the stride-length of sliding
window  and the brick-size ω “ q ¨ k for some integer q ě 1. Consequently  the total amortized cost
can be bounded by Op?q ¨ T C1q if k “ aT{q.

See Appendix A for a proof of the claim.

4

(b)

(c)

(a)

(d)

3

2   νp1q

Figure 1: (a) ShaRNN applies RNN Rp1q independently to bricks x1:k  xk`1:2k  . . . to compute νp1q
for all k. The second layer RNN Rp2q produces class labels or in multi-layer case  inputs for the
next layer. Note that νp1q
can be reused for evaluating the next window. (b)  (c): Mean squared
approximation error and the prediction accuracy of ShaRNN with zeroth and ﬁrst order approximation
(M “ 1  2 respectively in Claim 3) with different brick-sizes k (for Google-13). Note the large
error with M “ 1 (same as truncation method in [23]). M “ 2 introduces signiﬁcant improvement 
especially for small k  but clearly needs larger M to achieve better accuracy. (d): Comparison of
norm of gradient vs Hessian of Rpht  xt`1:t`kq with varying k. R is FastRNN [21] with swish
activation. Smaller Hessian norm indicates that the ﬁrst-order approximation of R (Claim 3) by
ShaRNN is more accurate than the 0-th order one (ShaRNN with M “ 1) suggested by [23].

k

3.2 Multi-layer ShaRNN

Above claim shows that selecting small k leads to a large number of bricks and hence  a large number
of points to be processed by second layer RNN Rp2q which will be the bottleneck in inference.
However  using the same approach  we can replace the second layer with another layer of ShaRNN to
bring down the cost. By repeating the same process  we can design a general L layer architecture
where each layer is equipped with a RNN model Rplq and the output of a l-th layer brick is given by:

νplq
j “ Rplqprνpl´1q
for all 1 ď j ď T{kl  where νp0q
j “ xj . The predicted label is given by ˆy “ fpνpLq

pj´1q¨k`1  . . .   νpl´1q

pj´1qk`ksq 

T {kL´1q.

Using argument similar to the claim in the previous section  we can reduce the total inference cost to
Oplog Tq by using k “ Op1q and L “ log T .
Claim 2. Let all layers of multi-layer ShaRNN have same hidden-size and per-time step complexity
C1 and let k “ ω. Then  the additional cost of applying ShaRNN to X s`1 is OpT{kL ` L ¨ kq ¨ C1 
where X s “ xps´1q¨ω`1:ps´1q¨ω`T . Consequently  selecting L “ logpTq  k “ Op1q  and assuming
ω “ Op1q  the total amortized cost is OpC1 ¨ log pTqq.
That is  we can achieve exponential speed-up over OpTq cost for standard RNN. However  such a
model can lead to a large loss in accuracy. Moreover  constants in the cost for large L are so large
that a network with smaller L might be more efﬁcient for typical values of T .

3.3 MI-ShaRNN

Recently  [10] showed that several time-series training datasets are coarse and the sliding window size
T can be decreased signiﬁcantly by using their multi-instance based algorithm (MI-RNN). MI-RNN
ﬁnds tight windows around the actual signature of the class  which leads to signiﬁcantly smaller
models and reduces inference cost. Our ShaRNN architecture is orthogonal to MI-RNN and can
be combined to obtain even higher amount of inference saving. That is  MI-RNN takes the dataset

5

020406080100Brick-size(k)0510152025MeanErrorM=1M=2020406080100Brick-size(k)0.10.20.30.40.50.60.70.80.91.0AccuracyM=1M=2020406080100Brick-size(k)0246810NormMeanNormofGradientvsHessianGradientNormHessianNormZ “ tpX1  y1q  . . .  pXn  ynqu with Xi being a sequential data point over T steps and produces a
new set of points X 1
j is sequential data point over T 1 and T 1 ď T .
MI-ShaRNN applies ShaRNN to the output of MI-RNN so that the inference cost is dependent only

j   where each X 1

j with labels y1

on T 1 ď T   and captures the key signal in each data point.

4 Analysis

In this section  we provide theoretical underpinnings of ShaRNN approach and we also put it in
context of work by [23] that discusses RNN models for which we can get rid of almost all of the
recurrence.

ˆd be a standard RNN model that maps the given hidden state ht´1 P R
Let R : Rd` ˆd Ñ R
and data point xt P Rd into the next hidden state ht “ Rpht´1  xtq. Overloading notation 
Rph0  x1  . . .   xtq “ Rpht´1  xtq. We deﬁne a function to be recurrent if the following holds:

ˆd

Rph0  x1  . . .   xtq “ RpRph0  x1  . . .   xt´1q  xtq.

The ﬁnal class prediction using feed-forward layer is given by: ˆy “ fphTq “ fpRph0  x1:Tqq. Now 
ShaRNN attempts to untangle and approximate the dependency of fphTq and Rph0  x1:Tq on h0 
by using Taylor’s theorem. Below claim shows the condition under which the approximation error
introduced by ShaRNN is small.

h

Claim 3. Let Rph0  x1  . . .   xtq be an RNN and let }∇M
ǫ ě 0 where ∇M
}∇m
Rp1q  Rp2q and brick-size k  s.t.:

Rph  xt:t`kq} ď Opǫ ¨ M !q for some
h is M th order derivative with respect to h. Also let }Rph0  x1:tq ´ h0} “ Op1q 
Rph0  xt`1:t`kq} “ Opm!q for all t P rTs. Then  there exists an ShaRNN deﬁned by functions
}Rp2qpνp1q

T {kq ´ Rph0  x1:Tq} ď ǫ ¨ M ¨ T  where νp1q

j “ Rp1qph0  xpj´1q¨k`1:j¨kq.

1   . . .   νp1q

h

See Appendix A for a detailed proof of the claim.

The above claim shows that the hidden state computed by ShaRNN is close to the state computed by
a fully recursive RNN  hence the ﬁnal output ˆy would also be close. We now compare this result to
the result of [23]  which showed that }Rph0  x1:Tq´ Rph0  xT ´k`1:Tq} ď ǫ for large enough k if R
satisﬁes a contraction property. That is  if }Rpht´1  xtq ´ Rph1
t´1} where
λ ă 1. However  λ ă 1 is a strict requirement and do not hold in practice. Due to this  if we only
compute Rph0  xT ´k`1:Tq as suggested by the above result (for some reasonable values of k)  then
resulting accuracy on several datasets drops signiﬁcantly (see Figure 1(b) (c)).
In the context of Claim 3  result of [23] is a special case with M “ 1  i.e.  the result only applies a
0´th order Taylor series expansion. Figure 1 (d) shows how norm of the gradient that bounds error
due to the 0-th order expansion is signiﬁcantly larger than the norm of the Hessian which bounds
error due to the 1-st order expansion.

t´1  xtq} ď λ}ht´1 ´ h1

h

Case study with FastRNN: We now instantiate Claim 3 for a simple FastRNN model [21] with a
ﬁrst-order approximation i.e.  with M “ 2 in Claim 3.
Claim 4. Let Rph0  x1  . . .   xtq be a FastRNN model with parameters U  W . Let }U} ď Op1q 
}∇2
Rph0  xt:t`kq} ď Opǫq for any k-length sequence. Then  there exists an ShaRNN deﬁned
by functions Rp1q  Rp2q and brick-size k s.t.: }Rp2qpνp1q
T {kq ´ Rph0  x1:Tq} ď ǫ  where
νp1q
j “ Rp1qph0  xpj´1q¨k`1:j¨kq.
Note that }U} “ Op1q holds for all the benchmarks that were tried in [21]. Moreover  this assumption
is signiﬁcantly weaker than the typical }U} ă 1 assumption required by [23]. Finally  the Hessian
term is signiﬁcantly smaller than the derivative term (Figure 1 (d))  hence the approximation error and
prediction error should be signiﬁcantly smaller than the one we would get by 0-th order approximation
(see Figure 1 (b)  (c)).

1   . . .   νp1q

5 Empirical Results

We conduct experiments to study: a) performance of MI-ShaRNN with varying hidden state dimen-
sions at both the layers Rp1q and Rp2q to understand how its accuracy stacks up against baseline

6

Table 1: Table compares maximum accuracy achieved by each of the method for different model sizes 
i.e.  different hidden-state sizes indicated by numbers in bracket; MI-ShaRNN reports two numbers
for the ﬁrst and the second layer  respectively. Table also reports the corresponding computational
cost (amortized number of ﬂops required per data point inference) for each method. T denotes the no.
of time-steps for the dataset  T 1 denotes the trimmed number of time-steps obtained by MI-RNN  k is
the selected brick-length for MI-ShaRNN. Note that for all but one datasets MI-ShaRNN is able to
achieve similar or better accuracy compared to baseline LSTMs.

Dataset

Google-13
HAR-6
GesturePod-5
STCI-2
DSA-19

Baseline LSTM

Acc(%)

Flops

T

91.13 (64)
93.04 (32)
97.13 (48)
99.01 (32)
85.17 (64)

4.89M 99
1.36M 128
8.37M 400
2.67M 162
7.23M 129

MI-RNN

Acc(%)

Flops

T 1

93.16 (64)
91.78 (32)
98.43 (48)
98.43 (32)
88.11 (64)

2.42M 49
0.51M 48
4.19M 200
1.33M 81
5.05M 90

MI-ShaRNN

Acc(%)

Flops k

94.01 (64  32)
94.02 (32  8)
99.21 (48  32)
99.23 (32  32)
87.36 (64  48)

0.59M 8
0.17M 16
0.83M 20
0.30M 8
1.10M 15

(a)

(b)

(c)

(d)

Figure 2: (a) (b) (c): Accuracy vs inference cost: we vary model size (hidden dimensions) to obtain
accuracy vs inference cost curve for different methods. All the three plots show that MI-ShaRNN
produces more accurate models with as much as 8´10x reduction in the inference cost. (d): Error-rate
of standard LAS method [12] and of ShaRNN based streaming LAS with varying brick-sizes k on
the TIMIT [13] dataset. We report results when both Listener+Speller use ShaRNN vs when only
Listener uses it. ShaRNN Listener+Speller with k “ 64 incurs 12x smaller lag in phoneme prediction
vs baseline LAS (k “ 784).

models across different model sizes  b) inference cost improvement that MI-ShaRNN produces
for standard time-series classiﬁcation problems over baseline models and MI-RNN models  c) if
MI-ShaRNN can enable certain time-series classiﬁcation tasks on devices based on the tiny Cortex
M4 with only 100MHz processor and 256KB RAM. Recall that MI-ShaRNN uses ShaRNN on top of
trimmed data points given by MI-RNN. MI-RNN is known to have better performance than baseline
LSTMs  so naturally MI-ShaRNN has better performance than ShaRNN. Hence  we present results
for MI-ShaRNN and compare them to MI-RNN to demonstrate advantage of ShaRNN technique.

Datasets: We benchmark our method on standard datasets from different domains like audio keyword
detection (Google-13)  wake word detection (STCI-2)  activity recognition (HAR-6)  sports activity
recognition (DSA-19)  gesture recognition (GesturePod-5). The number after hyphen in dataset name
indicates the number of classes in the dataset. See Table 3 in appendix for more details about the
datasets. All the datasets are available online (see Table 3) except STCI-2 which is a proprietary wake
word detection dataset.

Baselines: We compare our algorithm MI-ShaRNN (LSTM) against the baseline LSTM method as
well as MI-RNN (LSTM) method. Note that MI-RNN as well as MI-ShaRNN build upon an RNN
cell. For simplicity and consistency  we have selected LSTM as the base cell for all the methods  but
we can train each of them with other RNN cells like GRU [7] or FastRNN [21]. We implemented
all the algorithms on TensorFlow and used Adam for training the models [19]. The inference code
for Cortex M4 device was written in C and compiled onto the device. All the presented numbers are
averaged over 5 independent runs. The implementation of our algorithm is released as part of the
EdgeML [11] library.

Hyperparameter selection: The main hyperparameters are: a) hidden state sizes for both the layers
of MI-ShaRNN. b) brick-size k for MI-ShaRNN. In addition  the number of time-steps T is associated

with each dataset. MI-RNN prunes down T and works with T 1 ď T time-steps. We provide results

7

105105106106Num.Flops88909294AccuracyGoogle-13DatasetBaselineLSTMMI-ShaRNNMI-ShaRNN104105105106Num.Flops9091929394AccuracyHAR-6Dataset104105105106106107Num.Flops949596979899AccuracyGesturePod-5Dataset0200400600800Brick-size0.200.220.240.260.28PhonemeErrorRateOnlineShaRNNLASStandardLASShaRNNListenerShaRNNListener+SpellerTable 2: Deployment on Cortex M4: accuracy of different methods vs inference time cost (ms) on M4
device with 256KB RAM and 100MHz processor. For low-latency keyword spotting (Google-13) 
the total inference time budget is 120 ms.

Baseline
16
32

MI-RNN
16
32

Acc.
Cost

86.99
456

89.84
999

89.78
226

92.61
494

MI-ShaRNN

(16  16)

91.42
70.5

(32 16)
92.67
117

with varying hidden state sizes to illustrate trade-offs involved with selecting this hyperparameter

(Figure 2). We select k « ?T with some variation to optimize w.r.t the stride length ω for each

dataset; we also provide an ablation study to illustrate impact of different choices of k on accuracy
and the inference cost (Figure 3  Appendix).

Comparison of accuracies: Table 1 compares accuracy of MI-ShaRNN against baselines and
MI-RNN for different hidden dimensions at R1 and R2. In terms of prediction accuracies  MI-
ShaRNN performs much better than baselines and is competitive to MI-RNN on all the datasets. For
example  with only k “ 8  MI-ShaRNN is able to achieve 94% accuracy on the Google-13 dataset
while MI-RNN model is applied for T “ 49 steps and baseline LSTM for T “ 99 steps. That is  with
only 8-deep recurrence  MI-ShaRNN is able to compete with accuracies of 49 and 99 deep LSTMs.

For inference cost  we study the amortized cost per data point in the sliding window setting (See
Section 3). That is  baseline and MI-RNN for each sliding window recomputes the entire prediction
from scratch. But  MI-ShaRNN can re-use computation in the ﬁrst layer (see Section 3) leading
to signiﬁcant saving in inference cost. We report inference cost as the additional ﬂoating point
operations (ﬂops) each model would need to execute for every new inference. For simplicity  we treat
both addition and multiplication to be of same cost. The number of non-linearity computations are
small and are nearly same for all the methods so we ignore them.

Table 1 clearly shows that to achieve best accuracy  MI-ShaRNN is up to 10x faster than baselines
and up to 5x faster than MI-RNN  even on a single threaded hardware architecture. Figure 2 shows
computation vs accuracy trade-off for three datasets. We observe that for a range of desired accuracy
values  MI-ShaRNN is 5-10x faster than the baselines.

Next  we compute accuracy and ﬂops for MI-ShaRNN with different brick sizes k (see Figure 3

of Appendix). As expected  k „ ?T setting requires fewest ﬂops for inference  but the story for

accuracy is more complicated. For this dataset  we do not observe any particular trend for accuracy;
all the accuracy values are similar  irrespective of k.

Deployment of Google-13 on Cortex M4: we use ShaRNN to deploy a real-time keyword spotting
model (Google-13) on a Cortex M4 device. For time series classiﬁcation (Section 3)  we will need
to slide windows and infer classes on each window. Due to small working RAM of M4 devices
(256KB)  for real-time recognition  the method needs to ﬁnish the following tasks within a budget
of 120ms: collect data from the microphone buffer  process them  produce ML based inference and
smoothened out predictions for one ﬁnal output.
Standard LSTM models for this task work on 1s windows  whose featurization generates a 32 ˆ 99
feature vector; here T “ 99. So  even a relatively small LSTM (hidden size 16)  takes on 456ms
to process one window  exceeding the time budget (Table 2). MI-RNN is faster but still requires
225ms. Recently  a few CNN based methods have also been designed for low-resource keyword
spotting [26  20]. However  with just 40 ﬁlters applied to the standard 32ˆ 99 ﬁlter-bank features  the
working memory requirement balloons up to « 500KB which is beyond typical M4 devices’ memory
budget. Similarly  compute requirement of such architectures also easily exceed the latency budget of
120ms. See Figure 4  in the Appendix for a comparison between CNN models and ShaRNN.

In contrast  our method is able to produce inference in only 70ms  thus is well-within latency budget
of M4. Also  MI-ShaRNN holds two arrays in the working RAM: a) input features for 1 brick and
b) buffered ﬁnal states from previous bricks. For the deployed MI-ShaRNN model  with timesteps
T “ 49  brick-size k “ 8 working RAM requirement is just 1.5 KB.
ShaRNN for Streaming Listen Attend Spell (LAS): LAS is a popular architecture for phoneme
classiﬁcation in given audio stream. It forms non-overlapping time-windows of length 784 (« 8
seconds) and apply an encoder-decoder architecture to predict a sequence of phonemes. We study

8

LAS applied to TIMIT dataset [13]. We enhance the standard LAS architecture to exploit time-
annotated ground truth available in TIMIT dataset  which improved baseline phoneme error rate from
publicly reported 0.271 to 0.22. Both Encoder and Decoder layer in standard and enhanced LAS
consists of fully recurrent bi-LSTMs. So for each time window (of length 784) we would need to
apply entire encoder-decoder architecture to predict the phoneme sequence  implying a potential lag
of « 8 seconds (784 steps) in prediction.
Instead  using ShaRNN we can divide both the encoder and decoder layer in bricks of size k. This
makes it possible to give phoneme classiﬁcation for every k steps of points thereby bringing down lag
from 784 steps to k steps. However  due to small brick size k  in principle we might lose signiﬁcant
amount of context information. But due to the corrective second layer in ShaRNN (Figure 1) we
observe little loss in accuracy. Figure 2 shows performance of two variants of ShaRNN + LAS: a)
ShaRNN Listener that uses ShaRNN only in encoding layer  b) ShaRNN Listener + Speller that
uses ShaRNN in both the encoding and decoding layer. Figure 2 (d) shows that using ShaRNN in
both the encoder and decoder is more beneﬁcial than using it only in encoder layer. Furthermore 
decreasing k from 784 to 64 leads to marginal increase in error from 0.22 to 0.238 while reducing the
lag signiﬁcantly; from 8 seconds to 0.6 seconds. In fact  even at k “ 64 this model’s performance is
signiﬁcantly better than the reported error of standard LAS (0.27) [12]. See Appendix C for details.

9

References

[1] Yoshua Bengio  Nicolas Boulanger-Lewandowski  and Razvan Pascanu. Advances in optimizing
recurrent networks. 2013 IEEE International Conference on Acoustics  Speech and Signal
Processing  2013.

[2] James Bradbury  Stephen Merity  Caiming Xiong  and Richard Socher. Quasi-recurrent neural

networks. arXiv preprint arXiv:1611.01576  2016.

[3] Víctor Campos  Brendan Jou  Xavier Giró i Nieto  Jordi Torres  and Shih-Fu Chang. Skip RNN:
Learning to skip state updates in recurrent neural networks. In International Conference on
Learning Representations  2018.

[4] William Chan  Navdeep Jaitly  Quoc Le  and Oriol Vinyals. Listen  attend and spell: A neural
network for large vocabulary conversational speech recognition. In 2016 IEEE International
Conference on Acoustics  Speech and Signal Processing (ICASSP)  2016.

[5] Shiyu Chang  Yang Zhang  Wei Han  Mo Yu  Xiaoxiao Guo  Wei Tan  Xiaodong Cui  Michael
Witbrock  Mark A Hasegawa-Johnson  and Thomas S Huang. Dilated recurrent neural networks.
In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett 
editors  Advances in Neural Information Processing Systems 30  2017.

[6] Kyunghyun Cho  Bart Van Merriënboer  Dzmitry Bahdanau  and Yoshua Bengio. On
the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259  2014.

[7] Kyunghyun Cho  Bart van Merrienboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares 
Holger Schwenk  and Yoshua Bengio. Learning phrase representations using rnn encoder–
decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP)  2014.

[8] Jasmine Collins  Jascha Sohl-Dickstein  and David Sussillo. Capacity and trainability in

recurrent neural networks. arXiv preprint arXiv:1611.09913  2016.

[9] Yann Dauphin  Angela Fan  Michael Auli  and David Grangier. Language modeling with gated

convolutional networks. In ICML  2017.

[10] Don Kurian Dennis  Chirag Pabbaraju  Harsha Vardhan Simhadri  and Prateek Jain. Multiple
instance learning for efﬁcient sequential data classiﬁcation on resource-constrained devices. In
NeurIPS  2018.

[11] Don Kurian Dennis  Sridhar Gopinath  Chirag Gupta  Ashish Kumar  Aditya Kusupati  Shishir
G Patil  Harsha Vardhan Simhadri. EdgeML: Machine Learning for resource-constrained edge
devices.

[12] Janna Escur. Exploring automatic speech recognition with tensorﬂow. Master’s thesis  2018.

[13] John S Garofolo  Lori F Lamel  William M Fisher  Jonathan G Fiscus  David S Pallett  Nancy L

Dahlgren  and Victor Zue. Darpa timit acoustic phonetic continuous speech corpus  1993.

[14] Jonas Gehring  Michael Auli  David Grangier  Denis Yarats  and Yann Dauphin. Convolutional

sequence to sequence learning. In ICML  2017.

[15] Alex Graves  Abdel rahman Mohamed  and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. 2013 IEEE International Conference on Acoustics  Speech and
Signal Processing  2013.

[16] Michiel Hermans and Benjamin Schrauwen. Training and analysing deep recurrent neural
networks. In C. J. C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Q. Weinberger 
editors  Advances in Neural Information Processing Systems 26  2013.

[17] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8) 

1997.

10

[18] Herbert Jaeger  Mantas Lukosevicius  Dan Popovici  and Udo Siewert. Optimization and
applications of echo state networks with leaky-integrator neurons. Neural networks : the ofﬁcial
journal of the International Neural Network Society  20  2007.

[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[20] Rajath Kumar  Vaishnavi Yeruva  and Sriram Ganapathy. On convolutional lstm modeling for
joint wake-word detection and text dependent speaker veriﬁcation. Proc. Interspeech 2018 
2018.

[21] Aditya Kusupati  Manish Singh  Kush Bhatia  Ashish Kumar  Prateek Jain  and Manik Varma.
Fastgrnn: A fast  accurate  stable and tiny kilobyte sized gated recurrent neural network. In
NeurIPS  2018.

[22] Benoît Latré  Bart Braem  Ingrid Moerman  Chris Blondia  and Piet Demeester. A survey on

wireless body area networks. Wireless Networks.  2011.

[23] John Miller and Moritz Hardt. When recurrent models don’t need to be recurrent. arXiv preprint

arXiv:1805.10369  4  2018.

[24] Shishir G. Patil  Don Kurian Dennis  Chirag Pabbaraju  Nadeem Shaheer  Harsha Vardhan
Simhadri  Vivek Seshadri  Manik Varma  and Prateek Jain. Gesturepod: Enabling on-device
gesture-based interaction for white cane users. Technical report  New York  NY  USA  2019.

[25] Charith Perera  Chi Harold Liu  and Srimal Jayawardena. The emerging internet of things
marketplace from an industrial perspective: A survey. IEEE Transactions on Emerging Topics
in Computing  3(4)  2015.

[26] Tara N Sainath and Carolina Parada. Convolutional neural networks for small-footprint key-
word spotting. In Sixteenth Annual Conference of the International Speech Communication
Association  2015.

[27] Richard Socher  Alex Perelygin  Jean Wu  Jason Chuang  Christopher D Manning  Andrew Ng 
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language
processing  2013.

[28] Aäron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex
Graves  Nal Kalchbrenner  Andrew W. Senior  and Koray Kavukcuoglu. Wavenet: A generative
model for raw audio. In SSW  2016.

[29] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N. Gomez 

Lukasz Kaiser  and Illia Polosukhin. Attention is all you need. In NIPS  2017.

[30] Allen Y Yang  Sameer Iyengar  Shankar Sastry  Ruzena Bajcsy  Philip Kuryloski  and Roozbeh
Jafari. Distributed segmentation and classiﬁcation of human actions using a wearable motion
sensor network. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition Workshops. IEEE  2008.

[31] Jiong Zhang  Qi Lei  and Inderjit S Dhillon. Stabilizing gradients for deep neural networks via

efﬁcient svd parameterization. arXiv preprint arXiv:1803.09327  2018.

[32] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms  2015.

11

,Don Dennis
Durmus Alp Emre Acar
Vikram Mandikal
Vinu Sankar Sadasivan
Venkatesh Saligrama
Harsha Vardhan Simhadri
Prateek Jain