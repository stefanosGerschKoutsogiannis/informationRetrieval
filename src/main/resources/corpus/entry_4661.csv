2013,Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel,Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently  Chaudhuri et al. and Amini et al. proposed variations on the algorithm that artificially inflate the node degrees for improved statistical performance.  The current paper extends the previous theoretical results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and  provides guidance on the choice of tuning parameter.  Moreover  our results show how the star shape" in the eigenvectors--which are consistently observed in empirical networks--can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model  two statistical model that allow for highly heterogeneous degrees.  Throughout  the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.  ",Regularized Spectral Clustering under the
Degree-Corrected Stochastic Blockmodel

Tai Qin

Department of Statistics

University of Wisconsin-Madison

Madison  WI

qin@stat.wisc.edu

Karl Rohe

Department of Statistics

University of Wisconsin-Madison

Madison  WI

karlrohe@stat.wisc.edu

Abstract

Spectral clustering is a fast and popular algorithm for ﬁnding clusters in net-
works. Recently  Chaudhuri et al. [1] and Amini et al. [2] proposed inspired
variations on the algorithm that artiﬁcially inﬂate the node degrees for improved
statistical performance. The current paper extends the previous statistical esti-
mation results to the more canonical spectral clustering algorithm in a way that
removes any assumption on the minimum degree and provides guidance on the
choice of the tuning parameter. Moreover  our results show how the “star shape”
in the eigenvectors–a common feature of empirical networks–can be explained
by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Par-
tition model  two statistical models that allow for highly heterogeneous degrees.
Throughout  the paper characterizes and justiﬁes several of the variations of the
spectral clustering algorithm in terms of these models.

1

Introduction

Our lives are embedded in networks–social  biological  communication  etc.– and many researchers
wish to analyze these networks to gain a deeper understanding of the underlying mechanisms. Some
types of underlying mechanisms generate communities (aka clusters or modularities) in the network.
As machine learners  our aim is not merely to devise algorithms for community detection  but also
to study the algorithm’s estimation properties  to understand if and when we can make justiﬁable in-
ferences from the estimated communities to the underlying mechanisms. Spectral clustering is a fast
and popular technique for ﬁnding communities in networks. Several previous authors have studied
the estimation properties of spectral clustering under various statistical network models (McSherry
[3]  Dasgupta et al. [4]  Coja-Oghlan and Lanka [5]  Ames and Vavasis [6]  Rohe et al. [7]  Sussman
et al. [8] and Chaudhuri et al. [1]). Recently  Chaudhuri et al. [1] and Amini et al. [2] proposed two
inspired ways of artiﬁcially inﬂating the node degrees in ways that provide statistical regularization
to spectral clustering.
This paper examines the statistical estimation performance of regularized spectral clustering under
the Degree-Corrected Stochastic Blockmodel (DC-SBM)  an extension of the Stochastic Block-
model (SBM) that allows for heterogeneous degrees (Holland and Leinhardt [9]  Karrer and New-
man [10]). The SBM and the DC-SBM are closely related to the planted partition model and the
extended planted partition model  respectively. We extend the previous results in the following ways:
(a) In contrast to previous studies  this paper studies the regularization step with a canonical version
of spectral clustering that uses k-means. The results do not require any assumptions on the min-
imum expected node degree; instead  there is a threshold demonstrating that higher degree nodes
are easier to cluster. This threshold is a function of the leverage scores that have proven essential
in other contexts  for both graph algorithms and network data analysis (see Mahoney [11] and ref-
erences therein). These are the ﬁrst results that relate leverage scores to the statistical performance

1

of spectral clustering. (b) This paper provides more guidance for data analytic issues than previous
approaches. First  the results suggest an appropriate range for the regularization parameter. Sec-
ond  our analysis gives a (statistical) model-based explanation for the “star-shaped” ﬁgure that often
appears in empirical eigenvectors. This demonstrates how projecting the rows of the eigenvector
matrix onto the unit sphere (an algorithmic step proposed by Ng et al. [12]) removes the ancillary
effects of heterogeneous degrees under the DC-SBM. Our results highlight when this step may be
unwise.
Preliminaries: Throughout  we study undirected and unweighted graphs or networks. Deﬁne a
graph as G(E  V )  where V = {v1  v2  . . .   vN} is the vertex or node set and E is the edge set. We
will refer to node vi as node i. E contains a pair (i  j) if there is an edge between node i and j. The
edge set can be represented by the adjacency matrix A ∈ {0  1}n×n. Aij = Aji = 1 if (i  j) is in
the edge set and Aij = Aji = 0 otherwise. Deﬁne the diagonal matrix D and the normalized Graph
Laplacian L  both elements of RN×N   in the following way:

(cid:88)

Dii =

Aij 

L = D−1/2AD−1/2.

j

The following notations will be used throughout the paper: ||·|| denotes the spectral norm  and ||·||F
denotes the Frobenius norm. For two sequence of variables {xN} and {yN}  we say xN = ω(yN )
if and only if yN /xN = o(1). δ(. .) is the indicator function where δx y = 1 if x = y and δx y = 0
if x (cid:54)= y.

2 The Algorithm: Regularized Spectral Clustering (RSC)

For a sparse network with strong degree heterogeneity  standard spectral clustering often fails to
function properly (Amini et al. [2]  Jin [13]). To account for this  Chaudhuri et al. [1] proposed the
regularized graph Laplacian that can be deﬁned as

Lτ = D−1/2

τ

AD−1/2

τ

∈ RN×N

where Dτ = D + τ I for τ ≥ 0.
The spectral algorithm proposed and studied by Chaudhuri et al. [1] divides the nodes into two
random subsets and only uses the induced subgraph on one of those random subsets to compute
the spectral decomposition. In this paper  we will study the more traditional version of spectral
algorithm that uses the spectral decomposition on the entire matrix (Ng et al. [12]). Deﬁne the
regularized spectral clustering (RSC) algorithm as follows:

1. Given input adjacency matrix A  number of clusters K  and regularizer τ  calculate the
regularized graph Laplacian Lτ . (As discussed later  a good default for τ is the average
node degree.)
2. Find the eigenvectors X1  ...  XK ∈ RN corresponding to the K largest eigenvalues of Lτ .
Form X = [X1  ...  XK] ∈ RN×K by putting the eigenvectors into the columns.
3. Form the matrix X∗ ∈ RN×K from X by normalizing each of X’s rows to have
unit length. That is  project each row of X onto the unit sphere of RK (X∗
ij =

Xij/((cid:80)

j X 2

ij)1/2).

4. Treat each row of X∗ as a point in RK  and run k-means with K clusters. This creates K

non-overlapping sets V1  ...  VK whose union is V.

5. Output V1  ...  VK. Node i is assigned to cluster r if the i’th row of X∗ is assigned to Vr.
This paper will refer to “standard spectral clustering” as the above algorithm with L replacing Lτ .
These spectral algorithms have two main steps: 1) ﬁnd the principal eigenspace of the (regularized)
graph Laplacian; 2) determine the clusters in the low dimensional eigenspace. Later  we will study
RSC under the Degree-Corrected Stochastic Blockmodel and show rigorously how regularization
helps to maintain cluster information in step (a) and why normalizing the rows of X helps in step
(b). From now on  we use Xτ and X∗
τ instead of X and X∗ to emphasize that they are related to
Lτ . Let X i
The next section introduces the Degree-Corrected Stochastic Blockmodel and its matrix formulation.

τ ]i denote the i’th row of Xτ and X∗
τ .

τ and [X∗

2

3 The Degree-Corrected Stochastic Blockmodel (DC-SBM)

In the Stochastic Blockmodel (SBM)  each node belongs to one of K blocks. Each edge corresponds
to an independent Bernoulli random variable where the probability of an edge between any two
nodes depends only on the block memberships of the two nodes (Holland and Leinhardt [9]). The
formal deﬁnition is as follows.
Deﬁnition 3.1. For a node set {1  2  ...  N}  let z : {1  2  ...  N} → {1  2  ...  K} partition the N
nodes into K blocks. So  zi equals the block membership for node i. Let B be a K × K matrix
where Bab ∈ [0  1] for all a  b. Then under the SBM  the probability of an edge between i and j is
Pij = Pji = Bzizj for any i  j = 1  2  ...  n. Given z  all edges are independent.

each block  the summation of θi’s is 1. That is  (cid:80)

One limitation of the SBM is that it presumes all nodes within the same block have the same expected
degree. The Degree-Corrected Stochastic Blockmodel (DC-SBM) (Karrer and Newman [10]) is
a generalization of the SBM that adds an additional set of parameters (θi > 0 for each node i)
that control the node degrees. Let B be a K × K matrix where Bab ≥ 0 for all a  b. Then the
probability of an edge between node i and node j is θiθjBzizj   where θiθjBzizj ∈ [0  1] for any
i  j = 1  2  ...  n. Parameters θi are arbitrary to within a multiplicative constant that is absorbed into
B. To make it identiﬁable  Karrer and Newman [10] suggest imposing the constraint that  within
i θiδzi r = 1 for any block label r. Under
this constraint  B has explicit meaning: If s (cid:54)= t  Bst represents the expected number of links
between block s and block t and if s = t  Bst is twice the expected number of links within block s.
Throughout the paper  we assume that B is positive deﬁnite.
Under the DC-SBM  deﬁne A (cid:44) EA. This matrix can be expressed as a product of the matrices 

where (1) Θ ∈ RN×N is a diagonal matrix whose ii’th element is θi and (2) Z ∈ {0  1}N×K is the
membership matrix with Zit = 1 if and only if node i belongs to block t (i.e. zi = t).

A = ΘZBZ T Θ 

3.1 Population Analysis

Under the DC-SBM  if the partition is identiﬁable  then one should be able to determine the partition
from A . This section shows that with the population adjacency matrix A and a proper regularizer
τ  RSC perfectly reconstructs the block partition.

Deﬁne the diagonal matrix D to contain the expected node degrees  Dii = (cid:80)

Aij and deﬁne
Dτ = D + τ I where τ ≥ 0 is the regularizer. Then  deﬁne the population graph Laplacian L and
the population version of regularized graph Laplacian Lτ   both elements of RN×N   in the following
way:

j

Lτ = D−1/2

L = D−1/2A D−1/2 

Deﬁne DB ∈ RK×K as a diagonal matrix whose (s  s)’th element is [DB]ss =(cid:80)

t Bst. A couple
lines of algebra shows that [DB]ss = Ws is the total expected degrees of nodes from block s and
that Dii = θi[DB]zizi. Using these quantities  the next Lemma gives an explicit form for Lτ as a
product of the parameter matrices.
Lemma 3.2. (Explicit form for Lτ ) Under the DC-SBM with K blocks with parameters {B  Z  Θ} 
deﬁne θτ

A D−1/2

.

τ

τ

i as:

θτ
i =

θ2
i

θi + τ /Wzi

= θi

Dii

.

Dii + τ

Let Θτ ∈ Rn×n be a diagonal matrix whose ii’th entry is θτ
Lτ can be written

i . Deﬁne BL = D

−1/2
B BD

−1/2
B

  then

Lτ = D− 1

τ A D− 1

2

2

τ = Θ

τ ZBLZ T Θ

1
2

1
2

τ .

Recall that A = ΘZBZ T Θ. Lemma 3.2 demonstrates that Lτ has a similarly simple form that
separates the block-related information (BL) and node speciﬁc information (Θτ ). Notice that if
τ = 0  then Θ0 = Θ and L = D− 1
2 . The next lemma shows that Lτ
has rank K and describes how its eigen-decomposition can be expressed in terms of Z and Θ.

2 ZBLZ T Θ 1

2 A D− 1

2 = Θ 1

3

Lemma 3.3. (Eigen-decomposition for Lτ ) Under the DC-SBM with K blocks and parameters
{B  Z  Θ}  Lλ has K positive eigenvalues. The remaining N − K eigenvalues are zero. Denote
the K positive eigenvalues of Lτ as λ1 ≥ λ2 ≥ ... ≥ λK > 0 and let Xτ ∈ RN×K contain the
eigenvector corresponding to λi in its i’th column. Deﬁne X ∗
τ to be the row-normalized version of
Xτ   similar to X∗
τ as deﬁned in the RSC algorithm in Section 2. Then  there exists an orthogonal
matrix U ∈ RK×K depending on τ  such that

1
2

τ Z(Z T Θτ Z)−1/2U

1. Xτ = Θ
2. X ∗

τ = ZU  Zi (cid:54)= Zj ⇔ ZiU (cid:54)= ZjU  where Zi denote the i’th row of the membership
matrix Z.

τ and X j

i(cid:80)

τ . First  if two nodes i and j
τ ) both point
τ ||2 = (
)1/2. Second  if two nodes
τ are orthogonal to each other. Third  if zi = zj
τ ]j =
τ ]j. Figure 1 illustrates the
τ when there are three underlying blocks. Notice that running k-means on

This lemma provides four useful facts about the matrices Xτ and X ∗
belong to the same block  then the corresponding rows of Xτ (denoted as X i
in the same direction  but with different lengths: ||X i
i and j belong to different blocks  then X i
τ and X j
then after projecting these points onto the sphere as in X ∗
Uzi. Finally  if zi (cid:54)= zj  then the rows are perpendicular  [X ∗
geometry of Xτ and X ∗
the rows of X ∗
Note that if Θ were the identity matrix  then the left panel in Figure 1 would look like the right panel
in Figure 1; without degree heterogeneity  there would be no star shape and no need for a projection
step. This suggests that the star shaped ﬁgure often observed in data analysis stems from the degree
heterogeneity in the network.

τ ]i ⊥ [X ∗
λ (in right panel of Figure 1) will return perfect clusters.

θτ
j θτ
j δzj  zi

τ   the rows are equal: [X ∗

τ ]i = [X ∗

Figure 1: In this numerical example  A comes from the DC-SBM with three blocks. Each point
corresponds to one row of the matrix Xτ (in left panel) or X ∗
τ (in right panel). The different colors
correspond to three different blocks. The hollow circle is the origin. Without normalization (left
panel)  the nodes with same block membership share the same direction in the projected space.
After normalization (right panel)  nodes with same block membership share the same position in
the projected space.

4 Regularized Spectral Clustering with the Degree Corrected model

This section bounds the mis-clustering rate of Regularized Spectral Clustering under the DC-SBM.
The section proceeds as follows: Theorem 4.1 shows that Lτ is close to Lτ . Theorem 4.2 shows
that Xτ is close to Xτ and that X∗
τ . Finally  Theorem 4.4 shows that the output from
RSC with Lτ is close to the true partition in the DC-SBM (using Lemma 3.3).
Theorem 4.1. (Concentration of the regularized Graph Laplacian) Let G be a random graph  with
independent edges and pr(vi ∼ vj) = pij. Let δ be the minimum expected degree of G  that is
δ = mini Dii. For any  > 0  if δ + τ > 3 ln N + 3 ln(4/)  then with probability at least 1 −  

τ is close to X ∗

(cid:114) 3 ln(4N/)

δ + τ

||Lτ − Lτ|| ≤ 4

4

.

(1)

−0.2−0.15−0.1−0.050−0.2−0.100.10.2−0.15−0.1−0.0500.050.10.150.2−1−0.50−1−0.500.5−0.8−0.6−0.4−0.200.20.40.60.8τ

τ A− D−1

Remark: This theorem builds on the results of Chung and Radcliffe [14] and Chaudhuri et al. [1]
which give a seemingly similar bound on ||L− L || and ||D−1
A ||. However  the previous
papers require that δ ≥ c ln N  where c is some constant. This assumption is not satisﬁed in a large
proportion of sparse empirical networks with heterogeneous degrees. In fact  the regularized graph
Laplacian is most interesting when this condition fails  i.e. when there are several nodes with very
low degrees. Theorem 4.1 only assumes that δ + τ > 3 ln N + 3 ln(4/). This is the fundamental
reason that RSC works for networks containing some nodes with extremely small degrees. It shows
that  by introducing a proper regularizer τ  ||Lτ − Lτ|| can be well bounded  even with δ very small.
Later we will show that a suitable choice of τ is the average degree.
The next theorem bounds the difference between the empirical and population eigenvectors (and
their row normalized versions) in terms of the Frobenius norm.
Theorem 4.2. Let A be the adjacency matrix generated from the DC-SBM with K blocks and pa-
rameters {B  Z  Θ}. Let λ1 ≥ λ2 ≥ ... ≥ λK > 0 be the only K positive eigenvalues of Lτ .
Let Xτ and Xτ ∈ RN×K contain the top K eigenvectors of Lτ and Lτ respectively. Deﬁne
m = mini{min{||X i
τ and
τ ∈ RN×K be the row normalized versions of Xτ and Xτ   as deﬁned in step 3 of the RSC
X ∗
algorithm.
For any  > 0 and sufﬁciently large N  assume that

τ ||2}} as the length of the shortest row in Xτ and Xτ . Let X∗

τ||2 ||X i

(cid:114) K ln(4N/)

(a)

√
≤ 1
8

λK 

(b) δ + τ > 3 ln N + 3 ln(4/) 

δ + τ

then with probability at least 1 −   the following holds 
||X∗
||Xτ − Xτ O||F ≤ c0

(cid:114) K ln(4N/)

  and

3

1
λK

δ + τ

τ − X ∗

τ

O||F ≤ c0

1

mλK

(cid:114) K ln(4N/)

. (2)

δ + τ

The proof of Theorem 4.2 can be found in the supplementary materials.
Next we use Theorem 4.2 to derive a bound on the mis-clustering rate of RSC. To deﬁne “mis-
clustered”  recall that RSC applies the k-means algorithm to the rows of X∗
τ   where each row is a
point in RK. Each row is assigned to one cluster  and each of these clusters has a centroid from
k-means. Deﬁne C1  . . .   Cn ∈ RK such that Ci is the centroid corresponding to the i’th row of
τ . Similarly  run k-means on the rows of the population eigenvector matrix X ∗
X∗
τ and deﬁne the
population centroids C1  . . .  Cn ∈ RK. In essence  we consider node i correctly clustered if Ci is
closer to Ci than it is to any other Cj for all j with Zj (cid:54)= Zi.
The deﬁnition is complicated by the fact that  if any of the λ1  . . .   λK are equal  then only the
subspace spanned by their eigenvectors is identiﬁable. Similarly  if any of those eigenvalues are
close together  then the estimation results for the individual eigenvectors are much worse that for the
estimation results for the subspace that they span. Because clustering only requires estimation of the
correct subspace  our deﬁnition of correctly clustered is amended with the rotation O T ∈ RK×K 
the matrix which minimizes (cid:107)X∗
τ (cid:107)F . This is referred to as the orthogonal Procrustes
problem and [15] shows how the singular value decomposition gives the solution.
Deﬁnition 4.3. If CiO T is closer to Ci than it is to any other Cj for j with Zj (cid:54)= Zi  then we say
that node i is correctly clustered. Deﬁne the set of mis-clustered nodes:

O T − X ∗

τ

M = {i : ∃j (cid:54)= i  s.t.||CiO T − Ci||2 > ||CiO T − Cj||2}.

(3)

The next theorem bounds the mis-clustering rate |M|/N.
Theorem 4.4. (Main Theorem) Suppose A ∈ RN×N is an adjacency matrix of a graph G gener-
ated from the DC-SBM with K blocks and parameters {B  Z  Θ}. Let λ1 ≥ λ2 ≥ ... ≥ λK > 0
be the K positive eigenvalues of Lτ . Deﬁne M   the set of mis-clustered nodes  as in Deﬁnition 4.3.
Let δ be the minimum expected degree of G. For any  > 0 and sufﬁciently large N  assume (a)
and (b) as in Theorem 4.2. Then with probability at least 1 −   the mis-clustering rate of RSC with
regularization constant τ is bounded 

|M|/N ≤ c1

K ln(N/)

N m2(δ + τ )λ2
K

.

(4)

5

EM
N   β

C = (Z T Θτ Z)1/2BL(Z T Θτ Z)1/2 ∈ RK×K

In simulations  we ﬁnd that τ = M/N (i.e.

s. If EM = ω(N ln N ) where M = (cid:80)

(5)
has the same eigenvalues as the largest K eigenvalues of Lτ (see supplementary materials for de-
i within block
tails). The matrix Z T Θτ Z is diagonal and the (s  s)’th element is the summation of θτ
i Dii is the sum of the node degrees  then τ = ω(M/N )
sends the smallest diagonal entry of Z T Θτ Z to 0  sending λK  the smallest eigenvalue of C  to zero.
EM
The trade-off between these two suggests that a proper range of τ is (α
N )  where 0 < α < β
are two constants. Keeping τ within this range guarantees that λK is lower bounded by some
constant depending only on K.
the average node
degree) provides good results. The theoretical results only suggest that this is the correct rate. So 
one could adjust this by a multiplicative constant. Our simulations suggest that the results are not
sensitive to such adjustments.
Remark 2 (Thresholding m): Mahoney [11] (and references therein) shows how the leverage
scores of A and L are informative for both data analysis and algorithmic stability. For L  the leverage
score of node i is ||X i||2
2  the length of the ith row of the matrix containing the top K eigenvectors.
Theorem 4.4 is the ﬁrst result that explicitly relates the leverage scores to the statistical performance
of spectral clustering. Recall that m2 is the minimum of the squared row lengths in Xτ and Xτ  
that is the minimum leverage score in both Lτ and Lτ . This appears in the denominator of (4). The
leverage scores in Lτ have an explicit form ||X i
. So  if node i has small expected
τ ||2 small. This can deteriorate the bound in Theorem 4.4.
i is small  rendering ||X i
degree  then θτ
τ onto the unit sphere for a node i with small leverage; it
The problem arises from projecting X i
ampliﬁes a noisy measurement. Motivated by this intuition  the next corollary focuses on the high
leverage nodes. More speciﬁcally  let m∗ denote the threshold. Deﬁne S to be a subset of nodes
whose leverage scores in Lτ and Xτ   ||X i
S = {i : ||X i

τ || and ||X i
τ|| ≥ m∗}.
τ || ≥ m∗ ||X i
τ ]i  i ∈ S}  we cluster these nodes. The
Then by applying k-means on the set of vectors {[X∗
following corollary bounds the mis-clustering rate on S.
Corollary 4.5. Let N1 = |S| denote the number of nodes in S and deﬁne M1 = M ∩ S as the set of
√
mis-clustered nodes restricted in S. With the same settings and assumptions as in Theorem 4.4  let
γ > 0 be a constant and set m∗ = γ/
N. If N/N1 = O(1)  then by applying k-means on the set of
τ ]i  i ∈ S}  we have with probability at least 1 −   there exist constant c2 independent
vectors {[X∗
of   such that

τ|| exceed the threshold m∗:

τ ||2

2 =

i(cid:80)

θτ
j θτ
j δzj  zi

Remark 1 (Choice of τ): The quality of the bound in Theorem 4.4 depends on τ through three
terms: (δ + τ )  λK  and m. Setting τ equal to the average node degree balances these terms. In
essence  if τ is too small  there is insufﬁcient regularization. Speciﬁcally  if the minimum expected
degree δ = O(ln N )  then we need τ ≥ c() ln N to have enough regularization to satisfy condition
(b) on δ + τ. Alternatively  if τ is too large  it washes out signiﬁcant eigenvalues.
To see that τ should not be too large  note that

|M1|/N1 ≤ c2

K ln(N1/)
γ2(δ + τ )λ2
K

.

(6)

In the main theorem (Theorem 4.4)  the denominator of the upper bound contains m2. Since we do
not make a minimum degree assumption  this value potentially approaches zero  making the bound
useless. Corollary 4.5 replaces N m2 with the constant γ2  providing a superior bound when there
are several small leverage scores.
If λK (the Kth largest eigenvalue of Lτ ) is bounded below by some constant and τ = ω(ln N ) 
then Corollary 4.5 implies that |M1|/N1 = op(1). The above thresholding procedure only clusters
the nodes in S. To cluster all of the nodes  deﬁne the thresholded RSC (t-RSC) as follows:

(a) Follow step (1)  (2)  and (3) of RSC as in section 2.
(b) Apply k-means with K clusters on the set S = {i ||X i
(c) For each node i /∈ S  ﬁnd the centroid Cs such that ||[X∗

√
τ||2 ≥ γ/
them to one of V1  ...  VK. Let C1  ...  CK denote the K centroids given by k-means.
τ ]i − Cs||2 = min1≤t≤K||[X∗
Ct||2. Assign node i to Vs. Output V1  ...VK.

N} and assign each of
τ ]i −

6

Remark 3 (Applying to SC): Theorem 4.4 can be easily applied to the standard SC algorithm under
both the SBM and the DC-SBM by setting τ = 0. In this setting  Theorem 4.4 improves upon the
previous results for spectral clustering.
Deﬁne the four parameter Stochastic Blockmodel SBM (p  r  s  K) as follows: p is the probability
of an edge occurring between two nodes from the same block  r is the probability of an out-block
linkage  s is the number of nodes within each block  and K is the number of blocks.
Because the SBM lacks degree heterogeneity within blocks  the rows of X within the same block
already share the same length. So  it is not necessary to project X i’s to the unit sphere. Under the
four parameter model  λK = (K[r/(p − r)] + 1)−1 (Rohe et al. [7]). Using Theorem 4.4  with p
and r ﬁxed and p > r  and applying k-means to the rows of X  we have

|M|/N = Op

(cid:113) N
(7)
ln N )  then |M|/N → 0 in probability. This improves the previous results that required
If K = o(
K = o(N 1/3) (Rohe et al. [7]). Moreover  it makes the results for spectral clustering comparable to
the results for the MLE in Choi et al. [16].

N

(cid:18) K 2 ln N

(cid:19)

.

5 Simulation and Analysis of Political Blogs

This section compares ﬁve different methods of spectral clustering. Experiment 1 generates net-
works from the DC-SBM with a power-law degree distribution. Experiment 2 generates networks
from the standard SBM. Finally  the beneﬁts of regularization are illustrated on an empirical network
from the political blogosphere during the 2004 presidential election (Adamic and Glance [17]).
The simulations compare (1) standard spectral clustering (SC)  (2) RSC as deﬁned in section 2  (3)
RSC without projecting Xτ onto unit sphere (RSC wp)  (4) regularized SC with thresholding (t-
RSC)  and (5) spectral clustering with perturbation (SCP) (Amini et al. [2]) which applies SC to the
perturbed adjacency matrix Aper = A + a11T . In addition  experiment 2 compares the performance
of RSC on the subset of nodes with high leverage scores (RSC on S) with the other 5 methods. We
set τ = M/N  threshold parameter γ = 1  and a = M/N 2 except otherwise speciﬁed.
Experiment 1. This experiment examines how degree heterogeneity affects the performance of the
spectral clustering algorithms. The Θ parameters (from the DC-SBM) are drawn from the power law
distribution with lower bound xmin = 1 and shape parameter β ∈ {2  2.25  2.5  2.75  3  3.25  3.5}.
A smaller β indicates to greater degree heterogeneity. For each ﬁxed β  thirty networks are sampled.
In each sample  K = 3 and each block contains 300 nodes (N = 900). Deﬁne the signal to noise
ratio to be the expected number of in-block edges divided by the expected number of out-block
edges. Throughout the simulations  the SNR is set to three and the expected average degree is set to
eight.
The left panel of Figure 2 plots β against the misclustering rate for SC  RSC  RSC wp  t-RSC  SCP
and RSC on S. Each point is the average of 30 sampled networks. Each line represents one method.
If a method assigns more than 95% of the nodes into one block  then we consider all nodes to be
misclustered. The experiment shows that (1) if the degrees are more heterogeneous (β ≤ 3.5) 
then regularization improves the performance of the algorithms; (2) if β < 3  then RSC and t-
RSC outperform RSC wp and SCP  verifying that the normalization step helps when the degrees are
highly heterogeneous; and  ﬁnally  (3) uniformly across the setting of β  it is easier to cluster nodes
with high leverage scores.
Experiment 2. This experiment compares SC  RSC  RSC wp  t-RSC and SCP under the SBM with
no degree heterogeneity. Each simulation has K = 3 blocks and N = 1500 nodes. As in the
previous experiment  SNR is set to three. In this experiment  the average degree has three different
settings: 10  21  30. For each setting  the results are averaged over 50 samples of the network.
The right panel of Figure 2 shows the misclustering rate of SC and RSC for the three different
values of the average degree. SCP  RSC wp  t-RSC perform similarly to RSC  demonstrating that
under the standard SBM (i.e. without degree heterogeneity) all spectral clustering methods perform
comparably. The one exception is that under the sparsest model  SC is less stable than the other
methods.

7

Figure 2: Left Panel: Comparison of Performance for SC  RSC  RSC wp  t-RSC  SCP and (RSC
on S) under different degree heterogeneity. Smaller β corresponds to greater degree heterogeneity.
Right Panel: Comparison of Performance for SC and RSC under SBM with different sparsity.

Analysis of Blog Network. This empirical network is comprised of political blogs during the 2004
US presidential election (Adamic and Glance [17]). Each blog has a known label as liberal or
conservative. As in Karrer and Newman [10]  we symmetrize the network and consider only the
largest connected component of 1222 nodes. The average degree of the network is roughly 15. We
apply RSC to the data set with τ ranging from 0 to 30. In the case where τ = 0  it is standard
Spectral Clustering. SC assigns 1144 out of 1222 nodes to the same block  failing to detect the
ideological partition. RSC detects the partition  and its performance is insensitive to the τ. With
τ ∈ [1  30]  RSC misclusters (80 ± 2) nodes out of 1222.
If RSC is applied to the 90% of nodes with the largest leverage scores (i.e. excluding the nodes
with the smallest leverage scores)  then the misclustering rate among these high leverage nodes is
44/1100  which is almost 50% lower. This illustrates how the leverage score corresponding to a
node can gauge the strength of the clustering evidence for that node relative to the other nodes.
We tried to compare these results to the regularized algorithm in [1]. However  because there are
several very small degree nodes in this data  the values computed in step 4 of the algorithm in [1]
sometimes take negative values. Then  step 5 (b) cannot be performed.

6 Discussion

In this paper  we give theoretical  simulation  and empirical results that demonstrate how a simple
adjustment to the standard spectral clustering algorithm can give dramatically better results for net-
works with heterogeneous degrees. Our theoretical results add to the current results by studying the
regularization step in a more canonical version of the spectral clustering algorithm. Moreover  our
main results require no assumptions on the minimum node degree. This is crucial because it allows
us to study situations where several nodes have small leverage scores; in these situations  regular-
ization is most beneﬁcial. Finally  our results demonstrate that choosing a tuning parameter close to
the average degree provides a balance between several competing objectives.

Acknowledgements

Thanks to Sara Fernandes-Taylor for helpful comments. Research of TQ is supported by NSF Grant
DMS-0906818 and NIH Grant EY09946. Research of KR is supported by grants from WARF and
NSF grant DMS-1309998.

8

lllllll2.02.53.03.50.00.20.40.60.81.0betamis−clustering ratelSCRSCRSC_wpt−RSCSCPRSC on Slll10152025300.00.10.20.30.40.5expected average degreemis−clustering ratelSCRSCReferences
[1] K. Chaudhuri  F. Chung  and A. Tsiatas. Spectral clustering of graphs with general degrees
in the extended planted partition model. Journal of Machine Learning Research  pages 1–23 
2012.

[2] Arash A Amini  Aiyou Chen  Peter J Bickel  and Elizaveta Levina. Pseudo-likelihood methods

for community detection in large sparse networks. 2012.

[3] F. McSherry. Spectral partitioning of random graphs. In Foundations of Computer Science 

2001. Proceedings. 42nd IEEE Symposium on  pages 529–537. IEEE  2001.

[4] Anirban Dasgupta  John E Hopcroft  and Frank McSherry. Spectral analysis of random graphs
with skewed degree distributions. In Foundations of Computer Science  2004. Proceedings.
45th Annual IEEE Symposium on  pages 602–610. IEEE  2004.

[5] Amin Coja-Oghlan and Andr´e Lanka. Finding planted partitions in random graphs with general

degree distributions. SIAM Journal on Discrete Mathematics  23(4):1682–1714  2009.

[6] Brendan PW Ames and Stephen A Vavasis. Convex optimization for the planted k-disjoint-

clique problem. arXiv preprint arXiv:1008.2814  2010.

[7] K. Rohe  S. Chatterjee  and B. Yu. Spectral clustering and the high-dimensional stochastic

blockmodel. The Annals of Statistics  39(4):1878–1915  2011.

[8] D.L. Sussman  M. Tang  D.E. Fishkind  and C.E. Priebe. A consistent adjacency spectral
embedding for stochastic blockmodel graphs. Journal of the American Statistical Association 
107(499):1119–1128  2012.

[9] P.W. Holland and S. Leinhardt. Stochastic blockmodels: First steps. Social networks  5(2):

109–137  1983.

[10] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in

networks. Physical Review E  83(1):016107  2011.

[11] Michael W Mahoney. Randomized algorithms for matrices and data. Advances in Machine
Learning and Data Mining for Astronomy  CRC Press  Taylor & Francis Group  Eds.: Michael
J. Way  Jeffrey D. Scargle  Kamal M. Ali  Ashok N. Srivastava  p. 647-672  1:647–672  2012.
[12] Andrew Y Ng  Michael I Jordan  and Yair Weiss. On spectral clustering: Analysis and an

algorithm. Advances in neural information processing systems  2:849–856  2002.

[13] Jiashun Jin. Fast network community detection by score. arXiv preprint arXiv:1211.5803 

2012.

[14] Fan Chung and Mary Radcliffe. On the spectra of general random graphs.

journal of combinatorics  18(P215):1  2011.

the electronic

[15] Peter H Sch¨onemann. A generalized solution of the orthogonal procrustes problem. Psychome-

trika  31(1):1–10  1966.

[16] D.S. Choi  P.J. Wolfe  and E.M. Airoldi. Stochastic blockmodels with a growing number of

classes. Biometrika  99(2):273–284  2012.

[17] Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election:
divided they blog. In Proceedings of the 3rd international workshop on Link discovery  pages
36–43. ACM  2005.

9

,Tai Qin
Karl Rohe
Xiaojiao Mao
Yu-Bin Yang
Zhihao Xia
Ayan Chakrabarti