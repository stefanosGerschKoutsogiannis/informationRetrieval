2013,Variational Planning for Graph-based MDPs,Markov Decision Processes (MDPs) are extremely useful for modeling and solving sequential decision making problems. Graph-based MDPs provide a compact representation for MDPs with large numbers of random variables. However  the complexity of exactly solving a graph-based MDP usually grows exponentially in the number of variables  which limits their application. We present a new variational framework to describe and solve the planning problem of MDPs  and derive both exact and approximate planning algorithms. In particular  by exploiting the graph structure of graph-based MDPs  we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions  then solved by minimizing a Kullback-Leibler (KL) divergence. The KL divergence is optimized using the belief propagation algorithm  with complexity exponential in only the cluster size of the graph. Experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies.,Variational Planning for Graph-based MDPs

Qiang Cheng†

Qiang Liu‡

Feng Chen†

Alexander Ihler‡

†Department of Automation  Tsinghua University

‡Department of Computer Science  University of California  Irvine

†{cheng-q09@mails.  chenfeng@mail.}tsinghua.edu.cn

‡{qliu1@ ihler@ics.}uci.edu

Abstract

Markov Decision Processes (MDPs) are extremely useful for modeling and solv-
ing sequential decision making problems. Graph-based MDPs provide a compact
representation for MDPs with large numbers of random variables. However  the
complexity of exactly solving a graph-based MDP usually grows exponentially in
the number of variables  which limits their application. We present a new varia-
tional framework to describe and solve the planning problem of MDPs  and derive
both exact and approximate planning algorithms. In particular  by exploiting the
graph structure of graph-based MDPs  we propose a factored variational value iter-
ation algorithm in which the value function is ﬁrst approximated by the multiplica-
tion of local-scope value functions  then solved by minimizing a Kullback-Leibler
(KL) divergence. The KL divergence is optimized using the belief propagation
algorithm  with complexity exponential in only the cluster size of the graph. Ex-
perimental comparison on different models shows that our algorithm outperforms
existing approximation algorithms at ﬁnding good policies.

Introduction

1
Markov Decision Processes (MDPs) have been widely used to model and solve sequential decision
making problems under uncertainty  in ﬁelds including artiﬁcial intelligence  control  ﬁnance and
management (Puterman  2009  Barber  2011). However  standard MDPs are described by explicitly
enumerating all possible states of variables  and are thus not well suited to solve large problems.
Graph-based MDPs (Guestrin et al.  2003  Forsell and Sabbadin  2006) provide a compact repre-
sentation for large and structured MDPs  where the transition model is explicitly represented by a
dynamic Bayesian network. In graph-based MDPs  the state is described by a collection of random
variables  and the transition and reward functions are represented by a set of smaller (local-scope)
functions. This is particularly useful for spatial systems or networks with many “local” decisions 
each affecting small sub-systems that are coupled together and interdependent (Nath and Domingos 
2010  Sabbadin et al.  2012).
The graph-based MDP representation gives a compact way to describe a structured MDP  but the
complexity of exactly solving such MDPs typically still grows exponentially in the number of state
variables. Consequently  graph-based MDPs are often approximately solved by enforcing context-
speciﬁc independence or function-speciﬁc independence constraints (Sigaud et al.  2010). To take
advantage of context-speciﬁc independence  a graph-based MDP can be represented using decision
trees or algebraic decision diagrams (Bahar et al.  1993)  and then solved by applying structured
value iteration (Hoey et al.  1999) or structured policy iteration (Boutilier et al.  2000). However 
in the worst case  the size of the diagram still increases exponentially with the number of variables.
Alternatively  methods based on function-speciﬁc independence approximate the value function by
a linear combination of basis functions (Koller and Parr  2000  Guestrin et al.  2003). Exploit-
ing function-speciﬁc independence  a graph-based MDP can be solved using approximate linear
programming (Guestrin et al.  2003  2001  Forsell and Sabbadin  2006)  approximate policy itera-

1

tion (Sabbadin et al.  2012  Peyrard and Sabbadin  2006) and approximate value iteration (Guestrin
et al.  2003). Among these  the approximate linear programming algorithm in Guestrin et al. (2003 
2001) has an exponential number of constraints (in the treewidth)  and thus cannot be applied to
general MDPs with many variables. The approximate policy iteration algorithm in Sabbadin et al.
(2012)  Peyrard and Sabbadin (2006) exploits a mean ﬁeld approximation to compute and update
the local policies; unfortunately this can give loose approximations.
In this paper  we propose a variational framework for the MDP planning problem. This framework
provides a new perspective to describe and solve graph-based MDPs where both the state and deci-
sion spaces are structured. We ﬁrst derive a variational value iteration algorithm as an exact planning
algorithm  which is equivalent to the classical value iteration algorithm. We then design an approx-
imate version of this algorithm by taking advantage of the factored representation of the reward and
transition functions  and propose a factored variational value iteration algorithm. This algorithm
treats the value function as a unnormalized distribution and approximates it using a product of local-
scope value functions. At each step  this algorithm computes the value function by minimizing a
Kullback-Leibler divergence  which can be done using a belief propagation algorithm for inﬂuence
diagram problems (Liu and Ihler  2012) . In comparison with the approximate linear programming
algorithm (Guestrin et al.  2003) and the approximate policy iteration algorithm (Sabbadin et al. 
2012) on various graph-based MDPs  we show that our factored variational value iteration algo-
rithm generates better policies.
The remainder of this paper is organized as follows. The background and some notation for graph-
based MDPs are introduced in Section 2. Section 3 describes a variational view of planning for ﬁnite
horizon MDPs  followed by a framework for inﬁnite MDPs in Section 4. In Section 5  we derive
an approximate algorithm for solving inﬁnite MDPs based on the variational perspective. We show
experiments to demonstrate the effectiveness of our algorithm in Section 6.
2 Markov Decision Processes and Graph-based MDPs
2.1 Markov Decision Processes
A Markov Decision Process (MDP) is a discrete time stochastic control process  where the system
chooses the decisions at each step to maximize the overall reward. An MDP can be characterized
by a four tuple (X  D  R  T )  where X represents the set of all possible states; D is the set of all
possible decisions; R : X × D → R is the reward function of the system  and R (x  d) is the
reward of the system after choosing decision d in state x; T : X × D × X → [0  1] is the transition
function  and T (y|x  d) is the probability that the system arrives at state y  given that it starts from
x upon executing decision d. A policy of the system is a mapping from the states to the decisions
π (x) : X → D so that π (x) tells the decision chosen by the system in state x. The graphical
representation of an MDP is shown in Figure 1(a).
We consider the case of an MDP with inﬁnite horizon  in which the future rewards are discounted
exponentially with a discount factor γ ∈ [0  1]. The task of the MDP is to choose the best stationary
policy π∗ (x) that maximizes the expected discounted reward on the inﬁnite horizon. The value
function v∗ (x) of the best policy π∗ (x) then satisﬁes the following Bellman equation:

v∗ (x) = max

(1)
where v∗ (x) = v∗ (y)  ∀x = y. The Bellman equation can be solved using stochastic dynamic
programming algorithms such as value iteration and policy iteration  or linear programming algo-
rithms (Puterman  2009).

T (y|x  π (x)) (R (x  π (x)) + γv∗ (y)) 

(cid:88)

π(x)

y∈X

2.2 Graph-based MDPs
We assume that the full state x can be represented as a collection of state variables xi  so that X
is a Cartesian product of the domains of the xi: X = X1 × X2 × ··· × XN   and similarly for d:
D = D1 × D2 × ··· × DN . We consider the following particular factored form for MDPs: for each
variable i  there exist neighborhood sets Γi (including i) such that the value of xt+1
depends only
on the variable i’s neighborhood  xt [Γi]  and the ith decision dt
i. Then  we can write the transition
function in a factored form:

i

T (y|x  d) =

Ti (yi|x[Γi]  di) 

(2)

N(cid:89)

i=1

2

(a)

(b)

Figure 1: (a) A Markov decision process; (b) A graph-based Markov decision process.

where each factor is a local-scope function Ti : X [Γi]×Di ×Xi → [0  1]  ∀i ∈ {1  2  . . .   N} . We
also assume that the reward function is the sum of N local-scope rewards:

R (x  d) =

Ri (xi  di) 

(3)

N(cid:88)

i=1

with local-scope functions Ri : Xi × Di → R ∀i ∈ {1  2  . . .   N}.
To summarize  a graph-based Markov decision process is characterized by the following parameters:
({Xi : 1 ≤ i ≤ N} ;{Di : 1 ≤ i ≤ N} ;{Ri : 1 ≤ i ≤ N} ;{Γi : 1 ≤ i ≤ N} ;{Ti : 1 ≤ i ≤ N}) .
Figure 1(b) gives an example of a graph-based MDP. These assumptions for graph-based MDPs can
be easily generalized  for example to include Ti and Ri that depend on arbitrary sets of variables
and decisions  using some additional notation.
The optimal policy π (x) cannot be explicitly represented for large graph-based MDPs  since the
number of states grows exponentially with the number of variables. To reduce complexity  we con-
sider a particular class of local policies: a policy π (x) : X → D is said to be local if decision di is
made using only the neighborhood Γi  so that π (x) = (π1 (x [Γ1])   π2 (x [Γ2])   . . .   πN (x [ΓN ]))
where πi (x [Γi]) : X [Γi] → Di. The main advantage of local policies is that they can be concisely
expressed when the neighborhood sizes |Γi| are small.
3 Variational Planning for Finite Horizon MDPs
In this section  we introduce a variational planning viewpoint of ﬁnite MDPs. A ﬁnite MDP can
be viewed as an inﬂuence diagram; we can then directly relate planning to the variational decision-
making framework of Liu and Ihler (2012).
Inﬂuence diagrams (Shachter  2007) make use of Bayesian networks to represent structured decision
problems under uncertainty. The shaded part in Figure 1(a) shows a simple example inﬂuence
diagram  with random variables {x  y}  decision variable d and reward functions {R (x  d)   v (y)}.
The goal is then to choose a policy that maximizes the expected reward.
The best policy πt (x) for a ﬁnite MDP can be computed using backward induction (Barber  2011):

T (y|x  π (x))(cid:0)R (x  π (x)) + γvt (y)(cid:1) 

(4)

vt−1 (x) = max

π (x)

(cid:88)

y∈X

Let pt (x  y  d) = T (y|x  π (x)) (R (x  π (x)) + γvt (y)) be an augmented distribution (see  e.g. 
Liu and Ihler (2012)). Applying a variational framework for inﬂuence diagrams (Liu and Ihler 
2012  Theorem 3.1)  the optimal policy can be equivalently solved from the dual form of Eq. (4):

(cid:8)(cid:10)θ∆;t  τ(cid:11) + H (x  y  d; τ ) − H (d|x; τ )(cid:9)  

Φ(cid:0)θt(cid:1) = max

(5)
where θ∆;t (x  y  d) = log pt (x  y  d) = log T (y|x  d) + log (R (x  d) + γvt (y))  and τ is a
vector of moments in the marginal polytope M (Wainwright and Jordan  2008). In a mild abuse
of notation  we will use τ to refer both to the vector of moments and to the maximum entropy

τ∈M

3

      rrrxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxdRdxRRxxd1 5 6 95 62 5 6 7 106 73 6 7 8 117 8 4 7 8 12 5 6 5 6 5 6 7 6 7 6 7 8 7 8  7 8 xyxxdddxxx Rxd Rxd Rxd| Tyxd| Tyxd| Tyxd    rrxyxddxx Rxd Rxd| Tyxd| Tyxd      rrrxyxxdddxxx Rxd Rxd Rxd| Tyxd| Tyxd| Tyxd      1x2x3x1d2d3d1r2r3r      1d2d3d1r2r3r1y2y3y1x2x3x11 Rxd22 Rxd33 Rxd1121|  Tyxxd21232|   Tyxxxd3233|  Tyxxd      rrrxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxdRdxRRxxd1 5 6 95 62 5 6 7 106 73 6 7 8 117 8 4 7 8 12 5 6 5 6 5 6 7 6 7 6 7 8 7 8  7 8 xyxxdddxxx Rxd Rxd Rxd| Tyxd| Tyxd| Tyxd    rrxyxddxx Rxd Rxd| Tyxd| Tyxd      rrrxyxxdddxxx Rxd Rxd Rxd| Tyxd| Tyxd| Tyxd      1x2x3x1d2d3d1r2r3r      1d2d3d1r2r3r1y2y3y1x2x3x11 Rxd22 Rxd33 Rxd1121|  Tyxxd21232|   Tyxxxd3233|  Tyxxd1x2x3xdistribution τ (x  y  d) consistent with those moments; H(·; τ ) refers to the entropy or conditional
entropy of this distribution. See also Wainwright and Jordan (2008)  Liu and Ihler (2012) for details.
Let τ t (x  y  d) be the optimal solution of Eq. (5); then from Liu and Ihler (2012)  the optimal policy
πt (x) is simply arg maxd τ t (d|x). Moreover  the optimal value function vt−1 (x) can be obtained
from Eq. (5). This result is summarized in the following lemma.
Lemma 1. For ﬁnite MDPs with non-stationary policy  the best policy πt (x) and the value function
vt−1 (x) can be obtained by solving Eq. (5). Let τ t (x  y  d) be the optimal solution of Eq. (5).
(a) The optimal policy can be obtained from τ t (x  y  d)  as πt (x) = arg maxd τ t (d|x).
(b) The value function w.r.t. πt (x) can be obtained as vt−1 (x) = exp (Φ (θt)) τ t (x).

follows directly from Theorem 3.1 of Liu and Ihler

Proof. (a)
(b) Note that
T (y|x  πt (x)) (R (x  πt (x)) + γvt (y)) = exp (Φ (θt)) τ t (x  y  d). Making use of Eq. (4) 
summing over y and maximizing over d on exp (Φ (θt)) τ t (x  y  d)  we obtain vt−1 (x) =
exp (Φ (θt)) τ t (x).

(2012).

4 Variational Planning for Inﬁnite Horizon MDPs
Given the variational form of ﬁnite MDPs  we now construct a variational framework for inﬁnite
MDPs. Compared to the primal form (i.e.  Eq. (4)) of ﬁnite MDPs  the Bellman equation of an
inﬁnite MDP  Eq. (1)  has the additional constraint that vt−1 (x) = vt (y) when x = y. For an
inﬁnite MDP  we can simply consider a two-stage ﬁnite MDP with the variational form in Eq. (5) 
but with this additional constraint. The main result is given by the following theorem.
Theorem 2. Assume τ and Φ are the solution of the following optimization problem 

subject to Φ =(cid:10)θ∆  τ(cid:11) + H (x  y  d; τ ) − H (d|x; τ ) 

max

τ∈M Φ∈R Φ 

(6)

(7)

θ∆ = log T (y|x  d) + log (R (x  d) + γ exp (Φ) τx (y))  
where τx denotes the marginal distribution on x. With τ ∗ being the optimal solution  we have

(a) The optimal policy of the inﬁnite MDP can be decoded as π∗ (x) = arg maxd τ ∗ (d|x).
(b) The value function w.r.t. π∗ (x) is v∗ (x) = exp (Φ) τ ∗ (x).

Proof. The Bellman equation is equivalent to the backward induction in Eq. (4)  subject to an extra
constraint that vt = vt−1. The result follows by replacing Eq. (4) with its variational dual (5).

Like the Bellman equation (4)  its dual form (6) also has no closed-form solution. Analogously to the
value iteration algorithm for the Bellman equation  Eq. (6) can be solved by alternately ﬁxing τx (x) 
Φ in θ∆ and solving Eq. (6) with only the ﬁrst constraint using some convex optimization technique.
However  each step of solving for τ and Φ is equivalent to one step of value iteration; if τ (x  y  d) is
represented explicitly  it seems to offer no advantage over simply applying the elimination operators
as in (4). The usefulness of this form is mainly in opening the door to design new approximations.

5 Approximate Variational Algorithms for Graph-based MDPs
The framework in the previous section gives a new perspective on the MDP planning problem  but
does not by itself simplify the problem or provide new solution methods. For graph-based MDPs 
the sizes of the full state and decision spaces are exponential in the number of variables. Thus  the
complexity of exact algorithms is exponentially large. In this section  we present an approximate
algorithm for solving Eq. (6)  by exploiting the factorization structure of the transition function (2) 
the reward function (3) and the value function v (x).
Standard variational approximations take advantage of the multiplicative factorization of a dis-
tribution to deﬁne their approximations. While our (unnormalized) distribution p (x  y  d) =
exp[θ∆(x  y  d)] is structured  some of its important structure comes from additive factors  such
as the local-scope reward functions Ri (xi  di) in Eq. (3)  and the discounted value function γv (x)
in Eq. (1). Computing the sum of these additive factors directly would create a large factor over an
unmanageably large variable domain  and destroy most of the useful structure of p (x  y  d).

4

To avoid this effect  we convert the presence of additive factors into multiplicative factors by aug-
menting the model with a latent “selector” variable  which is similar to that used for the “complete
likelihood” in mixture models (Liu and Ihler  2012). For example  consider the sum of two factors:
¯f12(a  x1  x2) · ¯f23(a  x2  x3).

f (x) = f12 (x1  x2) + f23 (x2  x3) =

(f12)a · (f23)(1−a) =

(cid:88)

(cid:88)

a∈{0 1}

a∈{0 1}

Introducing the auxilliary variable a converts f into a product of factors  where marginalizing over
a yields the original function f.
Using this augmenting approach  the additive elements of the graph-based MDP are converted to
multiplicative factors  that is Ri (xi  di) → ˜Ri (xi  di  a)  and γv (x) → ˜vγ (x  a). In this way  the
parameter θ∆ of a graph-based MDP can be represented as

θ∆ (x  y  d  a) =

log ˜Ri (xi  di  a) + log ˜vγ (y  a) .

N(cid:88)

i=1

log Ti (yi|x[Γi]  di) +
N(cid:88)

N(cid:88)

i=1

N(cid:88)

i=1

(cid:88)
(cid:81)
(cid:81)

k∈C

(cid:81)
(cid:81)

Now  p (x  y  d  a) = exp[θ∆ (x  y  d  a)] has a representation in terms of a product of factors. Let

θ (x  y  d  a) =

log Ti (yi|x[Γi]  di) +

log ˜Ri (xi  di  a).

i=1

Before designing the algorithms  we ﬁrst construct a cluster graph (G;C;S) for the distribution
exp[θ (x  y  d  a)]  where C denotes the set of clusters and S is the set of separators. (See Liu and
Ihler (2012  2011)  Wainwright and Jordan (2008) for more details on cluster graphs.) We assign
each decision node di to one cluster that contains di and its parents pa(i); clusters so assigned are
called decision clusters A  while other clusters are called normal clusters R  so that C = {R A}.
Using the structure of the cluster graph  θ can be decomposed into

θck (xck   yck   dck   a) 

(8)

θ (x  y  d  a) =

and the distribution τ is approximated as

k∈C τck (zck )

τ (x  y  d  a) =

(kl)∈S τskl (zskl )

(9)
where zck = {xck   yck   dck   a}. Therefore  instead of optimizing the full distribution τ   we can
optimize the collection of marginal distributions τ = {τck   τsk}  with far lower computational cost.
These marginals should belong to the local consistency polytope L  which enforces that marginals
are consistent on their overlapping sets of variables (Wainwright and Jordan  2008).
We now construct a reduced cluster graph over x from the full cluster graph  to serve as the approx-
imating structure of the marginal τ (x). We assume a factored representation for τ (x):

 

τ (x) =

k∈C τck (xck )

(kl)∈S τskl (xskl )

 

(10)

factors into vγ (x) =(cid:81)

where the τck (xck ) is the marginal distribution of τck (zck ) on xck. Note that Eq. (10) also dictates a
factored approximation of the value function v (x)  because v (x) ≈ exp (Φ) τ (x). Assume vγ (x)
k vck (xck ). Then  the constraint (7) reduces to a set of simpler constraints

on the cliques of the cluster graph 

θ∆
ck

(xck   yck   dck   a) = θck

Correspondingly  the constraint (6) can be approximated by
H(cid:48)

(cid:104)θ∆

  τck(cid:105) +

Φ =

ck

(xck   yck   dck   a) + log vck x (yck   a)   k ∈ C.
(cid:88)

(cid:88)

ck − (cid:88)

Hck +

Hskl  

k∈R

k∈D

(kl)∈S

(11)

(12)

(cid:88)

k∈C

is the entropy of variables in cluster ck  Hck = H (xck   yck   dck   a; τ ) and H(cid:48)
ck

where Hck
=
H (xck   yck   dck   a; τ ) − H (dck|xck ; τ ). With these approximations  we solve the optimization in
Theorem 2 using “mixed” belief propagation (Liu and Ihler  2012) for ﬁxed {θ∆
}; we then update
{θ∆

} using the ﬁxed point condition (11). This gives the double loop algorithm in Algorithm 1.

ck

ck

5

Algorithm 1 Factored Variational Value Iteration Algorithm
Input: A graph-based MDP with ({Xi} ;{Di} ;{Ri} ;{Γi} ;{Ti})  the cluster graph (G;C;S)  and

the initial(cid:8)τ t=0

ck

(xck )  ∀ck ∈ C(cid:9).

Iterate until convergence (for both the outer loop and the inner loop).

Outer loop: Update θ∆;t

ck using Eq. (11).

1:
2:

Inner loop: Maximize the right side of Eq. (12) with ﬁxed θ∆;t
using the belief propagation algorithm proposed in Liu and Ihler (2012):

ck and compute τ t+1
ck

(xck )

mk→l(zck ) ∝ ψskl

(zskl )

(cid:88)

zck\skl

σ(cid:2)ψck (zck )m∼k(zck )(cid:3)
(cid:26)τck (zck )
(cid:88)
τck (zck )τck (dck|xck )

ml→k(zck )

 

τck (zck )

τck (xck ) = max
dck

yck  a

ck ∈ R
ck ∈ A 

where ψck (zck ) = exp[θ∆
ck

(zck )] 

and σ[τck (zck )] =

with

τck (zck ) = ψck (zck )m∼k(zck ) and

Output: The local policies {τ (di|x (Γi))}  and the value function ˆv (x) = exp (Φ) τ (x).

(cid:80)

6 Experiments
We perform experiments in two domains  disease management in crop ﬁelds and viral marketing  to
evaluate the performance of our factored variational value iteration algorithm (FVI). For comparison 
we use approximate policy iteration algorithm (API) (Sabbadin et al.  2012)  (a mean-ﬁeld based pol-
icy iteration approach)  and the approximate linear programming algorithm (ALP) (Guestrin et al. 
2001). To evaluate each algorithm’s performance  we obtain its approximate local policy  then com-
pute the expected value of the policy using either exact evaluation (if feasible) or a sample-based
estimate (if not). We then compare the expected reward U alg (x) = 1|X|
x valg (x) of each algo-
rithm’s policy.
6.1 Disease Management in Crop Fields
A graph-based MDP for disease management in crop ﬁelds was introduced in (Sabbadin et al. 
2012). Suppose we have a set of crop ﬁelds in an agricultural area  where each ﬁeld is susceptible to
contamination by a pathogen. When a ﬁeld is contaminated  it can infect its neighbors and the yield
will decrease. However  if a ﬁeld is left fallow  it has a probability (denoted by q) of recovering from
infection. The decisions of each year include two options (Di = {1  2}) for each ﬁeld: cultivate
normally (di = 1) or leave fallow (di = 2). The problem is then to choose the optimal stationary
policy to maximize the expected discounted yield. The topology of the ﬁelds is represented by an
undirected graph  where each node represents one crop ﬁeld. An edge is drawn between two nodes
if the ﬁelds share a common border (and can thus pass an infection). Each crop ﬁeld can be in
one of three states: xi = 1 if it is uninfected and xi = 2 to xi = 3 for increasing degrees of
infection. The probability that a ﬁeld moves from state xi to state xi + 1 with di = 1 is set to be
P = P (ε  p  ni) = ε + (1 − ε) (1 − (1 − p)ni)  where ε and p are parameters and ni is the number
of the neighbors of i that are infected. The transition function is summarized in Table 1. The reward
function depends on each ﬁeld’s state and local decision. The maximal yield r > 1 is achieved by an
uninfected  cultivated ﬁeld; otherwise  the yield decreases linearly with the level of infection  from
maximal reward r to minimal reward 1 + r/10. A ﬁeld left fallow produces reward 1.

Table 1: Local transition probabilities p(cid:0)x(cid:48)

(cid:1)  for the disease management problem.

i|xN (i)  ai

di = 1
xi = 2
0
1 − P
P

xi = 1
1 − P
P
0

xi = 3

xi = 1

xi = 3

x(cid:48)
i = 1
x(cid:48)
i = 2
x(cid:48)
i = 3
6.2 Viral Marketing
Viral marketing (Nath and Domingos  2010  Richardson and Domingos  2002) uses the natural
premise that members of a social network inﬂuence each other’s purchasing decisions or comments;
then  the goal is to select the best set of people to target for marketing such that overall proﬁt is

q/2
q/2
1 − q

0
0
1

1
0
0

di = 2
xi = 2
q
1 − q
0

6

maximized. Viral marketing has been previously framed as a one-shot inﬂuence diagram problem
(Nath and Domingos  2010). Here  we frame the viral marketing task as an MDP planning problem 
where we optimize the stationary policy to maximize long-term reward.
The topology of the social network is represented by a directed graph  capturing directional social
inﬂuence. We assume there are three states for each person in the social network: xi = 1 if i is
making positive comments  xi = 2 if not commenting  and xi = 3 for negative comments. There is
a binary decision corresponding to each person i: market to this person (di = 1) or not (di = 2). We
also deﬁne a local reward function: if a person gives good comments when di = 2  then the reward
is r; otherwise  the reward is less  decreasing linearly to minimum value 1 + r/10. For marketed

individuals (di = 1)  the reward is 1. The local transition p(cid:0)x(cid:48)

(cid:1) is set as in Table 1.

i|xN (i)  di

6.3 Experimental Results
We evaluate both problems above on two topologies of model  each of three sizes (6  10  and 20
nodes). Our ﬁrst topology type are random  regular graphs with three neighbors per node. Our
second are “chain-like” graphs  in which we order the nodes  then connect each node at random to
four of its six nearest nodes in the ordering. This ensures that the resulting graph has low tree-width
(≤ 6)  enabling comparison of the ALP algorithm. We set parameters r = 10 and ε = 0.1  and test
the results on different choices of p and q.
Tables 2– 4 show the expected rewards found by each algorithm for several settings. The best
performance (highest rewards) are labeled in bold. For models with 6 nodes  we also compute the
expected reward under the optimal global policy π∗ (x) for comparison. Note that this value over-
estimates the best possible local policy {π∗
i (Γi(x))} being sought by the algorithms; the best local
policy is usually much more difﬁcult to compute due to imperfect recall. Since the complexity of the
approximate linear programming (ALP) algorithm is exponential in the treewidth of graph deﬁned
by the neighborhoods Γi  we were unable to compute results for models beyond treewidth 6.
The tables show that our factored variational value iteration (FVI) algorithm gives policies with
higher expected rewards than those of approximate policy iteration (API) on the majority of models
(156/196)  over all sets of models and different p and q. Compared to approximate linear program-
ming  in addition to being far more scalable  our algorithm performed comparably  giving better
policies on just over half of the models (53/96) that ALP could be run on. However  when we
restrict to low-treewidth “chain” models  we ﬁnd that the ALP algorithm appears to perform better
on larger models; it outperforms our FVI algorithm on only 4/32 models of size 6  but this increases
to 14/32 at size 10  and 25/32 at size 20. It may be that ALP better takes advantage of the structure
of x on these cases  and more careful choice of the cluster graph could similarly improve FVI.
The average results across all settings are shown in Table 5  along with the relative improvements of
our factored variational value iteration algorithm to approximate policy iteration and approximate
linear programming. Table 5 shows that our FVI algorithm  compared to approximate policy iter-
ation  gives the best policies on regular models across sizes  and gives better policies than those of
the approximate linear programming on chain-like models with small size (6 and 10 nodes). Al-
though on average the approximate linear programming algorithm may provide better policies for
“chain” models with large size  its exponential number of constraints makes it infeasible for general
large-scale graph-based MDPs.
7 Conclusions
In this paper  we have proposed a variational planning framework for Markov decision processes.
We used this framework to develop a factored variational value iteration algorithm that exploits the
structure of the graph-based MDP to give efﬁcient and accurate approximations  scales easily to large
systems  and produces better policies than existing approaches. Potential future directions include
studying methods for the choice of cluster graphs  and improved solutions for the dual approxima-
tion (12)  such as developing single-loop message passing algorithms to directly optimize (12).
Acknowledgments
This work was supported in part by National Science Foundation grants IIS-1065618 and IIS-
1254071  a Microsoft Research Fellowship  National Natural Science Foundation of China
(#61071131 and #61271388)  Beijing Natural Science Foundation (#4122040)  Research Project
of Tsinghua University (#2012Z01011)  and Doctoral Fund of the Ministry of Education of China
(#20120002110036).

7

Table 2: The expected rewards of different algorithms on regular models with 6 nodes.

(p  q)

(0.2  0.2)

(0.4  0.2)

(0.6  0.2)

(0.8  0.2)

(0.2  0.4)

(0.4  0.4)

(0.6  0.4)

(0.8  0.4)

(0.2  0.6)

(0.4  0.6)

(0.6  0.6)

(0.8  0.6)

(0.2  0.8)

(0.4  0.8)

(0.6  0.8)

(0.8  0.8)

Exact
202.4
169.2
158.1
154.1
262.5
220.1
212.1
211.7
349.3
290.7
284.7
284.0
423.6
362.2
351.6
350.5

Disease Management

FVI
202.4
169.2
155.2
152.7
259.2
219.1
203.8
198.2
349.3
276.7
242.7
236.1
423.6
351.0
304.8
284.2

API
164.7
139.0
157.4
153.2
254.7
177.0
203.8
198.2
333.6
276.7
243.7
236.1
423.6
344.3
302.7
284.9

ALP
148.3
123.3
115.4
106.0
236.7
181.3
162.7
136.1
307.3
200.0
212.8
194.7
274.7
264.5
242.5
207.9

Viral Marketing
FVI
258.2
195.3
167.8
152.7
361.6
285.8
244.6
225.2
428.1
351.7
304.7
282.9
469.8
402.0
347.8
320.8

API
250.0
192.6
174.0
172.2
355.8
285.1
249.6
296.8
428.1
303.3
152.5
355.0
469.8
402.0
351.8
398.0

ALP
237.7
183.4
156.4
144.7
355.0
267.3
244.8
273.5
427.7
350.0
306.5
271.3
469.8
403.7
336.6
294.0

Exact
259.3
212.2
209.6
209.5
361.6
300.2
297.3
297.3
428.1
361.8
355.5
355.5
470.0
411.6
398.2
398.0

Table 3: The expected rewards of different algorithms on “chain-like” models with 10 nodes.

Viral Marketing

(p  q)

(0.3  0.3)

(0.5  0.3)

(0.7  0.3)

(0.3  0.5)

(0.5  0.5)

(0.7  0.5)

(0.3  0.7)

(0.5  0.7)

(0.7  0.7)

Disease Management
ALP
FVI
304.8
288.9
292.7
273.4
329.6
262.2
456.5
420.2
358.5
302.6
394.3
343.8
612.9
531.2
538.6
498.2
430.0
427.3

API
258.4
228.7
261.6
395.4
317.7
344.9
613.6
491.8
411.8

FVI
355.5
308.1
298.5
550.1
453.3
386.1
659.9
542.7
496.9

API
324.1
291.5
298.1
523.9
450.9
418.6
634.8
523.9
495.7

ALP
335.5
323.8
269.7
543.9
410.0
436.9
664.7
518.2
451.2

Table 4: The expected rewards (×102) of different algorithms on models with 20 nodes.

(p  q)
(0.2  0.2)

(0.6  0.2)

(0.4  0.4)

(0.4  0.4)

(0.6  0.6)

(0.6  0.6)

(0.8  0.8)

(0.8  0.8)

Disease Manag. Viral Marketing
FVI
API
7.88
7.17
5.33
5.28
9.10
11.52
7.04
7.65
12.29
13.85
10.02
8.50
15.27
14.53
10.90
11.50

FVI
7.87
5.99
11.56
7.95
13.85
10.22
15.25
11.82

API
6.33
4.94
8.82
6.17
12.11
8.72
14.57
10.78

(p  q)
(0.4  0.2)

(0.8  0.2)

(0.4  0.4)

(0.4  0.4)

(0.6  0.6)

(0.6  0.6)

(0.8  0.8)

(0.8  0.8)

Disease Manag. Viral Marketing
FVI
API
5.93
5.65
5.62
5.12
7.70
8.83
6.72
7.14
9.97
11.72
8.01
8.88
12.57
13.22
9.92
10.64

FVI
6.53
5.76
9.23
7.45
11.74
9.23
13.47
10.77

API
5.19
5.20
6.23
6.72
10.06
7.69
12.43
9.56

Table 5: Comparison of average expected rewards on regular and “chain-like” models.

Type
Regular

Chain

n = 6

n = 10

n = 20

FVI: 275.8
API: 271.4
FVI: 275.8
API: 271.6
ALP: 244.9

Rel. Imprv.

1.6%

Rel. Imprv.

1.6%
12.6%

FVI: 458.7
API: 452.3
FVI: 415.7
API: 399.4
ALP: 414.7

Rel. Imprv.

1.4%

Rel. Imprv.

4.1%
0.7%

FVI: 935.6
API: 905.1
FVI: 821.9
API: 749.6
ALP: 872.2

Rel. Imprv.
3.37%
Rel. Imprv.

9.7%−5.8%

8

References
R Iris Bahar  Erica A Frohm  Charles M Gaona  Gary D Hachtel  Enrico Macii  Abelardo Pardo  and Fabio
Somenzi. Algebraic decision diagrams and their applications. In IEEE/ACM International Conference on
Computer-Aided Design  pages 188–191  1993.

David Barber. Bayesian reasoning and machine learning. Cambridge University Press  2011.
Craig Boutilier  Richard Dearden  and Mois´es Goldszmidt. Stochastic dynamic programming with factored

representations. Artiﬁcial Intelligence  121(1):49–107  2000.

Nicklas Forsell and R´egis Sabbadin. Approximate linear-programming algorithms for graph-based markov

decision processes. Frontiers in Artiﬁcial Intelligence and Applications  141:590  2006.

Carlos Guestrin  Daphne Koller  and Ronald Parr. Multiagent planning with factored MDPs. Advances in

Neural Information Processing Systems  14:1523–1530  2001.

Carlos Guestrin  Daphne Koller  Ronald Parr  and Shobha Venkataraman. Efﬁcient solution algorithms for

factored mdps. Journal of Artiﬁcial Intelligence Research  19:399–468  2003.

Jesse Hoey  Robert St-Aubin  Alan Hu  and Craig Boutilier. SPUDD: Stochastic planning using decision
diagrams. In Proceedings of the Fifteenth conference on Uncertainty in Artiﬁcial Intelligence  pages 279–
288  1999.

Daphne Koller and Ronald Parr. Policy iteration for factored mdps. In Proceedings of the Sixteenth Conference

on Uncertainty in Artiﬁcial Intelligence  pages 326–334  2000.

Qiang Liu and Alexander Ihler. Variational algorithms for marginal MAP. In Uncertainty in Artiﬁcial Intelli-

gence (UAI)  2011.

Qiang Liu and Alexander Ihler. Belief propagation for structured decision making. In Uncertainty in Artiﬁcial

Intelligence (UAI)  pages 523–532  August 2012.

A. Nath and P. Domingos. Efﬁcient belief propagation for utility maximization and repeated inference. In The

Proceeding of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence  2010.

Nathalie Peyrard and R´egis Sabbadin. Mean ﬁeld approximation of the policy iteration algorithm for graph-

based markov decision processes. Frontiers in Artiﬁcial Intelligence and Applications  141:595  2006.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. Wiley-Interscience 

2009.

Matthew Richardson and Pedro Domingos. Mining knowledge-sharing sites for viral marketing. In Proceedings
of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining  pages 61–
70  2002.

R. Sabbadin  N. Peyrard  and N. Forsell. A framework and a mean-ﬁeld algorithm for the local control of

spatial processes. International Journal of Approximate Reasoning  53(1):66 – 86  2012.

Ross D Shachter. Model building with belief networks and inﬂuence diagrams. Advances in decision analysis:

from foundations to applications  pages 177–201  2007.

Olivier Sigaud  Olivier Buffet  et al. Markov decision processes in artiﬁcial intelligence. ISTE-Jonh Wiley &

Sons  2010.

M.J. Wainwright and M.I. Jordan. Graphical models  exponential families  and variational inference. Founda-

tions and Trends in Machine Learning  1(1-2):1–305  2008.

9

,Qiang Cheng
Qiang Liu
Feng Chen
Alexander Ihler
Siqi Sun
Mladen Kolar
Jinbo Xu
Huishuai Zhang
Yingbin Liang