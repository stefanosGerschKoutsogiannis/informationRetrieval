2015,The Consistency of Common Neighbors for Link Prediction in Stochastic Blockmodels,Link prediction and clustering are key problems for network-structureddata. While spectral clustering has strong theoretical guaranteesunder the popular stochastic blockmodel formulation of networks  itcan be expensive for large graphs. On the other hand  the heuristic ofpredicting links to nodes that share the most common neighbors withthe query node is much fast  and works very well in practice. We showtheoretically that the common neighbors heuristic can extract clustersw.h.p. when the graph is dense enough  and can do so even in sparsergraphs with the addition of a ``cleaning'' step. Empirical results onsimulated and real-world data support our conclusions.,The Consistency of Common Neighbors for
Link Prediction in Stochastic Blockmodels

Purnamrita Sarkar
Department of Statistics

University of Texas at Austin

purnamritas@austin.utexas.edu

Deepayan Chakrabarti

IROM  McCombs School of Business

University of Texas at Austin

deepay@utexas.edu

Peter Bickel

Department of Statistics

University of California  Berkeley

bickel@stat.berkeley.edu

Abstract

Link prediction and clustering are key problems for network-structured
data. While spectral clustering has strong theoretical guarantees under
the popular stochastic blockmodel formulation of networks  it can be ex-
pensive for large graphs. On the other hand  the heuristic of predicting
links to nodes that share the most common neighbors with the query node
is much fast  and works very well in practice. We show theoretically that
the common neighbors heuristic can extract clusters with high probabil-
ity when the graph is dense enough  and can do so even in sparser graphs
with the addition of a “cleaning” step. Empirical results on simulated and
real-world data support our conclusions.

1 Introduction

Networks are the simplest representation of relationships between entities  and as such have
attracted signiﬁcant attention recently. Their applicability ranges from social networks such
as Facebook  to collaboration networks of researchers  citation networks of papers  trust
networks such as Epinions  and so on. Common applications on such data include ranking 
recommendation  and user segmentation  which have seen wide use in industry. Most of
these applications can be framed in terms of two problems: (a) link prediction  where the
goal is to ﬁnd a few similar nodes to a given query node  and (b) clustering  where we want
to ﬁnd groups of similar individuals  either around a given seed node or a full partitioning
of all nodes in the network.
An appealing model of networks is the stochastic blockmodel  which posits the existence of
a latent cluster for each node  with link probabilities between nodes being simply functions
of their clusters. Inference of the latent clusters allows one to solve both the link prediction
problem and the clustering problem (predict all nodes in the query node’s cluster). Strong
theoretical and empirical results have been achieved by spectral clustering  which uses the
singular value decomposition of the network followed by a clustering step on the eigenvectors
to determine the latent clusters.
However  singular value decomposition can be expensive  particularly for (a) large graphs 
when (b) many eigenvectors are desired. Unfortunately  both of these are common require-
ments. Instead  many fast heuristic methods are often used  and are empirically observed
to yield good results [8]. One particularly common and eﬀective method is to predict links
to nodes that share many “common neighbors” with the query node q  i.e.  rank nodes
by |CN(q  i)|  where CN(q  i) = {u | q ∼ u ∼ i} (i ∼ j represents an edge between i

1

and j). The intuition is that q probably has many links with others in its cluster  and
hence probably also shares many common friends with others in its cluster. Counting com-
mon neighbors is particularly fast (it is a Join operation supported by all databases and
Map-Reduce systems). In this paper  we study the theoretical properties of the common
neighbors heuristic.
Our contributions are the following:
(a) We present  to our knowledge the ﬁrst  theoretical analysis of the common neighbors for
the stochastic blockmodel.
(b) We demarcate two regimes  which we call semi-dense and semi-sparse  under which
common neighbors can be successfully used for both link prediction and clustering.
(c) In particular  in the semi-dense regime  the number of common neighbors between the
query node q and another node within its cluster is signiﬁcantly higher than that with a
node outside its cluster. Hence  a simple threshold on the number of common neighbors
suﬃces for both link prediction and clustering.
(d) However  in the semi-sparse regime  there are too few common neighbors with any node 
and hence the heuristic does not work. However  we show that with a simple additional
“cleaning” step  we regain the theoretical properties shown for the semi-dense case.
(e) We empirically demonstrate the eﬀectiveness of counting common neighbors followed by
the “cleaning” post-process on a variety of simulated and real-world datasets.

2 Related Work

Link prediction has recently attracted a lot of attention  because of its relevance to important
practical problems like recommendation systems  predicting future connections in friendship
networks  better understanding of evolution of complex networks  study of missing or partial
information in networks  etc [9  8]. Algorithms for link prediction fall into two main groups:
similarity-based  and model-based.
Similarity-based methods: These methods use similarity measures based on network
topology for link prediction. Some methods look at nodes two hops away from the query
node: counting common neighbors  the Jaccard index  the Adamic-Adar score [1] etc. More
complex methods include nodes farther away  such as the Katz score [7]  and methods based
on random walks [16  2]. These are often intuitive  easily implemented  and fast  but they
typically lack theoretical guarantees.
Model-based methods: The second approach estimates parametric models for predict-
ing links. Many popular network models fall in the latent variable model category [12  3].
These models assign n latent random variables Z := (Z1  Z2  . . .   Zn) to n nodes in a net-
work. These variables take values in a general space Z. The probability of linkage between
two nodes is speciﬁed via a symmetric map h : Z × Z → [0  1]. These Zi’s can be i.i.d
Uniform(0 1) [3]  or positions in some d−dimensional latent space [12]. In [5] a mixture of
multivariate Gaussian distributions is used  each for a separate cluster. A Stochastic Block-
model [6] is a special class of these models  where Zi is a binary length k vector encoding
membership of a node in a cluster.
In a well known special case (the planted partition
model)  all nodes in the same cluster connect to each other with probability α  whereas
all pairs in diﬀerent clusters connect with probability γ. In fact  under broad parameter
regimes  the blockmodel approximation of networks has recently been shown to be analogous
to the use of histograms as non-parametric summaries of an unknown probability distribu-
tion [11]. Varying the number of bins or the bandwidth corresponds to varying the number
or size of communities. Thus blockmodels can be used to approximate more complex models
(under broad smoothness conditions) if the number of blocks are allowed to increase with
n.
Empirical results: As the models become more complex  they also become computation-
ally demanding. It has been commonly observed that simple and easily computable measures
like common neighbors often have competitive performance with more complex methods.

2

This behavior has been empirically established across a variety of networks  starting from
co-authorship networks [8] to router level internet connections  protein protein interaction
networks and electrical power grid network [9].
Theoretical results: Spectral clustering has been shown to asymptotically recover cluster
memberships for variations of Stochastic Blockmodels [10  4  13]. However  apart from [15] 
there is little understanding of why simple methods such as common neighbors perform so
well empirically.
Given their empirical success and computational tractability  the common neighbors heuris-
tic is widely applied for large networks. Understanding the reasons for the accuracy of
common neighbors under the popular stochastic blockmodel setting is the goal of our work.

3 Proposed Work

Many link prediction methods ultimately make two assumptions: (a) each node belongs to
a latent “cluster”  where nodes in the same cluster have similar behavior; and (b) each node
is very likely to connect to others in its cluster  so link prediction is equivalent to ﬁnding
other nodes in the cluster. These assumptions can be relaxed: instead of belonging to the
same cluster  nodes could have “topic distributions”  with links being more likely between
pairs of nodes with similar topical interests. However  we will focus on the assumptions
stated above  since they are clean and the relaxations appear to be fundamentally similar.

Model. Speciﬁcally  consider a stochastic blockmodel where each node i belongs to an
unknown cluster ci ∈ {C1  . . .   CK}. We assume that the number of clusters K is ﬁxed as
the number of nodes n increases. We also assume that each cluster has π = n/K members 
though this can be relaxed easily. The probability P(i ∼ j) of a link between nodes i and j
(i 6= j) depends only on the clusters of i and j: P(i ∼ j) = Bci cj
(cid:44) α{ci = cj} + γ{ci 6= cj}
for some α > γ > 0; in other words  the probability of a link is α between nodes in the same
cluster  and γ otherwise. By deﬁnition  P(i ∼ i) = 0. If the nodes were arranged so that all
nodes in a cluster are contiguous  then the corresponding matrix  when plotted  attains a
block-like structure  with the diagonal blocks (corresponding to links within a cluster) being
denser than oﬀ-diagonal blocks (since α > γ).
Under these assumptions  we ask the following two questions:
Problem 1 (Link Prediction and Recommendation). Given node i  how can we identify at
least a constant number of nodes from ci?
Problem 2 (Local Cluster Detection). Given node i  how can we identify all nodes in ci?
Problem 1 can be considered as the problem of ﬁnding good recommendations for a given
node i. Here  the goal is to ﬁnd a few good nodes that i could connect to (e.g.  recommending
a few possible friends on Facebook  or a few movies to watch next on Netﬂix). Since within-
cluster links have higher probability than across-cluster links (α > γ)  predicting nodes from
ci gives the optimal answer. Crucially  it is unnecessary to ﬁnd all good nodes. As against
that  Problem 2 requires us to ﬁnd everyone in the given node’s cluster. This is the problem
of detecting the entire cluster corresponding to a given node. Note that Problem 2 is clearly
harder than Problem 1.
We next present a summary of our results and the underlying intuition before delving into
the details.

Intuition and Result Summary

3.1
Current approaches. Standard approaches to inference for the stochastic blockmodel
attempt to solve an even harder problem:
Problem 3 (Full Cluster Detection). How can we identify the latent clusters ci for all i?
A popular solution is via spectral clustering  involving two steps: (a) computing the top-K
eigenvectors of the graph Laplacian  and (b) clustering the projections of each node on the

3

√

corresponding eigenspace via an algorithm like k-means [13]. A slight variation of this has
been shown to work as long as (α − γ)/
n) and the average degree grows
faster than poly-logarithmic powers of n [10].
However  (a) spectral clustering solves a harder problem than Problems 1 and 2  and (b)
eigen-decompositions can be expensive  particularly for very large graphs. Our claim is that
a simpler operation — counting common neighbors between nodes — can yield results that
are almost as good in a broad parameter regime.

α = Ω(log n/

√

Common neighbors. Given a node i  link prediction via common neighbors follows a
simple prescription: predict a link to node j such that i and j have the maximum number
|CN(i  j)| of shared friends CN(i  j) = {u | i ∼ u ∼ j}. The usefulness of common
neighbors have been observed in practice [8] and justiﬁed theoretically for the latent distance
model [15]. However  its properties under the stochastic blockmodel remained unknown.
Intuitively  we would expect a pair of nodes i and j from the same cluster to have many
common neighbors u from the same cluster  since both the links i ∼ u and u ∼ j occur with
probability α  whereas for ci 6= cj  at least one of the edges i ∼ u and u ∼ j must have the
lower probability γ.
P(u ∈ CN(i  j) | ci = cj) = α2P(cu = ci | ci = cj) + γ2P(cu 6= ci | ci = cj)

= πα2 + (1 − π)γ2

P(u ∈ CN(i  j) | ci 6= cj) = αγP(cu = ci or cu = cj | ci 6= cj) + γ2P(cu 6= ci  cu 6= cj | ci 6= cj)

= 2παγ + (1 − 2π)γ2 = P(u ∈ CN(i  j) | ci = cj) − π(α − γ)2
≤ P(u ∈ CN(i  j) | ci = cj)

Thus the expected number of common neighbors E [|CN(i  j)|] is higher when ci = cj. If
we can show that the random variable CN(i  j) concentrates around its expectation  node
pairs with the most common neighbors would belong to the same cluster. Thus  common
neighbors would oﬀer a good solution to Problem 1.
We show conditions under which this is indeed the case. There are three key points regarding
our method: (a) handling dependencies between common neighbor counts  (b) deﬁning the
graph density regime under which common neighbors is consistent  and (c) proposing a
variant of common neighbors which signiﬁcantly broadens this region of consistency.

Dependence. CN(i  j) and CN(i  j0) are dependent; hence  distinguishing between
within-group and outside-group nodes can be complicated even if each CN(i  j) concen-
trates around its expectation. We handle this via a careful conditioning step.

Dense versus sparse graphs.
In general  the parameters α and γ can be functions of
n  and we can try to characterize parameter settings when common neighbors consistently
returns nodes from the same cluster as the input node. We show that when the graph is
suﬃciently “dense” (average degree is growing faster than
n log n)  common neighbors is
powerful enough to answer Problem 2. Also  (α − γ)/α can go to zero at a suitable rate.
On the other hand  the expected number of common neighbors between nodes tends to
zero for sparser graphs  irrespective of whether the nodes are in the same cluster or not.
Further  the standard deviation is of a higher order than the expectation  so there is no
concentration. In this case  counting common neighbors fails  even for Problem 1.

√

A variant with better consistency properties. However  we show that the addition
of an extra post-processing step (henceforth  the “cleaning” step) still enables common
neighbors to identify nodes from its own cluster  while reducing the number of oﬀ-cluster
nodes to zero with probability tending to one as n → ∞. This requires a stronger separation
condition between α and γ. However  such “strong consistency” is only possible when
the average degree grows faster than (n log n)1/3. Thus  the cleaning step extends the
consistency of common neighbors beyond the O(1/

n) range.

√

4

4 Main Results

We ﬁrst split the edge set of the complete graph on n nodes into two sets: K1 and its
complement K2 (independent of the given graph G). We compute common neighbors on
G1 = G ∩ K1 and perform a “cleaning” process on G2 = G ∩ K2. The adjacency matrices
of G1 and G2 are denoted by A1 and A2. We will ﬁx a reference node q  which belongs to
class C1 without loss of generality (recall that there are K clusters C1 . . . CK  each of size
nπ).
Let Xi(i 6= q) denote the number of common neighbors between q and i. Algorithm 1
computes the set S = {i : Xi ≥ tn} of nodes who have at least tn common neighbors with
q on A1  whereas Algorithm 2 does a further degree thresholding on A2 to reﬁne S into S1.

Algorithm 1 Common neighbors screening algorithm
1: procedure Scan(A1  q  tn)
For 1 ≤ i ≤ n  Xi ← A2
2:
Xq ← 0
3:
S ← {i : Xi ≥ tn}
4:
return S
5:

1(q  i)

S1 ← {i :P

Algorithm 2 Post Selection Cleaning algorithm
1: procedure Clean(S  A2  q  sn)
j∈S A2(i  j) ≥ sn}
2:
3:

return S1

To analyze the algorithms  we must specify conditions on graph densities. Recall that α
and γ represent within-cluster and across-cluster link probabilities. We assume that α/γ
is constant while α → 0  γ → 0; equivalently  assume that both α and γ are both some
constant times ρ  where ρ → 0.
The analysis of graphs has typically been divided into two regimes. The dense regime
consists of graphs with nρ → ∞  where the expected degree nρ is a fraction of n as n grows.
In the sparse regime  nρ = O(1)  so degree is roughly constant. Our work explores a ﬁner
gradation  which we call semi-dense and semi-sparse  deﬁned next.
Deﬁnition 4.1 (Semi-dense graph). A sequence of graphs is called semi-dense if
nρ2/ log n → ∞ as n → ∞.
Deﬁnition 4.2 (Semi-sparse graph). A sequence of graphs is called semi-sparse if nρ2 → 0
but n2/3ρ/ log n → ∞ as n → ∞.
Our ﬁrst result is that common neighbors is enough to solve not only the link-prediction
problem (Problem 1) but also the local clustering problem (Problem 2) in the semi-dense
case. This is because even though both nodes within and outside the query node’s cluster
have a growing number of common neighbors with q  there is a clear distinction in the
expected number of common neighbors between the two classes. Also  since the standard
deviation is of a smaller order than the expectation  the random variables concentrate.
Thus  we can pick a threshold tn such that SCAN(A1  q  tn) yields just the nodes in the
same cluster as q with high probability. Note that the cleaning step (Algorithm 2) is not
necessary in this case.
Theorem 4.1 (Algorithm 1 solves Problem 2 in semi-dense graphs). Let
tn =
nw and no denote the number of nodes in S ∩ C1 and S \ C1 respectively. If the graph is
semi-dense  and if α−γ

n(cid:0)π(α + γ)2/2 + (1 − 2π)γ2(cid:1). Let S be the set of nodes returned by SCAN(A1  q  tn). Let

  then P(nw = nπ) → 1 and P(no = 0) → 1.

(cid:16) log n

(cid:17)1/4

α ≥ 2√

π

nα2

rial. Let dqa = P

Proof Sketch. We only sketch the proof here  deferring details to the supplementary mate-
A1(q  i) be the number of links from the query node q to nodes in

i∈Ca

5

cluster Ca. Let dq = {dq1  . . . qqK} and d =P

P(dq ∈ Good) (cid:44) P

ψn (cid:44)p(6 log n)/(nπγ) =

a dqa. We ﬁrst show that

(cid:18) dq1 ∈ nπα(1 ± ψn)
qplog n/n · Θ(plog n/(nρ2)) → 0.

dqa ∈ nπγ(1 ± ψn) ∀a 6= 1

(cid:19)

≥ 1 − K
n2  

(1)

(2)
Conditioned on dq  Xi is the sum of K Binomial(dqa  B1a) independent random variables
representing the number of common neighbors between q and i via nodes in each of the K
clusters: E[Xi | dq  i ∈ Ca] = dqaα + (d − dqa)γ. We have:

η1 (cid:44) E[Xi | dq ∈ Good  i ∈ C1] ≥ n(cid:0)πα2 + (1 − π)γ2(cid:1) (1 − ψn) (cid:44) ‘n(1 − ψn)
ηa (cid:44) E[Xi | dq ∈ Good  i ∈ Ca  a 6= 1] ≤ n(cid:0)2παγ + (1 − 2π)γ2(cid:1) (1 + ψn) (cid:44) un(1 + ψn)
Note that tn = (‘n+un)/2  un ≤ tn ≤ ‘n  and ‘n−un = nπ(α−γ)2 ≥ 4 log npnα2/ log n →

∞  where we applied condition on (α − γ)/α noted in the theorem statement. We show:

P (Xi ≤ tn | dq ∈ Good  i ∈ C1) ≤ n−4/3+o(1)
P (Xi ≥ tn | dq ∈ Good  i ∈ Ca  a 6= 1) ≤ n−4/3+o(1)

Conditioned on dq  both nw and no are sums of conditionally independent and identically
distributed Bernoullis.
P(nw = nπ) ≥ P(dq ∈ Good)P(nw = nπ | dq ∈ Good) ≥
P(no = 0) ≥ P(dq ∈ Good) · P(no = 0 | dq ∈ Good) ≥ 1 − Θ(n−1/3) → 1

· (1 − nπ · n−4/3) → 1

1 − K
n2

(cid:18)

(cid:19)

There are two major diﬀerences between the semi-sparse and semi-dense cases. First  in the
semi-sparse case  both expectations η1 and ηa are of the order O(nρ2) which tends to zero.
Second  standard deviations on the number of common neighbors are of a larger order than
expectations. Together  this means that the number of common neighbors to within-cluster
and outside-cluster nodes can no longer be separated; hence  Algorithm 1 by itself cannot
work. However  after cleaning  the entire cluster of the query node q can still be recovered.
Theorem 4.2 (Algorithm 1 followed by Algorithm 2 solves Problem 2 in semi-sparse
graphs). Let tn = 1 and sn = n2 (πα + (1 − π)γ)2 (α + γ)/2. Let S = Scan(A1  q  tn)
and S1 = Clean(S  A2  q  sn). Let n
(S1 \ C1). If the graph is semi-sparse  and πα ≥ 3(1 − π)γ  then P
P

(cid:17) denote the number of nodes in S1 ∩ C1
(cid:17) → 1 and

o = 0(cid:17) → 1.

(c)
w = nπ

(cid:16)

(cid:16)

(cid:16)

(c)
o

n

(c)

n

(c)
w

n

Proof Sketch. We only sketch the proof here  with details being deferred to the supplemen-
tary material. The degree bounds of Eq. 1 and the equations for E[Xi|dq ∈ Good] hold
even in the semi-sparse case. We can also bound the variances of Xi (which are sums of
conditionally independent Bernoullis):

var[Xi | dq ∈ Good  i ∈ C1] ≤ E[Xi | dq ∈ Good  i ∈ C1] = η1

Since the expected number of common neighbors vanishes and the standard deviation is an
order larger than the expectation  there is no hope for concentration; however  there are
slight diﬀerences in the probability of having at least one common neighbor.
First  by an application of the Paley-Zygmund inequality  we ﬁnd:

p1 (cid:44) P(Xi ≥ 1 | dq ∈ Good  i ∈ C1)

E[Xi | dq ∈ Good  i ∈ C1]2

var(Xi | dq ∈ Good  i ∈ C1) + E[Xi | dq ∈ Good  i ∈ C1]2

≥
≥ η2

1

η1 + η2
1

≥ ‘n(1 − ψn)(1 − η1)

since η1 → 0

6

For a > 1  Markov’s inequality gives:

pa (cid:44) P(Xi ≥ 1 | dq ∈ Good  i ∈ Ca  a 6= 1) ≤ E[Xi | dq ∈ Good  i ∈ Ca  a 6= 1] = ηa

Even though pa → 0  nπpa = Θ(n2ρ2) → ∞  so we can use concentration inequalities like
the Chernoﬀ bound again to bound nw and no.

P(nw ≥ nπp1(1 −p6 log n/nπp1)) ≥ 1 − n−4/3
P(no ≤ n(1 − π)pa(1 +p6 log n/n(1 − π)pa)) ≥ 1 − n−4/3

Unlike the denser regime  nw and no can be of the same order here. Hence  the candidate
set S returned by thresholding the common neighbors has a non-vanishing fraction of nodes
from outside q’s community. However  this fraction is relatively small  which is what we
would exploit in the cleaning step.
Let θw and θo denote the expected number of edges in A2 from a node to S. The separation
√
condition in the theorem statement gives θw − θo ≥ 4
θw log n. Setting the degree threshold
sn = (θw + θo)/2  we bound the probability of mistakes in the cleaning step:
A2(i  j) ≤ sn | dq ∈ Good) ≤ n−1/3+o(1)

P(∃i ∈ C1 s.t. X
P(∃i 6∈ C1 s.t. X

j∈S

A2(i  j) ≥ sn | dq ∈ Good) ≤ n−1/3+o(1)

j∈S

Removing the conditioning on dq ∈ Good (as in Theorem 4.1) yields the desired result.

5 Experiments

We present our experimental results in two parts. First  we use simulations to support our
theoretical claims. Next we present link prediction accuracies on real world collaborative
networks to show that common neighbors indeed perform close to gold standard algorithms
like spectral clustering and the Katz score.
Implementation details: Recall that our algorithms are based on thresholding. When
there is a large gap between common neighbors between node q and nodes in its cluster (e.g. 
in the semi-dense regime)  this is equivalent to using the k-means algorithm with k = 2 to
ﬁnd S in Algorithm 1. The same holds for ﬁnding S1 in algorithm 2. When the number
of nodes with more than two common neighbors is less than ten  we deﬁne the set S by
ﬁnding all neighbors with at least one common neighbor (as in the semi-sparse regime).
On the other hand  since the cleaning step works only when S is suﬃciently large (so that
degrees concentrate)  we do not perform any cleaning when |S| < 30. While we used the
split sample graph A2 in the cleaning step for ease of analysis  we did the cleaning using
the same network in the experiments.
Experimental setup for simulations: We use a stochastic blockmodel of 2000 nodes split
into 4 equal-sized clusters. For each value of (α  γ) we pick 50 query nodes at random  and
calculate the precision and recall of the result against nodes from the query node’s cluster
(for any subset S and true cluster C  precision = |S ∩ C|/|S| and recall = |S ∩ C|/|C|). We
report mean precision and recall over 50 random generated graph instances.
Accuracy on simulated data: Figure 1 shows the precision and recall as degree grows 
with the parameters (α  γ) satisfying the condition πα ≥ 3(1 − π)γ of Thm. 4.2. We see
that cleaning helps both precision and recall  particularly in the medium-degree range (the
semi-sparse regime). As a reference  we also plot the precision of spectral clustering  when
it was given the correct number of clusters (K = 4). Above average degree of 10  spectral
clustering gives perfect precision  whereas common neighbors can identify a large fraction of
the true cluster once average degree is above 25. On the other hand  for average degree less
than seven  spectral clustering performs poorly  whereas the precision of common neighbors
is remarkably higher. Precision is relatively higher than recall for a broad degree regime 
and this explains why common neighbors are a popular choice for link prediction. On a side

7

(A)

(B)

Figure 1: Recall and precision versus average degree: When degree is very small  none of the
methods work well. In the medium-degree range (semi-sparse regime)  we see that common
neighbors gets increasingly better precision and recall  and cleaning helps. With high enough
degrees (semi-dense regime)  just common neighbors is suﬃcient and gets excellent accuracy.

Dataset

n

HepTH 5969
Citeseer
4520
1222
NIPS

Table 1: AUC scores for co-authorship networks
AUC
Mean degree Time-steps
SPEC Katz Random
.82
.89
.68

CN CN-clean
.70
.88
.63

4
5
3.95

6
11
9

.74
.89
.69

.82
.95
.78

.49
.52
.47

note  it is not surprising that in a very sparse graph common neighbors cannot identify the
whole cluster  since not everyone can be reached in two hops.
Accuracy on real-world data: We used publicly available co-authorship datasets over
time where nodes represent authors and an edge represents a collaboration between two
authors.
In particular  we used subgraphs of the High Energy Physics (HepTH) co-
authorship dataset (6 timesteps)  the NIPS dataset (9 timesteps) and the Citeseer dataset
(11 timesteps). We obtain the training graph by merging the ﬁrst T-2 networks  use the
T-1th step for cross-validation and use the last timestep as the test graph. The number of
nodes and average degrees are reported in Table 1. We merged 1-2 years of papers to create
one timestep (so that the median degree of the test graph is at least 1).
We compare our algorithm (CN and CN-clean) with the Katz score which is used widely
in link prediction [8] and spectral clustering of the network. Spectral clustering is carried
out on the giant component of the network. Furthermore  we cross-validate the number of
clusters using the held out graph. Our setup is very similar to link prediction experiments
in related literature [14].
Since these datasets are unlabeled  we cannot calculate precision or recall as before. Instead
for any score or aﬃnity measure  we propose to perform link prediction experiments as
follows. For a randomly picked node we calculate the score from the node to everyone else.
We compute the AUC score of this vector against the edges in the test graph. We report
the average AUC for 100 randomly picked nodes. Table 1 shows that even in sparse regimes
common neighbors performs similar to benchmark algorithms.
6 Conclusions

Counting common neighbors is a particularly useful heuristic:
it is fast and also works
well empirically. We prove the eﬀectiveness of common neighbors for link prediction as
well as local clustering around a query node  under the stochastic blockmodel setting. In
particular  we show the existence of a semi-dense regime where common neighbors yields
the right cluster w.h.p  and a semi-sparse regime where an additional “cleaning” step is
required. Experiments with simulated as well as real-world datasets shows the eﬃcacy of
our approach  including the importance of the cleaning step.

8

References
[1] L. Adamic and E. Adar. Friends and neighbors on the web. Social Networks  25:211–230 

2003.

[2] L. Backstrom and J. Leskovec. Supervised random walks: Predicting and recommend-
ing links in social networks. In Proceedings of the Fourth ACM International Conference
on Web Search and Data Mining  pages 635–644  New York  NY  USA  2011. ACM.

[3] P. J. Bickel and A. Chen. A nonparametric view of network models and newman girvan
and other modularities. Proceedings of the National Academy of Sciences of the Unites
States of America  106(50):21068Ű21073  2009.

[4] K. Chaudhuri  F. C. Graham  and A. Tsiatas. Spectral clustering of graphs with general
degrees in the extended planted partition model. Journal of Machine Learning Research
- Proceedings Track  23:35.1–35.23  2012.

[5] M. S. Handcock  A. E. Raftery  and J. M. Tantrum. Model-based clustering for social
networks. Journal of the Royal Statistical Society: Series A (Statistics in Society) 
170(2):301–354  2007.

[6] P. W. Holland  K. Laskey  and S. Leinhardt. Stochastic blockmodels: First steps. Social

Networks  5(2):109–137  1983.

[7] L. Katz. A new status index derived from sociometric analysis.

volume 18  pages 39–43  1953.

In Psychometrika 

[8] D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In

Conference on Information and Knowledge Management. ACM  2003.

[9] L. Lü and T. Zhou. Link prediction in complex networks: A survey. Physica A 

390(6):11501170  2011.

[10] F. McSherry. Spectral partitioning of random graphs. In FOCS  pages 529–537  2001.
[11] S. C. Olhede and P. J. Wolfe. Network histograms and universality of blockmodel
approximation. Proceedings of the National Academy of Sciences of the Unites States
of America  111(41):14722–14727  2014.

[12] A. E. Raftery  M. S. Handcock  and P. D. Hoﬀ. Latent space approaches to social

network analysis. Journal of the American Statistical Association  15:460  2002.

[13] K. Rohe  S. Chatterjee  and B. Yu. Spectral clustering and the high-dimensional

stochastic blockmodel. Annals of Statistics  39:1878–1915  2011.

[14] P. Sarkar and P. J. Bickel. Role of normalization in spectral clustering for stochastic

blockmodels. To appear in the Annals of Statistics.  2014.

[15] P. Sarkar  D. Chakrabarti  and A. Moore. Theoretical justiﬁcation of popular link

prediction heuristics. In Conference on Learning Theory. ACM  2010.

[16] P. Sarkar and A. Moore. A tractable approach to ﬁnding closest truncated-commute-

time neighbors in large graphs. In Proc. UAI  2007.

9

,Purnamrita Sarkar
Deepayan Chakrabarti
peter bickel