2019,Cormorant: Covariant Molecular Neural Networks,We propose Cormorant  a rotationally covariant neural network architecture for learning the behavior and properties of complex many-body physical systems. We apply these networks to molecular systems with two goals: learning atomic potential energy surfaces for use in Molecular Dynamics simulations  and learning ground state properties of molecules calculated by Density Functional Theory. Some of the key features of our network are that (a) each neuron explicitly corresponds to a subset of atoms; (b) the activation of each neuron is covariant to rotations  ensuring that overall the network is fully rotationally invariant. Furthermore  the non-linearity in our network is based upon tensor products and the Clebsch-Gordan decomposition  allowing the network to operate entirely in Fourier space. Cormorant significantly outperforms competing algorithms in learning molecular Potential Energy Surfaces from conformational geometries in the MD-17 dataset  and is competitive with other methods at learning geometric  energetic  electronic  and thermodynamic properties of molecules on the GDB-9 dataset.,Cormorant: Covariant Molecular Neural Networks

Brandon Anderson∗‡  Truong-Son Hy∗ and Risi Kondor∗†(cid:93)
∗Department of Computer Science  †Department of Statistics

The University of Chicago

(cid:93) Center for Computational Mathematics  Flatiron Institute

‡ Atomwise

{hytruongson risi}@uchicago.edu

brandona@jfi.uchicago.edu

Abstract

We propose Cormorant  a rotationally covariant neural network architecture for
learning the behavior and properties of complex many-body physical systems.
We apply these networks to molecular systems with two goals: learning atomic
potential energy surfaces for use in Molecular Dynamics simulations  and learn-
ing ground state properties of molecules calculated by Density Functional Theory.
Some of the key features of our network are that (a) each neuron explicitly corre-
sponds to a subset of atoms; (b) the activation of each neuron is covariant to rota-
tions  ensuring that overall the network is fully rotationally invariant. Furthermore 
the non-linearity in our network is based upon tensor products and the Clebsch-
Gordan decomposition  allowing the network to operate entirely in Fourier space.
Cormorant signiﬁcantly outperforms competing algorithms in learning molecular
Potential Energy Surfaces from conformational geometries in the MD-17 dataset 
and is competitive with other methods at learning geometric  energetic  electronic 
and thermodynamic properties of molecules on the GDB-9 dataset.

1

Introduction

In principle  quantum mechanics provides a perfect description of the forces governing the behavior
of atoms  molecules and crystalline materials such as metals. However  for systems larger than a
few dozen atoms  solving the Schrödinger equation explicitly at every timestep is not a feasible
proposition on present day computers. Even Density Functional Theory (DFT) [Hohenberg and
Kohn  1964]  a widely used approximation to the equations of quantum mechanics  has trouble
scaling to more than a few hundred atoms.
Consequently  the majority of practical work in molecular dynamics today falls back on fundamen-
tally classical models  where the atoms are essentially treated as solid balls and the forces between
them are given by pre-deﬁned formulae called atomic force ﬁelds or empirical potentials  such as
the CHARMM family of models [Brooks et al.  1983  2009]. There has been a widespread real-
ization that this approach has inherent limitations  so in recent years a burgeoning community has
formed around trying to use machine learning to learn more descriptive force ﬁelds directly from
DFT computations [Behler and Parrinello  2007  Bartók et al.  2010  Rupp et al.  2012  Shapeev 
2015  Chmiela et al.  2016  Zhang et al.  2018  Schütt et al.  2017  Hirn et al.  2017]. More broadly 
there is considerable interest in using ML methods not just for learning force ﬁelds  but also for
predicting many other physical/chemical properties of atomic systems across different branches of
materials science  chemistry and pharmacology [Montavon et al.  2013  Gilmer et al.  2017  Smith
et al.  2017  Yao et al.  2018].
At the same time  there have been signiﬁcant advances in our understanding of the equivariance
and covariance properties of neural networks  starting with [Cohen and Welling  2016a b] in the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

context of traditional convolutional neural nets (CNNs). Similar ideas underly generalizations of
CNNs to manifolds [Masci et al.  2015  Monti et al.  2016  Bronstein et al.  2017] and graphs [Bruna
et al.  2014  Henaff et al.  2015]. In the context of CNNs on the sphere  Cohen et al. [2018] real-
ized the advantage of using “Fourier space” activations  i.e.  expressing the activations of neurons
in a basis deﬁned by the irreducible representations of the underlying symmetry group (see also
[Esteves et al.  2017])  and these ideas were later generalized to the entire SE(3) group [Weiler
et al.  2018]. Kondor and Trivedi [2018] gave a complete characterization of what operations are
allowable in Fourier space neural networks to preserve covariance  and Cohen et al generalized the
framework even further to arbitrary gauge ﬁelds [Cohen et al.  2019]. There have also been some
recent works where even the nonlinear part of the neural network’s operation is performed in Fourier
space: independently of each other [Thomas et al.  2018] and [Kondor  2018] were to ﬁrst to use
the Clebsch–Gordan transform inside rotationally covariant neural networks for learning physical
systems  while [Kondor et al.  2018] showed that in spherical CNNs the Clebsch–Gordan transform
is sufﬁcient to serve as the sole source of nonlinearity.
The Cormorant neural network architecture proposed in the present paper combines some of the
insights gained from the various force ﬁeld and potential learning efforts with the emerging theory
of Fourier space covariant/equivariant neural networks. The important point that we stress in the
following pages is that by setting up the network in such a way that each neuron corresponds to
an actual set of physical atoms  and that each activation is covariant to symmetries (rotation and
translation)  we get a network in which the “laws” that individual neurons learn resemble known
physical interactions. Our experiments show that this generality pays off in terms of performance
on standard benchmark datasets.

2 The nature of physical interactions in molecules

Ultimately interactions in molecular systems arise from the quantum structure of electron clouds
around constituent atoms. However  from a chemical point of view  effective atom-atom interactions
break down into a few simple classes based upon symmetry. Here we review a few of these classes in
the context of the multipole expansion  whose structure will inform the design of our neural network.

Scalar interactions. The simplest type of physical interaction is that between two particles that
are pointlike and have no internal directional degrees of freedom  such as spin or dipole moments.
A classical example is the electrostatic attraction/repulsion between two charges described by the
Coulomb energy

Here qA and qB are the charges of the two particles  rA and rB are their position vectors  rAB =
rA− rB  and 0 is a universal constant. Note that this equation already reﬂects symmetries: the fact
that (1) only depends on the length of rAB and not its direction or the position vectors individually
guarantees that the potential is invariant under both translations and rotations.

Dipole/dipole interactions. One step up from the scalar case is the interaction between two
dipoles. In general  the electrostatic dipole moment of a set of N charged particles relative to their
center of mass r is just the ﬁrst moment of their position vectors weighted by their charges:

The dipole/dipole contribution to the electrostatic potential energy between two sets of particles A
and B separated by a vector rAB is then given by

(cid:20) µA · µB
|rAB|3 − 3

Vd/d =

1

4π0

(cid:21)

.

(µA · rAB)(µB · rAB)

|rAB|5

One reason why dipole/dipole interactions are indispensible for capturing the energetics of
molecules is that most chemical bonds are polarized. However  dipole/dipole interactions also occur
in other contexts  such as the interaction between the magnetic spins of electrons.

2

VC = − 1
4π0

qAqB
|rAB| .

N(cid:88)

i=1

µ =

qi(ri − r).

(1)

(2)

Quadropole/quadropole interactions. One more step up the multipole hierarchy is the interac-
tion between quadropole moments. In the electrostatic case  the quadropole moment is the second
moment of the charge density (corrected to remove the trace)  described by the matrix

N(cid:88)

i=1

Θ =

qi(3rir(cid:62)

i − |ri|2 I).

Quadropole/quadropole interactions appear for example when describing the interaction between
benzene rings  but the general formula for the corresponding potential is quite complicated. As a
simpliﬁcation  let us only consider the special case when in some coordinate system aligned with
the structure of A  and at polar angle (θA  φA) relative to the vector rAB connecting A and B  ΘA
can be transformed into a form that is diagonal  with [ΘA]zz = ϑA and [ΘA]xx = [ΘA]yy = −ϑA/2
[Stone  1997]. We make a similar assumption about the quadropole moment of B. In this case the
interaction energy becomes

Vq/q =

3
4

ϑAϑB

4π0 |rAB|5

A −5 cos2 θB − 15 cos2 θA cos2 θB+

2(4 cos θAθB − sin θA sin θB cos(φA− φB))2(cid:3).

(3)
Higher order interactions involve moment tensors of order 3 4 5  and so on. One can appreciate that
the corresponding formulae  especially when considering not just electrostatics but other types of
interactions as well (dispersion  exchange interaction  etc)  quickly become very involved.

(cid:2)1 − 5 cosθ

3 Spherical tensors and representation theory

Fortunately  there is an alternative formalism for expressing molecular interactions  that of spherical
tensors  which makes the general form of physically allowable interactions more transparent. This
formalism also forms the basis of the our Cormorant networks described in the next section.
The key to spherical tensors is understanding how physical quantities transform under rotations.
Speciﬁcally  in our case  under a rotation R:

q (cid:55)−→ q

Θ (cid:55)−→ R ΘR(cid:62)

rAB (cid:55)−→ R rAB.

µ (cid:55)−→ R µ
Flattening Θ into a vector Θ ∈ R9 
its transformation rule can equivalently be written as
Θ (cid:55)→ (R⊗ R) Θ  showing its similarity to the other three cases. In general  a k’th order Carte-
sian moment tensor T (k) ∈ R3×3×...×3 (or its ﬂattened T (k) ∈ R3k equivalent) transforms as
T (k) (cid:55)→ (R⊗ R⊗ . . . ⊗ R) T (k).
Recall that given a group G  a representation ρ of G is a matrix valued function ρ : G → Cd×d
obeying ρ(xy) = ρ(x)ρ(y) for any two group elements x  y ∈ G. It is easy to see that R  and
consequently R ⊗ . . . ⊗ R are representations of the three dimensional rotation group SO(3). We
also know that because SO(3) is a compact group  it has a countable sequence of unitary so-called
irreducible representations (irreps)  and  up to a similarity transformation  any representation can
be reduced to a direct sum of irreps. In the speciﬁc case of SO(3)  the irreps are called Wigner
D-matrices and for any positive integer (cid:96) = 0  1  2  . . . there is a single corresponding irrep D(cid:96)(R) 
which is a (2(cid:96) + 1) dimensional representation (i.e.  as a function  D(cid:96) : SO(3) → C(2(cid:96)+1)×(2(cid:96)+1)).
The (cid:96) = 0 irrep is the trivial irrep D0(R) = (1).
The above imply that there is a ﬁxed unitary transformation matrix C (k) which reduces the k’th
order rotation operator into a direct sum of irreducible representations:

R⊗ R⊗ . . . ⊗ R

(cid:124)

(cid:123)(cid:122)

k

= C (k)(cid:104)(cid:77)

τ(cid:96)(cid:77)

(cid:96)

i=1

(cid:125)

(cid:105)

D(cid:96)(R)

C (k)†

.

Note that the transformation R⊗ R⊗ . . . ⊗ R contains redundant copies of D(cid:96)(R)  which we de-
note as the multiplicites τ(cid:96). For our present purposes knowing the actual values of the τ(cid:96) is not that
important  except that τk = 1 and that for any (cid:96) > k  τ(cid:96) = 0. What is important is that T (k)  the
vectorized form of the Cartesian moment tensor has a corresponding decomposition

T (k) = C (k)(cid:104)(cid:77)

τ(cid:96)(cid:77)

(cid:105)

Q(cid:96) i

.

(4)

(cid:96)

i=1

3

This is nice  because using the unitarity of Q(cid:96)i  it shows that under rotations the individual Q(cid:96) i
components transform independently as Q(cid:96) i (cid:55)→ D(cid:96)(R) Q(cid:96) i.
What we have just described is a form of generalized Fourier analysis applied to the transforma-
tion of Cartesian tensors under rotations. For the electrostatic multipole problem it is particularly
relevant  because it turns out that in that case  due to symmetries of T (k)  the only nonzero Q(cid:96) i com-
ponent of (4) is the single one with (cid:96) = k. Furthermore  for a set of N charged particles (indexing
its components −(cid:96)  . . .   (cid:96)) Q(cid:96) has the simple form

[Q(cid:96)]m =

qi (ri)(cid:96) Y m

(cid:96) (θi  φi)

m = −(cid:96)  . . .   (cid:96) 

(5)

(cid:18) 4π

(cid:19)1/2 N(cid:88)

2(cid:96) + 1

i=1

where (ri  θi  φi) are the coordinates of the i’th particle in spherical polars  and the Y m
(cid:96) (θ  φ) are
the well known spherical harmonic functions. Q(cid:96) is called the (cid:96)’th spherical moment of the charge
distribution. Note that while T ((cid:96)) and Q(cid:96) convey exactly the same information  T ((cid:96)) is a tensor with
3(cid:96) components  while Q(cid:96) is just a (2(cid:96) + 1) dimensional vector.
Somewhat confusingly  in physics and chemistry any quantity U that transforms under rotations as
U (cid:55)→ D(cid:96)(R) U is often called an ((cid:96)’th order) spherical tensor  despite the fact that in terms of its
presentation Q(cid:96) is just a vector of 2(cid:96) + 1 numbers. Also note that since D0(R) = (1)  a zeroth
order spherical tensor is just a scalar. A ﬁrst order spherical tensor  on the other hand  can be used
to represent a spatial vector r = (r  θ  φ) by setting [U1]m = r Y m

1 (θ  φ).

3.1 The general form of interactions

The beneﬁt of the spherical tensor formalism is that it makes it very clear how each part of a given

physical equation transforms under rotations. For example  if Q(cid:96) and (cid:101)Q(cid:96) are two (cid:96)’th order spherical
(cid:96)(cid:101)Q(cid:96) is a scalar  since under a rotation R  by the unitarity of the Wigner D-matrices 
(cid:96)(cid:101)Q(cid:96) (cid:55)−→ (D(cid:96)(R) Q(cid:96))† (D(cid:96)(R)(cid:101)Q(cid:96)) = Q

(cid:96) (D(cid:96)(R))† D(cid:96)(R) (cid:101)Q(cid:96) = Q
(cid:96)(cid:101)Q(cid:96).

†
tensors  then Q
†
Q

†

†

Even the dipole/dipole interaction (2) requires a more sophisticated way of coupling spherical ten-
sors than this  since it involves non-trivial interactions between not just two  but three different quan-
tites: the two dipole moments µA and µB and the the relative position vector rAB. Representing
interactions of this type requires taking tensor products of the constituent variables. For example 
in the dipole/dipole case we need terms of the form QA
. Naturally  these will transform
(cid:96)1
according to the tensor product of the corresponding irreps:

⊗ QB

(cid:96)2

⊗ QB

(cid:96)2

QA
(cid:96)1

(cid:55)→ (D(cid:96)1(R)⊗ D(cid:96)2(R)) (QA

⊗ QB

(cid:96)2

).

(cid:96)1

In general  D(cid:96)1(R)⊗ D(cid:96)2(R) is not an irreducible representation. However it does have a well
studied decomposition into irreducibles  called the Clebsch–Gordan decomposition:

D(cid:96)1(R)⊗ D(cid:96)2(R) = C

†
(cid:96)1 (cid:96)2

D(cid:96)(R)

C(cid:96)1 (cid:96)2 .

(cid:20) (cid:96)1+(cid:96)2(cid:77)

(cid:96)=|(cid:96)1−(cid:96)2|

(cid:21)

Letting C(cid:96)1 (cid:96)2 (cid:96) ∈ C(2(cid:96)+1)×(2(cid:96)1+1)(2(cid:96)2+2) be the block of 2(cid:96) + 1 rows in C(cid:96)1 (cid:96)2 corresponding to the
(cid:96) component of the direct sum  we see that C(cid:96)1 (cid:96)2 (cid:96)(QA
) is an (cid:96)’th order spherical tensor. In
(cid:96)1
particular  given some other spherical tensor quantity U(cid:96) 
†
(cid:96) · C(cid:96)1 (cid:96)2 (cid:96) · (QA
U

⊗ QB

⊗ QB

(cid:96)2

)

(cid:96)2

(cid:96)1

is a scalar  and hence it is a candidate for being a term in the potential energy. Note the similarity
of this expression to the bispectrum [Kakarala  1992  Bendory et al.  2018]  which is an already
established tool in the force ﬁeld learning literature [Bartók et al.  2013].
Almost any rotation invariant interaction potential can be expressed in terms of iterated Clebsch–
Gordan products between spherical tensors. In particular  the full electrostatic energy between two
sets of charges A and B separated by a vector r = (r  θ  φ) expressed in multipole form [Jackson 
1999] is

∞(cid:88)

∞(cid:88)

(cid:115)(cid:18)2(cid:96) + 2(cid:96)(cid:48)

(cid:19)(cid:114) 4π

(cid:96)=0

(cid:96)(cid:48)=0

2(cid:96)

VAB =

1

4π0

2(cid:96) + 2(cid:96)(cid:48) + 1

r−((cid:96)+(cid:96)(cid:48)+1) Y(cid:96)+(cid:96)(cid:48)(θ  φ) C(cid:96)1 (cid:96)2 (cid:96)+(cid:96)(cid:48) (QA

(cid:96) ⊗ QB
(cid:96)(cid:48) ).
(6)

4

Note the generality of this formula: the (cid:96) = (cid:96)(cid:48) = 1 case covers the dipole/dipole interaction (2) 
the (cid:96) = (cid:96)(cid:48) = 2 case covers the quadropole/quadropole interaction (3)  while the other terms cover
every other possible type of multipole/multipole interaction. Magnetic and other types of interac-
tions  including interactions that involve 3-way or higher order terms  can also be recovered from
appropriate combinations of tensor products and Clebsch–Gordan decompositions.
We emphasize that our discussion of electrostatics is only intended to illustrate the algebraic struc-
ture of interatomic interactions of any type  and is not restricted to electrostatics. In what follows  we
will not explicitly specify what interactions the network will learn. Nevertheless  there are physical
constraints on the interactions arising from symmetries  which we explicitly impose in our design of
Cormorant.

4 CORMORANT: COvaRiant MOleculaR Artiﬁcial Neural neTworks

The goal of using ML in molecular problems is not to encode known physical laws  but to provide
a platform for learning interactions from data that cannot easily be captured in a simple formula.
Nonetheless  the mathematical structure of known physical laws  like those discussed in the previ-
ous sections  give strong hints about how to represent physical interactions in algorithms. In partic-
ular  when using machine learning to learn molecular potentials or similar rotation and translation
invariant physical quantities  it is essential to make sure that the algorithm respects these invariances.
Our Cormorant neural network has invariance to rotations baked into its architecture in a way that
is similar to the physical equations of the previous section: the internal activations are all spherical
tensors  which are then combined at the top of the network in such a way as to guarantee that the
ﬁnal output is a scalar (i.e.  is invariant). However  to allow the network to learn interactions that are
more complicated than classical interatomic forces  we allow each neuron to output not just a single
spherical tensor  but a combination of spherical tensors of different orders. We will call an object
consisting of τ0 scalar components  τ1 components transforming as ﬁrst order spherical tensors  τ2
components transforming as second order spherical tensors  and so on  an SO(3)–covariant vector
of type (τ0  τ1  τ2  . . .). The output of each neuron in Cormorant is an SO(3)–vector of a ﬁxed type.
Deﬁnition 1. We say that F is an SO(3)-covariant vector of type τ = (τ0  τ1  τ2  . . .   τL) if it can
be written as a collection of complex matrices F0  F1  . . .   FL  called its isotypic parts  where each
F(cid:96) is a matrix of size (2(cid:96) + 1)× τ(cid:96) and transforms under rotations as F(cid:96) (cid:55)→ D(cid:96)(R) F(cid:96).
The second important feature of our architecture is that each neuron corresponds to either a single
atom or a set of atoms forming a physically meaningful subset of the system at hand  for example
all atoms in a ball of a given radius. This condition helps encourage the network to learn physically
meaningful and interpretable interactions. The high level deﬁnition of Cormorant nets is as follows.
Deﬁnition 2. Let S be a molecule or other physical system consisting of N atoms. A “Cormorant”
covariant molecular neural network for S is a feed forward neural network consisting of m neurons
n1  . . .   nm  such that
C1. Every neuron ni corresponds to some subset Si of the atoms. In particular  each input neuron

corresponds to a single atom. Each output neuron corresponds to the entire system S.

C2. The activation of each ni is an SO(3)-vector of a ﬁxed type τi.
C3. The type of each output neuron is τout = (1)  i.e.  a scalar. 1

Condition (C3) guarantees that whatever function a Cormorant network learns will be invariant to
global rotations. Translation invariance is easier to enforce simply by making sure that the interac-
tions represented by individual neurons only involve relative distances.

4.1 Covariant neurons

The neurons in our network must be such that if each of their inputs is an SO(3)–covariant vector
then so is their output. Classically  neurons perform a simple linear operation such as x (cid:55)→ W x + b 
followed by a nonlinearity like a ReLU. In convolutional neural nets the weights are tied together in

1Cormorant can learn data of arbitrary SO(3)-vector outputs. We restrict to scalars here to simplify the

exposition.

5

a speciﬁc way which guarantees that the activation of each layer is covariant to the action of global
translations. Kondor and Trivedi [2018] discuss the generalization of convolution to the action of
compact groups (such as  in our case  rotations) and prove that the only possible linear operation that
is covariant with the group action  is what  in terms of SO(3)–vectors  corresponds to multiplying
each F(cid:96) matrix from the right by some matrix W of learnable weights.
For the nonlinearity  one option would be to express each spherical tensor as a function on SO(3)
using an inverse SO(3) Fourier transform  apply a pointwise nonlinearity  and then transform the
resulting function back into spherical tensors. This is the approach taken in e.g.  [Cohen et al. 
2018]. However  in our case this would be forbiddingly costly  as well as introducing quadra-
ture errors by virtual of having to interpolate on the group  ultimately degrading the network’s co-
variance. Instead  taking yet another hint from the structure of physical interactions  we use the
Clebsch–Gordan transform introduced in 3.1 as a nonlinearity. The general rule for taking the CG
product of two SO(3)–parts F(cid:96)1 ∈ C(2(cid:96)1+1)×n1 and G(cid:96)2 ∈ C(2(cid:96)2+1)×n2 gives a collection of parts
[F(cid:96)1 ⊗cg G(cid:96)2 ]|(cid:96)1−(cid:96)1|  . . . [F(cid:96)1 ⊗cg G(cid:96)2](cid:96)1+(cid:96)1 with columns

(cid:2)[F(cid:96)1 ⊗cg G(cid:96)2 ](cid:96)

(cid:3)
∗ (i1 i2) = C(cid:96)1 (cid:96)2 (cid:96) ([F(cid:96)1 ]∗ i1 ⊗ [G(cid:96)2]∗ i2)  

(7)

i.e.  every column of F(cid:96)1 is separately CG-multiplied with every column of G(cid:96)2. The (cid:96)’th part of
the CG-product of two SO(3)–vectors consists of the concatenation of all SO(3)–part matrices with
index (cid:96) coming from multiplying each part of F with each part of G:
[F(cid:96)1 ⊗cg G(cid:96)2](cid:96).

[F ⊗cg G](cid:96) =

(cid:77)

(cid:77)

(cid:96)1

(cid:96)2

Here and in the following ⊕ denotes the appropriate concatenation of vectors and matrices.
In
Cormorant  however  as a slight departure from (7)  to reduce the quadratic blow-up in the number
of columns  we always have n1 = n2 and use the restricted “channel-wise” CG-product 

(cid:2)[F(cid:96)1 ⊗cg G(cid:96)2](cid:96)

(cid:3)
∗ i = C(cid:96)1 (cid:96)2 (cid:96) ([F(cid:96)1]∗ i ⊗ [G(cid:96)2 ]∗ i)  

where each column of F(cid:96)1 is only mixed with the corresponding column of G(cid:96)2. We note that similar
Clebsch–Gordan nonlinearities were used in [Kondor et al.  2018]  and that the Clebsch–Gordan
product is also an essential part of Tensor Field Networks [Thomas et al.  2018].

4.2 One-body and two-body interactions

As stated in Deﬁnition 2  the covariant neurons in a Cormorant net correspond to different subsets
of the atoms making up the physical system to be modeled. For simplicty in our present architecture
there are only two types of neurons: those that correspond to individual atoms and those that corre-
spond to pairs. For a molecule consisting of N atoms  each layer s = 0  1  . . .   S of the covariant
part of the network has N neurons corresponding to the atoms and N 2 neurons corresponding to the
(i  j) atom pairs. By loose analogy with graph neural networks  we call the corresponding F s
i and
i j activations vertex and edge activations  respectively.
gs
i activation is an SO(3)–vector consisting of L+1 distinct
In accordance with the foregoing  each F s
is a (2(cid:96)+1)×τ s
)  i.e.  each F s (cid:96)
parts (F s 0
(cid:96) dimensional complex matrix that trans-
i
(cid:55)→ D(cid:96)(R) F s (cid:96)
forms under rotations as F s (cid:96)
. The different columns of these matrices are regarded
as the different channels of the network  because they fulﬁll a similar role to channels in conven-
i j   . . .   gs L
tional convolutional nets. The gs
i j ) 
but these are invariant under rotations. Again for simplicity  in the version of Cormorant that we used
in our experiments L is the same in every layer (speciﬁcally L = 3)  and the number of channels is
also independent of both s and (cid:96)  speciﬁcally  τ s
The actual form of the vertex activations captures “one-body interactions” propagating information
from the previous layer related to the same atom and (indirectly  via the edge activations) “two-body
interactions” capturing interactions between pairs of atoms:

i j edge activations also break down into parts (gs 0

  . . .   F s L

i j   gs 1

  F s 1

i

i

i

i

i

(cid:104)

i ⊕(cid:0)F s−1
(cid:123)(cid:122)
i ⊗cg F s−1

i

one-body part

(cid:124)

F s

F s−1

i

=

(cid:96) ≡ nc = 16.
(cid:1)
(cid:125)

⊕(cid:16)(cid:88)
(cid:124)

j

6

(cid:105) · W vertex

s (cid:96)

(cid:17)
(cid:125)

i j ⊗cg F s−1
(cid:123)(cid:122)
Gs

two-body part

j

.

(8)

(cid:104)(cid:0) gs−1 (cid:96)
i j ⊕(cid:0)F s−1

i

· F s−1

j

(cid:1) ⊕ ηs (cid:96)(ri j)(cid:1) W edge

(cid:105)

i j Y (cid:96)((cid:98)ri j) 
where Y (cid:96)((cid:98)ri j) are the spherical harmonic vectors capturing the relative position of atoms i and j.

i j are SO(3)–vectors arising from the edge network. Speciﬁcally  Gs (cid:96)

i j = gs (cid:96)

Here Gs

The edge activations  in turn  are deﬁned

s (cid:96)

gs (cid:96)
i j = µs(ri j)

(9)
where we made the (cid:96) = 0  1  . . .   L irrep index explicit. As before  in these formulae  ⊕ denotes
concatenation over the channel index c  ηs (cid:96)
c(ri j) are
learnable cutoff functions limiting the inﬂuence of atoms that are farther away from atom i. The
learnable parameters of the network are the {W vertex
Note that the F s−1
dot product term is the only term in these formulae responsible for
the interaction between different atoms  and that this term always appears in conjunction with the
c (ri j) radial basis functions and µs
c(ri j) cutoff functions (as well as the SO(3)–covariant spher-
ηs (cid:96)
ical harmonic vector) making sure that interaction scales with the distance between the atoms. More
details of these activation rules are given in the Supplement.

c (ri j) are learnable radial functions  and µs

s (cid:96) } weight matrices.

s (cid:96) } and {W edge

· F s−1

j

i

4.3 Overall structure and comparison with other architectures

j

from the activations F s

} ← CGNet({F s

i
SO(3)-vector of type τ s
i .

In addition to the covariant neurons described above  our network also needs neurons to compute
the input featurization and the the ﬁnal output after the covariant layers. Thus  in total  a Cormorant
networks consists of three distinct parts:
1. An input featurization network {F s=0
2. An S-layer network {F s+1

charges/identities and (optionally) a scalar function of relative positions ri j.
i })of covariant activations F s

} ← INPUT({Zi  ri j}) that operates only on atomic

3. A rotation invariant network at the top y ← OUTPUT((cid:76)S

s=0{F s
i   and uses them to predict a regression target y.
We leave the details of the input and output featurization to the Supplement.
A key difference between Cormorant and other recent covariant networks (Tensor Field Net-
works [Thomas et al.  2018] and SE(3)-equivariant networks [Weiler et al.  2018]) is the use of
Clebsch-Gordan non-linearities. The Clebsch-Gordan non-linearity results in a complete interac-
tion of every degree of freedom in an activation. This comes at the cost of increased difﬁculty in
training  as discussed in the Supplement. We further note that SE(3)-equivariant networks use a
three-dimensional grid of points to represent data  and ensure both translational and rotational co-
variance (equivariance) of each layer. Cormorant on the other hand uses activations that are covariant
to rotations  and strictly invariant to translations.

i   each of which is a
i }) that constructs scalars

5 Experiments

We present experimental results on two datasets of interest to the computational chemistry com-
munity: MD-17 for learning molecular force ﬁelds and potential energy surfaces  and QM-9 for
learning the ground state properties of a set of molecules. The supplement provides a detailed sum-
mary of all hyperparameters  our training algorithm  and the details of the input/output levels used
in both cases. Our code is available at https://github.com/risilab/cormorant.
QM9 [Ramakrishnan et al.  2014] is a dataset of approximately 134k small organic molecules con-
taining the atoms H  C  N  O  F. For each molecule  the ground state conﬁguration is calculated
using DFT  along with a variety of molecular properties. We use the ground state conﬁguration as
the input to our Cormorant  and use a common subset of properties in the literature as regression tar-
gets. Table 1(a) presents our results averaged over three training runs compared with SchNet [Schütt
et al.  2017]  MPNNs [Gilmer et al.  2017]  and wavelet scattering networks [Hirn et al.  2017]. Of
the twelve regression targets considered  we achieve leading or competitive results on six (α  ∆ 
HOMO  LUMO  µ  Cv). The remaining four targets are within 40% of the best result  with the
exception of R2.
MD-17 [Chmiela et al.  2016] is a dataset of eight small organic molecules (see Table 1(b)) con-
taining up to 17 total atoms composed of the atoms H  C  N  O  F. For each molecule  an ab

7

Table 1: Mean absolute error of various prediction targets on QM-9 (left) and conformational
energies (in units of kcal/mol) on MD-17 (right). The best results within a standard deviation of
three Cormorant training runs (in parenthesis) are indicated in bold.

Cormorant

SchNet NMP WaveScatt

α (bohr3)
∆ (eV)
HOMO (eV)
LUMO (eV)
µ (D)
Cv (cal/mol K)
G (eV)
H (eV)
R2 (bohr2)
U (eV)
U0 (eV)
ZPVE (meV)

0.085 (0.001)
0.061 (0.005)
0.034 (0.002)
0.038 (0.008)
0.038 (0.009)
0.026 (0.000)
(0.000)
0.020
(0.001)
0.021
0.961
(0.019)
(0.000)
0.021
(0.003)
0.022
2.027
(0.042)

0.235
0.063
0.041
0.034
0.033
0.033
0.014
0.014
0.073
0.019
0.014
1.700

0.092
0.069
0.043
0.038
0.030
0.040
0.019
0.017
0.180
0.020
0.020
1.500

0.160
0.118
0.085
0.076
0.340
0.049
0.022
0.022
0.410
0.022
0.022
2.000

Cormorant DeepMD DTNN SchNet GDML sGDML

Aspirin
Benzene
Ethanol
Malonaldehyde
Naphthalene
Salicylic Acid
Toluene
Uracil

0.098
0.023
0.027
0.041
0.029
0.066
0.034
0.023

0.201
0.065
0.055
0.092
0.095
0.106
0.085
0.085

–
0.040
–
0.190
–
0.410
0.180
–

0.120
0.070
0.050
0.080
0.110
0.100
0.090
0.100

0.270
0.070
0.150
0.160
0.120
0.120
0.120
0.110

0.190
0.100
0.070
0.100
0.120
0.120
0.100
0.110

initio molecular dynamics simulation was run using DFT to calculate the ground state energy and
forces. At intermittent timesteps  the energy  forces  and conﬁguration (positions of each atom) were
recorded. For each molecule we use a train/validation/test split of 50k/10k/10k atoms respectively.
The results of these experiments are presented in Table 1(b)  where the mean-average error (MAE)
is plotted on the test set for each of molecules. (All units are in kcal/mol  as consistent with the
dataset and the literature.) To the best of our knowledge  the current state-of-the art algorithms on
this dataset are DeepMD [Zhang et al.  2018]  DTNN [Schütt et al.  2017]  SchNet [Schütt et al. 
2017]  GDML [Chmiela et al.  2016]  and sGDML [Chmiela et al.  2018]. Since training and testing
set sizes were not consistent  we used a training set of 50k molecules to compare with all neural
network based approaches. As can be seen from the table  our Cormorant network outperforms all
competitors.

6 Conclusions

To the best of our knowledge  Cormorant is the ﬁrst neural network architecture in which the opera-
tions implemented by the neurons is directly motivated by the form of known physical interactions.
Rotation and translation invariance are explicitly “baked into” the network by the fact all activations
are represented in spherical tensor form (SO(3)–vectors)  and the neurons combine Clebsch–Gordan
products  concatenation of parts and mixing with learnable weights  all of which are covariant op-
erations. In future work we envisage the potentials learned by Cormorant to be directly integrated
in MD simulation frameworks. In this regard  it is very encouraging that on MD-17  which is the
standard benchmark for force ﬁeld learning  Cormorant outperforms all other competing methods.
Learning from derivatives (forces) and generalizing to other compact symmetry groups are natural
extensions of the persent work.

Acknowledgements

This project was supported by DARPA “Physics of AI” grant number HR0011837139  and used
computational resources acquired through NSF MRI 1828629.

8

References
Albert P Bartók  Michael C Payne  Risi Kondor  and Gábor Csányi. Gaussian Approximation Potentials: the

accuracy of quantum mechanics  without the electrons. Phys Rev Lett  104(13):136403  2010.

Albert P. Bartók  Risi Kondor  and Gábor Csányi. On representing chemical environments. Phys. Rev. B  87:

184115  May 2013.

Jörg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional potential-

energy surfaces. Phys Rev Lett  98(14):146401  2007.

Tamir Bendory  Nicolas Boumal  Chao Ma  Zhizhen Zhao  and Amit Singer. Bispectrum inversion with appli-
cation to multireference alignment. Trans. Sig. Proc.  66(4):1037–1050  February 2018. ISSN 1053-587X.
doi: 10.1109/TSP.2017.2775591.

M. M. Bronstein  J. Bruna  Y. LeCun  A. Szlam  and P. Vandergheynst. Geometric deep learning: Going beyond
euclidean data. IEEE Signal Process. Mag.  34(4):18–42  July 2017. ISSN 1053-5888. doi: 10.1109/MSP.
2017.2693418.

B. R. Brooks  C. L. Brooks  A. D. Mackerell  L. Nilsson  R. J. Petrella  B. Roux  Y. Won  G. Archontis  C. Bar-
tels  S. Boresch  and et al. CHARMM: the biomolecular simulation program. Journal of Computational
Chemistry  30(10):1545–1614  Jul 2009. ISSN 1096-987X.

Bernard R. Brooks  Robert E. Bruccoleri  Barry D. Olafson  David J. States  S. Swaminathan  and Martin
Karplus. CHARMM: A program for macromolecular energy  minimization  and dynamics calculations.
Journal of Computational Chemistry  4(2):187–217  Jun 1983. ISSN 1096-987X.

J. Bruna  W. Zaremba  A. Szlam  and Y. LeCun. Spectral networks and locally connected networks on graphs.

3  2014.

Stefan Chmiela  Alexandre Tkatchenko  Huziel E. Sauceda  Igor Poltavsky  Kristof T. Schütt  and Klaus-Robert
Müller. Machine Learning of Accurate Energy-Conserving Molecular Force Fields. (May):1–6  2016. ISSN
2375-2548.

Stefan Chmiela  Huziel E. Sauceda  Klaus-Robert Muller  and Alexandre Tkatchenko. Towards exact molecular
dynamics simulations with machine-learned force ﬁelds. Nature Communications  9(1):3887  2018. doi:
10.1038/s41467-018-06169-2. URL https://doi.org/10.1038/s41467-018-06169-2.

Taco S. Cohen and Max Welling. Group equivariant convolutional networks. CoRR  abs/1602.07576  2016a.

URL http://arxiv.org/abs/1602.07576.

Taco S. Cohen and Max Welling. Steerable cnns. CoRR  abs/1612.08498  2016b. URL http://arxiv.org/

abs/1612.08498.

Taco S. Cohen  Mario Geiger  Jonas Köhler  and Max Welling. Spherical cnns. CoRR  abs/1801.10130  2018.

URL http://arxiv.org/abs/1801.10130.

Taco S. Cohen  Maurice Weiler  Berkay Kicanaoglu  and Max Welling. Gauge equivariant convolutional net-
works and the icosahedral CNN. CoRR  abs/1902.04615  2019. URL http://arxiv.org/abs/1902.
04615.

Carlos Esteves  Christine Allen-Blanchette  Ameesh Makadia  and Kostas Daniilidis. 3d object classiﬁcation
and retrieval with spherical cnns. CoRR  abs/1711.06721  2017. URL http://arxiv.org/abs/1711.
06721.

Justin Gilmer  Samuel S. Schoenholz  Patrick F. Riley  Oriol Vinyals  and George E. Dahl. Neural message
passing for quantum chemistry. CoRR  abs/1704.01212  2017. URL http://arxiv.org/abs/1704.
01212.

Mikael Henaff  Joan Bruna  and Yann LeCun. Deep convolutional networks on graph-structured data. CoRR 

abs/1506.05163  2015. URL http://arxiv.org/abs/1506.05163.

M. Hirn  S. Mallat  and N. Poilvert. Wavelet scattering regression of quantum chemical energies. Multiscale

Modeling & Simulation  15(2):827–863  Jan 2017. ISSN 1540-3459.

P. Hohenberg and W. Kohn. Inhomogeneous electron gas. Phys. Rev.  136:864–871  1964.
John David Jackson. Classical electrodynamics. Wiley  New York  NY  3rd ed. edition  1999.

9780471309321. URL http://cdsweb.cern.ch/record/490457.

ISBN

Ramakrishna Kakarala. Triple correlation on groups. PhD thesis  Department of Mathematics  UC Irvine 

1992.

R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks to the

action of compact groups. International Conference on Machine Learning (ICML)  2018.

Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic poten-

tials. CoRR  abs/1803.01588  2018. URL http://arxiv.org/abs/1803.01588.

Risi Kondor  Zhen Lin  and Shubhendu Trivedi. Clebsch–gordan nets: a fully fourier space spherical con-
volutional neural network. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and
R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages 10117–10126. Curran
Associates  Inc.  2018.

9

Jonathan Masci  Davide Boscaini  Michael M. Bronstein  and Pierre Vandergheynst. Geodesic convolutional
neural networks on riemannian manifolds. CoRR  abs/1501.06297  2015. URL http://arxiv.org/abs/
1501.06297.

G. Montavon  M. Rupp  V. Gobre  A. Vazquez-Mayagoitia  K. Hansen  A. Tkatchenko  K-R. Müller  and O. A.
von Lilienfeld. Machine learning of molecular electronic properties in chemical compound space. New J.
Phys.  15  09 2013.

Federico Monti  Davide Boscaini  Jonathan Masci  Emanuele Rodolà  Jan Svoboda  and Michael M. Bronstein.
Geometric deep learning on graphs and manifolds using mixture model cnns. CoRR  abs/1611.08402  2016.
URL http://arxiv.org/abs/1611.08402.

Raghunathan Ramakrishnan  Pavlo O Dral  Matthias Rupp  and O Anatole von Lilienfeld. Quantum chemistry

structures and properties of 134 kilo molecules. Scientiﬁc Data  1  2014.

M. Rupp  A. Tkatchenko  K. R. Müller  and O. A. von Lilienfeld. Fast and accurate modeling of molecular

atomization energies with machine learning. Phys. Rev. Lett.  108  2012.

Kristof Schütt  Pieter-Jan Kindermans  Huziel Enoc Sauceda Felix  Stefan Chmiela  Alexandre Tkatchenko 
and Klaus-Robert Müller. Schnet: A continuous-ﬁlter convolutional neural network for modeling quantum
interactions. 2017.

Kristof T. Schütt  Farhad Arbabzadah  Stefan Chmiela  Klaus R. M uller  and Alexandre Tkatchenko. Quantum-
chemical insights from deep tensor neural networks. Nature Communications  8:13890  Jan 2017. ISSN
2041-1723.

Alexander V Shapeev. Moment Tensor Potentials: a class of systematically improvable interatomic potentials.

arXiv  December 2015.

J. S. Smith  O. Isayev  and A. E. Roitberg. Ani-1: an extensible neural network potential with dft accuracy at

force ﬁeld computational cost. Chem. Sci.  8:3192–3203  2017. doi: 10.1039/C6SC05720A.

A.J. Stone. The Theory of Intermolecular Forces. International Series of Monographs on Chemistry. Clarendon

Press  1997. ISBN 9780198558835.

Nathaniel Thomas  Tess Smidt  Steven M. Kearnes  Lusann Yang  Li Li  Kai Kohlhoff  and Patrick Riley.
Tensor ﬁeld networks: Rotation- and translation-equivariant neural networks for 3d point clouds. CoRR 
abs/1802.08219  2018.

Maurice Weiler  Mario Geiger  Max Welling  Wouter Boomsma  and Taco Cohen. 3d steerable cnns: Learning
rotationally equivariant features in volumetric data. CoRR  abs/1807.02547  2018. URL http://arxiv.
org/abs/1807.02547.

Kun Yao  John E. Herr  David[space]W. Toth  Ryker Mckintyre  and John Parkhill. The tensormol-0.1 model
chemistry: a neural network augmented with long-range physics. Chem. Sci.  9:2261–2269  2018. doi:
10.1039/C7SC04934J.

Linfeng Zhang  Jiequn Han  Han Wang  Roberto Car  and Weinan E. Deep potential molecular dynamics: A
scalable model with the accuracy of quantum mechanics. Phys. Rev. Lett.  120:143001  Apr 2018. doi:
10.1103/PhysRevLett.120.143001. URL https://link.aps.org/doi/10.1103/PhysRevLett.120.
143001.

10

,Brandon Anderson
Truong Son Hy
Risi Kondor