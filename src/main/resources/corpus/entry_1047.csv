2011,Complexity of Inference in Latent Dirichlet Allocation,We consider the computational complexity of probabilistic inference in Latent Dirichlet Allocation (LDA).  First  we study the problem of finding the maximum a posteriori (MAP) assignment of topics to words  where the document's topic distribution is integrated out.  We show that  when the effective number of topics per document is small  exact inference takes polynomial time. In contrast  we show that  when a document has a large number of topics  finding the MAP assignment of topics to words in LDA is NP-hard.  Next  we consider the problem of finding the MAP topic distribution for a document  where the topic-word assignments are integrated out.  We show that this problem is also NP-hard.  Finally  we briefly discuss the problem of sampling from the posterior  showing that this is NP-hard in one restricted setting  but leaving open the general question.,Complexity of Inference in Latent Dirichlet

Allocation

David Sontag

New York University⇤

Daniel M. Roy

University of Cambridge

Abstract

We consider the computational complexity of probabilistic inference in La-
tent Dirichlet Allocation (LDA). First  we study the problem of ﬁnding
the maximum a posteriori (MAP) assignment of topics to words  where
the document’s topic distribution is integrated out. We show that  when
the e↵ective number of topics per document is small  exact inference takes
polynomial time. In contrast  we show that  when a document has a large
number of topics  ﬁnding the MAP assignment of topics to words in LDA
is NP-hard. Next  we consider the problem of ﬁnding the MAP topic dis-
tribution for a document  where the topic-word assignments are integrated
out. We show that this problem is also NP-hard. Finally  we brieﬂy discuss
the problem of sampling from the posterior  showing that this is NP-hard
in one restricted setting  but leaving open the general question.

1

Introduction

Probabilistic models of text and topics  known as topic models  are powerful tools for ex-
ploring large data sets and for making inferences about the content of documents. Topic
models are frequently used for deriving low-dimensional representations of documents that
are then used for information retrieval  document summarization  and classiﬁcation [Blei
and McAuli↵e  2008; Lacoste-Julien et al.  2009]. In this paper  we consider the computa-
tional complexity of inference in topic models  beginning with one of the simplest and most
popular models  Latent Dirichlet Allocation (LDA) [Blei et al.  2003]. The LDA model is
arguably one of the most important probabilistic models in widespread use today.

Almost all uses of topic models require probabilistic inference. For example  unsupervised
learning of topic models using Expectation Maximization requires the repeated computation
of marginal probabilities of what topics are present in the documents. For applications in
information retrieval and classiﬁcation  each new document necessitates inference to deter-
mine what topics are present.

Although there is a wealth of literature on approximate inference algorithms for topic mod-
els  such Gibbs sampling and variational inference [Blei et al.  2003; Griths and Steyvers 
2004; Mukherjee and Blei  2009; Porteous et al.  2008; Teh et al.  2007]  little is known
about the computational complexity of exact inference. Furthermore  the existing inference
algorithms  although well-motivated  do not provide guarantees of optimality. We choose
to study LDA because we believe that it captures the essence of what makes inference easy
or hard in topic models. We believe that a careful analysis of the complexity of popular
probabilistic models like LDA will ultimately help us build a methodology for spanning the
gap between theory and practice in probabilistic AI.

Our hope is that our results will motivate discussion of the following questions  guiding
research of both new topic models and the design of new approximate inference and learning

⇤ This work was partially carried out while D.S. was at Microsoft Research New England.

1

algorithms. First  what is the structure of real-world LDA inference problems? Might there
be structure in “natural” problem instances that makes them di↵erent from hard instances
(e.g.  those used in our reductions)? Second  how strongly does the prior distribution bias
the results of inference? How do the hyperparameters a↵ect the structure of the posterior
and the hardness of inference?

We study the complexity of ﬁnding assignments of topics to words with high posterior
probability and the complexity of summarizing the posterior distributions on topics in a
document by either its expectation or points with high posterior density. In the former case 
we show that the number of topics in the maximum a posteriori assignment determines the
hardness. In the latter case  we quantify the sense in which the Dirichlet prior can be seen
to enforce sparsity and use this result to show hardness via a reduction from set cover.

2 MAP inference of word assignments

We will consider the inference problem for a single document. The LDA model states that the
document  represented as a collection of words w = (w1  w2  . . .   wN )  is generated as follows:
a distribution over the T topics is sampled from a Dirichlet distribution  ✓ ⇠ Dir(↵); then 
for i 2 [N ] := {1  . . .   N}  we sample a topic zi ⇠ Multinomial(✓) and word wi ⇠ zi  where
 t  t 2 [T ] are distributions on a dictionary of words. Assume that the word distributions
 t are ﬁxed (e.g.  they have been previously estimated)  and let lit = log Pr(wi|zi = t) be
the log probability of the ith word being generated from topic t. After integrating out the
topic distribution vector  the joint distribution of the topic assignments conditioned on the
words w is given by

Pr(z1  . . .   zN|w) /

Pr(wi|zi) 

(1)

Qt (↵t)Qt (nt + ↵t)
(Pt ↵t)
(Pt ↵t + N )

NYi=1

where nt is the total number of words assigned to topic t.
In this section  we focus on the inference problem of ﬁnding the most likely assignment of
topics to words  i.e. the maximum a posteriori (MAP) assignment. This has many possible
applications. For example  it can be used to cluster the words of a document  or as part of
a larger system such as part-of-speech tagging [Li and McCallum  2005]. More broadly  for
many classiﬁcation tasks involving topic models it may be useful to have word-level features
for whether a particular word was assigned to a given topic. From both an algorithm design
and complexity analysis point of view  this MAP problem has the additional advantage of
involving only discrete random variables.

Taking the logarithm of Eq. 1 and ignoring constants  ﬁnding the MAP assignment is seen
to be equivalent to the following combinatorial optimization problem:

 = max

xitlit

(2)

xit2{0 1} nt Xt
subject to Xt

log( nt + ↵t) +Xi t
xit = 1  Xi

xit = nt 

where the indicator variable xit = I[zi = t] denotes the assignment of word i to topic t.

2.1 Exact maximization for small number of topics

Suppose a document only uses ⌧ ⌧ T topics. That is  T could be large  but we are
guaranteed that the MAP assignment for a document uses at most ⌧ di↵erent topics. In this
section  we show how we can use this knowledge to eciently ﬁnd a maximizing assignment
of words to topics. It is important to note that we only restrict the maximum number of
topics per document  letting the Dirichlet prior and the likelihood guide the choice of the
actual number of topics present.

We ﬁrst observe that  if we knew the number of words assigned to each topic  ﬁnding the
MAP assignment is easy. For t 2 [T ]  let n⇤t be the number of words assigned to topic t

2

t1

t2

t3

t4

t5

1ê4
1ê2

1

2

3.0

2.5

2.0

1.5

1.0

0.5

15

10

5

w1

w2

w3

w4

w5

w6

0.5

1.0

1.5

2.0

2.5

3.0

1

2

3

4

5

Figure 1:
(Left) A LDA instance derived from a k-set packing instance. (Center) Plot of
F (nt) = log (nt + ↵) for various values of ↵. The x-axis varies nt  the number of words assigned
to topic t  and they -axis shows F (nt). (Right) Behavior of log (nt + ↵) as ↵ ! 0. The function
is stable everywhere but at zero  where the reward for sparsity increases without bound.

in the MAP assignment. Then  the MAP assignment x is found by solving the following
optimization problem:

max

xit2{0 1} Xi t
subject to Xt

xitlit

xit = 1  Xi

xit = n⇤t  

(3)

which is equivalent to weighted b-matching in a bipartite graph (the words are on one side 
the topics on the other) and can be optimally solved in time O(bm3)  where b = maxt n⇤t =
O(N ) and m = N + T [Schrijver  2003].

We call (n1  . . .   nT ) a valid partition when ni  0 and Pt nt = N . Using weighted b-
matching  we can ﬁnd a MAP assignment of words to topics by trying all T
⌧ =⇥( T ⌧ )

choices of ⌧ topics and all possible valid partitions with at most ⌧ non-zeros.

for all subsets A ✓ [T ] such that |A| = ⌧ do

for all valid partitions n = (n1  n2  . . .   nT ) such that nt = 0 for t 62 A do
end for

A n Weighted-B-Matching(A  n  l) +Pt log( nt + ↵t)

end for
return arg maxA n A n

There are at most N ⌧1 valid partitions with ⌧ non-zero counts. For each of these  we solve
the b-matching problem to ﬁnd the most likely assignment of words to topics that satisﬁes
the cardinality constraints. Thus  the total running time is O((N T )⌧ (N + ⌧ )3). This is
polynomial when the number of topics ⌧ appearing in a document is a constant.

2.2

Inference is NP-hard for large numbers of topics

In this section  we show that probabilistic inference is NP-hard in the general setting where a
document may have a large number of topics in its MAP assignment. Let WORD-LDA(↵)
denote the decision problem of whether > V (see Eq. 2) for some V 2 R  where the
hyperparameters ↵t = ↵ for all topics. We consider both ↵< 1 and ↵  1 because  as
shown in Figure 1  the optimization problem is qualitatively di↵erent in these two cases.
Theorem 1. WORD-LDA(↵) is NP-hard for all ↵> 0.

Proof. Our proof is a straightforward generalization of the approach used by Halperin and
Karp [2005] to show that the minimum entropy set cover problem is hard to approximate.
The proof is done by reduction from k-set packing (k-SP)  for k  3. In k-SP  we are given
a collection of k-element sets over some universe of elements ⌃ with |⌃| = n. The goal
is to ﬁnd the largest collection of disjoint sets. There exists a constant c <1 such that
it is NP-hard to decide whether a k-SP instance has (i) a solution with n/k disjoint sets

3

covering all elements (called a perfect matching)  or (ii) at most cn/k disjoint sets (called a
(cn/k)-matching).

We now describe how to construct a LDA inference problem from a k-SP instance. This
requires specifying the words in the document  the number of topics  and the word log
probabilities lit. Let each element i 2 ⌃ correspond to a word wi  and let each set correspond
to one topic. The document consists of all of the words (i.e.  ⌃). We assign uniform
probability to the words in each topic  so that Pr(wi|zi = t) = 1
k for i 2 t  and 0 otherwise.
Figure 1 illustrates the resulting LDA model. The topics are on the top  and the words
from the document are on the bottom. An edge is drawn between a topic (set) and a word
(element) if the corresponding set contains that element.

What remains is to show that we can solve some k-SP problem by using this reduction and
solving a WORD-LDA(↵) problem. For technical reasons involving ↵> 1  we require that
k is suciently large. We will use the following result (we omit the proof due to space
limitations).
Lemma 2. Let P be a k-SP instance for k > (1 + ↵)2  and let P 0 be the derived WORD-
LDA(↵) instance. There exist constants CU and CL < CU such that  if there is a perfect
matching in P   then   CU . If  on the other hand  there is at most a (cn/k)-matching in
P   then  < CL.
Let P be a k-SP instance for k > (3 + ↵)2  P 0 be the derived WORD-LDA(↵) instance 
and CU and CL < CU be as in Lemma 2. Then  by testing < CL and > CU we can
decide whether P is a perfect matching or at best a (cn/k)-matching. Hence k-SP reduces
to WORD-LDA(↵).

The bold lines in Figure 1 indicate the MAP assignment  which for this example corresponds
to a perfect matching for the original k-set packing instance. More realistic documents would
have signiﬁcantly more words than topics used. Although this is not possible while keeping
k = 3  since the MAP assignment always has ⌧  N/k  we can instead reduce from a k-set
packing problem with k  3. Lemma 2 shows that this is hard as well.

3 MAP inference of the topic distribution

In this section we consider the task of ﬁnding the mode of Pr(✓|w). This MAP problem
involves integrating out the topic assignments  zi  as opposed to the previously considered
MAP problem of integrating out the topic distribution ✓. We will see that the MAP topic
distribution is not always well-deﬁned  which will lead us to deﬁne and study alternative
formulations. In particular  we give a precise characterization of the MAP problem as one
of ﬁnding sparse topic distributions  and use this fact to give hardness results for several
settings. We also show settings for which MAP inference is tractable.

There are many potential applications of MAP inference of the document’s topic distribu-
tion. For example  the distribution may be used for topic-based information retrieval or
as the feature vector for classiﬁcation. As we will make clear later  this type of inference
results in sparse solutions. Thus  the MAP topic distribution provides a compact summary
of the document that could be useful for document summarization.

Let ✓ = (✓1  . . .  ✓ T ). A straightforward application of Bayes’ rule allows us to write the
posterior density of ✓ given w as

where it = Pr(wi|zi = t). Taking the logarithm of the posterior and ignoring constants 
we obtain

Pr(✓|w) / TYt=1

✓↵t1
t

! NYi=1

(✓) =

TXt=1

(↵t  1) log(✓t) +

NXi=1

4

✓t it!  

TXt=1
log TXt=1

✓t it!

(4)

(5)

t=1 it✓t).

i=1 log(PT

We will use the shorthand (✓) =P (✓) + L(✓)  where P (✓) = PT
L(✓) =PN
To ﬁnd the MAP ✓  we maximize (5) subject to the constraint thatPT

t=1 ✓t = 1 and ✓t  0.
In particular  note that if
Unfortunately  this maximization problem can be degenerate.
✓t = 0 for ↵t < 1  then the corresponding term in P (✓) will take the value 1  overwhelming
the likelihood term. Thus  any feasible solution with the above property could be considered
‘optimal’.

t=1(↵t  1) log(✓t) and

A similar problem arises during the maximum-likelihood estimation of a normal mixture
model  where the likelihood diverges to inﬁnity as the variance of a mixture component
with a single data point approaches zero [Biernacki and Chr´etien  2003; Kiefer and Wol-
fowitz  1956]. In practice  one can enforce a lower bound on the variance or penalize such
conﬁgurations. Here we consider a similar tactic.

For ✏> 0  let TOPIC-LDA(✏) denote the optimization problem

max

✓

(✓)

subject to Xt

✓t = 1 ✏  ✓t  1.

(6)

For ✏ = 0  we will denote the corresponding optimization problem by TOPIC-LDA. When
↵t = ↵  i.e.
the prior distribution on the topic distribution is a symmetric Dirichlet 
we write TOPIC-LDA(✏ ↵ ) for the corresponding optimization problem.
In the follow-
ing sections we will study the structure and hardness of TOPIC-LDA  TOPIC-LDA(✏) and
TOPIC-LDA(✏ ↵ ).

3.1 Polynomial-time inference for large hyperparameters (↵t  1)
When ↵t  1  Eq. 5 is a concave function of ✓. As a result  we can eciently ﬁnd ✓⇤ using a
number of techniques from convex optimization. Note that this is in contrast to the MAP
inference problem discussed in Section 2  which we showed was hard for all choices of ↵.

Since we are optimizing over the simplex (✓ must be non-negative and sum to 1)  we can
apply the exponentiated gradient method [Kivinen and Warmuth  1995]. Initializing ✓0 to
be the uniform vector  the update for time s is given by
↵t  1
✓s
t

5s
t =
where ⌘ is the step size and 5s is the gradient.
When ↵ = 1 the prior disappears altogether and this algorithm simply corresponds to
optimizing the likelihood term. When ↵  1  the prior corresponds to a bias toward a
particular ✓ topic distribution.

t exp(⌘5s
✓s
t )
ˆt exp(⌘5s
ˆt )

Pˆt ✓s

 it
ˆt=1 ✓s

PT

 

ˆt iˆt

+

NXi=1

✓s+1
t =

 

(7)

3.2 Small hyperparameters encourage sparsity (↵< 1)

On the other hand  when ↵t < 1  the ﬁrst term in Eq. 5 is convex whereas the second term is
concave. This setting  of ↵ much smaller than 1  occurs frequently in practice. For example 
learning a LDA model on a large corpus of NIPS abstracts with T = 200 topics  we ﬁnd
that the hyperparameters found range from ↵t = 0.0009 to 0.135  with the median being
0.01. Although in this setting it is dicult to ﬁnd the global optimum (we will make this
precise in Theorem 6)  one possibility for ﬁnding a local maximum is the Concave-Convex
Procedure [Yuille and Rangarajan  2003].

In this section we prove structural results about the TOPIC-LDA(✏ ↵ ) solution space for
when ↵< 1. These results illustrate that the Dirichlet prior encourages sparse MAP so-
lutions: the topic distribution will be large on as few topics as necessary to explain every
word of the document  and otherwise will be close to zero.

The following lemma shows that in any optimal solution to TOPIC-LDA(✏ ↵ )  for every
word  there is at least one topic that both has large probability and gives non-trivial prob-
ability to this word. We use K(↵  T  N ) = e3/↵N1T 1/↵ to refer to the lower bound on
the topic’s probability.

5

Lemma 3. Let ↵< 1. All optimal solutions ✓⇤ to TOPIC-LDA(✏ ↵ ) have the following
property: for every word i  ✓⇤ˆt  K(↵  T  N ) where ˆt = arg maxt it✓⇤t .
Proof sketch. If ✏  K(↵  T  N ) the claim trivially holds. Assume for the purpose of contra-
diction that there exists a word ˆi such that ✓⇤ˆt < K(↵  T  N )  where ˆt = arg maxt ˆit✓⇤t .

Let Y denote the set of topics t 6= ˆt such that ✓⇤t  2✏. Let 1 = Pt2Y ✓⇤t and 2 =
Pt62Y t6=ˆt ✓⇤t . Note that 2 < 2T✏ . Consider
It is easy to show that 8t  ˆ✓t  ✏  and Pt

contradicting the optimality of ✓⇤. The full proof is given in the supplementary material.

ˆ✓t =✓ 1  2  1

ˆ✓t = 1. Finally  we show that (ˆ✓) > (✓⇤) 

◆ ✓⇤t for t 2 Y 

ˆ✓t = ✓⇤t for t 62 Y  t 6= ˆt.

ˆ✓ˆt =

1
n

(8)

1

n

 

 it
 it0

1

1

ˆ✓t it

1↵ +2✏.

NXi=1

1↵ +2✏. Consider ˆ✓ deﬁned as

1✓⇤ˆt⌘ ✓⇤t for t 6= ˆt. We have:

✏ ◆ + (T  1)(1  ↵) log✓ 1  ✓⇤ˆt
1  ✏ ◆ +

Next  we show that if a topic is not suciently “used” then it will be given a probability very
close to zero. By used  we mean that for at least one word  the topic is close in probability
to that of the largest contributor to the likelihood of the word. To do this  we need to deﬁne
the notion of the dynamic range of a word  given as i = maxt t0: it>0  it0 >0
. We let
the maximum dynamic range be  = maxi i. Note that   1 and  for most applications 
it is reasonable to expect  to be small (e.g.  less than 1000).
Lemma 4. Let ↵< 1  and let ✓⇤ be any optimal solution to TOPIC-LDA(✏ ↵ ). Suppose
topic ˆt has ✓⇤ˆt < (N )1K(↵  T  N ). Then  ✓⇤ˆt  e
Proof. Suppose for the purpose of contradiction that ✓⇤ˆt > e
follows: ˆ✓ˆt = ✏  and ˆ✓t =⇣ 1✏
(ˆ✓)  (✓⇤) = (1  ↵) log✓ ✓⇤ˆt
Using the fact that log(1  z)  2z for z 2 [0  1
(T  1)(1  ↵) log✓ 1  ✓⇤ˆt
We have ˆ✓t  ✓⇤t for t 6= ˆt  and so
= Pt6=ˆt
Pt6=ˆt ✓⇤t it + ✓⇤ˆt iˆt 
Recall from Lemma 3 that  for each word i and ˜t = arg maxt it✓⇤t   we have ✓˜t > K(↵  T  N ).
Necessarily ˜t 6= ˆt. Therefore  using the fact that log 1
✓⇤ˆt iˆtPt6=ˆt ✓⇤t it  

log Pt
Pt ✓⇤t it! .
1  ✏ ◆  (T  1)(1  ↵) log1  ✓⇤ˆt  2(T  1)(↵  1)✓⇤ˆt

Pt
Pt ✓⇤t it
Pt6=ˆt ✓⇤t it + ✓⇤ˆt iˆt!  
log Pt6=ˆt ✓⇤t it

Pt6=ˆt ✓⇤t it
Pt6=ˆt ✓⇤t it + ✓⇤ˆt iˆt

 2(T  1)(↵  1)(N )1K(↵  T  N )  2(↵  1).

Thus ( ˆ✓)  (✓⇤) > (1  ↵) log e
Finally  putting together what we showed in the previous two lemmas  we conclude that
all optimal solutions to TOPIC-LDA(✏ ↵ ) either have ✓t large or small  but not in between
(that is  we have demonstrated a gap). We have the immediate corollary:

1↵ +2 + 2(↵  1)  1 = 0  completing the proof.

1+z  z 
(N )1K(↵  T  N ) iˆt

2 ]  it follows that

ˆ✓t it + ✏ iˆt

K(↵  T  N ) i˜t

 

(9)

(10)

Theorem 5. For ↵< 1  all optimal solutions to TOPIC-LDA(✏ ↵ ) have ✓t ⇣e
or ✓t  1e3/↵N2T 1/↵.

1

1↵ +2⌘ ✏

.

(11)

1
n

.

(12)

ˆ✓t it

1

6

3.3

Inference is NP-hard for small hyperparameters (↵< 1)

The previous results characterize optimal solutions to TOPIC-LDA(✏ ↵ ) and highlight the
fact that optimal solutions are sparse. In this section we show that these same properties
can be the source of computational hardness during inference. In particular  it is possible to
encode set cover instances as TOPIC-LDA(✏ ↵ ) instances  where the set cover corresponds
to those topics assigned appreciable probability.
Theorem 6. TOPIC-LDA(✏ ↵ ) is NP-hard for ✏  K(↵  T  N )T /(1↵)T N/(1↵) and ↵< 1.
Proof. Consider a set cover instance consisting of a universe of elements and a family of
sets  where we assume for convenience that the minimum cover is neither a singleton  all
but one of the family of sets  nor the entire family of sets  and that there are at least two
elements in the universe. As with our previous reduction  we have one topic per set and
one word in the document for each element. We let Pr(wi|zi = t) = 0 when element wi is
not in set t  and a constant otherwise (we make every topic have the uniform distribution
over the same number of words  some of which may be dummy words not appearing in the
document). Let Si ✓ [T ] denote the set of topics to which word i belongs. Then  up to
additive constants  we have P (✓) = (1  ↵)PT
✓t).
Let C✓⇤ ✓ [T ] be those topics t 2 [T ] such that ✓⇤t  K(↵  T  n)  where ✓⇤ is an optimal
solution to TOPIC-LDA(✏ ↵ ). It immediately follows from Lemma 3 that C✓⇤ is a cover.
Suppose for the purpose of contradiction that C✓⇤ is not a minimal cover. Let ˜C be a
minimal cover  and let ˜✓t = ✏ for t 62 ˜C and ˜✓t = 1✏(T| ˜C|)
T otherwise. We will show
that (˜✓) > (✓⇤)  contradicting the optimality of ✓⇤  and thus proving that C✓⇤ is in fact
minimal. This suces to show that TOPIC-LDA(✏ ↵ ) is NP-hard in this regime.

t=1 log(✓t) and L(✓) =PN

i=1 log(Pt2Si

> 1

| ˜C|

(13)

(1  ↵)

1
✏  T log K(↵  T  N ) 

For all ✓ in the simplex  we have Pi log(maxt2Si ✓t)  L(✓)  0. Thus it follows that
L(✓⇤)  L(˜✓)  N log T . Likewise  using the assumption that T | ˜C| + 1  we have
P (˜✓)  P (✓⇤)
 (T | ˜C|) log ✏  (| ˜C| + 1) log K(↵  T  N ) + (T | ˜C|1) log ✏
 log

(14)
where we have conservatively only included the terms t 62 ˜C for P (˜✓) and taken ✓⇤ 2
{✏  K(↵  T  N )} with | ˜C| + 1 terms taking the latter value. It follows that
P (˜✓) +L (˜✓) P (✓⇤) +L (✓⇤) > (1  ↵) log
This is greater than 0 precisely when (1  ↵) log 1
Note that although ✏ is exponentially small in N and T   the size of its representation in
binary is polynomial in N and T   and thus polynomial in the size of the set cover instance.
It can be shown that as ✏ ! 0  the solutions to TOPIC-LDA(✏ ↵ ) become degenerate 
concentrating their support on the minimal set of topics C ✓ [T ] such that 8i 9t 2 C s.t.
 it > 0. A generalization of this result holds for TOPIC-LDA(✏) and suggests that  while
it may be possible to give a more sensible deﬁnition of TOPIC-LDA as the set of solutions
for TOPIC-LDA(✏) as ✏ ! 0  these solutions are unlikely to be of any practical use.
4 Sampling from the posterior

1
✏  (T log K(↵  T  N ) +N log T ).
✏ > log T N K(↵  T  N )T .

(15)

The previous sections of the paper focused on MAP inference problems. In this section  we
study the problem of marginal inference in LDA.
Theorem 7. For ↵> 1  one can approximately sample from Pr(✓ | w) in polynomial time.
Proof sketch. The density given in Eq. 4 is log-concave when ↵  1. The algorithm given
in Lovasz and Vempala [2006] can be used to approximately sample from the posterior.

7

Although polynomial  it is not clear whether the algorithm given in Lovasz and Vempala
[2006]  based on random walks  is of practical interest (e.g.  the running time bound has a
constant of 1030). However  we believe our observation provides insight into the complexity
of sampling when ↵ is not too small  and may be a starting point towards explaining the
empirical success of using Markov chain Monte Carlo to do inference in LDA.

Next  we show that when ↵ is extremely small  it is NP-hard to sample from the posterior.
We again reduce from set cover. The intuition behind the proof is that  when ↵ is small
enough  an appreciable amount of the probability mass corresponds to the sparsest possible
✓ vectors where the supported topics together cover all of the words. As a result  we could
directly read o↵ the minimal set cover from the posterior marginals E[✓t | w].
Theorem 8. When ↵< (4N + 4)T N (N )1  it is NP-hard to approximately sample from

Pr(✓ | w)  under randomized reductions.
The full proof can be found in the supplementary material. Note that it is likely that one
would need an extremely large and unusual corpus to learn an ↵ so small. Our results
illustrate a large gap in our knowledge about the complexity of sampling as a function of ↵.
We feel that tightening this gap is a particularly exciting open problem.

5 Discussion

In this paper  we have shown that the complexity of MAP inference in LDA strongly depends
on the e↵ective number of topics per document. When a document is generated from a small
number of topics (regardless of the number of topics in the model)  WORD-LDA can be
solved in polynomial time. We believe this is representative of many real-world applications.
On the other hand  if a document can use an arbitrary number of topics  WORD-LDA is
NP-hard. The choice of hyperparameters for the Dirichlet does not a↵ect these results.

We have also studied the problem of computing MAP estimates and expectations of the
topic distribution. In the former case  the Dirichlet prior enforces sparsity in a sense that
we make precise. In the latter case  we show that extreme parameterizations can similarly
cause the posterior to concentrate on sparse solutions. In both cases  this sparsity is shown
to be a source of computational hardness.

In related work  Sepp¨anen et al. [2003] suggest a heuristic for inference that is also applicable
to LDA: if there exists a word that can only be generated with high probability from one of
the topics  then the corresponding topic must appear in the MAP assignment whenever that
word appears in a document. Miettinen et al. [2008] give a hardness reduction and greedy
algorithm for learning topic models. Although the models they consider are very di↵erent
from LDA  some of the ideas may still be applicable. More broadly  it would be interesting
to consider the complexity of learning the per-topic word distributions t.
Our paper suggests a number of directions for future study. First  our exact algorithms
can be used to evaluate the accuracy of approximate inference algorithms  for example by
comparing to the MAP of the variational posterior. On the algorithmic side  it would be
interesting to improve the running time of the exact algorithm from Section 2.1. Also  note
that we did not give an analogous exact algorithm for the MAP topic distribution when
the posterior has support on only a small number of topics.
In this setting  it may be
possible to ﬁnd this set of topics by trying all S ✓ [T ] of small cardinality and then doing
a (non-uniform) grid search over the topic distribution restricted to support S.

Finally  our structural results on the sparsity induced by the Dirichlet prior draws connec-
tions between inference in topic models and sparse signal recovery. We proved that the MAP
topic distribution has  for each topic t  either ✓t ⇡ ✏ or ✓t bounded below by some value
(much larger than ✏). Because of this gap  we can approximately view the MAP problem
as searching for a set corresponding to the support of ✓. Our work motivates the study of
greedy algorithms for MAP inference in topic models  analogous to those used for set cover.
One could even consider learning algorithms that use this greedy algorithm within the inner
loop [Krause and Cevher  2010].

8

Acknowledgments D.M.R. is supported by a Newton International Fellowship. We
thank Tommi Jaakkola and anonymous reviewers for helpful comments.

References
C. Biernacki and S. Chr´etien. Degeneracy in the maximum likelihood estimation of univariate

Gaussian mixtures with EM. Statist. Probab. Lett.  61(4):373–382  2003. ISSN 0167-7152.

D. Blei and J. McAuli↵e. Supervised topic models. In J. Platt  D. Koller  Y. Singer  and S. Roweis 
editors  Adv. in Neural Inform. Processing Syst. 20  pages 121–128. MIT Press  Cambridge  MA 
2008.

D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res.  3:

993–1022  2003. ISSN 1532-4435.

T. L. Griths and M. Steyvers. Finding scientiﬁc topics. Proc. Natl. Acad. Sci. USA  101(Suppl

1):5228–5235  2004. doi: 10.1073/pnas.0307752101.

E. Halperin and R. M. Karp. The minimum-entropy set cover problem. Theor. Comput. Sci.  348

(2):240–250  2005. ISSN 0304-3975. doi: http://dx.doi.org/10.1016/j.tcs.2005.09.015.

J. Kiefer and J. Wolfowitz. Consistency of the maximum likelihood estimator in the presence of
inﬁnitely many incidental parameters. Ann. Math. Statist.  27:887–906  1956. ISSN 0003-4851.
J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predic-

tors. Inform. and Comput.  132  1995.

A. Krause and V. Cevher. Submodular dictionary selection for sparse representation. In Proc. Int.

Conf. on Machine Learning (ICML)  2010.

S. Lacoste-Julien  F. Sha  and M. Jordan. DiscLDA: Discriminative learning for dimensionality
reduction and classiﬁcation. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors 
Adv. in Neural Inform. Processing Syst. 21  pages 897–904. 2009.

W. Li and A. McCallum. Semi-supervised sequence modeling with syntactic topic models. In Proc.

of the 20th Nat. Conf. on Artiﬁcial Intelligence  volume 2  pages 813–818. AAAI Press  2005.

L. Lovasz and S. Vempala. Fast algorithms for logconcave functions: Sampling  rounding  integra-
tion and optimization. In Proc. of the 47th Ann. IEEE Symp. on Foundations of Comput. Sci. 
pages 57–68. IEEE Computer Society  2006. ISBN 0-7695-2720-5.

P. Miettinen  T. Mielik¨ainen  A. Gionis  G. Das  and H. Mannila. The discrete basis problem. IEEE

Trans. Knowl. Data Eng.  20(10):1348–1362  2008.

I. Mukherjee and D. M. Blei. Relative performance guarantees for approximate inference in latent
Dirichlet allocation. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors  Adv. in
Neural Inform. Processing Syst. 21  pages 1129–1136. 2009.

I. Porteous  D. Newman  A. Ihler  A. Asuncion  P. Smyth  and M. Welling. Fast collapsed gibbs
sampling for latent dirichlet allocation. In Proc. of the 14th ACM SIGKDD Int. Conf. on Knowl-
edge Discovery and Data Mining  pages 569–577  New York  NY  USA  2008. ACM.

A. Schrijver. Combinatorial optimization. Polyhedra and eciency. Vol. A  volume 24 of Algorithms
and Combinatorics. Springer-Verlag  Berlin  2003. ISBN 3-540-44389-4. Paths  ﬂows  matchings 
Chapters 1–38.

J. K. Sepp¨anen  E. Bingham  and H. Mannila. A simple algorithm for topic identiﬁcation in 0-1
data. In Proc. of the 7th European Conf. on Principles and Practice of Knowledge Discovery in
Databases  pages 423–434. Springer-Verlag  2003.

Y. W. Teh  D. Newman  and M. Welling. A collapsed variational Bayesian inference algorithm for

latent Dirichlet allocation. In Adv. in Neural Inform. Processing Syst. 19  volume 19  2007.

A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Comput.  15:915–936  April

2003. ISSN 0899-7667.

9

,Nagarajan Natarajan
Inderjit Dhillon
Pradeep Ravikumar
Ambuj Tewari
Ralph Bourdoukan
Sophie Denève
Sepehr Assadi
Eric Balkanski
Renato Leme