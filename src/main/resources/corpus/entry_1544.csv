2007,Discriminative K-means for Clustering,We present a theoretical study on the discriminative clustering framework  recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However  the inherent relationship between subspace selection and clustering in this framework is not well understood  due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship  we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering  as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.,Discriminative K-means for Clustering

Jieping Ye

Arizona State University

Tempe  AZ 85287

Zheng Zhao

Arizona State University

Tempe  AZ 85287

jieping.ye@asu.edu

zhaozheng@asu.edu

Abstract

Mingrui Wu

mingrui.wu@tuebingen.mpg.de

MPI for Biological Cybernetics

T¨ubingen  Germany

We present a theoretical study on the discriminative clustering framework  re-
cently proposed for simultaneous subspace selection via linear discriminant analy-
sis (LDA) and clustering. Empirical results have shown its favorable performance
in comparison with several other popular clustering algorithms. However  the in-
herent relationship between subspace selection and clustering in this framework
is not well understood  due to the iterative nature of the algorithm. We show in
this paper that this iterative subspace selection and clustering is equivalent to ker-
nel K-means with a speciﬁc kernel Gram matrix. This provides signiﬁcant and
new insights into the nature of this subspace selection procedure. Based on this
equivalence relationship  we propose the Discriminative K-means (DisKmeans)
algorithm for simultaneous LDA subspace selection and clustering  as well as an
automatic parameter estimation procedure. We also present the nonlinear exten-
sion of DisKmeans using kernels. We show that the learning of the kernel matrix
over a convex set of pre-speciﬁed kernel matrices can be incorporated into the
clustering formulation. The connection between DisKmeans and several other
clustering algorithms is also analyzed. The presented theories and algorithms are
evaluated through experiments on a collection of benchmark data sets.

1 Introduction
Applications in various domains such as text/web mining and bioinformatics often lead to very high-
dimensional data. Clustering such high-dimensional data sets is a contemporary challenge  due to the
curse of dimensionality. A common practice is to project the data onto a low-dimensional subspace
through unsupervised dimensionality reduction such as Principal Component Analysis (PCA) [9]
and various manifold learning algorithms [1  13] before the clustering. However  the projection may
not necessarily improve the separability of the data for clustering  due to the inherent separation
between subspace selection (via dimensionality reduction) and clustering.
One natural way to overcome this limitation is to integrate dimensionality reduction and clustering
in a joint framework. Several recent work [5  10  16] incorporate supervised dimensionality reduc-
tion such as Linear Discriminant Analysis (LDA) [7] into the clustering framework  which performs
clustering and LDA dimensionality reduction simultaneously. The algorithm  called Discrimina-
tive Clustering (DisCluster) in the following discussion  works in an iterative fashion  alternating
between LDA subspace selection and clustering. In this framework  clustering generates the class
labels for LDA  while LDA provides the subspace for clustering. Empirical results have shown the
beneﬁts of clustering in a low dimensional discriminative space rather than in the principal com-
ponent space (generative). However  the integration between subspace selection and clustering in
DisCluster is not well understood  due to the intertwined and iterative nature of the algorithm.
In this paper  we analyze this discriminative clustering framework by studying several fundamental
and important issues: (1) What do we really gain by performing clustering in a low dimensional
discriminative space? (2) What is the nature of its iterative process alternating between subspace

1

selection and clustering? (3) Can this iterative process be simpliﬁed and improved? (4) How to
estimate the parameter involved in the algorithm?
The main contributions of this paper are summarized as follows: (1) We show that the LDA pro-
jection can be factored out from the integrated LDA subspace selection and clustering formulation.
This results in a simple trace maximization problem associated with a regularized Gram matrix of
the data  which is controlled by a regularization parameter λ; (2) The solution to this trace max-
imization problem leads to the Discriminative K-means (DisKmeans) algorithm for simultaneous
LDA subspace selection and clustering. DisKmeans is shown to be equivalent to kernel K-means 
where discriminative subspace selection essentially constructs a kernel Gram matrix for clustering.
This provides new insights into the nature of this subspace selection procedure; (3) The DisKmeans
algorithm is dependent on the value of the regularization parameter λ. We propose an automatic
parameter tuning process (model selection) for the estimation of λ; (4) We propose the nonlinear
extension of DisKmeans using the kernels. We show that the learning of the kernel matrix over
a convex set of pre-speciﬁed kernel matrices can be incorporated into the clustering formulation 
resulting in a semideﬁnite programming (SDP) [15]. We evaluate the presented theories and algo-
rithms through experiments on a collection of benchmark data sets.
2 Linear Discriminant Analysis and Discriminative Clustering
Consider a data set consisting of n data points {xi}n
centered  that is 
given by xi. In clustering  we aim to group the data {xi}n
be the cluster indicator matrix deﬁned as follows:

(cid:80)n
i=1 ∈ Rm. For simplicity  we assume the data is
i=1 xi/n = 0. Denote X = [x1 ···   xn] as the data matrix whose i-th column is
j=1. Let F ∈ Rn×k

i=1 into k clusters {Cj}k

F = {fi j}n×k  where fi j = 1  iff xi ∈ Cj.

We can deﬁne the weighted cluster indicator matrix as follows [4]:
L = [L1  L2 ···   Lk] = F (F T F )− 1
2 .

It follows that the j-th column of L is given by

(1)

(2)

Lj = (0  . . .   0 

(3)
where nj is the sample size of the j-th cluster Cj. Denote µj =
x/nj as the mean of the j-th
cluster Cj. The within-cluster scatter  between-cluster scatter  and total scatter matrices are deﬁned
as follows [7]:

1  . . .   1  0  . . .   0)T /n

j  
x∈Cj

Sw =

(xi − µj)(xi − µj)T   Sb =

njµjµT

j = XLLT X T   St = XX T .

(4)

k(cid:88)

(cid:88)

j=1

xi∈Cj

It follows that trace(Sw) captures the intra-cluster distance  and trace(Sb) captures the inter-cluster
distance. It can be shown that St = Sw + Sb.
Given the cluster indicator matrix F (or L)  Linear Discriminant Analysis (LDA) aims to compute
a linear transformation (projection) P ∈ Rm×d that maps each xi in the m-dimensional space to
a vector ˆxi in the d-dimensional space (d < m) as follows: xi ∈ IRm → ˆxi = P T xi ∈ IRd 
such that the following objective function is maximized [7]: trace
. Since
St = Sw + Sb  the optimal transformation matrix P is also given by maximizing the following
objective function:

(P T SwP )−1P T SbP

(cid:162)

(cid:161)

(5)
For high-dimensional data  the estimation of the total scatter (covariance) matrix is often not reliable.
The regularization technique [6] is commonly applied to improve the estimation as follows:

trace

.

(P T StP )−1P T SbP

(cid:162)

(cid:161)

˜St = St + λIm = XX T + λIm 

(6)

where Im is the identity matrix of size m and λ > 0 is a regularization parameter.
In Discriminant Clustering (DisCluster) [5  10  16]  the transformation matrix P and the weighted
cluster indicator matrix L are computed by maximizing the following objective function:

(cid:180)

f(L  P ) ≡ trace
= trace

(P T ˜StP )−1P T SbP
(P T (XX T + λIm)P )−1P T XLLT X T P

.

(7)

(cid:162)

(cid:179)
(cid:161)

2

(cid:80)

1
2

(cid:122) (cid:125)(cid:124) (cid:123)

nj

k(cid:88)

j=1

.

(cid:162)

(cid:161)

f(L  P ) = trace

The algorithm works in an intertwined and iterative fashion  alternating between the computation
of L for a given P and the computation of P for a given L. More speciﬁcally  for a given L  P is
given by the standard LDA procedure. Since trace(AB) = trace(BA) for any two matrices [8]  for
a given P   the objective function f(L  P ) can be expressed as:

LT X T P (P T (XX T + λIm)P )−1P T XL

(8)
Note that L is not an arbitrary matrix  but a weighted cluster indicator matrix  as deﬁned in Eq. (3).
The optimal L can be computed by applying the gradient descent strategy [10] or by solving a kernel
K-means problem [5  16] with X T P (P T (XX T + λIm)P )−1P T X as the kernel Gram matrix [4].
The algorithm is guaranteed to converge in terms of the value of the objective function f(L  P )  as
the value of f(L  P ) monotonically increases and is bounded from above.
Experiments [5  10  16] have shown the effectiveness of DisCluster in comparison with several other
popular clustering algorithms. However  the inherent relationship between subspace selection via
LDA and clustering is not well understood  and there is need for further investigation. We show
in the next section that the iterative subspace selection and clustering in DisCluster is equivalent
to kernel K-means with a speciﬁc kernel Gram matrix. Based on this equivalence relationship 
we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace
selection and clustering.
3 DisKmeans: Discriminative K-means with a Fixed λ
Assume that λ is a ﬁxed positive constant. Let’s consider the maximization of the function in Eq. (7):
(9)
Here  P is a transformation matrix and L is a weighted cluster indicator matrix as in Eq. (3). It
follows from the Representer Theorem [14] that the optimal transformation matrix P ∈ IRm×d can
be expressed as P = XH  for some matrix H ∈ IRn×d. Denote G = X T X as the Gram matrix 
which is symmetric and positive semideﬁnite. It follows that

(P T (XX T + λIm)P )−1P T XLLT X T P

f(L  P ) = trace

(cid:161)

(cid:162)

.

H T GLLT GH

f(L  P ) = trace

H T (GG + λG) H

(10)
We show that the matrix H can be factored out from the objective function in Eq. (10)  thus dramat-
ically simplifying the optimization problem in the original DisCluster algorithm. The main result is
summarized in the following theorem:
Theorem 3.1. Let G be the Gram matrix deﬁned as above and λ > 0 be the regularization param-
eter. Let L∗ and P ∗ be the optimal solution to the maximization of the objective function f(L  P ) in
Eq. (7). Then L∗ solves the following maximization problem:
In − (In +

L∗ = arg max

G)−1

(cid:182)

(cid:181)

(cid:181)

(cid:182)

trace

(11)

LT

L

.

.

(cid:162)−1

(cid:179)(cid:161)

(cid:180)

L

1
λ

Proof. Let G = UΣU T be the Singular Value Decomposition (SVD) [8] of G  where U ∈ IRn×n
is orthogonal  Σ = diag (σ1 ···   σt  0 ···   0) ∈ IRn×n is diagonal  and t = rank(G). Let U1 ∈
IRn×t consist of the ﬁrst t columns of U and Σt = diag (σ1 ···   σt) ∈ IRt×t . Then

(cid:179)

2 ΣtU T

G = UΣU T = U1ΣtU T
1 .

(12)
Denote R = (Σ2
1 L and let R = MΣRN T be the SVD of R  where M and
N are orthogonal and ΣR is diagonal with rank(ΣR) = rank(R) = q. Deﬁne the matrix Z as
Z = Udiag
  where diag(A  B) is a block diagonal matrix. It follows that
˜Σ 0
0
0

t + λΣt)− 1
t + λΣt)− 1

  Z T (GG + λG) Z =

Z T(cid:161)

(cid:180)
(cid:181)

2 M  In−t

GLLT G

(cid:182)

(cid:181)

(cid:182)

Z =

(13)

(Σ2

(cid:162)

It
0

0
0

 

where ˜Σ = (ΣR)2 is diagonal with non-increasing diagonal entries. It can be veriﬁed that

(cid:179)

(cid:180)

f(L  P ) ≤ trace

˜Σ

= trace

(GG + λG)+ GLLT G

(cid:180)

(cid:179)
(cid:179)
(cid:181)

(cid:181)

(cid:180)

(cid:182)

(cid:182)

= trace

= trace

LT G (GG + λG)+ GL
G)−1

In − (In +

LT

1
λ

L

 

(14)

where the equality holds when P = XH and H consists of the ﬁrst q columns of Z.

3

3.1 Computing the Weighted Cluster Matrix L
The weighted cluster indicator matrix L solving the maximization problem in Eq. (11) can be com-
puted by solving a kernel K-means problem [5] with the kernel Gram matrix given by

(cid:181)

(cid:182)−1

˜G = In −

In +

1
λ

G

.

(15)

Thus  DisCluster is equivalent to a kernel K-means problem. We call the algorithm Discriminative
K-means (DisKmeans).
3.2 Constructing the Kernel Gram Matrix via Subspace Selection
The kernel Gram matrix in Eq. (15) can be expressed as

˜G = U diag (σ1/(λ + σ1)  σ2/(λ + σ2) ···   σn/(λ + σn)) U T .

(16)
Recall that the original DisCluster algorithm involves alternating LDA subspace selection and clus-
tering. The analysis above shows that the LDA subspace selection in DisCluster essentially con-
structs a kernel Gram matrix for clustering. More speciﬁcally  all the eigenvectors in G is kept
unchanged  while the following transformation is applied to the eigenvalues:

Φ(σ) = σ/(λ + σ).

This elucidates the nature of the subspace selection procedure in DisCluster. The clustering algo-
rithm is dramatically simpliﬁed by removing the iterative subspace selection. We thus address issues
(1)–(3) in Section 1. The last issue will be addressed in Section 4 below.
3.3 Connection with Other Clustering Approaches
Consider the limiting case when λ → ∞. It follows from Eq. (16) that ˜G → G/λ. The optimal L is
thus given by solving the following maximization problem:

(cid:161)

(cid:162)

arg max

trace

L

LT GL

.

The solution is given by the standard K-means clustering [4  5].
Consider the other extreme case when λ → 0. It follows from Eq. (16) that ˜G → U T
1 U1. Note that
the columns of U1 form the full set of (normalized) principal components [9]. Thus  the algorithm
is equivalent to clustering in the (full) principal component space.
4 DisKmeansλ: Discriminative K-means with Automatically Tuned λ
Our experiments show that the value of the regularization parameter λ has a signiﬁcant impact on
the performance of DisKmeans. In this section  we show how to incorporate the automatic tuning
of λ into the optimization framework  thus addressing issue (4) in Section 1.
The maximization problem in Eq. (11) is equivalent to the minimization of the following function:

(cid:195)

(cid:181)

(cid:182)−1

(cid:33)

1
λ

trace

LT

In +

G

L

.

(17)

It is clear that a small value of λ leads to a small value of the objective function in Eq. (17). To
overcome this problem  we include an additional penalty term to control the eigenvalues of the
matrix In + 1

λ G. This leads to the following optimization problem:

g(L  λ) ≡ trace

min
L λ

LT

In +

L

+ log det

In +

G

.

(18)

(cid:195)

(cid:181)

(cid:182)−1

(cid:33)

1
λ

G

(cid:181)

(cid:182)

1
λ

Note that the objective function in Eq. (18) is closely related to the negative log marginal likelihood
function in Gaussian Process [12] with In + 1
λ G as the covariance matrix. We have the following
main result for this section:
Theorem 4.1. Let G be the Gram matrix deﬁned above and let L be a given weighted cluster
1 be the SVD of G with Σt = diag (σ1 ···   σt)
indicator matrix. Let G = UΣU T = U1ΣtU T
as in Eq. (12)  and ai be the i-th diagonal entry of the matrix U T
1 LLT U1. Then for a ﬁxed L 

4

t(cid:88)

i=1

t(cid:88)

the optimal λ∗ solving the optimization problem in Eq. (18) is given by minimizing the following
objective function:

λai
λ + σi

+ log

1 + σi
λ

.

(19)

Proof. Let U = [U1  U2]  that is  U2 is the orthogonal complement of U1. It follows that

(cid:181)

(cid:195)

log det

(cid:181)

In +

G

1
λ

(cid:182)−1

= log det

(cid:195)

(cid:182)
(cid:33)

(cid:181)

1
λ

G

trace

LT

In +

L

= trace

LT U1

It +

Σt

U T

1 L

+ trace

LT U2U T

2 L

It +

Σ1

=

log (1 + σi/λ) .

(cid:161)

(20)

(cid:162)

(cid:179)

1
λ

(cid:181)

(cid:161)

(cid:180)

(cid:182)

1
λ

i=1

t(cid:88)
(cid:182)−1
(cid:161)
(cid:162)

(cid:33)

(cid:162)

=

(1 + σi/λ)−1ai + trace

LT U2U T

2 L

 

(21)

The result follows as the second term in Eq. (21)  trace

i=1

LT U2U T

2 L

  is a constant.

We can thus solve the optimization problem in Eq. (18) iteratively as follows: For a ﬁxed λ  we
update L by maximizing the objective function in Eq. (17)  which is equivalent to the DisKmeans
algorithm; for a ﬁxed L  we update λ by minimizing the objective function in Eq. (19)  which is a
single-variable optimization and can be solved efﬁciently using the line search method. We call the
algorithm DisKmeansλ  whose solution depends on the initial value of λ.
5 Kernel DisKmeans: Nonlinear Discriminative K-means using the kernels
The DisKmeans algorithm can be easily extended to deal with nonlinear data using the kernel trick.
Kernel methods [14] work by mapping the data into a high-dimensional feature space F equipped
with an inner product through a nonlinear mapping φ : IRm → F. The nonlinear mapping can
be implicitly speciﬁed by a symmetric kernel function K  which computes the inner product of the
images of each data pair in the feature space. For a given training data set {xi}n
i=1  the kernel Gram
matrix GK is deﬁned as follows: GK(i  j) = (φ(xi)  φ(xj)). For a given GK  the weighted cluster
matrix L = [L1 ···   Lk] in kernel DisKmeans is given by minimizing the following objective
function:

(cid:195)

(cid:181)

(cid:182)−1

(cid:33)

k(cid:88)

(cid:181)

(cid:182)−1

trace

LT

In +

L

=

LT
j

In +

GK

Lj.

(22)

j=1

The performance of kernel DisKmeans is dependent on the choice of the kernel Gram matrix.
Following [11]  we assume that GK is restricted to be a convex combination of a given set
of kernel Gram matrices {Gi}(cid:96)
i=1 satisfy
i=1 may be

(cid:80)(cid:96)
i=1 θiGi  where the coefﬁcients {θi}(cid:96)
If L is given  the optimal coefﬁcients {θi}(cid:96)

(cid:80)(cid:96)
i=1 θitrace(Gi) = 1 and θi ≥ 0 ∀i.

computed by solving a Semideﬁnite programming (SDP) problem as follows:
Theorem 5.1. Let GK be constrained to be a convex combination of a given set of kernel matrices
{Gi}(cid:96)
i=1 θiGi satisfying the constraints deﬁned above. Then the optimal GK
minimizing the objective function in Eq. (22) is given by solving the following SDP problem:

i=1 as GK =

i=1 as GK =

(cid:80)(cid:96)

1
λ

GK

1
λ

min

t1 ···  tk θ

(cid:181)

In + 1
λ

θi ≥ 0 ∀i 

tj

j=1

k(cid:88)
(cid:80)(cid:96)
(cid:96)(cid:88)
(cid:162)−1

LT
j

i=1

s.t.

(cid:161)

Proof. It follows as LT
j

(cid:182)

(cid:186) 0  for j = 1 ···   k 

i=1 θi ˜Gi Lj
tj

θi trace(Gi) = 1.

(cid:181)

I + 1
λ

(cid:80)(cid:96)

i=1 θi ˜Gi Lj
LT
tj
j

(cid:182)

(23)

(cid:186) 0.

In + 1

λ GK

Lj ≤ ti is equivalent to

5

This leads to an iterative algorithm alternating between the computation of the kernel Gram matrix
GK and the computation of the cluster indicator matrix L. The parameter λ can also be incorporated
into the SDP formulation by treating the identity matrix In as one of the kernel Gram matrix as in
[11]. The algorithm is named Kernel DisKmeansλ. Note that unlike the kernel learning in [11]  the
class label information is not available in our formulation.
6 Empirical Study
In this section  we empirically study the properties of DisKmeans and its variants  and evaluate the
performance of the proposed algorithms in comparison with several other representative algorithms 
including Locally Linear Embedding (LLE) [13] and Laplacian Eigenmap (Leigs) [1].
Experiment Setup:
All algorithms were implemented us-
ing Matlab and experiments were conducted on a PEN-
TIUM IV 2.4G PC with 1.5GB RAM. We test
these al-
gorithms on eight benchmark data sets.
They are ﬁve
banding  soybean  segment  satimage 
UCI data sets [2]:
pendigits; one biological data set:
leukemia (http://www.
upo.es/eps/aguilar/datasets.html) and two image
data sets: ORL (http://www.uk.research.att.com/
facedatabase.html  sub-sampled to a size of 100*100
= 10000 from 10 persons) and USPS (ftp://ftp.kyb.tuebingen.mpg.de/pub/bs/
data/). See Table 1 for more details. To make the results of different algorithms comparable 
we ﬁrst run K-means and the clustering result of K-means is used to construct the set of k initial
centroids  for all experiments. This process is repeated for 50 times with different sub-samples from
the original data sets. We use two standard measurements: the accuracy (ACC) and the normalized
mutual information (NMI) to measure the performance.

Table 1: Summary of benchmark data sets
Data Set
banding
soybean
segment
pendigits
satimage
leukemia
ORL
USPS

# CL
(k)
2
15
7
10
6
2
10
10

# DIM # INST
(m)
29
35
19
16
36
7129
10304
256

(n)
238
562
2309
10992
6435
72
100
9298

Figure 1: The effect of the regularization parameter λ on DisKmeans and Discluster.

Effect of the regularization parameter λ: Figure 1 shows the accuracy (y-axis) of DisKmeans
and DisCluster for different λ values (x-axis). We can observe that λ has a signiﬁcant impact on
the performance of DisKmeans. This justiﬁes the development of an automatic parameter tuning
process in Section 4. We can also observe from the ﬁgure that when λ → ∞  the performance of
DisKmeans approaches to that of K-means on all eight benchmark data sets. This is consistent with
our theoretical analysis in Section 3.3. It is clear that in many cases  λ = 0 is not the best choice.
Effect of parameter tuning in DisKmeansλ: Figure 2 shows the accuracy of DisKmeansλ using
4 data sets. In the ﬁgure  the x-axis denotes the different λ values used as the starting point for
DisKmeansλ. The result of DisKmeans (without parameter tuning) is also presented for comparison.
We can observe from the ﬁgure that in many cases the tuning process is able to signiﬁcantly improve
the performance. We observe similar trends on other four data sets and the results are omitted.

6

10−610−410−21001021041060.7620.7630.7640.7650.7660.7670.7680.7690.770.7710.772BandingACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.6240.6260.6280.630.6320.6340.6360.6380.640.6420.644soybeanACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.630.640.650.660.670.680.69segmentACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.680.6850.690.6950.7pendigitsACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.610.620.630.640.650.660.670.680.690.70.71satimageACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.7350.740.7450.750.7550.760.7650.770.7750.78leukemiaACCl K−meansDisClusterDisKmeans10−510010510100.7350.7360.7370.7380.7390.740.7410.7420.7430.7440.745ORLACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.540.560.580.60.620.640.660.680.70.72USPSACCl K−meansDisClusterDisKmeansFigure 2: The effect of the parameter tuning in DisKmeansλ using 4 data sets. The x-axis denotes the different
λ values used as the starting point for DisKmeansλ.

Figure 2 also shows that the tuning process is dependent on the initial value of λ due to its non-
convex optimization  and when λ → ∞  the effect of the tuning process become less pronounced.
Our results show that a value of λ  which is neither too large nor too small works well.

Figure 3: Comparison of the trace value achieved by DisKmean and DisCluster. The x-axis denotes the
number of iterations in Discluster. The trace value of DisCluster is bounded from above by that of DisKmean.

DisKmean versus DisCluster: Figure 3 compares the trace value achieved by DisKmean and
the trace value achieved in each iteration of DisCluster on 4 data sets for a ﬁxed λ.
It is clear
that the trace value of DisCluster increases in each iteration but is bounded from above by that of
DisKmean. We observe a similar trend on the other four data sets and the results are omitted. This is
consistent with our analysis in Section 3 that both algorithms optimize the same objective function 
and DisKmean is a direct approach for the trace maximization without the iterative process.
Clustering evaluation: Table 2 presents the accuracy (ACC) and normalized mutual information
(NMI) results of various algorithms on all eight data sets. In the table  DisKmeans (or DisCluster)
with “max” and “ave” stands for the maximal and average performance achieved by DisKmeans and
DisCluster using λ from a wide range between 10−6 and 106. We can observe that DisKmeansλ is
competitive with other algorithms. It is clear that the average performance of DisKmeansλ is robust
against different initial values of λ. We can also observe that the average performance of DisKmeans
and DisCluster is quite similar  while DisCluster is less sensitive to the value of λ.

7 Conclusion
In this paper  we analyze the discriminative clustering (DisCluster) framework  which integrates
subspace selection and clustering. We show that the iterative subspace selection and clustering in
DisCluster is equivalent to kernel K-means with a speciﬁc kernel Gram matrix. We then propose the
DisKmeans algorithm for simultaneous LDA subspace selection and clustering  as well as an auto-
matic parameter tuning procedure. The connection between DisKmeans and several other clustering
algorithms is also studied. The presented analysis and algorithms are veriﬁed through experiments
on a collection of benchmark data sets.
We present the nonlinear extension of DisKmeans in Section 5. Our preliminary studies have shown
the effectiveness of Kernel DisKmeansλ in learning the kernel Gram matrix. However  the SDP
formulation is limited to small-sized problems. We plan to explore efﬁcient optimization techniques
for this problem. Partial label information may be incorporated into the proposed formulations. This
leads to semi-supervised clustering [3]. We plan to examine various semi-learning techniques within
the proposed framework and their effectiveness for clustering from both labeled and unlabeled data.

7

10−610−410−21001021041060.60.620.640.660.680.70.72satimageACCl DisKmeansDisKmeansl10−610−410−21001021041060.680.6820.6840.6860.6880.690.6920.6940.6960.6980.7pendigitsACCl DisKmeansDisKmeansl10−510010510100.730.7320.7340.7360.7380.740.7420.7440.7460.7480.75ORLACCl DisKmeansDisKmeanl10−610−410−21001021041060.540.560.580.60.620.640.660.680.70.72USPSACCl DisKmeansDisKmeansl123456780.0840.0860.0880.090.0920.0940.0960.098 satimageTRACEl DisClusterDisKmeans123450.3410.3420.3430.3440.3450.3460.347pendigitsTRACEl DisClusterDisKmeans12345670.2140.2160.2180.220.2220.2240.2260.2280.23segmentTRACEl DisClusterDisKmeans11.522.533.544.550.0250.02550.0260.02650.0270.0275USPSTRACEl DisClusterDisKmeansTable 2: Accuracy (ACC) and Normalized Mutual Information (NMI) results on 8 data sets. “max” and “ave”
stand for the maximal and average performance achieved by DisKmeans and DisCluster using λ from a wide
range of values between 10−6 and 106. We present the result of DisKmeansλ with different initial λ values.
LLE stands for Local Linear Embedding and LEI for Laplacian Eigenmap. “AVE” stands for the mean of ACC
or NMI on 8 data sets for each algorithm.

DisKmeans

max

ave

DisCluster

max

ave

ACC

Data Sets

banding
soybean
segment
pendigits
satimage
leukemia
ORL
USPS
AVE

banding
soybean
segment
pendigits
satimage
leukemia
ORL
USPS
AVE

0.771
0.641
0.687
0.699
0.701
0.775
0.744
0.712
0.716

0.225
0.707
0.632
0.669
0.593
0.218
0.794
0.647
0.561

0.768
0.634
0.664
0.690
0.651
0.763
0.738
0.628
0.692

0.221
0.701
0.612
0.656
0.537
0.199
0.789
0.544
0.532

0.771
0.633
0.676
0.696
0.654
0.738
0.739
0.692
0.700

0.225
0.698
0.615
0.660
0.551
0.163
0.789
0.629
0.541

10−2

0.771
0.639
0.664
0.700
0.696
0.738
0.749
0.684
0.705

0.225
0.706
0.629
0.661
0.597
0.163
0.800
0.612
0.549

DisKmeansλ
10−1
100

0.771
0.639
0.659
0.696
0.712
0.753
0.743
0.702
0.709

0.225
0.707
0.625
0.658
0.608
0.185
0.795
0.637
0.555

0.771
0.638
0.671
0.696
0.696
0.738
0.748
0.680
0.705

0.225
0.704
0.628
0.658
0.596
0.163
0.801
0.609
0.548

101

0.771
0.637
0.680
0.697
0.683
0.738
0.748
0.684
0.705

0.225
0.704
0.632
0.660
0.586
0.163
0.800
0.612
0.548

LLE

LEI

0.648
0.630
0.594
0.599
0.627
0.714
0.733
0.631
0.647

0.093
0.691
0.539
0.577
0.493
0.140
0.784
0.569
0.486

0.764
0.649
0.663
0.697
0.663
0.686
0.317
0.700
0.642

0.213
0.709
0.618
0.645
0.548
0.043
0.327
0.640
0.468

0.767
0.632
0.672
0.690
0.642
0.738
0.738
0.683
0.695

0.219
0.696
0.608
0.654
0.541
0.163
0.788
0.613
0.535

NMI

Acknowledgments

This research is sponsored by the National Science Foundation Grant IIS-0612069.

References
[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. In

NIPS  2003.

[2] C.L. Blake and C.J. Merz. UCI repository of machine learning databases  1998.
[3] O. Chapelle  B. Sch¨olkopf  and A. Zien. Semi-Supervised Learning. The MIT Press  2006.
[4] I. S. Dhillon  Y. Guan  and B. Kulis. A uniﬁed view of kernel k-means  spectral clustering and graph

partitioning. Technical report  Department of Computer Sciences  University of Texas at Austin  2005.

[5] C. Ding and T. Li. Adaptive dimension reduction using discriminant analysis and k-means clustering. In

ICML  2007.

[6] J. H. Friedman. Regularized discriminant analysis. JASA  84(405):165–175  1989.
[7] K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press.
[8] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins Univ. Press  1996.
[9] I.T. Jolliffe. Principal Component Analysis. Springer; 2nd edition  2002.
[10] F. De la Torre Frade and T. Kanade. Discriminative cluster analysis. In ICML  pages 241–248  2006.
[11] G.R.G. Lanckriet  N. Cristianini  P. Bartlett  L. E. Ghaoui  and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. JMLR  5:27–72  2004.

[12] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. The MIT Press  2006.
[13] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science 

290:2323–2326  2000.

[14] B. Sch¨olkopf and A. Smola. Learning with Kernels: Support Vector Machines  Regularization  Optimiza-

tion and Beyond. MIT Press  2002.

[15] L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review  38:49–95  1996.
[16] J. Ye  Z. Zhao  and H. Liu. Adaptive distance metric learning for clustering. In CVPR  2007.

8

,Harish Ramaswamy
Shivani Agarwal
Ambuj Tewari
Mehryar Mohri
Scott Yang
Pratik Kumar Jawanpuria
Maksim Lapin
Matthias Hein
Bernt Schiele