2014,Extreme bandits,In many areas of medicine  security  and life sciences  we want to allocate limited resources to different sources in order to detect extreme values. In this paper  we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory  the most commonly optimized property is the regret with respect to the maximum mean reward. However  in other problems such as network intrusion detection  we are interested in detecting the most extreme value output by the sources. Therefore  in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the ExtremeHunter algorithm  provide its analysis  and evaluate it empirically on synthetic and real-world experiments.,Extreme bandits

Alexandra Carpentier

Statistical Laboratory  CMS
University of Cambridge  UK

Michal Valko
SequeL team

INRIA Lille - Nord Europe  France

a.carpentier@statslab.cam.ac.uk

michal.valko@inria.fr

Abstract

In many areas of medicine  security  and life sciences  we want to allocate lim-
ited resources to different sources in order to detect extreme values. In this paper 
we study an efﬁcient way to allocate these resources sequentially under limited
feedback. While sequential design of experiments is well studied in bandit theory 
the most commonly optimized property is the regret with respect to the maximum
mean reward. However  in other problems such as network intrusion detection  we
are interested in detecting the most extreme value output by the sources. There-
fore  in our work we study extreme regret which measures the efﬁciency of an al-
gorithm compared to the oracle policy selecting the source with the heaviest tail.
We propose the EXTREMEHUNTER algorithm  provide its analysis  and evaluate
it empirically on synthetic and real-world experiments.

1

Introduction

We consider problems where the goal is to detect outstanding events or extreme values in domains
such as outlier detection [1]  security [18]  or medicine [17]. The detection of extreme values is
important in many life sciences  such as epidemiology  astronomy  or hydrology  where  for example 
we may want to know the peak water ﬂow. We are also motivated by network intrusion detection
where the objective is to ﬁnd the network node that was compromised  e.g.  by seeking the one
creating the most number of outgoing connections at once. The search for extreme events is typically
studied in the ﬁeld of anomaly detection  where one seeks to ﬁnd examples that are far away from
the majority  according to some problem-speciﬁc distance (cf. the surveys [8  16]).
In anomaly detection research  the concept of anomaly is ambiguous and several deﬁnitions ex-
ist [16]: point anomalies  structural anomalies  contextual anomalies  etc. These deﬁnitions are
often followed by heuristic approaches that are seldom analyzed theoretically. Nonetheless  there
exist some theoretical characterizations of anomaly detection. For instance  Steinwart et al. [19]
consider the level sets of the distribution underlying the data  and rare events corresponding to rare
level sets are then identiﬁed as anomalies. A very challenging characteristic of many problems in
anomaly detection is that the data emitted by the sources tend to be heavy-tailed (e.g.  network traf-
ﬁc [2]) and anomalies come from the sources with the heaviest distribution tails. In this case  rare
level sets of [19] correspond to distributions’ tails and anomalies to extreme values. Therefore  we
focus on the kind of anomalies that are characterized by their outburst of events or extreme values 
as in the setting of [22] and [17].
Since in many cases  the collection of the data samples emitted by the sources is costly  it is im-
portant to design adaptive-learning strategies that spend more time sampling sources that have a
higher risk of being abnormal. The main objective of our work is the active allocation of the sam-
pling resources for anomaly detection  in the setting where anomalies are deﬁned as extreme values.
Speciﬁcally  we consider a variation of the common setting of minimal feedback also known as
the bandit setting [14]: the learner searches for the most extreme value that the sources output by
probing the sources sequentially. In this setting  it must carefully decide which sources to observe

1

because it only receives the observation from the source it chooses to observe. As a consequence 
it needs to allocate the sampling time efﬁciently and should not waste it on sources that do not have
an abnormal character. We call this speciﬁc setting extreme bandits  but it is also known as max-k
problem [9  21  20]. We emphasize that extreme bandits are poles apart from classical bandits  where
the objective is to maximize the sum of observations [3]. An effective algorithm for the classical
bandit setting should focus on the source with the highest mean  while an effective algorithm for the
extreme bandit problem should focus on the source with the heaviest tail. It is often the case that
a heavy-tailed source has a small mean  which implies that the classical bandit algorithms perform
poorly for the extreme bandit problem.
The challenging part of our work dwells in the active sampling strategy to detect the heaviest tail
under the limited bandit feedback. We proffer EXTREMEHUNTER  a theoretically founded algo-
rithm  that sequentially allocates the resources in an efﬁcient way  for which we prove performance
guarantees. Our algorithm is efﬁcient under a mild semi-parametric assumption common in ex-
treme value theory  while known results by [9  21  20] for the extreme bandit problem only hold in
a parametric setting (see Section 4 for a detailed comparison).

2 Learning model for extreme bandits

In this section  we formalize the active (bandit) setting and characterize the measure of performance
for any algorithm π. The learning setting is deﬁned as follows. Every time step  each of the K arms
(sources) emits a sample Xk t ∼ Pk  unknown to the learner. The precise characteristics of Pk are
deﬁned in Section 3. The learner π then chooses some arm It and then receives only the sample
XIt t. The performance of π is evaluated by the most extreme value found and compared to the
most extreme value possible. We deﬁne the reward of a learner π as:

Gπ

n = max
t≤n

XIt t

The optimal oracle strategy is the one that chooses at each time the arm with the highest potential
revealing the highest value  i.e.  the arm ∗ with the heaviest tail. Its expected reward is then:

E [G∗

n] = max
k≤K

E

max
t≤n

Xk t

(cid:20)

(cid:20)

(cid:21)

(cid:21)

The goal of learner π is to get as close as possible to the optimal oracle strategy. In other words  the
aim of π is to minimize the expected extreme regret:
Deﬁnition 1. The extreme regret in the bandit setting is deﬁned as:

(cid:20)

(cid:21)

max
t≤n

Xk t

max
t≤n

XIt t

− E

E [Rπ

n] = E [G∗

n] − E [Gπ

n] = max
k≤K

E

3 Heavy-tailed distributions

In this section  we formally deﬁne our observation model. Let X1  . . .   Xn be n i.i.d. observations
from a distribution P . The behavior of the statistic maxi≤n Xi is studied by extreme value theory.
One of the main results is the Fisher-Tippett-Gnedenko theorem [11  12] that characterizes the lim-
iting distribution of this maximum as n converges to inﬁnity. Speciﬁcally  it proves that a rescaled
version of this maximum converges to one of the three possible distributions: Gumbel  Fr´echet  or
Weibull. This rescaling factor depends on n. To be concise  we write “maxi≤n Xi converges to a
distribution” to refer to the convergence of the rescaled version to a given distribution. The Gum-
bel distribution corresponds to the limiting distribution of the maximum of ‘not too heavy tailed’
distributions  such as sub-Gaussian or sub-exponential distributions. The Weibull distribution co-
incides with the behaviour of the maximum of some speciﬁc bounded random variables. Finally 
the Fr´echet distribution corresponds to the limiting distribution of the maximum of heavy-tailed
random variables. As many interesting problems concern heavy-tailed distributions  we focus on
Fr´echet distributions in this work. The distribution function of a Fr´echet random variable is deﬁned
for x ≥ m  and for two parameters α  s as:

P (x) = exp(cid:8)−(cid:0) x−m

(cid:1)α(cid:9) .

s

2

|1 − P (x) − Cx−α|

lim
x→∞

x−α

= 0 

In this work  we consider positive distributions P : [0 ∞) → [0  1]. For α > 0  the Fisher-
Tippett-Gnedenko theorem also states that the statement ‘P converges to an α-Fr´echet distribution’
is equivalent to the statement ‘1− P is a −α regularly varying function in the tail’. These statements
are slightly less restrictive than the deﬁnition of approximately α-Pareto distributions1  i.e.  that there
exists C such that P veriﬁes:

(1)
or equivalently that P (x) = 1 − Cx−α + o(x−α). If and only if 1 − P is −α regularly varying in
the tail  then the limiting distribution of maxi Xi is an α-Fr´echet distribution. The assumption of
−α regularly varying in the tail is thus the weakest possible assumption that ensures that the (prop-
erly rescaled) maximum of samples emitted by a heavy tailed distributions has a limit. Therefore 
the very related assumption of approximate Pareto is almost minimal  but it is (provably) still not
restrictive enough to ensure a convergence rate. For this reason  it is natural to introduce an assump-
tion that is slightly stronger than (1). In particular  we assume  as it is common in the extreme value
literature  a second order Pareto condition also known as the Hall condition [13].
Deﬁnition 2. A distribution P is (α  β  C  C(cid:48))-second order Pareto (α  β  C  C(cid:48) > 0) if for x ≥ 0:

By this deﬁnition  P (x) = 1 − Cx−α + O(cid:0)x−α(1+β)(cid:1)  which is stronger than the assumption

(cid:12)(cid:12)1 − P (x) − Cx−α(cid:12)(cid:12) ≤ C(cid:48)x−α(1+β)

P (x) = 1 − Cx−α + o(x−α)  but similar for small β.
Remark 1. In the deﬁnition above  β deﬁnes the rate of the convergence (when x diverges to inﬁnity)
of the tail of P to the tail of a Pareto distribution 1 − Cx−α. The parameter α characterizes the
heaviness of the tail: The smaller the α  the heavier the tail. In the reminder of the paper  we will be
therefore concerned with learning the α and identifying the smallest one among the sources.

4 Related work

There is a vast body of research in ofﬂine anomaly detection which looks for examples that deviate
from the rest of the data  or that are not expected from some underlying model. A comprehensive
review of many anomaly detection approaches can be found in [16] or [8]. There has been also some
work in active learning for anomaly detection [1]  which uses a reduction to classiﬁcation. In online
anomaly detection  most of the research focuses on studying the setting where a set of variables is
monitored. A typical example is the monitoring of cold relief medications  where we are interested
in detecting an outbreak [17]. Similarly to our focus  these approaches do not look for outliers in a
broad sense but rather for the unusual burst of events [22].
In the extreme values settings above  it is often assumed  that we have full information about each
variable. This is in contrast to the limited feedback or a bandit setting that we study in our work.
There has been recently some interest in bandit algorithms for heavy-tailed distributions [4]. How-
ever the goal of [4] is radically different from ours as they maximize the sum of rewards and not
the maximal reward. Bandit algorithms have been already used for network intrusion detection [15] 
but they typically consider classical or restless setting. [9  21  20] were the ﬁrst to consider the
extreme bandits problem  where our setting is deﬁned as the max-k problem. [21] and [9] con-
sider a fully parametric setting. The reward distributions are assumed to be exactly generalized
extreme value distributions. Speciﬁcally  [21] assumes that the distributions are exactly Gumbel 
P (x) = exp(−(x − m)/s))  and [9]  that the distributions are exactly of Gumbel or Fr´echet
P (x) = exp(−(x − m)α/(sα))). Provided that these assumptions hold  they propose an algo-
rithm for which the regret is asymptotically negligible when compared to the optimal oracle reward.
These results are interesting since they are the ﬁrst for extreme bandits  but their parametric assump-
tion is unlikely to hold in practice and the asymptotic nature of their bounds limits their impact.
Interestingly  the objective of [20] is to remove the parametric assumptions of [21  9] by offering
the THRESHOLDASCENT algorithm. However  no analysis of this algorithm for extreme bandits is
provided. Nonetheless  to the best of our knowledge  this is the closest competitor for EXTREME-
HUNTER and we empirically compare our algorithm to THRESHOLDASCENT in Section 7.

1We recall the deﬁnition of the standard Pareto distribution as a distribution P   where for some constants α

and C  we have that for x ≥ C 1/α  P = 1 − Cx−α.

3

In this paper we also target the extreme bandit setting  but contrary to [9  21  20]  we only make a
semi-parametric assumption on the distribution; the second order Pareto assumption (Deﬁnition 2) 
which is standard in extreme value theory (see e.g.  [13  10]). This is light-years better and sig-
niﬁcantly weaker than the parametric assumptions made in the prior works for extreme bandits.
Furthermore  we provide a ﬁnite-time regret bound for our more general semi-parametric setting
(Theorem 2)  while the prior works only offer asymptotic results. In particular  we provide an up-
per bound on the rate at which the regret becomes negligible when compared to the optimal oracle
reward (Deﬁnition 1).

5 Extreme Hunter

In this section  we present our main results. In particular  we present the algorithm and the main
theorem that bounds its extreme regret. Before that  we ﬁrst provide an initial result on the expecta-
tion of the maximum of second order Pareto random variables which will set the benchmark for the
oracle regret. We ﬁrst characterize the expectation of the maximum of second order Pareto distribu-
tions. The following lemma states that the expectation of the maximum of i.i.d. second order Pareto
samples is equal  up to a negligible term  to the expectation of the maximum of i.i.d. Pareto samples.
This result is crucial for assessing the benchmark for the regret  in particular the expected value of
the maximal oracle sample. Theorem 1 is based on Lemma 3  both provided in the appendix.
Theorem 1. Let X1  . . .   Xn be n i.i.d. samples drawn according to (α  β  C  C(cid:48))-second order
Pareto distribution P (see Deﬁnition 2). If α > 1  then:

(cid:12)(cid:12)(cid:12)E(max

i

Xi) − (nC)1/αΓ(cid:0)1− 1

α

(cid:1)(cid:12)(cid:12)(cid:12) ≤ 4D2

n (nC)1/α + 2C(cid:48)Dβ+1

Cβ+1nβ (nC)1/α + B = o

(cid:16)

(nC)1/α(cid:17)

 

where D2  D1+β > 0 are some universal constants  and B is deﬁned in the appendix (9).

Theorem 1 implies that the optimal strategy in hindsight attains the following expected reward:

(cid:104)

(Ckn)1/αk Γ(cid:0)1− 1

α

(cid:1)(cid:105)

E [G∗

n] ≈ max

k

Input:

Initialize:

n] − E [Gπ

Γ(cid:0)1− 1

Run:
for t = 1 to n do

Algorithm 1 EXTREMEHUNTER

(cid:1) ≈ n1/α∗

K: number of arms
n: time horizon
b: where b ≤ βk for all k ≤ K
N: minimum number of pulls of each arm
Tk ← 0 for all k ≤ K
δ ← exp(− log2 n)/(2nK)

Our objective is therefore to ﬁnd a learner π
such that E [G∗
n] is negligible when
compared to E[G∗
n]  i.e.  when compared to
where ∗ is the
(nC∗)1/α∗
α∗
optimal arm.
From the discussion above  we know that the
minimization of the extreme regret is linked
with the identiﬁcation of the arm with the heav-
iest tail. Our EXTREMEHUNTER algorithm is
based on a classical idea in bandit theory: op-
timism in the face of uncertainty. Our strat-
egy is to estimate E [maxt≤n Xk t] for any k
and to pull the arm which maximizes its up-
per bound. From Deﬁnition 2  the estimation
of this quantity relies heavily on an efﬁcient es-
timation of αk and Ck  and on associated conﬁ-
dence widths. This topic is a classic problem in
extreme value theory  and such estimators exist
provided that one knows a lower bound b on βk
[10  6  7]. From now on we assume that a con-
stant b > 0 such that b ≤ mink βk is known
to the learner. As we argue in Remark 2  this
assumption is necessary .
Since our main theoretical result is a ﬁnite-time upper bound  in the following exposition we care-
fully describe all the constants and stress what quantities they depend on. Let Tk t be the number of
samples drawn from arm k at time t. Deﬁne δ = exp(− log2 n)/(2nK) and consider an estimator

estimate(cid:98)hk t that veriﬁes (2)
estimate (cid:98)Ck t using (3)

end if
end for
Play arm kt ← arg maxk Bk t
Tkt ← Tkt + 1

for k = 1 to K do
if Tk ≤ N then
else

Bk t ← ∞

update Bk t using (5) with (2) and (4)

end for

4

(cid:98)hk t of 1/αk at time t that veriﬁes the following condition with probability 1− δ  for Tk t larger than

some constant N2 that depends only on αk  Ck  C(cid:48) and b:

(2)
where D is a constant that also depends only on αk  Ck  C(cid:48)  and b. For instance  the estimator
in [6] (Theorem 3.7) veriﬁes this property and provides D and N2 but other estimators are possible.
Consider the associated estimator for Ck:

= B1(Tk t) 

αk

−b/(2b+1)
k t

(cid:12)(cid:12)(cid:12) 1

−(cid:98)hk t

(cid:12)(cid:12)(cid:12) ≤ D(cid:112)log(1/δ)T
 1
(cid:110)
Tk t(cid:88)

1

Tk t

u=1

(cid:98)hk t/(2b+1)

Xk u ≥ T

k t

(cid:111)

(3)

k t

(cid:98)Ck t = T 1/(2b+1)
(cid:113)
(cid:12)(cid:12)(cid:12) ≤ E
(cid:12)(cid:12)(cid:12)Ck − (cid:98)Ck t

For this estimator  we know [7] with probability 1 − δ that for Tk t ≥ N2:

where E is derived in [7] in the proof of Theorem 2. Let N = max(cid:0)A log(n)2(2b+1)/b  N2

log(Tk t/δ) log(Tk t)T

−b/(2b+1)
k T

= B2(Tk t) 

A depends on (αk  Ck)k  b  D  E  and C(cid:48)  and is such that:
max (2B1(N )  2B2(N )/Ck) ≤ 1  N ≥ (2D log2 n)(2b+1)/b  and N >

(cid:18)

√

log(n)2
2D
1−maxk 1/αk

(4)

(cid:1) where
(cid:19)(2b+1)/b

This inspires Algorithm 1  which ﬁrst pulls each arm N times and then  at each time t > KN  pulls
the arm that maximizes Bk t  which we deﬁne as:

(cid:17)
(cid:16)(cid:16)(cid:98)Ck t + B2 (Tk t)

n

(cid:17)(cid:98)hk t+B1(Tk t)

(cid:16)(cid:98)hk t  B1 (Tk t)

(cid:17)

¯Γ

 

(5)

where ¯Γ(x  y) = ˜Γ(1 − x − y)  where we set ˜Γ = Γ for any x > 0 and +∞ otherwise.
Remark 2. A natural question is whether it is possible to learn βk as well. In fact  this is not possible
for this model and a negative result was proved by [7]. The result states that in this setting it is not
possible to test between two ﬁxed values of β uniformly over the set of distributions. Thereupon  we
deﬁne b as a lower bound for all βk. With regards to the Pareto distribution  β = ∞ corresponds to
the exact Pareto distribution  while β = 0 for such distribution that is not (asymptotically) Pareto.

We show that this algorithm meets the desired properties. The following theorem states our main
result by upper-bounding the extreme regret of EXTREMEHUNTER.
Theorem 2. Assume that the distributions of the arms are respectively (αk  βk  Ck  C(cid:48)) second
order Pareto (see Deﬁnition 2) with mink αk > 1. If n ≥ Q  the expected extreme regret of EX-
TREMEHUNTER is bounded from above as:

n log(n)(2b+1)/b + n− log(n)(1−1/α∗) + n−b/((b+1)α∗)(cid:17)

E [Rn] ≤ L(nC∗)1/α∗(cid:16) K

= E [G∗

n] o(1) 

where L  Q > 0 are some constants depending only on (αk  Ck)k  C(cid:48)  and b (Section 6).
Theorem 2 states that the EXTREMEHUNTER strategy performs almost as well as the best (oracle)
strategy  up to a term that is negligible when compared to the performance of the oracle strategy.
Indeed  the regret is negligible when compared to (nC∗)1/α∗
  which is the order of magnitude of the
performance of the best oracle strategy E [G∗
n] = maxk≤K E [maxt≤n Xk t]. Our algorithm thus
detects the arm that has the heaviest tail.
For n large enough (as a function of (αk  βk  Ck)k  C(cid:48) and K)  the two ﬁrst terms in the regret
become negligible when compared to the third one  and the regret is then bounded as:

n]O(cid:16)

n−b/((b+1)α∗)(cid:17)

E [Rn] ≤ E [G∗

We make two observations: First  the larger the b  the tighter this bound is  since the model is then
closer to the parametric case. Second  smaller α∗ also tightens the bound  since the best arm is then
very heavy tailed and much easier to recognize.

5

6 Analysis

In this section  we prove an upper bound on the extreme regret of Algorithm 1 stated in Theorem 2.
Before providing the detailed proof  we give a high-level overview and the intuitions.
In Step 1  we deﬁne the (favorable) high probability event ξ of interest  useful for analyzing the
mechanism of the bandit algorithm. In Step 2  given ξ  we bound the estimates of αk and Ck  and
use them to bound the main upper conﬁdence bound. In Step 3  we upper-bound the number of pulls
of each suboptimal arm: we prove that with high probability we do not pull them too often. This
enables us to guarantee that the number of pulls of the optimal arms ∗ is on ξ equal to n up to a
negligible term.
The ﬁnal Step 4 of the proof is concerned with using this lower bound on the number of pulls of
the optimal arm in order to lower bound the expectation of the maximum of the collected samples.
Such step is typically straightforward in the classical (mean-optimizing) bandits by the linearity of
the expectation. It is not straightforward in our setting. We therefore prove Lemma 2  in which we
show that the expected value of the maximum of the samples in the favorable event ξ will be not too
far away from the one that we obtain without conditioning on ξ.

Step 1: High probability event.
δ def= exp(− log2n)/(2nK) and consider the event ξ such that for any k ≤ K  N ≤ T ≤ n:

In this step  we deﬁne the favorable event ξ. We set

(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)Ck − ˜Ck(T )

− ˜hk(T )

αk

(cid:12)(cid:12)(cid:12) ≤ D(cid:112)log(1/δ)T −b/(2b+1) 
(cid:12)(cid:12)(cid:12) ≤ E(cid:112)log(T /δ)T −b/(2b+1) 

Notice  they are not the same as(cid:98)hk t and (cid:98)Ck t which are the estimates of the same quantities at time

where ˜hk(T ) and ˜Ck(T ) are the estimates of 1/αk and Ck respectively using the ﬁrst T samples.
t for the algorithm  and thus with Tk t samples. The probability of ξ is larger than 1 − 2nKδ by a
union bound on (2) and (4).

Step 2: Bound on Bk t. The following lemma holds on ξ for upper- and lower-bounding Bk t.
Lemma 1. (proved in the appendix) On ξ  we have that for any k ≤ K  and for Tk t ≥ N:
−b/(2b+1)
k t

(cid:17)(cid:16)
1 + F log(n)(cid:112)log(n/δ)T

(cid:17) ≤ Bk t≤ (Ckn)

1− 1

1− 1

(Ckn)

1
αk Γ

1
αk Γ

(cid:17)

(cid:16)

(cid:16)

αk

αk

(6)

(C∗n)1/α∗

Rearranging the terms we get:
(C∗n)1/α∗

Step 3: Upper bound on the number of pulls of a suboptimal arm. We proceed by using the
bounds on Bk t from the previous step to upper-bound the number of suboptimal pulls. Let ∗ be the
best arm. Assume that at round t  some arm k (cid:54)= ∗ is pulled. Then by deﬁnition of the algorithm
B∗ t ≤ Bk t  which implies by Lemma 1:

−b/(2b+1)
k t

αk

α∗

α∗

1− 1

(cid:17)(cid:16)
1 + F log(n)(cid:112)log(n/δ)T

(cid:16)
(cid:1) ≤ (Ckn)1/αk Γ
Γ(cid:0)1− 1
Γ(cid:0)1− 1
(cid:1)
(cid:1) ≤ 1 + F log(n)(cid:112)log(n/δ)T
(Ckn)1/αk Γ(cid:0)1− 1
(cid:1)
Γ(cid:0)1− 1
(cid:1) − 1
(Ckn)1/αk Γ(cid:0)1− 1
(cid:17)(2b+1)/(2b) ≤ N + G(cid:0)log2n log(n/δ)(cid:1)(2b+1)(2b)

−b/(2b+1)
k t

(C∗n)1/α∗

∆k =

α∗

αk

αk

Tk t ≤ N + G(cid:48)(cid:16) log2n log(n/δ)

∆2
k

Since Tk t ≤ n  (7) implies for some problem dependent constants G and G(cid:48) dependent only on
(αk  Ck)k  C(cid:48) and b  but independent of δ that:

We now deﬁne ∆k which is analogous to the gap in the classical bandits:

(cid:17)

(7)

6

This implies that number T ∗ of pulls of arm ∗ is with probability 1 − δ(cid:48)  at least

n −(cid:88)
G(cid:0)log2n log(2nK/δ(cid:48))(cid:1)(2b+1)/(2b) − KN 
Q ≥ 2KN + 2GK(cid:0)log2n log (2nK/δ(cid:48))(cid:1)(2b+1)/(2b)

k(cid:54)=∗

 

where δ(cid:48) = 2nKδ. Since n is larger than

we have that T ∗ ≥ n

2 as a corollary.

max

(cid:20)

(cid:21)

(cid:20)

(cid:20)

(cid:21)

≥E

≥E

max
t≤n

max
t≤n

XIt Tk t

(cid:21)
X∗ T∗ t1{ξ}

(cid:21)
XIt Tk t 1{ξ}

Step 4: Bound on the expectation. We start by lower-bounding the expected gain:

(cid:20)
i≤T ∗ Xi1{ξ}
E[Gn] =E
The next lemma links the expectation of maxt≤T ∗ X∗ t with the expectation of maxt≤T ∗ X∗ t1{ξ}.
Lemma 2. (proved in the appendix) Let X1  . . .   XT be i.i.d. samples from an (α  β  C  C(cid:48))-second
order Pareto distribution F . Let ξ(cid:48) be an event of probability larger than 1 − δ. Then for δ < 1/2

and for T ≥ Q large enough so that c max(cid:0)1/T  1/T β(cid:1) ≤ 1/4 for a given constant c > 0  that
depends only on C  C(cid:48) and β  and also for T ≥ log(2) max(cid:0)C (2C(cid:48))1/β   8 log (2)(cid:1):
(cid:1) (T C)1/α δ1−1/α

max
t≤n

=E

Xt1{ξ}

(cid:1) −(cid:0)4 + 8

≥ (T C)1/α Γ(cid:0)1− 1
(cid:16) 4D2
T (T C)1/α + 2C(cid:48)D1+β

− 2

max
t≤T

(cid:17)

Since n is large enough so that 2n2Kδ(cid:48) = 2n2K exp(cid:0)− log2n(cid:1) ≤ 1/2  where δ(cid:48) = exp(cid:0)− log2n(cid:1)  
(cid:20)
(cid:21)
t≤T ∗ X∗ t1{ξ}

and the probability of ξ is larger than 1 − δ(cid:48)  we can use Lemma 2 for the optimal arm:
(C∗)1+b(T ∗)b −

(cid:20)
Γ(cid:0)1− 1

(cid:1)−(cid:0)4+ 8

T ∗ − 4C(cid:48)Dmax

(cid:1)δ(cid:48)1− 1

≥ (T ∗C∗)

α∗ − 8D2

2B
(T ∗C∗)

C1+β T β (T C)1/α + B

(cid:21)

α−1

max

(cid:21)

(cid:20)

α−1

α∗

E

E

1

α∗

α

.

 

1

α∗

where Dmax
we lower-bound the last three terms in the brackets using T ∗ ≥ n

def= maxi D1+βi. Using Step 3  we bound the above with a function of n. In particular 

factor as:

We are now ready to relate the lower bound on the gain of EXTREMEHUNTER with the upper bound
of the gain of the optimal policy (Theorem 1)  which brings us the upper bound for the regret:

n

1 − GK

(cid:17)
2 and the (T ∗C∗)1/α∗

(T ∗C∗)1/α∗ ≥ (nC∗)1/α∗(cid:16)
(cid:20)
(cid:0)log(2n2K/δ(cid:48))(cid:1) 2b+1

(cid:0)log(2n2K/δ(cid:48))(cid:1) 2b+1
(cid:20)
(cid:21)
t≤T ∗ X∗ t1{ξ}
n + δ(cid:48)1−1/α∗

(nC∗)b + GK

n] − E [Gn] ≤ E [G∗

2b − KN

≤ E [G∗

n] − E

n] − E

i≤T ∗ Xi
max

2b + KN

n + 1

(cid:21)

max

+

n

n

1

E [Rn] = E [G∗

≤ H(nC∗)1/α∗(cid:18)

(cid:19)

 

B

(nC∗)1/α∗

where H is a constant that depends on (αk  Ck)k  C(cid:48)  and b. To bound the last term  we use the
deﬁnition of B (9) to get the n−β∗/((β∗+1)α∗) term  upper-bounded by n−b/((b+1)α∗) as b ≤ β∗.
Notice that this ﬁnal term also eats up n−1 and n−b terms since b/((b + 1)α∗) ≤ min(1  b).

We ﬁnish by using δ(cid:48) = exp(cid:0)− log2n(cid:1) and grouping the problem-dependent constants into L to get

E [Rn] ≤ L(nC∗)1/α∗(cid:16) K

n log(n)(2b+1)/b + n− log(n)(1−1/α∗) + n−b/((b+1)α∗)(cid:17)

the ﬁnal upper bound:

7

Figure 1: Extreme regret as a function of time for the exact Pareto distributions (left)  approximate
Pareto (middle) distributions  and the network trafﬁc data (right).

7 Experiments
In this section  we empirically evaluate EXTREMEHUNTER on synthetic and real-world data. The
measure of our evaluation is the extreme regret from Deﬁnition 1. Notice that even thought we
evaluate the regret as a function of time T   the extreme regret is not cumulative and it is more in the
spirit of simple regret [5]. We compare our EXTREMEHUNTER with THRESHOLDASCENT [20].
Moreover  we also compare to classical UCB [3]  as an example of the algorithm that aims for the
arm with the highest mean as opposed to the heaviest tail. When the distribution of a single arm
has both the highest mean and the heaviest-tail  both EXTREMEHUNTER and UCB are expected to
perform the same with respect to the extreme regret. In the light of Remark 2  we set b = 1 to
consider a wide class of distributions.

Exact Pareto Distributions
In the ﬁrst experiment  we consider K = 3 arms with the distributions
Pk(x) = 1−x−αk  where α = [5  1.1  2]. Therefore  the most heavy-tailed distribution is associated
with the arm k = 2. Figure 1 (left) displays the averaged result of 1000 simulations with the time
horizon T = 104. We observe that EXTREMEHUNTER eventually keeps allocating most of the
pulls to the arm of the interest. Since in this case  the arm with the heaviest tail is also the arm
with the largest mean  UCB also performs well and it is even able to detect the best arm earlier.
THRESHOLDASCENT  on the other way  was not always able to allocate the pulls properly in 104
steps. This may be due to the discretization of the rewards that this algorithm is using.

Approximate Pareto Distributions For the exact Pareto distributions  the smaller the tail index
the higher the mean and even UCB obtains a good performance. However  this is no longer nec-
essarily the case for the approximate Pareto distributions. For this purpose  we perform the second
experiment where we mix an exact Pareto distribution with a Dirac distribution in 0. We consider
K = 3 arms. Two of the arms follow the exact Pareto distributions with α1 = 1.5 and α3 = 3.
On the other hand  the second arm has a mixture weight of 0.2 for the exact Pareto distribution with
α2 = 1.1 and 0.8 mixture weight of the Dirac distribution in 0. For this setting  the second arm
is the most heavy-tailed but the ﬁrst arms has the largest mean. Figure 1 (middle) shows the re-
sult. We see that UCB performs worse since it eventually focuses on the arm with the largest mean.
THRESHOLDASCENT performs better than UCB but not as good as EXTREMEHUNTER.

Computer Network Trafﬁc Data In this experiment  we evaluate EXTREMEHUNTER on heavy-
tailed network trafﬁc data which was collected from user laptops in the enterprise environment [2].
The objective is to allocate the sampling capacity among the computer nodes (arms)  in order to ﬁnd
the largest outbursts of the network activity. This information then serves an IT department to further
investigate the source of the extreme network trafﬁc. For each arm  a sample at the time t corre-
sponds to the number of network activity events for 4 consecutive seconds. Speciﬁcally  the network
events are the starting times of packet ﬂows. In this experiment  we selected K = 5 laptops (arms) 
where the recorded sequences were long enough. Figure 1 (right) shows that EXTREMEHUNTER
again outperforms both THRESHOLDASCENT and UCB.

Acknowledgements We would like to thank John Mark Agosta and Jennifer Healey for the net-
work trafﬁc data. The research presented in this paper was supported by Intel Corporation  by
French Ministry of Higher Education and Research  and by European Community’s Seventh Frame-
work Programme (FP7/2007-2013) under grant agreement no270327 (CompLACS).

8

010002000300040005000600070008000900010000010002000300040005000600070008000900010000time textreme regretComparison of extreme bandit strategies (K=3) ExtremeHunterUCBThresholdAscent01000200030004000500060007000800090001000005001000150020002500time textreme regretComparison of extreme bandit strategies (K=3) ExtremeHunterUCBThresholdAscent010002000300040005000600070008000900010000050100150200250time textreme regretComparison of extreme bandit strategies on the network data K=5 ExtremeHunterUCBThresholdAscentReferences
[1] Naoki Abe  Bianca Zadrozny  and John Langford. Outlier Detection by Active Learning. In
Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining  pages 504–509  2006.

[2] John Mark Agosta  Jaideep Chandrashekar  Mark Crovella  Nina Taft  and Daniel Ting. Mix-
ture models of endhost network trafﬁc. In IEEE Proceedings of INFOCOM   pages 225–229.
[3] Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite-time Analysis of the Multiarmed

Bandit Problem. Machine Learning  47(2-3):235–256  2002.

[4] S´ebastien Bubeck  Nicol`o Cesa-Bianchi  and G´abor Lugosi. Bandits With Heavy Tail. Infor-

mation Theory  IEEE Transactions on  59(11):7711–7717  2013.

[5] S´ebastien Bubeck  R´emi Munos  and Gilles Stoltz. Pure Exploration in Multi-armed Bandits

Problems. Algorithmic Learning Theory  pages 23–37  2009.

[6] Alexandra Carpentier and Arlene K. H. Kim. Adaptive and minimax optimal estimation of the

tail coefﬁcient. Statistica Sinica  2014.

[7] Alexandra Carpentier and Arlene K. H. Kim. Honest and adaptive conﬁdence interval for the

tail coefﬁcient in the Pareto model. Electronic Journal of Statistics  2014.

[8] Varun Chandola  Arindam Banerjee  and Vipin Kumar. Anomaly detection: A survey. ACM

Comput. Surv.  41(3):15:1–15:58  July 2009.

[9] Vincent A. Cicirello and Stephen F. Smith. The max k-armed bandit: A new model of explo-
ration applied to search heuristic selection. AAAI Conference on Artiﬁcial Intelligence  2005.
[10] Laurens de Haan and Ana Ferreira. Extreme Value Theory: An Introduction. Springer Series

in Operations Research and Financial Engineering. Springer  2006.

[11] Ronald Aylmer Fisher and Leonard Henry Caleb Tippett. Limiting forms of the frequency
distribution of the largest or smallest member of a sample. Mathematical Proceedings of the
Cambridge Philosophical Society  24:180  1928.

[12] Boris Gnedenko. Sur la distribution limite du terme maximum d’une s´erie al´eatoire. The

Annals of Mathematics  44(3):423–453  1943.

[13] Peter Hall and Alan H. Welsh. Best Attainable Rates of Convergence for Estimates of Param-

eters of Regular Variation. The Annals of Statistics  12(3):1079–1084  1984.

[14] Tze L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances

in Applied Mathematics  6(1):4–22  1985.

[15] Keqin Liu and Qing Zhao. Dynamic Intrusion Detection in Resource-Constrained Cyber Net-

works. In IEEE International Symposium on Information Theory Proceedings  2012.

[16] Markos Markou and Sameer Singh. Novelty detection: a review  part 1: statistical approaches.

Signal Process.  83(12):2481–2497  2003.

[17] Daniel B. Neill and Gregory F. Cooper. A multivariate Bayesian scan statistic for early event

detection and characterization. Machine Learning  79:261–282  2010.

[18] Carey E. Priebe  John M. Conroy  David J. Marchette  and Youngser Park. Scan Statistics on
Enron Graphs. In Computational and Mathematical Organization Theory  volume 11  pages
229–247  2005.

[19] Ingo Steinwart  Don Hush  and Clint Scovel. A Classiﬁcation Framework for Anomaly Detec-

tion. Journal of Machine Learning Research  6:211–232  2005.

[20] Matthew J. Streeter and Stephen F. Smith. A Simple Distribution-Free Approach to the Max
In Principles and Practice of Constraint Programming  volume

k-Armed Bandit Problem.
4204  pages 560–574  2006.

[21] Matthew J. Streeter and Stephen F. Smith. An Asymptotically Optimal Algorithm for the Max
k-Armed Bandit Problem. In AAAI Conference on Artiﬁcial Intelligence Intelligence  pages
135–142  2006.

[22] Ryan Turner  Zoubin Ghahramani  and Steven Bottone. Fast online anomaly detection using

scan statistics. IEEE Workshop on Machine Learning for Signal Processing  2010.

9

,Alexandra Carpentier
Michal Valko