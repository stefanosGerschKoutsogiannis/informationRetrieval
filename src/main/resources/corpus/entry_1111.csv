2018,Constant Regret  Generalized Mixability  and Mirror Descent,We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting  and for the right choice of loss function and ``mixing'' algorithm  it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example  a constant regret can be achieved for \emph{mixable} losses using the \emph{aggregating algorithm}. The \emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies)  which reduce to the aggregating algorithm when using the \emph{Shannon entropy} $\operatorname{S}$. For a given entropy $\Phi$  losses for which a constant regret is possible using the \textsc{GAA} are called $\Phi$-mixable. Which losses are $\Phi$-mixable was previously left as an open question. We fully characterize $\Phi$-mixability and answer other open questions posed by \cite{Reid2015}. We show that the Shannon entropy $\operatorname{S}$ is fundamental in nature when it comes to mixability; any $\Phi$-mixable loss is necessarily $\operatorname{S}$-mixable  and the lowest worst-case regret of the \textsc{GAA} is achieved using the Shannon entropy. Finally  by leveraging the connection between the \emph{mirror descent algorithm} and the update step of the GAA  we suggest a new \emph{adaptive} generalized aggregating algorithm and analyze its performance in terms of the regret bound.,Constant Regret  Generalized Mixability  and Mirror

Descent

Zakaria Mhammedi

Research School of Computer Science

Australian National University and DATA61

zak.mhammedi@anu.edu.au

Robert C. Williamson

Research School of Computer Science

Australian National University and DATA61

bob.williamson@anu.edu.au

Abstract

We consider the setting of prediction with expert advice; a learner makes predictions
by aggregating those of a group of experts. Under this setting  and for the right
choice of loss function and “mixing” algorithm  it is possible for the learner to
achieve a constant regret regardless of the number of prediction rounds. For
example  a constant regret can be achieved for mixable losses using the aggregating
algorithm. The Generalized Aggregating Algorithm (GAA) is a name for a family
of algorithms parameterized by convex functions on simplices (entropies)  which
reduce to the aggregating algorithm when using the Shannon entropy S. For a given
entropy Φ  losses for which a constant regret is possible using the GAA are called
Φ-mixable. Which losses are Φ-mixable was previously left as an open question.
We fully characterize Φ-mixability and answer other open questions posed by [6].
We show that the Shannon entropy S is fundamental in nature when it comes to
mixability; any Φ-mixable loss is necessarily S-mixable  and the lowest worst-case
regret of the GAA is achieved using the Shannon entropy. Finally  by leveraging
the connection between the mirror descent algorithm and the update step of the
GAA  we suggest a new adaptive generalized aggregating algorithm and analyze
its performance in terms of the regret bound.

1

Introduction

Two fundamental problems in learning are how to aggregate information and under what circum-
stances can one learn fast. In this paper  we consider the problems jointly  extending the understanding
and characterization of exponential mixing due to [10]  who showed that not only does the “aggregat-
ing algorithm” learn quickly when the loss is suitably chosen  but that it is in fact a generalization of
classical Bayesian updating  to which it reduces when the loss is log-loss [12]. We consider a general
class of aggregating schemes  going beyond Vovk’s exponential mixing  and provide a complete
characterization of the mixing behavior for general losses and general mixing schemes parameterized
by an arbitrary entropy function.
In the game of prediction with expert advice a learner predicts the outcome of a random variable
(outcome of the environment) by aggregating the predictions of a pool of experts. At the end of
each prediction round  the outcome of the environment is announced and the learner and experts
suffer losses based on their predictions. We are interested in algorithms that the learner can use to
“aggregate” the experts’ predictions and minimize the regret at the end of the game. In this case 
the regret is deﬁned as the difference between the cumulative loss of the learner and that of the best
expert in hindsight after T rounds.
The Aggregating Algorithm (AA) [10] achieves a constant regret — a precise notion of fast learning
— for mixable losses; that is  the regret is bounded from above by a constant R(cid:96) which depends only
on the loss function (cid:96) and not on the number of rounds T . It is worth mentioning that mixability

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

is a weaker condition than exp-concavity  and contrary to the latter  mixability is an intrinsic 
parametrization-independent notion [4].
Reid et al. [6] introduced the Generalized Aggregating Algorithm (GAA)  going beyond the AA. The
GAA is parameterized by the choice of a convex function Φ on the simplex (entropy) and reduces to
the AA when Φ is the Shannon entropy. The GAA can achieve a constant regret for losses satisfying
a certain condition called Φ-mixability (characterizing when losses are Φ-mixable was left as an open
problem). This regret depends jointly on the generalized mixability constant ηΦ
(cid:96) — essentially the
η Φ)-mixable — and the divergence DΦ(eθ  q)  where q ∈ ∆k is a prior
largest η such that (cid:96) is ( 1
distribution over k experts and eθ is the θth standard basis element of Rk [6]. At each prediction
round  the GAA can be divided into two steps; a substitution step where the learner picks a prediction
from a set speciﬁed by the Φ-mixability condition; and an update step where a new distribution q
over experts is computed depending on their performance. Interestingly  this update step is exactly
the mirror descent algorithm [8  5] which minimizes the weighted loss of experts.

Contributions. We introduce the notion of a support loss; given a loss (cid:96) deﬁned on any action
space  there exists a proper loss (cid:96) which shares the same Bayes risk as (cid:96). When a loss is mixable 
one can essentially work with a proper (support) loss instead — this will be the ﬁrst stepping stone
towards a characterization of (generalized) mixability.
The notion of Φ-mixable and the GAA were previously restricted to ﬁnite losses. We extend these to
allow for the use of losses which can take inﬁnite values (such as the log-loss)  and we show in this
case that under the Φ-mixability condition a constant regret is achievable using the GAA.
For an entropy Φ and a loss (cid:96)  we derive a necessary and sufﬁcient condition (Theorems 13 and
14) for (cid:96) to be Φ-mixable. In particular  if (cid:96) and Φ satisfy some regularity conditions  then (cid:96) is
Φ-mixable if and only if η(cid:96)Φ − S is convex on the simplex  where S is the Shannon entropy and η(cid:96)
is essentially the largest η such that (cid:96) is η-mixable [10  9]. This implies that a loss (cid:96) is Φ-mixable
only if it is η-mixable for some η > 0. This  combined with the fact that η-mixability is equivalently
η S)-mixability (Theorem 12)  reﬂects one fundamental aspect of the Shannon entropy.
( 1
Then  we derive an explicit expression for the generalized mixability constant ηΦ
(cid:96) (Corollary 17)  and
thus for the regret bound of the GAA. This allows us to compare the regret bound RΦ
(cid:96) of any entropy
(cid:96) ≤ RΦ
Φ with that of the Shannon entropy S. In this case  we show (Theorem 18) that RS
(cid:96) ; that is  the
GAA achieves the lowest worst-case regret when using the Shannon entropy — another result which
reﬂects the fundamental nature of the Shannon entropy.
Finally  by leveraging the connection between the GAA and the mirror descent algorithm  we present
a new algorithm — the Adaptive Generalized Aggregating Algorithm (AGAA). This algorithm
consists of changing the entropy function at each prediction round similar to the adaptive mirror
descent algorithm [8]. We analyze the performance of this algorithm in terms of its regret bound.

Layout.
In §2  we give some background on loss functions and present new results (Theorem 4 and
5) based on the new notion of a proper support loss; we show that  as far as mixability is concerned 
one can always work with a proper (support) loss instead of the original loss (which can be deﬁned on
an arbitrary action space). In §3  we introduce the notions of classical and generalized mixability and
derive a characterization of Φ-mixability (Theorems 13 and 14). We then introduce our new algorithm
— the AGAA — and analyze its performance. We conclude the paper by a general discussion and
direction for future work. All proofs  except for that of Theorem 16  are deferred to Appendix C.
Notation. Let m ∈ N. We denote [m] := {1  . . .   m} and ˜m := m − 1. We write (cid:104)· ·(cid:105) for the
standard inner product in Euclidean space. Let ∆m := {p ∈ [0  +∞[m : (cid:104)p  1m(cid:105) = 1} be the
probability simplex in Rm  and let ˜∆m := { ˜p ∈ [0  +∞[ ˜m : (cid:104) ˜p  1 ˜m(cid:105) ≤ 1}. We will extensively
make use of the afﬁne map (cid:113)m : R ˜m → Rm deﬁned by

(cid:113)m(u) := [u1  . . .   u ˜m  1 − (cid:104)u  1 ˜m(cid:105)]T.

(1)
We denote intC  riC  and rbdC the interior  relative interior  and relative boundary of a set C ∈ Rm 
respectively [2]. The sub-differential of a function f : Rm → R ∪ {+∞} at u ∈ Rm such that
f (u) < +∞ is deﬁned by ([2])

∂f (u) := {s∗ ∈ Rm : f (v) ≥ f (u) + (cid:104)s∗  v − u(cid:105)  ∀v ∈ Rm}.

(2)

2

Table 1 on page 9 provides a list of the main symbols used in this paper.

1≤θ≤k ∈ Ak  (cid:96)x(A) := [(cid:96)x(aθ)]T

denote (cid:96)x(·) := [(cid:96)(·)]x. We further extend the new deﬁnition of (cid:96) to the set(cid:83)

2 Loss Functions
In general  a loss function is a map (cid:96) : X × A → [0  +∞] where X is an outcome set and A is an
action set. In this paper  we only consider the case X = [n]  i.e. ﬁnite outcome space. Overloading
notation slightly  we deﬁne the mapping (cid:96) : A → [0  +∞]n by [(cid:96)(a)]x = (cid:96)(x  a) ∀x ∈ [n] and
k≥1 Ak such that for
x ∈ [n] and A := [aθ]T
1≤θ≤k ∈ [0  +∞]k. We deﬁne the effective
domain of (cid:96) by dom (cid:96) := {a ∈ A : (cid:96)(a) ∈ [0  +∞[n}  and the loss surface by S(cid:96) := {(cid:96)(a) : a ∈
dom (cid:96)}. We say that (cid:96) is closed if S(cid:96) is closed in Rn. The superprediction set of (cid:96) is deﬁned by
S ∞
Let a0  a1 ∈ A. The prediction a0 is said to be better than a1 if the component-wise inequality
(cid:96)(a0) ≤ (cid:96)(a1) holds and there exists some x ∈ [n] such that (cid:96)x(a0) < (cid:96)x(a1) [14]. A loss (cid:96) is
admissible if for any a ∈ A there are no better predictions.
For the rest of this paper (except for Theorem 4)  we make the following assumption on losses;
Assumption 1. (cid:96) is a closed  admissible loss such that dom (cid:96) (cid:54)= ∅.

:= {(cid:96)(a) + d : (a  d) ∈ A × [0  +∞[n}. Let S(cid:96) := S ∞

(cid:96) ∩ [0  +∞[n be its ﬁnite part.

(cid:96)

It is clear that there is no loss of generality in considering only admissible losses. The condition that (cid:96)
is closed is a weaker version of the more common assumption that A is compact and that a (cid:55)→ (cid:96)(x  a)
is continuous with respect to the extended topology of [0  +∞] for all x ∈ [n] [3  1]. In fact  we do
not make any explicit topological assumptions on the set A (A is allowed to be open in our case).
Our condition simply says that if a sequence of points on the loss surface converges in [0  +∞[n 
then there exists an action in A whose image through the loss is equal to the limit. For example the
0-1 loss (cid:96)0-1 is closed  yet the map p (cid:55)→ (cid:96)0-1(x  p) is not continuous on ∆2  for x ∈ {0  1}.
In this paragraph let A be the n-simplex  i.e. A = ∆n. We deﬁne the conditional risk L(cid:96) : ∆n×∆n →
R by L(cid:96)(p  q) = Ex∼p[(cid:96)x(q)] = (cid:104)p  (cid:96)(q)(cid:105) and the Bayes risk by L(cid:96)(p) := inf q∈∆n L(cid:96)(p  q). In
this case  the loss (cid:96) is proper if L(cid:96)(p) = (cid:104)p  (cid:96)(p)(cid:105) ≤ (cid:104)p  (cid:96)(q)(cid:105) for all p (cid:54)= q in ∆n (and strictly
proper if the inequality is strict). For example  the log-loss (cid:96)log : ∆n → [0  +∞]n is deﬁned by
(cid:96)log(p) = − log p  where the ‘log’ of a vector applies component-wise. One can easily check that
(cid:96)log is strictly proper. We denote Llog its Bayes risk.
The above deﬁnition of the Bayes risk is restricted to losses deﬁned on the simplex. For a general
loss (cid:96) : A → [0  +∞]n  we use the following deﬁnition;
Deﬁnition 2 (Bayes Risk). Let (cid:96) : A → [0  +∞]n be a loss such that dom (cid:96) (cid:54)= ∅. The Bayes risk
L(cid:96) : Rn → R ∪ {−∞} is deﬁned by

∀u ∈ Rn  L(cid:96)(u) := inf
z∈S(cid:96)

(cid:104)u  z(cid:105) .

(3)

The support function of a set C ⊆ Rn is deﬁned by σC(u) := supz∈C(cid:104)u  z(cid:105)  u ∈ Rn  and thus it is
easy to see that one can express the Bayes risk as L(cid:96)(u) = −σS(cid:96)(−u). Our deﬁnition of the Bayes
risk is slightly different from previous ones ([3  9  1]) in two ways; 1) the Bayes risk is deﬁned on all
Rn instead of [0  +∞[n; and 2) the inﬁmum is taken over the ﬁnite part of the superprediction set
S ∞
. The ﬁrst point is a mere mathematical convenience and makes no practical difference since
L(cid:96)(p) = −∞ for all p /∈ [0  +∞[n. For the second point  swapping S(cid:96) for S ∞
in (3) does not
change the value of L(cid:96) for mixable losses (see Appendix D). However  we chose to work with S(cid:96) —
a subset of Rn — as it allows us to directly apply techniques from convex analysis.
Deﬁnition 3 (Support Loss). We call a map (cid:96) : ∆n → [0  +∞]n a support loss of (cid:96) if

(cid:96)

(cid:96)

∀p ∈ ri ∆n  (cid:96)(p) ∈ ∂σS(cid:96)(−p);

∀p ∈ rbd ∆n ∃(pm) ⊂ ri ∆n  pm

m→∞→ p and (cid:96)(pm) m→∞→ (cid:96)(p) component-wise 
where ∂σS(cid:96) (see (2)) is the sub-differential of the support function — σS(cid:96) — of the set S(cid:96).
Theorem 4. Any loss (cid:96) : A → [0  +∞]n such that dom (cid:96) (cid:54)= ∅  has a proper support loss (cid:96) with the
same Bayes risk  L(cid:96)  as (cid:96).

3

Theorem 4 shows that regardless of the action space on which the loss is deﬁned  there always exists
a proper loss whose Bayes risk coincides with that of the original loss. This fact is useful in situations
where the Bayes risk contains all the information one needs — such is the case for mixability. The
next Theorem shows a stronger relationship between a loss and its corresponding support loss.
Theorem 5. Let (cid:96) : A → [0  +∞]n be a loss and (cid:96) be a proper support loss of (cid:96). If the Bayes risk L(cid:96)
is differentiable on ]0  +∞[n  then (cid:96) is uniquely deﬁned on ri ∆n and

∀p ∈ dom (cid:96)  ∃a∗ ∈ dom (cid:96) 
∀a ∈ dom (cid:96)  ∃(pm) ⊂ ri ∆n 

(cid:96)(a∗) = (cid:96)(p) 
(cid:96)(pm) m→∞→ (cid:96)(a) component-wise.

Theorem 5 shows that when the Bayes risk is differentiable (a necessary condition for mixability —
Theorem 12)  the support loss is almost a reparametrization of the original loss  and in practice  it is
enough to work with support losses instead. This will be crucial for characterizing Φ-mixability.

3 Mixability in the Game of Prediction with Expert Advice

M := M (at

1:k  (xs  as

1:k)1≤s<t) ∈ A  where a·

We consider the setting of prediction with expert advice [10]; there a is pool of k experts  parame-
terized by θ ∈ [k]  which make predictions at
θ ∈ A at each round t. In the same round  the learner
θ]1≤θ≤k  (xs) ⊂ [n] are outcomes
predicts at
of the environment  and M : Ak × ([n] × Ak)∗ → A is a merging strategy [9]. At the end of round
M)]  where
t  xt is announced and each expert θ [resp. learner] suffers a loss (cid:96)xt(aθ) [resp. (cid:96)xt(at
(cid:96) : A → [0  +∞]n. After T > 0 rounds  the cumulative loss of each expert θ [resp. learner] is given
by Loss(cid:96)
M)]. We say that M achieves a
constant regret if ∃R > 0 ∀T > 0 ∀θ ∈ [k]  Loss(cid:96)
θ(T ) + R. In what follows  this
game setting will be referred to by Gn

(cid:96) (A  k) and we only consider the case where k ≥ 2.

M(T ) := (cid:80)T

θ(T ) := (cid:80)T

t=1 (cid:96)xt(at
M(T ) ≤ Loss(cid:96)

θ) [resp. Loss(cid:96)

1:k := [a·

t=1 (cid:96)xt(at

(cid:96)

.

∀a1:k ∈ Ak ∃a∗ ∈ A ∀x ∈ [n] 

(cid:96)x(a∗) ≤ −η−1 log (cid:104)q  exp(−η(cid:96)x(a1:k))(cid:105)  

(cid:96) (A  k)
(cid:96) → A is a substitution function of the loss (cid:96) [10  4];

3.1 The Aggregating Algorithm and η-mixability
Deﬁnition 6 (η-mixability). For η > 0  a loss (cid:96) : A → [0  +∞]n is said to be η-mixable  if ∀q ∈ ∆k 
(4)
where the exp applies component-wise. Letting H(cid:96) := {η > 0 : (cid:96) is η-mixable}  we deﬁne the
mixability constant of (cid:96) by η(cid:96) := sup H(cid:96) if H(cid:96) (cid:54)= ∅; and 0 otherwise. (cid:96) is said to be mixable if
η(cid:96) > 0.
If a loss (cid:96) is η-mixable for η > 0  the AA (Algorithm 1) achieves a constant regret in the Gn
game[10]. In Algorithm 1  the map S(cid:96) : S ∞
that is  S(cid:96) satisﬁes the component-wise inequality (cid:96)(G(cid:96)(s)) ≤ s  for all s ∈ S ∞
It was shown by Chernov et al. [1] that the η-mixability condition (4) is equivalent to the convexity
of the η-exponentiated superprediction set of (cid:96) deﬁned by exp(−ηS ∞
(cid:96) ) := {exp(−ηs) : s ∈ S ∞
(cid:96) }.
Using this fact  van Erven et al. [9] showed that the mixability constant η(cid:96) of a strictly proper loss
(cid:96) : ∆n → [0  +∞[n  whose Bayes risk is twice continuously differentiable on ]0  +∞[n  is equal to
(5)
where H is the Hessian operator and ˜L· := L·◦(cid:113)n ((cid:113)n was deﬁned in (1)). The next theorem extends
this result by showing that the mixability constant η(cid:96) of any loss (cid:96) is lower bounded by η(cid:96) in (5)  as
long as (cid:96) satisﬁes Assumption 1 and its Bayes risk is twice differentiable.
Theorem 7. Let η > 0 and (cid:96) : A → [0  +∞]n be a loss. Suppose that dom (cid:96) = A and that L(cid:96) is
twice differentiable on ]0  +∞[n. If η(cid:96) > 0 then (cid:96) is η(cid:96)-mixable. In particular  η(cid:96) ≥ η(cid:96).
We later show that  under the same conditions as Theorem 7  we actually have η(cid:96) = η(cid:96) (Theorem 16)
which indicates that the Bayes risk contains all the information necessary to characterize mixability.
Remark 8. In practice  the requirement ‘dom (cid:96) = A’ is not necessarily a strict restriction to ﬁnite
losses; it is often the case that a loss ¯(cid:96) : ¯A → [0  +∞]n only takes inﬁnite values on the relative
boundary of ¯A (such is the case for the log-loss deﬁned on the simplex)  and thus the restriction
(cid:96) := ¯(cid:96)|A  where A = ri ¯A  satisﬁes dom (cid:96) = A. It follows trivially from the deﬁnition of mixability
(4) that if (cid:96) is η-mixable and ¯(cid:96) is continuous with respect to the extended topology of [0  +∞]n — a
condition often satisﬁed — then ¯(cid:96) is also η-mixable.

(λmax([H ˜Llog( ˜p)]−1H ˜L(cid:96)( ˜p)))−1 

η(cid:96) := inf

˜p∈int ˜∆n

4

3.2 The Generalized Aggregating Algorithm and (η  Φ)−mixability
A function Φ : Rk → R∪{+∞} is an entropy if it is convex  its epigraph epi Φ := {(u  h) : Φ(u) ≤
h} is closed in Rk × R  and ∆k ⊆ dom Φ := {u ∈ Rk : Φ(u) < +∞}. For example  the Shannon
entropy is deﬁned by S(q) = +∞ if q /∈ [0  +∞[k  and

(cid:88)

∀q ∈ [0  +∞[k 

S(q) =

qi log qi 

i∈[k] : qi(cid:54)=0

(6)

(7)

The divergence generated by an entropy Φ is the map DΦ : Rn × dom Φ → [0  +∞] deﬁned by

(cid:26) Φ(v) − Φ(u) − Φ(cid:48)(u; v − u) 

DΦ(v  u) :=

+∞ 

if v ∈ dom Φ;
otherwise.

where Φ(cid:48)(u; v − u) := limλ↓0[Φ(u + λ(v − u)) − Φ(u)]/λ (the limit exists since Φ is convex [7]).
Deﬁnition 9 (Φ-mixability). Let Φ : Rk → R ∪ {+∞} be an entropy. A loss (cid:96) : A → [0  +∞]n is
(η  Φ)-mixable for η > 0 if ∀q ∈ ∆k  ∀a1:k ∈ Ak  ∃a∗ ∈ A  such that

∀x ∈ [n]  (cid:96)x(a∗) ≤ Mixη

Φ((cid:96)x(a1:k)  q) := inf
ˆq∈∆k

(cid:104) ˆq  (cid:96)x(a1:k)(cid:105) + η−1DΦ( ˆq  q).

(8)

(cid:96) := {η >
(cid:96)   if

(cid:96) := sup HΦ

Φ. Letting HΦ

When η = 1  we simply say that (cid:96) is Φ-mixable and we denote MixΦ := Mix1
0 : (cid:96) is (η  Φ)-mixable}  we deﬁne the generalized mixability constant of ((cid:96)  Φ) by ηΦ
(cid:96) (cid:54)= ∅; and 0 otherwise.
HΦ
Reid et al. [6] introduced the GAA (see Algorithm 2) which uses an entropy function Φ : Rk →
R ∪ {+∞} and a substitution function S(cid:96) (see previous section) to specify the learner’s merging
strategy M. It was shown that the GAA reduces to the AA when Φ is the Shannon entropy S. It was
also shown that under some regularity conditions on Φ  the GAA achieves a constant regret in the
(cid:96) (A  k) game for any ﬁnite  (η  Φ)-mixable loss.
Gn
Our deﬁnition of Φ-mixability differs slightly from that of Reid et al. [6] — we use directional
derivatives to deﬁne the divergence DΦ. This distinction makes it possible to extend the GAA to losses
which can take inﬁnite values (such as the log-loss deﬁned on the simplex). We show  in this case 
that a constant regret is still achievable under the (η  Φ)-mixability condition. Before presenting this
result  we deﬁne the notion of ∆-differentiability; for l ⊆ [k]  let ∆l := {q ∈ ∆k : qθ = 0 ∀θ /∈ l}.
We say that an entropy Φ is ∆-differentiable if ∀l ⊆ [k]  ∀u  u0 ∈ ri ∆l  the map z (cid:55)→ Φ(cid:48)(u; z) is
linear on L0
Theorem 10. Let Φ : Rk → R ∪ {+∞} be a ∆-differentiable entropy. Let (cid:96) : A → [0  +∞]n be a
loss (not necessarily ﬁnite) such that L(cid:96) is twice differentiable on ]0  +∞[n. If (cid:96) is (η  Φ)-mixable
then the GAA achieves a constant regret in the Gn
θ(T ) ≤ RΦ

(cid:96) (A  k) game; for any sequence (xt  at

l := {λ(v − u0) : (λ  v) ∈ R × ∆l}.

DΦ(eθ  q)/ηΦ
(cid:96)  

1:k)T

t=1 

(9)

Loss(cid:96)

GAA(T ) − min
θ∈[k]

Loss(cid:96)

(cid:96) := inf
q∈∆k

max
θ∈[k]

for initial distribution over experts q0 = argminq∈∆k
element of Rk  and any substitution function S(cid:96).

maxθ∈[k] DΦ(eθ  q)  where eθ is the θth basis

Looking at Algorithm 2  it is clear that the GAA is divided into two steps; 1) a substitution step which
consists of ﬁnding a prediction a∗ ∈ A satisfying the mixability condition (8) using a substitution
function S(cid:96); and 2) an update step where a new distribution over experts is computed. Except for the
case of the AA with the log-loss (which reduces to Bayesian updating [12])  there is not a unique
choice of substitution function in general. An example of substitution function S(cid:96) is the inverse loss
[13]. Kamalaruban et al. [4] discuss other alternatives depending on the curvature of the Bayes risk.
Although the choice of S(cid:96) can affect the performance of the algorithm to some extent [4]  the regret
bound in (9) remains unchanged regardless of S(cid:96). On the other hand  the update step is well deﬁned
and corresponds to a mirror descent step [6] (we later use this fact to suggest a new algorithm).

5

Algorithm 1: Aggregating Algorithm
input

:q0 ∈ ∆k; η > 0; A η-mixable loss
(cid:96) : A → [0  +∞]n; A substitution
function S(cid:96).

output :Learner’s predictions (at∗)
for t = 1 to T do

1:k ∈ Ak;

(cid:16)− 1
θ)(cid:17)
η log(cid:80)
Observe At = at
at∗ ← S(cid:96)
Observe outcome xt ∈ [n];
exp(−η(cid:96)xt(at
θ ← qt−1
(cid:104)qt−1  exp(−η(cid:96)xt(At))(cid:105)  ∀θ ∈ [k];
qt

θ∈[k] qt−1

e−η(cid:96)(at

θ))

θ

θ

;

end

end

Algorithm 2: Generalized Aggregating Algorithm
input

:q0 ∈ ∆k; A ∆-differentiable entropy
Φ : Rk → R ∪ {+∞}; η > 0; A
(η  Φ)-mixable loss (cid:96) : A → [0  +∞]n; A
substitution function S(cid:96).
output :Learner’s predictions (at∗)
for t = 1 to T do

(cid:16)(cid:2)Mixη

1:k ∈ Ak;

Φ((cid:96)x(At)  qt−1)(cid:3)T

Observe At = at
at∗ ← S(cid:96)
Observe outcome xt ∈ [n];
qt ← argmin
µ∈∆k

(cid:104)µ  (cid:96)xt(At)(cid:105) + 1

1≤x≤n

η DΦ(µ  qt−1);

(cid:17)

;

We conclude this subsection with two new and important results which will lead to a characterization
of Φ-mixability. The ﬁrst result shows that (η  S)-mixability is equivalent to η-mixability  and the
second rules out losses and entropies for which Φ-mixability is not possible.
Theorem 11. Let η > 0. A loss (cid:96) : A → [0  +∞]n is η-mixable if and only if (cid:96) is (η  S)-mixable.
Proposition 12. Let Φ : Rk → R ∪ {+∞} be an entropy and (cid:96) : A → [0  +∞]n. If (cid:96) is Φ-mixable 
then the Bayes risk satisﬁes L(cid:96) ∈ C 1(]0  +∞[n). If  additionally  L(cid:96) is twice differentiable on
]0  +∞[n  then Φ must be strictly convex on ∆k.
It should be noted that since the Bayes risk of a loss (cid:96) must be differentiable for it to be Φ-mixable
for some entropy Φ  Theorem 5 says that we can essentially work with a proper support loss (cid:96) of (cid:96).
This will be crucial in the proof of the sufﬁcient condition of Φ-mixability (Theorem 14).

3.3 A Characterization of Φ-Mixability
In this subsection  we ﬁrst show that given an entropy Φ : Rk → R ∪ {+∞} and a loss (cid:96) : A →
[0  +∞]n satisfying certain regularity conditions  (cid:96) is Φ-mixable if and only if

η(cid:96)Φ − S is convex on ∆k.

(10)
Theorem 13. Let η > 0  (cid:96) : A → [0  +∞]n a η-mixable loss  and Φ : Rk → R ∪ {+∞} an entropy.
If ηΦ − S is convex on ∆k  then (cid:96) is Φ-mixable.
The converse of Theorem 13 also holds under additional smoothness conditions on Φ and (cid:96);
Theorem 14. Let (cid:96) : A → [0  +∞]n be a loss such that L(cid:96) is twice differentiable on ]0  +∞[n  and
Φ : Rk → R ∪ {+∞} an entropy such that ˜Φ := Φ ◦ (cid:113)k is twice differentiable on int ˜∆k. Then (cid:96) is
Φ-mixable only if η(cid:96)Φ − S is convex on ∆k.
As consequence of Theorem 14  if a loss (cid:96) is not classically mixable  i.e. η(cid:96) = 0  it cannot be
∗
Φ-mixable for any entropy Φ. This is because η(cid:96)Φ − S
= η(cid:96)Φ − S = − S is not convex (where
equality ‘*’ is due to Theorem 7).
We need one more result before arriving at (10); Recall that the mixability constant η(cid:96) is deﬁned as
the supremum of the set H(cid:96) := {η ≥ 0 : (cid:96) is η-mixable}. The next lemma essentially gives a sufﬁcient
condition for this supremum to be attained when H(cid:96) is non-empty — in this case  (cid:96) is η(cid:96)-mixable.
Lemma 15. Let (cid:96) : A → [0  +∞]n be a loss. If dom (cid:96) = A  then either H(cid:96) = ∅ or η(cid:96) ∈ H(cid:96).
Theorem 16. Let (cid:96) and Φ be as in Theorem 14 with dom (cid:96) = A. Then η(cid:96) = η(cid:96). Furthermore  (cid:96) is
Φ-mixable if and only if η(cid:96)Φ − S is convex on ∆k.

Proof. Suppose now that (cid:96) is mixable. By Lemma 15  it follows that (cid:96) is η(cid:96)-mixable  and from
(cid:96) S in Theorem 14 implies that (η(cid:96)/η(cid:96) − 1) S
Theorem 11  (cid:96) is (η−1
is convex on ri ∆k. Thus  η(cid:96) ≤ η(cid:96)  and since from Theorem 7 η(cid:96) ≤ η(cid:96)  we conclude that η(cid:96) = η(cid:96).

(cid:96) S)-mixable. Substituting Φ for η−1

6

From Theorem 14  if (cid:96) is Φ-mixable then η(cid:96)Φ − S is convex on ∆k. Now suppose that η(cid:96)Φ − S is
convex on ∆k. This implies that η(cid:96) > 0  and thus from Theorem 7  (cid:96) is η(cid:96)-mixable. Now since (cid:96) is
η(cid:96)-mixable and η(cid:96)Φ − S is convex on ∆k  Theorem 13 implies that (cid:96) is Φ-mixable.
Note that the condition ‘dom (cid:96) = A’ is in practice not a restriction to ﬁnite losses — see Remark
8. Theorem 16 implies that under the regularity conditions of Theorem 14  the Bayes risk L(cid:96) [resp.
(L(cid:96)  Φ)] contains all necessary information to characterize classical [resp. generalized] mixability.
Corollary 17 (The Generalized Mixability Constant). Let (cid:96) and Φ be as in Theorem 16. Then the
generalized mixability constant (see Deﬁnition 9) is given by

ηΦ
(cid:96) = η(cid:96)

inf

˜q∈int ˜∆k

λmin(H ˜Φ( ˜q)(H˜S( ˜q))−1) 

(11)

where ˜Φ := Φ ◦ (cid:113)k  ˜S = S◦(cid:113)k  and (cid:113)k is deﬁned in (1).
Observe that when Φ = S  (11) reduces to ηS

(cid:96) = η(cid:96) as expected from Theorem 11 and Theorem 16.

3.4 The (In)dependence Between (cid:96) and Φ and the Fundamental Nature of S

So far  we showed that the Φ-mixability of losses satisfying Assumption 1 is characterized by the
convexity of ηΦ−S  where η ∈]0  η(cid:96)] (see Theorems 13 and 14). As a result  and contrary to what was
conjectured previously [6]  the generalized mixability condition does not induce a correspondence
between losses and entropies; for a given loss (cid:96)  there is no particular entropy Φ(cid:96) — speciﬁc to the
choice of (cid:96) — which minimizes the regret of the GAA. Rather  the Shannon entropy S minimizes the
regret regardless of the choice of (cid:96) (see Theorem 18 below). This reﬂects one fundamental aspect of
the Shannon entropy.
Nevertheless  given a loss (cid:96) and entropy Φ  the curvature of the loss surface S(cid:96) determines the
(cid:96) of the GAA; the curvature of S(cid:96) is linked to η(cid:96) through the Hessian of
maximum ‘learning rate’ ηΦ
the Bayes risk (see Theorem 30 in Appendix H.2)  which is in turn linked to ηΦ
(cid:96) in (11) to explicitly compare the regret bounds RΦ
Given a loss (cid:96)  we now use the expression of ηΦ
(cid:96) achieved with the GAA (see (9)) using entropy Φ and the Shannon entropy S  respectively.
and RS
Theorem 18. Let S  Φ : Rk → R ∪ {+∞}  where S is the Shannon entropy and Φ is an entropy
such that ˜Φ := Φ ◦ (cid:113)k is twice differentiable on int ˜∆k. A loss (cid:96) : A → [0  +∞[n with L(cid:96) twice
differentiable on ]0  +∞[n  is Φ-mixable only if RS

(cid:96) through (11).

(cid:96)

(cid:96) ≤ RΦ
(cid:96) .

Theorem 18 is consistent with Vovk’s result [10  §5] which essentially states that the regret bound
(cid:96) = η−1
RS

log k is in general tight for η-mixable losses.

(cid:96)

4 Adaptive Generalized Aggregating Algorithm

In this section  we take advantage of the similarity between the GAA’s update step and the mirror
descent algorithm (see Appendix E) to devise a modiﬁcation to the GAA leading to improved
regret bounds in certain cases. The GAA can be modiﬁed in (at least) two immediate ways; 1)
changing the learning rate at each time step to speed-up convergence; and 2) changing the entropy 
i.e. the regularizer Φ  at each time step — similar to the adaptive mirror descent algorithm [8  5].
In the former case  one can use Corollary 17 to calculate the maximum ‘learning rate’ under the
Φ-mixability constraint. Here  we focus on the second method; changing the entropy at each round.
Algorithm 3 displays the modiﬁed GAA — which we call the Adaptive Generalized Aggregating
(cid:104)q  z(cid:105) − Φ(q) is
Algorithm (AGAA) — in its most general form. In Algorithm 3  Φ(cid:63)(z) := supq∈∆k
the entropic dual of Φ.
Given a (η  Φ)-mixable loss (cid:96)  we verify that Algorithm 3 is well deﬁned; for simplicity  assume that
dom (cid:96) = A and L(cid:96) is twice differentiable on ]0  +∞[n. From the deﬁnition of an entropy  |Φ| < +∞
t is deﬁned and ﬁnite on all Rk (in particular at θt). On the
on ∆k  and thus the entropic dual Φ(cid:63)
other hand  from Proposition 12  Φ is strictly convex on ∆k which implies that Φ(cid:63) (and thus Φ(cid:63)
t ) is
differentiable on Rk (see e.g. [2  Thm. E.4.1.1]). It remains to check that (cid:96) is (η  Φt)-mixable. Since
for η > 0  (η  Φt)-mixability is equivalent to ( 1
η Φt)-mixability (by deﬁnition)  Theorem 16 implies

7

Algorithm 3: Adaptive Generalized Aggregating Algorithm (AGAA)
input

:θ1 = 0 ∈ Rk; A ∆-differentiable entropy Φ : Rk → R ∪ {+∞}; η > 0; A (η  Φ)-mixable
loss (cid:96) : A → [0  +∞[n; A substitution function S(cid:96); A protocol of choosing βt at round t.

output :Learner’s predictions (at∗)
for t = 1 to T do

(cid:16)(cid:2)Mixη

Let Φt(w) := Φ(w) − (cid:104)w  βt − θt(cid:105);
Observe At := at
at∗ ← S(cid:96)
Observe xt ∈ [n] and pick some vt ∈ Rk;
θt+1 ← θt − η(cid:96)xt(At);

1:k ∈ Ak ;
((cid:96)x(At) ∇Φ(cid:63)

t (θt))(cid:3)T

Φt

1≤x≤n

end

(cid:17)

;

// New entropy
// Experts’ predictions
// Learner’s prediction

−η(cid:80)t−1

that (cid:96) is (η  Φt)-mixable if and only if η(cid:96)η−1Φt − S is convex on ∆k. This is in fact the case since
Φt is an afﬁne transformation of Φ  and we have assumed that (cid:96) is (η  Φ)-mixable.
In what follows  we focus on a particular instantiation of Algorithm 3 where we choose βt :=
s=1((cid:96)xs (As) + vs)  for some (arbitrary for now) (vs) ⊂ Rk. The (vt) vectors act as correction
terms in the update step of the AGAA. Using standard duality properties (see Appendix A)  it is easy
to show that the AGAA reduces to the GAA except for the update step where the new distribution
over experts at round t ∈ [T ] is now given by

qt = ∇Φ(cid:63)(∇Φ(qt−1) − η(cid:96)xt(At) − ηvt).

be a loss such that L(cid:96) is twice differentiable on ]0  +∞[n. Let βt = −η(cid:80)t−1

Theorem 19. Let Φ : Rk → R ∪ {+∞} be a ∆-differentiable entropy. Let (cid:96) : A → [0  +∞]n
s=1((cid:96)xs (As) + vs) 
1:k ∈ Ak. If (cid:96) is (η  Φ)-mixable then for initial distribution q0 =
where vs ∈ Rk and As := as
t=1  the AGAA achieves the regret
argminq∈∆k

maxθ∈[k] DΦ(eθ  q) and any sequence (xt  at

∀θ ∈ [k]  Loss(cid:96)

AGAA(T ) − Loss(cid:96)

(cid:96) + ∆Rθ(T ) 

(12)

1:k)T
θ(T ) ≤ RΦ

where ∆Rθ(T ) :=(cid:80)T−1

t=1 (vt

θ − (cid:104)vt  qt(cid:105)).

t

(cid:80)t

Theorem 19 implies that if the sequence (vt) is chosen such that ∆Rθ∗ (T ) is negative for the best
expert θ∗ (in hindsight)  then the regret bound ‘RΦ
(cid:96) + ∆Rθ∗ (T )’ of the AGAA is lower than that of
the GAA (see (9))  and ultimately that of the AA (when Φ = S). Unfortunately  due to Vovk’s result
[10  §5] there is no “universal” choice of (vt) which guarantees that ∆Rθ∗ (T ) is always negative.
However  there are cases where this term is expected to be negative.
Consider a dataset where it is typical for the best experts (i.e.  the θ∗’s) to perform poorly at some
point during the game  as measured by their average loss  for example. Under such an assumption 
choosing the correction vectors vt to be negatively proportional to the average losses of experts  i.e.
vt := − α
s=1 (cid:96)xs (As) (for small enough α > 0)  would be consistent with the idea of making
∆Rθ∗ (T ) negative. To see this  suppose expert θ∗ is performing poorly during the game (say at
t < T )  as measured by its instantaneous and average loss. At that point the distribution qt would put
more weight on experts performing better than θ∗  i.e. having a lower average loss. And since vt
θ is
θ∗ − (cid:104)vt  qt(cid:105) would be negative
negatively proportional to the average loss of expert θ  the quantity vt
— consistent with making ∆Rθ∗ (T ) < 0. On the other hand  if expert θ∗ performs well during the
θ∗ −(cid:104)vt  qt(cid:105) (cid:39) 0  since qt would put comparable weights between
game (say close to the best) then vt
θ∗ and other experts (if any) with similar performance.
Example 1. (A Negative Regret). One can construct an example that illustrates the idea above. Con-
(∆2  2); a probability game with 2 experts {θ1  θ2}  2 outcomes {0  1} 
sider the Brier game G2
and where the loss (cid:96)Brier is the Brier loss [11] (which is 1-mixable). Assume that; expert θ1 consis-
tently predicts Pr(x = 0) = 1/2; expert θ2 predicts Pr(x = 0) = 1/4 during the ﬁrst 50 rounds  then
switches to predicting Pr(x = 0) = 3/4 thereafter; the outcome is always x = 0. A straightforward
simulation using the AGAA with the Shannon entropy  Vovk’s substitution function for the Brier loss
+ ∆Rθ∗ (T ) (cid:39) −5 
[11]  βt as in Theorem 19 with vt := − 1

s=1 (cid:96)Brier(xs  As)  yields RΦ

(cid:80)t

(cid:96)Brier

(cid:96)Brier

8t

8

∀T ≥ 150  where in this case θ∗ = θ2 is the best expert for T ≥ 150. The learner then does better than
the best expert. If we use the AA instead  the learner does worse than θ2 by (cid:39) RS

= log 2.

(cid:96)Brier

In real data  the situation described above — where the best expert does not necessarily perform
optimally during the game — is typical  especially when the number of rounds T is large. We have
tested the aggregating algorithms on real data as studied by Vovk [11]. We compared the performance
of the AA with the AGAA  and found that the AGAA outperforms the AA  and in fact achieved a
negative regret on two data sets. Details of the experiments are in Appendix J.
As pointed out earlier  there are situations where ∆Rθ∗ (T ) ≥ 0 even for the choice of (vt) in
Example 1  and this could potentially lead to a large positive regret for the AGAA. There is an easy
way to remove this risk at a small price; the outputs of the AGAA and the AA can themselves be
considered as expert predictions. These predictions can in turn be passed to a new instance of the
AA to yield a meta prediction. The resulting worst case regret is guaranteed not to exceed that of the
original AA instance by more than η−1 log 2 for an η-mixable loss. We test this idea in Appendix J.

5 Discussion and Future Work

In this work  we derived a characterization of Φ-mixability  which enables a better understanding of
when a constant regret is achievable in the game of prediction with expert advice. Then  borrowing
techniques from mirror descent  we proposed a new “adaptive” version of the generalized aggregating
algorithm. We derived a regret bound for a speciﬁc instantiation of this algorithm and discussed
certain situations where the algorithm is expected to perform well. We empirically demonstrated the
performance of this algorithm on football game predictions (see Appendix J).
Vovk [10  §5] essentially showed that given an η-mixable loss there is no algorithm that can achieve
a lower regret bound than η−1 log k on all sequences of outcomes. There is no contradiction in trying
to design algorithms which perform well in expectation (maybe better than the AA) on “typical” data
while keeping the worst case regret close to η−1 log k. This was the motivation behind the AGAA.
In future work  we will explore other choices for the correction vector vt with the goal of lowering
the (expected) bound in (12). In the present work  we did not study the possibility of varying the
learning rate η. One might obtain better regret bounds using an adaptive learning rate as is the case
with the mirror descent algorithm. Our Corollary 17 is useful in that it gives an upper bound on the
maximal learning rate under the Φ-mixability constraint. Finally  although our Theorem 18 states that
worst-case regret of the GAA is minimized when using the Shannon entropy  it would be interesting
to study the dynamics of the AGAA with other entropies.

Table 1: A short list of the main symbols used in the paper

Symbol Description
(cid:96)
S(cid:96)
(cid:96)
L(cid:96)
˜L(cid:96)
S
η(cid:96)
η(cid:96)
ηΦ
(cid:96)
S(cid:96)
RΦ
(cid:96)

A loss function deﬁned on a set A and taking values in [0  +∞]n (see Sec. 2)
The ﬁnite part of the superprediction set of a loss (cid:96) (see Sec. 2)
The support loss of a loss (cid:96) (see Def. 3)
The Bayes risk corresponding to a loss (cid:96) (see Deﬁnition 2)
The composition of the Bayes risk with an afﬁne function; ˜L(cid:96) := L(cid:96) ◦ (cid:113)n (see (1))
The Shannon Entropy (see (6))
The mixability constant of (cid:96) (see Def. 6) ; essentially the largest η s.t. (cid:96) is η-mixable.
Essentially the largest η such that ηL(cid:96) − Llog is convex (see (5) and [9])
The generalized mixability constant (see Def. 9); the largest η s.t. (cid:96) is (η  Φ)-mixable.
A substitution function of a loss (cid:96) (see Sec. 3.1)
The regret achieved by the GAA using entropy Φ (see (9) and Algorithm 2)

Acknowledgments

This work was supported by the Australian Research Council and DATA61.

9

References
[1] Alexey Chernov  Yuri Kalnishkan  Fedor Zhdanov  and Vladimir Vovk. Supermartingales in

prediction with expert advice. Theoretical Computer Science  411(29-30):2647–2669  2010.

[2] J-B. Hiriart-Urruty and C. Lemaréchal. Fundamentals of convex analysis.  2001.

[3] Yuri Kalnishkan  Volodya Vovk  and Michael V. Vyugin. Loss functions  complexities  and the

legendre transformation. Theoretical Computer Science  313(2):195–207  2004.

[4] Parameswaran Kamalaruban  Robert Williamson  and Xinhua Zhang. Exp-concavity of proper

composite losses. In Conference on Learning Theory  pages 1035–1065  2015.

[5] Francesco Orabona  Koby Crammer  and Nicolò Cesa-Bianchi. A generalized online mirror
descent with applications to classiﬁcation and regression. Machine Learning  99(3):411–435 
2015.

[6] Mark D. Reid  Rafael M. Frongillo  Robert C. Williamson  and Nishant Mehta. Generalized

mixability via entropic duality. In Conference on Learning Theory  pages 1501–1522  2015.

[7] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press  Princeton  NJ  1997.

[8] Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated
gradient algorithm. In International Conference on Machine Learning  pages 1593–1601  2014.

[9] Tim van Erven  Mark D. Reid  and Robert C. Williamson. Mixability is Bayes risk curvature

relative to log loss. Journal of Machine Learning Research  13:1639–1663  2012.

[10] Vladimir Vovk. A game of prediction with expert advice. Journal of Computer and System

Sciences  56(2):153–173  1998.

[11] Vladimir Vovk and Fedor Zhdanov. Prediction with expert advice for the brier game. Journal of

Machine Learning Research  10(Nov):2445–2471  2009.

[12] Volodya Vovk. Competitive on-line statistics. International Statistical Review  69(2):213–248 

2001.

[13] Robert C. Williamson. The geometry of losses. In Conference on Learning Theory  pages

1078–1108  2014.

[14] Robert C. Williamson  Elodie Vernet  and Mark D. Reid. Composite multiclass losses. Journal

of Machine Learning Research  17:223:1–223:52  2016.

10

,Zakaria Mhammedi
Robert Williamson