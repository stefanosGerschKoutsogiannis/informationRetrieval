2016,Interaction Networks for Learning about Objects  Relations and Physics,Reasoning about objects  relations  and physics is central to human intelligence  and a key goal of artificial intelligence. Here we introduce the interaction network  a model which can reason about how objects in complex systems interact  supporting dynamical predictions  as well as inferences about the abstract properties of the system. Our model takes graphs as input  performs object- and relation-centric reasoning in a way that is analogous to a simulation  and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems  rigid-body collision  and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps  estimate abstract quantities such as energy  and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose  learnable physics engine  and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.,Interaction Networks for Learning about Objects 

Relations and Physics

Anonymous Author(s)

Afﬁliation
Address
email

Abstract

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

16

17
18
19
20
21
22
23
24

25
26
27
28
29
30
31

32
33
34
35
36

Reasoning about objects  relations  and physics is central to human intelligence  and
a key goal of artiﬁcial intelligence. Here we introduce the interaction network  a
model which can reason about how objects in complex systems interact  supporting
dynamical predictions  as well as inferences about the abstract properties of the
system. Our model takes graphs as input  performs object- and relation-centric
reasoning in a way that is analogous to a simulation  and is implemented using
deep neural networks. We evaluate its ability to reason about several challenging
physical domains: n-body problems  rigid-body collision  and non-rigid dynamics.
Our results show it can be trained to accurately simulate the physical trajectories of
dozens of objects over thousands of time steps  estimate abstract quantities such
as energy  and generalize automatically to systems with different numbers and
conﬁgurations of objects and relations. Our interaction network implementation
is the ﬁrst general-purpose  learnable physics engine  and a powerful general
framework for reasoning about object and relations in a wide variety of complex
real-world domains.

1

Introduction

Representing and reasoning about objects  relations and physics is a “core” domain of human common
sense knowledge [25]  and among the most basic and important aspects of intelligence [27  15]. Many
everyday problems  such as predicting what will happen next in physical environments or inferring
underlying properties of complex scenes  are challenging because their elements can be composed
in combinatorially many possible arrangements. People can nevertheless solve such problems by
decomposing the scenario into distinct objects and relations  and reasoning about the consequences
of their interactions and dynamics. Here we introduce the interaction network – a model that can
perform an analogous form of reasoning about objects and relations in complex systems.
Interaction networks combine three powerful approaches: structured models  simulation  and deep
learning. Structured models [7] can exploit rich  explicit knowledge of relations among objects 
independent of the objects themselves  which supports general-purpose reasoning across diverse
contexts. Simulation is an effective method for approximating dynamical systems  predicting how the
elements in a complex system are inﬂuenced by interactions with one another  and by the dynamics
of the system. Deep learning [23  16] couples generic architectures with efﬁcient optimization
algorithms to provide highly scalable learning and inference in challenging real-world settings.
Interaction networks explicitly separate how they reason about relations from how they reason about
objects  assigning each task to distinct models which are: fundamentally object- and relation-centric;
and independent of the observation modality and task speciﬁcation (see Model section 2 below
and Fig. 1a). This lets interaction networks automatically generalize their learning across variable
numbers of arbitrarily ordered objects and relations  and also recompose their knowledge of entities

Submitted to 30th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.

Figure 1: Schematic of an interaction network. a. For physical reasoning  the model takes objects and relations
as input  reasons about their interactions  and applies the effects and physical dynamics to predict new states. b.
For more complex systems  the model takes as input a graph that represents a system of objects  oj  and relations 
(cid:104)i  j  rk(cid:105)k  instantiates the pairwise interaction terms  bk  and computes their effects  ek  via a relational model 
fR(·). The ek are then aggregated and combined with the oj and external effects  xj  to generate input (as cj) 
for an object model  fO(·)  which predicts how the interactions and dynamics inﬂuence the objects  p.

37
38
39

40
41
42
43
44
45
46
47
48

49
50
51
52
53
54
55
56

57
58
59
60
61

62
63
64
65
66
67
68
69
70
71

and interactions in novel and combinatorially many ways. They take relations as explicit input 
allowing them to selectively process different potential interactions for different input data  rather
than being forced to consider every possible interaction or those imposed by a ﬁxed architecture.
We evaluate interaction networks by testing their ability to make predictions and inferences about var-
ious physical systems  including n-body problems  and rigid-body collision  and non-rigid dynamics.
Our interaction networks learn to capture the complex interactions that can be used to predict future
states and abstract physical properties  such as energy. We show that they can roll out thousands of
realistic future state predictions  even when trained only on single-step predictions. We also explore
how they generalize to novel systems with different numbers and conﬁgurations of elements. Though
they are not restricted to physical reasoning  the interaction networks used here represent the ﬁrst
general-purpose learnable physics engine  and even have the potential to learn novel physical systems
for which no physics engines currently exist.

Related work Our model draws inspiration from previous work that reasons about graphs and
relations using neural networks. The “graph neural network” [22] is a framework that shares learning
across nodes and edges  the “recursive autoencoder” [24] adapts its processing architecture to exploit
an input parse tree  the “neural programmer-interpreter” [21] is a composable neural network that
mimics the execution trace of a program  and the “spatial transformer” [11] learns to dynamically
modify network connectivity to capture certain types of interactions. Others have explored deep
learning of logical and arithmetic relations [26]  and relations suitable for visual question-answering
[1].
The behavior of our model is similar in spirit to a physical simulation engine [2]  which generates
sequences of states by repeatedly applying rules that approximate the effects of physical interactions
and dynamics on objects over time. The interaction rules are relation-centric  operating on two or
more objects that are interacting  and the dynamics rules are object-centric  operating on individual
objects and the aggregated effects of the interactions they participate in.
Previous AI work on physical reasoning explored commonsense knowledge  qualitative representa-
tions  and simulation techniques for approximating physical prediction and inference [28  9  6]. The
“NeuroAnimator” [8] was perhaps the ﬁrst quantitative approach to learning physical dynamics  by
training neural networks to predict and control the state of articulated bodies. Ladický et al. [14]
recently used regression forests to learn ﬂuid dynamics. Recent advances in convolutional neural
networks (CNNs) have led to efforts that learn to predict coarse-grained physical dynamics from
images [19  17  18]. Notably  Fragkiadaki et al. [5] used CNNs to predict and control a moving
ball from an image centered at its coordinates. Mottaghi et al. [20] trained CNNs to predict the 3D
trajectory of an object after an external impulse is applied. Wu et al. [29] used CNNs to parse objects
from images  which were then input to a physics engine that supported prediction and inference.

2

Object reasoningRelational reasoningCompute interaction Apply object dynamicsEffectsObjects relationsPredictions  inferencesa.b.72

73
74
75
76
77
78
79
80
81
82
83
84

85
86
87
88
89

90
91
92
93

94

95

96
97
98
99
100
101
102
103
104

105
106
107
108

109
110
111
112
113

114
115
116
117
118

2 Model

Deﬁnition To describe our model  we use physical reasoning as an example (Fig. 1a)  and build
from a simple model to the full interaction network (abbreviated IN). To predict the dynamics of a
single object  one might use an object-centric function  fO  which inputs the object’s state  ot  at
time t  and outputs a future state  ot+1. If two or more objects are governed by the same dynamics 
fO could be applied to each  independently  to predict their respective future states. But if the
objects interact with one another  then fO is insufﬁcient because it does not capture their relationship.
Assuming two objects and one directed relationship  e.g.  a ﬁxed object attached by a spring to a freely
moving mass  the ﬁrst (the sender  o1) inﬂuences the second (the receiver  o2) via their interaction.
The effect of this interaction  et+1  can be predicted by a relation-centric function  fR. The fR takes
as input o1  o2  as well as attributes of their relationship  r  e.g.  the spring constant. The fO is
modiﬁed so it can input both et+1 and the receiver’s current state  o2 t  enabling the interaction to
inﬂuence its future state  o2 t+1 

et+1 = fR(o1 t  o2 t  r)

o2 t+1 = fO(o2 t  et+1)

The above formulation can be expanded to larger and more complex systems by representing them
as a graph  G = (cid:104)O  R(cid:105)  where the nodes  O  correspond to the objects  and the edges  R  to the
relations (see Fig. 1b). We assume an attributed  directed multigraph because the relations have
attributes  and there can be multiple distinct relations between two objects (e.g.  rigid and magnetic
interactions). For a system with NO objects and NR relations  the inputs to the IN are 
O = {oj}j=1...NO   R = {(cid:104)i  j  rk(cid:105)k}k=1...NR where i (cid:54)= j  1 ≤ i  j ≤ NO   X = {xj}j=1...NO
The O represents the states of each object. The triplet  (cid:104)i  j  rk(cid:105)k  represents the k-th relation in the
system  from sender  oi  to receiver  oj  with relation attribute  rk. The X represents external effects 
such as active control inputs or gravitational acceleration  which we deﬁne as not being part of the
system  and which are applied to each object separately.
The basic IN is deﬁned as 

IN(G) = φO(a(G  X  φR( m(G) ) ))

m(G) = B = {bk}k=1...NR
fR(bk) = ek
φR(B) = E = {ek}k=1...NR

a(G  X  E) = C = {cj}j=1...NO
fO(cj)
= pj
= P = {pj}j=1...NO
φO(C)

(1)

(2)

The marshalling function  m  rearranges the objects and relations into interaction terms  bk =
(cid:104)oi  oj  rk(cid:105) ∈ B  one per relation  which correspond to each interaction’s receiver  sender  and
relation attributes. The relational model  φR  predicts the effect of each interaction  ek ∈ E  by
applying fR to each bk. The aggregation function  a  collects all effects  ek ∈ E  that apply to each
receiver object  merges them  and combines them with O and X to form a set of object model inputs 
cj ∈ C  one per object. The object model  φO  predicts how the interactions and dynamics inﬂuence
the objects by applying fO to each cj  and returning the results  pj ∈ P . This basic IN can predict
the evolution of states in a dynamical system – for physical simulation  P may equal the future states
of the objects  Ot+1.
The IN can also be augmented with an additional component to make abstract inferences about the
system. The pj ∈ P   rather than serving as output  can be combined by another aggregation function 
g  and input to an abstraction model  φA  which returns a single output  q  for the whole system. We
explore this variant in our ﬁnal experiments that use the IN to predict potential energy.
An IN applies the same fR and fO to every bk and cj  respectively  which makes their relational and
object reasoning able to handle variable numbers of arbitrarily ordered objects and relations. But
one additional constraint must be satisﬁed to maintain this: the a function must be commutative and
associative over the objects and relations. Using summation within a to merge the elements of E into
C satisﬁes this  but division would not.
Here we focus on binary relations  which means there is one interaction term per relation  but another
option is to have the interactions correspond to n-th order relations by combining n senders in each bk.
The interactions could even have variable order  where each bk includes all sender objects that interact
with a receiver  but would require a fR than can handle variable-length inputs. These possibilities are
beyond the scope of this work  but are interesting future directions.

3

119
120
121
122
123

124
125
126
127
128

129

130

131
132
133
134
135
136

137
138
139

140
141
142
143
144

145
146

147
148
149
150

151
152

153
154
155
156
157
158
159
160
161
162
163

164

165
166
167
168
169
170

(cid:104) 0 0

(cid:105)

1 1
0 0

0 0
0 1

(cid:105)

and Rs =

(cid:104) 1 0

Implementation The general deﬁnition of the IN in the previous section is agnostic to the choice
of functions and algorithms  but we now outline a learnable implementation capable of reasoning
about complex systems with nonlinear relations and dynamics. We use standard deep neural network
building blocks  multilayer perceptrons (MLP)  matrix operations  etc.  which can be trained efﬁciently
from data using gradient-based optimization  such as stochastic gradient descent.
We deﬁne O as a DS × NO matrix  whose columns correspond to the objects’ DS-length state vectors.
The relations are a triplet  R = (cid:104)Rr  Rs  Ra(cid:105)  where Rr and Rs are NO × NR binary matrices which
index the receiver and sender objects  respectively  and Ra is a DR × NR matrix whose DR-length
columns represent the NR relations’ attributes. The j-th column of Rr is a one-hot vector which
indicates the receiver object’s index; Rs indicates the sender similarly. For the graph in Fig. 1b 
. The X is a DX × NO matrix  whose columns are DX-length vectors
Rr =
that represent the external effect applied each of the NO objects.
The marshalling function  m  computes the matrix products  ORr and ORs  and concatenates them
with Ra: m(G) = [ORr; ORs; Ra] = B .
The resulting B is a (2DS + DR) × NR matrix  whose columns represent the interaction terms  bk 
for the NR relations (we denote vertical and horizontal matrix concatenation with a semicolon and
comma  respectively). The way m constructs interaction terms can be modiﬁed  as described in our
Experiments section (3).
The B is input to φR  which applies fR  an MLP  to each column. The output of fR is a DE-length
vector  ek  a distributed representation of the effects. The φR concatenates the NR effects to form the
DE × NR effect matrix  E.
The G  X  and E are input to a  which computes the DE × NO matrix product  ¯E = ERT
r   whose
j-th column is equivalent to the elementwise sum across all ek whose corresponding relation has
a(G  X  E) = [O; X; ¯E] = C.
receiver object  j. The ¯E is concatenated with O and X:
The resulting C is a (DS + DX + DE) × NO matrix  whose NO columns represent the object states 
external effects  and per-object aggregate interaction effects.
The C is input to φO  which applies fO  another MLP  to each of the NO columns. The output of fO
is a DP -length vector  pj  and φO concatenates them to form the output matrix  P .
To infer abstract properties of a system  an additional φA is appended and takes P as input. The g
aggregation function performs an elementwise sum across the columns of P to return a DP -length
vector  ¯P . The ¯P is input to φA  another MLP  which returns a DA-length vector  q  that represents
an abstract  global property of the system.
Training an IN requires optimizing an objective function over the learnable parameters of φR and φO.
Note  m and a involve matrix operations that do not contain learnable parameters.
Because φR and φO are shared across all relations and objects  respectively  training them is statisti-
cally efﬁcient. This is similar to CNNs  which are very efﬁcient due to their weight-sharing scheme.
A CNN treats a local neighborhood of pixels as related  interacting entities: each pixel is effectively
a receiver object and its neighboring pixels are senders. The convolution operator is analogous to
φR  where fR is the local linear/nonlinear kernel applied to each neighborhood. Skip connections 
recently popularized by residual networks  are loosely analogous to how the IN inputs O to both
φR and φO  though in CNNs relation- and object-centric reasoning are not delineated. But because
CNNs exploit local interactions in a ﬁxed way which is well-suited to the speciﬁc topology of images 
capturing longer-range dependencies requires either broad  insensitive convolution kernels  or deep
stacks of layers  in order to implement sufﬁciently large receptive ﬁelds. The IN avoids this restriction
by being able to process arbitrary neighborhoods that are explicitly speciﬁed by the R input.

3 Experiments

Physical reasoning tasks Our experiments explored two types of physical reasoning tasks: pre-
dicting future states of a system  and estimating their abstract properties  speciﬁcally potential energy.
We evaluated the IN’s ability to learn to make these judgments in three complex physical domains:
n-body systems; balls bouncing in a box; and strings composed of springs that collide with rigid
objects. We simulated the 2D trajectories of the elements of these systems with a physics engine  and
recorded their sequences of states. See the Supplementary Material for full details.

4

171
172
173
174
175
176

177
178
179
180
181
182
183
184
185
186

187
188
189
190
191
192
193
194
195
196
197
198

199
200
201
202
203
204
205
206
207
208
209
210

211
212
213
214
215

216
217
218
219
220
221
222

223
224
225

In the n-body domain  such as solar systems  all n bodies exert distance- and mass-dependent
gravitational forces on each other  so there were n(n − 1) relations input to our model. Across
simulations  the objects’ masses varied  while all other ﬁxed attributes were held constant. The
training scenes always included 6 bodies  and for testing we used 3  6  and 12 bodies. In half of
the systems  bodies were initialized with velocities that would cause stable orbits  if not for the
interactions with other objects; the other half had random velocities.
In the bouncing balls domain  moving balls could collide with each other and with static walls.
The walls were represented as objects whose shape attribute represented a rectangle  and whose
inverse-mass was 0. The relations input to the model were between the n objects (which included the
walls)  for (n(n− 1) relations). Collisions are more difﬁcult to simulate than gravitational forces  and
the data distribution was much more challenging: each ball participated in a collision on less than 1%
of the steps  following straight-line motion at all other times. The model thus had to learn that despite
there being a rigid relation between two objects  they only had meaningful collision interactions when
they were in contact. We also varied more of the object attributes – shape  scale and mass (as before)
– as well as the coefﬁcient of restitution  which was a relation attribute. Training scenes contained 6
balls inside a box with 4 variably sized walls  and test scenes contained either 3  6  or 9 balls.
The string domain used two types of relations (indicated in rk)  relation structures that were more
sparse and speciﬁc than all-to-all  as well as variable external effects. Each scene contained a string 
comprised of masses connected by springs  and a static  rigid circle positioned below the string. The
n masses had spring relations with their immediate neighbors (2(n − 1))  and all masses had rigid
relations with the rigid object (2n). Gravitational acceleration  with a magnitude that was varied
across simulation runs  was applied so that the string always fell  usually colliding with the static
object. The gravitational acceleration was an external input (not to be confused with the gravitational
attraction relations in the n-body experiments). Each training scene contained a string with 15 point
masses  and test scenes contained either 5  15  or 30 mass strings. In training  one of the point masses
at the end of the string  chosen at random  was always held static  as if pinned to the wall  while the
other masses were free to move. In the test conditions  we also included strings that had both ends
pinned  and no ends pinned  to evaluate generalization.
Our model takes as input the state of each system  G  decomposed into the objects  O (e.g.  n-body
objects  balls  walls  points masses that represented string elements)  and their physical relations  R
(e.g.  gravitational attraction  collisions  springs)  as well as the external effects  X (e.g.  gravitational
acceleration). Each object state  oj  could be further divided into a dynamic state component
(e.g.  position and velocity) and a static attribute component (e.g.  mass  size  shape). The relation
attributes  Ra  represented quantities such as the coefﬁcient of restitution  and spring constant. The
input represented the system at the current time. The prediction experiment’s target outputs were the
velocities of the objects on the subsequent time step  and the energy estimation experiment’s targets
were the potential energies of the system on the current time step. We also generated multi-step
rollouts for the prediction experiments (Fig. 2)  to assess the model’s effectiveness at creating visually
realistic simulations. The output velocity  vt  on time step t became the input velocity on t + 1  and
the position at t + 1 was updated by the predicted velocity at t.

Data Each of the training  validation  test data sets were generated by simulating 2000 scenes
over 1000 time steps  and randomly sampling 1 million  200k  and 200k one-step input/target pairs 
respectively. The model was trained for 2000 epochs  randomly shufﬂing the data indices between
each. We used mini-batches of 100  and balanced their data distributions so the targets had similar
per-element statistics. The performance reported in the Results was measured on held-out test data.
We explored adding a small amount of Gaussian noise to 20% of the data’s input positions and
velocities during the initial phase of training  which was reduced to 0% from epochs 50 to 250. The
noise std. dev. was 0.05× the std. dev. of each element’s values across the dataset. It allowed the
model to experience physically impossible states which could not have been generated by the physics
engine  and learn to project them back to nearby  possible states. Our error measure did not reﬂect
clear differences with or without noise  but rollouts from models trained with noise were slightly
more visually realistic  and static objects were less subject to drift over many steps.

Model architecture The fR and fO MLPs contained multiple hidden layers of linear transforms
plus biases  followed by rectiﬁed linear units (ReLUs)  and an output layer that was a linear transform
plus bias. The best model architecture was selected by a grid search over layer sizes and depths. All

5

Figure 2: Prediction rollouts. Each column contains three panels of three video frames (with motion blur) 
each spanning 1000 rollout steps. Columns 1-2 are ground truth and model predictions for n-body systems  3-4
are bouncing balls  and 5-6 are strings. Each model column was generated by a single model  trained on the
underlying states of a system of the size in the top panel. The middle and bottom panels show its generalization
to systems of different sizes and structure. For n-body  the training was on 6 bodies  and generalization was to 3
and 12 bodies. For balls  the training was on 6 balls  and generalization was to 3 and 9 balls. For strings  the
training was on 15 masses with 1 end pinned  and generalization was to 30 masses with 0 and 2 ends pinned.

6

TrueModelTrueModelTrueModelTimeTimeTime226
227
228

229
230
231
232
233
234
235

236
237
238

239
240
241

242
243
244
245
246

247
248
249
250

251

252
253
254
255
256
257
258
259
260

261
262
263
264
265
266

267
268
269
270
271
272
273
274

275
276
277

inputs (except Rr and Rs) were normalized by centering at the median and rescaling the 5th and 95th
percentiles to -1 and 1. All training objectives and test measures used mean squared error (MSE)
between the model’s prediction and the ground truth target.
All prediction experiments used the same architecture  with parameters selected by a hyperparameter
search. The fR MLP had four  150-length hidden layers  and output length DE = 50. The fO MLP
had one  100-length hidden layer  and output length DP = 2  which targeted the x  y-velocity. The
m and a were customized so that the model was invariant to the absolute positions of objects in the
scene. The m concatenated three terms for each bk: the difference vector between the dynamic states
of the receiver and sender  the concatenated receiver and sender attribute vectors  and the relation
attribute vector. The a only outputs the velocities  not the positions  for input to φO.
The energy estimation experiments used the IN from the prediction experiments with an additional
φA MLP which had one  25-length hidden layer. Its P inputs’ columns were length DP = 10  and
its output length was DA = 1.
We optimized the parameters using Adam [13]  with a waterfall schedule that began with a learning
rate of 0.001 and down-scaled the learning rate by 0.8 each time the validation error  estimated over
a window of 40 epochs  stopped decreasing.
Two forms of L2 regularization were explored: one applied to the effects  E  and another to the model
parameters. Regularizing E improved generalization to different numbers of objects and reduced
drift over many rollout steps. It likely incentivizes sparser communication between the φR and φO 
prompting them to operate more independently. Regularizing the parameters generally improved
performance and reduced overﬁtting. Both penalty factors were selected by a grid search.
Few competing models are available in the literature to compare our model against  but we considered
several alternatives: a constant velocity baseline which output the input velocity; an MLP baseline 
with two 300-length hidden layers  which took as input a ﬂattened vector of all of the input data; and
a variant of the IN with the φR component removed (the interaction effects  E  was set to a 0-matrix).

4 Results

Prediction experiments Our results show that the IN can predict the next-step dynamics of our task
domains very accurately after training  with orders of magnitude lower test error than the alternative
models (Fig. 3a  d and g  and Table 1). Because the dynamics of each domain depended crucially on
interactions among objects  the IN was able to learn to exploit these relationships for its predictions.
The dynamics-only IN had no mechanism for processing interactions  and performed similarly to the
constant velocity model. The baseline MLP’s connectivity makes it possible  in principle  for it to
learn the interactions  but that would require learning how to use the relation indices to selectively
process the interactions. It would also not beneﬁt from sharing its learning across relations and
objects  instead being forced to approximate the interactive dynamics in parallel for each objects.
The IN also generalized well to systems with fewer and greater numbers of objects (Figs. 3b-c  e-f
and h-k  and Table SM1 in Supp. Mat.). For each domain  we selected the best IN model from the
system size on which it was trained  and evaluated its MSE on a different system size. When tested
on smaller n-body and spring systems from those on which it was trained  its performance actually
exceeded a model trained on the smaller system. This may be due to the model’s ability to exploit its
greater experience with how objects and relations behave  available in the more complex system.
We also found that the IN trained on single-step predictions can be used to simulate trajectories over
thousands of steps very effectively  often tracking the ground truth closely  especially in the n-body
and string domains. When rendered into images and videos  the model-generated trajectories are
usually visually indistinguishable from those of the ground truth physics engine (Fig. 2; see Supp.
Mat. for videos of all images). This is not to say that given the same initial conditions  they cohere
perfectly: the dynamics are highly nonlinear and imperceptible prediction errors by the model can
rapidly lead to large differences in the systems’ states. But the incoherent rollouts do not violate
people’s expectations  and might be roughly on par with people’s understanding of these domains.

Estimating abstract properties We trained an abstract-estimation variant of our model to predict
potential energies in the n-body and string domains (the ball domain’s potential energies were always
0)  and found it was much more accurate (n-body MSE 1.4  string MSE 1.1) than the MLP baseline

7

Figure 3: Prediction experiment accuracy and generalization. Each colored bar represents the MSE between a
model’s predicted velocity and the ground truth physics engine’s (the y-axes are log-scaled). Sublots (a-c) show
n-body performance  (d-f) show balls  and (g-k) show string. The leftmost subplots in each (a  d  g) for each
domain compare the constant velocity model (black)  baseline MLP (grey)  dynamics-only IN (red)  and full IN
(blue). The other panels show the IN’s generalization performance to different numbers and conﬁgurations of
objects  as indicated by the subplot titles. For the string systems  the numbers correspond to: (the number of
masses  how many ends were pinned).

Table 1: Prediction experiment MSEs

Domain Constant velocity Baseline Dynamics-only IN
n-body
Balls
String

82
0.074
0.018

0.072
0.016

79

76

0.074
0.017

IN
0.25
0.0020
0.0011

278
279

280

281
282
283
284
285
286
287
288
289
290
291
292

293
294
295
296
297
298
299
300
301
302
303

304
305
306
307
308
309

(n-body MSE 19  string MSE 425). The IN presumably learns the gravitational and spring potential
energy functions  applies them to the relations in their respective domains  and combines the results.

5 Discussion

We introduced interaction networks as a ﬂexible and efﬁcient model for explicit reasoning about
objects and relations in complex systems. Our results provide surprisingly strong evidence of their
ability to learn accurate physical simulations and generalize their training to novel systems with
different numbers and conﬁgurations of objects and relations. They could also learn to infer abstract
properties of physical systems  such as potential energy. The alternative models we tested performed
much more poorly  with orders of magnitude greater error. Simulation over rich mental models is
thought to be a crucial mechanism of how humans reason about physics and other complex domains
[4  12  10]  and Battaglia et al. [3] recently posited a simulation-based “intuitive physics engine”
model to explain human physical scene understanding. Our interaction network implementation is the
ﬁrst learnable physics engine that can scale up to real-world problems  and is a promising template for
new AI approaches to reasoning about other physical and mechanical systems  scene understanding 
social perception  hierarchical planning  and analogical reasoning.
In the future  it will be important to develop techniques that allow interaction networks to handle
very large systems with many interactions  such as by culling interaction computations that will have
negligible effects. The interaction network may also serve as a powerful model for model-predictive
control inputting active control signals as external effects – because it is differentiable  it naturally
supports gradient-based planning. It will also be important to prepend a perceptual front-end that
can infer from objects and relations raw observations  which can then be provided as input to an
interaction network that can reason about the underlying structure of a scene. By adapting the
interaction network into a recurrent neural network  even more accurate long-term predictions might
be possible  though preliminary tests found little beneﬁt beyond its already-strong performance.
By modifying the interaction network to be a probabilistic generative model  it may also support
probabilistic inference over unknown object properties and relations.
By combining three powerful tools from the modern machine learning toolkit – relational reasoning
over structured knowledge  simulation  and deep learning – interaction networks offer ﬂexible 
accurate  and efﬁcient learning and inference in challenging domains. Decomposing complex
systems into objects and relations  and reasoning about them explicitly  provides for combinatorial
generalization to novel contexts  one of the most important future challenges for AI  and a crucial
step toward closing the gap between how humans and machines think.

8

10-210-3g. 15  1h. 5  1i. 30  1j. 15  0k. 15  2String110-110102MSE (log-scale)a. 6b. 3c. 12n-body10-210-110-3d. 6e. 3f. 9BallsIN (15 obj  1 pin)IN (5 obj  1 pin)IN (15 obj  0 pin)IN (30 obj  1 pin)IN (15 obj  2 pin)IN (3 obj)IN (12 obj)IN (6 obj)Constant velocityBaseline MLPDynamics-only ININ (3 obj)IN (9 obj)IN (6 obj)10-2310

311
312

313
314

315
316

317

318
319

320
321

322
323

324
325
326

327
328

329

330
331

332
333

334

335
336

337
338

339

340
341

342
343

344
345

346
347

348

349
350

351

352
353

354
355

356
357

358
359

360

361
362

References
[1] J Andreas  M Rohrbach  T Darrell  and D Klein. Learning to compose neural networks for question

answering. NAACL  2016.

[2] D Baraff. Physically based modeling: Rigid body simulation. SIGGRAPH Course Notes  ACM SIGGRAPH 

2(1):2–1  2001.

[3] PW Battaglia  JB Hamrick  and JB Tenenbaum. Simulation as an engine of physical scene understanding.

Proceedings of the National Academy of Sciences  110(45):18327–18332  2013.

[4] K.J.W. Craik. The nature of explanation. Cambridge University Press  1943.
[5] K Fragkiadaki  P Agrawal  S Levine  and J Malik. Learning visual predictive models of physics for playing

billiards. ICLR  2016.

[6] F. Gardin and B. Meltzer. Analogical representations of naive physics. Artiﬁcial Intelligence  38(2):139–

159  1989.

[7] Z. Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature  521(7553):452–459 

2015.

[8] R Grzeszczuk  D Terzopoulos  and G Hinton. Neuroanimator: Fast neural network emulation and control of
physics-based models. In Proceedings of the 25th annual conference on Computer graphics and interactive
techniques  pages 9–20. ACM  1998.

[9] P.J Hayes. The naive physics manifesto. Université de Genève  Institut pour les études s é mantiques et

cognitives  1978.

[10] M. Hegarty. Mechanical reasoning by mental simulation. TICS  8(6):280–285  2004.
[11] M Jaderberg  K Simonyan  and A Zisserman. Spatial transformer networks. In in NIPS  pages 2008–2016 

2015.

[12] P.N. Johnson-Laird. Mental models: towards a cognitive science of language  inference  and consciousness 

volume 6. Cambridge University Press  1983.

[13] D. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR  2015.
[14] L Ladický  S Jeong  B Solenthaler  M Pollefeys  and M Gross. Data-driven ﬂuid simulations using

regression forests. ACM Transactions on Graphics (TOG)  34(6):199  2015.

[15] B Lake  T Ullman  J Tenenbaum  and S Gershman. Building machines that learn and think like people.

arXiv:1604.00289  2016.

[16] Y LeCun  Y Bengio  and G Hinton. Deep learning. Nature  521(7553):436–444  2015.
[17] A Lerer  S Gross  and R Fergus. Learning physical intuition of block towers by example. arXiv:1603.01312 

2016.

[18] W Li  S Azimi  A Leonardis  and M Fritz. To fall or not to fall: A visual approach to physical stability

prediction. arXiv:1604.00066  2016.

[19] R Mottaghi  H Bagherinezhad  M Rastegari  and A Farhadi. Newtonian image understanding: Unfolding

the dynamics of objects in static images. arXiv:1511.04048  2015.

[20] R Mottaghi  M Rastegari  A Gupta  and A Farhadi. " what happens if..." learning to predict the effect of

forces in images. arXiv:1603.05600  2016.

[21] SE Reed and N de Freitas. Neural programmer-interpreters. ICLR  2016.
[22] F. Scarselli  M. Gori  A.C. Tsoi  M. Hagenbuchner  and G. Monfardini. The graph neural network model.

IEEE Trans. Neural Networks  20(1):61–80  2009.

[23] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks  61:85–117  2015.
[24] R Socher  E Huang  J Pennin  C Manning  and A Ng. Dynamic pooling and unfolding recursive autoen-

coders for paraphrase detection. In in NIPS  pages 801–809  2011.

[25] E Spelke  K Breinlinger  J Macomber  and K Jacobson. Origins of knowledge. Psychol. Rev.  99(4):605–

632  1992.

[26] I Sutskever and GE Hinton. Using matrices to model symbolic relationship. In D. Koller  D. Schuurmans 

Y. Bengio  and L. Bottou  editors  in NIPS 21  pages 1593–1600. 2009.

[27] J.B. Tenenbaum  C. Kemp  T.L. Grifﬁths  and N.D. Goodman. How to grow a mind: Statistics  structure 

and abstraction. Science  331(6022):1279  2011.

[28] P Winston and B Horn. The psychology of computer vision  volume 73. McGraw-Hill New York  1975.
[29] J Wu  I Yildirim  JJ Lim  B Freeman  and J Tenenbaum. Galileo: Perceiving physical object properties by

integrating a physics engine with deep learning. In in NIPS  pages 127–135  2015.

9

,Peter Battaglia
Razvan Pascanu
Matthew Lai
Danilo Jimenez Rezende
koray kavukcuoglu
Paul Rubenstein
Olivier Bousquet
Josip Djolonga
Carlos Riquelme
Ilya Tolstikhin