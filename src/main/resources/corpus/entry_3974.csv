2019,On the Global Convergence of (Fast) Incremental Expectation Maximization Methods,The EM algorithm is one of the most popular algorithm for inference in latent data models. The original formulation of the EM algorithm does not scale to large data set  because the whole data set is required at each iteration of the algorithm. To alleviate this problem  Neal and Hinton [1998] have proposed an incremental version of the EM (iEM) in which at each iteration the conditional expectation of the latent data (E-step) is updated only for a mini-batch of observations. Another approach has been proposed by Cappe and Moulines [2009] in which the E-step is replaced by a stochastic approximation step  closely related to stochastic gradient. In this paper  we analyze incremental and stochastic version of the EM algorithm as well as the variance reduced-version of [Chen et al.  2018] in a common unifying framework. We also introduce a new version incremental version  inspired by the SAGA algorithm by Defazio et al. [2014]. We establish non-asymptotic convergence bounds for global convergence. Numerical applications are presented in this article to illustrate our findings.,On the Global Convergence of (Fast) Incremental

Expectation Maximization Methods

Belhal Karimi

CMAP  ´Ecole Polytechnique

Palaiseau  France

Hoi-To Wai

The Chinese University of Hong Kong

Shatin  Hong Kong

belhal.karimi@polytechnique.edu

htwai@se.cuhk.edu.hk

Eric Moulines

CMAP  ´Ecole Polytechnique

Palaiseau  France

Marc Lavielle
INRIA Saclay

Palaiseau  France

eric.moulines@polytechnique.edu

marc.lavielle@inria.fr

Abstract

The EM algorithm is one of the most popular algorithm for inference in latent
data models. The original formulation of the EM algorithm does not scale to large
data set  because the whole data set is required at each iteration of the algorithm.
To alleviate this problem  Neal and Hinton [1998] have proposed an incremental
version of the EM (iEM) in which at each iteration the conditional expectation of
the latent data (E-step) is updated only for a mini-batch of observations. Another
approach has been proposed by Capp´e and Moulines [2009] in which the E-step is
replaced by a stochastic approximation step  closely related to stochastic gradient.
In this paper  we analyze incremental and stochastic version of the EM algorithm
as well as the variance reduced-version of [Chen et al.  2018] in a common unify-
ing framework. We also introduce a new version incremental version  inspired by
the SAGA algorithm by Defazio et al. [2014]. We establish non-asymptotic con-
vergence bounds for global convergence. Numerical applications are presented in
this article to illustrate our ﬁndings.

1

Introduction

n(cid:88)

i=1

1
n

(cid:8) − log g(yi; θ)(cid:9)  

n(cid:88)

i=1

Many problems in machine learning pertain to tackling an empirical risk minimization of the form

(1)

Li(θ) :=

1
n

L(θ) := R(θ) + L(θ) with L(θ) =

min
θ∈Θ
where {yi}n
i=1 are the observations  Θ is a convex subset of Rd for the parameters  R : Θ → R is a
smooth convex regularization function and for each θ ∈ Θ  g(y; θ) is the (incomplete) likelihood of
each individual observation. The objective function L(θ) is possibly non-convex and is assumed to
be lower bounded L(θ) > −∞ for all θ ∈ Θ. In the latent variable model  g(yi; θ)  is the marginal
Z f (zi  yi; θ)µ(dzi)  where
{zi}n
i=1 are the (unobserved) latent variables. We consider the setting where the complete data
likelihood belongs to the curved exponential family  i.e. 

of the complete data likelihood deﬁned as f (zi  yi; θ)  i.e. g(yi; θ) =(cid:82)

f (zi  yi; θ) = h(zi  yi) exp(cid:0)(cid:10)S(zi  yi)| φ(θ)(cid:11) − ψ(θ)(cid:1)  

(2)
where ψ(θ)  h(zi  yi) are scalar functions  φ(θ) ∈ Rk is a vector function  and S(zi  yi) ∈ Rk is the
complete data sufﬁcient statistics. Latent variable models are widely used in machine learning and

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

surrogate function is computed as θ (cid:55)→ Q(θ  θk−1) = (cid:80)n
−(cid:82)

statistics; examples include mixture models for density estimation  clustering document  and topic
modelling; see [McLachlan and Krishnan  2007] and the references therein.
The basic ”batch” EM (bEM) method iteratively computes a sequence of estimates {θk  k ∈ N}
with an initial parameter θ0. Each iteration of bEM is composed of two steps. In the E-step  a
i=1 Qi(θ  θk−1) where Qi(θ  θ(cid:48)) :=
Z log f (zi  yi; θ)p(zi|yi; θ(cid:48))µ(dzi) such that p(zi|yi; θ) := f (zi  yi; θ)/g(yi  θ) is the condi-
tional probability density of the latent variables zi given the observations yi. When f (zi  yi; θ)
follows the curved exponential family model  the E-step amounts to computing the conditional
expectation of the complete data sufﬁcient statistics 

n(cid:88)

s(θ) =

1
n

(cid:90)

si(θ) where

si(θ) =

i=1

Z

S(zi  yi)p(zi|yi; θ)µ(dzi) .

(3)

In the M-step  the surrogate function is minimized producing a new ﬁt of the parameter θk =
arg maxθ∈Θ Q(θ  θk−1). The EM method has several appealing features – it is monotone where
the likelihood do not decrease at each iteration  invariant with respect to the parameterization  nu-
merically stable when the optimization set is well deﬁned  etc. The EM method has been the subject
of considerable interest since its formalization in [Dempster et al.  1977].
With the sheer size of data sets today  the bEM method is not applicable as the E-step (3) involves
a full pass over the dataset of n observations. Several approaches based on stochastic optimization
have been proposed to address this problem. Neal and Hinton [1998] proposed (but not analyzed)
an incremental version of EM  referred to as the iEM method. Capp´e and Moulines [2009] de-
veloped the online EM (sEM) method which uses a stochastic approximation procedure to track
the sufﬁcient statistics deﬁned in (3). Recently  Chen et al. [2018] proposed a variance reduced
sEM (sEM-VR) method which is inspired by the SVRG algorithm popular in stochastic convex op-
timization [Johnson and Zhang  2013]. The applications of the above stochastic EM methods are
numerous  especially with the iEM and sEM methods; e.g.  [Thiesson et al.  2001] for inference with
missing data  [Ng and McLachlan  2003] for mixture models and unsupervised clustering  [Hinton
et al.  2006] for inference of deep belief networks  [Hofmann  1999] for probabilistic latent semantic
analysis  [Wainwright et al.  2008  Blei et al.  2017] for variational inference of graphical models
and [Ablin et al.  2018] for Independent Component Analysis.
This paper focuses on the theoretical aspect of stochastic EM methods by establishing novel non-
asymptotic and global convergence rates for them. Our contributions are as follows.

• We offer two complementary views for the global convergence of EM methods – one fo-
cuses on the parameter space  and one on the sufﬁcient statistics space. On one hand  the
EM method can be studied as an majorization-minimization (MM) method in the parameter
space. On the other hand  the EM method can be studied as a scaled-gradient method in
the sufﬁcient statistics space.
• Based on the two views described  we derive non-asymptotic convergence rate for stochas-
tic EM methods. First  we show that the iEM method [Neal and Hinton  1998] is a special
instance of the MISO framework [Mairal  2015]  and takes O(n/) iterations to ﬁnd an -
stationary point to the ML estimation problem. Second  the sEM-VR method [Chen et al. 
2018] is an instance of variance reduced stochastic scaled-gradient method  which takes
O(n2/3/) iterations to ﬁnd to an -stationary point.

• Lastly  we develop a Fast Incremental EM (ﬁEM) method based on the SAGA algorithm
[Defazio et al.  2014  Reddi et al.  2016b] for stochastic optimization. We show that the new
method is again a scaled-gradient method with the same iteration complexity as sEM-VR.
This new method offers trade-off between storage cost and computation complexity.

Importantly  our results capitalizes on the efﬁciency of stochastic EM methods applied on large
datasets  and we support the above ﬁndings using numerical experiments.

Prior Work Since the empirical risk minimization problem (1) is typically non-convex  most prior
work studying the convergence of EM methods considered either the asymptotic and/or local behav-
iors. For the classical study  the global convergence to a stationary point (either a local minimum or a
saddle point) of the bEM method has been established by Wu et al. [1983] (by making the arguments

2

developed in Dempster et al. [1977] rigorous). The global convergence is a direct consequence of
the EM method to be monotone. It is also known that in the neighborhood of a stationary point and
under regularity conditions  the local rate of convergence of the bEM is linear and is given by the
amount of missing information [McLachlan and Krishnan  2007  Chapters 3 and 4].
The convergence of the iEM method was ﬁrst tackled by Gunawardana and Byrne [2005] exploiting
the interpretation of the method as an alternating minimization procedure under the information
geometric framework developed in [Csisz´ar and Tusn´ady  1984]. Although the EM algorithm is
presented as an alternation between the E-step and M-step  it is also possible to take a variational
perspective on EM to view both steps as maximization steps. Nevertheless  Gunawardana and Byrne
[2005] assume that the latent variables take only a ﬁnite number of values and the order in which
the observations are processed remains the same from one pass to the other.
More recently  the local but non-asymptotic convergence of EM methods has been studied in several
works. These results typically require the initializations to be within a neighborhood of an isolated
stationary point and the (negated) log-likelihood function to be strongly convex locally. Such con-
ditions are either difﬁcult to verify in general or have been derived only for speciﬁc models; see for
example [Wang et al.  2015  Xu et al.  2016  Balakrishnan et al.  2017] and the references therein.
The local convergence of sEM-VR method has been studied in [Chen et al.  2018  Theorem 1] but
under a pathwise global stability condition. The authors’ work [Karimi et al.  2019] provided the ﬁrst
global non-asymptotic analysis of the online (stochastic) EM method [Capp´e and Moulines  2009].
In comparison  the present work analyzes the variance reduced variants of EM method. Lastly  it
is worthwhile to mention that Zhu et al. [2017] analyzed a variance reduced gradient EM method
similar to [Balakrishnan et al.  2017].

2 Stochastic Optimization Techniques for EM methods
Let k ≥ 0 be the iteration number. The kth iteration of a generic stochastic EM method is composed
of two sub-steps — ﬁrstly 

sE-step : ˆs(k+1) = ˆs(k) − γk+1

which is a stochastic version of the E-step in (3). Note {γk}∞
S (k+1) is a proxy for s( ˆθ(k))  and s is deﬁned in (3). Secondly  the M-step is given by

(cid:0)ˆs(k) − S (k+1)(cid:1) 
(cid:8) R(θ) + ψ(θ) −(cid:10)ˆs(k+1) | φ(θ)(cid:11)(cid:9) 

(4)
k=1 ∈ [0  1] is a sequence of step sizes 

(5)

M-step: ˆθ(k+1) = θ(ˆs(k+1)) := arg min

θ∈Θ

(cid:90)

Z

which depends on the sufﬁcient statistics in the sE-step. The stochastic EM methods differ in
the way that S (k+1) is computed. Existing methods employ stochastic approximation or variance
reduction without the need to fully compute s( ˆθ(k)). To simplify notations  we deﬁne

s(k)
i

:= si( ˆθ(k)) =

S(zi  yi)p(zi|yi; ˆθ(k))µ(dzi)

and

s((cid:96)) := s( ˆθ((cid:96))) =

1
n

s((cid:96))
i

.

(6)

If S (k+1) = s(k) and γk+1 = 1  (4) reduces to the E-step in the classical bEM method. To formally

describe the stochastic EM methods  we let ik ∈(cid:74)1  n(cid:75) be a random index drawn at iteration k and
i = max{k(cid:48) : ik(cid:48) = i  k(cid:48) < k} be the iteration index such that i ∈(cid:74)1  n(cid:75) is last drawn prior to

τ k
iteration k. The proxy S (k+1) in (4) is drawn as:

n(cid:88)

i=1

(iEM [Neal and Hinton  1998])

(sEM [Capp´e and Moulines  2009])

(sEM-VR [Chen et al.  2018])

(cid:0)s(k)
S (k+1) = s((cid:96)(k)) +(cid:0)s(k)

S (k+1) = S (k) + 1
S (k+1) = s(k)

ik

ik

n

− s

(τ k
ik
ik

)

− s((cid:96)(k))

ik

ik

(cid:1)
(cid:1)

(7)

(8)

(9)

The stepsize is set to γk+1 = 1 for the iEM method; γk+1 = γ is constant for the sEM-VR method.
In the original version of the sEM method  the sequence of step γk+1 is a diminishing step size.
Moreover  for iEM we initialize with S (0) = s(0); for sEM-VR  we set an epoch size of m and
deﬁne (cid:96)(k) := m(cid:98)k/m(cid:99) as the ﬁrst iteration number in the epoch that iteration k is in.

3

ﬁEM Our analysis framework can handle a new  yet natural application of a popular variance
reduction technique to the EM method. The new method  called ﬁEM  is developed from the SAGA
method [Defazio et al.  2014] in a similar vein as in sEM-VR.
For iteration k ≥ 0  the ﬁEM method draws two indices independently and uniformly as ik  jk ∈
(cid:74)1  n(cid:75). In addition to τ k
j = {k(cid:48) : jk(cid:48) = j  k(cid:48) < k} to be
the iteration index where the sample j ∈ (cid:74)1  n(cid:75) is last drawn as jk prior to iteration k. With the

i which was deﬁned w.r.t. ik  we deﬁne tk

= s(0)  we use a slightly different update rule from SAGA inspired by [Reddi

initialization S (0)
et al.  2016b]  as described by the following recursive updates
= S (k)

(cid:1)  S (k+1)

+(cid:0)s(k)

S (k+1) = S (k)

− s

)

+ n−1(cid:0)s(k)

jk

− s

(tk
jk
jk

)

(tk
ik
ik

ik

(cid:1).

(10)

i

i=1 s(tk
i )

= n−1(cid:80)n

where we set a constant step size as γk+1 = γ.
In the above  the update of S (k+1) corresponds
to an unbiased estimate of s(k)  while the up-
date for S (k+1) maintains the structure that
S (k)
for any k ≥ 0. The
two updates of (10) are based on two differ-
ent and independent indices ik  jk that are ran-

domly drawn from(cid:74)n(cid:75). This is used for our fast

convergence analysis in Section 3.
We summarize the iEM  sEM-VR  sEM  ﬁEM
methods in Algorithm 1. The random termina-
tion number (11) is inspired by [Ghadimi and
Lan  2013] which enables one to show non-
asymptotic convergence to stationary point for
non-convex optimization. Due to their stochas-
tic nature  the per-iteration complexity for all
the stochastic EM methods are independent of
n  unlike the bEM method. They are thus ap-
plicable to large datasets with n (cid:29) 1.

2.1 Example: Gaussian Mixture Model

Algorithm 1 Stochastic EM methods.
1: Input: initializations ˆθ(0) ← 0  ˆs(0) ← s(0) 
2: Set the terminating iteration number  K ∈

Kmax ← max. iteration number.
{0  . . .   Kmax − 1}  as a discrete r.v. with:

P (K = k) =

.

(11)

γk(cid:80)Kmax−1

(cid:96)=0

γ(cid:96)

3: for k = 0  1  2  . . .   K do
4:

Draw index ik ∈ (cid:74)1  n(cid:75) uniformly (and
jk ∈(cid:74)1  n(cid:75) for ﬁEM).

Compute the surrogate sufﬁcient statistics
S (k+1) using (8) or (7) or (9) or (10).
Compute ˆs(k+1) via the sE-step (4).
Compute ˆθ(k+1) via the M-step (5).

5:

6:
7:
8: end for
9: Return: ˆθ(K).

We discuss an example of learning a Gaussian Mixture Model (GMM) from a set of n observations
{yi}n
i=1. We focus on a simpliﬁed setting where there are M components of unit variance and
unknown means  the GMM is parameterized by θ = ({ωm}M−1
m=1) ∈ Θ = ∆M × RM  
where ∆M ⊆ RM−1 is the reduced M-dimensional probability simplex [see (29)]. We use the
m − log Dir(ω; M  ) where δ > 0 and Dir(·; M  ) is the M
penalization R(θ) = δ
2
dimensional symmetric Dirichlet distribution with concentration parameter  > 0. Furthermore  we

m=1  {µm}M
use zi ∈(cid:74)M(cid:75) as the latent label. The complete data log-likelihood is given by

(cid:80)M
M(cid:88)
1{m=zi}(cid:2)log(ωm) − µ2

1{m=zi}µmyi + constant 

(12)

m/2(cid:3) +

log f (zi  yi; θ) =

M(cid:88)

m=1 µ2

m=1

m=1

where 1{m=zi} = 1 if m = zi; otherwise 1{m=zi} = 0. The above can be rewritten in the
same form as (2)  particularly with S(yi  zi) ≡ (s(1)
) and φ(θ) ≡
(φ(1)

i M−1  s(3)

i M−1  s(2)

i 1   ...  s(1)

i 1   ...  s(2)

M−1(θ)  φ(3)(θ)) such that

1 (θ)  ...  φ(1)

M−1(θ)  φ(2)

i

m/2} − {log(1 −(cid:80)M−1

j=1 ωj) − µ2

M /2} 

s(1)
i m = 1{zi=m}  φ(1)
s(2)
i m = 1{zi=m}yi  φ(2)

and ψ(θ) = −{log(1 −(cid:80)M−1

1 (θ)  ...  φ(2)
m (θ) = {log(ωm) − µ2
m (θ) = µm 
m=1 ωm) − µ2

s(3)
i = yi  φ(3)(θ) = µM  

(13)

M /2σ2}. To evaluate the sE-step  the conditional expec-
tation required by (6) can be computed in closed form  as they depend on E ˆθ(k)[1{zi=m}|y = yi]
and E ˆθ(k)[yi1{zi=m}|y = yi]. Moreover  the M-step (5) solves a strongly convex problem and can

4

be computed in closed form. Given a sufﬁcient statistics s ≡ (s(1)  s(2)  s(3))  the solution to (5) is:



(1 + M )−1(cid:0)s(1)
(cid:0)(s(1)
(cid:0)1 −(cid:80)M−1

1 + δ)−1s(2)
m=1 s(1)

M−1 + (cid:1)(cid:62)
m + δ(cid:1)−1(cid:0)s(3) −(cid:80)M−1

M−1 + δ)−1s(2)
M−1
m=1 s(2)

1 +   . . .   s(1)

1   . . .   (s(1)

(cid:1)(cid:62)
(cid:1)

m

 .

θ(s) =

(14)

The next section presents the main results of this paper for the convergence of stochastic EM meth-
ods. We shall use the above example on GMM to illustrate the required assumptions.

3 Global Convergence of Stochastic EM Methods

We establish non-asymptotic rates for the global convergence of the stochastic EM methods. We
show that the iEM method is an instance of the incremental MM method; while sEM-VR  ﬁEM
methods are instances of variance reduced stochastic scaled gradient methods. As we will see  the
latter interpretation allows us to establish fast convergence rates of sEM-VR and ﬁEM methods.
Detailed proofs for the theoretical results in this section are relegated to the appendix.
First  we list a few assumptions which will enable the convergence analysis performed later in this
section. Deﬁne:

i=1 αisi : si ∈ conv {S(z  yi) : z ∈ Z}   αi ∈ [−1  1]  i ∈(cid:74)1  n(cid:75)}  

(15)
where conv{A} denotes the closed convex hull of the set A. From (15)  we observe that the iEM 
sEM-VR  and ﬁEM methods generate ˆs(k) ∈ S for any k ≥ 0. Consider:
H1. The sets Z  S are compact. There exists constants CS  CZ such that:

S := {(cid:80)n
CS := maxs s(cid:48)∈S (cid:107)s − s(cid:48)(cid:107) < ∞  CZ := maxi∈(cid:74)1 n(cid:75)(cid:82)

Z |S(z  yi)|µ(dz) < ∞.

(16)

φ(θ(cid:48))(cid:107) ≤ Cφ.

H1 depends on the latent data model used and can be satisﬁed by several practical models. For
instance  the GMM in Section 2.1 satisﬁes (16) as the sufﬁcient statistics are composed of indicator
κ(θ(cid:48)) the
functions and observations. Other examples can also be found in Section 4. Denote by Jθ
Jacobian of the function κ : θ (cid:55)→ κ(θ) at θ(cid:48) ∈ Θ. Consider:
H2. The function φ is smooth and bounded on int(Θ)  i.e.  the interior of Θ. For all θ  θ(cid:48) ∈ int(Θ)2 
(cid:107) Jθ

φ(θ) − Jθ
we have(cid:12)(cid:12)p(z|yi; θ) − p(z|yi; θ(cid:48))(cid:12)(cid:12) ≤ Lp (cid:107)θ − θ(cid:48)(cid:107).
H3. The conditional distribution is smooth on int(Θ). For any i ∈(cid:74)1  n(cid:75)  z ∈ Z  θ  θ(cid:48) ∈ int(Θ)2 
H4. For any s ∈ S  the function θ (cid:55)→ L(s  θ) := R(θ) + ψ(θ) −(cid:10)s| φ(θ)(cid:11) admits a unique global

φ(θ(cid:48))(cid:107) ≤ Lφ (cid:107)θ − θ(cid:48)(cid:107) and (cid:107) Jθ

φ(θ(s)) is full rank and θ(s) is Lθ-Lipschitz.

minimum θ(s) ∈ int(Θ). In addition  Jθ
Under H1  the assumptions H2 and H3 are standard for the curved exponential family distribu-
tion and the conditional probability distributions  respectively; H4 can be enforced by designing a
strongly convex regularization function R(θ) tailor made for Θ. For instance  the penalization for
GMM in Section 2.1 ensures θ(k) is unique and lies in int(∆M )× RM   which can further imply the
second statement in H4. We remark that for H3  it is possible to deﬁne the Lipschitz constant Lp in-
dependently for each data yi to yield a reﬁned characterization. We did not pursue such assumption
to keep the notations simple.
Denote by Hθ

L(s  θ) the Hessian w.r.t to θ for a given value of s of the function θ (cid:55)→ L(s  θ) =

R(θ) + ψ(θ) −(cid:10)s| φ(θ)(cid:11)  and deﬁne

φ(θ(s))

L(s  θ(s))

B(s) := Jθ

(17)
H5. It holds that υmax := sups∈S (cid:107) B(s)(cid:107) < ∞ and 0 < υmin := inf s∈S λmin(B(s)). There exists
a constant LB such that for all s  s(cid:48) ∈ S2  we have (cid:107) B(s) − B(s(cid:48))(cid:107) ≤ LB (cid:107)s − s(cid:48)(cid:107).
Again  H5 is satisﬁed by practical models. For GMM in Section 2.1  it can be veriﬁed by deriving
the closed form expression for B(s) and using H1; also see the other example in Section 4. The
derivation is  however  technical and will be relegated to the supplementary material.

φ(θ(s))(cid:62).
Jθ

(cid:16)

Hθ

(cid:17)−1

5

Under H1  we have (cid:107)ˆs(k)(cid:107) < ∞ since S is compact. On the other hand  under H4  the EM methods
generate ˆθ(k) ∈ int(Θ) for any k ≥ 0. Overall  these assumptions ensure that the EM methods
operate in a ‘nice’ set throughout the optimization process.

3.1

Incremental EM method

We show that the iEM method is a special case of the MISO method [Mairal  2015] utilizing the
majorization minimization (MM) technique. The latter is a common technique for handling non-
convex optimization. We begin by deﬁning a surrogate function that majorizes Li:

Qi(θ; θ(cid:48)) := −

{log f (zi  yi; θ) − log p(zi|yi; θ(cid:48))} p(zi|yi; θ(cid:48))µ(dzi) .

(18)

(cid:90)

Z

The second term inside the bracket is a constant that does not depend on the ﬁrst argument θ. Since
f (zi  yi; θ) = p(zi|yi; θ)g(yi; θ)  for all θ(cid:48) ∈ Θ  we get Qi(θ(cid:48); θ(cid:48)) = − log g(yi; θ(cid:48)) = Li(θ(cid:48)). For
all θ  θ(cid:48) ∈ Θ  applying the Jensen inequality shows

Qi(θ  θ(cid:48)) − Li(θ) =

log

p(zi|yi; θ(cid:48))µ(dzi) ≥ 0

(19)

p(·|yi; θ) and p(·|yi; θ(cid:48)). Hence  for all i ∈ (cid:74)1  n(cid:75)  Qi(θ; θ(cid:48)) is a majorizing surrogate to Li(θ) 

which is the Kullback-Leibler divergence between the conditional distribution of the latent data
i.e.  it satisﬁes for all θ  θ(cid:48) ∈ Θ  Qi(θ; θ(cid:48)) ≥ Li(θ) with equality when θ = θ(cid:48). For the special case
of curved exponential family distribution  the M-step of the iEM method is expressed as

(cid:90)

p(zi|yi; θ(cid:48))
p(zi|yi; θ)

ˆθ(k+1) ∈ arg minθ∈Θ
= arg minθ∈Θ

(cid:8) R(θ) + n−1(cid:80)n
(cid:110)
R(θ) + ψ(θ) −(cid:10)n−1(cid:80)n

i=1 Qi(θ; ˆθ(τ (k+1)
i=1 s(τ k+1

i

i

))(cid:9)
(cid:111)
| φ(θ)(cid:11))

)

.

i

(20)

The iEM method can be interpreted through the MM technique — in the M-step  ˆθ(k+1) minimizes
an upper bound of L(θ)  while the sE-step updates the surrogate function in (20) which tightens
the upper bound. Importantly  the error between the surrogate function and Li is a smooth function:
Lemma 1. Assume H1  H2  H3  H4. Let ei(θ; θ(cid:48)) := Qi(θ; θ(cid:48))−Li(θ). For any θ  ¯θ  θ(cid:48) ∈ Θ3  we
have (cid:107)∇ei(θ; θ(cid:48)) − ∇ei( ¯θ; θ(cid:48))(cid:107) ≤ Le (cid:107)θ − ¯θ(cid:107)  where Le := CφCZ Lp +CS Lφ.
For non-convex optimization such as (1)  it has been shown [Mairal  2015  Proposition 3.1] that
the incremental MM method converges asymptotically to a stationary solution of a problem. We
strengthen their result by establishing a non-asymptotic rate  which is new to the literature.
Theorem 1. Consider the iEM algorithm  i.e.  Algorithm 1 with (7). Assume H1  H2  H3  H4. For
any Kmax ≥ 1  it holds that

E[(cid:107)∇L( ˆθ(K))(cid:107)2] ≤ n

2 Le
Kmax

where Le is deﬁned in Lemma 1 and K is a uniform random variable on(cid:74)0  Kmax − 1(cid:75) [cf. (11)]

independent of the {ik}Kmax
k=0 .
We remark that under suitable assumptions  our analysis in Theorem 1 also extends to several non-
exponential family distribution models.

(21)

E(cid:2)L( ˆθ(0)) − L( ˆθ(Kmax))(cid:3) 

3.2 Stochastic EM as Scaled Gradient Methods

We interpret the sEM-VR and ﬁEMmethods as scaled gradient methods on the sufﬁcient statistics
ˆs  tackling a non-convex optimization problem. The beneﬁt of doing so is that we are able to demon-
strate a faster convergence rate for these methods through motivating them as variance reduced
optimization methods. The latter is shown to be more effective when handling large datasets [Reddi
et al.  2016b a  Allen-Zhu and Hazan  2016] than traditional stochastic/deterministic optimization
methods. To set our stage  we consider the minimization problem:

min
s∈S

V (s) := L(θ(s)) = R(θ(s)) +

1
n

Li(θ(s)) 

(22)

n(cid:88)

i=1

where θ(s) is the unique map deﬁned in the M-step (5). We ﬁrst show that the stationary points of
(22) has a one-to-one correspondence with the stationary points of (1):

6

Lemma 2. For any s ∈ S  it holds that

(23)

∇sV (s) = Js

θ(s)(cid:62)∇θL(θ(s)).

Assume H4. If s(cid:63) ∈ {s ∈ S : ∇sV (s) = 0}  then θ(s(cid:63)) ∈(cid:8)θ ∈ Θ : ∇θL(θ) = 0(cid:9). Conversely 
if θ∗ ∈(cid:8)θ ∈ Θ : ∇θL(θ) = 0(cid:9)  then s∗ = s(θ∗) ∈ {s ∈ S : ∇sV (s) = 0}.
(cid:3) = ˆs(k) − s(k) = ˆs(k) − s(θ(ˆs(k))) 

The next lemmas show that the update direction  ˆs(k) − S (k+1)  in the sE-step (4) of sEM-VR and
ﬁEM methods is a scaled gradient of V (s). We ﬁrst observe the following conditional expectation:
(24)

where Fk is the σ-algebra generated by {i0  i1  . . .   ik} (or {i0  j0  . . .   ik  jk} for ﬁEM).
The difference vector s − s(θ(s)) and the gradient vector ∇sV (s) are correlated  as we observe:
Lemma 3. Assume H4 H5. For all s ∈ S 

E(cid:2)ˆs(k) − S (k+1)|Fk
(cid:10)∇V (s)| s − s(θ(s))(cid:11) ≥(cid:13)(cid:13)s − s(θ(s))(cid:13)(cid:13)2 ≥ υ−2

max(cid:107)∇V (s)(cid:107)2 

υ−1

min

(25)

Combined with (24)  the above lemma shows that the update direction in the sE-step (4) of sEM-VR
and ﬁEM methods is a stochastic scaled gradient where ˆs(k) is updated with a stochastic direction
whose mean is correlated with ∇V (s).
Furthermore  the expectation step’s operator and the objective function in (22) are smooth functions:

Lemma 4. Assume H1  H3  H4  H5. For all s  s(cid:48) ∈ S and i ∈(cid:74)1  n(cid:75)  we have

(cid:107)si(θ(s)) − si(θ(s(cid:48)))(cid:107) ≤ Ls (cid:107)s − s(cid:48)(cid:107)  (cid:107)∇V (s) − ∇V (s(cid:48))(cid:107) ≤ LV (cid:107)s − s(cid:48)(cid:107) 

(26)

where Ls := CZ Lp Lθ and LV := υmax
The following theorem establishes the (fast) non-asymptotic convergence rates of sEM-VR and ﬁEM
methods  which are similar to [Reddi et al.  2016b a  Allen-Zhu and Hazan  2016]:
Theorem 2. Assume H1  H3  H4  H5. Denote Lv = max{LV   Ls} with the constants in Lemma 4.
• Consider the sEM-VR method  i.e.  Algorithm 1 with (9). There exists a universal constant
Lvn2/3 and the epoch

µ ∈ (0  1) (independent of n) such that if we set the step size as γ = µυmin
length as m =

min+µ   then for any Kmax ≥ 1 that is a multiple of m  it holds that

n
2µ2υ2

(cid:0)1 + Ls

(cid:1) + LB CS.

E[(cid:107)∇V (ˆs(K))(cid:107)2] ≤ n

2
3

2Lv

µKmax

υ2
max
υ2
min

E[V (ˆs(0)) − V (ˆs(Kmax))].

(27)

• Consider the ﬁEM method  i.e.  Algorithm 1 with (10). Set γ = υmin

max{6  1 + 4υmin}. For any Kmax ≥ 1  it holds that

αLvn2/3 such that α =

E(cid:2)V (ˆs(0)) − V (ˆs(Kmax))(cid:3).

(28)

E[(cid:107)∇V (ˆs(K))(cid:107)2] ≤ n

2
3

α2Lv
Kmax

υ2
max
υ2
min

We recall that K in the above is a uniform and independent r.v. chosen from(cid:74)Kmax − 1(cid:75) [cf. (11)].

In the supplementary materials  we also provide a local convergence analysis for ﬁEM method which
shows that the latter can achieve linear rate of convergence locally under a similar set of assumptions
used in [Chen et al.  2018] for sEM-VR method.
Comparing iEM  sEM-VR  and ﬁEM Note that by (23) in Lemma 2  if (cid:107)∇sV (ˆs)(cid:107)2 ≤   then
(cid:107)∇θL(θ(ˆs))(cid:107)2 = O()  and vice versa  where the hidden constant is independent of n. In other
words  the rates for iEM  sEM-VR  ﬁEM methods in Theorem 1 and 2 are comparable.
Importantly  the theorems show an intriguing comparison – to attain an -stationary point with
(cid:107)∇θL(θ(ˆs))(cid:107)2 ≤  or (cid:107)∇sV (ˆs)(cid:107)2 ≤   the iEM method requires O(n/) iterations (in expec-
tation) while the sEM-VR  ﬁEM methods require only O(n 2
3 /) iterations (in expectation). This
comparison can be surprising since the iEM method is a monotone method as it guarantees decrease
in the objective value; while the sEM-VR  ﬁEM methods are non-monotone. Nevertheless  it aligns
with the recent analysis on stochastic variance reduction methods on non-convex problems. In the
next section  we conﬁrm the theory by observing a similar behavior numerically.

7

4 Numerical Examples

4.1 Gaussian Mixture Models
As described in Section 2.1  our goal is to ﬁt a GMM model to a set of n observations {yi}n
i=1
whose distribution is modeled as a Gaussian mixture of M components  each with a unit variance.
with ω = {ωm}M−1
µ = {µm}M

Let zi ∈(cid:74)M(cid:75) be the latent labels  the complete log-likelihood is given in (12)  where θ := (ω  µ)
Θ = {ωm  m = 1  ...  M − 1 : ωm ≥ 0  (cid:80)M−1

m=1 are the mixing weights with the convention ωM = 1 −(cid:80)M−1

m=1 are the means. The constraint set on θ is given by

m=1 ωm ≤ 1} × {µm ∈ R  m = 1  ...  M}.

m=1 ωm and

(29)

In the following experiments of synthetic data  we generate samples from a GMM model with M =
2 components with two mixtures with means µ1 = −µ2 = 0.5  see Appendix G.1 for details of the
implementation and satisfaction of model assumptions for GMM inference. We aim at verifying the
theoretical results in Theorem 1 and 2 of the dependence on sample size n.

Fixed sample size We use n = 104 synthetic samples and run the bEM method until convergence
(to double precision) to obtain the ML estimate µ(cid:63). We compare the bEM  sEM  iEM  sEM-VR and
ﬁEM methods in terms of their precision measured by |µ − µ(cid:63)|2. We set the stepsize of the sEM as
γk = 3/(k + 10)  and the stepsizes of the sEM-VR and the ﬁEM to a constant stepsize proportional
to 1/n2/3 and equal to γ = 0.003. The left plot of Figure 1 shows the convergence of the precision
|µ − µ∗|2 for the different methods against the epoch(s) elapsed (one epoch equals n iterations).
We observe that the sEM-VR and ﬁEM methods outperform the other methods  supporting our
analytical results.
Varying sample size We compare the number of iterations required to reach a precision of 10−3 as
a function of the sample size from n = 103 to n = 105. We average over 5 independent runs for each
method using the same stepsizes as in the ﬁnite sample size case above. The right plot of Figure
1 conﬁrms that our ﬁndings in Theorem 1 and 2 are sharp. It requires O(n/) (resp. O(n 2
3 /))
iterations to ﬁnd a -stationary point for the iEM (resp. sEM-VR and ﬁEM) method.

Figure 1: Performance of stochastic EM methods for ﬁtting a GMM. (Left) Precision (|µ(k) − µ(cid:63)|2)
as a function of the epoch elapsed. (Right) Number of iterations to reach a precision of 10−3.

4.2 Probabilistic Latent Semantic Analysis

The second example considers probabilistic Latent Semantic Analysis (pLSA) whose aim is to clas-

i=1 where each token is a
pair of document and word yi = (y(d)
appears in document y(d)
.
The goal of pLSA is to classify the documents into K topics  which is modeled as a latent variable

sify documents into a number of topics. We are given a collection of documents(cid:74)D(cid:75) with terms
from a vocabulary(cid:74)V(cid:75). The data is summarized by a list of tokens {yi}n
zi ∈(cid:74)K(cid:75) associated with each token [Hofmann  1999].
d k }(cid:74)K−1(cid:75)×(cid:74)D(cid:75) and θ(w|t) = {θ(w|t)
as — for each d ∈(cid:74)D(cid:75)  θ(t|d)

To apply stochastic EM methods for pLSA  we deﬁne θ := (θ(t|d)  θ(w|t)) as the parameter variable 
k v }(cid:74)K(cid:75)×(cid:74)V −1(cid:75). The constraint set Θ is given
where θ(t|d) = {θ(t|d)
· k ∈ ∆V   where ∆K  ∆V

d · ∈ ∆K and for each k ∈(cid:74)K(cid:75)  we have θ(w|t)

) which indicates that y(w)

i

i

  y(w)

i

i

8

are the (reduced dimension) K  V -dimensional probability simplex; see (108) in the supplementary
material for the precise deﬁnition. Furthermore  denote θ(t|d)
and θ(w|t)
an additive constant term):

d k for each d ∈(cid:74)D(cid:75) 
for each k ∈(cid:74)K(cid:75)  the complete log likelihood for (yi  zi) is (up to

k=1 θ(t|d)

k V = 1 −(cid:80)V −1
(cid:96)=1 θ(w|t)
D(cid:88)
K(cid:88)

k (cid:96)

log f (zi  yi; θ) =

d K = 1−(cid:80)K−1
V(cid:88)
K(cid:88)

log(θ(w|t)

log(θ(t|d)

d k )1{k d}(zi  y(d)

i

) +

k v )1{k v}(zi  y(w)

i

). (30)

k=1

d=1

k=1

v=1

The penalization function is designed as

R(θ(t|d)  θ(w|t)) = − log Dir(θ(t|d); K  α(cid:48)) − log Dir(θ(w|t); V  β(cid:48)) 

(31)
such that we ensure θ(s) ∈ int(Θ). We can apply the stochastic EM methods described in Section 2
on the pLSA problem. The implementation details are provided in Appendix G.2  therein we also
verify the model assumptions required by our convergence analysis for pLSA.

Experiment We compare the stochastic EM methods on two FAO (UN Food and Agricul-
ture Organization) datasets [Medelyan  2009]. The ﬁrst (resp. second) dataset consists of 103
(resp. 10.5 × 103) documents and a vocabulary of size 300. The number of topics is set to K = 10
and the stepsizes for the ﬁEM and sEM-VR are set to γ = 1/n2/3 while the stepsize for the sEM
is set to γk = 1/(k + 10). Figure 1 shows the evidence lower bound (ELBO) as a function of the
number of epochs for the datasets. Again  the result shows that ﬁEM and sEM-VR methods achieve
faster convergence than the competing EM methods  afﬁrming our theoretical ﬁndings.

Figure 2: ELBO of the stochastic EM methods on FAO datasets as a function of number of epochs
elapsed. (Left) small dataset with 103 documents. (Right) large dataset with 10.5 × 103 documents.

5 Conclusion

This paper studies the global convergence for stochastic EM methods. Particularly  we focus on the
inference of latent variable model with exponential family distribution and analyze the convergence
of several stochastic EM methods. Our convergence results are global and non-asymptotic  and
we offer two complimentary views on the existing stochastic EM methods — one interprets iEM
method as an incremental MM method  and one interprets sEM-VR and ﬁEM methods as scaled
gradient methods. The analysis shows that the sEM-VR and ﬁEM methods converge faster than the
iEM method  and the result is conﬁrmed via numerical experiments.

Acknowledgement

BK and HTW contributed equally to this work. HTW’s work is supported by the CUHK Direct
Grant #4055113.

9

References
P. Ablin  A. Gramfort  J.-F. Cardoso  and F. Bach. EM algorithms for ICA. arXiv preprint

arXiv:1805.10054  2018.

Z. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In International

Conference on Machine Learning  pages 699–707  2016.

S. Balakrishnan  M. J. Wainwright  and B. Yu. Statistical guarantees for the EM algorithm: From
population to sample-based analysis. Ann. Statist.  45(1):77–120  02 2017. doi: 10.1214/
16-AOS1435.

D. M. Blei  A. Kucukelbir  and J. D. McAuliffe. Variational Inference: A Review for Statisticians.
Journal of the American statistical Association  112(518):859–877  JUN 2017. ISSN 0162-1459.
doi: {10.1080/01621459.2017.1285773}.

O. Capp´e and E. Moulines. On-line expectation–maximization algorithm for latent data models.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  71(3):593–613  2009.

J. Chen  J. Zhu  Y. W. Teh  and T. Zhang. Stochastic expectation maximization with variance reduc-

tion. In Advances in Neural Information Processing Systems  pages 7978–7988  2018.

I. Csisz´ar and G. Tusn´ady. Information geometry and alternating minimization procedures. Statist.
Decisions  suppl. 1:205–237  1984. ISSN 0721-2631. Recent results in estimation theory and
related topics.

A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with support
In Advances in neural information processing

for non-strongly convex composite objectives.
systems  pages 1646–1654  2014.

A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the royal statistical society. Series B (methodological)  pages 1–38 
1977.

S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic pro-

gramming. SIAM Journal on Optimization  23(4):2341–2368  2013.

A. Gunawardana and W. Byrne. Convergence theorems for generalized alternating minimization

procedures. Journal of Machine Learning Research  6:2049–2073  2005.

G. E. Hinton  S. Osindero  and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural

computation  18(7):1527–1554  2006.

T. Hofmann. Probabilistic latent semantic indexing.

In Proceedings of the 22Nd Annual In-
ternational ACM SIGIR Conference on Research and Development in Information Retrieval 
SIGIR ’99  pages 50–57  New York  NY  USA  1999. ACM.
doi:
10.1145/312624.312649.

ISBN 1-58113-096-1.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-

tion. In Advances in neural information processing systems  pages 315–323  2013.

B. Karimi  B. Miasojedow  E. Moulines  and H.-T. Wai. Non-asymptotic analysis of biased stochas-

tic approximation schemes. In Conference on Learning Theory  2019.

J. Mairal. Incremental majorization-minimization optimization with application to large-scale ma-

chine learning. SIAM Journal on Optimization  25(2):829–855  2015.

G. McLachlan and T. Krishnan. The EM algorithm and extensions  volume 382. John Wiley & Sons 

2007.

10

O. Medelyan. Human-competitive automatic topic indexing. PhD thesis  The University of Waikato 

2009.

R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental  sparse  and

other variants. In Learning in graphical models  pages 355–368. Springer  1998.

S. Ng and G. McLachlan. On the choice of the number of blocks with the incremental EM algorithm
for the ﬁtting of normal mixtures. Statistics and Computing  13(1):45–55  FEB 2003. ISSN 0960-
3174. doi: {10.1023/A:1021987710829}.

S. J. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for nonconvex

optimization. In International conference on machine learning  pages 314–323  2016a.

S. J. Reddi  S. Sra  B. P´oczos  and A. Smola. Fast incremental method for nonconvex optimization.

arXiv preprint arXiv:1603.06159  2016b.

B. Thiesson  C. Meek  and D. Heckerman. Accelerating EM for large databases. Machine Learning 

45(3):279–299  2001. ISSN 0885-6125. doi: {10.1023/A:1017986506241}.

M. J. Wainwright  M. I. Jordan  et al. Graphical models  exponential families  and variational infer-

ence. Foundations and Trends R(cid:13) in Machine Learning  1(1–2):1–305  2008.

Z. Wang  Q. Gu  Y. Ning  and H. Liu. High dimensional em algorithm: Statistical optimization
and asymptotic normality. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Gar-
nett  editors  Advances in Neural Information Processing Systems 28  pages 2521–2529. Curran
Associates  Inc.  2015.

C. J. Wu et al. On the convergence properties of the EM algorithm. The Annals of statistics  11(1):

95–103  1983.

J. Xu  D. J. Hsu  and A. Maleki. Global analysis of expectation maximization for mixtures of two
gaussians. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances
in Neural Information Processing Systems 29  pages 2676–2684. Curran Associates  Inc.  2016.

R. Zhu  L. Wang  C. Zhai  and Q. Gu. High-dimensional variance-reduced stochastic gradient
In Proceedings of the 34th International Conference on

expectation-maximization algorithm.
Machine Learning-Volume 70  pages 4180–4188. JMLR. org  2017.

11

,Ilya Sutskever
Oriol Vinyals
Quoc Le
Belhal Karimi
Hoi-To Wai
Eric Moulines
Marc Lavielle