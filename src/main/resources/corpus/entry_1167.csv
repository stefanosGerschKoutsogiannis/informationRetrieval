2016,Convergence guarantees for kernel-based quadrature rules in misspecified settings,Kernel-based quadrature rules are becoming important in machine learning and statistics  as they achieve super-$¥sqrt{n}$ convergence rates in numerical integration  and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness  which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However  this assumption can be violated in practice (e.g.  when the integrand is a black box function)  and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS  i.e.   when the integrand is less smooth than assumed. Specifically  we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand  where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces.,Convergence guarantees for kernel-based quadrature

rules in misspeciﬁed settings

Motonobu Kanagawa(cid:3)  Bharath K Sriperumbudury  Kenji Fukumizu(cid:3)

(cid:3)The Institute of Statistical Mathematics  Tokyo 190-8562  Japan

yDepartment of Statistics  Pennsylvania State University  University Park  PA 16802  USA

kanagawa@ism.ac.jp  bks18@psu.edu  fukumizu@ism.ac.jp

Abstract

p
Kernel-based quadrature rules are becoming important in machine learning and
statistics  as they achieve super-
n convergence rates in numerical integration 
and thus provide alternatives to Monte Carlo integration in challenging settings
where integrands are expensive to evaluate or where integrands are high dimen-
sional. These rules are based on the assumption that the integrand has a certain
degree of smoothness  which is expressed as that the integrand belongs to a cer-
tain reproducing kernel Hilbert space (RKHS). However  this assumption can be
violated in practice (e.g.  when the integrand is a black box function)  and no gen-
eral theory has been established for the convergence of kernel quadratures in such
misspeciﬁed settings. Our contribution is in proving that kernel quadratures can
be consistent even when the integrand does not belong to the assumed RKHS 
i.e.  when the integrand is less smooth than assumed. Speciﬁcally  we derive con-
vergence rates that depend on the (unknown) lesser smoothness of the integrand 
where the degree of smoothness is expressed via powers of RKHSs or via Sobolev
spaces.

1 Introduction
Numerical integration  or quadrature  is a fundamental task in the construction of various statistical
and machine learning algorithms. For instance  in Bayesian learning  numerical integration is gen-
erally required for the computation of marginal likelihood in model selection  and for the marginal-
ization of parameters in fully Bayesian prediction  etc [20]. It also offers ﬂexibility to probabilistic
modeling  since  e.g.  it enables us to use a prior that is not conjugate with a likelihood function.
Let P be a (known) probability distribution on a measurable space X and f be an integrand on X .
Suppose that the integral
f (x)dP (x) has no closed form solution. One standard form of numerical
integration is to approximate the integral as a weighted sum of function values f (X1); : : : ; f (Xn)
by appropriately choosing the points X1; : : : ; Xn 2 X and weights w1; : : : ; wn 2 R:

∫

∫

n∑

wif (Xi) (cid:25)

f (x)dP (x):

(1)

i=1

For example  the simplest Monte Carlo method generates the points X1; : : : ; Xn as an i.i.d. sample
from P and uses equal weights w1 = (cid:1)(cid:1)(cid:1) = wn = 1=n. Convergence rates of such Monte Carlo
(cid:0)1=2  which can be slow for practical purposes. For instance  in situations
methods are of the form n
where the evaluation of the integrand requires heavy computations  n should be small and Monte
Carlo would perform poorly; such situations typically appear in modern scientiﬁc and engineering
applications  and thus quadratures with faster convergence rates are desirable [18].
One way of achieving faster rates is to exploit one’s prior knowledge or assumption about the in-
tegrand (e.g. the degree of smoothness) in the construction of a weighted point set f(wi; Xi)gn
i=1.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Reproducing kernel Hilbert spaces (RKHS) have been successfully used for this purpose  with ex-
amples being Quasi Monte Carlo (QMC) methods based on RKHSs [13] and Bayesian quadratures
[19]; see e.g. [11  6] and references therein. We will refer to such methods as kernel-based quadra-
ture rules or simply kernel quadratures in this paper. A kernel quadrature assumes that the integrand
f belongs to an RKHS consisting of smooth functions (such as Sobolev spaces)  and constructs the
weighted points f(wi; Xi)gn
i=1 so that the worst case error in that RKHS is small. Then the error
(cid:0)b; b (cid:21) 1  which is much faster than the rates of Monte Carlo methods  will be
rate of the form n
guaranteed with b being a constant representing the degree of smoothness of the RKHS (e.g.  the
order of differentiability). Because of this nice property  kernel quadratures have been studied ex-
tensively in recent years [7  3  5  2  17] and have started to ﬁnd applications in machine learning and
statistics [23  14  12  6].
However  if the integrand f does not belong to the assumed RKHS (i.e. if f is less smooth than as-
sumed)  there is no known theoretical guarantee for fast convergence rate or even the consistency of
kernel quadratures. Such misspeciﬁcation is likely to happen if one does not have the full knowledge
of the integrand—such situations typically occur when the integrand is a black box function. As an
illustrative example  let us consider the problem of illumination integration in computer graphics
(see e.g. Sec. 5.2.4 of [6]). The task is to compute the total amount of light arriving at a camera in a
virtual environment. This is solved by numerical integration with integrand f (x) being the intensity
of light arriving at the camera from a direction x (angle). The value f (x) is only given by simulation
of the environment for each x  so f is a black box function. In such a situation  one’s assumption
on the integrand can be misspeciﬁed. Establishing convergence guarantees for such misspeciﬁed
settings has been recognized as an important open problem in the literature [6  Section 6].
Contributions. The main contribution of this paper is in providing general convergence guaran-
tees for kernel-based quadrature rules in misspeciﬁed settings. Speciﬁcally  we make the following
contributions:
(cid:15) In Section 4  we prove that consistency can be guaranteed even when the integrand f does
not belong to the assumed RKHS. Speciﬁcally  we derive a convergence rate of the form
(cid:0)(cid:18)b  where 0 < (cid:18) (cid:20) 1 is a constant characterizing the (relative) smoothness of the inte-
n
grand. In other words  the integration error decays at a speed depending on the (unknown)
smoothness of the integrand. This guarantee is applicable to kernel quadratures that employ
random points.
(cid:15) We apply this result to QMC methods called lattice rules (with randomized points) and
the quadrature rule by Bach [2]  for the setting where the RKHS is a Korobov space. We
show that even when the integrand is less smooth than assumed  the error rate becomes
the same as for the case when the (unknown) smoothness is known; namely  we show that
these methods are adaptive to the unknown smoothness of the integrand.
(cid:15) In Section 5  we provide guarantees for kernel quadratures with deterministic points  by
2 of order r 2 N (the order
nW r
2
2 of lesser smoothness. We
(cid:0)bs=r  where the ratio s=r determines the relative

focusing on speciﬁc cases where the RKHS is a Sobolev space W r
of differentiability). We prove that consistency can be guaranteed even if f 2 W s
where s (cid:20) r  i.e.  the integrand f belongs to a Sobolev space W s
derive a convergence rate of the form n
degree of smoothness.
(cid:15) As an important consequence  we show that if weighted points f(wi; Xi)gm
optimal rate in W r
achieve the optimal rate for the integrand f belonging to W s
the smoothness s of the integrand; one only needs to know its upper-bound s (cid:20) r.

2

2   then they also achieve the optimal rate in W s
2 .

i=1 achieve the
In other words  to
2   one does not need to know

This paper is organized as follows. In Section 2  we describe kernel-based quadrature rules  and
formally state the goal and setting of theoretical analysis in Section 3. We present our contributions
in Sections 4 and 5. Proofs are collected in the supplementary material.
Related work. Our work is close to [17] in spirit  which discusses situations where the true inte-
grand is smoother than assumed (which is complementary to ours) and proposes a control functional
approach to make kernel quadratures adaptive to the (unknown) greater smoothness. We also note
that there are certain quadratures which are adaptive to less smooth integrands [8  9  10]. On the
other hand  our aim here is to provide general theoretical guarantees that are applicable to a wide
class of kernel-based quadrature rules.

2

n

Notation. For x 2 Rd  let fxg 2 [0; 1]d be its fractional parts. For a probability distribution P on
a measurable space X   let L2(P ) be the Hilbert space of square-integrable functions with respect to
P . If P is the Lebesgue measure on X (cid:26) Rd  let L2(X ) := L2(P ) and further L2 := L2(Rd). Let
∑
0 (Rd) be the set of all functions on Rd that are continuously differentiable up to order s 2 N
0 := C s
C s
and vanish at inﬁnity. Given a function f and weighted points f(wi; Xi)gn
and Pnf :=
2 Kernel-based quadrature rules
Suppose that one has prior knowledge on certain properties of the integrand f (e.g. its order of differ-
entiability). A kernel quadrature exploits this knowledge by expressing it as that f belongs to a cer-
tain RKHS H that possesses those properties  and then constructing weighted points f(wi; Xi)gn
i=1
so that the error of integration is small for every function in the RKHS. More precisely  it pursues
the minimax strategy that aims to minimize the worst case error deﬁned by

i=1 wif (Xi) denote the integral and its numerical approximation  respectively.

i=1  P f :=

f (x)dP (x)

∫

en(P ;H) :=

jP f (cid:0) Pnfj :=

sup

sup

wif (Xi)

f2H:∥f∥H(cid:20)1

f2H:∥f∥H(cid:20)1

(2)
where ∥ (cid:1) ∥H denotes the norm of H. The use of RKHS is beneﬁcial (compared to other function
spaces)  because it results in an analytic expression of the worst case error (2) in terms of the re-
producing kernel. Namely  one can explicitly compute (2) in the construction of f(wi; Xi)gn
i=1 as a
criterion to be minimized. Below we describe this as well as examples of kernel quadratures.
2.1 The worst case error in RKHS
Let X be a measurable space and H be an RKHS of functions on X with k : X (cid:2) X ! R as the
bounded reproducing kernel. By the reproducing property  it is easy to verify that P f = ⟨f; mP⟩H
and Pnf = ⟨f; mPn

∫
⟩H for all f 2 H  where ⟨(cid:1);(cid:1)⟩H denotes the inner-product of H  and

i=1

mP :=

k((cid:1); x)dP (x) 2 H; mPn :=

wik((cid:1); Xi) 2 H:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∫

f (x)dP (x) (cid:0) n∑

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ;

n∑

i=1

sup

∥f∥H(cid:20)1

⟩H = ∥mP (cid:0) mPn

Therefore  the worst case error (2) can be written as the difference between mP and mPn:

jP f (cid:0) Pnfj = sup
∥f∥H(cid:20)1

√⟨f; f⟩H for f 2 H. From (3)  it is easy

⟨f; mP (cid:0) mPn
where ∥ (cid:1) ∥H denotes the norm of H deﬁned by ∥f∥H =
to see that for every f 2 H  the integration error jPnf (cid:0) P fj is bounded by the worst case error:
jPnf (cid:0) P fj (cid:20) ∥f∥H∥mP (cid:0) mPn
∫

We refer the reader to [21  11] for details of these derivations. Using the reproducing property of k 
the r.h.s. of (3) can be alternately written as:
en(P ;H) =

∥H = ∥f∥Hen(P ;H):
n∑

k(x; ~x)dP (x)dP (~x) (cid:0) 2

k(x; Xi)dP (x) +

n∑

n∑

wiwjk(Xi; Xj):

∥H;

(3)

wi

i=1

i=1

j=1

(4)
The integrals in (4) are known in closed form for many pairs of k and P ; see e.g. Table 1 of [6].
For instance  if P is the uniform distribution on X = [0; 1]d  and k is the Korobov kernel described
k(y; x)dP (x) = 1 for all y 2 X . To pursue the aforementioned minimax strategy 
below  then
one can explicitly use the formula (4) to minimize the worst case error (2). Often H is chosen as
an RKHS consisting of smooth functions  and the degree of smoothness is what a user speciﬁes; we
describe this in the example below.

vuut∫ ∫
∫

2.2 Examples of RKHSs: Korobov spaces
The setting X = [0; 1]d is standard in the literature on numerical integration; see e.g. [11]. In this
setting  Korobov spaces and Sobolev spaces have been widely used as RKHSs.1 We describe the
former here; for the latter  see Section 5.

1Korobov spaces are also known as periodic Sobolev spaces in the literature [4  p.318].

3

Korobov space on [0; 1]. The Korobov space W (cid:11)
kernel k(cid:11) is given by

Kor([0; 1]) of order (cid:11) 2 N is an RKHS whose

k(cid:11)(x; y) := 1 +

B2(cid:11)(fx (cid:0) yg);

(5)

((cid:0)1)(cid:11)(cid:0)1(2(cid:25))2(cid:11)

(2(cid:11))!

where B2(cid:11) denotes the 2(cid:11)-th Bernoulli polynomial. W (cid:11)
Kor([0; 1]) consists of periodic functions
on [0; 1] whose derivatives up to ((cid:11) (cid:0) 1)-th are absolutely continuous and the (cid:11)-th derivative be-
longs to L2([0; 1]) [16]. Therefore the order (cid:11) represents the degree of smoothness of functions in
Kor([0; 1]).
W (cid:11)
Korobov space on [0; 1]d. For d (cid:21) 2  the kernel of the Korobov space is given as the product of
one-dimensional kernels (5):
k(cid:11);d(x; y) := k(cid:11)(x1; y1)(cid:1)(cid:1)(cid:1) k(cid:11)(xd; yd); x := (x1; : : : ; xd)T ; y := (y1; : : : ; yd)T 2 [0; 1]d:
Kor([0; 1]d) on [0; 1]d is then the tensor product of one-dimensional
The induced Korobov space W (cid:11)
Korobov spaces: W (cid:11)
Kor([0; 1]). Therefore it consists of
functions having square-integrable mixed partial derivatives up to the order (cid:11) in each variable. This
means that by using the kernel (6) in the computation of (4)  one can make an assumption that the
integrand f has smoothness of degree (cid:11) in each variable. In other words  one can incorporate one’s
knowledge or belief on f into the construction of weighted points f(wi; Xi)g via the choice of (cid:11).

Kor([0; 1]) (cid:10) (cid:1)(cid:1)(cid:1) (cid:10) W (cid:11)

Kor([0; 1]d) := W (cid:11)

(6)

2.3 Examples of kernel-based quadrature rules

Kor([0; 1]d) can achieve the rate en(P; W (cid:11)

We brieﬂy describe examples of kernel-based quadrature rules.
Quasi Monte Carlo (QMC). These methods typically focus on the setting where X = [0; 1]d
with P being the uniform distribution on [0; 1]d  and employ equal weights wi = (cid:1)(cid:1)(cid:1) = wn = 1=n.
Popular examples are lattice rules and digital nets/sequences. Points X1; : : : ; Xn are selected in a
deterministic way so that the worst case error (4) is as small as possible. Then such deterministic
points are often randomized to obtain unbiased integral estimators  as we will explain in Section 4.2.
For a review of these methods  see [11].
For instance  lattice rules generate X1; : : : ; Xn in the following way (for simplicity assume n is
prime). Let z 2 f1; : : : ; n(cid:0)1gd be a generator vector. Then the points are deﬁned as Xi = fiz=ng 2
[0; 1]d for i = 1; : : : ; n. Here z is selected so that the resulting worst case error (2) becomes as small
as possible. The CBC (Component-By-Component) construction is a fast method that makes use of
the formula (4) to achieve this; see Section 5 of [11] and references therein. Lattice rules applied
(cid:0)(cid:11)+(cid:24)) for the
to the Korobov space W (cid:11)
worst case error with (cid:24) > 0 arbitrarily small [11  Theorem 5.12].
Bayesian quadratures. These methods are applicable to general X and P   and employ non-
uniform weights. Points X1; : : : ; Xn are selected either deterministically or randomly. Given the
points being ﬁxed  weights w1; : : : ; wn are obtained by minimizing (4)  which can be done by solv-
ing a linear system of size n. Such methods are called Bayesian quadratures  since the resulting
estimate Pnf in this case is exactly the posterior mean of the integral P f given “observations”
f(Xi; f (Xi))gn
i=1  with the prior on the integrand f being Gaussian Process with the covariance
kernel k. We refer to [6] for these methods.
For instance  the algorithm by Bach [2] proceeds as follows  for the case of H being a Korobov space
Kor([0; 1]d) and P being the uniform distribution on [0; 1]d: (i) Generate points X1; : : : ; Xn inde-
W (cid:11)
pendently from the uniform distribution on [0; 1]d; (ii) Compute weights w1; : : : ; wn by minimizing
(cid:20) 4=n. Bach [2] proved that this procedure gives the error rate
n
(4)  with the constraint
i=1 w2
(cid:0)(cid:11)+(cid:24)) for (cid:24) > 0 arbitrarily small.2
i
en(P; W (cid:11)
3 Setting and objective of theoretical analysis
We now formally state the setting and objective of our theoretical analysis in general form. Let P be
a known distribution and H be an RKHS. Our starting point is that weighted points f(wi; Xi)gn
i=1

Kor([0; 1]d) = O(n

Kor([0; 1]d) = O(n

∑

2Note that in [2]  the degree of smoothness is expressed in terms of s := (cid:11)d.

4

are already constructed for each n 2 N by some quadrature rule3  and that these provide consistent
approximation of P in terms of the worst case error:

en(P ;H) = ∥mP (cid:0) mPn

∥H = O(n

(cid:0)b)

(7)
where b > 0 is some constant. Here we do not specify the quadrature algorithm explicitly  to
establish results applicable to a wide class of kernel quadratures simultaneously.
Let f be an integrand that is not included in the RKHS: f =2 H. Namely  we consider a misspeciﬁed
setting. Our aim is to derive convergence rates for the integration error

(n ! 1);

jPnf (cid:0) P fj =

wif (Xi) (cid:0)

f (x)dP (x)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n∑

i=1

∫

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

based on the assumption (7). This will be done by assuming a certain regularity condition on f 
which expresses (unknown) lesser smoothness of f. For example  this is the case when the weighted
points are constructed by assuming the Korobov space of order (cid:11) 2 N  but the integrand f belongs
to the Korobov space of order (cid:12) < (cid:11): in this case  f is less smooth than assumed. As mentioned in
Section 1  such misspeciﬁcation is likely to happen if f is a black box function. But misspeciﬁcation
also occurs even when one has the full knowledge of f. As explained in Section 2.1  the kernel k
should be chosen so that the integrals in (4) allow analytic solutions w.r.t. P . Namely  the distribution
P determines an available class of kernels (e.g. Gaussian kernels for a Gaussian distribution)  and
therefore the RKHS of a kernel from this class may not contain the integrand of interest. This
situation can be seen in application to random Fourier features [23]  for example.
4 Analysis 1: General RKHS with random points
We ﬁrst focus on kernel quadratures with random points. To this end  we need to introduce certain
assumptions on (i) the construction of weighted points f(wi; Xi)gn
i=1 and on (ii) the smoothness of
the integrand f; we discuss them in Sections 4.1 and 4.2  respectively. In particular  we introduce
the notion of powers of RKHSs [22] in Section 4.2  which enables us to characterize the (relative)
smoothness of the integrand. We then state our main result in Section 4.3  and illustrate it with QMC
lattice rules (with randomization) and the Bayesian quadrature by Bach [2] in Korobov RKHSs.
4.1 Assumption on random points X1; : : : ; Xn
Assumption 1. There exists a probability distribution Q on X satisfying the following properties:
(i) P has a bounded density function w.r.t. Q; (ii) there is a constant D > 0 independent of n  such
that

(

[

E

(8)

1
n

g2(Xi)

8g 2 L2(Q);
where E[(cid:1)] denotes the expectation w.r.t. the joint distribution of X1; : : : ; Xn.
Assumption 1 is fairly general  as it does not specify any distribution of points X1; : : : ; Xn  but just
requires that the expectations over these points satisfy (8) for some distribution Q (also note that
it allows the points to be dependent). For instance  let us consider the case where X1; : : : ; Xn are
independently generated from a user-speciﬁed distribution Q; in this case  Q serves as a proposal
distribution. Then (8) holds for D = 1 with equality. Examples in this case include the Bayesian
quadratures by Bach [2] and Briol et al. [6] with random points.
Assumption 1 is also satisﬁed by QMC methods that apply randomization to deterministic points 
which is common in the literature [11  Sections 2.9 and 2.10]. Popular methods for randomization
are random shift and scrambling  both of which satisfy Assumption 1 for D = 1 with equality  where
Q( = P ) is the uniform distribution on X = [0; 1]d. This is because in general  randomization is
applied to make an integral estimator unbiased: E[ 1
[0;1]d f (x)dx [11  Section
2.9]. For instance  the random shift is done as follows. Let x1; : : : ; xn 2 [0; 1]d be deterministic
∫
points generated by a QMC method. Let ∆ be a random sample from the uniform distribution on
[0; 1]d. Then each Xi is given as Xi := fxi + ∆g 2 [0; 1]d. Therefore E[ 1
n
i=1 g2(Xi)] =
[0;1]d g2(x)dx =
)gn
3Note that here the weighted points should be written as f(w(n)
; X (n)
i=1  since they are constructed
depending on the number of points n. However  we just write as f(wi; Xi)gn
i=1 for notational simplicity.

g2(x)dQ(x) for all g 2 L2(Q)  so (8) holds for D = 1 with equality.

n
i=1 f (Xi)] =

∑

∑

∫

∫

i

i

])1=2 (cid:20) D∥g∥L2(Q);

n∑

i=1

n

n

5

4.2 Assumption on the integrand via powers of RKHSs

To state our assumption on the integrand f  we need to introduce powers of RKHSs [22  Section 4].
Let 0 < (cid:18) (cid:20) 1 be a constant. First  with the distribution Q in Assumption 1  we require that the
kernel satisﬁes

∫

k(x; x)dQ(x) < 1:
1∑

For example  this is always satisﬁed if the kernel is bounded. We also assume that the support of
Q is entire X and that k is continuous. These conditions imply Mercer’s theorem [22  Theorem 3.1
and Lemma 2.3]  which guarantees the following expansion of the kernel k:

k(x; y) =

(cid:22)iei(x)ei(y);

(9)
∑1
where (cid:22)1 (cid:21) (cid:22)2 (cid:21) (cid:1)(cid:1)(cid:1) > 0 and feig1
eig1
forms an orthonormal basis of H. Here the convergence of the series in (9) is pointwise. Assume
i (x) < 1 holds for all x 2 X . Then the (cid:18)-th power of the kernel k is a function
that
i e2
k(cid:18) : X (cid:2) X ! R deﬁned by

i=1 is an orthonormal series in L2(Q); in particular  f(cid:22)1=2

i=1 (cid:22)(cid:18)

i=1

i=1

i

x; y 2 X ;

1∑
{ 1∑

i=1

H(cid:18) =

ai(cid:22)(cid:18)=2

i

ei :

x; y 2 X :
}
i < 1
a2

:

1∑

k(cid:18)(x; y) :=

(cid:22)(cid:18)

i ei(x)ei(y);

(10)

This is again a reproducing kernel [22  Proposition 4.2]  and deﬁnes an RKHS called the (cid:18)-th power
of the RKHS H:

′

This is an intermediate space between L2(Q) and H  and the constant 0 < (cid:18) (cid:20) 1 determines how
close H(cid:18) is to H. For instance  if (cid:18) = 1 we have H(cid:18) = H  and H(cid:18) approaches L2(Q) as (cid:18) ! +0.
Indeed  H(cid:18) is nesting w.r.t. (cid:18):

i=1

i=1

H = H1 (cid:26) H(cid:18) (cid:26) H(cid:18)

′ (cid:26) L2(Q);

< (cid:18) < 1:

for all 0 < (cid:18)

(11)
In other words  H(cid:18) gets larger as (cid:18) decreases. If H is an RKHS consisting of smooth functions  then
H(cid:18) contains less smooth functions than those in H; we will show this in the example below.
Assumption 2. The integrand f lies in H(cid:18) for some 0 < (cid:18) (cid:20) 1.
We note that Assumption 2 is equivalent to assuming that f belongs to the interpolation space
[L2(Q);H](cid:18);2  or lies in the range of a power of certain integral operator [22  Theorem 4.6].
Powers of Tensor RKHSs. Let us mention the important case where RKHS H is given as the
tensor product of individual RKHSs H1; : : : ;Hd on the spaces X1; : : : ;Xd  i.e.  H = H1(cid:10)(cid:1)(cid:1)(cid:1)(cid:10)Hd
and X = X1 (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) Xd. In this case  if the distribution Q is the product of individual distributions
Q1; : : : ; Qd on X1; : : : ; Xn  it can be easily shown that the power RKHS H(cid:18) is the tensor product
of individual power RKHSs H(cid:18)
i :

Examples: Powers of Korobov spaces. Let us consider the Korobov space W (cid:11)
being the uniform distribution on [0; 1]d. The Korobov kernel (5) has a Mercer representation

(12)
Kor([0; 1]d) with Q

(cid:10) (cid:1)(cid:1)(cid:1) (cid:10) H(cid:18)
d:

1

H(cid:18) = H(cid:18)
1∑

k(cid:11)(x; y) = 1 +

(13)
2 sin 2(cid:25)ix. Note that c0(x) := 1 and fci; sig1
where ci(x) :=
i=1
constitute an orthonormal basis of L2([0; 1]). From (10) and (13)  the (cid:18)-th power of the Korobov
kernel k(cid:11) is given by

p
i=1
2 cos 2(cid:25)ix and si(x) :=

1
i2(cid:11) [ci(x)ci(y) + si(x)si(y)];
p

k(cid:18)
(cid:11)(x; y) = 1 +

1
i2(cid:11)(cid:18) [ci(x)ci(y) + si(x)si(y)]:

If (cid:11)(cid:18) 2 N  this is exactly the kernel k(cid:11)(cid:18) of the Korobov space W (cid:11)(cid:18)
other words  W (cid:11)(cid:18)
also show that the (cid:18)-th power of W (cid:11)

Kor([0; 1]) is nothing but the (cid:18)-power of W (cid:11)

Kor([0; 1]d) is W (cid:11)(cid:18)

Kor([0; 1]d) for d (cid:21) 2.

Kor([0; 1]) of lower order (cid:11)(cid:18). In
Kor([0; 1]). From this and (12)  we can

1∑

i=1

6

4.3 Result: Convergence rates for general RKHSs with random points

n
i=1 w2

i ] = O(n

i=1 be such that E[en(P ;H)] = O(n

The following result guarantees the consistency of kernel quadratures for integrands satisfying As-
∑
sumption 2  i.e.  f 2 H(cid:18).
Theorem 1. Let f(wi; Xi)gn
(cid:0)b) for some b > 0 and
E[
i=1 satis-
ﬁes Assumption 1. Let 0 < (cid:18) (cid:20) 1 be a constant. Then for any f 2 H(cid:18)  we have
(n ! 1):

(cid:0)2c) for some 0 < c (cid:20) 1=2  as n ! 1. Assume also that fXign
E [jPnf (cid:0) P fj] = O(n
∑

(14)
(cid:0)b) is w.r.t. the joint distri-

(cid:0)(cid:18)b+(1=2(cid:0)c)(1(cid:0)(cid:18)))

(cid:0)b).

n
i=1 w2

Remark 1. (a) The expectation in the assumption E[en(P ;H)] = O(n
bution of the weighted points f(wi; Xi)gn
i=1.
(cid:0)2c) requires that each weight wi decreases as n increases.
(b) The assumption E[
i ] = O(n
For instance  if maxi2f1;:::;ng jwij = O(n
(cid:0)1)  we have c = 1=2. For QMC methods  weights are
uniform wi = 1=n  so we always have c = 1=2. The quadrature rule by Bach [2] also satisﬁes
c = 1=2; see Section 2.3.
(cid:0)(cid:18)b)  which shows that the integral estimator
(c) Let c = 1=2. Then the rate in (14) becomes O(n
Pnf is consistent  even when the integrand f does not belong to H (recall H ⊊ H(cid:18) for (cid:18) < 1; see
(cid:0)(cid:18)b) is determined by 0 < (cid:18) (cid:20) 1 of the assumption f 2 H(cid:18) 
also (11)). The resulting rate O(n
which characterizes the closeness of f to H.
(cid:0)b) 
(d) When (cid:18) = 1 (well-speciﬁed case)  irrespective of the value of c  the rate in (14) becomes O(n
which recovers the rate of the worst case error E[en(P ;H)] = O(n
Examples in Korobov spaces. Let us illustrate Theorem 1
in the following setting described earlier. Let X = [0; 1]d 
H = W (cid:11)
Kor([0; 1]d)  and P be the uniform distribution on
[0; 1]d. Then H(cid:18) = W (cid:11)(cid:18)
Kor([0; 1]d)  as discussed in Section
4.2. Let us consider the two methods discussed in Section 2.3:
(i) the QMC lattice rules with randomization and (ii) the algo-
rithm by Bach [2]. For both the methods  we have c = 1=2 
and the distribution Q in Assumption 1 is uniform on [0; 1]d in
this setting. As mentioned before  these methods achieve the
(cid:0)(cid:11)+(cid:24) for arbitrarily small (cid:24) > 0 in the well-speciﬁed
rate n
setting: b = (cid:11) (cid:0) (cid:24) in our notation.
Then the assumption f 2 H(cid:18) reads f 2 W (cid:11)(cid:18)
obtain the rate O(n
speciﬁed case where W (cid:11)(cid:18)
we have shown that these methods are adaptive to the unknown smoothness of the integrand.
For the algorithm by Bach [2]  we conducted simulation experiments to support this observation 
by using code available from http://www.di.ens.fr/~fbach/quadrature.html. The setting
is what we have described with d = 1  and weights are obtained without regularization as in [2].
The result is shown in Figure 1  where r (= (cid:11)) denotes the assumed smoothness  and s (= (cid:11)(cid:18))
is the (unknown) smoothness of an integrand. The straight lines are (asymptotic) upper-bounds
in Theorem 1 (slope (cid:0)s and intercept ﬁtted for n (cid:21) 24)  and the corresponding solid lines are
numerical results (both in log-log scales). Averages over 100 runs are shown. The result indeed
shows the adaptability of the quadrature rule by Bach for the less smooth functions (i.e. s = 1; 2; 3).
We observed similar results for the QMC lattice rules (reported in Appendix D in the supplement).
5 Analysis 2: Sobolev RKHS with deterministic points
In Section 4  we have provided guarantees for methods that employ random points. However  the
result does not apply to those with deterministic points  such as (a) QMC methods without random-
ization  (b) Bayesian quadratures with deterministic points  and (c) kernel herding [7].
We aim here to provide guarantees for quadrature rules with deterministic points. To this end  we
focus on the setting where X = Rd and H is a Sobolev space [1]. The Sobolev space W r
2 of order
r 2 N is deﬁned by

Kor([0; 1]d) for 0 < (cid:18) (cid:20) 1. For such an integrand f  we
(cid:0)(cid:11)(cid:18)+(cid:24)) in (14) with arbitrarily small (cid:24) > 0. This is the same rate as for a well-
2 ([0; 1]d) was assumed for the construction of weighted points. Namely 

Figure 1: Simulation results

2 := ff 2 L2 : D(cid:11)f 2 L2 exists for allj(cid:11)j (cid:20) rg
W r

7

∑

∑
∥D(cid:11)f∥2

d

L2

= (

2

O(n

i=1 be such that en(P ; W r

jPnf (cid:0) P fj = O(n

(cid:0)b) for some b > 0 and

2 ) = O(n
(cid:0)2c) for some 0 < c (cid:20) 1=2  as n ! 1. Then for any f 2 C s
(cid:0)bs=r+(1=2(cid:0)c)(1(cid:0)s=r))

where (cid:11) := ((cid:11)1; : : : ; (cid:11)d) with (cid:11)i (cid:21) 0 is a multi-index with j(cid:11)j :=
i=1 (cid:11)i  and D(cid:11)f is the (cid:11)-th
(weak) derivative of f. Its norm is deﬁned by ∥f∥W r
)1=2. For r > d=2  this
j(cid:11)j(cid:20)r
is an RKHS with the reproducing kernel k being the Matèrn kernel; see Section 4.2.1. of [20] for
the deﬁnition.
2 of a lower order s (cid:20) r.
Our assumption on the integrand f is that it belongs to a Sobolev space W s
Note that the order s represents the smoothness of f (the order of differentiability). Therefore the
situation s < r means that f is less smooth than assumed; we consider the setting where W r
2 was
assumed for the construction of weighted points.
Rates under an assumption on weights. The ﬁrst result in this section is based on the same
assumption on weights as in Theorem 1.
Theorem 2. Let f(wi; Xi)gn

∑
2 with s (cid:20) r  we have
(15)
(cid:0)(cid:18)b+(1=2(cid:0)c)(1(cid:0)(cid:18)))  which
In other words  Theorem 2 provides a deterministic version of

(cid:0)sb=r) in (15). The minimax optimal rate in this setting (i.e.  c = 1=2) is given by n

Remark 2. (a) Let (cid:18) := s=r. Then the rate in (15) is rewritten as O(n
matches the rate of Theorem 1.
Theorem 1 for the special case of Sobolev spaces.
(b) Theorem 2 can be applied to quadrature rules with equally-weighted deterministic points  such
as QMC methods and kernel herding [7]. For these methods  we have c = 1=2 and so we obtain the
(cid:0)b with
rate O(n
(cid:0)s=d) in (15)  which is exactly
b = r=d [15]. For these choices of b and c  we obtain a rate of O(n
(cid:0)s=d) can
the optimal rate in W s
be achieved for an integrand f 2 W s
2 without knowing the degree of smoothness s; one just needs to
know its upper-bound s (cid:20) r. Namely  any methods of optimal rates in Sobolev spaces are adaptive
∑
to lesser smoothness.
Rates under an assumption on separation radius. Theorems 1 and 2 require the assumption
(cid:0)2c). However  for some algorithms  the value of c may not be available. For
instance  this is the case for Bayesian quadratures that compute the weights without any constraints
[6]; see Section 2.3. Here we present a preliminary result that does not rely on the assumption on
the weights. To this end  we introduce a quantity called separation radius:

2 . This leads to an important consequence that the optimal rate O(n

\ W s
(n ! 1):

i = O(n

n
i=1 w2

n
i=1 w2

i =

0

qn := min
i̸=j

∥Xi (cid:0) Xj∥:

In the result below  we assume that qn does not decrease very quickly as n increases. Let
diam(X1; : : : ; Xn) denote the diameter of the points.
(cid:0)b) for some b > 0 as n ! 1 
Theorem 3. Let f(wi; Xi)gn
qn (cid:21) Cn
2 with
s (cid:20) r  we have

(cid:0)b=r for some C > 0  and diam(X1; : : : ; Xn) (cid:20) 1. Then for any f 2 C s

i=1 be such that en(P ; W r

2 ) = O(n

\ W s

0

jPnf (cid:0) P fj = O(n

(cid:0) bs
r )

(n ! 1):

(cid:0)1=d for some C > 0. As above  the optimal rate for this setting is n

(16)
Consequences similar to those of Theorems 1 and 2 can be drawn for Theorem 3. In particular  the
rate in (16) coincides with that of (15) with c = 1=2. The assumption qn (cid:21) Cn
(cid:0)b=r can be veriﬁed
when points form equally-spaced grids in a compact subset of Rd. In this case  the separation radius
satisﬁes qn (cid:21) Cn
(cid:0)b with
b = r=d  which implies the separation radius satisﬁes the assumption as n
6 Conclusions
Kernel quadratures are powerful tools for numerical integration. However  their convergence guar-
antees had not been established in situations where integrands are less smooth than assumed  which
can happen in various situations in practice. In this paper  we have provided the ﬁrst known theoret-
ical guarantees for kernel quadratures in such misspeciﬁed settings.

(cid:0)b=r = n

(cid:0)1=d.

Acknowledgments

We wish to thank the anonymous reviewers for valuable comments. We also thank Chris Oates for
fruitful discussions. This work has been supported in part by MEXT Grant-in-Aid for Scientiﬁc
Research on Innovative Areas (25120012).

8

References
[1] R. A. Adams and J. J. F. Fournier. Sobolev Spaces. Academic Press  2nd edition  2003.
[2] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions.

Technical report  HAL-01118276v2  2015.

[3] F. Bach  S. Lacoste-Julien  and G. Obozinski. On the equivalence between herding and condi-

tional gradient algorithms. In Proc. ICML2012  pages 1359–1366  2012.

[4] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and

statistics. Kluwer Academic Publisher  2004.

[5] F-X. Briol  C. J. Oates  M. Girolami  and M. A. Osborne. Frank-Wolfe Bayesian quadrature:
Probabilistic integration with theoretical guarantees. In Adv. NIPS 28  pages 1162–1170  2015.
[6] F-X. Briol  C. J. Oates  M. Girolami  M. A. Osborne  and D. Sejdinovic. Probabilistic integra-

tion: A role for statisticians in numerical analysis? arXiv:1512.00933v4 [stat.ML]  2016.

[7] Y. Chen  M. Welling  and A. Smola. Supersamples from kernel-herding. In Proc. UAI 2010 

pages 109–116  2010.

[8] J. Dick. Explicit constructions of quasi-Monte Carlo rules for the numerical integration of

high-dimensional periodic functions. Siam J. Numer. Anal.  45:2141–2176  2007.

[9] J. Dick. Walsh spaces containing smooth functions and quasi–Monte Carlo rules of arbitrary

high order. Siam J. Numer. Anal.  46(3):1519–1553  2008.

[10] J. Dick. Higher order scrambled digital nets achieve the optimal rate of the root mean square

error for smooth integrands. The Annals of Statistics  39(3):1372–1398  2011.

[11] J. Dick  F. Y. Kuo  and I. H. Sloan. High dimensional numerical integration - the quasi-Monte

Carlo way. Acta Numerica  22(133-288)  2013.

[12] M. Gerber and N. Chopin. Sequential quasi Monte Carlo. Journal of the Royal Statistical

Society. Series B. Statistical Methodology  77(3):509–579  2015.

[13] F. J. Hickernell. A generalized discrepancy and quadrature error bound. Mathematics of Com-

putation of the American Mathematical Society  67(221):299–322  1998.

[14] S. Lacoste-Julien  F. Lindsten  and F. Bach. Sequential kernel herding: Frank-Wolfe optimiza-

tion for particle ﬁltering. In Proc. AISTATS 2015  2015.

[15] E. Novak. Deterministic and Stochastic Error Bounds in Numerical Analysis. Springer-Verlag 

1988.

[16] E. Novak and H. Wózniakowski. Tractability of Multivariate Problems  Vol. I: Linear Infor-

mation. EMS  2008.

[17] C. J. Oates and M. Girolami. Control functionals for quasi-Monte Carlo integration. In Proc.

AISTATS 2016  2016.

[18] C. J. Oates  M. Girolami  and N. Chopin. Control functionals for Monte Carlo integration.

Journal of the Royal Statistical Society. Series B  2017  to appear.

[19] A. O’Hagan. Bayes–Hermite quadrature.

29:245–260  1991.

Journal of Statistical Planning and Inference 

[20] E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[21] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Schölkopf  and G. R.G. Lanckriet. Hilbert

space embeddings and metrics on probability measures. JMLR  11:1517–1561  2010.

[22] I. Steinwart and C. Scovel. Mercer’s theorem on general domains: On the interaction between

measures  kernels  and RKHS. Constructive Approximation  35(363-417)  2012.

[23] J. Yang  V. Sindhwani  H. Avron  and M. W. Mahoney. Quasi-Monte Carlo feature maps for

shift-invariant kernels. In Proc. ICML 2014  2014.

9

,Calvin McCarter
Seyoung Kim
Motonobu Kanagawa
Bharath Sriperumbudur
Kenji Fukumizu
Ge Yang
Samuel Schoenholz